{
  "id": "http://arxiv.org/abs/1107.4687v2",
  "title": "Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification",
  "authors": [
    "Luis Quesada",
    "Fernando Berzal",
    "Francisco J. Cortijo"
  ],
  "abstract": "Model-based language specification has applications in the implementation of\nlanguage processors, the design of domain-specific languages, model-driven\nsoftware development, data integration, text mining, natural language\nprocessing, and corpus-based induction of models. Model-based language\nspecification decouples language design from language processing and, unlike\ntraditional grammar-driven approaches, which constrain language designers to\nspecific kinds of grammars, it needs general parser generators able to deal\nwith ambiguities. In this paper, we propose Fence, an efficient bottom-up\nparsing algorithm with lexical and syntactic ambiguity support that enables the\nuse of model-based language specification in practice.",
  "text": "arXiv:1107.4687v2  [cs.CL]  7 Oct 2011\nFence — An Eﬃcient Parser with Ambiguity Support\nfor Model-Driven Language Speciﬁcation\nLuis Quesada, Fernando Berzal, and Francisco J. Cortijo\nDepartment of Computer Science and Artiﬁcial Intelligence, CITIC, University of Granada,\nGranada 18071, Spain\nlquesada@decsai.ugr.es, fberzal@decsai.ugr.es, cb@decsai.ugr.es\nModel-based language speciﬁcation has applications in the implementation of language processors,\nthe design of domain-speciﬁc languages, model-driven software development, data integration,\ntext mining, natural language processing, and corpus-based induction of models. Model-based\nlanguage speciﬁcation decouples language design from language processing and, unlike traditional\ngrammar-driven approaches, which constrain language designers to speciﬁc kinds of grammars, it\nneeds general parser generators able to deal with ambiguities. In this paper, we propose Fence, an\neﬃcient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables\nthe use of model-based language speciﬁcation in practice.\nI. INTRODUCTION\nMost existing language speciﬁcation techniques [2] re-\nquire the developer to provide a textual speciﬁcation of\nthe language grammar.\nWhen the use of an explicit model is required, its im-\nplementation requires the development of the conversion\nsteps between the model and the grammar, and between\nthe parse tree and the model instance. Thus, in this case,\nthe implementation of the language processor becomes\nharder.\nWhenever the language speciﬁcation is modiﬁed, the\ndeveloper has to manually propagate changes throughout\nthe entire language processor pipeline. These updates are\ntime-consuming, tedious, and error-prone. This hampers\nthe maintainability and evolution of the language [10].\nTypically, diﬀerent applications that use the same lan-\nguage are developed. For example, the compiler, diﬀerent\ncode generators, and the tools within the IDE, such as\nthe editor or the debugger. The traditional language pro-\ncessor development procedure enforces the maintenance\nof several copies of the same language speciﬁcation in\nsync.\nIn contrast, model-based language speciﬁcation [12] al-\nlows the graphical speciﬁcation of a language. By follow-\ning this approach, no conversion steps have to be devel-\noped and the model can be modiﬁed as needed without\nhaving to worry about the language processor, which will\nbe automatically updated accordingly. Also, as the soft-\nware code can be combined with the model in a clean\nfashion, there is no embedding or mixing with the lan-\nguage processor.\nModel-based language speciﬁcation has direct applica-\ntions in the following ﬁelds:\n• The generation of language processors (compilers\nand interpreters) [1].\n• The\nspeciﬁcation\nof\ndomain-speciﬁc\nlanguages\n(DSLs), which are languages oriented to the do-\nmain of a particular problem, its representation, or\nthe representation of a speciﬁc technique to solve\nit [6, 7, 15].\n• The development of Model-Driven Software Devel-\nopment (MDSD) tools [19].\n• Data integration, as part of the preprocessing pro-\ncess in data mining [20].\n• Text mining applications [4, 21], in order to extract\nhigh quality information from the analysis of huge\ntext data bases.\n• Natural language processing [8] in restricted lexical\nand syntactic domains.\n• Corpus-based induction of models [11].\nHowever, due to the nature of this speciﬁcation tech-\nnique and the aforementioned application ﬁelds, the spec-\niﬁcation of separate elements may cause lexical ambigui-\nties to arise. Lexical ambiguities occur when an input\nstring simultaneously corresponds to several token se-\nquences [16]. Tokens within alternative sequences may\noverlap.\nThe Lamb lexical analyzer [17] captures all the possible\nsequences of tokens and generates a lexical analysis graph\nthat describes them all. In these graphs each token is\nlinked to its preceding and following tokens, and there\nmay be several starting tokens. Each path in this graph\ndescribes a possible sequence of tokens that can be found\nwithin the input string.\nOur proposal, Fence, accepts as input a lexical analy-\nsis graph, and performs an eﬃcient ambiguity-supporting\nsyntactic analysis, producing a parse graph that repre-\nsents all the possible parse trees. The parsing process\ndiscards any sequence of tokens that does not provide\na valid syntactic sentence conforming to the production\nset of the language speciﬁcation. Therefore, a context-\nsensitive lexical analysis is implicitly performed, as the\nparsing determines which tokens are valid.\n2\nThe combined use of a Lamb-like lexer and Fence al-\nlows processing languages with lexical and syntactic am-\nbiguities, which renders model-based language speciﬁca-\ntion techniques usable.\nII. BACKGROUND\nFormal grammars are used to specify the syntax of a\nlanguage [1]. Context-free grammars are formal gram-\nmars in which the productions are of the form N →\n(Σ ∪N)∗[3], where N is a ﬁnite set of nonterminal sym-\nbols, none of which appear in strings formed from the\ngrammar; and Σ is a ﬁnite set of terminal symbols, also\ncalled tokens, that can appear in strings formed from\nthe grammar, being Σ disjoint from N. These grammars\ngenerate context-free languages.\nA context-free grammar is said to be ambiguous if\nthere exists a string that can be generated in more than\none way. A context-free language is inherently ambiguous\nif all context-free grammars generating it are ambiguous.\nTypically, language processing tools divide the analysis\ninto two separate phases; namely, scanning (or lexical\nanalysis) and parsing (or syntax analysis).\nA lexical analyzer, also called lexer or scanner, pro-\ncesses an input string conforming to a language speciﬁ-\ncation and produces the sequence of tokens found within\nit.\nA syntactic analyzer, also called parser, processes an\ninput data structure consisting of tokens and determines\nits grammatical structure with respect to the given lan-\nguage grammar, usually in the form of parse trees.\nIII. LEXICAL ANALYSIS WITH AMBIGUITY SUPPORT\nWhen using a lex-generated lexer [14], tokens get as-\nsigned a priority based on the length of the performed\nmatches and, when there is a tie in the length, on the\norder of speciﬁcation.\nGiven a language speciﬁcation that describes the to-\nkens listed in Figure 1, the input string “&5.2& /25.20/”\ncan correspond to the four diﬀerent lexical analysis al-\nternatives enumerated in Figure 2, depending on whether\nthe sequences of digits separated by points are considered\nreal numbers or integer numbers separated by points.\n(-|\\+)?[0-9]+\nInteger\n(-|\\+)?[0-9]+\\.[0-9]+\nReal\n\\.\nPoint\n\\/\nSlash\n\\&\nAmpersand\nFigure 1 Speciﬁcation of the token types and associated reg-\nular expressions of a lexically-ambiguous language.\nThe productions shown in Figure 3 illustrate a sce-\nnario of lexical ambiguity sensitivity. Depending on the\nsurrounding tokens, which may be either Ampersand to-\nkens or Slash tokens, the sequences of digits separated by\n• Ampersand Integer Point Integer Ampersand\nSlash Integer Point Integer Slash\n• Ampersand Integer Point Integer Ampersand\nSlash Real Slash\n• Ampersand Real Ampersand Slash Integer Point\nInteger Slash\n• Ampersand Real Ampersand Slash Real Slash\nFigure 2 Diﬀerent possible token sequences in the input string\n“&5.2& /25.20/” due to the lexically-ambiguous language\nspeciﬁcation in Figure 1.\npoints should be considered either Real tokens or Integer\nPoint Integer token sequences. The expected results of\nanalyzing the input string “&5.2& /25.20/” is shown in\nFigure 4.\nE ::= A B\nA ::= Ampersand Real Ampersand\nB ::= Slash Integer Point Integer Slash\nFigure 3 Context-sensitive productions that solve the lexical\nambiguities in Figure 2.\nThe Lamb lexer [17] performs a lexical analysis that ef-\nﬁciently captures all the possible sequences of tokens and\ngenerates a lexical analysis graph that describes them all,\nas shown in Figure 5. The further application of a parser\nthat supports lexical ambiguities would produce the only\npossible valid sentence, which, in turn, would be based\non the only valid lexical analysis possible. The intended\nresults are shown in Figure 6.\nIV. SYNTACTIC ANALYSIS WITH AMBIGUITY\nSUPPORT\nTraditional eﬃcient parsers for restricted context-free\ngrammars, as the LL [18], SLL, LR [13], SLR, LR(1), or\nLALR parsers [1], do not consider ambiguities in syntac-\ntic analysis, so they cannot be used to perform parsing\nin those cases. The eﬃciency of these parsers is O(n),\nbeing n the token sequence length.\nExisting parsers for unrestricted context-free grammar\nparsing, as the CYK parser [9, 22] and the Earley parser\n[5], can consider syntactic ambiguities. The eﬃciency of\nthese parsers is O(n3), being n the token sequence length.\nIn contrast to the aforementioned techniques, our pro-\nposed parser, Fence, is able to eﬃciently process lexical\nanalysis graphs and, therefore, consider lexical ambigui-\nties. It also takes into consideration syntactic ambigui-\nties.\nFence produces a parse graph that contains as many\nstarting initial grammar symbols as diﬀerent parse trees\nexist.\n3\nPoint\n.\nInteger\n20\nSlash\n/\nReal\n5.2\nAmpersand\n&\nSlash\n/\nAmpersand\n&\nInteger\n25\nFigure 4 Intended lexical analysis.\nPoint\n.\nInteger\n20\nSlash\n/\nReal\n5.2\nAmpersand\n&\nSlash\n/\nAmpersand\n&\nInteger\n5\nPoint\n.\nReal\n25.20\nInteger\n25\nInteger\n2\nFigure 5 Lexical analysis graph, as produced by the Lamb lexer.\nPoint\n.\nInteger\n20\nSlash\n/\nReal\n5.2\nAmpersand\n&\nSlash\n/\nAmpersand\n&\nInteger\n25\nE\nB\nA\nFigure 6 Syntactic analysis graph, as produced by applying a parser that supports lexical ambiguities to the lexical analysis\ngraph shown in Figure 5. Squares represent nonterminal symbols found during the parse process.\nPoint\n.\nInteger\n20\nSlash\n/\nReal\n5.2\nAmpersand\n&\nSlash\n/\nAmpersand\n&\nInteger\n5\nPoint\n.\nReal\n25.20\nInteger\n25\nInteger\n2\nFigure 7 Extended lexical analysis graph corresponding to the lexical analysis graph shown in Figure 5. Gray nodes represent\ncores\nPoint\n.\nInteger\n20\nSlash\n/\nReal\n5.2\nAmpersand\n&\nSlash\n/\nAmpersand\n&\nInteger\n5\nPoint\n.\nReal\n25.20\nInteger\n25\nInteger\n2\nE\nB\nA\nFigure 8 Extended syntax analysis graph corresponding to the extended lexical analysis graph shown in Figure 7. Squares\nrepresent nonterminal symbols found during the parse process.\n4\nA. Extended Lexical Analysis Graph\nIn order to eﬃciently perform the parsing, Fence uses\nan extended lexical analysis graph that stores informa-\ntion about partially applied rules, namely handles, in\ndata structures, namely cores.\nGiven a sequence of symbols T = t1...tn as the right\nhand side of a production rule, a dotted rule is a pair\n(production, pos), where 0 ≤pos ≤n.\nA handle is a dotted rule associated to a starting po-\nsition in the input string.\nA core is a set of handles.\nIn an extended lexical analysis graph, tokens are not\nlinked to their preceding and following tokens, but to\ntheir preceding and following cores. Cores are, in turn,\nlinked to their preceding and following token sets. For\nexample, the extended lexical analysis graph correspond-\ning to the lexical analysis graph in Figure 5 is shown in\nFigure 7.\nAs cores represent a starting position in the input\nstring, handles are a dotted rule associated to a start-\ning core.\nEach handle could be used to make the analysis\nprogress (namely, SHIFT actions in LR-like parsers) or\nperform a reduction (namely, REDUCE actions in LR-\nlike parsers).\nA shift action needs to be performed associated to a\nsource core and a target core. Applying the shift action\nto a handle involves creating a new handle in each target\ncore that follows the symbols that follow the source core.\nA reduction action needs to be performed associated\nto a start core and an end core.\nB. Parsing Algorithm\nThe algorithm uses a global matched handle pool,\nnamely hPool, that contains handles associated to the\nnext symbol they can match.\nThe ﬁrst step of our algorithm converts the input lexi-\ncal analysis graph into an extended lexical analysis graph.\nThis conversion is performed by completing the graph\nwith a ﬁrst core, which links to the tokens with an empty\npreceding token set; a last core, which is linked from the\ntokens with an empty following token set; and, for each\none of the other tokens, a core that precedes it. Links\nbetween tokens are then converted to links from tokens\nto the cores preceding each token of their following token\nset and vice versa.\nThe second step of our algorithm performs the pars-\ning, by progressively applying productions and storing\nhandles in cores.\nFirst, the productions with an empty right hand side\nare removed from the grammar and their left hand side\nelement is stored in a set named epsilonSymbols.\nThe addProd procedure described in Figure 9 gener-\nates a handle conforming to a production and a starting\nright hand side element index, adds it to a core and, for\neach symbol in the following symbol set of that core that\nmatches the current production element, adds a handle\nto the production pool with an anchor to that symbol.\nIt also considers productions with an empty right hand\nside: if an element is in the epsilonSymbols set, both the\npossibilities of it being reduced or not by that produc-\ntion are considered, that is, if an element corresponds is\nin the epsilonSymbols set, a new handle that skips that\nelement is added to the same core. It should be noted\nthat this process is iterative, as many sucessive elements\nof the right hand side of a production could be in the\nepsilonSymbols set.\nprocedure addProd(Prod p,int index,Core c,\nCore start,Symbol[] contents):\ndo:\nh = new Handle(p,index,start)\nc.handles.add(h)\nif index < p.right.size:\nfor each Symbol s in c.following:\nif s.type == p.right[index].type:\nhPool.add({new Handle(p,index,start,\ncontents+s)})\nindex++\ncontents.add(null) // epsilon symbol case\nwhile index < p.right.size &&\nepsilonSymbols.has(p.right[index-1].type)\nFigure 9 The addProd procedure pseudocode.\nfor each Prod p in prodSet:\nfor each Core c in coreSet:\nflag = false\nfor each Token t in c.following:\nif t is in p.selectSet:\nflag = true\nif flag == true:\naddProd(p,0,c,c,null)\nFigure 10 Core initialization.\nThe SELECT set contains all of the terminal symbols\nﬁrst produced by the production.\nThe parser is initialized by generating every possible\nhandle that would match the ﬁrst right hand side element\nof a rule, and adding it to every core whose following to-\nkens are in the SELECT set of the production, as shown\nin Figure 10.\nThe parsing process consists on iteratively extracting\nhandles from hPool and matching them with the follow-\ning, already known, symbol. The handles derived from\nthat match are added to the corresponding cores and, for\neach symbol in the following set of symbols of the core\nthat matches the next unmatched element of the produc-\ntion, to the rule pool.\nIn case all the elements of a production match a se-\nquence of symbols, a new symbol is generated by reducing\nthem, and added to the rule start core. If a new added\n5\nwhile hPool is not empty:\n{h,symbol} = hPool.extract()\nif h.index == h.prod.right.size-1:\n// Production matched all its elements.\n// i.e. Reduction\ns = new Symbol(h.prod.left.type,h.contents)\nh.startCore.add(s)\ns.preceding.add(h.start)\nfor each Core c in h.following:\nc.preceding.add(s)\ns.following.add(c)\nfor each Handle h in h.startCore that\nis waiting for s.type:\nhPool.add({new Handle(h.prod,h.index,\nh.start,contents),s})\nelse: // i.e. Shift\nfor each Core c in h.following:\naddProd(h.prod,h.index+1,c,h.start)\nFigure 11 Pseudocode of the parsing algorithm.\nsymbol only has the ﬁrst core in its preceding core set\nand the last core in its following core set, and it is an\ninstance of the initial symbol of the grammar, it is added\nto the parse graph starting symbol set. The pseudocode\nfor this process is shown in Figure 11.\nIt should be noted that handles are never removed from\nthe cores when shift actions are performed. This allows\ngenerating parse trees that consist of nonterminal sym-\nbols found later in the parsing process.\nThe result is an extended parse graph, as the one\nshown in Figure 8.\nIn the last step of the algorithm, all the cores are\nstripped oﬀthe graph and the symbols are linked back to\ntheir new preceding and following symbol sets, in order\nto produce the output syntax analysis graph.\nC. Eﬃciency Analysis\nThe following eﬃciency analysis does not consider enu-\nmerating all the diﬀerent parse trees, which the pseu-\ndocode shown in section 4.2 does and has an exponential\norder of eﬃciency. Instead, it considers a simpliﬁed the-\noretical parsing process.\nLet n denote the input string length, p the number\nof productions of the grammar, l the maximum length\nof a production (the number of symbols in its right hand\nside), and s the number of terminal symbols of the gram-\nmar.\nWe deﬁne d as the dimension of a grammar, that is,\nthe sum of the number of symbols that appear in the\nright hand side of the productions of the grammar.\nNonterminal symbols, which are created whenever\na reduction is performed, can be deﬁned as tuples\n(X, start, end), being start the start core identiﬁer and\nend the end core identiﬁer, where end >= start. A non-\nterminal symbol corresponds to a single parse tree if the\ngrammar has no ambiguities, and may correspond to a\nset of parse trees if the grammar has lexical or syntactic\nambiguities.\nIf the input string is successfully parsed, the result will\nbe (S, 1, n), being S the initial symbol of the grammar.\nAn extended lexical analysis graph contains a number\nof tokens that is conditioned by the input length and the\npresence of lexical ambiguities. It also contains a number\nof cores that is conditioned by the number of tokens.\nEach core will store a number of handles that is con-\nditioned by the grammar power of expression and the\npresence of lexical ambiguities.\n1. Parsing LR Grammars without Lexical Ambiguities\nAn input string length of n means a maximum of n\ntokens can be found, in the absence of lexical ambigui-\nties. A lexical analysis graph with n tokens will contain\na maximum of n cores.\nIn this case, each core can initially store up to l han-\ndles, as symbols that appear in the left hand side of pro-\nductions with an empty right hand side may be skipped\nduring the initialization, and all the diﬀerent handles that\nrepresent these possibilities have to be considered. Thus,\nn · l handles may initially exist.\nEach handle can cause, at most, l shift actions, each\nof which would generate, at most, a single new handle.\nEach shift action can be performed in constant time.\nTherefore, a maximum of n · l · (1 + l) handles can be\ngenerated.\nEach handle can be generated in constant\ntime.\nAlso, each handle can cause, at most, a reduction,\nwhich would generate a single nonterminal symbol. This\nreduction can be performed in constant time.\nThus, the order of eﬃciency of our algorithm in this\ncase is O(n · l2).\n2. Parsing LR Grammars with Lexical Ambiguities\nAn input string length of n means a maximum of n · s\ntokens can be found, in the presence of lexical ambigui-\nties. A lexical analysis graph with n·s tokens will contain\na maximum of n · s cores.\nIn this case, each core can initially store up to l han-\ndles, as symbols that appear in the left hand side of pro-\nductions with an empty right hand side may be skipped\nduring the initialization, and all the diﬀerent handles that\nrepresent these possibilities have to be considered. Thus,\nn · s · l handles may initially exist.\nEach handle can cause, at most, l shift actions, each\nof which would generate up to s handles. This sums up\nto s · l handles.\nTherefore, a maximum of n · s · l · (1 + s · l) handles can\nbe generated. Each handle can be generated in constant\ntime.\nAlso, each handle can cause, at most, a reduction,\n6\nwhich would generate a single nonterminal symbol. This\nreduction can be performed in constant time.\nThus, the order of eﬃciency of our algorithm in this\ncase is O(n · s2 · l2). The memory it uses has an order of\nO(n · s2 · l2), too.\nConsidering s as a constant, the order of eﬃciency of\nour algorithm is O(n · l2). The reason s appears in the\norder of eﬃciency is that lexical ambiguities, which could\nbe solved by using a parser with syntactic ambiguity sup-\nport and rewriting the grammars in order to model them\nas syntactic ambiguities, are considered during a previous\nlexical analysis, thus generating tokens which, otherwise,\nwould be nonterminal symbols.\n3. Parsing CFG Grammars without Lexical Ambiguities\nAn input string length of n means a maximum of n\ntokens can be found, in the absence of lexical ambigui-\nties. A lexical analysis graph with n tokens will contain\na maximum of n cores.\nIn this case, each core can initially store up to d han-\ndles, as symbols that appear in the left hand side of pro-\nductions with an empty right hand side may be skipped\nduring the initialization, and all the diﬀerent handles that\nrepresent these possibilities have to be considered. Thus,\nn · d handles may initially exist.\nEach handle can cause, at most, l shift actions, each\nof which would generate, at most, a single new handle.\nEach shift action can be performed in constant time.\nTherefore, a maximum of n · d · (1 + l) handles can\nbe generated. Each handle can be generated in constant\ntime.\nAlso, each handle can cause, at most, a reduction,\nwhich would generate a single nonterminal symbol. This\nreduction can be performed in constant time.\nThus, the order of eﬃciency of our algorithm in this\ncase is O(n · d · l). The memory it uses has an order of\nO(n · d · l), too.\n4. Parsing CFG Grammars with Lexical Ambiguities\nAn input string length of n means a maximum of n · s\ntokens can be found, in the presence of lexical ambigui-\nties. A lexical analysis graph with n·s tokens will contain\na maximum of n · s cores.\nIn this case, each core can initially store up to d han-\ndles, as symbols that appear in the left hand side of pro-\nductions with an empty right hand side may be skipped\nduring the initialization, and all the diﬀerent handles that\nrepresent these possibilities have to be considered. Thus,\nn · s · d handles may initially exist.\nEach handle can cause, at most, l shift actions, each\nof which would generate up to s handles. This sums up\nto s · l handles.\nTherefore, a maximum of n·s·d·(1 + s·l) handles can\nbe generated. Each handle can be generated in constant\ntime.\nAlso, each handle can cause, at most, a reduction,\nwhich would generate a single nonterminal symbol. This\nreduction can be performed in constant time.\nThus, the order of eﬃciency of our algorithm in this\ncase is O(n · s2 · d · l). The memory it uses has an order\nof O(n · s2 · d · l), too.\nConsidering s as a constant, the order of eﬃciency of\nour algorithm is O(n · d · l). The reason s appears in the\norder of eﬃciency is that lexical ambiguities, which could\nbe solved by using a parser with syntactic ambiguity sup-\nport and rewriting the grammars in order to model them\nas syntactic ambiguities, are considered during a previous\nlexical analysis, thus generating tokens which, otherwise,\nwould be nonterminal symbols.\nV. CONCLUSIONS AND FUTURE WORK\nModel-based language speciﬁcation decouples lan-\nguage design from language processing. Languages spec-\niﬁed using such technique may be lexically and syn-\ntactically-ambiguous.\nThus, general parser generators\nable to deal with ambiguities are needed.\nWe have presented Fence, an eﬃcient bottom-up pars-\ning algorithm with lexical and syntactic ambiguity sup-\nport that enables the use of model-based language spec-\niﬁcation in practice.\nFence accepts a lexical analysis graph as input, per-\nforms a syntactic analysis conforming to a grammar spec-\niﬁcation, and produces as output a compact representa-\ntion of a set of parse trees.\nWe plan to apply model-based language speciﬁcation\nin the implementation of language processor genera-\ntors, model-driven software development, data integra-\ntion, corpus-based induction of models, text mining, and\nnatural language processing.\nReferences\n[1] Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeﬀrey D.\nUllman. Compilers: Principles, Techniques, and Tools.\nAddison Wesley, 2nd edition, 2006.\n[2] Alfred V. Aho and Jeﬀrey D. Ullman.\nThe Theory of\nParsing, Translation, and Compiling, Volume I: Parsing\n& Volume II: Compiling. Prentice Hall, Englewood Cliﬀs,\nN.J., 1972.\n[3] Noam Chomsky. Three models for the description of lan-\nguage. IRE Transactions on Information Theory, 2:113–\n123, 1956.\n[4] Valter Crescenzi and Giansalvatore Mecca.\nAutomatic\ninformation extraction from large websites. Journal of\nthe ACM, 51:731–779, 2004.\n[5] Jay Earley. An eﬃcient context-free parsing algorithm.\nCommunications of the ACM, 26:57–61, 1983.\n[6] Martin Fowler.\nDomain-Speciﬁc Languages.\nAddison-\nWesley Signature Series (Fowler), 2010.\n7\n[7] Paul Hudak.\nBuilding domain-speciﬁc embedded lan-\nguages. ACM Computing Surveys, vol. 28, no. 4es, art.\n196, 1996.\n[8] Daniel Jurafsky and James H. Martin. Speech and Lan-\nguage Processing: An Introduction to Natural Language\nProcessing, Computational Linguistics and Speech Recog-\nnition. Prentice Hall, 2nd edition, 2009.\n[9] Tadao Kasami and Koji Torii. A syntax-analysis proce-\ndure for unambiguous context-free grammars. Journal of\nthe ACM, 16:423–431, 1969.\n[10] Lennart C. L. Kats, Eelco Visser, and Guido Wachsmuth.\nPure and declarative syntax deﬁnition:\nparadise lost\nand regained. In Proceedings of the ACM international\nconference on Object oriented programming systems lan-\nguages and applications (OOPSLA ’10), pages 918–932,\n2010.\n[11] Dan Klein. Christopher d. manning. In Proceedings of the\n42nd Annual Meeting on Association for Computational\nLinguistics (ACL ’04), pages 478–485, 2004.\n[12] Anneke Kleppe. Towards the generation of a text-based\nide from a language metamodel. volume 4530 of Lecture\nNotes in Computer Science, pages 114–129, 2007.\n[13] Donald E. Knuth. On the translation of languages from\nleft to right. Information and Control, 8:607–639, 1965.\n[14] John\nR.\nLevine,\nTony\nMason,\nand\nDoug\nBrown.\nlex&yacc. O’Reilly, 2nd edition, 1992.\n[15] Marjan Mernik, Jan Heering, and Anthony M. Sloane.\nWhen and how to develop domain-speciﬁc languages.\nACM Computing Surveys, 37:316–344, 2005.\n[16] J. R. Nawrocki. Conﬂict detection and resolution in a lex-\nical analyzer generator. Information Processing Letters,\n38:323–328, 1991.\n[17] Luis\nQuesada,\nFernando\nBerzal,\nand\nJuan-Carlos\nCubero. Lamb — a lexical analyzer with ambiguity sup-\nport.\nIn Proc. of the 6th International Conference on\nSoftware and Data Technologies, 2011. (in press).\n[18] Daniel J. Rosenkrantz and Richard Edwin Stearns. Prop-\nerties of deterministic top-down grammars. Information\nand Control, 17:226–256, 1970.\n[19] Douglas C. Schmidt. Model-driven engineering. IEEE\nComputer, 39:25–31, 2006.\n[20] Pang-Ning Tan and Vipin Kumar. Introduction to Data\nMining. Addison Wesley, 2006.\n[21] Jordi Turmo, Alicia Ageno, and Neus Cata`a. Adaptive\ninformation extraction. ACM Computing Surveys, vol.\n38, no. 2, art. 4, 2006.\n[22] Daniel H. Younger. Recognition and parsing of context-\nfree languages in time n3.\nInformation and Control,\n10:189–208, 1967.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2011-07-23",
  "updated": "2011-10-07"
}