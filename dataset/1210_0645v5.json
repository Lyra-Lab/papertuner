{
  "id": "http://arxiv.org/abs/1210.0645v5",
  "title": "Nonparametric Unsupervised Classification",
  "authors": [
    "Yingzhen Yang",
    "Thomas S. Huang"
  ],
  "abstract": "Unsupervised classification methods learn a discriminative classifier from\nunlabeled data, which has been proven to be an effective way of simultaneously\nclustering the data and training a classifier from the data. Various\nunsupervised classification methods obtain appealing results by the classifiers\nlearned in an unsupervised manner. However, existing methods do not consider\nthe misclassification error of the unsupervised classifiers except unsupervised\nSVM, so the performance of the unsupervised classifiers is not fully evaluated.\nIn this work, we study the misclassification error of two popular classifiers,\ni.e. the nearest neighbor classifier (NN) and the plug-in classifier, in the\nsetting of unsupervised classification.",
  "text": "arXiv:1210.0645v5  [cs.LG]  20 May 2013\nNonparametric Unsupervised Classiﬁcation\nYingzhen Yang and Thomas S. Huang\nDepartment of Electrical and Computer Engineering\nUniversity of Illinois at Urbana-Champaign, USA\n{yyang58,huang}@ifp.uiuc.edu\nAbstract. Unsupervised classiﬁcation methods learn a discriminative\nclassiﬁer from unlabeled data, which has been proven to be an eﬀective\nway of simultaneously clustering the data and training a classiﬁer from\nthe data. Various unsupervised classiﬁcation methods obtain appealing\nresults by the classiﬁers learned in an unsupervised manner. However,\nexisting methods do not consider the misclassiﬁcation error of the unsu-\npervised classiﬁers except unsupervised SVM, so the performance of the\nunsupervised classiﬁers is not fully evaluated. In this work, we study the\nmisclassiﬁcation error of two popular classiﬁers, i.e. the nearest neighbor\nclassiﬁer (NN) and the plug-in classiﬁer, in the setting of unsupervised\nclassiﬁcation. The upper bound for the misclassiﬁcation error of both\nclassiﬁers involves only pairwise similarity between the data points. We\nprove that the error of the plug-in classiﬁer is asymptotically bounded by\nthe weighted volume of cluster boundary [1]. Also, the normalized graph\nLaplacian from the induced similarity kernel recovers diﬀerent types of\ntransition kernels for Diﬀusion maps [2,3], which reveals the close rela-\ntionship between manifold learning and unsupervised classiﬁcation. We\nshow that with the normalized graph Laplacian, the similarity kernel in-\nduced by the misclassiﬁcation error of the plug-in classiﬁer corresponds\nto the Fokker-Planck operator; and the similarity kernel induced by the\nvolume of misclassiﬁed region by the plug-in classiﬁer correspond to the\nLaplace-Beltrami operator on the data manifold.\n1\nIntroduction\nClustering methods partition the data into a set of self-similar clusters. Repre-\nsentative clustering methods include K-means [4] which minimizes the within-\ncluster dissimilarities, spectral clustering [5] which identiﬁes clusters of more\ncomplex shapes lying on some low dimensional manifolds, and statistical model-\ning method [6] approximates the data by a mixture of parametric distribution.\nOn the other hand, viewing clusters as classes, recent works on unsupervised\nclassiﬁcation learn a classiﬁer from unlabeled data, and they have established\nthe connection between clustering and multi-class classiﬁcation from a supervised\nlearning perspective. [7] learns a max-margin two-class classiﬁer in an unsuper-\nvised manner. Such method is known as unsupervised SVM, whose theoretical\nproperty is further analyzed in [8]. Also, [9] and [10] learn the kernelized Gaus-\nsian classiﬁer and the kernel logistic regression classiﬁer respectively. Both [10]\n2\nYingzhen Yang and Thomas S. Huang\nand [11] adopt the entropy of the posterior distribution of the class label by the\nclassiﬁer to measure the quality of the learned classiﬁer, and the parameters of\nsuch unsupervised classiﬁers can be computed by continuous optimization. More\nrecent work presented in [12] learns an unsupervised classiﬁer by maximizing the\nmutual information between cluster labels and the data, and the Squared-Loss\nMutual Information is employed to produce a convex optimization problem.\nHowever, previous methods either do not consider the misclassiﬁcation error\nof the learned unsupervised classiﬁers, one of the most important performance\nmeasures for classiﬁcation, or only minimizes the error of unsupervised SVM [7].\nTherefore, the performance of the unsupervised classiﬁer is not fully evaluated.\nAlthough Bengio et al. [13] analyze the out-of-sample error for unsupervised\nlearning algorithms, their method focuses on lower-dimensional embedding of\nthe data points and does not train a classiﬁer from unlabeled data.\nIn contrast, we analyze the unsupervised nearest neighbor classiﬁer (NN) and\nthe plug-in classiﬁer from unlabeled data by the training scheme for unsupervised\nclassiﬁcation introduced in [7], and derive the bound for their misclassiﬁcation\nerror. Although the generalization properties of the NN and the plug-in classiﬁer\nhave been extensity studied [14,15], to the best of our knowledge most analysis\nfocuses on the case of average generalization error. Unsupervised classiﬁcation\nmethods, such as unsupervised SVM [7], measure the quality of a speciﬁc data\npartition by its associated misclassiﬁcation error, so we derive the data depen-\ndent misclassiﬁcation error with respect to ﬁxed training data. The resultant\nerror bound comprises pairwise similarity between the data points, which also\ninduces the similarity kernel over the data. We prove that the error of the plug-in\nclassiﬁer is asymptotically bounded by the (scaled) weighted volume of cluster\nboundary [1], and the latter is designed to encourage the cluster boundary to\navoid high density regions following the Low Density Separation assumption [16].\nBuilding a graph where the nodes represent the data and the edge weight is\nset by the induced similarity kernel, clustering by minimizing the error bound\nof the two unsupervised classiﬁers reduces to (normalized) graph-cut problems,\nwhich can be solved by normalized graph Laplacian (or Normalized Cut [17]).\nThe normalized graph Laplacian from the similarity kernel induced by the upper\nbound for the error (or the volume of the misclassiﬁed region) of the plug-in\nclassiﬁer renders a certain type of transition kernel for Diﬀusion maps [2,3], and\nthe Fokker-Planck operator (or the Laplace-Beltrami operator) is recovered by\nthe inﬁnitesimal generator of the corresponding Markov chains. It is interesting\nto observe that the volume of the misclassiﬁed region is independent of the\nmarginal data distribution, which is consistent with the fact that the Laplace-\nBeltrami operator only captures the geometric information. This implies close\nrelationship between manifold learning and unsupervised classiﬁcation.\nThe rest part of this paper is organized as follows. We ﬁrst introduce the\nformulation of unsupervised classiﬁcation in Section 2, then derive the error\nbound for the unsupervised NN and plug-in classiﬁers and explain the connection\nto other related methods in Section 3. We conclude the paper in Section 4.\nNonparametric Unsupervised Classiﬁcation\n3\n2\nThe Model\nWe ﬁrst introduce the notations in the formulation of unsupervised classiﬁcation.\nLet (X, Y ) be a random couple with joint distribution PXY , where X ∈X ⊂IRd\nis a vector of d features and Y ∈{1, 2, ..., Q} is a label indicating the class to\nwhich X belongs. We assume that X is bounded by [−M0, M0]d. The sample\n(X1, Y1), . . . , (Xn, Yn) are independent copies of (X, Y ), and we only observe\n{Xl}n\nl=1. Suppose PX is the induced marginal distribution of X.\nThe Training Scheme for Unsupervised Classiﬁcation\nThe training scheme introduced by the unsupervised SVM [7,8] forms the basis\nfor learning a classiﬁer from unlabeled data in a principled way. With any hy-\npothetical labeling Y = {Yl}n\nl=1, we can build the corresponding training data\nSC ≜{(Ci, i)}Q\ni=1 for a potential classiﬁer, where Ci = {Xl : Yl = i, 1 ≤l ≤n}\nis the data with label i and {Ci}Q\ni=1 is a partition of the data. In this way, the\nquality of a labeling {Yl}n\nl=1, or equivalently a data partition, can be evaluated\nby the misclassiﬁcation error of the classiﬁer learned from the corresponding\ntraining data SC1. Existing methods [7,8] perform clustering by searching for\nthe data partition with minimum associated misclassiﬁcation error. We adopt\nthis training scheme for unsupervised classiﬁcation, and analyze the misclassiﬁ-\ncation error of the classiﬁer learned from any ﬁxed data partition (corresponding\nto a hypothetical labeling {Yi}n\ni=1 ).\nIt is worthwhile to mention that previous unsupervised classiﬁcation methods\n[12,10] circumvent the above combinatorial unsupervised training scheme by\nlearning a probabilistic classiﬁer from the whole data, so they can not evaluate\nthe classiﬁcation performance of the learned classiﬁer in the learning procedure.\nRather than learning the unsupervised SVM [7,8], we study the misclassiﬁcation\nerror of the unsupervised NN and plug-in classiﬁers, revealing the theoretical\nproperty of these popular classiﬁers in the setting of unsupervised classiﬁcation.\nThe Misclassiﬁcation Error\nBy the training scheme for unsupervised classiﬁcation, the misclassiﬁcation error\n(or the generalization error) of the classiﬁer FSC learned from the training data\nSC is:\nR (FSC) ≜P (FSC ̸= Y )\n(1)\nIn order to evaluate the quality of the data partition SC, we should estimate\nR(FSC) with any ﬁxed SC rather than the average generalization error IESC [R (FSC)].\nFSC indicates either NN or plug-in classiﬁer from SC, and FSC (X) is the clas-\nsiﬁcation function which returns the class label of a sample X. We also let\n1 Two labelings are equivalent if they produce the same data partition {Ci}Q\ni=1, and\nwe refer to SC as the data partition in the following text.\n4\nYingzhen Yang and Thomas S. Huang\nf be the probabilistic density function of PX, η(i) (x) be the regression func-\ntion of Y on X = x, i.e. η(i) (x) = P [Y = i |X = x], and\n\u0000f (i), π(i)\u0001\nbe the\nclass-conditional density function and the prior for class i (f = P\ni\nπ(i)f (i)). Let\nf, {f (i)}Q\ni=1, {η(i)}Q\ni=1 be measurable functions, and there are further assump-\ntions on f:\n(A1) f is bounded, i.e. 0 < fmin ≤f ≤fmax.\n(A2) f is H¨older-γ smooth: |f (x) −f (y)| ≤c∥x −y∥γ where c is the H¨older\nconstant and γ > 0.\nBecause we will estimate the underlying probabilistic density function fre-\nquently in the following text, we introduce the non-parametric kernel density\nestimator of f as below:\nˆfn,h (x) = 1\nn\nn\nX\nl=1\nKh (x −Xl)\n(2)\nwhere\nKh (x) = K\n\u0010x\nh\n\u0011\n, K (x) ≜\n1\n(2π)d/2 e−∥x∥2\n2\n(3)\nand Kh (·) is the isotropic Gaussian kernel with bandwidth h. We introuduce\none assumption on the kernel bandwidth sequence {hn}∞\nn=1:\n(B) (1) hn ց 0, (2) −log hn\nnhd+2γ\nn\n→0, (4) −log hn\nlog log n →∞, (5)hd\nn < ahd\n2n for some\na > 0.\n[18] proves that the kernel density estimator (2) almost sure uniformly con-\nverges to the underlying density:\nTheorem 1 (Theorem 2.3 in Gine et al. [18], in a slighted change form) Un-\nder the assumption (A1)-(A2), suppose the kernel bandwidth sequence {hn}∞\nn=1\nsatisﬁes assumption (B), then with probability 1\nlim\nn→∞h−γ\nn\n\r\r\r ˆfn,hn (x) −f (x)\n\r\r\r\n∞= CK\n(4)\nwhere CK =\nR\nX ∥x∥γ K (x) dx\nSimilarly, the kernel density estimator of the class-conditional density func-\ntion f (i) is\nˆf (i)\nn,h (x) =\n1\nnπ(i)\nn\nX\nl=1\nKh (x −Xl) 1I{Yl=i}\n(5)\n1I is an indicator function. By the similar argument for the kernel density estima-\ntor (2), under the assumption (B), (C1)-(C2), we have the almost sure uniform\nconvergence for ˆf (i)\nn , i.e. lim\nn→∞h−γ\nn\n\r\r\r ˆf (i)\nn,hn (x) −f (i) (x)\n\r\r\r\n∞= CK with probability\n1 for 1 ≤i ≤Q.\n(C1) {f (i)}Q\ni=1 are bounded, i.e. 0 < f (i)\nmin ≤f (i) ≤f (i)\nmax, 1 ≤i ≤Q.\n(C2) f (i) is H¨older-γ smooth:\n\f\ff (i) (x) −f (i) (y)\n\f\f ≤ci∥x −y∥γ where ci is\nthe H¨older constant, 1 ≤i ≤Q, and γ > 0.\nNote that the assumption (C2) indicates (A2).\nNonparametric Unsupervised Classiﬁcation\n5\n3\nMain Results\nWe prove the error bound for the unsupervised NN and plug-in classiﬁers, and\nthen show the connection to other related methods in this section.\n3.1\nUnsupervised Classiﬁcation By Nearest Neighbor\nSince the NN rule makes hard decision for a given datum, we introduce the\nfollowing soft NN cost function which converges to the NN classiﬁcation function,\nsimilar to the one adopted by Neighbourhood Components Analysis [19]:\nDeﬁnition 1 The soft NN cost function is deﬁned as\nˆ\nNNSC,h∗(x, i) =\nN\nP\nl=1\nKh∗(x −Xl) 1I{Yl=i}\nN\nP\nl=1\nKh∗(x −Xl)\n(6)\nwhere\nˆ\nNNSC,h∗(x, i) represents the probability that the datum x is assigned to\nclass i by the soft NN rule\nˆ\nNNSC,h∗learned from S.\nThen we have the misclassiﬁcation error of the soft NN:\nLemma 1 The misclassiﬁcation error of the soft NN is given by\nR( ˆ\nNN SC,h∗) =\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\nη(i) (X)\nˆ\nNNSC,h∗(X, j)\ni\n(7)\nLemma 1 can be proved by the deﬁnition of misclassiﬁcation error. Lemma 2\nshows that, with a large probability, the error of the soft NN (7) is bounded. To\nfacilitate our analysis, we introduce the cover of X as below:\nDeﬁnition 2 The τ-cover of the set X is a sequence of sets {P1, P2, ..., PL} such\nthat X ⊆\nLS\nr=1\nPr and each Pr is a box of length τ in IRd, 1 ≤r ≤L.\nLemma 2 Under the assumption (A1), (C1)-(C2), suppose the kernel band-\nwidth sequence {hn}∞\nn=1 satisﬁes assumption (B), then with probability greater\nthan 1 −2Le−Mh∗the misclassiﬁcation error of the soft NN, i.e. R( ˆ\nNN SC,h∗),\nsatisﬁes:\n1\nn\nX\ni̸=j\nn\nX\nl=1\n1I{Yl=j}\nZ\nX\nπ(i) ˆf (i)\nn,hn (x) Kh∗(x −Xl)\nˆIEZ [Kh∗(x −Z)] + O (hγ\nn) + eε\ndx + O (hγ\nn) ≤R( ˆ\nNN SC,h∗)\n≤1\nn\nX\ni̸=j\nn\nX\nl=1\n1I{Yl=j}\nZ\nX\nπ(i) ˆf (i)\nn,hn (x) Kh∗(x −Xl)\nˆIEZ [Kh∗(x −Z)] + O (hγ\nn) −eε\ndx + O (hγ\nn)\n(8)\n6\nYingzhen Yang and Thomas S. Huang\nwhere ˆIE [Z] =\nR\nX z ˆfn,hn (z)dx, L is the size of the τ-cover of X, Mh∗=\n2n(2π)dh∗2dε2, eε = T1(h∗)\n√\ndτ +c\n\u0010√\ndτ\n\u0011γ\n+ε, T1(h∗) =\n1\ne1/2(2π)d/2h∗d+1 , h∗, τ, ε\nare small enough such that eε < fmin.\nProof. Let {P1, P2, ..., PL} be the τ-cover of the set X. Suppose L points {bxr}L\nr=1\nare chosen from X and bxr ∈Pr. For each 1 ≤r ≤L, according to the Hoeﬀding’s\ninequality\nPr [|T (bxr) −IEZ [Kh∗(bxr −Z)]| > ε] < 2e−Mh∗\n(9)\nwhere T (x) ≜\nn\nP\nl=1\nKh∗(x−Xl)\nn\n. By the union bound, the probability that the above\nevent happens for {bxr}L\nr=1 is less than 2Le−Mh∗. It follows that with probability\ngreater than 1 −2Le−Mh∗,\n|T (bxr) −IEZ [Kh∗(bxr −Z)]| ≤ε\n(10)\nholds for any 1 ≤r ≤L. For any x ∈X, x ∈Pr for some Pr, so that\n|T (x) −T (ˆxr)| ≤T1(h∗) ∥x −ˆxr∥≤T1(h∗)\n√\ndτ\n(11)\nwhere T1(h∗) =\n1\ne1/2(2π)d/2h∗d+1 . Moreover,\n|IEZ [Kh∗(bxr −Z)] −IEZ [Kh∗(x −Z)]| ≤c\n\u0010√\ndτ\n\u0011γ\n(12)\nCombining (10), (11) and (12),\n|T (x) −IEZ [Kh∗(x −Z)]| ≤T1(h∗)\n√\ndτ + c\n\u0010√\ndτ\n\u0011γ\n+ ε\n(13)\nBy Theorem 1,\n\r\r\r ˆfn,hn (x) −f (x)\n\r\r\r\n∞= O (hγ\nn). Similarly,\n\r\r\r ˆf (i)\nn,hn (x) −f (i) (x)\n\r\r\r\n∞=\nO (hγ\nn). Substituting ˆfn,hn and ˆf (i)\nn,hn for f and f (i), and applying (13), (8) is ver-\niﬁed.\n⊓⊔\nDenote the classiﬁcation function of the NN by NN SC, it can be veriﬁed that\nlim\nh∗→0\nˆ\nNNSC,h∗(X, i) = NNSC (X). In order to approach the misclassiﬁcation\nerror of the NN, we construcut a sequence {h∗\nn} →0. Letting n →∞, we\nfurther have the asymptotic misclassiﬁcation errof of the soft NN:\nTheorem 2 Let {h∗\nn}∞\nn=1 be a sequence such that lim\nn→∞h∗\nn = 0 and h∗\nn ≥n−d0\nwith d0 <\n1\n2d. Under the assumption (A1), (C1)-(C2), when n →∞, then with\nprobability 1,\nlim\nn→∞{R\n\u0010\nˆ\nNNSC,h∗n\n\u0011\n−1\nn2\nX\nl<m\nθlmHlm} = 0\n(14)\nNonparametric Unsupervised Classiﬁcation\n7\nHlm = Khn (xl −xm)\n \n1\nˆfn,hn (Xl)\n+\n1\nˆfn,hn (Xm)\n!\n(15)\nwhere ˆfn,hn is the kernel density estimator deﬁned by (2) with kernel bandwidth se-\nquence {hn} under the assumption (B), θlm = 1I{Yl̸=Ym} is a class indicator function\nsuch that θlm = 1 if Xl and Xm belongs to diﬀerent classes, and 0 otherwise.\nProof. In Lemma 2, let τ = τn = τ0n−d0(d+1), ε = εn = n−ε0 for τ0 > 0 and\nε0 < 1 −2dd0. Since h∗\nn ≥n−d0, eε ≤λ0τ0 + cdγ/2τ γ\nn + εn with λ0 =\n√\nd\ne1/2(2π)d/2 .\nτ0 is small enough such that λ0τ0 + cdγ/2τ γ\nn + εn < fmin for suﬃciently large N.\nLet n →∞in the RHS of (8), note that hn →0 and L =\n\u0010\n2M0\nτn\n\u0011d\n, then with\nprobability 1,\nlim\nn→∞{R\n\u0010\nˆ\nNN SC,h∗\nn\n\u0011\n−1\nn\nX\ni̸=j\nn\nX\nl=1\n1I{Yl=j}\nπ(i) ˆf (i)\nn,hn (Xl) Kh∗(x −Xl)\nˆfn,hn (Xl) −λ0τ0\ndx} ≤0\n(16)\nSubstitute ˆf (i)\nn,hn into (16),\nlim\nn→∞{R\n\u0010\nˆ\nNN SC,h∗n\n\u0011\n−1\nn2\nX\nl<m\nθlm\n \nKhn (Xl −Xm)\nˆfn,hn (Xl) −λ0τ0\n+\nKhn (Xl −Xm)\nˆfn,hn (Xm) −λ0τ0\n!\n} ≤0\n(17)\nSince (17) holds for arbitrarily small τ0 > 0, lim\nn→∞{ ˆRNN SC ,h∗n−1\nn2\nP\nl<m\nθlmHlm} ≤\n0 with probability 1. Similarly lim\nn→∞\n{ ˆRNNSC ,h∗\nn −1\nn2\nP\nl<m\nθlmHlm} ≥0 with prob-\nability 1, and (14) is veriﬁed.\n⊓⊔\nBy Theorem 2, the misclassiﬁcation error\n1\nn2\nP\nl<m\nθlmHlm involves only pair-\nwise terms, and Hlm can be interpreted as the similarity kernel over xl and xm\ninduced by the misclassiﬁcation error of the unsupervised NN.\n3.2\nUnsupervised Classiﬁcation By Plug-in Classiﬁer\nNext we will derive the misclassiﬁcation error of the unsupervised plug-in clas-\nsiﬁer, that is, the classiﬁer with the form\nF P I\nn\n(X) = arg max\n1≤i≤Q\nˆη(i) (X)\n(18)\nwhere ˆη(i)\nn\nis a nonparametric estimator of the regression function η(i), and we\nchoose\n8\nYingzhen Yang and Thomas S. Huang\nˆη(i)\nn (x) =\nnP\nl=1\nKhn (x −Xl)1I{Yl=i}\nn ˆfn (x)\n(19)\nLet F ∗be the Bayesian classiﬁer which is a minimizer of the misclassiﬁcation\nerror of all classiﬁers, and F ∗(X) = arg max\n1≤i≤Q\nη(i) (X). Due to the almost sure\nuniform convergence of the kernel density estimator ˆf (i)\nn,hn and ˆfn,hn under the\nassumption (A1), (B), (C1)-(C2), ˆη(i)\nn\nconverges almost sure uniformly to η(i),\nand F P I\nn\nconverges to the Bayesian classiﬁer F ∗: lim\nn→∞F P I\nn\n= F ∗. It follows that\nlim\nn→∞R\n\u0000F P I\nn\n\u0001\n= R (F ∗) by the dominant convergence theorem. It is also known\nthat the excess risk of F P I\nn , namely IER\n\u0000F P I\nn\n\u0001\n−R (F ∗), converges to 0 of the\norder n\n−β\n2β+d under some complexity assumption on the class of the regression\nfunctions Σ with smooth parameter β that {η(i)} belongs to [20,15]. Again,\nthis result deals with the average generalization error and cannot be applied to\nderiving the data dependent misclassiﬁcation error of unsupervised classiﬁcation\nwith ﬁxed training data in our setting.\nSimilar to Lemma 1, it can be veriﬁed that\nR\n\u0000F P I\nn\n\u0001\n=\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\nη(i) (X) P\n\u0002\nF P I\nn\n(X) = j\n\u0003i\n(20)\nWe then give the upper bound for the misclassiﬁcation error of F P I\nn\nin Lemma 3.\nLemma 3 Under the assumption (A1), (B), (C1)-(C2), the asymptotic mis-\nclassiﬁcation error of the plug-in classiﬁer F P I\nn\nsatisﬁes\nR\n\u0000F P I\nn\n\u0001\n≤RP I\nn\n+ O (hγ\nn)\n(21)\nRP I\nn\n≜2\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\nˆη(i)\nn (X) ˆη(j)\nn (X)\ni\n(22)\nwhere\n\b\nη(i)\tQ\ni=1 is the regression functions, and this bound is tight.\nProof. By Theorem 1, for 1 ≤i ≤Q\n\r\r\rˆη(i)\nn −η(i)\r\r\r\n∞= O (hγ\nn)\n(23)\nAccording to (20), R\n\u0000F P I\nn\n\u0001\n=\nP\ni,j=1,...,Q,i̸=j\nIEX\nh\nˆη(i)\nn (X) P\n\u0002\nF P I\nn\n(X) = j\n\u0003i\n+\nO (hγ\nn).\nNonparametric Unsupervised Classiﬁcation\n9\nSuppose the decision regions of F P I\nn\nis {R1, R2, . . . RQ}, then on each Ri,\nˆη(i)\nn ≥ˆη(i\n′ )\nn\nfor any i\n′ ̸= i, and\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\nˆη(i)\nn (X) P\n\u0002\nF P I\nn\n(X) = j\n\u0003i\n(24)\n=\nX\ni,j=1,...,Q,i̸=j\nIEX∈Rj\n\"\nˆη(i)\nn (X) ·\nQ\nX\nk=1\nˆη(k)\nn\n(X)\n#\n= IEX\n\n\n Q\nX\nk=1\nˆη(k)\nn\n(X)\n!2\n−\nQ\nX\ni=1\nIEX∈Ri\n\"\nˆη(i)\nn (X) ·\nQ\nX\nk=1\nˆη(k)\nn\n(X)\n#\n≤IEX\n\n\n Q\nX\nk=1\nˆη(k)\nn\n(X)\n!2\n−\nQ\nX\ni=1\nIEX\n\u0014\u0010\nˆη(i)\nn (X)\n\u00112\u0015\n= 2\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\nˆη(i)\nn (X) ˆη(j)\nn (X)\ni\nSo that (21) is veriﬁed. Since the equality in (24) holds when ˆη(i)\nn\n≡\n1\nQ for\n1 ≤i ≤Q, the upper bound in (21) is tight.\n⊓⊔\nBased on Lemma 3, we can bound the error of the plug-in classiﬁer from\nabove by RP I\nn . In order to estimate the error bound RP I\nn , we introduce the\nfollowing generalized kernel density estimator:\nLemma 4 Suppose f is a probabilistic density function on X ⊂[−M0, M0]d that\nsatisﬁes assumption (A1)-(A2). {Xl}N\nl=1 are drawn i.i.d. according to f. Let g\nbe a H¨older-γ smooth continuous function deﬁned on X with H¨older constant g0,\nand g is bounded, i.e. 0 < gmin ≤g ≤gmax. Let e = f\ng . Deﬁne the generalized\nkernel density estimator of e as\nˆen,h ≜1\nn\nn\nX\nl=1\nKh (x −Xl)\ng (Xl)\n(25)\nWhen the kernel bandwidth sequence {hn}∞\nn=1 satisﬁes assumption (B), then the\nestimator ˆen,hn converges to e almost sure uniformly, i.e. with probability 1,\nlim\nn→∞h−γ\nn\n∥ˆen,hn (x) −e (x)∥∞= CK,f,g\n(26)\nwhere CK,f,g = fmax+gmax\ng2\nmin\nR\nX ∥x∥γ K (x)dx.\nProof. Deﬁne the class of functions on the measurable space (H, H):\nF ≜{K\n\u0012t −·\nh\n\u0013\n, t ∈IRd, h ̸= 0}\nFg ≜{K\n\u0000 t−·\nh\n\u0001\ng (·)\n, t ∈IRd, h ̸= 0}\n10\nYingzhen Yang and Thomas S. Huang\nIt is shown in [18,21] that F is a bounded VC class of measurable functions with\nrespect to the envelope F such that |u| ≤F for any u ∈F. Therefore, there\nexist positive numbers A and v such that for every probability measure P on\n(H, H) for which\nR\nF 2dP < ∞and any 0 < τ < 1,\nN\n\u0010\nF, ∥·∥L2(P ) , τ ∥F∥L2(P )\n\u0011\n≤\n\u0012A\nτ\n\u0013v\n(27)\nwhere N\n\u0010\nT , ˆd, ǫ\n\u0011\nis deﬁned as the minimal number of open ˆd-balls of radius ǫ\nand centers in T required to cover T . For any t1 and t2,\n\r\r\r\r\nK(\nt1−·\nh )\ng(·)\n−\nK(\nt2−·\nh )\ng(·)\n\r\r\r\r\nL2(P )\n≤\n1\ngmin\n\r\rK\n\u0000 t1−·\nh\n\u0001\n−K\n\u0000 t2−·\nh\n\u0001\r\r\nL2(P ). Let BF (t0, δ) ≜{t :\n\r\rK\n\u0000 t−·\nh\n\u0001\n−K\n\u0000 t0−·\nh\n\u0001\r\r\nL2(P ) ≤\nδ}, and BFg (t0, δ) ≜{t :\n\r\r\r\r\nK( t−·\nh )\ng(·)\n−\nK(\nt0−·\nh )\ng(·)\n\r\r\r\r\nL2(P )\n≤δ}. Then BF (t0, δ) ⊆\nBFg\n\u0010\nt0,\nδ\ngmin\n\u0011\n.\nWe choose the envelope function for Fg as Fg =\nF\ngmin and |ug| ≤Fg for any\nug ∈Fg, then\nN\n\u0010\nFg, ∥·∥L2(P ) , τ ∥Fg∥L2(P )\n\u0011\n≤\n\u0012A\nτ\n\u0013v\n(28)\nSo that Fg is also a bounded VC class. By similar arguement in the proof of\nTheorem 1, we can obtain (26).\n⊓⊔\nTheorem 3 Under the assumption (A1), (B), (C1)-(C2), the error of the plug-\nin classiﬁer F P I\nn\nsatisﬁes\nR\n\u0000F P I\nn\n\u0001\n≤2\nn2\nX\nl,m\nθlmGlm,hn + O (hγ\nn)\n(29)\nwhere θlm = 1I{Yl̸=Ym} is a class indicator function and\nGlm,hn = Ghn (Xl, Xm) , Gh (x, y) =\nKh (x −y)\nˆf\n1\n2\nn,h (x) ˆf\n1\n2\nn,h (y)\n(30)\nfor any kernel bandwidth sequence {hn}∞\nn=1 that satisﬁes assumption (B).\nProof. For any 1 ≤i, j ≤Q, by (23)\nIEX\nh\nˆη(i)\nn (X) ˆη(j)\nn (X)\ni\n≤IEX\nh\nη(i) (X) η(j) (X)\ni\n+ O (hγ\nn)\n(31)\nSince\nIEX\nh\nη(i) (X) η(j) (X)\ni\n=\nZ\nX\nπ(i)f (i) (x)\nf\n1\n2 (x)\n· π(j)f (j) (x)\nf\n1\n2 (x)\ndx,\nNonparametric Unsupervised Classiﬁcation\n11\nf, f (i), and f (j) are H¨older-γ smooth, and f\n1\n2 is also H¨older-γ smooth. For\nany sequence {˜hn}∞\nn=1 that satisﬁes assumption (B), we obtain the kernel esti-\nmator ˜η(i)\nn\nof π(i)f (i)(x)\nf\n1\n2 (x)\nusing the generalized kernel density estimator (25):\n˜η(i)\nn (x) = 1\nn\nn\nX\nl=1\nK˜hn (x −Xl) 1I{Yl=i}\nf\n1\n2 (Xl)\n(32)\nBy Lemma 4,\n\r\r\r\r˜η(i) −π(i)f (i)(x)\nf\n1\n2 (x)\n\r\r\r\r\n∞\n= O (hγ\nn) for 1 ≤i ≤Q.2 It follows that\nIEX\nh\nη(i) (X) η(j) (X)\ni\n≤IEX\nh\n˜η(i)\nn (X) ˜η(j)\nn (X)\ni\n+ O (hγ\nn)\n(33)\nAlso,\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\n˜η(i)\nn (X) ˜η(j)\nn (X)\ni\n= 1\nn2\nX\nl,m\nR\nX K˜hn (x −Xl) K˜hn (x −Xm)\nf\n1\n2 (Xl) f\n1\n2 (Xm)\nX\ni,j=1,...,Q,i̸=j\n1I{Yl=i}1I{Ym=j}\n= 1\nn2\nX\nl,m\nK√\n2˜hn (Xl −Xm)\nf\n1\n2 (Xl) f\n1\n2 (Xm)\nθlm\nand we obtain last equality by convolution of two Gaussian kernels. Let hn =\n√\n2˜hn, then hn satisﬁes assumption (B). The kernel density estimator ˆfn,hn sat-\nisﬁes\n\r\r\r ˆfn,hn −f\n\r\r\r\n∞= O (hγ\nn) by Theorem 1. Therefore, with n large enough,\n\f\f\f\f\f\f\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\n˜η(i)\nn (X) ˜η(j)\nn (X)\ni\n−1\nn2\nX\nl,m\nGlm,hnθlm\n\f\f\f\f\f\f\n(34)\n= 1\nn2\n\f\f\f\f\f\f\nX\nl,m\nKhn (Xl −Xm)\nf\n1\n2 (Xl) f\n1\n2 (Xm)\nθlm −\nX\nl,m\nKhn (Xl −Xm)\nˆf\n1\n2\nn,hn (Xl) ˆf\n1\n2\nn,hn (Xm)\nθlm\n\f\f\f\f\f\f\n≤1\nn2\nX\nl,m\nKhn (Xl −Xm)\n\f\f\ff\n1\n2 (Xl) f\n1\n2 (Xm) −ˆf\n1\n2\nn,hn (Xl) ˆf\n1\n2\nn,hn (Xm)\n\f\f\f\nˆf\n1\n2\nn,hn (Xl) ˆf\n1\n2\nn,hn (Xm) f\n1\n2 (Xl) f\n1\n2 (Xm)\n≤1\nn2\nX\nl,m\nKhn (Xl −Xm) f\n1\n2\nmaxO (hγ\nn)\n2f\n9\n2\nmin\n= f\n1\n2maxO (hγ\nn)\n2f\n9\n2\nmin\n· 1\nn\nn\nX\nl=1\nˆfn,hn (Xl) = O (hγ\nn)\n2 It can be verifed by applying Lemma 4 to {Xl, Yl}n\nl=1.\n12\nYingzhen Yang and Thomas S. Huang\nIt follows from (31), (33) and (34) that\nRP I\nn\n= 2\nX\ni,j=1,...,Q,i̸=j\nIEX\nh\nˆη(i)\nn (X) ˆη(j)\nn (X)\ni\n= 2\nn2\nX\nl,m\nGlm,hnθlm + O (hγ\nn)\nand (29) is veriﬁed.\n⊓⊔\nRemark 1 If we further assume that f α is H¨older-γ smooth for some 0 ≤α ≤1,\nthen Gh (x, y) in (30) can be\nGh (x, y) =\nKh (x −y)\nˆf α\nn,h (x) ˆf 1−α\nn,h (y)\n(35)\nDeﬁne V\n\u0000F P I\nn\n\u0001\nas the volume of the region in X misclassiﬁed by F P I\nn . Using\nalmost the same argument in Theorem 3, we have the tight upper bound for\nV\n\u0000F P I\nn\n\u0001\n:\nTheorem 4 Under the assumption (A1), (B), (C1)-(C2), the volume of the\nregion in X misclassiﬁed by F P I\nn\nsatisﬁes\nV\n\u0000F P I\nn\n\u0001\n≤2\nn2\nX\nl,m\nθlmVlm,hn + O (hγ\nn)\n(36)\nwhere\nVlm,hn = Vhn (Xl, Xm) , Vh (x, y) =\nKh (x −y)\nˆfn,h (x) ˆfn,h (y)\n(37)\nfor any bandwidth sequence {hn}∞\nn=1 that satisﬁes assumption (B).\n3.3\nConnection to Low Density Separation\nLow Density Separation [16], a well-known criteria for clustering, requires that\nthe cluster boundary should pass through regions of low density. Suppose the\ndata {Xi}n\ni=1 lies on a domain Ω⊆Rd. Let f be the probability density function\non Ω, S be the cluster boundary which separates Ωinto two parts S1 and S2.\nFollowing the Low Density Separation assumption, [1] suggests that the cluster\nboundary S with low weighted volume R\nS\nf (s)ds should be preferable. [1] also\nproves that a particular type of cut function converges to the weighted volume\nof S. By slight change of their proof, we obtain the following result relating the\nerror of the plug-in classiﬁer and the weighted volume of the cluster boundary.\nLemma 5 For any kernel bandwidth sequence {hn}∞\nn=1 such that lim\nn→∞hn = 0\nand hn > n−\n1\n4d+4 , with probability 1,\nlim\nn→∞\n1\nn2\n√\n2π\nhn\nX\nl,m\nθlmGlm,hn =\nZ\nS\nf (s)ds\n(38)\nNonparametric Unsupervised Classiﬁcation\n13\nCombining Lemma 5 and Theorem 3, 4, we have\nTheorem 5 Under the assumption (A1) and (C1)-(C2), for any kernel band-\nwidth sequence {hn}∞\nn=1 satisfying assumption (B) and hn > n−\n1\n4d+4 ,\nlim\nn→∞\nR\n\u0000F P I\nn\n\u0001\nhn\nR\nS\np (s)ds ≤\n√\n2π\nπ\n(39)\nAlso, the constant on the RHS of (39) is the best (cannot be smaller).\nProof. The conclusion follows from the fact that lim\nn→∞R\n\u0000F P I\nn\n\u0001\n≤lim\nn→∞\n2\nn2\nP\nl,m\nθlmGlm\nand the bound is tight.\n⊓⊔\nTheorem 5 shows that the error of the plug-in classiﬁer is bounded from\nabove by the weighted volume of the cluster boundary (scaled by hn). It is also\nworth noting that R\n\u0000F P I\nn\n\u0001\n= o\n\u0012R\nS\nf (s)ds\n\u0013\n.\n3.4\nConnection to Diﬀusion Maps\nConsider the complete graph G whose vertices are associated with the data lying\non a submanifold of IRd, and the weight of the edge between data x and y is\ndenoted by eh (x, y). eh (x, y) is determined by the similarity kernel induced\nby the bound for the misclassiﬁcation error or the volume of the misclassi-\nﬁed region of the unsupervised plug-in classiﬁer, namely Gh (x, y) or Vh (x, y)\ndeﬁned in (30) and (37) respectively. Empirically methods such as unsuper-\nvised SVM suggest clustering be performed by minimizing the error of unsu-\npervised classiﬁcation R (FSC). Minimizing the error bound for the plug-in clas-\nsiﬁer (29) (or (36)) is equivalent to ﬁnding the (normalized) minimum-cut in\nthe graph G, which is solved by computing the normalized graph Laplacian.\nLet dh (x) =\nR\nX\neh (x, y) f (y)dy, the normalized graph Laplacian computes the\nanisotropic kernel ph (x, y) = eh(x,y)\ndh(x) . It is shown in [3] that the forward inﬁnites-\nimal operator of the corresponding Markov chain is\nH(α)φ = ∆α −∆\n\u0000f 1−α\u0001\nf 1−α\nφ\n(40)\nwhere ∆is the Laplace-Beltrami operator.\nIf eh (x, y) = Gh (x, y), then α = 1\n2 in (40), the forward inﬁnitesimal operator\nreduces to\nH( 1\n2)φ = ∆φ −∆\n\u0000√f\n\u0001\n√f\nφ\n(41)\nwhich yields the backward Fokker-Planck operator.\n14\nYingzhen Yang and Thomas S. Huang\nMoreover, if eh (x, y) = Vh (x, y), then α = 1 in (40), and the forward in-\nﬁnitesimal operator is\nH(1)φ = ∆φ\n(42)\nwhich is the Laplace-Beltrami operator. In this case, the corresponding Markov\nchain converges to the Brownian motion, and the normalized graph Laplacian\ndisregards the data distribution and only captures the Riemannian geometry of\nthe data. It is consistent with the choice of the similarity kernel V (x, y) which\nreﬂects only the geometric property (volume of the misclassiﬁed region) of the\ndata.\n4\nConclusion\nEmpirical unsupervised classiﬁcation methods gain satisfactory practical results,\nbut few methods consider the error of unsupervised classiﬁers. We study the mis-\nclassiﬁcation error of unsupervised classiﬁcation by two popular classiﬁers, i.e.\nthe nearest neighbor classiﬁer (NN) and the plug-in classiﬁer, and build the\nconnection between the error of unsupervised plug-in classiﬁer and the weighted\nvolume of cluster boundary. The normalized graph Laplacian from the similarity\nkernel induced by the unsupervised plug-in classiﬁer recovers the Fokker-Planck\noperator and the Laplace-Beltrami operator, revealing close relationship to dif-\nferent types of Diﬀusion maps.\nReferences\n1. Narayanan, H., Belkin, M., Niyogi, P.: On the relation between low density sepa-\nration, spectral clustering and graph cuts. In: NIPS. (2006) 1025–1032\n2. Coifman, R.R., Lafon, S., Lee, A.B., Maggioni, M., Nadler, B., Warner, F., Zucker,\nS.W.: Geometric diﬀusions as a tool for harmonic analysis and structure deﬁnition\nof data: Diﬀusion maps. Proceedings of the National Academy of Sciences of the\nUnited States of America 102(21) (May 2005) 7426–7431\n3. Coifman, R.R., Lafon, S.: Diﬀusion maps. Applied and Computational Harmonic\nAnalysis 21(1) (July 2006) 5–30\n4. Hartigan, J.A., Wong, M.A.: A K-means clustering algorithm. Applied Statistics\n28 (1979) 100–108\n5. Ng, A.Y., Jordan, M.I., Weiss, Y.: On spectral clustering: Analysis and an algo-\nrithm. In: NIPS. (2001) 849–856\n6. Fraley, C., Raftery, A.E.:\nModel-Based Clustering, Discriminant Analysis, and\nDensity Estimation. Journal of the American Statistical Association 97(458) (June\n2002) 611–631\n7. Xu, L., Neufeld, J., Larson, B., Schuurmans, D.: Maximum margin clustering. In:\nNIPS. (2004)\n8. Karnin, Z., Liberty, E., Lovett, S., Schwartz, R., Weinstein, O.:\nUnsupervised\nsvms: On the complexity of the furthest hyperplane problem. Journal of Machine\nLearning Research - Proceedings Track 23 (2012) 2.1–2.17\n9. Agakov, F.V., Barber, D.: Kernelized infomax clustering. In: NIPS. (2005)\nNonparametric Unsupervised Classiﬁcation\n15\n10. Gomes, R., Krause, A., Perona, P.: Discriminative clustering by regularized infor-\nmation maximization. In: NIPS. (2010) 775–783\n11. Bridle, J.S., Heading, A.J.R., MacKay, D.J.C.: Unsupervised classiﬁers, mutual\ninformation and ’phantom targets’. In: NIPS. (1991) 1096–1101\n12. Sugiyama, M., Yamada, M., Kimura, M., Hachiya, H.:\nOn information-\nmaximization clustering: Tuning parameter selection and analytic solution.\nIn:\nICML. (2011) 65–72\n13. Bengio, Y., Paiement, J.F., Vincent, P., Delalleau, O., Roux, N.L., Ouimet, M.:\nOut-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering.\nIn: NIPS. (2003)\n14. Cover, T., Hart, P.: Nearest neighbor pattern classiﬁcation. Information Theory,\nIEEE Transactions on 13(1) (January 1967) 21–27\n15. Audibert, J.Y., Tsybakov, A.: Fast learning rates for plug-in classiﬁers. Annals of\nProbability 35 (2007) 608–633\n16. Chapelle, O., Zien, A.: Semi-Supervised Classiﬁcation by Low Density Separation.\nIn: AISTATS. (2005)\n17. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Trans. Pattern\nAnal. Mach. Intell. 22(8) (2000) 888–905\n18. Gin´e, E., Guillou, A.: Rates of strong uniform consistency for multivariate kernel\ndensity estimators. Ann. Inst. H. Poincar´e Probab. Statist. 38(6) (November 2002)\n907–921\n19. Goldberger, J., Roweis, S.T., Hinton, G.E., Salakhutdinov, R.:\nNeighbourhood\ncomponents analysis. In: NIPS. (2004)\n20. Yang, Y.: Minimax nonparametric classiﬁcation - part i: Rates of convergence.\nIEEE Transactions on Information Theory 45(7) (1999) 2271–2284\n21. Dudley, R.M.:\nUniform Central Limit Theorems.\nCambridge University Press\n(1999)\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2012-10-02",
  "updated": "2013-05-20"
}