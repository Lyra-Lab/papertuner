{
  "id": "http://arxiv.org/abs/1503.06452v1",
  "title": "Unsupervised model compression for multilayer bootstrap networks",
  "authors": [
    "Xiao-Lei Zhang"
  ],
  "abstract": "Recently, multilayer bootstrap network (MBN) has demonstrated promising\nperformance in unsupervised dimensionality reduction. It can learn compact\nrepresentations in standard data sets, i.e. MNIST and RCV1. However, as a\nbootstrap method, the prediction complexity of MBN is high. In this paper, we\npropose an unsupervised model compression framework for this general problem of\nunsupervised bootstrap methods. The framework compresses a large unsupervised\nbootstrap model into a small model by taking the bootstrap model and its\napplication together as a black box and learning a mapping function from the\ninput of the bootstrap model to the output of the application by a supervised\nlearner. To specialize the framework, we propose a new technique, named\ncompressive MBN. It takes MBN as the unsupervised bootstrap model and deep\nneural network (DNN) as the supervised learner. Our initial result on MNIST\nshowed that compressive MBN not only maintains the high prediction accuracy of\nMBN but also is over thousands of times faster than MBN at the prediction\nstage. Our result suggests that the new technique integrates the effectiveness\nof MBN on unsupervised learning and the effectiveness and efficiency of DNN on\nsupervised learning together for the effectiveness and efficiency of\ncompressive MBN on unsupervised learning.",
  "text": "JMLR: Workshop and Conference Proceedings 1–7\nUnsupervised model compression for multilayer bootstrap\nnetworks\nXiao-Lei Zhang\nhuoshan6@126.com\nDepartment of Computer Science and Engineering, The Ohio State University, Columbus, OH 43210,\nUSA.\nAbstract\nRecently, multilayer bootstrap network (MBN) has demonstrated promising performance\nin unsupervised dimensionality reduction. It can learn compact representations in stan-\ndard data sets, i.e. MNIST and RCV1. However, as a bootstrap method, the prediction\ncomplexity of MBN is high. In this paper, we propose an unsupervised model compression\nframework for this general problem of unsupervised bootstrap methods. The framework\ncompresses a large unsupervised bootstrap model into a small model by taking the boot-\nstrap model and its application together as a black box and learning a mapping function\nfrom the input of the bootstrap model to the output of the application by a supervised\nlearner. To specialize the framework, we propose a new technique, named compressive\nMBN. It takes MBN as the unsupervised bootstrap model and deep neural network (DNN)\nas the supervised learner. Our initial result on MNIST showed that compressive MBN\nnot only maintains the high prediction accuracy of MBN but also is over thousands of\ntimes faster than MBN at the prediction stage. Our result suggests that the new tech-\nnique integrates the eﬀectiveness of MBN on unsupervised learning and the eﬀectiveness\nand eﬃciency of DNN on supervised learning together for the eﬀectiveness and eﬃciency\nof compressive MBN on unsupervised learning.\nKeywords: Model compression, multilayer bootstrap networks, unsupervised learning.\n1. Introduction\nDimensionality reduction is a core problem of machine learning, where classiﬁcation and\nclustering can be regarded as its special cases that reduce high dimensional data to discrete\npoints.\nIn this paper, we focus on unsupervised learning.\nTraditionally, dimensionality\nreduction can be categorized to kernel methods, neural networks, probabilistic models, and\nsparse coding. Kernel methods are too costly on large-scale problems. Although neural\nnetworks are scalable to large scale data, they double the computational complexity by a\nbottleneck structure and take the input as the output of the bottleneck structure at the\ntraining stage which is slow, moreover, they learn data distribution globally which is not\nvery eﬀective on learning local structures.\nMultilayer bootstrap network (MBN) is a recently proposed bootstrap method (or un-\nsupervised ensemble method). It has multiple nonlinear layers. Each layer is an ensemble\nof k-centers clusterings. The centers of each k-centers clustering are only randomly sampled\ndata points (called a bootstrap sample) from the input. MBN is easily implemented and\ntrained, and scales well to large-scale problems as neural networks at the training stage.\nMoreover, MBN learns a data distribution locally so that it can learn eﬀective representa-\nc⃝X.-L. Zhang.\narXiv:1503.06452v1  [cs.LG]  22 Mar 2015\nZhang\ntions of data easily. However, MBN contains hundreds of clusterings, which is diﬃcult to\nbe used for prediction.\nMotivated by the aforementioned problem and the recent progress of compressing en-\nsemble classiﬁers to a single small classiﬁer in supervised learning (Bucilu et al. (2006); Hin-\nton et al. (2015)), in this abstract paper, we propose an unsupervised model compression\nframework. The framework uses a supervised model to approximate the mapping function\nfrom the input of an unsupervised bootstrap method to the output of the application of\nthe unsupervised bootstrap method. We further specify the framework by taking MBN\nas the unsupervised bootstrap method and DNN as the supervised model. The proposed\nmethod is named compressive MBN. To our best knowledge, this is the ﬁrst work of model\ncompression for bootstrap methods on unsupervised learning.\n2. Methods\nCompressive MBN is as follows:\n• The ﬁrst step trains MBN on a give training set, and outputs the low dimensional\nrepresentation of the training data points.\n• [A step driven by applications] The second step applies the low dimensional repre-\nsentation to a given application in unsupervised learning, and outputs the prediction\nresult of the training data.\n• The third step trains a DNN with the training set as the input and the prediction\nresult as the target. Finally, the DNN model will be used for prediction.\nThe algorithm is a very basic framework. We can easily extend compressive MBN to\nother techniques by simply using other unsupervised bootstrap techniques to replace MBN\nin the ﬁrst step for potentially better performance. We can also use many other supervised\nlearners to replace DNN in the third step, but to our knowledge, DNN is currently already\na good choice.\nWe may also design a lot of new algorithms by simply specifying the second step for\ndiﬀerent applications. Some examples are as follows. (i) When compressive MBN is used\nfor visualization, we may omit the second step, and simply take the input and output of the\nMBN as the input and output of DNN respectively. (ii) When compressive MBN is used for\nunsupervised prediction, we may run a hard clustering algorithm on the training set, and\nget the predicted indicator vector of each training data point. For example, if a data point\nis assigned to the second cluster, then its predicted indicator vector is [0, 1, 0, 0, . . . , 0]. We\nmay also get the probabilistic output of the clustering.\n3. Experiments\nWe conducted an initial experiment on MNIST. We showed that the technique is very helpful\nfor reducing the high computational cost of MBN on unsupervised prediction problems. The\nMNIST data was normalized by dividing its entries by 255.\n2\nUnsupervised model compression for multilayer bootstrap networks\nFigure 1: MBN. Its prediction time on the 5000 images is 2333.24 seconds.\nFigure 2: Compressive MBN. Its prediction time on the 5000 images is 0.58 seconds.\n3.1. Experiment on visualizing small subsets of MNIST\nIn this subsection, we did not consider the generalization ability of MBN and compressive\nMBN. Instead, we studied their visualization ability. A data set that contained 5000 unla-\nbeled images randomly selected from the training set of MNIST was used for both training\nand test.\nFor the MBN training, we trained MBN similarly as in the second experiment in (Zhang\n(2014)). Speciﬁcally, the number of clusterings in each layer was set to 400. The parameters\nk from layer 1 to layer 9 were set to 4000-2000-1000-500-250-125-65-30-15 respectively. As\nwe can see, MBN is a very large sparse model: (4000-2000-1000-500-250-125-65-30-15)×400.\nThe parameter a for random feature selection was set to 0.5. The parameter r for random\n3\nZhang\nreconstruction was set to 0.5. After getting the high-dimensional sparse representation from\nMBN at the top hidden layer, we mapped it to two dimensional space by the expectation-\nmaximization principle component analysis (EM-PCA) (Roweis (1998)).\nFor the training of compressive MBN, we omitted the second step, and used the\ninput and output of MBN as the input and output of a DNN model respectively. The\nparameter settings are as follows. We trained a 786-2048-2048-2 DNN. The dropout rate\nwas set to 0.2. The rectiﬁed linear unit was used as the hidden unit, and the linear function\nwas used as the output unit. The number of the training epoches was set to 120. The batch\nsize was set to 32. The learning rate was set to 0.001.\nThe two dimensional visualizations produced by MBN and compressive MBN were\nshown in Fig.\n1 and Fig.\n2 respectively.\nFrom the two ﬁgures, we found that the vi-\nsualizations of both MBN and compressive MBN by DNN were equivalently good. When\nwe used the features for clustering, the NMIs of both the methods were around 81%. Amaz-\ningly, the prediction time of the compressive MBN on the 5000 images was only\n0.58 seconds, which accelerated the prediction time of MBN by around 4000\ntimes!1\n3.2. Experiment on the full MNIST\nWe used all 60,000 training images for unsupervised model training and 10,000 test images\nfor test. We discussed the unsupervised generalization ability of the compressive MBN on\nthe test images.\n3.2.1. Compressive MBN without random reconstruction (i.e. parameter\nr = 0)\nFor the MBN training, we trained MBN similarly as in the third experiment in (Zhang\n(2014)). Speciﬁcally, the number of clusterings in each layer was set to 400. The parameters\nk from layer 1 to layer 9 were set to 4000-2000-1000-500-250-125-65-30-15 respectively. As\nwe can see, MBN is a very large sparse model: (4000-2000-1000-500-250-125-65-30-15)×400.\nThe parameter a for random feature selection was set to 0.5. The parameter r for random\nreconstruction was set to 0. After getting the high-dimensional sparse representation from\nMBN at the top hidden layer, we mapped it to 5 dimensional space by EM-PCA (Roweis\n(1998)). We further encoded the 5-dimensional representations to 10-dimensional indicator\nvectors by k-means clustering, which was a specialization of the second step of compressive\nMBN.\nFor the training of compressive MBN, we took the raw feature of the training set as\nthe input of DNN, and took the 10-dimensional predicted indicator vectors as the training\ntarget of DNN. The parameter settings of the DNN were as follows. We trained a 786-\n2048-2048-10 DNN. The dropout rate was set to 0.2. The rectiﬁed linear unit was used as\nthe hidden unit, and the sigmoid function was used as the output unit. The number of the\ntraining epoches was set to 50. The batch size was set to 128. The learning rate was set to\n0.001.\nBecause k-means clustering suﬀers from local minima, we ran the aforementioned meth-\nods 10 times and recorded the average results. The experimental comparison between MBN\n1. MBN did not enable parallel computing.\n4\nUnsupervised model compression for multilayer bootstrap networks\n0\n2\n4\n6\n8\n10\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nNormalized mutual information\nNumber of layers\nMBN & Compressive MBN without random reconstruction\n \n \nTraining accuracy by MBN\nTraining accuracy by compressive MBN\nTest accuracy by MBN\nTest accuracy by compressive MBN\nFigure 3: Comparison of the generalization ability of MBN and compressive MBN on clus-\ntering when the random reconstruction of MBN is not used (i.e. r = 0). The\nclustering accuracy is evaluated by normalized mutual information. The predic-\ntion time of MBN on the 10,000 test images is 4699.69 seconds. The prediction\ntime of compressive MBN on the 10,000 test images is 1.15 seconds.\n0\n2\n4\n6\n8\n10\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nNormalized mutual information\nNumber of layers\nMBN & Compressive MBN with random reconstruction\n \n \nTraining accuracy by MBN\nTraining accuracy by compressive MBN\nTest accuracy by MBN\nTest accuracy by compressive MBN\nFigure 4: Comparison of the generalization ability of MBN and compressive MBN on clus-\ntering when the random reconstruction of MBN is not used (i.e. r = 0.5). The\nprediction time of MBN on the 10,000 test images is 4857.45 seconds. The pre-\ndiction time of compressive MBN on the 10,000 test images is 1.10 seconds.\nand compressive MBN was summarized in Fig. 3. From the ﬁgure, we observed that the\ncurves of the training accuracy of MBN and compressive MBN were completely coincident;\n5\nZhang\nmoreover, the curve of prediction of compressive MBN was even slightly better than that\nof MBN; the highest prediction accuracy of compressive MBN reached 84% in terms of\nNMI. The most advanced property of compressive MBN is that it needed only 1.15 seconds\nto predict 10,000 images, while MBN needed 4699.69 seconds to predict 10,000 images. The\nprediction time was accelerated by around 4000 times.\n3.2.2. Compressive MBN with random reconstruction (i.e. parameter r = 0.5)\nIt is shown in (Zhang (2014)) that when the data is small scale (i.e.\nthe training size\nwas similar to the largest parameter k), the random reconstruction operation can be quite\nhelpful, however, it is still unclear that whether random reconstruction will be helpful when\nthe data is large scale (i.e. the training size is much larger than the largest parameter\nk), since the largest k in the third experiment of (Zhang (2014)) was only 1000 and the\nexperimental results of MBN with or without random reconstruction was not very exciting.\nIn this subsection, we enlarged k to 4000 as in Section 3.2.1.\nThe experimental settings of both MBN and compressive MBN were the same as in\nSection 3.2.1 except that we set r = 0.5 and mapped the sparse features to 2 dimensional\nspace by EM-PCA. The experimental results were summarized in Fig. 4. From the ﬁgure,\nwe observed that all experimental conclusions in Section 3.2.1 could also be summarized\nhere, except that when random reconstruction was used, the performance of both MBN and\ncompressive MBN was not as good as that without random reconstruction.\n4. Conclusions\nIn this paper, we proposed a general framework for unsupervised model compression. The\nframework takes MBN (Zhang (2014)) as a case of study. The specialized technique, named\ncompressive MBN, uses DNN as an auxiliary model for modeling the mapping function\nfrom the input of MBN to the prediction result of a given application that takes the low\ndimension output of MBN as its input.\nThe new technique aims to solve the problem\nthat although MBN is simple, eﬀective, robust, and eﬃcient-at-the-training-stage, it is time\nconsuming on prediction.\nOur initial experimental result on MNIST showed that compressive MBN not only\ninherited the generalization ability of MBN (and is even slightly better than MBN), but\nalso accelerated the prediction eﬃciency of MBN by over thousands of times.\nCompressive MBN concatenates the eﬀectiveness of MBN on unsupervised learning and\nthe eﬀectiveness and eﬃciency of DNN on supervised learning together for both its eﬀective-\nness and its eﬃciency on unsupervised learning. Moreover, we can easily extend compressive\nMBN to other unsupervised model compression techniques.\n5. Acknowledgements\nThe author thanks Dr Yuxuan Wang for providing the well-designed DNN toolbox and Prof\nDeLiang Wang for providing computing resources of the Ohio Supercomputing Center.\n6\nUnsupervised model compression for multilayer bootstrap networks\nReferences\nCristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In\nProc. 12th Int. Conf. Knowl. Disc., Data Min., pages 535–541. ACM, 2006.\nGeoﬀrey Hinton, Oriol Vinyals, and JeﬀDean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015.\nSam T Roweis. EM algorithms for PCA and SPCA. In Advances in Neural Information\nProcessing Systems 10, pages 626–632, Denver, CO, 1998.\nAlexander Strehl and Joydeep Ghosh. Cluster ensembles—a knowledge reuse framework for\ncombining multiple partitions. J. Mach. Learn. Res., 3:583–617, 2003.\nXiao-Lei Zhang. Nonlinear dimensionality reduction of data by multilayer bootstrap net-\nworks. arXiv preprint arXiv:1408.0848, pages 1–18, 2014.\n7\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2015-03-22",
  "updated": "2015-03-22"
}