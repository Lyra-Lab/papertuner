{
  "id": "http://arxiv.org/abs/1509.06095v1",
  "title": "Multilayer bootstrap network for unsupervised speaker recognition",
  "authors": [
    "Xiao-Lei Zhang"
  ],
  "abstract": "We apply multilayer bootstrap network (MBN), a recent proposed unsupervised\nlearning method, to unsupervised speaker recognition. The proposed method first\nextracts supervectors from an unsupervised universal background model, then\nreduces the dimension of the high-dimensional supervectors by multilayer\nbootstrap network, and finally conducts unsupervised speaker recognition by\nclustering the low-dimensional data. The comparison results with 2 unsupervised\nand 1 supervised speaker recognition techniques demonstrate the effectiveness\nand robustness of the proposed method.",
  "text": "MULTILAYER BOOTSTRAP NETWORK FOR UNSUPERVISED SPEAKER RECOGNITION\nXiao-Lei Zhang\nDepartment of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA\nxiaolei.zhang9@gmail.com\nABSTRACT\nWe apply multilayer bootstrap network (MBN), a recent pro-\nposed unsupervised learning method, to unsupervised speaker\nrecognition. The proposed method ﬁrst extracts supervectors\nfrom an unsupervised universal background model, then re-\nduces the dimension of the high-dimensional supervectors by\nmultilayer bootstrap network, and ﬁnally conducts unsuper-\nvised speaker recognition by clustering the low-dimensional\ndata. The comparison results with 2 unsupervised and 1 su-\npervised speaker recognition techniques demonstrate the ef-\nfectiveness and robustness of the proposed method.\nIndex Terms— multilayer bootstrap network, speaker\nrecognition, unsupervised learning.\n1. INTRODUCTION\nSpeaker recognition aims to identify speakers from their\nvoices.\nIt is important in many speech systems, such as\nspeaker diarization, language recognition, and speech recog-\nnition. Supervised methods include maximum a posteriori\nestimation [1, 2], linear discriminative analysis (LDA) [3, 4],\nsupport vector machines [2], deep neural networks [5,6], etc.\nBecause constructing a manually-labeled corpus is labor-\ning intensive and time-consuming, it is strongly needed to\ndevelop unsupervised speaker recognition methods. Existing\nmethods mainly include principle component analysis (PCA),\nk-means clustering, Gaussian mixture model (GMM), ag-\nglomerative hierarchical clustering, and joint factor analysis.\nFor example, Wooters and Huijbregts [7] used agglomerative\nclustering to merge speaker segments by Bayesian informa-\ntion criterion.\nIso [8] used vector quantization to encode\nspeech segments and used spectral clustering, which is a k-\nmeans clustering applied to a low-dimensional subspace of\ndata, for speaker recognition. Nwe et al. [9] used a group of\nGMM clusterings to improve individual base GMM cluster-\nings. Some methods apply clustering techniques, e.g. varia-\ntional Bayesian expectation-maximization (EM) GMM [10]\nand spectral clustering [11], to a low-dimensional total vari-\nability subspace [4] that is learned from high-dimensional\nsupervectors by joint factor analysis [4]. Some methods com-\npensate the total variability space with new items, e.g. [12].\nBecause little prior knowledge of data is known before-\nhand, an unsupervised method should satisfy the following\nconditions: (i) no need for manually-labeled training data; (ii)\nno hyperparameter tunning for a satisﬁed performance; and\n(iii) robustness to different data or modeling conditions. Due\nto these strict requirements, unsupervised speaker recognition\nis a very difﬁcult task. In this paper, we present a multilayer\nbootstrap network (MBN) [13] based algorithm. MBN is a re-\ncent proposed unsupervised nonlinear dimensionality reduc-\ntion algorithm. Experimental results show that the proposed\nmethod satisﬁes these requirements.\nThis paper is organized as follows.\nIn Section 2, we\npresent the MBN-based system.\nIn Section 3, we present\nthe MBN algorithm and its typical hyperparameter setting.\nIn Section 4, we present the relationship between MBN and\ndeep learning. In Section 5, we report comparison results. In\nSection 6, we conclude this paper.\n2. SYSTEM\nGiven an unlabeled speaker recognition corpus, we propose\nthe following unsupervised algorithm:1\n• The ﬁrst step trains a speaker- and session-independent\nunsupervised universal background model (UBM)\n[1] from an acoustic feature, which produces a d-\ndimensional supervector for each utterance, denoted\nas x = [nT , f T ]T where n is the accumulation of the\nmixture occupation over all frames of the utterance and\nf is the vector form of the centered ﬁrst order statistics.\n• The second step reduces the dimension of x from d to ¯d\n( ¯d ≪d) by multilayer bootstrap network (MBN) which\nis introduced in Section 3.\n• The third step conducts k-means clustering on the low-\ndimensional data if the number of the underlying speak-\ners is known, or agglomerative clustering if the number\nof the speakers is unknown.\n1The source code is downloadable from http://sites.google.com/site/\nzhangxiaolei321/speaker_recognition\narXiv:1509.06095v1  [cs.LG]  21 Sep 2015\n(k=6)\n(k=3)\n(k=2)\n1\n2\n3\nPCA\nHidden layer \nHidden layer \nHidden layer \nOutput layer \nFig. 1. The MBN network. Each square represents a k-centers\nclustering.\nCyclic-shift\nW1\nW2\nW3\nW1\nW2\nW3\nFig. 2. Random reconstruction step in MBN.\n3. MULTILAYER BOOTSTRAP NETWORK\nThe structure of MBN [13] is shown in Fig. 1. MBN is a\nmultilayer localized PCA algorithm that gradually enlarges\nthe area of a local region implicitly from the bottom hidden\nlayer to the top hidden layer by high-dimensional sparse cod-\ning, and gets a low-dimensional feature explicitly by PCA at\nthe output layer.\nEach hidden layer of MBN consists of a group of mutually\nindependent k-centers clusterings. Each k-centers clustering\nhas k output units, each of which indicates one cluster. The\noutput units of all clusterings are concatenated as the input of\ntheir upper layer [13].\nMBN is trained layer-by-layer from bottom up.\nFor\ntraining a hidden layer given a d-dimensional input X =\n{x1, . . . , xn}, MBN trains each clustering independently\n[13]:\n• Random feature selection. The ﬁrst step randomly se-\nlects ˆd dimensions of X ( ˆd ≤d) to form a new set\nˆ\nX = {ˆx1, . . . , ˆxn}. This step is controlled by a hyper-\nparameter a = ˆd/d.\n• Random sampling. The second step randomly selects\nk data points from ˆ\nX as the k centers of the clustering,\ndenoted as {w1, . . . , wk}. This step is controlled by a\nhyperparameter k.\n• Random reconstruction. The third step randomly se-\nlects d′ dimensions of the k centers (d′ ≤ˆd/2) and\ndoes a one-step cyclic-shift as shown in Fig. 2. This\nstep is controlled by a hyperparameter r = d′/ ˆd.\n• Sparse representation learning. The fourth step as-\nsigns the input ˆx to one of the k clusters and outputs\na k-dimensional indicator vector h = [h1, . . . , hk]T .\nFor example, if ˆx is assigned to the second cluster, then\nh = [0, 1, 0, . . . , 0]T . The assignment is calculated ac-\ncording to the similarities between ˆx and the k centers,\nin terms of some predeﬁned similarity measurement at\nthe bottom layer, such as the minimum squared loss\narg mink\ni=1 ∥wi−ˆx∥2, or in terms of arg maxk\ni=1 wT\ni ˆx\nat all other hidden layers [13].\n3.1. A typical hyperparameter setting\nMBN has ﬁve hyperparameters\n\b\nV, L, {kl}L\nl=1, a, r\n\t\nwhere\nV is the number of k-centers clusterings per layer, L is the\nnumber of hidden layers, and kl is the hyperparameter k at\nthe lth hidden layer. As shown in [13], MBN is robust to hy-\nperparameter selection. Here we introduce a typical setting:\n• Setting hyperparameter k. (i) k1 should be as large as\npossible, i.e. k1 →n. Suppose the largest k supported\nby hardware is kmax, then k1 = min(0.9n, kmax). (ii)\nkl decays with a factor of, e.g. 0.5, with the increase\nof hidden layers. That is to say, kl = 0.5kl−1. (iii) kL\nshould be larger than the number of speakers c. Typi-\ncally, kL ≈1.5c. If c is unknown, we simply set kL\nto a relatively large number, e.g. 30, since c is unlikely\nlarger than 30 in a practical dialog.\n• Setting hyperparameter r. When a problem is small-\nscale, e.g. k1 > 0.8n, then r = 0.5; otherwise, r = 0.\n• Setting other hyperparameters. Hyperparameter V\nshould be at least larger than 100, typically V = 400.\nHyperparameter a is ﬁxed to 0.5. Hyperparameter L is\ndetermined by k.\n4. RELATED WORK\nThe proposed method learns multilayer nonlinear transforms,\nwhich is related to deep learning (a.k.a., multilayer neural\nnetworks)—a recent advanced topic in many speech pro-\ncessing ﬁelds, e.g. speaker recognition [5, 6], speech recog-\nnition [14], speech separation and enhancement [15–18],\nspeech synthesis [19], and voice activity detection [20, 21].\nThe aforementioned deep learning methods are all supervised\nones and limited to neural networks, while the proposed\nmethod is an unsupervised one and different from neural\nnetworks.\n5. EXPERIMENTS\n5.1. Experimental setup\nWe used the training corpus of speech separation challenge\n(SSC) [22]. The training corpus contains 34 speakers, each\nof which has 500 clean utterances. We selected the ﬁrst 100\nutterances (a.k.a, sessions) of each speaker for evaluation,\nwhich amounts to 3400 utterances. We set the frame length\nto 25 milliseconds and frame shift to 10 milliseconds, and ex-\ntract a 25-dimensional MFCC feature.\nFor the proposed MBN-based speaker recognition, we\nadopted the typical parameter setting of MBN. Speciﬁ-\ncally, V\n= 400, a = 0.5, r = 0.5, and k were set to\n3060-1530-765-382-191-95. The output of PCA was set to\n{2, 3, 5, 10, 30, 50} dimensions respectively.\nWe assumed\nthat the number of speakers was known, and used k-means\nclustering for clustering the low-dimensional data.\nWe compared with PCA, k-means clustering, and an\nLDA-based system, where the ﬁrst two methods are unsu-\npervised and the third one is supervised. For the PCA-based\nmethod, we ﬁrst used the same UBM as the MBN-based\nmethod to extract high-dimensional supervectors, then re-\nduced the dimension of the supervectors to {2, 3, 5, 10, 30, 50}\nrespectively, and ﬁnally evaluated the low-dimensional output\nof PCA by k-means clustering. For the k-means-clustering-\nbased method, we apply k-means clustering to the high-\ndimensional supervectors directly.\nThe LDA-based system2 uses UBM to extract a high-\ndimensional feature, then uses joint factor analysis to reduce\nthe high-dimensional feature to an intermediately low di-\nmensional representation in an unsupervised way, and ﬁnally\nuses LDA, a supervised dimensionality reduction method, to\nreduce the intermediate representation to a low-dimensional\nsubspace where classiﬁcation is conducted by a probabilis-\ntic LDA algorithm.\nSince factor analysis is an unsuper-\nvised dimensionality reduction method, we set its output to\n{2, 3, 5, 10, 30, 50} dimensions respectively for comparison.\nWe constructed a training set from the SSC corpus for this\nsupervised method: each speaker consists of 100 training\nutterances, which are selected from the 400 remaining utter-\nances of the speaker.\nThe performance was measured by normalized mutual in-\nformation (NMI) [23]. MNI was proposed to overcome the la-\nbel indexing problem between the ground-truth labels and the\npredicted labels. It is one of the standard evaluation metrics of\nunsupervised learning. The higher the NMI is, the better the\nperformance is. We also report the classiﬁcation accuracy of\nthe LDA-based system in the Supplementary Material3 where\nwe can see that NMI is consistent with classiﬁcation accuracy.\n5.2. Results\nBecause all comparison methods use UBM to extract speaker-\nand session-independent supervectors, we need to study how\nthey behave in different UBM settings, in terms of mixture\nnumber and expectation-maximization (EM) iterations.\n(i)\n2The source code is downloadable from http://research.microsoft.com/en-\nus/downloads/a6262fec-03a7-4060-a08c-0b0d037a3f5b/\n3http://sites.google.com/site/ zhangxiaolei321/speaker_recognition\nMBN\n \nPCA\n \nFig. 3. Visualizations of 10 speakers by PCA and MBN re-\nspectively, where a 16-mixtures UBM with 20 EM iterations\nis used to produce their input supervectors. The speakers are\nlabeled in different colors.\nThe mixture number of UBM reﬂects the capacity of UBM\nfor modelling an underlying data distribution: if the mix-\nture number of UBM is smaller than the number of speakers,\nUBM is likely underﬁtting, i.e. it cannot grasp the data distri-\nbution well. To study this effect, we set the mixture number of\nUBM to {1, 2, 4, 8, 16, 32, 64} respectively. (ii) The number\nof EM iterations of UBM reﬂects the quality of the acoustic\nfeature produced by UBM: if the EM optimization is not sufﬁ-\ncient, the acoustic feature is noisy. To study this effect, we set\nthe number of EM iterations of UBM to {0, 20} respectively,\nwhere setting the number of iterations to 0 means that UBM\nis initialized with randomly sampled means without EM opti-\nmization, which is the worst case.\nFig.\n3 and Supplementary-Fig.\n1 give a comparison\nexample between PCA and MBN in visualizing the ﬁrst 10\nspeakers, where a 16-mixtures UBM with 20 and 0 EM iter-\nation are used to generate their inputs respectively. From the\nﬁgures, we can see that MBN produces ideal visualizations.\nFig. 4 reports results with respect to the mixture number\nof UBM. Fig. 5 reports results with respect to the number\nof output dimensions. Supplementary-Tables 1 and 3 report\nthe detailed results of the two ﬁgures. From the ﬁgures and\ntables, we observe the following phenomena: (i) the MBN-\nbased method outperforms the PCA- and k-means-clustering-\nbased methods and approaches to the supervised LDA system\nin all cases; (ii) the MBN-based method is less sensitive to dif-\nferent parameter settings of both UBM and MBN itself; (iii)\nthe LDA-based system is less sensitive to the mixture number\nof UBM, but sensitive to the number of output dimensions;\n(iv) the PCA-based method is sensitive to both the mixture\nnumber of UBM and the number of output dimensions, and\nstrongly relies on the effectiveness of UBM; (v) the perfor-\nmance of the k-means-clustering-based method is consistent\nwith that of the PCA-based method.\nFig. 6 reports results of the MBN-based method with re-\nspect to the number of hidden layers. From the ﬁgure, we ob-\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nNumber of layers\nAccuracy\n2-dimensional feature\n \n \n1\n2\n3\n4\n5\n6\nNumber of layers\n3-dimensional feature\n \n \n1\n2\n3\n4\n5\n6\nNumber of layers\n5-dimensional feature\n \n \n1\n2\n3\n4\n5\n6\nNumber of layers\n10-dimensional feature\n \n \n1\n2\n3\n4\n5\n6\nNumber of layers\n30-dimensional feature\n \n1\n2\n3\n4\n5\n6\nNumber of layers\n50-dimensional feature\n \n1\n2\n3\n4\n5\n6\n0 1\n0.2\n0.3\nMBN + UBM with 0 iteration\nMBN + UBM with 20 iterations\nFig. 6. Accuracy (in terms of NMI) of MBN-based method with respect to the number of hidden layers.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMixture number of UBM\nAccuracy\n(a) UBM with 20 EM iterations\n \n \n1\n2\n4\n8\n16\n32\n64\nLDA (supervised)\nk-means (unsupervised)\nPCA (unsupervised)\nMBN (unsupervised)\nMixture number of UBM\n(b) UBM with 0 EM iteration\n \n \n1\n2\n4\n8\n16\n32\n64\nFig. 4.\nAccuracy comparison (in terms of NMI) between\nLDA-, k-means clustering-, PCA-, and MBN-based speaker\nrecognition methods with respect to the mixture number of\nUBM. (a) Comparison when the EM iteration number of\nUBM is set to 20. (b) Comparison when the EM iteration\nnumber of UBM is set to 0. Note that given a mixture number\nof UBM, the accuracy of a method is the best result among\nthe results produced from 6 candidate output dimensions of\nthe method, except k-means clustering.\nserve that the accuracy improves gradually with the increase\nof the number of hidden layers.\n6. CONCLUSIONS\nIn this paper, we have proposed a multilayer bootstrap net-\nwork based unsupervised speaker recognition algorithm. The\nmethod ﬁrst uses UBM to extract a high-dimensional feature\nfrom the original MFCC acoustic feature, then uses MBN\nto reduce the high-dimensional feature to a low-dimensional\nspace, and ﬁnally clustering the low-dimensional data. We\nhave compared it with the PCA-, k-means-clustering-, and\nLDA-based methods, where the ﬁrst two methods are un-\nsupervised and the third method is supervised. Experimen-\ntal results have shown that the proposed method outperforms\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nNumber of dimensions\nAccuracy\n(a) UBM with 20 EM iterations\n 2\n3\n5\n10\n30\n50\nNumber of dimensions\n(b) UBM with 0 EM iteration\n2\n3\n5\n10\n30\n50\nLDA (supervised)\nPCA (unsupervised)\nMBN (unsupervised)\nFig. 5.\nAccuracy comparison (in terms of NMI) between\nLDA-, PCA-, and MBN-based speaker recognition methods\nwith respect to the number of output dimensions. (a) Com-\nparison when the EM iteration number of UBM is set to 20.\n(b) Comparison when the EM iteration number of UBM is\nset to 0. Note that given a number of output dimensions, the\naccuracy of a method is the best result among the results pro-\nduced from 7 candidate UBMs.\nthe unsupervised methods and approaches to the supervised\nmethod. Moreover, it is insensitive to different parameter set-\ntings of UBM and MBN, which facilitates its practical use.\n7. ACKNOWLEDGEMENT\nThe author thanks Prof DeLiang Wang for providing the Ohio\nComputing Center and Dr Ke Hu for helping with the SSC\ncorpus.\n8. REFERENCES\n[1] Douglas A Reynolds, Thomas F Quatieri, and Robert B\nDunn,\n“Speaker veriﬁcation using adapted gaussian\nmixture models,” Digital Signal Process., vol. 10, no.\n1, pp. 19–41, 2000.\n[2] William M Campbell, Douglas E Sturim, Douglas A\nReynolds, and Alex Solomonoff, “SVM based speaker\nveriﬁcation using a GMM supervector kernel and NAP\nvariability compensation,”\nin Proc. IEEE Int. Conf.\nAcoust., Speech, Signal Process., 2006, vol. 1, pp. 97–\n100.\n[3] Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and\nPierre Dumouchel, “Joint factor analysis versus eigen-\nchannels in speaker recognition,” IEEE Trans. Audio,\nSpeech, Lang. Process., vol. 15, no. 4, pp. 1435–1447,\n2007.\n[4] Najim Dehak, Patrick Kenny, Réda Dehak, Pierre Du-\nmouchel, and Pierre Ouellet, “Front-end factor analysis\nfor speaker veriﬁcation,” IEEE Trans. Audio, Speech,\nLang. Process., vol. 19, no. 4, pp. 788–798, 2011.\n[5] Ke Chen and Ahmad Salman,\n“Learning speaker-\nspeciﬁc characteristics with a deep neural architecture,”\nIEEE Trans. Neural Netw., vol. 22, no. 11, pp. 1744–\n1756, 2011.\n[6] Xiaojia Zhao, Yuxuan Wang, and DeLiang Wang,\n“Cochannel speaker identiﬁcation in anechoic and re-\nverberant conditions,” IEEE/ACM Trans. Audio, Speech,\nLang. Process., vol. 22, no. 11, pp. 1727–1736, 2015.\n[7] Chuck Wooters and Marijn Huijbregts,\n“The ICSI\nRT07s speaker diarization system,”\nin Multimodal\nTechnologies for Perception of Humans, pp. 509–519.\nSpringer, 2008.\n[8] Ken-ichi Iso, “Speaker clustering using vector quanti-\nzation and spectral clustering,” in Proc. IEEE Int. Conf.\nAcoust., Speech, Signal Process., 2010, pp. 4986–4989.\n[9] Tin Lay Nwe, Hanwu Sun, Bin Ma, and Haizhou Li,\n“Speaker clustering and cluster puriﬁcation methods for\nrt07 and rt09 evaluation meeting data,” IEEE Trans. Au-\ndio, Speech, Lang. Process., vol. 20, no. 2, pp. 461–473,\n2012.\n[10] Stephen H Shum, Najim Dehak, Réda Dehak, and\nJames R Glass, “Unsupervised methods for speaker di-\narization: An integrated and iterative approach,” IEEE\nTrans. Audio, Speech, Lang. Process., vol. 21, no. 10,\npp. 2015–2028, 2013.\n[11] Naohiro\nTawara,\nTetsuji\nOgawa,\nand\nTetsunori\nKobayashi, “A comparative study of spectral clustering\nfor\ni-vector-based\nspeaker\nclustering\nunder\nnoisy\nconditions,” in Proc. IEEE Int. Conf. Acoust., Speech,\nSignal Process., 2015, pp. 2041–2045.\n[12] Kui Wu, Yan Song, Wu Guo, and Lirong Dai, “Intra-\nconversation intra-speaker variability compensation for\nspeaker clustering,” in Proc. Int. Sym. Chinese Spoken\nLang. Process., 2012, pp. 330–334.\n[13] Xiao-Lei Zhang,\n“Nonlinear dimensionality reduc-\ntion of data by multilayer bootstrap networks,” arXiv\npreprint arXiv:1408.0848, pp. 1–20, 2014.\n[14] George E Dahl, Dong Yu, Li Deng, and Alex Acero,\n“Context-dependent pre-trained deep neural networks\nfor large-vocabulary speech recognition,” IEEE Trans.\nAudio, Speech, Lang. Process., vol. 20, no. 1, pp. 30–42,\n2012.\n[15] Yuxuan Wang and DeLiang Wang,\n“Towards scaling\nup classiﬁcation-based speech separation,” IEEE Trans.\nAudio, Speech, Lang. Process., vol. 21, no. 7, pp. 1381–\n1390, 2013.\n[16] Yong Xu, Jun Du, Li-Rong Dai, and Chin-Hui Lee,\n“A regression approach to speech enhancement based\non deep neural networks,”\nIEEE/ACM Trans. Audio,\nSpeech, Lang. Process., vol. 23, no. 1, pp. 7–19, 2015.\n[17] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson,\nand Paris Smaragdis, “Joint optimization of masks and\ndeep recurrent neural networks for monaural source sep-\naration,” IEEE/ACM Trans. Audio, Speech, Lang. Pro-\ncess., vol. 23, no. 12, pp. 2136–2147, 2015.\n[18] Xiao-Lei Zhang and DeLiang Wang,\n“Deep ensem-\nble learning for monaural speech separation,”\nTech.\nRep. OSU-CISRC-8/15-TR13, Department of Com-\nputer Science and Engineering, The Ohio State Univer-\nsity, Columbus, Ohio, USA, 2015.\n[19] Zhen-Hua Ling, Li Deng, and Dong Yu,\n“Model-\ning spectral envelopes using restricted boltzmann ma-\nchines and deep belief networks for statistical paramet-\nric speech synthesis,” IEEE Trans. Audio, Speech, Lang.\nProcess., vol. 21, no. 10, pp. 2129–2139, 2013.\n[20] Xiao-Lei Zhang and Ji Wu, “Deep belief networks based\nvoice activity detection,” IEEE Trans. Audio, Speech,\nLang. Process., vol. 21, no. 4, pp. 697–710, 2013.\n[21] Xiao-Lei Zhang and DeLiang Wang, “Boosting contex-\ntual information for deep neural network based voice ac-\ntivity detection,” Tech. Rep. OSU-CISRC-5/15-TR06,\nDepartment of Computer Science and Engineering, The\nOhio State University, Columbus, Ohio, USA, 2015.\n[22] Martin\nCooke\nand\nTe-Won\nLee,\n“Speech\nseparation\nchallenge,”\nhttp://staffwww.\ndcs.shef.ac.uk/people/M.Cooke/\nSpeechSeparationChallenge.htm, 2006.\n[23] Alexander Strehl and Joydeep Ghosh,\n“Cluster\nensembles—a knowledge reuse framework for combin-\ning multiple partitions,” J. Mach. Learn. Res., vol. 3, pp.\n583–617, 2003.\n",
  "categories": [
    "cs.LG",
    "cs.SD"
  ],
  "published": "2015-09-21",
  "updated": "2015-09-21"
}