{
  "id": "http://arxiv.org/abs/1510.00726v1",
  "title": "A Primer on Neural Network Models for Natural Language Processing",
  "authors": [
    "Yoav Goldberg"
  ],
  "abstract": "Over the past few years, neural networks have re-emerged as powerful\nmachine-learning models, yielding state-of-the-art results in fields such as\nimage recognition and speech processing. More recently, neural network models\nstarted to be applied also to textual natural language signals, again with very\npromising results. This tutorial surveys neural network models from the\nperspective of natural language processing research, in an attempt to bring\nnatural-language researchers up to speed with the neural techniques. The\ntutorial covers input encoding for natural language tasks, feed-forward\nnetworks, convolutional networks, recurrent networks and recursive networks, as\nwell as the computation graph abstraction for automatic gradient computation.",
  "text": "A Primer on Neural Network Models\nfor Natural Language Processing\nYoav Goldberg\nDraft as of October 6, 2015.\nThe most up-to-date version of this manuscript is available at http://www.cs.biu.\nac.il/˜yogo/nnlp.pdf. Major updates will be published on arxiv periodically.\nI welcome any comments you may have regarding the content and presentation. If you\nspot a missing reference or have relevant work you’d like to see mentioned, do let me know.\nfirst.last@gmail\nAbstract\nOver the past few years, neural networks have re-emerged as powerful machine-learning\nmodels, yielding state-of-the-art results in ﬁelds such as image recognition and speech\nprocessing. More recently, neural network models started to be applied also to textual\nnatural language signals, again with very promising results. This tutorial surveys neural\nnetwork models from the perspective of natural language processing research, in an attempt\nto bring natural-language researchers up to speed with the neural techniques. The tutorial\ncovers input encoding for natural language tasks, feed-forward networks, convolutional\nnetworks, recurrent networks and recursive networks, as well as the computation graph\nabstraction for automatic gradient computation.\n1. Introduction\nFor a long time, core NLP techniques were dominated by machine-learning approaches that\nused linear models such as support vector machines or logistic regression, trained over very\nhigh dimensional yet very sparse feature vectors.\nRecently, the ﬁeld has seen some success in switching from such linear models over\nsparse inputs to non-linear neural-network models over dense inputs. While most of the\nneural network techniques are easy to apply, sometimes as almost drop-in replacements of\nthe old linear classiﬁers, there is in many cases a strong barrier of entry. In this tutorial I\nattempt to provide NLP practitioners (as well as newcomers) with the basic background,\njargon, tools and methodology that will allow them to understand the principles behind\nthe neural network models and apply them to their own work. This tutorial is expected\nto be self-contained, while presenting the diﬀerent approaches under a uniﬁed notation and\nframework.\nIt repeats a lot of material which is available elsewhere.\nIt also points to\nexternal sources for more advanced topics when appropriate.\nThis primer is not intended as a comprehensive resource for those that will go on and\ndevelop the next advances in neural-network machinery (though it may serve as a good entry\npoint). Rather, it is aimed at those readers who are interested in taking the existing, useful\ntechnology and applying it in useful and creative ways to their favourite NLP problems. For\nmore in-depth, general discussion of neural networks, the theory behind them, advanced\n1\narXiv:1510.00726v1  [cs.CL]  2 Oct 2015\noptimization methods and other advanced topics, the reader is referred to other existing\nresources. In particular, the book by Bengio et al (2015) is highly recommended.\nScope\nThe focus is on applications of neural networks to language processing tasks. How-\never, some subareas of language processing with neural networks were decidedly left out of\nscope of this tutorial. These include the vast literature of language modeling and acoustic\nmodeling, the use of neural networks for machine translation, and multi-modal applications\ncombining language and other signals such as images and videos (e.g. caption generation).\nCaching methods for eﬃcient runtime performance, methods for eﬃcient training with large\noutput vocabularies and attention models are also not discussed. Word embeddings are dis-\ncussed only to the extent that is needed to understand in order to use them as inputs for\nother models. Other unsupervised approaches, including autoencoders and recursive au-\ntoencoders, also fall out of scope. While some applications of neural networks for language\nmodeling and machine translation are mentioned in the text, their treatment is by no means\ncomprehensive.\nA Note on Terminology\nThe word “feature” is used to refer to a concrete, linguistic\ninput such as a word, a suﬃx, or a part-of-speech tag. For example, in a ﬁrst-order part-\nof-speech tagger, the features might be “current word, previous word, next word, previous\npart of speech”. The term “input vector” is used to refer to the actual input that is fed\nto the neural-network classiﬁer. Similarly, “input vector entry” refers to a speciﬁc value\nof the input. This is in contrast to a lot of the neural networks literature in which the\nword “feature” is overloaded between the two uses, and is used primarily to refer to an\ninput-vector entry.\nMathematical Notation\nI use bold upper case letters to represent matrices (X, Y,\nZ), and bold lower-case letters to represent vectors (b). When there are series of related\nmatrices and vectors (for example, where each matrix corresponds to a diﬀerent layer in\nthe network), superscript indices are used (W1, W2). For the rare cases in which we want\nindicate the power of a matrix or a vector, a pair of brackets is added around the item to\nbe exponentiated: (W)2, (W3)2. Unless otherwise stated, vectors are assumed to be row\nvectors. We use [v1; v2] to denote vector concatenation.\n2\n2. Neural Network Architectures\nNeural networks are powerful learning models. We will discuss two kinds of neural network\narchitectures, that can be mixed and matched – feed-forward networks and Recurrent /\nRecursive networks. Feed-forward networks include networks with fully connected layers,\nsuch as the multi-layer perceptron, as well as networks with convolutional and pooling\nlayers. All of the networks act as classiﬁers, but each with diﬀerent strengths.\nFully connected feed-forward neural networks (Section 4) are non-linear learners that\ncan, for the most part, be used as a drop-in replacement wherever a linear learner is used.\nThis includes binary and multiclass classiﬁcation problems, as well as more complex struc-\ntured prediction problems (Section 8). The non-linearity of the network, as well as the\nability to easily integrate pre-trained word embeddings, often lead to superior classiﬁcation\naccuracy. A series of works (Chen & Manning, 2014; Weiss, Alberti, Collins, & Petrov,\n2015; Pei, Ge, & Chang, 2015; Durrett & Klein, 2015) managed to obtain improved syntac-\ntic parsing results by simply replacing the linear model of a parser with a fully connected\nfeed-forward network. Straight-forward applications of a feed-forward network as a classi-\nﬁer replacement (usually coupled with the use of pre-trained word vectors) provide beneﬁts\nalso for CCG supertagging (Lewis & Steedman, 2014), dialog state tracking (Henderson,\nThomson, & Young, 2013), pre-ordering for statistical machine translation (de Gispert,\nIglesias, & Byrne, 2015) and language modeling (Bengio, Ducharme, Vincent, & Janvin,\n2003; Vaswani, Zhao, Fossum, & Chiang, 2013). Iyyer et al (2015) demonstrate that multi-\nlayer feed-forward networks can provide competitive results on sentiment classiﬁcation and\nfactoid question answering.\nNetworks with convolutional and pooling layers (Section 9) are useful for classiﬁcation\ntasks in which we expect to ﬁnd strong local clues regarding class membership, but these\nclues can appear in diﬀerent places in the input. For example, in a document classiﬁcation\ntask, a single key phrase (or an ngram) can help in determining the topic of the document\n(Johnson & Zhang, 2015). We would like to learn that certain sequences of words are good\nindicators of the topic, and do not necessarily care where they appear in the document.\nConvolutional and pooling layers allow the model to learn to ﬁnd such local indicators,\nregardless of their position. Convolutional and pooling architecture show promising results\non many tasks, including document classiﬁcation (Johnson & Zhang, 2015), short-text cat-\negorization (Wang, Xu, Xu, Liu, Zhang, Wang, & Hao, 2015a), sentiment classiﬁcation\n(Kalchbrenner, Grefenstette, & Blunsom, 2014; Kim, 2014), relation type classiﬁcation be-\ntween entities (Zeng, Liu, Lai, Zhou, & Zhao, 2014; dos Santos, Xiang, & Zhou, 2015), event\ndetection (Chen, Xu, Liu, Zeng, & Zhao, 2015; Nguyen & Grishman, 2015), paraphrase iden-\ntiﬁcation (Yin & Sch¨utze, 2015) semantic role labeling (Collobert, Weston, Bottou, Karlen,\nKavukcuoglu, & Kuksa, 2011), question answering (Dong, Wei, Zhou, & Xu, 2015), predict-\ning box-oﬃce revenues of movies based on critic reviews (Bitvai & Cohn, 2015) modeling\ntext interestingness (Gao, Pantel, Gamon, He, & Deng, 2014), and modeling the relation\nbetween character-sequences and part-of-speech tags (Santos & Zadrozny, 2014).\nIn natural language we often work with structured data of arbitrary sizes, such as\nsequences and trees. We would like to be able to capture regularities in such structures,\nor to model similarities between such structures.\nIn many cases, this means encoding\nthe structure as a ﬁxed width vector, which we can then pass on to another statistical\n3\nlearner for further processing. While convolutional and pooling architectures allow us to\nencode arbitrary large items as ﬁxed size vectors capturing their most salient features,\nthey do so by sacriﬁcing most of the structural information. Recurrent (Section 10) and\nrecursive (Section 12) architectures, on the other hand, allow us to work with sequences\nand trees while preserving a lot of the structural information. Recurrent networks (Elman,\n1990) are designed to model sequences, while recursive networks (Goller & K¨uchler, 1996)\nare generalizations of recurrent networks that can handle trees. We will also discuss an\nextension of recurrent networks that allow them to model stacks (Dyer, Ballesteros, Ling,\nMatthews, & Smith, 2015; Watanabe & Sumita, 2015).\nRecurrent models have been shown to produce very strong results for language model-\ning, including (Mikolov, Karaﬁ´at, Burget, Cernocky, & Khudanpur, 2010; Mikolov, Kom-\nbrink, Luk´aˇs Burget, ˇCernocky, & Khudanpur, 2011; Mikolov, 2012; Duh, Neubig, Sudoh,\n& Tsukada, 2013; Adel, Vu, & Schultz, 2013; Auli, Galley, Quirk, & Zweig, 2013; Auli &\nGao, 2014); as well as for sequence tagging (Irsoy & Cardie, 2014; Xu, Auli, & Clark, 2015;\nLing, Dyer, Black, Trancoso, Fermandez, Amir, Marujo, & Luis, 2015b), machine transla-\ntion (Sundermeyer, Alkhouli, Wuebker, & Ney, 2014; Tamura, Watanabe, & Sumita, 2014;\nSutskever, Vinyals, & Le, 2014; Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares,\nSchwenk, & Bengio, 2014b), dependency parsing (Dyer et al., 2015; Watanabe & Sumita,\n2015), sentiment analysis (Wang, Liu, SUN, Wang, & Wang, 2015b), noisy text normal-\nization (Chrupala, 2014), dialog state tracking (Mrkˇsi´c, ´O S´eaghdha, Thomson, Gasic, Su,\nVandyke, Wen, & Young, 2015), response generation (Sordoni, Galley, Auli, Brockett, Ji,\nMitchell, Nie, Gao, & Dolan, 2015), and modeling the relation between character sequences\nand part-of-speech tags (Ling et al., 2015b).\nRecursive models were shown to produce state-of-the-art or near state-of-the-art re-\nsults for constituency (Socher, Bauer, Manning, & Andrew Y., 2013) and dependency (Le\n& Zuidema, 2014; Zhu, Qiu, Chen, & Huang, 2015a) parse re-ranking, discourse parsing\n(Li, Li, & Hovy, 2014), semantic relation classiﬁcation (Hashimoto, Miwa, Tsuruoka, &\nChikayama, 2013; Liu, Wei, Li, Ji, Zhou, & WANG, 2015), political ideology detection\nbased on parse trees (Iyyer, Enns, Boyd-Graber, & Resnik, 2014b), sentiment classiﬁcation\n(Socher, Perelygin, Wu, Chuang, Manning, Ng, & Potts, 2013; Hermann & Blunsom, 2013),\ntarget-dependent sentiment classiﬁcation (Dong, Wei, Tan, Tang, Zhou, & Xu, 2014) and\nquestion answering (Iyyer, Boyd-Graber, Claudino, Socher, & Daum´e III, 2014a).\n4\n3. Feature Representation\nBefore discussing the network structure in more depth, it is important to pay attention\nto how features are represented. For now, we can think of a feed-forward neural network\nas a function NN(x) that takes as input a din dimensional vector x and produces a dout\ndimensional output vector. The function is often used as a classiﬁer, assigning the input\nx a degree of membership in one or more of dout classes. The function can be complex,\nand is almost always non-linear.\nCommon structures of this function will be discussed\nin Section 4. Here, we focus on the input, x. When dealing with natural language, the\ninput x encodes features such as words, part-of-speech tags or other linguistic information.\nPerhaps the biggest jump when moving from sparse-input linear models to neural-network\nbased models is to stop representing each feature as a unique dimension (the so called\none-hot representation) and representing them instead as dense vectors. That is, each core\nfeature is embedded into a d dimensional space, and represented as a vector in that space.1\nThe embeddings (the vector representation of each core feature) can then be trained like\nthe other parameter of the function NN. Figure 1 shows the two approaches to feature\nrepresentation.\nThe feature embeddings (the values of the vector entries for each feature) are treated\nas model parameters that need to be trained together with the other components of the\nnetwork. Methods of training (or obtaining) the feature embeddings will be discussed later.\nFor now, consider the feature embeddings as given.\nThe general structure for an NLP classiﬁcation system based on a feed-forward neural\nnetwork is thus:\n1. Extract a set of core linguistic features f1, . . . , fk that are relevant for predicting the\noutput class.\n2. For each feature fi of interest, retrieve the corresponding vector v(fi).\n3. Combine the vectors (either by concatenation, summation or a combination of both)\ninto an input vector x.\n4. Feed x into a non-linear classiﬁer (feed-forward neural network).\nThe biggest change in the input, then, is the move from sparse representations in which\neach feature is its own dimension, to a dense representation in which each feature is mapped\nto a vector. Another diﬀerence is that we extract only core features and not feature com-\nbinations. We will elaborate on both these changes brieﬂy.\nDense Vectors vs. One-hot Representations\nWhat are the beneﬁts of representing\nour features as vectors instead of as unique IDs? Should we always represent features as\ndense vectors? Let’s consider the two kinds of representations:\nOne Hot Each feature is its own dimension.\n• Dimensionality of one-hot vector is same as number of distinct features.\n1. Diﬀerent feature types may be embedded into diﬀerent spaces. For example, one may represent word\nfeatures using 100 dimensions, and part-of-speech features using 20 dimensions.\n5\nFigure 1: Sparse vs. dense feature representations. Two encodings of the informa-\ntion: current word is “dog”; previous word is “the”; previous pos-tag is “DET”.\n(a) Sparse feature vector. Each dimension represents a feature. Feature combi-\nnations receive their own dimensions. Feature values are binary. Dimensionality\nis very high. (b) Dense, embeddings-based feature vector. Each core feature is\nrepresented as a vector.\nEach feature corresponds to several input vector en-\ntries. No explicit encoding of feature combinations. Dimensionality is low. The\nfeature-to-vector mappings come from an embedding table.\n• Features are completely independent from one another. The feature “word is\n‘dog’ ” is as dis-similar to “word is ‘thinking’ ” than it is to “word is ‘cat’ ”.\nDense Each feature is a d-dimensional vector.\n• Dimensionality of vector is d.\n• Similar features will have similar vectors – information is shared between similar\nfeatures.\nOne beneﬁt of using dense and low-dimensional vectors is computational: the majority\nof neural network toolkits do not play well with very high-dimensional, sparse vectors.\nHowever, this is just a technical obstacle, which can be resolved with some engineering\neﬀort.\nThe main beneﬁt of the dense representations is in generalization power: if we believe\nsome features may provide similar clues, it is worthwhile to provide a representation that\nis able to capture these similarities. For example, assume we have observed the word ‘dog’\nmany times during training, but only observed the word ‘cat’ a handful of times, or not at\n6\nall. If each of the words is associated with its own dimension, occurrences of ‘dog’ will not\ntell us anything about the occurrences of ‘cat’. However, in the dense vectors representation\nthe learned vector for ‘dog’ may be similar to the learned vector from ‘cat’, allowing the\nmodel to share statistical strength between the two events. This argument assumes that\n“good” vectors are somehow given to us. Section 5 describes ways of obtaining such vector\nrepresentations.\nIn cases where we have relatively few distinct features in the category, and we believe\nthere are no correlations between the diﬀerent features, we may use the one-hot representa-\ntion. However, if we believe there are going to be correlations between the diﬀerent features\nin the group (for example, for part-of-speech tags, we may believe that the diﬀerent verb\ninﬂections VB and VBZ may behave similarly as far as our task is concerned) it may be\nworthwhile to let the network ﬁgure out the correlations and gain some statistical strength\nby sharing the parameters. It may be the case that under some circumstances, when the\nfeature space is relatively small and the training data is plentiful, or when we do not wish to\nshare statistical information between distinct words, there are gains to be made from using\nthe one-hot representations. However, this is still an open research question, and there are\nno strong evidence to either side. The majority of work (pioneered by (Collobert & Weston,\n2008; Collobert et al., 2011; Chen & Manning, 2014)) advocate the use of dense, trainable\nembedding vectors for all features. For work using neural network architecture with sparse\nvector encodings see (Johnson & Zhang, 2015).\nFinally, it is important to note that representing features as dense vectors is an integral\npart of the neural network framework, and that consequentially the diﬀerences between\nusing sparse and dense feature representations are subtler than they may appear at ﬁrst.\nIn fact, using sparse, one-hot vectors as input when training a neural network amounts\nto dedicating the ﬁrst layer of the network to learning a dense embedding vector for each\nfeature based on the training data. We touch on this in Section 4.4.\nVariable Number of Features: Continuous Bag of Words\nFeed-forward networks\nassume a ﬁxed dimensional input.\nThis can easily accommodate the case of a feature-\nextraction function that extracts a ﬁxed number of features: each feature is represented\nas a vector, and the vectors are concatenated.\nThis way, each region of the resulting\ninput vector corresponds to a diﬀerent feature.\nHowever, in some cases the number of\nfeatures is not known in advance (for example, in document classiﬁcation it is common\nthat each word in the sentence is a feature). We thus need to represent an unbounded\nnumber of features using a ﬁxed size vector. One way of achieving this is through a so-\ncalled continuous bag of words (CBOW) representation (Mikolov, Chen, Corrado, & Dean,\n2013). The CBOW is very similar to the traditional bag-of-words representation in which\nwe discard order information, and works by either summing or averaging the embedding\nvectors of the corresponding features:2\n2. Note that if the v(fi)s were one-hot vectors rather than dense feature representations, the CBOW and\nWCBOW equations above would reduce to the traditional (weighted) bag-of-words representations,\nwhich is in turn equivalent to a sparse feature-vector representation in which each binary indicator\nfeature corresponds to a unique “word”.\n7\nCBOW(f1, ..., fk) = 1\nk\nk\nX\ni=1\nv(fi)\nA simple variation on the CBOW representation is weighted CBOW, in which diﬀerent\nvectors receive diﬀerent weights:\nWCBOW(f1, ..., fk) =\n1\nPk\ni=1 ai\nk\nX\ni=1\naiv(fi)\nHere, each feature fi has an associated weight ai, indicating the relative importance of\nthe feature. For example, in a document classiﬁcation task, a feature fi may correspond to\na word in the document, and the associated weight ai could be the word’s TF-IDF score.\nDistance and Position Features\nThe linear distance in between two words in a sentence\nmay serve as an informative feature. For example, in an event extraction task3 we may be\ngiven a trigger word and a candidate argument word, and asked to predict if the argument\nword is indeed an argument of the trigger. The distance (or relative position) between the\ntrigger and the argument is a strong signal for this prediction task. In the “traditional” NLP\nsetup, distances are usually encoded by binning the distances into several groups (i.e. 1, 2,\n3, 4, 5–10, 10+) and associating each bin with a one-hot vector. In a neural architecture,\nwhere the input vector is not composed of binary indicator features, it may seem natural to\nallocate a single input vector entry to the distance feature, where the numeric value of that\nentry is the distance. However, this approach is not taken in practice. Instead, distance\nfeatures are encoded similarly to the other feature types: each bin is associated with a\nd-dimensional vector, and these distance-embedding vectors are then trained as regular\nparameters in the network (Zeng et al., 2014; dos Santos et al., 2015; Zhu et al., 2015a;\nNguyen & Grishman, 2015).\nFeature Combinations\nNote that the feature extraction stage in the neural-network\nsettings deals only with extraction of core features. This is in contrast to the traditional\nlinear-model-based NLP systems in which the feature designer had to manually specify not\nonly the core features of interests but also interactions between them (e.g., introducing not\nonly a feature stating “word is X” and a feature stating “tag is Y” but also combined feature\nstating “word is X and tag is Y” or sometimes even “word is X, tag is Y and previous word\nis Z”). The combination features are crucial in linear models because they introduce more\ndimensions to the input, transforming it into a space where the data-points are closer to\nbeing linearly separable. On the other hand, the space of possible combinations is very\nlarge, and the feature designer has to spend a lot of time coming up with an eﬀective\nset of feature combinations. One of the promises of the non-linear neural network models\nis that one needs to deﬁne only the core features. The non-linearity of the classiﬁer, as\ndeﬁned by the network structure, is expected to take care of ﬁnding the indicative feature\ncombinations, alleviating the need for feature combination engineering.\n3. The event extraction task involves identiﬁcation of events from a predeﬁned set of event types. For\nexample identiﬁcation of “purchase” events or “terror-attack” events. Each event type can be triggered\nby various triggering words (commonly verbs), and has several slots (arguments) that needs to be ﬁlled\n(i.e. who purchased? what was purchased? at what amount?).\n8\nKernel methods (Shawe-Taylor & Cristianini, 2004), and in particular polynomial kernels\n(Kudo & Matsumoto, 2003), also allow the feature designer to specify only core features,\nleaving the feature combination aspect to the learning algorithm. In contrast to neural-\nnetwork models, kernels methods are convex, admitting exact solutions to the optimization\nproblem. However, the classiﬁcation eﬃciency in kernel methods scales linearly with the\nsize of the training data, making them too slow for most practical purposes, and not suitable\nfor training with large datasets. On the other hand, neural network classiﬁcation eﬃciency\nscales linearly with the size of the network, regardless of the training data size.\nDimensionality\nHow many dimensions should we allocate for each feature? Unfortu-\nnately, there are no theoretical bounds or even established best-practices in this space.\nClearly, the dimensionality should grow with the number of the members in the class (you\nprobably want to assign more dimensions to word embeddings than to part-of-speech embed-\ndings) but how much is enough? In current research, the dimensionality of word-embedding\nvectors range between about 50 to a few hundreds, and, in some extreme cases, thousands.\nSince the dimensionality of the vectors has a direct eﬀect on memory requirements and\nprocessing time, a good rule of thumb would be to experiment with a few diﬀerent sizes,\nand choose a good trade-oﬀbetween speed and task accuracy.\nVector Sharing\nConsider a case where you have a few features that share the same\nvocabulary. For example, when assigning a part-of-speech to a given word, we may have a\nset of features considering the previous word, and a set of features considering the next word.\nWhen building the input to the classiﬁer, we will concatenate the vector representation of\nthe previous word to the vector representation of the next word. The classiﬁer will then\nbe able to distinguish the two diﬀerent indicators, and treat them diﬀerently. But should\nthe two features share the same vectors? Should the vector for “dog:previous-word” be the\nsame as the vector of “dog:next-word”? Or should we assign them two distinct vectors?\nThis, again, is mostly an empirical question. If you believe words behave diﬀerently when\nthey appear in diﬀerent positions (e.g., word X behaves like word Y when in the previous\nposition, but X behaves like Z when in the next position) then it may be a good idea to\nuse two diﬀerent vocabularies and assign a diﬀerent set of vectors for each feature type.\nHowever, if you believe the words behave similarly in both locations, then something may\nbe gained by using a shared vocabulary for both feature types.\nNetwork’s Output\nFor multi-class classiﬁcation problems with k classes, the network’s\noutput is a k-dimensional vector in which every dimension represents the strength of a\nparticular output class. That is, the output remains as in the traditional linear models –\nscalar scores to items in a discrete set. However, as we will see in Section 4, there is a d × k\nmatrix associated with the output layer. The columns of this matrix can be thought of as\nd dimensional embeddings of the output classes. The vector similarities between the vector\nrepresentations of the k classes indicate the model’s learned similarities between the output\nclasses.\nHistorical Note\nRepresenting words as dense vectors for input to a neural network was\nintroduced by Bengio et al (Bengio et al., 2003) in the context of neural language modeling.\nIt was introduced to NLP tasks in the pioneering work of Collobert, Weston and colleagues\n9\n(2008, 2011). Using embeddings for representing not only words but arbitrary features was\npopularized following Chen and Manning (2014).\n10\n4. Feed-forward Neural Networks\nA Brain-inspired metaphor\nAs the name suggest, neural-networks are inspired by the\nbrain’s computation mechanism, which consists of computation units called neurons. In the\nmetaphor, a neuron is a computational unit that has scalar inputs and outputs. Each input\nhas an associated weight. The neuron multiplies each input by its weight, and then sums4\nthem, applies a non-linear function to the result, and passes it to its output. The neurons\nare connected to each other, forming a network: the output of a neuron may feed into the\ninputs of one or more neurons. Such networks were shown to be very capable computational\ndevices. If the weights are set correctly, a neural network with enough neurons and a non-\nlinear activation function can approximate a very wide range of mathematical functions (we\nwill be more precise about this later).\nx1\nx2\nx3\nx4\nInput layer\nR\nR\nR\nR\nR\nR\nHidden\nlayer\nR\nR\nR\nR\nR\nHidden\nlayer\ny1\ny2\ny3\nOutput\nlayer\nFigure 2: Feed-forward neural network with two hidden layers.\nA typical feed-forward neural network may be drawn as in Figure 2. Each circle is a\nneuron, with incoming arrows being the neuron’s inputs and outgoing arrows being the neu-\nron’s outputs. Each arrow carries a weight, reﬂecting its importance (not shown). Neurons\nare arranged in layers, reﬂecting the ﬂow of information. The bottom layer has no incom-\ning arrows, and is the input to the network. The top-most layer has no outgoing arrows,\nand is the output of the network. The other layers are considered “hidden”. The sigmoid\nshape inside the neurons in the middle layers represent a non-linear function (typically a\n1/(1 + e−x)) that is applied to the neuron’s value before passing it to the output. In the\nﬁgure, each neuron is connected to all of the neurons in the next layer – this is called a\nfully-connected layer or an aﬃne layer.\n4. While summing is the most common operation, other functions, such as a max, are also possible\n11\nWhile the brain metaphor is sexy and intriguing, it is also distracting and cumbersome\nto manipulate mathematically. We therefore switch to using more concise mathematic no-\ntation. The values of each row of neurons in the network can be thought of as a vector. In\nFigure 2 the input layer is a 4 dimensional vector (x), and the layer above it is a 6 dimen-\nsional vector (h1). The fully connected layer can be thought of as a linear transformation\nfrom 4 dimensions to 6 dimensions. A fully-connected layer implements a vector-matrix\nmultiplication, h = xW where the weight of the connection from the ith neuron in the\ninput row to the jth neuron in the output row is Wij.5 The values of h are then trans-\nformed by a non-linear function g that is applied to each value before being passed on to the\nnext input. The whole computation from input to output can be written as: (g(xW1))W2\nwhere W1 are the weights of the ﬁrst layer and W2 are the weights of the second one.\nIn Mathematical Notation\nFrom this point on, we will abandon the brain metaphor\nand describe networks exclusively in terms of vector-matrix operations.\nThe simplest neural network is the perceptron, which is a linear function of its inputs:\nNNPerceptron(x) = xW + b\nx ∈Rdin, W ∈Rdin×dout, b ∈Rdout\nW is the weight matrix, and b is a bias term.6 In order to go beyond linear functions, we\nintroduce a non-linear hidden layer (the network in Figure 2 has two such layers), resulting\nin the 1-layer Multi Layer Perceptron (MLP1). A one-layer feed-forward neural network\nhas the form:\nNNMLP1(x) = g(xW1 + b1)W2 + b2\nx ∈Rdin, W1 ∈Rdin×d1, b1 ∈Rd1, W2 ∈Rd1×d2, b2 ∈Rd2\nHere W1 and b1 are a matrix and a bias term for the ﬁrst linear transformation of the\ninput, g is a non-linear function that is applied element-wise (also called a non-linearity or\nan activation function), and W2 and b2 are the matrix and bias term for a second linear\ntransform.\nBreaking it down, xW1+b1 is a linear transformation of the input x from din dimensions\nto d1 dimensions. g is then applied to each of the d1 dimensions, and the matrix W2 together\nwith bias vector b2 are then used to transform the result into the d2 dimensional output\nvector. The non-linear activation function g has a crucial role in the network’s ability to\nrepresent complex functions. Without the non-linearity in g, the neural network can only\nrepresent linear transformations of the input.7\nWe can add additional linear-transformations and non-linearities, resulting in a 2-layer\nMLP (the network in Figure 2 is of this form):\nNNMLP2(x) = (g2(g1(xW1 + b1)W2 + b2))W3\n5. To see why this is the case, denote the weight of the ith input of the jth neuron in h as wij. The value\nof hj is then hj = P4\ni=1 xi · wij.\n6. The network in ﬁgure 2 does not include bias terms. A bias term can be added to a layer by adding to\nit an additional neuron that does not have any incoming connections, whose value is always 1.\n7. To see why, consider that a sequence of linear transformations is still a linear transformation.\n12\nIt is perhaps clearer to write deeper networks like this using intermediary variables:\nNNMLP2(x) =y\nh1 =g1(xW1 + b1)\nh2 =g2(h1W2 + b2)\ny =h2W3\nThe vector resulting from each linear transform is referred to as a layer. The outer-most\nlinear transform results in the output layer and the other linear transforms result in hidden\nlayers. Each hidden layer is followed by a non-linear activation. In some cases, such as in\nthe last layer of our example, the bias vectors are forced to 0 (“dropped”).\nLayers resulting from linear transformations are often referred to as fully connected, or\naﬃne. Other types of architectures exist. In particular, image recognition problems beneﬁt\nfrom convolutional and pooling layers. Such layers have uses also in language processing,\nand will be discussed in Section 9. Networks with more than one hidden layer are said to\nbe deep networks, hence the name deep learning.\nWhen describing a neural network, one should specify the dimensions of the layers and\nthe input. A layer will expect a din dimensional vector as its input, and transform it into a\ndout dimensional vector. The dimensionality of the layer is taken to be the dimensionality\nof its output. For a fully connected layer l(x) = xW + b with input dimensionality din and\noutput dimensionality dout, the dimensions of x is 1 × din, of W is din × dout and of b is\n1 × dout.\nThe output of the network is a dout dimensional vector. In case dout = 1, the network’s\noutput is a scalar. Such networks can be used for regression (or scoring) by considering\nthe value of the output, or for binary classiﬁcation by consulting the sign of the output.\nNetworks with dout = k > 1 can be used for k-class classiﬁcation, by associating each\ndimension with a class, and looking for the dimension with maximal value. Similarly, if\nthe output vector entries are positive and sum to one, the output can be interpreted as\na distribution over class assignments (such output normalization is typically achieved by\napplying a softmax transformation on the output layer, see Section 4.3).\nThe matrices and the bias terms that deﬁne the linear transformations are the parame-\nters of the network. It is common to refer to the collection of all parameters as θ. Together\nwith the input, the parameters determine the network’s output. The training algorithm is\nresponsible for setting their values such that the network’s predictions are correct. Training\nis discussed in Section 6.\n4.1 Representation Power\nIn terms of representation power, it was shown by (Hornik, Stinchcombe, & White, 1989;\nCybenko, 1989) that MLP1 is a universal approximator – it can approximate with any\ndesired non-zero amount of error a family of functions8 that include all continuous functions\n8. Speciﬁcally, a feed-forward network with linear output layer and at least one hidden layer with a “squash-\ning” activation function can approximate any Borel measurable function from one ﬁnite dimensional space\nto another.\n13\non a closed and bounded subset of Rn, and any function mapping from any ﬁnite dimensional\ndiscrete space to another. This may suggest there is no reason to go beyond MLP1 to more\ncomplex architectures. However, the theoretical result does not state how large the hidden\nlayer should be, nor does it say anything about the learnability of the neural network (it\nstates that a representation exists, but does not say how easy or hard it is to set the\nparameters based on training data and a speciﬁc learning algorithm).\nIt also does not\nguarantee that a training algorithm will ﬁnd the correct function generating our training\ndata. Since in practice we train neural networks on relatively small amounts of data, using\na combination of the backpropagation algorithm and variants of stochastic gradient descent,\nand use hidden layers of relatively modest sizes (up to several thousands), there is beneﬁt\nto be had in trying out more complex architectures than MLP1. In many cases, however,\nMLP1 does indeed provide very strong results. For further discussion on the representation\npower of feed-forward neural networks, see (Bengio et al., 2015, Section 6.5).\n4.2 Common Non-linearities\nThe non-linearity g can take many forms. There is currently no good theory as to which\nnon-linearity to apply in which conditions, and choosing the correct non-linearity for a\ngiven task is for the most part an empirical question. I will now go over the common non-\nlinearities from the literature: the sigmoid, tanh, hard tanh and the rectiﬁed linear unit\n(ReLU). Some NLP researchers also experimented with other forms of non-linearities such\nas cube and tanh-cube.\nSigmoid\nThe sigmoid activation function σ(x) = 1/(1 + e−x) is an S-shaped function,\ntransforming each value x into the range [0, 1].\nHyperbolic tangent (tanh)\nThe hyperbolic tangent tanh(x) = e2x−1\ne2x+1 activation func-\ntion is an S-shaped function, transforming the values x into the range [−1, 1].\nHard tanh\nThe hard-tanh activation function is an approximation of the tanh function\nwhich is faster to compute and take derivatives of:\nhardtanh(x) =\n\n\n\n\n\n−1\nx < −1\n1\nx > 1\nx\notherwise\nRectiﬁer (ReLU)\nThe Rectiﬁer activation function (Glorot, Bordes, & Bengio, 2011),\nalso known as the rectiﬁed linear unit is a very simple activation function that is easy to\nwork with and was shown many times to produce excellent results.9 The ReLU unit clips\neach value x < 0 at 0. Despite its simplicity, it performs well for many tasks, especially\nwhen combined with the dropout regularization technique (see Section 6.4).\n9. The technical advantages of the ReLU over the sigmoid and tanh activation functions is that it does not\ninvolve expensive-to-compute functions, and more importantly that it does not saturate. The sigmoid\nand tanh activation are capped at 1, and the gradients at this region of the functions are near zero,\ndriving the entire gradient near zero.\nThe ReLU activation does not have this problem, making it\nespecially suitable for networks with multiple layers, which are susceptible to the vanishing gradients\nproblem when trained with the saturating units.\n14\nReLU(x) = max(0, x) =\n(\n0\nx < 0\nx\notherwise\nAs a rule of thumb, ReLU units work better than tanh, and tanh works better than\nsigmoid.10\n4.3 Output Transformations\nIn many cases, the output layer vector is also transformed. A common transformation is\nthe softmax:\nx =x1, . . . , xk\nsoftmax(xi) =\nexi\nPk\nj=1 exj\nThe result is a vector of non-negative real numbers that sum to one, making it a discrete\nprobability distribution over k possible outcomes.\nThe softmax output transformation is used when we are interested in modeling a prob-\nability distribution over the possible output classes. To be eﬀective, it should be used in\nconjunction with a probabilistic training objective such as cross-entropy (see Section 4.5\nbelow).\nWhen the softmax transformation is applied to the output of a network without a hidden\nlayer, the result is the well known multinomial logistic regression model, also known as a\nmaximum-entropy classiﬁer.\n4.4 Embedding Layers\nUp until now, the discussion ignored the source of x, treating it as an arbitrary vector.\nIn an NLP application, x is usually composed of various embeddings vectors. We can be\nexplicit about the source of x, and include it in the network’s deﬁnition. We introduce c(·),\na function from core features to an input vector.\nIt is common for c to extract the embedding vector associated with each feature, and\nconcatenate them:\n10. In addition to these activation functions, recent works from the NLP community experiment with and\nreported success with other forms of non-linearities. The Cube activation function, g(x) = (x)3, was\nsuggested by (Chen & Manning, 2014), who found it to be more eﬀective than other non-linearities in\na feed-forward network that was used to predict the actions in a greedy transition-based dependency\nparser. The tanh cube activation function g(x) = tanh((x)3 + x) was proposed by (Pei et al., 2015),\nwho found it to be more eﬀective than other non-linearities in a feed-forward network that was used as\na component in a structured-prediction graph-based dependency parser.\nThe cube and tanh-cube activation functions are motivated by the desire to better capture interac-\ntions between diﬀerent features. While these activation functions are reported to improve performance\nin certain situations, their general applicability is still to be determined.\n15\nx = c(f1, f2, f3) =[v(f1); v(f2); v(f3)]\nNNMLP1(x) =NNMLP1(c(f1, f2, f3))\n=NNMLP1([v(f1); v(f2); v(f3)])\n=(g([v(f1); v(f2); v(f3)]W1 + b1))W2 + b2\nAnother common choice is for c to sum the embedding vectors (this assumes the em-\nbedding vectors all share the same dimensionality):\nx = c(f1, f2, f3) =v(f1) + v(f2) + v(f3)\nNNMLP1(x) =NNMLP1(c(f1, f2, f3))\n=NNMLP1(v(f1) + v(f2) + v(f3))\n=(g((v(f1) + v(f2) + v(f3))W1 + b1))W2 + b2\nThe form of c is an essential part of the network’s design. In many papers, it is common\nto refer to c as part of the network, and likewise treat the word embeddings v(fi) as resulting\nfrom an “embedding layer” or “lookup layer”. Consider a vocabulary of |V | words, each\nembedded as a d dimensional vector. The collection of vectors can then be thought of as a\n|V | × d embedding matrix E in which each row corresponds to an embedded feature. Let\nfi be a |V |-dimensional vector, which is all zeros except from one index, corresponding to\nthe value of the ith feature, in which the value is 1 (this is called a one-hot vector). The\nmultiplication fiE will then “select” the corresponding row of E. Thus, v(fi) can be deﬁned\nin terms of E and fi:\nv(fi) = fiE\nAnd similarly:\nCBOW(f1, ..., fk) =\nk\nX\ni=1\n(fiE) = (\nk\nX\ni=1\nfi)E\nThe input to the network is then considered to be a collection of one-hot vectors. While\nthis is elegant and well deﬁned mathematically, an eﬃcient implementation typically involves\na hash-based data structure mapping features to their corresponding embedding vectors,\nwithout going through the one-hot representation.\nIn this tutorial, we take c to be separate from the network architecture: the network’s\ninputs are always dense real-valued input vectors, and c is applied before the input is passed\nthe network, similar to a “feature function” in the familiar linear-models terminology. How-\never, when training a network, the input vector x does remember how it was constructed,\nand can propagate error gradients back to its component embedding vectors, as appropriate.\nA note on notation\nWhen describing network layers that get concatenated vectors x,\ny and z as input, some authors use explicit concatenation ([x; y; z]W + b) while others use\nan aﬃne transformation (xU + yV + zW + b). If the weight matrices U, V, W in the\naﬃne transformation are diﬀerent than one another, the two notations are equivalent.\n16\nA note on sparse vs. dense features\nConsider a network which uses a “traditional”\nsparse representation for its input vectors, and no embedding layer. Assuming the set of all\navailable features is V and we have k “on” features f1, . . . , fk, fi ∈V , the network’s input\nis:\nx =\nk\nX\ni=1\nfi\nx ∈N|V |\n+\nand so the ﬁrst layer (ignoring the non-linear activation) is:\nxW + b = (\nk\nX\ni=1\nfi)W\nW ∈R|V |×d,\nb ∈Rd\nThis layer selects rows of W corresponding to the input features in x and sums them,\nthen adding a bias term. This is very similar to an embedding layer that produces a CBOW\nrepresentation over the features, where the matrix W acts as the embedding matrix. The\nmain diﬀerence is the introduction of the bias vector b, and the fact that the embedding\nlayer typically does not undergo a non-linear activation but rather passed on directly to the\nﬁrst layer. Another diﬀerence is that this scenario forces each feature to receive a separate\nvector (row in W) while the embedding layer provides more ﬂexibility, allowing for example\nfor the features “next word is dog” and “previous word is dog” to share the same vector.\nHowever, these diﬀerences are small and subtle. When it comes to multi-layer feed-forward\nnetworks, the diﬀerence between dense and sparse inputs is smaller than it may seem at\nﬁrst sight.\n4.5 Loss Functions\nWhen training a neural network (more on training in Section 6 below), much like when\ntraining a linear classiﬁer, one deﬁnes a loss function L(ˆy, y), stating the loss of predicting\nˆy when the true output is y. The training objective is then to minimize the loss across the\ndiﬀerent training examples. The loss L(ˆy, y) assigns a numerical score (a scalar) for the\nnetwork’s output ˆy given the true expected output y.11 The loss is always positive, and\nshould be zero only for cases where the network’s output is correct.\nThe parameters of the network (the matrices Wi, the biases bi and commonly the em-\nbeddings E) are then set in order to minimize the loss L over the training examples (usually,\nit is the sum of the losses over the diﬀerent training examples that is being minimized).\nThe loss can be an arbitrary function mapping two vectors to a scalar. For practical\npurposes of optimization, we restrict ourselves to functions for which we can easily compute\ngradients (or sub-gradients). In most cases, it is suﬃcient and advisable to rely on a common\nloss function rather than deﬁning your own. For a detailed discussion on loss functions for\nneural networks see (LeCun, Chopra, Hadsell, Ranzato, & Huang, 2006; LeCun & Huang,\n2005; Bengio et al., 2015). We now discuss some loss functions that are commonly used in\nneural networks for NLP.\n11. In our notation, both the model’s output and the expected output are vectors, while in many cases it is\nmore natural to think of the expected output as a scalar (class assignment). In such cases, y is simply\nthe corresponding one-hot vector.\n17\nHinge (binary)\nFor binary classiﬁcation problems, the network’s output is a single scalar\nˆy and the intended output y is in {+1, −1}.\nThe classiﬁcation rule is sign(ˆy), and a\nclassiﬁcation is considered correct if y · ˆy > 0, meaning that y and ˆy share the same sign.\nThe hinge loss, also known as margin loss or SVM loss, is deﬁned as:\nLhinge(binary)(ˆy, y) = max(0, 1 −y · ˆy)\nThe loss is 0 when y and ˆy share the same sign and |ˆy| ≥1. Otherwise, the loss is linear.\nIn other words, the binary hinge loss attempts to achieve a correct classiﬁcation, with a\nmargin of at least 1.\nHinge (multiclass)\nThe hinge loss was extended to the multiclass setting by Crammer\nand Singer (2002). Let ˆy = ˆy1, . . . , ˆyn be the network’s output vector, and y be the one-hot\nvector for the correct output class.\nThe classiﬁcation rule is deﬁned as selecting the class with the highest score:\nprediction = arg max\ni\nˆyi\n,\nDenote by t = arg maxi yi the correct class, and by k = arg maxi̸=t ˆyi the highest scoring\nclass such that k ̸= t. The multiclass hinge loss is deﬁned as:\nLhinge(multiclass)(ˆy, y) = max(0, 1 −(ˆyt −ˆyk))\nThe multiclass hinge loss attempts to score the correct class above all other classes with a\nmargin of at least 1.\nBoth the binary and multiclass hinge losses are intended to be used with a linear output\nlayer. The hinge losses are useful whenever we require a hard decision rule, and do not\nattempt to model class membership probability.\nLog loss\nThe log loss is a common variation of the hinge loss, which can be seen as a\n“soft” version of the hinge loss with an inﬁnite margin (LeCun et al., 2006).\nLlog(ˆy, y) = log(1 + exp(−(ˆyt −ˆyk))\nCategorical cross-entropy loss\nThe categorical cross-entropy loss (also referred to as\nnegative log likelihood) is used when a probabilistic interpretation of the scores is desired.\nLet y = y1, . . . , yn be a vector representing the true multinomial distribution over the\nlabels 1, . . . , n, and let ˆy = ˆy1, . . . , ˆyn be the network’s output, which was transformed by the\nsoftmax activation function, and represent the class membership conditional distribution\nˆyi = P(y = i|x). The categorical cross entropy loss measures the dissimilarity between\nthe true label distribution y and the predicted label distribution ˆy, and is deﬁned as cross\nentropy:\nLcross−entropy(ˆy, y) = −\nX\ni\nyi log(ˆyi)\n18\nFor hard classiﬁcation problems in which each training example has a single correct\nclass assignment, y is a one-hot vector representing the true class. In such cases, the cross\nentropy can be simpliﬁed to:\nLcross−entropy(hard classiﬁcation)(ˆy, y) = −log(ˆyt)\nwhere t is the correct class assignment. This attempts to set the probability mass assigned\nto the correct class t to 1. Because the scores ˆy have been transformed using the softmax\nfunction and represent a conditional distribution, increasing the mass assigned to the correct\nclass means decreasing the mass assigned to all the other classes.\nThe cross-entropy loss is very common in the neural networks literature, and produces\na multi-class classiﬁer which does not only predict the one-best class label but but also\npredicts a distribution over the possible labels. When using the cross-entropy loss, it is\nassumed that the network’s output is transformed using the softmax transformation.\nRanking losses\nIn some settings, we are not given supervision in term of labels, but\nrather as pairs of correct and incorrect items x and x′, and our goal is to score correct\nitems above incorrect ones.\nSuch training situations arise when we have only positive\nexamples, and generate negative examples by corrupting a positive example. A useful loss\nin such scenarios is the margin-based ranking loss, deﬁned for a pair of correct and incorrect\nexamples:\nLranking(margin)(x, x′) = max(0, 1 −(NN(x) −NN(x′)))\nwhere NN(x) is the score assigned by the network for input vector x. The objective is to\nscore (rank) correct inputs over incorrect ones with a margin of at least 1.\nA common variation is to use the log version of the ranking loss:\nLranking(log)(x, x′) = log(1 + exp(−(NN(x) −NN(x′))))\nExamples using the ranking hinge loss in language tasks include training with the aux-\niliary tasks used for deriving pre-trained word embeddings (see section 5), in which we are\ngiven a correct word sequence and a corrupted word sequence, and our goal is to score\nthe correct sequence above the corrupt one (Collobert & Weston, 2008). Similarly, Van\nde Cruys (2014) used the ranking loss in a selectional-preferences task, in which the net-\nwork was trained to rank correct verb-object pairs above incorrect, automatically derived\nones, and (Weston, Bordes, Yakhnenko, & Usunier, 2013) trained a model to score correct\n(head,relation,trail) triplets above corrupted ones in an information-extraction setting. An\nexample of using the ranking log loss can be found in (Gao et al., 2014). A variation of the\nranking log loss allowing for a diﬀerent margin for the negative and positive class is given\nin (dos Santos et al., 2015).\n19\n5. Word Embeddings\nA main component of the neural-network approach is the use of embeddings – representing\neach feature as a vector in a low dimensional space. But where do the vectors come from?\nThis section will survey the common approaches.\n5.1 Random Initialization\nWhen enough supervised training data is available, one can just treat the feature embeddings\nthe same as the other model parameters: initialize the embedding vectors to random values,\nand let the network-training procedure tune them into “good” vectors.\nSome care has to be taken in the way the random initialization is performed. The method\nused by the eﬀective word2vec implementation (Mikolov et al., 2013; Mikolov, Sutskever,\nChen, Corrado, & Dean, 2013) is to initialize the word vectors to uniformly sampled random\nnumbers in the range [−1\n2d, 1\n2d] where d is the number of dimensions. Another option is to\nuse xavier initialization (see Section 6.3) and initialize with uniformly sampled values from\nh\n−\n√\n6\n√\nd,\n√\n6\n√\nd\ni\n.\nIn practice, one will often use the random initialization approach to initialize the em-\nbedding vectors of commonly occurring features, such as part-of-speech tags or individual\nletters, while using some form of supervised or unsupervised pre-training to initialize the\npotentially rare features, such as features for individual words. The pre-trained vectors can\nthen either be treated as ﬁxed during the network training process, or, more commonly,\ntreated like the randomly-initialized vectors and further tuned to the task at hand.\n5.2 Supervised Task-speciﬁc Pre-training\nIf we are interested in task A, for which we only have a limited amount of labeled data (for\nexample, syntactic parsing), but there is an auxiliary task B (say, part-of-speech tagging)\nfor which we have much more labeled data, we may want to pre-train our word vectors so\nthat they perform well as predictors for task B, and then use the trained vectors for training\ntask A. In this way, we can utilize the larger amounts of labeled data we have for task B.\nWhen training task A we can either treat the pre-trained vectors as ﬁxed, or tune them\nfurther for task A. Another option is to train jointly for both objectives, see Section 7 for\nmore details.\n5.3 Unsupervised Pre-training\nThe common case is that we do not have an auxiliary task with large enough amounts of\nannotated data (or maybe we want to help bootstrap the auxiliary task training with better\nvectors). In such cases, we resort to “unsupervised” methods, which can be trained on huge\namounts of unannotated text.\nThe techniques for training the word vectors are essentially those of supervised learning,\nbut instead of supervision for the task that we care about, we instead create practically\n20\nunlimited number of supervised training instances from raw text, hoping that the tasks\nthat we created will match (or be close enough to) the ﬁnal task we care about.12\nThe key idea behind the unsupervised approaches is that one would like the embedding\nvectors of “similar” words to have similar vectors. While word similarity is hard to deﬁne\nand is usually very task-dependent, the current approaches derive from the distributional\nhypothesis (Harris, 1954), stating that words are similar if they appear in similar contexts.\nThe diﬀerent methods all create supervised training instances in which the goal is to either\npredict the word from its context, or predict the context from the word.\nAn important beneﬁt of training word embeddings on large amounts of unannotated\ndata is that it provides vector representations for words that do not appear in the super-\nvised training set. Ideally, the representations for these words will be similar to those of\nrelated words that do appear in the training set, allowing the model to generalize better on\nunseen events. It is thus desired that the similarity between word vectors learned by the un-\nsupervised algorithm captures the same aspects of similarity that are useful for performing\nthe intended task of the network.\nCommon unsupervised word-embedding algorithms include word2vec 13 (Mikolov et al.,\n2013, 2013), GloVe (Pennington, Socher, & Manning, 2014) and the Collobert and Weston\n(2008, 2011) embeddings algorithm. These models are inspired by neural networks and\nare based on stochastic gradient training. However, they are deeply connected to another\nfamily of algorithms which evolved in the NLP and IR communities, and that are based on\nmatrix factorization (see (Levy & Goldberg, 2014b; Levy et al., 2015) for a discussion).\nArguably, the choice of auxiliary problem (what is being predicted, based on what kind\nof context) aﬀects the resulting vectors much more than the learning method that is being\nused to train them. We thus focus on the diﬀerent choices of auxiliary problems that are\navailable, and only skim over the details of the training methods. Several software packages\nfor deriving word vectors are available, including word2vec14 and Gensim15 implementing\nthe word2vec models with word-windows based contexts, word2vecf16 which is a modiﬁed\nversion of word2vec allowing the use of arbitrary contexts, and GloVe17 implementing the\nGloVe model. Many pre-trained word vectors are also available for download on the web.\nWhile beyond the scope of this tutorial, it is worth noting that the word embeddings\nderived by unsupervised training algorithms have a wide range of applications in NLP\nbeyond using them for initializing the word-embeddings layer of a neural-network model.\n5.4 Training Objectives\nGiven a word w and its context c, diﬀerent algorithms formulate diﬀerent auxiliary tasks.\nIn all cases, each word is represented as a d-dimensional vector which is initialized to a\nrandom value. Training the model to perform the auxiliary tasks well will result in good\n12. The interpretation of creating auxiliary problems from raw text is inspired by Ando and Zhang (Ando\n& Zhang, 2005a, 2005b).\n13. While often treated as a single algorithm, word2vec is actually a software package including various\ntraining objectives, optimization methods and other hyperparameters. See (Rong, 2014; Levy, Goldberg,\n& Dagan, 2015) for a discussion.\n14. https://code.google.com/p/word2vec/\n15. https://radimrehurek.com/gensim/\n16. https://bitbucket.org/yoavgo/word2vecf\n17. http://nlp.stanford.edu/projects/glove/\n21\nword embeddings for relating the words to the contexts, which in turn will result in the\nembedding vectors for similar words to be similar to each other.\nLanguage-modeling inspired approaches such as those taken by (Mikolov et al., 2013;\nMnih & Kavukcuoglu, 2013) as well as GloVe (Pennington et al., 2014) use auxiliary tasks\nin which the goal is to predict the word given its context. This is posed in a probabilistic\nsetup, trying to model the conditional probability P(w|c).\nOther approaches reduce the problem to that of binary classiﬁcation. In addition to\nthe set D of observed word-context pairs, a set ¯D is created from random words and\ncontext pairings.\nThe binary classiﬁcation problem is then: does the given (w, c) pair\ncome from D or not?\nThe approaches diﬀer in how the set ¯D is constructed, what is\nthe structure of the classiﬁer, and what is the objective being optimized. Collobert and\nWeston (2008, 2011) take a margin-based binary ranking approach, training a feed-forward\nneural network to score correct (w, c) pairs over incorrect ones. Mikolov et al (2013, 2014)\ntake instead a probabilistic version, training a log-bilinear model to predict the probability\nP((w, c) ∈D|w, c) that the pair come from the corpus rather than the random sample.\n5.5 The Choice of Contexts\nIn most cases, the contexts of a word are taken to be other words that appear in its\nsurrounding, either in a short window around it, or within the same sentence, paragraph\nor document. In some cases the text is automatically parsed by a syntactic parser, and\nthe contexts are derived from the syntactic neighbourhood induced by the automatic parse\ntrees. Sometimes, the deﬁnitions of words and context change to include also parts of words,\nsuch as preﬁxes or suﬃxes.\nNeural word embeddings originated from the world of language modeling, in which a\nnetwork is trained to predict the next word based on a sequence of preceding words (Bengio\net al., 2003). There, the text is used to create auxiliary tasks in which the aim is to predict\na word based on a context the k previous words. While training for the language modeling\nauxiliary prediction problems indeed produce useful embeddings, this approach is needlessly\nrestricted by the constraints of the language modeling task, in which one is allowed to look\nonly at the previous words. If we do not care about language modeling but only about the\nresulting embeddings, we may do better by ignoring this constraint and taking the context\nto be a symmetric window around the focus word.\n5.5.1 Window Approach\nThe most common approach is a sliding window approach, in which auxiliary tasks are\ncreated by looking at a sequence of 2k +1 words. The middle word is callled the focus word\nand the k words to each side are the contexts. Then, either a single task is created in which\nthe goal is to predict the focus word based on all of the context words (represented either\nusing CBOW (Mikolov et al., 2013) or vector concatenation (Collobert & Weston, 2008)),\nor 2k distinct tasks are created, each pairing the focus word with a diﬀerent context word.\nThe 2k tasks approach, popularized by (Mikolov et al., 2013) is referred to as a skip-gram\nmodel. Skip-gram based approaches are shown to be robust and eﬃcient to train (Mikolov\net al., 2013; Pennington et al., 2014), and often produce state of the art results.\n22\nEﬀect of Window Size\nThe size of the sliding window has a strong eﬀect on the re-\nsulting vector similarities. Larger windows tend to produce more topical similarities (i.e.\n“dog”, “bark” and “leash” will be grouped together, as well as “walked”, “run” and “walk-\ning”), while smaller windows tend to produce more functional and syntactic similarities (i.e.\n“Poodle”, “Pitbull”, “Rottweiler”, or “walking”,“running”,“approaching”).\nPositional Windows\nWhen using the CBOW or skip-gram context representations, all\nthe diﬀerent context words within the window are treated equally. There is no distinction\nbetween context words that are close to the focus words and those that are farther from\nit, and likewise there is no distinction between context words that appear before the focus\nwords to context words that appear after it. Such information can easily be factored in by\nusing positional contexts: indicating for each context word also its relative position to the\nfocus words (i.e. instead of the context word being “the” it becomes “the:+2”, indicating\nthe word appears two positions to the right of the focus word). The use of positional context\ntogether with smaller windows tend to produce similarities that are more syntactic, with\na strong tendency of grouping together words that share a part of speech, as well as being\nfunctionally similar in terms of their semantics. Positional vectors were shown by (Ling,\nDyer, Black, & Trancoso, 2015a) to be more eﬀective than window-based vectors when used\nto initialize networks for part-of-speech tagging and syntactic dependency parsing.\nVariants\nMany variants on the window approach are possible. One may lemmatize words\nbefore learning, apply text normalization, ﬁlter too short or too long sentences, or remove\ncapitalization (see, e.g., the pre-processing steps described in (dos Santos & Gatti, 2014).\nOne may sub-sample part of the corpus, skipping with some probability the creation of tasks\nfrom windows that have too common or too rare focus words. The window size may be\ndynamic, using a diﬀerent window size at each turn. One may weigh the diﬀerent positions\nin the window diﬀerently, focusing more on trying to predict correctly close word-context\npairs than further away ones. Each of these choices will eﬀect the resulting vectors. Some\nof these hyperparameters (and others) are discussed in (Levy et al., 2015).\n5.5.2 Sentences, Paragraphs or Documents\nUsing a skip-grams (or CBOW) approach, one can consider the contexts of a word to be all\nthe other words that appear with it in the same sentence, paragraph or document. This is\nequivalent to using very large window sizes, and is expected to result in word vectors that\ncapture topical similarity (words from the same topic, i.e. words that one would expect to\nappear in the same document, are likely to receive similar vectors).\n5.5.3 Syntactic Window\nSome work replace the linear context within a sentence with a syntactic one (Levy &\nGoldberg, 2014a; Bansal, Gimpel, & Livescu, 2014).\nThe text is automatically parsed\nusing a dependency parser, and the context of a word is taken to be the words that are\nin its proximity in the parse tree, together with the syntactic relation by which they are\nconnected. Such approaches produce highly functional similarities, grouping together words\nthan can ﬁll the same role in a sentence (e.g. colors, names of schools, verbs of movement).\n23\nThe grouping is also syntactic, grouping together words that share an inﬂection (Levy &\nGoldberg, 2014a).\n5.5.4 Multilingual\nAnother option is using multilingual, translation based contexts (Hermann & Blunsom,\n2014; Faruqui & Dyer, 2014). For example, given a large amount of sentence-aligned parallel\ntext, one can run a bilingual alignment model such as the IBM model 1 or model 2 (i.e.\nusing the GIZA++ software), and then use the produced alignments to derive word contexts.\nHere, the context of a word instance are the foreign language words that are aligned to it.\nSuch alignments tend to result in synonym words receiving similar vectors. Some authors\nwork instead on the sentence alignment level, without relying on word alignments.\nAn\nappealing method is to mix a monolingual window-based approach with a multilingual\napproach, creating both kinds of auxiliary tasks. This is likely to produce vectors that are\nsimilar to the window-based approach, but reducing the somewhat undesired eﬀect of the\nwindow-based approach in which antonyms (e.g. hot and cold, high and low) tend to receive\nsimilar vectors (Faruqui & Dyer, 2014).\n5.5.5 Character-based and Sub-word Representations\nAn interesting line of work attempts to derive the vector representation of a word from the\ncharacters that compose it. Such approaches are likely to be particularly useful for tasks\nwhich are syntactic in nature, as the character patterns within words are strongly related\nto their syntactic function. These approaches also have the beneﬁt of producing very small\nmodel sizes (only one vector for each character in the alphabet together with a handful of\nsmall matrices needs to be stored), and being able to provide an embedding vector for every\nword that may be encountered. dos Santos and Gatti (2014) and dos Santos and Zadrozny\n(2014) model the embedding of a word using a convolutional network (see Section 9) over\nthe characters. Ling et al (2015b) model the embedding of a word using the concatenation\nof the ﬁnal states of two RNN (LSTM) encoders (Section 10), one reading the characters\nfrom left to right, and the other from right to left. Both produce very strong results for\npart-of-speech tagging. The work of Ballesteros et al (2015) show that the two-LSTMs\nencoding of (Ling et al., 2015b) is beneﬁcial also for representing words in dependency\nparsing of morphologically rich languages.\nDeriving representations of words from the representations of their characters is moti-\nvated by the unknown words problem – what do you do when you encounter a word for\nwhich you do not have an embedding vector? Working on the level of characters alleviates\nthis problem to a large extent, as the vocabulary of possible characters is much smaller\nthan the vocabulary of possible words. However, working on the character level is very\nchallenging, as the relationship between form (characters) and function (syntax, semantics)\nin language is quite loose. Restricting oneself to stay on the character level may be an\nunnecessarily hard constraint. Some researchers propose a middle-ground, in which a word\nis represented as a combination of a vector for the word itself with vectors of sub-word\nunits that comprise it. The sub-word embeddings then help in sharing information between\ndiﬀerent words with similar forms, as well as allowing back-oﬀto the subword level when\nthe word is not observed. At the same time, the models are not forced to rely solely on\n24\nform when enough observations of the word are available. Botha and Blunsom (2014) sug-\ngest to model the embedding vector of a word as a sum of the word-speciﬁc vector if such\nvector is available, with vectors for the diﬀerent morphological components that comprise\nit (the components are derived using Morfessor (Creutz & Lagus, 2007), an unsupervised\nmorphological segmentation method). Gao et al (Gao et al., 2014) suggest using as core\nfeatures not only the word form itself but also a unique feature (hence a unique embedding\nvector) for each of the letter-trigrams in the word.\n25\n6. Neural Network Training\nNeural network training is done by trying to minimize a loss function over a training set,\nusing a gradient-based method. Roughly speaking, all training methods work by repeatedly\ncomputing an estimate of the error over the dataset, computing the gradient with respect\nto the error, and then moving the parameters in the direction of the gradient. Models diﬀer\nin how the error estimate is computed, and how “moving in the direction of the gradient”\nis deﬁned. We describe the basic algorithm, stochastic gradient descent (SGD), and then\nbrieﬂy mention the other approaches with pointers for further reading. Gradient calculation\nis central to the approach. Gradients can be eﬃciently and automatically computed using\nreverse mode diﬀerentiation on a computation graph – a general algorithmic framework for\nautomatically computing the gradient of any network and loss function.\n6.1 Stochastic Gradient Training\nThe common approach for training neural networks is using the stochastic gradient descent\n(SGD) algorithm (Bottou, 2012; LeCun, Bottou, Orr, & Muller, 1998a) or a variant of it.\nSGD is a general optimization algorithm. It receives a function f parameterized by θ, a\nloss function, and desired input and output pairs. It then attempts to set the parameters θ\nsuch that the loss of f with respect to the training examples is small. The algorithm works\nas follows:\nAlgorithm 1 Online Stochastic Gradient Descent Training\n1: Input: Function f(x; θ) parameterized with parameters θ.\n2: Input: Training set of inputs x1, . . . , xn and outputs y1, . . . , yn.\n3: Input: Loss function L.\n4: while stopping criteria not met do\n5:\nSample a training example xi, yi\n6:\nCompute the loss L(f(xi; θ), yi)\n7:\nˆg ←gradients of L(f(xi; θ), yi) w.r.t θ\n8:\nθ ←θ + ηkˆg\n9: return θ\nThe goal of the algorithm is to set the parameters θ so as to minimize the total loss\nPn\ni=1 L(f(xi; θ), yi) over the training set. It works by repeatedly sampling a training exam-\nple and computing the gradient of the error on the example with respect to the parameters\nθ (line 7) – the input and expected output are assumed to be ﬁxed, and the loss is treated\nas a function of the parameters θ. The parameters θ are then updated in the direction of\nthe gradient, scaled by a learning rate ηk (line 8). For further discussion on setting the\nlearning rate, see Section 6.3.\nNote that the error calculated in line 6 is based on a single training example, and is thus\njust a rough estimate of the corpus-wide loss that we are aiming to minimize. The noise in\nthe loss computation may result in inaccurate gradients. A common way of reducing this\nnoise is to estimate the error and the gradients based on a sample of m examples. This\ngives rise to the minibatch SGD algorithm:\n26\nAlgorithm 2 Minibatch Stochastic Gradient Descent Training\n1: Input: Function f(x; θ) parameterized with parameters θ.\n2: Input: Training set of inputs x1, . . . , xn and outputs y1, . . . , yn.\n3: Input: Loss function L.\n4: while stopping criteria not met do\n5:\nSample a minibatch of m examples {(x1, y1), . . . , (xm, ym)}\n6:\nˆg ←0\n7:\nfor i = 1 to m do\n8:\nCompute the loss L(f(xi; θ), yi)\n9:\nˆg ←ˆg + gradients of\n1\nmL(f(xi; θ), yi) w.r.t θ\n10:\nθ ←θ + ηkˆg\n11: return θ\nIn lines 6 – 9 the algorithm estimates the gradient of the corpus loss based on the\nminibatch.\nAfter the loop, ˆg contains the gradient estimate, and the parameters θ are\nupdated toward ˆg. The minibatch size can vary in size from m = 1 to m = n. Higher\nvalues provide better estimates of the corpus-wide gradients, while smaller values allow\nmore updates and in turn faster convergence. Besides the improved accuracy of the gradients\nestimation, the minibatch algorithm provides opportunities for improved training eﬃciency.\nFor modest sizes of m, some computing architectures (i.e. GPUs) allow an eﬃcient parallel\nimplementation of the computation in lines 6–9. With a small enough learning rate, SGD is\nguaranteed to converge to a global optimum if the function is convex. However, it can also\nbe used to optimize non-convex functions such as neural-network. While there are no longer\nguarantees of ﬁnding a global optimum, the algorithm proved to be robust and performs\nwell in practice.\nWhen training a neural network, the parameterized function f is the neural network,\nand the parameters θ are the layer-transfer matrices, bias terms, embedding matrices and\nso on. The gradient computation is a key step in the SGD algorithm, as well as in all\nother neural network training algorithms.\nThe question is, then, how to compute the\ngradients of the network’s error with respect to the parameters. Fortunately, there is an easy\nsolution in the form of the backpropagation algorithm (Rumelhart, Hinton, & Williams, 1986;\nLecun, Bottou, Bengio, & Haﬀner, 1998b). The backpropagation algorithm is a fancy name\nfor methodologically computing the derivatives of a complex expression using the chain-\nrule, while caching intermediary results. More generally, the backpropagation algorithm\nis a special case of the reverse-mode automatic diﬀerentiation algorithm (Neidinger, 2010,\nSection 7), (Baydin, Pearlmutter, Radul, & Siskind, 2015; Bengio, 2012).The following\nsection describes reverse mode automatic diﬀerentiation in the context of the computation\ngraph abstraction.\nBeyond SGD\nWhile the SGD algorithm can and often does produce good results, more\nadvanced algorithms are also available. The SGD+Momentum (Polyak, 1964) and Nesterov\nMomentum (Sutskever, Martens, Dahl, & Hinton, 2013) algorithms are variants of SGD in\nwhich previous gradients are accumulated and aﬀect the current update. Adaptive learning\nrate algorithms including AdaGrad (Duchi, Hazan, & Singer, 2011), AdaDelta (Zeiler, 2012),\n27\nRMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2014) are designed to\nselect the learning rate for each minibatch, sometimes on a per-coordinate basis, potentially\nalleviating the need of ﬁddling with learning rate scheduling. For details of these algorithms,\nsee the original papers or (Bengio et al., 2015, Sections 8.3, 8.4). As many neural-network\nsoftware frameworks provide implementations of these algorithms, it is easy and sometimes\nworthwhile to try out diﬀerent variants.\n6.2 The Computation Graph Abstraction\nWhile one can compute the gradients of the various parameters of a network by hand and\nimplement them in code, this procedure is cumbersome and error prone. For most pur-\nposes, it is preferable to use automatic tools for gradient computation (Bengio, 2012). The\ncomputation-graph abstraction allows us to easily construct arbitrary networks, evaluate\ntheir predictions for given inputs (forward pass), and compute gradients for their parameters\nwith respect to arbitrary scalar losses (backward pass).\nA computation graph is a representation of an arbitrary mathematical computation as\na graph. It is a directed acyclic graph (DAG) in which nodes correspond to mathematical\noperations or (bound) variables and edges correspond to the ﬂow of intermediary values\nbetween the nodes. The graph structure deﬁnes the order of the computation in terms of\nthe dependencies between the diﬀerent components. The graph is a DAG and not a tree, as\nthe result of one operation can be the input of several continuations. Consider for example\na graph for the computation of (a ∗b + 1) ∗(a ∗b + 2):\na\nb\n1\n2\n*\n+\n+\n*\nThe computation of a∗b is shared. We restrict ourselves to the case where the computation\ngraph is connected.\nSince a neural network is essentially a mathematical expression, it can be represented\nas a computation graph.\nFor example, Figure 3a presents the computation graph for a 1-layer MLP with a soft-\nmax output transformation. In our notation, oval nodes represent mathematical operations\nor functions, and shaded rectangle nodes represent parameters (bound variables). Network\ninputs are treated as constants, and drawn without a surrounding node. Input and param-\neter nodes have no incoming arcs, and output nodes have no outgoing arcs. The output of\neach node is a matrix, the dimensionality of which is indicated above the node.\nThis graph is incomplete: without specifying the inputs, we cannot compute an output.\nFigure 3b shows a complete graph for an MLP that takes three words as inputs, and predicts\nthe distribution over part-of-speech tags for the third word. This graph can be used for\nprediction, but not for training, as the output is a vector (not a scalar) and the graph does\nnot take into account the correct answer or the loss term. Finally, the graph in 3c shows the\ncomputation graph for a speciﬁc training example, in which the inputs are the (embeddings\n28\nx\n1 × 150\nW1\n150 × 20\nMUL\n1 × 20\nADD\n1 × 20\nb1\n1 × 20\ntanh\n1 × 20\nW2\n20 × 17\nb2\n1 × 17\nMUL\n1 × 17\nADD\n1 × 17\nsoftmax\n1 × 17\n(a)\nconcat\n1 × 150\nlookup\n1 × 50\nlookup\n1 × 50\nlookup\n1 × 50\n“the”\n“black”\n“dog”\nE\n|V | × 50\nW1\n150 × 20\nMUL\n1 × 20\nADD\n1 × 20\nb1\n1 × 20\ntanh\n1 × 20\nW2\n20 × 17\nb2\n1 × 17\nMUL\n1 × 17\nADD\n1 × 17\nsoftmax\n1 × 17\n(b)\nconcat\n1 × 150\nlookup\n1 × 50\nlookup\n1 × 50\nlookup\n1 × 50\n“the”\n“black”\n“dog”\nE\n|V | × 50\nW1\n150 × 20\nMUL\n1 × 20\nADD\n1 × 20\nb1\n1 × 20\ntanh\n1 × 20\nW2\n20 × 17\nb2\n1 × 17\nMUL\n1 × 17\nADD\n1 × 17\nsoftmax\n1 × 17\npick\n1 × 1\n5\nlog\n1 × 1\nneg\n1 × 1\n(c)\nFigure 3: Computation Graph for MLP1. (a) Graph with unbound input. (b) Graph\nwith concrete input. (c) Graph with concrete input, expected output, and loss\nnode.\nof) the words “the”, “black”, “dog”, and the expected output is “NOUN” (whose index is\n5).\nOnce the graph is built, it is straightforward to run either a forward computation (com-\npute the result of the computation) or a backward computation (computing the gradients),\nas we show below. Constructing the graphs may look daunting, but is actually very easy\nusing dedicated software libraries and APIs.\nForward Computation\nThe forward pass computes the outputs of the nodes in the\ngraph. Since each node’s output depends only on itself and on its incoming edges, it is\ntrivial to compute the outputs of all nodes by traversing the nodes in a topological order and\ncomputing the output of each node given the already computed outputs of its predecessors.\nMore formally, in a graph of N nodes, we associate each node with an index i according\nto their topological ordering. Let fi be the function computed by node i (e.g. multiplication.\naddition, . . . ). Let π(i) be the parent nodes of node i, and π−1(i) = {j | i ∈π(j)} the\nchildren nodes of node i (these are the arguments of fi). Denote by v(i) the output of node\n29\ni, that is, the application of fi to the output values of its arguments π−1(i). For variable\nand input nodes, fi is a constant function and π−1(i) is empty. The Forward algorithm\ncomputes the values v(i) for all i ∈[1, N].\nAlgorithm 3 Computation Graph Forward Pass\n1: for i = 1 to N do\n2:\nLet a1, . . . , am = π−1(i)\n3:\nv(i) ←fi(v(a1), . . . , v(am))\nBackward Computation (Derivatives, Backprop)\nThe backward pass begins by des-\nignating a node N with scalar (1 × 1) output as a loss-node, and running forward computa-\ntion up to that node. The backward computation will computes the gradients with respect\nto that node’s value. Denote by d(i) the quantity ∂N\n∂i . The backpropagation algorithm is\nused to compute the values d(i) for all nodes i.\nThe backward pass ﬁlls a table d(i) as follows:\nAlgorithm 4 Computation Graph Backward Pass (Backpropagation)\n1: d(N) ←1\n2: for i = N-1 to 1 do\n3:\nd(i) ←P\nj∈π(i) d(j) · ∂fj\n∂i\nThe quantity ∂fj\n∂i\nis the partial derivative of fj(π−1(j)) w.r.t the argument i ∈π−1(j).\nThis value depends on the function fj and the values v(a1), . . . , v(am) (where a1, . . . , am =\nπ−1(j)) of its arguments, which were computed in the forward pass.\nThus, in order to deﬁne a new kind of node, one need to deﬁne two methods: one for\ncalculating the forward value v(i) based on the nodes inputs, and the another for calculating\n∂fi\n∂x for each x ∈π−1(i).\nFor further information on automatic diﬀerentiation see (Neidinger, 2010, Section 7),\n(Baydin et al., 2015).\nFor more in depth discussion of the backpropagation algorithm and\ncomputation graphs (also called ﬂow graphs) see (Bengio et al., 2015, Section 6.4), (Lecun\net al., 1998b; Bengio, 2012). For a popular yet technical presentation, see Chris Olah’s\ndescription at http://colah.github.io/posts/2015-08-Backprop/.\nSoftware\nSeveral software packages implement the computation-graph model, including\nTheano18, Chainer19, penne20 and CNN/pyCNN21. All these packages support all the es-\nsential components (node types) for deﬁning a wide range of neural network architectures,\ncovering the structures described in this tutorial and more. Graph creation is made almost\ntransparent by use of operator overloading. The framework deﬁnes a type for representing\ngraph nodes (commonly called expressions), methods for constructing nodes for inputs and\n18. http://deeplearning.net/software/theano/\n19. http://chainer.org\n20. https://bitbucket.org/ndnlp/penne\n21. https://github.com/clab/cnn\n30\nparameters, and a set of functions and mathematical operations that take expressions as\ninput and result in more complex expressions. For example, the python code for creating\nthe computation graph from Figure (3c) using the pyCNN framework is:\nfrom pycnn import *\n# model initialization.\nmodel = Model()\nmodel.add_parameters(\"W1\", (20,150))\nmodel.add_parameters(\"b1\", 20)\nmodel.add_parameters(\"W2\", (17,20))\nmodel.add_parameters(\"b2\", 17)\nmodel.add_lookup_parameters(\"words\", (100, 50))\n# Building the computation graph:\nrenew_cg() # create a new graph.\n# Wrap the model parameters as graph-nodes.\nW1 = parameter(model[\"W1\"])\nb1 = parameter(model[\"b1\"])\nW2 = parameter(model[\"W2\"])\nb2 = parameter(model[\"b2\"])\ndef get_index(x): return 1\n# Generate the embeddings layer.\nvthe\n= lookup(model[\"words\"], get_index(\"the\"))\nvblack = lookup(model[\"words\"], get_index(\"black\"))\nvdog\n= lookup(model[\"words\"], get_index(\"dog\"))\n# Connect the leaf nodes into a complete graph.\nx = concatenate([vthe, vblack, vdog])\noutput = softmax(W2*(tanh(W1*x)+b1)+b2)\nloss = -log(pick(output, 5))\nloss_value = loss.forward()\nloss.backward() # the gradient is computed\n# and stored in the corresponding\n# parameters.\nMost of the code involves various initializations: the ﬁrst block deﬁnes model parameters\nthat are be shared between diﬀerent computation graphs (recall that each graph corresponds\nto a speciﬁc training example). The second block turns the model parameters into the graph-\nnode (Expression) types. The third block retrieves the Expressions for the embeddings of the\ninput words. Finally, the fourth block is where the graph is created. Note how transparent\nthe graph creation is – there is an almost a one-to-one correspondence between creating\nthe graph and describing it mathematically. The last block shows a forward and backward\npass. The other software frameworks follow similar patterns.\nTheano involves an optimizing compiler for computation graphs, which is both a blessing\nand a curse. On the one hand, once compiled, large graphs can be run eﬃciently on either\nthe CPU or a GPU, making it ideal for large graphs with a ﬁxed structure, where only the\ninputs change between instances. However, the compilation step itself can be costly, and it\nmakes the interface a bit cumbersome to work with. In contrast, the other packages focus on\nbuilding large and dynamic computation graphs and executing them “on the ﬂy” without a\ncompilation step. While the execution speed may suﬀer with respect to Theano’s optimized\nversion, these packages are especially convenient when working with the recurrent and\n31\nrecursive networks described in Sections 10, 12 as well as in structured prediction settings\nas described in Section 8.\nImplementation Recipe\nUsing the computation graph abstraction, the pseudo-code for\na network training algorithm is given in Algorithm 5.\nAlgorithm 5 Neural Network Training with Computation Graph Abstraction (using mini-\nbatches of size 1)\n1: Deﬁne network parameters.\n2: for iteration = 1 to N do\n3:\nfor Training example xi, yi in dataset do\n4:\nloss node ←build computation graph(xi, yi, parameters)\n5:\nloss node.forward()\n6:\ngradients ←loss node().backward()\n7:\nparameters ←update parameters(parameters, gradients)\n8: return parameters.\nHere, build computation graph is a user-deﬁned function that builds the computation\ngraph for the given input, output and network structure, returning a single loss node.\nupdate parameters is an optimizer speciﬁc update rule. The recipe speciﬁes that a new\ngraph is created for each training example. This accommodates cases in which the network\nstructure varies between training example, such as recurrent and recursive neural networks,\nto be discussed in Sections 10 – 12. For networks with ﬁxed structures, such as an MLPs, it\nmay be more eﬃcient to create one base computation graph and vary only the inputs and\nexpected outputs between examples.\nNetwork Composition\nAs long as the network’s output is a vector (1 × k matrix), it\nis trivial to compose networks by making the output of one network the input of another,\ncreating arbitrary networks. The computation graph abstractions makes this ability explicit:\na node in the computation graph can itself be a computation graph with a designated output\nnode. One can then design arbitrarily deep and complex networks, and be able to easily\nevaluate and train them thanks to automatic forward and gradient computation. This makes\nit easy to deﬁne and train networks for structured outputs and multi-objective training, as\nwe discuss in Section 7, as well as complex recurrent and recursive networks, as discussed\nin Sections 10–12.\n6.3 Optimization Issues\nOnce the gradient computation is taken care of, the network is trained using SGD or another\ngradient-based optimization algorithm. The function being optimized is not convex, and for\na long time training of neural networks was considered a “black art” which can only be done\nby selected few. Indeed, many parameters aﬀect the optimization process, and care has to\nbe taken to tune these parameters. While this tutorial is not intended as a comprehensive\nguide to successfully training neural networks, we do list here a few of the prominent issues.\nFor further discussion on optimization techniques and algorithms for neural networks, refer\nto (Bengio et al., 2015, Chapter 8). For some theoretical discussion and analysis, refer\n32\nto (Glorot & Bengio, 2010). For various practical tips and recommendations, see (LeCun\net al., 1998a; Bottou, 2012).\nInitialization\nThe non-convexity of the loss function means the optimization procedure\nmay get stuck in a local minimum or a saddle point, and that starting from diﬀerent initial\npoints (e.g. diﬀerent random values for the parameters) may result in diﬀerent results. Thus,\nit is advised to run several restarts of the training starting at diﬀerent random initializations,\nand choosing the best one based on a development set.22 The amount of variance in the\nresults is diﬀerent for diﬀerent network formulations and datasets, and cannot be predicted\nin advance.\nThe magnitude of the random values has an important eﬀect on the success of training.\nAn eﬀective scheme due to Glorot and Bengio (2010), called xavier initialization after\nGlorot’s ﬁrst name, suggests initializing a weight matrix W ∈Rdin×dout as:\nW ∼U\n\"\n−\n√\n6\n√din + dout\n, +\n√\n6\n√din + dout\n#\nwhere U[a, b] is a uniformly sampled random value in the range [a, b]. This advice works\nwell on many occasions, and is the preferred default initialization method by many.\nAnalysis by He et al (2015) suggests that when using ReLU non-linearities, the weights\nshould be initialized by sampling from a zero-mean Gaussian distribution whose standard\ndeviation is\nq\n2\ndin . This initialization was found by He et al to work better than xavier\ninitialization in an image classiﬁcation task, especially when deep networks were involved.\nVanishing and Exploding Gradients\nIn deep networks, it is common for the error\ngradients to either vanish (become exceedingly close to 0) or explode (become exceedingly\nhigh) as they propagate back through the computation graph. The problem becomes more\nsevere in deeper networks, and especially so in recursive and recurrent networks (Pascanu,\nMikolov, & Bengio, 2012). Dealing with the vanishing gradients problem is still an open\nresearch question. Solutions include making the networks shallower, step-wise training (ﬁrst\ntrain the ﬁrst layers based on some auxiliary output signal, then ﬁx them and train the upper\nlayers of the complete network based on the real task signal), or specialized architectures\nthat are designed to assist in gradient ﬂow (e.g., the LSTM and GRU architectures for\nrecurrent networks, discussed in Section 11).\nDealing with the exploding gradients has\na simple but very eﬀective solution: clipping the gradients if their norm exceeds a given\nthreshold. Let ˆg be the gradients of all parameters in the network, and ∥ˆg∥be their L2\nnorm. Pascanu et al (2012) suggest to set: ˆg ←threshold\n∥ˆg∥\nˆg if ∥ˆg∥> threshold.\nSaturation and Dead Neurons\nLayers with tanh and sigmoid activations can become\nsaturated – resulting in output values for that layer that are all close to one, the upper-\nlimit of the activation function. Saturated neurons have very small gradients, and should\nbe avoided. Layers with the ReLU activation cannot be saturated, but can “die” – most\nor all values are negative and thus clipped at zero for all inputs, resulting in a gradient\nof zero for that layer. If your network does not train well, it is advisable to monitor the\nnetwork for saturated or dead layers. Saturated neurons are caused by too large values\n22. When debugging, and for reproducibility of results, it is advised to used a ﬁxed random seed.\n33\nentering the layer. This may be controlled for by changing the initialization, scaling the\nrange of the input values, or changing the learning rate. Dead neurons are caused by all\nweights entering the layer being negative (for example this can happen after a large gradient\nupdate). Reducing the learning rate will help in this situation. For saturated layers, another\noption is to normalize the values in the saturated layer after the activation, i.e. instead of\ng(h) = tanh(h) using g(h) =\ntanh(h)\n∥tanh(h)∥. Layer normalization is an eﬀective measure for\ncountering saturation, but is also expensive in terms of gradient computation.\nShuﬄing\nThe order in which the training examples are presented to the network is im-\nportant. The SGD formulation above speciﬁes selecting a random example in each turn.\nIn practice, most implementations go over the training example in order. It is advised to\nshuﬄe the training examples before each pass through the data.\nLearning Rate\nSelection of the learning rate is important.\nToo large learning rates\nwill prevent the network from converging on an eﬀective solution.\nToo small learning\nrates will take very long time to converge. As a rule of thumb, one should experiment\nwith a range of initial learning rates in range [0, 1], e.g.\n0.001, 0.01, 0.1, 1.\nMonitor\nthe network’s loss over time, and decrease the learning rate once the network seem to be\nstuck in a ﬁxed region. Learning rate scheduling decrease the rate as a function of the\nnumber of observed minibatches. A common schedule is dividing the initial learning rate\nby the iteration number. L´eon Bottou (2012) recommends using a learning rate of the form\nηt = η0(1 + η0λt)−1 where η0 is the initial learning rate, ηt is the learning rate to use on\nthe tth training example, and λ is an additional hyperparameter. He further recommends\ndetermining a good value of η0 based on a small sample of the data prior to running on the\nentire dataset.\nMinibatches\nParameter updates occur either every training example (minibatches of size\n1) or every k training examples. Some problems beneﬁt from training with larger minibatch\nsizes. In terms of the computation graph abstraction, one can create a computation graph\nfor each of the k training examples, and then connecting the k loss nodes under an averaging\nnode, whose output will be the loss of the minibatch. Large minibatched training can also\nbe beneﬁcial in terms of computation eﬃciency on specialized computing architectures such\nas GPUs. This is beyond the scope of this tutorial.\n6.4 Regularization\nNeural network models have many parameters, and overﬁtting can easily occur. Overﬁtting\ncan be alleviated to some extent by regularization. A common regularization method is\nL2 regularization, placing a squared penalty on parameters with large values by adding\nan additive λ\n2∥θ∥2 term to the objective function to be minimized, where θ is the set of\nmodel parameters, ∥· ∥2 is the squared L2 norm (sum of squares of the values), and λ is a\nhyperparameter controlling the amount of regularization.\nA recently proposed alternative regularization method is dropout (Hinton, Srivastava,\nKrizhevsky, Sutskever, & Salakhutdinov, 2012). The dropout method is designed to prevent\nthe network from learning to rely on speciﬁc weights. It works by randomly dropping (set-\nting to 0) half of the neurons in the network (or in a speciﬁc layer) in each training example.\nWork by Wager et al (2013) establishes a strong connection between the dropout method\n34\nand L2 regularization. Gal and Gharamani (2015) show that a multi-layer perceptron with\ndropout applied at every layer can be interpreted as Bayesian model averaging.\nThe dropout technique is one of the key factors contributing to very strong results of\nneural-network methods on image classiﬁcation tasks (Krizhevsky, Sutskever, & Hinton,\n2012), especially when combined with ReLU activation units (Dahl, Sainath, & Hinton,\n2013). The dropout technique is eﬀective also in NLP applications of neural networks.\n35\n7. Cascading and Multi-task Learning\nThe combination of online training methods with automatic gradient computations using\nthe computation graph abstraction allows for an easy implementation of model cascading,\nparameter sharing and multi-task learning.\nModel cascading\nis a powerful technique in which large networks are built by composing\nthem out of smaller component networks. For example, we may have a feed-forward network\nfor predicting the part of speech of a word based on its neighbouring words and/or the\ncharacters that compose it. In a pipeline approach, we would use this network for predicting\nparts of speech, and then feed the predictions as input features to neural network that does\nsyntactic chunking or parsing. Instead, we could think of the hidden layers of this network\nas an encoding that captures the relevant information for predicting the part of speech. In\na cascading approach, we take the hidden layers of this network and connect them (and not\nthe part of speech prediction themselves) as the inputs for the syntactic network. We now\nhave a larger network that takes as input sequences of words and characters, and outputs a\nsyntactic structure. The computation graph abstraction allows us to easily propagate the\nerror gradients from the syntactic task loss all the way back to the characters.\nTo combat the vanishing gradient problem of deep networks, as well as to make better\nuse of available training material, the individual component network’s parameters can be\nbootstrapped by training them separately on a relevant task, before plugging them in to\nthe larger network for further tuning. For example, the part-of-speech predicting network\ncan be trained to accurately predict parts-of-speech on a relatively large annotated corpus,\nbefore plugging its hidden layer into the syntactic parsing network for which less training\ndata is available. In case the training data provide direct supervision for both tasks, we can\nmake use of it during training by creating a network with two outputs, one for each task,\ncomputing a separate loss for each output, and then summing the losses into a single node\nfrom which we backpropagate the error gradients.\nModel cascading is very common when using convolutional, recursive and recurrent\nneural networks, where, for example, a recurrent network is used to encode a sentence into\na ﬁxed sized vector, which is then used as the input of another network. The supervision\nsignal of the recurrent network comes primarily from the upper network that consumes the\nrecurrent network’s output as it inputs.\nMulti-task learning\nis used when we have related prediction tasks that do not neces-\nsarily feed into one another, but we do believe that information that is useful for one type\nof prediction can be useful also to some of the other tasks. For example, chunking, named\nentity recognition (NER) and language modeling are examples of synergistic tasks. Infor-\nmation for predicting chunk boundaries, named-entity boundaries and the next word in the\nsentence all rely on some shared underlying syntactic-semantic representation. Instead of\ntraining a separate network for each task, we can create a single network with several out-\nputs. A common approach is to have a multi-layer feed-forward network, whose ﬁnal hidden\nlayer (or a concatenation of all hidden layers) is then passed to diﬀerent output layers.\nThis\nway, most of the parameters of the network are shared between the diﬀerent tasks. Useful\ninformation learned from one task can then help to disambiguate other tasks. Again, the\ncomputation graph abstraction makes it very easy to construct such networks and compute\n36\nthe gradients for them, by computing a separate loss for each available supervision signal,\nand then summing the losses into a single loss that is used for computing the gradients. In\ncase we have several corpora, each with diﬀerent kind of supervision signal (e.g. we have\none corpus for NER and another for chunking), the training procedure will shuﬄe all of the\navailable training example, performing gradient computation and updates with respect to\na diﬀerent loss in every turn. Multi-task learning in the context of language-processing is\nintroduced and discussed in (Collobert et al., 2011).\n37\n8. Structured Output Prediction\nMany problems in NLP involve structured outputs: cases where the desired output is not\na class label or distribution over class labels, but a structured object such as a sequence,\na tree or a graph. Canonical examples are sequence tagging (e.g. part-of-speech tagging)\nsequence segmentation (chunking, NER), and syntactic parsing. In this section, we discuss\nhow feed-forward neural network models can be used for structured tasks. In later sections\nwe discuss specialized neural network models for dealing with sequences (Section 10) and\ntrees (Section 12).\n8.1 Greedy Structured Prediction\nThe greedy approach to structured prediction is to decompose the structure prediction\nproblem into a sequence of local prediction problems and training a classiﬁer to perform\neach local decision. At test time, the trained classiﬁer is used in a greedy manner. Examples\nof this approach are left-to-right tagging models (Gim´enez & M`arquez, 2004) and greedy\ntransition-based parsing (Nivre, 2008). Such approaches are easily adapted to use neural\nnetworks by simply replacing the local classiﬁer from a linear classiﬁer such as an SVM or a\nlogistic regression model to a neural network, as demonstrated in (Chen & Manning, 2014;\nLewis & Steedman, 2014).\nThe greedy approaches suﬀer from error propagation, where mistakes in early decisions\ncarry over and inﬂuence later decisions. The overall higher accuracy achievable with non-\nlinear neural network classiﬁers helps in oﬀsetting this problem to some extent. In addition,\ntraining techniques were proposed for mitigating the error propagation problem by either\nattempting to take easier predictions before harder ones (the easy-ﬁrst approach (Goldberg\n& Elhadad, 2010)) or making training conditions more similar to testing conditions by\nexposing the training procedure to inputs that result from likely mistakes (Hal Daum´e III,\nLangford, & Marcu, 2009; Goldberg & Nivre, 2013). These are eﬀective also for training\ngreedy neural network models, as demonstrated by Ma et al (Ma, Zhang, & Zhu, 2014)\n(easy-ﬁrst tagger) and (?) (dynamic oracle training for greedy dependency parsing).\n8.2 Search Based Structured Prediction\nThe common approach to predicting natural language structures is search based. For in-\ndepth discussion of search-based structure prediction in NLP, see the book by Smith (Smith,\n2011). The techniques can easily be adapted to use a neural-network. In the neural-networks\nliterature, such models were discussed under the framework of energy based learning (LeCun\net al., 2006, Section 7). They are presented here using setup and terminology familiar to\nthe NLP community.\nSearch-based structured prediction is formulated as a search problem over possible struc-\ntures:\npredict(x) = arg max\ny∈Y(x)\nscore(x, y)\nwhere x is an input structure, y is an output over x (in a typical example x is a sentence\nand y is a tag-assignment or a parse-tree over the sentence), Y(x) is the set of all valid\n38\nstructures over x, and we are looking for an output y that will maximize the score of the\nx, y pair.\nThe scoring function is deﬁned as a linear model:\nscore(x, y) = Φ(x, y) · w\nwhere Φ is a feature extraction function and w is a weight vector.\nIn order to make the search for the optimal y tractable, the structure y is decomposed\ninto parts, and the feature function is deﬁned in terms of the parts, where φ(p) is a part-local\nfeature extraction function:\nΦ(x, y) =\nX\np∈parts(x,y)\nφ(p)\nEach part is scored separately, and the structure score is the sum of the component\nparts scores:\nscore(x, y) =w · Φ(x, y) = w ·\nX\np∈y\nφ(p) =\nX\np∈y\nw · φ(p) =\nX\np∈y\nscore(p)\nwhere p ∈y is a shorthand for p ∈parts(x, y). The decomposition of y into parts is such\nthat there exists an inference algorithm that allows for eﬃcient search for the best scoring\nstructure given the scores of the individual parts.\nOne can now trivially replace the linear scoring function over parts with a neural-\nnetwork:\nscore(x, y) =\nX\np∈y\nscore(p) =\nX\np∈y\nNN(c(p))\nwhere c(p) maps the part p into a din dimensional vector.\nIn case of a one hidden-layer feed-forward network:\nscore(x, y) =\nX\np∈y\nNNMLP1(c(p)) =\nX\np∈y\n(g(c(p)W1 + b1))w\nc(p) ∈Rdin, W1 ∈Rdin×d1, b1 ∈Rd1, w ∈Rd1.\nA common objective in structured\nprediction is making the gold structure y score higher than any other structure y′, leading\nto the following (generalized perceptron) loss:\nmax\ny′\nscore(x, y′) −score(x, y)\nIn terms of implementation, this means: create a computation graph CGp for each of\nthe possible parts, and calculate its score. Then, run inference over the scored parts to\nﬁnd the best scoring structure y′. Connect the output nodes of the computation graphs\ncorresponding to parts in the gold (predicted) structure y (y′) into a summing node CGy\n(CG′\ny). Connect CGy and CG′\ny using a “minus” node, CGl, and compute the gradients.\nAs argued in (LeCun et al., 2006, Section 5), the generalized perceptron loss may not\nbe a good loss function when training structured prediction neural networks as it does not\nhave a margin, and a margin-based hinge loss is preferred:\n39\nmax(0, m + score(x, y) −max\ny′̸=y score(x, y′))\nIt is trivial to modify the implementation above to work with the hinge loss.\nNote that in both cases we lose the nice properties of the linear model. In particular, the\nmodel is no longer convex. This is to be expected, as even the simplest non-linear neural\nnetwork is already non-convex. Nonetheless, we could still use standard neural-network\noptimization techniques to train the structured model.\nTraining and inference is slower, as we have to evaluate the neural network (and take\ngradients) |parts(x, y)| times.\nStructured prediction is a vast ﬁeld and is beyond the scope of this tutorial, but loss func-\ntions, regularizers and methods described in, e.g., (Smith, 2011), such as cost-augmented\ndecoding, can be easily applied or adapted to the neural-network framework.23\nProbabilistic objective (CRF)\nIn a probabilistic framework (“CRF”), we treat each\nof the parts scores as a clique potential (see (Smith, 2011)) and deﬁne the score of each\nstructure y to be:\nscoreCRF (x, y) = P(y|x) =\nP\np∈y escore(p)\nP\ny′∈Y(x)\nP\np∈y′ escore(p) =\nP\np∈y eNN(c(p))\nP\ny′∈Y(x)\nP\np∈y′ eNN(c(p))\nThe scoring function deﬁnes a conditional distribution P(y|x), and we wish to set the pa-\nrameters of the network such that corpus conditional log likelihood P\n(xi,yi)∈training log P(yi|xi)\nis maximized.\nThe loss for a given training example (x, y) is then: −log scoreCRF (x, y). Taking the\ngradient with respect to the loss is as involved as building the associated computation\ngraph. The tricky part is the denominator (the partition function) which requires summing\nover the potentially exponentially many structures in Y. However, for some problems, a\ndynamic programming algorithm exists for eﬃciently solving the summation in polynomial\ntime. When such an algorithm exists, it can be adapted to also create a polynomial-size\ncomputation graph.\nWhen an eﬃcient enough algorithm for computing the partition function is not available,\napproximate methods can be used. For example, one may use beam search for inference,\nand for the partition function sum over the structures remaining in the beam instead of\nover the exponentially large Y(x).\nA hinge based approached was used by Pei et al (2015) for arc-factored dependency\nparsing, and the probabilistic approach by Durrett and Klein (Durrett & Klein, 2015) for a\nCRF constituency parser. The approximate beam-based partition function was eﬀectively\nused by Zhou et al (2015) in a transition based parser.\nReranking\nWhen searching over all possible structures is intractable, ineﬃcient or hard\nto integrate into a model, reranking methods are often used. In the reranking framework\n(Charniak & Johnson, 2005; Collins & Koo, 2005) a base model is used to produce a\n23. One should keep in mind that the resulting objectives are no longer convex, and so lack the formal guar-\nantees and bounds associated with convex optimization problems. Similarly, the theory, learning bounds\nand guarantees associated with the algorithms do not automatically transfer to the neural versions.\n40\nlist of the k-best scoring structures. A more complex model is then trained to score the\ncandidates in the k-best list such that the best structure with respect to the gold one is\nscored highest. As the search is now performed over k items rather than over an exponential\nspace, the complex model can condition on (extract features from) arbitrary aspects of the\nscored structure. Reranking methods are natural candidates for structured prediction using\nneural-network models, as they allow the modeler to focus on the feature extraction and\nnetwork structure, while removing the need to integrate the neural network scoring into a\ndecoder. Indeed, reranking methods are often used for experimenting with neural models\nthat are not straightforward to integrate into a decoder, such as convolutional, recurrent\nand recursive networks, which will be discussed in later sections. Works using the reranking\napproach include (Socher et al., 2013; Auli et al., 2013; Le & Zuidema, 2014; Zhu et al.,\n2015a)\nMEMM and hybrid approaches\nOther formulations are, of course, also possible. For\nexample, an MEMM (McCallum, Freitag, & Pereira, 2000) can be trivially adapted to the\nneural network world by replacing the logistic regression (“Maximum Entropy”) component\nwith an MLP.\nHybrid approaches between neural networks and linear models are also explored. In\nparticular, Weiss et al (Weiss et al., 2015) report strong results for transition-based depen-\ndency parsing in a two-stage model. In the ﬁrst stage, a static feed-forward neural network\n(MLP2) is trained to perform well on each of the individual decisions of the structured\nproblem in isolation. In the second stage, the neural network model is held ﬁxed, and the\ndiﬀerent layers (output as well as hidden layer vectors) for each input are then concatenated\nand used as the input features of a linear structured perceptron model (Collins, 2002) that\nis trained to perform beam-search for the best resulting structure. While it is not clear\nthat such training regime is more eﬀective than training a single structured-prediction neu-\nral network, the use of two simpler, isolated models allowed the researchers to perform a\nmuch more extensive hyper-parameter search (e.g. tuning layer sizes, activation functions,\nlearning rates and so on) for each model than is feasible with more complicated networks.\n41\n9. Convolutional Layers\nSometimes we are interested in making predictions based on ordered sets of items (e.g.\nthe sequence of words in a sentence, the sequence of sentences in a document and so on).\nConsider for example predicting the sentiment (positive, negative or neutral) of a sentence.\nSome of the sentence words are very informative of the sentiment, other words are less\ninformative, and to a good approximation, an informative clue is informative regardless\nof its position in the sentence.\nWe would like to feed all of the sentence words into a\nlearner, and let the training process ﬁgure out the important clues. One possible solution is\nfeeding a CBOW representation into a fully connected network such as an MLP. However,\na downside of the CBOW approach is that it ignores the ordering information completely,\nassigning the sentences “it was not good, it was actually quite bad” and “it was not bad,\nit was actually quite good” the exact same representation. While the global position of the\nindicators “not good” and “not bad” does not matter for the classiﬁcation task, the local\nordering of the words (that the word “not” appears right before the word “bad”) is very\nimportant. A naive approach would suggest embedding word-pairs (bi-grams) rather than\nwords, and building a CBOW over the embedded bigrams. While such architecture could be\neﬀective, it will result in huge embedding matrices, will not scale for longer n-grams, and will\nsuﬀer from data sparsity problems as it does not share statistical strength between diﬀerent\nn-grams (the embedding of “quite good” and “very good” are completely independent of\none another, so if the learner saw only one of them during training, it will not be able to\ndeduce anything about the other based on its component words). The convolution-and-\npooling (also called convolutional neural networks, or CNNs) architecture is an elegant and\nrobust solution to the this modeling problem. A convolutional neural network is designed\nto identify indicative local predictors in a large structure, and combine them to produce a\nﬁxed size vector representation of the structure, capturing these local aspects that are most\ninformative for the prediction task at hand.\nConvolution-and-pooling architectures (LeCun & Bengio, 1995) evolved in the neural\nnetworks vision community, where they showed great success as object detectors – recog-\nnizing an object from a predeﬁned category (“cat”, “bicycles”) regardless of its position in\nthe image (Krizhevsky et al., 2012). When applied to images, the architecture is using 2-\ndimensional (grid) convolutions. When applied to text, NLP we are mainly concerned with\n1-d (sequence) convolutions. Convolutional networks were introduced to the NLP commu-\nnity in the pioneering work of Collobert, Weston and Colleagues (2011) who used them for\nsemantic-role labeling, and later by Kalchbrenner et al (2014) and Kim (Kim, 2014) who\nused them for sentiment and question-type classiﬁcation.\n9.1 Basic Convolution + Pooling\nThe main idea behind a convolution and pooling architecture for language tasks is to apply\na non-linear (learned) function over each instantiation of a k-word sliding window over\nthe sentence. This function (also called “ﬁlter”) transforms a window of k words into a d\ndimensional vector that captures important properties of the words in the window (each\ndimension is sometimes referred to in the literature as a “channel”). Then, a “pooling”\noperation is used combine the vectors resulting from the diﬀerent windows into a single\nd-dimensional vector, by taking the max or the average value observed in each of the d\n42\nchannels over the diﬀerent windows.\nThe intention is to focus on the most important\n“features” in the sentence, regardless of their location. The d-dimensional vector is then\nfed further into a network that is used for prediction. The gradients that are propagated\nback from the network’s loss during the training process are used to tune the parameters\nof the ﬁlter function to highlight the aspects of the data that are important for the task\nthe network is trained for. Intuitively, when the sliding window is run over a sequence, the\nﬁlter function learns to identify informative k-grams.\nMore formally, consider a sequence of words x = x1, . . . , xn, each with their correspond-\ning demb dimensional word embedding v(xi). A 1d convolution layer24 of width k works by\nmoving a sliding window of size k over the sentence, and applying the same “ﬁlter” to each\nwindow in the sequence (v(xi); v(xi+1), . . . ; v(xi+k−1). The ﬁlter function is usually a linear\ntransformation followed by a non-linear activation function.\nLet the concatenated vector of the ith window be wi = v(xi); v(xi+1); v(xi+k−1), wi ∈\nRkdemb. Depending on whether we pad the sentence with k −1 words to each side, we may\nget either m = n−k +1 (narrow convolution) or m = n+k +1 windows (wide convolution)\n(Kalchbrenner et al., 2014). The result of the convolution layer is m vectors p1, . . . , pm,\npi ∈Rdconv where:\npi = g(wiW + b)\n.\ng is a non-linear activation function that is applied element-wise, W ∈Rk·demb×dconv and\nb ∈Rdconv are parameters of the network. Each pi is a dconv dimensional vector, encoding\nthe information in wi. Ideally, each dimension captures a diﬀerent kind of indicative infor-\nmation. The m vectors are then combined using a max pooling layer, resulting in a single\ndconv dimensional vector c.\ncj = max\n1<i≤m pi[j]\npi[j] denotes the jth component of pi. The eﬀect of the max-pooling operation is to get the\nmost salient information across window positions. Ideally, each dimension will “specialize”\nin a particular sort of predictors, and max operation will pick on the most important\npredictor of each type.\nFigure 4 provides an illustration of the process.\nThe resulting vector c is a representation of the sentence in which each dimension\nreﬂects the most salient information with respect to some prediction task. c is then fed\ninto a downstream network layers, perhaps in parallel to other vectors, culminating in an\noutput layer which is used for prediction. The training procedure of the network calculates\nthe loss with respect to the prediction task, and the error gradients are propagated all the\nway back through the pooling and convolution layers, as well as the embedding layers. 25\n24. 1d here refers to a convolution operating over 1-dimensional inputs such as sequences, as opposed to 2d\nconvolutions which are applied to images.\n25. Besides being useful for prediction, a by-product of the training procedure is a set of parameters W, B\nand embeddings v() that can be used in a convolution and pooling architecture to encode arbitrary length\nsentences into ﬁxed-size vectors, such that sentences that share the same kind of predictive information\nwill be close to each other.\n43\nthe quick brown\nquick brown fox\nbrown fox jumped\nfox jumped over\njumped over the\nover the lazy\nthe lazy dog\nMUL+tanh\nMUL+tanh\nMUL+tanh\nMUL+tanh\nMUL+tanh\nMUL+tanh\nMUL+tanh\nW\n6 × 3\nthe quick brown fox jumped over the lazy dog\nmax\nconvolution\npooling\nFigure 4: 1d convolution+pooling over the sentence “the quick brown fox jumped over the\nlazy dog”. This is a narrow convolution (no padding is added to the sentence)\nwith a window size of 3. Each word is translated to a 2-dim embedding vector\n(not shown). The embedding vectors are then concatenated, resulting in 6-dim\nwindow representations. Each of the seven windows is transfered through a 6 × 3\nﬁlter (linear transformation followed by element-wise tanh), resulting in seven\n3-dimensional ﬁltered representations. Then, a max-pooling operation is applied,\ntaking the max over each dimension, resulting in a ﬁnal 3-dimensional pooled\nvector.\nWhile max-pooling is the most common pooling operation in text applications, other\npooling operations are also possible, the second most common operation being average\npooling, taking the average value of each index instead of the max.\n9.2 Dynamic, Hierarchical and k-max Pooling\nRather than performing a single pooling operation over the entire sequence, we may want\nto retain some positional information based on our domain understanding of the prediction\nproblem at hand. To this end, we can split the vectors pi into ℓdistinct groups, apply\nthe pooling separately on each group, and then concatenate the ℓresulting dconv vectors\nc1, . . . , cℓ. The division of the pis into groups is performed based on domain knowledge. For\nexample, we may conjecture that words appearing early in the sentence are more indicative\nthan words appearing late. We can then split the sequence into ℓequally sized regions,\napplying a separate max-pooling to each region. For example, Johnson and Zhang (Johnson\n& Zhang, 2014) found that when classifying documents into topics, it is useful to have 20\naverage-pooling regions, clearly separating the initial sentences (where the topic is usually\nintroduced) from later ones, while for a sentiment classiﬁcation task a single max-pooling\noperation over the entire sentence was optimal (suggesting that one or two very strong\nsignals are enough to determine the sentiment, regardless of the position in the sentence).\n44\nSimilarly, in a relation extraction kind of task we may be given two words and asked to\ndetermine the relation between them. We could argue that the words before the ﬁrst word,\nthe words after the second word, and the words between them provide three diﬀerent kinds\nof information (Chen et al., 2015). We can thus split the pi vectors accordingly, pooling\nseparately the windows resulting from each group.\nAnother variation is performing hierarchical pooling, in which we have a succession\nof convolution and pooling layers, where each stage applies a convolution to a sequence,\npools every k neighboring vectors, performs a convolution on the resulting pooled sequence,\napplies another convolution and so on. This architecture allows sensitivity to increasingly\nlarger structures.\nFinally, (Kalchbrenner et al., 2014) introduced a k-max pooling operation, in which the\ntop k values in each dimension are retained instead of only the best one, while preserving\nthe order in which they appeared in the text. For example a, consider the following matrix:\n\n\n1\n2\n3\n9\n6\n5\n2\n3\n1\n7\n8\n1\n3\n4\n1\n\n\nA 1-max pooling over the column vectors will result in\n\u0002\n9\n8\n5\n\u0003\n, while a 2-max pooling\nwill result in the following matrix:\n\u00149\n6\n3\n7\n8\n5\n\u0015\nwhose rows will then be concatenated to\n\u0002\n9\n6\n3\n7\n8\n5\n\u0003\nThe k-max pooling operation makes it possible to pool the k most active indicators that\nmay be a number of positions apart; it preserves the order of the features, but is insensitive\nto their speciﬁc positions. It can also discern more ﬁnely the number of times the feature\nis highly activated (Kalchbrenner et al., 2014).\n9.3 Variations\nRather than a single convolutional layer, several convolutional layers may be applied in\nparallel. For example, we may have four diﬀerent convolutional layers, each with a diﬀerent\nwindow size in the range 2–5, capturing n-gram sequences of varying lengths. The result\nof each convolutional layer will then be pooled, and the resulting vectors concatenated and\nfed to further processing (Kim, 2014).\nThe convolutional architecture need not be restricted into the linear ordering of a sen-\ntence.\nFor example, Ma et al (2015) generalize the convolution operation to work over\nsyntactic dependency trees. There, each window is around a node in the syntactic tree,\nand the pooling is performed over the diﬀerent nodes. Similarly, Liu et al (2015) apply a\nconvolutional architecture on top of dependency paths extracted from dependency trees. Le\nand Zuidema (2015) propose to perform max pooling over vectors representing the diﬀerent\nderivations leading to the same chart item in a chart parser.\n45\n10. Recurrent Neural Networks – Modeling Sequences and Stacks\nWhen dealing with language data, it is very common to work with sequences, such as words\n(sequences of letters), sentences (sequences of words) and documents. We saw how feed-\nforward networks can accommodate arbitrary feature functions over sequences through the\nuse of vector concatenation and vector addition (CBOW). In particular, the CBOW rep-\nresentations allows to encode arbitrary length sequences as ﬁxed sized vectors. However,\nthe CBOW representation is quite limited, and forces one to disregard the order of fea-\ntures. The convolutional networks also allow encoding a sequence into a ﬁxed size vector.\nWhile representations derived from convolutional networks are an improvement above the\nCBOW representation as they oﬀer some sensitivity to word order, their order sensitivity is\nrestricted to mostly local patterns, and disregards the order of patterns that are far apart\nin the sequence.\nRecurrent neural networks (RNNs) (Elman, 1990) allow representing arbitrarily sized\nstructured inputs in a ﬁxed-size vector, while paying attention to the structured properties\nof the input.\n10.1 The RNN Abstraction\nWe use xi:j to denote the sequence of vectors xi, . . . , xj. The RNN abstraction takes as\ninput an ordered list of input vectors x1, ..., xn together with an initial state vector s0,\nand returns an ordered list of state vectors s1, ..., sn, as well as an ordered list of output\nvectors y1, ..., yn.\nAn output vector yi is a function of the corresponding state vector\nsi. The input vectors xi are presented to the RNN in a sequential fashion, and the state\nvector si and output vector yi represent the state of the RNN after observing the inputs\nx1:i. The output vector yi is then used for further prediction. For example, a model for\npredicting the conditional probability of an event e given the sequence m1:i can be deﬁned\nas p(e = j|x1:i) = softmax(yiW + b)[j].\nThe RNN model provides a framework for\nconditioning on the entire history x1, . . . , xi without resorting to the Markov assumption\nwhich is traditionally used for modeling sequences. Indeed, RNN-based language models\nresult in very good perplexity scores when compared to n-gram based models.\nMathematically, we have a recursively deﬁned function R that takes as input a state\nvector si and an input vector xi+1, and results in a new state vector si+1. An additional\nfunction O is used to map a state vector si to an output vector yi. When constructing an\nRNN, much like when constructing a feed-forward network, one has to specify the dimension\nof the inputs xi as well as the dimensions of the outputs yi. The dimensions of the states\nsi are a function of the output dimension.26\n26. While RNN architectures in which the state dimension is independent of the output dimension are\npossible, the current popular architectures, including the Simple RNN, the LSTM and the GRU do not\nfollow this ﬂexibility.\n46\nRNN(s0, x1:n) =s1:n, y1:n\nsi = R(si−1, xi)\nyi = O(si)\nxi ∈Rdin, yi ∈Rdout, si ∈Rf(dout)\nThe functions R and O are the same across the sequence positions, but the RNN keeps\ntrack of the states of computation through the state vector that is kept and being passed\nbetween invocations of R.\nGraphically, the RNN has been traditionally presented as in Figure 5.\nR,O\nxi\nyi\nsi\nsi−1\nθ\nFigure 5: Graphical representation of an RNN (recursive).\nThis presentation follows the recursive deﬁnition, and is correct for arbitrary long sequences.\nHowever, for a ﬁnite sized input sequence (and all input sequences we deal with are ﬁnite)\none can unroll the recursion, resulting in the structure in Figure 6.\ns0\nR,O\nx1\ny1\nR,O\nx2\ny2\ns1\nR,O\nx3\ny3\ns2\nθ\nR,O\nx4\ny4\ns3\nR,O\nx5\ny5\ns4\ns5\nFigure 6: Graphical representation of an RNN (unrolled).\nWhile not usually shown in the visualization, we include here the parameters θ in order\nto highlight the fact that the same parameters are shared across all time steps. Diﬀerent\n47\ninstantiations of R and O will result in diﬀerent network structures, and will exhibit diﬀerent\nproperties in terms of their running times and their ability to be trained eﬀectively using\ngradient-based methods. However, they all adhere to the same abstract interface. We will\nprovide details of concrete instantiations of R and O – the Simple RNN, the LSTM and the\nGRU – in Section 11. Before that, let’s consider modeling with the RNN abstraction.\nFirst, we note that the value of si is based on the entire input x1, ..., xi. For example,\nby expanding the recursion for i = 4 we get:\ns4 =R(s3, x4)\n=R(\ns3\nz\n}|\n{\nR(s2, x3), x4)\n=R(R(\ns2\nz\n}|\n{\nR(s1, x2), x3), x4)\n=R(R(R(\ns1\nz\n}|\n{\nR(s0, x1), x2), x3), x4)\nThus, sn (as well as yn) could be thought of as encoding the entire input sequence.27 Is\nthe encoding useful? This depends on our deﬁnition of usefulness. The job of the network\ntraining is to set the parameters of R and O such that the state conveys useful information\nfor the task we are tying to solve.\n10.2 RNN Training\nViewed as in Figure 6 it is easy to see that an unrolled RNN is just a very deep neural\nnetwork (or rather, a very large computation graph with somewhat complex nodes), in\nwhich the same parameters are shared across many parts of the computation. To train an\nRNN network, then, all we need to do is to create the unrolled computation graph for a\ngiven input sequence, add a loss node to the unrolled graph, and then use the backward\n(backpropagation) algorithm to compute the gradients with respect to that loss.\nThis\nprocedure is referred to in the RNN literature as backpropagation through time, or BPTT\n(Werbos, 1990).28 There are various ways in which the supervision signal can be applied.\nAcceptor\nOne option is to base the supervision signal only on the ﬁnal output vector,\nyn. Viewed this way, the RNN is an acceptor. We observe the ﬁnal state, and then decide\n27. Note that, unless R is speciﬁcally designed against this, it is likely that the later elements of the input\nsequence have stronger eﬀect on sn than earlier ones.\n28. Variants of the BPTT algorithm include unrolling the RNN only for a ﬁxed number of input symbols at\neach time: ﬁrst unroll the RNN for inputs x1:k, resulting in s1:k. Compute a loss, and backpropagate\nthe error through the network (k steps back). Then, unroll the inputs xk+1:2k, this time using sk as the\ninitial state, and again backpropagate the error for k steps, and so on. This strategy is based on the\nobservations that for the Simple-RNN variant, the gradients after k steps tend to vanish (for large enough\nk), and so omitting them is negligible. This procedure allows training of arbitrarily long sequences. For\nRNN variants such as the LSTM or the GRU that are designed speciﬁcally to mitigate the vanishing\ngradients problem, this ﬁxed size unrolling is less motivated, yet it is still being used, for example when\ndoing language modeling over a book without breaking it into sentences.\n48\non an outcome.29 For example, consider training an RNN to read the characters of a word\none by one and then use the ﬁnal state to predict the part-of-speech of that word (this is\ninspired by (Ling et al., 2015b)), an RNN that reads in a sentence and, based on the ﬁnal\nstate decides if it conveys positive or negative sentiment (this is inspired by (Wang et al.,\n2015b)) or an RNN that reads in a sequence of words and decides whether it is a valid\nnoun-phrase. The loss in such cases is deﬁned in terms of a function of yn = O(sn), and\nthe error gradients will backpropagate through the rest of the sequence (see Figure 7).30\nThe loss can take any familiar form – cross entropy, hinge, margin, etc.\nR,O\nx1\ns0\nR,O\nx2\ns1\nR,O\nx3\ns2\nR,O\nx4\ns3\nR,O\nx5\ns4\npredict &\ncalc loss\ny5\nloss\nFigure 7: Acceptor RNN Training Graph.\nEncoder\nSimilar to the acceptor case, an encoder supervision uses only the ﬁnal output\nvector, yn. However, unlike the acceptor, where a prediction is made solely on the basis\nof the ﬁnal vector, here the ﬁnal vector is treated as an encoding of the information in the\nsequence, and is used as additional information together with other signals. For example,\nan extractive document summarization system may ﬁrst run over the document with an\nRNN, resulting in a vector yn summarizing the entire document. Then, yn will be used\ntogether with with other features in order to select the sentences to be included in the\nsummarization.\nTransducer\nAnother option is to treat the RNN as a transducer, producing an output for\neach input it reads in. Modeled this way, we can compute a local loss signal Llocal( ˆyi, yi)\nfor each of the outputs ˆyi based on a true label yi. The loss for unrolled sequence will\nthen be: L( ˆ\ny1:n, y1:n) = Pn\ni=1 Llocal( ˆyi, yi), or using another combination rather than a\nsum such as an average or a weighted average (see Figure 8). One example for such a\ntransducer is a sequence tagger, in which we take xi:n to be feature representations for the\nn words of a sentence, and yi as an input for predicting the tag assignment of word i based\non words 1:i. A CCG super-tagger based on such an architecture provides state-of-the art\nCCG super-tagging results (Xu et al., 2015).\nA very natural use-case of the transduction setup is for language modeling, in which the\nsequence of words x1:i is used to predict a distribution over the i + 1th word. RNN based\n29. The terminology is borrowed from Finite-State Acceptors. However, the RNN has a potentially inﬁnite\nnumber of states, making it necessary to rely on a function other than a lookup table for mapping states\nto decisions.\n30. This kind of supervision signal may be hard to train for long sequences, especially so with the Simple-\nRNN, because of the vanishing gradients problem. It is also a generally hard learning task, as we do not\ntell the process on which parts of the input to focus.\n49\nR,O\nx1\ns0\npredict &\ncalc loss\ny1\nR,O\nx2\ns1\npredict &\ncalc loss\ny2\nR,O\nx3\ns2\npredict &\ncalc loss\ny3\nR,O\nx4\ns3\npredict &\ncalc loss\ny4\nR,O\nx5\ns4\npredict &\ncalc loss\ny5\nsum\nloss\nFigure 8: Transducer RNN Training Graph.\nlanguage models are shown to provide much better perplexities than traditional language\nmodels (Mikolov et al., 2010; Sundermeyer, Schl¨uter, & Ney, 2012; Mikolov, 2012).\nUsing RNNs as transducers allows us to relax the Markov assumption that is tradition-\nally taken in language models and HMM taggers, and condition on the entire prediction\nhistory. The power of the ability to condition on arbitrarily long histories is demonstrated\nin generative character-level RNN models, in which a text is generated character by charac-\nter, each character conditioning on the previous ones (Sutskever, Martens, & Hinton, 2011).\nThe generated texts show sensitivity to properties that are not captured by n-gram language\nmodels, including line lengths and nested parenthesis balancing. For a good demonstration\nand analysis of the properties of RNN-based character level language models, see (Karpathy,\nJohnson, & Li, 2015).\nEncoder - Decoder\nFinally, an important special case of the encoder scenario is the\nEncoder-Decoder framework (Cho, van Merrienboer, Bahdanau, & Bengio, 2014a; Sutskever\net al., 2014). The RNN is used to encode the sequence into a vector representation yn, and\nthis vector representation is then used as auxiliary input to another RNN that is used as\na decoder. For example, in a machine-translation setup the ﬁrst RNN encodes the source\nsentence into a vector representation yn, and then this state vector is fed into a separate\n(decoder) RNN that is trained to predict (using a transducer-like language modeling ob-\njective) the words of the target language sentence based on the previously predicted words\nas well as yn. The supervision happens only for the decoder RNN, but the gradients are\npropagated all the way back to the encoder RNN (see Figure 9).\nSuch an approach was shown to be surprisingly eﬀective for Machine Translation (Sutskever\net al., 2014) using LSTM RNNs. In order for this technique to work, Sutskever et al found it\neﬀective to input the source sentence in reverse, such that xn corresponds to the ﬁrst word\nof the sentence. In this way, it is easier for the second RNN to establish the relation be-\ntween the ﬁrst word of the source sentence to the ﬁrst word of the target sentence. Another\nuse-case of the encoder-decoder framework is for sequence transduction. Here, in order to\ngenerate tags t1, . . . , tn, an encoder RNN is ﬁrst used to encode the sentence x1:n into ﬁxed\nsized vector. This vector is then fed as the initial state vector of another (transducer) RNN,\nwhich is used together with x1:n to predict the label ti at each position i. This approach\n50\nRE,OE\nx1\nse\n0\nRE,OE\nx2\nse\n1\nRE,OE\nx3\nse\n2\nRE,OE\nx4\nse\n3\nRE,OE\nx5\nse\n4\nse\n5\nRD,OD\nx1\nsd\n0\npredict &\ncalc loss\ny1\nRD,OD\nx2\nsd\n1\npredict &\ncalc loss\ny2\nRD,OD\nx3\nsd\n2\npredict &\ncalc loss\ny3\nRD,OD\nx4\nsd\n3\npredict &\ncalc loss\ny4\nRD,OD\nx5\nsd\n4\npredict &\ncalc loss\ny5\nsum\nloss\nFigure 9: Encoder-Decoder RNN Training Graph.\nwas used in (Filippova, Alfonseca, Colmenares, Kaiser, & Vinyals, 2015) to model sentence\ncompression by deletion.\n10.3 Multi-layer (stacked) RNNs\nRNNs can be stacked in layers, forming a grid (Hihi & Bengio, 1996). Consider k RNNs,\nRNN1, . . . , RNNk, where the jth RNN has states sj\n1:n and outputs yj\n1:n. The input for the\nﬁrst RNN are x1:n, while the input of the jth RNN (j ≥2) are the outputs of the RNN\nbelow it, yj−1\n1:n . The output of the entire formation is the output of the last RNN, yk\n1:n.\nSuch layered architectures are often called deep RNNs. A visual representation of a 3-layer\nRNN is given in Figure 10.\nWhile it is not theoretically clear what is the additional power gained by the deeper\narchitecture, it was observed empirically that deep RNNs work better than shallower ones\non some tasks. In particular, Sutskever et al (2014) report that a 4-layers deep architec-\nture was crucial in achieving good machine-translation performance in an encoder-decoder\nframework. Irsoy and Cardie (2014) also report improved results from moving from a one-\nlayer BI-RNN to an architecture with several layers. Many other works report result using\nlayered RNN architectures, but do not explicitly compare to 1-layer RNNs.\n51\nR1,O1\nR2,O2\ny1\n1\ns1\n0\nR3,O3\ny2\n1\ns2\n0\ns3\n0\nx1\ny1\ny3\n1\nR1,O1\nR2,O2\ny1\n2\ns1\n1\nR3,O3\ny2\n2\ns2\n1\ns3\n1\nx2\ny2\ny3\n2\nR1,O1\nR2,O2\ny1\n3\ns1\n2\nR3,O3\ny2\n3\ns2\n2\ns3\n2\nx3\ny3\ny3\n3\nR1,O1\nR2,O2\ny1\n4\ns1\n3\nR3,O3\ny2\n4\ns2\n3\ns3\n3\nx4\ny4\ny3\n4\nR1,O1\nR2,O2\ny1\n5\ns1\n4\nR3,O3\ny2\n5\ns2\n4\ns3\n4\nx5\ny5\ny3\n5\ns1\n5\ns2\n5\ns3\n5\nFigure 10: A 3-layer (“deep”) RNN architecture.\n10.4 BI-RNN\nA useful elaboration of an RNN is a bidirectional-RNN (BI-RNN) (Schuster & Paliwal, 1997;\nGraves, 2008).31 Consider the task of sequence tagging over a sentence x1, . . . , xn. An RNN\nallows us to compute a function of the ith word xi based on the past – the words x1:i up\nto and including it. However, the following words xi:n may also be useful for prediction, as\nis evident by the common sliding-window approach in which the focus word is categorized\nbased on a window of k words surrounding it. Much like the RNN relaxes the Markov\nassumption and allows looking arbitrarily back into the past, the BI-RNN relaxes the ﬁxed\nwindow size assumption, allowing to look arbitrarily far at both the past and the future.\nConsider an input sequence x1:n.\nThe BI-RNN works by maintaining two separate\nstates, sf\ni and sb\ni for each input position i. The forward state sf\ni is based on x1, x2, . . . , xi,\nwhile the backward state sb\ni is based on xn, xn−1, . . . , xi. The forward and backward states\nare generated by two diﬀerent RNNs. The ﬁrst RNN (Rf, Of) is fed the input sequence\nx1:n as is, while the second RNN (Rb, Ob) is fed the input sequence in reverse. The state\nrepresentation si is then composed of both the forward and backward states.\nThe output at position i is based on the concatenation of the two output vectors\nyi = [yf\ni ; yb\ni ] = [Of(sf\ni ); Ob(sb\ni )], taking into account both the past and the future. The\nvector yi can then be used directly for prediction, or fed as part of the input to a more\ncomplex network. While the two RNNs are run independently of each other, the error gra-\ndients at position i will ﬂow both forward and backward through the two RNNs. A visual\nrepresentation of the BI-RNN architecture is given in Figure 11.\nThe use of BI-RNNs for sequence tagging was introduced to the NLP community by\nIrsoy and Cardie (2014).\n10.5 RNNs for Representing Stacks\nSome algorithms in language processing, including those for transition-based parsing (Nivre,\n2008), require performing feature extraction over a stack.\nInstead of being conﬁned to\n31. When used with a speciﬁc RNN architecture such as an LSTM, the model is called BI-LSTM.\n52\nRf,Of\nxthe\nconcat\nyf\n1\nsf\n0\nRf,Of\nxbrown\nconcat\nyf\n2\nsf\n1\nRf,Of\nxfox\nconcat\nyf\n3\nsf\n2\nRf,Of\nxjumped\nconcat\nyf\n4\nsf\n3\nRf,Of\nx∗\nconcat\nyf\n5\nsf\n4\nsf\n5\nRb,Ob\ns0\nsb\n0\nyb\n1\nRb,Ob\ns1\nsb\n1\nyb\n2\nRb,Ob\ns2\nsb\n2\nyb\n3\nRb,Ob\ns3\nsb\n3\nyb\n4\nRb,Ob\ns4\nsb\n4\nyb\n5\nsb\n5\nythe\nybrown\nyfox\nyjumped\ny∗\nFigure 11: BI-RNN over the sentence “the brown fox jumped .”.\nlooking at the k top-most elements of the stack, the RNN framework can be used to provide\na ﬁxed-sized vector encoding of the entire stack.\nThe main intuition is that a stack is essentially a sequence, and so the stack state can be\nrepresented by taking the stack elements and feeding them in order into an RNN, resulting\nin a ﬁnal encoding of the entire stack. In order to do this computation eﬃciently (without\nperforming an O(n) stack encoding operation each time the stack changes), the RNN state\nis maintained together with the stack state.\nIf the stack was push-only, this would be\ntrivial: whenever a new element x is pushed into the stack, the corresponding vector x\nwill be used together with the RNN state si in order to obtain a new state si+1. Dealing\nwith pop operation is more challenging, but can be solved by using the persistent-stack\ndata-structure (Okasaki, 1999; Goldberg, Zhao, & Huang, 2013). Persistent, or immutable,\ndata-structures keep old versions of themselves intact when modiﬁed. The persistent stack\nconstruction represents a stack as a pointer to the head of a linked list. An empty stack is\nthe empty list. The push operation appends an element to the list, returning the new head.\nThe pop operation then returns the parent of the head, but keeping the original list intact.\nFrom the point of view of someone who held a pointer to the previous head, the stack did\nnot change. A subsequent push operation will add a new child to the same node. Applying\nthis procedure throughout the lifetime of the stack results in a tree, where the root is an\nempty stack and each path from a node to the root represents an intermediary stack state.\nFigure 12 provides an example of such a tree. The same process can be applied in the\ncomputation graph construction, creating an RNN with a tree structure instead of a chain\nstructure. Backpropagating the error from a given node will then aﬀect all the elements\nthat participated in the stack when the node was created, in order. Figure 13 shows the\ncomputation graph for the stack-RNN corresponding to the last state in Figure 12. This\nmodeling approach was proposed independently by Dyer et al and Watanabe et al (Dyer\net al., 2015; Watanabe & Sumita, 2015) for transition-based dependency parsing.\n53\n⊥\na\nhead\n(1) push a\n⊥\na\nb\nhead\n(2) push b\n⊥\na\nb\nc\nhead\n(3) push c\n⊥\na\nb\nhead\nc\n(4) pop\n⊥\na\nb\nc\nd\nhead\n(5) push d\n⊥\na\nb\nhead\nc\nd\n(6) pop\n⊥\na\nhead\nb\nc\nd\n(7) pop\n⊥\na\nb\nc\nd\ne\nhead\n(8) push e\n⊥\na\nb\nc\nd\ne\nf\nhead\n(9) push f\nFigure 12: An immutable stack construction for the sequence of operations push a; push b;\npush c; pop; push d; pop; pop; push e; push f.\nso\nR,O\nya\nxa\nR,O\nsa\nya:b\nxb\nR,O\nsa:b\nya:c\nxc\nsa:c\nR,O\nsa:b\nya,b,d\nxd\nsa,b,d\nR,O\nsa\nya,e\nxe\nR,O\nsa,e\nya,e,f\nxf\nsa,e,f\nFigure 13: The stack-RNN corresponding to the ﬁnal state in Figure 12.\n54\n11. Concrete RNN Architectures\nWe now turn to present three diﬀerent instantiations of the abstract RNN architecture\ndiscussed in the previous section, providing concrete deﬁnitions of the functions R and O.\nThese are the Simple RNN (SRNN), the Long Short-Term Memory (LSTM) and the Gated\nRecurrent Unit (GRU).\n11.1 Simple RNN\nThe simplest RNN formulation, known as an Elman Network or Simple-RNN (S-RNN), was\nproposed by Elman (1990) and explored for use in language modeling by Mikolov (2012).\nThe S-RNN takes the following form:\nsi =RSRNN(si−1, xi) = g(xiWx + si−1Ws + b)\nyi =OSRNN(si) = si\nsi, yi ∈Rds, xi ∈Rdx, Wx ∈Rdx×ds, Ws ∈Rds×ds, b ∈Rds\nThat is, the state at position i is a linear combination of the input at position i and\nthe previous state, passed through a non-linear activation (commonly tanh or ReLU). The\noutput at position i is the same as the hidden state in that position.32\nIn spite of its simplicity, the Simple RNN provides strong results for sequence tagging\n(Xu et al., 2015) as well as language modeling.\nFor comprehensive discussion on using\nSimple RNNs for language modeling, see the PhD thesis by Mikolov (2012).\n11.2 LSTM\nThe S-RNN is hard to train eﬀectively because of the vanishing gradients problem. Error\nsignals (gradients) in later steps in the sequence diminish in quickly in the back-propagation\nprocess, and do not reach earlier input signals, making it hard for the S-RNN to capture\nlong-range dependencies. The Long Short-Term Memory (LSTM) architecture (Hochreiter\n& Schmidhuber, 1997) was designed to solve the vanishing gradients problem. The main\nidea behind the LSTM is to introduce as part of the state representation also “memory\ncells” (a vector) that can preserve gradients across time. Access to the memory cells is\ncontrolled by gating components – smooth mathematical functions that simulate logical\ngates. At each input state, a gate is used to decide how much of the new input should be\nwritten to the memory cell, and how much of the current content of the memory cell should\nbe forgotten. Concretely, a gate g ∈[0, 1]n is a vector of values in the range [0, 1] that is\nmultiplied component-wise with another vector v ∈Rn, and the result is then added to\nanother vector. The values of g are designed to be close to either 0 or 1, i.e. by using a\nsigmoid function. Indices in v corresponding to near-one values in g are allowed to pass,\nwhile those corresponding to near-zero values are blocked.\n32. Some authors treat the output at position i as a more complicated function of the state. In our presen-\ntation, such further transformation of the output are not considered part of the RNN, but as separate\ncomputations that are applied to the RNNs output. The distinction between the state and the output\nare needed for the LSTM architecture, in which not all of the state is observed outside of the RNN.\n55\nMathematically, the LSTM architecture is deﬁned as:33\nsj = RLSTM(sj−1, xj) =[cj; hj]\ncj =cj−1 ⊙f + g ⊙i\nhj = tanh(cj) ⊙o\ni =σ(xjWxi + hj−1Whi)\nf =σ(xjWxf + hj−1Whf)\no =σ(xjWxo + hj−1Who)\ng = tanh(xjWxg + hj−1Whg)\nyj = OLSTM(sj) =hj\nsj ∈R2·dh, xi ∈Rdx, cj, hj, i, f, o, g ∈Rdh, Wx◦∈Rdx×dh, Wh◦∈Rdh×dh,\nThe symbol ⊙is used to denote component-wise product. The state at time j is com-\nposed of two vectors, cj and hj, where cj is the memory component and hj is the output,\nor state, component. There are three gates, i, f and o, controlling for input, forget and\noutput. The gate values are computed based on linear combinations of the current input\nxj and the previous state hj−1, passed through a sigmoid activation function. An update\ncandidate g is computed as a linear combination of xj and hj−1, passed through a tanh\nactivation function. The memory cj is then updated: the forget gate controls how much\nof the previous memory to keep (cj−1 ⊙f), and the input gate controls how much of the\nproposed update to keep (g ⊙i). Finally, the value of hj (which is also the output yj) is\ndetermined based on the content of the memory cj, passed through a tanh non-linearity\nand controlled by the output gate. The gating mechanisms allow for gradients related to\nthe memory part cj to stay high across very long time ranges.\nFor further discussion on the LSTM architecture see the PhD thesis by Alex Graves\n(2008), as well as Chris Olah’s description.34 For an analysis of the behavior of an LSTM\nwhen used as a character-level language model, see (Karpathy et al., 2015).\nLSTMs are currently the most successful type of RNN architecture, and they are re-\nsponsible for many state-of-the-art sequence modeling results. The main competitor of the\nLSTM-RNN is the GRU, to be discussed next.\nPractical Considerations\nWhen training LSTM networks, Jozefowicz et al (2015) strongly\nrecommend to always initialize the bias term of the forget gate to be close to one. When\napplying dropout to an RNN with an LSTM, Zaremba et al (2014) found out that it is\n33. There are many variants on the LSTM architecture presented here. For example, forget gates were not\npart of the original proposal in (Hochreiter & Schmidhuber, 1997), but are shown to be an important\npart of the architecture. Other variants include peephole connections and gate-tying. For an overview\nand comprehensive empirical comparison of various LSTM architectures see (Greﬀ, Srivastava, Koutn´ık,\nSteunebrink, & Schmidhuber, 2015).\n34. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n56\ncrucial to apply dropout only on the non-recurrent connection, i.e. only to apply it between\nlayers and not between sequence positions.\n11.3 GRU\nThe LSTM architecture is very eﬀective, but also quite complicated. The complexity of the\nsystem makes it hard to analyze, and also computationally expensive to work with. The\ngated recurrent unit (GRU) was recently introduced by Cho et al (2014b) as an alternative\nto the LSTM. It was subsequently shown by Chung et al (2014) to perform comparably to\nthe LSTM on several (non textual) datasets.\nLike the LSTM, the GRU is also based on a gating mechanism, but with substantially\nfewer gates and without a separate memory component.\nsj = RGRU(sj−1, xj) =(1 −z) ⊙sj−1 + z ⊙h\nz =σ(xjWxz + hj−1Whz)\nr =σ(xjWxr + hj−1Whr)\nh = tanh(xjWxh + (hj−1 ⊙r)Whg)\nyj = OLSTM(sj) =sj\nsj ∈Rdh, xi ∈Rdx, z, r, h ∈Rdh, Wx◦∈Rdx×dh, Wh◦∈Rdh×dh,\nOne gate (r) is used to control access to the previous state sj−1 and compute a proposed\nupdate h. The updated state sj (which also serves as the output yj) is then determined based\non an interpolation of the previous state sj−1 and the proposal h, where the proportions of\nthe interpolation are controlled using the gate z.\nThe GRU was shown to be eﬀective in language modeling and machine translation.\nHowever, the jury between the GRU, the LSTM and possible alternative RNN architectures\nis still out, and the subject is actively researched. For an empirical exploration of the GRU\nand the LSTM architectures, see (Jozefowicz et al., 2015).\n11.4 Other Variants\nThe gated architectures of the LSTM and the GRU help in alleviating the vanishing gradi-\nents problem of the Simple RNN, and allow these RNNs to capture dependencies that span\nlong time ranges. Some researchers explore simpler architectures than the LSTM and the\nGRU for achieving similar beneﬁts.\nMikolov et al (2014) observed that the matrix multiplication si−1Ws coupled with the\nnonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo\nlarge changes at each time step, prohibiting it from remembering information over long\ntime periods. They propose to split the state vector si into a slow changing component ci\n(“context units”) and a fast changing component hi.35 The slow changing component ci is\n35. We depart from the notation in (Mikolov et al., 2014) and reuse the symbols used in the LSTM descrip-\ntion.\n57\nupdated according to a linear interpolation of the input and the previous component: ci =\n(1 −α)xiWx1 + αci−1, where α ∈(0, 1). This update allows ci to accumulate the previous\ninputs. The fast changing component hi is updated similarly to the Simple RNN update\nrule, but changed to take ci into account as well:36 hi = σ(xiWx2 + hi−1Wh + ciWc).\nFinally, the output yi is the concatenation of the slow and the fast changing parts of the\nstate: yi = [ci; hi]. Mikolov et al demonstrate that this architecture provides competitive\nperplexities to the much more complex LSTM on language modeling tasks.\nThe approach of Mikolov et al can be interpreted as constraining the block of the matrix\nWs in the S-RNN corresponding to ci to be a multiply of the identity matrix (see Mikolov\net al (2014) for the details). Le et al (Le, Jaitly, & Hinton, 2015) propose an even simpler\napproach: set the activation function of the S-RNN to a ReLU, and initialize the biases b\nas zeroes and the matrix Ws as the identify matrix. This causes an untrained RNN to copy\nthe previous state to the current state, add the eﬀect of the current input xi and set the\nnegative values to zero. After setting this initial bias towards state copying, the training\nprocedure allows Ws to change freely. Le et al demonstrate that this simple modiﬁcation\nmakes the S-RNN comparable to an LSTM with the same number of parameters on several\ntasks, including language modeling.\n36. The update rule diverges from the S-RNN update rule also by ﬁxing the non-linearity to be a sigmoid\nfunction, and by not using a bias term. However, these changes are not discussed as central to the\nproposal.\n58\n12. Modeling Trees – Recursive Neural Networks\nThe RNN is very useful for modeling sequences. In language processing, it is often natural\nand desirable to work with tree structures. The trees can be syntactic trees, discourse trees,\nor even trees representing the sentiment expressed by various parts of a sentence (Socher\net al., 2013). We may want to predict values based on speciﬁc tree nodes, predict values\nbased on the root nodes, or assign a quality score to a complete tree or part of a tree. In\nother cases, we may not care about the tree structure directly but rather reason about spans\nin the sentence. In such cases, the tree is merely used as a backbone structure which help\nguide the encoding process of the sequence into a ﬁxed size vector.\nThe recursive neural network (RecNN) abstraction (Pollack, 1990), popularized in NLP\nby Richard Socher and colleagues (Socher, Manning, & Ng, 2010; Socher, Lin, Ng, & Man-\nning, 2011; Socher et al., 2013; Socher, 2014) is a generalization of the RNN from sequences\nto (binary) trees.37\nMuch like the RNN encodes each sentence preﬁx as a state vector, the RecNN encodes\neach tree-node as a state vector in Rd. We can then use these state vectors either to predict\nvalues of the corresponding nodes, assign quality values to each node, or as a semantic\nrepresentation of the spans rooted at the nodes.\nThe main intuition behind the recursive neural networks is that each subtree is repre-\nsented as a d dimensional vector, and the representation of a node p with children c1 and c2\nis a function of the representation of the nodes: vec(p) = f(vec(c1), vec(c2)), where f is a\ncomposition function taking two d-dimensional vectors and returning a single d-dimensional\nvector. Much like the RNN state si is used to encode the entire sequence x1 : i, the RecNN\nstate associated with a tree node p encodes the entire subtree rooted at p. See Figure 14\nfor an illustration.\n12.1 Formal Deﬁnition\nConsider a binary parse tree T over an n-word sentence.\nAs a reminder, an ordered,\nunlabeled tree over a string x1, . . . , xn can be represented as a unique set of triplets (i, k, j),\ns.t. i ≤k ≤j. Each such triplet indicates that a node spanning words xi:j is parent of the\nnodes spanning xi:k and xk+1:j. Triplets of the form (i, i, i) correspond to terminal symbols\nat the tree leaves (the words xi). Moving from the unlabeled case to the labeled one, we can\nrepresent a tree as a set of 6-tuples (A →B, C, i, k, j), whereas i, k and j indicate the spans\nas before, and A, B and C are the node labels of of the nodes spanning xi:j, xi:k and xk+1:j\nrespectively. Here, leaf nodes have the form (A →A, A, i, i, i), where A is a pre-terminal\nsymbol. We refer to such tuples as production rules. For an example, consider the syntactic\ntree for the sentence “the boy saw her duck”.\n37. While presented in terms of binary parse trees, the concepts easily transfer to general recursively-deﬁned\ndata structures, with the major technical challenge is the deﬁnition of an eﬀective form for R, the\ncombination function.\n59\nV =\nNP1 =\ncombine\nVP =\nNP2 =\ncombine\nS =\nFigure 14: Illustration of a recursive neural network. The representations of V and NP1\nare combined to form the representation of VP. The representations of VP and\nNP2 are then combined to form the representation of S.\nS\nVP\nNP\nNoun\nduck\nDet\nher\nVerb\nsaw\nNP\nNoun\nboy\nDet\nthe\nIts corresponding unlabeled and labeled representations are :\nUnlabeled\nLabeled\nCorresponding Span\n(1,1,1)\n(Det, Det, Det, 1, 1, 1)\nx1:1 the\n(2,2,2)\n(Noun, Noun, Noun, 2, 2, 2)\nx2:2 boy\n(3,3,3)\n(Verb, Verb, Verb, 3, 3, 3)\nsaw\n(4,4,4)\n(Det, Det, Det, 4, 4, 4)\nher\n(5,5,5)\n(Noun, Noun, Noun, 5, 5, 5)\nduck\n(4,4,5)\n(NP, Det, Noun, 4, 4, 5)\nher duck\n(3,3,5)\n(VP, Verb, NP, 3, 3, 5)\nsaw her duck\n(1,1,2)\n(NP, Det, Noun, 1, 1, 2)\nthe boy\n(1,2,5)\n(S, NP, VP, 1, 2, 5)\nthe boy saw her duck\nThe set of production rules above can be uniquely converted to a set tree nodes qA\ni:j\n(indicating a node with symbol A over the span xi:j) by simply ignoring the elements\n60\n(B, C, k) in each production rule. We are now in position to deﬁne the Recursive Neural\nNetwork.\nA Recursive Neural Network (RecNN) is a function that takes as input a parse tree\nover an n-word sentence x1, . . . , xn. Each of the sentence’s words is represented as a d-\ndimensional vector xi, and the tree is represented as a set T of production rules (A →\nB, C, i, j, k). Denote the nodes of T by qA\ni:j. The RecNN returns as output a correspond-\ning set of inside state vectors sA\ni:j, where each inside state vector sA\ni:j ∈Rd represents the\ncorresponding tree node qA\ni:j, and encodes the entire structure rooted at that node. Like\nthe sequence RNN, the tree shaped RecNN is deﬁned recursively using a function R, where\nthe inside vector of a given node is deﬁned as a function of the inside vectors of its direct\nchildren.38 Formally:\nRecNN(x1, . . . , xn, T ) ={sA\ni:j ∈Rd | qA\ni:j ∈T }\nsA\ni:i =v(xi)\nsA\ni:j =R(A, B, C, sB\ni:k, sC\nk+1:j)\nqB\ni:k ∈T , qC\nk+1:j ∈T\nThe function R usually takes the form of a simple linear transformation, which may or\nmay not be followed by a non-linear activation function g:\nR(A, B, C, sB\ni:k, sC\nk+1:j) = g([sB\ni:k; sC\nk+1:j]W)\nThis formulation of R ignores the tree labels, using the same matrix W ∈R2d×d for all\ncombinations. This may be a useful formulation in case the node labels do not exist (e.g.\nwhen the tree does not represent a syntactic structure with clearly deﬁned labels) or when\nthey are unreliable. However, if the labels are available, it is generally useful to include them\nin the composition function. One approach would be to introduce label embeddings v(A)\nmapping each non-terminal symbol to a dnt dimensional vector, and change R to include\nthe embedded symbols in the combination function:\nR(A, B, C, sB\ni:k, sC\nk+1:j) = g([sB\ni:k; sC\nk+1:j; v(A); v(B)]W)\n(here, W ∈R2d+2dnt×d).\nSuch approach is taken by (Qian, Tian, Huang, Liu, Zhu, &\nZhu, 2015). An alternative approach, due to (Socher et al., 2013) is to untie the weights\naccording to the non-terminals, using a diﬀerent composition matrix for each B, C pair of\nsymbols:39\nR(A, B, C, sB\ni:k, sC\nk+1:j) = g([sB\ni:k; sC\nk+1:j]WBC)\n38. Le and Zuidema (2014) extend the RecNN deﬁnition such that each node has, in addition to its inside\nstate vector, also an outside state vector representing the entire structure around the subtree rooted\nat that node.\nTheir formulation is based on the recursive computation of the classic inside-outside\nalgorithm, and can be thought of as the BI-RNN counterpart of the tree RecNN. For details, see (Le &\nZuidema, 2014).\n39. While not explored in the literature, a trivial extension would condition the transformation matrix also\non A.\n61\nThis formulation is useful when the number of non-terminal symbols (or the number of\npossible symbol combinations) is relatively small, as is usually the case with phrase-structure\nparse trees. A similar model was also used by (Hashimoto et al., 2013) to encode subtrees\nin semantic-relation classiﬁcation task.\n12.2 Extensions and Variations\nAs all of the deﬁnitions of R above suﬀer from the vanishing gradients problem of the\nSimple RNN, several authors sought to replace it with functions inspired by the Long Short-\nTerm Memory (LSTM) gated architecture, resulting in Tree-shaped LSTMs (Tai, Socher, &\nManning, 2015; Zhu, Sobhani, & Guo, 2015b). The question of optimal tree representation\nis still very much an open research question, and the vast space of possible combination\nfunctions R is yet to be explored. Other proposed variants on tree-structured RNNs includes\na recursive matrix-vector model (Socher, Huval, Manning, & Ng, 2012) and recursive neural\ntensor network (Socher et al., 2013). In the ﬁrst variant, each word is represented as a\ncombination of a vector and a matrix, where the vector deﬁnes the word’s static semantic\ncontent as before, while the matrix acts as a learned “operator” for the word, allowing\nmore subtle semantic compositions than the addition and weighted averaging implied by\nthe concatenation followed by linear transformation function. In the second variant, words\nare associated with vectors as usual, but the composition function becomes more expressive\nby basing it on tensor instead of matrix operations.\n12.3 Training Recursive Neural Networks\nThe training procedure for a recursive neural network follows the same recipe as training\nother forms of networks: deﬁne a loss, spell out the computation graph, compute gradients\nusing backpropagation40, and train the parameters using SGD.\nWith regard to the loss function, similar to the sequence RNN one can associate a loss\neither with the root of the tree, with any given node, or with a set of nodes, in which case\nthe individual node’s losses are combined, usually by summation. The loss function is based\non the labeled training data which associates a label or other quantity with diﬀerent tree\nnodes.\nAdditionally, one can treat the RecNN as an Encoder, whereas the inside-vector associ-\nated with a node is taken to be an encoding of the tree rooted at that node. The encoding\ncan potentially be sensitive to arbitrary properties of the structure. The vector is then\npassed as input to another network.\nFor further discussion on recursive neural networks and their use in natural language\ntasks, refer to the PhD thesis of Richard Socher (2014).\n40. Before the introduction of the computation graph abstraction, the speciﬁc backpropagation procedure\nfor computing the gradients in a RecNN as deﬁned above was referred to as the Back-propagation trough\nStructure (BPTS) algorithm (Goller & K¨uchler, 1996).\n62\n13. Conclusions\nNeural networks are powerful learners, providing opportunities ranging from non-linear\nclassiﬁcation to non-Markovian modeling of sequences and trees. We hope that this expo-\nsition help NLP researchers to incorporate neural network models in their work and take\nadvantage of their power.\nReferences\nAdel, H., Vu, N. T., & Schultz, T. (2013). Combination of Recurrent Neural Networks and\nFactored Language Models for Code-Switching Language Modeling. In Proceedings\nof the 51st Annual Meeting of the Association for Computational Linguistics (Vol-\nume 2: Short Papers), pp. 206–211, Soﬁa, Bulgaria. Association for Computational\nLinguistics.\nAndo, R., & Zhang, T. (2005a). A High-Performance Semi-Supervised Learning Method\nfor Text Chunking. In Proceedings of the 43rd Annual Meeting of the Association for\nComputational Linguistics (ACL’05), pp. 1–9, Ann Arbor, Michigan. Association for\nComputational Linguistics.\nAndo, R. K., & Zhang, T. (2005b). A framework for learning predictive structures from\nmultiple tasks and unlabeled data. The Journal of Machine Learning Research, 6,\n1817–1853.\nAuli, M., Galley, M., Quirk, C., & Zweig, G. (2013). Joint Language and Translation Mod-\neling with Recurrent Neural Networks.\nIn Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1044–1054, Seattle, Washing-\nton, USA. Association for Computational Linguistics.\nAuli, M., & Gao, J. (2014). Decoder Integration and Expected BLEU Training for Recurrent\nNeural Network Language Models. In Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pp. 136–142,\nBaltimore, Maryland. Association for Computational Linguistics.\nBallesteros, M., Dyer, C., & Smith, N. A. (2015). Improved Transition-based Parsing by\nModeling Characters instead of Words with LSTMs. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language Processing, pp. 349–359, Lisbon,\nPortugal. Association for Computational Linguistics.\nBansal, M., Gimpel, K., & Livescu, K. (2014). Tailoring Continuous Word Representations\nfor Dependency Parsing. In Proceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers), pp. 809–815, Baltimore,\nMaryland. Association for Computational Linguistics.\nBaydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2015).\nAutomatic\ndiﬀerentiation in machine learning: a survey. arXiv:1502.05767 [cs].\nBengio, Y. (2012). Practical recommendations for gradient-based training of deep architec-\ntures. arXiv:1206.5533 [cs].\nBengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A Neural Probabilistic Lan-\nguage Model. J. Mach. Learn. Res., 3, 1137–1155.\n63\nBengio, Y., Goodfellow, I. J., & Courville, A. (2015). Deep Learning. Book in preparation\nfor MIT Press.\nBitvai, Z., & Cohn, T. (2015).\nNon-Linear Text Regression with a Deep Convolutional\nNeural Network. In Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference on Natural Lan-\nguage Processing (Volume 2: Short Papers), pp. 180–185, Beijing, China. Association\nfor Computational Linguistics.\nBotha, J. A., & Blunsom, P. (2014). Compositional Morphology for Word Representations\nand Language Modelling.\nIn Proceedings of the 31st International Conference on\nMachine Learning (ICML), Beijing, China. *Award for best application paper*.\nBottou, L. (2012). Stochastic gradient descent tricks. In Neural Networks: Tricks of the\nTrade, pp. 421–436. Springer.\nCharniak, E., & Johnson, M. (2005). Coarse-to-Fine n-Best Parsing and MaxEnt Discrim-\ninative Reranking. In Proceedings of the 43rd Annual Meeting of the Association for\nComputational Linguistics (ACL’05), pp. 173–180, Ann Arbor, Michigan. Association\nfor Computational Linguistics.\nChen, D., & Manning, C. (2014). A Fast and Accurate Dependency Parser using Neural\nNetworks. In Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 740–750, Doha, Qatar. Association for Compu-\ntational Linguistics.\nChen, Y., Xu, L., Liu, K., Zeng, D., & Zhao, J. (2015). Event Extraction via Dynamic\nMulti-Pooling Convolutional Neural Networks.\nIn Proceedings of the 53rd Annual\nMeeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 167–\n176, Beijing, China. Association for Computational Linguistics.\nCho, K., van Merrienboer, B., Bahdanau, D., & Bengio, Y. (2014a). On the Properties of\nNeural Machine Translation: Encoder–Decoder Approaches. In Proceedings of SSST-\n8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,\npp. 103–111, Doha, Qatar. Association for Computational Linguistics.\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &\nBengio, Y. (2014b). Learning Phrase Representations using RNN Encoder–Decoder for\nStatistical Machine Translation. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pp. 1724–1734, Doha, Qatar.\nAssociation for Computational Linguistics.\nChrupala, G. (2014). Normalizing tweets with edit scripts and recurrent neural embeddings.\nIn Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pp. 680–686, Baltimore, Maryland. Association for\nComputational Linguistics.\nChung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical Evaluation of Gated\nRecurrent Neural Networks on Sequence Modeling. arXiv:1412.3555 [cs].\nCollins, M. (2002). Discriminative Training Methods for Hidden Markov Models: Theory\nand Experiments with Perceptron Algorithms. In Proceedings of the 2002 Confer-\n64\nence on Empirical Methods in Natural Language Processing, pp. 1–8. Association for\nComputational Linguistics.\nCollins, M., & Koo, T. (2005). Discriminative Reranking for Natural Language Parsing.\nComputational Linguistics, 31(1), 25–70.\nCollobert, R., & Weston, J. (2008). A uniﬁed architecture for natural language processing:\nDeep neural networks with multitask learning. In Proceedings of the 25th international\nconference on Machine learning, pp. 160–167. ACM.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).\nNatural language processing (almost) from scratch. The Journal of Machine Learning\nResearch, 12, 2493–2537.\nCrammer, K., & Singer, Y. (2002). On the algorithmic implementation of multiclass kernel-\nbased vector machines. The Journal of Machine Learning Research, 2, 265–292.\nCreutz, M., & Lagus, K. (2007). Unsupervised Models for Morpheme Segmentation and\nMorphology Learning. ACM Trans. Speech Lang. Process., 4(1), 3:1–3:34.\nCybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics\nof Control, Signals and Systems, 2(4), 303–314.\nDahl, G., Sainath, T., & Hinton, G. (2013). Improving deep neural networks for LVCSR\nusing rectiﬁed linear units and dropout. In 2013 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 8609–8613.\nde Gispert, A., Iglesias, G., & Byrne, B. (2015). Fast and Accurate Preordering for SMT\nusing Neural Networks. In Proceedings of the 2015 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, pp. 1012–1017, Denver, Colorado. Association for Computational Linguistics.\nDong, L., Wei, F., Tan, C., Tang, D., Zhou, M., & Xu, K. (2014). Adaptive Recursive Neural\nNetwork for Target-dependent Twitter Sentiment Classiﬁcation.\nIn Proceedings of\nthe 52nd Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), pp. 49–54, Baltimore, Maryland. Association for Computational\nLinguistics.\nDong, L., Wei, F., Zhou, M., & Xu, K. (2015). Question Answering over Freebase with\nMulti-Column Convolutional Neural Networks.\nIn Proceedings of the 53rd Annual\nMeeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 260–\n269, Beijing, China. Association for Computational Linguistics.\ndos Santos, C., & Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment\nAnalysis of Short Texts. In Proceedings of COLING 2014, the 25th International Con-\nference on Computational Linguistics: Technical Papers, pp. 69–78, Dublin, Ireland.\nDublin City University and Association for Computational Linguistics.\ndos Santos, C., Xiang, B., & Zhou, B. (2015).\nClassifying Relations by Ranking with\nConvolutional Neural Networks. In Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1: Long Papers), pp. 626–634, Beijing,\nChina. Association for Computational Linguistics.\n65\nDuchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning\nand stochastic optimization. The Journal of Machine Learning Research, 12, 2121–\n2159.\nDuh, K., Neubig, G., Sudoh, K., & Tsukada, H. (2013). Adaptation Data Selection us-\ning Neural Language Models: Experiments in Machine Translation. In Proceedings\nof the 51st Annual Meeting of the Association for Computational Linguistics (Vol-\nume 2: Short Papers), pp. 678–683, Soﬁa, Bulgaria. Association for Computational\nLinguistics.\nDurrett, G., & Klein, D. (2015). Neural CRF Parsing. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 302–\n312, Beijing, China. Association for Computational Linguistics.\nDyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-\nBased Dependency Parsing with Stack Long Short-Term Memory. In Proceedings of\nthe 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pp. 334–343, Beijing, China. Association for Computational Linguistics.\nElman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14(2), 179–211.\nFaruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using Mul-\ntilingual Correlation. In Proceedings of the 14th Conference of the European Chapter\nof the Association for Computational Linguistics, pp. 462–471, Gothenburg, Sweden.\nAssociation for Computational Linguistics.\nFilippova, K., Alfonseca, E., Colmenares, C. A., Kaiser, L., & Vinyals, O. (2015). Sentence\nCompression by Deletion with LSTMs.\nIn Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing, pp. 360–368, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nGal, Y., & Ghahramani, Z. (2015). Dropout as a Bayesian Approximation: Representing\nModel Uncertainty in Deep Learning. arXiv:1506.02142 [cs, stat].\nGao, J., Pantel, P., Gamon, M., He, X., & Deng, L. (2014). Modeling Interestingness with\nDeep Neural Networks. In Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pp. 2–13, Doha, Qatar. Association for\nComputational Linguistics.\nGim´enez, J., & M`arquez, L. (2004). SVMTool: A general POS tagger generator based on\nSupport Vector Machines. In Proceedings of the 4th LREC, Lisbon, Portugal.\nGlorot, X., & Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward\nneural networks. In International conference on artiﬁcial intelligence and statistics,\npp. 249–256.\nGlorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectiﬁer neural networks. In\nInternational Conference on Artiﬁcial Intelligence and Statistics, pp. 315–323.\nGoldberg, Y., & Elhadad, M. (2010). An Eﬃcient Algorithm for Easy-First Non-Directional\nDependency Parsing. In Human Language Technologies: The 2010 Annual Conference\n66\nof the North American Chapter of the Association for Computational Linguistics, pp.\n742–750, Los Angeles, California. Association for Computational Linguistics.\nGoldberg, Y., & Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-\nsampling word-embedding method. arXiv:1402.3722 [cs, stat].\nGoldberg, Y., & Nivre, J. (2013). Training Deterministic Parsers with Non-Deterministic\nOracles. Transactions of the Association for Computational Linguistics, 1(0), 403–\n414.\nGoldberg, Y., Zhao, K., & Huang, L. (2013).\nEﬃcient Implementation of Beam-Search\nIncremental Parsers. In Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), pp. 628–633, Soﬁa, Bulgaria.\nAssociation for Computational Linguistics.\nGoller, C., & K¨uchler, A. (1996). Learning Task-Dependent Distributed Representations\nby Backpropagation Through Structure. In In Proc. of the ICNN-96, pp. 347–352.\nIEEE.\nGraves, A. (2008).\nSupervised sequence labelling with recurrent neural networks.\nPh.D.\nthesis, Technische Universit¨at M¨unchen.\nGreﬀ, K., Srivastava, R. K., Koutn´ık, J., Steunebrink, B. R., & Schmidhuber, J. (2015).\nLSTM: A Search Space Odyssey. arXiv:1503.04069 [cs].\nHal Daum´e III, Langford, J., & Marcu, D. (2009). Search-based Structured Prediction.\nMachine Learning Journal (MLJ).\nHarris, Z. (1954). Distributional Structure. Word, 10(23), 146–162.\nHashimoto, K., Miwa, M., Tsuruoka, Y., & Chikayama, T. (2013). Simple Customization\nof Recursive Neural Networks for Semantic Relation Classiﬁcation. In Proceedings\nof the 2013 Conference on Empirical Methods in Natural Language Processing, pp.\n1372–1376, Seattle, Washington, USA. Association for Computational Linguistics.\nHe, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectiﬁers: Surpassing\nHuman-Level Performance on ImageNet Classiﬁcation. arXiv:1502.01852 [cs].\nHenderson, M., Thomson, B., & Young, S. (2013). Deep Neural Network Approach for the\nDialog State Tracking Challenge. In Proceedings of the SIGDIAL 2013 Conference,\npp. 467–471, Metz, France. Association for Computational Linguistics.\nHermann, K. M., & Blunsom, P. (2013). The Role of Syntax in Vector Space Models of\nCompositional Semantics. In Proceedings of the 51st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers), pp. 894–904, Soﬁa,\nBulgaria. Association for Computational Linguistics.\nHermann, K. M., & Blunsom, P. (2014). Multilingual Models for Compositional Distributed\nSemantics. In Proceedings of the 52nd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pp. 58–68, Baltimore, Maryland.\nAssociation for Computational Linguistics.\nHihi, S. E., & Bengio, Y. (1996). Hierarchical Recurrent Neural Networks for Long-Term\nDependencies. In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), Advances\nin Neural Information Processing Systems 8, pp. 493–499. MIT Press.\n67\nHinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R.\n(2012). Improving neural networks by preventing co-adaptation of feature detectors.\narXiv:1207.0580 [cs].\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation,\n9(8), 1735–1780.\nHornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are\nuniversal approximators. Neural Networks, 2(5), 359–366.\nIrsoy, O., & Cardie, C. (2014). Opinion Mining with Deep Recurrent Neural Networks.\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 720–728, Doha, Qatar. Association for Computational Lin-\nguistics.\nIyyer, M., Boyd-Graber, J., Claudino, L., Socher, R., & Daum´e III, H. (2014a). A Neural\nNetwork for Factoid Question Answering over Paragraphs. In Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing (EMNLP), pp.\n633–644, Doha, Qatar. Association for Computational Linguistics.\nIyyer, M., Enns, P., Boyd-Graber, J., & Resnik, P. (2014b). Political Ideology Detection\nUsing Recursive Neural Networks. In Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 1113–1122,\nBaltimore, Maryland. Association for Computational Linguistics.\nIyyer, M., Manjunatha, V., Boyd-Graber, J., & Daum´e III, H. (2015). Deep Unordered\nComposition Rivals Syntactic Methods for Text Classiﬁcation. In Proceedings of the\n53rd Annual Meeting of the Association for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Pa-\npers), pp. 1681–1691, Beijing, China. Association for Computational Linguistics.\nJohnson, R., & Zhang, T. (2014). Eﬀective Use of Word Order for Text Categorization with\nConvolutional Neural Networks. arXiv:1412.1058 [cs, stat].\nJohnson, R., & Zhang, T. (2015). Eﬀective Use of Word Order for Text Categorization with\nConvolutional Neural Networks. In Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pp. 103–112, Denver, Colorado. Association for Computational\nLinguistics.\nJozefowicz, R., Zaremba, W., & Sutskever, I. (2015). An Empirical Exploration of Recur-\nrent Network Architectures. In Proceedings of the 32nd International Conference on\nMachine Learning (ICML-15), pp. 2342–2350.\nKalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A Convolutional Neural Network\nfor Modelling Sentences. In Proceedings of the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers), pp. 655–665, Baltimore,\nMaryland. Association for Computational Linguistics.\nKarpathy, A., Johnson, J., & Li, F.-F. (2015). Visualizing and Understanding Recurrent\nNetworks. arXiv:1506.02078 [cs].\n68\nKim, Y. (2014). Convolutional Neural Networks for Sentence Classiﬁcation. In Proceed-\nings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pp. 1746–1751, Doha, Qatar. Association for Computational Linguistics.\nKingma, D., & Ba, J. (2014).\nAdam: A Method for Stochastic Optimization.\narXiv:1412.6980 [cs].\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classiﬁcation with Deep\nConvolutional Neural Networks. In Pereira, F., Burges, C. J. C., Bottou, L., & Wein-\nberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 25, pp.\n1097–1105. Curran Associates, Inc.\nKudo, T., & Matsumoto, Y. (2003).\nFast Methods for Kernel-based Text Analysis.\nIn\nProceedings of the 41st Annual Meeting on Association for Computational Linguistics -\nVolume 1, ACL ’03, pp. 24–31, Stroudsburg, PA, USA. Association for Computational\nLinguistics.\nLe, P., & Zuidema, W. (2014). The Inside-Outside Recursive Neural Network model for\nDependency Parsing. In Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 729–739, Doha, Qatar. Association for\nComputational Linguistics.\nLe, P., & Zuidema, W. (2015). The Forest Convolutional Network: Compositional Distri-\nbutional Semantics with a Neural Chart and without Binarization. In Proceedings\nof the 2015 Conference on Empirical Methods in Natural Language Processing, pp.\n1155–1164, Lisbon, Portugal. Association for Computational Linguistics.\nLe, Q. V., Jaitly, N., & Hinton, G. E. (2015). A Simple Way to Initialize Recurrent Networks\nof Rectiﬁed Linear Units. arXiv:1504.00941 [cs].\nLeCun, Y., & Bengio, Y. (1995). Convolutional Networks for Images, Speech, and Time-\nSeries. In Arbib, M. A. (Ed.), The Handbook of Brain Theory and Neural Networks.\nMIT Press.\nLeCun, Y., Bottou, L., Orr, G., & Muller, K. (1998a). Eﬃcient BackProp. In Orr, G., &\nK, M. (Eds.), Neural Networks: Tricks of the trade. Springer.\nLecun, Y., Bottou, L., Bengio, Y., & Haﬀner, P. (1998b). Gradient Based Learning Applied\nto Pattern Recognition..\nLeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energy-\nbased learning. Predicting structured data, 1, 0.\nLeCun, Y., & Huang, F. (2005). Loss functions for discriminative training of energybased\nmodels.. AIStats.\nLevy, O., & Goldberg, Y. (2014a). Dependency-Based Word Embeddings. In Proceedings of\nthe 52nd Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), pp. 302–308, Baltimore, Maryland. Association for Computational\nLinguistics.\nLevy, O., & Goldberg, Y. (2014b). Neural Word Embedding as Implicit Matrix Factoriza-\ntion. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., & Weinberger,\n69\nK. Q. (Eds.), Advances in Neural Information Processing Systems 27, pp. 2177–2185.\nCurran Associates, Inc.\nLevy, O., Goldberg, Y., & Dagan, I. (2015).\nImproving Distributional Similarity with\nLessons Learned from Word Embeddings. Transactions of the Association for Com-\nputational Linguistics, 3(0), 211–225.\nLewis, M., & Steedman, M. (2014). Improved CCG Parsing with Semi-supervised Supertag-\nging. Transactions of the Association for Computational Linguistics, 2(0), 327–338.\nLi, J., Li, R., & Hovy, E. (2014). Recursive Deep Models for Discourse Parsing. In Proceed-\nings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pp. 2061–2069, Doha, Qatar. Association for Computational Linguistics.\nLing, W., Dyer, C., Black, A. W., & Trancoso, I. (2015a). Two/Too Simple Adaptations of\nWord2Vec for Syntax Problems. In Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pp. 1299–1304, Denver, Colorado. Association for Computational\nLinguistics.\nLing, W., Dyer, C., Black, A. W., Trancoso, I., Fermandez, R., Amir, S., Marujo, L., &\nLuis, T. (2015b). Finding Function in Form: Compositional Character Models for\nOpen Vocabulary Word Representation. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1520–1530, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nLiu, Y., Wei, F., Li, S., Ji, H., Zhou, M., & WANG, H. (2015). A Dependency-Based Neural\nNetwork for Relation Classiﬁcation. In Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2: Short Papers), pp. 285–290, Beijing,\nChina. Association for Computational Linguistics.\nMa, J., Zhang, Y., & Zhu, J. (2014). Tagging The Web: Building A Robust Web Tagger\nwith Neural Network. In Proceedings of the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers), pp. 144–154, Baltimore,\nMaryland. Association for Computational Linguistics.\nMa, M., Huang, L., Zhou, B., & Xiang, B. (2015). Dependency-based Convolutional Neural\nNetworks for Sentence Embedding. In Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2: Short Papers), pp. 174–179, Beijing,\nChina. Association for Computational Linguistics.\nMcCallum, A., Freitag, D., & Pereira, F. C. (2000). Maximum Entropy Markov Models for\nInformation Extraction and Segmentation.. In ICML, Vol. 17, pp. 591–598.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013).\nEﬃcient Estimation of Word\nRepresentations in Vector Space. arXiv:1301.3781 [cs].\nMikolov, T., Joulin, A., Chopra, S., Mathieu, M., & Ranzato, M. (2014). Learning Longer\nMemory in Recurrent Neural Networks. arXiv:1412.7753 [cs].\n70\nMikolov, T., Karaﬁ´at, M., Burget, L., Cernocky, J., & Khudanpur, S. (2010). Recurrent\nneural network based language model.. In INTERSPEECH 2010, 11th Annual Con-\nference of the International Speech Communication Association, Makuhari, Chiba,\nJapan, September 26-30, 2010, pp. 1045–1048.\nMikolov, T., Kombrink, S., Luk´aˇs Burget, ˇCernocky, J. H., & Khudanpur, S. (2011). Ex-\ntensions of recurrent neural network language model. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2011 IEEE International Conference on, pp. 5528–5531. IEEE.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed Rep-\nresentations of Words and Phrases and their Compositionality. In Burges, C. J. C.,\nBottou, L., Welling, M., Ghahramani, Z., & Weinberger, K. Q. (Eds.), Advances in\nNeural Information Processing Systems 26, pp. 3111–3119. Curran Associates, Inc.\nMikolov, T. (2012). Statistical language models based on neural networks. Ph.D. thesis, Ph.\nD. thesis, Brno University of Technology.\nMnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings eﬃciently with noise-\ncontrastive estimation. In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z.,\n& Weinberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 26,\npp. 2265–2273. Curran Associates, Inc.\nMrkˇsi´c, N., ´O S´eaghdha, D., Thomson, B., Gasic, M., Su, P.-H., Vandyke, D., Wen, T.-H.,\n& Young, S. (2015).\nMulti-domain Dialog State Tracking using Recurrent Neural\nNetworks. In Proceedings of the 53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pp. 794–799, Beijing, China. Association for\nComputational Linguistics.\nNeidinger, R. (2010).\nIntroduction to Automatic Diﬀerentiation and MATLAB Object-\nOriented Programming. SIAM Review, 52(3), 545–563.\nNguyen, T. H., & Grishman, R. (2015). Event Detection and Domain Adaptation with\nConvolutional Neural Networks. In Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2: Short Papers), pp. 365–371, Beijing,\nChina. Association for Computational Linguistics.\nNivre, J. (2008). Algorithms for Deterministic Incremental Dependency Parsing. Compu-\ntational Linguistics, 34(4), 513–553.\nOkasaki, C. (1999). Purely Functional Data Structures. Cambridge University Press, Cam-\nbridge, U.K.; New York.\nPascanu, R., Mikolov, T., & Bengio, Y. (2012). On the diﬃculty of training Recurrent\nNeural Networks. arXiv:1211.5063 [cs].\nPei, W., Ge, T., & Chang, B. (2015). An Eﬀective Neural Network Model for Graph-based\nDependency Parsing. In Proceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 313–322, Beijing, China. Associa-\ntion for Computational Linguistics.\n71\nPennington, J., Socher, R., & Manning, C. (2014). Glove: Global Vectors for Word Rep-\nresentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 1532–1543, Doha, Qatar. Association for Com-\nputational Linguistics.\nPollack, J. B. (1990). Recursive Distributed Representations. Artiﬁcial Intelligence, 46,\n77–105.\nPolyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.\nUSSR Computational Mathematics and Mathematical Physics, 4(5), 1 – 17.\nQian, Q., Tian, B., Huang, M., Liu, Y., Zhu, X., & Zhu, X. (2015). Learning Tag Embeddings\nand Tag-speciﬁc Composition Functions in Recursive Neural Network. In Proceedings\nof the 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pp. 1365–1374, Beijing, China. Association for Computational Linguistics.\nRong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738 [cs].\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by\nback-propagating errors. Nature, 323(6088), 533–536.\nSantos, C. D., & Zadrozny, B. (2014). Learning Character-level Representations for Part-\nof-Speech Tagging.. pp. 1818–1826.\nSchuster, M., & Paliwal, K. K. (1997).\nBidirectional recurrent neural networks.\nIEEE\nTransactions on Signal Processing, 45(11), 2673–2681.\nShawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods for Pattern Analysis. Cambridge\nUniversity Press.\nSmith, N. A. (2011). Linguistic Structure Prediction. Synthesis Lectures on Human Lan-\nguage Technologies. Morgan and Claypool.\nSocher, R. (2014). Recursive Deep Learning For Natural Language Processing and Computer\nVision. Ph.D. thesis, Stanford University.\nSocher, R., Bauer, J., Manning, C. D., & Andrew Y., N. (2013). Parsing with Compositional\nVector Grammars. In Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 455–465, Soﬁa, Bulgaria.\nAssociation for Computational Linguistics.\nSocher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic Compositionality\nthrough Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Joint Conference\non Empirical Methods in Natural Language Processing and Computational Natural\nLanguage Learning, pp. 1201–1211, Jeju Island, Korea. Association for Computational\nLinguistics.\nSocher, R., Lin, C. C.-Y., Ng, A. Y., & Manning, C. D. (2011). Parsing Natural Scenes\nand Natural Language with Recursive Neural Networks. In Getoor, L., & Scheﬀer, T.\n(Eds.), Proceedings of the 28th International Conference on Machine Learning, ICML\n2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 129–136. Omnipress.\n72\nSocher, R., Manning, C., & Ng, A. (2010). Learning Continuous Phrase Representations\nand Syntactic Parsing with Recursive Neural Networks. In Proceedings of the Deep\nLearning and Unsupervised Feature Learning Workshop of {NIPS} 2010, pp. 1–9.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., & Potts, C. (2013).\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1631–1642, Seattle, Washington, USA. Association for Computational\nLinguistics.\nSordoni, A., Galley, M., Auli, M., Brockett, C., Ji, Y., Mitchell, M., Nie, J.-Y., Gao, J.,\n& Dolan, B. (2015). A Neural Network Approach to Context-Sensitive Generation\nof Conversational Responses.\nIn Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pp. 196–205, Denver, Colorado. Association for Computational\nLinguistics.\nSundermeyer, M., Alkhouli, T., Wuebker, J., & Ney, H. (2014).\nTranslation Modeling\nwith Bidirectional Recurrent Neural Networks. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pp. 14–25, Doha,\nQatar. Association for Computational Linguistics.\nSundermeyer, M., Schl¨uter, R., & Ney, H. (2012). LSTM Neural Networks for Language\nModeling.. In INTERSPEECH.\nSutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization\nand momentum in deep learning. In Proceedings of the 30th international conference\non machine learning (ICML-13), pp. 1139–1147.\nSutskever, I., Martens, J., & Hinton, G. E. (2011). Generating text with recurrent neural\nnetworks. In Proceedings of the 28th International Conference on Machine Learning\n(ICML-11), pp. 1017–1024.\nSutskever, I., Vinyals, O., & Le, Q. V. V. (2014). Sequence to Sequence Learning with\nNeural Networks. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., &\nWeinberger, K. Q. (Eds.), Advances in Neural Information Processing Systems 27, pp.\n3104–3112. Curran Associates, Inc.\nTai, K. S., Socher, R., & Manning, C. D. (2015). Improved Semantic Representations From\nTree-Structured Long Short-Term Memory Networks. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npp. 1556–1566, Beijing, China. Association for Computational Linguistics.\nTamura, A., Watanabe, T., & Sumita, E. (2014). Recurrent Neural Networks for Word\nAlignment Model.\nIn Proceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 1470–1480, Baltimore,\nMaryland. Association for Computational Linguistics.\nTieleman, T., & Hinton, G. (2012). Lecture 6.5—RmsProp: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural Networks for Machine Learning.\n73\nVan de Cruys, T. (2014). A Neural Network Approach to Selectional Preference Acquisi-\ntion. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pp. 26–35, Doha, Qatar. Association for Computational\nLinguistics.\nVaswani, A., Zhao, Y., Fossum, V., & Chiang, D. (2013). Decoding with Large-Scale Neu-\nral Language Models Improves Translation. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1387–1392, Seattle, Washing-\nton, USA. Association for Computational Linguistics.\nWager, S., Wang, S., & Liang, P. S. (2013). Dropout Training as Adaptive Regularization.\nIn Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., & Weinberger, K. Q.\n(Eds.), Advances in Neural Information Processing Systems 26, pp. 351–359. Curran\nAssociates, Inc.\nWang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., & Hao, H. (2015a). Semantic Cluster-\ning and Convolutional Neural Network for Short Text Categorization. In Proceedings\nof the 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing (Volume 2: Short\nPapers), pp. 352–357, Beijing, China. Association for Computational Linguistics.\nWang, X., Liu, Y., SUN, C., Wang, B., & Wang, X. (2015b). Predicting Polarities of Tweets\nby Composing Word Embeddings with Long Short-Term Memory. In Proceedings of\nthe 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pp. 1343–1353, Beijing, China. Association for Computational Linguistics.\nWatanabe, T., & Sumita, E. (2015). Transition-based Neural Constituent Parsing. In Pro-\nceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), pp. 1169–1179, Beijing, China. Association for Computational Lin-\nguistics.\nWeiss, D., Alberti, C., Collins, M., & Petrov, S. (2015). Structured Training for Neural\nNetwork Transition-Based Parsing. In Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1: Long Papers), pp. 323–333, Beijing,\nChina. Association for Computational Linguistics.\nWerbos, P. J. (1990).\nBackpropagation through time: What it does and how to do it..\nProceedings of the IEEE, 78(10), 1550 – 1560.\nWeston, J., Bordes, A., Yakhnenko, O., & Usunier, N. (2013). Connecting Language and\nKnowledge Bases with Embedding Models for Relation Extraction. In Proceedings\nof the 2013 Conference on Empirical Methods in Natural Language Processing, pp.\n1366–1371, Seattle, Washington, USA. Association for Computational Linguistics.\nXu, W., Auli, M., & Clark, S. (2015). CCG Supertagging with a Recurrent Neural Network.\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference on Natural Language Processing\n(Volume 2: Short Papers), pp. 250–255, Beijing, China. Association for Computational\nLinguistics.\n74\nYin, W., & Sch¨utze, H. (2015). Convolutional Neural Network for Paraphrase Identiﬁcation.\nIn Proceedings of the 2015 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies, pp. 901–911,\nDenver, Colorado. Association for Computational Linguistics.\nZaremba, W., Sutskever, I., & Vinyals, O. (2014). Recurrent Neural Network Regularization.\narXiv:1409.2329 [cs].\nZeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. arXiv:1212.5701\n[cs].\nZeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation Classiﬁcation via Convolu-\ntional Deep Neural Network. In Proceedings of COLING 2014, the 25th International\nConference on Computational Linguistics: Technical Papers, pp. 2335–2344, Dublin,\nIreland. Dublin City University and Association for Computational Linguistics.\nZhou, H., Zhang, Y., Huang, S., & Chen, J. (2015). A Neural Probabilistic Structured-\nPrediction Model for Transition-Based Dependency Parsing. In Proceedings of the\n53rd Annual Meeting of the Association for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Pa-\npers), pp. 1213–1222, Beijing, China. Association for Computational Linguistics.\nZhu, C., Qiu, X., Chen, X., & Huang, X. (2015a). A Re-ranking Model for Dependency\nParser with Recursive Convolutional Neural Network.\nIn Proceedings of the 53rd\nAnnual Meeting of the Association for Computational Linguistics and the 7th Inter-\nnational Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npp. 1159–1168, Beijing, China. Association for Computational Linguistics.\nZhu, X., Sobhani, P., & Guo, H. (2015b). Long Short-Term Memory Over Tree Structures.\narXiv:1503.04881 [cs].\n75\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2015-10-02",
  "updated": "2015-10-02"
}