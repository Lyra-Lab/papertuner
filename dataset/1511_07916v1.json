{
  "id": "http://arxiv.org/abs/1511.07916v1",
  "title": "Natural Language Understanding with Distributed Representation",
  "authors": [
    "Kyunghyun Cho"
  ],
  "abstract": "This is a lecture note for the course DS-GA 3001 <Natural Language\nUnderstanding with Distributed Representation> at the Center for Data Science ,\nNew York University in Fall, 2015. As the name of the course suggests, this\nlecture note introduces readers to a neural network based approach to natural\nlanguage understanding/processing. In order to make it as self-contained as\npossible, I spend much time on describing basics of machine learning and neural\nnetworks, only after which how they are used for natural languages is\nintroduced. On the language front, I almost solely focus on language modelling\nand machine translation, two of which I personally find most fascinating and\nmost fundamental to natural language understanding.",
  "text": "Natural Language Understanding with\nDistributed Representation\nKyunghyun Cho\nCourant Institute of Mathematical Sciences and\nCenter for Data Science,\nNew York University\nNovember 26, 2015\narXiv:1511.07916v1  [cs.CL]  24 Nov 2015\nAbstract\nThis is a lecture note for the course DS-GA 3001 ⟨Natural Language Understanding\nwith Distributed Representation⟩at the Center for Data Science1, New York University\nin Fall, 2015. As the name of the course suggests, this lecture note introduces readers\nto a neural network based approach to natural language understanding/processing. In\norder to make it as self-contained as possible, I spend much time on describing basics of\nmachine learning and neural networks, only after which how they are used for natural\nlanguages is introduced. On the language front, I almost solely focus on language\nmodelling and machine translation, two of which I personally ﬁnd most fascinating\nand most fundamental to natural language understanding.\nAfter about a month of lectures and about 40 pages of writing this lecture note, I\nfound this fascinating note [47] by Yoav Goldberg on neural network models for natural\nlanguage processing. This note deals with wider topics on natural language processing\nwith distributed representations in more details, and I highly recommend you to read it\n(hopefully along with this lecture note.) I seriously wish Yoav had written it earlier so\nthat I could’ve simply used his excellent note for my course.\nThis lecture note had been written quite hastily as the course progressed, meaning\nthat I could spare only about 100 hours in total for this note. This is my lame excuse\nfor likely many mistakes in this lecture note, and I kindly ask for your understanding\nin advance. Again, how grateful I would’ve been had I found Yoav’s note earlier.\nI am planning to update this lecture note gradually over time, hoping that I will\nbe able to convince the Center for Data Science to let me teach the same course next\nyear. The latest version will always be available both in pdf and in latex source code\nfrom https://github.com/nyu-dl/NLP_DL_Lecture_Note. The arXiv\nversion will be updated whenever a major revision is made.\nI thank all the students and non-students who took2 this course and David Rosen-\nberg for feedback.\n1 http://cds.nyu.edu/\n2 In fact, they are still taking the course as of 24 Nov 2015. They have two guest lectures and a ﬁnal exam\nleft until the end of the course.\nContents\n1\nIntroduction\n5\n1.1\nRoute we will not take\n. . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.1.1\nWhat is Language? . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.1.2\nLanguage Understanding . . . . . . . . . . . . . . . . . . . .\n6\n1.2\nRoad we will take . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n1.2.1\nLanguage as a Function\n. . . . . . . . . . . . . . . . . . . .\n8\n1.2.2\nLanguage Understanding as a Function Approximation . . . .\n8\n2\nFunction Approximation as Supervised Learning\n11\n2.1\nFunction Approximation: Parametric Approach . . . . . . . . . . . .\n11\n2.1.1\nExpected Cost Function\n. . . . . . . . . . . . . . . . . . . .\n11\n2.1.2\nEmpirical Cost Function . . . . . . . . . . . . . . . . . . . .\n12\n2.2\nLearning as Optimization . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.2.1\nGradient-based Local Iterative Optimization . . . . . . . . . .\n13\n2.2.2\nStochastic Gradient Descent . . . . . . . . . . . . . . . . . .\n14\n2.3\nWhen do we stop learning? . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.3.1\nEarly Stopping . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.3.2\nModel Selection\n. . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.4\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.5\nLinear Regression for Non-Linear Functions . . . . . . . . . . . . . .\n20\n2.5.1\nFeature Extraction\n. . . . . . . . . . . . . . . . . . . . . . .\n20\n3\nNeural Networks and Backpropagation Algorithm\n22\n3.1\nConditional Distribution Approximation . . . . . . . . . . . . . . . .\n22\n3.1.1\nWhy do we want to do this? . . . . . . . . . . . . . . . . . .\n24\n3.1.2\nOther Distributions . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.2\nFeature Extraction is also a Function . . . . . . . . . . . . . . . . . .\n25\n3.3\nMultilayer Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.3.1\nExample: Binary classiﬁcation with a single hidden unit . . .\n27\n3.3.2\nExample: Binary classiﬁcation with more than one hidden units\n29\n3.4\nAutomating Backpropagation . . . . . . . . . . . . . . . . . . . . . .\n31\n3.4.1\nWhat if a Function is not Differentiable?\n. . . . . . . . . . .\n32\n2\n4\nRecurrent Neural Networks and Gated Recurrent Units\n35\n4.1\nRecurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.1.1\nFixed-Size Output y\n. . . . . . . . . . . . . . . . . . . . . .\n37\n4.1.2\nMultiple Child Nodes and Derivatives . . . . . . . . . . . . .\n38\n4.1.3\nExample: Sentiment Analysis\n. . . . . . . . . . . . . . . . .\n39\n4.1.4\nVariable-Length Output y: |x| = |y|\n. . . . . . . . . . . . . .\n40\n4.2\nGated Recurrent Units\n. . . . . . . . . . . . . . . . . . . . . . . . .\n43\n4.2.1\nMaking Simple Recurrent Neural Networks Realistic . . . . .\n43\n4.2.2\nGated Recurrent Units . . . . . . . . . . . . . . . . . . . . .\n44\n4.2.3\nLong Short-Term Memory . . . . . . . . . . . . . . . . . . .\n46\n4.3\nWhy not Rectiﬁers? . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n4.3.1\nRectiﬁers Explode\n. . . . . . . . . . . . . . . . . . . . . . .\n47\n4.3.2\nIs tanh a Blessing? . . . . . . . . . . . . . . . . . . . . . . .\n49\n4.3.3\nAre We Doomed? . . . . . . . . . . . . . . . . . . . . . . . .\n52\n4.3.4\nGated Recurrent Units Address Vanishing Gradient . . . . . .\n53\n5\nNeural Language Models\n55\n5.1\nLanguage Modeling: First Step . . . . . . . . . . . . . . . . . . . . .\n55\n5.1.1\nWhat if those linguistic structures do exist . . . . . . . . . . .\n56\n5.1.2\nQuick Note on Linguistic Units\n. . . . . . . . . . . . . . . .\n57\n5.2\nStatistical Language Modeling . . . . . . . . . . . . . . . . . . . . .\n58\n5.2.1\nData Sparsity/Scarcity . . . . . . . . . . . . . . . . . . . . .\n59\n5.3\nn-Gram Language Model . . . . . . . . . . . . . . . . . . . . . . . .\n61\n5.3.1\nSmoothing and Back-Off . . . . . . . . . . . . . . . . . . . .\n62\n5.3.2\nLack of Generalization . . . . . . . . . . . . . . . . . . . . .\n65\n5.4\nNeural Language Model\n. . . . . . . . . . . . . . . . . . . . . . . .\n66\n5.4.1\nHow does Neural Language Model Generalize to Unseen n-\nGrams? – Distributional Hypothesis . . . . . . . . . . . . . .\n68\n5.4.2\nContinuous Bag-of-Words Language Model:\nMaximum Pseudo–Likelihood Approach\n. . . . . . . . . . .\n71\n5.4.3\nSemi-Supervised Learning with Pretrained Word Embeddings\n74\n5.5\nRecurrent Language Model . . . . . . . . . . . . . . . . . . . . . . .\n76\n5.6\nHow do n-gram language model, neural language model and RNN-LM\ncompare?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n6\nNeural Machine Translation\n82\n6.1\nStatistical Approach to Machine Translation . . . . . . . . . . . . . .\n82\n6.1.1\nParallel Corpora: Training Data for Machine Translation . . .\n84\n6.1.2\nAutomatic Evaluation Metric . . . . . . . . . . . . . . . . . .\n87\n6.2\nNeural Machine Translation:\nSimple Encoder-Decoder Model . . . . . . . . . . . . . . . . . . . .\n90\n6.2.1\nSampling vs. Decoding . . . . . . . . . . . . . . . . . . . . .\n91\n6.3\nAttention-based Neural Machine Translation . . . . . . . . . . . . . .\n97\n6.3.1\nWhat does the Attention Mechanism do?\n. . . . . . . . . . .\n101\n6.4\nWarren Weaver’s Memorandum\n. . . . . . . . . . . . . . . . . . . .\n103\n3\n7\nFinal Words\n107\n7.1\nMultimedia Description Generation as Translation . . . . . . . . . . .\n107\n7.2\nLanguage Understanding with World Knowledge . . . . . . . . . . .\n109\n7.3\nLarger-Context Language Understanding:\nBeyond Sentences and Beyond Words . . . . . . . . . . . . . . . . .\n112\n7.4\nWarning and Summary . . . . . . . . . . . . . . . . . . . . . . . . .\n113\n4\nChapter 1\nIntroduction\nThis lecture is going to be the only one where I discuss some philosophical, meaning\nnonpractical, arguments, because according to Chris Manning and Hinrich Schuetze,\n“even practically-minded people have to confront the issue of what prior knowledge to\ntry to build into their model” [77].\n1.1\nRoute we will not take\n1.1.1\nWhat is Language?\nThe very ﬁrst question we must ask ourselves before starting this course is the ques-\ntion of what natural language is. Of course, the rest of this course does not in any\nway require us to know what natural language is, but it is a philosophical question I\nrecommend everyone, including myself, to ponder upon once a while.\nWhen I start talking about languages with anyone, there is a single person who\nnever misses to be mentioned, that is Noam Chomsky. His view has greatly inﬂuenced\nthe modern linguistics, and although many linguists I have talked to claim that their\nwork and ﬁeld have long moved on from Chomsky’s, I can feel his shadow all over\nthem.\nMy ﬁrst encounter with Chomsky was at the classroom of <Automata> from my\nearly undergrad years. I was not the most attentive student back then, and all I can\nremember is Chomsky’s hierarchy and how it has shaped our view on languages, in this\ncontext, programming/computer languages. A large part of the course was dedicated\nto explaining which class of languages emerges given a set of constraints on a set of\ngenerating rules, or production rules.\nFor instance, if we are given a set of generating rules that do not depend on the con-\ntext/meaning of non-terminal symbols (context-free grammar, CFG), we get a context-\nfree language. If we put a bit of constraints to CFG that each generating rule is such\nthat a non-terminal symbol is replaced by either a terminal symbol, a terminal symbol\nby a non-terminal symbol or an empty symbol, then we get a regular grammar. Sim-\nilarly to CFG, we get a regular language from the regular grammar, and the regular\n5\nlanguage is a subset of the context-free language.\nWhat Chomsky believes is that this kind of approach applies also to human lan-\nguages, or natural languages. There exists a set of generating rules that generates a\nnatural language. But, then, the obvious question to follow is where those generating\nrules are. Where are they stored? How are they stored? Do we have separate generating\nrules for different languages?\n1.1.2\nLanguage Understanding\nUnderstanding Human Language\nThose questions are interesting, but out of scope\nfor this course. Those questions are the ones linguists try to answer. Generative linguis-\ntics aims at ﬁguring out what those rules are, how they are combined to form a valid\nsentence, how they are adapted to different languages and so on. We will leave these to\nlinguists and continue on to our journey of building a machine that understands human\nlanguages.\nNatural Language Understanding\nSo, let’s put these questions aside and trust Chom-\nsky that we, humans, are specially designed to store those generating rules somewhere\nin the brain [30, 21]. Or, better yet, let’s trust Chomsky that there’s a universal gram-\nmar built in our brain. In other words, let’s say we were born with this set of generating\nrules for natural languages, and while growing, we have adapted this universal gram-\nmar toward our native tongue (language variation).\nWhen we decide to speak of something (whatever that is and however implausi-\nble that is), our brain quickly picks up a sequence of some of those generating rules\nand starts generating a sentence accordingly. Of course, those rules do not generate a\nsentence directly, but generates a sequence of control signals to move our muscles to\nmake sound. When heard by other people who understand your language, the sound\nbecomes a sentence.\nIn our case, we are more interested in a machine hearing that sound, or a sentence\nfrom here on. When a machine heard this sentence, what would/should a language un-\nderstanding machine do to understand a language, or more simply a sentence? Again,\nwe are assuming that this sentence was generated from applying a sequence of the\nexisting generating rules.\nUnder our assumption, a natural ﬁrst step that comes to my mind is to ﬁgure out\nthat sequence of the generating rules which led to the sentence. Once the sequence is\nfound, or in a fancier term, inferred, the next step will be to ﬁgure out what kind of\nmental state of the speaker led to those generating rules.\nLet’s take an example sentence “Our company is training workers” (from Sec. 1.3\nof [77]), which is a horrible choice, because this was used as an example of ambiguity\nin parsing. Regardless, a speaker obviously has an awesome image of her company\nwhich trains its workers and wants to tell a machine about this. This mental state is\nused to select the following generating rules (assuming a phrase structure grammar)1:\n(ROOT\n1 Stanford Parser: http://nlp.stanford.edu:8080/parser\n6\n(S\n(NP (PRP$ Our) (NN company))\n(VP (VBZ is)\n(VP (VBG training)\n(NP (NNS workers))))))\nS\nNP\nOur company\nVP\nAux\nis\nVP\nV\ntraining\nNP\nworkers\nFigure 1.1: A parse of “Our company is training workers”\nThe machine hears the sentence “Our company is training workers” and infers\nthe parse in Fig. 1.1. Then, we can make a simple set of rules (again!) to let the\nmachine answer questions about this sentence, kinds of questions that imply that the\nmachine has understood the sentence (language). For instance, given a question “Who\nis training workers?”, the machine can answer by noticing that the question is asking\nfor the subject of the verb phrase “is training” acted on the object “workers” and that\nthe subject is “Our company”.\nSide Note: Bayesian Language Understanding\nThis generative view of languages\nﬁts quite well with Bayesian modelling (see, e.g., [84].) There exists a hidden mecha-\nnism, or a set of generating rules and a rule governing their composition, which can be\nmodelled as a latent variable Z. Given these rules, a language or a sentence X is gen-\nerated according to the conditional distribution P(X|Z). Then, understanding language\n(by humans) is equivalent to computing the posterior distribution over all possible sets\nof generating rules and their compositional rules (i.e., P(Z|X).) This answers the ques-\ntion of what is the most likely mechanism underlying the observed language.\nFurthermore, from the perspective of machines, Bayesian approach is attractive. In\nthis case, we assume to know the set of rules in advance and let the latent variable Z\ndenote the speciﬁc conﬁguration (use) of those rules. Given this sequence of applying\nthe rules, a sentence X is generated via the conditional distribution P(X|Z). Machine\nunderstanding of language is equivalent to inferring the posterior distribution over Z\ngiven X.\nFor more details about Bayesian approaches (in the context of machine learning),\nplease, refer to [13] or take the course DS-GA 1005 Inference and Representation by\nProf. David Sontag.\n7\nUnderstanding vs. Using\nWhat’s clear from this example is that in this generative\nview of languages, there is a clear separation between understanding and using. In-\nferring the generating rules from a given sentence is understanding, and answering a\nquestion based on this understanding, using, is a separate activity. Understanding part\nis done when the underlying (true) structure has been determined regardless of how\nthis understanding be used.\nTo put it in a slightly different wording, language understanding does not require its\nuse, or downstream tasks. In this road that we will not take in this course, understanding\nexists as it is, regardless of what the understood insight/knowledge will be used for.\nAnd, this is the reason why we do not walk down this road.\n1.2\nRoad we will take\n1.2.1\nLanguage as a Function\nIn this course, we will view a natural/human language as “a system intended to com-\nmunicate ideas from a speaker to a hearer” [110]. What this means is that we do not\nview a language as a separate entity that exists on its own. Rather, we view a whole\nsystem or behaviour of communication as a language. Furthermore, this view dictates\nthat we must take into account the world surrounding a speaker and a hearer in order\nto understand language.\nUnder this view of language, language or rather its usage become somewhat similar\nto action or behaviour. Speaking of something is equivalent to acting on a listener, as\nboth of them inﬂuence the listener in one way or another. The purpose of language\nis then to inﬂuence another by efﬁciently communicate one’s will or intention.2 This\nhints at how language came to be (or may have come to be): (evolution) language\nhas evolved to facilitate the exchange of ideas among people (learning) humans learn\nlanguage by being either encouraged or punished for the use of language. This latter\nview on how language came to be is similar in spirit to the behaviourism of B. F.\nSkinner (“necessary mediation of reinforcement by another organism” [97].)\nThis is a radical departure from the generative view of human language, where\nlanguage existed on its own and its understanding does not necessarily require the\nexistence of the outside world nor the existence of a listener. It is no wonder why\nChomsky was so harsh in criticizing Skinner’s work in [30]. This departure, as I see\nit, is the departure toward a functional view of language. Language is a function of\ncommunication.\n1.2.2\nLanguage Understanding as a Function Approximation\nLet’s make a large jump here such that we consider this function as a mathematical\nfunction. This function (called language) takes as input the state of the surrounding\nworld, the speaker’s speech, either written, spoken or signed and the listener’s mental\n2 Chomsky does not agree: “it is wrong to think of human use of language as characteristically informa-\ntive, in fact or in intention.” [31].\n8\nstate3 Inside the function, the listener’s mental state is updated to incorporate the new\nidea from the speaker’s speech. The function then returns a response by the listener\n(which may include “no response” as well) and a set of non-verbal action sequences\n(what would be the action sequence if the speaker insulted the listener?).\nIn this case, language understanding, both from humans’ and machines’ perspec-\ntive, boils down to ﬁguring out the internal working of this function. In other words, we\nunderstand language by learning the internal mechanism of the function. Furthermore,\nthis view suggests that the underlying structures of language are heavily dependent on\nthe surrounding environment (context) as well as on the target task. The former (con-\ntext dependence) is quite clear, as the function takes as input the context, but the latter\nmay be confusing now. Hopefully, this will become clearer later in the course.\nHow can we approximate this function? How can we ﬁgure out the internal working\nmechanism of this function? What tools do we have?\nLanguage Understanding by Machine Learning\nThis functional view of languages\nsuddenly makes machine learning a very appealing tool for understanding human lan-\nguages. After all, function approximation is the core of machine learning. Classiﬁca-\ntion is a classical example of function approximation, clustering is a function approxi-\nmation where the target is not given, generative modeling learns a function that returns\na probability of an input, and so on.\nWhen we approximate a function in machine learning, the prime ingredient is data.\nWe are given data which was either generated from this function (unsupervised learn-\ning) or well ﬁt this function (supervised learning), based on which we adjust our ap-\nproximation to the function, often iteratively, to best ﬁt the data. But, I must note here\nthat it does not matter how well the approximated function ﬁts the data it was ﬁtted to,\nbut matters how well this approximation ﬁts unseen data.4\nIn language understanding, this means that we collect a large data set of input and\noutput pairs (or conversations together with the recording of the surrounding environ-\nment) and ﬁt some arbitrary function to well predict the output given an input. We\nprobably want to evaluate this approximation in a novel conversation. If this function\nmakes a conversation just like a person, voil`a, we made a machine that passed the\nTuring test. Simple, right?\nProblem\nUnfortunately, as soon as we try to do this, we run into a big problem. This\nproblem is not from machine learning nor languages, but the deﬁnition of this function\nof language.\nProperly approximating this function requires us to either simulate or record the\nwhole world (in fact, the whole universe.) For, this function takes as input and main-\ntains as internal state the surrounding world (context) and the mental state of the in-\ndividual (speaker.) This is unavoidable, if we wanted to very well approximate this\nfunction as a whole.\nIt is unclear, however, whether we want to approximate the full function. For a\nhuman to survive, yes, it is likely that the full function is needed. But, if our goal is\n3 We assume here that a such thing exists however it is represented in our brain.\n4 This is a matter of generalization, and we will talk about this more throughout the course.\n9\nrestricted to a certain task (such as translation, language modelling, and so on), we may\nnot want to approximate this function fully. We probably want to approximate only a\nsubset of this whole function. For instance, if our goal is to understand the process\nof translation from one language to another, we can perhaps ignore all but the speech\ninput to the function and all but the speech output from the function, because often a\n(trained) person can translate a sentence in one language to another without knowing\nthe whole context.\nThis latter approach to language understanding–approximating a partial function\nof languages– will be at the core of this course. We will talk about various language\ntasks that are a part of this whole function of language. These tasks will include, but\nare not limited to, language modelling, machine translation, image/video description\ngeneration and question answering. For these tasks and potentially more, we will study\nhow to use machine learning, or more speciﬁcally deep learning, to solve these tasks\nby approximating sub-functions of language.\n10\nChapter 2\nFunction Approximation as\nSupervised Learning\nThroughout this course, we will extensively use artiﬁcial neural networks1 to approx-\nimate (a part of) the function of natural language. This makes it necessary for us to\nstudy the basics of neural networks ﬁrst, and this lecture and a couple of subsequent\nones are designed to serve this purpose.\n2.1\nFunction Approximation: Parametric Approach\n2.1.1\nExpected Cost Function\nLet us start by deﬁning a data distribution pdata. pdata is deﬁned over a pair of input\nand output vectors, x ∈Id and y ∈Ok, respectively. I and O are respectively sets of\nall possible input and output values, such as R, {0,1} and {0,1,...,L}. This data\ndistribution is not known to us.\nThe goal is to ﬁnd a relationship between x and y. More speciﬁcally, we are in-\nterested in ﬁnding a function f : Rd →Ok that generates the output y given its corre-\nsponding input x. The very ﬁrst thing we should do is to put some constraints on the\nfunction f to make our search for the correct f a bit less impossible. In this lecture,\nand throughout the course, I will consider only a parametric function f, in which case\nthe function is fully speciﬁed with a set of parameters θ.\nNext, we must deﬁne a way to measure how well the function f approximates\nthe underlying mechanism of generation (x →y). Let’s denote by ˆy the output of the\nfunction with a particular set θ of parameters and a given input x:\nˆy = fθ(x)\n1 From here on, I will simply drop artiﬁcial and call them neural networks. Whenever I say “neural\nnetwork”, it refers to artiﬁcial neural networks.\n11\nHow well f approximates the true generating function is equivalent to how far ˆy is from\nthe correct output y. Let’s use D(ˆy,y) for now call this distance2 between ˆy and y\nIt is clear that we want to ﬁnd θ that minimizes D(ˆy,y) for every pair in the space\n(RRd ×Ok). But, wait, every pair equally likely? Probably not, for we do not care how\nwell fθ approximates the true function, when a pair of input x and output y is unlikely,\nmeaning we do not care how bad the approximation is, if pdata(x,y) is small. However,\nthis is a bit difﬁcult to take into account, as we must decided on the threshold below\nwhich we consider any pair irrelevant.\nHence, we weight the distance between the approximated ˆy and the correct y of\neach pair (x,y) in the space by its probability p(x,y). Mathematically saying, we want\nto ﬁnd\nargmin\nθ\nZ\nx\nZ\ny pdata(x,y)D(ˆy,y)dxdy,\nwhere the integral\nR should be replaced with the summation ∑if any of x and y is\ndiscrete.\nWe call this quantity being minimized with respect to the parameters θ a cost func-\ntion C(θ). This is equivalent to computing the expected distance between the predicted\noutput ˆy and the correct one y:\nC(θ) =\nZ\nx\nZ\ny pdata(x,y)D(ˆy,y)dxdy,\n(2.1)\n=E(x,y)∼pdata [D(ˆy,y)]\n(2.2)\nThis is often called an expected loss or risk, and minimizing this cost function is re-\nferred to as expected risk minimization [105].\nUnfortunately C(θ) cannot be (exactly) computed for a number of reasons. The\nmost important reason among them is simply that we don’t know what the data distri-\nbution pdata is. Even if we have access to pdata, we can exactly compute C(θ) only with\nheavy assumptions on both the data distribution and the distance function.3\n2.1.2\nEmpirical Cost Function\nThis does not mean that we are doomed from the beginning. Instead of the full-blown\ndescription of the data distribution pdata, we will assume that someone miraculously\ngave us a ﬁnite set of pairs drawn from the data distribution. We will call this a training\nset:\n\b\n(x1,y1),...,(xN,yN)\n\t\n.\nAs we have access to the samples from the data distribution, we can use Monte\nCarlo method to approximate the expected cost function C(θ) such that\nC(θ) ≈˜C(θ) = 1\nN\nN\n∑\nn=1\nD( ˆyn,yn).\n(2.3)\n2 Note that we do not require this distance to satisfy the triangular inequality, meaning that it does not\nhave to be a distance. However, I will just call it distance for now.\n3Why?\n12\nWe call this approximate ˜C(θ) of the expected cost function, an empirical cost function\n(or empirical risk or empirical loss.)\nBecause empirical cost function is readily computable, we will mainly work with\nthe empirical cost function not with the expected cost function. However, keep in mind\nthat at the end of the day, the goal is to ﬁnd a set of parameters that minimizes the\nexpected cost.\n2.2\nLearning as Optimization\nWe often call this process of ﬁnding a good set of parameters that minimizes the ex-\npected cost learning. This term is used from the perspective of a machine which imple-\nments the function fθ, as it learns to approximate the true generating function f from\ntraining data.\nFrom what I have described so far, it may have become clear even without me men-\ntioning that learning is optimization. We have a clearly deﬁned function (the empirical\ncost function ˜C) which needs to be minimized with respect to its input θ.\n2.2.1\nGradient-based Local Iterative Optimization\nThere are many optimization algorithms one can use to ﬁnd a set of parameters that\nminimizes ˜C. Sometimes, you can even ﬁnd the optimal set of parameters in a closed\nform equation.4 In most cases, because there is no known closed-form solution, it is\ntypical to use an iterative optimization algorithm (see [42] for in-depth discussion on\noptimization.)\nBy an iterative optimization, I mean an algorithm which reﬁnes its estimate of the\noptimal set of parameters little by little until the values of the parameters converge to\nthe optimal (expected) cost function. Also, it is worthwhile to note that most iterative\noptimization algorithms are local, in the sense that they do not require us to evaluate\nthe whole parameter space, but only a small subset along the path from the starting\npoint to the convergence point.5\nHere I will describe the simplest one among those local iterative optimization algo-\nrithms, called gradient descent (GD) algorithm. As the name suggests, this algorithm\ndepends entirely on the gradient of the cost function.6\n4 One such example is a linear regression where\n• fθ={W}(x) = Wx\n• D(ˆy,y) = 1\n2∥ˆy−y∥2\nIn this case, the optimal W is\nW = YX⊤(XX⊤)−1,\n(2.4)\nwhere\nX =\n\u0002\nx1;...;xN\u0003\n,Y =\n\u0002\ny1;...;yN\u0003\n.\nTry it yourself!\n5 There are global optimization algorithms, but they are out of scope for this course. See, for instance,\n[18] for one such algorithm called Bayesian optimization.\n6 From here on, I will use the cost function to refer to the empirical cost function.\n13\nFigure\n2.1:\n(blue)\nf(x) =\nsin(10x) + x. (red) a gradient at\nx = −0.6. (magenta) a negative\ngradient at x = −0.6.\nThe gradient of a function ∇˜C is a vector whose direction points to the direction of\nthe greatest rate of increase in the function’s value and whose magnitude measures this\nrate. At each point θt in the parameter space, the gradient of the cost function ∇˜C(θt)\nis the opposite direction toward which we want to move the parameters. See Fig. 2.1\nfor graphical illustration.\nOne important point of GD that needs to be mentioned here is on how large a\nstep one takes each time. As clear from the magenta line (the direction opposite to\nthe direction given by the gradient) in Fig. 2.1, if too large a step is taken toward the\nnegative gradient direction, the optimization process will overshoot and miss the (local)\nminimum around x = −0.8. This step size, or sometimes called learning rate, η is one\nmost important hyperparameter of the GD algorithm.\nNow we have all the ingredients for the GD algorithm: ∇˜C and η. The GD algo-\nrithm iterates the following step:\nθ ←θ −η∇˜C(θ).\n(2.5)\nThe iteration continues until a certain stopping criterion is met, which we will discuss\nshortly.\n2.2.2\nStochastic Gradient Descent\nThis simple GD algorithm works surprisingly quite well, and it is a fundamental basis\nupon which many advanced optimization algorithms have been built. I will present a\nlist of few of those advanced algorithms later on and discuss them brieﬂy. But, before\ngoing into those advanced algorithms, let’s solve one tiny, but signiﬁcant issue of the\nGD algorithm.\nThis tiny, but signiﬁcant issue arises especially often in machine learning. That is,\nit is computationally very expensive to compute ˜C and consequently its gradient ∇˜C,\nthanks to the ever increasing size of the training set D.\nWhy is the growing size of the training set making it more and more computation-\nally demanding to compute ˜C and ∇˜C? This is because both of them are essentially\n14\nthe sum of as many per-sample costs as there are examples in the training set. In other\nwords,\n˜C(θ) = 1\nN\nN\n∑\nn=1\n˜C(xn,yn|θ),\n∇˜C(θ) = 1\nN\nN\n∑\nn=1\n∇˜C(xn,yn|θ).\nAnd, N goes up to millions or billions very easily these days.\nThis enormous computational cost involved in each GD step has motivated the\nstochastic gradient descent (SGD) algorithm [88, 15].\nFirst, recall from Eq. (2.3) that the cost function we minimize is the empirical\ncost function ˜C which is the sample-based approximation to the expected cost function\nC. This approximation was done by assuming that the training examples were drawn\nrandomly from the data distribution pdata:\nC(θ) ≈˜C(θ) = 1\nN\nN\n∑\nn=1\nD( ˆyn,yn).\nIn fact, as long as this assumption on the training set holds, we can always approximate\nthe expected cost function with a fewer number of training examples:\nC(θ) ≈˜CM (θ) =\n1\n|M | ∑\nm∈M\nD( ˆym,ym),\nwhere M ≪N and M is the indices of the examples in this much smaller subset of the\ntraining set. We call this small subset a minibatch.\nSimilarly, this leads to a minibatch-based estimate of the gradient as well:\n∇˜CM (θ) =\n1\n|M | ∑\nm∈M\n∇D( ˆym,ym).\nIt must now be clear to you where I am headed toward. At each GD step, instead\nof using the full training set, we will use a small subset M which is randomly selected\nto compute the gradient estimate. In other words, we use ˜CM instead of ˜C, and ∇˜CM\ninstead of ∇˜C, in Eq. (2.5).\nBecause computing ˜CM and ∇˜CM is independent of the size of the training set, we\ncan use SGD to make as many steps as we want without worrying about the growing\nsize of training examples. This is highly beneﬁcial, as regardless of how many train-\ning examples you used to compute the gradient, we can only take a tiny step toward\nthat descending direction. Furthermore, the increased level of noisy in the gradient\nestimate due to the small sample size has been suspected to help reaching a better so-\nlution in high-dimensional non-convex problems (such as those in training deep neural\nnetworks) [71].7\n7 Why would this be the case? It is worth thinking about this issue further.\n15\nWe can set M to be any constant, and in an extreme, we can set it to 1 as well. In\nthis case, we call it online SGD.8 Surprisingly, already in 1951, it was shown that using\na single example each time is enough for the SGD to converge to a minimum (under\ncertain conditions, obviously) [88].\nThis SGD algorithm will be at the core of this course and will be discussed further\nin the future lectures.\n2.3\nWhen do we stop learning?\nFrom here on, I assume that we approximate the ground truth function by iteratively\nreﬁning its set of parameters, in most cases using stochastic gradient descent. In other\nwords, learning of a machine that approximates the true generating function f happens\ngradually as the machine goes over the training examples little by little over time.\nLet us go over again what kind of constraints/issue we have ﬁrst:\n1. Lack of access to the expected cost function C(θ)\n2. Computationally expensive empirical cost function ˜C(θ)\n3. (Potential) non-convexity of the empirical cost function ˜C(θ)\nThe most severe issue is that we do not have access to the expected cost function\nwhich is the one we want to minimize in order to work well with any pair of input x\nand output y. Instead, we have access to the empirical cost function which is a ﬁnite\nsample approximation to the expected cost function.\nWhy is this a problem? Because, we do not have a guarantee that the (local) mini-\nmum of the empirical cost function corresponds to the (local) minimum of the expected\ncost function. An example of this mismatch between the expected and empirical cost\nfunctions is shown in Fig. 2.2.\nAs in the case shown in Fig. 2.2, it is not desirable to minimize the empirical cost\nfunction perfectly. The parameters that perfectly minimize the empirical cost function\n(in the case of Fig. 2.2, the slope a of a linear function f(x) = ax) will likely be a\nsub-optimal cost for the expected cost function about which we really care.\n2.3.1\nEarly Stopping\nWhat should we do? There are many ways to avoid this weird contradiction where\nwe want to optimize the cost function well but not too well. Among those, one most\nimportant trick is early stopping, which is only applicable when iterative optimization\nis used.\nFirst, we will split the training set D into two partitions Dtrain and Dval.9 We call\nthem a training set and a validation set, respectively. In practice it is a good idea to\nkeep D much larger than D′, because of the reasons that will become clear shortly.\n8 Okay, this is not true in a strict sense. SGD is an online algorithm with M = 1 originally, and using\nM > 1, is a variant of SGD, often called, minibatch SGD. However, as using minibatches (M > 1) is almost\nalways the case in practice, I will refer to minibatch SGD as SGD, and to the original SGD as online SGD.\n9 Later on, we will split it further into three partitions.\n16\nFigure 2.2: (blue) Expected cost\nfunction C(θ).\n(red) Empirical\ncost function\n˜C(θ).\nThe un-\nderlying true generating function\nwas f(x) = sin(10x) + x.\nThe\ncost function uses the squared Eu-\nclidean distance.\nThe empiri-\ncal cost function was computed\nbased on 10 noisy examples of\nwhich x’s were sampled from the\nuniform distribution between 0\nand 1. For each sample input x,\nnoise from zero-mean Gaussian\ndistribution with standard devia-\ntion 0.01 was added to f(x) to\nemulate the noisy measurement\nchannel.\nFurther, let us deﬁne the training cost as\n˜C(θ) = Ctrain(θ) =\n1\n|Dtrain|\n∑\n(x,y)∈Dtrain\nDtrain(ˆy,y),\n(2.6)\nand the validation cost as\nCval(θ) =\n1\n|Dval|\n∑\n(x,y)∈Dval\nD(ˆy,y).\n(2.7)\nWith these two cost functions we are all ready to use early stopping now.\nAfter every few updates using SGD (or GD), the validation cost function is evalu-\nated with the current set of parameters. The parameters are updated (i.e., the training\ncost function is optimized) until the validation cost does not decrease, or starts to in-\ncrease instead of decreasing.\nThat’s it! It is almost free, as long as the size of the validation set is reasonable,\nsince each evaluation is at most as expensive as computing the gradient of the empirical\ncost function. Because of the simplicity and effectiveness, this early stopping strategy\nhas become de facto standard in deep learning and in general machine learning.\nThe question that needs to be asked here is what the validation cost function does\nhere. Clearly, it approximates the expected cost function C, similarly to the empirical\ncost function ˜C as well as the training cost function Ctrain. In the inﬁnite limit of the\nsize of either training or validation set, they should coincide, but in the case of a ﬁnite\nset, those two cost functions differ by the noise in sampling (sampling pairs from the\ndata distribution) and observation (noise in y = f(x).)\nThe fact that we explicitly optimize the training cost function implies that there is\na possibility (in fact, almost surely in practice) that the set of parameters found by this\noptimization process may capture not only the underlying generating function but also\n17\nnoise in the observation and sampling procedure. This is an issue, because we want our\nmachine to approximate the true generating function not the noise process involved.\nThe validation cost function measures both the true generating structure as well as\nnoise injected during sampling and observation. However, assuming that noise is not\ncorrelated with the underlying generating function, noise introduced in the validation\ncost function differs from that in the training cost function. In other words, the set\nof parameters that perfectly minimizes the training cost function (thereby capturing\neven noise in the training set) will be penalized when measured by the validation cost\nfunction.\n2.3.2\nModel Selection\nIn fact, the use of the validation cost does not stop at the early stopping. Rather, it has a\nmore general role in model selection. First, we must talk about model selection itself.\nThis whole procedure of optimization, or learning, can be cast as a process of\nsearching for the best hypothesis over the entire space H of hypotheses. Here, each\nhypothesis corresponds to each possible function (with a unique set of parameters and\na unique functional form) that takes the input x and output y. In the case of regression\n(x ∈Rd and y ∈R), the hypothesis space includes an n-th order polynomial function\nf(x) =\n∑\n∑d\nk=1 ik=n,ik≥0\nai1,i2,...,ik\nd\n∏\nk′=1\nxik\nk′,\nwhere ai1,i2,...,ik’s are the coefﬁcients, and any other functional form that you can imag-\nine as long as it can process x and return a real-valued scalar. In the case of neural\nnetworks, this space includes all the possible model architectures which are deﬁned by\nthe number of layers, the type of nonlinearities, the number of hidden units in each\nlayer and so on.\nLet us use M ∈H to denote one hypothesis.10 One important thing to remember is\nthat the parameter space is only a subset of the hypothesis space, because the parameter\nspace is deﬁned by a family of hypotheses (the parameter space of a linear function\ncannot include a set of parameters for a second-order polynomial function.)\nGiven a deﬁnition of expected cost function, we can score each hypothesis M by\nthe corresponding cost CM. Then, the whole goal of function approximation boils down\nto the search for a hypothesis M with the minimal expected cost function C. But, of\ncourse, we do not have access to the expected cost function and resort to the empirical\ncost function based on a given training set.\nThe optimization-based approach we discussed so far searches for the best hypoth-\nesis based on the empirical cost iteratively. However, because of the issue of overﬁtting\nwhich means that the optimization algorithm overshot and missed the local minimum\nof the expected cost function (because it was aimed at the local minimum of the empir-\nical cost function), I introduced the concept of early stopping based on the validation\ncost.\n10 M, because each hypothesis corresponds to one learning machine.\n18\nThis is unfortunately not satisfactory, as we have only searched for the best hypoth-\nesis inside a small subset of the whole hypothesis space H . What if another subset\nof the hypothesis space includes a function that better suits the underlying generating\nfunction f? Are we doomed?\nIt is clearly better to try more than one subsets of the hypothesis space. For in-\nstance, for a regression task, we can try linear functions (H1), quadratic (second-order\npolynomial) functions (H2) and sinusoidal functions (H3). Let’s say for each of these\nsubsets, we found the best hypothesis (using iterative optimization and early stopping);\nMH1, MH2 and MH3. Then, the question is how we should choose one of those hy-\npotheses.\nSimilar to what we’ve done with early stopping, we can use the validation cost to\ncompare these hypotheses. Among those three we choose one that has the smallest\nvalidation cost Cval(M).\nThis is one way to do model selection, and we will talk about another way to do\nthis later.\n2.4\nEvaluation\nBut, wait, if this is an argument for using the validation cost to early stop the optimiza-\ntion (or learning), one needs to notice something weird. What is it?\nBecause we used the validation cost to stop the optimization, there is a chance\nthat the set of parameters we found is optimal for the validation set (whose structure\nconsists of both the true generating function and sampling/observation noise), but not\nto the general data distribution. This means that we cannot tell whether the function\nestimate ˆf approximating the true generating function f is a good ﬁt by simply early\nstopping based on the validation cost. Once the optimization is done, we need yet\nanother metric to see how well the learned function estimate ˆf approximates f.\nTherefore, we need to split the training set not into two partitions but into three\npartitions. We call them a training set Dtrain, a validation set Dval and a test set Dtest.\nConsequently, we will have three cost functions; a training cost function Ctrain, a vali-\ndation cost function Cval and a test cost function Ctest, similarly to Eqs. 2.6–2.7.\nThis test cost function is the one we use to compare different hypotheses, or models,\nfairly. Any hypothesis that worked best in terms of the test cost is the one that you\nchoose.\nLet’s not Cheat\nOne most important lesson here is that you must never look at a test\nset. As soon as you take a peak at the test set, it will inﬂuence your choice in the model\nstructure as well as any other hyperparameters biasing toward a better test cost. The\nbest option is to never ever look at the test set until it is absolutely needed (e.g., need\nto present your result.)\n19\n2.5\nLinear Regression for Non-Linear Functions\nLet us start with a simple linear function to approximate a true generating function such\nthat\nˆy = f(x) = W⊤x,\nwhere W ∈Rd×l is the weight matrix. In this case, this weight matrix is the only\nparameter, i.e., θ = {W}.\nThe empirical cost function is then\n˜C(θ) = 1\nN\nN\n∑\nn=1\n1\n2\n\r\r\ryn −W⊤xn\r\r\r\n2\n2 .\nThe gradient of the empirical cost function is\n∇˜C(θ) = −1\nN\nN\n∑\nn=1\n\u0010\nyn −W⊤xn\u0011⊤\nxn.\n(2.8)\nWith these two well deﬁned, we can use the iterative optimization algorithm, such\nas GD or SGD, to ﬁnd the best W that minimizes the empirical cost function.11 Or,\nbetter is to use a validation set to stop the optimization algorithm at the point of the\nminimal validation cost function (remember early stopping?)\nNow, but we are not too satisﬁed with a linear network, are we?\n2.5.1\nFeature Extraction\nWhy are we not satisﬁed?\nFirst, we are not sure whether the true generating function f was a linear function.\nIf it is not, can we expect linear regression to approximate the true function well? Of\ncourse, not. We will talk about this shortly.\nSecond, because we were given x (meaning we did not have much control over what\nwe want to measure as x), it is unclear how well x represents the input. For instance,\nconsider doing a sales forecast of air conditioner at one store which opened ﬁve years\nago. The input x is the number of days since the opening date of the store (1 Jan 2009),\nand the output y is the number of units sold on each day.\nClearly, in this example, the relationship between x and y is not linear. Furthermore,\nperhaps the most important feature for predicting the sales of air conditioners is missing\nfrom the input x, which is a month (or a season, if you prefer.) It is likely that the\nsales bottoms out during the winter (perhaps sometime around December, January and\nFebruary,) and it hits the peak during summer months (around May, June and July.)\nIn other words, if we look at how far the month is away from July, we can predict the\nsales quite well even with linear regression.\n11 In fact, looking at Eq. (2.8), it’s quite clear that you can compute the optimal W analytically. See\nEq. (2.4).\n20\nLet us call this quantity φ(x), or equivalent feature, such that\nφ(x) = |m(x)−α|,\n(2.9)\nwhere m(x) ∈{1,2,...,12} is the month of x and α = 5.5. With this feature, we can ﬁt\nlinear regression to better approximate the sales ﬁgure of air conditioners. Furthermore,\nwe can add yet another feature to improve the predictive performance. For instance,\none such feature can be which day of week x is.\nThis whole process of extracting a good set of features that will make our choice\nof parametric function family (such as linear regression in this case) is called feature\nextraction. This feature extraction is an important step in machine learning and has\noften been at the core of many applications such as computer vision (the representative\nexample is SIFT [74].)\nFeature extraction often requires heavy knowledge of the domain in which this\nfunction approximation is applied. To use linear regression for computer vision, it is\na good idea to use computer vision knowledge to extract a good set of features. If we\nwant to use it for environmental problems, we must ﬁrst notice which features must be\nimportant and how they should be represented for linear regression to work.\nThis is okay for a machine learning practitioner in a particular ﬁeld, because the\nperson has in-depth knowledge about the ﬁeld. There are however many cases where\nthere’s simply not enough domain knowledge to exploit. To make the matter worse, it\nis likely that the domain knowledge is not correct, making the whole business of using\nmanually extracted features futile.\n21\nChapter 3\nNeural Networks and\nBackpropagation Algorithm\n3.1\nConditional Distribution Approximation\nI have mainly described so far as if the function we approximate or the function we\nuse to approximate returns only a constant value, as in one point y in the output space.\nThis is however not true, and in fact, the function can return anything including a\ndistribution [17, 35, 12].\nLet’s ﬁrst decompose the data distribution pdata into the product of two terms:\npdata(x,y) = pdata(x)pdata(y|x).\nIt becomes clear that one way to sample from pdata is to sample an input xn from\npdata(x) and subsequently sample the corresponding output yn from the conditional\ndistribution pdata(y|xn).\nThis implies that the function approximation of the generating function (f : x →y)\nis effectively equivalent to approximating the conditional distribution pdata(y|x). This\nmay suddenly sound much more complicated, but it should not alarm you at all. As\nlong as we choose to use a distribution parametrized by a small number of param-\neters to approximate the conditional distribution pdata(y|x), this is quite manageable\nwithout almost any modiﬁcation to the expected and empirical cost functions we have\ndiscussed.\nLet us use θ(x) to denote a set of parameters for the probability distribution ˜p(y|x,θ(x))\napproximating the true, underlying probability distribution pdata(y|x). As the notation\nsuggests, the function now returns the parameters of the distribution θ(x) given the\ninput x.\nFor example, let’s say y ∈{0,1}k is a binary vector and we chose to use inde-\npendent Bernoulli distribution to approximate the conditional distribution pdata(y|x).\nIn this case, the parameters that deﬁne the conditional distribution are the means of k\n22\ndimensions:\n˜p(y|x) =\nk\n∏\nk′=1\np(yk′|x) =\nk\n∏\nk′=1\nµ\nyk′\nk′ (1−µk′)1−yk′.\n(3.1)\nThen the function θ(x) should output a k-dimensional vector of which each element is\nbetween 0 and 1.\nAnother example: let’s say y ∈Rk is a real-valued vector. It is quite natural to use a\nGaussian distribution with a diagonal covariance matrix to approximate the conditional\ndistribution p(y|x):\n˜p(y|x) =\nk\n∏\nk′=1\n1\n√\n2πσk′ exp\n \n(yk′ −µk′)2\n2σ2\nk′\n!\n.\n(3.2)\nThe parameters for this conditional distribution are θ(x) = {µ1,µ2,...,µk,σ1,σ2,...,σk},\nwhere µk ∈R and σk ∈R>0.\nIn this case of probability approximation, it is natural to use Kullback-Leibler (KL)\ndivergence to measure the distance.1 The KL divergence from one distribution P to the\nother Q is deﬁned2 by\nKL(P∥Q) =\nZ\nP(x)log P(x)\nQ(x)dx.\nIn our case of function/distribution approximation, we want to minimize the KL di-\nvergence from the data distribution pdata(y|x) to the approximate distribution ˜p(y|x)\naveraged over the data distribution pdata(x):\nC(θ) =\nZ\npdata(x)KL(pdata∥˜p)dx =\nZ\npdata(x)\nZ\npdata(y|x)log pdata(y|x)\n˜p(y|x) dydx.\nBut again we do not have access to pdata and cannot compute this expected cost func-\ntion.\nSimilarly to how we deﬁned the empirical cost function earlier, we must approxi-\nmate this expected KL divergence using the training set:\n˜C(θ) = 1\nN\nN\n∑\nn=1\n−log ˜p(yn|xn).\n(3.3)\nAs an example, if we choose to return the binary vector y as in Eq. (3.1), the empirical\ncost function will be\n˜C(θ) = −1\nN\nN\n∑\nn=1\nk\n∑\nk′=1\nyk′ logµk′ +(1−yk′)log(1−µk′),\n1 Again, we use a loose deﬁnition of the distance where triangular inequality is not enforced.\n2 Why don’t I say the KL divergence between two distributions here? Because, the KL divergence is not\na symmetric measure, i.e., KL(P∥Q) ̸= KL(Q∥P).\n23\nwhich is often called a cross entropy cost. In the case of Eq. (3.2),\n˜C(θ) = −1\nN\nN\n∑\nn=1\nk\n∑\nk′=1\n(yk′ −µk′)2\n2σ2\nk′\n−logσk′.\n(3.4)\nDo you see something interesting in Eq. (3.4)? If we assume that the function\noutputs 1 for all σk′’s, we see that this cost function reduces to that using the Euclidean\ndistance between the true output y and the mean µ. What does this mean?\nThere will be many occasions later on to discuss more about this perspective when\nwe discuss language modelling. However, one thing we must keep in our mind is that\nthere is nothing different between approximating a function and a distribution.\n3.1.1\nWhy do we want to do this?\nBefore we move on to the main topic of today’s lecture, let’s try to understand why\nwe want to output the distribution. Unlike returning a single point in the space, the\ndistribution returned by the function f incorporates both the most likely outcome ˆy as\nwell as the uncertainty associated with this value.\nIn the case of the Gaussian output in Eq. (3.2), the standard deviation σk′, or the\nvariance σ2\nk′, indicates how uncertain the function is about the output centered at µk′.\nSimilarly, the mean µk′ of the Bernoulli output in Eq. (3.1) is directly proportional to\nthe function’s conﬁdence in predicting that the k′-th dimension of the output is 1.\nFigure 3.1: Is this a duck or a rab-\nbit? [68] At the end of the day,\nwe want our function f to return\na conditional distribution saying\nthat p(duck|x) = p(rabbit|x), in-\nstead of returning the answer out\nof these two possible answers.\nThis is useful in many aspects, but one important aspect is that it reﬂects the natural\nuncertainty of the underlying generating function. One input x may be interpreted in\nmore than one ways, leading to two possible outputs, which happens more often than\nnot in the real world. For instance, the famous picture in Fig. 3.1 can be viewed as a\npicture of a duck or a picture of a rabbit, in which case the function needs to output the\nprobability distribution by which the same probability mass is assigned to both a duck\nand a rabbit. Furthermore, there is observational noise that cannot easily be identiﬁed\nand ignored by the function, in which case the function should return the uncertainty\ndue to the observational noise along with the most likely (or the average) prediction.\n24\n3.1.2\nOther Distributions\nI have described two distributions (densities) that are widely used:\n• Bernoulli distribution: binary classiﬁcation\n• Gaussian distribution: real value regression\nHere, let me present one more distribution which we will use almost everyday through\nthis course.\nCategorical Distribution: Multi-Class Classiﬁcation\nMulti-class classiﬁcation is a\ntask in which each example belongs to one of K classes. For each input x, the problem\nreduces to ﬁnd a probability pk(x) of the k-th class under the constraint that\nK\n∑\nk=1\npk(x) = 1\nIt is clear that in this case, the function f returns K values {µ1,µ2,...,µK}, each\nof which is between 0 and 1. Furthermore, the sum of µk’s must sum to 1. This can be\nachieved easily by letting f to compute afﬁne transformation of x (or φ(x)) to return K\n(unbounded) real values followed by a so called softmax function [17]:\nµk =\nexp(w⊤\nk φ(x)+bk)\n∑K\nk′=1 exp(w⊤\nk′φ(x)+bk),\n(3.5)\nwhere wk ∈Rdim(φ(x)) and bk ∈R are the parameters of afﬁne transformation.\nIn this case, the (empirical) cost function based on the KL divergence is\nC(θ) = −1\nN\nN\n∑\nn=1\nK\n∑\nk=1\nIk=ynµk,\n(3.6)\nwhere\nIk=yn =\n\u001a\n1,\nif k = yn\n0,\notherwise\n(3.7)\n3.2\nFeature Extraction is also a Function\nWe talked about the manual feature extraction in the previous lecture (see Sec. 2.5.1.\nBut, this is quite unsatisfactory, because this whole process of manual feature extraction\nis heavily dependent on the domain knowledge, meaning that we cannot have a generic\nprinciple on which we design features. This raises a question: instead of manually\ndesigning features ourselves, is it possible for this to happen automatically?\nOne thing we notice is that the feature extraction process φ(x) is nothing but a\nfunction. A function of a function is a function, right? In other words, we will extend\nour deﬁnition of the function to include the feature extraction function:\nˆy = f(φ(x)).\n25\nWe will assume that the feature extraction function φ is also parametrized, and its\nparameters are included in the set of parameters which includes those of f. As an\nexample, α in Eq. (2.9) is a parameter of the feature extraction φ.\nA natural next question is which family of parametric functions we should use for\nφ. We run into the same issue we talked about earlier in Sec. 2.3: the size of hypothesis\nspace is simply too large!\nInstead of choosing one great feature extraction function, we can go for a stack of\nsimple transformations which are all learned.3 Each transformation can be as simple\nas afﬁne transformation followed by a simple point-wise nonlinearity:\nφ0(x) = g(W0x+b0),\n(3.8)\nwhere W0 is the weight matrix, b0 is the bias and g is a point-wise nonlinearity such\nas tanh.4\nOne interesting thing is that if the dimensionality of the transformed feature vector\nφ0(x) is much larger than that of x, the function f(φ0(x)) can approximate any func-\ntion from x to y under some assumptions, even when the parameters W0 and b0 are\nrandomly selected! [34]\nThe problem solved, right? We just put a huge matrix W0, apply some nonlinear\nfunction g to it and ﬁt linear regression as I described earlier. We don’t even need to\ntouch W0 and b0. All we need to do is replace the input xn of all the pairs in the training\nset to φ0(xn).\nIn fact, there is a group of researchers claiming to have ﬁgured this out by them-\nselves less than a decade ago (as of 2015) who call this model an extreme learning\nmachine [54]. There have been some debates about this so-called extreme learning\nmachine. Here I will not make any comment myself, but would be a good exercise for\nyou to ﬁgure out why there has been debates about this.\nBut, regardlessly, this is not what we want.5 What we want is to fully tune the\nwhole thing.\n3.3\nMultilayer Perceptron\nThe basic idea of multilayer perceptron is to stack a large number of those feature\nextraction layers in Eq. (3.8) between the input and the output. This idea is as old as\nthe whole ﬁeld of neural network research, dating back to early 1960s [89]. However,\nit took many more years for people to ﬁgure out a way to tune the whole network, both\nf and φ’s together. See [91] and [70], if you are interested in the history.\n3 A great article about this was posted recently in http://colah.github.io/posts/\n2014-03-NN-Manifolds-Topology/.\n4 Some of the widely used nonlinearities are\n• Sigmoid: σ(x) =\n1\n1+exp(−x)\n• Hyperbolic function: tanh(x) = 1−exp(−2x)\n1+exp(−2x)\n• Rectiﬁed linear unit: rect(x) = max(0,x)\n5 And, more importantly, I will not accept any ﬁnal project proposal whose main model is based on the\nELM.\n26\n3.3.1\nExample: Binary classiﬁcation with a single hidden unit\nLet us start with the simplest example. The input x ∈R is a real-valued scalar, and\nthe output y ∈{0,1} is a binary value corresponding to the input’s label. The feature\nextractor φ is deﬁned as\nφ(x) = σ(ux+c),\n(3.9)\nwhere u and c are the parameters. The function f returns the mean of the Bernoulli\nconditional distribution p(y|x):\nµ = f(x) = σ(wφ(x)+b).\n(3.10)\nIn both of these equations, σ is a sigmoid function:\nσ(x) =\n1\n1+exp(−x).\n(3.11)\nWe use the KL divergence to measure the distance between the true conditional\ndistribution p(y|x) and the predicted conditional distribution ˆp(y|x).\nKL(p∥ˆp) = ∑\ny∈{0,1}\np(y|x)log p(y|x)\nˆp(y|x)\n= ∑\ny∈{0,1}\np(y|x)log p(y|x)−p(y|x)log ˆp(y|x).\nNote that the ﬁrst term in the summation p(y|x)log p(y|x) can be safely ignored in our\ncase. Why? Because, this does not concern ˜p which is one we change in order to\nminimize this KL divergence.\nLet’s approximate this KL divergence with a single sample from p(y|x) and leave\nonly the relevant part. We will call this a per-sample cost:\nCx =−log ˆp(y|x)\n(3.12)\n=−logµy(1−µ)1−y\n(3.13)\n=−ylogµ −(1−y)log(1−µ),\n(3.14)\nwhere µ is from Eq. (3.10). It is okay to work with this per-sample cost function\ninstead of the full cost function, because the full cost function is almost always the\n(unweighted) sum of these per-sample cost functions. See Eq. (2.3).\nWe now need to compute the gradient of this cost function Cx with respect to all the\nparameters w, b, u and c. First, let’s start with w:\n∂Cx\n∂w = ∂Cx\n∂µ\n∂µ\n∂µ\n∂µ\n∂w,\nwhich is a simple application of chain rule of derivatives. Compare this to\n∂Cx\n∂b = ∂Cx\n∂µ\n∂µ\n∂µ\n∂µ\n∂b .\n27\nIn both equations, µ = wφ(x)+b which is the input to f.\nBoth of these derivatives share ∂Cx\n∂µ\n∂µ\n∂µ , where\n∂Cx\n∂µ\n∂µ\n∂µ\n|{z}\n=µ′\n= −y\nµ µ′ + 1−y\n1−µ µ′ = −y+yµ + µ −yµ\nµ(1−µ)\nµ′ =\nµ −y\nµ(1−µ)µ′ = µ −y,\n(3.15)\nbecause the derivative of the sigmoid function ∂µ\n∂µ is\nµ′ = µ(1−µ).\nNote that this corresponds to computing the difference between the correct label y and\nthe predicted label (probability) µ.\nGiven this output derivative ∂Cx\n∂µ , all we need to compute are\n∂µ\n∂w = φ(x)\n∂µ\n∂b = 1.\nFrom these computations, we see that\n∂Cx\n∂w = (µ −y)φ(x),\n(3.16)\n∂Cx\n∂b = (µ −y).\n(3.17)\nLet us continue on to u and c. We can again rewrite the derivatives w.r.t. these into\n∂Cx\n∂u =∂Cx\n∂µ\n∂µ\n∂φ\n∂φ\n∂φ\n∂φ\n∂u\n∂Cx\n∂c =∂Cx\n∂µ\n∂µ\n∂φ\n∂φ\n∂φ\n∂φ\n∂c ,\nwhere φ is the input to φ similarly to µ was to the input to µ.\nThere are two things to notice here. First, we already have ∂Cx\n∂µ from computing the\nderivatives w.r.t. w and b, meaning there is no need to re-compute it. Second,\n∂µ\n∂φ is\nshared between the derivatives w.r.t. u and c.\nTherefore, we ﬁrst compute\n∂µ\n∂φ :\n∂µ\n∂φ\n∂φ\n∂φ\n|{z}\n=φ′\n= wφ ′ = wφ(x)(1−φ(x))\n28\nNext, we compute\n∂φ\n∂u = x\n∂φ\n∂c = 1.\nNow all the ingredients are there:\n∂Cx\n∂u =(µ −y)wφ(x)(1−φ(x))x\n∂Cx\n∂c =(µ −y)wφ(x)(1−φ(x)).\nThe most important lession to learn from here is that most of the computations\nneeded to get the derivatives in this seemingly complicated multilayered computational\ngraph (multilayer perceptron) are shared. At the end of the day, the amount of compu-\ntation needed to compute the gradient of the cost function w.r.t. all the parameters in\nthe network is only as expensive as computing the cost function itself.\n3.3.2\nExample: Binary classiﬁcation with more than one hidden\nunits\nLet us try to generalize this simple, or rather simplest model, into a slightly more\ngeneral setting. We will still look at the binary classiﬁcation but with multiple hidden\nunits and a multidimensional input such that:\nφ(x) = Ux+c,\nwhere U ∈Rl×d and c ∈Rl. Consequently, w will be a l-dimensional vector.\nThe output derivative ∂Cx\n∂µ\n∂µ\n∂µ stays same as before. See Eq. (3.15). However, we\nnote that the derivative of µ with respect to w should now differ, because it’s a vector.6\nLet’s look at what this means.\nThe µ can be expressed as\nµ = w⊤φ(x)+b =\nl\n∑\ni=1\nwiφi(x)+b.\n(3.18)\nIn this case, we can start computing the derivative with respect to each element of wi\nseparately:\n∂µ\n∂wi\n= φi(x),\n6 The Matrix Cookbook [85] is a good reference for this section.\n29\nand will put them into a vector:\n∂µ\n∂w =\n\u0014 ∂µ\n∂w1\n,\n∂µ\n∂w2\n,...,\n∂µ\n∂wl\n\u0015⊤\n= [φ1(x),φ2(x),...,φl(x)]⊤= φ(x)\nThen, the derivative of the cost function Cy with respect to w can be written as\n∂Cy\n∂w = (µ −y)φ(x),\nin which case nothing really changed from the case of a single hidden unit in Eq. (3.16).\nNow, let’s look at ∂Cy\n∂φ . Again, because φ(x) is now a vector, there has to be some\nchanges. Because ∂Cy\n∂µ is already computed, we only need to look at\n∂µ\n∂φ . In fact, the\nprocedure for computing this is identical to that for computing\n∂µ\n∂w due to the symmetry\nin Eq. (3.18). That is,\n∂µ\n∂φ = w\nNext, what about ∂φ\n∂φ ?\nBecause the nonlinear activation function σ is applied\nelement-wise, we can simply compute this derivative for each element in φ(x) such\nthat\n∂φ\n∂φ = diag\n\u0010\u0002\nφ ′\n1(x),φ ′\n2(x),...,φ ′\nl (x)\n\u0003⊤\u0011\n,\nwhere diag returns a diagonal matrix of the input vector. In short, we will denote this\nas φ ′\nOverall so far, we have got\n∂Cy\n∂φ = (µ −y)w⊤φ ′(x) = (µ −y)(w⊙diag(φ ′(x))),\nwhere ⊙is an element-wise multiplication.\nNow it is time to compute\n∂φ\n∂U :\n∂φ\n∂U = ∂U⊤x\n∂U\n= x,\naccording to the Matrix Cookbook [85]. Then, let’s look at the whole derivative w.r.t.\nU:\n∂Cy\n∂U = (µ −y)(w⊙diag(φ ′(x)))x⊤.\nNote that all the vectors in this lecture note are column vectors.\nFor c, it’s straightforward, since\n∂φ\n∂c = 1.\n30\n3.4\nAutomating Backpropagation\nThis procedure, presented as two examples, is called a backpropagation algorithm. If\nyou read textbooks on neural networks, you see a fancier way to explain this back-\npropagation algorithm by introducing a lot of fancy terms such as local error δ and\nso on. But, personally I ﬁnd it much easier to understand backpropagation as a clever\napplication of the chain rule of derivatives to a directed acyclic graph (DAG) in which\neach node computes a certain function φ using the output of the previous nodes. I will\nrefer to this DAG as a computational graph from here on.\n1\n1\n1\n2\n2\n2\n(a)\n(b)\nFigure 3.2: (a) A graphical representation of the computational graph of the example\nnetwork from Sec. 3.3.2. (b) A graphical illustration of a function node (→: forward\npass, ←: backward pass.)\nA typical computational graph looks like the one in Fig. 3.2 (a). This computational\ngraph has two types of nodes; (1) function node (⃝) and (2) variable node (2). There\nare four different types of function nodes; (1) MatMul(A,B) = AB, (2) MatSum(A,B) =\nA+B, (3) σ: element-wise sigmoid function and (4) Cy: cost node. The variables nodes\ncorrespond to either parameters or data (x and y.) Each function node has a number\nassociated with it to distinguish between the nodes of the same function.\nNow, in this computational graph, let us start computing the gradient using the\nbackpropagation algorithm. We start from the last code, Cy, by computing ∂Cy\n∂y and ∂Cy\n∂σ1 .\nThen, the function node σ1 will compute its own derivative\n∂σ1\n∂MatSum1 and multiply it\nwith ∂Cy\n∂σ1 passed back from the function node Cy. So far we’ve computed\n∂Cy\n∂MatSum1 = ∂Cy\n∂σ1\n∂σ1\n∂MatSum1\n(3.19)\nThe function node MatSum1 has two inputs b and the output of MatMul1. Thus,\nthis node computes two derivatives ∂MatSum1\n∂b\nand ∂MatSum1\n∂MatMul1 . Each of these is multiplied\nwith the backpropagated derivative\n∂Cy\n∂MatSum1 from Eq. (3.19). At this point, we already\nhave the derivative of the cost function Cy w.r.t. one of the parameters b:\n∂Cy\n∂b =\n∂Cy\n∂MatSum1\n∂MatSum1\n∂b\n31\nThis process continues mechanically until the very beginning of the graph (a set\nof root variable nodes) is reached. All we need in this process of backpropagating the\nderivatives is that each function node implements both forward computation as well\nas backward computation. In the backward computation, the function node received\nthe derivative from the next function node, evaluates its own derivative with respect to\nthe inputs (at the point of the forward activation) and passes theses derivatives to the\ncorresponding previous nodes. See Fig. 3.2 (b) for the graphical illustration.\nImportantly, the inner mechanism of a function node does not change depending on\nits context (or equivalently where the node is placed in a computational graph.) In other\nwords, if each type of function nodes is implemented in advance, it becomes trivial to\nbuild a complicated neural network (including multilayer perceptrons) and compute\nthe gradient of the cost function (which is one such function node in the graph) with\nrespect to all the parameters as well as all the inputs.\nThis is a special case, called the reverse mode, of automatic differentiation.7 It\nis probably the most valuable tool in deep learning, and fortunately many widely used\ntoolkits such as Theano [10, 4] have implemented this reverse mode of automatic differ-\nentiation with an extensive number of function nodes used in deep learning everyday.\nBefore ﬁnishing this discussion on automating backpropagation, I’d like you to\nthink of pushing this even further. For instance, you can think of each function node\nreturning not its numerical derivative on its backward pass, but a computational sub-\ngraph computing its derivative. This means that it will return a computational graph\nof gradient, where the output is the derivatives of all the variable nodes (or a subset\nof them.) Then, we can use the same facility to compute the second-order derivatives,\nright?\n3.4.1\nWhat if a Function is not Differentiable?\nFrom the description so far, one thing we notice is that backpropagation works only\nwhen each and every function node (in a computational graph) is differentiable. In\nother words, the nonlinear activation function must be chosen such that almost every-\nwhere it is differentiable. All three activation functions I have presented so far have\nthis property.\nLogistic Functions\nA sigmoid function is deﬁned as\nσ(x) =\n1\n1+exp(−x),\nand its derivative is\nσ′(x) = σ(x)(1−σ(x)).\nA hyperbolic tangent function is\ntanh(x) = exp(2x)−1\nexp(2x)+1,\n7 If anyone’s interested in digging more into the whole ﬁeld of automatic differentiation, try to Google it\nand you’ll ﬁnd tons of materials. One such reference is [5].\n32\nand its derivative is\ntanh′(x) =\n\u0012\n2\nexp(x)+exp(−x)\n\u00132\n.\nPiece-wise Linear Functions\nI described a rectiﬁed linear unit (rectiﬁer or ReLU,\n[81, 46]) earlier:\nrect(x) = max(0,x).\nIt is clear that this function is not strictly differentiable, because of the discontinuity\nat x = 0. However, the chance of the input to this rectiﬁer lands exactly at 0 has\nzero probability, meaning that we can forget about this extremely unlikely event. The\nderivative of the rectiﬁer in this case is\nrect′(x) =\n\u001a\n1,\nif x > 0\n0,\nif x ≤0\nAlthough the rectiﬁer has become the most widely used nonlinearity, especially,\nin deep learning’s applications to computer vision,8 there is a small issue with the\nrectiﬁer. That is, for a half of the input space, the derivative is zero, meaning that the\nerror (the output derivative from Eq. (3.15)) will be not well propagated through the\nrectiﬁer function node.\nIn [48], the rectiﬁer was extended to a maxout unit so as to avoid this issue of the\nexistence of zero-derivative region in the input to the rectiﬁer. The maxout unit of rank\nk is deﬁned as\nmaxout(x1,...,xk) = max(x1,...,xk),\nand its derivative as\n∂maxout\n∂xi\n(x1,...,xk) =\n\u001a\n1,\nif max(x1,...,xk) = xi\n0,\notherwise\nThis means that the derivative is backpropagated only through one of the k inputs.\nStochastic Variables\nThese activation functions work well with the backpropagation\nalgorithm, because they are differentiable almost everywhere in the input space. How-\never, what happens if a function is non-differentiable at all. One such example is a\nbinary stochastic node, which is computed by\n1. Compute p = σ(x), where x is the input to the function node.\n2. Consider p as a mean of a Bernoulli distribution, i.e., B(p).\n3. Generate one sample s ∈{0,1} from the Bernoulli distribution.\n4. Output s.\n8 Almost all the winning entries in ImageNet Large Scale Visual Recognition Challenges (ILSVRC) use a\nconvolutional neural network with rectiﬁers. See http://image-net.org/challenges/LSVRC/.\n33\nClearly there is no derivative of this function node.\nDoes it mean that we’re doomed in this case? Fortunately, no. Although I will not\ndiscuss about this any further in this course, Bengio et al. [7] provide an extensive list\nof approaches we can take in order to compute the derivative of the stochastic function\nnodes.\n34\nChapter 4\nRecurrent Neural Networks and\nGated Recurrent Units\nAfter the last lecture I hope that it has become clear how to build a multilayer percep-\ntron. Of course, there are so many details that I did not mention, but are extremely im-\nportant in practice. For instance, how many layers of simple transformations Eq. (3.8)\nshould a multilayer perceptron have for a certain task? How wide (equiv. dim(φ0(x)))\nshould each transformation be? What other transformation layers are there? What kind\nof learning rate η (see Eq. (2.5)) should we use? How should we schedule this learning\nrate over training? Answers to many of these questions are unfortunately heavily task-,\ndata- and model-dependent, and I cannot provide any general answer to them.\n4.1\nRecurrent Neural Networks\nInstead, I will move on to describing how we can build a neural network1 to handle\na variable length input. Until now the input x was assumed to be either a scalar or\na vector of the ﬁxed number of dimensions. From here on however, we remove this\nassumption of a ﬁxed size input and consider the case of having a variable length input\nx.\nWhat do I mean by a variable length input? A variable length input x is a sequence\nwhere each input x has a different number of elements. For instance, the ﬁrst training\nexample’s input x1 may consist of l1 elements such that\nx1 = (x1\n1,x1\n2,...,x1\nl1).\nMeanwhile, another example’s input xn may be a sequence of ln ̸= l1 elements:\nxn = (xn\n1,xn\n2,...,xn\nln).\nLet’s go back to very basic about dealing with these kinds of sequences. Further-\nmore, let us assume that each element xi is binary, meaning that it is either 0 or 1. What\n1 Now, let me begin using a term neural network instead of a general function.\n35\nwould be the most natural way to write a function that returns the number of 1’s in\nan input sequence x = (x1,x2,...,xl)? My answer is to ﬁrst build a recursive function\ncalled ADD1, shown in Alg. 1. This function ADD1 will be called for each element of\nthe input x, as in Alg. 2.\nAlgorithm 1 A function ADD1\ns ←0\nfunction ADD1(v,s)\nif v = 0 then return s\nelse return s+1\nend if\nend function\nAlgorithm 2 A function ADD1\ns ←0\nfor i ←1,2,...,l do s ←ADD1(xi,s)\nend for\nThere are two important components in this implementation. First, there is a mem-\nory s which counts the number of 1’s in the input sequence x. Second, a single function\nADD1 is applied to each symbol in the sequence one at a time together with the mem-\nory s. Thanks to these two properties, our implementation of the function ADD1 can be\nused with the input sequence of any length.\nNow let us generalize this idea of having a memory and a recursive function that\nworks over a variable length sequence. One likely most general case of this idea is\na digital computer we use everyday. A computer program is a sequence x of instruc-\ntions xi. A central processing unit (CPU) reads each instruction of this program and\nmanipulates its registers according to what the instruction says. Manipulating registers\nis often equivalent to manipulating any input–output (I/O) device attached to the CPU.\nOnce one instruction is executed, the CPU moves on to the next instruction which will\nbe executed with the content of the registers from the previous step. In other words,\nthese registers work as a memory in this case (s from Alg. 2,) and the execution of an\ninstruction by the CPU corresponds to a recursive function (ADD1 from Alg. 1.)\nBoth ADD1 and CPU are hard coded in the sense that they do what they have been\ndesigned and manufactured to do. Clearly, this is not what we want, because nobody\nknows how to design a CPU or a recursive function for natural language understanding,\nwhich is our ultimate goal. Instead what we want is to have a parametric recursive\nfunction that is able to read a sequence of (linguistic) symbols and use a memory in\norder to understand natural languages.\nTo build this parametric recursive function2 that works on a variable-length input\nsequence x = (x1,x2,...,xl), we now know that there needs to be a memory. We will\nuse one vector h ∈Rdh as this memory vector. As is clear from Alg. 1, this recursive\nfunction takes as input both one input symbol xt and the memory vector h, and it\n2 In neural network research, we call this function a recurrent neural network.\n36\nreturns the updated memory vector. It often helps to time index the memory vector\nas well, such that the input to this function is ht−1 (the memory after processing the\nprevious symbol xt−1,) and we use ht to denote the memory vector returned by the\nfunction. This function is then\nht = f(xt,ht−1)\nNow the big question is what kind of parametric form this recursive function f\ntakes? We will follow the simple transformation layer from Eq. (3.8), in which case we\nget\nf(xt,ht−1) = g(Wφ(xt)+Uht−1),\n(4.1)\nwhere φ(xt) is a function that transforms the input symbol (often discrete) into a d-\ndimensional real-valued vector. W ∈Rdh×d and Udh×dh are parameters of this function.\nA nonlinear activation function g can be any function, but for now, we will assume that\nit is an element-wise nonlinear function such as tanh.\n4.1.1\nFixed-Size Output y\nBecause our goal is to approximate an underlying, true function, we now need to think\nof how we use this recursive function to return an output y. As with the case of variable-\nlength sequence input x, y can only be either a ﬁxed-size output, such as a category to\nwhich the input x belongs, or a variable-length sequence output. Here let us discuss the\ncase of having a ﬁxed-size output y.\nThe most natural approach is to use the last memory vector hl to produce the output\n(or more often output distribution.) Consider a task of binary classiﬁcation where y is\neither positive (1) or negative (0), in which case a Bernoulli distribution ﬁts perfectly.\nA Bernoulli distribution is fully characterized by a single parameter µ. Hence,\nµ = σ(v⊤hl),\nwhere v ∈Rdh is a weight vector, and σ is a sigmoid function.\nThis now looks very much like the multilayer perceptron from Sec. 3.3. The whole\nfunction given an input sequence x computes\nµ = σ(v⊤g(Wφ(xl)+Ug(Wφ(xl−1)+Ug(Wφ(xl−2)+···g(Wφ(x1)+Uh0)···)))\n|\n{z\n}\n(a) recurrence\n),\n(4.2)\nwhere h0 is an initial memory state which can be simply set to an all-zero vector.\nThe main difference is that the input is not given only to the ﬁrst simple trans-\nformation layer, but is given to all those transformation layers (one at a time.) Also,\neach transformation layer shares the parameters W and U.3 The ﬁrst two steps of the\n3 Note that for brevity, I have omitted bias vectors. This should not matter much, as having a bias vector\nis equivalent to augmenting the input with a constant element whose value is ﬁxed at 1. Why? Because,\n[W;b]\n\u0014\nx\n1\n\u0015\n= Wx+b\nNote that as I have declared before all vectors are column vectors.\n37\nrecurrence part (a) of Eq. (4.2) are shown as a computational graph in Fig. 4.1.\nFigure 4.1: Sample computational graph of the recurrence in Eq. (4.2).\nAs this is not any special computational graph, the whole discussion on how to au-\ntomate backpropagation (computing the gradient of the cost function w.r.t. the parame-\nters) in Sec. 3.4 applies to recurrent neural networks directly, except for one potentially\nconfusing point.\n4.1.2\nMultiple Child Nodes and Derivatives\nIt may be confusing how to handle those parameters that are shared across multiple\ntime steps; W and U in Fig. 4.1. In fact, in the earlier section (Sec. 3.4), we did not\ndiscuss about what to do when the output of one node is fed into multiple function\nnodes. Mathematically saying, what do we do in the case of\nc = g(f1(x), f2(x),..., fn(x))?\ng can be any function, but let us look at two widely used cases:\n• Addition: g(f1(x),..., fn(x)) = ∑n\ni=1 fi(x)\n∂c\n∂x = ∂c\n∂g\n∑\ni∈{1,2,...,n}\n∂fi\n∂x .\n• Multiplication: g(f1(x),..., fn(x)) = ∏n\ni=1 fi(x)\n∂c\n∂x = ∂c\n∂g\n∑\ni∈{1,2,...,n}\n \n∏\nj̸=i\nf j(x)\n!\n∂fi\n∂x .\nFrom these two cases, we can see that in general\n∂c\n∂x = ∂c\n∂g\n∑\ni∈{1,2,...,n}\n∂g\n∂fi\n∂fi\n∂x .\n38\nThis means that when multiple derivatives are backpropagated into a single node, the\nnode should ﬁrst sum them and multiply its summed derivative with its own derivative.\nWhat does this mean for the shared parameters of the recurrent neural network? In\nan equation,\n∂C\n∂W =\n∂C\n∂MatSuml\n|\n{z\n}\n(a)\n∂MatSuml\n∂MatMull\n∂MatMull\n∂W\n(4.3)\n+\n∂C\n∂MatSuml\n|\n{z\n}\n(a)\n∂MatSuml\n∂MatSuml−1\n|\n{z\n}\n(b)\n∂MatSuml−1\n∂MatMull−1\n∂MatMull−1\n∂W\n+\n∂C\n∂MatSuml\n|\n{z\n}\n(a)\n∂MatSuml\n∂MatSuml−1\n|\n{z\n}\n(b)\n∂MatSuml−1\n∂MatSuml−2\n|\n{z\n}\n(c)\n∂MatSuml−2\n∂MatMull−2\n∂MatMull−2\n∂W\n+··· ,\nwhere the superscript l of each function node denotes the layer at which the function\nnode resides.\nSimilarly to what we’ve observed in Sec. 3.4, many derivatives are shared across\nthe terms inside the summation in Eq. (4.3). This allows us to compute the derivative\nof the cost function w.r.t. the parameter W efﬁciently by simply running the recurrent\nneural network backward.\n4.1.3\nExample: Sentiment Analysis\nThere is a task in natural language processing called sentiment analysis. As the name\nsuggests, the goal of this task is to predict the sentiment of a given text. This is deﬁ-\nnitely one function that a human can do fairly well: when you read a critique’s review\nof a movie, you can easily tell whether the critique likes, hates or is neutral to the\nmovie. Also, even without a star rating of a product on Amazon, you can quite easily\ntell whether a user like it by reading her/his review of the product.\nIn this task, an input sequence x is a given text, and the ﬁxed-size output is its label\nwhich is almost always one of positive, negative or neutral. Let us assume for now\nthat the input is a sequence of words, where each word xi is represented as a so-called\none-hot vector.4 In this case, we can use\nφ(xt) = xt\nin Eq. (4.1).\n4 A one-hot vector is a way to represent a discrete symbol as a binary vector. The one-hot vector vi of a\nsymbol i ∈V = {1,2,...,|V|} is\nvi = [0,...,0\n| {z }\n1,...,i−1\n, 1\n|{z}\ni\n, 0,...,0\n| {z }\ni+1,...,|V|\n]⊤.\n39\nOnce the input sequence, or paragraph in this speciﬁc example, is read, we get\nthe last memory state hl of the recurrent neural network. We will afﬁne-transform hl\nfollowed by the softmax function to obtain the conditional distribution of the output\ny ∈{1,2,3} (1: positive, 2: neutral and 3: negative):\nµ = [µ1,µ2,µ3]⊤= softmax(Vhl),\n(4.4)\nwhere µ1, µ2 and µ3 are the probabilities of “positive”, “neural” and “negative”. See\nEq. (3.5) for more details on the softmax function.\nBecause this network returns a categorial distribution, it is natural to use the (cate-\ngorical) cross entropy as the cost function. See Eq. (3.6). A working example of this\nsentiment analyzer based on recurrent neural networks will be introduced and discussed\nduring the lab session.5\n4.1.4\nVariable-Length Output y: |x| = |y|\nLet’s generalize what we have discussed so far to recurrent neural networks here. In-\nstead of a ﬁxed-size output y, we will assume that the goal is to label each input symbol,\nresulting in the output sequence y = (y1,y2,...,yl) of the same length as the input se-\nquence x.\nWhat kind of applications can you think of that returns the output sequence as long\nas the input sequence? One of the most widely studied problems in natural language\nprocessing is a problem of classifying each word in a sentence into one of part-of-\nspeech tags, often called POS tagging (see Sec. 3.1 of [77].) Unfortunately, in my\npersonal opinion, this is perhaps the least interesting problem of all time in natural\nlanguage understanding, but perhaps the most well suited problem for this section.\nIn its simplest form, we can view this problem of POS tagging as classifying each\nword in a sentence as one of noun, verb, adjective and others. As an example, given\nthe following input sentence x\nx = (Children,eat,sweet,candy),\nthe goal is to output\ny = (noun,verb,adjective,noun).\nThis task can be solved by a recurrent neural network from the preceding section\n(Sec. 4.1.1) after a quite trivial modiﬁcation. Instead of waiting until the end of the\nsentence to get the last memory state of the recurrent neural network, we will use the\nimmediate memory state to predict the label at each time step t.\nAt each time t, we get the immediate memory state ht by\nht = f(xt,ht−1),\n(4.5)\nwhere f is from Eq. (4.1). Instead of continuing on to processing the next word, we\nwill ﬁrst predict the label of the t-th input word xt.\n5 For those eager to learn more, see http://deeplearning.net/tutorial/lstm.html in\nadvance of the lab session.\n40\nThis can be done by\nµt = [µt,1,µt,2,µt,3,µt,4]⊤= softmax(Vht).\n(4.6)\nFour µt,i’s correspond to the probabilities of the four categories; (1) noun, (2) verb, (3)\nadjective and (4) others.\nFrom this output distribution at time step t, we can deﬁne a per-step, per-sample\ncost function:\nCx,t(θ) = −\nK\n∑\nk=1\nIk=yµt,k,\n(4.7)\nwhere K is the number of categories, four in this case. We discussed earlier in Eq. (3.6).\nNaturally a per-sample cost function is deﬁned as the sum of these per-step, per-sample\ncost functions:\nCx(θ) = −\nl\n∑\nt=1\nK\n∑\nk=1\nIk=yµt,k.\n(4.8)\nIncorporating the Output Structures\nThis formulation of the cost function is equiv-\nalent to maximizing the log-probability of the correct output sequence given an input\nsequence, where the conditional log-probability is deﬁned as\nlog p(y|x) =\nl\n∑\nt=1\nlog p(yt|x1,...,xt)\n|\n{z\n}\nEq. (4.7)\n|\n{z\n}\nEq. (4.8)\n.\n(4.9)\nThis means that the network is predicting the label of the t-th input symbol using only\nthe input symbols read up to that point (i.e., x1,x2,...,xt.)\nIn other words, this means that the recurrent neural network is not taking into ac-\ncount the structure of the output sequence. For instance, even without looking at the\ninput sequence, in English it is well known that the probability of the next word being a\nnoun increases if the current word is an adjective.6 This kind of structures in the output\nare effectively ignored in this formulation.\nWhy is this so in this formulation? Because, we have made an assumption that\nthe output symbols y1,y2,...,yl are mutually independent conditioned on the input se-\nquence. This is clear from Eq. (4.9) and the deﬁnition of the conditional independence:\nY1 and Y2 are conditionally independent dependent on X\n⇐⇒p(Y1,Y2|X) = p(Y1|X)p(Y2|x).\nIf the underlying, true conditional distribution obeyed this assumption of condi-\ntional independence, there is no worry. However, this is a very strong assumption for\n6 Okay, this requires a more thorough analysis, but for the sake of the argument, which does not have to\ndo anything with actual POS tags, let’s believe that this is indeed the case.\n41\nmany of the tasks we run into, apparently from the example of POS tagging. Then,\nhow can we exploit the structure in the output sequence?\nOne simple way is to make a less strong assumption about the conditional proba-\nbility of the output sequence y given x. For instance, we can assume that\nlog p(y|x) =\nl\n∑\ni=1\nlog p(yi|y<i,x≤i),\nwhere y<i and x≤i denote all the output symbols before the i-th one and all the input\nsymbols up to the i-th one, respectively.\nNow the question is how we can incorporate this into the existing formulation of\na recurrent neural network from Eq. (4.5). It turned out that the answer is extremely\nsimple. All we need to do is to compute the memory state of the recurrent neural\nnetwork based not only on the current input symbol xt and the previous memory state\nht−1, but also on the previous output symbol yt−1 such that\nht = f(xt,yt−1,ht−1).\nSimilarly to Eq. (4.1), we can think of implementing f as\nf(xt,yt−1,ht−1) = g(Wxφx(xt)+Wyφy(yt−1)+Whht−1).\nThere are two questions naturally arising from this formulation. First, what do we\ndo when computing h1? This is equivalent to saying what φy(y0) is. There are two\npotential answers to this question:\n1. Fix φy(y0) to an all-zero vector\n2. Consider φy(y0) as an additional parameter\nIn the latter case, φy(y0) will be estimated together with all the other parameters such\nas those weight matrices Wx, Wy, Wh and V.\nInference\nThe second question involves how to handle yt−1. During training, it is\nquite straightforward, as our cost function (KL-divergence between the underlying,\ntrue distribution and the parametric conditional distribution p(y|x), approximated by\nMonte Carlo method) says that we use the groundtruth value for yt−1’s.\nIt is however not clear what we should do when we test the trained network, because\nthen we are not given the groundtruth output sequence. This process of ﬁnding an\noutput that maximizes the conditional (log-)probability is called inference7:\nˆy = argmax\ny\nlog p(y|x)\n7 Okay, I confess. The term inference refers to a much larger class of problems, even if we consider only\nmachine learning. However, let me simply use this term to refer to a task of ﬁnding the most likely output of\na function.\n42\nThe exact inference is quite straightforward. One can simply evaluate log p(y|x) for\nevery possible output sequence and choose the one with the highest conditional proba-\nbility. Unfortunately, this is almost always intractable, as the number of every possible\noutput sequence grows exponentially with respect to the length of the sequence:\n|Y | = Kl,\nwhere Y , K and l are the set of all possible output sequences, the number of labels and\nthe length of the sequence, respectively. Thus, this is necessary to resort to approximate\nsearch over the set Y .\nThe most naive approach to approximate inference is a greedy one. With the trained\nmodel, you predict the ﬁrst output symbol ˆy1 based on the ﬁrst input symbol x1 by\nselecting the category of the highest probability p(y1|x1). Now, given ˆy1, x1 and x2,\nwe compute p(y2|x1,x2,y1) from which we select the next output symbol ˆy2 with the\nhighest probability. We continue this process iteratively until the last output symbol ˆyl\nis selected.\nThis is greedy in the sense that any early choice with a high conditional probability\nmay turn out to be unlikely one due to extremely low conditional probabilities later on.\nIt is highly related to the so-called garden path sentence problem. To know more about\nthis, read, for instance, Sec. 3.2.4 of [77].\nIt is possible to alleviate this issue by considering N < K best hypotheses of the\noutput sequence at each time step. This procedure is called beam search, and we will\ndiscuss more about this in a later lecture on neural machine translation.\n4.2\nGated Recurrent Units\n4.2.1\nMaking Simple Recurrent Neural Networks Realistic\nLet us get back to the analogy we made in Sec. 4.1. We compared a recurrent neural\nnetwork to how CPU works. Executing a recurrent function f is equivalent to executing\none of the instructions on CPU, and the memory state of the recurrent neural network is\nequivalent to the registers of the CPU. This analogy does sound plausible, except that\nit is not.\nIn fact, how a simple recurrent neural network works is far from being similar to\nhow CPU works. I am now talking about how they are implemented in practice, but\nrather I’m talking at the conceptual level. What is it at the conceptual level that makes\nthe simple recurrent neural network unrealistic?\nAn important observation we make about the simple recurrent neural network is\nthat it refreshes the whole memory state at each time step. This is almost opposite to\nhow the registers on a CPU are maintained. Each time an instruction is executed, the\nCPU does not clear up the whole registers and repopulate them. Rather, it works only\non a small number of registers. All the other registers’ values are stored as they were\nbefore the execution of the instruction.\nLet’s try to write this procedure mathematically. Each time, based on the choice\nof instruction to be executed, a subset of the registers of a CPU, or a subset of the\n43\nelements in the memory state of a recurrent neural network, is selected. This can be\nwritten down as a binary vector u ∈{0,1}nh:\nui =\n\u001a\n0,\nif the register’s value does not change\n1,\nif the register’s value will change\nWith this binary vector, which I will call an update gate, a new memory state or a\nnew register value at time t can be computed as a convex interpolation such that\nht = (1−u)⊙ht−1 +u⊙˜ht,\n(4.10)\nwhere ⊙is as usual an element-wise multiplication. ˜ht denotes a new memory state or\na new register value, after executing the instruction at time t.\nAnother unrealistic point about the simple recurrent neural network is that each\nexecution considers the whole registers. It is almost impossible to imagine designing\nan instruction on a CPU that requires to read the values of all the registers. Instead,\nwhat almost always happens is that each instruction will consider only a small subset\nof the registers, which again we can use a binary vector to represent. Let me call it a\nreset gate r ∈{0,1}nh:\nri =\n\u001a\n0,\nif the register’s value will not be used\n1,\nif the register’s value will be used\nThis reset gate can be multiplied to the register values before being used by the\ninstruction at time t.8 If we use a recursive function f from Eq. (4.1), it means that\n˜ht = f(xt,r⊙ht−1) = g(Wφ(xt)+U(r⊙ht−1)).\n(4.11)\nNow, let us put these two gates that are necessary to make the simple recurrent\nneural network more realistic into one piece. At each time step, the candidate memory\nstate is computed based on a subset of the elements of the previous memory state:\n˜ht = g(Wφ(xt)+U(r⊙ht−1))\nA new memory state is computed as a linear interpolation between the previous mem-\nory state and this candidate memory state using the update gate:\nht = (1−u)⊙ht−1 +u⊙˜ht\nSee Fig. 4.2 for the graphical illustration.\n4.2.2\nGated Recurrent Units\nNow here goes a big question: How are the update u and reset r gates computed?\nIf we stick to our analogy to the CPU, those gates must be pre-conﬁgured per\ninstruction. Those binary gates are dependent on the instruction. Again however, this\n8 It is important to note that this is not resetting the actual values of the registers, but only the input to the\ninstruction/recursive function.\n44\nu\nr\nh\nh~\nx\nFigure 4.2: A graphical illustration of a\ngated recurrent unit [29].\nis not what we want to do in our case. There is no set of predeﬁned instructions, but the\nexecution of any instruction corresponds to computing a recurrent function based on the\ninput symbol and the memory state from the previous time step (see, e.g., Eq. (4.1).)\nSimilarly to this what we want with the update and reset gates is that they are computed\nby a function which depends on the input symbol and the previous memory state.\nThis sounds like quite straightforward, except that we deﬁned the gates to be binary.\nThis means that whatever the function we use to compute those gates, the function will\nbe a discontinuous function with zero derivative almost everywhere, except at the point\nwhere a sharp transition from 0 to 1 happens. We discussed the consequence of having\nan activation function with zero derivative almost everywhere in Sec. 3.4.1, and the\nconclusion was that it becomes very difﬁcult to compute the gradient of the cost func-\ntion efﬁciently and exactly with these discrete activation functions in a computational\ngraph.\nOne simple solution which turned out to be extremely efﬁcient is to consider those\ngates not as binary vectors but as real-valued coefﬁcient vectors. In other words, we\nredeﬁne the update and reset gates to be\nu ∈[0,1]nh ,r ∈[0,1]nh .\nThis approach makes these gates leaky in the sense that they always allow some leak\nof information through the gate.\nIn the case of the reset gate, rather than making a hard decision on which subset\nof the registers, or the elements of the memory state, will be used, it now decides how\nmuch information from the previous memory state will be used. The update gate on\nthe other hand now controls how much content in the memory state will be replaced,\nwhich is equivalent to saying that it controls how much information will be kept from\nthe previous memory state.\nUnder this deﬁnition we can simply use a sigmoid function from Eq. (3.11) to\ncompute these gates:\nr =σ(Wrφ(xt)+Urht−1),\nu =σ(Wuφ(xt)+Uu(r⊙ht−1)),\nwhere Wr, Ur, Wu and Uu are the additional parameters.9 Since the sigmoid function\nis differentiable everywhere, we can use the backpropagation algorithm (see Sec. 3.4)\n9 Note that this is not the formulation available for computing the reset and update gates. For instance,\n45\nto compute the derivatives of the cost function with respect to these parameters and\nestimate them together with all the other parameters.\nWe call this recurrent activation function with the reset and update gates a gated\nrecurrent unit (GRU), and a recurrent neural network having this GRU as a gated re-\ncurrent network.\n4.2.3\nLong Short-Term Memory\nThe gated recurrent unit (GRU) is highly motivated by a much earlier work on long\nshort-term memory (LSTM) units [53].10\nThe LSTM was proposed in 1997 with\nthe goal of building a recurrent neural network that can learn long-term dependen-\ncies across many number of timsteps, which was deemed to be difﬁcult to do so with a\nsimple recurrent neural network.\nUnlike the element-wise nonlinearity of the simple recurrent neural network and the\ngated recurrent unit, the LSTM explicitly separates the memory state ct and the output\nht. The output is a small subset of the hidden memory state, and only this subset of the\nmemory state is visibly exposed to any other part of the whole network.\nHow does a recurrent neural network with LSTM units decide how much of the\nmemory state it will reveal? As perhaps obvious at this point, the LSTM uses a so-\ncalled output gate o to achieve this goal. Similarly to the reset and update gates of the\nGRU, the output gate is computed by\no = σ(Woφ(xt)+Uoht−1).\nThis output vector is multiplied to the memory state ct point-wise to result in the output:\nht = o⊙tanh(ct).\nUpdating the memory state ct closely resembles how it is updated in the GRU (see\nEq. (4.10).) A major difference is that instead of using a single update gate, the LSTM\nuses two gates, forget and input gates, such that\nct = f⊙ct−1 +i⊙˜ct,\nwhere f ∈Rnh, i ∈Rnh and ˜ct are the forget gate, input gate and the candidate memory\nstate, respectively.\nThe roles of those two gates are quite clear from their names. The forget gate\ndecides how much information from the memory state will be forgotten, while the\ninput gate controls how much informationa about the new input (consisting of the input\none can use the following deﬁnitions of the reset and update gates:\nr =σ(Wrφ(xt)+Urht−1),\nu =σ(Wuφ(xt)+Uuht−1),\nwhich is more parallelizable than the original formulation from [29]. This is because there is no more direct\ndependency between r and u, which makes it possible to compute them in parallel.\n10 Okay, let me confess here. I was not well aware of long short-term memory when I was designing the\ngated recurrent unit together with Yoshua Bengio and Caglar Gulcehre in 2014.\n46\nsymbol and the previous output) will be inputted to the memory. They are computed\nby\nf =σ(Wf φ(xt)+U f ht−1),\n(4.12)\ni =σ(Wiφ(xt)+Uiht−1).\nThe candidate memory state is computed similarly to how it was done with the\nGRU in Eq. (4.11):\n˜ct = g(Wcφ(xt)+Ucht−1),\n(4.13)\nwhere g is often an element-wise tanh.\nAll the additional parameters speciﬁc to the LSTM–Wo,Uo,Wf ,U f ,Wi,Ui,Wc\nand Uc– are estimated together with all the other parameters. Again, every function\ninside the LSTM is differentiable everywhere, and we can use the backpropagation\nalgorithm to efﬁcient compute the gradient of the cost function with respect to all the\nparameters.\nAlthough I have described one formulation of the long short-term memory unit\nhere, there are many other variants proposed over more than a decade since it was ﬁrst\nproposed. For instance, the forget gate in Eq. (4.12) was not present in the original\nwork [53] but was ﬁxed to 1. Gers et al. [45] proposed the forget gate few years after\nthe LSTM was originally proposed, and it turned out to be one of the most crucial\ncomponent in the LSTM. For more variants of the LSTM, I suggest you to read [49,\n58].11\n4.3\nWhy not Rectiﬁers?\n4.3.1\nRectiﬁers Explode\nLet us go back to the simple recurrent neural network which uses the simple transfor-\nmation layer from Eq. (4.1):\nf(xt,ht−1) = g(Wφ(xt)+Uht−1),\nwhere g is an element-wise nonlinearity.\nOne of the most widely used nonlinearities is a hyperbolic tangent function tanh.\nThis is unlike the case in feedforward neural networks (multilayer perceptrons) where a\n(unbounded) piecewise linear function, such as a rectiﬁer and maxout, has become stan-\ndard. In the case of feedforward neural networks, you can safely assume that everyone\nuses some kind of piecewise linear function as an activation function in the network.\nThis has become pretty much standard since Krizhevsky et al. [67] shocked the (com-\nputer vision) research community by outperforming all the more traditional computer\nvision teams in the ImageNet Large Scale Visual Recognition Challenge 2012.12\n11 Interestingly, based on the observation in [58], it seems like the plain LSTM with a forget gate and the\nGRU seem to be close to the optimal gated unit we can ﬁnd.\n12 http://image-net.org/challenges/LSVRC/2012/results.html\n47\nThe main difference between logistic functions (tanh and sigmoid function) and\npiecewise linear functions (rectiﬁers and maxout) is that the former is bounded from\nboth above and below, while the latter is bounded only from below (or in some cases,\nnot bounded at all [50].13)\nThis unbounded nature of piece-wise linear functions makes it difﬁcult for them to\nbe used in recurrent neural networks. Why is this so?\nLet us consider the simplest case of unbounded element-wise nonlinearity; a linear\nfunction:\ng(a) = a.\nThe hidden state after l symbols is\nhl =U(U(U(U(···)+Wφ(xl−3))+Wφ(xl−2))+Wφ(xl−1))+Wφ(xl)\n=\n \nl−1\n∏\nl′=1\nU\n!\nWφ(x1)+\n \nl−2\n∏\nl′=1\nU\n!\nWφ(x2)+···+UWφ(xl−1)+Wφ(xl),\n=\nl\n∑\nt=1\n \nl−t\n∏\nl′=1\nU\n!\nWφ(xt)\n|\n{z\n}\n(a)\n(4.14)\nwhere l is the length of the input sequence.\nLet us assume that\n• U is a full rank matrix\n• The input sequence is sparse: ∑l\nt=1 Iφ(xt)̸=0 = c, where c = O(1)\n• [Wφ(x)]i > 0 for all i\nand consider Eq. (4.14) (a):\nht′\nl =\n \nl−t′\n∏\nl′=1\nU\n!\nWφ(xt′).\n(4.15)\nNow, let’s look at what happens to Eq. (4.15). First, the eigendecomposition of the\nmatrix U:\nU = QSQ−1,\nwhere S is a diagonal matrix whose non-zero entries are eigenvalues. Q is an orthogo-\nnal matrix. Then\nl−t′\n∏\nl′=1\nU = QSl−t′Q−1,\n13 A parametric rectiﬁer, or PReLU, is deﬁned as\ng(x) =\n\u001a\nx,\nif x ≥0\nax,\notherwise\n,\nwhere a is a parameter to be estimated together with all the other parameters of a network.\n48\nand\n \nl−t′\n∏\nl′=1\nU\n!\nWφ(xt′) = diag(Sl−t′)⊙(QQ−1\n| {z }\n=I\nWφ(xt′)),\nwhere ⊙is an element-wise product.\nWhat happens if the largest eigenvalue emax = maxdiag(S) is larger than 1, the\nnorm of hl will explode, i.e., ∥hl∥→∞. Furthermore, due to the assumption that\nWφ(xt′) > 0, each element of hl will explode to inﬁnity as well. The rate of growth is\nexponentially with respect to the length of the input sequence, meaning that even when\nthe input sequence is not too long, the norm of the memory state grows quickly if emax\nis reasonably larger than 1.\nThis happens, because the nonlinearity g is unbounded. If g is bounded from both\nabove and below, such as the case with tanh, the norm of the memory state is also\nbounded. In the case of tanh : R →[−1,1],\n∥hl∥≤dim(hl).\nThis is one reason why a logistic function, such as tanh and σ, is most widely used\nwith recurrent neural networks, compared to piecewise linear functions.14 I will call\nthis recurrent neural network with tanh as an element-wise nonlinear function a simple\nrecurrent neural network.\n4.3.2\nIs tanh a Blessing?\nNow, the argument in the previous section may sound like tanh and σ are the nonlinear\nfunctions that one should use. This seems quite convincing for recurrent neural net-\nworks, and perhaps so for feedforward neural networks as well, if the network is deep\nenough.\nHere let me try to convince you otherwise by looking at how the norm of backprop-\nagated derivative behaves. Again, this is much easier to see if we assume the following:\n• U is a full rank matrix\n• The input sequence is sparse: ∑l\nt=1 Iφ(xt)̸=0 = c, where c = O(1)\nSimilarly to Eq. (4.14), let us consider a forward computational path until hl, how-\never without assuming a linear activation function:\nhl = g(Ug(Ug(Ug(U(···)+Wφ (xl−3))+Wφ (xl−2))+Wφ (xl−1))+Wφ (xl)).\nWe will consider a subsequence of this process, in which all the input symbols are 0\nexcept for the ﬁrst symbol:\nhl1 = g\n\u0000Ug\n\u0000U\n\u0000···g\n\u0000Uhl0 +Wφ\n\u0000xl0+1\n\u0001\u0001\u0001\u0001\u0001\n.\n14 However, it is not to say that piecewise linear functions are never used for recurrent neural networks.\nSee, for instance, [69, 6].\n49\nIt should be noted that as l approaches inﬁnity, there will be at least one such sub-\nsequence whose length also approaches inﬁnity due to the sparsity of the input we\nassumed.\nFrom this equation, let’s look at\n∂hl1\n∂φ\n\u0000xl0+1\n\u0001.\nThis measures the effect of the (l0 +1)-th input symbol xl0+1 on the l1-th memory state\nof the simple recurrent neural network. This is also the crucial derivative that needs to\nbe computed in order to compute the gradient of the cost function using the automated\nbackpropagation procedure described in Sec. 3.4.\nThis derivative can be rewritten as\n∂hl1\n∂φ\n\u0000xl0+1\n\u0001 =\n∂hl1\n∂hl0+1\n| {z }\n(a)\n∂hl0+1\n∂hl0+1\n∂hl0+1\n∂φ\n\u0000xl0+1\n\u0001.\nAmong these three terms in the left hand side, we will focus on the ﬁrst one (a) which\ncan be further expanded as\n∂hl1\n∂hl0+1\n=\n\n\n\n\n\n\n∂hl1\n∂hl1\n|{z}\n(b)\n∂hl1\n∂hl1−1\n| {z }\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n∂hl1−1\n∂hl1−1\n| {z }\n(b)\n∂hl1−1\n∂hl1−2\n| {z }\n(c)\n\n\n\n\n\n\n···\n\n\n\n\n\n\n∂hl0+2\n∂hl0+2\n| {z }\n(b)\n∂hl0+2\n∂hl0+1\n| {z }\n(c)\n\n\n\n\n\n\n.\n(4.16)\nBecause this is a recurrent neural network, we can see that the analytical forms for\nthe terms grouped by the parentheses in the above equation are identical except for the\nsubscripts indicating the time index. In other words, we can simply only on one of\nthose groups, and the resulting analytical form will be generally applicable to all the\nother groups.\nFirst, we look at Eq. (4.16) (b), which is nothing but a derivative of a nonlinear\nactivation function used in this simple recurrent neural network. The derivatives of the\nwidely used logistic functions are\nσ′(x) =σ(x)(1−σ(x)),\ntanh′(x) =1−tanh2(x),\nas described earlier in Sec. 3.4.1. Both of these functions’ derivatives are bounded:\n0 < σ′(x) ≤0.25,\n(4.17)\n0 < tanh′(x) ≤1.\n(4.18)\nIn the simplest case in which g is a linear function (i.e., x = g(x),) we do not even\nneed to look at\n\r\r\r ∂ht\n∂ht\n\r\r\r. We simply ignore all the ∂ht\n∂ht from Eq. (4.16).\n50\nNext, consider Eq. (4.16) (c). In this case of simple recurrent neural network, we\nnotice that we have already learned how to compute this derivative earlier in Sec. 3.3.2:\n∂ht+1\n∂ht\n= U\nFrom these two, we get\n∂hl1\n∂hl0+1\n=\n\u0012∂hl1\n∂hl1\nU\n\u0013\u0012∂hl1−1\n∂hl1−1\nU\n\u0013\n···\n\u0012∂hl0+2\n∂hl0+2\nU\n\u0013\n=\nl1\n∏\nt=l0+2\n\u0012∂ht\n∂ht\nU\n\u0013\n.\nDo you see how similar it looks like Eq. (4.15)? If the recurrent activation function\nf is linear, this whole term reduces to\n∂hl1\n∂hl0+1\n= Ul1−l0+1,\nwhich according to Sec. 4.3.1, will explode as l →∞if\nemax > 1,\nwhere emax is the largest eigenvalue of U. When emax < 1, it will vanish, i.e., ∥\n∂hl1\n∂hl0+1 ∥→\n0, exponentially fast.\nWhat if the recurrent activation function f is not linear at all? Let’s look at ∂ht\n∂ht U as\n∂ht\n∂ht\nU =\n\n\nf ′\n1\n0\n···\n0\n0\nf ′\n2\n···\n0\n...\n...\n···\n...\n0\n0\n···\nf ′\nnh\n\n\n|\n{z\n}\n=diag\n\u0010 ∂ht\n∂ht\n\u0011\n\u0000QSQ−1\u0001\n,\nwhere we used the eigendecomposition of U = QSQ−1. This can be re-written into\n∂ht\n∂ht\nU = Q\n\u0012\ndiag\n\u0012∂ht\n∂ht\n\u0013\n⊙S\n\u0013\nQ−1.\nThis means that the eigenvalue of U will be scaled by the derivative of the recurrent ac-\ntivation function at each timestep. In this case, we can bound the maximum eigenvalue\nof ∂ht\n∂ht U by\net\nmax ≤λemax,\nwhere λ is the upperbound on g′ = ∂ht\n∂ht . See Eqs. (4.17)–(4.18) for the upperbounds of\nthe sigmoid and hyperbolic tangent functions.\n51\nIn other words, if the largest eigenvalue of U is larger than 1\nλ , it is likely that this\ntemporal derivative of hl1 with respect to hl0+1 will explode, meaning that its norm will\ngrow exponentially large. In the opposite case of emax < 1\nλ , the norm of the temporal\nderivative likely shrinks toward 0. The former case is referred to as exploding gradient,\nand the latter vanishing gradient. These cases were studied already at the very early\nyears of research in recurrent neural networks [9, 52].\nUsing tanh is a blessing in recurrent neural networks when running the network\nforward, as I described in the previous section. This is however not necessarily true in\nthe case of backpropagating derivaties. Especially because, there is a higher chance of\nvanishing gradient with tanh, or even worse with σ. Why? Because 1\nλ > 1 for almost\neverywhere.\n4.3.3\nAre We Doomed?\nExploding Gradient\nFortunately it turned out that the phenomenon of exploding\ngradient is quite easy to address. First, it is straightforward to detect whether the ex-\nploding gradient happened by inspecting the norm of the gradient fo the cost with\nrespect to the parameters\n\r\r∇θ ˜C\n\r\r. If the gradient’s norm is larger than some predeﬁned\nthrehold τ > 0, we can simply renormalize the norm of the gradient to be τ. Otherwise,\nwe leave it as it is.\nIn mathematics,\n˜∇=\n(\nτ ∇\n∥∇∥,\nif ∥∇∥> τ\n∇,\notherwise\n,\nwhere we used the shorthand notiation ∇for ∇θ ˜C.\n˜∇is a rescaled gradient update\ndirection which will be used by the stochastic gradient descent (SGD) algorithm from\nSec. 2.2.2. This algorithm is referred to as gradient clipping [83].\nVanishing Gradient\nWhat about vanishing gradient? But, ﬁrst, what does vanishing\ngradient mean? We need to understand the meaning of this phenomenon in order to tell\nwhether this is a problem at all from the beginning.\nLet us consider a case the variable-length output where |x| = |y| from Sec. 4.1.4.\nLet’s assume that there exists a clear dependency between the output label yt and the\ninput symbol xt′, where t′ ≪t. This means that the empirical cost will decrease when\nthe weights are adjusted such that\nlog p(yt = y∗\nt |...,φ(xt′),...)\nis maximized, where y∗\nt is the ground truth output label at time t. The value of φ(xt′)\nhas great inﬂuence on the t-th output yt, and the inﬂuence can be measured by\n∂log p(yt = y∗\nt |...)\n∂φ(xt′)\n.\nInstead of exactly computing ∂log p(yt=y∗t |...)\n∂φ(xt′)\n, we can approximate it by the ﬁnite\ndifference method. Let ε ∈Rdim(φ(xt′)) be a vector of which each element is a very\n52\nsmall real value (ε ≈0.) Then,\n∂log p(yt = y∗\nt |...)\n∂φ(xt′)\n= lim\nε→0(log p(yt = y∗\nt |...,φ(xt′)+ε,...)\n−log p(yt = y∗\nt |...,φ(xt′),...,))⊘ε,\nwhere ⊘is an element-wise division. This shows that ∂log p(yt=y∗t |...)\n∂φ(xt′)\ncomputes the\ndifference in the t-th output probability with respect to the change in the value of the\nt′-th input.\nIn other words ∂log p(yt=y∗t |...)\n∂φ(xt′)\ndirectly reﬂects the degree to which the t-th output\nyt depends on the t′-th input xt′, according to the network. To put it in another way,\n∂log p(yt=y∗t |...)\n∂φ(xt′)\nreﬂects how much dependency the recurrent neural network has cap-\ntured the dependency between yt and xt′.\nLet’s rewrite\n∂log p(yt = y∗\nt |...)\n∂φ(xt′)\n= ∂log p(yt = y∗\nt |...)\n∂ht\n∂ht\n∂ht−1\n··· ∂ht′+1\n∂ht′\n|\n{z\n}\n(a)\n∂ht′\n∂φ(xt).\nThe terms marked with (a) looks exactly identical to Eq. (4.16). We have already seen\nthat this term can easily vanish toward zero with a high probability (see Sec. 4.3.2.)\nThis means that the recurrent neural network is unlikely to capture this dependency.\nThis is especially true when the (temporal) distance between the output and input, i.e.,\n|t −t′| ≫0.\nThe biggest issue with this vanishing behaviour is that there is no straightforward\nway to avoid it. We cannot tell whether ∂log p(yt=y∗t |...)\n∂φ(xt′)\n≈0 is due to the lack of this\ndependency in the true, underlying function or due to the wrong conﬁguration (param-\neter setting) of the recurrent neural network. If we are certain that there are indeed\nthese long-term dependencies, we may simultaneously minimize the following auxil-\niary term together with the cost function:\nT\n∑\nt=1\n\n1−\n\r\r\r\n∂˜C\n∂ht+1\n∂ht+1\n∂ht\n\r\r\r\n\r\r\r\n∂˜C\n∂ht+1\n\r\r\r\n\n\n2\n.\nThis term, which was introduced in [83], is minimized when the norm of the derivative\ndoes not change as it is being backpropagated, effectively forcing the gradient not to\nvanish.\nThis term however was found to help signiﬁcantly only when the target task, or the\nunderlying function, does indeed exhibit long-term dependencies. How can we know\nin advance? Pascanu et al. [83] showed this with the well-known toy tasks which were\nspeciﬁcally designed to exhibit long-term dependencies [52].\n4.3.4\nGated Recurrent Units Address Vanishing Gradient\nWill the same problems of vanishing gradient happen with the gated recurrent units\n(GRU) or the long short-term memory units (LSTM)? Let us write the memory state at\n53\ntime t:\nht =ut ⊙˜ht +(1−ut)⊙\n\u0000ut−1 ⊙˜ht−1 +(1−ut−1)⊙\n\u0000ut−2 ⊙˜ht−2 +(1−ut−2)⊙(···)\n\u0001\u0001\n=ut ⊙˜ht +(1−ut)⊙ut−1 ⊙˜ht−1 +(1−ut)⊙(1−ut−1)⊙ut−2 ⊙˜ht−2 +···\nLet’s be more speciﬁc and see what happens to this with respect to xt′:\nht =ut ⊙˜ht +(1−ut)⊙ut−1 ⊙˜ht−1 +(1−ut)⊙(1−ut−1)⊙ut−2 ⊙˜ht−2 +···\n+\n \n∏\nk=t,...,t′+1\n(1−uk)\n!\n⊙ut′\n|\n{z\n}\n(a)\n⊙tanh(Wφ(xt′)+U(rt′ ⊙ht′−1)),\n(4.19)\nwhere ∏is for element-wise multiplication.\nWhat this implies is that the GRU effectively introduces a shortcut from time t′ to\nt. The change in xt′ will directly inﬂuence the value of ht, and subsequently the t-th\noutput symbol yt. In other words, all the issue with the simple recurrent neural network\nwe discussed earlier in Sec. 4.3.3.\nThe update gate controls the strength of these shortcuts. Let’s assume for now that\nthe update gate is ﬁxed to some predeﬁned value between 0 and 1. This effectively\nmakes the GRU a leaky integration unit [6]. However, as it is perhaps clear from\nEq. (4.19) that we will inevitably run into an issue. Why is this so?\nLet’s say we are sure that there are many long-term dependencies in the data. It is\nnatural to choose a large coefﬁcient for the leaky integration unit, meaning the update\ngate is close to 1. This will deﬁnitely help carrying the dependency across many time\nsteps, but this inevitably carries unnecessary information as well. This means that\nmuch of the representational power of the output function gout(ht) is wasted in ignoring\nthose unnecessary information.\nIf the update gate is ﬁxed to something substantially smaller than 1, all the shortcuts\n(see Eq. (4.19) (a)) will exponentially vanish. Why? Because it is a repeated multipli-\ncation of a scalar small than 1. In other words, it does not really help to have a leaky\nintegration unit in the place of a simple tanh unit.\nThis is however not the case with the actual GRU or LSTM, because those update\ngates are not ﬁxed but are adaptive with respect to the input. If the network detects\nthat there is an important dependency being captured, the update gate will be closed\n(uj ≈0.) This will effectively strengthen the shortcut connection (see Eq. (4.19) (a).)\nWhen the network detects that there is no dependency anymore, it will open the update\ngate (uj ≈1), which effectively cuts off the shortcut. How does the network know, or\ndetect, the existence or lack of these dependencies? Do we need to manually code this\nup? I will leave these questions for you to ﬁgure out.\n54\nChapter 5\nNeural Language Models\n5.1\nLanguage Modeling: First Step\nWhat does it mean for a machine to understand natural language? In other words, how\ncan we tell that the machine understood natural language? These are the two equivalent\nquestions that are at the core of this course.\nOne of the most basic capability of a machine that can signal us that it indeed\nunderstands natural language is for the machine to tell how likely a given sentence\nis. Of course, this is extremely ill-deﬁned, as we probably cannot deﬁne the likeliness\nof a sentence, because there are many different types of unlikeliness. For instance,\na sentence “Colorless green ideas sleep furiously” from Chomsky’s [32] is unlikely\naccording to our common sense, because\n1. An object (“idea”) cannot be both “colorless” and “green.”\n2. An object cannot “sleep” “furiously.”\n3. An “idea” does not “sleep.”\nOn the other hand, this sentence is a grammatically correct sentence.\nLet’s take a look at another sentence “Jane and me went to see a movie yesterday.”\nGrammatically, this is not the most correct sentence one can make. It should be “Jane\nand I went to see a movie yesterday.” Even with a grammatical error in the original\nsentence however, the meaning of the sentence is clear to me, and perhaps is much more\nunderstandable than the sentence “colorless green ideas sleep furiously.” Furthermore,\nmany people likely say this (saying “me” instead of “I”) quite often. This sentence is\nthus likely according to our common sense, but is not likely according to the grammar.\nThis observation makes us wonder what is the criterion to use. Is it correct for a\nmachine to tell whether the sentence is likely by analyzing its grammatical correctness?\nOr, is it possible that the machine should deem a sentence likely only when its meaning\nagrees well with common sense regardless of its grammatical correctness in the most\nstrict sense?\nAs we discussed in the ﬁrst lecture of the course, we are more interested in ap-\nproaching natural language as a means for one to communicate ideas to a listener. In\n55\nthis sense, language use is a function which takes as input the surrounding environ-\nment, including the others’ speech, and returns linguistic response, and this function\nis not given but learned via observing others’ use of language and the reinforcement\nby the surrounding environment [97]. Also, throughout this course, we are not con-\ncerned too much about the existing syntactic, or grammatical, structures underlying\nnatural language, which makes it difﬁcult for us to say anything about the grammatical\ncorrectness of a given sentence.\nIn short, we take the route here that the likeliness of a sentence be determined\nbased on its agreement with common sense. The common sense here is captured by\neveryday use of natural language, which consequently implies that the statistics of\nnatural language use can be a strong indicator for determining the likely of a natural\nlanguage sentence.\n5.1.1\nWhat if those linguistic structures do exist\nOf course, as we discussed earlier in Sec. 1.1 and in this section, not everyone agrees.\nThis is due to the fact that a perfect grammatical sentence may be considered unlikely,\njust because it does not happen often. In other words, statistical approaches to language\nmodeling may conclude that a sentence with perfectly valid grammatical construction\nis unlikely. Is this a problem?\nThis problem of telling how likely a given sentence is can be viewed very naturally\nas building a probabilistic model of sentences. In other words, given a sentence S, what\nis the probability p(S) of S? Let us brieﬂy talk about what this means for the case of\nviewing the likeliness of a sentence as equivalent to its grammatical correctness.1\nWe ﬁrst assume that there is an underlying linguistic structure G which has gener-\nated the observed sentence S. Of course, we do not know the correct G in advance, and\nunfortunately no one will tell us what the correct G is.2 Thus, G is a hidden variable\nin this case. This hidden structure G generates the observed sentence S according to an\nunknown conditional distribution p(S|G). Each and every grammatical structure G is\nassigned a prior probability which is also unknown in advance.3\nWith the conditional distribution S|G and the prior distribution G, we easily get the\njoint distribution S,G by\np(S,G) = p(S|G)p(G),\nfrom the deﬁnition of conditional probability.4 From this joint distribution we get the\n1 Why brieﬂy and why here? Because, we will not pursue this line at all after this section.\n2 Here, the correct G means the G that generated S, not the whole structure of G which is assumed to\nexist according to a certain set of rules.\n3 This is not necessarily true. If we believe that each and every grammatical correct sentence is equally\nlikely and that each correct grammatical structure generates a single corresponding sentence, the prior dis-\ntribution over the hidden linguistic structure is such that any correct structure is given an equal probability\nwhile any incorrect structure is given a zero probability. But, of course, if we think about it, there are clearly\ncertain structures that are more prevalent and others that are not.\n4 A conditional probability of A given B is deﬁned as\np(A|B) = p(A,B)\np(B)\n56\ndistribution over a given sentence S by marginalizing out G:\np(S) = ∑\nG\np(S,G).\nThis means that we should compute how likely a given sentence S is with respect to all\npossible underlying linguistic structure. This is very likely intractable, because there\nmust be inﬁnite possible such structures.\nInstead of computing p(S) exactly we can simply look at its lowerbound. For in-\nstance, one simplest, and probably not the best, way to do so is\np(S) = ∑\nG\np(S,G) ≥p(S, ˆG),\nwhere ˆG = argmaxG p(S,G) = argmaxG p(G|S).5\nThis lowerbound is tight, i.e., p(S) = p(S, ˆG), when there is only a single true un-\nderlying linguistic structure ˆG given S. What this says is that there is no other possible\nlinguistic structure possible for a single observed sentence, i.e., no ambiguity in infer-\nring the correct linguistic structure. In other words, we can compute the probability or\nlikeliness of a given sentence by inferring its correct underlying linguistic structure.\nHowever, there are a few issues here. First, it is not clear which formalism G\nfollows, and we have brieﬂy discussed about this at the very beginning of this course.\nSecond, it is quite well known that most of the formalisms do indeed have uncertainty\nin inference. Again, we looked at one particular example in Sec. 1.1.2. These two\nissues make many people, including myself, quite uneasy about this type of model-\nbased approaches.\nIn the remaining of this chapter, I will thus talk about model-free approaches (as\nopposed to these model-based approaches.)\n5.1.2\nQuick Note on Linguistic Units\nBefore continuing, there is one question that must be bugging you, or at least has\nbugged me a lot: what is the minimal linguistic unit?\nIf we think about written text, the minimal unit does seem like a character. With\nspoken language, the minimal unit seems to be a phoneme. But, is this the level at\nwhich we want to model the process of understanding natural language? In fact, to\nmost of the existing natural language processing researchers as well as some (or most)\nlinguists, the answer to this question is a hard “no.”\nThe main reason is that these low-level units, both characters and phonemes, do not\nconvey any meaning themselves. Does a Latin alphabet “q” have its own meaning? The\nanswer by most of the people will be “no.” Then, starting from this alphabet “q”, how\nfar should we climb up in the hierarchy of linguistic units to reach a level at which the\nunit begins to convey its own meaning? “qu” does not seem to have its own meaning\nstill. “qui” in French means “who”, but in English it does not really say much. “quit”\nin English is a valid word that has its own meaning, and similarly “quiet” is a valid\nword that has its own meaning, quite apart from that of “quit.”\n5 This inequality holds due to the deﬁnition of probability, which states that p(X) ≥0 and ∑X p(X) = 1.\n57\nIt looks like a word is the level at which meaning begins to form itself. However,\nthis raises a follow-up question on the deﬁnition of a word: What is a word?\nIt is tempting to say that a sequence of non-blank characters is a word. This makes\neveryone’s life so much easier, because we can simply split each sentence by a blank\nspace to get a sequence of words. Unfortunately this is a very bad strategy. The sim-\nplest counter example to this deﬁnition of words is a token (which I will use to refer to\na sequence of non-blank characters) consisting of a word followed by a punctuation.\nIf we simply split a sentence into words by blank spaces, we will get a bunch of re-\ndundant words. For instance, “llama”, “llama,”, “llama.”, “llama?”, “”llama”, “llama””\nand “llama!” will all be distinct words. We will run into an issue of exploding vocab-\nulary with any morphologically rich language. Furthermore, in some languages such\nas Chinese, there is no blank space at all inside a sentence, in which case this simple\nstrategy will completely fail to give us any meaningful, small linguistic unit other than\nsentences.\nNow at this point it almost seems like the best strategy is to use each character\nas a linguistic unit. This is not necessarily true due to the highly nonlinear nature of\northography.6 There are many examples in which this nonlinear nature shows its dif-\nﬁculty. One such example is to consider the following three words: “quite”, “quiet”\nand “quit”.7 All three character sequences have near identical forms, but their corre-\nsponding meanings differ from each other substantially. In other words, any function\nthat maps from these character sequences to the corresponding meanings will have to\nbe extremely nonlinear and thus difﬁcult to be learned from data. Of course, this is an\narea with active research, and I hope I am not giving you an impression that characters\nare not the units to use (see, e.g., [61].)\nNow then the question is whether there is some middle ground between characters\nand words (or blank-space-separated tokens) that are more suitable to be used as ele-\nmentary linguistic units (see, e.g., [93].) Unfortunately this is again an area with active\nresearch. Hopefully, we will have time later in the course to discuss this issue further.\nFor now, we will simply use blank-space-separated tokens as our linguistic units.\n5.2\nStatistical Language Modeling\nRegardless of which linguistic unit we use, any natural language sentence S can be\nrepresented as a sequence of T discrete symbols such that\nS = (w1,w2,...,wT).\nEach symbol is one element from a vocabulary V which contains all possible symbols:\nV =\n\b\nv1,v2,...,v|V|\n\t\n,\nwhere |V| is used to mean the size of the vocabulary, or the number of all symbols.\n6 Orthography is deﬁned as “the study of spelling and how letters combine to represent sounds and form\nwords.”\n7 I would like to thank Bart van Merrienboer for this example.\n58\nThe problem of language modeling is equivalent to ﬁnding a model that assigns a\nprobability p(S) to a sentence:\np(S) = p(w1,w2,...,wT).\n(5.1)\nOf course, we are not given this distribution and need to learn this from data.\nLet’s say we are given data D which contains N sentences such that\nD =\n\b\nS1,S2,...,SN\t\n,\nwhere each sentence Sn is\nSn = (wn\n1,wn\n2,...,wn\nT n),\nmeaning that each sentence has a different length.\nGiven this data D, let us estimate the probability of a certain sentence S. This is\nquite straightforward:\np(S) = ∑N\nn=1 IS=Sn\nN\n,\n(5.2)\nwhere I is the indicator function deﬁned earlier in Eq. (3.7) which is deﬁned as\nIS=Sn =\n\u001a\n1,\nif S = Sn\n0,\notherwise\nThis is equivalent to counting how many times S occurs in the data.8\n5.2.1\nData Sparsity/Scarcity\nHas this solved the whole problem of language model? No, unfortunately not. The\nvery major issue here is that however large your corpus is, it is unlikely to contain all\nreasonable sentences in the world. Let’s do simple counting here.\nThere are |V| symbols in a vocabulary. Each sentence can be as long as T symbols.\nThen, there are |V|T possible sentences. A reasonable range for the sentence length T\nis roughly between 1 to 50, meaning that there are\n50\n∑\nT=1\n|V|T\npossible sentences. As it’s quite clear, this is a huge space of sentences.\nOf course, not all those sentences are plausible. This is however conceivable that\neven the fraction of that space will be gigantic, especially considering that the size of\nvocabulary often goes up to 100k to 1M words. Many of the plausible sentences will\nnot appear in the corpus. Is this true? In fact, yes, it is.\nIt is quite easy to ﬁnd such an example.\nFor instance, Google Books Ngram\nViewer9 lets you search for a sentence or a sequence of up to ﬁve English words from\n8 A data set consisting of (written) text is often referred to as a corpus.\n9 https://books.google.com/ngrams\n59\nFigure 5.1: A picture of a llama lying down. From https://en.wikipedia.\norg/wiki/Llama\nthe gigantic corpus of Google Books. Let me try to search for a very plausible sen-\ntence “I like llama,” and the Google Books Ngram10 Viewer returns an error saying\nthat “Ngrams not found: I like llama.” (see Fig. 5.1 in the case you are not familiar\nwith a llama.) See Fig. 5.2 as an evidence.\nFigure 5.2: A resulting page of Google Books Ngram Viewer for the query “I like\nllama”.\nWhat does this mean for the estimate in Eq. (5.2)? It means that this estimator will\nbe too harsh for many of the plausible sentences that do not occur in the data. As soon\nas a given sentence does not appear exactly as it is in the corpus, this estimator will\nsay that there is a zero probability of the given sentence. Although the sentence “I like\nllama” is a likely sentence, according to this estimator in Eq. (5.2), it will be deemed\nextremely unlikely.\nThis problem is due to the issue of data sparsity. Data sparsity here refers to the\n10 We will discuss what Ngrams are in the later sections.\n60\nphenomenon where a training set does not cover the whole space of input sufﬁciently.\nIn more concrete terms, most of the points in the input space, which have non-zero\nprobabilities according to the true, underlying distribution, do not appear in the training\nset. If the size of a training set is assumed to be ﬁxed, the severity of data sparsity\nincreases as the average, or maximum length of the sentences. This follows from the\nfact that the size of the input space, the set of all possible sentences, grows with respect\nto the maximum possible length of a sentence.\nIn the next section, we will discuss the most straightforward approach to addressing\nthis issue of data sparsity.\n5.3\nn-Gram Language Model\nThe fact that the issue of data sparsity worsens as the maximum length of sentences\ngrows hints us a straightforward approach to addressing this: limit the maximum length\nof phrases/sentences we estimate a probability on. This idea is a foundation on which\na so-called n-gram language model is based.\nIn the n-gram language model, we ﬁrst rewrite the probability of a given sentence\nS from Eq. (5.1) into\np(S) = p(w1,w2,...,wT) = p(w1)p(w2|w1)··· p(wk|w<k)\n|\n{z\n}\n(a)\n··· p(wT|w<T),\n(5.3)\nwhere w<k denotes all the symbols before the k-th symbol wk.\nFrom this, the n-\ngram language model makes an important assumption that each conditional probability\n(Eq. (5.3) (a)) is only conditioned on the n−1 preceding symbols only, meaning\np(wk|w<k) ≈p(wk|wk−n,wk−n+1,...,wk−1).\nThis results in\np(S) ≈\nT\n∏\nt=1\np(wt|wt−n,...,wt−1).\nWhat does this mean? Under this assumption we are saying that any symbol in a\nsentence is predictable based on the n −1 preceding symbols. This is in fact a quite\nreasonable assumption in many languages. For instance, let us consider a phrase “I am\nfrom”. Even without any more context information surrounding this phrase, such as\nsurrounding words and the identity of a speaker, we know that the word following this\nphrase will be likely a name of place or country. In other words, the probability of a\nname of place or country given the three preceding words “I am from” is higher than\nthat of any other words.\nBut, of course, this assumption does not always work. For instance, consider a\nphrase “In Korea, more than half of all the residents speak Korean\n| {z }\n(a)\n.” Let us focus on\nthe last word “Korean” (marked with (a).) We immediately see that it will be useful\nto condition its conditional probability on the second word “Korea”. Why is this so?\n61\nBecause the conditional probability of “Korean” following “speak” should signiﬁcantly\nincrease over all the other words (that correspond to other languages) knowing the fact\nthat the sentence is talking about the residents of “Korea”. This requires the conditional\ndistribution to be conditioned on at least 10 words (“,” is considered a separate word,)\nand this certainly will not be captured by n-gram language model with n < 9.\nFrom these examples it is clear that there’s a natural trade-off between the quality\nof probability estimate and statistical efﬁciency based on the choice of n in n-gram\nlanguage modeling. The higher n the longer context the conditional distribution has,\nleading to a better model/estimate (second example,) however resulting in a situation\nof more sever data sparsity (see Sec. 5.2.1.) On the other hand, the lower n leads to\nthe worse language modeling (second example), but this will avoid the issue of data\nsparsity.\nn-gram Probability Estimation\nWe can estimate the n-gram conditional probability\np(wk|wk−n,...,wk−1) from the training corpus. Since it is a conditional probability, we\nneed to rewrite it according to the deﬁnition of the conditional probability:\np(wk|wk−n,...,wk−1) = p(wk−n,...,wk−1,wk)\np(wk−n,...,wk−1)\n(5.4)\nThis rewrite implies that the n-gram probability is equivalent to counting the occur-\nrences of the n-gram (wk−n,...,wk) among all n-grams starting with (wk−n,...,wk−1).\nLet us consider the denominator ﬁrst. The denominator can be computed by the\nmarginalizing the k-th word (w′ below):\np(wk−n,...,wk−1) = ∑\nw′∈V\np(wk−n,...,wk−1,w′).\n(5.5)\nFrom Eq. (5.2), we know how to estimate p(wk−n,...,wk−1,w′):\np(wk−n,...,wk−1,w′) ≈c(wk−n,...,wk−1,w′)\nNn\n,\n(5.6)\nwhere c(·) is the number of occurrences of the given n-gram in the training corpus, and\nNn is the number of all n-grams in the training corpus.\nNow let’s plug Eq. (5.6) into Eqs. (5.4)–(5.5):\np(wk|wk−n,...,wk−1) =\n\u0013\u0013\n1\nNn c(wk−n,...,wk−1,wk)\n\u0013\u0013\n1\nNn ∑w′∈V c(wk−n,...,wk−1,w′)\n(5.7)\n5.3.1\nSmoothing and Back-Off\nNote that I am missing many references this section, as I am writing this on my travel.\nI will ﬁll in missing references once I’m back from my travel.\nThe biggest issue of having an n-gram that never occurs in the training corpus is\nthat any sentence containing the n-gram will be given a zero probability regardless\nof how likely all the other n-grams are. Let us continue with the example of “I like\n62\nllama”. With an n-gram language model built using all the books in Google Books, the\nfollowing, totally valid sentence11 will be given a zero probability:\n• “I like llama which is a domesticated South American camelid.12\nWhy is this so? Because the probability of this sentence is given as a product of all\npossible trigrams:\np(“I”, “like”, “llama”, “which”, “is”, “a”, “domesticated”, “South”, “American”, “camelid”)\n=p(“I”)p(“like”|“I”) p(“llama”|“I”,“like”)\n|\n{z\n}\n=0\n··· p(“camelid”|“South”,“American”)\n=0\nOne may mistakenly believe that we can simply increase the size of corpus (col-\nlecting even more data) to avoid this issue. However, remember that “data sparsity is\nalmost always an issue in statistical modeling” [24], which means that more data call\nfor better statistical models with often more parameters leading to the issue of data\nsparsity.\nOne way to alleviate this problem is to assign a small probability to all unseen\nn-grams. At least, in this case, we will assign some small, non-zero probability to\nany sentence, thereby avoiding a valid, but zero-probability sentence under the n-gram\nlanguage model. One simplest implementation of this approach is to assume that each\nand every n-gram occurs at least α times and any occurrence in the training corpus is\nin addition to this background occurrence.\nIn this case, the estimate of an n-gram becomes\np(wk|wk−n,...,wk−1) =\nα +c(wk−n,wk−n+1,...,wk)\n∑w′∈V(α +c(wk−n,wk−n+1,...,w′))\n=\nα +c(wk−n,wk−n+1,...,wk)\nα|V|+∑w′∈V c(wk−n,wk−n+1,...,w′),\nwhere c(wk−n,wk−n+1,...,wk) is the number of occurrences of the given n-gram in the\ntraining corpus. c(wk−n,wk−n+1,...,w′) is the number of occurrences of the given n-\ngram if the last word wk is substituted with a word w′ from the vocabulary V. α is often\nset to be a scalar such that 0 < α ≤1. See the difference from the original estimate in\nEq. (5.7).\nIt is quite easy to see that this is a quite horrible estimator: how does it make sense\nto say that every unseen n-gram occurs with the same frequency? Also, knowing that\nthis is a horrible approach, what can we do about this?\nOne possibility is to smooth the n-gram probability by interpolating between the\nestimate of the n-gram probability in Eq. (5.7) and the estimate of the (n −1)-gram\nprobability. This can written down as\npS(wk|wk−n,...,wk−1) =λ(wk−n,...,wk−1)p(wk|wk−n,...,wk−1)\n+(1−λ(wk−n,...,wk−1))pS(wk|wk−n+1,...,wk−1). (5.8)\n11 This is not strictly true, as I should put “a” in front of the llama.\n12 The description of a llama taken from Wikipedia: https://en.wikipedia.org/wiki/Llama\n63\nThis implies that the n-gram (smoothed) probability is computed recursively by the\nlower-order n-gram probabilities. This is clearly an effective strategy, considering that\nfalling off to the lower-order n-grams contains at least some information of the original\nn-gram, unlike the previous approach of adding a scalar α to every possible n-gram.\nNow a big question here is how the interpolation coefﬁcient λ is computed. The\nsimplest approach we can think of is to ﬁt it to the data as well. However, the situ-\nation is not that easy, as using the same training corpus, which was used to estimate\np(wk|wk−n,...,wk−1) according to Eq. (5.7), will lead to a degenerate case. What is\nthis degenerate case? If the same corpus is used to ﬁt both the non-smoothed n-gram\nprobability and λ’s, the optimal solution is to simply set all λ’s to 1, as that will assign\nthe high probabilities to all the n-grams. Therefore, one needs to use a separate corpus\nto ﬁt λ’s.\nMore generally, we may rewrite Eq. (5.8) as\npS(wk|wk−n,...,wk−1) =\n\u001a α(wk|wk−n,...,wk−1), if c(wk−n,...,wk−1,wk) > 0\nγ(wk−n+1,...,wk)pS(wk|wk−n+1,...,wk−1), otherwise\n(5.9)\nfollowing the notation introduced in [63]. Speciﬁc choices of α and γ lead to a number\nof different smoothing techniques. For an extensive list of these smoothing techniques,\nsee [24].\nBefore ending this section on smoothing techniques for n-gram language modeling,\nlet me brieﬂy describe one of the most widely used smoothing technique, called the\nmodiﬁed Kneser-Ney smoothing (KN smoothing), described in [24]. This modiﬁed\nKN smoothing is efﬁciently implemented in the open-source software package called\nKenLM [51].\nFirst, let us deﬁne some quantities. We will use nk to denote the total number of\nn-grams that occur exactly k times in the training corpus. With this, we deﬁne the\nfollowing so-called discounting factors:\nY =\nn1\nn1 +2n2\nD1 =1−2Y n2\nn1\nD2 =2−3Y n3\nn2\nD3+ =3−4Y n4\nn3\n.\nAlso, let us deﬁne the following quantities describing the number of all possible words\nfollowing a given n-gram with a speciﬁed frequency l:\nNl(wk−n,...,wk−1) = |{c(wk−n,...,wk−1,wk) = l}|\nThe modiﬁed KN smoothing then deﬁnes α in Eq. (5.9) to be\nα(wk|wk−n,...,wk−1) = c(wk−n,...,wk−1,wk)−D(c(wk−n,...,wk−1,wk))\n∑w′∈V c(wk−n,...,wk−1,w′)\n,\n64\nwhere D is\nD(c) =\n\n\n\n\n\n\n\n0,\nif c = 0\nD1,\nif c = 1\nD2,\nif c = 2\nD3+,\nif c ≥3\nAnd, γ is deﬁned as\nγ(wk−n,...,wk−1) = D1N1(wk−n,...,wk−1)+D2N2(wk−n,...,wk−1)+D3+N3+(wk−n,...,wk−1)\n∑w′∈V c(wk−n,...,wk−1,w′)\n.\nFor details on how this modiﬁed KN smoothing has been designed, see [24].\n5.3.2\nLack of Generalization\nAlthough n-gram language modelling works like a charm in many cases. This is still\nnot totally satisfactory, because of the lack of generalization. What do I mean by\ngeneralization here?\nConsider an example where three trigrams13 were observed from a training corpus:\n“chases a cat”, “chases a dog” and “chases a rabbit”. There is a clear pattern here. The\npattern is that it is highly likely that “chases a” will be followed by an animal.\nHow do we know this? This is a trivial example of humans’ generalization abil-\nity. We have noticed a higher-level concept, in this case an animal, from observing\nwords such as “cat”, “dog” and “rabbit”, and based on this concept, we generalize this\nknowledge (that “chases a” is followed by an animal) to unseen trigrams in the form of\n“chases a [animal]”.\nThis however does not happen with n-gram language model. As an example, let’s\nconsider a trigram “chases a llama”. Unless this speciﬁc trigram occurred more than\nonce in the training corpus, the conditional probability given by n-gram language mod-\neling will be zero.14 This issue is closely related to data sparsity, but the main differ-\nence is that it is not the lack of data, or n-grams, but the lack of world knowledge. In\nother words, there exist relevant n-grams in the training corpus, but n-gram language\nmodelling is not able to exploit these.\nAt this point, it almost seems trivial to address this issue by incorporating existing\nknowledge into language modelling. For instance, one can think of using a dictionary\nto ﬁnd the deﬁnition of a word in interest (continuing on from the previous example,\nthe deﬁnition of “llama”) and letting the language model notice that “llama” is a “a\n13 Is “trigram” a proper term? Certainly not, but it is widely accepted by the whole community of natural\nlanguage processing researchers. Here’s an interesting discussion on how n-grams should be referred to\nas, from [77]: “these alternatives are usually referred to as a bigram, a trigram, and a four-gram model,\nrespectively. Revealing this will surely be enough to cause any Classicists who are reading this book to stop,\nand to leave the ﬁeld to uneducated engineering sorts ... with the declining levels of education in recent\ndecades ... some people do make an attempt at appearing educated by saying quadgram ”\n14 Here we assume that no smoothing or backoff is used. However, even when these techniques are used,\nwe cannot be satisﬁed, since the probability assigned to this trigram will be at best reasonable up to the point\nthat the n-gram language model is giving as high probability as the bigram “chases a”. In other words, we do\nnot get any generalization based on the fact that a “llama” is an animal similar to a “cat”, “dog” or “rabbit”.\n65\ndomesticated pack animal of the camel family found in the Andes, valued for its soft\nwoolly ﬂeece.” Based on this, the language model should ﬁgure out that the probability\nof “chases a llama” should be similar to “chases a cat”, “chases a dog” or “chases a\nrabbit” because all “cat”, “dog” and “rabbit” are animals according to the dictionary.\nThis is however not satisfactory for us. First, those deﬁnitions are yet another\nnatural language text, and letting the model understand it becomes equivalent to nat-\nural language understanding (which is the end-goal of this whole course!) Second,\na dictionary or any human-curated knowledge base is an inherently limited resource.\nThese are limited in the sense that they are often static (not changing rapidly to reﬂect\nthe changes in language use) and are often too generic, potentially not capturing any\ndomain-speciﬁc knowledge.\nIn the next section, I will describe an approach purely based on statistics of natural\nlanguage that is able to alleviate this lack of generalization.\n5.4\nNeural Language Model\nOne thing we notice from n-gram language modelling is that this boils down to com-\nputing the conditional distribution of a next word wk given n −1 preceding words\nwk−n,...,wk−1. In other words, the goal of n-gram language modeling is to ﬁnd a\nfunction that takes as input n−1 words and returns a conditional probability of a next\nword:\np(wk|wk−n,...,wk−1) = f wk\nθ (wk−n,...,wk−1).\nThis is almost exactly what we have learned in Chapter 2.\nFirst, we should deﬁne the input to this language modelling function. Clearly the\ninput will be a sequence of n −1 words, but the question is how each of these words\nwill be represented. Since our goal is to put the least amount of prior knowledge, we\nwant to represent each word such that each and every word in the vocabulary is equi-\ndistant away from the others. One encoding scheme that achieves this goal is 1-of-K\ncoding.\nIn this 1-of-K coding scheme, each word i in the vocabulary V is represented as a\nbinary vector wi whose sum equals 1. To denote the i-th word with the vector wi, we\nset the i-th element of the vector wi to be 1 (and consequently all the other elements\nare set to zero.) Mathematically,\nwi = [0,0,...,\n1\n|{z}\ni-th element\n,...,0]⊤∈{0,1}|V|\n(5.10)\nThis kind of vector is often called a one-hot vector.\nIt is easy to see that this encoding scheme perfectly ﬁts our goal of having minimal\nprior, because\n|wi −w j| =\n\u001a\n1,\nif i ̸= j\n0,\notherwise\n66\nNow the input to our function is a sequence of n −1 such vectors, which I will\ndenote by (w1,w2,...,wn−1). As we will use a neural network as a function approx-\nimator here,15 these vectors will be multiplied with a weight matrix E. After this, we\nget a sequence of continuous vectors (p1,p2,...,pn−1), where\npj = E⊤w j\n(5.11)\nand E ∈R|V|×d.\nBefore continuing to build this function, let us see what it means to multiply the\ntranspose of a matrix with an one-hot vector from left. Since only one of the elements\nof the one-hot vector is non-zero, all the rows of the matrix will be ignored except for\nthe row corresponding to the index of the non-zero element of the one-hot vector. This\nrow is multiplied by 1, which simply gives us the same row as the result of this whole\nmatrix–vector multiplication. In short, the multiplication of the transpose of a matrix\nwith an one-hot vector is equivalent to slicing out a single row from the matrix.\nIn other words, let\nE =\n\n\ne1\ne2\n...\ne|V|\n\n,\n(5.12)\nwhere ei ∈Rd. Then,\nE⊤wi = ei.\nThis view has two consequences. First, in practice, it will be much more efﬁcient\ncomputationally to implement this multiplication as a simple table look-up. For in-\nstance, in Python with NumPy, do\np = E[i,:]\ninstead of\np = numpy.dot(E.T, w_i)\nSecond, from this perspective, we can see each row of the matrix E as a continuous-\nspace representation of a corresponding word. ei will be a vector representation of the\ni-th word in the vocabulary V. This representation is often called a word embedding\nand should reﬂect the underlying meaning of the word. We will discuss this further\nshortly.\nClosely following [8], we will simply concatenate the continuous-space represen-\ntations of the input words such that\np =\n\u0002\np1;p2;...;pn−1\u0003⊤\n15 Obviously, this does not have to be true, but at the end of the day, it is unclear if there is any parametric\nfunction approximation other than neural networks.\n67\nThis vector p is a representation of n−1 input words in a continuous vector space and\noften referred to as a context vector.\nThis context vector is fed through a composition of nonlinear feature extraction\nlayers. We can for instance apply the simple transformation layer from Eq. (3.8) such\nthat\nh = tanh(Wp+b),\n(5.13)\nwhere W and b are the parameters.\nOnce a set of nonlinear layers has been applied to the context vector, it’s time to\ncompute the output probability distribution. In this case of language modelling, the\ndistribution outputted by the function is a categorical distribution. We discussed how\nwe can build a function to return a categorical distribution already in Sec. 3.1.2.\nAs a recap, a categorical distribution deﬁnes a probability of one event happening\namong K discrete events. The probability of the k-th event happening is often denoted\nas µk, and\nK\n∑\nk=1\nµk = 1.\nTherefore, the function needs to return a K-dimensional vector [µ1,µ2,...,µK]. In this\ncase of language modelling, K = |V| and µi corresponds to the probability of the i-th\nword in the vocabulary for the next word.\nAs discussed earlier in Sec. 3.1.2, we can use softmax to compute each of those\noutput probabilities:\np(wn = k|w1,w2,...,wn−1) = µk =\nexp(u⊤\nk h+ck)\n∑\n|V|\nk′=1 exp(u⊤\nk′h+ck′)\n,\n(5.14)\nwhere uk ∈Rdim(h).\nThis whole function is called a neural language model. See Fig. 5.3 (a) for the\ngraphical illustration of neural language model.\n5.4.1\nHow does Neural Language Model Generalize to Unseen n-\nGrams? – Distributional Hypothesis\nNow that we have described neural language model, let us take a look into what hap-\npens inside. Especially, we will focus on how the model generalizes to unseen n-grams.\nThe previously described neural language model can be thought of as a composite\nof two function (g ◦f). The ﬁrst stage f projects a sequence of context words, or\npreceding n−1 words to a continuous vector space:\nf : {0,1}|V|×n−1 →Rd\nWe will call the resulting vector h a context vector. The second stage g maps this\ncontinuous vector h to the target word probability, by applying afﬁne transformation to\nthe vector h followed by softmax normalization.\n68\n1-of-K coding\nContinuous-space\nWord Representation\nSoftmax\nNonlinear projection\nthree\nfour\nteams\ngroups\n(a)\n(b)\nFigure 5.3: (a) Schematics of neural language model. (b) Example of how neural\nlanguage model generalizes to an unseen n-gram.\nLet us look more closely at what g does in Eq. (5.14). If we ignore the effect of\nthe bias ck for now, we can clearly see that the probability of the k-th word in the\nvocabulary is large when the output vector uk (or the k-th row of the output matrix U)\nis well aligned with the context vector h. In other words, the probability of the next\nword being the k-th word in the vocabulary is roughly proportional to the inner product\nbetween the context vector h and the corresponding target word vector uk.\nNow let us consider two context vectors hj and hk. These contexts are followed by\na similar set of words, meaning that the conditional distributions of the next word are\nsimilar to each other. Although these distributions are deﬁned over all possibility target\nwords, let us look at the probabilities of only one of the target words wl:\npl\nj =p(wl|hj) = 1\nZj\nexp\n\u0010\nw⊤\nl hj\n\u0011\n,\npl\nk =p(wl|hk) = 1\nZk\nexp\n\u0010\nw⊤\nl hk\n\u0011\n.\nThe ratio between pl\nj and pl\nk is then16\npl\nj\npl\nk\n= Zk\nZj\nexp\n\u0010\nw⊤\nl (hj −hk)\n\u0011\n.\nFrom this, we can clearly see that in order for the ratio\npl\nj\npl\nk to be 1, i.e., pl\nj = pl\nk,\nw⊤\nl (hj −hk) = 0.\n(5.15)\n16 Note that both pl\nj and pl\nk are positive due to our use of softmax.\n69\nNow let us assume that wl is not an all-zero vector, as otherwise it will be too dull\na case. In this case, the way to achieve the equality in Eq. (5.15) is to drive the context\nvectors hj and hk to each other. In other words, the context vectors must be similar\nto each other (in terms of Euclidean distance) in order to result in similar conditional\ndistributions of the next word.\nWhat does this mean? This means that the neural language model must project\n(n−1)-grams that are followed by the same word to nearby points in the context vec-\ntor space, while keeping the other n-grams away from that neighbourhood. This is\nnecessary in order to give a similar probability to the same word. If two (n−1)-grams,\nwhich are followed by the same word in the training corpus, are projected to far away\npoints in the context vector space, it naturally follows from this argument that the prob-\nability over the next word will differ substantially, resulting in a bad language model.\nLet us consider an extreme example, where we do bigram modeling with the train-\ning corpus comprising only three sentences:\n• There are three teams left for the qualiﬁcation.\n• four teams have passed the ﬁrst round.\n• four groups are playing in the ﬁeld.\nWe will focus on the bold-faced phrases; “three teams”, “four teams” and “four group”.\nThe ﬁrst word of each of these bigrams is a context word, and neural language model\nis asked to compute the probability of the word following the context word.\nIt is important to notice that neural language model must project “three” and “four”\nto nearby points in the context space (see Eq. (5.13).) This is because the context\nvectors from these two words need to give a similar probability to the word “teams”.\nThis naturally follows from our discussion earlier on how dot product preserves the\nordering in the space. And, from these two context vectors (which are close to each\nother), the model assigns similar probabilities to “teams” and “groups”, because they\noccur in the training corpus. In other words, the target word vector uteams and ugroups\nwill also be similar to each other, because otherwise the probability of “teams” given\n“four” (p(teams|four)) and “groups” given “four” (p(groups|four)) will be very differ-\nent despite the fact that they occurred equally likely in the training corpus.\nNow, let’s assume the case where we use the neural language model trained on\nthis tiny training corpus to assign a probability to an unseen bigram “three groups”.\nThe neural language model will project the context word “three” to a point in the con-\ntext space close to the point of “four”. From this context vector, the neural language\nmodel will have to assign a high probability to the word “groups”, because the context\nvector hthree and the target word vector ugroups well align. Thereby, even without ever\nseeing the bigram “three groups”, the neural language model can assign a reasonable\nprobability. See Fig. 5.3 (b) for graphical illustration.\nWhat this example shows is that neural language model automatically learns the\nsimilarity among different context words (via context vectors h), and also among dif-\nferent target words (via target word vectors uk), by exploiting co-occurrences of words.\nIn this example, the neural language model learned that “four” and “three” are similar\nfrom the fact that both of them occur together with “teams”. Similarly, in the target\n70\nside, the neural language model was able to capture the similarity between “teams”\nand “groups” by noticing that they both follow a common word “four”.\nThis is a clear, real-world demonstration of the so-called distributional hypothe-\nsis. Distributional hypothesis states that “words which are similar in meaning appear\nin similar distributional contexts” [41]. By observing which words a given word co-\noccurs together, it is possible to peek into the word’s underlying meaning. Of course,\nthis is only a partial picture17 into the underlying meaning of each word, or as a mat-\nter of fact a phrase, but surely still a very interesting property that is being naturally\nexploited by neural language model.\nIn neural language model, the most direct way to observe the effect of this dis-\ntributional hypothesis/structure is to investigate the ﬁrst layer’s weight matrix E in\nEq. (5.12). This weight matrix can be considered as a set of dense vectors of the\nwords in the input vocabulary\n\b\ne1,e2,...,e|V|\n\t\n, and any visualization technique, such\nas principal component analysis (PCA) or t-SNE [104], can be used to project each\nhigh-dimensional word vector into a lower-dimensional space (often 2-D or 3-D).\n5.4.2\nContinuous Bag-of-Words Language Model:\nMaximum Pseudo–Likelihood Approach\nThis is about time someone asks a question why we are only considering the preceding\nwords when doing language modelling. Is it a good assumption that the conditional\ndistribution over a word is only dependent on preceding words?\nIn fact, we do not have to do so. We can certainly model a natural language sentence\nsuch that each word in a sentence is conditioned on 2n surrounding words (n words to\nthe left and n words to the right.) In this case, we get a Markov random ﬁeld (MRF)\nlanguage model [56].\nFigure 5.4: An example Markov random ﬁeld language model (MRF-LM) with the\norder n = 1.\nIn a Markov random ﬁeld (MRF) language model (MRF-LM), we say each word in\na given sentence is a random variable wi. We connect each word with its 2n surrounding\nwords with undirected edges, and these edges represent the conditional dependency\nstructure of the whole MRF-LM. An example of an MRF-LM with n = 1 is shown in\nFig. 5.4.\nA probability over a Markov random ﬁeld is deﬁned as a product of clique po-\ntentials. A potential is deﬁned for each clique as a positive function whose input is\nthe values of the random variables in the clique. In the case of MRF-LM, we will\nassign 1 as a potential to every clique except for cliques of two random variables (in\n17 We will discuss why this is only a partial picture later on.\n71\nother words, we only use pairwise potentials only.) The pairwise potential between the\nwords i and j is deﬁned as\nφ(wi,w j) = exp\n\u0010\n(E⊤wi)⊤E⊤wj\u0011\n= exp\n\u0010\ne⊤\nwiew j\n\u0011\n,\nwhere E is from Eq. (5.12), and wi is the one-hot vector of the i-th word. One must\nnote that this is one possible implementation of the pairwise potential, and there may be\nother possibilities, such as to replace the dot product between the word vectors (e⊤\nwiewj)\nwith a deeper network.\nWith this pairwise potential, the probability over the whole sentence is deﬁned as\np(w1,w2,...,wT) = 1\nZ\nT−n\n∏\nt=1\nt+n\n∏\nj=t\nφ(wt,w j) = 1\nZ exp\n \nT−n\n∑\nt=1\ne⊤\nwtew j\n!\n,\nwhere Z is the normalization constant. This normalization constant makes the product\nof the potentials to be a probability and often is at the core of computational intractabil-\nity in Markov random ﬁelds.\nFigure 5.5: Gray nodes indicate the Markov blank of the fourth word.\nAlthough compute the full sentence probability is intractable in this MRF-LM, it is\nquite straightforward to compute the conditional probability of each word wi given all\nthe other words. When computing the conditional probability, we must ﬁrst notice that\nthe conditional probability of wi only depends on the values of other words included\nin its Markov blanket. In the case of Markov random ﬁelds, the Markov blanket of\na random variable is deﬁned as a set of all immediate neighbours, and it implies that\nthe conditional probability of wi is dependent only on n preceding words and the n\nfollowing words. See Fig. 5.5 for an example.\nKeeping this in mind, we can easily see that\np(wi|wi−n,...,wi−1,wi+1,...,wi+n) = 1\nZ′ exp\n \ne⊤\nwi\n \nn\n∑\nk=1\newi−k +\nn\n∑\nk=1\newi+k\n!!\n,\nwhere Z′ is a normalization constant computed by\nZ′ = ∑\nv∈V\nexp\n \ne⊤\nv\n \nn\n∑\nk=1\newi−k +\nn\n∑\nk=1\newi+k\n!!\n.\nDo you see a stark similarity to neural language model we discussed earlier? This\nconditional probability is a shallow neural network with a single linear hidden layer\n72\n1-of-K coding\nSoftmax\n+\nContinuous Bag-of-Words\nFigure 5.6: Continuous Bag-of-Words model approximates the conditional distribution\nover the j-th word w j under the MRF-LM.\nwhose input are the context words (n preceding and n following words) and the output\nis the conditional distribution of the center word wi. We will talk about this shortly in\nmore depth. See Fig. 5.6 for graphical illustration.\nNow we know that it is often difﬁcult to compute the full sentence probability\np(w1,...,wT) due to the intractable normalization constant Z. We however know how\nto compute the conditional probabilities (for all words) quite tractably. The former\nfact implies that it is perhaps not the best idea to maximize log-likelihood to train this\nmodel.18 The latter however sheds a bit of light, because we can train a model to\nmaximize pseudo–likelihood [11] instead.19\nPseudo–likelihood of the MRF-LM is deﬁned as\nlogPL =\nT\n∑\ni=1\nlog p(wi|wi−n,...,wi−1,wi+1,...,wi+n).\n(5.16)\nMaximizing this pseudo–likelihood is equivalent to training a neural network in Fig. 5.6\nwhich approximates each conditional distribution p(wi|wi−n,...,wi−1,wi+1,...,wi+n)\nto give a higher probability to the ground-truth center word in the training corpus.\nUnfortunately, even after training the model by maximizing the pseudo–likelihood\nin Eq. (5.16), we do not have a good way to compute the full sentence probability under\nthis model. Under certain conditions maximizing pseudo–likelihood indeed converges\nto the maximum likelihood solution, but this does not mean that we can use the product\nof all the conditionals as a replacement of the full sentence probability. However, this\ndoes not mean that we cannot use this MRF-LM as a language model, since given\na ﬁxed model, the pseudo–probability (the product of all the conditionals) can score\ndifferent sentences.\n18 However this is not to say maximum likelihood in this case is impossible. There are different ways to\napproximate the full sentence probability under this model. See [56] for one such approach.\n19 See the note by Amir Globerson (later modiﬁed by David Sontag) available at http://cs.nyu.\nedu/˜dsontag/courses/inference14/slides/pseudolikelihood_notes.pdf.\n73\nThis is in contrast to the neural language model we discussed earlier in Sec. 5.4. In\nthe case of neural language model, we were able to compute the probability of a given\nsentence by computing the conditional probability of each word, reading from left until\nthe end of the sentence. This is perhaps one of the reasons why the MRF-LM is not\noften used in practice as a language model. Then, you must ask why I even bothered\nto explain this MRF-LM in the ﬁrst place.\nThis approach, which was proposed in [79] as a continuous bag-of-words (CBoW)\nmodel,20 was found to exhibit an interesting property. That is, the word embedding\nmatrix E learned as a part of this CBoW model very well reﬂects underlying structures\nof words, and this has become one of the darling models by natural language processing\nresearchers in recent years. We will discuss further in the next section.\nSkip-Gram and Implicit Matrix Factorization\nIn [79], another model, called skip-\ngram, is proposed. The skip-gram model is built by ﬂipping the continuous bag-of-\nwords model. Instead of trying to predict the middle word given 2n surrounding words,\nthe skip-gram model tries to predict randomly chosen one of the 2n surrounding words\ngiven the middle word. From this description alone, it is quite clear that this skip-gram\nmodel is not going to be great as a language model. However, it turned out that the\nword vectors obtained by training a skip-gram model were as good as those obtained by\neither a continuous bag-of-words model or any other neural language model. Of course,\nit is debatable which criterion be used to determine the goodness of word vectors, but in\nmany of the existing so-called “intrinsic” evaluations, those obtained from a skip-gram\nmodel have been shown to excel.\nThe authors of [72] recently showed that training a skip-gram model with negative\nsampling (see [79]) is equivalent to factorizing a positive point-wise mutual informa-\ntion matrix (PPMI) into two lower-dimensional matrices. The left lower-dimensional\nmatrix corresponds to the input word embedding matrix E in a skip-gram model. In\nother words, training a skip-gram model implicitly factorizes a PPMI matrix.\nTheir work drew a nice connection between the existing works on distributional\nword representations from natural language processing, or even computational linguis-\ntics and these more recent neural approaches. I will not go into any further detail in\nthis course, but I encourage readers to read [72].\n5.4.3\nSemi-Supervised Learning with Pretrained Word Embeddings\nOne thing I want to emphasize in these language models, including n-gram language\nmodel, neural language model and continuous bag-of-words model, is that they are\npurely unsupervised, meaning that all we need is a large corpus of unannotated text.\nThis is one thing that makes this statistical approach to language modelling much more\nappealing than any other approach based on linguistic structures (see Sec. 5.1.1 for a\nbrief discussion.)\n20 One difference between the model we derived in this section starting from the MRF-LM and the one\nproposed in [79] is that in our derivation, the neural network shares a single weight matrix E for both the\ninput and output layers.\n74\nWhen it comes to neural language model and continuous bag-of-words model, we\nnow know that these networks learn continuous vector representations of input words,\ntarget words and the context phrase (h from Eq. (5.13).) We also discussed how these\nvector representations encode similarities among different linguistic units, be it a word\nor a phrase.\nWhat this implies is that once we train this type of language model on a large, or\neffectively inﬁnite,21 corpus of unlabelled text, we get good vectors for those linguistic\nunits for free. Among these, word vectors, the rows of the input weight matrix E in\nEq. (5.12), have been extensively used in many natural language processing applica-\ntions in recent years since [103, 33, 79].\nLet us consider an extreme example of classifying each English word as either\n“positive” or “negative”. For instance, “happy” is positive, while “sad” is negative. A\ntraining set of 2 examples–1 positive and 1 negative words– is given. How would one\nbuild a classiﬁer?22\nThere are two issues here. First, it is unclear how we should represent the input, in\nthis case a word. A good reader who has read this note so far will be clearly ready to\nuse an one-hot vector and use a softmax layer in the output, and I commend you for\nthat. However, this still does not solve a more serious issue which is that we have only\ntwo training examples! All the word vectors, save for two vectors corresponding to the\nwords in the training set, will not be updated at all.\nOne way to overcome these two issues is to make somewhat strong, but reasonable\nassumption that similar input will have similar sentiments. This assumption is at the\nheart of semi-supervised learning [23]. It says that high-dimensional data points in\neffect lies on a lower-dimensional manifold, and the target values of the points on this\nmanifold change smoothly. Under this assumption, if we can well model this lower-\ndimensional data manifold using unlabelled training examples, we can train a good\nclassiﬁer23\nAnd, guess what? We have access to this lower-dimensional manifold, which is\nrepresented by the set of pretrained word vectors E. Believing that similar words have\nsimilar sentiment and that these pretrained word vectors indeed well reﬂect similarities\namong words, let me build a simple nearest neighbour (NN) classiﬁer which uses the\npretrained word vectors:\nNN(w) =\n\u001a\npositive,\nif cos(ew,ehappy) > cos(ew,ebad)\nnegative,\notherwise\n,\nwhere cos(·,·) is a cosine similarity deﬁned as\ncos(ei,ej) =\ne⊤\ni ej\n∥ei∥∥ej∥.\n21 Why? Because of almost universal broadband access to the Internet!\n22 Although the setting of 2 training examples is extreme, but the task itself turned out to be not-so-\nextreme. In fact, there is multiple dictionaries of words’ sentiment maintained. For instance, check http:\n//sentiwordnet.isti.cnr.it/search.php?q=llama.\n23 What do I mean by a good classiﬁer? A good classiﬁer is a classiﬁer that classiﬁes unseen test examples\nwell. See Sec. 2.3.\n75\nThis use of a term “similarity” almost makes this set of pretrained word vectors\nlook like some kind of magical wand that can solve everything.24 This is however not\ntrue, and using pretrained word vectors must be done with caution.\nWhy should we be careful in using these pretrained word vectors? We must remem-\nber that these word vectors were obtained by training a neural network to maximize a\ncertain objective, or to minimize a certain cost function. This means that these word\nvectors capture certain aspects of words’ underlying structures that are necessary to\nachieve the training objective, and that there is no reason for these word vectors to\ncapture any other properties of the words that are not necessary for maximizing the\ntraining objective. In other words, “similarity” among multiple words has many dif-\nferent aspects, and these word vectors will capture only a few of these many aspects.\nWhich few aspects will be determined by the choice of training objective.\nThe hope is that language modelling is a good training objective that will encourage\nthe word vectors to capture as many aspects of similarity as possible.25 But, is this true\nin general?\nLet’s consider an example of words describing emotions, such as “happy”, “sad”\nand “angry”, in the context of a continuous bag-of-words model. These emotion-\ndescribing words often follow some forms of a verb “feel”, such as “feel”, “feels”,\n“felt” and “feeling”. This means that those emotion-describing words will have to be\nprojected nearby in the context space in order to give a high probability to those forms\nof “feel” as a middle word. This is understandable and agrees quite well with our in-\ntuition. All those emotion-describing words are similar to each other in the sense that\nthey all describe emotion. But, wait, this aspect of similarity is not going to help sen-\ntiment classiﬁcation of words. In fact, this aspect of similarity will hurt the sentiment\nclassiﬁer, because a positive word “happy” will be close to negative words “sad” and\n“angry” in this word vector space!\nThe lesson here is that when you are solving a language-related task with very little\ndata, it is a good idea to consider using a set of pretrained word vectors from neural\nlanguage models. However, you must do so in caution, and perhaps try to pretrain your\nown word vectors by training a neural network to maximize a certain objective that\nbetter suits your ﬁnal task.\nBut, then, what other training objectives are there? We will get to that later.\n5.5\nRecurrent Language Model\nNeural language model indeed avoids the lack of generalization in the conventional n-\ngram language modeling. It still assumes the n-th order Markov property, meaning that\nit looks only as far back into the past as n−1 words. In Sec. 5.3, I gave an example of\n“In Korea, more than half of all the residents speak Korean”. In this example, the con-\nditional distribution over the last word in the sentence clearly will be better estimated\n24 For future reference, I must say there were many papers claiming that the pretrained word vectors are\nindeed magic wands at three top-tier natural language processing conferences (ACL, EMNLP, NAACL) in\n2014 and 2015.\n25 Some may ask how a single vector, which is a point in a space, can capture multiple aspects of similarity.\nThis is possible because these word vectors are high-dimensional.\n76\n(a)\n(b)\nFigure 5.7:\n(a) A recurrent neural network from Sec. 4.1.4. (b) A recurrent neural\nnetwork language model.\nif it is conditioned on the second word of the sentence which is more than 10 words\nback in the past.\nLet us recall what we learned in Sec. 4.1.4. There, we learn how to build a recurrent\nneural network to read a variable-length sequence and return a variable-length output\nsequence. An example we considered back then was a task of part-of-speech tagging,\nwhere the input is a sentence such as\nx = (Children,eat,sweet,candy),\nand the target output is a sequence of part-of-speech tags such as\ny = (noun,verb,adjective,noun).\nIn order to make less of an assumption on the conditional independence of the\npredicted tags, we made a small adjustment such that the prediction Yt at each timestep\nwas fed back into the recurrent neural network in the next timestep together with the\ninput Xt+1. See Fig. 5.7 (a) for graphical illustration.\nWhy am I talking about this again, after saying that the task of part-of-speech tag-\nging is not even going to be considered as a valid topic for the ﬁnal project? Because\nthe very same model for part-of-speech tagging will be turned into the very recurrent\nneural network language model in this section.\nLet us start by considering a single conditional distribution, marked (a) below, from\nthe full sentence probability:\np(w1,w2,...,wT) =\nT\n∏\nt=1\np(wt|w1,...,wt−1)\n|\n{z\n}\n(a)\n.\nThis conditional probability can be approximated by a neural network, as we’ve been\ndoing over and over again throughout this course, that takes as input (w1,...,wt−1) and\nreturns the probability over all possible words in the vocabulary V. This is not unlike\nneural language model we discussed earlier in Sec. 5.4, except that the input is now a\nvariable-length sequence.\n77\nFigure 5.8: A recurrent neural network language model\nIn this case, we can use a recurrent neural network which is capable of summariz-\ning/memorizing a variable-length input sequence. A recurrent neural network summa-\nrizes a given input sequence (w1,...,wt−1) into a memory state ht−1:\nht′ =\n\u001a 0,\nif t′ = 0\nf(ewt′,ht′−1),\notherwise ,\n(5.17)\nwhere t′ runs from 0 to t −1. f is a recurrent function which can be any of a naive\ntransition function from Eq. (4.1), a gated recurrent unit or a long short-term memory\nunit from Sec. 4.2.2. ewt′ is a word vector corresponding to the word wt′.\nThis summary ht−1 is afﬁne-transformed followed by a softmax nonlinear function\nto compute the conditional probability of wt. Hopefully, everyone remembers how it is\ndone. As in Eq. (4.6),\nµ = softmax(Vht−1),\nwhere µ is a vector of probabilities of all the words in the vocabulary.\nOne thing to notice here is that the iteration procedure in Eq. (5.17) computes a\nsequence of every memory state vector ht by simply reading the input sentence once.\nIn other words, we can let the recurrent neural network read one word wt at a time,\nupdate the memory state ht and compute the conditional probability of the next word\np(wt+1|w≤t).\nThis procedure is illustrated in Fig. 5.7 (b).26 This language model is called a\nrecurrent neural network language model (RNN-LM, [80]).\nBut, wait, from looking at Figs. 5.7 (a)–(b), there is a clear difference between\nthe recurrent neural networks for part-of-speech tagging and language model. That is,\nthere is no feedback connection from the output of the previous time step back into the\nrecurrent neural network in the RNN-LM. This is simply an illusion from the limitation\nin the graphical illustration, because the input wt+1 in the next time step is in fact the\noutput wt+1 at the current time step. This becomes clearer by drawing the same ﬁgure\nin a slightly different way, as in Fig. 5.8.\n26 In the ﬁgure, you should notice the beginning-of-the-sentence symbol ⟨s⟩. This is necessary in order to\nuse the very same recurrent function f to compute the conditional probability of the ﬁrst word in the input\nsentence.\n78\n5.6\nHow do n-gram language model, neural language\nmodel and RNN-LM compare?\nNow the question is which one of these language models we should use in practice. In\norder to answer this, we must ﬁrst discuss the metric most commonly used for evaluat-\ning language models.\nThe most commonly used metric is a perplexity. In the context of language mod-\nelling, the perplexity PPL of a model M is computed by\nPPL = b−1\nN ∑N\nn=1 logb pM (wn|w<n),\n(5.18)\nwhere N is the number of all the words in the validation/test corpus, and b is some\nconstant that is often 2 or 10 in practice.\nWhat is this perplexed metric? I totally agree with you on this one. Of course, there\nis a quite well principled way to explain what this perplexity is based on information\ntheory. This is however not necessary for us to understand this metric called perplexity.\nAs the exponential function (with base b in the case of perplexity in Eq. (5.18))\nis a monotonically increasing function, we see that the ordering of different language\nmodels based on the perplexity will not change even if we only consider the exponent:\n−1\nN\nN\n∑\nn=1\nlogb pM (wn|w<n).\nFurthermore, assuming that b > 1, we can simply replace logb with log (natural loga-\nrithm) without changing the order of different language models:\n−1\nN\nN\n∑\nn=1\nlog pM (wn|w<n).\nNow, this looks awfully similar to the cost function, or negative log-likelihood, we\nminimize in order to train a neural network (see Chapter 2.)\nLet’s take a look at a single term inside the summation above:\nlog pM (wn|w<n).\nThis is simply measuring how high a probability the language model M is assigning to\na correct next word given all the previous words. Again, because log is a monotonically\nincreasing function.\nIn summary, the (inverse) perplexity measures how high a probability the language\nmodel M assigns to correct next words in the test/validation corpus on average. There-\nfore, a better language model is the one with a lower perplexity. There is nothing so\nperplexing about the perplexity, once we start viewing it from this perspective.\nWe are now ready to compare different language models, or to be more precise,\nthree different classes of language models–count-based n-gram language model, neural\nn-gram language model and recurrent neural network language model. The biggest\nchallenge in doing so is that this comparison will depend on many factors that are not\neasy to control. To list a few of them,\n• Language\n79\n• Genre/Topic of training, validation and test corpora\n• Size of a training corpus\n• Size of a language model\nFigure 5.9: The perplexity, word error rate (WER) and character error rate (CER) of\nan automatic speech recognition system using different language models. Note that all\nthe results by neural or recurrent language models are by interpolating these models\nwith the count-based n-gram language model. Reprinted from [100].\nBecause of this difﬁculty, this kind of comparison has often been done in the con-\ntext of a speciﬁc downstream application. This choice of a downstream application\noften puts rough constraints on the size of available, or commonly used, corpus, target\nlanguage and reasonably accepted size of language models. For instance, the authors\nof [3] compared the conventional n-gram language model and neural language model,\nwith various approximation techniques, with machine translation as a ﬁnal task. In\n[100], the authors compared all the three classes of language model in the context of\nautomatic speech recognition.\nFirst, let us look at one observation made in [100]. From Fig. 5.9, we can see that\nit is beneﬁcial to use a recurrent neural network language model (RNN-LM) compared\nto a usual neural language model. Especially when long short-term memory units were\n80\nFigure 5.10: The trend of perplexity as the size of language model changes. Reprinted\nfrom [100].\nused, the improvement over the neural language model was signiﬁcant. Furthermore,\nwe see that it is possible to improve these language models by simply increasing their\nsize.\nSimilarly, in Fig. 5.10 from the same paper [100], it is observed that larger language\nmodels tend to get better/lower perplexity and that RNN-LM in general outperforms\nneural language models.\nThese two observations do seem to suggest that neural and recurrent language mod-\nels are better candidates as language model. However, this is not to be taken as an\nevidence for choosing neural or recurrent language models. It has been numerously\nobserved over years that the best performance, both in terms of perplexity and in terms\nof performance in the downstream applications such as machine translation and auto-\nmatic speech recognition, is achieved by combining a count-based n-gram language\nmodel and a neural, or recurrent, language model. See, for instance, [92].\nThis superiority of combined, or hybrid, language model suggests that the count-\nbased, or conventional, n-gram language model, neural language model and recurrent\nneural network language model are capturing underlying structures of natural language\nsentences that are complement to each other. However, it is not crystal clear how these\ncaptured structures differ from each other.\n81\nChapter 6\nNeural Machine Translation\nFinally, we have come to the point in this course where we discuss an actual natural\nlanguage task. In this chapter, we will discuss how translation from one language to\nanother can be done with statistical methods, more speciﬁcally neural networks.\n6.1\nStatistical Approach to Machine Translation\nLet’s ﬁrst think of what it means to translate one sentence X in a source language to an\nequivalent sentence Y in a target language which is different from the source language.\nA process of translation is a function that takes as input the source sentence X and\nreturns a correct translation Y, and it is clear that there may be more than one correct\ntranslations. The latter fact implies that this function of translation should return not a\nsingle correct translation, but a probability distribution that assigns high probabilities\nto more than one likely translations.\nNow, let us write it in a more formal way. First, the input is a sequence of words\nX = (x1,x2,...,xTx),\nwhere Tx is the length of the source sentence. A target sentence is\nY = (y1,y2,...,yTy).\nSimilarly, Ty is the length of the target sentence.\nThe translation function f then reads the input sequence X and computes the prob-\nability over target sentences. In other words,\nf : V +\nx →C+\n|Vy|−1\n(6.1)\nwhere Vx is a source vocabulary, and V +\nx is a set of all possible source sentences of any\nlength Tx > 0. Vy is a target vocabulary, and Ck is a standard k-simplex.\nWhat is a standard k-simplex? It is a set deﬁned by\nCk =\n(\n(t0,...,tk) ∈Rk+1\n\f\f\f\f\f\nk\n∑\ni=1\ntk = 1 and ti ≥0 for all i\n)\n.\n82\nCorpora\nf = (La, croissance, économique, s'est, ralentie, ces, dernières, années, .)\ne = (Economic, growth, has, slowed, down, in, recent, years, .)\nFigure 6.1: Graphical illustration of statistical machine translation\nIn short, this set contains all possible settings for categorical distributions of k + 1\npossible outcomes. This means that the translation function f returns a probability\ndistribution P(Y|X) over all possible translations of length Ty > 1.\nGiven a source sentence X, this translation function f returns the conditional prob-\nability of a translation Y: P(Y|X). Let us rewrite this conditional probability according\nto what we have discussed in Chapter 5:\nP(Y|X) =\nTy\n∏\nt=1\nP(yt|y1,...,yt−1,\nX\n|{z}\nconditional\n)\n|\n{z\n}\nlanguage modelling\n(6.2)\nLooking at it in this way, it is clear that this is nothing but conditional language mod-\nelling. This means that we can use any of the techniques we have used earlier in\nChapter 5 for statistical machine translation.\nTraining can be trivially done by maximizing the log-likelihood or equivalently\nminimizing the negative log-likelihood (see Sec. 3.1):\n˜C(θ) = −1\nN\nN\n∑\nn=1\nTy\n∑\nt=1\nlog p(yn\nt |yn\n<t,Xn),\n(6.3)\ngiven a training set\nD =\n\b\n(X1,Y 1),(X2,Y 2),...,(XN,Y N)\n\t\n(6.4)\nconsisting of N training pairs.\nAll these look extremely straightforward and do not deviate too much from what we\nhave learned so far in this course. A big picture on this process translation is shown in\nFig. 6.1. More speciﬁcally, building a statistical machine translation model is simple,\nbecause we have learned how to\n1. Assign a probability to a sentence in Sec. 5.2.\n2. Handle variable-length sequences with recurrent neural networks in Sec. 4.1.\n3. Compute the gradient of an empirical cost function ˜C with respect to the param-\neters θ of a recurrent neural network in Sec. 4.1.2 and Sec. 3.4.\n83\n4. Use stochastic gradient descent to minimize the cost function in Sec. 2.2.2.\nOf course, simply knowing all these does not get you a working neural network that\ntranslates from one language to another. We will discuss in detail how we can build\nsuch a neural network in the next section. Before going to the next section, we must\nﬁrst discuss two issues; (1) where do we get training data? (2) how do we evaluate\nmachine translation systems?\n6.1.1\nParallel Corpora: Training Data for Machine Translation\nFirst, let us consider again what the problem we’re trying to solve here. It is machine\ntranslation, and from the description in the previous section and from Eqs. (6.1)–(6.2),\nit is a sentence-to-sentence translation task. We approach this problem by building a\nmodel that takes as input a source sentence S and computes the probability P(Y|X) of\na target sentence Y, equivalently a translation. In order for this model to translate, we\nmust train it with a training set of pairs of a source sentence and its correct translation.\nThe very ﬁrst problem we run into is where we can ﬁnd this training set which is\noften called a parallel corpus. It is not easy to think of documents which have been\ntranslated into multiple languages. Let’s take for instance all the books that are being\ntranslated each year. According to [86], approximately 3% of titles published each year\nin English are translations from another language.1 A few international news agencies\npublish some of their news articles in multiple languages. For instance, AFP publishes\n1,500 stories in French, 700 stories in English, 400 stories in Spanish, 250 stories in\nArabic, 200 stories in German and 150 stories in Portuguese each day, and there are\nsome overlapping stories across these six languages.2 Online commerce sites, such as\neBay, often list their products in international sites with their descriptions in multiple\nlanguages.3\nUnfortunately these sources of multiple languages of the same content are not suit-\nable for our purpose. Why is this so? Most importantly, they are often copy-righted\nand sold for personal use only. We cannot buy more than 14,400 books in order to\ntrain a translation model. We will likely go broke before completing the purchase,\nand even if so, it is unclear whether it is acceptable under copyright to use these text\nto build a translation model. Because we are mixing multiple sources of which each\nis protected under copyright, is the translation model trained from a mix of all these\nmaterials considered a derivative work?4\nThis issue is nothing new, and has been there since the very ﬁrst statistical machine\ntranslation system was proposed in [19]. Fortunately, it turned out that there are a\nnumber of legitimate sources where we can get documents translated in more than\none languages, often very faithfully to their content. These sources are parliamentary\nproceedings of bilingual, or multilingual countries.\n1 “According to the information Bowker released in October of 2005, in 2004 there were 375,000 new\nbooks published in English.” .. “Of that total, approx. 14,440 were new translations, which is slightly more\nthan 3% of all books published.” [86].\n2 http://www.afp.com/en/products/services/text\n3 http://sellercentre.ebay.co.uk/international-selling-tools\n4 http://copyright.gov/circs/circ14.pdf\n84\nBrown et al. [19] used the proceedings from the Canadian parliament, which are by\nlaw kept in both French and English. All of these proceedings are digitally available\nand called Hansards. You can check it yourself online at http://www.parl.gc.\nca/, and here’s an excerpt from the Prayers of the 2nd Session, 41st Parliament, Issue\n152:5\n• French: “ELIZABETH DEUX, par la Grˆace de Dieu, REINE du Royaume-\nUni, du Canada et de ses autres royaumes et territoires, Chef du Commonwealth,\nD´efenseur de la Foi.”\n• English: “ELIZABETH THE SECOND, by the Grace of God of the United\nKingdom, Canada and Her other Realms and Territories QUEEN, Head of the\nCommonwealth, Defender of the Faith.”\nEvery single word spoken in the Canadian parliament is translated either into French\nor into English. A more recent version of Hansards preprocessed for research can be\nfound at http://www.isi.edu/natural-language/download/hansard/.\nSimilarly, the European parliament used to provided the parliamentary proceedings\nin all 23 ofﬁcial languages.6 This is a unique data in the sense that each and every\nsentence is translated into either 11 or 26 ofﬁcial languages. For instance, here is one\nexample [65]:\n• Danish: det er næsten en personlig rekord for mig dette efter˚ar.\n• German: das ist f¨ur mich fast pers¨onlicher rekord in diesem herbst .\n• Greek: (omitted)\n• English: that is almost a personal record for me this autumn !\n• Spanish: es la mejor marca que he alcanzado este oto˜no .\n• Finnish: se on melkein minun enn¨atykseni t¨an¨a syksyn¨a !\n• French: c ’ est pratiquement un record personnel pour moi , cet automne !\n• Italian: e ’ quasi il mio record personale dell ’ autunno .\n• Dutch: dit is haast een persoonlijk record deze herfst .\n• Portuguese: ´e quase o meu recorde pessoal deste semestre !\n• Swedish: det ¨ar n¨astan personligt rekord f¨or mig denna h¨ost !\nThe European proceedings has been an invaluable resource for machine translation\nresearch. At least, the existing multilingual proceedings (up to 2011) can be still used,\nand it is known in the ﬁeld as the “Europarl” corpus [65] and can be downloaded from\nhttp://www.statmt.org/europarl/.\nThese proceedings-based parallel corpora have two distinct advantages. First, in\nmany cases, the sentences in those corpora are well-formed, and their translations are\n5 This is one political lesson here: Canada is still headed by the Queen of the United Kingdom.\n6 Unfortunately, the European parliament decided to stop translating its proceedings into all 23 ofﬁ-\ncial languages on 21 Nov 2011 as an effort toward budget cut. See http://www.euractiv.com/\nculture/parliament-cuts-translation-budg-news-516201.\n85\ndone by professionals, meaning the quality of the corpora is guaranteed. Second, sur-\nprisingly, the topics discussed in those proceedings are quite diverse. Clearly the mem-\nbers of the parliament do not often chitchat too often, but they do discuss a diverse set\nof topics. Here’s one such example from the Europarl corpus:\n• English: Although there are now two Finnish channels and one Portuguese one,\nthere is still no Dutch channel, which is what I had requested because Dutch\npeople here like to be able to follow the news too when we are sent to this place\nof exile every month.\n• French: Il y a bien deux chaˆınes ﬁnnoises et une chaˆıne portugaise, mais il\nn’y a toujours aucune chaˆıne n´eerlandaise. Pourtant je vous avais demand´e une\nchaˆıne n´eerlandaise, car les N´eerlandais aussi d´esirent pouvoir suivre les actu-\nalit´es chaque mois lorsqu’ils sont envoy´es en cette terre d’exil.\nOne apparent limitation is that these proceedings cover only a handful of languages\nin the world, mostly west European languages. This is not desirable. Why? According\nto Ethnologue (2014)7, the top-ﬁve most spoken languages in the world are\n1. Chinese: approx. 1.2 billion\n2. Spanish: approx. 414 million\n3. English: approx. 335 million\n4. Hindi: approx. 260 million\n5. Arabic: approx. 237 million\nThere are only two European languages in this list.\nSo, then, where can we get all data for all these non-European languages? There\nare a number of resources you can use, and let me list a few of them here:\nYou can ﬁnd the translated subtitle of the TED talks at the Web Inventory of\nTranscribed and Translated Talks (WIT3, https://wit3.fbk.eu/) [22]. It is\na quite small corpus, but includes 104 languages. For Russian–English data, Yandex\nreleased a parallel corpus of one million sentence pairs. You can get it at https:\n//translate.yandex.ru/corpus?lang=en. You can continue with other\nlanguages by googling very hard, but eventually you run into a hard wall.\nThis hard wall is not only the lack of any resource, but also lack of enough resource.\nFor instance, I quickly googled for Korean–English parallel corpora and found the\nfollowing resources:\n• SWRC English-Korean multilingual corpus: 60,000 sentence pairs http://\nsemanticweb.kaist.ac.kr/home/index.php/Corpus10\n• Jungyeul’s English-Korean parallel corpus: 94,123 sentence pairs https://\ngithub.com/jungyeul/korean-parallel-corpora\nThis is just not large enough.\nOne way to avoid this or mitigate this problem is to automatically mine parallel\ncorpora from the Internet. There have been quite some work in this direction as a way\n7 http://www.ethnologue.com/world\n86\nto increase the size of parallel corpora [87, 112]. The idea is to build an algorithm that\ncrawls the Internet and ﬁnd a pair of corresponding pages in two different languages.\nOne of the largest preprocessed corpus of multiple languages from the Internet is the\nCommon Crawl Parallel Corpus created by Smith et al. [98] available at http://\nwww.statmt.org/wmt13/training-parallel-commoncrawl.tgz.\n6.1.2\nAutomatic Evaluation Metric\nLet’s say we have trained a machine translation model on a training corpus. A big\nquestion follows: how do we evaluate this model?\nIn the case of classiﬁcation, evaluation is quite straightforward. All we need to do is\nto classify held-out test examples with a trained classiﬁer and see how many examples\nwere correctly classiﬁed. This is however not true in the case of translation.\nThere are a number of issues, but let us discuss two most important problems here.\nFirst, there may be many correct translations given a single source sentence. For in-\nstance, the following three sentences are the translations made by a human translator\ngiven a single Chinese sentence [82]:\n• It is a guide to action that ensures that the military will forever heed Party com-\nmands.\n• It is the guiding principle which guarantees the military forces always being\nunder the command of the Party.\n• It is the practical guide for the army always to heed the directions of the party.\nThey all clearly differ from each other, although they are the translations of a single\nsource sentence.\nSecond, the quality of translation cannot be measured as either success or failure.\nIt is rather a smooth measure between success and failure. Let us consider an English\ntranslation of a French sentence “J’aime un llama, qui est un animal mignon qui vit en\nAm´erique du Sud”.8\nOne possible English translation of this French sentence is “I like a llama which is a\ncute animal living in South America”. Let’s give this translation a score 100 (success).\nAccording to Google translate, the French sentence above is “I like a llama, a cute\nanimal that lives in South America”. I see that Google translate has omitted “qui est”\nfrom the original sentence, but the whole meaning has well been captured. Let us give\nthis translation a slightly lower score of 90.\nThen, how about “I like a llama from South America”? This is certainly not a\ncorrect translation, but except for the part about a llama being cute, this sentence does\ncommunicate most of what the original French sentence tried to communicate. Maybe,\nwe can give this translation a score of 50.\nHow about “I do not like a llama which is an animal from South America”? This\ntranslation correctly describes the characteristics of llama exactly as described in the\nsource sentence. However this translation incorrectly states that I do not like a llama,\nwhen I like a llama according to the original French sentence. What kind of score\nwould you give this translation?\n8 I would like to thank Laurent Dinh for the French translation.\n87\nEven worse, we want an automated evaluation algorithm. We cannot look at thou-\nsands of validation or test sentence pairs to tell how well a machine translation model\ndoes. Even if we somehow did it for a single model, in order to compare this translation\nmodel against others, we must do it for every single machine translation model under\ncomparison. We must have an automatic evaluation metric in order to efﬁciently test\nand compare different machine translation models.\nBLEU\nOne of the most widely used automatic evaluation metric for assessing the\nquality of translations is BLEU proposed in [82]. BLEU computes the geometric mean\nof the modiﬁed n-gram precision scores multiplied by brevity penalty. Let me describe\nthis in detail here.\nFirst, we deﬁne the modiﬁed n-gram precision pn of a translation Y as\npn = ∑S∈C ∑ngram∈S ˆc(ngram)\n∑S∈C ∑ngram∈S c(ngram),\nwhere C is a corpus of all the sentences/translations, and S is a set of all unique n-grams\nin one sentence in C. c(ngram) is the count of the n-gram, and ˆc(ngram) is\nˆc(ngram) = min(c(ngram),cref(ngram)).\ncref(ngram) is the count of the n-gram in reference sentences.\nWhat does this modiﬁed n-gram precision measure? It measures the ratio between\nthe number of n-grams in the translation and the number of those n-grams actually\noccurred in a reference (ground-truth) translation. If there is no n-gram from the trans-\nlation in the reference, this modiﬁed precision will be zero because cref(·) will be zero\nall the time.\nIt is common to use the geometric average of modiﬁed 1-, 2-, 3- and 4-gram preci-\nsions, which is computed by\nP4\n1 = exp\n \n1\n4\n4\n∑\nn=1\nlog pn\n!\n.\nIf we use this geometric average P as it is, there is a big loophole. One can get\na high average modiﬁed precision by making as short a translation as possible. For\ninstance, a reference translation is\n• I like a llama, a cute animal that lives in South America .\nand a translation we are trying to evaluate is\n• cute animal that lives\nThis is clearly a very bad translation, but the modiﬁed 1-, 2-, 3- and 4-gram precisions\n88\nwill be high. The modiﬁed precisions are\np1 =1+1+1+1\n1+1+1+1 = 4\n4 = 1\np2 =1+1+1\n1+1+1 = 3\n3 = 1\np3 =1+1\n1+1 = 2\n2 = 1\np4 =1\n1 = 1\n1 = 1.\nTheir geometric average is then\nP4\n1 = exp\n\u00121\n4 (0+0+0+0)\n\u0013\n= 1\nwhich is the maximum modiﬁed precision you can get!\nIn order to avoid this behaviour, BLEU penalizes the geometric average of the\nmodiﬁed n-gram precisions by the ratio of the lengths between the reference r and\ntranslation l. This is done by ﬁrst computing a brevity penalty:\nBP =\n\u001a 1\n, if l ≥r\nexp\n\u00001−r\nl\n\u0001\n, if l < r\nIf the translation is longer than the reference, it uses the geometric average of the\nmodiﬁed n-gram precisions as it is. Otherwise, it will penalize it by multiplying the\naverage precision with a scalar less than 1. In the case of the example above, the brevity\npenalty is 0.064, and the ﬁnal BLEU score is 0.064.\n(a)\n(b)\nFigure 6.2: (a) BLEU vs. bilingual and monolingual judgements of three machine\ntranslation systems (S1, S2 and S3) and two humans (H1 and H2). Reprinted from\n[82]. (b) BLEU vs. human judgement (adequacy and ﬂuency separately) of three\nmachine translation systems (two statistical and one rule-based systems). Reprinted\nfrom [20].\nThe BLEU was shown to correlate well with human judgements in the original\narticle [82]. Fig. 6.2 (a) shows how BLEU correlates with the human judgements in\ncomparing different translation systems.\n89\nThis is however not to be taken as a message saying that the BLEU is the perfect\nautomatic evaluation metric. It has been shown that the BLEU is only adequate in\ncomparing two similar machine translation systems, but not too much so in comparing\ntwo very different systems. For instance, Callison-Burch et al. [20] observed that the\nBLEU underestimates the quality of the machine translation system that is not a phrase-\nbased statistical system. See Fig. 6.2 (b) for an example.\nBLEU is deﬁnitely not a perfect metric, and many researchers strive to build a better\nevaluation metric for machine translation systems. Some of the alternatives available\nat the moment are METEOR [36] and TER [99].\n6.2\nNeural Machine Translation:\nSimple Encoder-Decoder Model\nFrom the previous section and from Eq. 6.2, it is clear that we need to model each\nconditional distribution inside the product as a function. This function will take as\ninput all the previous words in the target sentence Y = (y1,...,yt−1) and the whole\nsource sentence X = (x1,...,xTx). Given these inputs the function will compute the\nprobabilities of all the words in the target vocabulary Vy. In this section, I will describe\nan approach that was proposed multiple times independently over 17 years in [43, 28,\n101].\nLet us start by tackling how to handle the source sentence X = (x1,...,xTx). Since\nthis is a variable-length sequence, we can readily use a recurrent neural network from\nChapter 4. However, unlike the previous examples, there is no explicit target/output in\nthis case. All we need is a (vector) summary of the source sentence.\nWe call this recurrent neural network an encoder, as it encodes the source sentence\ninto a (continuous vector) code. It is implemented as\nht = φenc\n\u0010\nht−1,E⊤\nx xt\n\u0011\n.\n(6.5)\nAs usual, φenc can be any recurrent activation function, but it is highly recommended to\nuse either gated recurrent units (see Sec. 4.2.2) or long short-term memory units (see\nSec. 4.2.3.) Ex ∈R|Vx|×d is an input weight matrix containing word vectors as its rows\n(see Eq. (5.12) in Sec. 5.4,) and xt is an one-hot vector representation of the word xt\n(see Eq. (5.10) in Sec. 5.4.) h0 is initialized as an all-zero vector.\nAfter reading the whole sentence up to xTx, the last memory state hTx of the encoder\nsummarizes the whole source sentence into a single vector, as shown in Fig. ?? (a).\nThanks to this encoder, we can now work with a single vector instead of a whole\nsequence of source words. Let us denote this vector as c and call it a context vector.\nWe now need to design a decoder, again, using a recurrent neural network. As I\nmentioned earlier, the decoder is really nothing but a language model, except that it is\nconditioned on the source sentence X. What this means is that we can build a recurrent\nneural network language model from Sec. 5.5 but feeding also the context vector at\neach time step. In other words,\nzt = φdec\n\u0010\nzt−1,\nh\nE⊤\ny yt−1;c\ni\u0011\n(6.6)\n90\n(a)\n(b)\nFigure 6.3: (a) The encoder and (b) the decoder of a simple neural machine translation\nmodel\nDo you see the similarity and dissimilarity to Eq. (5.17) from Sec. 5.5? It’s essentially\nsame, except that the input at time t is a concatenated vector of the word vector of the\nprevious word yt−1 and the context vector c.\nOnce the decoder’s memory state is updated, we can compute the probabilities of\nall possible target words by\np(yt = w′|y<t,X) ∝exp\n\u0010\ne⊤\nw′zt\n\u0011\n,\n(6.7)\nwhere ew′ is the target word vector associated the word w′. This is equivalent to afﬁne-\ntransforming zt followed by a softmax function from Eq. (3.5) from Sec. 3.1.\nNow, should we again initialize z0 to be an all-zero vector? Maybe, or maybe not.\nOne way to view what this decoder does is that the decoder models a trajectory in\na continuous vector space, and each point in the trajectory is zt. Then, z0 acts as a\nstarting point of this trajectory, and it is natural to initialize this starting point to be a\npoint relevant to the source sentence. Because we have access to the source sentence’s\ncontent via c, we can again use it to initialize z0 as\nz0 = φinit (c).\n(6.8)\nSee Fig. 6.3 (b) for the graphical illustration of the decoder.\nAlthough I have used c as if it is a separate variable, this is not true. c is simply\na shorthand notation of the last memory state of the encoder which is a function of\nthe whole source sentence. What does this mean? It means that we can compute the\ngradient of the empirical cost function in Eq. (6.3) with respect to all the parameters of\nboth the encoder and decoder and maximize the cost function using stochastic gradient\ndescent, just like any other neural network we have learned so far in this course.\n6.2.1\nSampling vs. Decoding\nSampling\nWe are ready to compute the conditional distribution P(Y|X) over all pos-\nsible translations given a source sentence. When we have a distribution, the ﬁrst thing\nwe can try is to sample from this distribution. Often, it is not straightforward to gen-\nerate samples from a distribution, but fortunately, in this case, we can readily generate\nexact samples from the distribution P(Y|X).\n91\nWe simply iterate over the following steps until a token indicating the end of a\nsentence (⟨eos⟩):\n1. Compute c (Eq. (6.5))\n2. Initialize z0 with c (Eq. (6.8))\n3. Compute zt given zt−1, yt−1 and c (Eq. (6.6))\n4. Compute p(yt|y<t,X) (Eq. (6.7))\n5. Sample ˜yt from the compute distribution\n6. Repeat (3)–(5) until ˜yt = ⟨eos⟩\nAfter taking these steps, we get a sample ˜Y =\n\u0010\n˜y1,..., ˜y|˜Y|\n\u0011\ngiven a source sentence\nX. Of course, there is no guarantee that this will be a good translation of X. In order to\nﬁnd a good translation, meaning a translation with a high probability P(˜Y|X), we need\nto repeatedly sample multiple translations from P(Y|X) and choose one with the high\nprobability.\nThis is not too desirable, as it is not clear how many translations we need to sample\nfrom P(Y|X) and also it will likely be computationally expensive. We must wonder\nwhether we can solve the following optimization problem directly:\n˜Y = argmax\nY\nlogP(Y|X).\nUnfortunately, the exact solution to this requires evaluating P(Y|X) for every possible\nY. Even if we limit our search space of Y to consist of only sentences of length up\nto a ﬁnite number, it will likely become too large (the cardinality of the set grows\nexponentially with respect to the number of words in a translation.) Thus, it only\nmakes sense to solving the optimization problem above approximately.\nApproximate Decoding: Beamsearch\nAlthough it is quite clear that ﬁnding a trans-\nlation ˜Y that maximizes the log-probability logP(˜Y|X) is extremely expensive, we will\nregardlessly try it here.\nOne very natural way to enumerate all possible target sentences and simultaneously\ncomputing the log-probability of each and every one of them is to start from all possible\nﬁrst word, compute the probabilities of them, and from each potential ﬁrst word branch\ninto all possible second words, and so on. This procedure forms a tree, and any path\nfrom the root of this tree to any intermediate node is a valid, but perhaps very unlikely,\nsentence. See Fig. 6.4 for the illustration. The conditional probabilities of all these\npaths, or sentences, can be computed as we expand this tree down by simply following\nEq. (6.2).\nOf course, we cannot compute the conditional probabilities of all possible sen-\ntences. Hence, we must resort to some kind of approximate search. Wait, search? Yes,\nthis whole procedure of ﬁnding the most likely translation is equivalent to searching\nthrough a space, in this case a tree, of all possible sentences for one sentence that has\nthe highest conditional probability.\n92\n(a)\n(b)\nFigure 6.4: (a) Search space depicted as a tree. (b) Greedy search.\nThe most basic approach to approximately searching for the most likely translation\nis to choose only a single branch at each time step t. In other words,\nˆyt = argmax\nw′∈V\nlog p(yt = w′|ˆy<t,X),\nwhere the conditional probability is deﬁned in Eq. (6.7), and ˆy<t = (ˆy1, ˆy2,..., ˆyt−1) is\na sequence of greedily-selected target words up to the (t −1)-th step. This procedure\nis repeated until the selected ˆyt is a symbol corresponding to the end of the translation\n(often denoted as ⟨eos⟩.) See Fig. 6.4 (b) for illustration.\nThere is a big problem of this greedy search. That is, as soon as it makes one\nmistake at one time step, there is no way for this search procedure to recover from this\nmistake. This happens because the conditional distributions at later steps depend on\nthe choices made earlier.\nConsider the following two sequences: (w1,w2) and (w′\n1,w2). These sequences’\nprobabilities are\np(w1,w2) = p(w1)p(w2|w1),\np(w′\n1,w2) = p(w′\n1)p(w2|w′\n1)\n93\nLet’s assume that\nλ p(w1) = p(w′\n1),\nwhere 0 < λ < 1, meaning that p(w1) > p(w′\n1). In this case, the greedy search will\nchoose w1 over w′\n1 and ignore w′\n1.\nNow we can see that there’s a problem with this. Let’s assume that\nλ p(w2|w1) < p(w2|w′\n1) ⇐⇒p(w2|w1) < 1\nλ p(w2|w′\n1)\nwhere λ was deﬁned earlier. In this case,\np(w1,w2) =p(w1)p(w2|w1) = λ p(w′\n1)p(w2|w1)\n<\u0013λ p(w′\n1) 1\n\u0013λ p(w2|w′\n1) = p(w′\n1)p(w2|w′\n1) = p(w′\n1,w2).\nIn short,\np(w1,w2) < p(w′\n1,w2).\nIt means that the sequence (w′\n1,w2) is more likely than (w1,w2), but the greedy search\nalgorithm is unable to notice this, because simply p(w1) > p(w′\n1).\nUnfortunately, the only way to completely avoid this undesirable situation is to\nconsider all the possible paths starting from the very ﬁrst time step. This is exactly the\nreason why we introduced the greedy search in the ﬁrst place, but the greedy search\nis too greedy. The question is then whether there is something in between the exact\nsearch and the greedy search.\nBeam Search\nLet us start from the very ﬁrst position t = 1. First, we compute the\nconditional probabilities of all the words in the vocabulary:\np(y1 = w|X) for all w ∈V.\nAmong these, we choose the K most likely words and initialize the K hypotheses:\n(w1\n1),(w1\n2),...,(w1\nK)\nWe use the subscript to denote the hypothesis and the subscript the time step. As an\nexample, w1\n1 is the ﬁrst hypothesis at time step 1.\nFor each hypothesis, we compute the next conditional probabilities of all the words\nin the vocabulary:\np(y2 = w|y<1 = (w1\ni ),X) for all w ∈V,\nwhere i = 1,...,K. We then have K ×|V| candidates with the corresponding probabil-\nities:\nK\n\n\n\n\n\n\n\n\n\n\n\np(w1\n1,w2\nc,1),\n...,\np(w1\n1,w2\nc,|V|)\np(w1\n2,w2\nc,1),\n...,\np(w1\n2,w2\nc,|V|)\n...\np(w1\nK,w2\nc,1),\n...,\np(w1\nK,w2\nc,|V|)\n|\n{z\n}\n|V|\n94\nFigure 6.5: Beam search with the beam width set to 3.\nAmong these K ×|V| candidates, we choose the K most likely candidates:\n(w1\n1,w2\n1),(w1\n2,w2\n2),...,(w1\nK,w2\nK).\nStarting from these K new hypotheses, we repeat the process of computing the proba-\nbilities of all K ×|V| possible candidates and choosing among them the K most likely\nnew hypotheses.\nIt should be clear that this procedure, called beam search and shown in Fig. 6.5,\nbecomes equivalent to the exact search, as K →∞. Also, when K = 1, this procedure is\nequivalent to the greedy search. In other words, this beam search interpolates between\nthe exact search, which is computationally intractable but exact, and the greedy search,\nwhich is computationally very cheap but probably quite inexact, by changing the size\nK of hypotheses maintained throughout the search procedure.\nHow do we choose K? One might mistakenly think that we can simply use as large\nK as possible given the constraints on computation and memory. Unfortunately, this is\nnot necessarily true, as this interpolation by K is not monotonic. That is, the quality of\nthe translation found by the beam search with a larger K is not necessarily better than\nthe translation found with a smaller K.\nLet us consider the case of vocabulary having three symbols {a,b,c} and any valid\ntranslation being of a length 3. In the ﬁrst step, we have\np(a) = 0.5, p(b) = 0.15, p(c) = 0.45.\nIn the case of K = 1, i.e., greedy search, we choose a. If K = 2, we will keep (a) and\n(c).\nGiven a as the ﬁrst symbol, we have\np(a|a) = 0.4, p(b|a) = 0.3, p(c|a) = 0.3,\n95\nin which case, we keep (a,a) with K = 1. With K = 2, we should check also\np(a|c) = 0.45, p(b|c) = 0.45, p(c|c) = 0.1,\nfrom which we maintain the hypotheses (c,a) and (c,b) (0.45×0.45 and 0.45×0.45,\nrespectively.) Note that with K = 2, we have discarded (a,a).\nNow, the greedy search ends by computing the last conditional probabilities:\np(a|a,a) = 0.9, p(b|a,a) = 0.05, p(c|a,a) = 0.05.\nThe ﬁnal verdict from the greedy search is therefore (a,a,a) with its probability being\n0.5×0.4×0.9 = 0.18.\nWhat happens with the beam search having K = 2? We need to check the following\nconditional probabilities:\np(a|c,a) = 0.7, p(b|c,a) = 0.2, p(c|c,a) = 0.1\np(a|c,b) = 0.4, p(b|c,b) = 0.0, p(c|c,b) = 0.6\nFrom here we consider (c,a,a) and (c,b,c) with the corresponding probabilities 0.45×\n0.45 × 0.7 = 0.14175 and 0.45 × 0.45 × 0.6 = 0.1215. Among these two, (c,a,a) is\nﬁnally chosen, due to its higher probability than that of (c,b,c).\nIn summary, the greedy search found (a,a,a) whose probability is\np(a,a,a) = 0.18,\nand the beam search with K = 2 found (c,a,a) whose probability is\np(c,a,a) = 0.14175.\nEven with a larger K, the beam search found a worse translation!\nNow, clearly, what one can do is to set the maximum beam width ¯K and try with\nall possible 1 ≤K ≤¯K. Among the translations given by ¯K beam search procedures,\nthe best translation can be selected based on their corresponding probabilities. From\nthe point of view of computational complexity, this is perhaps the best approach to\nupper-bound the worst-case memory consumption. Doing the beam search once with\n¯K or multiple beam searches with K = 1,..., ¯K are equivalent in terms of memory con-\nsumption, i.e., both are O(K|V|). Furthermore, the worst-case computation is O(K|V|)\n(assuming a constant time computation for computing each conditional probability.) In\npractice however, the constant in front of K|V| does matter, and we often choose K\nbased on the translation quality of the validation set, after trying a number of values–\n{1,2,4,8,16}.\nIf you’re interested in how to improve beam search by backtracking so that the\nbeam search becomes complete, refer to, e.g., [44, 113]. If you’re interested in general\nsearch strategies, refer to [90]. Also, in the context of statistical machine translation, it\nis useful to read [64].\n96\n6.3\nAttention-based Neural Machine Translation\nOne important property of the simple encoder-decoder model for neural machine trans-\nlation (from Sec. 6.2) is that a whole source sentence is compressed into a single real-\nvalued vector c. This sounds okay, since the space of all possible source sentences is\ncountable, while the context vector space [−1,1]d is uncountable. There exists a map-\nping from this sentence space to the context vector space, and all we need to ensure is\nthat training the simple encoder-decoder model ﬁnds this mapping. This is conditioned\non the assumption that the hypothesis space9 deﬁned by the model architecture–the\nnumber of hidden units and parameters– includes this mapping from any source sen-\ntence to a context vector.\nUnfortunately, considering the complexity of any natural language sentence, it is\nquite easy to guess that this mapping must be highly nonlinear and will require a huge\nencoder, and consequently, a huge decoder to map back from a context vector to a target\nsentence. In fact, this fact was empirically validated last year (2014), when the almost\nidentical models from two groups [101, 27] showed vastly different performances on\nthe same English–French translation task. The only difference there was that the au-\nthors of [101] used a much larger model than the authors of [27] did.\nAt a more fundamental level there’s a question of whether a natural language sen-\ntence should be fully represented as a single vector. For instance, there is now a famous\nquote by Prof. Raymond Mooney10 of the University of Texas at Austin: “You can’t\ncram the meaning of a whole %&!$# sentence into a single $&!#* vector!”11 Though,\nour goal is not in answering this fundamental question from linguistics.\nOur goal is rather to investigate the possibility of avoiding this situation of having\nto learn a highly nonlinear, complex mapping from a source sentence to a single vector.\nThe question we are more interested in is whether there exists a neural network that\ncan handle a variable-length sentence by building a variable-length representation of\nit. Especially, we are interested in whether we can build a neural machine translation\nsystem that can exploit a variable-length context representation.\nVariable-length Context Representation\nIn the simple encoder-decoder model, a\nsource sentence, regardless of its length, was mapped to a single context vector by a\nrecurrent neural network:\nht = φenc\n\u0010\nht−1,E⊤\nx xt\n\u0011\n.\nSee Eq. (6.5) and the surrounding text for more details.\nInstead, here we will encode a source sentence X = (x1,x2,...,xTx) with a set C of\ncontext vectors ht’s. This is achieved by having two recurrent neural networks rather\nthan a single recurrent neural networks, as in the simple encoder-decoder model. The\nﬁrst recurrent neural network, to which we will refer as a forward recurrent neural\nnetwork, reads the source sentence as usual and results in a set of forward memory\n9 See Sec. 2.3.2.\n10 https://www.cs.utexas.edu/˜mooney/\n11 http://nlpers.blogspot.com/2014/09/amr-not-semantics-but-close-maybe.\nhtml\n97\nFigure 6.6: An encoder with a bidirectional recurrent neural network\nstates −→h t, for t = 1,...,Tx. The second recurrent neural network, a reverse recurrent\nneural network, reads the source sentence in a reverse order, starting from xTx to x1.\nThis reverse network will output a sequence of reverse memory states ←−h t, for t =\n1,...,Tx.\nFor each xt, we will concatenate −→h t and ←−h t to form a context-dependent vector ht:\nht =\n\" −→h t\n←−h t\n#\n(6.9)\nWe will form a context set with these context-dependent vectors c = {h1,h2,...,hTx}.\nSee Fig. 6.6 for the graphical illustration of this process.\nNow, why is ht a context-dependent vector? We should look at what the input was\nto a function that computed ht. The ﬁrst half of ht, −→h t, was computed by\n−→h t = φfenc\n\u0010\nφfenc\n\u0010\n··· ,E⊤\nx xt−1\n\u0011\n,E⊤\nx xt\n\u0011\n,\nwhere φfenc is a forward recurrent activation function. From this we see that −→h t was\ncomputed by all the source words up to t, i.e., x≤t. Similarly,\n←−h t = φrenc\n\u0010\nφrenc\n\u0010\n··· ,E⊤\nx xt+1\n\u0011\n,E⊤\nx xt\n\u0011\n,\nwhere φrenc is a reverse recurrent activation function, and ←−h t depends on all the source\nwords from t to the end, i.e., x≥t.\nIn summary, ht =\nh−→h ⊤\nt ;←−h ⊤\nt\ni⊤\nis a vector representation of the t-th word, xt, with\nrespect to all the other words in the source sentence. This is why ht is a context-\ndependent representation. But, then, what is the difference among all those context-\ndependent representations {h1,...,hTx}? We will discuss this shortly.\nDecoder with Attention Mechanism\nBefore anything let us think of what the mem-\nory state zt of the decoder (from Eq. (6.6)) does:\nzt = φdec\n\u0010\nφdec\n\u0010\nφdec\n\u0010\n··· ,\nh\nE⊤\ny yt−3;c\ni\u0011\n,\nh\nE⊤\ny yt−2;c\ni\u0011h\nE⊤\ny yt−1;c\ni\u0011\n98\nFigure 6.7: Illustration of how the relevance score e2,3 of the second context vector h2\nat time step 3 (dashed curves and box.)\nIt is computed based on all the generated target words so far (˜y1, ˜y2,..., ˜yt−1) and\nthe context vector12 c which is the summary of the source sentence. The very reason\nwhy I designed the decoder in this way is so that the memory state zt is informative of\nwhich target word should be generated at time t after generating the ﬁrst t −1 target\nwords given the source sentence. In order to do so, zt must encode what have been\ntranslated so far among the words that are supposed to be translated (which is encoded\nin the context vector c.) Let’s keep this in mind.\nIn order to compute the new memory state zt with a context setC = {h1,h2,...,hTx},\nwe must ﬁrst get one vector out of Tx context vectors. Why is this necessary? Because\nwe cannot have an inﬁnitely large number of parameters to cope with any number of\ncontext vectors. Then, how can we get a single vector from an unspeciﬁed number of\ncontext vectors ht’s?\nFirst, let us score each context vector hj ( j = 1,...,Tx) based on how relevant it is\nfor translating a next target word. This scoring needs to be based on (1) the previous\nmemory state zt−1 which summarizes what has been translated up to the (t −2)-th\nword13, (2) the previously generated target word ˜yt−1, and (3) the j-th context vector\nhj:\ne j,t = fscore(zt−1,E⊤\ny ˜yt−1,hj).\n(6.10)\nConceptually, the score e j,t will be computed by comparing (zt−1, ˜yt−1) with the con-\ntext vector cj. See Fig. 6.7 for graphical illustration.\n12 We will shortly switch to using a context set instead.\n13 Think of why this is only up to the (t −2)-th word not up to the (t −1)-th one.\n99\nFigure 6.8: Computing the new memory state zt of the decoder based on the previous\nmemory state zt−1, the previous target word ˜yt−1 and the weighted average of context\nvectors according to the attention weights.\nOnce the scores for all the context vectors hj’s ( j = 1,...,Tx) are computed by\nfscore, we normalize them with a softmax function:\nαj,t =\nexp(e j,t)\n∑Tx\nj′=1 exp(e j′,t)\n.\n(6.11)\nWe call these normalized scores the attention weights, as they correspond to how much\nthe decoder attends to each of the context vectors. This whole process of computing\nthe attention weights is often referred to as an attention mechanism (see, e.g., [26].)\nWe take the weighted average of the context vectors with these attention weights:\nct =\nTx\n∑\nj=1\nαj,thj\n(6.12)\nThis weighted average is used to compute the new memory state zt of the decoder,\nwhich is identical to the decoder’s update equation from the simple encoder-decoder\nmodel (see Eq. (6.6)) except that ct is used instead of c ((a) in the equation below):\nzt = φdec\n\n\nzt−1,\n\nE⊤\ny yt−1; ct\n|{z}\n(a)\n\n\n\n\n\n100\nSee Fig. 6.8 for the graphical illustration of how it works.\nGiven the new memory state zt of the decoder, the output probabilities of all the\ntarget words in a vocabulary happen without any change from the simple encoder-\ndecoder model in Sec. 6.2.\nWe will call this model, which has a bidirectional recurrent neural network as an en-\ncoder and a decoder with the attention mechanism, an attention-based encoder-decoder\nmodel. This approach was proposed last year (2014) in the context of machine transla-\ntion in [2] and has been studied extensively in [76].\n6.3.1\nWhat does the Attention Mechanism do?\nOne important thing to notice is that this attention-based encoder-decoder model can be\nreduced to the simple encoder-decoder model easily. This happens when the attention\nmechanism fscore in Eq. (6.10) returns a constant regardless of its input. When this\nhappens, the context vector ct at each time step t (see Eq. (6.12)) is same for all the\ntime steps t = 1,...,Ty:\nct = 1\nTx\nTx\n∑\nj=1\nhj.\nThe encoder effectively maps the whole input sentence into a single vector, which was\nat the core of the simple encoder-decoder model from Sec. 6.2.\nThis is not the only situation in which this type of behaviour happens. Another\npossible scenario is for the encoder to make the last memory states, −→h Tx and ←−h 1, of\nthe forward and reverse recurrent neural networks to have a special mark telling that\nthese are the last states. The attention mechanism then can exploit this to assign a large\nscore to these two memory states (but still constant across time t.) This will become\neven closer to the simple encoder-decoder model.\nThe question is how we can avoid these degenerate cases. Or, is it necessary for us\nto explicitly make these degenerate cases unlikely? Of course, there is no single answer\nto this question. Let me give you my answer, which may differ from others’ answer:\nno.\nThe goal of introducing a novel network architecture is to guide a model according\nto our intuition or scientiﬁc observation so that it will do a better job at a target task. In\nour case, the attention mechanism was introduced based on our observation, and some\nintuition, that it is not desirable to ask the encoder to compress a whole source sentence\ninto a single vector.\nThis incorporation of prior knowledge however should not put a hard constraint.\nWe give a model a possibility of exploiting this prior knowledge, but should not force\nthe model to use this prior knowledge exclusively. As this prior knowledge, based\non our observation of a small portion of data, is not likely to be true in general, the\nmodel must be able to ignore this, if the data does not exhibit the underlying structure\ncorresponding to this prior knowledge. In this case of attention-based encoder-decoder\nmodel, the existence of those degenerate cases above is a direct evidence of what this\nattention-based model can do, if there is no such underlying structure present in the\ndata.\n101\nThen, a natural next question is whether there are such structures that can be well\nexploited by this attention mechanism in real data. If we train this attention-based\nencoder-decoder model on the parallel corpora we discussed earlier in Sec. 6.1.1, what\nkind of structure does this attention-based model learn?\nIn order to answer this question, we must ﬁrst realize that we can easily visualize\nwhat is happening inside this attention-based model. First, note that given a pair of\nsource X and target Y sentences,14 the attention-based model computes an alignment\nmatrix A ∈[0,1]|X|×|Y|:\nA =\n\n\nα1,1\nα1,2\n···\nα1,|Y|\nα2,1\nα2,2\n···\nα2,|Y|\n...\n...\n...\n...\nα|X|,1\nα|X|,2\n···\nα|X|,|Y|\n\n,\nwhere αj,t is deﬁned in Eq. (6.11).\nEach column at of this alignment matrix A is how well each source word (based\non its context-dependent vector representation from Eq. (6.9)) is aligned to the t-th\ntarget word. Each row bj similarly shows how well each target word is aligned to the\ncontent-dependent vector of the j-th source word. In other words, we can simply draw\nthe alignment matrix A as if it were a gray scale 2-D image.\nIn Fig. 6.9, the visualization of four alignment matrices is presented. It is quite\nclear, especially to a French-English bilingual speaker, that the model indeed captured\nthe underlying structure of word/phrase mapping between two languages. For instance,\nfocus on “European Economic Area” in Fig. 6.9 (a). The model correctly noticed\nthat “Area” corresponds to “zone”, “Economic” to “´economique”, and “European” to\n“europ´eenne”, without any supervision about this type of alignment.\nThis is nice to see that the model was able to notice these regularities from data\nwithout any explicit supervision. However, the goal of introducing the attention mech-\nanism was not to get these pretty ﬁgures. After all, our goal is not to build an inter-\npretable model, but a model that is predictive of the correct output given an input (see\nChapter 1 and [16].) In this regard, how much does the introduction of the attention\nmechanism help?\nIn [2], the attention-based encoder-decoder model was compared against the sim-\nple encoder-decoder model in the task of English-French translation. They observed\nthe relative improvement of up to 60% (in terms of BLEU, see Sec. 6.1.2,) as shown in\nTable 6.1. Furthermore, by using some of the latest techniques, such as handling large\nvocabularies [55], building a vocabulary of subword units [93] and variants of the atten-\ntion mechanism [76], it has been found possible to achieve a better translation quality\nwith neural machine translation than the existing state-of-the-art translation systems.\n14 Note that if you’re given only a source sentence, you can let the model translate and align simultane-\nously.\n102\nModel\nBLEU\nRel. Improvement\nSimple Enc–Dec\n17.82\n–\nAttention-based Enc–Dec\n28.45\n+59.7%\nAttention-based Enc–Dec (LV)\n34.11\n+90.7%\nAttention-based Enc–Dec (LV)⋆\n37.19\n+106.0%\nState-of-the-art SMT◦\n37.03\n–\nTable 6.1: The translation performances and the relative improvements over the simple\nencoder-decoder model on an English-to-French translation task (WMT’14), measured\nby BLEU [2, 55]. ⋆: an ensemble of multiple attention-based models. ◦: the state-of-\nthe-art phrase-based statistical machine translation system [39].\n6.4\nWarren Weaver’s Memorandum\nIn 1949 Warren Weaver15 wrote a memorandum, titled ⟨Translation⟩on machine trans-\nlation [108]. Although this text was written way before computers have become ubiq-\nuitous,16 there are many interesting ideas that are closely related to what we have dis-\ncussed so far in this chapter. Let us go over some parts of the Weaver’s memorandum\nand see how the ideas there corresponds to modern-day machine translation.\nNecessity of Linguistic Knowledge\nWeaver talks about a distinguished mathemati-\ncian P who was surprised by his colleague. His colleague “had an amateur interest in\ncryptography”, and one day presented P his method to “decipher” an encrypted Turkish\ntext successfully. “The most important point”, according to Weaver, from this instance\nis that “the decoding was done by someone who did not know Turkish.” Now, this\nsounds familiar, doesn’t it?\nAs long as there was a parallel corpus, we are able to use neural machine transla-\ntion models, described throughout this chapter, without ever caring about which lan-\nguages we are training a model to translate between. Especially if we decide to consider\neach sentence as a sequence of characters,17 there is almost no need for any linguistic\nknowledge when building these neural machine translation systems.\nThis lack of necessity for linguistic knowledge is not new. In fact, the most widely\nstudied and used machine translation approach, which is (count-based) statistical ma-\nchine translation [19, 66], does not require any prior knowledge about source and target\nlanguages. All it needs is a large corpus.\nImportance of Context\nRecall from Sec. 6.3 that the encoder of an attention-based\nneural machine translation uses a bidirectional recurrent neural network in order to ob-\ntain a context set. Each vector in the context set was considered a context-dependent\n15 Yes, this is the very same Weaver after which the building of the Courant Institute of Mathematical\nSciences has been named.\n16 Although Weaver talks about modern computers over and over in his memorandum, what he refers to\nis not exactly what we think of computers as these days.\n17 In fact, only very recently people have started investigating the possibility of building a machine trans-\nlation system based on character sequences [73]. This has been made possible due to the recent success of\nneural machine translation.\n103\nvector, as it represents what the center word means with respect to all the surround-\ning words. This context dependency is a necessary component in making the whole\nattention-based neural machine translation, as it helps disambiguating the meaning of\neach word and also distinguishing multiple occurrences of a single word by their con-\ntext.\nWeaver discusses this extensively in Sec. 3–4 in his memorandum. First, to Weaver,\nit was “amply clear that a translation procedure that does little more than handle a one-\nto-one correspondence of words can not hope to be useful .. in which the problems\nof .. multiple meanings .. are frequent.” In other words, it is simply not possible to\nlook at each word separately from surrounding words (or context) and translate it to a\ncorresponding target word, because there is uncertainty in the meaning of the source\nword which can only be resolved by taking into account its context.\nSo, what does Weaver propose in order to address this issue? He proposes in Sec. 5\nthat if “one can see not only the central word in question, but also say N words on\neither side, then if [sic] N is large enough one can unambiguously decide the meaning\nof the central word.” If we consider only a single sentence and take the inﬁnite limit of\nN →∞, we see that what Weaver refers to is exactly the bidirectional recurrent neural\nnetwork used by the encoder of the attention-based translation system. Furthermore,\nwe see that the continuous bag-of-words language model, or Markov random ﬁeld\nbased language model, from Sec. 5.4.2 exactly does what Weaver proposed by setting\nN to a ﬁnite number.\nIn Sec. 5.2.1, we talked about the issue of data sparsity, and how it is desirable to\nhave a larger N but it’s often not a good idea statistically to do so. Weaver was also\nworried about this by saying that “it would hardly be practical to do this by means of\na generalized dictionary which contains all possible phases [sic] 2N + 1 words long;\nfor the number of such phases [sic] is horrifying.” We learned that this issue of data\nsparsity can be largely avoided by adopting a fully parametric approach instead of a\ntable-based approach in Sec. 5.4.\nCommon base of human communications\nWeaver suggested in the last section of\nhis memorandum that “perhaps the way” for translation “is to descend, from each lan-\nguage, down to the common base of human communication – the real but as yet undis-\ncovered universal language – and then re-emerge by whatever particular route is conve-\nnient.” He speciﬁcally talked about a “universal language”, and this makes me wonder\nif we can consider the memory state of the recurrent neural networks (both of the en-\ncoder and decoder) as this kind of intermediate language. This intermediate language\nradically departs from our common notion of natural languages. Unlike conventional\nlanguages, it does not use discrete symbols, but uses continuous vectors. This use of\ncontinuous vectors allows us to use simple arithmetics to manipulate the meaning, as\nwell as its surface realization.18\nThis view may sound radical, considering that what we’ve discussed so far has been\nconﬁned to translating from one language to another. After all, this universal language\n18 If you ﬁnd this view too radical or fascinating, I suggest you to look at the presentation slides by\nGeoff Hinton at https://drive.google.com/file/d/0B16RwCMQqrtdMWFaeThBTC1mZkk/\nview?usp=sharing\n104\nof ours is very speciﬁc to only a single source language with respect to a single target\nlanguage. This is however not a constraint on the neural machine translation by design,\nbut simply a consequence of our having focused on this speciﬁc case.\nIndeed, in this year (2015), researchers have begun to report that it is possible to\nbuild a neural machine translation model that considers multiple languages, and even\nfurther multiple tasks [38, 75]. More works in this line are expected, and it will be\ninteresting to see if Weaver’s prediction again turns out to be true.\n105\nThe\nagreement\non\nthe\nEuropean\nEconomic\nArea\nwas\nsigned\nin\nAugust\n1992\n.\n<end>\nL'\naccord\nsur\nla\nzone\néconomique\neuropéenne\na\nété\nsigné\nen\naoût\n1992\n.\n<end>\nIt\nshould\nbe\nnoted\nthat\nthe\nmarine\nenvironment\nis\nthe\nleast\nknown\nof\nenvironments\n.\n<end>\nIl\nconvient\nde\nnoter\nque\nl'\nenvironnement\nmarin\nest\nle\nmoins\nconnu\nde\nl'\nenvironnement\n.\n<end>\n(a)\n(b)\nDestruction\nof\nthe\nequipment\nmeans\nthat\nSyria\ncan\nno\nlonger\nproduce\nnew\nchemical\nweapons\n.\n<end>\nLa\ndestruction\nde\nl'\néquipement\nsignifie\nque\nla\nSyrie\nne\npeut\nplus\nproduire\nde\nnouvelles\narmes\nchimiques\n.\n<end>\n\"\nThis\nwill\nchange\nmy\nfuture\nwith\nmy\nfamily\n,\n\"\nthe\nman\nsaid\n.\n<end>\n\"\nCela\nva\nchanger\nmon\navenir\navec\nma\nfamille\n\"\n,\na\ndit\nl'\nhomme\n.\n<end>\n(c)\n(d)\nFigure 6.9:\nVisualizations of the four sample alignment matrices. The alignment\nmatrices were computed from an attention-based translation model trained to translate\na sentence in English to French. Reprinted from [2].\n106\nChapter 7\nFinal Words\nLet me wrap up this lecture note by describing some aspects of natural language under-\nstanding with distributed representations that I have not discussed in this course. These\nare the topics that I would have spent time on, had the course been scheduled to last\ntwice the duration as it is now. Afterward, I will ﬁnalize this whole lecture note with a\nshort summary.\n7.1\nMultimedia Description Generation as Translation\nThose who have followed this course closely so far must have noticed that the neural\nmachine translation model described in the previous chapter is quite general in the\nsense that the input to the model does not have to be a sentence. In the case of the\nsimple encoder-decoder model from Sec. 6.2, it is clear that any type of input X can be\nused instead of a sentence, as long as there is a feature extractor that returns the vector\nrepresentation c of the input.\nAnd, fortunately, we already learned how to build a feature extractor throughout\nthis course. Almost every single model (that is, a neural network in our case) converts\nan input into a continuous vector. Let us take a multilayer perceptron from Sec. 3.3\nas an example. Any classiﬁer built as a multilayer perceptron can be considered as a\ntwo-stage process (see Sec. 3.3.2.) First, the feature vector of the input is extracted (see\nEq. (3.9)):\nφ(x) = σ(ux+c).\nThe extracted feature vector φ(x) is then afﬁne-transformed, followed by softmax func-\ntion. This results in a conditional distribution over all possible labels (see Eq. (4.4).)\nThis means that we can make the simple encoder-decoder model to work with non-\nlanguage input simply by replacing the recurrent neural network based encoder with\nthe feature extraction stage of the multilayer perceptron. Furthermore, it is possible to\npretrain this feature extractor by training the whole multilayer perceptron on a separate\nclassiﬁcation dataset.1\n1 This way of using a feature extractor pretrained from another network has become a de facto standard\n107\nThis approach of using the encoder-decoder model for describing non-language\ninput has become popular in recent years (especially, 2014 and 2015,) and has been\napplied to many applications, including image/video description generation and speech\nrecognition. For an extensive list of these applications, I refer the readers to a recent\nreview article by Cho et al. [26].\nExample: Image Caption Generation\nLet me take as an example the task of im-\nage caption generation. The possibility of using the encoder-decoder model for image\ncaption generation was noticed by several research groups (almost simultaneously) last\nyear (2014) [62, 106, 59, 78, 37, 40, 25].2 The success of neural machine translation\nin [101] and earlier success of deep convolutional network on object recognition (see,\ne.g., [67, 96, 102]) inspired them the idea to use the deep convolutional network’s fea-\nture extractor together with the recurrent neural network decoder for the task of image\ncaption generation.\nAnnotation\nVectors\nWord \nSsample\nui\nRecurrent\nState\nzi\nf = (a,   man,   is,   jumping,   into,   a,   lake,   .)\n+\nhj\nAttention\nMechanism\na\nAttention \n       weight\nj\naj\nΣ\n=1\nConvolutional Neural Network\nFigure 7.1: Image caption generation with\nthe attention-based encoder-decoder model\n[111].\nRight after these, Xu et al. [111]\nrealized that it is possible to use the\nattention-based encoder-decoder model\nfrom Sec. 6.3 for image caption gen-\neration.\nUnlike the simple model, the\nattention-based model requires a context\nset instead of a context vector. The con-\ntext set should contain multiple context\nvectors, and each vector should repre-\nsent a spatial location with respect to\nthe whole image, meaning each context\nvector is a spatially-localized, context-\ndependent image descriptor. This was\nachieved by using the last convolutional\nlayer’s activations of the pretrained deep\nconvolutional network instead of the last\nfully-connected layer’s. See Fig. 7.1 for\ngraphical illustration of this approach.\nThese approaches based on neural\nnetworks, or in other words based on dis-\ntributed representations, have been suc-\ncessful at image caption generation. Four out of ﬁve top rankers in the recent Microsoft\nCoCo Image Captioning Challenge 20153 were using variants of the neural encoder-\ndecoder model, based on human evaluation of the captions.\nin many of the computer vision tasks [94]. This is also closely related to semi-supervised learning with\npretrained word embeddings which we discussed in Sec. 5.4.3. In that case, it was only the ﬁrst input layer\nthat was pretrained and used later (see Eqs. (5.11)–(5.12).)\n2 I must however make a note that Kiros et al. [62] proposed a fully neural network based image caption\ngeneration earlier than all the others cited here did.\n3 http://mscoco.org/dataset/#captions-leaderboard\n108\n7.2\nLanguage Understanding with World Knowledge\nIn Sec. 1.2, we talked about how we view natural languages as a function. This function\nof natural language maps from a tuple of a speaker’s speech, a listener’s mental state\nand the surrounding world to the listener’s reaction, often as a form of natural language\nresponse. Unfortunately, in order to make it manageable, we decided to build a model\nthat approximates only a part of this true function.\nImmediate state of the surrounding world\nIn this course of action, one thing we\nhave dropped out is the surrounding world. The surrounding world may mean many\ndifferent things. One of them is the current state of the surrounding world. As an\nexample, when I say “look at this cute llama,” it is quite likely that the surrounding\nworld at the current state contains either an actual llama or at least a picture of a llama.\nA listener then understands easily what a llama is even without having known what a\nllama is in advance. By looking at the picture of llama, the listener makes a mental note\nthat the llama looks similar to a camel and therefore must be a four-legged animal.\nIf the surrounding world is not taken into account, as we’ve been doing so far,\nthe listener can only generalize based on the context words. Just like how the neural\nlanguage model from Sec. 5.4 generalized to unseen, or rarely seen words, the listener\ncan infer that “llama” must be a type of animal by remembering that the phrase “look\nat this cute” has mainly been followed by an animal such as “cat” or “dog”. However,\nit is quite clear that “look at this cute” is also followed by many other nouns, including\n“baby”, “book” and so on.\nThe question is then how to exploit this. How can we incorporate, for instance,\nvision information from the surrounding world into natural language understanding?\nThe simplest approach is to simply concatenate a word embedding vector (see\nEq. (5.12)) and a corresponding image vector (obtained from an existing feature ex-\ntractor, see above) [60]. This can be applied to any existing language models such as\nneural language model (see Sec. 5.4) and neural machine translation model (see Chap-\nter 6.) This approach gives a strong signal to the model the similarities among different\nwords based on the corresponding objects’ appearances. This approach of concatenat-\ning vectors of two different modalities, e.g., language and vision, was earlier proposed\nin [109].\nA more sophisticated approach is to design and train a model to solve a task that\nrequires tight interaction between language and other modalities. As our original goal\nis to build a natural language function, all we need to do is to build a function approxi-\nmator that takes as input both language and other modalities. Recently, Antol et al. [1]\nbuilt a large-scale dataset of question-answer-image triplets, called visual question an-\nswering (VQA) for this speciﬁc purpose. They have carefully built the dataset such\nthat many, if not most, questions can only be answered when the accompanying image\nis taken into consideration. Any model that’s able to solve the questions in this dataset\nwell will have to consider both language and vision.\nKnowledge base: Lost in a library\nSo far, we have talked about incorporating an\nimmediate state of the surrounding world. However, our use of languages is more\n109\nsophisticated. This is especially apparent in written languages. Let us take an example\nof me writing this lecture note. It is not the case where I simply sit and start writing\nthe whole text based purely on my mental state (with memory of my past research) and\nthe immediate surrounding world state (which has almost nothing to do with.) Rather,\na large part of this writing process is spent on going through various research articles\nand books written by others in order to ﬁnd relevant details of the topic.\nIn this case, the surrounding world is a database in which human knowledge is\nstored. You can think of a library or the Internet. As the amount of knowledge is\nsimply too large to be memorized in the entirety, it is necessary for a person to be able\nto search through the vast knowledge base. But, wait, what does it have to do with\nnatural language understanding?\nConsider the case where the context phrase is “Llama is a domesticated camelid\nfrom”. Without access to the knowledge base, or in this speciﬁc instance, access to\nWikipedia, any language model can only say as much as that this context phrase is\nlikely followed by a name of some place. This is especially true, if we assume that the\ntraining corpus did not mention “llama” at all. However, if the language model is able\nto search Wikipedia and condition on its search result, it suddenly becomes so obvious\nthat this context phrase is followed by “South America” or the name of any region on\nAndean mountain rages.\nAlthough this may sound too complicated a task to incorporate into a neural net-\nwork, the concept of how to incorporate this is not necessarily complicated. In fact, we\ncan use the attention mechanism, discussed in Sec. 6.3, almost as it is. Let us describe\nhere a conceptual picture of how this can be done.\nLet D = {d1,d2,...,dM} be a set of knowledge vectors. Each knowledge vector\ndi is a vector representation of a piece of knowledge. For instance, di can be a vector\nrepresentation of one Wikipedia article. It is certainly unclear what is the best way to\nobtain this vector representation of an entire article, but let us assume that an oracle\ngave us a means to do so.\nLet us focus on recurrent language modelling from Sec. 5.5.4 At each time step,\nwe have access to the following vectors:\n1. Context vector ht−1: the summary all the preceding words\n2. Current word wt: the current input word\nSimilarly to what we have done in Sec. 6.3, we will deﬁne a scoring function fscore\nwhich scores each knowledge vector di with respect to the context vector and the cur-\nrent word:\nαi,t ∝exp(fscore(di,ht−1,ewt)),\nwhere ewt is a vector representation of the current word wt.\nThis score reﬂects the relevance of the knowledge in predicting the next word, and\nonce it is computed for every knowledge vector, we compute the weighted sum of all\n4 This approach of using attention mechanism for external knowledge pieces has been proposed recently\nin [14], in the context of question-answering. Here, we stick to language modelling, as the course has not\ndealt with question-answering tasks.\n110\nthe knowledge:\n˜dt =\nM\n∑\ni=1\nαi,tdi.\nThis vector ˜dt is a vector summary of the knowledge relevant to the next word, taking\ninto account the context phrase. In the case of an earlier example, the scoring function\ngives a high score to the Wikipedia article on “llama” based on the history of preceding\nwords “Llama is a domesticated camelid from”.\nThis knowledge vector is used when updating the memory state of the recurrent\nneural network:\nht = frec\n\u0000ht−1,ewt, ˜dt\n\u0001\n.\nFrom this updated memory state, which also contains the knowledge extracted from\nthe selected knowledge vector, the next word’s distribution is computed according to\nEq. (4.6).\nOne important issue with this approach is that the size of knowledge set D is often\nextremely large. For instance, English Wikipedia contains more than 5M articles as of\n23 Nov 2015.5 It easily becomes impossible to score each and every knowledge vector,\nnot to mention to extract knowledge vectors of all the articles.6 It is an open question\nhow this unreasonable amount of computation needed for search can be avoided.\nWhy is this any signiﬁcant?\nOne may naively think that if we train a large enough\nnetwork with a large enough data which contains all those world knowledge, a trained\nnetwork will be able to contain all those world knowledge (likely in a compressed\nform) in its parameters together with its network architecture. This is true up to a\ncertain level, but there are many issues here.\nFirst, the world knowledge we’re talking about here contains all the knowledge\naccumulated so far. Even a human brain, arguably the best working neural network\nto date, cannot store all the world knowledge and must resort to searching over the\nexternal database of knowledge. It is no wonder we have libraries where people can go\nand look for relevant knowledge.\nSecond, the world knowledge is dynamic. Every day some parts of the world\nknowledge become obsolete, and at the same time previously unknown facts are added\nto the world knowledge. If anyone looked up “Facebook” before 2004, they would’ve\nended up with yearly facebooks from American universities. Nowadays, it is almost\ncertain that when a person looks up “Facebook”, they will ﬁnd information on “Face-\nbook” the social network site. Having all the current world knowledge encoded in the\nmodel’s parameters is not ideal in this sense.\n5 https://en.wikipedia.org/wiki/Wikipedia:Statistics\n6 This is true especially when those knowledge vectors are also updated during training.\n111\n7.3\nLarger-Context Language Understanding:\nBeyond Sentences and Beyond Words\nIf we view natural language as a function, it becomes clear that what we’ve discussed\nso far throughout the course is heavily restrictive. There are two reasons behind this\nrestriction.\nFirst, what we have discussed so far has narrowly focused on handling a sentence.\nIn Sec. 5.2, I have described language model as a way to model a sentence probability\np(S). This is a bit weird in the sense that we’ve been using a term “language” modelling\nnot “sentence” modelling. Keeping it in mind, we can start looking at a probability of a\ndocument or discourse D as a whole rather than as a product of sentence probabilities:\np(D) =\nN\n∏\nk=1\np(Sk|S<k),\nwhere the document D consists of N sentences. This approach is readily integrated into\nthe language modelling approaches we discussed earlier in Chapter 5 by\np(D) =\nN\n∏\nk=1\nTk\n∏\nj=1\np(w j|w<j,S<k).\nThis is applicable to any language-related models we have discussed so far, includ-\ning neural language model from Sec. 5.4, recurrent language model from Sec. 5.5,\nMarkov random ﬁeld language model from Sec. 5.4.2 and neural machine translation\nfrom Chapter 6.\nIn the context of language modelling, two recent articles proposed to explore this\ndirection. I refer the readers to [107] and [57].\nSecond, we have stuck to representing a sentence as a sequence of words so far,\ndespite a short discussion in Sec. 5.1.2 where I strongly claim that this does not have\nto be. This is indeed true, and in fact, even if we replace most occurrence of “word”\nin this course with, for instance, “character”, all the arguments stand. Of course, by\nusing smaller units than words, we run into many practical and theoretical issues. One\nmost severe practical issue is that each sentence suddenly becomes much longer. One\nmost sever theoretical issue is that it is a highly nonlinear mapping from a sequence\nof characters to its meaning, as we discussed earlier in Sec. 5.1.2. Nevertheless, the\nadvance in computing and deep neural networks, which are capable of learning such\na highly nonlinear mapping, have begun to let researchers directly work on this prob-\nlem of using subword units (see, e.g., [61, 73].) Note that I am not trying to say that\ncharacters are the only possible sub-word units, and recently an effective statistical ap-\nproach to deriving sub-word units off-line was proposed and applied to neural machine\ntranslation in [93].\n112\n7.4\nWarning and Summary\nBefore I ﬁnish this lecture note with the summary of what we have discussed through-\nout this course, let me warn you by quoting Claude Shannon [95]:7\nIt will be all too easy for our somewhat artiﬁcial prosperity to collapse\novernight when it is realized that the use of a few exciting words like in-\nformation, entropy, redundancy, do not solve all our problems.\nNatural language understanding with distributed representation is a fascinating topic\nthat has recently gathered large interest from both machine learning and natural lan-\nguage processing communities. This may give a wrong sign that this approach with\nneural networks is an ultimate winner in natural language understanding/processing,\nthough without any ill intention. As Shannon pointed out, this prosperity of distributed\nrepresentation based natural language understanding may collapse overnight, as can\nany other approaches out there.8 Therefore, I warn the readers, especially students, to\nkeep this quote in their mind and remember that it is not a few recent successes of this\napproach to natural language understanding but the fundamental ideas underlying this\napproach that matter and should be remembered after this course.\nSummary\nFinally, here goes the summary of what we have learned throughout this\nsemester. We began our journey by a brief discussion on how we view human language\nas, and we decided to stick to the idea that a language is a function not an entity existing\nindependent of the surrounding world, including speakers and listeners. Is this a correct\nway to view a human language? Maybe, maybe not.. I will leave it up to you to decide.\nIn order to build a machine that can approximate this language function, in Chap-\nter 2, we studied basic ideas behind supervised learning in machine learning. We de-\nﬁned what a cost function is, how we can minimize it using an iterative optimization\nalgorithm, speciﬁcally stochastic gradient descent, and learned the importance of hav-\ning a validation set for both early-stopping and model selection. These are all basic\ntopics that are dealt in almost any basic machine learning course, and the only thing\nthat I would like to emphasize is the importance of not looking at a held-out test set.\nOne must always select anything related to learning, e.g., hyperparameters, networks\narchitectures and so on, based solely on a validation set. As soon as one tunes any\nof those based on the test set performance, any result from this tuning easily becomes\ninvalid, or at least highly disputable.\nIn Chapter 3, we ﬁnally talked about deep neural networks, or more traditionally\ncalled multilayer perceptron.9 I tried to go over basic, but important details as slowly as\npossible, including how to build a deep neural network based classiﬁer, how to deﬁne\na cost function and how to compute the gradient w.r.t. the parameters of the network.\nHowever, I must confess that there are better materials for this topic than this lecture\nnote.\n7 I would like to thank Adam Lopez for pointing me to this quote.\n8 Though, it is interesting to note that information theory never really collapsed overnight. Rather its\nprosperity has been continuing for more than half a century since Shannon warned us about its potential\novernight collapse in 1956.\n9 I personally prefer “multilayer perceptron”, but it seems like it has gone out of fashion.\n113\nWe then moved on to recurrent neural networks in Chapter 4. This was a necessary\nstep in order to build a neural network based model that can handle both variable-length\ninput and output. Again, my goal here was to take as much time as it is needed to moti-\nvate the need of recurrent networks and to give you basic ideas underlying them. Also,\nI spent quite some time on why it has been considered difﬁcult to train recurrent neural\nnetworks by stochastic gradient descent like algorithms, and as a remedy, introduced\ngated recurrent units and long short-term memory units.\nOnly after these long four to ﬁve weeks, have I started talking about how to handle\nlanguage data in Chapter 5. I motivated neural language models by the lack of general-\nization and the curse of data sparsity. It is my regret that I have not spent much time on\ndiscussing the existing techniques for count-based n-gram language models, but again,\nthere are much better materials and better lecturers for these techniques already. Af-\nter the introduction of neural language model, I spent some time on describing how\nthis neural language model is capable of generalizing to unseen phrases. Continuing\nfrom this neural language model, in Sec. 5.5, language modelling using recurrent neu-\nral networks was introduced as a way to avoid Markov assumption of n-gram language\nmodel.\nThis discussion on neural language model naturally continued on to neural machine\ntranslation in Chapter 6. Rather than going directly into describing neural machine\ntranslation models, I have spent a full week on two issues that are often overlooked;\ndata preparation in Sec. 6.1.1 and evaluation in Sec. 6.1.2. I wish the discussion of these\ntwo topics has reminded students that machine learning is not only about algorithms\nand models but is about a full pipeline starting from data collection to evaluation (often\nwith loops here and there.) This chapter ﬁnished with where we are in 2015, compared\nto what Weaver predicted in 1949.\nOf course, there are so many interesting topics in this area of natural language\nunderstanding. I am not qualiﬁed nor knowledgeable to teach many, if not most, of\nthose topics unfortunately, and have focused on those few topics that I have worked on\nmyself. I hope this lecture note will serve at least as a useful starting point into more\nadvanced topics in natural language understanding with distributed representations.\n114\nBibliography\n[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh.\nVqa: Visual question answering. In International Conference on Computer Vi-\nsion (ICCV), 2015.\n[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[3] P. Baltescu and P. Blunsom. Pragmatic neural language modelling in machine\ntranslation. arXiv preprint arXiv:1412.7119, 2014.\n[4] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron,\nN. Bouchard, D. Warde-Farley, and Y. Bengio. Theano: new features and speed\nimprovements. arXiv preprint arXiv:1211.5590, 2012.\n[5] A. G. Baydin, B. A. Pearlmutter, and A. A. Radul. Automatic differentiation in\nmachine learning: a survey. arXiv preprint arXiv:1502.05767, 2015.\n[6] Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimiz-\ning recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP),\n2013 IEEE International Conference on, pages 8624–8628. IEEE, 2013.\n[7] Y. Bengio, N. L´eonard, and A. Courville.\nEstimating or propagating gradi-\nents through stochastic neurons for conditional computation.\narXiv preprint\narXiv:1308.3432, 2013.\n[8] Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L. Gauvain. Neural\nprobabilistic language models. In Innovations in Machine Learning, pages 137–\n186. Springer Berlin Heidelberg, 2006.\n[9] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with\ngradient descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–\n166, 1994.\n[10] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins,\nJ. Turian, D. Warde-Farley, and Y. Bengio. Theano: a cpu and gpu math expres-\nsion compiler. In Proceedings of the Python for scientiﬁc computing conference\n(SciPy), volume 4, page 3. Austin, TX, 2010.\n115\n[11] J. Besag. Statistical analysis of non-lattice data. The statistician, pages 179–195,\n1975.\n[12] C. M. Bishop. Mixture density networks. 1994.\n[13] C. M. Bishop. Pattern recognition and machine learning. springer, 2006.\n[14] A. Bordes, N. Usunier, S. Chopra, and J. Weston. Large-scale simple question\nanswering with memory networks. arXiv preprint arXiv:1506.02075, 2015.\n[15] L. Bottou. Online algorithms and stochastic approximations. In D. Saad, edi-\ntor, Online Learning and Neural Networks. Cambridge University Press, Cam-\nbridge, UK, 1998.\n[16] L. Breiman et al. Statistical modeling: The two cultures (with comments and a\nrejoinder by the author). Statistical Science, 16(3):199–231, 2001.\n[17] J. S. Bridle. Training stochastic model recognition algorithms as networks can\nlead to maximum mutual information estimation of parameters. In D. Touretzky,\neditor, Advances in Neural Information Processing Systems 2, pages 211–217.\nMorgan-Kaufmann, 1990.\n[18] E. Brochu, V. M. Cora, and N. de Freitas. A tutorial on Bayesian optimization\nof expensive cost functions, with application to active user modeling and hierar-\nchical reinforcement learning. arXiv:1012.2599 [cs.LG], Dec. 2010.\n[19] P. F. Brown, J. Cocke, S. A. D. Pietra, V. J. D. Pietra, F. Jelinek, J. D. Lafferty,\nR. L. Mercer, and P. S. Roossin. A statistical approach to machine translation.\nComputational linguistics, 16(2):79–85, 1990.\n[20] C. Callison-Burch, M. Osborne, and P. Koehn. Re-evaluation the role of bleu in\nmachine translation research. In EACL, volume 6, pages 249–256, 2006.\n[21] A. Carnie. Syntax: A generative introduction. John Wiley & Sons, 2013.\n[22] M. Cettolo, C. Girardi, and M. Federico. Wit3: Web inventory of transcribed\nand translated talks. In Proceedings of the 16th Conference of the European As-\nsociation for Machine Translation (EAMT), pages 261–268, Trento, Italy, May\n2012.\n[23] O. Chapelle, B. Sch¨olkopf, and A. Zien, editors. Semi-Supervised Learning.\nMIT Press, Cambridge, MA, 2006.\n[24] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for\nlanguage modeling. In Proceedings of the 34th annual meeting on Association\nfor Computational Linguistics, pages 310–318. Association for Computational\nLinguistics, 1996.\n[25] X. Chen and C. L. Zitnick. Learning a recurrent visual representation for image\ncaption generation. arXiv:1411.5654, 2014.\n116\n[26] K. Cho, A. Courville, and Y. Bengio.\nDescribing multimedia content using\nattention-based encoder–decoder networks. 2015.\n[27] K. Cho, B. van Merri¨enboer, D. Bahdanau, and Y. Bengio. On the properties\nof neural machine translation: Encoder-decoder approaches.\narXiv preprint\narXiv:1409.1259, 2014.\n[28] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation.\narXiv preprint arXiv:1406.1078,\n2014.\n[29] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Ben-\ngio. Learning phrase representations using RNN encoder-decoder for statistical\nmachine translation. In Proceedings of the Empiricial Methods in Natural Lan-\nguage Processing (EMNLP 2014), Oct. 2014.\n[30] N. Chomsky. A review of B. F. skinner’s verbal behavior. Language, 35(1):26–\n58, 1959.\n[31] N. Chomsky. Linguistic contributions to the study of mind (future). Language\nand thinking, pages 323–364, 1968.\n[32] N. Chomsky. Syntactic structures. Walter de Gruyter, 2002.\n[33] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa.\nNatural language processing (almost) from scratch. The Journal of Machine\nLearning Research, 12:2493–2537, 2011.\n[34] T. M. Cover. Geometrical and statistical properties of systems of linear inequal-\nities with applications in pattern recognition. IEEE Transactions on Electronic\nComputers, EC-14(3):326–334, 1965.\n[35] J. Denker and Y. Lecun. Transforming neural-net output levels to probability\ndistributions. In Advances in Neural Information Processing Systems 3. Citeseer,\n1991.\n[36] M. Denkowski and A. Lavie. Meteor universal: Language speciﬁc translation\nevaluation for any target language. In Proceedings of the EACL 2014 Workshop\non Statistical Machine Translation, 2014.\n[37] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,\nK. Saenko, and T. Darrell. Long-term recurrent convolutional networks for vi-\nsual recognition and description. arXiv:1411.4389, 2014.\n[38] D. Dong, H. Wu, W. He, D. Yu, and H. Wang. Multi-task learning for multiple\nlanguage translation. ACL, 2015.\n117\n[39] N. Durrani, B. Haddow, P. Koehn, and K. Heaﬁeld. Edinburgh’s phrase-based\nmachine translation systems for WMT-14. In Proceedings of the Ninth Work-\nshop on Statistical Machine Translation, pages 97–104. Association for Com-\nputational Linguistics Baltimore, MD, USA, 2014.\n[40] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll´ar, J. Gao, X. He,\nM. Mitchell, J. C. Platt, C. L. Zitnick, and G. Zweig. From captions to visual\nconcepts and back. arXiv:1411.4952, 2014.\n[41] J. R. Firth. A synopsis of linguistic theory 1930-1955. Oxford: Philological\nSociety, 1957.\n[42] R. Fletcher. Practical Methods of Optimization. Wiley-Interscience, New York,\nNY, USA, 2nd edition, 1987.\n[43] M. L. Forcada and R. P. ˜Neco. Recursive hetero-associative memories for trans-\nlation. In Biological and Artiﬁcial Computation: From Neuroscience to Tech-\nnology, pages 453–462. Springer, 1997.\n[44] D. Furcy and S. Koenig. Limited discrepancy beam search. In IJCAI, pages\n125–131, 2005.\n[45] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual\nprediction with lstm. Neural computation, 12(10):2451–2471, 2000.\n[46] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer neural networks.\nIn International Conference on Artiﬁcial Intelligence and Statistics, pages 315–\n323, 2011.\n[47] Y. Goldberg. A primer on neural network models for natural language process-\ning. arXiv preprint arXiv:1510.00726, 2015.\n[48] I. Goodfellow, D. Warde-farley, M. Mirza, A. Courville, and Y. Bengio. Maxout\nnetworks.\nIn Proceedings of the 30th International Conference on Machine\nLearning (ICML-13), pages 1319–1327, 2013.\n[49] K. Greff, R. K. Srivastava, J. Koutn´ık, B. R. Steunebrink, and J. Schmidhuber.\nLstm: A search space odyssey. arXiv preprint arXiv:1503.04069, 2015.\n[50] K. He, X. Zhang, S. Ren, and J. Sun.\nDelving deep into rectiﬁers: Sur-\npassing human-level performance on imagenet classiﬁcation.\narXiv preprint\narXiv:1502.01852, 2015.\n[51] K. Heaﬁeld, I. Pouzyrevsky, J. H. Clark, and P. Koehn.\nScalable modiﬁed\nKneser-Ney language model estimation.\nIn Proceedings of the 51st Annual\nMeeting of the Association for Computational Linguistics, pages 690–696, Soﬁa,\nBulgaria, August 2013.\n[52] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term dependencies, volume 1. A\nﬁeld guide to dynamical recurrent neural networks. IEEE Press, 2001.\n118\n[53] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computa-\ntion, 9(8):1735–1780, 1997.\n[54] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew. Extreme learning machine: Theory\nand applications. Neurocomputing, 70(1—3):489–501, 2006.\n[55] S. Jean, K. Cho, R. Memisevic, and Y. Bengio.\nOn using very large target\nvocabulary for neural machine translation. In ACL 2015, 2014.\n[56] Y. Jernite, A. M. Rush, and D. Sontag. A fast variational approach for learn-\ning markov random ﬁeld language models. 32nd International Conference on\nMachine Learning (ICML), 2015.\n[57] Y. Ji, T. Cohn, L. Kong, C. Dyer, and J. Eisenstein. Document context language\nmodels. arXiv preprint arXiv:1511.03962, 2015.\n[58] R. Jozefowicz, W. Zaremba, and I. Sutskever. An empirical exploration of recur-\nrent network architectures. In Proceedings of the 32nd International Conference\non Machine Learning (ICML-15), pages 2342–2350, 2015.\n[59] A. Karpathy and F.-F. Li. Deep visual-semantic alignments for generating image\ndescriptions. arXiv:1412.2306, 2014.\n[60] D. Kiela and L. Bottou. Learning image embeddings using convolutional neural\nnetworks for improved multi-modal semantics. Proceedings of EMNLP, 2014,\n2014.\n[61] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush. Character-aware neural language\nmodels. arXiv preprint arXiv:1508.06615, 2015.\n[62] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models.\nIn ICML’2014, 2014.\n[63] R. Kneser and H. Ney. Improved backing-off for m-gram language modeling.\nIn Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 Interna-\ntional Conference on, volume 1, pages 181–184. IEEE, 1995.\n[64] P. Koehn. Pharaoh: a beam search decoder for phrase-based statistical machine\ntranslation models. In Machine translation: From real users to research, pages\n115–124. Springer, 2004.\n[65] P. Koehn. Europarl: A parallel corpus for statistical machine translation. In MT\nsummit, volume 5, pages 79–86. Citeseer, 2005.\n[66] P. Koehn, F. J. Och, and D. Marcu. Statistical phrase-based translation. In Pro-\nceedings of the 2003 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics on Human Language Technology-Volume 1,\npages 48–54. Association for Computational Linguistics, 2003.\n119\n[67] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural information processing\nsystems, pages 1097–1105, 2012.\n[68] T. S. Kuhn. The structure of scientiﬁc revolutions. University of Chicago press,\n2012.\n[69] Q. V. Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent net-\nworks of rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015.\n[70] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–\n444, 2015.\n[71] Y. LeCun, L. Bottou, G. Orr, and K. R. M¨uller. Efﬁcient BackProp. In G. Orr\nand K. M¨uller, editors, Neural Networks: Tricks of the Trade, volume 1524 of\nLecture Notes in Computer Science, pages 5–50. Springer Verlag, 1998.\n[72] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factor-\nization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Wein-\nberger, editors, Advances in Neural Information Processing Systems 27, pages\n2177–2185. Curran Associates, Inc., 2014.\n[73] W. Ling, I. Trancoso, C. Dyer, and A. W. Black. Character-based neural machine\ntranslation. arXiv preprint arXiv:1511.04586, 2015.\n[74] D. G. Lowe. Object recognition from local scale-invariant features. In Computer\nvision, 1999. The proceedings of the seventh IEEE international conference on,\nvolume 2, pages 1150–1157. Ieee, 1999.\n[75] M.-T. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser.\nMulti-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[76] M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[77] C. D. Manning and H. Sch¨utze. Foundations of statistical natural language\nprocessing. MIT press, 1999.\n[78] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain images with multi-\nmodal recurrent neural networks. arXiv:1410.1090, 2014.\n[79] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[80] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent\nneural network based language model. In INTERSPEECH 2010, pages 1045–\n1048, 2010.\n[81] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann\nmachines.\nIn Proceedings of the 27th International Conference on Machine\nLearning (ICML-10), pages 807–814, 2010.\n120\n[82] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting\non association for computational linguistics, pages 311–318. Association for\nComputational Linguistics, 2002.\n[83] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent\nneural networks. In Proceedings of The 30th International Conference on Ma-\nchine Learning, pages 1310–1318, 2013.\n[84] A. Perfors, J. Tenenbaum, and T. Regier. Poverty of the stimulus? a rational\napproach. In Annual Conference, 2006.\n[85] K. B. Petersen, M. S. Pedersen, et al. The matrix cookbook. Technical University\nof Denmark, 7:15, 2008.\n[86] C. W. Post. The Three Percent Problem: Rants and Responses on Publishing,\nTranslation, and the Future of Reading. Open Letter, 2011.\n[87] P. Resnik and N. A. Smith. The web as a parallel corpus. Computational Lin-\nguistics, 29(3):349–380, 2003.\n[88] H. Robbins and S. Monro. A stochastic approximation method. The Annals of\nMathematical Statistics, 22(3):400–407, 1951.\n[89] F. Rosenblatt. Principles of neurodynamics: perceptrons and the theory of brain\nmechanisms. Report (Cornell Aeronautical Laboratory). Spartan Books, 1962.\n[90] S. Russell and P. Norvig. Artiﬁcial intelligence: a modern approach. 1995.\n[91] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Net-\nworks, 61:85–117, 2015.\n[92] H. Schwenk. Continuous space language models. Computer Speech & Lan-\nguage, 21(3):492–518, 2007.\n[93] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[94] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Over-\nfeat: Integrated recognition, localization and detection using convolutional net-\nworks. arXiv preprint arXiv:1312.6229, 2013.\n[95] C. Shannon. The bandwagon (edtl.). IRE Transactions on Information Theory,\n1(2):3, 1956.\n[96] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-\nscale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[97] B. F. Skinner. Verbal behavior. BF Skinner Foundation, 2014.\n121\n[98] J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn, C. Callison-Burch, and\nA. Lopez. Dirt cheap web-scale parallel text from the common crawl. In ACL\n(1), pages 1374–1383, 2013.\n[99] M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. A study of trans-\nlation edit rate with targeted human annotation. In Proceedings of association\nfor machine translation in the Americas, pages 223–231, 2006.\n[100] M. Sundermeyer, H. Ney, and R. Schluter. From feedforward to recurrent lstm\nneural networks for language modeling. Audio, Speech, and Language Process-\ning, IEEE/ACM Transactions on, 23(3):517–529, 2015.\n[101] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with\nneural networks. In Advances in neural information processing systems, pages\n3104–3112, 2014.\n[102] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van-\nhoucke, and A. Rabinovich. Going deeper with convolutions. arXiv preprint\narXiv:1409.4842, 2014.\n[103] J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general\nmethod for semi-supervised learning. In Proceedings of the 48th annual meeting\nof the association for computational linguistics, pages 384–394. Association for\nComputational Linguistics, 2010.\n[104] L. van der Maaten and G. E. Hinton. Visualizing data using t-SNE. Journal of\nMachine Learning Research, 9:2579–2605, November 2008.\n[105] V. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag New\nYork, Inc., New York, NY, USA, 1995.\n[106] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image\ncaption generator. arXiv preprint arXiv:1411.4555, 2014.\n[107] T. Wang and K. Cho.\nLarger-context language modelling.\narXiv preprint\narXiv:1511.03729, 2015.\n[108] W. Weaver. Translation. Machine translation of languages, 14:15–23, 1955.\n[109] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: learning to\nrank with joint word-image embeddings. Machine learning, 81(1):21–35, 2010.\n[110] T. Winograd. Understanding natural language. Cognitive psychology, 3(1):1–\n191, 1972.\n[111] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and\nY. Bengio. Show, attend and tell: Neural image caption generation with visual\nattention. In International Conference on Machine Learning, 2015.\n122\n[112] Y. Zhang, K. Wu, J. Gao, and P. Vines. Automatic acquisition of chinese–english\nparallel corpus from the web. In Advances in Information Retrieval, pages 420–\n431. Springer, 2006.\n[113] R. Zhou and E. A. Hansen. Beam-stack search: Integrating backtracking with\nbeam search. In ICAPS, pages 90–98, 2005.\n123\n",
  "categories": [
    "cs.CL",
    "stat.ML"
  ],
  "published": "2015-11-24",
  "updated": "2015-11-24"
}