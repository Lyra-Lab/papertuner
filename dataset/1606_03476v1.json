{
  "id": "http://arxiv.org/abs/1606.03476v1",
  "title": "Generative Adversarial Imitation Learning",
  "authors": [
    "Jonathan Ho",
    "Stefano Ermon"
  ],
  "abstract": "Consider learning a policy from example expert behavior, without interaction\nwith the expert or access to reinforcement signal. One approach is to recover\nthe expert's cost function with inverse reinforcement learning, then extract a\npolicy from that cost function with reinforcement learning. This approach is\nindirect and can be slow. We propose a new general framework for directly\nextracting a policy from data, as if it were obtained by reinforcement learning\nfollowing inverse reinforcement learning. We show that a certain instantiation\nof our framework draws an analogy between imitation learning and generative\nadversarial networks, from which we derive a model-free imitation learning\nalgorithm that obtains significant performance gains over existing model-free\nmethods in imitating complex behaviors in large, high-dimensional environments.",
  "text": "Generative Adversarial Imitation Learning\nJonathan Ho\nStanford University\nhoj@cs.stanford.edu\nStefano Ermon\nStanford University\nermon@cs.stanford.edu\nAbstract\nConsider learning a policy from example expert behavior, without interaction\nwith the expert or access to reinforcement signal. One approach is to recover the\nexpert’s cost function with inverse reinforcement learning, then extract a policy\nfrom that cost function with reinforcement learning. This approach is indirect\nand can be slow. We propose a new general framework for directly extracting a\npolicy from data, as if it were obtained by reinforcement learning following inverse\nreinforcement learning. We show that a certain instantiation of our framework\ndraws an analogy between imitation learning and generative adversarial networks,\nfrom which we derive a model-free imitation learning algorithm that obtains signif-\nicant performance gains over existing model-free methods in imitating complex\nbehaviors in large, high-dimensional environments.\n1\nIntroduction\nWe are interested in a speciﬁc setting of imitation learning—the problem of learning to perform a\ntask from expert demonstrations—in which the learner is given only samples of trajectories from\nthe expert, is not allowed to query the expert for more data while training, and is not provided\nreinforcement signal of any kind. There are two main approaches suitable for this setting: behavioral\ncloning [20], which learns a policy as a supervised learning problem over state-action pairs from\nexpert trajectories; and inverse reinforcement learning [25, 18], which ﬁnds a cost function under\nwhich the expert is uniquely optimal.\nBehavioral cloning, while appealingly simple, only tends to succeed with large amounts of data, due\nto compounding error caused by covariate shift [23, 24]. Inverse reinforcement learning (IRL), on\nthe other hand, learns a cost function that prioritizes entire trajectories over others, so compounding\nerror, a problem for methods that ﬁt single-timestep decisions, is not an issue. Accordingly, IRL has\nsucceeded in a wide range of problems, from predicting behaviors of taxi drivers [31] to planning\nfootsteps for quadruped robots [22].\nUnfortunately, many IRL algorithms are extremely expensive to run, requiring reinforcement learning\nin an inner loop. Scaling IRL methods to large environments has thus been the focus of much\nrecent work [7, 14]. Fundamentally, however, IRL learns a cost function, which explains expert\nbehavior but does not directly tell the learner how to act. Given that learner’s true goal often is to\ntake actions imitating the expert—indeed, many IRL algorithms are evaluated on the quality of the\noptimal actions of the costs they learn—why, then, must we learn a cost function, if doing so possibly\nincurs signiﬁcant computational expense yet fails to directly yield actions?\nWe desire an algorithm that tells us explicitly how to act by directly learning a policy. To develop such\nan algorithm, we begin in Section 3, where we characterize the policy given by running reinforcement\nlearning on a cost function learned by maximum causal entropy IRL [31, 32]. Our characterization\nintroduces a framework for directly learning policies from data, bypassing any intermediate IRL step.\nThen, we instantiate our framework in Sections 4 and 5 with a new model-free imitation learning\nalgorithm. We show that our resulting algorithm is intimately connected to generative adversarial\narXiv:1606.03476v1  [cs.LG]  10 Jun 2016\nnetworks [9], a technique from the deep learning community that has led to recent successes in\nmodeling distributions of natural images: our algorithm harnesses generative adversarial training to ﬁt\ndistributions of states and actions deﬁning expert behavior. We test our algorithm in Section 6, where\nwe ﬁnd that it outperforms competing methods by a wide margin in training policies for complex,\nhigh-dimensional physics-based control tasks over various amounts of expert data.\n2\nBackground\nPreliminaries\nR will denote the extended real numbers R ∪{∞}. Section 3 will work with\nﬁnite state and action spaces S and A to avoid technical machinery out of the scope of this paper\n(concerning compactness of certain sets of functions), but our algorithms and experiments later in the\npaper will run in high-dimensional continuous environments. Π is the set of all stationary stochastic\npolicies that take actions in A given states in S; successor states are drawn from the dynamics model\nP(s′|s, a). We work in the γ-discounted inﬁnite horizon setting, and we will use an expectation\nwith respect a policy π ∈Π to denote an expectation with respect to the trajectory it generates:\nEπ[c(s, a)] ≜E [P∞\nt=0 γtc(st, at)], where s0 ∼p0, at ∼π(·|st), and st+1 ∼P(·|st, at) for t ≥0.\nWe will use ˆEτ to denote empirical expectation with respect to trajectory samples τ, and we will\nalways refer to the expert policy as πE.\nInverse reinforcement learning\nSuppose we are given an expert policy πE that we wish to ratio-\nnalize with IRL. For the remainder of this paper, we will adopt maximum causal entropy IRL [31, 32],\nwhich ﬁts a cost function from a family of functions C with the optimization problem\nmaximize\nc∈C\n\u0012\nmin\nπ∈Π −H(π) + Eπ[c(s, a)]\n\u0013\n−EπE[c(s, a)]\n(1)\nwhere H(π) ≜Eπ[−log π(a|s)] is the γ-discounted causal entropy [3] of the policy π. In practice,\nπE will only be provided as a set of trajectories sampled by executing πE in the environment, so the\nexpected cost of πE in Eq. (1) is estimated using these samples. Maximum causal entropy IRL looks\nfor a cost function c ∈C that assigns low cost to the expert policy and high cost to other policies,\nthereby allowing the expert policy to be found via a certain reinforcement learning procedure:\nRL(c) = arg min\nπ∈Π\n−H(π) + Eπ[c(s, a)]\n(2)\nwhich maps a cost function to high-entropy policies that minimize the expected cumulative cost.\n3\nCharacterizing the induced optimal policy\nTo begin our search for an imitation learning algorithm that both bypasses an intermediate IRL\nstep and is suitable for large environments, we will study policies found by reinforcement learning\non costs learned by IRL on the largest possible set of cost functions C in Eq. (1): all functions\nRS×A = {c : S × A →R}. Using expressive cost function classes, like Gaussian processes [15]\nand neural networks [7], is crucial to properly explain complex expert behavior without meticulously\nhand-crafted features. Here, we investigate the best IRL can do with respect to expressiveness, by\nexamining its capabilities with C = RS×A.\nOf course, with such a large C, IRL can easily overﬁt when provided a ﬁnite dataset. Therefore,\nwe will incorporate a (closed, proper) convex cost function regularizer ψ : RS×A →R into our\nstudy. Note that convexity is a not particularly restrictive requirement: ψ must be convex as a\nfunction deﬁned on all of RS×A, not as a function deﬁned on a small parameter space; indeed, the\ncost regularizers of Finn et al. [7], effective for a range of robotic manipulation tasks, satisfy this\nrequirement. Interestingly, will in fact ﬁnd that ψ plays a central role in our discussion, not a nuisance\nin our analysis.\nNow, let us deﬁne an IRL primitive procedure, which ﬁnds a cost function such that the expert\nperforms better than all other policies, with the cost regularized by ψ:\nIRLψ(πE) = arg max\nc∈RS×A −ψ(c) +\n\u0012\nmin\nπ∈Π −H(π) + Eπ[c(s, a)]\n\u0013\n−EπE[c(s, a)]\n(3)\n2\nNow let ˜c ∈IRLψ(πE). We are interested in a policy given by RL(˜c)—this is the policy given by\nrunning reinforcement learning on the output of IRL.\nTo characterize RL(˜c), it will be useful to transform optimization problems over policies into convex\nproblems. For a policy π ∈Π, deﬁne its occupancy measure ρπ : S × A →R as ρπ(s, a) =\nπ(a|s) P∞\nt=0 γtP(st = s|π). The occupancy measure can be interpreted as the distribution of\nstate-action pairs that an agent encounters when navigating the environment with policy π, and it\nallows us to write Eπ[c(s, a)] = P\ns,a ρπ(s, a)c(s, a) for any cost function c. A basic result [21]\nis that the set of valid occupancy measures D ≜{ρπ : π ∈Π} can be written as a feasible set\nof afﬁne constraints: if p0(s) is the distribution of starting states and P(s′|s, a) is the dynamics\nmodel, then D =\nn\nρ : ρ ≥0\nand\nP\na ρ(s, a) = p0(s) + γ P\ns′,a P(s|s′, a)ρ(s′, a) ∀s ∈S\no\n.\nFurthermore, there is a one-to-one correspondence between Π and D:\nProposition 3.1 (Theorem 2 of Syed et al. [29]). If ρ ∈D, then ρ is the occupancy measure for\nπρ(a|s) ≜ρ(s, a)/ P\na′ ρ(s, a′), and πρ is the only policy whose occupancy measure is ρ.\nWe are therefore justiﬁed in writing πρ to denote the unique policy for an occupancy measure ρ. We\nwill need one more tool: for a function f : RS×A →R, its convex conjugate f ∗: RS×A →R is\ngiven by f ∗(x) = supy∈RS×A xT y −f(y).\nNow, we are ready to characterize RL(˜c), the policy learned by RL on the cost recovered by IRL:\nProposition 3.2. RL ◦IRLψ(πE) = arg minπ∈Π −H(π) + ψ∗(ρπ −ρπE)\n(4)\nThe proof of Proposition 3.2 is in Appendix A.1. The proof relies on the observation that the optimal\ncost function and policy form a saddle point of a certain function. IRL ﬁnds one coordinate of this\nsaddle point, and running reinforcement learning on the output of IRL reveals the other coordinate.\nProposition 3.2 tells us that ψ-regularized inverse reinforcement learning, implicitly, seeks a policy\nwhose occupancy measure is close to the expert’s, as measured by the convex function ψ∗. Enticingly,\nthis suggests that various settings of ψ lead to various imitation learning algorithms that directly solve\nthe optimization problem given by Proposition 3.2. We explore such algorithms in Sections 4 and 5,\nwhere we show that certain settings of ψ lead to both existing algorithms and a novel one.\nThe special case when ψ is a constant function is particularly illuminating, so we state and show it\ndirectly using concepts from convex optimization.\nCorollary 3.2.1. If ψ is a constant function, ˜c ∈IRLψ(πE), and ˜π ∈RL(˜c), then ρ˜π = ρπE.\nIn other words, if there were no cost regularization at all, then the recovered policy will exactly match\nthe expert’s occupancy measure. To show this, we will need a lemma that lets us speak about causal\nentropies of occupancy measures:\nLemma 3.1. Let ¯H(ρ) = −P\ns,a ρ(s, a) log(ρ(s, a)/ P\na′ ρ(s, a′)). Then, ¯H is strictly concave,\nand for all π ∈Π and ρ ∈D, we have H(π) = ¯H(ρπ) and ¯H(ρ) = H(πρ).\nThe proof of this lemma is in Appendix A.1. Proposition 3.1 and Lemma 3.1 together allow us to\nfreely switch between policies and occupancy measures when considering functions involving causal\nentropy and expected costs, as in the following lemma:\nLemma 3.2. If L(π, c) = −H(π) + Eπ[c(s, a)] and ¯L(ρ, c) = −¯H(ρ) + P\ns,a ρ(s, a)c(s, a), then,\nfor all cost functions c, L(π, c) = ¯L(ρπ, c) for all policies π ∈Π, and ¯L(ρ, c) = L(πρ, c) for all\noccupancy measures ρ ∈D.\nNow, we are ready to give a direct proof of Corollary 3.2.1.\nProof of Corollary 3.2.1. Deﬁne ¯L(ρ, c) = −¯H(ρ) + P\ns,a c(s, a)(ρ(s, a) −ρE(s, a)). Given that\nψ is a constant function, we have the following, due to Lemma 3.2:\n˜c ∈IRLψ(πE) = arg max\nc∈RS×A min\nπ∈Π −H(π) + Eπ[c(s, a)] −EπE[c(s, a)] + const.\n(5)\n= arg max\nc∈RS×A min\nρ∈D −¯H(ρ) +\nX\ns,a\nρ(s, a)c(s, a) −\nX\ns,a\nρE(s, a)c(s, a) = arg max\nc∈RS×A min\nρ∈D\n¯L(ρ, c). (6)\n3\nThis is the dual of the optimization problem\nminimize\nρ∈D\n−¯H(ρ)\nsubject to\nρ(s, a) = ρE(s, a)\n∀s ∈S, a ∈A\n(7)\nwith Lagrangian ¯L, for which the costs c(s, a) serve as dual variables for equality constraints. Thus,\n˜c is a dual optimum for (7). Because D is a convex set and −¯H is convex, strong duality holds;\nmoreover, Lemma 3.1 guarantees that −¯H is in fact strictly convex, so the primal optimum can\nbe uniquely recovered from the dual optimum [4, Section 5.5.5] via ˜ρ = arg minρ∈D ¯L(ρ, ˜c) =\narg minρ∈D −¯H(ρ) + P\ns,a ˜c(s, a)ρ(s, a) = ρE, where the ﬁrst equality indicates that ˜ρ is the\nunique minimizer of ¯L(·, ˜c), and the third follows from the constraints in the primal problem (7). But\nif ˜π ∈RL(˜c), then, by Lemma 3.2, its occupancy measure satisﬁes ρ˜π = ˜ρ = ρE.\nFrom this argument, we can deduce the following:\nIRL is a dual of an occupancy measure matching problem, and the recovered cost function is the\ndual optimum. Classic IRL algorithms that solve reinforcement learning repeatedly in an inner loop,\nsuch as the algorithm of Ziebart et al. [31] that runs a variant of value iteration in an inner loop, can be\ninterpreted as a form of dual ascent, in which one repeatedly solves the primal problem (reinforcement\nlearning) with ﬁxed dual values (costs). Dual ascent is effective if solving the unconstrained primal is\nefﬁcient, but in the case of IRL, it amounts to reinforcement learning!\nThe induced optimal policy is the primal optimum. The induced optimal policy is obtained by\nrunning RL after IRL, which is exactly the act of recovering the primal optimum from the dual\noptimum; that is, optimizing the Lagrangian with the dual variables ﬁxed at the dual optimum values.\nStrong duality implies that this induced optimal policy is indeed the primal optimum, and therefore\nmatches occupancy measures with the expert. IRL is traditionally deﬁned as the act of ﬁnding a cost\nfunction such that the expert policy is uniquely optimal, but now, we can alternatively view IRL as a\nprocedure that tries to induce a policy that matches the expert’s occupancy measure.\n4\nPractical occupancy measure matching\nWe saw in Corollary 3.2.1 that if ψ is constant, the resulting primal problem (7) simply matches\noccupancy measures with expert at all states and actions. Such an algorithm, however, is not\npractically useful. In reality, the expert trajectory distribution will be provided only as a ﬁnite set of\nsamples, so in large environments, most of the expert’s occupancy measure values will be exactly\nzero, and exact occupancy measure matching will force the learned policy to never visit these unseen\nstate-action pairs simply due to lack of data. Furthermore, with large environments, we would like to\nuse function approximation to learn a parameterized policy πθ. The resulting optimization problem of\nﬁnding the appropriate θ would have as many constraints as points in S × A, leading to an intractably\nlarge problem and defeating the very purpose of function approximation.\nKeeping in mind that we wish to eventually develop an imitation learning algorithm suitable for large\nenvironments, we would like to relax Eq. (7) into the following form, motivated by Proposition 3.2:\nminimize\nπ\ndψ(ρπ, ρE) −H(π)\n(8)\nby modifying the IRL regularizer ψ so that dψ(ρπ, ρE) ≜ψ∗(ρπ −ρE) smoothly penalizes violations\nin difference between the occupancy measures.\nEntropy-regularized apprenticeship learning\nIt turns out that with certain settings of ψ, Eq. (8)\ntakes on the form of regularized variants of existing apprenticeship learning algorithms, which\nindeed do scale to large environments with parameterized policies [11]. For a class of cost functions\nC ⊂RS×A, an apprenticeship learning algorithm ﬁnds a policy that performs better than the expert\nacross C, by optimizing the objective\nminimize\nπ\nmax\nc∈C Eπ[c(s, a)] −EπE[c(s, a)]\n(9)\nClassic apprenticeship learning algorithms restrict C to convex sets given by linear combinations\nof basis functions f1, . . . , fd, which give rise a feature vector f(s, a) = [f1(s, a), . . . , fd(s, a)] for\neach state-action pair. Abbeel and Ng [1] and Syed et al. [29] use, respectively,\nClinear = {P\niwifi : ∥w∥2 ≤1}\nand\nCconvex = {P\niwifi : P\niwi = 1, wi ≥0 ∀i} .\n(10)\n4\nClinear leads to feature expectation matching [1], which minimizes ℓ2 distance between expected\nfeature vectors: maxc∈Clinear Eπ[c(s, a)]−EπE[c(s, a)] = ∥Eπ[f(s, a)]−EπE[f(s, a)]∥2. Meanwhile,\nCconvex leads to MWAL [28] and LPAL [29], which minimize worst-case excess cost among the\nindividual basis functions, as maxc∈Cconvex Eπ[c(s, a)] −EπE[c(s, a)] = maxi∈{1,...,d} Eπ[fi(s, a)] −\nEπE[fi(s, a)].\nWe now show how Eq. (9) is a special case of Eq. (8) with a certain setting of ψ. With the indicator\nfunction δC : RS×A →R, deﬁned by δC(c) = 0 if c ∈C and +∞otherwise, we can write the\napprenticeship learning objective (9) as\nmax\nc∈C Eπ[c(s, a)]−EπE[c(s, a)] = max\nc∈RS×A−δC(c) +\nX\ns,a\n(ρπ(s, a)−ρπE(s, a))c(s, a) = δ∗\nC(ρπ−ρπE)\nTherefore, we see that entropy-regularized apprenticeship learning\nminimize\nπ\n−H(π) + max\nc∈C Eπ[c(s, a)] −EπE[c(s, a)]\n(11)\nis equivalent to performing RL following IRL with cost regularizer ψ = δC, which forces the implicit\nIRL procedure to recover a cost function lying in C. Note that we can scale the policy’s entropy\nregularization strength in Eq. (11) by scaling C by a constant α as {αc : c ∈C}, recovering the\noriginal apprenticeship objective (9) by taking α →∞.\nCons of apprenticeship learning\nIt is known that apprenticeship learning algorithms generally do\nnot recover expert-like policies if C is too restrictive [29, Section 1]—which is often the case for the\nlinear subspaces used by feature expectation matching, MWAL, and LPAL, unless the basis functions\nf1, . . . , fd are very carefully designed. Intuitively, unless the true expert cost function (assuming it\nexists) lies in C, there is no guarantee that if π performs better than πE on all of C, then π equals πE.\nWith the aforementioned insight based on Proposition 3.2 that apprenticeship learning is equivalent\nto RL following IRL, we can understand exactly why apprenticeship learning may fail to imitate: it\nforces πE to be encoded as an element of C. If C does not include a cost function that explains expert\nbehavior well, then attempting to recover a policy from such an encoding will not succeed.\nPros of apprenticeship learning\nWhile restrictive cost classes C may not lead to exact imitation,\napprenticeship learning with such C can scale to large state and action spaces with policy function\napproximation. Ho et al. [11] rely on the following policy gradient formula for the apprenticeship\nobjective (9) for a parameterized policy πθ:\n∇θ max\nc∈C Eπθ[c(s, a)] −EπE[c(s, a)] = ∇θEπθ[c∗(s, a)] = Eπθ [∇θ log πθ(a|s)Qc∗(s, a)]\nwhere c∗= arg max\nc∈C\nEπθ[c(s, a)] −EπE[c(s, a)], Qc∗(¯s, ¯a) = Eπθ[c∗(¯s, ¯a) | s0 = ¯s, a0 = ¯a] (12)\nObserving that Eq. (12) is the policy gradient for a reinforcement learning objective with cost c∗, Ho\net al. propose an algorithm that alternates between two steps:\n1. Sample trajectories of the current policy πθi by simulating in the environment, and ﬁt a\ncost function c∗\ni , as deﬁned in Eq. (12). For the cost classes Clinear and Cconvex (10), this cost\nﬁtting amounts to evaluating simple analytical expressions [11].\n2. Form a gradient estimate with Eq. (12) with c∗\ni and the sampled trajectories, and take a trust\nregion policy optimization (TRPO) [26] step to produce πθi+1.\nThis algorithm relies crucially on the TRPO policy step, which is a natural gradient step constrained\nto ensure that πθi+1 does not stray too far πθi, as measured by KL divergence between the two\npolicies averaged over the states in the sampled trajectories. This carefully constructed step scheme\nensures that divergence does not occur due to high noise in estimating the gradient (12). We refer the\nreader to Schulman et al. [26] for more details on TRPO.\nWith the TRPO step scheme, Ho et al. were able train large neural network policies for apprentice-\nship learning with linear cost function classes (10) in environments with hundreds of observation\ndimensions. Their use of these linear cost function classes, however, limits their approach to settings\nin which expert behavior is well-described by such classes. We will draw upon their algorithm to\ndevelop an imitation learning method that both scales to large environments and imitates arbitrarily\ncomplex expert behavior. To do so, we ﬁrst turn to proposing a new regularizer ψ that wields more\nexpressive power than the regularizers corresponding to Clinear and Cconvex (10).\n5\n5\nGenerative adversarial imitation learning\nAs discussed in Section 4, the constant regularizer leads to an imitation learning algorithm that exactly\nmatches occupancy measures, but is intractable in large environments. The indicator regularizers\nfor the linear cost function classes (10), on the other hand, lead to algorithms incapable of exactly\nmatching occupancy measures without careful tuning, but are tractable in large environments. We\npropose the following new cost regularizer that combines the best of both worlds, as we will show in\nthe coming sections:\nψGA(c) ≜\n\u001aEπE[g(c(s, a))]\nif c < 0\n+∞\notherwise where g(x) =\n\u001a−x −log(1 −ex)\nif x < 0\n+∞\notherwise\n(13)\nThis regularizer places low penalty on cost functions c that assign an amount of negative cost to\nexpert state-action pairs; if c, however, assigns large costs (close to zero, which is the upper bound\nfor costs feasible for ψGA) to the expert, then ψGA will heavily penalize c. An interesting property of\nψGA is that it is an average over expert data, and therefore can adjust to arbitrary expert datasets. The\nindicator regularizers δC, used by the linear apprenticeship learning algorithms described in Section 4,\nare always ﬁxed, and cannot adapt to data as ψGA can. Perhaps the most important difference between\nψGA and δC, however, is that δC forces costs to lie in a small subspace spanned by ﬁnitely many basis\nfunctions, whereas ψGA allows for any cost function, as long as it is negative everywhere.\nOur choice of ψGA is motivated by the following fact, shown in the appendix (Corollary A.1.1):\nψ∗\nGA(ρπ −ρπE) =\nmax\nD∈(0,1)S×A Eπ[log(D(s, a))] + EπE[log(1 −D(s, a))]\n(14)\nwhere the maximum ranges over discriminative classiﬁers D : S × A →(0, 1). Equation (14) is the\noptimal negative log loss of the binary classiﬁcation problem of distinguishing between state-action\npairs of π and πE. It turns out that this optimal loss is (up to a constant shift) the Jensen-Shannon\ndivergence DJS(ρπ, ρπE) ≜DKL (ρπ∥(ρπ + ρE)/2) + DKL (ρE∥(ρπ + ρE)/2), which is a squared\nmetric between distributions [9, 19]. Treating the causal entropy H as a policy regularizer, controlled\nby λ ≥0, we obtain a new imitation learning algorithm:\nminimize\nπ\nψ∗\nGA(ρπ −ρπE) −λH(π) = DJS(ρπ, ρπE) −λH(π),\n(15)\nwhich ﬁnds a policy whose occupancy measure minimizes Jensen-Shannon divergence to the expert’s.\nEquation (15) minimizes a true metric between occupancy measures, so, unlike linear apprenticeship\nlearning algorithms, it can imitate expert policies exactly.\nAlgorithm\nEquation (15) draws a connection between imitation learning and generative adversarial\nnetworks [9], which train a generative model G by having it confuse a discriminative classiﬁer\nD. The job of D is to distinguish between the distribution of data generated by G and the true\ndata distribution. When D cannot distinguish data generated by G from the true data, then G has\nsuccessfully matched the true data. In our setting, the learner’s occupancy measure ρπ is analogous\nto the data distribution generated by G, and the expert’s occupancy measure ρπE is analogous to the\ntrue data distribution.\nNow, we present a practical algorithm, which we call generative adversarial imitation learning\n(Algorithm 1), for solving Eq. (15) for model-free imitation in large environments. Explicitly, we\nwish to ﬁnd a saddle point (π, D) of the expression\nEπ[log(D(s, a))] + EπE[log(1 −D(s, a))] −λH(π)\n(16)\nTo do so, we ﬁrst introduce function approximation for π and D: we will ﬁt a parameterized policy\nπθ, with weights θ, and a discriminator network Dw : S × A →(0, 1), with weights w. Then, we\nalternate between an Adam [12] gradient step on w to increase Eq. (16) with respect to D, and a\nTRPO step on θ to decrease Eq. (16) with respect to π. The TRPO step serves the same purpose\nas it does with the apprenticeship learning algorithm of Ho et al. [11]: it prevents the policy from\nchanging too much due to noise in the policy gradient. The discriminator network can be interpreted\nas a local cost function providing learning signal to the policy—speciﬁcally, taking a policy step that\ndecreases expected cost with respect to the cost function c(s, a) = log D(s, a) will move toward\nexpert-like regions of state-action space, as classiﬁed by the discriminator. (We derive an estimator\nfor the causal entropy gradient ∇θH(πθ) in Appendix A.2.)\n6\nAlgorithm 1 Generative adversarial imitation learning\n1: Input: Expert trajectories τE ∼πE, initial policy and discriminator parameters θ0, w0\n2: for i = 0, 1, 2, . . . do\n3:\nSample trajectories τi ∼πθi\n4:\nUpdate the discriminator parameters from wi to wi+1 with the gradient\nˆEτi[∇w log(Dw(s, a))] + ˆEτE[∇w log(1 −Dw(s, a))]\n(17)\n5:\nTake a policy step from θi to θi+1, using the TRPO rule with cost function log(Dwi+1(s, a)).\nSpeciﬁcally, take a KL-constrained natural gradient step with\nˆEτi [∇θ log πθ(a|s)Q(s, a)] −λ∇θH(πθ),\nwhere Q(¯s, ¯a) = ˆEτi[log(Dwi+1(s, a)) | s0 = ¯s, a0 = ¯a]\n(18)\n6: end for\n6\nExperiments\nWe evaluated Algorithm 1 against baselines on 9 physics-based control tasks, ranging from low-\ndimensional control tasks from the classic RL literature—the cartpole [2], acrobot [8], and mountain\ncar [17]—to difﬁcult high-dimensional tasks such as a 3D humanoid locomotion, solved only recently\nby model-free reinforcement learning [27, 26]. All environments, other than the classic control tasks,\nwere simulated with MuJoCo [30]. See Appendix B for a complete description of all the tasks.\nEach task comes with a true cost function, deﬁned in the OpenAI Gym [5]. We ﬁrst generated expert\nbehavior for these tasks by running TRPO [26] on these true cost functions to create expert policies.\nThen, to evaluate imitation performance with respect to sample complexity of expert data, we sampled\ndatasets of varying trajectory counts from the expert policies. The trajectories constituting each\ndataset each consisted of about 50 state-action pairs. We tested Algorithm 1 against three baselines:\n1. Behavioral cloning: a given dataset of state-action pairs is split into 70% training data and\n30% validation data. The policy is trained with supervised learning, using Adam [12] with\nminibatches of 128 examples, until validation error stops decreasing.\n2. Feature expectation matching (FEM): the algorithm of Ho et al. [11] using the cost function\nclass Clinear (10) of Abbeel and Ng [1]\n3. Game-theoretic apprenticeship learning (GTAL): the algorithm of Ho et al. [11] using the\ncost function class Cconvex (10) of Syed and Schapire [28]\nWe used all algorithms to train policies of the same neural network architecture for all tasks: two\nhidden layers of 100 units each, with tanh nonlinearities in between. The discriminator networks for\nAlgorithm 1 also used the same architecture. All networks were always initialized randomly at the\nstart of each trial. For each task, we gave FEM, GTAL, and Algorithm 1 exactly the same amount of\nenvironment interaction for training.\nFigure 1 depicts the results, and the tables in Appendix B provide exact performance numbers. We\nfound that on the classic control tasks (cartpole, acrobot, and mountain car), behavioral cloning\nsuffered in expert data efﬁciency compared to FEM and GTAL, which for the most part were able\nproduce policies with near-expert performance with a wide range of dataset sizes. On these tasks,\nour generative adversarial algorithm always produced policies performing better than behavioral\ncloning, FEM, and GTAL. However, behavioral cloning performed excellently on the Reacher task,\non which it was more sample efﬁcient than our algorithm. We were able to slightly improve our\nalgorithm’s performance on Reacher using causal entropy regularization—in the 4-trajectory setting,\nthe improvement from λ = 0 to λ = 10−3 was statistically signiﬁcant over training reruns, according\nto a one-sided Wilcoxon rank-sum test with p = .05. We used no causal entropy regularization for all\nother tasks.\nOn the other MuJoCo environments, we saw a large performance boost for our algorithm over the\nbaselines. Our algorithm almost always achieved at least 70% of expert performance for all dataset\n7\n1\n4\n7\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCartpole\n1\n4\n7\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAcrobot\n1\n4\n7\n10\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMountain Car\n4\n11\n18\n25\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHalfCheetah\n4\n11\n18\n25\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHopper\n4\n11\n18\n25\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWalker\n4\n11\n18\n25\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\nAnt\n80\n160\n240\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHumanoid\nNumber of trajectories in dataset\nPerformance (scaled)\nExpert\nRandom\nBehavioral cloning\nFEM\nGTAL\nOurs\n(a)\n4\n11\n18\n−0.5\n0.0\n0.5\n1.0\nReacher\nNumber of trajectories in dataset\nPerformance (scaled)\nExpert\nRandom\nBehavioral cloning\nOurs (¸ = 0)\nOurs (¸ = 10¡3)\nOurs (¸ = 10¡2)\n(b)\nFigure 1: (a) Performance of learned policies. The y-axis is negative cost, scaled so that the expert\nachieves 1 and a random policy achieves 0. (b) Causal entropy regularization λ on Reacher.\nsizes we tested, nearly always dominating all the baselines. FEM and GTAL performed poorly for\nAnt, producing policies consistently worse than a policy that chooses actions uniformly at random.\nBehavioral cloning was able to reach satisfactory performance with enough data on HalfCheetah,\nHopper, Walker, and Ant; but was unable to achieve more than 60% for Humanoid, on which our\nalgorithm achieved exact expert performance for all tested dataset sizes.\n7\nDiscussion and outlook\nAs we demonstrated, our method is generally quite sample efﬁcient in terms of expert data. However,\nit is not particularly sample efﬁcient in terms of environment interaction during training. The number\nof such samples required to estimate the imitation objective gradient (18) was comparable to the\nnumber needed for TRPO to train the expert policies from reinforcement signals. We believe that we\ncould signiﬁcantly improve learning speed for our algorithm by initializing policy parameters with\nbehavioral cloning, which requires no environment interaction at all.\nFundamentally, our method is model free, so it will generally need more environment interaction than\nmodel-based methods. Guided cost learning [7], for instance, builds upon guided policy search [13]\nand inherits its sample efﬁciency, but also inherits its requirement that the model is well-approximated\nby iteratively ﬁtted time-varying linear dynamics. Interestingly, both our Algorithm 1 and guided cost\nlearning alternate between policy optimization steps and cost ﬁtting (which we called discriminator\nﬁtting), even though the two algorithms are derived completely differently.\nOur approach builds upon a vast line of work on IRL [31, 1, 29, 28], and hence, just like IRL,\nour approach does not interact with the expert during training. Our method explores randomly\nto determine which actions bring a policy’s occupancy measure closer to the expert’s, whereas\nmethods that do interact with the expert, like DAgger [24], can simply ask the expert for such actions.\nUltimately, we believe that a method that combines well-chosen environment models with expert\ninteraction will win in terms of sample complexity of both expert data and environment interaction.\nAcknowledgments\nWe thank Jayesh K. Gupta and John Schulman for assistance and advice. This work was supported\nby the SAIL-Toyota Center for AI Research, and by a NSF Graduate Research Fellowship (grant no.\nDGE-114747).\nReferences\n[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the\n21st International Conference on Machine Learning, 2004.\n[2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difﬁcult\nlearning control problems. Systems, Man and Cybernetics, IEEE Transactions on, (5):834–846, 1983.\n8\n[3] M. Bloem and N. Bambos. Inﬁnite time horizon maximum causal entropy inverse reinforcement learning.\nIn Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on, pages 4911–4916. IEEE, 2014.\n[4] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.\n[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI\nGym. arXiv preprint arXiv:1606.01540, 2016.\n[6] T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.\n[7] C. Finn, S. Levine, and P. Abbeel.\nGuided cost learning: Deep inverse optimal control via policy\noptimization. In Proceedings of the 33rd International Conference on Machine Learning, 2016.\n[8] A. Geramifard, C. Dann, R. H. Klein, W. Dabney, and J. P. How. Rlpy: A value-function-based reinforce-\nment learning framework for education and research. JMLR, 2015.\n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial nets. In NIPS, pages 2672–2680, 2014.\n[10] J.-B. Hiriart-Urruty and C. Lemaréchal. Convex Analysis and Minimization Algorithms, volume 305.\nSpringer, 1996.\n[11] J. Ho, J. K. Gupta, and S. Ermon. Model-free imitation learning with policy optimization. In Proceedings\nof the 33rd International Conference on Machine Learning, 2016.\n[12] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[13] S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown\ndynamics. In Advances in Neural Information Processing Systems, pages 1071–1079, 2014.\n[14] S. Levine and V. Koltun. Continuous inverse optimal control with locally optimal examples. In Proceedings\nof the 29th International Conference on Machine Learning, pages 41–48, 2012.\n[15] S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement learning with gaussian processes.\nIn Advances in Neural Information Processing Systems, pages 19–27, 2011.\n[16] P. W. Millar. The minimax principle in asymptotic statistical theory. In Ecole d’Eté de Probabilités de\nSaint-Flour XI—1981, pages 75–265. Springer, 1983.\n[17] A. W. Moore and T. Hall. Efﬁcient memory-based learning for robot control. 1990.\n[18] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In ICML, 2000.\n[19] X. Nguyen, M. J. Wainwright, and M. I. Jordan. On surrogate loss functions and f-divergences. The Annals\nof Statistics, pages 876–904, 2009.\n[20] D. A. Pomerleau. Efﬁcient training of artiﬁcial neural networks for autonomous navigation. Neural\nComputation, 3(1):88–97, 1991.\n[21] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &\nSons, 2014.\n[22] N. D. Ratliff, D. Silver, and J. A. Bagnell. Learning to search: Functional gradient techniques for imitation\nlearning. Autonomous Robots, 27(1):25–53, 2009.\n[23] S. Ross and D. Bagnell. Efﬁcient reductions for imitation learning. In AISTATS, pages 661–668, 2010.\n[24] S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to\nno-regret online learning. In AISTATS, pages 627–635, 2011.\n[25] S. Russell. Learning agents for uncertain environments. In Proceedings of the Eleventh Annual Conference\non Computational Learning Theory, pages 101–103. ACM, 1998.\n[26] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In\nProceedings of The 32nd International Conference on Machine Learning, pages 1889–1897, 2015.\n[27] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using\ngeneralized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n[28] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural\nInformation Processing Systems, pages 1449–1456, 2007.\n[29] U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In\nProceedings of the 25th International Conference on Machine Learning, pages 1032–1039, 2008.\n[30] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intelligent\nRobots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026–5033. IEEE, 2012.\n[31] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning.\nIn AAAI, AAAI’08, 2008.\n[32] B. D. Ziebart, J. A. Bagnell, and A. K. Dey. Modeling interaction via the principle of maximum causal\nentropy. In ICML, pages 1255–1262, 2010.\n9\nA\nProofs\nA.1\nProofs for Section 3\nProof of Lemma 3.1. First, we show strict concavity of ¯H. Let ρ and ρ′ be occupancy measures, and\nsuppose λ ∈[0, 1]. For all s and a, the log-sum inequality [6] implies:\n−(λρ(s, a) + (1 −λ)ρ′(s, a)) log\nλρ(s, a) + (1 −λ)ρ′(s, a)\nP\na′(λρ(s, a′) + (1 −λ)ρ′(s, a′))\n(19)\n= −(λρ(s, a) + (1 −λ)ρ′(s, a)) log\nλρ(s, a) + (1 −λ)ρ′(s, a)\nλ P\na′ ρ(s, a′) + (1 −λ) P\na′ ρ′(s, a′)\n(20)\n≥−λρ(s, a) log\nλρ(s, a)\nλ P\na′ ρ(s, a′) −(1 −λ)ρ′(s, a) log\n(1 −λ)ρ′(s, a)\n(1 −λ) P\na′ ρ′(s, a′)\n(21)\n= λ\n\u0012\n−ρ(s, a) log\nρ(s, a)\nP\na′ ρ(s, a′)\n\u0013\n+ (1 −λ)\n\u0012\n−ρ′(s, a) log\nρ′(s, a)\nP\na′ ρ′(s, a′)\n\u0013\n,\n(22)\nwith equality if and only if πρ ≜ρ(s, a)/ P\na′ ρ(s, a′) = ρ′(s, a)/ P\na′ ρ′(s, a′) ≜πρ′. Summing\nboth sides over all s and a shows that ¯H(λρ + (1 −λ)ρ′) ≥λ ¯H(ρ) + (1 −λ) ¯H(ρ′) with equality if\nand only if πρ = πρ′. Applying Proposition 3.1 shows that equality in fact holds if and only if ρ = ρ′,\nso ¯H is strictly concave.\nNow, we turn to verifying the last two statements, which also follow from Proposition 3.1 and the\ndeﬁnition of occupancy measures. First,\nH(π) = Eπ[−log π(a|s)]\n(23)\n= −\nX\ns,a\nρπ(s, a) log π(a|s)\n(24)\n= −\nX\ns,a\nρπ(s, a) log\nρπ(s, a)\nP\na′ ρπ(s, a′)\n(25)\n= ¯H(ρπ),\n(26)\nand second,\n¯H(ρ) = −\nX\ns,a\nρ(s, a) log\nρ(s, a)\nP\na′ ρ(s, a′)\n(27)\n= −\nX\ns,a\nρπρ(s, a) log πρ(a|s)\n(28)\n= Eπρ[−log πρ(a|s)]\n(29)\n= H(πρ).\n(30)\nProof of Proposition 3.2. This proof relies on properties of saddle points. For a reference, we refer\nthe reader to Hiriart-Urruty and Lemaréchal [10, section VII.4].\nLet ˜c ∈IRLψ(πE), ˜π ∈RL(˜c) = RL ◦IRLψ(πE), and\nπA ∈arg min\nπ\n−H(π) + ψ∗(ρπ −ρπE)\n(31)\n= arg min\nπ\nmax\nc\n−H(π) −ψ(c) +\nX\ns,a\n(ρπ(s, a) −ρπE(s, a))c(s, a)\n(32)\nWe wish to show that πA = ˜π. To do this, let ρA be the occupancy measure of πA, let ˜ρ be the\noccupancy measure of ˜π, and deﬁne ¯L : D × RS×A →R by\n¯L(ρ, c) = −¯H(ρ) −ψ(c) +\nX\ns,a\nρ(s, a)c(s, a) −\nX\ns,a\nρπE(s, a)c(s, a).\n(33)\n10\nThe following relationships then hold, due to Proposition 3.1:\nρA ∈arg min\nρ∈D\nmax\nc\n¯L(ρ, c),\n(34)\n˜c ∈arg max\nc\nmin\nρ∈D\n¯L(ρ, c),\n(35)\n˜ρ ∈arg min\nρ∈D\n¯L(ρ, ˜c).\n(36)\nNow D is compact and convex and RS×A is convex; furthermore, due to convexity of −¯H and ψ, we\nalso have that ¯L(·, c) is convex for all c, and that ¯L(ρ, ·) is concave for all ρ. Therefore, we can use\nminimax duality [16]:\nmin\nρ∈D max\nc∈C\n¯L(ρ, c) = max\nc∈C min\nρ∈D\n¯L(ρ, c)\n(37)\nHence, from Eqs. (34) and (35), (ρA, ˜c) is a saddle point of ¯L, which implies that\nρA ∈arg min\nρ∈D\n¯L(ρ, ˜c).\n(38)\nBecause ¯L(·, c) is strictly convex for all c (Lemma 3.1), Eqs. (36) and (38) imply ρA = ˜ρ. Since\npolicies corresponding to occupancy measures are unique (Proposition 3.1), we get πA = ˜π.\nA.2\nProofs for Section 5\nIn Eq. (13) of Section 5, we described a cost regularizer ψGA, which leads to an imitation learning\nalgorithm (15) that minimizes Jensen-Shannon divergence between occupancy measures. To justify\nour choice of ψGA, we show how to convert certain surrogate loss functions φ, for binary classiﬁcation\nof state-action pairs drawn from the occupancy measures ρπ and ρπE, into cost function regularizers\nψ, for which ψ∗(ρπ −ρπE) is the minimum expected risk Rφ(ρπ, ρπE) for φ:\nRφ(π, πE) =\nX\ns,a\nmin\nγ∈R ρπ(s, a)φ(γ) + ρπE(s, a)φ(−γ)\n(39)\nSpeciﬁcally, we will restrict ourselves to strictly decreasing convex loss functions. Nguyen et al.\n[19] show a correspondence between minimum expected risks Rφ and f-divergences, of which\nJensen-Shannon divergence is a special case. Our following construction, therefore, can generate any\nimitation learning algorithm that minimizes an f-divergence between occupancy measures, as long\nas that f-divergence is induced by a strictly decreasing convex surrogate φ.\nProposition A.1. Suppose φ : R →R is a strictly decreasing convex function. Let T be the range of\n−φ, and deﬁne gφ : R →R and ψφ : RS×A →R by:\ngφ(x) =\n\u001a−x + φ(−φ−1(−x))\nif x ∈T\n+∞\notherwise\nψφ(c) =\n\n\n\nX\ns,a\nρπE(s, a)gφ(c(s, a))\nif c(s, a) ∈T for all s, a\n+∞\notherwise\n(40)\nThen, ψφ is closed, proper, and convex, and RL ◦IRLψφ(πE) = arg minπ −H(π) −Rφ(ρπ, ρπE).\nProof. To verify the ﬁrst claim, it sufﬁces to check that gφ(x) = −x + φ(−φ−1(−x)) is closed,\nproper, and convex. Convexity follows from the fact that x 7→φ(−φ−1(−x)) is convex, because\nit is a concave function followed by a nonincreasing convex function. Furthermore, because T is\nnonempty, gφ is proper. To show that gφ is closed, note that because φ is strictly decreasing and\nconvex, the range of φ is either all of R or an open interval (b, ∞) for some b ∈R. If the range of\nφ is R, then gφ is ﬁnite everywhere and is therefore closed. On the other hand, if the range of φ is\n(b, ∞), then φ(x) →b as x →∞, and φ(x) →∞as x →−∞. Thus, as x →b, φ−1(−x) →∞,\nso φ(−φ−1(−x)) →∞too, implying that gφ(x) →∞as x →b, which means gφ is closed.\n11\nNow, we verify the second claim. By Proposition 3.2, all we need to check is that −Rφ(ρπ, ρπE) =\nψ∗\nφ(ρπ −ρπE):\nψ∗\nφ(ρπ −ρπE) = max\nc∈C\nX\ns,a\n(ρπ(s, a) −ρπE(s, a))c(s, a) −\nX\ns,a\nρπE(s, a)gφ(c(s, a))\n(41)\n=\nX\ns,a\nmax\nc∈T (ρπ(s, a) −ρπE(s, a))c −ρπE(s, a)[−c + φ(−φ−1(−c))]\n(42)\n=\nX\ns,a\nmax\nc∈T ρπ(s, a)c −ρπE(s, a)φ(−φ−1(−c))\n(43)\n=\nX\ns,a\nmax\nγ∈R ρπ(s, a)(−φ(γ)) −ρπE(s, a)φ(−φ−1(φ(γ)))\n(44)\n=\nX\ns,a\nmax\nγ∈R ρπ(s, a)(−φ(γ)) −ρπE(s, a)φ(−γ)\n(45)\n= −Rφ(ρπ, ρπE)\n(46)\nwhere we made the change of variables c →−φ(γ), justiﬁed because T is the range of −φ.\nHaving showed how to construct a cost function regularizer ψφ from φ, we obtain, as a corollary, a\ncost function regularizer for the logistic loss, whose optimal expected risk is, up to a constant, the\nJensen-Shannon divergence.\nCorollary A.1.1. The cost regularizer (13)\nψGA(c) ≜\n\u001aEπE[g(c(s, a))]\nif c < 0\n+∞\notherwise\nwhere\ng(x) =\n\u001a−x −log(1 −ex)\nif x < 0\n+∞\notherwise\nsatisﬁes\nψ∗\nGA(ρπ −ρπE) =\nmax\nD∈(0,1)S×A Eπ[log(D(s, a))] + EπE[log(1 −D(s, a))].\n(47)\nProof. Using the logistic loss φ(x) = log(1 + e−x), we see that Eq. (40) reduces to the claimed ψGA.\nApplying Proposition A.1, we get\nψ∗\nGA(ρπ −ρπE) = −Rφ(ρπ, ρπE)\n(48)\n=\nX\ns,a\nmax\nγ∈R ρπ(s, a) log\n\u0012\n1\n1 + e−γ\n\u0013\n+ ρπE(s, a) log\n\u0012\n1\n1 + eγ\n\u0013\n(49)\n=\nX\ns,a\nmax\nγ∈R ρπ(s, a) log\n\u0012\n1\n1 + e−γ\n\u0013\n+ ρπE(s, a) log\n\u0012\n1 −\n1\n1 + e−γ\n\u0013\n(50)\n=\nX\ns,a\nmax\nγ∈R ρπ(s, a) log(σ(γ)) + ρπE(s, a) log(1 −σ(γ)),\n(51)\nwhere σ(x) = 1/(1 + e−x) is the sigmoid function. Because the range of σ is (0, 1), we can write\nψ∗\nGA(ρπ −ρπE) =\nX\ns,a\nmax\nd∈(0,1) ρπ(s, a) log d + ρπE(s, a) log(1 −d)\n(52)\n=\nmax\nD∈(0,1)S×A\nX\ns,a\nρπ(s, a) log(D(s, a)) + ρπE(s, a) log(1 −D(s, a)),\n(53)\nwhich is the desired expression.\nWe conclude with a policy gradient formula for causal entropy.\nLemma A.1. The causal entropy gradient is given by\n∇θEπθ[−log πθ(a|s)] = Eπθ [∇θ log πθ(a|s)Qlog(s, a)] ,\nwhere\nQlog(¯s, ¯a) = Eπθ[−log πθ(a|s) | s0 = ¯s, a0 = ¯a].\n(54)\n12\nProof. For an occupancy measure ρ(s, a), deﬁne ρ(s) = P\na ρ(s, a). Next,\n∇θEπθ[−log πθ(a|s)] = −∇θ\nX\ns,a\nρπθ(s, a) log πθ(a|s)\n= −\nX\ns,a\n(∇θρπθ(s, a)) log πθ(a|s) −\nX\ns\nρπθ(s)\nX\na\nπθ(a|s)∇θ log πθ(a|s)\n= −\nX\ns,a\n(∇θρπθ(s, a)) log πθ(a|s) −\nX\ns\nρπθ(s)\nX\na\n∇θπθ(a|s)\nThe second term vanishes, because P\na ∇θπθ(a|s) = ∇θ\nP\na πθ(a|s) = ∇θ1 = 0. We are left with\n∇θEπθ[−log πθ(a|s)] =\nX\ns,a\n(∇θρπθ(s, a))(−log πθ(a|s)),\nwhich is the policy gradient for RL with the ﬁxed cost function clog(s, a) ≜−log πθ(a|s). The\nresulting formula is given by the standard policy gradient formula for clog.\nB\nEnvironments and detailed results\nThe environments we used for our experiments are from the OpenAI Gym [5]. The names and version\nnumbers of these environments are listed in Table 1, which also lists dimension or cardinality of their\nobservation and action spaces (numbers marked “continuous” indicate dimension for a continuous\nspace, and numbers marked “discrete” indicate cardinality for a ﬁnite space).\nTable 1: Environments\nTask\nObservation space\nAction space\nRandom policy performance\nExpert performance\nCartpole-v0\n4 (continuous)\n2 (discrete)\n18.64 ± 7.45\n200.00 ± 0.00\nAcrobot-v0\n4 (continuous)\n3 (discrete)\n−200.00 ± 0.00\n−75.25 ± 10.94\nMountain Car-v0\n2 (continuous)\n3 (discrete)\n−200.00 ± 0.00\n−98.75 ± 8.71\nReacher-v1\n11 (continuous)\n2 (continuous)\n−43.21 ± 4.32\n−4.09 ± 1.70\nHalfCheetah-v1\n17 (continuous)\n6 (continuous)\n−282.43 ± 79.53\n4463.46 ± 105.83\nHopper-v1\n11 (continuous)\n3 (continuous)\n14.47 ± 7.96\n3571.38 ± 184.20\nWalker-v1\n17 (continuous)\n6 (continuous)\n0.57 ± 4.59\n6717.08 ± 845.62\nAnt-v1\n111 (continuous)\n8 (continuous)\n−69.68 ± 111.10\n4228.37 ± 424.16\nHumanoid-v1\n376 (continuous)\n17 (continuous)\n122.87 ± 35.11\n9575.40 ± 1750.80\nThe amount of environment interaction used for FEM, GTAL, and our algorithm is shown in Table 2.\nTo reduce gradient variance for these three algorithms, we also ﬁt value functions, with the same\nneural network architecture as the policies, and employed generalized advantage estimation [27]\n(with γ = .995 and λ = .97). The exact experimental results are listed in Table 3. Means and\nstandard deviations are computed over 50 trajectories. For the cartpole, mountain car, acrobot, and\nreacher, these statistics are further computed over 7 policies learned from random initializations.\nTable 2: Parameters for FEM, GTAL, and Algorithm 1\nTask\nTraining iterations\nState-action pairs per iteration\nCartpole\n300\n5000\nMountain Car\n300\n5000\nAcrobot\n300\n5000\nReacher\n200\n50000\nHalfCheetah\n500\n50000\nHopper\n500\n50000\nWalker\n500\n50000\nAnt\n500\n50000\nHumanoid\n1500\n50000\n13\nTable 3: Learned policy performance\nTask\nDataset size\nBehavioral cloning\nFEM\nGTAL\nOurs\nCartpole\n1\n72.02 ± 35.82\n200.00 ± 0.00\n200.00 ± 0.00\n200.00 ± 0.00\n4\n169.18 ± 59.81\n200.00 ± 0.00\n200.00 ± 0.00\n200.00 ± 0.00\n7\n188.60 ± 29.61\n200.00 ± 0.00\n199.94 ± 1.14\n200.00 ± 0.00\n10\n177.19 ± 52.83\n199.75 ± 3.50\n200.00 ± 0.00\n200.00 ± 0.00\nAcrobot\n1\n−130.60 ± 55.08\n−133.14 ± 60.80\n−81.35 ± 22.40\n−77.26 ± 18.03\n4\n−93.20 ± 32.58\n−94.21 ± 47.20\n−94.80 ± 46.08\n−83.12 ± 23.31\n7\n−96.92 ± 34.51\n−95.08 ± 46.67\n−95.75 ± 46.57\n−82.56 ± 20.95\n10\n−95.09 ± 33.33\n−77.22 ± 18.51\n−94.32 ± 46.51\n−78.91 ± 15.76\nMountain Car\n1\n−136.76 ± 34.44\n−100.97 ± 12.54\n−115.48 ± 36.35\n−101.55 ± 10.32\n4\n−133.25 ± 29.97\n−99.29 ± 8.33\n−143.58 ± 50.08\n−101.35 ± 10.63\n7\n−127.34 ± 29.15\n−100.65 ± 9.36\n−128.96 ± 46.13\n−99.90 ± 7.97\n10\n−123.14 ± 28.26\n−100.48 ± 8.14\n−120.05 ± 36.66\n−100.83 ± 11.40\nHalfCheetah\n4\n−493.62 ± 246.58\n734.01 ± 84.59\n1008.14 ± 280.42\n4515.70 ± 549.49\n11\n637.57 ± 1708.10\n−375.22 ± 291.13\n226.06 ± 307.87\n4280.65 ± 1119.93\n18\n2705.01 ± 2273.00\n343.58 ± 159.66\n1084.26 ± 317.02\n4749.43 ± 149.04\n25\n3718.58 ± 1856.22\n502.29 ± 375.78\n869.55 ± 447.90\n4840.07 ± 95.36\nHopper\n4\n50.57 ± 0.95\n3571.98 ± 6.35\n3065.21 ± 147.79\n3614.22 ± 7.17\n11\n1025.84 ± 266.86\n3572.30 ± 12.03\n3502.71 ± 14.54\n3615.00 ± 4.32\n18\n1949.09 ± 500.61\n3230.68 ± 4.58\n3201.05 ± 6.74\n3600.70 ± 4.24\n25\n3383.96 ± 657.61\n3331.05 ± 3.55\n3458.82 ± 5.40\n3560.85 ± 3.09\nWalker\n4\n32.18 ± 1.25\n3648.17 ± 327.41\n4945.90 ± 65.97\n4877.98 ± 2848.37\n11\n5946.81 ± 1733.73\n4723.44 ± 117.18\n6139.29 ± 91.48\n6850.27 ± 39.19\n18\n1263.82 ± 1347.74\n4184.34 ± 485.54\n5288.68 ± 37.29\n6964.68 ± 46.30\n25\n1599.36 ± 1456.59\n4368.15 ± 267.17\n4687.80 ± 186.22\n6832.01 ± 254.64\nAnt\n4\n1611.75 ± 359.54\n−2052.51 ± 49.41\n−5743.81 ± 723.48\n3186.80 ± 903.57\n11\n3065.59 ± 635.19\n−4462.70 ± 53.84\n−6252.19 ± 409.42\n3306.67 ± 988.39\n18\n2597.22 ± 1366.57\n−5148.62 ± 37.80\n−3067.07 ± 177.20\n3033.87 ± 1460.96\n25\n3235.73 ± 1186.38\n−5122.12 ± 703.19\n−3271.37 ± 226.66\n4132.90 ± 878.67\nHumanoid\n80\n1397.06 ± 1057.84\n5093.12 ± 583.11\n5096.43 ± 24.96\n10200.73 ± 1324.47\n160\n3655.14 ± 3714.28\n5120.52 ± 17.07\n5412.47 ± 19.53\n10119.80 ± 1254.73\n240\n5660.53 ± 3600.70\n5192.34 ± 24.59\n5145.94 ± 21.13\n10361.94 ± 61.28\nTask\nDataset size\nBehavioral cloning\nOurs (λ = 0)\nOurs (λ = 10−3)\nOurs (λ = 10−2)\nReacher\n4\n−10.97 ± 7.07\n−67.23 ± 88.99\n−32.37 ± 39.81\n−46.72 ± 82.88\n11\n−6.23 ± 3.29\n−6.06 ± 5.36\n−6.61 ± 5.11\n−9.26 ± 21.88\n18\n−4.76 ± 2.31\n−8.25 ± 21.99\n−5.66 ± 3.15\n−5.04 ± 2.22\n14\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2016-06-10",
  "updated": "2016-06-10"
}