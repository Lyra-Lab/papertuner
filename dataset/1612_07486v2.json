{
  "id": "http://arxiv.org/abs/1612.07486v2",
  "title": "Continuous multilinguality with language vectors",
  "authors": [
    "Robert Östling",
    "Jörg Tiedemann"
  ],
  "abstract": "Most existing models for multilingual natural language processing (NLP) treat\nlanguage as a discrete category, and make predictions for either one language\nor the other. In contrast, we propose using continuous vector representations\nof language. We show that these can be learned efficiently with a\ncharacter-based neural language model, and used to improve inference about\nlanguage varieties not seen during training. In experiments with 1303 Bible\ntranslations into 990 different languages, we empirically explore the capacity\nof multilingual language models, and also show that the language vectors\ncapture genetic relationships between languages.",
  "text": "Continuous multilinguality with language vectors\nRobert Östling\nDepartment of Linguistics∗\nStockholm University\nrobert@ling.su.se\nJörg Tiedemann\nDepartment of Modern Languages\nUniversity of Helsinki\njorg.tiedemann@helsinki.fi\nAbstract\nMost existing models for multilingual nat-\nural language processing (NLP) treat lan-\nguage as a discrete category, and make\npredictions for either one language or the\nother.\nIn contrast, we propose using\ncontinuous vector representations of lan-\nguage. We show that these can be learned\nefﬁciently with a character-based neural\nlanguage model, and used to improve in-\nference about language varieties not seen\nduring training. In experiments with 1303\nBible translations into 990 different lan-\nguages, we empirically explore the ca-\npacity of multilingual language models,\nand also show that the language vectors\ncapture genetic relationships between lan-\nguages.\n1\nIntroduction\nNeural language models (Bengio et al., 2003;\nMikolov et al., 2010; Sundermeyer et al., 2012)\nhave become an essential component in several ar-\neas of natural language processing (NLP), such as\nmachine translation, speech recognition and im-\nage captioning. They have also become a common\nbenchmarking application in machine learning re-\nsearch on recurrent neural networks (RNN), be-\ncause producing an accurate probabilistic model\nof human language is a very challenging task\nwhich requires all levels of linguistic analysis,\nfrom pragmatics to phonology, to be taken into ac-\ncount.\nA typical language model is trained on text in\na single language, and if one needs to model mul-\ntiple languages the standard solution is to train a\n∗Work done while the author was at the University of\nHelsinki\nseparate model for each language. This presup-\nposes large quantities of monolingual data in each\nof the languages that needs to be covered and each\nmodel with its parameters is completely indepen-\ndent of any of the other models.\nWe propose instead to use a single model with\nreal-valued vectors to indicate the language used,\nand to train this model with a large number of\nlanguages. We thus get a language model whose\npredictive distribution p(xt|x1...t−1, l) is a contin-\nuous function of the language vector l, a property\nthat is trivially extended to other neural NLP mod-\nels. In this paper, we explore the “language space”\ncontaining these vectors, and in particular explore\nwhat happens when we move beyond the points\nrepresenting the languages of the training corpus.\nThe motivation of combining languages into\none single model is at least two-fold: First of all,\nlanguages are related and share many features and\nproperties, a fact that is ignored when using inde-\npendent models. The second motivation is data\nsparseness, an issue that heavily inﬂuences the\nreliability of data-driven models. Resources are\nscarce for most languages in the world (and also\nfor most domains in otherwise well-supported lan-\nguages), which makes it hard to train reasonable\nparameters. By combining data from many lan-\nguages, we hope to mitigate this issue.\nIn contrast to related work, we focus on mas-\nsively multilingual data sets to cover for the ﬁrst\ntime a substantial amount of the linguistic diver-\nsity in the world in a project related to data-driven\nlanguage modeling. We do not presuppose any\nprior knowledge about language similarities and\nevolution and let the model discover relations on\nits own purely by looking at the data. The only\nsupervision that is giving during training is a lan-\nguage identiﬁer as a one-hot encoding. From that\nand the actual training examples, the system learns\ndense vector representations for each language in-\narXiv:1612.07486v2  [cs.CL]  19 Mar 2017\ncluded in our data set along with the character-\nlevel RNN parameters of the language model it-\nself.\n2\nRelated Work\nMultilingual language models is not a new idea\n(Fugen et al., 2003), the novelty of our work lies\nprimarily in the use of language vectors and the\nempirical evaluation using nearly a thousand lan-\nguages.\nConcurrent with this work, Johnson et al. (2016)\nconducted a study using neural machine transla-\ntion (NMT), where a sub-word decoder is told\nwhich language to generate by means of a special\nlanguage identiﬁer token in the source sentence.\nThis is close to our model, although beyond a sim-\nple interpolation experiment (as in our Section 5.3)\nthey did not further explore the language vectors,\nwhich would have been challenging to do given\nthe small number of languages used in their study.\nAmmar et al. (2016) used one-hot language\nidentiﬁers as input to a multilingual word-based\ndependency parser, based on multilingual word\nembeddings.\nGiven that they report this result-\ning in higher accuracy than using features from a\ntypological database, it is a reasonable guess that\ntheir system learned language vectors which were\nable to encode syntactic properties relevant to the\ntask. Unfortunately, they also did not look closer\nat the language vector space, which would have\nbeen interesting given the relatively large and di-\nverse sample of languages represented in the Uni-\nversal Dependencies treebanks.\nOur evaluation in Section 5.2 calls to mind pre-\nvious work on automatic language classiﬁcation,\nby Wichmann et al. (2010) among others. How-\never, our purpose is not to detect genealogical re-\nlationships, even though we use the strong correla-\ntion between such classiﬁcations and our language\nvectors as evidence that the vector space captures\nsensible information about languages.\n3\nData\nWe base our experiments on a large collection of\nBible translations crawled from the web, coming\nfrom various sources and periods of times. Any\nother multilingual data collection would work as\nwell, but with the selected corpus we have the ad-\nvantage that we cover the same genre and roughly\nthe same coverage for each language involved. It\nis also easy to divide the data into training and test\nLanguage vector\nh\ne\nl\nl\no\nh\ne\nl\nl\n<s>\n64\n64\n64\nFigure 1: Schematic of our model. The three parts\nof the language vector are concatenated with the\ninputs to the two LSTM:s and the ﬁnal softmax\nlayer.\nsets by using Bible verse numbers, which allows\nus to control for semantic similarity between lan-\nguages in a way that would have been difﬁcult in\na corpus that is not multi-parallel. Altogether we\nhave 1,303 translations in 990 languages that we\ncan use for our purposes. These were chosen so\nthat the model alphabet size is below 1000 sym-\nbols, which was satisﬁed by choosing only trans-\nlations in Latin, Cyrillic or Greek script.\nCertainly, there are disadvantages as well, such\nas the limited size (roughly 500 million tokens in\ntotal, with most languages having only one trans-\nlation of the New Testament each, with roughly\n200 thousand tokens), the narrow domain and the\nhigh overlap of named entities. The latter can lead\nto some unexpected effects when using nonsensi-\ncal language vectors, as the model will then gen-\nerate a sequence of random names.\nThe corpus deviates in some ways from an\nideal multi-parallel corpus. Most translations are\nof the complete New Testament, whereas around\n300 also contain the Old Testament (thus several\ntimes longer), and around ten contain only por-\ntions of the New Testament. Additionally, several\nlanguages have multiple translations, which are\nthen concatenated. These translations may vary in\nage and style, but historical versions of languages\n(with their own ISO 639-3 code) are treated as dis-\ntinct languages. During training we enforce a uni-\nform distribution between languages when select-\ning training examples.\n4\nMethods\nOur model is based on a standard stacked\ncharacter-based LSTM (Hochreiter and Schmid-\nhuber, 1997) with two layers, followed by a hid-\nden layer and a ﬁnal output layer with softmax ac-\ntivations. The only modiﬁcation made to accom-\nmodate the fact that we train the model with text\nin nearly a thousand languages, rather than one,\nis that language embedding vectors are concate-\nnated to the inputs of the LSTMs at each time step\nand the hidden layer before the softmax. We used\nthree separate embeddings for these levels, in an\nattempt to capture different types of information\nabout languages.1 The model structure is summa-\nrized in Figure 1.\nIn our experiments we use 1024-dimensional\nLSTMs, 128-dimensional character embeddings,\nand 64-dimensional language embeddings. Layer\nnormalization (Ba et al., 2016) is used, but no\ndropout or other regularization since the amount\nof data is very large (about 3 billion characters)\nand training examples are seen at most twice. For\nsmaller models early stopping is used.\nWe use\nAdam (Kingma and Ba, 2015) for optimization.\nTraining takes between an hour and a few days on\na K40 GPU, depending on the data size.\n5\nResults\nIn this section, we present several experiments\nwith the model described. For exploring the lan-\nguage vector space, we use hierarchical agglomer-\native clustering for visualization. For measuring\nperformance, we use cross-entropy on held out-\ndata. For this, we use a set of the 128 most com-\nmonly translated Bible verses, to ensure that the\nheld-out set is as large and overlapping as possible\namong languages.\n5.1\nModel capacity\nOur ﬁrst experiment tries to answer what happens\nwhen more and more languages are added to the\nmodel. There are two settings: adding languages\nin a random order, or adding the most closely re-\nlated languages ﬁrst. Cross-entropy plots for these\nsettings are shown in Figure 2 and Figure 3.\nIn both cases, the model degrades gracefully (or\neven improves) for a number of languages, but\nthen degrades linearly (i.e. exponential growth of\n1The embeddings at the different levels are different, but\nwe did not see any systematic variation. We also found that\nusing the same embedding everywhere gives similar results.\n 0.8\n 1\n 1.2\n 1.4\n 1.6\n 1.8\n 2\n 2.2\n 1\n 10\n 100\n 1000\nTest set cross-entropy\nNumber of languages\nChayahuita (Cahuapanan, Peru)\nKonkomba (Gur, Ghana/Togo)\nTawala (Oceanic, Papua New Guinea)\nBenabena (Goroka, Papua New Guinea))\nFigure 2: Cross-entropy of the test sets from the\nﬁrst four languages added to our model. At the\nleftmost point (x = 1), only Chayahuita is used\nfor training the model so no results are available\nfor the other languages.\n 1\n 1.2\n 1.4\n 1.6\n 1.8\n 2\n 2.2\n 2.4\n 2.6\n 1\n 10\n 100\n 1000\nTest set cross-entropy\nNumber of languages\nDanish\nNorwegian (Nynorsk)\nNorwegian (Bokmal)\nSwedish\nFigure 3:\nCross-entropy of the test sets from\nScandinavian languages. The languages added at\neach step are: Swedish, Norwegian+Danish, Ice-\nlandic+Faroese, remaining Germanic, remaining\nIndo-European, all remaining languages.\n 1\n 1.2\n 1.4\n 1.6\n 1.8\n 2\n 2.2\n 2.4\n 2.6\n 1\n 10\n 100\n 1000\nTest set cross-entropy (Swedish)\nParameter reduction factor, Number of languages\ntotal parameters\nLSTM parameters\nlanguages\nFigure 4: Cross-entropy of the Swedish test set,\ngiven two conditions: increasing number of lan-\nguages by the given factor (adding the most sim-\nilar languages ﬁrst) or decreasing number of pa-\nrameters by the same factor (for a monolingual\nmodel, which is why the curves meet at x = 1).\nperplexity) with exponentially increasing number\nof languages.\nFor comparison, Figure 4 compares this to the\neffect of decreasing the number of parameters in\nthe LSTM by successively halving the hidden state\nsize.2 Here the behavior is similar, but unlike the\nSwedish model which got somewhat better when\nclosely related languages were added, the increase\nin cross-entropy is monotone. It would be inter-\nesting to investigate how the number of model\nparameters needs to be scaled up in order to ac-\ncommodate the additional languages, but unfortu-\nnately the computational resources for such an ex-\nperiment increases with the number of languages\nand would not be practical to carry out with our\ncurrent equipment.\n5.2\nStructure of the language space\nWe now take a look at the language vectors found\nduring training with the full model of 990 lan-\nguages. Figure 5 shows a hierarchical clustering of\nthe subset of Germanic languages, which closely\nmatches the established genetic relationships in\nthis language family. While our experiments in-\ndicate that ﬁnding more remote relationships (say,\nconnecting the Germanic languages to the Celtic)\nis difﬁcult for the model, it is clear that the lan-\nguage vectors preserves similarity properties be-\ntween languages.\nIn additional experiments we found the overall\nstructure of these clusterings to be relatively sta-\nble across models, but for very similar languages\n(such as Danish and the two varieties of Norwe-\ngian) the hierarchy might differ, and the some\nholds for languages or groups that are signiﬁcantly\ndifferent from the major groups. An example from\nFigure 5 is English, which is traditionally clas-\nsiﬁed as a West Germanic language with strong\ninﬂuences from North Germanic as well as Ro-\nmance languages. In the ﬁgure English is (weakly)\ngrouped with the West Germanic languages, but\nin other experiments it is instead weakly grouped\nwith North Germanic.\n5.3\nGenerating Text\nSince our language model is conditioned on a lan-\nguage vector, we can gain some intuitive under-\nstanding of the language space by generating text\nfrom different points in it. These points could be\n2Note that two curves are given, one counting all model\nparameters and one counting only the LSTM parameters. The\nlatter dominates the model size for large hidden states.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFaroese\nIcelandic\nSwedish\nNynorsk\nDanish\nBokmål\nEnglish\nM. English\nFrisian\nGerman\nAfrikaans\nDutch\nFigure 5: Hierarchical clustering of language vec-\ntors of Germanic languages.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(1-x) English  +  x German\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nCross-entropy (King James Version, English)\nFigure 6: Cross-entropy of interpolated language\nmodels for English and German measured on En-\nglish held-out text.\neither one of the vectors learned during training,\nor some arbitrary other point. Table 1 shows text\nsamples from different points along the line be-\ntween Modern English [eng] and Middle English\n[enm]. Consistent with the results of Johnson et al.\n(2016), it appears that the interesting region lies\nrather close to 0.5. Compare also to our Figure 6,\nwhich shows that up until about a third of the way\nbetween English and German, the language model\nis nearly perfectly tuned to English.\n5.4\nMixing and Interpolating Between\nLanguages\nBy means of cross-entropy, we can also visualize\nthe relation between languages in the multilingual\nspace. Figure 6 plots the interpolation results for\ntwo relatively dissimilar languages, English and\nGerman. As expected, once the language vector\nmoves too close to the German one, model perfor-\nmance drops drastically.\nMore interesting results can be obtained if\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(1-x) Modern English  +  x Middle English\n1.26\n1.28\n1.30\n1.32\n1.34\n1.36\n1.38\n1.40\nCross-entropy (King James Version)\nFigure 7: Cross-entropy of interpolated language\nmodels for modern and middle English tested on\ndata from the King James Bible.\nwe interpolate between two language variants\nand compute cross-entropy of a text that repre-\nsents an intermediate form. Figure 7 shows the\ncross-entropy of the King James Version of the\nBible (published 1611), when interpolating be-\ntween Modern English (1500–) and Middle En-\nglish (1050–1500). The optimal point turns out\nto be close to the midway point between them.\n5.5\nLanguage identiﬁcation\nIf we have a sample of an unknown language or\nlanguage variant, it is possible to estimate its lan-\nguage vector by backpropagating through the lan-\nguage model with all parameters except the lan-\nguage vector ﬁxed.3 We found that a very small set\nof sentences is enough to give a considerable im-\nprovement in cross-entropy on held-out sentences.\nIn this experiment, we used 32 sentences from the\nKing James Version of the Bible. Using the re-\nsulting language vector, test set cross-entropy im-\nproved from 1.39 (using the Modern English lan-\nguage vector as initial value) to 1.35. This is com-\nparable to the result obtained in Section 5.4, ex-\ncept that here we do not restrict the search space\nto points on a straight line between two language\nvectors.\n6\nConclusions\nWe have shown that language vectors, dense vec-\ntor representations of natural languages, can be\n3In practice, using error backpropagation is too computa-\ntionally expensive for most applications, and we use it here\nbecause it requires only minimal modiﬁcations to our model.\nA more reasonable method could be to train a separate lan-\nguage vector encoder network.\nTable 1: Examples generated by interpolating be-\ntween Modern English and Middle English.\n%\nRandom sample\n(temperature parameter τ = 0.5)\n30\nand thei schulen go in to alle these thingis, and\nschalt endure bothe in the weie\n40\nand there was a certaine other person who was called\nin a dreame that he went into a mountaine.\n44\nand the second sacriﬁce, and the father, and the\nprophet, shall be given to it.\n48\nand god sayd, i am the light of the world, and the\npowers of the enemies of the most high god may\nﬁnd ﬁrst for many.\n50\nbut if there be some of the seruants, and to all the\npeople, and the angels of god, and the prophets\n52\nthen he came to the gate of the city, and the bread\nwas to be brought\n56\ntherefore, behold, i will lose the sound of my soul,\nand i will not ﬁght it into the land of egypt\n60\nand the man whom the son of man is born of god,\nso have i therefore already sent to the good news of\nchrist.\nlearned efﬁciently from raw text and possess sev-\neral interesting properties. First, they capture lan-\nguage similarity to the extent that language family\ntrees can be reconstructed by clustering the vec-\ntors. Second, they allow us to interpolate between\nlanguages in a sensible way, and even allow adopt-\ning the model using a very small set of text, simply\nby optimizing the language vector.\nReferences\n[Ammar et al.2016] Waleed Ammar, George Mulcaire,\nMiguel Ballesteros, Chris Dyer, and Noah Smith.\n2016. Many languages, one parser. Transactions\nof the Association for Computational Linguistics,\n4:431–444.\n[Ba et al.2016] Jimmy Lei Ba, Jamie Ryan Kiros, and\nGeoffrey E. Hinton.\n2016.\nLayer normalization.\nArXiv e-prints, July.\n[Bengio et al.2003] Yoshua Bengio, Réjean Ducharme,\nPascal Vincent, and Christian Janvin. 2003. A neu-\nral probabilistic language model.\nJournal of Ma-\nchine Learning Research, 3:1137–1155, March.\n[Fugen et al.2003] Christian Fugen, Sebastian Stuker,\nHagen Soltau, Florian Metze, and Tanja Schultz.\n2003. Efﬁcient handling of multilingual language\nmodels.\nIn 2003 IEEE Workshop on Automatic\nSpeech Recognition and Understanding, pages 441–\n446, Nov.\n[Hochreiter and Schmidhuber1997] Sepp\nHochreiter\nand Jürgen Schmidhuber. 1997. Long short-term\nmemory.\nNeural Computation, 9(8):1735–1780.\ndoi: 10.1162/neco.1997.9.8.1735.\n[Johnson et al.2016] Melvin Johnson, Mike Schuster,\nQuoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng\nChen, Nikhil Thorat, Fernanda B. Viégas, Martin\nWattenberg, Greg Corrado, Macduff Hughes, and\nJeffrey Dean.\n2016.\nGoogle’s multilingual neu-\nral machine translation system: Enabling zero-shot\ntranslation. CoRR, abs/1611.04558.\n[Kingma and Ba2015] Diederik P. Kingma and Jimmy\nBa.\n2015.\nAdam: A method for stochastic opti-\nmization. The International Conference on Learning\nRepresentations.\n[Mikolov et al.2010] Tomáš Mikolov, Martin Karaﬁát,\nLukáš Burget, Jan ˇCernocký, and Sanjeev Khu-\ndanpur.\n2010.\nRecurrent neural network based\nlanguage model.\nIn INTERSPEECH 2010, pages\n1045–1048.\n[Sundermeyer et al.2012] Martin\nSundermeyer,\nRalf\nSchlüter, and Hermann Ney.\n2012.\nLSTM neu-\nral networks for language modeling.\nIn INTER-\nSPEECH 2012, pages 194–197.\n[Wichmann et al.2010] Søren Wichmann, Eric W. Hol-\nman, Dik Bakker, and Cecil H. Brown.\n2010.\nEvaluating linguistic distance measures.\nPhys-\nica A: Statistical Mechanics and its Applications,\n389(17):3632 – 3639.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2016-12-22",
  "updated": "2017-03-19"
}