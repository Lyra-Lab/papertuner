{
  "id": "http://arxiv.org/abs/1612.09030v2",
  "title": "Meta-Unsupervised-Learning: A supervised approach to unsupervised learning",
  "authors": [
    "Vikas K. Garg",
    "Adam Tauman Kalai"
  ],
  "abstract": "We introduce a new paradigm to investigate unsupervised learning, reducing\nunsupervised learning to supervised learning. Specifically, we mitigate the\nsubjectivity in unsupervised decision-making by leveraging knowledge acquired\nfrom prior, possibly heterogeneous, supervised learning tasks. We demonstrate\nthe versatility of our framework via comprehensive expositions and detailed\nexperiments on several unsupervised problems such as (a) clustering, (b)\noutlier detection, and (c) similarity prediction under a common umbrella of\nmeta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to\nestablish the theoretical foundations of our framework, and show that our\nframing of meta-clustering circumvents Kleinberg's impossibility theorem for\nclustering.",
  "text": "Meta-Unsupervised-Learning:\nA supervised approach to unsupervised learning\nVikas K. Garg and Adam Tauman Kalai\nJanuary 3, 2017\nAbstract\nWe introduce a new paradigm to investigate unsupervised learning,\nreducing unsupervised learning to supervised learning.\nSpeciﬁcally, we\nmitigate the subjectivity in unsupervised decision-making by leveraging\nknowledge acquired from prior, possibly heterogeneous, supervised learn-\ning tasks.\nWe demonstrate the versatility of our framework via com-\nprehensive expositions and detailed experiments on several unsupervised\nproblems such as (a) clustering, (b) outlier detection, and (c) similar-\nity prediction under a common umbrella of meta-unsupervised-learning.\nWe also provide rigorous PAC-agnostic bounds to establish the theoret-\nical foundations of our framework, and show that our framing of meta-\nclustering circumvents Kleinberg’s impossibility theorem for clustering.\n1\nIntroduction\nUnsupervised Learning (UL) is an elusive branch of machine learning, includ-\ning problems such as clustering and manifold learning, that seeks to identify\nstructure among unlabeled data. Unsupervised learning is notoriously diﬃcult\nto evaluate, and debates rage about which objective function to use, since the\nabsence of data labels makes it diﬃcult to deﬁne objective quality measures.\nThis paper proposes a meta-solution that, by considering the (meta)distribution\nover unsupervised problems, reduces UL to Supervised Learning (SL). This is a\ndata-driven approach to quantitatively evaluate and design new UL algorithms.\nGoing beyond typical transfer learning, we show how this approach can be used\nfor to improve UL performance for problems of diﬀerent sizes and from diﬀerent\ndomains.\nAs a thought-provoking example, consider clustering the course reviews in\nFigure 1. The clustering based on sentiment, done by a human, is clearly more\n“human” than the one based on word count, done by machine. How do humans\nlearn to cluster in this way, and how can computers learn to cluster in this way?\nPeople would identify the text as English and may recall related text challenges\nthey have faced, as opposed to image tasks. They may also draw on knowledge\nabout courses. We argue that computers should identify and leverage data from\nrelated prior tasks, such as text corpora, clustered documents, clustered reviews,\nor even clustered course reviews if available.\nA die-hard K-means advocate\nmay say that the K-means objective of average distance to cluster centers is\nright, and that the bag-of-words representation used for computing distances is\n1\narXiv:1612.09030v2  [cs.LG]  3 Jan 2017\nSentiment based clustering\nWord based clustering\nThe class was great!\nThe class was great!\nBest course I have taken in a long time.\nThe class was boring.\nThe class was terrible!\nI slept the whole time and snored loudly.\nThe class was boring.\nBest course I have taken in a long time.\nWaste of time. I now hate psychology.\nThe class was terrible!\nI slept the whole time and snored loudly.\nWaste of time. I now hate psychology.\nFigure 1:\nTwo diﬀerent course review clusterings. Left: a sentiment-based\nclustering, drawing on prior knowledge to establish context. Right: a clustering\nbased on common words and number of words.\ninappropriate. But who should provide the right representation? As computers\nbecome increasingly intelligent and human-like, they are being tasked to learn\nthe appropriate representations themselves.\nNow, the algorithms and experiments in this paper demonstrate that this\napproach has potential but will not necessarily give good clusterings on this\ndata if problems like this are not well represented in our data. However, our\nproblem framing does allow algorithms to use prior data to perform better UL,\njust as humans cluster based on knowledge from their own prior experience.\nThis model is intended to capture, to some extent, how humans perform UL\nand also recent trends in machine learning work on word embeddings and DNN\nfor image classiﬁcation.\nAs a motivating example, consider clustering your course reviews. If you\nwere using a popular ML tool, such as scikit-learn [11], you would have to\nchoose among a number of diﬀerent clustering algorithms, and then you would\nhave to choose how many clusters to have. Is there any principled, data-driven\napproach to making these decisions? In this paper, we oﬀer a solution using an\nannotated collection of prior datasets. We present a theoretical model accom-\npanied by empirical results from a repository of classiﬁcation problems.1 Each\nclassiﬁcation problem, after removing labels, is viewed as a clustering problem\nover the unlabeled data where the (hidden) ground-truth clusters are the sets\nof examples with each label.\nOur model requires a loss function measuring the quality of a solution (e.g., a\nclustering) with respect to some conceptual2 ground truth. Further, we suppose\nthat we have a repository of datasets, annotated with ground truth labels, that\nis drawn from a meta-distribution µ over problems, and that the given data\nX was drawn from this same distribution (though without labels). From this\ncollection, one could, at a minimum, learn which clustering algorithm works\nbest, or, even better, which type of algorithm works best for which type of\ndata. The same meta-approach could help in selecting how many clusters to\nhave, which outliers to remove, and so forth. With some luck, there might even\nbe a large dataset of course reviews annotated with ratings in {1, 2, 3, 4}, where\na pre-trained classiﬁer could be leveraged.\n1Ideally, we would use a clustering repository such as the IFCS Cluster Benchmark Data\nRepository “datasets with and without given ‘true’ clusterings,” at http://ifcs.boku.ac.at/\nrepository once it is suﬃciently large.\n2The ground truth labels may be impossible to acquire for the data at hand.\n2\nOur theoretical model, closely related to Agnostic [8] and PAC Learning\n[19], treats each entire labeled problem analogous to an training example in a\nsupervised learning task. We show how one can provably learn to perform UL\nas well as the best algorithm in certain classes of algorithms.\nEmpirically, we run meta-algorithms on the repository of classiﬁcation prob-\nlems from openml.org which has a variety of datasets from various domains\nincluding NLP, computer vision, and bioinformatics, among others. Ignoring\nlabels, each dataset can be viewed as an unsupervised learning problem. In-\nterestingly, we ﬁnd that this seemingly unrelated collection of problems can be\nleveraged to improve average performance across datasets. In particular, we\nﬁrst ﬁnd that the K-means clustering algorithm generally outperforms four oth-\ners across the collection of problems. While this is not surprising in the sense\nthat K-means is one of the most popular clustering algorithms, our ﬁnding is\nbased upon data and not simply “word of mouth.” Second, we show that there\nare systematic but correctable biases among a standard heuristic for selecting\nthe number of clusters. Third, we show how the data can be used to decide\nhow many outliers to remove to improve clustering performance. Finally, we\nalso show how to train a neural network using data from multiple classiﬁca-\ntion problems of very diﬀerent natures to improve performance on a new UL\nproblem.\nIn some sense, this meta-approach is arguably being used by today’s NLP\nand computer vision practitioners, as well as by humans. For instance, when\na human clusters course reviews, they heavily use their knowledge of English\nthat has been already acquired from a number of other learning tasks. Sim-\nilarly, modern NLP algorithms rely upon Word Embeddings [14] that encode\nthe meanings of words in a constant number of dimensions and are trained on\nother text corpora or billions of words. In some sense, expecting an algorithm to\ncluster a small set of text reviews without external data is like asking a person\nto cluster course reviews written in a foreign language without a dictionary –\nnot only would one have to learn the new language from the reviews themselves\nbut there might not even be enough reviews to adequately cover the language.\nSimilarly, a recent theme in computer vision, given a set of thousands of im-\nages, is to re-use pieces of neural networks trained on a labeled set of millions\nof images.\n2\nRelated work\nThis work relates to and uniﬁes a number of areas within machine learning. In\nunsupervised learning, the frustrating sense of futility in debating the “best”\nclustering algorithm is conveyed by Kleinberg’s impossibility theorem for clus-\ntering [9], though his axioms have been the subject of further debate [20, 1]. In\nSection 5, we show how meta-clustering circumvents this impossibility in some\nsense.\nSome supervised approaches [4], [6] have been proposed to learn a clustering\nfunction, using a dataset where each instance is comprised a set of homogeneous\nitems (such as news articles) and their partitioning into clusters. The learned\nfunction could then be used to cluster a new set of items of the same kind.\nOur approach generalizes the notion by learning across heterogeneous datasets\nthat are compiled from several domains, to ﬁnd a good clustering for new data.\n3\nThese domains may have data represented with diﬀerent dimensionalities.\nOur framework can also be viewed as taking an extreme viewpoint of transfer\nlearning. The body of work on (supervised) transfer learning is too large to\nsurvey here, see e.g., [10] and references therein. Our work can be viewed as\nmore extreme than typical transfer learning in that the multiple source datasets\nmight have completely diﬀerent distributions and data generating processes from\neach other, and from the (multiple) test datasets. As mentioned, typically in\ntransfer learning the problems have the same dimensionality or at least the same\ntype of data (text, images, etc.), whereas our experiments consist of varied data\ntypes. Unlike many transfer learning methods, we do not use the features from\ntest data during training.\nOur work also relates to supervised learning questions of meta-learning,\nsometimes referred to as auto-ml (e.g., [16, 5]), learning to learn (e.g., [18]),\nBayesian optimization (e.g., [15]) and lifelong learning (e.g., [17, 3]). In the\ncase of supervised learning, where accuracy is easy to evaluate, meta-learning\nenables algorithms to achieve accuracy more quickly with less data. We argue\nthat for unsupervised learning, the meta approach oﬀers a principled means of\ndeﬁning and evaluating unsupervised learning.\n3\nLearning Preliminaries\nA learning task consists of a universe X, labels Y, outputs Z, and a bounded loss\nfunction ℓ: X × Y × Z →[0, 1]. A learner L : (X × Y)∗→ZX takes a training\nset T = (X1, Y1), . . . (Xn, Yn) consisting of a ﬁnite number of iid samples from\nµ and outputs a classiﬁer L(T) ∈ZX , which denotes the set of functions from\nX to Z. The loss of a classiﬁer c ∈ZX is ℓµ(c) = E(X,Y )∼µ [ℓ(X, Y, c(X))], and\nthe expected loss of L is ℓµ(L) = ET ∼µn[ℓµ(L(T))].\nMeta-unsupervised-learning is a special case of supervised learning. There\nare two subtle diﬀerences.\nThe ﬁrst is conceptual:\nthe output of a meta-\nunsupervised-learning algorithm is an unsupervised algorithm (e.g., a clustering\nalgorithm) and the training examples are entire data sets. Also note that, while\nwe require the training data to be fully labeled (e.g., clustered/classiﬁed), we\nmay never see the true clusters of any problem encountered after deployment.\nThis is diﬀerent than online learning, where it is assumed that for each exam-\nple, after you make a prediction, you ﬁnd out the ground truth. The second\ndiﬀerence is that Z ̸= Y, the output of the algorithm may or may not be of\nthe same “type” as the ground truth labels. For instance, in feature selection\nthe output may be a subset of the features whereas the training data may have\nclassiﬁcation labels, and the loss function connects the two.\nLearning is with respect to a concept class C ⊆ZX .\nDeﬁnition 1 (Agnostic learning of C). For countable sets3 X, Y, Z and ℓ:\nX × Y × Z →[0, 1], a learner L agnostically learns A ⊆ZX if there exists\na polynomial p such that for any distribution µ over X × Y and for any n ≥\np(1/ϵ, 1/δ),\nPr\nT ∼µn\n\u0014\nℓµ(L(T)) ≤min\nc∈C ℓµ(c) + ϵ\n\u0015\n≥1 −δ.\n3For simplicity of presentation, we assume that these sets are countable, but with appro-\npriate measure theoretic assumptions the analysis in this paper extends to inﬁnite cases in a\nstraightforward manner.\n4\nFurther, L and the classiﬁer L(T) must run in time polynomial in the length of\ntheir inputs.\nPAC learning refers to the special case where an additional assumption is\nplaced on µ such that minc∈C ℓµ(c) = 0. A learner L is called proper if it only\noutputs classiﬁers in C. We call the task homogeneous if Y = Z. (Most tasks\nwe consider will be homogeneous.)\n3.1\nMeta-unsupervised-learning deﬁnitions\nMeta-unsupervised-learning, which is henceforth the focus of this paper, simply\nrefers to case where µ is a meta-distribution over datasets X ∈X and ground\ntruth labelings Y ∈Y, and classiﬁers c are unsupervised learning algorithms\nthat take an entire dataset X as input, such as clustering algorithms. Unlike\nonline learning, as mentioned, true labels are only observed for the training\ndatasets.\n3.1.1\nMeta-clustering\nFor a ﬁnite set S, the clusterings Π(S) denotes the set of disjoint partitions of S\ninto 2 or more sets, e.g., Π({1, 2, 3}) =\n\b\n{{1}, {2, 3}}, {{2}, {1, 3}}, {{1, 2}, {3}}\n\t\n.\nFor a clustering C, denote by ∪C = ∪S∈CS the set of points clustered. We as-\nsume that each X ∈X is a ﬁnite set of two or more elements and\nY = Z =\n\b\nΠ(X) | X ∈X\n\t\n.\nThe loss function deﬁned below is 1 if the clusterings are invalid.4 To deﬁne the\nloss, for clustering C, we ﬁrst deﬁne the distance function dC(x, x′) to be 0 if\nthey are in the same cluster, i.e., x, x′ ∈S for some S ∈C, i.e., and 1 otherwise.\nThe loss will be the following clustering distance,\nℓ(X, Y, Z) =\n\n\n\n\n\n1\n|X|(|X| −1)\nX\nx,x′∈X\n|dY (x, x′) −dZ(x, x′)|\nif Y, Z ∈Π(X)\n1\notherwise\n(1)\nwhere |S| denotes the size of set S. In words, this is the fraction of pairs of\ndistinct points where the two clusterings diﬀer on whether or not the two points\nare in the same cluster. Note that this loss is 1 −RI(Y, Z), where RI is the\nso-called Rand Index, one of the most common measures of clustering accuracy\nwith respect to a ground truth. In our experiments, the loss we will measure is\nthe standard Adjusted Rand Index (ARI) which attempts to correct the Rand\nIndex by accounting for chance agreement [7]. We denote by ARI(Y, Z) the\nadjusted rand index between two clusterings Y and Z. We abuse notation and\nalso write ARI(Y, Z) when Y is a vector of class labels, by converting it to a\nper-class clustering with one cluster for each class label.\nWe refer to the problem of clustering into k=2 clusters as 2-clustering.\nIn Euclidean clustering, the points are Euclidean, so each dataset X ⊂Rd\nfor some d ≥1. Note that diﬀerent datasets may have diﬀerent dimensionalities\n4It would be natural to impose the requirement that ∪Y = X with probability 1 over µ,\nbut this is not formally necessary as all clustering algorithms c ∈C have ∪c(X) = X and\nhence incur loss of 1 in the case that ∪Y ̸= X.\n5\nd. Other frameworks could be modeled, e.g., in list clustering (see, e.g., [2])\nwhere the algorithm outputs a bounded list of clusterings Z = (Z1, . . . , Zl), the\nloss could be mini d(Y, Zi).\nRand Index measures clustering quality with respect to an extrinsic ground\ntruth. In many cases, such a ground truth is unavailable, and an intrinsic metric\nis useful. Such is the case when choosing the number of clusters. Given diﬀerent\nclusterings of size k = 2, 3, . . ., how can one compare and select? One approach\nis the so-called silhouette score [12], deﬁned as follows for a Euclidean clustering:\nsil(C) =\n1\n| ∪C|\nX\nx∈∪C\nb(x) −a(x)\nmax{a(x), b(x)},\n(2)\nwhere a(x) denotes the average distance between point x and other points in\nits own cluster and b(x) denotes the average distance between x and points in\nan alternative cluster, where the alternative cluster is the one whose minimum\naverage distance to x is smallest among those clusters diﬀerent than the one\ncontaining x.\n4\nMeta-unsupervised-ERM\nThe simplest approach to meta-unsupervised-learning is Empirical Risk Mini-\nmization (ERM), namely choosing the unsupervised algorithm from some family\nC with lowest empirical error on training set T, which we write as ERMC(T).\nStandard learning bounds imply a logarithmic dependence on the size of the\nfamily.\nLemma 1. For any ﬁnite family C of unsupervised learning algorithms, any\ndistribution µ over problems X, Y ∈X × Y, and any m ≥1, δ > 0,\nPr\nT ∼µn\n\"\nℓµ(ERMC(T)) ≤min\nc∈C ℓµ(c) +\nr\n2\nn log |C|\nδ\n#\n≥1 −δ,\nwhere ERMC(T) ∈arg minc∈C\nP\nT ℓ(Y, c(X)) is any empirical loss minimizer\nover c ∈C.\nThis standard generalization bound, which follows from Chernoﬀand union\nbounds, suﬃces to bound the performance of ERM over ﬁnite sets of unsuper-\nvised learning algorithms.\nSelecting among algorithms. An immediate corollary is that, given m clus-\ntering algorithms, one can perform within O(\np\nlog(m)/n) as the best by evalu-\nating all clustering algorithms on a training set of n clustering problems (with\nground truths) and choosing the best.\nFitting a parameter. Next, consider choosing the threshold parameter of a\nsingle linkage clustering algorithm. Fix the set of possible vertices V. Take the\nuniverse X to consist of undirected weighted graphs X = (V, E, W) with vertices\nV ⊆V, edges E ⊆{{u, v} | u, v ∈V } and non-negative weights W : E →R+.\nData in Rd can be viewed as a complete graph in with W({x, x′}) = ∥x −x′∥.\nThe prediction and output sets are simply Y = Z = Π(V) the sets of parti-\ntions of vertices, and the loss is deﬁned as in Eq. (1). The classic single-linkage\nclustering algorithm with parameter r ≥0, Lr(V, E, w), creates as clusters the\n6\nconnected components of the subgraph of (V, E) consisting of all edges whose\nweights are less than or equal to r. That is, u, v ∈V are in the same cluster if\nand only if there is a path from u to v where all edges in the path have weights\nat most r.\nIt is trivial to see that one can ﬁnd the best cutoﬀfor r in polynomial time:\nfor each edge weight r in the set of edge weights across all graphs, compute\nthe mean loss of Lr across the training set. Since Lr runs in polynomial time,\nloss can be computed in polynomial time, and the number of diﬀerent possible\ncutoﬀs is bounded by the number of edge weights which is polynomial in the\ninput size, the entire algorithm runs in polynomial time.\nFor a quasilinear-time algorithm (in the input size |T| = Θ(P\ni |Vi|2)), run\nKruskal’s algorithm on the union graph of all of the graphs in the training set\n(i.e., the number of nodes and edges are the sum of the number of nodes and\nedges in the training graphs, respectively). As Kruskal’s algorithm adds each\nnew edge to its forest (in order of non-decreasing edge weight), eﬀectively two\nclusters in some training graph (Vi, Ei, Wi) have been merged. The change in\nloss of the resulting clustering can be computed from the loss of the previous\nclustering in time proportional to the product of the two clusters that are being\nmerged, since these are the only values on which Zi changed.\nNaively, this\nmay seem to take order P\ni |Vi|3. However, note that, each pair of nodes begins\nseparately and is updated, exactly once during the course of the algorithm, to be\nin the same cluster. Hence, the total number of updates is O(P\ni |Vi|2), and since\nKruskal’s algorithm is quasilinear time itself, the entire algorithm is quasilinear.\nFor correctness, it is easy to see that as Kruskal’s algorithm runs, Lr has been\ncomputed for each possible r at the step just preceding when Kruskal adds the\nﬁrst edge whose weight is greater than r.\nFor generalization bounds, let us simply assume that numbers are repre-\nsented with some constant b number of bits, as is common on all modern com-\nputers. By Lemma 1, we see that with m training graphs and |{Lr}| ≤2b, we\nhave that with probability ≥1−δ, the error of ERM is within\np\n2(b + log 1/δ)/n\nof minr ℓµ(Lr). Combining this argument with the eﬃcient algorithm, proves:\nTheorem 1. The class {Lr | r > 0} of single-linkage algorithms with threshold\nr (where numbers are represented using b bits), can be agnostically learned.\nIn particular, a quasilinear time algorithm achieves error ≤minr ℓµ(Lr) +\np\n2(b + log 1/δ)/n, with probability ≥1 −δ over the n training problems.\n4.1\nMeta-K: choosing the number of clusters, k\nWe now discuss a meta approach to choosing the number of clusters, a cen-\ntral problem in clustering. we refer to this as the Meta-K problem: selecting\namong the output of a base clustering algorithm that outputs one clustering\nwith k clusters for each k ∈{2, 3, . . . , K} in a bounded range. Clearly, we re-\nquire additional information as diﬀerent clustering problems require diﬀerent\nnumbers of clusters. This additional “meta-information” includes information\nspeciﬁc to the clustering and to the problem. In particular, suppose that each\nproblem X encodes, among other things, a set of points V to be clustered and\nproblem metadata φ ∈Φ.\nAlso, suppose that the base algorithm produces\nclusterings (C2, γ2), . . . , (CK, γK) ∈CV × Γ where Ck has k clusters and γk\nis meta-information about the clustering. For instance, one common type of\n7\nmeta-information would be some clustering quality heuristic such as the silhou-\nette score for a clustering as deﬁned in eq. (2).\nHowever, instead of simply\nchoosing the clustering maximizing silhouette score, we can learn how to choose\nthe best clustering based on k, φ, and γk.\nGiven a family F of functions f : Φ × ΓK−1 →{2, . . . , K} that select the\nnumber of clusters, as long as F can be parametrized by a ﬁxed number of\nb-bit numbers, the ERM approach of choosing the “best” f will be statistically\neﬃcient. If, for training problem Xi, the metadata is φi and clustering algorithm\noutputs (Ci2, γi2), . . . , (CiK, γiK), then ERM amounts to:\nERMF = arg min\nf∈F\nX\ni\nℓ\n\u0000Xi, Yi, Cif(φi,γi2,...,γiK)\n\u0001\n.\nThe ﬁnal clustering algorithm output would be ERMf run among the output\nof the base clustering algorithm. If ERMf cannot be computed exactly within\ntime constraints, an approximate minimizer may be used.\n4.2\nMeta-outlier-removal\nFor simplicity, we consider learning the single hyperparameter of the fraction of\nexamples, furthest from the mean, to remove. In particular, suppose training\nproblems are classiﬁcation instances, i.e., Xi ∈Rdi×mi and Yi ∈{1, 2, . . . , ki}mi.\nThis problem could apply to clustering or any other unsupervised learning al-\ngorithm. For this section, we assume that the base classiﬁer algorithm (e.g.,\nclustering algorithm) takes a parameter θ which is how many outliers to ignore\nduring ﬁtting.\nFor instance, in clustering, given an algorithm C, one might deﬁne Cθ with\nparameter θ ∈[0, 1) on data x1, . . . , xn ∈Rd as follows:\n1. Compute the data mean µ = 1\nn\nP\ni xi.\n2. Set aside as outliers the θ examples where xi is furthest from µ in Euclidean\ndistance.\n3. Run the base clustering algorithm C on the data with outliers removed.\n4. Assign each outlier to the cluster whose center it is closest to.\nWe can then compute the loss of the resulting output Z and the ground\ntruth Y , for any dataset. Given a meta-dataset of n datasets, we choose θ so\nas to optimize performance. With a single b-bit parameter θ, Lemma 1 easily\nimplies that this choice of θ will give a loss within\np\n2(b + log 1/δ)/n of the\noptimal θ, with probability ≥1 −δ of the sample of datasets. The number of\nθ’s that need to be considered is bounded by the total number of inputs across\nproblems, so the meta-algorithm runs in polynomial time.\n4.3\nProblem recycling\nFor this model, suppose that each problem belongs to a set of common problem\ncategories, e.g., digit recognition, sentiment analysis, image classiﬁcation among\nthe thousands of classes of ImageNet [13], etc. The idea is that one can recycle\nthe solution to one version of the problem in a later incarnation. For instance,\n8\nsuppose that one trained a digit recognizer on a previous problem. For a new\nproblem, the input may be encoded diﬀerently (e.g., diﬀerent image size, diﬀer-\nent pixel ordering, diﬀerent color representation), but there is a transformation\nT that maps this problem into the same latent space as the previous problem\nso that the prior solution can be re-used.\nIn particular, for each problem category i = 1, 2, . . . , N, there is a latent\nproblem space Λi and a solver Si : Λi →Zi. Each problem X, Y of this category\ncan be transformed to T(X) ∈Λi with low solution loss ℓ(X, Y, S(T(X))). Note\nthat one of these categories could be “miscellaneous,” a catch-all category whose\nsolution could be another unsupervised (or meta-unsupervised) algorithm.\nA problem recycling model consists of three components:\n• Solvers Si : Λi →Zi, for i = 1, 2, . . . , N, where each solver operates over\na latent space Λi.\n• A (meta)classiﬁer M : X →{1, 2, . . . , N} that, for a problem X, identiﬁes\nwhich solver i = M(X) to use.\n• Transformation procedures Ti : M −1(i) →Li that maps any X such that\nM(X) = i into latent space Λi.\nThe output of the meta-classiﬁer is simply SM(X)(TM(X)(X)). Lemma 1 implies\nthat if one can optimize over meta-classiﬁers and the parameters of the meta-\nclassiﬁer are represented by D b-bit numbers, then one achieves loss within ϵ of\nthe best meta-classiﬁer with m = O\n\u0000Db/ϵ2\u0001\nproblems.\n5\nThe possibility of meta-clustering\nIn this section, we point out how the framing of meta-clustering circumvents\nKleinberg’s impossibility theorem for clustering [9]. To review, Kleinberg con-\nsiders clustering ﬁnite sets of points X endowed with symmetric distance func-\ntions d ∈D(X), where the set of valid distance functions is:\nD(X) = {d : X×X →R | ∀x, x′ ∈X d(x, x′) = d(x′, x) ≥0, d(x, x′) = 0 iﬀx = x′}.\nA clustering algorithm A takes a distance function d ∈D(X) and returns a\npartition, i.e., A(d) ∈Π(X).\nHe deﬁnes the following three axioms that should hold for any clustering\nalgorithm A:\n1. Scale-Invariance. For any distance function d and any α > 0, A(d) =\nA(α · d), where α · d is the distance function d scaled by α.\n2. Richness.\nFor any ﬁnite X and clustering C ∈Π(X), there exists a\ndistance d ∈D(X) such that A(d) = C.\n3. Consistency. Let d, d′ ∈D(X) such that A(d) = C, and for all x, x′ ∈X,\nif x, x′ are in the same cluster in C then d′(x, x′) ≤d(x, x′) while if x, x′\nare in diﬀerent clusters in C then d′(x, x′) ≥d(x, x′). Then A(d′) = A(d).\nThe key diﬃculty to deﬁning clustering is to establish a scale. To get intuition\nfor why, consider clustering two points X = {x1, x2} so there is a single distance\n9\nin the problem. For a moment, consider allowing the trivial clustering into a\nsingle cluster, so there are two legal clusterings {{x1, x2}} and {{x1}, {x2}}. To\nsee that axioms 1 and 2 are impossible to satisfy note that, by richness, there\nmust be some distances δ1, δ2 > 0 such that if d(x1, x2) = δ1 then they are in the\nsame cluster while if d(x1, x2) = δ2 they are in diﬀerent clusters. This, however\nviolates Scale-Invariance, since the two problems are at a scale α = δ2/δ1 of each\nother. Now, this example fails to hold when there are two or more clusters, but\nit captures the essential intuition.\nNow, suppose we deﬁne the clustering problem with respect to a non-empty\ntraining set of clustering problems. So a meta-clustering algorithm takes t ≥1\ntraining clustering problems M(d1, C1, . . . , dt, Ct) = A with their ground-truth\nclusterings (on corresponding sets Xi, i.e., di ∈D(Xi) and Ci ∈Π(Xi)) and\noutputs a clustering algorithm A.\nWe can use these training clusterings to\nestablish a scale.\nIn particular, we will show a meta-clustering algorithm whose output A\nalways satisﬁes the second two axioms and which satisﬁes the following variant\nof Scale-Invariance:\n1. Meta-Scale-Invariance. Fix any distance functions d1, d2, . . . , dt and\nground truth clusterings C1, . . . , Ct on sets X1, . . . , Xt. For any α > 0,\nand any distance function d, if M(d1, C1, . . . , dt, Ct) = A and M(α ·\nd1, C1, . . . , α · dt, Ct) = A′, then A(d) = A′(α · d).\nWith meta-clustering, the scale can be established using training data.\nTheorem 2. There is a meta-clustering algorithm that satisﬁes Meta-Scale-\nInvariance and whose output always satisﬁes Richness and Consistency.\nProof. There are a number of such clustering algorithms, but for simplicity we\ncreate one based on single-linkage clustering. Single-linkage clustering satisﬁes\nRichness and Consistency (see [9], Theorem 2.2). The question is how to choose\nit’s single-linkage parameter. One can choose it to be the minimum distance\nbetween any two points in diﬀerent clusters across all training problems. It is\neasy to see that if one scales the training problems and d by the same factor\nα, the clusterings remain unchanged, and hence the meta-clustering algorithm\nsatisﬁes meta-scale-invariance.\n6\nExperiments\nWe conducted several experiments to substantiate the eﬃcacy of the proposed\nframework under various unsupervised settings. We downloaded all classiﬁca-\ntion datasets from OpenML5 that had at most 10,000 instances, 500 features,\n10 classes, and no missing data to obtain a corpus of 339 datasets. For our pur-\nposes, we extracted the numeric features from all these datasets ignoring the\ncategorical features. We now describe in detail the results of our experiments,\nhighlighting the gains achieved due to our meta paradigm.\n5http://www.openml.org .\n10\n6.1\nSelecting an algorithm\nAs a ﬁrst step, we consider the question of which of a number of given clustering\nalgorithms to use to cluster a given data set. In this section, we focus on k = 2\nclusters. Later, we consider meta-algorithms for choosing the number of clusters.\nFirst, one can run each of the algorithms on the repository and see which\nalgorithm has the lowest average error. Error is calculated with respect to the\nground truth labels by the ARI (see Section 3). We compare algorithms on\nthe 250 openml binary classiﬁcation datasets with at most 2000 instances. The\nten base clustering algorithms were chosen to be ﬁve clustering algorithms from\nscikit-learn (K-Means, Spectral, Agglomerative Single Linkage, Complete Link-\nage, and Ward) together with a second version of each in which each attribute\nis ﬁrst normalized to have zero mean and unit variance. Each algorithm is run\nwith the default scikit-learn parameters.\nBeyond simply choosing the algorithm that performs best across the prob-\nlems, we can learn to choose a diﬀerent algorithm for each problem. In order\nto do this, we begin with problem meta-features such as the number of features\nand examples in the datasets, the maximum and minimum singular values of\nthe covariance matrix of the unlabeled data. In fact, we can use even more\ninformation. In particular, the meta-clustering algorithm for this section runs\neach of ten clustering algorithms on the dataset and computes what is known as\na Silhouette score (see Section 3), which is a standard heuristic for evaluating\nclustering performance. However, rather than simply choosing the clustering\nwith the best Silhouette score, the meta-clustering algorithm learns to select a\nclustering based on these scores.\nTo formally deﬁne the algorithm, given a clustering Π of a dataset X ∈\nRd×m, let the feature vector Φ(X, Π) consist of the dimensionality, number of\nexamples, minimum and maximum eigenvalues of the covariance matrix, and\nthe silhouette score of the clustering Π:\nΦ(X, Π) = (d, m, σmin(Σ(X)), σmax(Σ(X)), sil(Π)) ,\nwhere Σ(X) denotes the covariance matrix of X, and σmin(M) and σmax(M)\ndenote the minimum and maximum eigenvalues, respectively, of matrix M.\nLet the training datasets be\n\nXi ∈Rdi×mi, Yi ∈{0, 1}mi\u000bn\ni=1. Let the base\nclustering algorithms be Cj,. The ﬁrst phase of the algorithm learns a weight\nvector wj ∈R5 for each algorithm j. To ﬁnd wj, one can solve, for instance, a\nsimple least squares linear regression6,\nwj = arg min\nw∈R5\nn\nX\ni=1\n\u0000w · Φ(Xi, Cj(Xi)) −ARI(Yi, Cj(Xi))\n\u00012.\nTo cluster a new dataset X ∈Rd×m, the meta-algorithm then computes ARI\nestimates, aj = wj · φ(X, Cj(X)). It then takes the j with maximal aj and\noutputs the clustering Cj(x).\nAs a small modiﬁcation, for each of our base algorithms, we run them a\nsecond time normalizing the data X ﬁrst to have mean 0 and variance 1 in each\ncoordinate, again generating ARI estimates and taking the one with greatest\nestimated ARI.\n6We instead used the standard ν-SVR formulation for our experiments.\n11\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\nTraining fraction\nAdjusted Rand Index (ARI)\nPerformance of diﬀerent algorithms\nMeta\nKMeans\nKMeans-N\nWard\nWard-N\nAverage\nAverage-N\nComplete\nComplete-N\nSpectral\nSpectral-N\nFigure 2: Adjusted Rand Index (ARI) scores of diﬀerent clustering (k=2) al-\ngorithms on 250 openml binary classiﬁcation problems. The meta algorithm is\ncompared with standard clustering baselines on both the original data as well\nas the transformed data where all features in the datasets were normalized to\nhave zero mean and unit variance (denoted by the suﬃx “-N”). The ﬁgure shows\n95% conﬁdence intervals for the meta approach.\nThe results, shown in Figure 2, demonstrate two points of interest. First,\none can see that the diﬀerent baseline clustering algorithms had very diﬀerent\naverage performances. Hence, a practitioner with unlabeled data (and hence lit-\ntle means to evaluate the diﬀerent algorithms), may very likely choose a random\nalgorithm and suﬀer poor performance. If one had to choose a single algorithm,\nK-means with normalization performed best. Perhaps this is not surprising as\nK-means is arguably the most popular clustering algorithm, and normalization\nis a common preprocessing step. However, our experiment justiﬁes this choice\nbased on evaluation across problems from the openml repository.\nSecond, Figure 2 also demonstrates that the meta-algorithm, given suﬃ-\nciently many training problems, is able to outperform, on average, all the base-\nline algorithms. This is despite the fact that the 250 problems have diﬀerent\ndimensionalities and come from diﬀerent domains.\n6.2\nMeta-K\nFor the purposes of this section, we ﬁx the clustering algorithm to be K-means\nand compare two approaches to choosing the number of clusters, k from k = 2\nto 10. First, we consider a standard heuristic for the baseline choice of k: for\n12\neach cluster size k and each dataset, we generate 10 clusterings from diﬀerent\nrandom starts for K-means and take one with best Silhouette score among the\n10.\nThen, over the 9 diﬀerent values of k, we choose the one with greatest\nSilhouette score so that the resulting clustering is the one of greatest Silhouette\nscore among all 90.\nSimilar to the approach for choosing the best algorithm above, in our meta-K\napproach, we learn to choose k as a function of Silhouette score and k by choosing\nthe k with highest estimated ARI. As above, for any given problem and any\ngiven k, among the 10 clusterings, we choose the one that maximizes Silhouette\nscore. However, rather than simply choosing k that maximizes Silhouette score,\nwe choose k to maximize a linear estimate of ARI that varies based on both the\nSilhouette score and k. We evaluate on exactly the same 90 clusterings for of\nthe 339 datasets as the baseline.\nWe computed the Silhouette scores and Adjusted Rand Index (ARI) scores\nfor each of the 339 datasets from k = 2 to 10. We conducted 10 such independent\nexperiments for each dataset to account for statistical signiﬁcance, and thus\nobtained two 90-dimensional vectors per dataset for Silhouette and ARI scores.\nMoreover, as is standard in the clustering literature, we assumed the best-ﬁt k∗\ni\nfor dataset i to be the one that yielded maximum ARI score across the diﬀerent\nruns, which is often diﬀerent from ki, the number of clusters in the ground truth,\ni.e., the number of class labels.\nThe training and test sets were obtained using the following procedure. Each\nof the 339 datasets were designated to be either a training set or test set. The\nnumber of training sets was varied over a wide range to take values in the set\n{140, 160, . . . , 280}. For each such split size, the training examples from the\ndiﬀerent sets together formed the meta-training set, and the remaining sets\nformed the meta-test set. Thus, 8 such (training, test) partitions were obtained\ncorresponding to these sizes.\nFor any particular partition into training and test sets, we followed a regres-\nsion procedure to estimate the ARI score as a function of the Silhouette score.\nSpeciﬁcally, for each k ∈{2, . . . , 9}, we ﬁt a separate linear regression model for\nARI (target or dependent variable) using Silhouette (observation) using all the\ndata from the meta-training set in the partition pertaining to k: each dataset in\nthe meta-training set provided 10 target values, corresponding to diﬀerent runs\nwhere number of clusters was ﬁxed to k. The models were ﬁt using simple least-\nsquares linear regression. Thus, we ﬁtted 9 single feature single output linear\nregression models, each of size 10*number of training sets in the meta-training\nset.\nWe then used the parameters of the regression models to predict the best k\nseparately for each dataset in the meta-test set. Speciﬁcally, for each dataset in\nthe meta-test set, we predicted the ARI score for each k and each run using the\nparameters of regression model for k. Then, for each k, we took its predicted\nscore to be the max score over the diﬀerent runs. We took an argmax over the\nmax scores for diﬀerent k to predict ˆki for the dataset i, i.e., the optimal number\nof clusters (if more than one k resulted in the max score, we took the smallest k\nas ˆki to break the ties). We then computed the average root mean square error\n(RMSE) between ˆki and k∗\ni over the datasets in the meta-test to quantify the\ndiscrepancy between the predicted and best values of the number of clusters.\nWe considered our baseline ˜k, for each set, to be the k that resulted in\n13\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\n3.4\n3.6\n3.8\n4\nTraining fraction\nAverage RMSE\nBest-k Root Mean Square Error\nSilhouette\nMeta\nFigure 3: RMSE of distance to the best k, k∗, across datasets, i.e., comparing\nq\n1\nn\nP(˜ki −k∗\ni )2 and\nq\n1\nn\nP(ˆki −k∗\ni )2. Clearly, the meta-K method outputs a\nnumber of clusters much closer to k∗than the Silhouette score maximizer.\nmaximum Silhouette score. Note that this is a standard heuristic to predicting\nthe “right” number of clusters based solely on the Silhouette score. We compare\nthe RMSE between ˜k and k∗to the average RMSE resulting from the meta-\nK procedure. We report the average RMSE and the corresponding standard\ndeviation error-bars for both the baseline and the meta-K procedures (the latter\naveraged over 10 independent partitions for each t ∈T).\nExperiments suggest that the meta-K outperforms the Silhouette baseline\nboth in terms of the distance between the chosen k and k∗(Fig. 3) and in\nterms of ultimate loss, i.e., ARI (Fig. 4). Note again that the simple methods\nemployed here could clearly be improved using more advance methods, but\nsuﬃce to demonstrate the advantage of the meta approach.\n6.3\nRemoving outliers\nWe also conducted experiments to validate the applicability of the meta frame-\nwork to removing outliers on the same 339 binary and multi-class classiﬁcation\nproblems. Our objective was to choose a single best fraction to remove from\nall the meta-test sets. We now describe our experimental design for this set-\nting. For each meta-training set X in any instantiation of random partitioning\nobtained via the aforesaid procedure, we removed a p ∈{0, 0.01, 0.02, . . . , 0.05}\nfraction examples with the highest euclidean norm in X as outliers, and likewise\n14\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\n0.14\nTraining fraction\nAverage ARI\nAdjusted Rand Index\nSilhouette\nMeta\nFigure 4: Average ARI scores of the meta-algorithm and the baseline for choos-\ning the number of clusters, versus the fraction of problems used for training.\nWe observe that the best k predicted by the meta approach registered a higher\nARI than that by Silhouette score maximizer. 95% Conﬁdence interval obtained\nwith 1000 random splits.\n15\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\n0.14\nTraining fraction\nAverage Adjusted RI\nAdjusted Rand Index (Outlier Removal)\n5%\n4%\n3%\n2%\n1%\n0%\nFigure 5: Outlier removal results. Removing 1% of the instances as outliers\nimproves on the ARI scores obtained without outlier removal.\nfor each meta-test set in the partition. Note that no examples were removed\nwhen p = 0, which is exactly the setting we considered in section 6.2. We ﬁrst\nclustered the data without outliers, and obtained the corresponding Silhouette\nscores. We then put back the outliers by assigning them to their nearest clus-\nter center, and computed the ARI score thereof. Then, following an identical\nprocedure to the meta-K algorithm of section 6.2, we ﬁtted regression models\nfor ARI corresponding to complete data using the silhouette scores on pruned\ndata, and measured the eﬀect of outlier removal in terms of the true average\nARI (corresponding to the best predicted ARI) over entire data. Again, we\nreport the results averaged over 10 independent partitions.\nAs shown in Fig.\n5, by treating 1% of the instances in each dataset as\noutliers, we achieved an improvement in ARI scores relative to clustering with\nall the data as in section 6.2. As the fraction of data considered outlier was\nincreased to 2% or higher, however, we observed a substantial dip in the per-\nformance. While 1% was the optimal fraction among the candidates, clearly\nfurther parameter search and improved outlier removal algorithms could pro-\nvide additional beneﬁt. Our goal in this paper is mainly to pose the questions\nand demonstrate that meta-learning can improve performance across problems\nfrom diﬀerent domains and of possibly diﬀerent dimensionality.\n16\n6.4\nDeep learning a binary similarity function\nIn this section, we consider a new unsupervised problem of learning a binary\nsimilarity function (BSF) that predicts whether two examples from a given\nproblem should belong to the same cluster (i.e., have the same class label).\nFormally, a problem is speciﬁed by a set X of data and meta-features φ. The\ngoal is to learn a classiﬁer f(x, x′, φ) ∈{0, 1} that takes two examples x, x′ ∈X\nand the corresponding problem meta-features φ, and predicts 1 if the input pair\nwould belong to the same cluster (or have the same class labels).\nHere, we\napproximate this function using a neural network.\nIn our experiments, we take Euclidean data X ⊆Rd (each problem may\nhave diﬀerent dimensionality d) and the meta-features φ = Σ(X) consist of the\ncovariance matrix of the unlabeled data. We restricted our experiments to the\n146 datasets with at most 1000 examples and 10 features. We normalized each\ndataset to have zero mean and unit variance along every coordinate. Hence the\ncovariance matrix was 1 on the diagonal.\nWe randomly sampled pairs of examples from each dataset to form meta-\ntraining and meta-test sets as described in the following section 6.4.1. For each\npair, we concatenated the features to create data with 20 features (padding ex-\namples with fewer than 10 features with zeros). We then computed the empirical\ncovariance matrix of the dataset, and vectorized the entries of the covariance\nmatrix on and above the leading diagonal to obtain an additional 55 covariance\nfeatures. We then concatenated these features with the 20 features to form a\n75-dimensional feature vector per pair. Thus all pairs sampled from the same\ndataset shared the 55 covariance features. Moreover, we derived a new binary\nlabel dataset in the following way. We assigned a label 1 to pairs formed by\ncombining examples belonging to the same class, and 0 otherwise. In summary,\neach of the 146 datasets resulted in a new dataset with 75 features and binary\nlabels.\n6.4.1\nSampling pairs to form meta-train and meta-test datasets\nWe formed a partition of the new datasets by randomly assigning each dataset\nto one of the two categories with equal probability. Each dataset in the ﬁrst\ncategory was used to sample data pairs for both the meta-training and the\nmeta-internal test (meta-IT) datasets, while the second category did not con-\ntribute any training data and was exclusively used to generate only the meta-\nexternal test (meta-ET) dataset.\nWe constructed meta-training pairs by sampling randomly pairs from each\ndataset in the ﬁrst category. In order to mitigate the bias resulting from the\nvariability in size of the diﬀerent datasets, we restricted the number of pairs\nsampled from each dataset to at most 2500. Likewise, we obtained the meta-IT\ndataset by collecting randomly sampling each dataset subject to the maximum\n2500 pairs. Speciﬁcally, we randomly shuﬄed each dataset belonging to the\nﬁrst category, and used the ﬁrst half (or 2500 examples, whichever was fewer) of\nthe dataset for the meta-training data, and the following indices for the meta-\nIT data, again subject to maximum 2500 instances. This procedure ensured\na disjoint intersection between the meta-training and the meta-IT data. Note\nthat combining thousands of examples from each of hundreds of problems yields\nhundreds of thousands of examples, turning small data into big data. This\n17\nprovides a means of making DNNs naturally applicable to data sets that might\nhave otherwise been too small.\nWe created the meta-ET data using datasets belonging to the second cat-\negory.\nAgain, we sampled at most 2500 examples from each dataset in the\nsecond category. We emphasize that the datasets in the second category did\nnot contribute any training data for our experiments.\nWe performed 10 independent experiments to obtain multiple partitions of\nthe datasets into two categories, and repeated the aforementioned procedure to\nprepare 10 separate (meta-training, meta-IT, meta-ET) triplets. This resulted\nin the following (average size +/- standard deviation) statistics for dataset sizes:\nmeta-training and meta-IT\n:\n173, 762 ± 10, 739\nmeta-ET\n:\n172, 565 ± 11, 915\nIn order to ensure symmetry of the binary similarity function, we introduced\nan additional meta-training pair for each meta-training pair in the meta-training\nset: in this new pair, we swapped the order of the feature vectors of the two\ninstances while replicating the covariance features of the underlying dataset that\ncontributed the two instances (the covariance features, being symmetric, carried\nover unchanged).\n6.4.2\nTraining neural models\nFor each meta-training set, we trained an independent deep net model with\n4 hidden layers having 100, 50, 25, and 12 neurons respectively over just 10\nepochs, and used batches of size 250 each.\nWe updated the parameters of\nthe model via the Adadelta7 implementation of the stochastic gradient descent\n(SGD) procedure supplied with the Torch library8 with the default setting of\nthe parameters, speciﬁcally, interpolation parameter equal to 0.9 and no weight\ndecay. We trained the model via the standard negative log-likelihood criterion\n(NLL). We employed ReLU non-linearity at each hidden layer but the last one,\nwhere we invoked the log-softmax function.\nWe tested our trained models on meta-IT and meta-ET data.\nFor each\nfeature vector in meta-IT (respectively meta-ET), we computed the predicted\nsame class probability. We added the predicted same class probability for the\nfeature vector obtained with ﬂipped order, as described earlier for the feature\nvectors in the meta-training set. We predicted the instances in the corresponding\npair to be in the same cluster if the average of these two probabilities exceeded\n0.5, otherwise we segregated them.\n6.4.3\nResults\nWe compared the meta approach to a hypothetical majority rule that had pre-\nscience about the class distribution.9 As the name suggests, the majority rule\npredicted all pairs to have the majority label, i.e., on a problem-by-problem ba-\nsis we determined whether 1 (same class) or 0 (diﬀerent class) was more accurate\n7described in http://arxiv.org/abs/1212.5701 .\n8available at: https://github.com/torch/torch7 .\n9Recall that we reduced the problem of clustering to pairwise binary classiﬁcation, where\n1 represented same cluster relations.\n18\nInternal test (IT)\nExternal test (ET)\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nAverage accuracy\nAverage binary similarity prediction accuracy\nMeta\nMajority\nFigure 6: Mean accuracy and standard deviation on meta-IT and meta-ET\ndata. Comparison between the fraction of pairs correctly predicted by the meta\nalgorithm and the majority rule. Recall that meta-ET, unlike meta-IT, was\ngenerated from a partition that did not contribute any training data. Nonethe-\nless, the meta approach signiﬁcantly improved upon the majority rule even on\nmeta-ET.\n19\nand gave the baseline the advantage of this knowledge for each problem, even\nthough it normally wouldn’t be available at classiﬁcation time. This information\nabout the distribution of the labels was not accessible to our meta-algorithm.\nFig. 6 shows the average fraction of similarity pairs correctly identiﬁed rel-\native to the corresponding pairwise ground truth relations on the two test sets,\nand the corresponding standard deviations across the 10 independent (meta-\ntraining, meta-IT, meta-ET) collections. Clearly, the meta approach outper-\nforms the majority rule on meta-IT, illustrating the beneﬁts of the meta ap-\nproach in a multi-task transductive setting. More interesting, still, is the signif-\nicant improvement exhibited by the meta method on meta-ET, despite having its\ncategory precluded from contributing any data for training. The result clearly\ndemonstrates the beneﬁts of leveraging archived supervised data for informed\ndecision making in unsupervised settings such as binary similarity prediction.\n7\nConclusion\nCurrently, UL is diﬃcult to deﬁne. We argue that this is because UL problems\nare generally considered in isolation. We suggest that they can be viewed as\nrepresentative samples from some meta-distribution µ over UL problems. We\nshow how a repository of multiple datasets annotated with ground truth labels\ncan be used to improve average performance. Theoretically, this enables us to\nmake UL problems, such as clustering, well-deﬁned.\nAt a very high level, one could even consider the distribution of data run\nthrough a popular clustering toolkit, such as scikit-learn.\nWhile there is a\ntremendous variety of problems encountered by such a group, an individual\nclustering course reviews may ﬁnd that they are not even the ﬁrst to wish to\ncluster course reviews using the tool.\nPrior datasets may prove useful for a variety of reasons, from simple to\ncomplex. The prior datasets may help one choose the best clustering algorithm\nor parameter settings, or shared features may be transferred from prior datasets\nthat can be identiﬁed as useful. We also demonstrate how the combination of\nmany small data sets can form a large data set appropriate for a neural network.\nThis work is meant only as a ﬁrst step in deﬁning the problem of meta-\nunsupervised-learning. The algorithms we have introduced can surely be im-\nproved in numerous ways, and the experiments are mainly intended to show that\nthere is potential for improving performance using a heterogeneous repository\nof prior labeled data.\nReferences\n[1] M. Ackerman and S. Ben-David. Measures of clustering quality: A working\nset of axioms for clustering. In NIPS, 2008.\n[2] M.-F. Balcan, A. Blum, and S. Vempala. A discriminative framework for\nclustering via similarity functions. In Proceedings of the Fortieth Annual\nACM Symposium on Theory of Computing (STOC), pages 671–680, 2008.\n20\n[3] M.-F. Balcan, A. Blum, and S. Vempala. Eﬃcient representations for life-\nlong learning and autoencoding. In Workshop on Computational Learning\nTheory (COLT), 2015.\n[4] H. Daum`e III and D. Marcu. A bayesian model for supervised clustering\nwith the dirichlet process prior.\nJournal of Machine Learning Research\n(JMLR), 6:1551–1577, 2005.\n[5] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and\nF. Hutter. Eﬃcient and robust automated machine learning. In Advances\nin Neural Information Processing Systems, pages 2962–2970, 2015.\n[6] T. Finley and T. Joachims.\nSupervised clustering with support vector\nmachines. In ICML, 2005.\n[7] L. Hubert and P. Arabie. Comparing partitions. Journal of classiﬁcation,\n2(1):193–218, 1985.\n[8] M. J. Kearns, R. E. Schapire, and L. M. Sellie. Toward eﬃcient agnostic\nlearning. Machine Learning, 17(2-3):115–141, 1994.\n[9] J. Kleinberg. An impossibility theorem for clustering. Advances in neural\ninformation processing systems, pages 463–470, 2003.\n[10] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions\non Knowledge and Data Engineering (TKDE), 22:1345–1359, 2010.\n[11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,\nM. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-\nsos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-\nlearn: Machine learning in Python. Journal of Machine Learning Research,\n12:2825–2830, 2011.\n[12] P. J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and\nvalidation of cluster analysis. Journal of computational and applied math-\nematics, 20:53–65, 1987.\n[13] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Ima-\ngeNet Large Scale Visual Recognition Challenge. International Journal of\nComputer Vision (IJCV), 115(3):211–252, 2015.\n[14] M. Sahlgren.\nThe word-space model:\nUsing distributional analysis to\nrepresent syntagmatic and paradigmatic relations between words in high-\ndimensional vector spaces. 2006.\n[15] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimiza-\ntion of machine learning algorithms. In Advances in neural information\nprocessing systems, pages 2951–2959, 2012.\n[16] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Auto-weka:\nCombined selection and hyperparameter optimization of classiﬁcation algo-\nrithms. In Proceedings of the 19th ACM SIGKDD international conference\non Knowledge discovery and data mining, pages 847–855. ACM, 2013.\n21\n[17] S. Thrun and T. M. Mitchell. Lifelong robot learning. In The biology and\ntechnology of intelligent autonomous agents, pages 165–196. Springer, 1995.\n[18] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business\nMedia, 2012.\n[19] L. G. Valiant. A theory of the learnable. Communications of the ACM,\n27(11):1134–1142, 1984.\n[20] R. B. Zadeh and S. Ben-David. A uniqueness theorem for clustering. In\nProceedings of the twenty-ﬁfth conference on uncertainty in artiﬁcial intel-\nligence, pages 639–646. AUAI Press, 2009.\n22\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV"
  ],
  "published": "2016-12-29",
  "updated": "2017-01-03"
}