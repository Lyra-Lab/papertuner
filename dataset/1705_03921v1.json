{
  "id": "http://arxiv.org/abs/1705.03921v1",
  "title": "Why & When Deep Learning Works: Looking Inside Deep Learnings",
  "authors": [
    "Ronny Ronen"
  ],
  "abstract": "The Intel Collaborative Research Institute for Computational Intelligence\n(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\nresearch from its foundation in 2012. We have asked six leading ICRI-CI Deep\nLearning researchers to address the challenge of \"Why & When Deep Learning\nworks\", with the goal of looking inside Deep Learning, providing insights on\nhow deep networks function, and uncovering key observations on their\nexpressiveness, limitations, and potential. The output of this challenge\nresulted in five papers that address different facets of deep learning. These\ndifferent facets include a high-level understating of why and when deep\nnetworks work (and do not work), the impact of geometry on the expressiveness\nof deep networks, and making deep networks interpretable.",
  "text": "WHY & WHEN DEEP LEARNING WORKS: LOOKING INSIDE DEEP LEARNING \n \n1 \nWhy & When Deep Learning Works: \nLooking Inside Deep Learning \nRonny Ronen \nronny.ronen@intel.com \nThe Intel Collaborative Research Institute for \nComputational Intelligence (ICRI-CI)1 \nIn recent years, Deep Learning has emerged as the leading technology for accomplishing broad \nrange of artificial intelligence tasks (LeCun et al. (2015); Goodfellow et al. (2016)). Deep learning is \nthe state-of-the-art approach across many domains, including object recognition and identification, \ntext understating and translation, question answering, and more. In addition, it is expected to play a \nkey role in many new usages deemed almost impossible before, such as fully autonomous driving. \nWhile the ability of Deep Learning to solve complex problems has been demonstrated again and \nagain, there is still a lot of mystery as to why it works, what is it really capable of accomplishing, and \nwhen it works (and when it does not). Such an understanding is important for both theoreticians and \npractitioners, in order to know how such methods can be utilized safely and in the best possible \nmanner. An emerging body of work has sought to develop some insights in this direction, but much \nremains unknown. The general feeling is that Deep learning is still by and large “black magic” we \nknow it works, but we do not truly understand why. This lack of knowledge disturbs the scientists \nand are a cause for concern for developers would you let an autonomous car be driven by a system \nwhose mechanisms and weak spots are not fully understood? \nThe Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been \nheavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We \nhave asked six leading ICRI-CI Deep Learning researchers to address the challenge of “Why & When \nDeep Learning works”, with the goal of looking inside Deep Learning, providing insights on how \ndeep networks function, and uncovering key observations on their expressiveness, limitations, and \npotential. \nThe output of this challenge call was quite impressive, resulting in five papers that address \ndifferent facets of deep learning. These papers summarize the researchers’ ongoing recent work \npublished in leading conferences and journals as well as new research results made especially for this \ncompilations. These different facets include a high-level understating of why and when deep networks \nwork (and do not work), the impact of geometry on the expressiveness of deep networks, and making \ndeep networks interpretable. \nUnderstating of why and when deep networks work (and do not work) \n1. Naftali Tishby and Ravid Schwartz-Ziv in Opening the Black Box of Deep Neural \nNetworks via Information study Deep Networks by analyzing their information-theoretic \nproperties, looking at what information on the input and output each layer preserves, and \nsuggests that the network implicitly attempts to optimize the Information-Bottleneck (IB) \ntradeoff between compression and prediction, successively, for each layer. Moreover, they \nshow that the stochastic gradient descent (SGD) epochs used to train such networks have two \n                                                             \n1  This work was done with the support of the Intel Collaborative Research institute for Computational \nIntelligence (ICRI-CI). This paper is the preface part of the ’Why & When Deep Learning works looking \ninside Deep Learning’ ICRI-CI paper bundle. \nWHY & WHEN DEEP LEARNING WORKS: LOOKING INSIDE DEEP LEARNING \n \n2 \ndistinct phases for each layer: fast empirical error minimization, followed by slow \nrepresentation compression. They then present a new theoretical argument for the \ncomputational benefit of the hidden layers \n2. Shai Shalev-Shwartz, Ohad Shamir and Shaked Shamma in Failures of Gradient-Based \nDeep Learning attempt to gain a deeper understanding of the difficulties and limitations \nassociated with common approaches and algorithms. They describe four families of problems \nfor which some of the commonly used existing algorithms fail or suffer significant difficulty, \nillustrate the failures through practical experiments, and provide theoretical insights explaining \ntheir source and suggest remedies to overcome the failures that lead to performance \nimprovements. \nThe impact of geometry on the expressiveness of deep networks \n3. Amnon Shashua, Nadav Cohen, Or Sharir, Ronen Tamari, David Yakira and Yoav \nLevine in Analysis and Design of Convolutional Networks via Hierarchical Tensor \nDecompositions analyze the expressive properties of deep convolutional networks. Through \nan equivalence to hierarchical tensor decompositions, they study the expressive efficiency and \ninductive bias of various architectural features in convolutional networks (depth, width, pooling \ngeometry, inter-connectivity, overlapping operations etc.). Their results shed light on the \ndemonstrated effectiveness of convolutional networks, and in addition, provide new tools for \nnetwork design. \n4. Nathan Srebro, Behnam Neyshabur, Ryota Tomioka and Ruslan Salakhutdinov in \nGeometry of Optimization and Implicit Regularization in Deep Learning argue that the \noptimization methods used for training neural networks play a crucial role in generalization \nability of deep learning models, through implicit regularization. They demonstrate that \ngeneralization ability is not controlled simply by network size, but rather by some other implicit \ncontrol. Then, by studying the geometry of the parameter space of deep networks and devising \nan optimization algorithm attuned to this geometry, they demonstrate how changing the \nempirical optimization procedure can improve generalization performance. \nInterpretability of deep networks \n5. Shie Mannor, Tom Zahavy and Nir Baram in Graying the black box: Understanding \nDQNs present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind \nmatter. They propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), \nand an algorithm that learns it automatically. Using these tools they reveal that the features \nlearned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. \nMoreover, they are able to look into the network to understand and describe the policies learned \nby DQNs for three different Atari2600 games and suggest ways to interpret, debug and \noptimize deep neural networks in reinforcement learning. \nReferences \nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT Press, 2016. \nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553): 436–444, \n2015. \n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-05-10",
  "updated": "2017-05-10"
}