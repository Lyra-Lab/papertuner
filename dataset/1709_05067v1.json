{
  "id": "http://arxiv.org/abs/1709.05067v1",
  "title": "Deep Reinforcement Learning for Conversational AI",
  "authors": [
    "Mahipal Jadeja",
    "Neelanshi Varia",
    "Agam Shah"
  ],
  "abstract": "Deep reinforcement learning is revolutionizing the artificial intelligence\nfield. Currently, it serves as a good starting point for constructing\nintelligent autonomous systems which offer a better knowledge of the visual\nworld. It is possible to scale deep reinforcement learning with the use of deep\nlearning and do amazing tasks such as use of pixels in playing video games. In\nthis paper, key concepts of deep reinforcement learning including reward\nfunction, differences between reinforcement learning and supervised learning\nand models for implementation of reinforcement are discussed. Key challenges\nrelated to the implementation of reinforcement learning in conversational AI\ndomain are identified as well as discussed in detail. Various conversational\nmodels which are based on deep reinforcement learning (as well as deep\nlearning) are also discussed. In summary, this paper discusses key aspects of\ndeep reinforcement learning which are crucial for designing an efficient\nconversational AI.",
  "text": "Deep Reinforcement Learning for Conversational AI\nMahipal Jadeja\nDA-IICT\nGandhinagar, India\nmahipaljadeja5@gmail.com\nNeelanshi Varia\nDA-IICT\nGandhinagar, India\nneelanshiV2@gmail.com\nAgam Shah\nDA-IICT\nGandhinagar, India\nshahagam4@gmail.com\nABSTRACT\nDeep reinforcement learning is revolutionizing the artiﬁcial in-\ntelligence ﬁeld. Currently, it serves as a good starting point for\nconstructing intelligent autonomous systems which oﬀer a beter\nknowledge of the visual world. It is possible to scale deep reinforce-\nment learning with the use of deep learning and do amazing tasks\nsuch as use of pixels in playing video games. In this paper, key\nconcepts of deep reinforcement learning including reward function,\ndiﬀerences between reinforcement learning and supervised learn-\ning and models for implementation of reinforcement are discussed.\nKey challenges related to the implementation of reinforcement\nlearning in conversational AI domain are identiﬁed as well as dis-\ncussed in detail. Various conversational models which are based\non deep reinforcement learning (as well as deep learning) are also\ndiscussed. In summary, this paper discusses key aspects of deep\nreinforcement learning which are crucial for designing an eﬃcient\nconversational AI.\nKEYWORDS\nDeep learning, deep reinforcement learning, conversational AI\n1\nINTRODUCTION\nArtiﬁcial intelligence is playing role everywhere - banking, educa-\ntion, healthcare, services and almost every important sector. One of\nthe key reason behind its success is conversational AI which has not\nonly led us from typing commands to speaking while we are doing\nsome other activity but also given us personal assistants which\nare almost humanlike in their speeches. Conversational AI will\nhelp us solve problems like language formation, context sensitive\nconversations, translation, beter identiﬁcation and other aspects\nwhich make the intelligent assistants more human-like. We have\nat our hand natural language processing, speech recognition, ma-\nchine learning, neural networks, deep learning and other domains\nto transform the way we perceive artiﬁcial intelligence.\nDeep/Hierarchical Learning is a subset of machine learning. It\nincludes various architectures and neural networks which work on\nthe information given to it. It works on the principle of knowledge\nbuilding. It also predicts or classiﬁes whether the knowledge is\nrelevant or falls into which category. Reinforcement learning is one\nthe three - supervised, unsupervised and reinforcement learning\nwhich is able to train a network by means of trial and error that\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nICTIR’ 17 Workshop on Search-Oriented Conversational AI (SCAI’ 2017), Amsterdam,\nNetherlands\n© 2016 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00\nDOI: 10.1145/nnnnnnn.nnnnnnn\nis by punishing for error and rewarding for correct results. Te\ndeep reinforcement learning branch has emerged from the notion\nof training an artiﬁcially intelligent agent like human that is, to\ngive it knowledge and improve by rewarding or punishing. A lot\nof research has already proved to beter and beter from what we\nhad been seeing till years and it is expected that it will be one of\nthe cornerstones for the dream future of AI.\n2\nDEEP REINFORCEMENT LEARNING IN\nCONVERSATIONAL AI\nComputers that can play games have always impressed the comput-\ning world. For computing world, computer machines that can play\ngames excellently is always a topic of interest. In a breakthrough\npaper published by DeepMind (London based company), with the\nuse of Deep reinforcement learning, automated Atari playing [1]\nwas demonstrated. Afer around one month of this amazing work,\nthe company DeepMind was bought by Google. Afer Google’s en-\ntry in this ﬁeld, there is a lot of buzz about reinforcement learning\nin the ﬁeld of AI. A relatively recent success by Google is AlphaGo\n[2] (artiﬁcial agent) who has won against the Go champion of the\nworld.\nBasics of Reinforcement Learning\nReinforcement learning is related to three broad ﬁelds namely 1)\nArtiﬁcial Intelligence, 2) animal psychology and 3) control theory.\nTe idea is to have a robot/person/animal/deep net who is trying\nto learn to navigate in an environment which is dynamic and un-\ncertain. Te goal of the autonomous agent is to maximize a reward\n(see Figure 1) and generally, this reward is a quantitative entity\n(numeric).\nIt is easy to understand this concept with the help of sports. In the\ncase of Tennis, we can think about following actions of the virtual\nagent: serves, returns, volleys. Te state of the game depends upon\nthe smart selection of these actions. Here, the goal is to perform\nseries of actions in order to win a point, game, set as well as match.\nSo this numeric reward is always being considered by the virtual\nagent. Te objective of the agent is to implement a strategy or a\nset of strategies in order to get best possible score. In other words,\nthe objective is to maximize the scoring function of the game. Here\nthe issue is: the state of the game is not static. Depending upon the\nactions of agents the state will change rapidly which makes such\ntype of modeling very tricky. Input for this type of model is the\npresent state of the game as well as an action and it is supposed to\ngenerate the best possible value for scoring function as an output.\nBut this scheme is just for one step whereas the overall objective is\nto win the game. Terefore the agent has to consider all the actions\nfrom the current state to the possible ﬁnal state. Terefore, this\nmodeling approach is highly application dependent since for each\napplication the scoring function will be diﬀerent. So one cannot\narXiv:1709.05067v1  [cs.AI]  15 Sep 2017\nuse the same strategy which is used in building of Tennis agent for\nChess agent and vice versa.\n1\nFigure 1: Deep Reinforcement Learning Agent (1)\nEarly Models of DRL\nIn the case of Atari agent, convolutional neural network (with a\nlot of adjustments) was build by researchers with the help of Atari\nscreenshots. Te scoring function was dependent upon a target\nnumber (maximum possible reward) not a class.\nAnother model is Deep Q-Network which is also known by\nDQN and again this is a contribution from Google [3]. It uses the\nsame underlying principle: to maximize the reward points with the\ngiven state and action. DQN oﬀers improvements including but not\nlimited to Experience Relay, Dueling Network Architecture.\nReinforcement learning vs Supervised Learning\nReinforcement learning is not at all rewording of supervised learn-\ning. In the case of supervised learning, the historical examples are\nused in order to understand the environment but this approach\ndoes not necessarily the best. Consider the example of a car driving\nin heavy traﬃc to understand diﬀerences. In the case of super-\nvised learning, the idea is to use the past data (let’s say 2 weeks\nbefore) in order to establish road paterns and use those paterns\nin the current scenario. But here the problem is it may possible\nthat 2 weeks before, the roads were very clear in terms of traﬃc\nand today in a heavy traﬃc scenario, the available information\nis not that much useful and there is no eﬀective way to use it in\norder to obtain best results. Whereas, in the case of reinforcement\nlearning, the focus is on rewards. Te driver will get points for\nhis/her action. Actions like maintaining speed of the vehicle less\nthan the speed limit, lane driving, proper signaling as and when\nrequired etc. Negative points are given for undesirable actions like\nspeeding and tailgating. Here the objective function is to maximize\nthe points and input is the current state of the traﬃc and action.\nReinforcement learning focuses more on a change of the current\nstate of the environment afer each action and supervised learning\nmodels don’t consider it.\n1\nSource:\nhtps://ai2-s2-public.s3.amazonaws.com/ﬁgures/2016-11-\n08/ec4a764e062153c911097495c7e4b7e93612b75d/2-Figure1-1.png\n2\nFigure 2: Summary of Reinforcement Learning (2)\n2.1\nStudy of Challenges in Reinforcement\nLearning in Conversational AI Domain\nDetailed study of reinforcement learning is beyond the scope of\nthis paper. So, instead of discussing mathematical equations or\nalgorithms, we focus on challenges associated with reinforcement\nproblems.\nBackground: Reward Functions\nTe reward functions provide a signal/feedback which is an\nindicator of the performance of the system with respect to the\nunderlying action. Tey also indicate the importance of each action\nby considering value addition by each action towards achieving the\nﬁnal goal/solution. Supervised learning actually indicates which\ntype of action is correct and should be taken by user whereas in the\ncase of reinforcement learning, the only signal is given depending\nupon how good/bad the action is for achieving overall goal. Tere\nis no notion of correctness of local actions.\nTere are two diﬀerent ways to deﬁne reward functions for con-\nversational AI: 1) Sparse functions 2) Non-sparse functions. Sparse\nfunctions are easy to design/deﬁne but very hard to solve whereas\nnon-sparse functions are diﬃcult to design but very easy to solve.\nIn the case of sparse functions there is no signaling mechanism i.e.\nthe user won’t get feedback for his local choices and at terminal\nstage only, he/she will get information about whether the desired\noutput is achieved or not. For example, consider sparse function de-\nﬁned for playing chess, where no feedback is given for local actions\n(moves) but towards the end of the game the user gets information\nabout whether he/she has achieved the goal (winning the game) or\nnot. In the case of a non-sparse function, depending on the useful-\nness of the function in achieving the ﬁnal desired objective, signals\nare provided to the user. So that the user can drive the system for\nachieving an overall optimal solution.\n2 Source: htps://qph.ec.quoracdn.net/main-qimg-b135e50fd568eac846f112ee8a0a1bbc\n2\nWe can make an analogy between the success of companies with\nrewards. Traditional conventional approach for most of the compa-\nnies is to achieve ﬁnite limited rewards (proﬁts) with known odds\nwhereas other companies like Amazon wants to achieve out-sized\nmassive rewards at long odds. Te later type of companies prefer\nexploration of new possibilities. In the case of reinforcement learn-\ning, the idea is to select one path which gives the maximum value\nof expected reward by exploring trade-oﬀbetween exploitation and\nexploration. It may possible that a company gets a massive success\nafer a long string of failures and the same thing is possible for\nrewards too. Terefore one can’t ignore exploration part. Summary\nof reinforcement learning model is shown in Figure 2.\nKey Challenges:\nChallenge 1: Multiple goals in the case of conversational AI\nTere are several objectives of a conversational AI including 1) Ro-\nbust performance 2) Meaningful /informative interaction with the\nuser 3) Provide excellent user experience 4) Oﬀer personalization.\nSo naturally, in the case of conversational AI, single reward\nfunction is not suﬃcient. Te next challenge is how to assign\nweights to these goals/objectives? i.e. how much importance should\nbe given to each of the desired objectives. In summary, it is hard to\ndesign reward function which include these many challenges with\nappropriate weights.\nChallenge 2: Trade-oﬀbetween various goals\nTe next challenge is to handle trade-oﬀbetween the goals. For\nexample, it is diﬃcult to oﬀer extreme personalization as well as\neﬃcient performance for all the messages for a conversational AI.\nSo how to achieve optimal behavior in this scenario?\nAccording to us, designing a weighting scheme in order to com-\nbine several goals is the biggest challenge for conversational AI\nsince most of the goals of conversational AI are depending upon\nusers’ experience which is diﬃcult to quantify. Some type of auto-\nmated negotiation between diﬀerent goals is desirable using which\nit may possible to combine several objectives in a single way(action).\nBut again, trading between diﬀerent goals while considering the\nunderlying environment is a very hard task. In the conversational\nAI there is also trade-oﬀin generating dialogue between 1) length\nof dialogue 2) Diversity of dialogue and again 3) personalization.\nChallenge 3: Coherent dialogue design\nTe agent should generate consistent response while generating\nanswer for semantically identical input. For example, if a user ask\nquestion like ”Where do you live now?” and ”In which country do\nyou live now?” he/she wants the same answer in both the cases.\nTis problem looks simple but it is diﬃcult to implement since the\nunderling model should also generate linguistic plausible answer.\nHere, training data is huge and it consists data from multiple diﬀer-\nent users. A Persona-Based Neural Conversation Model is making\nﬁrst steps into the direction of explicitly modelling a personality\n[6].\nChallenge 4: Evaluate conversational agents\nWe can evaluate conversational agent by both subjective and ob-\njective evaluation technique. Te subjective evaluation technique\nconsiders users’ experiences of diﬀerent aspects of the conversa-\ntions, while the objective evaluation technique are based on an\nanalysis of the logs of the actual conversations. Tese evaluation\nmethods are well described in the literature [11]. Reward function\ncan send feedback to agent based on this evaluation.\nEvaluation of conversational agents depends upon quantitative\nas well as qualitative features and most of the qualitative features\nare user dependent. Terefore, we feel that extreme personalization\nand universal deﬁned metrics for qualitative features are the biggest\nchallenges for evaluation of conversational agents.\n2.2\nDeep Reinforcement Learning based\nConversational Models\nDeep reinforcement learning is emerging area for development of\nconversational models [7]. Idea is to learn conversational patern\nvia trial and error method. Such training is performed via clients\nor a dialogue set predeﬁned in computers. A huge dataset is re-\nquired to train the deep neural network and so automatic chatbot\nalgorithms are applied for training. For providing such training\nBayesian models, Markov models, etc. have been developed. It is a\ngreat challenge to be able to model such algorithms which are ac-\ncurate enough to train the reinforcement networks which includes\ngathering of relevant and sometimes speciﬁcally irrelavant dataset,\nsemantics,etc. While following a statistical approach. To model\ndynamic training algorithm, human clients with enough knowledge\nand clarity of purpose or intelligent enough AI devices/algorithms\nare required which is a problem we face currently.\nTe above mentioned sequence to sequence model is able to\ngenerate dialogues given a conversation and context pre hand based\non maximum likelihood estimation but it generates a very high\namount of responses which in a way means that the intelligent agent\nis unable to answer. Tis model works on reward and punishment\nstrategy like any other reinforcement model unlike MLE which\nhelps in building long conversational training and learning for the\nAI assistant. Supervised learning in AlphaGo style strategy and\noptimisation techniques are also applied for the achievement.\nBy using large data and computing resources, the rise of deep\nreinforcement learning has boosted our ability to build computa-\ntional models which are applicable in our lives. Te AI bots built\nwith Deep Reinforcement Learning understand the semantics of\nall domains and are capable of scaling. Tis advancement allows\nus to solve dialogue problems in various domains. Te behaviour\nbased on random, rule-based and supervision based learning out-\nperformed by DRL based learned policies. A report of experiments\nconcludes that the DRL-based policy has a 53% win rate versus 3\nautomated players, whereas a supervised player trained on a dia-\nlogue corpus in this seting achieved only 27%, versus the same 3\nbots [10]. Te above results prove that DRL is a reliable framework\nfor training dialogue systems and strategic agents with negotia-\ntion abilities. Te experimental results report that all DRL agents\nsubstantially outperform all the baseline agents.\n2.3\nDeep Learning based Conversational\nModels\nDeep learning along with other emerging areas has greatly im-\npacted the way we perceive artiﬁcial intelligence. With context to\ndevelopment of conversational artiﬁcial intelligence, deep learning\nhas been able to make great leaps. Evolution of deep learning [9] is\nshown in Figure 3.\n3\nVarious aspects pertaining to speech and conversations have\nbeen addressed in the past and much work has to be done in the\nareas speciﬁc to conversational AI, speech recognition and natural\nlanguage processing. Tis section addresses various models devel-\noped, comparison and further scope of research and challenges.\nNeural networks and deep learning help in user (model) based\nlearning. Te present intelligent personal assistant models are\nable to address any conversation at hand brieﬂy. Recurrent Neu-\nral Network models [4] are able to address ’intention’ apart from\n’atention’. Origin RNN encodes the inputs so as to make the con-\nversation more intentional and continuable whereas the destination\nneural network is able pay atention to speciﬁc words of user so\nas to keep learn from them and reply accordingly. Te model is\ndivided into three parts - ﬁrst one collects the words and sentences\nspoken/entered by the user, second one captures the context of\nthe conversation by various parts of sentence and third one saves\nvarious characteristics, objects, etc. Tis model is one step towards\nthe intelligent assistant becoming more human-like and having\nconversations which are contextual and not absolute.\nTe conversational AI is not limited to Siri, Cortana, Ok Google,\netc. but is used in various mobile and web applications in the do-\nmains of healthcare, education, banking, etc. Some of these models\nare very speciﬁc based on their real-time application. Te data\nfed in and the algorithm applied determines the eﬃciency of the\npersonal assistant. One such important leap is in the education\nﬁeld. Smart boards and computers are limited in performance in\nthe sense it depends on how the student accesses various materials.\nAuto-tutor is a conversational AI based model which teaches con-\ncepts via dialogues/conversations. Instead of providing information\ndirectly, it builds the knowledge based on questions posed by the\nagent as well as student. Te tutor [5]. also possesses expressions,\ngestures, dialogues in natural language, etc.\n3\nFigure 3: Deep learning evolution (3)\nAnother aspect that is important is to take care of multi-users.\nTat is, it might be possible that device is used by various clients\nfor a particular use. In that case, the intelligent assistant should\nbe able to serve according to the speaker. A neural model based\non characteristics of the speaker [6] has been developed. It takes\n3\nSource:\nhtps://image.slidesharecdn.com/deeplearningframeworksslides-\n160531204623/95/deep-learning-frameworks-slides-6-638.jpg?cb=1464727714\ncare of output based on speaker, their characteristics, speaking\nmethods, language and background knowledge. A speaker model\nis generated based on inputs and that records the features required\nto impersonate a model which has humanlike behaviour. Each\nindividual speaker is considered as a vector that helps in encoding\ndetails regarding that particular user. Further, what we need is\nto take care of user data leakage and tampering by users. Speech\nrecognition along with deep neural networks play an important\nrole for development of these methods.\nDeep Neural Networks (DNNs) are also another set of algorithms\nbased on which dialogue generation and simulation models have\nbeen developed. Te LSTM (Long-Short Term Memory) based ap-\nproach vectors a dialogue system. It then includes various layers\nof vectored architectures to get the output. DNNs are able to per-\nform parallel computing in a very optimised way which thus makes\ndialogue generation easier. But only those datasets whose inputs\nand outputs have a ﬁxed dimension can be modelled which is a big\nlimitation and working towards this has to be one of our future\ngoals via sequential learning algorithms [8], feedforward neural\nnetworks, recurrent neural networks, etc.\n3\nCONCLUSIONS\nReinforcement learning is the sub domain of artiﬁcial intelligence\nand it focuses on aspects like perception, goal seting and planing.\nReinforcement learning has potential for combining AI with other\nengineering disciplines. We conclude that reinforcement learning\nis simple yet powerful technique and it has a tremendous potential\nto contribute in the advancement of conversational based AI. For\nconversational AI, most of the challenges related to reinforcement\nlearning are related to reward functions and therefore how to quan-\ntify user experiences/personalization in terms of reward function\nis one of the future direction of research. Since there are multiple\ngoals in the case of conversational AI, the equally critical question\nis how to handle trade-oﬀbetween various goals.\nAs seen in Section 2, various mentioned models have their own\nachieved results and limitations. Combining them together to ob-\ntain all functionalities in one set to obtain a near perfect intelligent\nassistant is a future direction to work on. Individually, speech\nrecognition, NLP, NN, Deep Learning, etc. have excelled in the ﬁeld\nof producing conversational intelligence but to combine them to\novercome limitations of diﬀerent areas is where we should start\nworking. Limitations consist of building a dataset, seting up the\ncontext of convention, detecting a speaker, performing a particular\ntask and most importantly replying in a humanlike manner which\nincludes features like natural language, sentence formation and\ntranslation, continuable reply.\n4\nREFERENCES\n(1) Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,\nI., Wierstra, D., & Riedmiller, M. (2013). Playing atari with\ndeep reinforcement learning. arXiv preprint arXiv:1312.5602.\n(2) Chen, J. X. (2016). Te evolution of computing: AlphaGo.\nComputing in Science & Engineering, 18(4), 4-7.\n(3) Van Hasselt, H., Guez, A., & Silver, D. (2016, February).\nDeep Reinforcement Learning with Double Q-Learning. In\nAAAI (pp. 2094-2100).\n4\n(4) Yao, K., Zweig, G., & Peng, B. (2015). Atention with in-\ntention for a neural network conversation model. arXiv\npreprint arXiv:1510.08565.\n(5) Graesser, A. C., VanLehn, K., Rose, C. P., Jordan, P. W., &\nHarter, D. (2001). Intelligent tutoring systems with conver-\nsational dialogue. AI magazine, 22(4), 39.\n(6) Li, J., Galley, M., Brocket, C., Spithourakis, G. P., Gao, J.,\n& Dolan, B. (2016). A persona-based neural conversation\nmodel. arXiv preprint arXiv:1603.06155.\n(7) Li, J., Monroe, W., Riter, A., Galley, M., Gao, J., & Juraf-\nsky, D. (2016). Deep reinforcement learning for dialogue\ngeneration. arXiv preprint arXiv:1606.01541.\n(8) Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to\nsequence learning with neural networks. In Advances in\nneural information processing systems (pp. 3104-3112).\n(9) LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning.\nNature, 521(7553), 436-444.\n(10) Cuayahuitl, H., Keizer, S., & Lemon, O. (2015). Strategic\ndialogue management via deep reinforcement learning.\narXiv preprint arXiv:1511.08099.\n(11) Silvervarg, A., & Jonsson, A. (2011, July). Subjective and\nobjective evaluation of conversational agents in learning\nenvironments for young teenagers. In Proceedings of the\n7th IJCAI Workshop on Knowledge and Reasoning in Prac-\ntical Dialogue Systems.\n5\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2017-09-15",
  "updated": "2017-09-15"
}