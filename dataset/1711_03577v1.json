{
  "id": "http://arxiv.org/abs/1711.03577v1",
  "title": "What Really is Deep Learning Doing?",
  "authors": [
    "Chuyu Xiong"
  ],
  "abstract": "Deep learning has achieved a great success in many areas, from computer\nvision to natural language processing, to game playing, and much more. Yet,\nwhat deep learning is really doing is still an open question. There are a lot\nof works in this direction. For example, [5] tried to explain deep learning by\ngroup renormalization, and [6] tried to explain deep learning from the view of\nfunctional approximation. In order to address this very crucial question, here\nwe see deep learning from perspective of mechanical learning and learning\nmachine (see [1], [2]). From this particular angle, we can see deep learning\nmuch better and answer with confidence: What deep learning is really doing? why\nit works well, how it works, and how much data is necessary for learning. We\nalso will discuss advantages and disadvantages of deep learning at the end of\nthis work.",
  "text": "What Really is Deep Learning Doing? ∗\nChuyu Xiong\nIndependent Researcher, New York, USA\nEmail: chuyux99@gmail.com\nAbstract\nDeep learning has achieved a great success in many areas, from computer vision to natural\nlanguage processing, to game playing, and much more. Yet, what deep learning is really doing is\nstill an open question. There are a lot of works in this direction. For example, [6] tried to explain\ndeep learning by group renormalization, and [5] tried to explain deep learning from the view of\nfunctional approximation. In order to address this very crucial question, here we see deep learning\nfrom perspective of mechanical learning and learning machine (see [1], [2]). From this particular\nangle, we can see deep learning much better and answer with conﬁdence: What deep learning is\nreally doing? why it works well, how it works, and how much data is necessary for learning. We\nalso will discuss advantages and disadvantages of deep learning at the end of this work.\nKeywords: Mechanical learning, learning machine, deep learning, X-form, universal\nlearning machine, internal representation space, data sufficiency\n1\nIntroduction\nIn recent years, deep learning (a branch of machine learning) has achieved many successes in lot of\nﬁelds. However, a clear theoretical framework of deep learning is still missing. Consequently, there are\nmany fundamental questions about deep learning are still open. For example: What is deep learning\nreally doing? Is it really learning, or just a kind of fancy approximation to a function? Why it indeed\nhas so many success? Why deep learning needs big data? For a particular problem, how much data\nis suﬃcient to drive deep learning to learn? Up to now, there is no satisfactory answer for these\nfundatamental questions. We here are trying to address these questions from a new angle.\nWe introduced term ”Mechanical learning” in [1]. Mechanical learning is a computing system that is\nbased on a simple set of ﬁxed rules (this is so called mechanical), and can modify itself according to\nincoming data (this is so called learning). A learning machine is a system that realizes mechanical\nlearning.\nIn [2], we described learning machine in a lot of details. By doing so, we gained some useful knowledges\nand insights of mechanical learning.\nIn this short article, by using those knowledges and insights, we are trying to view deep learning from\na new angle. First, we will brieﬂy discuss learning machine, pattern, internal representation space,\nX-form, data suﬃciency, and learning strategies and methods. Then, we will use the view of learning\nmachine to see deep learning. We start from the simplest deep learning, i.e., 2-1 RBM, then go to\n3-1 RBM, N-1 RBM, N-M RBM, stacks of RBMs. Then we discuss the learning dynamics of deep\nlearning. By this approach, we see clearly what deep learning is doing, why deep learning is working,\nunder what condition deep learning can learn well, how much data are needed, and what disadvantages\ndeep learning has.\n2\nMechanical Learning and Learning Machine\nWe here very brieﬂy sum up the discussions that we did in [1] and [2]. A learning machine M has 2\nmajor aspects: it is an IPU, i.e. it is able to process information; and it is learning, i.e. its information\n∗Great thanks for whole heart support of my wife.\n1\narXiv:1711.03577v1  [cs.LG]  6 Nov 2017\n2\nWhat really is deep learning?\nprocessing ability is changing according to data. Without learning (since it is a machine we design,\nwe can stop learning), M is very similar to a CPU. However, one major diﬀerence between learning\nmachine and CPU is: learning machine treat incoming data according to its pattern, not bit-wise.\nThus, in order to understand a learning machine, it’s absolutely necessary to understand pattern well.\nThere are 2 kinds of patterns: objective pattern and subjective pattern. Subjective pattern is crucial\nfor learning machine. In [2], we proved one theorem: For any objective pattern Po, we can ﬁnd a\nproper subjective pattern p that can express Po well and is build upon a least set of base patterns.\nTo describe subjective patterns, it is best to use X-form, which is one algebraic expression upon some\nbase patterns. X-form is one very important mathematical object. X-form could have sub-forms.\nX-form and its sub-forms actually forms the fundamental fabric of a learning machine.\nWe also deﬁned learning by teaching and learning without teaching. Then, further specify typical\nmechanical learning. Learning by teaching requires we know learning machine and the pattern to\nlearn well. By these knowledges, we can design a teaching sequence to make learning machine learn\nwell. We proved that if a learning machine has certain capabilities for learning by teaching, it is\nuniversal, i.e. able to learn anything.\nHowever, most learning is not learning by teaching. In order to understand typical mechanical learning,\nwe introduced internal representation space. Structurally, a learning machine M has components:\ninput space, output space, internal representation space, and learning methods and learning strategies.\nThe most important part is internal representation space. We studies internal representation space\nE in details, and revealed, in fact, it is equivalent to a collection of X-forms.\nThis fact tells us\nthat learning is nothing but a dynamics on E, moving from one X-form to another.\nWith clear\nand reachable internal representation space, learning can be understood much better, and can be\ndone much more eﬃciently.\nFor example, we can unify all 5 kinds of learning – logic reasoning,\nconnectionism, probabilistic approach, analogy, and evolution (see [3]) – together on E naturally.\nFor mechanical learning, we need to understand data suﬃciency. This is very crucial concept. We\nuse X-form and its sub-forms to deﬁne data suﬃcient to support one X-form and suﬃcient to bound\none X-form. With suﬃcient data, we can see how learning strategy and learning method work. There\ncould be many learning strategies and learning methods. We show 3 learning strategies: 1. Embed\nX-form into parameter space. 2. Squeeze X-form from inside to higher abstraction. 3. Squeeze X-\nform from inside and outside to higher abstraction. We prove that with certain capabilities, the last\n2 strategies and methods will make universal learning machine. Of course, this is theoretical results,\nsince we have not designed a speciﬁc learning machine yet.\nHere, we will show that deep learning is actually doing mechanical learning by the ﬁrst strategy, i.e.\nembed X-forms into parameter space. Such a fact will help us to understand deep learning much\nbetter.\n3\nSee Deep Learning from View of Learning Machine\nAccording to our deﬁnition, if without human intervention, deep learning is mechanical learning. Of\ncourse, this ”if” is a big if. Often, deep learning program is running with a lot of human intervention,\nspecially at the time of model set up. We will restrict our discussion to Hinton’s original model [4],\ni.e., a stack of RBMs. Each level of RBM is clearly a N-M learning machine (N andM are dimensions\nof input and output). Hinton’s deep learning model is by stacking RBM together. If without further\nhuman intervention, it is a learning machine. This is the original model of deep learning. Other deep\nlearning program can be thought as variations based on this model. Though in the past few years deep\nlearning has leaped forward greatly, stacking RBM together still reﬂects the most typical properties\nof deep learning.\nThus, we would expect many things in mechanical learning could be applied to deep learning. The\npoint is, we are viewing deep learning from a quite diﬀerent angle – angle of mechanical learning. For\nexample, we can view Hinton’s original deep learning program [4] as one 258-4 learning machine, and\nwe ask what is the internal representation space of it? We expect such angle and questions would\nreveal useful things for us.\nThe central questions indeed are: what is the internal representation\nspace of deep learning? what is the learning dynamics? At ﬁrst, seems it is quite hard since learning\nis conducted on a huge parameter space (dimension could be hundreds of millions), and learning\nmethods are overwhelmingly a big body of math. However, when we apply the basic thoughts of\nlearning machine to deep learning, starting from simplest RBM, i.e. 2-1 RBM, we start to see much\nChuyu Xiong\n3\nmore clearly.\n2-1 RBM\n2-1 RBM is the simplest. However, it is also very useful since we can examine all details, and such\ndetails will give us a good guide on more general RBM.\n2-1 RBM is one IPU. We know 2-1 IPU totally has 16 processing (222). But, we only consider those\nprocessing: p(0, 0) = 0, so totally 8 processing, which we denote as Pj, j = 0, . . . , 7 (see [1]). For 2-1\nRBM, any processing p can be written as: for input (i1, i2), the output o is:\no = p(i1, i2) = Sign(ai1 + bi2), where (a, b) ∈R2, Sign(x) =\n\u001a 1\nif x ≥0\n0\nif x < 0\nThe parameters (a, b) determine what the processing really is. Parameter space R2 has inﬁnite many\nchoices of parameters. But, there are only 6 processing, thus, for many diﬀerent parameters, the\nprocessing is actually same. We can see all processing in below table:\nP0\nP1\nP2\nP3\nP4\nP5\nP6\nP7\n(0,0)\n0\n0\n0\n0\n0\n0\n0\n0\n(1,0)\n0\n1\n0\n1\n0\n1\n0\n1\n(0,1)\n0\n0\n1\n0\n1\n1\n0\n1\n(1,1)\n0\n0\n0\n1\n1\n0\n1\n1\nRegion\nR4\nR3\nR5\nR2\nR6\nNone\nNone\nR1\nX-form\n0\nb1\nb2\nb1 + b3\nb2 + b3\nb1 + b2\nb3\nb1 + b2 + b3\nTab. 1. Table for all processing of 2-1 RBM\nFig. 1. Parameter space of 2-1 RBM that is cut into 6 regions\nIn ﬁrst row of table, Pi, i = 0, . . . , 7 are all processing of 2-1 IPU. Under ﬁrst row, there is value table\nfor each processing. We point out some quite familiar processing: P7 is actually logical OR gate, P6\nis logical AND gate, P5 is logical XOR gate. Note, P5, P6 are processing for 2-1 IPU, but not in 2-1\nRBM. It is well known, 2-1 RBM has no XOR and AND (i.e. no P5, P6).\nRj, j = 1, . . . , 6 indicate regions in parameter space R2, each region for one processing. There are\nonly 6 regions, since 2-1 RBM only has 6 processing. We brieﬂy discuss how we get these regions. See\nillustration in ﬁgure.\nSuppose p is processing. Line a = 0 cuts R2 into 2 regions: a ≥0 and a < 0. If (a, b) is in ﬁrst region,\nthen p(1, 0) = 1, in second, p(1, 0) = 0. Line b = 0 is perpendicular to a = 0, so, it cuts the previous\n4\nWhat really is deep learning?\n2 regions into 4 regions: a ≥0, b ≥0 and a ≥0, b < 0 and a < 0, b ≥0 and a < 0, b < 0. Clearly, if\nb ≥0, p(0, 1) = 1, if b < 0, p(0, 1) = 0. Line a + b = 0 could no longer cuts the previous 4 regions into\n8 regions, it could only cut 2 regions (2nd, 4th quadrant) into 4 regions (R2, R3, R5, R6). So, totally,\nwe have 6 regions, and each region is for one processing. This argument about regions is very simple,\nyet very eﬀective. We can extend this argument to N-1 RBM.\nEach region is for one processing. So, region can be used to represent processing. That is 6th row\nin the table shown. Yet, a much better expression is by X-form ([2]). We explain them here. Here\nb0 = (0, 0), b1 = (1, 0), b2 = (0, 1), b3 = (1, 1) are base patterns. For 2-dim pattern space, there are\nonly these 4 base patterns. But, bi can also be used to represent one processing of 2-1 IPU, i.e. bi\nis such a processing: when input is bi, output is 1, otherwise output is zero. X-forms are expressions\nbased on all N-1 base patterns, operations +, ·, ¬, composition, Cartesian product, and apply them\nconsecutively. Example, b1 + b3, b1 · b2, b1 + ¬b2 are X-forms. Any processing of 2-1 IPU can be\nexpressed by at least one X-form [2]. For example, if region is R3, processing is P1, X-form is b1.\nAnother example, region is R1, processing is P7 (this is OR gate), X-form is b1 + b2 + b3. P5 is a\nprocessing of 2-1 IPU (XOR gate), but not in 2-1 RBM, its X-form is b1 +b2. The 7th row in the table\nshows X-forms representing processing. We can say that each processing is represented by a region,\nand by a X-form as well.\nWhen 2-1 RBM is learning, clearly, parameter (a, b) is adapting. But, only when (a, b) cross region,\nprocessing changes. Before crossing, change of parameters is just for preparation for crossing (perhaps\nmany parameter changes are just wasted).\nLearning is moving from one region to another.\nOr,\nequivalently, learning is from one X-form to another. Such view is crucial. Now, we are clear, on\nsurface, learning on 2-1 RBM is a dynamics on parameter space R2, but real learning dynamics is on\n6 regions (or X-forms). Such indirectness causes a lot of problem.\n3-1 RBM\nJust increase input dimension 1, we have 3-1 RBM. To discuss it, we can gain some insights for general\nRBM. For 3-1 RBM, still we can write: for any input (i1, i3, i3) ∈B3, output o ∈B is:\no = p(i1, i2, i3) = Sign(ai1 + bi2 + ci3), where (a, b, c) ∈R3, Sign(x) =\n\u001a 1\nif x ≥0\n0\nif x < 0\nHowever, while we can easily write down all possible processing of 2-1 RBM, it would be hard to do\nso for 3-1 RBM. For 3-1 IPU, we know that the number of all possible processing is 223 = 28 = 256.\nSince only considering such processing p: p(0, 0, 0) = 0, the number becomes 256/2 = 128. We expect\n3-1 RBM has less processing. But, how many possible processing of 3-1 RBM could have?\nFollowing the guidance that 2-1 RBM gives to us, i.e.\nto consider the hyperplane generated by\nnonlinearity that cuts parameter spaces, we examine parameter space (a, b, c) ∈R3, and following\nplanes: a = 0, b = 0, c = 0, a + b = 0, a + c = 0, b + c = 0, a + b + c = 0.\nThese planes are\nnaturally obtained. For example, if we consider the input (1, 0, 0), it is easy to see plane a = 0 is\nwhere cut the value of output: 1 or 0. So, the above planes are associated with following inputs:\n(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)\nWe can clearly see that in one region that is cut out by above 7 planes, the output values are same.\nTherefore, one region actually is representing one processing: in the region, processing is same. So,\nquestion of how many possible processing becomes how many possible regions cut out by the 7 planes.\nWe do counting for the regions below.\nFirst, a = 0 cuts parameter space into 2 pieces: R1\n1, R1\n2. Second, b = 0 perpendicular to a = 0,\nso, it cuts each region R1\n1, R1\n2 into 2 pieces, we then have 4 regions: R2\n1, R2\n2.R2\n3, R2\n4. Then, c = 0\nperpendicular to a = 0 and b = 0, so, we have 8 regions: R3\nj, j = 1, . . . , 8. Then, consider a + b = 0.\nThis plane no longer could be perpendicular to all a = 0, b = 0, c = 0. We will not have 2 ∗8 = 16\nregions. We only have 1.5 ∗8 = 12 regions. Following the same argument, we have: For a + c = 0,\n1.5 ∗12 = 18 regions. For b + c = 0, 1.5 ∗18 = 27 regions. For a + b + c = 0, 1.5 ∗27 < 41 regions.\nSo, for 3-1 RBM, there are at most 41 possible processing, comparing to 128 possible processing of\nfull 3-1 IPU. However, there are possibility that the number of processing is even less than 41, since\namong those regions, it is possible that 2 diﬀerent regions give same processing. We do not consider\nthese details here.\nSince regions can be represented by X-form, each processing 3-1 RBM can be represented by at least\none X-form. b1 = (1, 0, 0), b2 = (0, 1, 1), b3 = (0, 0, 1), . . . , b7 = (1, 1, 1) are X-form for all base patterns.\nChuyu Xiong\n5\nExamples, X-form b1 +b2 is in 3-1 RBM. But, b1 ·b2 is not in 3-1 RBM. There are a lot of such X-form\nthat is not in 3-1 RBM.\nLearning dynamics on 3-1 RBM is also in such way: on surface, it is dynamics on R3, but real learning\ndynamics is on 41 regions (or X-forms).\nN-1 RBM\nThe argument for 3-1 RBM can be extended to N-1 RBM (See details in [2]). We consider hyperplanes\nand regions cut oﬀby these hyperplanes. The number of these regions is less than: 2N1.52N−N−1.\nCompare to the number of all processing of N-1 IPU, which is 22N−1, easy to see, N-1 RBM has much\nless processing. This means that N-1 RBM could not express many processing.\nFor N-1 RBM, still we can write: for any input (i1, . . . , iN) ∈BN, output o ∈B is:\no = p(i1, . . . , i3) = Sign(a1i1 +a2i2 +. . .+aNi3), where (a1, . . . , aN) ∈RN, Sign(x) =\n\u001a 1\nif x ≥0\n0\nif x < 0\na1 = 0, . . . , a1 + a2 = 0, . . . , a1 + a2 + a3 = 0 . . . , . . . , a1 + a2 + a3 + . . . = 0, . . .\n(1)\nThere are\n\u0000N\n1\n\u0001\nhyperplanes such as a1 = 0;\n\u0000N\n2\n\u0001\nhyperplanes such as a1 + a2 = 0; . . . . We also have\nthis: First N hyperplanes will cut parameter space into 2N regions. Then, later each hyperplanes will\ncut more regions by the rate of multiplying factor 1.5. Thus, we can see the number of regions are:\n2N ∗1.5K2 ∗1.5K3 ∗. . . ∗1.5KN\nwhere K2 =\n\u0000N\n2\n\u0001\nis the number of hyperplanes such as a + b = 0, etc.\nAnd, we have the equation:\n2N =\n\u0012N\n0\n\u0013\n+\n\u0012N\n1\n\u0013\n+\n\u0012N\n2\n\u0013\n+ . . . +\n\u0012N\nN\n\u0013\n(2)\nSo,\nK2 + K3 + . . . + KN =\n\u0012N\n2\n\u0013\n+\n\u0012N\n3\n\u0013\n+ . . . +\n\u0012N\nN\n\u0013\n= 2N −\n\u0012N\n1\n\u0013\n−\n\u0012N\n0\n\u0013\n= 2N −N −1\n(3)\nThus, the number of regions are\n2N ∗1.52N−N−1 = 2N ∗(3\n2)2N−N−1\nThis is a very big number. Yet, compare to the total possible processing of full IPU, it is quite small.\nSee their quotient:\n22N\n2N ∗( 3\n2)2N−N−1 = 2 ∗(4/3)2N−N−1\nIt tells that full IPU has f = 2 ∗(4/3)2N−N−1 times more processing comparing to RBMs. This is\nhuge diﬀerence. Say, just for N = 10, f is more than 120 digits, i.e. the number of processing of full\nIPU would has more 120 digits at the end than the number of RBMs.\nAlso, each region can be expressed by at least one X-form. For examples, b1 + bN, b1 + b3 + b5, etc.\nLearning dynamics on N-1 RBM is in such way: on surface, it is dynamics on RN, but real learning\ndynamics is on those 2N1.52N−N−1 regions (or X-forms).\nN-M RBM\nSuppose Ri, i = 1, . . . , M are M N-1 RBMs, we can form a N-M RBM, denote as R = (R1, . . . .RM),\nwhose processing are p = (p1, p2, . . . , pM), where pi, i = 1, . . . , M are processing of Ri. So, R is\nCartesian product of Ri, i = 1, . . . , M.\nSince all Ri are cut into regions, and in each region, processing is same, we can see R is also cut into\nregions, and each region is a Cartesian product of regions of Ri: R = R1 × R2 × . . . × RM, where Ri\n6\nWhat really is deep learning?\nis one region from i-th RBM Ri. Thus, the number of all possible regions of R is (2N1.52N−N−1)M =\n2NM1.5M(2N−N−1). This is a much smaller number than 2M2N , which is the number of total possible\nprocessing for N-M IPU.\nX-form for each region of R, are actually Cartesian product of X-form for those regions of Ri. Suppose\nR = R1 × R2 × . . . × RM, and fi are X-forms for region Ri in Ri, i = 1, . . . , M, then X-form for R is\nf = (f1, . . . , fM). For example, (b1, b1 + b3, . . . , b2 · b4) is one X-form of R.\nLearning dynamics on N-M RBM is in such way: on surface, it is dynamics on parameter space RNM,\nbut real learning dynamics is on those 2NM1.5M(2N−N−1) regions (or X-forms).\nStacking RBMs\nConsider a N-M RBM R1, and a M-L RBM R2, stacking them together, we get one N-L IPU R: A\nprocessing p of R are composition of processing p1, p2 of R1, R2: p(i1, i2, . . . , iN) = p2(p1(i1, i2, . . . , iN)).\nAnd we denote as: R = R1 ⊗R.\nThe parameter space of R clearly is RNM × RML. We know RNM is cut into some regions, in each\nregion processing is same. So does RML. Thus, RNM+ML is cut into some regions, in each region\nprocessing is same, and these regions are Cartesian product of regions in RNM and RML. So, we\nknow number of total possible processing R equals total possible processing of R1 times R2, i.e.\n2NM1.5M(2N−N−1) × 2ML1.5L(2M−M−1) = 2NM+ML1.5M(2N−N−1)+L(2M−M−1).\nWe can easily see if M is large enough, the above number will become greater than 2L2N , which is\ntotal possible processing of R. We can see, at least potentially, R has enough ability to become a N-L\nfull IPU. But, we will not consider here. In fact, it is very closely related to the so called Universal\nApproximation Theorem. Indeed, stacking RBM together is powerful.\nX-form can be expressed as composition as well. For example, consider 3 2-1 RBM R1, R2, and\nR3.\nUsing R1 and R2 to form a 2-2 RBM, and using R3 to stack on it, we get a 2-1 IPU R:\nR = R3 ⊗(R1, R2). If for this case, R1 has X-form b1, and R2 has X-form b2, and R3 has X-form\nb1 + b2 + b3, them, R has X-form (b1 + b2 + b3)(b1, b2). Easy to see this X-form is processing P5 (XOR\ngate), which is not expressible by one 2-1 RBM. So, putting 3 2-1 RBMs together, more X-form can\nbe expressed.\n4\nLearning Dynamics of Deep Learning\nWith these understandings RBMs, which is the most essential building block of deep learning, we can\nsee how the model of deep learning is build up, and how learning dynamics is doing. Clearly, today’s\ndeep learning is much more than original Hinton’s model of stacking RBMs (see [4]). But, we would\nﬁrst talking about this model.\nThe deep learning model is by stacking more RBMs (this is so called deep). Once several RBMs are\nputting together, a deep learning model is formed. Suppose Rj, are Nj-Nj+1 RBM, j = 1, . . . , J, where\nN1, . . . , NJ, NJ+1 are sequence of integers. So, we can stack these RBM together to form one N1-NJ+1\nIPU, whose processing could be written in such way: p(i1, i2, . . . , iN1) = pJ(. . . p2(p1(i1, i2, . . . , iN1))\nwhere each pj is processing of Rj. We denote this IPU as R = R1 ⊗. . . ⊗RJ. All parameters of R\nform a huge Euclidean space RN1N2+...+NJNJ+1. We can denote this huge parameter space as R∗.\nThen, clearly, deep learning is conducted on R to reach a good processing by modifying the parameters\nin R∗. Of course, it requires skills to modify such a huge number of parameters. There are methods,\nsuch as CD (convergent divergence), SGD (stochastic gradient descent), etc. are invented for such\npurpose.\nHowever, no matter what methods are used to modify the parameters, it is modifying parameters\nto form the dynamics of learning. So, seems the phase space of learning dynamics is on the space\nR∗. But, this is just on surface. As we discussed in last section, the true dynamics is not conducted\non parameters, but on those regions. The learning dynamics is conducted on these regions cut by\nhyperplanes and Cartesian products. The number of regions are huge: 2N1N21.5N2(2N1−N1−1) × . . ..\nMore precisely, the situation is: as learning, a huge number of parameters are adapting, but only\nwhen parameters cross region, the processing of R changes. Before crossing, processing remains same,\nthe changes of parameter at most can be thought as the preparation for crossing (perhaps many\nsuch changes of parameters are just wasted). Thus, learning dynamics is to move from one region\nChuyu Xiong\n7\nto another. We also know each region is associated with one X-form. Thus, learning dynamics is to\nmove from one X-form to another X-form.\nFig. 2. Illustration on parameter space is cut into regions\nFig.\n2 gives one illustration on parameter space is cut into regions.\nOf course, R∗is very high\ndimension Euclidean space, so regions could be shown on paper precisely, and Fig.\n2 is just one\nillustration. However, this illustration gives us one clear picture about deep learning structure.\nThe deep learning structure is formed by these factors: how many RBMs, dimension of each RBM, how\nto stack, how to do Cartesian product. Once the structure is formed, if no further human intervention\n(such as manually adjust numbers or subroutines in model), the structure will not change.\nSuch\nstructure is formed by people at set up time. So, for ﬁxed structure, we will have ﬁxed region cut (as\nillustrated in Fig. 2). Further, we will have a ﬁxed set of X-forms, and learning is conducted on this\nset of X-forms.\nWe can see one example. R1, R2, R3 are 3 2-1 RBMs. We put them like this: R = R3 ⊗(R1, R2).\nWe have 3 parameter space (a, b), (c, d), (e, f). We have 6 regions for each parameter space. Put them\ntogether, we have 6x6x6 = 216 regions. R is one 2-1 IPU. So, R at most has 8 processing. Thus,\namong those 216 regions some diﬀerent regions will have same processing. But, each region will have\none X-form. That is to say, for one processing, there could have several X-form associated with.\nFor example, consider this region: R3\n1 × (R1\n3, R2\n5). This gives processing P5 (XOR gate). Normally,\nfor this processing, we can use X-form b1 + b2 for it. But, for the region, naturally, the X-form is:\n(b1+b2+b3)⊗(b1, b2). That is to say, this X-form will generate the same processing as b1+b2. Another\nregion: R3\n2 × (R1\n2, R2\n6) will give the same processing. And, the X-form is: (b1 + b3) ⊗(b1 + b3, b2 + b3).\nFor deep learning model build on stacking RBMs (original Hinton’s model, [4]), as we discussed in\nlast section, the situation is same: the parameter space R∗is cut into regions (by hyperplanes, etc),\nand each region is associated with one X-form, when parameter cross the boundary of the regions,\none X-form moves to another X-form, learning dynamics is conducted on this set of X-form.\nThis is the learning dynamics of deep learning, this is what really deep learning is doing.\nFor more complicated deep learning, such as convolution is used, connection pruning is done, non-\nlinearity is other than sign function (like ReLU), the situation will be more complicated. However,\nif there is no human intervention, it will surely be mechanical learning. We still can prove that the\nlearning dynamics is the same: the parameter space R∗is cut into regions, and each region is as-\nsociated with one X-form, when parameter cross the boundary of the regions, one X-form moves to\nanother X-form, learning dynamics is conducted on this set of X-form.\nTo prove this for general deep learning mathematically, additional works are needed. We will do this\nwork in other place. But, we have no doubt this can be done.\nSuch a learning strategy is exactly what we described in [2]: ”Embedded X-forms into parameter\nspace”.\n8\nWhat really is deep learning?\n5\nRemark\nNow, we know the fact: deep learning is using strategy of ”Embedded X-forms into parameter space”.\nThis fact is very essential and many consequences can be derived from it.\nHere we make some\ncomments.\nTrue nature of deep learning:\nOn surface, deep learning seems build a model from data feed into eventually (by using neural network,\nstacking RBMs, and more other tools). However, as we reveal in previous sections, it is not such a\ncase. Essentially, a deep learning model is doing this: at the time of model setting up, to cut the\nhuge parameter space R∗into many regions, and each region is associated with one X-form that is\none logical statement, then driven by big amount of data, following certain learning dynamics, i.e. to\nmove from region to another region, which is equivalent to move from one X-form to another X-form,\nand eventually to reach a satisfactory X-form, which is the learning result.\nSo, we say that deep learning is not to building up a model from data input, but it is to choose a\ngood X-form from a set of X-forms established at the time deep learning model is set up. This is the\ntrue nature of deep learning.\nSuch a view is diﬀerent than popular view about deep learning. However, it is true and help us to\nunderstand deep learning better. For example, [6] might be right, there are some group renormalization\ngoing on, but, it missed this issue: group renormalization happens at the setting stage not at the\nlearning stage. Another example, [5] gives a very good explanation about the power of multi-stage-\ncomposition. However, it failed to realize that learning is not only to get a good processing, but to\nﬁnd a best possible X-form for the good processing (since one particular processing could have many\nassociated X-form, and some of such X-form is bad, some of such X-form is good).\nFundamental limitation of deep learning:\nThe fundamental limitation of a deep learning model is from its nature: it acts on a pre-selected set\nof X-forms X, which is formed at the time the model is set up.\nSo, mostly likely, a deep learning model could not be an universal learning machine [2].\nIf it is\nuniversal, X must contains at least one X-forms for any possible processing. This is nearly impossible.\nActually, a deep learning model is set up by human, and is for one particular task. So, most likely, X\nonly contains X-forms specially for this task. And, this deep learning model is limited by X.\nIf the learning target is for a particular processing, and if in X there is at least one X-form associated\nto this processing, a deep learning model could possible to get the target. Otherwise, a deep learning\nmodel could not reach the processing, no matter how hard to try and how much data. In another\nword, the model is a bad model. But, deep learning has no any method to tell if a model is good or\nbad before trying it out. This is one huge limitation.\nYet, even X contains a X-form associated with the desired processing, we still do not know if the\nX-form is a good one. As we know in [2], there are many X-forms associated with one processing,\nsome is bad, and some is good. For example, one X-form is more robust for certain conditions. If X\ndoes not contain the robust X-form, no matter how hard to try and how much data, learning could\nnot get a robust solution. Again, deep learning has no any method to tell if X contains such X-form.\n.\nThese limitations are fundamental and are derived from the fact: deep learning is chosen X-form from\na pre-selected set X, not dynamically building a X-form from input data.\nLogical or Probabilistic:\nQuite often, people think deep learning is doing probability learning. They think that a probability\nmodel is essential for deep learning since a lot data feed in, specially, stochastic gradient descent is one\nvery essential learning method. However, we would like to point out: deep learning fundamentally is\nviewing its learning target logically. Why? Each X-form in X is a logical statement, often a very long\nlogical statement (so called deep). So, the very essential thing is: when a deep learning model does its\ninformation processing, it is doing according to a solid logical statement. Not doing the information\nprocessing probabilistically.\nOf course, the way to get the X-form might not be pure logical, it could involve a lot of probabilistic\nviews and actions, such as stochastic gradient descent. However, we would point out, even the way\nto get the X-form, could be pure deterministic not probabilistic.\nIt is possible to design a pure\ndeterministic learning dynamics, at least in theory.\nChuyu Xiong\n9\nWhy works well:\nPractice shows, deep learning works very well for many problems. Now, we can see the reason for\nsuch success clearly: when a deep learning model for one problem is set up by experienced people,\nthe desired X-form is already build into the model. More precisely, if the desired X-form is X, when\ndeep learning model is set up, we have X ∈X, where X is the pre-selected set of X-forms. If so, it is\npossible to learn the X-form X successfully, so the processing associated with X. Thus, the success\nof a deep learning model depends on its set up. Having a good set up, the model will work well.\nOtherwise, no matter what data and what eﬀorts, the model will not work well.\nOf course, besides the set up of model, learning methods are crucial. It is not easy to choose the right\nX from X at all. We would like to point out the methods currently used indeed have some advantages:\n1. Methods act on Euclidean space, which is most easy to calculate, with many sophisticated\nalgorithms, library, packages, and hardwares available.\n2. Methods are mostly doing linear algebraic calculus, which are easy to be parallelized. And, high\nparallelization is the key of its success. However, this advantage is build on this fact: no dynamic\nadaption. If dynamic adaption is used (such as recently introduced Capsule), this advantage\nmight be lost.\nData for deep learning:\nAs discussed above, the logical statement (X-form) is the core of deep learning. Without supporting\ndata, a deep learning model could not reach a sophisticated logical statement (X-form). We deﬁned\ndata suﬃciency in [2], which tells what kind of data can support one X-form (logical statement).\nOf course, the data suﬃciency we deﬁned is only the ﬁrst step to understand data. Since deep learning\napproaching the desired X-form by some learning methods, it is easy to see that we need more data\nthan just suﬃcient to support one X-form. The relationships here could be quite complicated, which\nwould be the topic for further research.\nHowever, we can tell that data suﬃcient to support and suﬃcient to bound the desired X-form is\nthe necessary condition for deep learning. In this sense, for so called big data for deep learning, we\nunderstand the necessity and lower bound.\nDisadvantages of deep learning:\nDeep learning has some fundamental disadvantages from its root. We list some of them below:\n1. It is acting on huge parameter space, but, actual learning dynamics is on a ﬁxed set of regions\n(which is equivalent to a set of X-forms). This indirectness makes every aspects of learning\nharder, especially, it is near impossible to know what is exactly happening in learning dynamics.\n2. Successful learning needs data suﬃcient to support and to bound. This is very costly.\n3. The structure of learning is setup by human. Once setup, structure (how many layers, how big\na layer is, how layers ﬁtting together, how to do convolution, etc) is not be able to change. This\nmeans that learning is restricted on a ﬁxed group of regions, equivalent a ﬁxed group of X-forms.\nIf best X-form is not in this set, deep learning has no way to reach best X-form, no matter how\nbig data are and how hard to try. Consequently, it is not universal learning machine.\n4. It is very costly to embed X-forms into a huge parameter space. Perhaps, among all computing\nspend on learning, only a very small fraction is used on critical part, i.e. moving X-form to\nanother, and most are just wasted.\n5. Since there is no clear and reachable internal representation (due to the embedding), it will be\nvery hard to do advanced learning, such as to unite all 5 learning methods together (see [3], [2]).\n6. Since there is no clear internal representation space, it is hard to deﬁne initial X-form, which is\nvery essential to eﬃciency improving and several stages of learning.\nLooking forward to universal learning machine:\nSince deep learning model is not universal learning machine, naturally, we looking forward to universal\nlearning machine. We discussed this in [1] and [2]. There, we proved that with certain capabilities,\nwe can make universal learning machine. Also, we actually invented a concrete universal learning\nmachine which is in the patent application process. We think universal learning machine has many\nadvantages over deep learning model. There are many research works needed to be done for universal\nlearning machine.\n10\nWhat really is deep learning?\nReferences\n[1] Chuy Xiong. Discussion on Mechanical Learning and Learning Machine, arxiv.org, 2016.\nhttp://arxiv.org/pdf/1602.00198.pdf\n[2] Chuy Xiong. Descriptions of Objectives and Processes of Mechanical Learning, arxiv.org, 2017.\nhttp://arxiv.org/abs/1706.00066.pdf\n[3] Pedro Domingos. The Master Algorithm, Talks at Google.\nhttps://plus.google.com/117039636053462680924/posts/RxnFUqbbFRc\n[4] E. G. Hinton. Learning multiple layers of representation, Trends in Cognitive Sciences, Vol. 11,\npp 428-434.. http://www.cs.toronto.edu/ hinton/absps/tics.pdf\n[5] Henry W. Lin, Max Tegmark, David Rolnick. Why does deep and cheap learning work so well?,\narxiv.org, 2016.\nhttp://arxiv.org/pdf/1608.08225.pdf\n[6] Pankaj Mehta, David J. Schwab, David Rolnick. An exact mapping between the Variational\nRenormalization Group and Deep Learning, arxiv.org, 2014.\nhttp://arxiv.org/pdf/1410.3831.pdf\n",
  "categories": [
    "cs.LG",
    "cs.NE"
  ],
  "published": "2017-11-06",
  "updated": "2017-11-06"
}