{
  "id": "http://arxiv.org/abs/1805.04825v1",
  "title": "Deep Learning in Software Engineering",
  "authors": [
    "Xiaochen Li",
    "He Jiang",
    "Zhilei Ren",
    "Ge Li",
    "Jingxuan Zhang"
  ],
  "abstract": "Recent years, deep learning is increasingly prevalent in the field of\nSoftware Engineering (SE). However, many open issues still remain to be\ninvestigated. How do researchers integrate deep learning into SE problems?\nWhich SE phases are facilitated by deep learning? Do practitioners benefit from\ndeep learning? The answers help practitioners and researchers develop practical\ndeep learning models for SE tasks. To answer these questions, we conduct a\nbibliography analysis on 98 research papers in SE that use deep learning\ntechniques. We find that 41 SE tasks in all SE phases have been facilitated by\ndeep learning integrated solutions. In which, 84.7% papers only use standard\ndeep learning models and their variants to solve SE problems. The\npracticability becomes a concern in utilizing deep learning techniques. How to\nimprove the effectiveness, efficiency, understandability, and testability of\ndeep learning based solutions may attract more SE researchers in the future.",
  "text": "1 \n \nDeep Learning in Software Engineering \nXiaochen Li1, He Jiang1,2, Zhilei Ren1, Ge Li3, Jingxuan Zhang4 \n1School of Software, Dalian University of Technology \n2School of Computer Science & Technology, Beijing Institute of Technology \n3Software Institute, Peking University \n4College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics \nli1989@mail.dlut.edu.cn, jianghe@dlut.edu.cn, zren@dlut.edu.cn,  \nlige@pku.edu.cn, jingxuanzhang@mail.dlut.edu.cn \nAbstract \nRecent years, deep learning is increasingly prevalent in the field of Software \nEngineering (SE). However, many open issues still remain to be investigated. How do \nresearchers integrate deep learning into SE problems? Which SE phases are \nfacilitated by deep learning? Do practitioners benefit from deep learning? The \nanswers help practitioners and researchers develop practical deep learning models \nfor SE tasks. To answer these questions, we conduct a bibliography analysis on 98 \nresearch papers in SE that use deep learning techniques. We find that 41 SE tasks in \nall SE phases have been facilitated by deep learning integrated solutions. In which, \n84.7% papers only use standard deep learning models and their variants to solve SE \nproblems. The practicability becomes a concern in utilizing deep learning techniques. \nHow to improve the effectiveness, efficiency, understandability, and testability of \ndeep learning based solutions may attract more SE researchers in the future. \nIntroduction \nDriven by the success of deep learning in data mining and pattern recognition, recent \nyears have witnessed an increasing trend for industrial practitioners and academic \nresearchers to integrate deep learning into SE tasks [1]-[3]. For typical SE tasks, deep \nlearning helps SE participators extract requirements from natural language text [1], \ngenerate source code [2], predict defects in software [3], etc. As an initial statistics of \nresearch papers in SE in this study, deep learning has achieved competitive \nperformance against previous algorithms on about 40 SE tasks. There are at least 98 \nresearch papers published or accepted in 66 venues, integrating deep learning into \nSE tasks.  \nDespite the encouraging amount of papers and venues, there exists little overview \nanalysis on deep learning in SE, e.g., the common way to integrate deep learning into \nSE, the SE phases facilitated by deep learning, the interests of SE practitioners on deep \nlearning, etc. Understanding these questions is important. On the one hand, it helps \npractitioners and researchers get an overview understanding of deep learning in SE. \nOn the other hand, practitioners and researchers can develop more practical deep \nlearning models according to the analysis. \nFor this purpose, this study conducts a bibliography analysis on research papers in \nthe field of SE that use deep learning techniques. In contrast to literature reviews, \n2 \n \nbibliography analysis can reflect the overview trends, techniques, topics on deep \nlearning in SE by statistical data. First, we collect 4,443 research papers that contain \nboth SE and deep learning related keywords. Next, we filter the research papers by \nreading their contents, citations and references. Finally, 98 research papers related \nto both SE and deep learning are identified. With these papers, we analyze the \npublication trend, research topics, used deep learning models, and industrial research \ninterests of these papers. \nWe find that research papers related to deep learning has increased significantly in \nSE in recent years, which have facilitated 41 SE tasks. Both communities of SE and \nArtificial Intelligent (AI) show great interests in utilizing deep learning in SE. \nSurprisingly, more than one fifth research papers have industrial practitioners to \nparticipate in, which means that industrial practitioners are also interested in \nintegrating deep learning into their SE solutions. Despite the encouraging success of \ndeep learning, we find several concerns about using deep learning in SE. Practitioners \nand researchers worry about the practicability of utilizing a complex method with \nalmost opaque internal representations like deep learning [6]. Hence, the \neffectiveness and efficiency [7], understandability [8], and testability [9] become the \nburden to use deep learning in practice. Fortunately, recent studies have conducted \nsome initial investigation on these problems [6]-[9]. These findings may guide the \nfuture studies of using deep learning in SE. \nExample of using deep learning in SE \nDeep learning is a technique that allows computational models composed of multiple \nprocessing layers to learn representations of data with multiple levels of abstraction \n[14]. In this section, we present an example of using deep learning in SE. In this \nexample, we apply the deep learning model AutoEncoder on a typical SE task, i.e., bug \nreports summarization [10].  \nSE Task\nSE Data\nCollection\nSE Data \nPreprocessing\nModel \nSelection & \nConfiguration\nInput\nConstruction\nModel \nTraining\nApplying\nModels\n1 \n2 \n3 \n4 \n5 \n6 \nRaw SE \nData \nProcessed \nSE Data \nDesigned  \nModel  \nInput Vectors \n& Designed \nModel  \nTrained \nModel \nFig. 1. A framework to summarize bug reports with AutoEncoder \n \n3 \n \nBug reports are texts to describe the bugs in software. Facing numerous bug \nreports, bug report summarization aims to generate a summary by extracting and \nhighlighting informative sentences of a bug report to shorten the reading time. To \nidentify informative sentences, researchers utilize AutoEncoder to encode the words \nin bug report sentences in an unsupervised way. Since the hidden state of \nAutoEncoder provides a compressed representation of the input data, the weights of \nwords in a bug report can be measured by calculating how much information of a \nword is reserved in the hidden states. Based on the word weights, informative \nsentences are identified [10]. As shown in Fig. 1, the example consists of six steps.  \n1. SE data collection decides the available data for an SE task. For bug report \nsummarization, the commonly available data are bug reports. Each bug report \nmainly contains a title, a description of the bug, and several comments.  \n2. SE data preprocessing removes the noises in SE data. For a bug report, the \nEnglish stop words and some programming-specific ones are the noises. \nBesides, extremely short sentences are also noises, since they may be \nuninformative. \n3. Model selection and configuration select suitable deep learning models for \nSE data and decide model configurations, e.g., the number of layers and neural \nunits of each layer. The widely used deep learning models include \nAutoEncoder, CNN, RNN, etc. (explained in Fig. 3). These standard models \nusually have several variants, e.g., LSTM, Bi-LSTM, and attention-based RNN \nare classical variants of RNN. In this example, AutoEncoder is selected. \nAutoEncoder usually has a symmetric architecture, i.e., the number of neural \nunits of input and output layers are the same. The output layer is defined as a \npattern to reconstruct the input layer. The number of neural units of hidden \nlayers decreases as towards the middle of the network. After training, the \nhidden states reserve the key information for reconstructing the input layer. \n4. Input construction transforms SE data into vectors for deep learning models. \nFor bug report summarization, researchers calculate the word frequency in \nbug reports and transform the word frequency values into vectors. These \nvectors are regarded as a training set for AutoEncoder.   \n5. Model training trains the designed model with the training set. A deep \nlearning model usually has thousands of parameters representing the weights \nof connections among neural units. Hence, training the model is to tune these \nparameters according to the training set. For AutoEncoder, the parameters are \ntrained by minimizing the difference between the input and output layers in \nan unsupervised way.  \n6. Applying models is to utilize the trained model to solve SE problems. In this \nexample, the trained model can encode the word frequency vector of a new \nbug report into the hidden states. We can trace and calculate the changes of \nthe value in each vector dimension along with the encoding process, and then \ndeduce the weights of words in each dimension. These word weights help \nresearchers assign weights of the sentences and select informative ones.  \n4 \n \nData collection \nTo collect deep learning related papers in SE, we design three criteria to search \nresearch papers from four well-known digital libraries, including Web of Science, \nACM Digital Library, IEEE Xplore, and Scopus. \nC1. Research papers should contain at least one of the following SE phrases, \nincluding \"software engineer*\", \"software develop*\", \"software test*\", \n\"software design\", \"requir* analysis\", \"software requir*\", \"software \nmaintain*\", and \"software manag*\". The sign \"*\" is a wildcard character to \nmatch zero or more characters in a word. \nC2. Research papers should contain at least one phrase about deep learning \nconcept, i.e., \"deep learn*\" and \"neural network*\". \nC3. Research papers are conference or journal papers written in English on the \ntopic of computer science. \nUnder these criteria, we achieve 4,443 candidate research papers published before \nMarch 2018, including 414 from Web of Science, 207 from ACM Digital Library, 2,271 \nfrom IEEE Xplore, and 1,551 from Scopus. We remove the duplicate papers and short \npapers with less than 4 pages. At last, 3,351 research papers are reserved. We \ndownload and manually examine the contents of the papers:  \n1. We remove 35 papers that the full-contents cannot be downloaded.  \n2. We remove 2,441 papers that the searching phrases in C1 and C2 merely \nmatch some supplementary information in the paper. For example, \"software \nengineer*\" may match the phrase of \"school of software engineering\" in author \nbiography or the publication venue \"Transaction on Software Engineering\". \n3. After step 1 and 2, another 812 papers are removed as they do not focus on SE \nor deep learning. For example, \"deep learning\" is also a concept in computer \neducation and \"neural network\" may refer to a shallow network structure with \na single hidden layer. \nAt last, 63 research papers are remained. We take these papers as seeds to search \ntheir references and citations. If a new SE research paper about deep learning is found, \nwe recursively examine the new paper. Finally, another 35 research papers are found. \nHence, we collect in total 98 published or accepted research papers for analysis. \nBibliography Analysis \nWe analyze the collected papers to investigate the status of deep learning in SE. \nA. The prevalence of deep learning in SE \nWe count the number of research papers each year and the venues of the publications \nin Fig. 2(a) and Fig. 2(b) respectively. In Fig. 2(a), we find that deep learning attracts \nlittle attention in SE for a long time, i.e., only less than 3 papers are published each \nyear before 2015. The reason may be that although deep learning performs well on \nimage processing, speech recognition, etc., it takes time for the practitioners and \n5 \n \nresearchers in SE to validate deep learning on domain-specific SE tasks. However, the \nresearches boom in SE after 2015, e.g., 28 publications in 2016 and 39 publications in \n2017. Furthermore, only in the first three months in 2018, there are already 12 \npublications using deep learning in SE. \nFor these research papers, we count the publication venues. Surprisingly, out of the \n98 SE papers, 66 venues have published at least one paper on the topic of deep \nlearning. Fig. 2(b) presents the publication venues that publish more than one paper \nWe explain these venue names in Fig. 2(c). We find that using deep learning in SE \nattracts the attention from both communities of SE and AI, including some premier \nSE venues like ICSE, ESEC/FSE, ASE, ICSME, ICPC and some renowned AI venues like \nAAAI, ICLR, ICML, NIPS, ACL, IJCAI. These venues may be a good guidance to study the \nprogress of deep learning in SE.  \nTo conclude, deep learning is prevalent in SE. It attracts the attention from both SE \nand AI communities. \nB. The way to integrate deep learning into SE \nAs the prevalence of deep learning in SE, we analyze the way to integrate deep \nlearning into SE. Fig. 3 shows the name of deep learning models and the number of \npapers using these models. We find that most studies (55 papers) directly transfer \nstandard deep learning models into SE, including AutoEncoder, CNN, DBN, RNN and \na simple fully-connected DNN. Meanwhile, the classical variants of these models in AI \nare also widely used (28 papers) such as SDAEs, LSTM, etc. The above models are \nused in 84.7% research papers. Besides using a single model, combined deep learning \n(c) Full names of publication venues \nVenue \nExplanation \nICSE \nInt’l Conf.on Softw. Eng. \nESEC/FSE Joint European Softw. Eng. Conf. and Symposium on \nthe Foundations of Softw. Eng. \nASE \nInt’l Conf. on Automated Softw. Eng. \nICSME \nInt’l Conf. on Softw. Maintenance and Evolution \nAAAI \nAAAI Conf. on Artificial Intelligence \nICLR \nInt’l Conf. on Representation Learning \nICML \nInt’l Conf. on Machine Learning \nICPC \nInt’l Conf. on Program Comprehension \nNIPS \nConf. on Neural Information Processing Systems \nACL \nAnnual Meeting of the Association for Computational \nLinguistics \nESWA \n \nExpert Systems With Applications \nIJCAI \nInt’l Joint Conf. on Artificial Intelligence \nIST \nInformation and Softw. Tech. \nKSEM \nInt’l Conf. on Knowledge Science, Eng. and \nManagement \nQRS \nInt’l Conf. on Quality Softw \nSEKE \nInt’l Conf. on Softw. Eng. and Knowledge Eng. \nSNAPL \nSummit on Advances in Programming Languages \n \n6\n4\n4\n4\n3\n3\n3\n3\n3\n2\n2\n2\n2\n2\n2\n2\n2\n0\n1\n2\n3\n4\n5\n6\n7ASEESEC/FSEICSEICSMEAAAIICLRICMLICPCNIPSACLESWAIJCAIISTKSEMQRSSEKESNAPL\nNumber of papers\nJournal/Conference\n(b) Top venues of the publications\n1\n1\n1\n3\n1\n2\n1\n1\n1\n7\n28\n39\n12\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n2000 2003 2007 2008 2009 2010 2011 2013 2014 2015 2016 2017 2018\nNumber of papers\nYear\n(a) The number of publications per year\nFig. 2. Basic information of deep learning in SE \n6 \n \nmodels (8 papers) also show competitiveness in SE, e.g., a combination of RNN and \nCNN. For the remaining papers, researchers design specific deep learning \narchitectures for SE data like Stepped AutoEncoder and TBCNN. The above \nphenomenon suggests that when integrating deep learning into SE tasks, a new \npractitioner may be willing to first try some standard models and their variants to \ninvestigate whether deep learning works or not. \nFurthermore, we investigate what types of SE data are usually fed into these \nmodels. We analyze the inputs of the 98 papers. The inputs can be categorized into \nfive categories. \n1. Predefined software metrics (25 papers). Researchers first manually define and \ncalculate some software metrics, e.g., lines of code, the number of bugs in source \ncode. Then, they construct vectors based on the values of these metrics to feed \ninto deep learning models. \n2. Dynamic software status (14 papers). This category takes the dynamic \ninformation when running the software as input, e.g., the CPU utilization, the \ninvoked APIs. The values of these dynamic information can be transformed into \nvectors for deep learning models. \n3. Raw text or source code without sequence (32 papers). These papers treat the \nbag-of-words of text and source code as deep learning input without \nExplanation of abbreviation \nAutoEncoder\nAE\n• SAE: Stacked AE\n• SDAE: Stacked Denoising AE\nConvolutional Neural Network\nCNN\n• NPCNN: Natural language and Programming \nlanguage CNN\n• TBCNN: Tree-Based CNN\nDeep Belief Network\nDBN\nRecurrent Neural Network\nRNN\n• LSTM: Long Short-term Memory\n• Bi-LSTM: Bi-Directional LSTM\n• R3NN: Reverse-Recursive-Reverse NN\n• RNNBPTT: RNN with Backpropagation \nThrough Time\nMIX\n• DNN: Deep Neural Network\n• ANFIS: Artificial Neural network with Fuzzy \nInference System\n• FG-HPNN: Fuzzy Granule-based Hierarchical \nPolynomial NN\n• FNN: Fuzzy Neural Network,\n• LPN: Latent Predictor Network\n• PNN: Probabilistic NN\n• RFNN: Rule-based Fuzzy NN\n2\n1\n4\n1\n12\n1\n2\n3\n13\n3\n9\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n25\n4\n1\n1\n1\n1\n1\n0\n5\n10\n15\n20\n25\n30\nstandard\nSAEs\nSDAEs\nstepped AE\nstandard\nNPCNN\nTBCNN\nstandard\nstandard\nattention-based RNN\nLSTM\nR3NN\nRNNBPTT\nStackAugment RNN\nBi-LSTM\nDBN+SAEs\nLSTM+RNN\nAE+DNN\nsequential & averaged RNNs\nRNN+CNN\nBi-LSTM+CNN\nBi-LSTM+DNN\nBi-LSTM+RNN\n(fully-connected) DNN\nANFIS\nFG-HPNN\nFNN\nLPNs\nPNN\nRFNN\nAE\nCNN\nD\nRNN\nMIXED\nOther\nNumber of papers\nDeep learning models\nFig. 3. Deep learning models in the research papers \nDBN \n7 \n \nconsidering word sequences [10]. Based on the bag-of-words, word embedding, \none-hot representation and word frequency are widely used to transform \nwords into vector space for deep learning. \n4. Raw text or source code in sequence (22 papers). In contrast to category 3, this \ncategory considers the sequence of words [2].  Such inputs are usually \nassociated with RNN-based models, which utilizes the order of words to predict \nthe next word or class label of software documents and source code, e.g., \nprogram learning and program synthesis. \n5. Others (5 papers). Most of the other inputs are multimedia data such as images. \nFor example, researchers utilize images to test deep learning models [9]. The \npixels of the images are fed into deep learning models. \nTo conclude, practitioners and researchers can achieve competitive results on \nmore than 80% SE problems when only using standard deep learning models and \ntheir variants. Deep learning can well handle many types of SE data, including \npredefined software metrics, dynamic software status, and raw text or source code. \nC. The SE phases facilitated by deep learning \nDue to the diversity of SE tasks, it is important to identify the existing SE tasks \nfacilitated by deep learning, since it helps practitioners find the potential to leverage \ndeep learning in their own problems. \nAs suggested by classical SE models, i.e., Waterfall Model and Incremental Model \n[11], SE can be divided into five phases, including requirement analysis, software \ndesign, development, testing and maintenance. In addition, since SE is an activity \ninvolving many stakeholders (developers, testers, project managers, etc.), we also \nadd project management as an SE phase. Fig. 4 shows the SE tasks facilitated by deep \nlearning in the six phases.  \nAs shown in Fig. 4, researchers have tried deep learning on at least 41 SE tasks. In \nrequirement analysis, deep learning helps requirement analysts automatically \nextract requirements from natural language texts [1]. In software design, design \npatterns of software can be recognized [12]. In software development, deep learning \nhelps developers on 14 SE tasks from 30 research papers, including program learning \nand program synthesis [2], code suggestion [6], etc. Besides, software testing and \nmaintenance are also major phases to attempt deep learning. There are 54 research \npapers in these two phases which cover 21 SE tasks like defect prediction [3] and \nreliability or changeability estimation [4]. For the 41 SE tasks, program learning and \nprogram synthesis [2], malware detection [5], defect prediction [3], reliability or \nchangeability estimation [4], and development cost or effort estimation [13] are the \ntop 5 tasks studied by the researchers. Hence, practitioners may have a board \nselection of methodologies and deep learning models when using deep learning on \nthese tasks. \nTo conclude, deep learning has facilitated at least 41 SE tasks in all SE phases, \nincluding requirement analysis, software design, development, testing, maintenance, \nand project management. \n8 \n \nD. Research interests of industrial practitioners  \nWe analyze the SE tasks participated by industrial practitioners to understand \nresearch interests in practice. \nThe industrial practitioners are identified when at least one author affiliation in the \nauthor list of a research paper is a company. In Fig. 4, we label the industry-\nparticipated SE tasks in bold and list the company names. To our surprise, there are \n21 research papers (more than one fifth) on 13 SE tasks with at least one industrial \npractitioner, which implies the interest of industrial practitioners in integrating deep \nlearning into SE problems. Among the 13 tasks, program learning and program \nsynthesis attract the most attention [2]. Eight research papers from four companies \nhave tried deep learning on this task, including DeepMind, Facebook, Google, and \nMicrosoft. Besides, practitioners also apply deep learning on SE tasks like malware \ndetection [5], development cost or effort estimation [13], etc., and achieve \ncompetitive results. Hence, deep learning may be useful on these tasks from the \nperspective of practitioners. This finding provides a guidance for academic \nresearchers to apply deep learning in practice. \nHowever, we find a mismatch from the top researched SE tasks and the industrial \ninterests. For the top 5 tasks studied by deep learning in Fig. 4, only program learning \n# \nTasks in development (30 papers) \nC1 Program learning and program \nsynthesis (14) \nC2 Automatic software repair (2) \nC3 Code suggestion (2) \nC4 Knowledge unit linking in Stack \nOverflow (2) \nC5 Autonomous driving software (1) \nC6 API description selection (1) \nC7 API sequence recommendation (1) \nC8 Cross-lingual question retrieval (1) \nC9 Code comment generation (1) \nC10 Commit message generation (1) \nC11 Hot path prediction (1) \nC12 Just-in-time defection prediction (1) \nC13 Model visualization (1) \nC14 Source code summarization (1) \n# \nTasks in design (1 papers) \nB1 Design pattern recognition (1) \n# \nTasks in Testing (27 papers) \nD1 Defect prediction (9) \nD2 Reliability or changeability \nestimation (8) \nD3 Deep learning testing (3) \nD4 Energy consumption estimation (1) \nD5 Grammar-based fuzzing testing (1) \nD6 Retesting necessity estimation (1) \nD7 Reliability model selection (1) \nD8 Robot testing (1) \nD9 Test input generation for mobile (1)\nD10 Testing effort estimation (1) \n# Tasks in requirement (1 paper) \nA1 Requirement extraction from natural \nlanguages (1)  \n \n# Tasks in management (12 papers) \nF1 Development cost or effort \nestimation (6) \nF2 Source code classification (4) \nF3 Software size estimation (1) \nF4 Traceable link generation (1) \n \n# Tasks in maintenance (27papers)\nE1 Malware detection (10) \nE2 Bug localization (4) \nE3 Clone detection (3) \nE4 System anomaly prediction (2) \nE5 Workload prediction in the cloud (2) \nE6 Bug report summarization (1) \nE7 Bug triager (1) \nE8 Duplicate bug report detection (1) \nE9 Feature location (1) \nE10 Real-time task scheduling (1) \nE11 Test report classification (1) \n \nIndustrial practitioners participate \nin 13 SE tasks (21 papers) \n• C1: DeepMind, Facebook, Google, \nMicrosoft (8 papers) \n• C5: Fiat Chrysler Automobiles (1) \n• C7: Microsoft (1) \n• C11: Clinc Inc. (1) \n• C13: Facebook (1) \n• D2: URU Video, Inc. (1) \n• D5: Microsoft (1) \n• D9: IBM (1) \n• E1: Baidu, Microsoft (2) \n• E4: Tencent Corporation (1) \n• E8: Accenture Tech. (1) \n• E9: ABB Corporate (1) \n• F1: Motorola Canada Ltd. (1) \nFig. 4. The SE tasks solved by deep learning and participated by industrial practitioners \n9 \n \nand program synthesis, and malware detection attract more than one industrial \npractitioner to participate in. The reason may be that, on the one hand, practitioners \nhave not found a suitable way to apply deep learning on other SE tasks in practice. On \nthe other hand, practitioners already select some lightweight methods to solve these \ntasks. Hence, there is still a long way to apply a complex method like deep learning in \nindustry. \nTo conclude, practitioners participate in more than one fifth research papers. They \nbenefit from deep learning on 13 SE tasks, including program learning and program \nsynthesis, malware detection, etc. \nE. Concerns to use deep learning in SE \nDespite the prevalence of improving SE tasks with deep learning, many concerns \nemerge on the practicability of using deep learning in SE [6]. As a complex and almost \nopaque model, several factors limit the practicability of deep learning, including the \neffectiveness, efficiency, understandability, and testability. These issues may \ninfluence the development of deep learning in SE.  \nEffectiveness and Efficiency. Recent studies show that by applying a simple \noptimizer Differential Evolution to fine tune SVM, it achieves similar results with deep \nlearning on linking the knowledge unit in Stack Overflow [7]. Most importantly, this \nmethod is 84 times faster than training deep learning models. The same phenomenon \nis also observed on code suggestion, in which an adapting n-gram language model \nspecifically designed for software surpasses RNN and LSTM [6]. Although techniques \nlike off-line training and cloud computing may partially resolve the efficiency \nproblem [10], there is still a tradeoff between deep learning and other lightweight, \ndomain-specific models. Such tradeoff drives a deep investigation on deep learning, \ne.g., what types of SE data and tasks are suitable for deep learning and how to \nintegrate the domain knowledge into deep learning.  \nUnderstandability. The understandability is a burden to \"control\" deep learning. \nRecently, several methods are proposed to improve the understandability of deep \nlearning. For example, practitioners in Facebook explore to visualize industry-scale \ndeep neural networks [8]. The proposed tool help software engineers understand the \nneuron activations, individual instances, classification results, and differences \nbetween activation patterns of deep learning. Such tool is a good start to increase the \nunderstandability of deep learning in SE. \nTestability. As a complex model, the testability limits the security of applying deep \nlearning in SE. Hence, researchers attempt to use software testing techniques to \nimprove the testability of deep learning, i.e., deep learning testing [9]. To test deep \nlearning models, coverage testing and metamorphic testing are recently applied [9]. \nCoverage testing validates whether all the neural units in deep learning are correctly \nactivated. Metamorphic testing generates the test oracle for coverage testing. These \nstudies demonstrate the importance of SE techniques on validating artificial \nintelligence techniques like deep learning. \nTo conclude, the practicability of deep learning is still a rising and hot topic for SE \npractitioners and researchers. \n10 \n \nConclusion \nDeep learning recently plays an important role for solving SE tasks. In this study, we \nconduct a bibliography analysis on the status of deep learning in SE. We find that deep \nlearning has been integrated into more than 40 SE tasks by both industrial \npractitioners and academic researchers. Most studies use standard deep learning \nmodels and their variants to solve SE problems. The practicability of deep learning \nmay hider SE practitioners from using deep learning in practice, which is a rising and \nhot topic for further investigation.  \nReference \n[1] Madala, K., Gaither, D., Nielsen, R., & Do, H. Automated identification of component state \ntransition model elements from requirements. International Requirements Engineering Conference \nWorkshops. 2017. (pp.386-392). \n[2] Joulin, A., & Mikolov, T. Inferring algorithmic patterns with stack-augmented recurrent nets. \nNIPS’15. (pp. 190-198).  \n[3] Tong, H., Liu, B., & Wang, S. Software defect prediction using stacked denoising autoencoders \nand two-stage ensemble learning. IST’17. \n[4] Pang, Y., Xue, X., & Wang, H. Predicting vulnerable software components through deep neural \nnetwork. International Conference on Deep Learning Technologies. 2017. (pp.6-10).  \n[5] Dahl, G. E., Stokes, J. W., Deng, L., & Yu, D. Large-scale malware classification using random \nprojections and neural networks. International Conference on Acoustics, Speech and Signal \nProcessing. 2013. (pp. 3422-3426). \n[6] Hellendoorn, V. J., & Devanbu, P. Are deep neural networks the best choice for modeling source \ncode? FSE’17. (pp.763-773). \n[7] Fu, W., & Menzies, T. Easy over hard: a case study on deep learning. FSE’17. (pp.49-60). \n[8] Kahng, M., Andrews, P.Y., Kalro, A., & Chau, D.H.P. ActiVis: visual exploration of industry-\nscale deep neural network models. IEEE Transactions on Visualization and Computer Graphics, \n24(1), 2018. (pp.88-97). \n[9] Tian, Y., Pei, K., Jana, S., Ray, B. DeepTest: automated testing of deep-neural-network-driven \nautonomous cars. ICSE’18. \n[10] Li, X., Jiang, H., Liu, D., Ren, Z., & Li, G. Unsupervised deep bug report summarization. ICPC’18. \n[11] Munassar, N.M.A., & Govardhan, A. A comparison between five models of software engineering. \nInternational Journal of Computer Science Issues, 5, 2010. (pp.95-101). \n[12] Dwivedi, A. K., Tirkey, A., Ray, R. B., & Rath, S. K. Software design pattern recognition using \nmachine learning techniques. Region 10 Conference. 2016. (pp.222-227). \n[13] Huang, X., Ho, D., Ren, J., & Capretz, L.F. Improving the COCOMO model using a neuro-fuzzy \napproach. Applied Soft Computing, 7(1), 2007. (pp.29-40). \n[14] Bengio, Y., LeCun, Y., Hinton, G. Deep Learning. Nature. 2015, 521. (pp.436-444). \n \n",
  "categories": [
    "cs.SE"
  ],
  "published": "2018-05-13",
  "updated": "2018-05-13"
}