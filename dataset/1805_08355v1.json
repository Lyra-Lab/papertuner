{
  "id": "http://arxiv.org/abs/1805.08355v1",
  "title": "Opening the black box of deep learning",
  "authors": [
    "Dian Lei",
    "Xiaoxiao Chen",
    "Jianfei Zhao"
  ],
  "abstract": "The great success of deep learning shows that its technology contains\nprofound truth, and understanding its internal mechanism not only has important\nimplications for the development of its technology and effective application in\nvarious fields, but also provides meaningful insights into the understanding of\nhuman brain mechanism. At present, most of the theoretical research on deep\nlearning is based on mathematics. This dissertation proposes that the neural\nnetwork of deep learning is a physical system, examines deep learning from\nthree different perspectives: microscopic, macroscopic, and physical world\nviews, answers multiple theoretical puzzles in deep learning by using physics\nprinciples. For example, from the perspective of quantum mechanics and\nstatistical physics, this dissertation presents the calculation methods for\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\nMachine, as well as the selection of cost functions, explains why deep learning\nmust be deep, what characteristics are learned in deep learning, why\nConvolutional Neural Networks do not have to be trained layer by layer, and the\nlimitations of deep learning, etc., and proposes the theoretical direction and\nbasis for the further development of deep learning now and in the future. The\nbrilliance of physics flashes in deep learning, we try to establish the deep\nlearning technology based on the scientific theory of physics.",
  "text": "Opening the black box of deep learning\nDian Lei , Xiaoxiao Chen , Jianfei Zhao\nSchool of Mechatronics Engineering and Automation,\nShanghai University, Shanghai 200072, China\nAbstract\nThe great success of deep learning shows that its technology contains profound\ntruth, and understanding its internal mechanism not only has important impli-\ncations for the development of its technology and effective application in various\nﬁelds, but also provides meaningful insights into the understanding of human brain\nmechanism. At present, most of the theoretical research on deep learning is based\non mathematics. This dissertation proposes that the neural network of deep learn-\ning is a physical system, examines deep learning from three different perspectives:\nmicroscopic, macroscopic, and physical world views, answers multiple theoretical\npuzzles in deep learning by using physics principles. For example, from the per-\nspective of quantum mechanics and statistical physics, this dissertation presents\nthe calculation methods for convolution calculation, pooling, normalization, and\nRestricted Boltzmann Machine, as well as the selection of cost functions, explains\nwhy deep learning must be deep, what characteristics are learned in deep learn-\ning, why Convolutional Neural Networks do not have to be trained layer by layer,\nand the limitations of deep learning, etc., and proposes the theoretical direction\nand basis for the further development of deep learning now and in the future. The\nbrilliance of physics ﬂashes in deep learning, we try to establish the deep learning\ntechnology based on the scientiﬁc theory of physics.\n1\nIntroduction\nDeep learning is the main representative of the breakthrough in artiﬁcial intelligence today, it has\nreached nearly human level in image classiﬁcation [1], speech recognition [2], natural language\nprocessing [3] and so on. The method of deep learning is developing rapidly, which almost subverts\nall branches of computer vision ﬁeld. However, the fundamental problem of deep learning at present\nis the lack of theoretical research on its internal principles, and there is no accepted theoretical\nexplanation, namely, the so-called black box problem: Why use such a deep model in deep learning?\nWhy is deep learning successful? What’s the key inside? The lack of theoretical basis has led to\nthe academic community being unable to explain the fundamental reason for the success of deep\nlearning. The theoretical basis is not clear, and we simply do not know from what angle to look at\nit. The black box model is purely based on data without considering the physical laws of the model,\nit lacks the ability to adhere to mechanistic understandings of the underlying physical processes.\nHence, even if the model achieves high accuracy but it lacks of theoretical support, it cannot be\nused as a basis for subsequent scientiﬁc developments [4]. We must not rely solely on intuition\ndesigned algorithmic structures and several empirically tried examples to prove the general validity\nof an algorithm. This research method has the potential to learn false modes from non-generic\nrepresentations of data, the explanatory nature of the model is very low, and the resulting research\nresults are difﬁcult to pass on in the long term. As people’s new ideas have been replaced by more\nand more complex model architectures, which are almost invisible under the weight of layers of\nmodels, calls for attention to the explanatory nature of machine learning are also getting higher.\nTherefore, we need to thoroughly understand the entire system operation of deep learning. We\nneed to explain what the most fundamental problems are in the ﬁeld of deep learning and whether\n1\narXiv:1805.08355v1  [cs.LG]  22 May 2018\nthese fundamental issues are mature enough to be accurately described in mathematical and physical\nlanguages.\nThe great success of deep learning shows that its technology contains profound truth, but the most\nwidely understood way is mathematical analysis, so far, very little attention has been paid to its sci-\nentiﬁc issues. However, purely mathematical explanations may lead to misdirection. For example,\nthe neural network is mathematically trying to approximate any function. In mathematics, it has\nbeen proved that a single-layer neural network can approximate any function if it is long enough,\nthis viewpoint has greatly hindered the development of neural networks, this is why most people in\nthe past neglected multi-layer networks for a long time and without studying in depth. Only a small\nnumber of people such as Yann LeCun, Geoffrey Hinton, and Yoshua Bengio still insist on research\nin multi-layer neural networks [5]. Therefore, from the great successes achieved in deep learning,\nit is far from enough to explain deep learning in mathematics, and the technology of deep learning\nneeds to be based on scientiﬁc theory.\nAs deep learning has made breakthroughs in many aspects such as images, phonetics, and text,\nmethods based on deep learning are increasingly being applied in various other ﬁelds, for example,\nrecently effective in solving many-body quantum physics problems has also been proved. Therefore,\nthe theory of deep learning methods must reﬂect some objective laws of the real world. obviously the\nmost basic and universal theory is quantum physics and statistical physics. What is science? Physics\nis the most perfect science that has been developed so far. Just as most engineering disciplines\nare based on physics, the engineering foundation for deep learning now and in the future will be\nphysics. We need to describe the deep learning concept model in the language of physics, so that\nwe can scientiﬁcally guide the development and design of deep learning. From this we say that the\nkey to the current and future success of artiﬁcial intelligence depends not only on the mathematical\ncalculation, but also on the laws of physics. The theory of deep learning requires physics.\nThe data in the information world is divided into two different types of data: one is symbolic data,\nwhich is designated by our humans; the other is physical data, which objectively reﬂects the data\nof the real world, any actual data set we care about (whether it is a natural image or a voice signal,\netc.) is physical data. Reference [6] shows that the reason why neural networks can perform clas-\nsiﬁcation tasks well is that these physical data x follow some simple physical laws and models can\nbe generated with relatively few free parameters: for example, they may exhibit symmetry, locality,\nor a simple form as an exponent of a low-order polynomial; and symbolic data, such as ”variable\ny=cat” is speciﬁed by humans, in this case the symmetry or polynomial is meaningless, and they are\nnot related to physics. However, the probability distribution of non-physical data y can be obtained\nby Bayes’ theorem using the physical characteristics of x. In the reference [4], a Physics-guided\nNeural Networks (PGNN) is proposed, which combines the scientiﬁc knowledge of physics-based\nmodels with the deep learning. The PGNN leverages the output of physics-based model simulations\nalong with observational features to generate predictions using a neural network architecture. Ref-\nerence [7] shows that deep learning is intimately related to one of the most important and successful\ntechniques in theoretical physics, the renormalization group (RG). Reference [8] using DBM and\nRBM to represent quantum many-body states illustrates why the depth of neural networks in the\nquantum world is so important, revealing the close relationship between deep neural networks and\nquantum many-body problems. Reference [9] establishes a mapping of tensor network (TN) based\non quantum mechanics and neural network in deep learning. Reference [10] mentions that people\nhave found more and more connections between basic physics and artiﬁcial intelligence, such as\nRestricted Boltzmann Machine and spin systems, deep neural networks and renormalization groups;\nthe effectiveness of machine learning allows people to think about the deeper connection between\nphysics and machine learning, and perhaps it can help us gain insights into intelligence and the\nnature of the universe.\nThe research of the above reference mainly takes the neural network as a computational tool, or as a\nmethod to solve the quantum many-body problem. This dissertation studies the artiﬁcial deep neural\nnetwork as a real physical system, considers that the neural network model is a real physical model.\nThe goal of deep learning training is to obtain the neural network system model which accords with\nthe physical laws by the interaction or response between the neural network system and the input\nphysical information. Because the deep neural network is a physical system, its trained model and\nits evolution in training must meet the laws of physics.\n2\nThis dissertation analyzes the principles of physics embodied in deep learning from three different\nperspectives: microscopic, macroscopic, and world view, and describes deep learning with physics\nlanguage, aiming to provide theoretical guidance and basis for further study and future development\ndirection, and tries to establish the technology of deep learning based on the scientiﬁc theory of\nphysics.\n2\nA microscopic view of deep learning\nThe biggest rule of the universe is that the world is made up of microscopic particles such as atoms,\nelectrons and photons, which obey quantum mechanics. Quantum mechanics is the science of study-\ning the motion law of the microscopic particles in the material world, so the neural network model of\ndeep learning as a physical system requires that the model must be governed by quantum mechanics.\nThe following explains deep learning from the basic principles of quantum mechanics.\nThe human brain neural network is composed of atoms, the number of billions of neurons is the\nsame, and the computational methods of the human brain should be similar. The neural network,\nas an interactive quantum many-body system, determines the deep learning system to be described\nby the wave function. The coordinate operator, momentum operator (corresponding translation\noperator), angular momentum operator (corresponding rotation operator), energy operator, and spin\noperator in the neural network are the most basic and important physical quantity or mechanical\nquantity operator.\n2.1\nThe physical meaning of neurons\nInformation has both physical and symbolic meanings, so neurons also have two meanings: 1)\nphysical, 2) symbolic mappings. Now discuss the meaning of its physics. In this dissertation, the ﬁrst\nhypothesis is that the neuron is the scattering source of the quasi-particle wave and the superposition\nof receiving the quasi-particle wave. First look at a classic physics experiment—Young’s double slit\nexperiment.\nFigure 1: Young’s double slit experiment.\nAs shown in Figure 1, the electrons are diffracted through the aperture a, and then diffracted and\ninterfered by b and c. The bright diffraction fringes and patches at F indicate that there is a greater\nprobability of electrons appearing there, and the dark part is there is little or no chance of the appear-\nance of electrons. This dissertation holds that neurons act as electron diffraction interference. When\nwe look at a neuron as a physical unit, the neuron is a scattering potential well that causes scattering\nof quasiparticles (perhaps this scattering originates from the quantum effect in the microtubules of\nthe neurons, perhaps the electron-phonon coherence coupling in the biological system, perhaps some\nother kind of elementary excitation). Therefore, the output after the input of the neuron calculation\nis like the scattering output of the electrons through the circular hole, and the law is determined by\nthe quantum physics theorem. The physical meaning of neurons indicates that, as white light can be\nscattered as red, orange, yellow, green, cyan, blue, violet, it is a natural classiﬁer, calculator.\n3\nIn the Young’s double slit experiment, whatever the input is light, or electrons, polarized electrons,\nneutrons, and any particle including atomic and subatomic levels will cause diffraction and interfer-\nence effects. In the same way, the physical meaning of neurons indicates that neurons are inherently\ncapable of discriminating characterizations and are inherently capable of excellent generalization.\nThe structure of Young’s double slit experiment provides the foundation for a universal quantum\nneural network. The physical model of neurons assumed in this dissertation is:\nThe input of neurons is a multichannel wave function, for example, the input image is a wave\nfunction of multiple pixel points, and the photon (quasi-particle) wave function is superimposed to\nbecome the new scattering source output. The state value of the neuron is the number of quasi-\nparticles (probability) of excitation after superposition, if you visualize multiple or large numbers\nof neurons, you see images of the same nature as the interference diffraction experimentłstreaks and\npatches.\nFigure 2: Neuron physical model diagram.\nThe fundamental of this model is that the neuron is a physical model, which reﬂects the probabil-\nity of its state value, and the related theories and deep learning techniques discussed later in this\ndissertation can conﬁrm the correctness of its hypothesis.\n2.2\nQuantum physical model of CNN\nIn this section, we ﬁrst establish the quantum physics model of the Convolutional Neural Network,\nand then make a scientiﬁc analysis of the CNN based on this model, give a physical explanation of\nthe success of the convolutional neural network, and explore the prospects for further development.\nThe structure of Young’s double slit experiment provides the foundation for a universal quantum\nneural network, as shown in Figure 3:\nFigure 3: Universal quantum neural network.Figure courtesy: [11]\n4\nSimilar to the quantum neural network in reference [11], this dissertation considers that the neural\nnetwork model of deep learning is the physical model of the interference diffraction of photon or\nquasiparticle. The difference is that the input of the network is changed from the photon gun to\nthe image of the input layer. The detection screen is made up of many neurons. Some neurons\nsuperimpose the input photon or quasi-particle states on each path to obtain intensity (probability of\nphotons or quasiparticles) and re-scattering output, the intensity values of all neurons constitute an\ninterference diffraction pattern. In addition to the input, the interference diffraction pattern is also\nclosely related to the structure of the neural network; the connecting line between neurons is related\nto the excitation mode or interaction potential of the neuron and the structure of the neural network,\nand has nothing to do with the input, which is consistent with the usual concept of artiﬁcial neural\nnetwork.\nIs this model correct? Let’s look at two ﬁgures:\nFigure 4: A real interference pattern\nFigure 5: The ﬁrst convolution layer of a typical convolution neural network after training.Figure\ncourtesy:http://cs231n.github.io/understanding-cnn/\nFigure 4 is a diffraction interference fringe obtained from a real double-slit experiment, which con-\ntains the total intensity distribution information of double-slit interference and single-slit diffrac-\ntion. Figure 5 shows the ﬁrst convolution layer after a typical CNN training, it also sees stripes and\npatches. Comparing the two graphs, we can see that the two are very similar, and all of them are\nstripes and patches, which fully shows that our neural network model is the quasi-particle diffrac-\ntion interference model in physics. The neuron receives multiple signals from the previous layer,\nperforms quantum superposition, if the interference superposition elimination, then the neuron re-\nmains silent; if the interference superposition intensity is extremely high (the stripe or patches in the\nﬁgure), then the neuron activates the impulse, emits the superposition intensity output signal. This\nalso explains why physiological neurons may remain silent even if they have strong signal input.\nFrom the ﬁrst layer of the convolutional layer in Figure 5, we can see that most people think that\nthe ﬁrst layer of convolution is to learn the edge of the image, and what the later convolution layer\n5\nlearned is no one can understand. In this theoretical model, the ﬁrst and the back layers are in-\nterference diffraction fringes! Interferometric diffraction or scattering experiments in physics can\nanalyze and discover elements, and deep learning neural networks can be used for a wide range of\napplications such as image recognition because its mechanism is also the mechanism of interference\ndiffraction or scattering experiments. Each fringe or patch reﬂects the momentum and angular mo-\nmentum characteristics of the system, and the momentum or wave vector mechanics respectively\nrepresent the translation and rotation invariance of the space.\nLet us ﬁrst look at the relationship between translation operation and convolutional neural networks.\n2.2.1\nCNN and translation operation\nAccording to quantum mechanics, the space translation operator is:\nˆD(a) = eia.ˆp/hor ˆD(a) = eiˆk.a\n(1)\nWhere a is the translational magnitude, the wave vector operator ˆk = −i∇, the momentum op-\nerator ˆp = −ih∇, ˆp ≡hˆk, h is the Planck constant, the wave vector k and the momentum p are\nindependent of a, and are mechanical quantities only related to the system.\nThe physical quantities that describe the spatial invariance or spatial symmetry of the arbitrary func-\ntion f(x + a) = f(x) are the momentum or wave vector, i.e. the physical conservation quantities\ncorresponding to the spatial symmetry are the wave vector k or the momentum p. For example,\nthe free particle or plane wave function ψ(x) = Ceikx−iωt , where k is the wave vector, equal to\nthe reciprocal of the wavelength, it describes its spatial properties. The edge boundary, texture, and\ncolor change of the image must destroy the translation symmetry. The mechanical quantity wave\nvectors can be used to express whether the edge, texture, color of the image is equal or unequal in\nspace. The wave vector is the basic physical quantity quantifying these attributes.\nThe translation in a one-dimensional case is easy to prove:\nea d\ndx f(x) = f(x) +\n∞\nX\nn=1\nan\nn!\ndn\ndxn f(x) = f(x + a)\n(2)\nwhich is:\nˆD(a)f(x) = f(x + a)\n(3)\nThe convolution calculation in the CNN actually reﬂect the translation operation, and the one-\ndimensional convolution in book [12] for image I(x) and kernel function K(a) is as follows:\nS(x) =\nX\na K(a)I(x −a)\n=\nX\na K(a)e−a d\ndx +a d\ndy I(x)\n=\nX\na K(a)e−ˆk.aI(x)\n=\nX\na K(a) ˆD(a)I(x)\n(4)\nThe two-dimensional form is as follows:\nS(i, j) =\nX\nm\nX\nn K(m, n)I(i −m, j −n)\n(5)\nThe entire feature map layer image is obtained by translation. To understand the image, consider the\none-dimensional case, the minimum unit of translation is a pixel, if K(a) =\n\u001a\n1, (a < r)\n0, (a ≥r)\nis set.\nFor the input layer is the image, only two points are taken within the |r| range, the range is small,\nand the output of the convolutional layer is:\nS(x) =\nX\na K(a) ˆD(a)I(x) = ( ˆD(0)+ ˆD(1))I(x) ≈(2 + d\ndx)I(x)\n(6)\n6\nSo the ﬁrst convolution layer sees the edge of the image element in the background, but it is not\nequal to the edge of the image element because of the convolution kernel.\nIf the image I(x) has spatial symmetry with wavelength λ, or if the wave vector has spatial symmetry\nof k = 2π/λ , i.e. I(x) = sin(kx) , then the output of the convolutional layer is:\nS(x) =\nZ r\n0\nˆD(a)I(x)da =\nZ r\n0\nsin k(x + a)da = 2\nk sin kr\n2 sin(kx + kr\n2 )\n(7)\nAfter convolution, the value still has the same wave vector k volatility, but the intensity is\n4\nk2 sin2 kr\n2 ,\nthe interference diffraction envelope appears, the maximum and the minimum intensity values ap-\npear in:\nkr = nπ\n\u001a\nn = 1, 3, 5, · · · the intensity is extremely high\nn = 2, 4, 6, · · · the intensity is zero\nThe above results show that an interferometric diffraction image is obtained by spatial convolution.\nWhen the input wavelength is longer, the intensity at the maximum by the convolution is ampliﬁed\nto 4/k2 times the original input wave intensity. When the convolution kernel structure is certain,\nonly the wave vectors satisfying certain conditions will appear extremely large. Assuming that only\nthe brightest point is taken, then only the input wave vector satisﬁes k = π/λ sine wave through the\nconvolution after the ”bright spot.” That is to say, in the case of a convolutional kernel structure, the\n”brightest” reﬂects the wave vector k of the sine wave. The actual wave is the superposition of sine\nwave, and the ”brightest” point of the interferometric diffraction image will not have only one point.\nThe interference diffraction image reﬂects the spatial symmetry of the input wave.\nThe next section explains what the physical meaning of the convolution formula (Equation 4) is,\nexplains the nature of the convolution kernel and explains why the convolution calculation is not\nsummed over the entire image range, but instead use a ﬁnite window convolution calculation such\nas 3*3? Why do these moving convolution windows use the same convolution kernel, the so-called\nshared weight? Why does a convolutional layer use multiple feature mapping layers? Why does\neach feature mapping layer convolutional kernels differently? Why pooling? Why use Relu acti-\nvate functions, etc. In our model, the neuron is a unit that receives multiple inputs and quantum\nsuperimposes and then scatters, so a perfect answer can be obtained by using the scattering theory\nof quantum mechanics.\n2.2.2\nPhysical meaning of convolution calculations and convolution kernels\nA general CNN interprets a convolution kernel as a ﬁlter, but what does it ﬁlter? What does it\nkeep? It is not clear, so this argument is difﬁcult to convince, and the ﬁlter’s opinion is not clear.\nConvolutional neural networks have achieved great success in the ﬁeld of computer vision, one of the\nmost important components is the convolution kernel, which is primarily responsible for capturing\nmost of the abstraction of the network. In contrast, this component is the least understood processing\nblock because it requires the maximum computational learning [13].\nThis section builds the convolutional calculations on the quantum physics model described in Sec-\ntion 2.1. The physical basis of the interference diffraction experiment is quantum mechanical scat-\ntering theory. According to the scattering theory, the system Hamiltonian does not contain time, and\nthe incident wave is directed at the target. The wave function ψ at the position r is: [14].\nψk(r) = eik·r −\nZ\nG(r −r′)U(r′)ψ(r′)dr′\n(8)\nEquation 8 is the integral equation, the integral is performed in the entire space, the ﬁrst is the input\nwave, and the second is the scattering wave. The physical meaning of the second item is very clear,\nU is the interaction potential function. The input wave scatters in the dr′ range near r′ point to\nform a scattering point source with intensity U(r′)ψ(r′)dr′ . This point source propagates to the\nobservation screen r point according to the outgoing Green function, and the scattering of the points\nis superimposed as the scattering probability amplitude ψsc .\nThe Green function is:\nG(r, r′) = −\neikr\n4π |r −r′|\n(9)\n7\nThe incident wave requirement in Equation 8 is monochromatic and planar wave, and does not do\nany other approximation. The equation is applicable both to elastic scattering and inelastic scattering\nor collision scattering. So the total intensity of a neuron scattering from a quasi particle of a certain\nwavelength is\nS =\nZ\n|ψ(r)|2dr\n(10)\nThe integral r is in the range of neuron size. According to this model, the total intensity of a neuron\nis deﬁned as the probability density ψ2 of the excited particle, so\nS ≈|ψ(0)|2\n(11)\nLet S be the square of the convolution s , that is S = s2 , then the convolution s is:\ns =\n\f\f\f\f\nZ\nG(r′)U(r′)ψ(r′)dr + b\n\f\f\f\f\n(12)\nNote that the left side of Equation 8 is a wave function, and there are also wave functions in the\nintegral term, so it is difﬁcult to solve. As an approximate calculation, let the ψ in integral as input\nimage wave, converted to discrete expression. The volume element dxdydz = 1 , so the convolution\nfor a wave vector k is sk :\nsk =\n\f\f\f\nX\nr′ G(r′; k)U(r′)ψ(r′) + b\n\f\f\f\n(13)\nWhere the sum range of r′ is within the neuron volume range. The convolution kernel is:\nK = G(r′; k)U(r′)\n(14)\nψ(r′) is the input image wave, it is only related to x′, y′ , and has nothing to do with other coordi-\nnates:\nsk =\n\f\f\f\nX\nx′y′z′ G(x′, y′, z′; k)U(x′, y′, z′)ψ(x′, y′) + b\n\f\f\f\n=\n\f\f\f\nX\nx′y′\nX\nz′ G(x′, y′, z′; k)U(x′, y′, z′)ψ(x′, y′) + b\n\f\f\f\n(15)\nThe square s2\nk of sk represents the quasi-particle probability density that is excited when the incident\nwave (or quasi-particle) has a wave vector k (size, direction, and polarization direction). So the\nconvolution kernel:\nKx′y′(k) =\nX\nz′ G(x′, y′, z′; k)U(x′, y′, z′)\n(16)\nx′, y′ is a two-dimensional image coordinate, sk represents the convolution of the feature mapping\nlayer, and different wave vectors have different sk (if the quasi-particles are electrons also have a\ndirection: spin)\nsk =\nX\nx′y′ Kx′y′(k)ψx′y′ + b\n(17)\nThe convolution of all k is a set S = {sk}.\nThe following is a discussion of its physical meaning and its implications for the calculation of\nconvolutional neural networks.\n2.2.3\nQuantum mechanics interpretation of CNN\nThere are various interpretations of convolutional neural networks, but according to the above quan-\ntum physics model, CNN can be perfectly interpreted. The interpretation is fundamental and deter-\nministic, it will inspire the architectural design of deep convolutional networks.\n(1) When the inner product of the convolution is represented by the norm and the angle:\n∥ω∥∥x∥cos(θ(ω,x)) , the trained inner product of the convolutional neural network can\nbe decoupled to ﬁnd the relationship between norm and angle in the feature map [15].\n8\nFigure 6: CNN learned features are naturally decoupled.Figure courtesy: [15]\nIf the convolution of spherical coordinates is used, the relationship between the norm and\nangle in the feature map is more obvious:\nFigure 7: 2D feature visualization on MNIST dataset with natural training.Figure courtesy: [15]\nThe ﬁgure shows that the angles represent semantic/label differences, and the feature norm\nrepresents within-class differences. This result can be explained in our model. It can be\nseen from (Equation 14) that the general action potential U(r) is only related to r , i.e.\nU(r) = U(r) . So the convolution kernel is related to the norm and has nothing to do with\nthe angle, the CNN feature image is related to the angle. The above experiments verify the\ncorrectness of the physical model of the convolutional neural network.\n(2) The number of convolution kernels is related to the color of the image. The input re-\nquirement in the scattering equation(Equation 8) is a monochromatic wave, that is, the\nconvolution kernel(Equation 16) is related to the wavelength of the input wave, and differ-\nent wavelength should have different convolution kernel Kmn. Applied to the color image,\nconvolution neural network should have different convolution kernel for different colors\nimage. For example, they can be composed of three kinds of convolution kernels: red,\ngreen, and blue. Of course, they can also be composed of multiple kernels such as red,\norange, yellow, green, blue, and purple.\n(3) The number of convolution kernels is also related to the polarization direction of the inci-\ndent wave. It can be seen from(Equation 16, 17) that even if the image is monochromatic,\nthe convolution kernels of different z are different. So even if the input is a black-and-\nwhite image, multiple feature mapping layers (multiple convolution kernels) are needed,\njust as white light contains light of all colors, and random monochromatic light also con-\ntains polarized light of all polarization directions. If the input is an electron, the number\nof convolution cores is related to the spin direction. In a comparable size, the number of\nfeature map layers is much larger than the convolutional window size.\n9\n(4) The reason of partial receptive ﬁelds is not because the pixels are usually highly corre-\nlated in neighboring regions. The convolution kernel sharing of feature layer is not due to\ntranslation invariance, they are due to the limited scope of the action potential U(r′) of the\nneuron, that is, the coordinate x′, y′ in the convolution kernel (Equation 16) is 0 outside the\nrange of action potential. Therefore, the summation of the x′, y′ in (Equation 17) does not\nrequire full space, i.e.the CNN should not be fully connected but partial connection. For\nthe same feature mapping layer, all the neuron potentials U(r′) are the same, so the convo-\nlution kernels of all the sliding window convolutions of the same feature mapping layer are\nthe same or share one, which explains from the scientiﬁc theory that the fundamental rea-\nson for the success of CNN is the important characteristic of CNN: partial receptive ﬁelds\n+ weight sharing. In the actual application of CNN, the convolution kernel size (that is, the\nconvolution window size) is often 3x3, 5x5, 9x9 and so on, scattering is mainly concen-\ntrated in the incident direction, so the window size is too large to be meaningful. However,\nif the 1*1 size of the convolution kernel is used, the interference effect will also be poor. In\naddition, considering the symmetry of K, convolution kernel size is generally used odd.\n(5) The purpose of pooling operations is not only to reduce the size of the output eigenvectors.\n(Equation 15) is the intensity of a neuron, and the image formed by all the neurons of a\nconvolutional feature layer is an interference diffraction pattern. Because the coherence of\nthe wave is strong in some places and weak in some places and not even in some places,\nand those ”bright” neurons will be the scattering sources of the subsequent convolutions,\nso each convolutional layer must be pooling. That is, in the convolution window area,\nthe ”bright” neurons must be selected according to the intensity values of the neurons in\neach convolution layer feature layer, which is also the physical reason why the biological\nneuron can either output or not output even if it has a signal input. Obviously, pooling\nis an important computing component. In the early years, many of the studies based on\nconvolution architecture used average pooling, now they are replaced by Max Pooling.\nFrom our model we can see that in practical application, because the window size is not\nlarge, it is reasonable to select one of the most bright neurons, that is, to adopt the max\npooling method. It is also seen from here that the pooling is of decisive importance to the\ninitial convolution layer, but as the number of layers increases, the interferogram sharpens\nand bright points become less, so the effect of pooling is weakened, which is the same as\nthe result of the reference [16]. Pooling can also be explained by the renormalization group\nin Section3.4.\n(6) The convolution kernel of different layer is not same, because each of the ”brightest” neu-\nron positions represents the coherence of the corresponding convolution window image,\nwhich is related to the symmetry of the image and the structure of neural network. After\npooling, the spacing as a new scattering source is different, so subsequent convolution ker-\nnels will also be different. That is, except for the same feature mapping layer, convolution\nkernels of each feature map layer of the convolution layer are different.\n(7) Coherence is the most important condition for multilayer CNN. In our model, the purpose\nof convolution is to create the entanglement of each pixel in the image under the action\nof neurons, thus forming the interference diffraction fringes or patches according to the\nspatial translation structure. According to the quantum mechanics, this is a coherence\nphenomenon caused by the superposition principle of waves. The total intensity after inter-\nference superposition is not necessarily equal to the sum of the intensity of the sub-beams,\nmay be more strong or may be equal to 0 under the interference. The important condition\nof coherence is the coherent wave. There are two ways to produce coherent wave, the ﬁrst\none is to ensure that the monochrome of the wave and the phase of each wave of the ﬁxed,\nso need multi-layer convolution; the second is the interference of oneself and oneself, so\nneed multi-layer convolution, generally need at least 3 or more.\n10\nFigure 8: Interfering with oneself causes CNN to require multiple layers of convolutional layers.\nIf we understand from the biological neural network, the wavelength of the natural color\nof the physicist’s world changes monotonously from red to purple, but the system of hu-\nman perception of color is closed-loop, such as the combination of red light and purple\nlight is understood as the monochromatic magenta color. However, does not physically\nexist with a single physical wavelength of light corresponding to the color, but the human\nperception system fusion understands as a single color. Therefore, the convolution neural\nnetwork must be fused or transformed after the input scattering (decomposition) must be\nmultilayered.\n(8) Convolution is not the extraction of human knowledge such as image edges or colors, the\nconvolution is the physical law and does not require human prior knowledge. From (Equa-\ntion 2) to know the image gradient is obtained after the translation operation, so for the ﬁrst\nfeature layer of the image we can see that it reﬂects the edge of the image. But as the subse-\nquent continuous pooling and convolution, feature layer image will become more and more\ndifﬁcult to understand and abstract compared to the original layer. However, in our model,\nthe convolution image is a diffraction stripe or patch image after destructive interference\nor constructive interference, which is the superposition of the same or coherent wave. That\nis, the images are decomposed and classiﬁed by the same coherent attribute. The wave\nof the same phase has a destructive interference and the opposite phase has a constructive\ninterference, and the interference diffraction patterns will effectively deconstruct the entire\ninformation of the incident image. And this coherent property is the symmetry of the im-\nage, according to quantum mechanics, the physical quantity described by the symmetry is\nthe wave vector. It can also be said that CNN is to measure the wave vector of the image,\nbecause it is the XY plane convolution of the image, i.e. the measurement of the momen-\ntum px, py in the x, y direction. According to quantum mechanics, the z directional angular\nmomentum operator is deﬁned as: ˆlz = xˆpy −yˆpx , so it also detects angular momentum\nor rotational symmetry around the z direction. That’s to say, a trained CNN measures the\ntranslational symmetry of the x, y direction of the image or the rotational symmetry of the\nz direction through the translational operation and interaction of the image, is classiﬁed\nas the interference diffraction fringes or patch image that we see. The feature map layer\nis effectively classiﬁed according to the symmetry, and then the whole connection layer is\nmapped, which can effectively recognize the image.\nIn summary, the above research shows that LeCun’s most famous contribution, the convolutional\nneural network, is entirely based on prior knowledge and that the idea of not requiring human struc-\ntured knowledge is completely correct. CNN is based on the scientiﬁc theory, so it is the best method\nin image recognition. CNN is a physical model, so it is very successful in the processing of physical\ndata such as images, videos, sounds, condensed matter physics, etc., but the performance is worse\nthan other models when processing the strong subjectivity of symbolic modeling. For example, no\nmatter how much data is input, it is impossible to train a model that can read product descriptions\nand generate an appropriate code base [17].\n11\n2.2.4\nSigniﬁcance of classiﬁcation layers in CNN\nThe convolutional layer is followed by a classiﬁcation layer whose meaning maps the mechanical\ncharacteristics learned in the convolution layer to human symbols (marks). The signiﬁcance of the\nclassiﬁcation layer is expressed in:\n• On the signiﬁcance of the classiﬁcation layer, the ﬁrst is mathematics. In order to effec-\ntively apply the ability of neural network that can approximate arbitrary functions mathe-\nmatically, the classiﬁcation layer must be fully connected.\n• The classifying layer in neural network is also physical. In our model, the neuron is a\nprobability wave. The output of the last layer’s activation function must be guaranteed\nto meet the normalized requirement, generally, the softmax function with a clear physical\nmeaning is used. The activation function is shown in Section 2.5.\n• The classiﬁcation must have generalization and must obey the laws of statistical physics\nand must cooperate with entropy. The content of entropy is shown in Section 3.2.\n2.2.5\nDevelopment prospects of CNN\n(1) According to the neuron scattering theory, strictly speaking, the convolutional neural net-\nwork should be extended to the complex number ﬁeld so that it can use the wave function\nwith phase information. Thus, the input image is the intensity, and it should be squared\nroot when used as a wave function, which is equivalent to the fact part:\nψ(x, y) =\np\nI(x, y)\nHowever, in the actual calculation of CNN, the calculation on the complex number ﬁeld\nmay not be signiﬁcant. In Equation 10, when we compute S, we need to square ψ , and\n|ψ|2 is proportional to the intensity of the image. The calculation of S in the real ﬁeld\nwill appear negative, but subsequent Relu rectiﬁcation guarantees that the output is posi-\ntive. It still conforms to the model where the value of the neuron is the intensity value, the\nsigniﬁcance of max pooling is to ﬁnd the brightest neurons. In short, if the convolution\nis calculated in the complex number ﬁeld, the square root is computed ﬁrst, and then the\nsquare is computed, which is equivalent to repeating computation. And if the convolu-\ntion is calculated in the real ﬁeld, input do not square root, output also do not square, the\nRelu rectiﬁcation function guarantees that no negative number will appear. By reducing\nthe repetition calculation and saving the calculation amount, the calculation error can be\nreduced. There is no big difference between the calculated effect on the real ﬁeld and the\nstrict complex number ﬁeld.\n(2) Choosing the appropriate convolution kernel is crucial for obtaining the most signiﬁcant\nand important information contained in the input signal. This allows the model to make\nbetter inferences about the content of the signal. The goal of this transformation is to\nchange the data in a way that is more easily separated by the classiﬁer. According to\nEquation 16, a better model is designed using the convolutional features, such as better\ncoordinates: cylinder coordinates or spherical coordinates of CNN [15].\n(3) The training of CNN based on quantum scattering theory obtains a large amount of input\ninformation, but the ﬁnal classiﬁcation only occupies a small part of the information. These\nare correct and objective reﬂections of the input information, it is easy to drown in the\ntraining process. This is undoubtedly a waste of valuable prior probabilities that can be used\nto migrate large-scale network knowledge into small-scale networks. Therefore, making\nfull use of this information or data migration learning in other ﬁelds is also a topic worthy\nof study.\n(4) Multilayer CNN can not only perceive the translational symmetry and rotational symme-\ntry of input, but also transform and perceive the fusion of various combinations of col-\nors. In addition, it should also can transform and perceive the polarization direction of\nlight. That is, CNN can perceive very rich three-dimensional scenes and behavioral in-\nformation from static two-dimensional images. The geometry school will laugh : how\ncan an image calculate three-dimensional, which is mathematically impossible. It doesn’t\nmake any sense mathematically, but it makes sense physically! Human beings can perceive\n12\nthree-dimensions from two-dimensional images, and deep convolution neural networks can\nextract 2.5-dimensional information from two-dimensional images. This is the quantum\neffect: White light can be decomposed into red, orange, yellow, green, cyan, blue and vio-\nlet through neuronal interaction or coupling or entanglement, and random monochromatic\nlight can be decomposed into various polarized light. According to the physical model\nabove, a lot of new information can be found from the polarization information, such as\nstereo information, like the principle of stereoscopic ﬁlm. This is an important prediction\nof the deep convolution neural network in this dissertation.\n(5) The CNN discussed above is to classify the spatial symmetry of the image by spatially\ntranslating and interacting with neurons. After the above measurement, the state after the\ncollapse is measured as the new environment of Hamiltonian to state a new round of evo-\nlution. If the incident wave changes, such as the time dependent dispersion of the packet,\nthe visual retention, or the Hamiltonian contains time, then it will be a Schr¨odinger equa-\ntion that contained time. At this time Equation 17 is time-containing, the green function\nis divided into the delayed Green function and the advanced Green function, which can\nbe computed to predict the short future by or to recall a brief past. For time translation\noperations, according to quantum mechanics, the conserved quantity of time translation\ninvariance is energy. Through the convolution of time translation, it is also possible to clas-\nsify the energy properties of the input information. And the scattering of neurons contains\ntime, so it will be an important research direction and may explain the important functions\nof visual retention, visual prediction and memory.\n(6) The convolution neural network based on quantum scattering theory is expected to compute\nthe internal properties of the biological neurons and to gain more understanding of the\nfunctions of the biological neurons and the human brain.\n(7) CNN is supervised training, according to the above theory, it can be used in unsupervised\npre-training and training.\n(8) In our neural network model, all the neuron cells of CNN form an image of a grid cell and\na location cell, which is very similar to the image of the biological grid cell and position\ncell. It will be an important research direction to combine CNN with LSTM to achieve\nnavigation like the brain grid cells.\n2.3\nDeep learning based on energy model\nEnergy is one of the most important concepts in physics and even in the entire natural sciences.\nAll forms of physical movement have the concept of energy, such as mechanical, electromagnetic,\nthermal, light, chemical movement, and biology, etc. The analysis of energy can greatly simplify the\nanalysis of material motion. Energy is the measure of movement transformation, and it is an additive\namount, following the law of conservation of energy. Energy in quantum mechanics is represented\nby the Hamiltonian operator.\nIn our model, the strength of neurons is described by the square of the wave function. The typical\nmodel of the neural network composed of interacting neurons in deep learning is the Boltzmann\nmachine.\nAccording to quantum mechanics, the wave function of the system is Ψ, and the statistical average\nof the mechanical quantity O is:\n⟨Ψ | O |Ψ⟩\n⟨Ψ | Ψ⟩\n=\nP\nx |Ψ(x)|2O(x)\nP\nx |Ψ(x)|2\n(18)\nThere is an important conclusion between the quantum mechanics measurement and the statistical\nsampling of wave functions: if Ψ is the eigenstate of the system energy, then:\n\nH2\u000b\n=\nD\nH⟩2 = E2\n0\n(19)\nThat is, the variance of the Hamiltonian energy H is 0, which means that if the system is in the\nground state E0 , the statistical ﬂuctuation completely disappears. This feature is very important\nbecause it means that the closer we are to the ground state, the less ﬂuctuations we have in the amount\n13\nwe want to minimize, and the less energy we have. According to quantum statistical physics, if the\nsystem is a closed system, there is energy ﬂuctuations, but the total number of particles is constant,\nthe multi-body probability density ρ(x) satisﬁes the Boltzmann distribution:\nρ(XN) =\n1\ncNzN(T, V )e−βH(XN)\n(20)\nPartition function is: Z =\n1\ncN\nR\ndXNe−βH(XN) , probability density is the normalized |Ψ|2 . For a\nneural network composed of interacting multi-body quantum systems, the general form of Hamilto-\nnian is:\nH = −ℏ2\n2m\nN\nX\ni\n∇2\n⃗ri +\nX\ni\nV1(⃗ri) +\nX\ni<j\nV2(⃗ri,⃗rj)\n(21)\nThe potential energy between two particles in physics is chosen as a function of the distance between\nparticles in the form of:\nV12 = V (⃗r1,⃗r2) = V (|⃗r1 −⃗r2|) = V (r12)\n(22)\nWithout regard to kinetic energy, only the interaction is considered, the interactions always interact\nin pairs, that is, the interaction energy (potential energy) takes only quadratic term, so the energy\nmodel is obtained:\nE(v, h) = −bT v −cT h −vT Wh\n(23)\nWhere b,c and W are all unconstrained, real-valued learning parameters. The model is divided into\nvisible layer v and hidden layer h, and the interaction between them is described by matrix W [18].\nThis is the most basic component of deep learningłthe Restricted Boltzmann Machine energy model.\nIt is consistent with the view put forward in this dissertation that neural network in deep learning is\na physical system and conforms to the laws of physics. For Boltzmann distribution sampling, the\nmethod of random gradient descent that minimize the energy can determine network parameters b,\nc and W.\n2.4\nRelationship between energy model and convolution model\nFrom the above analysis, CNN is based on accurate quantum physics, and RBM is based on statisti-\ncal physics which will be described later, the difference is pure ensemble and hybrid ensemble.\nThe traditional RBM can only express the probability distribution function with a positive value.\nIn order to make it suitable for describing the wave function with phase information, Carleo and\nothers extend the parameters of RBM to the complex ﬁeld. In addition, in the actual calculation, the\nfunction form used by Carleo is the product of multiple RBM that share weights. This structure is\nequivalent to a single hidden-layer convolutional neural network, thus ensuring the spatial translation\ninvariance of the physical system in the structure [19].\nIn addition, the RBM ignores the effect of the kinetic energy of the neuron, that is, the centroid\nmotion, or ignores the quantum effect of the neuron itself, so the effect of the model is obviously\nworse than that of the CNN.\n2.5\nNormalization of neural networks\nIt has been explained that the wave function of the neural network system is ψ . According to\nquantum mechanics, the meaning of the neuron is that each neuron represents the probability |cn|2\nthat eigenvalue Fn can be measured by a group of mechanical quantities F , and the wave function\nsatisﬁes the normalized condition, so P\nn\nc2\nn = 1.\nOnce the wave function of the system is determined, the average value of the mechanical quantity\nand the probability distribution of the measurement result can be obtained through the mechanical\nquantity measurement. The results obtained by the physical system are obtained by measurement,\n14\nwhereas in quantum mechanics, measurements differ from those in classical mechanics. The quan-\ntum measurements affect the measured subsystems, such as changing the state of the measured\nsubsystem, which called the wave function collapse, and the measured result accords with a certain\nprobability distribution. In other words, when the mechanical quantity is measured, the state is col-\nlapsed to the eigenstate of this mechanical quantity, then the mechanical quantity is its eigenvalue.\nBut it may also be other eigenvalues, that is, it may also be collapsed to other momentum eigenstate,\nwhich is a probability, so it must have:\nX\nihl\ni = 1\nThat is to meet the normalization conditions. Quantum measurement is the core issue of the quantum\nmechanics interpretation system.\nIn deep learning, considering the concept of mechanical quantity measurement, the result output of\neach layer neural network characteristic (eigenvalue) learning should be a quantum measurement,\nso the activation function is needed to determine its probability distribution. If there is no activation\nfunction, then the network can only express the linear mapping. Even if there are more hidden layers,\nthe entire network and the single-layer neural network are equivalent. The meaning of the activation\nfunction is the output of the quantum measurement, which determines its probability distribution\nand is normalized. The most important activation functions are:\n• softmax(hi) =\nexp(hi)\nP\nj exp(hj).\n• sigmoid(hi) =\n1\n1 + exp(−hi)\n• Relu(hi) =\n\u001a\nhi, (hi > 0)\n0, (hi ≤0)\nThe ﬁrst activation function guarantees that the sum of all output neurons is 1.0, which guarantees\nthe normalization of its probability distribution. So the last layer basically apply this activation\nfunction. The softmax function has a clear physical meaning, which is the Boltzmann probability\ndistribution of the ideal gas or quasi particle.\nThe sigmoid activation function is more commonly used in traditional neural networks, but it is not\nsuitable for deep neural networks. A major breakthrough in deep learning technology is the use of\na third activation function. The Relu function directly outputs a probability or probability density\ndistribution which is a quasi-particle number in our physical image, and it must be greater than or\nequal to 0.\n3\nA macroscopic view of deep learning\nThe deep neural network is an interacting quantum multibody system, according to quantum me-\nchanics, by determining the wave function p of the quantum multi-body system, the whole in-\nformation of the quantum system is grasped. The core goal is still to ﬁnd the solution ψ of the\nSchr¨odinger equation, this is generally very difﬁcult. However, because we are mainly concerned\nwith the macroscopic properties such as statistical mean of system observations, macroscopic per-\nformance is subject to statistical physics. Statistical physics is a bridge between microscopic and\nmacroscopic, which determines the condition of macroscopic equilibrium state and stable state, and\nthe change direction of macroscopic state. Quantum statistical physics is described in two different\nways, pure ensemble and hybrid ensemble, so there also are two different kinds of neural networks.\nEnsemble is an abstract concept, which is divided into pure ensemble and mixed ensemble. If\nthe N subsystems in the ensemble are all in the same state |ψ⟩, then the ensemble is called pure\nensemble and they are described in pure state |ψ⟩. If N systems have N1 systems in state |ψ1⟩, N2\nsystems are in state |ψ2⟩, ... Ni are in state |ψi⟩,..., the probability of each measurement system\nbeing in |ψ1⟩, |ψ2⟩, · · · , |ψi⟩, · · · state is P1 =\nN1\nN , P2 =\nN2\nN , · · · , Pi =\nNi\nN , · · · , and the set\nof N systems(N →∞)is called a hybrid ensemble, which is described by a set of all |ψi⟩and\nPi(i = 1, 2, · · · ) .\n15\n3.1\nPurely ensemble and hybrid ensemble neural networks\nThe theory of quantum statistical physics is used to design the neural network architecture, which\nhas two kinds of neural networks, pure ensemble and mixed ensemble. The practical applications of\ndeep learning generally have both pure ensemble and mixed ensemble neural network structures.\nPure ensemble is described by the pure state wave function, so the pure ensemble neural network\narchitecture goal is to ﬁnd the connection weights of the neural network through training so that the\npure state can be constructed together with the input, and each layer can obtain the probability of\neach value (characteristic quantity) of the observed amount. What it pursues is a numerical solution\nof the wave function.\nThe neural network structure of hybrid ensemble is suitable for using unsupervised training to ﬁnd\nthe set—mixed state, which does not correspond to an observation. and is designed to reconstruct\ninput. It is not from the Schr?dinger equation of quantum mechanics to ﬁnd the wave function of\nthe system. Instead, it believes that under certain macroscopic conditions, at certain moments the\nsystem will be in a quantum state with a certain probability. The hybrid ensemble requires secondary\nstatistics, which cant be solved in quantum mechanics, so wo need to introduce additional basic\nassumptions of statistical physics—when the isolated system reaches equilibrium, the probability\nof appearance of various microscopic states is equal. The macroscopic quantity of the system is\nthe statistical average of the various quantum states that the system may be in according to the\ncorresponding microscopic quantity.\nOne important technical difference between these two neural network structures is:\nPure Ensemble only needs to ﬁnd out the probability distribution |cn|2 of each layer’s observable\namount through supervised training, it can satisfy the requirement of human application, do not have\nto perform layer-by-layer pre-training, and each individual layer is entirely determined by quantum\nmechanics. From Equation 13 we know that the process is a physical calculation and the result is\na possible microscopic state, whether this microscopic state is a macroscopic state that satisﬁes the\nrequirements will be determined by supervised training at all levels. The deep learning of hybrid\nensemble, because there is no computational formula of quantum mechanics like Equation 13, it\nmust be pre-trained layer by layer to ﬁnd the probability distribution |ψi(x)|2 of the various states of\nthe ensemble, then all layers should have supervised training to ﬁnd the probability distribution P of\nthe systems state ψi(x) , so there must be a second statistical calculation. The size of P cant be solved\nby quantum mechanics, it requires statistical mechanics principles or assumptions. This answers\nYoshua Bengio’s perplexity in the paper [20]: training depth-supervised neural networks is usually\nvery difﬁcult without pre-training with unsupervised learning, but the exception is CNNs. So we are\nvery curious about what special point in the structure of CNN make it has very good generalization\nperformance in tasks such as image processing? The answer is that CNN is the purely ensemble\nof neural network structures, as explained earlier, its individual layers are completely described by\nquantum mechanics. The hybrid ensemble of neural networks that must be used for unsupervised\nlearning layer-by-layer pre-training, such as the RBM described previously.\n3.1.1\nPure ensemble neural network\nThe pure ensemble neural network are similar to instrumentation, belongs to the category of physical\nmeasurement, so its learning performance can be checked because of its clear physical meaning.\nThe pure ensemble neural network has a prominent advantage. The model it trains is likely to be\na general model, sometimes it is not only useful in this ﬁeld, but also can be used across ﬁelds.\nFor example, the CNN trained on ImageNet can also be used for image recognition in furniture\nprojects, as well as for a wide range of computer vision applications, and can effectively serve as a\ngeneral model for our visual world. However, as the network deepens, the versatility of the neural\nnetwork away from the input layer will be signiﬁcantly reduced, and it is generally not reusable. The\nreusability depends on the depth of the model, because measurement is essentially the interaction of\nthe input signal and the neural network. As the layer increases, the more damage to the system, the\nstronger the speciﬁcity of the measurement results and the less versatility. For example, in a speciﬁc\nCNN, layers appearing earlier in the model will extract locally highly generalized feature maps,\nwhile higher levels will abstract more abstract concepts (such as cat ears or dog eyes). Therefore, if\nyour new dataset differs signiﬁcantly from the dataset trained by the original model, it is better to use\n16\nonly the ﬁrst few layers of the model for feature extraction instead of using the entire convolution\nmodel.\nThe pure ensemble neural networks are completely determined by quantum mechanics, so it is bound\nto have a good effect once applied. For example, using CNN to recognize the image is the best, and\ncan be widely applied to the front end of various types of neural networks, and can be widely used\nin the front-end of various types of neural networks.\n3.1.2\nHybrid ensemble neural network\nThe hybrid ensemble neural networks are neural networks such as classiﬁer, generator, automatic\nencoder decoder, and the like. The hybrid ensemble neural networks and pure ensemble neural\nnetworks are often opposite in their characteristics, and the well-trained hybrid ensemble neural net-\nworks are generally not reusable; the performance of learning is not checkable, and it is essentially\na black box, because the view of the wave function does not have any macro meaning. For example,\nthe information that the classiﬁer in image recognition learns about the probability of existence of a\nclass in the entire graph, completely frees from the concept of space. For issues where the location\nof the object is important, densely connected characteristic will be essentially useless.\nIn hybrid ensemble, the observations of mechanical quantities are the results of two statistics. The\nﬁrst is quantum mechanics, which is caused by the probability property of the state vector; and the\nsecond is due to the mixture of states, which is caused by the statistical nature of the state at which the\nsystem may be. So the hybrid ensemble of neural networks must be pre-trained with unsupervised\nlearning to meet the probability distribution of the state vector of quantum mechanics, and then use\nsupervised learning to ﬁne tune the entire network (second statistic) to determine macroeconomic\nequilibrium or stable state.\n3.2\nUnderstanding deep learning from the concept of entropy\n3.2.1\nThe role of entropy in deep learning\nInformation is not a simple mathematical concept, but a basic physical concept like matter and\nenergy. Therefore, all the processing of information (such as computing) is subject to the basic laws\nof physics, and a key concept in information theory is entropy. Thermodynamic entropy, which is\nwell known to physicists, is homologous to the information entropy that Shannon uses to measure\ninformation. The physical metric that describes the number of system states is the entropy. For\nexample, Brownian motion is the irregular motion exhibited by tiny particles, and the higher the\ntemperature, the more violent the irregular motion of the molecules. From the microscopic point of\nview, Brownian motion is disorganized, but from a macroscopic point of view, it is a macroscopic\nlaw with irreversible (increased entropy). Therefore, the entropy points out the direction of the\nevolution of the system and describes the conditions under which the system is in equilibrium. In\ndeep learning, a large amount of data is needed to train the discovery system model. Different\nmodels have different entropy, the number of all microscopic states corresponding to the correct\nmodel is extremely large (namely, the entropy is maximum). The cost function of correlated entropy\nin deep learning applications is deﬁned based on the entropy of the physical principle.\nThe deﬁnition of entropy in physics is: H = k ln Ω, where Ωis the number of microscopic state.\nEntropy is additive, the entropy of system is the sum of the entropies of each subsystem, and entropy\nis a homogeneous function. Let X be a discrete random variable whose probability distribution is\nP(X = xk) = pk, k = 1, 2, 3, · · · , n , the probability of occurrence of xk is pk , then the number\nof microscopic state is 1/pk , so the cumulative sum of all the entropy of the random variable X is:\nH =\nX\npi ln 1/pi =\nX\npi ln pi\n(24)\nThe probability distribution is P(x) , and the random variable is the number of microscopic states\nof X, i.e. the entropy is:\nH(X) = −\nN\nX\nk=1\nP(X = k) log P(X = k)\n(25)\n17\nCross entropy is an extended concept of entropy, it introduces a second probability distribution.\nThen the number of microstates (entropy) for a random variable X is:\nH(p(x), q(x)) = −\nX\nx∈X\np(x) logq(x)\n(26)\nCross entropy measures the number of microstates of a random variable X under these two proba-\nbility distributions using physical quantity entropy. Cross entropy contains the difference between\ntwo probability distributions, and when the two distributions are the same, the entropy is Equation\n25. It is used in deep learning applications to measure the degree to which the model distribution\napproximates the unknown distribution.\nTherefore, entropy plays an important role in deep learning, it has become the objective function of\nthe selection and adjustment of the parameters of the deep learning model:\n• How the neural network adjusts parameters, in which direction the parameters are adjusted,\nthe basis is the entropy of the random variables.\n• How to determine the conditions for the physical balance and stability of the neural net-\nwork, the basis is the entropy of the system. For example, the entropy of an isolated equi-\nlibrium system must be maximum.\n• Entropy allows us to precisely deﬁne ” correlation ” and extract features from massive data.\n3.2.2\nApplication of entropy in cost function\nAccording to statistical physics, a macroscopic state with a steady state and balance is a state where\nthe number of microscopic states is maximum (i.e. the entropy is maximum). This is the physical\nbasis for the selection of the cost function.\nThe deep learning neural network discussed above are divided into two parts. The former is the neu-\nral networks that conforms to the laws of physics, such as each convolution layers of CNN, not only\nthe characteristics of the trained convolution layer have physical signiﬁcance, each of the conditions\nin the training also has physical signiﬁcance. Because it is calculated according to the physical\nequations, except that this microscopic state does not correspond to the ﬁnal trained macroscopic\nstate, or the number of microscopic states that the ﬁnal trained macroscopic state has is very small\n(entropy is small). The latter part is the classiﬁcation of neural networks, such as a convolution\nlayer that then maps human symbols (labels) to their physical characteristics. This is a distribution\nof two completely different concepts, so the cross-entropy of the model’s output (physical) and the\ntraining target (labeled) should be used as a cost function to approach the real physical model. The\nmagic of the wonderful combination of classiﬁcation problems and cross-entropy is that even if the\ncross-entropy on the test set is over-ﬁtted, the classiﬁcation error will not be overﬁt [21].\nFor the regression problem, it is the same kind of problem, so the mean square error is often used\nas the objective function or the cost function, which is equivalent to the general entropy maximum.\nKL divergence is closely related to cross entropy, but it does not include the entropy of the model,\nit reﬂects the difference between the two distributions. Unlike the use of cross entropy, the KL\ndivergence is used when there is no more physical distribution. These analyses show the powerful\npower of statistical physics in understanding deep learning.\n3.3\nUnderstanding deep learning from the concept of master equations\nThe evolution equation of the Markov process probability distribution is the master equation, which\nis one of the most important equations in statistical physics because it is almost universally applica-\nble. The Markov process is a process in which most of the memory effects can be omitted during\nthe evolution process. When the system evolution of a random variable occurs, the transfer process\nwill occur between different values of the random variables, by transferring the probability of the\nsystem changes in a given state until the system reaches a ﬁnal equilibrium state.\nIn deep learning, one of the simplest examples of the Markov process is the Markov chain. The\nMarkov chain is deﬁned by a random state Xt and a transition distribution T(Xt+1|Xt) . The\ntransition distribution T is a probability distribution showing the probability of a random transfer to\n18\nXt+1 in the case of a given state Xt . Running a Markov chain is the process of constantly updating\nthe state Xt based on the value Xt+1 of the transition distribution T. Among them, the probability\ndistribution of the system state at time t + 1 is only related to the state at time t, and has nothing to\ndo with the state before t.\n3.4\nUnderstanding deep learning from the concept of renormalization group\nDavid Schwab and Pankaj Mehta found that the Deep Belief Network (DBN) invented by Hinton\nresembled the renormalization in physics in a speciﬁc case, which means that the details of the phys-\nical system are obtained in a coarse-graining manner to calculate its overall state. When Schwab and\nMehta applied DBN to a magnetic model at the critical point (when the system is fractal and self-\nsimilar at any scale), they found that the network automatically uses a reorganization-like process\nto discover the state of the model. This discovery is shocking, as the biophysicist Ilya Nemenman\ncommented, it shows that extracting related features in the context of statistical physics and extract-\ning related features in the context of deep learning are not just similar, but they are completely the\nsame.” [22].\nThe renormalization group is a mathematical tool for examining changes in the physical system\nat different length scales. The details of the physical system are physically obtained in a coarse-\ngrained manner to calculate its overall state. The important feature of this method is that it is\nindependent of the system type. Each key step in the renormalization group method is based on the\nmain characteristics of the system, rather than putting the system into the framework we are familiar\nwith, and then adjusting the parameters [23]. The standard renormalization process in statistical\nphysics is equivalent to the feature extraction process of supervised learning in depth learning. The\ninformation is transmitted layer by layer and eventually converges to the theoretical boundary (ﬁxed\npoints). The purpose of the renormalization group method is to obtain new features, and to ensure\nthat the Hamiltonian function forms are unchanged under the new scale of renormalization.\nFigure 9: The pooling operation in convolution neural network.\nFor example, the pooling operation (pooling layer) in CNN is based on the main characteristics of\nthe system and uses the max pooling method to integrate the feature points in the small neighborhood\naccording to self-similarity to obtain new features. As shown in Figure 9, the pooling layer uses the\nmax pooling (where the size of the ﬁltering core is 2*2 and the step size is 2) to fuse a feature with\nan input size of 4*4 and retains only the largest feature point in the area, then the characteristic size\nafter the pooling operation is 2*2, and the pooling layer plays a role of dimensionality reduction.\nIt has been proved that there is a one-to-one mapping relationship between RBM-based deep neural\nnetworks and variational renormalization groups. The paper [7] illustrates the mapping relation-\nship by analyzing the DNN and numerical two-dimensional Ising model of a one-dimensional Ising\nmodel, and it ﬁnds that these DNNs self-realize a coarse graining process, i.e. Kadanoff block renor-\nmalization. The results show that deep learning may adopt a generalized renormalization group class\n19\nscheme to learn relevant features from the data. The paper proved that deep learning is closely related\nto the renormalization group, one of the most important and successful technologies in theoretical\nphysics.\n4\nA physical world view of deep learning\n4.1\nThe interpretability of deep learning\nThe interpretability of deep learning models can be divided into the following categories:\n(1) Feature attribution VS Internal logic:\nThe former maps the behavior of the model back to the original set of input features (or\nartiﬁcially creates optional input features). In the complex decision-making process of the\nmodel, the larger the inﬂuence of the characteristics will be assigned to the larger weight,\nthe structure of human knowledge plays a decisive role in this model; The latter argues\nthat: In the process of obtaining the ﬁnal answer of the model, it is the abstract role of\nthe physical meaning of the model itself and the internal working logic, rather than human\nstructural knowledge. Obviously, the interpretation of deep learning in this dissertation\nbelongs to the latter. This paper analyzes deep learning from the internal logic according\nto the principle of physics, while most of the papers use the former method to explain deep\nlearning.\n(2) Simulation acquires knowledge VS Introspection acquires knowledge:\nKnowledge based on simulation means that we obtain an understanding of our own model\nby generating some form of simulation data, capture how the model represents these data\npoints for understanding; Introspection acquires knowledge comes from the ﬁxed orienta-\ntion of the model and use them to gain knowledge without having to simulate the former.\nObviously, the interpretation of deep learning in this dissertation belongs to the latter, while\nmost papers belong to the former, the focus of their interpretation is to visualize the char-\nacteristics of deep learning. However, this dissertation holds that the deep neural network\ndata has high dimensional data characteristics, and human beings cannot understand the\nvisual characteristics of high-dimensional data.\nAccording to quantum statistical physics, a system with s classical degrees of freedom, the dynamic\nstate of the system is determined by the wave function:\nψσ1σ1,···(q1, q2, · · · qs, t)\nWhere q is the classical coordinate and σ is the non-classical degree of freedom. People think that\ndeep learning is incomprehensible. It is precisely because it uses the ψ to characterize the macro-\nscopic nature, which cannot be understood through visualization, nor can it be understood through\nmathematics. Deep learning is the interpretation of observed data using the dynamic characteristics\nof microscopic layers that are not observed by humans or that are not intuitively understood.\n4.2\nLocality\nOne of the deepest principles of physics is locality, that is, things directly affect their surroundings.\nLocality is a relative concept, usually referring to the scope and degree of inﬂuence of a physical\nquantity. Locality has two effects:\n(1) Short-range effect:\nThe interaction is only a nearby function, ignoring the effect of the remote, that is, the\neffect of the remote is averaged or canceled. For example, it has been explained before\nthat during the operation of the Markov chain, the system state at the current moment is\nonly related to the state at the previous moment, has nothing to do with the state before the\nprevious moment, that is, the short-range effect.\n(2) Local coupling:\nThe local roles of each regions are coupled and then coupled as localization. Scale changes,\nrenormalization, long procedures, strong correlation, and coarse-graining. For example,\n20\npooling in CNN has already been described in the previous section. By using max pool-\ning, similar features of neighboring regions are merged (that is, processes that are locally\ncoupled), thereby achieving the effect of dimensionality reduction.\n4.3\nSymmetry\nSymmetry and conservation law are the basic laws of nature. Symmetry can not only reduce the\nnumber of parameters, but also reduce the computational complexity. See the convolutional neu-\nral network under the symmetry (translation invariance) we introduced in Section 2.2. Whenever\nHamiltonian obeys a certain symmetry, that is, invariable under some transformation, the number\nof independent parameters required to describe it decreases further. The most important mechanical\nquantities of atoms are momentum and angular momentum, so the corresponding translation and ro-\ntation transformations are the most important and basic operations. For example, many probability\ndistributions are constant in the case of translation and rotation.\nIf the system has translational symmetry, then the state after the translation operation differs from\nthe original state by a maximum of one phase factor, that is, the difference between the input state\nand the output state mainly appears as a phase shift. The input wave function can be labeled by the\nwave vector, and the physical meaning of the wave vector is momentum, which reﬂects the spatial\nsymmetry of the system. The state with translational symmetry is the eigenstate of the momentum\noperator, and the momentum represented by the wave vector is the corresponding eigenvalue. There-\nfore, the process of learning the eigenvalue and the eigenvalue is the process of obtaining the wave\nvector.\n4.4\nConjugacy and Duality\nConjugacy is the amount of pairing. For example, in physics: pressure and volume, temperature\nand entropy, intensity and extension are conjugate quantities, in mathematics: real and imaginary\nnumbers, transposed conjugates of matrix, and so on. Duality is the correspondence between the\ndifferent physical theories that lead to the same physical results, such as, there is a reaction force\nwith acting force, there are holes with electrons. These are the important organizations of physics,\nand they are related to each other and to each other in physical relations. This worldview is also\nused in deep learning. Here are two examples:\n(1) Stochastic gradient descent algorithm using momentum\nAlthough random gradient descent is a very popular optimization method in the training process, the\nlearning process is sometimes very slow, and even cannot found the best point. The basic physical\nidea to solve this problem is to introduce conjugates and use duality to solve this problem.\nThe cost function has a gradient g, there is ﬂow (or velocity or momentum), so the introduction of\nthe momentum variable v. The gradient of the cost function is seen as a force that pushes the cost\nfunction to accelerate downhill and the momentum increases; if there is only this kind of force, the\noptimization process can never stop, so we need to add another force or gradient (called the viscous\nresistance) to make the cost function converge to a minimum; but the minimum may not be the\nsmallest, so wo need to increase a momentum, let him out of the local minimum, so as to achieve\nthe overall optimal. Update the algorithm as follows:\nv ←αν −εg\nθ ←θ + ν\nAs shown in Figure 10, the contour depicts a quadratic loss function (Hessian matrix with ill-\nconditioned conditions). The red path across the proﬁle represents the path followed by the mo-\nmentum learning rule, which minimizes the function. We draw an arrow at each step of the path,\nindicating the step the gradient will take at that point. We can see that the quadratic objective func-\ntion of a pathological condition looks like a long and narrow valley or a canyon with a steep edge.\nThe momentum passes right through the gorge, while ordinary gradient steps waste time moving\naround the narrow axis of the canyon [12].\n21\nFigure 10: Stochastic gradient descent algorithm using momentum. Figure courtesy: [12]\n(2) Annealing and tempering\nTemperature and entropy are conjugate quantities, when the temperature equals 0, the entropy equals\n0(minimum); the temperature inﬁnity is the most chaotic, the model becomes uniform distribution.\nUsing the temperature in the Boltzmann distribution, tempering: The Markov chain temporarily\nsamples from a high temperature distribution and returns to sampling at unit temperature. Annealing:\nThe Markov chain temporarily samples from a low temperature distribution and returns to sampling\nat unit temperature to ﬁnd the best among the different peaks. However, it is necessary to pay\nattention to the existence of a critical temperature.\nMost of the models used for prediction in deep learning generally use the softmax activation function\nto assign probability distributions to tags. The reference [24] mentions that, in the image classiﬁ-\ncation problem, we divide the pictures into cats, dogs, and tigers three kinds. In a training, the\nprobability of the three types is [0.0010, 0.0001, 0.9989], and the one-hot code of [0, 0, 1] is ob-\ntained as the classiﬁcation result (hard-target). However, the intrinsic link between cats and tigers\ncan easily be overwhelmed during training. This is undoubtedly a waste of valuable a priori prob-\nabilities that can be used to transfer large-scale network knowledge into small-scale networks. In\norder to make full use of the correlation between such class categories, we need to change the prob-\nability distribution in some way to make it more smooth. In the reference, Hinton further modiﬁed\nthe softmax function:\nqi =\nexp(zi/T)\nP\nj exp(zj/T)\nWhen the temperature T = 1 in the equation, it degenerates into the traditional softmax function;\nwhen T is inﬁnite, the result approaches 1/C, that is, the probabilities on all classes approach to\nequal; when T > 1, we can obtain the soft target label. By increasing the temperature T, the\nmapping curve of the softmax layer will be smoother, so the probability mapping of the instances\nwill be more concentrate and the goal will be ”soft”. Therefore, in order to make full use of the\ncorrelation between such class categories, the method of changing the probability distribution is to\nincrease T.\n4.5\nHierarchy\nOne of the most striking features of the physical world is its hierarchy. Spatially, it is an object\nhierarchy: elementary particles form atoms, then form molecules, cells, organisms, planets, the solar\nsystem, galaxies, and so on. Complex structures are usually layered and created through a series of\nsimple steps. The observables of world are inherently hierarchical and cannot be commutative.\nThe main reason for the success of deep learning is the hierarchical nature of neural networks (deep\nneural networks). In order to extract uncomplicated features, a multi-layer neural network is required\nto stack simple networks and then effectively implement the generation process through layering and\ncombination. Multilayer networks provide layer-by-layer abstract channels from low to high levels.\n22\n4.6\nModel Reusability\nAs a physical model, especially the convolution neural network, the deep neural network can be used\nas a universal method because of its physics, so it is reusable in any ﬁeld. The important basis for\njudging whether a model can be reused or reused is its physical properties. The following analyzes\nthe problem of model reusability of convolutional neural networks.\n(1) The convolutional network used for image classiﬁcation consists of two parts: they start\nwith a series of pooling and convolution layers and end up with densely connected clas-\nsiﬁers. The ﬁrst part is called the ”convolution base” of the model, it learns completely\nfrom the physical characteristics, so its model can be used in various ﬁelds, but the dense\nconnection layer behind is the mapping between annotation and physics, and its model is\nnot reusable. So in practice, ”feature extraction” simply use the convolved basis of the\npreviously trained network, run new data through it, and train a new classiﬁer over the\noutput.\n(2) There are two conditions for the reusable base to be reused. First, the input and output are\nthe probability waves in the physical sense. Second, it internally measures the wave vector\nof the input physical quantity. According to the uncertainty principle of quantum mechan-\nics, the coordinate and wave vector cannot be measured at the same time. Therefore, its\noutput does not contain the input position information, it is impossible to obtain accurate\nposition information of the original input image. And the position is very important for the\nobject position, the dense connection is more completely useless.\n(3) Feature characterization extracted using multi-layer architecture will evolve from simple\nand local to abstract and global in structure, but because of the process, the further away\nfrom the input layer the convolutional base is, the lower the reusability is. On the one hand,\nbecause in the continuous pooling, the speciﬁcity of features is getting stronger, and more\nand more physical information is lost. On the other hand, equation 8 is calculated using the\ninput as the wave function, the error is getting bigger and bigger, it deviates more and more\nfrom the real physical model, so the reusability is getting lower and lower. So ﬁne-tuning is\nanother widely used model reuse technique. Fine-tuning freezes convolutional groups near\nthe input layer without training, thaws convolutions away from the input layer, and train\nwith the new classiﬁer to make them more relevant to the problem at hand.\n4.7\nModel vulnerability\nAlthough deep learning has achieved great success in many applications, there are still many limi-\ntations: for example, it needs a lot of data, the vulnerability of the algorithm etc. Why is the neural\nnetwork easily disturbed by the input of small disturbances? From the physical model of deep neural\nnetwork in this paper, we can see that on the one hand this is a physical problem, behind which we\nhave not found new physical phenomena or new physical modes; On the other hand, labeling does\nnot match the actual physical characteristics, such as cross-entropy, the two distributions cannot be\napproximated, or they are approached in a group of training data, it does not mean that the test data\ncan match nonexistent labels, especially the deception problem. Therefore, the key to the vulnera-\nbility of the model is the limitation of human understanding. According to the physical world view\nof solving problems, providing anti-classiﬁcation is a direction worthy of study.\n4.8\nCausality and Correlation\nAt present, deep learning pays attention to relevance instead of causality, and uses joint probability\ndistribution to replace traditional theorems and laws. The theoretical foundation of deep learning\nmethods lies in the representation and transformation of statistical probability distributions. It is\nconsistent with the physical model of the deep neural network in this paper. That is, the state of the\nmicroscopic particle is completely described by the wave function ψ . After the wave function is\ndetermined, all the characteristics of the system can be obtained, the average value of any mechanical\nquantity and its measurement possible value and the corresponding probability distribution are also\ncompletely determined.\nHowever, in practice, a mathematical model of causality derived from statistical data will be com-\nprehensively used. For example, these models can be used to establish a causal relationship between\n23\nsmoking and cancer, or to analyze the risks of a construction project, and so on. Can these mathe-\nmatical models be extended to the microscopic world dominated by quantum mechanics? Can it be\nincorporated into the deep learning quantum physics model? Since quantum mechanics itself has\nmany strange features, for example, if two or more quantum systems are entangled with each other,\nit is difﬁcult to deduce whether the statistical correlation between them is causal.\nThe concept of causal information actually exceeds statistical relevance. For example, we can com-\npare these two sentences: ”The number of cars is related to the amount of air pollution” and ”The\ncar causes air pollution.” The ﬁrst sentence is a statistical statement, and the latter sentence is a\ncausal statement. Causality differs from relevance because it tells us how the system will change\nunder intervention. In statistics, causal models can be used to extract causality from the empirical\ndata of complex systems. However, there is only one component in a system of quantum physicsłthe\nwave function ψ , so mathematical models that use causality derived from statistical data cannot be\napplied, including Bayesian inference. John-Mark Allen of Oxford University in the United King-\ndom proposed a generalized quantum causal model based on Reichenbach’s principle of common\nreason [25], successfully combining causal intervention and Bayesian inference into a model.\n5\nConclusions\nAt present, the research on the internal theory of deep learning is very scarce, and the successful\napplication of deep learning and the limitations of its existing technology further illustrate the im-\nportance of studying its internal technical mechanism from a scientiﬁc perspective. Only knowing\nwhy to do it can transform existing methods or means from a deeper level. This is the scientiﬁc way\nof thinking. Based on the principles of physics, this paper interprets the deep learning techniques\nfrom three different perspectives: microscopic, macroscopic, and physical world perspectives. In-\nspired by the biological neural network, a new neuron physics model was proposed. Based on this,\nit explains the success of deep learning well, and fully reveals the internal mechanism of deep learn-\ning by scientiﬁc methods. A good theory can not only explain existing experiments, but also predict\nnew phenomena and technologies. Therefore, this dissertation also proposes the direction of further\nresearch in deep learning. Some of the main conclusions of this paper are as follows:\n(1) The deep neural network is a physical system, and its architecture and algorithm should\nconform to the principles of physics. The technical foundation for deep learning is physics,\nespecially quantum physics and quantum statistical physics.\n(2) In this dissertation, the physical meaning of neurons in deep neural network is proposed:\nits output value is the distribution of quasi-particles.\n(3) Two physical models of deep neural networks are proposed in this paper, one is pure ensem-\nble deep neural network and the other is hybrid ensemble deep neural network. The former\nlearning model corresponds to a quantum measurement of a microscopic state, such as\nCNN; the latter corresponds to a microscopically statistically averaged macroscopic state,\nsuch as RBM.\n(4) The physical model of neurons in CNN is a quantum superposition of a quasi-particle\nincident wave (Figure 1) and is excited by the output. This excitation may be the elastic\nscattering caused by the incident wave (exit only includes the incident wave), or inelastic\nscattering (exit also includes internal new excited states), or various possible actions such\nas chemical reactions (exit only includes new quasiparticles). It obeys quantum mechanics.\nAccording to the superposition principle of quantum mechanics, the excitation output of a\nneuron is related not only to the intensity of incident quasi-particles in other neurons, but\nalso to their coherence, and to their polarization direction or spin.\n(5) The input of the deep learning network is treated as a wave function, and the image is also a\nwave. The state of the neuron is also expressed by a wave function. If the measured neuron\nis the number or probability of excited quasi-particles, the value of a layer of neurons\nin the neural network is a probability distribution. The deep learning operator should be\nperformed on the complex number ﬁeld, but because the activation function is ReLU, the\ncomputational difference in the real number domain may not be large.\n(6) Under such a physical model, the convolutional neural network algorithm is exactly the\nsame as the quantum calculation method for measuring the number of quasi-particles ex-\n24\ncited by neurons, so that it can perfectly explain the technology of each important com-\nponents of a convolutional neural network algorithm (convolution, rectiﬁcation, activation,\npooling, etc.) The purpose of convolution is to measure the number or probability of quasi-\nparticles excited by each neuron. The convolution kernel is related to the Hamiltonian\ninteraction potential of neurons. All neurons present an interference diffraction pattern -\nstripes and patches. That is, the deep convolutional neural network can measure the in-\nput wave vector or momentum, and its computational model can decompose white light\ninto monochromatic light and decompose random-direction vibration into single-direction\npolarization. Therefore, the physical model of this paper can explain deep learning tech-\nnology and success. This physical model shows that the deep convolutional neural network\nhas natural learning ability and cognitive ability, and the model learns the ability to char-\nacterize the input micromechanical quantities, so it is reusable and can be applied across\nﬁelds.\n(7) The basis for parameter adjustment and optimization in the deep learning classiﬁcation\ntraining process is the entropy in statistical physics, which is the number of microscopic\nstates corresponding to the corresponding macroscopic state. Different types of training\nmodels should choose different cost functions according to the meaning of entropy, for\nexample, the convolutional neural network model should use cross entropy as the objective\nfunction (cost function).\n(8) A large number of operators, techniques, and methods in deep learning are related to the\nprinciples of physics such as energy, entropy, renormalization techniques, and translation\noperations; they are also related to physical world views such as symmetry, conjugacy,\nlocality, hierarchy, etc.\nThe research in this paper shows that there are physics glimmers everywhere in deep learning. Deep\nlearning techniques can be based on scientiﬁc theories. From the principles of physics, this dis-\nsertation presents the calculation methods for convolution calculation, pooling, normalization, and\nRBM, as well as the selection of cost functions, explains why deep learning must be deep, what\ncharacteristics are learned in deep learning, why convolutional neural networks do not have to be\ntrained layer by layer, and the limitations of deep learning, etc. The physical model proposed in\nthis paper can not only explain the successful technology of existing deep learning, but also predict\nmany researchable directions and topics, such as positional neurons (these are in the research stage,\nand still need to be experimentally veriﬁed).\nThere is a striking homogeneity in the appearance and structure of the human cerebral cortex. In\nother words, the cerebral cortex uses the same calculations to accomplish all its functions. All the\nintelligence that humans exhibit (vision, auditory, physical movement...) are based on a uniﬁed set\nof algorithms. The deep learning technology is also based on a uniﬁed algorithm and is supported\nby physical theories. It will have a broad prospect for development.\nReferences\n[1] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation from predict-\ning 10,000 classes. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1891–1898, 2014.\n[2] David Imseng, Petr Motlicek, Philip N. Garner, and Herve Bourlard. Impact of deep mlp\narchitecture on different acoustic modeling techniques for under-resourced speech recognition.\nIn Automatic Speech Recognition and Understanding, pages 332–337, 2013.\n[3] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. Recent trends in deep\nlearning based natural language processing. arXiv preprint arXiv:1708.02709, 2017.\n[4] Anuj Karpatne, William Watkins, Jordan Read, and Vipin Kumar. Physics-guided neural net-\nworks (pgnn): An application in lake temperature modeling. arXiv preprint arXiv:1710.11431,\n2017.\n[5] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436,\n2015.\n[6] Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so\nwell? Journal of Statistical Physics, 168(6):1223–1247, 2017.\n25\n[7] Pankaj Mehta and David J Schwab. An exact mapping between the variational renormalization\ngroup and deep learning. arXiv preprint arXiv:1410.3831, 2014.\n[8] Xun Gao and Lu-Ming Duan. Efﬁcient representation of quantum many-body states with deep\nneural networks. Nature communications, 8(1):662, 2017.\n[9] Yoav Levine, Or Sharir, Nadav Cohen, and Amnon Shashua. Bridging many-body quantum\nphysics and deep learning via tensor networks. arXiv preprint arXiv:1803.09780, 2018.\n[10] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artiﬁcial\nneural networks. Science, 355(6325):602–606, 2017.\n[11] Ajit Narayanan and Tammy Menneer. Quantum artiﬁcial neural network architectures and\ncomponents. Information Sciences, 128(3-4):231–255, 2000.\n[12] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, vol-\nume 1. MIT press Cambridge, 2016.\n[13] Isma Hadji and Richard P Wildes. What do we understand about convolutional networks?\narXiv preprint arXiv:1803.08834, 2018.\n[14] Ramamurti Shankar. Principles of quantum mechanics. Springer Science & Business Media,\n2012.\n[15] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and\nLe Song. Decoupled networks. arXiv preprint arXiv:1804.08071, 2018.\n[16] Avraham Ruderman, Neil Rabinowitz, Ari S Morcos, and Daniel Zoran. Learned deformation\nstability in convolutional neural networks. arXiv preprint arXiv:1804.04438, 2018.\n[17] Francois Chollet. Deep learning with Python. Manning Publications Co., 2017.\n[18] Jing Chen, Song Cheng, Haidong Xie, Lei Wang, and Tao Xiang. Equivalence of restricted\nboltzmann machines and tensor network states. Physical Review B, 97(8):085104, 2018.\n[19] Song Cheng, Jing Chen, and Lei Wang. Quantum entanglement: from quantum states of matter\nto deep learning. Physics, 2017.\n[20] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R⃝in Machine\nLearning, 2(1):1–127, 2009.\n[21] T Poggio, K Kawaguchi, Q Liao, B Miranda, L Rosasco, X Boix, J Hidary, and HN Mhaskar.\nTheory of deep learning iii: the non-overﬁtting puzzle. Technical report, CBMM memo 073,\n2018.\n[22] Shuo-Hui Li and Lei Wang.\nNeural network renormalization group.\narXiv preprint\narXiv:1802.02840, 2018.\n[23] Maciej Koch-Janusz and Zohar Ringel. Mutual information, neural networks and the renor-\nmalization group. Nature Physics, page 1, 2018.\n[24] Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Ge-\noffrey E Hinton. Large scale distributed neural network training through online distillation.\narXiv preprint arXiv:1804.03235, 2018.\n[25] John-Mark A Allen, Jonathan Barrett, Dominic C Horsman, Ciar´an M Lee, and Robert W\nSpekkens.\nQuantum common causes and quantum causal models.\nPhysical Review X,\n7(3):031021, 2017.\n[26] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum\nphysics: A fundamental bridge. arXiv preprint arXiv:1704.01552, 2017.\n[27] W Kinzel. Physics of neural networks. Europhysics News, 21(6):108–110, 1990.\n[28] Gary Marcus. Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631, 2018.\n[29] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quan-\ntum entanglement: Fundamental connections with implications to network design. CoRR,\nabs/1704.01552, 2017.\n[30] Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R⃝in Machine\nLearning, 2(1):1–127, 2009.\n[31] IE Lagaris, A Likas, and DI Fotiadis. Artiﬁcial neural network methods in quantum mechanics.\nComputer Physics Communications, 104(1-3):1–14, 1997.\n26\n[32] Alaa Sagheer and Mohammed Zidan. Autonomous quantum perceptron neural network. arXiv\npreprint arXiv:1312.4149, 2013.\n[33] Max Tegmark. Why the brain is probably not a quantum computer. Information Sciences,\n128(3-4):155–179, 2000.\n[34] G Perry, ET Rolls, and SM Stringer. Continuous transformation learning of translation invari-\nant representations. Experimental brain research, 204(2):255–270, 2010.\n[35] Giuseppe Carleo, Matthias Troyer, Giacomo Torlai, Roger Melko, Juan Carrasquilla, and\nGuglielmo Mazzola. Neural-network quantum states. Bulletin of the American Physical Soci-\nety, 2018.\n[36] Rongxin Xia and Sabre Kais. Quantum machine learning for electronic structure calculations.\narXiv preprint arXiv:1803.10296, 2018.\n[37] Rene Vidal, Joan Bruna, Raja Giryes, and Stefano Soatto. Mathematics of deep learning. arXiv\npreprint arXiv:1712.04741, 2017.\n[38] IE Lagaris, A Likas, and DI Fotiadis. Artiﬁcial neural network methods in quantum mechanics.\nComputer Physics Communications, 104(1-3):1–14, 1997.\n[39] Giacomo Torlai, Guglielmo Mazzola, Juan Carrasquilla, Matthias Troyer, Roger Melko, and\nGiuseppe Carleo. Neural-network quantum state tomography. Nature Physics, page 1, 2018.\n[40] MV Altaisky. Quantum neural network. arXiv preprint quant-ph/0107012, 2001.\n[41] Judea Pearl. Theoretical impediments to machine learning with seven sparks from the causal\nrevolution. arXiv preprint arXiv:1801.04016, 2018.\n[42] Daniel J Buehrer. A mathematical framework for superintelligent machines. arXiv preprint\narXiv:1804.03301, 2018.\n27\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-05-22",
  "updated": "2018-05-22"
}