{
  "id": "http://arxiv.org/abs/1806.04640v3",
  "title": "Unsupervised Meta-Learning for Reinforcement Learning",
  "authors": [
    "Abhishek Gupta",
    "Benjamin Eysenbach",
    "Chelsea Finn",
    "Sergey Levine"
  ],
  "abstract": "Meta-learning algorithms use past experience to learn to quickly solve new\ntasks. In the context of reinforcement learning, meta-learning algorithms\nacquire reinforcement learning procedures to solve new problems more\nefficiently by utilizing experience from prior tasks. The performance of\nmeta-learning algorithms depends on the tasks available for meta-training: in\nthe same way that supervised learning generalizes best to test points drawn\nfrom the same distribution as the training points, meta-learning methods\ngeneralize best to tasks from the same distribution as the meta-training tasks.\nIn effect, meta-reinforcement learning offloads the design burden from\nalgorithm design to task design. If we can automate the process of task design\nas well, we can devise a meta-learning algorithm that is truly automated. In\nthis work, we take a step in this direction, proposing a family of unsupervised\nmeta-learning algorithms for reinforcement learning. We motivate and describe a\ngeneral recipe for unsupervised meta-reinforcement learning, and present an\ninstantiation of this approach. Our conceptual and theoretical contributions\nconsist of formulating the unsupervised meta-reinforcement learning problem and\ndescribing how task proposals based on mutual information can be used to train\noptimal meta-learners. Our experimental results indicate that unsupervised\nmeta-reinforcement learning effectively acquires accelerated reinforcement\nlearning procedures without the need for manual task design and these\nprocedures exceed the performance of learning from scratch.",
  "text": "Unsupervised Meta-Learning for Reinforcement Learning\nAbhishek Gupta * 1 Benjamin Eysenbach * 2 Chelsea Finn 3 Sergey Levine 1\nAbstract\nMeta reinforcement learning (meta-RL) algo-\nrithms leverage experience from learning previ-\nous tasks to learn how to learn new tasks quickly.\nHowever, this process requires a large number\nof meta-training tasks to be provided for meta-\nlearning. In effect, meta-RL shifts the human bur-\nden from algorithm to task design. In this work\nwe automate the process of task design, devising\na meta-learning algorithm that does not require\nmanual design of meta-training tasks. We propose\na family of unsupervised meta-RL algorithms\nbased on the insight that task proposals based on\nmutual information can be used to train optimal\nmeta learners. Experimentally, our unsupervised\nmeta-RL algorithm, which does not require man-\nual task design, substantially improves on learn-\ning from scratch, and is competitive with super-\nvised meta-RL approaches on benchmark tasks.\n1. Introduction\nReusing past experience for faster learning of new tasks is a\nkey challenge for machine learning. Meta-learning methods\nachieve this by using past experience to explicitly optimize\nfor rapid adaptation (Mishra et al., 2017; Snell et al., 2017;\nSchmidhuber, 1987; Finn et al., 2017a; Gupta et al., 2018;\nWang et al., 2016; Al-Shedivat et al., 2017). In the context of\nreinforcement learning (RL), meta-reinforcement learning\n(meta-RL) algorithms can learn to solve new RL tasks more\nquickly through experience on past tasks (Duan et al., 2016b;\nGupta et al., 2018; Finn et al., 2017a). Typical meta-RL\nalgorithms assume the ability to sample from a pre-speciﬁed\ntask distribution, and these algorithms learn to solve new\ntasks drawn from this distribution very quickly. However,\nspecifying a task distribution is tedious and requires a sig-\nniﬁcant amount of supervision (Finn et al., 2017b; Duan\net al., 2016b) that may be difﬁcult to provide for large, real-\nworld problem settings. The performance of meta-learning\nalgorithms critically depends on the meta-training task dis-\ntribution, and meta-learning algorithms generalize best to\n*Equal contribution\n1UC Berkeley 2Carnegie Mellon Univer-\nsity 3Stanford University. Correspondence to: Abhishek Gupta\n<abhigupta@eecs.berkeley.edu>.\nnew tasks which are drawn from the same distribution as\nthe meta-training tasks (Finn & Levine, 2018). In effect,\nmeta-RL ofﬂoads the design burden from algorithm design\nto task design. While meta-RL acquires representations for\nfast adaptation to the speciﬁed task distribution, specifying\nthis task distribution is often tedious and challenging. Can\nwe automate the process of task design, thereby doing away\nwith human supervision entirely?\nIn this paper, we take a step towards unsupervised meta-\nRL: meta-learning from a task distribution that is acquired\nautomatically, rather than requiring manual design of the\nmeta-training tasks. While unsupervised meta-RL does not\nmake any assumptions about the reward functions on which\nit will be evaluated at test time, it does assume that the\nenvironment dynamics remain the same. This allows an\nunsupervised meta-RL agent to utilize environment interac-\ntions to meta-train a model that is optimized to be effective\nfor learning from previously unseen reward functions in\nthat environment at meta-test time. Our method can also\nbe thought of as automatically acquiring an environment-\nspeciﬁc learning procedure for deep neural network policies,\nsomewhat related to data-driven initialization procedures\nexplored in supervised learning (Kr¨ahenb¨uhl et al., 2015;\nHsu et al., 2018).\nThe primary contribution of our work is a framework for un-\nsupervised meta-RL. We describe a family of unsupervised\nmeta-RL algorithms and provide analysis to show that unsu-\npervised meta-RL methods based on mutual information can\nbe optimal, in a minimax sense. Our experiments shows that,\nfor a variety of robotic control tasks, unsupervised meta-RL\ncan effectively acquire RL procedures. These procedures\nnot only learn faster than standard RL approaches that learn\nfrom scratch, but also outperform prior methods that do pure\nexploration and then ﬁne-tuning at test time. Our results\neven approach the performance of an oracle method that\nrelies on hand-designed task distributions.\n2. Related Work\nOur work lies at the intersection of meta-RL, goal gen-\neration, and unsupervised exploration. Meta-learning al-\ngorithms use data from multiple tasks to learn how to\nlearn, acquiring rapid adaptation procedures from experi-\nence (Schmidhuber, 1987; Naik & Mammone, 1992; Thrun\n& Pratt, 1998; Bengio et al., 1992; Hochreiter et al., 2001;\narXiv:1806.04640v3  [cs.LG]  30 Apr 2020\nUnsupervised Meta-Learning for Reinforcement Learning\nenvironment\nUnsupervised Meta-RL\nMeta-learned \nenvironment-specific \nRL algorithm\nreward-maximizing \npolicy\nreward \nfunction\nUnsupervised \nTask Acquisition\nMeta-RL\nFast \nAdaptation\nFigure 1. Unsupervised meta-reinforcement learning: Given an\nenvironment, unsupervised meta-RL produces an environment-\nspeciﬁc learning algorithm that quickly acquire new policies that\nmaximize any task reward function.\nSantoro et al., 2016; Andrychowicz et al., 2016; Ravi &\nLarochelle, 2017; Finn et al., 2017a; Munkhdalai & Yu,\n2017). These approaches have been extended into the set-\nting of RL (Duan et al., 2016b; Wang et al., 2016; Finn\net al., 2017a; Sung et al., 2017; Gupta et al., 2018; Men-\ndonca et al., 2019; Houthooft et al., 2018; Stadie et al., 2018;\nRakelly et al., 2019; Nagabandi et al., 2018a). In practice,\nthe performance of meta-learning algorithms depends on\nthe user-speciﬁed meta-training task distribution. We aim to\nlift this limitation and provide a general recipe for avoiding\nmanual task engineering for meta-RL. A handful of prior\nmeta-learning methods have used self-proposed task distri-\nbutions for learning supervised learning procedures (Hsu\net al., 2018; Antoniou & Storkey, 2019; Lin et al., 2019; Ji\net al., 2019). In contrast, our work deals with the RL setting,\nwhere the environment dynamics provides a rich inductive\nbias that our meta-learner can exploit. In the RL setting,\ntask distributions can be obtained in a variety of ways, in-\ncluding adversarial goal generation (Sukhbaatar et al., 2017;\nHeld et al., 2017), information-theoretic methods (Gregor\net al., 2016; Eysenbach et al., 2018; Co-Reyes et al., 2018;\nAchiam et al., 2018). The most similar work is Jabri et al.\n(2019), which also considers the unsupervised application\nof meta-learning to RL tasks. We build upon this work by\nproving that an optimal meta-learner can be acquired using\nmutual information-based task proposal.\nExploration methods that seek out novel states are also\nclosely related to goal generation methods (Pathak et al.,\n2017; Schmidhuber, 2009; Bellemare et al., 2016; Osband\net al., 2016; Stadie et al., 2015), but do not by themselves\naim to generate new tasks or learn to adapt more quickly to\nnew tasks, only to achieve wide coverage of the state space.\nModel-based RL methods (Deisenroth & Rasmussen, 2011;\nChua et al., 2018; Srinivas et al., 2018; Nagabandi et al.,\n2018b; Finn & Levine, 2017b; Atkeson & Santamaria, 1997)\nuse unsupervised experience to learn a dynamics model but\ndo not learn how to efﬁciently use this model to explore to\nsolve new tasks.\nGoal-conditioned RL (Schaul et al., 2015; Andrychowicz\net al., 2017; Pong et al., 2018) is also related to our work,\nand our analysis will study this special case ﬁrst before\ngeneralizing to the general case of arbitrary tasks. As we\ndiscuss in Section 3.4, goal-reaching itself is not enough, as\ngoal-reaching agents are not optimized to efﬁciently explore\nto determine which goal they should reach, relying instead\non a hand-speciﬁed goal parameterization that doesn’t allow\nthese algorithms to work with arbitrary reward functions.\n3. Unsupervised Meta-RL\nWe consider the problem of learning a reinforcement learn-\ning algorithm that can quickly solve new tasks in a given\nenvironment. This meta-RL process could, for example,\ntune the hyperparameters of another RL algorithm, or could\nreplace the RL update rule itself with a learned update rule.\nUnlike prior work, we aim to do so without depending on\nany human supervision or information about the tasks that\nwill be provided for meta-testing. A task reward is provided\nat meta-test time, and the learned RL procedure should adapt\nto this task reward as quickly as possible. We assume that\nall test-time tasks have the same dynamics, and differ only\nin their reward functions. Our algorithm will therefore need\nto utilize unsupervised environment interaction to learn an\nRL algorithm. In effect, the dynamics themselves will be the\nsupervision for our learning algorithm.\nWe formalize the meta-training setting as a controlled\nMarkov process (CMP) – a Markov decision process with-\nout a reward function, C = (S, A, P, γ, ρ), with state space\nS, action space A, transition dynamics P, discount factor\nγ and initial state distribution ρ. The CMP, along with a\nreward function r, produces a Markov decision processes\nM = (S, A, P, γ, ρ, r). We deﬁne a learning algorithm\nf : D →π as a function that takes as input a dataset of\nexperience from the MDP, D = {(si, ai, ri, s′\ni)} ∼M,\nand outputs a policy π(a | s). Evaluation of the learning\nprocedure f is carried out over a handful of episodes. In\nepisode i, the learning procedure f observes all previous\ndata {τ1, · · · , τi−1} and outputs a policy to be used in itera-\ntion i. We evaluate the learning procedure f by summing\nits cumulative reward across iterations:\nR(f, rz) =\nX\ni\nEπ=f({τ1,··· ,τi−1})\nτ∼π\n\"X\nt\nrz(st, at)\n#\nOur aim is to take this CMP and produce an environment-\nspeciﬁc learning algorithm f that can quickly learn an op-\ntimal policy π∗\nr(a | s) for any reward function r. We refer\nto this problem as unsupervised meta-RL, and illustrate the\nproblem setting in Fig. 1.\nWe now sketch a recipe for unsupervised meta-RL, analyze\nwhen this recipe is optimal, and then instantiate a practical\napproximation to this theoretically-motivated approach by\nbuilding upon known meta-learning algorithms and unsu-\npervised exploration methods.\n3.1. A General Recipe\nTo construct an unsupervised meta-RL algorithm, we lever-\nage the insight that, to acquire a fast learning algorithm\nwithout task supervision, we can simply leverage standard\nmeta-learning techniques, but with unsupervised task pro-\nposal mechanisms. Our unsupervised meta-RL framework\nUnsupervised Meta-Learning for Reinforcement Learning\ntherefore consists of a task proposal mechanism and a meta-\nlearning method. For reasons that will become more ap-\nparent later, we will deﬁne the task distribution as a map-\nping from a latent variable z ∼p(z) to a reward function\nrz(s, a) : S × A →R1. That is, for each value of the ran-\ndom variable z, we have a different reward function rz(s, a).\nUnder this formulation, learning a task distribution amounts\nto optimizing a parametric form for the reward function\nrz(s, a) that maps each z ∼p(z) to a different reward func-\ntion. The choice of this parametric form represents an im-\nportant design decision for an unsupervised meta-learning\nmethod, and the resulting set of tasks is often referred to as\na task or goal proposal procedure. In the following section,\nwe will discuss a theoretical framework that allows us to\nmake this choice in the following section so as to minimize\nworst case regret of the subsequently meta-learned learning\nalgorithm f.\nThe second component is the meta-learning algorithm,\nwhich takes the family of reward functions induced by p(z)\nand rz(s, a), along with the associated CMP, and meta-\nlearns an RL algorithm f that can quickly adapt to any task\nfrom the task distribution deﬁned by p(z) and rz(s, a) in the\ngiven CMP. The meta-learned algorithm f can then learn\nnew tasks quickly at meta-test time, when a user-speciﬁed\nreward function is actually provided. Fig. 1 summarizes this\ngeneric design for an unsupervised meta-RL algorithm.\nThe “no free lunch theorem” (Wolpert et al., 1995; Whitley\n& Watson, 2005) might lead us to expect that a truly generic\napproach to proposing a task distribution would not yield\na learning procedure f that is effective on any real tasks.\nHowever, the assumption that the dynamics remain the same\nacross tasks affords us an inductive bias with which we pay\nfor our lunch. In the following sections, we will discuss how\nto formulate acquiring the optimal unsupervised learning\nprocedure, which minimizes regret on new meta-test tasks\nin the absence of any prior knowledge. Since our analysis\nwill focus on a restricted class of learning procedures, our\nresults are lower bounds for the performance of general\nlearning procedures. We ﬁrst deﬁne an optimal meta-learner\nand then show how we can train one without requiring task\ndistributions to be hand-speciﬁed.\n3.2. Optimal Meta-Learners\nWe begin our analysis by considering the optimal learning\nprocedure when the task distribution is known. For a task\ndistribution p(rz), the optimal learning procedure f ∗is\ngiven by\nf ∗≜arg max\nf\nEp(rz) [R(f, rz)] .\nOther learning procedures f may achieve lower reward, and\nwe deﬁne the regret incurred by using a suboptimal learning\n1In most cases p(z) is chosen to be a uniform categorical so it\nis not challenging to specify\nprocedure as the difference in expected reward, compared\nwith the optimal learning procedure:\nREGRET(f, p(rz)) ≜Ep(rz) [R(f ∗, rz)]−Ep(rz) [R(f, rz)] .\nMinimizing this regret is equivalent to maximizing the\nexpected reward objective used by most meta-RL meth-\nods (Finn et al., 2017a; Duan et al., 2016b). Note that\ndifferent task distributions p(rz) will have different optimal\nlearning procedures f ∗. For example, the optimal behav-\nior for manipulation tasks involves moving a robot’s arms,\nwhile the optimal behavior for locomotion tasks involves\nmoving a robot’s legs. Therefore, f ∗depends on p(rz). We\nnext deﬁne the notion of an optimal unsupervised meta-\nlearner, which does not require prior knowledge of p(rz).\nIn unsupervised meta-reinforcement learning, the reward\ndistribution p(rz) is unknown. In this setting, we evaluate a\nlearning procedure f based on its regret against the worst-\ncase task distribution for CMP C:\nREGRETWC(f, C) = max\np(rz) REGRET(f, p(rz)).\n(1)\nFor a CMP C, we deﬁne the optimal unsupervised learning\nprocedure as follows:\nDeﬁnition 1. The optimal unsupervised learning procedure\nf ∗\nC for a CMP C is deﬁned as\nf ∗\nC ≜arg min\nf\nREGRETWC(f, C).\nNote the optimal unsupervised learning procedure may be\ndifferent for different CMPs. We can also deﬁne the opti-\nmal unsupervised meta-learning algorithm F∗, which takes\nas input a CMP C and returns the optimal unsupervised\nlearning procedure f ∗\nC for that CMP:\nDeﬁnition 2. The optimal unsupervised meta-learner\nF∗(C) = f ∗\nC is a function that takes as input a CMP C\nand outputs the corresponding optimal unsupervised learn-\ning procedure f ∗\nC:\nF∗≜arg min\nF\nREGRETWC(F(C), C)\nNote that the optimal unsupervised meta-learner F∗is uni-\nversal – it does not depend on any particular task distribution,\nor any particular CMP. The next sections discuss how to\nﬁnd the minimax learning procedure, which minimizes the\nworst-case regret (Eq. 1).\n3.3. Special Case: Goal-Reaching Tasks\nWe start by deriving an optimal unsupervised meta-learner\nfor the special case where all tasks are assumed to be goal\nstate reaching tasks, and then generalize this approach to\nsolve arbitrary tasks in Section 3.4. We restrict our anal-\nysis to CMPs with deterministic dynamics, and consider\nepisodes with ﬁnite horizon T and a discount factor of\nUnsupervised Meta-Learning for Reinforcement Learning\nγ = 1. Each tasks corresponds to reaching a goal states sg\nat the last time step in the episode, so the reward function is\nrg(st) ≜1(t = T) · 1(st = g).\nWe ﬁrst derive the optimal learning procedure for the case\nwhere p(sg) is known, and then derive the optimal procedure\nfor the case where p(sg) is unknown.\n3.3.1. THE OPTIMAL LEARNING PROCEDURE FOR\nKNOWN p(sg)\nIn the case of goal reaching tasks, the optimal fast learning\nprocedure f searches through potential goal states until it\nﬁnds the goal and then navigates to that goal state in all\nsubsequent episodes. Deﬁne fπ as the learning procedure\nthat uses policy π to explore until the goal is found, and then\nalways returns to the goal state. We will restrict our attention\nto the set of learning procedures Fπ ≜{fπ} constructed in\nthis fashion, so our theoretical results will be lower bound\non the performance of arbitrary learning procedures. The\nlearning procedure fπ incurs one unit of regret for each step\nbefore it has found the goal, and zero regret afterwards. The\nexpected cumulative regret is therefore the expectation of\nthe hitting time. To compute the expected hitting time, we\ndeﬁne ρT\nπ (s) as the probability that policy π visits state s\nat time step t = T. If sg is the true goal, then the event\nthat the policy π reaches sg at the ﬁnal step of an episode\nis a Bernoulli random variable with parameter p = ρT\nπ (sg).\nThus, the expected hitting time of this goal state is\nHITTINGTIMEπ(sg) =\n1\nρTπ (sg).\nThe regret of the learning procedure fπ is\nREGRET(fπ, p(rg)) =\nZ\nHITTINGTIMEπ(sg)p(sg)dsg\n=\nZ\np(sg)\nρTπ (sg)dsg.\n(2)\nTo now compute the optimal learning procedure fπ, we can\nminimize the regret in Equation 2 w.r.t. the marginal distri-\nbution ρT\nπ . Using the calculus of variations (for more details\nrefer to Appendix C in Lee et al. (2019)), the exploration\npolicy for the optimal meta-learner, π∗, satisﬁes:\nρT\nπ∗(sg) =\np\np(sg)\nR q\np(s′g)ds′g\n.\n(3)\nThus, when the goal sampling distribution p(sg) is known,\nthe optimal learning procedure is obtained by ﬁnding π∗\nsatisfying Eq. 3 and then using fπ∗as the learning proce-\ndure. The next section considers the case where p(sg) is not\nknown.\n3.3.2. THE OPTIMAL UNSUPERVISED LEARNING\nPROCEDURE FOR GOAL REACHING TASKS\nIn the case of goal-reaching tasks where the goal distribu-\ntion p(sg) is not known, the optimal unsupervised learning\nprocedure can be constructed from a policy with a uniform\nmarginal state distribution (proof in Appendix A):\nLemma 1. Let π be a policy for which ρT\nπ (s) is uniform.\nThen fπ is has lowest worst-case regret among learning\nprocedures in Fπ.\nOne route for constructing this optimal unsupervised learn-\ning procedure is to ﬁrst acquire a policy π for which ρT\nπ (s)\nis uniform and then return fπ. However, ﬁnding such a\npolicy π is challenging, especially in high-dimensional state\nspaces and in the absense of resets. Instead, we will take\nan alternate route, acquiring fπ directly without every com-\nputing π. In addition to sidestepping the requirement of\ncomputing π, this approach will also have the beneﬁt of\ngeneralizing beyond goal-reaching tasks to arbitrary task\ndistributions.\nOur approach for directly computing the optimal unsuper-\nvised learning procedure hinges on the observation that\nthe optimal unsupervised learning procedure is the optimal\n(supervised) learning procedure for goals proposed from\na uniform distribution. Thus, the optimal unsupervised\nlearning procedure will come not as a result of a careful\nconstruction, but rather as the output of the an optimiza-\ntion procedure (i.e., meta-learning). Thus, we can obtain\nthe optimal unsupervised learning procedure by applying\na meta-learning algorithm to a task distribution that sam-\nples goals uniformly. To ensure that the resulting learning\nprocedure f lies within the set Fπ, we will only consider\n“memoryless” meta-learning algorithms that maintain no\ninternal state before the true goal is found.2 While sampling\ngoals uniform is itself a challenging problem, we can use the\nsame trick as before: instead of constructing this uniform\ngoal distribution directly, we instead ﬁnd an optimization\nproblem for which the solution is to sample goals uniformly.\nThe optimization problem that we use will involve two latent\nvariables, the ﬁnal state sT and an auxiliary latent variable z\nsampled from a prior µ(z). The optimization problem will\nbe to ﬁnd a conditional distribution µ(sT | z) such that the\nmutual information between z and sT is optimized:\nmax\nµ(sT |z) Iµ(sT ; z)\n(4)\nThe conditional distribution µ(sT | z) that optimizes Equa-\ntion 4 is one with a uniform marginal distribution over ter-\nminal states (proof in Appendix A):\nLemma 2. Assume there exists a conditional distribution\nµ(sT | z) satisfying the following two properties:\n1. The marginal distribution over terminal states is uni-\nform: µ(sT ) =\nR\nµ(sT | z)µ(z)dz = UNIF(S); and\n2MAML satisﬁes this requirement, as the internal parameters\nare updated by policy gradient, which is zero because the reward\nis zero before the true goal is found.\nUnsupervised Meta-Learning for Reinforcement Learning\n2. The conditional distribution µ(sT | z) is a Dirac:\n∀z, sT ∃sz s.t. µ(sT | z) = 1(sT = sz).\nThen any solution µ(sT | z) to the mutual information\nobjective (Eq. 4) satisﬁes the following:\nµ(sT ) = UNIF(S)\nand\nµ(sT | z) = 1(sT = sz).\n3.3.3. OPTIMIZING MUTUAL INFORMATION\nTo optimize the above mutual information objective, we\nnote that a conditional distribution µ(sT | z) can be deﬁned\nimplicitly via a latent-conditioned policy µ(a | s, z). This\npolicy is not a meta-learned model, but rather will become\npart of the task proposal mechanism. For a given prior\nµ(z) and latent-conditioned policy µ(a | s, z), the joint\nlikelihood is\nµ(τ, z) = µ(z)p(s1)\nY\nt\np(st+1 | st, at)µ(at | st, z),\nand the marginal likelihood is simply given by\nµ(sT , z) =\nZ\nµ(τ, z)ds1a1 · · · aT −1.\nThe purpose of our repeated indirection now becomes clear:\nprior work (Eysenbach et al., 2018; Achiam et al., 2018)\nhas proposed efﬁcient algorithms for maximizing the mu-\ntual information objective (Eq. 4) when the conditional\ndistribution µ(sT | z) is deﬁned implicitly in terms of a\nlatent-conditioned policy. At this point, we ﬁnally can sam-\nple goals uniformly, by sampling z ∼µ(z) followed by\nsT ∼µ(sT | z).\nRecall that we wanted to obtain a uniform goal distribution\nso that we could apply meta-learning to obtain the optimal\nlearning procedure. However, the input to meta-learning\nprocedures is not a distribution over goals but a distribution\nover reward functions. We then deﬁne our task proposal\ndistribution p(rz) by sampling z ∼p(z) and using the\ncorresponding reward function rz(sT , aT ) ≜log p(sT | z),\nresulting in a uniform distribution as described in Lemma 2.\n3.4. General Case: Trajectory-Matching Tasks\nTo extend the analysis in the previous section to the general\ncase, and thereby derive a framework for optimal unsuper-\nvised meta-learning, we will consider “trajectory-matching”\ntasks. These tasks are a trajectory-based generalization\nof goal reaching: while goal reaching tasks only provide\na positive reward when the policy reaches the goal state,\ntrajectory-matching tasks only provide a positive reward\nwhen the policy executes the optimal trajectory. The trajec-\ntory matching case is more general because, while trajectory\nmatching can represent different goal-reaching tasks, it can\nalso represent tasks that are not simply goal reaching, such\nas reaching a goal while avoiding a dangerous region or\nreaching a goal in a particular way. Moreover, the trajectory\nmatching case is actually also a generalization of the typ-\nical reinforcement learning case with Markovian rewards,\nbecause any such task can be represented by a trajectory\nreaching objective as well. Please refer to Section 3.4.3 for\na more complete discussion of the same.\nAs before, we will restrict our attention to CMPs with deter-\nministic dynamics. These non-Markovian tasks essentially\namount to a problem where an RL algorithm must “guess”\nthe optimal policy, and only receives a reward if its behavior\nis perfectly consistent with that optimal policy.\nWe will show that optimizing the mutual information be-\ntween z and trajectories to obtain a task proposal distribu-\ntion, and subsequently optimizing a meta-learner for this dis-\ntribution will give us the optimal unsupervised meta-learner\nfor this class of reward functions. We subsequently show\nthat unsupervised meta-learning for the trajectory-matching\ntask is at least as hard as unsupervised meta-learning for\ngeneral tasks. As before, let us begin within an analysis of\noptimal meta-learners in the case where the distribution over\ntrajectory matching tasks p(τ ∗) is known, and subsequently\ndirect our attention to formulating an optimal unsupervised\nmeta-learner.\n3.4.1. OPTIMAL META-LEARNER FOR KNOWN p(τ ∗)\nFormally, we deﬁne a distribution of trajectory-matching\ntasks by a distribution over desired trajectories, p(τ ∗). For\neach goal trajectory τ ∗, the corresponding trajectory-level\nreward function is\nr∗\nτ(τ) ≜1(τ = τ ∗)\nAnalysis from Section 3.3 can be repurposed here. As be-\nfore, restrict our attention to learning procedures fπ ∈Fπ.\nAfter running the exploration policy to discover trajectories\nthat obtain reward, the policy will deterministically keep\nexecuting the desired trajectory. We can deﬁne the hitting\ntime as the expected number of episodes to match the target\ntrajectory:\nHITTINGTIMEπ(τ ∗) =\n1\nπ(τ ∗)\nWe then deﬁne regret as the expected hitting time:\nREGRET(fπ, p(rτ)) =\nZ\nHITTINGTIMEπ(τ)p(τ)dτ)\n=\nZ p(τ)\nπ(τ)dτ.\n(5)\nThis deﬁnition of regret allows us to optimize for an optimal\nlearning procedure, and we obtain an exploration policy for\nthe optimal learning procedure satisfying the requirement\nπ∗(τ) =\np\np(τ)\nR p\np(τ ′)dτ ′ .\nUnsupervised Meta-Learning for Reinforcement Learning\n3.4.2. OPTIMAL UNSUPERVISED LEARNING PROCEDURE\nFOR TRAJECTORY-MATCHING TASKS\nAs described in Section 3.2, obtaining such a policy requires\nknowing the trajectory distribution p(τ), and we must resort\nto optimizing the worst-case regret. As argued in Lemma\n1, the solution to this min-max optimization is a learning\nprocedure which has an exploration policy that is uniform\ndistribution over trajectories.\nLemma 3. Let π be a policy for which π(τ) is uniform.\nThen fπ has lowest worst-case regret among learning pro-\ncedures in Fπ.\nWe can acquire an unsupervised meta-learner of this form\nby proposing and meta-learning on a task distribution that is\nuniform over trajectories. How might we actually propose a\ntask distribution that is uniform over trajectories? As argued\nfor the goal reaching case, we can do so by optimizing a\ntrajectory-level mutual information objective:\nI(τ; z) = H[τ] −H[τ | z]\nThe optimal policy for this objective has a uniform distri-\nbution over trajectories that, conditioned on a particular\nlatent z, deterministically produces a single trajectory in\na deterministic CMP. The analysis for the case of stochas-\ntic dynamics is more involved and is left to future work.\nBy optimizing a task proposal distribution that maximizes\ntrajectory-level mutual information, and subsequently per-\nforming meta-learning on the proposed tasks, we can ac-\nquire the optimal unsupervised meta-learner for trajectory\nmatching tasks, under the deﬁnition in Section 3.2.\n3.4.3. RELATIONSHIP TO GENERAL REWARD\nMAXIMIZING TASKS\nNow that we have derived the optimal meta-learner for\ntrajectory-matching tasks, we observe that trajectory-\nmatching is a super-set of the problem of optimizing any\npossible Markovian reward function at test-time. For a given\ninitial state distribution, each reward function is optimized\nby a particular trajectory. However, trajectories produced by\na non-Markovian policy (i.e., a policy with memory) are not\nnecessarily the unique optimum for any Markovian reward\nfunction. Let Rτ denote the set of trajectory-level reward\nfunctions, and Rs,a denote the set of all state-action level\nreward functions. Bounding the worst-case regret on Rτ\nminimizes an upper bound on the worst-case regret on Rs,a:\nmin\nrτ ∈Rτ Eπ [rτ(τ)] ≤min\nr∈Rs,a Eπ\n\"X\nt\nr(st, at)\n#\n∀π.\nThis inequality holds for all policies π, including the policy\nthat maximizes the LHS. While we aim to maximize the\nRHS, we only know how to maximize the LHS, which gives\nus a lower bound on the RHS. This inequality holds for all\npolicies π, so it also holds for the policy that maximizes the\nLHS. In general, this bound is loose, because the set of all\nMarkovian reward functions is smaller than the set of all\ntrajectory-level reward functions (i.e., trajectory-matching\ntasks). However, this bound becomes tight when consider-\ning meta-learning on the set of all possible (non-Markovian)\nreward functions.\nIn the discussion of meta-learning thus far, we have re-\nstricted our attention to tasks where the reward is provided\nat the last time step T of each episode and to the set of\nlearning procedures Fπ that maintain no internal state be-\nfore the true goal or trajectory is found. In this restricted\nsetting case, the best that an optimal meta-learner can do\nis go directly to a goal or execute a particular trajectory at\nevery episode according to the optimal exploration policy\nas discussed previously, essentially performing a version of\nposterior sampling. In the more general case with arbitrary\nreward functions and arbitrary learning procedures, interme-\ndiate rewards along a trajectory may be informative, and the\noptimal exploration strategy may be different from posterior\nsampling (Rothfuss et al., 2019; Duan et al., 2016b; Wang\net al., 2016).\nNonetheless, the analysis presented in this section provides\nus insight into the behavior of optimal meta-learning algo-\nrithms and allows us to understand the qualities desirable for\nunsupervised task proposals. The general proposed scheme\nfor unsupervised meta-learning has a signiﬁcant beneﬁt over\nstandard universal value function and goal reaching style\nalgorithms: it can be applied to arbitrary reward functions\ngoing beyond simple goal reaching, and doesn’t require the\ngoal to be known in a parametric form beforehand.\n3.5. Summary of Analysis\nThrough our analysis, we introduced the notion of optimal\nmeta-learners and analyze their exploration behavior and\nregret on a class of goal reaching problems. We showed\nthat on these problems, when the test-time task distribution\nis unknown, the optimal meta-training task distribution for\nminimizing worst-case test-time regret is uniform over the\nspace of goals. We also showed that this optimal task dis-\ntribution can be acquired by a simple mutual information\nmaximization scheme. We subsequently extend the analysis\nto the more general case of matching arbitrary trajectories,\nas a proxy for the more general class of arbitrary reward\nfunctions. In the following section, we will discuss how\nwe can derive a practical algorithm for unsupervised meta-\nlearning from this analysis.\n3.6. A Practical Algorithm\nFollowing the derivation in the previous section, we can\ninstantiate a practical unsupervised meta-RL algorithm by\nconstructing a task proposal mechanism based on a mu-\ntual information objective. A variety of different mutual\nUnsupervised Meta-Learning for Reinforcement Learning\nAlgorithm 1 Unsupervised Meta-RL Pseudocode\nInput: M \\ R, an MDP without a reward function\nDφ ←DIAYN() or Dφ ←random\nwhile not converged do\nSample latent task variables z ∼p(z)\nDeﬁne task reward rz(s) using Dφ(z|s)\nUpdate f using MAML with reward rz(s)\nend while\nReturn: a learning algorithm f : Dφ →π\ninformation objectives can be formulated, including mutual\ninformation between single states and z (Eysenbach et al.,\n2018), pairs of start and end states and z (Gregor et al.,\n2016), and entire trajectories and z (Achiam et al., 2018;\nSharma et al., 2019; Warde-Farley et al., 2018). We will\nuse DIAYN and leave a full examination of possible mutual\ninformation objectives for future work.\nDIAYN optimizes mutual information by training a discrim-\ninator network Dφ(z|·) that predicts which z was used to\ngenerate the states in a given rollout according to a latent-\nconditioned policy π(a|s, z). Our task proposal distribu-\ntion is thus deﬁned by rz(s, a) = log(Dφ(z|s)). The com-\nplete unsupervised meta-learning algorithm is as follows:\nﬁrst, we acquire rz(s, a) by running DIAYN, which learns\nDφ(z|s) and a latent-conditioned policy π(a|s, z) (which\nis discarded). Then, we use z ∼p(z) to propose tasks\nrz(s, a) to a standard meta-RL algorithm. This meta-RL\nalgorithm uses the proposed tasks to learn how to learn,\nacquiring a fast learn algorithm f which can then learn new\ntasks quickly. While, in principle, any meta-RL algorithm\ncould be used, we use MAML (Finn et al., 2017a) as our\nmeta-learning algorithm. Note that the learning algorithm f\nreturned by MAML is deﬁned simply as running gradient\ndescent using the initial parameters found by MAML as\ninitialization, as discussed in prior work (Finn & Levine,\n2017a). The method is summarized in Algorithm 1.\nIn addition to mutual information maximizing task propos-\nals, we will also consider random task proposals, where\nwe also use a discriminator as the reward, according to\nr(s, z) = log Dφrand(z|s), but where the parameters φrand\nare chosen randomly (i.e., a random weight initialization for\na neural network). While such random reward functions are\nnot optimal, we ﬁnd that they can surprisingly be used to ac-\nquire useful task distributions for simple tasks, though they\nare not as effective as the tasks become more complicated.\n4. Experimental Evaluation\nIn our experiments, we aim to understand whether unsuper-\nvised meta-learning as described in Section 3.1 can provide\nus with an accelerated RL procedure on new tasks. Whereas\nstandard meta-learning requires a hand-speciﬁed task dis-\ntribution at meta-training time, unsupervised meta-learning\nlearns the task distribution through unsupervised interaction\nwith the environment. A fair baseline that likewise uses\nrequires no reward supervision at training time, and only\nuses rewards at test time, is learning via RL from scratch\nwithout any meta-learning. As an upper bound, we include\nthe unfair comparison to a standard meta-learning approach,\nwhere the meta-training distribution is manually designed.\nThis method has access to a hand-speciﬁed task distribution\nthat is not available to our method. We evaluate two vari-\nants of our approach: (a) task acquisition based on DIAYN\nfollowed by meta-learning using MAML, and (b) task acqui-\nsition using a randomly initialized discriminator followed\nby meta-learning using MAML.\n4.1. Tasks and Implementation Details\nOur experiments study three simulated environments of\nvarying difﬁculty: 2D point navigation, 2D locomotion us-\ning the “HalfCheetah,” and 3D locomotion using the “Ant,”\nwith the latter two environments are modiﬁcations of pop-\nular RL benchmarks (Duan et al., 2016a). While the 2D\nnavigation environment allows for direct control of posi-\ntion, HalfCheetah and Ant can only control their center of\nmass via feedback control with high dimensional actions\n(6D for HalfCheetah, 8D for Ant) and observations (17D\nfor HalfCheetah, 111D for Ant).\nThe evaluation tasks, shown in Figure 5, are similar to prior\nwork (Finn et al., 2017a; Pong et al., 2018): 2D navigation\nand ant require navigating to goal positions, while the half\ncheetah must run at different goal velocities. These tasks are\nnot accessible to our algorithm during meta-training. Please\nrefer to Appendix C for details about hyperparameters for\nboth MAML and DIAYN.\n4.2. Fast Adaptation after Unsupervised Meta RL\nThe comparison between the two variants of unsupervised\nmeta-learning and learning from scratch is shown in Fig-\nure 2. We also add a comparison to VIME (Houthooft\net al., 2016), a standard novelty-based exploration method,\nwhere we pretrain a policy with the VIME reward and then\nﬁnetune it on the meta-test tasks. In all cases, the UML-\nDIAYN variant of unsupervised meta-learning produces\nan RL procedure that outperforms RL from scratch and\nVIME-init, suggesting that unsupervised interaction with\nthe environment and meta-learning is effective in producing\nenvironment-speciﬁc but task-agnostic priors that accelerate\nlearning on new, previously unseen tasks. The comparison\nwith VIME shows that the speed of learning is not just about\nexploration but is indeed about fast adaptation. In our exper-\niments thus far, UML-DIAYN always performs better than\nlearning from scratch, although the beneﬁt varies across\ntasks depending on the actual performance of DIAYN. We\nalso perform signiﬁcantly better than a baseline of simply\nUnsupervised Meta-Learning for Reinforcement Learning\n2D navigation\nHalf-Cheetah\nAnt\nFigure 2. Unsupervised meta-learning accelerates learning: After unsupervised meta-learning, our approach (UML-DIAYN and UML-\nRANDOM) quickly learns a new task signiﬁcantly faster than learning from scratch, especially on complex tasks. Learning the task\ndistribution with DIAYN helps more for complex tasks. Results are averaged across 20 evaluation tasks, and 3 random seeds for testing.\nUML-DIAYN and random also signiﬁcantly outperform learning with DIAYN initialization or VIME.\n2D Navigation\nHalf-Cheetah\nAnt Navigation\nFigure 3. Comparison with handcrafted tasks: Unsupervised meta-learning (UML-DIAYN) is competitive with meta-training on\nhandcrafted reward functions (i.e., an oracle). A misspeciﬁed, handcrafted meta-training task distribution often performs worse,\nillustrating the beneﬁts of learning the task distribution.\ninitializing from a DIAYN trained contextual policy, and\nthen ﬁnetuning the best skill with the actual task reward.\nInterestingly, in many cases (in Figure 3) the performance\nof unsupervised meta-learning with DIAYN matches that\nof the hand-designed task distribution. We see that on the\n2D navigation task, while handcrafted meta-learning is able\nto learn very quickly initially, it performs similarly after\n100 steps. For the cheetah environment as well, handcrafted\nmeta-learning is able to learn very quickly to start off, but\nis quickly matched by unsupervised meta-RL with DIAYN.\nOn the ant task, we see that hand-crafted meta-learning\ndoes do better than UML-DIAYN, likely because the task\ndistribution is challenging, and a better unsupervised task\nproposal algorithm would improve performance.\nThe comparison between the two unsupervised meta-\nlearning variants is also illuminating: while the DIAYN-\nbased variant of our method generally achieves the best\nperformance, even the random discriminator is often able to\nprovide a sufﬁcient diversity of tasks to produce meaningful\nacceleration over learning from scratch in the case of 2D\nnavigation and ant. This result has two interesting impli-\ncations. First, it suggests that unsupervised meta-learning\nis an effective tool for learning an environment prior. Al-\nthough the performance of unsupervised meta-learning can\nbe improved with better coverage using DIAYN (as seen in\nFigure 2), even the random discriminator version provides\ncompetitive advantages over learning from scratch. Second,\nthe comparison provides a clue for identifying the source of\nthe structure learned through unsupervised meta-learning:\nthough the particular task distribution has an effect on per-\nformance, simply interacting with the environment (without\nstructured objectives, using a random discriminator) already\nallows meta-RL to learn effective adaptation strategies in a\ngiven environment.\n5. Discussion and Future Work\nWe presented an unsupervised approach to meta-RL, where\nmeta-learning is used to acquire an efﬁcient RL procedure\nwithout requiring hand-speciﬁed task distributions. This ap-\nproach accelerates RL without relying on the manual super-\nvision required for conventional meta-learning algorithms.\nWe provide a theoretical derivation that argues that task\nproposals based on mutual information maximization can\nprovide a minimum worst-case regret meta-learner, under\ncertain assumptions. Our experiments indicate unsupervised\nmeta-RL can accelerate learning on a range of tasks.\nOur approach also opens a number of questions about un-\nsupervised meta-learning algorithms. One limitation of our\nanalysis is that it only considers deterministic dynamics, and\nonly considers task distributions where posterior sampling\nis optimal. Extending our analysis to stochastic dynamics\nand more realistic task distributions may allow unsuper-\nvised meta-RL to acquire learning algorithms that can more\neffectively solve real-world tasks.\nUnsupervised Meta-Learning for Reinforcement Learning\nReferences\nJoshua Achiam, Harrison Edwards, Dario Amodei, and Pieter\nAbbeel. Variational option discovery algorithms. arXiv preprint\narXiv:1807.10299, 2018.\nMaruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever,\nIgor Mordatch, and Pieter Abbeel. Continuous adaptation via\nmeta-learning in nonstationary and competitive environments.\narXiv preprint arXiv:1710.03641, 2017.\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W\nHoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learn-\ning to learn by gradient descent by gradient descent. In Neural\nInformation Processing Systems (NIPS), 2016.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schnei-\nder, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin,\nOpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight expe-\nrience replay. In Advances in Neural Information Processing\nSystems, pp. 5048–5058, 2017.\nAntreas Antoniou and Amos Storkey. Assume, augment and learn:\nUnsupervised few-shot meta-learning via random labels and\ndata augmentation. arXiv preprint arXiv:1902.09884, 2019.\nChristopher G Atkeson and Juan Carlos Santamaria. A comparison\nof direct and model-based reinforcement learning. In Proceed-\nings of International Conference on Robotics and Automation,\nvolume 4, pp. 3557–3564. IEEE, 1997.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom\nSchaul, David Saxton, and R´emi Munos. Unifying count-based\nexploration and intrinsic motivation. CoRR, abs/1606.01868,\n2016. URL http://arxiv.org/abs/1606.01868.\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei.\nOn the optimization of a synaptic learning rule. In Optimality\nin Artiﬁcial and Biological Neural Networks, 1992.\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey\nLevine. Deep reinforcement learning in a handful of trials\nusing probabilistic dynamics models. In Advances in Neural\nInformation Processing Systems, pp. 4754–4765, 2018.\nJohn D Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Ey-\nsenbach, Pieter Abbeel, and Sergey Levine. Self-consistent\ntrajectory autoencoder: Hierarchical reinforcement learning\nwith trajectory embeddings. arXiv preprint arXiv:1806.02813,\n2018.\nMarc Deisenroth and Carl E Rasmussen. Pilco: A model-based and\ndata-efﬁcient approach to policy search. In Proceedings of the\n28th International Conference on machine learning (ICML-11),\npp. 465–472, 2011.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter\nAbbeel. Benchmarking deep reinforcement learning for continu-\nous control. In International Conference on Machine Learning,\npp. 1329–1338, 2016a.\nYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya\nSutskever, and Pieter Abbeel.\nRl2:\nFast reinforcement\nlearning via slow reinforcement learning.\narXiv preprint\narXiv:1611.02779, 2016b.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey\nLevine. Diversity is all you need: Learning skills without a\nreward function. arXiv preprint arXiv:1802.06070, 2018.\nChelsea Finn and Sergey Levine. Meta-learning and universality:\nDeep representations and gradient descent can approximate\nany learning algorithm. CoRR, abs/1710.11622, 2017a. URL\nhttp://arxiv.org/abs/1710.11622.\nChelsea Finn and Sergey Levine. Deep visual foresight for plan-\nning robot motion. In 2017 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 2786–2793. IEEE, 2017b.\nChelsea Finn and Sergey Levine. Meta-learning and universality:\nDeep representations and gradient descent can approximate\nany learning algorithm. International Conference on Learning\nRepresentations, 2018.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic\nmeta-learning for fast adaptation of deep networks.\narXiv\npreprint arXiv:1703.03400, 2017a.\nChelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and\nSergey Levine. One-shot visual imitation learning via meta-\nlearning.\nCoRR, abs/1709.04905, 2017b.\nURL http://\narxiv.org/abs/1709.04905.\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Varia-\ntional intrinsic control. arXiv preprint arXiv:1611.07507, 2016.\nAbhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel,\nand Sergey Levine. Meta-reinforcement learning of structured\nexploration strategies. arXiv preprint arXiv:1802.07245, 2018.\nDavid Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel.\nAutomatic goal generation for reinforcement learning agents.\narXiv preprint arXiv:1705.06366, 2017.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learn-\ning to learn using gradient descent. In International Conference\non Artiﬁcial Neural Networks, 2001.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De\nTurck, and Pieter Abbeel. VIME: variational information max-\nimizing exploration. In Advances in Neural Information Pro-\ncessing Systems, 2016.\nRein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie,\nFilip Wolski, Jonathan Ho, and Pieter Abbeel. Evolved policy\ngradients. arXiv preprint arXiv:1802.04821, 2018.\nKyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning\nvia meta-learning. arXiv preprint arXiv:1810.02334, 2018.\nAllan Jabri, Kyle Hsu, Abhishek Gupta, Ben Eysenbach, Sergey\nLevine, and Chelsea Finn. Unsupervised curricula for visual\nmeta-reinforcement learning. In Advances in Neural Informa-\ntion Processing Systems, pp. 10519–10530, 2019.\nZilong Ji, Xiaolong Zou, Tiejun Huang, and Si Wu. Unsupervised\nfew-shot learning via self-supervised training. arXiv preprint\narXiv:1912.12178, 2019.\nPhilipp Kr¨ahenb¨uhl, Carl Doersch, Jeff Donahue, and Trevor Dar-\nrell.\nData-dependent initializations of convolutional neural\nnetworks. arXiv preprint arXiv:1511.06856, 2015.\nLisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing,\nSergey Levine, and Ruslan Salakhutdinov. Efﬁcient exploration\nvia state marginal matching. CoRR, abs/1906.05274, 2019.\nURL http://arxiv.org/abs/1906.05274.\nUnsupervised Meta-Learning for Reinforcement Learning\nJianxin Lin, Yijun Wang, Yingce Xia, Tianyu He, and Zhibo Chen.\nLearning to transfer: Unsupervised meta domain translation.\narXiv preprint arXiv:1906.00181, 2019.\nRussell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel,\nSergey Levine, and Chelsea Finn. Guided meta-policy search.\nCoRR, abs/1904.00956, 2019.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel.\nA simple neural attentive meta-learner. In NIPS 2017 Workshop\non Meta-Learning, 2017.\nTsendsuren Munkhdalai and Hong Yu. Meta networks. Interna-\ntional Conference on Machine Learning (ICML), 2017.\nAnusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fear-\ning, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learn-\ning to adapt in dynamic, real-world environments through\nmeta-reinforcement learning. arXiv preprint arXiv:1803.11347,\n2018a.\nAnusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey\nLevine. Neural network dynamics for model-based deep rein-\nforcement learning with model-free ﬁne-tuning. In 2018 IEEE\nInternational Conference on Robotics and Automation (ICRA),\npp. 7559–7566. IEEE, 2018b.\nDevang K Naik and RJ Mammone. Meta-neural networks that\nlearn by learning. In International Joint Conference on Neural\nNetowrks (IJCNN), 1992.\nIan Osband, Charles Blundell, Alexander Pritzel, and Ben-\njamin Van Roy. Deep exploration via bootstrapped DQN. CoRR,\nabs/1602.04621, 2016. URL http://arxiv.org/abs/\n1602.04621.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Dar-\nrell. Curiosity-driven exploration by self-supervised prediction.\nIn ICML, 2017.\nVitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine.\nTemporal difference models: Model-free deep rl for model-\nbased control. arXiv preprint arXiv:1802.09081, 2018.\nKate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn,\nand Sergey Levine. Efﬁcient off-policy meta-reinforcement\nlearning via probabilistic context variables.\narXiv preprint\narXiv:1903.08254, 2019.\nSachin Ravi and Hugo Larochelle. Optimization as a model for\nfew-shot learning. In International Conference on Learning\nRepresentations (ICLR), 2017.\nJonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and\nPieter Abbeel. Promp: Proximal meta-policy search. In In-\nternational Conference on Learning Representations, ICLR,\n2019.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wier-\nstra, and Timothy Lillicrap.\nMeta-learning with memory-\naugmented neural networks. In International Conference on\nMachine Learning (ICML), 2016.\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Uni-\nversal value function approximators. In International Confer-\nence on Machine Learning, pp. 1312–1320, 2015.\nJ¨urgen Schmidhuber. Evolutionary principles in self-referential\nlearning, or on learning how to learn: the meta-meta-... hook.\nPhD thesis, Technische Universit¨at M¨unchen, 1987.\nJ¨urgen Schmidhuber.\nDriven by compression progress:\nA\nsimple principle explains essential aspects of subjective\nbeauty, novelty, surprise, interestingness, attention, curios-\nity, creativity, art, science, music, jokes.\nIn Computa-\ntional Creativity: An Interdisciplinary Approach, 12.07. -\n17.07.2009, 2009. URL http://drops.dagstuhl.de/\nopus/volltexte/2009/2197/.\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and\nKarol Hausman. Dynamics-aware unsupervised discovery of\nskills. arXiv preprint arXiv:1907.01657, 2019.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical net-\nworks for few-shot learning. In Advances in Neural Information\nProcessing Systems, pp. 4080–4090, 2017.\nAravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and\nChelsea Finn. Universal planning networks. arXiv preprint\narXiv:1804.00645, 2018.\nBradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentiviz-\ning exploration in reinforcement learning with deep predictive\nmodels. arXiv preprint arXiv:1507.00814, 2015.\nBradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan,\nYuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some consider-\nations on learning to explore via meta-reinforcement learning.\nCoRR, abs/1803.01118, 2018. URL http://arxiv.org/\nabs/1803.01118.\nSainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Syn-\nnaeve, Arthur Szlam, and Rob Fergus. Intrinsic motivation and\nautomatic curricula via asymmetric self-play. arXiv preprint\narXiv:1703.05407, 2017.\nFlood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and\nYongxin Yang. Learning to learn: Meta-critic networks for\nsample efﬁcient learning. arXiv preprint arXiv:1706.09529,\n2017.\nSebastian Thrun and Lorien Pratt. Learning to learn. Springer\nScience & Business Media, 1998.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,\nJoel Z Leibo, Remi Munos, Charles Blundell, Dharshan Ku-\nmaran, and Matt Botvinick. Learning to reinforcement learn.\narXiv preprint arXiv:1611.05763, 2016.\nDavid Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin\nIonescu, Steven Hansen, and Volodymyr Mnih. Unsupervised\ncontrol through non-parametric discriminative rewards. arXiv\npreprint arXiv:1811.11359, 2018.\nDarrell Whitley and Jean Paul Watson. Complexity theory and the\nno free lunch theorem, 2005.\nDavid H Wolpert, William G Macready, et al. No free lunch\ntheorems for search. Technical report, Technical Report SFI-\nTR-95-02-010, Santa Fe Institute, 1995.\nUnsupervised Meta-Learning for Reinforcement Learning\nA. Proofs\nLemma 1 Let π be a policy for which ρT\nπ (s) is uniform.\nThen π has lowest worst-case regret.\nProof of Lemma 1. To begin, we note that all goal distribu-\ntions p(sg) have equal regret for policies where ρT\nπ (s) =\n1/|S| is uniform:\nREGRETp(π) =\nZ\np(sg)\nρTπ (sg)dsg =\nZ p(sg)\n1/|S|dsg = |S|\nNow, consider a policy π′ for which ρT\nπ (s) is not uniform.\nFor simplicity, we will assume that the argmin is unique,\nthough the proof holds for non-unique argmins as well. The\nworst-case goal distribution will choose the state s−where\nthat the policy is least likely to visit:\np−(sg) ≜1(sg = arg min\ns\nρT\nπ (s))\nThus, the worst-case regret for policy π′ is strictly greater\nthan the regret for a uniform π:\nmax\np\nREGRETp(π) = REGRETp−(π)\n=\nZ 1(sg = arg mins ρT\nπ (s))\nρTπ (sg)\ndsg\n=\n1\nmins ρT\nπ′(s) > |S|\n(6)\nThus, a policy π′ for which ρT\nπ is non-uniform cannot be\nminimax, so the optimal policy has a uniform marginal\nρT\nπ .\nLemma 2: Mutual information I(sT ; z) is maximized by a\ntask distribution p(sg) which is uniform over goal states.\nProof of Lemma 2. We deﬁne a latent variable model,\nwhere we sample a latent variable z from a uniform prior\np(z) and sample goals from a conditional distribution\np(sT | z). To begin, note that the mutual information can\nbe written as a difference of entropies:\nIp(sT ; z) = Hp[sT ] −Hp[sT | z]\nThe conditional entropy Hp[sT | z] attains the smallest pos-\nsible value (zero) when each latent variable z corresponds to\nexactly one ﬁnal state, sz. In contrast, the marginal entropy\nHp[sT ] attains the largest possible value (log |S|) when the\nmarginal distribution p(sT ) =\nR\np(sT | z)p(z)dz is uni-\nform. Thus, a task uniform distribution p(sg) maximizes\nI(sT ; z). Note that for any non-uniform task distribution\nq(sT ), we have Hq[sT ] < Hp[sT ]. Since the conditional\nentropy Hp[sT | z] is zero, no distribution can achieve a\nsmaller conditional entropy. This, for all non-uniform task\ndistributions q, we have Iq(sT ; z) < Ip(sT ; z). Thus, the\noptimal task distribution must be uniform.\nB. Ablations\nFigure 4. Analysis of effect of additional meta-training on meta-\ntest time learning of new tasks. For larger iterations of meta-trained\npolicies, we have improved test time performance, showing that\nadditional meta-training is beneﬁcial.\nTo understand the method performance more clearly, we\nalso add an ablation study where we compare the meta-test\nperformance of policies at different iterations along meta-\ntraining. This shows the effect that additional meta-training\nhas on the fast learning performance for new tasks. This\ncomparison is shown in Figure 4. As can be seen here, at\niteration 0 of meta-training the policy is not a very good ini-\ntialization for learning new tasks. As we move further along\nthe meta-training process, we see that the meta-learned ini-\ntialization becomes more and more effective at learning new\ntasks. This shows a clear correlation between additional\nmeta-training and improved meta test-time performance.\nB.1. Analysis of Learned Task Distributions\nWe can analyze the tasks discovered through unsupervised\nexploration and compare them to tasks we evaluate on at\nmeta-test time. Figure 5 illustrates these distributions using\nscatter plots for 2D navigation and the Ant, and a histogram\nfor the HalfCheetah. Note that we visualize dimensions of\nthe state that are relevant for the evaluation tasks – positions\nand velocities – but these dimensions are not speciﬁed in any\nway during unsupervised task acquisition, which operates\non the entire state space. Although the tasks proposed via\nunsupervised exploration provide fairly broad coverage, they\nare clearly quite distinct from the meta-test tasks, suggesting\nthe approach can tolerate considerable distributional shift.\nQualitatively, many of the tasks proposed via unsupervised\nexploration such as jumping and falling that are not relevant\nfor the evaluation tasks. Our choice of the evaluation tasks\nwas largely based on prior work, and therefore not tailored\nto this exploration procedure. The results for unsupervised\nUnsupervised Meta-Learning for Reinforcement Learning\n2D navigation\nAnt\nHalf-Cheetah\nFigure 5. Learned meta-training task distribution and evaluation tasks: We plot the center of mass for various skills discovered by\npoint mass and ant using DIAYN, and a blue histogram of goal velocities for cheetah. Evaluation tasks, which are not provided to the\nalgorithm during meta-training, are plotted as red ‘x’ for ant and pointmass, and as a green histogram for cheetah. While the meta-training\ndistribution is broad, it does not fully cover the evaluation tasks. Nonetheless, meta-learning on this learned task distribution enables\nefﬁcient learning on a test task distribution.\nmeta-RL therefore suggest quite strongly that unsupervised\ntask acquisition can provide an effective meta-training set,\nat least for MAML, even when evaluating on tasks that do\nnot closely match the discovered task distribution.\nC. Hyperparameter Details\nHalf-Cheetah\nAnt\nFigure 6. Environments: (Left) Half-Cheetah and (Right) Ant\nFor all our experiments, we used DIAYN to acquire the task\nproposals using 20 skills for half-cheetah and for ant and 50\nskills for the 2D navigation. We illustrate these half cheetah\nand ant in Fig. 6. We ran the domains using the standard DI-\nAYN hyperparameters described in https://github.\ncom/ben-eysenbach/sac to acquire task proposals.\nThese proposals were then fed into the MAML algorithm\nhttps://github.com/cbfinn/maml_rl, with in-\nner learning rate 0.1, meta learning rate 0.01, inner batch\nsize 40, outer batch size, path length 100, using 2 layer\nnetworks with 300 units each with ReLu nonlinearities.\nWe vary the meta-batch size according to the number of\nskills: 50 for pointmass, 20 for cheetah, and 20 ant. The\ntest time learning is done with the same parameters for\nthe UMRL variants, and done using REINFORCE with\nthe Adam optimizer for the comparison with learning from\nscratch. We swept over learning rates for learning from\nscratch via vanilla policy gradient, and found that using\nADAM with adaptive step size is the most stable and quick\nat learning.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-06-12",
  "updated": "2020-04-30"
}