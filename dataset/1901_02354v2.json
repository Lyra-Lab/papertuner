{
  "id": "http://arxiv.org/abs/1901.02354v2",
  "title": "Geometrization of deep networks for the interpretability of deep learning systems",
  "authors": [
    "Xiao Dong",
    "Ling Zhou"
  ],
  "abstract": "How to understand deep learning systems remains an open problem. In this\npaper we propose that the answer may lie in the geometrization of deep\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\nand quantum computation and this may result in a new scheme to reveal the rule\nof the physical world. By comparing the geometry of image matching and deep\nnetworks, we show that geometrization of deep networks can be used to\nunderstand existing deep learning systems and it may also help to solve the\ninterpretability problem of deep learning systems.",
  "text": "arXiv:1901.02354v2  [cs.LG]  13 Jan 2019\n1\nGeometrization of deep networks for the interpretability of deep\nlearning systems\nXiao Dong, Ling Zhou\nFaculty of Computer Science and Engineering, Southeast University, Nanjing, China\nHow to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the\ngeometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation\nand this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and\ndeep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may\nalso help to solve the interpretability problem of deep learning systems.\nIndex Terms—deep networks, geometrization, physics, computation\nI. MOTIVATION\nAs a general tool to solve complex problems, the thriving\ndeep learning technology is showing its power in almost all\nresearch ﬁelds. But we are still lacking a general theoretical\nframework to to answer the following questions: Why does\ndeep learning work so well? What’s the relationship between\nthe structure of a deep network and its functionality? How to\ndesign a proper deep network structure for a given task? How\ncan we predict and control the behaviour of a deep network\nduring training? How can our brain construct efﬁcient network\nstructures for different tasks with limited resources?\nIn this work we propose a general framework to understand\ndeep learning systems, the geometrization of deep networks.\nA. Why geometrization\nOur motivation to understand deep learning systems from a\ngeometric perspective falls in three folds.\nDeep networks are physical The reason that deep learning\nis so powerful and universally effective in different ﬁelds is\nthat deep networks reveal the structures of physical systems.\nThat’s to say, deep networks are effective representations of\nphysical systems and their evolutions. Besides the enormous\nexamples of AI based applications on computer vision, nat-\nural language processing and robot control, deep networks\nare also closely related with the fundamental laws of our\nworld, for example the effective representation of many-\nbody quantum systems[1], renormalization group and entan-\nglement renormalization[2][3], tensor networks and AdS/CFT\nduality[4][5][6][7][8][9][10]. So we believe the effectiveness\nof deep networks has a fundamental physical origin. That is,\nthe deep network is at least a replica of our physical world so\nthat every physical system has a correspondent deep network\nrepresentation. Deep networks may share the same structure\nof the physical world and they may obey the same rules. The\ngreat success of geometrization of physics inspires our idea\nthat geometrization may also be the ultimate framework to\nunderstand deep networks and deep learning systems.\nDeep networks are computational programmes From\nthe quantum computation point of view, any physical system\ncan be regarded as being generated from an initial simple\nstate by an unitary operation. Similarly the evolution of any\nphysical system is also an unitary operation or equivalently\na computation process. As effective descriptions of physical\nsystems and their evolutions, deep networks are essentially\ncomputation processes and can be understood as computa-\ntional programmes to generate and evolve physical systems.\nThen the geometrization of quantum states and quantum\ncomputations[11][12] also leads to the geometrization of deep\nlearning systems. For example, quantum computation com-\nplexity has a clear geometric picture and concrete physi-\ncal meanings as discussed in complexity=action, complex-\nity=volume, Hamiltonian complexity, tensor networks and the\nemergent spacetime structure from quantum information.\nDeep networks as optimal control and optimization\nsystems Recently there are emerging efforts to formulate deep\nlearning systems as either optimization or optimal control\nproblems[13][14]. It’s well-known that these are also closely\nrelated with geometry and physics. We will show this point\nwith a concrete example of template image matching, which\nhas a clear geometric picture as an optimization or an optimal\ncontrol problem.\nAll the above observations lead to the same conclusion,\ngeometrization scheme may bring us new perspectives to\nunderstand deep networks and deep learning systems. What’s\nmore, if the geometrization of deep networks can be accom-\nplished, this may also change our ways to understand the\nphysical world, i.e. a physical world built by deep networks.\nNow we show how to build the geometrization of deep net-\nworks and how this can help us to understand deep networks\nand deep learning.\nB. An abstract description of deep networks\nIn order to establish the geometric picture of deep networks,\nwe now give an abstract description of it.\nAs mentioned above, deep networks are programmes or\ndata processing systems, which can achieve a transformation\nfrom the input data space Vin to the output data space Vout.\nNormal deep learning tasks, such as feature extraction and\ngenerative models, are all mappings between different data\nspaces. And usually we prefer one of them to be a vector\n2\nspace so that algebraic operations or classiﬁcations can be\neasily carried out on it. From the general computation point\nof view, a data processing or a computation system can be\nabstracted as a mapping C : V × G →V , where V is the\nspace of data and G is the space of operations on data. A\ncomputation process is given by C(vin, g) = g(vin) = vout,\nwhere vi, so ∈V are the input and output data and g ∈G\nis an operation or transformation on data. A programme or\nan algorithm is a realization of g, which is usually achieved\nby a series of simple primitive operations as in both classical\nand quantum computers. The structure of a deep network is\nessentially a parametric realization of g and the process of\ntraining is to ﬁnd the proper network parameters that achieve\ng. Here we would like to note that the parameters may also\ninclude part of the network structure so that network structure\nitself may also be learned during training.\nThe key feature of deep networks, the deep structure means\nthat g is realized by a discrete time sequence of transforma-\ntions {¯gn|n ∈[0, N], ¯g0 = Id, ¯gN = g}, where Id stands for\nthe identity transformation. In this transformation sequence,\nthe nth step achieves an operation ¯gn ◦¯g−1\nn−1, which is usually\na simple low complexity operation. To make an analytical\nstudy of deep learning networks, we introduce a continuous\ntime ﬂow {gt|t ∈[0, 1], g0 = Id, g1 = g}. The validity\nof this continuous ﬂow fundamentally lies in the continuous\nevolution of quantum states. This is to say, a discrete time\nmodel of quantum information processing system such as the\nquantum circuit model is essentially only an approximation\nof a continuous time quantum evolution. Similarly a discrete\ntime deep network is only an approximation of a continuous\nﬂow of transformation.\nObviously the continuous time ﬂow of transformation has\na geometric picture. It is a continuous curve in the space\nof transformation connecting the identity operation I and the\ntarget operation g. Accordingly for each input data vin, there\nis a curve in the data space given by {vi,t|t ∈0, 1, v0 =\nvin, v1 = vout}. The purpose of deep learning systems is\nto ﬁnd an optimal transformation ﬂow to realize the target\ntransformation, where the correspondent collection of the\ntrajectories of all the input data {vk\nin,t, vk\nin ∈Vin} should\nshow a good shape, where a good shape means the trajectories\nshould be smooth, stable and well distributed so that the the\nnetwork has a good genearalization performace.\nWe now focus on the continuous transformation curve\ngt, t ∈[0, 1]. If we regard the space of transformation G\nas a manifold, then we can build a Riemannian structure on\nit. We can deﬁne the time derivative of gt as ˙gt = ut ◦gt\nand a right invariant metric on the tangent space T G of\nG as < ˙gt, ˙gt >T G=< ut, ut >Id. Then we can calculate\nthe length of the curve gt, t ∈[0, 1], g0 = I, g1 = g as\nR 1\n0 < ut, ut > dt, which is the algorithmic complexity of\nthe realization gt, t ∈[0, 1] of g.\nNow we have a simple geometric picture of deep networks.\nThe structure of a deep network and the metric determines\nthe length of the curve. Network parameters and the metric\ndetermines the shape of the curve. The optimal realization of\ng under a constraint to minimize the length of the curve is the\ngeodesic from Id to g.\nOf course, keen readers will argue that above geometric\npicture is too abstract for a quantitative or even a qualitative\nunderstanding of deep learning systems. In the remaining part\nof this paper, we will ﬁrstly give a solid example of the\ngeometrization of deep networks by comparing deep networks\nwith the geometry of image matching. Then we scratch a\nbroader picture of the geometrization of deep networks by\ncomparing deep networks with other physical systems in-\ncluding quantum information processing, quantum many-body\nsystems, spacetime structure and general relativity.\nII. GEOMETRY OF IMAGE REGISTRATION\nComputational anatomy[15] is a research ﬁeld to study\nthe variability of anatomical shapes, where the comparison\nbetween shapes is the key issue. Mathematically a shape can\nbe described by a function on a spatial space I : Rn →Rm,\nwhich we call an image I. Here n = 2, 3 stands for a 2D\nor a 3D image. m = 1 and m > 1 mean scalar images\nand vector/tensor images. For two different shapes represented\nby correspondent images I0, I1, the task of image registration\nis to ﬁnd a transformation ϕ so that the difference between\nthe target image I1 and the transformed source image I0\nis minimized, i.e. minϕ ∥I1 −I0 ◦ϕ∥. The details of the\ntransformation I0 ◦ϕ depends on the type of the image I0[15].\nA. Diffeomorphic image registration: optimization vs opti-\nmal control\nDiffeomorphic image registration is a framework for shape\ncomparison by modeling transformations between shapes as a\nsmooth invertible function ϕ : Rn →Rn. For example the\nspace of transformations of volumetric images can be taken\nas G = Diff(R3), which is the diffeomorphism group of\nR3, and V = I(R3) as the space of volumetric images on R3.\nDeforming an image I0 ∈V by a transformation ϕ ∈G is just\nthe change of coordinate as I0 ◦ϕ. Following [16][17], image\nregistration can be abstracted as a map G× V →V , where G\nis the group of diffeomorphic image transformations and V is\nthe vector space of images. Large deformation diffeomorphic\nmetric mapping (LDDMM)[18] generates a deformation ϕ as\na ﬂow ϕu\nt of a time-dependent vector ﬁeld ut ∈Te(G) = g\nso that\n˙ϕu\nt = ut ◦ϕu\nt , ϕu\n0 = Id, ϕu\n1 = ϕ\n(1)\nThe diffeomorphic matching of two images I0 and I1 with\nLDDMM is to ﬁnd a vector ﬁeld ut, t ∈[0, 1] to minimize the\ncost function\nE(ut) =\nZ 1\n0\nl(ut)2dt+β|I1−Io◦ϕu\n1|2, ˙ϕu\nt = ut◦ϕu\nt , ϕu\n0 = Id\n(2)\nHere the regularity on ut is a kinetic energy term l(ut) =\n1\n2\nR 1\n0 |ut|2dt with |ut| a norm on the vector ﬁeld deﬁned\nas |ut|2 = ⟨Lut, ut⟩L2. The operator L is a positive self-\nadjoint differential operator, for example Lut = ut −α2∆ut.\nObviously the norm |ut|2 = ⟨Lut, ut⟩L2 deﬁnes a Riemannian\nmetric on the manifold of the diffeomorphic transformation\ngroup Diff(Rn). The second term of E(ut) computes the\ndifference between the transformed image Io ◦ϕu\n1 and I1.\n3\nA necessary condition DE(ut) = 0 to minimize the cost\nfunction is that the vector ﬁeld ut should satisfy the Euler-\nPoincar´e (E-P) equation\nLut = −ϕu\n0,tI0 ⋄ϕu\n0,tϕu\n1,0π\n(3)\nwhere ϕu\ns,t = ϕu\nt ◦ϕu\ns−1, π := β(ϕu\n0,tI0 −I1)♭∈V ∗. The\n♭operator is deﬁned as ♭: V →V ∗, ⟨u♭, v⟩V ∗×V = ⟨u, v⟩\nand ⋄: T V ∗→g∗, ⟨I ⋄π, u⟩g∗×g = ⟨π, ζu(I)⟩V ∗×V is the\nmomentum map.\nThe E-P equation can also be given as\nd\ndt\n∂l(ut)\n∂ut\n= −ad∗\nut\n∂l(ut)\n∂ut\n(4)\nwhere ∂l(ut)\n∂ut\nis the momentum and ad∗: g →gl(g) is the\ncoadjoint representation of the Lie algebra g of the Lie group\nG. For more details please refer to [16][17].\nIn LDDMM framework, the curve satisfying the E-P equa-\ntion is found by a gradient descent algorithm, while the\ngradient is given by ut + Kϕu\n0,tI0 ⋄ϕu\n0,tϕu\n1,0π with K = L−1.\nThe geometric picture of LDDMM is quite simple: LD-\nDMM ﬁnds a minimal length curve, i.e. a geodesic given\nby the E-P equation, in Diff(Rn) connecting Id and ψ,\nwhich can transform the source I0 to a near neighbour of the\ntarget image I1. Equivalently we can also induce a Riemannian\nstructure on the image space V by the map G × V →V so\nthat the geodesic on G leads to a geodesic on V[17].\nHere we point out that this is exactly the same as in the\ngeometry of quantum computation[11][19] that a Riemannian\nmetric on the quantum operation group induces a Rieman-\nnian metric on the Hilbert space of quantum states. Another\ninteresting observation is that the map G × V\n→V can\nalso be understood as a typical computation system, where\nV is the data representation space and G is the data operation\nspace. So in fact image registration and quantum computation\nessentially have the same abstract descriptions and geometric\npictures[10].\nLDDMM based image registration is formulated as an\noptimization problem and solved by a gradient descent based\noptimization. The optimal solution ϕt is parameterized by the\ntime-dependent vector ﬁeld ut and the optimization procedure\nis a parameter estimation of ut. We can easily see this is\nvery similar with the abstract model of deep networks we\nintroduced above.\nAn alternative framework of LDDMM is to formulate it as\nan optimal control problem[20], where the image registration\nprocedure is regarded as a dynamical process. The state of\nthe dynamical system is the transformed source image I0 ◦ϕt\nand the vector ﬁeld ut is taken as the control signal to adjust\nthe transformation ϕt. The problem is then to minimize the\nenergy function\nE(ut, J0\nt , λt, γ)\n=\nZ 1\n0\nl(ut) + ⟨λt, ˙J0\nt + ∇It · ut⟩dt(5)\n+\n< γ, J0\n0 −I0 > +β|J0\n1 −I1|\n(6)\nwhere J0\nt\n= I0 ◦ϕu\n0,t, J1\nt\n= I1 ◦ϕu\n1,t and λt, γ are the\nLagrangian multipliers.\nThis leads to the optimality conditions as follows\n˙J0\nt + ∇J0\nt · ut\n=\n0\n(7)\n˙λt + ∇· (λt · ut)\n=\n0\n(8)\nut + K ⋆∇J0\nt λt\n=\n0\n(9)\nJ0\n0\n=\nI0\n(10)\nλ1\n=\nβ(I1 −J0\n1)\n(11)\nThe optimization procedure is a bi-directional information\nﬂow. Given the current control signal ut and initial values\nof J0\n0 = I0, the forward information ﬂow compute J0\nt for\nt ∈[0, 1]. In the backward adjoint ﬂow, we update λt starting\nfrom λ1 = β(I1 −J0\n1) and then ut can be updated by a\ngradient descent using both J0\nt and the adjoint variable λt.\nWe note that the gradient based update of ut here is in fact\nthe same as the updating of ut in the optimization formulation\nto fulﬁll the E-P equation. But the idea of bi-directional adjoint\ncomputation is a new characteristic. This is different from the\ndirect computation of gradient in the optimization formulation.\nThe Lagrangian multiplier based formulation can lead to more\ngeneral strategies for parameter optimization as will be shown\nlater.\nB. Geodesic shooting\nIn LDDMM, both the optimization and optimal control\nformulations aim to ﬁnd a geodesic by ﬁnding a vector ﬁeld\nut satisfying the E-P equation. It’s well known that for a given\nRiemannian manifold, a geodesic is completely determined by\nthe starting point and the initial velocity of the geodesic. So\nif our goal is to ﬁnd a geodesic, then the vector ﬁeld ut as a\ncontrol signal is highly redundant since it can be completely\ndetermined by u0 and the E-P equation.\nGeodesic shooting[21] can ﬁnd the initial vector ﬁeld u0 or\nequivalently the initial momentum Lu0 with the E-P equation\nas an explicit constraint. Obviously here the geodesic shoot-\ning is also formulated as an optimal control problem. The\ncorrespondent optimization procedure is also a bi-directional\ninformation ﬂow. Starting from the initial momentum and\nthe E-P equation, the forward ﬂow updates the vector ﬁeld\nut, the transformation ϕu\nt and the transformed source image\nJ0\nt = I0 ◦ϕu\nt . The backward adjoint ﬂow updates the adjoint\nvariables, the Lagrangian multipliers of the constraints, and\nﬁnally the initial vector ﬁeld u0 can be updated by a gradient\ndescent. For more details of geodesic shooting and the related\nadjoint calculation, please refer to [21][15].\nThe lesson we can draw from geodesic shooting is that,\nwhen the optimal conﬁguration of a subset of the parameters\ncan be determined as a function of all the other parameters, this\nfunction can be regarded as a constraint and the optimization\ncan be simpliﬁed as an optimal problem. Or in another word,\nwhen there exists explicit constraints among parameters, the\noptimization can be simpliﬁed. This may help to design the\nnetwork structure in deep learning systems.\nThis idea can be generalized to the case of a general optimal\ncontrol, where explicit constraints among parameters should\nbe respected. Then the Lagrangian multiplier based variational\nmethod will lead to a similar bi-directional information passing\n4\nalgorithm which can be shown later to be closely related with\ndeep learning systems.\nC. Semiproduct group and metamorphosis\nMetamorphosis is used to modify the original LDDMM or\nthe geodesic shooting to support a second transformation ﬂow\nηt, vt = ϕu\nt ˙ηt, which is used to change the image appearance\nof the template image I0 so that the image transformation is\na composition of both the coordinate transformation and the\nimage appearance transformation. Essentially this is to replace\nthe Lie group G with a semiproduct group[17][22]. That’s\nto say, we are now working with a composite operation of\nmultiple operations.\nUnder the composite transformation, the image is trans-\nformed as ˙J0\nt = vt+ut◦ϕu\nt . This is a new constraint involving\nboth the coordinate and image appearance transformations.\nAccordingly the energy function to be minimized includes\nboth the kinetic energies of ut and vt. In another word, the\nRiemannian manifold of transformations is extended and a\nnew composite metric is deﬁned. But basically the geometric\npicture is similar with the original LDDMM or the geodesic\nshooting framework. An alternative perspective of the meta-\nmorphosis is that the transformation on the image appearance\nηt can be regarded as introducing noise on the image. The\nconstraint on the kinetic energy of vt can be understood as to\nconstrain the power of the noise. For more details of the idea\nof metamorphesis, please refer to [15][22][23].\nD. Summary of diffeomorphic image registration\nImage registration can be formalized by either energy based\noptimization or an optimal control problem. It has a clear\ngeometric picture, where the optimal solution is a geodesic\non the Riemannian manifold on the transformation space.\nThe geodesic is represented by a parameterized model and is\nobtained by a parameter estimation procedure. What’s more,\nthe image registration problem is closely related with the\ngeometric mechanics in that they share lots of geometric\nstructures.\nA complete description of the geometric structure of image\nregistration is given in [16][17][21][22][23][15] and references\ntherein.\nIII. GEOMETRIC PICTURE OF DEEP LEARNING SYSTEMS\nTo show the validness of the geometrization of deep net-\nworks, we now compare the diffeomorphic image registration\nand deep learning systems to build a dictionary between\ncorrespondent concepts in both ﬁelds. Since image registra-\ntion has both a clear geometric and a physical (geometric\nmechanical) picture, we hope the dictionary will give us a\nnew understanding of deep networks from both the geometric\nand physical points of view. Here we directly give a list of the\ncontent of the dictionary with a brief explaination. Interested\nreaders can check the details by themselves.\nA. A dictionary between image registration and deep net-\nworks\n(1)Network structure and G: Geometrically the network\nstructure deﬁnes the space of possible solutions, which is a set\nof curves in the transformation space. Also the network struc-\nture deﬁnes in which way this space is explored as explained in\nthe relational inductive bias[24]. Due to the limited complexity\nof the network and limited allowed operations, the network\nstructure only represents a subset of all possible curves that\ncan reach the target transformation ϕ in image registration or\ng in deep learning from the identical transformation Id. In\nfact the deep network structure deﬁnes the operation group G\nand the network parameters θ falls in its Lie algebra g of G.\nNormal CNNs are just discretisized transformation curves and\nthe norms of network parameters θ along the curve can be\nroughly regarded as the non-uniform discretization step sizes.\n(2)Constraints and Riemannian metric: The network\nstructure only deﬁnes the space of possible solutions. To ﬁnd\nthe optimal solution by solving an optimization problem, we\nneed to introduce constraints on the parameters of the network.\nGeometrically constraints can be regarded as Riemannian\nmetrics on G deﬁned on the manifold of possible solutions\nencoded in the network structure and network parameters.\nCarefully adjusting constraints can change the curvature dis-\ntribution of the solution manifold and generally we prefer to\nwork on a ﬂat manifold so that the optimal solution can be\neasily found.\n(3)Supervised training and landmark registration: Given\nthe parametric description of the manifold of possible solutions\nand the Riemannian metric deﬁned by constraint, supervised\ntraining on a set of N labeled training data estimates the\nparameters ut(θ) to ﬁnd the optimal transformation curve gt to\nreach the desired target transformation. This can be understood\nto achieve a diffeomorphic image registration based on N\npairs of landmarks on the image or to simultaneously match\nN images using the same diffeomorphic transformation.\n(4)Optimal deep networks and geodesics: In LDDMM,\nthe optimal transformation is achieved by a geodesic on G\ndetermined by the E-P equation. In deep networks, an optimal\nnetwork should also exist as a geodesic on the Riemannian\nmanifold deﬁned by the network structure and constraints,\nwhich is the deep network with a minimal complexity. Accord-\ningly, if the E-P equation of deep networks can be explicitly\ndescribed as in LDDMM, then geodesic shooting can also\nbe implemented on the optimization of deep networks. Since\ngeodesic shooting only optimize the initial momentum of\nthe geodesic, the training of an optimal deep network has a\nmuch smaller degree of freedom than a normal non-optimal\ndeep network. Network distillation and network pruning are\nessentially both efforts to ﬁnd the optimal deep networks.\n(5)Back propagation and LDDMM: The back propagation\nbased optimization of deep networks are essentially the same\nas the gradient descent based LDDMM optimization[18].\n(6)Neural ODE and optimal control framework: The\noptimal control based optimization procedure of LDDMM is\nessentially the same as the optimization used in neural ODE\nand other related works[13][14].\n5\n(7)Equilibrium propagation and geodesic shooting: Ben-\ngio’s equilibrium propagation[25] share the same structure as\ngeodesic shooting used in LDDMM framework[21].\n(8)Attention mechanism and semiproduct group: Es-\nsentially attention mechanism is a composition of multiple\ndeep networks. It shares lots of similarity with the semiprod-\nuct group and metamorphoses in LDDMM framework since\nsemiproduct group also plays with composite operations. In the\nsemiproduct group case of LDDMM, the multiple operations\nare coupled and a generalized E-P equation can obtained to\nrepresent the optimal ﬂow. This indicates that theoretically we\nalso have an optimal attention mechanism and the geodesic\nshooting scheme can be applied.\n(9)Generalization and Riemannian curve length: The\ngeneralization capability of deep networks is a key issue of\nthe performance of deep networks. Usually generalization is\ndescribed by a norm based factor, which can be understood\nas the complexity of the network[26]. It has been found that\ndeep networks have the tendency to reduce complexity during\ntraining and a lower complexity means a better generalization\ncapability. In LDDMM, the complexity of the registration\ntransformation is the length of the transformation curve evalu-\nated using the Riemannian metric on G. In deep networks, we\nalso have a correspondent network complexity using a special\nRiemannian metric, the Fisher-Rao metric[26]. In fact this\nmetric is closely related with general relativity[27][28][29],\nwhich is another evidence that deep networks have a deep\nphysical origin. We will give more details on this point in the\ndiscussion section.\n(10) Batch normalization and geodesics: It’s well known\nthat batch normalization can help the convergence of deep\nnetworks. From the geometrical point of view, since CNNs\nare just discretisized transformaition curves and the norms\nof θ are the discretization step sizes, BN can be regarded\nas an operation to adaptively adjust the ratio of thrown-away\ninformation (energy) along the curve. This is because CNNs\nare the same as the entanglement renormalization algorithm,\nwhich extracts global information by iteratively throwing away\nlocal information[2]. By normalizing the data, BN aims to\nkeep a constant speed of throwing away information along the\nnetwork. In entanglement renormalization algorithms, this is\nto throw away a ﬁxed percentage of low amplitude states and\nkeep only those high amplitude states so that the strong global\ninformation patterns are kept. This will result in a transforma-\ntion curve with an isometric-like property, which coincides the\nproperty of geodesics. So geometrically BN can be understood\nas a constraint to force the network to be a geodesic-alike\ncurve. This geometric picture is the same as the conclusion of\n[30]. In [30] the Hessian matrix was introduced, which is in\nfact the Fisher-Rao metric to evaluate the complexity of the\ndeep network or the length of the transformation curve. If the\nnetwork can be constrained by BN to form a geodesic-like\ncurve, then it has the minimal curve length or equivalently\nthe minimal deformation energy as in the geometry of image\nregistration problem. Or BN forces the Fisher-Rao metric to\nvary smoothly along the network. Obviously a transformation\nwith a minimal deformation energy or a smooth curve will\nhave a smooth loss landscape, which coincides with a key\nconclusion of [30].\n(11)Training convergence and curvature: The conver-\ngence of deep networks highly depends on the back propa-\ngation of gradients along deep networks. From the geometric\npicture of LDDMM, we know that this is related with the\ncurvature of the manifold since the curvature determines the\nstability of geodesics. In deep learning ﬁelds, random matrix\nbased analysis[31] shows that when the network has a dynamic\nisometric property, the forward and backward information can\nﬂow freely along the network so that a better convergence\ncan be achieved. In fact, isometry is exactly the property of a\ngeodesic. So the dynamic isometric property of a deep network\nis essentially to say, the network is a geodesic on Euclidean\nmanifold, i.e. a straight line. Similarly, batch normalization is\nessentially to adjust the curvature of the manifold by adjusting\nthe Fisher-Rao metric along the network.\n(12)GAN and current based shape matching: The key\ngoal of GAN is to approximate a distribution density. The main\nchallenge of GAN is to ﬁnd a proper metric to measure the\ndifference of distributions. This is why WGAN emerges as a\nbreak-through since it provides an efﬁcient metric for distribu-\ntions without one-to-one correspondence between samples of\ndistributions. In LDDMM, there is also a way to compare two\nshapes without position correspondence using current based\nshape representation[32]. It will be interesting to ﬁnd if there\nexists a correspondence between WGAN and currents.\n(13)Dropout and stachastic shape evolutions: As a so-\nlution to enhance the robustness of deep networks, dropout\nachieves its goal by adding perturbations on the network,\neither on the operation group G (dropout of neurons) or\non the data space V by adding perturbation layers[33]. In\nLDDMM framework, there are also similar shape registration\nmethods by adding perturbations on either the momentum or\nthe positions of landmarks on shapes[34]. Obviously dropout\naims to ﬁnd a curve that is robust against perturbations on\neither G or V. But how about a perturbation on the Lie algebra\ng? We will also address this issue in the following section.\n(14)ResNet, Lie algebra,curvature and reparameteriza-\ntion of curves: ResNet as the most successful deep network\nstructure shows a superior performance than normal CNNs.\nFrom a geometric point of view, the success of ResNet falls\nin that ResNet is a network running on the Lie algebra g,\nwhile normal CNNs run on G. This can be understood by\ntaking the quantum computation as an analogue of CNNs[10],\nwhere normal CNNs try to construct an quantum algorithm by\ncomposing elementary unitary gates and ResNets achieve the\nsame algorithm by ﬁnding the proper Hamiltonian. It’s well\nknown that ResNet is essentially a differential equation, which\nperfectly matches the structure of LDDMM. For ResNets, the\ncurvature along the network is much smoother than normal\nCNNs since the network parameters of ResNets are only weak\nperturbations and therefore can not lead to rough curvature\nchange along the network. ResNets can also easily achieve\nreparameterization of the curve gt by just adjusting the ampli-\ntudes of the weights. In another word, compared with normal\nCNNs, ResNets run on a much smoother manifold and can\napproach a smooth geodesic much easier than normal CNNs.\n(15) Geometric structure of deep networks and Rie-\n6\nmannian structure on V : In deep learning ﬁelds, there\nare also works to explore the geometric structure of deep\nnetworks[35][36]. These works are closely related with the\ngeometrization of deep networks. But both of them are work-\ning on the Riemannian geometry on V as described in [17]\ninstead of on the Riemannian geometry on G. Since the\ngeometry on V is induced by a projection from the geometry\nof G, a complete geometrization of deep networks should be\naccomplished on G and only the geometry of G can fully\nexplore the dynamics of deep networks.\nThis is only a partial list of the correspondence between the\ngeometry of image registration and deep learning systems. We\nhope we have convinced readers to believe the geometrization\nof deep networks is a promising candidate for the interpreta-\ntion of deep learning systems.\nB. A concrete example\nNow we will give a concrete sample on how we can\nunderstand deep learning using the geometrization framework.\nIn [37] the problem of how the training data will inﬂuence the\nprediction of a deep network was addressed. They considered\na supervised training with n data points zi = xi, yi, i = 1....n\nand the cost function is L(z, θ) 1\nnΣn\ni=1L(zi, θ) with θ as the\nnetwork parameters. The optimal network conﬁguration is\ngiven by\nˆ\ntheta = arg min mintheta L(z, θ).\nThe key results of [37] are two items to evaluate how the\nperturbation on the training data will inﬂuence the parameter\nˆ\ntheta and the loss at a test point ztest given by\nIup,params(z) = −H−1\nˆθ ∇θL(z, ˆθ)\n(12)\nIup,loss(z, ztest) = −∇θL(ztest, ˆθ)H−1\nˆθ ∇θL(z, ˆθ)\n(13)\nwhere Hˆθ = 1\nn\nPn\ni=1 ∇2\nˆθL(zi, ˆθ) is the Hessian.\nTo interpret the results using our geometrization framework,\nwe can regard the supervised network training as an optimiza-\ntion problem following the formulation of image registration\nwith a cost function\nE(ut) =\nZ 1\n0\nl(θ)dt + L(z, θ)\n(14)\nwhere l(θ)\n=\n⟨θI(θ)θ⟩, I(θ)\n=\nPn\ni=1[∇θL(zi, θ) ⊗\n∇θL(zi, θ)] is the Fisher-Rao metric used in [26] to describe\ncomplexity of deep networks. Obviously the Fisher-Rao metric\nis essentially the same as the Hessian Hˆθ since I(θ) = −Hˆθ\nfrom information geometry.\nThis can be understood as either to match n pairs of\nimages simultaneously using diffeomorphic transformations on\na higher dimensional space (due to the overparameterization\nof deep networks) or a landmark based image registration\ntaking all the training data as paired landmarks on an image.\nThe goal of the optimization is to ﬁnd a proper transforma-\ntion that can match the training data and also show good\ngeneralization performance. The Riemannian metric on the\nRiemannian manifold to measure the deformation energy of\nthe transformation or the curve length or equivalently the\ncomplexity of the deep network is the Fisher-Rao metric of\nthe deep network. From a physical point of view, it’s obvious\nto see that why the Fisher-Rao norm is used in [26] to\nrepresent the generalization capability. This is because a lower\ncomplexity network means a lower deformation energy and\ntherefore a smoother image deformation ﬁeld. Of course for a\nlandmark based image registration, a smooth deformation will\nhave a better generalization performance.\nComparing (14) with (2)(3)(4), we can observe that\n∇θL(z, ˆθ) in (12) is exactly the momentum in g∗of E-P\nequation and (12) is the correspondent vector in g. 13 is related\nwith the angle between two vectors in g using the Fisher-Rao\nmetric. We can easily draw a clear physical or a geometric\npicture of (12)(13). If the deep network is a geodesic under\nthe Fisher-Rao metric, then roughly (12) indicates how the\ndirection of the geodesic will be shifted with a perturbation\nof a landmark. (13) shows under a perturbation of a training\nlandmark, how the direction of the trajectory of a test data\nztest transformed by the perturbed geodesic will be shifted.\nOf course generally deep networks are not geodesics. Then\nthe networks in [37] are just normal non-geodesic curves. But\nstill the above geometric picture holds approximately.\nThe drawback of [37] is that it only consider a perturbation\naround the current conﬁguration ˆˆθ so that the Fisher-Rao\nmetric is ﬁxed by the network conﬁguration. Another more\ninteresting work [38] tried to explore the complete dynamics\nof the Fisher-Rao metric by iteratively updating the weighting\nof training data and the network conﬁguration ˆθ. Similar to\n[37] this can be formulated as an image registration problem\ngive by\nE(ut) =\nZ 1\n0\nl( ˆ\nθ(ǫ))dt + L(z, θ, )\n(15)\nwith ǫi, i = 1, ..., n are the weights of the training data and\nl( ˆ\nθ(ǫ)) are deﬁned on the Fisher-Rao metric determined by\nthe optimal network parameters\nˆ\nθ(ǫ) which are ǫ dependent.\nWhat’s new here? The main difference with the image\nregistration problem is that the Fisher-Rao metric on G is\nnow data dependent! In another word, the Riemannian metric\nis not a ﬁxed background metric as in the image registration\nproblem, instead the metric is emergent from the deep network\nitself. Readers with a physics background can immediately\nsee that we have an analogue of this in physics. The data\nindependent image registration is the Neutonian mechanics\nwith a ﬁxed spacetime background and the data dependent\ndeep network systems correspond to general relativity with a\ndynamic spacetime. What’s more, the Fisher-Rao metric used\nhere is in fact closely related with general relativity since\ngravitation equation can be derived from it[27][28][29]. So in\n(15) the network structure and data (information) are coupled\njust as spacetime and matter are coupled in general relativity.\nFollowing John Wheeler, in our physical world, spacetime tells\nmatter how to move, matter tells spacetime how to curve. In\ndeep networks, network tells data (information) how to move,\ndata (information) tells network how to curve. We believe this\nis not just an analogue between the physical world and deep\nnetworks, this should be regarded as a general principle to\ndesign and understand deep networks. The key component\nof interpret deep networks is to understand how the network\n7\nstructure and data information interact. That’s to say to ﬁnd\nthe gravitation equation for deep networks. Here we point out,\nsince the Fisher-Rao metric is data dependent, the optimal\nsolution can not be written as the E-P equation with a ﬁxed\nRiemannian metric any more since the metric is also dynamic.\nIn [38], the solution is approximated by a two-level gradient\ndescent algorithm which updates the network parameter ˆθ and\nsample weights ǫ iteratively. The ﬁnal solution is a critical\npoint that ˆθ is stable with respect to the perturbation of ǫ. At\nthe critical point, the solution still satisﬁes the E-P equation\nwith a Fisher-Rao metric determined by the network parameter\nˆθ.\nAs a conclusion of this section, the geometrization frame-\nwork tells us that (1)The Fisher-Rao based network complexity\nmeasure is an effective signature for the generalization prop-\nerty for deep networks since it’s a measure for the network\ncomplexity; (2)The network structure and data information\nare coupled just as matter and spacetime are coupled in\ngeneral relativity and the ultimate law of deep networks is\na gravitational equation of deep networks; (3)The optimal\nsolution is a result of the competition between the two terms,\nthe network complexity and training error, in (15).\nIV. DISCUSSION\nTill now we have seen the validness of the geometrization\nframework on the interpretability of deep learning systems by\nshowing that deep networks can correspond to geometrical\nmechanics and general relativity. The basic idea of geometriza-\ntion is that deep networks have correspondence in the physical\nworld. Therefore we can regard deep networks as physical\nsystems and ask the following questions:\n(1) Is there a GUT (grand uniﬁed theory) of deep networks?\nIf there is a correspondence between deep networks and\nour physical world, then the ultimate interpretability of deep\nnetworks lies in ﬁnding the GUT of deep networks just as the\ninterpretability of the physical world lies in the GUT of the\nphysical world. It’s a common sense that the physical GUT is\ndeﬁnitely a geometrical theory. So we believe geometrization\nshould be the right roadmap for the interpretability problem\nof deep learning systems. Also the GUT of deep networks\nshould have the same structure as the GUT of our physical\nworld. Therefore even the GUT of our physical world is\nnot available yet, exploring the similarity between physical\nsystems and deep networks can provide guidelines for us to\nbetter understand deep networks.\n(2) Real physical systems obey a least action principle,\nis this also true for deep networks? We have seen that the\ngeometry of image registration results in an optimal solution\ngiven by the E-P equation. But for deep networks, generally we\nare working with systems far from optimal. But still usually\nthese non-optimal systems work well in practice. There are\nalso works taking deep networks as general dynamic systems\nsuch as in neural ODE[13]. Shall we investigate non-optimal\ndeep networks as general information processing systems or\nshould we stick to optimal deep networks since they are more\nphysical? Our geometrization framework will deﬁnitely work\nbetter on the optimal deep networks. But non-optimal systems\nmight not be properly geometrized. So maybe we should\nﬁrst focus on understanding the optimal systems with clear\ngeometric pictures.\n(3) What can we learn from the geometrization of physics?\nTill now we are only working with Riemannian structures as in\nthe geometry of image registration. In fact the geometrization\nof physics is far beyond Riemannian structures. A natural\nextension is the ﬁbre bundle structure which plays a key\nrole in gauge theory. Can we describe deep networks using\nﬁbre bundles? Good candidates in deep networks that may be\ndescribed by ﬁbre bundles are transfer learning, meta learning,\nneural Turing machines (NTM) and differentiable neural com-\nputers(DNC). They all aim to ﬁnd some kind of reconﬁgurable\nsystems. In the language of Riemannian geometry, this usually\nmeans to reconﬁgure the metric of the system so that the\noptimal geodesic curves can be reconﬁgurable. A natural way\nto achieve this is to reconﬁgure the connection form on ﬁbre\nbundles. Roughly transfer learning can be understood as to\ntransfer (part of) a geodesic to another task. Meta learning\naims to ﬁnd some universal descriptions of different but similar\ngeodesics. NTM and DNC mainly achieve the reﬁguration of\nsystems by carefully changing their memories. We can see the\nmemory can be understood as the ﬁbre bundle above the base\nspace of the LSTM states. It’s interesting to check if NTM\nand DNC can be written as deﬁning a connection on their\nﬁbre bundles.\nAnother possibility is that ﬁbre bundles can be used to\ndescribe the coupling of multiple deep networks. It’s get-\nting more and more obvious that complex tasks can only\nbe achieved by coupling multiple deep networks just as in\nour human brains. In AI systems, typical coupled composite\nsystems are GANs and attention. The coupling of systems\nleads to interactions between subsystems, just as interactions\n(forces) between physical systems. In physics, interactions are\ndescribed by ﬁbre bundles. Accordingly interactions between\ncoupled deep networks should also be described by ﬁbre\nbundles.\nThe last but not the least, the coupling of multiple deep\nnetworks might be related with the existence of consciousness.\nWe hypothesize that when multiple neural networks in our\nbrains are coupled, the coupling may be achieved by an inde-\npendent coupling system, which not only couples the multiple\nneural subsystems but also has its own latent state space and\na stable dynamics. This independent coupling system may be\nthe origin of our consciousness. If this is the case, then can\nour consciousness also be geometrized?\n(4) How to geometrize reinforcement (RF) learning sys-\ntems? Geometrically RF is essentially to learn the metric\nof G from the interaction with the system and then ﬁnd\ngeodesics using the learned metric. Imitation learning can\nbe understood to design a metric so that the expert’s action\nbecomes a geodesic. Can we formulate a geometrization of\nthese procedures?\n(5) What’s the curvature of the emergent Fisher-Rao metric\nin deep networks? If deep networks can be formulated as a\ndynamic system using emergent Fisher-Rao metric, we need\nto check what’s the curvature of this metric. Because the\ncurvature will determine the stability of the geodesic. And the\n8\nmetric is dependent on both the structure and the parameters of\nthe network. Just as in general relativity, the solution spacetime\ncan have either positive or negative curvature, deep networks\nmay have the same problem. Taking CNN as an example,\nin quantum information ﬁeld the correspondent system is\nMERA or the entanglement renormalization algorithm which\nshow a similar structure as CNN. We know that MERA\nbuilds a negative curvature geometry and is related with the\nfamous AdS/CFT duality. Similarly the geometry of quantum\ncomputation, which is another analogues of image registration\nand CNNs also has an almost negative curvature[19], where\nthe Riemannian metric used here is static just as in image\nregistration. It’s reasonable to guess that CNN may also have a\nsimilar negative curvature. This might be an explaination of the\nexistence of adversarial examples in CNN based classiﬁcation\nnetworks.\n(6) How to understand the overparameterization of deep\nnetworks? Overparameterization plays a key role in nowadays\ndeep networks. It’s closely related with the training conver-\ngence, generalization and adversarial attacks. Geometrically\nthis means to choose a higher dimensional group G to accom-\nplish the transformation. In physics we also meet overparam-\neterization problems. For example, in quantum computation\noverparameterization means to achieve a quantum algorithm\nusing auxiliary qubits[19]. In tensor network representation of\nquantum states, overparameterization is closely related with\nthe concepts of parent and uncle Hamiltonians[39]. Overpa-\nrameterization not only brings a higher dimensional G but\nalso a potentially more ﬂexible network structure. As we\nsee above, the structure of deep networks will inﬂuence the\ncurvature of the geometry built by the Fisher-Rao metric of\nnetworks. A complete understanding of the consequence of\noverparameterization is still needed.\nV. CONCLUSIONS\nIn this work, inspired by the geometrization of physics, we\nproposed a geometrization framework for the interpretabil-\nity of deep learning systems. By comparing the geometry\nof image registration with deep networks, we showed that\ngeometrization does bring us new pictures of deep networks.\nUnder this framework, we also discussed some key problems\nfor the understanding of deep learning systems. Our future\nwork will be then to answer these questions.\nAs a ﬁnal remark, besides the geometrization of physics to\nconnect physics and geometry, currently there is a trend to\nunderstand physical laws from the computation point of view\nso that computational complexity starts to play a key role in\nphysics. If we further bring deep networks into this game, we\nhope the interactions among physics, geometry, computation\nand deep networks may completely change our understanding\nof the world. A possible picture of our world may be: The\nworld is an information processing (computation) system\nthat generates our universe by a deep network of basic\ncomputational operators. The structure of the deep network\nis determined by the information structure of our universe.\nThat’s to say the deep network is the optimal network to gen-\nerate the information pattern of our universe, i.e. a geodesic\naccording to a certain Riemannian metric to measure the\ncomputational complexity. Physical laws are encoded in\nthe correspondence between the geometric structure of the\nnetwork and the information pattern of our universe. So still\nour world obeys a least action principle with the action is\ngiven by the computational complexity of the physical world.\nREFERENCES\n[1] X. Gao and L. M. Duan. Efﬁcient representation of quantum many-body\nstates with deep neural networks. Nature Communications, 8(1):662,\n2017.\n[2] Glen Evenbly.\nAlgorithms\nfor tensor network\nrenormalization.\nPhys.rev.b, 95(4), 2017.\n[3] Cdric\nBny.\nDeep\nlearning\nand\nthe\nrenormalization\ngroup.\narxiv:1301.3124, 2013.\n[4] G. Evenbly and G. Vidal. Tensor network states and geometry. Journal\nof Statistical Physics, 145(4):891–918, 2011.\n[5] Brian Swingle. Entanglement renormalization and holography. Physical\nReview D Particles and Fields, 86(6):–, 2009.\n[6] Patrick Hayden, Sepehr Nezami, Xiao Liang Qi, Nathaniel Thomas,\nMichael Walter, and Zhao Yang.\nHolographic duality from random\ntensor networks. Journal of High Energy Physics, 2016(11):9, 2016.\n[7] Brian Swingle. Constructing holographic spacetimes using entanglement\nrenormalization. Physics, 2012.\n[8] Xiao Liang Qi. Exact holographic mapping and emergent space-time\ngeometry. Physics, arXiv:1309.6282v1, 2013.\n[9] Wen Cong Gan and Fu Wen Shu.\nHolography as deep learning.\nInternational Journal of Modern Physics D, 26:1743020, 2017.\n[10] J.S. Wu X. Dong and L. Zhou. How deep learning works –the geometry\nof deep learning. arXiv:1710.10784, 2017.\n[11] M. Gu M. A. Nielsen, M. R. Dowling and A. C. Doherty. Quantum\ncomputation as geometry. Science 311,1133, 2006.\n[12] H.\nHeydari.\nGeometric\nformulation\nof\nquantum\nmechanics.\narXiv:1503.00238, 2015.\n[13] Qi Chen Tian, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud.\nNeural ordinary differential equations. arxiv:1806.07366, 2018.\n[14] E Weinan, Jiequn Han, and Qianxiao Li. A mean-ﬁeld optimal control\nformulation of deep learning. arxiv:1807.01083v1, 2018.\n[15] Laurent Younes. Shapes and Diffeomorphisms. 2010.\n[16] M. Bruveris, F. Gay-Balmaz, D. D. Holm, and T. S. Ratiu.\nThe\nmomentum map representation of images. Journal of Nonlinear Science,\n21(1):115–150, 2011.\n[17] Martins Bruveris and Darryl D. Holm. Geometry of image registration:\nThe diffeomorphism group and momentum maps.\nFields Institute\nCommunications, 73:19–56, 2013.\n[18] Mirza Faisal Beg, Michael I. Miller, Alain Trouve, and Laurent Younes.\nComputing large deformation metric mappings via geodesic ﬂows. 2004.\n[19] M. R. Dowling and M. A. Nielsen.\nThe geometry of quantum\ncomputation. Quantum Information and Computation, 8(10):861–899,\n2008.\n[20] G. L. Hart, C. Zach, and M. Niethammer. An optimal control approach\nfor deformable registration. In IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition Workshops, 2013.\n[21] Vialard, FranoisXavier, Risser, Laurent, Rueckert, Daniel, Cotter, and\nJ Colin.\nDiffeomorphic 3d image registration via geodesic shooting\nusing an efﬁcient adjoint calculation. International Journal of Computer\nVision, 97(2):229–241, 2012.\n[22] Darryl D. Holm, Alain Trouve, and Laurent Younes. The euler-poincare\ntheory of metamorphosis. Quarterly of Applied Mathematics, 67(4):661–\n685, 2008.\n[23] Darryl D. Holm, Tanya Schmah, and Cristina Stoica.\nGeometric\nmechanics and symmetry. Oxford University Press Oxford, (2):xvi+515,\n2009.\n[24] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-\nGonzalez, Vin´ıcius Flores Zambaldi, Mateusz Malinowski, Andrea Tac-\nchetti, David Raposo, Adam Santoro, Ryan Faulkner, C¸ aglar G¨ulc¸ehre,\nFrancis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish\nVaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer,\nNicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick,\nOriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases,\ndeep learning, and graph networks. arxiv:1806.01261, 2018.\n[25] Benjamin Scellier and Yoshua Bengio.\nEquilibrium propagation:\nBridging the gap between energy-based models and backpropagation.\nFrontiers in Computational Neuroscience, 11:24–, 2017.\n9\n[26] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James\nStokes. Fisher-rao metric, geometry, and complexity of neural networks.\narxiv:1711.01530, 2017.\n[27] Hiroaki Matsueda. Emergent general relativity from ﬁsher information\nmetric. arXiv:1310.1831v2, 2013.\n[28] Hiroaki Matsueda.\nDerivation of gravitational ﬁeld equation from\nentanglement entropy. arXiv:1408.5589v2, 70, 2014.\n[29] Hiroaki Matsueda. Geodesic distance in ﬁsher information space and\nholographic entropy formula. arXiv:1408.6633v1, 2014.\n[30] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander\nMadry. How does batch normalization help optimization? In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems 31,\npages 2488–2498. Curran Associates, Inc., 2018.\n[31] Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoen-\nholz, and Jeffrey Pennington.\nDynamical isometry and a mean ﬁeld\ntheory of cnns: How to train 10,000-layer vanilla convolutional neural\nnetworks. arxiv:1806.05393, 2018.\n[32] Stanley Durrleman, Xavier Pennec, Alain Trouv, and Nicholas Ayache.\nStatistical models of sets of curves and surfaces based on currents.\nMedical Image Analysis, 13(5):793–808, 2009.\n[33] Zhonghui You, Jinmian Ye, Kunming Li, Zenglin Xu, and Ping Wang.\nAdversarial noise layer: Regularize neural network by adding noise.\n2018.\n[34] Alain Trouv and Franoisxavier Vialard. Shape splines and stochastic\nshape evolutions: A second order point of view. Quarterly of Applied\nMathematics, 70(2):219–251, 2012.\n[35] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein,\nand Surya Ganguli. Exponential expressivity in deep neural networks\nthrough transient chaos. In D. D. Lee, M. Sugiyama, U. V. Luxburg,\nI. Guyon, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems 29, pages 3360–3368. 2016.\n[36] Michael Hauser and Asok Ray. Principles of riemannian geometry in\nneural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30, pages 2807–2816. 2017.\n[37] Pang Wei Koh and Percy Liang. Understanding black-box predictions\nvia inﬂuence functions. In Doina Precup and Yee Whye Teh, editors,\nProceedings of the 34th International Conference on Machine Learning,\nvolume 70 of Proceedings of Machine Learning Research, pages 1885–\n1894, International Convention Centre, Sydney, Australia, 06–11 Aug\n2017. PMLR.\n[38] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning\nto reweight examples for robust deep learning. In Jennifer Dy and An-\ndreas Krause, editors, Proceedings of the 35th International Conference\non Machine Learning, volume 80 of Proceedings of Machine Learning\nResearch, pages 4334–4343, Stockholmsmssan, Stockholm Sweden, 10–\n15 Jul 2018. PMLR.\n[39] C. Fernndez-Gonzlez, N. Schuch, M. M. Wolf, J. I. Cirac, and D. Prez-\nGarca. Frustration free gapless hamiltonians for matrix product states.\nCommunications in Mathematical Physics, 333(1):299–333, 2015.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-01-06",
  "updated": "2019-01-13"
}