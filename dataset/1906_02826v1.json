{
  "id": "http://arxiv.org/abs/1906.02826v1",
  "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier Parameter Estimation",
  "authors": [
    "Yu Liu",
    "Li Deng",
    "Jianshu Chen",
    "Chang Wen Chen"
  ],
  "abstract": "Many important classification problems, such as object classification, speech\nrecognition, and machine translation, have been tackled by the supervised\nlearning paradigm in the past, where training corpora of parallel input-output\npairs are required with high cost. To remove the need for the parallel training\ncorpora has practical significance for real-world applications, and it is one\nof the main goals of unsupervised learning. Recently, encouraging progress in\nunsupervised learning for solving such classification problems has been made\nand the nature of the challenges has been clarified. In this article, we review\nthis progress and disseminate a class of promising new methods to facilitate\nunderstanding the methods for machine learning researchers. In particular, we\nemphasize the key information that enables the success of unsupervised learning\n- the sequential statistics as the distributional prior in the labels.\nExploitation of such sequential statistics makes it possible to estimate\nparameters of classifiers without the need of paired input-output data.\n  In this paper, we first introduce the concept of Caesar Cipher and its\ndecryption, which motivated the construction of the novel loss function for\nunsupervised learning we use throughout the paper. Then we use a simple but\nrepresentative binary classification task as an example to derive and describe\nthe unsupervised learning algorithm in a step-by-step, easy-to-understand\nfashion. We include two cases, one with Bigram language model as the sequential\nstatistics for use in unsupervised parameter estimation, and another with a\nsimpler Unigram language model. For both cases, detailed derivation steps for\nthe learning algorithm are included. Further, a summary table compares\ncomputational steps of the two cases in executing the unsupervised learning\nalgorithm for learning binary classifiers.",
  "text": "From Caesar Cipher to Unsupervised Learning:\nA New Method for Classiﬁer Parameter Estimation\nYu Liu ∗\nAI Research\nCitadel LLC\nLi Deng\nAI Research\nCitadel LLC\nJianshu Chen\nAI Lab\nTencent\nChang Wen Chen\nComputer Science and Engineering\nUniversity at Buffalo, SUNY\nAbstract\nMany important classiﬁcation problems, such as object classiﬁcation, speech recog-\nnition, and machine translation, have been tackled by the supervised learning\nparadigm in the past, where training corpora of parallel input-output pairs are\nrequired with high cost. To remove the need for the parallel training corpora has\npractical signiﬁcance for real-world applications, and it is one of the main goals of\nunsupervised learning aimed for pattern classiﬁcation with label-free data sources\nfor training. Recently, encouraging progress in unsupervised learning for solving\nsuch classiﬁcation problems has been made and the nature of the challenges has\nbeen clariﬁed. In this article, we review this progress and disseminate a class\nof promising new methods to facilitate understanding the methods for machine\nlearning researchers. In particular, we emphasize the key information that enables\nthe success of unsupervised learning — the sequential statistics as the distributional\nprior in the labels as represented by language models. Exploitation of such sequen-\ntial statistics makes it possible to learn parameters of classiﬁers without the need to\npair input data and output labels. The idea of exploiting sequential statistics for our\nunsupervised learning is inspired by an ancient encryption technique called Caesar\nCipher.\nIn this paper, we ﬁrst introduce the concept of Caesar Cipher and its decryption,\nwhich motivated the construction of the novel loss function for unsupervised learn-\ning we use throughout the paper. The loss function serves as the basis for a novel\nlearning algorithm that forms the core content of this paper. Then we use a simple\nbut representative binary classiﬁcation task as an example to derive and describe\nthe unsupervised learning algorithm in a step-by-step, easy-to-understand fashion.\nWe include two cases, one with Bigram language model as the sequential statistics\nfor use in unsupervised parameter estimation, and another with a simpler Unigram\nlanguage model. For both cases, detailed derivation steps for the learning algorithm\nare included in the paper. Further, a summary table of computational steps in\nexecuting the unsupervised learning algorithm for learning binary classiﬁers is pro-\nvided. In the summary, we also include a comparison between the computational\nsteps for the use of Unigran and Bigram language models, respectively.\n1\nIntroduction and Background\nThe long-standing success of supervised learning, especially with the use of deep neural networks on\nbig data since 2010 or so, has brought to machine learning practitioners a tremendous opportunity to\nsolve a wide range of challenging problems in pattern recognition and artiﬁcial intelligence (LeCun\n∗Yu Liu and Li Deng are with AI Research of Citadel LLC, Seattle, WA. Jianshu Chen is with Tencent AI\nLab, Belleve, WA. And Chang Wen Chen is with Department of Computer Science and Engineering, State\nUniversity of New York at Buffalo. The research reported in this paper is based on the Ph.D. thesis work of the\nﬁrst author and on the research carried out at Microsoft Research while he was a research intern there.\n1\narXiv:1906.02826v1  [cs.LG]  6 Jun 2019\net al., 2015; Bahdanau et al., 2015; Deng & Li, 2013; Hinton et al., 2012; Abdel-Hamid et al., 2014).\nClassiﬁcation or input-to-output prediction is one of the most fundamental problems in machine\nlearning, and it has typically been formulated in the setting of supervised learning, where the learning\nalgorithms require both input data and the corresponding output labels in pairs (i.e. parallel data) to\nlearn the input-to-output prediction with the model.\nHowever, the acquisition of output labels to pair with the input data for supervised learning is often\nvery expensive, especially for large-scale deep-learning systems that generally require big data for\nthe training. It is not uncommon that thousands of hours of human labor are devoted to manually\nlabel a considerable size of a dataset for a speciﬁc task before the training can even start to take\nplace. For example, over ten million images had been hand-annotated to build the ImageNet dataset\nfor the image classiﬁcation task (ImageNet, 2010). Each image was annotated with reasonably\nunambiguous tags to indicate what notable objects are included. For large-scale speech recognition,\neven greater amounts of annotated data are required to achieve the desired high accuracy (Yu & Deng,\n2015; Deng et al., 2013). Collecting such labeled datasets incurs high cost in human labor and thus\nprevents the modern deep learning systems based on the current supervised-learning paradigm from\nscaling out to even greater data in training. It is thus highly desirable to invent methods that would\nenable the training of large-scale classiﬁcation models without using labels that match the data on the\nsample-by-sample basis.\nThis calls for Unsupervised learning in the context of predicting output labels from input data.\nWhile unsupervised learning has been studied for decades in the literature, none of common existing\nunsupervised learning methods can handle the task described above, For example, the most common\nunsupervised learning methods of clustering analysis include K-means and spectral clustering (Ng\net al., 2002). They learn to exploit the structure of input data and extracts grouping patterns by\nexploring the inter-relation between data samples. Clustering analysis alone cannot address the\nproblem of predicting output labels from input data.\nThere are many classes of unsupervised learning methods, i.e. learning without labels, developed in\nrecent years. In one class, structure of the data in terms of density functions is modeled (no labels\nneeded), either explicitly (van den Oord et al., 2016a,b) or implicitly (Goodfellow et al., 2014), giving\na generative model of the data when one can draw samples and enabling one to examine what the\nmodel has learned or otherwise (Eslami et al., 2018; Jakab et al., 2018; Gupta et al., 2018). The\ngenerative models can also be used to imagine possible scenarios for model-based reinforcement\nlearning and control.\nIn another type, the structure or inherent representations of the data are modeled not by deriving its\ndensity functions but by exploiting and predicting the temporal sequence (future and history) of the\ndata (Finn et al., 2016; Mikolov et al., 2013). A related class of unsupervised representation learning\nis to model the hierarchical internal representations of input data not via sequence prediction but\nthrough modeling latent representations including graph representations (Vincent et al., 2010; Deng\net al., 2010; Kingma & Welling, 2013; Yang et al., 2018a).\nThe next class of unsupervised learning methods targets the task of classiﬁcation by exploiting the\nstructured bidirectional mapping between input and output, both of the same modality such as images\nor texts. These methods take advantage of the similarity of information rates in input and output,\nenabling both forward and inverse mappings and accomplishing impressive tasks of image-to-image\ntranslation (Mejjati et al., 2018; Kazemi et al., 2018; Zhu et al., 2017) and of language-to-language\ntranslation (Artetxe et al., 2017; Lample et al., 2017; Yang et al., 2018b). However, when the input\nand output are of substantially different information rate or of different modalities — e.g. mapping\nfrom image to text or from speech to text — these methods would not work because the forward and\ninverse mappings can no longer take the same functional form as required by these methods.\nRelated to the above class of methods but without constraining input and output to be of the same\nmodality, another popular class of unsupervised learning for classiﬁcation is to perform pre-training\nwithout labels (Ramachandran et al., 2017). Subsequently ﬁne-tuning the pre-trained parameters is\ncarried out with limited labels. This class of methods has accounted for the early success of deep\nlearning in speech recognition (Yu et al., 2010; Dahl et al., 2012; Deng & Yu, 2014; Yu & Deng,\n2015). Very recently since October of 2018, they have also shown large success in natural language\nprocessing (Devlin et al., 2018; OpenAI, 2019).\n2\nHowever, to solve classiﬁcation problems helped by the stage of unsupervised pre-training still\nrequires labeled data in the ﬁne-tuning stage. When labeled data is not so costly, this problem is not\nvery serious. To be completely label free, it is desirable to carry out end-to-end learning via direct\noptimization. A class of new methods with this motivation have been developed in recent years, and\nare the focus of this paper.\nWe in this paper introduce a most interesting class of unsupervised learning methods aimed to address\nthe problem of classifying output labels (typically of low information rate) from input data (typically\nof high information rate) in parametric models via direct optimization, where, unlike all previous\nmethods, the training of the model parameters does not require explicit labels for each training sample.\nThe essence of the methods is a novel objective function for the training, together with an uncommon\ntechnique of optimization, called stochastic primal-dual gradient (SPDG) (Boyd & Vandenberghe,\n2004; Chen et al., 2016). These methods have recently been developed and shown to be effective\nin an unsupervised optical character recognition task (Liu et al., 2017). Since the material in (Liu\net al., 2017) is general and dense in mathematical treatment and written mainly for machine learning\nexperts, this article is aimed to disseminate detail of the unsupervised learning methods and to serve\nas a tutorial for signal processing readers who are less experienced in machine learning. The article\nwill ﬁrst intuitively introduce a speciﬁc method, starting from an inspiration of decryption of Caesar\nCipher for motivating the construction of the objective function. We will then formulate the problem\nby connecting this Caesar Cipher problem to a simple binary classiﬁcation problem whose model\ntraining would not require data-label pairs. The deciphering procedure in Caesar Cipher contains\nthree critical steps, which can be closely linked to the analogous three steps in the SPDG optimization\nmethod.\nThe remaining content of this article is organized as follows. Section 2 introduces the procedure\nof decrypting Caesar Cipher, which consists of three critical steps. Next, in Section 3, we connect\nthe binary classiﬁcation problem to this Caesar Cipher problem, and formulate the problem as an\noptimization problem for a novel objective function that leverages a prior distribution of labels or\na “(unigram) language” model. In Section 4, We show that this objective function for unsupervised\nlearning is intractable with common stochastic gradient descent (SGD) techniques. Thus the op-\ntimization problem is transformed to an mathematically equivalent, saddle-point, problem. The\nsaddle-point problem can be approached by a variant method of SPDG, called primal-dual method.\nIn Section 5, we extend the formulation and optimization of our unsupervised learning problem from\nthe use of simple unigram language model to a more complex bigram language model. In Section 6,\nwe show our experimental results, where we create a synthetic dataset and apply the unsupervised\nlearning method with the use of unigram and bigram language models, respectively. We conclude\nand summarize the article in Section 7.\n2\nDecryption of Caesar Cipher — A Motivating Example of Unsupervised\nLearning\n2.1\nCaesar Cipher\nThe Caesar cipher, also known as shift cipher, is one of the simplest forms of encryption. It has been\nﬁrst used by Julius Caesar, the well-known Roman general who led to the rise of the Roman Empire.\nAnd the encryption technique was named after him. Caesar Cipher is a substitution cipher where\neach letter in the plain text, or the original message, is replaced with another letter corresponding to a\ncertain number of letters up or down in the alphabet. For example, in Figure 1, the message in the\nplain text is “THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG”. With a shift of 3 letters\nup, the encrypted message becomes “QEB NRFZH YOLTK CLU GRJMP LSBO IXWV ALD”.\n2.2\nDecryption of Caesar Cipher\nTo decrypt Caesar Cipher with unknown shift, we need three steps: 1) acquiring standard English\nstatistics as a “prior”; 2) calculating the same statistics of the encrypted message, and 3) matching the\ntwo statistics with a trial-and-error scheme to recover the shift. Here we assume that the encrypted\nmessage is large enough that the number of letters has statistical signiﬁcance. The process is illustrated\nin Figure 2 and explained in details as follows Cipher (2009):\n3\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nA\nB\nC\nD\nE\nF\nG\nX\nY\nZ\n...\n...\nPlaintext: THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\nCiphertext: QEB NRFZH YOLTK CLU GRJMP LSBO QEB IXWV ALD\nFigure 1: Illustration of Caesar Cipher with shift-3 up\nStep 1: Prior standard English statistics To discover the correspondence between the encrypted\nletters and standard English letters, we ﬁrst need to know precisely how the encrypted letters look\nlike in the standard alphabet. To this end, for example, one can take a large bulk of standard English\ncorpus (e.g. newspapers) and calculate the frequency or histogram of each of 26 English letters, as\nshown in lower part of the Figure 2.\nStep 2: Encrypted message statistics Similarly, we can determine the same statistics on our en-\ncrypted original message. This can be accomplished by counting the frequency of each letters in the\nencrypted message. We may give the histogram that looks like the one on the upper part of Figure 2\n(after a shift).\nStep 3: Matching the two statistics With the two histogram charts side by side, one can easily\ntry out in a brute-force way all of 25 shifts and ﬁnd the best match, as shown in Figure 2. After\ndetermining the shift, one can obtain the shifted alphabet and recover the original message.\nFigure 2: Letter frequency analysis and match between standard English and original message.\n[Screenshot from Caesar Cipher’s online tool (Cipher, 2009)]\n3\nFormulation of Unsupervised Learning\nThe above motivating example of deciphering the original messages (input) from the encrypted\nmessage by Caesar Cipher (input) highlighted the importance of exploiting the prior output statistics\nof English letters. Note that the three-step procedure from input to output described in Section II does\nnot require the kind of supervised learning with costly input-output pairs in the training set. We now\nformulate the unsupervised learning problem without such a requirement as well.\n4\n3.1\nFormulation of the Unsupervised Learning Problem\nOur unsupervised learning problem here is formulated in the simplest setting of binary classiﬁcation\nwhere no input-output pairs are needed in the classiﬁer training. The input X = (x1, . . . , xT) is a\nsequence of 2-dimensional vectors, where xt = (xa\nt , xb\nt) ∈R2×1 is a 2-dimensional vector. The\nground-truth labels Y = (y1, · · · yT ) are the corresponding sequence, where each yt denotes the class\nthat xt corresponds to. Note that the ground-truth labels are not used in the training. Here, analog to\nCaesar Cipher, the input sequence X acts as the encrypted message, and Y is the original message to\nrecover.\nImportantly, as any original English message is subject to English language rules, we also ask that\nyt be subject to a given “rule”. In this context, we call this rule as “language model” in the form\nof N-gram, i.e. the statistics of yt ∼p(yN|yN−1, . . . , y1). In case of N = 1, the language model\nreduces to the basic frequency distribution, i.e. Unigram, just like the statistics used in the example of\nCaesar Cipher. In case of N > 1, the language model describes the sequential pattern by predicting\nthe next letter in the form of a (N −1)-th order Markov model, e.g. N = 2 the Bigram model. We\nwill formulate unsupervised learning in detail assuming the Unigram model in this section, and walk\nthrough the Bigram case in Section V.\nOur problem here is very similar to that of decryption of Caesar Cipher; i.e. given only the N-gram\nstatistics (analogous to the Prior standard English statistics) and a sequence (x1, . . . , xT), which\nis analogous to the original message) as the input, we desire to predict the true label sequence\n(y1, · · · yT ), which is analogous to the decrypted message).\n3.2\nAn Objective Function for Unsupervised Learning\nSince the problem discussed here for pedagogical purposes is binary classiﬁcation, we use the most\ncommon model, the log-linear model, as our classiﬁer. In the log-linear model, the output of the\nclassiﬁer is a posterior probability. Recalling that the input is two-dimensional xt = (xa\nt , xb\nt), we\ndeﬁne the model that has two learnable parameters denoted as θ = {wa, wb}:\npθ,t(0) ≜pθ(yt = 0|xt) =\neγwaxa\nt\neγwaxa\nt + eγwbxb\nt\npθ,t(1) ≜pθ(yt = 1|xt) =\neγwbxb\nt\neγwaxa\nt + eγwbxb\nt\n(1)\nwhere γ is a constant (e.g. we ﬁx γ = 10 in our experiments described in Section VI). These formulas\ncalculate the probability of producing yt when receiving input xt. For example, if pθ,t(0) ≥0.5, we\npredict label ¯yt = 0; otherwise we predict label ¯yt = 1. Note that from Eqn.(1), we always have\npθ,t(0) + pθ,t(1) = 1.\nWe approach the binary classiﬁcation problem with unsupervised learning in three steps, analogous\nto decrypting Caesar Cipher described in Section III-B. The corresponding three main steps are:\n1) Determine language model statistics as the prior on labels Y; 2) Calculate the statistics on the\nclassiﬁer output, and 3) Match the two statistics with which a cost function is determined. Next we\ncarry out this procedure step-by-step to create the cost function.\nStep 1: Prior language model statistics. Since the dataset we use is synthetic data, i.e. both X and\nY are known, we can easily obtain the statistics of a Unigram language model by calculating the\nprobabilities below with all yt ∈Y given in the dataset:\nPLM ≜[p(yt = 0), p(yt = 1)] = [p0, p1]\n(2)\nNote that p0, p1 ≥0 are constants with p0 + p1 = 0.\nStep 2: Classiﬁer output statistics. The classiﬁer outputs from Eqn.(1) directly determine the\nprobability of data being assigned with each label. The Unigram (or histogram) statistics can be\neasily calculated by\nPLM(θ) =\n\u0002 1\nT\nPT\nt=1 pθ,t(0),\n1\nT\nPT\nt=1 pθ,t(1)\n\u0003T\n(3)\nwhere PLM(θ) stands for the estimated language model from classiﬁer which is parameterized by θ.\n5\nStep 3: Matching the two statistics. In the same spirit of decryption of Caesar Cipher, we need\nto compare and then match the two statistics PLM and PLM(θ) in order to ﬁnd the best decipher.\nAs suggested in (Chen et al., 2016), the difference between two distributions can be measured by\nKullback-Leibler (KL) divergence. The KL divergence of the true prior distribution PLM and the\nestimated distribution PLM is:\nDKL(PLM||PLM) = PLM ln PLM\nT ⊘PLM(θ) = −PLM ln PLM(θ) + PLM ln PLM\nT\n(4)\nwhere the sign ⊘denotes element-wise division. We minimize DKL in order to learn the model\nparameters θ to the effect that the estimated statistics become as close to the prior statistics as possible.\nNote that in DKL of Eqn. (4), the second term PLM, ln PLM\nT, is a constant and can be ignored in\noptimization. Thus, our objective function to be minimized becomes:\nJ (θ) = −PLM ln PLM(θ) = −p0 ln 1\nT\nT\nX\nt=1\npt,θ(0) −p1 ln 1\nT\nT\nX\nt=1\npt,θ(1)\n(5)\nIn summary, we formulate our problem of unsupervised learning in this pedagogical example as an\noptimization problem of ﬁnding the best classiﬁer parameters θ∗according to\nθ∗= arg min\nθ\nJ (θ)\n(6)\n3.3\nIssues of SGD-based Optimization\nHowever, for the objective function of Eqn.(5), it is not feasible to use SGD-based optimizer to solve\nthe above minimization problem above. This is due to two reasons: the high cost of computing the\ngradient and the highly non-convex proﬁle of the objective function with respect to the parameters.\nHigh cost of computing the gradient. The SGD-based optimizers, such as AdaGrad (Duchi et al.,\n2012) and ADAM (Kingma & Ba, 2014), require that the sum over training samples be outside of the\nlogarithmic loss (Ferguson, 1982). However, in the above loss function, the sum over samples t, i.e.\n1\nT\nPT\nt=1, is inside the logarithmic loss (ln). This makes a very large difference from traditional neural\nnetwork error minimization problems solved by SGD. The special form of the objective function in\nEqn. (5) prevents us from applying SGD to minimize the cost.\nHighly non-convex proﬁle. Even worse, the cost function is highly non-convex, and hard to converge\nto the optimum. Figure 3 illustrates this non-convex surface of the cost function J (θ) in in Eqn.\n(5). To obtain this proﬁle surface, we take two steps. First, we use both data X and true labels Y to\nﬁnd the global optimum θ0 = (wa0, wb0) (the red points in the ﬁgures) by solving the supervised\nlearning problem\nθ0 = arg max\nθ\nT\nX\nt=1\nln pt,θ(yt|xt).\nThis is an ordinary maximum posterior optimization problem with the sum over training sameples\nlying outside of the logarithm. Hence we solve it by plain SGD and plot with red dot in Figure 3. In\nthe second step, we plot the two-dimensional function around wa0, wb0, i.e. J (wa0 + λ1, wb0 + λ2)\nwith respect to λ1, λ2 ∈R. We can observe the surface of the cost function J (θ) and ﬁnd that it is\nhighly non-convex. There are many local optimal solutions and there is a high barrier that makes it\ndifﬁcult for naive optimization algorithms to reach the global optimum.\nTo overcome the above two difﬁculties, we can transform the optimization problem for the cost\nfunction in Eqn. (5) into an equivalent one, called saddle point problem, where the cost function has\na more desirable proﬁle surface for gradient-based methods. One such technique is introduced in\nSection IV next.\n4\nAn Equivalent Saddle Point Problem & Its Solution\n4.1\nProblem Formulation\nLet us ﬁrst introduce a lemma that will be used in the later description of the saddle point problem.\n6\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3: J (θ) proﬁle in the primal domain. The six sub-ﬁgures show the same proﬁle from six\ndifferent angles, spinning clock-wise from (a)-(f). The red dots indicate the global minimum.\nLemma 1. For any scaler u > 0, the equation below holds:\n−ln u = max\nv<0 (uv + ln (−v)).\nProof. We use convex conjugate functions to prove this equation. For a given convex function f(u),\nits convex conjugate function f ∗(v) is deﬁned in (Boyd & Vandenberghe, 2004) as:\nf ∗(v) ≜sup\nu (vT u −f(u)).\n(7)\nFurthermore, the convex function f(u) can be related to its convex conjugate function f ∗(v) as\nfollowing relation:\nf(u) = sup\nv (uT v −f ∗(v)).\n(8)\nWe now consider a convex function f(u) = −ln u where u is a scalar and u > 0. From Eqn. (??),\nthe conjugate function for f(u) can be obtained:\nf ∗(v)|f(u)=−ln u = sup\nu (uv + ln u) = max\nu\n(uv + ln u).\nLet ∂(uv + ln u)/∂u = v + 1/u = 0 and we can obtain the maximum point for u is at u = −1/v.\nSubstituting it in above formula, we can solve to get\nf ∗(v)|f(u)=−ln u = (uv + ln u)|u=−1\nv = −1 −ln v\n(9)\nwith v < 0 is a scaler. We then replace the f ∗(v) in the right side of Formula (8) with Formula (9),\nand replace the f(u) in left side with f(u) = −ln u. Finally, for any scaler u > 0, we obtan:\n−ln u = max\nv<0 (uv + ln (−v))\n7\nExamining Lemma 1, we ﬁnd ln u on the left side of the equation is transformed into an maximum\nover a different variable v of a formula where u comes outside the logarithm. Thus, we use this\nLemma to rewrite the logarithm part in the cost function in Eqn. (5):\nmin\nθ J (θ) = min\nθ\n(\n−p0 ln 1\nT\nT\nX\nt=1\npt,θ(0) −p1 ln 1\nT\nT\nX\nt=1\npt,θ(1)\n)\n= min\nθ\n(\np0 max\nv0<0\n\"\nv0\n1\nT\nT\nX\nt=1\npt,θ(0) + ln (−v0)\n#\n+ p1 max\nv1<0\n\"\nv1\n1\nT\nT\nX\nt=1\npt,θ(1) + ln (−v1)\n#)\n= min\nθ\nmax\nv0,v1<0\n(\np0v0\n1\nT\nT\nX\nt=1\npt,θ(0) + p0 ln (−v0) + p1v1\n1\nT\nT\nX\nt=1\npt,θ(1) + p1 ln (−v1)\n)\n= min\nθ\nmax\nv0,v1<0\n(\n1\nT\nT\nX\nt=1\n[p0v0pt,θ(0) + p1v1pt,θ(1)] + p0 ln (−v0) + p1 ln (−v1)\n)\n(10)\nLet us now deﬁne the new cost function L(θ, V ) to be the quantity inside the min max in Equation\n10 above:\nL(θ, V ) = 1\nT\nT\nX\nt=1\n[p0v0pt,θ(0) + p1v1pt,θ(1)] + p0 ln (−v0) + p1 ln (−v1)\n(11)\nwhere V = {v0, v1}. With the use of matrix form V = [v0, v1], as well as Equations (2) and (24), we\nrewrite the new cost function into a matrix form (where ⊙stands for pairwise multiplication between\ntwo matrices):\nmin\nθ\nmax\nV<0 L(θ, V) = min\nθ\nmax\nV<0\n\u001a\n[PLM ⊙V]PLM(θ) + PLM ln (−V)T\n\u001b\n(12)\nLet us provide interpretation for the new cost function L(θ, V) in Eqn. (12), contrasting the earlier\nform of J (θ) in Eqn. (5). In the new form, parameter V is called dual variables, and the original\nparameters θ are called primal variables. Minimization of J (θ) over primal variables θ has been\ntransformed to a min-max problem on L(θ, V) over both primal variables θ and dual variables V.\nThe min-max problem in this new form is called saddle-point problem. Speciﬁcally, the saddle-point\nproblem can be solved by seeking a point, which is not only the minimum point along primal domain\n(θ) but also the maximum point along dual domain (V). This point, denoted as (θ∗, V∗), is called the\nsaddle point for the domain of (θ, V) [2]. Importantly, we ﬁnd that the sum over t has been taken\noutside of logarithm. As a result, we can now apply SGD or its variation in learning the model.\n4.2\nProﬁle Surface of L(θ, V)\nWe now draw the proﬁle of cost function L(θ, V) in Figure 4.\nLike the proﬁle surface\nof J (θ) in Section IV-C, we need to ﬁrst get θ0 by solving the supervised problem θ0 =\narg maxθ\nPT\nt=1 ln pt,θ(yt|xt). Then, we inference V 0 by\nV0 = −\n1\nPLM\nT (θ0)\n(13)\nWith the supervised optima θ0, V0, we can plot the 3-D cost function surface around this optima.\nWe randomly choose two direction (θ1 −θ0) and (V1 −V0), where θ1 and V1 are random vectors\nwith same size of θ0 and V0. Then we plot on the 3-D space the point (λp, λd, Z(λp, λd)), with\nZ(λp, λd) = J (θ∗+ λp(θ1 −θ0), V0 + λd(V1 −V0)). The Figure 4 shows the plotting results.\nComparing surface of original surface in Figure 3 with surface of transformed cost function in Figure\n4, we can see that, after the primal-dual reformulation, the barrier in original cost function J (θ)\ndisappears. Thus the optimization become much easier on the new cost function.\n8\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 4: L(θ, V) proﬁle surface in the primal-dual domain of the Unigram case. The six sub-ﬁgures\nshow the same proﬁle from six different angles, spinning clock-wise from (a)-(f). The red dots\nindicate the saddle point (global optima).\n4.3\nSolution with Primal-Dual Optimization\nIn this section, we proceed to ﬁnd the solution to the saddle-point problem in Eqn. (12).\nTo solve a min-max problem, our idea is to minimize L(θ, V) with respect to θ, while, at the same\ntime, maximize L(θ, V) with respect to V. Speciﬁcally, we solve the min-max problem by stochastic\ngradient descent of L(θ, V) over θ while by stochastic accent of L(θ, V) over V with following\nupdate equations:\nθτ = θτ−1 −µθ\n∂L\n∂θ\n\f\f\f\f\nθ=θτ−1,V=Vτ−1\nVτ = Vτ−1 + µV\n∂L\n∂V\n\f\f\f\f\nθ=θτ−1,V=Vτ−1\n(14)\nThe updates happens at time τ. And µθ, µV > 0 are learning rate for primal variables and dual\nvariables, respectively. Note that we update two of primal and dual variables in a synchronized manor.\nNow we derive the close form of ∂L\n∂θ and ∂L\n∂V. First, for ∂L\n∂θ , from Eqn. (12), we take partial derivative:\n∂L\n∂θ = ∂\n∂θ\n\u001a\n[PLM ⊙V]PLM(θ) + PLM ln (−V)T\n\u001b\n= [PLM ⊙V]∂PLM(θ)\n∂θ\n(15)\nwhere the term ∂PLM(θ)\n∂θ\nin above can be decomposed and calculated in element-wise. Recall that\nPLM = [p0, p1] is constant, PLM(θ) = [ 1\nT\nPT\nt=1 pθ,t(0),\n1\nT\nPT\nt=1 pθ,t(1)]T and θ = [wa, wb]T .\nFrom matrix derivative rules we know:\n∂PLM(θ)\n∂θ\n= 1\nT\nT\nX\nt=1\n\u0014\n∂pt,θ(0)/∂wa\n∂pt,θ(0)/∂wb\n∂pt,θ(1)/∂wa\n∂pt,θ(1)/∂wb\n\u0015\n(16)\n9\nwhere pt,θ(0) and pt,θ(1) are deﬁned in Eqn. (1) being log-linear classiﬁer model for classes 0 and\n1, respectively. The four derivatives in Eqn. (16) can be calculated from deﬁnition in Eqn. (1). For\nexample:\n∂pt,θ(0)\n∂wa\n=\n∂\n∂wa\n\u001a\neγwaxa\nt\neγwaxa\nt + eγwbxb\nt\n\u001b\n=\n∂\n∂wa\n\u001a\n1\n1 + e−(γwaxa\nt −γwbxb\nt)\n\u001b\n=\n∂\n∂wa σ(γwaxa\nt −γwbxb\nt)\n= σ(γwaxa\nt −γwbxb\nt)(1 −σ(γwaxa\nt −γwbxb\nt))∂(γwaxa\nt −γwbxb\nt)\n∂wa\n= pt,θ(0)(1 −pt,θ(0))(γxa\nt )\n= γpt,θ(0)pt,θ(1)xa\nt\n(17)\nIn above derivation, σ(·) indicate sigmoid function that σ(a) =\n1\n1+e−a . Note that we use the equation\n∂σ(a)\n∂a\n= σ(a)(1 −σ(a)) in above derivation. Similarly, we can get all the derivatives in matrix of 16\nas:\n∂pt,θ(0)\n∂wa\n= γpt,θ(0)pt,θ(1)(xa\nt )\n∂pt,θ(0)\n∂wb\n= γpt,θ(0)pt,θ(1)(−xb\nt)\n∂pt,θ(1)\n∂wa\n= γpt,θ(0)pt,θ(1)(−xa\nt )\n∂pt,θ(1)\n∂wb\n= γpt,θ(0)pt,θ(1)(xb\nt)\n(18)\nThen we substitute all derivative terms in Eqn. (16) using (18):\n∂PLM(θ)\n∂θ\n= 1\nT\nT\nX\nt=1\nγpt,θ(0)pt,θ(1)X\nwhere X =\n\u0014\nxa\nt\n−xb\nt\n−xa\nt\nxb\nt\n\u0015\n(19)\nThus we get the ﬁnal close form of ∂L\n∂θ :\n∂L\n∂θ = 1\nT\nT\nX\nt=1\n\u001a\nγpt−1,θ(0)pt−1,θ(1)[PLM ⊙V]X\n\u001b\n(20)\nThen for ∂L\n∂V, recall that V = [v0, v1]. Simply use matrix derivative rules and we can get:\n∂L\n∂V =\n∂\n∂V\n\u001a\n[PLM ⊙V]PLM(θ) + PLM ln (−V)T\n\u001b\n= PLM ⊙PLM\nT (θ) + PLM\nV\n(21)\nFor mini-batch updates, we simply replace the sum over total data T with sum over the mini-batch\ndata B = (xs, ..., xs+N−1). For each update, a sequence of data with size N is randomly taken\nfrom X as a mini-batch. Then we use Formula (14) to update both primal and dual variable with\nclose form (20) and (21). Repeating this process until convergence. We describe the this process with\nmini-batch gradient in Algorithm 1.\n4.4\nThe Weakness of Unigram\nHowever, we found the language model of Unigram is too weak. It does not capture the sequence\nstructure which is important information for unsupervised classiﬁcation. As in Chen et al. (2016),\n10\nAlgorithm 1 Stochastic Primal-Dual Gradient Method\n1: Input data: X = (x1, . . . , xT) and PLM.\n2: Initialize θ and V from random numbers where the elements of V are negative\n3: repeat\n4:\nRandomly sample a mini-batch of B subsequences of length N, i.e., B = (xs, . . . , xs+N−1).\n5:\nCompute the stochastic gradients ∂L/∂θ and ∂L/∂V for the subsequence in the mini-batch\nusing Formula (20) and (21).\n6:\nUpdate θ and V according to\nθ ←θ −µθ ∂L\n∂θ ,\nV ←V + µV ∂L\n∂V\n7: until convergence or a certain stopping condition is met\nthe sequence prior of output labels is a important structure to employ in unsupervised problems,\nespecially when the order is high. However, the Unigram language model does not capture such\nstructure.\nHowever, the experiment results turns out to be unsatisﬁed. The average classiﬁcation error rate\nis 30.8%, which is just about the error rate of majority guess. The model constantly predicts class\nwith label 1, regardless of the input data. In experiment we conduct the experiments on the binary\nclassiﬁcation task with the dataset we created for (X, Y).\nNext, we will walk through this problem by using Bigram language model, the simplest language\nmodel with sequence prior statistics. The main procedure of the derivation is the same with Unigram\nwe just did in previous sections. So it can be easier for readers to apply to Bigram case, in spite of\nmore mathematics will be involved.\n5\nExtension to the Case with Sequence Prior Statistics\n5.1\nProblem Formulation for the Sequential Bigram Case\nIn Bigram case, the problem formulation and approach is the same as Unigram case that we also use\nmodel for classiﬁer as in formula 1. except using Bigram language models. Here below are the three\nmain steps accordingly.\nStep 1: Prior language model statistics (Bigram) We will create synthetic data with arbitrary given\nlanguage model. First, we create output labels Y = (y1, y2, . . . , yT ) from a 1st-order Markov model\nthat is based on a given transition probability:\np(y1, y2, . . . , yT ) =\nT\nY\nt=1\np(yt|yt−1)\n(22)\np(yt|yt−1) denotes conditional probability of the transition between adjacent labels yt−1 to yt, which\nis given in transition matrix PTrans. Then, the language model PLM can be calculated from the\ntransition matrix:\nPLM ≜\n\u0014\np00\np01\np10\np11\n\u0015\n= diag(pst)PTrans\n(23)\npst denotes the steady state vector of transition PTrans. The creation of data, as well as connection\nbetween language model and transition matrix will be explained in detail in experiment section VII.\nStep 2: Classiﬁer output statistics (Bigram) Due to ﬁrst order dependency, the probability of data\nbeing assigned with each label by the classiﬁer in Eqn. (1) can be calculated by:\nPLM(θ) =\n\u0014 1\nT\nPT\nt=1 pθ,t(0)pθ,t−1(0),\n1\nT\nPT\nt=1 pθ,t(0)pθ,t−1(1)\n1\nT\nPT\nt=1 pθ,t(1)pθ,t−1(0),\n1\nT\nPT\nt=1 pθ,t(1)pθ,t−1(1)\n\u0015\n= 1\nT\nT\nX\nt=1\n\u0014\npθ,t−1(0)\npθ,t−1(1)\n\u0015 \u0014\npθ,t(0)\npθ,t(1)\n\u0015T\n11\n= 1\nT\nT\nX\nt=1\nPt−1,θPt,θ\nT\nwhere\nPt,θ = [pt,θ(0) pt,θ(1)]T\n(24)\nStep 3: Matching the two statistics As with Unigram, we still use KL-divergence to measure the\ndifference, and obtain cost function :\nJ (θ) = −\n\nPLM , ln PLM(θ)\n\u000b\n= −\nX\ni,j∈{0,1}\npij ln 1\nT\nT\nX\nt=1\n[pt−1,θ(i)pt,θ(j)]\n(25)\nwhere the notation ⟨A, B⟩= P\ni\nP\nj aijbij stands for the inner product of two matrices A and\nB, speciﬁcally, sum of the products of the corresponding components of two matrices. Thus, we\nformulate our problem as an optimization problem ﬁnding the best classiﬁer parameters θ∗:\nθ∗= arg min\nθ\nJ (θ)\n(26)\nWe again observe the sum inside logarithm. With using Lemma 1, we can transform this problem to\nits equivalent saddle-point problem:\nmin\nθ\nJ (θ) = min\nθ\n\n\n\nX\ni,j∈{0,1}\npij max\nvij<0\n\"\nvij\n1\nT\nT\nX\nt=1\n[pt−1,θ(i)pt,θ(j)] + ln (−vij)\n#\n\n\n= min\nθ\nmax\nvij<0\n\n\n\nX\ni,j∈{0,1}\npijvij\n1\nT\nT\nX\nt=1\n[pt−1,θ(i)pt,θ(j)] + pij ln (−vij)\n\n\n\n= min\nθ\nmax\nvij<0\n(\n1\nT\nT\nX\nt=1\nX\ni,j∈{0,1}\npijvijpt−1,θ(i)pt,θ(j) +\nX\ni,j∈{0,1}\npij ln (−vij)\n)\n= min\nθ\nmax\nV<0\n\u001a 1\nT\nT\nX\nt=1\n(pT\nt−1,θ[PLM ⊙V]pt,θ) + ⟨PLM, ln (−V)⟩\n\u001b\nAs in the case of Unigram, we deﬁne the new cost:\nL(θ, V) = 1\nT\nT\nX\nt=1\n(pT\nt−1,θ[PLM ⊙V]pt,θ) + ⟨PLM, ln (−V)⟩\nV =\n\u0014\nv00 v01\nv10 v11\n\u0015\nPt,θ =\n\u0014\npt,θ(0)\npt,θ(1)\n\u0015\nPLM =\n\u0014\np00 p01\np10 p11\n\u0015\nθ =\n\u0014wa\nwb\n\u0015\n(27)\nθ is called primal variables, and V is called dual variables. Thus, the minimization problem of J (θ)\nover primal variables θ is transformed to an equivalent saddle-point problem of L(θ, V), and the sum\nover t has been taken outside of logarithm. In other words, the optimal solution (θ∗, V∗) to L(θ, V)\nis called the saddle point [2], which is relative minimum along primal domain θ while is a relative\nmaximum along dual domain V.\n5.2\nProﬁle Surface of L(θ, V)\nAs in the Unigram case, we also visualize the proﬁle surface of cost function L(θ, V) in Figure 5.\nAgain, we need to ﬁrst get θ0 by solving the supervised problem θ0 = arg maxθ\nPT\nt=1 ln pt,θ(yt|xt),\nand inference V 0 using\nV0 = −\nT\nPT\nt=1 Pt−1,θ0PT\nt,θ0\n(28)\nUsing the same method, we ﬁrst randomly choose two direction (θ1 −θ0) and (V1 −V0), where θ1\nand V1 are random vectors with same size of θ0 and V0. Then we plot on the 3-D space the point\n(λp, λd, Z(λp, λd)), with Z(λp, λd) = J (θ∗+ λp(θ1 −θ0), V0 + λd(V1 −V0)). The Figure 5\nshows the plotting results.\n12\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 5: L(θ, V) proﬁle in the primal-dual domain of the Bigram case. The six sub-ﬁgures show\nthe same proﬁle from six different angles, spinning clock-wise from (a)-(f). The red dots indicate the\nsaddle point (global optima).\n5.3\nPrimal-Dual Optimization for Bigram Case\nSimilarly, we solve the min-max problem by stochastic gradient descent of L(θ, V ) over θ while by\nstochastic accent of L(θ, V ) over V with the same update equations:\nθτ = θτ−1 −µθ\n∂L\n∂θ\n\f\f\f\f\nθ=θτ−1,V=Vτ−1\nVτ = Vτ−1 + µV\n∂L\n∂V\n\f\f\f\f\nθ=θτ−1,V=Vτ−1\n(29)\nThe updates happens at time τ. And µθ, µV > 0 are learning rate for primal variables and dual\nvariables, respectively. Note that we update two of primal and dual variables in a synchronized manor.\nBy using Formula (19), we calculate ∂L/∂θ and ∂L/∂V similar as the Unigram Case:\n∂L\n∂θ = 1\nT\nT\nX\nt=1\n\u001a\nγPt−1,θ(0)Pt−1,θ(1)([XT\nt−1(PLM ⊙V)Pt,θ + (PLM ⊙V)T XT\nt Pt−1,θ]\n\u001b\n∂L\n∂V = 1\nT\nT\nX\nt=1\n\u001a\nPLM · (Pt−1,θPT\nt,θ) + PLM\nV\n\u001b\nwhere X =\n\u0014\nxa\nt\n−xb\nt\n−xa\nt\nxb\nt\n\u0015\n(30)\nAs Unigram, we also use mini-batch to calculate each update during training in the Bigram case. It is\nthe same as the Unigram case with mini-batch gradient in Algorithm 1.\n13\n6\nExperiments\n6.1\nThe Case of Unigram\nData Generation We ﬁrst create synthetic data with Unigram language model, where the data points\nand associated labels subject to a given Unigram language model, such as:\nPLM = [p0 p1] = [0.692 0.308]\n(31)\nThat is 69.2% of labels “0” and 30.8% of label “1” in Y = (y1, . . . , yT ).\nGiven the corresponding label of yt, we can create the input points on a 2-dimensional grid X =\n(x1 . . . xT) by sampling over independent following Gaussian distributions:\np(xt|yt = 0) ∼N(µ0; Σ)\np(xt|yt = 1) ∼N(µ1; Σ)\nwhere the parameters µ0, µ1 ∈R2×1 are centers of two classes data points, and Σ denotes the\ncovariance matrix. In experiment, the central points µ0 and µ1 are randomly generated, for example,\nµ0 = [−0.504, −0.264]T and µ1 = [1.646, 0.181]T . Particularly, Σ is manually chosen to make\nsure the data points of two classes slightly “blend” into each other, because we need to insure that an\nunique optimal solution can be reached. So we set Σ = 0.4 · I. Following above settings, we sample\ntotal size of T = 60, 000 data points, which is then split to 50,000 for training, 5,000 for testing and\n5,000 for validation. Figure 6 shows examples of the plot of data points X, with blue points being\nclass “0” and red ones being class “1”.\nExperimental Results To validate the analysis and to conﬁrm mathematical derivation, we implement\nAlgorithm 1 with all hyper-parameters list in Table 1. We conduct the experiments on the binary\nclassiﬁcation task with the dataset we created for (X, Y). However, the results turns out to be\nunsatisfactory. The average classiﬁcation error rate is 30.8%, which is just about the error rate of\nmajority guess: it constantly predicts class “0” regardless of the input data. We repeated the above\nexperiment 5 times and collect the results as shown in Table 2, which further validate that this method\nis nothing more than guessing majority class.\nProblems of Non-sequence Language Model The reason for bad results is that the Unigram lan-\nguage model is too weak. Unigram is a non-sequence language model that loses important information\nfor unsupervised classiﬁcation problem. As in Chen et al. (2016), the sequence prior of output labels\nis a important structure to employ in unsupervised problems, especially when the order is high.\nHowever, the Unigram language model does not capture such structure. In next experiment we use\nBigram language model that will show much better results.\n-3\n-2\n-1\n0\n1\n2\n3\n4\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n2\nFigure 6: Plot of data points of two classes.\n14\nTable 1: The hyper-parameters used in training\nHyper-parameter\nNotation\nValue\nLearning Rate\nµθ\n10−6\nLearning Rate\nµV\n10−4\nMini-batch Size\nN\n10\nTraining Size\nTtrn\n50,000\nTest Size\nTtst\n5,000\nValidation Size\nTval\n5,000\nTable 2: Experiment Results using Unigram\nPLM\nError Rate\nExp1\n[0.692, 0.307]\n30.7%\nExp2\n[0.385, 0.615]\n38.5%\nExp3\n[0.583, 0.417]\n41.6%\nExp4\n[0.692, 0.308]\n30.8%\nExp5\n[0.667, 0.333]\n33.3%\nClass 0\nClass 1\n0.1\n0.9\n0.8\n0.2\nFigure 7: The transition probability of output observation.\n6.2\nThe Case of Bigram\nData Generation Different from Unigram data, now we need to create synthetic data that has\nadditional Markov property: that the class label of future data point is solely dependent on the current\nlabel. We decided to use Hidden Markov Model to create the dataset, by sampling from a ﬁrst-order\nHMM. First, we sample Y = (y1, . . . , yT ) from a given transition matrix PTrains. For example,\nPTrains =\n\u0014\np(yt = 0|yt−1 = 0)\np(yt = 1|yt−1 = 0)\np(yt = 0|yt−1 = 1)\np(yt = 1|yt−1 = 1)\n\u0015\n=\n\u0014\n0.6\n0.4\n0.9\n0.1\n\u0015\n.\n(32)\nwhere PT rans(i, j) = p(yt = j|yt−1 = i). This transition relations are visualized in graph of Figure\n7.\nThen, similar to Unigram case, we use two Gaussian emission models to sample the data for\nX = (x1 . . . xT) given the sampled labels Y = (y1, . . . , yT ):\np(yt = i|yt−1 = j) = PT rans(i, j)\nand\np(xt|yt = 0) ∼N(µ0; Σ)\np(xt|yt = 1) ∼N(µ1; Σ)\nIn our experiment, we use the same setting for the emission models as in Unigram case, i.e. µ0 =\n[−0.504, −0.264]T , µ1 = [1.646, 0.181]T and Σ = 0.4 · I. A total of T = 60, 000 labels Y and\ndata points X are created, and randomly split to 50,000 training, 5,000 testing and 5,000 validation\npartitions.\nNote that, instead of estimating the ground-truth language model PLM from the generated data,\nwe directly calculate PLM from transition matrix PTrans as follows. Due to Markov property, the\nlanguage model PLM = p(yt, yt−1) is actually joint probability, i.e. product of prior probability and\n15\nTable 3: The classiﬁcation error on test data using Bigram\nSupervised\nUnsupervised\nSynData1\n5.38%\n5.55%\nSynData2\n3.62%\n3.63%\nSynData3\n2.95%\n2.96%\nSynData4\n2.79%\n2.79%\nSynData5\n1.06%\n1.06%\nSynData6\n4.52%\n4.54%\nSynData7\n5.66%\n6.25%\nSynData8\n5.13%\n5.66%\nSynData9\n3.89%\n6.65%\nSynData10\n5.70%\n5.70%\nconditional probability as:\nPLM = p(yt−1, yt) = p(yt−1)p(yt|yt−1)\n(33)\nwhere p(yt−1) is actually marginal probability being steady state of transition of PTrans after inﬁnite\nsteps:\npst = [p(yt−1 = 0), p(yt−1 = 1)] = e · P∞\nTrans\n(34)\nwhere e indicate unit row vector, i.e. e = (1, 0). For our transition example in Eqn (32), one can\ncalculate that pst = [0.692, 0.308]. Thus, from Eqn. (32, 33, 34), the Bigram language model PLM\ncan be calculated by:\nPLM =\n\u0014\np(yt−1 = 0)p(yt = 0|yt−1 = 0)\np(yt−1 = 0)p(yt = 1|yt−1 = 0)\np(yt−1 = 1)p(yt = 0|yt−1 = 1)\np(yt−1 = 1)p(yt = 1|yt−1 = 1)\n\u0015\n= diag(pst)PTrans\n(35)\nwhere diag(pst) stands for the diagonal matrix with vector pst as its main diagonal. Using Eqn.\n(35), one can calculate the Bigram language model for our example is:\nPLM =\n\u0014\n0.4154\n0.2769\n0.2769\n0.0308\n\u0015\nExperiment Results To fully validate the method, we repeatedly generate 10 synthetic datasets using\ndifferent HMMs stated in the previous part of the article. Each of the HMMs in the i −th experiment\nhas all the same settings except the transition matrix P(i)\nTrans. Here list all the P(i)\nTrans we used in\nexperiments:\nP(1)\nTrans =\n\u0014\n0.1\n0.9\n0.8\n0.2\n\u0015\nP(2)\nTrans =\n\u0014\n0.6\n0.4\n0.9\n0.1\n\u0015\nP(3)\nTrans =\n\u0014\n0.2\n0.8\n0.5\n0.5\n\u0015\nP(4)\nTrans =\n\u0014\n0.3\n0.7\n0.4\n0.6\n\u0015\nP(5)\nTrans =\n\u0014\n0.4\n0.6\n0.1\n0.9\n\u0015\nP(6)\nTrans =\n\u0014\n0.5\n0.5\n0.7\n0.3\n\u0015\nP(7)\nTrans =\n\u0014\n0.7\n0.3\n0.8\n0.2\n\u0015\nP(8)\nTrans =\n\u0014\n0.8\n0.2\n0.4\n0.6\n\u0015\nP(9)\nTrans =\n\u0014\n0.5\n0.5\n0.4\n0.6\n\u0015\nP(10)\nTrans =\n\u0014\n0.9\n0.1\n0.3\n0.7\n\u0015\nThen, we train the model using algorithm 1 with the same set of hyper-parameters in Table 1. We use\ntraining set for learning the model and validation set for early stop, test set for producing results. We\nrepeat this process 10 times on the 10 generated dataset, and the test errors are listed in Table 3.\nDiscussion of Experiment Results From Table 2, we ﬁnd all the error rates in the unsupervised\nlearning column are very low, which verify the SPDG method we introduce in the article being\neffective.\nSpeciﬁcally, we ﬁrst observe by comparing the numbers horizontally that the unsupervised SPDG\nmethod can achieve the almost the same performance as supervised learning on each dataset. We also\n16\nobserve that the margin between supervised and unsupervised learning for each experiment are mostly\nless than 1%. Furthermore, comparing the numbers vertically in the table, it seems that different\nperformances are yielded by different transition matrix. However, the variance of performances\nover 10 experiments is actually caused by the data sampling from Gaussian models, not the label\ngeneration using PTrans, because the error rates of supervised and unsupervised for each experiment\nare consistent. Otherwise, the results of supervised ones should be all the same since this training\nmethod is independent from sequence property lying in the PTrans.\nRecall the Unigram result we obtained previously (error rate 30.8%), with only changing the lan-\nguage model from Unigram to Bigram, the unsupervised model gains signiﬁcant improvement in\nperformance. Therefore, we can conclude that the sequential structure is indeed an important prior to\nbe used in supervised learning. The Bigram language model is capable of capturing this prior, despite\nthat it is only the simplest sequential statistics prior.\n7\nConclusions & Summary\nIn this article, we ﬁrst surveyed major classes of unsupervised learning methods developed in the past.\nWe then focused on a special class of unsupervised learning techniques designed for classiﬁcation\ntasks without using labeled data in training via direct, end-to-end optimization. We motivated the\nmethods using the prominent example of encryption technique called Caesar Cipher. In technical\nterms, we described a novel objective function for optimization in solving this class of unsupervised\nlearning problems and introduced a new stochastic primal-dual gradient method for solving the\noptimization, accomplishing the desired unsupervised learning for classiﬁcation.\nIn this tutorial exposition, we thoroughly carried out the needed mathematical analysis and illustrated\nthe step-by-step solutions to unsupervised learning for the cases of Unigram language model and\nBigram language model, respectively. The readers should be able to follow the self-contained\nderivations we provided in this article step by step, without referring to other material. For the\nreaders who want to implement the unsupervised learning methods without following the detailed\nderivation steps, we have provided a summary of the computation steps in the learning algorithm for\nthe Unigram and Bigram cases in two separate columns in Table 4.\nOne technical contribution of this article worth mentioning here is the data synthesis experiments, as\ndescribed in Section 6, which we used to explain and evaluate the unsupervised learning algorithm.\nIn these computational experiments, we generated a large set of synthetic sequence data, and use\nthem to validate the mathematical analysis and derivation. We conﬁrm that the optimization method\nof SPDG is capable of effectively exploiting the sequential structure of data on learning the mapping\nfrom input to output without the use of output labels.\nThe ideas explained in this article open up the potential for unsupervised learning that would have\nmany practical applications involving sequential data in real world. An example is unsupervised\ncharacter recognition and English spelling correction as explored in (Liu et al., 2017). More recently,\nthis approach is extended by Yeh et al. (2019) to speech recognition where the output sequences\ncontain both sequence and segmentation structures, creating the ﬁrst success of fully unsupervised\nspeech recognition following the earlier proposals in (Chen et al., 2016; Deng, 2015). Another future\nextension is to further exploit the structures of the input data and combine it with our method. For\nexample, in (Eslami et al., 2018), the authors proposed a Generative Query Network (GQN) to learn\ninternal representations of scenes from different viewpoints and then use them to render the same\nscene of a different viewpoint. With such a learning process, GQN is able to learn representations\nthat model the inherent input data regularity. Our method is orthogonal to GQN in that we can always\ncombine our cost function with those that exploit the input structures, which could potentially lead to\nbetter unsupervised learning methods.\nThe exposition provided in this article has been limited to a binary sequence classiﬁcation problem\nwith label-free data in training the classiﬁer parameters. This was made possible by making use of the\nproperty of sequential structure in the data as represented by language models. In practically all past\nwork, sequential classiﬁcation or pattern recognition problems have been tackled with supervised\nlearning requiring labeled data, even if the sequential structure of the data was available (He et al.,\n2008; Mesnil et al., 2013). While the structure of the data exploited is expressed in terms of language\nmodels (both Unigram and Bigram), other ways of representing the structure of the data (e.g. graph)\n17\ncan also be exploited within the framework of unsupervised learning described in this tutorial, and\nwe leave this extension to the interested readers.\n18\nTable 4: Summary of Computation Steps and Comparisons of Using Unigram and Bigram\nUnigram\nBigram\nPLM\nh\np0, p1\ni\n(Eqn. 2)\n\np00\np01\np10\np11\n\n(Eqn. 23)\nPLM(θ)\nh\n1\nT\nPT\nt=1 pθ,t(0),\n1\nT\nPT\nt=1 pθ,t(1)\niT\n(Eqn. 24)\n1\nT\nPT\nt=1 Pt−1,θPt,θ\nT (Eqn. 24)\nJ (θ)\n−PLM ln PLM(θ) (Eqn. 5)\n−\n\nPLM , ln PLM(θ)\n\u000b\n(Eqn. 25)\nL(θ, V)\n[PLM ⊙V]PLM(θ)\n+PLM ln (−V)T\n(Eqn. 12)\n1\nT\nT\nX\nt=1\n(pT\nt−1,θ[PLM ⊙V]pt,θ)\n+ ⟨PLM, ln (−V)⟩\n(Eqn. 27)\n∂L\n∂θ\n1\nT\nPT\nt=1\n\u001a\nγpt−1,θ(0)pt−1,θ(1)[PLM ⊙V]X\n\u001b\n(Eqn.\n20)\n1\nT\nT\nX\nt=1\n\u001a\nγPt−1,θ(0)Pt−1,θ(1)([XT\nt−1(PLM ⊙V)Pt,θ\n+(PLM ⊙V)T XT\nt Pt−1,θ]\n\u001b\n(Eqn. 30)\n∂L\n∂V\n∂\n∂V\n\u001a\n[PLM ⊙V]PLM(θ) + PLM ln (−V)T\n\u001b\n(Eqn.\n21)\n1\nT\nPT\nt=1\n\u001a\nPLM · (Pt−1,θPT\nt,θ) + PLM\nV\n\u001b\n(Eqn. 30)\nV0\n−\n1\nPLM\nT (θ0) (Eqn. 13)\n−\nT\nPT\nt=1 Pt−1,θ0PT\nt,θ0 (Eqn. 28)\n19\nReferences\nAbdel-Hamid, O., Mohamed, A., Jiang, H., Deng, L., Penn, G., and Yu, D. Convolutional neural\nnetworks for speech recognition. IEEE/ACM Trans. Audio, Speech and Language Processing, 22,\n2014.\nArtetxe, Mikel, Labaka, Gorka, Agirre, Eneko, and Cho, Kyunghyun. Unsupervised neural machine\ntranslation. In arXiv:1710.11041v1, 2017.\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly\nlearning to align and translate. ICLR, 2015.\nBoyd, S. and Vandenberghe, L. Convex optimization. Cambridge university press, 2004.\nChen, Jianshu, Huang, Po-Sen, He, Xiaodong, Gao, Jianfeng, and Deng, Li. Unsupervised learning\nof predictors from unpaired input-output samples. arXiv:1606.04646, 2016.\nCipher, Caesar. Online tool of caesar cipher. https://studio.code.org/s/hoc-encryption/stage/1/puzzle/4,\n2009.\nDahl, George E, Yu, Dong, Deng, Li, and Acero, Alex. Context-dependent pre-trained deep neural\nnetworks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE\nTransactions on, 20(1):30–42, 2012.\nDeng, L. and Yu, D. Deep Learning: Methods and Applications. NOW Publishers, 2014.\nDeng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton, G. Binary coding of speech\nspectrograms using a deep autoencoder. Proceedings of Interspeech, 2010.\nDeng, L., Hinton, G., and Kingsbury, B. New types of deep neural network learning for speech\nrecognition and related applications: An overview. Proceedings of ICASSP, 2013.\nDeng,\nLi.\nDeep learning for speech and language processing:\nA tutorial.\nIn\nTutorial\nat\nInterspeech\nConf,\nDresden,\nGermany,\nhttps://www.microsoft.com/en-\nus/research/wpcontent/uploads/2016/07/interspeech-tutorial-2015-lideng-sept6a.pdf, 2015.\nDeng, Li and Li, Xiao. Machine learning paradigms for speech recognition: An overview. IEEE\nTransactions on Audio, Speech, and Language Processing, 21(5):1060–1089, 2013.\nDevlin, Jacob, Chang, Ming-Wei, Lee, Kenton, and Toutanova, Kristina. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 2012.\nEslami, SM Ali, Rezende, Danilo Jimenez, Besse, Frederic, Viola, Fabio, Morcos, Ari S, Garnelo,\nMarta, Ruderman, Avraham, Rusu, Andrei A, Danihelka, Ivo, Gregor, Karol, et al. Neural scene\nrepresentation and rendering. Science, 360(6394):1204–1210, 2018.\nFerguson, Thomas S. An inconsistent maximum likelihood estimate. Journal of the American\nStatistical Association, 1982.\nFinn, Chelsea, Goodfellow, Ian, and Levine, Sergey. Unsupervised learning for physical interaction\nthrough video prediction. Conference on Neural Information Processing Systems, 2016.\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil,\nCourville, Aaron, and Bengio, Yoshua. Generative adversarial nets. In Proceedings of the Advances\nin Neural Information Processing Systems (NIPS), pp. 2672–2680, 2014.\nGupta, Ankush, Vedaldi, Andrea, and Zisserman, Andrew. Learning to read by spelling: Towards\nunsupervised text recognition. arXiv preprint arXiv:1809.08675, 2018.\nHe, X., Deng, L., and Chou, W. Discriminative learning in sequential pattern recognition. 25(5),\n2008.\n20\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-Rahman, Jaitly, Navdeep,\nSenior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N., and Kingsbury, B. Deep\nneural networks for acoustic modeling in speech recognition: The shared views of four research\ngroups. IEEE Signal Processing Magazine, 29(6):82–97, November 2012.\nImageNet. Imagenet website. http://www.image-net.org/, 2010.\nJakab, Tomas, Gupta, Ankush, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object\nlandmarks through conditional image generation. 2018.\nKazemi, Hadi, Soleymani, Sobhan, Taherkhani, Fariborz, Iranmanesh, Seyed, and Nasrabadi, Nasser.\nUnsupervised image-to-image translation using domain-speciﬁc variational information bound.\nConference on Neural Information Processing Systems, 2018.\nKingma, Diederik P. and Ba, Jimmy.\nAdam: A method for stochastic optimization.\nCoRR,\nabs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.\nKingma, Diederik P and Welling, Max.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nLample, Guillaume, Denoyer, Ludovic, and Ranzato, Marc Aurelio. Unsupervised machine translation\nusing monolingual corpora only. In arXiv:1711.00043v1, 2017.\nLeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature, 521, 2015.\nLiu, Yu, Chen, Jianshu, and Deng, Li. An unsupervised learning method exploiting output statistics.\nConference on Neural Information Processing Systems, 2017.\nMejjati, Youssef A., Richardt, Christian, Tompkin, James, Cosker, Darren, and Kim, Kwang In.\nUnsupervised attention-guided image to image translation. Conference on Neural Information\nProcessing Systems, 2018.\nMesnil, Gregoire, He, Xiaodong, Deng, Li, and Bengio, Yoshua. Investigation of recurrent-neural-\nnetwork architectures and learning methods for spoken language understanding. In Proceedings of\nInterspeech, 2013.\nMikolov, T., Sutskever, I., Chen, K, Corrado, G., and Dean, J. Distributed representations of words\nand phrases and their compositionality. Proceedings of NIPS, 2013.\nNg, A., Jordan, M., and Weiss, Y. On spectral clustering: analysis and an algorithm. Conference on\nNeural Information Processing Systems, 2002.\nOpenAI.\nBetter language models and their implications.\nIn http://aiweb.techfak.\nuni-bielefeld.de/content/bworld-robot-control-software/, 2019.\nRamachandran, Prajit, Liu, Peter, and Le, Quoc. Unsupervised pretraining for sequence to sequence\nlearning.\nProceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, 2017.\nvan den Oord et al. Wavenet: A generative mode for raw audio. arXiv, 2016a.\nvan den Oord et al. Pixel recurrent neural networks. International Conf. Machine Learning, 2016b.\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Bengio, Yoshua, and Manzagol, Pierre-Antoine.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.\nYang, Zhilin, Zhao, Jake, Dhingra, Bhuwan, He, Kaiming, Cohen, William W., Salakhutdinov,\nRuslan R., and LeCun, Yann. Glomo: Unsupervised learning of transferable relational graphs.\n2018a.\nYang, Zichao, Hu, Zhiting, Dyer, Chris, Xing, Eric P., and Berg-Kirkpatrick, Taylor. Unsupervised\ntext style transfer using language models as discriminators. Conference on Neural Information\nProcessing Systems, 2018b.\n21\nYeh, Chih-Kuan, Chen, Jianshu, Yu, Chengzhu, and Yu, Dong. Unsupervised speech recognition\nvia segmental empirical output distribution matching. Proc. International Conference of Learning\nRepresentations, 2019.\nYu, D. and Deng, L. Automatic Speech Recognition: A Deep Learning Approach. Springer, 2015.\nYu, D., Deng, L., and Dahl, G. Roles of pre-training and ﬁne-tuning in context-dependent dbn-hmms\nfor real-world speech recognition. NIPS Workshop, 2010.\nZhu, J, Park, T, Isola, P, and Efros, A. Unpaired image-to-image translation using cycle-consistent\nadversarial networks. In arXiv:1703, 2017.\n22\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-06-06",
  "updated": "2019-06-06"
}