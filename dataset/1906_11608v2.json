{
  "id": "http://arxiv.org/abs/1906.11608v2",
  "title": "Simple Natural Language Processing Tools for Danish",
  "authors": [
    "Leon Derczynski"
  ],
  "abstract": "This technical note describes a set of baseline tools for automatic\nprocessing of Danish text. The tools are machine-learning based, using natural\nlanguage processing models trained over previously annotated documents. They\nare maintained at ITU Copenhagen and will always be freely available.",
  "text": "arXiv:1906.11608v2  [cs.CL]  26 Jul 2019\nSIMPLE NATURAL LANGUAGE PROCESSING TOOLS\nFOR DANISH\nLeon Derczynski\nDepartment of Computer Science\nITU Copenhagen\nDenmark DK2300\nld@itu.dk\nJuly 29, 2019\nABSTRACT\nThis technical note describes a set of baseline tools for automatic processing of Danish text. The\ntools are machine-learning based, using natural language processing models trained over previously\nannotated documents. They are maintained at ITU Copenhagen and will always be freely available.\nKeywords natural language processing · Danish · open access\n1\nIntroduction\nDanish, a language with few resources for automatic processing, is spoken by six million people, largely concentrated\nin the Scandinavian country of Denmark. At the NLP (natural language processing) group at ITU Copenhagen, one\nof our foci is Nordic and Danish languages. Thus, we aim to include local languages in our NLP research whenever\npossible, and to provide tools for others working on these languages. Despite a modest number of researcher in the\ncountry, there has been a gap in usable Danish NLP tools that are available to the wider community. As in every\ntime that new technology is not effectively mediated to those who can use it, this restriction stymies the impact and\ndevelopment NLP in Denmark. To bridge this artiﬁcial lacuna, a basic set of easy-to-run utilities that build on existing\ndatasets and open tools has been developed at ITU, which this note introduces.\nThe tools are available at https://nlp.itu.dk/resources/.\n2\ndaner: Named Entity Recognition\nNamed entity recognition means ﬁnding words in text that refer to things by a speciﬁc name. This could be e.g. using\n“Nanna\" to refer to a speciﬁc individual, instead of “them\" or “she\" or “the person\"; or referring to “Moscow\" instead\nof “the nearest city\". The important thing is that one mentions a name speciﬁcally. Named entity recognition (NER)\nﬁnds those names. In the case of daner, names of Locations, Organisation and Persons are tagged.\nThe daner tool wraps the CoreNLP [1] named entity recognition component [2], using the DKIE data [3] developed\nas part of the GATE tool [4], which is derived from the UD part of the Copenhagen Dependency Treebank [5], itself\nincluding data from the Danish Dependency Treebank [6]. Further ad-hoc data is added as required, from newswire\nand other sources. The data overall comprises 20K tokens from the CDT (largely 1990s newswire), 3K tokens from\nnon-newswire Danish, and 10K tokens from post-2015 newswire. The mode performs best over Danish newswire\ntext. The data is annotated for three classes, person (PER), location (LOC), and organisation (ORG). Tags are in BIO\nformat.\nThe tool recognises names of people, of organisations, and of locations. It performs automatic tokenization, and\noutputs in slashed-tag format. For example:\nEn/O stor/O reform/O skal/O derfor/O blandt/O andet/O styrke/O tilliden/O til/O politikere/O\nog/O medier/O ,/O genopbygge/O tilliden/O til/O Skat/O og/O mindske/O de/O økonomiske/O\nforskelle/O i/O Danmark/B-LOC ./O\nHere, “Danmark\" is tagged as a location. The B indicates that “Danmark\" is the ﬁrst token in the location name (the\nbeginning ).\nAdditional data is added to an internal repository that is also used to train daner; some of this data has copyright\nrestrictions, and so cannot be distributed, though the aggregated statistical representation in daner’s model is free\nfrom that restriction, and so can be downloaded by anyone.\nThe NER model is conﬁgured to use contextual features, including n-grams up to 5-grams, and to use type sequences\nand word shape features.\ndaner uses Brown clusters induced over large Danish-language corpus. These are created using the Generalised\nBrown formulation [7], which offers more ﬂexibility in use than the classical version – speciﬁcally, one clustering run\ncan be used to generate clusterings of any size. For daner, 2500 clusters1 are induced from a clustering with window\nsize of 5000, over a set of 134M Danish tokens. These 134M tokens are taken from Danish Wikipedia and Danish\ncontent from the Common Crawl corpus ﬁltered through the FastText language ID [9], both using June 2019 data. The\nclusters are used inside the model, but also available separately at http://itu.dk/~leod/dansk-brown.tar.bz2.\n3\ndapipe: Tokenization, Part-of-speech tagging, and Dependency Parsing\nAnother utility in the toolkit, dapipe, provides three functions: breaking text ﬁles into words and sentences, ﬁnding\nwhat class each word is through PoS tagging, and determining one part of the syntax that links a sentence’s words\ntogether, through dependency parsing.\nDeﬁning where words and sentences start and stop is very important if we are going to do NLP. These processes are\nboth called tokenization. dapipe uses the Universal dependencies method for tokenizing, breaking text into sentences\nand words.\nThe next step is to give a “part of speech\" to each word. This would be something like a noun (substantiv), verb\n(verbum), or preposition (præposition). This tells us the function each particular word has. dapipe works generally\nwell, though for social media text, I recommend structbilty,2 which is more resilient [10, 11] to the unusual words\nand noise found in this colloquial setting [12].\nOnce the words and their classes are known, one may continue by parsing the words in the sentence. The dapipe\ntool uses the Universal Dependencies schema3 for this, ﬁnding the main grammatical relations between each word to\nconstruct a dependency tree that links every word in the sentence together.\nAll these tasks are achieved by using UDPipe [13] based on the universal dependencies [14] data for Danish. Sample\noutput follows.\nleon@blade:~/dapipe$ ./dapipe dktest.txt\n...\n# text = Derfor presser EU-siden hårdt på, for at amerikanerne ikke saboterer aftalen, der\nblandt andet lader udenlandske observatører inspicere iranske atomanlæg.\n1 Derfor derfor ADV _ _ 2 advmod _ _\n2 presser presse VERB _ Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Act 0 root _ _\n3 EU-siden EU-sid NOUN _ Definite=Def|Gender=Com|Number=Sing 2 nsubj _ _\n4 hårdt hårdt ADV _ Degree=Pos 2 advmod _ _\n5 på på ADP _ AdpType=Prep 4 case _ SpaceAfter=No\n6 ,,PUNCT _ _ 2 punct _ _\n7 for for ADP _ AdpType=Prep 11 mark _ _\n8 at at SCONJ _ _ 11 mark _ _\n9 amerikanerne amerikaner NOUN _ Definite=Def|Gender=Com|Number=Plur 11 nsubj _ _\n10 ikke ikke ADV _ _ 11 advmod _ _\n11 saboterer sabotere VERB _ Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Act 2 advcl _ _\n1A size that works well for NER in English [8]\n2See https://github.com/bplank/bilstm-aux\n3See https://universaldependencies.org/introduction.html\n2\n12 aftalen aftale NOUN _ Definite=Def|Gender=Com|Number=Sing 11 obj _ SpaceAfter=No\n13 ,,PUNCT _ _ 12 punct _ _\n14 der der PRON _ PartType=Inf 17 nsubj _ _\n15 blandt blandt ADP _ AdpType=Prep 17 advmod _ _\n16 andet anden PRON _ Gender=Neut|Number=Sing|PronType=Ind 15 fixed _ _\n17 lader lade VERB _ Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Act 12 acl:relcl _ _\n18 udenlandske udenlandsk ADJ _ Degree=Pos|Number=Plur 19 amod _ _\n19 observatører observatør NOUN _ Definite=Ind|Gender=Com|Number=Plur 17 obj _ _\n20 inspicere inspicere ADV _ Degree=Cmp 21 advmod _ _\n21 iranske iransk ADJ _ Degree=Pos|Number=Plur 22 amod _ _\n22 atomanlæg atomanlæg NOUN _ Definite=Ind|Gender=Neut|Number=Plur 17 obj _ SpaceAfter=No\n23 . . PUNCT _ _ 2 punct _ SpacesAfter=\\n\n4\nSummary\nThis note describes a free toolkit for basic natural language processing. The two tools daner and dapipe are built on\nfreely-available resources and maintaned at ITU Copenhagen.\nFeedback, requests, and so on can be addressed to the ITU NLP research group: http://nlp.itu.dk/\nReferences\n[1] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. The\nStanford CoreNLP natural language processing toolkit. In Proceedings of 52nd annual meeting of the Association\nfor Computational Linguistics, pages 55–60. Association for Computational Linguistics, 2014.\n[2] Jenny Rose Finkel, Trond Grenager, and Christopher Manning. Incorporating non-local information into infor-\nmation extraction systems by Gibbs sampling. In Proceedings of the 43rd annual meeting on association for\ncomputational linguistics, pages 363–370. Association for Computational Linguistics, 2005.\n[3] Leon Derczynski and Kenneth S Bøgh. DKIE: Open source information extraction for Danish. In Proceedings\nof the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational\nLinguistics, pages 61–64. Association for Computational Linguistics, 2014.\n[4] Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, Genevieve\nGorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Leon Derczynski, et al. Developing language pro-\ncessing components with GATE version 8.0. University of Shefﬁeld Department of Computer Science, 2012.\n[5] Anders Johannsen, Héctor Martínez Alonso, and Barbara Plank. Universal dependencies for danish. In Proc.\nInternational Workshop on Treebanks and Linguistic Theories, page 157, 2015.\n[6] Matthias T Kromann, Line Mikkelsen, and Stine Kern Lynge. Danish dependency treebank. In Proc. Interna-\ntional Workshop on Treebanks and Linguistic Theories, pages 217–220, 2003.\n[7] Leon Derczynski and Sean Chester. Generalised brown clustering and roll-up feature generation. In Proc. AAAI\nConference on Artiﬁcial Intelligence, 2016.\n[8] Leon Derczynski, Sean Chester, and Kenneth S Bøgh. Tune your brown clustering, please. In Proc. Interna-\ntional Conference Recent Advances in Natural Language Processing, RANLP, volume 2015, pages 110–117.\nAssociation for Computational Linguistics, 2015.\n[9] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efﬁcient text classiﬁca-\ntion. arXiv preprint arXiv:1607.01759, 2016.\n[10] Barbara Plank and Željko Agi´c. Distant supervision from disparate sources for low-resource part-of-speech\ntagging. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages\n614–620. Association for Computational Linguistics, 2018.\n[11] Barbara Plank, Anders Søgaard, and Yoav Goldberg. Multilingual Part-of-Speech Tagging with Bidirectional\nLong Short-Term Memory Models and Auxiliary Loss. In ACL 2016, arXiv preprint arXiv:1604.05529, 2016.\n[12] Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke Van Erp, Genevieve Gorrell, Raphaël Troncy, Jo-\nhann Petrak, and Kalina Bontcheva. Analysis of named entity recognition and linking for tweets. Information\nProcessing & Management, 51(2):32–49, 2015.\n[13] Milan Straka, Jan Hajic, and Jana Straková. UDPipe: Trainable Pipeline for Processing CoNLL-U Files Perform-\ning Tokenization, Morphological Analysis, POS Tagging and Parsing. In Proc. LREC, 2016.\n3\n[14] Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning,\nRyan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al. Universal dependencies v1: A multilin-\ngual treebank collection. In Proceedings of the Tenth International Conference on Language Resources and\nEvaluation (LREC 2016), pages 1659–1666, 2016.\n4\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-06-27",
  "updated": "2019-07-26"
}