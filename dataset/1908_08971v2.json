{
  "id": "http://arxiv.org/abs/1908.08971v2",
  "title": "Deploying Technology to Save Endangered Languages",
  "authors": [
    "Hilaria Cruz",
    "Joseph Waring"
  ],
  "abstract": "Computer scientists working on natural language processing, native speakers\nof endangered languages, and field linguists to discuss ways to harness\nAutomatic Speech Recognition, especially neural networks, to automate\nannotation, speech tagging, and text parsing on endangered languages.",
  "text": "1 \n \n \nUsing technology to save endangered languages1 \nHow linguists and computer scientists came together in a retreat to explore ways to advance \nautomatic speech recognition for endangered languages \nHilaria Cruz and Joseph Waring   \nAbstract \nIn August 2018 a retreat in Quechee, Vermont, brought together computer scientists specializing \nin natural language processing, linguists, native speakers, and endangered language activists \nunder one roof. During the retreat, participants discussed ways to utilize the latest advances in \nAutomatic Speech Recognition, especially neural networks, to transcribe endangered languages \nand tackle the difficulties of transcribing natural language, addressing what is known as “the \nbottle neck” of language transcription. In a relaxed environment where work was mixed with \nfun, everyone who participated became friends quickly and interacted with collegiality, \nexhibiting great potential for future collaborations. \n1.0 Background \nAutomatic Speech Recognition (ASR) is a burgeoning technology with near limitless \npotential. Voice recognition, which was once considered science fiction, is now commonplace: \nAlexa, Siri, and OK Google live on countertops and bedside tables in our homes. Nevertheless, \nthis technology has been limited to major, dominant languages such as English, German, and \nSpanish.  \n \nIn the 19th and 20th centuries, nation states sought to stamp out linguistic and cultural \ndiversity, setting language decline in rapid motion all over the world. Most of the world's 7,000 \nlanguages will fall out of use by the end of this century if language loss continues at its current \nrate, limiting humanity’s ability “to appreciate the full creative capacities of the human mind” \n(Mithun 1998:189). \n \nThe rapid development of ASR technologies, especially of neural networks, could \nexpedite the transcription, translation, and linguistic annotation of endangered languages, \nproviding resources for research, revitalization, and promotion. Before this can be achieved, \nlinguists and computational scientists must overcome several technological and methodological \nobstacles. \n \nTraditional ASR models, such as Markov chains, forced alignment, and hidden Markov \nmodels, require vast quantities of training data to get a model running (Michaud et al. 2018). \nMost endangered languages lack large corpora, alphabets, or speakers for data elicitation. A great \nnumber of these languages only have a few speakers left. Manual transcription and annotation of \n                                                \n1 We are grateful to the Neukom Foundation, the Linguistics Program, and the Native American Program at \nDartmouth College for supporting the retreat. Likewise, many thanks to Michael Abramov and Oliver Adams for \ncomments on this paper.  \n2 \n \nspeech is time-consuming. As a native speaker of Chatino, it takes me on average 30 minutes to \ntranscribe one minute of text. For non-native speakers, the process could take much longer.  \n \nSpeakers of minority languages frequently forgo literacy in their native languages to \nbecome competent in the dominant language of their nation state, which makes it difficult to \nobtain corpora for ASR models of minority languages. The first author was literate in Spanish \nand English long before she could read and write Chatino, which did not have a working \nalphabet until the author was forty years old.  \n \nResearchers of dominant languages, such as Spanish, do not face the same problem. For \ninstance, Carlos Hernandez Mena, a computer scientist from UNAM who participated in the \nretreat related that he uses students doing their compulsory social service to transcribe and edit \ndata that Carlos downloads from Youtube (Hernández Mena & Herrera 2017). This workflow \nhas allowed him to acquire massive corpora of Mexican Spanish. Researchers of endangered \nlanguages do not have this same privilege. \n \nIn contrast, transcription of endangered languages is a long and roundabout process: a \nfield linguist, usually with Western training, collaborates with a language consultant; they listen \nand discuss the sounds of the language together; the linguist makes notes and formulates \nquestions; and the language consultant repeats those sounds over and over again. \n \nBecause of these obstacles, cross-disciplinary dialogue between linguists and computer \nscientists is a necessity. Computer scientists developing ASR models often do not understand the \nmany difficulties that field linguists face: conflict-ridden locales, malaria outbreaks, or a severe \nlack of remaining language consultants. On the other hand, linguists are frequently unaware of \nthe ASR technologies available to them. \n \nHigh-tech industries do not develop ASR models for lesser-studied languages because \nthey are not deemed profitable, while computational linguists often cite pressure to get university \ntenure as a reason to forgo research on minority languages. What has been written about tools for \n“low resource languages” in the literature of computer science focuses on the needs and desires \nof Western linguists, with little mention of native speakers and their role in the advancement of \nthese tools. Collaborative conversations have not taken place in part because native speakers lack \nthe influence, the funding, and the connections to convene researchers with a common vision. \nMost conversations about ASR take place in formal settings, such as conferences, workshops, \nand forums, and are not well attended by native speakers.  \n \nThe need to automate the transcription of Chatino became more urgent for the first author \nwhen she began to transcribe audio recordings that Lynn Hou, an Assistant professor of \nLinguistics at the University of California Santa Barbara, made in San Juan Quiahije by working \nwith families of deaf children. Lina is deaf and relies on transcription of any spoken language \nshe encounters. \n  \nThe author endeavored to provide Lina with careful annotations of the Chatino materials \nshe collected so that she could have reliable data to analyze. The task was extremely time-\nconsuming and laborious. The author found herself typing the same words over and over again, \n3 \n \nand she yearned to be able to automate the process. She began to ask linguists what it would take \nto automate the transcription of Chatino. She was told that most said ASR was not possible for \nminority languages because they lacked large corpora to feed the ASR models.  \n \nThis question led her to collaborate with Damir and Malgorzata Cavar from Indiana \nUniversity. By reading aloud numerous previously transcribed texts, they developed the first \ncorpus of Chatino texts for ASR training (Cavar et al. 2016). This corpus has a creative \ncommons license, meaning that anyone can download and use it. When the author became a \nNeukom Fellow at Dartmouth College, she set out to improve and expand on this corpus, while \nlooking for ways to develop ASR for Chatino and other minority languages.  \n \nIn another part of the world, linguist Alexis Michaud, who works with Yongning Na, a \nlanguage of Southwest China, had similar goals: to automate the transcription process of the Na \nlanguages (Michaud et al. 2018). He designed his recordings so that they could eventually be \nused for ASR training. He began a successful collaboration with Oliver Adams, a computer \nscientist based out of the Melbourne University, who is now a Postdoctoral Fellow at Johns \nHopkins University.  \n \nAlong the way, Adams developed an open-source ASR toolkit called Persephone, which \nrelies on neural networks, and quickly began to yield promising results in the transcription of Na. \nThe tool was yielding a 20% error rate, and Michaud began to deploy Persephone into his \nlinguistic workflow (Adams et al. 2018).They also found that Persephone could attain reasonable \naccuracy for a single speaker with as little as thirty minutes of data—an auspicious sign for \nendangered languages (Adams et al. 2018). \n \nNext, Michaud and Adams wished to test the model on a comparable tonal language, and \nthey found their way to the Chatino corpus in Global Open Resources and Information for \nLanguage and Linguistics Analysis. They invited me to evaluate some of Persephone’s output \n(Adams et al. 2018), and to my great surprise, the system performed well for Chatino. At this \ntime, Persephone is only accessible to computer scientists, requiring an interface overhaul to \nreach a broader audience. Seeing the results from the cross-comparation of Yongning Na and \nChatino, motivated the first author to continue seeking collaborations with specialists in Natural \nlanguage processing to continue advancing and improving ASR for Chatino and other indigenous \nlanguages of the Americas. This is how the idea of the retreat began. \n2.0 The retreat \nThe William H. Neukom Foundation at Dartmouth College advocates for and funds \ninterdisciplinary working groups to foster cross-disciplinary dialogue to advance scientific \nresearch. Often organized through “retreats,” scholars with a common mission come together in \na friendly and intimate place—usually the Upper Valley in New Hampshire and Vermont—to \ndiscuss solutions to a problem or a question that they have been pondering. \n \nDaniel Rockmore, the dean of sciences at Dartmouth College and director of the Neukom \nInstitute, encouraged the first author to host a retreat to discuss the development of ASR for \nendangered languages. With Rockmore’s support, she proceeded to invite computer scientists, \nlinguists, native speakers, and language activists to join me in Quechee, Vermont, where we \n4 \n \nwould discuss ways to advance ASR for lesser-studied languages.2 The event took place from \nJuly 12-14, 2018. \n \nThe twenty people who participated in the retreat came from John Hopkins, Carnegie \nMellon, Yale University, University of North Texas, University of Texas at Austin, Universidad \nAutonoma de Mexico, and the Centro de Investigaciones y Estudios Superiores en Antropología, \nMexico. The meeting struck a balance between engineers, linguists, native speakers, and \nlanguage activists. \n \nAll of the participants stayed in a ski lodge called the Owl's Nest. The environment was \nlow-key and relaxed. There were no PowerPoint presentations, just everyone gathered together in \nthe living room. During catered meal breaks, participants organically broke into mixed groups \nand carried on the conversations of the day. We spent the first full day with introductions, \nsharing research interests, and setting an agenda for the weekend. Before dinner, we took an \nexcursion paddling in canoes on the Connecticut River. The second day was marked with more \ndetailed descriptions of everyone's research and concluded the day to a visit to Vermont Institute \nof Natural Science, a raptor education center and bird sanctuary. \n \nOn the second day, we shared and discussed what resources we had at our disposal for \nmodel training. Questions ranged from what languages participants had worked on and how \nmany hours of recordings we had elicited, to more technical matters, such as recording format, \nfile types (word, PDF, and ELAN),3 to demographic information, including the number, age, and \ngender of speakers in the corpus.   \n \nParticipants spoke or studied languages from six language families and four continents: \nfrom Australia: Nyulnyulan (Bardi) Pama-Nyungan (Djambarrpuyngu, Djapu); from Africa: \nMasso (Burkina Faso); from India: Tibeto-Burma (Manipuri and 17 others); and from the \nAmericas: Otomanguean (Chatino, Otomi); Maya (Tzetsal, Tzotzil, and Mocho). Many of these \nlanguages are severely endangered. The Australian languages, for instance, have an average of \nfour to five speakers, while one of the two varieties of Mocho Maya languages have one speaker \neach.  \n \nBoth the linguists and the computer scientists were eager to learn about each other’s workflows \nand the steps needed to complete a task. While most field linguists undergo an iterative process \nof data collection and documentation, computer scientists usually begin their research by reading \nacademic papers and then replicating what they learn.  \n \n \n \n \n \n \n \n \n                                                \n2 ASREL retreat: https://sites.dartmouth.edu/neukom/asrel/) \n3 A software used by linguists for transcription, translation, and annotation. \n5 \n \n \n \n \n \n \nThough collaboration was our goal, we quickly learned that ASR models require \nsubstantial technical expertise to be used effectively. Persephone, for example, has preliminary \nsupport for ELAN files—which is great for linguists— and a web-API is under development so \nthat Persephone might be used by a broader audience. \n \nMoreover, we learned that there is no theoretical obstacle to creating an interface that \nwould allow a linguist to upload speech and transcriptions for model training. It is largely a \nmatter of having a professional software engineer develop the tool. Such automation has the \npotential to improve the rate of language documentation. Automating the transcription process \nyields three beneficial results. First, there is a potential for greater consistency in the \ntranscription. Second, the researcher becomes less of a transcriber and more of an editor. Finally, \nautomated transcriptions can provide fresh insight into the nature of the language under study.  \n \nWe reached several milestones during our retreat. It was the first meeting of its kind to be \nconvened by a native speaker of an endangered language. In the past, there has been little interest \nin ASR projects for minority languages. Native speakers have been historically denied the right \nto become literate in their languages, and as a result, those minority languages are under-\nrepresented and under-resourced in academia.  \n \nNative speakers need to be involved in conversations about ASR, as they bring \ncommunity-oriented perspectives and accountability that are often overlooked or ignored by non-\n6 \n \nnative speaker linguists and computer scientists. The event was a resounding success. It was \nproductive and enjoyable. People left eager to continue the conversation on how improve and \npromote ASR for endangered languages.  \n \nReferences \n \nAdams, Oliver & Cohn, Trevor & Neubig, Graham & Cruz, Hilaria & Bird, Steven & Michaud, \nAlexis. 2018. Evaluating phonemic transcription of low-resource tonal languages for \nlanguage documentation. Proceedings of LREC 2018 (Language Resources and \nEvaluation Conference).. https://halshs.archives-ouvertes.fr/halshs-01709648. \n \nĆavar, Małgorzata E. &Cavar, Damir & Cruz, Hilaria. 2016. Endangered Language \nDocumentation: Bootstrapping a Chatino Speech Corpus, Forced Aligner, ASR. LREC. \n4004–4011. \n \nHernández Mena, Carlos Daniel & Herrera, Abel. 2017. CIEMPIESS (Corpus de Investigación \nen Español de México del Posgrado de Ingeniería Eléctrica y Servicio Social) Light. \nLinguistic Data Consortium. (https://catalog.ldc.upenn.edu/LDC2017S23). \n \nMichaud, Alexis & Adams, Oliver & Cohn, Trevor A. & Neubig, Graham & Guillaume, \nSéverine. 2018. Integrating automatic transcription into the language documentation \nworkflow: experiments with Na data and the Persephone toolkit. Language \nDocumentation and Conservation 12. 393-429.  \n \nMithun, Marianne. 1998. The significance of diversity in language endangerment and \npreservation. In L. Grenoble and L. Whaley (eds.), Endangered languages: Current \nIssues and Future Prospects, 163-191. Cambridge: Cambridge University Press. \n \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-08-23",
  "updated": "2019-09-01"
}