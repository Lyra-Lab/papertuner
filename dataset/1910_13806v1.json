{
  "id": "http://arxiv.org/abs/1910.13806v1",
  "title": "Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition",
  "authors": [
    "Zheng Lian",
    "Jianhua Tao",
    "Bin Liu",
    "Jian Huang"
  ],
  "abstract": "Prior works on speech emotion recognition utilize various unsupervised\nlearning approaches to deal with low-resource samples. However, these methods\npay less attention to modeling the long-term dynamic dependency, which is\nimportant for speech emotion recognition. To deal with this problem, this paper\ncombines the unsupervised representation learning strategy -- Future\nObservation Prediction (FOP), with transfer learning approaches (such as\nFine-tuning and Hypercolumns). To verify the effectiveness of the proposed\nmethod, we conduct experiments on the IEMOCAP database. Experimental results\ndemonstrate that our method is superior to currently advanced unsupervised\nlearning strategies.",
  "text": "Unsupervised Representation Learning with Future Observation Prediction\nfor Speech Emotion Recognition\nZheng Lian1,3, Jianhua Tao1,2,3, Bin Liu1, Jian Huang1,3\n1National Laboratory of Pattern Recognition, CASIA, Beijing, China\n2CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China\n3School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n{zheng.lian, jhtao, liubin, jian.huang}@nlpr.ia.ac.cn\nAbstract\nPrior works on speech emotion recognition utilize various un-\nsupervised learning approaches to deal with low-resource sam-\nples. However, these methods pay less attention to modeling the\nlong-term dynamic dependency, which is important for speech\nemotion recognition. To deal with this problem, this paper com-\nbines the unsupervised representation learning strategy – Fu-\nture Observation Prediction (FOP), with transfer learning ap-\nproaches (such as Fine-tuning and Hypercolumns). To verify\nthe effectiveness of the proposed method, we conduct experi-\nments on the IEMOCAP database. Experimental results demon-\nstrate that our method is superior to currently advanced unsuper-\nvised learning strategies.\nIndex Terms: speech emotion recognition, unsupervised learn-\ning, Future Observation Prediction, transfer learning\n1. Introduction\nObtaining large amounts of realistic data is currently challeng-\ning and expensive in speech emotion recognition [1]. Com-\npared with automatic speech recognition which has thousands\nof hours of transcribed speech, databases annotated in emo-\ntional categories are still scarce. For example, the eNTERFACE\n[2] and EMODB [3] databases contain less than 1000 samples.\nAlthough the IEMOCAP [4] and FAU-AIBO [5] databases con-\ntain 10039 and 13348 samples respectively, the distribution of\nemotional categories is extremely unbalanced.\nPrior works on speech emotion recognition utilize various\nmethods to deal with low-resource training samples, including\nunsupervised representation learning [6, 7] and transfer learning\n[8, 9]. Unsupervised representation learning takes full advan-\ntage of the information from unlabeled data. It does not utilize\nthe label information but aims to learn robust representations\nthat can capture intrinsic structures of the data. While transfer\nlearning makes use of additional labeled data from a different\nbut related task. Its main idea is to share the “knowledge” from\nthe source task to the target task.\nMost of existing unsupervised learning approaches pro-\nvide salient representations, leading to notable improvement for\nspeech emotion recognition [6, 7, 10, 11]. A mainstream ap-\nproach of unsupervised learning is to train autoencoders (AE)\n[12, 13], including denoising autoencoders (DAE) [14], varia-\ntional autoencoders (VAE) [15] and autoencoders with ladder\nstructures [16]. These methods [12, 13] take the whole input\ninto account, aiming to learn intermediate feature representa-\ntions that can reconstruct the input. However, they pay less at-\ntention to modeling the long-term dynamic dependency, which\nis important for speech emotion recognition [17, 18].\nTo deal with this problem, this paper focuses on another\npopular unsupervised learning strategy, whose main idea is to\npredict future, missing or contextual information [19, 20, 21,\n22]. For convenience, it is marked as Future Observation Pre-\ndiction (FOP). As FOP needs to infer the global information\nin the prediction process, capturing long-range dynamic infor-\nmation is also important [19, 20]. The main intuition behind\nFOP is to learn the representations that encode the underlying\nshared information between different parts of the signal.\nIn\ntime series, FOP uses next step prediction to exploit the local\nsmoothness of the signal, and discards low-level information\nand noise that are local. When predicting further in the future,\nthe amount of shared information become much lower, and the\nFOP model is able to infer more global structures [19]. Re-\ncent works have successfully used the idea behind FOP to learn\nrobust and generic representations in audio, textual and visual\ndomains [19, 20, 21, 22]. Mikolov et al. [20] learned word rep-\nresentations by predicting neighboring words. Zhang et al. [21]\nlearned image representations by predicting color for grey-scale\nimages. Oord et al. [19] proposed a general framework to learn\neffective representations for multiple domains.\nTo capture long-term dynamic dependencies, we propose\nto use the self-attention mechanism [23] for FOP. As the self-\nattention mechanism can provide an opportunity for injecting\nglobal context of the whole sequence into each input frame, it\ncan attend to longer sequences than many typical RNN-based\nmodels [23, 24].\nFOP can learn discriminative representations from the in-\nputs. To distill the “knowledge” from FOP to emotion recog-\nnition, two transfer learning approaches are utilized, including\nFine-tuning [9, 25] and Hypercolumns [26]. Fine-tuning [9, 25]\nis an approach that ﬁne-tunes either the last or several of the\nlast layers in the pre-trained model and leaves the remaining\nlayers unchanged. While Hypercolumns [26] concatenate em-\nbeddings at different layers in a pre-trained model. The target\ntask can beneﬁt from Hypercolumns as they connect low-level\nfeatures with high-level semantics.\nThis paper combines the unsupervised representation learn-\ning strategy – FOP, with transfer learning approaches (such as\nFine-tuning and Hypercolumns) to improve the performance of\nspeech emotion recognition. The main contributions of this pa-\nper lie in three aspects: 1) to deal with long-range temporal\ndependencies, we propose to utilize the self-attention mecha-\nnism for FOP; 2) to share the “knowledge” from FOP to speech\nemotion recognition, two transfer learning strategies are eval-\nuated; 3) our proposed method is superior to other currently\nadvanced unsupervised learning strategies. To the best of our\nknowledge, it is the ﬁrst time that FOP is utilized as the unsu-\npervised learning strategy to improve the performance of speech\nemotion recognition.\narXiv:1910.13806v1  [eess.AS]  24 Oct 2019\n2. Proposed Method\nOur training procedure consists of two stages. In the ﬁrst stage,\nwe train a model for FOP to learn discriminative representa-\ntions from the inputs, which is marked as the FOP model. In\nthe second stage, to share the “knowledge” from FOP to speech\nemotion recognition, we take advantage of transfer learning\napproaches including Fine-tuning (in Fig.\n1(a)) and Hyper-\ncolumns (in Fig. 1(b)).\nFigure 1: Overall structure of the proposed framework. (a)\nFOP combined with Fine-tuning: Through unsupervised pre-\ntraining, the FOP model can learn discriminative representa-\ntions from the inputs. Then the output layer of the pre-trained\nFOP model is replaced by the combination of a global average\npooling (GAP) layer and an emotion-speciﬁc projection layer.\nFinally, the whole model is ﬁne-tuned for speech emotion recog-\nnition. (b) FOP combined with Hypercolumns: Bottleneck fea-\ntures are extracted from the pre-trained FOP model, followed\nwith additional classiﬁers for speech emotion recognition.\n2.1. Unsupervised pre-training\nFOP can learn discriminative representations from the inputs.\nThis paper utilizes a generative model operating on audio fea-\ntures for FOP. As depicted in Fig. 1, given an 80-dimensional\nmel-scale spectrogram F = [f1, f2, ..., fT ], where T is the\nnumber of frames and ft is the features at timestep t. The joint\nprobability of a feature sequence F is factorized as a product of\nconditional probabilities, which is computed as follows:\nP(F) =\nT\nY\nt=1\nP(ft|f1, f2, ..., ft−1; θ)\n(1)\nwhere the conditional probability P is modeled using neural\nnetworks with parameters θ. Each ft is therefore conditioned\non the features at all previous timesteps f1, f2, ..., ft−1.\nCapturing long-term dynamic dependencies is important\nfor FOP. Several architectures are evaluated, including convo-\nlutional neural networks (CNNs) [27], long-short term memory\n(LSTM) [28] and the masked multi-head self-attention mecha-\nnism [23]. We ﬁnd that the last one is more suitable for FOP.\nAs the self-attention mechanism can inject global context of the\nwhole sequence into each input frame, it can attend to longer\nsequences than other two architectures. Therefore, the condi-\ntional probability distribution is modeled by a stack of masked\nmulti-head self-attention mechanisms [23].\nAs depicted in Fig. 1, the FOP model has an input em-\nbedding layer and L identical attention layers. The input em-\nbedding layer injects position information into the inputs. The\nattention layer captures long-term dynamic dependencies. We\ndescribe these components below.\nFigure 2: Two components in the FOP model. (a) Input embed-\nding layer: The input embeddings are the sum of correspond-\ning feature embeddings and position embeddings. (b) Atten-\ntion layer: The attention layer has two sub-modules: a masked\nmulti-head self-attention layer [23] and a feed forward layer.\n2.1.1. Input embedding layer\nWe ﬁrst describe the input embedding layer, illustrated in Fig.\n2(a). The FOP model relies on the self-attention mechanism,\nwhich contains no recurrence and no convolution networks. If\nwe shufﬂe the inputs F = [f1, f2, ..., fT ], we will get the same\noutput [23]. To take the order of the sequence into considera-\ntion, we inject the position information into F.\nThis paper utilizes position embeddings the same with that\nin [23]. These embeddings are computed as follows:\nPE(pos,2i) = sin(pos/100002i/dmodel)\n(2)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\n(3)\nwhere pos is the time step index, i is the dimension and dmodel\nis the vector dimension of each frame.\nThe input embeddings zt are the sum of corresponding fea-\nture embeddings pt and position embeddings PEt.\npt = Weft\n(4)\nzt = pt + PEt\n(5)\nwhere We is the feature embedding matrix and ft is the inputs\nat timestep t.\n2.1.2. Attention layer\nThe input embeddings for all timesteps h0 = [z1, z2, ..., zT ] are\npassed into a stack of L identical attention layers. The outputs\nof each attention layer are marked as hl, l ∈[1, L]. As depicted\nin Fig. 2(b), each attention layer has two sub-modules. The\nﬁrst is a masked multi-head self-attention mechanism [23], and\nthe second is a feed forward layer. Then we employ a residual\nconnection [29] around two sub-modules, followed with layer\nnormalization [30]. Finally, the outputs of the last layer hL is\nutilized to predict the next frame’s features.\nThe main ingredient of the attention layer is the masked\nself-attention mechanism [23]. With the help of this mecha-\nnism, we make sure that the prediction P(ft|f1, f2, ..., ft−1) at\ntimestep t cannot depend on any of the future timesteps features\nft, ft+1, ..., fT .\n2.2. Supervised transfer learning\nTo take advantage of discriminative representations that are\nlearned from FOP, two transfer learning approaches are eval-\nuated, including Fine-tuning and Hypercolumns.\n2.2.1. Fine-tuning\nAs depicted in Fig. 1(a), the output layer of the pre-trained\nFOP model is replaced by the combination of a global average\npooling (GAP) layer and an emotion-speciﬁc projection layer.\nThen the whole model is ﬁne-tuned for speech emotion recog-\nnition. Therefore, only parameters in the projection layer is\ntrained from scratch in Fine-tuning.\nWe assume that a labeled sample consists of a mel-scale\nspectrogram F, along with an emotional label y. We ﬁrst pass\nF into the FOP model to obtain the outputs of the last attention\nlayer hL. Then hL is fed into the GAP layer and the projection\nlayer with parameters Wy to predict the emotional state y. This\ngives us the following objective to maximize:\nP(y|F) = softmax(GAP(hL)Wy)\n(6)\n2.2.2. Hypercolumns\nAs depicted in Fig. 1(b), the mel-scale spectrogram F and the\noutputs of each attention layer hl, l ∈[1, L] are utilized as dif-\nferent levels of abstraction for the waveform. As F has the same\nfeature dimensionality as hl, l ∈[1, L], we further add and\nconcatenate these representations together, which are marked\nas hadd and hconcat, respectively.\nhadd = F +\nL\nX\nl=1\nhl\n(7)\nhconcat = Concat(F, h1, ..., hL)\n(8)\n3. Databases\nIEMOCAP: The IEMOCAP [4] database contains about 12.46\nhours of audiovisual data. There are ﬁve sessions with two ac-\ntors each (one female and one male) and each session has dif-\nferent actors. Each session has been segmented into utterances,\nwhich are labeled into ten discrete labels (e.g., happy, sad, an-\ngry). To compared with other currently advanced approaches\n[31, 32], we form a four-class emotion classiﬁcation dataset\ncontaining angry, happy, sad and neutral, where happy and\nexcited categories are merged into a single happy category.\nVCTK: The CSTR VCTK Corpus (Center for Speech\nTechnology Voice Cloning Toolkit) [33] consists of 44 hours of\ndata from 109 native speakers of English with various accents.\nEach speaker reads out about 400 sentences, which are selected\nfrom a newspaper plus the Rainbow Passage and an elicitation\nparagraph.\n4. Experiments and Results\n4.1. Experimental setup\nIn the experiments, we train a 2-layer FOP model with masked\nself-attention heads (256 dimensional states and 4 attention\nheads). For the feed forward layer in the FOP model, 513 di-\nmensional inner states are chosen. We use the Adam [34] opti-\nmization scheme with a learning rate of 0.001 and a batch size\nof 32. To prevent over-ﬁtting, we use dropout [35] with p = 0.2\nand early stopping [36]. To alleviate the impact of the weight\ninitialization, each conﬁguration is tested 20 times. To consider\nthe unbalanced number of samples between classes, weighted\naccuracy (WA) is chosen as our evaluation criterion.\nSource task (FOP): We utilize all samples (without labels)\nin the IEMOCAP and VCTK databases.\nTarget task (speech emotion recognition): We conduct\nexperiments on the IEMOCAP dataset. There are ﬁve sessions\nand each session has different actors. To ensure that models are\ntrained and tested on speaker independent sets, utterances from\nthe ﬁrst 2 speakers are used as the testing set and utterances\nfrom other speakers are used as the training set.\n4.2. Evaluation of FOP combined with Fine-tuning\nThis section discusses the effectiveness of FOP combined with\nFine-tuning. As the FOP model can be pre-trained on differ-\nent datasets, three pre-training settings are discussed including\n“VCTK”, “IEMOCAP” and “None”.\n“VCTK” and “IEMO-\nCAP” represent that the FOP model is pre-trained on the VCTK\ndataset and the IEMOCAP dataset, respectively. While the pre-\ntraining process is not adopted in “None”.\nTable 1: Performance of FOP combined with Fine-tuning under\ndifferent pre-training settings.\nPre-training Settings\nWA(%)\nNone: FOP without pre-training\n65.45\nVCTK: FOP pre-trained on VCTK\n67.37\nIEMOCAP: FOP pre-trained on IEMOCAP\n68.14\nExperimental results in Table 1 show that the pre-training\nprocess can improve the performance of speech emotion recog-\nnition. It reveals that after the pre-training process, the FOP\nmodel that learns to predict future information can also learn\ndiscriminative representations from waveforms.\nThese high-\nlevel representations can be utilized to improve the recognition\nperformance through Fine-tuning. What’s more, experimental\nresults reveal that “IEMOCAP” gains higher classiﬁcation accu-\nracy than “VCTK”. As speech emotion recognition is evaluated\non the IEMOCAP dataset, the FOP model pre-trained on the\nIEMOCAP dataset can match with speech emotion recognition\nbetter than the FOP model pre-trained on the VCTK dataset. It\nindicates that, in transfer learning, the similarity of the source\ntask and the target task can improve the performance of the tar-\nget task.\nTherefore, the FOP model is pre-trained on the unlabeled\nIEMOCAP dataset in the following experiments.\n4.3. Evaluation of FOP combined with Hypercolumns\nHypercolumns extract the outputs of each attention layer in\nthe FOP model, followed with additional classiﬁers for speech\nemotion recognition.\nIn this section, we discuss the perfor-\nmance of FOP combined with Hypercolumns.\nMultiple features (in Sec 2.2) are investigated. Since the\nmel-scale spectrogram F is extracted without the FOP model,\nit is considered to be our baseline. In the 2-layer FOP model,\nthe outputs of each attention layer are marked as h1 and h2,\nrespectively. As F has the same feature dimensionality as h1\nand h2, we add and concatenate these representations together,\nwhich are marked as hadd and hconcat, respectively. Besides\nvarious features, we also evaluate different classiﬁers for speech\nemotion recognition, including support vector machine (SVM),\nrandom forest (RF) and attention-based LSTM [17].\nTable 2: Weighted accuracy (%) of FOP combined with Hy-\npercolumns under different combinations of features and clas-\nsiﬁers. (Note: Baseline F is extracted without the FOP model.)\nFeatures\nSVM\nRF\nA-LSTM\nBest\nF(Baseline)\n61.04\n62.57\n58.93\n62.57\nh1\n63.92\n56.66\n64.68\n64.68\nh2\n63.53\n57.39\n62.38\n63.53\nhadd\n63.72\n58.35\n64.88\n64.88\nhconcat\n64.11\n58.16\n65.54\n65.54\nExperimental results in Table 2 demonstrate that F gains\nthe worst performance, 62.57%, among all features. It indicates\nthat FOP can learn robust and generic representations (such as\nh1, h2) from the inputs F. These representations (such as h1,\nh2) are more effective than the inputs F for speech emotion\nrecognition. Therefore, we can conclude that transferring the\n“knowledge” from FOP into speech emotion recognition can\nimprove recognition performance.\nFurthermore, experimental results in Table 2 show that hadd\nand hconcat gain better performance than h1, h2 and F in\nspeech emotion recognition. F, h1 and h2 learn different lev-\nels of representations from the waveforms. As these represen-\ntations are relevant to emotion recognition, hadd and hconcat\nthat combine these emotion-related representations together can\ngain better performance. What’s more, we ﬁnd that hconcat is\nsuperior to hadd. The reason lies in that hadd may result in the\nloss of original information in h1, h2 and F. While hconcat not\nonly can preserve original information, but also can combine\nthese information properly.\n4.4. Comparison to other advanced approaches\nTo show the effectiveness of the proposed method, we com-\npare our method with currently advanced approaches through\nthe ﬁve-folder cross validation.\nTable 3: Weighted accuracy (%) of currently advanced ap-\nproaches and the proposed approach on the IEMOCAP dataset.\nSingle-folder\nCross-validation\nNeumann et al. [31]\n—\n63.85\nLian et al. [32]\n—\n62.20\nDAE in [37]\n—\n56.40\nLadder network in [37]\n—\n59.10\nFOP+Hypercolumns\n65.54\n63.56\nFOP+Fine-tuning\n68.14\n65.03\nCompared with our proposed method, these approaches\n[31, 32] also utilized mel-scale spectrograms as inputs, and\nshowed promising results for speech emotion recognition. Neu-\nmann et al. [31] proposed an attentive CNN with multi-view\nlearning objective function for speech emotion recognition. The\nCNN learned the representations of the audio signal, while the\nattention layer computed the weighted sum of all the infor-\nmation extracted from different parts. Lian et al. [32] intro-\nduced the contrastive loss function for speech emotion recogni-\ntion. This loss function encouraged intra-class compactness and\ninter-class separability between different emotional categories.\nExperimental results in Table 3 demonstrate the effectiveness\nof the proposed method. Our method outperforms currently ad-\nvanced approaches [31, 32] in weight accuracy. It proves that\nFOP, which learns discriminative representations from the origi-\nnal inputs, can be utilized to improve the performance of speech\nemotion recognition.\nMeanwhile, we compare our method with other semi-\nsupervised and unsupervised learning strategies. Huang et al.\n[37] utilized semi-supervised learning with ladder networks,\nwhich outperformed unsupervised DAE above 2% for speech\nemotion recognition. Experimental results in Table 3 demon-\nstrate that our method shows above 4% performance improve-\nment over the ladder network in [37]. DAE and ladder structures\npay less attention to modeling long-term dynamic dependen-\ncies. However, temporal information is important for speech\nemotion recognition [17, 18].\nTherefore, our self-attention\nbased FOP model, which can capture long-term dynamic depen-\ndencies, is superior to other currently advanced unsupervised\nlearning strategies.\n5. Conclusions\nThis paper combines the unsupervised representation learning\nstrategy – FOP, with transfer learning approaches (such as Fine-\ntuning and Hypercolumns) for speech emotion recognition. To\nevaluate the effectiveness of the proposed method, we con-\nduct experiments on the IEMOCAP database.\nExperimental\nresults reveal that FOP can learn robust and discriminative rep-\nresentations from the mel-scale spectrograms. These represen-\ntations are more effective than the original mel-scale spectro-\ngrams for speech emotion recognition. As our method can cap-\nture long-term dynamic dependencies, it also outperforms the\nmainstream semi-supervised and unsupervised learning strate-\ngies (such as DAE and ladder structures) above 4% for speech\nemotion recognition.\nFuture investigations include a detailed analysis of input\nfeatures for FOP. Besides mel-scale spectrograms, other audio\nfeatures (e.g., Mel Frequency Cepstrum Coefﬁcients (MFCCs),\nLinear Predictive Codings (LPCs)) should be evaluated. Addi-\ntionally we aim to investigate and further improve the classi-\nﬁcation accuracy using other advanced ﬁne-tuning approaches\n(e.g., discriminative ﬁne-tuning in [38]). Finally, we will apply\nthe proposed method to other audio classiﬁcation tasks, such as\nkey word spotting and speaker identiﬁcation.\n6. Acknowledgements\nThis work is supported by the National Key Research & De-\nvelopment Plan of China (No.2017YFC0820602), the National\nNatural Science Foundation of China (NSFC) (No.61425017,\nNo.61831022, No.61773379, No.61771472), and the Strategic\nPriority Research Program of Chinese Academy of Sciences\n(No.XDC02050100).\n7. References\n[1] B. Schuller, A. Batliner, S. Steidl, and D. Seppi, “Recognising\nrealistic emotions and affect in speech: State of the art and lessons\nlearnt from the ﬁrst challenge,” Speech Communication, vol. 53,\nno. 9-10, pp. 1062–1087, 2011.\n[2] O. Martin, I. Kotsia, B. Macq, and I. Pitas, “The enterface’05\naudio-visual emotion database,” in 22nd International Conference\non Data Engineering Workshops (ICDEW’06).\nIEEE, 2006, pp.\n8–8.\n[3] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and\nB. Weiss, “A database of german emotional speech,” in Ninth Eu-\nropean Conference on Speech Communication and Technology,\n2005.\n[4] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,\nS. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional dyadic motion capture database,” Language\nresources and evaluation, vol. 42, no. 4, p. 335, 2008.\n[5] A. Batliner, C. Hacker, S. Steidl, E. N¨oth, S. D’Arcy, M. J. Rus-\nsell, and M. Wong, “” you stupid tin box”-children interacting\nwith the aibo robot: A cross-linguistic emotional speech corpus.”\nin Lrec, 2004.\n[6] J. Deng, R. Xia, Z. Zhang, Y. Liu, and B. Schuller, “Introducing\nshared-hidden-layer autoencoders for transfer learning and their\napplication in acoustic emotion recognition,” in 2014 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2014, pp. 4818–4822.\n[7] S. E. Eskimez, Z. Duan, and W. Heinzelman, “Unsupervised\nlearning approach to feature analysis for automatic speech emo-\ntion recognition,” in 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2018,\npp. 5099–5103.\n[8] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE\nTransactions on knowledge and data engineering, vol. 22, no. 10,\npp. 1345–1359, 2010.\n[9] X. Ouyang, S. Kawaai, E. G. H. Goh, S. Shen, W. Ding, H. Ming,\nand D.-Y. Huang, “Audio-visual emotion recognition using deep\ntransfer learning and multiple temporal models,” in Proceedings\nof the 19th ACM International Conference on Multimodal Inter-\naction.\nACM, 2017, pp. 577–582.\n[10] R. Xia and Y. Liu, “Using denoising autoencoder for emotion\nrecognition.” in Interspeech, 2013, pp. 2886–2889.\n[11] S. Ghosh, E. Laksana, L.-P. Morency, and S. Scherer, “Represen-\ntation learning for speech emotion recognition.” in Interspeech,\n2016, pp. 3603–3607.\n[12] C. Poultney, S. Chopra, Y. L. Cun et al., “Efﬁcient learning of\nsparse representations with an energy-based model,” in Advances\nin neural information processing systems, 2007, pp. 1137–1144.\n[13] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy\nlayer-wise training of deep networks,” in Advances in neural in-\nformation processing systems, 2007, pp. 153–160.\n[14] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Man-\nzagol, “Stacked denoising autoencoders: Learning useful repre-\nsentations in a deep network with a local denoising criterion,”\nJournal of machine learning research, vol. 11, no. Dec, pp. 3371–\n3408, 2010.\n[15] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”\narXiv preprint arXiv:1312.6114, 2013.\n[16] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko,\n“Semi-supervised learning with ladder networks,” in Advances in\nneural information processing systems, 2015, pp. 3546–3554.\n[17] C.-W. Huang and S. S. Narayanan, “Attention assisted discovery\nof sub-utterance structure in speech emotion recognition.” in IN-\nTERSPEECH, 2016, pp. 1387–1391.\n[18] J. Kim, G. Englebienne, K. P. Truong, and V. Evers, “To-\nwards speech emotion recognition” in the wild” using aggre-\ngated corpora and deep multi-task learning,” arXiv preprint\narXiv:1708.03920, 2017.\n[19] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748,\n2018.\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient esti-\nmation of word representations in vector space,” arXiv preprint\narXiv:1301.3781, 2013.\n[21] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,”\nin European Conference on Computer Vision. Springer, 2016, pp.\n649–666.\n[22] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual rep-\nresentation learning by context prediction,” in Proceedings of the\nIEEE International Conference on Computer Vision, 2015, pp.\n1422–1430.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n5998–6008.\n[24] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser,\nand N. Shazeer, “Generating wikipedia by summarizing long se-\nquences,” arXiv preprint arXiv:1801.10198, 2018.\n[25] S. Thrun and L. Pratt, Learning to learn.\nSpringer Science &\nBusiness Media, 2012.\n[26] B. Hariharan, P. Arbelez, R. Girshick, and J. Malik, “Hyper-\ncolumns for object segmentation and ﬁne-grained localization,”\nin IEEE Conference on Computer Vision and Pattern Recognition,\n2015, pp. 447–456.\n[27] Y. LeCun, Y. Bengio et al., “Convolutional networks for images,\nspeech, and time series,” The handbook of brain theory and neural\nnetworks, vol. 3361, no. 10, p. 1995, 1995.\n[28] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 770–778.\n[30] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv preprint arXiv:1607.06450, 2016.\n[31] M. Neumann and N. T. Vu, “Attentive convolutional neural net-\nwork based speech emotion recognition: A study on the impact of\ninput features, signal length, and acted speech,” INTERSPEECH,\n2017.\n[32] Z. Lian, Y. Li, J. Tao, and H. Jian, “Speech emotion recogni-\ntion via contrastive loss under siamese networks,” in International\nWorkshop on Affective Social Multimedia Computing, 2018.\n[33] J.\nYamagishi,\n“English\nmulti-speaker\ncorpus\nfor\ncstr\nvoice cloning toolkit,”\nURL http://homepages. inf. ed. ac.\nuk/jyamagis/page3/page58/page58. html, 2012.\n[34] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” arXiv preprint arXiv:1412.6980, 2014.\n[35] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural net-\nworks from overﬁtting,” The Journal of Machine Learning Re-\nsearch, vol. 15, no. 1, pp. 1929–1958, 2014.\n[36] L. Prechelt, “Automatic early stopping using cross validation:\nquantifying the criteria,” Neural Networks, vol. 11, no. 4, pp. 761–\n767, 1998.\n[37] J. Huang, Y. Li, J. Tao, Z. Lian, M. Niu, and J. Yi, “Speech emo-\ntion recognition using semi-supervised learning with ladder net-\nworks,” in 2018 First Asian Conference on Affective Computing\nand Intelligent Interaction (ACII Asia).\nIEEE, 2018, pp. 1–5.\n[38] J. Howard and S. Ruder, “Universal language model ﬁne-tuning\nfor text classiﬁcation,” in Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long\nPapers), vol. 1, 2018, pp. 328–339.\n",
  "categories": [
    "eess.AS",
    "cs.LG",
    "cs.SD"
  ],
  "published": "2019-10-24",
  "updated": "2019-10-24"
}