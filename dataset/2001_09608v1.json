{
  "id": "http://arxiv.org/abs/2001.09608v1",
  "title": "Some Insights into Lifelong Reinforcement Learning Systems",
  "authors": [
    "Changjian Li"
  ],
  "abstract": "A lifelong reinforcement learning system is a learning system that has the\nability to learn through trail-and-error interaction with the environment over\nits lifetime. In this paper, I give some arguments to show that the traditional\nreinforcement learning paradigm fails to model this type of learning system.\nSome insights into lifelong reinforcement learning are provided, along with a\nsimplistic prototype lifelong reinforcement learning system.",
  "text": "Some Insights into Lifelong Reinforcement Learning Systems\nChangjian Li 1\nAbstract\nA lifelong reinforcement learning system is a\nlearning system that has the ability to learn\nthrough trail-and-error interaction with the en-\nvironment over its lifetime. In this paper, I give\nsome arguments to show that the traditional rein-\nforcement learning paradigm fails to model this\ntype of learning system. Some insights into life-\nlong reinforcement learning are provided, along\nwith a simplistic prototype lifelong reinforcement\nlearning system.\n1. Introduction\nAn agent is an abstraction of a decision-maker. At each time\ninstance t, it receives an observation ot ∈O, and outputs an\naction at ∈A to be carried out in the environment it lives in.\nHere, O is the (ﬁnite) set of possible observations the agent\ncan receive, and A is the (ﬁnite) set of actions the agent can\nchoose from. An agent’s observation ot depends on the cur-\nrent environment state st ∈S through an agent observation\nfunction S →O, where S is the set of possible environment\nstates. The observation history ho\nt = (o1, o2..., ot) is the\nsequence of observations the agent has received till time t.\nLet Ho\nt be the set of possible observation histories of length\nt, the policy πt : Ho\nt →A at time t is deﬁned as the map-\nping from an observation history of length t to the action the\nagent will take. An agent’s behavior can thus be fully speci-\nﬁed by its policy across all timesteps π = (π1, π2, ..., πt, ...).\nThroughout the paper, it is assumed that an agent has a ﬁnite\nlifespan T.\n1.1. Scalar Reward Reinforcement Learning System\nWe are interested in agents that can achieve some goal. In\nreinforcement learning, a goal is expressed by a scalar signal\nrt ∈R called the reward. The reward is dependent on the\nagent’s observation history, and is assumed to be available\nto the agent at each timestep in addition to the observation\n1Department of Electrical and Computer Engineering, Uni-\nversity of Waterloo, Canada. Correspondence to: Changjian Li\n<changjian.li@uwaterloo.ca>.\nPreliminary work. Under Review.\not. Our aim is to ﬁnd policies that maximize the expected\ncumulative reward an agent receives over its lifetime:\nmax\nπ\nE[\nT\nX\nt=1\nrt(ho\nt)]\n(1)\nUsing the maximization of expected cumulative scalar re-\nward to formulate the general notion of goal is a design\nchoice in reinforcement learning, based on what is com-\nmonly known as the reward hypothesis (Sutton & Barto,\n2018), In Sutton’s own words:\nThat all of what we mean by goals and purposes\ncan be well thought of as the maximization of\nthe expected value of the cumulative sum of a\nreceived scalar signal (called reward).\nThis design choice, however, is somewhat arbitrary. Among\nother things, the reward needs not be a scalar (e.g. multi-\nobjective reinforcement learning (White, 1982)), nor does\nit have to be a quantity whose cumulative sum is to be\nmaximized (which we will come to shortly). Leaving aside\nthe question of whether or not all goals can be formulated\nby Eq. 1, I intend to show in this paper that the problem\nof lifelong reinforcement learning probably should not be\nformulated as such.\nNote that in Eq. 1, I deﬁned the reward in terms of the obser-\nvation history, instead of the history of environment states\nas in most reinforcement learning literature. This reﬂects\nthe view that reward signals are internal to the agent, as\npointed out by Singh et al. (2004) in their work on intrinsic\nmotivation. Since the observations are all that the agent has\naccess to from the external environment, the intrinsic reward\nshould depend on the environment state only through the\nagent’s observation history.\nAlthough the above reinforcement learning formulation rec-\nognizes the reward as a signal intrinsic to an agent, it fo-\ncuses on learning across different generations 1 of agents,\nas opposed to learning within an agent’s lifespan. From an\nagent’s point of view, the cumulative reward is known only\nwhen it reaches its end of life, by which time no learning can\n1Usage of the word ‘generation’ here is only to emphasize that\nlearning cannot be achieved within an agent’s lifespan, and does\nnot imply that evolution algorithms need to be used.\narXiv:2001.09608v1  [cs.LG]  27 Jan 2020\nSome Insights into Lifelong Reinforcement Learning Systems\nFigure 1: Architecture of a traditional reinforcement learn-\ning system. At the beginning of an agent’s life, it receives\na policy πi = (πi\n1, πi\n2, ...πi\nT ) from the learning algorithm\nthat carries out a mix of exploitation and exploration, where\nthe superscript i indicates that the agent belongs to the ith\ngeneration. The agent receives an observation ot at each\ntimestep t, and act according to πi\nt. At the end of the agent’s\nlife, the learning algorithm gathers the observation history\nho\nT and the cumulative reward PT\nt=1 r(ho\nt) from the agent,\nand outputs the the next policy πi+1 to be executed. The\nlearning algorithm does not need to optimize the perfor-\nmance of any particular πi, as long as it is guaranteed to\nbe able to eventually ﬁnd the policies that maximize the\nexpected cumulative reward.\nbe done by the ‘dying’ agent itself. The individual reward\nreceived at each timestep does not really matter, since the\noptimization objective is the cumulative sum (of reward).\nThe information gathered by the agent, however, can be used\nto improve the policy of the next generation. In other words,\nwith the conventional reinforcement learning formulation,\nlearning can only happen at a level higher than the lives of\nindividual agents (Figure 1), with the goal that an optimal\nagent can eventually be found — the lifetime behavior of a\nparticular agent is not of concern.\n1.2. Towards Lifelong Reinforcement Learning\nIn lifelong reinforcement learning, on the other hand, the\nfocus is the agent’s ability to learn and adapt to the environ-\nment throughout its lifetime. Intuitively, this implies that\nlearning component of the learning system should reside\nwithin the agent.\nTo shed some lights on lifelong reinforcement learning,\nconsider the Q-learning (Watkins & Dayan, 1992) algorithm\nfor the standard reinforcement learning problem formulated\nby Eq. 1. For the purpose of this example only, it is further\nassumed that:\n• The reward depends only on the current observation.\nI.e., r(ho\nt) = r(ot)\n• Observations are Markov with respect to past observa-\ntions and actions. I.e., P(ot|ot−1, at−1, ..., o1, a1) =\nP(ot|ot−1, at−1)\nThese assumptions are only made so that Q-learning will\nﬁnd the solution to Eq. 1, and are not essential for the gen-\neral discussion. The (non-lifelong) learning system works\nas follows:\n1. The agent receives its initial Q estimate from the past\ngeneration.\n2. At each timestep t, the agent takes an ϵ-greedy action\nbased on the current Q estimate, then does a Bellman\nupdate on the Q estimate:\nQ(ot, at) :=\nQ(ot, at) + α(r(ot) + max\na\nQ(ot+1, a) −Q(ot, at))\n(2)\n3. When the agent dies, pass the updated Q estimate to\nthe next generation.\nAt ﬁrst sight, the fact that the Q estimate is updated every\ntimestep seems to contradict my argument that learning only\nhappens across generations. However, for Eq. 2 to be a valid\nupdate, the timestep t needs to be part of the observation\n— the observation ot here is in fact the raw observation o−\nt\naugmented by time t, i.e., ot = (o−\nt , t). Since the timestep\nis part of the observation, no same observation will be expe-\nrienced more than once throughout the agent’s lifetime, and\nit makes no difference to the agent whether the Q estimate\nis updated every timestep, or after its life ends 2.\nIt’s clear that for an agent to exhibit any sensible behavior,\nthe initial Q estimate it inherits from the past generation is\nvital. If the agent receives a random initial Q estimate, then\nit’s lifelong behavior is bound to be random and meaningless.\nOn the other side of the spectrum, if the agent receives\nthe true Q function, then it will behave optimally. This\nsuggests that if we care about the lifetime behaviour (which\nincludes lifelong learning behavior) of a Q-learning agent,\nthen Q(ot, ·) is a fundamental signal the agent needs to\nreceive in addition to the scalar reward. In a sense, if the\nsignal represented by the scalar reward is a speciﬁcation\nof what the goal is, then the signal represented by the Q\nestimate is the knowledge past generations have collected\nabout what the goal means for this type of agent. As an\nanalogy, the pain associated with falling to the ground could\nbe the former signal, while the innate fear of height could\nbe the latter.\n2The statement does not strictly hold true if function approxi-\nmation is used. An update to Qθ(ot, a) can potentially affect the\nQ estimate of all other observations. However, this is more a side\neffect than a desired property.\nSome Insights into Lifelong Reinforcement Learning Systems\nFrom a computational perspective, the separation of these\ntwo signals may not be necessary. Both signals can be\nconsidered as ‘annotations’ for the observation history that\nthe agent receives along with its observation, and can be\nincorporated into the concept of reward. The reward signals\nare no longer restricted scalars, nor are they necessarily\nquantities whose cumulative sum is to be maximized — they\nare just messages in some reward language that ‘encode’\nthe knowledge pertaining to an agent’s observation history\n— knowledge that enables the agent to learn continuously\nthroughout its life. Such knowledge may include the goals\nof the agent, the subgoals that constitute these goals, the\nheuristics for achieving them, and so on. The reward is\nthen ‘decoded’ by the learning algorithm, which deﬁnes\nhow the agent responds to the reward given the observation\nhistory. The learning system should be designed such that\nby responding to the reward in its intended way, the agent\nwill learn to achieve the goals implied by the reward before\nits end of life (Figure 2).\nTo be precise, the reward r(ho\nt) ∈Σ now belongs to some\nreward space Σ. The learning algorithm is a mapping from\nreward histories to policies. Denoting the set of possible\nreward history of length t as Hr\nt , and the set of all possi-\nble policies at time t as Πt, the learning algorithm m can\nbe represented by m = (m1, m2, ..., mt, ..., mT ), where\nmt : Hr\nt →Πt. The formulation is general, and a learning\nsystem formulated as such is not automatically a lifelong\nlearning system. In fact, it subsumes traditional reinforce-\nment learning: the reward space is set to the real numbers\n(Σ = R), and the learning algorithm can be set to any algo-\nrithm that converges to a policy that maximizes the expected\ncumulative reward. Unfortunately, the reward in traditional\nreinforcement learning does not contain enough information\nfor an agent to learn within its lifetime.\nViewing the reward as a general language, and the learning\nalgorithm as the response to the reward opens up the possi-\nbilities for principled ways to embed learning bias such as\nguidance and intrinsic motivation into the learning system,\ninstead of relying solely on manipulating the scalar reward\non an ad-hoc basis. In the rest of the paper, my focus re-\nmains on lifelong reinforcement learning, more speciﬁcally,\nwhat lifelong reinforcement learning requires of the reward\nlanguage and the corresponding learning algorithm.\n1.3. Reward as Formal Language\nAlthough the term ‘language’ used above can be understood\nin its colloquial sense, it can also be understood as the formal\nterm in automata theory. To see this, consider the following\ndeterministic ﬁnite automaton ⟨Σ, Q, δ, q0, F⟩, where:\n• Σ is the alphabet of the automaton, and is set to the\nreward space of the learning system. In other words,\nFigure 2: Architecture of lifelong reinforcement learning\nsystem. In contrast to traditional reinforcement learning\n(Figure 1), the learning algorithm resides inside the agent.\nThe internal environment of the agent can be thought of as a\nbuilt-in mechanism for the agent-designer to communicate\nwith the agent (through the reward). At each timestep, the\nlearning algorithm receives some message (encoded in the\nform of reward r(ho\nt)) from the agent’s internal environment,\nand outputs a policy πt as a response.\nthe alphabet of this automaton consists of all possible\nreward the agent can receive at any single timestep.\nA string is a sequence of symbols chosen from some\nalphabet. For this particular automaton, a string is in\nfact a sequence of reward, so the notation for reward\nhistory hr\nt is also used to denote a string of length t.\nThe set of all strings of length k over Σ is denoted as\nΣk, and the set of all strings (of any length) is denoted\nas Σ∗.\n• Q is the set of states of the automaton. Each state of\nthis automaton is a possible pair of reward history and\npolicies till some timestep t. For example, members of\nQ include:\n⟨hr\nt=1,\n(π1)⟩\n⟨hr\nt=2,\n(π1, π2)⟩\n...\n⟨hr\nt=T ,\n(π1, π2, ..., πT )⟩\nfor any π1 ∈Π1, π2 ∈Π2, ..., πT ∈ΠT , and hr\nt=1 ∈\nΣ1, hr\nt=2 ∈Σ2, ..., hr\nt=T ∈ΣT . In addition, Q has a\nspecial ‘empty’ member q0, which corresponds to the\ninitial state before any reward is received.\n• δ\n:\n(Q × Σ)\n→\nQ\nis\nthe\ntransition\nfunction.\nThe transition function corresponds\nto the learning algorithm of the learning sys-\ntem, so we have δ(⟨hr\nt, (π1, ..., πt)⟩,\nrt+1)\n=\n⟨hr\nt+1,\n(π1, ..., πt, mt+1(hr\nt+1))⟩, where hr\nt+1\n=\n(hr\nt, rt+1).\nSome Insights into Lifelong Reinforcement Learning Systems\n• q0 is the initial state of the automaton as explained\nabove.\n• F ⊂Q is the set of accepting states, which are the\ndesired states of the automaton.\nIt’s not hard to see that this automaton is a model of the\nlearning system described in Section 1.2, with its desired\nproperty speciﬁed by the accepting states F. In this pa-\nper, the desired property is that the system be a lifelong\nlearning system, so the accepting states F are the set of\n⟨hr\nT ,\n(π1, π2, ..., πT )⟩pairs that correspond to a lifelong\nlearner 3.\nTo specify learning objectives, each possible reward r ∈Σ\nis assigned some semantics. These semantics implicitly\ndeﬁne the set of valid reward sequences L ⊂Σ∗. Since L\nis a subset of Σ∗, it is a language over Σ. We want to make\nsure that — for all reward sequences in L, lifelong learning\ncan be achieved by the learning system abstracted by this\nautomaton, or equivalently, all reward sequences in L lead\nto accepting states F.\n2. A Prototype Lifelong Reinforcement\nLearning System\nDesigning a lifelong reinforcement learning system involves\ndesigning the reward language and the learning algorithm\nholistically. Intuitively, the reward needs to contain enough\ninformation to control the relevant aspects of the learning\nalgorithm, and the learning algorithm in turn needs to ‘inter-\npret’ the reward signal in its intended way. In this section, I\naim to provide some insights into the design process with a\nprototype lifelong reinforcement learning system.\n2.1. Reward Language\nThe main reason lifelong learning is impossible in conven-\ntional reinforcement learning is that the learning objective in\nconventional reinforcement learning is global, in the sense\nthat the goal of the agent is deﬁned in terms of the observa-\ntion history of its entire life. For a lifelong reinforcement\nlearning agent, the learning objectives should instead be lo-\ncal, meaning that the goals should be deﬁned only for some\nsmaller tasks that the agent can encounter multiple times\nduring its lifetime. Once a local goal expires, whether it is\nbecause the goal has been achieved or because the agent has\nfailed to achieve it within a certain time limit, a new local\ngoal (can potentially be another instantiation of the same\ngoal) ensues. This way, the agent has the opportunity to\ngather knowledge for each of the goals, and improve upon\n3Recall that an agent’s behavior is fully decided by its policy\nπ = (π1, π2, ..., πT ). Therefore given a reward history hr\nT , the\npolicy is sufﬁcient for us to tell whether the agent is a successful\nlifelong learner.\nthem, all within one life. Local goals like this are ubiquitous\nfor humans. For example, when a person is hungry, his main\nconcern is probably not the global goal of being happy for\nthe rest of his life — his goal is to have food. After the per-\nson is full, he might feel like taking a nap, which is another\nlocal goal. In fact, the local goals and the transition of them\nseems to embody what we mean by intrinsic motivation.\nTo be able to specify a series of local goals, the reward in\nthis prototype learning system has two parts: the reward\nstate rs\nt ∈G, and the reward value rv\nt ∈R, where G is the\nset of local goals the agent may have. This form of reward is\ninspired by the reward machine (Icarte et al., 2018), a Mealy\nmachine for specifying history-dependent reward, but the se-\nmantics we assign to the reward will be different. Also note\nthat this Mealy machine bears no relation to the automaton\nwe discussed in Section 1.3 — the reward machine models\nthe reward, while the automaton in Section 1.3 models the\nlearning system, and takes the reward as input. Each reward\nstate rs corresponds to a local goal. When a local goal (or\nequivalently, a reward state) expires, the agent receives a\nnumerical reward value rv. For all other timesteps (other\nthan the expiration of local goals), the reward value can be\nconsidered to take a special NULL value, meaning that no\nreward value is received. The reward value is an evalua-\ntion of the agent’s performance in an episode of a reward\nstate, where an episode of a reward state is deﬁned as the\ntime period between the expiration of the previous reward\nstate (exclusive) and the expiration of the reward state itself\n(inclusive). The reward state can potentially depend on the\nentire observation history, while the reward value can only\ndepend on the observation history of the episode it is assess-\ning. Overall, the reward is speciﬁed by (rs\nt , rv\nt ) = r(ho\nt).\nThe local goals described here are technically similar to\nsubgoals in hierarchical reinforcement learning (Dietterich,\n2000; Sutton et al., 1999; Parr & Russell, 1997). However,\nthe term ‘subgoal’ suggests that there is some higher-level\ngoal that the agent needs to achieve, and that the higher-level\ngoal is the true objective the agent needs to optimize. That\nis not the case here — although it is totally possible that the\nlocal goals are designed in such a way that some global goal\ncan be achieved, the agent only needs to optimize the local\ngoals.\nThe reward language in this prototype system makes two\nassumptions on the learning algorithm. As long as the two\nassumptions are met, the learning algorithm is considered\nto ‘interpret’ the reward correctly. The ﬁrst assumption is\nthat the learning algorithm only generates policies that are\nepisode-wise stationary, meaning that πt1 = πt2 for any\ntimesteps t1 and t2 in the same episode of a reward state,\nand that πt1 : O →A. This assumption is not particularly\nrestrictive, because in cases where a local goal requires a\nmore complex policy, we can always split the goal into mul-\nSome Insights into Lifelong Reinforcement Learning Systems\ntiple goals (by modifying the reward function) for which the\npolicies are episode-wise stationary. With this assumption,\nwe can use a single policy πrs : O →A to represent the\npolicies at all timesteps within an episode of reward state\nrs. The second assumption is that the learning algorithm\nkeeps a pool of ‘elite’ policies for each reward state: a pol-\nicy that led to high reward value in some episode has the\nopportunity to enter the pool, and a policy that consistently\nleads to higher reward value eventually dominates the policy\npool. The exact criterion for selection into the pool (e.g., to\nuse the expected reward value as the criterion, or to use the\nprobability of the reward value being higher than a certain\nthreshold, etc.) is not enforced, and is left up to the learning\nalgorithm.\n2.2. Learning Algorithm\nThe learning algorithm in this prototype lifelong learning\nsystem is an evolutionary algorithm, adjusted to meet the\nassumptions made by the reward. The algorithm maintains\na policy pool Drs of maximum size d for each reward state\nrs ∈G. Each item in the pool is a two tuple ⟨π, rv\nπ⟩where\nπ is a policy and rv\nπ is the reward value of the last episode in\nwhich π was executed. Conceptually, the algorithm consists\nof three steps: policy generation, policy execution, and\n(policy) pool update, which are described below.\nPOLICY GENERATION\nWhen an episode of reward state rs starts, a policy πrs is\ngenerated from one of the following methods with probabil-\nity p1, p2, p3, respectively:\n1. Randomly sample a policy from the policy pool Drs,\nand mutate the policy.\n2. Randomly sample a policy from Drs and keep it as\nis. Remove the sampled policy from Drs. This is to\nre-evaluate a policy in the pool. Since the transition of\nobservations might be stochastic, the same policy does\nnot necessarily always result in the same reward value.\n3. Randomly generate a new policy πrs : O →A from\nscratch. This is to keep the diversity of the policy pool.\np1, p2 and p3 should sum up to 1, and are hyper-parameters\nof the algorithm.\nPOLICY EXECUTION\nExecute the generated policy πrs until a numerical reward\nvalue rv is received.\nPOOL UPDATE\nIf the policy pool is not full, insert ⟨πrs, rv⟩into the pool.\nOtherwise compare rv with the minimum reward value in\nFigure 3: A simplistic abstraction of guidance in reinforce-\nment learning.\nthe pool. If rv is greater than or equal to the minimum\nreward value, replace the policy and reward value pair (that\nhas the minimum reward value) with ⟨πrs, rv⟩.\n2.3. Embedding Learning Bias\nLearning bias in reinforcement learning systems refers to\nthe explicit or implicit assumptions made by the learning\nalgorithm about the policy. Our assumption that the policy\nis episode-wise stationary is an example of learning bias.\nArguably, a good learning bias is as important as a good\nlearning algorithm, therefore it is important that mechanisms\nare provided to embed learning bias into the learning system.\nA straight-forward way to embed learning bias into the\nabove lifelong learning system is through the policy gen-\neration process. This includes how existing policies are\nmutated, and what distribution new policies are sampled\nfrom. The learning bias provided this way does not depend\non the agent’s observation and reward history, and is some-\ntimes implicit (e.g., the learning bias introduced by using a\nneural network of particular architecture).\nAnother type of learning bias common in reinforcement\nlearning is guidance, the essence of which can be illustrated\nby Figure 3. Suppose in some reward state, the agent starts\nfrom observation o and the goal is to reach 4 observation\no′. Prior knowledge indicates that to reach o′, visiting o′′\nis a good heuristic, but reaching o′′ itself has little or no\nmerit. In other words, we would like to encourage the\nagent to visit and explore around o′′ more frequently (than\nother parts of the observation space) until a reliable policy\nto reach o′ is found.\nTo provide guidance to the agent\nin the prototype lifelong learning system, we can utilize\nthe property of the learning algorithm that policies leading\nto high reward values will enter the policy pool. Once a\npolicy enters the pool, it has the opportunity to be sampled\n(possibly with mutation) and executed. Therefore, we just\nneed to assign a higher reward value for reaching o′′ (before\nthe expiration of the reward state) than reaching neither o′\nnor o′′. Also important is the ability to control the extent\nto which region around o′′ is explored. To achieve this,\nrecall that the learning algorithm occasionally re-evaluates\npolicies in the policy pool. If we assign a lower reward value\nfor reaching o′′ with some probability, we can prevent the\n4For sake of terminological convenience, we pretend that the\nobservations here are environment states.\nSome Insights into Lifelong Reinforcement Learning Systems\npolicy pool from being overwhelmed only by policies that\nlead to o′′. In other words, the reward value for reaching o′′\nshould have multiple candidates. Let rv({O −{o′, o′′}})\ndenote the reward value for an episode where the agent\nreaches neither o′ nor o′′, rv(o′) denote the reward value for\nreaching o′, we can set the reward value rv(o′′) for reaching\no′′ as:\nrv(o′′) :=\n(\na,\nwith probability p\nb,\nwith probability 1 −p\nwhere b < rv({O −{o′, o′′}}) < a < rv(o′). The proba-\nbility p controls the frequency region around o′′ is to be ex-\nplored compared the other parts of the observation space 5.\n3. Experiment\nNow we evaluate the behaviour of the prototype lifelong\nreinforcement learning system. The source code of the\nexperiments can be found at https://gitlab.com/\nlifelong-rl/lifelongRL_gridworld\n3.1. Environment\nConsider a gridworld agent whose life revolves around get-\nting food and taking the food back home for consumption.\nThe agent lives in a 11 by 11 gridworld shown in Figure\n4. The shaded areas are barriers that the agent cannot go\nthrough. Some potential positions of interest are marked\nwith letters: F is the food source and is assumed to have\ninﬁnite supply of food; H is the agent’s home. To get to the\nfood source from home, and to carry the food home, the\nagent must pass through one of the two tunnels — the tunnel\non the left is marked with L and the tunnel on the right is\nmarked with R. At each timestep, the agent observes its posi-\ntion in the gridworld as well as a signal indicating whether it\nis in one of the four positions of interest (if yes, which), and\nchooses from one of the four actions: UP, RIGHT, DOWN\nand LEFT. Each action deterministically takes the agent\nto the adjacent grid in the corresponding direction, unless\nthe destination is a barrier, in which case the agent remains\nin its original position. The agent starts from home at the\nbeginning of its life, and needs to go to the food source to\nget food. Once it reaches the food source, it needs to carry\nthe food back home. This process repeats until the agent\ndies. The lifespan of the agent is assumed to be 100 million\ntimesteps. The agent is supposed to learn to reliably achieve\nthese two local goals within its lifetime.\n5Note that the word ‘probability’ here should be interpreted as\nthe ‘long-run proportion’, and therefore the reward value needs\nnot be truly stochastic. E.g., we can imagine that the reward has a\nthird component which is the state of a pseudo-random generator.\nFigure 4: Gridworld environment.\n3.2. Learning System Setup\nThe reward state in this experiment is represented by the\nconjunction of Boolean variables. For example, if three\nBoolean variables A, B and C are deﬁned, then the re-\nward state would be in the form of rs = A ∧B ∧C or\nrs = A ∧¬B ∧C, etc. At the bare minimum, one Boolean\nvariable GET FOOD needs to be deﬁned for this agent, where\nGET FOOD being true corresponds to the local goal of going\nto the food source, and ¬GET FOOD corresponds to the local\ngoal of carrying the food home. The agent receives a reward\nvalue of +1 if GET FOOD is true and the agent reaches F,\nin which case the Boolean variable GET FOOD transitions\nto false. Similarly, the agent receives a reward value of\n+1 if ¬GET FOOD is true and the agent reaches H, in which\ncase GET FOOD transitions to true. On top of GET FOOD, we\ndeﬁne another Boolean variable TIMED OUT, which indi-\ncates whether the agent has exceeded a certain time limit\nfor trying to get to the food source, or for trying to carry the\nfood home. If the reward state is ¬TIMED OUT ∧GET FOOD,\nand the agent fails to reach F within the time limit, itre-\nceives a reward value of −1, and the reward state transition\nto TIMED OUT ∧GET FOOD. From TIMED OUT ∧GET FOOD,\nif the agent still fails to get to F within the time limit, it\nreceives a reward value of 0. The agent will remain in\nTIMED OUT ∧GET FOOD, until it reaches F, when the reward\nstate transitions to ¬TIMED OUT ∧¬GET FOOD (and receive\na +1 reward value as already mentioned). For the case\nwhen GET FOOD is false, the reward transition is deﬁned\nsimilarly. Throughout the experiments, the time limit is set\nto 24, which is enough for the agent to accomplish any of\nthe local goals. We refer to this reward design as the base\ncase.\nUnfortunately, even for a toy problem like this, learning can\nbe difﬁcult if no proper learning bias is provided. Since\nthere are 4 actions and 74 possible positions, the number\nof possible episode-wise stationary policies is 474 for each\nreward state. Among those policies, very few can achieve\nthe local goals. If the policy generation and mutation is\npurely random, it will take a long time for the agent to ﬁnd\nSome Insights into Lifelong Reinforcement Learning Systems\na good policy.\nBIASED POLICY\nThe ﬁrst learning bias we consider is biased policy, which is\nin contrast to the unbiased policy case where the policy gen-\neration and mutation is purely random. More speciﬁcally,\nwe make the policy generation process biased towards poli-\ncies that take the same action for similar observations. This\nwould encourage policies that head consistently in one di-\nrection, and discourage those that indeﬁnitely roam around\nbetween adjacent positions.\nPROGRESS-BASED GUIDANCE\nThe second learning bias we consider is guidance based on\nthe agent’s progress. Different from the base case where\nthe agent always receives a 0 (if TIMED OUT is true) or\n−1 (if TIMED OUT is false) reward value when it fails to\nachieve the local goal within the time limit, the agent now\nhas some probability p = 0.8 of receiving a reward value\nproportional to the Manhattan distance d it has traveled\nsince the beginning of the episode. To be precise:\nrv :=\n(\n0.01d\nwith probability p\nsame as the base case\nwith probability 1 −p\nThis way, policies leading to more progress (albeit not nec-\nessary towards the local goal) will be encouraged.\nSUB-OPTIMAL GUIDANCE\nFinally, we consider a case of sub-optimal guidance that\nencourages the agent to explore a sub-optimal trajectory.\nAs we have mentioned, both reaching the food source from\nhome and carrying the food home require the agent to go\nthrough one of the two tunnels. However, if the agent\ngoes through the left tunnel, it has to travel more dis-\ntance. Suppose that we prefer the agent to take the shorter\nroute, but we only know the route that goes through the\nleft tunnel; and as a result, we sub-optimally encourage\nthe agent to explore the left tunnel. To guide the agent\nto take the left tunnel, Boolean variable VISITED LEFT is\nintroduced as an indicator of whether L has been visited\nsince the last visitation of F or H. Now we have 23 = 9\nelements in the reward space, corresponding to 9 possible\nlocal goals. The reward transition is different from the base\ncase in that if the agent has already visited L when the lo-\ncal goal GET FOOD ∧¬TIMED OUT ∧¬VISITED LEFT or\n¬GET FOOD ∧¬TIMED OUT ∧¬VISITED LEFT times out,\nVISITED LEFT becomes true, and the agent will receive a\nreward value of +0.6 with 0.8 probability, and −0.2 with\n0.2 probability. To express our preference for the shorter\nroute, the agent receives a reward value of +0.8 (instead of\n+1) when it reaches F (when GET FOOD is true) or H (when\nGET FOOD is false) through the left tunnel.\n3.3. Results\n(a) unbiased policy, progressed-based guidance\n(b) biased policy, progressed-based guidance\nFigure 5: Learning curve for unbiased/biased policy with\nprogress-based guidance, averaged over 20 runs.\nFigure 5 shows the learning curves for reward state\nGET FOOD ∧¬TIMED OUT with progress-based guidance.\nThe x-axis is the timesteps (in million), and the y-axis is the\npercentage of times the agent transitions into a particular\nnext reward state starting from GET FOOD∧¬TIMED OUT. A\nnext reward state of ¬GET FOOD ∧¬TIMED OUT means that\nthe agent successfully reached F within the time limit, and a\nnext reward state of GET FOOD ∧TIMED OUT means that the\nagent failed to do so. As we can see, with unbiased policy, it\ntook the agent around 25 million timesteps to achieve 100%\nsuccess rate; while with biased policy, this only took around\n8 million timesteps.\nFigure 6 shows the learning curves for reward state\nGET FOOD ∧¬TIMED OUT ∧¬VISITED LEFT with the sub-\noptimal guidance described in Section 3.2. Similar to Fig-\nure 5, the x-axis is the timesteps (in million), and the y-\naxis is the percentage of times the agent transitioned into\na particular next reward state starting from GET FOOD ∧\nSome Insights into Lifelong Reinforcement Learning Systems\n(a) unbiased policy, sub-optimal guidance\n(b) biased policy, sub-optimal guidance\nFigure 6: Learning curve for unbiased/biased policy with\nsub-optimal guidance, averaged over 20 runs.\n¬TIMED OUT ∧¬VISITED LEFT. A next reward state of\n¬GET FOOD ∧¬TIMED OUT ∧¬VISITED LEFT means that\nthe agent successfully reached F within the time limit; a next\nreward state of GET FOOD ∧TIMED OUT ∧VISITED LEFT\nmeans that the agent failed to reach the food source, but was\nable to ﬁnd a way to the left tunnel; and a next reward state\nof GET FOOD ∧TIMED OUT ∧¬VISITED LEFT means that\nthe agent was neither able to reach the left tunnel nor the\nfood source within the time limit. As we can see, for both\nunbiased and biased policy, learning is much slower than\nprogress-based guidance. This is likely due to the much\nsparser guidance signal — the agent receives guidance only\nwhen it reaches the left tunnel. For the unbiased policy case,\n100% success rate was not achieved within 100 million\ntimesteps, but we can clearly see that exploration around\nthe left tunnel was encouraged as intended. For the biased\npolicy case, the agent was able to reach 100% success rate\nafter 50 million timesteps. But was the agent able to ﬁgure\nout the optimal route, or did it only learn to take the sub-\noptimal route as guided? Recall that the agent receives a\nreward value of +1 if it takes the optimal route, and a reward\nvalue of +0.8 if it takes the sub-optimal route. As shown\nin Figure 7, although the agent was taking the sub-optimal\nroute by 50 million timesteps when it just learned to reach\nthe food source reliably, it was eventually able to ﬁgure out\nthe optimal route by 90 million timesteps.\nFigure 7: Reward value for GET FOOD ∧¬TIMED OUT ∧\n¬VISITED LEFT (biased policy with sub-optimal guidance,\naveraged over 20 runs).\n4. Conclusions\nLifelong reinforcement learning is sometimes viewed as\na multi-task reinforcement learning problem (Abel et al.,\n2018), where the agent must learn to solve tasks sampled\nfrom some distribution D. The agent is expected to (explic-\nitly or implicitly) discover the relation between tasks, and\ngeneralize its policy to unseen tasks from D. The focus is\ntherefore on the transfer learning (Taylor & Stone, 2009)\nand continual learning (Ring, 1998) aspects of lifelong rein-\nforcement learning.\nIn this paper, I provided a systems view on lifelong rein-\nforcement learning. In particular, I showed that the reward\nin a lifelong reinforcement learning system can be a general\nlanguage, and that the language needs to be designed holis-\ntically with the learning algorithm. A prototype lifelong\nreinforcement learning system was given, with an empha-\nsize on how learning bias can be embedded into the learning\nsystem through the synergy of the reward language and the\nlearning algorithm.\nAcknowledgements\nThe author would like to thank Gaurav Sharma (Borealis\nAI) for his comments on a draft of the paper.\nReferences\nAbel, D., Arumugam, D., Lehnert, L., and Littman, M. L.\nState abstractions for lifelong reinforcement learning. In\nProceedings of the 35th International Conference on Ma-\nSome Insights into Lifelong Reinforcement Learning Systems\nchine Learning, ICML 2018, Stockholmsm¨assan, Stock-\nholm, Sweden, July 10-15, 2018, pp. 10–19, 2018.\nDietterich, T. G. Hierarchical reinforcement learning with\nthe MAXQ value function decomposition. J. Artif. Intell.\nRes., 13:227–303, 2000.\nIcarte, R. T., Klassen, T. Q., Valenzano, R. A., and McIl-\nraith, S. A. Using reward machines for high-level task\nspeciﬁcation and decomposition in reinforcement learn-\ning. In Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsm¨assan,\nStockholm, Sweden, July 10-15, 2018, pp. 2112–2121,\n2018.\nParr, R. and Russell, S. J. Reinforcement learning with\nhierarchies of machines. In Advances in Neural Informa-\ntion Processing Systems 10, [NIPS Conference, Denver,\nColorado, USA, 1997], pp. 1043–1049, 1997.\nRing, M. B. Child: A ﬁrst step towards continual learning.\nIn Learning to Learn, pp. 261–292. 1998. doi: 10.1007/\n978-1-4615-5529-2\\ 11. URL https://doi.org/\n10.1007/978-1-4615-5529-2_11.\nSingh, S. P., Barto, A. G., and Chentanez, N. Intrinsically\nmotivated reinforcement learning. In Advances in Neural\nInformation Processing Systems 17 [Neural Information\nProcessing Systems, NIPS 2004, December 13-18, 2004,\nVancouver, British Columbia, Canada], pp. 1281–1288,\n2004.\nSutton, R. S. and Barto, A. G. Reinforcement Learning: An\nIntroduction. The MIT Press, second edition, 2018.\nSutton, R. S., Precup, D., and Singh, S. P. Between mdps\nand semi-mdps: A framework for temporal abstraction in\nreinforcement learning. Artif. Intell., 112(1-2):181–211,\n1999.\nTaylor, M. E. and Stone, P. Transfer learning for reinforce-\nment learning domains: A survey. J. Mach. Learn. Res.,\n10:1633–1685, 2009.\nWatkins, C. J. C. H. and Dayan, P. Technical note q-learning.\nMachine Learning, 8:279–292, 1992.\nWhite, D.\nMulti-objective inﬁnite-horizon discounted\nmarkov decision processes. Journal of Mathematical\nAnalysis and Applications, 89(2):639 – 647, 1982. ISSN\n0022-247X.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-01-27",
  "updated": "2020-01-27"
}