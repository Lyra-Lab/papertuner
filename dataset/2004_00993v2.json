{
  "id": "http://arxiv.org/abs/2004.00993v2",
  "title": "Augmented Q Imitation Learning (AQIL)",
  "authors": [
    "Xiao Lei Zhang",
    "Anish Agarwal"
  ],
  "abstract": "The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.",
  "text": "Augmented Q Imitation Learning (AQIL)\nXiao Lei Zhang\nYork University\nToronto, Canada\nzhang205@cse.yorku.ca\nAnish Agarwal\nUniversity of Waterloo\nWaterloo, Canada\na22agarw@outlook.com\nThe study of unsupervised learning can be generally divided\ninto  two  categories:  imitation  learning  and  reinforcement\nlearning.  In  imitation  learning  the  machine  learns  by\nmimicking  the  behavior  of  an  expert  system  whereas  in\nreinforcement  learning  the  machine  learns  via  direct\nenvironment  feedback.  Traditional  deep  reinforcement\nlearning takes a significant time before the machine starts to\nconverge to an optimal policy. This paper proposes Augmented\nQ-Imitation-Learning, a method by which deep reinforcement\nlearning  convergence  can  be  accelerated  by  applying  Q-\nimitation-learning as the initial training process in traditional\nDeep Q-learning.\nReference code:  https://github.com/veda-s4dhak/AQIL\nKeywords  —  Q-imitation-learning,  Deep  Reinforcement\nLearning, Deep Q-learning, Behavioral Cloning, SQIL\nI. INTRODUCTION\nImitation learning in deep learning systems is generally \neither focused on modeling the behavior of an expert system\n(behavioral cloning) or focused on modeling the reward \nfunction which best approximates the expert system’s \nbehavior (inverse reinforcement learning). \nThe performance of an imitation learning system alone is \nlimited by the performance of the expert player. In the ideal \nsense we want systems which can increase the upper bound \nof performance by going beyond that of an expert system. \nTo achieve this, imitation learning alone is not sufficient. \nWe are bounded by the limits of supervision.\nCurrent fully unsupervised deep learning systems which \nhave performance beyond known expert systems have been \ndesigned using reinforcement learning, where machines are \nlearning from direct environment interaction. Deep \nreinforcement learning[6] (DQN) however requires quite a bit\nof training period till the model reaches expert level \nperformance. This is exacerbated in increasingly complex \nenvironments. \nIt seems that there is a mutual advantage to merge \nreinforcement learning with imitation learning. A \nreinforcement learning model can accelerate its initial \ntraining time by imitating an expert system. An imitation \nlearning model can increase its upper bound and go beyond \nthe expert system by switching to direct environment \ninteraction. \nIn this paper we consider this augmentation. We use \ntraditional imitation learning approaches as a precursor to \ndeep reinforcement learning. A deep neural network first \nimitates an expert system and then is allowed to reinforce \ndirectly through the environment. We first setup the \nframework of the experiment, followed by the \nimplementation details and concluding with the \nexperimental results.\nThe imitation learning framework we used is a custom Q-\nimitation-learning protocol similar to SQIL[1]. Whereas \nSQIL uses a soft Bellman equation[5], we use the Bellman \nequation with a hard argmax. Q imitation learning is the \nsame as traditional Q-learning in all aspects except that the \nstate-action reward is determined by adherence to an experts\nstate-action rather than from direct environment feedback. \nThis paper uses a Gaussian reward function proportional to \ndifference between expert’s action and agent’s action. \nWe follow the Q-imitation-learning with a traditional Q-\nlearning training sequence. In the Q-learning portion we use \na Gaussian reward function proportional to the difference \nbetween the optimal state and the actual state. \nII. FRAMEWORK\nA. Markov Decision Processes\nWe define the MDP parameters as {S, A, P, R, I}, where:\n•\nS is the set of states\n•\nA is the finite set of actions\n•\nP = P(s, a, s’) is the state transition probability\nwhich denotes the probability to transition to state\ns’ given than the previous state was s and action a\nwas taken.\n•\nR = R(s, a) is the reward in state s given action a\nwas taken\n•\nI is the initial state distribution\nAdditional parameters are specified as follows:\n•\nN denotes the number of episodes\n•\nT denotes the time horizon\n•\nπ\n denotes  the policy that  determines  which\naction is taken at state s \n•\nπ\n'\n denotes the experts policy\n•\nπ\n' '\n denotes the optimal policy\nB. Problem Definition\nGiven the above we can make the following conclusions:\n•\nV (π)=T∗E R(s,π(s))\ndenotes the total rewards of all trajectories given\nthe initial state I\n•\nV (π\n')−V (π)\ndenotes the imitation regret\n•\nV (π\n'')−V (π)\ndenotes  the  reinforcement\nregret\n•\nV (π\n'')−V (π\n')\n denotes the expert regret\nThe  goal  of  augmented  reinforcement  learning  is  to\naccelerate  reduction of reinforcement  regret  to the point\nwhere  it  is  below expert  regret.  That  is  using imitation\nlearning to reach the point where\nV (π\n'')−V (π)≤V (π\n'')−V (π\n')\nas fast as possible.\nIII. IMPLEMENTATION\nA. Agent-Environment Interaction\nWe  implement  our  experiment  in  CartPole-v1  Gym\nenvironment  from  the  OpenAI  gym.  CartPole  is  a\nconventional controls problem and is suitable for imitation\nlearning since an expert model is readily available in the\nform of a PID controller.\nThe mechanics of CartPole consist of a pole attached by\na joint to a cart, which is controlled by applying a force of\n+1 or -1.  The pole initially starts upright and the goal is to\nprevent the pole from falling over. Each episode ends, when\nthe pole is more than theta degrees from the vertical or the\ncart moves more than 2.4 units away from the center. For\nthis  experiment,  we  set  theta  to  50  degrees  to  reduce\nlearning time of each agent.\nFigure 1. View of  Cartpole-v1 environment.\nWe  first  train the  model  via  Q-imitation-learning  by\nmodeling the PID. Then we train the model using deep\nreinforcement learning directly from the environment. We\ncompare  these  results  to  a  model  trained  via  deep\nreinforcement  learning alone and to a model trained via\nimitation learning alone. \nIn  both  Q-imitation-learning  and  deep  reinforcement\nlearning, we used the Q-learning methodology for training.\nDuring the imitation learning process, the Q-learning model\noptimizes the reward based on following the expert input.\nThe expert input was taken by implementing a simple PID\ncontroller to control the cart. The PID system was tuned to\nscore  much  higher  than  an  average  human  player.  The\nproportional,  integral  and  derivative  parameters  are  as\nfollows:\nP = 0.6, I = 0.00625, D = 0.8\nThe  reward  during  Q  imitation  learning  is  a  Gaussian\nfunction  defined  below.  The  reward  depends  on  the\ndifference between the expert action and the model action as\nwell as the difference between the optimal and actual pole\nangles. The reward function is highest when the pole angle\nis optimal and the model action matches the expert action. \nR(θ,aPID,amodel)=0.2e\n−1\n2( θoptimal−θ\nσ1\n)\n2\n+0.8e\n−1\n2( aPID−amodel\nσ 2\n)\n2\nθoptimal=0∧σ1=10∧σ2=0.5\nFor  deep  reinforcement  learning  the  Gaussian  reward\nfunction becomes\nR(θ)=e\n−1\n2(θ optimal−θ\nσ1\n)\n2\nθoptimal=0∧σ1=10\nThis reward function is based on the difference between the\ntarget and actual pole angles\nWe define the loss function for the ith training step as the\nBellman error.\nLi(θi)=( yi−Q(s,a;θi))\n2\nwhere \nyi=r+γ max\na' Q(s ', a';θi−1)\nB. Model Architecture\nThe model architecture is shown in Figure 1.We use a fully\nconnected neural network model for the Deep Q-Learning\nand Imitation Learning agents.  We use ReLU activation for\nthe inner layers and linear activation for the output layer. \nFigure 1. DQN Model Architecture. Dense layers are\ndescribed by number of features (n). \n2\nC. Imitation Training Methodology\nFor  imitation  training,  we  use  the  forward  training\nmethodology used by Ross and Bagnell (2010)[2] modified to\nuse a stationary policy. We use a stationary policy since the\nT is unbounded in the training environment.\nThe training methodology is summarized as follows:\nAlgorithm 1: Q Imitation Training\n1:\nInitialize π\n2:\nFor I = 1 to num_epochs do\n3:\nExecute x trajectories using π\n'\n4:\nSample dataset D = {states, action} taken by \nexpert\n5:\nTrain π\nusing DQN with\nReward=R(θ,aPID,amodel)\n6:\nEnd for\n7:\nReturn π\nD. Reinforcement Learning Methodology\nThe reinforcement training is similar to imitation training\nexcept that the training classifier uses a reward function\ndirectly from the environment rather than the expert player.\nAlgorithm 2: Reinforcement Training\n1:\nInitialize π\n2:\nFor I = 1 to num_epochs do\n3:\nExecute x trajectories using π\n'\n4:\nSample dataset D = {states, action} taken by \nexpert\n5:\nTrain π\nusing DQN with\nReward=R(θ)\n6:\nEnd for\n7:\nReturn π\nIV. RESULTS\nA. Deep Q Learning\nThe first model is trained using deep reinforcement learning \nalone, denoted as RL500. The model loss and reward curves \nare shown in Figure 2.  The summary results are shown in \nTable 2. The model achieves an average score of 331.63 and\na peak reward of 1949.39 after 500 episodes of training.\n \nFigure 2. Model Loss and Reward for CartPole-v1 with\nDeep Q Learning, denoted as RL500.\nFigure 3. Model Weights for RL500 Model.\nB. Q-Imitation-Learning with Expert Player (PID)\nThe second and third model are trained by Q Imitation \nLearning via the expert PID system, referred as IL250 and \nIL500. Figure 4 and 6 shows the loss and reward of the \nexpert. The model imitation loss after 250 episodes of \ntraining and 500 episodes of training are also shown in \nFigure 4 and 6. Table 2 shows the IL250 producing an \naverage score of 593.1 and a peak score of 6082.91, while \nIL500 reaches an average score of 681.91 and peak score of \n4652.33.  Model weights are shown in Figure 5 and 7.\n3\nFigure 4. Model Loss and Reward for Q Imitation Learning\nusing PID expert player, denoted as IL250.\nFigure 5. Model Weights for IL250 Model.\nFigure 6. Model Loss and Reward for Q Imitation Learning\nusing PID expert player, denoted as IL500.\nFigure 7. Model Weights for IL500 Model\n4\nC. Augmented Q-Imitation-Learning\nWe train the fourth model using Deep Q Learning with the \nimitation learning model from B. This model is referred to \nas IL250 + RL250. Model loss and reward plots are shown \nin Figure 8. As shown in Table 2, the model achieves an \naverage reward of 2000 and a peak reward of 13000 after \n250 episodes of training. Model weights are shown in Figure\n9.\nFigure 8. Model Loss and Reward for CartPole-v1 with\nDeep Q Learning after Q Imitation Learning,  denoted as\nIL250 + RL250.\nFigure 9. Model Weights for IL250 + RL250 Model.\nTable 2. Comparison of average reward and best reward for\neach learning method. Best reward measures the single best\nperforming episode for each learning method.\nV. CONCLUSION\nIL250+RL250 outperforms IL250, IL500 and RL500 by a \nlarge margin on both average and best score. This is given \nthat the total number of episodes are equal. This shows that \nAQIL may indeed be an effective approach to accelerate Q-\nlearning training in cases where an expert system is readily \navailable and the Deep Q-learning methodology is relevant.\nFuture research should include characterizing performance \nof AQIL with changes in training configuration, model \ntopology and environment complexity. It would be \ninteresting to see the performance of AQIL on a variety of \nenvironments. Repeated bouts of imitation and \nreinforcement learning may also provide some insight. \nLastly the effects of different reward functions and error \nfunctions should be characterized.  \nREFERENCES\n[1]\nReddy,  S.,  Dragan,  A.D.,  &  Levine,  S.  (2019).  SQIL:\nImitation\n \nLearning  \nvia\n \nRegularized\n \nBehavioral\nCloning. ArXiv, abs/1905.11108.\n[2]\nAttia, A., & Dayan, S. (2018). Global overview of Imitation\nLearning. ArXiv, abs/1801.06503.\n[3]\nJudah, K., Fern, A., Dietterich, T.G., & Tadepalli, P. (2014).\nActive lmitation learning: formal and practical reductions to\nI.I.D. learning. J. Mach. Learn. Res., 15, 3925-3963.\n[4]  Ng, A.Y., & Russell, S.J. (2000). Algorithms for Inverse\nReinforcement Learning. ICML.\n[5]  Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017).\nReinforcement Learning with Deep Energy-Based Policies. ICML.\n[6]    Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., \nAntonoglou, I., Wierstra, D., & Riedmiller, M.A. (2013). Playing \nAtari with Deep Reinforcement Learning. ArXiv, abs/1312.5602.\n5\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-03-31",
  "updated": "2020-04-05"
}