{
  "id": "http://arxiv.org/abs/2009.07888v7",
  "title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
  "authors": [
    "Zhuangdi Zhu",
    "Kaixiang Lin",
    "Anil K. Jain",
    "Jiayu Zhou"
  ],
  "abstract": "Reinforcement learning is a learning paradigm for solving sequential\ndecision-making problems. Recent years have witnessed remarkable progress in\nreinforcement learning upon the fast development of deep neural networks. Along\nwith the promising prospects of reinforcement learning in numerous domains such\nas robotics and game-playing, transfer learning has arisen to tackle various\nchallenges faced by reinforcement learning, by transferring knowledge from\nexternal expertise to facilitate the efficiency and effectiveness of the\nlearning process. In this survey, we systematically investigate the recent\nprogress of transfer learning approaches in the context of deep reinforcement\nlearning. Specifically, we provide a framework for categorizing the\nstate-of-the-art transfer learning approaches, under which we analyze their\ngoals, methodologies, compatible reinforcement learning backbones, and\npractical applications. We also draw connections between transfer learning and\nother relevant topics from the reinforcement learning perspective and explore\ntheir potential challenges that await future research progress.",
  "text": "1\nTransfer Learning in Deep Reinforcement\nLearning: A Survey\nZhuangdi Zhu, Kaixiang Lin, Anil K. Jain, and Jiayu Zhou\nAbstract—Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed\nremarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of\nreinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges\nfaced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the\nlearning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep\nreinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under\nwhich we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw\nconnections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential\nchallenges that await future research progress.\nIndex Terms—Transfer Learning, Reinforcement Learning, Deep Learning, Survey.\n✦\n1\nINTRODUCTION\nR\nEinforcement Learning (RL) is an effective framework\nto solve sequential decision-making tasks, where a\nlearning agent interacts with the environment to improve\nits performance through trial and error [1]. Originated\nfrom cybernetics and thriving in computer science, RL\nhas been widely applied to tackle challenging tasks which\nwere previously intractable. Traditional RL algorithms were\nmostly designed for tabular cases, which provide principled\nsolutions to simple tasks but face difficulties when handling\nhighly complex domains, e.g. tasks with 3D environments.\nWith the recent advances in deep learning research, the\ncombination of RL and deep neural networks is developed to\naddress challenging tasks. The combination of deep learning\nwith RL is hence referred to as Deep Reinforcement Learning\n(DRL) [2], which learns powerful function approximators\nusing deep neural networks to address complicated domains.\nDRL has achieved notable success in applications such\nas robotics control [3, 4] and game playing [5]. It also\nthrives in domains such as health informatics [6], electricity\nnetworks [7], intelligent transportation systems[8, 9], to name\njust a few.\nBesides its remarkable advancement, RL still faces in-\ntriguing difficulties induced by the exploration-exploitation\ndilemma [1]. Specifically, for practical RL problems, the\nenvironment dynamics are usually unknown, and the agent\ncannot exploit knowledge about the environment until\nenough interaction experiences are collected via exploration.\nDue to the partial observability, sparse feedbacks, and\nthe high complexity of state and action spaces, acquiring\nsufficient interaction samples can be prohibitive or even\nincur safety concerns for domains such as automatic-driving\nand health informatics. The abovementioned challenges\n•\nZhuangdi Zhu, Anil K. Jain, and Jiayu Zhou are with the Department\nof Computer Science and Engineering, Michigan State University, East\nLansing, MI, 48824. E-mail: {zhuzhuan, jain, jiayuz}@msu.edu\n•\nKaixiang Lin is with the Amazon Alexa AI. E-mail: lkxcarson@gmail.com\nhave motivated various efforts to improve the current RL\nprocedure. As a result, transfer learning (TL), or equivalently\nreferred as knowledge transfer, which is a technique to utilize\nexternal expertise to benefit the learning process of the target\ndomain, becomes a crucial topic in RL.\nWhile TL techniques have been extensively studied in\nsupervised learning [10], it is still an emerging topic for RL.\nTransfer learning can be more complicated for RL, in that\nthe knowledge needs to transfer in the context of a Markov\nDecision Process. Moreover, due to the delicate components\nof the Markov decision process, expert knowledge may take\ndifferent forms that need to transfer in different ways.\nNoticing that previous efforts on summarizing TL in the\nRL domain did not cover research of the last decade [11, 12],\nduring which time considerate TL breakthroughs have been\nachieved empowered with deep learning techniques. Hence,\nin this survey, we make a comprehensive investigation of the\nlatest TL approaches in RL.\nThe contributions of our survey are multifold: 1) we inves-\ntigated up-to-date research involving new DRL backbones\nand TL algorithms over the recent decade. To the best of\nour knowledge, this survey is the first attempt to survey TL\napproaches in the context of deep reinforcement learning.\nWe reviewed TL methods that can tackle more evolved\nRL tasks, and also studied new TL schemes that are not\ndeeply discussed by prior literatures, such as representation\ndisentanglement (Sec 5.5) and policy distillation (Sec 5.3). 2)\nWe provided systematic categorizations that cover a broader\nand deeper view of TL developments in DRL. Our main\nanalysis is anchored on a fundamental question, i.e. what is\nthe transferred knowledge in RL, following which we conducted\nmore refined analysis. Most TL strategies, including those\ndiscussed in prior surveys are well suited in our categoriza-\ntion framework. 3) Reflecting on the developments of TL\nmethods in DRL, we brought new thoughts on its future\ndirections, including how to do reasoning over miscellaneous\nknowledge forms and how to leverage knowledge in more\narXiv:2009.07888v7  [cs.LG]  4 Jul 2023\n2\nefficient and principled manner. We also pointed out the\nprominent applications of TL for DRL and its opportunities\nto thrive in the future era of AGI.\nThe rest of this survey is organized as follows: In Section\n2 we introduce RL preliminaries, including the recent key\ndevelopment based on deep neural networks. Next, we\ndiscuss the definition of TL in the context of RL and\nits relevant topics (Section 2.4). In Section 3, we provide\na framework to categorize TL approaches from multiple\nperspectives, analyze their fundamental differences, and\nsummarize their evaluation metrics (Section 3.3). In Section\n5, we elaborate on different TL approaches in the context of\nDRL, organized by the format of transferred knowledge, such\nas reward shaping (Section 5.1), learning from demonstrations\n(Section 5.2), or learning from teacher policies (Section 5.3). We\nalso investigate TL approaches by the way that knowledge\ntransfer occurs, such as inter-task mapping (Section 5.4), or\nlearning transferrable representations (Section 5.5), etc. We\ndiscuss contemporary applications of TL in the context of\nDRL in Section 6 and provide some future perspectives and\nopen questions in Section 7.\n2\nDEEP REINFORCEMENT LEARNING AND TRANS-\nFER LEARNING\n2.1\nReinforcement Learning Basics\nMarkov Decision Process: A typical RL problem can be\nconsidered as training an agent to interact with an envi-\nronment that follows a Markov Decision Process (MPD) [13].\nThe agent starts with an initial state and performs an action\naccordingly, which yields a reward to guide the agent actions.\nOnce the action is taken, the MDP transits to the next state by\nfollowing the underlying transition dynamics of the MDP. The\nagent accumulates the time-discounted rewards along with its\ninteractions. A subsequence of interactions is referred to as\nan episode. The above-mentioned components in an MDP can\nbe represented using a tuple, i.e. M = (µ0, S, A, T , γ, R), in\nwhich:\n• µ0 is the set of initial states.\n• S is the state space.\n• A is the action space.\n• T : S×A×S →R is the transition probability distribution,\nwhere T (s′|s, a) specifies the probability of the state\ntransitioning to s′ upon taking action a from state s.\n• R : S × A × S →R is the reward distribution, where\nR(s, a, s′) is the reward that an agent can get by taking\naction a from state s with the next state being s′.\n• γ is a discounted factor, with γ ∈(0, 1].\nA RL agent behaves in M by following its policy π,\nwhich is a mapping from states to actions: π : S →A . For a\nstochastic policy π, π(a|s) denotes the probability of taking\naction a from state s. Given an MDP M and a policy π, one\ncan derive a value function V π\nM(s), which is defined over\nthe state space: V π\nM(s) = E\n\u0002r0 + γr1 + γ2r2 + . . . ; π, s\n\u0003 ,\nwhere ri = R(si, ai, si+1) is the reward that an agent\nreceives by taking action ai in the i-th state si, and the\nnext state transits to si+1. The expectation E is taken over\ns0 ∼µ0, ai ∼π(·|si), si+1 ∼T (·|si, ai). The value function\nestimates the quality of being in state s, by evaluating the ex-\npected rewards that an agent can get from s following policy\nπ. Similar to the value function, a policy also carries a Q-\nfunction, which estimates the quality of taking action a from\nstate s: Qπ\nM(s, a) = Es′∼T (·|s,a) [R(s, a, s′) + γV π\nM(s′)] .\nReinforcement Learning Goals: Standard RL aims to learn\nan optimal policy π∗\nM with the optimal value and Q-\nfunction, s.t. ∀s ∈S, π∗\nM(s) = arg max\na∈A\nQ∗\nM(s, a), where\nQ∗\nM(s, a) = sup\nπ\nQπ\nM(s, a). The learning objective can be\nreduced as maximizing the expected return:\nJ(π) := E(s,a)∼µπ(s,a)[\nX\nt\nγtrt],\nwhere µπ(s, a) is the stationary state-action distribution in-\nduced by π [14].\nBuilt upon recent progress of DRL, some literature has\nextended the RL objective to achieving miscellaneous goals\nunder different conditions, referred to as Goal-Conditional\nRL (GCRL). In GCRL, the agent policy π(·|s, g) is dependent\nnot only on state observations s but also the goal g being\noptimized. Each individual goal g ∼G can be differentiated\nby its reward function r(st, at, g), hence the objective for\nGCRL becomes maximizing the expected return over the dis-\ntribution of goals: J(π) := E(st,at)∼µπ,g∼G [P\nt γtr(s, a, g)]\n[15]. A prototype example of GCRL can be maze locomotion\ntasks, where the learning goals are manifested as desired\nlocations in the maze [16].\nEpisodic vs. Non-episodic Reinforcement Learning: In\nepisodic RL, the agent performs in finite episodes of length\nH, and will be reset to an initial state ∈µ0 upon the episode\nends [1]. Whereas in non-episodic RL, the learning agent\ncontinuously interacts with the MDP without any state\nreset [17]. To encompass the episodic concept in infinite\nMDPs, episodic RL tasks usually assume the existence of\na set of absorbing states S0, which indicates the termination\nof episodic tasks [18, 19], and any action taken upon an\nabsorbing state will only transit to itself with zero rewards.\n2.2\nReinforcement Learning Algorithms\nThere are two major methods to conduct RL: Model-Based\nand Model-Free. In model-based RL, a learned or provided\nmodel of the MDP is used for policy learning. In model-free\nRL, optimal policy is learned without modeling the transition\ndynamics or reward functions. In this section, we start intro-\nducing RL techniques from a model-free perspective, due to\nits relatively simplicity, which also provides foundations for\nmany model-based methods.\nPrediction and Control: an RL problem can be disas-\nsembled into two subtasks: prediction and control [1]. In the\nprediction phase, the quality of the current policy is being\nevaluated. In the control phase or the policy improvement phase,\nthe learning policy is adjusted based on evaluation results\nfrom the prediction step. Policies can be improved by iterating\nthrough these two steps, known as policy iteration.\nFor model-free policy iterations, the target policy is opti-\nmized without requiring knowledge of the MDP transition\ndynamics. Traditional model-free RL includes Monte-Carlo\nmethods, which estimates the value of each state using\nsamples of episodes starting from that state. Monte-Carlo\nmethods can be on-policy if the samples are collected by\n3\nfollowing the target policy, or off-policy if the episodic samples\nare collected by following a behavior policy that is different\nfrom the target policy.\nTemporal Difference (TD) Learning is an alternative to\nMonte-Carlo for solving the prediction problem. The key idea\nbehind TD-learning is to learn the state quality function by\nbootstrapping. It can also be extended to solve the control\nproblem so that both value function and policy can get\nimproved simultaneously. Examples of on-policy TD-learning\nalgorithms include SARSA [20], Expected SARSA [21], Actor-\nCritic [22], and its deep neural network extension called A3C\n[23]. The off-policy TD-learning approaches include SAC [24]\nfor continuous state-action spaces, and Q-learning [25] for\ndiscrete state-action spaces, along with its variants built on\ndeep-neural networks, such as DQN [26], Double-DQN [26],\nRainbow [27], etc. TD-learning approaches focus more on\nestimating the state-action value functions.\nPolicy Gradient, on the other hand, is a mechanism\nthat emphasizes on direct optimization of a parameteriz-\nable policy. Traditional policy-gradient approaches include\nREINFORCE [28]. Recent years have witnessed the joint\npresence of TD-learning and policy-gradient approaches.\nRepresentative algorithms along this line include Trust region\npolicy optimization (TRPO) [29], Proximal Policy optimization\n(PPO) [30], Deterministic policy gradient (DPG) [31] and its\nextensions such as DDPG [32] and Twin Delayed DDPG [33].\nUnlike model-free methods that learn purely from trial-\nand-error, Model-Based RL (MBRL) explicitly learns the\ntransition dynamics or cost functions of the environment.\nThe dynamics model can sometimes be treated as a black-box\nfor better sampling-based planning. Representative examples\ninclude the Monte-Carlo method dubbed random shooting [34]\nand its cross-entropy method (CEM) variants [35, 36]. The\nmodeled dynamics can also facilitate learning with data gen-\neration [37] and value estimation [38]. For MBRL with white-\nbox modeling, the transition models become differentiable\nand can facilitate planning with direct gradient propogation.\nMethods along this line include differential planning for policy\ngradient [39] and action sequences search [40], and value\ngradient methods [41, 42]. One advantage of MBRL is its\nhigher sample efficiency than model-free RL, although it\ncan be challenging for complex domains, where it is usually\nmore difficult to learn the dynamics than learning a policy.\n2.3\nTransfer Learning in the Context of Reinforcement\nLearning\nRemark 1. Without losing clarify, for the rest of this survey, we\nrefer to MDPs, domains, and tasks equivalently.\nRemark 2. [Transfer Learning in the Context of RL] Given\na set of source domains Ms = {Ms|Ms ∈Ms} and a target\ndomain Mt, Transfer Learning aims to learn an optimal policy\nπ∗for the target domain, by leveraging exterior information Is\nfrom Ms as well as interior information It from Mt:\nπ∗= arg max\nπ\nEs∼µt\n0,a∼π[Qπ\nM(s, a)],\nwhere π = ϕ(Is ∼Ms, It ∼Mt) : St →At is a policy\nlearned for the target domain Mt based on information from both\nIt and Is.\nIn the above definition, we use ϕ(I) to denote the\nlearned policy based on information I, which is usually\napproximated with deep neural networks in DRL. For the\nsimplistic case, knowledge can transfer between two agents\nwithin the same domain, resulting in |Ms| = 1, and\nMs = Mt. One can consider regular RL without TL as\na special case of the above definition, by treating Is = ∅, so\nthat a policy π is learned purely on the feedback provided\nby the target domain, i.e. π = ϕ(It).\n2.4\nRelated Topics\nIn addition to TL, other efforts have been made to benefit RL\nby leveraging different forms of supervision. In this section,\nwe briefly discuss other techniques that are relevant to TL by\nanalyzing the differences and connections between transfer\nlearning and these relevant techniques, which we hope can\nfurther clarify the scope of this survey.\nContinual Learning is the ability of sequentially learning\nmultiple tasks that are temporally or spatially related, with-\nout forgetting the previously acquired knowledge. Continual\nLearning is a specialized yet more challenging scenario of TL,\nin that the learned knowledge needs to be transferred along\na sequence of dynamically-changing tasks that cannot be\nforeseen, rather than learning a fixed group of tasks. Hence,\ndifferent from most TL methods discussed in this survey,\nthe ability of automatic task detection and avoiding catastrophic\nforgetting is usually indispensable in continual learning [43].\nHierarchical RL has been proposed to resolve complex\nreal-world tasks. Different from traditional RL, for hierarchi-\ncal RL, the action space is grouped into different granularities\nto form higher-level macro actions. Accordingly, the learning\ntask is also decomposed into hierarchically dependent sub-\ngoals. Well-known hierarchical RL frameworks include Feu-\ndal learning [44], Options framework[45], Hierarchical Abstract\nMachines [46], and MAXQ [47]. Given the higher-level\nabstraction on tasks, actions, and state spaces, hierarchical\nRL can facilitate knowledge transfer across similar domains.\nMulti-task RL learns an agent with generalized skills\nacross various tasks, hence it can solve MDPs randomly\nsampled from a fixed yet unknown distribution [48]. A larger\nconcept of multi-task learning also incorporates multi-task\nsupervised learning and unsupervised learning [49]. Multi-task\nlearning is naturally related to TL, in that the learned skills,\ntypically manifested as representations, need to be effectively\nshared among domains. Many TL techniques later discussed\nin this survey can be readily applied to solve multi-task RL\nscenarios, such as policy distillation [50], and representation\nsharing [51]. One notable challenges in multi-task learning\nis negative transfer, which is induced by the irrelevance or\nconflicting property for learned tasks. Hence, some recent\nwork in multi-task RL focused on a trade-off between sharing\nand individualizing function modules [52–54].\nGeneralization in RL refers to the ability of learning\nagents to adapt to unseen domains. Generalization is a crucial\nproperty for RL to achieve, especially when classical RL\nassumes identical training and inference MDPs, whereas the\nreal world is constantly changing. Generalization in RL is\nconsidered more challenging than in supervised learning\ndue to the non-stationarity of MDPs, where the latter has\nprovided inspirations for the former [55]. Meta-learning is an\n4\neffective direction towards generalization, which also draws\nclose connections to TL. Some TL techniques discussed in this\nsurvey are actually designed for meta-RL. However, meta-\nlearning is particularly focused on the learning methods\nthat lead to fast adaptation to unseen domains, whereas TL\nis a broader concept and covers scenarios where the target\nenvironment can be (partially) observable. To tackle unseen\ntasks in RL, some meta-RL methods focused on training\nMDPs generation [56] and variations estimation [57]. We\nrefer readers to [58] for a more focused survey on meta RL.\n3\nANALYZING TRANSFER LEARNING\nIn this section, we discuss TL approaches in RL from different\nangles. We also use a prototype to illustrate the potential\nvariants residing in knowledge transfer among domains,\nthen summarize important metrics for TL evaluation.\n3.1\nCategorization of Transfer Learning Approaches\nTL approaches can be organized by answering the following\nkey questions:\n1) What knowledge is transferred: Knowledge from the\nsource domain can take different forms, such as expert\nexperiences [59], the action probability distribution of\nan expert policy [60], or even a potential function that\nestimates the quality of demonstrations in the target\nMDP [61]. The divergence in representations and granu-\nlarities of knowledge fundamentally influences how TL\nis performed. The quality of the transferred knowledge,\ne.g. whether it comes from an oracle [62] or a suboptimal\nteacher [63] also affects the way TL methods are designed.\n2) What RL frameworks fit the TL approach: We can\nrephrase this question into other forms, e.g., is the TL\napproach policy-agnostic, or only applicable to certain RL\nbackbones, such as the Temporal Difference (TD) methods?\nAnswers to this question are closely related to the\nrepresentaion of knowledge. For example, transferring\nknowledge from expert demonstrations are usually policy-\nagnostic (see Section 5.2), while policy distillation, to be\ndiscussed in Section 5.3, may not be suitable for DQN\nbackbone which does not explicitly learn a policy function.\n3) What is the difference between the source and the target\ndomain: Some TL approaches fit where the source domain\nMs and the target domain Mt are equivalent, whereas\nothers are designed to transfer knowledge between differ-\nent domains. For example, in video gaming tasks where\nobservations are RGB pixels, Ms and Mt may share the\nsame action space (A) but differs in their observation\nspaces (S). For goal-conditioned RL [64], the two domains\nmay differ only by the reward distribution: Rs ̸= Rt.\n4) What\ninformation\nis\navailable\nin\nthe\ntarget\ndo-\nmain: While knowledge from source domains is usually\naccessible, it can be prohibitive to sample from the target\ndomain, or the reward signal can be sparse or delayed.\nExamples include adapting an auto-driving agent pre-\ntrained in simulated platforms to real environments [65],\nThe accessibility of information in the target domain can\naffect the way that TL approaches are designed.\n5) How sample-efficient the TL approach is: TL enables\nthe RL with better initial performance, hence usually\nrequires fewer interactions compared with learning from\nscratch. Based on the sampling cost, we can categorize TL\napproaches into the following classes: (i) Zero-shot transfer,\nwhich learns an agent that is directly applicable to the\ntarget domain without requiring any training interactions;\n(ii) Few-shot transfer, which only requires a few samples\n(interactions) from the target domain; (iii) Sample-efficient\ntransfer, where an agent can benefit by TL to be more\nsample efficient compared to normal RL.\n3.2\nCase Analysis of Transfer Learning in the context of\nReinforcement Learning\nWe now use HalfCheetah1 as a working example to illustrate\nhow TL can occur between the source and the target domain.\nHalfCheetah is a standard DRL benchmark for solving physical\nlocomotion tasks, in which the objective is to train a two-leg\nagent to run fast without losing control of itself.\n3.2.1\nPotential Domain Differences:\nDuring TL, the differences between the source and target\ndomain may reside in any component of an MDP:\n• S (State-space): domains can be made different by ex-\ntending or constraining the available positions for the\nHalfCheetah agent to move.\n• A (Action-space) can be adjusted by changing the range of\navailable torques for the thigh, shin, or foot of the agent.\n• R (Reward function): a domain can be simplified by\nusing only the distance moved forward as rewards or\nbe perplexed by using the scale of accelerated velocity in\neach direction as extra penalty costs.\n• T\n(Transition dynamics): two domains can differ by\nfollowing different physical rules, leading to different\ntransition probabilities given the same state-action pairs.\n• µ0 (Initial states): the source and target domains may have\ndifferent initial states, specifying where and with what\nposture the agent can start moving.\n• τ (Trajectories): the source and target domains may allow\na different number of steps for the agent to move before a\ntask is done.\n3.2.2\nTransferrable Knowledge:\nWithout losing generality, we list below some transferrable\nknowledge assuming that the source and target domains are\nvariants of HalfCheetah:\n• Demonstrated trajectories: the target agent can learn from\nthe behavior of a pre-trained expert, e.g. a sequence of\nrunning demonstrations.\n• Model dynamics: the RL agent may access a model of the\nphysical dynamics for the source domain that is also partly\napplicable to the target domain. It can perform dynamic\nprogramming based on the physical rules, running fast\nwithout losing its control due to the accelerated velocity.\n• Teacher policies: an expert policy may be consulted by the\nlearning agent, which outputs the probability of taking\ndifferent actions upon a given state example.\n• Teacher value functions: besides teacher policy, the learning\nagent may also refer to the value function derived by a\nteacher policy, which implies the quality of state-actions\nfrom the teacher’s point of view.\n1. https://gym.openai.com/envs/HalfCheetah-v2/\n5\n3.3\nEvaluation metrics\nIn this section, we present some representative metrics for\nevaluating TL approaches, which have also been partly\nsummarized in prior work [11, 66]:\n• Jumpstart performance( jp): the initial performance (returns)\nof the agent.\n• Asymptotic performance (ap): the ultimate performance\n(returns) of the agent.\n• Accumulated rewards (ar): the area under the learning curve\nof the agent.\n• Transfer ratio (tr): the ratio between asymptotic performance\nof the agent with TL and asymptotic performance of the agent\nwithout TL.\n• Time to threshold (tt): the learning time (iterations) needed\nfor the target agent to reach certain performance threshold.\n• Performance with fixed training epochs (pe): the performance\nachieved by the target agent after a specific number of\ntraining iterations.\n• Performance sensitivity (ps): the variance in returns using\ndifferent hyper-parameter settings.\nThe above criteria mainly focus on the learning process\nof the target agent. In addition, we introduce the following\nmetrics from the perspective of transferred knowledge, which,\nalthough commensurately important for evaluation, have not\nbeen explicitly discussed by prior art:\n• Necessary knowledge amount (nka): the necessary amount of\nthe knowledge required for TL in order to achieve certain\nperformance thresholds. Examples along this line include\nthe number of designed source tasks [67], the number of\nexpert policies, or the number of demonstrated interactions\n[68] required to enable knowledge transfer.\n• Necessary knowledge quality (nkq): the guaranteed quality\nof the knowledge required to enable effective TL. This\nmetric helps in answering questions such as (i) Does the\nTL approach rely on near-oracle knowledge, such as expert\ndemonstrations/policies [69], or (ii) is the TL technique\nfeasible even given suboptimal knowledge [63]?\nTL approaches differ in various perspectives, including the\nforms of transferred knowledge, the RL frameworks utilized\nto enable such transfer, and the gaps between the source and\nthe target domain. It maybe biased to evaluate TL from just\none viewpoint. We believe that explicating these TL related\nmetrics helps in designing more generalizable and efficient\nTL approaches.\nIn general, most of the abovementioned metrics can be\nconsidered as evaluating two abilities of a TL approach: the\nmastery and generalization. Mastery refers to how well the\nlearned agent can ultimately perform in the target domain,\nwhile generalization refers to the ability of the learning agent\nto quickly adapt to the target domain.\n4\nRELATED WORK\nThere are prior efforts in summarizing TL research in RL. One\nof the earliest literatures is [11] . Their main categorization\nis from the perspective of problem setting, in which the TL\nscenarios may vary in the number of domains involved, and\nthe difference of state-action space among domains. Similar\ncategorization is adopted by [12], with more refined analysis\ndimensions including the objective of TL. As pioneer surveys\nfor TL in RL, neither [11] nor [12] covered recent research\nover the last decade. For instance, [11] emphasized on\ndifferent task-mapping methods, which are more suitable for\ndomains with tabular or mild state-action space dimensions.\nThere are other surveys focused on specific subtopics that\ninterplay between RL and TL. For instance, [70] consolidated\nsim-to-real TL methods. They explored work that is more tai-\nlored for robotics domains, including domain generalization\nand zero-shot transfer, which is a favored application field\nof DRL as we discussed in Sec 6. [71] conducted extensive\ndatabase search and summarized benchmarks for evaluating\nTL algorithms in RL. [72] surveyed recent progress in\nmulti-task RL. They partially shared research focus with\nus by studying certain TL oriented solutions towards multi-\ntask RL, such as learning shared representations, pathNets,\netc. We surveyed TL for RL with a broader spectrum in\nmethodologies, applications, evaluations, which naturally\ndraws connections to the above literatures.\n5\nTRANSFER LEARNING APPROACHES DEEP DIVE\nIn this section, we elaborate on various TL approaches and\norganize them into different sub-topics, mostly by answering\nthe question of “what knowledge is transferred”. For each type\nof TL approach, we analyze them by following the other\ncriteria mentioned in Section 3 and and summarize the key\nevaluation metrics that are applicable to the discussed work.\nFigure 1 presents an overview of different TL approaches\ndiscussed in this survey.\n5.1\nReward Shaping\nWe start by introducing the Reward Shaping approach, as\nit is applicable to most RL backbones and also largely\noverlaps with the other TL approaches discussed later.\nReward Shaping (RS) is a technique that leverages the\nexterior knowledge to reconstruct the reward distribution of\nthe target domain to guide the agent’s policy learning. More\nspecifically, in addition to the environment reward signals,\nRS learns a reward-shaping function F : S × S × A →R\nto render auxiliary rewards, provided that the additional\nrewards contain external knowledge to guide the agent\nfor better action selections. Intuitively, an RS strategy will\nassign higher rewards to more beneficial state-actions to\nnavigate the agent to desired trajectories. As a result, the\nagent will learn its policy using the newly shaped rewards\nR′: R′ = R + F, which means that RS has altered the target\ndomain with a different reward function:\nM = (S, A, T , γ, R)) →M′ = (S, A, T , γ, R′).\n(1)\nAlong the line of RS, Potential based Reward Shaping (PBRS)\nis one of the most classical approaches. [61] proposed PBRS\nto form a shaping function F as the difference between two\npotential functions (Φ(·)):\nF(s, a, s′) = γΦ(s′) −Φ(s),\n(2)\nwhere the potential function Φ(·) comes from the knowledge\nof expertise and evaluates the quality of a given state.\nIt has been proved that, without further restrictions on\nthe underlying MDP or the shaping function F, PBRS\nis sufficient and necessary to preserve the policy invari-\nance. Moreover, the optimal Q-function in the original and\ntransformed MDP are related by the potential function:\nQ∗\nM′(s, a) = Q∗\nM(s, a) −Φ(s), which draws a connection\n6\nFig. 1: An overview of different TL approaches, organized by the format of transferred knowledge.\nbetween potential based reward-shaping and advantage-\nbased learning approaches [73].\nThe idea of PBRS was extended to [74], which formulated\nthe potential as a function over both the state and the\naction spaces. This approach is called Potential Based state-\naction Advice (PBA). The potential function Φ(s, a) therefore\nevaluates how beneficial an action a is to take from state s:\nF(s, a, s′, a′) = γΦ(s′, a′) −Φ(s, a).\n(3)\nPBA requires on-policy learning and can be sample-costly,\nas in Equation (3), a′ is the action to take upon state s is\ntransitioning to s′ by following the learning policy.\nTraditional RS approaches assumed a static potential\nfunction, until [75] proposed a Dynamic Potential Based (DPB)\napproach which makes the potential a function of both states\nand time: F(s, t, s′, t′) = γΦ(s′, t′) −Φ(s, t).They proved\nthat this dynamic approach can still maintain policy invari-\nance: Q∗\nM′(s, a) = Q∗\nM(s, a) −Φ(s, t),where t is the current\ntilmestep. [76] later introduced a way to incorporate any\nprior knowledge into a dynamic potential function structure,\nwhich is called Dynamic Value Function Advice (DPBA).\nThe rationale behind DPBA is that, given any extra reward\nfunction R+ from prior knowledge, in order to add this extra\nreward to the original reward function, the potential function\nshould satisfy: γΦ(s′, a′) −Φ(s, a) = F(s, a) = R+(s, a).\nIf Φ is not static but learned as an extra state-action\nValue function overtime, then the Bellman equation for Φ\nis : Φπ(s, a) = rΦ(s, a) + γΦ(s′, a′). The shaping rewards\nF(s, a) is therefore the negation of rΦ(s, a) :\nF(s, a) = γΦ(s′, a′) −Φ(s, a) = −rΦ(s, a).\n(4)\nThis leads to the approach of using the negation of R+ as\nthe immediate reward to train an extra state-action Value\nfunction Φ and the policy simultaneously. Accordingly, the\ndynamic potential function F becomes:\nFt(s, a) = γΦt+1(s′, a′) −Φt(s, a).\n(5)\nThe advantage of DPBA is that it provides a framework to\nallow arbitrary knowledge to be shaped as auxiliary rewards.\nResearch along this line mainly focus on designing\ndifferent shaping functions F(s, a), while not much work\nhas tackled the question of what knowledge can be used to\nderive this potential function. One work by [77] proposed to\nuse RS to transfer an expert policy from the source domain\nMs to the target domain Mt. This approach assumed\nthe existence of two mapping functions MS and MA that\ncan transform the state and action from the source to the\ntarget domain. Another work used demonstrated state-\naction samples from an expert policy to shape rewards [78].\nLearning the augmented reward involves learning a dis-\ncriminator to distinguish samples generated by an expert\npolicy from samples generated by the target policy. The\nloss of the discriminator is applied to shape rewards to\nincentivize the learning agent to mimic the expert behavior.\nThis work combines two TL approaches: RS and Learning\nfrom Demonstrations, the latter of which will be elaborated in\nSection 5.2.\nThe above-mentioned RS approaches are summarized\nin Table 1. They follow the potential based RS principle\nthat has been developed systematically: from the classical\nPBRS which is built on a static potential shaping function of\nstates, to PBA which generates the potential as a function of\nboth states and actions, and DPB which learns a dynamic\npotential function of states and time, to the most recent\nDPBA, which involves a dynamic potential function of states\nand actions to be learned as an extra state-action Value\nfunction in parallel with the environment Value function.\nAs an effective TL paradigm, RS has been widely applied to\nfields including robot training [79], spoken dialogue systems\n[80], and question answering [81]. It provides a feasible\nframework for transferring knowledge as the augmented\nreward and is generally applicable to various RL algorithms.\nRS has also been applied to multi-agent RL [82] and model-\nbased RL [83]. Principled integration of RS with other TL\napproaches, such as Learning from demonstrations (Section 5.2)\nand Policy Transfer (Section 5.3) will be an intriguing question\nfor ongoing research.\nNote that RS approaches discussed so far are built upon a\nconsensus that the source information for shaping the reward\ncomes externally, which coincides with the notion of knowledge\ntransfer. Some RS work also tackles the scenario where the\nshaped reward comes intrinsically. For instance, Belief Reward\nShaping was proposed by [84], which utilizes a Bayesian\nreward shaping framework to generate the potential value\nthat decays with experience, where the potential value comes\nfrom the critic itself.\n7\nMethods\nMDP difference\nFormat of shaping reward\nKnowledge source\nEvaluation metrics\nPBRS\nMs = Mt\nF = γΦ(s′) −Φ(s)\n✗\nap, ar\nPBA\nMs = Mt\nF = γΦ(s′, a′) −Φ(s, a)\n✗\nap, ar\nDPB\nMs = Mt\nF = γΦ(s′, t′) −Φ(s, t)\n✗\nap, ar\nDPBA\nMs = Mt\nFt = γΦt+1(s′, a′) −Φt(s, a) ,\nΦ learned as an extra Q function\n✗\nap, ar\n[77]\nSs ̸= St, As ̸= At\nFt = γΦt+1(s′, a′) −Φt(s, a)\nπs\nap, ar\n[78]\nMs = Mt\nFt = γΦt+1(s′, a′) −Φt(s, a)\nDE\nap, ar\nTABLE 1: A comparison of reward shaping approaches. ✗denotes that the information is not revealed in the paper.\n5.2\nLearning from Demonstrations\nLearning from Demonstrations (LfD) is a technique to assist\nRL by utilizing external demonstrations for more efficient\nexploration. The demonstrations may come from different\nsources with varying qualities. Research along this line\nusually address a scenario where the source and the target\nMDPs are the same: Ms = Mt, although there has been\nwork that learns from demonstrations generated in a different\ndomain [85, 86].\nDepending on when the demonstrations are used for\nknowledge transfer, approaches can be organized into offline\nand online methods. For offline approaches, demonstrations\nare either used for pre-training RL components, or for offline\nRL [87, 88]. When leveraging demonstrations for pre-training,\nRL components such as the value function V (s) [89], the\npolicy π [90], or the model of transition dynamics [91], can\nbe initialized by learning from demonstrations. For the online\napproach, demonstrations are directly used to guide agent\nactions for efficient explorations [92]. Most work discussed in\nthis section follows the online transfer paradigm or combines\noffline pre-training with online RL [93].\nWork along this line can also be categorized depending on\nwhat RL frameworks are compatible: some adopts the policy-\niteration framework [59, 94, 95], some follow a Q-learning\nframework [92, 96], while recent work usually follows the\npolicy-gradient framework [63, 78, 93, 97]. Demonstrations\nhave been leveraged in the policy iterations framework by [98].\nLater, [94] introduced the Direct Policy Iteration with Demon-\nstrations (DPID) algorithm. This approach samples complete\ndemonstrated rollouts DE from an expert policy πE, in\ncombination with the self-generated rollouts Dπ gathered\nfrom the learning agent. Dπ ∪DE are used to learn a Monte-\nCarlo estimation of the Q-value: ˆQ, from which a learning\npolicy can be derived greedily: π(s) = arg max\na∈A\nˆQ(s, a). This\npolicy π is further regularized by a loss function L(s, πE) to\nminimize its discrepancy from the expert policy decision.\nAnother example is the Approximate Policy Iteration with\nDemonstration (APID) algorithm, which was proposed by [59]\nand extended by [95]. Different from DPID where both DE\nand Dπ are used for value estimation, the APID algorithm\nsolely applies Dπ to approximate on the Q function. Expert\ndemonstrations DE are used to learn the value function,\nwhich, given any state si, renders expert actions πE(si) with\nhigher Q-value margins compared with other actions that\nare not shown in DE:\nQ(si, πE(si)) −\nmax\na∈A\\πE(si)Q(si, a) ≥1 −ξi.\n(6)\nThe term ξi is used to account for the case of imperfect\ndemonstrations. [95] further extended the work of APID\nwith a different evaluation loss:\nLπ = E(s,a)∼Dπ∥T ∗Q(s, a) −Q(s, a)∥,\n(7)\nwhere T ∗Q(s, a) = R(s, a) + γEs′∼p(.|s,a)[max\na′ Q(s′, a′)].\nTheir work theoretically converges to the optimal Q-function\ncompared with APID, as Lπ is minimizing the optimal\nBellman residual instead of the empirical norm.\nIn addition to policy iteration, the following two ap-\nproaches integrate demonstration data into the TD-learning\nframework, such as Q-learning. Specifically, [92] proposed\nthe DQfD algorithm, which maintains two separate replay\nbuffers to store demonstrated data and self-generated data,\nrespectively, so that expert demonstrations can always be\nsampled with a certain probability. Their method leverages\nthe refined priority replay mechanism [99] where the prob-\nability of sampling a transition i is based on its priority pi\nwith a temperature parameter α: P(i) =\npα\ni\nP\nk pα\nk . Another\nalgorithm named LfDS was proposed by [96], which draws a\nclose connection to reward shaping (Section 5.1). LfDS builds\nthe potential value of a state-action pair as the highest simi-\nlarity between the given pair and the expert demonstrations.\nThis augmented reward assigns more credits to state-actions\nthat are more similar to expert demonstrations, encouraging\nthe agent for expert-like behavior.\nBesides Q-learning, recent work has integrated LfD into\npolicy gradient [63, 69, 78, 93, 97]. A representative work\nalong this line is Generative Adversarial Imitation Learning\n(GAIL) [69]. GAIL introduced the notion of occupancy measure\ndπ, which is the stationary state-action distributions derived\nfrom a policy π. Based on this notion, a new reward function\nis designed such that maximizing the accumulated new\nrewards encourages minimizing the distribution divergence\nbetween the occupancy measure of the current policy π and\nthe expert policy πE. Specifically, the new reward is learned\nby adversarial training [62]: a discriminator D is learned to\ndistinguish interactions sampled from the current policy π\nand the expert policy πE:\nJD =\nmax\nD:S×A→(0,1) Edπ log[1 −D(s, a)] + EdE log[D(s, a)]\n(8)\nSince πE is unknown, its state-action distribution dE is\nestimated based on the given expert demonstrations DE.\nThe output of the discriminator is used as new rewards to\nencourage distribution matching, with r′(s, a) = −log(1 −\nD(s, a)). The RL process is naturally altered to perform\ndistribution matching by min-max optimization:\nmax\nπ\nmin\nD J(π, D) : = Edπ log[1 −D(s, a)] + EdE log[D(s, a)].\nThe philosophy in GAIL of using expert demonstrations\nfor distribution matching has inspired other LfD algorithms.\nFor example, [97] extended GAIL with an algorithm called\n8\nPolicy Optimization from Demonstrations (POfD), which com-\nbines the discriminator reward with the environment reward:\nmax\nθ\n= Edπ[r(s, a)] −λDJS[dπ||dE].\n(9)\nBoth GAIL and POfD are under an on-policy RL frame-\nwork. To further improve the sample efficiency of TL,\nsome off-policy algorithms have been proposed, such as\nDDPGfD [78] which is built upon the DDPG framework.\nDDPGfD shares a similar idea as DQfD in that they both\nuse a second replay buffer for storing demonstrated data,\nand each demonstrated sample holds a sampling priority\npi. For a demonstrated sample, its priority pi is augmented\nwith a constant bias ϵD > 0 for encouraging more frequent\nsampling of expert demonstrations:\npi = δ2\ni + λ∥∇aQ(si, ai|θQ)∥2 + ϵ + ϵD,\nwhere δi is the TD-residual for transition, ∥∇aQ(si, ai|θQ)∥2\nis the loss applied to the actor, and ϵ is a small positive\nconstant to ensure all transitions are sampled with some prob-\nability. Another work also adopted the DDPG framework to\nlearn from demonstrations [93]. Their approach differs from\nDDPGfD in that its objective function is augmented with\na Behavior Cloning Loss to encourage imitating on provided\ndemonstrations: LBC = P|DE|\ni=1 ||π(si|θπ) −ai||2.\nTo further address the issue of suboptimal demonstrations,\nin [93] the form of Behavior Cloning Loss is altered based on\nthe critic output, so that only demonstration actions with\nhigher Q values will lead to the loss penalty:\nLBC =\n|DE|\nX\ni=1\n∥π(si|θπ) −ai∥2 1[Q(si, ai) > Q(si, π(si))].\n(10)\nThere are several challenges faced by LfD, one of which\nis the imperfect demonstrations. Previous approaches usu-\nally presume near-oracle demonstrations. Towards tackling\nsuboptimal demonstrations, [59] leveraged the hinge-loss\nfunction to allow occasional violations of the property that\nQ(si, πE(si)) −\nmax\na∈A\\πE(si)Q(si, a) ≥1. Some other work\nuses regularized objective to alleviate overfitting on biased\ndata [92, 99]. A different strategy is to leverage those sub-\noptimal demonstrations only to boost the initial learning\nstage. For instance, [63] proposed Self-Adaptive Imitation\nLearning (SAIL), which learns from suboptimal demonstra-\ntions using generative adversarial training while gradually\nselecting self-generated trajectories with high qualities to\nreplace less superior demonstrations.\nAnother challenge faced by LfD is covariate drift ([100]):\ndemonstrations may be provided in limited numbers, which\nresults in the learning agent lacking guidance on states that\nare unseen in the demonstration dataset. This challenge is\naggravated in MDPs with sparse reward feedbacks, as the\nlearning agent cannot obtain much supervision information\nfrom the environment either. Current efforts to address this\nchallenge include encouraging explorations by using an\nentropy-regularized objective [101], decaying the effects of\ndemonstration guidance by softening its regularization on\npolicy learning over time [102], and introducing disagreement\nregularizations by training an ensemble of policies based on\nthe given demonstrations, where the variance among policies\nserves as a negative reward function [103].\nWe summarize the above-discussed approaches in Table 2.\nIn general, demonstration data can help in both offline pre-\ntraining for better initialization and online RL for efficient\nexploration. During the RL phase, demonstration data can\nbe used together with self-generated data to encourage\nexpert-like behaviors (DDPGfD, DQFD), to shape value\nfunctions (APID), or to guide the policy update in the form\nof an auxiliary objective function (PID,GAIL, POfD).\nTo\nvalidate the algorithm robustness given different knowledge\nresources, most LfD methods are evaluated using metrics that\neither indicate the performance under limited demonstrations\n(nka) or suboptimal demonstrations (nka). The integration of\nLfD with off-policy RL backbone makes it natural to adopt pe\nmetrics for evaluating how learning efficiency can be further\nimproved by knowledge transfer. Developing more general\nLfD approaches that are agnostic to RL frameworks and can\nlearn from sub-optimal or limited demonstrations would be\nthe ongoing focus for this research domain.\n5.3\nPolicy Transfer\nPolicy transfer is a TL approach where the external knowledge\ntakes the form of pre-trained policies from one or multiple\nsource domains. Work discussed in this section is built upon\na many-to-one problem setting, described as below:\nPolicy Transfer. A set of teacher policies πE1, πE2, . . . , πEK\nare trained on a set of source domains M1, M2, . . . , MK,\nrespectively. A student policy π is learned for a target domain\nby leveraging knowledge from {πEi}K\ni=1.\nFor the one-to-one scenario with only one teacher policy,\none can consider it as a special case of the above with K = 1.\nNext, we categorize recent work of policy transfer into two\ntechniques: policy distillation and policy reuse.\n5.3.1\nTransfer Learning via Policy Distillation\nThe idea of knowledge distillation has been applied to the\nfield of RL to enable policy distillation. Knowledge distillation\nwas first proposed by [104] as an approach of knowledge\nensemble from multiple teacher models into a single stu-\ndent model.\nConventional policy distillation approaches\ntransfer the teacher policy following a supervised learning\nparadigm [105, 106]. Specifically, a student policy is learned\nby minimizing the divergence of action distributions between\nthe teacher policy πE and student policy πθ, which is denoted\nas H×(πE(τt)|πθ(τt)):\nmin\nθ\nEτ∼πE[\n|τ|\nX\nt=1\n∇θH×(πE(τt)|πθ(τt))].\n(11)\nThe above expectation is taken over trajectories sampled from\nthe teacher policy πE, hence this approach is called teacher\ndistillation. One example along this line is [105], in which N\nteacher policies are learned for N source tasks separately, and\neach teacher yields a dataset DE = {si, qi}N\ni=0 consisting of\nobservations s and vectors of the corresponding Q-values\nq, such that qi = [Q(si, a1), Q(si, a2), ...|aj ∈A]. Teacher\npolicies are further distilled to a single student πθ by min-\nimizing the KL-Divergence between each teacher πEi(a|s)\nand the student πθ, approximated using the dataset DE:\nminθ DKL(πE|πθ) ≈P|DE|\ni=1 softmax\n\u0010\nqE\ni\nτ\n\u0011\nln\n\u0010\nsoftmax(qE\ni )\nsoftmax(qθ\ni )\n\u0011\n.\n9\nMethods\nOptimality\nguarantee\nFormat of transferred demonstrations\nRL framework\nEvaluation metrics\nDPID\n✓\nIndicator binary-loss : L(si) = 1{πE(si) ̸=\nπ(si)}\nAPI\nap, ar, nka\nAPID\n✗\nHinge loss on the marginal-loss:\n\u0002\nL(Q, π, πE)\n\u0003\n+\nAPI\nap, ar, nta, nkq\nAPID extend\n✓\nMarginal-loss: L(Q, π, πE)\nAPI\nap, ar, nta, nkq\n[93]\n✓\nIncreasing sampling priority and behavior cloning loss\nDDPG\nap, ar, tr, pe, nkq\nDQfD\n✗\nCached transitions in the replay buffer\nDQN\nap, ar, tr\nLfDS\n✗\nReward shaping function\nDQN\nap, ar, tr\nGAIL\n✓\nReward shaping function: −λ log(1 −D(s, a))\nTRPO\nap, ar, tr, pe, nka\nPOfD\n✓\nReward\nshaping\nfunction:\nr(s, a) −λ log(1 −D(s, a))\nTRPO,PPO\nap, ar, tr, pe, nka\nDDPGfD (pe)\n✓\nIncreasing sampling priority\nDDPG\nap, ar, tr, pe\nSAIL\n✗\nReward shaping function: r(s, a) −λ log(1 −D(s, a))\nDDPG\nap, ar, tr, pe, nkq, nka\nTABLE 2: A comparison of learning from demonstration approaches.\nAnother policy distillation approach is student distil-\nlation [51, 60], which is resemblant to teacher distilla-\ntion except that during the optimization step, the ob-\njective expectation is taken over trajectories sampled\nfrom the student policy instead of the teacher policy,\ni.e.: minθ Eτ∼πθ\nhP|τ|\nt=1 ∇θH×(πE(τt)|πθ(τt))\ni\n. [60] summa-\nrized related work on both kinds of distillation approaches.\nAlthough it is feasible to combine both distillation ap-\nproaches [100], we observe that more recent work focuses\non student distillation, which empirically shows better\nexploration ability compared to teacher distillation, especially\nwhen the teacher policies are deterministic.\nTaking an alternative perspective, there are two ap-\nproaches of policy distillation: (1) minimizing the cross-\nentropy between the teacher and student policy distributions\nover actions [51, 107]; and (2) maximizing the probability\nthat the teacher policy will visit trajectories generated by\nthe student, i.e. maxθ P(τ ∼πE|τ ∼πθ) [50, 108]. One\nexample of approach (1) is the Actor-mimic algorithm [51].\nThis algorithm distills the knowledge of expert agents into\nthe student by minimizing the cross entropy between the\nstudent policy πθ and each teacher policy πEi over actions:\nLi(θ) = P\na∈AEi πEi(a|s) logπθ(a|s), where each teacher\nagent is learned using a DQN framework. The teacher policy\nis therefore derived from the Boltzmann distributions over\nthe Q-function output: πEi(a|s) =\ne\nτ−1QEi (s,a)\nP\na′∈AEi e\nτ−1QEi (s,a′) . An\ninstantiation of approach (2) is the Distral algorithm [50].\nwhich learns a centroid policy πθ that is derived from K\nteacher policies. The knowledge in each teacher πEi is dis-\ntilled to the centroid and get transferred to the student, while\nboth the transition dynamics Ti and reward distributions Ri\nfor source domain Mi are heterogeneous. The student policy\nis learned by maximizing a multi-task learning objective\nmaxθ\nPK\ni=1 J(πθ, πEi), where\nJ(πθ, πEi) =\nX\nt\nE(st,at)∼πθ\nh X\nt≥0\nγt(ri(at, st)+\nα\nβ log πθ(at|st) −1\nβ log(πEi(at|st)))\ni\n,\nin which both log πθ(at|st) and πθ are used as augmented\nrewards. Therefore, the above approach also draws a close\nconnection to Reward Shaping (Section 5.1). In effect, the\nlog πθ(at|st) term guides the learning policy πθ to yield\nactions that are more likely to be generated by the teacher\npolicy, whereas the entropy term −log(πEi(at|st) encour-\nages exploration. A similar approach was proposed by [107]\nwhich only uses the cross-entropy between teacher and\nstudent policy λH(πE(at|st)||πθ(at|st)) to reshape rewards.\nMoreover, they adopted a dynamically fading coefficient\nto alleviate the effect of the augmented reward so that the\nstudent policy becomes independent of the teachers after\ncertain optimization iterations.\n5.3.2\nTransfer Learning via Policy Reuse\nPolicy reuse directly reuses policies from source tasks to build\nthe target policy. The notion of policy reuse was proposed by\n[109], which directly learns the target policy as a weighted\ncombination of different source-domain policies, and the\nprobability for each source domain policy to be used is\nrelated to its expected performance gain in the target domain:\nP(πEi) =\nexp (tWi)\nPK\nj=0 exp (tWj), where t is a dynamic temperature\nparameter that increases over time. Under a Q-learning\nframework, the Q-function of the target policy is learned\nin an iterative scheme: during every learning episode, Wi\nis evaluated for each expert policy πEi, and W0 is obtained\nfor the learning policy, from which a reuse probability P\nis derived. Next, a behavior policy is sampled from this\nprobability P. After each training episode, both Wi and\nthe temperature t for calculating the reuse probability is\nupdated accordingly. One limitation of this approach is that\nthe Wi, i.e. the expected return of each expert policy on the\ntarget task, needs to be evaluated frequently. This work was\nimplemented in a tabular case, leaving the scalability issue\nunresolved. More recent work by [110] extended the policy\nimprovement theorem [111] from one to multiple policies,\nwhich is named as Generalized Policy Improvement. We refer\nits main theorem as follows:\nTheorem. [Generalized Policy Improvement (GPI)] Let\n{πi}n\ni=1 be n policies and let { ˆQπi}n\ni=1 be their approximated\naction-value functions, s.t:\n\f\f\fQπi(s, a) −ˆQπi(s, a)\n\f\f\f ≤ϵ ∀s ∈\nS, a ∈A, and i ∈[n]. Define π(s) = arg max\na\nmax\ni\nˆQπi(s, a),\nthen: Qπ(s, a) ≥max\ni Qπi(s, a) −\n2\n1−γ ϵ, ∀s ∈S, a ∈A.\nBased on this theorem, a policy improvement approach\ncan be naturally derived by greedily choosing the action\nwhich renders the highest Q-value among all policies for\na given state. Another work along this line is [110], in\nwhich an expert policy πEi is also trained on a differ-\nent source domain Mi with reward function Ri, so that\nQπ\nM0(s, a) ̸= Qπ\nMi(s, a). To efficiently evaluate the Q-\nfunctions of different source policies in the target MDP,\na disentangled representation ψ(s, a) over the states and\n10\nactions is learned using neural networks and is generalized\nacross multiple tasks. Next, a task (reward) mapper wi is\nlearned, based on which the Q-function can be derived:\nQπ\ni (s, a) = ψ(s, a)T wi. [110] proved that the loss of GPI is\nbounded by the difference between the source and the target\ntasks. In addition to policy-reuse, their approach involves\nlearning a shared representation ψ(s, a), which is also a\nform of transferred knowledge and will be elaborated more\nin Section 5.5.2.\nWe summarize the abovementioned policy transfer ap-\nproaches in Table 3. In general, policy transfer can be realized\nby knowledge distillation, which can be either optimized\nfrom the student’s perspecive (student distillation), or from\nthe teacher’s perspective (teacher distillation) Alternatively,\nteacher policies can also be directly reused to update the target\npolicy. Regarding evaluation, most of the abovementioned\nwork has investigated a multi-teacher transfer scenario, hence\nthe generalization ability or robustness is largely evaluated on\nmetrics such as performance sensitivity(ps) (e.g. performance\ngiven different numbers of teacher policies or source tasks\n). Performance with fixed epochs (pe) is another commonly\nshared metric to evaluate how the learned policy can quickly\nadapt to the target domain. All approaches discussed so far\npresumed one or multiple expert policies, which are always\nat the disposal of the learning agent. Open questions along\nthis line include How to leverage imperfect policies for knowledge\ntransfer, or How to refer to teacher policies within a budget.\n5.4\nInter-Task Mapping\nIn this section, we review TL approaches that utilize mapping\nfunctions between the source and the target domains to assist\nknowledge transfer. Research in this domain can be analyzed\nfrom two perspectives: (1) which domain does the mapping\nfunction apply to, and (2) how is the mapped representation\nutilized. Most work discussed in this section shares a common\nassumption as below:\nAssumption. One-to-one mappings exist between the source\ndomain Ms and the target domain Mt.\nEarlier work along this line requires a given mapping\nfunction [66, 112]. One examples is [66] which assumes that\neach target state (action) has a unique correspondence in\nthe source domain, and two mapping functions XS, XA\nare provided over the state space and the action space,\nrespectively, so that XS(St) →Ss, XA(At) →As. Based\non XS and XA, a mapping function over the Q-values\nM(Qs) →Qt can be derived accordingly. Another work\nis done by [112] which transfers advice as the knowledge\nbetween two domains. In their settings, the advice comes\nfrom a human expert who provides the mapping function\nover the Q-values in the source domain and transfers it to the\nlearning policy for the target domain. This advice encourages\nthe learning agent to prefer certain good actions over others,\nwhich equivalently provides a relative ranking of actions in\nthe new task.\nMore later research tackles the inter-task mapping prob-\nlem by learning a mapping function [113–115]. Most work\nlearns a mapping function over the state space or a subset of\nthe state space. In their work, state representations are usu-\nally divided into agent-specific and task-specific representations,\ndenoted as sagent and senv, respectively. In [113] and [114],\nthe mapping function is learned on the agent-specific sub\nstate, and the mapped representation is applied to reshape\nthe immediate reward. For [113], the invariant feature space\nmapped from sagent can be applied across agents who have\ndistinct action space but share some morphological similarity.\nSpecifically, they assume that both agents have been trained\non the same proxy task, based on which the mapping function\nis learned. The mapping function is learned using an encoder-\ndecoder structure [116] to largely reserve information about\nthe source domain. For transferring knowledge from the\nsource agent to a new task, the environment reward is\naugmented with a shaped reward term to encourage the\ntarget agent to imitate the source agent on an embedded\nfeature space:\nr′(s, ·) = α\n\r\rf(ss\nagent; θf) −g(st\nagent; θg)\n\r\r ,\n(12)\nwhere f(ss\nagent) is the agent-specific state in the source\ndomain, and g(st\nagent) is for the target domain.\nAnother work is [115] which applied the Unsupervised\nManifold Alignment (UMA) method [117] to automatically\nlearn the state mapping. Their approach requires collecting\ntrajectories from both the source and the target domain\nto learn such a mapping. While applying policy gradient\nlearning, trajectories from the target domain Mt are first\nmapped back to the source: τt →τs, then an expert policy\nin the source domain is applied to each initial state of those\ntrajectories to generate near-optimal trajectories\n∼τs, which are\nfurther mapped to the target domain:\n∼τs →\n∼τt. The deviation\nbetween\n∼τt and τt are used as a loss to be minimized in order\nto improve the target policy. Similar ideas of using UMA for\ninter-task mapping can also be found in [118] and [119].\nIn addition to approaches that utilizes mapping over\nstates or actions, [120] proposed to learn an inter-task\nmapping over the transition dynamics space: S × A × S.\nTheir work assumes that the source and target domains\nare different in terms of the transition space dimensionality.\nTransitions from both the source domain ⟨ss, as, s′s⟩and\nthe target domain ⟨st, at, s′t⟩are mapped to a latent space\nZ. Given the latent feature representations, a similarity\nmeasure can be applied to find a correspondence between\nthe source and target task triplets. Triplet pairs with the\nhighest similarity in this feature space Z are used to learn a\nmapping function X : ⟨st, at, s′t⟩= X(⟨ss, as, s′s⟩). After the\ntransition mapping, states sampled from the expert policy in\nthe source domain can be leveraged to render beneficial states\nin the target domain, which assists the target agent learning\nwith a better initialization performance. A similar idea of\nmapping transition dynamics can be found in [121], which,\nhowever, requires a stronger assumption on the similarity\nof the transition probability and the state representations\nbetween the source and the target domains.\nAs summarized in Table 4, for TL approaches that utilize\nan inter-task mapping, the mapped knowledge can be (a\nsubset of) the state space [113, 114], the Q-function [66], or\n(representations of) the state-action-sate transitions [120].\nIn addition to being directly applicable in the target do-\nmain [120], the mapped representation can also be used as an\naugmented shaping reward [113, 114] or a loss objective [115]\nin order to guide the agent learning in the target domain.\nMost inter-task mapping methods tackle domains with\n11\nPaper\nTransfer approach\nMDP difference\nRL framework\nEvaluation metrics\n[105]\nDistillation\nS, A\nDQN\nap, ar\n[106]\nDistillation\nS, A\nDQN\nap, ar, pe, ps\n[51]\nDistillation\nS, A\nSoft Q-learning\nap, ar, tr, pe, ps\n[50]\nDistillation\nS, A\nA3C\nap, ar, pe, tt\n[109]\nReuse\nR\nTabular Q-learning\nap, ar, ps, tr\n[110]\nReuse\nR\nDQN\nap, ar, pe, ps\nTABLE 3: A comparison of policy transfer approaches.\nmoderate state-action space dimensions, such as maze tasks\nor tabular MDPs, where the goal can be reaching a target\nstate with a minimal number of transitions. Accordingly, tt\nhas been used to measure TL performance. For tasks with\nlimited and discrete state-action space, evaluation is also\nconducted with different number of initial states collected in\nthe target domain (nka).\n5.5\nRepresentation Transfer\nThis section review approaches that transfer knowledge in\nthe form of representations learned by deep neural networks.\nThey are built upon the following consensual assumption:\nAssumption. [Existence of Task-Invariance Subspace]\nThe state space (S), action space (A), or the reward space (R)\ncan be disentangled into orthogonal subspaces, which are task-\ninvariant such that knowledge can be transferred between domains\non the universal subspace.\nWe organize recent work along this line into two\nsubtopics: 1) approaches that directly reuse representations\nfrom the source domain (Section 5.5.1), and 2) approaches\nthat learn to disentangle the source domain representations\ninto independent sub-feature representations, some of which\nare on the universal feature space shared by both the source\nand the target domains (Section 5.5.2).\n5.5.1\nReusing Representations\nA representative work of reusing representations is [122],\nwhich proposed the progressive neural network structure to\nenable knowledge transfer across multiple RL tasks in a\nprogressive way. A progressive network is composed of\nmultiple columns, and each column is a policy network for\none specific task. It starts with one single column for training\nthe first task, and then the number of columns increases\nwith the number of new tasks. While training on a new task,\nneuron weights on the previous columns are frozen, and\nrepresentations from those frozen tasks are applied to the\nnew column via a collateral connection to assist in learning\nthe new task.\nProgressive network comes with a cost of large network\nstructures, as the network grows proportionally with the\nnumber of incoming tasks. A later framework called PathNet\nalleviates this issue by learning a network with a fixed\nsize [123]. PathNet contains pathways, which are subsets of\nneurons whose weights contain the knowledge of previous\ntasks and are frozen during training on new tasks. The pop-\nulation of pathway is evolved using a tournament selection\ngenetic algorithm [124].\nAnother approach of reusing representations for TL is\nmodular networks [52, 53, 125]. For example, [52] proposed\nto decompose the policy network into a task-specific module\nand agent-specific module. Specifically, let π be a policy\nperformed by any agent (robot) r over the task Mk as a\nfunction ϕ over states s, it can be decomposed into two\nsub-modules gk and fr, i.e.:\nπ(s) := ϕ(senv, sagent) = fr(gk(senv), sagent),\nwhere fr is the agent-specific module and gk is the task-\nspecific module. Their core idea is that the task-specific\nmodule can be applied to different agents performing\nthe same task, which serves as a transferred knowledge.\nAccordingly, the agent-specific module can be applied to\ndifferent tasks for the same agent.\nA model-based approach along this line is [125], which\nlearns a model to map the state observation s to a latent-\nrepresentation z. The transition probability is modeled on\nthe latent space instead of the original state space, i.e. ˆzt+1 =\nfθ(zt, at), where θ is the parameter of the transition model,\nzt is the latent-representation of the state observation, and at\nis the action accompanying that state. Next, a reward module\nlearns the value function as well as the policy from the\nlatent space z using an actor-critic framework. One potential\nbenefit of this latent representation is that knowledge can be\ntransferred across tasks that have different rewards but share\nthe same transition dynamics.\n5.5.2\nDisentangling Representations\nMethods discussed in this section mostly focus on learning a\ndisentangled representation. Specifically, we elaborate on TL\napproaches that are derived from two techniques: Successor\nRepresentation (SR) and Universal Value Function Approximat-\ning (UVFA).\nSuccessor Representations (SR) is an approach to de-\ncouple the state features of a domain from its reward\ndistributions. It enables knowledge transfer across multiple\ndomains: M = {M1, M2, . . . , MK}, so long as the only\ndifference among them is the reward distributions: Ri ̸= Rj.\nSR was originally derived from neuroscience, until [126]\nproposed to leverage it as a generalization mechanism for\nstate representations in the RL domain.\nDifferent from the v-value or Q-value that describes\nstates as dependent on the reward function, SR features\na state based on the occupancy measure of its successor\nstates. Specifically, SR decomposes the value function of\nany policy into two independent components, ψ and R:\nV π(s) = P\ns′ ψ(s, s′)w(s′), where w(s′) is a reward map-\nping function that maps states to scalar rewards, and ψ\nis the SR which describes any state s as the occupancy\nmeasure of the future occurred states when following π,\nwith 1[S = s′] = 1 as an indicator function:\nψ(s, s′) = Eπ[\n∞\nX\ni=t\nγi−t1[Si = s′]|St = s].\nThe successor nature of SR makes it learnable using\nany TD-learning algorithms. Especially, [126] proved the\n12\nMethods\nRL\nframework\nMDP\ndifference\nMapping\nfunction\nUsage of mapping\nEvaluation\nmetrics\n[66]\nSARSA\nSt ̸= St, As ̸= At\nM(Qs) →Qt\nQ value reuse\nap, ar, tt, tr\n[112]\nQ-learning\nAs ̸= At, Rs ̸= Rt\nM(Qs) →advice\nRelative Q ranking\nap, ar, tr\n[113]\n−\nSs ̸= St\nM(st) →r′\nReward shaping\nap, ar, pe, tr\n[114]\nSARSA(λ)\nSs ̸= St Rs ̸= Rt\nM(st) →r′\nReward shaping\nap, ar, pe, tt\n[115]\nFitted Value Iter-\nation\nSs ̸= St\nM(ss) →st\nPenalty loss on state deviation\nfrom expert policy\nap, ar, pe, tr\n[121]\nFitted Q Itera-\ntion\nSs × As ̸= St × At\nM\n\u0000(ss, as, s′\ns)\n→\n(st, at, s′\nt)\n\u0001\nReduce random exploration\nap, ar, pe, tr, nta\n[120]\n−\nSs × As ̸= St × At\nM\n\u0000(ss, as, s′\ns)\n→\n(st, at, s′\nt)\n\u0001\nReduce random exploration\nap, ar, pe, tr, nta\nTABLE 4: A comparison of inter-task mapping approaches. “−” indicates no RL framework constraints.\nfeasibility of learning such representation in a tabular case,\nin which the state transitions can be described using a matrix.\nSR was later extended by [110] from three perspectives:\n(i) the feature domain of SR is extended from states to\nstate-action pairs; (ii) deep neural networks are used as\nfunction approximators to represent the SR ψπ(s, a) and\nthe reward mapper w; (iii) Generalized policy improvement\n(GPI) algorithm is introduced to accelerate policy transfer for\nmulti-tasks (Section 5.3.2). These extensions, however, are\nbuilt upon a stronger assumption about the MDP:\nAssumption. [Linearity of Reward Distributions] The reward\nfunctions of all tasks can be computed as a linear combination\nof a fixed set of features: r(s, a, s′) = ϕ(s, a, s′)⊤w, where\nϕ(s, a, s′) ∈Rd denotes the latent representation of the state\ntransition, and w ∈Rd is the task-specific reward mapper.\nBased on this assumption, SR can be decoupled from the\nrewards when evaluating the Q-function of any policy π in\na task. The advantage of SR is that, when the knowledge\nof ψπ(s, a) in the source domain Ms is observed, one can\nquickly get the performance evaluation of the same policy\nin the target domain Mt by replacing ws with wt: Qπ\nMt =\nψπ(s, a)wt. Similar ideas of learning SR as a TD-algorithm\non a latent representation ϕ(s, a, s′) can also be found in\n[127, 128]. Specifically, the work of [127] was developed\nbased on a weaker assumption about the reward function:\nInstead of requiring linearly-decoupled rewards, the latent\nspace ϕ(s, a, s′) is learned in an encoder-decoder structure to\nensure that the information loss is minimized when mapping\nstates to the latent space. This structure, therefore, comes\nwith an extra cost of learning a decoder fd to reconstruct the\nstate: fd(ϕ(st)) ≈st.\nAn intriguing question faced by the SR approach is: Is\nthere a way that evades the linearity assumption about reward\nfunctions and still enables learning the SR without extra modular\ncost? An extended work of SR [67] answered this question\naffirmatively, which proved that the reward functions does\nnot necessarily have to follow the linear structure, yet at the\ncost of a looser performance lower-bound while applying the\nGPI approach for policy improvement. Especially, rather than\nlearning a reward-agnostic latent feature ϕ(s, a, s′) ∈Rd for\nmultiple tasks, [67] aims to learn a matrix ϕ(s, a, s′) ∈RD×d\nto interpret the basis functions of the latent space instead,\nwhere D is the number of seen tasks. Assuming k out of\nD tasks are linearly independent, this matrix forms k basis\nfunctions for the latent space. Therefore, for any unseen task\nMi, its latent features can be built as a linear combination\nof these basis functions, as well as its reward functions\nri(s, a, s′). Based on the idea of basis-functions for a task’s\nlatent space, they proposed that ϕ(s, a, s′) can be approxi-\nmated as learning R(s, a, s′) directly, where R(s, a, s′) ∈RD\nis a vector of reward functions for each seen task:\nR(s, a, s′) =\n\u0002r1(s, a, s′); r2(s, a, s′), . . . , rD(s, a, s′)\n\u0003.\nAccordingly, learning ψ(s, a) for any policy πi in Mi\nbecomes equivalent to learning a collection of Q-functions:\nψπi(s, a) =\n\u0002Qπi\n1 (s, a), Qπi\n2 (s, a), . . . , Qπi\nD (s, a)\n\u0003.\nA similar idea of using reward functions as features to\nrepresent unseen tasks is also proposed by [129], which\nassumes the ψ and w as observable quantities from the\nenvironment.\nUniversal Function Approximation (UVFA) is an alter-\nnative approach of learning disentangled state representa-\ntions [64]. Same as SR, UVFA allows TL for multiple tasks\nwhich differ only by their reward functions (goals). Different\nfrom SR which focuses on learning a reward-agnostic state\nrepresentation, UVFA aims to find a function approximator\nthat is generalized for both states and goals.\nThe UVFA\nframework is built on a specific problem setting of goal\nconditional RL: task goals are defined in terms of states, e.g. given\nthe state space S and the goal space G, it satisfies that G ⊆S.\nOne instantiation of this problem setting can be an agent\nexploring different locations in a maze, where the goals are\ndescribed as certain locations inside the maze. Under this\nproblem setting, a UVFA module can be decoupled into\na state embedding ϕ(s) and a goal embedding ψ(g), by\napplying the technique of matrix factorization to a reward\nmatrix describing the goal-conditional task.\nOne merit of UVFA resides in its transferrable embedding\nϕ(s) across tasks which only differ by goals. Another benefit\nis its ability of continual learning when the set of goals keeps\nexpanding over time. On the other hand, a key challenge\nof UVFA is that applying the matrix factorization is time-\nconsuming, which makes it a practical concern for complex\nenvironments with large state space |S|. Even with the\nlearned embedding networks, the third stage of fine-tuning\nthese networks via end-to-end training is still necessary.\nUVFA has been connected to SR by [67], in which a set\nof independent rewards (tasks) themselves can be used as\nfeatures for state representations. Another extended work\nthat combines UVFA with SR is called Universal Successor\nFeature Approximator (USFA), which is proposed by [130].\nFollowing the same linearity assumption, USFA is proposed\nas a function over a triplet of the state, action, and a policy\nembedding z: ϕ(s, a, z) : S × A × Rk →Rd, where z is the\n13\noutput of a policy-encoding mapping z = e(π) : S × A →Rk.\nBased on USFA, the Q-function of any policy π for a task\nspecified by w can be formularized as the product of a\nreward-agnostic Universal Successor Feature (USF) ψ and a\nreward mapper w: Q(s, a, w, z) = ψ(s, a, z)⊤w. Facilitated\nby the disentangled rewards and policy generalization, [130]\nfurther introduced a generalized TD-error as a function over\ntasks w and policy z, which allows them to approximate the\nQ-function of any policy on any task using a TD-algorithm.\n5.5.3\nSummary and Discussion\nWe provide a summary of the discussed work in this\nsection in Table 5. Representation transfer can facilitate TL\nin multiple ways based on assumptions about certain task-\ninvariant property. Some assume that tasks are different only\nin terms of their reward distributions. Other stronger as-\nsumptions include (i) decoupled dynamics, rewards [110], or\npolicies [130] from the Q-function representations, and (ii) the\nfeasibility of defining tasks in terms of states [130]. Based on\nthose assumptions, approaches such as TD-algorithms [67]\nor matrix-factorization [64] become applicable to learn\nsuch disentangled representations. To further exploit the\neffectiveness of disentangled structure, we consider that\ngeneralization approaches, which allow changing dynamics or\nstate distributions, are important future work that is worth\nmore attention in this domain.\nMost discussed work in this section tackles multi-task RL\nor meta-RL scenarios, hence the agent’s generalization ability\nis extensively investigated. For instance, methods of modular\nnetworks largely evaluated the zero-shot performance from\nthe meta-RL perspective [52, 130]. Given a fixed number of\ntraining epochs (pe), Transfer ratio (tr) is manifested differently\namong these methods. It can be the relative performance of\na modular net architecture compared with a baseline, or\nthe accumulated return in modified target domains, where\nreward scores are negated for evaluating the dynamics\ntransfer. Performance sensitivity (ps) is also broadly studied to\nestimate the robustness of TL. [110] analyzed the performance\nsensitivity given varying source tasks, while [130] studied\nthe performance on different unseen target domains.\nThere are unresolved questions in this intriguing research\ntopic. One is how to handle drastic changes of reward\nfunctions between domains. As discussed in [131], good\npolicies in one MDP may perform poorly in another due to\nthe fact that beneficial states or actions in Ms may become\ndetrimental in Mt with totally different reward functions.\nLearning a set of basis functions [67] to represent unseen\ntasks (reward functions), or decoupling policies from Q-\nfunction representation [130] may serve as a good start\nto address this issue, as they propose a generalized latent\nspace, from which different tasks (reward functions) can be\ninterpreted. However, the limitation of this work is that it is\nnot clear how many and what kind of sub-tasks need to be\nlearned to make the latent space generalizable enough.\nAnother question is how to generalize the representation\nlearning for TL across domains with different dynamics or\nstate-action spaces. A learned SR might not be transferrable\nto an MDP with different transition dynamics, as the distri-\nbution of occupancy measure for SR may no longer hold.\nPotential solutions may include model-based approaches\nthat approximate the dynamics directly or training a latent\nrepresentation space for states using multiple tasks with dif-\nferent dynamics for better generalization [132]. Alternatively,\nTL mechanisms from the supervised learning domain, such\nas meta-learning, which enables the ability of fast adaptation\nto new tasks [133], or importance sampling [134], which can\ncompensate for the prior distribution changes [10], may also\nshed light on this question.\n6\nAPPLICATIONS\nIn this section we summarize recent applications that are\nclosely related to using TL techniques for tackling RL\ndomains.\nRobotics learning is a prominent application domain of\nRL. TL approaches in this field include robotics learning from\ndemonstrations, where expert demonstrations from humans\nor other robots are leveraged [135] Another is collaborative\nrobotic training [136, 137], in which knowledge from different\nrobots is transferred by sharing their policies and episodic\ndemonstrations. Recent research focus is this domain is fast\nand robust adaptation to unseen tasks. One example towards\nthis goal is [138], in which robust robotics policies are trained\nusing synthetic demonstrations to handle dynamic environ-\nments. Another solution is to learn domain-invariant latent\nrepresentations. Examples include [139], which learns the\nlatent representation using 3D CAD models, and [140, 141]\nwhich are derived based on the Generative-Adversarial\nNetwork. Another example is DARLA [142], which is a zero-\nshot transfer approach to learn disentangled representations\nthat are robust against domain shifts. We refer readers to\n[70, 143] for detailed surveys along this direction.\nGame Playing is a common test-bed for TL and RL\nalgorithms. It has evolved from classical benchmarks such\nas grid-world games to more complex settings such as\nonline-strategy games or video games with multimodal\ninputs. One example is AlphaGo, which is an algorithm\nfor learning the online chessboard games using both TL\nand RL techniques [90]. AlphaGo is first pre-trained offline\nusing expert demonstrations and then learns to optimize its\npolicy using Monte-Carlo Tree Search. Its successor, AlphaGo\nMaster [144], even beat the world’s first ranked human\nplayer. TL-DRL approaches are also thriving in video game\nplaying. Especially, OpenAI has trained Dota2 agents that\ncan surpass human experts [145]. State-of-the-art platforms\ninclude MineCraft, Atari, and Starcraft. [146] designed new RL\nbenchmarks under the MineCraft platform. [147] provided\na comprehensive survey on DL applications in video game\nplaying, which also covers TL and RL strategies from certain\nperspectives. A large portion of TL approaches reviewed in\nthis survey have been applied to the Atari platforms [148].\nNatural Language Processing (NLP) has evolved rapidly\nalong with the advancement of DL and RL. Applications\nof RL to NLP range widely, from Question Answering\n(QA) [149], Dialogue systems [150], Machine Translation [151],\nto an integration of NLP and Computer Vision tasks, such as\nVisual Question Answering (VQA) [152], Image Caption [153],\netc. Many NLP applications have implicitly applied TL\napproaches. Examples include learning from expert demon-\nstrations for Spoken Dialogue Systems [154], VQA [152]; or\nreward shaping for Sequence Generation [155], Spoken Dialogue\nSystems [80],QA [81, 156], and Image Caption [153], or trans-\nferring policies for Structured Prediction [157] and VQA [158].\n14\nMethods\nRepresentations format\nAssumptions\nMDP difference\nLearner\nEvaluation\nmetrics\nProgressive Net [122]\nLateral connections to\npreviously learned net-\nwork modules\nN/A\nS, A\nA3C\nap, ar, pe, ps, tr\nPathNet [123]\nSelected neural paths\nN/A\nS, A\nA3C\nap, ar, pe, tr\nModular Net [52]\nTask(agent)-specific net-\nwork module\nDisentangled state rep-\nresentation\nS, A\nPolicy Gradient\nap, ar, pe, tt\nModular Net [125]\nDynamic\ntransitions\nmodule learned on state\nlatent representations.\nN/A\nS, A\nA3C\nap, ar, pe, tr, ps\nSR [110]\nSF\nReward function can be\nlinearly decoupled\nR\nDQN\nap, ar, nka, ps\nSR [127]\nEncoder-decoder\nlearned SF\nN/A\nR\nDQN\nap, ar, pe, ps\nSR [67]\nEncoder-decoder\nlearned SF\nRewards can be repre-\nsented by set of basis\nfunctions\nR\nQ(λ)\nap, pe\nUVFA [64]\nMatrix-factorized UF\nGoal conditional RL\nR\nTabular Q-learning\nap, ar, pe, ps\nUVFA with SR [130]\nPolicy-encoded UF\nReward function can be\nlinearly decoupled\nR\nϵ-greedy Q-learning\nap, ar, pe\nTABLE 5: A comparison of TL approaches of representation transfer.\nLarge Model Training: RL from human and model-\nassisted feedback becomes indispensable in training large\nmodels (LMM), such as GPT4 [159], Sparrow [160],\nPaLM [161], LaMDA [162], which have shown tremendous\nbreakthrough in dialogue applications, search engine answer\nimprovement, artwork generation, etc. The TL method at\nthe core of them is using human preferences as a reward\nsignal for model fine-tuning, where the preference ranking\nitself is considered as shaped rewards. We believe that TL\nwith carefully crafted human knowledge will help better\nalign large models with human intent and hence achieve\ntrustworthy and de-biased AI.\nHealth Informatics: RL has been applied to various\nhealthcare tasks [163], including automatic medical diagno-\nsis [164, 165], health resource scheduling [166], and drug discov-\nery and development, [167, 168], etc. Among these applications\nwe observe emerging trends of leveraging prior knowledge\nto improve the RL procedure, especially given the difficulty\nof accessing large amounts of clinical data. Specifically,\n[169] utilized Q-learning for drug delivery individualization.\nThey integrated the prior knowledge of the dose-response\ncharacteristics into their Q-learning framework to avoid\nrandom exploration. [170] applied a DQN framework for\nprescribing effective HIV treatments, in which they learned\na latent representation to estimate the uncertainty when\ntransferring a pertained policy to the unseen domains. [171]\nstudied applying human-involved interactive RL training for\nhealth informatics.\nOthers: RL has also been utilized in many other real-life\napplications. Applications in the Transportation Systems\nhave adopted RL to address traffic congestion issues with\nbetter traffic signal scheduling and transportation resource\nallocation [8, 9, 172, 173]. We refer readers to [174] for a review\nalong this line. Deep RL are also effective solutions to prob-\nlems in Finance, including portfolio management [175, 176],\nasset allocation [177], and trading optimization [178]. Another\napplication is the Electricity Systems, especially the intelligent\nelectricity networks, which can benefit from RL techniques for\nimproved power-delivery decisions [179, 180] and active\nresource management [181]. [7] provides a detailed survey\nof RL techniques for electric power system applications.\n7\nFUTURE PERSPECTIVES\nIn this section, we present some open challenges and future\ndirections in TL that are closely related to the DRL domain,\nbased on both retrospectives of the methods discussed in this\nsurvey and outlooks to the emerging trends of AI.\nTransfer Learning from Black-Box:\nRanging from exterior\nteacher demonstrations to pre-trained function approxima-\ntors, black-box resource is more accessible and predominant\nthan well-articulated knowledge. Therefore, leveraging such\nblack-box resource is indispensable for practical TL in\nDRL. One of its main challenges resides in estimating the\noptimality of black-box resource, which can be potentially\nnoisy or biased. We consider that efforts can be made from\nthe following perspectives:\n1) Inferring the reasoning mechanism inside the black-box.\nResemblant ideas have been explored in inverse RL\nand model-based RL, where the goal is to approximate\nthe reward function or to learn the dynamics model\nunder which the demonstrated knowledge becomes\nreasonable.\n2) Designing effective feedback schemes, including lever-\naging domain-provided rewards, intrinsic reward feed-\nback, or using human preference as feedback.\n3) Improving the interpretability of the transferred knowl-\nedge [182, 183], which benefits in evaluating and explain-\ning the process of TL from black-box. It can also alleviate\ncatastrophic decision-making for high-stake tasks such\nas auto-driving.\nKnowledge Disentanglement and Fusion are both towards\nbetter knowledge sharing across domains. Disentangling\nknowledge is usually a prerequisite for efficient knowledge\nfusion, which may involve external knowledge from mul-\ntiple source domains, with diverging qualities, presented in\ndifferent modalities, etc. Disentangling knowledge in RL can\nbe interpreted from different perspectives: i) disentangling\nthe action, state, or reward representations, as discussed in\nSec 5.5; 2) decomposing complex tasks into multiple skill\nsnippets. The former is an effective direction in tackling meta-\nRL and multi-task RL, although some solutions hinge on\nstrict assumptions of the problem setting, such as linear\ndependence among domain dynamics or learning goals. The\n15\nlatter is relevant to hierarchical RL and prototype learning from\nsequence data [184]. It is relatively less discussed besides few\npioneer research [132]. We believe that this direction is worth\nmore research efforts, which not only benefits interpretable\nknowledge learning, but also aligns with human perception.\nFramework-Agnostic Knowledge Transfer: Most contempo-\nrary TL approaches are designed for certain RL frameworks.\nSome are applicable to RL algorithms designed for the\ndiscrete-action space, while others may only be feasible\ngiven a continuous action space. One fundamental reason\nbehind is the diversified development of RL algorithms. We\nexpect that unified RL frameworks would contribute to the\nstandardization of TL approaches in this field.\nEvaluation and Benchmarking: Variant evaluation metrics\nhave been proposed to measure TL from different but\ncomplementary perspectives, although no single metric\ncan summarize the efficacy of a TL approach. Designing\na set of generalized, novel metrics is beneficial for the\ndevelopment of TL in DRL. Moreover, with the effervescent\ndevelopment of large-scale models, it is crucial to standardize\nevaluation from the perspectives of ethics and groundedness.\nThe appropriateness of the transferred knowledge, such as\npotential stereotypes in human preference, and the bias in the\nmodel itself should also be quantified as metrics.\nKnowledge Transfer to and from Pre-Trained Large Models:\nBy the time of this survey being finalized, unprecedented\nDL breakthroughs have been achieved in learning large-\nscale models built on massive computation resource and\nattributed data. One representative example is the Generative\nPre-trained Transformer (GPT) [159]. Considering them as\ncomplete knowledge graphs whose training process maybe\ninaccessible, there are more challenges in this direction\nbesides learning from a black-box, which are faced by a larger\nAI community including the RL domain. We briefly point\nout two directions that are worth ongoing attention:\n1) Efficient model fine-tuning with knowledge distillation. One\nimportant method for fine-tuning large models is RL\nwith human feedback, in which the quantity and quality\nof human ratings are critical for realizing a good reward\nmodel. We anticipate other forms of TL methods in\nRL to be explored to further improve the efficiency of\nfine-tuning, such as imitation learning with adversarial\ntraining to achieve human-level performance.\n2) Principled prompt-engineering for knowledge extraction.\nMore often the large model itself cannot be accessed,\nbut only input and output of models are allowed.\nSuch inference based knowledge extraction requires\ndelicate prompt designs. Some efficacious efforts include\ndesigning prompts with mini task examples as one-\nshot learning, decomposing complex tasks into architec-\ntural, contextual prompts. Prompt engineering is being\nproved an important direction for effective knowledge\nextraction, which with proper design, can largely benefit\ndownstream tasks that depend on large model resources.\n8\nACKNOWLEDGEMENTS\nThis research was supported by the National Science Foun-\ndation (IIS-2212174, IIS-1749940), National Institute of Aging\n(IRF1AG072449), and the Office of Naval Research (N00014-\n20-1-2382).\nREFERENCES\n[1]\nR. S. Sutton and A. G. Barto, Reinforcement learning: An\nintroduction.\nMIT press, 2018.\n[2]\nK. Arulkumaran, M. P. Deisenroth, M. Brundage, and\nA. A. Bharath, “A brief survey of deep reinforcement\nlearning,” arXiv preprint arXiv:1708.05866, 2017.\n[3]\nS. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-\nend training of deep visuomotor policies,” The Journal\nof Machine Learning Research, 2016.\n[4]\nS. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and\nD. Quillen, “Learning hand-eye coordination for\nrobotic grasping with deep learning and large-scale\ndata collection,” The International Journal of Robotics\nResearch, 2018.\n[5]\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling,\n“The arcade learning environment: An evaluation plat-\nform for general agents,” Journal of Artificial Intelligence\nResearch, 2013.\n[6]\nM. R. Kosorok and E. E. Moodie, Adaptive Treat-\nmentStrategies in Practice: Planning Trials and Analyzing\nData for Personalized Medicine.\nSIAM, 2015.\n[7]\nM. Glavic, R. Fonteneau, and D. Ernst, “Reinforce-\nment learning for electric power system decision and\ncontrol: Past considerations and perspectives,” IFAC-\nPapersOnLine, 2017.\n[8]\nS. El-Tantawy, B. Abdulhai, and H. Abdelgawad,\n“Multiagent reinforcement learning for integrated net-\nwork of adaptive traffic signal controllers (marlin-atsc):\nmethodology and large-scale application on downtown\ntoronto,” IEEE Transactions on Intelligent Transportation\nSystems, 2013.\n[9]\nH. Wei, G. Zheng, H. Yao, and Z. Li, “Intellilight: A\nreinforcement learning approach for intelligent traffic\nlight control,” ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, 2018.\n[10]\nS. J. Pan and Q. Yang, “A survey on transfer learning,”\nIEEE Transactions on knowledge and data engineering,\n2009.\n[11]\nM. E. Taylor and P. Stone, “Transfer learning for\nreinforcement learning domains: A survey,” Journal\nof Machine Learning Research, 2009.\n[12]\nA. Lazaric, “Transfer in reinforcement learning: a\nframework and a survey.”\nSpringer, 2012.\n[13]\nR. Bellman, “A markovian decision process,” Journal of\nmathematics and mechanics, 1957.\n[14]\nM. G. Bellemare, W. Dabney, and R. Munos, “A\ndistributional perspective on reinforcement learning,”\nin International conference on machine learning.\nPMLR,\n2017, pp. 449–458.\n[15]\nM. Liu, M. Zhu, and W. Zhang, “Goal-conditioned\nreinforcement learning: Problems and solutions,” arXiv\npreprint arXiv:2201.08299, 2022.\n[16]\nC. Florensa, D. Held, X. Geng, and P. Abbeel, “Au-\ntomatic goal generation for reinforcement learning\nagents,” in International conference on machine learning.\nPMLR, 2018, pp. 1515–1528.\n[17]\nZ. Xu and A. Tewari, “Reinforcement learning in\nfactored mdps: Oracle-efficient algorithms and tighter\nregret bounds for the non-episodic setting,” NeurIPS,\nvol. 33, pp. 18 226–18 236, 2020.\n16\n[18]\nC. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen,\nand Y. Wu, “The surprising effectiveness of ppo in\ncooperative multi-agent games,” NeurIPS, vol. 35, pp.\n24 611–24 624, 2022.\n[19]\nI. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and\nJ. Tompson, “Discriminator-actor-critic: Addressing\nsample inefficiency and reward bias in adversarial\nimitation learning,” arXiv preprint arXiv:1809.02925,\n2018.\n[20]\nG. A. Rummery and M. Niranjan, On-line Q-learning\nusing connectionist systems.\nUniversity of Cambridge,\nDepartment of Engineering Cambridge, England, 1994.\n[21]\nH. Van Seijen, H. Van Hasselt, S. Whiteson, and\nM. Wiering, “A theoretical and empirical analysis of\nexpected sarsa,” IEEE Symposium on Adaptive Dynamic\nProgramming and Reinforcement Learning, 2009.\n[22]\nV. Konda and J. Tsitsiklis, “Actor-critic algorithms,”\nNeurIPS, 2000.\n[23]\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap,\nT. Harley, D. Silver, and K. Kavukcuoglu, “Asyn-\nchronous methods for deep reinforcement learning,”\nICML, 2016.\n[24]\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft\nactor-critic: Off-policy maximum entropy deep rein-\nforcement learning with a stochastic actor,” Interna-\ntional Conference on Machine Learning, 2018.\n[25]\nC. J. Watkins and P. Dayan, “Q-learning,” Machine\nlearning, 1992.\n[26]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,\nJ. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski et al., “Human-level\ncontrol through deep reinforcement learning,” Nature,\n2015.\n[27]\nM. Hessel, J. Modayil, H. Van Hasselt, T. Schaul,\nG. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar,\nand D. Silver, “Rainbow: Combining improvements in\ndeep reinforcement learning,” AAAI, 2018.\n[28]\nR. J. Williams, “Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning,”\nMachine learning, 1992.\n[29]\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and\nP. Moritz, “Trust region policy optimization,” ICML,\n2015.\n[30]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, “Proximal policy optimization algorithms,”\narXiv preprint arXiv:1707.06347, 2017.\n[31]\nD. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra,\nand M. Riedmiller, “Deterministic policy gradient\nalgorithms,” 2014.\n[32]\nT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,\nY. Tassa, D. Silver, and D. Wierstra, “Continuous con-\ntrol with deep reinforcement learning,” arXiv preprint\narXiv:1509.02971, 2015.\n[33]\nS. Fujimoto, H. Van Hoof, and D. Meger, “Addressing\nfunction approximation error in actor-critic methods,”\narXiv preprint arXiv:1802.09477, 2018.\n[34]\nA. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine,\n“Neural network dynamics for model-based deep\nreinforcement learning with model-free fine-tuning,”\nin 2018 IEEE international conference on robotics and\nautomation (ICRA).\nIEEE, 2018, pp. 7559–7566.\n[35]\nZ. I. Botev, D. P. Kroese, R. Y. Rubinstein, and\nP. L’Ecuyer, “The cross-entropy method for optimiza-\ntion,” in Handbook of statistics.\nElsevier, 2013, vol. 31,\npp. 35–59.\n[36]\nK. Chua, R. Calandra, R. McAllister, and S. Levine,\n“Deep reinforcement learning in a handful of trials\nusing probabilistic dynamics models,” NeurIPS, vol. 31,\n2018.\n[37]\nR. S. Sutton, “Integrated architectures for learning,\nplanning, and reacting based on approximating dy-\nnamic programming,” in Machine learning proceedings\n1990.\nElsevier, 1990, pp. 216–224.\n[38]\nV. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gon-\nzalez, and S. Levine, “Model-based value estimation\nfor efficient model-free reinforcement learning,” arXiv\npreprint arXiv:1803.00101, 2018.\n[39]\nS. Levine and V. Koltun, “Guided policy search,” in\nInternational conference on machine learning.\nPMLR,\n2013, pp. 1–9.\n[40]\nH. Bharadhwaj, K. Xie, and F. Shkurti, “Model-\npredictive control via cross-entropy and gradient-based\noptimization,” in Learning for Dynamics and Control.\nPMLR, 2020, pp. 277–286.\n[41]\nM. Deisenroth and C. E. Rasmussen, “Pilco: A model-\nbased and data-efficient approach to policy search,” in\nProceedings of the 28th International Conference on machine\nlearning (ICML-11), 2011, pp. 465–472.\n[42]\nY. Gal, R. McAllister, and C. E. Rasmussen, “Improving\npilco with bayesian neural network dynamics models,”\nin Data-efficient machine learning workshop, ICML, vol. 4,\nno. 34, 2016, p. 25.\n[43]\nC. H. Lampert, H. Nickisch, and S. Harmeling, “Learn-\ning to detect unseen object classes by between-class\nattribute transfer,” IEEE Conference on Computer Vision\nand Pattern Recognition, 2009.\n[44]\nP. Dayan and G. E. Hinton, “Feudal reinforcement\nlearning,” NeurIPS, 1993.\n[45]\nR. S. Sutton, D. Precup, and S. Singh, “Between mdps\nand semi-mdps: A framework for temporal abstraction\nin reinforcement learning,” Artificial intelligence, 1999.\n[46]\nR. Parr and S. J. Russell, “Reinforcement learning with\nhierarchies of machines,” NeurIPS, 1998.\n[47]\nT. G. Dietterich, “Hierarchical reinforcement learning\nwith the maxq value function decomposition,” Journal\nof artificial intelligence research, 2000.\n[48]\nA. Lazaric and M. Ghavamzadeh, “Bayesian multi-\ntask reinforcement learning,” in ICML-27th international\nconference on machine learning.\nOmnipress, 2010, pp.\n599–606.\n[49]\nY. Zhang and Q. Yang, “A survey on multi-task\nlearning,” IEEE Transactions on Knowledge and Data\nEngineering, vol. 34, no. 12, pp. 5586–5609, 2021.\n[50]\nY. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirk-\npatrick, R. Hadsell, N. Heess, and R. Pascanu, “Distral:\nRobust multitask reinforcement learning,” NeurIPS,\n2017.\n[51]\nE. Parisotto, J. L. Ba, and R. Salakhutdinov, “Actor-\nmimic: Deep multitask and transfer reinforcement\nlearning,” ICLR, 2016.\n[52]\nC. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine,\n“Learning modular neural network policies for multi-\n17\ntask and multi-robot transfer,” 2017 IEEE International\nConference on Robotics and Automation (ICRA), 2017.\n[53]\nJ. Andreas, D. Klein, and S. Levine, “Modular multitask\nreinforcement learning with policy sketches,” ICML,\n2017.\n[54]\nR. Yang, H. Xu, Y. Wu, and X. Wang, “Multi-task rein-\nforcement learning with soft modularization,” NeurIPS,\nvol. 33, pp. 4767–4777, 2020.\n[55]\nT. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey,\n“Meta-learning in neural networks: A survey,” IEEE\ntransactions on pattern analysis and machine intelligence,\nvol. 44, no. 9, pp. 5149–5169, 2021.\n[56]\nZ. Jia, X. Li, Z. Ling, S. Liu, Y. Wu, and H. Su, “Im-\nproving policy optimization with generalist-specialist\nlearning,” in International Conference on Machine Learn-\ning.\nPMLR, 2022, pp. 10 104–10 119.\n[57]\nW. Ding, H. Lin, B. Li, and D. Zhao, “Generalizing goal-\nconditioned reinforcement learning with variational\ncausal reasoning,” arXiv preprint arXiv:2207.09081,\n2022.\n[58]\nR. Kirk, A. Zhang, E. Grefenstette, and T. Rockt¨aschel,\n“A survey of zero-shot generalisation in deep reinforce-\nment learning,” Journal of Artificial Intelligence Research,\nvol. 76, pp. 201–264, 2023.\n[59]\nB. Kim, A.-m. Farahmand, J. Pineau, and D. Pre-\ncup, “Learning from limited demonstrations,” NeurIPS,\n2013.\n[60]\nW. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar,\nG. Swirszcz, and M. Jaderberg, “Distilling policy dis-\ntillation,” The 22nd International Conference on Artificial\nIntelligence and Statistics, 2019.\n[61]\nA. Y. Ng, D. Harada, and S. Russell, “Policy invariance\nunder reward transformations: Theory and application\nto reward shaping,” ICML, 1999.\n[62]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n“Generative adversarial nets,” NeurIPS, pp. 2672–2680,\n2014.\n[63]\nZ. Zhu, K. Lin, B. Dai, and J. Zhou, “Learning sparse\nrewarded tasks from sub-optimal demonstrations,”\narXiv preprint arXiv:2004.00530, 2020.\n[64]\nT. Schaul, D. Horgan, K. Gregor, and D. Silver, “Uni-\nversal value function approximators,” ICML, 2015.\n[65]\nC. Finn and S. Levine, “Meta-learning: from few-shot\nlearning to rapid reinforcement learning,” ICML, 2019.\n[66]\nM. E. Taylor, P. Stone, and Y. Liu, “Transfer learning via\ninter-task mappings for temporal difference learning,”\nJournal of Machine Learning Research, 2007.\n[67]\nA. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver,\nM. Hessel, D. Mankowitz, A. ˇZ´ıdek, and R. Munos,\n“Transfer in deep reinforcement learning using suc-\ncessor features and generalised policy improvement,”\nICML, 2018.\n[68]\nZ. Zhu, K. Lin, B. Dai, and J. Zhou, “Off-policy\nimitation learning from observations,” NeurIPS, 2020.\n[69]\nJ. Ho and S. Ermon, “Generative adversarial imitation\nlearning,” NeurIPS, 2016.\n[70]\nW. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real\ntransfer in deep reinforcement learning for robotics: a\nsurvey,” in 2020 IEEE symposium series on computational\nintelligence (SSCI).\nIEEE, 2020, pp. 737–744.\n[71]\nM. Muller-Brockhausen, M. Preuss, and A. Plaat,\n“Procedural content generation: Better benchmarks\nfor transfer reinforcement learning,” in 2021 IEEE\nConference on games (CoG).\nIEEE, 2021, pp. 01–08.\n[72]\nN. Vithayathil Varghese and Q. H. Mahmoud, “A\nsurvey of multi-task deep reinforcement learning,”\nElectronics, vol. 9, no. 9, p. 1363, 2020.\n[73]\nR. J. Williams and L. C. Baird, “Tight performance\nbounds on greedy policies based on imperfect value\nfunctions,” Tech. Rep., 1993.\n[74]\nE. Wiewiora, G. W. Cottrell, and C. Elkan, “Principled\nmethods for advising reinforcement learning agents,”\nICML, 2003.\n[75]\nS. M. Devlin and D. Kudenko, “Dynamic potential-\nbased reward shaping,” ICAAMAS, 2012.\n[76]\nA. Harutyunyan, S. Devlin, P. Vrancx, and A. Now´e,\n“Expressing arbitrary reward functions as potential-\nbased advice,” AAAI, 2015.\n[77]\nT. Brys, A. Harutyunyan, M. E. Taylor, and A. Now´e,\n“Policy transfer using reward shaping,” ICAAMS, 2015.\n[78]\nM. Veˇcer´ık, T. Hester, J. Scholz, F. Wang, O. Pietquin,\nB. Piot, N. Heess, T. Roth¨orl, T. Lampe, and M. Ried-\nmiller, “Leveraging demonstrations for deep rein-\nforcement learning on robotics problems with sparse\nrewards,” arXiv preprint arXiv:1707.08817, 2017.\n[79]\nA. C. Tenorio-Gonzalez, E. F. Morales, and L. Vil-\nlase˜nor-Pineda, “Dynamic reward shaping: Training\na robot by voice,” Advances in Artificial Intelligence –\nIBERAMIA, 2010.\n[80]\nP.-H. Su, D. Vandyke, M. Gasic, N. Mrksic, T.-H.\nWen, and S. Young, “Reward shaping with recur-\nrent neural networks for speeding up on-line policy\nlearning in spoken dialogue systems,” arXiv preprint\narXiv:1508.03391, 2015.\n[81]\nX. V. Lin, R. Socher, and C. Xiong, “Multi-hop knowl-\nedge graph reasoning with reward shaping,” arXiv\npreprint arXiv:1808.10568, 2018.\n[82]\nS. Devlin, L. Yliniemi, D. Kudenko, and K. Tumer,\n“Potential-based difference rewards for multiagent\nreinforcement learning,” ICAAMS, 2014.\n[83]\nM. Grzes and D. Kudenko, “Learning shaping rewards\nin model-based reinforcement learning,” Proc. AAMAS\nWorkshop on Adaptive Learning Agents, 2009.\n[84]\nO. Marom and B. Rosman, “Belief reward shaping in\nreinforcement learning,” AAAI, 2018.\n[85]\nF.\nLiu,\nZ.\nLing,\nT.\nMu,\nand\nH.\nSu,\n“State\nalignment-based imitation learning,” arXiv preprint\narXiv:1911.10947, 2019.\n[86]\nK. Kim, Y. Gu, J. Song, S. Zhao, and S. Ermon, “Domain\nadaptive imitation learning,” ICML, 2020.\n[87]\nY. Ma, Y.-X. Wang, and B. Narayanaswamy, “Imitation-\nregularized offline learning,” International Conference\non Artificial Intelligence and Statistics, 2019.\n[88]\nM. Yang and O. Nachum, “Representation matters:\nOffline pretraining for sequential decision making,”\narXiv preprint arXiv:2102.05815, 2021.\n[89]\nX. Zhang and H. Ma, “Pretraining deep actor-critic\nreinforcement learning algorithms with expert demon-\nstrations,” arXiv preprint arXiv:1801.10459, 2018.\n[90]\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,\nG. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\n18\nV. Panneershelvam, M. Lanctot et al., “Mastering the\ngame of go with deep neural networks and tree search,”\nNature, 2016.\n[91]\nS. Schaal, “Learning from demonstration,” NeurIPS,\n1997.\n[92]\nT. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul,\nB. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband\net al., “Deep q-learning from demonstrations,” AAAI,\n2018.\n[93]\nA. Nair, B. McGrew, M. Andrychowicz, W. Zaremba,\nand P. Abbeel, “Overcoming exploration in reinforce-\nment learning with demonstrations,” IEEE International\nConference on Robotics and Automation (ICRA), 2018.\n[94]\nJ. Chemali and A. Lazaric, “Direct policy iteration\nwith demonstrations,” International Joint Conference on\nArtificial Intelligence, 2015.\n[95]\nB. Piot, M. Geist, and O. Pietquin, “Boosted bellman\nresidual minimization handling expert demonstra-\ntions,” Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases, 2014.\n[96]\nT. Brys, A. Harutyunyan, H. B. Suay, S. Chernova, M. E.\nTaylor, and A. Now´e, “Reinforcement learning from\ndemonstration through shaping,” International Joint\nConference on Artificial Intelligence, 2015.\n[97]\nB. Kang, Z. Jie, and J. Feng, “Policy optimization with\ndemonstrations,” ICML, 2018.\n[98]\nD. P. Bertsekas, “Approximate policy iteration: A\nsurvey and some new methods,” Journal of Control\nTheory and Applications, 2011.\n[99]\nT. Schaul, J. Quan, I. Antonoglou, and D. Silver,\n“Prioritized experience replay,” ICLR, 2016.\n[100] S. Ross, G. Gordon, and D. Bagnell, “A reduction of\nimitation learning and structured prediction to no-\nregret online learning,” AISTATS, 2011.\n[101] Y. Gao, J. Lin, F. Yu, S. Levine, T. Darrell et al., “Re-\ninforcement learning from imperfect demonstrations,”\narXiv preprint arXiv:1802.05313, 2018.\n[102] M. Jing, X. Ma, W. Huang, F. Sun, C. Yang, B. Fang,\nand H. Liu, “Reinforcement learning from imperfect\ndemonstrations under soft expert guidance.” AAAI,\n2020.\n[103] K. Brantley, W. Sun, and M. Henaff, “Disagreement-\nregularized imitation learning,” ICLR, 2019.\n[104] G. Hinton, O. Vinyals, and J. Dean, “Distilling the\nknowledge in a neural network,” Deep Learning and\nRepresentation Learning Workshop, NeurIPS, 2014.\n[105] A.\nA.\nRusu,\nS.\nG.\nColmenarejo,\nC.\nGulcehre,\nG. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih,\nK. Kavukcuoglu, and R. Hadsell, “Policy distillation,”\narXiv preprint arXiv:1511.06295, 2015.\n[106] H. Yin and S. J. Pan, “Knowledge transfer for deep\nreinforcement learning with hierarchical experience\nreplay,” AAAI, 2017.\n[107] S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Do-\nersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler, A. Zis-\nserman, K. Simonyan et al., “Kickstarting deep rein-\nforcement learning,” arXiv preprint arXiv:1803.03835,\n2018.\n[108] J. Schulman, X. Chen, and P. Abbeel, “Equivalence\nbetween policy gradients and soft q-learning,” arXiv\npreprint arXiv:1704.06440, 2017.\n[109] F. Fern´andez and M. Veloso, “Probabilistic policy reuse\nin a reinforcement learning agent,” Proceedings of the\nfifth international joint conference on Autonomous agents\nand multiagent systems, 2006.\n[110] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul,\nH. P. van Hasselt, and D. Silver, “Successor features for\ntransfer in reinforcement learning,” NuerIPS, 2017.\n[111] R. Bellman, “Dynamic programming,” Science, 1966.\n[112] L. Torrey, T. Walker, J. Shavlik, and R. Maclin, “Using\nadvice to transfer knowledge acquired in one reinforce-\nment learning task to another,” European Conference on\nMachine Learning, 2005.\n[113] A. Gupta, C. Devin, Y. Liu, P. Abbeel, and S. Levine,\n“Learning invariant feature spaces to transfer skills\nwith reinforcement learning,” ICLR, 2017.\n[114] G. Konidaris and A. Barto, “Autonomous shaping:\nKnowledge transfer in reinforcement learning,” ICML,\n2006.\n[115] H. B. Ammar and M. E. Taylor, “Reinforcement learn-\ning transfer via common subspaces,” Proceedings of the\n11th International Conference on Adaptive and Learning\nAgents, 2012.\n[116] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet:\nA deep convolutional encoder-decoder architecture\nfor image segmentation,” IEEE transactions on pattern\nanalysis and machine intelligence, 2017.\n[117] C. Wang and S. Mahadevan, “Manifold alignment\nwithout correspondence,” International Joint Conference\non Artificial Intelligence, 2009.\n[118] B. Bocsi, L. Csat´o, and J. Peters, “Alignment-based\ntransfer learning for robot models,” The 2013 Interna-\ntional Joint Conference on Neural Networks (IJCNN), 2013.\n[119] H. B. Ammar, E. Eaton, P. Ruvolo, and M. E. Taylor,\n“Unsupervised cross-domain transfer in policy gradient\nreinforcement learning via manifold alignment,” AAAI,\n2015.\n[120] H. B. Ammar, K. Tuyls, M. E. Taylor, K. Driessens, and\nG. Weiss, “Reinforcement learning transfer via sparse\ncoding,” ICAAMS, 2012.\n[121] A. Lazaric, M. Restelli, and A. Bonarini, “Transfer of\nsamples in batch reinforcement learning,” ICML, 2008.\n[122] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,\nJ. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and\nR. Hadsell, “Progressive neural networks,” arXiv\npreprint arXiv:1606.04671, 2016.\n[123] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha,\nA. A. Rusu, A. Pritzel, and D. Wierstra, “Pathnet:\nEvolution channels gradient descent in super neural\nnetworks,” arXiv preprint arXiv:1701.08734, 2017.\n[124] I. Harvey, “The microbial genetic algorithm,” European\nConference on Artificial Life, 2009.\n[125] A. Zhang, H. Satija, and J. Pineau, “Decoupling dy-\nnamics and reward for transfer learning,” arXiv preprint\narXiv:1804.10689, 2018.\n[126] P. Dayan, “Improving generalization for temporal dif-\nference learning: The successor representation,” Neural\nComputation, 1993.\n[127] T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gersh-\nman, “Deep successor reinforcement learning,” arXiv\npreprint arXiv:1606.02396, 2016.\n[128] J. Zhang, J. T. Springenberg, J. Boedecker, and W. Bur-\n19\ngard, “Deep reinforcement learning with successor\nfeatures for navigation across similar environments,”\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), 2017.\n[129] N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern,\n“Transfer in variable-reward hierarchical reinforcement\nlearning,” Machine Learning, 2008.\n[130] D. Borsa, A. Barreto, J. Quan, D. Mankowitz, R. Munos,\nH. van Hasselt, D. Silver, and T. Schaul, “Universal\nsuccessor features approximators,” ICLR, 2019.\n[131] L. Lehnert, S. Tellex, and M. L. Littman, “Advan-\ntages and limitations of using successor features for\ntransfer in reinforcement learning,” arXiv preprint\narXiv:1708.00102, 2017.\n[132] J. C. Petangoda, S. Pascual-Diaz, V. Adam, P. Vrancx,\nand\nJ.\nGrau-Moya,\n“Disentangled\nskill\nembed-\ndings for reinforcement learning,” arXiv preprint\narXiv:1906.09223, 2019.\n[133] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic\nmeta-learning for fast adaptation of deep networks,”\nICML, 2017.\n[134] B. Zadrozny, “Learning and evaluating classifiers\nunder sample selection bias,” ICML, 2004.\n[135] B. D. Argall, S. Chernova, M. Veloso, and B. Browning,\n“A survey of robot learning from demonstration,”\nRobotics and autonomous systems, 2009.\n[136] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A\nsurvey of research on cloud robotics and automation,”\nIEEE Transactions on automation science and engineering,\n2015.\n[137] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep\nreinforcement learning for robotic manipulation with\nasynchronous off-policy updates,” IEEE international\nconference on robotics and automation (ICRA), 2017.\n[138] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for\nthe unknown: Learning a universal policy with online\nsystem identification,” arXiv preprint arXiv:1702.02453,\n2017.\n[139] F. Sadeghi and S. Levine, “Cad2rl: Real single-image\nflight without a single real image,” arXiv preprint\narXiv:1611.04201, 2016.\n[140] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey,\nM. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Kono-\nlige et al., “Using simulation and domain adaptation to\nimprove efficiency of deep robotic grasping,” IEEE\nInternational Conference on Robotics and Automation\n(ICRA), 2018.\n[141] H. Bharadhwaj, Z. Wang, Y. Bengio, and L. Paull, “A\ndata-efficient framework for training and sim-to-real\ntransfer of navigation policies,” International Conference\non Robotics and Automation (ICRA), 2019.\n[142] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess,\nA. Pritzel, M. Botvinick, C. Blundell, and A. Lerchner,\n“Darla: Improving zero-shot transfer in reinforcement\nlearning,” ICML, 2017.\n[143] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement\nlearning in robotics: A survey,” The International Journal\nof Robotics Research, 2013.\n[144] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,\nA. Huang, A. Guez, T. Hubert, L. Baker, M. Lai,\nA. Bolton et al., “Mastering the game of go without\nhuman knowledge,” Nature, 2017.\n[145] OpenAI. (2019) Dotal2 blog. [Online]. Available:\nhttps://openai.com/blog/openai-five/\n[146] J. Oh, V. Chockalingam, S. Singh, and H. Lee, “Control\nof memory, active perception, and action in minecraft,”\narXiv preprint arXiv:1605.09128, 2016.\n[147] N. Justesen, P. Bontrager, J. Togelius, and S. Risi, “Deep\nlearning for video game playing,” IEEE Transactions on\nGames, 2019.\n[148] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,\nI. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing\natari with deep reinforcement learning,” arXiv preprint\narXiv:1312.5602, 2013.\n[149] H. Chen, X. Liu, D. Yin, and J. Tang, “A survey on\ndialogue systems: Recent advances and new frontiers,”\nAcm Sigkdd Explorations Newsletter, 2017.\n[150] S. P. Singh, M. J. Kearns, D. J. Litman, and M. A. Walker,\n“Reinforcement learning for spoken dialogue systems,”\nNeurIPS, 2000.\n[151] B.\nZoph\nand\nQ.\nV.\nLe,\n“Neural\narchitecture\nsearch with reinforcement learning,” arXiv preprint\narXiv:1611.01578, 2016.\n[152] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and\nK. Saenko, “Learning to reason: End-to-end module\nnetworks for visual question answering,” IEEE Interna-\ntional Conference on Computer Vision, 2017.\n[153] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, “Deep\nreinforcement learning-based image captioning with\nembedding reward,” IEEE Conference on Computer\nVision and Pattern Recognition, 2017.\n[154] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein,\n“Learning to compose neural networks for question\nanswering,” arXiv preprint arXiv:1601.01705, 2016.\n[155] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe,\nJ. Pineau, A. Courville, and Y. Bengio, “An actor-\ncritic algorithm for sequence prediction,” arXiv preprint\narXiv:1607.07086, 2016.\n[156] F. Godin, A. Kumar, and A. Mittal, “Learning when not\nto answer: a ternary reward structure for reinforcement\nlearning based question answering,” Proceedings of\nthe 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, 2019.\n[157] K.-W. Chang, A. Krishnamurthy, A. Agarwal, J. Lang-\nford, and H. Daum´e III, “Learning to search better than\nyour teacher,” 2015.\n[158] J. Lu, A. Kannan, J. Yang, D. Parikh, and D. Batra,\n“Best of both worlds: Transferring knowledge from\ndiscriminative learning to a generative visual dialog\nmodel,” NeurIPS, 2017.\n[159] OpenAI, “Gpt-4 technical report,” arXiv, 2023.\n[160] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides,\nV. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chad-\nwick, P. Thacker et al., “Improving alignment of dia-\nlogue agents via targeted human judgements,” arXiv\npreprint arXiv:2209.14375, 2022.\n[161] A. Chowdhery, S. Narang, J. Devlin, M. Bosma,\nG. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sut-\nton, S. Gehrmann et al., “Palm: Scaling language mod-\neling with pathways,” arXiv preprint arXiv:2204.02311,\n2022.\n20\n[162] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer,\nA. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker,\nY. Du et al., “Lamda: Language models for dialog\napplications,” arXiv preprint arXiv:2201.08239, 2022.\n[163] C. Yu, J. Liu, and S. Nemati, “Reinforcement learning in\nhealthcare: A survey,” arXiv preprint arXiv:1908.08796,\n2019.\n[164] A. Alansary, O. Oktay, Y. Li, L. Le Folgoc, B. Hou,\nG. Vaillant, K. Kamnitsas, A. Vlontzos, B. Glocker,\nB. Kainz et al., “Evaluating reinforcement learning\nagents for anatomical landmark detection,” 2019.\n[165] K. Ma, J. Wang, V. Singh, B. Tamersoy, Y.-J. Chang,\nA. Wimmer, and T. Chen, “Multimodal image reg-\nistration with deep context reinforcement learning,”\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, 2017.\n[166] T. S. M. T. Gomes, “Reinforcement learning for primary\ncare e appointment scheduling,” 2017.\n[167] A. Serrano, B. Imbern´on, H. P´erez-S´anchez, J. M. Ce-\ncilia, A. Bueno-Crespo, and J. L. Abell´an, “Accelerating\ndrugs discovery with deep reinforcement learning:\nAn early approach,” International Conference on Parallel\nProcessing Companion, 2018.\n[168] M. Popova, O. Isayev, and A. Tropsha, “Deep rein-\nforcement learning for de novo drug design,” Science\nadvances, 2018.\n[169] A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A.\nJacobs, J. M. Zurada, and M. E. Brier, “Incorporating\nprior knowledge into q-learning for drug delivery\nindividualization,” Fourth International Conference on\nMachine Learning and Applications, 2005.\n[170] T. W. Killian, S. Daulton, G. Konidaris, and F. Doshi-\nVelez, “Robust and efficient transfer learning with hid-\nden parameter markov decision processes,” NeurIPS,\n2017.\n[171] A. Holzinger, “Interactive machine learning for health\ninformatics: when do we need the human-in-the-loop?”\nBrain Informatics, 2016.\n[172] L. Li, Y. Lv, and F.-Y. Wang, “Traffic signal timing\nvia deep reinforcement learning,” IEEE/CAA Journal of\nAutomatica Sinica, 2016.\n[173] K. Lin, R. Zhao, Z. Xu, and J. Zhou, “Efficient large-\nscale fleet management via multi-agent deep reinforce-\nment learning,” ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, 2018.\n[174] K.-L. A. Yau, J. Qadir, H. L. Khoo, M. H. Ling, and\nP. Komisarczuk, “A survey on reinforcement learning\nmodels and algorithms for traffic signal control,” ACM\nComputing Surveys (CSUR), 2017.\n[175] J. Moody, L. Wu, Y. Liao, and M. Saffell, “Performance\nfunctions and reinforcement learning for trading sys-\ntems and portfolios,” Journal of Forecasting, 1998.\n[176] Z. Jiang and J. Liang, “Cryptocurrency portfolio man-\nagement with deep reinforcement learning,” IEEE\nIntelligent Systems Conference (IntelliSys), 2017.\n[177] R. Neuneier, “Enhancing q-learning for optimal asset\nallocation,” NeurIPS, 1998.\n[178] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, “Deep\ndirect reinforcement learning for financial signal rep-\nresentation and trading,” IEEE transactions on neural\nnetworks and learning systems, 2016.\n[179] G. Dalal, E. Gilboa, and S. Mannor, “Hierarchical\ndecision making in electricity grid management,” In-\nternational Conference on Machine Learning, 2016.\n[180] F. Ruelens, B. J. Claessens, S. Vandael, B. De Schutter,\nR. Babuˇska, and R. Belmans, “Residential demand\nresponse of thermostatically controlled loads using\nbatch reinforcement learning,” IEEE Transactions on\nSmart Grid, 2016.\n[181] Z. Wen, D. O’Neill, and H. Maei, “Optimal demand\nresponse using device-based reinforcement learning,”\nIEEE Transactions on Smart Grid, 2015.\n[182] Y. Li, J. Song, and S. Ermon, “Infogail: Interpretable im-\nitation learning from visual demonstrations,” NeurIPS,\n2017.\n[183] R. Ramakrishnan and J. Shah, “Towards interpretable\nexplanations for transfer learning in sequential tasks,”\nAAAI Spring Symposium Series, 2016.\n[184] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and\nW. Stewart, “Retain: An interpretable predictive model\nfor healthcare using reverse time attention mechanism,”\nNeurIPS, vol. 29, 2016.\nZhuangdi Zhu is currently a senior data and\napplied scientist with Microsoft. She obtained her\nPh.D. degree from the Computer Science depart-\nment of Michigan State University. Zhuangdi has\nregularly published on prestigious machine learn-\ning conferences including NeurIPs, ICML, KDD,\nAAAI, etc. Her research interests reside in both\nfundamental and applied machine learning. Her\ncurrent research involves reinforcement learning\nand distributed machine learning.\nKaixiang Lin is an applied scientist at Amazon\nweb services. He obtained his Ph.D. from Michi-\ngan State University. He has broad research in-\nterests across multiple fields, including reinforce-\nment learning, human-robot interactions, and nat-\nural language processing. His research has been\npublished on multiple top-tiered machine learning\nand data mining conferences such as ICLR, KDD,\nNeurIPS, etc. He serves as a reviewer for top\nmachine learning conferences regularly.\nAnil K. Jain is a University distinguished pro-\nfessor in the Department of Computer Science\nand Engineering at Michigan State University. His\nresearch interests include pattern recognition and\nbiometric authentication. He served as the editor-\nin-chief of the IEEE Transactions on Pattern\nAnalysis and Machine Intelligence and was a\nmember of the United States Defense Science\nBoard. He has received Fulbright, Guggenheim,\nAlexander von Humboldt, and IAPR King Sun Fu\nawards. He is a member of the National Academy\nof Engineering and a foreign fellow of the Indian National Academy of\nEngineering and the Chinese Academy of Sciences.\nJiayu Zhou is an associate professor in the De-\npartment of Computer Science and Engineering\nat Michigan State University. He received his\nPh.D. degree in computer science from Arizona\nState University in 2014. He has broad research\ninterests in the fields of large-scale machine\nlearning and data mining as well as biomedical in-\nformatics. He has served as a technical program\ncommittee member for premier conferences such\nas NIPS, ICML, and SIGKDD. His papers have\nreceived the Best Student Paper Award at the\n2014 IEEE International Conference on Data Mining (ICDM), the Best\nStudent Paper Award at the 2016 International Symposium on Biomedical\nImaging (ISBI) and the Best Paper Award at IEEE Big Data 2016.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-09-16",
  "updated": "2023-07-04"
}