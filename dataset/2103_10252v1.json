{
  "id": "http://arxiv.org/abs/2103.10252v1",
  "title": "Augmenting Supervised Learning by Meta-learning Unsupervised Local Rules",
  "authors": [
    "Jeffrey Cheng",
    "Ari Benjamin",
    "Benjamin Lansdell",
    "Konrad Paul Kordin"
  ],
  "abstract": "The brain performs unsupervised learning and (perhaps) simultaneous\nsupervised learning. This raises the question as to whether a hybrid of\nsupervised and unsupervised methods will produce better learning. Inspired by\nthe rich space of Hebbian learning rules, we set out to directly learn the\nunsupervised learning rule on local information that best augments a supervised\nsignal. We present the Hebbian-augmented training algorithm (HAT) for combining\ngradient-based learning with an unsupervised rule on pre-synpatic activity,\npost-synaptic activities, and current weights. We test HAT's effect on a simple\nproblem (Fashion-MNIST) and find consistently higher performance than\nsupervised learning alone. This finding provides empirical evidence that\nunsupervised learning on synaptic activities provides a strong signal that can\nbe used to augment gradient-based methods.\n  We further find that the meta-learned update rule is a time-varying function;\nthus, it is difficult to pinpoint an interpretable Hebbian update rule that\naids in training. We do find that the meta-learner eventually degenerates into\na non-Hebbian rule that preserves important weights so as not to disturb the\nlearner's convergence.",
  "text": "Augmenting Supervised Learning by Meta-learning\nUnsupervised Local Rules\nJeffrey Cheng ∗\nDepartment of Computer Science\nUniversity of Pennsylvania\nPhiladelphia, PA 19104\njeffch@seas.upenn.edu\nAri Benjamin\nDepartment of Bioengineering\nUniversity of Pennsylvania\nPhiladelphia, PA 19104\naarrii@seas.upenn.edu\nBenjamin Lansdell\nDepartment of Bioengineering\nUniversity of Pennsylvania\nPhiladelphia, PA 19104\nlansdell@seas.upenn.edu\nKonrad Paul Kording\nDepartment of Bioengineering\nUniversity of Pennsylvania\nPhiladelphia, PA 19104\nkording@upenn.edu\nAbstract\nThe brain performs unsupervised learning and (perhaps) simultaneous supervised\nlearning. This raises the question as to whether a hybrid of supervised and un-\nsupervised methods will produce better learning. Inspired by the rich space of\nHebbian learning rules, we set out to directly learn the unsupervised learning\nrule on local information that best augments a supervised signal. We present the\nHebbian-augmented training algorithm (HAT) for combining gradient-based learn-\ning with an unsupervised rule on pre-synpatic activity, post-synaptic activities, and\ncurrent weights. We test HAT’s effect on a simple problem (Fashion-MNIST) and\nﬁnd consistently higher performance than supervised learning alone. This ﬁnd-\ning provides empirical evidence that unsupervised learning on synaptic activities\nprovides a strong signal that can be used to augment gradient-based methods.\nWe further ﬁnd that the meta-learned update rule is a time-varying function; thus,\nit is difﬁcult to pinpoint an interpretable Hebbian update rule that aids in training.\nWe do ﬁnd that the meta-learner eventually degenerates into a non-Hebbian rule\nthat preserves important weights so as not to disturb the learner’s convergence.\n1\nPrior Work and the Local Meta-Learning Setting\nBackpropagation achieves great performance in neural net optimization, but might not be biologi-\ncally plausible because most problems are not explicitly phrased as classiﬁcation with true labels,\nbecause neurons only know local signals (e.g. synaptic density, ACh levels, current), and because\nbackpropagation uses the computational graph, a separate data structure with no known biological\nbasis.\nAlthough some supervised training schemes are more biologically plausible (e.g. contrastive Hebbian\nlearning[9] and equilibrium propagation[8]), it’s currently unknown whether the behavior of all\nneurons is accurately encapsulated by these models. We speculate that some local, unsupervised\nlearning occurs in the brain and demonstrate that the addition of local, unsupervised rules to standard\nbackpropagation actually improves the speed and robustness of learning.\n∗Please ﬁnd my most recent contact information at http://jeffreyscheng.com/.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:2103.10252v1  [cs.LG]  17 Mar 2021\n2\n1.1\nLocal Learning Rules\nWe begin by deﬁning a local learning rule. Consider two adjacent neurons i, j with weight wij: given\nan impulse traversing i, j with activations vi, vj, a local learning rule computes updates ∆wij using\nlocal data vi, wij, vj. Note that by this deﬁnition, a local learning rule is unsupervised at face value.\nMany neuroscientists have hypothesized speciﬁc functions that describe the brain’s true (unsupervised)\nlocal learning rule. Most such rules involve using the correlation of activations as part of the update\nrule. Examples include Hebb’s Rule [4], Oja’s Rule [7], the Generalized Hebbian Algorithm [3],\nand nonlineear Hebbian rules [5].\nIt is not obvious which of these rules (if any) describe the true behavior of neurons. We employ\nmeta-learning (learning how to learn) as an investigative tool.\n1.2\nThe Meta-Learning Framework\nOptimization functions are algorithms too; it stands to reason that we can learn the best optimization\nfunction. In the meta-learning framework, one model A learns a task (e.g. Fashion-MNIST) while\nanother model B learns how to optimize A.\nMeta-learning has achieved great results in ﬁnding robust optimization schemes. Andrychowicz et. al.\nused meta-learning to ﬁnd the best gradient-based optimization function (B learns to update A using\nA’s gradients) [1], and Chen et. al. used meta-learning to ﬁnd the best gradient-free optimization\nfunction (B learns to update A using only the sequence of A’s losses). [2] Finally, Metz et al.\ndemonstrated a fully differentiable architecture for learning to learn unsupervised local rules and\ndemonstrate better-than-random performance on a few-shot basis. [6]\nIf B consistently converges to some stable rule, we take it as strong evidence that this rule may\noccur in biological brains as well. We therefore wish to extend Metz’s approach to learning semi-\nsupervised local rules not only to improve performance but also to investigate the functional form of\nthe meta-learned update rule.\n2\nThe Hebbian-Augmented Training Algorithm\nThe Hebbian-Augmented Training algorithm (HAT) is an algorithm that trains the neural net L\ntwice per sample: using local, unsupervised rules on the forward pass and using backpropagation-\nbased gradient descent on the backward pass.\nFormally, we create 2 multilayer perceptrons: a learner L(· | φL) with parameters φL and a meta-\nlearner M(vi, vj, wij | φM) with parameters φM, which takes inputs vi, wij, vj and returns ∆wij.\nFor a single sample (⃗x, ⃗y), we train L without supervision using M and ⃗x; we simultaneously train L\nand M with supervision using A and ⃗y.\n2.1\nPhase 1: The Forward Pass\nOn the forward pass, we compute activations for each layer. For a given layer ℓ, we now have the\ninputs, outputs, and current weights – all of the inputs of local learning rule. We can then apply\nthe outputs of meta-learner M to update the weights of layer ℓ. We then recompute the activations\nof layer ℓusing the new weights. This process is done efﬁciently by convolution (for details, see\nAppendix A). We compute the activations of the ﬁrst layer ℓ1, update ℓ1, compute the activations of\nthe second layer ℓ2, update ℓ2, and so on until we compute the predicted Weights ˆ⃗y and update ℓ|L|.\n2.2\nPhase 2: The Backward Pass\nOn the backward pass, we backpropagate. Since we recomputed the activations of each layer using\nweights updated by M, the weights of M are upstream of the weights of L in the computational\ngraph; thus, a single iteration of the backpropagation algorithm will compute gradients for both M\nand L. Given a gradient ∇p for each parameter p ∈φL ∪φM, we then perform a supervised update\np ←p + A(p, ∇p). The key insight is that the convolution of the meta-learner over the weights of\nthe learner forms a fully differentiable framework M ⇝L ⇝⃗y.\n3\nAlgorithm 1 Hebbian-Augmented Training Algorithm\n1: procedure TRAIN-EXAMPLE(L, M, A, ⃗x, ⃗y)\n2:\n⃗v0 ←⃗x\n▷Let ⃗vi represent the impulse in layer i\n3:\nfor weights Wℓ, ℓ∈[1...|L|] do\n▷Forward pass\n4:\nˆvℓ+1 = σ(Wℓ× ⃗vℓ+ bℓ)\n▷ˆvℓ+1 is a placeholder output as input to M\n5:\nWℓ←Wℓ+ M(⃗vℓ, Wℓ,⃗vℓ+1)\n▷Updates weight using local rule M\n6:\n⃗vℓ+1 ←σ(Wℓ× ⃗vℓ+ bℓ)\n▷Propagate ˆvℓ+1 as actual layer output\n7:\nBackpropagate loss H(⃗v|L|, ⃗y).\n8:\nfor layer weight Wℓin L and M do\n▷Backward pass\n9:\nWℓ←A\n\u0010\n∂H\n∂Wℓ\n\u0011\n▷Apply gradient update using optimizer A\n10:\nreturn L, M\n▷Return updated learner and updated meta-learner\n3\nHAT Improves Performance on Fashion-MNIST\nWe hypothesize that the HAT algorithm will have three positive effects.\n• HAT will train the learner L faster since there are twice as many updates. In ordinary\nbackpropagation the metadata generated from the forward pass is computed and wasted; in\nHAT, the metadata is computed and used to generate a (potentially) useful update.\n• HAT will improve the convergence of L. The second update should introduce some\nstochasticity in the loss landscape since it is not directly tied to gradient descent, which may\nlead L into better local optima.\n• HAT will improve the performance of L when some examples are not labeled. Backprop-\nagation has no ability to learn from just the input ⃗x, while HAT is able to perform the\nunsupervised update.\nWe generate two learning curves to test these hypotheses: one with respect to time and one with\nrespect to the proportion of labeled examples. The charts below represent the aggregated learning\ncurves of 100 pairs (Li, Mi).\nFigure 1: The effect of HAT’s on median accuracy curves.\nWe ﬁnd that the effects of HAT on training are clearly positive. The median accuracy of the neural\nnets trained by HAT is clearly increased along the learning curve, and the HAT-group neural nets\nreach a higher asymptotic value than the control group. We do note that the two learning curves seem\nto inﬂect around the same point – HAT does not seem to cause a faster convergence, just a better one.\nWe attribute this to the meta-learner’s convergence; it may take the meta-learner up to 0.5 epochs to\nstart to have positive effects.\nOne potential concern with adding unsupervised meta-learner updates is that after the convergence\nof the base learner L, the meta-learner’s continued output of non-zero updates might “bounce” the\nbase learner out of an optimum. Remarkably, we see in the above plot that the performance of the\nHAT-trained neural nets is quite stable for the entire 18 epochs of post-convergence duration.\n4\nTo our surprise, we ﬁnd that HAT is more effective when there are more labels, even though the\nself-supervised component of the algorithm is designed to take advantage of scarce labels. We\nattribute this to slow convergence of the meta-learner M – when labels are scarce, the meta-learner\nmay actually converge slower than the learner and thus provide bad update suggestions.\n4\nThe Behavior of the Meta-Learned Update Rule\nWe would like insight into why HAT improves the training of neural nets over vanilla gradient descent.\nThus, we will analyze the functional form of the learned update rule M after it has fully converged.\nRecall the setting from experiments 1 and 2: we generate 100 pairs of learners and meta-learners:\n(Li, Mi) for i ∈{1, ..., 100}. We then investigate the pointwise mean Mof these meta-learners.\nWe ﬁrst visualize the dependence of the function M on its inputs (vi, vj, wij).\nFigure 2: These plots show very little dependence of the converged rule on vi and wij.\nWe ﬁnd that a remarkably linear dependence on vj explains almost all of the variance in the outputs of\nthe meta-learned update rule. This indicates that the rule is a “rich-get-richer” scheme: neurons that\nalready ﬁred with high magnitude will experience larger incoming weights and thus be encouraged to\nﬁre with high activation in the future.\nThis linear dependence is surprising since all of the hypothesized rules in neuroscience have a\ndependence on vi·vj. As a sanity check, we attempted to directly apply this update rule (∆wij ≈2·vj)\nwithout meta-learning to see if we can replicate HAT’s performance improvement. However, the\nresults were decisively negative – HAT improves performance, but the a priori application of HAT’s\nupdate rule decreases it. We present three hypotheses:\n• Perhaps M learns a good update rule while L is training, then learns a degenerate rule once\nL has converged. The sole purpose of this degenerate rule would be to not un-learn the\nimportant weights that have already converged (thus explaining the rich-gets-richer behavior\nof the rule f(·) = 2vj). Thus, analyzing the black-box function at epoch 20 is merely the\nwrong time – perhaps observing the meta-learned rule at epoch 1 would be more insightful\nand useful.\n5\n• Perhaps M learns a good update rule in each run, and these update rules are all complex\nfunctions with no good low-order polynomial approximations; however, their pointwise\nmean (which is itself not a good local update rule) happens to be linear. Thus, M is the\nwrong object to analyze and presents behaviors that are not indicative of the results of\nexperiments 1 and 2.\n• Perhaps the learning of M is extremely transient. For any given point in time, there is a\ndifferent optimal learning rule, and our exercise in ﬁnding a ﬁxed local, unsupervised update\nrule that is universal across training is futile.\n5\nConclusion\nThe HAT algorithm demonstrates that local, unsupervised signals can provide performance-improving\nweight updates. Neural nets under HAT converge to better asymptotic losses as long as there is\nsufﬁcient time (> 0.5 epochs) and a sufﬁcient number of labels (> 20% of the data is labeled). The\nlatter ﬁnding is surprising since the addition of an unsupervised learning algorithm depends on the\npresence of labels in order to deliver marginal beneﬁts over gradient descent.\nThe underlying form of the learned rule that makes HAT successful is still a mystery; we ﬁnd that\nwhile the meta-learner may learn a useful update rule during training, the meta-learner does not\nconverge to this useful rule in the long run and instead devolves into a linear function Converged-\nRule. This converged function preserves fully-converged weights by reinforcing incoming weights\nfor neurons with high activations.\n5.1\nFuture Work\nThe discovery that HAT does not stably converge to a function makes analysis quite difﬁcult. However,\nthere is potential for future work to do more subtle analyses.\nImagine a time t during training in which the meta-learner M has converged to a useful function, but\nthe learner L has not yet ﬁnished training. A follow-up to this thesis might be to discover whether\nthere such a time t exists, what the structure of M at time t is, and how M changes the weights of L at\ntime t. One potential methodology might be to observe the function f not as a 3-dimensional function\nin (vi, wij, vj) but rather as a 4-dimensional function in (vi, wij, vj, t). Observing the function along\nthe t-axis and checking for phase changes would shed light on whether a single useful update rule is\nlearned during training or whether HAT’s learning is truly transient and continuous. If this follow-up\nwere to succeed, then we could have an a priori rule to apply without having to metalearn update\nrules.\nExtracting the local rules from multiple domains could either ﬁnd that HAT learns a universal rule\nor that functional distance between two rules describes the “difference” between their originating\ndomains.\n• Suppose we always metalearn the same rule, regardless of problem domain. Optimal-Hebb\nis then a universal learning rule.\n• Suppose Optimal-Hebb is not universal for all problems. For local rules RA, RB on\nproblems A, B, integrating\nR\nR3(RA −RB) · dF(vi, wij, vj) for input distribution F gives\nan explicit measure for how similar A and B are. This provides a systematic way to identify\npairs of learning problems that are good candidates for transfer learning.\n6\n[1] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom\nSchaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by\ngradient descent. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 29, pages 3981–3989. Curran Associates,\nInc., 2016.\n[2] Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P.\nLillicrap, Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by\ngradient descent. In Proceedings of the 34th International Conference on Machine Learning -\nVolume 70, ICML’17, pages 748–756. JMLR.org, 2017.\n[3] Genevieve Gorrell. Generalized hebbian algorithm for incremental singular value decomposition\nin natural language processing. In 11th conference of the European chapter of the association for\ncomputational linguistics, 2006.\n[4] D. O. Hebb. The organization of behavior; a neuropsychological theory, (by) D.O. Hebb. Science\nEditions. John Wiley and Sons, 1967.\n[5] Aapo Hyvärinen and Erkki Oja. Independent component analysis by general nonlinear hebbian-\nlike learning rules. signal processing, 64(3):301–313, 1998.\n[6] Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Learning to learn\nwithout labels. International Conference on Learning Representations, 2018.\n[7] Erkki Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of Mathematical\nBiology, 15(3):267–273, 1982.\n[8] Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between\nenergy-based models and backpropagation. Frontiers in Computational Neuroscience, 11:24,\n2017.\n[9] Xiaohui Xie and H. Sebastian Seung. Equivalence of backpropagation and contrastive hebbian\nlearning in a layered network. Neural Computation, 15(2):441–454, 2003.\n7\n6\nAppendices\n6.1\nAppendix A\nOne implementation detail is notably not covered in the HAT pseudocode; this implementation detail\npatches an inadequacy in modern deep learning frameworks.\nGiven two neural net layers ℓi and ℓi+1 and minibatches of size B, we have B instances of |ℓi| × ℓi+1\nneuron pairs, each of which has 3 salient properties (vi, wij, vj). Therefore, we would like to apply\nthe function M over the zeroth dimension of a tensor of size 3×B ×|ℓi|×|ℓi+1| in order to compute\nthe unsupervised weight updates.\nHowever, as of this writing date, it is not possible to apply an arbitrary function M to slices of a\ntensor in parallel in any modern deep learning framework (e.g. Tensorﬂow, PyTorch, Keras); the\nreason is that this plays poorly with optimization of the computational graph. We thus implement the\napplication of M’s updates to the weights by convoluting M over a state tensor.\nThis is best clariﬁed with an example. Suppose we have a neural net with consecutive layers ℓ1, ℓ2 of\nsize 784 and 183, respectively. Suppose further that we have batches of size 50. Finally, suppose that\nwe require a meta-learner that is a neural net of architecture 3 × 100 × 1.\nFor a single batch, we have the tensors of the following sizes:\n⃗vi : 50 × 784\n→\n50 × 1 × 784\n⃗\nwij : 183 × 784\n→\n1 × 183 × 784\n⃗vj : 50 × 183\n→\n50 × 183 × 1\nWe then copy the tensors along the boxed dimensions to stack them.\n⃗vi : 50 × 1 × 784\n→50 × 183 × 784\n⃗\nwij : 1 × 183 × 784\n→\n50 × 183 × 784\n⃗vj : 50 × 183 × 1\n→50 × 183 × 784\nInput to to meta-learner:\n3 × 50 × 183 × 784\nWe instantiate M as a sequence of 3 composed functions:\n1. a convolutional layer of kernel size 1 × 1 with 3 in-channels and 100 out-channels,\n2. a ReLU activation, and\n3. a convolutional layer of kernel size 1 × 1 with 100 in-channels and 1 out-channels.\nApplying this series of functions to a 1× image with 3 channels is equivalent to passing the 3 channels\ninto a neural net with architecture 3 × 100 × 1.\nPyTorch (the framework used for this research) does not support the vectorization of arbitrary\nfunctions along torch tensors. However, it does support (and heavily optimize for) convolutions.\nThus, we implement our neural net function M as a series of convolutions, and we convolve the\nfunction over the input tensor of size 3 × 50 × 183 × 784. The output of M is of size 50 × 183 × 784;\nwe average over the zeroth dimension to ﬁnally get a weight update of dimension 183 × 784, which\nis the same size as the original weight tensor.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-03-17",
  "updated": "2021-03-17"
}