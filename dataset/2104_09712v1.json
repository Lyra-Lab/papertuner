{
  "id": "http://arxiv.org/abs/2104.09712v1",
  "title": "Problems and Countermeasures in Natural Language Processing Evaluation",
  "authors": [
    "Qingxiu Dong",
    "Zhifang Sui",
    "Weidong Zhan",
    "Baobao Chang"
  ],
  "abstract": "Evaluation in natural language processing guides and promotes research on\nmodels and methods. In recent years, new evalua-tion data sets and evaluation\ntasks have been continuously proposed. At the same time, a series of problems\nexposed by ex-isting evaluation have also restricted the progress of natural\nlanguage processing technology. Starting from the concept, com-position,\ndevelopment and meaning of natural language evaluation, this article classifies\nand summarizes the tasks and char-acteristics of mainstream natural language\nevaluation, and then summarizes the problems and causes of natural language\npro-cessing evaluation. Finally, this article refers to the human language\nability evaluation standard, puts forward the concept of human-like machine\nlanguage ability evaluation, and proposes a series of basic principles and\nimplementation ideas for hu-man-like machine language ability evaluation from\nthe three aspects of reliability, difficulty and validity.",
  "text": "第 ** 卷  第 * 期 \n中文信息学报 \nVol. **，No. * \n201* 年 * 月 \nJOURNAL OF CHINESE INFORMATION PROCESSING \n***. ，201* \n \n收稿收稿日期：****-**-**；定稿日期：****-**-**  \n基金项目：国家科技创新2030“新一代人工智能”重大项目（2020AAA006701）；国家自然科学基金（U19A2065） \n文章编号：1003-0077（2017）00-0000-00 \n \n!\"#$%&'()*+,-./\n \n董青秀\n1,2,穗志方\n1,2,詹卫东\n1,3,常宝宝\n1,2 \n（1.北京大学 计算语言学教育部重点实验室，北京 100871； \n2.北京大学 信息科学技术学院，北京 100871； \n3.北京大学 中文系，北京 100871） \n  \n!\"#!\"#$%&'()*+,-./01234567/89:(;<=>?@AB()*CDE/)*+,FGH\nIJAKLMNAOP)*QR(STUVWXYZ[!\"#$%&34(\\]=^_`!\"#$)*(ab5cd5ef\n/ghJeAijkl[mn!\"#$)*(+,/opA\\qrstu[!\"#$%&)*'(VW/dv=wxA^_\nyz{j#$|})*~•AIJj{€•#$|})*(abA‚`ƒ„5…„5†„‡ˆ8‰IJ[STUj{€•#\n$|})*(Š^‹Œ/•Ž••A‚‘)*34(’@ef\\“[f”=•\n$%&#!\"#$)*–CDE—˜–)*™š!\n'()*+#›œ•žŸ,!!!!!!!!!-./01# !\nProblems and Countermeasures in Natural Language Processing Evaluation  \nDONG Qingxiu1,2 , SUI Zhifang1,2 , ZHAN Weidong1,3 , CHANG Baobao1,2 \n \n(1. MOE Key Laboratory of Computational Linguistics, Peking University, Beijing 100871, China; \n2. School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; \n3.Department of Chinese language and Literature, Peking University, Beijing 100871, China) \n \nAbstract: Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new \nevaluation data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by existing \nevaluation have also restricted the progress of natural language processing technology. Starting from the concept, composition, devel-\nopment and meaning of natural language evaluation, this article classifies and summarizes the tasks and characteristics of mainstream \nnatural language evaluation, and then summarizes the problems and causes of natural language processing evaluation. Finally, this \narticle refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability \nevaluation, and proposes a series of basic principles and implementation ideas for human-like machine language ability evaluation \nfrom the three aspects of reliability, difficulty and validity. \nKey words: natural language evaluation; data set bias; evaluation metric \n \n!\"\"#$\"\n近年来，随着自然语言处理领域模型和方法的\n迅捷发展和快速更迭，技术的突破不断引发人们对\n自然语言处理（Natural Language Processing, \n2 \n中文信息学报 \n第2*卷 \n \n \nNLP）的进一步探索和评估。因此，伴随着大规模评\n测数据集的新型评测任务如雨后春笋般涌现，如\nGLUE 等NLP 评测如火如荼地开展起来。 \nNLP 评测作为对机器理解、处理、应用自然语\n言能力的一种评估和量化手段，是NLP 领域的技\n术水平和研究进展的直观体现，也为相关方向的\n模型和方法的发展提供了标杆和方向，激励着研\n究者们更多地参与到相关方向的研究中，是NLP\n相关研究的工具和重要驱动力。 \n与此同时，Trichelair 等\n[1]研究者也对部分评\n测本身的科学性和客观性提出了质疑。研究人员经\n过设计诊断样例等方法，发现WSC\n[2]等多个数据集\n存在偏差，导致机器模型可能仅仅因为一些与测试\n意图无关的线索而在评测上取得很好的成绩，实际\n上机器并没有获得处理相应任务所需要的语言能\n力。 \n除了多种类型偏差的存在，评测指标的失真也\n影响了对机器的语言处理能力的准确评估和比较。\nBLEU 等\n[3][4]基于N-gram 重叠的自动指标在评测中\n被反复证明与人工评测差距较大，BEER 等\n[5][6]依赖\n人工标注的指标则又难以应对不同领域的大量评\n测需求。 \n可见，尽管NLP 评测的数量越来越多，评测的\n质量和效力却参差不齐。自然语言处理领域亟需对\n评测本身的质量提出目标、建立一些基本原则，规\n范NLP 评测的任务设计，从而保证评测对整个NLP\n领域的发展起到正确的、科学的推动作用。 \n为此，本文对机器语言能力评测中的问题和对\n策进行了归纳和探讨。首先，介绍了NLP 评测的概\n念、一般构成、发展历程和意义影响。进而，从现\n有NLP 评测出发，分类详细介绍了主流的评测。基\n于对现有评测的调研，本文详细阐述和归纳了NLP\n评测中的问题，探究了导致该类问题的成因。综合\n上述讨论，本文参照人类语言能力评测规范，提出\n类人机器语言能力评测的概念，从信度、难度、效\n度三个不同视角，尝试提出了一系列类人机器语言\n能力评测的基本原则和实施设想，希望为未来的机\n器语言能力评测提供相对明确的方向和规范，推动\n评测向更科学、更有效、更系统的方向发展进步。 \n%\"\"&'( )*+,\"\"\n!\"!#$%& '()*+#\n \nNLP 评测是指以定义规范的任务为载体，根据\n机器在任务上的表现评估机器理解、处理、应用自\n然语言能力的活动。评测需要给出体现机器单方面\n或者综合能力的量化成绩，为后续自然语言处理研\n究提供基准和比较标准。 \n \n!\",#$%& '()-.#\n \n一般来说，一个完整的NLP评测由核心部件和\n外围部件共同构成。如图1所示，核心部件指评测任\n务、评测数据集和评测方法，是评测中相对固定和\n关键的部分，由评测组织者提供。其中，评测任务\n是NLP评测的内核，决定了组织评测的目标，即旨在\n考察机器的何种能力，同时给出了任务的形式化定\n义，即输入和输出的具体形式、有无辅助信息等。\n评测数据集与评测任务相辅相成，评测组织者根据\n评测任务的形式化定义构建或改造数据集，以服务\n于机器模型的训练和测试，评测数据集也是评测任\n务的实现载体。为了评测的公平性，需要事先划分\n训练集、开发集和测试集，近期Matt Gardner等\n[7]\n又提出在数据集中增设对比集的想法。在训练集和\n开发集上训练模型之后，将模型在测试集上得到的\n结果与测试集预设的参考答案比较评分，一般使用\n准确率、F1、BLEU等自动指标，近年来Tianyi Zhang\n等和Thibault Sellam等也分别提出BERTScore\n[8]、\nBLEURT\n[9]等基于预训练的指标，主要用于文本生成\n质量的评估。 \n \n \n图1 NLP 评测的一般构成 \n \n除了上述核心部件，不同评测包括了不同的外\n围部件。外围部件指容易发生变更、选择性提供的\n部分，包括由参评单位提供的模型和由组织单位提\n供的人类基准、排行榜、工具包和说明文档等。以\n小样本对话语言理解评测FEWJOINT\n[10]为例，评测组\n织单位提供了配套小样本工具平台METADIALOG，方\n便研究者快速开展实验，间接鼓励更多研究者参与\n评测，从而推动该任务上的研究进展。此外，完整\n的评测往往会给出人类在该任务上的表现作为参\n考基准，同时将人类基准和当前较好的模型得分展\n*期 \n董青秀等: 自然语言评测中的问题和对策 \n3 \n \n \n示在排行榜上，以便相关研究者及时了解该任务上\n的最新进展，推动评测的公平性和公开性。 \n \n!\"/#$%& '()01#\n \n1950 年提出的图灵测试\n[11]可视作最早的NLP\n评测方案，自此NLP 评测进入萌芽期。图灵测试的\n具体方案是：如果一台机器能够与人类展开对话\n（通过电传设备）而不能被辨别出其机器身份，那\n么称这台机器具有智能。然而，图灵测试缺乏量化\n评估指标，也无法考察机器某项具体能力，故只能\n作为NLP 评测的初步构想。 \n1966 年，ALPAC 报告\n[12]的发布极大地削减了美\n国用于机器翻译研究的资金，这也引发了其他国家\n削减NLP 研究经费的连锁效应。在后来很长一段时\n间内，整个NLP 领域的研究陷入低潮期。在此背景\n下，NLP 评测也随之进入冷淡期\n[13]。 \n1987 年该状况开始改变，NLP 评测进入发展期。\n这个时期出现了一系列语音处理评测\n[14]，和文本理\n解评测\n[15]。1992 年，美国国防部和美国国家标准\n技术研究院（NIST）共同发起TREC\n[16]，第一个信息\n检索方向的评测会议应运而生，满足了信息和文档\n检索研究界的需求。此后，自然语言处理相关会议\n的数量和研究者的数量不断增长，评测对该领域发\n展的重要性也越来越被人们认识到，著名的RTE 系\n列评测和SENSEVAL 系列评测都是在这个时期被提\n出的。 \n2016 年开始，随着深度学习的发展和应用，NLP\n评测进入高涨期。2017 年，Morgenstern 等\n[2]提出\nWinograd Schema Challenge (WSC)数据集，以评\n测机器解决指代消解和常识推理问题的能力。同年，\n包含10 万个问答样例的SQuAD 数据集\n[17]公布，推\n动了此后具有代表性的模型的大量涌现，极大地促\n进了机器阅读理解领域的发展。除此之外，高涨期\n还出现了GLUE 等\n[18]综合多任务评测基准，目的是\n覆盖足够大的自然语言处理领域，旨在推动通用的、\n鲁棒的自然语言理解系统的研究。高涨期的NLP 评\n测呈现出3 个特点：1.评测基准数量多；2.评测数\n据集规模大；3.评测迭代快。 \n2018 年以来，虽然新的NLP 评测如雨后春笋般\n涌现，NLP 评测依然处于高涨期，但越来越多的研\n究者开始探究评测中可能存在的偏差并反思评测\n本身的正确性。2018 年Trichelair 等人\n[1]和2019\n年McCoy 等人\n[19]提出模型可能仅仅凭借学习高频样\n例的启发式规则或者问题与答案之间的简单词汇\n关联而在评测中取得很好的成绩，但可能并没有学\n到实质的信息，导致模型在下游任务上表现不佳。\n2020 年，Matt Gardner 等人\n[7]提出数据集构建者应\n当在训练集和测试集的基础上增设人工构建的对\n比集，减小系统偏差对评测结果的影响。随着人们\n对评测过程中的问题的进一步探究和反思，我们期\n待未来NLP 评测能步入稳定发展期，借助于评测基\n本原则和规范的指导，评测将更加规范化、体系化，\n不断向类人类语言能力评测逼近，以充分考察模型\n各方面能力，为NLP 各个子领域的发展起到引领作\n用。 \n \n!\"2#$%& '()34#\n \nNLP 评测对于自然语言处理的各个方面都有着\n巨大的指导意义，具体表现在如下3 点： \n1.评测成绩作为机器理解、处理、应用自然语\n言能力的量化结果，是对模型理解和处理文本能力\n的直观说明和展示，体现了每个阶段自然语言处理\n领域的水平和进展； \n2.作为NLP 各子领域的标杆，评测为模型的发\n展提供了方向，帮助各子领域树立短期和长期优化\n目标； \n3.评测使得模型的能力直观可比，激励着研究\n者们更多参与相关方向的研究，极大地促进了自然\n语言处理领域的发展。 \n正是由于NLP 评测的重大指导意义，如果评测\n本身存在问题或偏差，将严重影响相关领域工作价\n值评定的公平性和客观性，还可能掣肘模型和方法\n的迭代进步，导致NLP 研究的方向偏离正确发展的\n轨道。因此，NLP 评测本身的科学性和合理性的意\n义也至关重要。 \n-\"\"./ &'( )*,0\"\"\nNLP 能力评测有多种分类方式，例如根据评测\n过程中的是否关注模型内部表现而分为白盒评测\n和黑盒评测，根据评测过程是否有人工参与分为人\n工评测和自动评测等。由于自然语言处理涉及的种\n种能力紧耦合，因此很难在一个完整的系统中评测\n代表所观察功能的一组独立变量，这增加了评测本\n身的复杂性和公平可靠性。为了尽可能降低复杂性、\n保证公平可靠，现在主流的评测方法大多是自动评\n测和黑盒评测。 \n下文按照考察的能力分类介绍这些主流NLP 评\n测的基本情况。 \n \n,\"!#5678'(##\n \n4 \n中文信息学报 \n第2*卷 \n \n \n根据评测中用到数据集数量的不同，单项能力\n评测又可分为单数据集单项能力评测和多数据集\n单项能力评测。然而，目前人们尚未将各类评测任\n务与其考察的机器处理自然语言的能力类型规范\n化对齐，很多情况下，不严格区分单个数据集和其\n所对应的评测任务。因此，即使不包含1.2 中的所\n有构件，当单个数据集定义了一个有价值的任务时，\n便可将其视为单项能力评测。 \n这类评测的门槛较低，故而数量和类型很多，\n此处仅介绍几类经典的单项能力评测及其相关的\n数据集。 \n2.1.1 命名实体识别——CoNLL2003 \nCoNLL2003\n[20]作为2003 年CoNLL 会议的共享任\n务被提出，是经典的命名实体识别评测。该任务给\n定一个句子，要求机器正确识别句子中人名、地名\n等包含名称的短语。CoNLL2003 数据集包括1393 篇\n英语新闻文章和909 篇德语新闻文章，英语语料取\n自路透社收集的共享任务数据集，数据集中标注了\n4 种实体类型。 \n2.1.2 常识推理——WSC \nWSC 是多伦多大学计算机科学家Levesque 于\n2011 年提出的常识推理测试\n[21]，该测试试图通过向\n机器询问特别设计的选择题来考察机器常识推理\n的能力。任务是给定一对句子，要求机器正确判断\n问题中某一代词的先行词，是一个二分类问题。由\n于WSC 数据集规模很小，仅包含273 个问题，后续\n规模更大的类似数据集，如DPR、COPA 等不断被提\n出。 \n2.1.3 文本蕴含推理——MNLI \nMNLI 由纽约大学于2017 年发布\n[22]，是一个文\n本蕴含的任务。在给定前提下，需要判断假设是否\n成立，其中因为MNLI 主要特点是集合了许多不同\n领域风格的文本，因此又分为matched 和\nmismatched 两个版本的MNLI 数据集，前者指训练\n集和测试集的数据来源一致，而后者指来源不一致。\n该任务属于句子对的文本三分类（蕴含，矛盾，中\n立）问题。 \n2.1.4 关系抽取——TACRED \nTACRED 是一个拥有106264 条实例的大规模关\n系抽取数据集\n[23]，这些数据来自于每年的TAC KBP\n比赛使用的语料库中的新闻专线和网络文本。在每\n一年的TAC KBP 评测中（2009-2015），100 个实体\n作为查询给出，参与的系统需要为其找到对应的关\n系和对象实体。工作者使用Mechanical Turk 来注\n释源语料库中包含这些查询实体之一的每个句子。\n对于每个句子，要求群组工作者标注主体和对象实\n体跨度以及关系类型。 \nTACRED 中涵盖了TAC KBP 比赛中使用的41 种\n关系类型和1 个无关类型。 \n2.1.5 情感分类——SST \nSST 是斯坦福大学于2013 年发布的一个情感\n分析数据集\n[24]，主要针对电影评论来做情感分类。\n因此SST 属于单个句子的文本分类任务，输入一个\n句子，要求输出该句子的情感倾向，即输出“非常\n积极”、“积极”、“中立”、“消极”或“非常\n消极”中任一类型。SST 包含11,855 个句子及相应\n情感标签，更有挑战性的是，SST 同时给出了这些\n句子的语法分析树中215,154 个短语的细粒度情感\n标签。 \n2.1.6 文本摘要——DUC \n自2001 年起，NIST 组织发布了DUC 系列评测\n数据集，用于评估机器文本摘要能力。文本摘要任\n务给定一段长文本，要求机器输出保留其主要信息\n的短句。DUC 系列中最被广泛使用的是DUC2004 数\n据集，其包含500 组文档-摘要对，文档平均35.6\n个词，摘要平均10.4 个词。 \n2.1.7 阅读理解——SQuAD \nSQuAD 是斯坦福大学于2016 年推出的机器阅\n读理解数据集\n[17]，是最流行的机器阅读理解评测。\n任务是给定上下文和问题，要求机器在上下文中抽\n取可作为答案的片段。SQuAD 规模较大，包含十万\n个问题答案对。 \n在原来的SQuAD 的十万个问题答案对的基础上，\nSQuAD 2.0\n[25]中新增了超过五万个新增的、由众包\n工作者对抗性地设计的无法回答的问题。执行\nSQuAD 2.0 阅读理解任务的模型不仅要能够在问题\n可回答时给出答案，还要判断哪些问题是阅读文本\n中没有材料支持的，并拒绝回答这些问题。 \n2.1.8 对话生成——UDC \nUbuntu 对话库UDC 由蒙特利尔大学Lowe 等人\n于2015 年建立\n[26]，是目前可用的最大的公共对话\n数据集。该任务给定一段对话和一句可能的回应，\n要求判断该回应是不是给定对话的下一句。UDC \n1.0 版本包含约100 万条多轮对话数据，以及超过\n700 万条回复和超过19 亿个词。UDC 2.0 删除了\nUDC 1.0 中的分词和指代消解等处理，而用特殊的\n符号对这些信息进行表示。 \n除了上述介绍的单项能力评测，在自然语言处\n理的各个子领域都有相应的评测任务，不一而足。\n带有多重标注或者经过改造的数据集还能被应用\n于多种不同任务的评测，例如CNN/Daily Mail 数\n据集起初作为大型的有监督问答数据集被提出\n[27]，\n后来也被广泛用作生成式文本摘要的基准数据集。\n得益于自然语言处理社区庞大的语料，这样的单项\n能力评测类型丰富，设计灵活，能充分评测机器完\n*期 \n董青秀等: 自然语言评测中的问题和对策 \n5 \n \n \n成具体任务的能力，例如常识推理能力、语义理解\n能力。 \n#\n,\",#9:78'(#\n \n区别于单项能力评测，综合能力评测一般都聚\n合了多个数据集，其中每个数据集自身都可以被视\n为一个单数据集单项能力评测。 \n2.2.1 DecaNLP \nDecaNLP 由Salesforce 于2018 年提出\n[28]，旨\n在用问答框架统一各种自然语言处理任务。该评测\n包含了机器翻译、文本摘要等10 项任务对应的公\n开数据集，数据样例被统一转换为问题、上下文和\n回答的三元组。 \n2.2.2 GLUE \nGLUE\n[18]是2019 年由来自纽约大学、华盛顿大\n学、DeepMind 等机构的研究者创建的英语自然语\n言理解基准和分析平台，是九种语言理解任务的集\n合。其设计目的是覆盖足够大的 NLP 领域。只有\n开发出足够通用的工具，才能在这一基准上表现良\n好，GLUE 的最终目标是推动通用的、鲁棒的自然语\n言理解系统的研究。 \nGLUE 中任务分成三大类：第一大类是分类任务，\n包括语法错误检测和情感分类，这类任务的输入都\n是一个序列，输出都是一个类别；第二大类是输入\n是两个句子，输出是二者的语义是否相似对应；第\n三大类都是自然语言推理相关的任务，输入前提和\n假设，希望机器能判断二者的关系是矛盾，蕴含还\n是无关。 \n2.2.3 SuperGLUE \n由于BERT 等模型的出现，GLUE 基准在新模型\n的评估方面日渐乏力，很多模型在GLUE 的大部分\n任务上都达到了90 分以上，GLUE 基准在新模型的\n评估方面渐渐达到上限。研究者决定将其升级为\nSuperGLUE\n[29]。SuperGLUE 保留了两项GLUE 任务，\n还引入了五项难度更大的新任务，增加了对机器共\n指消解和问答能力的考察，提高了这一测试基准的\n难度。 \n截至2020 年底，SuperGLUE 排行榜上效果最好\n的模型T5 已经非常接近人类水平。 \n2.2.4 CLUE \nCLUE\n[30]即 ChineseGLUE，顾名思义，是中文版\n本的多任务自然语言理解基准和分析平台。其定位\n是为更好的服务中文语言理解、任务和产业界，作\n为通用语言模型测评的补充，通过完善中文语言理\n解基础设施的方式来促进中文语言模型的发展。\nCLUE 包含了十个中文语言评测任务，其中很多任务\n所用的数据集是GLUE 中的数据集中文版。类似地，\n2019 年Le 等人提出了法语上的综合评测基准\nFLUE\n[31]，2020 年Wilie 等人提出了为印尼语提出了\nIndoNLU 评测基准\n[32]。 \n2.2.5 LUGE \nLUGE 是由百度和中国中文信息学会等于2020\n年8 月联合推出的中文开源数据集合。针对每个自\n然语言处理问题，LUGE 中收集和整理了多个开源数\n据集，进行统一的处理并提供统一的测评方式。其\n包括了情感分析、阅读理解、开放域对话、文本相\n似度、语义解析、机器同传、信息抽取7 类任务，\n汇集来自11 所高校和企业的22 个开源数据集。 \n2.2.6 XTREME  \n由于过去的大多数综合能力评测仅限于单种\n语言，无法在统一标准下评估机器能力，也无法对\n低资源语言作出相应评测。为了解决这一问题，\n2020 年3 月，来自CMU、谷歌研究院和DeepMind 的\n科学家们提出了覆盖四十种语言的大规模多语言\n多任务基准XTREME\n[33]。该评测基准覆盖了40 种类\n型不同的语言（跨12 个语系），并包含了9 项需\n要对不同句法或语义层面进行推理的任务，实现了\n语言多样性、现有任务覆盖以及训练数据可用性的\n最大化。 \n2.2.7 XGLUE \n无独有偶，2020 年5 月微软发布了XGLUE 基准\n数据集\n[34]，用于评估跨语言预训练模型在跨语言自\n然语言理解和生成方面的性能。XGLUE 由11 种任务\n组成，涵盖19 种语言。XGLUE 中每个任务的训练数\n据都是英语数据，因此要求模型具有强大的零样本\n跨语言迁移能力，考察模型从特定任务的英语数据\n中学习并将其学到的知识迁移到其他语言的能力。 \n与近乎同时期提出的XTREME 相比，一方面，\nXGLUE 同时包含跨自然语言理解和生成任务，且除\n了包含5 个经典单项能力评测任务，还包含从实际\n应用场景中选择的6 个新任务；另一方面，XTREME\n覆盖的语言、语系比XGLUE 更丰富。但总的来说，\n二者的动机都是评估跨语言预训练模型在跨语言\n任务上的迁移能力，是非常同质的。 \n2.2.8 GLGE  \nGLGE 是2020 年11 月由四川大学和微软提出\n的NLP 评估基准\n[35]，旨在填补GLUE、CLUE 等以分\n类任务为主的评测在文本生成领域的不足。GLGR 包\n含8 个英语生成任务，涵盖了文本摘要、问题生成、\n问答和对话四项能力。此外，作者按难度将GLGE 设\n计为GLGE-简单，GLGE-中等和GLGE-困难三类，体\n现了对难度分级的考量。但GLGE-中等和GLGE-困\n难实际是直接通过对训练数据作筛减得到的，难度\n分级的设计仍然比较局限。 \n6 \n中文信息学报 \n第2*卷 \n \n \n1\"\"&'( )*2345\"\n/\"!#'(;<=>?#\n#\n由于缺乏评测的基本原则或者要求，提出单项\n能力评测往往被简化成提出一个新数据集，综合能\n力评测则大多是单项能力评测数据的简单聚合。故\n而，现阶段NLP 评测的准入门槛低，这导致评测数\n量过多而质量参差不齐。如图2 所示，仅自然语言\n推理方向\n[36]，2005-2020 年就有数十个评测被提出。\n从图2 还可以发现，2016 年起，评测的数量和规模\n都开始大规模增长，即进入了1.3 所述的评测发展\n高涨期。 \n \n图2 2005-2020 年提出的自然语言推理（NLI）评测 \n \n在评测数量过多的情况下，针对同一项任务的\n多个评测集给出的结果可能矛盾，但缺乏统一的规\n范来评估和比较评测本身的合理性。此时，研究者\n往往各自采用对自己的模型最有利的数据集并声\n称达到了最好结果，导致后续研究者难以客观比较\n和超越。 \n \n/\",#'(@8AB#\n#\n面对参数量越来越庞大的模型，大部分现有评\n测难以通过传统指标和单一排行榜明显区分机器\n模型和人类在测试集上的表现。具体言之，在评测\n任务提出不久后，机器模型得分往往就已接近甚至\n超越人类得分。如图3 所示，我们选取了12 个提\n供人类基准和排行榜的评测(具体信息参见表1)，\n统计了其提出时最好模型的表现、当前最好模型的\n表现与人类水平基准。对于这12 个评测，当前最\n好模型表现与人类基准的平均差距已不足3 个百分\n点。在SQuAD2.0 等评测上，当前最好模型表现甚\n至超越了人类基准。但上述现象并不意味着机器语\n言能力已与人类语言能力不相上下，其真正反映的\n问题是当前评测在评估机器语言能力方面的效力\n不足。 \n \n图3 12 类评测上的人类基准与模型表现 \n \n乔姆斯基提出\n[37]，语言能力是人内在的语言系\n统的属性，指人具有句法结构系统、词汇系统等语\n言知识的能力。语言表现是人用语言做事的外在行\n为特征，是语言能力的外化呈现。语言能力对应着\n“懂一种语言”（可能属于无意识地知道）；语言\n表现对应着“能在行为上表现出语言行为”。对比\n当前最好模型的表现和人类基准可知，现阶段大部\n分评测上，最好的机器模型表现与人类表现的差距\n都小于10 个百分点，在部分评测上甚至超过了人\n类水平。这说明现阶段大部分NLP 评测只是在测试\n语言表现而非语言能力，无法客观反映机器语言能\n力与人类语言能力的真实差距。 \n \n/\"/#'(CDEFG#\n \n3.3.1 类型分布偏差 \n同一评测数据集内可能包含多种类型的数据，\n如果这些类型分布情况存在较大差距，我们称该数\n据集存在的类型分布偏差较大，这种偏差通常由众\n包过程引入。一个典型的例子就是机器阅读理解任\n务中的问题类别分布偏差，在构建问题答案对时，\n若不加约束地请标注人员拟写问题，可能数据集中\n大部分问题都是问“是谁……”，这导致评测实际\n上只是在评测机器阅读和理解人名或指代消解的\n能力，而无法真实反映机器阅读和理解文本的综合\n能力。 \n已经有一系列工作通过人为限制控制类型分\n布偏差的产生，对于英语问答任务，保持问题类型\n平衡的一种方法是控制每个问题的第一个单词的\n分布，参考CoQA\n[38]和CommonsenseQA\n[39]的创建过程\n中采用的方法。为了完全规避问题类型偏差，可以\n在数据标注前设计规范的限定规则，例如等比例随\n机给众包工作者分配问题类型。 \n3.3.2 答案分布偏差 \n答案分布偏差是最容易检测和避免的一种偏\n差。 例如在多项选择题中，如果正确答案以更高的\n概率出现在某个位置的选项，即答案分布不均匀，\n模型在学习过程中可能就会引入选项次序信息，尽\n管模型可能因此在评测中取得了更好的成绩，但显\n然这并不是我们期待模型具有的语言能力。对于二\n*期 \n董青秀等: 自然语言评测中的问题和对策 \n7 \n \n \n分类任务，合理的评测正确答案应该以50%的概率\n分布在两个选项中，所以按照服从多数的原则建立\n的多数类基线应该只有50 分，但DNC 评测中的\nMegaVeridicality 子任务\n[40]由于类别标签分布不\n均而具有67％的多数类基线，这样的评测数据集存\n在较大的答案分布偏差。 \n3.3.3 无关线索偏差 \n机器并非通过掌握所要评测的语言能力，仅仅\n利用一些无关的线索推出正确答案的情况，可称为\n无关线索偏差。 \n2019 年，T. Niven 和H. Y. Kao 发现BERT 在\n多个评测上取得很好成绩可能只是因为模型学习\n到了一些虚假相关的统计线索\n[41]，例如“不”、\n“是”\n这种词。他们还通过构造等价对抗数据发现，在对\n抗数据下BERT 模型的效果基本等价于随机分类器\n的效果。这充分说明，无关线索偏误对模型能力评\n估的误导性，这种情况下，评测本身的效力和信度\n大大降低了。 \n无关线索偏差是最容易在众包标注的过程中\n被引入的偏差类型。McCoy 等在2019 年提出了统计\n自然语言推理模型可能利用的三种表层句法偏差\n（词重叠、子序列重叠、成分重叠），并由此构建\n了HANS 诊断数据集\n[19]。McCoy 等用HANS 数据集在\n众包构建的MNLI 数据集上测试，发现了模型在\nMNLI 上成绩“虚高”的问题。图4 是可分解式注意\n力模型（DA）、增强顺序推理模型（ESMI）、叠增\n解析器神经网络（SPINN）和预训练模型（BERT）在\nMNLI 测试集上的准确率，图5 则是各个模型在HANS\n数据集上的准确率。可以发现，各个模型在MNLI 测\n试集上呈现出的高分并不意味着模型本身文本蕴\n含推理能力较强，而是模型利用了MNLI 数据集上\n的表层句法偏差“蒙”对了答案，如图5 所示，模\n型只要在遇到前提和假设存在词重叠、子序列重叠\n或成分重叠时直接判断结果为“蕴含”，就能在MNLI\n的测试集上取得高分。 \n \n图4 各模型在MNLI 上的准确率 \n \n图5 各模型在HANS 数据集上的准确率 \n \n事实上，专家创作的数据也可能存在无关线索\n偏差\n[1]。在故事完形评测任务\n[42]中，专家给故事提\n供了一个合理和一个不合理的结尾，要求机器选出\n哪个结尾是合理的。Schwartz 等人的研究发现\n[43]，\n仅查看两个的结局即可完成这项测试的准确性高\n达72.4％，只需要利用人类写作风格的关联线索偏\n差来做到这一点，而不需要进行基于事实的推理。\n例如，他们发现否定性描述通常用于错误的结尾\n（例如“讨厌”），而正确的结尾更可能使用热情\n的表述方式（例如“！”）。 \n3.3.4 关联线索偏差 \n最难以辨别和规避的偏差是答案和问题的特\n征之间的偶然相关性引起的关联线索偏差。一个经\n典的例子是性别偏差，当对有偏的数据进行训练时，\n在测试集上表现良好的模型可能十分脆弱。\nRudinger 等\n[40]在共指消解中强调了这个问题，他们\n的研究表明，在性别代名词歧义消除方面，接受过\n性别偏见数据训练的系统表现更差。 \n \n图6 Winogender 数据集\n[44]中的关联线索偏差 \n \n例如，在图6 的数据集样例中考虑：“急救人\n员对乘客进行了心肺复苏术，尽管她知道为时已\n晚。”在确定“她”是谁时，受性别偏差训练数据\n训练的系统更可能错误地选择“乘客”而不是“急\n救人员”，因为急救医务工作中，男性性别代词出\n现在训练数据中的可能性比女性性别代词的高。在\n来自电影脚本的Event2Mind 数据\n[45]中也发现了相\n8 \n中文信息学报 \n第2*卷 \n \n \n似的性别偏见。此类关联线索偏差往往是源于原始\n语料分布不均衡。 \nSakaguchi 等人\n[11]的研究发现，众包标注的过\n程也可能引入相关线索偏差。在编写自然语言数据\n（例如生成问题或假设）时，众包标注者词汇使用\n上的习惯或者潜意识也会导致这些关联线索偏差。\n图7 是与WSC 同类的DPR 数据集\n[46]中一组有偏示\n例，事实上，机器只需要根据所问代词“它们”周\n围的词“食肉”还是“肉多”就可以判断答案是“狮\n子”还是“斑马”，而不需要对理解整个句子，此\n类词汇级关联偏差是受众包标注者潜意识影响产\n生的。 \n \n图7 DPR 数据集中的关联线索偏差 \n \n/\"2#'(HIJK#\n \n评测指标的选择高度依赖于任务的类型。分类\n任务的评测指标相对较为统一和明确，如果正确答\n案或类别标签均匀分布，则分类任务通常使用完全\n匹配的准确率（Acc）作为评测指标，否则则使用F\n值作为评测指标，某些情况下也可能使用准确率或\n召回率指标，但F 值综合考虑了两者，所以近来更\n广泛地被使用。 \n文本生成领域缺少这样公认的评测指标，往往\n容易出现评测指标失真，无法真正反映机器能力的\n情况。目前主要有两种主要方式对自然语言生成系\n统进行评测：人工测评和自动化度量。人工测评需\n要人类标注员在每个模型版本上进行大规模的质\n量检查，这种方法虽然精度很高但劳动密集型的检\n查任务十分消耗人力；而像BLEU\n[3]和ROUGE\n[4]这样\n自动化测评方法可以对模型进行迅速的评测，但它\n们仅仅基于N-gram 重叠对生成文本和参考答案作\n相似性度量。这些度量标准只对词汇变化敏感，不\n能识别句子语义或语法的变化。因此，它们被反复\n证明与人工评估差距较大。诸如此类评测指标使用\n不当或者指标不统一的情况，也是当前NLP 评测中\n暴露且尚未完全解决的重要课题。 \n \n/\"L#'(MNOPQ#\n \n近几年NLP 领域的发展进步非常快，这导致部\n分评测数据集提出后不久，最好的机器模型得分就\n超过了人类基准，评测系统过快失去效力，这样的\n评测系统是缺少生命性的。多轮对话推理数据集\nMuTual\n[47]于2020 年4 月被提出时，最佳的RoBERTa\n模型在其上的得分仅为70 分左右，比人类在多轮对\n话推理任务上的得分低了20 多分，但不到半年，上\n海交通大学和华为提出的MDFN 模型\n[48]的得分已经\n和人类基准只有1-2 分的差距了。 \n表1 列出了3.2 中用到的12 个提供人类基准\n和排行榜的评测基本信息，定义提升率为当前最好\n模型表现与提出时最好模型表现之差与周期的比\n值（周期为提出时间到当前时间所隔月数）。参考\n提升率可以发现大部分评测上机器模型的提升率\n很高，例如SuperGLUE 或SQuAD，平均每个月能提\n升一个百分点，由此可以预测，如果不及时更新迭\n代，不到一年SuperGLUE 和SQuAD2.0 也会彻底走\n到各自生命周期的尽头。\n \n表1 12 类评测的基本信息 \n评测 \n提出时间\n1 \n提出机构 \n任务类型 \n规模 \n提出时的\n最好模型 \n提出时最好\n模型表现 \n当前最好\n模型表现 提升率 指标 \nGLUE \n2018 年4 月 \n纽约大学等 \n综合 \n9 个任务 BiLSTM+A\ntt+ELMo \n70.0 \n90.7 \n0.6 \nF1 \nSuperGLUE \n2019 年7 月 \n纽约大学等 \n综合 \n8 个任务 BERT+ft \n71.5 \n89.3 \n0.9 \nF1 \nCLUE \n2020 年11 月 \nCLUE 团队 \n综合 \n9 个任务 \nRoBERTa-\nwwm-ext-\nlarge \n74.9 \n80.1 \n5.2 \nEM、\nAcc \n \n1 提出时间以论文为准，当前时间仅SQuAD（官方不再更新排行榜）使用2019 年7 月，其余使用2020 年12 月 \n*期 \n董青秀等: 自然语言评测中的问题和对策 \n9 \n \n \nSQuAD1.1 \n2016 年10 月 \n斯坦福大学 \n阅读理解 \n10 万题 \nLogistic \nRegres-\nsion \n51.0 \n89.9 \n1.1 \nF1 \nSQuAD2.0 \n2018 年6 月 \n斯坦福大学 \n阅读理解 \n15 万题 \nDocQA+EL\nMo \n66.3 \n93.0 \n0.8 \nF1 \nWinoGrande \n2019 年11 月 艾伦研究所等 自然语言推\n理 \n4.4 万题 RoBERTa \n79.3 \n87.0 \n0.6 \nAUC \nCoQA \n2019 年3 月 \n斯坦福大学 \n阅读理解 12.7 万题 Augmt.Dr\nQA \n65.4 \n90.7 \n1.1 \nF1 \nCommonsenseQA 2019 年3 月 特拉维夫大学等 阅读理解 1.2 万题 BiDAF++ \n32.0 \n83.3 \n2.2 \nAcc \nRACE \n2017 年12 月 卡内基梅隆大学 阅读理解 \n10 万题 \nGA \n44.1 \n91.4 \n1.2 \nAcc \nDROP \n2019 年4 月 加州大学欧文分\n校等 \n阅读理解 9.7 万题 NAQANet \n47.0 \n90.1 \n2.1 \nF1 \nQuAC \n2018 年8 月 \n华盛顿大学等 \n阅读理解 \n10 万题 \nBiDAF++ \n60.6 \n74.6 \n0.5 \nF1 \nMutual \n2020 年4 月 \n浙江大学等 \n对话理解 0.9 万题 RoBERTa \n69.5 \n91.6 \n2.5 \nR@1 \n \n/\"R#ST;<UV?#\n \n虽然GLUE 和CLUE 等多任务评测声称通用\nNLP 评测，试图综合考察模型自然语言理解和处\n理的能力，但他们本质上并不是完整的、综合的、\n系统的。各个评测任务都是分立的，只能宏观反\n映在某个任务上有怎样的成绩，并求一个简单的\n平均作为最终成绩，这样是不科学的。所谓的综\n合性评测并不“综合”，只是简单的数据聚合，各\n个任务没有真正结合成一个系统，没有不重不漏\n地衡量一个模型的综合能力，无法说明每个任务\n具体在考察什么，也无法解释任务直接的联系是\n怎样的。 \n \n/\"W#'(XY5Z#\n#\n现在大部分评测都仅仅依托于一个包含了固\n定训练集、开发集和测试集的数据集，一方面，\n一成不变的数据集很容易被机器模型学会、突破，\n导致评测的效力非常短；另一方面，与训练集基\n于相同分布的测试集往往只能反映模型在该种分\n布下的能力，而无法评测模型在真实场景下的性\n能，无法完成模型对泛化性能的评测。 \n \n/\"[#\\]^?_G#\n#\n完全依赖端到端的方式进行评测存在诸多弊\n端，例如无法给出带有语言知识的细粒度评测，导\n致评测的可解释性很差，能给模型改进带来的帮助\n也十分受限。 \n6\"\"789:;<=>$?@)*\"\n针对现阶段NLP 评测中的一系列问题，我们\n参照人类语言能力评测方式，提出类人机器语言\n能力评测的概念。 \n在评估人类语言能力时，有语言能力综合评\n测（Language Ability Test，简称LAT），旨在\n加强对考生语言综合运用能力的考查，注重知识\n的联系和融合，增强考试内容的基础性和综合性。\nLAT 能相对公平高效地实现人类语言水平考核，\n全面体现考生综合的语言理解和运用能力，保证\n成绩的有效可信和考试的公平、公正。教育心理\n学中测试的信度,效度和区分度 \n借鉴LAT 的思路，NLP 中的评测也应当是一\n个有机的整体，充分、客观地反映机器理解和运\n用语言的能力，即未来面向强人工智能，研究者\n应当突破传统NLP 评测的局限，进行类人的机器\n语言能力评测。 \nA\"\":;<=>$?@)*3BCDE\"\n机器语言能力评测和人类语言能力评测一样，\n需要原则指导和规范。教育心理学中使用难度、\n信度和效度评估试卷，受此启发，我们根据NLP\n评测的现状与问题，从信度、难度、效度着眼，\n10 \n中文信息学报 \n第2*卷 \n \n \n参照人类语言能力评测规范，提出下列12 条类人\n机器语言能力评测的基本原则。\n \n表2 类人机器语言能力评测的基本原则\n类别 \n原则 \n内容 \n信度 \n无偏性 \n评测能真实反映出机器解决语言问题的能力，而非机器在特定数据集上的得分 \n鲁棒性 \n相同模型的多次评测结果应当保持一致；评测指标在不同领域或者不同时间跨度使用\n时漂移尽可能小 \n无悖性 \n评测内部各个评测任务给出的结果不相悖 \n科学性 \n机器语言能力评测给出的结果应和人工评价结果基本一致 \n难度 \n挑战性 \n评测需给出更具挑战的任务，使机器模型与人类基准的差距得以显现；难度要在当前机\n器能力可以触及的范畴 \n区分性 \n评测对当下的各个模型的能力有较强的区分度，排行榜上所有模型结果的离散系数足\n够大 \n量化性 \n评测需要更具深度的量化，以统一标准为模型内部各部件的作用作出定量评估 \n生命性 \n每个阶段的评测任务都应当符合当时人工智能发展的需要，但也不能过快失效 \n效度 \n深层次 \n评测不仅要给出端到端的测试，还要分析模型能否区分相关性和因果性，并对模型处理\n信息、认知推理等过程的正确性作深层次分析 \n可解释 \n评测要给予未来研究一定的指导，解释起作用的部件并指出模型的缺陷 \n易用性 \n评测应当具有完备的构件（评测数据、任务体系、评测指标、基准模型、人类基准、排\n行榜、评测报告等），同时具有便捷高效的特性，以便在各环境下高效使用 \n综合性 \n评测任务需要全面多样，同时保证不重不漏，有机结合；最好涵盖跨模态任务 \nL\"!\"`a#\n \n信度面向评测的可靠程度，包括无偏性、鲁\n棒性、无悖性、科学性四项基本原则，是一个机\n器语言能力评测合理可靠的基本保证。 \n5.1.1 无偏性 \n无偏性要求评测能真实反映出机器解决语言\n问题的能力，而非机器在特定数据集上的得分。3.3\n中介绍了数据集偏差的四种类型，这些偏差妨碍\n了评测如实反映模型完成某项任务的能力，高信\n度的机器语言能力评测应对尽量规避。 \n对于答案分布这类显见的偏差，可以设计一\n系列简单而通用的基线来检测和消除。例如，用\n多数类基线（始终选择多数正确答案所在的选项）\n的结果来检测是否存在答案分布偏差，结果越低\n说明答案分布越均衡；也可以在构建数据的时候\n做一些规范化设计，从源头尽量保证无偏性。 \n5.1.2 鲁棒性 \n鲁棒性包括两个方面，一方面，相同模型的多\n次评测结果应当保持一致。尤其当设计随机变化\n的测试集来做评测时，需要保证结果不因为测试\n集的随机性而发生变动（这要求测试集足够大、\n足够平衡）。另一方面，评测指标在不同领域或者\n不同时间跨度使用时漂移尽可能小。 \n5.1.3 无悖性 \n评测内部各个评测任务给出的结果不相悖。\n如果有两个任务都能测试模型在某一方面的能力，\n那么在该评测下，这两项任务测试的结果不应该\n相悖，否则说明该评测不科学。 \n一个反例是LUGE 评测中，ChnSentiCorp 和\nNLPCC14-SC 同为句子级情感分类任务数据集，但\n在排行榜上出现了在ChnSentiCorp 上A 模型的\n得分高于B 的，而在NLPCC14-SC 上A 模型的得\n*期 \n董青秀等: 自然语言评测中的问题和对策 \n11 \n \n \n分却低于B 的情况2。LUGE 系统的排行方式是直\n接对两个成绩取均值，但二者的结果已经违背了\n无悖性，这样的结果是不合理的。 \n5.1.4 科学性 \n机器语言能力评测给出的结果应和人工评价\n结果基本一致。检查评测是否科学准确可以采用\n和人工评价结果相比较的方式，由于机器与人类\n评价方式的差异，往往无法用相同指标直接作比\n较，可以考虑用横向对比分析的方法。固定几个\n经典基线，例如人类基准和BERT 基准，人工评估\n这二者在各个任务上给出的结果哪个更好，再看\n评测给出的各个任务上哪个结果更好，对比二者\n是否一致来判断评测是否满足科学性。可以使用\nSpearman 相关系数来衡量科学性。 \n \nL\",#ba#\n \n难度面向评测的能力，包括挑战性、区分性、\n量化性、生命性四项基本原则，是一个评测对不\n同模型区分度的体现。 \n5.2.1 挑战性 \n类人机器语言能力评测应当给出更多具有挑\n战的任务，这体现在现有基线模型得分不会很高，\n使机器模型与人类基准的差距得以显现；但同时，\n也要注意这种挑战难度要在当前机器能力可以触\n及的范畴，如果挑战性过大，评测可能会因为过\n于超前而暂时无法为研究者提供任何驱动或指导，\n研究者参与评测的积极性可能会过低。 \n5.2.2 区分性 \n评测应当对当下的各个模型的能力有较强的\n区分度，可以用当前排行榜上所有模型结果的离\n散系数表示。不同模型的离散系数越大，说明该\n评测的区分度越大。 \n5.2.3 量化性 \n为了衡量结果，现在主流的方法是使用一些\n评估矩阵作为度量指标，这已经是对机器语言能\n力一种有效量化。在此基础上，更具深度的量化\n是类人机器语言能力评测的重要原则，要求以统\n一标准为模型内部各部件的作用作出定量评估。 \n5.2.4 生命性 \n每个阶段的评测任务都应当符合当时人工智\n能发展的需要，但也不能过快失效。在生命性原\n则下，至少两年内的最佳模型成绩应低于90/100，\n才能保证评测的效力和效果。 \n \n2 Alvin0310 的团队模型和基线ERNIE 的得分 \n进一步，有价值的评测应当在大量模型取得\n很高成绩（例如90/100）的时候需要及时迭代更\n新，不断引领NLP 发展。 \n \nL\"/#@a#\n \n效度面向评测的效力，包括深层次、可解释、\n易用性、综合性四项基本原则，对类人机器语言\n能力评测至关重要。 \n5.3.1 深层次 \n面向未来的人工智能，我们不应当只用一些\n表层的、形式化的任务（简单情感分类、抽取式\n问答）评估模型，而应进一步考查模型深度理解\n语义的能力。评测内容除了客观信息分析，还要\n包括主观意图分析、认知过程分析、动态交互分\n析等。具体来说，从面向弱人工智能的评测到面\n向强人工智能的评测的一个重要转变是由面向结\n果到面向过程作评测，评测不仅需要给出端到端\n的测试，还要分析模型能否区分相关性和因果性，\n并对模型处理信息、认知推理等过程的正确性作\n深层次评测。 \n5.3.2 可解释 \n类人机器语言能力评测一定要给予未来研究\n一定的指导。在评测报告（可以自动生成）中应\n当指出模型的缺陷是什么，面对哪些问题处理得\n较差或者模型相较于其他模型在理解哪些的能力\n更强，或者是模型的哪个部件起到了相应作用。 \n5.3.3 易用性 \n评测应当具有完备的构件（评测数据、任务\n体系、评测指标、基准模型、人类基准、排行榜、\n评测报告），同时具有便捷高效的特性，以便在各\n环境下方便使用。 \n5.3.4 综合性 \n大量模型在部分易解的自然语言处理问题上\n取得了很好的结果，但是在一些复杂难解的问题\n上还有很长的路要走。这并不意味着评测可以直\n接丢弃那些容易的任务，而应从人类语言不同能\n力（阅读、推理、创造等）出发，综合地评估机\n器模型。 \n一方面，评测任务需要全面多样。综合的NLP\n评测既要考查到模型理解文本的能力，也要考查\n其生成文本的能力；既要涉及命名实体识别等经\n典任务，也要前瞻性地针对多模态任务和知识驱\n动任务作设计；既要有面向有监督方法的测试和\n \n12 \n中文信息学报 \n第2*卷 \n \n \n比较方案，也要为小样本、零样本、无监督等方\n法提供测试和比较方案。 \n另一方面，评测需要是一个有机系统。评测\n设计者应当为评测本身提供充分的可解释性（每\n个任务评测了模型哪些方面的能力，任务之间的\n联系与区别，评测结果的解读）。任务与任务之间\n应当做到不重不漏，有机结合，作为对机器语言\n能力的完整评测呈现。 \n值得一提的是，多任务的综合评测最好能够\n涵盖跨模态任务。跨模态的任务需要模型具有建\n模和联合不同模态信息的能力，是更加符合人类\n智能的综合型任务。人类往往在一个多模态场景\n中理解语言，也能将文本作不同模态的输出。图\n片、影音信息数据可以作为文本任务的辅助信息，\n也可以作为生成目标来与自然语言处理任务结合，\n这是当前人工智能发展的重要趋势。因此，类人\n机器语言能力评测不应当局限于纯文本任务，需\n要评估模型对这类多模态信息处理能力。 \nF\"\":;<=>$?@)*GH\"\nR\"!#cUV)'(de \n \n基于可解释性原则和综合性原则的要求，类\n人机器语言能力评测需要在体系化的组织和指导\n下展开。因此，需要事先建立评测大纲，系统梳\n理NLP 的各项核心技术、各项评测任务之间的关\n联性和有机性，建立机器语言能力与人类语言能\n力相对清晰的对应关系，全面盘点目前NLP 各项\n技术的进程。 \n评测大纲可有效规范和指导未来的评测，避\n免评测数量过多而质量参差不齐、可解释性差等\n乱象。 \n \nR\",#cfgh)'(ST#\n \n类人机器语言能力评测应当以人类语言能力\n为参照，根据当前机器语言能力发展状况，提出\n有挑战性的任务。例如心理学上通过语用测试来\n考察人的语言认知能力，要求被试回答“如果他\n(她)给你弄的吃的和喝的都不是你喜欢的，你会\n怎么让他(她)知道？”机器语言能力评测也应当\n设计此类有深度、考察认知的任务，而非一味设\n计只需要捕获简单统计规律即可解决的任务。 \n \nR\"/#cij)'(kl#\n \n6.3.1 模组化评测 \n从任务组织模式的角度来说，可以把某一类\n标注数据作为一种部件，按照评测需求快速搭建\n任务框架，实现评测模组化。 \n例如，把一组未标注的新闻领域语段作为一\n个一级部件A1，将关于这组数据的命名实体标注\n作为一个二级部件B1，将这组数据的摘要标注作\n为一个二级部件B2，将这组数据相关的图片也作\n为一个二级部件B3。A1+B2 可以作为一个文本摘\n要任务，A1+B2+B3 可以作为一个跨模态摘要任务。\n该种模组化的评测组织模式提高了数据重用率，\n同时提升了评测的可解释性和综合性，实现了额\n外特征标注信息和机器模型本身的分离，能促进\n关于机器语言能力的公平比较和评析。 \n6.3.2 层次化评测 \n层次化的评测要求多个子任务之间呈现层级\n递进的关系，从而体现机器语言能力评测的基本\n原则中关于难度的各个原则。如图8 所示，对于\n包含图文的多模态任务，可以设计如下四种评测\n层级。各上级子任务对其下级子任务存在包含关\n系，即模型必须在下级任务上取得较好的表现时，\n才能完成上级任务，否则模型可能仅仅是因为学\n到了数据集偏差，例如依赖一些错误的表层线索\n推出答案。 \n \n \n图8 层次化评测示意图 \n \n层次化的评测能有效规避评测中的偏差，提\n供更有说服力评测结果。 \n6.3.3 交互式评测 \n人类面试过程中，面试官可能一步步提供信\n息引导面试者给出更好的答案，整个过程中面试\n者与面试官是动态交互的状态。借鉴这种思路，\n机器语言能力评测也可以设计成动态交互式的评\n测，分角度引导机器思考并一步一步给出目标信\n息。 \n6.3.4 动态调整式评测 \n如前文所述，机器语言能力评测应当满足生\n命性原则，即评测在至少两年内应当保证评测效\n力。现阶段，很多NLP 评测的生命周期短的一大\n*期 \n董青秀等: 自然语言评测中的问题和对策 \n13 \n \n \n原因就是采用了固定测试集。尽管模型在训练阶\n段无法看到测试集数据，但模型不断迭代比较的\n过程都直接参照测试集的成绩，从长期来看，测\n试集实际以一种“软开发集”的形式被利用了起\n来。这种情况下，机器模型在测试集上的表现自\n然会以较快的速度提升，从而间接导致评测本身\n生命性的减弱。为了缓解这种情况，增强机器语\n言能力评测的生命性，可以利用数据增强、对抗\n攻击等技术，在评测过程中引入随机变动的测试\n集，增强机器语言能力评测的生命性。 \n6.3.5 应用检验式评测 \n机器语言能力评测原则中的无偏性，换言之，\n是要求评测把模型的泛化性能纳入考量。传统\nNLP 评测往往把模型在与训练集呈现相同分布的\n测试集上的表现作为最终结果，这导致了模型在\n实际应用场景测试时可能会出现巨大落差。未来，\n依托于工业界的测试场景和真实环境，研究者可\n以建立应用检验式的评测，把模型在真实场景上\n的表现作为评测结果，或者作为辅助信息为评测\n结果提供参考。 \n \nR\"2#cZmn)'(op#\n \n机器语言能力评测可以建设成通用平台，走\n一体化的道路。一方面，可以实现评测和诊断的\n一体化，即通过一系列探针任务把评测和诊断合\n二为一，自动生成可解释的评测结果和细粒度的\n诊断报告。另一方面，也可以实现领域和任务的\n一体化，即把领域内和领域外迁移学习评测，大\n样本、少样本和零样本评测在相同任务下同时实\n现。我们期待未来能建立通用的一体化评测平台，\n组织通用的、可交互可解释的评测与诊断服务，\n为机器语言能力的研究探索提供高效、准确和全\n面的反馈。 \nI\"\"JK\"\n本文介绍了现阶段主流自然语言评测，并归\n纳现阶段自然语言评测中存在的问题为四个类型\n——数据集偏差、评测指标失真、评测任务不科\n学和评测技术单一，分别对每类问题的成因进行\n了深入探析。在此基础上，本文结合目前最新的\n研究进展与研究思路和成果，针对上述关键问题，\n提出类比人类语言能力评测的机器语言能力评测，\n并从信度、难度、效度三个方面设计了共12 条类\n人机器语言能力评测的基本原则，希望为未来机\n器语言能力评测规范的设计提供参考。 \n目前，自然语言评测虽然处于高涨期，但其\n在各类任务上暴露的种种问题表明，现阶段的自\n然语言评测缺少原则和规范的约束，这也导致自\n然语言处理评测迟迟无法向稳定发展的阶段迈进。\n未来，自然语言评测应当以具有综合考察能力的\n类人机器语言能力评测为目标，在参考本文提出\n的基本原则和未来展望的基础上，采取更多样、\n更鲁棒的评测手段，科学高效地为机器模型提供\n客观、公平、类人的评测结果，继续引领和推动\n自然语言处理领域各类模型、方法的提出和创新。 \n \nLMNO\"\n[1] Trichelair P, Emami A, Trischler A, et al. \nHow reasonable are common-sense reasoning tasks: \nA case-study on the Winograd Schema Challenge and \nSWAG[J]. arXiv preprint arXiv:1811.01778, 2018. \n[2] Davis E, Morgenstern L, Ortiz C L. The first \nWinograd schema challenge at IJCAI-16[J]. AI Mag-\nazine, 2017, 38(3): 97-98. \n[3] Papineni K, Roukos S, Ward T, et al. BLEU: a \nmethod for automatic evaluation of machine trans-\nlation[C]//Proceedings of the 40th annual meeting \nof the Association for Computational Linguistics. \n2002: 311-318. \n[4] Lin C Y. Rouge: A package for automatic eval-\nuation \nof \nsummaries[C]//Text \nsummarization \nbranches out. 2004: 74-81. \n[5] Stanojević M, Sima’an K. Beer: Better eval-\nuation as ranking[C]//Proceedings of the Ninth \nWorkshop on Statistical Machine Translation. 2014: \n414-419. \n[6] Shimanaka H, Kajiwara T, Komachi M. Ruse: Re-\ngressor using sentence embeddings for automatic \nmachine translation evaluation[C]//Proceedings of \nthe Third Conference on Machine Translation: \nShared Task Papers. 2018: 751-758. \n[7] Gardner M, Artzi Y, Basmov V, et al. Evaluat-\ning models’ local decision boundaries via con-\ntrast sets[C]//Proceedings of the 2020 Conference \non Empirical Methods in Natural Language Pro-\ncessing: Findings. 2020: 1307-1323. \n[8] Zhang T, Kishore V, Wu F, et al. Bertscore: \nEvaluating text generation with bert[J]. arXiv \npreprint arXiv:1904.09675, 2019. \n[9] Sellam T, Das D, Parikh A P. BLEURT: Learning \nRobust Metrics for Text Generation[J]. arXiv pre-\nprint arXiv:2004.04696, 2020. \n[10] Hou Y, Mao J, Lai Y, et al. FewJoint: A Few-\nshot Learning Benchmark for Joint Language Under-\nstanding[J]. arXiv preprint arXiv:2009.08138, \n14 \n中文信息学报 \n第2*卷 \n \n \n2020. \n[11] Turing A M. Computing machinery and intelli-\ngence[M]//Parsing the turing test. Springer, Dor-\ndrecht, 2009: 23-65. \n[12] National Research Council. Automatic Language \nProcessing Advisory Committee. Language and ma-\nchines: computers in translation and linguistics; \na report[M]. National Academies, 1966. \n[13] King M. When is the next Alpac report \ndue?[C]//10th International Conference on Compu-\ntational Linguistics and 22nd Annual Meeting of \nthe Association for Computational Linguistics. \n1984: 352-353. \n[14] Pallett D S. A look at NIST's benchmark ASR \ntests: past, present, and future[C]//2003 IEEE \nWorkshop on Automatic Speech Recognition and Un-\nderstanding (IEEE Cat. No. 03EX721). IEEE, 2003: \n483-488. \n[15] Harman D. The DARPA tipster project[C]//ACM \nSIGIR Forum. New York, NY, USA: ACM, 1992, 26(2): \n26-28. \n[16] TREC: Experiment and evaluation in infor-\nmation retrieval[M]. Cambridge: MIT press, 2005. \n[17] Rajpurkar P, Zhang J, Lopyrev K, et al. Squad: \n100,000+ questions for machine comprehension of \ntext[J]. arXiv preprint arXiv:1606.05250, 2016. \n[18] Wang A, Singh A, Michael J, et al. Glue: A \nmulti-task benchmark and analysis platform for \nnatural language understanding[J]. arXiv preprint \narXiv:1804.07461, 2018. \n[19] McCoy R T, Pavlick E, Linzen T. Right for the \nwrong reasons: Diagnosing syntactic heuristics in \nnatural language inference[J]. arXiv preprint \narXiv:1902.01007, 2019. \n[20] Sang E F, De Meulder F. Introduction to the \nCoNLL-2003 \nshared \ntask: \nLanguage-independent \nnamed entity recognition[J]. arXiv preprint \ncs/0306050, 2003. \n[21] Levesque H, Davis E, Morgenstern L. The wino-\ngrad schema challenge[C]//Thirteenth Interna-\ntional Conference on the Principles of Knowledge \nRepresentation and Reasoning. 2012. \n[22] Williams A, Nangia N, Bowman S R. A broad-\ncoverage challenge corpus for sentence understand-\ning \nthrough \ninference[J]. \narXiv \npreprint \narXiv:1704.05426, 2017. \n[23] Zhang Y, Zhong V, Chen D, et al. Position-\naware attention and supervised data improve slot \nfilling[C]//Proceedings of the 2017 Conference on \nEmpirical Methods in Natural Language Processing. \n2017: 35-45. \n[24] Socher R, Perelygin A, Wu J, et al. Recursive \ndeep models for semantic compositionality over a \nsentiment treebank[C]//Proceedings of the 2013 \nconference on empirical methods in natural lan-\nguage processing. 2013: 1631-1642. \n[25] Rajpurkar P, Jia R, Liang P. Know what you \ndon't know: Unanswerable questions for SQuAD[J]. \narXiv preprint arXiv:1806.03822, 2018. \n[26] Lowe R, Pow N, Serban I, et al. The ubuntu \ndialogue corpus: A large dataset for research in \nunstructured \nmulti-turn \ndialogue \nsystems[J]. \narXiv preprint arXiv:1506.08909, 2015. \n[27] Hermann K M, Kocisky T, Grefenstette E, et al. \nTeaching machines to read and comprehend[J]. Ad-\nvances in neural information processing systems, \n2015, 28: 1693-1701. \n[28] McCann B, Keskar N S, Xiong C, et al. The \nnatural language decathlon: Multitask learning as \nquestion \nanswering[J]. \narXiv \npreprint \narXiv:1806.08730, 2018. \n[29] Wang A, Pruksachatkun Y, Nangia N, et al. \nSuperglue: A stickier benchmark for general-pur-\npose language understanding systems[C]//Advances \nin Neural Information Processing Systems. 2019: \n3266-3280. \n[30] Xu L, Zhang X, Li L, et al. CLUE: A Chinese \nLanguage Understanding Evaluation Benchmark[J]. \narXiv preprint arXiv:2004.05986, 2020. \n[31] Le H, Vial L, Frej J, et al. Flaubert: Unsu-\npervised language model pre-training for french[J]. \narXiv preprint arXiv:1912.05372, 2019. \n[32] Wilie B, Vincentio K, Winata G I, et al. In-\ndoNLU: Benchmark and resources for evaluating In-\ndonesian natural language understanding[J]. arXiv \npreprint arXiv:2009.05387, 2020. \n[33] Hu J, Ruder S, Siddhant A, et al. Xtreme: A \nmassively multilingual multi-task benchmark for \nevaluating cross-lingual generalization[J]. arXiv \npreprint arXiv:2003.11080, 2020. \n[34] Liang Y, Duan N, Gong Y, et al. Xglue: A new \nbenchmark dataset for cross-lingual pre-training, \nunderstanding and generation[J]. arXiv preprint \narXiv:2004.01401, 2020. \n[35] Liu D, Yan Y, Gong Y, et al. GLGE: A New \nGeneral Language Generation Evaluation Bench-\nmark[J]. arXiv preprint arXiv:2011.11928, 2020. \n[36] Storks S, Gao Q, Chai J Y. Recent advances in \nnatural language inference: A survey of benchmarks, \nresources, and approaches[J]. arXiv preprint \narXiv:1904.01172, 2019. \n[37] Chomsky N. Aspects of the Theory of Syntax[M]. \nMIT press, 2014. \n[38] Reddy S, Chen D, Manning C D. Coqa: A conver-\nsational question answering challenge[J]. Trans-\nactions of the Association for Computational Lin-\nguistics, 2019, 7: 249-266. \n[39] Talmor A, Herzig J, Lourie N, et al. Com-\nmonsenseqa: A question answering challenge tar-\ngeting commonsense knowledge[J]. arXiv preprint \narXiv:1811.00937, 2018. \n*期 \n董青秀等: 自然语言评测中的问题和对策 \n15 \n \n \n[40] Poliak A, Haldar A, Rudinger R, et al. Col-\nlecting diverse natural language inference prob-\nlems for sentence representation evaluation[J]. \narXiv preprint arXiv:1804.08207, 2018. \n[41] Niven T, Kao H Y. Probing neural network com-\nprehension of natural language arguments[J]. arXiv \npreprint arXiv:1907.07355, 2019. \n[42] Sharma R, Allen J, Bakhshandeh O, et al. Tack-\nling the story ending biases in the story cloze \ntest[C]//Proceedings of the 56th Annual Meeting \nof the Association for Computational Linguistics \n(Volume 2: Short Papers). 2018: 752-757. \n[43] Schwartz R, Sap M, Konstas I, et al. The ef-\nfect of different writing tasks on linguistic \nstyle: A case study of the ROC story cloze task[J]. \narXiv preprint arXiv:1702.01841, 2017. \n[44] Rudinger R, Naradowsky J, Leonard B, et al. \nGender bias in coreference resolution[J]. arXiv \npreprint arXiv:1804.09301, 2018. \n[45] Rashkin H, Sap M, Allaway E, et al. Event2mind: \nCommonsense inference on events, intents, and re-\nactions[J]. arXiv preprint arXiv:1805.06939, 2018. \n[46] Rahman A, Ng V. Resolving complex cases of \ndefinite pronouns: the winograd schema chal-\nlenge[C]//Proceedings of the 2012 Joint Conference \non Empirical Methods in Natural Language Pro-\ncessing and Computational Natural Language Learn-\ning. 2012: 777-789. \n[47] Cui L, Wu Y, Liu S, et al. MuTual: A Dataset \nfor Multi-Turn Dialogue Reasoning[J]. arXiv pre-\nprint arXiv:2004.04494, 2020. \n[48] Ma W, Wu Y, Cen F, et al. Mdfn: Multi-scale \ndeep feature learning network for object detec-\ntion[J]. Pattern Recognition, 2020, 100: 107149. \n[49]  \n \n董青秀（1998—），博士研究生，主要研究领域为自\n然语言处理。 \nE-mail：qingxiudong@icloud.com \n \n \n \n \n \n穗志方（1970—），通讯作者，博士，教授，主要\n研究领域为计算语言学、知识工程。 \nE-mail：szf@pku.edu.cn \n \n \n \n \n詹卫东（1972—），博士，教授，主要研究领域\n为现代汉语形式语法、中文信息处理、汉语语言\n知识工程。 \nE-mail：zwd@pku.edu.cn \n \n \n \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-04-20",
  "updated": "2021-04-20"
}