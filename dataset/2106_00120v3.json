{
  "id": "http://arxiv.org/abs/2106.00120v3",
  "title": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep Probabilistic Models",
  "authors": [
    "Daniel T. Chang"
  ],
  "abstract": "Probabilistic deep learning is deep learning that accounts for uncertainty,\nboth model uncertainty and data uncertainty. It is based on the use of\nprobabilistic models and deep neural networks. We distinguish two approaches to\nprobabilistic deep learning: probabilistic neural networks and deep\nprobabilistic models. The former employs deep neural networks that utilize\nprobabilistic layers which can represent and process uncertainty; the latter\nuses probabilistic models that incorporate deep neural network components which\ncapture complex non-linear stochastic relationships between the random\nvariables. We discuss some major examples of each approach including Bayesian\nneural networks and mixture density networks (for probabilistic neural\nnetworks), and variational autoencoders, deep Gaussian processes and deep mixed\neffects models (for deep probabilistic models). TensorFlow Probability is a\nlibrary for probabilistic modeling and inference which can be used for both\napproaches of probabilistic deep learning. We include its code examples for\nillustration.",
  "text": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep \nProbabilistic Models  \n \nDaniel T. Chang (张遵) \n \nIBM (Retired) dtchang43@gmail.com \nAbstract:  Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data \nuncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to \nprobabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural \nnetworks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that \nincorporate deep neural network components which capture complex non-linear stochastic relationships between the random \nvariables. We discuss some major examples of each approach including Bayesian neural networks and mixture density \nnetworks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects \nmodels (for deep probabilistic models). TensorFlow Probability is a library for probabilistic modeling and inference which \ncan be used for both approaches of probabilistic deep learning. We include its code examples for illustration.  \n1 Introduction \nStandard deep learning is deterministic and does not account for uncertainty, either model uncertainty or data uncertainty \n[1]. This is a major limitation, particularly for mission-critical and real-world applications such as biomedical applications. \nProbabilistic deep learning removes this limitation by accounting for uncertainty, both model uncertainty and data \nuncertainty. \nProbabilistic deep learning is based on the use of probabilistic models and deep neural networks. We distinguish two \napproaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models, due to their \ndifference in focus. The former employs deep neural networks that utilize probabilistic layers [7] which can represent and \nprocess uncertainty; the latter uses probabilistic models that incorporate deep neural network components [10] which capture \ncomplex non-linear stochastic relationships between the random variables. The distinction, however, may not always be clear \ncut: A probabilistic neural network may be used to realize a deep probabilistic model, and a deep probabilistic model may \nincorporate probabilistic neural network components. \nFor probabilistic neural networks, we discuss two major examples: Bayesian neural networks and mixture density \nnetworks. Bayesian neural networks [8] utilize probabilistic layers that capture uncertainty over weights and activations, and \nare trained using Bayesian inference. Mixture density networks [9] are designed to quantify neural network prediction \nuncertainty. They consist of two components: a mixture model and a deep neural network. \n2 \n \nFor deep probabilistic models, we discuss three major examples: variational autoencoders, deep Gaussian processes, and \ndeep mixed effects models. Variational autoencoders [1] are prescribed deep generative models realized using an \nautoencoder neural network that consists of an encoder network and a decoder network, which encodes a data sample to a \nlatent representation and generates data samples from the latent space, respectively. Deep Gaussian processes [11] are multi-\nlayer generalizations of Gaussian processes [2], which capture the compositional nature of (deterministic) deep learning \nwhile mitigating some of the disadvantages through a Bayesian approach. Deep mixed effects models [12] are extensions to \nlinear mixed effects models that use nonlinear mappings between the response variable and predictor variables, which are \nrealized using deep neural networks. \nTensorFlow Probability  [4-6] is a library for probabilistic modeling and inference which, with Keras / TensorFlow, can \nbe used for building both probabilistic neural networks and deep probabilistic models. We include its code examples, where \napplicable, for illustration. \n2 Background Information \nAs background information, we briefly discuss uncertainty, probabilistic models, and Bayesian inference, which are the \nprobabilistic foundation of probabilistic deep learning. We also briefly summarize TensorFlow Probability. The discussions \non uncertainty and probabilistic models are extracted from [1]. Please see it for references and further discussions. \n2.1 Uncertainty \nUncertainty arises because of limitations in our ability to observe the world, limitations in our ability to model it, and \npossibly inherent non-determinism (e.g., quantum phenomena). From the perspective of deep learning, there are two types of \nuncertainty: model uncertainty and data uncertainty. Model uncertainty accounts for uncertainty in the model structure and \nmodel parameters. This is important to consider for mission-critical applications and small datasets. Data uncertainty \naccounts for out-of-distribution data and noisy data. This is important to consider for real-world applications and large \ndatasets.  \nUncertainty, therefore, should be an inescapable aspect of deep learning. Because of this, we need to allow our learning \nsystem to represent uncertainty and our reasoning system to consider different possibilities due to uncertainty. Further, to \nobtain meaningful conclusions, we need to reason not just about what is possible, but also about what is probable. Probability \n3 \n \ntheory provides us with a formal framework for representing uncertainty and for considering multiple possible outcomes and \ntheir likelihood. \n2.2 Probabilistic Models \nThe probabilistic approach to modeling uses probability theory to represent all forms of uncertainty and for inference: \n- Probability distributions are used to represent all uncertain elements in a model (including structural, parametric, out-\nof-distribution and noise-related) and how they relate to the data. \n- The basic rules of probability theory are used to infer the uncertain elements given the observed data. \n- Learning from data occurs through the transformation of the prior distributions (defined before observing the data) into \nposterior distributions (obtained after observing the data). The application of probability theory to learning from data is \ncalled Bayesian learning or Bayesian inference. \nSimple probability distributions over a single or a few random variables can be composed to form the building blocks of \nlarger, more complex models. The compositionality of probabilistic models means that the behavior of these building blocks \nin the context of the larger model is often much easier to understand. The dominant paradigm for representing such \ncompositional probabilistic models is probabilistic graphical models. \nProbabilistic Graphical Models \nProbabilistic graphical models (PGMs) are structured probabilistic models. A PGM is a way of describing a probability \ndistribution of random variables, using a graph to describe which variables in the probability distribution directly interact \nwith each other, with each node representing a variable and each edge representing a direct interaction. This allows the \nmodels to have significantly fewer parameters which can in turn be estimated reliably from less data. These smaller models \nalso have dramatically reduced computational cost in terms of storing the model, performing inference in the model, and \ndrawing samples from the model. \nUsually a PGM comes with both a graphical representation of the model and a generative process to depict how the \nrandom variables are generated. Due to its Bayesian nature, a PGM is easy to extend to incorporate other information or to \n4 \n \nperform other tasks. There are essentially two types of PGM: directed PGM (also known as Bayesian network) and \nundirected PGM (also known as Markov random field). \nA good PGM needs to accurately capture the distribution over the observed variables x, p(x). Often the different \nelements of x are highly dependent on each other. The approach most commonly used to model these dependencies is to \nintroduce several latent variables z. The model can then capture dependencies between any pair of variables xi and xj \nindirectly, via direct dependencies between xi and z, and direct dependencies between z and xj. Latent variables have \nadvantages beyond their role in efficiently capturing p(x). The latent variables z also provides an alternative representation \nfor x. Many approaches accomplish representation learning by learning latent variables. \n2.3 Bayesian Inference \nThe main idea of Bayesian inference [3] is to infer a posterior distribution over the parameters θ of a probabilistic model \ngiven some observed data D using Bayes theorem as: \np(θ | D) = p(D | θ)p(θ) / p(D) = p(D | θ)p(θ) / ∫ p(D | θ)p(θ)dθ,  \nwhere p(D | θ) is the likelihood, p(D) is the marginal likelihood (or evidence), and p(θ) is the prior. Computing the posterior, \nand moreover sampling from it, is usually intractable, especially since computing the evidence (integrals) is hard. \nApproximate inference methods are usually used, including: \n1. Markov Chain Monte Carlo: Approximates integrals via sampling. \n2. Variational Inference: Approximates integrals via optimization. \nThe Bayesian posterior can be used to model new unseen data D∗ using the posterior predictive: \np(D* | D) = ∫ p(D* | θ)p(θ | D)dθ,  \nwhich is also called the Bayesian model average because it averages the predictions of all plausible models weighted by their \nposterior probability.  \nThe choice of prior [3], p(θ), is one of the most critical parts of the Bayesian inference. It should be chosen in a way such \nthat it accurately reflects our beliefs about the parameters θ before seeing any data. There is no universally preferred prior, \nbut each probabilistic model / inference task is potentially endowed with its own optimal prior. \n5 \n \n2.4 TensorFlow Probability \nTensorFlow Probability (TFP) [4-6] is a library for probabilistic modeling and inference in TensorFlow. It provides \nintegration of probabilistic models with deep neural networks, gradient-based inference via automatic differentiation, and \nscalability to large datasets and models via hardware acceleration (e.g., GPUs) and distributed computation. \nTFP is structured as following layers, from bottom to top: \n1. TensorFlow: Numerical operations. \n2. Probabilistic building blocks \na. \nDistributions (tfp.distributions): A large collection of probability distributions which provide fast, \nnumerically stable methods for generating samples and computing statistics. \nb. Bijectors (tfp.bijectors): Reversible and composable transformations of distributions, with automatic \ncaching, preserving the ability to generate samples and compute statistics. \n3. Probabilistic model / neural network building \na. \nJoint Distributions (tfp.distributions.JointDistributionSequential and others): Joint distributions over one or \nmore possibly-interdependent distributions, designed for building small- to medium-size PGMs. \nb. Probabilistic Layers (tfp.layers): Neural network layers with uncertainty. \n4. Bayesian inference \na. \nMarkov Chain Monte Carlo (tfp.mcmc): Algorithms for approximating integrals via sampling. \nb. Variational Inference (tfp.vi): Algorithms for approximating integrals via optimization. \nc. \nOptimizers (tfp.optimizer): Stochastic optimization methods. \nd. Monte Carlo (tfp.monte_carlo): Tools for computing Monte Carlo expectations. \nTFP can be used for building both probabilistic neural networks and deep probabilistic models, as can be seen in \nsubsequent discussions. \n3 Probabilistic Neural Networks \nProbabilistic neural networks are deep neural networks that utilize probabilistic layers which can represent and process \nuncertainty. Probabilistic layers [7] can capture uncertainty over weights (Bayesian neural networks), pre-activation units \n(dropout), activations (“probabilistic output layers”), or the function itself (Gaussian processes). They can also be layers that \n6 \n \npropagate uncertainty from input to output. Probabilistic layers are designed to be drop-in replacement of their deterministic \ncounter parts. \n3.1 Bayesian Neural Networks \nBayesian neural networks (BNNs) [8] utilize probabilistic layers that capture uncertainty over weights and activations, \nand are trained using Bayesian inference. At a high level, they can be represented as follow: \nθ ∼ p(θ), \ny = BNNθ(x) + ϵ, \nwhere θ represents BNN parameters and ϵ represents random noise. \nTo design a BNN, the first step is to choose a neural network architecture, e.g., a convolutional neural network. Then, \none has to choose a probability model: a prior distribution over the possible model parameters p(θ) and a prior confidence in \nthe predictive power of the model p(y | x, θ), i.e., BNNθ(x). For supervised learning, the Bayesian posterior can be written as: \np(θ | D) = p(Dy | Dx, θ)p(θ) / ∫ p(Dy | Dx, θ)p(θ)dθ, \nwhere D is the training set, Dx the training features, and Dy the training labels. Computing the posterior, and moreover \nsampling from it, is usually intractable, especially since computing the evidence (denominator) is hard. In general one uses a \nvariational inference approach, which learns a variational distribution qϕ(θ) to approximate the exact posterior. \nGiven the Bayesian posterior, or in practice its variational approximation, the posterior predictive can be computed as: \np(y | x, D) = ∫ p(y | x, θ)p(θ | D)dθ. \nIn practice, the distribution p(y | x, θ) is sampled indirectly using BNNθ(x), and θ is sampled from the variational distribution \nqϕ(θ). \nThe following shows an example code of a convolutional BNN model in TensorFlow Probability [4]. It defines a LeNet-\n5 model using three convolutional (with max pooling) layers and two fully connected dense layers. It uses the Flipout Monte \nCarlo estimator for these layers. KL divergence, using Lambda function, is passed as input to the kernel_divergence_fn on \n7 \n \nflipout layers. The Keras API will automatically add the KL divergence to the cross entropy loss, effectively calculating the \n(negated) Evidence Lower Bound Loss (ELBO). \n \n \n \n \n \n \n \n \n \n3.2 Mixture Density Networks \nMixture density networks (MDNs) [9] are designed to quantify neural network prediction uncertainty. MDNs consist of \ntwo components: a mixture model and a deep neural network. The mixture model can be written as: \np(y | x) = ∑\np(y; фk(x))p(фk(x); π(x))\n௄\n௞ୀଵ\n. \nThe deep neural network can be of any valid architecture that maps x onto both the parameters {фk(x)}k=1\nK of the K mixture \ncomponents and the parameters π(x) of the mixing distribution. MDNs can be trained by maximizing the log-likelihood of \nthe parameters of the deep neural network given the training set D = {xn, yn} n=1\nN using gradient-based optimizers. \n   \n  kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /        \n                            tf.cast(NUM_TRAIN_EXAMPLES, dtype=tf.float32)) \n \n  model = tf.keras.models.Sequential([ \n      tfp.layers.Convolution2DFlipout( \n          6, kernel_size=5, padding='SAME', \n          kernel_divergence_fn=kl_divergence_function, \n          activation=tf.nn.relu), \n      tf.keras.layers.MaxPooling2D( \n          pool_size=[2, 2], strides=[2, 2], \n          padding='SAME'), \n      tfp.layers.Convolution2DFlipout( \n          16, kernel_size=5, padding='SAME', \n          kernel_divergence_fn=kl_divergence_function, \n          activation=tf.nn.relu), \n      tf.keras.layers.MaxPooling2D( \n          pool_size=[2, 2], strides=[2, 2], \n          padding='SAME'), \n      tfp.layers.Convolution2DFlipout( \n          120, kernel_size=5, padding='SAME', \n          kernel_divergence_fn=kl_divergence_function, \n          activation=tf.nn.relu), \n      tf.keras.layers.Flatten(), \n      tfp.layers.DenseFlipout( \n          84, kernel_divergence_fn=kl_divergence_function, \n          activation=tf.nn.relu), \n      tfp.layers.DenseFlipout( \n          NUM_CLASSES, kernel_divergence_fn=kl_divergence_function, \n          activation=tf.nn.softmax) \n  ]) \n8 \n \nThe following shows an example code of a MDN model in TensorFlow Probability [4]. It utilizes a mixture distribution \nlayer with independent normal (Gaussian) components. \n \n4 Deep Probabilistic Models \nDeep probabilistic models [10] are probabilistic models that incorporate deep neural network components which capture \ncomplex non-linear stochastic relationships between the random variables. \nUnsupervised and semi-supervised learning methods are often based on generative models which are probabilistic \nmodels that express hypotheses about the way in which data may have been generated. PGMs have emerged as a broadly \nuseful approach to specifying generative models. Deep generative models (DGMs) [1] are PGMs that employ deep neural \nnetworks for parameterizing the models. In particular, prescribed DGMs are those that provide an explicit parametric \nspecification of the probability distribution of the observed variable x, specifying the likelihood  function pθ(x) with \nparameter θ. DGMs are among the most widely used deep probabilistic models. \n4.1 Variational Autoencoders \n(The discussion in this subsection is extracted from [1]. Please see it for references and further discussions.) \nThe variational autoencoder (VAE) is a prescribed DGM realized using an autoencoder neural network that consists of \nan encoder network and a decoder network, which encodes a data sample to a latent representation and generates data \nsamples from the latent space, respectively. The decoder network is a differentiable generator network and the encoder \nnetwork is an auxiliary inference network. Both networks are jointly trained using variational learning. Variational learning \nuses the variational lower bound of the marginal log-likelihood as the single objective function to optimize both the \ngenerator network and the auxiliary inference network. \n   \nevent_shape = [1] \nnum_components = 5 \nparams_size = tfpl.MixtureNormal.params_size(num_components, event_shape) \n \nmodel = tfk.Sequential([ \n  tfkl.Dense(12, activation='relu'), \n  tfkl.Dense(params_size, activation=None), \n  tfpl.MixtureNormal(num_components, event_shape) \n]) \n9 \n \nThe likelihood function of a prescribed DGM can be written as: \npθ(x) ≡ pθ(z) pθ(x | z) \nIt is usually intractable to directly evaluate and maximize the marginal log-likelihood log(pθ(x)). Following the variational \ninference approach, one introduces an auxiliary inference model qφ(z | x) with parameters φ, which serves as an \napproximation to the exact posterior pθ(z | x). \nThe variational lower bound can be rewritten as: \n \nL(x; θ, φ) = ∑qφ(𝐳 | 𝐱)\n௭\nlog(pθ(x | z)) – DKL(qφ(z | x) || pθ(z)) \nThe first term is the expected reconstruction quality, requiring that pθ(x | z) is high for samples of z from qφ(z | x), while the \nsecond term (the KL divergence between the approximate posterior and the prior) acts as a regularizer, ensuring we can \ngenerate realistic data by sampling latent variables from pθ(z). \n \nVariational learning is to maximize the variational lower bound over the training data. It performs something like the \nautoencoder, with qφ(z | x) as the encoder and pθ(x | z) as the decoder. As such, the VAE introduces the constraint on the \nautoencoder that the latent variable z is distributed according to a prior p(z). The encoder qφ(z | x) approximates the posterior \np(z | x), and the decoder pθ(x | z) parameterizes the likelihood p(x | z). The generation model is then z ~ p(z); x ~ p(x | z). \nThe following shows an example code of a convolutional VAE in TensorFlow Probability [4]: encoder, decoder and \nVAE. Note that KLDivergenceRegulizer is used in the encoder. \n10 \n \n \n \n   \ndecoder = tfk.Sequential([ \n    tfkl.InputLayer(input_shape=[encoded_size]), \n    tfkl.Reshape([1, 1, encoded_size]), \n    tfkl.Conv2DTranspose(2 * base_depth, 7, strides=1, \n                         padding='valid', activation=tf.nn.leaky_relu), \n    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=1, \n                         padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2DTranspose(2 * base_depth, 5, strides=2, \n                         padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2DTranspose(base_depth, 5, strides=1, \n                         padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2DTranspose(base_depth, 5, strides=2, \n                         padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2DTranspose(base_depth, 5, strides=1, \n                         padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2D(filters=1, kernel_size=5, strides=1, \n                padding='same', activation=None), \n    tfkl.Flatten(), \n    tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits), \n]) \n   \nencoder = tfk.Sequential([ \n    tfkl.InputLayer(input_shape=input_shape), \n    tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5), \n    tfkl.Conv2D(base_depth, 5, strides=1, \n                padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2D(base_depth, 5, strides=2, \n                padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2D(2 * base_depth, 5, strides=1, \n                padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2D(2 * base_depth, 5, strides=2, \n                padding='same', activation=tf.nn.leaky_relu), \n    tfkl.Conv2D(4 * encoded_size, 7, strides=1, \n                padding='valid', activation=tf.nn.leaky_relu), \n    tfkl.Flatten(), \n    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size), \n               activation=None), \n    tfpl.MultivariateNormalTriL( \n        encoded_size, \n        activity_regularizer=tfpl.KLDivergenceRegularizer(prior)), \n]) \n11 \n \n \n4.2 Deep Gaussian Processes \nGaussian Processes \n(The discussion in this subsection is extracted from [2]. Please see it for references and further discussions.)  \nThe Gaussian process (GP) is a well-known non-parametric probabilistic model. A Gaussian process is a generalization \nof the Gaussian distribution. Whereas a probability distribution describes random variables which are scalars or vectors, a \nstochastic process governs the properties of functions. We can loosely think of a function as an infinite vector, each entry in \nthe vector specifying the function value f(xi) at a particular input xi. A key aspect of this is that if we ask only for the \nproperties of the function at a finite number of points, then GP inference will give us the same answer if we ignore the \ninfinitely many other points. \nIn the function-space view a Gaussian process defines a distribution over functions, and inference takes place directly in \nthe space of functions. A Gaussian process is completely specified by its mean function (average of all functions) and kernel \n(covariance) function (how much individual functions can vary around the mean function): \nm(x) = E[f(x)], \nk(x, x’) = E[(f(x) – m(x))(f(x’) – m(x’))] \nAs such, we write the Gaussian process as \nf(x) ∼ GP(m(x), k(x, x’)). \n \nUnder the Gaussian process, the true objective is modeled by a GP prior with a mean and a kernel function. Given a set \nof (noisy) observations from initial evaluations, a Bayesian posterior update gives the GP posterior with an updated mean \nand kernel function. The mean function of the GP posterior gives the best predictions at any point conditional on the \n   \nvae = tfk.Model(inputs=encoder.inputs, \n                outputs=decoder(encoder.outputs[0])) \n12 \n \navailable observations, and the kernel function quantifies the uncertainty in the predictions. The GP prior is a multivariate \nGaussian distribution:  \np(f | θ) ∼ 𝒩(µ, K(θ)). \nSo is the GP posterior: \np(f | D, θ) ∼ 𝒩(µ’, K’(θ)). \nIn the above, θ is GP parameters (parameters of the GP kernel function). \nDeep Gaussian Processes \nGaussian processes have the following limitations [11]:   \n \nThe assumption of Gaussian marginals: A Gaussian process cannot model heavy-tailed, asymmetric or \nmultimodal marginal distributions. \n \nThe requirement to define a prior entirely in terms of mean and covariance: Even very complicated covariance \nfunctions cannot express prior relationships that are not captured by mean and covariance alone. \n \nThe predictive covariance of the exact posterior process is independent of the observed outputs: It is \nunsatisfactory that the predictive uncertainty is decoupled from the observations. \n \nThe predictive mean is linear in the observations: There are many situations where this linearity might be \ninappropriate. \nDeep Gaussian processes (DGPs) [11] are multi-layer generalizations of GPs that promise to overcome some of the \nabove limitations of the single layer model. The DGP presents a paradigm for building deep models from a Bayesian \nperspective. It captures the compositional nature of (deterministic) deep learning while mitigating some of the disadvantages, \ne.g., lack of treatment for uncertainty, through a Bayesian approach. \nIndependent GPs f1, … , fL (at layer l = 1, …, L) can be combined through function composition to construct a DGP [11]. \nThe composite function g(x) is given by  \ng(x) = fL(fL−1(…f1(x))), \n13 \n \nwhere the outputs of each layer are the inputs to the next higher layer. Given a likelihood p(y | g(x)), the DGP model has the \njoint distribution, \np(y, {fl}l=1\nL) = ∏i=1\nN p(yi  | fL(fL−1(…f1(xi)))) ∏l=1\nL p(fl),  \nwhere the first part on the right-hand side is likelihood, the second part is prior, and each layer is a GP p(fl) = GP (ml, kl). \nThe compositional and hierarchical structure of a DGP makes it natural for a DGP to be realized using a deep neural \nnetwork. The following shows an example code of a DGP in Edward2 [7]. \n \n4.3 Deep Mixed Effects Models \nGrouped data commonly occur in social sciences and medicine. The usual approach when dealing with multiple samples \nfrom each group is to utilize the mixed effects models [12]. The mixed effects are composed of two parts: the fixed effects and \nthe random effects. The fixed effects are common in all samples and so their coefficients are called fixed. In contrast, the \nrandom effects are specific to groups and their coefficients can vary depending on the groups, which are assumed to be drawn \nfrom some unknown distributions. Using mixed effects is natural whenever data are clustered in groups / hierarchy. \nLinear Mixed Effects Models \nA linear mixed effects model [12] can be expressed as: \nyi = β0 + β1x1 + … + βpxp + ui1z1 + … + uiqzq + ϵi, \n   \nbatch_size = 256 \nfeatures, labels = load_spatial_data(batch_size) \n \nmodel = tf.keras.Sequential([ \n    tf.keras.layers.Flatten(), \n    layers.SparseGaussianProcess(units=256,num_inducing=512), \n    layers.SparseGaussianProcess(units=256,num_inducing=512), \n    layers.SparseGaussianProcess(units=10,num_inducing=512), \n]) \n14 \n \nwhere β := [β1, ... , βp]T are the fixed effects shared over the entire sample, ui := [ui1 , … ,uip ]T are the random effects of the \nith group, and z = [z1, ... , zq] is a design vector for random effects. ui and ϵi are drawn from group-specific unknown \ndistributions. Typically, the unknown distributions are assumed to be a zero-mean Gaussian with an unknown covariance \nstructure: \nui ∼ 𝒩(0, Σu) and ϵi ∼ 𝒩(0, Σϵi). \nSince the linear mixed effects models have multiple random effects from unknown distributions, no closed form solution \nis available. For estimation, Expectation-Maximization algorithms and Markov Chain Monte Carlo sampling are used. \nDeep Mixed Effects Models \nA deep mixed effects model [12] is an extension to linear mixed effects models that uses nonlinear mappings between the \nresponse variable and predictor variables, which are realized using deep neural networks. Let yi = [y(ij)]j=1\nni be a set of ni \nobservations for a response variable from group i, Xi = [x(ij)1, … , x(ij)p]j=1\nni the corresponding predictor variables for group i, \nand Zi = [z(ij)1, … , z(ij)q]j=1\nni the corresponding design matrix for group i. The deep mixed effects model can be written as \nyi = Γ(Xi)β + Γ(Zi)ui, \nwhere β are the coefficients for the fixed effects and ui are the coefficients for the random effects of group i. Γ(.) is a nonlinear \ntransformation that can be realized by a deep neural network of any valid architecture. Recall that \nui ∼ 𝒩(0, Σu), \n i.e., ui is drawn from a zero-mean Gaussian with an unknown covariance structure. \nThe deep mixed effects model thus consists of a fixed effects component, f(x(ij)) = Γ(x(ij))β, and a separate random effects \ncomponent, f’(x(ij)) = Γ(x(ij))ui. For regression problems, the deep mixed effects model can be written as an optimization \nproblem by training the deep neural network Γ(.) with the loss function \n∑|| y(ij) - f(x(ij)) - f’(x(ij))||2\nij\n. \n15 \n \n5 Summary and Conclusion \nProbabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It \nis based on the use of probabilistic models and deep neural networks. As background information, we discussed the \nprobabilistic foundation of probabilistic deep learning: uncertainty, probabilistic models, and Bayesian inference. \nWe distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic \nmodels. For probabilistic neural networks, we discussed Bayesian neural networks and mixture density networks; for deep \nprobabilistic models we discussed variational autoencoders, deep Gaussian processes and deep mixed effects models. Where \napplicable, we included TensoFlow Probability code examples for illustration. \nWith the availability of libraries for probabilistic modeling and inference, such as TensorFlow Probability, it is hopeful \nthat probabilistic deep learning will become more widely used in the near future so that uncertainty, both model uncertainty \nand data uncertainty, in deep learning will routinely be accounted for and quantified. \nAcknowledgement: Thanks to my wife Hedy (期芳) for her support. \nReferences \n \n[1] Daniel T. Chang, “Concept-Oriented Deep Learning: Generative Concept Representations,” arXiv preprint \narXiv:1811.06622 (2018). \n[2] Daniel T. Chang, “Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax,” arXiv preprint arXiv: \n1912.05686 (2019). \n[3] Vincent Fortuin, “Priors in Bayesian Deep Learning: A Review,” arXiv preprint arXiv: 2105.06868 (2021). \n[4] TensorFlow Probability (https://www.tensorflow.org/probability) \n[5] Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex \nAlemi, Matt Hoffman, and Rif A. Saurous, “TensorFlow Distributions,“ arXiv preprint arXiv:1711.10604 (2017). \n[6] Dan Piponi, Dave Moore, and Joshua V. Dillon, “Joint Distributions for TensorFlow Probability,” arXiv preprint arXiv: \n2001.11819 (2020). \n[7] Dustin Tran, Michael W. Dusenberry, Mark van der Wilk, and Danijar Hafner, “Bayesian Layers: A Module for Neural \nNetwork Uncertainty,” arXiv preprint arXiv:1812.03973 (2019).  \n[8] Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, and Mohammed Bennamoun, \n“Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users”, ACM Comput. Surv. 1, 1 (July 2020). \n[9] Agustinus Kristiadi, Sina Daubener, and Asja Fischer, “Predictive Uncertainty Quantification with Compound Density \nNetworks,” arXiv preprint arXiv:1902.01080 (2019). \n[10] Andres R. Masegosa, Rafael Cabanas, Helge Langseth, Thomas D. Nielsen, and Antonio Salmeron, “Probabilistic \nModels with Deep Neural Networks,” arXiv preprint arXiv:1908.03442 (2019). \n[11] Hugh Salimbeni, “Deep Gaussian Processes: Advances in Models and Inference,” Ph.D. Thesis, Imperial College \nLondon (June 7, 2020). \n[12] Y. Xiong, H. J. Kim and V. Singh, \"Mixed Effects Neural Networks (MeNets) With Applications to Gaze Estimation,\" \nProc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 7743-7752 (2019). \n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2021-05-31",
  "updated": "2021-06-09"
}