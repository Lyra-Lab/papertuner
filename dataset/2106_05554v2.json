{
  "id": "http://arxiv.org/abs/2106.05554v2",
  "title": "Progressive Stage-wise Learning for Unsupervised Feature Representation Enhancement",
  "authors": [
    "Zefan Li",
    "Chenxi Liu",
    "Alan Yuille",
    "Bingbing Ni",
    "Wenjun Zhang",
    "Wen Gao"
  ],
  "abstract": "Unsupervised learning methods have recently shown their competitiveness\nagainst supervised training. Typically, these methods use a single objective to\ntrain the entire network. But one distinct advantage of unsupervised over\nsupervised learning is that the former possesses more variety and freedom in\ndesigning the objective. In this work, we explore new dimensions of\nunsupervised learning by proposing the Progressive Stage-wise Learning (PSL)\nframework. For a given unsupervised task, we design multilevel tasks and define\ndifferent learning stages for the deep network. Early learning stages are\nforced to focus on lowlevel tasks while late stages are guided to extract\ndeeper information through harder tasks. We discover that by progressive\nstage-wise learning, unsupervised feature representation can be effectively\nenhanced. Our extensive experiments show that PSL consistently improves results\nfor the leading unsupervised learning methods.",
  "text": "Progressive Stage-wise Learning for Unsupervised\nFeature Representation Enhancement\nZefan Li14*\nleezf@sjtu.edu.cn\nChenxi Liu2‚Ä†\ncxliu@waymo.com\nAlan Yuille2\nalan.l.yuille@gmail.com\nBingbing Ni14‚Ä°\nnibingbing@sjtu.edu.cn\nWenjun Zhang1\nzhangwenjun@sjtu.edu.cn\nWen Gao3\nwgao@pku.edu.cn\n1Shanghai Jiao Tong University\n2Johns Hopkins University\n3Peking University\n4MoE Key Lab of ArtiÔ¨Åcial Intelligence, AI Institute, Shanghai Jiao Tong University\nAbstract\nUnsupervised learning methods have recently shown\ntheir competitiveness against supervised training.\nTypi-\ncally, these methods use a single objective to train the en-\ntire network. But one distinct advantage of unsupervised\nover supervised learning is that the former possesses more\nvariety and freedom in designing the objective.\nIn this\nwork, we explore new dimensions of unsupervised learning\nby proposing the Progressive Stage-wise Learning (PSL)\nframework. For a given unsupervised task, we design multi-\nlevel tasks and deÔ¨Åne different learning stages for the deep\nnetwork. Early learning stages are forced to focus on low-\nlevel tasks while late stages are guided to extract deeper\ninformation through harder tasks. We discover that by pro-\ngressive stage-wise learning, unsupervised feature repre-\nsentation can be effectively enhanced. Our extensive ex-\nperiments show that PSL consistently improves results for\nthe leading unsupervised learning methods.\n1. Introduction\nAiming at learning features from label-free data, unsu-\npervised representation learning, including self-supervised\nlearning, is an important problem to study. Many efforts\nhave been made, to bridge the performance gap between\nsupervised and unsupervised learning algorithms.\nThese\nmethods can be roughly divided into two categories: i)\nhandcrafted pretext tasks, that learns data-level invariant\nfeatures (e.g., jigsaw puzzle [38], image rotation [21], im-\nage colorization [13]) and ii) contrastive visual representa-\ntion learning, which learns the similarity and dissimilarity\n*Work done while visiting Johns Hopkins University.\n‚Ä†Now at Waymo.\n‚Ä°Corresponding author.\nùëÜ!\nùëÜ\"\nùëÜ#\nUnsupervised/self-supervised Task\nPermutation Set ùíû\nJigsaw Puzzle Task ùí¢\nùí¢#\nùí¢\"\nùí¢!\nMulti-Level Task Design\nProgressive Stage-wise Learning\nFigure 1. We present the framework of the proposed Progres-\nsive Stage-wise Learning (PSL) algorithm, aiming for improv-\ning unsupervised/self-supervised task. We take the jigsaw puz-\nzle task G for example.\nWe Ô¨Årst do multi-level task partition\nG ‚Üí{G1, G2, G3} with an increased task complexity and perform\nprogressive stage-wise training for different learning stages of the\nnetwork. The black arrow denotes forward pass while colored ar-\nrow represents the backward pass of each learning stage (i.e., S1,\nS2, and S3).\nbetween data pairs [9, 10, 11, 26]. For approaches using\npretext tasks, they usually generate pseudo labels based on\nsome data attributes and learn visual features through cor-\nresponding objective functions of the pretext tasks. There-\nfore, the Ô¨Ånal performance of these approaches is highly re-\nlated to how the pretext tasks were initially designed. Most\npretext tasks are designed heuristically, limiting the qual-\nity of learned representation. Contrastive learning methods\nusually generate positive/negative sample pairs through a\nset of image transformations and learn visual representa-\ntion by bringing positive sample pairs closer while pushing\nnegative sample pairs away from each other. The design\nof the contrastive loss and the conÔ¨Åguration of image trans-\nformations are essential to the quality of the resulting net-\narXiv:2106.05554v2  [cs.CV]  11 Jun 2021\nworks. Those methods have shown great promise in the\narea of unsupervised learning, achieving state-of-the-art re-\nsults [25, 16, 40, 2]. Some recent methods show it is possi-\nble for unsupervised learned features to surpass supervised\nlearning in some downstream applications [9]. However,\nperformance gaps still exists between unsupervised and su-\npervised learning methods in most cases [23]. Therefore,\nhow to fully explore the potential of unsupervised learning\nand improve the learning quality is a valuable topic.\nInstead of designing a new pretext task or a better con-\ntrastive learning loss, we try to look into this problem from a\nnew perspective. As curriculum learning [5] suggests, when\ndealing with a complex learning target, learning things pro-\ngressively can be very useful. Indeed, as humans, we learn\nvisual concepts from easy to hard, and from elementary to\nÔ¨Åne-grained. Similarly, learning high-quality feature repre-\nsentations in an unsupervised manner is a challenging task,\nand may beneÔ¨Åt from such ideas. In this paper, we propose\nPSL, a progressive stage-wise learning framework for un-\nsupervised visual representation learning.\nAs presented in Fig 1, for a given unsupervised learn-\ning task G (e.g., the jigsaw puzzle pretext task), we Ô¨Årst\ndo multi-level task design G ‚Üí{G1, G2, G3} with an in-\ncreased task complexity. Then, a stage-wise network par-\ntition is performed to get early/mid/late stages (i.e., S1,\nS2 and S3). Each learning stage is assigned with a task,\nfollowing the principle of easy-to-hard.\nThen, a stage-\nwise training is performed. The training of lower stages\nbecome much easier as they focus on more simple tasks.\nThe feature representations learned in upper stages are of\nbetter quality because they are trained upon the learning\nexperience of former tasks.\nOur starting point is to de-\nsign PSL as a plug-in learning method, which can be ap-\nplied in any unsupervised learning scenarios under proper\nmulti-level task design. We validate the effectiveness of\nthe proposed PSL framework by evaluating our method on\nseveral unsupervised/self-supervised tasks (e.g., the jiasaw\npuzzle [38] and image rotation [21] pretext task and con-\ntrastive learning [10]) and present results on linear classiÔ¨Å-\ncation, semi-supervised learning and transfer learning.\nIn general, the contributions of this paper can be summa-\nrized as follows:\n‚Ä¢ PSL creates new dimensions for unsupervised learning\nresearch. SpeciÔ¨Åcally, this includes task series, net-\nwork partitions, and stage-wise training.\n‚Ä¢ PSL is design to be a general framework, that can\nbe applied to multiple unsupervised learning tasks be-\nlonging to either pretext tasks or contrastive learning\n(e.g., jigsaw puzzle, image rotation, and SimCLR).\n‚Ä¢ By experiments of downstream applications (e.g.,\nsemi-supervised learning, transfer learning), we show\nthat the feature representations learned by PSL consis-\ntently achieve better quality than the original unsuper-\nvised task.\n2. Related Work\nOur method falls in the area of unsupervised visual rep-\nresentation learning. We Ô¨Årst revisit two categories of un-\nsupervised learning method.\nThen, we review methods\ninvolving self-paced learning and local network training,\nwhich give inspiration in our PSL training scheme.\nHandcrafted Pretext Tasks.\nMany self-supervised meth-\nods use a handcrafted pretext task to learning visual rep-\nresentations. Typically, a pretext task involves predicting\nan explicit property of an image transformation and the\nnetwork is then trained to learn the feature representation.\nThe quality of the learned representation is highly related\nto these tasks (e.g., predicting context [14], image rota-\ntion [21], image colorization [13, 49, 30, 32, 33], jigsaw\npuzzle [38, 6, 23] and visual counting [39]). Instead of de-\nsigning a new pretext task, we propose a plug-in method\nto enhance the self-supervised learning, which can be used\ncollaboratively with many pretext tasks.\nContrastive learning.\nUnsupervised contrastive learning\nrecently attracts lots of attention, for achieving state-of-the-\nart results on ImageNet [10, 26, 9]. Typically, a contrastive\nlearning method learns feature representations by contrast-\ning positive pairs against negative pairs, which is Ô¨Årstly pro-\nposed by Hadsell et al. [25]. Then, Dosovitskiy et al. [16]\npropose to represent each instance with a parametric vector.\nLater, the concept of memory bank, which stores the infor-\nmation of instance class representation, is adopted and de-\nveloped further in many works [51, 44, 37]. Besides, there\nare many clustering-based methods [7, 8, 1, 20, 29, 47]. Our\nPSL training scheme can also Ô¨Åt into contrastive learning\nmethods, bringing improvement to the quality of the learned\nfeature representation.\nSelf-paced Learning\nMany self-paced learning methods\nsimulate the learning process of ‚Äúeasy-to-hard‚Äù [24, 19, 42,\n18]. [24] incorporates self-paced learning into deep clus-\ntering methods by controlling the number of selected data\nsamples. [19] uses a self-paced learning strategy by iden-\ntifying reliable and unreliable clusters to improve the accu-\nracy in the re-clustering step. These methods are based on\ndata-level information by deÔ¨Åning easy/hard data samples\nwhile our method focus on the task-level design.\nLocal Network Learning.\nThe end-to-end training pro-\ntocol inaugurated a new era in deep learning. Some works\njump out of traditional forward-backward training mode,\nwith inspiration from neuroscience. An early research [35]\nshows that the way of the brain processing its perceptions\nis to maximally preserve the information contained in each\nlayer.\nSeveral methods try to explore greedy layer-wise\ntraining schemes [4, 3], in which the possibility of scaling\nthis scheme to ImageNet is discussed. Later, GIM [36] ar-\ngues that with greedy self-supervised training, end-to-end\npropagation of a supervised loss is not necessary. Based\non GIM [36], LoCo [48] improves the performance of lo-\ncal contrastive learning.\nInspired by these local training\nstrategy, we propose a stage-wise training algorithm and\nimprove the performance in multiple unsupervised learning\ntask.\n3. Method\n3.1. Overview\nThis work aims to introduce an unsupervised pre-\ntraining strategy.\nMany previous methods focus on de-\nsigning handcrafted pretext tasks in a self-supervised learn-\ning setting [38, 21, 13]. Correspondingly, the results are\nhighly related to how the pretext tasks are designed. An-\nother type of work focuses on exploring the potential behind\ncontrastive learning.\nThese works concentrate on learn-\ning similar/dissimilar representations from organized simi-\nlar/dissimilar data pairs. Both kinds are trying to uncover\ninternal information of unlabeled data.\nAs unsupervised\nlearning becomes more and more important and challeng-\ning, how to take the best advantage of existing unsupervised\nlearning ways is vital. In another word, how to do unsuper-\nvised learning more effectively?\nIn this work, we provide a new dimension in enhanc-\ning the unsupervised learning representation. Inspired by\ncurriculum learning, we try to guide the neural network to\nlearning feature representations in a progressive way (e.g.,\nfrom easy tasks to hard tasks, from low-level features to\nhigh-level). To do so, we introduce our progressive stage-\nwise learning (PSL) framework for unsupervised learning.\n3.2. Progressive Stage-wise Learning\nIn this sub-section, we explain how our progressive\nstage-wise learning (PSL) works. Suppose we have a learn-\ning target G, which can be an unsupervised contrastive\nlearning task or any pretext task in the self-supervised learn-\ning setting. We use a neural network with a block-based ar-\nchitecture (designed by stacking blocks vertically, such as\nResNet [27] and Inception [43]). What we do can be sum-\nmarized into the following steps:\nMulti-level Task Design\nFirstly, for the given learning\ntarget G, we design a series of learning tasks that share a\nsimilar form (e.g. the same pretext task) but with different\ntask complexity. We sort these tasks according to their com-\nplexity: {G1, G2, G3}, where G3 is more complex than G2,\nand so is G2 compared to G1 . We use L1, L2, L3 to repre-\nsent the corresponding loss function. Instead of focusing on\nthe hardest task (e.g., G3) at the very beginning, we enable\neasier learning targets in the early learning stage. By doing\nso, we can train the neural network in a progressive learning\nmanner, which turns out to be more efÔ¨Åcient in many differ-\nent unsupervised settings. We introduce our multi-level task\ndesign for speciÔ¨Åc tasks in Sec 3.3.\nStage-wise Network Partition\nIn this step, we deÔ¨Åne dif-\nferent learning stages, which are basically determined by\nthe layer depth. We take ResNet-50 for an example. Based\non the resolution of the feature map, we divide all layers\ninto Ô¨Åve large block: B1, B2, B3, B4 and B5, where B1\nrepresents the layer conv1, B2 consists of all layers named\nconv2 x, and so on1. Further, we group every three con-\nsecutive blocks into one learning stage: Stage S1 consists of\nB1, B2 and B3; Stage S2 consists of B2, B3 and B4; Stage\nS3 consists of B3, B4 and B5. By doing so, we get three\nstages (S1, S2 and S3), representing lower, mid and upper\nlearning stages of the network. Notice that there are over-\nlaps between each two learning stages, which is discussed\nmore in Sec 3.4.\nProgressive Network Learning\nAfter we get a series of\nmulti-level tasks and a proper stage partition, we begin the\ntraining process. As shown in Fig 2, the traditional end-\nto-end training protocol, forces the whole neural network\nto learn the Ô¨Ånal target directly. Instead, we train the neural\nnetwork progressively in a stage-wise manner. More specif-\nically, we set a different learning target for each stage, fol-\nlowing the principle of easy-to-hard. We force lower layers\nof the network (e.g., layers in S1) to learn low-level fea-\ntures by solving easier tasks (e.g., G1). As the lower part\nof the network learns a good feature representation for the\neasy task, we increase the task complexity for the mid and\nupper stages by targeting at the task G2 and G3. By doing\nso, we naturally guide the network to gain better feature\nrepresentation ability as the layer goes deeper. The beneÔ¨Åts\nare three-fold. Firstly, the lower stage only has to focus on\nthe easy task, which leads to easier training. Secondly, the\nupper stage can take advantage of the lower stages through\nweight sharing when dealing with a harder task. Thirdly,\nthe backward propagation path is much shorter within each\nlearning stage compared to end to end training.\nWith-\nout gradient error accumulation, the overall training can be\nmuch more efÔ¨Åcient and effective.\nMore details of our framework are shown in Fig 2.\nThe training targets {G1, G2, G3} are assigned to stages\n1Here, all layers within the same block shares the same feature map\nresolution. The layer name conv1 and conv2 x are inherited from the\nnotation of ResNet [27].\nùêµ!\nùêµ\"\nùêµ#\nùêµ$\nùêµ\"\nùêµ#\nùêµ$\nùêµ%\nùêµ#\n‚Ñí!\n‚Ñí\"\n‚Ñí#\nStage 1\nStage 2\nStage 3\nùêµ!\nùêµ\"\nùêµ#\nùêµ$\nùêµ%\n‚Ñí\nEnd-to-End\nStage-wise Progressive Learning (ours)\nBlock\nTarget\nLoss\nForward\ndata-flow\nBackward\ndata-flow\nWeight\nSharing\nTask Partition\nùëî!\nùëî\"\nùëî#\nùëî\nFigure 2. We present the detail of the proposed Stage-wise Progressive Learning framework. In the right is the end-to-end learning scheme\nwhile we present PSL in the middle. g and {gi}3\ni=1 are projection heads, mapping the intermediate representation to the target feature\nspace. After the training is completed, we throw away the projection heads and use the backbone network for downstream tasks.\n{S1, S2, S3} respectively. Notice that different from end-\nto-end training, backward gradients only Ô¨Çow back to layers\nin the same stage.\n3.3. Multi-level Task Design Cases\nIn this sub-section, we introduce our multi-level task\ndesign for several different unsupervised/self-supervised\nlearning tasks.\n3.3.1\nMulti-level Jigsaw Puzzle\nThe jigsaw puzzle task [38] was Ô¨Årst introduced to learn vi-\nsual representations from unlabeled data, which turns out to\nbe very useful in many downstream tasks, such as detection\nand classiÔ¨Åcation. To create a jigsaw puzzle, a 225 √ó 225\npixel window is randomly cropped from an image. Then,\nthe whole window is divided into a 3 √ó 3 grid, leading to\nnine 75√ó75 pixel cells. For each cell, a 64√ó64 pixel tile is\npicked randomly. Then an index permutation is sampled\n(e.g., {1, 2, 3, 4, 5, 6, 7, 8, 9}\n‚Üí\n{3, 5, 7, 8, 4, 6, 9, 2, 1})\nfrom a pre-deÔ¨Åned permutation set C. The obtained 9 tiles\nare reordered according to the permutation. Finally, the re-\nordered tiles of the puzzle are stacked along the channels\nand fed to the neural network to predict the permutation,\nwhich is usually a classiÔ¨Åcation task. There are several fac-\ntors that inÔ¨Çuence the complexity of the jigsaw puzzle. Ac-\ncording to [38], the permutation set C inÔ¨Çuences the jigsaw\ntask from two aspects: the set cardinality of C and the ele-\nment similarity of C. Generally, the difÔ¨Åculty of the jigsaw\ntask increases as the set cardinality increases or the element\nsimilarity decreases. Under the original task setting, there\nare 9! = 362880 different permutations in total for every 9\ntiles. Therefore, it is nearly impossible to include all permu-\ntations in the permutation set. Previous methods [38, 6] usu-\nally deÔ¨Åne a permutation set with a Ô¨Åxed size (e.g., 1000) in\nadvance.\nTask\nSet‚Ä†\nCardinality\nHamming\nG1\nC1\n500\n‚àº8.0\nG2\nC2\n1000\n‚àº8.0\nG3\nC3\n2000\n‚àº8.0\nTable 1. Multi-level task design for jigsaw puzzle. Set‚Ä† denotes\nthe permutation set.\nIn this work, we design multi-level jigsaw puzzle tasks\nby changing cardinality of C. As shown in Table 1, we gen-\nerate three permutation sets C1, C2, C3 with cardinality 500,\n1000, and 2000. Notice that C1 ‚äÜC2 ‚äÜC3. We keep the av-\nerage hamming distance of each permutation set around 8.0\nso that the element similarity within each set stays in the\nsame level. The task complexity increases as permutation\ngets bigger.\nIn addition to the change of cardinality of permutation\nsets, there are other methods to control the task complexity.\nFor example, one can increase or decrease the size of the\ngrid (e.g. from 3 √ó 3 to 2 √ó 2 and 4 √ó 4). However, the\nresulting difÔ¨Åculty gap between adjacent tasks is too larger\nunder our multi-level jigsaw puzzle design. [23] reported\nthat increasing the number of patches (i.e. from 9 to 16)\ndoes not necessarily result in a higher quality representa-\ntion. Therefore, we do not adopt this scheme in this work.\n3.3.2\nMulti-level Image Rotation\nThe Image rotation task was Ô¨Årstly designed in self-\nsupervised learning [21].\nImage rotation can be consid-\nered as one of the image geometric transformations, which\nis very easy to perform. The core idea for this task is to\nuse the neural network to estimate geometric transforma-\ntion (i.e., the angle of rotation in this case). A common\npractice is to deÔ¨Åne the set of geometric transformation R\nTask\nSet‚Ä†\nCardinality\nAngle Base\nG1\nR1\n2\n180‚ó¶\nG2\nR2\n4\n90‚ó¶\nG3\nR3\n8\n45‚ó¶\nTable 2. Multi-level task design for image rotation. Set‚Ä† denotes\nthe rotation transformation set.\nTask\nSet‚Ä†\nscheme\nG1\nT1\nrandom crop\nG2\nT2\ncrop+color distortion\nG3\nT3\ncrop+color distortion+Ô¨Åltering\nTable 3. Multi-level task design for contrastive learning. Set‚Ä† de-\nnotes the transformation set.\nas all the image rotations of 90 degrees (e.g., 2d image\nrotations by 0, 9, 180 and 270 degrees).\nHere, we use\nthe size of transformation set R to control the task difÔ¨Å-\nculty. As shown in Table 2, we deÔ¨Åned three set of geo-\nmetric transformations as all image rotations of 180/90/45\ndegree, with 2/4/8 operations within each set respectively.\nFor example, R1 = {0‚ó¶, 180‚ó¶}, R2 = R1 ‚à™{90‚ó¶, 270‚ó¶}\nand R3 = R2 ‚à™{45‚ó¶, 135‚ó¶, 225‚ó¶, 315‚ó¶}.\nNotice that,\nR1 ‚äÜR2 ‚äÜR3, which indicates an increase in task com-\nplexity.\n3.3.3\nMulti-level Contrastive Learning\nContrastive learning has recently become a dominant ap-\nproach in the area of self-supervised learning [10, 26, 11].\nTypically, contrastive learning approaches learn represen-\ntations by contrasting positive pairs against negative pairs.\nFor example, SimCLR [10] makes use of multiple data aug-\nmentation operations to generate positive data pairs. Then\na based encoder network is trained to maximize the simi-\nlarity of positive data pairs meanwhile minimize the sim-\nilarity of negative data pairs using a contrastive loss. We\ndesign our multi-level contrastive tasks based on the set-\nting of SimCLR [10].\nWe control the task difÔ¨Åculty by\nmanipulating the augmentation set T .\nFor the low-level\ntask (e.g., G1), we use a simple augmentation scheme and\nwe increase the complexity of the augmentation scheme for\nhigh-level tasks (e.g., G3). Three kinds of augmentation are\nadopted: i) geometric transformation of data, such as crop-\nping and resizing; ii) appearance transformation, such as\ncolor distortion and iii) other transformation, such as Gaus-\nsian blur and Sobel Filtering. As shown in Table 3, we\nset T1 = {Random Crop} as the Ô¨Årst augmentation set for\ntask G1 and we add color distortion into the augmentation\nset ,2 for task G2. Other transformation operations are in-\ncluded in the augmentation set T3 for task G3. Notice that\nT1 ‚äÜT2 ‚äÜT3.\nAlgorithm 1 Progressive Stage-wise Learning Algorithm.\nInput: Learning Target G, Learning Loss L, backbone Net-\nwork f, projection head g1, g2 and g3, batch size N\n1: G ‚Üí{G1, G2, G3}\n‚ñ∑Multi-level Task Design\n2: L ‚Üí{L1, L2, L3}\n3: f ‚Üí{S1, S2, S3}\n‚ñ∑Stage-wise Network Partition\n4: for i ‚àà{1, 2, 3} do\n‚ñ∑Stage-wise Learning\n5:\nfor sampled minibatch {xk}N\nk=1 do\n6:\nData pre-processing for task Gi\n7:\nhk = f(xk|Si)\n‚ñ∑Forward propagation\n8:\nzk = gi(hk)\n9:\nComputer gradient with respect to Li(xk, zk)\n10:\nUpdate layers within Si\n11:\nUpdate gi\n12:\nend for\n13: end for\n14: Return f, and discard g1, g2 and g3\n3.4. Stage-wise Network Training\nAs explained in section 3.2, three learning tasks\n{G1, G2, G3} are obtained, corresponding to three losses\n{L1, L2, L3} and stages S1, S2, S3. Unlike traditional end-\nto-end learning, we adopt a local learning strategy. To be\nspeciÔ¨Åc, the gradient of each learning stage does not Ô¨Çow\nback to other stages. For example, gradients generated by\nL2 only inÔ¨Çuence layers within S2 (i.e.,B2, B3 and B4) dur-\ning the second learning stage. This allow the corresponding\nlayers focus on the current learning target, which turns out\nto be good for overall training. Our learning algorithm is\nsummarized in Algorithm 1. After we Ô¨Ånish multi-level task\ndesign and stage-wise network partition, we start progres-\nsive stage-wise learning. There are three learning stages in\ntotal. During the i-th stage, we do data pre-processing Ô¨Årst,\nwhich is determined by the speciÔ¨Åc learning task Gi. Then\nwe do forward propagation with hk = f(xk|Si) represent-\ning the output feature of stage Si. Then, hk is sent to a\ndecoder gi for further processing before applying the stage\nloss Li. Only layers within Si and the decoder gi are up-\ndated. After the training, all decoders will be removed and\nno extra computation cost is introduced in f.\nNotice that there are overlappings between each stage.\nAnother stage partition approach is to cut the encoder into\nseveral non-overlapping parts and train the whole network\nin a greedy layer-wise manner like GIM [36]. In the case\nof GIM, upper layers/stages cannot receive gradient feed-\nback from lower layers/stages. However, as the difÔ¨Åculty of\nthe multi-level task increases, the quality of the intermedi-\nate representation of lower stages has a large inÔ¨Çuence on\nthe Ô¨Ånal performance of the upper stages. Therefore, it is\nnecessary for stages to have overlapping layers, which play\na role in connecting and communicating between stages.\n4. Experiment\nIn this section, we conduct experiments to validate the\neffectiveness of the proposed Progressive Stage-wise Learn-\ning (PSL) framework. Firstly, we apply PSL on several dif-\nferent kinds of unsupervised/self-supervised learning tasks\nto evaluate the quality of the learned representation on Im-\nageNet [12]. Then, we report experiment results of down-\nstream tasks, i.e., semi-supervised and transfer learning.\n4.1. Implementation Details\nGenerally, we conduct contrast experiments on three dif-\nferent unsupervised/self-supervised learning methods. The\nbasic implementation details for each task as follows:\nJigsaw puzzle.\nWe perform multi-level task design G ‚Üí\n{G1, G2, G3} following Sec 3.3.1. Correspondingly, we use\nthree different cardinality {500, 1000, 2000} for the permu-\ntation set {C1, C2, C3}, with C1 ‚äÜC2 ‚äÜC3, representing an\nincrease in task complexity. We use ResNet-50 [27] as our\nbackbone network and do PSL training on 8-gpus. For each\ninput image, we resize the shorter side to resolution 256,\nrandomly crop a 225x225 image and apply horizontal Ô¨Çip\nwith 50% probability. For the training, we use mini-batch\nsize of 256, initial learning rate of 0.01 with the learning\nrate dropped by a factor of 10. Following [23], we use mo-\nmentum of 0.9, weight decay 1e-4 with no decay for the bias\nparameters. For each training stage of PSL, we train for 60\nepochs and use the learning rate schedule of 20/20/10/10.\nImage rotation.\nWe perform multi-level task design\nG ‚Üí{G1, G2, G3} following Sec 3.3.2. Correspondingly,\nwe set the rotation transformation set R1 ={0‚ó¶, 180‚ó¶},\nR2 ={(90 √ó i)‚ó¶|0 ‚â§i ‚â§3} and R3 ={(45 √ó i)‚ó¶|0 ‚â§i ‚â§\n7}. Notice that R1 ‚äÜR2 ‚äÜR3, representing an increase\nin task complexity. We use RevNet50 [22] as our backbone\nnetwork and do PSL training on 8-gpus2. For each input\nimage, we resize the shorter side to 256 and do the rota-\ntion transformation according to the transformation set. We\nperform a center crop on the rotated image remaining the\nresolution and then randomly crop 224x224 image. For the\ntraining, we use mini-batch size of 256, initial learning rate\nof 0.01 with the learning rate dropped by a factor of 10. For\neach training stage of PSL, we train for 60 epochs and use\nthe learning rate schedule of 20/20/10/10.\nContrastive learning.\nWe perform multi-level task de-\nsign G ‚Üí{G1, G2, G3} following Sec 3.3.3. Correspond-\ningly, we set the transformation set as: T1 ={Random\ncrop}, T2 = T1‚à™{Color Distortion} and T3 = T2‚à™\n{Gaussian Blur, Sobel Filtering}.\nNotice that T1\n‚äÜ\n2Compared with ResNet [27], RevNet [22] is more suitable in image\nrotation tasks [31]. We don‚Äôt use RevNet50w4√ó, which is reported with\nbetter performance, because scaling up model complexity is not discussed\nin this paper.\nMethod\nArch\n# Param(M)\nTop 1\nColorization [49]\nR50\n24\n39.6\nBigBiGAN [15]\nR50\n24\n56.6\nLA [51]\nR50\n24\n58.8\nNPID++ [37]\nR50\n24\n59.0\nMoCo [26]\nR50\n24\n60.6\nPIRL [37]\nR50\n24\n63.6\nCPC v2 [28]\nR50\n24\n63.8\nPCL [34]\nR50\n24\n65.9\nSwAV [9]\nR50\n24\n75.3\nJigsaw [38]\nR50\n24\n45.7\nJigsaw +PSL\nR50\n24\n50.9\nRotation [21]\nRv50w4√ó\n86\n47.3\nRotation*\nRv50\n24\n48.6\nRotation+PSL\nRv50\n24\n53.3\nSimCLR [10]\nR50\n24\n61.9\nSimCLR+PSL\nR50\n24\n64.3\nMoCov2 [11]\nR50\n24\n67.5\nMoCov2+PSL\nR50\n24\n68.1\nTable 4. ImageNet accuracy of linear classiÔ¨Åers trained on self-\nsupervised learned representations. All are reported as unsuper-\nvised pre-training on ImageNet, followed by supervised linear\nclassiÔ¨Åcation and evaluated on the ImageNet validation set. Note\nthat Rotation [21] uses R2 as the transformation set while Ro-\ntation* uses R3 as the transformation set. SimCLR results are\nobtained by 200 training epochs with batchsize 256.\nT2 ‚äÜT3, representing an increase in task complexity.\nWe use ResNet-50 as our backbone network and a 2-layer\nMLP projection head to project the representation to a\n128-dimensional space. Limited by computing resources,\nwe use mini-batch size of 256 and train for 200 epochs\n(60/70/70 epochs for each learning stage) on 8-gpus. No-\ntice that the max performance is not obtained in 200 epochs\nand 256 batch-size3, reasonable results and fair compari-\nson can still be achieved. Based on SimCLR [10], We use\nNT-Xent loss, learned in LARS optimizer with learnig rate\nof 0.3 with a cosine decay schedule without restart. Simi-\nlarly, we apply PSL on the data-augmentation part of Mo-\nCov2 [11] and get an improvement of 0.6%.\n4.2. Linear ClassiÔ¨Åcation\nIn this subsection, we verify our method by linear clas-\nsiÔ¨Åcation on ImageNet [12].\nFollowing a common pro-\ntocol, We conduct contrast experiments on three different\nunsupervised/self-supervised learning methods. Firstly, we\nperform unsupervised pre-training on ImageNet. Then, we\ntrain a supervised linear classiÔ¨Åer (a fully-connect layer fol-\nlowed by softmax).\nTable 4 summaries the single crop\n31000 epochs with batch-size 4096 is reported with the best perfor-\nmance in SimCLR [10].\nMethod\nArch\nB1\nB2\nB3\nB4\nB5\nSupervised\nR50\n11.6\n33.3\n48.7\n67.9\n75.5\nJigsaw\nR50\n12.4\n28.0\n39.9\n45.7\n34.2\nSL\nR50\n10.8\n27.2\n39.6\n45.5\n34.7\nPSL\nR50\n10.9\n27.0\n43.2\n50.9\n38.5\nPSLf\nR50\n10.8\n27.3\n43.1\n51.0\n38.2\nTable 5.\nJigsaw Puzzle task of ResNet-50 top-1 centrer-crop\naccuracy for linear classiÔ¨Åcation on the ImageNet dataset. Here\nB1 ‚àºB5 represent the blocks deÔ¨Åned in Sec 3.2. The supervised\nresults are presented for reference. Jigsaw is end-to-end trained\nwith the task G3. SL is short for stage-wise learning. In SL, we do\nstage-wise training with the jiasaw puzzle task G3 as the learning\ntarget of each stage. PSLf is a full-gradient version of PSL. More\ndiscussion can be found in ablation in Sec 4.5.\nMethod\nArch\nB1\nB2\nB3\nB4\nB5\nSupervised\nRv50\n11.7\n32.6\n47.8\n66.6\n74.3\nRotation\nRv50\n10.9\n30.1\n40.2\n48.6\n46.5\nSL\nRv50\n11.1\n29.1\n41.5\n50.7\n47.9\nPSL\nRv50\n11.3\n30.8\n42.9\n53.3\n49.5\nPSLf\nR50\n10.5\n31.1\n42.1\n52.9\n49.7\nTable 6. Image rotation task of RevNet-50 top-1 centrer-crop ac-\ncuracy for linear classiÔ¨Åcation on the ImageNet dataset.\nHere\nB1 ‚àºB5 represent the block deÔ¨Åned in Sec 3.2. The supervised\nresults are presented for reference. Rotation is end-to-end trained\nwith the task G3. SL is short for stage-wise learning. In SL, we do\nstage-wise training with the image rotation task G3 as the learning\ntarget of each stage. PSLf is a full-gradient version of PSL. More\ndiscussion can be found in ablation in Sec 4.5.\ntop-1 classiÔ¨Åcation accuracy on the ImageNet validation\nset, comparing our results with previous approaches [49,\n15, 51, 37, 26, 28, 34, 11, 9] as well as three baseline\nmethod [38, 21, 10].\nCompared with the vanilla jigsaw\npuzzle task, PSL training improves the results by 5.2%\n(45.7%‚Üí50.9%). Compared with the vanilla image ro-\ntation task, PSL training improves the results by 4.7%\n(48.6%‚Üí53.3%). As for the SimCLR contrastive learn-\ning, PSL training improves the results by 2.4% (61.9% ‚Üí\n64.3%), under the setting of 200 training epoch and batch-\nsize 256. These results indicate the PSL training can effec-\ntively improve the unsupervised learning quality. SpeciÔ¨Å-\ncally, we extract image features from Ô¨Åve different layers\n(i.e., the output of B1, B2, B3, B4 and B5) after unsuper-\nvised pre-training and train linear classiÔ¨Åers on these Ô¨Åxed\nrepresentations. For the jigsaw puzzle task, detailed results\nare shown in Table 5. Results of the image rotation task are\npresented in Table 6.\nTask\n1% labels\n10% labels\nSupervised\n48.4\n80.4\nJigsaw [38]\n45.4\n79.6\nJigsaw+PSL\n48.7\n83.5\nRotation [21]\n52.1\n82.5\nRotation+PSL\n54.8\n83.7\nTable 7. Semi-supervised learning on ImageNet.\nWe use\nResNet-50 as our backbone networks and report single-crop top-\n5 accuracy on the ImageNet validation set. All models are self-\nsupervised trained on ImageNet and Ô¨Ånetuned on 1% and 10% of\nthe ImageNet training data, following [46, 37]. The supervised\nresults are presented for reference.\n4.3. Semi-supervised Learning\nFollowing the experiment setup of [46, 37], we perform\nsemi-supervised image classiÔ¨Åcation on ImageNet to eval-\nuate the effectiveness of the pre-trained network in data-\nefÔ¨Åcient settings. We do a class-balanced data selection to\nobtain 1% and 10% of the ImageNet training data and Ô¨Åne-\ntuned the whole pre-trained network. Table 7 reports the\ntop-5 accuracy of the resulting models on the ImageNet val-\nidation set. By contrast experiments on two pretext tasks,\nthe proposed PSL training method can improve the top-5\naccuracy by a large margin. In the jigsaw puzzle task, PSL\nbrings an improvement of 3.3% and 3.9% for 1% and 10%\nlabeled data respectively. For the image rotation task, PSL\nimproves the performance by 2.7% and 1.2% respectively.\n4.4. Transfer Learning\nTo further investigate the generalization ability of our\nmethod, we conduct transfer learning experiments includ-\ning object detection on PASCAL VOC [17] and image clas-\nsiÔ¨Åcation results on three datasets. In this subsection, we\nuse ResNet-50 as our backbone network both jigsaw puzzle\nand image roration tasks. Results are reported in Table 8\nand Table 9.\n4.4.1\nObject Detection\nFollowing previous works [23, 37], we perform object de-\ntection experiments on the PASCAL VOC dataset [17] us-\ning VOC07+12 training split. We use Faster RCNN [41] ob-\nject detector and ResNet-50 C4 [27] backbone. We follow\nthe same training schedule as [23, 37] to Ô¨Ånetune all models\non VOC with BatchNorm parameters Ô¨Åxed during Ô¨Ånetun-\ning. We report our performance of {AP50, AP75, ‚àÜAP75}\nin Table 8. Compared with the vanilla pretext task, our PSL\ntraining scheme can substantially improve the three indi-\ncators, representing an enhancement of the learned feature\nrepresentation. In the jigsaw puzzle pretext task, we get an\nimprovement of {2.2, 3.8, 2.4} respectively, while in the\nTask\nObject Detection\nAPall\nAP50\nAP75\n‚àÜAP75\nSupervised\n52.6\n81.1\n57.4\n0.0\nJigsaw [38]\n48.9\n75.1\n52.9\n-4.5\nJigsaw+PSL\n51.1\n78.9\n55.3\n-2.1\nRotation [21]\n46.3\n72.5\n49.3\n-8.1\nRotation+PSL\n47.6\n74.6\n51.1\n-6.3\nTable 8. Object detection results of transfer learning on PASCAL\nVOC dataset. We report AP on the test set after Ô¨Ånetuning Faster\nR-CNN modelswith a ResNet-50 backbone, that are unsupervised\npre-trained on ImageNet. The supervised results are presented for\nreference.\nTask\nTransfer Dataset\nPASCAL\nPlaces\niNat18\nSupervised\n87.5\n51.5\n45.4\nJigsaw [38]\n64.5\n41.2\n21.3\nJigsaw+PSL\n67.2\n44.7\n24.1\nRotation [21]\n63.5\n41.9\n23.0\nRotation+PSL\n65.9\n43.0\n25.9\nTable 9. Image classiÔ¨Åcation results of transfer learning on PAS-\nCAL [17], Places [50] and iNat18 [45]. We use linear classiÔ¨Åers\non image representations obtained by self-supervised learners that\nare pre-trained on ImageNet. We report mAP on the PASCAL\ndataset and top-1 accuracy on Places and iNat18. We compare re-\nsults on the jigsaw puzzle and image rotation pretext tasks with the\nproposed PSL. The supervised results are presented for reference.\nimage rotation pretext task, PSL brings an improvement of\n{1.3, 2.1, 1.8}.\n4.4.2\nImage ClassiÔ¨Åcation on other datasets\nNext, we conduct transfer learning experiments on the im-\nage classiÔ¨Åcation task. We use models pre-trained on Ima-\ngeNet and assess the quality of learned features by training\nlinear classiÔ¨Åers on Ô¨Åxed image representations. Following\nthe setting of [23], we evaluate feature representations ex-\ntracted from Ô¨Åve intermediate blocks of the pre-trained net-\nwork, and report the best classiÔ¨Åcation results in Table 9.\nWe report transfer learning performance on PASCAL [17],\nPlaces [50] and iNat18 [45]. In PASCAL, our PSL training\nimproves the jigsaw puzzle task by 2.7%, and image rota-\ntion task by 2.4%. In Places, the PSL training improves\nthe performance by 3.5% and 1.1% while in iNat18, the\nimprovement is 2.8% and 2.9% for these two tasks respec-\ntively.\n4.5. Analysis\nAblation: Progressive Mechanism.\nHere we discuss the\neffectiveness of the progressive mechanism.\nWe design\ncomparison experiments in ImageNet linear classiÔ¨Åcation\nto compare stage-wise learning w/o progressive learning,\nnamely PSL vs.\nSL. For SL, we use the same learning\ntask (G3) in each learning stage, which means the network\nis trained to learn the hardest task for each learning stage.\nFor PSL, the task complexity increases for learning stages\nS1, S2 and S3. We present PSL vs.\nSL in Table 5 and\nTable 6. For both tasks, PSL leads to better results than SL\nwithout the progressive mechanism.\nAblation: Gradient Association.\nAs discussed in Sec-\ntion 3.4, we adopt a stage-wise training scheme with limited\na gradient association in each learning stage. SpeciÔ¨Åcally,\nthe gradient of L2 will not Ô¨Çow back to B1 and the gradi-\nent of L3 will not inÔ¨Çuence B1 and B2. We implement the\nfull gradient version of PSL where in each learning stage,\nall layers are updated without gradient restriction. We show\nthe linear classiÔ¨Åcation results on ImageNet dataset as PSLf\nin Table 5 and Table 2. From the results, we conclude that\nfull gradient training does not necessarily bring an improve-\nment in the unsupervised training performance. Therefore,\nwe set gradient restriction in each learning stage, which will\nalso reduce the computation cost during training.\nLocal Training.\nWe do not adopt a greedy block-wise\nlearning scheme like [36]. Instead, we enhance stage-wise\nthe connection by enabling graident Ô¨Çow between stages. A\nsimilar approach is adopted in LoCo [48], where gradient\nconnections are enabled in adjacent blocks. In PSL, block\nconnections are further enhanced (e.g., learning stage S3\nhave impact on B3, which is also included in learning stage\nS1). By comparing PSL and PSLf in Table 5 and 6, we\nshow that this design helps enhance the learning quality of\nvarious unsupervised tasks.\n5. Conclusion\nIn this work, we present a Progressive Stage-wise Learn-\ning (PSL) framework for unsupervised/self-supervised\nlearning. Through multi-level task design and progressive\nstage-wise training, PSL improves many mainstream unsu-\npervised methods. We provide experiments in three differ-\nent tasks (i.e., jigsaw puzzle, image rotation and contrastive\nlearning) in this paper and validate the effectiveness of PSL.\nOur future work involves exploring PSL on other tasks and\nsearching for the optimal architecture for PSL framework.\nWe hope PSL can be a universal unsupervised training ap-\nproach in enhancing the learned feature representation.\nAcknowledgements.\nThis\nwork\nwas\nsupported\nby\nNational\nScience\nFoundation\nof\nChina\n(U20B2072,\n61976137), NSFC(U19B2035), Shanghai Municipal Sci-\nence and Technology Major Project (2021SHZDZX0102),\nONR N00014-20-1-2206 and China Scholarship Council\n(NO.201906230178).\nReferences\n[1] Yuki Markus Asano, Christian Rupprecht, and Andrea\nVedaldi.\nSelf-labelling via simultaneous clustering and\nrepresentation learning. arXiv preprint arXiv:1911.05371,\n2019. 2\n[2] Philip Bachman, R Devon Hjelm, and William Buchwalter.\nLearning representations by maximizing mutual information\nacross views. In Adv. Neural Inform. Process. Syst., pages\n15535‚Äì15545, 2019. 2\n[3] Eugene Belilovsky, Michael Eickenberg, and Edouard Oy-\nallon. Greedy layerwise learning can scale to imagenet. In\nInternational Conference on Machine Learning, pages 583‚Äì\n593. PMLR, 2019. 3\n[4] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo\nLarochelle.\nGreedy layer-wise training of deep networks.\nAdv. Neural Inform. Process. Syst., 19:153‚Äì160, 2006. 3\n[5] Yoshua Bengio, J¬¥erÀÜome Louradour, Ronan Collobert, and Ja-\nson Weston. Curriculum learning. In International confer-\nence on machine learning, pages 41‚Äì48, 2009. 2\n[6] Fabio M Carlucci, Antonio D‚ÄôInnocente, Silvia Bucci, Bar-\nbara Caputo, and Tatiana Tommasi. Domain generalization\nby solving jigsaw puzzles. In IEEE Conf. Comput. Vis. Pat-\ntern Recog., pages 2229‚Äì2238, 2019. 2, 4\n[7] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In Eur. Conf. Comput. Vis., pages 132‚Äì\n149, 2018. 2\n[8] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Ar-\nmand Joulin. Unsupervised pre-training of image features\non non-curated data. In Int. Conf. Comput. Vis., pages 2959‚Äì\n2968, 2019. 2\n[9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. Adv.\nNeural Inform. Process. Syst., 33, 2020. 1, 2, 6, 7\n[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey E. Hinton. A simple framework for contrastive learn-\ning of visual representations. CoRR, abs/2002.05709, 2020.\n1, 2, 5, 6, 7\n[11] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 1, 5, 6, 7\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In IEEE Conf. Comput. Vis. Pattern Recog., pages\n248‚Äì255. Ieee, 2009. 6\n[13] Aditya Deshpande, Jason Rock, and David Forsyth. Learn-\ning large-scale automatic image colorization. In Int. Conf.\nComput. Vis., pages 567‚Äì575, 2015. 1, 2, 3\n[14] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nInt. Conf. Comput. Vis., pages 1422‚Äì1430, 2015. 2\n[15] Jeff Donahue and Karen Simonyan. Large scale adversar-\nial representation learning. In Adv. Neural Inform. Process.\nSyst., pages 10542‚Äì10552, 2019. 6, 7\n[16] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-\nmiller, and Thomas Brox. Discriminative unsupervised fea-\nture learning with convolutional neural networks. In Adv.\nNeural Inform. Process. Syst., pages 766‚Äì774, 2014. 2\n[17] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-\npher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes challenge: A retrospective. Int.\nJ. Comput. Vis., 111(1):98‚Äì136, 2015. 7, 8\n[18] Hongchang Gao and Heng Huang. Self-paced network em-\nbedding. In Proceedings of the 24th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining,\npages 1406‚Äì1415, 2018. 2\n[19] Yixiao Ge, Dapeng Chen, Feng Zhu, Rui Zhao, and Hong-\nsheng Li.\nSelf-paced contrastive learning with hybrid\nmemory for domain adaptive object re-id.\narXiv preprint\narXiv:2006.02713, 2020. 2\n[20] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\nP¬¥erez, and Matthieu Cord. Learning representations by pre-\ndicting bags of visual words. In IEEE Conf. Comput. Vis.\nPattern Recog., pages 6928‚Äì6938, 2020. 2\n[21] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. In Int. Conf. Learn. Represent., 2018. 1, 2, 3, 4, 6, 7,\n8\n[22] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\nGrosse. The reversible residual network: Backpropagation\nwithout storing activations. In Adv. Neural Inform. Process.\nSyst., pages 2214‚Äì2224, 2017. 6\n[23] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan\nMisra. Scaling and benchmarking self-supervised visual rep-\nresentation learning. In Int. Conf. Comput. Vis., pages 6391‚Äì\n6400, 2019. 2, 4, 6, 7, 8\n[24] Xifeng Guo, Xinwang Liu, En Zhu, Xinzhong Zhu, Miao-\nmiao Li, Xin Xu, and Jianping Yin. Adaptive self-paced deep\nclustering with data augmentation. IEEE Transactions on\nKnowledge and Data Engineering, 32(9):1680‚Äì1693, 2019.\n2\n[25] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-\nality reduction by learning an invariant mapping. In IEEE\nConf. Comput. Vis. Pattern Recog., volume 2, pages 1735‚Äì\n1742. IEEE, 2006. 2\n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning.\nIn IEEE Conf. Comput. Vis. Pattern\nRecog., pages 9729‚Äì9738, 2020. 1, 2, 5, 6, 7\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In IEEE Conf.\nComput. Vis. Pattern Recog., pages 770‚Äì778, 2016. 3, 6, 7\n[28] Olivier J H¬¥enaff, Aravind Srinivas, Jeffrey De Fauw, Ali\nRazavi, Carl Doersch, SM Eslami, and Aaron van den Oord.\nData-efÔ¨Åcient image recognition with contrastive predictive\ncoding. arXiv preprint arXiv:1905.09272, 2019. 6, 7\n[29] Jiabo Huang, Qi Dong, Shaogang Gong, and Xiatian Zhu.\nUnsupervised deep learning by neighbourhood discovery.\narXiv preprint arXiv:1904.11567, 2019. 2\n[30] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Let\nthere be color! joint end-to-end learning of global and local\nimage priors for automatic image colorization with simulta-\nneous classiÔ¨Åcation. ACM Trans. Graph., 35(4):1‚Äì11, 2016.\n2\n[31] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Re-\nvisiting self-supervised visual representation learning.\nIn\nIEEE Conf. Comput. Vis. Pattern Recog., pages 1920‚Äì1929,\n2019. 6\n[32] Gustav\nLarsson,\nMichael\nMaire,\nand\nGregory\nShakhnarovich.\nLearning representations for automatic\ncolorization.\nIn Eur. Conf. Comput. Vis., pages 577‚Äì593.\nSpringer, 2016. 2\n[33] Gustav\nLarsson,\nMichael\nMaire,\nand\nGregory\nShakhnarovich.\nColorization as a proxy task for visual\nunderstanding. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 6874‚Äì6883, 2017. 2\n[34] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and\nSteven CH Hoi. Prototypical contrastive learning of unsu-\npervised representations. arXiv preprint arXiv:2005.04966,\n2020. 6, 7\n[35] Ralph Linsker. Self-organization in a perceptual network.\nComputer, 21(3):105‚Äì117, 1988. 2\n[36] Sindy Lowe, Peter O‚ÄôConnor, and Bastiaan S. Veeling.\nPutting an end to end-to-end: Gradient-isolated learning of\nrepresentations. In Adv. Neural Inform. Process. Syst., pages\n3033‚Äì3045, 2019. 3, 5, 8\n[37] Ishan Misra and Laurens van der Maaten. Self-supervised\nlearning of pretext-invariant representations. In IEEE Conf.\nComput. Vis. Pattern Recog., pages 6707‚Äì6717, 2020. 2, 6,\n7\n[38] Mehdi Noroozi and Paolo Favaro.\nUnsupervised learning\nof visual representations by solving jigsaw puzzles. In Eur.\nConf. Comput. Vis., volume 9910 of Lecture Notes in Com-\nputer Science, pages 69‚Äì84, 2016. 1, 2, 3, 4, 6, 7, 8\n[39] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Rep-\nresentation learning by learning to count. In Int. Conf. Com-\nput. Vis., pages 5898‚Äì5906, 2017. 2\n[40] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 2\n[41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Adv. Neural Inform. Process. Syst.,\npages 91‚Äì99, 2015. 7\n[42] Enver Sangineto, Moin Nabi, Dubravko Culibrk, and Nicu\nSebe. Self paced deep learning for weakly supervised ob-\nject detection. IEEE transactions on pattern analysis and\nmachine intelligence, 41(3):712‚Äì725, 2018. 2\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 1‚Äì9, 2015. 3\n[44] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nCon-\ntrastive multiview coding. arXiv preprint arXiv:1906.05849,\n2019. 2\n[45] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and\nSerge Belongie. The inaturalist species classiÔ¨Åcation and de-\ntection dataset. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 8769‚Äì8778, 2018. 8\n[46] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 3733‚Äì3742, 2018. 7\n[47] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised\ndeep embedding for clustering analysis.\nIn International\nConference on Machine Learning, pages 478‚Äì487, 2016. 2\n[48] Yuwen Xiong, Mengye Ren, and Raquel Urtasun. Loco: Lo-\ncal contrastive representation learning. Adv. Neural Inform.\nProcess. Syst., 33, 2020. 3, 8\n[49] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In Eur. Conf. Comput. Vis., pages 649‚Äì\n666. Springer, 2016. 2, 6, 7\n[50] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-\nralba, and Aude Oliva.\nLearning deep features for scene\nrecognition using places database. In Adv. Neural Inform.\nProcess. Syst., pages 487‚Äì495, 2014. 8\n[51] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local\naggregation for unsupervised learning of visual embeddings.\nIn Int. Conf. Comput. Vis., pages 6002‚Äì6012, 2019. 2, 6, 7\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-06-10",
  "updated": "2021-06-11"
}