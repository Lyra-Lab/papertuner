{
  "id": "http://arxiv.org/abs/2108.03258v2",
  "title": "Memory-two strategies forming symmetric mutual reinforcement learning equilibrium in repeated prisoners' dilemma game",
  "authors": [
    "Masahiko Ueda"
  ],
  "abstract": "We investigate symmetric equilibria of mutual reinforcement learning when\nboth players alternately learn the optimal memory-two strategies against the\nopponent in the repeated prisoners' dilemma game. We provide a necessary\ncondition for memory-two deterministic strategies to form symmetric equilibria.\nWe then provide three examples of memory-two deterministic strategies which\nform symmetric mutual reinforcement learning equilibria. We also prove that\nmutual reinforcement learning equilibria formed by memory-two strategies are\nalso mutual reinforcement learning equilibria when both players use\nreinforcement learning of memory-$n$ strategies with $n>2$.",
  "text": "arXiv:2108.03258v2  [physics.soc-ph]  28 Dec 2022\nMemory-two strategies forming symmetric mutual\nreinforcement learning equilibrium in repeated\nprisoners’ dilemma game\nMasahiko Ueda\nGraduate School of Sciences and Technology for Innovation, Yamaguchi University,\nYamaguchi 753-8511, Japan\nAbstract\nWe investigate symmetric equilibria of mutual reinforcement learning when both\nplayers alternately learn the optimal memory-two strategies against the oppo-\nnent in the repeated prisoners’ dilemma game. We provide a necessary condi-\ntion for memory-two deterministic strategies to form symmetric equilibria. We\nthen provide three examples of memory-two deterministic strategies which form\nsymmetric mutual reinforcement learning equilibria. We also prove that mu-\ntual reinforcement learning equilibria formed by memory-two strategies are also\nmutual reinforcement learning equilibria when both players use reinforcement\nlearning of memory-n strategies with n > 2.\nKeywords:\nRepeated prisoners’ dilemma game; Reinforcement learning;\nMemory-two strategies\n1. Introduction\nThe prisoners’ dilemma is one of the simplest situations in which rational\nactions of individuals do not maximize social welfare [1]. Although the best\naction of each agent is defection, mutual cooperation improves the utility of\nboth agents. On the other hand, if the prisoners’ dilemma game is inﬁnitely\nrepeated, the situation changes. In fact, mutual cooperation can be realized by\nrational behavior of each agent, and this result is known as folk theorem [2].\nThe folk theorem was also extended to a stronger version that any individually\nrational payoﬀs can be realized as subgame perfect equilibria [3].\nAt the same time, it has been pointed out by experiments that the realistic\nagents like human beings are not necessarily rational, and theories of bounded\nrationality have been needed [4]. One of the mainstream is modeling agents\nby ﬁnite automata (agents with ﬁnite complexity) [5, 6, 7, 8, 9, 10, 11]. Es-\npecially, Abreu and Rubinstein found that the equilibrium payoﬀs realized by\nEmail address: m.ueda@yamaguchi-u.ac.jp (Masahiko Ueda)\nPreprint submitted to Elsevier\nDecember 29, 2022\nﬁnite automaton selection games, where players choose ﬁnite automata as their\nstrategies in repeated games so as to maximize their payoﬀs and to minimize\nthe number of states of the ﬁnite automata lexicographically, are restricted to\nsome small region in individually rational payoﬀs [8]. Kalai and Stanford proved\nthat every subgame perfect equilibrium of repeated games can be approximated\nby a subgame perfect ǫ-equilibrium of ﬁnite complexity [7]. A slightly diﬀerent\napproach from ﬁnite automata is modeling agents by ones with ﬁnite memory,\nwhich recall only a ﬁnite number past periods [12]. (Although there is distinc-\ntion between memory and recall in computer science, we use these two words\ninterchangeably.) Deterministic ﬁnite-memory strategies are contained in a class\nof ﬁnite automata. Sabourian and co-workers investigated how the folk theorem\ncan be extended to ﬁnite-memory strategies [13, 14, 15].\nAnother trend of studies of bounded rationality is modeling agents as adap-\ntive ones which gradually acquire favorable strategies. One of the most suc-\ncessful approach is evolutionary game theory, where a population of individuals\nevolves by natural selection [16]. The concept of evolutionarily stable strategy,\nwhich is interpreted as stability against mutation, succeeded in strengthening\nthe concept of Nash equilibrium. However, it was also shown that any strategy\nin the inﬁnitely repeated prisoners’ dilemma game is not an evolutionarily stable\nstrategy, and is not stable against neutral drift [17]. There are also studies of\nevolutionarily stable strategies with ﬁnite complexity [18, 19, 20]. Particularly,\nBinmore and Samuelson proposed a modiﬁed version of evolutionarily stable\nstrategy and showed that such strategies must maximize the sum of payoﬀs of\ntwo players [19]. Furthermore, many evolutionary simulations on ﬁnite-memory\nstrategies have been done for various population sizes, mutation rates, and types\nof interaction [21, 22, 23, 24, 25, 26]. Stewart and Plotkin proposed the concept\nto evolutionary robust strategies, which is an extension of evolutionarily stable\nstrategies to systems of ﬁnite population size and cannot be selectively replaced\nby any mutant strategies [27].\nLearning is another way of adaptation of human beings, and has also at-\ntracted much attention in theoretical economics [28, 29, 30], computer science\n[31], and complex systems theory [32, 33, 34, 35, 36, 37]. Many methods of learn-\ning have been proposed in game theory [38], and compared with experimental\nresults [39, 40, 41]. One of the most popular learning methods is reinforcement\nlearning [42]. In reinforcement learning, an agent gradually learns the optimal\npolicy against a stationary environment. Mutual reinforcement learning in game\ntheory is a more diﬃcult problem since the existence of multiple agents makes\nan environment nonstationary [43, 44, 45, 46, 47]. Several methods have been\nproposed for reinforcement learning with multiple agents [48].\nRecently, memory-n strategies (n periods memory strategies) with n > 1 at-\ntract much attention in computational evolutionary game theory, because longer\nmemory enables more complicated behavior [49, 50, 51, 52, 53, 54]. Especially,\nlonger memory enables us to design robust strategies against implementation\nerrors. Since agents in evolutionary biology are organisms, which are far from\nrational, it has been traditionally assumed that the length of memory of such\nagents is assumed to be short. This is in contrast to chronology of game theory\n2\nin economics, where behaviors of rational and forward-looking agents were ﬁrst\nstudied and then memory length becomes shorter in order to describe agents\nwith bounded rationality. Because rationality of realistic agents is bounded,\nshorter-memory strategies will be preferred if complexity is also considered.\nHere, we investigate mutual reinforcement learning in the repeated prisoners’\ndilemma game [1]. More explicitly, we investigate properties of equilibria formed\nby learning agents when the two agents alternately learn their optimal strategies\nagainst the opponent. In the previous study [55], it was found that, among all\ndeterministic memory-one strategies, only the Grim trigger strategy, the Win-\nStay Lose-Shift strategy, and the All-D strategy can form symmetric equilibrium\nof mutual reinforcement learning. A natural question is “How does the set of\nsuch equilibria grow as the length of memory increases?”. Such direction of\nresearch can be useful when we construct strong strategies based on memory-\none strategies, as in computational evolutionary game theory.\nFurthermore,\nwe want to understand mutual reinforcement learning equilibria in terms of\nstrategies, not equilibrium payoﬀs. However, even whether the above equilibria\nformed by memory-one strategies are still equilibria in memory-n settings or\nnot has not been known.\nIn this paper, we extend the analysis of Ref. [55] to memory-two strate-\ngies.\nFirst, we provide a necessary condition for memory-two deterministic\nstrategies to form symmetric equilibria. Then we provide three non-trivial ex-\namples of memory-two deterministic strategies which form symmetric mutual\nreinforcement learning equilibria. Furthermore, we also prove that mutual re-\ninforcement learning equilibria formed by memory-n′ strategies are also mutual\nreinforcement learning equilibria when both players use reinforcement learning\nof memory-n strategies with n > n′.\nThis paper is organized as follows. In Section 2, we introduce the repeated\nprisoners’ dilemma game with memory-n strategies, and players using reinforce-\nment learning. In Section 3, we show that the structure of the optimal strategies\nis constrained by the Bellman optimality equation. In Section 4, we introduce\nthe concepts of mutual reinforcement learning equilibrium and symmetric equi-\nlibrium. We then provide a necessary condition for memory-two deterministic\nstrategies to form symmetric equilibria.\nIn Section 5, we provide three ex-\namples of memory-two deterministic strategies which form symmetric mutual\nreinforcement learning equilibria. In Section 6, we show that mutual reinforce-\nment learning equilibria formed by memory-n′ strategies are also mutual rein-\nforcement learning equilibria when both players use reinforcement learning of\nmemory-n strategies with n > n′. Section 7 is devoted to conclusion.\n2. Model\nWe introduce the repeated prisoners’ dilemma game [43].\nThere are two\nplayers (1 and 2) in the game. Each player chooses cooperation (C) or defection\n(D) on every round. The action of player a is written as σa ∈{C, D}. We\ncollectively write σ := (σ1, σ2), and call σ an action proﬁle. We also write\nthe space of all possible action proﬁles as Ω:= {C, D}2. The payoﬀof player\n3\na ∈{1, 2} when the action proﬁle is σ is described as ra (σ). The payoﬀs in the\nprisoners’ dilemma game are given by\n(r1 (C, C) , r1 (C, D) , r1 (D, C) , r1 (D, D))\n=\n(R, S, T, P)\n(1)\n(r2 (C, C) , r2 (C, D) , r2 (D, C) , r2 (D, D))\n=\n(R, T, S, P)\n(2)\nwith T > R > P > S and 2R > T + S. The (time-independent) memory-\nn strategy (n ≥1) of player a is described as the conditional probability\nTa\n\u0010\nσa|\n\u0002\nσ(−m)\u0003n\nm=1\n\u0011\nof taking action σa when the action proﬁles in the previous\nn rounds are\n\u0002\nσ(−m)\u0003n\nm=1, together with an initial condition, where we have in-\ntroduced the notation\n\u0002\nσ(−m)\u0003n\nm=1 :=\n\u0000σ(−1), · · · , σ(−n)\u0001\nfrom newest to oldest\n[54]. (As a strategy of bounded rational players, we use ﬁnite-memory strate-\ngies, not ﬁnite automata, because the former allows strategies to be stochastic.\nAlthough stochastic strategies are allowed in our framework, we investigate only\ndeterministic strategies in this paper.) We write the length of memory of player\na as na and deﬁne n := max {n1, n2}. In this paper, we assume that n is ﬁnite.\nAssumption 1. Both players use time-independent ﬁnite-memory strategies.\nBelow we introduce the notation −a := {1, 2}\\a.\nWe consider the situation that both players learn their optimal strategies\nagainst the strategy of the opponent by reinforcement learning [42]. In rein-\nforcement learning, each player learns mapping (called policy) from the action\nproﬁles\n\u0002\nσ(−m)\u0003n\nm=1 in the previous n rounds to his/her action σ so as to max-\nimize his/her expected future reward. We write the action of player a at round\nt as σa(t). In addition, we write ra(t) := ra (σ(t)). We deﬁne the action-value\nfunction of player a as\nQa\n\u0010\nσa,\nh\nσ(−m)in\nm=1\n\u0011\n:=\nE\n\" ∞\nX\nk=0\nγkra(t + k)\n\f\f\f\f\f σa(t) = σa, [σ(s)]t−n\ns=t−1 =\nh\nσ(−m)in\nm=1\n#\n,\n(3)\nwhere γ is a discounting factor satisfying 0 ≤γ < 1. The action-value function\nQa\n\u0010\nσa,\n\u0002\nσ(−m)\u0003n\nm=1\n\u0011\nrepresents the expected future payoﬀs P∞\nk=0 γkra(t+k) of\nplayer a after round t by taking action σa when action proﬁles in the previous n\nrounds are\n\u0002\nσ(−m)\u0003n\nm=1. Therefore, the action-value function suggests the best\naction in each action proﬁle. It should be noted that the right-hand side does\nnot depend on t. Due to the property of memory-n strategies, the action-value\nfunction Qa obeys the Bellman equation against a ﬁxed strategy T−a of the\n4\nopponent:\nQa\n\u0010\nσa,\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\n+γ\nX\nσ′a\nX\nσ−a\nTa\n\u0012\nσ′\na| σ,\nh\nσ(−m)in−1\nm=1\n\u0013\nT−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\nQa\n\u0012\nσ′\na, σ,\nh\nσ(−m)in−1\nm=1\n\u0013\n.\n(4)\nSee Appendix A for the derivation of Eq. (4). It has been known that the\noptimal policy T ∗\na and the optimal action-value function Q∗\na obeys the following\nBellman optimality equation:\nQ∗\na\n\u0010\nσa,\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\n+γ\nX\nσ−a\nT−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\nmax\nˆσ\nQ∗\na\n\u0012\nˆσ, σ,\nh\nσ(−m)in−1\nm=1\n\u0013\n,\n(5)\nwith the support\nsuppT ∗\na\n\u0010\n·|\nh\nσ(−m)in\nm=1\n\u0011\n=\narg max\nσ\nQ∗\na\n\u0010\nσ,\nh\nσ(−m)in\nm=1\n\u0011\n.\n(6)\nSee Appendix B for the derivation of Eqs. (5) and (6). In other words, in the\noptimal policy against T−a, player a takes the action σa which maximizes the\nvalue of Q∗\na\n\u0010\n·,\n\u0002\nσ(−m)\u0003n\nm=1\n\u0011\nwhen the action proﬁles at the previous n rounds\nare\n\u0002\nσ(−m)\u0003n\nm=1.\nIn Q-learning, which is one of the simplest algorithms of\nreinforcement learning, it is known that values of action-value functions converge\nto the solutions of the Bellman optimality equation if all state-action pairs are\nvisited an inﬁnite number of times [42].\nWe investigate the situation that players inﬁnitely repeat the inﬁnitely-\nrepeated games and players alternately learn their optimal strategies in each\ngame, as in Ref. [55]. We write the optimal strategy and the corresponding\noptimal action-value function of player a at d-th game as T ∗(d)\na\nand Q∗(d)\na\n, re-\nspectively. Given an initial strategy T ∗(0)\n2\nof player 2, in the (2l −1)-th game\n(l ∈N), player 1 learns T ∗(2l−1)\n1\nagainst T ∗(2l−2)\n2\nby calculating Q∗(2l−1)\n1\n. In the\n2l-th game, player 2 learns T ∗(2l)\n2\nagainst T ∗(2l−1)\n1\nby calculating Q∗(2l)\n2\n. We are\ninterested in the ﬁxed points of the dynamics, that is, T ∗(∞)\na\nand Q∗(∞)\na\n.\nIn this paper, we mainly investigate situations that the support (6) contains\nonly one action, that is, strategies are deterministic. The number of determinis-\ntic memory-n strategies in the repeated prisoners’ dilemma game is 222n, which\nincreases rapidly as n increases.\n5\n3. Structure of optimal strategies\nBelow we consider only the case n = 2. The Bellman optimality equation\n(5) for n = 2 is\nQ∗\na\n\u0010\nσa, σ(−1), σ(−2)\u0011\n=\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a| σ(−1), σ(−2)\u0011\n+γ\nX\nσ−a\nT−a\n\u0010\nσ−a| σ(−1), σ(−2)\u0011\nmax\nˆσ\nQ∗\na\n\u0010\nˆσ, σ, σ(−1)\u0011\n(7)\nwith\nsuppT ∗\na\n\u0010\n·| σ(−1), σ(−2)\u0011\n=\narg max\nσ\nQ∗\na\n\u0010\nσ, σ(−1), σ(−2)\u0011\n.\n(8)\nThe number of memory-two deterministic strategies is 216, which is quite large,\nand therefore we cannot investigate all memory-two deterministic strategies as in\nthe case of memory-one deterministic strategies [55]. Instead, we ﬁrst investigate\ngeneral properties of optimal strategies.\nWe introduce the matrix representation of a strategy:\nTa (σ)\n:=\n\n\n\n\nTa (σ| (C, C), (C, C))\nTa (σ| (C, C), (C, D))\nTa (σ| (C, C), (D, C))\nTa (σ| (C, C), (D, D))\nTa (σ| (C, D), (C, C))\nTa (σ| (C, D), (C, D))\nTa (σ| (C, D), (D, C))\nTa (σ| (C, D), (D, D))\nTa (σ| (D, C), (C, C))\nTa (σ| (D, C), (C, D))\nTa (σ| (D, C), (D, C))\nTa (σ| (D, C), (D, D))\nTa (σ| (D, D), (C, C))\nTa (σ| (D, D), (C, D))\nTa (σ| (D, D), (D, C))\nTa (σ| (D, D), (D, D))\n\n\n\n.\n(9)\nFor deterministic strategies, each component in the matrix is 0 or 1.\nWe now prove the following proposition:\nProposition 1. For two diﬀerent action proﬁles σ(−2) and σ(−2)′, if\nT−a\n\u0010\nσ| σ(−1), σ(−2)\u0011\n=\nT−a\n\u0010\nσ| σ(−1), σ(−2)′\u0011\n(∀σ)\n(10)\nholds for some σ(−1), then\nT ∗\na\n\u0010\nσ| σ(−1), σ(−2)\u0011\n=\nT ∗\na\n\u0010\nσ| σ(−1), σ(−2)′\u0011\n(∀σ)\n(11)\nalso holds.\n6\nProof. For such σ(−1), because of Eq. (7), we ﬁnd\nQ∗\na\n\u0010\nσa, σ(−1), σ(−2)\u0011\n=\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a| σ(−1), σ(−2)\u0011\n+γ\nX\nσ−a\nT−a\n\u0010\nσ−a| σ(−1), σ(−2)\u0011\nmax\nˆσ\nQ∗\na\n\u0010\nˆσ, σ, σ(−1)\u0011\n=\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a| σ(−1), σ(−2)′\u0011\n+γ\nX\nσ−a\nT−a\n\u0010\nσ−a| σ(−1), σ(−2)′\u0011\nmax\nˆσ\nQ∗\na\n\u0010\nˆσ, σ, σ(−1)\u0011\n=\nQ∗\na\n\u0010\nσa, σ(−1), σ(−2)′\u0011\n(12)\nfor all σa. Since T ∗\na is determined by Eq. (8), we obtain Eq. (11).\nThis proposition implies that the structure of the matrix T ∗\na (σ) is the same\nas that of T−a(σ). For deterministic strategies, in order to see this in more\ndetail, we introduce the following sets for a ∈{1, 2} and σ(−1) ∈Ω:\nN (a)\nx\n\u0010\nσ(−1)\u0011\n:=\nn\nσ(−2) ∈Ω\n\f\f\f Ta\n\u0010\nC| σ(−1), σ(−2)\u0011\n= x\no\n,\n(13)\nwhere x ∈{0, 1}. That is, N (a)\n1\n\u0000σ(−1)\u0001\ndescribes the set of σ(−2) such that\nplayer a using strategy Ta cooperates after the history\n\u0002\nσ(−m)\u00032\nm=1.\nSimi-\nlarly, N (a)\n0\n\u0000σ(−1)\u0001\ndescribes the set of σ(−2) such that player a using strat-\negy Ta defects after the history\n\u0002\nσ(−m)\u00032\nm=1. We remark that N (a)\n0\n\u0000σ(−1)\u0001\n∪\nN (a)\n1\n\u0000σ(−1)\u0001\n= Ωfor all a and σ(−1). Then, Proposition 1 leads the following\ncorollary:\nCorollary 1. For a deterministic strategy T−a of player −a, if the optimal strat-\negy T ∗\na of player a against T−a is also deterministic, then one of the following\nfour relations holds for each σ(−1) ∈Ω:\n(a) N (a)\nx\n\u0000σ(−1)\u0001\n= N (−a)\nx\n\u0000σ(−1)\u0001\nfor all x\n(b) N (a)\nx\n\u0000σ(−1)\u0001\n= N (−a)\n1−x\n\u0000σ(−1)\u0001\nfor all x\n(c) N (a)\n0\n\u0000σ(−1)\u0001\n= N (−a)\n0\n\u0000σ(−1)\u0001\n∪N (−a)\n1\n\u0000σ(−1)\u0001\n= Ωand N (a)\n1\n\u0000σ(−1)\u0001\n= ∅\n(d) N (a)\n1\n\u0000σ(−1)\u0001\n= N (−a)\n0\n\u0000σ(−1)\u0001\n∪N (−a)\n1\n\u0000σ(−1)\u0001\n= Ωand N (a)\n0\n\u0000σ(−1)\u0001\n= ∅.\n4. Symmetric equilibrium\nIn this section, we investigate symmetric equilibrium of mutual reinforcement\nlearning.\nFirst, we introduce the notation C := D, D := C, and π (σ1, σ2) := (σ2, σ1).\nWe deﬁne the word same strategy.\n7\nDeﬁnition 1. A strategy Ta of player a is the same strategy as that of player\n−a iﬀ\nTa\n\u0010\nσ| σ(−1), σ(−2)\u0011\n=\nT−a\n\u0010\nσ| π\n\u0010\nσ(−1)\u0011\n, π\n\u0010\nσ(−2)\u0011\u0011\n(14)\nfor all σ, σ(−1) and, σ(−2).\nNext, we introduce equilibria achieved by mutual reinforcement learning.\nDeﬁnition 2. A pair of strategy T1 and T2 is a mutual reinforcement learning\nequilibrium iﬀTa is the optimal strategy against T−a for a = 1, 2.\nWe emphasize that such equilibria are deﬁned only for a time-independent\npart of ﬁnite-memory strategies Ta, although ﬁnite-memory strategies of play-\ners are generally deﬁned as a pair of a time-independent part Ta and an initial\ncondition. This deﬁnition is in contrast to that of Nash equilibrium or sub-\ngame perfect equilibrium. When some appropriate initial condition is chosen, it\nbecomes a subgame perfect equilibrium of all time-independent ﬁnite-memory\nstrategies. In addition, because the optimal policy is determined by compar-\ning the action-value functions, which are functions of ﬁnite-length histories in-\ncluding oﬀ-equilibrium path, mutual reinforcement learning equilibrium is quite\ndiﬀerent from Nash equilibrium.\nWe also remark that a mutual reinforcement learning equilibrium can be\nachieved by Q-learning if all state-action pairs are visited an inﬁnite number\nof times as mentioned above, and if an initial strategy of player 2 is appropri-\nate. Even if not all state-action pairs are visited an inﬁnite number of times,\nwe can obtain the mutual reinforcement learning equilibrium by introducing\ninﬁnitesimal error probability to the opponent’s strategy as in Ref. [55].\nFor deterministic mutual reinforcement learning equilibria, the following\nproposition is the direct consequence of Corollary 1.\nProposition 2. For mutual reinforcement learning equilibria formed by deter-\nministic strategies, one of the following two relations holds for each σ(−1) ∈Ω:\n(a) N (1)\nx\n\u0000σ(−1)\u0001\n= N (2)\nx\n\u0000σ(−1)\u0001\nfor all x\n(b) N (1)\nx\n\u0000σ(−1)\u0001\n= N (2)\n1−x\n\u0000σ(−1)\u0001\nfor all x.\nProof. According to Corollary 1, one of the four situations (a)-(d) holds for the\noptimal strategy T1 against T2. However, because T2 is also the optimal strategy\nagainst T1, the cases (c) and (d) are excluded or integrated into the case (a) or\n(b).\nFurthermore, we introduce symmetric equilibria of mutual reinforcement\nlearning.\nDeﬁnition 3. A pair of strategy T1 and T2 is a symmetric mutual reinforcement\nlearning equilibrium iﬀTa is the optimal strategy against T−a and Ta is the same\nstrategy as T−a for a = 1, 2.\n8\nIt should be noted that the deterministic optimal strategies can be written\nas\nT ∗\na\n\u0010\nσ| σ(−1), σ(−2)\u0011\n=\nI\n\u0010\nQ∗\na\n\u0010\nσ, σ(−1), σ(−2)\u0011\n> Q∗\na\n\u0010\nσ, σ(−1), σ(−2)\u0011\u0011\n,\n(15)\nwhere I(· · · ) is the indicator function that returns 1 when · · · holds and 0\notherwise. We also introduce the following sets for a ∈{1, 2} and σ(−1) ∈Ω:\n˜N (a)\nx\n\u0010\nσ(−1)\u0011\n:=\nn\nσ(−2) ∈Ω\n\f\f\f Ta\n\u0010\nC| σ(−1), π\n\u0010\nσ(−2)\u0011\u0011\n= x\no\n,\n(16)\nwhere x ∈{0, 1}. We now prove the ﬁrst main result of this paper.\nTheorem 1. For symmetric mutual reinforcement learning equilibria formed\nby deterministic strategies, the following relations must hold:\n(a) For σ(−1) ∈{(C, C), (D, D)},\nTa\n\u0010\nC| σ(−1), (C, D)\n\u0011\n=\nTa\n\u0010\nC| σ(−1), (D, C)\n\u0011\n(17)\nfor all a.\n(b) For σ(−1) ∈{(C, D), (D, C)},\nN (a)\nx\n\u0010\nπ\n\u0010\nσ(−1)\u0011\u0011\n=\n˜N (a)\nx\n\u0010\nσ(−1)\u0011\n(∀x)\n(18)\nor\nN (a)\nx\n\u0010\nπ\n\u0010\nσ(−1)\u0011\u0011\n=\n˜N (a)\n1−x\n\u0010\nσ(−1)\u0011\n(∀x)\n(19)\nholds.\nProof. For σ(−1) ∈{(C, C), (D, D)}, π\n\u0000σ(−1)\u0001\n= σ(−1) holds. Because T1 and\nT2 are the same strategies as each other,\nT1\n\u0010\nC| σ(−1), σ(−2)\u0011\n=\nT2\n\u0010\nC| σ(−1), σ(−2)\u0011\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(20)\nholds. This and Proposition 2 imply that N (1)\nx\n\u0000σ(−1)\u0001\n= N (2)\nx\n\u0000σ(−1)\u0001\n(∀x ∈\n{0, 1}) must holds. On the other hand, due to Eq. (14),\nT1\n\u0010\nC| σ(−1), (C, D)\n\u0011\n=\nT2\n\u0010\nC| σ(−1), (D, C)\n\u0011\n(21)\nT1\n\u0010\nC| σ(−1), (D, C)\n\u0011\n=\nT2\n\u0010\nC| σ(−1), (C, D)\n\u0011\n(22)\nalso hold. This means that, if (C, D) ∈N (1)\nx\n\u0000σ(−1)\u0001\n, then (D, C) ∈N (2)\nx\n\u0000π\n\u0000σ(−1)\u0001\u0001\n=\nN (2)\nx\n\u0000σ(−1)\u0001\n= N (1)\nx\n\u0000σ(−1)\u0001\n, leading to Eq. (17).\n9\nFor σ(−1) ∈{(C, D), (D, C)}, because T1 and T2 are the same strategies as\neach other,\nT2\n\u0010\nC| π\n\u0010\nσ(−1)\u0011\n, σ(−2)\u0011\n=\nT1\n\u0010\nC| σ(−1), π\n\u0010\nσ(−2)\u0011\u0011\n(23)\nholds for ∀σ(−2) ∈Ω. This means that\nN (2)\nx\n\u0010\nπ\n\u0010\nσ(−1)\u0011\u0011\n=\n˜N (1)\nx\n\u0010\nσ(−1)\u0011\n(∀x ∈{0, 1})\n(24)\nholds. On the other hand, Proposition 2 implies that\nN (1)\nx\n\u0010\nπ\n\u0010\nσ(−1)\u0011\u0011\n=\nN (2)\nx\n\u0010\nπ\n\u0010\nσ(−1)\u0011\u0011\n(∀x ∈{0, 1})\n(25)\nor\nN (1)\nx\n\u0010\nπ\n\u0010\nσ(−1)\u0011\u0011\n=\nN (2)\n1−x\n\u0010\nπ\n\u0010\nσ(−1)\u0011\u0011\n(∀x ∈{0, 1})\n(26)\nmust hold. By combining Eq. (24) and Eq. (25) or (26), we obtain Eq. (18) or\n(19).\nTheorem 1 provides a necessary condition for a deterministic strategy to\nform a symmetric mutual reinforcement learning equilibrium.\nIn particular,\nEqs. (18) and (19) imply that the second row and the third row of Ta cannot be\nindependent of each other. Explicitly, Ta must be one of the following 8 forms:\n\n\n\n\na1\nb1\nb1\na2\nc1\nc1\nc1\nc1\nd1\nd1\nd1\nd1\na3\nb2\nb2\na4\n\n\n\n,\n\n\n\n\na1\nb1\nb1\na2\nc1\nc1\nc1\n1 −c1\nd1\nd1\nd1\n1 −d1\na3\nb2\nb2\na4\n\n\n\n,\n\n\n\n\na1\nb1\nb1\na2\nc1\nc1\n1 −c1\nc1\nd1\n1 −d1\nd1\nd1\na3\nb2\nb2\na4\n\n\n\n,\n\n\n\n\na1\nb1\nb1\na2\nc1\nc1\n1 −c1\n1 −c1\nd1\n1 −d1\nd1\n1 −d1\na3\nb2\nb2\na4\n\n\n\n,\n\n\n\n\na1\nb1\nb1\na2\nc1\n1 −c1\nc1\nc1\nd1\nd1\n1 −d1\nd1\na3\nb2\nb2\na4\n\n\n\n,\n\n\n\n\na1\nb1\nb1\na2\nc1\n1 −c1\nc1\n1 −c1\nd1\nd1\n1 −d1\n1 −d1\na3\nb2\nb2\na4\n\n\n\n,\n\n\n\n\na1\nb1\nb1\na2\nc1\n1 −c1\n1 −c1\nc1\nd1\n1 −d1\n1 −d1\nd1\na3\nb2\nb2\na4\n\n\n\n,\n\n\n\n\na1\nb1\nb1\na2\nc1\n1 −c1\n1 −c1\n1 −c1\nd1\n1 −d1\n1 −d1\n1 −d1\na3\nb2\nb2\na4\n\n\n\n,(27)\nwhere ai, bj, c1, d1 ∈{0, 1} (i = 1, 2, 3, 4), (j = 1, 2) independently. For example,\nthe Tit-for-Tat-anti-Tit-for-Tat (TFT-ATFT) strategy [50]\nT1 (C)\n=\n\n\n\n\n1\n1\n1\n1\n0\n0\n0\n1\n0\n1\n0\n1\n1\n0\n1\n0\n\n\n\n,\n(28)\n10\ndoes not satisfy the condition of Theorem 1, and therefore it cannot form a\nsymmetric mutual reinforcement learning equilibrium. However, there are still\nmany memory-two strategies which satisfy the necessary condition, and further\nreﬁnement will be needed.\n5. Examples of deterministic strategies forming symmetric equilib-\nrium\nIn this section, we provide three examples of memory-two deterministic\nstrategies forming symmetric mutual reinforcement learning equilibrium. For\nconvenience, we deﬁne the following sixteen quantities:\nq1\n:=\nR + γ max\nσ\nQ∗\n1 (σ, (C, C), (C, C))\n(29)\nq2\n:=\nT + γ max\nσ\nQ∗\n1 (σ, (D, C), (C, C))\n(30)\nq3\n:=\nS + γ max\nσ\nQ∗\n1 (σ, (C, D), (C, C))\n(31)\nq4\n:=\nP + γ max\nσ\nQ∗\n1 (σ, (D, D), (C, C))\n(32)\nq5\n:=\nR + γ max\nσ\nQ∗\n1 (σ, (C, C), (C, D))\n(33)\nq6\n:=\nT + γ max\nσ\nQ∗\n1 (σ, (D, C), (C, D))\n(34)\nq7\n:=\nS + γ max\nσ\nQ∗\n1 (σ, (C, D), (C, D))\n(35)\nq8\n:=\nP + γ max\nσ\nQ∗\n1 (σ, (D, D), (C, D))\n(36)\nq9\n:=\nR + γ max\nσ\nQ∗\n1 (σ, (C, C), (D, C))\n(37)\nq10\n:=\nT + γ max\nσ\nQ∗\n1 (σ, (D, C), (D, C))\n(38)\nq11\n:=\nS + γ max\nσ\nQ∗\n1 (σ, (C, D), (D, C))\n(39)\nq12\n:=\nP + γ max\nσ\nQ∗\n1 (σ, (D, D), (D, C))\n(40)\nq13\n:=\nR + γ max\nσ\nQ∗\n1 (σ, (C, C), (D, D))\n(41)\nq14\n:=\nT + γ max\nσ\nQ∗\n1 (σ, (D, C), (D, D))\n(42)\nq15\n:=\nS + γ max\nσ\nQ∗\n1 (σ, (C, D), (D, D))\n(43)\nq16\n:=\nP + γ max\nσ\nQ∗\n1 (σ, (D, D), (D, D))\n(44)\n11\nThe Bellman optimality equation for symmetric equilibrium is\nQ∗\n1\n\u0010\nσ1, σ(−1), σ(−2)\u0011\n=\nX\nσ2\n\u001a\nr1 (σ) + max\nˆσ\nQ∗\n1\n\u0010\nˆσ, σ, σ(−1)\u0011\u001b\n×I\n\u0010\nQ∗\n1\n\u0010\nσ2, π\n\u0010\nσ(−1)\u0011\n, π\n\u0010\nσ(−2)\u0011\u0011\n> Q∗\n1\n\u0010\nσ2, π\n\u0010\nσ(−1)\u0011\n, π\n\u0010\nσ(−2)\u0011\u0011\u0011\n.\n(45)\nWe want to ﬁnd solutions of this equation.\n5.1. Delayed Grim trigger strategy\nThe ﬁrst candidate of the solution of Eq. (45) is\nT1 (C) = T2 (C)\n=\n\n\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n\n.\n(46)\nWe can easily check that this strategy satisﬁes the necessary condition for sym-\nmetric equilibrium in Theorem 1. Because this strategy is a variant of the Grim\ntrigger strategy [56]\nT1 (C)\n=\n\n\n\n\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n(47)\nbut uses only information at the second last action proﬁle, the strategy (46) can\nbe called as delayed Grim strategy.\nTheorem 2. A pair of the strategy (46) forms a symmetric mutual reinforce-\nment learning equilibrium if γ >\nq\nT −R\nT −P .\nProof. The Bellman optimality equation against the strategy (46) is\nQ∗\n1 (C, (C, C), (C, C))\n=\nq1\n(48)\nQ∗\n1 (D, (C, C), (C, C))\n=\nq2\n(49)\nQ∗\n1\n\u0010\nC, (C, C), σ(−2)\u0011\n=\nq3\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(50)\nQ∗\n1\n\u0010\nD, (C, C), σ(−2)\u0011\n=\nq4\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(51)\nQ∗\n1 (C, (C, D), (C, C))\n=\nq5\n(52)\nQ∗\n1 (D, (C, D), (C, C))\n=\nq6\n(53)\nQ∗\n1\n\u0010\nC, (C, D), σ(−2)\u0011\n=\nq7\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(54)\nQ∗\n1\n\u0010\nD, (C, D), σ(−2)\u0011\n=\nq8\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(55)\n12\nQ∗\n1 (C, (D, C), (C, C))\n=\nq9\n(56)\nQ∗\n1 (D, (D, C), (C, C))\n=\nq10\n(57)\nQ∗\n1\n\u0010\nC, (D, C), σ(−2)\u0011\n=\nq11\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(58)\nQ∗\n1\n\u0010\nD, (D, C), σ(−2)\u0011\n=\nq12\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(59)\nQ∗\n1 (C, (D, D), (C, C))\n=\nq13\n(60)\nQ∗\n1 (D, (D, D), (C, C))\n=\nq14\n(61)\nQ∗\n1\n\u0010\nC, (D, D), σ(−2)\u0011\n=\nq15\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(62)\nQ∗\n1\n\u0010\nD, (D, D), σ(−2)\u0011\n=\nq16\n\u0010\nσ(−2) ̸= (C, C)\n\u0011\n(63)\nwith the self-consistency condition\nq1\n>\nq2\nq3\n<\nq4\nq5\n>\nq6\nq7\n<\nq8\nq9\n>\nq10\nq11\n<\nq12\nq13\n>\nq14\nq15\n<\nq16.\n(64)\nThe solution is\nq1\n=\n1\n1 −γ R\n(65)\nq2\n=\nT +\nγ\n1 −γ2 R +\nγ2\n1 −γ2 P\n(66)\nq3\n=\nS +\nγ\n1 −γ2 R +\nγ2\n1 −γ2 P\n(67)\nq4\n=\n1\n1 −γ2 P +\nγ\n1 −γ2 R\n(68)\nq5 = q9 = q13\n=\n1\n1 −γ2 R +\nγ\n1 −γ2 P\n(69)\nq6 = q10 = q14\n=\nT +\nγ\n1 −γ P\n(70)\nq7 = q11 = q15\n=\nS +\nγ\n1 −γ P\n(71)\nq8 = q12 = q16\n=\n1\n1 −γ P.\n(72)\n13\nFor these solution, the inequalities (64) are satisﬁed if\nγ\n>\nr\nT −R\nT −P .\n(73)\nWe remark that the condition (73) is more strict than the condition that\nGrim forms a symmetric equilibrium [55]: γ > T −R\nT −P .\n5.2. Delayed Win-Stay Lose-Shift strategy\nThe second candidate of the solution of Eq. (45) is\nT1 (C) = T2 (C)\n=\n\n\n\n\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n0\n1\n\n\n\n.\n(74)\nWe can easily check that this strategy satisﬁes the necessary condition for sym-\nmetric equilibrium in Theorem 1.\nBecause this strategy is a variant of the\nWin-Stay Lose-Shift (WSLS) strategy [22]\nT1 (C)\n=\n\n\n\n\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n\n\n\n\n(75)\nbut uses only information at the second last action proﬁle, the strategy (74) can\nbe called as delayed WSLS strategy.\nTheorem 3. When 2R > T + P holds, a pair of the strategy (74) forms a\nsymmetric mutual reinforcement learning equilibrium if γ >\nq\nT −R\nR−P .\nProof. The Bellman optimality equation against the strategy (74) is\nQ∗\n1\n\u0010\nC, (C, C), σ(−2)\u0011\n=\nq1\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(76)\nQ∗\n1\n\u0010\nD, (C, C), σ(−2)\u0011\n=\nq2\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(77)\nQ∗\n1\n\u0010\nC, (C, C), σ(−2)\u0011\n=\nq3\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(78)\nQ∗\n1\n\u0010\nD, (C, C), σ(−2)\u0011\n=\nq4\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(79)\nQ∗\n1\n\u0010\nC, (C, D), σ(−2)\u0011\n=\nq5\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(80)\nQ∗\n1\n\u0010\nD, (C, D), σ(−2)\u0011\n=\nq6\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(81)\nQ∗\n1\n\u0010\nC, (C, D), σ(−2)\u0011\n=\nq7\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(82)\nQ∗\n1\n\u0010\nD, (C, D), σ(−2)\u0011\n=\nq8\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(83)\n14\nQ∗\n1\n\u0010\nC, (D, C), σ(−2)\u0011\n=\nq9\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(84)\nQ∗\n1\n\u0010\nD, (D, C), σ(−2)\u0011\n=\nq10\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(85)\nQ∗\n1\n\u0010\nC, (D, C), σ(−2)\u0011\n=\nq11\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(86)\nQ∗\n1\n\u0010\nD, (D, C), σ(−2)\u0011\n=\nq12\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(87)\nQ∗\n1\n\u0010\nC, (D, D), σ(−2)\u0011\n=\nq13\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(88)\nQ∗\n1\n\u0010\nD, (D, D), σ(−2)\u0011\n=\nq14\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(89)\nQ∗\n1\n\u0010\nC, (D, D), σ(−2)\u0011\n=\nq15\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(90)\nQ∗\n1\n\u0010\nD, (D, D), σ(−2)\u0011\n=\nq16\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(91)\nwith the self-consistency condition\nq1\n>\nq2\nq3\n<\nq4\nq5\n>\nq6\nq7\n<\nq8\nq9\n>\nq10\nq11\n<\nq12\nq13\n>\nq14\nq15\n<\nq16.\n(92)\nThe solution is\nq1 = q13\n=\n1\n1 −γ R\n(93)\nq2 = q14\n=\nT + γR + γ2P +\nγ3\n1 −γ R\n(94)\nq3 = q15\n=\nS + γR + γ2P +\nγ3\n1 −γ R\n(95)\nq4 = q16\n=\nP +\nγ\n1 −γ R\n(96)\nq5 = q9\n=\nR + γP +\nγ2\n1 −γ R\n(97)\nq6 = q10\n=\nT + γP + γ2P +\nγ3\n1 −γ R\n(98)\nq7 = q11\n=\nS + γP + γ2P +\nγ3\n1 −γ R\n(99)\nq8 = q12\n=\nP + γP +\nγ2\n1 −γ R.\n(100)\n15\nFor these solution, the inequalities (92) are satisﬁed if\nγ\n>\nr\nT −R\nR −P .\n(101)\nIt should be noted that such γ < 1 exists only if 2R > T + P.\nWe remark that the condition (101) is more strict than the condition that\nWSLS forms a symmetric equilibrium [55]: γ > T −R\nR−P .\n5.3. All-or-None strategy\nThe third example of the solution of Eq. (45) is the All-or-None strategy\nAON2 [51]:\nT1 (C) = T2 (C)\n=\n\n\n\n\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n.\n(102)\nWe can easily check that this strategy satisﬁes the necessary condition for sym-\nmetric equilibrium in Theorem 1. It has been known that AON2 forms subgame\nperfect equilibrium [51]. A similar strategy as AON2 was also observed in nu-\nmerical simulation of evolution of cooperation [57].\nTheorem 4. When 3R −2P −T > 0 and 2R −3P + S > 0 hold, a pair of the\nstrategy (102) forms a symmetric mutual reinforcement learning equilibrium if\nγ\n>\nmax\n(\n1\n2\n r\n4T −3R −P\nR −P\n−1\n!\n, 1\n2\n r\nR + 3P −4S\nR −P\n−1\n!)\n.(103)\nProof. The Bellman optimality equation against the strategy (102) is\nQ∗\n1\n\u0010\nC, (C, C), σ(−2)\u0011\n=\nq1\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(104)\nQ∗\n1\n\u0010\nD, (C, C), σ(−2)\u0011\n=\nq2\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(105)\nQ∗\n1\n\u0010\nC, (C, C), σ(−2)\u0011\n=\nq3\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(106)\nQ∗\n1\n\u0010\nD, (C, C), σ(−2)\u0011\n=\nq4\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(107)\nQ∗\n1\n\u0010\nC, (C, D), σ(−2)\u0011\n=\nq7\n\u0010\nσ(−2) ∈Ω\n\u0011\n(108)\nQ∗\n1\n\u0010\nD, (C, D), σ(−2)\u0011\n=\nq8\n\u0010\nσ(−2) ∈Ω\n\u0011\n(109)\nQ∗\n1\n\u0010\nC, (D, C), σ(−2)\u0011\n=\nq11\n\u0010\nσ(−2) ∈Ω\n\u0011\n(110)\nQ∗\n1\n\u0010\nD, (D, C), σ(−2)\u0011\n=\nq12\n\u0010\nσ(−2) ∈Ω\n\u0011\n(111)\n16\nQ∗\n1\n\u0010\nC, (D, D), σ(−2)\u0011\n=\nq13\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(112)\nQ∗\n1\n\u0010\nD, (D, D), σ(−2)\u0011\n=\nq14\n\u0010\nσ(−2) ∈{(C, C), (D, D)}\n\u0011\n(113)\nQ∗\n1\n\u0010\nC, (D, D), σ(−2)\u0011\n=\nq15\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(114)\nQ∗\n1\n\u0010\nD, (D, D), σ(−2)\u0011\n=\nq16\n\u0010\nσ(−2) ∈{(C, D), (D, C)}\n\u0011\n(115)\nwith the self-consistency condition\nq1\n>\nq2\nq3\n<\nq4\nq7\n<\nq8\nq11\n<\nq12\nq13\n>\nq14\nq15\n<\nq16.\n(116)\nThe solution is\nq1 = q13\n=\n1\n1 −γ R\n(117)\nq2 = q14\n=\nT + γP + γ2P +\nγ3\n1 −γ R\n(118)\nq3 = q7 = q11 = q15\n=\nS + γP + γ2P +\nγ3\n1 −γ R\n(119)\nq4 = q16\n=\nP +\nγ\n1 −γ R\n(120)\nq8 = q12\n=\nP + γP +\nγ2\n1 −γ R.\n(121)\nFor these solution, the inequalities (116) are satisﬁed if\n(R −P)γ2 + (R −P)γ −(T −R)\n>\n0\n(122)\nand\n(R −P)γ2 + (R −P)γ −(P −S)\n>\n0,\n(123)\nwhich are equivalent to Eq. (103) for γ ≥0. It should be noted that such γ < 1\nexists only if 3R −2P −T > 0 and 2R −3P + S > 0.\n6. Optimality in longer memory\nIn previous sections, we investigated symmetric equilibrium of mutual re-\ninforcement learning when both players use memory-two strategies, and ob-\ntained three examples of deterministic strategies forming symmetric equilib-\nrium. A natural question is “Do these strategies forming symmetric equilibrium\n17\nin memory-two reinforcement learning also form symmetric equilibrium of mu-\ntual reinforcement learning of longer memory strategies?”. In this section, we\nshow that the answer to this question is “yes”.\nWe ﬁrst prove the following theorem.\nTheorem 5. Let T−a be a memory-n′ strategy of player −a. Let T ∗\na be the op-\ntimal strategy of player a against T−a when player a use reinforcement learning\nof memory-n′ strategies. When player a use reinforcement learning of memory-\nn strategies with n > n′ to obtain the optimal strategy ˇT ∗\na against T−a, then\nˇT ∗\na = T ∗\na .\nProof. When player −a use memory-n′ strategy and player a use memory-n re-\ninforcement learning with n > n′, the Bellman optimality equation (5) becomes\nQ∗\na\n\u0010\nσa,\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\nσ−a\nra (σ) T−a\n\u0012\nσ−a|\nh\nσ(−m)in′\nm=1\n\u0013\n+γ\nX\nσ−a\nT−a\n\u0012\nσ−a|\nh\nσ(−m)in′\nm=1\n\u0013\nmax\nˆσ\nQ∗\na\n\u0012\nˆσ, σ,\nh\nσ(−m)in−1\nm=1\n\u0013\n.\n(124)\nThen, we ﬁnd that the right-hand side does not depend on σ(−n), and therefore\nQ∗\na\n\u0010\nσa,\nh\nσ(−m)in\nm=1\n\u0011\n=\nQ∗\na\n\u0012\nσa,\nh\nσ(−m)in−1\nm=1\n\u0013\n(125)\nThen, the Bellman optimality equation becomes\nQ∗\na\n\u0012\nσa,\nh\nσ(−m)in−1\nm=1\n\u0013\n=\nX\nσ−a\nra (σ) T−a\n\u0012\nσ−a|\nh\nσ(−m)in′\nm=1\n\u0013\n+γ\nX\nσ−a\nT−a\n\u0012\nσ−a|\nh\nσ(−m)in′\nm=1\n\u0013\nmax\nˆσ\nQ∗\na\n\u0012\nˆσ, σ,\nh\nσ(−m)in−2\nm=1\n\u0013\n.\n(126)\nBy repeating the same argument until the length of memory decreases to n′, we\nﬁnd that\nQ∗\na\n\u0010\nσa,\nh\nσ(−m)in\nm=1\n\u0011\n=\nQ∗\na\n\u0012\nσa,\nh\nσ(−m)in′\nm=1\n\u0013\n,\n(127)\nwhich implies that ˇT ∗\na = T ∗\na .\n18\nThat is, when the opponent −a uses a memory-two strategy T−a, and player\na learns the optimal memory-n strategy with n ≥2 against T−a, then, such op-\ntimal strategy is memory-two. Similarly, when the opponent −a uses a memory-\none strategy, and player a learns the optimal memory-n strategy with n ≥1,\nthen, such optimal strategy is memory-one.\nThis theorem results in the following corollary.\nCorollary 2. A mutual reinforcement learning equilibrium obtained by memory-\nn′ reinforcement learning is also a mutual reinforcement learning equilibrium\nobtained by memory-n reinforcement learning with n > n′.\nTherefore, the strategies (46) and (74) in the previous section also form mu-\ntual reinforcement learning equilibria even if players use memory-n reinforce-\nment learning with n > 2. Similarly, the (memory-one) Grim strategy and the\n(memory-one) WSLS strategy still form mutual reinforcement learning equilib-\nria even if players use memory-two reinforcement learning, since it has been\nknown that Grim and WSLS form memory-one mutual reinforcement learning\nequilibria, respectively [55]. We remark that this property is similar to that\nof Nash equilibrium in ﬁnite automaton selection game, which claims that two\nautomata must have an equal number of states in equilibria [8].\n7. Conclusion\nIn this paper, we investigated symmetric equilibrium of mutual reinforce-\nment learning when both players use memory-two deterministic strategies in the\nrepeated prisoners’ dilemma game. First, we ﬁnd that the structure of the op-\ntimal strategies is constrained by the Bellman optimality equation (Proposition\n1). Then, we ﬁnd a necessary condition for deterministic symmetric equilib-\nrium of mutual reinforcement learning (Theorem 1). Furthermore, we provided\nthree examples of memory-two deterministic strategies which form symmetric\nmutual reinforcement learning equilibrium, some of which can be regarded as\nvariants of the Grim strategy and the WSLS strategy (Theorem 2, Theorem 3\nand Theorem 4). Finally, we proved that mutual reinforcement learning equilib-\nria achieved by memory-two strategies are also mutual reinforcement learning\nequilibria when both players use reinforcement learning of memory-n strategies\nwith n > 2 (Theorem 5).\nWe want to investigate whether other symmetric mutual reinforcement learn-\ning equilibria of deterministic memory-two strategies exist or not in future. For\nsuch purpose, novel methods are needed, because the number of strategies is\nquite large. Furthermore, extension of our analysis to (i) asymmetric equilib-\nrium and (ii) mixed strategies is also a subject of future work. Ultimately, if\nwe would develop some method to ﬁnd all equilibria in all length of memory n,\nanalysis of mutual reinforcement learning equilibria is completed.\n19\nAcknowledgement\nWe thank Genki Ichinose and Mashiho Mukaida for useful discussions. This\nstudy was supported by JSPS KAKENHI Grant Number JP20K19884 and In-\namori Research Grants.\nAppendix A. Derivation of Eq. (4)\nFirst we introduce the notation\nT\n\u0010\nσ|\nh\nσ(−m)in\nm=1\n\u0011\n:=\n2\nY\na=1\nTa\n\u0010\nσa|\nh\nσ(−m)in\nm=1\n\u0011\n.\n(A.1)\nWe remark that the joint probability distribution of the action proﬁles σ(µ),\n· · · , σ(0) given\n\u0002\nσ(−m)\u0003n\nm=1 is described as\nP\n\u0010\nσ(µ), · · · , σ(0) \f\f\f\nh\nσ(−m)in\nm=1\n\u0011\n=\nµ\nY\ns=0\nT\n\u0010\nσ(s)\f\f\f\nh\nσ(s−m)in\nm=1\n\u0011\n. (A.2)\n20\nThe action-value function (3) is rewritten as\nQa\n\u0010\nσ(0)\na ,\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\n[σ(s)]\n∞\ns=1\nX\nσ(0)\n−a\n∞\nX\nk=0\nγkra\n\u0010\nσ(k)\u0011 ( ∞\nY\ns=1\nT\n\u0010\nσ(s)\f\f\f\nh\nσ(s−m)in\nm=1\n\u0011)\n×T−a\n\u0010\nσ(0)\n−a\n\f\f\f\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\n[σ(s)]\n∞\ns=1\nX\nσ(0)\n−a\n\"\nra\n\u0010\nσ(0)\u0011\n+ γ\n∞\nX\nk=0\nγkra\n\u0010\nσ(k+1)\u0011#\n×\n( ∞\nY\ns=1\nT\n\u0010\nσ(s)\f\f\f\nh\nσ(s−m)in\nm=1\n\u0011)\nT−a\n\u0010\nσ(0)\n−a\n\f\f\f\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\nσ(0)\n−a\nra\n\u0010\nσ(0)\u0011\nT−a\n\u0010\nσ(0)\n−a\n\f\f\f\nh\nσ(−m)in\nm=1\n\u0011\n+γ\nX\n[σ(s)]\n∞\ns=1\nX\nσ(0)\n−a\n∞\nX\nk=0\nγkra\n\u0010\nσ(k+1)\u0011 ( ∞\nY\ns=2\nT\n\u0010\nσ(s)\f\f\f\nh\nσ(s−m)in\nm=1\n\u0011)\n×T−a\n\u0010\nσ(1)\n−a\n\f\f\f\nh\nσ(1−m)in\nm=1\n\u0011\nTa\n\u0010\nσ(1)\na\n\f\f\f\nh\nσ(1−m)in\nm=1\n\u0011\nT−a\n\u0010\nσ(0)\n−a\n\f\f\f\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\nσ(0)\n−a\nra\n\u0010\nσ(0)\u0011\nT−a\n\u0010\nσ(0)\n−a\n\f\f\f\nh\nσ(−m)in\nm=1\n\u0011\n+γ\nX\nσ(1)\na\nX\nσ(0)\n−a\nQa\n\u0010\nσ(1)\na ,\nh\nσ(1−m)in\nm=1\n\u0011\nTa\n\u0010\nσ(1)\na\n\f\f\f\nh\nσ(1−m)in\nm=1\n\u0011\nT−a\n\u0010\nσ(0)\n−a\n\f\f\f\nh\nσ(−m)in\nm=1\n\u0011\n,\n(A.3)\nwhich is Eq. (4).\n21\nAppendix B. Derivation of Eqs. (5) and (6)\nWe deﬁne Q∗\na as the optimal value of Qa, which is obtained by choosing\noptimal policy T ∗\na . Then, Q∗\na obeys\nQ∗\na\n\u0010\nσa,\nh\nσ(−m)in\nm=1\n\u0011\n=\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\n+γ\nX\nσ′a\nX\nσ−a\nT ∗\na\n\u0012\nσ′\na| σ,\nh\nσ(−m)in−1\nm=1\n\u0013\nT−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\nQ∗\na\n\u0012\nσ′\na, σ,\nh\nσ(−m)in−1\nm=1\n\u0013\n≤\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\n+γ\nX\nσ′a\nX\nσ−a\nT ∗\na\n\u0012\nσ′\na| σ,\nh\nσ(−m)in−1\nm=1\n\u0013\nT−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\nmax\nˆσ\nQ∗\na\n\u0012\nˆσ, σ,\nh\nσ(−m)in−1\nm=1\n\u0013\n=\nX\nσ−a\nra (σ) T−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\n+γ\nX\nσ−a\nT−a\n\u0010\nσ−a|\nh\nσ(−m)in\nm=1\n\u0011\nmax\nˆσ\nQ∗\na\n\u0012\nˆσ, σ,\nh\nσ(−m)in−1\nm=1\n\u0013\n.\n(B.1)\nThe equality in the third line holds when\nsuppT ∗\na\n\u0010\n·|\nh\nσ(−m)in\nm=1\n\u0011\n=\narg max\nσ\nQ∗\na\n\u0010\nσ,\nh\nσ(−m)in\nm=1\n\u0011\n,\n(B.2)\nwhich is Eq. (6).\nReferences\nReferences\n[1] A. Rapoport, A. M. Chammah, C. J. Orwant, Prisoner’s dilemma: A study\nin conﬂict and cooperation, Vol. 165, University of Michigan Press, 1965.\n[2] G. J. Mailath, L. Samuelson, Repeated games and reputations: long-run\nrelationships, Oxford University Press, 2006.\n[3] D. Fudenberg, E. Maskin, The folk theorem in repeated games with dis-\ncounting or with incomplete information, Econometrica: Journal of the\nEconometric Society 54 (3) (1986) 533–554.\n[4] R. J. Aumann, Rationality and bounded rationality, Games and Economic\nBehavior 21 (1997) 2–14.\n[5] A. Neyman, Bounded complexity justiﬁes cooperation in the ﬁnitely re-\npeated prisoners’ dilemma, Economics Letters 19 (3) (1985) 227–229.\n22\n[6] A. Rubinstein, Finite automata play the repeated prisoner’s dilemma, Jour-\nnal of Economic Theory 39 (1) (1986) 83–96.\n[7] E. Kalai, W. Stanford, Finite rationality and interpersonal complexity in\nrepeated games, Econometrica: Journal of the Econometric Society 56 (2)\n(1988) 397–410.\n[8] D. Abreu, A. Rubinstein, The structure of nash equilibrium in repeated\ngames with ﬁnite automata, Econometrica: Journal of the Econometric\nSociety 56 (6) (1988) 1259–1281.\n[9] J. S. Banks, R. K. Sundaram, Repeated games, ﬁnite automata, and com-\nplexity, Games and Economic Behavior 2 (2) (1990) 97–117.\n[10] E. Ben-Porath, Repeated games with ﬁnite automata, Journal of Economic\nTheory 59 (1) (1993) 17–32.\n[11] A. Neyman, Finitely repeated games with ﬁnite automata, Mathematics of\nOperations Research 23 (3) (1998) 513–552.\n[12] E. Lehrer, Repeated games with stationary bounded recall strategies, Jour-\nnal of Economic Theory 46 (1) (1988) 130–144.\n[13] H. Sabourian, Repeated games with m-period bounded memory (pure\nstrategies), Journal of Mathematical Economics 30 (1) (1998) 1–35.\n[14] M. Barlo, G. Carmona, H. Sabourian, Repeated games with one-memory,\nJournal of Economic Theory 144 (1) (2009) 312–336.\n[15] M. Barlo, G. Carmona, H. Sabourian, Bounded memory folk theorem, Jour-\nnal of Economic Theory 163 (2016) 728–774.\n[16] J. M. Smith, G. R. Price, The logic of animal conﬂict, Nature 246 (5427)\n(1973) 15.\n[17] R. Boyd, J. P. Lorberbaum, No pure strategy is evolutionarily stable in the\nrepeated prisoner’s dilemma game, Nature 327 (6117) (1987) 58–59.\n[18] D. Fudenberg, E. Maskin, Evolution and cooperation in noisy repeated\ngames, The American Economic Review 80 (2) (1990) 274–279.\n[19] K. G. Binmore, L. Samuelson, Evolutionary stability in repeated games\nplayed by ﬁnite automata, Journal of Economic Theory 57 (2) (1992) 278–\n305.\n[20] M. A. Nowak, K. Sigmund, E. El-Sedy, Automata, repeated games and\nnoise, Journal of Mathematical Biology 33 (7) (1995) 703–722.\n[21] M. A. Nowak, K. Sigmund, Tit for tat in heterogeneous populations, Nature\n355 (6357) (1992) 250–253.\n23\n[22] M. Nowak, K. Sigmund, A strategy of win-stay, lose-shift that outperforms\ntit-for-tat in the prisoner’s dilemma game, Nature 364 (6432) (1993) 56–58.\n[23] C. T. Bergstrom, M. Lachmann, The red king eﬀect: when the slowest\nrunner wins the coevolutionary race, Proceedings of the National Academy\nof Sciences 100 (2) (2003) 593–598.\n[24] L. A. Imhof, D. Fudenberg, M. A. Nowak, Evolutionary cycles of coop-\neration and defection, Proceedings of the National Academy of Sciences\n102 (31) (2005) 10797–10800.\n[25] A. Szolnoki, M. Perc, G. Szab´o, Phase diagrams for three-strategy evolu-\ntionary prisoner’s dilemma games on regular graphs, Physical Review E\n80 (5) (2009) 056104.\n[26] M. Perc, J. G´omez-Gardenes, A. Szolnoki, L. M. Flor´ıa, Y. Moreno, Evolu-\ntionary dynamics of group interactions on structured populations: a review,\nJournal of the Royal Society Interface 10 (80) (2013) 20120997.\n[27] A. J. Stewart, J. B. Plotkin, From extortion to generosity, evolution in\nthe iterated prisoner’s dilemma, Proceedings of the National Academy of\nSciences 110 (38) (2013) 15348–15353.\n[28] E. Kalai, E. Lehrer, Rational learning leads to nash equilibrium, Econo-\nmetrica: Journal of the Econometric Society (1993) 1019–1045.\n[29] D. Fudenberg, D. K. Levine, Steady state learning and nash equilibrium,\nEconometrica: Journal of the Econometric Society (1993) 547–573.\n[30] S. Hart, A. Mas-Colell, A simple adaptive procedure leading to correlated\nequilibrium, Econometrica 68 (5) (2000) 1127–1150.\n[31] T. Roughgarden, Twenty lectures on algorithmic game theory, Cambridge\nUniversity Press, 2016.\n[32] D. Kraines, V. Kraines, Pavlov and the prisoner’s dilemma, Theory and\nDecision 26 (1) (1989) 47–79.\n[33] G. I. Bischi, A. Naimzada, Global analysis of a dynamic duopoly game\nwith bounded rationality, in: Advances in dynamic games and applications,\nSpringer, 2000, pp. 361–385.\n[34] Y. Sato, E. Akiyama, J. D. Farmer, Chaos in learning a simple two-person\ngame, Proceedings of the National Academy of Sciences 99 (7) (2002) 4748–\n4751.\n[35] M. W. Macy, A. Flache, Learning dynamics in social dilemmas, Proceedings\nof the National Academy of Sciences 99 (suppl 3) (2002) 7229–7236.\n24\n[36] N. Masuda, M. Nakamura, Numerical analysis of a reinforcement learning\nmodel with the dynamic aspiration level in the iterated prisoner’s dilemma,\nJournal of Theoretical Biology 278 (1) (2011) 55–62.\n[37] T. Galla, J. D. Farmer, Complex dynamics in learning complicated games,\nProceedings of the National Academy of Sciences 110 (4) (2013) 1232–1236.\n[38] D. Fudenberg, D. K. Levine, The theory of learning in games, Vol. 2, MIT\nPress, 1998.\n[39] I. Erev, A. E. Roth, Predicting how people play games: Reinforcement\nlearning in experimental games with unique, mixed strategy equilibria,\nAmerican Economic Review 88 (4) (1998) 848–881.\n[40] P. Dal B´o, Cooperation under the shadow of the future: experimental\nevidence from inﬁnitely repeated games, American Economic Review 95 (5)\n(2005) 1591–1604.\n[41] P. Dal B´o, G. R. Fr´echette, The evolution of cooperation in inﬁnitely re-\npeated games: Experimental evidence, American Economic Review 101 (1)\n(2011) 411–29.\n[42] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT\nPress, 2018.\n[43] A. Rapoport, Optimal policies for the prisoner’s dilemma., Psychological\nReview 74 (2) (1967) 136.\n[44] T. W. Sandholm, R. H. Crites, Multiagent reinforcement learning in the\niterated prisoner’s dilemma, Biosystems 37 (1-2) (1996) 147–166.\n[45] J. Hu, M. P. Wellman, Nash q-learning for general-sum stochastic games,\nJournal of Machine Learning Research 4 (Nov) (2003) 1039–1069.\n[46] M. Harper, V. Knight, M. Jones, G. Koutsovoulos, N. E. Glynatsi,\nO. Campbell, Reinforcement learning produces dominant strategies for the\niterated prisoner’s dilemma, PloS One 12 (12) (2017) e0188046.\n[47] W. Barfuss, J. F. Donges, J. Kurths, Deterministic limit of temporal diﬀer-\nence reinforcement learning for stochastic games, Physical Review E 99 (4)\n(2019) 043305.\n[48] L. Busoniu, R. Babuska, B. De Schutter, A comprehensive survey of mul-\ntiagent reinforcement learning, IEEE Transactions on Systems, Man, and\nCybernetics, Part C (Applications and Reviews) 38 (2) (2008) 156–172.\n[49] J. Li, G. Kendall, The eﬀect of memory size on the evolutionary stability\nof strategies in iterated prisoner’s dilemma, IEEE Transactions on Evolu-\ntionary Computation 18 (6) (2013) 819–826.\n25\n[50] S. D. Yi, S. K. Baek, J.-K. Choi, Combination with anti-tit-for-tat remedies\nproblems of tit-for-tat, Journal of Theoretical Biology 412 (2017) 1–7.\n[51] C. Hilbe, L. A. Martinez-Vaquero, K. Chatterjee, M. A. Nowak, Memory-\nn strategies of direct reciprocity, Proceedings of the National Academy of\nSciences 114 (18) (2017) 4715–4720.\n[52] Y. Murase, S. K. Baek, Seven rules to avoid the tragedy of the commons,\nJournal of Theoretical Biology 449 (2018) 94–102.\n[53] Y. Murase, S. K. Baek, Five rules for friendly rivalry in direct reciprocity,\nScientiﬁc Reports 10 (2020) 16904.\n[54] M. Ueda, Memory-two zero-determinant strategies in repeated games,\nRoyal Society Open Science 8 (5) (2021) 202186.\n[55] Y. Usui, M. Ueda, Symmetric equilibrium of multi-agent reinforcement\nlearning in repeated prisoner’s dilemma, Applied Mathematics and Com-\nputation 409 (2021) 126370.\n[56] J. W. Friedman, A non-cooperative equilibrium for supergames, The Re-\nview of Economic Studies 38 (1) (1971) 1–12.\n[57] C. Hauert, H. G. Schuster, Eﬀects of increasing the number of players and\nmemory size in the iterated prisoner’s dilemma: a numerical approach,\nProceedings of the Royal Society of London. Series B: Biological Sciences\n264 (1381) (1997) 513–519.\n26\n",
  "categories": [
    "physics.soc-ph",
    "cs.GT"
  ],
  "published": "2021-08-05",
  "updated": "2022-12-28"
}