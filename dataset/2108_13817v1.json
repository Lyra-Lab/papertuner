{
  "id": "http://arxiv.org/abs/2108.13817v1",
  "title": "Unsupervised Open-Domain Question Answering",
  "authors": [
    "Pengfei Zhu",
    "Xiaoguang Li",
    "Jian Li",
    "Hai Zhao"
  ],
  "abstract": "Open-domain Question Answering (ODQA) has achieved significant results in\nterms of supervised learning manner. However, data annotation cannot also be\nirresistible for its huge demand in an open domain. Though unsupervised QA or\nunsupervised Machine Reading Comprehension (MRC) has been tried more or less,\nunsupervised ODQA has not been touched according to our best knowledge. This\npaper thus pioneers the work of unsupervised ODQA by formally introducing the\ntask and proposing a series of key data construction methods. Our exploration\nin this work inspiringly shows unsupervised ODQA can reach up to 86%\nperformance of supervised ones.",
  "text": "Unsupervised Open-Domain Question Answering\nPengfei Zhu\nShanghai JiaoTong\nUniversity\nzhupf97@sjtu\n.edu.cn\nXiaoguang Li\nHuawei Noah’s\nArk Lab\nlixiaoguang11@\nhuawei.com\nJian Li\nHuawei Noah’s\nArk Lab\nlijian703@\nhuawei.com\nHai Zhao\nShanghai JiaoTong\nUniversity\nzhaohai@cs.sjtu\n.edu.cn\nAbstract\nOpen-domain Question Answering (ODQA)\nhas achieved signiﬁcant results in terms of su-\npervised learning manner. However, data an-\nnotation cannot also be irresistible for its huge\ndemand in an open domain. Though unsuper-\nvised QA or unsupervised Machine Reading\nComprehension (MRC) has been tried more\nor less, unsupervised ODQA has not been\ntouched according to our best knowledge. This\npaper thus pioneers the work of unsupervised\nODQA by formally introducing the task and\nproposing a series of key data construction\nmethods. Our exploration in this work inspir-\ningly shows unsupervised ODQA can reach up\nto 86% performance of supervised ones.\n1\nIntroduction\nOpen-domain Question Answering (ODQA) is the\ntask of answering questions based on information\nfrom a very large collection of documents which\nhas a variety of topics (Chen and Yih, 2020). Un-\nlike Machine Reading Comprehension (MRC) task\nwhere a passage containing evidences and answers\nis provided for each question, ODQA is more chal-\nlenging as there is no such supporting passage\nbeforehand. ODQA systems need to go through\na large collection of passages such as the whole\nWikipedia to ﬁnd the correct answer.\nWhile tremendous progress on ODQA have been\nmade based on pretrained language models such\nas BERT (Devlin et al., 2019), ELECTRA (Clark\net al., 2020), and T5 (Raffel et al., 2020), ﬁne-\ntuning these language models requires large-scale\nlabeled data, i.e., passage-question-answer triples\n(Lewis et al., 2019). Apparently, it is costly and\npractically infeasible to manually create a dataset\nfor every new domain.\nThough previous studies which have made at-\ntempts in unsupervised MRC like (Lewis et al.,\n2019; Li et al., 2020; Fabbri et al., 2020; Hong\net al., 2020; Perez et al., 2020), as to our best\nknowledge, no such manner of attempts have been\nmade in terms of ODQA. Thus in this paper, for\nthe ﬁrst time, we tackle the ODQA setting with-\nout human-annotated data, which we term Unsu-\npervised ODQA (UODQA). Concretely, our set-\nting is: starting from an automatically generated\nquestion or question-like sentence, we employ a\nlexical-based retriever like BM25 to retrieve posi-\ntive passages that contain the answer and negative\npassages without the answer, from the Wikipedia\ncorpus. Together with these, we can effectively\ntrain a question answering model which can handle\nmultiple passages.\nUnlike UQA which the supporting passage is cer-\ntain for each question, UODQA needs to construct\nmore than one passages through retrieval-based\nmethod and solve a multi-passage MRC problem.\nAs the ﬁrst attempt to tackle UODQA, we pro-\npose a series of methods about how to synthesize\ndata from a set of selected natural sentences and\ncompare end-to-end performance, and ﬁnally we\nachieve up to 86% performance of previous SOTA\nsupervised method on three ODQA benchmarks.\n2\nRelated Work\n2.1\nOpen-Domain Question Answering\nOpen-domain Question Answering (ODQA) needs\nto ﬁnd answers from tremendous open domain in-\nformation such as Wikipedia or web pages. Tra-\nditional methods usually adopt retriever-reader\narchitecture (Karpukhin et al., 2020), which is to\nﬁrst retrieve relevant documents and then generate\nanswers based on these retrieved documents, which\nis the main focus of our paper. Besides, there is\nalso end-to-end method (Guu et al., 2020), but it\ncosts too much computation resources to be widely\napplied. The improvements of retriever (Izacard\nand Grave, 2020a) and reader (Izacard and Grave,\n2020b) are both critical for the overall performance,\nand there is still huge room for improvements.\narXiv:2108.13817v1  [cs.CL]  31 Aug 2021\n2.2\nUnsupervised Question Answering\nUnsupervised Question Answering (UQA) is to\nalleviate the problem of huge cost of data annota-\ntion. Generally speaking, the key issue of UQA\naims at automatically generating context-question-\nanswer triples from publicly available data. (Lewis\net al., 2019) uses an Unsupervised Neural Machine\nTranslation method to generate questions. (Fabbri\net al., 2020) proposes to retrieve relevant sentence\nthat contains the answer and reform the sentence\nwith template-based rules to generate questions.\n(Li et al., 2020) proposes an iterative process to\nreﬁne the generated questions turn by turn. (Hong\net al., 2020) proposes paraphrasing and trimming\nmethods to respectively solve the problem of word\noverlap and unanswerable generated questions.\n3\nTask Deﬁnition\nFor UODQA task, there is no limitation to use or\nconstruct data for training, only development and\ntest sets from ODQA benchmark have to be used\nfor evaluation and fair comparison. Therefore we\nwill focus on data construction hereafter.\nBased on a speciﬁc corpus C, several <\nQ, P +, P −, A > triples are constructed, For each\nconstructed example, Q denotes the question, A\ndenotes the answer, P + denotes multiple positive\npassages that contains the answer supporting the\nquestion to solve, P −denotes multiple negative\npassages that do not contain the answer, and help\nmake the model learn to distinguish distracting in-\nformation. To train a reader model, these data are\nleveraged to learn a function F(Q, P +, P −) = A.\n4\nMethod\n4.1\nData Construction\nThe procedure is shown in Figure 1. The purpose\nis to automatically construct < Q, P +, P −, A >\ntriples for model training. Obviously, the quality\nof constructed data decides whether a model can\nbe trained well.\nFirstly, based on some speciﬁc corpus C, we se-\nlect a set of sentences to construct < Q, A >. In\nODQA, most of the questions are factoid. Many\nworks show that knowing Named Entities (NEs)\nmay help construct < Q, A > pairs (Glass et al.,\n2020; Guu et al., 2020), thus a good practice is to\nselect NEs as A in the constructed data. Meanwhile\nthe sentence where the NE is from is Q after the\nselected NE is masked. The constructed Q is thus\na pseudo-question, or conceptually deﬁned as In-\nformation Request. Note that there is an obvious\nexpression difference between real questions and\nour constructed information request, in which the\nformer usually starts with an interrogative and the\nlatter does not so but is just plain statement. De-\nspite of this syntactic difference, both questions\nmay relate to the same concerned factoids and be\nused for effective model training. Meanwhile, to\nalign the train data and test data, many works aim\nto do question generation based on constructed\npseudo-questions, which is to reform statements\nas the expression of real questions. However, this\nprocedure also introduces noises.\nWhen selecting sentences to generate Q from\ncorpus C, previous works on UQA do not set con-\nstraints (Lewis et al., 2019; Li et al., 2020; Hong\net al., 2020), which brings no guarantee the con-\nstructed information request is reasonable or an-\nswerable. Such none-guarantee will become much\nmore severe in UODQA. Basically, the source sen-\ntences selected need to have complete information.\nFor example, “It was instead produced by Norro\nWilson, although the album still had a distinguish-\nable country pop sound.” is ambiguous because of\ntoo many coreferences. Moreover, when selecting\nphrases as A, it needs to be answerable based on the\nconstructed information request. For example, in\nsentence “Yao Ming played for the Houston Rock-\nets of the National Basketball Association (NBA).”\nif the phrase “Yao Ming” is selected as A, the con-\nstructed “\nplayed for the Houston Rockets of\nthe National Basketball Association (NBA).” is not\ncertain and answerable.\nTo obtain < Q, A > pairs with higher quality,\nwe use sentences from the dataset in (Elsahar et al.,\n2019), which is an alignment corpus for WikiData\nand natural language. Each sentence is aligned with\na Subject-Predicate-Object triple, and we select the\nobject as A.\nTo obtain P + and P −, our model retrieves doc-\numents from knowledge source C∗, and selects the\ndocuments containing A as positive P + otherwise\nnegative. This heuristic can not assure enough ev-\nidences but still make the model learn reasoning.\nTo ﬁlter the trivial cases of P + that the context text\nsurrounding the answer has too much overlap with\nthat in the Q so the answer can be simply gener-\nated based on shortcuts, we set a window size n and\ncheck the left and right n-gram of the selected A.\nThus, < Q, P +, P −, A > triples are constructed.\nQuestion-answer pairs\nKnowledge source\nCorpus\n⊕\nReader\nAnswer\nQuestion: Ronald Joseph \"Ron\" Walker AC CBE is a former Lord \nMayor of [MASK] and prominent Australian businessman.\nMelbourne\nAnswer: Melbourne\nRelevant passages\nPositive\nNegative\nRetrieve\nSelect\n...... Walker received ...... In 1975 Walker was named as \nVictoria's Outstanding Man of the Year during his term \nas Lord Mayor of Melbourne; Victorian Father ......\n...... Ron Sharpe (businessman) Ronald \nDouglas Sharpe, OAM (born 1950) is an \nAustralian businessman ......\nFigure 1: Our proposed method for synthesizing data and training.\nDataset\ntrain\ndev\ntest\nNatural Questions\n79,168\n8,757\n3,610\nWebQuestions\n3,417\n361\n2,032\nTriviaQA\n78,785\n8,837\n11,313\nTable 1: Data statistics of three datasets.\n4.2\nModel Training\nFollowing previous common practice in ODQA,\nwe adopt retriever-reader architecture to perform\nUODQA. BM25 serves as retrieval metric in an\nunsupervised manner. After retrieving top K pas-\nsages, a reader receives the questions and passages\nas input to output an answer. Following (Izacard\nand Grave, 2020b), we adopt a generative reader\nbased on T5 (Raffel et al., 2020).\n5\nExperiments and Analysis\n5.1\nEvaluation Settings\nThe evaluation metrics are Exact Match (EM). For\nEM, if generated answer hits any one of the la-\nbeled list of possible golden answer, the sample\nis positive. The accuracy of EM is calculated as\nEM = N+/N where N+ is number of positive\nsamples and N is number of all evaluated samples.\nWe evaluate our model on three ODQA bench-\nmarks, Natural Questions (Kwiatkowski et al.,\n2019), WebQuestions (Berant et al., 2013) and\nTriviaQA (Joshi et al., 2017). Statistics are shown\nin Table 1. The train/dev/test split follows (Lee\net al., 2019). As this is an unsupervised ODQA\ntask, we discard training set and only adopt de-\nvelopment and test sets for evaluation. Natural\nQuestions(NQ) is commonly used ODQA bench-\nmark which was constructed according to real\nGoogle search engine queries. The answers are\nshort phrases from Wikipedia articles containing\nvarious NEs. WebQuestions(WQ) contains ques-\ntions collected from Google Suggest API, and the\nanswers are all entities from the structured knowl-\nedge base (Freebase). TriviaQA(TQA) consists of\ntrivia questions from online collection.\n5.2\nImplementation Details\nWe adopt dataset from (Elsahar et al., 2019) and\nselect sentences that have only one object to con-\nstruct question-answer pairs. Sentences containing\ncharacter number more than 250 or less than 50\nare discarded. The object is used as answer and we\nuse the token [MASK] to replace the answer in the\nsentence as the question. Following (Karpukhin\net al., 2020), the version of Wikipedia corpus we\nuse is Dec. 20, 2018 dump and we split the whole\ncorpus into 100-word segments as units of retrieval.\nFor retrieving documents, we use Apache Lucene\n1 to build index and perform BM25 retrieval. To\nﬁlter P + using n-gram, we use n as 3. We ﬁrst\nretrieve top 100 documents, and select the top 40\ndocuments to construct the input for reader. If none\nof top 40 documents contains the answer, we fur-\nther ﬁnd top 41-100 documents that contains the\nanswer, and replace the 40th document with it, oth-\nerwise this sample is discarded. Finally, we obtain\n844,100 samples to train for 2-3 day using 8 V100s.\nWe implement the reader following (Izacard and\nGrave, 2020b) and perform training using learning\nrate of 1e-4, batch size of 256 and the number\nof concatenated passages each sample is 40. The\nmodel size setting we use is T5-base. We save and\nevaluate the model checkpoint every 500 training\nsteps and stop training if the performance does\nnot increase any more in 5 evaluations, and the\ncheckpoint of best EM score is selected.\n5.3\nResults\nAs shown in Table 2, we perform experiments\nbased on four kinds of settings, to study to what\n1https://lucene.apache.org/\nWQ\nNQ\nTQA\nsup.\nDPR(2020)\n42.4\n41.5\n57.9\nFiD(2020b)\n-\n51.4\n67.6\nunsup.\nRandSent10\n12.01\n15.90\n40.39\nRandEnt10\n15.01\n18.14\n45.38\nQuesGen10\n10.43\n13.88\n43.44\nOurMethod10\n16.14\n18.73\n46.64\nOurMethod50\n18.60\n20.69\n50.23\nTable 2: Experimental results EM on test set of three\ndatasets. “sup.” denotes supervised methods and “un-\nsup.” denotes unsupervised methods. The subscript de-\nnotes number of passages to input the reader.\nSetting // Question // Answer\nRandSent // He had 16 caps for Italy, from\n1995 to [MASK], scoring 5 tries, 25 points in\naggregate. // 1999\nRandEnt // [MASK] stiphra is a species of sea\nsnail, a marine gastropod mollusk in the family\nRaphitomidae. // Daphnella\nQuesGen // What is a multi-state state high-\nway in the New England region of the United\nStates, running across the southern parts of\nNew Hampshire, Vermont and Maine, and\nnumbered, owned, and maintained by each\nof those states? // Route 9\nOurMethod // Ronald Joseph “Ron” Walker\nAC CBE is a former Lord Mayor of [MASK]\nand prominent Australian businessman. // Mel-\nbourne\nTable 3: Examples of the settings for comparison ex-\nperiments.\nextent the quality of constructed training data af-\nfects performance. RandSent means we select ran-\ndom sentences from Wikipedia articles and NEs to\nconstruct question-answer pairs. RandEnt means\nwe use data from (Elsahar et al., 2019) and se-\nlect a random NE from each sentence as answer.\nThis expands the scope of types of answers and\nmakes the model learn more diversiﬁed knowledge.\nQuesGen means we perform a question genera-\ntion step after obtaining the constructed data based\non our method. This makes the expression of the\npseudo-question more close to the real question\nand makes the model learn a question answering\nmanner better, but it may hurts the reasonibility\nof constructed questions because of the noise in-\ntroduced by question generation methods. Some\nexamples are shown in Table 3.\nAs shown in Table 2, improving the quality of\nconstructed training data improves the performance\nby a large margin. Moreover, the performance gap\nbetween supervised and unsupervised method in-\ndicates that the task is very challenging and shows\nhuge space for improvements.\n5.4\nAnalysis and Discussion\nThere are three main factors of the differene among\ndifferent settings, reasonability, answerability and\nstrategy to select answer span. Reasonability in-\ndicates to what extent the question conforms to\nthe expression of natural language, answerability\nmeans whether the sentence describes an fact with\naccurate meanings and has enough information for\ndeducing answer, and strategy to select answer span\ndetermines what knowledge the model learns.\nFor the setting of RandSent, because random\nsentences are usually ambiguous and lack enough\nevidence to infer corresponding answer, the an-\nswerability is very weak. For the setting of Ran-\ndEnt, though the original sentence contains com-\nplete information and expresses accurate fact, the\nrandomly masked NE may be too difﬁcult to de-\nduce. Compared with this, our strategy that only\nselects the object as answer is better, because in the\nstructure of Subject-Predicate-Object, the object\nusually can be accurately deduced. QuesGen at-\ntempts to reform the expression of question to make\nit more like a real question, however, it also intro-\nduces noise to do harm to the performance. For the\npurpose of implementing unsupervised manner, we\nonly adopt simple rule-based question-generation\nmethod, which applies semantic role labeling on\nthe original question and selects one of the parsed\nargument as answer, and converts the order and\ntense of the sentence to reform it as a question ex-\npression. It indicates that if the question generation\nmethod introduces too much noise and hurts the rea-\nsonability of sentences too much, it is even worse\nthan doing nothing and maintaining the statement\nexpression of original constructed information re-\nquest sentences.\n6\nConclusion\nIn this paper, we ﬁrst propose the task of Unsu-\npervised Open-domain Question Answering, and\nexplore to what extent it can perform based on our\nsuggested data construction methods. We compare\nseveral strategies for synthesizing better data, as a\nresult achieve up to 86% performance of previous\nsupervised method. We hope this work inspires a\nnew line of ODQA in the future and helps build\nmore practical readers for real use.\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013.\nSemantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural lan-\nguage processing, pages 1533–1544.\nDanqi Chen and Wen-tau Yih. 2020.\nOpen-domain\nquestion answering. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: Tutorial Abstracts, pages 34–37.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020. Pre-training trans-\nformers as energy-based cloze models. In EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Elena Simperl,\nand Frederique Laforest. 2019. T-rex: A large scale\nalignment of natural language with knowledge base\ntriples.\nAlexander R Fabbri,\nPatrick Ng,\nZhiguo Wang,\nRamesh Nallapati, and Bing Xiang. 2020. Template-\nbased question generation from retrieved sentences\nfor improved unsupervised question answering.\narXiv preprint arXiv:2004.11892.\nMichael Glass, Alﬁo Gliozzo, Rishav Chakravarti, An-\nthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Di-\nnesh Garg, and Avi Sil. 2020. Span selection pre-\ntraining for question answering.\nIn Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2773–2782, On-\nline. Association for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\narXiv\npreprint arXiv:2002.08909.\nGiwon Hong, Junmo Kang, Doyeon Lim, and Sung-\nHyon Myaeng. 2020. Handling anomalies of syn-\nthetic questions in unsupervised question answering.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 3441–3448.\nGautier Izacard and Edouard Grave. 2020a. Distilling\nknowledge from reader to retriever for question an-\nswering. arXiv preprint arXiv:2012.04584.\nGautier Izacard and Edouard Grave. 2020b.\nLever-\naging passage retrieval with generative models for\nopen domain question answering.\narXiv preprint\narXiv:2007.01282.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering.\narXiv preprint\narXiv:2004.04906.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics,\n7:453–466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6086–6096, Florence,\nItaly. Association for Computational Linguistics.\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel.\n2019.\nUnsupervised question answering by cloze\ntranslation. arXiv preprint arXiv:1906.04980.\nZhongli Li, Wenhui Wang, Li Dong, Furu Wei, and\nKe Xu. 2020.\nHarvesting and reﬁning question-\nanswer pairs for unsupervised qa.\narXiv preprint\narXiv:2005.02925.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised ques-\ntion decomposition for question answering.\nIn\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8864–8880, Online. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020.\nExploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-08-31",
  "updated": "2021-08-31"
}