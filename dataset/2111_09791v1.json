{
  "id": "http://arxiv.org/abs/2111.09791v1",
  "title": "Supporting Undotted Arabic with Pre-trained Language Models",
  "authors": [
    "Aviad Rom",
    "Kfir Bar"
  ],
  "abstract": "We observe a recent behaviour on social media, in which users intentionally\nremove consonantal dots from Arabic letters, in order to bypass\ncontent-classification algorithms. Content classification is typically done by\nfine-tuning pre-trained language models, which have been recently employed by\nmany natural-language-processing applications. In this work we study the effect\nof applying pre-trained Arabic language models on \"undotted\" Arabic texts. We\nsuggest several ways of supporting undotted texts with pre-trained models,\nwithout additional training, and measure their performance on two Arabic\nnatural-language-processing downstream tasks. The results are encouraging; in\none of the tasks our method shows nearly perfect performance.",
  "text": "Supporting Undotted Arabic with Pre-trained Language Models\nAviad Rom and Kﬁr Bar\nThe Data Science Institute, Reichman University, Herzliya, Israel\n{aviad.rom,kfir.bar}@post.idc.ac.il\nAbstract\nWe observe a recent behaviour on social me-\ndia, in which users intentionally remove con-\nsonantal dots from Arabic letters, in order to\nbypass content-classiﬁcation algorithms. Con-\ntent classiﬁcation is typically done by ﬁne-\ntuning pre-trained language models, which\nhave been recently employed by many natural-\nlanguage-processing applications. In this work\nwe study the effect of applying pre-trained\nArabic language models on “undotted” Ara-\nbic texts.\nWe suggest several ways of sup-\nporting undotted texts with pre-trained models,\nwithout additional training, and measure their\nperformance on two Arabic natural-language-\nprocessing downstream tasks. The results are\nencouraging; in one of the tasks our method\nshows nearly perfect performance.\n1\nIntroduction\nArabic is a highly inﬂected Semitic language, spo-\nken by almost 400 million native speakers around\nthe world. Arabic words are highly ambiguous,\nmostly due to the lack of short vowels, represented\nby diacritic vocalization marks, which are typically\nomitted in standard writing. Modern Standard Ara-\nbic (MSA), is the language that is used in ofﬁcial\nsettings, while the dialectal variants of Arabic are\nused in day-to-day conversations. In addition to\nvocalization marks, some Arabic letters carry dots,\ncalled i’jaam (ÐA \u000bj. \u0015«@\r\u000b), which are used to distinguish\nbetween consonants represented by the same or-\nthographic form, or rasm (Õæ\u0015\u000bP) in Arabic. For ex-\nample, the letters T¯a (\u0005\u0010K), Y¯a (\u0005K\n), Th¯a (\u0005\u0011K), B¯a (\u0005K.),\nand Nun (\u0005\tK) have exactly the same orthographic\nshape, excluding the number and location of the\ndots they carry. Without the dots, the letter remains\nambiguous. Nevertheless, some dots are sometimes\nforgotten in handwritten scripts, forcing the reader\nto use the surrounding context in order to resolve\nsuch ambiguities. It becomes slightly more com-\nplicated when some of the letters turn into other\nArabic letters after their dots are being removed.\nFor example, by removing the three dots from the\nletter Sh¯ın ( \u0011) we get the letter S¯ın (), and that\nmakes different words look the same. For exam-\nple, the reader may have difﬁculty understanding\nthe meaning of the word I. ª (sa’b), which can be\ninterpreted without the dots as \"sigh\", and with the\ndots I. ª \u0011 (sha’b) as \"people\". Additional examples\nare provided in Table 2.\nFortunately, dots are strictly used in digitized\ntexts. However, we have noticed a recent trend of\nremoving dots from Arabic posts on social media\n(Drißner, 2021)1, where people use special key-\nboards and applications to naturally write with-\nout dots, mainly for bypassing automatic content-\nﬁltering algorithms to avoid having their message\nclassiﬁed as offensive. It seems like most native\nArabic speakers can still understand the meaning\nof the text, even if provided dotless. Table 1 shows\nan example for a text and its undotted version.\nThe use of dots for distinguishing between con-\nsonants was introduced to the Arabic language after\nthe rise of Islam, when non native speakers started\nshowing interest in the new religion. Until that\ntime, the knowledge of how to pronounce undot-\nted text was based on the reader’s memory and the\nsurrounding context (Daniels, 2014).\nThe use of Transformer (Vaswani et al., 2017)\nin natural language processing (NLP) has become\nfundamental to achieve state-of-the-art results in\ndifferent downstream tasks, including content ﬁl-\ntering. Since Transformer-based language mod-\nels are trained with digitized texts, the vocabulary\nacquired from the data is represented with dots.\nTherefore, the undotted letters that are not part of\nthe ofﬁcial Arabic language, are not recognized\nby the model, even if they exist in the Unicode\ncharacter set (e.g., \"Dotless Archaic Beh\" [H]).\nIn this work, we study the effect of removing\ndots from text written in Arabic, on the perfor-\n1https://arabic-for-nerds.com/dotless-arabic/\narXiv:2111.09791v1  [cs.CL]  18 Nov 2021\nmance of a Transformer-based language model,\nemployed as a typical content-ﬁltering classiﬁer.\nOur results show that replacing the dotted MSA\nletters with their corresponding dotless versions,\ncauses a strong adversarial effect on the perfor-\nmance of the language model that was ﬁne-tuned\non various downstream tasks. We describe our at-\ntempts to handle undotted Arabic, none of them\nrequire re-training the language model, and discuss\ntheir results and potential contributions.\n2\nRelated Work\n2.1\nArabic Transformer Models\nMultilingual BERT, or mBERT (Devlin et al.,\n2019), was the ﬁrst pre-trained language model\nto include Arabic. It covers only MSA, and usually\ndo not perform well enough on downstream Ara-\nbic NLP tasks, due to the relatively small Arabic\ntraining data it was trained on. AraBERT (An-\ntoun et al., 2020) and GigaBERT (Lan et al., 2020)\nare two language models that were trained on a\nmuch larger portion of Arabic texts, still only MSA.\nBoth offer better performance on downstream tasks.\nTwo recent models, MARBERT (Abdul-Mageed\net al., 2021a), and CAMeLBERT (Inoue et al.,\n2021), include Dialectical Arabic in their training\ndata, reaching better performance on relevant tasks.\nNone of these models have been used with undotted\nArabic, which is the main focus of our work.\n2.2\nAdversarial Inputs in NLP\nAdversarial inputs are crafted examples to deceive\nneural networks at inference time. Such attacks\nhave already been introduced and discussed by\nSzegedy et al. 2013 and Goodfellow et al. 2015, fo-\ncusing mostly on adversarial perturbations in vision\ntasks. Generating adversarial inputs in NLP is con-\nsidered to be more challenging than in computer vi-\nsion, mostly due to the relatively large importance\nevery word has in a given input text, comparing to\nthe small importance a single pixel has in an input\nimage. Nonetheless, it has been recently addressed\nby Jin et al. (2020), who presented an efﬁcient way\nof generating adversarial textual inputs for a BERT\n(Devlin et al., 2019), by modifying the texts seman-\ntically based on some word statistics taken from\nthe language model itself. They showed that while\ntheir modiﬁed texts are understandable by human\nreaders, their BERT-based models have struggled\nto produce the correct output. In this work, we eval-\nuate a more natural approach for fooling an Arabic\nlanguage model, simply by converting some letters\nto their undotted versions, keeping the modiﬁed\ntext understandable for human readers.\n3\nHandling Undotted Arabic\nWe begin by ﬁne-tuning a Transformer-based Ara-\nbic language model on two downstream tasks, and\nevaluate their performance on undotted inputs. Fol-\nlowing that, we develop different computational\napproaches for recovering the missing information\nthat was lost with undotting, without pre-training\nthe language model itself. We evaluate the different\napproaches on the same downstream tasks, and re-\nport on the results in the following section. For all\nour experiments we use the recent CAMeLBERT-\nMix base model (Inoue et al., 2021), which was\npre-trained on a mix of MSA, Classical Arabic, and\nDialectical Arabic texts.\n3.1\nUndotting\nIn order to remove dots from the text, we created\na mapping for all the Arabic characters available\nin the Unicode character set, for which we match\nthe most resemblant undotted character. The map-\nping table is provided in Appendix A. Some Arabic\nletters have different forms, depending on whether\nthey appear at the beginning, middle or end of a\nword. Therefore, we map all the forms of a relevant\nletter. Undotting an input text is a simple replace-\nment of all relevant letters with their orthographic\nequivalents.\n3.2\nSupporting Undotted Arabic\nAs reported in the following section, ﬁne-tuned\nArabic language models do not perform well on\nundotted texts. Therefore, we suggest two ways\nto handle undotted texts. In one way, we make\nchanges to the tokenizer of the model, and in an-\nother way we develop an algorithm for restoring\nthe dots of the input text, which runs as a pre-\nprocessing step before submitting the text to the\nlanguage model.\n3.2.1\nChanging the Tokenizer\nBefore processing the text with a pre-trained lan-\nguage model, it is necessary to break it into tokens\nusing the same tokenizer that was used during the\npre-training phase of the model. CAMeLBERT-\nMix uses a standard BERT tokenizer, provided by\nHugging Face2, with a vocabulary of 30,000 tokens.\n2https://github.com/huggingface/\ntokenizers\nOriginal\n\u0010éJ\n\u0010\u001cK\nñºË@ \u0010éJ\nÒJ\nÊ\u0010¯B\r @ èAJ\nÖÏ@ ú\n\t¯ Qå\u0011\u0010J\t\u001c\u0010K ú\n\u0010æË@ ©\u0010\u001cË@ \u0010éJ\n\u0010\u001cK\nñºË@ P \tQm.Ì'@ øYg@\r \u0015èðPA\u0010¯\u0016@ Yª\u0010Kð\nUndotted\néJJKñºË@ éJÒJÊ¯B\r @ èAJÖÏ@ ú¯ QåJJK úæË@ ©\u001cË@ éJJKñºË@ PQmÌ'@ øYg@\r \u0015èðPA¯\u0016@ YªKð\nTranslation\nQaruh is one of the nine Kuwaiti islands in the Kuwaiti territorial waters\nTable 1: Arabic text, given with and without the dots. The text was taken from https://arabic.cnn.com/\ntravel/article/2021/07/13/qaruh-island-kuwait.\nEach token has a numeric identiﬁer. We take two\ndifferent approaches for changing the conﬁguration\nof the tokenizer in order to handle undotted texts\nwithout having to pre-train the language model, nor\nﬁne-tuning it on a downstream task.\nUndotting the Tokenizer Vocabulary.\nAccord-\ning to this approach, we undot the entire vocabu-\nlary of the tokenizer, thereby enabling it to seam-\nlessly recognize undotted letters and words. Obvi-\nously, after undotting the vocabulary some of the\ntokens (5,852 out of the original 30,000 tokens,\nor 19.52%) become identical, leaving some of the\ntoken identiﬁers unused; therefore, the model’s vo-\ncabulary get smaller. Since we suspect that work-\ning with a smaller vocabulary may be detrimental\nto the performance of the model on downstream\ntasks, we suggest another approach for modifying\nthe tokenizer.\nExtending the Tokenizer Vocabulary.\nUnder\nthis approach, we extend the tokenizer’s vocabu-\nlary by adding the undotted version of the relevant\ntokens and mapping them to the same identiﬁer of\ntheir original token. This way the tokenizer keeps\nthe original dotted version of every token, and thus\ncan accept both, dotted and undotted inputs. We\nadd the undotted version of a token only if it is not\nalready part of the vocabulary; overall, we added\n17,280 undotted versions. The resulting vocabulary\nhas about 57% token identiﬁers that are mapped to\ntwo token versions.\n3.2.2\nAReDotter: Restoring Arabic Dots\nAs opposed to the previous approach, here we de-\nvelop an algorithm for pre-processing the input un-\ndotted text to restore its dots. The language model\nitself remains unmodiﬁed.\nWe train a sequence-to-sequence machine-\ntranslation (MT) model on the unlabeled 10M Ara-\nbic tweets dataset published with the second NADI\nshared task (Abdul-Mageed et al., 2021b). The\ntweets were posted from multiple geographies.\nFor creating parallel texts for training, every\ntweet from the original corpus was paired with\nits automatically generated undotted version, using\nthe mappings provided in Appendix A. We remove\nfrom the tweets URLs, user mentions, and hash-\ntags.\nOur MT model is based on the pre-trained\nArabic-to-English Marian MT (Tiedemann and\nThottingal, 2020) architecture3, which was ﬁne-\ntuned for \"undotted Arabic\"-to-Arabic translation.\nWe ﬁne-tune our model on the entire parallel\ndataset for two epochs.\n4\nExperimental Results\nTo evaluate our proposed methods, we ﬁne-tune\nCAMeLBERT on two tasks, sentence level and\ntoken level.\nFor the sentence-level task we use the sentiment\nanalysis subtask of ArSarcasm-v2 (Abu Farha et al.,\n2021), designed as a three-labels (positive, nega-\ntive, neutral) classiﬁcation task. As we did with\nNADI, we preprocess the text to remove URLs,\nuser mentions, and hashtags. For evaluation, we\nuse the ofﬁcial evaluation objective metric, deﬁned\nas macro average F1 score of both non-neutral la-\nbels.\nFor a token-level downstream task, we evaluate\nour language model on the named-entity recogni-\ntion (NER) task using the ANERcorp dataset (Be-\nnajiba et al., 2007). We use the modiﬁed version of\nthe dataset, which was recently released by Obeid\net al. (2020). Following previous works on NER,\nwe use the micro average F1 metric for evaluation.\nFor each task, we ﬁne-tune CAMeLBERT on the\noriginal preprocessed training data for 10 epochs,\nusing the ofﬁcial train/test split, and evaluate it\non the undotted version of the test set. We use\nthe standard Hugging Face’s pipelines, AutoMod-\nelForSequenceClassiﬁcation and AutoModelFor-\nTokenClassiﬁcation4 for the sentiment analysis and\nNER tasks, respectively. We evaluate the models\nunder different conditions of supporting undotted\n3Speciﬁcally, we used the Helsinki-NLP/opus-mt-ar-en\nmodel from Hugging Face.\n4https://huggingface.co/transformers/\nmodel_doc/auto.html\nUndotted\nOption 1 (pronunciation, meaning)\nOption 2 (pronunciation, meaning)\nIjJ¯\nI. j. J\n\t¯ (fyajib, \"must\")\n\u0010Ij\u0010J\t¯ (fatahat, \"opened\")\nPA®K\n\u0010PA\t®\u0010K (tafaruq, \"leave\")\n\u0010PA\t®K. (bifariq, \"difference\")\nPAm\u001a'\nPAm.\u001a\t' (najaar, \"carpenter\")\nPAm\u001a'. (bahaar, \"seas\")\nHñJk\nH. ñJ.k (hubub, \"cereal\")\nH. ñ\tJk. (janub, \"south\")\nTable 2: Examples of undotted ambiguous words. We do not provide all the possible pronunciations in each row.\nArSarcasm V2 - Sentiment\nANERCorp\nOriginal Text\n70.55\n81.39\nUndotted Text\n44.86\n9.16\nUndotted Text + Undotted Tokenizer\n64.50\n72.85\nUndotted Text + Extended Tokenizer\n65.03\n71.68\nAReDotter\n68.27\n67.97\nTable 3: Model performance on downstream tasks, using different undotted text handling approaches.\ntexts, as described in the previous section.\n4.1\nResults\nThe results, reported in Table 3, demonstrate the ad-\nversarial effect of processing undotted Arabic with\na vanilla, unmodiﬁed CAMeLBERT model. The\nﬁrst row lists the results we get by working with the\noriginal texts. In the second row we provide the re-\nsults of using the same model, but this time applied\non the undotted version of the texts. As observed,\nthe metrics measured for the two tasks dropped\nsigniﬁcantly on undotted texts. Unsurprisingly, the\ntokenizer of the vanilla language model does not\nrecognize tokens with undotted letters, which are\nexcluded from the modern Arabic script, and thus\ntreating them as “unknown” tokens.\nThe two tokenizer-updating approaches, whose\nresults are reported in the 3rd and 4th rows, prove\nto be effective for undotted texts, in both tasks.\nThis improvement is achieved mainly due to the\nreduction in the number of unknown tokens the\nmodel is assigned with. Among the two, we ob-\nserve that the extended tokenizer is slightly better\non the sentiment analysis task, while the undotted\ntokenizer is better on the NER task. However, the\ndifference in those results is insigniﬁcant.\nInterestingly, AReDotter, our MT dots restora-\ntion model, which we run as a preprocessor be-\nfore submitting the text to the language model,\nprovides competitive results in both task. It is\nslightly better than the tokenizer-updating tech-\nniques on sentiment analysis, but slightly worse on\nthe NER task. Naturally, a sequence-to-sequence\ntranslation model may sometimes generate some\nout-of-context tokens in the target sequence. We\nbelieve that NER is more sensitive to this type of\nmistakes than sentiment analysis task. For future\nwork, we plan to work with a simple sequential-\ntagging model instead of the sequence-to-sequence\nMT model, to avoid generating such tokens. The\nresults we get from AReDotter are encouraging; it\nprovides an elegant way to support undotted text\nwithout modifying the model or the tokenizer.\n5\nConclusion\nUndotting has been recently adopted by social-\nmedia users in order to bypass content-ﬁltering\ngateways. We studied the effect of undotting on\nthe performance of a standard pre-trained language\nmodel. Our results show that processing undotted\ntext with a vanilla, unmodiﬁed language model, has\na detrimental effect in two downstream NLP tasks.\nBy simply editing the tokenizer, which is used by\nthe language model, we are able to show signiﬁcant\nimprovements over the vanilla model.\nOur third approach, which does not require\nchanging the tokenizer, is using a machine-\ntranslation model for restoring the missing dots.\nWith this technique we show competitive results to\nthe tokenizer-updating techniques, without having\nto modify the model or its tokenizer. We believe\nthat our study provides some conclusions as for\nhow undotted texts should be treated with modern\nTransformer-based language models. We recom-\nmend that at least one of our techniques will be\nadopted as a standard step in a common Arabic\nNLP pipeline.\nReferences\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021a.\nARBERT\n& MARBERT: Deep Bidirectional Transformers for\nArabic. In Proceedings of the ACL-IJCNLP 2021\nMain Conference. Association for Computational\nLinguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, Abdel-\nRahim Elmadany,\nHouda Bouamor,\nand Nizar\nHabash. 2021b. NADI 2021: The second nuanced\nArabic dialect identiﬁcation shared task. In Proceed-\nings of the Sixth Arabic Natural Language Process-\ning Workshop, pages 244–259, Kyiv, Ukraine (Vir-\ntual). Association for Computational Linguistics.\nIbrahim Abu Farha, Wajdi Zaghouani, and Walid\nMagdy. 2021. Overview of the WANLP 2021 shared\ntask on sarcasm and sentiment detection in Arabic.\nIn Proceedings of the Sixth Arabic Natural Lan-\nguage Processing Workshop, pages 296–305, Kyiv,\nUkraine (Virtual). Association for Computational\nLinguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding.\nIn Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nYassine Benajiba, Paolo Rosso, and José Miguel\nBenedíRuiz. 2007.\nANERsys: An Arabic named\nentity recognition system based on maximum en-\ntropy. In Computational Linguistics and Intelligent\nText Processing, pages 143–153, Berlin, Heidelberg.\nSpringer Berlin Heidelberg.\nPeter T. Daniels. 2014. The Type and Spread of Ara-\nbic Script, pages 25 – 39. Brill, Leiden, The Nether-\nlands.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGerald Drißner. 2021. Social media & palestine: Dot-\nless Arabic outsmarts algorithms.\nIan Goodfellow,\nJonathon Shlens,\nand Christian\nSzegedy. 2015. Explaining and harnessing adversar-\nial examples. In International Conference on Learn-\ning Representations.\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda\nBouamor, and Nizar Habash. 2021. The interplay\nof variant, size, and task type in Arabic pre-trained\nlanguage models. In Proceedings of the Sixth Ara-\nbic Natural Language Processing Workshop, Kyiv,\nUkraine (Online). Association for Computational\nLinguistics.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is BERT really robust? A strong\nbaseline for natural language attack on text clas-\nsiﬁcation and entailment.\nIn The Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence, EAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pages 8018–8025. AAAI\nPress.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020.\nAn empirical study of pre-trained transformers for\nArabic information extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4727–4734,\nOnline. Association for Computational Linguistics.\nOssama Obeid, Nasser Zalmout, Salam Khalifa, Dima\nTaji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl\nEryani, Alexander Erdmann, and Nizar Habash.\n2020. CAMeL tools: An open source python toolkit\nfor Arabic natural language processing. In Proceed-\nings of the 12th Language Resources and Evaluation\nConference, pages 7022–7032, Marseille, France.\nEuropean Language Resources Association.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. 2013. Intriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199.\nJörg\nTiedemann\nand\nSanthosh\nThottingal.\n2020.\nOPUS-MT — Building open translation services for\nthe World. In Proceedings of the 22nd Annual Con-\nferenec of the European Association for Machine\nTranslation (EAMT), Lisbon, Portugal.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez,\nukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nA\nAppendix A: Undotting Table\nInitial/Medial\nTerminal\nLetter\nMSA\nUndotted\nUndotted\nBa\nH.\n\u0005K\nH\nTa\n\u0010H\n\u0005K\nH\nTha\n\u0011H\n\u0005K\nH\nJim\nh.\n\u0005k\nh\nKha\np\n\u0005k\nh\nDhal\n\tX\nX\nX\nZayn\n\tP\nP\nP\nShin\n\u0011\n\u0005\n\nDad\n\t\n\u0005\n\nZa’\n\t \n\u0005£\n \nGhayn\n\t¨\n\u0005«\n¨\nFa\n\t¬\n\u0005¯\n¬\nQaf\n\u0010\n\u0005¯\n\nNun\n\tà\n\u0005K\nà\nYa\nø\n\u0005K\nø\nTable 4: Character mapping used for our undotting\nfunction, specifying only the letters which are not iden-\ntical to their undotted version.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-11-18",
  "updated": "2021-11-18"
}