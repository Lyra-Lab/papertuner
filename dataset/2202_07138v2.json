{
  "id": "http://arxiv.org/abs/2202.07138v2",
  "title": "Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge",
  "authors": [
    "Kebing Jin",
    "Hankz Hankui Zhuo"
  ],
  "abstract": "Natural language processing (NLP) aims at investigating the interactions\nbetween agents and humans, processing and analyzing large amounts of natural\nlanguage data. Large-scale language models play an important role in current\nnatural language processing. However, the challenges of explainability and\ncomplexity come along with the developments of language models. One way is to\nintroduce logical relations and rules into natural language processing models,\nsuch as making use of Automated Planning. Automated planning (AI planning)\nfocuses on building symbolic domain models and synthesizing plans to transit\ninitial states to goals based on domain models. Recently, there have been\nplenty of works related to these two fields, which have the abilities to\ngenerate explicit knowledge, e.g., preconditions and effects of action models,\nand learn from tacit knowledge, e.g., neural models, respectively. Integrating\nAI planning and natural language processing effectively improves the\ncommunication between human and intelligent agents. This paper outlines the\ncommons and relations between AI planning and natural language processing,\nargues that each of them can effectively impact on the other one by five areas:\n(1) planning-based text understanding, (2) planning-based natural language\nprocessing, (3) planning-based explainability, (4) text-based human-robot\ninteraction, and (5) applications. We also explore some potential future issues\nbetween AI planning and natural language processing. To the best of our\nknowledge, this survey is the first work that addresses the deep connections\nbetween AI planning and Natural language processing.",
  "text": "Integrating AI Planning with Natural Language Processing:\nA Combination of Explicit and Tacit Knowledge\nKEBING JIN and HANKZ HANKUI ZHUO∗, School of Computer Science and Engineering, Sun\nYat-sen University, China\nNatural language processing (NLP) aims at investigating the interactions between agents and humans, pro-\ncessing and analyzing large amounts of natural language data. Large-scale language models play an important\nrole in current natural language processing. However, the challenges of explainability and complexity come\nalong with the developments of language models. One way is to introduce logical relations and rules into\nnatural language processing models, such as making use of Automated Planning. Automated planning (AI\nplanning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals\nbased on domain models. Recently, there have been plenty of works related to these two fields, which have\nthe abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from\ntacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing\neffectively improves the communication between human and intelligent agents. This paper outlines the\ncommons and relations between AI planning and natural language processing, argues that each of them can\neffectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based\nnatural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and\n(5) applications. We also explore some potential future issues between AI planning and natural language\nprocessing. To the best of our knowledge, this survey is the first work that addresses the deep connections\nbetween AI planning and Natural language processing.\nCCS Concepts: • Computing methodologies →Natural language processing; Planning and schedul-\ning; Information extraction; Natural language generation.\nAdditional Key Words and Phrases: AI planning, Natural language processing, Natural language understanding,\nHuman-robot interaction, Explainability\nACM Reference Format:\nKebing Jin and Hankz Hankui Zhuo. 2022. Integrating AI Planning with Natural Language Processing: A\nCombination of Explicit and Tacit Knowledge. ACM Trans. Intell. Syst. Technol. 1, 1 (April 2022), 24 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nNatural language processing (NLP) aims at investigating the interactions between agents and\nhumans, processing and analyzing large amounts of natural language data. In recent years, for\nattaining better performance and handling large corpora, building large-scale language models\nis an inevitable trend in real applications [62, 78, 126]. Despite the success of language models in\nvarious domains, the explainability and complexity of language models have drawn intense research\ninterests recently. In order to make models explainable and lightweight, integrating models with\n∗Corresponding author\nAuthors’ address: Kebing Jin, jinkb@mail2.sysu.edu.cn; Hankz Hankui Zhuo, zhuohank@mail.sysu.edu.cn, School of\nComputer Science and Engineering, Sun Yat-sen University, Guangzhou, China, 510006.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2022 Association for Computing Machinery.\n2157-6904/2022/4-ART $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\narXiv:2202.07138v2  [cs.AI]  13 Apr 2023\n2\nJin et al.\nsymbolic planning has been demonstrated effective in various NLP tasks. Symbolic planning (AI\nplanning) is a branch of artificial intelligence that focuses on building symbolic domain models and\nsynthesizing plans to transit initial states to goals based on domain models. The plans are typically\nfor execution by intelligent agents, autonomous robots, and unmanned vehicles. Different from\nclassical control and classification problems, the solutions are complex and must be discovered and\noptimized in multidimensional space. Generally, those approaches are mostly based on structured\ndata, which has a well-defined structure and logically explainable to humans.\nCompared with structured data used in AI planning, natural language descriptions are often\ncomplicated by omissions, inverted order, etc., resulting in difficulties in reasoning about language\ndescriptions. It is thus often hard to directly train neural models to generate available and correct\nsolutions, although deep learning has been widely used to handle unstructured data. Deep learning\nmethods do well in acquiring knowledge from data, capturing implied rules, and expressing them\nby mathematical and neural models, which are tacit and unable to be directly shared with other\nhumans and agents. Different from deep learning methods that aim to learn tacit knowledge,\nplanning-based methods are better at capturing changes, formalizing them by rules, and generating\nvalid plans when handling structured data. Rules are already codified, namely explicit knowledge,\nwhich can be clearly expressed and easily shared with others. Therefore, AI planning is one of the\nconsiderable steps to understand implied rules and build domain models from large amount of texts\nin natural language processing [32, 72].\nOn the other hand, unstructured data in real world is not disorderly but often a sequence based\non rules. As for a natural language description, there is a theme running through it, along with a\nseries of relevant events and a coherent text unfolds. Each sentence relates to the preceding texts\nand influences following sentences, just like preconditions and effects of actions in AI planning. For\nexample, in a recipe about making a meatloaf shown in Figure 1(a), humans can easily understand it\nand capture the main information including verbs, e.g., “Heat”, and objects, e.g., “butter” and “skillet”.\nHowever, as for agents, when given a mass of data in the form of sentences, it is hard to directly build\nmodels to reason about the implied rules and predict next moves. If we extract these information and\nformalize them structurally, as shown in Figure 1(b), it is easier to construct models based on planning\nmethods for guiding future unseen tasks.\nBesides using AI planning to help reason about implied rules in texts, the power of AI planning\nabout capturing implied relations and computing valid solutions is another effective way to improve\nnatural language processing, such as text summarization and machine translation. For example,\nthere have been planning-based text generation methods [57, 119] extending a clear storyline\nordered in advance. Those methods first compute sequences composed of keywords, key phrases,\nor contents as storylines, then use natural language processing techniques to extend storylines\nto coherent texts. In the above-mentioned example, generating an available recipe in a correct order\nshown in Figure 1(a) is hard. However, given some rules, such as domain models about the operations\nof cooking, agents can compute plans toward achieving specified goals like a theme about making a\nmeatloaf, as shown in Figure 1(b). Agents can easily extend the plan and gain a valid recipe.\nThe integration of AI planning and natural language processing combines the best of tacit\nknowledge learning from sentences and explicit knowledge in the form of rules. As discussed\nin [52], it would be more effective to combine explicit and tacit knowledge rather than giving\nup explicit knowledge and learning everything from tacit knowledge, which is the current trend.\nIntegrating AI planning and natural language processing allows human to communicate with\nagents in a more comfortable way, and enables intelligent agents to explain themselves to human\nin a human-understandable way. Natural language, as the most comfortable way to communicate\nwith humans, establishes a relationship between humans and intelligent agents. In recent years,\nresearchers have made efforts to connect with natural language and robots, such as by dialogue\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n3\nHeat butter in a large skillet.  Add the onions, garlic, celery and carrot. \nCook the mixture over medium heat for 5 minutes, stirring frequently. Add \nthe ketchup sauce  and the salt and pepper. Cook the mixture for another \nminute. Combine the meat, bread crumbs, eggs  and parsley in a mixing \nbowl. Add the cooked vegetables from the skillet. Mix everything well \ntogether. Push the mixture into the baking loaf pan. Then put the loaf into \nthe oven and bake for about an hour. When the meatloaf is done, allow it \nto cool slightly, then slowly remove the loaf from the baking pan by cutting \nalong the sides with a butter knife. Gently lift the meatloaf from the pan \nand place on serving plate. Cut into slices of approximately 3/4 inch thick. \nHeat(butter, skillet) → Add (onions, garlic, celery, carrot, skillet) → \nCook(mixture, medium heat, 5 minutes) → stirring() → Add(ketchup sauce, \nsalt, pepper, skillet) → Cook(mixture, medium heat, one minute) → \nCombine(meat, bread crumbs, eggs, parsley ,bowl) → Add(cooked \nvegetables, bowl) → Mix() → Push(mixture, loaf pan) → Put(loaf, oven) → \nBake(an hour) →  Cool() → Remove(loaf) → Cut(sides, knife) → \nLift(meatloaf, pan) → Place(meatloaf, plate) → Cut(meatloaf, 3/4 inch thick)\n(a) A recipe for making a meatloaf.\n  \n \n \n \n(b)\n \nA\n \ntrace\n \nextracted\n \nfrom\n \n(a).\nFig. 1. An example textual recipe about making a meatloaf.\nsystems [82, 105] and natural language commands understanding [59, 108]. On the other hand,\nplanning-based natural language models are based on structured data or implied rules, such as\npredicted storylines, which allows human to partly understand the principles of models.\nIn this paper, we first introduce some background knowledge in AI planning and natural language\nprocessing as well as their relations. Then we give a comprehensive overview of integrating AI\nplanning and natural language processing by four aspects and their challenges: planning-based text\nunderstanding, planning-based natural language processing, planning-based explainability, and\ntext-based human-robot interaction. Their relations are shown in Figure 2. Firstly, planning-based\nnatural language understanding includes extracting actions from texts and learning domain models\nfrom texts. Secondly, we introduce planning-based natural language processing by three tasks\nintegrated with AI planning, i.e., text generation, text summarization, and machine translation. Then\nwe discuss planning-based explainability. Next, we introduce text-based human-robot interaction by\nextracting actions from natural language instructions, natural language command understanding,\nand dialogue generation. Finally, we present current applications, several future directions and\nconclude this paper. To the best of our knowledge, this survey is the first work that addresses the\ndeep connections between AI planning and NLP.\n2\nPLANNING DOMAIN DESCRIPTION LANGUAGE AND NLP\nIn this section, we introduce modeling knowledge in AI planning, backgrounds in natural language\nprocessing (NLP), and relations between AI planning and NLP, including similarities, differences,\nand language model-based planning.\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n4\nJin et al.\nNatural Language\nText\nMake use of extracted rules \nto improve coherence and \nreasonability.\nHuman\nRobot\nActions Extraction\nNatural Language \nCommand Understanding\nDialogue Systems\nHuman-robot \nInteraction\nAs a \nMedium \nExplainability\nText Generation\nText Summarization\nMachine Translation\nPlanning-based\nNatural Language Processing\nAction traces Extraction\nDomain Models Learning\nPlanning-based Natural \nLanguage Understanding\nExtract information, capture rules, \nand build domain models.\nLearn implied rules, \norganize skeletons, \nand predict key \ninformation.\nGive instructions\nExecute \ninstructions\nFig. 2. Relations between AI planning and Natural language processing\n2.1\nPlanning domain description language\nA planning problem is composed of a planning domain D and an instance 𝑝, defined by planning\ndomain description language. With the development of AI planning, more and more extended\nplanning domain description languages [34, 39, 92] have been proposed. Taking PDDL (Planning\nDomain Definition Language) [71] as an example, a planning domain D is made up by several\naction models. An action model is defined by a tuple of 𝐴= ⟨𝑎, pre(𝑎), eff(𝑎)⟩, where 𝑎is an action\nname with zero or more types of parameters. An action is a grounding of an action model, each\nof whose parameters is an object. pre(𝑎) is a set of preconditions requiring to be satisfied when\nexecuting 𝑎, each of which is a proposition or a numeric constraint. Similarly, eff(𝑎) is a set of effects,\nan effect can be a topical proposition, added into or deleted from the state after executing 𝑎, or a\nnumeric updating, increasing or decreasing the value of variables according to specified functions.\nAn instance is defined by 𝑝= ⟨𝑠0,𝑔, 𝜉⟩, where 𝑠0 is a set of initial assignments by propositions\nand variables, and 𝑔is a set of goals requiring to be achieved. 𝜉is an objective function guiding\nplanner to compute for a minimum cost or maximum reward. A planning problem is to compute\nan available action sequence, which can transfer 𝑠0 to a state containing desired goals 𝑔.\nFor example, parts of action models in the Rover domain are shown in Figure 3(a), where “(equipped_for\n_imaging ?r)” is a proposition asking that a rover “?r” should equip with a camera for taking image\nwhen executing action “Calibrate”. “(>= (energy ?r) 1)” is a numeric precondition requiring the energy\nof rover “?r” should be larger than 1. Figure 3(b) and (c) show an initial state and goals, respectively.\nAn example valid plan, updating the initial state to a state achieving the goals, is shown in Figure 3(d),\nwhere each action is a grounding action model with parameters.\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n5\n(:action Calibrate\n :parameters (?r - rover ?i - camera ?t - objective ?w - waypoint)\n :precondition (and (equipped_for_imaging ?r) (>= (energy ?r) 2) (calibration_target ?i ?t) (at ?r ?w) \n        (visible_from ?t ?w) (on_board ?i ?r))\n :effect (and (decrease (energy ?r) 2)(calibrated ?i ?r) ))\n(:action Take_image\n :parameters (?r - rover ?p - waypoint ?o - objective ?i - camera ?m - mode)\n :precondition (and (calibrated ?i ?r) (on_board ?i ?r) (equipped_for_imaging ?r) (supports ?i ?m) \n         (visible_from ?o ?p) (at ?r ?p) (>= (energy ?r) 1))\n :effect (and (have_image ?r ?o ?m)(not (calibrated ?i ?r))(decrease (energy ?r) 1)))\n...\nCalibrate (rover0 camera0 objective1 waypoint3) → Take_image (rover0 waypoint3 objective1 camera0 high_res) → Communicate_image_data (rover0 camera objective1 high_res waypoint3 \nwaypoint0) → Sample_rock (rover0 rover0store waypoint3) → Drop (rover0 rover0store) → Communicate_rock_data (rover0 camera waypoint3 waypoint3 waypoint0) → Navigate (rover0 \nwaypoint3 waypoint1) → Navigate (rover0 waypoint1 waypoint2) → Sample_soil (rover0 rover0store waypoint2) → Communicate_soil_data (rover0 camera waypoint2 waypoint2 waypoint0)\n(:init\n(equipped_for_imaging rover0) (calibration_target camera0 objective1) \n(at rover0 waypoint3) (on_board camera0 rover0) (available rover0)\n(calibration_target camera0 objective1) (supports camera0 colour)\n(supports camera0 high_res) (in_sun waypoint0) (= (energy rover0) 50)\n.... )\n(:goal (and\n(communicated_soil_data waypoint2)\n(communicated_rock_data waypoint3)\n(communicated_image_data objective1 high_res)))\n(a) Action models.\n(b) Initial state.\n(c) Goals.\n(d) An example plan for the planning problem.\nFig. 3. An example in the Rover domain, including action models, initial state, goals, and an available plan.\n2.2\nNatural language processing\nRecently, natural language processing (NLP) [17, 54, 114] has attracted lots of attention, and it\nbuilds a bridge between human and agents. Natural language processing is grand, including various\nfields, such as natural language understanding (NLU)[81, 107], natural language generation (NLG),\n[30, 37] machine translation[80], and spelling correction[43]. NLP has undergone several stages\nof rule-based models, statistic-based models, and neural network models. Rule-based NLP [97] is\nled by hand-crafted rule sets, whose main task is to understand natural language. It is, however,\ndifficult and time-consuming to build all hand-crafted rules, lacking of scalability. Statistic-based\nNLP [65] makes use of probability distributions to generate proper words and sentences, promoting\nthe application of statistical machine learning methods based on large-scale corpora in natural\nlanguage processing. Nevertheless, statistic-based models is barely to capture long-term relations\nand use information included in contexts. Recently, deep learning has been widely used in NLP\ntasks[106], it is able to capture tacit knowledge implied in texts. However, deep learning is to fit\nneural networks and predict based on statistics, it can not “understand” the real meaning in natural\nlanguage. In this paper, we focus on those NLP tasks related to AI planning. Compared with NLP\napproaches totally based on deep learning, planning-based NLP methods are more curious about\nimplied logic and reasons of the solutions.\n2.3\nRelations between AI planning and natural language processing\nIn this section, we will sketch the relations between AI planning and natural language processing,\nwhich is mentioned by [38, 116, 122]. We will first introduce the similarities between AI planning\nand natural language processing, and then talk about their differences and deep relations.\nWe discuss the similarities between AI planning and natural language processing by two aspects.\nFirst of all, AI planning and natural language processing both revolve around observations and\nknowledge [38]. Planning tasks aim at either solving problems based on a current observation and\ngoals along with priori knowledge like action models, or constructing knowledge such as transition\nfunctions based on sequential observations. Similarly, in natural language processing tasks, a text\ncan be regarded as a sequence of observations, each observation is a sentence describing a partially\nobserved state, where observations changes following implied rules. Secondly, they share two\nmajor common problems in planning tasks and natural language processing tasks: consistency and\ndiversity. Both of plan traces and texts are cohesive descriptions and stringed by some rules and\ngoals. As for planning, the rules are action models and transition functions, which update current\nstates to next ones. The goals are sets of goal states or objective functions, guiding planners to attain\noptimal solutions. As for natural language processing, the rules are implied in the organization of\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n6\nJin et al.\ntexts, such as transition connections or consequences. Their goals can be titles, themes, or topics.\nOn the other hand, in natural language processing, given goals, we can generate lots of coherent\ntexts composed of different events, similar to different plan traces computed for the same goal\nstates. Moreover, a sequence of events can be written as various stylized texts. As shown in Table\n1, we enumerate some concepts in AI planning and natural language processing, which can have\nsome similarities. For example, objects, such as “rover0” and “objective1” in Figure 3, in planning\nproblems are similar to entities in texts, e.g., “skillet” and “onions” in Figure 1.\nTable 1. Similar concepts between AI planning and natural language processing\nAI planning\nNatural language texts\nObjects\nEntities\nStates\nSentences, sentiments, and intentions\nActions\nEvents\nDomain models\nImplied rules, transitions, and relations\nGoals\nTopics, and themes\nPlan traces\nStorylines, skeletons, and frameworks\nAlthough AI planning and natural language processing have those commons, the difference\nbetween them is that AI planning is good at generating explicit knowledge, such as domain models\n[128–131], while natural language processing often learns tacit knowledge, such as training models\nfrom natural language data. [52] argues that AI systems should be able to know when to take advice\nand when to learn, to find a balance between explicit and tacit knowledge. Taking planning-based\ntext generation as an example, although texts are required to be coherent and with correct logic,\nnatural language processing is weak in computing available and valid events, which AI planning\nis good at. And the integration of both allows agents to generate coherent texts by first using AI\nplanning to generate storylines and then learning text generator by natural language processing\ntechniques.\nIn a word, there are close ties between AI planning and natural language processing, due to\ntheir different advantages of explicit and tacit knowledge. The combination allows each of them to\neffectively impact on the other one.\n2.4\nLanguage Model-based Planning\nAlthough AI planning can effectively capture rules from action sequences, it is hard for humans\nwithout expert knowledge to construct structured plans. A natural and intuitive way is to use human\nnatural language to describe plans, asking language model-based planners to be able to handle\ntext sequences and predict the next moves [94, 132]. Prior works mostly make use of pre-trained\nlanguage models (LMs) to understand abstract, high-level textual actions and learn actionable\nknowledge for guiding planning [47, 50, 61]. Specifically, Huang et al. [46] use large language\nmodels (LLMs) to generate natural language actions, they investigated actionable knowledge already\ncontained pre-trained LLMs. On the other hand, some works map natural language instructions\nand high-level goals to actions and goals, and learn policy to make decisions [98]. For example, LID\n[61] uses policies initialized with pre-trained LMs and fine-tunes policies for predicting actions,\nvalidating that LMs are able to contain rich actionable knowledge. Language model-based planners\nare mostly based on templated textual actions datasets, rather than complex natural language\ninstructions with various styles of descriptions. In the following sections, we introduce deep\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n7\nconnections between AI planning and natural language processing, to rise to the dual challenges\nfrom rules and natural language.\n3\nPLANNING-BASED NATURAL LANGUAGE UNDERSTANDING\nIn this section, we introduce a comprehension overview of natural language understanding based on\nAI planning. Natural language understanding aims at comprehending human language, including\nsentiments, relations in contexts, topics, etc. Compared with learning relations from structured state\ntraces, learning relations between extracted events from texts are even more challenging. It asks\nagents to reason about contexts, capture themes of sentences by selecting words to represent them,\ncompute causal relationships between the selected words. There are two major areas introduced in\nthe following section, which are both based on AI planning, to understand the texts and learn causal\nrelationships from texts. The first one is to extract action sequences from texts, which requires\nagents to understand complex contexts from action descriptions. Moreover, it requires agents to be\ncapable of reasoning about connotations in texts, such as exclusion relations and optional relations\nbetween actions. The other one is to first select words to represent the main ideas of sentences,\nand then learn the rules implied in sentences and formalize them by readable domain models for\nguiding agents to solve future unseen tasks and helping agents and people understand logical\nrelations between events.\n3.1\nExtracting actions from texts\nThere have been works on extracting action sequences from action descriptions [127]. The inputs\nof the task mostly include some texts describing some actions and procedures, the outputs are\naction sequences from the texts, each action is composed of a verb as action name and some objects.\nFigure 4 shows an example in [32], where an input text is in left part of Figure 4, extracting action\ntraces are shown in the right part of Figure 4, the relations between actions are shown in the middle\nof Figure 4. This task does not only need extract a word standing for an action of the sentence,\nbut also reason about contexts for completing omissions caused by pronouns. Early approaches\n[55, 70] mostly make use of specialized resources, such as semantic parsers and learned lexicons,\nto reason about natural language route instructions. For example, MARCO [68] was proposed to\nCook the rice the day before, or use leftover \nrice in the refrigerator.  The important thing \nto remember is not to heat up the rice, but \nkeep it cold.  In a bowl, add 1 tablespoon of \noil to rice.  Use a spoon or your hands to \nwork the oil into the rice, evenly coating the \nrice.  Transfer the rice to a colander and \ndrain.  Combine eggs and salt in a small bowl \nand gently whisk until blended.  Heat 1 \ntablespoon oil in a wok.  Add whisked eggs \nand cumin seeds to wok.  Stir frequently, \nworking the eggs to a scramble.  Heat the \nremaining oil in the wok.  If desired, you can \nrecycle some of the oil that drained from the \nrice.  Add the garlic and onion to the wok.  \nStir-fry together over high heat for about 5 \nminutes or until the onion looks transparent, \nbut is not soft.  Add the rice, eggs, soy sauce, \nchili sauce, vinegar, and celery.  Mix \ntogether, continuing to stir-fry over high \nheat for 1-2 minutes while stirring \nfrequently.  Spoon onto a plate and serve. \nInput Training Text\nMission \nStart\nCook\nUse\nKeep\nHeat\nAdd\nRecycle\nWork\nServe\nEX \nEX\nES\nES\nES\nES\nES\nOP\nExtracting Action Names and Action Arguments\nSome Possible Outputs\nES: essential \nOP: optional \nEX: exclusive\nMake Egg Fried Rice\nMission \nEnd\nCook\nrice\nUse\nleftover \nrice\nKeep\nrice, \ncold\nAdd\noil\nspoon\nUse\nhands\nAction \nNames\nAction \nArguments\nES\nES\nES\nES\nEX\nEX\nx Cook (rice) Æ Keep (rice, cold) Æ Add \n(oil) Æ Use (spoon) Æ Work (oil, rice) \nÆ …  Æ Work (eggs) Æ Heat (oil) Æ … \nÆ Serve ()\nx Use (leftover rice) Æ Keep (rice, cold) \nÆ Add (oil) Æ Use (spoon) Æ Work \n(oil, rice) Æ …  Æ Work (eggs) Æ Heat \n(oil) Æ … Æ Serve ()\nx Use (leftover rice) Æ Keep (rice, cold) \nÆ Add (oil) Æ Use (hands) Æ Work \n(oil, rice) Æ …  Æ Work (eggs) Æ Heat \n(oil) Æ … Æ Serve ()\nx Use (leftover rice) Æ Keep (rice, cold) \nÆ Add (oil) Æ Use (hands) Æ Work \n(oil, rice) Æ …  Æ Work (eggs) Æ \nRecycle (oil) Æ Heat (oil) Æ … Æ \nServe ()\nx ...\nFig. 4. An illustration of action sequence extraction problem in [32]\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n8\nJin et al.\nmap free-form natural language route instructions to action sequences, arising great interest in\nnatural language processing community. MARCO is able to model a sentence by an instruction,\ne.g., “Turn to face the green hallway” can be modeled by “Turn(until=(object=Path, appear=Green,\nside=Front, dist=0:))”. [22] presented a system along with a plan refinement algorithm to transform\nnatural language navigation instructions into executable formal plans. Generally, methods with\nsemantics parsers require high simplicity of the texts. Therefore, mostly approaches are based on\ninstructional texts or similar texts following some templates.\nIn recent years, learning methods, such as reinforcement learning and LSTM, have been widely\nused in natural language processing, as well as extracting action sequences from natural language\ntexts, with the rapid development of artificial intelligence. For example, [13] proposed a reinforce-\nment learning approach for mapping natural language instructions in two domains, Windows\ntroubleshooting guides and game tutorials, to sequences of executable actions. It uses a reward\nfunction to define the quality of the executed actions, and a policy gradient algorithm to estimate\nthe parameters of a log-linear model for action selection. The learner repeatedly constructs action\nsequences for a set of documents, executes those actions, and observes the resulting reward. To\nhandle free natural language without restricted templates, EASDRL [32] was presented to extract\naction sequences from texts, making use of deep reinforcement learning. It builds Q-networks\nto learn policies of extracting actions and extract plans from the labeled texts. EASDRL regards\ntexts associated with actions as “states”, and associating words in texts with labels as “actions”.\nDuring capturing relations, EASDRL considers previously extracted actions as parts of states for\ndeciding the choice of next operations. Therefore, EASDRL is able to reason about connotations in\ntexts, such as exclusion relations and optional relations between actions. Except for reinforcement\nlearning, there are more techniques used in extracting actions from texts. With the help of with long\nshort-term memory (LSTM) recurrent neural networks, Mei at al. [72] proposed a neural sequence-\nto-sequence model to translate single-sentence natural language instructions to action sequences\nbased upon a representation of the observable world state. LSTMs are applicable to a number of\nsequence learning problems, due to their ability to learn long-term dependencies, and they have\nbeen shown to be effective in tasks existing sequences. The LSTM framework allows agents to\nbidirectionally encode the navigational instruction sequence and decode the representation to an\naction sequence, based on a representation of the current state.\n3.2\nLearning domain models from texts\nBesides extracting action sequences from texts, another way to understand text is to learn the\nimplied relations from sentences. The input of the learning task is a set of texts, and the output\nis a planning domain model composed of action models describing the relations by propositional\npreconditions and effects following the syntax of planning domain description language, such as\nPDDL [71]. Preconditions and effects make use of propositions to describe conditions that must be\nsatisfied when executing actions and results after executing them, respectively.\nLearning domain models from instructional texts is a little different from narrative stories. In-\nstructional texts are simpler than narrative stories, and the words are domain-dependent. Narrative\nstories are mostly third person synopses and they are always along with omission, which results in\ncomplexity when parsing sentences. A general way to construct a domain model is first to extract\nwords and objects by parsing sentences for annotations (e.g., OpenNLP1 and Stanford CoreNLP2 ),\nand then learn causal relationships between them and formalize the relations by action models.\n1https://opennlp.apache.org/\n2http://stanfordnlp.github.io/CoreNLP/\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n9\nTo learn domain models from instructional texts, Sil and Yates [103] used text mining via a\nsearch method to identify documents that contain words that represent target verbs or events.\nThen they used inductive learning techniques to identify appropriate action preconditions and\neffects. The method relies on handcrafted Pointwise Mutual Information to learn a SVM-based\nclassifier that scores preconditions for a given action. Branavan et al. [14] presented a reinforcement\nlearning framework to extract precondition and effects relations implied by the text, and used\nthese relations to compute action sequences for completing given tasks in the environment. Single\nargument predicates are extracted from the text as states, and regarded as sub-goals to construct\nhierarchical planning problems. Yordanova and Kirste [121] extracted verbs and objects from text\ninstructions based on part of speech (POS) tagging module, and discovered causal relations on the\nbasis of the order of appearance to build PDDL models. However, due to lacking of connections\nbetween texts and world states and analyses between variable texts with the same meaning, it is\nhard to directly construct domain models as learning action models from structured data. In this\npaper, domain models are constructed according to some templates after parsing sentences. For\nexample, “If the apple is ripe, put the apple on the table. ” indicates that “ripe”, a state of an apple, is\na precondition of action “put”. Therefore, a precondition of action “put” is “(state-ripe)”. Although\nthe model is readable for human, it is kind of redundant and not easy to understand for agents\nbecause of synonyms and polysemous words. Similarly, Lindsay et al. [64] assumed texts are in\nrestricted templates when describing actions. They generated sequences of actions by constructing\nrepresentations of sentences and cluster operators by computing similarity, and built PDDL domain\nmodels with the help of a domain model acquisition tool.\nConsidering the power of AI planning about offering correct causality and flexible narrative\ngeneration possibilities, constructing domain models has been used in narrative systems recently.\nHayton et al. [42] proposed an approach taking natural language sentences which summarise\nthe main elements of stories as inputs and generating action representations following PDDL,\na narrative planning domain model. To overcome difficulties in parsing narrative stories, they\npresented two sets of rules to handle pronouns in stories. Then they used a template similar to [121]\nto construct planning domains. Another specific difficulty for planning-based narrative systems\nis that hand-crafted domain models require more narrative actions and types of narrative objects\ncompared to generated planning domains. The plentiful actions and objects let generated plan\ntraces and storyline be more interesting and they can be extended to enjoyable stories. To achieve\nit, Porteous et al. [87] tried to anticipate the consequences of plan failure and the remedial actions\nor objects needed, or described several potential alternatives. They extended narrative planning\ndomains by two types of principled mechanisms to operationalize narrative action and object\nsubstitution during narrative plan authoring. An original domain model can be extended with the\naddition generated by two mechanisms alternately.\n3.3\nChallenges and future prospects\nIn AI planning, rules implied in states and actions are enforcedly constrained by preconditions\nand effects. Compared with AI planning, rules between events, such as concurrence, causality, and\nprogression, in natural language processing are more flexible, which often intricately correlate\nwith others. Moreover, the preconditions and effects are implied in natural language, which are\nabstract and hard to be modeled by propositions. In the other hand, in natural language text, a event\ncan be written in different words, and a word owns various meanings. It therefore is difficult to\ndirectly distinguish them by only parsers. Those challenges make it hard to extract detailed logical\nrelations implied in natural language, let along model implied relations by constructing structural\ndomain models. It might be interesting to dig out clear logical relations implied in natural language\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n10\nJin et al.\ntext, which can lay a foundation for natural language processing tasks, such as explainability and\ncontrollable text generation.\n4\nPLANNING-BASED NATURAL LANGUAGE PROCESSING\nIn this section, we introduce three natural language tasks integrated with AI planning, including\ntext generation, text summarization, and machine translation. Planning-based natural language\nprocessing tasks concern about reasonability and coherence, making use of the power of AI planning\nabout reasoning about rules and relations.\n4.1\nPlanning-based text generation\nOne important field combined with AI planning is text generation, in which there have been\nsignificant advances, recently. Text generation asks models to generate coherent and interesting\ntext based on preceding parts of the text, topics, titles, or themes, requiring agents to be capable\nof generating valid and clear logical frameworks. AI planning is one of crucial steps to guide\nmodels to generate well-organized long texts, owing to its power in learning domain models\nand computing solutions for goal-driven tasks. In this section, we introduce planning-based text\ngeneration methods with respect to the following two features:\n• Symbolic planning text generation combines text generation with a classical planning\nframework, taking prior knowledge, e.g., domain models formalized by planning domain\ndescription language, as extra inputs.\n• Neural planning text generation are neural generators combined learning with a skeleton\nplanning, to make up for the difficulties in building hand-crafted domain models.\n4.1.1\nSymbolic planning text generation. In order to generate coherent text with correct logic, it is\nnatural to give agents some prior knowledge about basic rules between events. On the other hand,\nearly rule-based researches [9, 21, 83] about natural language processing explore constructing\nrepresentations of texts and combine with hand-crafted rules. It, however, is hard and tedious to\nenumerate all rules. Therefore, making use of AI planning to capture implied rules in the form of\ndomain models with symbolic representations and compute proper skeletons is a natural way, which\ncan overcome the difficulties of manually constructing rules [36, 56, 66]. For example, Porteous\net al. [86] proposed an approach injecting narrative control into plan generation through the use\nof PDDL [71] state trajectory constraints, to express narrative control information within the\nplanning representation. They constructed constraint trees according to input domain models, and\ninjected control into automatically generated narratives system. With the help of constraints, the\napproach decomposes problems into sets of smaller subproblems using the temporal orderings\ndescribed by the constraints, and solves subproblems incrementally by a planner. Intentional Partial\nOrder Causal Link (IPOCL) planning framework [90] is an extension of classical planning, it aims\nat finding a sound and believable sequence of character actions that transforms an initial state\ninto a state arriving goals. IPOCL does not only create causally sound plot progression, but also\nreasons about character intentionality by identifying possible character goals that explain their\nactions and creating plans that explain why those characters commit to their goals. Compared to\nIPOCL, CPOCL [115] preserves the conflicting subplans without damaging the causal soundness of\nthe overall story to generate interesting stories. CPOCL is an extension of IPOCL that explicitly\ncaptures how characters can thwart one another in pursuit of their goals, which is the essence of\nnarrative conflicts. Making use of hand-crafted, well-defined domain models, symbolic planning\ntext generation methods have the ability to produce impressive results in limited domains.\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n11\n4.1.2\nNeural planning text generation. However, it is often tedious or difficult to build domain mod-\nels by hand due to the high requirements of manual efforts and domain knowledge. Automatically\nlearning domains and constructing storylines have significantly attracted researchers’ attention\nrecently [88, 104].\nTo automatically learn domain models for helping generate coherent and valid stories, Li et\nal. [60] used a crowd-sourced corpus of stories to learn plot graphs that can then be used as\nconstrained search spaces for sequences of story events, instead of relying on priori domain\nmodels. Specifically, the approach crowdsources a corpus of narrative examples of a new domain,\nautomatically constructs domain models capturing different possible, non-contradictory story\ntrajectories, and samples from the space of stories allowed by the domain model according to\nsome story quality criteria. During the plot graph learning, learning mutual exclusion relations\nand optional events lets the generated story be coherent. C2PO [2] learns a branching story graph\nstructure that can be searched, and introduces soft causal relations as causal relations inferred\nfrom commonsense reasoning. It creates a branching space of possible story continuations that\nbridge between plot points that are automatically extracted from existing natural language plot\nsummaries.\nAnother way for constructing valid line of text is to construct storylines in advance, which can be\nskeletons, or sequences of keywords, key phrases, or contents. Xu et al. [118] generated skeletons\ncomposed of phrases learned by a reinforcement learning method, and then expanded skeletons\nto complete and fluent sentences. Fan et al. [31] proposed a novel approach which first generates\nplans in the form of predicate-argument structures, then generates stories with placeholder tokens\nto indicate entities, and finally replaces tokens by entities based on the global story contexts. The\ninputs of the task are short descriptions of scenes or events, and the approach outputs relevant\nnarrative stories following the inputs.\nInstead of generating skeletons with detailed prompts, some approaches first plan out storylines,\nwhich enable them to generate controllable stories with goals. [119] proposed a hierarchical\ngeneration framework that first planned a controllable storyline composed of keywords towards a\ngoal (i.e., a title), and then generated a story based on the storyline. The RAKE algorithm [91] takes\neach sentence as an input and combines several word frequency based and graph-based metrics to\nweight the importance of the words. The approach regards the most important word as the keyword.\nThe storyline is planned out based on the title, previously generated sentences, and the previous\nkeywords in the storyline. In the experiments, they explore two strategies, dynamic schema and\nstatic schema. Results show the static schema performs better than the other one because it plans\nthe storyline holistically, thus tends to generate more coherent and relevant stories. Similarly,\n[57] made use of a related framework, which first plans out storyline composed of a sequence\nof keywords and then generates the whole story, to handle stylized story generation. Stylized\nstory generation is to generate stories with specified style given a leading context. Keywords are\nselected following some emotion-driven style, such as “fear”, “anger”, and “surprsie”. According\nto the stylized keywords, the approach can generate generates the whole stylized story with the\nguidance of the keywords. Yu et al. [123] followed Yao et al. [119] and used the RAKE algorithm to\nextract keywords to train a generation model for conducting keyword planning given story titles\nas inputs. Then the approach combines the story titles and the corresponding keywords of each\nstory as the inputs of the graph module to automatically generate a graph for each story. According\nto the keywords, titles, story graphs, the approach encodes them into latent variables and further\ndecodes them to generate the corresponding stories.\nExcept for generating stories according to prompts or goals, DYPLOC [44], a dynamic planning\ngeneration framework, takes a set of content items as inputs, each content item consists of a title, a\nset of entities, and a set of core concepts. To organize unordered content items, DYPLOC introduces\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n12\nJin et al.\na plan scoring network, which learns to dynamically select and order contents based on what has\nbeen produced previously while generating the outputs.\n4.1.3\nChallenges and future prospects. Recently, text generation [49, 51, 124] has gained lots\nof attraction, aiming at letting intelligent agents express like humans. Numbers of methods to\ngenerate coherent texts have been proposed in recent years. Generating logical and controllable\ntexts, however, is still a challenging task. AI planning is one of critical ways to enable agents to\ngenerate logical and controllable texts. Compared with symbolic planning text generation methods\nand neural planning text generation methods, the former is more capable of generating logical\nstorylines with goals, and the texts generated by the latter are more diverse and coherent. Specially,\nsymbolic planning methods can generate more explainable storylines, which is still challenging\nfor deep learning methods. In general, it would be interesting to combine both for generating\nlogical, controllable, and coherent text with diversity. Although there have been some approaches\n[31, 96, 109] proposed, based on the syntax of plans or representation of structure in AI planning,\nthey use neural networks to predict unseen events instead of speculating based on logical relations\nimplied. We hold the opinion that, compared with only learning blackbox neural models with\nimplicit rules, appropriately combined with explicit logical relations would be a new attempt, which\nmaybe contribute to natural language processing tasks.\n4.2\nPlanning-based Text Summarization\nText summarization is to extract important information from texts and generate new texts based\non those information in the form of summarizes. It requires agents to understand texts, filter\ninformation from abundant descriptions and organize them to form summarizes, which is one of\nthe most researched areas among the NLP community. Text summarization can be categorized into\nextractive and abstractive techniques. Extractive summarization aims at selecting subsets of words\nor sentences from input articles to summarize them. Abstractive summarization takes articles as\ninputs, tries to understand the texts and generate summarizes. In recent years, some researchers try\nto integrate text summarization models with AI planning. The combination of AI planning and text\nsummarization is mostly based on deep learning abstractive summarization, making use of content\nplanning which describes or predicts skeletons of articles. Planning-based text summarization\nmethods first plan out skeletons of summarizies or compute probability distributions, and then\ngenerate the whole sentences based on the skeletons or predictions. For example, Narayan et al.\n[77] first computed plans in the form of entity chains, which are ordered sequences of entities, and\nthen generated summaries conditioned on the plans. Marfurt et al. [69] proposed an abstractive\nsummarization model implemented with a planning step, done by a hierarchical decoder, which first\nplans out an outline for the next sentence in the form of sentence representations and generates\nwords according to the representations. Amplayo et al. [3] incorporated content planning in\nunsupervised summarization and datasets creation. They predicted aspect and sentiment probability\ndistributions as content plans and generated sentences according to the predictions. During creating\ndatasets, they made use of the distributions parametrized by the content planner to control the\nstructures of created datasets.\nIn current planning-based text summarization approaches, content planning is based on deep\nlearning which learns models and probability distributions to predict. However, only fitting deep\nlearning models based on representations of sentences and words is hard to capture the relations\nimplied. We believe that it would be interesting and challenging to let content planning modules\nunderstand relations implied in texts, which will allow content plans to be more coherent and\ncontrollable. On the other hand, although some mainstream text summarization methods are not\nbased on AI planning, there are some commonalities between them. For example, tree-based and\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n13\ngraph-based text summarization approaches[4, 8, 58] first find the most important information from\nthe text and then use trees and graphs to create summaries. Those structures aim at representing the\nrelations between sentences, which is similar to the relations between actions in AI planning but at\na more abstract level. Secondly, some text summarization methods try to obtain important words\nfrom sentences, such as verbs, objects, and subjects, to represent sentences semantically [1]. These\nforms have similarities with the structured representations in AI planning such as propositions\nand first-order predicates. Thirdly, ontology-based text summarization methods [74, 117] collect\nentities and their relationships, which reminds us of domain models in AI planning. We believe\nthat there is a vast scope for researchers to combine AI planning and text summarization.\n4.3\nPlanning-based Machine Translation\nMachine translation aims at automatically translating content from source language to another\ntarget language, having a long history. One way to understand source texts and generate target\ntexts is to combine neural language models with planning phases, i.e., first generating skeletons,\nand extending them by target languages. For example, Gülçehre et al. [41] integrated an auto-\nencoder with a planning mechanism, the auto-encoder first encodes texts by sequences of vector\nrepresentations and decodes representations by generating target translation character-by-character.\nSpecifically, they first created plans ahead in the form of action matrices, which are sequences of\nprobability distributions, and made use of commitment plan vectors to govern whether to recompute\nplans or use them. Then they computed soft alignments based on the plan and generated texts in\ntarget language at each time-step. Shu and Nakayama [102] combined neural machine translation\nwith a planning phase, which first generates planner codes to disambiguate uncertain information\nabout the sentence structure and control the structure of output sentences. Bahdanau et al. [7]\nused actor-critic methods from reinforcement learning (RL) to generate sequences. They showed\nthat sequences can be used in machine translation tasks, gaining better translation performance.\nThose approaches first produce big pictures of output texts by planning, then generate complete\nsentences conditioned on plans. They take advantages of capturing implied rules to generate more\naccurate and coherent target texts.\nIn natural language texts, the transitions between sentences imply relations and rules. Machine\ntranslation tasks do not only need to understand word-level structures of sentences, but also need\nto capture sentence-level relationships. Only relying on word-by-word text generation is hard and\nchallenging to generate coherent and logical texts, especially when generating accurate transitions\nbetween sentences. Current approaches to capture implied rules and generate plans are mostly based\non neural black-box models, lacking the explainability of making decisions. Previous researches\nabout rule-based translation are explainable, they are based on hand-crafted rules. Although those\nrules are explicit and accurate, it is hard to manually write rules and the rules are not scalable.\nProducing rules are time-consuming and tedious. However, it would be interesting if we regard it as\naction models learning in planning community, which structurally formalize rules by preconditions\nand effects. We believe that combining rule-based machine translation with planning and neural\nmachine translation may spark new ideas.\n5\nPLANNING-BASED EXPLAINABILITY\nNowadays, although deep learning approaches have been widely used in AI fields, human cannot\nunderstand practical meaning inside black-box neural models. Differently, AI planning is able to\noffer explicit knowledge, in the form of first-order logic, domain models, etc, implied in natural\nlanguage. Therefore, the combination of natural language and AI planning may enable AI systems\nto be able to explain their reasoning to humans, which meets the needs for AI systems to work\nsynergistically with humans. Those systems require agents to be aware of the intentions, capabilities\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n14\nJin et al.\nLack \nexplainability\nOffer rules, \nstorylines,\naction sequences,\ndomain models\nNeural \nAI systems\nHuman\nPlanning-based\nAI systems\nHuman-\nunderstandable \nsymbolic interface\nGive feedbacks, \nsuch as rewards \nand punishments\nAnnotate, \nanswer, or \nexplain\nGive human-\nunderstandable \nqueries\nQuery\nCombine with\nAI planning\nFig. 5. Two types of making neural AI systems be explainable: (1) integrating AI systems with planning\ntechniques; (2) making use of human-understandable interface.\nand mental model of the human in the loop during its decision process. As shown in Figure 5,\nbesides extracting action sequences [32, 68] and building domain models [14, 64] mentioned\nabove, another way for making AI system explainable is to construct human-understandable\nsymbolic interfaces (cf. [53]). A human-understandable symbolic interface is not only developed\nfor its own computational efficiency, but also beneficial to humans. EXPAND system [40] and\nSERLfD framework [125], respectively. EXPAND system accelerates Human-in-the-Loop deep\nreinforcement learning by using human evaluative feedback and visual explanation. SERLfD uses\nself-explanation to recognize valuable high-level relational features as an interpretation of why a\nsuccessful trajectory is successful, allowing SERLfD to guide itself and improve the efficiency.\n6\nTEXT-BASED HUMAN-ROBOT INTERACTION\nThe rapid developments of artificial intelligence let robots move out from industrial environments,\nand enter the daily life of humans, such as homes and hospitals. It requires robots to be able to\nrespond quickly and effectively to rapidly-changing conditions and expectations. Language–based\ncommunication is the most natural method for humans to communicate with others, so natural\nlanguage is a good candidate to be robot instruction for human-robot interaction. In this section,\nwe will introduce text-based human-robot interaction from three aspects: extracting actions from\nnatural language commands, natural language command understanding, and dialogue generation,\nas shown in the Figure 6.\n6.1\nExtracting actions from natural language command\nOne of important tasks in text-based human-robot interaction is to extract actions from natural\nlanguage commands [48, 79, 85, 111]. Extracting actions from natural language commands is\nsimilar to action extraction in Section 3.1. Differently, extracting actions from natural language\ncommands is not only to capture important words to indicate sentences, but also to understand\nimplied rules and generate action sequences to guide robots to achieve tasks. Some approaches\nmake use of with language descriptions, then build models that map language commands to\naction sequences. Tellex et al. [110] introduced a system, Generalized Grounding Graphs (G3),\ntaking a natural language command as input and outputting a plan for the robot. G3 instantiates a\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n15\nHuman\nRobot\nNatural language \ncommands\nNatural language \ncommands\nNatural language \ncommands\nAction \nsequences\nAction models \n(understand \nimplied \nintentions)\nPlanning-based \ndialogue systems\nExtract actions \nExecute\nCapture \nimplied rules and \nintentions\nCommunicate\nGenerate \nutterances\nGive feedbacks\nUnderstand new \ncommands\nFig. 6. Relations of researches in human-robot interaction.\nprobabilistic graphical model for a particular natural language command according to hierarchical\nand compositional semantic structure of the command. Cantrell et al. [20] presented a robotic\narchitecture equipping with a planner that uses newly discovered information to produce new and\nupdated plans, specifically information originating in spoken input produced by human operators.\nThe robot can learn action sequences with defined preconditions and effects from natural language\ndescriptions, and immediately apply this knowledge to improve planning. On the other hand, some\napproaches [16, 120] focus on temporal logic between natural language commands, aiming at\nhandling semantic disambiguation of natural language.\n6.2\nNatural Language Command Understanding\nAnother important task is to enable agents to understand natural language commands given by\nhumans, which requires agents to understand natural language commands and capture implied rules\n[10, 10, 12, 25, 25, 99], or learn new actions based on natural language commands and dialogues[35,\n75, 95, 101]. To understand natural language commands, Thomason et al. [113] introduced a dialog\nagent to understand human natural language commands through semantic parsing, actively resolve\nambiguities using a dialog manager, and incrementally learn from human-robot conversations. The\nagent employs incremental learning of a semantic parser from conversations on a mobile robot.\nIt is implemented and tested both on a web interface with hundreds of users and on a mobile\nrobot over several days, tasked with understanding navigation and delivery requests through\nnatural language in an office environment. Brawer et al. [15] presented a framework for effectively\ngrounding situated and natural language to action selection during human-robot interaction. It\nintegrates verbal commands from a human partner with contextual information in the form of a\ntask model. The approach is capable of acquiring and deploying new task representations from\nlimited and natural language data sets, and without any prior domain knowledge of language\nor the task itself. Moreover, understanding action models and predicting next actions are widely\nresearched for robotic tasks and navigation tasks with natural language commands [24, 26, 112].\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n16\nJin et al.\nOn the other hand, another challenge in natural language command understanding tasks is\nto learn new actions when facing unknown natural language commands [6, 100]. Cantrell et al.\n[19] introduced an algorithm, to learn meanings of action verbs through dialogue-based natural\nlanguage descriptions and integrated it in the robot’s natural language subsystem. The algorithm\nallows robots to perform the actions associated with the learned verb meanings right away without\nany additional help or learning trials. Moreover, it allows human to interact with a robot to explain\nnew action words in natural language, and lets the robot be able to perform the new action and\nstore the procedural knowledge for future usage. Learning by Instruction Agent (LIA) [5] was\nproposed to learn new commands by natural language interaction with human. When facing a\nnew natural language command that LIA does not understand, it prompts users to explain how\nto achieve the command through a sequence of natural language steps. LIA interprets commands\nusing a semantic parser that maps each command to a logical form, which contains one or more of\nfunctions and predicates.\nUndoubtedly, letting agents understand natural language commands and infer about next actions\nis an important but challenging task. An agent does not only need to capture rules implied in\nutterances, but also need to distinguish sentences described in different ways. We believe that the\ncombination of planning-based natural language processing and human-robot interaction would\ncreate something interesting for natural language commands understanding tasks.\n6.3\nDialogue Systems\nDialogue systems [23, 67] have been a bridge between human and robots, which interacts with\nhuman in natural language. AI planning is one of crucial mechanisms used in dialogue systems\nto recognize the intentions conveyed in dialogues [18, 76]. Planning-based dialogue systems take\nadvantage of the power of capturing and expressing rules and use it to manage utterances or guide\nthe generation. Rules, plans and intentions offer proper logical forms which derive appropriate\ncommunication acts in dialogue systems [28]. Plan-based model [27, 93] were proposed to manage\nthe intentions and information implied in dialogues. Those models describe the common activities\nand relations between utterances, and can be used in the following generation processes. However,\nplanning-based dialogue systems are still in early stages [28, 84], which facing the challenges\nof complex representations in open-domain, difficulties of manually constructing models and\nlimitations of scalability of dialogue models. Nevertheless, we believe that planning could be a\nstrong suit for dialogue systems by integrating with automatically domain models learning and\nsearching strategies, especially for controlled dialogue generation. Moreover, we are interested in\nthe explainability of planning-based dialogue systems, it would be interesting to know the reason\nfor intentions generation.\n7\nAPPLICATIONS\nIn this section, we introduce some reality applications based on combinations of AI planning and\nnatural language processing. AI planning is widely used in reality management systems, such as\nlogistics management, workshop schedule, and reservoir operation. Moreover, natural language\nprocessing (NLP) helps human communicate with agents, the combination of AI planning and NLP\nenables applications to be found in many fields, such as emergency managements [11, 29, 29, 33]\nand urban planning [63, 63, 89]. For example, the Urban Redevelopment Authority (URA) Centre in\nSingapore 3 deployed Robotic Process Automation and NLP to help conduct operations for resource\noptimization. The combination allows routine tasks to make use of AI planning and NLP, such\nas chatbots for public queries, which capture information from large datasets, analyze textual\n3https://www.ura.gov.sg/Corporate/Resources/Ideas-and-Trends/AI-in-Urban-Planning\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n17\nfeedback, make planning decisions, and respond intelligently. On the other hand, the navigation\nsystems in daily use, such as Baidu Maps 4 and Google Maps 5, combine making decisions and\nNLP techniques, which plan out routes with different objectives according to goals, and generate\nnatural language suggestions to guide human. Moreover, agents learn from human commands and\nnavigation datasets, helping agents understand human behaviors [45, 73].\n8\nCONCLUSION\nIn this paper, we consider that AI planning and natural language processing have strong ties, and we\nintroduce recent works about four related tasks, i.e., planning-based text understanding, planning-\nbased natural language processing, planning-based explainability, and text-based human-robot\ninteraction. We first introduce backgrounds about AI planning and natural language processing\nand discuss commons between them, as well as their abilities to generate explicit knowledge,\ne.g., domain models, and learning from tacit knowledge, e.g., neural models. We then introduce\nmethods of planning-based text understanding by extracting action sequences from texts and\nlearning domain models from texts. Next, we give an overview of planning-based natural language\nprocessing about text generation, text summarization, and machine translation. Then, we introduce\nrecent works in planning-based explainability and text-based human-robot interaction.\nWith this paper, we aim to provide a high-level view of AI planning and natural language\nprocessing for further studies, about integrating them for a combination of explicit and tacit\nknowledge. Combining learning from tacit knowledge and using explicit knowledge in a fully\nprincipled way is an open problem, although there are non-negligible relations between AI planning\nand natural language processing, allowing each of them can effectively impact the other one.\nHowever, there is not enough communication between these two fields. While many advances have\nbeen made in natural language processing by using AI planning algorithms, a significant amount\nof research is still required to understand the implied knowledge hidden in texts. Meanwhile,\nimproving the ability to describe environments by domain models and solve large-scale planning\nproblems is also beneficial to understanding texts and generating coherent and interesting texts.\nWe believe that integrating AI planning and natural language processing, a complex combination of\nexplicit and tacit knowledge, is a promising research area, which can improve the communication\nbetween human and intelligent agents.\nREFERENCES\n[1] S Alshaina, Ansamma John, and Aneesh G Nath. 2017. Multi-document abstractive summarization based on predicate\nargument structure. In 2017 IEEE International Conference on Signal Processing, Informatics, Communication and Energy\nSystems (SPICES). IEEE, 1–6.\n[2] Prithviraj Ammanabrolu, Wesley Cheung, William Broniec, and Mark O. Riedl. 2021. Automated Storytelling\nvia Causal, Commonsense Plot Ordering. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium\non Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. 5859–5867. https:\n//ojs.aaai.org/index.php/AAAI/article/view/16733\n[3] Reinald Kim Amplayo, Stefanos Angelidis, and Mirella Lapata. 2021. Unsupervised Opinion Summarization with\nContent Planning. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference\non Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. 12489–12497. https://ojs.aaai.org/index.php/AAAI/\narticle/view/17481\n[4] Mozhgan Nasr Azadani, Nasser Ghadiri, and Ensieh Davoodijam. 2018. Graph-based biomedical text summarization:\nAn itemset mining and sentence clustering approach. J. Biomed. Informatics 84 (2018), 42–58. https://doi.org/10.1016/\nj.jbi.2018.06.005\n4https://map.baidu.com/\n5https://www.google.de/maps\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n18\nJin et al.\n[5] Amos Azaria, Jayant Krishnamurthy, and Tom M. Mitchell. 2016. Instructable Intelligent Personal Agent. In Proceedings\nof the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, Dale Schuurmans\nand Michael P. Wellman (Eds.). 2681–2689. http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12383\n[6] Amos Azaria, Shashank Srivastava, Jayant Krishnamurthy, Igor Labutov, and Tom M. Mitchell. 2020. An agent for\nlearning new natural language commands. Auton. Agents Multi Agent Syst. 34, 1 (2020), 6. https://doi.org/10.1007/\ns10458-019-09425-x\n[7] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C. Courville,\nand Yoshua Bengio. 2017. An Actor-Critic Algorithm for Sequence Prediction. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\nhttps:\n//openreview.net/forum?id=SJDaqqveg\n[8] Siddhartha Banerjee, Prasenjit Mitra, and Kazunari Sugiyama. 2015. Multi-Document Abstractive Summarization\nUsing ILP Based Multi-Sentence Compression. In Proceedings of the Twenty-Fourth International Joint Conference on\nArtificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, Qiang Yang and Michael J. Wooldridge\n(Eds.). 1208–1214. http://ijcai.org/Abstract/15/174\n[9] Robert Baud, Christian Lovis, Laurence Alpay, Anne-Marie Rassinoux, JR Scherrer, Anthony Nowlan, and Alan\nRector. 1993. Modelling for natural language understanding.. In Proceedings of the Annual Symposium on Computer\nApplication in Medical Care. American Medical Informatics Association, 289.\n[10] Yonatan Bisk, Kevin J. Shih, Yejin Choi, and Daniel Marcu. 2018. Learning Interpretable Spatial Operations in a Rich\n3D Blocks World. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th\ninnovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in\nArtificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q.\nWeinberger (Eds.). 5028–5036. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17410\n[11] Simon Blindheim, Sebastien Gros, and Tor Arne Johansen. 2020. Risk-based model predictive control for autonomous\nship emergency management. IFAC-PapersOnLine 53, 2 (2020), 14524–14531.\n[12] Valts Blukis, Dipendra Kumar Misra, Ross A. Knepper, and Yoav Artzi. 2018. Mapping Navigation Instructions to\nContinuous Control Actions with Position-Visitation Prediction. In 2nd Annual Conference on Robot Learning, CoRL\n2018, Zürich, Switzerland, 29-31 October 2018, Proceedings (Proceedings of Machine Learning Research, Vol. 87). 505–518.\nhttp://proceedings.mlr.press/v87/blukis18a.html\n[13] S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, and Regina Barzilay. 2009. Reinforcement Learning for Mapping\nInstructions to Actions. In ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational\nLinguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP, 2-7 August 2009,\nSingapore, Keh-Yih Su, Jian Su, and Janyce Wiebe (Eds.). 82–90. https://aclanthology.org/P09-1010/\n[14] S. R. K. Branavan, Nate Kushman, Tao Lei, and Regina Barzilay. 2012. Learning High-Level Planning from Text. In\nThe 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14,\n2012, Jeju Island, Korea - Volume 1: Long Papers. 126–135. https://aclanthology.org/P12-1014/\n[15] Jake Brawer, Olivier Mangin, Alessandro Roncone, Sarah Widder, and Brian Scassellati. 2018. Situated Human-\nRobot Collaboration: predicting intent from grounded natural language. In 2018 IEEE/RSJ International Conference on\nIntelligent Robots and Systems, IROS 2018, Madrid, Spain, October 1-5, 2018. 827–833. https://doi.org/10.1109/IROS.\n2018.8593942\n[16] Igor Buzhinsky. 2019. Formalization of natural language requirements into temporal logics: a survey. In 17th\nIEEE International Conference on Industrial Informatics, INDIN 2019, Helsinki, Finland, July 22-25, 2019. 400–406.\nhttps://doi.org/10.1109/INDIN41052.2019.8972130\n[17] Erik Cambria and Bebo White. 2014. Jumping NLP curves: A review of natural language processing research. IEEE\nComputational intelligence magazine 9, 2 (2014), 48–57.\n[18] Guy Camilleri. 2002. Dialogue systems and planning. In International Conference on Text, Speech and Dialogue. Springer,\n429–436.\n[19] Rehj Cantrell, Paul W. Schermerhorn, and Matthias Scheutz. 2011. Learning actions from human-robot dialogues. In\n20th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN 2011, Atlanta, Georgia,\nUSA, July 31 - August 3, 2011, Henrik I. Christensen (Ed.). 125–130. https://doi.org/10.1109/ROMAN.2011.6005199\n[20] Rehj Cantrell, Kartik Talamadupula, Paul W. Schermerhorn, J. Benton, Subbarao Kambhampati, and Matthias Scheutz.\n2012. Tell me when and why to do it!: run-time planner model updates via natural language instruction. In HRI,\nHolly A. Yanco, Aaron Steinfeld, Vanessa Evers, and Odest Chadwicke Jenkins (Eds.). 471–478. https://doi.org/10.\n1145/2157689.2157840\n[21] Nick Cercone and Gordon McCalla. 1986. Accessing knowledge through natural language. In Advances in Computers.\nVol. 25. Elsevier, 1–99.\n[22] David L. Chen and Raymond J. Mooney. 2011. Learning to Interpret Natural Language Navigation Instructions from\nObservations. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco,\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n19\nCalifornia, USA, August 7-11, 2011, Wolfram Burgard and Dan Roth (Eds.). http://www.aaai.org/ocs/index.php/AAAI/\nAAAI11/paper/view/3701\n[23] Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A Survey on Dialogue Systems: Recent Advances\nand New Frontiers. SIGKDD Explor. 19, 2 (2017), 25–35. https://doi.org/10.1145/3166054.3166058\n[24] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2019.\nTOUCHDOWN: Nat-\nural Language Navigation and Spatial Reasoning in Visual Street Environments. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. 12538–\n12547.\nhttp://openaccess.thecvf.com/content_CVPR_2019/html/Chen_TOUCHDOWN_Natural_Language_\nNavigation_and_Spatial_Reasoning_in_Visual_Street_CVPR_2019_paper.html\n[25] Haonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, and Ron Alterovitz. 2020. Enabling Robots to Understand\nIncomplete Natural Language Instructions Using Commonsense Reasoning. In 2020 IEEE International Conference\non Robotics and Automation, ICRA 2020, Paris, France, May 31 - August 31, 2020. 1963–1969. https://doi.org/10.1109/\nICRA40945.2020.9197315\n[26] Ta-Chung Chi, Minmin Shen, Mihail Eric, Seokhwan Kim, and Dilek Hakkani-Tür. 2020. Just Ask: An Interactive\nLearning Framework for Vision and Language Navigation. In The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020. 2459–2466. https://ojs.aaai.org/index.php/AAAI/article/view/5627\n[27] Jennifer Chu-Carroll and Sandra Carberry. 1994. A plan-based model for response generation in collaborative\ntask-oriented dialogues. arXiv preprint cmp-lg/9405011 (1994).\n[28] Philip R Cohen. 2020. Back to the future for dialogue research. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 13514–13519.\n[29] Daniel G Costa, João Paulo J Peixoto, Thiago C Jesus, Paulo Portugal, Francisco Vasques, Elivelton Rangel, and Maycon\nPeixoto. 2022. A Survey of Emergencies Management Systems in Smart Cities. IEEE Access (2022).\n[30] Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, and Min Yang. 2021. A Survey of\nNatural Language Generation. arXiv preprint arXiv:2112.11739 (2021).\n[31] Angela Fan, Mike Lewis, and Yann N. Dauphin. 2019. Strategies for Structuring Story Generation. In Proceedings\nof the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). 2650–2660.\nhttps:\n//doi.org/10.18653/v1/p19-1254\n[32] Wenfeng Feng, Hankz Hankui Zhuo, and Subbarao Kambhampati. 2018. Extracting Action Sequences from Texts\nBased on Deep Reinforcement Learning. In IJCAI. 4064–4070.\n[33] Daniela Fogli and Giovanni Guida. 2013. Knowledge-centered design of decision support systems for emergency\nmanagement. Decision Support Systems 55, 1 (2013), 336–347.\n[34] Maria Fox and Derek Long. 2002. PDDL+: Modeling continuous time dependent effects. In Proceedings of the 3rd\nInternational NASA Workshop on Planning and Scheduling for Space, Vol. 4. 34.\n[35] Tyler M. Frasca, Bradley Oosterveld, Meia Chita-Tegmark, and Matthias Scheutz. 2021. Enabling Fast Instruction-\nBased Modification of Learned Robot Skills. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium\non Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. 6075–6083. https:\n//ojs.aaai.org/index.php/AAAI/article/view/16757\n[36] Konstantina Garoufi. 2014. Planning-Based Models of Natural Language Generation. Lang. Linguistics Compass 8, 1\n(2014), 1–10. https://doi.org/10.1111/lnc3.12053\n[37] Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks,\napplications and evaluation. Journal of Artificial Intelligence Research 61 (2018), 65–170.\n[38] Christopher W. Geib and Mark Steedman. 2007. On Natural Language Processing and Plan Recognition. In IJCAI\n2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12,\n2007, Manuela M. Veloso (Ed.). 1612–1617. http://ijcai.org/Proceedings/07/Papers/260.pdf\n[39] Alfonso Gerevini and Derek Long. 2005. Plan constraints and preferences in PDDL3. Technical Report. Technical\nReport 2005-08-07, Department of Electronics for Automation ....\n[40] Lin Guan, Mudit Verma, Sihang Guo, Ruohan Zhang, and Subbarao Kambhampati. 2021. Widening the Pipeline in\nHuman-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation. Advances in\nNeural Information Processing Systems 34 (2021).\n[41] Çaglar Gülçehre, Francis Dutil, Adam Trischler, and Yoshua Bengio. 2017. Plan, Attend, Generate: Character-Level\nNeural Machine Translation with Planning. In Proceedings of the 2nd Workshop on Representation Learning for NLP,\nRep4NLP@ACL 2017, Vancouver, Canada, August 3, 2017, Phil Blunsom, Antoine Bordes, Kyunghyun Cho, Shay B.\nCohen, Chris Dyer, Edward Grefenstette, Karl Moritz Hermann, Laura Rimell, Jason Weston, and Scott Yih (Eds.).\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n20\nJin et al.\n228–234. https://doi.org/10.18653/v1/w17-2627\n[42] Thomas Hayton, Julie Porteous, João Fernando Ferreira, and Alan Lindsay. 2020. Narrative Planning Model Acquisition\nfrom Text Summaries and Descriptions. In AAAI. 1709–1716.\n[43] Daniel Hládek, Ján Staš, and Matúš Pleva. 2020. Survey of automatic spelling correction. Electronics 9, 10 (2020), 1670.\n[44] Xinyu Hua, Ashwin Sreevatsa, and Lu Wang. 2021. DYPLOC: Dynamic Planning of Content Using Mixed Language\nModels for Text Generation. In Proceedings of ACL. 6408–6423.\n[45] Jizhou Huang, Haifeng Wang, Miao Fan, An Zhuo, Yibo Sun, and Ying Li. 2020. Understanding the Impact of\nthe COVID-19 Pandemic on Transportation-related Behaviors with Human Mobility Data. (2020), 3443–3450.\nhttps://doi.org/10.1145/3394486.3412856\n[46] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language Models as Zero-Shot Planners:\nExtracting Actionable Knowledge for Embodied Agents. 162 (2022), 9118–9147. https://proceedings.mlr.press/v162/\nhuang22a.html\n[47] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor\nMordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,\nand Brian Ichter. 2022. Inner Monologue: Embodied Reasoning through Planning with Language Models. CoRR\nabs/2207.05608 (2022). https://doi.org/10.48550/arXiv.2207.05608 arXiv:2207.05608\n[48] Pham Ngoc Hung and Takashi Yoshimi. 2016. Extracting actions from instruction manual and testing their execution\nin a robotic simulation. ASEAN Engineering Journal 6, 1 (2016), 47–58.\n[49] Touseef Iqbal and Shaima Qureshi. 2020. The survey: Text generation models in deep learning. Journal of King Saud\nUniversity-Computer and Information Sciences (2020).\n[50] Peter A. Jansen. 2020. Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from\nHigh-level Instructions. EMNLP 2020 (2020), 4412–4417. https://doi.org/10.18653/v1/2020.findings-emnlp.395\n[51] HanQi Jin, Yue Cao, TianMing Wang, XinYu Xing, and XiaoJun Wan. 2020. Recent advances of neural text generation:\nCore tasks, datasets, models and challenges. Science China Technological Sciences 63, 10 (2020), 1990–2010.\n[52] Subbarao Kambhampati. 2021. Polanyi’s revenge and AI’s new romance with tacit knowledge. Commun. ACM 64, 2\n(2021), 31–32. https://doi.org/10.1145/3446369\n[53] Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, and Lin Guan. 2021. Symbols as a Lingua\nFranca for Bridging Human-AI Chasm for Explainable and Advisable AI Systems. CoRR abs/2109.09904 (2021).\narXiv:2109.09904 https://arxiv.org/abs/2109.09904\n[54] Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. 2022. Natural language processing: State of the art,\ncurrent trends and challenges. Multimedia Tools and Applications (2022), 1–32.\n[55] Joohyun Kim and Raymond J. Mooney. 2012. Unsupervised PCFG Induction for Grounded Language Learning with\nHighly Ambiguous Supervision. In EMNLP-CoNLL, Jun’ichi Tsujii, James Henderson, and Marius Pasca (Eds.). 433–444.\nhttps://aclanthology.org/D12-1040/\n[56] Alexander Koller and Jörg Hoffmann. 2010. Waking Up a Sleeping Rabbit: On Natural-Language Sentence Generation\nwith FF. In Proceedings of the 20th International Conference on Automated Planning and Scheduling, ICAPS 2010, Toronto,\nOntario, Canada, May 12-16, 2010, Ronen I. Brafman, Hector Geffner, Jörg Hoffmann, and Henry A. Kautz (Eds.).\nAAAI, 238–241. http://www.aaai.org/ocs/index.php/ICAPS/ICAPS10/paper/view/1415\n[57] Xiangzhe Kong, Jialiang Huang, Ziquan Tung, Jian Guan, and Minlie Huang. 2021. Stylized Story Generation with\nStyle-Guided Planning. In ACL-IJCNLP. 2430–2436.\n[58] Litton J Kurisinkel, Yue Zhang, and Vasudeva Varma. 2017. Abstractive multi-document summarization by partial\ntree extraction, recombination and linearization. In Proceedings of the Eighth International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers). 812–821.\n[59] Anastassia Küstenmacher and Paul G Plöger. 2021. Improving the Reliability of Service Robots by Symbolic Represen-\ntation of Execution Specific Knowledge. In Robust and Reliable Autonomy in the Wild (R2AW).\n[60] Boyang Li, Stephen Lee-Urban, George Johnston, and Mark Riedl. 2013. Story Generation with Crowdsourced Plot\nGraphs. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, July 14-18, 2013, Bellevue,\nWashington, USA, Marie desJardins and Michael L. Littman (Eds.). http://www.aaai.org/ocs/index.php/AAAI/AAAI13/\npaper/view/6399\n[61] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek,\nAnima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. 2022. Pre-Trained Language\nModels for Interactive Decision-Making. CoRR abs/2202.01771 (2022). arXiv:2202.01771 https://arxiv.org/abs/2202.\n01771\n[62] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. 2021. Terapipe:\nToken-level pipeline parallelism for training large-scale language models. In International Conference on Machine\nLearning. PMLR, 6543–6552.\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n21\n[63] Claudius Lieven, Bianca Lüders, Daniel Kulus, and Rosa Thoneick. 2021. Enabling digital co-creation in urban\nplanning and development. (2021), 415–430.\n[64] Alan Lindsay, Jonathon Read, João F. Ferreira, Thomas Hayton, Julie Porteous, and Peter Gregory. 2017. Framer:\nPlanning Models from Natural Language Action Descriptions. In ICAPS. 434–442.\n[65] Adam Lopez. 2008. Statistical Machine Translation. ACM Comput. Surv. 40, 3, Article 8 (aug 2008), 49 pages.\nhttps://doi.org/10.1145/1380584.1380586\n[66] Stephanie M Lukin and Marilyn A Walker. 2019. A narrative sentence planner and structurer for domain independent,\nparameterizable storytelling. Dialogue & Discourse 10, 1 (2019), 34–86.\n[67] Longxuan Ma, Mingda Li, Wei-Nan Zhang, Jiapeng Li, and Ting Liu. 2022. Unstructured Text Enhanced Open-Domain\nDialogue System: A Systematic Survey. ACM Trans. Inf. Syst. 40, 1 (2022), 9:1–9:44. https://doi.org/10.1145/3464377\n[68] Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the Talk: Connecting Language, Knowledge,\nand Action in Route Instructions. In AAAI. 1475–1482. http://www.aaai.org/Library/AAAI/2006/aaai06-232.php\n[69] Andreas Marfurt and James Henderson. 2021. Sentence-level Planning for Especially Abstractive Summarization. In\nProceedings of the Third Workshop on New Frontiers in Summarization. 1–14.\n[70] Cynthia Matuszek, Dieter Fox, and Karl Koscher. 2010. Following directions using statistical machine translation. In\nHRI, Pamela J. Hinds, Hiroshi Ishiguro, Takayuki Kanda, and Peter H. Kahn Jr. (Eds.). 251–258. https://doi.org/10.\n1145/1734454.1734552\n[71] Drew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock, Ashwin Ram, Manuela M. Veloso, Daniel S.\nWeld, and David E. Wilkins. 1998. PDDL-the planning domain definition language.\n[72] Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. Listen, Attend, and Walk: Neural Mapping of Navigational\nInstructions to Action Sequences. In Thirtieth AAAI Conference on Artificial Intelligence, Dale Schuurmans and\nMichael P. Wellman (Eds.). 2772–2778. http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12522\n[73] Shah Jahan Miah, Huy Quan Vu, and Damminda Alahakoon. 2022. A social media analytics perspective for human-\noriented smart city planning and management. Journal of the Association for Information Science and Technology 73, 1\n(2022), 119–135.\n[74] M Jishma Mohan, C Sunitha, Amal Ganesh, and A Jaya. 2016. A study on ontology based abstractive summarization.\nProcedia Computer Science 87 (2016), 32–37.\n[75] Shiwali Mohan and John E. Laird. 2014. Learning Goal-Oriented Hierarchical Tasks from Situated Interactive\nInstruction. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Québec\nCity, Québec, Canada, Carla E. Brodley and Peter Stone (Eds.). 387–394. http://www.aaai.org/ocs/index.php/AAAI/\nAAAI14/paper/view/8630\n[76] Christian Muise, Tathagata Chakraborti, Shubham Agarwal, Ondrej Bajgar, Arunima Chaudhary, Luis A Lastras-\nMontano, Josef Ondrej, Miroslav Vodolan, and Charlie Wiecha. 2019. Planning for goal-oriented dialogue systems.\narXiv preprint arXiv:1910.08137 (2019).\n[77] Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simões, Vitaly Nikolaev, and Ryan McDonald. 2021. Planning with\nlearned entity prompts for abstractive summarization. Transactions of the Association for Computational Linguistics 9\n(2021), 1475–1492.\n[78] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri\nVainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model\ntraining on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis. 1–15.\n[79] Daniel Nyga and Michael Beetz. 2012. Everything robots always wanted to know about housework (but were afraid\nto ask). In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 243–250.\n[80] MD Okpor. 2014. Machine translation approaches: issues and challenges. International Journal of Computer Science\nIssues (IJCSI) 11, 5 (2014), 159.\n[81] Daniel W Otter, Julian R Medina, and Jugal K Kalita. 2020. A survey of the usages of deep learning for natural\nlanguage processing. IEEE transactions on neural networks and learning systems 32, 2 (2020), 604–624.\n[82] Vishal Pallagani and Biplav Srivastava. 2021. A Generic Dialog Agent for Information Retrieval Based on Automated\nPlanning Within a Reinforcement Learning Platform. Bridging the Gap Between AI Planning and Reinforcement\nLearning (PRL) (2021).\n[83] C. Raymond Perrault and James F. Allen. 1980. A Plan-Based Analysis of Indirect Speech Acts. Am. J. Comput.\nLinguistics 6, 3-4 (1980), 167–182.\n[84] Ronald PA Petrick and Mary Ellen Foster. 2016. Using general-purpose planning for action selection in human-robot\ninteraction. In 2016 AAAI Fall Symposium Series.\n[85] Ngoc Hung Pham and Takashi Yoshimi. 2015. Extraction of actions and objects from instruction manual for executable\nrobot planning. In 2015 15th International Conference on Control, Automation and Systems (ICCAS). IEEE, 881–885.\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n22\nJin et al.\n[86] Julie Porteous and Marc Cavazza. 2009. Controlling Narrative Generation with Planning Trajectories: The Role of\nConstraints. In ICIDS 2009 (Lecture Notes in Computer Science, Vol. 5915), Ido Iurgel, Nelson Zagalo, and Paolo Petta\n(Eds.). 234–245. https://doi.org/10.1007/978-3-642-10643-9_28\n[87] Julie Porteous, João F. Ferreira, Alan Lindsay, and Marc Cavazza. 2021. Automated narrative planning model extension.\nAuton. Agents Multi Agent Syst. 35, 2 (2021), 19. https://doi.org/10.1007/s10458-021-09501-1\n[88] Julie Porteous, João F Ferreira, Alan Lindsay, and Marc Cavazza. 2021. Automated narrative planning model extension.\nAutonomous Agents and Multi-Agent Systems 35, 2 (2021), 1–29.\n[89] Bernd Resch, Anja Summa, Peter Zeile, and Michael Strube. 2016. Citizen-centric urban planning through extracting\nemotion information from twitter in an interdisciplinary space-time-linguistics algorithm. Urban Planning 1, 2 (2016),\n114–127.\n[90] Mark O. Riedl and Robert Michael Young. 2010. Narrative Planning: Balancing Plot and Character. J. Artif. Intell. Res.\n39 (2010), 217–268. https://doi.org/10.1613/jair.2989\n[91] Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. 2010. Automatic keyword extraction from individual\ndocuments. Text mining: applications and theory 1 (2010), 1–20.\n[92] Scott Sanner et al. 2010. Relational dynamic influence diagram language (rddl): Language description. Unpublished\nms. Australian National University 32 (2010), 27.\n[93] Milene Santos Teixeira and Mauro Dragoni. 2022. A Review of Plan-Based Approaches for Dialogue Management.\nCognitive Computation (2022), 1–20.\n[94] Buser Say. 2021. A Unified Framework for Planning with Learned Neural Network Transition Models. (2021),\n5016–5024. https://ojs.aaai.org/index.php/AAAI/article/view/16635\n[95] Matthias Scheutz, Evan A. Krause, Bradley Oosterveld, Tyler M. Frasca, and Robert Platt Jr. 2017. Spoken Instruction-\nBased One-Shot Object and Action Learning in a Cognitive Robotic Architecture. In Proceedings of the 16th Conference\non Autonomous Agents and MultiAgent Systems, AAMAS 2017, São Paulo, Brazil, May 8-12, 2017, Kate Larson, Michael\nWinikoff, Sanmay Das, and Edmund H. Durfee (Eds.). 1378–1386. http://dl.acm.org/citation.cfm?id=3091315\n[96] Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li, Baobao Chang, and Zhifang Sui. 2018. Order-Planning Neural\nText Generation From Structured Data. In AAAI, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). 5414–5421.\n[97] Khaled Shaalan et al. 2010. Rule-based approach in Arabic natural language processing. The International Journal on\nInformation and Communication Technologies (IJICT) 3, 3 (2010), 11–19.\n[98] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. 2022. Skill Induction and Planning with Latent Language.\n(2022), 1713–1726. https://doi.org/10.18653/v1/2022.acl-long.120\n[99] Lanbo She and Joyce Yue Chai. 2017. Interactive Learning of Grounded Verb Semantics towards Human-Robot\nCommunication. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017,\nVancouver, Canada, July 30 - August 4, Volume 1: Long Papers, Regina Barzilay and Min-Yen Kan (Eds.). 1634–1644.\nhttps://doi.org/10.18653/v1/P17-1150\n[100] Lanbo She, Yu Cheng, Joyce Yue Chai, Yunyi Jia, Shaohua Yang, and Ning Xi. 2014. Teaching Robots New Actions\nthrough Natural Language Instructions. In The 23rd IEEE International Symposium on Robot and Human Interactive\nCommunication, IEEE RO-MAN 2014, Edinburgh, UK, August 25-29, 2014. 868–873. https://doi.org/10.1109/ROMAN.\n2014.6926362\n[101] Lanbo She, Shaohua Yang, Yu Cheng, Yunyi Jia, Joyce Yue Chai, and Ning Xi. 2014. Back to the Blocks World: Learning\nNew Actions through Situated Human-Robot Dialogue. In Proceedings of the SIGDIAL 2014 Conference, The 15th\nAnnual Meeting of the Special Interest Group on Discourse and Dialogue, 18-20 June 2014, Philadelphia, PA, USA. 89–97.\nhttps://doi.org/10.3115/v1/w14-4313\n[102] Raphael Shu and Hideki Nakayama. 2018. Discrete Structural Planning for Neural Machine Translation. CoRR\nabs/1808.04525 (2018). arXiv:1808.04525 http://arxiv.org/abs/1808.04525\n[103] Avirup Sil and Alexander Yates. 2011. Extracting STRIPS Representations of Actions and Events. In RANLP. 1–8.\n[104] Nisha Simon and Christian Muise. 2022. TattleTale: Storytelling with Planning and Large Language Models. (2022).\n[105] Sarath Sreedharan, Tathagata Chakraborti, Christian Muise, Yasaman Khazaeni, and Subbarao Kambhampati. 2020.\n–d3wa+–a case study of xaip in a model acquisition task for dialogue planning. In Proceedings of the International\nConference on Automated Planning and Scheduling, Vol. 30. 488–497.\n[106] Felix Stahlberg. 2020. Neural Machine Translation: A Review. J. Artif. Intell. Res. 69 (2020), 343–418.\nhttps:\n//doi.org/10.1613/jair.1.12007\n[107] Shane Storks, Qiaozi Gao, and Joyce Y Chai. 2019. Commonsense reasoning for natural language understanding: A\nsurvey of benchmarks, resources, and approaches. arXiv preprint arXiv:1904.01172 (2019), 1–60.\n[108] Gavin Suddrey, Ben Talbot, and Frederic Maire. 2022. Learning and executing re-usable behaviour trees from natural\nlanguage instruction. IEEE Robotics and Automation Letters (2022).\n[109] Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J. Martin, Animesh Mehta, Brent Harrison, and Mark O. Riedl.\n2019. Controllable Neural Story Plot Generation via Reward Shaping. In Proceedings of the Twenty-Eighth International\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\nIntegrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge\n23\nJoint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.). 5982–5988.\nhttps://doi.org/10.24963/ijcai.2019/829\n[110] Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and\nNicholas Roy. 2011. Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation.\nIn AAAI, Wolfram Burgard and Dan Roth (Eds.). http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3623\n[111] Moritz Tenorth, Daniel Nyga, and Michael Beetz. 2010. Understanding and executing instructions for everyday\nmanipulation tasks from the World Wide Web. In IEEE International Conference on Robotics and Automation, ICRA\n2010, Anchorage, Alaska, USA, 3-7 May 2010. 1486–1491. https://doi.org/10.1109/ROBOT.2010.5509955\n[112] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. 2019. Vision-and-Dialog Navigation. In 3rd\nAnnual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings (Proceedings\nof Machine Learning Research, Vol. 100), Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (Eds.). 394–406.\nhttp://proceedings.mlr.press/v100/thomason20a.html\n[113] Jesse Thomason, Shiqi Zhang, Raymond J. Mooney, and Peter Stone. 2015. Learning to Interpret Natural Language\nCommands through Human-Robot Dialog. In IJCAI 2015, Qiang Yang and Michael J. Wooldridge (Eds.). 1923–1929.\nhttp://ijcai.org/Abstract/15/273\n[114] Amirsina Torfi, Rouzbeh A Shirvani, Yaser Keneshloo, Nader Tavaf, and Edward A Fox. 2020. Natural language\nprocessing advancements by deep learning: A survey. arXiv preprint arXiv:2003.01200 (2020).\n[115] Stephen G. Ware and Robert Michael Young. 2011. CPOCL: A Narrative Planner Supporting Conflict. In AIIDE, Vadim\nBulitko and Mark O. Riedl (Eds.). http://www.aaai.org/ocs/index.php/AIIDE/AIIDE11/paper/view/4058\n[116] Robert Wilensky. 1981. Meta-Planning: Representing and Using Knowledge About Planning in Problem Solving and\nNatural Language Understanding. Cogn. Sci. 5, 3 (1981), 197–233. https://doi.org/10.1207/s15516709cog0503_2\n[117] Chuncheng Xiang, Tingsong Jiang, Baobao Chang, and Zhifang Sui. 2015. ERSOM: A Structural Ontology Matching\nApproach Using Automatically Learned Entity Representation. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, Lluís Màrquez, Chris\nCallison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (Eds.). 2419–2429. https://doi.org/10.18653/v1/d15-1289\n[118] Jingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xiaoyan Cai, and Xu Sun. 2018. A Skeleton-Based Model for Promoting\nCoherence Among Sentences in Narrative Story Generation. In EMNLP. 4306–4315. https://aclanthology.org/D18-\n1462/\n[119] Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-and-Write:\nTowards Better Automatic Storytelling. In AAAI. 7378–7385.\n[120] Rongguang Ye, Qingchuan Xu, Jie Liu, Yang Hong, Chengfeng Sun, Wenzheng Chi, and Lining Sun. 2021. A Natural\nLanguage Instruction Disambiguation Method for Robot Grasping. In IEEE International Conference on Robotics and\nBiomimetics, ROBIO 2021, Sanya, China, December 27-31, 2021. 601–606. https://doi.org/10.1109/ROBIO54168.2021.\n9739456\n[121] Kristina Y. Yordanova and Thomas Kirste. 2016. Learning Models of Human Behaviour from Textual Instructions. In\nICAART. 415–422.\n[122] R Michael Young, Stephen G Ware, Brad A Cassell, and Justus Robertson. 2013. Plans and planning in narrative\ngeneration: a review of plan-based approaches to the generation of story, discourse and interactivity in narratives.\nSprache und Datenverarbeitung, Special Issue on Formal and Computational Models of Narrative 37, 1-2 (2013), 41–64.\n[123] Meng-Hsuan Yu, Juntao Li, Zhangming Chan, Rui Yan, and Dongyan Zhao. 2021. Content Learning with Structure-\nAware Writing: A Graph-Infused Dual Conditional Variational Autoencoder for Automatic Storytelling. In AAAI.\n6021–6029.\n[124] Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. 2022. A survey of\nknowledge-enhanced text generation. ACM Computing Surveys (CSUR) (2022).\n[125] Yantian Zha, Lin Guan, and Subbarao Kambhampati. 2021. Learning from Ambiguous Demonstrations with Self-\nExplanation Guided Reinforcement Learning. CoRR abs/2110.05286 (2021). arXiv:2110.05286 https://arxiv.org/abs/\n2110.05286\n[126] Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan,\nPei Ke, et al. 2021. Cpm-2: Large-scale cost-effective pre-trained language models. AI Open 2 (2021), 216–224.\n[127] Fengda Zhao, Zhikai Yang, Xianshan Li, Dingding Guo, and Haitao Li. 2021. Extract Executable Action Sequences\nfrom Natural Language Instructions Based on DQN for Medical Service Robots. Int. J. Comput. Commun. Control 16,\n2 (2021). https://doi.org/10.15837/ijccc.2021.2.4115\n[128] Hankz Hankui Zhuo and Subbarao Kambhampati. 2013. Action-Model Acquisition from Noisy Plan Traces. In IJCAI.\n2444–2450.\n[129] Hankz Hankui Zhuo and Subbarao Kambhampati. 2017. Model-lite planning: Case-based vs. model-based approaches.\nArtif. Intell. 246 (2017), 1–21. https://doi.org/10.1016/j.artint.2017.01.004\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n24\nJin et al.\n[130] Hankz Hankui Zhuo, Héctor Muñoz-Avila, and Qiang Yang. 2014. Learning hierarchical task network domains from\npartially observed plan traces. Artif. Intell. 212 (2014), 134–157.\n[131] Hankz Hankui Zhuo and Qiang Yang. 2014. Action-model acquisition for planning via transfer learning. Artif. Intell.\n212 (2014), 80–103. https://doi.org/10.1016/j.artint.2014.03.004\n[132] Hankz Hankui Zhuo, Yantian Zha, Subbarao Kambhampati, and Xin Tian. 2020. Discovering Underlying Plans Based\non Shallow Models. ACM Trans. Intell. Syst. Technol. 11, 2 (2020), 18:1–18:30. https://doi.org/10.1145/3368270\nReceived xxxx; revised xxxx; accepted xxxx\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: April 2022.\n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2022-02-15",
  "updated": "2023-04-13"
}