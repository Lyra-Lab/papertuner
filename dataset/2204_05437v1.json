{
  "id": "http://arxiv.org/abs/2204.05437v1",
  "title": "Implementing Online Reinforcement Learning with Temporal Neural Networks",
  "authors": [
    "James E. Smith"
  ],
  "abstract": "A Temporal Neural Network (TNN) architecture for implementing efficient\nonline reinforcement learning is proposed and studied via simulation. The\nproposed T-learning system is composed of a frontend TNN that implements online\nunsupervised clustering and a backend TNN that implements online reinforcement\nlearning. The reinforcement learning paradigm employs biologically plausible\nneo-Hebbian three-factor learning rules. As a working example, a prototype\nimplementation of the cart-pole problem (balancing an inverted pendulum) is\nstudied via simulation.",
  "text": "January 29, 2022 \nJ. E. Smith  \nPage 1 \n \nImplementing Online Reinforcement Learning \nwith \n Temporal Neural Networks \n \nJ. E. Smith \nUniversity of Wisconsin-Madison (Emeritus) \n01/29/2022 \n1. Introduction \nTemporal neural networks (TNNs) encode information as precise spike timing relationships. The term \nâ€œTNNâ€ is used here to distinguish networks that use temporal communication and computation from \ntodayâ€™s broader class of spiking neural networks (SNNs) many of which use spike rates for \ncommunication and computation, rather than individual spike timing relationships.  Moreover, the TNNs \nconsidered here employ online localized learning via spike timing dependency (STDP) to support \ncontinual, adaptive online learning.  This contrasts with a large fraction of proposed TNNs that rely on  \ncompute-intensive back propagation learning methods, adapted, transferred, or inspired by conventional \nartificial neural networks (ANNs).   \nOf the TNNs that provide STDP-like learning, most have been developed for offline classification \nproblems -- as exemplified by the MNIST benchmark [1] [6][8][9][11][16][17]. Although offline \nsupervised classification benchmarks are useful for research studies, as far as practical applications these \ntypes of problems do not play to the strengths of TNNs with STDP learning.  As opinion: the likelihood \nthat any of the proposed TNNs or SNNs will replace deep convolutional ANNs for offline classification \nproblems is extremely small. For those types of problems, deep convolutional neural networks have been \nthoroughly refined, are highly successful, and are widely used (albeit with extremely compute-intensive \noffline training methods). The future of TNNs and STDP lies in efficient  implementation of online \napplications, especially those that employ online reinforcement learning (RL). \nIn this document, a TNN architecture for implementing efficient online RL is proposed and studied via \nsimulation. The proposed T-learning system is composed of a frontend TNN that implements online \nunsupervised clustering and a backend TNN that implements online reinforcement learning.  The \nreinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules \nas articulated by Gerstner et al. [4]. As a working example, a prototype implementation of the cart-pole \nproblem (balancing an inverted pendulum) is studied via simulation.  \n2. Background \nA generic reinforcement learning system is illustrated in Figure 1.  The system observes a sequence of \ninputs in the form of state variables coming from the Environment.  An Agent takes the combination of \ninput state variables and Learned State to enact a Policy that produces an action which is then applied to \nthe environment.   In response to actions, and possibly other external environmental variables (not \nshown), the Environment produces new state variables and, optionally, a reward (a punishment is a \nnegative reward).  The new state variables and accompanying reward (if any) are then assimilated into the \nLearned State. \nJanuary 29, 2022 \nJ. E. Smith  \nPage 2 \n \n \nFigure 1. Generic RL System Architecture.  \nIn online systems as considered here, learning operates continually as a sequence of observed state \nvariables and rewards are presented to the inputs. For each observation, the learning method updates \nLearned State (typically embodied as weights), and then moves on to the next observation.  \n2.1 Q-Learning Baseline \nQ-Learning [15] serves as a standard for performance comparisons, and Q-learning-based RL \nimplementations provide a good context for motivating and describing T-learning. Q-learning is an off-\npolicy reinforcement learning method -- it strives to learn the optimal action for each input state, \nirrespective of the policyâ€™s actual actions.  Q-learning employs a Q-table, which serves as a data structure \nupon which agent-based policies can be constructed. \nOf interest here are systems that traverse sequences of environment states.  A state is defined by a set of \nstate variables, whose combined values determine the overall state. A very large number of states may be \ndescribed with a much smaller number of state variables. For example, 4 state variables with 16 values \neach yield 64K total states.  \nThe classic Q-table consists of a row for each input state and a column for each output action (see for \nexample Table 1). Each entry in the Q-Table is a relative measure of the expected reward if the given \naction is taken when observing the given state.  Note that the table entries are not probabilities: it is their \nrelative magnitudes that is important. Based on the history of past state traversals and rewards, the Q-table \nmechanism gradually constructs and incrementally modifies table entries, as described in the next \nsubsection. \nTable 1. Example Q-Table \n \nreward \naction  \nstate \nvariables  \nReinforcement Learning Process\nLearned\nState\nAgent\nPolicy\nEnvironment\nQ(s,a)\n0\n0\n2\n...\n0\n0.24\n0.01\n0.04\n0.05\n1\n0.02\n0.02\n0.1\n0.04\n2\n0.07\n0.41\n0.08\n0.06\n3\n0.15\n0.26\n0.02\n0.09\n...\n0.16\n0.04\n0.28\n0.07\nAction (a)\nState (s)\nJanuary 29, 2022 \nJ. E. Smith  \nPage 3 \n \n2.2 Ideal Q-Learning \nWhen in state st at time step t, some policy -- not necessarily related to current Q-Table contents -- \ndetermines an action at .  In response to the action, the environment yields the next state st+1 and a reward \nrt+1.  Then, given the 4-tuple [st  at  st+1 rt+1], the Q-Table entry at Q(st , at ) is updated according to \nBellmanâ€™s equation: \nQ(st , at ) â† Q(st , at ) + Î± ( rt+1 + Î³ maxa Q(st+1, a ) - Q(st , at )) \n= Q(st , at )(1- Î± )+ Î± ( rt+1 + Î³ maxa Q(st+1 , a)) \nif rt+1  = 0, then: \n= Q(st , at )(1- Î± ) + Î±Î³ maxa Q(st+1 , a) \nwhere  Î± is a learning rate and Î³ is a discount factor. \nFigure 2 is a block diagram for the classic Q-Learning method.  The environment provides state variables \nas inputs to the system.  These are encoded into a one-hot format so there are as many signal lines as there \nare Q-Table rows.  This is input st to the Q-Learning mechanism.  \n \nFigure 2. Classic Q-Table implementation. To be consistent with the T-learning implementation \nto follow,  the state subscripts are time shifted by one unit, i.e. st and st-1 are used. \nA policy determines the action to be taken at each step.  Initially, when the Q-Table is devoid of \ninformation, the policy employs an exploration phase.  Exploration does not use the Q-Table for making \npolicy decisions. The exploration policy may follow some heuristic or may follow a sequence of pseudo-\nrandom actions.  At each step, the state and action are buffered  (st-1 and at-1 in the figure) to be used for \ntable updating during the next step. After the environment produces the next state and reward, the Q-\nTable is backward updated via Bellmanâ€™s equation. The Q-Table is updated at every step regardless of the \npolicy used.  By constantly updating with Bellmanâ€™s equation, rewards are back propagated incrementally \nthroughout the Q-Table, and, in a well-behaved system, entries will converge to a set of values that are \noptimal or nearly so.  After some level of convergence has been achieved, the Q-Learning policy is \nswitched to a phase that exploits the Q-Table, i.e., for a given state input, it selects the action associated \nwith the table entry having the highest value.  In most implementations the policy occasionally switches \nback to an exploration phase in an attempt to discover new paths. \nstate\nvariables\n(one-hot)\nQ-Table\nS x A\nQ-Table \nrow\n(action \nweights)\nExploit\nPolicy\n(max)\nExplore\nPolicy\naction at\nSelect\nEnvironment\nreward rt\nUpdate\n(Bellman s \neqn.)\nEncode\nst-1\nat-1\npolicy\nmode\nst\nst\nS\nA\nrt\nat\nJanuary 29, 2022 \nJ. E. Smith  \nPage 4 \n \nAlthough the classic method works well in theory, in most practical applications the state space (and \ntherefore, the Q-Table) is huge.  Not only that, but the convergence process requires many state traversals, \nand the more states, the more traversals are required. Deep Q-Learning, summarized in the next section is \na neural network-based method that deals with the state space problem. \n2.3 Deep Q-Learning \n \nFigure 3. Deep Q-learning implementation.  Dashed lines indicate paths used during training. \nIn Deep Q-Learning, a convolutional neural network (CNN) is trained to map the state variables onto \nwhat can be described as approximate Q-Table rows, and the exponential expansion from state variables \nto states is avoided.  The CNNâ€™s weights are updated via a back propagation learning process in which \nupdate information is placed in a replay buffer during exploration.  After a certain amount of exploration, \na version of Bellmanâ€™s equation is used to determine â€œcorrectâ€ outputs for a given set of inputs, and the \ncorrect values are used as part of a brief offline back propagation learning session.  These offline learning \nsessions are intermixed with inference.  Or, two networks can be used -- one performing inference (and \ntherefore actions), while the other is learning from the replay buffer.  Periodically, the networks can be \nswitched. \nThe CNN that is at the heart of the Deep Q-learning method is trained via conventional back propagation.  \nThe wrinkle is that training is done periodically and incrementally by using saved update information held \nin a replay buffer.  Essentially, this is a mini-batch method of training, and although it works, it strikes \nthis author as a rather inelegant semi-offline solution to an online learning problem. \n2.4 TNN Reinforcement Learning \nThe overall T-learning architecture is in Figure 4.  The design consists of two cascaded TNNs: a \nClustering TNN (C-TNN) and a reinforcement learning TNN (R-TNN). The state variables are first \npresented directly to the C-TNN which performs concurrent inference and unsupervised online learning.  \nIn doing so, the C-TNN compresses sets of similar state variables to a single one-hot cluster identifier \n(CId).  The underlying assumption is that similar state variables should lead to the same action. \nThe R-TNN uses reinforcement learning to map CIds to actions.  This is a one-hot to one-hot mapping, so \nsynaptic learning is simplified in some respects.  A reward signal from the environment is broadcast to all \nthe synapses in the R-TNN.  When a reward signal is non-zero, a reward (or punishment) is applied, and \nthe reward signal is combined with local spiking history to update the R-TNNâ€™s synaptic weights.   \nstate\nvariables\n Q-Table  \nrow\n(action \nweights)\nExploit\nPolicy\n(max)\nExplore\nPolicy\naction\nSelect\nEnvironment\nreward\nUpdate\n(Bellman s \neqn.)\npolicy\nmode\nReplay Buffer\naction\nSynaptic\nWeights\nCNN\nJanuary 29, 2022 \nJ. E. Smith  \nPage 5 \n \nLike the C-TNN, the R-TNN operates in an online streaming fashion where inference and training take \nplace continually and concurrently.  Although both TNNs are feedforward, this is a recurrent system \noverall with feedback through the environment. \nObserve that the C-TNN follows an off-policy method like the Q-Table.  Regardless of the actions taken \nby the R-TNN, the C-TNN simply clusters its observed input patterns without supervision. \n \nFigure 4. T-learning system: a TNN implementation of reinforcement learning.  \n2.5 Credit Assignment \nThe RL agent implemented as a column produces a sequence of actions.  Call this the action path or \naction sequence. When there is a reward or punishment, credit assignment determines which of the prior \nactions should be rewarded (by potentiating associated synapses) or punished (by depressing associated \nsynapses). In credit assignment terms, synaptic updates along the action path are a function of which \nactions were taken and when. \nThe three-factor learning rules as described by Gerstner et al. [4] is in this authorâ€™s opinion the most \npromising (perhaps the only) biologically plausible credit assignment method. With this approach, the \nsegment of the action path to be updated trails the head of the path (the current action) by a parameterized \nfixed length.  The amount of weight update is maximum at the head of the path and decays along the \nlength of the fixed-length path.  Along the path, both synapses that received input spikes and those that \ndid not are potentially updated. Rather than incrementally propagating rewards and punishments as in \nclassic T-learning, all the synaptic weight adjustments occur immediately when there is a reward or \npunishment. \n3. T-Learning Architecture \nIn this section, the T-Learning architecture is described with the cart-pole problem as a running example.  \nThe formulation given here uses Nagendra et al. [10] as a starting point -- although it diverges somewhat \nfrom that formulation.  \n3.1 Cart-Pole Problem \nThe objective is to learn to balance an inverted pendulum (a pole) on a moving cart (Figure 5). This is to  \nbe done through the actions of applying forces +F and - F to the cart.    \nstate\nvariables\naction \n(movement)\nEnvironment\nreward\nEncode\nencoded\nstate\nvariables\nCId\n(compressed state)\nFeedforward\nOnline\nUnsupervised Learning\nLocalized synaptic updates\nFeedforward\nOnline\nReinforced Learning\nBroadcast reward\nSynaptic\nX-bar\nÎ£\nÎ£\nÎ£\nÎ£\nÎ£\nW\nT\nA\nClustering TNN\nSynaptic\nX-bar\nÎ£\nÎ£\nÎ£\nÎ£\nÎ£\nW\nT\nA\nReinforced TNN\nJanuary 29, 2022 \nJ. E. Smith  \nPage 6 \n \n \nFigure 5. Cart-Pole dimensions. \nEquations describing the system are given below, taken from [10]. \nâˆ Ìˆ = {(M+m)gsinâˆ  - cosâˆ [F+ml âˆ Ì‡ 2 sinâˆ  ]}  /  {(4/3)(M+m)l-mlcos2âˆ } \nğ‘‘Ìˆ ={F+ml[âˆ Ì‡ 2sinâˆ  - âˆ Ìˆ cosâˆ ]}  / (M+m) \nParameters for baseline experiments are given below; also taken from [10]  \nM =  .711 kg    \n \n(1.6 lbs) \nm = .209 kg  \n \n \n(.46 lbs) \ng = 9.8 m/sec2   \n \n(32 ft/sec2) \nF = Â± 10 newtons \n \n(2.25 lbs-force) \nl = .326 meters  \n \n(1.1 ft) \nÏ„ = .02 sec. time intervals \n3.2 Implementation Overview \nAs classically defined, there are four state variables: the angle of the pole âˆ  and its first derivative âˆ Ì‡ , and \nthe cartâ€™s displacement d and its first derivative ğ‘‘Ì‡. \nThe model T-learning system is shown in Figure 6.  The environment (EV) is a part of the simulation \nsystem that models the physics in 64-bit floating point and passes floating point state variables to the \nencoding block (EN).  The Q-Table in the figure is not part of the T-learning system.  Rather, it is present \nin the simulator to allow later performance comparisons with conventional Q-learning.   The Q-Table \ntakes the same inputs as the TNN. \n \nM\nm\n-2.4 meters\n \n-F\n+F\nl\nd \n+2.4 meters\nJanuary 29, 2022 \nJ. E. Smith  \nPage 7 \n \n \nFigure 6  Simulated T-Learning system. The Q-Table block is in the simulator to provide \nbaseline comparisons; it is not part of the actual TNN system.   \n3.3 Input Encoding Methods \nBecause a basic principle behind the RL architecture (Figure 4) is that similar inputs lead to the same \naction, a critical part of the design process is encoding input information coming from the environment.  \nGiven encoded inputs, it is up to the C-TNN to form clusters based on similarity, then all the inputs that \nbelong to the same cluster are treated the same way by the R-TNN. Input volleys may appear similar or \ndissimilar depending on the chosen encoding method. The type and quality of input encoding is therefore \ncritical for determining the accuracy, efficiency, and hardware cost of TNN processing. \nIn a sense there are two coding dimensions in spike volley: 1) the presence or absence of a spike on a \ngiven line, 2) the time of a spike.  The spike time is low precision to begin with, and binarized volleys \nmay be adequate for many implementations.  Although binarized volleys are used at the inputs, volleys \ninternal to the TNN are not binarized.  For the cart-pole problem input volleys are binarized. If the spike \ntime (strength) dimension is useful for a given application, then it is available to be used. \nThe encoding process is reduced to encoding an external state variable as a binary volley -- 1-hot volleys \nin particular.  For example in the cart-pole problem, the pole angle ranges from -12 to +12 degrees that \ncan be divided into 12 equal intervals having endpoints:  [-12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12]. So, \nfor example the value 10.89 maps to the interval with endpoints [10, 12]. \nObserve that for this example, not only is the state variable discretized, but some implicit clustering also \ntakes place.  For example the values 10.11, 10.36, 10.89, 11.67 all map to the same interval [10, 12]; i.e., \nto the same cluster.  When clustering is performed on a single state variable, it is defined to be unitary \nclustering.  \nNext, observe that when performing unitary clustering over a range of values, the intervals do not have be \nthe same size.  For example, pole angles can be divided into 6 ranges defined by [-12, -6, -1, 0, 1, 6, 12] \nFinally, observe that a state variable does not have to span a numerical range.  They can be discrete items \nsuch as letters of the alphabet.  In that case, 26 1-hot encoding can be used.  Another option is to use two \n1-hot fields, one with 5 lines and the other with 6 to encode the 26 letters. \nMultiple State Variables \nTo encode multiple state variables, multiple spike volleys are concatenated, or merged, into a single larger \nvolley. Refer to Figure 7.  First, as part of the encoding process, the range of each external state variable \nis discretized into N intervals and encoded as  1-hot spike volleys on N lines x1... xN.  N = 8 in this \nexample. A value in interval i is encoded with a spike on line xi. In Figure 7a there are three discretized \nstate variables having the values 2 | 3 | 8  .  \n \n \nreward\naction\nEV\ncurrent state (FP)\nCId \nMG\ncurrent state (FP)\nC-TNN\nR-TNN\nQ-Table\nEN\nd\nJanuary 29, 2022 \nJ. E. Smith  \nPage 8 \n \nSimilarity Encoding: m-hot codes \nA 1-hot code is deficient for clustering methods that rely on the number of coincident spikes to determine \nsimilarity. With a 1-hot code, two differing input volleys have no coincident spikes, so all encoded values \nare equally dissimilar.   \nThis deficiency can be resolved by using an m-hot code, where m is odd.  For N discrete intervals, the \nnumber of lines is extended to N+m-1. A value in interval i is encoded with spikes on lines xi...xi+m-1.  \nExamples of 3-hot encodings for 2 | 3 | 8 and 2 | 4 | 6  are in Figure 7b and c.  Note that values that differ \nby 1 (3 and 4 in the example) have two spikes in common, reflecting their similarity.  Values that differ \nby 2 (8 and 6 in the example) have one spike in common, reflecting less similarity.  Values that differ by \nmore than 2 have no spikes in common indicating dissimilarity.  This encoding method is also put \nforward by Purdy [13] in the context of hierarchical temporal memory. \n \nFigure 7 Example mappings of input values to m-hot codes. Each spike volley consumes a single \ntime step.  In this example, each step consists of three time units (0, 1, and 2). \nThe examples in Figure 7a, b, and c are all binarized.  There is either a spike or not, and they all occur at \nthe same time.  As alluded to above, however, for some applications values may also be temporized: the \nrelative time of a spike indicates its relative strength or importance with respect to the other spikes, where \nearlier spikes are stronger.   In Figure 7d, the three values with their associated times are 2,0 | 3,2 | 8,1 .    \nHence, the strongest (or most important) state variable is the first (value = 2) because its spikes occur at \ntime = 0.  The weakest is the second state variable (value = 3); its spikes occur 2 time units after the first. \nRegarding the cart-pole problem specifically: at discrete intervals, the EN block receives state variables \nfrom the environment and produces encoded state variables.  In simulation, the environment (EV) \noperates on 64-bit floating point numbers to model the physics. The EN block reduces the incoming \nfloating point values to small discrete ranges and encodes them as a 3-hot binarized spike volleys.    \nNote: Two time scales are used in this  work.  A step or time step is composed of multiple time units.  In \nthe cart-pole implementation to follow, a single step is composed of 4 time units. The letter â€œsâ€  denotes \nsteps, and â€œtâ€ denotes time units.  An exception is the discussion of Q-Learning above in Section 2, where \nthe usual Q-Learning convention is used: â€œsâ€ represents a state and â€œtâ€ represents a time step. \n2\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\na) 1-hot  binarized\nb) 3-hot  binarized\nd) 3-hot  temporized\n1\n0\n2\ntime\nc) 3-hot  binarized\nThree State \nVariables\n3\n8\n2\n3\n8\n2\n4\n6\n2,0\n3, 2\n8,1\nJanuary 29, 2022 \nJ. E. Smith  \nPage 9 \n \n3.4 C-TNN Architecture  \nIn this section, the basic elements of the C-TNN are described. \nTemporal Communication and Excitatory Neuron Processing \nCommunication and processing are illustrated in Figure 8. The mathematics is discrete and operates over \nthe non-negative integers and the â€œâˆâ€ symbol.  Spikes occur at points in discrete time, measured \naccording to a unit time clock. The figure shows the processing of a single volley. The first spike in the \nvolley is assigned value (time) 0, and the other spikes occurring at later times are assigned integer values \n(their times relative to 0).  If there is no spike in a given volley, it is assigned â€œâˆâ€.   \n \nFigure 8. Example of temporal encoding and excitatory neuron operation.  Input values x1 - x4 \nare communicated as a volley of spikes conveyed over a bundle of four lines. Times increase from \nleft to right, and all times are relative to the first spike observed by the excitatory neuronâ€™s \nintegration function (Î£).  The input volley encodes values [1, 0, ï‚¥, 3], where ï‚¥ is the assigned \nvalue when there is no spike on a given line.  As the spikes arrive at the synapses, weights \ndetermine the amplitudes of response functions that are passed to the neuron body for \nsummation; the response functions are shifted according to the relative spike times.  At the \nneuron body, the time-shifted response functions are summed to yield the body potential, and an \noutput spike is generated the first time the body potential reaches the threshold Î¸ (at t = 4).  If it \nnever reaches the threshold the output spike time is ï‚¥ . \nA useful interpretation of spike time encoding is that the presence of a spike on a line indicates the \npresence of a feature, and the relative spike time indicates the strength of that feature. As in the case of \nbiological neurons, spike time resolution is quite low, and the example values in Figure 8 are \nrepresentative of an actual implementation. As an example, one might interpret a spike at relative time t = \n0 as â€œvery strongâ€, at t=1 as â€œmedium strongâ€, t=2 as â€œmedium weakâ€ and t=3 as â€œvery weakâ€. \nExcitatory neurons use the spike response model (SRM0)[5]. Figure 8 illustrates SRM0 operation. \nSynapses in the SRM0 model convert spikes to response functions which are summed, with an output \nspike being emitted when/if a threshold value is first reached.    By choosing different response functions, \na designer can influence the neuronâ€™s functional capabilities.  As depicted in Figure 9, ramp integrate-\nand-fire (RIF) response functions are used in this work.  The RIF model has a response function with a \nsloping leading edge that provides essential temporal processing capabilities.   \nresponse \nfunctions\nw1= 3\n \nbody \npotential\nx2\nx1\nx4\ny\ninput spikes\nsynaptic\nweights\ntime\nq\n4\n2\n0\n4\n3\n  \nx3\n1\n1\nJanuary 29, 2022 \nJ. E. Smith  \nPage 10 \n \n \nFigure 9. Discrete ramp response functions for weights 1 through 8.  The offset of one time unit \nfor weights 4 and lower accounts for slower body potential rise for lower weights.  \nSynaptic learning is achieved via spike timing dependent plasticity (STDP)[3][7], a localized, online \nmethod.  STDP is described below in the context of columns containing multiple neurons. \nModeling Inhibition \nInhibition is modeled as a winner-take-all (WTA) process. A 1-WTA inhibition block has the same \nnumber of input and output lines.  The input with the earliest spike time is passed through uninhibited as \nan output spike; all other input spikes yield no output spike. \nThe handling of tie cases is important because they occur frequently due to the very low precision \nintegers. When there is a 1-WTA tie, the tie breaker first selects spikes emitted from neurons with the \nhighest body potential at the time the spike is generated (so causality is maintained), and any remaining \nties may be broken either randomly or systematically. To simplify simulations, ties are broken \nsystematically by selecting the neuron with the lowest index. \nClustering Columns \nA column (Figure 10) implements an online unsupervised clustering function.  A sequence of input \npatterns (spike volleys) is applied to a columnâ€™s inputs, and the column uses STDP to organize the \npatterns into clusters: cluster centroids are encoded in the weights.  An individual synapse is illustrated in \nFigure 11. \n \n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\ntime\namplitude\n6\n7\n8\nw = 8\nw = 7\nw = 6\nw = 5\nw = 4\nw = 3\nw = 2\nw = 1\nJanuary 29, 2022 \nJ. E. Smith  \nPage 11 \n \n \nFigure 10.  A column consists of a synaptic crossbar feeding a set of parallel excitatory RIF \nneurons. WTA inhibition selects the first output to spike.  The crossbar is drawn using an \nabbreviated notation: a wide line (a bundle) feeding a neuron body contains the individual lines \nfrom each synapse.  STDP adjusts weights according to input and output spike times.  For \nsimplicity, binarized inputs are used in this example, and synapses having 0 weight are not \nshown. In this example, wmax = 4, and after sufficient training the synaptic weights form a stable, \nbimodal distribution.  The weights conceptually encode cluster centroids.    The fourth neuron \n(driving z4) has three input synapses at wmax that receive spike inputs at t = 0.   In this case, the \nbody potential of z4 reaches the threshold of 6 at t = 1.  Three neurons have two synapses of \nweight wmax that receive input spikes, and the summation of their response functions does not \nreach the threshold until t = 2.  At the outputs, WTA inhibition selects the fastest spike, so only z4 \nproduces a non-âˆ output value.  Functionally, WTAâ€™s one-hot output volley identifies the cluster \ncentroid nearest to the applied input. \n \n \nFigure 11. The synapse with weight wij  is dependent on timing of spikes on xi and zj.  Update \nparameters are common to all synapses in the network. \nFor an applied input pattern, the column outputs a one-hot cluster identifier, determined via the learned \ncentroids.  Concurrently, every input pattern in a sequence is assimilated by the learning process so it can \nbe used immediately for clustering later inputs.  Because of this continual learning process, over time \ncluster centroids are free to shift in order to match any drift in the input patterns. \nSynaptic weights are potentiated or depressed depending on the spike times of their inputs and the output \nof their post-synaptic neuron.  See Table 2.  The first two rows cover the case where both input and output \ncontain a spike, i.e. their spike time is not ï‚¥.  This is the situation most commonly discussed as classic \nSTDP.  If the input spike precedes (or occurs at the same time as) the output spike, then the weight is \nÎ¸ =  6 \n0\n  \n0\n0\nÎ£\nÎ£\nÎ£\nÎ£\nÎ£\nÎ£\nÎ£\nÎ£\n2\nW\nT\nA\n1\n1\n2\n2\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nsynapse\nwij\nÎ£\nWTA \nInhibition\nxi\nupdate\nlogic\nzj\nparameters\nÎ£\nÎ£\nÂµc\nÂµb\nÂµs\nJanuary 29, 2022 \nJ. E. Smith  \nPage 12 \n \nincreased by the value Âµc , up to the maximum weight wmax.  Otherwise, the weight is decreased by Âµb, \ndown to the minimum of 0.   \nThe next three rows cover the remaining cases. If there is an input spike, but no output spike, the weight \nis increased by Âµs, typically a small value or 0.  If there is an output spike, but no input spike, then the \nweight is decreased by Âµb.  Plausibility for the search case may be found in glial cells [12]. \nTable 2. Weight update functions for excitatory neurons. \ninput conditions \nweight update \ndescription \nxi ï‚¹ ï‚¥  \nzj ï‚¹ ï‚¥ \nxi â‰¤ zj \nÎ”wij =  +Âµc \ncapture \nxi > zj \nÎ”wij =  - Âµb \nbackoff \nxi ï‚¹ ï‚¥    zj = ï‚¥ \nÎ”wij =  +Âµs \nsearch \nxi = ï‚¥    zj ï‚¹ ï‚¥ \nÎ”wij =  - Âµb \nbackoff \nxi = ï‚¥    zj = ï‚¥ \nÎ”wij = 0 \nno-op \nExample:  Assume a network inputs x1-6, outputs z1-4, response functions as in Figure 9 with wmax = 8. All \nsynaptic weights are initialized to 5.  Assume all input patterns x consist of 3 spikes, and the neuron \nthreshold is set at 3. Further, assume Âµc = 1, Âµb = 1, and  Âµs = 0. When the input volley x = [âˆ 0 âˆ 0 0 âˆ] is \napplied to the column, all the response functions reach the threshold at t = 0.  All the body potentials are \nthe same, so a systematic 1-WTA tie-break selects the tying neuron with the lowest index, and weights on \nsynapses connected to z1 that receive an input spike are potentiated (+1) and others are depressed (-1).   \nThe input pattern has been captured by z1, and z1 will start to form a cluster with this pattern as its initial \ncentroid.  When a different 3 spike input is applied, [0 âˆ âˆ 0 0 âˆ], then  z1 will have a body potential \nbelow the threshold.  All the others tie,  and with the systematic tie breaker, z2 captures the second \npattern.  The example continues through five additional input patterns, where [âˆ 0 âˆ 0 0 âˆ] appears twice \nmore. As this happens, the weights of z1â€™s synapses continue to rise, further strengthening z1â€™s affinity for \nthe pattern. After the first six inputs, all the neurons have captured a cluster.  The 7th input differs from all \nof the previous.  The closest matches are with neurons z1 and z2.  The tie-break selects z1.  Finally, STDP \naccounts for the z1 clusterâ€™s new member by adjusting z1â€™s synaptic weights.  \n \nFigure 12. STDP for initial learning steps.  Synaptic weights are shown as a 2-D array above and \nbody potential for times 0 and 1 are shown in the bottom two rows.  Green highlights the input \npattern during a given step, and red highlights the spiking output for the given input pattern. \nWhen search mode is enabled (Âµs > 0) new clusters may be captured if there is a macro-level shift in input \npatterns.  If a cluster goes a long time without a cluster member triggering an output spike, search will \ncause the other, non-cluster, synapses to gradually rise.  Eventually, they will trigger a spike and a new \ncluster is captured.  \n \n \nz 1 z 2 z 3 z 4\nz 1 z 2 z 3 z 4\nz 1 z 2 z 3 z 4\nz 1 z 2 z 3 z 4\nz 1 z 2 z 3 z 4\nz 1 z 2 z 3 z 4\nz 1 z 2 z 3 z 4\nz 1 z 2 z 3 z 4\nx 1\n5\n5\n5\n5\nx 1\n4\n5\n5\n5\nx 1\n4\n6\n5\n5\nx 1\n3\n6\n5\n5\nx 1\n3\n6\n6\n5\nx 1\n3\n7\n6\n5\nx 1\n3\n7\n6\n6\nx 1\n2\n7\n6\n6\nx 2\n5\n5\n5\n5\nx 2\n6\n5\n5\n5\nx 2\n6\n4\n5\n5\nx 2\n7\n4\n5\n5\nx 2\n7\n4\n6\n5\nx 2\n7\n3\n6\n5\nx 2\n7\n3\n6\n4\nx 2\n6\n3\n6\n4\nx 3\n5\n5\n5\n5\nx 3\n4\n5\n5\n5\nx 3\n4\n4\n5\n5\nx 3\n3\n4\n5\n5\nx 3\n3\n4\n4\n5\nx 3\n3\n3\n4\n5\nx 3\n3\n3\n4\n4\nx 3\n4\n3\n4\n4\nx 4\n5\n5\n5\n5\nx 4\n6\n5\n5\n5\nx 4\n6\n6\n5\n5\nx 4\n7\n6\n5\n5\nx 4\n7\n6\n4\n5\nx 4\n7\n7\n4\n5\nx 4\n7\n7\n4\n6\nx 4\n8\n7\n4\n6\nx 5\n5\n5\n5\n5\nx 5\n6\n5\n5\n5\nx 5\n6\n6\n5\n5\nx 5\n7\n6\n5\n5\nx 5\n7\n6\n4\n5\nx 5\n7\n7\n4\n5\nx 5\n7\n7\n4\n4\nx 5\n8\n7\n4\n4\nx 6\n5\n5\n5\n5\nx 6\n4\n5\n5\n5\nx 6\n4\n4\n5\n5\nx 6\n3\n4\n5\n5\nx 6\n3\n4\n6\n5\nx 6\n3\n3\n6\n5\nx 6\n3\n3\n6\n6\nx 6\n2\n3\n6\n6\nt=0\n3\n3\n3\n3\n2\n3\n3\n3\n3\n2\n3\n3\n1\n1\n3\n3\n2\n3\n1\n3\n1\n2\n2\n3\n2\n2\n0\n1\nt=1\n5\n5\n3\n4\nJanuary 29, 2022 \nJ. E. Smith  \nPage 13 \n \n3.5 R-TNN Architecture \nThe R-TNN performs reinforcement learning. Although inference and learning happen concurrently, they \nare better explained separately. \nReferring to Figure 4, an R-TNN column has p 1-hot inputs x1..xp, that correspond to CIds coming from \nthe C-TNN.  It has q 1-hot outputs z1..q, which correspond to the types of output actions.  Thus, the \ncolumn contains a p Ã— q synaptic crossbar coupling its inputs to the q neuron bodies which generate \noutput spikes. \nSynaptic learning in the R-TNN employs neo-Hebbian three-factor learning rules as put forward by \nGerstner et al. [4]. The first two factors are the same as with conventional two-factor STDP: a synapseâ€™s \ninput spike time and its post-synaptic neuronâ€™s output spike time.  The third factor is a broadcast reward \nsignal R (a punishment is a negative reward). \nBecause the system acts in a series as steps, the one-hot input at step s is denoted xi (s), the one-hot output \nis zj (s), and the reward signal is R(s).   Reminder: a single step consists of multiple time units. \nWeight Update Implementation. \nJust as with conventional STDP, each synapse has its own local update logic, the only shared signal is the \nbroadcast reward. In general, there may be both rewards and punishments in the same system, and this is \nthe case with the cart-pole implementation described here. The three factor weight update function is \nshown schematically in Figure 13.   The three factors are the xi, the zj, and the broadcast reward R. \n \nFigure 13. The R-TNN synapse implementation employs a neo-Hebbian three factor update rule. \nA key part of the update process is a time window Ï‰ that contains the most recent spiking outputs zj , \npaired with the xi  that triggered the spike. When a reward is broadcast, all the synapses belonging to all \nthe neurons in the window are subject to three-factor STDP update. \nAs shown in  Figure 13, the implementation of synapse i,j consists of: \nVariables / State: \nwij : a weight up/down counter that saturates at 0 and wmax.  Weights are initialized at an initial \nvalue winit.  \ncij: a counter that is cleared to 0 whenever a spike on xi is received. At each subsequent step it \ncounts up until it saturates at Ï‰Ï or Ï‰Ï€ . \neij: a binary flag that sets to 1 if output zj spikes during a step that xi spikes.  It is cleared to 0 if zj \ndoes not spike during a step that xi spikes. \nR: the reward signal; +1 if there is a reward, -1 if there is a punishment \nsynapse\nwij\nÎ£\nWTA \nInhibition\nxi(s)\ncij\neij\nclear to 0  \non xi spike\ndecay counter \ninc. every cycle\nupdate\nlogic\nzj(s)\nset to 1 if xi \nand zj spike  \nbroadcast \nreward\nR(s)\nparams\nflag:\neligibility trace \nJanuary 29, 2022 \nJ. E. Smith  \nPage 14 \n \nReward (Ï) Parameters: \nÏ‰Ï : update window size \nÏ0+: maximum potentiation amount  (when cij = 0). \nÏ0-: maximum depression amount  (when cij = 0). \nPunishment (Ï€) Parameters \nÏ‰Ï€ : update window size \nÏ€0+: maximum potentiation amount (when cij = 0). \nÏ€0-: maximum potentiation amount (when cij = 0). \nThe potentiation and depression values decay over the length of the window.  Assuming linear decay, for \nsynapse i,j, update values for wij  are: \nÏij+ = Ï0+ - cij*Ï0+/ Ï‰Ï \nÏij- = Ï0- - cij*Ï0-/ Ï‰Ï \nÏ€ij+ = Ï€0+- cij* Ï€0+/Ï‰Ï€  \nÏ€ij- = Ï€0-- cij* Ï€0-/Ï‰Ï€ \nBecause the output neurons form a 1-hot code, at most one of the neurons will spike.  When a reward is \nreceived at step s, all synaptic weights for neurons along the action path are updated, not just synapses \nthat received an input spike (as determined by eij). For any synapse, i,j, if cij < Ï‰  then the associated \nsynaptic weight should be updated according to the following update rules: \nIf R(s) = 1 then  \nif eij = 1 then Î”wij = Ïij+   \nif eij = 0 then Î”wij = Ïij-   \nelse if R(s) = -1 then  \nif eij = 0 then Î”wij = Ï€ij+   \nif eij = 1 then Î”wij = Ï€ij-    \nFor an example, refer to Figure 14. \nNote that the same update parameters apply to all neurons and synapses within a column. They are \nestablished at the time the system is initialized, and they are not affected by the learning process.  Also \nnote that with the exception of the broadcast reward signal, synaptic update rules act on local information: \nwhether the synapsesâ€™s input and/or its neuronâ€™s output has spiked.  \nJanuary 29, 2022 \nJ. E. Smith  \nPage 15 \n \n \nFigure 14. Three factor synaptic updates.   A dynamic (unrolled) sequence of steps along an \naction path is shown; actions are encoded as neuron spikes zi.  The same neuron may appear \nmore than once, but only the most recent dynamic occurrence results in a synaptic update.  All \nsynapses associated with a neuron on the path are subject to updates.  If the reward is positive \n(R > 0), the synapses directly on the path are potentiated and other synapses belonging to the \nsame neuron are depressed.    If the reward is negative, synapses directly on the path are \ndepressed and the others are potentiated.  The amounts of potentiation and depression decrease \nalong the path of length Ï‰, as determined by counter values (c).   \nR-TNN Inference \nInference in an R-TNN is a simple version of inference in a regular TNN. An example trained R-TNN \ncolumn is in Figure 15.   This is a 1-hot to 1-hot mapping, so inference basically determines the highest \nweighted output for the given spiking input.  There is no need to add synaptic weights because there can \nbe most one active synapse per neuron. \nThe excitatory neurons have a threshold Î¸R that is kept relatively low so in most cases there is an output \nspike.  If there is a tie, then the first tie breaker chooses the neuron with the highest body potential (i.e. a \nsingle weight). The second tie breaker is a consistent pseudo-random selection. It is consistent in the \nsense that once a tie has been decided in a given way, the same decision continues to be used for \nfollowing consecutive ties. Finally, if the threshold Î¸R is not reached by any of the neurons, then a \nconsistently pseudo-random neuron is selected to spike.  \nstep\nÏ‰ = 7 \nÏ0\n+ \nR> 0\nz7\nz4\nz7\nz4\nz3\nz5\nz7\nÏ0\n- \nÏ0\n- \nÏ0\n- \nÏ0\n- \nÏ2\n+ \nÏ2\n- \nÏ2\n- \nÏ2\n- \nÏ2\n- \nÏ2\n+ \nÏ3\n- \nÏ3\n- \nÏ3\n- \nÏ3\n- \nÏ6\n+ \nÏ6\n- \nÏ6\n- \nÏ6\n- \nÏ6\n- \nÏ€0\n- \nR< 0\nz7\nz4\nz7\nz4\nz3\nz5\nz7\nÏ€0\n+ \nÏ€0\n+ \nÏ€0\n+ \nÏ€0\n+ \nÏ€2\n- \nÏ€2\n+ \nÏ€2\n+ \nÏ€2\n+ \nÏ€2\n+ \nÏ€3\n- \nÏ€3\n+ \nÏ€3\n+ \nÏ€3\n+ \nÏ€3\n+ \nÏ€6\n- \nÏ€6\n+ \nÏ€6\n+ \nÏ€6\n+ \nÏ€6\n+ \nc=0\nc=1\nc=2\nc=3\nc=4\nc=5\nc=6\nc=0\nc=1\nc=2\nc=3\nc=4\nc=5\nc=6\nJanuary 29, 2022 \nJ. E. Smith  \nPage 16 \n \n \nFigure 15. R-TNN column.  wmax = 3. The outputs zi indicate actions.  For the cart-pole problem, \nthere are only two actions that correspond to application of forces +F and -F. \n4. Initial Simulation Study \nTo illustrate the basic design methodology, a simple system is first simulated. Two TNNs each consisting \nof a single column cooperate to balance the pole.  The frontend C-TNN feeds the backend R-TNN, and \nthere is a feedback path from the R-TNN to the C-TNN through the environment. \nFor initial simulations only one state variable is used: the pole angle. The pole angle is restricted to fall \nwithin a range of Â± 12o, and this range is discretized into 16 equal intervals and encoded in a 3-hot code \n(Figure 7). Codings are binarized (Figure 7b).    \nAs a matter of methodology, the first set of simulations are directed at the R-TNN back-end rather than \nthe C-TNN clustering front-end. After the R-TNN has been studied and optimized, a subsequent set of \nsimulations focuses on the C-TNN. \nEpisodes \nA simulation run consists of multiple episodes, each beginning with a random initial pole angle:                \n-2.0 â‰¤  âˆ   â‰¤ +2.0 and initial displacement: d = 0. Synaptic weights (and therefore learning) are preserved \nbetween episodes.  Simulation of each episode proceeds as a series of steps until there is a failure (âˆ  falls \nout of range Â± 12o  or  d exceeds Â± 2.4 meters) or until 10,000 steps are reached successfully (i.e., 200 \nseconds of simulated time).    \nA pole failure yields negative reward (R(s) = -1). In the initial simulations, the track displacement d is not \nused as a state variable, so a track failure results in zero reward (R(s) = 0). Nevertheless, a track failure \nterminates an episode.  If a simulation reaches at least 500 steps (and for every 500 steps thereafter), a \nreward R(s) = +1 is generated. In all other cases R(s) = 0.  \nFor the dimensions and parameters used here, if there is a failure, it is because the cart hits the end of the \ntrack.  The common scenario is that the pole is kept within range for long periods of time, but there is a \nslow drift in cart displacement (Figure 16).  Eventually, the cart drifts to the end of the track.  Of course, \nthis behavior is to be expected because displacement is not used as a state variable. \n0\n  \n0\n  \n  \n  \n  \n  \n  \n  \n  \n  \nÎ¸R =2\nW\nT\nA\nÎ¸R =2\nÎ¸R =2\nÎ¸R =2\nJanuary 29, 2022 \nJ. E. Smith  \nPage 17 \n \n \nFigure 16. Pole angle and cart displacement as a function of the step.  Results for a typical \nsimulation run are shown. The cart gradually drifts to the left until it hits the end of the track.  \nMeanwhile, the pole angle remains within valid range. \nMetrics \nThe primary metric is based on numbers of successful steps each episode consumes, i.e., how long the \npole remains upright, and the cart is on-track.   \nTo compute the metric, the network is initially warmed up for a number of â€œtrainingâ€ episodes, and the \nsimulation continues for a number of â€œtestâ€ episodes. Training continues as an ongoing process during the \ntest phase.  After all the episodes are complete, the numbers of steps per episode are averaged over the \nnumber of test episodes. \nRandom Number Seeds \nThe initial pole angle for each episode is generated by a Matlab pseudo-random number generator.  It was \nfound that the specific seed has a significant effect on the results because different seeds lead to different \nwarm-up sequences.  To avoid inadvertent  cherry-picking by using what happens to be a good seed, eight \nrandom seeds -- 1 through 8 -- are used, and simulation results for all 8 are given (or are averaged).  \nNaive/Optimal Approach \nAn ideal method implemented as a standard for comparison. With only one state variable, if the pole leans \nleft, then apply force to the left, and if the pole leans right, apply force to the right.  In the absence of any \nother information, this naive approach is also optimal in a sense -- given the sparse state information (only \nthe pole angle) it appears to be the only viable approach.  \nQ-Learning \nA Q-Learning method was implemented as a standard for comparison.  The method uses the TNN for \ninitial exploration and Q-Table updates, and then an exploitation test phase uses the Q-Table to determine \nactions in the usual way (i.e., by selecting the action with the highest value in a given row).  Q-Learning \ncontinues during the test phase. The implementation is a text-book implementation using Bellmanâ€™s \nequation. The Î³ and Î± parameters were determined via good-faith, manual design space exploration; there \nare no guarantees of optimality. \nWith this method, at the time the exploration phase is turned on, both methods have observed exactly the \nsame sequence of input state variables and have had the opportunity to update weights or table entries. \nComparisons are then fair in the sense that both methods have explored the same state space when the test \nphase begins. \n \nJanuary 29, 2022 \nJ. E. Smith  \nPage 18 \n \n4.1 Simulation Results \nNaive/Optimal \nWhen the only state variable is the angle, the only reasonable strategy is to push the cart to the right when \nthe pole leans to right and to push the cart to the left when the pole leans to the left.  Accordingly, weights \nin the R-TNN were manually set at fixed values to achieve this.  The balancing methods were simulated \nfor 8 different pseudo-random generator seeds.  Then the average number of successful steps during the \ntest phase were sorted from best to worst.  This is to give an idea of the variability due to the individual \npng seeds.  \nFor comparison, results for this manually weighted method and the Q-Learning and T-learning methods \nfor 30+50 (warmup + test) are in Figure 17.  Allowing for variations due to pseudo-random seeds, the T-\nlearning method is on par with the optimal method.  Given that the pole angle is the only input, all three \nmethods appear to be at the performance ceiling. \n \nFigure 17.  For a single state variable (the angle), Ideal, 30+50 Q-Learning, and T-Learning yield \nsimilar performance except for one of the Q-Learning runs. \nR-TNN Implementation  \nR-TNN The initial baseline system uses binarized 3-hot inputs (Figure 7b and c).  In both the C-TNN and \nR-TNN, the maximum weight wmax = 8 and weights are initialized to winit = 5.  Other parameters are listed \nin Table 3.  These parameter values were determined via a series of initial exploratory simulations.  Some \nof the weight increments and decrements are fractions, so this adds a few bits of precision to the weight \ncounters. However, the ceiling function is applied to weights before they used for inference,  so inference \nis a purely small integer operation.    \n \n \n \n \n \n \n \n \nJanuary 29, 2022 \nJ. E. Smith  \nPage 19 \n \nTable 3. Column Configurations for Initial Simulations \nC-TNN parameter \nvalue \ndescription \nÎ¸C \n6 \nthreshold \nÂµc \n1/16 \ncapture \nÂµb \n1/16 \nbackoff \nÂµs \n0 \nsearch \nR-TNN parameter \nvalue \ndescription \nÎ¸R \n2 \nthreshold \nÏ0+ ,  Ï0- \n3/2 \nreward potentiation and depression \nÏ‰Ï \n2 \nreward window \nÏ€0+ , Ï€0- \n3/2 \npunishment potentiation and depression \nÏ‰Ï€ \n16 \npunishment window \nFor comparison Q-Learning was used.  For Q-Learning, Î³ = .95 and Î± = .9.  Each simulation was run for a \nvariable number warm-up (training) episodes, from 10 to 50, each followed by 50 test episodes.  The \nperformance metric is the average number of successful steps per episode.  As noted above, due to \nvariations caused by the random number seed, 8 different seeds are simulated. Then, the 8 results are each \nsorted from worst to best, irrespective of the seeds, and plotted. Simulation results are in Figure 18. \nThe Q-Learning method is capable of slightly higher performance for some seeds, but also suffers \nsignificantly lower performance for other random seeds.  An explanation is that the T-Learning method \nlearns so fast that the Q-Table exploration phase is not long enough to thoroughly explore the state space. \nSome outlier states are seldom or never visited, and this leads to incomplete Q-Learning.  It appears that \nthe 30+50 case works best for Q-Learning; in that case only one of the 8 random seeds results in a fall-off \nin performance.  It seems likely, however, that if one were to use a more sophisticated exploration \nstrategy, the Q-Learning method would be on par with the T-Learning method across the board. \n \nFigure 18. Average number of successful steps per test episode for varying number of training \nepisodes.  The x-axis represents 8 different random number generator seeds.  Each curve reflects \nsorted performance from worst to best for a given simulation, irrespective of the specific seeds. \nJanuary 29, 2022 \nJ. E. Smith  \nPage 20 \n \n4.2 C-TNN Design \nIn previous simulations, the number of CIds (and excitatory model neurons) was the same as the number \nof input states (16), so no true clustering was performed.  To exploit true clustering, a set of simulations \nwith reduced numbers of C-TNN neurons were run.   \nParameters are the same as in Table 3, except that search mode is used (Âµs = 1/128).  Simulation results \nare plotted in Figure 19. The number of neurons can be cut in half with no significant reduction in \nperformance. Reducing the neurons to 6 results in reduction for two of the random seeds. \n \nFigure 19. Performance for reduced numbers of C-TNN neurons (Zcnt). \nSummary \nThe combination of C-TNN and R-TNN begin with a blank slate of synaptic weights.  The pole angle is \nstreamed in at the rate of one sample per 20 msec., expressed as one of 16 intervals.  The C-TNN learns a \nmapping that reduces the 16 intervals onto 8 clusters, based on similarity.  Then, the 8 CIds are streamed \nto the R-TNN that learns, online, the optimal action for each of the clusters.  The action, when applied to \nthe cart, affects the pole angle which is fed back into the C-TNN.  Although the problem is a relatively \nsimple one, all the components of the RL system work as expected. \n5. Multiple State Variables \nAlthough the single state variable system just described seems to perform reasonably well, many episodes \nfail because the cart eventually drifts to an end of the track (Figure 16).   \nTo improve performance, state variable(s) can be added to allow the agent to adjust for the drift. To this \nend, four state variables and intervals are taken from [10], using the lower precision â€œ15 binsâ€ intervals \ngiven in that work.  The ranges and intervals for each of the four state variables are in Table 4.   The \ninterval# is used in Figure 20 for indexing synaptic weights.  One hot encodings are also given in the \ntable. \nAs an aside, it is generally better to use the least precision that will give satisfactory results.  In a sense, \nreducing precision is a simple way of performing similarity clustering.  \n \n \n \n \nJanuary 29, 2022 \nJ. E. Smith  \nPage 21 \n \n \nTable 4. State Variable Intervals and Encodings \n \ninterval# \ninterval \nencoding \nangle âˆ  \n1 \n-12, -6 \n100000 \n2 \n-6, -1 \n010000 \n3 \n-1, 0 \n001000 \n4 \n0, 1 \n000100 \n5 \n1, 6 \n000010 \n6 \n6, 12 \n000001 \ndisplacement d \n1 \n-2.4, -.8 \n100 \n2 \n-.8, .8 \n010 \n3 \n.8,  2.4 \n001 \nangular velocity \ndâˆ /dt \n1 \n-inf, -50 \n100 \n2 \n-50, +50 \n010 \n3 \n50, +inf \n001 \ncart velocity dd/dt \n1 \n-inf., -5 \n100 \n2 \n-5, +5 \n010 \n3 \n5, +inf \n001 \nIn the simulations in this section, all episodes begin with a pseudo-randomly chosen initial angle between \nplus and minus 1.5 degrees.  The metric is the same as used earlier: it is the number of successful steps \naveraged over a set of consecutive episodes (typically 30), with the maximum of 10,000. \nDuring initial simulations,  it became evident that the angular velocity of the pole is always in the mid-\nrange. In other words it provides no useful information and is not considered further. This leaves three \nstate variables, and the following three systems are considered: \n1 SV system \nangle only \n2 SV system \nangle & cart velocity \n3 SV system \nangle & cart velocity & cart displacement \nOptimal Performance \nFor the next step, through an ad hoc method, weights yielding what is apparently optimal performance \nwere determined and were coded into the synapses manually. These will be referred to as â€œoptimalâ€ for \nlack of a better term, although they have not been strictly proven to be optimal. See Figure 20. Optimal \nsynaptic weights -- strongly conjectured to be optimal, but not proved to be so.  Ranges for each of the \nstate variable ranges can be found in Table 4.   The 1 SV weights lead to behavior as described above: if \nthe pole leans to the left, then force is applied to push the cart to the left (-F).  If the pole leans to the right, \na force of +F is applied.  For 2 SV and 3 SV systems, the weight matrix is symmetric around the pole \nangle and is more complex but does make intuitive sense when examined closely.  \nJanuary 29, 2022 \nJ. E. Smith  \nPage 22 \n \n \n \n \n \na) 1 SV system \nb) 2 SV system \nc) 3 SV system \nFigure 20. Optimal synaptic weights -- strongly conjectured to be optimal, but not proved to be \nso.  Ranges for each of the state variable ranges can be found in Table 4.  \nangle\n-F\n+F\n1\n8\n0\n2\n8\n0\n3\n8\n0\n4\n0\n8\n5\n0\n8\n6\n0\n8\nangle\ndd /dt\n-F\n+F\n1\n1\n0\n8\n1\n2\n8\n0\n1\n3\n8\n0\n2\n1\n0\n8\n2\n2\n8\n0\n2\n3\n8\n0\n3\n1\n8\n0\n3\n2\n0\n8\n3\n3\n8\n0\n4\n1\n0\n8\n4\n2\n8\n0\n4\n3\n0\n8\n5\n1\n0\n8\n5\n2\n0\n8\n5\n3\n8\n0\n6\n1\n0\n8\n6\n2\n0\n8\n6\n3\n8\n0\nangle\nd\ndd /dt\n-F\n+F\n1\n1\n1\n8\n0\n1\n1\n2\n0\n8\n1\n1\n3\n8\n0\n1\n2\n1\n8\n0\n1\n2\n2\n8\n0\n1\n2\n3\n8\n0\n2\n1\n1\n8\n0\n2\n1\n2\n0\n8\n2\n1\n3\n8\n0\n2\n2\n1\n8\n0\n2\n2\n2\n8\n0\n2\n2\n3\n8\n0\n2\n3\n1\n0\n8\n2\n3\n2\n8\n0\n2\n3\n3\n8\n0\n3\n1\n1\n0\n8\n3\n1\n2\n0\n8\n3\n1\n3\n8\n0\n3\n2\n1\n8\n0\n3\n2\n2\n0\n8\n3\n2\n3\n8\n0\n3\n3\n1\n8\n0\n3\n3\n2\n0\n8\n3\n3\n3\n8\n0\n4\n1\n1\n0\n8\n4\n1\n2\n8\n0\n4\n1\n3\n0\n8\n4\n2\n1\n0\n8\n4\n2\n2\n8\n0\n4\n2\n3\n0\n8\n4\n3\n1\n0\n8\n4\n3\n2\n8\n0\n4\n3\n3\n8\n0\n5\n1\n1\n0\n8\n5\n1\n2\n0\n8\n5\n1\n3\n8\n0\n5\n2\n1\n0\n8\n5\n2\n2\n0\n8\n5\n2\n3\n0\n8\n5\n3\n1\n0\n8\n5\n3\n2\n8\n0\n5\n3\n3\n0\n8\n6\n2\n1\n0\n8\n6\n2\n2\n0\n8\n6\n2\n3\n0\n8\n6\n3\n1\n0\n8\n6\n3\n2\n8\n0\n6\n3\n3\n0\n8\nJanuary 29, 2022 \nJ. E. Smith  \nPage 23 \n \n \nFor each of the optimal systems, a total of 32 trials with random initial angles and random weights were \nsimulated and results are plotted in Figure 21.   The optimal 1 SV system performs reasonably well (given \nthat episodes top out at 10,000 steps), however, adding the cart velocity in the 2 SV system improves \nperformance significantly, and the 3 SV system is better yet with one of the trials achieving 30 \nconsecutive episodes that all max out at 10,000 steps. \n \nFigure 21. Optimal accuracies for 32 trials with pseudo-random initial angles and weights. \n5.1 2 SV Systems \nTable 5. Column Configurations for 2 SV Simulations \nC-TNN parameter \nvalue \ndescription \nÎ¸C \n12 \nthreshold \nÂµc \n1/16 \ncapture \nÂµb \n1/16 \nbackoff \nÂµs \n0 \nsearch \nR-TNN parameter \nvalue \ndescription \nÎ¸R \n2 \nthreshold \nÏ0+ ,  Ï0- \n1 \nreward potentiation and depression \nÏ‰Ï \n1 \nreward window \nÏ€0+ , Ï€0- \n1 \npunishment potentiation and depression \nÏ‰Ï€ \n32 \npunishment window \nAfter some performance tuning (see Table 5), a set of simulations for the 2 SV system was run, using 32 \npseudo-random seeds.   Each of the 32 trials consists of warm-up sequence of 170 episodes, followed by \n30 episodes where the number successful steps are recorded and averaged (see  Figure 22).  \nFor some of the trials the synaptic weights do not converge (right side of the figure).   When more \nwarmup episodes are added, the weights eventually converge.   In the middle region of Figure 22, the \nweights converge, but they converge to the 1 SV weights, i.e., there is no dependence on the cart velocity.  \nFinally at the extreme left end of the plot are the trials where weights converge to the 2 SV optima.   \nJanuary 29, 2022 \nJ. E. Smith  \nPage 24 \n \nWith a single state variable 1 SV, convergence to the optimum is straightforward so the naive approach  is \nquickly learned. By adding state variables, however, the problem becomes a non-linear optimization \nproblem where there are multiple stable sets of weights.   Many of these stable states lead to the 1 SV \noptimum.  Only a few lead to the 2 SV optimum (and there are hybrids in between).  \n \nFigure 22. Performance (average episode length) for the 2 SV system, with optimal 1 SV and 2 \nSV performance given for comparison. \nHence, there are multiple sets of stable weights (and multiple local optima). Each random seed sends the \nsystem toward some stable state.  \nRegarding rate of convergence, 16 trials with pseudo-random seeds were simulated, where convergence \nof some type is assumed when a moving average of at least 6000 successful steps over 30 episodes was \nachieved.  That is, at least 1 SV convergence is reached.  Results are in  Figure 23. Keep in mind, that the \n30 measured episodes are included in the totals, so, for example, the trial that â€œconvergedâ€ at around 50 \nepisodes, actually converges after 50-30 = 20 episodes. \n \n \nJanuary 29, 2022 \nJ. E. Smith  \nPage 25 \n \n \nFigure 23. Numbers of episodes to reach an average of 6,000 successful steps over a sliding \nwindow of 30 consecutive episodes.  \nFinally, the angle and cart displacement for one of the simulations that achieved the 2 SV optimum is in  \nFigure 24.   This solution holds the angle to a very tight range of about + or - 1 degrees.  Meanwhile, the \ncart displacement is kept within range through a process of constant adjustments.  This is the type of \nimproved behavior that adding SVs was intended to achieve. \n \n \n \nFigure 24. Simulation results for a 2 SV optimum system.  \n \n \n \nJanuary 29, 2022 \nJ. E. Smith  \nPage 26 \n \nDiscussion and Follow-on Research \nThis is ongoing research.  It appears 3-factor STDP enables relatively quick convergence to a stable set of \nweights.   So, the basic mechanism drives the weights toward stability, and in some cases it drives the \nsystem toward the 2 SV optimum.  However, in the majority of cases it drives the system toward non-\noptimum 1 SV stability. The challenge, then, is the same as in many non-linear optimization problems: to \nsomehow steer the system toward a global optimum and away from non-optimal local stability.    \nWe are faced with what amounts to the stability plasticity dilemma. A general approach is to add some \nrandomness to the convergence process. \nA sledge-hammer approach employs an agent that starts with pseudo-random weights and runs for some \nspecified number of episodes.  If the target performance is not achieved (say 30 consecutive episodes with \n9,000 average steps), then the agent re-sets the weights to a new set of pseudo-random values and the \nprocess can be repeated.  Eventually (see the left side of Figure 22) the objective will be achieved, i.e., \nconvergence to the optimum 2 SV weights.  Although this method will work, it is not a very satisfying \nsolution. \nFollow-on research will consider more subtle ways of adding randomness to the convergence process, say \nby periodically re-randomizing a small subset of the weights. \nOther follow-on research will use the previous action(s) as an additional input to the process. This will \nlead to networks that are not only recurrent through the environment, but also internally recurrent. \n6. Concluding Remarks \nThe column appears to be a fundamental TNN building block;  it is composed of 1) a set of parallel \nexcitatory neurons fed by 2) a synaptic crossbar and feeding into 3) a 1-WTA inhibition block. To draw \nan analogy: in conventional logic design terms, the column is a universal RTL building block.  Its \nfunction is fairly powerful, like an ALU or mux, and it easy to understand and apply.  Multiple columns \nare connected so they can work in concert to perform some cognitive task.  The column has several \nvariations, two of which are demonstrated in this paper.  Both the C-TNN and R-TNN are composed of \ncolumns: they follow the same biologically plausible paradigm, although they are configured differently.  \nThe frontend/backend architecture may be archetypical for a wide variety of RL systems. In an agnostic \nway, the frontend merely looks for similarities in the inputs it observes.  Then, the RL backend uses the \nsimilarity information to determine the action to be taken for a given input.  The author has found this \narchitecture and division of labor for other RL applications. \nFinally, regarding reinforcement learning: the neo-Hebbian update rules articulated by Gerstner et al. [4] \nare demonstrated to work in a biologically plausible spiking neuron system performing a reinforcement \nlearning task. \n \n \nJanuary 29, 2022 \nJ. E. Smith  \nPage 27 \n \n7. References \n[1] Diehl, Peter U., and Matthew Cook. \"Unsupervised learning of digit recognition using spike-timing-\ndependent plasticity.\" Frontiers in computational neuroscience 9 (2015). \n[2] Dutech, Alain, Timothy Edmunds, Jelle Kok, Michail Lagoudakis, Michael Littman, Martin Riedmiller, \nBryan Russell et al. \"Reinforcement learning benchmarks and bake-offs II.\" Advances in Neural \nInformation Processing Systems (NIPS) 17 (2005): 6. \n[3] Gerstner, Wulfram, Richard Kempter, J. Leo van Hemmen, and Hermann Wagner. â€œA neuronal learning \nrule for sub-millisecond temporal coding.â€ Nature 383, no. 6595 (1996): 76-78. \n[4] Gerstner, Wulfram, Marco Lehmann, Vasiliki Liakoni, Dane Corneil, and Johanni Brea. \"Eligibility \ntraces and plasticity on behavioral time scales: experimental support of neohebbian three-factor learning \nrules.\" Frontiers in neural circuits 12 (2018): 53. \n[5] Gerstner, Wulfram, and J. Leo Van Hemmen. â€œHow to describe neuronal activity: spikes, rates, or \nassemblies?â€ In Advances in neural information processing systems, (1993): 463-470. \n[6] Gupta, Ankur. \"Efficient and Scalable Biologically Plausible Spiking Neural Networks with Learning \nApplied to Vision.\" PhD diss., The Pennsylvania State University, 2010. \n[7] Guyonneau, Rudy, Rufin Vanrullen, and Simon J. Thorpe. â€œNeurons tune to the earliest spikes through \nSTDP.â€ Neural Computation 17, no. 4 (2005): 859-879. \n[8] Kheradpisheh, Saeed Reza, Mohammad Ganjtabesh, Simon J. Thorpe, and TimothÃ©e Masquelier. \n\"STDP-based spiking deep neural networks for object recognition.\" Neural Networks 99 (2018): 56-67.    \npdf is arXiv preprint arXiv:1611.01421 (2016).  \n[9] Mozafari, Milad, Mohammad Ganjtabesh, Abbas Nowzari-Dalini, Simon J. Thorpe, and TimothÃ©e \nMasquelier. \"Bio-inspired digit recognition using spike-timing-dependent plasticity (stdp) and reward-\nmodulated stdp in deep convolutional networks.\" Pattern Recognition 94 (2019): 87-95. \n[10] Nagendra, Savinay, Nikhil Podila, Rashmi Ugarakhod, and Koshy George. \"Comparison of \nreinforcement learning algorithms applied to the cart-pole problem.\" In 2017 International Conference \non Advances in Computing, Communications and Informatics (ICACCI), pp. 26-32. IEEE, 2017. \n[11] Nessler, Bernhard, Michael Pfeiffer, and Wolfgang Maass. \"STDP enables spiking neurons to detect \nhidden causes of their inputs.\" In NIPS, pp. 1357-1365. 2009. \n[12] Perea, Gertrudis, Marta Navarrete, and Alfonso Araque. \"Tripartite synapses: astrocytes process and \ncontrol synaptic information.\" Trends in neurosciences 32, no. 8 (2009): 421-431. \n[13] Purdy, Scott. \"Encoding data for HTM systems.\" arXiv preprint arXiv:1602.05925 (2016). \n[14] Smith, James E. \"A Neuromorphic Paradigm for Online Unsupervised Clustering.\" arXiv preprint \narXiv:2005.04170 (2020). \n[15] Watkins, Christopher JCH, and Peter Dayan. \"Q-learning.\" Machine learning 8, no. 3 (1992): 279-292. \n[16] Yu, Qiang, Huajin Tang, Kay Chen Tan, and Haizhou Li. \"Rapid feedforward computation by temporal \nencoding and learning with spiking neurons.\" (2013): 1-1.  \n[17] Zhao, Bo, Ruoxi Ding, Shoushun Chen, Bernabe Linares-Barranco, and Huajin Tang. \"Feedforward \ncategorization on AER motion events using cortex-like features in a spiking neural network.\" IEEE \ntransactions on neural networks and learning systems 26, no. 9 (2015): 1963-1978.  \n",
  "categories": [
    "cs.NE",
    "68T07",
    "I.2.6"
  ],
  "published": "2022-04-11",
  "updated": "2022-04-11"
}