{
  "id": "http://arxiv.org/abs/2204.11897v3",
  "title": "Reinforcement Teaching",
  "authors": [
    "Calarina Muslimani",
    "Alex Lewandowski",
    "Dale Schuurmans",
    "Matthew E. Taylor",
    "Jun Luo"
  ],
  "abstract": "Machine learning algorithms learn to solve a task, but are unable to improve\ntheir ability to learn. Meta-learning methods learn about machine learning\nalgorithms and improve them so that they learn more quickly. However, existing\nmeta-learning methods are either hand-crafted to improve one specific component\nof an algorithm or only work with differentiable algorithms. We develop a\nunifying meta-learning framework, called Reinforcement Teaching, to improve the\nlearning process of \\emph{any} algorithm. Under Reinforcement Teaching, a\nteaching policy is learned, through reinforcement, to improve a student's\nlearning algorithm. To learn an effective teaching policy, we introduce the\nparametric-behavior embedder that learns a representation of the student's\nlearnable parameters from its input/output behavior. We further use learning\nprogress to shape the teacher's reward, allowing it to more quickly maximize\nthe student's performance. To demonstrate the generality of Reinforcement\nTeaching, we conduct experiments in which a teacher learns to significantly\nimprove both reinforcement and supervised learning algorithms. Reinforcement\nTeaching outperforms previous work using heuristic reward functions and state\nrepresentations, as well as other parameter representations.",
  "text": "Published in Transactions on Machine Learning Research (06/2023)\nReinforcement Teaching\nCalarina Muslimani∗,1,2\nmusliman@ualberta.ca\nAlex Lewandowski∗,1,2\nlewandowski@ualberta.ca\nDale Schuurmans1,3,4\ndaes@ualberta.ca\nMatthew E. Taylor1,4\nmatthew.e.taylor@ualberta.ca\nJun Luo2\njun.luo1@huawei.com\n1 Department of Computing Science, University of Alberta\n2 Noah’s Ark Lab, Huawei Technologies Canada Co., Ltd.\n3 Google Brain\n4 Alberta Machine Intelligence Institute (Amii)\n∗Equal Contribution. Work done while interning at Huawei.\nReviewed on OpenReview: https: // openreview. net/ forum? id= G2GKiicaJI\nAbstract\nMachine learning algorithms learn to solve a task, but are unable to improve their ability to\nlearn. Meta-learning methods learn about machine learning algorithms and improve them\nso that they learn more quickly. However, existing meta-learning methods are either hand-\ncrafted to improve one specific component of an algorithm or only work with differentiable\nalgorithms. We develop a unifying meta-learning framework, called Reinforcement Teach-\ning, to improve the learning process of any algorithm. Under Reinforcement Teaching, a\nteaching policy is learned, through reinforcement, to improve a student’s learning algorithm.\nTo learn an effective teaching policy, we introduce the parametric-behavior embedder that\nlearns a representation of the student’s learnable parameters from its input/output behav-\nior. We further use learning progress to shape the teacher’s reward, allowing it to more\nquickly maximize the student’s performance. To demonstrate the generality of Reinforce-\nment Teaching, we conduct experiments in which a teacher learns to significantly improve\nboth reinforcement and supervised learning algorithms. Reinforcement Teaching outper-\nforms previous work using heuristic reward functions and state representations, as well as\nother parameter representations.\n1\nIntroduction\nAs machine learning becomes ubiquitous, there is a growing need for algorithms that generalize better, learn\nmore quickly, and require less data. Meta-learning is one way to improve a machine learning algorithm,\nwithout hand-engineering the underlying algorithm. Meta-learning is often thought of as “learning to learn”\nin which the goal is to learn about and improve another machine learning process (Schmidhuber, 1994; Thrun\n& Pratt, 1998; Hospedales et al., 2022). A variety of sub-domains have emerged that design hand-crafted\nsolutions for learning about and improving a specific component of a machine learning process. The work in\nthese sub-domains focus on solving one specific problem, whether that be finding the best way to augment\ndata (Cubuk et al., 2019), sample minibatches (Fan et al., 2018), adapt objectives (Wu et al., 2018a), or\npoison rewards (Zhang et al., 2020). Consequently, the meta-learning methods used in these domains are\nhandcrafted to solve the problem and cannot be applied to solve new problems in a different domain.\nCurrent literature fails to recognize that a more general framework can be used to simultaneously address\nmultiple problems across these varied sub-domains. Therefore, this work takes an important step toward\nanswering the following question:\nCan we develop a unifying framework for improving machine learning algorithms that can be applied across\nsub-domains and learning problems?\n1\narXiv:2204.11897v3  [cs.LG]  25 Jan 2025\nPublished in Transactions on Machine Learning Research (06/2023)\nAs a crucial step towards this unifying framework, we introduce Reinforcement Teaching: an approach that\nframes meta-learning in terms of learning in a Markov decision process (MDP). Although the individual\ncomponents of our system are based on previously proposed principles and methods from teacher-student\nreinforcement learning (Almeida et al., 2021; Garcia & Thomas, 2019; Huang et al., 2019; Cubuk et al., 2019;\nRuiz et al., 2019; Campero et al., 2020; Florensa et al., 2018; Fan et al., 2018; Narvekar et al., 2017; Narvekar\n& Stone, 2019; Zhang et al., 2020; Zhu et al., 2019; Zoph & Le, 2017; Jomaa et al., 2019; Biedenkapp et al.,\n2020; Sabbioni et al., 2020), learned parameter representations (Harb et al., 2020; Parker-Holder et al., 2020),\nand learning progress (Oudeyer et al., 2007), our system combines these components into a novel framework\nthat unifies meta-learning approaches and can improve machine learning algorithms across sub-domains. To\nthe best of our knowledge, Reinforcement Teaching is the first attempt at using reinforcement learning as a\ngeneral-purpose meta-learning solution method.\nIn Reinforcement Teaching, a teacher learns a policy via reinforcement learning (RL) to improve the learning\nprocess of a student. The teacher observes a problem-agnostic representation of the student’s behavior and\ntakes actions that adjust components of the student’s learning process that the student is unable to change,\nsuch as the student’s objective, optimizer, data, or environment. The teacher’s reward is then based on the\nstudent’s performance. The choice of action space for the teacher induces different meta-learning problem\ninstances. This allows our single teaching-architecture to learn a variety of policies, such as a curriculum\npolicy to sequence tasks for an RL student or a step-size adaptation policy for a supervised learning student.\nOur Reinforcement Teaching framework has several advantages over both gradient-based meta-learning and\nother RL teaching methods. Like gradient-based meta-learning, our MDP formalism is problem-agnostic\nand thus does not rely on problem-specific heuristics used in other RL teaching methods (Huang et al., 2019;\nAlmeida et al., 2021; Garcia & Thomas, 2019; Cubuk et al., 2019; Ruiz et al., 2019; Fan et al., 2018; Jomaa\net al., 2019). These prior RL teaching methods use hand-designed features for the RL teaching policy. This\nchoice limits the generality of these approaches to the base problems they were intended to solve. Other\nworks use parameter-based state representations which have been shown to learn successful teaching policies\nfor tabular and linear students (Biedenkapp et al., 2020; Zhang et al., 2020; Narvekar et al., 2017; Sabbioni\net al., 2020). Although the parameter state representation is problem-agnostic, it is difficult to scale to\nmore complex problems such as if the student uses a deep non-linear neural network. This motivated the\ndevelopment of the parametric-behavior embedder, a problem-agnostic state representation that can be used\nacross different problem settings and can scale to deep non-linear students.\nMoreover, although successful, gradient-based meta-learning methods (Finn et al., 2017; Xu et al., 2018;\nJaved & White, 2019) do not learn a teaching policy and are therefore unable to adapt to the student’s needs\nat each step in the student’s learning process. Another limitation of gradient-based meta-learning methods\nis the requirement that all learning components are fully-differentiable, which is not always possible. One\nexample of a component that is not differentiable is the configuration of an environment, which a teacher\npolicy may control to induce a curriculum for a student.\nThis paper makes the following contributions:\n1. The Reinforcement Teaching framework is formalized as an MDP in which the teacher learns a policy\nthat adapts the student’s algorithm to improve its performance towards a goal. Unlike previous work,\nReinforcement Teaching can be applied across different problem settings.\n2. Rather than having the teacher learn directly from the student’s parameters, a parametric-behavior\nembedder learns a state representation from the student’s inputs and outputs.\nThis provides a\nproblem-agnostic state representation that improves the teacher’s learning, and allows Reinforcement\nTeaching with deep non-linear students.\n3. We define a learning progress reward function that further accelerates learning by improving the\nteacher’s credit assignment.\nTo demonstrate the generality and effectiveness of Reinforcement Teaching, we apply this framework, with the\nparametric-behavior embedded state and learning progress reward, in two domains (1) curriculum learning\nand (2) step-size adaptation. Results in discrete and continuous control environments show examples of\n2\nPublished in Transactions on Machine Learning Research (06/2023)\nReinforcement Teaching, in which the teacher learns a policy that selects sub-tasks for an RL student. In\nstep-size adaptation for supervised learning students, we show that a reinforcement teacher can learn a\npolicy that adapts the step-size of Adam (Kingma & Ba, 2015), improving upon the best constant step-size.\nMoreover, in both settings our Reinforcement Teaching method learns a superior teaching policy, compared\nto several other baselines, that results in improved student learning. The primary goal of this paper is to\nspur novel developments in meta-learning using the tools of RL, and to unify different RL-based approaches\nunder the single framework of Reinforcement Teaching.\n2\nSequential Decision Making for Meta-learning\nBefore introducing Reinforcement Teaching, we argue for the importance of a sequential decision making\nperspective for meta-learning. Reinforcement Teaching, presented in Section 4, develops a framework that\nallows reinforcement learning algorithms to be applied in sequential meta-learning settings.\nMany meta-learning settings are sequential, such as hyper-parameter adaptation, curriculum learning, and\nlearned optimization. While there are many meta-learning settings of interest, we use step-size adaptation\nas an illustrative example because of its history and ubiquity in the meta-learning literature (Schraudolph,\n1999; Maclaurin et al., 2015; Sutton, 1992; 2022; Kearney et al., 2018). Step-size adaptation is the problem\nof selecting a learning rate at each step of an optimization algorithm. This is different from hyperparameter\noptimization, or tuning, where a grid-search is used to select the best constant learning rate for all steps\nof the optimization algorithm. In particular, we will use the noisy-quadratic problem studied in Schaul\net al. (2013) and Wu et al. (2018b), in which a learner with parameters θ attempts to minimize an objective\nwith a stochastic minimum. This problem, while simple, is an illustrative example of the importance of\nstep-size adaptation in stochastic settings. The objective function, defined for a d-dimensional parameter\nvector, θ = (θ1, . . . , θd), depends on a stochastic variable, c = (c1, . . . , cd), determining the minimum and a\nfixed diagonal hessian with entries, h = (h1, . . . , hd). If we assume that the stochastic minimum follows an\nindependent Gaussian distribution, ci = N(0, σi), then we can write the expected value of the objective as,\nL(θ) = E\n\u0002 ˆL(θ)\n\u0003\n= E\nh\n1\n2\nPd\ni=1 hi\n\u0000θi −ci\n\u00012i\n= 1\n2\nPd\ni=1 hi\n\u0010\nE\n\u0002\nθi\n\u00032 + V\n\u0002\nθi\n\u0003\n+ σ2\ni\n\u0011\n. The learner uses gradient\ndescent (with or without momentum) to update its parameters from θ(t−1) to θ(t) with a step-size of α(t). In\nthis setting, the stochastic gradient is ∂ˆ\nL\n∂θi = hi(θi −ci) and the deterministic gradient is ∂L\n∂θi = hiθi. Meta-\nlearning in the noisy quadratic problem amounts to selecting each α(t) such that the learnable parameters\nafter T steps of gradient descent best minimizes the objective, given by L(θ(T )).\nThere are two strategies for meta-learning α(t): fully-optimized or one-step greedy-optimal.\nThe fully-\noptimized sequence of step-sizes {α(t)}T\nt=1 is jointly chosen to minimize the loss at step T, where the loss\nis given by L(θ(T )).\nEven in the simple noisy quadratic problem, fully-optimizing the step-size can be\ncomputationally costly. An alternative is the one-step greedy-optimal schedule which selects α(t) so as to\nminimize the loss at the next iteration, L(θ(t)). In either the deterministic-gradient or spherical-gradient\nsetting (where all entries of h are the same, see Wu et al. (2018b), Theorem 3), the one-step greedy-optimal\nstep-size coincides with the fully-optimized schedule.\nIn general, however, the fully-optimized step-size\nschedule can result in a much lower final loss compared to one-step greedy-optimal schedule.\nIf we consider the step-size as the meta-learner’s action, at = α(t), the one-step greedy-optimal strategy\ntreats noisy quadratic optimization as a contextual bandit problem where the state is the current parameter,\nst = θ(t). At each time-step, the action is selected so as to minimize the next immediate loss. In the context\nof rewards, we may define the reward as rt = −L(θ(t)). The fully optimized sequence of {α(t)}T\nt=1 can also be\nthought of as a bandit problem where the action is the joint selection of {α(t)}T\nt=1. This is costly, requiring\nre-computation of every parameter iterate, {θ(t)}T\nt=1, for each candidate step-size sequence. A more natural\nformulation to learning the fully-optimized schedule can use reinforcement learning, where at each time-step\na policy observes the learnable parameters, θ(t−1), and selects a step-size, at = α(t), so as to minimize the\nlong-term loss. There are many reward functions that incentive the policy to minimize the long-term loss,\nsuch as a finite-horizon terminal reward (rt<T = 0, rT = −L(θ(T ))), or having a reward of −1 until a loss\nthreshold, L∗, is reached which then terminates the episode (rt = −I(L(θ(t)) > L∗), I(x > y) = 1 if x > y).\nThis reinforcement perspective is not specific to step-size adaptation; we develop Reinforcement Teaching in\nSection 4 for any sequential meta-learning problem.\n3\nPublished in Transactions on Machine Learning Research (06/2023)\n3\nRelated Work\nLearning to Teach Using Reinforcement Learning\nUsing an RL teacher to control particular aspects\nof another student’s learning process has been previously explored (Almeida et al., 2021; Garcia & Thomas,\n2019; Huang et al., 2019; Cubuk et al., 2019; Ruiz et al., 2019; Wu et al., 2018a; Dennis et al., 2020; Campero\net al., 2020; Florensa et al., 2018; Fan et al., 2018; Narvekar et al., 2017; Narvekar & Stone, 2019; Zhu et al.,\n2019; Zoph & Le, 2017; Jomaa et al., 2019; Biedenkapp et al., 2020; Sabbioni et al., 2020).\nHowever, by the design of these solution methods, they are only suitable for solving specific meta-learning\nproblems and lack applicability across different learning problems. More specifically, these works use problem-\nspecific heuristics to construct the teacher’s state representation that results in non-Markov state represen-\ntations (Wu et al., 2018a; Dennis et al., 2020; Campero et al., 2020; Florensa et al., 2018; Fan et al., 2018;\nHuang et al., 2019; Jomaa et al., 2019; Zhu et al., 2019).\nThis is commonly done because the Markov\nstate representation, the student’s parameters, is a large and unstructured state representation that makes\nit difficult to learn an effective policy. As a representative of the heuristic approach, the L2T framework\n(Fan et al., 2018) successfully learned to sample minibatches for a supervised learner. In this approach, the\nteacher’s state representation includes several heuristics about the data and student model and is heavily\ndesigned for the task of minibatch sampling (Fan et al., 2018). These works are tailored to the base problems\nthey solve and are unable to generalize to new problems with their state and reward design. Some works\nhave identified that learning from parameters is theoretically ideal for curriculum learning (Narvekar et al.,\n2017). However, the success of parameter state representation has been limited to either toy problems (e.g.,\n1D regression) or tabular/linear RL students (Biedenkapp et al., 2020; Zhang et al., 2020; Narvekar et al.,\n2017; Sabbioni et al., 2020). Until now, no work has attempted to use the behavioral approach proposed in\nthis paper to enable tractable, generalizable, and transferable learning algorithms.\nThese approaches can be contrasted bandit formulations of the student-teacher setting (Portelas et al., 2019;\nGraves et al., 2017; Jiang et al., 2021a; Parker-Holder et al., 2022; Jiang et al., 2021b). Although the bandit\nformulation has demonstrated promising results in the automatic curriculum learning domain, it can be\nlimiting for other meta-learning problems such as step-size adaptation (See Section 2).\nLearning Progress\nConnected to the idea of teaching is a rich literature on learning progress. Learning\nprogress prescribes that a learning agent should focus on tasks for which it can improve on. This mechanism\ndrives the agent to learn easier tasks first, before incrementally learning tasks of increasing complexity\n(Oudeyer et al., 2007). Learning progress has been represented in several ways such as the change in model\nloss, model complexity, and prediction accuracy. In addition, learning progress has been successfully applied\nin a variety of contexts, including curriculum learning (Portelas et al., 2019; Oudeyer et al., 2007; Matiisen\net al., 2020; Graves et al., 2017), developmental robotics (Blank et al., 2003; Moulin-Frier Clément, 2014;\nOudeyer et al., 2007), and intelligent tutoring systems (Clement et al., 2015).\nLearned Parameter Representations\nPrevious work in RL has argued that policies can be represented\nby a concatenated set of outputs (Harb et al., 2020; Parker-Holder et al., 2020). Policy eValuation Networks\n(PVN) in RL show that representations of a neural policy can be learned through the concatenated outputs\nof a set of learned inputs. PVN is similar to the parametric-behavior embedder that we propose because it\ncharacterizes a neural network by its output behavior. Learning a PVN representation, however, requires\na fixed set of inputs, referred to as probing inputs. While the probing inputs can be learned, they are still\nfixed after learning and cannot adapt to different policies. In our setting, the student’s neural network is\nfrequently changing due to parameter updates and it is unlikely that the outputs of a fixed set of inputs can\nrepresent the changing parameters during learning. Furthermore, Faccio et al. (2021) showed that learning to\nevaluate policies directly from parameters is more performant than PVNs for policy improvement, suggesting\nthat fixed probing inputs are insufficient for representing many neural networks.\nReward Design\nIn standard reinforcement learning, an agent’s goal is to maximize the expected sum of a\ndiscounted scalar reward signal. However, the source of this reward is unspecified and it is typically left to a\ndesigner to craft the agent’s reward function. As reward design is a non-trivial task, past work has cast the\nproblem of designing the reward function as an optimization problem – the optimal reward problem (Singh\n4\nPublished in Transactions on Machine Learning Research (06/2023)\net al., 2009; Sorg et al., 2010; Jain et al., 2021). Another related sub-area is adaptive reward shaping, in\nwhich an RL teacher agent learns to adaptively shape the student’s reward function (Mguni et al., 2023).\nReward design can be seen as a special case of Reinforcement Teaching. From this perspective, the RL\nteacher would take actions that adjust the student’s reward function during the student’s learning process\nto improve their overall learning.\nMachine Teaching\nMachine teaching is a general paradigm in which a teacher is used to guide a student.\nA widely studied application of machine teaching is the supervised learning setting in which a teacher is\ntasked with choosing the best training set such that a machine learning student can learn a target model\n(Zhu et al., 2018). Recent work has applied machine teaching to sequential decision-making tasks. In this\nsetting, machine teaching has been used to study a wide range of problems, from finding the best set of\ndemonstrations to finding the best reward shaping strategy (Brown & Niekum, 2019; Zhang et al., 2020).\nUnder the Reinforcement Teaching perspective, machine teaching can be viewed as an RL teacher whose\naction determines the data that the student uses for learning. The primary issue with traditional machine\nteaching approaches is that they assume the teacher has access to an optimal student model, learning\nalgorithm, and objective function (Zhu et al., 2018). These assumptions are unrealistic in practice. We show\nthat our parametric-behavior embedded state and learning progress reward allows the teacher to learn a\npolicy while only having access to the student’s inputs/outputs and performance.\nMeta-Learning\nWhile Reinforcement Teaching does not explicitly build on previous meta-learning work,\nwe point out common meta-learning methods and how they relate to Reinforcement Teaching. Early work in\nmeta-learning with neural networks (Younger et al., 2001; Hochreiter et al., 2001; Schmidhuber, 1987; Sutton,\n1992) inspired follow-up work on learned optimizers (Ravi & Larochelle, 2017; Andrychowicz et al., 2016).\nLearned optimizers replace the fixed learning algorithm with a memory-based parameterization, usually an\nLSTM (Hochreiter & Schmidhuber, 1997). Learning the optimizer through reinforcement learning has also\nbeen explored (Li & Malik, 2017a;b). This work, like the approach by Fan et al. (2018), employs an ad-hoc\nstate representation and reward function. Optimization-based meta-learning has other applications, such\nas in few-shot learning (Ravi & Larochelle, 2017) and meta-RL (Duan et al., 2016; Wang et al., 2016).\nAnother approach to meta-learning is gradient-based meta-learning, such as Model Agnostic Meta Learning\n(MAML) (Finn et al., 2017) and other work in meta-RL (Xu et al., 2018). These methods are distinguished\nfrom optimization-based meta-learning for the lack of a separately parameterized meta-learner. Instead,\nmeta-information is encoded in θ by differentiating through gradient descent.\n4\nReinforcement Teaching\nBefore introducing Reinforcement Teaching, we first describe the MDP formalism that underpins reinforce-\nment learning (Lattimore & Szepesvári, 2020; Sutton & Barto, 2018; Puterman, 2014). An MDP M is\ndefined by the tuple (S, A, r, p, µ, γ), where S is the state space, A denotes the action space, S is the\nstate space, r : A × S →R is the reward function that maps a state and an action to a scalar reward,\np : S × A × S →[0, 1] is the state transition function, µ is the initial state distribution, and γ is the discount\nfactor. Lastly, a Markov reward process (MRP) is an MDP without actions (Sutton & Barto, 2018). For an\nMRP, both the reward function r : S →R and state transition p : S × S →[0, 1] are no longer explicitly a\nfunction of an action. Instead, actions are unobserved and selected by some unknown behavior policy.\nIn Reinforcement Teaching, student refers to any learning agent or machine learning model, and teacher\nrefers to an RL agent whose role is to adapt to and improve the student’s learning process.\nWe start\nby defining the components of the student’s learning process. We then identify states and rewards, thereby\nformulating the student’s learning process as an MRP. This MRP perspective on learning processes allows the\nReinforcement Teaching framework to be applied to different types of students with varying data domains,\nlearning algorithms, and goals. Lastly, we introduce an action set for the teacher which allows the teacher to\nalter the student’s learning process. This induces an MDP, in which the teacher learns a policy that interacts\nwith a student’s learning process to achieve a goal (see Figure 1).\n5\nPublished in Transactions on Machine Learning Research (06/2023)\nFigure 1: The teacher takes actions a ∈A, which will influence an aspect of the teaching MDP, such as the\nstudent, fθ, learning algorithm, Alg, or learning domain D. The student will then update its parameters, θ,\nand the teaching MDP will then output r, s′ based on the student’s new parameters.\n4.1\nComponents of the Learning Process\nTo start, we define the student learning process and its components. Consider a student, fθ, with learnable\nparameters θ ∈Θ. The student receives experience from a learning domain D, which can be labeled data\n(supervised learning), unlabelled data (unsupervised learning), or an MDP (reinforcement learning). How\nthe student interacts with, and learns, in a domain is specified by the student’s learning algorithm Alg. The\nstudent’s learning algorithm updates the student’s parameters, θt+1 ∼Alg(fθt, D), in order to maximize a\nperformance measure that evaluates the student’s current ability, m(fθ, D). Written this way, m can be seen\nas the objective function directly optimized by Alg, but m can also be a non-differentiable metric such as\naccuracy in classification, or the Monte-Carlo return in RL.\nThe combination of the student, learning domain, learning algorithm, and performance measure is hereafter\nreferred to as the student’s learning process, E = (fθ, D, Alg, m). In the remainder of Section 4, we will\noutline how the components of the learning process interact as the student learns the optimal parameters\nthat maximize its performance measure, θ∗= arg maxθ m(fθ, D).\n4.2\nStates of Reinforcement Teaching\nWe define the state of the learning process as the student’s current learnable parameters, st = θt. Therefore,\nthe state space is the set of possible parameters, S = Θ. The initial state distribution, µ, is determined by the\ninitialization method of the parameters, such as Glorot initialization for neural networks (Glorot & Bengio,\n2010). Lastly, the state transitions, p, are defined through the learning algorithm, θt+1 = Alg(fθt, D), which\ncan be stochastic in general.\nThe sequence of learnable parameters, {θt}t≥0, form a Markov chain as long as D and Alg do not maintain\ntheir own state that depends on the parameter history. This is the case, for example, when the learning\ndomain is a dataset1, D = {xi, yi}N\ni=1, and the learning algorithm is gradient descent on an objective function,\nθ′ := Alg(fθ, D) = θ−α∇θ 1\nN\nPN\ni=1 J(fθ(xi), yi) (Mandt et al., 2017; Dieuleveut et al., 2020). While adaptive\noptimizers violate the Markov property of Alg, we discuss ways to remedy this issue in Appendix D and\ndemonstrate that it is possible to learn a policy that controls Adam (Kingma & Ba, 2015) in Section 5.2.\n1RL environments are also Markovian learning domains if the environment itself is Markovian.\n6\nPublished in Transactions on Machine Learning Research (06/2023)\n4.2.1\nParametric-behavior Embedder\nAlthough θ is a Markov state representation, it is not ideal for learning a policy. To start, the parameter\nspace is large and mostly unstructured, especially for nonlinear function approximators. While there is some\nstructure and symmetry to the weight matrices of neural networks (Brea et al., 2019; Fort & Jastrzebski,\n2019), this information cannot be readily encoded as an inductive bias of a meta-learning architecture.\nOften, the parameter set is de-structured through flattening and concatenation, further obfuscating any\npotential regularities in the parameter space. Ideally, the teacher’s state representation should be much\nsmaller than the parameters. As smaller state spaces simplify the learning problem on behalf of the teacher.\nIn addition, the teacher’s state representation should allow for generalization to new student models with\ndifferent architectures or activations, which is not feasible with the parameter state representation. With\nthis property, the teacher does not have to learn a separate teaching policy for each type of student model.\nSee Section 5.2 for empirical evidence of the difficulty of learning from parameters.\nTo avoid learning from the parameters directly, we propose the parametric-behavior embedder (PE), a novel\nmethod that learns a representation of the student’s parameters from the student’s behavior. To capture the\nstudent’s behavior, we use the inputs and outputs of fθ, as well as the targets for the student. For example, if\nthe student is a classifier, the inputs to fθ would be the features xi, the targets would be the label yi, and the\noutputs would be the classifier’s predictions, fθ(xi). To learn the PE state representation, we first assume that\nwe have a dataset or replay buffer to obtain the student inputs and targets. Then we can randomly sample\na minibatch of M inputs, {xi, yi}M\ni=1, and retrieve the student’s corresponding outputs, fθ(xi). The set of\ninputs, targets and student outputs ˆs = {xi, yi, fθ(xi)}M\ni=1, or mini-state, provides local information about the\ntrue underlying state s = θ. To learn a vectorized representation from the mini-state, we recognize that ˆs is\na set and use a permutation invariant function h to provide the PE state representation h(ˆs) (Zaheer et al.,\n2017). The input-output pair is jointly encoded before pooling, h(ˆs) = hpool\n\u0000{hjoint(xi, yi, fθ(xi))}M\ni=1\n\u0001\n,\nwhere hpool is a pooling operation over the minibatch dimension (see Figure 2).\nWe argue that the parametric-behavior embedder approximates the Markov state θ. This state representa-\ntion uses local information provided by the student’s behavior. With a large enough minibatch of inputs\nand outputs, it can summarize pertinent information about the current θ and how it will change, thereby\napproximating the Markov state (See Appendix F for more details). Methods that attempt to learn directly\nfrom the parameters must learn to ignore aspects of the parameters that have no bearing on the student’s\nprogress. This is inefficient for even modest neural networks. As we demonstrate in Section 5, the PE state\nrepresentation allows the teacher to learn an effective teaching policy compared to several other baselines.\n4.3\nRewards of Reinforcement Teaching\nGiven a reward function, r, we further formalize the learning process as an MRP, E = (S, r, p, µ), where the\nstate-space (S), initial distribution (µ), and state-transition dynamics (p) are defined in Section 4.2. The\nlearning process is formalized as an MRP for two reasons: (1) learning processes are inherently sequential,\nand therefore an MRP is a natural way to depict the evolution of the student’s parameters and performance,\nand (2) MRPs provide a unifying framework for different students’ algorithms and learning domains.\nTo specify the reward function, we first identify that reaching a high-level of performance is a common\ncriterion for training and measuring a learner’s performance.2 For ease of reference, let m(θ) := m(fθ, D). A\nsimple approach is the time-to-threshold reward in which a learner is trained until a performance condition is\nreached, such as a sufficiently high performance measure (i.e., m(θ) ≥m∗for some threshold m∗) (Narvekar\net al., 2017). In this case, the reward is constant r(θ) = −I (m(θ) < m∗) until the condition, m(θ) ≥m∗, is\nreached, which then terminates the episode.\nSimilar to the argument in Section 4.2, the reward function r(θ) = −I (m(fθ, D) < m∗) is also Markov as long\nas the learning domain is Markov. The performance measure itself is always Markov because, by definition,\nit evaluates the student’s current ability.\n2Appendix C outlines alternative reward criteria and reward shaping in the Teaching MRP.\n7\nPublished in Transactions on Machine Learning Research (06/2023)\nFigure 2: The neural network architecture used for Reinforcement Teaching with the parametric-behavior\nembedding state representation. For a given student, fθ, the parametric-behavior embedder independently\nprojects a mini-batch of student inputs, {xi}M\ni=1, and student outputs, {fθ(xi)}M\ni=1, into a latent space before\nconcatenation and pooling, providing a state representation of θ.\n4.3.1\nReward Shaping with Learning Progress\nUnder the time-to-threshold reward (Narvekar et al., 2017), the teacher is rewarded for taking actions such\nthat the student reaches a performance threshold m∗as quickly as possible. We argue, however, that this\nbinary reward formulation lacks integral information about the student’s learning process.\nTo address this shortcoming, we define a new reward function based on the student’s learning progress. The\nlearning progress signal provides feedback about the student’s relative improvement and better informs the\nteacher about how its policy influences the student.\nWe define Learning Progress (LP) as the change in the student’s performance measure, LP(θ′, θ) = m(θ′) −\nm(θ) at subsequent states θ and θ′ of the student’s learning process. To shape the time-to-threshold reward,\nwe add the learning progress term LP(θ′, θ) to the existing reward r(θ′) previously described. Therefore,\nour resulting LP reward function is r(θ′, θ) = −I (m(θ) < m∗) + LP(θ′, θ) until m(θ) ≥m∗, terminating the\nepisode. It follows that learning progress is a potential-based reward shaping, given by r′ = r +Φ(θ′)−Φ(θ),\nwhere the potential is the performance measure Φ(θ) = m(θ). This means that combining learning progress\nwith the time-to-threshold reward does not change the optimal policy (Ng et al., 1999).\nUnlike the time-to-threshold reward function, the LP reward provides critical information to the teacher\nregarding how its actions affected the student’s performance. The LP term indicates the extent to which\nthe teacher’s adjustment (i.e., action) improved or worsened the student’s performance. For example, if\nthe teacher’s action results in a negative LP term, this informs the teacher that with the student’s current\nskill level (as defined by the student’s parameters), this specific action worsened the student’s performance,\nthereby deterring the teacher from selecting such an action. We show empirically that compared to the\n8\nPublished in Transactions on Machine Learning Research (06/2023)\ntime-to-threshold reward and other reward functions found in the literature, the LP reward function enables\nthe teacher to learn a more effective teaching policy (See Section 5.1, Figure 4 and Table 3).\n4.4\nActions of Reinforcement Teaching\nThe MRP model demonstrates how the student’s learning process can be viewed as a sequence of param-\neters, {θt}t≥0, with rewards describing the student’s performance at particular points in time, {m(θt)}t>0.\nHowever, the goal of meta-learning is to improve this learning process. The teacher now oversees the stu-\ndent’s learning process and takes actions that intervene on this process, thus transforming the MRP into the\nTeaching MDP, M = (S, A, p, r, µ). Aside from the action space, A, the remaining elements of the Teaching\nMDP tuple have been defined in the previous subsections.\nWe now introduce the action set, A, that enables the teacher to control some component of the student\nlearning process. An action can change the student configuration or learning domain of the student, as\nshown in Figure 1. Similar to RL environments, we take the action set as part of the meta-learning task\ndescription and do not make further assumptions about the role of the action. The choice of action space\ninduces different meta-learning problem instances (see Appendix B), such as learning to sample, learning to\nexplore, curriculum learning (learning a policy for sequencing tasks), and adaptive optimization (learning to\nadapt the step-size).\nLastly, the action set determines the time-step of the teaching MDP. The base time-step is each application\nof Alg, which updates the student’s parameters. The teacher can operate at this frequency in settings where\nit controls an aspect of the learning algorithm, such as the step-size. In this setting, the teacher would take\nan action (e.g., select a step-size) after every parameter update for the student. Acting at a slower rate\ninduces a semi-MDP (Sutton et al., 1999). If the teacher controls the learning domain, such as setting an\nepisodic goal for an RL agent, then the teacher could operate at a slower rate than the base time-step. This\nwould result in the teacher taking an action (e.g., selecting a goal) after a complete student episode(s) which\ncomprises several student parameter updates. With the full Reinforcement Teaching framework outlined,\nsee Algorithm 1 for the corresponding pseudocode of the teacher-student interaction.\nAlgorithm 1 Reinforcement Teaching Framework\nInput: teacher RL algorithm ψT , student ML algorithm Alg, replay buffer D for student inputs/outputs,\nteacher action set A, initial teacher parameters θT , learning domain D, and minibatch size M and student\nperformance threshold m∗∈[0, 1]\nLoop for each teacher episode:\nReset student parameters θs and m(θs) = 0\nSet initial teacher state S\nWhile m(θs) < m∗do:\nChoose teacher action A ∈A and update the student’s learning process E\nTrain student via Alg. During this training store student inputs x in D\nRandomly sample a minibatch of M inputs from D, {xi}M\ni=1\nRetrieve the student’s corresponding outputs to obtain {xi, fθs(xi)}M\ni=1\nCalculate S′ = hpool\n\u0000{hjoint(xi, fθ(xi))}M\ni=1\n\u0001\nEvaluate student on learning domain D to obtain m(θ′\ns)\nCalculate LP = m(θ′\ns) −m(θs)\nCalculate R′ = −I (m(θ′\ns) < m∗) + LP\nUpdate θT according to ψT\n5\nExperiments\nTo demonstrate the generality and effectiveness of Reinforcement Teaching, we conduct experiments in both\ncurriculum learning (Section 5.1) and step-size adaptation (Section 5.2).\n9\nPublished in Transactions on Machine Learning Research (06/2023)\nTeacher Action\n# of Teacher Actions\nFrequency of Teacher Action\nTeacher State\nTeacher Reward\nCurriculum Learning\nMaze\nStart state\n11\nAfter complete student episode(s)\nPE variants\nLP\nFour Rooms\nStart state\n10\nAfter complete student episode(s)\nPE variants\nLP\nFetch Reach\nGoal distribution\n9\nAfter complete student episode(s)\nPE variants\nLP\nStep-size Adaption\nSynthetic Classification with SGD\nRelative change in step-size\n3\nAfter each student gradient step\nPE variants\nLP\nSynthetic Classification with Adam\nRelative change in step-size\n3\nAfter each student gradient step\nPE variants\nLP\nTable 1: Initialization of teaching MDP for the Curriculum Learning and Step-size adaption problem settings.\nIn the curriculum learning setting, we show that the teacher using the PE state representation and LP reward\nfunction significantly outperforms other RL teaching baselines in both discrete and continuous environments.\nFor the step-size adaptation setting, we show that only the PE state representation can learn a step-size\nadaptation policy that improves over Adam with the best constant step-size. We further show that this step-\nsize adapting teacher learns a policy that generalizes to new architectures and datasets. Our results confirm\nthat both PE state and LP reward are critical for Reinforcement Teaching, and significantly improves over\nbaselines that use heuristic state representations and other parameter representations.\n5.1\nCurriculum Learning For Reinforcement Learning Students\nIn this section, we apply our Reinforcement Teaching framework to the curriculum learning problem. Our\ngoal is for the teacher to learn a policy for sequencing sub-tasks such that the student can solve a target task\nquickly. In our experiments, we consider both discrete and continuous environments: an 11 by 16 tabular\nmaze, Four Rooms adapted from the MiniGrid suite (Chevalier-Boisvert et al., 2018), and Fetch Reach\n(Plappert et al., 2018). For the maze and Four Rooms environment, the student’s objective is to learn the\nmost efficient path from a target start state to a target terminal state. For the Fetch Reach environment,\nthe student’s goal is to learn how to move the end-effector to random locations in 3D space, given a fixed\nstart state. See Appendix I.1 for more details on the student environments.\nTeaching MDP for Curriculum Learning\nTo formalize curriculum learning through Reinforcement\nTeaching, we establish the teaching MDP (see Table 1). We begin by discussing the teacher’s action space.\nThe teacher’s actions will control an aspect of the student’s environment. For the maze and Four Rooms\nenvironment, the teacher’s action will change the student’s start state. The teacher can select the student’s\ninitial position from a pre-determined set of states, which can include states that are completely blocked off.\nFor Fetch Reach, the teacher’s actions determine the goal distribution. The goal distribution determines the\nlocation the goal is randomly sampled from. Each action gradually increases the maximum distance between\nthe goal distribution and the starting configuration of the end-effector. Therefore, “easier” student sub-tasks\nare ones in which the set of goals are very close to the starting configuration. Conversely, “harder” tasks are\nones in which the set of goals are far from the starting configuration of the end-effector.\nFor the teacher’s state representation, we consider two variants of PE that use different student outputs\nfθ. In both cases, the inputs are the states that the student encounters during its learning process (i.e.,\nstudent training episodes). For PE-Values, the embedded outputs are the state/state-action values, whereas\nfor PE-Action, the embedded outputs are the student’s actions. Specifically, during each student episode,\nthe student encounters (state, action) pairs. These pairs are then stored in a buffer. If the student state\nis already in the buffer, we keep the latest action that was taken. When it’s time to retrieve the teacher’s\nstate representation, we randomly sample a minibatch of M (state, action) pairs from this buffer. For the\nPE-Values representation, we query the state/action value corresponding to each state in the minibatch\nusing the most up-to-date value network. Finally, for all reward functions, the performance measure is the\nstudent’s return on the target task.\nFor the student’s learning algorithm, we used Q learning (Sutton & Barto, 2018), PPO (Schulman et al.,\n2017), and DDPG (Lillicrap et al., 2016) for the maze, Four Rooms, and Fetch Reach environments, respec-\ntively. This highlights that Reinforcement Teaching can be useful for a variety of students. See Table 10 for\nfull specification of student hyperparameters.\nTeacher Training\nNow, to train the teacher, we use DQN (Mnih et al., 2015). We use DQN for two\nreasons.\nFirst, we wanted our Reinforcement Teaching framework to have low sample complexity.\nAs\n10\nPublished in Transactions on Machine Learning Research (06/2023)\na single teacher episode corresponds to an entire training trajectory for the student, generating numerous\nteacher episodes involves training numerous students. The teacher agent cannot afford an inordinate amount\nof interaction with the student. One way to meet the sample complexity needs of the teacher is to use off-\npolicy learning, such as Q-learning. Off-policy learning is generally more sample efficient than on-policy\nmethods because of its ability to reuse past experiences that are stored in the replay buffer. Therefore,\nthe family of DQN algorithms is one natural choice. Second, our goal is to evaluate the efficacy of our\nReinforcement Teaching framework in solving multiple meta-learning problems. Although there have been\nadvancements in off-policy learning algorithms (Lillicrap et al., 2016; Haarnoja et al., 2018; Fujimoto et al.,\n2018) and these improvements are likely to improve the performance of our framework, we wanted to study\nReinforcement Teaching in the simplest deep RL setting possible.\nWe follow the pseudocode in Algorithm 1 to train the teacher. See Appendix J for full details on teacher\nhyperparameters.\nFigure 3: The beginning (left), middle (center), and ending (right) stages of the curriculum generated by\nthe PE-Actions + LP method for the Maze environment. States outlined in white indicate possible teacher\nactions. The state outlined in blue indicates the target start state and the green state indicates the target\ngoal state. Brighter colors (more yellow/white) indicate the start state was chosen more frequently by the\nteacher. Darker red/black indicates the start state was chosen less frequently by the teacher.\nTeacher Evaluation\nTo evaluate the teacher’s policy, we follow a similar protocol as done in training.\nThe teacher’s policy is first frozen, and the teacher is assigned a single newly initialized student.\nThe\nteacher then interacts with this student by taking actions (e.g., providing sub-tasks) that are provided to the\nstudent. During the teacher evaluation, the goal is to determine whether the teacher provides a curriculum\nof sub-tasks to the student such that the student can learn its target task efficiently. To show this, we\nreport the student’s learning curves (while using the teacher’s curriculum) in Figure 4.\nTo analyze the\neffectiveness of the PE state and the LP reward function on the teacher’s policy, we compare against the\nfollowing RL teaching baselines: L2T (Fan et al., 2018) and Narvekar et al. (2017). Narvekar et al. (2017)\nuses the parameter state representation with the time-to-threshold reward. Fan et al. (2018) uses a heuristic-\nbased state representation and a variant of the time-to-threshold reward. We also compare against TSCL\nOnline (Matiisen et al., 2020), a representative of the multi-armed bandit approaches from the curriculum\nlearning literature, a random teacher policy, and a student learning the target task from scratch (no teacher).\nMoreover, as a typical consequence of using RL to train a teacher is the additional training computation, we\nalso compare the teacher’s own learning efficiency across the RL-teaching methods (see Figure 4-left). These\nresults indicate that with our method, the teacher can learn effective curricula more quickly than existing\nRL teaching baselines. All results are averaged over 30 seeds with shaded regions indicating 95 % confidence\nintervals (CI).\nExperimental Results\nAcross all environments, we found that by using either of our PE state represen-\ntations along with our LP reward signal, the teacher is able to learn a comparable or superior curriculum\npolicy compared to the baselines. These teacher policies generated a curriculum of start/goal states for the\nstudent that improved the student’s learning efficiency and/or final performance, as shown in Figure 4-right.\nFor example, we found that in the Maze domain, the PE-Actions + LP teacher policy initially selected\n11\nPublished in Transactions on Machine Learning Research (06/2023)\nstarting states close to the target goal state. However, as the student’s skill set improved over time, the\nteacher adapted its policy and selected starting states farther away from the goal state (see Figure 3).\nFigure 4: The left plots are learning curves for the teacher. The y-axis is the number of episodes needed for\nthe student to reach the performance threshold, m∗, with the teacher’s current policy, as the teacher learns\nover episodes on the x-axis (lower is better). The right plots are the student’s training curves while using\nthe trained teacher’s curriculum policy (higher is better).\nMoreover, in the Maze domain, we found that the teacher was able to learn a comparable policy using the\nNarvekar et al. (2017) baseline. This is not surprising because in this domain the student’s parameters are\nrepresented by the tabular action-value table. This parameter set is small and does not come with the same\nissues as the parameters of a function approximator as described in Section 4.2.\nHowever, only our method is able to maintain significant improvements in student learning even as the\nstudent environment becomes more complex as demonstrated by Four Rooms and Fetch Reach results.\nLastly, we found that with our approach, the teacher is able to learn these curriculum policies efficiently\ncompared to the other RL-teaching baselines (See Figure 4-left). This is important because RL-teaching\napproaches, like Reinforcement Teaching, require computation on behalf of both the teacher and student\nalgorithm. Therefore, it is crucial that our framework learns effective policies as quickly as possible.\n12\nPublished in Transactions on Machine Learning Research (06/2023)\nAblating State and Reward Functions\nTo highlight the importance of our state representation and\nreward function on the teacher’s learned policy, we ablate over various state representations and reward\nfunctions used in the literature. We report the area under the student’s learning curve (AUC) when trained\nusing the teacher’s learned curriculum (See Tables 2 and 3). We use a one-tailed independent-samples Welch\nt-test (i.e., equal variances are not assumed) to determine if there is a difference in the average AUC between\nmethods with a p-value of 0.05.3\nState ablation\nPE-Value (Ours) PE-Action (Ours) L2T (Fan 2018) Parameters (Narvekar 2017)\nMaze\n62.12 ± 1.73\n61.62 ± 1.90\n61.44 ± 2.51\n66.62 ± 0.96\nFour Rooms 25.33 ± 0.56\n22.98 ± 0.76\n25.18 ± 1.10\n6.0 ± 2.94**\nFetch Reach 29.72 ± 2.95\n34.76 ± 1.94\n29.75 ± 1.56*\n16.13 ± 4.54**\nTable 2: Ablation of teacher state representation functions with fixed LP reward function. Reporting mean\narea under the student’s learning curve plus/minus standard error. The results are over 10 runs. * Indicates\na significant difference (p<.05) between our PE state representation and the baseline representations. **\nIndicates a significant difference between baseline and both of our state representations (PE Values/Actions).\nBold indicates the highest mean area under the curve.\nReward ablation\nLP (Ours)\nTime-to-threshold L2T reward\nRuiz (2019)\nreward\nMatiisen (2020)\nreward\nMaze\nPE-Value (Ours)\n62.12 ± 1.73\n57.42 ± 6.20\n6.94 ± 6.58*\n63.42 ± 1.55 15.80 ± 4.03*\nPE-Action (Ours) 61.62 ± 1.90 59.06 ± 5.18\n3.80 ± 3.60*\n14.67 ± 7.53*\n53.83 ± 2.64*\nFour Rooms PE-Value (Ours)\n25.33 ± 0.56 17.61 ± 1.99*\n17.27 ± 2.42* 13.00 ± 1.98*\n24.05 ± 0.84\nPE-Action (Ours) 22.98 ± 0.76 12.61 ± 3.11*\n19.93 ± 1.83\n21.18 ±1.02\n21.81 ± 0.97\nFetch Reach PE-Value (Ours)\n29.72 ± 2.95\n16.40 ± 3.50*\n15.94 ± 4.35* 23.51 ± 3.54\n33.55 ± 1.54\nPE-Action (Ours) 34.76 ± 1.94 18.08 ± 3.71*\n14.07 ± 2.98* 23.37 ± 2.78*\n33.56 ± 1.20\nTable 3: Ablation of teacher reward functions with fixed PE state representations. Reporting mean area\nunder the student’s learning curve plus/minus standard error. The results are over 10 runs. * Indicates\na significant difference (p<.05) between our LP reward function and the baseline reward functions. Bold\nindicates the highest mean area under the curve.\nWe first compare both variants of our parametric-behavior embedder, PE-Values and PE-Actions, against\nthe student parameters (Narvekar et al., 2017) and the heuristic state representation used by Fan et al.\n(2018). In this setting, the teacher’s reward is fixed to be our LP reward. Overall, we found that the PE\nstate representation is a more robust teacher state representation as the student’s environments get more\ncomplex. With our PE state representations, the teacher’s curriculum policy resulted in a higher AUC for\nthe student in both Four Rooms and Fetch Reach environments (see Table 2).\nNext, we compare our LP reward against the reward functions used in Narvekar et al. (2017), Fan et al.\n(2018), Ruiz et al. (2019) and Matiisen et al. (2020). In this setting, the teacher’s state representation is\nfixed to be either our PE-Actions or PE-Values representation. We found that in 4/6 of our experiments\n(student environment x PE variant), the student achieves a higher AUC value when trained with a teacher\nutilizing the LP reward (see Table 3).\nMoreover, we found that in both the reward and state ablation\nexperiments, by using our LP reward or PE state representations, the teacher has comparable or improved\nlearning efficiency across the differing student environments (see Figures 19 and 20 in Appendix K). To\nthat end, we have successfully demonstrated that (1) Reinforcement Teaching can be used to learn effective\ncurricula that improve student learning and (2) our PE state representations and LP reward function are\nimportant elements of our framework.\n3The Welch t-test was found to be more robust to violations of their assumptions compared to other parametric and non-\nparametric tests (e.g., t-test, ranked t-test) (Colas et al., 2019). In certain results we found the normality assumption to be\nviolated, therefore the Welch t-test a better choice than others.\n13\nPublished in Transactions on Machine Learning Research (06/2023)\n5.2\nStep-size Adaptation for Supervised Learning Students\nFor the step-size adaption setting, the goal is for the teacher to learn a policy that adapts the step-size of\na student’s base optimizer. The student is a supervised learning algorithm with the objective of learning a\nsynthetic classification task. See Appendix I.2 for more details on the classification task. Learning a step-size\nadaptation policy that improves over a tuned optimizer is a challenging problem because of the effectiveness\nof natively adaptive optimizers, such as Adam (Kingma & Ba, 2015).\nTeaching MDP for Step-size Adaption\nWe start by formalizing the teaching MDP for the step-size\nadaption problem setting (see Table 1).\nFor this problem, the teacher will control the step-size of the\nstudent’s optimizer. More specifically, the teacher’s action is a relative change in the step-size, doubling it,\nhalving it, or remaining constant. For each step in the student’s learning process, the student neural network\ntakes a gradient step with a step-size determined by the teacher.\nFor the PE state representation, we fix the mini-state size at 256 and include three variations: PE-0, which\nobserves only outputs, PE-X, which observes inputs and outputs, and PE-Y, which observes targets and\noutputs. In this setting, the inputs are the features xi, the outputs are the classifier’s predictions, fθ(xi),\nand the targets are the ground truth labels yi. Lastly, for all reward functions, the performance measure is\nthe student’s validation accuracy.\nTeacher Training\nTo train the teacher, we use a variant of DQN, Double DQN, and follow the pseudocode\nin Algorithm 1. As discussed in Section 5.1, we use DQN style algorithms for the teacher because of their\nsimplicity and sample efficiency. Our use of Double DQN here also shows that our results are robust to\ndifferent choices of RL algorithms. See Appendix J.2 for full details on teacher and student hyperparameters.\nTeacher Evaluation\nWe evaluate the teacher in a similar manner as mentioned in Section 5.1.\nThe\nteacher’s policy is fixed and then evaluated on a newly initialized student. One goal is to determine whether\nthe teacher learned an effective policy to adapt the student optimizer’s step-size over time. Therefore, we show\nthe student’s learning curve, while the student uses the step-sizes proposed by the teacher (see Figures 5-right\nand 6-right). It is also important that our Reinforcement Teaching approach is sample-efficient, therefore\nwe show the teacher’s own learning curve during training (see Figures 5-left and 6-left). Furthermore, we\nperform a policy-transfer experiment, where we demonstrate that with our approach, the teacher can learn a\nstep-size adaptation policy that can be transferred to new students classifying different benchmark datasets\n(MNIST, Fashion MNIST) and even new students with different architectures (see Figure 7).\nTo compare against existing work, we first conduct two ablation studies on the state representation using\nSGD and Adam as the base optimizers for the synthetic classification task. We compare the variants of our\nparametric-behavior embedder against (1) student parameters (Narvekar et al., 2017), (2) Policy Evaluation\nNetworks (PVNs), and (3) a heuristic state representation that contains the time-step, train accuracy and\nvalidation accuracy. The heuristic state is representative of previous work like L2T (Fan et al., 2018).\nMoreover, in the same synthetic classification task with the Adam optimizer, we further ablate the reward\nof Reinforcement Teaching, comparing our LP reward function to the time-to-threshold reward (Narvekar\net al., 2017) and the L2T reward (Fan et al., 2018). All results are averaged over 30 random seeds, and the\nshaded regions are 95% CIs.\nAblating State Representations\nUsing SGD as the base optimizer, we use an easy synthetic classifi-\ncation task where most random teacher trajectories can reach the performance threshold. We do this to\ndisentangle any effects of the reward function, and use only the time-to-threshold reward. We find that\nusing the PE variants significantly increases the teacher’s learning efficiency compared to the baselines (see\nFigure 5-top left). In particular, PE-X is slower to fit the data because it must learn to fit the Gaussian\ninputs, whereas PE-0 is able to more quickly learn from its smaller state representation (while this seems\nsurprising that outputs alone are effective, we discuss why this is effective in supervised learning in Appendix\nE). Both the PVN and the parameter state representation are no better than the simple heuristic in this\nproblem. Observing the learning rate schedule that the teaching policy induces in Figure 21, we see that the\nparameter state representation uses a nearly constant learning rate and is not adaptive. The parameter state\n14\nPublished in Transactions on Machine Learning Research (06/2023)\nFigure 5: State ablation experiments.\nThe left plots are learning curves for the teacher.\nThe y-axis is\nthe number of gradient steps needed for the student to reach the performance threshold with the teacher’s\ncurrent policy, as the teacher learns over episodes on the x-axis (lower is better). The right plots are the\nstudent’s training curves while using the trained teacher’s step-size policy (higher is better). Top: student’s\nbase optimizer is SGD. Bottom: student’s base optimizer is Adam, classification task is harder.\nrepresentation is the Markov state for SGD, but, learning from parameters is difficult even for this student’s\n2-layer neural network (19k parameters). PVN is also unable to improve even after increasing the number\nof probing inputs from 10 to 128. Furthermore, with respect to the student’s learning curve in Figure 5 (top\nright), we see similar results as previously found in the Curriculum Learning setting. With the PE state\nrepresentations, the teacher is able to output a policy that either improves the student’s learning efficiency\nor results in greater final validation accuracy compared to the baselines.\nAblating Mini-state Size\nUsing the same synthetic classification problem as before, with the student\nusing the SGD optimizer, we now ablate PE’s mini-state size (i.e. the number of inputs and outputs used\nbefore pooling).\nIn Figure 6 (bottom center), we find that the teacher improves with larger mini-state\nsizes. However, even a mini-state size of 32 provides a state representation that is able to improve over the\nbaselines: heuristic, parameters, and PVNs.\nAblating State Representation With Adam as Base Optimizer\nWe now conduct an experiment\nwith Adam as the base optimizer and with a more difficult synthetic classification task. The only difference\nin this synthetic classification task, is that the performance threshold is higher. This is needed because\nAdam can quickly solve the previous synthetic classification task with a large range of constant learning\nrates. Adam uses a running trace of the parameter gradients in the momentum term, so the Reinforcement\nTeaching MDP is no longer Markov in the parameters. To account for momentum, the mini-state can be\naugmented to include, in addition to the inputs and outputs, the change in outputs after a gradient step\n(denoted by -grad in legend, see Appendix D for details). Referring to Figure 5 (bottom left and right), we\nfind that PE is the only state representation to improve over Adam with the best constant step-size. More\nspecifically, we found that in 200 teacher episodes, by using the PE state representations the teacher can learn\na step-size adaption policy that results in the student reaching its performance threshold in approximately\n200 time-steps. Given the same amount of teacher training time with the heuristic state, the teacher’s policy\nonly enables the student to reach its performance threshold after 300 time-steps (see Figure 5-bottom left).\nMoreover, it is surprising to note that PE-0 is the best performing state representation despite not being a\n15\nPublished in Transactions on Machine Learning Research (06/2023)\nFigure 6: Top: Reward ablation experiments where the teacher adapts step-size of Adam. The left plot is\nthe learning curve for the teacher (lower is better). The right plot is the student’s training curves while\nusing the trained teacher’s step-size policy (higher is better). Bottom: Teacher’s learning curve ablating the\nPE mini-state size where the teacher adapts the SGD optimizer’s step size.\nMarkov state representation for this problem. The policy learned by Reinforcement Teaching with PE also\nsuccessfully transfers to new architectures (see Appendix L.2).\nAblating Reward Functions\nUsing Adam, PE-0, and the hard synthetic classification problem from the\nprevious experiment, we now ablate the reward function of Reinforcement Teaching. The earlier experiments\nwere designed to be insensitive to the reward function in such a way that a random teaching policy would\nreach the performance threshold. We note that the policy found in the Adam experiments can reach the\nperformance threshold in under 200 steps, while the initially random policy takes more than 350 steps. We\nnow ablate reward shaping with a max steps of only 200, making both the L2T and time-to-threshold reward\nrelatively sparse due to time-outs. Referring to Figure 6 (top-left), we find that learning progress shapes\nthe reward and allows the teacher to learn a step-size adaptation policy that improves over Adam in only\n100 teacher episodes, compared to 200 episodes in the previous experiment in Figure 5 (bottom-left). This\nindicates that our LP reward function is important for improving the teacher’s learning efficiency. Similarly,\nwe found that with the LP reward the teacher’s policy significantly improves the student’s final validation\naccuracy and learning efficiency compared to the other reward baselines (see Figure 6- top right).\nTransferring the Policy\nTo learn a general step-size adaptation policy, which is effective across bench-\nmark datasets, the teacher must train students on a large range of optimization problems. We now conduct\nexperiments in which the teacher learns in the “Neural Network Training Gym” environment, in which we\nsample a new synthetic classification task at the beginning of each episode. The teacher then learns to adapt\nthe step-size for the student’s neural network on that classification task for that episode. While synthetic,\nthis problem covers a large range of optimization problems by varying the classification task at each episode.\nAfter training the teacher’s policy in the NN Training Gym, we transfer the policy to adapt the step-size for\na student learning on benchmark datasets: MNIST (LeCun et al., 2010) and Fashion-MNIST (Xiao et al.,\n2017). This transfer experiment changes not only the data, but also the student’s neural network architecture\n(see details in Appendix I.2). We find that the heuristic state representation is able to reach the performance\nthreshold for the synthetic data (see Figure 7-top left). Referring to Figure 7 (top-right and bottom), the\nheuristic teaching policy does not transfer well to benchmark datasets. Our PE state representation, how-\n16\nPublished in Transactions on Machine Learning Research (06/2023)\n0\n100\n200\n300\n400\nNumber of Episodes\n160\n170\n180\n190\n200\nNum Steps\nNN Training Gym\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, MnistCNN\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, FashionCNN\nFigure 7: Reinforcement Teaching in the Neural Network Training Gym. Student learning curves use either\nthe best constant step-size or a step-size adaptation policy that was transferred after being learned in the\ntraining gym. Top-left: Teacher learning curves, lower is better. Top-right: Student learning curves with\nCNN on MNIST. Bottom: Student learning curves with CNN on Fashion MNIST.\never, is able to transfer the step-size adaptation policy to both MNIST and Fashion MNIST, as well as to\na student that is learning with a Convolutional Neural Network (CNN). This is surprising because the NN\nTraining Gym did not provide the teacher with any experience in training students with CNNs.\n6\nDiscussion\nOur experiments have focused on a narrow slice of Reinforcement Teaching: meta-learning curricula for a\nreinforcement learner and the step-size of an optimizer for a supervised learner. However, several other\nmeta-learning problems can be formulated using Reinforcement Teaching, such as learning to explore.\nThe main limitation of Reinforcement Teaching is the limitation of current RL algorithms. In designing the\nreward function, we used an episodic formulation because RL algorithms currently struggle in the continuing\nsetting. Another limitation of the RL approach is that the dimensionality of the teacher’s action space\ncannot be too large, such as directly parameterizing an entire neural network. While we have developed the\nparametric-behavior embedder to learn indirectly from parameters, an important extension of Reinforcement\nTeaching would be to learn to represent actions in parameter space.\nIn this paper, we presented Reinforcement Teaching: a general formulation for meta-learning using RL.\nTo facilitate learning in the teacher’s MDP, we introduced the parametric-behavior embedder that learns a\nrepresentation of the student’s parameters from behavior. For credit assignment, we shaped the reward with\nlearning progress. We demonstrated the generality of Reinforcement Teaching across several meta-learning\nproblems in RL and supervised learning. While an RL approach to meta-learning has certain limitations,\nReinforcement Teaching provides a unifying framework that will continue to scale as RL algorithms improve.\n17\nPublished in Transactions on Machine Learning Research (06/2023)\n7\nAcknowledgements\nPart of this work was done in the Intelligent Robot Learning (IRL) Lab at the University of Alberta. The\nauthors would like to thank Antonie Bodley, as well as all the anonymous reviewers for their constructive\nfeedback. This research was supported, in part, by funding from the Canada CIFAR AI Chairs program,\nAlberta Machine Intelligence Institute (Amii), Compute Canada, Huawei, Mitacs, Alberta Innovates, and\nthe Natural Sciences and Engineering Research Council (NSERC).\nReferences\nDiogo Almeida, Clemens Winter, Jie Tang, and Wojciech Zaremba. A generalizable approach to learning\noptimizers. ArXiv, 2021. URL https://arxiv.org/abs/2106.00958.\nMarcin Andrychowicz, Misha Denil, Sergio Gómez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan\nShillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. Proceedings\nof the 30th International Conference on Neural Information Processing Systems, 2016.\nURL https:\n//proceedings.neurips.cc/paper/2016/file/fb87582825f9d28a8d42c5e5e5e8b23d-Paper.pdf.\nAndre Biedenkapp, H Furkan Bozkurt, Theresa Eimer, Frank Hutter, and Marius Lindauer.\nDynamic\nalgorithm configuration: Foundation of a new meta-algorithmic framework. In Twenty-fourth European\nConference on Artificial Intelligence, 2020. URL https://ecai2020.eu/papers/1237_paper.pdf.\nDouglas Blank, Deepak Kumar, Lisa Meeden, and James Marshall.\nBringing up robot: Fundamental\nmechanisms for creating a self-motivated, self-organizing architecture.\nCybernetics & Systems, 36:125\n– 150, 2003. doi: 10.1080/01969720590897107. URL https://www.tandfonline.com/doi/abs/10.1080/\n01969720590897107.\nJohanni Brea, Berfin Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry in deep networks\ngives rise to permutation saddles, connected by equal-loss valleys across the loss landscape. ArXiv, 2019.\nURL http://arxiv.org/abs/1907.02911v1.\nDaniel Brown and Scott Niekum.\nMachine teaching for inverse reinforcement learning: Algorithms and\napplications. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 7749–7758, 2019. URL\nhttps://arxiv.org/pdf/1805.07687.pdf.\nAndres Campero, Roberta Raileanu, Heinrich Küttler, Joshua B. Tenenbaum, Tim Rocktäschel, and Ed-\nward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. Eighth International\nConference on Learning Representations, 2020. URL https://openreview.net/pdf?id=ETBc_MIMgoX.\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai\ngym. https://github.com/maximecb/gym-minigrid, 2018.\nBenjamin Clement, Didier Roy, Pierre-Yves Oudeyer, and Manuel Lopes. Multi-armed bandits for intelligent\ntutoring systems. Journal of Educational Data Mining, 7(2):20–48, 2015. doi: 10.5281/zenodo.3554667.\nURL https://jedm.educationaldatamining.org/index.php/JEDM/article/view/JEDM111.\nCédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.\nA hitchhiker’s guide to statistical comparisons\nof reinforcement learning algorithms.\nArXiv, 2019.\ndoi: 10.48550/ARXIV.1904.06979.\nURL https:\n//arxiv.org/abs/1904.06979.\nEkin Dogus Cubuk, Barret Zoph, Dandelion Mané, Vijay Vasudevan, and Quoc V. Le.\nAutoaugment:\nLearning augmentation strategies from data.\nConference on Computer Vision and Pattern Recogni-\ntion, 2019. URL https://openaccess.thecvf.com/content_CVPR_2019/papers/Cubuk_AutoAugment_\nLearning_Augmentation_Strategies_From_Data_CVPR_2019_paper.pdf.\nMichael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and\nSergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. Pro-\nceedings of the 34th International Conference on Neural Information Processing Systems, 2020.\nURL\nhttps://dl.acm.org/doi/abs/10.5555/3495724.3496819.\n18\nPublished in Transactions on Machine Learning Research (06/2023)\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John\nSchulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov.\nOpenai baselines.\nhttps://github.com/\nopenai/baselines, 2017.\nAymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic\ngradient descent and markov chains.\nThe Annals of Statistics, 48(3):1348–1382, 2020.\nURL https:\n//alain.perso.math.cnrs.fr/data/paper/sgd_markov.pdf.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel.\nRl2: Fast\nreinforcement learning via slow reinforcement learning. ArXiv, 2016. URL http://arxiv.org/abs/1611.\n02779v2.\nFrancesco Faccio, Louis Kirsch, and Jürgen Schmidhuber.\nParameter-based value functions.\nNinth In-\nternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=\ntV6oBfuyLTQ.\nYang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. Sixth International\nConference on Learning Representations, 2018. URL https://openreview.net/forum?id=HJewuJWCZ1.\nChelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-agnostic meta-learning for fast adaptation\nof deep networks.\nFifth International Conference on Learning Representations, 2017.\nURL https:\n//proceedings.mlr.press/v70/finn17a/finn17a.pdf.\nSebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, and Satinder\nSingh. Bootstrapped meta-learning. The Tenth International Conference on Learning Representations,\n2022. URL https://openreview.net/forum?id=b-ny3x071E5.\nCarlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement\nlearning agents.\nProceedings of the 35th International Conference on Machine Learning, 2018.\nURL\nhttps://proceedings.mlr.press/v80/florensa18a.html.\nStanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural network loss landscapes. Thirty-\nthird Conference on Neural Information Processing Systems, 2019. URL https://proceedings.neurips.\ncc/paper/2019/file/48042b1dae4950fef2bd2aafa0b971a1-Paper.pdf.\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. ArXiv,\n2021. URL http://arxiv.org/abs/2106.06860v1.\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic\nmethods. Thirty fifth International Conference on Machine Learning, 2018. URL https://arxiv.org/\npdf/1802.09477.pdf.\nFrancisco M. Garcia and Philip S. Thomas. A meta-mdp approach to exploration for lifelong reinforce-\nment learning. Thirty-third Conference on Neural Information Processing Systems, 2019. URL https:\n//proceedings.neurips.cc/paper/2019/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf.\nXavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.\nProceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 2010. URL\nhttps://proceedings.mlr.press/v9/glorot10a.html.\nAlex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curricu-\nlum learning for neural networks. Proceedings of the 34th International Conference on Machine Learning,\n2017. URL https://proceedings.mlr.press/v70/graves17a.html.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic actor. 2018. URL https://arxiv.org/pdf/1801.\n01290.pdf.\nJean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon. Policy evaluation networks. ArXiv, 2020.\nURL http://arxiv.org/abs/2002.11833v1.\n19\nPublished in Transactions on Machine Learning Research (06/2023)\nSepp\nHochreiter\nand\nJürgen\nSchmidhuber.\nLong\nshort-term\nmemory.\nNeural\ncomputation,\n9(8):1735–1780,\n1997.\nURL\nhttps://direct.mit.edu/neco/article-abstract/9/8/1735/6109/\nLong-Short-Term-Memory?redirectedFrom=fulltext.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. Inter-\nnational Conference on Artificial Neural Networks, 2001. URL https://link.springer.com/chapter/\n10.1007/3-540-44668-0_13.\nTimothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-learning in neural\nnetworks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44:5149–5169,\n2022. URL https://www.computer.org/csdl/journal/tp/2022/09/09428530/1twaJR3AcJW.\nChen Huang, Shuangfei Zhai, Walter A. Talbott, Miguel Ángel Bautista, Shi Sun, Carlos Guestrin,\nand Joshua M. Susskind. Addressing the loss-metric mismatch with adaptive loss alignment. Thirty-\nsixth International Conference on Machine Learning, 2019.\nURL https://www.semanticscholar.\norg/paper/Addressing-the-Loss-Metric-Mismatch-with-Adaptive-Huang-Zhai/\n55112cbf0d65380072281c43f10f0f8472fa4b20.\nAvik Jain, Lawrence Chan, Daniel S. Brown, and Anca D. Dragon. Optimal cost design for model predictive\ncontrol. in learning for dynamics and control.\nIn Proceedings of the 3rd Conference on Learning for\nDynamics and Control, pp. 1205–1217. PMLR, 2021. URL https://arxiv.org/pdf/2104.11353.pdf.\nKhurram Javed and Martha White.\nMeta-learning representations for continual learning.\nThirty-third\nConference on Neural Information Processing Systems, 2019. URL https://proceedings.neurips.cc/\npaper/2019/hash/f4dd765c12f2ef67f98f3558c282a9cd-Abstract.html.\nMinqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob N. Foerster, Edward Grefenstette, and Tim Rock-\ntaschel. Replay-guided adversarial environment design. Thirty-fifth Conference on Neural Information\nProcessing Systems, 2021a. URL https://openreview.net/forum?id=5UZ-AcwFDKJ.\nMinqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. Proceedings of the 38th\nInternational Conference on Machine Learning, 2021b. URL https://proceedings.mlr.press/v139/\njiang21b.html.\nHadi S. Jomaa, Josif Grabocka, and Lars Schmidt-Thieme. Hyp-rl: Hyperparameter optimization by rein-\nforcement learning. 2019. URL https://arxiv.org/pdf/1906.11527.pdf.\nAlex Kearney, Vivek Veeriah, Jaden B Travnik, Richard S Sutton, and Patrick M Pilarski. Tidbd: Adapting\ntemporal-difference step-sizes through stochastic meta-descent. ArXiv, 2018. URL https://arxiv.org/\nabs/1804.03334.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Third International\nConference on Learning Representations, 2015. URL https://openreview.net/forum?id=8gmWwjFyLj.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforce-\nment learning. Thirty-fourth Conference on Neural Information Processing Systems, 2020. URL https://\nproceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html.\nT. Lattimore and C. Szepesvári. Bandit Algorithms. Cambridge University Press, 2020. ISBN 9781108486828.\nURL https://books.google.ca/books?id=bydXzAEACAAJ.\nYann LeCun, Corinna Cortes, and CJ Burges.\nMnist handwritten digit database.\nATT Labs [Online].\nAvailable: http://yann.lecun.com/exdb/mnist, 2, 2010.\nKe Li and Jitendra Malik. Learning to optimize. Fifth International Conference on Learning Representations,\n2017a. URL https://openreview.net/forum?id=ry4Vrt5gl.\nKe Li and Jitendra Malik. Learning to optimize neural nets. ArXiv, 2017b. URL https://arxiv.org/pdf/\n1703.00441.pdf.\n20\nPublished in Transactions on Machine Learning Research (06/2023)\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David\nSilver, and Daan Wierstra. Continuous control with deep reinforcement learning. Fourth International\nConference on Learning Representations, 2016. URL https://openreview.net/forum?id=tX_O8O-8Zl.\nDougal Maclaurin, David Duvenaud, and Ryan Adams.\nGradient-based hyperparameter optimization\nthrough reversible learning.\nIn International conference on machine learning, pp. 2113–2122. PMLR,\n2015.\nStephan Mandt, Matthew D. Hoffman, and David M. Blei.\nStochastic gradient descent as approximate\nbayesian inference. Journal of Machine Learning Research, 18(1):4873–4907, 2017. URL https://www.\njmlr.org/papers/volume18/17-214/17-214.pdf.\nTambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning.\nIEEE Transactions on Neural Networks and Learning Systems, 31:3732 – 3740, 2020. doi: 10.1109/tnnls.\n2019.2934906. URL https://ieeexplore.ieee.org/document/8827566.\nDavid Mguni, Taher Jafferjee, Jianhong Wang, Nicolas Perez-Nieves, Yaodong Yang, Tianpei Yang, Matthew\nTaylor, Wenbin Song, Feifei Tong, Hui Chen, Jiangcheng Zhu, and Jun Wang. Learning to shape rewards\nusing a game of two partners. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intel-\nligence, 2023. URL https://arxiv.org/pdf/2103.09159.pdf.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529–533, 2015.\nURL https://www.nature.com/articles/\nnature14236.\nOudeyer Pierre-Yves Moulin-Frier Clément, Nguyen Sao Mai. Self-organization of early vocal development\nin infants and machines: the role of intrinsic motivation. Frontiers in Psychology, 4, 2014. doi: 10.3389/\nfpsyg.2013.01006. URL https://www.frontiersin.org/articles/10.3389/fpsyg.2013.01006.\nSanmit Narvekar and Peter Stone. Learning curriculum policies for reinforcement learning. Proceedings of\nthe 18th International Conference on Autonomous Agents and MultiAgent System, 2019. URL https:\n//dl.acm.org/doi/abs/10.5555/3306127.3331670.\nSanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous task sequencing for customized curriculum\ndesign in reinforcement learning.\nProceedings of the Twenty-Sixth International Joint Conference on\nArtificial Intelligence, 2017. URL https://doi.org/10.24963/ijcai.2017/353.\nAndrew Y Ng, Daishi Harada, and Stuart J Russell.\nPolicy invariance under reward transformations:\nTheory and application to reward shaping.\nProceedings of the Sixteenth International Conference on\nMachine Learning, 1999. URL https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/\nNgHaradaRussell-shaping-ICML1999.pdf.\nPierre-Yves Oudeyer, Frdric Kaplan, and Verena V. Hafner. Intrinsic motivation systems for autonomous\nmental development. IEEE Transactions on Evolutionary Computation, 11(2):265–286, 2007. doi: 10.\n1109/TEVC.2006.890271. URL http://www.pyoudeyer.com/ims.pdf.\nJack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts.\nEffec-\ntive diversity in population based reinforcement learning.\nThirty-third Conference on Neural In-\nformation Processing Systems, 2020.\nURL https://proceedings.neurips.cc/paper/2020/file/\nd1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf.\nJack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette,\nand Tim Rocktäschel. Evolving curricula with regret-based environment design. Proceedings of the 39th\nInternational Conference on Machine Learning, 2022.\nURL https://proceedings.mlr.press/v162/\nparker-holder22a.html.\n21\nPublished in Transactions on Machine Learning Research (06/2023)\nMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas\nSchneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-\ngoal reinforcement learning: Challenging robotics environments and request for research, 2018.\nURL\nhttps://arxiv.org/abs/1802.09464.\nRémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum\nlearning of deep rl in continuously parameterized environments. Proceedings of the Conference on Robot\nLearning, 2019. URL https://proceedings.mlr.press/v100/portelas20a.html.\nMartin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &\nSons, 2014. URL https://dl.acm.org/doi/10.5555/528623.\nSachin Ravi and H. Larochelle. Optimization as a model for few-shot learning. Fifth International Conference\non Learning Representations, 2017. URL https://openreview.net/pdf?id=rJY0-Kcll.\nMartin Riedmiller.\nNeural fitted q iteration–first experiences with a data efficient neural reinforcement\nlearning method. 16th European Conference on Machine Learning, 2005. URL https://link.springer.\ncom/chapter/10.1007/11564096_32.\nNataniel Ruiz, Samuel Schulter, and Manmohan Chandraker. Learning to simulate. Seventh International\nConference on Learning Representations, 2019. URL https://openreview.net/forum?id=HJgkx2Aqt7.\nLuca Sabbioni, Francesco Corda, and Marcello Restelli.\nMeta learning the step size in policy gradient\nmethods. In Eighth International Conference on Machine Learning: Workshop on Automated Machine\nLearning, 2020. URL https://openreview.net/pdf?id=zRn12do9p0.\nTom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. Proceedings of the 30th Interna-\ntional Conference on Machine Learning, 2013. URL https://proceedings.mlr.press/v28/schaul13.\nhtml.\nJ Schmidhuber. On learning how to learn learning strategies (technical report fki-198-94). Fakultat Fur\nInformatik, Technische Universitat Munchen, 1994.\nJürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: The\nmeta-meta-... hook. Diplomarbeit, Technische Universität München, München, 1987.\nNN Schraudolph. Local gain adaptation in stochastic gradient descent. In 1999 Ninth International Con-\nference on Artificial Neural Networks ICANN 99.(Conf. Publ. No. 470), volume 2, pp. 569–574. IET,\n1999.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-\ntion algorithms. ArXiv, 2017. URL http://arxiv.org/abs/1707.06347v2.\nSatinder Singh, Richard L. Lewis, and Andrew G. Barto. Where do rewards come from?\nIn Proceedings\nof the annual conference of the cognitive science society, 2009. URL https://all.cs.umass.edu/pubs/\n2009/singh_l_b_09.pdf.\nJonathan Sorg, Richard L. Lewis, and Andrew G. Barto. Reward design via online gradient ascent. In Ad-\nvances in Neural Information Processing Systems, 2010. URL https://papers.nips.cc/paper_files/\npaper/2010/file/168908dd3227b8358eababa07fcaf091-Paper.pdf.\nRichard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. Tenth\nNational Conference on Artificial Intelligence, 1992. URL https://dl.acm.org/doi/10.5555/1867135.\n1867162.\nRichard S. Sutton. A history of meta-gradient: Gradient methods for meta-learning. ArXiv, 2022. URL\nhttp://arxiv.org/abs/2202.09701v1.\n22\nPublished in Transactions on Machine Learning Research (06/2023)\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive computation\nand machine learning. MIT Press, 2018.\nISBN 0262193981.\nURL http://www.worldcat.org/oclc/\n37293240.\nRichard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for\ntemporal abstraction in reinforcement learning.\nArtificial intelligence, 112(1-2):181–211, 1999.\nURL\nhttps://www.sciencedirect.com/science/article/pii/S0004370299000521.\nSebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn, pp.\n3–17. Springer, 1998.\nHado\nvan\nHasselt.\nDouble\nq-learning.\nTwenty-fourth\nConference\non\nNeural\nInformation\nProcessing\nSystems,\n2010.\nURL\nhttps://proceedings.neurips.cc/paper/2010/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract.html.\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning.\nProceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2016. URL https://dl.acm.\norg/doi/10.5555/3016100.3016191.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. ArXiv, 2016. URL\nhttp://arxiv.org/abs/1611.05763v3.\nLucas\nWillems\nand\nKiran\nKarra.\nPytorch\nactor-critic\ndeep\nreinforcement\nlearning\nalgo-\nrithms:\nA2c\nand\nppo,\n2020.\nURL\nhttps://github.com/lcswillems/torch-ac/tree/\n85d0b2b970ab402e3ab289a4b1f94572f9368dad.\nLijun\nWu,\nFei\nTian,\nYingce\nXia,\nYang\nFan,\nTao\nQin,\nJianhuang\nLai,\nand\nTie-Yan\nLiu.\nLearning\nto\nteach\nwith\ndynamic\nloss\nfunctions.\nThirty-second\nConference\non\nNeural\nInfor-\nmation Processing Systems,\n2018a.\nURL https://proceedings.neurips.cc/paper/2018/file/\n8051a3c40561002834e59d566b7430cf-Paper.pdf.\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. ArXiv,\n2019. URL http://arxiv.org/abs/1911.11361v1.\nYuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in stochastic\nmeta-optimization. Sixth International Conference on Learning Representations, 2018b. URL https:\n//openreview.net/forum?id=H1MczcgR-.\nHan Xiao, Kashif Rasul, and Roland Vollgraf.\nFashion-mnist: a novel image dataset for benchmarking\nmachine learning algorithms. ArXiv, 2017. URL https://arxiv.org/abs/1708.07747.\nZhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. Thirty-second\nConference on Neural Information Processing Systems, 2018. URL https://proceedings.neurips.cc/\npaper/2018/hash/2715518c875999308842e3455eda2fe3-Abstract.html.\nA Steven Younger, Sepp Hochreiter, and Peter R Conwell. Meta-learning with backpropagation. International\nJoint Conference on Neural Networks, 2001. URL https://ieeexplore.ieee.org/document/938471.\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn,\nand Tengyu Ma.\nMopo: Model-based offline policy optimization.\nThirty-fourth Conference on Neu-\nral Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\na322852ce0df73e204b7e67cbbef0d0a-Abstract.html.\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexan-\nder Smola. Deep sets. Thirty-first Conference on Neural Information Processing Systems, 2017. URL\nhttps://papers.nips.cc/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html.\n23\nPublished in Transactions on Machine Learning Research (06/2023)\nXuezhou Zhang, Yuzhe Ma, Adish Kumar Singla, and Xiaojin Zhu.\nAdaptive reward-poisoning attacks\nagainst reinforcement learning. Proceedings of the 37th International Conference on Machine Learning,\n2020. URL http://proceedings.mlr.press/v119/zhang20u/zhang20u.pdf.\nXiaojin Zhu, Adish Kumar Singla, Sandra Zilles, and Anna N. Rafferty. An overview of machine teaching.\nArXiv, 2018. URL https://arxiv.org/pdf/1801.05927.pdf.\nYingda Zhu, Teruaki Hayashi, and Yukio Ohsawa. Gradient descent optimization by reinforcement learning.\nIn Thirty-third Annual Conference of the Japanese Society of Artificial Intelligence, 2019. URL https:\n//www.jstage.jst.go.jp/article/pjsai/JSAI2019/0/JSAI2019_2H4E204/_pdf/-char/ja.\nBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Fifth International\nConference on Learning Representations, 2017. URL https://arxiv.org/pdf/1611.01578.pdf.\n24\nPublished in Transactions on Machine Learning Research (06/2023)\nA\nCode for Experiments\nThe\nsource\ncode\nto\nrun\nour\nexperiments\ncan\nbe\nfound\nin\nthis\nanonymized\ndropbox\nlink:\nhttps://www.dropbox.com/sh/hjkzzgctnqf6d8w/AAAYEycaDvPOeifz8FZbR3kLa?dl=0\nB\nTeacher’s Action Space\nThe diagram highlights how the choice of action space for the teacher enable the teacher to learn varied\npolicies that can be applied across different domains\nFigure 8\nC\nMore Details on Reward Functions\nThe reward function discussed in Section 4.3 is a time-to-threshold reward function for some threshold m∗.\nAnother common criterion trains the learner for T iterations and records the performance at the end. The\nlearning process in this case is a fixed horizon, undiscounted, episodic learning problem and the reward is\nzero everywhere except that rT = m(θT , D). In this setting, the policy that optimizes the learning progress\nalso optimizes the final performance m(θT ). Hence, adding learning progress can be seen as balancing the\ncriteria previously discussed and in Section 4.3: reaching a performance threshold and maximizing overall\nperformance.\nFor reward shaping, one issue with a linear potential is that a constant improvement in performance at lower\nperformance levels is treated as equivalent to higher performance levels. Improving the performance of a\nclassifier, for example, is much more difficult when the performance is higher. One way to account for this\nnon-linearity in the classification setting is to introduce a non-linearity into the shaping, Φ(θ) = log(1−m(θ)).\n25\nPublished in Transactions on Machine Learning Research (06/2023)\nIn the non-linear potential function, we may need to add ϵ to ensure numerical stability. With this nonlinear\nlearning progress, the agent will receive higher rewards for increasing the performance measure at higher\nperformance levels as opposed to lower ones.\nIn addition to learning progress, we can shape with only the new performance m′.\nAssuming that the\nperformance measure is bounded, 0 ≤m′ ≤1, such as for the accuracy of a classifier, we have that −2 ≥\n−1 + m′ ≥0. Because the reward function is still negative, it still encodes the time-to-threshold objective.\nThis, however, changes the optimal policy. The optimal policy will maximize its discounted sum of the\nperformance measure, which is analogous to the area under the curve.\nWhen the performance measure m is not bounded between 0 and 1, as is the case for the sum of rewards when\nthe student is a reinforcement learner, we outline three alternatives. The first is to simply normalize the\nperformance measure if a maximum and minimum is known. The second, when the maximum or minimum\nis not known, is to clip the shaping term to be between −1 and 1. The last possibility, which is used when\nthe scale of the performance measure changes such as in Atari (Mnih et al., 2015), is to treat any increase\n(resp. any decrease) in the performance measure as equivalent. In this case, we cannot use a potential\nfunction and instead shape with a constant, F(s, a, s′) = 2 I(γm′ −m > 0)−1. The teacher receives a reward\nof 1 for increasing the performance measure and a reward of −1 for decreasing the reward function. This\nalso respects the structure of the time-to-threshold reward, while still providing limited feedback about the\nimprovement in the agent’s performance measure.\nD\nNon-Markov Learning Settings\nMost components of the learner’s environment will not depend on more than the current parameters. Adap-\ntive optimizers, however, accumulate gradients and hence depend on the history of parameters.\nIn the\ncontext of reinforcement learning, this introduces partial observability. To enforce the Markov property in\nthe teaching MDP, we would need to include the state of the optimizer or maintain a history of past states\nof the teaching MDP. Both appending the state of the optimizer and maintaining a history can be avoided\nby augmenting the mini-state ˆs = {xi, fθ(xi)}M\ni=1 with additional local information about the change due to\na gradient step, gθ(xi) = fθ−α∇θJ(xi) −fθ(xi) yielding ˆsgrad = {xi, fθ(xi), gθ(xi)}M\ni=1. We will investigate\nthe necessity of this additional state variable in Section 5.2.\nE\nLearning From Outputs Alone in Stationary Problems\nEach of the mini-states is a minibatch of inputs and outputs from the student. This means that training a\nteacher using stochastic gradient descent involves sampling a minibatch of minibatches. When the inputs\nare high-dimensional, such as the case of images, the mini-state that approximates the state can still be\nlarge. The inputs are semantically meaningful and provide context to the teacher for the outputs. Despite\ncontextualizing the output value, the inputs put a large memory burden on training the teacher. We can\nfurther approximate the representation of the parameters by looking at the outputs alone.\nTo see this, suppose hpool is mean pooling and that the joint encoder hjoint is a linear weighting of the\nconcatenated input and output. Then the parametric behavior embedder simplifies\n1\nM\nPM\ni=1 W\n\u0002\nxi, fθ(xi)\n\u0003\n=\nW\n\u0002 1\nM\nPM\ni=1 xi, 1\nM\nPM\ni=1 fθ(xi)\n\u0003\n. For a large enough sample size, and under a stationary distribution x ∼p(x),\n1\nM\nP\ni xi ≈E[xi] is a constant. Hence, if the minibatch batch size is large enough and the distribution on\ninputs is stationary, such as in supervised learning, we can approximate the state θ by the outputs of fθ\nalone. While this intuition is for mean pooling and a linear joint encoding, we will verify empirically that this\nsimplification assumption is valid for both a non-linear encoder and non-linear pooling operation in Section\n5.2.\n26\nPublished in Transactions on Machine Learning Research (06/2023)\nF\nParametric-behavior Embedder As Approximating A Markov State Representation\nIf θ is a Markov state representation, then the Parametric-behavior Embedding (PE) of θ is an approximate\nMarkov state representation. Requiring that θ is a Markov state representation is described in the paper\nand holds for SGD without momentum in supervised learning for example.\nDenote the student’s objective function as J(θ) = Ex,y∼p(x,y)J(fθ(x), y). We show how PE is approximately\nMarkov by showing that the mini-state (which is the input to PE) can represent J(θ) (reward) and ∇θJ(θ)\n(state-transition).\nNote that for any particular θ, the objective function is determined by the input (x), output of the student\n(fθ(x)), and the target (y). Hence, J(θ) is representable as function of the mini-state (the input to PE). It\nremains to show that the objective function after a step of gradient descent is also representable in terms of\nthe mini-state.\nUsing a first-order Taylor expansion: J(θ′(θ)) = J(θ) + (θ′(θ) −θ)∇θJ(θ) + o(α2). As argued previously,\nJ(θ) = Ex,y∼p(x,y)J(fθ(x), y) can be represented in terms of the mini-state. We now turn our attention to\n(θ′(θ) −θ)∇θJ(θ) = α∇θJ(θ) · ∇θJ(θ).\nFor a linear function, fθ(x) = θx, we have that ∇θJ(θ) = Ex,y∼p(x,y)\n\u0002\n∇fθ(x)J(fθ(x), y)x⊤\u0003\n. This means\nthat ∇θJ(θ) is also representable in terms of the mini-state, with inputs x, outputs y and targets fθ(x).\nFor deep linear networks and non-linear networks, we require that the mini-state includes the outputs of\neach layer. However, we have demonstrated empirically that we are able to learn policies using PE without\nthis theoretically needed information in the mini-state.\nOur experiments show that even for deep non-\nlinear neural networks, the student’s outputs (fθ(x)) and the targets (y) is enough to learn a policy that\noutperforms the Markov state (θ) and cruder heuristic approximations used in the literature.\nG\nEfficiently Learning to Reinforcement Teach\nOne criterion for a good Reinforcement Teaching algorithm is low sample complexity.\nInteracting with\nthe teacher’s MDP and evaluating a teacher can be expensive, due to the student, its algorithm or its\nenvironment.\nA teacher’s episode corresponds to an entire training trajectory for the student.\nHence,\ngenerating numerous teacher episodes involves training numerous students. The teacher agent cannot afford\nan inordinate amount of interaction with the student. One way to meet the sample complexity needs of\nthe teacher is to use off-policy learning, such as Q-learning. Offline learning can also circumvent the costly\ninteraction protocol, but may not provide enough feedback on the teacher’s learned policy. There is a large\nand growing literature on offline and off-policy RL algorithms (Yu et al., 2020; Wu et al., 2019; Fujimoto\n& Gu, 2021; Kumar et al., 2020). However, we found that DQN (Mnih et al., 2015; Riedmiller, 2005) and\nDoubleDQN (van Hasselt, 2010; Van Hasselt et al., 2016) were sufficient to learn adaptive teaching behaviour\nand leave investigation of more advanced deep RL algorithms for future work.\n27\nPublished in Transactions on Machine Learning Research (06/2023)\nH\nConnecting Reinforcement Teaching to Gradient-Based Meta-Learning\nSummarized briefly, gradient-based meta-learning (or meta-gradient) methods learn some traditionally non-\nlearnable parts of a machine learning algorithm by backpropagating through the gradient-descent learning\nupdate. Meta-gradient’s broad applicability, relative simplicity, and overall effectiveness make it a common\nframework for meta-learning.\nFor example, Model-Agnostic Meta Learning (MAML) is a meta-gradient\nmethod that can be applied to any learning algorithm that uses gradient-descent to improve few-shot per-\nformance (Finn et al., 2017) and similar ideas have been extended to continual learning (Javed & White,\n2019) and meta RL (Xu et al., 2018). Here we outline how MAML and other meta-gradient methods can be\nviewed relative to Reinforcement Teaching.\nWhen Alg and m are both differentiable, such as when Alg is an SGD update on a fixed dataset, meta-\ngradient methods unroll the computation graph to optimize the meta objective directly. Using MAML as\nan example, the meta-gradient\n∂\n∂θ0 m(fθT , D) can be compute by noticing that fθT = Alg(fθt−1, D) and\nexpanding recursively, we have\nm(fθT , D) = m(Alg(fθT −1, D), D) = m(Alg(· · · Alg(fθ0, D)), D).\nUsing the language of Reinforcement Teaching, we can express MAML and other meta-gradient algorithms\nas a type of Reinforcement Learning algorithm. Meta-gradient algorithms make use of the known gradient-\nupdate model and its connection to the teaching MDP’s state-transition model. If the teacher is interacting\nwith the teaching MDP in such a way that the state-transition model is only a gradient update, then a meta-\ngradient algorithm is analogous to a type of model-based trajectory optimization method. In trajectory\noptimization, an action sequence is planned by unrolling both the state-transition model and the reward\nmodel in simulation.\nThen, the first action in the sequence is taken by the teacher.\nAt the next time\nstep, the teacher must execute the planning procedure again. This planning procedure is costly, requiring\nmemory proportional to the product of the state-space size and the planning horizon, O(|S||T|). Others\nhave also noted, that meta-gradient learning can have difficult to optimize loss landscapes especially as the\nunrolling length of the computation graph increases (Flennerhag et al., 2022). We remark, however, that\nthe Reinforcement Teaching approach described in this work is not mutually exclusive to meta-gradient\nmethods. An interesting direction for future work is combining both model-free (Reinforcement Teaching)\nand model-based (meta-gradient) in one meta-learning algorithm.\nLastly, we provide further discussion on why gradient-based meta-learning is not a good approach for the\nproblems that we address in our experiments. Gradient-based meta-learning, which can be thought of as\nmodel-based trajectory optimization, is only applicable if 1) we have the state-transition model, 2) that state-\ntransition model is fully-differentiable and 3) planning an entire trajectory using the fully-differentiable model\nis computationally feasible. In our reinforcement learning experiments, the state-transition model is not just\na gradient-update but a sequence of interactions between the student’s policy and its environment. While\nthe gradient-update itself is differentiable, the interaction between the student’s policy and the environment\nis not differentiable. Hence, the state-transition model is not differentiable, making gradient-based meta-\nlearning inapplicable. For our supervised learning experiments, gradient-based meta-learning is applicable\nin principle because the state-transition model is just a gradient-update. As discussed in Section 2, however,\ngradient-based optimization requires extensive trajectory-based optimization that is specific to each state\nand student or architecture. Reinforcement teaching allows us to learn a policy that can be queried quickly\nat any step and for any student or architecture, but gradient-based meta-learning requires expensive re-\ncomputation at each time-step. Hence gradient-based meta-learning is computationally infeasible, and less\ngeneralizable to changes in the meta-learning problem, compared to Reinforcement Teaching.\n28\nPublished in Transactions on Machine Learning Research (06/2023)\nI\nEnvironment and Baseline Specification\nIn this section, we will outline the environments used for both the RL and supervised learning experiments.\nI.1\nEnvironments for RL experiments\nMaze\nThe Maze environment is an 11 × 16 discrete grid with several blocked states (see Figure 9). An\nagent can take four deterministic actions: up, down, left, or right. If an agent’s action takes the agent off\nthe grid or into a blocked state, the agent will remain in its original location. See Table 4 for details on the\nenvironment reward. To make this environment more difficult, we limited the max number of time-steps\nper episode to only 40.\nTherefore, the agent cannot simply randomly explore until it reaches the goal.\nFurthermore, in this environment, the teacher’s action will change the student’s start state. The teacher\ncan start the student at 11 possible locations, including the start state of the target task. The teacher’s\naction set contains both impossible tasks (e.g., start states that are completely blocked off) and irrelevant\ntasks (e.g., start states that are not necessary to learn for the target task). This environment is useful to\nstudy for several reasons. First, the reduced maximum time-step makes exploration difficult thus curriculum\nlearning becomes a necessity. Secondly, the set of impossible and irrelevant sub-tasks in the teacher’s action\nset ensure that the teacher is able to learn to avoid these actions and only suggest actions that enable the\nstudent to learn the target task efficiently (i.e., navigating from the blue to green state, see Figure 9).\nFour Rooms\nThe Four Rooms environment is adapted from the MiniGrid suite Chevalier-Boisvert et al.\n(2018). It is a discrete state and action grid-world. Although the state space is discrete, it is very large.\nThe state encodes each grid tile with a 3 element tuple. The tuple contains information on the color and\nobject type in the tile. Due to the large state space, this environment requires a neural network function\napproximator on behalf of the RL student agent. The large state space makes Four Rooms much more\ndifficult than the tabular Maze environment. Similar to the Maze domain, Four Rooms has a fixed start and\ngoal state, as shown in see Figure 10. In addition, the objective is for an agent to navigate from the start state\nto the goal state as quickly as possible. In our implementation, we used the compact state representation\nand reward function provided by the developers. The state representation is fully observable and encodes\nthe color and objects of each tile in the grid. See Table 4 for more details on the environment.\nFetch Reach\nFetch Reach is a continuous state and action simulated robotic environment Plappert et al.\n(2018). It is based on a 7-DoF Fetch robotics arm, which has a two-fingered parallel end-effector (see Figure\n11). In Fetch Reach, the end-effector starts at a fixed initial position, and the objective is to move the\nend-effector to a specific goal position.\nThe goal position is 3-dimensional and is randomly selected for\nevery episode. Therefore, an agent has to learn how to move the end-effector to random locations in 3D\nspace.\nFurthermore, the observations in this environment are 10-dimensional and include the Cartesian\nposition and linear velocity of the end-effector. The actions are 3-dimensional and specify the desired end-\neffector movement in Cartesian coordinates. See Table 4 for more details on the environment. The teacher\ncontrols the goal distribution. The goal distribution determines the location the goal is randomly sampled\nfrom. There are 9 actions in total, each action gradually increasing the maximum distance between the goal\ndistribution and the starting configuration of the end-effector. Therefore, “easier” tasks are ones in which\nthe set of goals are very close to the starting configuration. Conversely, “harder” tasks are ones in which the\nset of goals are far from the starting configuration of the end-effector. It is important to note, however, that\nthe goal distribution of each action subsumes the goal distribution of the previous action. For example, if\naction 1 allows the goal to be sampled within the interval [0, .1], then action 2 allows the goal to be sampled\nwithin the interval [0, .2]. This allows for learning on “easy” tasks to be useful for learning on “harder”\ntasks.\n29\nPublished in Transactions on Machine Learning Research (06/2023)\nFigure 9: Maze: The green square represents the goal state, and the blue square represents the start state\nof the target task. Yellow squares indicate the teacher’s possible actions — possible starting states for the\nstudent.\nFigure 10: Four Rooms: The green square represents the goal state, and the blue square represents the start\nstate of the target task. Yellow squares indicate the teacher’s possible actions — possible starting states for\nthe student.\nFigure 11: Fetch Reach\n30\nPublished in Transactions on Machine Learning Research (06/2023)\nMaze\nFour Rooms\nFetch Reach\nEnv action type\nDiscrete\nDiscrete\nContinuous\nNumber of env actions\n4\n3\nNA\nEnv state space type\nDiscrete\nContinuous\nContinuous\nDimension of env state\n1\n243\n10\nMax number of time-steps\n40\n40\n50\nEnv reward\nR(t) = 0 except R(T) = (.99)T\nR(t) = 0 except R(T) = 1 −0.9 ∗\nT\nmaxsteps\nR(t) = −1 except R(T) = 0\nPerformance Threshold\n.77 (discounted return)\n.6 (discounted return)\n.9 (success rate)\nTable 4: Environment characteristics. T denotes the time-step at termination.\nRL Experiment Baselines\nFor the L2T Fan et al. (2018) baseline, we used the reward function exactly as\ndescribed in the paper. For the state representation, we used an approximation of their state which consisted\nof the teacher’s action, the student’s target task score, the source task score, and the student episode number.\nFor the Narvekar et al. (2017) baseline, we used the time-to-threshold reward function which is a variant of\ntheir reward function. For the state, we used the student parameters, as described in their paper. Lastly,\nfor the Matiisen et al. (2020) baseline, we implemented it as directed by the pseudocode in the paper. We\nalso swept over the tau and alpha hyperparameters, as those were the only hyperparameters required. For\nboth, we swept over the values in {.01, .1, .5, 1.0}.\nI.2\nSupervised Learning\nWe describe the classification datasets used by the student. Note that the teacher’s action is a relative\nchange in the step size, so we also append the current step-size for all state representations.\nSynthetic Classification:\nAt the beginning of each episode, we initialize a student neural network\nwith 2 hidden layers, 128 neurons, and relu activations. The batch size is 64. For each episode, we also\nsample data xi ∼N(0, I), i = 1, . . . , 1000 and 0 ∈R10 and I is the identity matrix. Each xi is labelled\nyi ∈1, . . . , 10 according to its argmax yi = arg max xi. For each step in the environment, the student\nneural network takes a gradient step with a step size determined by the teacher. We use a relative action\nset, where the step size can be increased, kept constant or decreased. This problem was designed so that\nthe default step size of the base optimizer would be able to reach the termination condition within the 200\ntime steps allotted in the episode. Exploration is not a requirement to solve this problem, as we are pri-\nmarily evaluating the state representations for Reinforcement Teaching and the quality of the resulting policy.\n• SGD Variant: Termination condition based on performance threshold of m∗= 0.95, max steps is\n200.\n• Adam Hard Variant: Termination condition based on performance threshold of m∗= 0.99, max\nsteps is 400.\nNeural Network Training Gym: At the beginning of each episode, we initialize a student neural network\nwith 2 hidden layers, 128 neurons, and relu activations. The batch size is 128. For each episode, we also\nsample data xi ∼N(0, I), i = 1, . . . , 4000 and 0 ∈R784 and I is the identity matrix. The data xi are\nclassified by a randomly initialized labeling neural network yi = f ∗(xi). The labeling neural network f ∗\nhas the same number of layers as the student’s neural network but has 512 neurons per layer and tanh\nactivations to encourage a roughly uniform distribution over the 10 class labels.\nMNIST: The student’s neural network is a feed-forward neural network with 128 neurons and 2 hidden\nlayers. The CNN Variant uses a LeNet5 CNN with a batch size of 64. Subsampled dataset to 10000 so that\nan episode covers one epoch of training.\nFashion-MNIST: The student’s neural network is a feed-forward neural network with 128 neurons and 2\nhidden layers. The CNN Variant uses a LeNet5 CNN with a batch size of 256. Subsampled dataset to 10000\nso that an episode covers one epoch of training.\n31\nPublished in Transactions on Machine Learning Research (06/2023)\nCIFAR-10: The student’s neural network is a LeNet5 CNN wih a batch size of 128. Subsampled dataset\nto 10000 so that an episode covers one epoch of training.\n32\nPublished in Transactions on Machine Learning Research (06/2023)\nJ\nHyperparameters for Experiments\nIn this section, we will outline all hyperparameters used for the RL and supervised learning experiments.\nJ.1\nReinforcement Learning Experiments\nTeacher Hyperparameters\nIn the Maze experiments, for the DQN teacher, we performed a grid search\nover batch size ∈{64, 128, 256}, learning rate ∈{.001, .005}, and minibatch ∈{75, 100}. Next, in the Four\nRooms experiments, for the DQN teacher, we performed a grid search over batch size ∈{128, 256}, and\nminibatch ∈{75, 100}. We use a constant learning rate of .001. Lastly, in the Fetch Reach experiments, for\nthe DQN teacher, we performed a grid search over batch size ∈{128, 256}. We use a constant learning rate\nof .001 and mini-batch size of 200. The best hyperparameters for each of the baselines are reported in the\ntables.\nHyperparameters used across all envs\nTeacher agent type\nDQN\nOptimizer\nADAM\nGamma\n.99\nTau\n10−3\nTarget network update frequency\n1\nStarting epsilon\n.5\nEpsilon decay rate\n.99\nValue network\n2 layers with 128 units each, Relu activation\nTable 5: Fixed teacher hyperparameters used across all methods.\nMaze\nBaseline\nBatch size\nLearning rate\nL2T state and LP reward\n128\n.001\nStudent parameters state and LP reward\n64\n.001\nFour Rooms\nBaseline\nBatch size\nLearning rate\nL2T state and LP reward\n128\n.001\nStudent parameters state and LP reward\n128\n.001\nFetch Reach\nBaseline\nBatch size\nLearning rate\nL2T state and LP reward\n256\n.001\nStudent parameters state and LP reward\n128\n.001\nTable 8: Teacher agent hyperparameters for teacher state ablation experiments.\n33\nPublished in Transactions on Machine Learning Research (06/2023)\nMaze\nBaseline\nBatch size\nLearning rate\nMini-batch size\nTau\nAlpha\nPE-Actions and LP (Ours)\n256\n.001\n100\nNA\nNA\nPE-Values and LP (Ours)\n256\n.005\n100\nNA\nNA\nNarvekar et al. (2017)\n64\n.001\nNA\nNA\nNA\nL2T Fan et al. (2018)\n128\n.005\nNA\nNA\nNA\nTCSL Online\nNA\nNA\nNA\n0.1\n1.0\nFour Rooms\nBaseline\nBatch size\nLearning rate\nMini-batch size\nTau\nAlpha\nPE-Actions and LP (Ours)\n128\n.001\n100\nNA\nNA\nPE-Values and LP (Ours)\n128\n.001\n100\nNA\nNA\nNarvekar et al. (2017)\n256\n.001\nNA\nNA\nNA\nL2T Fan et al. (2018)\n128\n.001\nNA\nNA\nNA\nTCSL Online\nNA\nNA\nNA\n0.1\n1.0\nFetch Reach\nBaseline\nBatch size\nLearning rate\nMini-batch size\nTau\nAlpha\nPE-Actions and LP (Ours)\n256\n.001\n200\nNA\nNA\nPE-Values and LP (Ours)\n256\n.001\n200\nNA\nNA\nNarvekar et al. (2017)\n256\n.001\nNA\nNA\nNA\nL2T Fan et al. (2018)\n128\n.001\nNA\nNA\nNA\nTCSL Online\nNA\nNA\nNA\n0.1\n0.5\nTable 6: Teacher agent hyperparameters for all methods (excluding ablation experiments).\nTeacher-Student Protocol Hyperparameters\nThis section contains information about the hyperpa-\nrameters used for the teacher-student interaction protocol.\nMaze\nFour Rooms\nFetch Reach\nStudent training iterations\n100\n50\n50\n# episodes/epochs per student training iteration\n10\n25\n6\n# evaluation episodes/epochs per student training iteration\n30\n40\n80\n# of teacher episodes\n300\n100\n50\nTable 9: Hyperparameters used in the teacher-student training procedure.\nStudent Hyperparameters\nFor the PPO student, we used the open-source implementation in (Willems\n& Karra, 2020). For the DDPG student, we used the OpenAI Baselines implementation Dhariwal et al.\n(2017). We used the existing hyperparameters as in the respective implementations. We did not perform a\ngrid search over the student hyperparameters.\n34\nPublished in Transactions on Machine Learning Research (06/2023)\nMaze\nBaseline\nBatch size\nLearning rate\nMini-batch size\nPE-Actions and Time-to-threshold\n256\n.001\n100\nPE-Values and Time-to-threshold\n128\n.005\n75\nPE-Actions and L2T reward\n256\n.001\n75\nPE-Values and L2T reward\n256\n.001\n100\nPE-Actions and Ruiz et al. (2019) reward\n128\n.001\n100\nPE-Values and Ruiz et al. (2019) reward\n64\n.001\n100\nPE-Actions and Matiisen et al. (2020) reward\n128\n.001\n75\nPE-Values and Matiisen et al. (2020) reward\n128\n.001\n75\nFour Rooms\nBaseline\nBatch size\nLearning rate\nMini-batch size\nPE-Actions and Time-to-threshold\n256\n.001\n75\nPE-Values and Time-to-threshold\n256\n.001\n100\nPE-Actions and L2T reward\n256\n.001\n75\nPE-Values and L2T reward\n256\n.001\n100\nPE-Actions and Ruiz et al. (2019) reward\n256\n.001\n75\nPE-Values and Ruiz et al. (2019) reward\n128\n.001\n100\nPE-Actions and Matiisen et al. (2020) reward\n256\n.001\n75\nPE-Values and Matiisen et al. (2020) reward\n128\n.001\n75\nFetch Reach\nBaseline\nBatch size\nLearning rate\nMini-batch size\nPE-Actions and Time-to-threshold\n256\n.001\n200\nPE-Values and Time-to-threshold\n256\n.001\n200\nPE-Actions and L2T reward\n256\n.001\n200\nPE-Values and L2T reward\n128\n.001\n200\nPE-Actions and Ruiz et al. (2019) reward\n128\n.001\n200\nPE-Values and Ruiz et al. (2019) reward\n256\n.001\n200\nPE-Actions and Matiisen et al. (2020) reward\n128\n.001\n200\nPE-Values and Matiisen et al. (2020) reward\n256\n.001\n200\nTable 7: Teacher agent hyperparameters for teacher reward ablation experiments.\nMaze\nFour Rooms\nFetch Reach\nStudent Agent Type\nTabular Q Learning\nPPO\nDDPG\nOptimizer\nNA\nADAM\nADAM\nBatch size\nNA\n256\n256\nLearning rate\n.5\n.001\n.001\nGamma\n.99\n.99\nNA\nEntropy coefficient/Epsilon\n.01\n.01\nNA\nAdam epsilon\nNA\n10−8\n10−3\nClipping epsilon\nNA\n.2\nNA\nMaximum gradient norm\nNA\n.5\nNA\nGAE\nNA\n.95\nNA\nValue loss coefficient\nNA\n.5\nNA\nPolyak-averaging coefficient\nNA\nNA\n.95\nAction L2 norm coefficient\nNA\nNA\n1\nScale of additive Gaussian noise\nNA\nNA\n.2\nProbability of HER experience replay\nNA\nNA\nNA\nActor Network\nNA\n3 layers with 64 units each, Tanh activation\n3 layers with 256 units each, ReLU activation\nCritic Network\nNA\n3 layers with 64 units each, Tanh activation\n3 layers with 256 units each, ReLU activation\nTable 10: Student hyperparameters.\n35\nPublished in Transactions on Machine Learning Research (06/2023)\nJ.2\nSupervised Learning Experiments\nThe teacher in the supervised learning experiment used DoubleDQN with ϵ-greedy exploration and an ϵ\nvalue of 0.01. The batch size and hidden neural network size was 256. The action-value network had 1\nhidden layer, but the state encoder has 2 hidden layers. There are three actions, one of which keeps the step\nsize the same and the other two increase or decrease the step size by a factor of 2.\nOptenv Sgd\nOptenv Adam\nOptenv Miniabl\nInit Num Episodes\n200\n200\n200\nOptimizer\nADAM\nADAM\nADAM\nBatch Size\n256\n256\n256\nUpdate Freq\n100\n100\n100\nAgentType\nDoubleDQN\nDoubleDQN\nDoubleDQN\nNum Episodes\n200\n200\n200\nNum Env Steps\n2\n2\n2\nHidden Size\n256\n256\n256\nMax Num Episodes\n200\n200\n200\nActivation\nRelu\nRelu\nRelu\nNum Grad Steps\n1\n1\n1\nNum Layers\n1\n1\n1\nInit Policy\nRandom\nRandom\nRandom\nGamma\n0.99\n0.99\n0.99\nMax Episode Length\n200\n400\n200\nTable 11: Fixed hyperparameter settings for (Left-Right): SGD state ablation experiment, Adam state\nablation experiment, Ministate ablation experiment.\nOptenv Reward\nOptenv Pooling\nOptenv Transfer\nInit Num Episodes\n200\n200\n200\nOptimizer\nADAM\nADAM\nADAM\nBatch Size\n256\n256\n256\nUpdate Freq\n100\n100\n100\nAgentType\nDoubleDQN\nDoubleDQN\nDoubleDQN\nNum Episodes\n400\n400\n400\nNum Env Steps\n2\n2\n2\nHidden Size\n256\n256\n256\nMax Num Episodes\n200\n200\n200\nActivation\nRelu\nRelu\nRelu\nNum Grad Steps\n1\n1\n1\nNum Layers\n1\n1\n1\nInit Policy\nRandom\nRandom\nRandom\nGamma\n0.99\n0.99\n0.99\nMax Episode Length\n200\n200\n200\nFigure 12: Fixed hyperparameter settings for (Left-Right): Reward shaping ablation experiment, Pooling\nFunction ablation experiment, Transferring to real data experiment.\n36\nPublished in Transactions on Machine Learning Research (06/2023)\nOptenv Sgd\nPooling Func\n[\"mean\"]\nLr\n[0.001, 0.0005, 0.0001]\nState Representation\n[\"PE-0\", \"PE-x\", \"PE-y\", \"heuristic\", \"parameters\", \"PVN_10\", \"PVN_128\"]\nNum. Seeds\n30\nEnvType\n[\"OptEnv-NoLP-syntheticClassification-SGD\"]\nFigure 13: Other specification and hyperparameters that are swept over in the SGD state ablation experi-\nment.\nOptenv Adam\nPooling Func\n[\"mean\"]\nLr\n[0.001, 0.0005, 0.0001]\nState Representation\n[\"PE-0\", \"PE-0-grad\", \"PE-x-grad\", \"heuristic\"]\nNum. Seeds\n30\nEnvType\n[\"OptEnv-NoLP-syntheticClassification-ADAM\"]\nFigure 14: Other specification and hyperparameters that are swept over in the Adam state ablation experi-\nment.\nOptenv Miniabl\nPooling Func\n[\"mean\"]\nLr\n[0.001, 0.0005, 0.0001]\nState Representation\n[\"PE-0_4\", \"PE-0_8\", \"PE-0_16\", \"PE-0_32\", \"PE-0_64\", \"PE-0_128\"]\nNum. Seeds\n30\nEnvType\n[\"OptEnv-NoLP-syntheticClassification-SGD\"]\nFigure 15: Other specification and hyperparameters that are swept over in the ministate size ablation\nexperiment.\nOptenv Reward\nPooling Func\n[\"mean\"]\nLr\n[0.001, 0.0005, 0.0001]\nState Representation\n[\"PE-0\"]\nNum. Seeds\n30\nEnvType\nOptEnv-[\"L2T\",\"LP\", \"NoLP\"]-syntheticClassification-ADAM\nFigure 16: Other specification and hyperparameters that are swept over in the reward ablation experiment.\nOptenv Pooling\nPooling Func\n[\"attention\", \"max\", \"mean\"]\nLr\n[0.001, 0.0005, 0.0001]\nState Representation\n[\"PE-0\"]\nNum. Seeds\n30\nEnvType\n[\"OptEnv-LP-syntheticClassification-ADAM\"]\nFigure 17: Other specification and hyperparameters that are swept over in the pooling ablation experiment.\n37\nPublished in Transactions on Machine Learning Research (06/2023)\nOptenv Transfer\nPooling Func\n[\"mean\"]\nLr\n[0.001, 0.0005, 0.0001]\nState Representation\n[\"PE-0\", \"heuristic\"]\nNum. Seeds\n30\nEnvType\n[\"OptEnv-LP-syntheticNN-ADAM\"]\nFigure 18: Other specification and hyperparameters that are swept over in transferring to benchmark datasets\nexperiment.\nK\nAdditional RL Experimental Results\nA typical consequence of using RL to train a teacher is the additional training computation. In our method,\nthere is both an inner RL training loop to train the student, and an outer RL training loop to train the\nteacher. Although this is true, we show that our method can greatly improve the teacher’s learning efficiency\nand therefore reduce the overall amount of computation.\nFigure 19: Left: Maze, Middle: Four Rooms, Right: Fetch Reach. This figure shows the teacher’s training\ncurves in the reward ablation experiments. Top: The state is fixed to be PE-Actions. Bottom: The state is\nfixed to be PE-Values. Lower is better.\nFigure 20: Left: Maze, Middle: Four Rooms, Right: Fetch Reach. This figure shows the teacher’s training\ncurves in the state ablation experiments. The reward is fixed to be LP. Lower is better.\n38\nPublished in Transactions on Machine Learning Research (06/2023)\nL\nAdditional Supervised Learning Experimental Results\nL.1\nTraining Curves with Base SGD Optimizer After Meta-Training\nFigure 21: SGD State Ablation experiment.\nTop Student training curves with a trained teacher.\nTop:\nStep sizes selected by the teacher. Right: Same architecture as training. Center: A narrower but deeper\narchitecture. Right: A wider but shallower architecture.\nL.2\nTraining Curves with Base Adam Optimizer After Meta-Training\nFigure 22: Adam State Ablation experiment. Top Student training curves with a trained teacher. Top:\nStep sizes selected by the teacher. Right: Same architecture as training. Center: A narrower but deeper\narchitecture.\nRight: A wider but shallower architecture.\nUnlike using SGD as the base optimizer, the\nReinforcement Teaching Adam optimizer generalizes well in both narrow and wide settings.\n39\nPublished in Transactions on Machine Learning Research (06/2023)\nL.3\nTraining Curves from Ministate Size Ablation\nFigure 23: Synthetic Classification, Adam, Ministate size Ablation. Student training trajectories with a\ntrained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture\nas training, narrower architecture but deeper, wide architecture but shallower.\n40\nPublished in Transactions on Machine Learning Research (06/2023)\nL.4\nTraining Curves from Reward Ablation\nFigure 24: Synthetic Classification, Adam, Reward Ablation. Student training trajectories with a trained\nteacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture as\ntraining, narrower architecture but deeper, wide architecture but shallower.\n41\nPublished in Transactions on Machine Learning Research (06/2023)\nL.5\nTraining Curves from Pooling Ablation\nFigure 25: Synthetic Classification, Adam, Pooling Ablation. Student training trajectories with a trained\nteacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture as\ntraining, narrower architecture but deeper, wide architecture but shallower.\n42\nPublished in Transactions on Machine Learning Research (06/2023)\nL.6\nTraining Curves from Synthetic NN Transfer Gym\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nNN Training Gym\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.1\n0.2\n0.3\n0.4\nAccuracy\nNN Training Gym, Test\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nNN Training Gym, Narrow\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.1\n0.2\n0.3\n0.4\nAccuracy\nNN Training Gym, Test, Narrow\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nNN Training Gym, Wide\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.1\n0.2\n0.3\n0.4\nAccuracy\nNN Training Gym, Test, Wide\nFigure 26: Transfer Gym Experiment using Adam as the base optimizer. Student training trajectories with\na trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: same architecture\nas training, narrower architecture but deeper, wide architecture but shallower.\n43\nPublished in Transactions on Machine Learning Research (06/2023)\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, Mnist\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, Test, Mnist\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, MnistCNN\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, Test, MnistCNN\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, Fashion\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, Test, Fashion\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, FashionCNN\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.2\n0.4\n0.6\n0.8\nAccuracy\nNN Training Gym, Test, FashionCNN\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.1\n0.2\n0.3\n0.4\nAccuracy\nNN Training Gym, Cifar\n0\n50\n100\n150\n200\nNumber of Gradient Steps\n0.1\n0.2\n0.3\n0.4\nAccuracy\nNN Training Gym, Test, Cifar\nFigure 27: Transfer Gym Experiment using Adam as the base optimizer. Student training trajectories with\na trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom: Transfer to\nMNIST, Transfer to MNIST and CNN, transfer to Fashion MNIST, transfer to Fashion MNIST and CNN,\ntransfer to CIFAR and CNN.\n44\nPublished in Transactions on Machine Learning Research (06/2023)\n50\n100\n150\n200\nNumber of Gradient Steps\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nStepsize\nNN Training Gym\n50\n100\n150\n200\nNumber of Gradient Steps\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nStepsize\nNN Training Gym, Narrow\n50\n100\n150\n200\nNumber of Gradient Steps\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nStepsize\nNN Training Gym, Wide\n50\n100\n150\n200\nNumber of Gradient Steps\n0.00\n0.05\n0.10\n0.15\nStepsize\nNN Training Gym, Cifar\n50\n100\n150\n200\nNumber of Gradient Steps\n0.000\n0.005\n0.010\n0.015\n0.020\nStepsize\nNN Training Gym, Mnist\n50\n100\n150\n200\nNumber of Gradient Steps\n?0.01\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nStepsize\nNN Training Gym, MnistCNN\n50\n100\n150\n200\nNumber of Gradient Steps\n0.00\n0.02\n0.04\n0.06\nStepsize\nNN Training Gym, Fashion\n50\n100\n150\n200\nNumber of Gradient Steps\n0.00\n0.02\n0.04\n0.06\n0.08\nStepsize\nNN Training Gym, FashionCNN\nFigure 28: Transfer Gym Experiment using Adam as the base optimizer. Stepsizes selected by a trained\nteacher.\n45\nPublished in Transactions on Machine Learning Research (06/2023)\nL.7\nAdditional Experiment: Accuracy-Based Reward\nFigure 29: Hard Synthetic Classification expeirment using Adam as the base optimizer. Instead of time-\nto-threshold, the reward is just the accuracy at that time step. There is no termination. Student training\ntrajectories with a trained teacher. Left is training accuracy, right is testing accuracy. From top to bottom:\nsame architecture as training, narrower architecture but deeper, wide architecture but shallower.\n46\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2022-04-25",
  "updated": "2025-01-25"
}