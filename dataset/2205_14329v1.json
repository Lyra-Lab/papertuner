{
  "id": "http://arxiv.org/abs/2205.14329v1",
  "title": "Speech Augmentation Based Unsupervised Learning for Keyword Spotting",
  "authors": [
    "Jian Luo",
    "Jianzong Wang",
    "Ning Cheng",
    "Haobin Tang",
    "Jing Xiao"
  ],
  "abstract": "In this paper, we investigated a speech augmentation based unsupervised\nlearning approach for keyword spotting (KWS) task. KWS is a useful speech\napplication, yet also heavily depends on the labeled data. We designed a\nCNN-Attention architecture to conduct the KWS task. CNN layers focus on the\nlocal acoustic features, and attention layers model the long-time dependency.\nTo improve the robustness of KWS model, we also proposed an unsupervised\nlearning method. The unsupervised loss is based on the similarity between the\noriginal and augmented speech features, as well as the audio reconstructing\ninformation. Two speech augmentation methods are explored in the unsupervised\nlearning: speed and intensity. The experiments on Google Speech Commands V2\nDataset demonstrated that our CNN-Attention model has competitive results.\nMoreover, the augmentation based unsupervised learning could further improve\nthe classification accuracy of KWS task. In our experiments, with augmentation\nbased unsupervised learning, our KWS model achieves better performance than\nother unsupervised methods, such as CPC, APC, and MPC.",
  "text": "Speech Augmentation Based Unsupervised Learning\nfor Keyword Spotting\nJian Luo1, Jianzong Wang1∗, Ning Cheng1, Haobin Tang1,2, Jing Xiao1\n1Ping An Technology (Shenzhen) Co., Ltd.\n2University of Science and Technology of China\nAbstract—In this paper, we investigated a speech augmentation\nbased unsupervised learning approach for keyword spotting\n(KWS) task. KWS is a useful speech application, yet also heavily\ndepends on the labeled data. We designed a CNN-Attention\narchitecture to conduct the KWS task. CNN layers focus on\nthe local acoustic features, and attention layers model the long-\ntime dependency. To improve the robustness of KWS model, we\nalso proposed an unsupervised learning method. The unsuper-\nvised loss is based on the similarity between the original and\naugmented speech features, as well as the audio reconstructing\ninformation. Two speech augmentation methods are explored in\nthe unsupervised learning: speed and intensity. The experiments\non Google Speech Commands V2 Dataset demonstrated that\nour CNN-Attention model has competitive results. Moreover, the\naugmentation based unsupervised learning could further improve\nthe classiﬁcation accuracy of KWS task. In our experiments,\nwith augmentation based unsupervised learning, our KWS model\nachieves better performance than other unsupervised methods,\nsuch as CPC, APC, and MPC.\nIndex Terms—Speech Augmentation, Unsupervised Learning,\nKeyword Spotting\nI. INTRODUCTION\nKeyword Spotting (KWS) is a useful speech application\nin real-world scenarios. KWS aims at detecting a relatively\nsmall set of pre-deﬁned keywords in an audio stream, which\nusually exists on the interactive agents. The KWS systems\nusually have two kinds of applications: Firstly, it can detect\nthe startup commands, such as “hey Siri” or “OK, Google”,\nproviding explicit cues for interactions. Secondly, KWS can\nhelp to detect some sensitive words to protect the privacy of the\nspeaker. Therefore, highly accurate and robust KWS systems\ncan be of great signiﬁcance to real speech applications [1]–[3].\nRecently, extensive literature research on KWS has been\npublished [4]–[6]. As a traditional solution, keyword/ﬁller Hid-\nden Markov Model (HMM) has been widely applied to KWS\ntasks, and remains competitive results [7]. In this generative\napproach, an HMM model is trained for each keyword, while\nanother HMM model is trained from not-keyword speech\nsegments. At inference, the Viterbi decoding is required, which\nmight be computationally expensive depending on the HMM\ntopology. In recent years, deep learning models have gained\npopularity on the KWS task, which show better performance\nthan traditional approaches. Google proposed to use Deep\nNeural Networks (DNN) to predict sub-keyword targets. It\nuses the posterior processing method to generate the ﬁnal\n*Corresponding author: Jianzong Wang, jzwang@188.com\nconﬁdence score, and outperforms the HMM-based system [8].\nIn contrast, Convolutional Neural Networks (CNN) is more\nattractive, because DNN ignores the input topology, but audio\nfeatures could have a strong dependency in time or frequency\ndomains [9]–[11]. However, there is a potential drawback\nthat CNN might not model much contextual information.\nAlso, Recurrent Neural Networks (RNN) with Connectionist\nTemporal Classiﬁcation (CTC) loss was also investigated for\nKWS. However, the limitation of RNN is that it directly\nmodels the speech features without learning local structure\nbetween successive time series and frequency steps [12]. There\nare also some works that combined CNN and RNN to improve\nthe accuracy of KWS. For example, Convolutional Recurrent\nNeural Networks (CRNN) and Gated Convolutional Long\nShort-Term Memory (LSTM), achieved better performance\nthan that of only using CNN or RNN [13]. In recent years,\nmany researchers focus on the transformer-based models with\nself-attention mechanism. As a typical model, Bidirectional\nEncoder Representations from Transformer (BERT) has been\nproven to be an effective model in many Natural Language\nProcessing (NLP) tasks [14]–[16]. The transformer-based\nmodels have also obtained much application in Automatic\nSpeech Recognition (ASR) tasks [17], [18]. In this work, we\nintroduced transformer to the network architecture of KWS.\nWe think that transformer encoder has great advantage on the\nspeech representation, and established a CNN-Attention based\nnetwork to deal with the KWS task. The CNN helps network\nto learn the local feature, and the self-attention mechanism of\ntransformer focuses on the long-time information.\nThe above supervised approaches have acquired good per-\nformance, but these models require a lot of labeled datasets.\nObviously, for KWS task, the negative samples could be more\nprocurable than positive samples, meaning that the positive\nsamples might not be obtained easily. Especially when the\nkeyword changes, it requires much time to collect the positive\ntarget samples, and the existing models might not easily\ntransfer to other KWS models. In this paper, we focus on the\nunsupervised learning approach to alleviate this problem. The\nunsupervised learning mechanism allows the neural network to\nbe trained on unlabeled datasets. With unsupervised learning,\nthe performance of downstream task could be improved with\nlimited labeled datasets. Unsupervised learning has made\ngreat success in the audio, image and text tasks [19]. In\nspeech area, researchers also proposed some unsupervised pre-\ntraining algorithms [20]–[22]. Contrastive Predictive Coding\narXiv:2205.14329v1  [cs.SD]  28 May 2022\n(CPC) is one of those unsupervised approaches, extracting\nspeech representation by predicting future information [23].\nApart from CPC, the Autoregressive Predictive Coding (APC)\nis another pre-training model, which also gets comparable\nresults on phoneme classiﬁcation and speaker veriﬁcation\ntasks [24]. Meanwhile, Masked Predictive Coding (MPC)\ndesigns a Masked Language Model (MLM) objective in the\nunsupervised pre-training, and enables the model to incor-\nporate context from both directions [25]. Based on these\nunsupervised learning methods, lots of unlabeled audio data\ncan be used to obtain a better audio representation and this\nrepresentation can be applied to the follow-up tasks through\nﬁne-tuning mechanism. For a robust KWS system, it should\ndeal with different styles of speech in real-world applications.\nSpeed and volume are major variations of the speech style.\nUnlike traditional unsupervised learning focuses on the general\naudio representation, we proposed an augmentation based\napproach. Our approach is to improve the model performance\non KWS task with different speed and intensity situations.\nWe designed an unsupervised loss based on the distance\nbetween the original and augmented speech, as well as the\naudio reconstructing information for auxiliary training. We\nthink that speech utterances with the same keyword but at\ndifferent speeds or volumes should have similar high-level\nfeature representations for KWS tasks.\nThis paper investigated unsupervised speech representative\nmethods to conduct KWS task. The unsupervised learning\nmethods could utilize a lot of unlabeled audio datasets to\nimprove the performance of downstream KWS task when\nlabeled data are limited. In addition, speech augmentation\nbased unsupervised representation might help the network to\nlearn the speech information in various speech styles, and get a\nmore robust performance. In summary, our major contributions\nof this work are the followings:\n• Propose a CNN-Attention architecture for keyword spot-\nting task, having competitive results on Google Speech\nCommands V2 Dataset.\n• Design an unsupervised loss based on the Mean Square\nError (MSE) to measure the distance between the original\nand augmented speech.\n• Deﬁne a speech augmentation based unsupervised learn-\ning approach, utilizing the similarity between the bottle-\nneck layer feature, as well as the audio reconstructing\ninformation for auxiliary training.\nThe rest of the paper is organized as follows. Sec. II\nhighlights the related prior works about data augmentation,\nunsupervised learning, and other methodologies of KWS tasks.\nSec. III describes the proposed model architecture and aug-\nmentation based unsupervised learning loss. Sec. IV reports\nthe experimental results compared with other pre-training\nmethods. We also discuss relationship between pre-training\nsteps and performance of downstream KWS tasks. In Sec. V,\nwe conclude with the summary of the paper and future works.\nII. RELATED WORK\nData augmentation is a common strategy to enlarge the\ntraining set of speech applications, such as Automatic Speech\nRecognition (ASR) and Keyword Spotting (KWS). The\nwork [26] studied the vocal tract length perturbation method to\nimprove the performance of ASR systems. The work [27] in-\nvestigated a speed-perturbation technique to change the speed\nof the audio signal. Noisy audio signals have been used in [28],\ncorrupting clean training speech with noise signal, to improve\nthe robustness of the speech recognizer. SpecAugment [29] is\na spectral-domain augmentation whose effect is to mask bands\nof frequency and/or time axes. SpecAugment is also explored\nfurther on large scale dataset in [30]. WavAugment [31]\ncombines pitch modiﬁcation, additive noise and reverberation\nto increase the performance of Contrastive Predictive Coding\n(CPC). In this work, we apply the speed and volume pertur-\nbation in our speech augmentation method.\nAlthough supervised learning has been the major approach\nin keyword spotting area, current supervised learning models\nrequire large amounts of labeled data. Those high quality\nlabeled datasets require substantial effort and are hardly\navailable for the less frequently used languages. For this\nreason, recently there has been a great surge of interest in\nweakly supervised solutions that use datasets with few human\nannotations. Noisy student training, a semi-supervised learning\nmethod was proposed to ASR [32] and later used for robust\nkeyword spotting [33]. There also have been related researches\ninvestigating the use of unsupervised methods to perform\nkeyword spotting [34]–[36]. [34] proposed a self-organizing\nspeech recognizer, and minimal transcriptions are used to train\na grapheme-to-sound-unit converter. [35] presented a prototype\nKWS system that doesn’t need manually transcribed data to\ntrain the acoustic model. In [36], the authors proposed an un-\nsupervised learning framework without transcription. A GMM\nmodel is used to label keyword samples and test utterances by\nGaussian posteriorgram. After that, segmental dynamic time\nwarping (SDTW) gives a relevant score, and ranks the score\nto ﬁgure out the output. The feasibility and effectiveness of\nthese results encourage us to introduce unsupervised learning\nframework to the task of keyword spotting.\nGoogle Speech Commands V2 Dataset, is a well-studied\nand benchmarked dataset for novel ideas in KWS. A lot\nof previous works perform experiments on this dataset. [37]\nintroduced a convolutional recurrent network with attention\non multiple KWS tasks. MatchboxNet [38] is a deep residual\nnetwork composed from 1D time-channel separable convolu-\ntion, batch-norm layers, ReLU and dropout layers. Inspired\nby [37] and [38], EdgeCRNN [39] was proposed, an edge-\ncomputing oriented model of acoustic feature enhancement\nfor keyword spotting. Recently, [40] combined a triplet loss-\nbased embedding and a variant of K-Nearest Neighbor (KNN)\nfor classiﬁcation. We also evaluated our speech augmen-\ntation based unsupervised learning method on this dataset,\nand compared with other unsupervised approaches, including\nCPC [23], APC [24] and MPC [25].\nIII. PROPOSED METHOD\nA. KWS Model Architecture\nThe keyword spotting task could be described as a sequence\nclassiﬁcation task. The keyword spotting network maps an\ninput audio sequence X = (x0, x1, ..., xT ) to a limited of\nkeyword classes Y ∈y1:S. In which, T is the number of\naudio frames and S is the number of classes. Our proposed\nmodel architecture for keyword spotting is shown in Fig 1. The\nnetwork contains ﬁve parts: (1) CNN Block, (2) Transformer\nBlock, (3) Feature Selecting Layer, (4) Bottleneck Layer, and\n(5) Project Layer.\nFig. 1. The Architecture of our CNN-Attention model for keyword spotting\ntask. The network is composed of CNN layers, self-attention layers, feature\nselecting layer, bottleneck layer, and project layer. In the feature selecting\nlayer, the last few frames are selected. Finally, the project layer maps the\nfeatures to predict the keyword classiﬁcation.\nThe CNN block consists of several 2D-convolutional layers,\nhandling the local variance on time and spectrum axes.\nEcnn = 2DConv×N(X)\n(1)\nIn which, N is the number of convolutional layers. Then, the\nCNN output Ecnn is inputted to the transformer block, to\ncapture long-time information with self-attention mechanism.\nEtran = SelfAttention×M(Ecnn)\n(2)\nIn which, M is the number of self-attention layers. After\ntransformer block, we designed a feature selecting layer to\nextract keyword information from sequence Etran.\nEfeat = Concat(Etran[T −r, T])\n(3)\nIn feature selecting layer, we ﬁrstly collect last r frames of\nEtran. And then, we concatenate all the collected frames\ntogether, into one feature vector Efeat. After feature selecting\nlayer, we use a bottleneck layer and a project layer, projecting\nthe hidden states to the predicted classiﬁcation classes ˜Y .\nEbn = FCbn(Efeat)\n(4)\n˜Y = FCproj(Ebn)\n(5)\nFinally, the the cross-entropy (CE) loss for supervised learning\nand model ﬁne-tuning is calculated via predicted classes ˜Y and\nground truth classes Y .\nLce = CE(Y, ˜Y )\n(6)\nB. Augmentation Method\nData augmentation are the most common used methods\nto promote the robustness and performance of the model in\nspeech tasks. In this work, speed and volume based augmen-\ntation are investigated in the unsupervised learning of keyword\nspotting. For a given audio sequence X, we denote it as the\namplitude A and time index t.\nX = A(t)\n(7)\nFor speed augmentation, we set a speed ratio λspeed to adjust\nthe speed of X.\nXaug = A(λspeedt)\n(8)\nFor volume augmentation, we also set an intensity ratio\nλvolume to change the volume of X.\nXaug = λvolumeA(t)\n(9)\nWith different ratios λspeed and λvolume, we could obtain\nmultiple speech sequence pairs (X, Xaug), to train the audio\nrepresentation network with unsupervised learning. We think\nthat speech utterances at different speed or volume should have\nsimilar high-level feature representation for KWS tasks.\nC. Unsupervised Learning Loss\nThe overall architecture of augmentation based unsuper-\nvised learning is shown in Fig 2. Similar to other unsupervised\nmethods, the proposed approach also consists of two stages:\n(1) pre-training on unsupervised data, and (2) ﬁne-tuning on\nsupervised KWS data. In the pre-training stage, the bottleneck\nfeature was obtained through training the unlabeled speech. In\nﬁne-tuning stage, the extracted bottleneck features are used for\nKWS prediction.\nIn the pre-training stage, the pair speech data (X, Xaug) are\ninputted into the CNN-Attention models respectively, but with\nthe same model parameters. Because Xaug comes from X, our\ndesigned unsupervised methods expect that X and Xaug will\noutput similar high-level bottleneck features. It means that no\nmatter how fast or how loud a speaker says, the content of the\nspeech is the same. Thus, the optimization of network needs\nto reﬂect the similarity of X and Xaug. We choose the Mean\nSquare Error (MSE) Lsim to measure the distance between\nthe output of X and Xaug.\nLsim =\n1\nUbn\nUbn\nX\nu=0\n|Ebn(u) −Eaug\nbn (u)|2\n(10)\nWhere Ubn represents the dimension of the bottleneck feature\nvector. Ebn and Eaug\nbn\nare the output of bottleneck layer of\noriginal speech X and augmented speech Xaug respectively.\nFig. 2. The proposed speech augmentation based audio unsupervised learning\nmethod. In the pre-training stage, the pair of original and augmented speech\nwill be inputted into the network separately but with the same model\nparameters. The network will output the average speech feature values and\nthe bottleneck feature. The two bottleneck features are calculated by MSE\nloss, since the augmented and original speech should output similar high-\nlevel features for keyword spotting.\nIn addition, the designed network has another branch for\nauxiliary training, which predicts the average feature of the\ninput speech segment. This branch guides the network to learn\nthe intrinsic feature of the speech utterance. We ﬁrstly compute\nthe average vector of the input Fbank vector X alongside\nthe time axis t. Then, we use another reconstructing layer\nattached to the bottleneck layer, to reconstruct the average\nFbank vector\n˜X. We also use MSE loss Lx to calculate\nthe similarity between these two audio vectors alongside the\nfeature dimension Ux.\nX = 1\nT\nX\nT\n(X)\n˜X = FCreconstruct(Ebn)\nLx = 1\nUx\nUx\nX\nu=0\n|X(u) −˜X(u)|2\n(11)\nIn which, Ux represents the dimension of Fbank feature\nvector, and X denotes the average vector of X. Similarly, the\nloss Laug\nx\nbetween the augmented average audio X aug and\nured feature ˜Xaug could be deﬁned as follows:\nLaug\nx\n= 1\nUx\nUx\nX\nu=0\n|X aug(u) −˜Xaug(u)|2\n(12)\nTherefore, the ﬁnal loss function Lul of the unsupervised\nlearning (UL) consists of the above three losses Lsim, Lx, and\nLaug\nx\n.\nLul = λ1Lsim + λ2Lx + λ3Laug\nx\n(13)\nWhere λ1, λ2, λ3 are factor ratio of each loss component.\nIn ﬁne-tuning stage, the branch of average feature prediction\nis removed. A project layer and a softmax layer are added after\nthe bottleneck layer to make the KWS prediction. In the ﬁne-\ntuning, the parameters of original network could be ﬁxed or\nupdated. In our experiments, we found that updating all the\nparameters could help to improve the performance. Thus, we\nchoose to update all parameters in the ﬁne-tuning stage.\nIV. EXPERIMENTS\nIn this section, we evaluated the proposed method in\nkeyword spotting tasks. We implemented our CNN-Attention\nmodel with supervised training and compared it with Google’s\nmodel. We also made an ablation study, to explore the effect\nof speed and volume augmentation on unsupervised learn-\ning. What’s more, other unsupervised learning methods are\ncompared with our approach, including CPC, APC, MPC.\nWhen implementing these approaches, we used the network\nand hyperparameters in their publications, but all experimental\ntricks were not leveraged [23]–[25]. We also discuss the\nimpact of different pre-training steps on the performance and\nconvergence of downstream KWS task.\nA. Datasets\nWe used Google’s Speech Commands V2 Dataset [41] for\nevaluating the proposed models. The dataset contains about\n106000 one-second or more long utterances. Total 30 short\nwords were recorded by thousands of different people, as\nwell as background noise such as pink noise, white noise, and\nhuman-made sounds. The KWS task is to discriminate among\n12 classes: “yes”, “no”, “up”, “down”, “left”, “right”, “on”,\n“off”, “stop”, “go”, unknown, or silence. The dataset was split\ninto training, validation, and test sets, with 80% training, 10%\nvalidation, and 10% test. This results in about 37000 samples\nfor training, and 4600 each for validation and testing. We\nTABLE I\nMODEL CONFIGURATIONS\nUnit Name\nHyperparameters\n#CNN Blocks\nM = 2 layers, 3 × 3 kernel, 2 × 2 stride, 32 channels\n#Transformer Block\nN = 2 layers, dimension = 320, 4 head, feedforward = 1024\n#Feature Selecting Layer\nLast r = 2 frames, 2 × 320 dimension\n#Bottleneck Layer\none FC layer, 800 dimension\n#Project Layer\none FC layer, 12 dimension softmax\n#Reconstruct Layer\none FC layer, 40 dimension softmax\n#Factor Ratio\nλ1 = 0.9, λ2 = 0.05, λ3 = 0.05\nTABLE II\nRESULTS COMPARISON OF KWS MODEL, CLASSIFICATION ACCURACY (%)\nModel Name\nSupervised Training Data\nDev\nEval\nSainath and Parada (Google)\nSpeech Commands\n-\n84.7\nCNN-Attention (ours)\nSpeech Commands\n86.4\n85.3\nCNN-Attention + volume & speed augment (ours)\nSpeech Commands\n87\n85.7\nTABLE III\nABLATION STUDY, THE EFFECT OF SPEED AND VOLUME AUGMENTATION, CLASSIFICATION ACCURACY (%)\nModel Name\nPre-training Data\nFine-tuning Data\nDev\nEval\nCNN-Attention + volume pre-training\nSpeech Commands\nSpeech Commands\n86.1\n85.9\nCNN-Attention + speed pre-training\nSpeech Commands\nSpeech Commands\n87.8\n86.9\nCNN-Attention + volume & speed pre-training\nSpeech Commands\nSpeech Commands\n87.9\n87.2\nCNN-Attention + volume pre-training\nLibrispeech-100\nSpeech Commands\n86.3\n86.0\nCNN-Attention + speed pre-training\nLibrispeech-100\nSpeech Commands\n87.9\n87.9\nCNN-Attention + volume & speed pre-training\nLibrispeech-100\nSpeech Commands\n88.2\n88.1\nused the real noisy data HuNonspeech1 to corrupt the original\nspeech. In the experiments, the Aurora4 tools were used to\nimplement this strategy2. Each utterance will be randomly\ncorrupted by public 100 kinds of noise in HuNonspeech. Each\nutterance has a level of 0-20dB Signal Noise Ratio (SNR), and\nall datasets have an average 10dB SNR.\nSimilar to other unsupervised methods, a large unlabeled\ncorpus, 100 hours of Librispeech [42] clean speech were\nalso leveraged to pre-train the network by unsupervised\nlearning. Firstly, the long utterances were split up into 1\nsecond segments, keeping consistent with Speech Commands\ndatasets. Nextly, the clean segments were also mixed with\nnoisy HuNonspeech data by Aurora 4 tools, and the corrupted\nmechanism was as same as the Speech Commands.\nB. Experimental Setups\nThe acoustic features were 40-dimensional log-mel ﬁlter-\nbank with 30ms frame length and 10ms frame shift. The\ndetailed hyperparameters of our proposed network were shown\nin Table I. For training the KWS model, all of the matrix\nweights are initialized with random uniform initialization, and\nthe bias parameters are initialized with the constant value 0.1.\nIn our experiments, we trained all the networks with Adam\noptimizer for 30k steps with a batchsize 200 until the loss\nbecomes little change. In addition, the factor ratios of loss λ1,\nλ2, and λ3 are set to 0.9, 0.05, 0.05 respectively.\nTo demonstrate the effectiveness of our proposed model,\nwe investigated several other approaches for comparison. For\nsupervised learning, we used Sainath and Parada’s model\nby Google [43] as the baseline model. The Google blog\npost released the Sainath and Parada’s model implemented\nby TensorFlow. For unsupervised learning, we compared our\nmethod with other pre-training models:\n• Contrastive Predictive Coding (CPC) [23]: Through an\nunsupervised mechanism by utilizing next step prediction,\nCPC learns representations from high-dimensional signal.\n1http://web.cse.ohio-state.edu/pnl/corpus/HuNonspeech/\n2http://aurora.hsnr.de/index-2.html\nThe CPC network mainly contains a non-linear encoder\nand an autoregressive decoder. An input sequence is\nembedded to a latent space, producing a context repre-\nsentation. Targeting at predicting future observations, the\ndensity ratio is established to maximize the mutual infor-\nmation between future observations and current context\nrepresentation.\n• Autoregressive Predictive Coding (APC) [24]: APC\nalso belongs to the family of predictive models. APC\ndirectly optimizes L1 loss between input sequence and\noutput sequence. APC has proved an effective method\nin recent language model pre-training task and speech\nrepresentation.\n• Masked Predictive Coding (MPC) [25]: Inspired by\nBERT, MPC uses Masked Language Model (MLM)\nstructure to perform predictive coding on Transformer\nbased models. Similar to BERT, 15% of feature frames\nin each utterance are chosen to be masked during the\npre-training procedure. Among these chosen frames, 80%\nare replaced with zero vectors, 10% are replaced with\nrandom positions, and the rest remain unchanged. L1 loss\nis computed between masked input features and encoder\noutput at corresponding position. Dynamic masking was\nalso adopted where the masking pattern is generated when\na sequence is fed into the model.\nC. Results\nTable II lists the experimental results of supervised learning\nwith Speech Commands dataset. We ﬁrstly implemented the\nGoogle’s Sainath and Parada model by the original TensorFlow\nrecipes, achieving the accuracy of 84.7%. Secondly, our CNN-\nAttention model is implemented by supervised loss Lce with-\nout any augmented data and achieved 0.6% higher accuracy\nthan Google’s model. It is proved that our designed CNN-\nAttention architecture is effective for KWS task. Finally, after\nadding speed and volume augmentation to speech, we got a\nhigher accuracy. It corresponds with the existing research that\naugmented dataset is helpful for improving the performance\nTABLE IV\nCOMPARED WITH OTHER UNSUPERVISED LEARNING METHODS, CLASSIFICATION ACCURACY (%)\nModel Name\nPre-training Data\nFine-tuning Data\nDev\nEval\nContrastive Predictive Coding (CPC) [23]\nSpeech Commands\nSpeech Commands\n87.6\n86.9\nAutoregressive Predictive Coding (APC) [24]\nSpeech Commands\nSpeech Commands\n87.2\n86.5\nMasked Predictive Coding (MPC) [25]\nSpeech Commands\nSpeech Commands\n87.0\n86.7\nCNN-Attention + volume & speed pre-training (ours)\nSpeech Commands\nSpeech Commands\n87.9\n87.2\nContrastive Predictive Coding (CPC) [23]\nLibrispeech-100\nSpeech Commands\n87.8\n87.4\nAutoregressive Predictive Coding (APC) [24]\nLibrispeech-100\nSpeech Commands\n87.7\n87.5\nMasked Predictive Coding (MPC) [25]\nLibrispeech-100\nSpeech Commands\n87.9\n87.0\nCNN-Attention + volume & speed pre-training (ours)\nLibrispeech-100\nSpeech Commands\n88.2\n88.1\nof the model. It also inspires our motivation for building\naugmentation based unsupervised learning methods.\nTo analyze the effect of speed and volume augmentation on\nunsupervised learning, we also made an ablation study in our\nexperiments. The experimental results are shown in Table III.\nThe volume pre-training model means that the augmented\nspeech pairs (X, Xaug) only contain the intensity augment\ndata. Meanwhile, the speed pre-training model is trained\nonly by speed augmented pairs. For better investigation, we\npre-trained the model with two datasets by unsupervised\nlearning loss Lul. The results indicate that speed augmented\nunsupervised learning has better performance than intensity\nbased augmented pre-training. With both volume and speed\naugmentation, we could achieve better classiﬁcation accuracy\nthan only with single augmentation method. In addition, large\ndatasets pre-training (Librispeech-100) results in better perfor-\nmance than small datasets (Speech Commands). Our proposed\naugmentation based unsupervised method (Eval 87.2% in\nTable III) also promotes the accuracy of adding augmentation\nto supervised training (Eval 85.7% in Table II) even with the\nsame training data.\nAfter that, we established the CPC, APC, MPC and made\nthe comparison with these unsupervised learning methods. As\ndepicted in Table IV, CPC achieves better performance than\nAPC and MPC. Our augmentation based approach outperforms\nall of the other unsupervised methods on both two pre-\ntraining datasets (Speech Commands and Librispeech-100).\nThe comparison demonstrated that our proposed augmentation\nbased unsupervised learning is capable of extracting the speech\ninformation, and is an effective approach for KWS tasks.\nD. Pre-training Analysis\nMore pre-training steps usually help to improve the per-\nformance of downstream tasks. To get a better understanding\nof our unsupervised approach, we also conducted experiments\nwith different pre-training steps. The 5K, 10K, 20K, 30K pre-\ntraining steps were used for making this comparison. The\nperformance of different steps is plotted in Fig 3.\nWe show the model training of supervised learning with\nthese different steps of pre-training. Our experiments demon-\nstrated that more pre-training steps are not only helpful for\nachieving better performance but also making downstream\nKWS task converge faster. Unsupervised learning with 30K\nsteps has the highest classiﬁcation accuracy and the fastest\nFig. 3.\nThe results comparison with different pre-training steps. Different\npre-training steps of unsupervised learning result in different accuracy per-\nformance and ﬁne-tuning convergence. In our experiments, pre-training 30K\nsteps have the highest classiﬁcation accuracy, and fastest convergence.\nconvergence. It also should be noted that the difference be-\ntween 20K and 30K was very close, meaning that the pre-\ntraining steps are enough to obtain the desired performance.\nV. CONCLUSION\nThis paper investigated unsupervised learning method for\nkeyword spotting task. We designed a CNN-Attention ar-\nchitecture and achieved competitive results on the Speech\nCommands dataset. In addition, we proposed a speech aug-\nmentation based unsupervised learning approach for KWS.\nOur method uses speed and intensity augmentation to establish\ntraining pairs, and pre-trains the network via the similarity loss\nbetween the speech pair and the speech reconstructed loss. In\nour experiments, the proposed unsupervised approach could\nfurther improve the model performance, and outperform other\nunsupervised methods, such as CPC, APC and MPC. We also\nfound that more pre-training steps are not only helpful for\nbetter performance but also for faster convergence. In future\nworks, we are interested in applying the augmentation based\nunsupervised learning approach to other speech tasks, such as\nspeaker veriﬁcation and speech recognition.\nVI. ACKNOWLEDGEMENT\nThis paper is supported by the Key Research and De-\nvelopment Program of Guangdong Province under grant\nNo. 2021B0101400003. Corresponding author is Jianzong\nWang from Ping An Technology (Shenzhen) Co., Ltd\n(jzwang@188.com).\nREFERENCES\n[1] B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bacchiani, A. Misra,\nI. Shafran, H. Sak, G. Pundak, K. K. Chin et al., “Acoustic modeling\nfor google home.” in Conference of the International Speech Communi-\ncation Association (INTERSPEECH), 2017.\n[2] J. Luo, J. Wang, N. Cheng, G. Jiang, and J. Xiao, “End-to-end silent\nspeech recognition with acoustic sensing,” in IEEE Spoken Language\nTechnology Workshop (SLT), 2021.\n[3] J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne, C. Chelba, M. Co-\nhen, M. Kamvar, and B. Strope, “your word is my command: Google\nsearch by voice: A case study,” Springer Advances in speech recognition,\n2010.\n[4] X. Wang, S. Sun, C. Shan, J. Hou, L. Xie, S. Li, and X. Lei, “Adversarial\nexamples for improving end-to-end attention-based small-footprint key-\nword spotting,” in IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2019.\n[5] A.\nRosenberg,\nK.\nAudhkhasi,\nA.\nSethy,\nB.\nRamabhadran,\nand\nM. Picheny, “End-to-end speech recognition and keyword search on low-\nresource languages,” in IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2017.\n[6] C. Shan, J. Zhang, Y. Wang, and L. Xie, “Attention-based end-\nto-end\nmodels\nfor\nsmall-footprint\nkeyword\nspotting,”\nin\narXiv\npreprint:1803.10916, 2018.\n[7] M.-C. Silaghi, “Spotting subsequences matching an hmm using the\naverage observation probability criteria with application to keyword\nspotting,” in The Association for the Advancement of Artiﬁcial Intel-\nligence (AAAI), 2005.\n[8] G. Chen, C. Parada, and G. Heigold, “Small-footprint keyword spotting\nusing deep neural networks,” in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2014.\n[9] R. Tang and J. Lin, “Deep residual learning for small-footprint keyword\nspotting,” in IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2018.\n[10] J. Luo, J. Wang, N. Cheng, and J. Xiao, “Unidirectional memory-\nself-attention transducer for online speech recognition,” in IEEE In-\nternational Conference on Acoustics Speech and Signal Processing\nProceedings (ICASSP), 2021.\n[11] M. Xu and X.-L. Zhang, “Depthwise Separable Convolutional ResNet\nwith Squeeze-and-Excitation Blocks for Small-Footprint Keyword Spot-\nting,” in Conference of the International Speech Communication Asso-\nciation (INTERSPEECH), 2020.\n[12] M. Sun, A. Raju, G. Tucker, S. Panchapagesan, G. Fu, A. Mandal,\nS. Matsoukas, N. Strom, and S. Vitaladevuni, “Max-pooling loss train-\ning of long short-term memory networks for small-footprint keyword\nspotting,” in IEEE Spoken Language Technology Workshop (SLT), 2016.\n[13] S. O. Arik, M. Kliegl, R. Child, J. Hestness, A. Gibiansky, C. Fougner,\nR. Prenger, and A. Coates, “Convolutional recurrent neural networks for\nsmall-footprint keyword spotting,” in Conference of the International\nSpeech Communication Association (INTERSPEECH), 2017.\n[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems (NIPS), 2017.\n[15] X. Jia, J. Wang, Z. Zhang, N. Cheng, and J. Xiao, “Large-scale transfer\nlearning for low-resource spoken language understanding,” in IEEE\nConference of the International Speech Communication Association\n(INTERSPEECH), 2020.\n[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” in arXiv\npreprint:1810.04805, 2018.\n[17] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., “A\ncomparative study on transformer vs rnn in speech applications,” in\nIEEE Workshop on Automatic Speech Recognition and Understanding\n(ASRU), 2019.\n[18] J. Luo, J. Wang, N. Cheng, G. Jiang, and J. Xiao, “Multi-quartznet:\nMulti-resolution convolution for speech recognition with multi-layer\nfeature fusion,” in IEEE Spoken Language Technology Workshop (SLT),\n2021.\n[19] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation\nlearning by predicting image rotations,” in arXiv preprint:1803.07728,\n2018.\n[20] Y.-C. Wang, S. Venkataramani, and P. Smaragdis, “Self-supervised learn-\ning for speech enhancement,” in Proceedings of the 37-th International\nConference on Machine Learning (ICML), 2020.\n[21] J. Luo, J. Wang, N. Cheng, and J. Xiao, “Dropout regularization for\nself-supervised learning of transformer encoder speech representation,”\nin IEEE Conference of the International Speech Communication Asso-\nciation (INTERSPEECH), 2021.\n[22] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec:\nUnsupervised\npre-training\nfor\nspeech\nrecognition,”\nin\narXiv\npreprint:1904.05862, 2019.\n[23] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” in arXiv preprint:1807.03748, 2018.\n[24] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised au-\ntoregressive model for speech representation learning,” in Conference of\nthe International Speech Communication Association (INTERSPEECH),\n2019.\n[25] D. Jiang, X. Lei, W. Li, N. Luo, Y. Hu, W. Zou, and X. Li, “Improving\ntransformer-based speech recognition using unsupervised pre-training,”\nin arXiv preprint:1910.09932, 2019.\n[26] N. Jaitly and G. E. Hinton, “Vocal tract length perturbation (vtlp)\nimproves speech recognition,” in ICML Workshop on Deep Learning\nfor Audio, Speech and Language, 2013.\n[27] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation\nfor speech recognition,” in Sixteenth annual conference of the interna-\ntional speech communication association (INTERSPEECH), 2015.\n[28] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep speech:\nScaling up end-to-end speech recognition,” in arXiv preprint:1412.5567,\n2014.\n[29] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V. Le, “Specaugment: A simple data augmentation method for\nautomatic speech recognition,” in arXiv preprint:1904.08779, 2019.\n[30] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le,\nand Y. Wu, “Specaugment on large scale datasets,” in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\n[31] E. Kharitonov, M. Rivi`ere, G. Synnaeve, L. Wolf, P.-E. Mazar´e,\nM. Douze, and E. Dupoux, “Data augmenting contrastive learning of\nspeech representations in the time domain,” in 2021 IEEE Spoken\nLanguage Technology Workshop (SLT), 2021.\n[32] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V.\nLe, “Improved noisy student training for automatic speech recognition,”\nin arXiv preprint:2005.09629, 2020.\n[33] H.-J. Park, P. Zhu, I. L. Moreno, and N. Subrahmanya, “Noisy\nstudent-teacher\ntraining\nfor\nrobust\nkeyword\nspotting,”\nin\narXiv\npreprint:2106.01604, 2021.\n[34] A. Garcia and H. Gish, “Keyword spotting of arbitrary words using min-\nimal speech resources,” in IEEE International Conference on Acoustics\nSpeech and Signal Processing Proceedings (ICASSP), 2006.\n[35] P. Li, J. Liang, and B. Xu, “A novel instance matching based unsuper-\nvised keyword spotting system,” in Second International Conference on\nInnovative Computing, Informatio and Control (ICICIC), 2007.\n[36] Y. Zhang and J. R. Glass, “Unsupervised spoken keyword spotting\nvia segmental dtw on gaussian posteriorgrams,” in IEEE Workshop on\nAutomatic Speech Recognition & Understanding (ASRU), 2009.\n[37] D. C. de Andrade, S. Leo, M. L. D. S. Viana, and C. Bernkopf,\n“A neural attention model for speech command recognition,” in arXiv\npreprint:1808.08929, 2018.\n[38] S. Majumdar and B. Ginsburg, “Matchboxnet: 1d time-channel sepa-\nrable convolutional neural network architecture for speech commands\nrecognition,” in arXiv preprint:2004.08531, 2020.\n[39] Y. Wei, Z. Gong, S. Yang, K. Ye, and Y. Wen, “Edgecrnn: an edge-\ncomputing oriented model of acoustic feature enhancement for keyword\nspotting,” in Journal of Ambient Intelligence and Humanized Computing,\n2021.\n[40] R. Vygon and N. Mikhaylovskiy, “Learning efﬁcient representations for\nkeyword spotting with triplet loss,” in arXiv preprint:2101.04792, 2021.\n[41] P. Warden, “Speech commands: A dataset for limited-vocabulary speech\nrecognition,” arXiv preprint:1804.03209, 2018.\n[42] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An\nasr corpus based on public domain audio books,” in IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[43] T. N. Sainath and C. Parada, “Convolutional neural networks for small-\nfootprint keyword spotting,” in Conference of the International Speech\nCommunication Association (INTERSPEECH), 2015.\n",
  "categories": [
    "cs.SD",
    "cs.CL",
    "eess.AS"
  ],
  "published": "2022-05-28",
  "updated": "2022-05-28"
}