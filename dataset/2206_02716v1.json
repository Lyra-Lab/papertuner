{
  "id": "http://arxiv.org/abs/2206.02716v1",
  "title": "Stacked unsupervised learning with a network architecture found by supervised meta-learning",
  "authors": [
    "Kyle Luther",
    "H. Sebastian Seung"
  ],
  "abstract": "Stacked unsupervised learning (SUL) seems more biologically plausible than\nbackpropagation, because learning is local to each layer. But SUL has fallen\nfar short of backpropagation in practical applications, undermining the idea\nthat SUL can explain how brains learn. Here we show an SUL algorithm that can\nperform completely unsupervised clustering of MNIST digits with comparable\naccuracy relative to unsupervised algorithms based on backpropagation. Our\nalgorithm is exceeded only by self-supervised methods requiring training data\naugmentation by geometric distortions. The only prior knowledge in our\nunsupervised algorithm is implicit in the network architecture. Multiple\nconvolutional \"energy layers\" contain a sum-of-squares nonlinearity, inspired\nby \"energy models\" of primary visual cortex. Convolutional kernels are learned\nwith a fast minibatch implementation of the K-Subspaces algorithm. High\naccuracy requires preprocessing with an initial whitening layer,\nrepresentations that are less sparse during inference than learning, and\nrescaling for gain control. The hyperparameters of the network architecture are\nfound by supervised meta-learning, which optimizes unsupervised clustering\naccuracy. We regard such dependence of unsupervised learning on prior knowledge\nimplicit in network architecture as biologically plausible, and analogous to\nthe dependence of brain architecture on evolutionary history.",
  "text": "Stacked unsupervised learning with a network\narchitecture found by supervised meta-learning\nKyle Luther\nPrinceton University\nkluther@princeton.edu\nH. Sebastian Seung\nPrinceton University\nAbstract\nStacked unsupervised learning (SUL) seems more biologically plausible than\nbackpropagation, because learning is local to each layer. But SUL has fallen far\nshort of backpropagation in practical applications, undermining the idea that SUL\ncan explain how brains learn. Here we show an SUL algorithm that can perform\ncompletely unsupervised clustering of MNIST digits with comparable accuracy\nrelative to unsupervised algorithms based on backpropagation. Our algorithm is\nexceeded only by self-supervised methods requiring training data augmentation by\ngeometric distortions. The only prior knowledge in our unsupervised algorithm is\nimplicit in the network architecture. Multiple convolutional “energy layers” contain\na sum-of-squares nonlinearity, inspired by “energy models” of primary visual\ncortex. Convolutional kernels are learned with a fast minibatch implementation of\nthe K-Subspaces algorithm. High accuracy requires preprocessing with an initial\nwhitening layer, representations that are less sparse during inference than learning,\nand rescaling for gain control. The hyperparameters of the network architecture\nare found by supervised meta-learning, which optimizes unsupervised clustering\naccuracy. We regard such dependence of unsupervised learning on prior knowledge\nimplicit in network architecture as biologically plausible, and analogous to the\ndependence of brain architecture on evolutionary history.\n1\nIntroduction\nRecently there has been renewed interest in the hypothesis that the brain learns through some version\nof the backpropagation algorithm [31]. This hypothesis runs counter to the neuroscience textbook\naccount that local learning mechanisms, such as Hebbian synaptic plasticity, are the basis for learning\nby real brains. The concept of local learning has fallen out of favor because it has been far eclipsed\nby backpropagation in practical applications. This was not always the case. Historically, a popular\napproach to visual object recognition was to repeatedly stack a single-layer unsupervised learning\nmodule to generate a multilayer network, as exempliﬁed by Fukushima’s pioneering Neocognitron\n[11]. Stacked unsupervised learning (SUL) avoids the need for the backward pass of backpropagation,\nbecause learning is local to each layer.\nIn the 2000s, SUL was quite popular. There were attempts to stack diverse kinds of unsupervised\nlearning modules, such as sparse coding [21], restricted Boltzmann machines [14, 29], denoising\nautoencoders [46], K-Means [7], and independent subspace analysis [25].\nSUL managed to generate impressive-looking feature hierarchies that are reminiscent of the hierarchy\nof visual cortical areas. Stacking restricted Boltzmann machines yielded features that were sensitive\nto oriented edges in the ﬁrst layer, eyes and noses in the second, and entire faces in the third layer\n[29]. Stacking three sparse coding layers yielded an intuitive feature hierarchy where higher layers\nwere more selective to whole MNIST digits and lower layers were selective to small strokes [43].\nAlthough these feature hierarchies are pleasing to the eye, they have not been shown to be effective\nPreprint. Under review.\narXiv:2206.02716v1  [cs.NE]  6 Jun 2022\nfor visual object recognition, in spite of recent efforts to revive SUL using sparse coding [43, 6] and\nsimilarity matching [39].\nHere we show an SUL algorithm that can perform unsupervised clustering of MNIST digits with high\naccuracy (2% error). The clustering accuracy is as good as the best unsupervised learning algorithms\nbased on backpropagation. As far as we know, our accuracy is only exceeded by self-supervised\nmethods that require training data augmentation or architectures with hand-designed geometric\ntransformations. Such methods use explicit prior knowledge in the form of geometric distortions to\naid learning.\nOur network contains three convolutional energy layers inspired by energy models of primary visual\ncortex [1], which contain a sum-of-squares nonlinearity. Our energy layer was previously used by\n[16] in their independent subspace analysis (ISA) algorithm for learning complex cells. The kernels\nof our convolutional energy layers are trained by K-Subspaces clustering [45] rather than ISA. We\nalso provide a novel minibatch algorithm for K-Subspaces learning.\nAfter training, the ﬁrst energy layer contains neurons that are selective for simple features but invariant\nto local distortions. These are analogous to the complex cells in energy models of the primary visual\ncortex [1]. The invariances are learned here rather than hand-designed, similar to previous work\n[16, 15]. We go further by stacking multiple energy layers. The second and third energy layers learn\nmore sophisticated kinds of invariant feature selectivity. As mentioned above, feature hierarchies\nhave previously been demonstrated for SUL. The novelty here is the learning of a feature hierarchy\nthat is shown to be useful for pattern recognition.\nIn the special case that the sum-of-squares contains a single term, or equivalently the subspaces are\nrestricted to be rank one, our convolutional energy layer reduces to a conventional convolutional layer.\nAccuracy worsens considerably, consistent with the idea that the energy layers are important for\nlearning invariances. The energy layers contain an adaptive thresholding that allows representations\nto be less sparse for inference than for learning. This is also shown to be important for attaining high\naccuracy, as has been reported for other SUL algorithms [8, 24]. Representations are rescaled for\ngain control, and the energy layers are preceded by a convolutional whitening layer. These aspects of\nthe network are also important for high accuracy.\nThe detailed architecture of our unsupervised network depends on subspace number and rank, kernel\nsize, and sparsity. We use automatic tuning software [2] to systematically search for a hyperparameter\nconﬁguration that optimizes the clustering accuracy of the unsupervised network. Evaluating the\nclustering accuracy requires labeled examples, so the meta-learning is supervised, while the learning\nis unsupervised. A conceptually similar meta-learning approach has previously been applied to search\nfor biologically plausible unsupervised learning algorithms [37].\nFor each iteration of the meta-learning, the weights of our network are initialized randomly before\nrunning the SUL algorithm. Therefore the only prior knowledge available to SUL resides in the\narchitectural hyperparameters; no weights are retained from previous networks. We regard this\nimplicit encoding of prior knowledge in network architecture as biologically plausible, because brain\narchitecture also contains prior knowledge gained during evolutionary history, and meta-learning is\nanalogous to biological evolution. In its own “lifetime” our network is able to learn with no labels at\nall. This is possible because the network is “born” with an architecture inherited from networks that\n“lived” previously.\n2\nRelated work\nIndependent subspace analysis Our method is related to past works on independent subspace\nanalysis [16, 17, 26], mixtures of principal components analyzers [13], and subspace clustering\n[45, 47]. A core idea behind these works is that invariances can be represented via subspaces. The\nmost similar of these works to ours is [26] who stacked 2 layers of subspace features learned with\nindependent subspace analysis to action recognition datasets.\nK-Means based features Mathematically the work of [7, 9] is similar to ours. They use a variant of\nK-Means to learn patch features. Their learning algorithm (Algorithm 1) is a special case of our alg. 1\nwhere they use 1D subspaces and full batch updates. Their inference procedure is also very similar,\nin that they use a dynamic threshold to sparsify patch vector representations. They use spatial pooling\nlayers for invariance, whereas our pooling is learned by the energy layers. Their primary mode of\n2\nFigure 1: Our multilayer convolutional energy network. The last step of K-Means clustering can be regarded as\npart of the evaluation rather than the network itself.\nevaluation was linear evaluation with the full labeled dataset, whereas we will seek to produce an\nunsupervised learning algorithm which clusters inputs without labels. They additionally employ a\nreceptive ﬁeld selection method.\nCapsule networks Capsules are vector representations where the vector’s length represents the\nprobability of entity existence and the direction represents properties of that entity [41]. In our\nnetworks, the r-dimensional subspace vectors Vx can be interpreted as learned “pose” vectors.\nHowever our goal is to learn invariance, so we only propagate the norm ∥Vx∥, thus suppressing the\npose details at each layer. Compared to the unsupervised capsule networks [23], our networks do not\nuse any backpropagation of gradients, and do not rely on hand-designed afﬁne transformations to\ngenerate representations.\nMeta-learning of unsupervised learning rules Our work will rely on using a label-based clustering\nobjective to evaluate and tune an unsupervised learning rule. In other words there are two levels of\nlearning, an unsupervised inner loop and a supervised outer loop. This is the domain of meta-learning.\nThe more common scenario for meta-learning is to focus on optimizing over a distribution of tasks,\nbut for this work we will focus on one task. The work of [36] is perhaps the most closely related to\nours. They use a few-shot supervised learning rule to tune an unsupervised learning algorithm that is\na form of randomized backward propagation [30]. We wish to go further and remove any form of\ngradient feedback from a higher layer L to a lower layer L −1.\n3\nNetwork architecture\nThe overall network architecture is shown in Figure 1, and consists of a whitening layer followed by\nmultiple convolutional energy layers and a ﬁnal average pooling layer. The energy layers include\nadaptive thresholding to control sparsity of activity, as well as normalization of activity by rescaling.\nConvolutional ZCA We deﬁne ZCA whitening for image patches in terms of the eigenvalues of the\npixel-pixel correlation matrix. Our ZCA ﬁlter attenuates the top k −1 eigenvalues, setting them equal\nto the kth largest eigenvalue. The smaller eigenvalues pass through unchanged. Our deﬁnition is\nslightly different from [10], which zeros out the smaller eigenvalues completely.\nThe ﬁrst layer of our network is a convolutional variant of ZCA, in which each pixel of the output\nimage is computed by applying ZCA whitening to the input patch centered at that location, and\ndiscarding all but the center pixel of the whitened output patch (p. 118 of [10]). The kernel has a single-\npixel center with a diffuse surround (see Appendix). Patch size and number of whitened eigenvalues\nare speciﬁed in Table 1. Reﬂection padding is used to preserve the output size. Unsupervised learning\nalgorithms such as sparse coding [40] and ICA [5] are often preceded by whitening when applied to\nimages.\nConvolutional energy layer We deﬁne the following modiﬁcation of a convolutional layer, which\ncomputes k output feature maps given m input feature maps. We deﬁne the “feature vector” at a\nlocation to consist of the values of a set of feature maps at a given location.\n1. Convolve the m input feature maps with kernels to produce kr feature maps (“S-maps”) in\nthe standard way, except with no bias or threshold.\n2. Divide the kr feature maps into k groups of r. For each group, compute the Euclidean\nnorm of the r-dimensional feature vector at each location. The result is k feature maps\n(“C-maps”).\n3\nFigure 2: Diagram of a single energy layer. The outputs of this layer are the C feature maps, while S are just\nintermediate feature maps. We produce kr feature maps (S) with a standard convolution. We produce (C)\nfeature maps by square root the sum of squares inside each group of S feature maps, followed by an adaptive\nthresholding and rescaling.\nIndividual values in the S- and C-maps of Steps 1 and 2 will be called S-cells and C-cells, respectively.\nEach C-cell computes the square root of the sum-of-squares (Euclidean norm) of r S-cells. The terms\nS-cells and C-cells are used in homage to [11]. They are reminiscent of energy models of primary\nvisual cortex, in which complex cells compute the sum-of-squares of simple cell outputs [1].\nThe sum-of-squares can be regarded as a kind of pooling operation, but applied to S-cells at the same\nlocation in different feature maps. Pooling is typically performed on neighboring locations in the\nsame feature map, yielding invariance to small translations by design. We will see later on that the\nsum-of-squares in the energy layer acquires invariances due to the learned kernels.\nAdaptive thresholding and normalization It turns out to be important to postprocess the output of\na convolutional energy layer as follows.\n3. For each k-dimensional feature vector, adaptively threshold so that there are W winners\nactive.\n4. Normalize the feature vector to unit Euclidean length, and rescale by multiplying with the\nEuclidean norm of the input patch at the same location.\nDeﬁne f as the k-dimensional vector which is the k feature map values at a location u in the C feature\nmaps. In Step 3, the adaptive thresholding of f takes the form max{0, fi −τ} for i = 1 to k where τ\nis the W + 1st largest element of f. Note that τ is set adaptively for each location. Such adaptive\nthresholding was used by [44] and is a version of the well-known W-winners-take-all concept [33].\nAfter thresholding, C-cells are sparsely active, with sparseness controlled by the hyperparameter W.\nS-cells, on the other hand, will typically be densely active, since they are linear.\nStep 4 normalizes the k-dimensional feature vector, and also multiplies by the Euclidean norm of the\ninput patch, to prevent the normalized output from being large even if the input is vanishingly small.\nThe kernels in the layer are size p × p, so that the input patch at any location contains mp2 values\nwhere m is the number of input feature maps.\nFinal average pooling layer To reduce the output dimensionality, we average pool each output\nfeature map of the last energy layer to a 2 × 2 grid, exactly as in [8]. We have generally avoided\npooling because we want to learn invariances rather than design them in. However, a ﬁnal average\npooling will turn out to be advantageous later on for speeding up meta-learning. In Table 2 we show\nthat this pooling layer has only a modest impact on the ﬁnal clustering accuracy.\n4\nStacked unsupervised learning\nThe outputs of the ZCA layer are used as inputs to a convolutional energy layer. We train the kernels\nof this layer, and then we freeze the kernels. The outputs of the ﬁrst convolutional energy layer are\nused as inputs to a second convolutional energy layer, and the kernels in this layer are trained. We\nrepeat this procedure to stack a total of three convolutional energy layers, and then conclude with a\nﬁnal pooling layer.\n4\nLAYER\n# SUBSPACES (k)\nSUBSPACE RANK (r)\n# WINNERS (w)\nKERNEL SIZE\nPADDING\nL1\n37\n2\n9\n8\n2\nL2\n9\n3\n8\n5\n1\nL3\n58\n16\n2\n21\n2\nTable 1: Detailed architecture of our three energy layer net. The ﬁrst energy layer is preceded by convolutional\nZCA whitening of the input image, where the kernel size is 9 and the number of whitened eigenvalues is 9.\nThese parameters are found with automated hyperparameter tuning.\nREPRESENTATION\nPIXELS\nZCA\nLAYER 1\nLAYER 2\nLAYER 3\n2X2 POOL\nCLUSTERING ERROR (%)\n46.2\n50.0\n22.7\n22.2\n2.3\n2.1\nTable 2: Clustering error after every layer of our network with three energy layers.\nK-Subspaces clustering Consider an energy layer with k C-cells and kr S-cells at each location.\nThe S-cells at one location are linearly related to the input patch at that location by the set of k\nmatrices Vj for j = 1 to k, each of size r × mp2. Here mp2 is the size of the ﬂattened input patch,\nwhere m is the number of input feature maps and p is the kernel size.\nWe can think of these matrices as deﬁning a set of k linear subspaces of rank r embedded in Rmp2.\nWe learn these matrices with a convolutional extension of the K-Subspace learning algorithm [45].\nLet xn be the previous layer’s m-dimensional feature maps for pattern n. Deﬁne xn,i as the mp2-\ndimensional feature vector created by ﬂattening a p×p patch centered around location i. K-Subspaces\naims to learn k r-dimensional subspaces Vk ∈Rr×mp2 such that every patch is well modeled by\none of these subspaces. This is formalized with the following optimization:\nmin\nV min\nC\nX\nn,i\nX\nk\ncnik\n\r\rxni −V⊤\nk Vkxni\n\r\r2\n(1)\nsuch that cnik ∈{0, 1} and P\nk cnik = 1. We provide a novel minibatch algorithm for this opti-\nmization in Appendix A. During the learning, each matrix Vj is constrained so that its rows are\northonormal. Therefore at each location the C-cells contain the Euclidean norms of the projection of\nthe input patch onto each of the subspaces, before the adaptive thresholding and rescaling. This is\nwhy the K-Subspaces algorithm is naturally well-suited for learning the convolutional energy layer.\nDuring K-Subspaces learning, an input patch is assigned to a single subspace, which means there\nis winner-take-all competition between C-cells at a given location. During inference, on the other\nhand, there can be many C-cells active at a given location (depending on the \"number of winners\"\nhyperparameter w).\nThis idea of making representations less sparse for inference than for learning has been exploited by\na number of authors [7, 24]. This may be advantageous because overly sparse representations can\nsuffer from sensitivity to distortions [32].\nExperiments with MNIST digits We train a network with the architecture of Fig. 1. The details\nof the ZCA layer and the three convolutional energy layers are speciﬁed by Table 1. Each energy\nlayer is trained using a single pass through the 60,000 MNIST [27] training examples (without using\nthe labels), with a minibatch size of 512. Training on a single Nvidia Titan 1080-Ti GPU takes 110\nseconds.\nEvaluation of learned representations We adopt the intuitive notion that the output representation\nvectors of a “good” network should easily cluster into the underlying object classes as deﬁned by\nimage labels. This is quantiﬁed by applying the trained network to 10,000 MNIST test examples.\nThe resulting output representation vectors are clustered with the scikit-learn implementation of\nK-Means. This uses full batch EM-style updates and additionally returns the lowest mean squared\nerror clustering found by running the algorithm using 10 different initializations.\nWe set the number of clusters to be 10, to match the number of MNIST digit classes. We compute the\ndisagreement between the cluster assignments and image labels, and minimize over permutations of\nthe clusters. The minimal disagreement is the ﬁnal evaluation, which we will call the clustering error\n[49].\n5\nREPRESENTATION\nCLUSTERING ERROR (%)\nPIXELS\n46.2\nNMF‡ [28]\n44.0\nSTACKED DENOISING AUTOENCODERS† [46]\n18.8\nUMAP [35]\n17.9\nGENERATIVE ADVERSARIAL NETWORKS† [12]\n17.2\nVARIATIONAL AUTOENCODER† [22]\n16.8\nDEEP EMBEDDED CLUSTERING† [48]\n15.7\nVADE [19]\n5.5\nCLUSTERGAN‡ [38]\n5.0\nUMAP + GMM [35]\n3.6\nN2D [34]\n2.1\nOURS (THREE LAYER NET)\n2.1\nINVARIANT INFORMATION CLUSTERING (AVG SUB-HEAD) † [18]\n1.6\nSTACKED CAPSULE AUTOENCODERS [23]\n1.3\nINVARIANT INFORMATION CLUSTERING (BEST SUB-HEAD)† [18]\n0.8\nTable 3: Comparison of clustering accuracy for other unsupervised learning algorithms. For methods which do\nnot generate clusters, k-means is used to cluster representations into 10 clusters, with the exception of UMAP +\nGMM in which case we use a Gaussian Mixture Model to cluster. The errors for methods with a dagger † are all\ntaken from [18], methods with double dagger ‡ are taken from [38]. UMAP uses “out-of-the-box” parameter\nsettings.\nFigure 3: Layer 1 subspaces learned with our algorithm. Each 8 × 8 image corresponds to a kernel.\nTable 2 quantiﬁes the accuracy of each layer. The accuracy of the ZCA representation is actually\nworse than that of the raw pixels. However, the accuracy of the representations improves with each\nadditional convolutional energy layer, until the ﬁnal error is just 2.1%.\nComparisons with other algorithms are shown in Table 3. It is helpful to distinguish between algo-\nrithms that require training data augmentation, and those that do not. Many well-known unsupervised\nalgorithms that do not make use of training data augmentation, such as GANs, variational autoen-\ncoders, and stacked denoising autoencoders, yield clustering errors of 15 to 19%. Methods such as\nVaDE and ClusterGAN encourage clustered latent representations and these give rise to much lower\nclustering error. Clustering 2D UMAP representations with K-Means gives suprisingly high error,\nand this is likely the returned clusters are not spherical. Using a Gaussian Mixture Model instead\ngives much lower error. See the appendix for more discussion.\nSelf-supervised algorithms require training data augmentation, using prior knowledge to create same-\nclass image pairs. For MNIST clustering one of the highest performance is Invariant Information\nClustering, with a clustering error of 1.6 - 0.8% [18]. Our approach delivers roughly 2 % error and is\nnoticeably better than the other algorithms that do not require training data augmentation.\nStacked capsule autoencoders [23] also achieve high accuracy. However, this algorithm incorporates\na model of geometric distortions. Furthermore, despite the modiﬁer “stacked,” the algorithm does not\nconform to the original idea of repeatedly stacking the same learning module. The architecture uses\nseveral distinct types of layers and still backpropagates gradients.\nLearned kernels Figure 3 shows that the kernels in the ﬁrst energy layer look like bars or edges,\nmore often curved than straight. Since the subspaces are of rank 2 (Table 1), the kernels come in\npairs. The kernels in a pair look quite similar to each other, and typically appear to be related by\nsmall distortions. Therefore the two S-cells in a pair should prefer slightly different versions of the\nsame feature. By computing the square root of the sum-of-squares of the S-cells, the C-cell should\ndetect the same feature with more invariance to distortions.\n6\nFigure 4: Output \"C\" feature maps for 3 input images.\nThis behavior is reminiscent of energy models of simple and complex cells in primary visual cortex\n[1, 42]. A complex cell computes the sum-of-squares of simple cells, which are quadrature pairs of\nGabor ﬁlters. The sum-of-squares is invariant to phase shifts, much as the sum-of-squares of sine\nand cosine is constant. Our model also contains a sum-of-squares, but the ﬁlters are learned rather\nthan hand-designed. Learning of complex cells was previously demonstrated by [16] for an energy\nmodel and by [20] for a similar model. [15] showed how to substitute rectiﬁcation for quadratic\nnonlinearity. Our contribution is to stack multiple energy layers, and investigate the accuracy of the\nresulting network at a clustering task.\nFeature maps Figure 4 shows the C-maps for the ﬁrst 3 MNIST digits. The ﬁrst energy layer exhibits\nan intermediate degree of sparsity, (approximately 20 % of the maps are active at central locations).\nThe second energy layer exhibits dense representations (nearly all maps are active at central locations).\nThe ﬁnal energy layer shows quite sparse representations (approximately 5% of the maps are active).\nThe feature maps appear to be broad spots.\n5\nMeta-learning of network architecture for unsupervised learning\nThe detailed architecture of the network is speciﬁed in Table 1, and one might ask where it came from.\nFor example, the kernel size is 21 in the third energy layers, but sizes are only single digit integers in\nother layers. The subspaces are rank 2 in the ﬁrst energy layer, but rank 3 and 16 in the subsequent\nenergy layers. The second energy layer is highly dense (all but one neuron is active at each location),\nwhile the third layer is highly sparse (only two neurons are active at each location). There is a total\nof 17 numbers in Table 1, and we can regard them as hyperparameters of the unsupervised learning\nalgorithm.\nIn the initial stages of our research, we set hyperparameters by hand, guided by intuitive criteria such\nas increasing the subspace rank with layer (meaning more invariances learned). Later on, we resorted\nto automated hyperparameter tuning. For this purpose, we employed the Optuna software package,\nwhich implements a Bayesian method [2]. We found that we were able to ﬁnd hyperparameter\nconﬁgurations with considerably better performance than our hand-designed conﬁgurations.\nIn particular, the conﬁguration of Table 1 was obtained by automated search through 2000 hyper-\nparameter conﬁgurations, which took approximately 20 hours using 8 Nvidia GTX 1080 Ti GPUS.\nEach hyperparameter conﬁguration was used to generate an unsupervised clustering of the training\nset, and its accuracy with respect to all 60,000 training labels was the objective function of the search.\nFor the ZCA layer we tune the kernel size kzca ∈[1, 11] and number of whitened eigenvalues\nnzca ∈[0, k2\nzca]. For each subspace layer we tune number of subspaces k ∈[2, 64], subspace\ndimension r ∈[1, 16], number of winners w ∈[1, k], kernel size ks ∈[1, input_size], and padding\nps ∈[0, ﬂoor(ks/2)]. The stride is ﬁxed at 1.\nIn some respects, the optimized conﬁguration of Table 1 ended up conforming to our qualitative\nexpectations. Sparsity and kernel size increased with layer, consistent with the idea of a feature\n7\nNUMBER OF LABELS IN TRAINING SET\n10\n30\n50\n100\n500\n5000\n60000\n% MISCLASSIFIED (TEST)\n30.0\n9.2\n10.7\n5.3\n4.6\n2.9\n2.1\n% MISCLASSIFIED (TRAIN)\n0.0\n0.0\n0.0\n3.0\n2.8\n2.9\n2.5\nTable 4: Label efﬁciency of our stacked learning algorithm. In each case, the algorithm has access to all 60K\nunlabeled images from the training set. What varies is the number of labels we use to evaluate each setting of\nlearning parameters.\nEXPERIMENT\n% ERROR (TEST)\n% ERROR (TRAIN)\n4 ENERGY LAYERS\n3.1\n3.6\n3 ENERGY LAYERS\n2.1\n2.5\n2 ENERGY LAYERS\n2.9\n3.3\n1 ENERGY LAYERS\n20.2\n20.7\nNO ZCA - 3 ENERGY LAYERS\n4.0\n4.8\nNO RESCALING - 3 ENERGY LAYERS\n4.1\n4.6\nNO ZCA & NO RESCALING - 3 ENERGY LAYERS\n9.7\n10.1\n1D SUBSPACES - 3 ENERGY LAYERS\n16.0\n17.3\nRANDOM SUBSPACES - 3 ENERGY LAYERS\n29.4\n31.1\nTable 5: Systematic studies with our multilayer energy model. The learning hyperparameters are tuned using all\n60K labels from the training set.\nhierarchy with progressively greater selectivity and invariance. However, the number of subspaces\nbehaved nonmonotonically with layer, which was unexpected.\nWe can think of the hyperparameter tuning as an outer loop surrounding the unsupervised learning\nalgorithm. We will refer to this outer loop as meta-learning. Given an architecture, the unsupervised\nlearning algorithm requires no labels at all. However, the outer loop searches for the optimal network\narchitecture by using training labels. Therefore, while the learning is unsupervised, the meta-learning\nis supervised. Alternatively, the outer loop can use only a fraction of the training labels, in which\ncase the meta-learning is semi-supervised.\nIt is interesting to vary the number of training labels used for hyperparameter search. The results\nare shown in Table 4. The best accuracy is obtained when all 60,000 training set labels are used.\nAccuracy degrades slightly for 5000 labels, and more severely for fewer labels than that. The test\nerror can be lower than the training error. This is not a mistake, and it appears to result from a\nnon-random ordering of patterns in the MNIST train and test sets.\nTo be clear about the use of data, we note that neither test images nor labels are used during learning\nor meta-learning. Training images but not labels are used during learning. Training labels are used\nby meta-learning. With each iteration of meta-learning, the weights of the network are randomly\ninitialized.\n6\nExperiments\nThe hyperparameter search explores the space of networks deﬁned by Fig. 1. We can widen the\nspace of exploration by performing ablation studies, with results given in Table 5. In all experiments,\nwe completely retune the hyperparameters using the full 60K training labels to evaluate clustering\naccuracy. With the exception of the experiment where we vary the number of layers, we use the 3\nenergy layer network in this section.\nVary number of energy layers One can vary the depth of the network by adding or removing energy\nlayers. 2 and 4 energy layers yield similar accuracy, and are roughly 1% absolute error (50% relative\nerror) worse than for 3 energy layers. A 1 energy layer net is dramatically worse (>20% train/test\nerror), suggesting the stacking is critical for performance.\nThe hyperparameters for each of these optimal architectures are provided in the Appendix. The\noptimal 2 energy layer net resembles the 3 energy layer net with its 2nd energy layer removed, while\nsimultaneously making the 1st energy layer less sparse.\n8\nRemove ZCA whitening Removing the ZCA layer increases both train and test error of the three\nenergy layer net by roughly 2×. This might seem surprising, as Table 2 shows that whitening by\nitself decreases accuracy if we directly cluster whitened pixels instead of raw pixels. Apparently, it\nis helpful to “take a step backward, before taking 3 steps forward” in the case of our networks with\nthree energy layers. We have no theoretical explanation for this interesting empirical fact.\nPreprocessing images by whitening has a long history. The retina has been said to perform a whitening\noperation, which is held to be an “efﬁcient coding” that reduces redundancy in an information theoretic\nsense [4, 3]. Our experiment suggests that whitening is useful because it improves the accuracy\nof subsequent representations produced by stacked unsupervised learning. This seems at least\nsuperﬁcially different from efﬁcient coding theory, because the invariant feature detectors in our\nnetworks appear to discard some information (Figure 4).\nRemove rescaling We now remove the rescaling operation from our energy layers. We observe\na roughly 2× increase in both test and train error. It may be surprising that such a seemingly\ninnocuous change can double the resulting train/test errors. This is likely because we only have 17\nhyperparameters to optimize; with a limited set of parameters to tune, small changes in architecture\ncan lead to dramatic performance changes as we only have limited ﬂexibility to tune hyperparameters.\nRemove rescaling and ZCA whitening Removing both ZCA and the per-layer rescaling operations\ncauses a 4× increase in train and test error. Apparently the damage to the resulting performance is\nmultiplicative: removing ZCA alone doubles the train/test error, removing rescaling alone doubles\ntrain/test error, and removing both quadruples the train/test error.\n1D subspaces We rerun the automated hyperparameter tuning experiments from the previous section,\nthis time restricting our subspaces to be 1D. The subspace norm can be thought of as a conventional\ndot product followed by absolute value nonlinearity ∥Vx∥= |v · x| where v is the one row of the\nsubspace matrix V. When the subspaces are 1D, our algorithm in fact reduces to that of [9].\nRandom subspaces Finally we ask the question: does learning actually help or is it simply the\narchitecture that matters? To do so, we run tuning experiments with random orthogonal subspaces.\nWe observe that performance is almost completely erased by using random subspaces, telling us that\nthe unsupervised learning component is indeed critical for clustering performance.\n7\nDiscussion\nThe elements of our SUL algorithm were already known in the 2000s: ZCA whitening, energy layers,\nK-Subspaces, and adaptive thresholding and rescaling of activities during inference. To achieve\nstate-of-the-art unsupervised clustering accuracy on MNIST, we employed one more trick, automated\ntuning of hyperparameters. Such meta-learning is more feasible now than it was in the 2000s, because\ncomputational power has increased since then.\nGiven that meta-learning optimizes a supervised criterion, is our SUL algorithm really unsupervised?\nIt is true that the complete system is supervised. However, even if the outer loop (meta-learning) is\nsupervised, it is accurate to say that the inner loop (learning) is unsupervised. For any hyperparameter\nconﬁguration, the network starts from randomly initialized weights, and proceeds to learn in a purely\nlabel-free unsupervised manner.\nAlthough MNIST is an easy dataset by today’s standards, we think that the success of our SUL\nalgorithm at unsupervised clustering is still surprising. We are only tuning 17 hyperparameters, and it\nis not clear a priori that our approach would be ﬂexible enough to succeed even for MNIST.\nIn future work, it will be important to investigate more complex datasets or tasks. Following the\nmore common scenario for meta-learning, future work should train an unsupervised algorithm on a\ndistribution of tasks, and test transfer to some held-out distribution. We have done some preliminary\nexperiments with the CIFAR-10 dataset. The meta-learning is more time-consuming because larger\nnetwork architectures must be explored. The research is still in progress, but we speculate that the\nwinner-take-all behavior of K-Subspaces learning may turn out to be a limitation. If so, it will be\nimportant to relax the winner-take-all condition in Equation (1).\nEvolution created brains through eons of trial-and-error. For us to discover how the brain learns, it\nwill be important to exploit our computational resources, and use meta-learning to empirically search\nthe space of biologically plausible learning algorithms.\n9\nAcknowledgements\nWe would like to thank Lawrence Saul and Runzhe Yang for their helpful insights and discussions.\nThis research was supported by the Intelligence Advanced Research Projects Activity (IARPA) via\nDepartment of Interior/ Interior Business Center (DoI/IBC) contract number D16PC0005, NIH/NIMH\nRF1MH117815, RF1MH123400. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer:\nThe views and conclusions contained herein are those of the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of IARPA,\nDoI/IBC, or the U.S. Government.\nReferences\n[1] EH Adelson and JR Bergen. Spatiotemporal energy models for the perception of motion.\nJournal of the Optical Society of America. A, Optics and Image Science, 2(2):284–299, 1985.\n[2] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:\nA next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM\nSIGKDD international conference on knowledge discovery & data mining, pages 2623–2631,\n2019.\n[3] Joseph J Atick and A Norman Redlich. What does the retina know about natural scenes? Neural\ncomputation, 4(2):196–210, 1992.\n[4] Horace B Barlow. Unsupervised learning. Neural computation, 1(3):295–311, 1989.\n[5] Anthony J Bell and Terrence J Sejnowski. The “independent components” of natural scenes are\nedge ﬁlters. Vision research, 37(23):3327–3338, 1997.\n[6] Yubei Chen, Dylan Paiton, and Bruno Olshausen. The sparse manifold transform. Advances in\nneural information processing systems, 31, 2018.\n[7] Adam Coates and Andrew Ng. Selecting receptive ﬁelds in deep networks. Advances in neural\ninformation processing systems, 24, 2011.\n[8] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-\nvised feature learning. In Proceedings of the fourteenth international conference on artiﬁcial\nintelligence and statistics, pages 215–223. JMLR Workshop and Conference Proceedings, 2011.\n[9] Adam Coates and Andrew Y Ng. Learning feature representations with k-means. In Neural\nnetworks: Tricks of the trade, pages 561–580. Springer, 2012.\n[10] D. Eigen. Predicting images using convolutional networks: Visual scene understanding with\npixel maps, 2015.\n[11] K. Fukushima. Neocognitron: a self organizing neural network model for a mechanism of\npattern recognition unaffected by shift in position. Biol Cybern, 36(4):193–202, 1980.\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural\ninformation processing systems, 27, 2014.\n[13] Geoffrey E Hinton, Michael Revow, and Peter Dayan. Recognizing handwritten digits using\nmixtures of linear models. Advances in neural information processing systems, pages 1015–\n1022, 1995.\n[14] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with\nneural networks. science, 313(5786):504–507, 2006.\n[15] Haruo Hosoya and Aapo Hyvärinen. Learning visual spatial pooling by strong pca dimension\nreduction. Neural computation, 28(7):1249–1264, 2016.\n[16] Aapo Hyvärinen and Patrik Hoyer. Emergence of phase-and shift-invariant features by decom-\nposition of natural images into independent feature subspaces. Neural computation, 12(7):1705–\n1720, 2000.\n[17] Aapo Hyvärinen and Urs Köster. Fastisa: A fast ﬁxed-point algorithm for independent subspace\nanalysis. In EsANN, pages 371–376. Citeseer, 2006.\n10\n[18] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper-\nvised image classiﬁcation and segmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 9865–9874, 2019.\n[19] Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational\ndeep embedding: An unsupervised and generative approach to clustering. arXiv preprint\narXiv:1611.05148, 2016.\n[20] Yan Karklin and Michael S Lewicki. Emergence of complex cell properties by learning to\ngeneralize in natural scenes. Nature, 457(7225):83–86, 2009.\n[21] Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Michaël Mathieu, Yann\nCun, et al. Learning convolutional feature hierarchies for visual recognition. Advances in neural\ninformation processing systems, 23, 2010.\n[22] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n[23] Adam Kosiorek, Sara Sabour, Yee Whye Teh, and Geoffrey E Hinton. Stacked capsule autoen-\ncoders. Advances in neural information processing systems, 32, 2019.\n[24] Dmitry Krotov and John J Hopﬁeld. Unsupervised learning by competing hidden units. Pro-\nceedings of the National Academy of Sciences, 116(16):7723–7731, 2019.\n[25] Quoc V Le, Will Y Zou, Serena Y Yeung, and Andrew Y Ng. Learning hierarchical invariant\nspatio-temporal features for action recognition with independent subspace analysis. In CVPR\n2011, pages 3361–3368. IEEE, 2011.\n[26] Quoc V Le, Will Y Zou, Serena Y Yeung, and Andrew Y Ng. Learning hierarchical invariant\nspatio-temporal features for action recognition with independent subspace analysis. In CVPR\n2011, pages 3361–3368. IEEE, 2011.\n[27] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/,\n1998.\n[28] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix\nfactorization. Nature, 401(6755):788–791, 1999.\n[29] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations. In Proceedings of\nthe 26th annual international conference on machine learning, pages 609–616, 2009.\n[30] Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synap-\ntic feedback weights support error backpropagation for deep learning. Nature communications,\n7(1):1–10, 2016.\n[31] Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton.\nBackpropagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, 2020.\n[32] Kyle Luther and H Sebastian Seung. Sensitivity of sparse codes to image distortions. arXiv\npreprint arXiv:2204.07466, 2022.\n[33] Alireza Makhzani and Brendan Frey. K-sparse autoencoders. arXiv preprint arXiv:1312.5663,\n2013.\n[34] Ryan McConville, Raul Santos-Rodriguez, Robert J Piechocki, and Ian Craddock. N2d:(not\ntoo) deep clustering via clustering the local manifold of an autoencoded embedding. In 2020\n25th International Conference on Pattern Recognition (ICPR), pages 5145–5152. IEEE, 2021.\n[35] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation\nand projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n[36] Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning\nupdate rules for unsupervised representation learning. arXiv preprint arXiv:1804.00222, 2018.\n[37] Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning\nupdate rules for unsupervised representation learning. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,\n2019.\n[38] Sudipto Mukherjee, Himanshu Asnani, Eugene Lin, and Sreeram Kannan. Clustergan: Latent\nspace clustering in generative adversarial networks. In Proceedings of the AAAI conference on\nartiﬁcial intelligence, volume 33, pages 4610–4617, 2019.\n11\n[39] Dina Obeid, Hugo Ramambason, and Cengiz Pehlevan. Structured and deep similarity matching\nvia structured and deep hebbian networks. Advances in neural information processing systems,\n32, 2019.\n[40] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive ﬁeld properties by\nlearning a sparse code for natural images. Nature, 381(6583):607–609, 1996.\n[41] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. arXiv\npreprint arXiv:1710.09829, 2017.\n[42] Ko Sakai and Shigeru Tanaka. Spatial pooling in the second-order spatial structure of cortical\ncomplex cells. Vision Research, 40(7):855–871, 2000.\n[43] Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multilayer convolutional\nsparse modeling: Pursuit and dictionary learning. IEEE Transactions on Signal Processing,\n66(15):4090–4104, 2018.\n[44] Louis Thiry, Michael Arbel, Eugene Belilovsky, and Edouard Oyallon. The unreasonable effec-\ntiveness of patches in deep convolutional kernels methods. arXiv preprint arXiv:2101.07528,\n2021.\n[45] René Vidal. Subspace clustering. IEEE Signal Processing Magazine, 28(2):52–68, 2011.\n[46] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol,\nand Léon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep\nnetwork with a local denoising criterion. Journal of machine learning research, 11(12), 2010.\n[47] Dingding Wang, Chris Ding, and Tao Li. K-subspace clustering. In Joint European Conference\non Machine Learning and Knowledge Discovery in Databases, pages 506–521. Springer, 2009.\n[48] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering\nanalysis. In International conference on machine learning, pages 478–487. PMLR, 2016.\n[49] Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. Image clustering using\nlocal discriminant models and global integration. IEEE Transactions on Image Processing,\n19(10):2761–2773, 2010.\nA\nOnline minimization of Eq. 1\nAlgorithm 1: K-Subspace clustering via minibatch power iterations\nInitialize subspaces {Vk ∈Rr×mp2}\nfor i = 1, 2, 3 do\nSample patches from a minibatch of images {xu ∈Rmp2}\nCluster patches\nXk := {xu : k = argmin\nq\n\r\rxu −V⊤\nq Vqxu\n\r\r}\nfor\nk = 1, 2, 3, . . .\nApply one orthogonal power iteration to every subspace\nUSV⊤:= X⊤\nk XkV⊤\nk\nSVD decomposition\nVk := U⊤\nfor\nk = 1, 2, 3, . . .\nend\nA standard algorithm for minimizing Eq. 1 is a full batch EM algorithm that alternates between\ncluster assignment (E-step) and using PCA to set Vk to the top r principle components of the patches\nassigned to each cluster k (M-step) [45]. The full batch requirement makes this algorithm rather slow.\nThe algorithm we present and use in this paper is described in Algorithm 1. We make two core\nchanges to the standard EM algorithm. One, we use minibatch updates instead of full batch updates.\nTwo, we perform a single step of power-iteration after each cluster assignment step, instead of\nperforming a full PCA.\n12\nProper initialization can impact on the quality of learned subspaces. We initialize subspaces by\nsetting the ﬁrst dimension to a randomly chosen patch, and the other dimensions to white Gaussian\nnoise with µ = 0, σ = 0.01. We then perform a few “warmup” iterations of the main loop in alg. 1,\nexcept we only cluster using the ﬁrst subspace dimension. We use warmup_iter = 10 in all our\nexperiments. During these warmup iterations, the power updates are still performed on the full rank-r\nsubspaces. Intuitively, this warmup procedure generates clusters with 1D subspace clustering, and\ninitializes subspaces be the top r components within these clusters.\nConvergence of our algorithm is discussed in the Appendix. We prove that the clustering+power\niteration step ensures the loss computed over the minibatch is non-decreasing. We show empirically\nthat the loss computed over the whole dataset decreases with iteration for a reasonable setting of\nparameters. Because we only use a single pass through the data to train each layer, our algorithm is\nmuch faster than applying the full batch EM-style algorithm described in [45] to the whole dataset.\nB\nConvergence analysis of Algorithm 1\nB.1\nTheory: full-batch convergence\nWe will not be able to provide a full proof that Algorithm 1 converges in the minibatch setting.\nHowever we can at least show that in the full batch setting, every iteration of Algorithm 1 decreases\nthe energy in Equation (1). Of course the cluster assignment portion of the algorithm decreases the\nenergy. What we will show here is that the power iteration step also decreases the energy at every\niteration.\nWe recall the notation from Algorithm 1. We deﬁne the matrix Xk whose rows are the input patches\nassigned to cluster k:\nXk := {xu : k = argmin\nq\n\r\rxu −V⊤\nq Vqxu\n\r\r}\n(2)\nThe energy in Equation (1) can be written as a sum of energies for each cluster e = P\nk ek where:\nek = ∥Xk −XkV⊤\nk Vk∥2\nF\n(3)\nWe will show that a power update for cluster k now gives a non-decreasing energy ek. To avoid\nnotational clutter, we will drop the subspace index k and assume we are working with a single ﬁxed\ncluster for now. It will actually be easier to show that a more general class of updates than the SVD\nupdate in Algorithm 1 cause the energy to remain or decrease. Suppose we have any subspace update\ndeﬁned by:\nV′ := Q(VCV)†CV\n(4)\nwhere Q is any orthogonal r × r matrix (that can also be a function of V, C. The power update\nin Algorithm 1 is an example of such an update. We will show that the energy for every subspace\nis non-decreasing with this update: e′ ≤e. We do so by relating the update in Equation (4) to one\nsequence of steps in an alternating least squares problem. Deﬁne:\nh(A, B) := ∥X −AB⊤∥2\n(5)\nDeﬁne A = XV⊤and B = V⊤. Then e = h(A, B). Deﬁne the sequence:\nB′ = argmin\nU\nh(A, U) = XA(A⊤A)†\nA′ = argmin\nU\nh(U, B′) = XB′((B′)⊤B′)†\n(6)\nWe can multiply A′(B′)⊤and substitute the above equations to get:\nA′(B′)⊤= X(V′)⊤V′\n(7)\nWe therefore have that e′ = h(A′, B′) ≤h(A, B) = e, so the subspace energy is non-increasing\nunder the updates in Algorithm 1.\n13\n0\n50\n100\n150\n200\niteration\n40\n42\n44\n46\n48\n50\n52\n54\nenergy\nlearning curves for a single layer \n applied to 60K MNIST digits\ntrain set energy\ntest set energy\nFigure 5: Learning algorithms for Algorithm 1 applied to MNIST digits.\nB.2\nExperiment: learning curves\nWe show learning curves using Algorithm 1 applied to MNIST digits in Figure 5. We run this\nalgorithm for a single pass through the 60k training set patterns. Inputs are ﬁrst whitened with\nConvZCA using a kernel size of 11 and num component of 16. We learn 64 subspaces, each of\ndimension 5, and kernel size of 9. We use a minibatch size of 256.\nThe training set energy is computed over a minibatch of 256 inputs at each iteration. The test set\nenergy is computed over all 10K test set patterns, which is why it is less noisy. Empirically we\nsee that for this setting of parameters at least, our minibatch K-Subspace algorithm does lead to a\ndecreasing energy computed over unseen patterns.\nWe apply 10 warmup iterations (only using the ﬁrst subspace dimension to cluster for the ﬁrst 10\niterations), which is why we observe the sharp drop-off in energy after 10 iterations.\nC\nHyperparameters for 1,2,3,4 energy layer nets\nIn Table 6 we show the learned architectures for the 1,2,3,4 energy layer networks.\nD\nLearned ZCA ﬁlter\nIn Figure 6 we show the learned ConvZCA kernel for the 3 energy layer network. It resembles an\noblique center surround ﬁlter. It is interesting to calculate the relative weight of the negative surround\nvs positive center term. Speciﬁcally we calculate:\nf =\nI[4, 4] −P\nu,v max{0, −Iu,v}\nI[4, 4]\n= 0.10\n(8)\nwhere I is the 9x9 kernel and I[4, 4] is the central pixel of that kernel. In other words, there is a small\nDC component (the central pixel is not perfectly cancelled out by the negative surround).\n14\nTable 6: Network architectures for 1,2,3,4 energy layer nets from Table 5.\nHYPERPARAMETER\n1 LAYER\n2 LAYER\n3 LAYER\n4 LAYER\nCONVZCA KERNEL SIZE\n5\n9\n9\n11\nCONVZCA N COMPONENTS\n0\n18\n9\n18\nLAYER1 SUBSPACE NUMBER (k)\n59\n20\n37\n15\nLAYER1 SUBSPACE RANK (r)\n2\n5\n2\n2\nLAYER1 ACTIVE FEATURES (w)\n1\n16\n9\n10\nLAYER1 KERNEL SIZE\n10\n10\n8\n7\nLAYER1 PADDING\n4\n2\n2\n3\nLAYER2 SUBSPACE NUMBER (k)\n-\n55\n9\n63\nLAYER2 SUBSPACE RANK (r)\n-\n16\n3\n12\nLAYER2 ACTIVE FEATURES (w)\n-\n1\n8\n1\nLAYER2 KERNEL SIZE\n-\n19\n5\n17\nLAYER2 PADDING\n-\n1\n1\n1\nLAYER3 SUBSPACE NUMBER (k)\n-\n-\n58\n57\nLAYER3 SUBSPACE RANK (r)\n-\n-\n16\n7\nLAYER3 ACTIVE FEATURES (w)\n-\n-\n2\n6\nLAYER3 KERNEL SIZE\n-\n-\n21\n8\nLAYER3 PADDING\n-\n-\n2\n2\nLAYER4 SUBSPACE NUMBER (k)\n-\n-\n-\n22\nLAYER4 SUBSPACE RANK (r)\n-\n-\n-\n1\nLAYER4 ACTIVE FEATURES (w)\n-\n-\n-\n1\nLAYER4 KERNEL SIZE\n-\n-\n-\n3\nLAYER4 PADDING\n-\n-\n-\n1\nFigure 6: Learned ZCA kernel for 3 energy layer net. We have zero-ed out the central pixel, as its value is 1.17\nand thus would make it harder to see the surround structure. This ﬁlter resembles an oblique center surround\nﬁlter.\nE\nClustering UMAP embeddings\nIn the main text we reported a surprisingly low performance for K-Means applied to UMAP em-\nbedding, and that using a Gaussian mixture model instead can signiﬁcantly improve the accuracy.\nHere we show the UMAP embeddings, and corresponding clusterings generated via K-Means and\nGaussian Mixture Models.\nNote that our result is not inconsistent with the result given in Table 1 of [34] who reported 82.5%\nclustering accuracy (compared to our result of 96.4% in Table 5) when using a GMM to cluster the\nembedding vectors return by UMAP. This is because we are using UMAP to embed to 2D while they\nused UMAP to embed to 10 dimennsions. For this task it seems that the extremely low dimensional\nembeddings are actually more suitable for downstream clustering.\n15\nFigure 7: K-Means vs. Gaussian Mixture-based clustering applied to UMAP with \"out-of-the-box\" parameters\napplied to MNIST handwritten digits. Colors are assigned to each point based of the clustering (not the ground\ntruth labels). K-Means erroneously merges and splits some of the clusters, while Gaussian Mixture models give\na much more intuitive clustering (and ultimately a much lower clustering error as shown in Table 5)\n16\n",
  "categories": [
    "cs.NE",
    "q-bio.NC"
  ],
  "published": "2022-06-06",
  "updated": "2022-06-06"
}