{
  "id": "http://arxiv.org/abs/2303.01980v1",
  "title": "Towards energy-efficient Deep Learning: An overview of energy-efficient approaches along the Deep Learning Lifecycle",
  "authors": [
    "Vanessa Mehlin",
    "Sigurd Schacht",
    "Carsten Lanquillon"
  ],
  "abstract": "Deep Learning has enabled many advances in machine learning applications in\nthe last few years. However, since current Deep Learning algorithms require\nmuch energy for computations, there are growing concerns about the associated\nenvironmental costs. Energy-efficient Deep Learning has received much attention\nfrom researchers and has already made much progress in the last couple of\nyears. This paper aims to gather information about these advances from the\nliterature and show how and at which points along the lifecycle of Deep\nLearning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation)\nit is possible to reduce energy consumption.",
  "text": " \n \n \n \nTowards energy-efficient Deep Learning: An overview of energy-\nefficient approaches along the Deep Learning Lifecycle  \n \n \nVanessa Mehlin \nUniversity of Applied Science Ansbach \nvanessamehlin@gmail.com \nSigurd Schacht \nUniversity of Applied Science Ansbach \nsigurd.schacht@hs-ansbach.de \nCarsten Lanquillon \nUniversity of Applied Science Heilbronn \ncarsten.lanquillon@hs-heilbronn.de \n \nFebruary 1, 2023 \n \nABSTRACT \nDeep Learning has enabled many advances in machine learning applications in the last few years. \nHowever, since current Deep Learning algorithms require much energy for computations, there are \ngrowing concerns about the associated environmental costs. Energy-efficient Deep Learning has \nreceived much attention from researchers and has already made much progress in the last couple of \nyears. This paper aims to gather information about these advances from the literature and show how and \nat which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, \nDeployment, Evaluation) it is possible to reduce energy consumption.  \n1. Introduction \nThe last decade has seen tremendous developments in the fields of Artificial Intelligence (AI) and \nMachine Learning (ML), mainly due to the availability of massive datasets and the advancement of \nDeep Learning (DL). DL consists of neural networks with multiple hidden layers known as deep neural \nnetworks (DNNs) [1]. DNNs are used in various products and services, such as speech assistance, \nautonomous driving, or facial recognition. They provide superior performance in these tasks compared \nto traditional models, and there is a trend for developing even larger and more powerful DNNs [2]. \nHowever, these models generally require large amounts of data and high computing power, which is \nassociated with high energy consumption incurring high financial and environmental costs [3]. \nFurthermore, for emerging applications such as autonomous driving and the Internet of Things, models \nmust be run on low-power devices [4]. Therefore, energy-efficient DL is crucial from an economic, \nenvironmental, and application perspective.  \nThe efficiency of DL can be divided into training efficiency and inference efficiency [5]. Model training \ncomes at a high environmental cost, as energy is required to run it on hardware for weeks or months at \na time [3]. However, training a DL model is only the beginning of the lifecycle. Once the model is \ntrained, it will be implemented and used. This process, called inference, also consumes enormous \nenergy. Inference does not last weeks or months, but unlike training, it is not a one-time event. It takes \n \n2 \n \nplace continuously and can therefore exceed the energy consumption of the training after a certain \nnumber of inference events [6].  \nVarious researchers have presented approaches, methods, and techniques such as mixed-precision \ntraining, pruning or knowledge distillation that can accelerate training and inference time and reduce the \nenergy consumption of the models. This paper aims to provide an overview of these existing approaches \nand categorize them into the phases of the DL lifecycle. The lifecycle considered in this paper \nencompasses IT-Infrastructure, Data, Modeling, Training, Deployment (Inference) and Evaluation. The \nsubdivision of the lifecycle is based on the Data Science Process Model (DASC-PM) [7] as well as on \nthe CRISP-DM [8], which is the Cross Industry Standard Data Mining Process Model. To the authors' \nknowledge, there has been no previous work aimed at providing a global overview of existing energy-\nreducing approaches along the lifecycle of DL. There have been overviews of energy-efficient DL ([5], \n[9]–[12]); however, these overviews have neglected the evaluation part of the DL lifecycle. Thus, this \npaper is intended not only as a holistic overview of energy-efficient approaches along the lifecycle of \nDL, it also provides new insights beyond the information provided by these previous overviews.  \nThe paper is structured as follows. Section II presents the overviews that served as the foundation for \nthe present work. This is followed by Section III, in which the methodology and research questions are \noutlined. Section IV provides an overview of the various approaches to reducing energy consumption \nin the different phases of the DL lifecycle. Finally, a conclusion and a discussion on future work are \ngiven. \n2. Related Work \nA great deal about energy-efficient DL has been written, including approaches to reduce energy \nconsumption [13]–[19], studies about how much energy DL is consuming ([3], [20], [21]), publications \nthat point out the need for efficiency improvements due to carbon emissions or the use of edge devices \n([22], [23]) as well as the following overviews of energy-efficient DL:  \nMenghani [5] gave an overview of efficient modeling techniques, infrastructure and hardware. A \ncollection of algorithms, techniques, and tools related to efficient DL in the five areas of (1) compression \ntechniques, (2) learning techniques, (3) automation, (4) efficient architectures, and (5) infrastructure are \npresented. Furthermore, an experimental guide with code to show how these tools and techniques work \nwith each other is provided. This guide helps practitioners to optimize model training and deployment \nXu et al. [9] conducted a systematic review of energy-efficient DL technologies within the four \ncategories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference \napproaches, and (4) efficient data usage. For each category, they provided a taxonomy. Furthermore, \nthey discussed the progress made and the unresolved challenges in each category.  \nCai et al. [10] and Liu et al. [11] focused on memory and compute-limited devices (edge/mobile \ndevices). They surveyed DL approaches including lightweight network design, network compression, \nhardware-aware neural architecture search, adaptive models, efficient on-device learning, and efficient \nsystem design. \nLee et al. [12] provided an overview of resource-efficient DL techniques in terms of (1) modeling, (2) \narithmetic, and (3) implementation techniques. The focus of this overview is on resource-efficient \ntechniques for Convolutional Neural Network architectures, since it is one of the most commonly used \nDL architectures. \n \n \n \n3 \n \n3. Methodology  \nTo provide a holistic overview of energy-efficient approaches along the lifecycle of DL an initial \nliterature review on existing overviews and literature reviews were conducted. As a result, four recent \noverviews of energy-efficient DL ([5], [9]–[12]) were found (see Related Work). Two of those ([10], \n[11]) focus on mobile/edge devices and one of those [12] focuses on resource efficiency. DL on edge \nand mobile devices are closely related topics, since those devices have limited memory and compute \nresources, depending on energy efficiency [10]. Resource-efficient DL is also strongly related, as some \nresource-efficient techniques, such as mixed-precision training are associated with energy efficiency. \nHowever, in the further course of this work, these edge/mobile- and resource-focused overviews are not \nconsidered to the same extent as the other two surveys, due to (1) a focus on the energy efficiency of \nDL in general and (2) due to the limited scope of the study. Since the remaining two overviews ([5], [9]) \ntogether already comprehensively cover the topic, it was decided to synthesize the collected information \nfrom these two overviews as a basis rather than conduct another individual literature review on energy-\nefficient DL. \nThe next step was to verify whether the techniques and approaches gathered in the overviews cover the \nentire DL lifecycle (table 1). The key takeaway was that the reviewed overviews neglect the evaluation \npart of the DL lifecycle. The topic is addressed, but no such overview is provided as in the other \ncategories. Since the evaluation part is not equally represented, it was decided to guide this paper with \nthe two research questions and methods presented in Table 2.  \nTab. 1: Mapping of the content of the overviews along the DL lifecycle \n* = it is addressed, but no such overview is provided as in the other categories \nOverview \nIT-Infrastr. \nData \nModeling \nTraining \nDeployment  Evaluation \nMenghani [5] \nü \nü \nü \nü \nü \n* \nXu et al. [9] \n \nü \nü \nü \nü \n* \n \nTab. 2: Research questions and methods \nNo. \nResearch question \nMethod \nRQ1 \nWhich techniques and approaches can be applied in the \nphases of the DL lifecycle to reduce the energy consumption \nof DL models? \nSynthesizing information from related \noverviews and mapping them along the \nDL lifecycle. \nRQ2 \nHow can models be evaluated in terms of their energy \nconsumption and carbon emissions? \n(1) Synthesizing information from the \nrelated overviews \n(2) Literature review \n \nTo answer RQ1 the techniques and approaches of the two reviewed overviews were analyzed and then \nclassified into a suitable phase of the DL lifecycle. As mentioned by Xu et al. [9] it can be challenging \nto provide an overview of energy-efficient DL due to the lack of a unified standard measurement for \nenergy-efficient DL. Therefore, it is sometimes difficult to tell whether an approach is energy-efficient \nor not. One controversial example is Neural Architecture Search Algorithms (explained in section 4). \nRunning such algorithms usually needs large computational resources [5]. But the resulting efficient \nmodels can significantly reduce the computational burden in downstream research and production, \nleading to reduced energy consumption [24]. Therefore, it is debatable whether a technology is defined \nas energy-efficient or not. However, if an approach has the potential to reduce the energy costs of DL \nmodels, it is included in the overview. Furthermore, a clear classification of some approaches in a certain \nphase of the DL lifecycle is not given. In some cases, an approach could be assigned to two or three \n \n4 \n \nphases, depending on the interpretation. For example: Pre-trained models could be mapped in the \n\"Data\", \"Modeling\" or \"Training\" phase. Thus, the classification of some approaches depends on the \nauthor's interpretation, which is based on the categorization of the two studies reviewed. Not all of the \ntechniques listed in the overviews will be presented, because some of them are just mentioned within \nthe overview and not described as for example “K-Means Clustering” in Menghani [5]. Additionally, \nsome of them are not relevant in the same extent as the others, for example Hyper-Parameter \nOptimization. It is mentioned in Menghani [5], but in the remainder of this paper, the focus is on Neural \nArchitecture Search, as this is the most recent advance in the field. To answer RQ2 a literature review \nwas conducted. The search was performed during May 2022 and the meta search engines Web of \nScience, Scopus as well as Google Scholar, which cover all major publishers and journals such as \nSpringerlink, Elsevier, IEEE Xplore and Research Gate, were used. Searches utilizing the following \nsearch terms were performed across all databases:  \n(“deep learning” OR “DNN” OR “deep neural network” OR “machine learning”) AND (“measure*” \nOR “metric” OR “evaluation” OR “report*”) AND (“environment” OR “energy” OR “emission” OR \n“footprint”) \nThese search strings were modified for the different databases, using specific search functions for each \ndatabase. Using this search strategy, 78 journal articles were identified for further analysis. After reading \nabstract and full text 14 relevant articles remained to answer RQ2. \n4. Findings \nRQ1: Which techniques and approaches can be applied in the phases of the DL lifecycle to reduce the \nenergy consumption of DL models? \nTable 3 summarizes the different energy-efficient techniques and approaches, extracted from the \noverviews, at each stage of the DL lifecycle. In the following, each phase will be presented in more \ndetail (tables 4 to 8). Since this paper aims to provide a holistic overview of which energy-efficient \nmethods exist and in which phase of the DL lifecycle they can be used for more energy efficiency, the \ndifferent approaches are not described in detail. A detailed description can be found in the additional \nreferences (Add. Ref.). These references are either collected from the overviews or included by the \nauthor.  \nTab. 3: Energy-efficient techniques and approaches along the DL lifecycle \nIT-Infrastr. \nData \nModeling \nTraining \nDeployment  \nSoftware  \n•Tensorflow \n• PyTorch \n• Hardware-\noptimized \nlibraries \nActive Learning \nDesign: \n• Compact Convolution \n• Efficient Attention \n• Lightweight Softmax \n• Compact Embedding \nInitialization \nPruning \nHardware  \n• GPU \n• TPU \n \nData \nAugmentation  \nAssembling: \n• Memory Sharing \n• Static Weight Sharing \n• Dynamic Weight \nSharing \nNormalization \nLow-Rank \nFactorization \nQuantization \nNeuromorphic \nComputing \nPre-trained \nmodels \nHyper-Parameter \nOptimization/Neural \nArchitecture Search  \nProgressive \nTraining \nKnowledge \nDistillation \nMixed-Precision \nTraining \nDeployment \nSharing \n \n \n5 \n \n4.1 IT-Infrastructure \nThe basis for running a DL model efficiently is a robust software and hardware infrastructure [5]. This \nsection provides an overview of software and hardware components that are critical to model efficiency. \nTab. 4: Efficient IT-Infrastructure \nTechnique/ \nApproach \nDescription \nAdd. \nRef. \nSoftware \n \n• \nTensorflow is a machine learning framework which has some of the most \nextensive software support for model efficiency (e.g. TF Lite, TF Model \nOptimization toolkit) [5]. \n• \nPyTorch is a machine learning framework, which includes for example PyTorch \nMobile (light-weight interpreter that enables running PyTorch models on \nmobiles) and a model tuning guide that lists various options available to \npractitioners such as mixed-precision training or enabling device-specific \noptimizations [5].  \n• \nHardware-optimized libraries: Efficiency can be further increased by optimizing \nfor the hardware on which the neural networks run [5]. \n[25] \n \n \n \n[26] \n \n \n \n \n \nHardware \n \n• \nGPU (Graphics Processing Units) were originally used for computer graphics, \nuntil a study in 2009 [27] demonstrated that it can be used to accelerate DL models \n[5].  \n• \nTPUs (Tensor Processing Units) are proprietary application-specific integrated \ncircuits (ASICs) that Google developed to accelerate DL applications using \nTensorflow [5].  \n \n \n \n \n \n[28] \nNeuromorphic \nComputing \nNeuromorphic Computing refers to a variety of computers, devices, and models that \nare inspired by the brain and contrast the widespread von Neumann computer \narchitecture [29]. Schuman et al. 2017 [29] discovered that one of the main \nmotivations for neuromorphic computing is speed of computation and their potential \nfor extremely low power operation. Especially the developers of early systems \nhighlighted that it is possible to perform much faster neural network computations \nwith customized chips than with traditional von Neumann architectures, in part by \nexploiting their natural parallelism, but also by building customized hardware for \nneural computations. This early focus on speed was a forerunner of the future of using \nneuromorphic systems as accelerators for machine learning or neural network style \ntasks [29]. \n[29] \n[30] \n \n4.2 Data \nIt is common to increase training data to achieve better model performance. The downside of this \napproach is the significant increase in training costs [9]. This is one of the reasons why data efficiency \nhas received significant attention over the years [22]. In this phase three techniques are presented that \ncan be used to gain data efficiency, accelerate training time, and achieve competitive results with fewer \ndata resources.  \nTab. 5: Efficient Data Usage \nTechnique/ \nApproach \nDescription \nAdd. \nRef. \nData \nAugmentation \n \nData augmentation is a solution to address the scarcity of labeled data. The basic idea \nis to synthetically inflate the existing dataset through augmentation methods, so that \nthe new label of the augmentation example does not change or can be derived in a cost-\neffective way. An example of this is the classic dog or cat image classification task. If \nthe image of a dog is shifted horizontally/vertically by a small number of pixels or it \nis rotated by a small angle, the image does not change significantly, so the transformed \nimage should still be classified as \"dog\". As a result, the classifier is forced to learn a \n[31] \n[32] \n \n6 \n \nrobust representation of the image that is more generalizable across these \ntransformations. [5] \nActive \nLearning \n \nActive learning aims to achieve good results with as few samples as possible. It was \noriginally proposed to reduce annotation costs. Today, pool-based active learning is \nwidely used to reduce training costs by selecting the most useful samples for training \na network. The idea behind active learning is the following: Annotated training data \ndo not contribute equally to final performance. Thus, if only the most useful sample is \nselected for training models, the waste of training on irrelevant samples can be largely \nreduced. [9] \n[33] \n[34] \n[35] \n[36] \nPre-trained \nmodels \nPre-trained models as initialization can be an effective approach to reduce data \nrequirements in downstream tasks [5]. This approach is further discussed in the \ntraining phase.  \n[37] \n \n4.3 Modeling \nContinuous improvements in model architectures lead to significant reductions in computational effort \nrequired to achieve a given level of accuracy. For example, the Transformer architecture developed in \n2017 required 10 to 100 times less computational effort while achieving better results than the state-of-\nthe-art models at the time [13]. This chapter concentrates on such efficient neural networks. \nFurthermore, assembling and automation as presented in Xu et al. [9] will be addressed. \nTab. 6: Efficient Modeling \nTechnique/ \nApproach \nDescription \nAdd. \nRef. \nEfficient Architecture Design \nCompact \nConvolution \n(Vision) \nIn the following compact convolution methods that improve resource-efficiency are \nlisted: \n• \nDepthwise separable convolution  \n• \nDownsampling  \n• \nFlattened convolution  \n• \nGroup convolution  \n• \nLinear bottleneck layer  \n• \nOctave convolution \n• \nShrinked convolution  \n• \nSqueezing channel/fire convolution \n \n \n[38] \n[39] \n[40] \n[41] \n[42] \n[43] \n[44] \n[45] \nEfficient \nAttention \nThe attention mechanism directly aligns all tokens from sequence-to-sequence models \ntogether, which can address long-distance dependencies to some extent. But due to the \nfact that any two tokens have an attention score, the required computations grow \nquadratically with the input length [9]. To address this problem, several studies \nproposed efficient attention variants. Xu et al. [9] classified them into the following \ncategories: \n• \nSparse attention: Reduces the span of attention  \n• \nAttention approximation: Different attention estimation formats \n[16] \n[46] \n[47] \n[48] \n[49] \nLightweight \nSoftmax \n(NLP) \nSoftmax layer introduces embeddings for all tokens, which leads to many \ncomputations for a large vocabulary [9]. Therefore, several efficient lightweight \nsoftmax variants have been proposed [9]. Xu et al. [9] distinguished these variants as \nfollows: \n• \nReduction of parameters: The proposal is to create a sequence at character \nlevel instead of word level. The number of characters is much smaller than \nthat of words, which helps to reduce the computations for softmax \nsignificantly. \n• \nReduction of computations: Xu et al. [9] classified softmax variants with \nfewer computations into five categories: \no \nHierarchical softmax \no \nSoftmax with dynamic embeddings \no \nSampling-based softmax \n[50] \n[51] \n[52] \n[53] \n[54] \n[55] \n[56] \n \n7 \n \no \nHashing-based softmax \no \nNormalization-based softmax \nCompact \nEmbedding \n(NLP) \nThe first step for NLP tasks is building token embeddings. Reducing the parameters \nof these embeddings to make them compacter is an important topic. There are several \napproaches to compress neural networks such as pruning, knowledge distillation, low-\nrank approximation, and quantization. Xu et al. [9] divided approaches for compact \nembeddings into four categories:  \n• \nReuse-based approaches \n• \nKnowledge-distillation-based approaches \n• \nLow-rank-based approaches \n• \nFine-grained vocabularies \n[57] \n[58] \n[59] \n[60] \n[61] \n[62] \nAssembling \nComponent assembling solutions aim for efficient architecture design. The key idea of \nefficient component assembling lies in sharing [9]. \nMemory \nSharing \nMemory sharing is a technique for storing large models on devices with limited \nmemories (IoT). The idea is to share the same storage among intermediate forward \nvectors [63] or backward vectors [64] to reduce memory requirements [9]. \n[65] \n[66] \nStatic Weight \nSharing \nStatic weight sharing aims to explore how weights can be reused for a neural network. \nThe weights are fixed during inference and shared among all samples. To save \nmemory, many models choose to reuse parameters across different layers or different \ntasks (dealing with problems involving multiple tasks, multiple domains, or \nmultilingual problems) [9]. \n[67] \n[68] \n[69] \n[70] \nDynamic \nWeight \nSharing \n \n \nStatic parameter sharing usually fails when dealing with tasks that are not closely \nrelated. To decide which layers/components should be shared by different input \nsamples dynamic solutions can be used. Dynamic networks are neural networks with \ndynamic computational graphs in which the computational topology or parameters are \nspontaneously determined to reduce computation costs and improve the adaptiveness \nof networks [9]. Xu et al. [9] provided an overview of general dynamic architectures. \nWithin the cascading-style network architectures, for example, first smaller networks \nand then larger ones are executed. If a smaller network can already process the input \nsample, the model stops the execution process and does not execute any further \nmodels. Skipping-style networks speed up inference by either skipping certain layers \nor omitting unimportant input spans in the entire input sequence. Other general \ndynamic architectures are early-exit-style networks and mixture-of-experts-style \nnetworks [9]. \n[71] \n[72] \n[73] \n[74] \n[75] \n[76] \n[77] \n[78] \n \nAutomation \nAutomated approaches can automatically search for ways to train more efficient models. The \ndownside is that these methods may require large computational resources [5]. However, while \nthe initial effort can be computationally intensive, the resulting efficient models can \nsignificantly reduce the computational effort required in downstream production, resulting in \nlower overall energy consumption [24]. \nNeural \nArchitecture \nSearch (NAS) \nNAS is a technique to search automatically for a global optimal efficient DNN model. \nBut it is very time-consuming and computationally expensive [79]. To solve this \nproblem, some new NAS algorithms are proposed [5]: \n• \nEvolution based search reduces the search costs. It is a two-stage search \napproach, where several well-functioning parent architectures are selected in \nthe first stage. In the second stage mutations are applied on these architectures \nto select the best one. For this stage pre-trained parent networks which does \nnot require too many computations to train child networks are used. This \nmethod requires validation accuracy as search criterion, which still makes it \ncomputationally expensive.  \n• \nDifferentiable search: This method is proposed to completely eliminate the \ndependency on validation accuracy with re-formulating the task in a \ndifferentiable manner and allowing efficient search using gradient descent.  \n• \nAnother research direction aims to represent a model in a continuous space \nwhere there is a mapping between structures and outcomes. By doing so, the \nmodel only learns how to predict the performance of architectures based on \ntheir continuous representations where the downstream training is not \nneeded.  \n• \nTraining-free NAS approaches: These approaches directly extract features \nfrom randomly-initialized models and use these features as evaluation \ncriterion to select optimal networks.  \n[80] \n[81] \n[82] \n[83] \n[84] \n[85] \n[86] \n[17] \n \n8 \n \n4.4 Training \nIn this section, the focus is on the energy efficiency of training DL models. Many approaches have been \nproposed to reduce training costs, including initialization, normalization, progressive training, and \nmixed-precision training. \nTab. 7: Efficient Training \nTechnique/ \nApproach \nDescription \nAdd. \nRef. \nInitialization \nPre-trained models from other domains (or other tasks) can be used for initialization. \nIt is generally assumed that initialization from existing models can improve the \ngeneralization ability with fewer training iterations [9]. Xu et al. [9] categorize these \npre-training initialization approaches as follows:  \n• \nFeature-based initialization: The parameters (usually from the lower or \nmiddle layers) are borrowed from other domains/tasks as initialization.  \n• \nFine-tuning-based initialization: Here the target data is used to train all \nparameters (including new parameters and borrowed parameters). It can \nfurther optimize the target objectives by fine-tuning all parameters to better \nfit the training data.  \n• \nSupervised initialization: This approach is popular in low-resource \nenvironments and is extensively studied on domain adaptation/transfer \nlearning. As a common solution, the target model is pre-trained using similar \ntasks/datasets and then the pre-trained parameters are used as initialization \nfor the target task.  \n• \nSelf-supervised initialization: To reduce the requirements of supervised \ndata, previous studies have dealt with self-supervised pre-training, which \nuses unlabeled data to construct supervision signals to learn representations. \nSince self-supervised pre-training does not require human annotated labels, \nit is easy to obtain sufficient training data.  \n[87] \n[88] \nNormalization \nNormalization is based on a special component which can accelerate convergence \nand is used to normalize hidden outputs in deep neural networks. There are different \nnormalization variants that have almost the same calculation process but are applied \nto different dimensions or objectives, such as batch normalization, layer \nnormalization, group normalization and weight normalization. Ioffe & Szegedy 2015 \n[89] were able to perform 14 times fewer training steps with the same accuracy on a \nstate-of-the-art image classification model when they used batch normalization. ([9], \n[90]) \n[90] \n[91] \nProgressive \nTraining \nThe idea of progressive training is to add layers constructively. When compared to \nfull training, progressive training does not require full gradients to all parameters and \ncan therefore reduce the computations required for training. Furthermore, the well-\ntrained lower layers also accelerate the training of the higher layers [9]. \n[92] \n[93] \nMixed-\nPrecision \nTraining \nA distinction can be made between a simple precision format (FP32), which requires \n32 bits of memory, and lower precision format (FP16), which requires 16 bits of \nmemory. The lower precision offers numerous advantages, such as the ability to train \nand deploy larger neural networks, the reduced load on memory, which speeds up \ndata transfer operations, and finally, the acceleration of computations. However, \ntraining with low precision also affects the accuracy of the model: The fewer bits \nused, the lower the accuracy. Mixed precision training can be used as a solution. This \napproach can reduce the memory requirements by almost half while maintaining the \nmodel accuracy. This is achieved by identifying the steps that require full precision \nto use FP32 for them, while using FP16 for all other steps. ([12], [94]) \n[95] \n \n4.5 Deployment (Inference) \nState-of-the-art neural network models have millions of parameters that increase the trained model's size \nand thus increase inference costs [1]. Therefore, it is important to compress and accelerate these models \nbefore deploying them without compromising model accuracy. This section describes common model \n \n9 \n \ncompression methods for reducing inference costs, including pruning, low-rank factorization, \nquantization, and knowledge distillation. These techniques are commonly used to compress a model \nafter the training to accelerate inference [1]. Deployment sharing is also presented in this phase. \nTab. 8: Efficient Inference \nTechnique/ \nApproach \nDescription \nAdd. \nRef. \nPruning  \nPruning can be used to remove redundant elements in neural networks to reduce the \nsize of the model and the computational cost. The main idea is to create a smaller \nnetwork by removing unimportant weights, filters, channels, or even layers from the \noriginal DNN while keeping the accuracy ([9], [96]). Generally, network pruning can \nbe divided into (1) pruning of connections (weights) (unstructured pruning) and (2) \npruning of filters or channels (structural pruning) [96]. \n• \nVery effective for reducing the number of parameters in DNNs [5] \n• \nRequires iteratively scoring weights and re-training the network for many \niterations [5] \n• \nOften leads to non-negligible performance drop when applied to powerful \nDNNs [5] \n[19] \nLow-Rank \nFactorization \nThis techniques uses tensor/matrix decomposition to reduce the complexity of \nconvolutional or fully connected layers in deep neural networks [10]. The aim of low \nrank factorization is to factorize a weight matrix into two matrices with low dimensions \n[1]. \n• \nReduces memory storage and accelerates DNNs [1] \n• \nIn comparison to other common compression methods, it can effectively \nreduce the size of models with a large compression ratio while maintaining \ngood performance [9] \n• \nComputationally complicated [9] \n• \nLess effective in reducing computational cost and inference time than other \ncommon compression methods [9] \n[18] \nQuantization \nQuantization reduces computation by reducing the number of bits per weight. It \nminimizes the bit-width of data storage and flow through the DNN. Computing and \nstoring data with a smaller bit-width enables fast inference with lower energy \nconsumption [96]. \n• \nCan help significantly reduce model size and inference latency [5] \n• \nEasy to implement [9] \n• \nPost-training quantization often causes a non-negligible drop in performance, \nwhereas quantization-sensitive training can effectively reduce the \nperformance drop [9] \n[97] \nKnowledge \nDistillation \nThe knowledge gained from a large-scale high performing model (teacher model), \nwhich generalizes well on unseen data, is transferred to a smaller and lighter network \nknown as the student model [1]. \n• \nImproves resource efficiency [1] \n• \nSame/comparable performance as the larger model [1] \n[98] \nDeployment \nSharing \n \nThe optimal neural network architecture varies significantly with different hardware \nresources [14]. Therefore, developing elastic or dynamic models that meet different \nconstraints is crucial for practical applications. During inference, the appropriate \nsubnetwork is selected to satisfy the resource constraints from different devices. By \namortization of the one-time training cost, the total cost of the specialized design can \nbe reduced [9]. \n[99] \n[100] \n \nRQ2: How can models be evaluated in terms of their energy consumption and carbon emissions?  \nEven though more and more research is done in energy-efficient DL, standardized metrics of models' \nenergy efficiency and carbon emissions still do not exist till do not exist [9]. However, many studies \nencourage researchers to consider these two values when evaluating models ([3], [22], [101], [102]). \nThis would help promote those models that achieve high accuracy with lower energy consumption. It \n \n10 \n \nwould also raise practitioners' awareness of their carbon emissions, which could encourage them to take \nactive steps to reduce them. To determine the evaluation possibilities of DL models in terms of their \nenergy consumption and carbon emissions, (1) information from the relevant overviews was collected \nand (2) a literature review was conducted. The found insights and publications can be categorized into \n“Metrics” and “Tools” as shown in table 9.  \nTab. 9: Evaluation Metrics and Tools \nEvaluation metrics \nand tools \nDescription \nMetric \n \nCarbon emission \nTaking carbon emissions as a metric would be the most logical, since this is what is \nto be minimized. However, it is difficult to measure the exact amount of carbon \nreleased by training or running a model, as this number is highly dependent on the \nlocal power infrastructure. Therefore, this metric would not be comparable between \nresearchers at different locations or even at the same location at different times [22]. \nTo measure the carbon emission various online tools (see section “tools”) can be \nused. \nRun Time \nMeasuring the running time of training and inference could be a valuable metric if \nall models had the same hardware and software settings. Since this is not the case \nand running time is highly dependent on infrastructure settings, models running with \ndifferent settings are not comparable. Nevertheless, reporting the running time can \nbe important for a general understanding of the impact. ([9], [22]) \nEnergy  \nThe energy consumption can be calculated by multiplying the maximum power \nconsumption of the individual hardware used according to their technical \nspecifications by the number of hardware used for training and then by the training \ntime in hours [20].  \nAccuracy per Joule \n \nThis metric captures the accuracy, latency, and energy tradeoffs between models \n[12]. Hanafy et al. [103] propose to compute it as follows: \nAccuracy/Joule = ModelAccuracy/Energy_per_Request.  \nIt indicates how much energy is required for one unit of accuracy and serves as a \nnormalized metric and a benchmark for comparison between two or more models. \nFull-Cycle Energy \nConsumption Metric  \n“Greeness” \nThis metric proposed from Li et al. [104] focuses on the energy consumption during \nthe training and inference, the accuracy, as well as the model usage intensity. It is \ncalculated as follows: \nG(MUI) = Accτ/(MUI ∗ IEC + TEC) \n• \nTrain Energy Cost (TEC) calculates the total energy consumption of efficient \nDL throughout the training phase, including base model training, model \ncompression, and retraining the model.  \n• \nInference energy cost (IEC) refers to the energy consumed to perform inference \nfor one time. \n• \nModel Usage Intensity (MUI) is defined as the average number of inferences in \neach lifecycle. The importance of TEC and IEC varies depending on the MUI. \nIf an AI system uses the model intensively and the number of inferences in a \nlifecycle is large, then IEC accounts for a large portion of the energy \nconsumption and conversely.  \n• \nAccuracy (Acc) refers to the accuracy of the model on a given task. Efficient DL \nmodels usually trade accuracy for efficiency and their accuracy degradation can \nvary significantly. For this reason, the accuracy of the models should also be \nconsidered. \nModel Size/Number of \nparameters  \n \nModel size/Number of parameters are also a crucial factor when it comes to training \nand inference costs [9]. It can be determined quite easily and it is usually directly \ncorrelated with the complexity of the computations [20]. Unlike the previously \ndescribed measures, it is not dependent on the underlying hardware. Nevertheless, \ndifferent algorithms use its parameters differently, for example by making the model \ndeeper or wider. As a result, different models with a similar number of parameters \nare often not comparable [22]. \nFLOPs \nFloating point operations (FLOPs) count the number of operations required to run a \nmodel when executing a specific instance. The advantage of this metric is that it is \n \n11 \n \nnearly independent of hardware and software settings and can therefore be an easy \nway to make a fair comparison between different models [9].  \nTools \n \neco2AI \neco2AI [105] is an open-source Python library for carbon emission tracking. It allows \nusers to monitor energy consumption of CPU and GPU devices and estimate \nequivalent carbon emissions taking into account the regional carbon emissions \naccounting. The eco2AI library currently includes the largest and permanently \nenriched and maintained database of regional carbon emissions accounting based on \nthe public available data in 209 countries.  \nCarbontracker \nCarbontracker [106] is an open-source Python package for tracking and predicting \nthe energy consumption and carbon footprint of computational workloads. The \ntraining of models can be cancelled as soon as the limit of environmental costs set by \nthe user has been exceeded. \nCodeCarbon \nCodeCarbon [107] is an open-source software package that can easily be integrated \nin the Python codebase. It estimates the amount of CO2 generated by the cloud or \npersonal computing resources used to run the code. It further shows how the code \ncan be optimized to reduce emissions and provides suggestions for hosting a cloud \ninfrastructure in geographic regions that use renewable energy sources.   \nEnergy Usage Reports \nIt is an open-source Python package [101] for calculating the energy and carbon \nemissions of algorithms. It provides an energy usage report in which the results are \nput into a context that is more understandable, such as car kilometers traveled. (This \npackage is no longer actively maintained since it has been integrated into the tool \nCarbonCode). \nEnergyVis \nEnergyVis [108] is an interactive energy consumption tracker for ML models. It can \nbe used for tracking, visualizing, and comparing model energy consumption in terms \nof energy consumption and CO2 metrics. It further shows alternative deployment \nlocations and hardware that can reduce the carbon footprint. \nExperiment-Impact-\nTracker \nThe experiment-impact-tracker [102] is an open-source drop-in method to track \nenergy usage, carbon emissions, and compute utilization of the system used. It also \ngenerates carbon impact statements for enabling standardized reporting.  \nGreen Algorithms \nGreen Algorithms [109] is a self-reporting free online tool that allows users to \nestimate the carbon footprint of their computations. The tool requires minimal \ninformation and considers a wide range of hardware configurations. After providing \nall the required information, the tool visualizes and contextualizes the estimated \ncarbon footprint of the computations. \nML CO2 Impact \nThis free online tool relies on self-reporting [110]. The tool can estimate the carbon \nfootprint of GPU computation by specifying hardware type, hours used, cloud \nprovider, and region. \n \nOne important issue that needs to be kept in mind when evaluating energy-efficient models is accuracy. \nIf two models are compared and one is significantly more energy-efficient than the other, but at the same \ntime has significantly lower accuracy, the comparison is not fair. Therefore, for example, Lee et al. [12] \nand Li et al. [104] include accuracy in their metrics. Furthermore, the optimal metric for energy \nefficiency should allow a fair comparison between different models. Therefore, this metric should \nideally be stable under different influencing factors such as infrastructure. FLOPs could be an interesting \nmetric, since it is nearly independent of hardware and software settings. Xu et al. [6] and Douwes et al. \n[20] suggest reporting the FLOPs during model training and inference. However, they would also \nencourage researchers to specify the total FLOPs during all experiments, including but not limited to \nparameter tuning and base implementation, to identify the wasted and redundant computations required \nto develop a new model/algorithm.  \n \n \n \n \n12 \n \n5. Discussion  \nThis paper gives a holistic overview of techniques and approaches along the DL lifecycle to reduce the \nenergy consumption of DL. Since it is an overview, the approaches are not described in detail. However, \nrelevant references with more information are listed for each approach. It must be emphasized that the \nanalyzed overviews are comprehensive, but not exhaustive. Due to the fact that the overviews focusing \non resource-limited devices were not included, there are existing energy-efficient approaches (f. e. \nfederated learning) that are not listed in this overview. However, this overview will hopefully give \npractitioners an idea at which point along the DL lifecycle and how the energy consumption of DL \nmodels can be reduced.  \n6. Conclusion \nThere are various approaches to increase the energy efficiency of DL. Such approaches have been \nextracted from recent overviews and classified within the six defined phases of the DL lifecycle. During \nthe classification process, it was noticeable that approaches for the evaluation phase were not listed to \nthe same extent in the overviews. This is because this phase is mainly about evaluating the energy \nefficiency of DL, not improving it. However, evaluating the energy efficiency of DL can also lead to \nimproved energy efficiency, as practitioners' awareness about emitting CO2 increases and some \nevaluation tools even provide recommendations on how to reduce energy consumption. The reporting \nof the models' energy metrics can also help achieve more energy efficiency in further research, as \nresearchers can compare the models concerning their energy efficiency and thus, for example, use the \nmodel with lower energy consumption as a base or pre-trained model.   \n7. Future Work  \nLooking to the future of energy-efficient DL, it would be important to continue research in this direction, \nto develop more efficient hardware and software, more efficient architectures, and more or improved \napproaches reducing energy consumption. In addition, a standardized metric for energy efficiency \nshould be introduced to enable the evaluation and comparison of DL models and to raise awareness \namong researchers and practitioners. Moreover, a clear and transparent reporting of the measurements \nis crucial. Here, not only the energy consumption of training but also that of the inference should be \nconsidered. Further research could include defining guidelines, proposing a process model or a \nframework that can serve as a step-by-step guide for practitioners to achieve energy improvements \nthroughout the DL lifecycle.  \nReferences \n[1] \nT. Choudhary, V. Mishra, A. Goswami, and J. Sarangapani, ‘A comprehensive survey on model \ncompression and acceleration’, Artif. Intell. Rev., vol. 53, no. 7, pp. 5113–5155, Oct. 2020, doi: \n10.1007/s10462-020-09816-7. \n[2] \nS. Wang, ‘Efficient deep learning’, Nat. Comput. Sci., vol. 1, no. 3, pp. 181–182, Mar. 2021, doi: \n10.1038/s43588-021-00042-x. \n[3] \nE. Strubell, A. Ganesh, and A. McCallum, ‘Energy and Policy Considerations for Deep Learning \nin NLP’, Jan. 2019, doi: 10.48550/arXiv.1906.02243. \n[4] \nM. Kumar, X. Zhang, L. Liu, Y. Wang, and W. Shi, ‘Energy-Efficient Machine Learning on the \nEdges’, in 2020 IEEE International Parallel and Distributed Processing Symposium Workshops \n(IPDPSW), \nNew \nOrleans, \nLA, \nUSA, \nMay \n2020, \npp. \n912–921. \ndoi: \n10.1109/IPDPSW50202.2020.00153. \n \n13 \n \n[5] \nG. Menghani, ‘Efficient Deep Learning: A Survey on Making Deep Learning Models  Smaller, \nFaster, and Better’, Jun. 2021. [Online]. Available: http://arxiv.org/pdf/2106.08962v2 \n[6] \nC.-J. Wu et al., ‘Sustainable AI: Environmental Implications, Challenges and Opportunities’, \n2021, doi: 10.48550/ARXIV.2111.00364. \n[7] \nM. Schulz et al., ‘DASC-PM v1.0 - Ein Vorgehensmodell für Data-Science-Projekte’, Feb. 2021, \ndoi: 10.25673/32872.2. \n[8] \nR. Wirth and J. Hipp, ‘CRISP-DM: Towards a Standard Process Model for Data Mining.’ 2000. \n[9] \nJ. Xu, W. Zhou, Z. Fu, H. Zhou, and L. Li, ‘A Survey on Green Deep Learning’, Nov. 2021. \n[Online]. Available: http://arxiv.org/pdf/2111.05193v2 \n[10] \nH. Cai et al., ‘Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications’, \nACM Trans. Des. Autom. Electron. Syst., vol. 27, no. 3, pp. 1–50, May 2022, doi: \n10.1145/3486618. \n[11] \nD. Liu, H. Kong, X. Luo, W. Liu, and R. Subramaniam, ‘Bringing AI to edge: From deep \nlearning’s perspective’, Neurocomputing, vol. 485, pp. 297–320, May 2022, doi: \n10.1016/j.neucom.2021.04.141. \n[12] \nJ. Lee et al., ‘Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and \nImplementation-Level Techniques’. arXiv, Dec. 30, 2021. Accessed: Jun. 14, 2022. [Online]. \nAvailable: http://arxiv.org/abs/2112.15131 \n[13] \nA. Vaswani et al., ‘Attention Is All You Need’. arXiv, Dec. 05, 2017. Accessed: Jul. 04, 2022. \n[Online]. Available: http://arxiv.org/abs/1706.03762 \n[14] \nH. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, ‘Once-for-All: Train One Network and \nSpecialize it for Efficient Deployment’, ArXiv190809791 Cs Stat, Apr. 2020, Accessed: May 10, \n2022. [Online]. Available: http://arxiv.org/abs/1908.09791 \n[15] \nA. Fan et al., ‘Training with Quantization Noise for Extreme Model Compression’, ArXiv Learn., \nApr. 2020. \n[16] \nN. Kitaev, Ł. Kaiser, and A. Levskaya, ‘Reformer: The Efficient Transformer’. arXiv, Feb. 18, \n2020. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/2001.04451 \n[17] \nJingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, and Hongxia Yang, ‘KNAS: \nGreen Neural Architecture Search’, ICML, 2021. \n[18] \nS. Cahyawijaya et al., ‘Greenformer: Factorization Toolkit for Efficient Deep Neural Networks’, \nArXiv Learn., Sep. 2021. \n[19] \nT. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, ‘Sparsity in Deep Learning: \nPruning and growth for efficient inference and training in neural networks’, ArXiv Learn., 2021. \n[20] \nC. Douwes, P. Esling, and J.-P. Briot, ‘Energy Consumption of Deep Generative Audio Models’. \narXiv, \nOct. \n13, \n2021. \nAccessed: \nJul. \n06, \n2022. \n[Online]. \nAvailable: \nhttp://arxiv.org/abs/2107.02621 \n[21] \nDavid A. Patterson et al., ‘Carbon Emissions and Large Neural Network Training’, ArXiv, 2021. \n[22] \nR. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, ‘Green AI’, Jul. 2019. [Online]. Available: \nhttp://arxiv.org/pdf/1907.10597v3 \n[23] \nA. van Wynsberghe, ‘Sustainable AI: AI for sustainability and the sustainability of AI’, AI Ethics, \nvol. 1, no. 3, pp. 213–218, Jan. 2021, doi: 10.1007/s43681-021-00043-6. \n[24] \nS. R. Young et al., ‘Evolving Energy Efficient Convolutional Neural Networks’, in 2019 IEEE \nInternational Conference on Big Data (Big Data), Los Angeles, CA, USA, Dec. 2019, pp. 4479–\n4485. doi: 10.1109/BigData47090.2019.9006239. \n[25] \nMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu \nDevin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, \nRajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, and Vijay \nVasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng, ‘TensorFlow: A \nSystem for Large-Scale Machine Learning’, 12th USENIX Symp. Oper. Syst. Des. Implement. \nOSDI 16, pp. 265–283, 2016. \n[26] \nA. Paszke et al., ‘PyTorch: An Imperative Style, High-Performance Deep Learning Library’. \narXiv, \nDec. \n03, \n2019. \nAccessed: \nJun. \n25, \n2022. \n[Online]. \nAvailable: \nhttp://arxiv.org/abs/1912.01703 \n \n14 \n \n[27] \nR. Raina, A. Madhavan, and A. Y. Ng, ‘Large-scale deep unsupervised learning using graphics \nprocessors’, in Proceedings of the 26th Annual International Conference on Machine Learning \n- ICML ’09, Montreal, Quebec, Canada, 2009, pp. 1–8. doi: 10.1145/1553374.1553486. \n[28] \nN. P. Jouppi et al., ‘In-Datacenter Performance Analysis of a Tensor Processing Unit’, in \nProceedings of the 44th Annual International Symposium on Computer Architecture, Toronto \nON Canada, Jun. 2017, pp. 1–12. doi: 10.1145/3079856.3080246. \n[29] \nC. D. Schuman et al., ‘A Survey of Neuromorphic Computing and Neural Networks in \nHardware’. arXiv, May 19, 2017. Accessed: Jun. 26, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1705.06963 \n[30] \nA. Shrestha, H. Fang, Z. Mei, D. P. Rider, Q. Wu, and Q. Qiu, ‘A Survey on Neuromorphic \nComputing: Models and Hardware’, IEEE Circuits Syst. Mag., vol. 22, no. 2, pp. 6–35, 2022, \ndoi: 10.1109/MCAS.2022.3166331. \n[31] \nE. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, ‘Randaugment: Practical automated data \naugmentation with a reduced search space’, in 2020 IEEE/CVF Conference on Computer Vision \nand Pattern Recognition Workshops (CVPRW), Seattle, WA, USA, Jun. 2020, pp. 3008–3017. \ndoi: 10.1109/CVPRW50498.2020.00359. \n[32] \nX. Wang, H. Pham, Z. Dai, and G. Neubig, ‘SwitchOut: an Efficient Data Augmentation \nAlgorithm for Neural Machine Translation’. arXiv, Aug. 28, 2018. Accessed: Jul. 06, 2022. \n[Online]. Available: http://arxiv.org/abs/1808.07512 \n[33] \nS. Mohamadi and H. Amindavar, ‘Deep Bayesian Active Learning, A Brief Survey on Recent \nAdvances’. arXiv, Apr. 21, 2022. Accessed: Jun. 13, 2022. [Online]. Available: \nhttp://arxiv.org/abs/2012.08044 \n[34] \nP. Ren et al., ‘A Survey of Deep Active Learning’, ACM Comput. Surv., vol. 54, no. 9, pp. 1–40, \nDec. 2022, doi: 10.1145/3472291. \n[35] \nP. F. Jacobs, G. M. de B. Wenniger, M. Wiering, and L. Schomaker, ‘Active learning for reducing \nlabeling effort in text classification tasks’. Nov. 03, 2021. Accessed: Jun. 15, 2022. [Online]. \nAvailable: http://arxiv.org/abs/2109.04847 \n[36] \nK. Margatina, L. Barrault, and N. Aletras, ‘Bayesian Active Learning with Pretrained Language \nModels’, \nJan. \n2021, \nAccessed: \nJun. \n15, \n2022. \n[Online]. \nAvailable: \nhttps://openreview.net/forum?id=oO1QGIKkEsY \n[37] \nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, ‘Pre-trained models for natural language \nprocessing: A survey’, Sci. China Technol. Sci., vol. 63, no. 10, pp. 1872–1897, Oct. 2020, doi: \n10.1007/s11431-020-1647-3. \n[38] \nF. Chollet, ‘Xception: Deep Learning with Depthwise Separable Convolutions’. arXiv, Apr. 04, \n2017. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1610.02357 \n[39] \nZ. Qin, Z. Zhang, X. Chen, C. Wang, and Y. Peng, ‘Fd-Mobilenet: Improved Mobilenet with a \nFast Downsampling Strategy’, in 2018 25th IEEE International Conference on Image Processing \n(ICIP), Athens, Oct. 2018, pp. 1363–1367. doi: 10.1109/ICIP.2018.8451355. \n[40] \nJ. Jin, A. Dundar, and E. Culurciello, ‘Flattened Convolutional Neural Networks for Feedforward \nAcceleration’. arXiv, Nov. 20, 2015. Accessed: Jun. 22, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1412.5474 \n[41] \nG. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, ‘Densely Connected Convolutional \nNetworks’. arXiv, Jan. 28, 2018. Accessed: Jun. 22, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1608.06993 \n[42] \nG. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger, ‘CondenseNet: An Efficient \nDenseNet using Learned Group Convolutions’. arXiv, Jun. 07, 2018. Accessed: Jun. 22, 2022. \n[Online]. Available: http://arxiv.org/abs/1711.09224 \n[43] \nM. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, ‘MobileNetV2: Inverted \nResiduals and Linear Bottlenecks’. arXiv, Mar. 21, 2019. Accessed: Jun. 22, 2022. [Online]. \nAvailable: http://arxiv.org/abs/1801.04381 \n[44] \nA. G. Howard et al., ‘MobileNets: Efficient Convolutional Neural Networks for Mobile Vision \nApplications’. arXiv, Apr. 16, 2017. Accessed: Jun. 22, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1704.04861 \n \n15 \n \n[45] \nF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, ‘SqueezeNet: \nAlexNet-level accuracy with 50x fewer parameters and <0.5MB model size’. arXiv, Nov. 04, \n2016. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1602.07360 \n[46] \nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, ‘Transformer-XL: \nAttentive Language Models Beyond a Fixed-Length Context’. arXiv, Jun. 02, 2019. Accessed: \nJun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1901.02860 \n[47] \nG. M. Correia, V. Niculae, and A. F. T. Martins, ‘Adaptively Sparse Transformers’. arXiv, Sep. \n06, 2019. Accessed: Jun. 22, 2022. [Online]. Available: http://arxiv.org/abs/1909.00015 \n[48] \nR. Child, S. Gray, A. Radford, and I. Sutskever, ‘Generating Long Sequences with Sparse \nTransformers’. arXiv, Apr. 23, 2019. Accessed: Jun. 22, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1904.10509 \n[49] \nK. Choromanski et al., ‘Rethinking Attention with Performers’. arXiv, Mar. 09, 2021. Accessed: \nJun. 22, 2022. [Online]. Available: http://arxiv.org/abs/2009.14794 \n[50] \nM. R. Costa-Jussà and J. A. R. Fonollosa, ‘Character-based Neural Machine Translation’. arXiv, \nJun. 30, 2016. Accessed: Jun. 24, 2022. [Online]. Available: http://arxiv.org/abs/1603.00810 \n[51] \nR. Sennrich, B. Haddow, and A. Birch, ‘Neural Machine Translation of Rare Words with \nSubword Units’. arXiv, Jun. 10, 2016. Accessed: Jun. 24, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1508.07909 \n[52] \nQ. Liu et al., ‘Hierarchical Softmax for End-to-End Low-resource Multilingual Speech \nRecognition’. arXiv, Apr. 08, 2022. Accessed: Jul. 12, 2022. [Online]. Available: \nhttp://arxiv.org/abs/2204.03855 \n[53] \nW. Chen, D. Grangier, and M. Auli, ‘Strategies for Training Large Vocabulary Neural Language \nModels’. arXiv, Dec. 15, 2015. Accessed: Jun. 24, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1512.04906 \n[54] \nS. Jean, K. Cho, R. Memisevic, and Y. Bengio, ‘On Using Very Large Target Vocabulary for \nNeural Machine Translation’. arXiv, Mar. 18, 2015. Accessed: Jun. 24, 2022. [Online]. \nAvailable: http://arxiv.org/abs/1412.2007 \n[55] \nS. Vijayanarasimhan, J. Shlens, R. Monga, and J. Yagnik, ‘Deep Networks With Large Output \nSpaces’. arXiv, Apr. 10, 2015. Accessed: Jun. 24, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1412.7479 \n[56] \nJ. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul, ‘Fast and Robust Neural \nNetwork Joint Models for Statistical Machine Translation’, in Proceedings of the 52nd Annual \nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, \nMaryland, 2014, pp. 1370–1380. doi: 10.3115/v1/P14-1129. \n[57] \nR. Shu and H. Nakayama, ‘Compressing Word Embeddings via Deep Compositional Code \nLearning’. arXiv, Nov. 17, 2017. Accessed: Jun. 24, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1711.01068 \n[58] \nM. Joshi, E. Choi, O. Levy, D. S. Weld, and L. Zettlemoyer, ‘pair2vec: Compositional Word-\nPair Embeddings for Cross-Sentence Inference’. arXiv, Apr. 05, 2019. Accessed: Jun. 24, 2022. \n[Online]. Available: http://arxiv.org/abs/1810.08854 \n[59] \nH.-J. M. Shi, D. Mudigere, M. Naumov, and J. Yang, ‘Compositional Embeddings Using \nComplementary Partitions for Memory-Efficient Recommendation Systems’, in Proceedings of \nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, \nVirtual Event CA USA, Aug. 2020, pp. 165–175. doi: 10.1145/3394486.3403059. \n[60] \nL. Mou, R. Jia, Y. Xu, G. Li, L. Zhang, and Z. Jin, ‘Distilling Word Embeddings: An Encoding \nApproach’, in Proceedings of the 25th ACM International on Conference on Information and \nKnowledge Management, Indianapolis Indiana USA, Oct. 2016, pp. 1977–1980. doi: \n10.1145/2983323.2983888. \n[61] \nP. H. Chen, S. Si, Y. Li, C. Chelba, and C. Hsieh, ‘GroupReduce: Block-Wise Low-Rank \nApproximation for Neural Language Model Shrinking’. arXiv, Jun. 18, 2018. Accessed: Jul. 12, \n2022. [Online]. Available: http://arxiv.org/abs/1806.06950 \n[62] \nC. Wang, K. Cho, and J. Gu, ‘Neural Machine Translation with Byte-Level Subwords’, Proc. \nAAAI Conf. Artif. Intell., vol. 34, no. 05, pp. 9154–9160, Apr. 2020, doi: \n10.1609/aaai.v34i05.6451. \n \n16 \n \n[63] \nG. Pleiss, D. Chen, G. Huang, T. Li, L. van der Maaten, and K. Q. Weinberger, ‘Memory-\nEfficient Implementation of DenseNets’. arXiv, Jul. 21, 2017. Accessed: Jun. 24, 2022. [Online]. \nAvailable: http://arxiv.org/abs/1707.06990 \n[64] \nT. Chen, B. Xu, C. Zhang, and C. Guestrin, ‘Training Deep Nets with Sublinear Memory Cost’. \narXiv, \nApr. \n22, \n2016. \nAccessed: \nJun. \n24, \n2022. \n[Online]. \nAvailable: \nhttp://arxiv.org/abs/1604.06174 \n[65] \nL. Wang et al., ‘Superneurons: dynamic GPU memory management for training deep neural \nnetworks’, in Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of \nParallel Programming, Vienna Austria, Feb. 2018, pp. 41–53. doi: 10.1145/3178487.3178491. \n[66] \nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, ‘ZeRO: Memory optimizations Toward \nTraining Trillion Parameter Models’, in SC20: International Conference for High Performance \nComputing, Networking, Storage and Analysis, Atlanta, GA, USA, Nov. 2020, pp. 1–16. doi: \n10.1109/SC41405.2020.00024. \n[67] \nM. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser, ‘Universal Transformers’. \narXiv, \nMar. \n05, \n2019. \nAccessed: \nJun. \n24, \n2022. \n[Online]. \nAvailable: \nhttp://arxiv.org/abs/1807.03819 \n[68] \nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, ‘ALBERT: A Lite BERT \nfor Self-supervised Learning of Language Representations’. arXiv, Feb. 08, 2020. Accessed: Jun. \n24, 2022. [Online]. Available: http://arxiv.org/abs/1909.11942 \n[69] \nS. Takase and S. Kiyono, ‘Lessons on Parameter Sharing across Layers in Transformers’. arXiv, \nApr. 20, 2022. Accessed: Jun. 24, 2022. [Online]. Available: http://arxiv.org/abs/2104.06022 \n[70] \nK. Hashimoto, C. Xiong, Y. Tsuruoka, and R. Socher, ‘A Joint Many-Task Model: Growing a \nNeural Network for Multiple NLP Tasks’. arXiv, Jul. 24, 2017. Accessed: Jun. 24, 2022. \n[Online]. Available: http://arxiv.org/abs/1611.01587 \n[71] \nE. Park et al., ‘Big/little deep neural network for ultra low power inference’, in 2015 \nInternational \nConference \non \nHardware/Software \nCodesign \nand \nSystem \nSynthesis \n(CODES+ISSS), \nAmsterdam, \nNetherlands, \nOct. \n2015, \npp. \n124–132. \ndoi: \n10.1109/CODESISSS.2015.7331375. \n[72] \nA. Gormez and E. Koyuncu, ‘Class Means as an Early Exit Decision Mechanism’. arXiv, Oct. \n20, 2021. Accessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/2103.01148 \n[73] \nL. Yang, Y. Han, X. Chen, S. Song, J. Dai, and G. Huang, ‘Resolution Adaptive Networks for \nEfficient Inference’. arXiv, May 18, 2020. Accessed: Jun. 25, 2022. [Online]. Available: \nhttp://arxiv.org/abs/2003.07326 \n[74] \nX. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez, ‘SkipNet: Learning Dynamic Routing \nin Convolutional Networks’. arXiv, Jul. 25, 2018. Accessed: Jun. 25, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1711.09485 \n[75] \nZ. Wu et al., ‘BlockDrop: Dynamic Inference Paths in Residual Networks’. arXiv, Jan. 28, 2019. \nAccessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/1711.08393 \n[76] \nA. Veit and S. Belongie, ‘Convolutional Networks with Adaptive Inference Graphs’. arXiv, May \n08, 2020. Accessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/1711.11503 \n[77] \nD. Lepikhin et al., ‘GShard: Scaling Giant Models with Conditional Computation and Automatic \nSharding’. arXiv, Jun. 30, 2020. Accessed: Jun. 25, 2022. [Online]. Available: \nhttp://arxiv.org/abs/2006.16668 \n[78] \nJ. Lin et al., ‘M6: A Chinese Multimodal Pretrainer’. arXiv, May 29, 2021. Accessed: Jun. 25, \n2022. [Online]. Available: http://arxiv.org/abs/2103.00823 \n[79] \nJ.-X. Mi, J. Feng, and K.-Y. Huang, ‘Designing efficient convolutional neural network structure: \nA \nsurvey’, \nNeurocomputing, \nvol. \n489, \npp. \n139–156, \nJun. \n2022, \ndoi: \n10.1016/j.neucom.2021.08.158. \n[80] \nE. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ‘Regularized Evolution for Image Classifier \nArchitecture Search’, Proc. AAAI Conf. Artif. Intell., vol. 33, pp. 4780–4789, Jul. 2019, doi: \n10.1609/aaai.v33i01.33014780. \n[81] \nY. Jiang, C. Hu, T. Xiao, C. Zhang, and J. Zhu, ‘Improved Differentiable Architecture Search \nfor Language Modeling and Named Entity Recognition’, in Proceedings of the 2019 Conference \non Empirical Methods in Natural Language Processing and the 9th International Joint \n \n17 \n \nConference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, 2019, pp. \n3583–3588. doi: 10.18653/v1/D19-1367. \n[82] \nA. Zela, T. Elsken, T. Saikia, Y. Marrakchi, T. Brox, and F. Hutter, ‘Understanding and \nRobustifying Differentiable Architecture Search’. arXiv, Jan. 28, 2020. Accessed: Jun. 25, 2022. \n[Online]. Available: http://arxiv.org/abs/1909.09656 \n[83] \nX. Chu and B. Zhang, ‘Noisy Differentiable Architecture Search’. arXiv, Oct. 17, 2021. \nAccessed: Jun. 25, 2022. [Online]. Available: http://arxiv.org/abs/2005.03566 \n[84] \nR. Luo, F. Tian, T. Qin, E. Chen, and T.-Y. Liu, ‘Neural Architecture Optimization’. arXiv, Sep. \n04, 2019. Accessed: Jul. 12, 2022. [Online]. Available: http://arxiv.org/abs/1808.07233 \n[85] \nW. Chen, X. Gong, and Z. Wang, ‘Neural Architecture Search on ImageNet in Four GPU Hours: \nA Theoretically Inspired Perspective’. arXiv, Mar. 15, 2021. Accessed: Jun. 25, 2022. [Online]. \nAvailable: http://arxiv.org/abs/2102.11535 \n[86] \nM. S. Abdelfattah, A. Mehrotra, Ł. Dudziak, and N. D. Lane, ‘Zero-Cost Proxies for Lightweight \nNAS’. \narXiv, \nMar. \n19, \n2021. \nAccessed: \nJun. \n25, \n2022. \n[Online]. \nAvailable: \nhttp://arxiv.org/abs/2101.08134 \n[87] \nZ. Alyafeai, M. S. AlShaibani, and I. Ahmad, ‘A Survey on Transfer Learning in Natural \nLanguage Processing’. arXiv, May 31, 2020. Accessed: Jun. 15, 2022. [Online]. Available: \nhttp://arxiv.org/abs/2007.04239 \n[88] \nF. Zhuang et al., ‘A Comprehensive Survey on Transfer Learning’, Proc. IEEE, vol. 109, no. 1, \npp. 43–76, Jan. 2021, doi: 10.1109/JPROC.2020.3004555. \n[89] \nS. Ioffe and C. Szegedy, ‘Batch Normalization: Accelerating Deep Network Training by \nReducing Internal Covariate Shift’. arXiv, Mar. 02, 2015. Accessed: Jul. 12, 2022. [Online]. \nAvailable: http://arxiv.org/abs/1502.03167 \n[90] \nJ. L. Ba, J. R. Kiros, and G. E. Hinton, ‘Layer Normalization’. arXiv, Jul. 21, 2016. Accessed: \nJul. 03, 2022. [Online]. Available: http://arxiv.org/abs/1607.06450 \n[91] \nY. Wu and K. He, ‘Group Normalization’. arXiv, Jun. 11, 2018. Accessed: Jul. 03, 2022. \n[Online]. Available: http://arxiv.org/abs/1803.08494 \n[92] \nL. Gong, D. He, Z. Li, T. Qin, L. Wang, and T.-Y. Liu, ‘Efficient Training of BERT by \nProgressively Stacking’, presented at the Proceedings of the 36 th International Conference on \nMachine Learnin, Long Beach, California, 2019, vol. PMLR 97. \n[93] \nC. Yang, S. Wang, C. Yang, Y. Li, R. He, and J. Zhang, ‘Progressively Stacking 2.0: A Multi-\nstage Layerwise Training Method for BERT Training Speedup’. arXiv, Nov. 27, 2020. Accessed: \nJul. 03, 2022. [Online]. Available: http://arxiv.org/abs/2011.13635 \n[94] \nX. He, F. Xue, X. Ren, and Y. You, ‘Large-Scale Deep Learning Optimizations: A \nComprehensive Survey’. arXiv, Nov. 01, 2021. Accessed: Jun. 09, 2022. [Online]. Available: \nhttp://arxiv.org/abs/2111.00856 \n[95] \nP. Micikevicius et al., ‘Mixed Precision Training’. arXiv, Feb. 15, 2018. Accessed: Jun. 30, 2022. \n[Online]. Available: http://arxiv.org/abs/1710.03740 \n[96] \nD. Ghimire, D. Kil, and S. Kim, ‘A Survey on Efficient Convolutional Neural Networks and \nHardware Acceleration’, Electronics, vol. 11, no. 6, p. 945, Mar. 2022, doi: \n10.3390/electronics11060945. \n[97] \nA. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, ‘A Survey of \nQuantization Methods for Efficient Neural Network Inference’, ArXiv Comput. Vis. Pattern \nRecognit., 2021. \n[98] \nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, ‘Knowledge Distillation: A Survey’, Int. J. Comput. \nVis., vol. 129, no. 6, pp. 1789–1819, Jun. 2021, doi: 10.1007/s11263-021-01453-z. \n[99] \nJ. Yu, L. Yang, N. Xu, J. Yang, and T. S. Huang, ‘Slimmable Neural Networks.’, ArXiv Comput. \nVis. Pattern Recognit., Dec. 2018. \n[100] A. Fan, E. Grave, and A. Joulin, ‘Reducing Transformer Depth on Demand with Structured \nDropout’. arXiv, Sep. 25, 2019. Accessed: Jun. 24, 2022. [Online]. Available: \nhttp://arxiv.org/abs/1909.11556 \n[101] K. Lottick, S. Susai, S. A. Friedler, and J. P. Wilson, ‘Energy Usage Reports: Environmental \nawareness as part of algorithmic accountability’, ArXiv Learn., Nov. 2019. \n \n18 \n \n[102] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau, ‘Towards the Systematic \nReporting of the Energy and Carbon Footprints of  Machine Learning’, Jan. 2020. [Online]. \nAvailable: http://arxiv.org/pdf/2002.05651v1 \n[103] W. A. Hanafy, T. Molom-Ochir, and R. Shenoy, ‘Design Considerations for Energy-efficient \nInference on Edge Devices’, E-Energy, pp. 302–308, Jun. 2021, doi: 10.1145/3447555.3465326. \n[104] B. Li et al., ‘Full-Cycle Energy Consumption Benchmark for Low-Carbon Computer Vision’, \nArXiv210813465 Cs, Oct. 2021, Accessed: May 10, 2022. [Online]. Available: \nhttp://arxiv.org/abs/2108.13465 \n[105] S. Budennyy et al., ‘Eco2AI: carbon emissions tracking of machine learning models as the first \nstep towards sustainable AI’. arXiv, Aug. 03, 2022. Accessed: Sep. 26, 2022. [Online]. \nAvailable: http://arxiv.org/abs/2208.00406 \n[106] L. F. W. Anthony, B. Kanding, and R. Selvan, ‘Carbontracker: Tracking and Predicting the \nCarbon Footprint of Training Deep Learning Models’, ArXiv200703051 Cs Eess Stat, Jul. 2020, \nAccessed: May 10, 2022. [Online]. Available: http://arxiv.org/abs/2007.03051 \n[107] V. Schmidt et al., mlco2/codecarbon: v2.1.4. Zenodo, 2022. doi: 10.5281/ZENODO.4658424. \n[108] O. Shaikh et al., ‘EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML \nModels’, ArXiv Learn., 2021. \n[109] L. Lannelongue, J. Grealey, and M. Inouye, ‘Green Algorithms: Quantifying the carbon \nemissions of computation.’, Jul. 2020. \n[110] A. Lacoste, Alexandra Luccioni, A. Luccioni, V. Schmidt, and T. Dandres, ‘Quantifying the \nCarbon Emissions of Machine Learning.’, ArXiv Comput. Soc., Oct. 2019. \n",
  "categories": [
    "cs.LG"
  ],
  "published": "2023-02-05",
  "updated": "2023-02-05"
}