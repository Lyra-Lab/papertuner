{
  "id": "http://arxiv.org/abs/2306.13586v1",
  "title": "NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants",
  "authors": [
    "Zhongzhi Yu",
    "Yonggan Fu",
    "Jiayi Yuan",
    "Haoran You",
    "Yingyan Lin"
  ],
  "abstract": "Tiny deep learning has attracted increasing attention driven by the\nsubstantial demand for deploying deep learning on numerous intelligent\nInternet-of-Things devices. However, it is still challenging to unleash tiny\ndeep learning's full potential on both large-scale datasets and downstream\ntasks due to the under-fitting issues caused by the limited model capacity of\ntiny neural networks (TNNs). To this end, we propose a framework called\nNetBooster to empower tiny deep learning by augmenting the architectures of\nTNNs via an expansion-then-contraction strategy. Extensive experiments show\nthat NetBooster consistently outperforms state-of-the-art tiny deep learning\nsolutions.",
  "text": "NetBooster: Empowering Tiny Deep Learning By\nStanding on the Shoulders of Deep Giants\nZhongzhi Yu1, Yonggan Fu1, Jiayi Yuan2, Haoran You1, Yingyan (Celine) Lin1\n1Georgia Institute of Technology, 2Rice University\n{zyu401, yfu314, hyou37, celine.lin}@gatech.edu, jy101@rice.edu\nAbstract—Tiny deep learning has attracted increasing attention\ndriven by the substantial demand for deploying deep learning\non numerous intelligent Internet-of-Things devices. However, it\nis still challenging to unleash tiny deep learning’s full potential\non both large-scale datasets and downstream tasks due to the\nunder-fitting issues caused by the limited model capacity of tiny\nneural networks (TNNs). To this end, we propose a framework\ncalled NetBooster to empower tiny deep learning by augmenting\nthe architectures of TNNs via an expansion-then-contraction\nstrategy. Extensive experiments show that NetBooster consistently\noutperforms state-of-the-art tiny deep learning solutions.\nIndex Terms—Tiny Neural Networks, Efficient Deep Learning\nI. INTRODUCTION\nTiny deep learning, which aims to develop tiny neural\nnetworks (TNNs) featuring much-reduced network sizes along\nwith lower memory and computational costs, has emerged\nas a promising direction to enable deep learning’s wider\nreal-world applications in resource-constrained Internet-of-\nThings (IoT) devices [17] and has attracted an increasingly\ngrowing interest from both industry and academia [3], [17].\nIn particular, existing tiny deep learning works strive to\nimprove the achievable accuracy-efficiency trade-off of TNNs\nby either designing novel efficient network architectures [17],\n[26] or compressing a large deep neural network (DNN) to\nreduce their network redundancy [9]. However, the achieved\naccuracy-efficiency trade-off of existing TNN works is still\nfar from satisfactory for many IoT emerging applications [3].\nSpecifically, we summarize that the constraints to the achievable\naccuracy-efficiency trade-offs of TNNs stem from two factors:\nConstraint 1: It is challenging for TNNs to learn complex but\nrepresentative features [3] and achieve satisfactory accuracy\non commonly used large-scale datasets (e.g., ImageNet), and\nConstraint 2: TNNs’ limited accuracy on large-scale datasets\nfurther hinders TNN-based solutions from leveraging the\nwidely adopted pretrain-then-finetune paradigm for real-world\ndownstream tasks.\nIn parallel, it has recently been recognized that a dedicated\ntraining recipe can boost the accuracy of TNNs [3], although\nthis area remains still under-explored. Unlike DNN training,\nwhich requires techniques like data augmentation [4] and/or\nregularization [10], [27] to alleviate the over-fitting issue, a\nrecent study [3] has shown that the small network capacity\nof TNNs makes them more prone to the under-fitting issue.\nSpecifically, due to TNNs’ limited ability to learn complex\nThis work was supported by the NSF SCH program (Award number:\n1838873) and NSF NIH program (Award number: R01HL144683).\nMFLOPs\nAccuracy (%)\n+1.4%\n+1.3%\n+2.6%\n-0.5%\n-0.3%\n-0.3%\nVanilla, r=224\n+1.3%\n+0.2%\nEpochs\nVanilla, r=144\nNetBooster, r=144\nNetBooster\nDropBlock\nVanilla\n(a)\n(b)\nFig. 1: (a) Constraint 1: TNN training suffers from under-\nfitting issues. When training MobileNetV2 [26] on ImageNet,\nregularization techniques (e.g., DropBlock [10]) even lead to\ninferior accuracy compared with vanilla training. Our proposed\nNetBooster can boost TNNs’ accuracy by increasing its capacity\nduring training. (b) Constraint 2: Inadequately trained TNNs\ncannot learn complex features and thus suffer from limited\ndownstream task accuracy. Finetuning ImageNet pretrained\nvanilla MobileNetV2-35 with a resolution of 224 × 224 and\n144 × 144, respectively, on the CIFAR-100 dataset for even\nfour times more epochs (i.e., 600 epochs) still cannot improve\nthe achievable accuracy. Our proposed NetBooster can boost\nTNNs’ accuracy by inheriting pretrained deep giants’ learned\ncomplex features.\nfeatures, extensively augmented training data or a heavily\nregularized training process can hurt the achievable accuracy\nof TNNs on large-scale datasets (i.e., Constraint 1), as shown\nin Fig. 1 (a). The lack of learned complex and representative\nfeatures in the pretrained TNNs further limits the achievable\naccuracy of downstream tasks, which cannot be mitigated by\nadditional training epochs (i.e., Constraint 2), as shown in\nFig. 1 (b).\nTo narrow the gap between the increasing demand for more\npowerful TNNs in real-world applications and the lack of\neffective TNN training schemes, we aim to develop a technique\nthat can boost the achievable task accuracy of TNNs, while\npreserving their appealing efficiency, by empowering TNNs’\nlearned features. In particular, this work makes the following\ncontributions:\n• To the best of our knowledge, we are the first to discover\nand promote a new paradigm of training TNNs to boost\ntheir achievable accuracy via constructing a competent\ndeep giant using compound network augmentation (i.e.,\naugmenting both width and depth dimensions of the\ngiven TNNs), which is simple, effective, and generally\napplicable.\n• By leveraging the above discovery, we propose a TNN\narXiv:2306.13586v1  [cs.LG]  23 Jun 2023\ntraining framework, dubbed NetBooster, that alleviates\nTNNs’ under-fitting issue during training, boosting their\nachievable accuracy while preserving their original net-\nwork complexity and thus inference efficiency. Specifi-\ncally, NetBooster incorporates a two-step expansion-then-\ncontraction training strategy: Step-1: Network Expansion\nconstructs an expanded deep giant by converting some\nlayers of the original TNN into multi-layer blocks,\nfacilitating the learning of more complex features by\nleveraging the corresponding deep giant counterpart, which\nequips the original TNN with an initial state already\npossessing sufficient knowledge, and Step-2: Progressive\nLinearization Tuning (PLT) then reverts the deep giant\nback to the original TNN’s structure by removing the\nnon-linear layers from the expanded blocks and then\ncontracting them.\n• We make heuristic efforts to empirically investigate the\noptimal setting for effectively boosting the achievable\naccuracy of TNNs when implementing Network Expansion\nin NetBooster. Specifically, we address the following\nquestions: Q1. What kind of block to use for expansion,\nQ2. Where to expand within a TNN, and Q3. How to\ndetermine the expansion ratio.\n• Extensive experiments and ablation studies on two tasks,\nfour networks, and seven datasets demonstrate that\nNetBooster consistently achieves a non-trivial accuracy\nboost (e.g., 1.3% ∼2.6%) compared to state-of-the-art\n(SOTA) TNNs on the ImageNet dataset and up to 4.7%\nhigher accuracy on various downstream tasks, while still\nmaintaining the original TNNs’ inference efficiency.\nII. RELATED WORKS\nA. Tiny Neural Network\nTiny deep learning aims to develop TNNs with reduced\nnetwork sizes, lower memory and computational costs, and\nacceptable accuracy, enabling deep learning-powered solutions\nin resource-constrained IoT devices. Existing techniques to-\nwards fulfilling the goal of tiny deep learning can mostly be\ncategorized into two trends. One trend is to design novel TNN\narchitectures by leveraging either human expertise [26] or\nautomated tools, e.g., neural architecture search [2]. The other\ntrend is to make use of compression techniques, including\npruning [19], quantization [9], dynamic inference [29], to\nfurther reduce network complexity on top of existing TNN\narchitectures.\nIn this work, we propose to pursue a tiny deep learning\nsolution that boosts TNNs’ accuracy-efficiency trade-off from\nan underexplored and orthogonal direction: how to train TNNs\nto unleash their achievable accuracy more effectively. To\nthe best of our knowledge, the only pioneering work that\nfocuses on a similar direction is NetAug [3], which proposes\nto augment TNNs from the width dimension by introducing a\nwider supernet to assist training and then directly remove the\nsupernet during inference. In contrast, NetBooster proposes to\nfirst expand a TNN from both the depth and width dimensions\nto create a competent deep giant during TNN training, and\nthen gradually contract it back to the original structure, instead\nof directly removing expanded parts, to avoid unrecoverable\ninformation loss that could result in nontrivial accuracy drops.\nB. Data Augmentation and Regularization\nData augmentation and regularization techniques have been\nproposed to alleviate the over-fitting issue associated with large-\nscale DNNs in order to boost their network generalization\nability and thus the achievable accuracy. Specifically, data\naugmentation techniques focus on manipulating the input data\nsamples [5], while regularization techniques focus on the\nnetwork aspect and randomly drop different components from\nthe network [27] during training.\nHowever, a recent study [3] and Fig. 1 (a) have shown that\nTNN training suffers from under-fitting instead of over-fitting.\nAs a result, existing data augmentation and regularization\ntechniques are unable to fully unleash the potential of TNNs.\nC. Knowledge Distillation\nKnowledge distillation (KD) aims to transfer the already\nlearned knowledge from a larger teacher network to a smaller\nstudent network [12]. Instead of relying on a teacher network\nto provide guidance during training, NetBooster aims to inherit\nthe learned features from the expanded deep giants to achieve\nhigher accuracy than the original TNNs. As such, our proposed\nNetBooster is orthogonal to KD, and when combined with KD,\nit is expected to further enhance the performance of TNNs.\nD. Transfer Learning\nMotivated by DNNs’ strong feature extraction capability,\ntransfer learning [7] has become a widely used paradigm for\ntransferring knowledge across different domains [21]. Despite\nthe extensive efforts to boost transferability, larger pretrained\nnetworks are commonly believed to have better transferability\ndue to their representative and generalizable features. Our\nproposed NetBooster aims to empower TNNs with high-\nquality features and thus better transferability by leveraging\nTNNs’ corresponding deep giants through compound network\naugmentation.\nIII. THE PROPOSED NETBOOSTER FRAMEWORK\nA. Motivations and Inspirations\nThe key challenge for TNN training. Due to the lack of\nsufficient network capacity, TNNs tend to suffer from severe\nunder-fitting issues when being trained on large-scale datasets\n(e.g., ImageNet), limiting their ability to learn complex but\nrepresentative features [3] and further hindering their achievable\naccuracy on downstream tasks.\nInspirations for our works. Recent works have demon-\nstrated that overparameterization during training can lead\nto improved achievable accuracy, while network complexity\nduring inference can be reduced without adversely affecting\naccuracy [18]. Various techniques for compressing DNNs have\nalso supported this observation. For example, the pruning\nmethod [19] preserves the original dense network during\ntraining and then removes redundant neurons for inference.\nPointwise Conv\nNetwork Expansion on Large Dataset\nProgressive Linearization Tuning on the Target Dataset \nAct.\nAct.\nPointwise Conv\nPointwise Conv\nDepthwise Conv\nPointwise Conv\nPointwise Conv\nAct.\nPointwise Conv\nPointwise Conv\nDepthwise Conv\nPointwise Conv\nAct.\nPointwise Conv\nAct.\nPointwise Conv\nPointwise Conv\nDepthwise Conv\nPointwise Conv\nAct.\nContract\nLayer i\nLayer i-1\nLayer i+1\nTiny Neural\nNetworks\nLayer i\nLayer i-1\nLayer i+1\nTiny Neural\nNetworks\nExpand\nNon-Linear Func.\nLinear Func.\nNon-linearity Removal\nLayer i+1\nLayer i-1\nLayer i+1\nLayer i+1\nLayer i-1\nLayer i-1\nFig. 2: An overview of the proposed NetBooster framework.\nIn NetBooster, we augment the original TNN from both depth\nand width dimensions. Specifically, we uniformly select layers\nfrom the original TNN and expand them into inverted residual\nblocks [26] to formulate the deep giant, helping to learn\ncomplex features. Then in PLT, we progressively decay the\nnon-linear activation functions within the expanded inverted\nresidual blocks to an identity mapping function and contract the\nexpanded blocks back to the corresponding layers to maintain\nthe original TNN’s structure and inference efficiency.\nNevertheless, the aforementioned methods only focus on\noverparameterization from the width dimension. At the same\ntime, existing work [22] shows that DNNs with varying depth\nand width tend to learn different features, urging the need to\nintroduce overparameterization into both dimensions. Thus,\nif we can equip a given TNN (e.g., original TNN) with\ncomprehensive overparameterization during training, and then\nrestore the original TNN’s structure during inference, the\nunder-fitting issue can be effectively mitigated to achieve more\naccurate yet efficient TNN inference. This has inspired us to\ndesign a principled expansion-then-contraction methodology by\nfirst expanding the original TNN to a more overparameterized\nnetwork (e.g., deep giant) for better feature learning and then\ncontracting it back to the original structure to preserve its\nefficiency.\nB. Overview\nImplementation of expansion-then-contraction. Given the\nexpansion-then-contraction principle, there are several potential\nimplementations. Inspired by the success of RepVGG [6],\nwhich shows that parallel branches can be merged thanks\nto their linearity, we hypothesize that if we can properly\nremove the non-linear activation functions between layers, the\nconsecutive layers can also be merged via a linear combination.\nFortunately, recent works show that some of the activation\nfunctions can be safely removed from the network without\nhurting the task accuracy [13]. This motivates us to propose our\nexpansion-then-contraction-based NetBooster training frame-\nwork, which adopts two steps: Step-1: Network Expansion,\nwhere we augment the original TNN from both depth and width\ndimensions during training to construct the corresponding deep\ngiant by replacing some layers in the original TNN with multi-\nlayer blocks, aiming to increase the original TNN’s capacity\nand alleviate its under-fitting issue during training and thus\nenabling a better feature learning on the large-scale training\ndataset, and Step-2: Progressive Linearization Tuning (PLT),\nwhere we progressively remove the non-linearity inside the\nexpanded blocks on the target dataset. After the non-linear\nlayers inside the blocks are removed, we contract the expanded\ndeep giant back to the original TNN at the end of training to\ninherit the learned features while ensuring the boosted accuracy\ndoes not come with additional inference overhead.\nTechnical challenges to achieve NetBooster. While the\naforementioned principle sounds straightforward, implementing\nsuch a training pipeline is non-trivial. In Step-1, naively\nexpanding all layers with a high expansion ratio can lead\nto an excessively large network which is difficult to train. To\nenable a practical and effective expansion strategy, at least\nthe following three questions need to be addressed: Q1: what\nblocks to insert? What kind of blocks we should use to expand\nthe original TNN, Q2: where to expand? How to find a set of\ntarget layers to be expanded, and Q3: how to determine the\nexpansion ratio? To what extent we should expand the target\nlayers. In Step-2, how to contract the deep giant expanded\nfrom both width and depth dimensions while preserving the\nlearned knowledge of the deep giant is still an open question.\nDespite the method proposed in [6] can merge the parallel\nconvolution layers into one single layer via linear combination,\nthe non-linear activation layers between sequentially connected\nconvolution layers make it impossible to merge convolution\nlayers along the depth dimension directly. We next elaborate\non our proposed solutions to tackle the above challenges and\nthe design of each step.\nC. Step 1: Network Expansion\nThe network expansion step aims to increase the capacity of\nthe original TNN, transforming them into a more powerful deep\ngiant. This boosts their ability to learn complex and representa-\ntive features from large-scale training datasets, improving task\naccuracy and transferability. To answer the questions raised in\nSec. III-B, we propose the following criteria when expanding\nthe network:\na. Structural consistency: To guarantee that applying Net-\nBooster does not change the original TNN’s structure\nfor inference, each expansion block needs to be able\nto be contracted back to the original single layer via\nlinear combination in the PLT step. Thus, for the network\nexpansion step, the receptive field of each expansion block\nshould be equal to that of the original layer.\nb. Sufficient capacity: Motivated by the findings in [3],\nwe aim to alleviate the under-fitting issue and ease the\nlearning process by creating a deep giant with increased\ncapacity. Thus, we should sufficiently expand the original\nTNN from multiple positions and dimensions (i.e., expand\nwidth with increased expansion ratios and depth by\ninserting multiple layers).\nc. Effective feature inheritance: In addition to having suffi-\ncient capacity, it is equally important to effectively inherit\nthe learned features of the deep giant. As suggested in [20],\nexcessively large networks tend to learn significantly\ndifferent feature distribution from that of small networks,\nwhich can not only forbid small networks from inheriting\nbut even hurt small networks’ task accuracy. Thus, (1)\nthe complexity gap between the original TNN and its\nexpanded deep giant should not be too large and (2) the\nselected layers to be expanded should contain sufficient\nparameters to ensure an effective knowledge inheritance\nfrom the deep giant.\nBased on the above criteria, we answer the questions raised\nin Sec. III-B below:\nQ1. What kind of block to use? We select the type of inserted\nblocks from a pool of well-established DNN building blocks\n(e.g., the basic and bottleneck blocks in ResNet [11] and the\ninverted residual block in MobileNetV2 [26]). To maintain\nstructure consistency (criteria a.), we eliminate the basic block\nas it stacks two layers with large convolution kernels, leading to\na receptive field larger than that of the original layer. To narrow\ndown the complexity gap for effective feature inheritance\n(criteria c.), we select the inverted residual block over the\nbottleneck block.\nQ2. Where to expand? The achievable accuracy of Net-\nBooster is limited by a trade-off between increasing the network\ncapacity by constructing a larger deep giant (criteria b.) and\nimproving the feature inheritance effectiveness by narrowing\ndown the capacity gap between the deep giant and original\nTNN (criteria c.). A simple but effective way to push the\naforementioned trade-off further is to consider the knowledge\ninheritance effectiveness from a more fine-grained granularity\n(i.e., layer-wise instead of model-wise). Specifically, multiple\nlayers can have a better representation ability than a single\nlayer. Thus, the expanded block’s learned complex features can\nbe more effectively inherited by distributing them to multiple\nadjacent layers in the original TNN. To this end, we propose\nto uniformly select layers to be expanded from the original\nTNN, which can guarantee that there are sufficient layers to\ninherit learned features from each of the expanded blocks.\nQ3. How to determine the expansion ratio? Similar to Q2,\nthe selection of the adopted expansion ratio has to trade-off\nbetween the network capacity (criteria b.) and the effectiveness\nof knowledge inheritance (criteria c.). However, thanks to the\nproposed uniform expansion strategy in Q2, we empirically\nfind that the commonly used expansion ratio, 6 [26] in the\ninserted inverted residual blocks works well on balancing the\naforementioned two criteria.\nD. Step 2: Progressive Linearization Tuning (PLT)\nThe next step is to recover the original TNN’s structure\non the target dataset while inheriting the knowledge learned\nby the deep giant. Inspired by [6], which proposes to merge\nparallel convolution layers into one single layer, we find that\nsequentially connected layers can also be merged via linear\ncombinations by properly removing the non-linear operations\nbetween them. To achieve this, we propose PLT to progressively\nremove the non-linearity from the expanded deep giant and\nthen contract it back to the original TNN during finetuning on\nthe target dataset.\nMotivating observation. Non-linearity has been considered\na key enabler for the promising performance of DNNs and\nmost existing works use the combination of convolution and\nnon-linear activation layers as a basic design unit. In parallel,\nrecent works [13] have shown that non-linearity within DNNs\ncan be highly redundant for inference, a large portion of\nelement-wise non-linear activation functions can be removed\nfrom a DNN, and the complex features learned from the original\nTNN during training can be largely preserved. Inspired by the\nrevolution from element-wise pruning to structure pruning, we\naim to step further and remove the non-linearity in a\nstructured manner (i.e., layer-wisely).\nNon-linearity removal. We propose to transform the expanded\ndeep giant back to the original TNN meanwhile preserve the\nlearned features by slowly decaying the non-linear activation\nfunctions.\nWithout loss of generality, we take the ReLU activation\nfunction as an example as it is the most commonly adopted\nactivation function in TNNs, and the following discussion can\nalso be extended to other activation functions like ReLU6. Here\nthe ReLU function is defined as:\nYl = max(0, Xl),\n(1)\nwhere Xl and Yl are the input and output of layer l, respectively.\nWe change the formulation of ReLU to the following format,\nYl = max(αlXl, Xl),\n(2)\nwhere 0 < αl < 1 is the slope parameter to manipulate the non-\nlinearity of the corresponding activation layer. When αl = 0,\nit is exactly the ReLU function. When αl = 1, the activation\nfunction is decayed to an identity mapping.\nGiven a list L of non-linear activation layers to be removed,\nwe gradually increase αl′ for l′ ∈L from 0 to 1 in Ed epochs,\nthe value of αl′ is uniformly increased in each iteration. When\nαl′ = 1, Eq. 2 is an identity mapping, and thus the non-linearity\nis removed.\nExpanded block contraction. With the non-linear activation\nlayers removed, the remaining layers can be contracted into\none layer via simple linear combinations.\nFormulation: Without loss of generality, we take two con-\nvolution layers as an example. Given the input to the first\nlayer X ∈Rh1×w1×c1, the output Y ∈Rh3×w3×c3, as well\nas the kernels of two layers K1 ∈Rk1×k1×c1×c2 and K2 ∈\nRk2×k2×c2×c3, where k1/2 are the kernel sizes, h1/2/3, w1/2/3\nare the heights and widths of corresponding feature maps,\nc1/2/3 are the channel numbers. The overall functionality of\nthe two convolution layers can be formulated as\nYp,q,o =\nk−1\n∑\ni=0\nk−1\n∑\nj=0\nc1−1\n∑\nm=0\nXp−i,q−j,mKi,j,m,o,\n(3)\nwhere Ki,j,m,o =\nsh\n∑\ns=sl\nth\n∑\nt=tl\nc2−1\n∑\nn=0\nK1\ni−s,j−t,m,nK2\ns,t,n,o,\n(4)\nwhere k = k1 +k2 −1, sl = max(0, i−k1 +1), sh = min(k2 −\n1, i), tl = max(0, j −k2 + 1) and th = min(k2 −1, j).\nRemark: It is worth noting that different expansion ratios of\nthe inserted inverted residual block will result in the same\ncomputational cost after contraction since the input and output\nTABLE I: Benchmarking on ImageNet. ’r’ is the input\nresolution.\nNetwork\nFLOPs\nParams\nTraining Method\nAccuracy\nMobileNetV2-Tiny\n(r=144)\n23.5M\n0.75M\nVanilla\n51.2\nRocketLaunch [31]\n51.8\ntf-KD [30]\n51.9\nRCO-KD [14]\n52.6\nNetAug [3]\n53.0\nNetBooster\n53.7\nMCUNet\n(r=176)\n81.8M\n0.74M\nVanilla\n61.4\nNetAug [3]\n62.5\nNetBooster\n62.8\nMobileNetV2-50\n(r=160)\n50.2M\n1.95M\nVanilla\n61.4\nNetAug [3]\n62.5\nNetBooster\n62.7\nMobileNetV2-100\n(r=160)\n154.1M\n3.47M\nVanilla\n69.6\nNetAug [3]\n70.5\nNetBooster\n70.9\nchannels after contraction are always equal to the input channel\nof the first layer and the output channel of the last layer,\nrespectively, regardless of intermediate channel numbers (i.e.,\nc2).\nIV. EXPERIMENTS\nA. Experiments Setup\nTasks, datasets, and networks We consider two tasks,\nincluding image classification and object detection, with seven\ndatasets to provide a thorough evaluation of NetBooster.\nSpecifically, to evaluate NetBooster’s performance in alleviating\nthe under-fitting issue to achieve a higher accuracy on the\nlarge-scale dataset, we consider the ImageNet dataset. To\nevaluate how deep giant’s learned representation helps with\ndownstream tasks, we consider image classification tasks on\nfive datasets, including CIFAR-100, Cars [15], Flowers102 [23],\nFood101 [1], and Pets [24]. We also evaluate NetBooster on\nthe downstream object detection task with the Pascal VOC\ndataset [8]. We consider four networks, including MobileNetV2-\n100/50/Tiny [26], and a neural architecture searched hardware\nfriendly network MCUNet [17].\nBaselines. We benchmark the proposed NetBooster over\nfive baselines including networks trained with standard vanilla\ntraining, a series of SOTA KD algorithms (i.e., tf-KD [30],\nRCO-KD [14], and RocketLaunch [31]), and NetAug [3], which\nis a pioneering work in boosting TNN training performance.\nExpansion strategy. We uniformly expand 50% of blocks\nin the original TNN. To expand each block, we replace the first\npointwise convolution layer with an inverted residual block\nwith an expansion ratio of 6. The kernel size of the depthwise\nconvolution layer in the inserted inverted residual block is set\nto 1 to make the inserted block’s receptive field the same as\nthe pointwise convolution.\nTraining settings. We develop our training settings based on\nthe commonly adopted settings. Specifically, when evaluating\nNetBooster’s performance in improving TNNs’ accuracy on\nImageNet dataset, we follow [3] to train the deep giant for\n160 epochs using SGD optimizer with a batch size of 1024,\nan initial learning rate of 0.2 and cosine anneal learning rate\nschedule. In PLT, we set Ed = 40, and further finetune for 110\nepochs. For downstream tasks, we use the ImageNet pretrained\ndeep giant as the starting point and develop our training recipe\nTABLE II: Benchmarking on downstream image classification\ndatasets. ’r’ is the input resolution.\nNetwork\nTraining Method\nCIFAR-100\nCars\nFlowers102\nFood101\nPets\nMobileNetV2-Tiny\n(r=144)\nVanilla\n74.07\n76.18\n90.01\n75.43\n78.30\nNetBooster\n75.46\n80.93\n90.53\n75.96\n78.90\nMobileNetV2-35\n(r=160)\nVanilla\n76.08\n78.36\n90.63\n76.80\n80.64\nVanilla + KD\n76.38\n77.47\n91.41\n77.02\n82.44\nNetBooster\n76.66\n80.91\n91.16\n77.26\n80.92\nNetBooster + KD\n77.15\n83.36\n92.68\n77.81\n83.37\nTABLE III: Benchmarking on object detection tasks with\nPascal VOC dataset with MobileNetV2-35 at 416 resolution.\nMethod\nVanilla\nNetAug\nNetBooster\nAP50\n60.8\n62.4\n62.6\non CIFAR-100 based on [28], on Cars, Flowers102, Food101,\nand Pets based on [25], and on Pascal VOC based on [3]. In\nall experiments on downstream tasks, we assign Ed to be 20%\nof the total tuning epochs in PLT.\nB. Easing Constraint 1: Benchmarking on Large-scale Dataset\nTo evaluate whether the proposed NetBooster can help\nTNNs to learn the complex features of the large-scale dataset\nand thus improve accuracy, we benchmark NetBooster on the\nImageNet dataset with vanilla training, NetAug, and various KD\nalgorithms. As shown in Table I, NetBooster achieves 1.3% ∼\n2.5% accuracy improvements over the vanilla training, showing\nits strong ability in boosting the TNNs’ accuracy by standing\non the shoulder of deep giants generated by NetBooster.\nCompared with the KD baselines, our proposed NetBooster\nachieves 0.9% ∼1.1% accuracy improvement without guidance\nfrom the teacher network (Assemble-ResNet50 [16]), suggest-\ning that Network Expansion in NetBooster can equip the deep\ngiant with sufficient capacity to learn complex features at\nleast comparable with the large teacher DNN used in the KD\nbaselines and the learned features can be effectively inherited\nfrom the PLT step.\nCompared with NetAug, which is a pioneering work fo-\ncusing on a similar scenario as NetBooster, NetBooster also\nachieves superior accuracy over NetAug, suggesting the multi-\ndimensional Network Expansion and the PTL for features\ninheritance is more effective than the network width expansion\nand directly dropping augmented neuron adopted in NetAug.\nC. Easing Constraint 2: Benchmarking on Downstream Tasks\nTo evaluate whether the learned complex and representative\nfeatures in the deep giant from the large-scale dataset can\nfurther help the original TNN to achieve better accuracy on\ndownstream tasks, we first evaluate NetBooster’s performance\nwhen transferring the ImageNet pretrained deep giant to five\nrepresentative downstream image classification datasets with\nPLT. As shown in Table II, compared with vanilla training,\nNetBooster achieves 0.46% ∼4.75% accuracy improvement,\nshowing that the features learned by the deep giant are effec-\ntively inherited after PLT. It is worth noting that NetBooster\nis also orthogonal to KD, applying KD on top of NetBooster\ncan lead to another 0.49% ∼2.45% accuracy boost over using\nNetBooster alone.\nWe further evaluate NetBooster’s performance when trans-\nferring to the Pascal VOC object detection task. As shown\nTABLE IV: Ablation study on what kind of block to insert.\nInserted Block Type\nExpanded Acc.\nFinal Acc.\nVanilla\n-\n51.20\nInverted Residual\n54.90\n53.70\nBasic Block\n54.52\n53.41\nBottleneck Block\n55.23\n53.62\nTABLE V: Ablation study on which block to expand.\nExpansion\nExpanded\nExpanded Acc.\nFinal Acc.\nFLOPs\nParams\nVanilla\n29.4M\n0.75M\n-\n51.20\nExpand First 8\n65.0M\n0.83M\n51.46\n51.50\nExpand Middle 8\n49.6M\n0.93M\n52.98\n52.62\nExpand Last 8\n51.2M\n1.25M\n53.90\n52.47\nUniform Expand 8\n63.9M\n0.99M\n54.90\n53.70\nin Table III, NetBooster achieves 1.8 and 0.2 higher AP50\ncompared with vanilla training and NetAug, respectively. This\nproves that NetBooster can be considered as a general method\nto boost TNNs’ performance across various tasks.\nD. Validating Expansion Strategy\nWe validate our answer to each question in Sec. III-C by\nvalidating the impact of replacing our proposed strategy with\nalternatives when training MobileNet-Tiny on the ImageNet\ndataset with an input resolution of 144.\nQ1. What kind of block to use: We ablate the impact of\nexpanding with different kinds of blocks and report the results\nin Table IV. Expanding with inverted residual blocks leads\nto slightly better results (0.08% ∼0.29%). It shows (1) the\nNetBooster framework can robustly boost TNNs’ performance,\nand (2) inserting inverted residual blocks is an effective choice.\nQ2. Where to expand: We ablate different expansion loca-\ntions’ impacts and report our findings in Table V. We observe\nthat uniformly expanding the model achieves a 1.08% ∼2.20%\nhigher accuracy compared with excessively expanding the\nfirst/middle/last part of the network, proving the necessity\nto expand the model uniformly.\nQ3. How to determine expansion ratio: We ablate different\nselections of expansion ratios in the inserted inverted residual\nblocks and report the results in Table. VI. We observe that\nNetBooster with commonly used expansion ratios (i.e., 4 ∼\n6) consistently improves the TNNs’ accuracy, further proving\nNetBooster’s robustness to hyperparameter selection.\nV. CONCLUSION\nIn this paper, we discover and promote a new paradigm\nfor training TNNs to empower their achievable accuracy via\naugmenting both dimensions of TNNs (i.e., depth and width)\nduring training. Furthermore, we propose a framework dubbed\nNetBooster, which is dedicated to boosting the accuracy of\nSOTA TNNs by using an expand-then-contract training strategy\nto alleviate TNNs’ under-fitting issues. Finally, we make\nheuristic efforts to empirically explore what/when/where to\naugment when training TNNs with our proposed NetBooster.\nExtensive experiments show that NetBooster consistently leads\nto a nontrivial accuracy boost (e.g., 1.3% ∼2.5%) on top\nTABLE VI: Ablation study on the expansion ratio.\nExpansion ratio\n2\n4\n6\n8\nFinal Acc.\n52.94\n53.52\n53.70\n52.56\nof SOTA TNNs on ImageNet and as much as 4.7% higher\naccuracy on various downstream tasks, while maintaining their\ninference efficiency.\nREFERENCES\n[1] L. Bossard et al., “Food-101–mining discriminative components with\nrandom forests,” in ECCV.\nSpringer, 2014, pp. 446–461.\n[2] H. Cai, “Proxylessnas: Direct neural architecture search on target task\nand hardware,” arXiv preprint, 2018.\n[3] H. Cai et al., “Network augmentation for tiny deep learning,” arXiv\npreprint, 2021.\n[4] E. D. Cubuk, “Randaugment: Practical automated data augmentation\nwith a reduced search space,” in CVPR Workshops, 2020, pp. 702–703.\n[5] E. D. Cubuk et al., “Autoaugment: Learning augmentation policies from\ndata,” arXiv preprint, 2018.\n[6] X. Ding et al., “Repvgg: Making vgg-style convnets great again,” in\nCVPR, 2021, pp. 13 733–13 742.\n[7] J. Donahue et al., “Decaf: A deep convolutional activation feature for\ngeneric visual recognition,” in ICML.\nPMLR, 2014, pp. 647–655.\n[8] M. Everingham et al., “The pascal visual object classes (voc) challenge,”\nIJCV, vol. 88, no. 2, pp. 303–338, 2010.\n[9] Y. Fu et al., “Double-win quant: Aggressively winning robustness of\nquantized deep neural networks via random precision training and\ninference,” in ICML.\nPMLR, 2021, pp. 3492–3504.\n[10] G. Ghiasi et al., “Dropblock: A regularization method for convolutional\nnetworks,” arXiv preprint, 2018.\n[11] K. He et al., “Deep residual learning for image recognition,” in CVPR,\n2016, pp. 770–778.\n[12] G. Hinton et al., “Distilling the knowledge in a neural network,” arXiv\npreprint, 2015.\n[13] N. K. Jha et al., “Deepreduce: Relu reduction for fast private inference,”\nin ICML.\nPMLR, 2021, pp. 4839–4849.\n[14] X. Jin et al., “Knowledge distillation via route constrained optimization,”\nin ICCV, 2019, pp. 1345–1354.\n[15] J. Krause et al., “3d object representations for fine-grained categorization,”\nin ICCV workshops, 2013, pp. 554–561.\n[16] J. Lee et al., “Compounding the performance improvements of assembled\ntechniques in a convolutional neural network,” arXiv preprint, 2020.\n[17] J. Lin et al., “Mcunet: Tiny deep learning on iot devices,” arXiv preprint,\n2020.\n[18] S. Liu et al., “Do we actually need dense over-parameterization? in-time\nover-parameterization in sparse training,” in ICML, 2021.\n[19] Z. Liu et al., “Learning efficient convolutional networks through network\nslimming,” in ICCV, 2017, pp. 2736–2744.\n[20] S. I. Mirzadeh et al., “Improved knowledge distillation via teacher\nassistant,” in AAAI, vol. 34, no. 04, 2020, pp. 5191–5198.\n[21] R. Mormont et al., “Comparison of deep transfer learning strategies for\ndigital pathology,” in CVPR workshops, 2018, pp. 2262–2271.\n[22] T. Nguyen et al., “Do wide and deep networks learn the same things?\nuncovering how neural network representations vary with width and\ndepth,” arXiv preprint, 2020.\n[23] M.-E. Nilsback et al., “Automated flower classification over a large\nnumber of classes,” in ICVGIP.\nIEEE, 2008, pp. 722–729.\n[24] O. M. Parkhi et al., “Cats and dogs,” in CVPR.\nIEEE, 2012, pp.\n3498–3505.\n[25] H. Salman et al., “Do adversarially robust imagenet models transfer\nbetter?” NeurIPS, vol. 33, pp. 3533–3545, 2020.\n[26] M. Sandler et al., “Mobilenetv2: Inverted residuals and linear bottlenecks,”\nin CVPR, 2018, pp. 4510–4520.\n[27] N. Srivastava et al., “Dropout: a simple way to prevent neural networks\nfrom overfitting,” JMLR, vol. 15, no. 1, pp. 1929–1958, 2014.\n[28] Y. Tian et al., “Contrastive representation distillation,” arXiv preprint,\n2019.\n[29] Z. Yu et al., “Mia-former: Efficient and robust vision transformers via\nmulti-grained input-adaptation,” arXiv preprint, 2021.\n[30] L. Yuan et al., “Revisiting knowledge distillation via label smoothing\nregularization,” in CVPR, 2020, pp. 3903–3911.\n[31] G. Zhou et al., “Rocket launching: A universal and efficient framework\nfor training well-performing light net,” in AAAI, 2018.\n",
  "categories": [
    "cs.LG",
    "cs.DC"
  ],
  "published": "2023-06-23",
  "updated": "2023-06-23"
}