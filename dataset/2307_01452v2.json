{
  "id": "http://arxiv.org/abs/2307.01452v2",
  "title": "Causal Reinforcement Learning: A Survey",
  "authors": [
    "Zhihong Deng",
    "Jing Jiang",
    "Guodong Long",
    "Chengqi Zhang"
  ],
  "abstract": "Reinforcement learning is an essential paradigm for solving sequential\ndecision problems under uncertainty. Despite many remarkable achievements in\nrecent decades, applying reinforcement learning methods in the real world\nremains challenging. One of the main obstacles is that reinforcement learning\nagents lack a fundamental understanding of the world and must therefore learn\nfrom scratch through numerous trial-and-error interactions. They may also face\nchallenges in providing explanations for their decisions and generalizing the\nacquired knowledge. Causality, however, offers a notable advantage as it can\nformalize knowledge in a systematic manner and leverage invariance for\neffective knowledge transfer. This has led to the emergence of causal\nreinforcement learning, a subfield of reinforcement learning that seeks to\nenhance existing algorithms by incorporating causal relationships into the\nlearning process. In this survey, we comprehensively review the literature on\ncausal reinforcement learning. We first introduce the basic concepts of\ncausality and reinforcement learning, and then explain how causality can\naddress core challenges in non-causal reinforcement learning. We categorize and\nsystematically review existing causal reinforcement learning approaches based\non their target problems and methodologies. Finally, we outline open issues and\nfuture directions in this emerging field.",
  "text": "Causal Reinforcement Learning: A Survey\nZhihong Deng\nzhi-hong.deng@student.uts.edu.au\nAustralian Artificial Intelligence Institute, University of Technology Sydney\nJing Jiang\njing.jiang@uts.edu.au\nAustralian Artificial Intelligence Institute, University of Technology Sydney\nGuodong Long\nguodong.long@uts.edu.auu\nAustralian Artificial Intelligence Institute, University of Technology Sydney\nChengqi Zhang\nchengqi.zhang@uts.edu.au\nAustralian Artificial Intelligence Institute, University of Technology Sydney\nAbstract\nReinforcement learning is an essential paradigm for solving sequential decision problems\nunder uncertainty. Despite many remarkable achievements in recent decades, applying rein-\nforcement learning methods in the real world remains challenging. One of the main obstacles\nis that reinforcement learning agents lack a fundamental understanding of the world and\nmust therefore learn from scratch through numerous trial-and-error interactions. They may\nalso face challenges in providing explanations for their decisions and generalizing the ac-\nquired knowledge. Causality, however, offers notable advantages by formalizing knowledge\nin a systematic manner and harnessing invariance for effective knowledge transfer. This\nhas led to the emergence of causal reinforcement learning, a subfield of reinforcement learn-\ning that seeks to enhance existing algorithms by incorporating causal relationships into the\nlearning process. In this survey, we provide a comprehensive review of the literature in\nthis domain. We begin by introducing basic concepts in causality and reinforcement learn-\ning, and then explain how causality can help address key challenges faced by traditional\nreinforcement learning. We categorize and systematically evaluate existing causal reinforce-\nment learning approaches, with a focus on their ability to enhance sample efficiency, advance\ngeneralizability, facilitate knowledge transfer, mitigate spurious correlations, and promote\nexplainability, fairness, and safety. Lastly, we outline the limitations of current research and\nshed light on future directions in this rapidly evolving field.\n1\nIntroduction\n‚ÄúAll reasonings concerning matter of fact seem to be founded on the relation of cause and effect. By means\nof that relation alone we can go beyond the evidence of our memory and senses.\"\n‚ÄîDavid Hume, An Enquiry Concerning Human Understanding.\nHumans possess an inherent capacity to grasp the concept of causality from a young age (Wellman, 1992;\nInagaki & Hatano, 1993; Koslowski & Masnick, 2002; Sobel & Sommerville, 2010). This innate understanding\nempowers us to recognize that altering specific factors can lead to corresponding outcomes, enabling us to\nactively manipulate our surroundings to accomplish desired objectives and acquire fresh insights. A deep\nunderstanding of cause and effect enables us to explain behaviors (Schult & Wellman, 1997), predict future\noutcomes (Shultz, 1982), and use counterfactual reasoning to dissect past events (Harris et al., 1996). These\ncognitive abilities inherently shape human thought and reasoning (Sloman, 2005; Sloman & Lagnado, 2015;\nPearl, 2009b; Pearl & Mackenzie, 2018), forming the basis for modern society and civilization, as well as\npropelling advancements in science and technology (Glymour, 1998).\n1\narXiv:2307.01452v2  [cs.LG]  21 Nov 2023\nWhat is the effect of certain treatments? \nDoes receiving the treatment directly \ncontribute to the patient's recovery, or \nare other unmeasured factors at play?\nDoes changing the size of an object affect \nthe outcome? What about shape and color? \nAre there any invariants across tasks and \nenvironments?\nWhat if I hadn‚Äôt slowed down? \nWould I cause car accidents if I had \nviolated traffic rules or regulations? \nFigure 1: Illustrative examples of causality in reinforcement learning and its impact on decision making.\nIn the pursuit of developing agents with these crucial abilities, reinforcement learning (RL) (Sutton & Barto,\n2018) emerges as a promising path. It involves learning optimal decision-making policies by interacting with\nthe environment to learn from the outcome of certain behaviors. This property makes RL naturally connected\nto causality, as agents can sometimes directly estimate the causal effect for policy-specified interventions.\nIn fact, many important concerns related to causality become pronounced when we examine real-world\napplications of RL.\nConsider the scenarios and the questions presented in Figure 1 where the phrases related to causality are\nhighlighted in orange. In robotic manipulation, RL agents need to understand the effect of altering different\nfactors on the outcomes to avoid learning decisions sensitive to irrelevant factors. Moreover, for effective\nknowledge transfer, agents are expected to leverage invariance across different tasks and environments. In\nmedical scenarios, which often involve observational studies, agents should be aware of unobserved con-\nfounders that may remain hidden in the data generation process, such as socioeconomic status.\nThese\nconfounders can simultaneously affect treatments and outcomes, introducing bias into decision-making by\nerroneously attributing unobserved factors to treatment, resulting in either overestimation or underestima-\ntion of the true treatment effect on outcomes. Additionally, in scenarios like autonomous driving, safety\nconcerns often lead to questions like whether the vehicle would have collided with the leading vehicle if it\nhadn‚Äôt slowed down. Answering such queries necessitates comparing the factual world and a hypothetical\none in which the vehicle had not slowed down. To address this challenge, agents are expected to utilize the\nability of counterfactual reasoning, which empowers them to envision and gain insights from scenarios absent\nfrom the collected data, thus improving the efficiency of learning safe driving policies.\nThese examples underscore the potential of incorporating causality into RL, yet the journey is fraught with\nchallenges. RL agents, lacking a fundamental understanding of the world, typically rely on extensive trial and\nerror to make rational decisions and understand causal relationships among different factors. They may also\nface difficulties in identifying and learning invariant mechanisms from a limited set of environments and tasks.\nMoreover, since traditional RL lacks a built-in capacity for modeling causality, agents may be susceptible to\nspurious correlations, especially in scenarios involving offline learning and partial observability. Addressing\nthese problems necessitates a careful investigation of several critical questions: how to formalize these issues,\nwhat assumptions or prior knowledge are required, how to introduce them in a principled manner, and what\nare the consequences of making incorrect assumptions. Consequently, in recent years, researchers have been\nactively exploring systematic approaches to integrate causality into the realm of reinforcement learning,\ngiving rise to an emerging field known as causal RL.\nCausal RL harnesses the power of causal inference (Pearl, 2009b; Peters et al., 2017), which offers a math-\nematical framework for formalizing the data generation process and reasoning about causality (Sch√∂lkopf\net al., 2021; Kaddour et al., 2022). It is an umbrella term for RL approaches that incorporate additional\nassumptions or knowledge about the underlying causal model to inform decision-making. Modeling causality\nexplicitly not only holds the promise of a principled approach to solving complex decision-making problems\nbut also enhances the transparency and interpretability of the decision-making process. To further elaborate\non the distinctive capabilities of causal RL in addressing the challenges faced by traditional RL, we will delve\ninto a more detailed discussion spanning sections 3 to 6. Note that causal inference, while powerful, is not\n2\na panacea; its effectiveness depends on the quality of assumptions and domain expertise applied. We will\nexplore these nuances further in section 7.1 to provide a balanced understanding of the role and limitations\nof causal RL in addressing the challenges of traditional RL.\nGiven the successful application of causal inference in various domains such as computer vision (Lopez-Paz\net al., 2017; Shen et al., 2018; Tang et al., 2020; Wang et al., 2020b), natural language processing (Wu\net al., 2021; Jin et al., 2021; Feder et al., 2022), and recommender systems (Zheng et al., 2021; Zhang\net al., 2021b; Gao et al., 2022), it is reasonable to expect that causal RL would help resolve core challenges\nfaced by traditional RL methods and tackle new challenges in increasingly complex application scenarios.\nNevertheless, a significant obstacle lies in the lack of a clear and consistent conceptualization of causal\nassumptions and knowledge, as they have been encoded in diverse forms across prior research, tailored to\nspecific problems and objectives. The use of disparate terminologies and techniques makes it challenging\nto understand the essence, implications, limitations, and opportunities of causal RL, particularly for those\nnew to the realms of causal inference and RL. In light of this, this paper aims to provide a comprehensive\nsurvey of causal RL, consolidating the diverse advancements and contributions within this field, thereby\nestablishing meaningful connections and fostering a cohesive understanding.\nOur main contributions to the field are as follows.\n‚Ä¢ We present a comprehensive survey of causal RL, exploring fundamental questions such as its defi-\nnition, motivations, and its improvements over traditional RL approaches. Additionally, we provide\na clear and concise overview of the foundational concepts in both causality research and RL. To\nthe best of our knowledge, this is the first comprehensive survey of causal RL in the existing RL\nliterature 1.\n‚Ä¢ We identify the key challenges in RL that can be effectively addressed or improved by explicitly\nconsidering causality. To facilitate a deeper understanding of the benefits of incorporating causality-\naware techniques, we propose a problem-oriented taxonomy. Furthermore, we conduct a comparative\nanalysis of existing causal reinforcement learning approaches, examining their methodologies and\nlimitations.\n‚Ä¢ We shed light on promising research directions in causal RL. These include advancing theoretical\nanalyses, establishing benchmarks, and tackling specific learning problems. As these research topics\ngain momentum, they will propel the application of causal RL in real-world scenarios.\nHence,\nestablishing a common ground for discussing these valuable ideas in this burgeoning field is crucial\nand will foster its continuous development and success.\n2\nBackground\nTo better understand causal RL, an emerging field that combines the strengths of causality research and RL,\nwe start by introducing the fundamentals of and some common concepts relevant to the two research areas.\n2.1\nA Brief Introduction to Causality\nWe first discuss how to use mathematical language to describe and study causality. In general, there are\ntwo primary frameworks that researchers use to formalize causality: SCMs (structural causal models) Pearl\n(2009a); Glymour et al. (2016) and PO (potential outcome) (Rubin, 1974; Imbens & Rubin, 2015). We focus\non the former in this paper because it provides a graphical methodology that can help researchers abstract\nand better understand the data generation process. It is noteworthy that these two frameworks are logically\nequivalent, and most assumptions are interchangeable.\nDefinition 2.1 (Structural Causal Model). An SCM M is represented by a tuple (V, U, F, P(U)), where\n1We note that Sch√∂lkopf et al. (2021) and Kaddour et al. (2022) discussed causal RL alongside many other research subjects in\ntheir papers. The former mainly studied the causal representation learning problem, and the latter comprehensively investigated\nthe field of causal machine learning. The present study, however, focuses on examining the literature on causal RL and provides\na systematic review of the field.\n3\nCitrus Fruits\nVitamin C\nScurvy\nSCM\n(a)\nChain\nFork\nCollider\n(b)\nFigure 2: (a) The SCM and causal graph for the scurvy example, in which consuming fruits helps to prevent\nscurvy by influencing the intake of vitamin C. (b) The three basic building blocks of causal graphs.\n‚Ä¢ V = {V1, V2, ¬∑ ¬∑ ¬∑ , Vm} is a set of endogenous variables that are of interest in a research problem,\n‚Ä¢ U = {U1, U2, ¬∑ ¬∑ ¬∑ , Un} is a set of exogenous variables that represent the source of stochasticity in the\nmodel and are determined by external factors that are generally unobservable,\n‚Ä¢ F = {f1, f2, ¬∑ ¬∑ ¬∑ , fm} is a set of structural equations that assign values to each of the variables in V\nsuch that fi maps PA(Vi) ‚à™Ui to Vi, where PA(Vi) ‚äÜV\\Vi and Ui ‚äÜU,\n‚Ä¢ P(U) is the joint probability distribution of the exogenous variables in U.\nStructural causal model provides a rigorous framework for examining how relevant features of the world\ninteract. Each structural equation fi ‚ààF specifies the value of an endogenous variable Vi based on an exoge-\nnous variable Ui and PA(Vi). These equations establish the causal links between variables and mathemati-\ncally characterize the underlying mechanisms of the data generation process. If all Ui ‚ààU are independent of\neach other, it follows that each endogenous variable Vi ‚ààV is independent of all its nondescendants, given its\nparents PA(Vi). In this case, we refer to the model as Markovian and PA(Vi) is considered complete, as it\nincludes all the relevant immediate causes of Vi. This assumption is prevalent in human discourse, probably\nbecause it allows us to omit some causes from PA(Vi) (which are aggregated as exogenous variables and\nsummarized by probabilities), forming a useful abstraction of the underlying physical processes that might\nbe overly detailed for practical use (Pearl, 2009b, Chapter 2).\nCausal graph. Each SCM M is associated with a causal graph G = {V, E}, where nodes V represent\nendogenous variables and edges E represent causal relationships determined by the structural equations.\nSpecifically, an edge eij ‚ààE from node Vj to node Vi exists if the random variable Vj ‚ààPA(Vi). In some\ncases, there may be causes that are omitted from PA(Vi) but affect more than one variable in V. These\nomitted variables are referred to as unobserved confounders.\nIn such cases, exogenous variables are not\nindependent of each other, which leads to the loss of Markov property. If we explicitly treat such variables as\nlatent variables and represent them with nodes in the graph, the Markov property is restored (Pearl, 2009b,\nChapter 2). Figure 2a illustrates an SCM and its corresponding causal graph. This example includes three\nbinary endogenous variables - consumption of fruits, intake of vitamin C, and occurrence of scurvy - along\nwith the relevant exogenous variables. In this example, the consumption of fruits does not directly protect\nthe health of sailors from scurvy; instead, it produces an indirect effect by influencing the intake of vitamin\nC. Therefore it is not part of the structural equation that determines sailors‚Äô health. Figure 2b introduces\nthe three fundamental building blocks of the causal graph: chain, fork, and collider. These simple structures\ncan be combined together to create more complex data generation processes.\nProduct decomposition involves representing a complex joint probability distribution P(V) as a product\nof conditional probability distributions that are easier to model and analyze. By applying the chain rule, we\ncan always decompose P(V) of n variables V1, ¬∑ ¬∑ ¬∑ , Vn ‚ààV as a product of n conditional distributions:\nP(V) =\nn\nY\ni=1\nP(Vi|V1, ¬∑ ¬∑ ¬∑ , Vi‚àí1).\n4\nWhile this decomposition method is general, it does not consider the causal relationships within the data.\nTherefore, predecessor variables V1, ¬∑ ¬∑ ¬∑ , Vi‚àí1 are not necessarily the causes of Vi and the conditional proba-\nbility of Vi is not necessarily sensitive to all predecessor variables. Assuming that the data is generated by\na Markovian causal model, the causal Markov condition (Pearl, 2009b, Chapter 1) helps us establish a con-\nnection between causation and probabilities, thereby allowing for a more parsimonious decomposition. This\ncondition states that for every Markovian causal model with a causal graph G, the induced joint distribution\nP(V) is Markov relative to G. Specifically, a variable Vi ‚ààV is independent of any non-descendants given\nits parents PA(Vi) in G. This property enables a structured decomposition along causal directions, which is\nreferred to as the causal factorization (or the disentangled factorization) (Sch√∂lkopf et al., 2021):\nP(V) =\nn\nY\ni=1\nP(Vi|PA(Vi)),\n(1)\nwhere the (conditional) probability distributions of the form P(Vi|PA(Vi)) are commonly referred to as\ncausal Markov kernels (Peters et al., 2017) or causal mechanisms (Sch√∂lkopf et al., 2021). While equation 1\nis not the only method for product decomposition, it is the only one that decomposes P(V) as the product of\ncausal mechanisms. For further information regarding the product decomposition of semi-Markovian models,\nin which the causal graph is acyclic but the exogenous variables are not jointly independent, please refer\nto Bareinboim et al. (2022).\nIntervention. SCMs not only offer a rigorous mathematical framework for studying causal relationships\nbut also facilitate the modeling of external interventions. Specifically, an intervention can be encoded as an\nalteration of some of the structural equations in M. For example, forcing the variable X = x forms a sub-\nmodel Mx, in which the set of structural equations Fx = {fi : Vi /‚ààX} ‚à™{X = x}. To predict the outcome\nof such an intervention, we simply employ the submodel Mx to compute the new probability distribution.\nIn addition to directly setting the variables to constant values (known as hard intervention), we can also ma-\nnipulate the probability distributions that govern the variables, preserving some of the original dependencies\n(known as soft intervention). Many research questions involve estimating the effects of interventions. For\nexample, preventing scurvy requires identifying effective interventions (e.g., through dietary changes) that\nreduce the probability of getting scurvy. To distinguish from conditional probability, researchers introduced\nthe do-operator, using P(Y |do(X = x)) to denote the intervention probability, meaning the probability\ndistribution of the outcome variable Y when X is fixed to x. Figure 3a illustrates the difference between\nconditional and intervention probabilities. It is noteworthy that a critical distinction between statistical\nmodels and causal models lies in the fact that the former specifies a single probability distribution, while\nthe latter represents a collection of distributions, one for each possible intervention (including the null in-\ntervention). Consequently, causal models can provide a more comprehensive understanding of the world,\npotentially enhancing an agent‚Äôs robustness against certain distribution shifts (Sch√∂lkopf et al., 2021; Thams,\n2022).\nCounterfactual.\nCounterfactual thinking involves posing hypothetical questions, such as, ‚ÄúWould the\nscurvy patient have stayed healthy if they had eaten enough fruit‚Äù. This cognitive process allows us to\nretrospectively analyze past events and reason about the potential outcomes of altering certain factors\nin the past. This type of thinking helps us gain insights from experiences and identify opportunities for\nfurther improvements. Since counterfactuals involve hypothetical scenarios, collecting counterfactual data is\ngenerally impossible in reality, which is the core difference between it and interventions.\nIn the context of SCMs, counterfactual variables are often denoted with a subscript, such as YX=x (or Yx\nwhen there is no ambiguity) where X and Y are two sets of variables in V. This notation helps researchers\ndifferentiate counterfactuals from the original variable Y . The key difference between Y and Yx is that the\nlatter is generated by a submodel Mx in which X is set to x. Figure 3b provides an illustration for evaluating\ncounterfactuals with the twin network method (Pearl, 2009b, Chapter 7). The two networks represent the\nfactual and counterfactual (imaginary) world respectively. They are connected by the exogenous variables,\nsharing the same structure and variables of interest, except the counterfactual one removes arrows pointing\nto the variables corresponding to the hypothetical interventions. Remark that there are many different types\nof counterfactual quantities implied by a causal model other than the one shown in this example. In some\n5\nCondition\nùëÉ(ùëå)\nùëÉ(ùëå‚îÇùëã=         )\nùëÉ(ùëå‚îÇdo (ùëã=         ) )\nIntervention\n(a)\nCounterfactual\n(b)\nFigure 3: (a) An illustration of the difference between condition and intervention, with Y representing the\nhealth of sailors and X being a discrete variable with three possible values: fruit, juice, or meat.\nThe\nmarginal probability distribution P(Y ) studies all subgroups within the population, while the conditional\nprobability distribution P(Y |X = fruit) focuses on the subgroup of sailors who have consumed fruits. In\ncontrast, the interventional probability distribution P(Y |do(X = fruit)) examines the population in which\nall sailors are required to consume fruits. (b) An illustration of counterfactual probabilities, which study\nevents occurring in an imaginary world (the bottom network), e.g., considering the sick sailors who ate meat\nin the factual world (the upper network), would they have stayed healthy if they had consumed fruits?\nscenarios, such as fairness analysis (Carey & Wu, 2022; Plecko & Bareinboim, 2022), we may need to study\nnested counterfactuals, e.g., the direct and indirect effects.\nTogether, correlation, intervention, and counterfactuals form the three rungs of the ladder of causation, also\nknown as the ‚ÄúPearl Causal Hierarchy‚Äù (Bareinboim et al., 2022), which naturally emerges from an SCM.\nThis hierarchy involves increasingly refined reasoning tasks, as does the knowledge required to complete these\ntasks. In summary, we have provided a concise overview of several fundamental concepts in this section.\nFor readers interested in delving deeper into the realm of causality, please refer to Appendix A and the\ncited references.\nSubsequent sections will discuss related concepts and techniques within the context of\nreinforcement learning.\n2.2\nA Brief Introduction to Reinforcement Learning\nReinforcement learning studies sequential decision problems. Mathematically, we can formalize these prob-\nlems as Markov decision processes.\nDefinition 2.2 (Markov decision process). An MDP M is specified by a tuple {S, A, P, R, ¬µ0, Œ≥}, where\n‚Ä¢ S denotes the state space and A denotes the action space,\n‚Ä¢ P : S√óA√óS ‚Üí[0, 1] is the transition probability function that yields the probability of transitioning\ninto the next states st+1 after taking an action at at the current state st,\n‚Ä¢ R : S √ó A ‚ÜíR is the reward function that assigns the immediate reward for taking an action at at\nstate st,\n‚Ä¢ ¬µ0 : S ‚Üí[0, 1] is the probability distribution that specifies the generation of the initial state, and\n‚Ä¢ Œ≥ ‚àà[0, 1] denotes the discount factor that accounts for how much future events lose their value as\ntime passes.\nMarkov decision processes. In definition 2.2, the decision process starts by sampling an initial state s0\nwith ¬µ0. An agent takes responsive action using its policy œÄ (a function that maps a state to an action) and\nreceives a reward from the environment assigned by R. The environment evolves to a new state following P;\n6\nthen, the agent senses the new state and repeats interacting with the environment. The goal of an RL agent\nis to search for the optimal policy œÄ‚àóthat maximizes the return (cumulative reward) G0. In particular, at\nany timestep t, the return Gt is defined as the sum of discounted future rewards, i.e., Gt = P‚àû\ni=0 Œ≥iRt+i. A\nmulti-armed bandit (MAB) is a special type of MDP that focuses on single-step decision-making problems.\nOn the other hand, a partially observable Markov decision process (POMDP), generalizes the scope of MDPs\nby considering partial observability. In a POMDP, the system still operates based on an MDP, but the agent\ncan only access a partial or incomplete description of the system state, often referred to as the observation\nOt, when making decisions. For example, in a video game, a player may need to deduce the motion of a\ndynamic object based on the visual cues displayed on the current screen.\nValue functions. The return Gt evaluates how good an action sequence is. However, in stochastic en-\nvironments, the same action sequence can lead to diverse trajectories and consequently, different returns.\nMoreover, a stochastic policy œÄ outputs a probability distribution over the action space. Considering these\nstochastic factors, the return Gt associated with a policy œÄ becomes a random variable. In order to evaluate\npolicies under uncertainty, RL introduces the concept of value functions. There are two types of value func-\ntions: V œÄ(s) denotes the expected return obtained by following the policy œÄ from state s; QœÄ(s, a) denotes\nthe expected return obtained by performing action a at state s and following the policy œÄ thereafter. The\noptimal value functions correspond to the optimal policy œÄ‚àóare denoted by V ‚àó(s) and Q‚àó(s, a).\nBellman equations. By definition, V œÄ(s) = EœÄ[Gt|St = s] and QœÄ(s, a) = EœÄ[Gt|St = s, At = a]. These\ntwo types of value functions can be expressed in terms of one another. By expanding the return Gt, we can\nrewrite value functions in a recursive manner:\nV œÄ(s) =\nX\na‚ààA\nœÄ(a|s)\n \nR(s, a) + Œ≥\nX\ns‚Ä≤‚ààS\nP(s‚Ä≤|s, a)V œÄ(s‚Ä≤)\n!\nQœÄ(s, a) = R(s, a) + Œ≥\nX\ns‚Ä≤‚ààS\nX\na‚Ä≤‚ààA\nœÄ(a‚Ä≤|s‚Ä≤)QœÄ(s‚Ä≤, a‚Ä≤).\n(2)\nWhen the timestep t is not specified, s and s‚Ä≤ are often used to refer to the states of two adjacent steps. The\nabove equations are known as the Bellman expectation equations, which establish the connection between\ntwo adjacent steps. Similarly, the Bellman optimality equations relate the optimal value functions:\nV ‚àó(s) = max\na‚ààA\n \nR(s, a) + Œ≥\nX\ns‚Ä≤‚ààS\nP(s‚Ä≤|s, a)V ‚àó(s‚Ä≤)\n!\nQ‚àó(s, a) = R(s, a) + Œ≥\nX\ns‚Ä≤‚ààS\nP(s‚Ä≤|s, a) max\na‚Ä≤‚ààA Q‚àó(s‚Ä≤, a‚Ä≤).\n(3)\nWhen the environment (also referred to as the dynamic model or, simply, the model) is known, the learning\nproblem simplifies into a planning problem that can be solved using dynamic programming techniques based\non the Bellman equations. However, in the realm of RL, the main focus is on unknown environments. In\nother words, agents do not possess complete knowledge of the transition function P(s‚Ä≤|s, a) and the reward\nfunction R(s, a). This characteristic brings RL closer to decision-making problems in real-world scenarios.\nCategorizing reinforcement learning methods.\nThere are several ways to categorize RL methods.\nOne approach is based on the agent‚Äôs components. Policy-based methods generally focus on optimizing an\nexplicitly parameterized policy to maximize the return, while value-based methods use collected data to\nfit a value function and derive the policy implicitly from it. Actor-critic methods combine both of them,\nequipping an agent with both a value function and a policy.\nAnother classification criterion is whether\nRL methods use an environmental model. Model-based reinforcement learning (MBRL) methods typically\nemploy a well-defined environmental model (such as AlphaGo (Silver et al., 2017)) or construct one using\nthe collected data. The model assists the agent in planning or generating additional training data, thereby\nenhancing the learning process. Furthermore, RL can also be divided into on-policy, off-policy, and offline\napproaches based on data collection. On-policy RL only utilizes data from the current policy, while off-policy\nRL involves data collected by other policies. Offline RL disallows data collection, restricting the agent to\nlearn from a fixed dataset.\n7\n... ...\n... ...\nFigure 4: An illustrative example of casting a POMDP problem into an SCM. States are marked with dashed\ncircles to emphasize that they are latent variables in the POMDP problem, while actions are marked with\nhammers as they represent intervention variables controlled by policies.\n2.3\nCausal Reinforcement Learning\nBefore formally defining causal RL, let us cast a POMDP problem into an SCM M. To do this, we consider\nthe observation, state, action, and reward at each step to be endogenous variables. The observation, state\ntransition, and reward functions are then described as deterministic functions with independent exogenous\nvariables, represented by a set of structural equations F in M. This transformation is always possible using\nautoregressive uniformization (Buesing et al., 2019), without imposing any extra constraints. It allows us to\nformally discuss causality in RL, including addressing counterfactual queries that cannot be explained by non-\ncausal methods. Figure 4 presents an illustrative example of this transformation. From this formalization, it\nis straightforward to derive the MDP regime by introducing an additional constraint: Ot = St. In practice,\nstates and actions may have high dimensionality, and the granularity of the causal model can be adjusted\nbased on our prior knowledge. While the SCM representation allows us to reason about causality in decision-\nmaking problems and organize causal knowledge in a clear and reusable way, it does not constitute causal\nRL on its own. In this paper, we define causal RL as follows.\nDefinition 2.3 (Causal reinforcement learning). Causal RL is an umbrella term for RL approaches that\nincorporate additional assumptions or prior knowledge to analyze and understand the causal mechanisms\nunderlying actions and their consequences, enabling agents to make more informed and effective decisions.\nThis definition emphasizes two fundamental aspects that distinguish causal RL from non-causal RL. 1) It\nemphasizes a focus on causality, seeking to advance beyond superficial associations or data patterns. To\nmeet this goal, 2) it necessitates the incorporation of additional assumptions or knowledge that accounts for\nthe causal relationships inherent in decision-making problems.\nThe primary objective of RL is to determine the policy œÄ that yields the highest expected return, rather\nthan inferring the causal effect of a specific intervention. The policy œÄ can be seen as a soft intervention\nthat preserves the dependence of the action on the state, i.e., do(a ‚àºœÄ(¬∑|s)). Different policies result in\nvarying trajectory distributions. As mentioned earlier, RL is close to causality because on-policy RL can\ndirectly learn the total effects of actions on the outcome from interventional data. However, when we reuse\nobservational data, as in off-policy/offline RL, the learning problem becomes more intricate, as agents may\nsuffer from spurious correlations (Zhang et al., 2020b; Deng et al., 2021). Additionally, we may be interested\nin certain types of counterfactual quantities other than total effects in causal RL, as they hold the promises\nof improving data efficiency and performance (Bareinboim et al., 2015; Buesing et al., 2019; Lu et al., 2020).\nWe note that there is a lack of clarity and coherence in the existing literature on causal RL, primarily\nbecause causal modeling is more of a mindset than a specific problem setting or solution. Previous work\nhas explored diverse forms of causal modeling, driven by different prior knowledge and research purposes.\nIdeally, a perfect understanding of the data generation process would grant access to the true causal model,\n8\nenabling us to answer any correlation, intervention, and even counterfactual inquiries. However, given the\ninherent complexity of the world, it is often impractical to access a fully specified SCM. Most of the time, we\ncan only access data generated by this model. Unfortunately, data alone is generally insufficient to overcome\nthe knowledge gap. The Causal Hierarchy Theorem (CHT) (Bareinboim et al., 2022) demonstrates that\nthe ability to address questions at one layer almost never guarantees the ability to address the questions\nat higher layers. Fortunately, certain forms of knowledge, such as causal graphs, may provide sufficient\nbut attainable insights into the underlying model. They can serve as valuable surrogates, enabling us to\nidentify the quantities of interest as if the SCM were accessible.\nNote that, inferring an interventional\ndistribution from the observational distribution and the causal graph may not always be feasible due to\nthe presence of unobserved confounders.\nFor more information about the identifiability issue, we refer\nto Bareinboim et al. (2022). Additionally, in scenarios involving multiple domains, it is often beneficial\nto examine invariant factors across these domains, including causal mechanisms, causal structure (causal\ngraph), and causal representation (high-level variables that capture the causal mechanisms underlying the\ndata). In cases where prior knowledge about these factors is lacking, we can introduce certain inductive\nbiases, such as sparsity, independent causal mechanisms, and sparse mechanism shifts, to obtain reasonable\nestimates Sch√∂lkopf et al. (2021); Sontakke et al. (2021); Huang et al. (2022a;b).\nWith a solid understanding of the foundational concepts and definitions, we are now well-equipped to explore\nthe realm of causal reinforcement learning. The upcoming sections delve into four crucial challenges where\ncausal RL demonstrates its potential: generalizability and knowledge transfer, spurious correlations, sample\nefficiency, and considerations beyond return, e.g., explainability, fairness, and safety.\n3\nAdvancing Generalizability and Knowledge Transfer through Causal\nReinforcement Learning\n3.1\nThe Issue of Generalizability in Reinforcement Learning\nGeneralizability poses a major challenge in the deployment of RL algorithms in real-world applications. It\nrefers to the ability of a trained policy to perform effectively in new and unseen situations (Kirk et al.,\n2022). The issue of training and testing in the same environment has long been a critique faced by the\nRL community (Irpan, 2018).\nWhile people often expect RL to work reliably in different (but similar)\nenvironments or tasks, traditional RL algorithms are typically designed for solving a single MDP. They\ncan easily overfit the environment, failing to adapt to minor changes. Even in the same environment, RL\nalgorithms may produce widely varying results with different random seeds (Zhang et al., 2018a;b), indicating\ninstability and overfitting. Lanctot et al. (2017) presented an example of overfitting in multi-agent scenarios\nin which a well-trained RL agent struggles to adapt when the adversary slightly changes its strategy. A\nsimilar phenomenon was observed by Raghu et al. (2018). Furthermore, considering the non-stationarity\nand constant evolution of the real world (Hamadanian et al., 2022), there is a pressing need for robust\nRL algorithms that can effectively handle changes. Agents should possess the capability to transfer their\nacquired skills across varying situations rather than relearning from scratch.\n3.2\nHow can Causality Help to Improve Generalization and Facilitate Knowledge Transfer?\nSome previous studies have shown that data augmentation improves generalization (Lee et al., 2020a; Wang\net al., 2020a; Yarats et al., 2021), particularly for vision-based control. This process involves generating new\ndata by randomly shifting, mixing, or perturbing observations, which makes the learned policy more resistant\nto irrelevant changes. Another common practice is domain randomization. In sim-to-real reinforcement\nlearning, researchers randomized the parameters of simulators to facilitate adaptation to reality (Tobin\net al., 2017; Peng et al., 2018). Additionally, some approaches have attempted to incorporate inductive bias\nby designing special network structures to improve generalization performance (Kansky et al., 2017; Higgins\net al., 2017; Zambaldi et al., 2019; Raileanu & Fergus, 2021).\nWhile these works have demonstrated empirical success, explaining why certain techniques outperform oth-\ners remains challenging. This knowledge gap hinders our understanding of the underlying factors that drive\nsuccessful generalization and the design of algorithms that reliably generalize in real-world scenarios. To\n9\n(a) Generalize to noise or ir-\nrelevant variables.\n(b) Generalize to different\nreward assignments.\n(c) Generalize to different\ngoals.\n(d) Generalize to different\nphysical properties.\nFigure 5: Different types of generalization problems in reinforcement learning represented by causal graphs.\nIn these graphs, S and S‚Ä≤ represent states in adjacent time steps, A represents actions, R represents rewards,\nN represents irrelevant variables (e.g., background color), G represents goals (e.g., target position), and P\nrepresents physical properties (e.g., mass).\ntackle this challenge, it is essential to identify the factors that drive changes. Kirk et al. (2022) proposed\nusing contextual MDP (CMDP) (Hallak et al., 2015) to formalize generalization problems in RL. A CMDP\nresembles a standard MDP but explicitly captures the variability across a set of environments or tasks, which\nis determined by contextual variables such as goals, colors, shapes, mass, or the difficulty of game levels.\nFrom a causal perspective, these variabilities can be interpreted as different types of external interventions\nin the data generation process (Sch√∂lkopf et al., 2021; Thams, 2022). Figure 5 illustrates some examples\nof the generalization problems corresponding to different interventions. Previous methods simulate these\ninterventions during the training phase by augmenting original data or randomizing certain attributes, al-\nlowing the model to learn from various domains. By carefully scrutinizing the causal relationships behind\nthe data, we can gain a better understanding of the sources of generalization ability and provide a more\nlogical explanation.\nMore importantly, by making explicit assumptions on what changes and what remains invariant, we can\nderive principled methods for effective knowledge transfer and adaptation (Zhang et al., 2015; Gong et al.,\n2016). To illustrate this point, let us recall the example discussed in Figure 2a, where we can use X, Y , and\nZ to represent fruit consumption, vitamin C intake, and the occurrence of scurvy, respectively. Consider an\nintervention on fruit consumption (the variable X), one would have to retrain all modules in a non-causal\nfactorization such as P(X, Y, Z) = P(Y )P(Z|Y )P(X|Y, Z) due to the change in P(X). In contrast, with\nthe causal factorization P(X, Y, Z) = P(X)P(Z|X)P(Y |Z), only P(X) needs to be adjusted to fit the new\ndomain. The intuition behind this example is quite straightforward: altering fruit consumption (P(X)) does\nnot impact the vitamin C content in specific fruits (P(Z|X = x)) or the likelihood of developing scurvy\nconditioned on the amount of vitamin C intake (P(Y |Z = z)). This property is referred to as independent\ncausal mechanisms (Sch√∂lkopf et al., 2021), indicating that the causal generation process comprises stable\nand autonomous modules (causal mechanisms) (Pearl, 2009b, Chapter 2) such that changing one does not\naffect the others. Building on this concept, the sparse mechanism shift hypothesis (Sch√∂lkopf et al., 2021;\nPerry et al., 2022) suggests that small changes in the data distribution generally correspond to changes in\nonly a subset of causal mechanisms. These assumptions provide a basis for designing efficient algorithms\nand models for knowledge transfer.\n3.3\nCausal Reinforcement Learning for Improving Generalizability\nGeneralization involves various settings. Zero-shot generalization entails the agent solely acquiring knowledge\nin training environments and then being evaluated in unseen scenarios. While this setting is appealing, it is\noften impractical in real-world scenarios. Alternatively, we may allow agents to receive additional training\nin target domains, categorized as transfer RL (Zhu et al., 2020), multitask RL (Vithayathil Varghese &\nMahmoud, 2020), lifelong RL (Khetarpal et al., 2022), among others. omprehend the essential capabilities\nrequired for generalization and the expected outcomes of learning algorithms, causal RL explicitly considers\n10\nTable 1: Selected methods utilizing causality to improve generalizability.\nCategory\nPaper\nTechniques\nSettings\nEnvironments or Tasks\nIrrelevant\nvariables\nZhang et al. (2020a)\nCausal representation learning\nonline\nToy 2\nCart-pole (dm_control)\nCheetah (dm_control)\nBica et al. (2021b)\nCausal representation learning\nimitation\nOpenAI Gym\nMIMIC III\nWang et al. (2022)\nCausal dynamics learning\nonline‚àó\nChemical\nManipulation (robosuite)\nSaengkyongam et al. (2022)\nCausal representation learning\noffline\nToy\nDing et al. (2022)\nCausal discovery\nonline\nManipulation (Not accessible)\nUnlock (Minigrid)\nCrash (highway-env)\nDynamics\nSontakke et al. (2021)\nCausal representation learning\noffline-to-online\nManipulation (CausalWolrd)\nLee et al. (2021)\nIntervention\nDomain randomization\nonline\nManipulation (Isaac Gym)\nZhu et al. (2021)\nCounterfactual reasoning\nonline\nManipulation (CausalWolrd)\nGuo et al. (2022b)\nMediation analysis\nonline\nPendulum (OpenAI Gym)\nLocomotion (OpenAI Gym)\nTasks\nEghbal-zadeh et al. (2021)\nCausal representation learning\nonline\nContextual-Gridworld\nPitis et al. (2022)\nCounterfactual reasoning\noffline\nSpriteworld\nPong (Roboschool)\nManipulation (OpenAI)\nDynamics\nand Tasks\nZhang & Bareinboim (2017)\nCausal bound 3\nonline‚àó\nToy\nDasgupta et al. (2018)\nMeta learning\nonline\nToy\nNair et al. (2019)\nCausal induction 4\nonline\nLight\nHuang et al. (2022a)\nCausal dynamics learning\nonline\nCart Pole (OpenAI Gym)\nPong (OpenAI Gym)\nOthers\nZhu et al. (2022b)\nCausal discovery\nCausal dynamics learning\noffline\nToy\nInverted Pendulum (OpenAI Gym)\nthe factors that govern changes in distribution.\nThis section, therefore, categorizes existing causal RL\napproaches for generalization based on specific factors of change. The representative works are shown in\nTable 1. Furthermore, we also label the problem settings for these approaches, including offline, online,\noffline-to-online, and imitation learning. In cases where an approach employs online training augmented by\noffline datasets, it is labeled as online‚àó. To be self-contained, we offer a concise overview of the environments\nand tasks in Appendix B.\n3.3.1\nGeneralize to Different Environments\nFirst, we consider how to generalize to different environments.\nFrom a causal perspective, different en-\nvironments share most of the causal mechanisms but differ in certain modules, resulting from different\ninterventions in the state or observation variables. Building on the causal relationships within these vari-\nables, we can categorize existing approaches into two main groups: generalization to irrelevant variables and\ngeneralization to varying dynamics.\nTo enhance the ability to generalize to irrelevant factors, RL agents must examine the causality to identify\nthe invariance in the data generation process. Zhang et al. (2020a) investigated the problem of generaliz-\ning to diverse observation spaces within the block MDP framework, such as robots equipped with different\ntypes of cameras and sensors, which is a common scenario in reality. In the block MDP framework, the\nobservation space may be infinite, but we can uniquely determine the state (finite but unobservable) given\nthe observation. The authors proposed using invariant prediction to learn the causal representation that\ngeneralizes to unseen observations. Similarly, Bica et al. (2021b) introduced invariant causal imitation learn-\n2The term ‚Äútoy‚Äù refers to simple, synthetically constructed datasets or simulation environments that are used to experimen-\ntally verify findings. It is not a concrete environment or task. We use this term consistently throughout the paper.\n3When the causal effect is unidentifiable, we can resort to set identification (partial identification), deriving its upper and\nlower bounds, which allows us to assess the robustness of our estimates against unobserved confounding.\n4Causal induction involves the process of extracting abstract causal variables from high-dimensional, low-level pixel repre-\nsentations, followed by recovering the underlying causal graph.\n11\ning, which learns the imitation policy based on invariant causal representation across multiple environments.\nWang et al. (2022) studied the causal dynamics learning problem, which attempts to eliminate irrelevant\nvariables and unnecessary dependencies so that policy learning will not be affected by these nuisance factors.\nSaengkyongam et al. (2022) focused on the offline contextual bandit problems. Their proposed approach\ninvolves iteratively assessing the invariance condition for various subsets of variables to learn an optimal\ninvariant policy.\nThe algorithm begins by generating a sample dataset using an initial policy and then\ntests the invariance of each subset across different environments. If a subset is found to be invariant, an\noptimal policy within that subset is learned through off-policy optimization. The experimental results sug-\ngest that invariance is crucial for obtaining distributionally robust policies, particularly in the presence of\nunobserved confounders. Ding et al. (2022) proposed an approach to address the generalization problem\nin goal-conditioned reinforcement learning (GCRL). Their method involves treating the causal graph as a\nlatent variable and optimizing it using a variational likelihood maximization procedure. This method trains\nagents to discover causal relationships and learn a causality-aware policy that is robust against changes in\nirrelevant variables.\nGeneralizing to new dynamics is a complex issue that involves different types of variations. These variations\nmay include changes in physical properties (e.g., gravitational acceleration), disparities between the simula-\ntion environment and reality, and alternations in the range of attribute values, etc. Sontakke et al. (2021)\nproposed training RL agents to infer and categorize causal factors in the environment with experimental\nbehavior learned in a self-supervised manner. These behaviors can help the agent to extract discrete causal\nrepresentations from collected trajectories, which can be applicable to unseen environments, empowering the\nagent to effectively generalize to unseen contexts. Lee et al. (2021) proposed an approach that conducts\ninterventions to identify relevant state variables for successful robotic manipulation, i.e., the features that\ncausally influence the outcome. The robot exhibited excellent sim-to-real generalizability after training with\ndomain randomization on the identified features. Zhu et al. (2021) developed an algorithm to improve the\nability of agents to generalize to rarely seen or unseen object properties. This algorithm models the envi-\nronmental dynamics with SCMs, allowing the agent to generate counterfactual trajectories about objects\nwith different attribute values, which leads to improved generalizability. Guo et al. (2022b) investigated\nthe unsupervised dynamics generalization problem, which allows the learned model to generalize to new\nenvironments. The authors approached this challenge by leveraging the intuition that data originating from\nthe same trajectory or similar environments should have similar properties (hidden variables encoded from\ntrajectory segments) that lead to similar causal effects. To measure similarity, they employed conditional\ndirect effects in mediation analysis. The experimental results show that the learned model performs well in\nnew environmental dynamics.\n3.3.2\nGeneralize to Different Tasks\nAnother important topic is how to generalize to different tasks. In the SCM framework, different tasks are\ncreated by altering the structural equation of the reward variable or its parent nodes on the causal graph.\nThese tasks have the same underlying environmental dynamics, but the rewards are assigned differently.\nEghbal-zadeh et al. (2021) introduced causal contextual RL, where agents aim to learn adaptive policies\nthat can effectively adapt to new tasks defined by contextual variables. The authors proposed a contextual\nattention module that enables agents to incorporate disentangled features as contextual factors, leading to\nimproved generalization compared to non-causal agents. In order to make RL more effective in complex,\nmulti-object environments, Pitis et al. (2022) suggested factorizing the state-action space into separate local\nsubsets. This approach allows for learning the causal dynamic model as well as generating counterfactual\ntransitions in a more efficient manner. By training agents on counterfactual data, the proposed algorithm\nexhibits improved generalization to out-of-distribution tasks.\nFurthermore, in reality, generalization may involve changes in both the environmental dynamics and the\ntask. Several studies have explored this problem from a causal viewpoint. Zhang & Bareinboim (2017)\ninvestigated knowledge transfer across bandit agents in scenarios where causal effects are unidentifiable. The\nproposed strategy combines two steps: deriving the upper and lower bounds of causal effects using structural\nknowledge and then incorporating these bounds in a dynamic allocation procedure to guide the search\ntoward more promising actions in new bandit problems. The results indicated that this strategy dominates\n12\npreviously known algorithms and achieves faster convergence rates. Dasgupta et al. (2018) explored whether\nthe ability to perform causal reasoning emerges from meta-learning on a simple domain with five variables.\nThe experimental results suggested that the agents demonstrated the ability to conduct interventions and\nmake sophisticated counterfactual predictions. These emergent abilities can effectively generalize to new\ncausal structures. Nair et al. (2019) studied the causal induction problem with visual observation. They\nincorporated attention mechanisms into the agent to generate a causal graph based on visual observations\nand use it to make informed decisions. The experiments demonstrated that the agent effectively generalizes to\nnew tasks and environments with unknown causal structures. More recently, Huang et al. (2022c) proposed\nAdaRL, a novel framework for adaptive RL that learns a latent representation with domain-shared and\ndomain-specific components across multiple source domains.\nThe latent representation is then used in\npolicy optimization. This framework allows for efficient policy adaptation to new environments, tasks, or\nobservations, by estimating new domain-specific parameters using a small number of samples.\n3.3.3\nOther Generalization Problems\nIn offline RL, the agent can only learn from pre-collected datasets. In this setting, agents may encounter\npreviously unseen state-action pairs during the testing phase, leading to the distributional shift issue (Levine\net al., 2020). Most existing approaches mitigate this issue through conservative or pessimistic learning (Fu-\njimoto et al., 2019; Kumar et al., 2020; Yang et al., 2021b), rarely considering generalization to new states.\nZhu et al. (2022b) proposed a solution to generalize to unseen states. They recovered the causal structure\nfrom offline data by using causal discovery techniques, and then employ an offline MBRL algorithm to learn\nfrom the causal model. The experimental results suggested that the causal world model exhibits better\ngeneralization performance than a traditional world model, and effectively facilitates offline policy learning.\nAt the end of this section, it is worth noting that in the field of causal inference, there exists a closely\nrelated concept known as transportability. Specifically, transportability focuses on extrapolating experimen-\ntal findings across domains, i.e., transferring causal effects learned in experimental studies to new domains\n(populations/environments) in which only observational studies are feasible. Researchers have developed\ngraphical methods and a complete algorithmic program to address this challenge (Pearl & Bareinboim,\n2014). Given sufficient structured knowledge (which can qualitatively determine the differences between the\ntwo populations), this algorithmic program can help us determine whether the causal effects of the target\npopulation can be inferred from the experimental findings of the source population. Furthermore, it can\nelucidate what experimental and observational findings from the two populations are essential for such an\ninference. This setting has received limited attention in the causal RL literature, whereas many studies\nfocus on settings that allow experiments (online data collection) to be conducted in multiple domains or use\nobservational data to enhance online learning. Note that this problem is a very prevalent and important\naspect of scientific investigations. As researchers in both fields engage in more active knowledge exchange,\nwe believe new and valuable research directions will emerge. For further information on transportability,\nplease refer to Pearl & Bareinboim (2014); Bareinboim & Pearl (2016).\n4\nAddressing Spurious Correlations through Causal Reinforcement Learning\n4.1\nThe Issue of Spurious Correlation in Reinforcement Learning\nMaking reliable decisions based solely on data is inherently challenging, as correlation does not necessarily\nimply causation. It is important to recognize the presence of spurious correlations, which are deceptive\nassociations between two variables that may appear causally related but are not. These spurious correlations\nintroduce undesired bias to the learning problem, posing a significant challenge in various machine learning\napplications. Here are a few illustrative examples of this phenomenon.\n‚Ä¢ In recommendation systems, user behavior and preferences are often influenced by conformity, which\nrefers to the tendency of individuals to align with larger groups or social norms. Users may feel\ninclined to conform to popular trends or recommendations. Ignoring the impact of conformity can\nlead to an overestimation of a user‚Äôs preference for certain items (Gao et al., 2022);\n13\nUnobserved\nconfounder\nUser\npreference\nPopularity\nPopularity\nExposure\nClick\nClick\nCollider  \nnode\nDoes the correlation between X and Y\nindicate causation, or does it arise from an\nunobserved confounder?\nDoes the correlation between X and Y\nindicate causation, or does it arise from a\ncollider node?\nFigure 6: Causal graphs illustrating the two types of spurious correlations, with examples from real-world\napplications.\n‚Ä¢ In image classification, when dogs frequently appear alongside grass in the training set, the classifier\nmay incorrectly label an image of grass as a dog. This misclassification arises because the model\nrelies on the background (the irrelevant factors) rather than focusing on the specific pixels that\ncorrespond to dogs (the actual causal) (Zhang et al., 2021a; Wang et al., 2021c).\n‚Ä¢ When determining the ranking of tweets, the use of gender icons in tweets is usually not causally\nrelated to the number of likes; their statistical correlation comes from the topic, as it influences both\nthe choice of icon and the audience. Therefore, it is not appropriate to determine the ranking by\ngender icons (Feder et al., 2022).\nIf we want to apply RL in real-world scenarios, it is important to be mindful of spurious correlations, especially\nwhen the agent is working with biased data.\nFor instance, when optimizing long-term user satisfaction\nin multiple-round recommendations, there is often a spurious correlation between exposure and clicks in\nadjacent timesteps. This is because they are both influenced by item popularity. From another perspective,\nwhen we observe a click, it may depend on user preference or item popularity, which creates a spurious\ncorrelation between the two factors. In both scenarios, agents may make incorrect predictions or decisions,\nsuch as only recommending popular items (a suboptimal policy for both the system and the user), and\nthis can further cause filter bubbles. In a nutshell, if the agent learns a spurious correlation between two\nvariables, it may mistakenly believe that changing one will affect the other. This misunderstanding can lead\nto suboptimal or even harmful behavior in real-world decision-making problems.\n4.2\nHow can Causality Help to Address Spurious Correlations?\nThe non-causal approaches lack a language for systematically discussing spurious correlations. From the\ncausal perspective, spurious correlations arise when the data generation process involves unobserved con-\nfounders (common cause) or when a collider node (common effect) serves as the condition. The former leads\nto confounding bias, while the latter results in selection bias. See Figure 6 for a visual interpretation of these\nphenomena. Causal graphs enable us to trace the source of spurious correlations by closely scrutinizing the\ndata generation process. To eliminate the bias induced by spurious correlations, it is necessary to make\ndecisions regarding causality instead of statistical correlations. This is where causal reasoning comes in: It\nprovides principled tools to analyze and deal with confounding and selective bias (Pearl, 2009b; Bareinboim\net al., 2014; Glymour et al., 2016; Bareinboim et al., 2022), helping RL agents accurately estimate the causal\neffects in decision-making problems.\nOne may assume that on-policy RL is immune to spurious correlations since it directly learns causal effects of\nthe form p(r|s, do(a)) from interventional data. However, it is important to note that understanding the effect\nof an action on the outcome alone is insufficient for comprehending the complete data generation process.\nThe influence of different covariates on the outcome also plays a crucial role. For example, in personalized\nrecommendation systems, the covariates can be divided into relevant and spurious features. If the training\nenvironment consistently pairs the clicked items with specific values of spurious features (e.g., clickbait in\ntextual features), the agent may unintentionally learn a policy based on these spurious features. When the\nfeature distribution changes in the test environment, the agent may make erroneous decisions (Gao et al.,\n14\nTable 2: Selected methods utilizing causality to address spurious correlations.\nCategory\nPaper\nTechniques\nSettings\nEnvironments or Tasks\nConfounding bias\nZhang et al. (2020b)\nCausal graph 5\nimitation\nToy\nKumor et al. (2021)\nCausal graph\nimitation\nToy\nSwamy et al. (2022)\nInstrumental variables\nimitation\nLunar Lander (OpenAI Gym)\nLocomotion (PyBullet Gym)\nNamkoong et al. (2020)\nSensitivity analysis\noffline\nToy\nBennett et al. (2021)\nProxy variables\noffline\nToy\nLu & Lobato (2018)\nBackdoor adjustment\noffline\nPendulum (OpenAI Gym)\nCart Pole (OpenAI Gym)\nMNIST\nZhang & Bareinboim (2019)\nCausal bound\nSensitivity analysis\nonline‚àó\nToy\nRezende et al. (2020)\nBackdoor adjustment\noffline\nToy\nMiniPacman\n3D Maze (Unity)\nZhang & Bareinboim (2020)\nCausal graph\nCausal bound\nonline‚àó\nToy\nWang et al. (2021b)\nFrontdoor adjustment\nBackdoor adjustment\nonline‚àó\n-\nLiao et al. (2021)\nInstrumental variables\noffline\n-\nGuo et al. (2022a)\nInstrumental variables\noffline\n-\nGasse et al. (2023)\nBackdoor adjustment\nonline‚àó\nToy\nPace et al. (2023)\nCausal graph\noffline\nSepsis\nHiRID\nYang et al. (2022a)\nCausal graph\nonline\nToy\nCart Pole (OpenAI Gym)\nLunar Lander (OpenAI Gym)\nZhang & Bareinboim (2022b)\nCausal graph\nonline‚àó\nToy\nSelection bias\nBai et al. (2021)\nInverse probability weighting\nonline\nManipulation (OpenAI)\nDeng et al. (2021)\nCausal graph\noffline\nD4RL\n2022).\nThis example demonstrates the prevalence of spurious correlations in real-world decision-making\nproblems. Demystifying the causal relationships helps resolve such challenges.\n4.3\nCausal Reinforcement Learning for Addressing Spurious Correlations\nBased on the underlying causal structure of the decision-making problem, spurious correlations can manifest\nin two distinct types: confounding bias arising from fork structures, and selective bias arising from collider\nstructures (as depicted by Figure 6). Accordingly, we categorize existing methods into two groups. Addition-\nally, to be self-contained, we incorporate studies on imitation learning (IL) and off-policy evaluation (OPE),\ngiven their close relevance to policy learning in RL. The representative works are shown in Table 2.\n4.3.1\nAddressing Confounding Bias\nWe start by introducing an important technique in causal inference named do-calculus (Pearl, 1995). It is an\naxiomatic system that enables us to replace probability formulas containing the do operator with ordinary\nconditional probabilities.\nThe do-calculus includes three axiom schemas that provide graphical criteria\nfor making certain substitutions. It has been proven to be complete for identifying causal effects (Huang &\nValtorta, 2006; Shpitser & Pearl, 2006). Derived from the do-calculus, the backdoor and frontdoor adjustment\nare two widely used methods for eliminating confounding bias Glymour et al. (2016); Bareinboim et al. (2022).\nThe key intuition is to block spurious correlations between the treatment and outcome variables that pass\nthrough the confounders. In situations where unobserved confounding exists, it is still possible to identify\nthe causal effect of interest if observed proxy variables for the confounder are available (Miao et al., 2018;\nGhassami et al., 2023). Another popular approach for addressing unobserved confounding is to identify and\nemploy instrumental variables (Angrist et al., 1996; Baiocchi et al., 2014). An instrumental variable, denoted\n5Causal graph as a technique refers to using causal graphs to describe the data generation process, and designing graphical\ncriteria for determining properties such as identifiability or developing algorithms based on causal graphs.\n15\nas I, must satisfy three conditions: 1) I is a cause of T (the treatment variable); 2) I affects Y (the outcome\nvariable) only through T; and 3) The effect of I on Y is unconfounded. Since the instrumental variable I\ninfluences Y only through T, and this effect is unconfounded, we can indirectly estimate the effect of T on Y\nthrough the effect of the instrumental variable Z on Y . Furthermore, we can evaluate the robustness of the\nestimated causal effect against unobserved confounding by varying the strength of the confounder‚Äôs effect,\nwhich is referred to as sensitivity analysis (D√≠az & van der Laan, 2013; Kuang et al., 2020).\nZhang et al. (2020b) studied a single-step imitation learning problem using a combination of demonstration\ndata and structural knowledge about the data-generating process. They proposed a graphical criterion for\ndetermining the feasibility of imitation learning in the presence of unobserved confounders and a practical\nprocedure for estimating valid imitating policies with confounded expert data.\nThis approach was then\nextended to the sequential setting in a subsequent paper (Kumor et al., 2021). Swamy et al. (2022) de-\nsigned an algorithm for imitation learning with corrupted data. They proposed to use instrumental variable\nregression (Stock & Trebbi, 2003) to resolve the spurious correlations caused by unobserved confounding.\nSeveral research focused on the OPE problem, which seeks to estimate the performance of policies using\ndata generated by different policies. For example, Namkoong et al. (2020) conducted sensitivity analyses on\nOPE methods under unobserved confounding. They derived worst-case bounds on the performance of an\nevaluation policy and proposed an efficient procedure for estimating these bounds with statistical consistency,\nallowing for a reliable selection of policies. Bennett et al. (2021), on the other hand, proposed a new estimator\nfor OPE in infinite-horizon RL. The authors established the identifiability of the policy value from off-policy\ndata by employing a latent variable model for states and actions (which can be seen as proxy variables\nfor the unobserved confounders). The authors further presented a strategy for estimating the stationary\ndistribution ratio using proxies, which is then utilized for policy evaluation.\nLu & Lobato (2018) introduced a novel approach called deconfounding RL, aiming to learn effective policies\nfrom historical data affected by unobserved factors. Their method begins by estimating a latent-variable\nmodel using observational data, which identifies latent confounders and assesses their impact on actions and\nrewards. Then these confounders are utilized in backdoor adjustment to address confounding bias, enabling\nthe policy to be optimized based on a deconfounding model. Experimental results demonstrate the method‚Äôs\nsuperiority over traditional RL approaches when applied to observational data with confounders. Liao et al.\n(2021) also focused on the offline setting. They found that RL practitioners often encounter unobserved\nconfounding in medical scenarios, but there are some common sources of instrumental variables, including\npreferences, distance to specialty care providers, and genetic variants (Baiocchi et al., 2014). Therefore, they\nproposed an efficient algorithm to recover the transition dynamics from observational data using instrumen-\ntal variables, and employ a planning algorithm, such as value iteration, to search for the optimal policy.\nSimilarly, Guo et al. (2022a) embraced a comparable approach when tackling the POMDP problem. They\naddressed the estimation of the transfer kernel by framing it as a series of confounded regression problems\nthat can be solved by selecting suitable instrumental variables. After constructing confidence regions for\nthe model parameters, the final policy can be derived using a pessimistic planning method within these\nregions. Wang et al. (2021b), on the other hand, studied how confounded observational data can facilitate\nexploration during online learning. They proposed the deconfounded optimistic value iteration algorithm,\nwhich combines observational and interventional data to update the value function estimates. This algorithm\neffectively handles confounding bias by leveraging backdoor and frontdoor adjustment, achieving a smaller\nregret than the optimal online-only algorithm. Gasse et al. (2023) studied MBRL for POMDP problems.\nSpecifically, They used ideas from the do-calculus framework to formulate model learning as a causal infer-\nence problem. They introduced a novel method to learn a latent-based causal transition model capable of\nexplaining both interventional and observational regimes. By utilizing the latent variable, they were able to\nrecover the standard transition model, which, in turn, facilitated the training of RL agents using simulated\ntransitions. While the majority of existing research focuses on addressing identifiable hidden confounding,\nsuch as using instrumental or backdoor variables, hidden confounding is possibly non-identifiable in some\nreal-world scenarios. Pace et al. (2023) showed that even in such scenarios, a careful analysis from a causal\nperspective can still contribute to significant improvements in policy learning. Specifically, they proposed\na new type of uncertainty known as ‚Äúdelphic uncertainty‚Äù, which, in contrast to the widely studied epis-\ntemic uncertainty and aleatoric uncertainty, quantifies the variability across world models compatible with\n16\nùëÜt\nùëÜt+1\nùëçùë°\nùëÜt+2\nùëçùë°+1\nùê¥t\nùê¥t+1\nùëã1\nùëã2\nùëç\nùëå\nùëá1\nùëá2\n‚ãØ‚ãØ\n‚ãØ‚ãØ\nFigure 7: Causal graphs illustrating the confounded MDP (left) discussed in Wang et al. (2021b) and a DTR\nwith two stages of treatments (right). The relationship between these two settings can be derived easily. If\nnone of the unobserved confounders {Zt} are influenced by the states {St}, we can aggregate them into a\nglobal confounder Z. Let Y denotes the outcome of treatments in a DTR, and let X and T represent the\ncovariates and treatments, respectively. By decomposing the states into covariates and treatments at each\nstep (e.g., s2 = {X1, T1, X2}), a confounded MDP reduces to a DTR.\nthe observational data. They devised an offline RL algorithm that incorporates this uncertainty as a Bell-\nman penalty. Their algorithm demonstrates effectiveness in reducing non-identifiable confounding bias on\ntwo medical datasets. Yang et al. (2022a) proposed the causal inference Q-network algorithm to address\nconfounding bias arising from various types of observational interferences, such as Gaussian noise and obser-\nvation black-out. The authors began by analyzing the impact of these interferences on the decision-making\nprocess using causal graphs. They then devised a novel algorithm that incorporates the interference label\nto learn the relationship between interfered observations and Q-values in order to maximize rewards in the\npresence of observational interference. The model learns to infer the interference label and adjusts the policy\naccordingly. Experimental results demonstrate the algorithm‚Äôs improved resilience and robustness against\ndifferent types of interferences. Rezende et al. (2020) discussed the application of partial models in RL, a\nmodel-based approach that does not require modeling the complete (and usually high-dimensional) obser-\nvation. They showed that the causal correctness of a partial model can be influenced by behavior policies,\nleading to an overestimation of the rewards associated with suboptimal actions. To address this issue, the\nauthors proposed a simple yet effective solution. Since we have full control of the agent‚Äôs computational\ngraph, we can choose any node situated between the internal state and the produced action as a backdoor\nvariable, e.g., the intended action. By applying backdoor adjustment, we can ensure the causal correctness\nof partial models.\nA less familiar but highly important research topic for RL researchers is dynamic treatment regimes\n(DTRs) (Murphy, 2003). Closely related to the fields of biostatistics, epidemiology, and clinical medicine,\nthis topic focuses on determining personalized treatment strategies, including dosing or treatment planning.\nIt aims to maximize the long-term clinical outcome and can be mathematically modeled as an MDP with\na global confounder, as depicted in Figure 7. In real-world medical scenarios, global confounders such as\ngenetic characteristics and lifestyle often exist but may not be observed or recorded due to various reasons.\nWhen applying RL to learn DTRs, we must properly handle the confounders; thus, it is highly relevant to\nthe topic discussed in this section.\nZhang & Bareinboim (2019) considered a setting in which causal effect is not identifiable. They proposed\nan online learning algorithm to solve DTRs by combining confounded observational data. Their algorithm\nhinges on sensitivity analyses and incorporates causal bounds to accelerate the learning process. This online\nlearning setting has gained popularity as it allows for conducting sequential and adaptive experimentation\nto maximize the outcome variable. However, a significant challenge arises when dealing with a vast space of\ncovariates and treatments, where online learning algorithms may result in unacceptable regret. To address\nthis challenge, Zhang & Bareinboim (2020) proposed an efficient procedure that leverages the structural\nknowledge encoded in the causal graph to reduce the dimensionality of the candidate policy space.\nBy\nexploiting the sparsity of the graph, where certain covariates are affected by only a small subset of treatments,\n17\nthe proposed method exponentially reduces the dimensionality of the learning problem. The experimental\nresults consistently demonstrate that the proposed method outperforms state-of-the-art (SOTA) methods in\nterms of performance. More recently, Zhang & Bareinboim (2022b) further explored the challenge of policy\nspace with mixed scopes, i.e., the agent has to optimize over candidate policies with varying state-action\nspaces. This becomes particularly crucial in medical domains such as cancer, HIV, and depression, where\nfinding the optimal combination of treatments is essential for achieving desired outcomes. To tackle this\nissue, the authors propose a novel method that utilizes causal graphs to identify the maximal sets of variables\nthat are causally related to each other. These sets are then utilized to parameterize SCMs, enabling the\nrepresentation of different interventional distributions using a minimal set of components, thereby enhancing\nthe efficiency of the learning process.\nA notable trend that emerges from the above literature is the combination of observational and experimental\ndata from diverse sources, commonly referred to as data fusion (Bareinboim et al., 2022). This idea has been\nproven to be effective in addressing complex problems related to reasoning and decision-making (Zhang &\nBareinboim, 2019; Lee et al., 2020b; Correa et al., 2021; Zhang et al., 2022b; Gasse et al., 2023). It represents\nan important research direction in causal inference and may have profound implications for reinforcement\nlearning. In reinforcement learning, agents often have access to both observational and experimental data,\ne.g., augmenting online learning with confounded observational data collected by other policies. As a result,\ndata fusion introduces new challenges and holds promising potential for the research community.\n4.3.2\nAddressing Selection Bias\nSelective bias occurs when data samples fail to represent the target population. For example, selective bias\narises when researchers seek to understand the effect of a certain drug on curing a disease by investigating\npatients in a selected hospital. This is because those patients may differ significantly from the population\nregarding their residence, wealth, and social status, making them unrepresentative.\nBai et al. (2021) investigated the selective bias associated with using hindsight experience replay (HER)\nin goal-conditioned reinforcement learning (GCRL) problems. In particular, HER relabels the goal of each\ncollected trajectory and computes new rewards, thereby allowing the agent to learn from failure experiences\n‚Äî‚Äî instances where it failed to reach the original goal but succeeded in reaching the relabeled one. How-\never, the relabeled target distribution may not accurately represent the original target distribution. This\ndiscrepancy can misguide agents trained with HER, leading them to mistakenly believe that repeating deci-\nsions made in the failure experience will result in high rewards. To address this issue, the authors proposed\nto use inverse probability weighting, a technique in causal inference, to assign appropriate weights to the\nrewards computed for the relabeled goals. By reweighting the samples, the agent can mitigate the selection\nbias induced by HER and effectively learn from a balanced mixture of successful and unsuccessful outcomes,\nultimately enhancing the overall performance. Deng et al. (2021) examined the offline RL problem through\nthe lens of selective bias. In the offline setting, agents are vulnerable to the spurious correlation between\nuncertainty and decision-making, which can result in learning suboptimal policies. Taking a causal perspec-\ntive, the empirical return is the outcome of both uncertainty and actual return. Since it is infeasible to\nreduce uncertainty by acquiring more data in the offline setting, an agent might mistakenly assume a causal\nrelationship between uncertainty and actual return. As a result, it may favor policies that achieve high\nreturns by chance (high uncertainty). To address this issue, the authors propose quantifying uncertainty and\nusing it as a penalty term in the learning process. The results show that this method outperforms various\nbaselines that do not consider spurious correlations in the offline setting.\nIn general, a standard reinforcement learning setup allows agents to learn from the same environment on\nwhich they will be tested, so issues related to sample representativeness are often not a primary concern.\nHowever, as discussed above, specific algorithmic choices or offline learning can introduce selection bias.\nOverall, this issue remains relatively underexplored in reinforcement learning. In the realm of causal infer-\nence, substantial efforts have been dedicated to addressing the challenge of selection bias. For instance, the\ngraphical conditions proposed by Bareinboim et al. (2014) offer a method for assessing the recoverability of\ncertain conditional probabilities and causal effects from data affected by selection bias. These efforts provide\nvaluable tools for further investigations into this issue in reinforcement learning.\n18\nTable 3: Selected methods utilizing causality to optimize sample efficiency.\nCategory\nPaper\nTechniques\nSettings\nEnvironments or Tasks\nRepresentation Learning\nSontakke et al. (2021)\nCausal representation learning\noffline to online\nManipulation (CausalWorld)\nLee et al. (2021)\nIntervention\nDomain randomization\nonline\nManipulation (Isaac Gym)\nHuang et al. (2022b)\nCausal dynamics learning\nonline‚àó\nCar Racing (OpenAI Gym)\nVizDoom\nWang et al. (2022)\nCausal dynamics learning\nonline‚àó\nChemical\nManipulation (robosuite)\nDirected Exploration\nSeitzer et al. (2021)\nIntervention\nonline\nManipulation (OpenAI)\nData Augmentation\nBuesing et al. (2019)\nCounterfactual reasoning\nonline\nSokoban\nLu et al. (2020)\nCounterfactual reasoning\nonline\nCart Pole (OpenAI Gym)\nMIMIC-III\nPitis et al. (2020)\nCounterfactual reasoning\noffline\nSpriteworld\nPong (Roboschool)\nManipulation (OpenAI)\nZhu et al. (2021)\nCounterfactual reasoning\nonline\nManipulation (CausalWorld)\nIn summary, this section presents some instances of spurious correlations in reinforcement learning and the\ncausal RL methods to solve these problems. A careful reader may notice the subtle connection between\nspurious correlation and generalization. For instance, we can treat unobserved confounders as contextual\nvariables and create new domains by applying external interventions to these variables.\nIn such cases,\npolicies that exhibit strong generalization (robust to different contexts) tend to be less sensitive to spurious\ncorrelations, and vice versa. Therefore, the distributional robustness framework designed for generalization\nmay also serve as a tool to mitigate spurious correlations under certain conditions (Ding et al., 2023).\nHowever, by comparison, causal RL methods for addressing generalization and spurious correlation may not\nshare the same focus and techniques. The former is usually performance-oriented, focusing on how well a\nlearned policy performs on new domains. In contrast, the latter is more focused on identifying the causal\neffects of interest rather than dealing with the distributional shifts associated with confounding variables.\nTechnically, research on spurious correlations often introduces structural assumptions (e.g., causal graphs)\nand employs causal inference techniques like backdoor/frontdoor adjustments to eliminate biases. In recent\nyears, there has been an interesting trend (Zhang et al., 2021a; Cui & Athey, 2022; Ding et al., 2023) in\nexploring the intersection of the two research fields, which has the potential to offer valuable insights and\ntechniques to researchers in both fields.\n5\nEnhancing Sample Efficiency through Causal Reinforcement Learning\n5.1\nThe Issue of Sample Efficiency in Reinforcement Learning\nIn RL, training data is typically not provided before interacting with the environment. Unlike supervised\nand unsupervised learning methods that directly learn from a fixed dataset, an RL agent needs to actively\ngather data to optimize its policy towards achieving the highest return. An effective RL algorithm should\nbe able to master the optimal policy with as few experiences as possible (in other words, it need to be\nsample-efficient). Current methods often require collecting millions of samples to succeed in even simple\ntasks, let alone more complicated environments and reward mechanisms. For example, AlphaGo Zero was\ntrained over roughly 3√ó107 games of self-play (Silver et al., 2017); OpenAI‚Äôs Rubik‚Äôs Cube robot took nearly\n104 years of simulation experience (OpenAI et al., 2019). This inefficiency entails a high training cost and\nprevents the use of RL techniques for solving real-world decision-making problems. Therefore, the sample\nefficiency issue is a core challenge in RL, necessitating the development of RL algorithms that can save time\nand computational resources. In this section, in addition to examining sample-efficient algorithms that are\ndesigned to minimize regret in online learning, we also cover a number of approaches related to knowledge\ntransfer. These methods require only a small number of samples from the target domain to converge after\npre-training on the source domains, thus indirectly enhancing the sample efficiency within the target domain.\n19\n5.2\nCausal Reinforcement Learning for Addressing Sample Inefficienty\nSample efficiency has been extensively explored in RL literature (Kakade, 2003; Osband et al., 2013; Grande\net al., 2014; Yu, 2018) and causality offers some valuable principles for designing sample-efficient RL al-\ngorithms. Accordingly, we can organize existing research into three main lines: representation learning,\ndirected exploration, and data augmentation. The representative works are shown in Table 3.\n5.2.1\nRepresentation Learning for Sample Efficiency\nA good representation of the environment can be beneficial for sample-efficient RL. By providing a compact\nand informative representation of the environment, an RL agent can learn more effectively with fewer samples.\nThis is because a good representation can help the agent identify important features of the environment and\nabstract away unnecessary details, allowing the agent to make better use of its experiences.\nMotivated by the principle of independent causal mechanisms (Sch√∂lkopf et al., 2021), Sontakke et al. (2021)\nargue that the information in an observed trajectory is the sum of information ‚Äúinjected‚Äù by different causes.\nThus, when learning in multiple environments with different physical properties, if the collected trajectories\nare well-clustered, it suggests that there may be only a single causal feature that differs among these trajec-\ntories, such as the mass, size, or shape of an object. Based on this idea, they employ clustering performance\nto induce experimental behavior and use clustering results as a surrogate for causal representation. With the\nstate augmented by these representations, the learned policies exhibit outstanding zero-shot generalization\nability and require only a small number of training samples to converge in new environments.\nFigure 8: An illustration of a state transition between\nadjacent time steps. (Left) No abstraction. All vari-\nables are fully connected. (Middle) An irrelevant co-\nvariate S1 is removed but the rest are still fully con-\nnected. (Right) Only the causal edges are preserved.\nAnother way to improve sample efficiency through\nrepresentations is state abstraction. Lee et al. (2021)\nassumed the availability of an environmental model\nand used causal reasoning to identify relevant con-\ntext variables.\nSpecifically, the proposed method\nconducts interventions to alter one context variable\nat a time and observes the causal influence on the\noutcome. This approach effectively reduces the di-\nmensionality of the state space and simplifies the\nlearning problem. However, in many scenarios, di-\nrect intervention may not be feasible.\nSome non-\ncausal approaches (Jong & Stone, 2005; Zhang et al.,\n2022a) achieve abstraction by aggregating states\nthat yield the same reward sequence. While these\napproaches help reduce the dimensionality of the state space, they still suffer from redundant dependencies\nand are vulnerable to spurious correlations. In contrast, causal relationships in the real world are typically\nsparse and stable(Sch√∂lkopf et al., 2021; Huang et al., 2022a), leading to more effective abstraction. See\nFigure 8 for a comparison between these two types of abstraction.\nWhen observations involve high-dimensional and low-level data, causal representation learning helps identify\nthe relevant high-level concepts for the given tasks. Therefore, many causal RL approaches are motivated\nby the idea that exploiting the true causal structure of the problem will reduce the complexity of learning,\nand thus improve sample efficiency. For example, Huang et al. (2022b) introduced a method to learn action-\nsufficient state representations from data collected by a random policy. These representations consist of a\nminimal set of state variables that contain sufficient information for decision-making. Wang et al. (2022),\non the other hand, studied task-independent state abstraction which only omits action-irrelevant variables\nthat neither change with actions nor influence actions‚Äô results, identified through independence tests based\non conditional mutual information. Their approach includes variables that are potentially useful for future\ntasks rather than being restricted to a particular training task.\n20\nFACTUAL WORLD\nABDUCTION\nACTION\n+\n=\n=\n=\n=\n=\n+\nCOUNTERFACTUAL WORLD\nAUGMENTATION\n+\n=\n=\n+\nPREDICTION\n=\n=\n=\n=\nDifferent\nInterventions\nIntervention\nFigure 9: An example of counterfactual data augmentation following the counterfactual reasoning procedure:\nabduction, action, and prediction. The outcome of this procedure is then used to augment the training data\nobserved in the factual world.\n5.2.2\nDirected Exploration for Sample Efficiency\nWhile a good representation of the environment is beneficial, it is not necessarily sufficient for sample efficient\nRL (Du et al., 2020).\nTo improve sample efficiency, researchers have been studying various exploration\nstrategies (Yang et al., 2022b). Some research has drawn inspiration from developmental psychology (Ryan &\nDeci, 2000; Barto, 2013) and used intrinsic motivation to motivate agents to explore unknown environments\nefficiently (Pathak et al., 2017; Burda et al., 2022). This can be done by giving bonuses to exploratory\nbehaviors that discover novel or uncertain states. However, from a causal perspective, it is important to\nnote that not all regions of high uncertainty are equally important. Those regions which establish a causal\nrelationship with the task‚Äôs success are more worthy of exploration.\nSeitzer et al. (2021) studied the problem of directed exploration in robotic manipulation tasks, where the\nagent must physically interact with the target object to generate valuable data before acquiring complex\nmanipulation skills such as object relocation. The authors proposed a method to quantify the causal influence\nof actions on the object and incorporated it into the exploration process to guide the agent. Experimental\nresults demonstrate that the proposed method significantly improves the sample efficiency across various\nrobotic manipulation tasks. On the other hand, Sontakke et al. (2021) introduced a method to learn self-\nsupervised experiments based on the principle of independent causal mechanisms. This method utilizes the\nconcept of One-Factor-at-A-Time (OFAT), wherein good experimental behavior should examine one factor\nat a time while keeping others constant. The rationale behind this approach is that altering only one causal\nfactor should yield less information compared to changing multiple factors simultaneously. Consequently,\nthe learning problem of experimental behavior can be reformulated as minimizing the amount of information\ncontained in the generated data (also referred to as maximizing ‚Äúcausal curiosity‚Äù). Empirical results show\nthat RL agents pre-trained with causal curiosity exhibit improved efficiency in solving new tasks.\n5.2.3\nData Augmentation for Sample Efficiency\nData augmentation is a common machine learning technique aimed at improving algorithm performance by\ngenerating additional training data. Counterfactual data augmentation is a causality-based approach that\n21\nuses a causal model to imitate the environment and generate data that is unobserved in the real world. This\nis particularly useful for RL problems because collecting large amounts of real-world data is often difficult or\nexpensive. By simulating diverse counterfactual scenarios, RL agents can determine the effects of different\nactions without interacting with the environment, resulting in sample-efficient learning.\nThe implementation of counterfactual data augmentation follows a counterfactual reasoning procedure 6 that\nconsists of three steps (Pearl, 2009a, Chapter 7), as demonstrated in Figure 9:\n1. Abduction is about using observed data to infer the values of the exogenous variables U;\n2. Action involves modifying the structural equations of the variables of interest in the SCM; and\n3. Prediction uses the modified SCM to generate counterfactual data by plugging the exogenous\nvariables back into the equations for computation.\nWhile traditional model-based reinforcement learning (MBRL) methods (Wang et al., 2019; Luo et al., 2022)\ncan also generate samples by fitting a probabilistic density model, they lack the capability to effectively model\nexogenous variables.\nThis limitation can lead to under-fitting when dealing with complex distributions\nof exogenous variables (Buesing et al., 2019).\nIn contrast, counterfactual data augmentation explicitly\nincorporates exogenous variables using the SCM framework. From a Bayesian perspective, traditional MBRL\napproaches implicitly rely on a fixed prior, such as the Gaussian distribution, for exogenous variables, whereas\ncounterfactual data augmentation leverages additional information (evidence) from the collected data to\nestimate the posterior distribution of exogenous variables. Consequently, counterfactual data generation\nholds promise in producing high-quality training data, potentially leading to improved policy evaluation and\noptimization.\nBuesing et al. (2019) proposed the counterfactually-guided policy search (CF-GPS) algorithm, designed to\nsearch for the optimal policies in POMDPs. They framed model-based POMDP problems using SCMs.\nThe proposed algorithm evaluates the outcome of counterfactual actions based on real experience, thereby\nimproving the utilization of experience data. In a similar vein, Lu et al. (2020) proposed a sample-efficient\nRL algorithm based on the SCM framework. Their objective was to address issues related to mechanism\nheterogeneity and data scarcity. Their approach empowers agents to evaluate the potential consequences of\ncounterfactual actions, thereby circumventing the need for actual exploration and alleviating biases arising\nfrom limited experience. Pitis et al. (2020) presented a novel framework that leverages a locally factored\ndynamics model to generate counterfactual transitions for RL agents. Specifically, the term ‚Äúlocally factored‚Äù\nindicates that the state-action space can be partitioned into a disjoint union of local subsets, each has its\nown causal structure.\nThis locally factored approach allows for an exponential reduction in the sample\ncomplexity of training a dynamics model and enables reliable generalization to unseen states and actions.\nMore recently, Zhu et al. (2021) proposed a novel approach to overcome the limitations of existing MBRL\nmethods in the context of robotic manipulation tasks. These tasks are particularly challenging due to the\ndiversity of object properties and the risk of robot damage. The proposed method uses SCMs to capture\nthe underlying environmental dynamics and generate counterfactual episodes involving rarely seen or unseen\nobjects. Experimental results demonstrate superior sample efficiency, requiring fewer environment steps to\nconverge compared to existing MBRL algorithms.\n6\nPromoting Explainability, Fairness, and Safety with Causal Reinforcement\nLearning\nIn general, the primary objective of RL is to maximize returns. However, with the increasing integration of\nRL-based automated decision systems into our daily lives, it becomes imperative to examine the interactions\n6This procedure intuitively elucidates the computational process for evaluating counterfactual statements based on available\nevidence. Nonetheless, it is not always feasible in practice. Abduction necessitates access to the distribution of exogenous\nvariables, while action and prediction require knowledge about the underlying functional relationships. In the causal inference\nliterature, extensive research has been dedicated to investigating the combination of qualitative assumptions and data to identify\ncounterfactual distributions. For more details, please refer to Shpitser & Pearl (2007); Correa et al. (2021); Zhang et al. (2022b).\n22\nTable 4: Selected methods utilizing causality for goals beyond maximizing returns.\nCategory\nPaper\nTechniques\nSettings\nEnvironments or Tasks\nExplainability\nFoerster et al. (2018)\nCounterfactual\nonline\nStarCraft\nMadumal et al. (2020)\nCounterfactual reasoning\nonline\nOpenAI Gym\nStarCraft\nBica et al. (2021a)\nCounterfactual reasoning\nimitation\nToy\nMIMIC-III\nMesnard et al. (2021)\nCounterfactual reasoning\nonline\nToy\nKey-to-Door (Not accessible)\nInterleaving (Not accessible)\nTsirtsis et al. (2021)\nCounterfactual reasoning\nonline\nToy\nTherapy\nTriantafyllou et al. (2022)\nCounterfactual reasoning\nonline\nGoofspiel (Not accessible)\nHerlau & Larsen (2022)\nMediation analysis\nonline\nToy\nDoorKey (Not accessible)\nFairness\nZhang & Bareinboim (2018)\nCounterfactual reasoning\nMediation analysis\noffline\nToy\nHuang et al. (2022c)\nCausal graph\nCausal reasoning\nonline\nToy\nBalakrishnan et al. (2022)\nCausal graph\nCounterfactual reasoning\nonline\nToy\nSafety\nHart & Knoll (2020)\nCounterfactual reasoning\noffline\nBARK-ML\nEveritt et al. (2021)\nCausal graph\nonline\n-\nbetween RL agents and humans, as well as their potential societal implications. In this section, we explore\ncausal RL methods that aim to address and alleviate challenges related to explainability, fairness, and safety.\nThe representative works are shown in Table 4.\n6.1\nExplainability\n6.1.1\nExplainability in Reinforcement Learning\nExplainability in RL refers to the ability to understand and interpret the decisions made by an RL agent. It\nis important to both researchers and general users. Explanations reflect the knowledge learned by the agent,\nfacilitating in-depth understanding. They also allow researchers to participate efficiently in the design and\ncontinual optimization of an algorithm. Furthermore, explanations unveil the internal logic of the decision-\nmaking process. When agents outperform humans, we can extract valuable insights from these explanations\nto inform human practice within a specific domain. For general users, explanations provide a rationale behind\neach decision, thereby enhancing their comprehension of intelligent agents and instilling greater confidence\nin the agent‚Äôs capabilities.\n6.1.2\nExplainable Reinforcement Learning with Causality\nExplainable RL methods can be broadly categorized into two groups:\npost hoc and intrinsic ap-\nproaches (Puiutta & Veith, 2020; Heuillet et al., 2021). Post hoc explanations are provided after the model\nexecution, whereas intrinsic approaches inherently possess transparency.\nPost hoc explanations, such as\nthe saliency map approach (Greydanus et al., 2018; Mott et al., 2019), often rely on correlations. How-\never, as we mentioned earlier, conclusions drawn based on correlations may be unreliable and fail to answer\ncausal questions. On the other hand, intrinsic explanations can be achieved using interpretable algorithms,\nsuch as linear regression or decision trees, but the limited modeling capacity of these algorithms may prove\ninsufficient in explaining complex behaviors (Puiutta & Veith, 2020).\nIn contrast, humans possess an innate and powerful ability to explain the connections between different events\nthrough a ‚Äúmental causal model (Sloman, 2005)‚Äù. This cognitive ability enables us to employ causal language\nin our everyday interactions, using phrases such as ‚Äúbecause,‚Äù ‚Äútherefore,‚Äù and ‚Äúif only.‚Äù By harnessing causal\nrelationships, we acquire natural and flexible explanations that do not rely on specific algorithms or models,\n23\nthereby greatly facilitating efficient communication and collaboration.\nDrawing inspiration from human\ncognition, we can integrate causality into RL to provide explanations that enable agents to articulate their\ndecisions and comprehension of the environment and tasks using causal language. Furthermore, in cases\nwhen the agent makes mistakes, we can respond with tailored solutions guided by causal insights.\nOne way to generate explanations is to use the concept of counterfactuals such as exploring the minimal\nchanges necessary to produce a different outcome.\nThe term ‚Äúcounterfactual‚Äù is popular in multi-agent\nreinforcement learning (MARL). For example, Foerster et al. (2018) proposed a method named counterfactual\nmulti-agent policy gradients for efficiently learning decentralized policies in cooperative multi-agent systems.\nMore precisely, counterfactuals help resolve the challenge of multi-agent credit assignment so that agents\nand humans can better understand the contribution of individual behavior to the team. Some subsequent\nstudies followed the same idea (Su et al., 2020; Zhou et al., 2022). These approaches did not perform the\ncomplete counterfactual reasoning procedure as shown in Figure 9, missing the critical step of abduction,\nwhich offers opportunities for further enhancements. More recently, Triantafyllou et al. (2022) established\na connection between Dec-POMDPs and SCM, enabling them to investigate the credit assignment problem\nin MARL using causal language. They proposed to formalize the notion of responsibility attribution based\non actual causality, as defined by counterfactuals, which is a significant stride in developing a rigorous\nframework that supports accountable MARL research. Mesnard et al. (2021), on the other hand, studied\nthe temporal credit assignment problem, i.e., measuring an action‚Äôs influence on future rewards. Inspired by\nthe concept of counterfactuals from causality theory, the authors proposed conditioning value functions on\nfuture events, which separate the influence of actions on future rewards from the effects of other sources of\nstochasticity. This approach not only facilitates explainable credit assignment but also reduces the variance\nof policy gradient estimates.\nMadumal et al. (2020) used theories from cognitive science to explain how humans understand the world\nthrough causal relationships and how these relationships can help us understand and explain the behavior\nof RL agents. They presented an approach that integrates an SCM into reinforcement learning and used the\nlearned model to generate explanations of behavior based on counterfactual analysis. For example, when\nquiring why a Starcraft II agent builds supply depots instead of barracks (a typical counterfactual query),\nthe agent can respond by explaining that constructing supply depots is more desirable, as it helps increase\nthe number of destroyed units and buildings. To evaluate the proposed approach, a study was conducted\ninvolving 120 participants. The results demonstrate that the causality-based explanations outperformed\nother explanation models in terms of understanding, explanation satisfaction, and trust. Bica et al. (2021a)\nproposed an innovative approach to gain insights into expert decision processes by integrating counterfactual\nreasoning into batch inverse reinforcement learning. Their method focuses on learning explanations of expert\ndecisions by modeling their reward function based on preferences with respect to counterfactual outcomes.\nThis framework is particularly helpful in real-world scenarios where active experimentation may not be\nfeasible. Tsirtsis et al. (2021) conducted a study on the identification of optimal counterfactual explanations 7\nfor a sequential decision process. They approached this problem by formulating it as a constrained search\nproblem and devised a polynomial time algorithm based on dynamic programming to find the solution.\nSpecifically, this problem requires the algorithm to use a causal model of the environment to search for\nanother sequence of actions that differs from the observed sequence of actions by a specified number of\nactions. The study conducted by Herlau & Larsen (2022) explores the application of mediation analysis in\nRL. The proposed method focuses on training an RL agent to optimize natural indirect effects, which allows\nfor identifying critical decision points. For instance, in the task of unlocking a door, the agent can effectively\nrecognize the event of acquiring a key. By leveraging mediation analysis, the agent can acquire a concise\nand interpretable causal model, enhancing its overall performance and explanatory capabilities.\n7Remark that the term ‚Äúcounterfactual explanation‚Äù is commonly used in the field of explainability. It refers to a broad\nrange of methods that offer explanations by analyzing the changes that could result from altering the inputs to a model in a\nparticular way. These methods do not necessarily imply causality. For further details, please refer to Verma et al. (2022).\n24\n6.2\nFairness\n6.2.1\nFairness in Reinforcement Learning\nIn addition to explainability, we also want agents to align with human values and avoid potential harm\nto human society, with fairness being a key consideration. As machine learning applications continue to\npermeate various aspects of our daily lives, fairness is increasingly recognized as a significant concern by\nbusiness owners, general users, and policymakers (Carey & Wu, 2022).\nIn real-world scenarios, fairness\nconcerns often exhibit a dynamic nature (Gajane et al., 2022), involving multiple decision rounds.\nFor\nexample, resource allocation and college admissions can be modeled as MDPs (D‚ÄôAmour et al., 2020),\nwherein actions have cumulative effects on the population, leading to dynamic changes in fairness. Ignoring\nthis dynamic nature of a system may lead to unintended consequences, e.g., exacerbating the disparity\nbetween advantaged and disadvantaged groups (Liu et al., 2018; Creager et al., 2020; D‚ÄôAmour et al., 2020).\nIn decision-making problems like these, RL agents should strive to genuinely benefit humans and promote\nsocial good, avoiding any form of discrimination or harm towards specific individuals or groups.\n6.2.2\nFair Reinforcement Learning with Causality\nWhen considering the application of reinforcement learning to solve fairness-aware decision-making problems,\nit is crucial to first examine the available prior knowledge. Detailed causal modeling allows us to characterize\nand understand the intricate interplay between decision-making and environmental dynamics (Zhang et al.,\n2020c; Tang et al., 2023). Specifically, we can determine what observable variables and latent factors are\ninvolved in a specific problem and how they affect (long-term) fairness. For example, Balakrishnan et al.\n(2022) used causal graphs to study fairness principles in decision-making problems. They encoded these\nprinciples into causal, non-causal and utility components, and analyzed the relationship between different\ncausal paths and fairness. The author then turned fairness measures into constraints to enforce certain\nfairness principles, thereby framing fair policy learning into a constrained optimization problem.\nTo ensure fairness and prevent discrimination, it is sometimes necessary to consider counterfactual reason-\ning (Plecko & Bareinboim, 2022; Pearl & Mackenzie, 2018, Chapter 9). For example, in Carson v. Bethlehem\nSteel Corporation (1996) 8, the ruling stated that the core of discrimination issues lies in determining whether\nan individual or a group would have been treated differently by altering only the sensitive attribute (e.g., sex,\nage, and race) while keeping other factors constant. This is apparently a counterfactual statement. Zhang\n& Bareinboim (2018) introduced the SCM framework in fair decision-making problems, which provides re-\nsearchers with a formal language to discuss fairness issues, particularly regarding counterfactual queries.\nBased on the causal language, the authors propose counterfactual direct effects, indirect effects, and spuri-\nous effects, which correspond to different types of discrimination. Further, the authors derived the causal\nexplanation formula, which quantitatively analyzes and explains the observed disparities of decisions and\nhelps us to examine the influence of discrimination within the decision-making process. Huang et al. (2022c)\nstudied fairness in recommendation scenarios, focusing on the contextual bandits problem. They employed\ncausal graphs to formally analyze the fairness concerns related to this problem, introducing a concept known\nas counterfactual individual fairness. Specifically, this concept involves evaluating the expected reward an\nindividual would have received if placed in a different sensitive group. They designed an algorithm that esti-\nmates the fairness discrepancy using do-calculus. The experimental results demonstrate that their proposed\napproach not only enhances fairness but also maintains good recommendation performance.\nVarious types of fairness measures have been studied extensively in fairness literature. However, as high-\nlighted by Kusner et al. (2017) and Zhang & Bareinboim (2018), fairness measures built upon correla-\ntions (Zafar et al., 2015; Wen et al., 2021), e.g., demographic parity and equal opportunity, do not explicitly\ndifferentiate the mechanisms through which sensitive attributes or other factors influence outcomes, which\nmay increase discrimination in certain scenarios. Plecko & Bareinboim (2022) proposed a framework for\nfairness analysis through a causal lens, relating the observed changes to the unobservable causal mecha-\nnisms. They articulated a principled framework to quantify and decompose fairness measures, as well as the\ninterplay between these measures when examining fairness from a population to an individual level. The\n8https://caselaw.findlaw.com/us-7th-circuit/1304532.html\n25\nauthors also provided insightful illustrations of these concepts using a range of examples. Finally, we note\nthat there has been a growing body of research papers investigating the intersection of causality and fair\ndecision-making in recent years (Nabi et al., 2019; Creager et al., 2020; Zhang et al., 2020c; Tang et al.,\n2023; Plecko & Bareinboim, 2023), but how to apply causal reinforcement learning techniques to solve such\nproblems remains to be further explored.\n6.3\nSafety\n6.3.1\nSafety in Reinforcement Learning\nSafety is a crucial concern in RL (Garcƒ±a & Fern√°ndez, 2015; Gu et al., 2022). RL agents may sometimes\nact unexpectedly, especially when faced with unseen situations. This issue poses a significant risk in safety-\ncritical applications, such as healthcare or autonomous vehicles, where even a single error could have severe\nconsequences. Additionally, RL agents may prioritize higher returns over their own safety, known as the\nagent safety problem (Fulton & Platzer, 2018; Beard & Baheri, 2022). For instance, in domains like robotic\ncontrol, agents may sacrifice their lifespan for a higher mission completion rate. Addressing these safety\nconcerns and developing robust methods to ensure safe decision-making in RL are vital for the practical\ndeployment of RL systems in real-world applications.\n6.3.2\nSafe Reinforcement Learning with Causality\nSafe RL problems are typically formulated as constrained MDPs (Altman, 1995; 1999), which extend MDPs\nby incorporating an additional constraint set to express various safety concerns. Existing methods primarily\nfocus on preventing constraint violations (Achiam et al., 2017; Chow et al., 2017), and seldom explicitly\nconsidering causality. Causal inference provides some valuable tools for studying safety. As an example,\nHart & Knoll (2020) investigated the safety issue relates to autonomous driving. Researchers can conduct\ncounterfactual policy evaluations before deploying any policy to the real world by utilizing counterfactual\nreasoning. The experimental results show that their method demonstrated a high success rate while signifi-\ncantly reducing the collision rate. On the other hand, Everitt et al. (2021) studied a critical concern known\nas reward tampering, which refers to the potential for RL agents to manipulate their reward signals. This\nmanipulation can lead to unintended consequences and undermine the effectiveness of the learning process,\nthus posing a potential safety threat. In this paper, the authors presented a set of design principles aimed\nat developing RL agents that are robust against reward tampering, ensuring their behavior remains aligned\nwith the intended objectives. To establish these design principles, the authors developed a causal framework\nsupported by causal graphs, which provide a precise and intuitive understanding of the reward tampering\nissue. In summary, incorporating causality helps identify potential safety threats, develop preventive so-\nlutions, and trace the causes behind unexpected outcomes, thereby preventing RL agents from repeatedly\nbreaching safety constraints. As a result, RL methods and systems can be employed safely and responsibly,\nreducing the risk of catastrophic consequences.\nWe summarize the core ideas discussed spanning sections 3 to 6 by presenting Figure 10. This schematic\ndiagram captures the various approaches to integrating causality into the reinforcement learning process,\nhighlighting the key components and their interactions. As we conclude this section, we recognize that while\nsignificant progress has been made in the field of causal RL, there remain important yet underexplored\navenues for future research and development. Many open problems persist within the four core challenges\nwe have discussed. In the final section of this paper, we turn our attention to the limitations and future\ndirections of causal RL. By examining these aspects, we aim to inspire future studies and contribute to the\ncontinued advancement of causal RL, paving the way for new breakthroughs and applications.\n7\nLimitations and Future Directions\n7.1\nLimitations\nSo far, we have demonstrated that causal RL methods hold great promise in enhancing the decision-making\ncapabilities of RL agents by enabling them to understand and leverage causal relationships. However, it\n26\nEnvironment\nOther Environments\nAgent\nBuffer\nModel\nPolicy\n/Value\nfunctions\nOther Sources of Data\nHuman\n1\n6\n5\n4\n7\n8\n9\n10\n2\n3\nFigure 10: A schematic diagram illustrating the integration of causality into the reinforcement learning\nprocess.\nThe numbered edges represent some key components: 1) Abstraction and extraction of causal\nrepresentations from raw observations; 2) Directed exploration guided by causal knowledge; 3) Fusing (pos-\nsibly confounded) data; 4) Incorporating causal assumptions or knowledge from humans.\n5) Providing\ncausality-based explanations; 6) Generalization and knowledge transfer; 7) Learning causal world models; 8)\nCounterfactual data generation; 9) Planning with world models; 10) Enhanced training of policies and value\nfunctions with causal reasoning.\nis crucial to acknowledge the limitations associated with these methods. In this part, we will outline the\nlimitations that researchers and practitioners may encounter when employing causal RL techniques.\nOne of the foremost limitations lies in the requirement for domain knowledge. Many causal RL methods\nheavily rely on causal graphs, thus, making accurate causal assumptions is of significance. For example, when\ndealing with unobserved confounding, the use of proxy variables and instrumental variables may introduce\nadditional risks (Martens et al., 2006; Sainani, 2018; Cui et al., 2023). An inaccurate representation of the\nvariables of interest through proxies can introduce new confounding or other forms of biases. In terms of\ninstrumental variables, meeting all the rigorous conditions in its definition or establishing and validating\nthese conditions in practical scenarios can be highly challenging. These limitations highlight the importance\nof carefully considering the underlying assumptions and the potential risks associated with the lack of\nknowledge.\nIn some real-world scenarios, the raw data is often unstructured, such as images and textual data. The causal\nvariables involved in the data generation process may remain unknown, necessitating the development of\napproaches to extract causal representations from high-dimensional raw data.\nHowever, the presence of\nunobserved confounders may complicate the representation learning process. Learning a causal model in\na latent space is generally infeasible without sufficient domain knowledge.\nEffectively extracting causal\nrepresentations thus relies on making certain assumptions (Sch√∂lkopf et al., 2021).\nLearning a causally\ncorrect model with limited prior knowledge poses a significant challenge (Rezende et al., 2020). In certain\nscenarios, acquiring a causal model can be more demanding than directly learning the optimal policies, which\nmay offset the sample efficiency gains.\nMoreover, scaling causal RL to complex environments presents a significant challenge due to increased\ncomputational costs and model complexity. For example, the algorithm proposed by Sontakke et al. (2021)\nexhibits exponential growth in complexity. To simplify the learning problem, Buesing et al. (2019) assumed\naccess to the true transition function and reward function in their experiments, which significantly limits the\npractical applicability of the proposed method.\nWhen focusing on generalizability, we observe additional limitations.\nMany causal RL methods aimed\nat generalizability adopt causal representation learning or dynamics learning. These approaches generally\nrequire learning from multi-domain data or allowing for explicit interventions in environmental components\nto simulate the generation of interventional data. The quality and granularity of the learned representations\n27\nTable 5: The three components of causal learning.\nAvailable information\nTargets to identify\nTypical questions\nCausal representation learning\nObservations\nCausal variables\n(representation)\nWhat factors account for the\nchange in position?\nCausal discovery\nCausal variables\nCausal graph\nDoes mass determine the change in\nposition of an object?\nCausal mechanisms learning\nCausal graph\nCausal mechanisms\nHow mass determine the change in\nposition of an object?\nclosely rely on which distributional shifts, interventions or relevant signals are available (Sch√∂lkopf et al.,\n2021), while agents often have access to only a limited number of domains in reality.\nLastly, it is important to acknowledge the limitations associated with counterfactual reasoning. Obtaining\naccurate and reliable counterfactual estimates often requires making strong assumptions about the underly-\ning causal structure (Shpitser & Pearl, 2007; Correa et al., 2021; Zhang et al., 2022b), as counterfactuals, by\ndefinition, cannot be directly observed. Some counterfactual quantities are almost never identifiable while\nsome are identifiable under certain assumptions, such as the effect of treatment on the treated (ETT) (Pearl,\n2009b, Chapter 11). Furthermore, the computational complexity of counterfactual reasoning can be a bot-\ntleneck, especially when dealing with high-dimensional state and action spaces. This complexity can hinder\nreal-time decision-making in complex tasks, which remains an ongoing challenge.\n7.2\nCausal Learning in Reinforcement Learning\nIn section 3.3 and section 5.2, we explained how causal dynamics learning - a class of methods closely related\nto MBRL - can improve generalizability and sample efficiency (Wang et al., 2022; Huang et al., 2022b). These\nmethods focus on understanding the cause-and-effect relationships between variables and the process that\ngenerates these variables. Instead of using complex, redundant connections, to model the data generation\nprocess, these methods prefer a sparse, modular style. As a result, they are more efficient and stable than\ntraditional model-based methods and allow RL agents to adapt quickly to unseen environments or tasks.\nHowever, we may not have perfect knowledge of the causal variables in reality. Sometimes, we must deal\nwith high-dimensional and unstructured data like visual information. In this case, RL agents need to be able\nto extract causal representations from raw data (Sch√∂lkopf et al., 2021). Depending on the tasks of interest,\ncausal representations can take various forms, ranging from abstract concepts like emotions and preferences\nto more concrete entities such as physical objects.\nThe complete process of learning a causal model from raw data is known as causal learning (Peters et al.,\n2017).\nIt is different from causal reasoning (Imbens & Rubin, 2015; Glymour et al., 2016), which only\nfocuses on estimating specific causal effects given the causal model. Causal learning involves extracting causal\nrepresentation, discovering causal relationships, and learning causal mechanisms. Table 5 briefly summarizes\ntheir characteristics. All three of these components are significant and deserve further investigation. A great\ndeal of research has been done on causal discovery (Spirtes et al., 2000; Pearl, 2009b; Peters et al., 2017;\nVowels et al., 2022), a process of recovering the causal structure of a set of variables from data, particularly\nconcerning conditional independence tests (Spirtes et al., 2000; Sun et al., 2007; Hoyer et al., 2008; Zhang\net al., 2011). Under certain assumptions, such as faithfulness, algorithms can identify the Markov equivalence\nclass of the underlying causal graph from observational data. It is very difficult to uniquely identify a causal\ngraph from observational data without strong assumptions about the data generation process. Therefore,\nsome research efforts in causal inference have been done on partially identifiable causal graphs, such as\nmaximally oriented partially directed acyclic graphs (MPDAGs) (Meek, 1995; Perkovic et al., 2017; Perkovic,\n2020). With additional background knowledge, we can identify more causal directions, going beyond Markov\nequivalence classes. Nonetheless, there remains a dearth of research on effectively leveraging these partially\ndirected graphs to address research challenges in reinforcement learning. This represents a promising area\nfor future exploration.\nIn addition, combining RL with causal discovery empowers an agent to actively\ngather interventional data from the environment to recover the underlying causal structure. This opens up\nan intriguing research direction that focuses on exploring methods for leveraging interventional data, or a\n28\ncombination of observational and interventional data, to facilitate efficient causal discovery (Addanki et al.,\n2020; Jaber et al., 2020; Brouillard et al., 2020; Zhu et al., 2022a).\nAs for causal representation learning (Sch√∂lkopf et al., 2021; Wang & Jordan, 2022; Shen et al., 2022), one\npossible solution is to learn latent factors from high-dimensional observations using autoencoders (Yang\net al., 2021a; Eghbal-zadeh et al., 2021; Tran et al., 2022). These methods can approximatively recover\ncausal representations and structures by virtue of carefully designed constraint terms. This idea inherently\nembeds an SCM into the learner, implicitly binding causal discovery and causal mechanisms learning in one\nsolution. Additionally, in scenarios involving multiple environments or tasks, causal representations can also\nbe derived through techniques like mining invariance (Zhang et al., 2020a; Bica et al., 2021b; Saengkyongam\net al., 2022) or clustering trajectories from diverse domains (Sontakke et al., 2021). However, determining the\noptimal number and granularity of causal variables remains challenging, as the optimal causal representations\noften depend on the specific task.\nOverall, causal learning in RL is an underexplored problem and has the potential to advance the RL com-\nmunity. Additionally, RL techniques show promise in contributing to the field of causal learning. As we\ndelve deeper into the connections between these two fields, an exchange of insights can nurture reciprocal\nbenefits, propelling both fields forward (Zhu et al., 2022a).\n7.3\nCausality-aware Multitask and Meta Reinforcement Learning\nMultitask reinforcement learning (Parisotto et al., 2015; Teh et al., 2017; D‚ÄôEramo et al., 2020; Vithay-\nathil Varghese & Mahmoud, 2020) focuses on simultaneously learning multiple tasks by sharing knowledge\nand leveraging synergies across tasks. This scenario commonly arises in robot manipulation, where a robot\nneeds to acquire various skills, such as reaching, grasping, and pushing. Meta-learning (Duan et al., 2016;\nFinn et al., 2017; Gupta et al., 2018; Xu et al., 2018), on the other hand, involves training on a task dis-\ntribution to gain the ability to adapt quickly to a new task. Impressive results have been achieved without\nexplicitly considering causality, leading to the question: Is training a high-capacity model on a diverse range\nof tasks sufficient to generalize to new tasks?\nRecent research empirically validates this hypothesis, as\nlarge pre-trained models on diverse datasets have demonstrated remarkable performance in tasks requiring\nfew-shot learning or even zero-shot generalization ability (Brown et al., 2020; Wei et al., 2021).\nAs shown in Figure 5, different tasks are essentially different interventions in the data generation pro-\ncess (Sch√∂lkopf et al., 2021), so it is not surprising that models trained on multiple tasks can achieve excellent\ngeneralizability ‚Äì they implicitly learn the causal relationships governing the data generation process of these\ntasks. Dasgupta et al. (2018) provide compelling evidence for this proposition. Their study demonstrates\nthat the capability of causal inference may emerge from large-scale meta-RL. Nonetheless, testing all possible\ninterventions and their combinations in real-world scenarios is impractical. This is where causal modeling\ncomes into play. As discussed in the previous section, causal models enable the explicit incorporation of prior\nknowledge, helping agents to align their understanding of the world with human cognition. Moreover, causal\nrelationships facilitate problem abstraction (Wang et al., 2022), thereby eliminating the need of testing irrel-\nevant interventions. Additionally, agents that organize their knowledge using causal structures benefit from\nthe stability of causal relationships. While non-causal agents would require finetuning the whole model for\neven a slightly changed task, a causality-aware agent would only need to adjust a few modules (Huang et al.,\n2022a), exhibiting a stronger knowledge transfer ability. These properties may also contribute to lifelong (or\ncontinual) learning (Xie & Finn, 2022; Khetarpal et al., 2022), allowing for fast adaptation to new tasks that\narise in sequence.\n7.4\nHuman-in-the-loop Learning and Reinforcement Learning from Human Feedback\nHuman-in-the-loop learning (HiLL) (Mosqueira-Rey et al., 2022) is a form of machine learning in which\nhumans actively participate in the development cycle of machine learning models or algorithms. This can\ninvolve providing labels, preferences, or other types of feedback.\nWhen the data or task being learned\nis complex or requires high levels of cognition, HiLL often produces better results because humans can\nprovide valuable insights or knowledge to the model that it may be difficult for the model to learn on its\nown (Mosqueira-Rey et al., 2022; Zhang & Bareinboim, 2022a).\n29\nIn the context of RL, HiLL typically involves incorporating human expertise into the MDP to replace the\nreward function that provides feedback signals. This approach allows us to train RL agents with the help\nof human knowledge and values, alleviating the challenges associated with defining a sophisticated reward\nfunction (Zhang & Bareinboim, 2022a). This idea is closely related to RLHF (Reinforcement Learning from\nHuman Feedback) (Christiano et al., 2017), a concept that has gained increasing attention recently in the\ntraining of large language models (Ziegler et al., 2020; Glaese et al., 2022; Ouyang et al., 2022), where human\ninstructors provide rewards (or penalties) to a model to encourage (or discourage) certain behaviors. From\na causal perspective, humans can provide machine learning models with a strong understanding of causality\nbased on their knowledge of the world, which can help filter out behaviors that may lead to negative outcomes.\nHowever, it is important to note that humans and machines may have different observations or perceptions of\nthe world, and non-causal-aware RL agents may be influenced by confounding variables (Gasse et al., 2023).\nIn addition, we often need to consider the issue of limited budgets, as our goal is to provide meaningful\nfeedback to RL agents at the lowest possible cost. Finally, in addition to scalar feedback, we may also\nprovide more informative feedback to agents in the form of counterfactuals (Karalus & Lindner, 2022).\n7.5\nTheoretical Advances in Causal Reinforcement Learning\nTheoretical research in causal RL has mainly concentrated on the MAB problem. Lattimore et al. (2016)\nfirst introduced the causal bandit problem, where interventions are treated as arms, and their impact on the\nreward and other observable covariates is associated with a known causal graph. Unlike contextual bandits,\nin this setting, the observations occur after each intervention is made. Using the structural knowledge from\nthe causal model, the agent can efficiently identify good interventions, achieving strictly better regret than\nalgorithms that lack causal information. Sen et al. (2017) further considered incorporating prior knowledge\nabout the strength of interventions. Since different arms (interventions) share the same causal graph, samples\nfrom one arm can inform us about the expected value of other arms. The authors demonstrated significant\ntheoretical and practical improvements by leveraging such information leakage.\nHowever, these works only focused on intervening in a single node on the causal graph, where causal effects\npropagate only to the first-order neighbors of the intervened node. Yabe et al. (2018) generalize this setting\nby studying an arbitrary set of interventions, allowing for causal effects to propagate throughout the causal\ngraph. Lee & Bareinboim (2018) also investigated the scenario of pulling arms in a combinatorial manner.\nThey showed that considering all interventable variables as one arm may lead to a suboptimal policy, and\nthe structural information can be used to identify the minimal intervention set that is worth intervening in.\nLu et al. (2021) improved the na√Øve bounds derived in Lattimore et al. (2016), devising algorithms for causal\nbandit problems with sublinear cumulative regret bounds. Nair et al. (2021) study the causal bandit problem\nwith budget constraints. In this setting, there is a trade-off between the more economical observational arm\n(i.e., no interventions on the causal graph) and the high-cost interventional arms. While the observational\narm is less rewarding, it offers valuable information for studying other arms at a significantly lower cost.\nMore recently, Kroon et al. (2022) introduced the concept of ‚Äúseparating set‚Äù from causal discovery to\ncausal bandit problems, which renders a target variable independent of a context variable when conditioned\nupon. By utilizing this separating set, the authors developed a bandit algorithm that does not rely on the\nassumption of prior causal knowledge. On the other hand, Bilodeau et al. (2022) studied the adaptivity\nissue concerning the d-separator, i.e., whether an algorithm can perform nearly as well as an algorithm with\noracle knowledge of the presence or absence of a d-separator.\nWhile causal bandit problems have yielded fruitful results, there have been notably fewer studies dedicated\nto MDP problems.\nAs discussed in Section 4.3, some studies focused on DTRs (Zhang & Bareinboim,\n2019; 2020), which can be modeled as an MDP with a global confounder variable.\nAdditionally, there\nhave been some investigations into confounded MDP (Zhang & Bareinboim, 2016; Wang et al., 2021b),\nwhich extend the concept of DTR by admitting the presence of unobserved confounders at each time step.\nIn the theory of causal RL, aside from understanding how causal information, especially causal graphs,\nenhance the regret bounds, the identifiability of causal effects (Zhang et al., 2020b; Lu et al., 2022) and\nstructure (Huang et al., 2022a) are also of great interest. While existing research provides valuable insights\ninto the theoretical foundations of causal RL, further theoretical advancements are necessary. In addition\nto helping us comprehend the reasons behind the success of existing causal RL methods, we also hope that\n30\nfuture theoretical advances will pave the way for designing novel and effective approaches that are both\ninterpretable and robust for real-world applications.\n7.6\nBenchmarking Causal Reinforcement Learning\nIn RL, we are typically interested in the efficiency and convergence of the algorithm. Atari 2600 Games\nand Mujoco locomotion tasks (Brockman et al., 2016) are commonly used as benchmarks for discrete and\ncontinuous control problems. There are also experimental environments that evaluate the generalizability\nand robustness of RL, such as Procgen (Cobbe et al., 2020). Some benchmarks focus on multitask learning,\nmeta-learning, and curriculum learning for reinforcement learning, such as RLBench (James et al., 2019),\nMeta-World (Yu et al., 2021), Alchemy (Wang et al., 2021a), and Causal-World Ahmed et al. (2022). Among\nthem, Causal-World offers a wide range of robotic manipulation tasks that share a common attribute set\nand structure. In these tasks, the robot is required to construct a goal shape using the provided building\nblocks. One notable feature of this benchmark is its provision of interfaces that allow manual modification\nof object attributes such as size, mass, and color. This enables researchers to intervene in these attributes\nand generate a series of tasks with consistent causal structures.\nSince causal RL is not limited to a particular type of problem, evaluation metrics may vary depending on the\nspecific mission. While existing experimental environments have provided good benchmarks for evaluating\nalgorithms with various metrics, the underlying data generation processes in these environments often remain\nopaque, concealed within game simulators or physics engines. This lack of transparency makes it difficult\nfor researchers to fully understand the causal mechanisms behind the problems they are attempting to solve,\nhindering the development of the field. Recently, Ke et al. (2021) proposed a new set of environments focusing\non causal discovery in visual-based RL, allowing researchers to specify the causal graph and customize its\ncomplexity. Nevertheless, as we demonstrated in sections 5 to 6, a large portion of the existing research\nstill relies on toy environments to evaluate the effectiveness of algorithms. Furthermore, in addition to the\npreviously mentioned properties, a good benchmark should consider the multiple factors comprehensively, as\ndiscussed in section 6. Thus, developing a comprehensive benchmark for assessing the performance of causal\nRL methods remains an ongoing challenge.\n7.7\nReal-world Causal Reinforcement Learning\nFinally, it is worth noting that the practical implementation of causal RL in real-world applications remains\nlimited. To make it more widely applicable, we must carefully examine the challenges posed by reality.\nDulac-Arnold et al. (2020; 2021) identified nine critical challenges that are holding back the use of RL in the\nreal world: limited samples; unknown and large delays; high-dimensional input; safety constraints; partial\nobservability; multi-objective or unspecified reward functions; low latencies; offline learning; and the need\nfor explainability.\nWe have discussed some of these issues throughout this paper, many of which are related to ignorance of\ncausality. For instance, the challenge of learning from limited samples corresponds to the sample efficiency\nissue discussed in section 5.2. Learning from high-dimensional inputs and multiple reward functions relates to\nthe generalization problem outlined in section 3.3. Offline learning raises concerns about spurious correlations\n(section 4.3), and security and explainability are covered in section 6.\nAlthough causal models offer promising solutions to these real-world challenges, current experimental envi-\nronments often fall short of meeting the research needs. As discussed in section 7.6, popular benchmarks\nare often treated as black boxes, and researchers have limited access to and understanding of the causal\nmechanisms by which these black boxes generate data. This lack of transparency significantly hinders the\ndevelopment of this research field. In order to facilitate the widespread adoption and application of causal\nRL, it is crucial to address these limitations and cultivate a culture of causal thinking.\n31\n8\nConclusion\nIn summary, Causal RL holds promise for tackling complex decision-making problems under uncertainty.\nIt represents an understudied yet significant research direction. By explicitly integrating assumptions or\nknowledge about the causal relationship underlying the data generation process, causal RL algorithms can\nlearn optimal policies more efficiently and make more informed decisions. In this survey, we aimed to clarify\nthe terminologies and concepts related to causal RL and to establish connections between existing work.\nWe proposed a problem-oriented taxonomy and systematically discussed and analyzed the latest advances\nin causal RL, focusing on how they address the four critical challenges facing RL.\nWhile there is still much work to be done in this field, the results to date are encouraging. They suggest\nthat causal RL has the potential to significantly improve the performance of RL systems in a wide range of\nproblems. Here, we summarize the key conclusions of this survey.\n‚Ä¢ Causal RL is an emerging branch of RL that emphasizes understanding and utilizing causality to\nmake better decisions (section 2.3).\n‚Ä¢ Causal modeling has the potential to enhance generalization (section 3.3) and sample efficiency\n(section 5.2), yet it comes with fundamental challenges and limitations that require careful consid-\neration (section 7.1). With limited causal information, RL agents may need to learn about causal\nrepresentation and environmental dynamics from raw data (section 7.2).\n‚Ä¢ Causal effects and relationships are identifiable under certain conditions (section 2.3 and section 7.5),\nenabling agents to effectively utilize observational data. Furthermore, multitask learning and meta-\nlearning may help facilitate causal learning (section 7.3) In turn, harnessing causality can enhance\nthe ability to transfer knowledge and effectively address a wide range of tasks (section 3.3).\n‚Ä¢ Correlation does not imply causation. Spurious correlations can lead to a distorted understanding\nof the environment and task, resulting in suboptimal policies (section 4.3). In addition to employ-\ning causal reasoning techniques, leveraging human understanding of causality may further enhance\nreinforcement learning (section 7.4).\n‚Ä¢ In real-world applications, performance is not the only concern.\nFactors such as explainability,\nfairness, and security, must also be considered (section 6). Current benchmarks call for increased\ntransparency and a comprehensive, multi-faceted evaluation protocol for reinforcement learning (sec-\ntion 7.6), which has significant implications for advancing real-world applications of causal reinforce-\nment learning (section 7.7).\nWe hope this survey will help establish connections between existing work in causal reinforcement learning,\ninspire further exploration and development, and provide a common ground and comprehensive resource for\nthose looking to learn more about this exciting field.\nReferences\nJoshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Interna-\ntional Conference on Machine Learning, pp. 22‚Äì31. PMLR, 2017.\nRaghavendra Addanki, Shiva Kasiviswanathan, Andrew Mcgregor, and Cameron Musco. Efficient Interven-\ntion Design for Causal Discovery with Latents. In Proceedings of the 37th International Conference on\nMachine Learning, pp. 63‚Äì73. PMLR, November 2020.\nOssama Ahmed, Frederik Tr√§uble, Anirudh Goyal, Alexander Neitz, Manuel Wuthrich, Yoshua Bengio,\nBernhard Sch√∂lkopf, and Stefan Bauer. CausalWorld: A Robotic Manipulation Benchmark for Causal\nStructure and Transfer Learning.\nIn International Conference on Learning Representations, February\n2022.\nEitan Altman. Constrained Markov Decision Processes. PhD thesis, INRIA, 1995.\n32\nEitan Altman. Constrained Markov Decision Processes: Stochastic Modeling. Routledge, 1999.\nJoshua D. Angrist, Guido W. Imbens, and Donald B. Rubin. Identification of Causal Effects Using Instru-\nmental Variables. Journal of the American Statistical Association, 91(434):444‚Äì455, June 1996. ISSN\n0162-1459. doi: 10.1080/01621459.1996.10476902.\nChenjia Bai, Lingxiao Wang, Yixin Wang, Zhaoran Wang, Rui Zhao, Chenyao Bai, and Peng Liu. Addressing\nHindsight Bias in Multigoal Reinforcement Learning. IEEE Transactions on Cybernetics, pp. 1‚Äì14, 2021.\nISSN 2168-2275. doi: 10.1109/TCYB.2021.3107202.\nMichael Baiocchi, Jing Cheng, and Dylan S. Small. Instrumental variable methods for causal inference.\nStatistics in Medicine, 33(13):2297‚Äì2340, 2014. ISSN 1097-0258. doi: 10.1002/sim.6128.\nSreejith Balakrishnan, Jianxin Bi, and Harold Soh.\nSCALES: From Fairness Principles to Constrained\nDecision-Making. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, AIES\n‚Äô22, pp. 46‚Äì55, New York, NY, USA, July 2022. Association for Computing Machinery. ISBN 978-1-4503-\n9247-1. doi: 10.1145/3514094.3534190.\nElias Bareinboim and Judea Pearl. Causal inference and the data-fusion problem. Proceedings of the National\nAcademy of Sciences, 113(27):7345‚Äì7352, July 2016. doi: 10.1073/pnas.1510507113.\nElias Bareinboim, Jin Tian, and Judea Pearl.\nRecovering from Selection Bias in Causal and Statistical\nInference. Proceedings of the AAAI Conference on Artificial Intelligence, 28(1), June 2014. ISSN 2374-\n3468. doi: 10.1609/aaai.v28i1.9074.\nElias Bareinboim, Andrew Forney, and Judea Pearl.\nBandits with Unobserved Confounders: A Causal\nApproach. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.,\n2015.\nElias Bareinboim, Juan D. Correa, Duligur Ibeling, and Thomas Icard. On Pearl‚Äôs hierarchy and the foun-\ndations of causal inference. In Probabilistic and Causal Inference: The Works of Judea Pearl, pp. 507‚Äì556.\n2022.\nAndrew G. Barto. Intrinsic Motivation and Reinforcement Learning. In Gianluca Baldassarre and Marco\nMirolli (eds.), Intrinsically Motivated Learning in Natural and Artificial Systems, pp. 17‚Äì47. Springer,\nBerlin, Heidelberg, 2013. ISBN 978-3-642-32375-1. doi: 10.1007/978-3-642-32375-1_2.\nJared J. Beard and Ali Baheri. Safety Verification of Autonomous Systems: A Multi-Fidelity Reinforcement\nLearning Approach. arXiv preprint arXiv:2203.03451, 2022.\nAndrew Bennett, Nathan Kallus, Lihong Li, and Ali Mousavi. Off-policy Evaluation in Infinite-Horizon\nReinforcement Learning with Latent Confounders. In Proceedings of The 24th International Conference\non Artificial Intelligence and Statistics, pp. 1999‚Äì2007. PMLR, March 2021.\nIoana Bica, Daniel Jarrett, Alihan H√ºy√ºk, and Mihaela van der Schaar. Learning \"What-if\" Explanations\nfor Sequential Decision-Making. In International Conference on Learning Representations, March 2021a.\nIoana Bica, Daniel Jarrett, and Mihaela van der Schaar. Invariant Causal Imitation Learning for Generaliz-\nable Policies. In Advances in Neural Information Processing Systems, volume 34, pp. 3952‚Äì3964. Curran\nAssociates, Inc., 2021b.\nBlair Bilodeau, Linbo Wang, and Daniel M. Roy. Adaptively Exploiting d-Separators with Causal Bandits.\nIn Advances in Neural Information Processing Systems, October 2022.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. OpenAI Gym, June 2016.\nPhilippe Brouillard, S√©bastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin.\nDifferentiable Causal Discovery from Interventional Data. In Advances in Neural Information Processing\nSystems, volume 33, pp. 21865‚Äì21877. Curran Associates, Inc., 2020.\n33\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are\nFew-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877‚Äì1901.\nCurran Associates, Inc., 2020.\nLars Buesing, Theophane Weber, Yori Zwols, Nicolas Heess, Sebastien Racaniere, Arthur Guez, and Jean-\nBaptiste Lespiau. Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search. In International\nConference on Learning Representations, February 2019.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation.\nIn International Conference on Learning Representations, February 2022.\nAlycia N. Carey and Xintao Wu. The Causal Fairness Field Guide: Perspectives From Social and Formal\nSciences. Frontiers in Big Data, 5, 2022. ISSN 2624-909X. doi: 10.3389/fdata.2022.892837.\nYinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement\nlearning with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070‚Äì6120, 2017.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep Rein-\nforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017.\nKarl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural Generation to\nBenchmark Reinforcement Learning, July 2020.\nJuan Correa, Sanghack Lee, and Elias Bareinboim. Nested Counterfactual Identification from Arbitrary\nSurrogate Experiments. In Advances in Neural Information Processing Systems, volume 34, pp. 6856‚Äì\n6867. Curran Associates, Inc., 2021.\nElliot Creager, David Madras, Toniann Pitassi, and Richard Zemel. Causal modeling for fairness in dynamical\nsystems. In International Conference on Machine Learning, pp. 2185‚Äì2195. PMLR, 2020.\nPeng Cui and Susan Athey.\nStable learning establishes some common ground between causal inference\nand machine learning. Nature Machine Intelligence, 4(2):110‚Äì115, February 2022. ISSN 2522-5839. doi:\n10.1038/s42256-022-00445-z.\nYifan Cui, Hongming Pu, Xu Shi, Wang Miao, and Eric Tchetgen Tchetgen.\nSemiparametric Proximal\nCausal Inference. Journal of the American Statistical Association, 0(0):1‚Äì12, 2023. ISSN 0162-1459. doi:\n10.1080/01621459.2023.2191817.\nAlexander D‚ÄôAmour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, David Sculley, and Yoni Halpern.\nFairness is not static: Deeper understanding of long term fairness via simulation studies. In Proceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency, pp. 525‚Äì534, 2020. doi: 10.1145/\n3351095.3372878.\nIshita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes,\nPeter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal Reasoning from Meta-reinforcement\nlearning. arXiv preprint arXiv:1901.08162, December 2018.\nZhihong Deng, Zuyue Fu, Lingxiao Wang, Zhuoran Yang, Chenjia Bai, Zhaoran Wang, and Jing Jiang.\nSCORE: Spurious COrrelation REduction for Offline Reinforcement Learning, October 2021.\nCarlo D‚ÄôEramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing Knowledge\nin Multi-Task Deep Reinforcement Learning. In International Conference on Learning Representations,\nMarch 2020.\n34\nIv√°n D√≠az and Mark J. van der Laan. Sensitivity Analysis for Causal Inference under Unmeasured Confound-\ning and Measurement Error Problems. The International Journal of Biostatistics, 9(2):149‚Äì160, November\n2013. ISSN 1557-4679. doi: 10.1515/ijb-2013-0004.\nWenhao Ding, Haohong Lin, Bo Li, and Ding Zhao. Generalizing Goal-Conditioned Reinforcement Learning\nwith Variational Causal Reasoning. In Advances in Neural Information Processing Systems, October 2022.\nWenhao Ding, Laixi Shi, Yuejie Chi, and Ding Zhao. Seeing is not Believing: Robust Reinforcement Learning\nagainst Spurious Correlation, July 2023.\nSimon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a Good Representation Sufficient\nfor Sample Efficient Reinforcement Learning? In International Conference on Learning Representations,\nMarch 2020.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel.\nRL2: Fast\nReinforcement Learning via Slow Reinforcement Learning. arXiv preprint arXiv:1611.02779, November\n2016.\nGabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd\nHester. An empirical investigation of the challenges of real-world reinforcement learning. arXiv preprint\narXiv:2003.11881, 2020.\nGabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd\nHester. Challenges of real-world reinforcement learning: Definitions, benchmarks and analysis. Machine\nLearning, 110(9):2419‚Äì2468, 2021. doi: 10.1007/s10994-021-05961-4.\nHamid Eghbal-zadeh, Florian Henkel, and Gerhard Widmer. Learning to infer unseen contexts in causal\ncontextual reinforcement learning. In Self-supervision for Reinforcement Learning (SSL-RL) Workshop,\nICLR, pp. 11, 2021.\nTom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward tampering problems and\nsolutions in reinforcement learning: A causal influence diagram perspective. Synthese, 198(27):6435‚Äì6467,\nNovember 2021. ISSN 1573-0964. doi: 10.1007/s11229-021-03141-4.\nMartin Faltys, Marc Zimmermann, Xinrui Lyu, Matthias H√ºser, Stephanie Hyland, Gunnar R√§tsch, and\nTobias Merz. Hirid, a high time-resolution icu dataset (version 1.1. 1). Physio. Net, 10, 2021.\nAmir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty,\nJacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch,\nand Diyi Yang. Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation\nand Beyond. Transactions of the Association for Computational Linguistics, 10:1138‚Äì1158, October 2022.\nISSN 2307-387X. doi: 10.1162/tacl_a_00511.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of\nDeep Networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 1126‚Äì1135.\nPMLR, July 2017.\nJakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Coun-\nterfactual multi-agent policy gradients. In Proceedings of the Thirty-Second AAAI Conference on Artificial\nIntelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI\nSymposium on Educational Advances in Artificial Intelligence, AAAI‚Äô18/IAAI‚Äô18/EAAI‚Äô18, pp. 2974‚Äì\n2982, New Orleans, Louisiana, USA, February 2018. AAAI Press. ISBN 978-1-57735-800-8.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.\nIn International Conference on Machine Learning, pp. 2052‚Äì2062. PMLR, 2019.\nNathan Fulton and Andr√© Platzer. Safe reinforcement learning via formal methods: Toward safe control\nthrough proof and learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,\n2018. doi: 10.1609/aaai.v32i1.12107.\n35\nPratik Gajane, Akrati Saxena, Maryam Tavakol, George Fletcher, and Mykola Pechenizkiy. Survey on Fair\nReinforcement Learning: Theory and Practice, May 2022.\nChen Gao, Yu Zheng, Wenjie Wang, Fuli Feng, Xiangnan He, and Yong Li. Causal Inference in Recommender\nSystems: A Survey and Future Directions. arXiv preprint arXiv:2208.12397, August 2022.\nJavier Garcƒ±a and Fernando Fern√°ndez. A comprehensive survey on safe reinforcement learning. Journal of\nMachine Learning Research, 16(1):1437‚Äì1480, 2015.\nMaxime Gasse, Damien Grasset, Guillaume Gaudron, and Pierre-Yves Oudeyer. Using Confounded Data in\nLatent Model-Based Reinforcement Learning. Transactions on Machine Learning Research, March 2023.\nISSN 2835-8856.\nAmirEmad Ghassami, Ilya Shpitser, and Eric Tchetgen Tchetgen. Partial Identification of Causal Effects\nUsing Proxy Variables, April 2023.\nAmelia Glaese, Nat McAleese, Maja Trƒôbacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-\nSen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen,\nDoug Fritz, Jaume Sanchez Elias, Richard Green, So≈àa Mokr√°, Nicholas Fernando, Boxi Wu, Rachel\nFoley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu,\nLisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human\njudgements, September 2022.\nClark Glymour. Review of The Art of Causal Conjecture. Journal of the American Statistical Association,\n93(444):1513‚Äì1515, 1998. ISSN 0162-1459. doi: 10.2307/2670064.\nClark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical\nmodels. Frontiers in genetics, 10:524, 2019. doi: 10.3389/fgene.2019.00524.\nMadelyn Glymour, Judea Pearl, and Nicholas P. Jewell. Causal Inference in Statistics: A Primer. John\nWiley & Sons, 2016.\nMingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Sch√∂lkopf. Do-\nmain Adaptation with Conditional Transferable Components. In Proceedings of The 33rd International\nConference on Machine Learning, pp. 2839‚Äì2848. PMLR, June 2016.\nRobert Grande, Thomas Walsh, and Jonathan How. Sample Efficient Reinforcement Learning with Gaussian\nProcesses.\nIn Proceedings of the 31st International Conference on Machine Learning, pp. 1332‚Äì1340.\nPMLR, June 2014.\nSamuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari\nagents. In International Conference on Machine Learning, pp. 1792‚Äì1801. PMLR, 2018.\nShangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois\nKnoll. A Review of Safe Reinforcement Learning: Methods, Theory and Applications. arXiv preprint\narXiv:2205.10330, 2022.\nHongyi Guo, Qi Cai, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang.\nProvably Efficient Offline Re-\ninforcement Learning for Partially Observable Markov Decision Processes.\nIn Proceedings of the 39th\nInternational Conference on Machine Learning, pp. 8016‚Äì8038. PMLR, June 2022a.\nJiaxian Guo, Mingming Gong, and Dacheng Tao. A Relational Intervention Approach for Unsupervised Dy-\nnamics Generalization in Model-Based Reinforcement Learning. In International Conference on Learning\nRepresentations, March 2022b.\nAbhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-Reinforcement\nLearning of Structured Exploration Strategies. In Advances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc., 2018.\n36\nAssaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv preprint\narXiv:1502.02259, 2015.\nPouya Hamadanian, Malte Schwarzkopf, Siddartha Sen, and Mohammad Alizadeh.\nHow Reinforcement\nLearning Systems Fail and What to do About It. In The 2nd Workshop on Machine Learning and Systems\n(EuroMLSys), 2022.\nPaul L. Harris, Tim German, and Patrick Mills. Children‚Äôs use of counterfactual thinking in causal reasoning.\nCognition, 61(3):233‚Äì259, December 1996. ISSN 0010-0277. doi: 10.1016/S0010-0277(96)00715-9.\nPatrick Hart and Alois Knoll. Counterfactual Policy Evaluation for Decision-Making in Autonomous Driving,\nNovember 2020.\nTue Herlau and Rasmus Larsen. Reinforcement Learning of Causal Variables Using Mediation Analysis.\nProceedings of the AAAI Conference on Artificial Intelligence, 36(6):6910‚Äì6917, June 2022. ISSN 2374-\n3468. doi: 10.1609/aaai.v36i6.20648.\nAlexandre Heuillet, Fabien Couthouis, and Natalia D√≠az-Rodr√≠guez. Explainability in deep reinforcement\nlearning. Knowledge-Based Systems, 214:106685, 2021. doi: 10.1016/j.knosys.2020.106685.\nIrina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew\nBotvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement\nlearning. In International Conference on Machine Learning, pp. 1480‚Äì1490. PMLR, 2017.\nPatrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Sch√∂lkopf. Nonlinear causal\ndiscovery with additive noise models. In Advances in Neural Information Processing Systems, volume 21.\nCurran Associates, Inc., 2008.\nBiwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. AdaRL: What, Where, and How\nto Adapt in Transfer Reinforcement Learning. In International Conference on Learning Representations,\nMay 2022a.\nBiwei Huang, Chaochao Lu, Liu Leqi, Jose Miguel Hernandez-Lobato, Clark Glymour, Bernhard Sch√∂lkopf,\nand Kun Zhang. Action-Sufficient State Representation Learning for Control with Structural Constraints.\nIn Proceedings of the 39th International Conference on Machine Learning, pp. 9260‚Äì9279. PMLR, June\n2022b.\nWen Huang, Lu Zhang, and Xintao Wu. Achieving Counterfactual Fairness for Causal Bandit. Proceedings\nof the AAAI Conference on Artificial Intelligence, 36(6):6952‚Äì6959, June 2022c. ISSN 2374-3468. doi:\n10.1609/aaai.v36i6.20653.\nYimin Huang and Marco Valtorta. Pearl‚Äôs calculus of intervention is complete. In Proceedings of the Twenty-\nSecond Conference on Uncertainty in Artificial Intelligence, UAI‚Äô06, pp. 217‚Äì224, Arlington, Virginia,\nUSA, July 2006. AUAI Press. ISBN 978-0-9749039-2-7.\nStephanie L. Hyland, Martin Faltys, Matthias H√ºser, Xinrui Lyu, Thomas Gumbsch, Crist√≥bal Esteban,\nChristian Bock, Max Horn, Michael Moor, Bastian Rieck, Marc Zimmermann, Dean Bodenham, Karsten\nBorgwardt, Gunnar R√§tsch, and Tobias M. Merz. Early prediction of circulatory failure in the intensive\ncare unit using machine learning. Nature Medicine, 26(3):364‚Äì373, March 2020. ISSN 1546-170X. doi:\n10.1038/s41591-020-0789-4.\nGuido W. Imbens and Donald B. Rubin. Causal Inference in Statistics, Social, and Biomedical Sciences.\nCambridge University Press, 2015.\nKayoko Inagaki and Giyoo Hatano. Young Children‚Äôs Understanding of the Mind-Body Distinction. Child\nDevelopment, 64(5):1534‚Äì1549, 1993. ISSN 1467-8624. doi: 10.1111/j.1467-8624.1993.tb02969.x.\nAlex Irpan. Deep Reinforcement Learning Doesn‚Äôt Work Yet, 2018.\n37\nAmin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal Discovery from Soft\nInterventions with Unknown Targets: Characterization and Learning. In Advances in Neural Information\nProcessing Systems, volume 33, pp. 9551‚Äì9561. Curran Associates, Inc., 2020.\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. RLBench: The Robot Learning\nBenchmark & Learning Environment, September 2019.\nZhijing Jin, Julius von K√ºgelgen, Jingwei Ni, Tejas Vaidhya, Ayush Kaushal, Mrinmaya Sachan, and Bern-\nhard Schoelkopf. Causal Direction of Data Collection Matters: Implications of Causal and Anticausal\nLearning for NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 9499‚Äì9513, Online and Punta Cana, Dominican Republic, 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.emnlp-main.748.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible\ncritical care database. Scientific data, 3(1):1‚Äì9, 2016.\nNicholas K. Jong and Peter Stone. State Abstraction Discovery from Irrelevant State Variables. In IJCAI,\nvolume 8, pp. 752‚Äì757. Citeseer, 2005.\nJean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, and Ricardo Silva. Causal machine learning: A\nsurvey and open problems. arXiv preprint arXiv:2206.15475, 2022.\nSham Machandranath Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University\nCollege London, March 2003.\nKen Kansky, Tom Silver, David A. M√©ly, Mohamed Eldawy, Miguel L√°zaro-Gredilla, Xinghua Lou, Nimrod\nDorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-shot transfer with\na generative causal model of intuitive physics. In International Conference on Machine Learning, pp.\n1809‚Äì1818. PMLR, 2017.\nJakob Karalus and Felix Lindner. Accelerating the Learning of TAMER with Counterfactual Explanations.\narXiv preprint arXiv:2108.01358, 2022.\nNan Rosemary Ke, Aniket Didolkar, Sarthak Mittal, Anirudh Goyal, Guillaume Lajoie, Stefan Bauer, Danilo\nRezende, Yoshua Bengio, Christopher Pal, and Michael Mozer. Systematic Evaluation of Causal Discovery\nin Visual Model Based Reinforcement Learning. arXiv preprint arXiv:2107.00848, pp. 13, 2021.\nKhimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup.\nTowards Continual Reinforcement\nLearning: A Review and Perspectives. Journal of Artificial Intelligence Research, 75:1401‚Äì1476, December\n2022. ISSN 1076-9757. doi: 10.1613/jair.1.13673.\nRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt√§schel. A Survey of Generalisation in Deep\nReinforcement Learning. arXiv preprint arXiv:2111.09794, January 2022.\nBarbara Koslowski and Amy Masnick. The Development of Causal Reasoning. In Usha Goswami (ed.),\nBlackwell Handbook of Childhood Cognitive Development, pp. 257‚Äì281. Blackwell Publishers Ltd, Malden,\nMA, USA, January 2002. ISBN 978-0-470-99665-2 978-0-631-21840-1. doi: 10.1002/9780470996652.ch12.\nArnoud De Kroon, Joris Mooij, and Danielle Belgrave.\nCausal Bandits without prior knowledge using\nseparating sets. In Proceedings of the First Conference on Causal Learning and Reasoning, pp. 407‚Äì427.\nPMLR, June 2022.\nKun Kuang, Lian Li, Zhi Geng, Lei Xu, Kun Zhang, Beishui Liao, Huaxin Huang, Peng Ding, Wang Miao,\nand Zhichao Jiang. Causal Inference. Engineering, 6(3):253‚Äì263, March 2020. ISSN 2095-8099. doi:\n10.1016/j.eng.2019.08.016.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforce-\nment learning. Advances in Neural Information Processing Systems, 33:1179‚Äì1191, 2020.\n38\nDaniel Kumor, Junzhe Zhang, and Elias Bareinboim. Sequential Causal Imitation Learning with Unobserved\nConfounders. In Advances in Neural Information Processing Systems, volume 34, pp. 14669‚Äì14680. Curran\nAssociates, Inc., 2021.\nMatt J. Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances in neural\ninformation processing systems, 30, 2017.\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David\nSilver, and Thore Graepel. A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.\nIn Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\nFinnian Lattimore, Tor Lattimore, and Mark D. Reid. Causal bandits: Learning good interventions via\ncausal inference. In Proceedings of the 30th International Conference on Neural Information Processing\nSystems, NIPS‚Äô16, pp. 1189‚Äì1197, Red Hook, NY, USA, December 2016. Curran Associates Inc. ISBN\n978-1-5108-3881-9.\nKimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network Randomization: A Simple Technique for\nGeneralization in Deep Reinforcement Learning. In International Conference on Learning Representations,\nMarch 2020a.\nSanghack Lee and Elias Bareinboim. Structural Causal Bandits: Where to Intervene? In Advances in Neural\nInformation Processing Systems, volume 31. Curran Associates, Inc., 2018.\nSanghack Lee, Juan D. Correa, and Elias Bareinboim.\nGeneral Identifiability with Arbitrary Surrogate\nExperiments. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, pp. 389‚Äì398.\nPMLR, August 2020b.\nTabitha E. Lee, Jialiang Alan Zhao, Amrita S. Sawhney, Siddharth Girdhar, and Oliver Kroemer. Causal\nReasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies. In 2021\nIEEE International Conference on Robotics and Automation (ICRA), pp. 4776‚Äì4782, May 2021.\ndoi:\n10.1109/ICRA48506.2021.9561439.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,\nand perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nLuofeng Liao, Zuyue Fu, Zhuoran Yang, Yixin Wang, Mladen Kolar, and Zhaoran Wang. Instrumental\nVariable Value Iteration for Causal Offline Reinforcement Learning, July 2021.\nLydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine\nlearning. In International Conference on Machine Learning, pp. 3150‚Äì3158. PMLR, 2018.\nDavid Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and Leon Bottou. Discovering\nCausal Signals in Images. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 58‚Äì66, Honolulu, HI, July 2017. IEEE. ISBN 978-1-5386-0457-1. doi: 10.1109/CVPR.2017.14.\nChaochao Lu and Jos√© Miguel Hern√°ndez Lobato. Deconfounding Reinforcement Learning in Observational\nSettings. arXiv preprint arXiv:1812.10576, December 2018.\nChaochao Lu, Biwei Huang, Ke Wang, Jos√© Miguel Hern√°ndez-Lobato, Kun Zhang, and Bernhard Sch√∂lkopf.\nSample-Efficient Reinforcement Learning via Counterfactual-Based Data Augmentation, December 2020.\nYangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Causal Bandits with Unknown Graph Structure. In\nAdvances in Neural Information Processing Systems, October 2021.\nYangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Efficient Reinforcement Learning with Prior Causal\nKnowledge. In First Conference on Causal Learning and Reasoning, February 2022.\nFan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui Chen, Weinan Zhang, and Yang Yu. A Survey on Model-based\nReinforcement Learning, June 2022.\n39\nPrashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere.\nExplainable Reinforcement Learning\nthrough a Causal Lens. Proceedings of the AAAI Conference on Artificial Intelligence, 34(03):2493‚Äì2500,\nApril 2020. ISSN 2374-3468. doi: 10.1609/aaai.v34i03.5631.\nEdwin P. Martens, Wiebe R. Pestman, Anthonius de Boer, Svetlana V. Belitser, and Olaf H. Klungel.\nInstrumental Variables: Application and Limitations. Epidemiology, 17(3):260‚Äì267, 2006. ISSN 1044-\n3983. doi: 10.1097/01.ede.0000215160.88317.cb.\nChristopher Meek. Causal inference and causal explanation with background knowledge. In Proceedings\nof the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI‚Äô95, pp. 403‚Äì410, San Francisco,\nCA, USA, August 1995. Morgan Kaufmann Publishers Inc. ISBN 978-1-55860-385-1.\nThomas Mesnard, Theophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade, Anna Harutyunyan,\nWill Dabney, Thomas S. Stepleton, Nicolas Heess, Arthur Guez, Eric Moulines, Marcus Hutter, Lars\nBuesing, and Remi Munos. Counterfactual Credit Assignment in Model-Free Reinforcement Learning. In\nProceedings of the 38th International Conference on Machine Learning, pp. 7654‚Äì7664. PMLR, July 2021.\nWang Miao, Zhi Geng, and Eric Tchetgen Tchetgen.\nIdentifying Causal Effects With Proxy Variables\nof an Unmeasured Confounder.\nBiometrika, 105(4):987‚Äì993, December 2018.\nISSN 0006-3444.\ndoi:\n10.1093/biomet/asy038.\nEduardo Mosqueira-Rey, Elena Hern√°ndez-Pereira, David Alonso-R√≠os, Jos√© Bobes-Bascar√°n, and √Ångel\nFern√°ndez-Leal. Human-in-the-loop machine learning: A state of the art. Artificial Intelligence Review,\nAugust 2022. ISSN 1573-7462. doi: 10.1007/s10462-022-10246-w.\nAlexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende. Towards\ninterpretable reinforcement learning using attention augmented agents. Advances in Neural Information\nProcessing Systems, 32, 2019.\nSusan A. Murphy. Optimal dynamic treatment regimes. Journal of the Royal Statistical Society: Series B\n(Statistical Methodology), 65(2):331‚Äì355, 2003. doi: 10.1111/1467-9868.00389.\nRazieh Nabi, Daniel Malinsky, and Ilya Shpitser. Learning Optimal Fair Policies. In Proceedings of the 36th\nInternational Conference on Machine Learning, pp. 4674‚Äì4682. PMLR, May 2019.\nSuraj Nair, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Causal Induction from Visual Observations for Goal\nDirected Tasks, October 2019.\nVineet Nair, Vishakha Patil, and Gaurav Sinha. Budgeted and Non-Budgeted Causal Bandits. In Proceedings\nof The 24th International Conference on Artificial Intelligence and Statistics, pp. 2017‚Äì2025. PMLR,\nMarch 2021.\nHongseok Namkoong, Ramtin Keramati, Steve Yadlowsky, and Emma Brunskill. Off-policy Policy Evaluation\nFor Sequential Decisions Under Unobserved Confounding. In Advances in Neural Information Processing\nSystems, volume 33, pp. 18819‚Äì18831. Curran Associates, Inc., 2020.\nMichael Oberst and David Sontag.\nCounterfactual Off-Policy Evaluation with Gumbel-Max Structural\nCausal Models. In Proceedings of the 36th International Conference on Machine Learning, pp. 4881‚Äì4890.\nPMLR, May 2019.\nOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,\nAlex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry\nTworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving Rubik‚Äôs\nCube with a Robot Hand, October 2019.\nIan Osband, Daniel Russo, and Benjamin Van Roy. (More) Efficient Reinforcement Learning via Posterior\nSampling. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.,\n2013.\n40\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In Advances in Neural Information Processing\nSystems, October 2022.\nAliz√©e Pace, Hugo Y√®che, Bernhard Sch√∂lkopf, Gunnar R√§tsch, and Guy Tennenholtz.\nDelphic Offline\nReinforcement Learning under Nonidentifiable Hidden Confounding, June 2023.\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-Mimic: Deep Multitask and Transfer\nReinforcement Learning. arXiv preprint arXiv:1511.06342, November 2015.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven Exploration by Self-\nsupervised Prediction.\nIn Proceedings of the 34th International Conference on Machine Learning, pp.\n2778‚Äì2787. PMLR, July 2017.\nJudea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669‚Äì688, December 1995. ISSN\n0006-3444. doi: 10.1093/biomet/82.4.669.\nJudea Pearl. Causal inference in statistics: An overview. Statistics surveys, 3:96‚Äì146, 2009a. doi: 10.1214/\n09-SS057.\nJudea Pearl. Causality. Cambridge university press, 2009b.\nJudea Pearl and Elias Bareinboim. External Validity: From Do-Calculus to Transportability Across Popu-\nlations. Statistical Science, 29(4), November 2014. ISSN 0883-4237. doi: 10.1214/14-STS486.\nJudea Pearl and Dana Mackenzie. The Book of Why: The New Science of Cause and Effect. Basic books,\n2018.\nXue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic\ncontrol with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation\n(ICRA), pp. 3803‚Äì3810. IEEE, 2018. doi: 10.1109/ICRA.2018.8460528.\nEmilija Perkovic.\nIdentifying causal effects in maximally oriented partially directed acyclic graphs.\nIn\nProceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 530‚Äì539. PMLR,\nAugust 2020.\nEmilija Perkovic, Markus Kalisch, and Marloes H. Maathuis. Interpreting and using CPDAGs with back-\nground knowledge.\nIn Proceedings of the 2017 Conference on Uncertainty in Artificial Intelligence\n(UAI2017), pp. ID: 120. AUAI Press, 2017.\nRonan Perry, Julius Von K√ºgelgen, and Bernhard Sch√∂lkopf. Causal Discovery in Heterogeneous Environ-\nments Under the Sparse Mechanism Shift Hypothesis.\nIn Advances in Neural Information Processing\nSystems, October 2022.\nJonas Peters, Dominik Janzing, and Bernhard Sch√∂lkopf. Elements of Causal Inference: Foundations and\nLearning Algorithms. The MIT Press, 2017. ISBN 978-0-262-03731-0 978-0-262-34429-6.\nSilviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual Data Augmentation using Locally Factored\nDynamics. In Advances in Neural Information Processing Systems, volume 33, pp. 3976‚Äì3990. Curran\nAssociates, Inc., 2020.\nSilviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. MoCoDA: Model-based Counterfactual\nData Augmentation. In Advances in Neural Information Processing Systems, October 2022.\nDrago Plecko and Elias Bareinboim. Causal Fairness Analysis, July 2022.\nDrago Plecko and Elias Bareinboim. Causal Fairness for Outcome Control, June 2023.\n41\nErika Puiutta and Eric Veith. Explainable reinforcement learning: A survey. In International Cross-Domain\nConference for Machine Learning and Knowledge Extraction, pp. 77‚Äì95. Springer, 2020. doi: 10.1007/\n978-3-030-57321-8_5.\nAniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi.\nDeep Reinforcement Learning for Sepsis Treatment, November 2017.\nMaithra Raghu, Alex Irpan, Jacob Andreas, Bobby Kleinberg, Quoc Le, and Jon Kleinberg.\nCan deep\nreinforcement learning solve Erdos-Selfridge-Spencer games?\nIn International Conference on Machine\nLearning, pp. 4238‚Äì4246. PMLR, 2018.\nRoberta Raileanu and Rob Fergus. Decoupling value and policy for generalization in reinforcement learning.\nIn International Conference on Machine Learning, pp. 8787‚Äì8798. PMLR, 2021.\nDanilo J. Rezende, Ivo Danihelka, George Papamakarios, Nan Rosemary Ke, Ray Jiang, Theophane We-\nber, Karol Gregor, Hamza Merzic, Fabio Viola, Jane Wang, Jovana Mitrovic, Frederic Besse, Ioannis\nAntonoglou, and Lars Buesing. Causally Correct Partial Models for Reinforcement Learning, February\n2020.\nDonald B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal\nof educational Psychology, 66(5):688, 1974. doi: 10.1037/h0037350.\nRichard M. Ryan and Edward L. Deci. Intrinsic and Extrinsic Motivations: Classic Definitions and New\nDirections. Contemporary Educational Psychology, 25(1):54‚Äì67, January 2000. ISSN 0361-476X. doi:\n10.1006/ceps.1999.1020.\nSorawit Saengkyongam, Nikolaj Thams, Jonas Peters, and Niklas Pfister. Invariant Policy Learning: A\nCausal Perspective, September 2022.\nKristin L. Sainani. Instrumental Variables: Uses and Limitations. PM&R, 10(3):303‚Äì308, March 2018. ISSN\n1934-1482. doi: 10.1016/j.pmrj.2018.02.002.\nBernhard Sch√∂lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh\nGoyal, and Yoshua Bengio.\nToward causal representation learning.\nProceedings of the IEEE, 109(5):\n612‚Äì634, 2021. doi: 10.1109/JPROC.2021.3058954.\nCarolyn A Schult and Henry M Wellman. Explaining human movements and actions: Children‚Äôs understand-\ning of the limits of psychological explanation. Cognition, 62(3):291‚Äì324, March 1997. ISSN 0010-0277.\ndoi: 10.1016/S0010-0277(96)00786-X.\nMaximilian Seitzer, Bernhard Sch√∂lkopf, and Georg Martius.\nCausal Influence Detection for Improving\nEfficiency in Reinforcement Learning. In Advances in Neural Information Processing Systems, October\n2021.\nRajat Sen, Karthikeyan Shanmugam, Alexandros G. Dimakis, and Sanjay Shakkottai.\nIdentifying Best\nInterventions through Online Importance Sampling. In Proceedings of the 34th International Conference\non Machine Learning, pp. 3057‚Äì3066. PMLR, July 2017.\nXinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang.\nWeakly Supervised\nDisentangled Generative Causal Representation Learning.\nJournal of Machine Learning Research, 23\n(241):1‚Äì55, 2022. ISSN 1533-7928.\nZheyan Shen, Peng Cui, Kun Kuang, Bo Li, and Peixuan Chen. Causally Regularized Learning with Agnostic\nData Selection Bias. In Proceedings of the 26th ACM International Conference on Multimedia, pp. 411‚Äì419,\nOctober 2018. doi: 10.1145/3240508.3240577.\nIlya Shpitser and Judea Pearl. Identification of joint interventional distributions in recursive semi-Markovian\ncausal models. In Proceedings of the National Conference on Artificial Intelligence, volume 21, pp. 1219.\nMenlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.\n42\nIlya Shpitser and Judea Pearl. What counterfactuals can be tested. In Proceedings of the Twenty-Third\nConference on Uncertainty in Artificial Intelligence, UAI‚Äô07, pp. 352‚Äì359. AUAI Press, July 2007.\nThomas R. Shultz. Rules of Causal Attribution. Monographs of the Society for Research in Child Develop-\nment, 47(1):1‚Äì51, 1982. ISSN 0037-976X. doi: 10.2307/1165893.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas\nHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent\nSifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without\nhuman knowledge. Nature, 550(7676):354‚Äì359, October 2017. ISSN 1476-4687. doi: 10.1038/nature24270.\nSteven Sloman. Causal Models: How People Think about the World and Its Alternatives. Oxford University\nPress, 2005.\nSteven A. Sloman and David Lagnado. Causality in Thought. Annual Review of Psychology, 66(1):223‚Äì247,\n2015. doi: 10.1146/annurev-psych-010814-015135.\nDavid Sobel and Jessica Sommerville. The Importance of Discovery in Children‚Äôs Causal Learning from\nInterventions. Frontiers in Psychology, 1, 2010. ISSN 1664-1078. doi: 10.3389/fpsyg.2010.00176.\nSumedh A. Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Sch√∂lkopf. Causal Curiosity: RL Agents\nDiscovering Self-supervised Experiments for Causal Representation Learning. In Proceedings of the 38th\nInternational Conference on Machine Learning, pp. 9848‚Äì9858. PMLR, July 2021.\nPeter Spirtes, Clark N. Glymour, Richard Scheines, and David Heckerman.\nCausation, Prediction, and\nSearch. MIT Press, 2000. ISBN 978-0-262-19440-2.\nJames H. Stock and Francesco Trebbi.\nRetrospectives: Who invented instrumental variable regression?\nJournal of Economic Perspectives, 17(3):177‚Äì194, 2003. doi: 10.1257/089533003769204416.\nJianyu Su, Stephen Adams, and Peter A. Beling. Counterfactual Multi-Agent Reinforcement Learning with\nGraph Convolution Communication, December 2020.\nXiaohai Sun, Dominik Janzing, Bernhard Sch√∂lkopf, and Kenji Fukumizu. A kernel-based causal learning\nalgorithm. In Proceedings of the 24th International Conference on Machine Learning, ICML ‚Äô07, pp. 855‚Äì\n862, New York, NY, USA, June 2007. Association for Computing Machinery. ISBN 978-1-59593-793-3.\ndoi: 10.1145/1273496.1273604.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\nGokul Swamy, Sanjiban Choudhury, Drew Bagnell, and Steven Wu. Causal Imitation Learning under Tem-\nporally Correlated Noise. In Proceedings of the 39th International Conference on Machine Learning, pp.\n20877‚Äì20890. PMLR, June 2022.\nKaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased Scene Graph Gen-\neration From Biased Training. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pp. 3713‚Äì3722, Seattle, WA, USA, June 2020. IEEE.\nISBN 978-1-72817-168-5.\ndoi:\n10.1109/CVPR42600.2020.00377.\nZeyu Tang, Yatong Chen, Yang Liu, and Kun Zhang. Tier Balancing: Towards Dynamic Fairness over Un-\nderlying Causal Factors. In The Eleventh International Conference on Learning Representations, February\n2023.\nYee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess,\nand Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc., 2017.\nNikolaj Thams. Causality and Distribution Shift. PhD thesis, University of Copenhagen, 2022.\n43\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.\nDomain\nrandomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pp. 23‚Äì30. IEEE, 2017. doi: 10.1109/\nIROS.2017.8202133.\nThien Q. Tran, Kazuto Fukuchi, Youhei Akimoto, and Jun Sakuma. Unsupervised Causal Binary Concepts\nDiscovery with VAE for Black-Box Model Explanation. Proceedings of the AAAI Conference on Artificial\nIntelligence, 36(9):9614‚Äì9622, June 2022. ISSN 2374-3468. doi: 10.1609/aaai.v36i9.21195.\nStelios Triantafyllou, Adish Singla, and Goran Radanovic. Actual Causality and Responsibility Attribution\nin Decentralized Partially Observable Markov Decision Processes. In Proceedings of the 2022 AAAI/ACM\nConference on AI, Ethics, and Society, AIES ‚Äô22, pp. 739‚Äì752, New York, NY, USA, July 2022. Association\nfor Computing Machinery. ISBN 978-1-4503-9247-1. doi: 10.1145/3514094.3534133.\nStratis Tsirtsis, Abir De, and Manuel Rodriguez. Counterfactual Explanations in Sequential Decision Making\nUnder Uncertainty. In Advances in Neural Information Processing Systems, volume 34, pp. 30127‚Äì30139.\nCurran Associates, Inc., 2021.\nSahil Verma, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P. Dickerson, and Chirag Shah.\nCounterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review, November\n2022.\nNelson Vithayathil Varghese and Qusay H. Mahmoud. A survey of multi-task deep reinforcement learning.\nElectronics, 9(9):1363, 2020. doi: 10.3390/electronics9091363.\nMatthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden. D‚Äôya Like DAGs? A Survey on Structure\nLearning and Causal Discovery. ACM Computing Surveys, 55(4):82:1‚Äì82:36, November 2022. ISSN 0360-\n0300. doi: 10.1145/3527154.\nJane X. Wang, Michael King, Nicolas Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie Deck, Peter Choy, Mary\nCassin, Malcolm Reynolds, Francis Song, Gavin Buttimore, David P. Reichert, Neil Rabinowitz, Loic\nMatthey, Demis Hassabis, Alexander Lerchner, and Matthew Botvinick. Alchemy: A benchmark and\nanalysis toolkit for meta-reinforcement learning agents, October 2021a.\nKaixin Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving generalization in reinforcement learning\nwith mixture regularization. Advances in Neural Information Processing Systems, 33:7968‚Äì7978, 2020a.\nLingxiao Wang, Zhuoran Yang, and Zhaoran Wang. Provably Efficient Causal Reinforcement Learning with\nConfounded Observational Data. In Advances in Neural Information Processing Systems, volume 34, pp.\n21164‚Äì21175. Curran Associates, Inc., 2021b.\nTan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. Visual Commonsense R-CNN. In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10757‚Äì10767, Seattle,\nWA, USA, June 2020b. IEEE. ISBN 978-1-72817-168-5. doi: 10.1109/CVPR42600.2020.01077.\nTan Wang, Chang Zhou, Qianru Sun, and Hanwang Zhang. Causal Attention for Unbiased Visual Recog-\nnition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3091‚Äì3100,\n2021c.\nTingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang,\nGuodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking Model-Based Reinforcement Learning,\nJuly 2019.\nYixin Wang and Michael I. Jordan. Desiderata for Representation Learning: A Causal Perspective, February\n2022.\nZizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal Dynamics Learning for Task-\nIndependent State Abstraction. In Proceedings of the 39th International Conference on Machine Learning,\npp. 23151‚Äì23180. PMLR, June 2022.\n44\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. Finetuned Language Models Are Zero-Shot Learners. arXiv preprint\narXiv:2109.01652, September 2021.\nHenry M. Wellman. The Child‚Äôs Theory of Mind. The Child‚Äôs Theory of Mind. The MIT Press, Cambridge,\nMA, US, 1992. ISBN 978-0-262-23153-4 978-0-262-73099-0.\nMin Wen, Osbert Bastani, and Ufuk Topcu.\nAlgorithms for fairness in sequential decision making.\nIn\nInternational Conference on Artificial Intelligence and Statistics, pp. 1144‚Äì1152. PMLR, 2021.\nXiaobao Wu, Chunping Li, and Yishu Miao. Discovering Topics in Long-tailed Corpora with Causal Inter-\nvention. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 175‚Äì185,\nOnline, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.15.\nAnnie Xie and Chelsea Finn. Lifelong Robotic Reinforcement Learning by Retaining Experiences. In Pro-\nceedings of The 1st Conference on Lifelong Learning Agents, pp. 838‚Äì855. PMLR, November 2022.\nZhongwen Xu, Hado P van Hasselt, and David Silver. Meta-Gradient Reinforcement Learning. In Advances\nin Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\nAkihiro Yabe, Daisuke Hatano, Hanna Sumita, Shinji Ito, Naonori Kakimura, Takuro Fukunaga, and Ken-\nichi Kawarabayashi. Causal Bandits with Propagating Inference. In Proceedings of the 35th International\nConference on Machine Learning, pp. 5512‚Äì5520. PMLR, July 2018.\nChao-Han Huck Yang, I.-Te Danny Hung, Yi Ouyang, and Pin-Yu Chen. Training a Resilient Q-network\nagainst Observational Interference.\nIn Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pp. 8814‚Äì8822, June 2022a. doi: 10.1609/aaai.v36i8.20862.\nMengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang.\nCausalVAE: Dis-\nentangled Representation Learning via Neural Structural Causal Models.\nIn 2021 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR), pp. 9588‚Äì9597, June 2021a.\ndoi:\n10.1109/CVPR46437.2021.00947.\nTianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, Peng Liu, and Zhen\nWang. Exploration in Deep Reinforcement Learning: A Comprehensive Survey, July 2022b.\nYijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, and Yuhui Shi. Pareto Policy Pool for Model-based Offline\nReinforcement Learning. In International Conference on Learning Representations, 2021b.\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image Augmentation Is All You Need: Regularizing Deep\nReinforcement Learning from Pixels. In International Conference on Learning Representations, 2021.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Avnish Narayan, Hayden Shively, Adithya Bellathur,\nKarol Hausman, Chelsea Finn, and Sergey Levine. Meta-World: A Benchmark and Evaluation for Multi-\nTask and Meta Reinforcement Learning, June 2021.\nYang Yu. Towards sample efficient reinforcement learning. In Proceedings of the 27th International Joint\nConference on Artificial Intelligence, IJCAI‚Äô18, pp. 5739‚Äì5743, Stockholm, Sweden, July 2018. AAAI\nPress. ISBN 978-0-9992411-2-7.\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi.\nFairness\nConstraints: Mechanisms for Fair Classification, July 2015.\nVinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David\nReichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu,\nMatthew Botvinick, Oriol Vinyals, and Peter Battaglia.\nDeep reinforcement learning with relational\ninductive biases. In International Conference on Learning Representations, 2019.\nAmy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overfitting and generalization in continuous\nreinforcement learning. arXiv preprint arXiv:1806.07937, 2018a.\n45\nAmy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and\nDoina Precup. Invariant Causal Prediction for Block MDPs. In Proceedings of the 37th International\nConference on Machine Learning, pp. 11214‚Äì11224. PMLR, November 2020a.\nAmy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning Invariant\nRepresentations for Reinforcement Learning without Reconstruction.\nIn International Conference on\nLearning Representations, February 2022a.\nChiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep reinforcement\nlearning. arXiv preprint arXiv:1804.06893, 2018b.\nJunzhe Zhang and Elias Bareinboim. Markov Decision Processes with Unobserved Confounders: A Causal\nApproach. Technical report, Technical report, Technical Report R-23, Purdue AI Lab, 2016.\nJunzhe Zhang and Elias Bareinboim. Transfer Learning in Multi-Armed Bandits: A Causal Approach. In\nProceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pp. 1340‚Äì1346,\nMelbourne, Australia, August 2017. International Joint Conferences on Artificial Intelligence Organization.\nISBN 978-0-9992411-0-3. doi: 10.24963/ijcai.2017/186.\nJunzhe Zhang and Elias Bareinboim. Fairness in Decision-Making ‚Äî The Causal Explanation Formula.\nProceedings of the AAAI Conference on Artificial Intelligence, 32(1), April 2018. ISSN 2374-3468. doi:\n10.1609/aaai.v32i1.11564.\nJunzhe Zhang and Elias Bareinboim. Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes.\nIn Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\nJunzhe Zhang and Elias Bareinboim. Designing Optimal Dynamic Treatment Regimes: A Causal Reinforce-\nment Learning Approach. In Proceedings of the 37th International Conference on Machine Learning, pp.\n11012‚Äì11022. PMLR, November 2020.\nJunzhe Zhang and Elias Bareinboim. Can Humans Be out of the Loop? In Proceedings of the First Conference\non Causal Learning and Reasoning, pp. 1010‚Äì1025. PMLR, June 2022a.\nJunzhe Zhang and Elias Bareinboim. Online Reinforcement Learning for Mixed Policy Scopes. In Advances\nin Neural Information Processing Systems, October 2022b.\nJunzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved confounders.\nIn Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS‚Äô20,\npp. 12263‚Äì12274, Red Hook, NY, USA, December 2020b. Curran Associates Inc. ISBN 978-1-71382-954-6.\nJunzhe Zhang, Jin Tian, and Elias Bareinboim. Partial Counterfactual Identification from Observational\nand Experimental Data. In Proceedings of the 39th International Conference on Machine Learning, pp.\n26548‚Äì26558. PMLR, June 2022b.\nKun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Sch√∂lkopf. Kernel-based conditional independence\ntest and application in causal discovery. In Proceedings of the Twenty-Seventh Conference on Uncertainty\nin Artificial Intelligence, UAI‚Äô11, pp. 804‚Äì813, Arlington, Virginia, USA, July 2011. AUAI Press. ISBN\n978-0-9749039-7-2.\nKun Zhang, Mingming Gong, and Bernhard Sch√∂lkopf. Multi-source domain adaptation: A causal view. In\nTwenty-Ninth AAAI Conference on Artificial Intelligence, 2015. doi: 10.1609/aaai.v29i1.9542.\nXingxuan Zhang, Peng Cui, Renzhe Xu, Linjun Zhou, Yue He, and Zheyan Shen. Deep Stable Learning\nfor Out-of-Distribution Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 5372‚Äì5382, 2021a.\nXueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and Cheng Zhang. How\ndo fair decisions fare in long-term qualification? In Advances in Neural Information Processing Systems,\nvolume 33, pp. 18457‚Äì18469. Curran Associates, Inc., 2020c.\n46\nYang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang.\nCausal Intervention for Leveraging Popularity Bias in Recommendation. In Proceedings of the 44th Inter-\nnational ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ‚Äô21, pp.\n11‚Äì20, New York, NY, USA, July 2021b. Association for Computing Machinery. ISBN 978-1-4503-8037-9.\ndoi: 10.1145/3404835.3462875.\nYu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. Disentangling User Interest and\nConformity for Recommendation with Causal Embedding. In Proceedings of the Web Conference 2021,\nWWW ‚Äô21, pp. 2980‚Äì2991, New York, NY, USA, June 2021. Association for Computing Machinery. ISBN\n978-1-4503-8312-7. doi: 10.1145/3442381.3449788.\nHanhan Zhou, Tian Lan, and Vaneet Aggarwal. PAC: Assisted Value Factorization with Counterfactual Pre-\ndictions in Multi-Agent Reinforcement Learning. In Advances in Neural Information Processing Systems,\nOctober 2022.\nDeyao Zhu, Li Erran Li, and Mohamed Elhoseiny. CausalDyna: Improving Generalization of Dyna-style\nReinforcement Learning via Counterfactual-Based Data Augmentation. November 2021.\nShengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal Discovery with Reinforcement Learning. In Interna-\ntional Conference on Learning Representations, February 2022a.\nZheng-Mao Zhu, Xiong-Hui Chen, Hong-Long Tian, Kun Zhang, and Yang Yu.\nOffline Reinforcement\nLearning with Causal Structured World Models, June 2022b.\nZhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A survey.\narXiv preprint arXiv:2009.07888, 2020.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano,\nand Geoffrey Irving. Fine-Tuning Language Models from Human Preferences, January 2020.\n47\nA\nSupplementary Introduction to Causality\nA.1\nMediation analysis\nMediation is a causal concept that closely relates to counterfactuals. The goal of mediation analysis is to\nexamine the direct or indirect effects of a treatment variable 9 X on an outcome variable Y , mediated by a\nthird variable M (referred to as the mediator). To illustrate, let‚Äôs consider the direct effect of different food\nconsumption on preventing scurvy (See Figure3). In a counterfactual world, if we prevent changes in the\nmediator variable M (e.g., vitamin C intake), then whatever changes the outcome variable Y (e.g., whether a\nsailor gets scurvy) can only be attributed to changes in the treatment variable X (e.g., the food consumed),\nallowing us to establish the observed effect as a direct effect of X on Y . It is worth noting that, in cases like\nthis, the statistical language can only provide the conditioning operator, which merely shifts our attention\nto individuals with equal values of M. On the other hand, the do-operator precisely captures the concept\nof keeping the mediator M unchanged. These two operations lead to fundamentally different results, and\nconflating them can yield opposite conclusions (Pearl & Mackenzie, 2018, Chapter 9). In summary, such\nanalyses are crucial for understanding the potential causal mechanisms and paths in complex systems, with\napplications spanning various fields including psychology, sociology, and epidemiology (Carey & Wu, 2022).\nA.2\nCausal discovery and causal reasoning\n. In the field of causal inference, there are two primary areas of focus: causal discovery and causal reasoning.\nCausal discovery involves inferring the causal relationships between variables of interest (in other words,\nidentifying the causal graph of the data generation process). Traditional approaches use conditional inde-\npendence tests to infer causal relationships, and recently some studies have been conducted based on large\ndatasets using deep learning techniques. Glymour et al. (2019) and Vowels et al. (2022) comprehensively\nsurvey the field of causal discovery.\nAs opposed to causal discovery, causal reasoning investigates how to estimate causal effects, such as in-\ntervention probability, given the causal model. Interventions involve actively manipulating the system or\nenvironment, which can be costly and potentially dangerous (e.g., testing a new drug in medical experiments).\nTherefore, a core challenge of causal reasoning is how to translate causal effects into estimands that can be\nestimated from observational data using statistical methods. Given the causal graph, the identifiability of\ncausal effects can be determined systematically through the use of do-calculus (Pearl, 1995).\nA.3\nCausal representation learning\nA fundamental limitation of traditional causal inference is that most research starts with the assumption\nthat causal variables are given, which does not align with the reality of dealing with high-dimensional\nand low-level data, such as images, in our daily lives.\nCausal representation learning (Sch√∂lkopf et al.,\n2021) is an emerging field dedicated to addressing this challenge. Specifically, causal representation learning\nfocuses on identifying high-level variables from low-level observations. These high-level variables are not only\ndescriptive of the observed data but also explanatory of the data generation process, as they capture the\nunderlying causal mechanisms. By effectively discovering meaningful and interpretable high-level variables,\ncausal representation learning facilitates causal inference in complex, high-dimensional domains.\nB\nA Brief Introduction to Environments and Tasks\nIn this section, we briefly introduce the environments and tasks mentioned spanning sections 3 to 6.\n9A treatment variable, also known as an intervention variable, refers to a variable that is purposefully manipulated to assess\nits impact on one or more outcome variables of interest.\n48\nB.1\nAutonomous Driving\nBARK-ML: https://github.com/bark-simulator/bark-ml. BARK is an open-source behavior bench-\nmarking environment. It covers a full variety of real-world, interactive human behaviors for traffic partici-\npants, including Highway, Merging, and Intersection (Unprotected Left Turn).\nCrash (highway-env): https://github.com/eleurent/highway-env. The highway-env project gathers\na collection of environments for autonomous driving and tactical decision-making tasks, including Highway,\nMerge, Roundabout, Parking, Intersection, and Racetrack. Crash is a modified version by (Ding et al., 2022)\nthat is not publicly available.\nB.2\nClassical Control\nCart-pole (dm_control): https://github.com/svikramank/dm_control. Cart-pole is an environment\nbelonging to the DeepMind Control Suite. It involves swinging up and balancing an unactuated pole by\napplying forces to a cart at its base.\nAcrobot (OpenAI Gym): https://www.gymlibrary.dev/environments/classic_control/acrobot/.\nThe Acrobot environment consists of two links connected linearly to form a chain, with one end of the chain\nfixed. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a\ngiven height while starting from the initial state of hanging downwards.\nMountain Car (OpenAI Gym):\nhttps://www.gymlibrary.dev/environments/classic_control/\nmountain_car/. The Mountain Car MDP is a deterministic MDP that consists of a car placed stochas-\ntically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can\nbe applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach\nthe goal state on top of the right hill.\nCart Pole (OpenAI Gym):\nhttps://www.gymlibrary.dev/environments/classic_control/cart_\npole/. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The\npendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and\nright direction on the cart.\nPendulum\n(OpenAI\nGym):\nhttps://www.gymlibrary.dev/environments/classic_control/\npendulum/. The system consists of a pendulum attached at one end to a fixed point, and the other end\nbeing free. The pendulum starts in a random position and the goal is to apply torque on the free end to\nswing it into an upright position, with its center of gravity right above the fixed point.\nInverted\nPendulum\n(OpenAI\nGym):\nhttps://www.gymlibrary.dev/environments/mujoco/\ninverted_pendulum/. This environment involves a cart that can moved linearly, with a pole fixed on it\nat one end and having another end free. The cart can be pushed left or right, and the goal is to balance the\npole on the top of the cart by applying forces on the cart.\nB.3\nGame\nMiniPacman:\nhttps://github.com/higgsfield/Imagination-Augmented-Agents.\nMiniPacman is\nplayed in a 15 √ó 19 grid-world. Characters, the ghosts and Pacman, move through a maze.\nLunar Lander (OpenAI Gym): https://www.gymlibrary.dev/environments/box2d/lunar_lander/.\nThis environment is a classic rocket trajectory optimization problem. The landing pad is always at coordi-\nnates (0, 0). The coordinates are the first two numbers in the state vector. Landing outside of the landing\npad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\nBipedal\nWalker\n(OpenAI\nGym):\nhttps://www.gymlibrary.dev/environments/box2d/bipedal_\nwalker/. This is a simple 4-joint walker robot environment.\nCar Racing (OpenAI Gym): https://www.gymlibrary.dev/environments/box2d/car_racing/. Car\nRacing is a top-down racing environment. The generated track is random in every episode. Some indicators\n49\nare shown at the bottom of the window along with the state RGB buffer. From left to right: true speed,\nfour ABS sensors, steering wheel position, and gyroscope.\nBeam Rider (OpenAI Gym): https://www.gymlibrary.dev/environments/atari/beam_rider/. The\nagent controls a space-ship that travels forward at a constant speed. The agent can only steer it sideways\nbetween discrete positions. The goal is to destroy enemy ships, avoid their attacks and dodge space debris.\nPong (OpenAI Gym): https://www.gymlibrary.dev/environments/atari/pong/. The agent controls\nthe right paddle, competing against the left paddle controlled by the computer.\nPong (Roboschool): https://github.com/openai/roboschool. Roboschool is an open-source software\nfor robot simulation, which is now deprecated. Pong allows for multiplayer training.\nSokoban: https://github.com/mpSchrader/gym-sokoban. This game is a transportation puzzle, where\nthe player has to push all boxes in the room on the storage locations/ targets. The possibility of making\nirreversible mistakes makes these puzzles so challenging especially for RL algorithms.\nSC2LE: https://github.com/deepmind/pysc2. SC2LE is a RL environment based on the StarCraft II\ngame. It is a multi-agent problem with multiple players interacting. This domain poses a grand challenge\nraising from the imperfect information, large action and state space, and delayed credit assignment.\nVizDoom: https://github.com/Farama-Foundation/ViZDoom.\nViZDoom is based on Doom, a 1993\ngame. It allows developing AI bots that play Doom using only visual information.\nB.4\nHealthcare\nMIMIC-III: https://physionet.org/content/mimiciii/1.4/. MIMIC-III (Johnson et al., 2016) is a\nlarge, freely-available database comprising deidentified health-related data associated with over forty thou-\nsand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between\n2001 and 2012.\nLu et al. (2020) use the code (https://github.com/aniruddhraghu/sepsisrl) pro-\nvided by Raghu et al. (2017) for preprocessing the data and Bica et al. (2021b) open-source their code\non https://github.com/vanderschaarlab/Invariant-Causal-Imitation-Learning/tree/main.\nTherapy:\nhttps://github.com/Networks-Learning/counterfactual-explanations-mdp/blob/main/\ndata/therapy/README.md. This dataset contains real data from cognitive behavioral therapy. The data\nwere collected during a clinical trial with the patients‚Äô written consent A post-processed version of the data\nis available upon request from Kristina.Fuhr@med.uni-tuebingen.de.\nSepsis: https://github.com/clinicalml/gumbel-max-scm. This environment, originally developed by\nOberst & Sontag (2019) to simulate intensive care trajectories, is adopted by Pace et al. (2023) to investi-\ngate the impact of hidden confounders in offline RL. The state space comprises 4-dimensional normalized\nobservation vectors (heart rate, systolic blood pressure, oxygenation, and blood glucose levels), and the\ndiscrete action space encompasses three binary treatments (antibiotic, vasopressor, and ventilation).\nIn\naddition, the patient‚Äôs diabetic status serves as an unobserved binary variable in this environment.\nHiRID: https://physionet.org/content/hirid/1.1.1/. The HiRID dataset (Hyland et al., 2020; Fal-\ntys et al., 2021), comprising data from over 34,000 patient admissions at the Bern University Hospital‚Äôs\nDepartment of Intensive Care Medicine in Switzerland, offers a high-resolution collection of demographic,\nphysiological, diagnostic, and treatment information. Pace et al. (2023) focused on optimizing fluid and\nvasopressor administration to prevent circulatory failure in their research targeted at non-identifiable hidden\nconfounding in offline RL.\nB.5\nRobotics\nB.5.1\nLocomotion\nCheetah (dm_control): https://github.com/svikramank/dm_control.\nCheetah is an environment\nbelonging to the DeepMind Control Suite. It is a running planar bipedal robot.\n50\nOpenAI Gym: https://www.gymlibrary.dev/environments/mujoco/.\nThese environments are built\nupon the MuJoCo (Multi-Joint dynamics with Contact) engine. The goal is to make the 3D robots move in\nthe forward direction by applying torques on the hinges connecting the links of each leg and the torso.\nPyBullet Gym: https://github.com/benelot/pybullet-gym. This is an open-source implementation\nof OpenAI Gym MuJoCo environments using the Bullet Physics (https://github.com/bulletphysics/\nbullet3).\nD4RL: https://github.com/Farama-Foundation/D4RL. D4RL is an open-source benchmark for offline\nRL. It includes several OpenAI Gym benchmark tasks, such as the Hopper, HalfCheetah, and Walker\nenvironments.\nB.5.2\nManipulation\nCausalWorld: https://github.com/rr-learning/CausalWorld. CausalWorld is an open-source simula-\ntion framework and benchmark for causal structure and transfer learning in a robotic manipulation environ-\nment where tasks range from rather simple to extremely hard. Tasks consist of constructing 3D shapes from\na given set of blocks - inspired by how children learn to build complex structures.\nIsaac Gym: https://github.com/NVIDIA-Omniverse/IsaacGymEnvs. Isaac Gym offers a high perfor-\nmance learning platform to train policies for wide variety of robotics tasks directly on GPU.\nOpenAI: The origin version is developed by OpenAI, known as ‚ÄúIngredients for robotics research‚Äù (https:\n//openai.com/research/ingredients-for-robotics-research), and now is maintained by the Farama\nFoundation (https://github.com/Farama-Foundation/Gymnasium-Robotics). It contains eight simulated\nrobotics environments.\nrobosuite: https://github.com/ARISE-Initiative/robosuite.\nrobosuite is a simulation framework\npowered by the MuJoCo physics engine for robot learning.\nIt contains seven robot models, eight grip-\nper models, six controller modes, and nine standardized tasks. It also offers a modular design for building\nnew environments with procedural generation.\nB.6\nNavigation\nUnlock (Minigrid): https://github.com/Farama-Foundation/MiniGrid. The Minigrid library contains\na collection of discrete grid-world environments to conduct research on Reinforcement Learning. Unlock is\ntask designed by (Ding et al., 2022), which is not publicly available.\nContextual-Gridworld: https://github.com/eghbalz/contextual-gridworld. Agents are trained on\na group of training contexts and are subsequently tested on two distinct sets of testing contexts within this\nenvironment. The objective is to assess the extent to which agents have grasped the causal variables from\nthe training phase and can accurately deduce and extend to new (test) contexts.\n3D Maze (Unity): https://github.com/Harsha-Musunuri/Shaping-Agent-Imagination. This envi-\nronment is built on the Unity3d game development engine. It contains an agent that can move around. The\nenvironment automatically changes to a new view after every episode.\nSpriteworld: https://github.com/deepmind/spriteworld. Spriteworld is an environment that consists\nof a 2D arena with simple shapes that can be moved freely. The motivation was to provide as much flexibility\nfor procedurally generating multi-object scenes while retaining as simple an interface as possible.\nTaxi: https://www.gymlibrary.dev/environments/toy_text/taxi/. There are four designated locations\nin the grid world. When the episode starts, the taxi starts off at a random square and the passenger is at a\nrandom location. The taxi drives to the passenger‚Äôs location, picks up the passenger, drives to the passenger‚Äôs\ndestination (another one of the four specified locations), and then drops off the passenger. Once the passenger\nis dropped off, the episode ends.\n51\nB.7\nOthers\nChemical: https://github.com/dido1998/CausalMBRL#chemistry-environment. By allowing arbitrary\ncausal graphs, this environment facilitates studying complex causal structures of the world. This is illustrated\nthrough simple chemical reactions, where changes in one element‚Äôs state can cause changes in the state of\nanother variable.\nLight: https://github.com/StanfordVL/causal_induction. It consists of the light switch environment\nfor studying visual causal induction, where N switches control N lights, under various causal structures.\nIncludes common cause, common effect, and causal chain relationships.\nMNIST: http://yann.lecun.com/exdb/mnist/. The MNIST dataset contains 70,000 images of handwrit-\nten digits.\n52\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-07-04",
  "updated": "2023-11-21"
}