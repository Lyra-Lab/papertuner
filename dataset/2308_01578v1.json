{
  "id": "http://arxiv.org/abs/2308.01578v1",
  "title": "Unsupervised Representation Learning for Time Series: A Review",
  "authors": [
    "Qianwen Meng",
    "Hangwei Qian",
    "Yong Liu",
    "Yonghui Xu",
    "Zhiqi Shen",
    "Lizhen Cui"
  ],
  "abstract": "Unsupervised representation learning approaches aim to learn discriminative\nfeature representations from unlabeled data, without the requirement of\nannotating every sample. Enabling unsupervised representation learning is\nextremely crucial for time series data, due to its unique annotation bottleneck\ncaused by its complex characteristics and lack of visual cues compared with\nother data modalities. In recent years, unsupervised representation learning\ntechniques have advanced rapidly in various domains. However, there is a lack\nof systematic analysis of unsupervised representation learning approaches for\ntime series. To fill the gap, we conduct a comprehensive literature review of\nexisting rapidly evolving unsupervised representation learning approaches for\ntime series. Moreover, we also develop a unified and standardized library,\nnamed ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast\nimplementations and unified evaluations on various models. With ULTS, we\nempirically evaluate state-of-the-art approaches, especially the rapidly\nevolving contrastive learning methods, on 9 diverse real-world datasets. We\nfurther discuss practical considerations as well as open research challenges on\nunsupervised representation learning for time series to facilitate future\nresearch in this field.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nUnsupervised Representation Learning for Time\nSeries: A Review\nQianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui, Senior Member, IEEE\nAbstract—Unsupervised representation learning approaches\naim to learn discriminative feature representations from unla-\nbeled data, without the requirement of annotating every sam-\nple. Enabling unsupervised representation learning is extremely\ncrucial for time series data, due to its unique annotation\nbottleneck caused by its complex characteristics and lack of\nvisual cues compared with other data modalities. In recent years,\nunsupervised representation learning techniques have advanced\nrapidly in various domains. However, there is a lack of systematic\nanalysis of unsupervised representation learning approaches for\ntime series. To fill the gap, we conduct a comprehensive literature\nreview of existing rapidly evolving unsupervised representation\nlearning approaches for time series. Moreover, we also develop a\nunified and standardized library, named ULTS (i.e., Unsupervised\nLearning for Time Series), to facilitate fast implementations\nand unified evaluations on various models. With ULTS, we\nempirically evaluate state-of-the-art approaches, especially the\nrapidly evolving contrastive learning methods, on 9 diverse real-\nworld datasets. We further discuss practical considerations as\nwell as open research challenges on unsupervised representation\nlearning for time series to facilitate future research in this field.\nIndex Terms—Time series, representation learning, unsuper-\nvised learning, contrastive learning, self-supervised learning.\nI. INTRODUCTION\nT\nIME series data can record the changing trends of\nvariables over time and are ubiquitous across various\nfields such as Internet of Things (IoT) [1, 2, 3], industry\n4.0 [4, 5], financial services [6], and healthcare services [7, 8].\nIn order to extract and infer useful information from complex\nraw time series data, time series representation learning has\nemerged as an effective learning paradigm. On the one hand,\nThis work was supported in part by the National Key R&D Program\nof China No. 2021YFF0900800; the Major Scientific and Technological\nInnovation Project of Shandong Province No. 2021CXGC010108; the Nat-\nural Science Foundation of Shandong Province No. ZR202111180007; the\nFundamental Research Fund of Shandong University, and the Chinese Gov-\nernment Scholarship - High Level Postgraduate Program of China Scholarship\nCouncil. (Corresponding authors: Hangwei Qian; Lizhen Cui.)\nQ. Meng is with the School of Software, Shandong University, Jinan, China,\nand Joint SDU-NTU Centre for Artificial Intelligence Research, Shandong\nUniversity, Jinan, China (Email: mqw sdu@mail.sdu.edu.cn).\nH. Qian is with Centre for Frontier AI Research (CFAR), A*STAR,\nSingapore (Email: qian hangwei@cfar.a-star.edu.sg).\nY. Liu is with Joint NTU-UBC Research Centre of Excellence in Active\nLiving for the Elderly (LILY), Nanyang Technological University, Singapore\n(Email: stephenliu@ntu.edu.sg).\nY. Xu is with the School of Software, Shandong University, Jinan, China\nand Joint SDU-NTU Centre for Artificial Intelligence Research, Shandong\nUniversity, Jinan, China (Email: xuyonghui@sdu.edu.cn).\nZ. Shen is with the School of Computer Science and Engineering, Nanyang\nTechnological University, Singapore (Email: zqshen@ntu.edu.sg).\nL. Cui is with the School of Software, Shandong University, Jinan, China\nand Joint SDU-NTU Centre for Artificial Intelligence Research, Shandong\nUniversity, Jinan, China (Email: clz@sdu.edu.cn).\nthe learned latent representations can capture potentially valu-\nable information within time series and reveal the underlying\nmechanisms of the corresponding systems or phenomena.\nOn the other hand, high-quality representations are essen-\ntial for numerous downstream tasks, including time series\nclassification [9, 10, 11], forecasting [12, 13, 14, 15], and\nclustering [16, 17]. Pervasive raw time series data are collected\nthrough a wide spectrum of sensors or wearable devices with\nvarying frequencies [18]. Annotating corresponding labels for\nsuch data, however, is inevitably inefficient and could be\nerroneous due to the lack of obvious visual patterns in the\nraw data. This would inhibit the training of conventional\nsupervised learning models since full label annotations are\nrequired beforehand. Weakly-supervised learning methods can\nalleviate the annotation challenge to some extent, but they still\nnecessitate partial or coarse labels for model training [19].\nIn this paper, we turn to the techniques that aim to learn\nhigh-quality representations from raw time series data in an\nunsupervised learning manner.\nIn general, unsupervised representation learning aims to\nextract latent representations from complex raw time series\ndata without human supervision [28]. Previous studies have\nshown remarkable performance with joint clustering and fea-\nture learning approaches [29]. However, clustering primarily\nfocuses on identifying specific data patterns and can be unre-\nliable due to the poor generality of predefined priors [30]. The\nadvent of deep learning techniques has led to the widespread\nadoption of auto-encoders and seq-to-seq models for repre-\nsentation learning [31]. These approaches employ appropriate\nlearning objectives, such as self-reconstruction and context\nprediction, to learn meaningful representations [16, 32]. Never-\ntheless, accurately reconstructing the complex time series data\nremains challenging, especially with the high-frequency phys-\niological signals [33, 34]. Recently, self-supervised learning is\nemerging as a new paradigm, which induces supervision by\ndesigning pretext tasks instead of relying on pre-defined prior\nknowledge [35, 36, 37]. Diverging from fully unsupervised\napproaches, self-supervised learning methods leverage pretext\ntasks to autonomously generate labels by utilizing intrinsic\ninformation derived from the unlabeled data [38]. The learning\nefficiency can be improved through discriminative pre-training\nbased on self-generated supervised signals that are freely\nobtained from the raw data [25, 39, 40, 41].\nMore recently, self-supervised learning has made significant\nleaps fueled by advancements in contrastive learning, which\nemploys the instance discrimination pretext task to bring simi-\nlar pairs closer while pushing dissimilar pairs apart in the fea-\nture space [42]. Contrastive learning has achieved remarkable\narXiv:2308.01578v1  [cs.LG]  3 Aug 2023\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nSurveys\nApplications\nCore Techniques\nTypes\nMain Contributions\nClassification\nForecasting\nOthers\nSupervised\nUnsupervised\nDeep\nNon-deep\nUnivariate\nMultivariate\n[10]\n✓\n✓\n✓\n✓\nThey classify algorithms according to the type of discriminatory features the\ntechnique is attempting to find. The algorithms are grouped into those that\nuse the whole series, intervals, shapelets or repeating pattern counts.\n[11]\n✓\n✓\n✓\n✓\n✓\nThey review recently proposed bespoke multivariate time series classification\nalgorithms based on deep learning, shapelets and bag-of-words approaches.\n[20]\n✓\n✓\n✓\n✓\n✓\nThey group deep learning approaches for time series classification into\ngenerative models including auto-encoders and echo state networks, and\ndiscriminative models including feature engineering and end-to-end methods.\n[21]\n✓\n✓\n✓\n✓\n✓\nThey examine the current state of eXplainable AI (XAI) methods with a focus\non approaches for opening up deep learning black boxes including MLPs,\nRNNs, CNNs and ResNets for the task of time series classification.\n[22]\n✓\n✓\n✓\n✓\n✓\n✓\nThey categorize early classification approaches for incomplete time series into\n4 exclusive categories based on their proposed solution strategies, including\nprefix-based, shapelet-based, model-based and miscellaneous ones.\n[23]\n✓\n✓\n✓\n✓\n✓\nThey propose a taxonomy of distance-based classifiers, including KNN-based,\ndistance features-based and distance kernels-based ones.\n[14]\n✓\n✓\n✓\n✓\nThey review 3 categories of deep learning architectures for time series\nforecasting, including MLPs, RNNs and CNNs.\n[12]\n✓\n✓\n✓\n✓\n✓\nThey classify deep learning methods for time series forecasting by application\ndomains (energy and fuels, image and video, finance, industry, health, etc.)\nand network architectures (ENN, LSTM, GRU, BRNN, DFFNN, CNN, TCN).\n[24]\n✓\n✓\n✓\n✓\n✓\nThey divide compression algorithms into dictionary-based methods, functional\napproximation, auto-encoders, and sequential algorithms.\n[25]\n✓\n✓\n✓\n✓\nThey review approaches specifically proposed for multi-modal and temporal\ndata, then categorize discriminative self-supervised representation learning\nmodels into pretext, contrastive, clustering and regularisation-based models.\n[26]\n✓\n✓\n✓\nThey divide main research orientations into 3 subfields: dimensionality reduc-\ntion (time series representation), similarity measures and data mining tasks.\n[27]\n✓\n✓\n✓\n✓\n✓\nThey review a variety of feature learning algorithms that have been developed\nto explicitly capture temporal relationships (such as RBM, AE, RNN, TDNN).\nOurs\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nOur comprehensive review encompasses a broad spectrum of unsupervised\nrepresentation learning methods, including both established and novel ap-\nproaches, with a particular emphasis on the latest advancements in contrastive\nlearning techniques. Furthermore, we develop a unified and standardized\nlibrary called ULTS, which serves as a valuable resource for convenient\nempirical evaluations and comparisons of these methods.\nTABLE I\nCOMPARISONS OF EXISTING RELATED SURVEYS ON TIME SERIES REPRESENTATION LEARNING.\nadvantages in representation learning for various types of data,\nincluding image [43, 44, 45, 46], video [47, 48, 49] and time\nseries [50, 51, 52, 53]. Alignment serves as a key motivation\nin representation learning methods [54]. Specifically, in the\ncontext of contrastive learning, alignment involves the map-\nping of 2 samples that form a positive pair to nearby features,\nresulting in a high degree of invariance to unneeded noise\nfactors. Contrastive learning primarily focuses on 3 levels,\nnamely instance-level contrast [55, 56, 57], prototype-level\ncontrast [58, 59, 60], and temporal-level contrast [33, 51, 61].\nThe instance-level contrast treats samples independently and\ndistinguishes them by pulling positive pairs together and push-\ning negative pairs apart [62]. Prototype-level contrast, instead,\ngoes beyond the independence assumption and exploits latent\ncluster information present within samples. Researchers have\ndemonstrated that the learned representations are expected to\nretain higher-level semantic information by taking advantage\nof additional prior information brought by clustering [58, 59,\n63, 64]. Furthermore, considering the temporal dependency\ninherent in time series data, researchers have explored the\nfeasibility of distinguishing contextual information at a fine-\ngrained temporal level [50, 51, 52].\nAs shown in Table I, existing empirical surveys primarily fo-\ncus on end-to-end models associated with specific downstream\ntasks such as time series classification [10, 11, 31, 65], fore-\ncasting [12, 14], or anomaly detection [1, 2]. However, they\ntend to overlook the crucial representation learning process\nthat precedes these downstream tasks. The models reviewed\nin current surveys primarily prioritize the optimization of the\nentire end-to-end model, neglecting the in-depth exploration\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nof learning meaningful representations. Consequently, there is\na lack of systematic analysis of unsupervised representation\nlearning methods that prioritize the acquisition of effective\nrepresentations, whose efficacy on downstream tasks can be\nverified by appending simple classifiers or predictors on top\nof frozen representations. In addition, existing related surveys\nfail to cover the rapidly evolving self-supervised approaches,\nespecially contrastive learning techniques. It can be observed\nthat approximately half of existing surveys primarily focus\non models that employ non-deep fundamental methods as\ntheir core techniques. Among the surveys that do incorporate\ndeep learning models, the emphasis is mainly on supervised\napproaches. In addition, most of them exclusively cover tra-\nditional methods within a completely unsupervised setting,\nsuch as clustering [16, 66] or reconstruction-based meth-\nods [20, 24], thereby overlooking numerous contemporary and\nhighly effective techniques like self-supervised learning that\nhave proven to yield superior results in recent years. To fill\nthis gap, we carry out this extensive review of representation\nlearning methods for time series from an unsupervised deep\nlearning perspective.\nOur comprehensive literature review encompasses i) the\ndeep clustering methods, ii) the reconstruction-based meth-\nods, and iii) the self-supervised learning methods particularly\ncontrastive learning methods. The purpose of this review is\nto provide readers with a clear and thorough overview of\ncutting-edge research in the field of unsupervised time series\nrepresentation learning. In addition, we observe that it is\nextremely difficult to conduct fair comparisons to evaluate\nexisting works due to the rapid evolution and various tax-\nonomies of unsupervised learning methods. These evaluations\noften differ in components such as augmentation methods,\nbackbones, and even datasets, making it difficult to draw\nmeaningful comparisons. Consequently, there is a lack of com-\nprehensive benchmark evaluations that can offer researchers\na holistic understanding of the strengths and weaknesses of\nexisting approaches from various perspectives. We notice that\nthe previous work [20] conducted an experimental evaluation\nof pure discriminative time series classification models, but\nexcluded representation learning methods. In this work, we\naim to bridge this gap by developing a unified and standardized\nlibrary ULTS1 to enable quick and convenient evaluations\nof unsupervised representation learning approaches for time\nseries. Furthermore, we conduct empirical evaluations over\nstate-of-the-art approaches on 9 diverse real-world datasets\nwithin this unified evaluation testbed. Our evaluation serves\nas a valuable reference for understanding the strengths and\napplicable scenarios of different models, helping researchers\ndesign and evaluate customized models in the future. Overall,\nthe main contributions of this work are summarized as follows:\n• We conduct a comprehensive literature review of unsuper-\nvised representation learning approaches for time series,\nand propose an up-to-date taxonomy that categorizes\ndifferent approaches.\n• We develop a standardized library ULTS, to conveniently\nevaluate various models in a unified testbed, and conduct\n1The library ULTS is available at https://github.com/mqwfrog/ULTS.\nempirical evaluations of state-of-the-art methods, partic-\nularly contrastive ones, on 9 diverse real-world datasets.\n• We discuss practical considerations as well as open re-\nsearch challenges on unsupervised learning for time series\nand provide new insights to facilitate future research in\nthis field.\nThe rest of this paper is organized as follows: In Section II,\nwe introduce the definitions and terminology that are used\nthroughout this paper, and propose a taxonomy that classifies\nexisting unsupervised representation learning methods for time\nseries. Additionally, we introduce a unified and standardized\nlibrary ULTS that integrates 17 representative models encom-\npassing various categories. Then, we briefly review related\nworks including deep clustering, reconstruction-based and\nself-supervised learning methods in Section III, Section IV,\nand Section V respectively. Finally, we conclude this paper\nand discuss promising future research directions in Section VI.\nII. OVERVIEW\nA. Definitions and Terminology\nTime series representation learning holds utmost signifi-\ncance in time series modeling, as it strives to extract meaning-\nful and informative latent representations within the reservoir\nspace from the intricate raw complex time series data. The pri-\nmary obstacle in time series representation learning lies in the\nlack of well-annotated labels, resulting in insufficient super-\nvisory signals for training supervised models. Consequently,\nunsupervised learning approaches are devised to tackle this\nchallenge by creating various pretext tasks that generate self-\ngenerated labels without relying on human supervision. We\nconsider a set of N time series X = {xi}N\ni=1, where xi ∈\nRT ×V denotes a sequence of time series data collected from\nV variables within T timestamps. When V = 1, it becomes\nthe univariate time series. In general, we can formulate the\nproblem as follows: given an original time series xi, unsuper-\nvised representation learning aims to learn an encoder fe(·)\nthat can map xi into a D-dimensional representation vector zi\nin the latent space. A crucial measure to assess the quality and\neffectiveness of the acquired representations is their ability to\nserve subsequent downstream tasks. Consequently, once the\nlatent representations are obtained, they are frozen without\nfurther modification, and then can be utilized as input for\ndownstream tasks such as classification, forecasting, cluster-\ning, and anomaly detection to evaluate their performance. The\nunified mathematical notations used throughout this paper are\nsummarized in Table II.\nNotations\nDescriptions\nX\nThe set of original time series data\nZ\nThe set of learned representations in the latent space\nC\nThe set of cluster centroids (prototypes)\nN\nThe number of samples\nV\nThe number of variables\nT\nThe number of timestamps (the length of time series)\nfe(·)\nThe encoder that maps original data X to representations Z\nfd(·)\nThe decoder that uses representations Z to reconstruct original data X\nh(·)\nThe nonlinear projection head to remap the representations\nfm(·)\nThe momentum encoder\nTABLE II\nLIST OF MATHEMATICAL NOTATIONS USED THROUGHOUT THIS PAPER.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nSelf-supervised Learning Methods\nDeep Clustering\nMethods\nPredictive Methods\nTST [104]\nCaSS [103]\nCPC [56]\nEEG-SSL [71]\nWave2vec [102]\nAdversarial Methods\nSection Ⅲ\nSection Ⅴ\nSection Ⅴ- A\nSection Ⅴ- B\nTS-TCC [51]\nUnsupervised Representation Learning Methods for Time Series\nContrastive Methods\nSection Ⅴ- C\nInstance-level\nContrast\nSimCLR [43]\nMoCo [45]\nBYOL [44]\nCPC [56]\nSimSiam [57]\nSection Ⅴ- C1\nTimeCLR [105]\nMCL [106]\nPrototype-level\nContrast\nSwAV [59]\nPCL [58]\nCCL [64]\nCC [81]\nMHCCL [60]\nSection Ⅴ- C2\nSCCL [80]\nSLIC [82]\nTemporal-level\nContrast\nTS2Vec [52]\nTS-TCC [51]\nTNC [33]\nTCL [61]\nT-Loss [50]\nSection Ⅴ- C3\nBTSF [112]\nCoST [113]\nODC [29]\nDEC [78]\nIDFD [63]\nDTCR [77]\nDeepCluster [67]\nCRLI [76]\nReconstruction-\nbased Methods\nSection  Ⅳ\nTimeNet [32]\nDeconv [68]\nHeartSpace [87]\nSimMTM [90]\nMBTGMM [89]\nCRT [88]\nTimeGAN [69]\nMAD-GAN [97]\nTSGAN [98]\nRGAN [96]\nDIVERSIFY [101]\nTimeVAE [70]\nFig. 1. An up-to-date taxonomy of unsupervised representation learning methods for time series, including a) Deep Clustering Methods b) Reconstruction-\nbased Methods and c) Self-supervised Learning Methods. The self-supervised learning methods can be further divided into adversarial methods, predictive\nmethods and contrastive methods, depending on different types of pretext tasks employed for acquiring self-supervised signals.\nB. A Taxonomy of Unsupervised Representation Learning\nMethods for Time Series\nAs presented in Figure 1, we classify the existing un-\nsupervised representation learning methods for time series\ninto 3 categories: deep clustering, reconstruction-based, and\nself-supervised learning methods. Deep clustering methods\ntypically combine traditional clustering techniques with deep\nlearning neural networks, utilizing clustering results as labels\nfor representation learning while updating clustering results\nbased on learned representations. Reconstruction-based meth-\nods usually employ auto-encoders or seq-to-seq models to\nreconstruct either partial or the entire raw time series through\njoint training with decoders. More recently, self-supervised\nlearning has gained popularity for extracting meaningful rep-\nresentations from unlabeled data as it avoids the need for\nmanual annotation by designing pretext tasks. We further\ncategorize self-supervised learning methods into adversarial,\npredictive, and contrastive ones, depending on different types\nof pretext tasks employed for acquiring self-supervised signals.\nAdversarial methods discriminate between fake and real data\nby using the discriminator, where the fake data similar to\ntraining data are produced by the generator. Predictive methods\ninvolve predicting future inputs from past inputs, predicting\nthe original view from some other corrupted views, or predict-\ning masked parts from unmasked parts. Contrastive methods\nperform distance discrimination using self-supervised signals\ngenerated from data augmentations. Specifically, contrastive\nmethods can be subdivided into fine-grained subcategories\naccording to different levels of contrast, including instance-\nlevel, prototype-level, and temporal-level approaches. Figure 2\nshows the organization of the research in unsupervised learn-\ning techniques for time series, based on the 3 main categories\noutlined in our proposed up-to-date taxonomy.\nAdversarial Methods\nContrastive Methods\nPredictive Methods\nInstance-level\n Contrast\nTemporal-level\nContrast\nTS2Vec [52]\nTNC [33]\nTCL [61]\nT-Loss [50] \nBTSF [112]\nCoST [113]\nSimCLR [43]\nTimeCLR [105]\nMoCo [45]\nBYOL [44]\nCPC [56]\nSimSiam [57]\nMCL [106]\nSwAV [59]\nPCL [58]\nCCL [64]\nSCCL [80]\nCC [81]\nSLIC [82]\nMHCCL [60]\nDeepCluster [67]\nCRLI [76]\nDTCR [77]\nDEC [78]\nODC [29]\nIDFD [63]\nDeep Clustering\nMethods\nTST [104]\nEEG-SSL [71]\nWave2vec [102]\nCaSS [103]\nSelf-supervised Learning Methods\nReconstruction-based\nMethods\nTimeNet [32]\nDeconv [68]\nHeartSpace [87]\nMBTGMM [89]\nSimMTM [90]\nCRT [88]\nTimeGAN [69]\nMAD-GAN [97]\nTSGAN [98]\nRGAN [96]\nTimeVAE [70]\nDIVERSIFY [101]\nPrototype-level\nContrast\nCPC [56]\nTS-TCC [51]\nFig. 2. The organization of the research in unsupervised learning techniques\nfor time series, based on 3 main categories outlined in our proposed taxonomy.\nC. The ULTS Library\nWe have developed ULTS, a unified and standardized library\nunder the PyTorch framework, to facilitate comparisons of\nexisting models for unsupervised time series representation\nlearning. ULTS integrates 17 state-of-the-art models, providing\na unified testbed environment for evaluation. This library\ncovers 2 deep clustering models, 2 reconstruction-based mod-\nels, and 13 self-supervised learning models consisting of 2\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nadversarial models, 2 predictive models, and 9 contrastive\nmodels. Table III lists all the models implemented in our\nULTS library. ULTS also offers various flexible options for\ndata augmentation transformations in both time and frequency\ndomains. By default, it applies a combination of the weak aug-\nmentation (i.e., jitter-and-scale) and the strong augmentation\n(i.e., permutation-and-jitter), which aligns with the strategy\nused in TS-TCC [51]. For more implementation details, please\nrefer to the ULTS website. The comprehensive empirical\nexperimental evaluation and analysis conducted by ULTS are\nprovided in the Supplementary Materials.\n1st Category\n2nd Category\n3rd Category\nModel\nDeep Clustering\n-\n-\nDeepCluster [67]\nIDFD [63]\nReconstruction-based\n-\n-\nTimeNet [32]\nDeconv [68]\nSelf-supervised\nAdversarial\n-\nTimeGAN [69]\nTimeVAE [70]\nPredictive\n-\nEEG-SSL [71]\nTST [104]\nContrastive\nInstance-Level\nSimCLR [43]\nBYOL [44]\nCPC [56]\nPrototype-Level\nSwAV [59]\nPCL [58]\nMHCCL [60]\nTemporal-Level\nTS2Vec [52]\nTS-TCC [51]\nT-Loss [50]\nTABLE III\nMODELS IMPLEMENTED IN ULTS LIBRARY.\nIII. DEEP CLUSTERING METHODS\nJoint clustering and feature learning approaches have shown\nremarkable performance in unsupervised representation learn-\ning [29]. The primary objective is to discover underlying\nsimilar patterns and structures within data without relying on\nexplicit labels or annotations. Traditional clustering algorithms\nsuch as flat clustering [16, 72], hierarchical clustering [73]\nand spectral clustering [74] essentially employ handcrafted\nfeature extractors to group samples with similar characteris-\ntics together. However, these methods often exhibit limited\nperformance when applied to time series data due to the\ncomplex and high-dimensional structures inherited therein.\nSimple distance-based metrics in traditional clustering struggle\nto effectively represent these intricate structures. Additionally,\nmanually extracted features usually lack quality guarantees\nand the process is not automated. Deep neural networks have\nthe ability to learn complex and hierarchical representations\nof data, making them well-suited for capturing the intricate\nnature of time series data. In this case, the integration of\ntraditional clustering techniques with deep learning, known\nas deep clustering, has garnered significant attention from\nresearchers. Deep clustering methods break down the barriers\nbetween clustering and representation learning, allowing for\nthe iterative optimization of the clustering-oriented objective\nto learn mappings from the input space to a new latent\nspace [29, 75]. The iterative optimization approaches such as\nDeepCluster [67] and ODC [29] enable the neural network\nand the clustering algorithm to mutually reinforce each other\nthrough interactive learning, leading to improved performance\nand more meaningful representations. CRLI [76], DTCR [77]\nand DEC [78] employ auto-encoder architectures as backbones\nand jointly optimize the reconstruction loss and clustering loss.\nIn addition to the widely used K-Means clustering, spectral\nclustering is also explored by IDFD [63] to learn clustering-\nfriendly representations. It is worth noting that clustering\nassignments can serve as self-supervised signals, thereby fa-\ncilitating the self-learning process. Particularly, state-of-the-art\ncontrastive learning highlights the importance of the alignment\nproperty that encourages the samples with similar features or\nsemantic categories to be close in the low-dimensional space,\nwhich is also essential for deep clustering [79]. In this case, re-\ncent studies that leverage clustering-based contrastive learning\nto acquire highly effective representations with higher-level se-\nmantic information have emerged [58, 59, 60, 64, 80, 81, 82],\nwhich will be discussed in Section V-C2. More details of deep\nclustering methods for unsupervised representation learning\nare listed as follows.\n1) DeepCluster [67]: DeepCluster iteratively groups sam-\nples with the standard K-Means clustering algorithm, and uses\nthe clustering assignments as supervised signals to update the\nweights of the network. Initially, the model extracts repre-\nsentations through the convolutional network. Subsequently,\nsimilar representations are grouped into the same class through\nclustering, and the resulting clustering labels are then utilized\nto train the classifier. However, such an iterative approach is\nsusceptible to the accumulation of errors during the alternating\nstages of representation learning and clustering. Consequently,\nsub-optimal clustering performance may arise as a result.\n2) CRLI [76]: Clustering Representation Learning on In-\ncomplete time series data (CRLI) jointly optimizes the imputa-\ntion and clustering process to obtain more discriminant values\nfor clustering, so that the learned representations possess good\nclustering property. In addition to reconstructing the original\ninput time series, CRLI also encourages the learned represen-\ntations to form cluster structures by integrating the standard\nsoft K-means objective into the encoder-decoder network.\n3) DTCR [77]: Deep Temporal Clustering Representation\n(DTCR) integrates the temporal reconstruction and K-Means\nobjective into the seq-to-seq model to generate cluster-specific\ntemporal representations. To further enhance the capability\nof the encoder, DTCR proposes an auxiliary classification\ntask and a fake-sample generation strategy involving the\nrandom shuffling of certain time steps. The model employs\nbidirectional dilated recurrent neural networks as the encoder,\nenabling the learned representations to effectively capture the\ntemporal dynamics and multi-scale characteristics present in\nthe original time series data.\n4) DEC [78]: Deep Embedded Clustering (DEC) achieves\njoint optimization by iteratively optimizing a KL divergence-\nbased clustering objective with a centroid-based probability\ndistribution. The underlying assumption of DEC is that the\ninitial classifier’s highly confident predictions are mostly ac-\ncurate. In contrast to DTCR [77] that directly optimizes the\nK-Means process, DEC enhances the clustering performance\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nfollowing the initialization in the new feature space with a\nstacked auto-encoder(SAE) and standard K-Means.\n5) ODC [29]: Online Deep Clustering (ODC) devises\na joint clustering and feature learning paradigm with high\nstability by performing clustering and network update si-\nmultaneously rather than performing them alternatively. To\nachieve this, ODC introduces novel techniques including loss\nre-weighting according to the number of samples in each class,\nand processing small clusters in advance to avoid ODC from\ngetting stuck into trivial solutions. Loss re-weighting helps\nto prevent the formation of huge clusters and eliminates ex-\ntremely small clusters in advance. Specifically, the samples and\ncentroids’ memories are initialized via K-Means clustering,\nand then the uninterrupted ODC is performed iteratively.\n6) IDFD [63]:\nInspired by the properties of classical\nspectral clustering, IDFD applies deep representation learning\nvia instance discrimination to clustering. In addition, IDFD in-\ntroduces a softmax-formulated feature decorrelation constraint\nfor learning presentations in the latent space to achieve stable\nimprovements in clustering performance. The instance dis-\ncrimination module is used to capture the similarities among\ndata, while the feature decorrelation module is used to remove\nthe redundant correlations among features.\nIV. RECONSTRUCTION-BASED METHODS\nCompared to deep clustering methods that jointly learn rep-\nresentations and clustering assignments, reconstruction-based\nmethods instead prioritize minimizing the discrepancy between\nthe reconstructed output and the raw input, effectively training\nthe network to disregard insignificant data that may contain\nnoise [83, 84, 85]. Reconstruction-based methods generally\nadopt utilize an encoder-decoder architecture to reconstruct\nthe original input data from a modified or incomplete version\nof the input data. They learn meaningful representations by\nemphasizing the salient features and filtering out irrelevant\nor noisy information in the data [86]. Approaches such as\nTimeNet [32] and Deconv [68] reconstruct the whole time\nseries in a reverse order to cater to the characteristics of time\nseries data. Particularly, HeartSpace [87] segments the heart\nrate data into day-long time series because human behavior\npresents day-long regularities. In addition, CRT [88] and\nMBTGMM [89] are able to capture long-term dependencies\nby utilizing transformer encoders compared to those that use\nconvolutional neural networks [68, 87]. Unlike methods such\nas SimMTM [90] that mask certain segments of time series\ndata for construction, CRT [88] drops segments instead, pre-\nserving the original pattern and minimizing noise introduced\nduring representation learning. The effectiveness of learned\nrepresentations is evaluated by how accurately the decoder\ncan regenerate the input data. Such evaluation ensures that\nthe model captures and encodes crucial information from the\ninput while maintaining the capacity to reconstruct it faithfully.\nConsequently, reconstruction can also be utilized as a pretext\ntask into self-supervised learning since the generalization per-\nformance of pre-trained representations in downstream tasks\ncan be further enhanced by reconstructing richer contextual in-\nformation [91]. Below are additional details of reconstruction-\nbased methods used for unsupervised representation learning.\n1) TimeNet [32]: TimeNet tackles the challenges associated\nwith time series data, including the varying lengths and\nthe data scarcity. To reconstruct the time series, TimeNet\nemploys a sequence auto-encoder (SAE) network based on\nthe sequence-to-sequence model to transform the original time\nseries of varying lengths into fixed-dimensional representa-\ntions. The encoder RNN serves as a pre-trained model while\nthe decoder RNN operates in reverse order, aligning with the\ninherent temporal nature of time series data. They are trained\njointly to minimize the reconstruction error on time series.\n2) Deconv [68]: Deconv utilizes deconvolutional networks\nthat employ inverse operations of convolution and pooling to\nreconstruct time series data. This reverse procedure of con-\nvolution enables the reconstruction of hidden representations\nwithin the network. The output of traditional unpooling layers\nis usually an enlarged but sparse activation map, which may\nlead to a loss of expressiveness in complex features required\nfor accurate reconstruction. To address this, Deconv combines\nunpooling and deconvolution techniques to effectively capture\nthe cross-channel correlations with convolutions. By doing so,\nit forces the pooling operation to perform dimension reduction\nalong each position of the individual channel, enabling more\nrobust reconstructions of the time series data.\n3) HeartSpace [87]:\nHeartSpace designs a deep auto-\nencoder module to reconstruct the day-specific time series to\nlatent representations, and optimizes parameters through the\nSiamese-triplet network optimization strategy. The optimiza-\ntion is based on the reconstruction loss and Siamese-triple\nloss, where the Siamese-triple loss is constructed by sampling\nthe intra-series and the inter-series. To mitigate the side effect\ncaused by zero padding, HeartSpace introduces a dual-stage\ngating mechanism into the convolutional auto-encoder module\nto re-weight the hidden units. The channel-wise and temporal-\nwise gating mechanisms focus on capturing dependencies in\nthe channel dimension and temporal dimension, respectively.\n4) MBTGMM [89]: MBTGMM focuses on learning repre-\nsentations of time series and detecting anomalies based on the\nreconstruction error, where the anomalies are associated with\nhigh reconstruction errors. MBTGMM utilizes a multi-branch\ntransformer-based model, which is designed to capture both\nshort-term and long-term temporal dependencies in the time\nseries while learning the context-aware representations. The\nlearned representations are then fed into a Gaussian mixture\nmodel to estimate the density of learned normal representa-\ntions. By identifying low-density regions, MBTGMM is able\nto effectively detect anomalies in the raw time series data.\n5) SimMTM [90]: SimMTM offers an effective approach\nfor recovering masked time points by leveraging the weighted\naggregation of neighbors outside the manifold. Directly mask-\ning a portion of time points can severely disrupt the temporal\nvariations present in the original time series. Such disruption\nposes significant challenges for guiding representation learning\nof time series during reconstruction. SimMTM facilitates the\nreconstruction by assembling the ruined but complementary\ntemporal variations from multiple masked series. In addition\nto the reconstruction loss, SimMTM incorporates a constraint\nloss to guide the series-wise representation learning based on\nthe neighborhood assumption of the time series manifold.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\n6) CRT [88]: Motivated by observations that dropping is\nbetter than masking, adding phase aids in frequency learning,\nand cross-domain learning is more effective than single-\ndomain learning, CRT utilizes the cross-domain dropping-\nreconstruction task to facilitate effective representation learn-\ning. CRT slices the input into patches and randomly drops\nsome sliced patches, where the remaining patches are projected\nas 3 types (time, magnitude, or phase) of embeddings using a\ntransformer encoder. Then, CRT uses a transformer decoder to\nreconstruct original data from the 3 types of data separately,\nenabling the model to receive segments of real data without\ncorrupted zeros portions. This helps maximally preserve the\nglobal context and capture the intrinsic long-term dependen-\ncies.\nV. SELF-SUPERVISED LEARNING METHODS\nIn past years, supervised learning has achieved success in\ndirectly learning semantic information from a vast number\nof labeled samples. However, it requires a large amount\nof annotated training data and is often tailored to specific\ntasks, making the trained models less transferable. To address\nthese challenges, self-supervised learning has emerged as a\npromising technique in representation learning. It eliminates\nthe need for expensive manual labeling by designing diverse\npretext tasks that automatically generate useful supervised\nsignals from the original data. In this section, we classify\nself-supervised learning methods into 3 categories: adversarial\nmethods, predictive methods and contrastive methods. These\ncategories reflect different types of pretext tasks employed in\ndifferent methods to generate self-supervised signals. Adver-\nsarial methods learn representations by distinguishing between\nthe generated fake data and the real data. Predictive methods\nfocus on learning representations by predicting missing or\ntransformed parts of the original input. Contrastive methods\ninstead focus on learning discriminative representations by\nemphasizing similarities and differences between samples.\nA. Adversarial Methods\nAdversarial methods treat distinguishing real data from fake\ndata as a pretext task to learn robust representations for\ntime series [92]. In previous studies, Generative Adversarial\nNetworks (GANs) demonstrate their ability on augmenting\nsmaller time series datasets by generating previously unseen\ndata [93]. The explicit reconstruction cost minimized by GANs\ntends to emphasize the higher-level semantic details in learned\nrepresentations [94]. Conventional GAN-based methods facili-\ntate the acquisition of representations by leveraging backprop-\nagation signals through a competitive process that involves\ntwo parameterized feed-forward neural networks, namely, a\ngenerator and a discriminator [95]. These methods usually\ncreate a two-player mini-max game, where the generator tries\nto improve its ability to deceive the discriminator, and the\ndiscriminator tries to become more adept at distinguishing\nreal from fake data [92]. By iteratively training the generator\nand discriminator in an adversarial manner, such methods\nencourage the generator to learn representations that effec-\ntively capture important characteristics of the underlying data\ndistribution. Adversarial methods differ from reconstruction-\nbased methods in that they consider not only reconstruction\nerrors from the generator but also prediction errors from\nthe discriminator. This joint optimization process encourages\nthe generator to generate more realistic samples while the\ndiscriminator becomes more proficient at detecting generated\nsamples. In addition to conventional GANs, variants of GANs\ncan be designed, such as incorporating RNNs [96] or au-\ntoregressive models [69] to capture temporal dependencies.\nFurthermore, unlike studies [69, 96, 97, 98] that utilize GANs\nto generate synthetic time series data, TimeVAE [70] instead\nemploys variational auto-encoders to facilitate data generation.\nAccording to [70], the standard approach of discriminating\nreal data versus synthetic data is insufficient to capture tem-\nporal dependencies. Hence, TimeVAE [70] injects temporal\nstructures within the decoder to ensure that the synthetic data\nexhibit desired temporal patterns. Further details about the\naforementioned adversarial methods are provided as follows.\n1) TimeGAN [69]: TimeGAN merges the advantages of\nthe GAN framework’s versatility with the control afforded by\ntraining in autoregressive models. It achieves this by integrat-\ning a learned embedding space that is jointly optimized using\nboth supervised and adversarial objectives. This approach\nenables the adversarial network to capture and replicate the\ntemporal dynamics present in the training data during the\nsampling process. The embedding network plays a crucial\nrole by establishing a reversible mapping between the features\nand latent representations, thereby effectively reducing the\ndimensionality of the adversarial learning space.\n2) MAD-GAN [97]:\nMAD-GAN performs multivariate\nanomaly detection with GAN to identify whether the testing\ndata conform to the normal data distributions. MAD-GAN\nleverages a generator and discriminator trained by the GAN\nframework to capture the intricate multivariate correlations\ncontained in time series data. The GAN-trained generator\nproduces fake time series by taking sequences from a random\nlatent space as input, while the GAN-trained discriminator\nlearns to detect fake data from real data in an unsupervised\nmanner. To optimize the model, MAD-GAN designs a com-\nbined anomaly score called DR-score, which incorporates both\ndiscrimination and reconstruction aspects. This allows for the\neffective detection of anomalies within the time series data.\n3) TSGAN [98]: Time Series GAN (TSGAN) is designed\nto address the challenges of generating realistic time series\ndata, particularly in domains where data acquisition is difficult\nor limited. TSGAN utilizes 2 GANs, one regular and one\nconditional, to model and generate synthetic time series ex-\namples. The architecture leverages the concepts of Wasserstein\nGANs (WGANs) [99] and conditional GANs (CGANs) [100]\nto improve the quality and relevance of the generated data.\nTSGAN has shown promising results in generating realistic\n1D signals across various data types, such as sensor, medical,\nsimulated, and motion data. It also demonstrates superior\nperformance in real applications, such as creating classifiers\nbased on synthetic data.\n4) RGAN [96]: Recurrent GAN (RGAN) generates real-\nvalued multi-dimensional time series with improved realism\nand coherence. RGAN works by utilizing recurrent neural\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nnetworks (RNNs) in both the generator and discriminator\ncomponents of the GAN framework. By incorporating RNNs,\nRGAN can capture temporal dependencies and generate se-\nquences that exhibit realistic patterns and dynamics. This\napproach is particularly valuable for generating realistic med-\nical time series data, where capturing temporal relationships\nis crucial for accurate modeling and analysis. The use of\nRNNs in both the generator and discriminator sets RGAN\napart from traditional GAN architectures and contributes to\nits effectiveness in generating realistic time series data.\n5) TimeVAE [70]: TimeVAE utilizes a decoder that allows\nfor the incorporation of user-defined distributions, enabling\nflexibility in the generated data. TimeVAE is trained by using\nthe evidence lower bound loss function. A weight is used\non the reconstruction error, allowing for the adjustment of\nemphasis placed on the reconstruction loss compared to the\nKL-Divergence loss between the encoded latent space distri-\nbution and the prior. In addition, TimeVAE injects temporal\nstructures including level, trend, and seasonal components into\nthe data generation process within the decoder to enhance the\ninterpretability of the modeled data generation process.\n6) DIVERSIFY [101]: DIVERSIFY employs an adversarial\ngame strategy to simultaneously maximize the ’worst-case’\ndistribution scenario while minimizing distribution divergence.\nSpecifically, it learns to segment time series data into distinct\nlatent sub-domains, aiming to maximize the distribution gap\nat the segment level to preserve diversities. Simultaneously,\nit also focuses on reducing distribution divergence between\nthese obtained latent domains to achieve domain-invariant\nrepresentations. This approach leverages the inherent presence\nof diverse latent distributions in time series data, such as\ndistinct activity patterns exhibited by multiple individuals.\nB. Predictive Methods\nPredictive methods learn meaningful representations that\ncapture the underlying shared information between different\nparts of time series data by maximizing the mutual information\nderived from related slices of original time series or diverse\nviews generated by data augmentations. The size of mutual\ninformation indicates the strength of dependencies between re-\nlated slices or augmented views. Compared to reconstruction-\nbased methods, predictive methods help eliminate the need to\nreconstruct the complete input and learn representations by\npredicting future, missing or contextual information. Hence,\nthey usually utilize context prediction or cross-view predic-\ntion as the pretext task to predict partial data based on\nlimited views. The key insight of predictive methods is to\nlearn representations by predicting future or mixed values of\npartial time series [56, 102, 103], predicting whether time\nwindows are sampled from the same temporal context or\nnot [71, 104], or predicting the cross-view representations of\noriginal samples [51]. Such mechanism can be served as a\npretext task for contrastive learning as well, and methods such\nas CPC [56] and TS-TCC [51] combine predictive coding\nand noise-contrastive estimation to induce the latent space to\ncapture information that is maximally useful for prediction.\nFurther elaboration about CPC [56] and TS-TCC [51] will be\nintroduced in Section V-C. Below are the additional details\nof the remaining predictive methods used for unsupervised\nrepresentation learning.\n1) TST [104]: TST employs a transformer encoder to\nextract dense representations of time series. This is achieved\nthrough an input denoising objective, where the model is\ntrained to reconstruct the entire input under noise corruption,\ntypically using Gaussian noise. Unlike methods that rely on\nadditional unlabeled samples, TST maximizes the utilization\nof existing samples for representation learning by employing\nan unsupervised autoregressive objective. TST encourages the\nmodel to attend to both preceding and succeeding segments\nwithin individual variables, as well as the contemporary values\nof the other variables within time series, enabling the model\nto capture complex relationships across different dimensions.\n2) EEG-SSL [71]: EEG-SSL focuses on predicting whether\ntime windows are sampled from the same temporal context\nor not. EEG-SSL aims to learn informative representations\nfrom EEG time series data by employing 2 kinds of temporal-\nbased pretext tasks: relative positioning and temporal shuffling.\nIn the relative positioning task, EEG-SSL predicts whether 2\ntime windows in the EEG signals are closer or farther apart in\ntime. The temporal shuffling task involves predicting whether\nthe selected time windows are in their original order or have\nbeen shuffled. By training on these pretext tasks, EEG-SSL is\nable to capture temporal dependencies and extract meaningful\nrepresentations from EEG time series data.\n3) Wave2vec [102]: Wave2vec aims to predict the next time\nstep based on the given signal context. Wave2vec consists\nof 2 multi-layer convolutional neural networks: an encoder\nnetwork that converts raw audio signals into a latent space,\nand a context network that combines multiple time-steps of the\nencoder’s output to obtain contextualized representations. The\npre-training approach allows Wave2vec to learn meaningful\naudio representations that can capture the temporal dependen-\ncies and facilitate the downstream audio processing tasks.\n4) CaSS [103]: Prior research primarily emphasizes the\npretext task and tends to overlook the intricate issue of\nencoding time series data, which often yields unsatisfactory\noutcomes. In contrast, CaSS comprehensively addresses this\nchallenge from 2 aspects: encoder and pretext task. CaSS\nintroduces a channel-aware transformer architecture, which is\nspecifically designed to capture the intricate relationships be-\ntween different time channels within time series. Furthermore,\nCaSS integrates 2 innovative pretext tasks, namely next trend\nprediction and contextual similarity, to further enhance the\nencoding process.\nC. Contrastive Methods\nContrastive methods learn meaningful representations from\ntime series by optimizing self-discrimination tasks. Instead of\ndirectly modeling the complex raw data, they employ pretext\ntasks that leverage the underlying similarity between samples,\nwhich eliminates the need for reconstructing the complete\ninput and allows for the discovery of contextualized underly-\ning factors of variations [107]. Contrastive methods typically\ngenerate augmented views of the raw data through various\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nModels\nMain Contributions\nBackbones\nDA Datasets\nEvaluations\nMetrics\nSimCLR [43]\nSimCLR advocates for a learnable nonlinear transforma-\ntion bridging the representation and the contrastive loss\nto enhance the quality of learned representations, and\nhighlights the significance of larger batch sizes, more\ntraining steps, and the composition of data augmentations.\nResNet\n✓\nImageNet,\nCIFAR-10\nClassification,\nTransferability\nACC\nTimeCLR [105] TimeCLR extends SimCLR to the time series domain,\namalgamating the benefits of dynamic time warping data\naugmentation tailored for univariate time series, and the\npotent feature extraction capability of InceptionTime, thus\nfacilitating the acquisition of representations.\nInceptionTime ✓\nHand Atlas\nClassification\nACC,\nF1\nMoCo [45]\nMoCo maintains a dynamic queue to enrich the set of\nnegative samples, and proposes a slowly progressing key\nencoder, implemented as a momentum-based moving av-\nerage of the query encoder, to preserve the consistency of\nkey representations.\nResNet\n✓\nImageNet,\nCIFAR-10\nClassification,\nTransferability\nACC\nBYOL [44]\nBYOL removes the need of using negative samples, and\nemploys a predictor on top of the online network to learn\nthe mapping from the online encoder to the target encoder,\nwhich helps prevent mode collapse.\nResNet\n✓\nImageNet,\nCIFAR,\nSUN397,\nVOC07, DTD\nClassification,\nTransferability\nACC\nCPC [56]\nCPC extracts compact latent representations to encode\npredictions over the future observations by combining\nautoregressive modeling and noise-contrastive estimation\nwith intuitions from predictive coding.\nResNet, GRU\nLibriSpeech,\nImageNet,\nBookCorpus\nClassification\nACC\nSimSiam [57]\nSimsiam learns representations by using Siamese archi-\ntectures without negative sample pairs, large batches and\nmomentum encoders. SimSiam tackles the mode collapse\nproblem by using the stop-gradient mechanism.\nResNet\n✓\nImageNet,\nVOC07, COCO\nClassification,\nTransferability\nACC\nMCL [106]\nMCL learns representations through the injection of noise.\nInspired by label smoothing, MCL adopts mixup that\ncreates new samples by convex combinations of training\nexamples, and predicts the strength of the mixing compo-\nnent based on 2 data points and the augmented sample.\nFCN\n✓\nUCR\nDatasets,\nUEA Datasets\nClassification,\nTransferability\nACC\nTABLE IV\nSUMMARY OF INSTANCE-LEVEL CONTRASTIVE METHODS FOR TIME SERIES REPRESENTATION LEARNING. DA INDICATES WHETHER THE MODEL\nUTILIZES DATA AUGMENTATIONS. ACC REFERS TO ACCURACY, F1 REFERS TO F1 SCORE.\ntransformations and then learn representations by contrasting\npositive samples against negative samples [108]. Exploring\nnegative samples in a completely unsupervised manner can\nbe challenging for contrastive learning, as it often encounters\nhard negative samples with features that are highly similar to\nanchors but have different labels. To address this, researchers\nhave explored various techniques to introduce more negatives,\nsuch as increasing the batch size [43] or exploiting external\ndata structures [45, 58]. There are also studies that remove\nthe need of using negative samples and instead use other\nstrategies such as utilizing a predictor [44], a stop-gradient op-\neration [57], or clustering [59] to learn the mapping. Within the\nrealm of computer vision, contrastive methods are commonly\ncategorized into instance-level and prototype-level methods,\nwith each approach focusing on different levels of contrast.\nAdditionally, there are specific methods tailored for time series\ndata, which consider the temporal-level contrast. We analyze\nthe similarities and differences between these representative\ncontrastive models in terms of their main contributions, back-\nbone architectures, data augmentations (DA), datasets, evalua-\ntion strategies, and performance metrics. Contrastive learning\nenables the discovery of informative representations in time\nseries data without relying on explicit reconstruction tasks.\nThis analysis provides insights into the key characteristics and\napproaches of representative contrastive models in the field\nof representation learning for time series data. The concise\noverview of instance-level, prototype-level, and temporal-level\nmethods can be found in Table IV, Table V, and Table VI,\nrespectively. The following sections provide more in-depth\ndetails about these methods.\n1) Instance-level Contrastive Learning Models: Instance-\nlevel contrastive learning models treat individual samples\nindependently for the purpose of instance discrimination. They\nutilize data augmentations to transform original inputs into\na new embedding space. Within this space, augmentations\nderived from the same sample are considered as positive pairs,\nwhile those from different samples are treated as negative\npairs. During training, these models are optimized by maxi-\nmizing the similarity between representations of positive pairs,\nwhile simultaneously minimizing the similarity between repre-\nsentations of negative pairs. Instance-level contrastive methods\nachieve significant performance improvement by setting larger\nbatch size [43], using stronger augmentations [105, 106],\nor introducing an additional storage space to store more\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\ncontrastive candidates [45, 109]. However, these methods can\nbe memory-intensive, and ensuring the consistency of repre-\nsentations is challenging when randomly extracting negative\nsamples from an additional storage space. Recent advance-\nments in non-contrastive learning, which eliminate the need for\nnegative samples, have exhibited promising outcomes [110].\nBYOL [44] and SimSiam [57] serve as exemplars, achieving\nremarkable results without the reliance on negative samples.\nThese methods leverage an additional learnable predictor\nand employ a stop-gradient operation to prevent collapsing,\ncontributing to their successful performance [111]. Table IV\nprovides a succinct summary of instance-level contrastive\nmethods for time series representation learning. Further elab-\noration concerning these methods are explicated below.\na) SimCLR [43]: SimCLR greatly improves the quality\nof representation learning by introducing a learnable nonlinear\nprojection between the representation and the contrastive loss.\nSince the representation obtained from the encoder retains\ninformation related to data augmentations, the role of the\nnonlinear layer is to remove such information and allow repre-\nsentations to reflect the essence of data. This aids in preventing\nsimilarity-based losses from discarding crucial features during\ntraining. SimCLR randomly draws a minibatch of N samples\nand generates 2N augmented views by applying geometric\nand appearance transformations sequentially from the same\nminibatch. Among these augmented views, a positive pair is\nformed by selecting 2 views derived from the same original\nsample. The remaining 2(N −1) augmented views within the\nminibatch are treated as negative ones. Such a sampling strat-\negy has been widely adopted in various contrastive models.\nb) TimeCLR [105]: Direct applying SimCLR to the time\nseries field usually performs poorly due to data augmentation\nand the feature extractor not being adapted to the temporal\ndependencies within the time series data. TimeCLR adopts\nDynamic Time Warping (DTW) data augmentation not only\ngenerates DTW-targeted phase shifts and amplitude changes,\nbut also retains the structure and feature information of\nthe time series. In addition, TimeCLR adopts InceptionTime\nwhich has good feature extraction capabilities as the feature\nextractor, to convert the time series into the corresponding\nrepresentations in an end-to-end manner.\nc) MoCo [45]: MoCo leverages a moving-averaged mo-\nmentum encoder and maintains a dynamic queue to enhance\nnegative sampling in contrastive learning. The dynamic queue\nstores a diverse set of samples, enabling the decoupling of\ndictionary size from the minibatch size and facilitating the\ncontrastive learning process. In each minibatch, MoCo treats\nthe encoded queries and their corresponding keys as positive\nsample pairs, while the negative samples are selected from the\nremaining samples in the dynamic queue. This approach helps\nenhance the quality and diversity of the contrastive learning\nprocess in MoCo.\nd) BYOL [44]: BYOL utilizes a pair of neural networks\nto perform training, namely the online network and the target\nnetwork. The online network is trained to predict the represen-\ntation of the target network, and the target network is updated\nwith a slow-moving average of the online network. BYOL\nexclusively treats 2 different augmented views of the same\nsample as the positive pair and learns representations without\nexplicitly using negative pairs, which provides new insight into\nnegative sampling in contrastive learning. Then, the augmen-\ntations of the same sample are fed into both the online and\ntarget networks, which interact and mutually learn from one\nanother. BYOL directly bootstraps the representations, which\nmakes it more robust to the choice of augmentations.\ne) CPC [56]: Contrastive Predictive Coding (CPC) en-\ncodes the underlying shared information between different\nparts of the original high-dimensional signal, while discarding\nlow-level information and noise that is more local. The com-\nbination of predictive coding and a probabilistic contrastive\nloss enables CPC to extract slow features that maximize\nthe mutual information between the encoded representations\nthat span many time steps. CPC utilizes an encoder and an\nautoregressive model to reduce the dimensionality of the high-\ndimensional signal, and then uses noise-contrastive estima-\ntion to predict future representations. The encoder maps the\ninput high-dimensional signal to a latent vector in a lower-\ndimensional space, and the autoregressive model aggregates\nthe latent vectors from previous and current time steps.\nf) SimSiam [57]: Simsiam explores the use of Siamese\narchitectures in representation learning and adopts the stop-\ngradient operation to prevent collapsing, while getting rid of\nnegative samples, large batches and momentum encoders. The\nlearning mechanism of SimSiam is similar to the Expectation-\nMaximization (EM) algorithm to estimate the expected value\nafter data augmentation. Siamese networks are chosen as the\nbackbone due to their ability to incorporate inductive biases for\nmodeling invariance, which aids in capturing essential features\nduring representation learning.\ng) MCL [106]: The inherent invariances within time\nseries data are often unknown beforehand, and incautious\napplication can lead to representations where dissimilar sam-\nples are embedded in close proximity. MCL exploits a data\naugmentation scheme in which new samples are generated\nby mixing 2 data samples with a mixing component. The\npretext task motivated through label smoothing is to predict\nthe strength of the mixing component based on the 2 data\npoints and the augmented sample. In addition, MCL is tasked\nwith predicting the mixing factor instead of hard decisions to\ntackle the problem of overconfidence in neural networks.\n2) Prototype-level Contrastive Learning Models: To ad-\ndress the limitation that instance-level contrastive learning\nmodels tend to treat semantically similar samples as nega-\ntives, prototype-level contrastive learning models break the\nindependence between samples, and explore to exploit the\nimplicit semantics shared by samples within the same cluster.\nBy incorporating the additional prior information brought by\nclustering, the learned representations are expected to preserve\nmore higher-level semantic information [60]. Prototype-level\ncontrastive learning models consider groups of similar samples\nas clusters and pull the representations of samples from\ndifferent augmented views but belonging to the same class\n(with similar semantics) together in the embedding space\nduring contrastive learning. For instance, CC [81], SwAV [59]\nand PCL [58] are such methods that perform clustering-based\ndiscrimination between groups of similar samples. However,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\nModels\nMain Contributions\nBackbones\nDA Datasets\nEvaluations\nMetrics\nSwAV [59]\nSwAV optimizes the learned representations by developing\na swapped prediction mechanism to simultaneously perform\nscalable online clustering while enforcing the consistency\nbetween cluster assignments produced for different aug-\nmented views of the same sample.\nResNet\n✓\nImageNet,\nVOC07, COCO\nClassification,\nTransferability\nACC\nPCL [58]\nPCL formulates prototypical contrastive learning as an\nExpectation-Maximization algorithm to perform clustering\nand representation learning iteratively. The E-step aims to\nfind the distribution of prototypes via clustering and the M-\nstep aims to optimize the network via contrastive learning.\nResNet\n✓\nImageNet,\nVOC07,\nPlaces205,\nCOCO\nClassification,\nClustering,\nObject Detection\nACC,\nAMI\nCCL [64]\nCCL refines the learned representations acquired through\ndeep convolutional neural networks by discovering dataset\nclusters with high purity and typically few samples per clus-\nter, and leverage these cluster assignments as the potentially\nnoisy supervision.\nCNN\n✓\nBBT-0101,\nBF-\n0502, ACCIO\nClassification,\nClustering\nACC,\nPrecision,\nRecall,\nF1\nSCCL [80]\nSCCL leverages contrastive learning for short text cluster-\ning to promote better separated and less dispersed clusters.\nIt effectively combines the top-down clustering with the\nbottom-up instance-wise contrastive learning to achieve\nbetter inter-cluster distance and intra-cluster distance.\nDistilBERT\n✓\nAgNews,\nTweet,\nSearchSnippets,\nStackOverflow,\nBiomedical,\nGooglenews\nClustering\nACC,\nNMI\nCC [81]\nCC seamlessly integrates deep clustering and representation\nlearning by revealing that the row and column of the feature\nmatrix intrinsically correspond to the instance and cluster\nrepresentation when projecting instances into a subspace\nwhose dimensionality is equal to the cluster number.\nResNet\n✓\nCIFAR-10,\nCIFAR-100,\nSTL-10,\nImageNet-10\nClassification,\nClustering\nACC,\nNMI,\nARI\nSLIC [82]\nSLIC combines iterative clustering with multi-view en-\ncoding and temporal discrimination to learn view-invariant\nvideo representations and fine-grained motion features. The\nclustering assignments are used to guide the sampling of\npositive and negative pairs for updating representations.\nCNN\n✓\nUCF101,\nHMDB51,\nKinetics400\nClassification,\nVideo Retrieval\nRecall\nMHCCL [60] MHCCL incorporates the implicit semantic information ob-\ntained from hierarchical clustering to guide the construction\nof contrastive pairs. MHCCL exploits downward masking\nto filter out fake negatives and supplement positives, while\nalso employing upward masking to refine prototypes.\nResNet\n✓\nHAR,\nWISDM,\nSHAR, Epilepsy,\nUEA Datasets\nClassification\nACC,\nMF1, κ\nTABLE V\nSUMMARY OF PROTOTYPE-LEVEL CONTRASTIVE METHODS FOR TIME SERIES REPRESENTATION LEARNING. DA INDICATES WHETHER THE MODEL\nUTILIZES DATA AUGMENTATIONS. ACC REFERS TO ACCURACY, AMI REFERS TO ADJUSTED MUTUAL INFORMATION, NMI REFERS TO NORMALIZED\nMUTUAL INFORMATION, F1 REFERS TO F1 SCORE, ARI REFERS TO ADJUSTED RAND INDEX, MF1 REFERS TO MACRO-AVERAGED F1 SCORE, AND κ\nREFERS TO COHEN’S KAPPA COEFFICIENT.\nthese methods require prior knowledge to pre-specify the\nnumber of clusters, which is non-trivial for the unlabeled\ntime series data. What’s worse, the flat clustering algorithms\nthey adopt can only capture a single hierarchy of semantics,\nwhich is insufficient to reflect the ground-truth data distri-\nbutions. Recently, methods such as CCL [64], SLIC [82]\nand MHCCL [60] have merged to incorporate hierarchical\nclustering into contrastive learning. By exploring the hierar-\nchical structures, these methods are able to capture diverse\ngranularities of semantics in time series data. The differences\namong the varying prototype-level contrastive learning models\nlie in the selection of cluster centroids, clustering methods, and\ncluster-level sampling strategies. Table V provides a succinct\nsummary of prototype-level contrastive methods for time series\nrepresentation learning. Further elaboration on these methods\nis explicated below.\na) SwAV [59]: SwAV enhances representation learning\nin a distinctive manner by leveraging cluster assignments\nand a swapped prediction mechanism to perform contrasting.\nSwAV assigns samples to different clusters rather than directly\ncontrasting samples, where the cluster assignments are used to\nconstruct the implied contrastive pairs. Instead of approximat-\ning each sample into a hard assignment, the soft assignment is\nproduced by the Sinkhorn-Knopp algorithm. Then, the implied\npositive pair is formed by the one augmented view and the\ncluster assignment of the other view, and the remaining pairs\nare treated as implied negative pairs. The swapped prediction\nmechanism involves predicting the cluster assignment of one\naugmented view from the other augmented view.\nb) PCL [58]:\nPCL formulates the prototypical con-\ntrastive learning framework as an Expectation-Maximization\n(EM) algorithm. The framework consists of 2 iterative steps:\nthe E-step and the M-step. In the E-step, PCL performs\nclustering to find the distribution of prototypes. In the M-\nstep, the network is optimized through contrastive learning.\nThe total loss is composed of a standard InfoNCE loss for\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\ninstance-level contrast and a ProtoNCE loss for prototype-\nlevel contrast. The instance-level sampling follows the same\nstrategy as MoCo [45]. For prototype-level contrast, each\nsample forms a positive pair with its corresponding prototype,\nwhich represents the centroid of the cluster it belongs to. The\nremaining prototypes are considered negative ones.\nc) CCL [64]:\nClustering-based Contrastive Learning\n(CCL) learns cluster-based representations without relying on\nprior knowledge of the number of clusters. It automatically\nobtains labels from natural groupings using hierarchical clus-\ntering that does not require any hyper-parameters. To construct\npositive and negative pairs, CCL leverages clusters with high\npurity and a few samples at early hierarchical partitions. For\neach data point within a cluster, CCL randomly samples from\nthe same or nearest cluster to create one positive pair, while\nusing samples from the farthest clusters to create 2 negative\npairs. This approach ensures that the generated pairs capture\nboth intra-cluster similarities and inter-cluster differences.\nd) SCCL [80]: SCCL focuses on short text data and\ndevelops a joint model that leverages the beneficial properties\nof instance-wise contrastive learning to improve unsupervised\nclustering. This is implemented by jointly optimizing a top-\ndown clustering loss over the original data samples and a\nbottom-up instance-wise contrastive loss over the associated\naugmented pairs. The Student’s t-distribution is utilized to\ncompute the probability of assigning each sample to different\nclusters. The instance-wise contrastive learning focuses on\ndistance discrimination, while unsupervised clustering focuses\non clustering-based discrimination between groups of samples\nwith higher-level semantic concepts.\ne) CC [81]: Contrastive Clustering (CC) regards rows\nof the feature matrix projected from augmentations as soft\nlabels of samples, while treating columns as prototype-level\nrepresentations distributed over the dataset. This is motivated\nby the observation of “label as representation” when the di-\nmensionality of representations during projection matches the\nnumber of clusters. CC simultaneously learns discriminative\nrepresentations and performs online clustering in an end-to-\nend manner. It adopts a dual contrastive learning strategy that\noperates at both the instance level and cluster level to extract\ninformative representations.\nf) SLIC [82]: SLIC is an iterative clustering-based con-\ntrastive learning framework to learn feature representations\nfrom human action videos. SLIC integrates iterative clustering\nwith multi-view encoding and a temporal discrimination loss\nto sample harder positives and negatives during pre-training.\nThe encoder is optimized by incorporating a temporal dis-\ncrimination loss and an instance-based triplet loss. To be\nspecific, SLIC alternates between periodically clustering video\nrepresentations to produce cluster assignments, which are used\nto inform the sampling of positive and negative pairs to update\nthe video representations, by minimizing a triplet margin loss.\ng) MHCCL [60]: MHCCL incorporates implicit hierar-\nchical semantic information obtained by hierarchical clustering\ninto contrastive learning. This is inspired by the observation\nof the multi-granularity of clustering. The novel bidirectional\nmasking strategies are employed to guide the selection of posi-\ntive and negative pairs for multi-level contrast. Specifically, up-\nward masking helps remove outliers while refining prototypes\nto accelerate the hierarchical clustering process and enhance\nthe quality of clustering. Meanwhile, downward masking helps\nsupplement latent positives and filter out fake negatives for\neffective contrastive learning. In this way, MHCCL enables\nthe extraction of informative representations that capture both\nfine-grained and high-level semantic information.\n3) Temporal-level Contrastive Learning Models: Although\ncontrastive learning methods have demonstrated remarkable\nsuccess in the field of computer vision, their application\nto time series data often ignores the intricate characteristics\nthat are inherent to such data. Acknowledging this limitation,\nresearchers have recently turned their attention towards investi-\ngating the influence of timestamps, and have begun developing\nspecifically tailored strategies to address the challenge posed\nby time series data. Instance-level contrastive learning models\nare able to learn general representations that capture the overall\ncharacteristics of the entire time series. These representations\nprovide a holistic view of data and can be useful for tasks that\nrequire a global understanding of the sequence. Temporal-level\ncontrastive learning models instead focus on capturing scale-\ninvariant representations at each individual timestamp. By con-\nsidering both instance-level and temporal-level representation\nlearning strategies, researchers aim to enhance the capability\nof contrastive learning methods in capturing the complexities\ninherent in time series data. Existing temporal-level contrastive\nlearning models either consider the temporal dependencies\nby leveraging temporal contrasting modules [33, 50, 51], or\nfocus on capturing multi-scale contextual information across\ndifferent granularities [52, 61, 112, 113]. Approaches such\nas T-Loss [50] and TNC [33] utilize the information from\nthe neighborhood to construct positive and negative samples\nfor contrastive learning. TS-TCC [51] instead designs time-\nseries-specific augmented views for pair constructions and\nperforms cross-view temporal contrasting to learn represen-\ntations. There also exist methods that explicitly emphasize\nthe timestamp-level [52] or segment-level [61, 112, 113]\nrepresentations in addition to the entire time-series level.\nThese models are designed to capture fine-grained temporal\npatterns and variations, enabling them to extract more detailed\nand localized information from time series. However, they\nmay ignore the higher-level semantic information which is\ninvolved in the entire set of time series. Incorporating potential\nsemantic information such as class labels can help eliminate\nfake negative samples of the same class, thereby reducing\nnoise in contrastive learning. Table VI provides a succinct\nsummary of temporal-level contrastive methods for time series\nrepresentation learning. Further elaboration on these methods\nis explicated below.\na) TS2Vec [52]: TS2Vec utilizes multi-scale contextual\ninformation with granularities to distinguish samples. It en-\nables the learning of timestamp-level representations, while\nalso supporting instance-level representations of the entire time\nseries by applying max pooling strategy across timestamps.\nTS2Vec eliminates the need of the input projection layer so\nthat masks can be directly applied to raw input by setting\nmasked timestamps to zeros. Through the masking of latent\nvectors rather than the raw values, the model acquires the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n13\nModels\nMain Contributions\nBackbones\nDA Datasets\nEvaluations\nMetrics\nTS2Vec [52] TS2Vec utilizes multi-scale contextual information at both\ntimestamp-level and instance-level to distinguish positive\nand negative samples, thereby improving the generaliza-\ntion capability of the representation model and effectively\nhandling time series data with missing values.\nDilated CNN ✓\nUEA\nDatasets,\nETT\nDatasets,\nElectricity\nClassification,\nForecasting\nACC,\nMSE,\nMAE\nTS-TCC [51] TS-TCC constructs simple yet efficient time-series-specific\naugmented views to perform temporal and contextual con-\ntrasting, and designs a tough cross-view prediction task to\nlearn the robust and discriminative representations.\nCNN, Trans-\nformer\n✓\nHAR, Sleep-EDF,\nEpilepsy, FD\nClassification,\nTransferability\nACC, MF1\nTNC [33]\nTNC learns the underlying dynamics of non-stationary\nsignals and models the progression over time by defining\na temporal neighborhood. It incorporates concepts from\nPositive-Unlabeled learning to account for potential bias\nintroduced in sampling negative examples for contrastive\nloss.\nBidirectional\nRNN\nSimulation, ECG\nWaveform, HAR\nClassification,\nClustering\nACC,\nAUPRC,\nSilhouette\nScore, DBI\nTCL [61]\nTCL learns representations for time series that allow\noptimal discrimination of different time segments based on\nthe temporal non-stationary structure captured by nonlinear\nindependent component analysis.\nNonlinear\nICA\nMEG\nClassification\nACC\nT-Loss [50]\nT-Loss learns scalable representations by taking highly\nvariable lengths and sparse labeling properties of time\nseries data into account. It employs an efficient triplet loss\nwith time-based negative sampling to differentiate anchors\nfrom negatives, and assimilate anchors with positives.\nCausal CNN\nUEA\nDatasets,\nUCI\nDatasets,\nUCR Datasets\nClassification,\nTransferability\nACC\nBTSF [112]\nBTSF applies dropout to generate diverse views for repre-\nsentation learning, and devises iterative bilinear temporal-\nspectral fusion to explicitly model pairwise cross-domain\ndependencies for discriminating and enriching representa-\ntions in a fusion-and-squeeze manner.\nCausal CNN\n✓\nHAR, Sleep-EDF,\nECG\nWaveform,\nETT\nDatasets,\nWeather,\nSAaT,\nWADI,\nSMD,\nSMAP, MSL\nClassification,\nForecasting,\nAnomaly\nDetection\nACC,\nAUPRC,\nMSE,\nMAE, F1\nCoST [113]\nCoST simulates interventions on the error variable via data\naugmentation and exploits prior knowledge to learn time\nseries representations. It leverages inductive biases in the\nmodel architecture to learn disentangled seasonal and trend\nrepresentations via contrastive learning.\nCausal CNN\n✓\nETT\nDatasets,\nElectricity,\nWeather\nForecasting\nMSE,\nMAE\nTABLE VI\nSUMMARY OF TEMPORAL-LEVEL CONTRASTIVE METHODS FOR TIME SERIES REPRESENTATION LEARNING. DA INDICATES WHETHER THE MODEL\nUTILIZES DATA AUGMENTATIONS. ACC REFERS TO ACCURACY, AUPRC REFERS TO THE AREA UNDER THE PRECISION-RECALL CURVE, DBI REFERS TO\nDAVIES-BOULDIN INDEX, F1 REFERS TO F1 SCORE, MF1 REFERS TO MACRO-AVERAGED F1 SCORE, MSE REFERS TO MEAN SQUARE ERROR, AND MAE\nREFERS TO MEAN ABSOLUTE ERROR.\ncapability to distinguish between masking tokens and original\nvalues.\nb) TS-TCC [51]: TS-TCC employs a combination of\ntime-series-specific weak augmentations (i.e., jitter-and-scale)\nand strong augmentations (i.e., permutation-and-jitter) on orig-\ninal data. This approach aims to enhance the quality of the\nlearned features through contrastive learning. TS-TCC designs\ncross-view temporal and contextual contrasting modules to\nlearn robust and discriminative representations. The temporal\ncontrasting module focuses on capturing the temporal de-\npendency within the data. The contextual contrasting module\nperforms a cross-view prediction task to predict the future of\none view based on the past of the other view.\nc) TNC [33]: TNC addresses potential bias in negative\nsamples by employing a sample weight adjustment strategy.\nInstead of randomly selecting negative examples from the data\ndistribution, TNC utilizes Positive-Unlabeled (PU) learning\nwithout explicitly using negative samples. In PU learning, the\ncombinations of positive and negative samples are treated as\nunlabeled data. TNC considers samples from the neighborhood\nas positive examples and samples outside the neighborhood\nas unlabeled examples. This approach helps mitigate bias and\nimproves the accuracy of the learning process.\nd) TCL [61]: TCL aims to learn representations that cap-\nture the temporal non-stationary structure of time series data,\nenabling optimal discrimination of different time segments.\nTCL combines a heuristic principle for analyzing temporal\nstructure with a rigorous treatment of a nonlinear independent\ncomponent analysis model [114]. The model is trained through\na multinomial logistic regression classifier, which aims to\naccurately discriminate all time segments in a time series by\nutilizing the segment indices as labels for the data points.\ne) T-Loss [50]: T-Loss learns scalable general-purpose\nrepresentations by considering inherent characteristics of time\nseries, including highly variable lengths and sparse labeling.\nIt introduces an efficient unsupervised triplet loss that utilizes\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n14\ntime-based negative sampling and leverages the encoder’s\nresilience to handle time series of unequal lengths. A random\nsub-series from a given time series is considered the anchor,\nwhile any sub-series of the anchor is treated as the positive\nsample. Sub-series from randomly selected series are treated\nas negative samples. The model is optimized to distinguish an-\nchors from negatives while assimilating anchors and positives,\nenabling the learning of discriminative representations.\nf) BTSF [112]: Prior works primarily use time-based\naugmentations to sample positive and negative pairs for con-\ntrastive training. However, these approaches mainly rely on\nsegment-level augmentations derived from time slicing, which\ncan introduce sampling bias and incorrect optimization due to\nthe loss of global context. Furthermore, they often overlook the\nincorporation of spectral information in representations. BTSF\ndevises a novel iterative bilinear temporal-spectral fusion\napproach to explicitly encode the affinities of numerous time-\nfrequency pairs, and employs instance-level augmentation with\na simple dropout applied to the entire time series, thereby\ncapturing long-term dependencies more effectively.\ng) CoST [113]: An effective representation should pos-\nsess the capability to disentangle multiple explanatory sources,\nenabling robustness against complex and richly structured\nvariations. CoST leverages inductive biases within the model\narchitecture to learn disentangled seasonal and trend rep-\nresentations. Additionally, it introduces a novel frequency\ndomain contrastive loss to encourage the development of\ndiscriminative seasonal representations. By providing insights\nfrom a causal perspective, CoST highlights the advantages of\nlearning disentangled seasonal-trend representations through\ncontrastive learning for time series forecasting.\nVI. CONCLUSION AND FUTURE DIRECTIONS\nA. Summary of The Review\nTime series representation learning aims to map the time\nseries into a latent space where the learned semantic-rich\nrepresentations then can be effectively utilized to benefit down-\nstream tasks such as classification and forecasting. In recent\nyears, numerous unsupervised representation learning methods\nhave been proposed, resulting in a dilemma over selecting\nthe most suitable approach for downstream tasks of interest,\nespecially those tasks on time series data. To fill this gap, in\nthis work, we conduct a thorough literature review of existing\nmethods for time series representation learning, aiming to\npropose a comprehensive categorization of these approaches.\nIn addition to analysis, we perform empirical evaluations, with\na focus on state-of-the-art contrastive methods, using 9 public\nreal-world datasets from diverse domains. The purpose of this\nexperimental evaluation is to offer readers a better, faster, and\nmore comprehensive understanding of cutting-edge research\nin unsupervised time series representation learning.\nB. Future Research Directions\nContrastive learning has emerged as a powerful unsuper-\nvised learning approach due to its capability for capturing rich\nrepresentations without relying on explicit labels. However,\nthere exist notable challenges that need to be addressed to\nfully harness the potential of contrastive learning. One critical\nchallenge that warrants consideration is automatic data aug-\nmentation, which plays a crucial role in contrastive learning as\nit generates diverse data covering unexplored input space while\npreserving correct labels for learning more robust and gener-\nalizable representations. However, the augmentation transfor-\nmations selected by current manual selection strategies based\non empirical observations or domain knowledge can be sub-\noptimal. Hence, developing efficient strategies for automatic\ndata augmentation optimization is imperative. In addition, the\nprimary goal of contrastive learning is to distinguish between\nsimilar and dissimilar samples, promoting the learning of\ndiscriminative features. Consequently, constructing effective\ncontrastive pairs becomes crucial, particularly in selecting\nappropriate negative sampling methods, as they directly influ-\nence the quality and informativeness of the learned represen-\ntations. Moreover, efficiency and scalability pose significant\nchallenges for contrastive learning models, especially as the\namount and dimensions of input data expand. Developing\nscalable architectures and efficient algorithms is essential to\nhandle the computational demands of large-scale real-world\ntasks. Furthermore, overfitting and poor generalization are\nalso common challenges encountered in contrastive learning\nmodels. Pre-trained models often struggle to generalize to new\ndata distributions with different statistical properties from the\ntraining data, which can hinder their performance on samples\nfrom novel classes or unseen domains. By addressing these\nchallenges, the potential of contrastive learning can be fully\nrealized, enabling the extraction of rich representations from\nunlabeled data, and facilitating breakthroughs in diverse appli-\ncations. we delve deeper into the aforementioned challenges\nin the following.\n1) Automatic Data Augmentation Optimization: Data aug-\nmentation is a vital component in contrastive learning as it\nincreases diversity and mitigates the risk of false negatives. A\ntheoretical framework [115] has been proposed to assess the\ntransferability of contrastive learning, with a focus on investi-\ngating the impact of data augmentation on its performance.\nThe findings indicate that the effectiveness of contrastive\nlearning in downstream tasks is heavily influenced by the\nselection of data augmentation techniques. The selection of\nappropriate augmentations is critical to ensure the preservation\nof semantic consistency among labels and to foster general-\nization and robustness, thereby preventing overfitting on the\noriginal data. Automating the process of augmentation opti-\nmization becomes imperative in order to efficiently identify the\nmost effective augmentation transformations that maximize the\nlearning potential of the model while maintaining the desired\nproperties of the representations [116]. The future direction\ninvolves developing novel augmentation transformations tai-\nlored to time series data and devising efficient strategies to\nautomatically optimize the combinations and parameters of\naugmentation transformations within a fixed transformation\nspace. Existing data augmentation transformations for time\nseries data [51, 52, 117, 118] are heuristic and are usually sub-\noptimal as they are manually designed or selected based on\nempirical observations or domain knowledge [119]. There are\nseveral existing works that focus on augmentations specifically\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n15\ndesigned for image data. For instance, Mixup-noise is utilized\nin [120] to generate augmentations by mixing data samples\neither at the input or hidden-state levels, without explicitly\nleveraging the underlying data manifold structure. Distribu-\ntion divergence is employed in\n[121] between weakly and\nstrongly augmented samples over the representation bank to\nsupervise the retrieval of strongly augmented queries from a\npool of instances. Time series data possess unique charac-\nteristics and temporal dependencies that require specialized\ntransformations. Conventional time series data augmentations\nmay disrupt the inherent strong temporal dependencies present\nin the data [105]. In this case, designing novel augmentation\ntransformations that can capture temporal patterns, variations,\nand dependencies in time series data is still an open problem\nthat requires further exploration. In addition, optimizing the\ncombinations and parameters of various augmentation trans-\nformations becomes challenging due to the large transforma-\ntion space involved. Every augmentation promotes invariance\nto a specific transformation, which can be advantageous in\ncertain scenarios and detrimental in others [122]. Reinforce-\nment Learning (RL) emerges as a promising solution to\nautomatically search for optimal sub-policies of augmentation\ntransformations and their associated parameters. RL-based\nmethods [123, 124] formulate the optimization as a discrete\nsearch problem and use reward signals to guide the search\nprocess. However, these approaches often demand extensive\ncomputational resources due to the size of the search space,\nmaking them inefficient for practical usage. Thus, it is still\nan ongoing problem to design more efficient strategies for\nautomatic data augmentation optimization.\n2) Designing Appropriate Contrasting Views: The selection\nof appropriate augmentation transformations to form the pos-\nitive pairs is a decisive aspect in contrastive learning [114].\nDesigning appropriate contrasting views usually requires do-\nmain knowledge, intuition, trial-and-error and luck [125].\nExisting studies [126, 127] suggest that contrastive learning\ntends to generate sub-optimal representations in the presence\nof noisy views such as false positive pairs. This is because\nrepresentations of these noisy views are forced to align with\neach other even if there is no apparent shared information.\nHowever, there is no guarantee that all task-relevant infor-\nmation is shared between views [128]. Current contrastive\nlearning approaches [43, 45, 55, 58] commonly consider\neither the remaining data within the same batch or a large\nbuffer as the default source of negative samples. However,\nsamples with similar higher-level implicit semantics, are easily\ntreated as negative ones [60, 64, 82]. Recent studies has\nstarted to tackle this problem by incorporating the information\nof nearest-neighbors to enrich the set of semantic positives\nas they find that a generic query sample and its neighbors\nare likely to belong to the same class [129, 130, 131].\nNote that, however, such binary instance labeling is insuf-\nficient to measure correlations between different samples,\nthere still exist numerous promising avenues for exploring\ndifferent approaches to measure the similarity of neighbors,\noffering greater semantic variations compared to pre-defined\ntransformations [132]. Negative sampling is also critical in\nconstructing contrastive pairs, yet choosing the appropriate\nsampling method and number of negative samples can be\nchallenging. The presence of a collision-coverage trade-off\nindicates that the ideal number of negative examples should be\nscaled according to the number of underlying concepts present\nin the data [133, 134]. The occurrence of false negatives\nadversely affects the discrimination of individual samples as\nthey are erroneously pushed away [60]. One direction of\nexploration is to eliminate the need of negative samples [44].\nHowever, negative cancellation relies on a sufficient number\nof clearly distinguishable positive samples. If the positive\nsamples lack clear separability or exhibit significant intra-\nclass variations, the model may still encounter difficulties\nin discerning between positive and negative pairs, leading to\nsub-optimal performance. An alternative direction to enhance\nnegative sampling is to employ filtering techniques. These\ntechniques involve selecting negative samples that meet spe-\ncific criteria, such as similarity or density thresholds, to help\nthe model focus on more informative and diverse negatives.\nHowever, it is important to ensure that valuable negative\nsamples are not inadvertently excluded, and that the selected\nnegatives effectively challenge the discrimination ability of\nmodels. In addition, clustering techniques can also be em-\nployed to group similar negative samples together, reducing\nthe chance of false negatives and promoting more effective\ndiscrimination [60, 64, 80].\n3) Efficiency and Scalability: Due to the inherent charac-\nteristics of time series data including variable duration, high\ndimensionality and high frequency, a time series represen-\ntation learning framework should satisfy 2 essential criteria\nsimultaneously: efficiency and scalability [135, 136]. Although\nresearchers have made notable efforts in improving the effi-\nciency and scalability of contrastive models such as adopting\nearly stopping, increasing batch size, applying diverse data\naugmentations and adopting contrastive regularizations [44,\n45, 55, 136], there are still unexplored directions. One poten-\ntial direction is to develop memory-efficient network architec-\ntures to reduce the computational and memory requirements\nof contrastive models, such as those equipped with weight\nsharing or pruning. Another way is to incorporate transfer\nlearning, where pre-trained models serve as an initial stage for\ncontrastive learning. Such approaches leverage the knowledge\ngained from a pre-trained model and fine-tune the model\nwith contrastive learning, thereby reducing training time and\ncomputational resources. Additionally, distillation techniques,\nwhich compress large models into smaller ones while preserv-\ning functionality, are also worth exploring. These approaches\nhave the potential to reduce training time and memory require-\nments in contrastive learning while maintaining or enhancing\nperformance. Improving the scalability of contrastive learning\nmodels is crucial particularly when encountered with large-\nscale and high-dimensional time series inputs. Techniques such\nas distributed training, approximate nearest neighbor search\nand online learning can also be incorporated to enhance the\nscalability of contrastive models while ensuring efficiency.\n4) Overfitting and Generalization: Overfitting and poor\ngeneralization are common problems in contrastive learning\nmethods. Recent findings suggest that dropout serves as a\nform of minimal data augmentation within the contrastive\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n16\nlearning process, and removing it could result in a repre-\nsentation collapse in natural language processing tasks [137].\nA theoretical investigation into the generalization ability of\ncontrastive learning reveals key insights regarding its effec-\ntiveness, highlighting the significance of the alignment of\npositive samples, the divergence of class centers, and the\nconcentration of augmented data [79]. Contrastive learning\nmodels may struggle to generalize to new data distributions\nwith different statistical properties from those of the training\ndata. As a result, the pre-trained model becomes proficient at\nrecognizing samples from the base class but performs poorly\non samples from novel classes due to overfitting [138]. In\norder to address the challenges, a promising research direction\nis to investigate the application of pre-trained contrastive\nlearning models under the few-shot learning setting. Few-shot\nlearning is a learning paradigm that aims to recognize new\nsamples using only a limited number of labeled examples.\nWhile such solutions have been explored extensively in the\nfield of computer vision [139, 140], their applicability in time\nseries data remains an open area for further investigation.\nAlternatively, another research direction is to leverage few-shot\nlearning to pre-train a model using a small number of labeled\nexamples, enabling the model to capture essential information\nfrom limited data. The pre-trained model can then be fine-\ntuned using contrastive learning, with the objective of maxi-\nmizing similarity between positive examples and minimizing\nsimilarity between negative examples. This two-step process\noffers the potential for the model to learn representations that\nare both discriminative and transferable, leading to improved\ngeneralization on unseen data and novel classes.\n5) Robustness Verification:\nAn effective representation\nshould possess the ability to disentangle diverse explanatory\nsources, enabling robustness to withstand complex and in-\ntricately structured variations [113]. The goal of robustness\nverification is to prove that small perturbations on the designed\nmodel do not change the advisories produced for certain\ninputs [141]. Recent research has investigated the feasibility\nof learning provably robust deep neural networks that are\nverifiably guaranteed to be robust to adversarial perturbations\nunder some specified attack models [142]. Although there\nare methods for discovering the adversarial perturbations,\nthe capacity to verify their absence remains limited [141].\nExisting studies demonstrate that the robustness of contrastive\nlearning models can be improved by augmenting the training\nset with adversarial samples [115, 125, 143]. More future\nwork can be explored on measuring the performance under\nadversarial attacks or other relevant factors. There also ex-\nist studies [144] which prove that the representation matrix\nlearned by contrastive learning boosts robustness by preventing\ndeep neural networks from overfitting. The robustness metrics\nused in existing methods such as robust accuracy, however,\ndemonstrate a strong correlation with the attack algorithms,\nimage labels, and downstream tasks in the field of computer vi-\nsion [143]. These factors hence may introduce inconsistencies\nand reduce the reliability of robustness metrics in the context\nof contrastive learning. From another perspective, there are\nunique characteristics such as frequency and amplitude in time\nseries data that are valuable for consideration. Consequently,\nit is crucial to explore the design of robustness verification\nmetrics that are specifically tailored to address the intricacies\nof time series data, ensuring their effectiveness in assessing\nthe robustness of such data.\nREFERENCES\n[1] A. Sgueglia, A. D. Sorbo, C. A. Visaggio, and G. Can-\nfora, “A systematic literature review of iot time series\nanomaly detection solutions,” Future Gener. Comput.\nSyst., vol. 134, pp. 170–186, 2022.\n[2] A. A. Cook, G. Misirli, and Z. Fan, “Anomaly detection\nfor iot time-series data: A survey,” IEEE Internet Things\nJ., vol. 7, no. 7, pp. 6481–6494, 2020.\n[3] Y. Jiao and C. Wang, “A blockchain-based trusted\nupload scheme for the internet of things nodes,” In-\nternational Journal of Crowd Science, vol. 6, no. 2, pp.\n92–97, 2022.\n[4] J. I. Porta, M. A. Dom´ınguez, and F. Tamarit, “Auto-\nmatic data imputation in time series processing using\nneural networks for industry and medical datasets,” in\nSIMBig, ser. Communications in Computer and Infor-\nmation Science, vol. 1577.\nSpringer, 2021, pp. 3–16.\n[5] J. Polge, J. Robert, and Y. L. Traon, “A case driven\nstudy of the use of time series classification for flexi-\nbility in industry 4.0,” Sensors, vol. 20, no. 24, p. 7273,\n2020.\n[6] O. B. Sezer, M. U. Gudelek, and A. M. ¨Ozbayoglu,\n“Financial time series forecasting with deep learning :\nA systematic literature review: 2005-2019,” Appl. Soft\nComput., vol. 90, p. 106181, 2020.\n[7] C. Sun, S. Hong, M. Song, and H. Li, “A review of\ndeep learning methods for irregularly sampled medical\ntime series data,” CoRR, vol. abs/2010.12493, 2020.\n[8] C. Guo, M. Lu, and J. Chen, “An evaluation of time se-\nries summary statistics as features for clinical prediction\ntasks,” BMC Medical Informatics Decis. Mak., vol. 20,\nno. 1, p. 48, 2020.\n[9] P.\nSch¨afer\nand\nU.\nLeser,\n“Multivariate\ntime\nse-\nries classification with WEASEL+MUSE,” CoRR, vol.\nabs/1711.11343, 2017.\n[10] A. J. Bagnall, J. Lines, A. Bostrom, J. Large, and E. J.\nKeogh, “The great time series classification bake off:\na review and experimental evaluation of recent algo-\nrithmic advances,” Data Min. Knowl. Discov., vol. 31,\nno. 3, pp. 606–660, 2017.\n[11] A. P. Ruiz, M. Flynn, J. Large, M. Middlehurst, and\nA. J. Bagnall, “The great multivariate time series clas-\nsification bake off: a review and experimental evaluation\nof recent algorithmic advances,” Data Min. Knowl.\nDiscov., vol. 35, no. 2, pp. 401–449, 2021.\n[12] J. F. Torres, D. Hadjout, A. Sebaa, F. Mart´ınez- ´Alvarez,\nand A. Troncoso, “Deep learning for time series fore-\ncasting: A survey,” Big Data, vol. 9, no. 1, pp. 3–21,\n2021.\n[13] K. M. Ahmad, G. He, W. Yu, X. Xu, J. Kumar, and\nM. A. Saleem, “A survey on semi-parametric machine\nlearning technique for time series forecasting,” CoRR,\nvol. abs/2104.00871, 2021.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n17\n[14] P.\nLara-Ben´ıtez,\nM.\nCarranza-Garc´ıa,\nand\nJ.\nC.\nRiquelme, “An experimental review on deep learning\narchitectures for time series forecasting,” Int. J. Neural\nSyst., vol. 31, no. 3, pp. 2 130 001:1–2 130 001:28, 2021.\n[15] H. Wang, Y. Li, and H. V. Zhao, “Caseload prediction\nusing graphical evolutionary game theory and time se-\nries analysis,” International Journal of Crowd Science,\nvol. 6, no. 3, pp. 142–149, 2022.\n[16] Q. Ma, J. Zheng, S. Li, and G. W. Cottrell, “Learning\nrepresentations for time series clustering,” in NeurIPS,\n2019, pp. 3776–3786.\n[17] S. Rambhatla, Z. Che, and Y. Liu, “I-SEA: importance\nsampling and expected alignment-based deep distance\nmetric learning for time series analysis and embedding,”\nin AAAI.\nAAAI Press, 2022, pp. 8045–8053.\n[18] J. Tang, S. Wu, L. Wei, W. Liu, T. Qin, Z. Zhou,\nand J. Gu, “Energy-efficient sensory data collection\nbased on spatiotemporal correlation in iot networks,”\nInternational Journal of Crowd Science, vol. 6, no. 1,\npp. 34–43, 2022.\n[19] H. Qian, S. J. Pan, and C. Miao, “Weakly-supervised\nsensor-based activity segmentation and recognition via\nlearning from distributions,” Artif. Intell., vol. 292, p.\n103429, 2021.\n[20] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and\nP. Muller, “Deep learning for time series classification:\na review,” Data Min. Knowl. Discov., vol. 33, no. 4, pp.\n917–963, 2019.\n[21] I. Simic, V. Sabol, and E. E. Veas, “XAI methods for\nneural time series classification: A brief review,” CoRR,\nvol. abs/2108.08009, 2021.\n[22] A. Gupta, H. P. Gupta, B. Biswas, and T. Dutta,\n“Approaches and applications of early classification of\ntime series: A review,” IEEE Trans. Artif. Intell., vol. 1,\nno. 1, pp. 47–61, 2020.\n[23] A. Abanda, U. Mori, and J. A. Lozano, “A review on\ndistance based time series classification,” Data Min.\nKnowl. Discov., vol. 33, no. 2, pp. 378–412, 2019.\n[24] G. Chiarot and C. Silvestri, “Time series compression:\na survey,” CoRR, vol. abs/2101.08784, 2021.\n[25] S. Deldari, H. Xue, A. Saeed, J. He, D. V. Smith, and\nF. D. Salim, “Beyond just vision: A review on self-\nsupervised representation learning on multimodal and\ntemporal data,” CoRR, vol. abs/2206.02353, 2022.\n[26] A. Fakhrazari and H. Vakilzadian, “A survey on time\nseries data mining,” in EIT.\nIEEE, 2017, pp. 476–481.\n[27] M. L¨angkvist, L. Karlsson, and A. Loutfi, “A review\nof unsupervised feature learning and deep learning for\ntime-series modeling,” Pattern Recognit. Lett., vol. 42,\npp. 11–24, 2014.\n[28] Y. Li, Z. Chen, D. Zha, M. Du, J. Ni, D. Zhang,\nH. Chen, and X. Hu, “Towards learning disentangled\nrepresentations for time series,” in KDD.\nACM, 2022,\npp. 3270–3278.\n[29] X. Zhan, J. Xie, Z. Liu, Y. Ong, and C. C. Loy,\n“Online deep clustering for unsupervised representation\nlearning,” in CVPR.\nComputer Vision Foundation /\nIEEE, 2020, pp. 6687–6696.\n[30] L. Ge, Z. Yang, and W. Ji, “Crowd evolution method\nbased on intelligence level clustering,” International\nJournal of Crowd Science, vol. 5, no. 2, pp. 204–215,\n2021.\n[31] P. Vadiraja and M. A. Chattha, “A survey on knowl-\nedge integration techniques with artificial neural net-\nworks for seq-2-seq/time series models,” CoRR, vol.\nabs/2008.05972, 2020.\n[32] P. Malhotra, V. TV, L. Vig, P. Agarwal, and G. Shroff,\n“Timenet: Pre-trained deep recurrent neural network for\ntime series classification,” in ESANN, 2017.\n[33] S. Tonekaboni, D. Eytan, and A. Goldenberg, “Unsuper-\nvised representation learning for time series with tempo-\nral neighborhood coding,” in ICLR.\nOpenReview.net,\n2021.\n[34] P. Sarkar and A. Etemad, “Self-supervised ECG repre-\nsentation learning for emotion recognition,” CoRR, vol.\nabs/2002.03898, 2020.\n[35] Y. Xie, Z. Xu, Z. Wang, and S. Ji, “Self-supervised\nlearning of graph neural networks: A unified review,”\nCoRR, vol. abs/2102.10757, 2021.\n[36] S. Albelwi, “Survey on self-supervised learning: Aux-\niliary pretext tasks and contrastive learning methods in\nimaging,” Entropy, vol. 24, no. 4, p. 551, 2022.\n[37] A. Khan, S. AlBarri, and M. A. Manzoor, “Contrastive\nself-supervised learning: A survey on different architec-\ntures,” in ICAI.\nIEEE, 2022, pp. 1–6.\n[38] H. Qian, T. Tian, and C. Miao, “What makes good con-\ntrastive learning on small-scale wearable-based tasks?”\nin KDD.\nACM, 2022, pp. 3761–3771.\n[39] I. Misra and L. van der Maaten, “Self-supervised\nlearning of pretext-invariant representations,” in CVPR.\nComputer Vision Foundation / IEEE, 2020, pp. 6706–\n6716.\n[40] A. Kolesnikov, X. Zhai, and L. Beyer, “Revisiting self-\nsupervised visual representation learning,” in CVPR.\nComputer Vision Foundation / IEEE, 2019, pp. 1920–\n1929.\n[41] Q. Xie, M. Luong, E. H. Hovy, and Q. V. Le, “Self-\ntraining with noisy student improves imagenet classifi-\ncation,” in CVPR. Computer Vision Foundation / IEEE,\n2020, pp. 10 684–10 695.\n[42] T. Huynh, S. Kornblith, M. R. Walter, M. Maire,\nand M. Khademi, “Boosting contrastive self-supervised\nlearning with false negative cancellation,” in WACV.\nIEEE, 2022, pp. 986–996.\n[43] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton,\n“A simple framework for contrastive learning of visual\nrepresentations,” in ICML, ser. Proceedings of Machine\nLearning Research, vol. 119.\nPMLR, 2020, pp. 1597–\n1607.\n[44] J. Grill, F. Strub, F. Altch´e, C. Tallec, P. H. Richemond,\nE. Buchatskaya, C. Doersch, B. ´A. Pires, Z. Guo,\nM. G. Azar, B. Piot, K. Kavukcuoglu, R. Munos, and\nM. Valko, “Bootstrap your own latent - A new approach\nto self-supervised learning,” in NeurIPS, 2020.\n[45] K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick, “Mo-\nmentum contrast for unsupervised visual representation\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n18\nlearning,” in CVPR.\nComputer Vision Foundation /\nIEEE, 2020, pp. 9726–9735.\n[46] P. Chen, D. Huang, D. He, X. Long, R. Zeng, S. Wen,\nM. Tan, and C. Gan, “Rspnet: Relative speed percep-\ntion for unsupervised video representation learning,” in\nAAAI.\nAAAI Press, 2021, pp. 1045–1053.\n[47] I. R. Dave, R. Gupta, M. N. Rizve, and M. Shah,\n“TCLR: temporal contrastive learning for video repre-\nsentation,” Comput. Vis. Image Underst., vol. 219, p.\n103406, 2022.\n[48] S. Ma, Z. Zeng, D. McDuff, and Y. Song, “Active con-\ntrastive learning of audio-visual video representations,”\nin ICLR.\nOpenReview.net, 2021.\n[49] R. Qian, T. Meng, B. Gong, M. Yang, H. Wang, S. J.\nBelongie, and Y. Cui, “Spatiotemporal contrastive video\nrepresentation learning,” in CVPR.\nComputer Vision\nFoundation / IEEE, 2021, pp. 6964–6974.\n[50] J. Franceschi, A. Dieuleveut, and M. Jaggi, “Unsuper-\nvised scalable representation learning for multivariate\ntime series,” in NeurIPS, 2019, pp. 4652–4663.\n[51] E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh,\nX. Li, and C. Guan, “Time-series representation learn-\ning via temporal and contextual contrasting,” in IJCAI.\nijcai.org, 2021, pp. 2352–2359.\n[52] Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong,\nand B. Xu, “Ts2vec: Towards universal representation\nof time series,” in AAAI. AAAI Press, 2022, pp. 8980–\n8987.\n[53] H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long,\n“Timesnet: Temporal 2d-variation modeling for general\ntime series analysis,” in ICLR.\nOpenReview.net, 2023.\n[54] T. Wang and P. Isola, “Understanding contrastive repre-\nsentation learning through alignment and uniformity on\nthe hypersphere,” in ICML, ser. Proceedings of Machine\nLearning Research, vol. 119.\nPMLR, 2020, pp. 9929–\n9939.\n[55] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and\nG. E. Hinton, “Big self-supervised models are strong\nsemi-supervised learners,” in NeurIPS, 2020.\n[56] A. van den Oord, Y. Li, and O. Vinyals, “Representation\nlearning with contrastive predictive coding,” CoRR, vol.\nabs/1807.03748, 2018.\n[57] X. Chen and K. He, “Exploring simple siamese rep-\nresentation learning,” in CVPR.\nComputer Vision\nFoundation / IEEE, 2021, pp. 15 750–15 758.\n[58] J. Li, P. Zhou, C. Xiong, and S. C. H. Hoi, “Prototypical\ncontrastive learning of unsupervised representations,” in\nICLR.\nOpenReview.net, 2021.\n[59] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski,\nand A. Joulin, “Unsupervised learning of visual features\nby contrasting cluster assignments,” in NeurIPS, 2020.\n[60] Q. Meng, H. Qian, Y. Liu, Y. Xu, Z. Shen, and\nL. Cui, “MHCCL: masked hierarchical cluster-wise\ncontrastive learning for multivariate time series,” CoRR,\nvol. abs/2212.01141, 2022.\n[61] A. Hyv¨arinen and H. Morioka, “Unsupervised feature\nextraction by time-contrastive learning and nonlinear\nICA,” in NIPS, 2016, pp. 3765–3773.\n[62] Y. Wang, Q. Zhang, Y. Wang, J. Yang, and Z. Lin,\n“Chaos is a ladder: A new theoretical understanding of\ncontrastive learning via augmentation overlap,” in ICLR.\nOpenReview.net, 2022.\n[63] Y. Tao, K. Takagi, and K. Nakata, “Clustering-friendly\nrepresentation learning via instance discrimination and\nfeature decorrelation,” in ICLR. OpenReview.net, 2021.\n[64] V. Sharma, M. Tapaswi, M. S. Sarfraz, and R. Stiefel-\nhagen, “Clustering based contrastive learning for im-\nproving face representations,” in FG.\nIEEE, 2020, pp.\n109–116.\n[65] O. L. Hasna and R. Potolea, “Time series - A taxonomy\nbased survey,” in ICCP.\nIEEE, 2017, pp. 231–238.\n[66] T. W. Liao, “Clustering of time series data - a survey,”\nPattern Recognit., vol. 38, no. 11, pp. 1857–1874, 2005.\n[67] M. Caron, P. Bojanowski, A. Joulin, and M. Douze,\n“Deep clustering for unsupervised learning of visual\nfeatures,” in ECCV (14), ser. Lecture Notes in Computer\nScience, vol. 11218.\nSpringer, 2018, pp. 139–156.\n[68] W. Song, L. Liu, M. Liu, W. Wang, X. Wang, and\nY. Song, “Representation learning with deconvolution\nfor multivariate time series classification and visualiza-\ntion,” in ICPCSEE (1), ser. Communications in Com-\nputer and Information Science, vol. 1257.\nSpringer,\n2020, pp. 310–326.\n[69] J. Yoon, D. Jarrett, and M. van der Schaar, “Time-series\ngenerative adversarial networks,” in NeurIPS, 2019, pp.\n5509–5519.\n[70] A. Desai, C. Freeman, Z. Wang, and I. Beaver,\n“Timevae: A variational auto-encoder for multivariate\ntime series generation,” CoRR, vol. abs/2111.08095,\n2021.\n[71] H. J. Banville, G. Moffat, I. Albuquerque, D. Enge-\nmann, A. Hyv¨arinen, and A. Gramfort, “Self-supervised\nrepresentation learning from electroencephalography\nsignals,” in MLSP.\nIEEE, 2019, pp. 1–6.\n[72] M. S. Sarfraz, V. Sharma, and R. Stiefelhagen, “Ef-\nficient parameter-free clustering using first neighbor\nrelations,” in CVPR.\nComputer Vision Foundation /\nIEEE, 2019, pp. 8934–8943.\n[73] S. Shin, K. Song, and I. Moon, “Hierarchically clustered\nrepresentation learning,” in AAAI.\nAAAI Press, 2020,\npp. 5776–5783.\n[74] C. Zhang, H. Fu, J. Wang, W. Li, X. Cao, and Q. Hu,\n“Tensorized multi-view subspace representation learn-\ning,” Int. J. Comput. Vis., vol. 128, no. 8, pp. 2344–\n2361, 2020.\n[75] S. Zhou, H. Xu, Z. Zheng, J. Chen, Z. Li, J. Bu, J. Wu,\nX. Wang, W. Zhu, and M. Ester, “A comprehensive\nsurvey on deep clustering: Taxonomy, challenges, and\nfuture directions,” CoRR, vol. abs/2206.07579, 2022.\n[76] Q. Ma, C. Chen, S. Li, and G. W. Cottrell, “Learning\nrepresentations for incomplete time series clustering,”\nin AAAI.\nAAAI Press, 2021, pp. 8837–8846.\n[77] Q. Ma, J. Zheng, S. Li, and G. W. Cottrell, “Learning\nrepresentations for time series clustering,” in NeurIPS,\n2019, pp. 3776–3786.\n[78] J. Xie, R. B. Girshick, and A. Farhadi, “Unsupervised\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n19\ndeep embedding for clustering analysis,” in ICML, ser.\nJMLR Workshop and Conference Proceedings, vol. 48.\nJMLR.org, 2016, pp. 478–487.\n[79] W. Huang, M. Yi, X. Zhao, and Z. Jiang, “Towards the\ngeneralization of contrastive self-supervised learning,”\nin ICLR.\nOpenReview.net, 2023.\n[80] D. Zhang, F. Nan, X. Wei, S. Li, H. Zhu, K. R.\nMcKeown, R. Nallapati, A. O. Arnold, and B. Xiang,\n“Supporting clustering with contrastive learning,” in\nNAACL-HLT.\nAssociation for Computational Linguis-\ntics, 2021, pp. 5419–5430.\n[81] Y. Li, P. Hu, J. Z. Liu, D. Peng, J. T. Zhou, and X. Peng,\n“Contrastive clustering,” in AAAI.\nAAAI Press, 2021,\npp. 8547–8555.\n[82] S. H. Khorasgani, Y. Chen, and F. Shkurti, “SLIC: self-\nsupervised learning with iterative clustering for human\naction videos,” in CVPR.\nIEEE, 2022, pp. 16 070–\n16 080.\n[83] G. E. Hinton and R. R. Salakhutdinov, “Reducing the\ndimensionality of data with neural networks,” science,\nvol. 313, no. 5786, pp. 504–507, 2006.\n[84] W. Yu, I. Y. Kim, and C. Mechefske, “Analysis of\ndifferent rnn autoencoder variants for time series classi-\nfication and machine prognostics,” Mechanical Systems\nand Signal Processing, vol. 149, p. 107322, 2021.\n[85] X. Lyu, M. H¨user, S. L. Hyland, G. Zerveas, and\nG. R¨atsch, “Improving clinical predictions through un-\nsupervised time series representation learning,” CoRR,\nvol. abs/1812.00490, 2018.\n[86] A. Xiao, J. Huang, D. Guan, and S. Lu, “Unsupervised\nrepresentation learning for point clouds: A survey,”\nCoRR, vol. abs/2202.13589, 2022.\n[87] X. Wu, C. Huang, P. Robles-Granda, and N. V. Chawla,\n“Representation learning on variable length and in-\ncomplete wearable-sensory time series,” CoRR, vol.\nabs/2002.03595, 2020.\n[88] W. Zhang, L. Yang, S. Geng, and S. Hong, “Cross re-\nconstruction transformer for self-supervised time series\nrepresentation learning,” CoRR, vol. abs/2205.09928,\n2022.\n[89] R. Zhang, F. Cheng, and A. Pandey, “Representation\nlearning using a multi-branch transformer for industrial\ntime series anomaly detection,” in KDD 2022 Workshop\non Mining and Learning from Time Series – Deep\nForecasting: Models, Interpretability, and Applications,\n2022.\n[90] J. Dong, H. Wu, H. Zhang, L. Zhang, J. Wang, and\nM. Long, “Simmtm: A simple pre-training frame-\nwork for masked time-series modeling,” CoRR, vol.\nabs/2302.00861, 2023.\n[91] H. Zhou, C. Lu, S. Yang, X. Han, and Y. Yu, “Preserva-\ntional learning improves self-supervised medical image\nmodels by reconstructing diverse contexts,” in ICCV.\nIEEE, 2021, pp. 3479–3489.\n[92] M. Mehralian and B. Karasfi, “Rdcgan: Unsupervised\nrepresentation learning with regularized deep convo-\nlutional generative adversarial networks,” in 2018 9th\nConference on Artificial Intelligence and Robotics and\n2nd Asia-Pacific International Symposium, 2018, pp.\n31–38.\n[93] E. Brophy, Z. Wang, Q. She, and T. Ward, “Genera-\ntive adversarial networks in time series: A survey and\ntaxonomy,” CoRR, vol. abs/2107.11098, 2021.\n[94] J. Donahue and K. Simonyan, “Large scale adversarial\nrepresentation learning,” in NeurIPS, 2019, pp. 10 541–\n10 551.\n[95] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran,\nB. Sengupta, and A. A. Bharath, “Generative adversarial\nnetworks: An overview,” IEEE Signal Process. Mag.,\nvol. 35, no. 1, pp. 53–65, 2018.\n[96] C. Esteban, S. L. Hyland, and G. R¨atsch, “Real-valued\n(medical) time series generation with recurrent condi-\ntional gans,” CoRR, vol. abs/1706.02633, 2017.\n[97] D. Li, D. Chen, B. Jin, L. Shi, J. Goh, and S. Ng,\n“MAD-GAN: multivariate anomaly detection for time\nseries data with generative adversarial networks,” in\nICANN (4), ser. Lecture Notes in Computer Science,\nvol. 11730.\nSpringer, 2019, pp. 703–716.\n[98] K. E. Smith and A. O. Smith, “Conditional GAN\nfor timeseries generation,” CoRR, vol. abs/2006.16477,\n2020.\n[99] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein\nGAN,” CoRR, vol. abs/1701.07875, 2017.\n[100] M. Mirza and S. Osindero, “Conditional generative\nadversarial nets,” CoRR, vol. abs/1411.1784, 2014.\n[101] W. Lu, J. Wang, X. Sun, Y. Chen, and X. Xie, “Out-\nof-distribution representation learning for time series\nclassification,” in ICLR.\nOpenReview.net, 2023.\n[102] S. Schneider, A. Baevski, R. Collobert, and M. Auli,\n“wav2vec: Unsupervised pre-training for speech recog-\nnition,” in INTERSPEECH.\nISCA, 2019, pp. 3465–\n3469.\n[103] Y. Chen, X. Zhou, Z. Xing, Z. Liu, and M. Xu, “Cass:\nA channel-aware self-supervised representation learning\nframework for multivariate time series classification,” in\nDASFAA (2), ser. Lecture Notes in Computer Science,\nvol. 13246.\nSpringer, 2022, pp. 375–390.\n[104] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and\nC. Eickhoff, “A transformer-based framework for mul-\ntivariate time series representation learning,” in KDD.\nACM, 2021, pp. 2114–2124.\n[105] X. Yang, Z. Zhang, and R. Cui, “Timeclr: A self-\nsupervised contrastive learning framework for univariate\ntime series representation,” Knowl. Based Syst., vol.\n245, p. 108606, 2022.\n[106] K. Wickstrøm, M. Kampffmeyer, K. Ø. Mikalsen,\nand R. Jenssen, “Mixing up contrastive learning: Self-\nsupervised representation learning for time series,” Pat-\ntern Recognit. Lett., vol. 155, pp. 54–61, 2022.\n[107] A. Mohamed, H. Lee, L. Borgholt, J. D. Havtorn,\nJ. Edin, C. Igel, K. Kirchhoff, S. Li, K. Livescu,\nL. Maaløe, T. N. Sainath, and S. Watanabe, “Self-\nsupervised speech representation learning: A review,”\nCoRR, vol. abs/2205.10643, 2022.\n[108] Y. Zhu, Y. Xu, Q. Liu, and S. Wu, “An empirical study\nof graph contrastive learning,” in NeurIPS Datasets and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n20\nBenchmarks, 2021.\n[109] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised\nfeature learning via non-parametric instance-level dis-\ncrimination,” CoRR, vol. abs/1805.01978, 2018.\n[110] A. Pokle, J. Tian, Y. Li, and A. Risteski, “Contrasting\nthe landscape of contrastive and non-contrastive learn-\ning,” in AISTATS, ser. Proceedings of Machine Learning\nResearch, vol. 151.\nPMLR, 2022, pp. 8592–8618.\n[111] Y. Tian, X. Chen, and S. Ganguli, “Understanding\nself-supervised learning dynamics without contrastive\npairs,” in ICML, ser. Proceedings of Machine Learning\nResearch, vol. 139.\nPMLR, 2021, pp. 10 268–10 278.\n[112] L. Yang and S. Hong, “Unsupervised time-series rep-\nresentation learning with iterative bilinear temporal-\nspectral fusion,” in ICML, ser. Proceedings of Machine\nLearning Research, vol. 162. PMLR, 2022, pp. 25 038–\n25 054.\n[113] G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. C. H. Hoi,\n“Cost: Contrastive learning of disentangled seasonal-\ntrend representations for time series forecasting,” in\nICLR.\nOpenReview.net, 2022.\n[114] R.\nS.\nZimmermann,\nY.\nSharma,\nS.\nSchneider,\nM. Bethge, and W. Brendel, “Contrastive learning\ninverts the data generating process,” in ICML, ser.\nProceedings of Machine Learning Research, vol. 139.\nPMLR, 2021, pp. 12 979–12 990.\n[115] X. Zhao, T. Du, Y. Wang, J. Yao, and W. Huang, “Arcl:\nEnhancing contrastive learning with augmentation-\nrobust representations,” in ICLR.\nOpenReview.net,\n2023.\n[116] H. Ling, Z. Jiang, Y. Luo, S. Ji, and N. Zou, “Learning\nfair graph representations via automated data augmen-\ntations,” in ICLR.\nOpenReview.net, 2023.\n[117] Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang,\nand H. Xu, “Time series data augmentation for deep\nlearning: A survey,” in IJCAI.\nijcai.org, 2021, pp.\n4653–4660.\n[118] B. K. Iwana and S. Uchida, “An empirical survey of data\naugmentation for time series classification with neural\nnetworks,” CoRR, vol. abs/2007.15951, 2020.\n[119] M. T. Nonnenmacher, L. Oldenburg, I. Steinwart, and\nD. Reeb, “Utilizing expert features for contrastive\nlearning of time-series representations,” in ICML, ser.\nProceedings of Machine Learning Research, vol. 162.\nPMLR, 2022, pp. 16 969–16 989.\n[120] V. Verma, T. Luong, K. Kawaguchi, H. Pham, and Q. V.\nLe, “Towards domain-agnostic contrastive learning,” in\nICML, ser. Proceedings of Machine Learning Research,\nvol. 139.\nPMLR, 2021, pp. 10 530–10 541.\n[121] X. Wang and G. Qi, “Contrastive learning with stronger\naugmentations,” IEEE Trans. Pattern Anal. Mach. In-\ntell., vol. 45, no. 5, pp. 5549–5560, 2023.\n[122] T. Xiao, X. Wang, A. A. Efros, and T. Darrell, “What\nshould not be contrastive in contrastive learning,” in\nICLR.\nOpenReview.net, 2021.\n[123] E. D. Cubuk, B. Zoph, D. Man´e, V. Vasudevan, and\nQ. V. Le, “Autoaugment: Learning augmentation strate-\ngies from data,” in CVPR. Computer Vision Foundation\n/ IEEE, 2019, pp. 113–123.\n[124] S. Lim, I. Kim, T. Kim, C. Kim, and S. Kim, “Fast\nautoaugment,” in NeurIPS, 2019, pp. 6662–6672.\n[125] C. Chuang, R. D. Hjelm, X. Wang, V. Vineet, N. Joshi,\nA. Torralba, S. Jegelka, and Y. Song, “Robust con-\ntrastive learning against noisy views,” in CVPR. IEEE,\n2022, pp. 16 649–16 660.\n[126] P. Morgado, I. Misra, and N. Vasconcelos, “Robust\naudio-visual instance discrimination,” in CVPR.\nCom-\nputer Vision Foundation / IEEE, 2021, pp. 12 934–\n12 945.\n[127] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and\nP. Isola, “What makes for good views for contrastive\nlearning?” in NeurIPS, 2020.\n[128] H. Wang, X. Guo, Z. Deng, and Y. Lu, “Rethinking min-\nimal sufficient representation in contrastive learning,” in\nCVPR.\nIEEE, 2022, pp. 16 020–16 029.\n[129] M. Bosnjak, P. H. Richemond, N. Tomasev, F. Strub,\nJ. C. Walker, F. Hill, L. H. Buesing, R. Pascanu,\nC. Blundell, and J. Mitrovic, “Semppl: Predicting\npseudo-labels for better contrastive representations,” in\nICLR.\nOpenReview.net, 2023.\n[130] C. Ge, J. Wang, Z. Tong, S. Chen, Y. Song, and P. Luo,\n“Soft neighbors are positive supporters in contrastive\nvisual representation learning,” in ICLR.\nOpenRe-\nview.net, 2023.\n[131] Z. Zhong, E. Fini, S. Roy, Z. Luo, E. Ricci, and N. Sebe,\n“Neighborhood contrastive learning for novel class dis-\ncovery,” in CVPR.\nComputer Vision Foundation /\nIEEE, 2021, pp. 10 867–10 875.\n[132] D. Dwibedi, Y. Aytar, J. Tompson, P. Sermanet, and\nA. Zisserman, “With a little help from my friends:\nNearest-neighbor contrastive learning of visual repre-\nsentations,” in ICCV.\nIEEE, 2021, pp. 9568–9577.\n[133] J. T. Ash, S. Goel, A. Krishnamurthy, and D. Misra,\n“Investigating the role of negatives in contrastive rep-\nresentation learning,” in AISTATS, ser. Proceedings of\nMachine Learning Research, vol. 151.\nPMLR, 2022,\npp. 7187–7209.\n[134] P. Awasthi, N. Dikkala, and P. Kamath, “Do more nega-\ntive samples necessarily hurt in contrastive learning?” in\nICML, ser. Proceedings of Machine Learning Research,\nvol. 162.\nPMLR, 2022, pp. 1101–1116.\n[135] A. D. Nguyen, T. H. Tran, H. H. Pham, P. L. Nguyen,\nand L. M. Nguyen, “Learning robust and consistent\ntime series representations: A dilated inception-based\napproach,” CoRR, vol. abs/2306.06579, 2023.\n[136] Z. Shi, J. Chen, K. Li, J. Raghuram, X. Wu, Y. Liang,\nand S. Jha, “The trade-off between universality and la-\nbel efficiency of representations from contrastive learn-\ning,” in ICLR.\nOpenReview.net, 2023.\n[137] T. Gao, X. Yao, and D. Chen, “Simcse: Simple con-\ntrastive learning of sentence embeddings,” in EMNLP\n(1).\nAssociation for Computational Linguistics, 2021,\npp. 6894–6910.\n[138] Z. Yang, J. Wang, and Y. Zhu, “Few-shot classification\nwith contrastive learning,” in ECCV (20), ser. Lecture\nNotes in Computer Science, vol. 13680.\nSpringer,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n21\n2022, pp. 293–309.\n[139] B. Sun, B. Li, S. Cai, Y. Yuan, and C. Zhang, “FSCE:\nfew-shot object detection via contrastive proposal en-\ncoding,” in CVPR.\nComputer Vision Foundation /\nIEEE, 2021, pp. 7352–7362.\n[140] C. Liu, Y. Fu, C. Xu, S. Yang, J. Li, C. Wang, and\nL. Zhang, “Learning a few-shot embedding model with\ncontrastive learning,” in AAAI.\nAAAI Press, 2021, pp.\n8635–8643.\n[141] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J.\nKochenderfer, “Reluplex: An efficient SMT solver for\nverifying deep neural networks,” in CAV (1), ser. Lec-\nture Notes in Computer Science, vol. 10426.\nSpringer,\n2017, pp. 97–117.\n[142] E. Wong, F. R. Schmidt, J. H. Metzen, and J. Z. Kolter,\n“Scaling provable adversarial defenses,” in NeurIPS,\n2018, pp. 8410–8419.\n[143] Z. Wang and W. Liu, “Robustness verification for con-\ntrastive learning,” in ICML, ser. Proceedings of Machine\nLearning Research, vol. 162. PMLR, 2022, pp. 22 865–\n22 883.\n[144] Y. Xue, K. Whitecross, and B. Mirzasoleiman, “In-\nvestigating why contrastive learning benefits robustness\nagainst label noise,” in ICML, ser. Proceedings of Ma-\nchine Learning Research, vol. 162.\nPMLR, 2022, pp.\n24 851–24 871.\n[145] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image recognition,” in CVPR.\nIEEE\nComputer Society, 2016, pp. 770–778.\n[146] H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F.\nSchmidt, J. Weber, G. I. Webb, L. Idoumghar, P. Muller,\nand F. Petitjean, “Inceptiontime: Finding alexnet for\ntime series classification,” Data Min. Knowl. Discov.,\nvol. 34, no. 6, pp. 1936–1962, 2020.\n[147] E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le, “Ran-\ndaugment: Practical automated data augmentation with\na reduced search space,” in NeurIPS, 2020.\n[148] E. Fons, P. Dawson, X. Zeng, J. A. Keane, and A. Iosi-\nfidis, “Adaptive weighting scheme for automatic time-\nseries data augmentation,” CoRR, vol. abs/2102.08310,\n2021.\nQianwen Meng is currently a Ph.D student in\nSchool of Software, Shandong University, China,\nand Joint SDU-NTU Centre for Artificial Intelli-\ngence Research, Shandong University, China. She\nreceived the Master’s degree in School of Software,\nShandong University, China in 2020, and the Bach-\nelor’s degree in School of Computer Science and\nTechnology, Shandong University, China in 2017.\nHer research interests mainly lie in representation\nlearning, unsupervised learning, time series model-\ning, and disease risk prediction.\nHangwei Qian is a research scientist at Centre for\nFrontier AI Research (CFAR), A*STAR, Singapore.\nShe was previously a Wallenberg-NTU Presidential\nPostdoctoral Fellow from 2020 to 2022. She ob-\ntained her Ph.D. in School of Computer Science and\nEngineering at NTU, Singapore in 2020 and B.E.\nfrom University of Science and Technology of China\n(USTC) in 2015. Her research interests include unsu-\npervised learning, transfer learning, kernel methods,\nand wearable-based studies. She has published top-\ntier academic articles in KDD, AAAI, IJCAI, AIJ\nand FAccT.\nYong Liu is a Senior Principal Researcher at Huawei\nNoah’s Ark Lab. Prior to joining Huawei, he was\na Senior Research Scientist at Nanyang Technolog-\nical University (NTU), a Data Scientist at NTUC\nEnterprise, and a Research Scientist at Institute for\nInfocomm Research (I2R), A*STAR, Singapore. He\nreceived his Ph.D. degree in Computer Engineering\nfrom NTU in 2016 and B.S. degree in Electronic\nScience and Technology from University of Science\nand Technology of China (USTC) in 2008. His\nresearch interests include recommendation systems,\nnatural language processing, and knowledge graph. He has been invited as a\nPC member of major conferences such as KDD, SIGIR, ACL, IJCAI, AAAI,\nand reviewer for IEEE/ACM transactions.\nYonghui Xu is a professor at Joint SDU-NTU\nCentre for Artificial Intelligence Research (C-FAIR),\nShandong University. Before that, he was a research\nfellow in the Joint NTU-UBC Research Centre of\nExcellence in Active Living for the Elderly (LILY),\nNanyang Technological University, Singapore. He\nreceived his Ph.D. from the School of Computer Sci-\nence and Engineering at South China University of\nTechnology in 2017 and BS from the Department of\nMathematics and Information Science Engineering\nat Henan University of China in 2011. His research\nareas include various topics in Trustworthy AI, knowledge graphs, expert\nsystems and their applications in e-commerce and healthcare.\nZhiqi Shen received the B.Sc. degree in computer\nscience and technology from Peking University, Bei-\njing, China, the M.Eng. degree in computer en-\ngineering from the Beijing University of Technol-\nogy, Beijing, and the Ph.D. degree from Nanyang\nTechnological University, Singapore. He is a Se-\nnior Lecturer and a Senior Research Scientist with\nthe School of Computer Science and Engineering,\nNanyang Technological University. His current re-\nsearch interests include multiagent systems, goal-\noriented modeling, agent augmented interactive me-\ndia, and interactive storytelling.\nLizhen Cui (IET Fellow, IEEE Senior Member)\nis a professor and doctoral supervisor at School\nof Software, Shandong University. He is the Co-\nDirector of Joint SDU-NTU Centre for Artificial\nIntelligence Research (C-FAIR) and Research Cen-\nter of Software and Data Engineering, Shandong\nUniversity. He received his B.Sc., M.Sc., and Ph.D.\ndegrees from Shandong University, Jinan, China, in\n1999, 2002 and 2005, respectively. He has published\nover 200 high-level academic articles in TKDE,\nTPAMI, TPDS, TSC, TCC, TNNLS, AAAI, SIGIR,\nCIKM, BIBM, ICDCS, DASFAA, ICWS, ICSOC, AAMAS, and MICCAI.\nHe regularly serves as PC for prestigious conferences (i.e., KDD, IJCAI,\nand CIKM) and reviewers for IEEE/ACM Transactions Journals. His research\ninterests include big data analysis, data mining, and crowd science.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n22\nSupplementary Materials\nAPPENDIX\nEMPIRICAL EVALUATIONS AND ANALYSIS\nWe reorganize the publicly available benchmark multivariate\ntime series datasets, and select 9 commonly used datasets for\nevaluation. Then, we introduce the experimental setup and\nevaluation metrics used to assess the performance. Finally, we\npresent a comprehensive comparison of the selected state-of-\nthe-art models, with a particular focus on contrasting methods.\nA. Datasets\nIn the field of multivariate time series analysis, there are\nmainly from 3 archives of datasets, including UCI2, UEA3\nand MTS4. Particularly, it is important to note that there may\nbe some overlap and discrepancies within these datasets. In\nthis work, we carefully reorganize the multivariate time series\ndatasets considering multiple aspects, including application\ndomains, sequence lengths, number of samples, variables, and\nclasses, to ensure our evaluation cover datasets with diverse\ncharacteristics. Our reorganization allows for a more system-\natic and comprehensive analysis of the datasets in relation to\ntheir specific characteristics.\n• UCI archive: The UCI archive contains 85 multivariate\ntime series datasets specifically curated for classification\ntasks. These datasets encompass a wide range of applica-\ntion fields, such as audio spectra classification, business,\nECG/EEG classification, human activity recognition, gas\ndetection, motion classification, etc. More details are\nlisted in Table VIII.\n• UEA archive: The UEA archive collects 30 multivariate\ntime series classification datasets that span various ap-\nplication domains, including audio spectra classification,\nECG/EEG/MEG classification, human activity recogni-\ntion, motion classification, etc. More details are listed in\nTable IX.\n• MTS archive: The MTS archive, also known as Bay-\ndogan’s archive, consists of 13 multivariate time series\ndatasets covering applications of audio spectra classifica-\ntion, ECG classification, human activity recognition, mo-\ntion classification, etc. More details are listed in Table X.\nAdditionally, we also prioritize datasets that have been\nwidely adopted in existing research works. Considering the\nfact that MTS has lower usage and has significant overlap with\nUEA and UCI, we ultimately select 9 representative datasets\nprimarily from the UCI and UEA archives for evaluation.\nThese datasets include PhonemeSpectra (PS), DuckDuckGeese\n(DDG), EigenWorms (EW), PenDigits (PD), Epileptic Seizure\nRecognition (Epilepsy), FingerMovements (FM), Human Ac-\ntivity Recognition Using Smartphones (HAR), Smartphone\nand Smartwatch Activity and Biometrics Dataset (WISDM)\nand UniMiB SHAR (SHAR). They encompass various appli-\ncation fields, such as digits, images, and biology. For datasets\n2http://archive.ics.uci.edu/ml/datasets.php\n3http://www.timeseriesclassification.com/dataset.php\n4http://www.mustafabaydogan.com/multivariate-time-series-discretization-\nfor-classification.html\nfrom UEA, we utilize the pre-defined train-test split. For the\nremaining datasets from the UCI archive, we partition the data\ninto 80% for training and 20% for testing. Detailed informa-\ntion about the selected datasets can be found in Table VII.\nB. Experimental Setup\nWe conduct linear evaluation to assess the performance of\nstate-of-the-art unsupervised learning models, with a particular\nfocus on various contrastive methods, on 9 representative\ndatasets mentioned in Section A. All models are implemented\nusing the PyTorch framework, and the experimental evalua-\ntions are conducted on an NVIDIA GeForce RTX 3090 GPU\nfor efficient computation. To ensure a fair comparison, we\nfollow the standard linear benchmarking evaluation scheme\nemployed in [43, 51, 56]. This involves freezing the represen-\ntations pre-trained by each unsupervised model and attaching\na linear classifier on top of the frozen representations. The\nlinear classifier is a logistic regression classifier, which consists\nof a fully-connected layer followed by the softmax activation\nfunction. We disable gradient computation on the inputs to the\nlinear classifier. The classification performance is measured\nby using the following 3 commonly used metrics: ACC\n(Accuracy), MF1 (macro-averaged F1 score) and κ (Cohen’s\nKappa coefficient). All the models are trained for 200 epochs.\nThe mean and standard deviation of empirical results obtained\nfrom 5 repeated experiments are reported for each metric.\n• ACC: Accuracy is the ratio of the number of correctly\nclassified samples to the total number of samples, i.e.,\nACC =\nTP+TN\nTP+TN+FP+FN, where TP, TN, FP, FN are true\npositive, true negative, false positive and false negative.\n• MF1: Macro-averaged F1 score is calculated as the arith-\nmetic mean of individual classes’ F1 score, i.e., MF1\n= 2×PR\nP+R , where Precision P and Recall R are calculated\nby P =\nTP\nTP+FP, R =\nTP\nTP+FN.\n• κ: Cohen’s Kappa coefficient measures the inter-annotator\nagreement that expresses the level of agreement between\npredicted and true labels on classification, i.e., κ =\nACC−pe\n1−pe , where pe =\n[(TP+FN)∗(TP+FP)+(FP+TN)∗(FN+TN)]\nN2\nis the hypothetical probability of chance agreement, and\nN denotes the total number of samples.\nTask\nArchive\nDataset\nN\nV\nClasses\nLength\nASC\nUEA\nPS\n6,668\n11\n39\n217\nUEA\nDDG\n100\n1,345\n5\n270\nMC\nUEA\nEW\n259\n6\n5\n17,984\nUCI/UEA\nPD\n10,992\n2\n10\n8\nEC\nUCI\nEpilepsy\n12,500\n1\n2\n178\nUEA\nFM\n416\n28\n2\n50\nHAR\nUCI\nHAR\n10,299\n9\n6\n128\nUCI\nWISDM\n4,091\n3\n6\n200\nOthers\nSHAR\n11,771\n3\n17\n151\nTABLE VII\nSTATISTICAL INFORMATION OF THE SELECTED TIME SERIES\nCLASSIFICATION DATASETS FOR EVALUATION. ASC REFERS TO AUDIO\nSPECTRA CLASSIFICATION, MC REFERS TO MOTION CLASSIFICATION,\nEC REFERS TO ECG/EEG/EMG/MEG CLASSIFICATION, AND HAR\nREFERS TO HUMAN ACTIVITY RECOGNITION. N DENOTES THE NUMBER\nOF SAMPLES AND V DENOTES THE NUMBER OF VARIABLES. CLASSES\nREFER TO THE NUMBER OF CLASSES IN EACH DATASET AND LENGTH\nREFERS TO THE LENGTH OF EACH TIME SERIES.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n23\nDataset\nApplication\nN\nV\nFMA: A Dataset For Music Analysis\nAudio Spectra Classification\n106,574\n518\nJapanese Vowels\nAudio Spectra Classification\n640\n12\nSpoken Arabic Digit\nAudio Spectra Classification\n8,800\n13\nBAUM-1\nAudio Spectra Classification\n1,184\n-\nBAUM-2\nAudio Spectra Classification\n1,047\n-\n3W dataset\nBusiness\n1984\n8\nCNNpred: CNN-based stock market prediction using a diverse set of variables\nBusiness\n1,985\n84\nDow Jones Index\nBusiness\n750\n16\nISTANBUL STOCK EXCHANGE\nBusiness\n536\n8\nMachine Learning based ZZAlpha Ltd. Stock Recommendations 2012-2014\nBusiness\n314,080\n-\nOnline Retail\nBusiness\n541,909\n8\nOnline Retail II\nBusiness\n1,067,371\n8\nMHEALTH Dataset\nECG Classification\n120\n23\nEEG Eye State\nEEG Classification\n14,980\n15\nEEG Steady-State Visual Evoked Potential Signals\nEEG Classification\n9,200\n16\nEpileptic Seizure Recognition\nEEG Classification\n11,500\n179\nEMG data for gestures\nEMG Classification\n30,000\n6\nEMG Physical Action Data Set\nEMG Classification\n10,000\n8\nsEMG for Basic Hand movements\nEMG Classification\n3,000\n2,500\nRobot Execution Failures\nFailure Detection\n463\n90\nAI4I 2020 Predictive Maintenance Dataset\nFailure Detection\n10,000\n14\nBreath Metabolomics\nGas Detection\n104\n1,656\nGas Sensor Array Drift Dataset at Different Concentrations\nGas Detection\n13,910\n129\nGas sensor array exposed to turbulent gas mixtures\nGas Detection\n180\n150,000\nGas sensor array temperature modulation\nGas Detection\n4,095,000\n20\nGas sensor array under dynamic gas mixtures\nGas Detection\n4,178,504\n19\nGas sensor array under flow modulation\nGas Detection\n58\n120,432\nGas sensor arrays in open sampling settings\nGas Detection\n18,000\n1,950,000\nGas sensors for home activity monitoring\nGas Detection\n919,438\n11\nOzone Level Detection\nGas Detection\n2,536\n73\nTwin gas sensor arrays\nGas Detection\n640\n480,000\nActivities of Daily Living (ADLs) Recognition Using Binary Sensors\nHuman Activity Recognition\n2,747\n-\nActivity Recognition from Single Chest-Mounted Accelerometer\nHuman Activity Recognition\n-\n-\nActivity Recognition system based on Multisensor data fusion (AReM)\nHuman Activity Recognition\n42,240\n6\nBar Crawl: Detecting Heavy Drinking\nHuman Activity Recognition\n14,057,567\n3\nBasketball dataset\nHuman Activity Recognition\n10,000\n7\nDaily and Sports Activities\nHuman Activity Recognition\n9,120\n5,625\nDaphnet Freezing of Gait\nHuman Activity Recognition\n237\n9\nDataset for ADL Recognition with Wrist-worn Accelerometer\nHuman Activity Recognition\n3\n14\nHeterogeneity Activity Recognition\nHuman Activity Recognition\n43,930,257\n16\nHuman Activity Recognition from Continuous Ambient Sensor Data\nHuman Activity Recognition\n13,956,534\n37\nHuman Activity Recognition Using Smartphones\nHuman Activity Recognition\n10,299\n561\nIntelligent Media Accelerometer and Gyroscope (IM-AccGyro) Dataset\nHuman Activity Recognition\n800\n9\nLocalization Data for Person Activity\nHuman Activity Recognition\n164,860\n8\nMEx\nHuman Activity Recognition\n6,262\n710\nOPPORTUNITY Activity Recognition\nHuman Activity Recognition\n2,551\n242\nPAMAP2 Physical Activity Monitoring\nHuman Activity Recognition\n3,850,505\n52\nREALDISP Activity Recognition Dataset\nHuman Activity Recognition\n1,419\n120\nselfBACK\nHuman Activity Recognition\n26,136\n6\nSimulated Falls and Daily Living Activities Data Set\nHuman Activity Recognition\n3,060\n138\nSmartphone Dataset for Human Activity Recognition in Ambient Assisted Living (AAL)\nHuman Activity Recognition\n5,744\n561\nSmartphone-Based Recognition of Human Activities and Postural Transitions\nHuman Activity Recognition\n10,929\n561\nUser Identification From Walking Activity\nHuman Activity Recognition\n-\n-\nVicon Physical Action Data Set\nHuman Activity Recognition\n3,000\n27\nWISDM Smartphone and Smartwatch Activity and Biometrics Dataset\nHuman Activity Recognition\n15,630,426\n6\nBLE RSSI dataset for Indoor localization\nIndoor localization\n23,570\n5\nBLE RSSI Dataset for Indoor localization and Navigation\nIndoor localization\n6,611\n15\nGeo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone\nIndoor localization\n153,540\n25\nHybrid Indoor Positioning Dataset from WiFi RSSI, Bluetooth and magnetometer\nIndoor localization\n1,540\n65\nUJIIndoorLoc-Mag\nIndoor localization\n40,000\n13\nKitsune Network Attack Dataset\nMalware/Attack Identification\n27,170,754\n115\nURL Reputation\nMalware/Attack Identification\n2,396,130\n3,231,961\nDetect Malware Types\nMalware/Attack Identification\n7,107\n280\nDynamic Features of VirusShare Executables\nMalware/Attack Identification\n107,888\n482\nBitcoinHeistRansomwareAddressDataset\nMalware/Attack Identification\n2,916,697\n10\nCharacter Trajectories\nMotion Classification\n2,858\n3\nGesture Phase Segmentation\nMotion Classification\n9,900\n50\nIndoor User Movement Prediction from RSS data\nMotion Classification\n13,197\n4\nPedestrian in Traffic Dataset\nMotion Classification\n4,760\n14\nWESAD (Wearable Stress and Affect Detection)\nMotion Classification\n63,000,000\n12\nAustralian Sign Language signs\nMotion Classification\n6,650\n15\nAustralian Sign Language signs (High Quality)\nMotion Classification\n2,565\n22\nPEMS-SF\nOthers\n440\n138,672\nParking Birmingham\nOthers\n35,717\n4\nOccupancy Detection\nOthers\n20,560\n7\nSynthetic Control Chart Time Series\nOthers\n600\n6\nCrop mapping using fused optical-radar data set\nOthers\n325,834\n175\nAbsenteeism at work\nOthers\n740\n21\nEducational Process Mining (EPM): A Learning Analytics Data Set\nOthers\n230,318\n13\nProductivity Prediction of Garment Employees\nOthers\n1,197\n15\nBuzz in social media\nOthers\n140,000\n77\nCondition monitoring of hydraulic systems\nOthers\n2,205\n43,680\nOpen University Learning Analytics dataset\nOthers\n-\n-\nData for Software Engineering Teamwork Assessment in Education Setting\nOthers\n74\n102\nBehavior of the urban traffic of the city of Sao Paulo in Brazil\nOthers\n135\n18\nTABLE VIII\nTHE UCI MULTIVARIATE TIME SERIES CLASSIFICATION ARCHIVE - 85 DATASETS. N DENOTES THE NUMBER OF SAMPLES AND V DENOTES THE\nNUMBER OF VARIABLES. CLASSES REFER TO THE NUMBER OF CLASSES IN EACH DATASET AND LENGTH REFERS TO THE LENGTH OF EACH TIME SERIES.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n24\nDataset\nApplication\nN\nV\nClasses\nLength\nDuckDuckGeese\nAudio Spectra Classification\n100\n1,345\n5\n270\nHeartbeat\nAudio Spectra Classification\n409\n61\n2\n405\nInsectWingbeat\nAudio Spectra Classification\n50,000\n200\n10\n30\nJapaneseVowels\nAudio Spectra Classification\n640\n12\n9\n29\nPhonemeSpectra\nAudio Spectra Classification\n6,668\n11\n39\n217\nSpokenArabicDigits\nAudio Spectra Classification\n8,798\n13\n10\n93\nAtrialFibrillation\nECG Classification\n30\n2\n3\n640\nStandWalkJump\nECG Classification\n27\n4\n3\n2,500\nFaceDetection\nEEG/MEG Classification\n9,414\n144\n2\n62\nFingerMovements\nEEG/MEG Classification\n416\n28\n2\n50\nHandMovementDirection\nEEG/MEG Classification\n234\n10\n4\n400\nMotorImagery\nEEG/MEG Classification\n378\n64\n2\n3,000\nSelfRegulationSCP1\nEEG/MEG Classification\n561\n6\n2\n896\nSelfRegulationSCP2\nEEG/MEG Classification\n380\n7\n2\n1,152\nBasicMotions\nHuman Activity Recognition\n80\n6\n4\n100\nCricket\nHuman Activity Recognition\n180\n6\n12\n1,197\nEpilepsy\nHuman Activity Recognition\n275\n3\n4\n206\nERing\nHuman Activity Recognition\n300\n4\n6\n65\nHandwriting\nHuman Activity Recognition\n1,000\n3\n26\n152\nLibras\nHuman Activity Recognition\n360\n2\n15\n45\nNATOPS\nHuman Activity Recognition\n360\n24\n6\n51\nRacketSports\nHuman Activity Recognition\n303\n6\n4\n30\nUWaveGestureLibrary\nHuman Activity Recognition\n440\n3\n8\n315\nArticularyWordRecognition\nMotion Classification\n575\n9\n25\n144\nCharacterTrajectories\nMotion Classification\n2,858\n3\n20\n182\nEigenWorms\nMotion Classification\n259\n6\n5\n17,984\nPenDigits\nMotion Classification\n10,992\n2\n10\n8\nEthanolConcentration\nOthers\n524\n3\n4\n1,751\nLSST\nOthers\n4,925\n6\n14\n36\nPEMS-SF\nOthers\n440\n963\n7\n144\nTABLE IX\nTHE UEA MULTIVARIATE TIME SERIES CLASSIFICATION ARCHIVE - 30 DATASETS. N DENOTES THE NUMBER OF SAMPLES AND V DENOTES THE\nNUMBER OF VARIABLES. CLASSES REFER TO THE NUMBER OF CLASSES IN EACH DATASET AND LENGTH REFERS TO THE LENGTH OF EACH TIME SERIES.\nDataset\nApplication\nN\nV\nClasses\nLength\nArabicDigits\nAudio Spectra Classification\n8,800\n13\n10\n4 - 93\nJapaneseVowels\nAudio Spectra Classification\n640\n12\n9\n7 - 29\nECG\nECG Classification\n200\n2\n2\n39 - 152\nLibras\nHuman Activity Recognition\n360\n2\n15\n45\nUWave\nHuman Activity Recognition\n4,478\n3\n8\n315\nAUSLAN\nMotion Classification\n2,565\n22\n95\n45 - 136\nCharacterTrajectories\nMotion Classification\n2,858\n3\n20\n109 - 205\nCMUsubject16\nMotion Classification\n58\n62\n2\n127 - 580\nKickvsPunch\nMotion Classification\n26\n62\n2\n274 - 841\nWalkvsRun\nMotion Classification\n44\n62\n2\n128 - 1,918\nNetFlow\nOthers\n1,337\n4\n2\n50 - 997\nPEMS\nOthers\n440\n963\n7\n144\nWafer\nOthers\n1,194\n6\n2\n104 - 198\nTABLE X\nTHE MTS MULTIVARIATE TIME SERIES CLASSIFICATION ARCHIVE - 13 DATASETS. N DENOTES THE NUMBER OF SAMPLES AND V DENOTES THE\nNUMBER OF VARIABLES. CLASSES REFER TO THE NUMBER OF CLASSES IN EACH DATASET AND LENGTH REFERS TO THE LENGTH OF EACH TIME SERIES.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n25\nCategory\nModel\nHAR\nWISDM\nEpilepsy\nACC\nMF1\nκ\nACC\nMF1\nκ\nACC\nMF1\nκ\nDeep Clustering\nDeepCluster 69.95±2.47 67.41±2.19 63.43±2.20 72.89±2.38 70.39±3.16 68.07±3.45 86.07±1.84 80.99±1.97 76.43±2.18\nIDFD\n67.07±1.62 64.12±1.45 60.87±1.40 68.14±1.82 64.78±1.91 61.08±1.93 79.56±1.73 74.84±1.52 70.24±1.56\nReconstruction-based TimeNet\n74.61±2.42 73.64±2.51 70.35±2.73 79.23±2.76 71.82±2.73 70.49±3.04 89.32±1.39 85.71±1.43 75.76±1.92\nDeconv\n71.53±1.65 70.76±1.84 62.72±2.39 75.76±2.30 69.45±2.79 66.28±3.21 85.44±1.92 79.85±2.08 76.01±2.57\nSelf-supervised\nTimeGAN\n73.57±1.96 70.04±1.94 66.71±2.30 79.22±2.61 72.43±2.88 69.26±3.03 89.79±1.24 86.55±1.36 74.83±1.77\n-Adversarial\nTimeVAE\n70.69±2.07 65.61±2.41 61.45±2.89 74.46±2.97 67.93±3.41 64.10±3.94 86.91±1.17 82.40±1.41 76.98±2.56\n-Predictive\nEEG-SSL\n65.34±1.63 63.75±1.37 57.20±1.22 70.67±1.11 66.50±0.93 62.44±1.07 93.72±0.45 89.15±0.93 77.65±1.64\nTST\n70.73±2.29 66.36±2.03 63.14±2.74 76.68±2.85 70.06±2.94 67.30±3.41 91.21±0.88 87.64±1.33 87.49±1.75\n-Contrastive\nSimCLR\n81.06±2.35 80.62±2.31 77.25±2.82 83.04±4.21 75.83±6.47 75.15±6.27 93.00±0.57 88.09±0.97 76.27±1.93\nBYOL\n89.46±0.17 89.31±0.17 87.33±0.20 87.84±0.38 84.02±0.77 82.43±0.57 98.08±0.09 96.99±0.15 93.99±0.30\nSwAV\n68.81±1.50 66.69±1.56 62.41±1.81 73.44±1.28 53.90±3.56 59.75±2.29 94.30±0.85 90.80±1.37 81.62±2.73\nPCL\n74.49±1.95 65.02±1.69 71.96±2.12 69.47±1.51 61.75±3.29 56.36±2.57 89.93±1.34 87.68±1.42 85.78±1.88\nTS-TCC\n89.22±0.70 89.23±0.76 87.03±0.85 81.48±0.98 69.17±2.25 73.13±1.33 97.19±0.18 95.47±0.31 90.94±0.61\nT-Loss\n91.06±0.94 90.94±0.96 89.26±1.13 91.48±1.05 88.79±1.53 87.79±1.51 96.94±0.20 95.20±0.30 90.41±0.61\nTABLE XI\nCOMPARISONS BETWEEN DIFFERENT CATEGORIES OF UNSUPERVISED LEARNING METHODS. ACC REFERS TO ACCURACY, MF1 REFERS TO\nMACRO-AVERAGED F1 SCORE, AND κ REFERS TO COHEN’S KAPPA COEFFICIENT.\nC. Results and Discussions\nWe choose 3 time series datasets for conducting compar-\nisons among various categories of unsupervised learning meth-\nods. Following this analysis, due to the promising results and\ngrowing popularity of contrastive learning methods over other\nunsupervised learning methods, we allocate special emphasis\non contrastive learning methods covering multiple levels of\ncontrast in the following subsections.\n1) A comparative analysis of different categories of un-\nsupervised learning methods: The comparisons between dif-\nferent categories of unsupervised learning methods on 3 se-\nlected datasets are presented in Table XI. In general, self-\nsupervised learning approaches outperform deep clustering\nand reconstruction-based methods. Such performance gain in-\ndicates that self-supervised models offer improved representa-\ntion learning capabilities. This can be attributed to the broader\nrange of pretext tasks, such as predicting missing elements,\ntemporal order, or context, which encourage the model to learn\nrich and meaningful representations. This diversity of tasks\nenables the model to capture different aspects of the data,\nleading to more comprehensive and robust representations.\nParticularly, contrastive methods have demonstrated promising\nresults in most datasets as they distinguish samples from\nmultiple views without the need for explicit labels. Such\ndiscrimination allows the learned representations to be more\neffective in capturing meaningful differences between data\nsamples. Instead, adversarial methods such as TimeGAN [69]\nand TimeVAE [70] assume specific data distributions, re-\nquire iterative optimization and often are more challenging to\nconverge. These methods can be computationally expensive\nand require larger amounts of data for training. Whereas,\ncontrastive methods are less sensitive to the data distribution\ncompared to adversarial methods and are generally easier\nto train. Additionally, predictive methods including EEG-\nSSL [71] and TST [104], tend to perform worse than most\ncontrastive methods. This is because predictive methods rely\non a large number of given contexts to accurately predict part\nof the data, which can be challenging in practice. Conversely,\ncontrastive methods can make better use of the available data\nby focusing on differentiating between a smaller number of\nsimilar and dissimilar data pairs.\nDataset\nLevel\nModel\nACC\nMF1\nκ\nEpilepsy\nInstance\nSimCLR 93.00±0.57 88.09±0.97\n76.27±1.93\nBYOL\n98.08±0.09 96.99±0.15\n93.99±0.30\nCPC\n96.61±0.43 94.44±0.69\n88.67±1.22\nPrototype\nSwAV\n94.30±0.85 90.80±1.37\n81.62±2.73\nPCL\n89.93±1.34 87.68±1.42\n85.78±1.88\nMHCCL\n97.85±0.49 95.44±0.82\n91.08±1.57\nTemporal\nTS2Vec\n96.32±0.23 94.27±0.37\n88.54±0.74\nTS-TCC\n97.19±0.18 95.47±0.31\n90.94±0.61\nT-Loss\n96.94±0.20 95.20±0.30\n90.41±0.61\nHAR\nInstance\nSimCLR 81.06±2.35 80.62±2.31\n77.25±2.82\nBYOL\n89.46±0.17 89.31±0.17\n87.33±0.20\nCPC\n83.85±1.51 83.27±1.66\n79.76±1.90\nPrototype\nSwAV\n68.81±1.50 66.69±1.56\n62.41±1.81\nPCL\n74.49±1.95 65.02±1.69\n71.96±2.12\nMHCCL\n91.60±1.06 91.77±1.11\n89.90±1.27\nTemporal\nTS2Vec\n90.47±0.66 90.46±0.64\n89.15±0.79\nTS-TCC\n89.22±0.70 89.23±0.76\n87.03±0.85\nT-Loss\n91.06±0.94 90.94±0.96\n89.26±1.13\nSHAR\nInstance\nSimCLR 67.22±1.76 53.05±2.42\n63.84±1.97\nBYOL\n67.00±0.98 59.30±1.30\n63.62±1.10\nCPC\n68.57±1.40 62.44±1.77\n61.62±0.96\nPrototype\nSwAV\n57.49±2.75 58.82±2.53\n58.89±3.50\nPCL\n56.28±1.47 49.42±1.62\n51.78±1.65\nMHCCL\n83.42±1.76 78.45±2.09\n80.58±1.84\nTemporal\nTS2Vec\n82.94±2.91 77.89±2.95\n78.94±3.20\nTS-TCC\n70.80±0.75 64.28±0.72\n67.74±0.85\nT-Loss\n80.88±3.94 77.06±3.44\n79.65±4.35\nWISDM\nInstance\nSimCLR 83.04±4.21 75.83±6.47\n75.15±6.27\nBYOL\n87.84±0.38 84.02±0.77\n82.43±0.57\nCPC\n80.35±0.98 73.24±1.21\n72.23±1.07\nPrototype\nSwAV\n73.44±1.28 53.90±3.56\n59.75±2.29\nPCL\n69.47±1.51 61.75±3.29\n56.36±2.57\nMHCCL\n93.60±1.06 91.70±1.10\n90.96±1.08\nTemporal\nTS2Vec\n92.33±1.05 90.27±1.07\n90.36±1.49\nTS-TCC\n81.48±0.98 69.17±2.25\n73.13±1.33\nT-Loss\n91.48±1.05 88.79±1.53\n87.79±1.51\nTABLE XII\nA COMPARATIVE ANALYSIS OF DIFFERENT LEVELS OF CONTRAST ON 4\nDATASETS. ACC REFERS TO ACCURACY, MF1 REFERS TO\nMACRO-AVERAGED F1 SCORE, AND κ REFERS TO COHEN’S KAPPA\nCOEFFICIENT.\n2) A comparative analysis of performing different levels of\ncontrast: We conducted a specific comparison and analysis of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n26\nDataset Level\nModel\nACC\nMF1\nκ\nInstance\nSimCLR 40.00±0.60\n39.01±0.69\n25.00±0.74\nBYOL\n51.60±1.20\n52.04±1.34\n39.50±1.50\nCPC\n44.72±2.19\n42.90±3.45\n30.77±4.16\nPrototype\nSwAV\n52.40±3.44\n50.95±4.46\n40.50±4.30\nDDG\nPCL\n48.62±3.56\n46.86±3.97\n35.67±5.74\nMHCCL\n50.76±3.19\n49.52±3.54\n39.50±4.27\nTemporal\nTS2Vec\n52.00±3.27\n51.02±3.64\n40.00±4.08\nTS-TCC\n38.67±5.96\n35.51±6.48\n23.33±7.45\nT-Loss\n54.50±4.33\n52.70±5.48\n43.13±5.41\nInstance\nSimCLR 49.20±0.98\n43.08±4.90\n-0.69±2.29\nBYOL\n49.60±1.11\n49.38±1.12\n-0.49±2.22\nCPC\n49.73±1.45\n44.76±1.69\n-0.27±2.70\nPrototype\nSwAV\n44.22±2.19\n44.19±2.20\n8.47±4.40\nFM\nPCL\n50.44±1.95\n43.68±1.30\n1.19±3.89\nMHCCL\n52.09±1.45\n50.51±2.06\n17.87±3.39\nTemporal\nTS2Vec\n50.00±1.63\n49.99±1.64\n-0.01±3.30\nTS-TCC\n50.17±1.60\n49.07±2.10\n0.33±4.58\nT-Loss\n50.50±1.72\n50.33±1.76\n4.01±6.65\nInstance\nSimCLR 93.35±0.17\n93.27±0.17\n92.61±0.19\nBYOL\n94.93±0.08\n94.96±0.07\n94.37±0.09\nCPC\n92.03±0.32\n91.00±0.35\n90.04±0.36\nPrototype\nSwAV\n78.02±2.86\n76.86±3.06\n71.10±3.19\nPD\nPCL\n86.18±1.25\n83.36±1.11\n80.69±1.01\nMHCCL\n98.69±0.41\n98.71±0.55\n97.43±0.72\nTemporal\nTS2Vec\n97.83±0.24\n97.80±0.25\n97.59±0.27\nTS-TCC\n97.44±0.23\n97.45±0.23\n97.16±0.26\nT-Loss\n97.86±0.52\n97.87±0.52\n97.63±0.57\nInstance\nSimCLR 15.14±0.30\n12.64±0.33\n12.91±0.31\nBYOL\n12.08±0.21\n11.62±0.19\n9.77±0.21\nCPC\n14.31±0.78\n14.32±0.77\n9.95±0.84\nPrototype\nSwAV\n11.66±0.64\n8.92±0.58\n9.33±0.65\nPS\nPCL\n8.92±1.04\n8.96±1.24\n6.53±1.49\nMHCCL\n15.43±0.79\n14.29±0.76\n12.63±0.79\nTemporal\nTS2Vec\n15.69±0.90\n15.69±0.95\n13.47±0.92\nTS-TCC\n10.90±0.76\n9.16±0.65\n8.56±0.78\nT-Loss\n18.60±0.25\n18.15±0.28\n16.46±0.25\nInstance\nSimCLR 60.87±1.84\n55.62±4.69\n47.99±3.27\nBYOL\n79.39±0.84\n75.12±1.05\n71.84±1.19\nCPC\n67.03±1.38\n63.06±1.38\n58.26±1.53\nPrototype\nSwAV\n43.75±1.80\n35.16±2.31\n21.82±2.27\nEW\nPCL\n43.28±1.06\n40.02±2.01\n22.58±1.59\nMHCCL\n79.10±2.21\n76.01±1.45\n69.53±1.90\nTemporal\nTS2Vec\n82.80±2.52\n81.59±3.73\n80.74±3.41\nTS-TCC\n73.21±1.44\n64.33±0.81\n63.41±1.17\nT-Loss\n75.00±3.39\n67.85±3.64\n65.12±4.91\nTABLE XIII\nA COMPARATIVE ANALYSIS OF DIFFERENT LEVELS OF CONTRAST ON\nUEA DATASETS. ACC REFERS TO ACCURACY, MF1 REFERS TO\nMACRO-AVERAGED F1 SCORE, AND κ REFERS TO COHEN’S KAPPA\nCOEFFICIENT.\ncontrastive methods focusing on different levels of contrast.\nTable XIII shows the comparative analysis at different levels\nof contrast on UEA datasets, and the results on 4 other datasets\nare shown in Table XII. From the results, it can be observed\nthat contrastive learning models such as TS2Vec [52], TS-\nTCC [51] and T-Loss [50], which emphasize the impact at the\ntemporal level, achieve better results compared to methods that\nfocus on instance-level or prototype-level contrast. However,\nprototype-level contrastive learning models such as SwAV [59]\nand PCL [58], which have shown great success in image data,\ndo not perform as well in time series data. These methods\nusually employ flat clustering algorithms, which are capable\nof capturing only a single hierarchy of semantic clusters.\nNonetheless, the result obtained from one clustering iteration\ncannot be consistently relied upon as there is no assurance\nof their accuracy, rendering them inadequate. In this case,\nDataset\nMetric\nResNet-18\nResNet-50\nInceptionTime\nHAR\nACC\n78.70±2.27\n81.06±2.35\n82.25±1.94\nMF1\n78.44±2.19\n80.62±2.31\n80.56±2.28\nκ\n76.21±2.43\n77.25±2.82\n78.39±2.65\nWISDM\nACC\n86.94±1.92\n83.04±4.21\n87.46±1.87\nMF1\n81.65±3.26\n75.83±6.47\n82.14±2.85\nκ\n80.91±2.95\n75.15±6.27\n81.62±2.24\nEpilepsy\nACC\n93.42±0.26\n93.00±0.57\n93.68±0.41\nMF1\n88.72±0.54\n88.09±0.97\n88.80±0.64\nκ\n77.54±1.06\n76.27±1.93\n77.89±1.39\nTABLE XIV\nTHE CLASSIFICATION PERFORMANCE OF USING DIFFERENT BACKBONES\nIN SIMCLR. ACC REFERS TO ACCURACY, MF1 REFERS TO\nMACRO-AVERAGED F1 SCORE, AND κ REFERS TO COHEN’S KAPPA\nCOEFFICIENT.\nit is worth noting that MHCCL [60] demonstrates superior\nperformance as it incorporates hierarchical clustering into the\nconstruction of contrastive pairs, which helps address the\nlimitation of flat clustering algorithms.\n3) A comparative analysis of using different backbones:\nTable XIV presents a comparative analysis of employing\ndifferent backbones in the SimCLR model for unsupervised\ntime series representation learning. Typically, we find that most\nmodels summarized in Table IV, Table V and Table VI utilize\nResNets [145] as the backbone and achieve satisfactory perfor-\nmance. Therefore, we compare the results of using ResNet-18\nand ResNet-50 as backbones. Furthermore, we evaluate Incep-\ntionTime [146], which is specifically designed for time series\ndata by incorporating the temporal dependencies. As shown\nin Table XIV, InceptionTime outperforms the general-purpose\nResNets in most datasets. This observation highlights the\ncapability of InceptionTime to identify both local and global\nshape patterns (i.e., low-level and high-level features) in time\nseries data. InceptionTime proves to be exceptionally fitting\nfor time series data due to its utilization of 1D convolutions\nwith multiple kernel sizes in parallel. This approach facilitates\nefficient feature extraction on time series data across various\nscales, making it well-suited to capture diverse granularities of\nsemantics in time series data. In contrast, ResNets primarily\nrely on 2D convolutions and are optimized for image data,\nwhich may not yield the same level of effectiveness when\napplied to time series data analysis.\n4) A comparative analysis of employing different augmenta-\ntion selection strategies: Table XV presents the classification\naccuracy of employing different data augmentation selection\nstrategies. We evaluate 4 strategies in 3 popular contrastive\nlearning models on 3 datasets. As shown in Table XV, we com-\npare the original strategy (Original) used in each model with 3\nalternative augmentation selection strategies. The specified one\n(Specified) follows the implementation in TS-TCC [51] which\nemploys simple yet efficient transformations that can fit any\ntime series data to create a strong and a weak view. Therefore,\nthe specified strategy is equivalent to the original one in TS-\nTCC model. Both the aforementioned original and specified\naugmentation selection strategies are developed based on do-\nmain expert knowledge and empirical results. Additionally, we\nalso compare with 2 automatic selection strategies including\nRandAugment [147] and W-Augment [148]. Nevertheless, it\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n27\nis noteworthy that the advancements achieved through the\nutilization of automatic augmentation selection strategies have\nshown minimal improvement. In some cases, the performance\ndoes not surpass that of the original performance. Conse-\nquently, developing automatic augmentation selection strate-\ngies specifically tailored for time series data or optimizing\nthe combinations of various transformations remains an open\nproblem for boosting contrastive learning.\nDataset\nStrategy\nSimCLR\nTS-TCC\nBYOL\nHAR\nOriginal\n81.83±2.18\n89.22±0.70\n89.36±2.09\nSpecified\n81.06±2.35\n89.22±0.70\n89.46±0.17\nRandAugment\n81.02±2.37\n87.94±2.14\n89.90±2.37\nW-Augment\n81.27±1.66\n87.57±1.69\n89.35±1.96\nWISDM\nOriginal\n83.71±3.48\n81.48±0.98\n87.91±2.42\nSpecified\n83.04±4.21\n81.48±0.98\n87.84±0.38\nRandAugment\n82.78±2.69\n82.32±2.36\n88.03±2.99\nW-Augment\n84.30±1.85\n82.47±1.87\n87.84±2.25\nEpilepsy\nOriginal\n92.97±1.62\n97.19±0.18\n97.64±1.23\nSpecified\n93.00±0.57\n97.19±0.18\n98.08±0.09\nRandAugment\n93.26±2.17\n98.04±0.64\n98.06±0.91\nW-Augment\n92.19±1.79\n97.52±0.92\n97.67±0.79\nTABLE XV\nTHE CLASSIFICATION ACCURACY OF EMPLOYING DIFFERENT DATA\nAUGMENTATION SELECTION STRATEGIES.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2023-08-03",
  "updated": "2023-08-03"
}