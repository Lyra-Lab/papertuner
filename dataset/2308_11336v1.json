{
  "id": "http://arxiv.org/abs/2308.11336v1",
  "title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
  "authors": [
    "Xiaocong Chen",
    "Siyu Wang",
    "Julian McAuley",
    "Dietmar Jannach",
    "Lina Yao"
  ],
  "abstract": "Reinforcement learning serves as a potent tool for modeling dynamic user\ninterests within recommender systems, garnering increasing research attention\nof late. However, a significant drawback persists: its poor data efficiency,\nstemming from its interactive nature. The training of reinforcement\nlearning-based recommender systems demands expensive online interactions to\namass adequate trajectories, essential for agents to learn user preferences.\nThis inefficiency renders reinforcement learning-based recommender systems a\nformidable undertaking, necessitating the exploration of potential solutions.\nRecent strides in offline reinforcement learning present a new perspective.\nOffline reinforcement learning empowers agents to glean insights from offline\ndatasets and deploy learned policies in online settings. Given that recommender\nsystems possess extensive offline datasets, the framework of offline\nreinforcement learning aligns seamlessly. Despite being a burgeoning field,\nworks centered on recommender systems utilizing offline reinforcement learning\nremain limited. This survey aims to introduce and delve into offline\nreinforcement learning within recommender systems, offering an inclusive review\nof existing literature in this domain. Furthermore, we strive to underscore\nprevalent challenges, opportunities, and future pathways, poised to propel\nresearch in this evolving field.",
  "text": "111\nOn the Opportunities and Challenges of Offline\nReinforcement Learning for Recommender Systems\nXIAOCONG CHEN, Data61, CSIRO, Australia\nSIYU WANG, UNSW Sydney, Australia\nJULIAN MCAULEY, UCSD, USA\nDIETMAR JANNACH, University of Klagenfurt, Austria\nLINA YAO, Data61, CSIRO & UNSW Sydney, Australia\nReinforcement learning serves as a potent tool for modeling dynamic user interests within recommender\nsystems, garnering increasing research attention of late. However, a significant drawback persists: its poor data\nefficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender\nsystems demands expensive online interactions to amass adequate trajectories, essential for agents to learn\nuser preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable\nundertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement\nlearning present a new perspective. Offline reinforcement learning empowers agents to glean insights from\noffline datasets and deploy learned policies in online settings. Given that recommender systems possess\nextensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a\nburgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain\nlimited. This survey aims to introduce and delve into offline reinforcement learning within recommender\nsystems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore\nprevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.\nCCS Concepts: • Information systems →Recommender Systems; • Computing methodologies →\nReinforcement Learning.\nAdditional Key Words and Phrases: Offline Reinforcement Learning\nACM Reference Format:\nXiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, and Lina Yao. 2023. On the Opportunities\nand Challenges of Offline Reinforcement Learning for Recommender Systems. J. ACM 37, 4, Article 111\n(August 2023), 24 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nIn recent years, notable advancements have materialized in the realm of recommendation techniques,\ntranscending the scope of traditional approaches (such as collaborative filtering, content-based\nrecommendation, and matrix factorization [32]). This evolution has led to the emergence of deep\nlearning-based methods in the field of recommender systems (RS). The appeal of deep learning\nstems from its ability to comprehend intricate non-linear relationships between users and items,\nmaking it adept at accommodating diverse data sources like images and text. The adoption of\nAuthors’ addresses: Xiaocong Chen, xiaocong.chen@data61.csiro.au, Data61, CSIRO, Australia; Siyu Wang, siyu.wang5@\nunsw.edu.au, UNSW Sydney, Australia; Julian McAuley, jmcauley@eng.ucsd.edu, UCSD, USA; Dietmar Jannach, dietmar.\njannach@aau.at, University of Klagenfurt, Austria; Lina Yao, lina.yao@unsw.edu.au, Data61, CSIRO & UNSW Sydney,\nAustralia.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2023 Association for Computing Machinery.\n0004-5411/2023/8-ART111 $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\narXiv:2308.11336v1  [cs.IR]  22 Aug 2023\n111:2\nChen et al.\ndeep learning in RS has proven beneficial in tackling multifaceted challenges. Its strength lies in\naddressing intricate tasks and managing complex data structures [61].\nTraditional recommendation systems (RS) have limitations in capturing interest dynamics, a\nchallenge that emphasizes the distinction between users’ long-term and short-term interests [7, 61].\nSpecifically, while these traditional methods are adept at recognizing and modeling long-term\ninterests based on historical data and patterns, they often fall short in accounting for the rapidly\nchanging and more nuanced short-term interests. This gap in responsiveness to short-term shifts\ncan lead to recommendations that are out-of-sync with a user’s current preferences or situational\nneeds. In contrast, deep reinforcement learning (RL) aims to train an agent with the capacity to\nlearn from interaction trajectories provided by the environment, achieved through the integration\nof deep learning and RL techniques as expounded in [11]. Notably, this approach empowers the\nagent to proactively glean insights from real-time user feedback, thereby enabling the discernment\nof evolving user preferences within the dynamic context of reinforcement learning.\nRL provides a structured mathematical framework for acquiring learning-based control strategies.\nBy employing RL, we can systematically attain highly effective behavioral policies, which encap-\nsulate action strategies. These policies are engineered to optimize predefined objectives referred\nto as reward functions. In essence, the reward function serves as a directive, guiding the RL algo-\nrithm towards desired actions, while the algorithm itself devises the means to enact these actions.\nThroughout its history, the field of RL has been a subject of intensive research. More recently, the\nintegration of robust tools like deep neural networks into RL methodologies has yielded substantial\nadvancements. These neural networks act as versatile approximators, empowering RL techniques\nto exhibit exceptional performance across a diverse array of problem domains.\nNevertheless, a pertinent challenge to the widespread implementation of RL techniques emerges.\nRL methods fundamentally follow an incremental learning approach, wherein they gather knowl-\nedge by iteratively engaging with their environment. Subsequent refinements are informed by\nprevious experiences. While this iterative learning approach is effective in numerous scenarios,\nits practicality is not universal. Consider cases such as real-world robotics, educational software\npedagogy, or healthcare interventions; these situations entail potential risks and resource expenses\nthat cannot be disregarded. Moreover, even within scenarios conducive to online learning, such as\nin the context of RS, a preference for historical data often arises. This preference is particularly\npronounced in intricate domains where sound decision-making hinges upon substantial data in-\nputs. The rationale is that leveraging previously amassed data enables informed decisions without\nnecessitating continuous real-world experimentation.\nThe success of machine learning methods in solving real-world problems in the past decade is\nlargely thanks to new ways of learning from large amounts of data. These methods get better as\nthey’re trained with more data. However, applying this approach to online Reinforcement Learning\n(RL) doesn’t fit well. While this wasn’t a big problem when RL methods were simpler and used\nsmall datasets for easy problems, adding complex neural networks to RL makes us wonder if we\ncan use the same data-driven approach for RL goals. This would mean creating a system where RL\nlearns from existing data without needing more data collected in real-time [27].\nHowever, this idea of using existing data for RL brings its own challenges. As we discuss in this\narticle, many common RL methods can learn from data collected differently from how the policy\nbehaves. But these methods often struggle when trying to learn effectively from a whole set of\ndata collected in advance, without more data being collected as the policy improves. Making things\nmore complicated with high-dimensional neural networks can make this problem worse. A big\nissue with using pre-existing data for RL is that the data’s distribution may not match real-world\nconditions [27]. Still, the potential of a fully offline RL system is exciting. Just like how machine\nlearning can turn data into useful tools like image recognition or speech understanding, an offline\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:3\nOffline \nRL4RS\nCurrent \nProgress\nChallenges \nand \nOpportunities\nFuture \nDirections\nOff-policy with \nLogged Data\nOffline RLRS\nOffline Data \nQuality\nDistribution Shift\nBia and Variance\nExplainability\nCross-domain \nRecommendation\nLarge Language \nModel\nCausality\nRobustness\nFig. 1. The overall structure of this survey including the section index.\nRL system, using strong function approximators, might turn data into smart decision-makers. This\ncould mean that people with lots of data could make policies that help them make better choices\nfor what they want to achieve [35].\nRS and advertising are particularly well-suited areas for applying offline RL. This is because\ncollecting data is straightforward and efficient, often done by recording user actions. Moreover,\nthe existing RS literature provides sufficient datasets which can be used for training offline RL.\nHowever, these domains are also critical in terms of safety. Making a very poor decision could lead\nto significant financial losses. Therefore, traditional online exploration methods are not practical\nhere. This is why offline RL methods have a history of being used in these fields.\nOne technique commonly employed is called off-policy evaluation. This approach is useful for\nrunning A/B tests and estimating the effectiveness of advertising and RS methods without needing\nto interact with the environment further.\nIn the case of RS, things are a bit different compared to other applications. RS policy evaluation\nis often set up as a contextual bandit problem. Here, \"states\" might represent a user’s past behavior,\nand \"actions\" are the recommendations made to them. This simplification avoids the complexity\nof sequential decision making, which is useful. However, it can lead to inaccuracies if actions are\nconnected over time, like in robotics or healthcare scenarios.\nUsing offline RL for RS has practical applications such as optimizing recommendations presented\ntogether on a page, improving entire web pages, and estimating website visits with the help of\ndoubly robust estimation. Another use is A/B testing to fine-tune click rates for optimization.\nResearchers have also used offline data to learn policies. This includes efforts like improving click-\nthrough rates for newspaper articles, ranking advertisements on search pages, and tailoring ad\nrecommendations for digital marketing.\nIn this survey, our main focus will be on offline RL in RS (offline RL4RS). We aim to provide\na comprehensive overview of existing works, along with discussing open challenges and future\ndirections.\n1.1\nRelations to existing surveys\nTwo existing surveys have centered on the topic of RL in RS [1, 11]. While Afsar et al. [1] provides\nan overview of RL in RS, it does not comprehensively explore the expanding realm of deep RL. In\ncontrast, [11] delves more deeply into the analysis and discussion of RL in RS, but predominantly\nfocuses on online RL and its RS applications. It’s noteworthy that [11] identifies offline RL in RS as\na potential future direction but does not offer an all-encompassing review of this area. The limited\ncoverage of offline RL in RS can be attributed to its emergence around the same time as these two\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:4\nChen et al.\nsurveys. Furthermore, due to the recent establishment of the offline RL concept, certain works\nexamined in these two existing surveys are classified as special cases of policy-based methods.\nDifferently, this survey endeavors to refine these categorizations by reclassifying prior works into\nthe domain of offline RL in RS. Additionally, we extend the literature to encompass the most recent\ndevelopments in offline RL for RS, thereby augmenting our understanding of recent progress in\nthis field.\n1.2\nStructure of this Survey\nThis survey is structured into four distinct sections. Firstly, we offer an introduction to RL ba-\nsics, providing readers with a foundational understanding of various RL algorithms, including\nQ-Learning, Policy-based Methods, Actor-Critic Methods, and Model-based RL. Subsequently, we\ndelve into the concept of offline RL and present a problem formulation that explores how to integrate\nrecommender systems (RS) into the offline RL framework. Continuing, we conduct a comprehensive\nreview of existing works from two main perspectives: off-policy evaluation using logged data and\nthe realm of offline RL in RS. This examination highlights current research trends and insights.\nFollowing the review, we outline the open challenges and promising opportunities that warrant\nin-depth exploration. Finally, building upon the identified challenges and opportunities, we propose\npotential future directions that could serve as solutions to these challenges. This forward-looking\nsection aims to guide future research endeavors in the field, by suggesting pathways to address the\noutstanding issues and capitalize on the untapped opportunities.\n2\nOFFLINE RL OVERVIEW AND PROBLEM STATEMENT\nIn this section, we delve into fundamental concepts essential to understanding the field of RL. We\ninitiate with RL preliminaries, encompassing Markov Decision Processes, On-Policy and Off-Policy\nLearning, and Typical RL algorithms. In doing so, we establish the foundational understanding by\nclarifying key principles and terminologies employed throughout this survey. Subsequently, we\nshift our focus toward the concept of offline RL and how it can be used to formulate RS. For the\nsake of clarity, we have summarized the common notations used in this survey in Table 1.\n2.1\nMarkov Decision Process\nTable 1. Common notations used in this survey\nNotations\nName\nNotations\nName\nNotes\nM\nMarkov Decision Process\n𝑠\nState\nUser related info\n𝜋𝛽\nBehavior Policy\n𝑎\nAction\nRecommended item(s)\n𝛾\nDiscount Factor\n𝜋\nPolicy\nRecommendation policy\nE\nExpected Value\nR(·, ·)\nReward function\nUsers’ click behavior\n𝜃\nPolicy Parameter\nD\nOffline Dataset\nSet of {(𝑠𝑡,𝑎𝑡,𝑠𝑡+1,𝑟𝑡)}\nIn this section, we shall expound upon fundamental concepts within the realm of RL, adhering\nclosely to established standard definitions as outlined in[44]. RL deals with the challenge of learning\nhow to control dynamic systems in a broad context. RL4RS are typically described by fully observed\nMarkov decision processes (MDP) or partially observed ones known as Partially Observable Markov\nDecision Processes (POMDP). Moreover, we will also provide\nDefinition 1 (Markov decision process). The Markov decision process is formalized as the tuple\nM = {S, A, P, R,𝛾}. Within this structure, each component serves a distinct purpose: S encompasses\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:5\nthe set of states 𝑠∈S, capable of adopting discrete or continuous values, potentially even multi-\ndimensional vectors. A characterizes the set of actions 𝑎∈A, which may be discrete or continuous in\nnature. P defines a conditional probability distribution, P(𝑠𝑡+1|𝑠𝑡,𝑎𝑡), delineating the progression of\nthe system’s dynamics over time. R : S × A →R serves as the reward function, linking states and\nactions to real-valued rewards. 𝛾∈[0, 1] assumes the role of a scalar discount factor, influencing the\nextent to which future rewards are taken into consideration.\nThroughout most of this article, we will primarily employ fully-observed formalism. However,\nwe also include the definition of the partially observed Markov decision process (POMDP) to ensure\ncomprehensiveness. The MDP definition can be extended to the partially observed setting in the\nfollowing manner:\nDefinition 2 (Partially observed Markov decision process). The partially observed Markov\ndecision process is defined as a tuple M = {S, A, O, P, R,𝛾}, where S, A, P, R,𝛾are defined as\nbefore, O is a set of observations, where each observation is given by 𝑜∈O.\nThe ultimate objective within a RL problem is to acquire a policy, denoted as 𝜋, which establishes\na probability distribution over actions conditioned upon states, 𝜋(𝑎𝑡|𝑠𝑡), or alternatively condi-\ntioned upon observations within the context of partially observed scenarios, 𝜋(𝑎𝑡|𝑜𝑡). From these\ndefinitions, we can derive the trajectory distribution. A trajectory in this context refers to a sequence\nencompassing both states and actions, spanning a length of𝑇, represented as 𝜏= {𝑠0,𝑎0, · · · ,𝑠𝑇,𝑎𝑇}.\nIt is noteworthy that the parameter 𝑇can be an infinite value, implying the consideration of sce-\nnarios with an indefinite time horizon, as seen in infinite horizon MDP [44].\nThe trajectory distribution 𝑝𝜋for a given MDP tuple M and policy 𝜋is given by\n𝑝𝜋(𝜏) = 𝑑0(𝑠0)\n𝑇\nÖ\n𝑡=0\n𝜋(𝑎𝑡|𝑠𝑡)P(𝑠𝑡+1|𝑠𝑡,𝑎𝑡),\n(1)\nwhere 𝑑0(𝑠0) represents the initial state distribution. This definition can easily be extended into\nthe partially observed setting by including the observations 𝑜𝑡. The RL objective 𝐽(𝜋), can then be\nwritten as an expectation under this trajectory distribution:\n𝐽(𝜋) = E𝜏∼𝑝𝜋(𝜏)\n\u0014 𝑇∑︁\n𝑡=0\n𝛾𝑡R(𝑠𝑡,𝑎𝑡)\n\u0015\n.\n(2)\n2.2\nOn-Policy and Off-Policy Learning\nWhile the process of interaction unfolds, gathering additional episodes enhances the precision\nof the function estimates. Nevertheless, a challenge arises. If the policy improvement algorithm\nconsistently adjusts the policy in a greedy manner—prioritizing actions with immediate rewards—\nthen actions and states lying outside this advantageous route might not be adequately sampled.\nConsequently, superior rewards that could exist in these unexplored areas remain concealed from\nthe learning process. Fundamentally, we confront a decision between opting for the optimal choice\nbased on existing data or delving deeper into exploration to collect more data. This predicament is\ncommonly recognized as the Exploration vs. Exploitation Dilemma.\nWhat we need is a middle ground between these two extremes. Pure exploration would require\na significant amount of time to collect the necessary information, while pure exploitation could\ntrap the agent in a local reward maximum. To address this, there are two approaches that ensure\nall actions are adequately sampled, known as on-policy and off-policy methods.\nOn-policy methods resolve the exploration vs. exploitation dilemma by incorporating randomness\nthrough a soft policy. This means that non-greedy actions are chosen with some probability. These\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:6\nChen et al.\npolicies are referred to as 𝜖-greedy policies because they select random actions with a probability\nof 𝜖and follow the optimal action with a probability of 1-𝜖.\nSince the probability of randomly selecting an action from the action space is 𝜖, the probability\nof choosing any specific non-optimal action is 𝜖/|A(𝑠)|. On the other hand, the probability of\nfollowing the optimal action will always be slightly higher due to the 1 - 𝜖probability of selecting\nit outright and the 𝜖/|A(𝑠)| probability of choosing it through sampling the action space:\nOff-policy methods offer a different solution to the exploration vs. exploitation problem. While\non-policy algorithms attempt to improve the same 𝜖-greedy policy used for exploration, off-policy\napproaches utilize two distinct policies: a behavior policy and a target policy. The behavioral policy\n(denoted as 𝜋𝛽) is employed for exploration and episode generation, while the target or goal policy\n(denoted as 𝜋) is used for function estimation and improvement.\nThe efficacy of this approach lies in the capacity of the target policy 𝜋to attain a balanced\nperspective of the environment, enabling it to assimilate insights from the behavioral policy𝑏, while\nconcurrently capturing advantageous actions and seeking further improvements. Nevertheless, it is\nimperative to acknowledge that in off-policy learning, a distributional discrepancy arises between\nthe target policy estimation and the sampled policy. Consequently, a widely employed technique\nknown as importance sampling is applied to address this disparity [28].\n2.3\nTypical RL algorithms\nLet’s briefly outline various types of RL algorithms and present their definitions. At a high level, all\nstandard RL algorithms follow a common learning loop: the agent engages with the MDP M using\na behavior policy 𝜋𝛽. This behavior policy, which could or could not align with 𝜋(𝑎|𝑠), leads the\nagent to observe the current state 𝑠𝑡, choose an action 𝑎𝑡, and then witness the subsequent state\n𝑠𝑡+1 and the reward value 𝑟𝑡= R(𝑠𝑡,𝑎𝑡). This sequence can repeat over multiple steps, allowing the\nagent to gather transitions {𝑠𝑡,𝑎𝑡,𝑠𝑡+1,𝑟𝑡}. These observed transitions are then used by the agent to\nadjust its policy, and this update process might incorporate earlier observed transitions as well.\nWe’ll denote the set of available transitions for policy updating as D = {(𝑠𝑡,𝑎𝑡,𝑠𝑡+1,𝑟𝑡)}. This set\ncould encompass all the transitions seen thus far or a subset thereof.\nQ-learning [51] is an off-policy value-based learning scheme aimed at finding a target policy\nthat selects the best action:\n𝜋(𝑠) = arg max\n𝑎\n𝑄𝜋(𝑠,𝑎)\n(3)\nHere, 𝑄𝑢(𝑠,𝑎) represents the Q-value and applies to a discrete action space. For deterministic\npolicies, the Q-value can be computed as:\n𝑄(𝑠𝑡,𝑎𝑡) = E𝜏∼𝜋[𝑟(𝑠𝑡,𝑎𝑡) + 𝛾𝑄(𝑠′\n𝑡,𝑎′\n𝑡)].\n(4)\nDeep Q learning (DQN) [38] employs deep learning to approximate a nonlinear Q function\nparameterized by 𝜃𝑞: 𝑄𝜃𝑞(𝑠,𝑎). DQN involves a network 𝑄𝜃𝑞that’s updated asynchronously by\nminimizing the Mean Squared Error (MSE) as defined by:\nL(𝜃𝑞) = E𝜏∼𝜋\nh\n𝑄𝜃𝑞(𝑠𝑡,𝑎𝑡) −(𝑟(𝑠𝑡,𝑎𝑡) + 𝛾𝑄𝜃𝑞(𝑠′𝑡,𝑎′𝑡))\ni2\n(5)\nIn this equation, 𝜏signifies a sampled trajectory including (𝑠,𝑎,𝑠′,𝑟(𝑠,𝑎)). Notably, 𝑠′\n𝑡and 𝑎′\n𝑡\noriginate from the behavior policy 𝜋𝑏, while 𝑠,𝑎come from the target policy 𝜋.\nFurthermore, the concept of value functions plays a role. These assess states and actions. The\nvalue function 𝑉𝜋(𝑠) evaluates states, and 𝑄𝜋(𝑠𝑡,𝑎𝑡) evaluates actions. The relationship between\nthem is given by:\n𝑉𝜋(𝑠) = E𝑎∼𝜋[𝑄𝜋(𝑠,𝑎)].\n(6)\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:7\nThe value function gets updated via the Temporal Difference (TD) method:\n𝑉𝜋(𝑠𝑡) ←𝑉𝜋(𝑠𝑡) + 𝛼[𝑟(𝑠′\n𝑡,𝑎′𝑡) + 𝛾𝑉𝜋(𝑠′𝑡) −𝑉𝜋(𝑠𝑡)],\n(7)\nwhere 𝛼represents the learning rate.\nPolicy gradient [52] is a technique used in reinforcement learning that tackles scenarios\nwhere actions are high-dimensional or continuous—something not easily managed by Q-learning.\nUnlike Q-learning, which focuses on finding optimal actions, policy gradient aims to find optimal\nparameters 𝜃for a policy 𝜋𝜃in order to maximize the total reward.\nThe central goal of policy gradient is to maximize the expected return, or accumulated reward,\nstarting from the initial state. This is captured by the equation:\n𝐽(𝜋𝜃) = E𝜏∼𝜋𝜃[𝑟(𝜏)] =\n∫\n𝜋𝜃(𝜏)𝑟(𝜏)𝑑𝜏\n(8)\nHere, 𝜋𝜃(𝜏) signifies the probability of observing trajectory 𝜏. The technique learns the optimal\nparameter 𝜃by computing the gradient ∇𝜃𝐽(𝜋𝜃) as follows:\n∇𝜃𝐽(𝜋𝜃) = E𝜏∼𝑑𝜋𝜃\n\" 𝑇∑︁\n𝑡=1\n𝑟(𝑠𝑡,𝑎𝑡)\n𝑇∑︁\n𝑡=1\n∇𝜃log 𝜋𝜃(𝑠𝑡,𝑎𝑡)\n#\n.\n(9)\nIn the above equation, 𝑑𝜋𝜃is the distribution of trajectories generated by policy 𝜋𝜃.\nThe derivation involves the substitution:\n𝜋𝜃(𝜏) = 𝑝(𝑠1)\n𝑇\nÖ\n𝑡=1\n𝜋𝜃(𝑠𝑡,𝑎𝑡)𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡)\n(10)\nHere, 𝑝(·) is independent of the policy parameter 𝜃, and for simplicity, it’s not explicitly included\nin the derivation.\nPrior policy gradient algorithms, like REINFORCE, have often used Monte-Carlo sampling to\nestimate 𝜏from 𝑑𝜋𝜃.\nActor-critic networks bring together the strengths of both Q-learning and policy gradient\ntechniques. They can function as either on-policy methods [26] or off-policy methods [12]. An\nactor-critic network is composed of two key components:\n• The actor: This component optimizes the policy 𝜋𝜃based on the guidance provided by\n∇𝜃𝐽(𝜋𝜃).\n• The critic: The critic evaluates the learned policy 𝜋𝜃using the Q-value function 𝑄𝜃𝑞(𝑠,𝑎).\nThe overall gradient expression for an actor-critic network is as follows:\nE𝑠∼𝑑𝜋𝜃[𝑄𝜃𝑞(𝑠,𝑎)∇𝜃log 𝜋𝜃(𝑠,𝑎)].\n(11)\nIn the case of off-policy learning, the value function for 𝜋𝜃(𝑎|𝑠) can be further defined using\ndeterministic policy gradient (DPG):\nE𝑠∼𝑑𝜋𝜃[∇𝑎𝑄𝜃𝑞(𝑠,𝑎)|𝑎= 𝜋𝜃(𝑠)∇𝜃𝜋𝜃(𝑠,𝑎)].\n(12)\nIt’s worth noting that while traditional policy gradient calculations involve integrating over both\nthe state space S and the action space A, DPG only requires integrating over the state space S. In\nDPG, given a state 𝑠∈S, there corresponds only one action 𝑎∈A such that 𝜇𝜃(𝑠) = 𝑎.\nModel-based RL is a broad term encompassing methods that rely on explicit estimates of the\ntransition or dynamics function P. The distinguishing feature of model-based RL is that it assumes\nthe dynamics model P is known and can be learned. This is in contrast to other forms of RL where\nsuch a dynamics model is neither known nor learnable.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:8\nChen et al.\n2.4\nOffline RL\nThe offline RL problem can be defined as a data-driven formulation of the RL problem [27]. The\nultimate objective remains centered on optimizing the goal presented in Equation (2). Notably, the\nagent’s capacity to engage with the environment and amass supplementary transitions using the\nbehavior policy is nullified. Instead, the learning algorithm receives a fixed collection of transitions\ndenoted as D = {𝑠𝑖\n𝑡,𝑎𝑖\n𝑡,𝑠𝑖\n𝑡+1,𝑟𝑖\n𝑡}, and its task is to acquire the most optimal policy using this provided\ndataset. This approach aligns more closely with the supervised learning paradigm, and we can view\nD as the training dataset for the policy.\nFundamentally, offline RL necessitates that the learning algorithm comprehends the underlying\ndynamics of the MDP M solely from a fixed dataset. Subsequently, it must create a policy 𝜋(𝑎|𝑠)\nthat achieves the highest cumulative reward during the interaction with the MDP. We will denote\nthe distribution over states and actions in D as 𝜋𝛽(also referred to as the behavior policy). Here,\nwe assume that state-action pairs (𝑠,𝑎) ∈D are drawn from 𝑠∼𝑑𝜋𝛽(𝑠), and actions are sampled\naccording to the behavior policy, i.e., 𝑎∼𝜋𝛽(𝑎|𝑠).\nThis problem formulation has been expressed using a range of terminologies. Within the field\nof RS, the term that frequently induces confusion is “off-policy RL”. This phrase is commonly\nemployed as a broad label encompassing all RL algorithms that can leverage datasets of transitions\nD, wherein the actions in each transition were acquired using policies distinct from the current\npolicy 𝜋(𝑎|𝑠). However, it’s important to note that the term “off-policy” typically signifies an RL\nalgorithm where the behavior policy 𝜋𝛽differs from the target policy 𝜋, as discussed earlier. This\ndistinction can sometimes cause confusion. Hence, the terms “fully off-policy RL” or “offline RL”\nare recently introduced to indicate situations where no additional online data collection takes place.\nWe have presented various illustrations of distinct RL approaches to emphasize the disparities\nbetween them in Figure 2.\n(c) offline RL4RS\n(a) on-policy RL4RS\n(b) off-policy RL4RS\n…\nState st \nPolicy πt\n…\nAction at \nUser u\nReward rt \nPolicy πt+1\nUpdate\n rollout data {(st , at , st+1 , rt )}  \nπt+1\nReward\nimplies\nBuffer\n…\nState st \nPolicy πt\n…\nAction at \nUser u\nReward rt \nPolicy πt+1\nUpdate\n rollout data {(st , at , st+1 , rt )}  \nπt+1\nReward\nimplies\n…\nState st \nPolicy πβ\n…\nAction at \nUser u\nReward rt \n rollout data {(st , at , st+1 , rt )}  \nReward\nimplies\nData Collection: data collected once with any policy\nOffline Dataset\nPolicy π\nLearn\nTraining phase\n…\nState st \nPolicy π\n…\nAction at \nUser u\nReward rt \nReward\nimplies\nDeployment/Fine-tune\nFig. 2. Illustration of classic on-policy RL (a), classic off-policy RL (b), and offline RL (c). Where (a) and (b)\ncan also be recognized as online RL.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:9\nThe challenge of offline RL can be tackled through algorithms belonging to any of the four main\ncategories in RL: Q-learning, policy gradient, actor-critic, and model-based RL. In principle, any off-\npolicy RL algorithm could function as an offline RL approach when the online interaction process\nis excluded. For instance, a straightforward offline RL technique can be created by employing\nQ-learning without requiring supplementary online exploration. This method utilizes the dataset D\nto pre-fill the data buffer.\n2.5\nOffline RL4RS - Problem Formulation\nIn this section, we establish a problem formulation for Offline RL4RS. We begin with a standard MDP\nframework, commonly used in RS. The setup involves a set of users denoted as U = 𝑢,𝑢1,𝑢2,𝑢3, ...\nand a set of items denoted as I = 𝑖,𝑖1,𝑖2,𝑖3, .... The process begins with the system recommending\nitem 𝑖to user 𝑢and receiving feedback 𝑓𝑢\n𝑖. This feedback is then utilized to enhance future\nrecommendations, leading to the identification of an optimal policy 𝜋∗that guides the selection of\nitems to recommend in order to achieve positive feedback.\nThe MDP framework treats the user as the environment while the system acts as the agent. The\nfundamental components within the MDP context, especially in Deep Reinforcement Learning\n(DRL)-based RS, include:\n• State S: At a given time 𝑡, the state 𝑠𝑡∈S is defined by a combination of the user’s charac-\nteristics and the recent 𝑙items that the user has shown interest in prior to time 𝑡. This may\nalso include demographic information if relevant.\n• Action A: The action 𝑎𝑡∈A represents the agent’s prediction of the user’s evolving\npreferences at time 𝑡. Here, A encompasses the entire set of potential candidate items, which\ncould be vast, potentially numbering in the millions.\n• Transition Probability P: The transition probability 𝑝(𝑠𝑡+1|𝑠𝑡,𝑎𝑡) quantifies the likelihood\nof transitioning from state 𝑠𝑡to 𝑠𝑡+1 when the agent performs action 𝑎𝑡. In the context of a\nrecommender system, this probability corresponds to the likelihood of user behavior changes.\n• Reward function R: After the agent selects an appropriate action 𝑎𝑡based on the current\nstate 𝑠𝑡at time 𝑡, the user receives the item recommended by the agent. The feedback from\nthe user regarding the recommended item contributes to the reward 𝑟𝑡= R(𝑠𝑡,𝑎𝑡). This\nreward reflects the user’s response and guides the enhancement of the learned policy 𝜋by\nthe recommendation agent.\n• Discount Factor 𝛾: The discount factor 𝛾∈[0, 1] is employed to balance the consideration of\nfuture and immediate rewards. A value of 𝛾= 0 indicates the agent prioritizes immediate\nrewards, while a non-zero value implies a blend of both immediate and future rewards.\n• Offline Dataset D: The offline dataset D is amassed by an unknown behavior policy 𝜋𝛽. This\ndataset serves as the historical records of user interactions and is utilized to improve the\nrecommendation policy.\nThis MDP-based framework lays the groundwork for Offline RL4RS, where the aim is to devise\neffective recommendation policies using historical interaction data, even when the data is collected\nunder an unknown or different behavior policy. If a POMDP is used, we just need to add the\nobservation O which is the partial information from users and 𝑙items in which the user was\ninterested before time 𝑡.\nDefinition 3. Given an offline dataset D, which contains the trajectories when user 𝑢∈U\ninteracts with the system for a certain period with an unknown behavior policy 𝜋𝛽, the RL agent aims\nto learn a policy 𝜋from the offline dataset D. After that, the trained policy 𝜋will be deployed/tested\non a production or evaluation environment with a similar scenario with the collected dataset D.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:10\nChen et al.\n3\nCURRENT PROGRESS IN OFFLINE RL4RS\nIn this section, we survey offline RL-based RS. Generally speaking, it can be divided into two\ncategories: off-policy with logged data (i.e., “full” off-policy) and offline RL. These two concepts are\ngenerally the same except for some specific settings in off-policy methods such as assuming bandit\nconditions. Due to the recent introduction of offline RL, we have opted to distinguish and separate\nthese for clarity and to prevent potential confusion.\n3.1\nOff-policy with Logged Data\n3.1.1\nOff-policy Evaluation. The typical method in this domain is known as off-policy evaluation.\nOff-policy evaluation methods are rooted in the direct estimation of policy returns. These methods\noften utilize a technique known as importance sampling, which involves estimating the return of a\ngiven policy or approximating the corresponding policy gradient. A straightforward application\nof importance sampling involves using trajectories sampled from 𝜋𝛽(𝜏) to derive an unbiased\nestimator of 𝐽(𝜋):\n𝐽(𝜋𝜃) = E𝜏∼𝜋𝛽(𝜏)\n\u0014𝜋𝜃(𝜏)\n𝜋𝛽(𝜏)\n𝑇∑︁\n𝑡=0\n𝛾𝑡R(𝑠,𝑎)\n\u0015\n= E𝜏∼𝜋𝛽(𝜏)\n\u0014 𝑇\nÖ\n𝑡=0\n𝜋𝜃(𝑎𝑠|𝑠𝑡)\n𝜋𝛽(𝑠𝑡|𝑎𝑡)\n𝑇∑︁\n𝑡=0\n𝛾𝑡R(𝑠,𝑎)\n\u0015\n(13)\n≈\n𝑛\n∑︁\n𝑖=1\n𝑤𝑖\n𝑇\n𝑇∑︁\n𝑡=0\n𝛾𝑡𝑟𝑖\n𝑡.\n(14)\nHowever, this estimator often exhibits high variance, particularly if𝑇(the time horizon) is large, due\nto the product of importance weights. To address this, a weighted importance sampling estimator\ncan be used. This involves dividing the weights by Í𝑛\n𝑖=1 𝑤𝑖\n𝑇to normalize them, resulting in a biased\nestimator with significantly lower variance, while still maintaining strong consistency.\nWhen considering the estimation of Q-values for each state-action pair (𝑠𝑡,𝑎𝑡), denoted as\nˆ𝑄𝜋(𝑠𝑡,𝑎𝑡), an approximate model comes into play. This model could be derived from estimating\nthe transition probability P(𝑠𝑡+1|𝑠𝑡,𝑎𝑡) of the Markov Decision Process (MDP) and subsequently\nsolving for the corresponding Q-function. Alternatively, other methods for approximating Q-values\ncould be employed.\nThe integration of these approximated Q-values as control variates within the importance\nsampled estimator leads to an enhanced approach:\n𝐽(𝜋𝜃) =\n𝑛\n∑︁\n𝑖=1\n𝑇∑︁\n𝑡=0\n𝛾𝑡\u0010\n𝑤𝑖\n𝑡(𝑟𝑖\n𝑡−ˆ𝑄𝜋(𝑠𝑡,𝑎𝑡)) −𝑤𝑖\n𝑡−1E𝑎∼𝜋𝜃(𝑎|𝑠𝑡)[ ˆ𝑄𝜋(𝑠𝑡,𝑎)]\n\u0011\n.\n(15)\nThis method, referred to as a doubly robust estimator [25] , exhibits unbiasedness either when\n𝜋𝛽is known or when the model is accurate. In essence, it leverages both the unbiasedness of the\nimportance sampling method and the approximated Q-values to produce an estimator with lower\nvariance and strong consistency.\n3.1.2\nRecent works. The recent advancements in off-policy using logged data method can be\nbroadly categorized into three distinct domains: estimator improvement (focus on the discrepancy\nbetween the offline data and online data), algorithmic improvement (focus on the recommendation\nalgorithm itself), and miscellaneous application domains. We have compiled a summary of these\nworks in Figure 3.\nEstimator Improvement Hoiles and Schaar [18] focus on the problem of student course sched-\nuling and curriculum design. It proposes an algorithm for personalized course recommendation\nand curriculum design based on logged student data. The algorithm uses a regression estimator\nfor contextual multi-armed bandits and provides guarantees on their predictive performance. The\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:11\nOff-policy \nEvaluation\nAlgorithmic \nImprovement\nEstimator \nImprovement\nOthers\nHoiles and \nSchaar \n[17]\nJeunen and \nGoethals \n[20, 21]\nNarita et al.\n[38]\nSwaminathan \net al.\n[38]\nJagerman \net al. \n[19]\nChen\net al.\n[5]\nJeunen \net al. \n[22]\nWang\net al. \n[45]\nHong \net al.\n[18]\nSakhi\n et al.\n[40]\nXiao and \nWang\n[54]\nFig. 3. Off-Policy Evaluation Works Classifications\npaper also addresses the issue of missing data and provides guidelines for including expert domain\nknowledge in the recommendations. The algorithms can be used to identify curriculum gaps and\nprovide recommendations for course schedules. The paper also discusses off-policy evaluation\ntechniques and the use of the regression estimator for estimating the expected reward of a new\npolicy. One drawback is that the proposed approach assumes a fixed set of courses and does not\nconsider the dynamic nature of course offerings and student preferences.\nSwaminathan et al. [45] address the problem of off-policy evaluation and optimization in combi-\nnatorial contextual bandits. The motivation behind this research is the need to estimate the reward\nof a new target policy using data collected by a different logging policy. The authors propose a\npseudoinverse (PI) estimator that makes a linearity assumption about the evaluated metric, allow-\ning for more efficient estimation compared to importance sampling. The PI estimator requires\nexponentially fewer samples to achieve a given error bound and can be used for off-policy opti-\nmization as well. The methodology involves using the PI estimator to impute action-level rewards\nfor each context, enabling direct optimization of whole-page metrics through pointwise learning\nto rank algorithms. The authors demonstrate the effectiveness of their approach on real-world\nsearch ranking datasets, showing that the PI estimator outperforms prior baselines in terms of\noff-policy evaluation of whole-page metrics. This method has several limitations. One drawback of\nthis method is that it relies on the linearity assumption, which may not always hold in practice.\nMoreover, there is a bias-variance tradeoff between the weighted pseudoinverse (wPI) method and\nthe direct method, with wPI showing bias for the Expected Reciprocal Rank metric. The wPI method\nalso deteriorates for larger slate spaces and is sensitive to linearity assumptions. These drawbacks\nhighlight areas where further refinement and research are needed to enhance the robustness and\neffectiveness of the approach.\nJeunen and Goethals [21, 22] focus on improving the recommendation performance of policies\nthat rely on value-based models (i.e., Q-learning) of expected reward. The authors propose a\npessimistic reward modeling framework that incorporates Bayesian uncertainty estimates to express\nskepticism about the reward model. This allows for the generation of conservative decision rules\nbased on lower-confidence-bound estimates, rather than the usual maximum likelihood or maximum\nPI estimates. The approach is agnostic to the logging policy and does not require propensity scores,\nmaking it more flexible and avoiding the limitations of inverse propensity score weighting. The\nmethodology involves training reward models using a range of datasets generated under different\nenvironmental conditions. The authors compare the performance of policies that act based on reward\nmodels using maximum likelihood or maximum PI estimates, with policies that use lower confidence\nbounds based on tuned parameters. The evaluation is done through simulated A/B tests, with the\nresulting click-through-rate (CTR) estimates compared to the logging policy and an unattainable\nskyline policy. The experiments show that the pessimistic decision-making approach consistently\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:12\nChen et al.\ndecreases post-decision disappointment and can significantly increase the policy’s attained CTR.\nOne drawback of this approach is that it relies on the assumption that the reward estimates are\nconditionally unbiased, which may not always hold in practice. The authors acknowledge that\nunderfitting and model misspecification can make this assumption unrealistic. Additionally, the\napproach requires tuning the hyperparameter alpha, which determines the lower confidence bound,\nand finding the optimal value may not always be straightforward.\nNarita et al. [39] proposes a new off-policy evaluation method for RL4RS. The motivation behind\nthis work is to address the limitations of existing estimators, such as inverse propensity weighting\nand doubly robust estimators, which suffer from bias and overfitting issues. The authors introduce\na new estimator that combines the doubly robust estimator with double/debiased machine learning\ntechniques. The key features of this estimator are its robustness to bias in behavior policy and\nstate-action value function estimates, as well as the use of a sample-splitting procedure called\ncross-fitting to remove overfitting bias. However, the experiments are limited to specific domains,\nsuch as the CartPole-v0 environment and online ads, and it is unclear how the estimator would\nperform in other tasks in RS.\nJagerman et al. [20] address the problem of off-policy evaluation in non-stationary environments,\nwhere user preferences change over time. Existing off-policy evaluation techniques fail to work in\nsuch environments. It proposes several off-policy estimators that operate well in non-stationary\nenvironments. These estimators rely more on recent bandit feedback and accurately capture changes\nin user preferences. They provide a rigorous analysis of the proposed estimators’ bias and show that\nthe bias does not grow over time, unlike the standard Inverse Propensity Scoring (IPS) estimator.\nThey also create adaptive variants of the estimators that change their parameters in real-time to\nimprove estimation performance. Extensive empirical evaluation on recommendation datasets\nshows that the proposed estimators significantly outperform the regular IPS estimator and provide a\nmore accurate estimation of a policy’s true performance. One drawback of the work is the trade-off\nbetween bias and variance. While the estimators avoid a bias term that grows with time, they\nintroduce variance that scales with the window size or decay factor. Choosing a smaller window\nsize or larger decay factor reduces bias but increases variance, and vice versa. Finding the optimal\nbalance between bias and variance is a challenge.\nAlgorithmic Improvement Wang et al. [46] address the problem of designing a stable off-\npolicy RL method for RS. Moreover, the exploration error is also highlighted, which arises from\nthe mismatch between the recommendation policy and the distribution of customers’ feedback in\nthe training data. This exploration error can lead to unstable training processes and potentially\ndiverging results. To mitigate this problem, the authors propose an off-policy logged data method\ncalled Generator Constrained deep Q-learning (GCQ). GCQ combines a neural generator that\nsimulates customers’ possible feedback with a Q-network that selects the highest valued action\nto form the recommendation policy. The authors also design the generator’s architecture based\non Huffman Trees to reduce decision time. One drawback of this work is the limited capability to\nhandle long sequences of user behavior.\nChen et al. [5] address the problem of data biases that arise when applying policy gradient\nmethods in a recommendation system. The primary goal is to address the distribution mismatch\nfrom the behavior policy 𝜋𝛽and the learned policy 𝜋. As a result, an off-policy-corrected gradient\nestimator is introduced to reduce the variance of each gradient term while still correcting for the\nbias of a non-corrected policy gradient. A recurrent neural network (RNN) is adopted to model\nthe user state at each time step. To estimate the behavior policy 𝜋𝛽, which is a mixture of the\npolicies of multiple agents in the system, the authors use a context-dependent neural estimator\nwhich is a contextual bandit based method. One drawback of the proposed method is the variance\nof the estimator, which can be large when there are very low or high values of the importance\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:13\nweights. To reduce this variance, the authors take a first-order approximation and ignore the state\nvisitation differences under the two policies. This results in a slightly biased estimator with a lower\nvariance. Another drawback is the difficulty in estimating the behavior policy 𝜋𝑏𝑒𝑡𝑎, especially\nwhen there are multiple agents in the system and the collected trajectories are generated by a\nmixture of deterministic policies and stochastic policies.\nJeunen et al. [23] propose a new approach called the Dual Bandit, which combines value-based\nand policy-based methods to improve performance in recommendation settings. It highlights that\nexisting offline evaluation results are often even contradictory over different runs and datasets, or\nextremely hard to reproduce in a robust manner. Hence, they introduce simulation environments\nas an alternative and reproducible evaluation approach.\nOthers Sakhi et al. [41] introduce a probabilistic model known as BLOB (Bayesian Latent Organic\nBandit) designed for bandit-based RS. BLOB aims to enhance recommendation quality by combining\norganic user behavior (items viewed without intervention) with bandit signals (recommendations\nand their outcomes). Traditional recommendation algorithms often focus on either organic-based\nor bandit-based approaches, but the authors recognize the potential to enhance recommendation\nquality by integrating both aspects. The goal is to create a model that leverages the relationship\nbetween organic and bandit behaviors to provide more accurate and personalized recommendations.\nThe proposed model uses a matrix variate prior distribution to relate these two types of behaviors,\nand variational autoencoders are employed for training. However, the proposed model requires a\ntwo-state training process which needs to train the model for organic behavior and bandit signals\nseparately instead of training simultaneously.\nXiao and Wang [55] present a value ranking algorithm that combines RL and ranking metrics\nto improve the effectiveness of ranking algorithms. The proposed method uses the concept of\nextrapolation and regularization to address the challenges of partial and sparse rewards. Extrapola-\ntion is used to estimate rewards from logged feedback, while regularization is used to incorporate\nranking signals into the RL policy. The authors propose a sequential Expectation-Maximization\n(EM) framework that alternates between the E-step, which estimates rewards and ranking signals,\nand the M-step, which optimizes the RL policy. They show that this framework can effectively\nlearn from rewards and ranking signals. This proposed algorithm’s drawback lies in the bandit\nsetting, as it doesn’t account for future rewards. Additionally, in the full RL setting, it might suffer\nfrom the curse of dimensionality.\nHong et al. [19] address the complex issue of multi-task off-policy learning from bandit feedback,\na challenge that has significant implications for various applications, including RS. It is motivated\nto develop a solution that can efficiently handle multiple tasks simultaneously, leveraging the rela-\ntionships between tasks to enhance performance. It proposes a hierarchical off-policy optimization\nalgorithm (HierOPO) to tackle this problem. The problem is formulated as a contextual off-policy\noptimization within a hierarchical graphical model, specifically focusing on linear Gaussian mod-\nels. The authors provide an efficient implementation and analysis, proving per-task bounds on\nthe sub-optimality of the learned policies. They demonstrate that using the hierarchy improves\nperformance compared to solving each task independently. The algorithm is evaluated on synthetic\nproblems and applied to a multi-user recommendation system. However, the proposed method is a\nmodel-based off-policy approach, the model-based approaches tend to be biased, due to using a\npotentially misspecified model.\n3.2\nOffline RL4RS\nIn this section, we will provide reviews of existing offline RL4RS methods. Different from off-policy\nevaluation, offline RL4RS does not limit the setting to bandit-based methods. Moreover, in this part,\nwe have included the off-policy learning based methods as offline RL. However, the existing works\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:14\nChen et al.\nin this field lack organization, with no apparent interconnection among the various works that\noften emphasize different aspects. Currently, we lack a systematic approach to review these works,\nresorting to a sequential examination of each one individually.\nMa et al. [33] discuss off-policy learning in two-stage RS. The proposed method consists of a\ncandidate generation model in the first stage and a ranking model in the second stage. The authors\npropose a two-stage off-policy policy gradient method that takes into account the ranking model\nwhen training the candidate generation model. The proposed method employs IPS to correct the\nbias and design variance reduction tricks to reduce the variance. However, the proposed method\ndoes not provide a comprehensive experiment about how the ranking model and the candidate\ngeneration model affect the final performance.\nChen et al. [6] focus on scaling an off-policy actor-critic algorithm for industrial recommendation\nsystems. The motivation behind their research is to address the challenges of offline evaluation\nand learning in RS, where only partial feedback is available. The authors propose an approach\nthat combines off-policy learning with importance weighting to estimate the value of state-action\npairs under the target policy. They use a critic network to estimate the value function and update\nthe policy network accordingly. The methodology involves minimizing the temporal difference\nloss and using a Huber loss to handle outliers. The authors also investigate the impact of different\nestimation methods for the target value function. However, the proposed methods have several\nlimitations. One drawback is the potential bias introduced by using the cumulative future return on\nthe behavior trajectory while ignoring the importance weighting on future trajectories. Another\ndrawback is the conservative nature of the learned policy when using sampling from the learned\npolicy. The softmax policy parameterization used in the approach leads to a more myopic policy,\nrecommending more popular and longer content and less novel content.\nGao et al. [15] centre around the problem of the Matthew effect in offline RL based RS. The\nMatthew effect [36] describes a phenomenon where popular items or categories are recommended\nmore frequently, leading to the neglect of less popular ones. This bias towards popular items can\nreduce the diversity in recommendations and decrease user satisfaction. To address the Matthew\neffect, the authors propose a Debiased model-based Offline RL (DORL) method. DORL introduces a\npenalty term to the RL algorithm, encouraging exploration and diversity in recommendations. By\nadding this penalty, the method aims to reduce the bias towards popular items and promote a more\nvaried selection.\nWang et al. [47] address the challenges inherent in designing reward functions and handling\nlarge-scale datasets within RL4RS. Traditional RL4RS approaches may fall short in accurately\nestimating rewards only based on limited observations. To address this problem, a Causal Decision\nTransformer for RS (CDT4Rec) is proposed, a novel model that integrates offline RL and transformer\narchitecture. CDT4Rec employs a causal mechanism to estimate rewards based on user behavior,\nallowing for a more accurate understanding of user preferences. The transformer architecture is\nused to process large datasets and capture dependencies, enabling the model to handle complex\ndata structures.\nYuan et al. [57] is motivated by the challenges associated with optimizing mobile notification\nsystems. Traditional response-prediction models often struggle to accurately attribute the impact to\na single notification, leading to inefficiencies in managing and delivering notifications. Recognizing\nthis limitation, the authors aim to explore the application of RL to enhance the decision-making\nprocess for sequential notifications, seeking to provide a more effective and targeted approach to\nmobile notification systems. Hence, an offline RL framework specifically designed for sequential\nnotification decisions is proposed. They introduce a state-marginalized importance sampling policy\nevaluation approach, which is a novel method to assess the effectiveness of different notification\nstrategies. Through simulations, the authors demonstrate the performance of the approach, and\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:15\nthey also present a real-world application of the framework, detailing the practical considerations\nand results.\nWang et al. [50] are motivated by the challenge of adapting to new users in recommendation\nsystems, particularly when there are limited interactions to understand user preferences. This\nsituation, often referred to as the “cold-start” problem, can hinder the ability to provide personalized\nrecommendations that align with long-term user interests. The proposed approach introduces a\nuser context variable to represent user preferences, employing a meta-level model-based RL method\nfor rapid user adaptation. The user model and recommendation agent interact alternately, with the\ninteraction relationship modeled from an information-theoretic perspective.\nZhang et al. [59] discuss the problem of interactive recommendation with natural-language\nfeedback and proposes an offline RL framework to address the challenges of collecting experience\nthrough user interaction. The authors develop a behavior-agnostic off-policy correction framework\nthat leverages the conservative Q-function for off-policy evaluation. This allows for learning\neffective policies from fixed datasets without further interactions.\nXiao and Wang [54] propose a general offline RL framework for the interactive recommendation.\nThe proposed method introduces different techniques such as support constraints, supervised\nregularization, policy constraints, dual constraints, and reward extrapolation. These methods aim\nto minimize the mismatch between the recommendation policy and logging policy and to balance\nthe supervised signal and task reward.\n4\nCHALLENGES AND OPPORTUNITIES\nOffline RL4RS is an emerging domain that introduces multiple challenges demanding comprehensive\nexploration. In this section, we aim to outline the open challenges in offline RL4RS. Given that RS\nfall under the application scope of offline RL, several shared challenges naturally arise. We will\nbegin by addressing some common challenges before delving into the specific challenges unique to\nRS when utilizing offline RL techniques.\n4.1\nHigh-quality Offline Data and Cold-Start Problems\nOne of the most prominent challenges in offline Reinforcement Learning (RL) lies in the fact that\nthe learning process hinges solely on the provided static dataset D. This limitation results in a\nsignificant obstacle to enhancing exploration, as exploration falls outside the algorithm’s purview.\nConsequently, if the dataset D lacks transitions that demonstrate regions of the state space yielding\nhigh rewards, the algorithm may be fundamentally incapable of uncovering these rewarding regions.\nIn contrast to control tasks, which are common in offline RL applications and often face challenges\nin gathering comprehensive data to facilitate effective learning from high-reward scenarios, the\nlandscape changes when it comes to RS. In this domain, a plethora of offline datasets, such as\nthose from MovieLens, GoodReads, and Amazon, are publicly available. These datasets stem from\nreal-world interactions and adeptly capture users’ preferences.\nHowever, RS diverge from traditional offline RL application domains due to their distinct charac-\nteristics. To illustrate, let’s consider implicit feedback, particularly review data. This kind of data\nposes a challenge when attempting to embed it within the state space due to its reliance on text.\nAlthough techniques like word2vec [37] exist to transform textual data into vectors that might\npotentially be integrated into the state space, the question of how to effectively guide the agent in\nutilizing such data in RS remains unexplored.\nAnother intriguing aspect is the presence of graph data, extensively used in RS to represent\nsocial connections, item relationships, and more. The prevalent form of representation is a knowl-\nedge graph, which can be transformed into embeddings through the application of Graph Neural\nNetworks (GNN) [53]. Nonetheless, it faces a similar challenge as textual data: how to empower the\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:16\nChen et al.\nagent to effectively utilize this information. There are some works investigating graph RL which\nmay be able to provide some directions to offline RL4RS [24, 34, 56].\nHowever, a challenge surfaces due to what’s known as the “data sparsity problem”. This means\nthat despite having ample data, there’s no assurance that the collected user interactions or behaviors\ncover all the situations where users have expressed positive feedback, like giving high ratings. In\nother words, there might be important scenarios where users found something valuable, but the\ndata doesn’t reflect those instances well [10].\nOn the other hand, there is s another widely recognized hurdle in RS that also applies to Offline\nRL4RS: the cold-start problem. Unlike data sparsity, cold-start challenges emerge when the agent\naims to provide recommendations to a new user. This issue arises due to the absence of adequate\nhistorical data or interactions, which in turn hampers the understanding of preferences and traits\nrelated to these new users or items. While addressing the cold-start problem is an ongoing research\navenue in conventional RS tasks, it hasn’t received sufficient attention in the context of RL4RS.\nConsidering the interactive procedure of the RL4RS, new users have limited contextual information\nthat they can use to formulate the state representation; this contributes to the difficulty of making\nrecommendations. This predicament continues to remain an unsolved puzzle within the realm of\noffline RL4RS.\n4.2\nDistribution Shift\nA challenge of significant intricacy within the context of offline RL pertains to the effective formu-\nlation and addressing of counterfactual queries—a task that might not be readily apparent but is of\ngreat importance.\nCounterfactual queries, in essence, are defined as hypothetical “what if” scenarios. These queries\ninvolve creating educated guesses about potential outcomes if the agent were to undertake actions\ndifferent from those observed in the data. It is the core behind offline RL, as our objective is to\nlearn a policy that can perform better than the behavior recorded in the dataset D. Hence, the\nagent must execute an action that is different from the learned policy. This situation, unfortunately,\nplaces a substantial strain on the capabilities of several prevailing deep-learning methods. Existing\nmethods have been methodically fashioned under the assumption that the data is independence\nand identical distribution (i.i.d.). In traditional supervised learning based RS, the goal is to train a\nmodel to achieve superior performance, such as higher accuracy, recall or precision. The evaluation\ndataset follows the same distribution as the training dataset. Hence, in offline RL4RS, the key point\nis to learn a policy that can recommend different items (ideally with better feedback) from the\nbehavior recorded in the dataset D.\nThe challenge behind counterfactual queries is that of distribution shift. The policy is trained\nunder one distribution, but it will be evaluated on a different distribution. Given that such a problem\nis not widely discussed in the RS literature, we will provide some algorithmic insights from the\noffline RL perspective to help address this in offline RL4RS. Distribution shift issues can be addressed\nin several ways, with the simplest one being to constrain something about the learning process\nsuch that the distribution shift is bounded. For example, we can constrain how much the learned\npolicy 𝜋(𝑎|𝑠) differs from behavior policy 𝜋𝛽(𝑎|𝑠) by using some techniques like Trust Region\nPolicy Optimization (TRPO) [42].\nHowever, if there is a significant disparity between the distribution of the training dataset\nand that of the evaluation environment, it might lead to the emergence of out-of-distribution\n(OOD) behavior. Several recent studies have delved into OOD recommendation [16, 48], taking\ninto account shifts in user features. These efforts can be categorized into two main groups: OOD\ngeneralization [16] and OOD adaptation [48].\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:17\nThe underlying notion here is to acquire a causal representation of users’ preferences by leverag-\ning their most recent behaviors. This representation is then utilized within a causal graph framework\nto comprehend how shifts in features could impact users’ preferences. Furthermore, the current\nmethodologies primarily target sequential recommendation systems, which share certain properties\nwith MDPs, rendering them relevant to offline RL4RS.\nHowever, this domain is still in its exploratory phase, and it has not garnered substantial attention.\nAs a result, this presents an open challenge with significant potential for further exploration.\n4.3\nBias and Variance Trade-off\nAnother prevalent issue within offline RL4RS pertains to the bias inherited from RS, a topic that has\nrecently gained increasing research attention. This bias stems from the nature of offline data, with\nrecent studies [4] revealing that user behavior data are not experimental but rather observational,\nintroducing bias-related challenges.\nThe prevalence of bias can be attributed to two primary factors. Firstly, the inherent character of\nuser behavior data is observational rather than experimental. In simpler terms, the data fed into\nRS are susceptible to selection bias. For instance, in a video recommendation system, users tend\nto engage with, rate, and comment on movies that align with their personal interests. Secondly, a\ndiscrepancy in distribution exists, signifying that the distributions of users and items within the\nrecommender system are uneven. This imbalance can lead to a “popularity bias”, where popular\nitems receive disproportionately frequent recommendations compared to others. Nonetheless,\ndisregarding products within the \"long tail\" of less popular items can have adverse effects on\nbusinesses, given that these items are equally essential, albeit less likely to be discovered by chance.\nAs mentioned earlier, a substantial portion of existing offline off-policy with logged data methods\nprimarily focus on off-policy evaluation. This approach employs importance sampling to tackle the\nbias issue. However, the importance sampling gives rise to another hurdle—high variance. While\nimportance sampling already contends with high variance, this issue is further exacerbated in the\ncontext of sequential scenarios. In this setting, the importance weights at consecutive time steps\nare multiplied together (as depicted in Equation 14), leading to an exponential amplification of\nvariance.\nApproximate and marginalized importance sampling methods mitigate this concern to some\nextent by circumventing the multiplication of importance weights across multiple time steps. Yet,\nthe fundamental challenge persists: when the behavior policy 𝜋𝛽substantially diverges from the\ncurrent learned policy 𝜋𝜃, the importance weights degenerate. Consequently, any estimations of\nthe return or gradient encounter excessive variance, particularly in scenarios characterized by\nhigh-dimensional state and action spaces or extended time horizons (as seen in problems like\nrecommendation systems).\nFor this reason, importance-sampled estimators are most effective when the policy’s deviation\nfrom the behavior policy remains within a reasonable limit. In the general off-policy setting, this\ncondition generally holds true, as new trajectories are frequently amassed and integrated into\nthe dataset using the latest policy. However, in the offline context, this is not typically the case.\nConsequently, the extent of enhancement achievable through importance sampling is confined by\nseveral factors: (i) the relative suboptimality of the behavior policy; (ii) the dimensionality of the\nstate and action space; (iii) the effective task horizon.\nHence, the tradeoff between bias and variance in offline RL4RS presents an intriguing potential\navenue for advancement.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:18\nChen et al.\n4.4\nExplainability\nWhile deep learning-based models can significantly enhance the performance of RS, they often\nlack interpretability. Consequently, the task of rendering recommender outputs understandable\nbecomes vital, all while maintaining high-quality recommendations. Elevating explainability in RS\ncarries benefits beyond aiding end-users in comprehending suggested items. It empowers system\ndesigners to scrutinize the inner workings of RS [63]. Additionally, the realm of explainability in\nRL (RL) has been garnering attention [17], although the current focus primarily revolves around\nvisualizing learned representations. What remains is an explanation of how the learned policy\ntranslates into actionable decisions. In the transition to RL4RS, the emphasis on explainability will\nshift towards elucidating how the agent justifies its recommended items. Hence, explainability\nbecomes a relatively easy task compared with interpreting the learning process or decision process.\nAttention models have emerged as powerful tools that not only bolster predictive performance but\nalso enhance explainability [61]. For instance,Wang et al. [49] introduce an RL framework coupled\nwith an attention model for explainable recommendations. This approach ensures model-agnostic\nby segregating the recommendation model from the explanation generator. Agents instantiated\nthrough attention-based neural networks facilitate the generation of sentence-level explanations.\nThis approach could prove promising given the close connection between offline RL4RS and online\nRL4RS.\nMoreover, with access to offline datasets in offline RL4RS, more solutions become feasible.\nKnowledge graphs, for instance, contain abundant user and item information, enabling the creation\nof more personalized, intuitive explanations for recommendation systems [63]. However, the\nprocessing of graph data presents challenges. One potential strategy involves embedding a pre-\nlearned knowledge graph from the offline dataset into the environment. The final objective then\nshifts from recommending items to navigating the knowledge graph. As an example, Zhao et al.\n[64] extract informative path demonstrations with minimal labeling effort. Then an adversarial\nactor-critic model for demonstration-guided pathfinding is proposed. This approach enhances\nrecommendation accuracy and explainability through RL and knowledge graph reasoning and can\nbe further expanded by integrating offline RL features.\n5\nFUTURE DIRECTIONS\nIn offline RL4RS, several key areas emerge as promising avenues. Cross-domain recommendation\nsystems offer the potential in transferring insights between diverse domains, enhancing recom-\nmendation effectiveness. The integration of large language models holds the prospect of enriching\ncontextual understanding and refining user-item interactions. Incorporating causality into offline\nRL4RS can deepen comprehension of user behaviors, leading to more accurate and interpretable\nrecommendations. The exploration of self-supervised learning and graph-based techniques presents\ninnovative possibilities for capturing intricate user-item relationships. Moreover, addressing un-\ncertainty and fortifying the robustness of RL4RS against noise and adversarial inputs stand out as\nessential directions for ensuring dependable and consistent recommendation outcomes.\n5.1\nCross-Domain Recommendation\nCross-domain recommendation refers to the task of providing recommendations to users by lever-\naging data and knowledge from multiple distinct domains. Cross-domain recommendation systems\ncan be particularly useful in scenarios where user data is sparse within a single domain but might\nbe enriched when multiple domains are combined. Additionally, they enable more comprehensive\nand diverse recommendations by tapping into different aspects of users’ interests. From this view-\npoint, we may be able to treat offline RL4RS as a type of cross-domain recommendation in certain\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:19\nsituations. For example, when the evaluation environment is significantly different from the offline\ndataset D, we may treat the evaluation platform as a new domain and we would like to transfer\nthose learned knowledge from D into such a platform.\nThe challenge in cross-domain recommendation lies in effectively transferring knowledge and\npatterns across domains while accounting for variations in user behaviors and item characteristics.\nTechniques such as domain adaptation, transfer learning, and hybrid models are often employed to\nbridge the gaps between different domains and optimize recommendation performance. Moreover,\nrecent work in cross-domain offline RL would be beneficial.Liu et al. [31] present BOSA (Beyond\nOOD State Actions), a method for cross-domain offline RL (RL). BOSA tackles the challenges of\nout-of-distribution (OOD) state actions and data inefficiency by incorporating additional source-\ndomain data. The authors propose specific objectives to address OOD transition dynamics and\ndemonstrate that BOSA improves data efficiency and outperforms existing methods. The method is\nalso applicable to model-based RL and data augmentation techniques. However, in offline RL4RS,\nthis problem is still open for investigation as the techniques mentioned have not yet been explored\nin offline RL4RS.\n5.2\nImplicit Feedback and Large Language Models\nImplicit feedback serves as a commonly employed feedback mechanism for learning recommenda-\ntion policies in RS. Implicit feedback encompasses user actions like clicks, views, purchases, time\nspent, and dwell time during interactions with platforms or systems, signifying user preferences\nand interests. Although not as explicit as ratings or reviews, these behaviors offer valuable insights.\nIn the context of RL4RS, the reward mechanism evaluates recommended items. Typically, this\ninvolves binary rewards based on click behavior, with some efforts, like Zheng et al. [66], incorpo-\nrating dwell times for a more comprehensive reward signal. However, accommodating multiple\nimplicit feedback sources concurrently in RL4RS poses challenges due to limited relevant datasets\nor simulations. Additionally, harnessing review comments, a common type of implicit feedback in\nRS, within RL4RS remains a subject of exploration. Zhang et al. [60] propose a text encoder solution,\nalbeit relying on a manually gathered generator to produce review texts, which primarily validate\nfeature learning rather than directly influencing the final reward. Transitioning this approach to\noffline RL4RS presents difficulties. Firstly, integrating review comments into the reward function\nrequires careful study. Secondly, textual data introduces high-dimensional state representations,\npotentially necessitating novel algorithms tailored to this scenario.\nRecently, Large Language Models (LLMs) have received increasing research interest in RS. LLM\ndemonstrates a superior capability in handling textual data from multiple tasks such as natural\nlanguage understanding, contextual understanding and sentiment analysis [65]. Existing RS works\nprovides some insights about how LLMs can be adopted in RS such as prompt engineering to\ninstruct the LLM to make recommendations [58], utilizing the Generative Pre-trained Transformer\n(GPT) as the backbone to process features [43] etc.\nMoreover, some attempts have been undertaken about how LLM can be used in RL.Du et al. [14]\nintroduce a method called ELLM (Exploring with Large Language Models) that aims to enhance\npretraining in RL by using LLM. ELLM works by prompting an LLM with a description of the\nagent’s current state and then rewarding the agent for achieving goals suggested by the LLM. This\nmethod biases exploration towards behaviors that are meaningful and potentially useful from a\nhuman perspective, without needing human intervention. Meanwhile,Carta et al. [3] explore the\nuse of LLM in interactive environments through an approach called GLAM (Grounding Language\nModels). This method aligns the knowledge of LLMs with the environment, focusing on aspects\nlike sample efficiency, generalization to new tasks, and the impact of online RL.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:20\nChen et al.\n5.3\nCausality\nIn the previous section, we mentioned that offline RL can be formulated as answering counterfactual\nqueries. It is an intuitive choice to integrate the causality into offline RL from this perspective.\nMoreover, causality is widely used in RS and receiving increasing interest in offline RL. We believe\nit would be a promising topic in offline RL4RS.\nIn the work by Zhu et al. [67], an exploration is undertaken regarding the integration of causal\nworld models into the domain of model-based offline RL. The theoretical underpinning of their\nstudy accentuates the superiority of causal world models over ordinary world models in the\ncontext of offline RL. This advantage is attributed to the incorporation of causal structure within\nthe generalization error bound. The authors introduce an operational algorithm termed FOCUS\n(Offline Model-based RL with Causal Structure) to exemplify the potential value derived from\ncomprehending and effectively utilizing causal structure in the domain of offline RL.\nAdditionally, Liao et al. [29] introduce the notion of instrumental variable value iteration for\ncausal offline RL. The presentation of their work introduces IV-aided Value Iteration (IVVI), an\nalgorithm designed with efficiency in mind, aimed at extracting optimal policies from observational\ndata in the presence of unobserved variables. The utilization of instrumental variables (IVs) forms\nthe foundation, with the authors devising a framework named Confounded Markov Decision\nProcess with Instrumental Variables (CMDP-IV) to contextualize the problem. Notably, the IVVI\nalgorithm, established upon a primal-dual reformulation of a conditional moment restriction,\nemerges as the first demonstrably efficient solution for instrument-aided offline RL.\nOne of the most common applications of integrating causality into the RL4RS is counterfactual\naugmentation.Chen et al. [8, 9] develop a data augmentation technique that employs counterfactual\nreasoning to produce more informative interaction trajectories for RL4RS.Wang et al. [48] introduces\nthe Causal Decision Transformer for RS (CDT4Rec), a model that merges offline RL with the\ntransformer architecture. CDT4Rec is designed to tackle the challenges of crafting reward functions\nand leveraging large-scale datasets in RS. It employs a causal mechanism to deduce rewards\nfrom user behavior and uses the transformer architecture to handle vast datasets and identify\ndependencies.\nDrawing inspiration from the works mentioned above, exploring causality in offline RL4RS\nemerges as a promising avenue for future research. Particularly, as causal offline RL4RS advances,\nits primary emphasis on counterfactual augmentation highlights an exciting direction. However, it\nis important to recognize the need for additional endeavors in different domains, including but not\nlimited to distribution shifts and the presence of biases.\n5.4\nRobustness\nThe vulnerability of deep learning-based methods is evident through adversarial samples, un-\nderscoring the pressing concern of robustness in both RS and RL. Particularly, the exploration\nof adversarial attacks and defense strategies within the domain of RS has garnered significant\nattention in recent times, as emphasized by the comprehensive survey conducted by [13]. This\nattention is fueled by the critical importance of security within the realm of RS operations.\nFurthermore, the vulnerability of RL policies to adversarial perturbations in agents’ observations\nhas been established by [30]. In the context of RL4RS,Cao et al. [2] introduce an adversarial attack\ndetection approach. This method leverages the utilization of a Gated Recurrent Unit (GRU) to\nencode the action space into a lower-dimensional representation, alongside the design of decoders\nto identify potential attacks. However, it’s important to note that this method exclusively addresses\nattacks rooted in the Fast Gradient Sign Method (FGSM) and strategically-timed maneuvers. As a\nresult, its ability to detect other forms of attacks is limited.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:21\nWithin the arena of offline RL, recent advancements provide a promising direction.Panaganti\net al. [40] address the challenge of robust offline RL, centering on the learning of policies that can\nwithstand uncertainties in model parameters. The authors introduce the Robust Fitted Q-Iteration\n(RFQI) algorithm, which relies solely on offline data to determine the optimal robust policy. This\nalgorithm adeptly tackles concerns such as offline data collection, model optimization, and unbiased\nestimation. Additionally,Zhang et al. [62] concentrate on a scenario involving a batch dataset of\nstate-action-reward-next state tuples, susceptible to potential corruption by adversaries. Their\nobjective is to extract a near-optimal policy from this compromised dataset.\n6\nCONCLUSION\nThe recent advancements in RL4RS pave the way for efficiently capturing users’ dynamic interests.\nHowever, the nature of online interactions necessitates costly trajectory collection procedures,\nposing a significant hurdle for researchers interested in this field. In this survey, our goal is\nto provide a comprehensive overview of offline RL4RS, a novel paradigm that eliminates the\nneed for an expensive data collection process. Alongside reviewing recent works, we also offer\ninsights into potential future opportunities. Specifically, we’ve compiled and analyzed recent\nprogress in offline RL4RS, organized into two distinct categories: off-policy learning utilizing logged\ndata and offline RL4RS techniques. Furthermore, we address several prevailing challenges in this\ndomain: offline data quality, distribution shift, bias and variance, and explainability. Additionally,\nwe present potential avenues for future exploration in this rapidly evolving field, such as cross-\ndomain recommendation, LLMs, causality, and robustness. Being an emerging topic, offline RL4RS\nintroduces fresh possibilities for integrating pre-existing offline datasets into the realm of RL4RS.\nThis survey can also be perceived as a visionary paper, offering potential benefits to researchers\nwho are newcomers to this field.\nREFERENCES\n[1] M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2022. Reinforcement learning based recommender systems: A\nsurvey. Comput. Surveys 55, 7 (2022), 1–38.\n[2] Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, and Wei Emma Zhang. 2020. Adversarial attacks and detection\non reinforcement learning-based interactive recommender systems. In Proceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Information Retrieval. Association for Computing Machinery, New York,\nNY, USA, 1669–1672.\n[3] Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. 2023.\nGrounding large language models in interactive environments with online reinforcement learning. arXiv preprint\narXiv:2302.02662 (2023).\n[4] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023. Bias and debias in recommender\nsystem: A survey and future directions. ACM Transactions on Information Systems 41, 3 (2023), 1–39.\n[5] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. 2019. Top-k off-policy correction\nfor a REINFORCE recommender system. In Proceedings of the Twelfth ACM International Conference on Web Search and\nData Mining. Association for Computing Machinery, New York, NY, USA, 456–464.\n[6] Minmin Chen, Can Xu, Vince Gatto, Devanshu Jain, Aviral Kumar, and Ed Chi. 2022. Off-policy actor-critic for\nrecommender systems. In Proceedings of the 16th ACM Conference on Recommender Systems. Association for Computing\nMachinery, New York, NY, USA, 338–349.\n[7] Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wenjie Zhang, et al. 2020. Knowledge-guided deep\nreinforcement learning for interactive recommendation. In 2020 International Joint Conference on Neural Networks\n(IJCNN). IEEE, 1–8.\n[8] Xiaocong Chen, Siyu Wang, Lianyong Qi, Yong Li, and Lina Yao. 2023. Intrinsically motivated reinforcement learning\nbased recommendation with counterfactual data augmentation. World Wide Web (2023), 1–22.\n[9] Xiaocong Chen, Lina Yao, Xiaojun Chang, and Siyu Wang. 2022. Empowerment-driven Policy Gradient Learning with\nCounterfactual Augmentation in Recommender Systems. In 2022 IEEE International Conference on Data Mining (ICDM).\nIEEE, 885–890.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:22\nChen et al.\n[10] Xiaocong Chen, Lina Yao, Julian McAuley, Weili Guan, Xiaojun Chang, and Xianzhi Wang. 2022. Locality-sensitive\nstate-guided experience replay optimization for sparse rewards in online recommendation. In Proceedings of the 45th\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval. 1316–1325.\n[11] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang. 2023. Deep reinforcement learning in\nrecommender systems: A survey and new perspectives. Knowledge-Based Systems 264 (2023), 110335.\n[12] Thomas Degris, Martha White, and Richard S. Sutton. 2012. Off-Policy Actor-Critic. In Proceedings of the 29th\nInternational Coference on International Conference on Machine Learning (Edinburgh, Scotland) (ICML’12). Omnipress,\nMadison, WI, USA, 179–186.\n[13] Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra. 2021. A survey on adversarial recommender systems:\nfrom attack/defense strategies to generative adversarial networks. ACM Computing Surveys (CSUR) 54, 2 (2021), 1–38.\n[14] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob An-\ndreas. 2023. Guiding pretraining in reinforcement learning with large language models. arXiv preprint arXiv:2302.06692\n(2023).\n[15] Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang, Shiqi Wang, Zhong Zhang, and Xiangnan\nHe. 2023. Alleviating Matthew Effect of Offline Reinforcement Learning in Interactive Recommendation. In Proceedings\nof the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan)\n(SIGIR ’23). Association for Computing Machinery, New York, NY, USA, 238–248. https://doi.org/10.1145/3539618.\n3591636\n[16] Yue He, Zimu Wang, Peng Cui, Hao Zou, Yafeng Zhang, Qiang Cui, and Yong Jiang. 2022. Causpref: Causal preference\nlearning for out-of-distribution recommendation. In Proceedings of the ACM Web Conference 2022. Association for\nComputing Machinery, New York, NY, USA, 410–421.\n[17] Alexandre Heuillet, Fabien Couthouis, and Natalia Díaz-Rodríguez. 2021. Explainability in deep reinforcement learning.\nKnowledge-Based Systems 214 (2021), 106685.\n[18] William Hoiles and Mihaela Schaar. 2016. Bounded Off-Policy Evaluation with Missing Data for Course Recommenda-\ntion and Curriculum Design. In Proceedings of The 33rd International Conference on Machine Learning (Proceedings of\nMachine Learning Research, Vol. 48), Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, New\nYork, USA, 1596–1604.\n[19] Joey Hong, Branislav Kveton, Manzil Zaheer, Sumeet Katariya, and Mohammad Ghavamzadeh. 2023. Multi-task\noff-policy learning from bandit feedback. In International Conference on Machine Learning (Proceedings of Machine\nLearning Research). PMLR, New York, New York, USA, 13157–13173.\n[20] Rolf Jagerman, Ilya Markov, and Maarten de Rijke. 2019. When people change their mind: Off-policy evaluation in\nnon-stationary recommendation environments. In Proceedings of the Twelfth ACM International Conference on Web\nSearch and Data Mining. Association for Computing Machinery, New York, NY, USA, 447–455.\n[21] Olivier Jeunen and Bart Goethals. 2021. Pessimistic reward models for off-policy learning in recommendation. In\nProceedings of the 15th ACM Conference on Recommender Systems. Association for Computing Machinery, New York,\nNY, USA, 63–74.\n[22] Olivier Jeunen and Bart Goethals. 2023. Pessimistic Decision-Making for Recommender Systems. ACM Transactions on\nRecommender Systems 1, 1 (2023), 1–27.\n[23] Olivier Jeunen, David Rohde, Flavian Vasile, and Martin Bompaire. 2020. Joint policy-value learning for recommendation.\nIn Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. Association\nfor Computing Machinery, New York, NY, USA, 1223–1233.\n[24] Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. 2018. Graph convolutional reinforcement learning. arXiv\npreprint arXiv:1810.09202 (2018).\n[25] Nan Jiang and Lihong Li. 2016. Doubly robust off-policy value evaluation for reinforcement learning. In International\nConference on Machine Learning. PMLR, 652–661.\n[26] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In Advances in neural information processing systems.\nCiteseer, 1008–1014.\n[27] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and\nperspectives on open problems. arXiv preprint arXiv:2005.01643 (2020).\n[28] Yuxi Li. 2017. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274 (2017).\n[29] Luofeng Liao, Zuyue Fu, Zhuoran Yang, Yixin Wang, Mladen Kolar, and Zhaoran Wang. 2021. Instrumental variable\nvalue iteration for causal offline reinforcement learning. arXiv preprint arXiv:2102.09907 (2021).\n[30] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. 2017. Tactics of adversarial\nattack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748 (2017).\n[31] Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang, Sibo Gai, and Donglin Wang. 2023. Beyond ood\nstate actions: Supported cross-domain offline reinforcement learning. arXiv preprint arXiv:2306.12755 (2023).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\nOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems\n111:23\n[32] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015. Recommender system application\ndevelopments: a survey. Decision Support Systems 74 (2015), 12–32.\n[33] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong, and Ed H Chi. 2020. Off-policy\nlearning in two-stage recommender systems. In Proceedings of The Web Conference 2020. 463–473.\n[34] Sephora Madjiheurem and Laura Toni. 2019. Representation learning on graphs: A reinforcement learning application.\nIn The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 3391–3399.\n[35] Bogdan Mazoure, Ilya Kostrikov, Ofir Nachum, and Jonathan J Tompson. 2022. Improving zero-shot generalization\nin offline reinforcement learning using generalized similarity functions. Advances in Neural Information Processing\nSystems 35 (2022), 25088–25101.\n[36] Robert K Merton. 1988. The Matthew effect in science, II: Cumulative advantage and the symbolism of intellectual\nproperty. isis 79, 4 (1988), 606–623.\n[37] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781 (2013).\n[38] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement\nlearning. nature 518, 7540 (2015), 529–533.\n[39] Yusuke Narita, Shota Yasui, and Kohei Yata. 2021. Debiased off-policy evaluation for recommendation systems. In\nProceedings of the 15th ACM Conference on Recommender Systems. 372–379.\n[40] Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. 2022. Robust reinforcement learning\nusing offline data. Advances in neural information processing systems 35 (2022), 32211–32224.\n[41] Otmane Sakhi, Stephen Bonner, David Rohde, and Flavian Vasile. 2020. BLOB: A probabilistic model for recommendation\nthat combines organic and bandit signals. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining. 783–793.\n[42] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization.\nIn International conference on machine learning. PMLR, 1889–1897.\n[43] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recom-\nmendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international\nconference on information and knowledge management. 1441–1450.\n[44] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.\n[45] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien Jose, and Imed\nZitouni. 2017. Off-policy evaluation for slate recommendation. Advances in Neural Information Processing Systems 30\n(2017).\n[46] Chengwei Wang, Tengfei Zhou, Chen Chen, Tianlei Hu, and Gang Chen. 2020. Off-policy recommendation system\nwithout exploration. In Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020,\nSingapore, May 11–14, 2020, Proceedings, Part I 24. Springer, 16–27.\n[47] Siyu Wang, Xiaocong Chen, Dietmar Jannach, and Lina Yao. 2023. Causal Decision Transformer for Recommender\nSystems via Offline Reinforcement Learning. In Proceedings of the 46th International ACM SIGIR Conference on Research\nand Development in Information Retrieval (Taipei, Taiwan) (SIGIR ’23). Association for Computing Machinery, New\nYork, NY, USA, 1599–1608. https://doi.org/10.1145/3539618.3591648\n[48] Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Min Lin, and Tat-Seng Chua. 2022. Causal representation learning\nfor out-of-distribution recommendation. In Proceedings of the ACM Web Conference 2022. Association for Computing\nMachinery, New York, NY, USA, 3562–3571.\n[49] Xiting Wang, Yiru Chen, Jie Yang, Le Wu, Zhengtao Wu, and Xing Xie. 2018. A reinforcement learning framework for\nexplainable recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 587–596.\n[50] Yanan Wang, Yong Ge, Li Li, Rui Chen, and Tong Xu. 2020. Offline meta-level model-based reinforcement learning\napproach for cold-start recommendation. arXiv preprint arXiv:2012.02476 (2020).\n[51] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning 8, 3-4 (1992), 279–292.\n[52] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine learning 8, 3-4 (1992), 229–256.\n[53] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural networks in recommender systems: a\nsurvey. Comput. Surveys 55, 5 (2022), 1–37.\n[54] Teng Xiao and Donglin Wang. 2021. A general offline reinforcement learning framework for interactive recommenda-\ntion. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4512–4520.\n[55] Teng Xiao and Suhang Wang. 2022. Towards off-policy learning for ranking policies with logged feedback. In Proceedings\nof the AAAI Conference on Artificial Intelligence, Vol. 36. 8700–8707.\n[56] Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. Deeppath: A reinforcement learning method for\nknowledge graph reasoning. arXiv preprint arXiv:1707.06690 (2017).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n111:24\nChen et al.\n[57] Yiping Yuan, Ajith Muralidharan, Preetam Nandy, Miao Cheng, and Prakruthi Prabhakar. 2022. Offline reinforcement\nlearning for mobile notifications. In Proceedings of the 31st ACM International Conference on Information & Knowledge\nManagement. 3614–3623.\n[58] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as\ninstruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001\n(2023).\n[59] Ruiyi Zhang, Tong Yu, Yilin Shen, and Hongxia Jin. 2022. Text-Based Interactive Recommendation via Offline\nReinforcement Learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 11694–11702.\n[60] Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, and Changyou Chen. 2019. Text-based interactive recommendation via\nconstraint-augmented reinforcement learning. Advances in neural information processing systems 32 (2019).\n[61] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recommender system: A survey and new\nperspectives. ACM Computing Surveys (CSUR) 52, 1 (2019), 1–38.\n[62] Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. 2022. Corruption-robust offline reinforcement learning. In\nInternational Conference on Artificial Intelligence and Statistics. PMLR, 5757–5773.\n[63] Yongfeng Zhang, Xu Chen, et al. 2020. Explainable recommendation: A survey and new perspectives. Foundations and\nTrends® in Information Retrieval 14, 1 (2020), 1–101.\n[64] Kangzhi Zhao, Xiting Wang, Yuren Zhang, Li Zhao, Zheng Liu, Chunxiao Xing, and Xing Xie. 2020. Leveraging\nDemonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval. 239–248.\n[65] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\n[66] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A\ndeep reinforcement learning framework for news recommendation. In Proceedings of the 2018 world wide web conference.\nAssociation for Computing Machinery, New York, NY, USA, 167–176.\n[67] Zheng-Mao Zhu, Xiong-Hui Chen, Hong-Long Tian, Kun Zhang, and Yang Yu. 2022. Offline reinforcement learning\nwith causal structured world models. arXiv preprint arXiv:2206.01474 (2022).\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2023.\n",
  "categories": [
    "cs.IR",
    "cs.AI"
  ],
  "published": "2023-08-22",
  "updated": "2023-08-22"
}