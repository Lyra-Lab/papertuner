{
  "id": "http://arxiv.org/abs/2401.02349v2",
  "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
  "authors": [
    "Ezgi Korkmaz"
  ],
  "abstract": "Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto large language models, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will formalize and analyze generalization in deep\nreinforcement learning. We will explain the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\ngeneralization capabilities. Furthermore, we will categorize and explain the\nmanifold solution approaches to increase generalization, and overcome\noverfitting in deep reinforcement learning policies. From exploration to\nadversarial analysis and from regularization to robustness our paper provides\nan analysis on a wide range of subfields within deep reinforcement learning\nwith a broad scope and in-depth view. We believe our study can provide a\ncompact guideline for the current advancements in deep reinforcement learning,\nand help to construct robust deep neural policies with higher generalization\nskills.",
  "text": "A Survey Analyzing Generalization in\nDeep Reinforcement Learning\nEzgi Korkmaz\nUniversity College London\nLondon, United Kingdom\nAbstract\nReinforcement learning research obtained significant success and attention with the\nutilization of deep neural networks to solve problems in high dimensional state or action\nspaces. While deep reinforcement learning policies are currently being deployed in many\ndifferent fields from medical applications to large language models, there are still ongoing\nquestions the field is trying to answer on the generalization capabilities of deep reinforce-\nment learning policies. In this paper, we will formalize and analyze generalization in deep\nreinforcement learning. We will explain the fundamental reasons why deep reinforcement\nlearning policies encounter overfitting problems that limit their generalization capabilities.\nFurthermore, we will categorize and explain the manifold solution approaches to increase\ngeneralization, and overcome overfitting in deep reinforcement learning policies. From ex-\nploration to adversarial analysis and from regularization to robustness our paper provides\nan analysis on a wide range of subfields within deep reinforcement learning with a broad\nscope and in-depth view. We believe our study can provide a compact guideline for the\ncurrent advancements in deep reinforcement learning, and help to construct robust deep\nneural policies with higher generalization skills.\n1. Introduction\nThe performance of reinforcement learning algorithms (Watkins, 1989; Sutton, 1984, 1988)\nhas been boosted with the utilization of deep neural networks as function approximators\n(Mnih et al., 2015). Currently, it is possible to learn deep reinforcement learning policies\nthat can operate in large state and/or action space MDPs (Silver et al., 2017; Vinyals\net al., 2019). This progress consequently resulted in building reasonable deep reinforcement\nlearning policies that can play computer games with high dimensional state representations\n(e.g. Atari, StarCraft), solve complex robotics control tasks, design algorithms (Mankowitz\net al., 2023; Fawzi et al., 2022), guide large language models (OpenAI, 2023; Google Gemini,\n2023), and play some of the most complicated board games (e.g. Chess, Go) (Schrittwieser\net al., 2020). However, deep reinforcement learning algorithms also experience several prob-\nlems caused by their overall limited generalization capabilities. Some studies demonstrated\nthese problems via adversarial perturbations introduced to the state observations of the pol-\nicy (Huang et al., 2017; Kos and Song, 2017; Korkmaz, 2022; Korkmaz and Brown-Cohen,\n2023), several focused on exploring the fundamental issues with function approximation, es-\ntimation biases in the state-action value function (Thrun and Schwartz, 1993; van Hasselt,\n2010), or with new architectural design ideas (Wang et al., 2016). The fact that we are not\nable to completely explore the entire MDP for high dimensional state representation MDPs,\neven with deep neural networks as function approximators, is one of the root problems that\narXiv:2401.02349v2  [cs.LG]  30 Oct 2024\nKorkmaz\nlimits generalization. On top of this, some portion of the problems are directly caused by\nthe utilization of deep neural networks and thereby the intrinsic problems inherited from\ntheir utilization (Goodfellow et al., 2015; Szegedy et al., 2014; Korkmaz, 2022, 2024b).\nIn order to address open questions on generalization in deep reinforcement learning, there\nneeds to be some commonly agreed standard of what is meant by generalization. Currently,\ndifferent aspects of generalization are considered in various subfields either working on\nthe fundamental questions regarding or the applications of deep reinforcement learning.\nWe take the point of view in this paper that these various aspects can, and should, be\ndescribed and studied in a unified way. In particular, we argue that the various approaches\nto generalization can be succinctly classified based on which part of the Markov Decision\nProcess is expected to vary. We make this classification formal and unify how much current\nwork on generalization in deep reinforcement learning fits clearly into the classification we\nintroduce.\nIn this paper we will focus on generalization in deep reinforcement learning\nand the underlying causes of the limitations deep reinforcement learning research currently\nfaces. In particular, we will try to answer the following questions:\n• How can we formalize the concept of generalization in deep reinforcement learning?\n• What is the role of exploration in overfitting for deep reinforcement learning?\n• What are the causes of overestimation bias observed in state-action value functions?\n• What has been done to overcome the overfitting problems that deep reinforcement\nlearning algorithms have encountered so far, and to enable deep neural policies to\ngeneralize to non-stationary complex environments?\nTo answer these questions we will go through research connecting several subfields in\nreinforcement learning on the problems and corresponding proposed solutions regarding\ngeneralization. In this paper we introduce a formal definition of generalization and cate-\ngorization of the different methods used to both achieve and assess generalization, and use\nit to systematically summarize and consolidate the current body of research. We further\ndescribe the issue of value function overestimation, and the role of exploration in overfitting\nin reinforcement learning. Furthermore, we explain new emerging research areas that can\npotentially target these questions in the long run including meta-reinforcement learning\nand lifelong learning. The objective of the paper is to introduce a formal generalization\ndefinition and provide a compact overview and unification of the current advancements and\nlimitations in the field.\n2. Preliminaries on Deep Reinforcement Learning\nThe aim in deep reinforcement learning is to learn a policy via interacting with an environ-\nment in a Markov Decision Process (MDP) that maximize expected cumulative discounted\nrewards. An MDP is represented by a tuple M = (S, A, P, r, ρ0, γ), where S represents\nthe state space, A represents the action space, r : S × A →R is a reward function,\nP : S × A →∆(S) is a transition probability kernel, ρ0 represents the initial state dis-\ntribution, and γ represents the discount factor. The objective in reinforcement learning\nis to learn a policy π : S × A →R which maps states to probability distributions on ac-\ntions in order to maximize the expected cumulative reward R = E PT−1\nt=0 γtr(st, at) where\n2\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nat ∼π(st, ·), st+1 ∼P(·|st, at). The temporal difference updates achieves this objective by\nupdating the value function V (s) (Sutton, 1984, 1988)\nV (st) ←V (st) + α[r(st+1, a) + γV (st+1) −V (st)]\n(1)\nWhile Equation 1 represents the one-step temporal difference update, i.e.\nTD(0), it is\nfurther possible to consider multi-step TD which focuses on multi-step return, i.e. TD(λ).\nIn Q-learning the goal is to learn the optimal state-action value function (Watkins, 1989)\nQ∗(s, a) = r(s, a) +\nX\ns′∈S\nP(s′|s, a) max\na′∈A Q∗(s′, a′).\n(2)\nThis is achieved via iterative Bellman update (Bellman, 1957; Bellman and Dreyfus, 1959)\nwhich updates Q(st, at) by\nQ(st, at) + α[Rt+1 + γ max\na\nQ(st+1, a) −Q(st, at)].\nThus, the optimal policy is determined by choosing the action a∗(s) = arg maxa Q(s, a) in\nstate s. The optimal Bellman operator is (Bellman, 1957)\nBQ(s, a) := E[r(s, a)] + γEP[max\na′\nQ(s′, a′)]\nIn high dimensional state space or action space MDPs the optimal policy is decided via a\nfunction-approximated state-action value function represented by a deep neural network.\nThe loss function in deep reinforcement learning is the quadratic difference between that\ntarget network and the current state-action value function.\nLi(θi) = Ee∼D[(r(s, a) + γ max\na′\nQ(s′, a′, θtarget) −Q(s, a, θi))2]\nwhere D is the experience replay buffer (Lin, 1993) in which the experiences e = {st, at, rt, st+1}\nsampled from D = {e1, e2, . . . eN}. The loss function is optimized by taking the gradient\nwith respect function approximation weights\nθi+1 = θi + α(r(st, at, st+1) + γQ(st+1, arg max\na\nQ(st+1, a; θtarget\ni−1\n); θtarget\ni−1\n)\n−Q(st, at; θi))∇θiQ(st, at; θi).\nIn a parallel line of algorithm families the policy itself is directly parametrized by πθ (Sutton\net al., 1999), and the gradient estimator used in learning is\ng = Et\n\u0002\n∇θ log πθ(st, at)(Q(st, at) −max\na\nQ(st, a))\n\u0003\nwhere Q(st, at) refers to the state-action value function at time step t. The algorithms that\nfocus on directly parameterizing the policy try to solve the following optimization problem\n(Schulman et al., 2015).\nmax\nθ\nEs∼ρθold;a∼πθold(s,·)\n\u0014 πθ(s, a)\nπθold(s, a)Qθold(s, a)\n\u0015\nsubject to Es∼ρθoldDKL(πθ(s, ·)||πold(s, ·)) ≤δ\n3\nKorkmaz\n3. How to Achieve Generalization?\n3.1 Generic Reinforcement Learning Algorithm\nTo be able to understand and analyze the connection between different approaches to achieve\ngeneralization first we will provide a clear definition intended to capture the behavior of a\ngeneric reinforcement learning algorithm.\nDefinition 3.1 (Generic reinforcement learning algorithm). A reinforcement learning train-\ning algorithm A learns a policy π by interacting with an MDP M. We divide up the exe-\ncution of A into discrete time steps as follows. At each time t, the algorithm has a current\npolicy πt, observes a state st, takes an action at ∼πt(st, ·), and observes a transition to\nstate s′\nt ∼P(· | st, at) with corresponding reward rt = r(st, at, s′\nt). We define the history\nof algorithm A in MDP M to be the sequence Ht = (π0, s0, a0, s′\n0, r0), . . . (πt, st, at, s′\nt, rt) of\nall the transitions observed by the algorithm so far. We require that the policy πt and state\nst at time t are a function only of Ht−1, i.e the transitions observed so far by A. At time\nt = T, the algorithm stops and outputs the policy π = πT . We use the notation A to denote\nthe set of reinforcement learning training algorithms and Π to denote the set of policies π\nin an MDP M.\nIntuitively, a reinforcement learning algorithm has a current policy πt, performs a se-\nquence of queries (st, at) to the MDP, and observes the resulting state transitions and\nrewards. In order to be as generic as possible, the definition makes no assumptions about\nhow the algorithm chooses the sequence of queries, other than that at ∼πt(st, ·). Notably,\nif taking action at in state st leads to a transition to state s′\nt, there is no requirement that\nst+1 = s′\nt. Indeed, the only assumption is that st+1 and πt+1 may depend only on Ht, the\nhistory of transitions observed so far. This allows the definition to capture deep reinforce-\nment learning algorithms, which may choose to query states and actions in a complex way\nas a function of previously observed state transitions.\n3.2 Base Generalization in Deep Reinforcement Learning\nWe next introduce a basic metric capturing how well an algorithm generalizes given a fixed\namount of interaction with a given MDP.\nDefinition 3.2 (Base generalization). Given an MDP M = (S, A, P, r, ρ0, γ), let πT and\nˆπT be policies output by training algorithms taking T steps. The base generalization Gbase\nis the difference between the expected discounted cumulative rewards obtained by policy πT\nand ˆπT in M.\nGbase(πT , ˆπT ) = Eat∼πT (st,·)\n\" ∞\nX\nt=0\nγtr(st, at, st+1)\n#\n−Eˆat∼ˆπT (ˆst,·)\n\" ∞\nX\nt=0\nγtr(ˆst, ˆat, ˆst+1)\n#\nThe base generalization definition captures how well an algorithm can generalize to\nunseen states and transitions, given only access to T interactions with the MDP M. Hence,\nin base generalization the role of exploration is exceedingly dominant and this will be further\nexplained in Section 5.\n4\nA Survey Analyzing Generalization in Deep Reinforcement Learning\n3.3 Algorithmic Generalization\nBased on the definition of generic reinforcement learning algorithm, we will now further\ndefine the different approaches proposed to achieve generalization.\nAt a high level, the\napproaches we will discuss will be divided into two classes:\nI. Techniques that solely modify the training algorithm,\nII. Techniques that directly modify the MDP (i.e. learning environment, training data)\nthat forms the interactions of the training algorithm with the learning environment.\nOur first definition formalizes the techniques that solely modify the training algorithm.\nDefinition 3.3 (Algorithmic generalization). Let A be a training algorithm that takes an\nMDP as input and outputs a policy. Given an MDP M = (S, A, P, r, ρ0, γ), an algorithmic\ngeneralization method GA is given by a function F : A →A that runs the algorithm F(A)\nin the MDP M.\nAlgorithmic generalization captures modifications to the training algorithm itself that\ncan range from the choice of optimization methods or regularizers, to update rules for the\npolicy.\n3.4 Generalization Through Rewards\nDefinition 3.4 (Rewards transforming generalization). Let A be a training algorithm that\ntakes as input an MDP and outputs a policy. Given an MDP M = (S, A, P, r, ρ0, γ), a\nrewards transforming generalization method GR is given by a sequence of functions Ft :\n(Π × S × A × S × R)t × R →R. The method attempts to achieve generalization by running\nA on MDP M, but modifying the rewards at each time t to be ˆrt(st, at, s′\nt) = Ft−1(Ht−1, rt),\nwhere Ht−1 is the history of algorithm A when running with the transformed rewards.\nIn particular, a method under the rewards transforming generalization category runs\nthe original algorithm to train the policy, but modifies the observed rewards. The instances\nof these techniques will be mentioned and explained in Section 6.2, i.e. direct function\nregularization, in Section 5, i.e. the role of exploration in overfitting, and in Section 8, i.e.\ntransfer in reinforcement learning.\n3.5 Generalization Through Observations\nFollowing the definition of reward transforming generalization we define state transforming\ngeneralization which is one of the canonical approaches for achieving generalization in deep\nreinforcement learning. The instances of generalization through observations will be cate-\ngorized and explained in detail in Section 6.1, i.e. data augmentation, and Section ??, i.e.\nthe adversarial perspective for deep neural policy generalization.\nDefinition 3.5 (State transforming generalization). Let A be a training algorithm that\ntakes as input an MDP and outputs a policy.\nGiven an MDP M = (S, A, P, r, ρ0, γ),\na state transforming generalization method GS is given by a sequence of functions Ft :\n(Π × S × A × S × R)t × S →S. The method attempts to achieve generalization by running\n5\nKorkmaz\nA on MDP M, but modifying the state chosen at time t to be ˆst = Ft−1(Ht−1, st), where\nHt−1 is the history of algorithm A when running with the transformed states.\n3.6 Generalization Through Environment Dynamics\nAnother category of algorithms that tries to achieve generalization in deep reinforcement\nlearning focuses on achieving this objective through environment dynamics transformation.\nThe methods focusing on generalization through environment dynamics will be referred to\nand explained in Section 6.2, i.e. direct function regularization.\nDefinition 3.6 (Transition probability transforming generalization). Let A be a train-\ning algorithm that takes as input an MDP and outputs a policy. Given an MDP M =\n(S, A, P, r, ρ0, γ), a transition probability transforming generalization method GP is given\nby a sequence of functions Ft : (Π × S × A × S × R)t × (S × A × S) →R. The method\nattempts to achieve generalization by running A on MDP M, but modifying the transition\nprobabilities at time t to be ˆP(st, at, s′\nt) = Ft−1(Ht−1, st, at, s′\nt), where Ht−1 is the history of\nalgorithm A when running with the transformed transition probabilities.\n3.7 Generalization Through Policy\nThe last type of generalization method we define is based on directly modifying the current\npolicy used by the algorithm to select actions at each time step.\nWe will explain the\ninstances of the techniques that focus on generalization through policy in Section 5, i.e. the\nrole of exploration in overfitting, and Section 7, i.e. meta reinforcement learning and meta\ngradients.\nDefinition 3.7 (Policy transforming generalization). Let A be a training algorithm that\ntakes as input an MDP and outputs a policy.\nGiven an MDP M = (S, A, P, r, ρ0, γ),\na policy transforming generalization method Gπ is given by a sequence of functions Ft :\n(Π × S × A × S × R)t × S × ∆(A) →∆(A). The method attempts to achieve generalization\nby running A on MDP M, but modifying the current policy by which A chooses the action\nat time t to be ˆπt(st, ·) = Ft−1(Ht−1, st, πt(st, ·)), where Ht−1 is the history of algorithm A\nwhen running with the transformed policy.\n3.8 Assessing Generalization\nAll the definitions so far categorize methods to modify either training algorithms and/or the\nMDP, i.e. learning environment, training data, in order to achieve generalization. However,\nmany such methods for modifying training algorithms have a corresponding method which\ncan be used to assess the generalization capabilities of a trained policy. Our final definition\ncaptures this correspondence.\nDefinition 3.8 (Generalization testing). Let ˆπ be a trained policy for an MDP M. Let Ft\nbe a sequence of functions corresponding to a generalization method from one of the previous\ndefinitions. The generalization testing method of Ft is given by executing the policy ˆπ in\nM, but in each time step applying the modification Ft where the history Ht is given by the\ntransitions executed by ˆπ so far. When both a generalization method and a generalization\ntesting method are used concurrently, we will use subscripts to denote the generalization\n6\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nmethod and superscripts to denote the testing method.\nFor instance, Gπ\nS corresponds to\ntraining with a state transforming method, and testing with a policy transforming method.\n4. Roots of Overestimation in Deep Reinforcement Learning\nMany reinforcement learning algorithms compute estimates for the state-action values in\nan MDP. Because these estimates are usually based on a stochastic interaction with the\nMDP, computing accurate estimates that correctly generalize to further interactions is one\nof the most fundamental tasks in reinforcement learning. A major challenge in this area\nhas been the tendency of many classes of reinforcement learning algorithms to consistently\noverestimate state-action values. Initially the overestimation bias for Q-learning is discussed\nand theoretically justified by Thrun and Schwartz (1993) as a biproduct of using function\napproximators for state-action value estimates. In particular, Thrun and Schwartz (1993)\nproves that if the reinforcement learning policy overestimates the state-action values by γc\nduring learning then the Q-learning algorithm will fail to learn optimal policy if γ >\n1\n1 + c.\nFollowing this initial discussion it has been shown that several parts of the deep reinforce-\nment learning process can cause overestimation bias. Learning overestimated state-action\nvalues can be caused by statistical bias of utilizing a single max operator (van Hasselt,\n2010), coupling between value function and the optimal policy (Raileanu and Fergus, 2021;\nCobbe et al., 2021), or caused by the accumulated function approximation error (Boyan\nand Moore, 1994).\nSeveral methods have been proposed to target overestimation bias for value iteration\nalgorithms. In particular, van Hasselt (2010) demonstrated that the expectation of a maxi-\nmum of a random variable is not equal to maximum of the expectation of a random variable.\nE[max\ni [Xi]] ̸= max\ni [E[Xi]] where X = {X1, X2, . . . , XN}\nThis clear distinction shows that simple Q-learning is a biased estimator, and to solve this\noverestimation bias introduced by the max operator van Hasselt (2010) proposed to utilize\na double estimator for the state-action value estimates. In particular, the double estimator\nfor double Q-learning works as follows\nQI(s, a) ←QI(s, a) + α(s, a)(r(s, a) + γQII(s′, max\na\nQI(s′, a)) −QI(s′, a))\nand\nQII(s, a) ←QII(s, a) + α(s, a)(r(s, a) + γQI(s′, max\na\nQII(s′, a)) −QII(s′, a)).\nLater, the authors also created a version of this algorithm that can solve high dimensional\nstate space problems (Hasselt et al., 2016).\nSome of the work on this line of research\ntargeting overestimation bias for value iteration algorithms is based on simply averaging\nthe state-action values with previously learned state-action value estimates during training\ntime (Anschel et al., 2017). While overestimation bias was demonstrated to be a problem\nand discussed over a long period of time (Thrun and Schwartz, 1993; van Hasselt, 2010),\nrecent studies also further demonstrated that actor critic algorithms also suffer from this\nissue (Fujimoto et al., 2018).\n7\nKorkmaz\nTable 1: Environment and algorithm details for different exploration strategies for general-\nization.\nCitation\nMethod\nLearning Environment\nAlgorithm\nMnih et al. (2015)\nϵ-greedy\nArcade Learning Environment\nDQN\nBellemare et al. (2016)\nCount-based\nArcade Learning Environment\nA3C and DQN\nOsband et al. (2016b)\nRLSVI\nTetris\nTabular Q\nOsband et al. (2016a)\nBootstrapped DQN\nArcade Learning Environment\nDQN\nHouthooft et al. (2016)\nVIME\nDeepMind Control Suite\nTRPO\nFortunato et al. (2018)\nNoisyNet\nArcade Learning Environment\nA3C and DQN\nLee et al. (2021)\nSUNRISE\nDCS1& ALE\nSAC & RDQN\nMahankali et al. (2024)\nRandom Latent\nArcade Learning Environment\nPPO\n5. The Role of Exploration in Overfitting\nThe fundamental trade-off of exploration vs exploitation is the dilemma that the agent\ncan try to take actions to move towards more unexplored states by sacrificing the current\nimmediate rewards. While there is a significant body of studies on provably efficient explo-\nration strategies the results from these studies do not necessarily directly transfer to the\nhigh dimensional state or action MDPs. The most prominent indication of this is that, even\nthough it is possible to use deep neural networks as function approximators for large state\nspaces, the agent will simply not be able to explore the full state space. The fact that the\nagent is able to only explore a portion of the state space simply creates a bias in the learnt\nvalue function (III, 1995).\nIn this section, we will go through several exploration strategies in deep reinforcement\nlearning and how they affect policy overfitting. A quite simple version of this is based on\nadding noise in action selection during training e.g. ϵ-greedy exploration. Note that this is\nan example of a policy transforming generalization method Gπ in Definition 3.7 in Section\n3. While ϵ-greedy exploration is widely used in deep reinforcement learning (Wang et al.,\n2016; Hamrick et al., 2020; Kapturowski et al., 2023), it has also been proven that to ex-\nplore the state space these algorithms may take exponentially long (Kakade, 2003). Several\nothers focused on randomizing different components of the reinforcement learning training\nalgorithms. In particular, Osband et al. (2016b) proposes the randomized least squared\nvalue iteration algorithm to explore more efficiently in order to increase generalization in\nreinforcement learning for linearly parametrized value functions. This is achieved by simply\nadding Gaussian noise as a function of state visitation frequencies to the training dataset.\nLater, the authors also propose the bootstrapped DQN algorithm (i.e. adding temporally\ncorrelated noise) to increase generalization with non-linear function approximation Osband\net al. (2016a). Recently, Mahankali et al. (2024) proposed to randomize the reward func-\ntion to enhance exploration in high dimensional observation MDPs where policy gradient\nalgorithms are used to explore. This study is also a clear example of the generalization\nthrough rewards as has been explained in Definition 3.4 in Section 3.\n1. DeepMind Control Suite\n8\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nHouthooft et al. (2016) proposed an exploration technique centered around maximizing\nthe information gain on the agent’s belief of the environment dynamics. In practice, the\nauthors use Bayesian neural networks for effectively exploring high dimensional action space\nMDPs. Following this line of work on increasing efficiency during exploration Fortunato\net al. (2018) proposes to add parametric noise to the deep reinforcement learning policy\nweights in high dimensional state MDPs.\nWhile several methods focused on ensemble\nstate-action value function learning (Osband et al., 2016a), Lee et al. (2021) proposed\nreweighting target Q-values from an ensemble of policies (i.e. weighted Bellman backups)\ncombined with highest upper-confidence bound action selection. Another line of research\nin exploration strategies focused on count-based methods that use the direct count of state\nvisitations. In this line of work, Bellemare et al. (2016) tried to lay out the relationship\nbetween count based methods and intrinsic motivation, and used count-based methods for\nhigh dimensional state MDPs (i.e. Arcade Learning Environment). Yet it is worthwhile\nto note that most of the current deep reinforcement learning algorithms use very simple\nexploration techniques such as ϵ-greedy which is based on taking the action maximizing\nthe state-action value function with probability 1 −ϵ and taking a random action with\nprobability ϵ (Mnih et al., 2015; Hasselt et al., 2016; Wang et al., 2016; Hamrick et al.,\n2020; Kapturowski et al., 2023).\nIt is possible to argue that the fact that the deep reinforcement learning policy obtained\na higher score with the same number of samples by a particular type of training method A\ncompared to method B is by itself evidence that the technique A leads to more generalized\npolicies. Even though the agent is trained and tested in the same environment, the explored\nstates during training time are not exactly the same states visited during test time. The\nfact that the policy trained with technique A obtains a higher score at the end of an episode\nis sole evidence that the agent trained with A was able to visit further states in the MDP\nand thus succeed in them. Yet, throughout the paper we will discuss different notions of\ngeneralization investigated in different subfields of reinforcement learning research. While\nexploration vs exploitation stands out as one of the main problems in reinforcement learning\npolicy performance most of the work conducted in this section focuses on achieving higher\nscore in hard-exploration games (i.e.\nMontezuma’s Revenge) rather than aiming for a\ngenerally higher score for each game overall across a given benchmark. Thus, it is possible\nthat the majority of work focusing on exploration so far might not be able to obtain policies\nthat perform as well as those in the studies described in Section 6 across a given benchmark.\n6. Regularization\nIn this section we will focus on different regularization techniques employed to increase\ngeneralization in deep reinforcement learning policies. We will go through these works by\ncategorizing each of them under data augmentation, adversarial training, and direct function\nregularization. Under each category we will connect these different lines of approach to\nincrease generalization in deep reinforcement learning to the settings we defined in Section\n3.\n9\nKorkmaz\nTable 2: Environment and algorithm details for data augmentation techniques for state\nobservation generalization. All of the studies in this section focus on state trans-\nformation methods GS defined in Section 3.\nCitation\nMethod\nEnvironment\nAlgorithm\nYarats et al. (2021)\nDrQ\nDCS, Arcade Learning Environment\nDQN\nLaskin et al. (2020b)\nCuRL\nDCS, Arcade Learning Environment\nSAC and DQN\nLaskin et al. (2020a)\nRAD\nDeepMind Control Suite, ProcGen\nSAC and PPO\nKorkmaz (2023)\nSemantic Changes\nArcade Learning Environment\nDDQN & A3C\nWang et al. (2020)\nMixreg\nProcGen\nDQN and PPO\n6.1 Data Augmentation\nSeveral studies focus on diversifying the observations of the deep reinforcement learning\npolicy to increase generalization capabilities. A line of research in this regard focused on\nsimply employing versions of data augmentation techniques (Laskin et al., 2020a,b; Yarats\net al., 2021) for high dimensional state representation environments. In particular, these\nstudies involve simple techniques such as cropping, rotating or shifting the state observations\nduring training time. While this line of work got considerable attention, a quite recent study\nAgarwal et al. (2021b) demonstrated that when the number of random seeds is increased to\none hundred the relative performance achieved and reported in the original papers of (Laskin\net al., 2020b; Yarats et al., 2021) on data augmentation training in deep reinforcement\nlearning decreases to a level that might be significant to mention.\nWhile some of the work on this line of research simply focuses on using a set of data\naugmentation methods (Laskin et al., 2020a,b; Yarats et al., 2021), other work focuses on\nproposing new environments to train in (Cobbe et al., 2020). The studies on designing new\nenvironments to train deep reinforcement learning policies basically aim to provide high\nvariation in the observed environment such as changing background colors and changing\nobject shapes in ways that are meaningful in the game, in order to increase test time gen-\neralization. In the line of robustness and test time performance, a more recent work that\nis also mentioned in Section 6.3 demonstrated that imperceptible semantically meaningful\ndata augmentations can cause significant damage on the policy performance and certified\nrobust deep reinforcement learning policies are more vulnerable to these imperceptible aug-\nmentations (Korkmaz, 2021a, 2023).\nWithin this category some work focuses on producing more observations by simply\nblending in (e.g. creating a mixture state from multiple different observations) several ob-\nservations to increase generalization (Wang et al., 2020). While most of the studies trying\nto increase generalization by data augmentation techniques are primarily conducted in the\nDeepMind Control Suite or the Arcade Learning Environment (ALE) (Bellemare et al.,\n2013), some small fraction of these studies (Wang et al., 2020) are conducted in relatively\nrecently designed training environments like ProcGen (Cobbe et al., 2020). In the line of\nresearch proposing learning environments Dennis et al. (2020) proposed unsupervised envi-\nronment design by changing the environment parameters to asses generalization for maze\nstructured environments by minimax training where the ”adversary” creating an environ-\n10\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nTable 3: Environment and algorithm details for different direct function regularization\nstrategies for trying to overcome overfitting problems in reinforcement learning.\nNote that most of the methods based on direct function regularization are a form\nof algorithmic generalization GA to overcome overfitting as described in Section 3.\nCitation\nProposed Method\nLearning Environment\nIgl et al. (2019)\nSNI and IBAC\nGridWorld and CoinRun\nVieillard et al. (2020b)\nMunchausen RL\nArcade Learning Environment\nLee et al. (2020)\nNetwork Randomization\n2D CoinRun and 3D DeepMind Lab\nAmit et al. (2020)\nDiscount Regularization\nGridWorld and MuJoCo2\nAgarwal et al. (2021a)\nPSM\nDDMC and Rectangle Game3\nLiu et al. (2021)\nBN and dropout and L2/L1\nMuJoCo\nment for the policy to solve a task with goal and obstacles as an underspecified parameter.\nCobbe et al. (2019) focuses on decoupling the training and testing set for reinforcement\nlearning via simply proposing a new game environment CoinRun.\n6.2 Direct Function Regularization\nWhile some of the work we have discussed so far focuses on regularizing the data (i.e. state\nobservations) as in Section 6.1, some focuses on directly regularizing the function learned\nwith the intention of simulating techniques from deep neural network regularization like\nbatch normalization and dropout (Igl et al., 2019). While some studies have attempted to\nsimulate these known techniques in reinforcement learning, some focus on directly applying\nthem to overcome overfitting. In this line of research, Liu et al. (2021) proposes to use\nknown techniques from deep neural network regularization to apply in continuous control\ndeep reinforcement learning training. In particular, these techniques are batch normaliza-\ntion (BN) (Ioffe and Szegedy, 2015), weight clipping, dropout, entropy and L2/L1 weight\nregularization. All these methods fall under the algorithmic generalization category GA as\ndescribed in Section3.\nLee et al. (2020) proposes to utilize a random network to essentially achieve a version of\nrandomization in the input observations to increase generalization skills of deep reinforce-\nment learning policies, and tests the proposal in the 2D CoinRun game proposed by Cobbe\net al. (2019) and 3D DeepMind Lab (?). In particular, the authors essentially introduce a\nrandom convolutional layer to achieve this objective. This study is an example of an algo-\nrithmic generalization method GA described in Definition 3.3 when the single layer random\nnetwork is not placed at the first layer of the deep neural network. However, when this\nsingle layer random network is placed at the first layer of the neural network, this method\nis essentially just introducing some noise to the state observations of the policy, thus this is\nan example of state transforming generalization. When this single random layer is placed\n2. Low dimensional setting of MuJoCo is used for this study (Todorov et al., 2012).\n3. Rectangle game is a simple video game with only two actions, ”Right” and ”Jump”. The game has black\nbackground and two rectangles where the goal of the game is to avoid white obstacles and reach to the\nright side of the screen. Agarwal et al. (2021a) is the only paper we encountered experimenting with\nthis particular game.\n11\nKorkmaz\nTable 4: Algorithm details for different direct function regularization strategies for trying\nto overcome overfitting problems in reinforcement learning. Note that most of\nthe methods based on direct function regularization are a form of algorithmic\ngeneralization GA to overcome overfitting as described in Section 3.\nCitation\nProposed Method\nReinforcement Learning Algorithm\nIgl et al. (2019)\nSNI and IBAC\nProximal Policy Optimization (PPO)\nVieillard et al. (2020b)\nMunchausen RL\nDQN and IQN\nLee et al. (2020)\nNetwork Randomization\nProximal Policy Optimization (PPO)\nAmit et al. (2020)\nDiscount Regularization\nTwin Delayed DDPG (TD3)\nAgarwal et al. (2021a)\nPSM\nData Regularized-Q (DrQ)\nLiu et al. (2021)\nBN and dropout and L2/L1\nPPO, TRPO, SAC, A2C\nother than first, the method is no longer a state transforming generalization method because\nthe states are not modified before they have been observed by the algorithm, but rather\nimplicitly changed due to a random convolutional layer added in the architecture. We will\nfurther provide clear instances of the state transformation generalization also in Section\n6.3 when the worst-case perturbation methods to target generalization in reinforcement\nlearning policies are explained.\nSome work employs contrastive representation learning to learn deep reinforcement\nlearning policies from state observations that are close to each other (Agarwal et al., 2021a).\nThis study leverage the temporal aspect of reinforcement learning and propose a policy sim-\nilarity metric. The main goal of the paper is to lay out the sequential structure and utilize\nrepresentation learning to learn generalizable abstractions from state representations. One\ndrawback of this study is that most of the experimental study is conducted in a non-baseline\nenvironment (i.e. Rectangle game and Distracting DM Control Suite). Malik et al. (2021)\nstudies query complexity of reinforcement learning policies that can generalize to multiple\nenvironments. The authors of this study focus on an example of the transition probability\ntransformation setting GP in Definition 3.6, and the reward function transformation setting\nGR in Definition 3.4.\nAnother line of study in direct function generalization investigates the relationship be-\ntween reduced discount factor and adding an ℓ2-regularization term to the loss function,\ni.e. weight decay (Amit et al., 2020). The authors in this work demonstrate the explicit\nconnection between reducing the discount factor and adding an ℓ2-regularizer to the value\nfunction for temporal difference learning. In particular, this study demonstrates that adding\nan ℓ2-regularization term to the loss function is equal to training with a lower discount term,\nwhich the authors refer to as discount regularization. The results of this study however are\nbased on experiments from tabular reinforcement learning, and the low dimensional setting\nof the MuJoCo environment (Todorov et al., 2012). This study is also another clear example\nof algorithmic generalization GA as described in Definition 3.3.\nOn the reward transformation for generalization setting GR defined in Definition 3.4,\nVieillard et al. (2020b) adds the scaled log policy to the current rewards. To overcome\noverfitting some work tries to learn explicit or implicit similarity between the states to\nobtain a reasonable policy (Lan et al., 2021). In particular, the authors in this work try to\n12\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nTable 5: Environment and algorithm details for adversarial policy regularization and at-\ntack techniques in deep reinforcement learning. Note that most of the methods\nbased on adversarial approaches are a form of generalization assessment through\nstate observations GS as described in Definition 3.8, and some falls under the\ngeneralization through environment dynamics GP as described in Definition 3.6.\nCitation\nMethod\nEnvironment\nAlgorithm\nHuang et al. (2017)\nFast Gradient Sign (FGSM)\nALE\nDQN, TRPO, A3C\nKos and Song (2017)\nFast Gradient Sign (FGSM)\nALE\nDQN & IQN\nKorkmaz (2022)\nAdversarial Framework\nALE\nDDQN & A3C\nLin et al. (2017)\nTiming\nALE\nA3C & DQN\nPinto et al. (2017)\nZero-sum game\nMuJoCo\nRARL\nGleave et al. (2020)\nAdversarial Policies\nMuJoCo\nPPO\nKorkmaz (2023)\nNatural Attacks\nALE\nDDQN & A3C\nHuan et al. (2020)\nState Adversarial-DQN\nALE and LM4\nDDQN & PPO\nKorkmaz (2024c)\nDiagnostic Adversarial Volatility\nALE\nDDQN\nunify the state space representations by providing a taxonomy of metrics in reinforcement\nlearning. Several studies proposed different ways to include Kullback-Leibler divergence\nbetween the current policy and the pre-updated policy to add as a regularization term\nin the reinforcement learning objective (Schulman et al., 2015).\nRecently, some studies\nargued that utilizing Kullback-Leibler regularization implicitly averages the state-action\nvalue estimates (Vieillard et al., 2020a).\n6.3 The Adversarial Perspective for Deep Neural Policy Generalization\nOne of the ways to regularize the state observations is based on considering worst-case\nperturbations added to state observations (i.e.\nadversarial perturbations).\nThis line of\nwork starts with introducing perturbations produced by the fast gradient sign method\nproposed by Goodfellow et al. (2015) into deep reinforcement learning observations at test\ntime (Huang et al., 2017; Kos and Song, 2017), and compares the generalization capabilities\nof the trained deep reinforcement learning policies in the presence worst-case perturbations\nand Gaussian noise. These gradient based adversarial methods are based on taking the\ngradient of the cost function used to train the policy with respect to the state observation.\nsadv = s + ϵ ·\n∇xJ(s, Q(s, a))\n∥∇sJ(s, Q(s, a))∥p\n,\nSeveral other techniques have been proposed on the optimization line of the adversarial\nalteration of state observations. In this line of work, Korkmaz (2020) suggested a Nes-\nterov momentum-based method to produce adversarial perturbations for deep reinforcement\n4. Low dimensional state MuJoCo refers to the setting of MuJoCo where the state dimensions are not\nrepresented by pixels and dimensions of the state observations range from 11 to 117.\n13\nKorkmaz\nlearning policies.\nvt+1 = µ · vt +\n∇sadvJ(st\nadv + µ · vt, a)\n∥∇sadvJ(st\nadv + µ · vt, a)∥1\nst+1\nadv = st\nadv + α ·\nvt+1\n∥vt+1∥2\nHere J(sadv, a) is based on the cost function used to train the policy, sadv represents the\nadversarial state observation, and µ is the momentum acceleration parameter. While a line\nof studies focused on optimization aspects of the adversarial perturbations, some studies\ndemonstrated further the hidden linearity of deep reinforcement learning policies by re-\nvealing how these policies learn shared adversarial features across states, MDPs and across\nalgorithms (Korkmaz, 2022).\nIn this work the authors investigate the root causes of this problem, and demonstrate\nthat policy high-sensitivity directions and the perceptual similarity of the state observations\nare uncorrelated. Furthermore, the study demonstrates that the current state-of-the-art\nadversarial training techniques also learn similar high-sensitivity directions as the vanilla\ntrained deep reinforcement learning policies.5 More recently, a line of work proposed the-\noretically founded algorithms to understand the temporal and spatial correlation of deep\nreinforcement learning decision making and what affects this decision making process (Ko-\nrkmaz, 2024c). In particular, this study identifies what precisely affects and contributes to\nthe decision making process of deep reinforcement learning policies from distributional shift\nto worst-case perturbations (i.e. adversarial), from algorithmic differences to architectural\nchanges.\nWhile several studies focused on improving computation techniques to optimize optimal\nperturbations, a line of research focused on making deep neural policies resilient to these\nperturbations. Pinto et al. (2017) proposed to model the dynamics between the adversary\nand the deep neural policy as a zero-sum game (Littman, 1994) where the goal of the\nadversary is to minimize expected cumulative rewards of the deep reinforcement learning\npolicy.\nRagent = Es0∼ρ;aagent∼πagent;aadv∼πadv[\nT−1\nX\nt=0\nragent(s, aagent, aadv)]\nHere the adversarial policy is represented by πadv, the policy of the agent represented by\nπagent, and the rewards received by the agent represented by ragent. The Nash equilibrium\nof the optimal rewards for this zero-sum game is\nRagent∗= min\nπadv max\nπagent Ragent(πagent, πadv) = max\nπagent min\nπadv Ragent(πagent, πadv)\nThis study is a clear example of transition probability perturbation to achieve generaliza-\ntion GP in Definition 3.6 of Section 3. Gleave et al. (2020) approached this problem with an\n5. From the security point of view, this adversarial framework is under the category of black-box adversarial\nattacks for which this is the first study that demonstrated that deep reinforcement learning policies\nare vulnerable to black-box adversarial attacks (Korkmaz, 2022).\nFurthermore, note that black-box\nadversarial perturbations are more generalizable global perturbations that can affect many different\npolicies.\n14\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nFigure 1: Robust adversarial reinforcement learning proposed in (Pinto et al., 2017). This\npaper proposes the zero-sum game to model the relationship between the agent\nand the adversary while focusing on introducing disturbances to the environment\ndynamics. Here the empirical studies are conducted in the MuJoCo environment.\nadversary model which is restricted to take natural actions in the MDP instead of modifying\nthe observations with ℓp-norm bounded perturbations. The authors model this dynamic as\na zero-sum Markov game and solve it via self play Proximal Policy Optimization (PPO).\nSome recent studies, proposed to model the interaction between the adversary and the deep\nreinforcement learning policy as a state-adversarial MDP, and claimed that their proposed\nalgorithm State Adversarial Double Deep Q-Network (SA-DDQN) learns theoretically cer-\ntified robust policies against natural noise and perturbations. In particular, these certified\nadversarial training techniques aim to add a regularizer term to the temporal difference loss\nin deep Q-learning\nH(ri + γ max\na\nˆQˆθ(si, a; θ) −Qθ(si, ai; θ)) + κR(θ)\nwhere H is the Huber loss, ˆQ refers to the target network and κ is to adjust the level of reg-\nularization for convergence. The regularizer term can vary for different certified adversarial\ntraining techniques yet the baseline technique uses R(θ)\nmax{ max\nˆs∈B(s)\nmax\na̸=arg maxa′ Q(s,a′) Qθ(ˆs, a) −Qθ(ˆs, arg max\na′\nQ(s, a′)), −c}.\nwhere B(s) is an ℓp-norm ball of radius ϵ. While these certified adversarial training tech-\nniques drew some attention from the community, more recently manifold concerns have been\nraised on the robustness of theoretically certified adversarially trained deep reinforcement\nlearning policies (Korkmaz, 2021c,b, 2022, 2024a). In these studies, the authors argue that\nadversarially trained (i.e. certified robust) deep reinforcement learning policies learn inac-\ncurate state-action value functions and non-robust features from the environment. More\nimportantly, recently it has been shown that certified robust deep reinforcement learning\npolicies have worse generalization capabilities compared to vanilla trained reinforcement\nlearning policies in high dimensional state space MDPs (Korkmaz, 2023). While this study\nprovides a contradistinction between adversarial and natural directions that are intrinsic to\nthe MDP, it further demonstrates that the certified adversarial training techniques block\ngeneralization capabilities of standard deep reinforcement learning policies. Furthermore\nnote that this study is also a clear example of a state observation perturbation generaliza-\ntion testing method GS\nS in Definition 3.8 in Section 3. For a more comprehensive view on\ngeneralization and robustness see Korkmaz (2024b).\n15\nKorkmaz\nBase State\nShift\nPT\nBlur\nDCT\nB&C\nFigure 2: State transformation generalization under adversarial perspective in the Arcade\nLearning Environment (Korkmaz, 2023). Note that under the adversarial influ-\nence direction of research, the state transformation generalization is constrained\nby the imperceptibility of the transformations. Columns: base frame, shifting,\nperspective transformation, blurring, discrete cosine transform artifacts, bright-\nness and contrast. Up: JamesBond. Down: BankHeist.\nIt is important to observe that the methods that focuses on improving generalization, i.e.\nrobust training, described in this section rarely employ the different generalization testing\nmethods proposed by other work. Thus, focusing narrowly on one aspect of generalization\nwith one dimensional improvements in actuality decreases generalization on another aspect,\nas has been shown in the case of adversarial training (Korkmaz, 2023). Therefore we again\nemphasize the need to understand the significance of a concrete definition of generalization,\nand a unified baseline to precisely measure it.\n7. Meta-Reinforcement Learning and Meta Gradients\nA quite recent line of research directs its research efforts to discovering reinforcement learn-\ning algorithms automatically, without explicitly designing them, via meta-gradients (Oh\net al., 2020; Xu et al., 2020). This line of study targets learning the ”learning algorithm”\nby only interacting with a set of environments as a meta-learning problem. In particular,\nη∗= arg max\nη\nEε∼ρ(ε)Eθ0∼ρ(θ0)[EθN [\n∞\nX\nt=0\nγtrt]]\nhere the optimal update rule is parametrized by η, for a distribution on environments ρ(ε)\nand initial policy parameters ρ(θ0) where EθN [P∞\nt=0 γtrt] is the expected return for the end\nof the lifetime of the agent. The objective of meta-reinforcement learning is to be able to\nbuild agents that can learn how to learn over time, thus allowing these policies to adapt to\na changing environment or even any other changing conditions of the MDP.\nQuite recently, a significant line of research has been conducted to achieve this objective,\nparticularly Oh et al. (2020) proposes to discover update rules for reinforcement learning.\nThis line of work also falls under the algorithmic generalization GA in Definition 3.3 defined\nin Section 3. Following this work Xu et al. (2020) proposed a joint meta-learning frame-\nwork to learn what the policy should predict and how these predictions should be used in\n16\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nFigure 3: Meta training of the learned policy gradient that have been described in (Oh\net al., 2020). Right: The learned policy gradient algorithm that has been trained\nin toy examples can generalize to more complex environment such as the Arcade\nLearning Environment.\nupdating the policy. Recently, Kirsch et al. (2022) proposes to use symmetry information\nin discovering reinforcement learning algorithms and discusses meta-generalization. There\nis also some work on enabling reinforcement learning algorithms to discover temporal ab-\nstractions (Veeriah et al., 2021). In particular, temporal abstraction refers to the ability of\nthe policy to abstract a sequence of actions to achieve certain sub-tasks. As it is promised\nwithin this subfield, meta-reinforcement learning is considered to be a research direction\nthat could enable us to build deep reinforcement learning policies that can generalize to\ndifferent environments, to changing environments over time, or even to different tasks.\n8. Transfer in Reinforcement Learning\nTransfer in reinforcement learning is a subfield heavily discussed in certain applications of\nreinforcement learning algorithms, e.g. robotics. In current robotics research there is not\na safe way of training a reinforcement learning agent by letting the robot explore in real\nlife. Hence, the way to overcome this is to train policies in a simulated environment, and\ninstall the trained policies in the actual application setting. The fact that the simulation\nenvironment and the installation environment are not identical is one of the main problems\nfor reinforcement learning application research. This is referred to as the sim-to-real gap.\nAnother subfield in reinforcement learning research focusing on obtaining generalizable\npolicies investigates this concept through transfer in reinforcement learning. The consider-\nation in this line of research is to build policies that are trained for a particular task with\nlimited data and to try to make these policies perform well on slightly different tasks. An\ninitial discussion on this starts with Taylor and Stone (2007) to obtain policies initially\ntrained in a source task and transferred to a target task in a more sample efficient way.\nLater, Tirinzoni et al. (2018) proposes to transfer value functions that are based on learning\na prior distribution over optimal value functions from a source task. However, this study is\nconducted in simple environments with low dimensional state spaces. Barreto et al. (2017)\nconsiders the reward transformation setting GR in Definition 3.4 from Section 3. In partic-\nular, the authors consider a policy transfer between a specific task with a reward function\nr(s, a) and a different task with reward function r′(s, a). The goal of the study is to decouple\n17\nKorkmaz\nFigure 4: Transfer in reinforcement learning as has been described in (Gamrian and Gold-\nberg, 2019) that falls under the generalization through observation category ex-\nplained in Definition 3.5. The frames are taken from Breakout game in the Arcade\nLearning Environment. The left frames represent the target task and the right\nframes represents the source tasks generated via generative adversarial networks.\nthe state representations from the task. In the setting of state transformation for general-\nization GS in Definition 3.5 Gamrian and Goldberg (2019) focuses on state-wise differences\nbetween source and target task. In particular, the authors use unaligned generative adver-\nsarial networks to create target task states from source task states. In the setting of policy\ntransformation for generalization Gπ in Definition 3.7 Jain et al. (2020) focuses on zero-shot\ngeneralization to a newly introduced action set to increase adaptability. While transfer\nlearning is a promising research direction for reinforcement learning, the studies in this\nsubfield still remain oriented only towards reinforcement learning applications, and thus\nthe main focus on applications centered on this subfield provides a non-unified progress\nin research due to the lack of an established baseline in which the proposed claims and\nalgorithms can be consistently compared.\n9. Lifelong Reinforcement Learning\nLifelong learning is a subfield closely related to transfer learning that has recently drawn\nattention from the reinforcement learning community. Lifelong learning aims to build poli-\ncies that can sequentially solve different tasks by being able to transfer knowledge between\ntasks. On this line of research, Lecarpentier et al. (2021) provide an algorithm for value-\nbased transfer in the Lipschitz continuous task space with theoretical contributions for\nlifelong learning goals. In the setting of action transformation for generalization Gπ in Def-\ninition 3.7 Chandak et al. (2020) focuses on temporally varying (e.g. variations between\nsource task and target task) the action set in lifelong learning. In lifelong reinforcement\nlearning some studies focus on different exploration strategies. In particular, Garcia and\nThomas (2019) models the exploration strategy problem for lifelong learning as another\nMDP, and the study uses a separate reinforcement learning agent to find an optimal ex-\nploration method for the initial lifelong learning agent. The lack of benchmarks limits the\nprogress of lifelong reinforcement learning research by restricting the direct comparison be-\ntween proposed algorithms or methods. However, quite recent work proposed a new training\nenvironment benchmark based on robotics applications for lifelong learning to overcome this\nissue (Wolczyk et al., 2021)6.\n6. The state dimension for this benchmark is 12. Hence, the state space is low dimensional.\n18\nA Survey Analyzing Generalization in Deep Reinforcement Learning\n10. Inverse Reinforcement Learning\nInverse reinforcement learning focuses on learning a functioning policy in the absence of\na reward function. Since the real reward function is inaccessible in this setting and the\nreward function needs to be learnt from observing an expert completing the given task,\nthe inverse reinforcement learning setting falls under the reward transformation for gener-\nalization setting GR defined in Definition 3.4 in Section 3. The initial work that introduced\ninverse reinforcement learning was proposed by Ng and Russell (2000) demonstrating that\nmultiple different reward functions can be constructed for an observed optimal policy. The\nauthors of this initial study achieve this objective via linear programming,\nmax\nX\ns∈Sρ\nmin\na∈A{p(Es′∼P(s,a1|·)Vπ(s′) −Es′∼P(s,a|·)Vπ(s′))}\ns.t. |αi| ≤1 , i = 1, 2, . . . , d\nwhere p(x) = x if x ≥0, p(x) = 2x otherwise and Vπ = α1Vπ\n1 + α2Vπ\n2 + · · · + αdVπ\nd . In\nthis line of work, there has been recent progress that achieved learning functioning policies\nin high-dimensional state observation MDPs (Garg et al., 2021). The study achieves this\nby learning a soft Q-function from observing expert demonstrations, and the study further\nargues that it is possible to recover rewards from the learnt soft state-action value function.\n11. Conclusion\nIn this paper we tried to answer the following questions: (i) What are the explicit problems\nlimiting reinforcement learning algorithms from obtaining high-performing policies that can\ngeneralize to complex environments? (ii) How can we unify and categorize the concept of\ngeneralization in deep reinforcement learning considering many subfields under reinforce-\nment learning at their core focus on the same objective? (iii) What are the similarities\nand differences of these different techniques proposed by different subfields of reinforcement\nlearning research to build reinforcement learning policies that can robustly generalize? To\nanswer these questions first we introduce a theoretical analysis and mathematical framework\nto unify and categorize the concept of generalization in deep reinforcement learning. Then\nwe explain the connection and the significance of exploration in overfitting to a learning\nenvironment, and explain the manifold causes of overestimation bias in reinforcement learn-\ning. Starting from all the different regularization techniques in either state representations\nor in learnt value functions from worst-case to average-case, we provide a current layout of\nthe wide range of reinforcement learning subfields that are essentially working towards the\nsame objective, i.e. generalizable deep reinforcement learning policies. Finally, we provided\na discussion for each category on the drawbacks and advantages of these algorithms. We\nbelieve our study can provide a compact unifying formalization on recent reinforcement\nlearning generalization research. We believe our theoretical framework can guide current\nand future research to build deep reinforcement learning agents that can robustly generalize\nto complex environments.\n19\nKorkmaz\nReferences\nAgarwal, R., Machado, M. C., Castro, P. S., and Bellemare, M. G. (2021a). Contrastive\nbehavioral similarity embeddings for generalization in reinforcement learning. In Inter-\nnational Conference on Learning Representations (ICLR).\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. G. (2021b).\nDeep reinforcement learning at the edge of the statistical precipice.\nIn Ranzato, M.,\nBeygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W., editors, Advances in\nNeural Information Processing Systems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 29304–\n29320.\nAmit, R., Meir, R., and Ciosek, K. (2020). Discount factor as a regularizer in reinforcement\nlearning. In International Conference on Machine Learning (ICML).\nAnschel, O., Baram, N., and Shimkin, N. (2017). Averaged-dqn: Variance reduction and\nstabilization for deep reinforcement learning. In International Conference on Machine\nLearning (ICML).\nBarreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., Silver, D., and van Has-\nselt, H. (2017). Successor features for transfer in reinforcement learning. In Guyon, I.,\nvon Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and\nGarnett, R., editors, Advances in Neural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 4055–4065.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning\nenvironment: An evaluation platform for general agents. Journal Artificial Intelligence\nResearch (JAIR), 47:253–279.\nBellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos,\nR. (2016). Unifying count-based exploration and intrinsic motivation. In Lee, D. D.,\nSugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R., editors, Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural Information Process-\ning Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1471–1479.\nBellman, R. (1957). Dynamic programming. Princeton University Press, Princeton.\nBellman, R. and Dreyfus, S. (1959). Functional approximation and dynamic programming.\nMathematical Tables and Other Aids to Computation.\nBoyan, J. A. and Moore, A. W. (1994). Generalization in reinforcement learning: Safely\napproximating the value function. In Tesauro, G., Touretzky, D. S., and Leen, T. K., ed-\nitors, Advances in Neural Information Processing Systems 7, [NIPS Conference, Denver,\nColorado, USA, 1994], pages 369–376. MIT Press.\nChandak, Y., Theocharous, G., Nota, C., and Thomas, P. S. (2020). Lifelong learning with\na changing action set. In AAAI Conference on Artificial Intelligence, AAAI .\n20\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nCobbe, K., Hesse, C., Hilton, J., and Schulman, J. (2020). Leveraging procedural gen-\neration to benchmark reinforcement learning. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research, pages 2048–2056. PMLR.\nCobbe, K., Hilton, J., Klimov, O., and Schulman, J. (2021). Phasic policy gradient. In Meila,\nM. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine\nLearning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of\nMachine Learning Research, pages 2020–2027. PMLR.\nCobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. (2019). Quantifying gen-\neralization in reinforcement learning. In International Conference on Machine Learning\n(ICML).\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine,\nS. (2020). Emergent complexity and zero-shot transfer via unsupervised environment\ndesign. In Advances in Neural Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nFawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain, M., Novikov,\nA., Ruiz, F. J. R., Schrittwieser, J., Swirszcz, G., Silver, D., Hassabis, D., and Kohli, P.\n(2022). Discovering faster matrix multiplication algorithms with reinforcement learning.\nNature, 610(7930):47–53.\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos,\nR., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. (2018). Noisy networks for\nexploration. International Conference on Learning Representations (ICLR).\nFujimoto, S., van Hoof, H., and Meger, D. (2018). Addressing function approximation error\nin actor-critic methods. In International Conference on Machine Learning (ICML).\nGamrian, S. and Goldberg, Y. (2019). Transfer learning for related reinforcement learning\ntasks via image-to-image translation. In International Conference on Machine Learning\n(ICML).\nGarcia, F. M. and Thomas, P. S. (2019). A meta-mdp approach to exploration for lifelong\nreinforcement learning. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc,\nF., Fox, E. B., and Garnett, R., editors, Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5692–5701.\nGarg, D., Chakraborty, S., Cundy, C., Song, J., and Ermon, S. (2021). Iq-learn: Inverse\nsoft-q learning for imitation. Neural Information Processing Systems (NeurIPS) [Spotlight\nPresentation].\nGleave, A., Dennis, M., Wild, C., Neel, K., Levine, S., and Russell, S. (2020). Adversarial\npolicies: Attacking deep reinforcement learning. International Conference on Learning\nRepresentations (ICLR).\n21\nKorkmaz\nGoodfellow, I., Shelens, J., and Szegedy, C. (2015). Explaning and harnessing adversarial\nexamples. International Conference on Learning Representations (ICLR).\nGoogle Gemini (2023). Gemini: A family of highly capable multimodal models. Technical\nReport, https://arxiv.org/abs/2312.11805.\nHamrick, J., Bapst, V., SanchezGonzalez, A., Pfaff, T., Weber, T., Buesing, L., and\nBattaglia, P. (2020). Combining q-learning and search with amortized value estimates.\nIn 8th International Conference on Learning Representations, ICLR.\nHasselt, H. v., Guez, A., and Silver, D. (2016). Deep reinforcement learning with double\nq-learning. AAAI Conference on Artificial Intelligence, AAAI.\nHouthooft, R., Chen, X., Duan, Y., Schulman, J., Turck, F. D., and Abbeel, P. (2016).\nVIME: variational information maximizing exploration. In Lee, D. D., Sugiyama, M.,\nvon Luxburg, U., Guyon, I., and Garnett, R., editors, Advances in Neural Information\nProcessing Systems 29: Annual Conference on Neural Information Processing Systems\n2016, December 5-10, 2016, Barcelona, Spain, pages 1109–1117.\nHuan, Z., Hongge, C., Chaowei, X., Li, B., Boning, M., Liu, D., and Hsiesh, C. (2020). Ro-\nbust deep reinforcement learning against adversarial perturbations on state observatons.\nConference on Neural Information Processing Systems (NeurIPS).\nHuang, S., Papernot, N., Goodfellow, Ian an Duan, Y., and Abbeel, P. (2017). Adversarial\nattacks on neural network policies. International Conference on Learning Representations\n(ICLR).\nIgl, M., Ciosek, K., Li, Y., Tschiatschek, S., Zhang, C., Devlin, S., and Hofmann, K. (2019).\nGeneralization in reinforcement learning with selective noise injection and information\nbottleneck. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc, F., Fox,\nE. B., and Garnett, R., editors, Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages 13956–13968.\nIII, L. C. B. (1995). Residual algorithms: Reinforcement learning with function approxi-\nmation. In Prieditis, A. and Russell, S., editors, Machine Learning, Proceedings of the\nTwelfth International Conference on Machine Learning, Tahoe City, California, USA,\nJuly 9-12, 1995, pages 30–37. Morgan Kaufmann.\nIoffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. In Bach, F. R. and Blei, D. M., editors, Proceedings of\nthe 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11\nJuly 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 448–456.\nJMLR.org.\nJain, A., Szot, A., and Lim, J. J. (2020). Generalization to new actions in reinforcement\nlearning. In Proceedings of the 37th International Conference on Machine Learning, ICML\n2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning\nResearch, pages 4661–4672. PMLR.\n22\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nKakade, S. (2003). On the sample complexity of reinforcement learning. In PhD Thesis.\nKapturowski, S., Campos, V., Jiang, R., Rakicevic, N., van Hasselt, H., Blundell, C.,\nand Badia, A. P. (2023). Human-level atari 200x faster. In The Eleventh International\nConference on Learning Representations, ICLR 2023.\nKirsch, L., Flennerhag, S., van Hasselt, H., Friesen, A. L., Oh, J., and Chen, Y. (2022).\nIntroducing symmetries to black box meta reinforcement learning. In AAAI Conference\non Artificial Intelligence, AAAI.\nKorkmaz, E. (2020). Nesterov momentum adversarial perturbations in the deep reinforce-\nment learning domain. International Conference on Machine Learning (ICML) Work-\nshop.\nKorkmaz, E. (2021a). Adversarial training blocks generalization in neural policies. Inter-\nnational Conference on Learning Representation (ICLR) Robust and Reliable Machine\nLearning in the Real World Workshop.\nKorkmaz, E. (2021b). Investigating vulnerabilities of deep neural policies. In de Campos,\nC. P., Maathuis, M. H., and Quaeghebeur, E., editors, Proceedings of the Thirty-Seventh\nConference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event, 27-30 July\n2021, volume 161 of Proceedings of Machine Learning Research, pages 1661–1670. AUAI\nPress.\nKorkmaz, E. (2021c). Non-robust feature mapping in deep reinforcement learning. Inter-\nnational Conference on Machine Learning (ICML) Adversarial Machine Learning Work-\nshop.\nKorkmaz, E. (2022). Deep reinforcement learning policies learn shared adversarial features\nacross mdps. AAAI Conference on Artificial Intelligence, AAAI.\nKorkmaz, E. (2023). Adversarial robust deep reinforcement learning requires redefining\nrobustness. AAAI Conference on Artificial Intelligence, AAAI.\nKorkmaz, E. (2024a). Adversarial Robust Deep Reinforcement Learning is Neither Robust\nnor Safe. Conference on Neural Information Processing Systems (NeurIPS) Workshop on\nStatistical Foundations of LLMs and Foundation Models.\nKorkmaz, E. (2024b). Principled Analysis of Machine Learning Paradigms. PhD Thesis.\nKorkmaz, E. (2024c). Understanding and diagnosing deep reinforcement learning. In Pro-\nceedings of the 41st International Conference on Machine Learning ICML, Proceedings\nof Machine Learning Research (PMLR). PMLR.\nKorkmaz, E. and Brown-Cohen, J. (2023). Detecting adversarial directions in deep rein-\nforcement learning to make robust decisions. In International Conference on Machine\nLearning, ICML 2023, volume 202 of Proceedings of Machine Learning Research, pages\n17534–17543. PMLR.\n23\nKorkmaz\nKos, J. and Song, D. (2017). Delving into adversarial attacks on deep policies. International\nConference on Learning Representations (ICLR).\nLan, C. L., Bellemare, M. G., and Castro, P. S. (2021). Metrics and continuity in reinforce-\nment learning. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,\nVirtual Event, February 2-9, 2021, pages 8261–8269. AAAI Press.\nLaskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. (2020a). Reinforce-\nment learning with augmented data. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,\nM., and Lin, H., editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nLaskin, M., Srinivas, A., and Abbeel, P. (2020b). CURL: contrastive unsupervised repre-\nsentations for reinforcement learning. In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Pro-\nceedings of Machine Learning Research, pages 5639–5650. PMLR.\nLecarpentier, E., Abel, D., Asadi, K., Jinnai, Y., Rachelson, E., and Littman, M. L. (2021).\nLipschitz lifelong reinforcement learning. In Thirty-Fifth AAAI Conference on Artifi-\ncial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 8270–8278.\nAAAI Press.\nLee, K., Laskin, M., Srinivas, A., and Abbeel, P. (2021).\nSUNRISE: A simple unified\nframework for ensemble learning in deep reinforcement learning. In Meila, M. and Zhang,\nT., editors, Proceedings of the 38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning\nResearch, pages 6131–6141. PMLR.\nLee, K., Lee, K., Shin, J., and Lee, H. (2020). Network randomization: A simple technique\nfor generalization in deep reinforcement learning. In International Conference on Learning\nRepresentations (ICLR).\nLin, L.-J. (1993). Reinforcement learning for robots using neural networks. Technical report.\nLin, Y.-C., Zhang-Wei, H., Liao, Y.-H., Shih, M.-L., Liu, i.-Y., and Sun, M. (2017). Tactics\nof adversarial attack on deep reinforcement learning agents. IJCAI.\nLittman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learn-\ning.\nIn Cohen, W. W. and Hirsh, H., editors, Machine Learning, Proceedings of the\nEleventh International Conference, Rutgers University, New Brunswick, NJ, USA, July\n10-13, 1994, pages 157–163. Morgan Kaufmann.\nLiu, Z., Li, X., and Darrell, T. (2021).\nRegularization matters in policy optimization\n- an empirical study on continuous control.\nIn International Conference on Learning\nRepresentations (ICLR).\n24\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nMahankali, S., Hong, Z., Sekhari, A., Rakhlin, A., and Agrawal, P. (2024). Random latent\nexploration for deep reinforcement learning. In Forty-first International Conference on\nMachine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net.\nMalik, D., Li, Y., and Ravikumar, P. (2021). When is generalizable reinforcement learning\ntractable? In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan,\nJ. W., editors, Advances in Neural Information Processing Systems 34: Annual Confer-\nence on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pages 8032–8045.\nMankowitz, D. J., Michi, A., Zhernov, A., Gelmi, M., Selvi, M., Paduraru, C., Leurent,\nE., Iqbal, S., Lespiau, J., Ahern, A., K¨oppe, T., Millikin, K., Gaffney, S., Elster, S.,\nBroshear, J., Gamble, C., Milan, K., Tung, R., Hwang, M., Cemgil, T., Barekatain, M.,\nLi, Y., Mandhane, A., Hubert, T., Schrittwieser, J., Hassabis, D., Kohli, P., Riedmiller,\nM. A., Vinyals, O., and Silver, D. (2023). Faster sorting algorithms discovered using deep\nreinforcement learning. Nature, 618(7964):257–263.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, a. G., Graves,\nA., Riedmiller, M., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,\nAntonoglou, King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).\nHuman-level control through deep reinforcement learning. Nature, 518:529–533.\nNg, A. Y. and Russell, S. J. (2000).\nAlgorithms for inverse reinforcement learning.\nIn\nLangley, P., editor, Proceedings of the Seventeenth International Conference on Machine\nLearning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000,\npages 663–670.\nOh, J., Hessel, M., Czarnecki, W. M., Xu, Z., van Hasselt, H., Singh, S., and Silver, D.\n(2020). Discovering reinforcement learning algorithms. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Process-\ning Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nOpenAI (2023). Gpt-4 technical report. CoRR.\nOsband, I., Blundell, C., Pritzel, A., and Roy, B. V. (2016a). Deep exploration via boot-\nstrapped DQN. In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett,\nR., editors, Advances in Neural Information Processing Systems 29: Annual Conference\non Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,\npages 4026–4034.\nOsband, I., Roy, B. V., and Wen, Z. (2016b). Generalization and exploration via randomized\nvalue functions. In International Conference on Machine Learning (ICML).\nPinto, L., Davidson, J., Sukthankar, R., and Gupta, A. (2017).\nRobust adversarial re-\ninforcement learning. In Precup, D. and Teh, Y. W., editors, Proceedings of the 34th\nInternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,\n6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2817–\n2826. PMLR.\n25\nKorkmaz\nRaileanu, R. and Fergus, R. (2021).\nDecoupling value and policy for generalization in\nreinforcement learning. In International Conference on Machine Learning (ICML).\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A.,\nLockhart, E., Hassabis, D., Graepel, T., Lillicrap, T. P., and Silver, D. (2020). Mastering\natari, go, chess and shogi by planning with a learned model. Nat., 588(7839):604–609.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M. I., and Moritz, P. (2015). Trust region\npolicy optimization. In Bach, F. R. and Blei, D. M., editors, Proceedings of the 32nd In-\nternational Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference Proceedings, pages 1889–1897. JMLR.org.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert,\nT., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T. P., Hui, F., Sifre, L., van den\nDriessche, G., Graepel, T., and Hassabis, D. (2017). Mastering the game of go without\nhuman knowledge. Nat., 550(7676):354–359.\nSutton, R. (1984).\nTemporal credit assignment in reinforcement learning.\nPhD Thesis\nUniversity of Massachusetts Amherst.\nSutton, R. (1988). Learning to predict by the methods of temporal difference. Machine\nLearning.\nSutton, R. S., McAllester, D. A., Singh, S., and Mansour, Y. (1999).\nPolicy gradient\nmethods for reinforcement learning with function approximation. In Solla, S. A., Leen,\nT. K., and M¨uller, K., editors, Advances in Neural Information Processing Systems 12,\n[NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999], pages\n1057–1063. The MIT Press.\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus,\nR. (2014). Intriguing properties of neural networks. International Conference on Learning\nRepresentations (ICLR).\nTaylor, M. E. and Stone, P. (2007). Cross-domain transfer for reinforcement learning. In\nGhahramani, Z., editor, Machine Learning, Proceedings of the Twenty-Fourth Interna-\ntional Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007, volume 227\nof ACM International Conference Proceeding Series, pages 879–886. ACM.\nThrun, S. and Schwartz, A. (1993). Issues in using function approximation for reinforcement\nlearning. In Fourth Connectionist Models Summer School.\nTirinzoni, A., Rodr´ıguez-S´anchez, R., and Restelli, M. (2018). Transfer of value functions via\nvariational methods. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-\nBianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems\n31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montr´eal, Canada, pages 6182–6192.\nTodorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,\npages 5026–5033. IEEE.\n26\nA Survey Analyzing Generalization in Deep Reinforcement Learning\nvan Hasselt, H. (2010). Double q-learning. In Lafferty, J. D., Williams, C. K. I., Shawe-\nTaylor, J., Zemel, R. S., and Culotta, A., editors, Advances in Neural Information Pro-\ncessing Systems 23: 24th Annual Conference on Neural Information Processing Systems\n2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia,\nCanada, pages 2613–2621. Curran Associates, Inc.\nVeeriah, V., Zahavy, T., Hessel, M., Xu, Z., Oh, J., Kemaev, I., van Hasselt, H., Silver, D.,\nand Singh, S. (2021). Discovery of options via meta-learned subgoals. In Ranzato, M.,\nBeygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W., editors, Advances in\nNeural Information Processing Systems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 29861–\n29873.\nVieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M. (2020a).\nLeverage the average: an analysis of KL regularization in reinforcement learning.\nIn\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nVieillard, N., Pietquin, O., and Geist, M. (2020b). Munchausen reinforcement learning. In\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi,\nD. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I.,\nHuang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M., Vezhnevets, A. S., Leblond,\nR., Pohlen, T., Dalibard, V., Budden, D., Sulsky, Y., Molloy, J., Paine, T. L., G¨ul¸cehre,\nC¸., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., W¨unsch, D., McKinney, K.,\nSmith, O., Schaul, T., Lillicrap, T. P., Kavukcuoglu, K., Hassabis, D., Apps, C., and Sil-\nver, D. (2019). Grandmaster level in starcraft II using multi-agent reinforcement learning.\nNat., 575(7782):350–354.\nWang, K., Kang, B., Shao, J., and Feng, J. (2020). Improving generalization in reinforce-\nment learning with mixture regularization. In Larochelle, H., Ranzato, M., Hadsell, R.,\nBalcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016).\nDueling network architectures for deep reinforcement learning. In Balcan, M. and Wein-\nberger, K. Q., editors, Proceedings of the 33nd International Conference on Machine\nLearning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR\nWorkshop and Conference Proceedings, pages 1995–2003. JMLR.org.\nWatkins, C. (1989). Learning from delayed rewards. In PhD thesis, Cambridge.\n27\nKorkmaz\nWolczyk, M., Zajac, M., Pascanu, R., Kucinski, L., and Milos, P. (2021). Continual world: A\nrobotic benchmark for continual reinforcement learning. In Ranzato, M., Beygelzimer, A.,\nDauphin, Y. N., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information\nProcessing Systems 34: Annual Conference on Neural Information Processing Systems\n2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 28496–28510.\nXu, Z., van Hasselt, H. P., Hessel, M., Oh, J., Singh, S., and Silver, D. (2020). Meta-gradient\nreinforcement learning with an objective discovered online. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nYarats, D., Kostrikov, I., and Fergus, R. (2021).\nImage augmentation is all you need:\nRegularizing deep reinforcement learning from pixels. In International Conference on\nLearning Representations (ICLR).\n28\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2024-01-04",
  "updated": "2024-10-30"
}