{
  "id": "http://arxiv.org/abs/cs/0111064v1",
  "title": "A procedure for unsupervised lexicon learning",
  "authors": [
    "Anand Venkataraman"
  ],
  "abstract": "We describe an incremental unsupervised procedure to learn words from\ntranscribed continuous speech. The algorithm is based on a conservative and\ntraditional statistical model, and results of empirical tests show that it is\ncompetitive with other algorithms that have been proposed recently for this\ntask.",
  "text": "arXiv:cs/0111064v1  [cs.CL]  30 Nov 2001\nA procedure for unsupervised lexicon learning\nAnand Venkataraman\nanand@speech.sri.com\nSpeech Technology and Research Lab\nSRI International\n333 Ravenswood Ave\nMenlo Park, CA 94025\nAbstract\nWe describe an incremental unsupervised\nprocedure to learn words from transcribed\ncontinuous speech.\nThe algorithm is based\non a conservative and traditional statistical\nmodel, and results of empirical tests show\nthat it is competitive with other algorithms\nthat have been proposed recently for this\ntask.\n1. Introduction\nEnglish speech lacks the acoustic analog of blank\nspaces that people are accustomed to seeing between\nwords in written text. Discovering words in continu-\nous spoken speech then is an interesting problem that\nhas been treated at length in the literature. The issue\nis also particularly prominent in the parsing of written\ntext in languages that do not explicitly include spaces\nbetween words.\nIn this paper, we describe an incremental unsuper-\nvised algorithm based on a formal statistical model\nto infer word boundaries from continuous speech. The\nmain contributions of this study are as follows: First,\nit demonstrates the applicability and competitiveness\nof a conservative traditional approach for a task for\nwhich nontraditional approaches have been proposed\neven recently (?; ?; ?; ?; ?).\nSecond, although the\nmodel leads to the development of an algorithm that\nlearns the lexicon in an unsupervised fashion, results\nof partial supervision are also presented, showing that\nits performance is consistent with results from learning\ntheory.\n2. Related Work\nWhile there exists a reasonable body of literature with\nregard to word discovery and text segmentation, espe-\ncially with respect to languages such as Chinese and\nJapanese, which do not explicitly include spaces be-\ntween words, most of the statistically based models\nand algorithms tend to fall into the supervised learning\ncategory. These require the model to ﬁrst be trained\non a large corpus of text before it can segment its\ninput.1\nIt is only of late that interest in unsuper-\nvised algorithms for text segmentation seems to have\ngained ground. In the last ANLP/NAACL joint lan-\nguage technology conference, ? (?) proposed an al-\ngorithm to infer word boundaries from character n-\ngram statistics of Japanese Kanji strings. For exam-\nple, a decision to insert a word boundary between two\ncharacters is made solely based on whether charac-\nter n-grams adjacent to the proposed boundary are\nrelatively more frequent than character n-grams that\nstraddle it. However, even this algorithm is not truly\nunsupervised. There is a threshold parameter involved\nthat must be tuned in order to get optimal segmenta-\ntions when single character words are present. Also,\nthe number of orders of n-grams that are signiﬁcant in\nthe segmentation decision making process is a tunable\nparameter. The authors state that these parameters\ncan be set with a very small number of pre-segmented\ntraining examples, as a consequence of which they call\ntheir algorithm mostly unsupervised. A further factor\ncontributing to the incommensurability of their algo-\nrithm with our approach is that it is not immediately\nobvious how to adapt their algorithm to operate in-\ncrementally. Their procedure is more suited to batch\nsegmentation, where corpus n-gram statistics can be\nobtained during a ﬁrst pass and segmentation deci-\nsions made during the second. Our algorithm, how-\never, is purely incremental and unsupervised and does\nnot need to make multiple passes over the data, nor re-\nquire tunable parameters to be set from training data\nbeforehand. In this respect, it is most similar to Model\nBased Dynamic Programming, hereafter referred to as\nMBDP-1, which has been proposed in (?).\nTo the\n1See, for example, ? (?) for a survey and ? (?) for the\nmost recent such approach.\nauthor’s knowledge, MBDP-1 is probably the most\nrecent and only other completely unsupervised work\nthat attempts to discover word boundaries from un-\nsegmented speech data. Both the approach presented\nin this paper and MBDP-1 are based on explicit prob-\nability models. As the name implies, MBDP-1 uses\ndynamic programming to infer the best segmentation\nof the input corpus. It is assumed that the entire input\ncorpus, consisting of a concatenation of all utterances\nin sequence, is a single event in probability space and\nthat the best segmentation of each utterance is implied\nby the best segmentation of the corpus itself.\nThe\nmodel thus focuses on explicitly calculating probabili-\nties for every possible segmentation of the entire cor-\npus, subsequently picking the segmentation with the\nmaximum probability. More precisely, the model at-\ntempts to calculate\nP( ¯wm) =\nX\nn\nX\nL\nX\nf\nX\ns\nP( ¯wm|n, L, f, s) · P(n, L, f, s)\nfor each possible segmentation of the input corpus\nwhere the left-hand side is the exact probability of\nthat particular segmentation of the corpus into words\n¯wm = w1w2 · · · wm and the sums are over all possible\nnumbers of words, n, in the lexicon, all possible lex-\nicons, L, all possible frequencies, f, of the individual\nwords in this lexicon and all possible orders of words, s,\nin the segmentation. In practice, the implementation\nuses an incremental approach that computes the best\nsegmentation of the entire corpus up to step i, where\nthe ith step is the corpus up to and including the ith\nutterance. Incremental performance is thus obtained\nby computing this quantity anew after each segmen-\ntation i −1, assuming, however, that segmentations of\nutterances up to but not including i are ﬁxed. Thus,\nalthough the segmentation algorithm itself is incre-\nmental, the formal statistical model of segmentation\nis not.\nFurthermore, making the assumption that the corpus\nis a single event in probability space signiﬁcantly in-\ncreases the computational complexity of the incremen-\ntal algorithm.\nThe approach presented in this pa-\nper circumvents these problems through the use of a\nconservative statistical model that is directly imple-\nmentable as an incremental algorithm. In the follow-\ning sections, we describe the model and the algorithm\nderived from it. The technique can basically be seen\nas an modiﬁcation of Brent’s work, borrowing in par-\nticular his successful dynamic programming approach\nwhile substituting his statistical model with a more\nconservative one.\n3. Model Description\nThe language model described here is fairly standard\nin nature. The interested reader is referred to ?, (?,\np.57–78), where a detailed exposition can be found.\nBasically, we seek\nˆ\nW\n=\nargmax\nW\nP(W)\n(1)\n=\nargmax\nW\nn\nY\ni=1\nP(wi|w1, · · · , wi−1)\n(2)\n=\nargmin\nW\nn\nX\ni=1\n−log P(wi|w1, · · · , wi−1) (3)\nwhere W = w1, · · · , wn denotes a particular string of\nn words.\nEach word is assumed to be made up of\na ﬁnite sequence of characters representing phonemes\nfrom a ﬁnite inventory.\nWe make the unigram approximation that word his-\ntories are irrelevant to their probabilities. This allows\nus to rewrite the right-hand side of Equation 3 as un-\nconditional probabilities. We also employ back-oﬀ(?)\nusing the Witten-Bell technique (?) when novel words\nare encountered. This enables us to use an open vo-\ncabulary and estimate familiar word probabilities from\ntheir relative frequencies in the observed corpus while\nbacking oﬀto the letter level for novel words. In our\ncase, a novel word is decomposed into its constituent\nphonemes and its probability is then calculated as the\nnormalized product of its phoneme probabilities. To\ndo this, we introduce the sentinel phoneme ’#’, which\nis assumed to terminate every word. The model can\nnow be summarized very simply as follows:\nP(w)\n=\n(\nC(wi)\nN+S\nif C(w) > 0\nN\nN+S PΣ(w)\notherwise\n(4)\nPΣ(w)\n=\nr(#)\n|w|\nQ\nj=1\nr(w[j])\n1 −r(#)\n(5)\nwhere C() denotes the count or frequency function,\nN denotes the number of distinct words in the word\ntable, S denotes the sum of their frequencies, |w| de-\nnotes the length of word w, excluding the sentinel ‘#’,\nw[j] denotes its jth phoneme, and r() denotes the rel-\native frequency function. The normalization by divid-\ning using 1−r(#) in Equation (5) is necessary because\notherwise\nX\nw\nP(w)\n=\n∞\nX\ni=1\n(1 −P(#))iP(#)\n(6)\n=\n1 −P(#)\n(7)\nSince we estimate P(w[j]) by r(w[j]), dividing by 1 −\nr(#) will ensure that P\nw P(w) = 1.\n4. Method\nAs in ? (?), the model described in Section 3 is pre-\nsented as an incremental learner.\nThe only knowl-\nedge built into the system at start-up is the phoneme\ntable with a uniform distribution over all phonemes,\nincluding the sentinel phoneme.\nThe learning algo-\nrithm considers each utterance in turn and computes\nthe most probable segmentation of the utterance us-\ning a Viterbi search (?) implemented as a dynamic\nprogramming algorithm described shortly. The most\nlikely placement of word boundaries computed thus is\ncommitted to before considering the next presented ut-\nterance. Committing to a segmentation involves learn-\ning word probabilities as well as phoneme probabilities\nfrom the inferred words.\nThese are used to update\ntheir respective tables. To account for eﬀects that any\nspeciﬁc ordering of input utterances may have on the\nsegmentations that are output, the performance of the\nalgorithm is averaged over 1000 runs, with each run\nreceiving as input a random permutation of the input\ncorpus.\nThe input corpus\nThe corpus, which is identical to the one used by ?\n(?), consists of orthographic transcripts made by ?\n(?)\nfrom the CHILDES collection (?).\nThe speak-\ners in this study were nine mothers speaking freely to\ntheir children, whose ages averaged 18 months (range\n13–21). Brent and his colleagues also transcribed the\ncorpus phonemically (using an ASCII phonemic rep-\nresentation), ensuring that the number of subjective\njudgments in the pronunciation of words was mini-\nmized by transcribing every occurrence of the same\nword identically.\nFor example, “look”, “drink” and\n“doggie” were always transcribed “lUk”, “drINk” and\n“dOgi” regardless of where in the utterance they oc-\ncurred and which mother uttered them in what way.\nThus transcribed, the corpus consists of a total of 9790\nsuch utterances and 33,399 words including one space\nafter each word and one newline after each utterance.\nIt is noteworthy that the choice of this particular cor-\npus for experimentation is motivated purely by its use\nin ? (?). The algorithm is equally applicable to plain\ntext in English or other languages. The main advan-\ntage of the CHILDES corpus is that it allows for ready\nand quick comparison with results hitherto obtained\nand reported in the literature.\nIndeed, the relative\nperformance of all the discussed algorithms is mostly\nunchanged when tested on the 1997 Switchboard tele-\nphone speech corpus with disﬂuency events removed.\n5. Algorithm\nThe dynamic programming algorithm ﬁnds the most\nprobable word sequence for each input utterance by\nassigning to each segmentation a score equal to the\nlogarithm of its probability and committing to the seg-\nmentation with the highest score. In practice, the im-\nplementation computes the negative logarithm of this\nscore and thus commits to the segmentation with the\nleast negative logarithm of the probability.\nThe al-\ngorithm is presented in recursive form in Figure 1 for\nreadability. The actual implementation, however, used\nan iterative version.\nThe algorithm to evaluate the\nback-oﬀprobability of a word is given in Figure 2. Es-\nsentially, the algorithm description can be summed up\nsemiformally as follows: For each input utterance u,\nwhich has either been read in without spaces, or from\nwhich spaces have been deleted, we evaluate every pos-\nsible way of segmenting it as u = u′ + w where u′ is\na subutterance from the beginning of the original ut-\nterance up to some point within it and w, the lexical\ndiﬀerence between u and u′, is treated as a word. The\nsubutterance u′ is itself evaluated recursively using the\nsame algorithm.\nThe base case for recursion when\nthe algorithm rewinds is obtained when a subutter-\nance cannot be split further into a smaller component\nsubutterance and word, that is, when its length is zero.\nSuppose for example, that a given utterance is abcde,\nwhere the letters represent phonemes. If seg(x) rep-\nresents the best segmentation of the utterance x and\nword(x) denotes that x is treated as a word, then\nseg(abcde) = best of\n\n\n\n\n\n\n\n\n\n\n\nword(abcde)\nseg(a) + word(bcde)\nseg(ab) + word(cde)\nseg(abc) + word(de)\nseg(abcd) + word(e)\nThe evalUtterance algorithm in Figure 1 does pre-\ncisely this. It initially assumes the entire input utter-\nance to be a word on its own by assuming a single\nsegmentation point at its right end. It then compares\nthe log probability of this segmentation successively to\nthe log probabilities of segmenting it into all possible\nsubutterance, word pairs. Once the best segmentation\ninto words has been found, then spaces are inserted\ninto the utterance at the inferred points and the seg-\nmented utterance is printed out.\nThe implementation maintains two separate tables in-\nternally, one for words and one for phonemes. When\nthe procedure is initially started, the word table is\nempty.\nOnly the phoneme table is populated with\nequipossible phonemes. As the program considers each\nutterance in turn and commits to its best segmenta-\ntion according to the evalUtterance algorithm, the\ntwo tables are updated correspondingly. For example,\nafter some utterance “abcde” is segmented into “a bc\nde”, the word table is updated to increment the fre-\nquencies of the three entries “a”, “bc” and “de” each\nby 1, and the phoneme table is updated to increment\nthe frequencies of each of the phonemes in the utter-\nance including one sentinel for each word inferred. Of\ncourse, incrementing the frequency of a currently un-\nknown word is equivalent to creating a new entry for\nit with frequency 1.\n5.1 Algorithm: evalUtterance\nBEGIN\nInput (by ref) utterance u[0..n]\nwhere u[i] are the characters in it.\nbestSegpoint := n;\nbestScore := evalWord(u[0..n]);\nfor i from 0 to n-1; do\nsubUtterance := copy(u[0..i]);\nword := copy(u[i+1..n]);\nscore := evalUtterance(subUtterance)\n+ evalWord(word);\nif (score < bestScore); then\nbestScore = score;\nbestSegpoint := i;\nfi\ndone\ninsertWordBoundary(u, bestSegpoint)\nreturn bestScore;\nEND\nFigure 1. Recursive optimization algorithm to ﬁnd the best\nsegmentation of an input utterance using the language\nmodel described in this paper.\nOne can easily see that the running time of the pro-\ngram is O(mn2) in the total number of utterances (m)\nand the length of each utterance (n), assuming an ef-\nﬁcient implementation of a hash table allowing nearly\nconstant lookup time is available. A single run over the\nentire corpus typically completes in under 10 seconds\non a 300 MHz i686-based PC running Linux 2.2.5-15.\nAlthough all the discussed algorithms tend to complete\nwithin one minute on the reported corpus, MBDP-1’s\nrunning time is quadratic in the number of utterances,\nwhile the language model presented here enables com-\nputation in almost linear time. The typical running\ntime of MBDP-1 on the 9790-utterance corpus aver-\nages around 40 seconds per run on a 300 MHz i686\nPC while the algorithm described in this paper aver-\nages around 7 seconds.\n5.2 Function: evalWord\nBEGIN\nInput (by reference) word w[0..k]\nwhere w[i] are the phonemes in it.\nscore := 0;\nN := number of distinct words;\nS := sum of their frequencies;\nif freq(word) == 0; then {\nescape := N/(N+S);\nP_0 := relativeFrequency(’#’);\nscore := -log(esc) -log(P_0/(1-P_0));\nfor each w[i]; do\nscore -= log(relativeFrequency(w[i]));\ndone\n} else {\nP_w := frequency(w)/(N + S);\nscore := -log(P_w);\n}\nreturn score;\nEND\nFigure 2. The function to compute −log P(w) of an input\nword w. If the word is novel, then the function backs oﬀ\nto using a distribution over the phonemes in the word.\n6. Results and Discussion\nIn line with the results reported in ? (?), three scores\nwere calculated — precision, recall and lexicon pre-\ncision. Precision is deﬁned as the proportion of pre-\ndicted words that are actually correct. Recall is de-\nﬁned as the proportion of correct words that were\npredicted. Lexicon precision is deﬁned as the propor-\ntion of words in the predicted lexicon that are correct.\nPrecision and recall scores were computed incremen-\ntally and cumulatively within scoring blocks, each of\nwhich consisted of 100 utterances. We emphasize that\nthe segmentation itself proceeded incrementally, on an\nutterance-by-utterance basis. Only the scores are re-\nported on a per-block basis for brevity. These scores\nwere computed and averaged only for the utterances\nwithin each block scored and thus they represent the\nperformance of the algorithm on the block of utter-\nances scored, occurring in the exact context among\nthe other scoring blocks. Lexicon scores carried over\nblocks cumulatively. As Figures 3 through 5 show, the\nperformance of our algorithm matches that of MBDP-\n1 on all grounds. In fact, we found to our surprise\nthat the performances of both algorithms were almost\nidentical except in a few instances, discussion of which\nspace does not permit here.\nThis leads us to suspect the two, substantially diﬀer-\nent, statistical models may essentially be capturing the\nsame nuances of the domain.\nAlthough ?\n(?)\nex-\nplicitly states that probabilities are not estimated for\nwords, it turns out that considering the entire corpus\n50\n55\n60\n65\n70\n75\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAverage Precision %\nScoring blocks\n’1-gram’\n’MBDP’\nFigure 3. Word discovery precision as a function of number\nof utterances considered. Each scoring block (checkpoint)\nconsists of 10% of the total number of utterances (roughly\n1000). It is hard to discern two separate plots above be-\ncause of the close match in their performance. 1-gram de-\nnotes the performance of the procedure reported in this\npaper whereas MBDP denotes the performance of Brent’s\nModel Based Dynamic Programming algorithm.\nas a single event in probability space does end up hav-\ning the same eﬀect as estimating probabilities from\nrelative frequencies as our statistical model does. The\nrelative probability of a familiar word is given in Equa-\ntion 22 of ? (?) as\nfk(ˆk)\nk\n·\n \nfk(ˆk) −1\nfk(ˆk)\n!2\nwhere k is the total number of words and fk(ˆk) is the\nfrequency at that point in segmentation of the ˆkth\nword. It eﬀectively approximates to the relative fre-\nquency\nfk(ˆk)\nk\nas fk(ˆk) grows. The language model presented in this\npaper explicitly claims to use this speciﬁc estimator\nfor the word probabilities. From this perspective, both\nMBDP-1 and the present model tend to favor the seg-\nmenting out of familiar words that do not overlap. In\nthis context, we are curious to see how the algorithms\nwould fare if in fact the utterances were favorably or-\ndered, that is, in order of increasing length. Clearly,\nthis is an important advantage for both algorithms.\nThe results of experimenting with a generalization of\nthis situation, where instead of ordering the utterances\nfavorably, we treat an initial portion of the corpus as\na training component eﬀectively giving the algorithms\nfree word boundaries after each word, are presented in\nSection 7.\n50\n55\n60\n65\n70\n75\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAverage Recall %\nScoring blocks\n’1-gram’\n’MBDP’\nFigure 4. Word discovery recall as a function of number of\nutterances considered.\n0\n10\n20\n30\n40\n50\n60\n70\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAverage Lexicon Precision %\nScoring blocks\n’1-gram’\n’MBDP’\nFigure 5. Lexicon precision (percentage of correctly in-\nferred words in the lexicon) as a function of number of\nutterances considered.\nIn contrast with MDBP-1, we note that the model pro-\nposed in this paper has been entirely developed along\nconventional lines and has not made the somewhat\nradical assumption of treating the entire observed cor-\npus as a single event in probability space. Assuming\nthat the corpus consists of a single event requires the\nexplicit calculation of the probability of the lexicon in\norder to calculate the probability of any single segmen-\ntation. This calculation is a nontrivial task since one\nhas to sum over all possible orders of words in the lex-\nicon, L. This fact is recognized in ? (?), where the\nexpression for P(L) is derived in Appendix 1 of his pa-\nper as an approximation. One can imagine then that\nit will be correspondingly more diﬃcult to extend the\nlanguage model in ? (?) past the case of unigrams.\nAs a practical issue, recalculating lexicon probabilities\nbefore each segmentation also increases the running\ntime of an implementation of the algorithm.\nFurthermore, the language model presented in this pa-\nper estimates probabilities as relative frequencies using\nthe commonly used back-oﬀprocedure and so they do\nnot assume any priors over integers. However, MBDP-\n1 requires the assumption of two distributions over in-\ntegers, one to pick a number for the size of the lexicon\nand another to pick a frequency for each word in the\nlexicon. Each is assumed such that the probability of\na given integer P(i) is given by\n6\nπ2i2 . We have since\nfound some evidence suggesting that the choice of a\nparticular prior does not have any signiﬁcant advan-\ntage over the choice of any other prior.\nFor exam-\nple, we have tried running MBDP-1 using P(i) = 2−i\nand still obtained comparable results. It is notewor-\nthy, however, that no such subjective prior needs to be\nchosen in the model presented in this paper.\nThe other important diﬀerence between MBDP-1 and\nthe present model is that MBDP-1 assumes a uniform\ndistribution over all possible word orders and explicitly\nderives the probability expression for any particular\nordering. That is, in a corpus that contains nk distinct\nwords such that the frequency in the corpus of the ith\ndistinct word is given by fk(i), the probability of any\none ordering of the words in the corpus is\nQnk\ni=1 fk(i)!\nk!\nbecause the number of unique orderings is precisely the\nreciprocal of the above quantity. In contrast, this in-\ndependence assumption is already implicit in the uni-\ngram language model adopted in the present approach.\nBrent mentions that there may well be eﬃcient ways\nof using n-gram distributions within MBDP-1. How-\never, the framework presented in this paper is a formal\nstatement of a model that lends itself to such easy n-\ngram extensibility using the back-oﬀscheme proposed.\nIt is now a simple matter to include bigrams and tri-\ngrams among the tables being learned. Since back-oﬀ\nhas already been incorporated into the model, we sim-\nply substitute for the probability expression of a word\n(which currently uses no history), the probability ex-\npression given its immediate history (typically n −1\nwords). Thus, we use an expression like\nP(w|h)\n=\n(\nα C(h,w)\nC(h)\nif C(h, w) > 0\n(1 −α)P(w|h′)\notherwise\nwhere P(w|h) denotes the probability of word w condi-\ntioned on its history h, normally the immediately pre-\nvious 1 (for bigrams) or 2 (for trigram) words, α is the\nback-oﬀweight or discount factor, which we may cal-\nculate using any of a number of standard techniques,\nfor example by using the Witten-Bell technique as we\nhave done in this paper, C() denotes the count or fre-\nquency function of its argument in its respective table,\nand h′ denotes reduced history, usually by one word.\nReports of experiments with such extensions can, in\nfact, be found in a forthcoming article (?).\n7. Training\nAlthough we have presented the algorithm as an unsu-\npervised learner, it is interesting to compare its respon-\nsiveness to the eﬀect of training data. Here, we extend\nthe work in ? (?) by reporting the eﬀect of training\nupon the performance of both algorithms. Figures 6\nand 7 plot the results (precision and recall) over the\nwhole input corpus, that is, blocksize = ∞, as a func-\ntion of the initial proportion of the corpus reserved for\ntraining. This is done by dividing the corpus into two\nsegments, with an initial training segment being used\nby the algorithm to learn word and phoneme probabil-\nities and the latter actually being used as the test data.\nA consequence of this is that the amount of data avail-\nable for testing becomes progressively smaller as the\npercentage reserved for training grows. So, the signiﬁ-\ncance of the test would diminish correspondingly. We\nmay assume that the plots cease to be meaningful and\ninterpretable when more than about 75% (about 7500\nutterances) of the corpus is used for training. At 0%,\nthere is no training information for any algorithm, and\nthe performances of the various algorithms are identi-\ncal to those of the unsupervised case. We increase the\namount of training data in steps of approximately 1%\n(100 utterances). For each training set size, the results\nreported are averaged over 25 runs of the experiment,\neach over a separate random permutation of the cor-\npus. The motivation was both to account for ordering\nidiosyncrasies and to smooth the graphs to make them\neasier to interpret.\nWe interpret Figures 6 and 7 as suggesting that the\nperformance of all the discussed algorithms can be\nboosted signiﬁcantly with even a small amount of\ntraining. It is also noteworthy and reassuring to see\nthat, as one would expect from results in computa-\ntional learning theory (?), the number of training ex-\namples required to obtain a desired value of precision,\np, appears to grow with 1/(1 −p).\nSigniﬁcance of single word utterances\nThe results we have obtained provide some insight into\nthe actual learning process, which appears to be one\nin which rapid bootstrapping happens with very lim-\nited data. As we had remarked earlier, all the internal\ntables are initially empty. Thus, the very ﬁrst utter-\n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPrecision %\nPercentage of input used for training\n’1-gram’\n’MBDP’\n100-100/x\nFigure 6. Responsiveness of the algorithm to training in-\nformation. The horizontal axis represents the initial per-\ncentage of the data corpus that was used for training the\nalgorithm. This graph shows the improvement in segmen-\ntation precision with training size.\nance is necessarily segmented as a single novel word.\nThe reason that fewer novel words are preferred ini-\ntially is this:\nSince the word table is empty when\nthe algorithm attempts to segment the ﬁrst utterance,\nbacking-oﬀcauses all probabilities to necessarily be\ncomputed from the level of phonemes up. Thus, the\nmore words in it, the more sentinel characters that will\nbe included in the probability calculation and so that\nmuch lesser will be the corresponding segmentation\nprobability.\nAs the program works its way through\nthe corpus, correctly inferred words, by virtue of their\nrelatively greater preponderance compared to noise,\ntend to dominate the distributions and thus dictate\nhow future utterances are segmented.\nFrom this point of view, we see that the presence of\nsingle word utterances is of paramount importance to\nthe algorithm. Fortunately, very few such utterances\nsuﬃce for good performance, for every correctly in-\nferred word helps in the inference of other words that\nare adjacent to it. This is the role played by training,\nwhose primary use can now be said to be in supplying\nthe word table with seed words. We can now further\nreﬁne our statement about the importance of single\nword utterances. Although single word utterances are\nimportant for the learning task, what are critically im-\nportant are words that occur both by themselves in\nan utterance and in the context of other words after\nthey are ﬁrst seen. This brings up a potentially in-\nteresting issue. Suppose disﬂuencies in speech can be\ninterpreted, in some sense, as free word boundaries.\nWe are then interested in whether their distribution\nin speech is high enough in the vicinity of generally\n86\n88\n90\n92\n94\n96\n98\n100\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nRecall %\nPercentage of input used for training\n’1-gram’\n’MBDP’\nFigure 7. Improvement in segmentation recall with train-\ning size.\nfrequent words. If that is the case, then ums and ahs\nare potentially useful from a cognitive point of view to\na person acquiring a lexicon since these are the very\nevents that will bootstrap his or her lexicon with the\ninitial seed words that are instrumental in the rapid\nacquisition of further words.\n8. Summary\nIn summary, we have presented a formal model of word\ndiscovery in speech transcriptions. The main advan-\ntages of this model over that of ? (?) are, ﬁrst, that\nthe present model has been developed entirely by di-\nrect application of standard techniques and procedures\nin speech processing. Second, the model is easily ex-\ntensible to incorporate more historical detail in the\nusual way.\nThird, the presented model makes few\nassumptions about the nature of the domain and re-\nmains as far as possible conservative and simple in its\ndevelopment. Results from experiments suggest that\nthe algorithm performs competitively with other un-\nsupervised techniques recently proposed for inferring\nwords from transcribed speech. Finally, although the\nalgorithm is originally presented as an unsupervised\nlearner, we have shown the eﬀect that training data\nhas on its performance.\nFuture work\nOther extensions being worked on include the incor-\nporation of more complex phoneme distributions into\nthe model. These are, namely, the biphone and tri-\nphone models. Using the lead from ? (?), attempts to\nmodel more complex distributions for words such as\nthose based on template grammars and the systematic\nincorporation of prosodic, stress and phonotactic con-\nstraint information into the model are also the subject\nof current interest. We already have some unpublished\nresults suggesting that biasing the segmentation using\na constraint that every word must have at least one\nvowel in it dramatically increases segmentation preci-\nsion from 67.7% to 81.8%, and imposing a constraint\nthat words can begin or end only with permitted clus-\nters of consonants increases precision to 80.65%.\nAnother avenue of current research is concerned with\niterative sharpening of the language model wherein\nword probabilities are periodically reestimated using\na ﬁxed number of iterations of the Expectation Mod-\niﬁcation (EM) algorithm (?). Such reestimation has\nbeen found to improve the performance of language\nmodels in other similar tasks. It has also been sug-\ngested that the algorithm could be usefully adapted to\nuser modeling in human-computer interaction, where\nthe task lies in predicting the most likely atomic ac-\ntion a computer user will perform next. However, we\nhave as yet no results or work to report on in this area.\n9. Acknowledgments\nThe author thanks Michael Brent for stimulating his\ninterest in the area. Thanks are also due to Koryn\nGrant for cross-checking the results presented here.\nEleanor Olds Batchelder and Andreas Stolcke oﬀered\nmany constructive comments and useful pointers in\npreparing a revised version of this paper. Anonymous\nreviewers of an initial version helped signiﬁcantly in\nimproving its content and Judy Lee proof-read the ﬁ-\nnal version carefully.\nReferences\nAndo and Lee][April, 2000]Ando:MUS00 Ando, R. K.,\n& Lee, L. (April, 2000). Mostly-unsupervised sta-\ntistical segmentation of japanese: Applications to\nkanji. Proceedings of the ANLP-NAACL. Seattle,\nWA.\nBernstein-Ratner][1987]Bernstein:PPC87\nBernstein-\nRatner, N. (1987). The phonology of parent child\nspeech.\nIn K. Nelson and van A. Kleeck (Eds.),\nChildren’s language, vol. 6. Hillsdale, NJ: Erlbaum.\nBrent][1999]Brent:EPS99 Brent, M. R. (1999). An eﬃ-\ncient probabilistically sound algorithm for segmen-\ntation and word discovery. Machine Learning, 34,\n71–105.\nBrent\nand\nCartwright][1996]Brent:DRP96\nBrent,\nM. R., & Cartwright, T. A. (1996). Distributional\nregularity and phonotactics are useful for segmenta-\ntion. Cognition, 61, 93–125.\nChristiansen et al.][1998]Christiansen:LSS98 Chris-\ntiansen, M. H., Allen, J., & Seidenberg, M. (1998).\nLearning to segment speech using multiple cues: A\nconnectionist model.\nLanguage and cognitive pro-\ncesses, 13, 221–268.\nde Marcken][1995]deMarcken:UAL95 de Marcken, C.\n(1995). Unsupervised acquisition of a lexicon from\ncontinuous speech (Technical Report AI Memo No.\n1558). Massachusetts Institute of Technology, Cam-\nbridge, MA.\nDempster\net al.][1977]Dempster:MLF77 Dempster,\nA. P., Laird, N. M., & Rubin, D. B. (1977). Max-\nimum likelihood from incomplete data via the EM\nalgorithm. Journal of the Royal Statistical Society,\nSeries B, 39, 1–38.\nElman][1990]Elman:FST90 Elman, J. L. (1990). Find-\ning structure in time. Cognitive Science, 14, 179–\n211.\nHaussler][1988]Haussler:QIB88 Haussler, D. (1988).\nQuantifying inductive bias: AI learning algorithms\nand Valiant’s learning framework. Artiﬁcial Intelli-\ngence, 36, 177–221.\nJelinek][1997]Jelinek:SMS97 Jelinek, F. (1997). Sta-\ntistical methods for speech recognition. Cambridge,\nMA: MIT Press.\nKatz][1987]Katz:EPF87 Katz, S. M. (1987).\nEsti-\nmation of probabilities from sparse data for the\nlanguage model component of a speech recognizer.\nIEEE Transactions on Acoustics, Speech and Signal\nProcessing, ASSP-35, 400–401.\nMacWhinney and Snow][1985]MACWHINNEY:CLD85\nMacWhinney, B., & Snow, C. (1985).\nThe child\nlanguage data exchange system. Journal of Child\nLanguage, 12, 271–296.\nTeahan et al.][2000]Teahan:CBA00 Teahan, W. J.,\nWen, Y., McNab, R., & Witten, I. H. (2000). A\ncompression based algorithm for Chinese word seg-\nmentation. Computational Linguistics, 26, 374–393.\nVenkataraman][2001]Venkataraman:ASM01\nVenkataraman, A. (2001).\nA statistical model\nfor word discovery in transcribed speech. Compu-\ntational linguistics, to appear.\nViterbi][1967]Viterbi:EBC67 Viterbi, A. J. (1967). Er-\nror bounds for convolutional codes and an asymp-\ntotically optimal decoding algorithm. IEEE Trans-\nactions on Information Theory, IT-13, 260–269.\nWitten and Bell][1991]Witten:ZFP91 Witten, I. H., &\nBell, T. C. (1991). The zero-frequency problem: Es-\ntimating the probabilities of novel events in adaptive\ntext compression. IEEE Transactions on Informa-\ntion Theory, 37, 1085–1091.\nZimin and Tseng][1993]Zimin98:CTS93 Zimin, W., &\nTseng, G. (1993).\nChinese text segmentation for\ntext retrieval problems and achievements. JASIS,\n44, 532–542.\n60\n65\n70\n75\n80\n85\n90\n95\n100\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nLexicon Precision %\nPercentage of input used for training\n’1-gram’\n’MBDP’\n",
  "categories": [
    "cs.CL",
    "I.2.6;I.2.7"
  ],
  "published": "2001-11-30",
  "updated": "2001-11-30"
}