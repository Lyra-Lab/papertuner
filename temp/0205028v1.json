{
  "id": "http://arxiv.org/abs/cs/0205028v1",
  "title": "NLTK: The Natural Language Toolkit",
  "authors": [
    "Edward Loper",
    "Steven Bird"
  ],
  "abstract": "NLTK, the Natural Language Toolkit, is a suite of open source program\nmodules, tutorials and problem sets, providing ready-to-use computational\nlinguistics courseware. NLTK covers symbolic and statistical natural language\nprocessing, and is interfaced to annotated corpora. Students augment and\nreplace existing components, learn structured programming by example, and\nmanipulate sophisticated models from the outset.",
  "text": "arXiv:cs/0205028v1  [cs.CL]  17 May 2002\nNLTK: The Natural Language Toolkit\nEdward Loper and Steven Bird\nDepartment of Computer and Information Science\nUniversity of Pennsylvania, Philadelphia, PA 19104-6389, USA\nAbstract\nNLTK, the Natural Language Toolkit,\nis a suite of open source program\nmodules, tutorials and problem sets,\nproviding ready-to-use computational\nlinguistics courseware.\nNLTK covers\nsymbolic and statistical natural lan-\nguage processing, and is interfaced to\nannotated corpora. Students augment\nand replace existing components, learn\nstructured programming by example,\nand manipulate sophisticated models\nfrom the outset.\n1\nIntroduction\nTeachers of introductory courses on compu-\ntational linguistics are often faced with the\nchallenge of setting up a practical programming\ncomponent\nfor\nstudent\nassignments\nand\nprojects.\nThis is a diﬃcult task because\ndiﬀerent\ncomputational\nlinguistics\ndomains\nrequire a variety of diﬀerent data structures\nand functions, and because a diverse range of\ntopics may need to be included in the syllabus.\nA widespread practice is to employ multiple\nprogramming languages, where each language\nprovides native data structures and functions\nthat are a good ﬁt for the task at hand. For\nexample, a course might use Prolog for pars-\ning, Perl for corpus processing, and a ﬁnite-state\ntoolkit for morphological analysis.\nBy relying\non the built-in features of various languages, the\nteacher avoids having to develop a lot of software\ninfrastructure.\nAn\nunfortunate\nconsequence\nis\nthat\na\nsigniﬁcant part of such courses must be devoted\nto teaching programming languages.\nFurther,\nmany interesting projects span a variety of\ndomains,\nand\nwould\nrequire\nthat\nmultiple\nlanguages be bridged. For example, a student\nproject that involved syntactic parsing of corpus\ndata from a morphologically rich language might\ninvolve all three of the languages mentioned\nabove: Perl for string processing; a ﬁnite state\ntoolkit for morphological analysis; and Prolog\nfor parsing. It is clear that these considerable\noverheads and shortcomings warrant a fresh\napproach.\nApart from the practical component, compu-\ntational linguistics courses may also depend on\nsoftware for in-class demonstrations. This con-\ntext calls for highly interactive graphical user\ninterfaces, making it possible to view program\nstate (e.g. the chart of a chart parser), observe\nprogram execution step-by-step (e.g.\nexecu-\ntion of a ﬁnite-state machine), and even make\nminor modiﬁcations to programs in response to\n“what if” questions from the class.\nBecause\nof these diﬃculties it is common to avoid live\ndemonstrations, and keep classes for theoreti-\ncal presentations only. Apart from being dull,\nthis approach leaves students to solve important\npractical problems on their own, or to deal with\nthem less eﬃciently in oﬃce hours.\nIn this paper we introduce a new approach to\nthe above challenges, a streamlined and ﬂexible\nway of organizing the practical component\nof an introductory computational linguistics\ncourse.\nWe\ndescribe NLTK, the Natural\nLanguage Toolkit, which we have developed in\nconjunction with a course we have taught at\nthe University of Pennsylvania.\nThe\nNatural\nLanguage\nToolkit\nis\navail-\nable\nunder\nan\nopen\nsource\nlicense\nfrom\nhttp://nltk.sf.net/.\nNLTK runs on all\nplatforms\nsupported\nby\nPython,\nincluding\nWindows, OS X, Linux, and Unix.\n2\nChoice of Programming Language\nThe most basic step in setting up a practical\ncomponent is choosing a suitable programming\nlanguage.\nA\nnumber\nof\nconsiderations\ninﬂuenced our choice. First, the language must\nhave a shallow learning curve, so that novice\nprogrammers get immediate rewards for their\neﬀorts.\nSecond, the language must support\nrapid prototyping and a short develop/test\ncycle;\nan obligatory compilation step is a\nserious detraction.\nThird, the code should be\nself-documenting, with a transparent syntax and\nsemantics.\nFourth, it should be easy to write\nstructured programs, ideally object-oriented but\nwithout the burden associated with languages\nlike C++.\nFinally, the language must have\nan easy-to-use graphics library to support the\ndevelopment of graphical user interfaces.\nIn surveying the available languages,\nwe\nbelieve that Python oﬀers an especially good\nﬁt to the above requirements.\nPython is an\nobject-oriented scripting language developed\nby Guido van Rossum and available on all\nplatforms (www.python.org).\nPython oﬀers\na shallow learning curve; it was designed to\nbe easily learnt by children (van Rossum,\n1999).\nAs an interpreted language, Python is\nsuitable for rapid prototyping. Python code is\nexceptionally readable, and it has been praised\nas “executable pseudocode.”\nPython is an\nobject-oriented language, but not punitively\nso, and it is easy to encapsulate data and\nmethods inside Python classes. Finally, Python\nhas an interface to the Tk graphics toolkit\n(Lundh, 1999), and writing graphical interfaces\nis straightforward.\n3\nDesign Criteria\nSeveral criteria were considered in the design\nand implementation of the toolkit. These design\ncriteria are listed in the order of their impor-\ntance.\nIt was also important to decide what\ngoals the toolkit would not attempt to accom-\nplish; we therefore include an explicit set of non-\nrequirements, which the toolkit is not expected\nto satisfy.\n3.1\nRequirements\nEase of Use.\nThe primary purpose of the\ntoolkit is to allow students to concentrate on\nbuilding natural language processing (NLP) sys-\ntems. The more time students must spend learn-\ning to use the toolkit, the less useful it is.\nConsistency.\nThe toolkit should use consis-\ntent data structures and interfaces.\nExtensibility.\nThe\ntoolkit\nshould\neasily\naccommodate new components, whether those\ncomponents replicate or extend the toolkit’s\nexisting\nfunctionality.\nThe toolkit\nshould\nbe structured in such a way that it is obvious\nwhere new extensions would ﬁt into the toolkit’s\ninfrastructure.\nDocumentation.\nThe\ntoolkit,\nits\ndata\nstructures, and its implementation all need to\nbe carefully and thoroughly documented.\nAll\nnomenclature must be carefully chosen and\nconsistently used.\nSimplicity.\nThe toolkit should structure the\ncomplexities of building NLP systems, not hide\nthem.\nTherefore, each class deﬁned by the\ntoolkit should be simple enough that a student\ncould implement it by the time they ﬁnish an\nintroductory course in computational linguis-\ntics.\nModularity.\nThe interaction between diﬀer-\nent components of the toolkit should be kept\nto a minimum, using simple, well-deﬁned inter-\nfaces.\nIn particular, it should be possible to\ncomplete individual projects using small parts\nof the toolkit, without worrying about how they\ninteract with the rest of the toolkit. This allows\nstudents to learn how to use the toolkit incre-\nmentally throughout a course. Modularity also\nmakes it easier to change and extend the toolkit.\n3.2\nNon-Requirements\nComprehensiveness.\nThe\ntoolkit\nis\nnot\nintended to provide a comprehensive set of\ntools. Indeed, there should be a wide variety of\nways in which students can extend the toolkit.\nEﬃciency.\nThe toolkit does not need to\nbe highly optimized for runtime performance.\nHowever, it should be eﬃcient enough that\nstudents can use their NLP systems to perform\nreal tasks.\nCleverness.\nClear designs and implementa-\ntions are far preferable to ingenious yet inde-\ncipherable ones.\n4\nModules\nThe toolkit is implemented as a collection of\nindependent modules, each of which deﬁnes a\nspeciﬁc data structure or task.\nA set of core modules deﬁnes basic data\ntypes and processing systems that are used\nthroughout the toolkit.\nThe token module\nprovides basic classes for processing individual\nelements of text, such as words or sentences.\nThe tree module deﬁnes data structures for\nrepresenting tree structures over text,\nsuch\nas syntax trees and morphological trees.\nThe\nprobability module implements classes that\nencode frequency distributions and probability\ndistributions, including a variety of statistical\nsmoothing techniques.\nThe remaining modules deﬁne data structures\nand interfaces for performing speciﬁc NLP tasks.\nThis list of modules will grow over time, as we\nadd new tasks and algorithms to the toolkit.\nParsing Modules\nThe parser module deﬁnes a high-level inter-\nface for producing trees that represent the struc-\ntures of texts. The chunkparser module deﬁnes\na sub-interface for parsers that identify non-\noverlapping linguistic groups (such as base noun\nphrases) in unrestricted text.\nFour\nmodules\nprovide\nimplementations\nfor these abstract interfaces.\nThe srparser\nmodule\nimplements\na\nsimple\nshift-reduce\nparser.\nThe chartparser module deﬁnes a\nﬂexible parser that uses a chart\nto record\nhypotheses about syntactic constituents.\nThe\npcfgparser\nmodule\nprovides\na\nvariety\nof\ndiﬀerent parsers for probabilistic grammars.\nAnd\nthe\nrechunkparser module deﬁnes\na\ntransformational\nregular-expression\nbased\nimplementation of the chunk parser interface.\nTagging Modules\nThe tagger module deﬁnes a standard interface\nfor augmenting each token of a text with supple-\nmentary information, such as its part of speech\nor its WordNet synset tag; and provides several\ndiﬀerent implementations for this interface.\nFinite State Automata\nThe fsa module deﬁnes a data type for encod-\ning ﬁnite state automata; and an interface for\ncreating automata from regular expressions.\nType Checking\nDebugging time is an important factor in the\ntoolkit’s ease of use. To reduce the amount of\ntime students must spend debugging their code,\nwe provide a type checking module, which can\nbe used to ensure that functions are given valid\narguments. The type checking module is used\nby all of the basic data types and processing\nclasses.\nSince type checking is done explicitly, it can\nslow the toolkit down. However, when eﬃciency\nis an issue, type checking can be easily turned\noﬀ; and with type checking is disabled, there is\nno performance penalty.\nVisualization\nVisualization\nmodules\ndeﬁne\ngraphical\ninterfaces\nfor\nviewing\nand\nmanipulating\ndata\nstructures,\nand\ngraphical\ntools\nfor\nexperimenting with NLP tasks. The draw.tree\nmodule\nprovides\na\nsimple\ngraphical\ninter-\nface\nfor\ndisplaying\ntree\nstructures.\nThe\ndraw.tree edit module provides an interface\nfor building and modifying tree structures.\nThe draw.plot graph module can be used to\ngraph mathematical functions.\nThe draw.fsa\nmodule provides a graphical tool for displaying\nand simulating ﬁnite state automata.\nThe\ndraw.chart module provides an interactive\ngraphical tool for experimenting with chart\nparsers.\nThe visualization modules provide interfaces\nfor interaction and experimentation; they do\nnot directly implement NLP data structures or\ntasks. Simplicity of implementation is therefore\nless of an issue for the visualization modules\nthan it is for the rest of the toolkit.\nText Classiﬁcation\nThe classifier module deﬁnes a standard\ninterface for classifying texts into categories.\nThis interface is currently implemented by two\nmodules. The classifier.naivebayes module\ndeﬁnes a text classiﬁer based on the Naive Bayes\nassumption.\nThe classifier.maxent module\ndeﬁnes the maximum entropy model for text\nclassiﬁcation, and implements two algorithms\nfor training the model:\nGeneralized Iterative\nScaling and Improved Iterative Scaling.\nThe classifier.feature module provides\na standard encoding for the information that\nis used to make decisions for a particular\nclassiﬁcation task.\nThis standard encoding\nallows\nstudents\nto\nexperiment\nwith\nthe\ndiﬀerences between diﬀerent text classiﬁcation\nalgorithms, using identical feature sets.\nThe classifier.featureselection module\ndeﬁnes a standard interface for choosing which\nfeatures are relevant for a particular classiﬁca-\ntion task.\nGood feature selection can signiﬁ-\ncantly improve classiﬁcation performance.\n5\nDocumentation\nThe\ntoolkit\nis\naccompanied\nby\nextensive\ndocumentation that explains the toolkit, and\ndescribes how to use and extend it.\nThis\ndocumentation is divided into three primary\ncategories:\nTutorials\nteach students how to use the\ntoolkit, in the context of performing speciﬁc\ntasks. Each tutorial focuses on a single domain,\nsuch as tagging, probabilistic systems, or text\nclassiﬁcation. The tutorials include a high-level\ndiscussion that explains and motivates\nthe\ndomain, followed by a detailed walk-through\nthat uses examples to show how NLTK can be\nused to perform speciﬁc tasks.\nReference Documentation\nprovides precise\ndeﬁnitions for every module, interface, class,\nmethod, function, and variable in the toolkit. It\nis automatically extracted from docstring com-\nments in the Python source code, using Epydoc\n(Loper, 2002).\nTechnical Reports\nexplain and justify the\ntoolkit’s design and implementation. They are\nused by the developers of the toolkit to guide\nand document the toolkit’s construction. Stu-\ndents can also consult these reports if they would\nlike further information about how the toolkit is\ndesigned, and why it is designed that way.\n6\nUses of NLTK\n6.1\nAssignments\nNLTK can be used to create student assign-\nments of varying diﬃculty and scope.\nIn the\nsimplest assignments, students experiment with\nan existing module. The wide variety of existing\nmodules provide many opportunities for creat-\ning these simple assignments.\nOnce students\nbecome more familiar with the toolkit, they can\nbe asked to make minor changes or extensions to\nan existing module. A more challenging task is\nto develop a new module. Here, NLTK provides\nsome useful starting points:\npredeﬁned inter-\nfaces and data structures, and existing modules\nthat implement the same interface.\nExample: Chunk Parsing\nAs an example of a moderately diﬃcult\nassignment, we asked students to construct\na chunk parser that correctly identiﬁes base\nnoun\nphrase\nchunks\nin\na\ngiven\ntext,\nby\ndeﬁning a cascade of transformational chunking\nrules.\nThe NLTK rechunkparser module\nprovides\na\nvariety\nof\nregular-expression\nbased\nrule\ntypes,\nwhich\nthe\nstudents\ncan\ninstantiate\nto\nconstruct\ncomplete\nrules.\nFor\nexample,\nChunkRule(’<NN.*>’)\nbuilds\nchunks from sequences of consecutive nouns;\nChinkRule(’<VB.>’)\nexcises\nverbs\nfrom\nexisting chunks; SplitRule(’<NN>’, ’<DT>’)\nsplits\nany\nexisting\nchunk\nthat\ncontains\na\nsingular\nnoun\nfollowed\nby\ndeterminer\ninto\ntwo pieces; and MergeRule(’<JJ>’, ’<JJ>’)\ncombines two adjacent chunks where the ﬁrst\nchunk ends and the second chunk starts with\nadjectives.\nThe chunking tutorial motivates chunk pars-\ning, describes each rule type, and provides all\nthe necessary code for the assignment. The pro-\nvided code is responsible for loading the chun-\nked, part-of-speech tagged text using an existing\ntokenizer, creating an unchunked version of the\ntext, applying the chunk rules to the unchunked\ntext, and scoring the result. Students focus on\nthe NLP task only – providing a rule set with\nthe best coverage.\nIn the remainder of this section we reproduce\nsome of the cascades created by the students.\nThe ﬁrst example illustrates a combination of\nseveral rule types:\ncascade = [\nChunkRule(’<DT><NN.*><VB.><NN.*>’),\nChunkRule(’<DT><VB.><NN.*>’),\nChunkRule(’<.*>’),\nUnChunkRule(’<IN|VB.*|CC|MD|RB.*>’),\nUnChunkRule(\"<,|\\\\.|‘‘|’’>\"),\nMergeRule(’<NN.*|DT|JJ.*|CD>’,\n’<NN.*|DT|JJ.*|CD>’),\nSplitRule(’<NN.*>’, ’<DT|JJ>’)\n]\nThe next example illustrates a brute-force sta-\ntistical approach. The student calculated how\noften each part-of-speech tag was included in\na noun phrase. They then constructed chunks\nfrom any sequence of tags that occurred in a\nnoun phrase more than 50% of the time.\ncascade = [\nChunkRule(’<\\\\$|CD|DT|EX|PDT\n|PRP.*|WP.*|\\\\#|FW\n|JJ.*|NN.*|POS|RBS|WDT>*’)\n]\nIn the third example, the student constructed\na single chunk containing the entire text, and\nthen excised all elements that did not belong.\ncascade = [\nChunkRule(’<.*>+’)\nChinkRule(’<VB.*|IN|CC|R.*|MD|WRB|TO|.|,>+’)\n]\n6.2\nClass demonstrations\nNLTK provides graphical tools that can be used\nin class demonstrations to help explain basic\nNLP concepts and algorithms. These interactive\ntools can be used to display relevant data struc-\ntures and to show the step-by-step execution of\nalgorithms.\nBoth data structures and control\nﬂow can be easily modiﬁed during the demon-\nstration, in response to questions from the class.\nSince these graphical tools are included with\nthe toolkit, they can also be used by students.\nThis allows students to experiment at home with\nthe algorithms that they have seen presented in\nclass.\nExample: The Chart Parsing Tool\nThe chart parsing tool is an example of a\ngraphical tool provided by NLTK. This tool can\nbe used to explain the basic concepts behind\nchart parsing, and to show how the algorithm\nworks. Chart parsing is a ﬂexible parsing algo-\nrithm that uses a data structure called a chart to\nrecord hypotheses about syntactic constituents.\nEach hypothesis is represented by a single edge\non the chart. A set of rules determine when new\nedges can be added to the chart. This set of rules\ncontrols the overall behavior of the parser (e.g.,\nwhether it parses top-down or bottom-up).\nThe chart parsing tool demonstrates the pro-\ncess of parsing a single sentence, with a given\ngrammar and lexicon. Its display is divided into\nthree sections: the bottom section displays the\nchart; the middle section displays the sentence;\nand the top section displays the partial syntax\ntree corresponding to the selected edge.\nBut-\ntons along the bottom of the window are used\nto control the execution of the algorithm. The\nmain display window for the chart parsing tool\nis shown in Figure 1.\nThis tool can be used to explain several dif-\nferent aspects of chart parsing. First, it can be\nused to explain the basic chart data structure,\nand to show how edges can represent hypothe-\nses about syntactic constituents.\nIt can then\nbe used to demonstrate and explain the indi-\nvidual rules that the chart parser uses to create\nnew edges. Finally, it can be used to show how\nthese individual rules combine to ﬁnd a complete\nparse for a given sentence.\nTo reduce the overhead of setting up demon-\nstrations during lecture, the user can deﬁne a\nlist of preset charts. The tool can then be reset\nto any one of these charts at any time.\nFigure 1: Chart Parsing Tool\nThe chart parsing tool allows for ﬂexible con-\ntrol of the parsing algorithm. At each step of\nthe algorithm, the user can select which rule or\nstrategy they wish to apply. This allows the user\nto experiment with mixing diﬀerent strategies\n(e.g., top-down and bottom-up). The user can\nexercise ﬁne-grained control over the algorithm\nby selecting which edge they wish to apply a rule\nto.\nThis ﬂexibility allows lecturers to use the\ntool to respond to a wide variety of questions;\nand allows students to experiment with diﬀerent\nvariations on the chart parsing algorithm.\n6.3\nAdvanced Projects\nNLTK provides students with a ﬂexible frame-\nwork for advanced projects.\nTypical projects\ninvolve the development of entirely new func-\ntionality for a previously unsupported NLP task,\nor the development of a complete system out of\nexisting and new modules.\nThe toolkit’s broad coverage allows students\nto explore a wide variety of topics. In our intro-\nductory computational linguistics course, topics\nfor student projects included text generation,\nword sense disambiguation, collocation analysis,\nand morphological analysis.\nNLTK eliminates the tedious infrastructure-\nbuilding\nthat\nis\ntypically\nassociated\nwith\nadvanced\nstudent\nprojects\nby\nproviding\nstudents with the basic data structures, tools,\nand interfaces that they need. This allows the\nstudents to concentrate on the problems that\ninterest them.\nThe collaborative, open-source nature of the\ntoolkit can provide students with a sense that\ntheir projects are meaningful contributions, and\nnot just exercises. Several of the students in our\ncourse have expressed interest in incorporating\ntheir projects into the toolkit.\nFinally, many of the modules included in the\ntoolkit provide students with good examples\nof what projects should look like, with well\nthought-out interfaces, clean code structure, and\nthorough documentation.\nExample: Probabilistic Parsing\nThe probabilistic parsing module was created\nas a class project for a statistical NLP course.\nThe toolkit provided the basic data types and\ninterfaces for parsing.\nThe project extended\nthese, adding a new probabilistic parsing inter-\nface, and using subclasses to create a prob-\nabilistic version of the context free grammar\ndata structure.\nThese new components were\nused in conjunction with several existing compo-\nnents, such as the chart data structure, to deﬁne\ntwo implementations of the probabilistic parsing\ninterface.\nFinally, a tutorial was written that\nexplained the basic motivations and concepts\nbehind probabilistic parsing, and described the\nnew interfaces, data structures, and parsers.\n7\nEvaluation\nWe used NLTK as a basis for the assignments\nand student projects in CIS-530, an introduc-\ntory computational linguistics class taught at\nthe University of Pennsylvania.\nCIS-530 is a\ngraduate level class, although some advanced\nundergraduates were also enrolled.\nMost stu-\ndents had a background in either computer sci-\nence or linguistics (and occasionally both). Stu-\ndents were required to complete ﬁve assign-\nments, two exams, and a ﬁnal project. All class\nmaterials are available from the course website\nhttp://www.cis.upenn.edu/~cis530/.\nThe experience of using NLTK was very pos-\nitive, both for us and for the students.\nThe\nstudents liked the fact that they could do inter-\nesting projects from the outset. They also liked\nbeing able to run everything on their computer\nat home. The students found the extensive doc-\numentation very helpful for learning to use the\ntoolkit.\nThey found the interfaces deﬁned by\nNLTK intuitive, and appreciated the ease with\nwhich they could combine diﬀerent components\nto create complete NLP systems.\nWe did encounter a few diﬃculties during the\nsemester. One problem was ﬁnding large clean\ncorpora that the students could use for their\nassignments.\nSeveral of the students needed\nassistance ﬁnding suitable corpora for their\nﬁnal projects. Another issue was the fact that\nwe were actively developing NLTK during the\nsemester; some modules were only completed\none or two weeks before the students used\nthem.\nAs a result, students who worked at\nhome needed to download new versions of the\ntoolkit several times throughout the semester.\nLuckily,\nPython\nhas\nextensive\nsupport for\ninstallation scripts, which made these upgrades\nsimple. The students encountered a couple of\nbugs in the toolkit, but none were serious, and\nall were quickly corrected.\n8\nOther Approaches\nThe computational component of computational\nlinguistics courses takes many forms. In this sec-\ntion we brieﬂy review a selection of approaches,\nclassiﬁed according to the (original) target audi-\nence.\nLinguistics Students. Various books intro-\nduce programming or computing to linguists.\nThese are elementary on the computational side,\nproviding a gentle introduction to students hav-\ning no prior experience in computer science.\nExamples of such books are: Using Computers\nin Linguistics (Lawler and Dry, 1998), and Pro-\ngramming for Linguistics: Java Technology for\nLanguage Researchers (Hammond, 2002).\nGrammar\nDevelopers.\nInfrastructure\nfor grammar development has a long history\nin\nuniﬁcation-based\n(or\nconstraint-based)\ngrammar\nframeworks,\nfrom\nDCG\n(Pereira\nand Warren,\n1980) to HPSG (Pollard and\nSag, 1994). Recent work includes (Copestake,\n2000; Baldridge et al., 2002a).\nA concurrent\ndevelopment has been the ﬁnite state toolkits,\nsuch\nas\nthe\nXerox\ntoolkit\n(Beesley\nand\nKarttunen,\n2002).\nThis\nwork\nhas\nfound\nwidespread pedagogical application.\nOther\nResearchers\nand\nDevelopers.\nA variety of toolkits have been created for\nresearch or R&D purposes.\nExamples include\nthe\nCMU-Cambridge\nStatistical\nLanguage\nModeling\nToolkit\n(Clarkson\nand\nRosenfeld,\n1997),\nthe\nEMU\nSpeech\nDatabase\nSystem\n(Harrington and Cassidy, 1999), the General\nArchitecture for Text Engineering (Bontcheva\net al., 2002), the Maxent Package for Maximum\nEntropy Models (Baldridge et al., 2002b), and\nthe Annotation Graph Toolkit (Maeda et al.,\n2002).\nAlthough not originally motivated by\npedagogical needs, all of these toolkits have\npedagogical applications and many have already\nbeen used in teaching.\n9\nConclusions and Future Work\nNLTK provides a simple, extensible, uniform\nframework for assignments, projects, and class\ndemonstrations. It is well documented, easy to\nlearn, and simple to use. We hope that NLTK\nwill allow computational linguistics classes to\ninclude more hands-on experience with using\nand building NLP components and systems.\nNLTK is unique in its combination of three\nfactors.\nFirst, it was deliberately designed as\ncourseware and gives pedagogical goals primary\nstatus. Second, its target audience consists of\nboth linguists and computer scientists, and it\nis accessible and challenging at many levels of\nprior computational skill. Finally, it is based on\nan object-oriented scripting language support-\ning rapid prototyping and literate programming.\nWe plan to continue extending the breadth\nof materials covered by the toolkit.\nWe are\ncurrently working on NLTK modules for Hidden\nMarkov Models, language modeling, and tree\nadjoining grammars. We also plan to increase\nthe number of algorithms implemented by some\nexisting modules, such as the text classiﬁcation\nmodule.\nFinding suitable corpora is a prerequisite for\nmany student assignments and projects. We are\ntherefore putting together a collection of corpora\ncontaining data appropriate for every module\ndeﬁned by the toolkit.\nNLTK is an open source project, and we wel-\ncome any contributions. Readers who are inter-\nested in contributing to NLTK, or who have\nsuggestions for improvements, are encouraged to\ncontact the authors.\n10\nAcknowledgments\nWe are indebted to our students for feedback\non the toolkit, and to anonymous reviewers, Jee\nBang, and the workshop organizers for com-\nments on an earlier version of this paper. We are\ngrateful to Mitch Marcus and the Department of\nComputer and Information Science at the Uni-\nversity of Pennsylvania for sponsoring the work\nreported here.\nReferences\n[Baldridge et al.2002a] Jason\nBaldridge,\nJohn\nDowding,\nand Susana Early.\n2002a.\nLeo:\nan\narchitecture\nfor\nsharing\nresources\nfor\nuniﬁcation-based\ngrammars.\nIn\nProceedings\nof the Third Language Resources and Evaluation\nConference. Paris: European Language Resources\nAssociation.\nhttp://www.iccs.informatics.ed.ac.uk/\n~jmb/leo-lrec.ps.gz.\n[Baldridge et al.2002b] Jason\nBaldridge,\nThomas\nMorton, and Gann Bierner. 2002b. The MaxEnt\nproject.\nhttp://maxent.sourceforge.net/.\n[Beesley and Karttunen2002] Kenneth\nR.\nBeesley\nand\nLauri\nKarttunen.\n2002.\nFinite-State\nMorphology: Xerox Tools and Techniques. Studies\nin\nNatural\nLanguage\nProcessing.\nCambridge\nUniversity Press.\n[Bontcheva et al.2002] Kalina\nBontcheva,\nHamish\nCunningham, Valentin Tablan, Diana Maynard,\nand Oana Hamza.\n2002.\nUsing GATE as an\nenvironment for teaching NLP. In Proceedings of\nthe ACL Workshop on Eﬀective Tools and Method-\nologies for Teaching NLP and CL. Somerset, NJ:\nAssociation for Computational Linguistics.\n[Clarkson and Rosenfeld1997] Philip R. Clarkson and\nRonald Rosenfeld.\n1997.\nStatistical language\nmodeling using the CMU-Cambridge Toolkit.\nIn\nProceedings\nof\nthe\n5th\nEuropean\nConfer-\nence on Speech Communication and Technology\n(EUROSPEECH ’97).\nhttp://svr-www.eng.\ncam.ac.uk/~prc14/eurospeech97.ps.\n[Copestake2000] Ann Copestake.\n2000.\nThe (new)\nLKB system.\nhttp://www-csli.stanford.edu/~aac/doc5-2.\npdf.\n[Hammond2002] Michael Hammond. 2002. Program-\nming for Linguistics: Java Technology for Lan-\nguage Researchers. Oxford: Blackwell. In press.\n[Harrington and Cassidy1999] Jonathan\nHarrington\nand Steve Cassidy. 1999. Techniques in Speech\nAcoustics. Kluwer.\n[Lawler and Dry1998] John\nM.\nLawler\nand\nHelen\nAristar\nDry,\neditors.\n1998.\nUsing\nComputers in Linguistics. London: Routledge.\n[Loper2002] Edward Loper. 2002. Epydoc.\nhttp://epydoc.sourceforge.net/.\n[Lundh1999] Fredrik Lundh. 1999. An introduction\nto tkinter.\nhttp://www.pythonware.com/library/\ntkinter/introduction/index.htm.\n[Maeda et al.2002] Kazuaki\nMaeda,\nSteven\nBird,\nXiaoyi Ma, and Haejoong Lee.\n2002.\nCreat-\ning annotation tools with the annotation graph\ntoolkit. In Proceedings of the Third International\nConference on Language Resources and Evalua-\ntion. http://arXiv.org/abs/cs/0204005.\n[Pereira and Warren1980] Fernando C.\nN. Pereira\nand David H. D. Warren. 1980. Deﬁnite clause\ngrammars for language analysis – a survey of the\nformalism and a comparison with augmented tran-\nsition grammars.\nArtiﬁcial Intelligence, 13:231–\n78.\n[Pollard and Sag1994] Carl Pollard and Ivan A. Sag.\n1994.\nHead-Driven Phrase Structure Grammar.\nChicago University Press.\n[van Rossum1999] Guido van Rossum. 1999. Com-\nputer programming for everybody.\nTechnical\nreport, Corporation for National Research Ini-\ntiatives.\nhttp://www.python.org/doc/essays/\ncp4e.html.\n",
  "categories": [
    "cs.CL",
    "D.2.6; I.2.7; J.5; K.3.2"
  ],
  "published": "2002-05-17",
  "updated": "2002-05-17"
}