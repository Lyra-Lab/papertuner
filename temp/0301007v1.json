{
  "id": "http://arxiv.org/abs/cs/0301007v1",
  "title": "Kalman filter control in the reinforcement learning framework",
  "authors": [
    "Istvan Szita",
    "Andras Lorincz"
  ],
  "abstract": "There is a growing interest in using Kalman-filter models in brain modelling.\nIn turn, it is of considerable importance to make Kalman-filters amenable for\nreinforcement learning. In the usual formulation of optimal control it is\ncomputed off-line by solving a backward recursion. In this technical note we\nshow that slight modification of the linear-quadratic-Gaussian Kalman-filter\nmodel allows the on-line estimation of optimal control and makes the bridge to\nreinforcement learning. Moreover, the learning rule for value estimation\nassumes a Hebbian form weighted by the error of the value estimation.",
  "text": "arXiv:cs/0301007v1  [cs.LG]  9 Jan 2003\nKALMAN FILTER CONTROL IN THE REINFORCEMENT\nLEARNING FRAMEWORK\nISTV´AN SZITA AND ANDR´AS L˝ORINCZ\nAbstract. There is a growing interest in using Kalman-ﬁlter models in brain\nmodelling. In turn, it is of considerable importance to make Kalman-ﬁlters\namenable for reinforcement learning. In the usual formulation of optimal con-\ntrol it is computed oﬀ-line by solving a backward recursion.\nIn this tech-\nnical note we show that slight modiﬁcation of the linear-quadratic-Gaussian\nKalman-ﬁlter model allows the on-line estimation of optimal control and makes\nthe bridge to reinforcement learning. Moreover, the learning rule for value esti-\nmation assumes a Hebbian form weighted by the error of the value estimation.\n1. Motivation\nKalman ﬁlters and their various extensions are well studied and widely applied\ntools in both state estimation and control. Recently, there is an increasing inter-\nest in Kalman-ﬁlters or Kalman-ﬁlter like structures as models for neurobiological\nsubstrates. It has been suggested that Kalman-ﬁltering (i) may occur at sensory\nprocessing [6, 7], (ii) may be the underlying computation of the hippocampus, and\nmay be the underlying principle in control architectures [8, 9]. Detailed architec-\ntural similarities between Kalman-ﬁlter and the entorhinal-hippocampal loop as\nwell as between Kalman-ﬁlters and the neocortical hierarchy have been described\nrecently [2, 3]. Interplay between the dynamics of Kalman-ﬁlter-like architectures\nand learning of parameters of neuronal networks has promising aspects for explain-\ning known and puzzling phenomena, such as priming, repetition suppression and\ncategorization [4, 1].\nAs it is well known, Kalman-ﬁlter provides an on-line estimation of the state\nof the system. On the other hand, optimal control cannot be computed on-line,\nbecause it is typically given by a backward recursion (the Ricatti-equations). For\non-line parameter estimations without control aspects, see [5].\nThe aim of this paper is to derive an on-line control method for the Kalman-ﬁlter\nand achieve optimal performance asymptotically. Slight modiﬁcation of the linear-\nquadratic-Gaussian (LQG) Kalman-ﬁlter model is introduced for treating the LQG\nmodel as a reinforcement learning (RL) problem.\n2. the Kalman filter and the LQG model\nConsider a linear dynamical system with state xt ∈Rn, control ut ∈Rm, obser-\nvation yt ∈Rk, noises wt ∈Rn and et ∈Rk (which are assumed to be Gaussian\nand white, with covariance matrix Ωw and Ωe, respectively), in discrete time t:\nxt+1\n=\nFxt + Gut + wt\n(1)\nyt\n=\nHxt + et,\n(2)\nKey words and phrases. reinforcement learning, Kalman-ﬁlter, neurobiology.\n1\n2\nISTV´AN SZITA AND ANDR´AS L ˝ORINCZ\nthe initial state has mean ˆx1 and covariance Σ1. Executing the control step ut in\nxt costs\nc(xt, ut) := xT\nt Qxt + uT\nt Rut,\n(3)\nand after the Nth step the controller halts and receives a ﬁnal cost of xT\nNQNxN.\nThis problem has the well known solution\nˆxt+1\n=\nF ˆxt + Gut + Kt(yt −Hˆxt)\n(4)\nKt\n=\nFΣtHT (HΣtHT + Ωe)−1\n(5)\nΣt+1\n=\nΩw + FΣtF T −KtHΣtAT\n(state estimation)\n(6)\nand\nut\n=\n−Ltˆxt\n(7)\nLt\n=\n(GT St+1G + R)−1GT St+1F\n(8)\nSt\n=\nQt + F T St+1F −F T St+1GLt.\n(optimal control)\n(9)\nUnfortunately, the optimal control equations are not on-line, because they can\nbe solved only by stepping backward from the ﬁnal, Nth step.\n3. Kalman Filtering in the Reinforcement Learning Framework\nFirst of all, we slightly modify the problem: the run time of the controller\nwill not be a ﬁxed number N. Instead, after each time step, the process will be\nstopped with some ﬁxed probability p (and then the controller incurs the ﬁnal cost\ncf(xf) := xt\nfQfxf).\n3.1. The cost-to-go function. Let V ∗\nt (x) be the optimal cost-to-go function at\ntime step t, i.e.\nV ∗\nt (x) :=\ninf\nut,ut+1,... E\n\u0002\nc(xt, ut) + c(xt+1, ut+1) + . . . + cf(xf)\n\f\fxt = x\n\u0003\n.\n(10)\nClearly, for any x,\nV ∗\nt (x) = p · cf(x) + (1 −p) · inf\nu\n\u0010\nc(x, u) + Ew\n\u0002\nV ∗\nt+1(Fx + Gu + w)\n\u0003\u0011\n(11)\nIt can be easily shown that the optimal cost-to-go function is time-independent,\nfurthermore, it is a quadratic function of x, that is, it is of the form\nV ∗(x) = xT Π∗x.\n(12)\nOur task is to estimate V ∗(in fact, the parameter matrix Π∗) on-line. This will be\ndone by value iteration.\n3.2. Value iteration, greedy action selection and the temporal diﬀer-\nencing error. Value iteration starts with an arbitrary initial cost-to-go function\nV0(x) = xT Π0x. After this, control actions are selected according to the current\nvalue function estimate, the value function is updated according to the experience,\nand these two steps are iterated.\nKALMAN FILTER CONTROL IN THE REINFORCEMENT LEARNING FRAMEWORK\n3\nThe tth estimate of V ∗is Vt(x) = xT Πtx. The greedy control action according\nto this is given by\nut\n=\narg min\nu\n\u0010\nc(xt, u) + E\n\u0002\nVt(Fxt + Gu + w)\n\u0003\u0011\n(13)\n=\narg min\nu\n\u0010\nuT Ru + (Fxt + Gu)T Πt(Fxt + Gu)\n\u0011\n(14)\n=\n−(R + GT ΠtG)−1(GT ΠtF)xt.\n(15)\nFor the sake of simplicity, the cost-to-go function will be updated by using the\n1-step temporal diﬀerencing (TD) method. Naturally, it can be substituted with\nmore sophisticated methods like multi-step TD or eligibility traces. The TD error\nis\nδt =\n(\nVt(xt) −cf(xt)\nif the controller was stopped at the tth time step,\n\u0000c(xt, ut) + Vt(xt+1)\n\u0001\n−Vt(xt),\notherwise.\n(16)\nand the update rule for the parameter matrix Πt is\nΠt+1\n=\nΠt + αt · δt · ∇ΠtVt(xt)\n(17)\n=\nΠt + αt · δt · xtxT\nt ,\n(18)\nwhere αt is the learning rate. Note that value-estimation error weighted Hebbian\nlearning rule has emerged.\n4. Concluding remarks\nThe Kalman-ﬁlter control problem was slightly modiﬁed to ﬁt the RL framework\nand an on-line control rule was achieved. The well-founded theory of reinforcement\nlearning ensures asymptotic optimality for the algorithm. The described method is\nhighly extensible. There are straightforward generalizations to other cases, e.g., to\nextended Kalman ﬁlters, dynamics with unknown parameters, non-quadratic cost\nfunctions, or more advanced RL algorithms, e.g. eligibility traces. For quadratic\nloss functions, we have found that learning is Hebbian and it is weighted by the\nerror of value-estimation.\nReferences\n1. Sz. K´eri, Gy. Benedek, Z. Janka, P. Aszal´os, B. Szatm´ary, G. Szirtes, and A. L˝orincz, Cate-\ngories, prototypes and memory systems in alzheimer’s disease, Trends in Cognitive Science 6\n(2002), no. 132-136.\n2. A. L˝orincz and G. Buzs´aki, The parahippocampal region: Implications for neurological and\npsychiatric dieseases, Annals of the New York Academy of Sciences (H.E. Scharfman, M.P.\nWitter, and R. Schwarz, eds.), vol. 911, New York Academy of Sciences, New York, 2000,\npp. 83–111.\n3. A. L˝orincz, B. Szatm´ary, and G. Szirtes, Mystery of structure and function of sensory process-\ning areas of the neocortex: A resolution, J. Comp. Neurosci. 13 (2002), 187205.\n4. A. L˝orincz, G. Szirtes, B. Tak´acs, I. Biederman, and R. Vogels, Relating priming and repetition\nsuppression, Int. J. of Neural Systems 12 (2002), 187–202.\n5. R.P.N. Rao, An optimal estimation approach to visual perception and learning, Vision Research\n39 (1999), 1963–1989.\n6. R.P.N. Rao and D.H. Ballard, Dynamic model of visual recognition predicts neural response\nproperties in the visual cortex, Neural Comput 9 (1997), 721–763.\n7.\n, Predictive coding in the visual cortex: A functional interpretation of some extra-\nclassical receptive-ﬁeld eﬀects, Nature Neuroscience 2 (1999), 79–87.\n4\nISTV´AN SZITA AND ANDR´AS L ˝ORINCZ\n8. E. Todorov and M.I. Jordan, Optimal feedback control as a theory of motor coordination,\nNature Neuroscience 5 (2002), 1226–1235.\n9.\n, Supplementary notes for optimal feedback control as a theory of motor coordination,\nNature Neuroscience website, 2002.\nDepartment of Information Systems, E¨otv¨os Lor´and University of Sciences, P´azm´any\nP´eter s´et´any 1/C, 1117 Budapest, Hungary\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "I.2.6; I.2.8"
  ],
  "published": "2003-01-09",
  "updated": "2003-01-09"
}