{
  "id": "http://arxiv.org/abs/cond-mat/0301627v1",
  "title": "Combining Hebbian and reinforcement learning in a minibrain model",
  "authors": [
    "R. J. C. Bosman",
    "W. A. van Leeuwen",
    "B. Wemmenhove"
  ],
  "abstract": "A toy model of a neural network in which both Hebbian learning and\nreinforcement learning occur is studied. The problem of `path interference',\nwhich makes that the neural net quickly forgets previously learned input-output\nrelations is tackled by adding a Hebbian term (proportional to the learning\nrate $\\eta$) to the reinforcement term (proportional to $\\rho$) in the learning\nrule. It is shown that the number of learning steps is reduced considerably if\n$1/4 < \\eta/\\rho < 1/2$, i.e., if the Hebbian term is neither too small nor too\nlarge compared to the reinforcement term.",
  "text": "arXiv:cond-mat/0301627v1  [cond-mat.dis-nn]  31 Jan 2003\nCombining Hebbian and reinforcement\nlearning in a minibrain model\nR. J. C. Bosman, W. A. van Leeuwen and B. Wemmenhove\nInstitute for Theoretical Physics, University of Amsterdam, Valckenierstraat 65,\n1018 XE Amsterdam, The Netherlands\nAbstract\nA toy model of a neural network in which both Hebbian learning and reinforcement\nlearning occur is studied. The problem of ‘path interference’, which makes that the\nneural net quickly forgets previously learned input–output relations is tackled by\nadding a Hebbian term (proportional to the learning rate η) to the reinforcement\nterm (proportional to ρ) in the learning rule. It is shown that the number of learning\nsteps is reduced considerably if 1/4 < η/ρ < 1/2, i.e., if the Hebbian term is neither\ntoo small nor too large compared to the reinforcement term.\nPreprint submitted to Elsevier Science\n5 October 2018\n1\nIntroduction\nThe central question which we address in this article is in what way a biological\nneural network, i.e., the brain, or, more generally, a part of the nervous system\nof an animal or human being, may learn to realize input–output relations. By\nbiological we here mean: realizable with the help of elements occurring in\nnature, e.g., neurons or chemical substances that may inﬂuence other neurons\nor the synaptic eﬃcacy.\nAn example of an input–output relation is a motor task, like catching a prey,\nin reaction to visual, auditive, or other input. Many attempts to explain the\nway input–output relations of this kind might be realized by (artiﬁcial) neural\nnets are encountered in the literature, most of which are not satisfactory from\na biological point of view as we will illustrate in subsection 1.1.\nIt is the purpose of this article to combine ideas which do satisfy certain\nbiological constraints and study a toy model, in particular with respect to its\nability to learn and realize input–output relations.\nThe widely accepted idea of Hebbian learning [5] at the one hand will be com-\nbined with some rule that implements a feedback signal at the other hand,\nin a way that, in principle, might be biologically realizable. Without the ad-\ndition of any feedback-signal, learning of prescribed input–output relations\n—whether in reality or in a model— is, of course, impossible.\n1.1\nArtiﬁcial learning rules\nIf one wants a network to learn to realize input–output relations, there are var-\nious well-known prescriptions, associated with names like perceptron learning\nrule, back-propagation or Boltzmann machines [12,7]. None of these, however,\nmodel the functioning of real brains, since the learning rules in question vi-\nolate the existing biological limitations. In order to illustrate this statement,\nlet us give an example.\nConsider a single layered feed-forward network, i.e., a network consisting of\nan input and an output layer only, in which signals are sent by neurons of the\ninput layer to neurons of the output layer, and not the other way around. Let\nwij be the strengths or ‘weights’ of the connections in this simple net. In 1962,\nRosenblatt [13] proved that such a network will realize desired input–output\nrelations if, a ﬁnite number of times, the weights are adapted according to the\nrule\nwij →wij + ∆wij\n(1)\n2\nwith\n∆wij = ε(xTi −xOi)xj\n(2)\nwhere xTi is the desired or target output of neuron i, and xOi is its actual\noutput. Furthermore, xj is the state of the pre-synaptic input neuron j and ε\nis some function of the neuron states and properties of neurons i and j. This\nlearning rule can not be realized by a biological neural net since neuron i,\nproducing xOi, cannot know that it should produce xTi. If, e.g., an animal does\nnot succeed in catching a prey, its neurons get no speciﬁc feedback individually,\non what the right output xTi should have been. Hence, xTi −xOi cannot be\ndetermined by the biological system, and, therefore, neither can it adapt the\nweights according to (2). Consequently, the perceptron learning rule (2) is\nunsuitable for a realistic modeling of the way in which a biological neural net\ncan learn and realize input–output relations. Similar observations can be made\nfor back-propagation or Boltzmann machines.\n1.2\nBiological learning rules; Hebbian learning and reinforcement learning\nAlready in 1949, Hebb suggested [5] that, in biological systems, learning takes\nplace through the adaptation of the strengths of the synaptic interactions\nbetween neurons, depending on the activities of the neurons involved. In a\nmodel using binary neurons, i.e., xi = 0 or xi = 1, the most general form of\na learning rule based on this principle is a linear function in xi and xj since\nx2\ni = xi and x2\nj = xj. It therefore reads\n∆wij = aij + bijxi + cijxj + dijxixj\n(3)\nIn a biological setting, the coeﬃcients aij, bij, cij and dij in this learning rule\ncan only depend on locally available information, such as the values of the\nmembrane potential\nhi =\nX\nj\nwijxj\n(4)\nand the threshold potential θi of neuron i. In this way, the system adapts its\nweights without making use of neuron-speciﬁc information, like, e.g., xTi, of\nwhich there can be no knowledge, locally, at the position of the synapse.\nIn a recurrent neural net, a Hebbian learning rule suﬃces to store patterns\n[6,12] if all neurons are clamped to the patterns which are to be learned during\na certain period, the ‘learning stage’. In feed-forward networks, however, only\n3\nthe neurons of the input layers are clamped, and some kind of feed-back sig-\nnal, governing the direction of adaptation of the synapses during the learning\nprocedure, is indispensable. Probably the simplest form of such a signal one\ncan think of is a ‘success’ or ‘failure’ signal that is returned to the network\nafter each attempt to associate the correct output to given input. On the basis\nof trial and error, a neural network can then indeed learn certain tasks, the\nprinciple on which ‘reinforcement learning’ is based [3,14]. This principle of re-\ninforcement learning has a rather natural interpretation: satisfactory behavior\nis rewarded, or reinforced, causing this behavior to occur more frequently. The\nreinforcement signal is supplied by the subject’s environment, or by its own\njudgment of the eﬀect of its behavior. In a biological perspective, one could\nthink of the synaptic change being inﬂuenced by some chemical substance,\nwhich is released depending on whether the evaluation by the subject of the\neﬀect of the output is positive or negative, i.e., whether it is happy or unhappy\nwith the attempt it made.\nNote that, in learning by reinforcement, the search for the correct output\nis more diﬃcult, and, hence, slower, than for non-biologically realizable al-\ngorithms like the perceptron learning rule or back-propagation. This is not\nsurprising, since the latter give the system locally speciﬁc information on how\nto adjust individual weights, while reinforcement rules only depend upon a\nglobal ‘measure of correctness’.\nThe most general form of a reinforcement learning algorithm is given by the\nprescription\n∆wij = aij(r) + bij(r)xi + cij(r)xj + dij(r)xixj\n(5)\nHere the coeﬃcients aij, bij, cij, dij, besides their dependence on the local vari-\nables such as the membrane potential, will in principle also depend on a rein-\nforcement signal, denoted by r. The value of r is usually a real number between\n0 and 1, denoting the degree of success (r = 1 means success, r = 0 means\nfailure).\nAn important issue in the literature on reinforcement learning is the so called\n‘credit assignment problem’ [14]. It refers to the question how a neural net\nknows which connections wij were responsible for a successful or unsuccessful\ntrial, and, as a consequence, which connections should be ‘rewarded’, and\nwhich should be ‘punished’, respectively.\nIn their article ‘Learning from mistakes’ (1999), Chialvo and Bak [4], pro-\nposed a class of networks, in which learning takes place on the basis of a\n‘deinforcement signal’ only, i.e., the weights of active synapses are decreased\nif the output is wrong, they are ‘punished’, so to say, in case of wrong perfor-\nmance of the network. If the output is right nothing happens. This procedure\n4\nworks as long as the average activity in the net is kept very low: when only a\nfew neurons are active at an unsuccessful attempt, one can be sure that the\nconnections between these active neurons were the ones which were responsi-\nble, and thus should be ‘punished’. In this way Chialvo and Bak obtained an\nelegant solution to the credit assignment problem.\nThe absence of a reinforcement signal (nothing happens if r = 1) makes their\nlearning rule relatively simple. It is a version of the general rule (5) with bij = 0\nand cij = 0:\n∆wij = −(1 −r)(ρxixj −ϕ)\n(6)\nwhere ρ and ϕ are positive constants; in this article we will suppose ϕ << ρ.\nA biological mechanism that could implement the learning rule (6) is the fol-\nlowing: if the output is correct, nothing happens, since the network obviously\nperforms satisfactory. If not, a chemical substance is released, which has the\neﬀect that synapses between neurons that have just been active, and thereby\nare ‘tagged’ in some electro-chemical way, are depressed.\n1.3\nPurpose\nThe success of the ‘minibrain model’ of Chialvo and Bak [4] (as Wakeling\nand Bak referred to it in [15]), is limited to feed-forward neural nets in which\nthe number of input and output neurons (or, equivalently in this model, the\nnumber of patterns) is small compared to the number of neurons in the hidden\nlayer. As the number of neurons in the hidden layer decreases, learning, at a\ncertain moment, becomes impossible: ‘path interference’ is the phenomenon\nwhich causes this eﬀect [16]. Essentially, it amounts to the following. If, in\neach layer of the feed-forward neural net, only one neuron is active at each\ntime step, an input–output relation corresponds to a path of activity along the\nstrongest connections between the neurons. Basically, path interference comes\ndown to the erasure of an existing path of activity, which was correct at a\nprevious learning step, by a change due to a punishment of a connection while\ntrying to learn a diﬀerent input–output relation. If the probability for this\npath interference to occur becomes too large, learning times tend to inﬁnity.\nIn this article we attempt to improve the performance of the minibrain model\nof Chialvo and Bak —in the sense of decreasing the learning time— by making\nsure that, at the occurrence of path interference, the punishment of a correct\nactivity path is no longer such that the memory is erased. We achieve this by\nadding to the deinforcement term in the learning rule (6), which is propor-\ntional to ρ, a Hebbian term proportional to η. The latter term has the eﬀect\nthat an active path is strengthened, mitigating in this way the punishment.\n5\nBy choosing the ratio between the coeﬃcients η and ρ of both terms in the\nlearning rule appropriately, we are able to reduce the number of learning steps\nsigniﬁcantly, without making the model less realistic from a biological point\nof view. In fact, in the class of models we study, Hebbian learning is a most\nappropriate way to account for biological observations like LTP and LTD [8].\nIn section 4 we explain that if the quotient of the Hebbian learning rate and\nthe coeﬃcient of the deinforcement term is in the range\n1\n4 < η\nρ < 1\n2\n(7)\nlearning times are reduced considerably.\nIn their article [2], Chialvo and Bak proposed a diﬀerent way to solve the\nproblem of path interference. They reduced the amount of punishment of the\nconnections that once had been active in a correct response. In this model\na neuron needs to memorize whether it previously was active in a successful\ntrial. In our model such a neuron memory is not needed.\nLet us denote the deinforcement contribution to learning by ∆w′\nij and denote\nthe Hebbian part by ∆w′′\nij. We will study a learning rule of the form\n∆wij = ∆w′\nij + ∆w′′\nij\n(8)\nFrom all possibilities for Hebbian learning summarized by equation (3), we\nchoose for ∆w′′\nij a rule in which the coeﬃcients aij and bij both are zero:\n∆w′′\nij = ε(xi, xj)(2xi −1)xj\n(9)\nWe choose this particular form since it can be argued that this form is a most\nplausible candidate from a biological point of view [6].\nOur paper has been set up as follows. In section 2, we describe a feed-forward\nnetwork with one hidden layer, which we will use to study the learning rule\n(8), with (6) and (9). In section 3, numerical studies of various situations are\ngiven and explained. It turns out, in general, that taking into account Hebbian\nlearning, and viewing it as a process which is permanently active, irrespective\nof the occurrence of reinforcement learning, has a positive inﬂuence on the\nlearning time of the neural net. This is a new result, which, to the best of our\nknowledge, has not been noticed before.\n6\nNH\nNO\nNI\nFig. 1. An example of a fully connected feed forward network with NI input neu-\nrons, NH hidden neurons and NO output neurons. The ﬁlled circles represent active\nneurons.\n2\nDescription of the model: updating rules for neuron activities\nand connection weights\nIn order to explore a simpliﬁed model of the brain we consider a fully con-\nnected, feed-forward neural network with an input layer of NI neurons, one\nhidden layer of NH neurons, and an output layer of NO neurons, see ﬁgure 1.\nThe state xi of neuron i is 1 if neuron i ﬁres and 0 if it does not. In general, a\nneuron ﬁres if its potential hi is suﬃciently high, where hi stands for the mem-\nbrane potential Vex −Vin, the diﬀerence between the intra- and extra cellular\npotentials Vin and Vex. Following Chialvo and Bak [4], we model the dynamics\nof the neural net by simply stating that in the hidden and output layers a\ngiven number of neurons having the highest potentials hi —in their respec-\ntive layers— are those that will ﬁre. In their terminology: we use extremal\ndynamics.\nFor McCulloch and Pitts neurons a way to control the average activity might\nbe realized via suitably chosen threshold potentials ϑi (see e.g. [1], [3]). In\nnature, the average activity will depend on the threshold potentials and may,\nmoreover, be inﬂuenced by chemical substances or the network structure [8,10].\nIn our model we put the threshold potentials θi equal to zero:\nθi = 0\n(10)\nThe number of neurons in the input, hidden and output layers that we choose\nto be active, will be denoted by N(a)\nI , N(a)\nH\nand N(a)\nO , respectively.\nThe input pattern, a speciﬁc set of states of neurons in the input layer, will be\ndenoted by ξI = (ξI1, ..., ξINI). The network is to associate every input pattern\nwith a desired, or target, output pattern, ξT = (ξT1, ..., ξTNO). The ξI and ξT\nare vectors with components equal to 0 or 1. Consequently, the number of\nactive neurons of the input and output layer are given by\n7\nN(a)\nI\n=\nNI\nX\nj=1\nξIj\n(11)\nN(a)\nO =\nNO\nX\nj=1\nξTj\n(12)\nIn our numerical experiments, these numbers will be taken to be equal. More-\nover, the number of active neurons in the hidden layer,\nN(a)\nH\n=\nNH\nX\nj=1\nxHj\n(13)\nwhere xHj is the neuron state of neuron j in the hidden layer, will also be\nequal to the number of active neurons in the other layers. Hence, we choose\nN(a)\nI\n= N(a)\nO = N(a)\nH .\nWe thus have explicitated the network dynamics. We now come to the update\nrules for the synaptic weights wij. Updating of the network state will happen\nat discrete time steps. At every time step tn, all neuron states are updated\nin the order: input layer – hidden layer – output layer. This being done, the\nvalues of the weights are updated, according to\nwij(tn+1) = wij(tn) + ∆wij(tn)\n(14)\nSubstituting (9) and (6) into (8), we ﬁnd\n∆wij = ε(xi, xj)(2xi −1)xj −(1 −r)(ρxixj −ϕ)\n(15)\nFor the pre-factor of the Hebbian term we take [6]\nε(xi, xj) = η(κ −(2xi −1)(hi −θi))\n(16)\nThe constants η and κ are positive numbers, the so-called learning rate and\nmargin parameter. Finally, combining the above ingredients and noting that\nwe chose θi = 0, the learning rule reads:\n∆wij(tn) = η[κ −hi(tn)(2xi(tn) −1)][2xi(tn) −1]xj(tn)\n+(1 −r)[−ρxi(tn)xj(tn) + ϕ]\n(17)\nNote that xi(tn) and xj(tn) are the activities of neurons in adjacent layers,\nsince in our model there are no lateral connections. The constant ϕ is chosen\nin such a way that the change in P\ni,j wij, where the sum is extended over i\nand j in adjacent layers, due to the ρ-term (not due to the Hebbian term),\n8\nis independent of ρ. This can easily be achieved by choosing ϕ = ρ/P, where\nP is the product of the numbers of neurons in two adjacent layers, i.e., ϕ is\nequal to either ρ/NINH or ρ/NHNO.\n3\nNumerical Simulations\nThe network described in the previous section will now ﬁrst be studied nu-\nmerically. The numerical details are:\n– The initial weights wij(t0) are randomly chosen with values between −0.01\nand +0.01.\n– The punishment rate ρ will be kept constant at 0.02. Thus when we vary\nthe η/ρ ratio, we vary the learning rate η.\n– The margin parameter κ, appearing in (16), will be kept at the value 1.\n– Whenever the number of neurons in the input, hidden or output layer is\nﬁxed, we choose NI = 8, NH = 512 and NO = 8.\n– All data are averaged over 512 samples.\nThe network is confronted with p diﬀerent input patterns ξµ\nI , (µ = 1, . . . , p),\nto which correspond equally many target patterns ξµ\nT. Learning proceeds as\nfollows. The input layer remains clamped to the ﬁrst input pattern until the\ntime step at which the target pattern has been found. As soon as this input-\noutput relation µ = 1 has been learned, we switch to input pattern µ = 2.\nAfter the corresponding target pattern has been found we continue, up to the\np-th input-output relation. Then, we have completed what we will refer to as\none ‘learning cycle’.\nAfter this cycle we start the process again, up to the point where the network\ncan recall all p input-target relations at once. When that is the case, learning\nstops. We count the number of learning steps needed to learn all input–output\nrelations.\nChialvo and Bak consider the case of one active neuron in each layer. In section\n3.1 we present a numerical experiment with a neural network for which the\nactivities are larger than one, i.e., N(a)\nI\n> 1, N(a)\nH\n> 1 and N(a)\nO\n> 1. In\nparticular we study the total number of learning steps as a function of the\nratio η/ρ. In section 3.2 we vary the number of neurons in input and output\nlayer and keep the hidden layer ﬁxed, and vice versa. Finally, in section 4, we\ninterpret our results.\n9\n3.1\nEﬀect of the Hebbian term\nIn ‘Learning from mistakes’ Chialvo and Bak [4] studied the case of one active\nneuron in the input, the hidden and the output layers: N(a)\nI\n= N(a)\nH = N(a)\nO = 1.\nWe here will study what happens when N(a)\nI , N(a)\nH\nand N(a)\nO all are larger than\none.\nIn our ﬁrst numerical experiment we take a network with p = 8 input–target\nrelations for which, in each input or target pattern µ, the number of active\nneurons is 2, i.e., N(a)\nI\n= N(a)\nH\n= N(a)\nO\n= 2. In ﬁgure 2 the number of learning\nsteps is plotted against the ratio η/ρ of the two proportionality coeﬃcients\nrelated to the Hebbian and the deinforcement term respectively.\n\u0011\n=\u001a\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nNum\nb\ner\nof\nlearning\nsteps\n0\nFig. 2. The number of learning steps as a function of the quotient η/ρ. There are\neight input–target relations to be learned and two active neurons in each layer. The\nnumber of neurons in the input, hidden and output layers are NI = 8, NH = 512\nand NO = 8. Initially, the number of learning steps increases as a result of the\nHebbian learning term, but at η/ρ = 0.1 the number of learning steps starts to\ndecrease dramatically from 7500 to 250 at η/ρ = 0.25. For η/ρ > 0.50, learning is\nimpossible.\nFrom ﬁgure 2 we see that, when there is no Hebbian component in the learning\nrule (η = 0), the net needs 2500 learning steps to learn an input-output\ntask. When we add a slight Hebbian component (η small) the number of\nlearning steps increases, and, hence, the ability of the net to learn diminishes.\nHowever, when the Hebbian component becomes more and more important,\nthe number of learning steps starts to decrease dramatically: for η/ρ between\n0.25 and 0.5 the number of learning steps is approximately 250. The Hebbian\ncomponent, which has the tendency to engrave previously learned patterns,\nseems to help to not forget the old patterns. If η/ρ exceeds the value 0.5,\nlearning fails. Apparently, the ‘progressive’ ρ term, the power of which is to\nhelp the network to quickly adapt itself to new input-output relations, cannot\nconquer the ‘conservative’ power of the η-dependent (i.e., the Hebbian) term.\n10\nWe will come back to these points and consider the eﬀects of the η and ρ terms\nin some detail in section 4.\n3.2\nSize dependences\nIn this section we consider the network of ﬁgure 1 for varying numbers NI =\nNO and NH.\n3.2.1\nEﬀect of varying the sizes of the input and output layers\nIn this subsection we test the performance of the network for various sizes\nof the input, output and hidden layers. In particular, we chose to study the\nsize-dependence for three diﬀerent values of the learning parameter: η/ρ = 0,\nη/ρ = 0.10 and η/ρ = 0.45, values which we selected on the basis of the results\nof the previous subsection.\nFirst, we take a network with the ﬁxed number of 512 neurons in the hidden\nlayer, and only one active neuron per layer. The input and output layers will\nconsist of increasing, equal numbers of neurons, starting with NI = NO = 4.\nMoreover, we choose the number of input–output relations p to be learned\nequal to the number of neurons in the input and output layers. The input and\noutput layers will be enlarged in steps of 4 neurons, up to NI = NO = 28\nneurons.\nIn Figure 3 we give the number of learning steps per pattern for the above\nmentioned three values of η/ρ. The positive eﬀect of the addition of a Hebbian\nterm to the learning rule becomes more and more convincing with increasing\nnumber of input–output relations to be learned.\n3.2.2\nEﬀect of varying the size of the hidden layer\nNext we consider a network with 8 input neurons, 8 output neurons and 8\nsubsequent patterns. The number of active neurons is 2 for all input and\ntarget patterns. We vary the number of neurons in the hidden layer.\nIn Figure 4 we have plotted the number of learning steps as a function of\nthe number of neurons in the hidden layer for three values of the quotient\nη/ρ. Note that, in agreement with ﬁgure 2 the number of learning steps is\nthe largest for η/ρ = 0.1 (the symbols □in ﬁgure 4). A suitably chosen value\nfor the coeﬃcient η of the Hebbian term makes it possible for the network to\nperform satisfactory with very small number of neurons in the hidden layer\n(the symbols ⃝in the ﬁgure).\n11\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n4\n8\n12\n16\n20\n24\n28\nNumber of learning steps per pattern\nNumber of patterns to be learned\nFig. 3. The number of learning steps per pattern as a function of the number of\ninput–output relations p, for η/ρ = 0 (■), η/ρ = 0.1 (□) and η/ρ = 0.45 (⃝). Input\nand output patterns have only one active neuron. The number of neurons in input\nand output layers equals the number of patterns p. Note the logarithmic scale of\nthe vertical axis. For a small number of input–output patterns, the learning time\nis roughly equal for diﬀerent values of η/ρ. The advantageous eﬀect of a Hebbian\nterm in the learning rule for this learning task becomes more and more pronounced\nwith increasing numbers of input–target relations.\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n0\n512\n1024\n1536\n2048\n2560\n3072\n3584\n4096\nNumber of learning steps\nNumber of neurons in the hidden layer\nFig. 4. Dependence of the number of learning steps on the number of neurons in the\nhidden layer of the network. The symbols (■), (□) and (⃝) correspond to η/ρ = 0,\nη/ρ = 0.1 and η/ρ = 0.45 respectively. All input patterns and output patterns have\n2 active neurons. The number of input neurons, output neurons and patterns are\nﬁxed; NI = 8, NO = 8, p = 8.\n12\nw\nmax\nh\n=\nw\nmax\nI\nH\nO\nA\nB\nt\np\nt\nq\nC\nD\nw\n om\nh\n=\nw\n om\nFig. 5. Path interference At both the time steps tp and tq the neuron C of the hidden\nlayer ﬁres, and as a result the same neuron D of the output layer is activated. This\nunwanted eﬀect is due to the fact that the connection of B and C happens to be\nthe largest one. The paths ACD and BCD partially overlap.\n4\nExplanation of the eﬀect of the Hebbian term\nThe diﬀerent behavior for diﬀerent values of η/ρ is mainly due to its con-\nsequences for the eﬀect we call path interference, after Wakeling [16], who\nstudied the critical behavior of the Chialvo & Bak minibrain.\nAs an example, let us consider the case in which only one neuron is active in\neach layer. In this case, the ‘path of activity’ from the active input neuron to\nthe corresponding output neuron runs along the outgoing connections with the\nlargest weights. During the learning process, it is possible that an established\npath (connecting, e.g., the active neuron of pattern ξ1\nI with the active neuron,\nin the output layer, of ξ1\nO) is ‘wiped out’ by an attempt to learn one of the\nother input–output relations. This is likely to happen if the same neuron in\nthe hidden layer becomes active, and, consequently, the connection to the\noutput neuron corresponding to the previously learned pattern is punished\nby an amount ρ. This phenomenon of path interference will occur once in\na while, irrespective of the values of the parameters η and ρ. However, the\nquestion whether the memory of the old pattern is wiped out (i.e., whether\nthe connection to the output neuron under consideration is no longer the\nlargest), does depend on the parameters ρ and η. To ﬁnd out how, we should\nconsider the change of this connection compared to the change of the other\nconnections from the same hidden neuron to the other output neurons. For\nthe total relative change, two diﬀerent learning steps should be taken into\naccount. Firstly, the one at tp, at which the right output was found, and\nthe deinforcement term did not contribute, and, secondly, the learning step\nat tq, at which path interference occurred, and the deinforcement term did\ncontribute.\nLet wwin be the largest outgoing weight from the active neuron in the hidden\n13\nlayer to the output layer, and let wcom be a weight value which is representative\nfor one of the other, competing weights connecting the same neuron in the\nhidden layer to a diﬀerent neuron in the output layer.\nThe membrane potentials of each neurons i of the output layer are given,\naccording to (4), by hi = wij, where j is a neuron of the hidden layer. From\n(17), with xj = 1, we ﬁnd in case of success (r = 1) for the changes of the\nconnections to the winning (xi) and the competing (xi = 0) neurons in the\noutput layer:\n∆wwin(tp) = η[κ −wwin(tp)]\n(18)\n∆wcom(tp) = −η(κ + wcom(tp))\n(19)\nrespectively. Similarly, in case of failure (r = 0) these changes are\n∆wwin(tq) = η(κ −wwin(tq)) −ρ + ϕ\n(20)\n∆wcom(tq) = −η(κ + wcom(tq)) + ϕ\n(21)\nrespectively. Only if the increase of wcom is larger than the increase of wwin,\nthe memory path can be wiped out, since then wcom may become the largest\nweight, i.e., if\n∆wcom(tp) + ∆wcom(tq) > ∆wwin(tp) + ∆wwin(tq)\n(22)\nWe now substitute (18)–(21) into (22). In the resulting inequality we can ignore\nthe values of wwin and wcom relative to κ as long as the number of adaptations\nof wwin and wcom is small; recall that κ = 1, ρ = 0.02, and the initial values of\nthe weights are in the range [−0.01, 0.01] in our numerical experiments. With\nthese approximations, the inequality reduces to ρ > 4η. In the opposite case,\nρ < 4η\n(23)\nwwin will remain larger than wcom and, consequently, path interference will\nnot wipe out learned input–output relations, which explains the decrease of\nthe number of learning steps for η > 1\n4ρ. For η > 1\n2 the network is incapable\nof learning input–output relations. This can be seen as follows. Each time a\nwinning connection is punished, i.e., the output is wrong, it changes approxi-\nmately by an amount η −ρ, whereas the competing connection changes by an\namount of −η. Hence, only when η −ρ < −η, or, equivalently, when\n2η < ρ\n(24)\nthe winning connection decreases more than the competing connection. In\n14\nthe opposite case, 2η > ρ, the winning connection remains larger than its\ncompetitor, and, at the next learning step, the output will be wrong again.\nCombining the inequalities (23) and (24), we ﬁnd the central result of this\narticle (7), the parameter region for which a Hebbian term in the learning rule\nis advantageous. This observation is conﬁrmed by the numerical experiment\nof section 3.1, so, in particular, ﬁgure 2.\nNote that the reasoning leading to the main results (23) and (24) was based\non an assumption regarding the initial values. In particular, it was assumed\nthat the weights were small compared to κ (which was put equal to 1). In\nreference [6] it was shown that the pre-factor (16) of the Hebbian term tends\nto zero during the learning process:\nκ −(2xi −1)(hi −θi) →0\n(25)\nimplying, that for a small number of active neurons the absolute values of the\nweights are of the order κ, as follows with (4) and (10). Hence, the assumption\nthat the weights are small compared to κ (κ = 1) is guaranteed to break down\nat a certain point in the learning process.\n5\nSummary\nWe have shown, in a particular model, that a Hebbian component in a rein-\nforcement rule improves the ability to learn input–output relations by a neural\nnet.\nReferences\n[1] Alstrom P and Stassinopoulos D 1995 Phys. Rev. E 51 5027\n[2] Bak P and Chialvo D R 2001 Phys. Rev. E 63 031912\n[3] Barto A G 1995 Reinforcement learning: The Handbook of Brain Theory and\nNeural Networks ed M A Arbib (Cambridge, Massachusetts: MIT Press) 804-\n809\n[4] Chialvo D R and Bak P 1999 Neuroscience 90 1137\n[5] Hebb D O 1949 The Organization of Behaviour (New York: Wiley) 62\n[6] Heerema M and van Leeuwen W A 1999 J. Phys. A: Math. Gen. 32 263\n15\n[7] Hertz J, Krogh A and Palmer R G 1991 Introduction to the theory of neural\ncomputation (Redwood City, California: Addison-Wesley)\n[8] Kandel E R, Schwartz J H and Jessell T M 1991 Principles of Neural Science,\nThird Edition (Englewood-Cliﬀs, New Jersey: Prentice-Hall International Inc.)\n[9] Klemm K, Bornhodt S and Schuster H G 2000 Phys. Rev. Lett. 84 1813\n[10] Kolb B and Whishaw I Q 1990 Human Neuropsychology, 3rd edition (New York:\nFreeman)\n[11] Minsky\nM\nL\nand\nPapert\nS\nA\n1969\nPerceptrons:\nAn Introduction\nto\nComputational Geometry (Cambridge, Massachusetts: MIT Press)\n[12] M¨uller B, Reinhardt J and Strickland M T 1990 Neural Networks, An\nIntroduction (Berlin: Springer-Verlag)\n[13] Rosenblatt F 1962 Principles of Neurodynamics (New York: Spartan)\n[14] Sutton R S and Barto A G 1998 Reinforcement learning: An introduction\n(Cambridge, Massachusetts: MIT Press)\n[15] Wakeling J and Bak P 2001 arXiv:nlin.AO/0201046\n[16] Wakeling J 2002 cond-mat/0204562\n16\n",
  "categories": [
    "cond-mat.dis-nn",
    "q-bio"
  ],
  "published": "2003-01-31",
  "updated": "2003-01-31"
}