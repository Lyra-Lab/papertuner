{
  "id": "http://arxiv.org/abs/cs/0302015v1",
  "title": "Unsupervised Learning in a Framework of Information Compression by Multiple Alignment, Unification and Search",
  "authors": [
    "J. G. Wolff"
  ],
  "abstract": "This paper describes a novel approach to unsupervised learning that has been\ndeveloped within a framework of \"information compression by multiple alignment,\nunification and search\" (ICMAUS), designed to integrate learning with other AI\nfunctions such as parsing and production of language, fuzzy pattern\nrecognition, probabilistic and exact forms of reasoning, and others.",
  "text": "arXiv:cs/0302015v1  [cs.AI]  12 Feb 2003\nUnsupervised Learning in a Framework of\nInformation Compression by Multiple Alignment,\nUniﬁcation and Search\nJ Gerard Wolﬀ\nCognitionResearch.org.uk,\nEmail: gerry@cognitionresearch.org.uk.\n11 February 2003\nAbstract\nThis paper describes a novel approach to unsupervised learning that\nhas been developed within a framework designed to integrate learning\nwith such things as parsing and production of language, fuzzy pattern\nrecognition and best-match information retrieval, class hierarchies with\ninheritance of attributes, probabilistic and exact forms of reasoning, and\nothers. This framework, called information compression by multiple align-\nment, uniﬁcation and search (ICMAUS), is founded on principles of Min-\nimum Length Encoding pioneered by Solomonoﬀand others. This work\noriginated in an earlier programme of research into unsupervised learning\nof natural languages and many of the goals remain the same: to use a\nframework for knowledge representation that is powerful enough to ac-\ncommodate the structure of natural languages, to integrate the learning\nof segmental structure with the learning of disjunctive (class) structures,\nto distinguish ‘correct’ generalisations from ‘over’ generalisations without\nnegative samples or external error correction or the grading of samples\nfrom simple to complex, learning ‘correct’ forms despite corrupted data,\nand others.\nThe main body of the paper describes SP70, a computer\nmodel of the ICMAUS framework that incorporates processes for unsu-\npervised learning. Examples are presented to show how the model can\ninfer plausible grammars from appropriate input. Limitation of the cur-\nrent model and how they may be overcome are brieﬂy discussed.\nkeywords: unsupervised learning, information compression, multiple align-\nment, uniﬁcation, minimum length encoding.\n1\n1\nIntroduction\nThis paper describes a novel approach to unsupervised learning that has been\ndeveloped within a research programme whose overarching goal is the integration\nof diverse functions—learning, recognition, reasoning and others—within one\nrelatively simple framework. This has had a substantial impact on the way in\nwhich the learning processes are organised.\nFor reasons that will be explained, the conceptual framework that has been\ndeveloped within this research programme is called information compression by\nmultiple alignment, uniﬁcation and search (ICMAUS).\nThe next section describes the origins and motivation for the ICMAUS ap-\nproach and how it relates to other work in machine learning. This is followed\nby a section describing the ICMAUS theory in outline. The bulk of the article\ndescribes the SP70 model—a ﬁrst implementation of the ICMAUS approach to\nlearning—and shows what it can do. Limitations of the current model—and\nhow they may be overcome—are brieﬂy discussed.\n2\nOrigins, motivation and relation to other re-\nsearch\nSome years ago, I developed a computer model, MK10, that is quite successful\nat discovering, without supervision, words and other linguistic segments in un-\nsegmented natural language texts [Wolﬀ, 1980, 1977, 1975]. MK10 was further\ndeveloped into SNPR, a model that is able, without supervision, to discover\nplausible context-free phrase-structure grammars from unsegmented samples of\nartiﬁcial languages (Wolﬀ[1982, 1988], see also Langley and Stromsten [2000]).\nEach of these models may be seen to be largely a process of information\ncompression (IC) by matching patterns against each other and merging or ‘uni-\nfying’ patterns that are the same. Both models incorporate a process of search-\ning through the abstract space of alternative matches to ﬁnd those that yield\nrelatively large amounts of compression.\nTo a large extent, both models operate in accordance with principles of\nMinimum Length Encoding (MLE) described in Section 3.2, below.1\nIn the early 1980s, I was struck by the parallels that seemed to exist between\nmy two programs for unsupervised learning and the Prolog system, designed\noriginally for theorem proving. A prominent feature of both learning models is\nIC by pattern matching, uniﬁcation and search. Although IC is not a recognised\nfeature of Prolog, a process of searching for patterns that match each other is\nfundamental in that system and the merging of matching patterns is an impor-\ntant part of ‘uniﬁcation’ as that term is understood in logic. It seemed possible\nthat IC might have the same fundamental role in logic—and perhaps also in\n‘computing’ and mathematics—as it does in grammatical inference.\n1MLE is an umbrella term for Minimum Message Length (MML) encoding, Minimum\nDescription Length (MDL) encoding and related ideas such as ‘Kolmogorov complexity’ [see\nLi and Vit´anyi, 1997].\n2\nThese observations led to the thought that it might be possible to integrate\nunsupervised learning and logical inference within a single system, dedicated\nto IC by pattern matching, uniﬁcation and search. Further thinking suggested\nthat the scope of this integrated system might be expanded to include such\nthings as best-match information retrieval, fuzzy pattern recognition, parsing\nand production of language, probabilistic inference and other aspects of AI.\nSome of this thinking is described in Wolﬀ[1993].\n2.1\nDevelopment\nDevelopment of these ideas has been underway since 1987.\nGiven the overall goal of integrating diverse functions within a single system,\nit was evident quite early that the new system would need to be organised in\na way that was rather diﬀerent from the organisation of the MK10 and SNPR\nmodels. And, notwithstanding the development of Inductive Logic Program-\nming, it seemed that Prolog, in itself, was not suitable as a vehicle for the\nproposed developments—largely because of unwanted complexity in the system\nand because of the relative inﬂexibility of the search processes in Prolog. It\nseemed necessary to build the proposed new integrated system from new and\n‘deeper’ foundations.\nInitial eﬀorts focussed on the development of an improved version of ‘dy-\nnamic programming’ for ﬁnding full matches and good partial matches between\npairs of patterns [see, for example, Sankoﬀand Kruskall, 1983]. About 1994,\nit became apparent that the scope of the system could be greatly enhanced by\nreplacing the concept of ‘pattern matching’ with the more speciﬁc concept of\n‘multiple alignment’, similar to that concept in bio-informatics but with impor-\ntant diﬀerences.\nNotwithstanding the origin of these ideas in research on inductive learning,\nmost of the eﬀort to date has been focussed on such things as probabilistic\nreasoning [Wolﬀ, 2001, 1999b], natural language processing [Wolﬀ, 2000], and\nthe application of these ideas to the interpretation of ‘computing’ [Wolﬀ, 1999a]\nand concepts in mathematics and logic [Wolﬀ, 2002a]. Now, with the integration\nof learning capabilities within the new framework, the circle is largely complete.\n2.2\nGoals of the research\nAll of the goals that were set for the earlier research on the learning of natural\nlanguages have been carried forward into the ICMAUS programme. The main\nreason for this is that the unsupervised learning of a natural language remains\none of the biggest challenges for research in machine learning and it seems likely\nthat insights gained in that area will prove useful in other areas.\nBy no means all the goals have been met with the current model but long-\nterm goals that have yet to be met may, nevertheless, have a bearing on design\ndecisions made in current work.\n3\n2.2.1\nRepresentation of knowledge\nThe system for representing knowledge should be ‘powerful’ enough to accom-\nmodate the kinds of structures found in natural languages, both syntax and\nsemantics—and their integration.\nThe system should represent structures in a form that is readable and\ncomprehensible—which means the explicit, symbolic style associated with lin-\nguistics rather than the implicit style of most artiﬁcial neural networks.\n2.2.2\nThe goals of learning\nWith respect to learning, the system should be able to achieve the following\nthings:\n• It should be able to learn the kinds of segmental structures found in natural\nlanguages such as words, phrases and sentences without the need for these\nthings to be marked explicitly in the raw data.\n• It should be able to learn word classes and other distributionally-equivalent\ngroupings of segmental structures (e.g., ‘nouns’, ‘verbs’ and ‘adjectives’).\n• The learning of segmental structures and distributional classes should\nbe integrated and it should be possible to learn both kinds of structure\nthrough two or more levels of abstraction.\n• Ultimately, the system should be able to learn such things as discontinuous\ndependencies in syntax, it should be able to learn semantic structures and\nit should be able to learn structures that integrate syntax with semantics.\n• The system should be able to create rules that generalise the structures\nfound in the corpus of raw data, and it should be able to correct ‘over’\ngeneralisations that may be formed. This corresponds to childrens ability\nto generalise beyond the language that they hear and to correct the kinds\nof overgeneralisations that they make when they are very young.\n• The system should be able to learn ‘correct’ grammars despite corruption\nof the corpus of raw data.\nThis corresponds to the way in which we\ncan learn to distinguish sharply between utterances that belong in our\nnative language and those that do not, despite the fact that the language\nwe hear as children often contains false starts, incomplete sentences and\nother ungrammatical forms.\nApart from the learning of discontinuous dependencies and semantic struc-\ntures, all of these goals have already been met to some degree by the SNPR\nmodel [Wolﬀ, 1988, 1982]. In current work, the challenge has been to achieve\nthese goals in the new framework.\n4\n2.2.3\nUnsupervised learning\nThe weight of available evidence is that a child can learn his or her ﬁrst language\nor languages without error correction by a ‘teacher’, without the provision of\n‘negative’ samples (marked as ‘wrong’), and without the grading of language\nsamples from simple to complex. In short, children can apparently learn without\n‘supervision’ although they may take advantage of error correction and the like\nif it is available.\nUnless one subscribes to a Chomskyan ‘nativist’ view of language learning,\nthis feature of children’s learning appears to conﬂict with Gold’s [1967] proof\nthat language learning, conceived as “language identiﬁcation in the limit”, is\nnot possible without at least one of the sources of error correction mentioned\nabove.\nThis apparent conﬂict can be resolved if it is accepted that Gold’s concept\nof grammatical inference is not a good model of the way a child learns his or\nher native language. Rather than suppose that there is a ‘correct’ grammar\nwhich is the ‘target’ to be learned [see also Valiant, 1984], we may suppose that\nlearning is a process of optimisation.\nGiven that the abstract ‘space’ of potential grammars for any natural lan-\nguage is astronomically large, learning must rely on heuristic methods such as\nhill climbing. This means that it is not possible to know whether or not one\nhas found the ‘best’ grammar in terms of one’s chosen criterion. However, for\nall practical purposes, this does not matter. All that is necessary is to ﬁnd a\ngrammar that is ‘good enough’.\nIn the light of these considerations, this project has aimed to develop a\nlearning system conceived as an unsupervised process of optimisation, without\na target grammar.\n2.2.4\nMinimum Length Encoding\nA major goal in this research has been to develop the idea that MLE principles\n(described in Section 3.2) might serve as a unifying foundation for all kinds\nof information processing. Care has been taken to ensure that the ICMAUS\nframework should operate in accordance with those principles.\n2.2.5\nIntegration of learning with other AI functions\nAs already noted, a major goal of this research is the integration of learning with\nother AI functions such as pattern recognition, reasoning, planning and problem\nsolving, and others. This requirement has led to a substantial reorganisation of\nlearning processes compared with the earlier models (MK10 and SNPR) that\nwere dedicated purely to learning.\n2.2.6\nRealisation of the model in terms of neural mechanisms\nThe ICMAUS framework is intended as an abstract model of information pro-\ncessing in systems of all kinds, both artiﬁcial and natural. Given that it should\n5\nbe relevant to understanding brains and nervous systems as well as computers,\nthe model has been developed with an eye to its possible realisation in terms\nof neural mechanisms. Recent work has shown that the main elements of the\nmodel may indeed be instantiated in those terms [Wolﬀ, 2002b].\n2.3\nHow the research relates to other research on machine\nlearning\nThe way in which this research relates to other work on machine learning is\nbest seen in terms of the goals of the research, just described. It is, for example,\nmore closely related to research on grammar induction using phrase-structure\ngrammars (augmented or otherwise) than it is to research on the learning of\nDFAs, n-grams or Markov models—which are known to be less adequate for\nrepresenting the structure of natural languages [Chomsky, 1957]. The research\nis closer in spirit to other research on unsupervised learning than it is to research\non learning with external error correction or negative samples. There is a closer\nrelation with work in which learning is conceived as a process of optimisation\nusing MLE principles than with research on “language identiﬁcation in the limit”\nor related ideas. And there is a stronger aﬃnity with systems that are designed\nto learn explicit symbolic structures that are readable and comprehensible than\nwith systems—such as many artiﬁcial neural networks—that learn knowledge\nin an implicit form.\nStudies that are, perhaps, most closely related to the present research in-\nclude: Adriaans et al. [2000], Allison et al. [1992], Clark [2001], Denis [2001],\nHenrichsen [2002], Johnson and Riezler [2002], Klein and Manning [2001], Nevill-\nManning and Witten [1997], Oliveira and Sangiovanni-Vincentelli [1996], Rapp\net al. [1994], S.Watkinson and Manandhar [2001].\nCompared with all other work on unsupervised learning of grammar-like\nstructures, the most distinctive features of this research are:\n• The integration of learning with other areas of AI and computation.\n• The multiple alignment concept as it has been developed in the ICMAUS\nframework, described below.\n3\nInformation Compression by Multiple Align-\nment, Uniﬁcation and Search (ICMAUS)\nThe ICMAUS framework is intended as an abstract model of any kind of system\nfor computing or cognition, either natural or artiﬁcial. The overall organisation\nof the framework is as follows:\n• The system receives raw data from the world via its ‘senses’. These data\nare designated ‘New’. In the course of learning, New information is trans-\nferred to a repository of stored information, initially empty, which is des-\nignated ‘Old’.\n6\n• The system tries to compress each section of New as much as possible by\nsearching for matching patterns both within the given section and between\nthat section and information already stored in Old. If for example, the\npattern ‘information compression’ appears two or more times, it may be\nassigned a relatively short identiﬁer or ‘code’ (e.g., “IC”) which may then\nbe used as an abbreviation for that pattern wherever it appears.\n• In Old, the system stores repeating patterns of this kind. It also stores\nunmatched portions of New in case they may prove useful in compressing\nsections of New that come later. As we shall see, the system also creates\nand stores patterns that represent higher levels of abstraction. Each of\nthe patterns stored in Old has appropriate code symbols.\n• Periodically, the patterns in Old are evaluated to diﬀerentiate those that\nare proving useful in the encoding of New from those that are not. The\nlatter may be purged from the system.\nIn broad terms, this incremental scheme is similar to the well-known and\nwidely-used Lempel-Ziv algorithms for IC. What is diﬀerent about the ICMAUS\nscheme (as it has been developed for AI applications) is an emphasis on partial\nmatching and on relatively thorough searching of the space of alternative pos-\nsible matches. Also distinctive is the concept of ‘multiple alignment’ as it has\nbeen developed in this research to support the encoding of New information in\nterms of Old information in a hierarchy of ‘levels’ (as will be seen in examples\nbelow).\nInformation compression may be interpreted as a process of maximising Sim-\nplicity in information (by removing redundancy) whilst retaining as much as\npossible of its non-redundant, descriptive Power. Hence the sobriquet ‘SP’ that\nhas been adopted for the ICMAUS proposals and the computer models in which\nthe framework is realised.\n3.1\nRepresentation of Knowledge\nGiven the intended wide scope of the framework, a goal of the research has\nbeen to devise a ‘universal’ scheme for the representation of knowledge, capable\nof representing diverse kinds of knowledge in a succinct manner and capable\nof integrating diverse kinds of knowledge in a seamless manner.\nNaturally,\nthe design of the scheme would depend, in part, on the ways in which stored\nknowledge is going to be used.\nWhat has emerged is almost the simplest conceivable format for knowledge:\narrays or patterns of atomic symbols in one or more dimensions. So far, the\nfocus has been on one-dimensional arrays—‘strings’ or ‘sequences’ of symbols.\nHowever, since the system is intended eventually to embrace arrays in two di-\nmensions and possibly more, the relatively general term ‘pattern’ is normally\nused.\nIn this context, a ‘symbol’ is merely a ‘mark’ that can be discriminated in\na yes/no manner from other symbols. In all the examples in this paper, each\n7\nsymbol is represented by a string of one or more non-space characters bounded\nby spaces.\nIn general, symbols do not have ‘hidden’ meanings (e.g., “multiply” for the\nsymbol ‘×’)—any meanings that may attach to symbols within a given body of\nknowledge are to be expressed in the form of other symbols and patterns within\nthat knowledge.\nAn apparent exception to the slogan “no hidden meanings” is that, within\neach pattern, there is normally a distinction between symbols that represent the\nsubstance or contents of the pattern and other symbols that serve to identify\nor ‘encode’ the pattern (as will be described). This kind of distinction may be\njustiﬁed on the grounds that it is part of the mechanism by which the system\norganises and uses its knowledge. What the slogan really applies to are meanings\nthat are part of the knowledge itself.\nThe ‘granularity’ of symbols in the ICMAUS framework is undeﬁned. Sym-\nbols may be used to represent very ﬁne-grained details of a body of information\nor they may be used to represent relatively large chunks of information.\nDespite the extreme simplicity of the format for representing knowledge, the\nway it is processed within the ICMAUS framework means that it can model\na variety of established representational schemes including context-free phrase-\nstructure grammars (CF-PSGs), context-sensitive grammars, production rules,\nnetworks, trees and other schemes. Some examples will be seen below and many\nmore may be found in Wolﬀ[to appear, 2002a, 2000, 1999a,b].\nWith respect to the representation of structures in NL (Section 2.2.1), the\nICMAUS system can accommodate the major features of NL syntax, includ-\ning discontinuous dependencies [Wolﬀ, 2000], it can represent various kinds of\nnon-syntactic ‘semantic’ structures [see Wolﬀ, to appear] and preliminary work\nsuggests that syntax and semantics can indeed be integrated.\n3.2\nMinimum Length Encoding\nThe ICMAUS framework is founded explicitly on MLE principles pioneered by\nSolomonoﬀ[1997, 1964] and also by Wallace and Boulton [1968] and Rissanen\n[1978] [see also Li and Vit´anyi, 1997].\nThe key idea in MLE is that, in grammatical inference and related kinds\nof learning, one should seek to minimise (G + E), where G is the size of the\ngrammar (in bits or equivalent measure) and E is the size of the sample (in bits)\nafter it has been encoded in terms of the grammar. This principle guards against\nthe induction of grammars that are trivially small (where a small G is oﬀset by a\ndisproportionately large E) and it also guards against the induction of grammars\nwhere a small value for E is achieved at the cost of a disproportionately large\nvalue for G.\n3.2.1\nMLE and Human Intuition\nPart of the motivation for focussing on MLE principles is that they seem to have\na bearing on human perception and learning [see Chater, 1999, 1996, Wolﬀ, 1988]\n8\nand the human brain is currently the best learning system in existence. Human\nintuitions about what constitutes ‘good’ or ‘natural’ ways of structuring infor-\nmation do seem to seem to be conditioned to a large extent by MLE principles\n[Wolﬀ, 1988, 1977].\n3.2.2\nMLE and Bayesian principles\nIt is widely recognised that MLE principles may be translated into Bayesian\nconcepts of probability [see, for example, Eisner, 2002]. Although the two views\nare equivalent at one level of analysis, they diﬀer in terms of their heuristic\nvalue: primitive concepts of pattern matching and the uniﬁcation of identical\npatterns have a much more direct and transparent relation to IC than they do\nto probabilistic concepts. In this research, a focus on IC by the matching and\nuniﬁcation of patterns has proved to be a more fruitful source of new insights\nthan analyses in terms of probabilities.\n3.3\nComputer Models\nDuring the development of the ICMAUS framework, computer models have been\ndeveloped as a means of testing the ideas and also as a means of demonstrating\nwhat can be done with the framework.\nTwo main models have been developed to date:\n• SP61 which realises the encoding of New information in terms of pre-\ndeﬁned Old information but without any attempt to modify Old by adding\npatterns or purging them. SP61 also contains procedures for calculating\nprobabilities of inferences that may be drawn by the system, as described\nin Section 3.4.4, below.\n• SP70 which realises all four elements of the framework, including the ad-\ndition of patterns to Old and their evaluation in terms of MLE principles.\nAlthough SP61 is largely a subset of SP70, it is convenient to run it as a\nseparate model for some applications. The organisation of SP70 is described in\nSection 4, below.\n3.4\nApplications\nAs an indication of the scope of the ICMAUS framework, this subsection brieﬂy\nsketches some of the things that it can do.\nThe ﬁrst example (parsing of natural language) is described relatively fully\nbecause it serves to illustrate the main elements of the multiple alignment con-\ncept as it has been developed in this research. Owing to limitations of space,\nthe remaining examples of ICMAUS applications are necessarily brief.\nA much fuller overview of the framework and its capabilities is provided in\nWolﬀ[to appear] and more detail about speciﬁc aspects of the framework may\nbe found in other sources cited below.\n9\n3.4.1\nNatural Language Processing\nFigure 1 shows the best alignment found by SP61 with the sentence ‘o n e o f t h\ne m d o e s’ in New and patterns representing grammatical rules in Old. In this\ncontext, ‘best’ means the alignment that allows New to be encoded economically\nin terms of patterns in Old, as explained below.\nBy convention, the New pattern is always shown at the top of each alignment\nwith patterns from Old in the rows underneath, one in each row. The order of\nthe rows below the top row is entirely arbitrary and has no special signiﬁcance.\n0\no n e\no f\nt h e m\nd o e s\n0\n| | |\n| |\n| | | |\n| | | |\n1\n| | |\n| |\n< N Np 0 t h e m >\n| | | |\n1\n| | |\n| |\n| |\n|\n| | | |\n2\n| | |\n< Q 0 < P\n| | > < N\n> >\n| | | |\n2\n| | |\n| |\n| |\n| | |\n|\n| | | |\n3\n| | |\n| |\n< P 2 o f >\n|\n| | | |\n3\n| | |\n| |\n|\n| | | |\n4\n| | |\n| |\n|\n< V Vs 1 d o e s > 4\n| | |\n| |\n|\n| | |\n|\n5 S Num\n; < NP\n| | |\n| |\n| > < V |\n> 5\n|\n| | |\n| | |\n| |\n| |\n|\n6\n|\n| | |\n< N Ns 3 o n e > | |\n| |\n|\n6\n|\n| | |\n| | |\n| | |\n| |\n|\n7\n|\n| < NP 0 < N |\n> < Q\n> >\n|\n7\n|\n|\n|\n|\n|\n8\nNum SNG ;\nNs\nQ\nVs\n8\nFigure 1: The best alignment found by SP61 with ‘o n e o f t h e m d o e s’ in\nNew and patterns representing grammatical rules in Old.\nApart from the pattern in row 8, the patterns from Old in this example are\nlike re-write rules in a context-free phrase-structure grammar (CF-PSG) with\nthe re-write arrow omitted. If we ignore row 8, the alignment shown in Figure 1\nis very much like a conventional parsing, marking the main components of the\nsentence: words and phrases and the sentence pattern itself (shown in row 5).\nRow 8 shows how the ‘discontinuous’ dependency that exists between the\nsingular noun in the subject of the sentence (‘Ns’) and the singular verb (‘Vs’)\ncan be marked within the alignment in a relatively direct manner. Despite the\nsimplicity of the format for representing knowledge, the formation of multiple\nalignments enables the system to express ‘context sensitive’ aspects of language\nand other kinds of knowledge.\nWithin each pattern, some symbols have the status of identiﬁcation (ID)\nsymbols and others serve as contents (C) symbols. In general, ID symbols in\nany given pattern comprise bracket symbols (‘<’ and ’>’) that deﬁne the left\nand right boundaries of the pattern (where that is necessary) together with one\nor more symbols, usually near the start of the pattern, that serve to identify the\npattern uniquely within Old or otherwise deﬁne how it relates to other patterns.\nAll other symbols are C symbols. Examples of ID symbols in Figure 1 include\n‘< N Np 0 >’ in row 1, ‘< NP 0 >’ in row 7 and ‘Num SNG ; Q’ in row 8. The\ncorresponding C symbols are ‘t h e m’ in row 1, ‘< N > < Q >’ in row 7 and\n10\n‘Ns Vs’ in row 8.2\nAn encoding for New can be derived from each alignment by looking for\ncolumns in the alignment containing a single ID symbol, not matched to any\nother symbol, and copying these symbols into a code pattern in the same se-\nquence as they appear in the alignment.\nIn this example, the code pattern\nderived in this way is ‘S SNG 0 3 0 0 1’.\nA neat feature of the ICMAUS framework is that, without any modiﬁcation,\nit may be used to produce sentences as well as parse them. If the program is run\nagain with the sentence in New replaced by the code pattern ‘S SNG 0 3 0 0 1’,\nthe best alignment found by the system is, apart from the top row, the same as\nthe one shown in Figure 1. All the words in the original sentence appear within\nthe alignment in their correct order.\nMuch more detail, with many more examples, may be found in Wolﬀ[2000].\n3.4.2\nFuzzy Pattern Recognition and Best-Match Information Re-\ntrieval\nThe dynamic programming built into the SP models means that they are just as\nat home with ‘fuzzy’, partial matches between patterns as with exact matches.\nThis means they lend themselves well to modelling human-like capabilities for\nrecognising patterns and objects despite errors of omission, commission or sub-\nstitution, including the way in which we can recognise something even if it is\npartially obscured by other things. In a similar way, the models are able to\nmodel best-match retrieval of information from Old.\nExamples may be seen in Wolﬀ[to appear, 2001, 1999b].\n3.4.3\nClass Hierarchies with Inheritance of Attributes\nThe ICMAUS framework also lends itself quite well to modelling class hierarchies\nwith inheritance of attributes and recognition of objects or patterns at multiple\nlevels of abstraction. Any class and its associated attributes may be represented\nby a pattern and ‘isa’ links between classes may be established in much the same\nway as grammatical patterns are linked by means of matching symbols in the\nparsing example shown in Figure 1.\nExamples may be seen in Wolﬀ[to appear, 1999b].\n3.4.4\nProbabilistic Reasoning\nAny column within an alignment that does not contain a symbol from New\nmay be regarded as an inference that may be drawn from the alignment. For\nexample, the column in Figure 1 that contains (two instances of) the symbol\n‘V’ may be seen as representing the inference that ‘d o e s’ is a verb.\nThis and related aspects of the ICMAUS framework provide a powerful\nmeans of modelling several kinds of probabilistic inference including probabilis-\n2Notice that the distinction between ID symbols and C symbols does not depend on their\nappearance. The status of each symbol within a pattern is marked when the pattern is created.\n11\ntic chains of reasoning, abductive reasoning, default reasoning, nonmonotonic\nreasoning, the phenomenon of “explaining away” and the solution of geometric\nanalogy problems. These aspects of the ICMAUS framework are explained quite\nfully, with examples, in Wolﬀ[1999b] [see also Wolﬀ, 2001].\n3.4.5\nModelling ‘Computing’ and Concepts in Mathematics and\nLogic\nIn Wolﬀ[1999a], I have argued that the ICMAUS framework provides an in-\nterpretation for the Turing model of ‘computing’ and equivalent models such\nas the Post Canonical System. Although the ICMAUS model is a little more\ncomplex than earlier models it appears to illuminate a range of issues (mainly\nin AI) that are outside the scope of earlier models.\nIn Wolﬀ[2002a], I have argued that the ICMAUS framework provides an\ninterpretation for a range of concepts in mathematics and logic, including both\nthe static structures found in those disciplines and the dynamics of calculation\nand inference.\n4\nSP70\nAll the main components of the ICMAUS framework outlined in Section 3 are\nnow realised within the SP70 software model (version 9.2).\nIn the description of the model that follows, the examples have a ‘linguistic’\nﬂavour with an emphasis on the simpler aspects of English syntax. However,\nthe term ‘grammar’ in this context should be construed very broadly since, as\nwe noted in Section 3.1, it assumed that similar principles may be applied to\nmany diﬀerent kinds of knowledge.\nThe model gives results in the area of unsupervised learning that are good\nenough to show that the framework is sound. As we shall see, the model is\nable to abstract plausible grammars from sets of simple sentences without prior\nknowledge of word segments or the classes to which they belong (Section 6) and\nthe computational complexity of the model appears to be acceptable (Section\n5.1).\nHowever, in its current form, the model has at least two signiﬁcant short-\ncomings and some other deﬁciencies.\nA programme of further development,\nexperimentation and reﬁnement is still needed to realise the full potential of the\nmodel for unsupervised learning.\nThis model is governed by mathematical principles—explained at pertinent\npoints below—but these are remarkably simple and in accordance with estab-\nlished theory. The main focus in what follows is on the organisation of the\nmodel and the computational techniques employed within it.\n4.1\nObjectives\nIn keeping with the goals of learning described above (Section 2.2.2), the main\nproblems addressed in the development of this model have been:\n12\n• How to identify signiﬁcant segments in the ‘corpus’ of data which is the\nbasis of learning when the boundary between one segment and the next is\nnot marked explicitly.\n• How to identify classes of syntactically-equivalent segments.\n• How to combine the learning of segmental structure with the learning of\ndisjunctive classes.\n• How to learn segments and disjunctive classes through two or more levels\nof abstraction.\n• How to generalize grammatical rules beyond the data and how to correct\nover-generalizations without feedback from a ‘teacher’ or the provision of\n‘negative’ samples or the grading of the data from ‘easy’ to ‘hard’.\nSolutions to these problems were found in the SNPR model [Wolﬀ, 1988,\n1982] but, as noted earlier, the organisation of this model is quite unsuited to\nthe wider goals of the present research—integration of diverse functions within\none framework. Finding new solutions to these problems within the ICMAUS\nframework has been a signiﬁcant challenge.\nThe SP70 model (v. 9.2) provides solutions to the ﬁrst three problems and\npartial solutions to the fourth and ﬁfth problems. Further development will\nbe required to achieve robust learning of structures with more than two levels\nof abstraction and more work is required on the generalization of grammatical\nrules and the correction of overgeneralizations (see Section 7, below).\n4.2\nOverall Structure of the Model\nFigure 2 shows the high-level organisation of the SP70 model. The program\nstarts with a set of New patterns and with a repository of Old patterns that is\ninitially empty.\nIn broad terms, the model comprises two main phases:\n• Create a set of patterns that may be used to encode the patterns from\nNew in an economical manner (operations 1 to 3 in Figure 2).\n• From the patterns created in the ﬁrst phase, compile one or more alterna-\ntive ‘grammars’ for the patterns in New in accordance with MLE principles\n(operation 4 in Figure 2).\nAfter reading a set of patterns into New, the system compiles an ‘alphabet’\nof the diﬀerent types of symbols appearing in those patterns, counts their fre-\nquencies of occurrence and calculates encoding costs as described below. These\nvalues will be needed for the evaluation of alignments. The term symbol type in\nthis connection means a representative template or example of a set of identical\nsymbols.\nNext (operation 3), each pattern from New is processed by searching for\nalignments that allow the given pattern to be encoded economically in terms\n13\nSP70()\n{\n1 Read a set of patterns into New. Old is initially empty.\n2 Compile an alphabet of symbol types in New and, for each type,\nfind its frequency of occurrence and the number of bits\nrequired to encode it.\n3 While (there are unprocessed patterns\nin New)\n{\n3.1 Identify the first or next pattern from New as the\n‘current pattern from New’ (CPFN).\n3.2 Apply the function CREATE_MULTIPLE_ALIGNMENTS() to\ncreate multiple alignments, each one between the\nCPFN and one or more patterns from Old.\n3.3 During 3.2, the CPFN is copied into Old, one symbol\nat a time, in such a way that the CPFN can be\naligned with its copy but that any one symbol in\nthe CPFN cannot be aligned with the corresponding\nsymbol in the copy.\n3.4 Sort the alignments formed by this function in order\nof their compression scores.\n3.5 From amongst the best alignments, select a subset\nthat conforms to other constraints described in the\ntext.\n3.6 Process the selected alignments with the function\nDERIVE_PATTERNS(). This function derives encoded\npatterns from alignments and adds them to Old.\n}\n4 Apply the function SIFTING_AND_SORTING() to create one or\nmore alternative grammars for the patterns in New, each\none scored in terms of MLE principles. Each grammar is\na subset of the patterns in Old.\n}\nFigure 2:\nThe organisation of SP70.\nThe workings of the functions cre-\nate multiple alignments(), derive patterns() and sifting and sorting() are ex-\nplained in Sections 4.3, 4.5 and 4.6, respectively.\nof patterns in Old (as outlined in Section 3.4.1). From a selection of the best\nalignments found, the program ‘learns’ new patterns, as explained below, and\nadds them to Old. A copy of each pattern from New is also added to Old,\nmarked with new ID symbols as will be explained.\nWhen all the patterns from New have been processed in this way, there is\na process of sifting and sorting to create one or more alternative grammars for\nthe patterns from New (operation 4). Each grammar comprises a subset of the\npatterns in Old and each one is scored in terms of MLE principles.\n14\n4.3\nCreating Multiple Alignments\nThe function create multiple alignments() referred to in Figure 2 creates zero or\nmore multiple alignments, each one comprising the current pattern from New\n(CPFN3) and one or more patterns from Old. Each alignment has a compression\nscore which is a measure of the amount of compression of the CPFN that can\nbe achieved by encoding it in terms of patterns from Old.\nApart from some minor modiﬁcations and improvements, this function is\nessentially the same as the main component of the SP61 model, described quite\nfully in Wolﬀ[2000]. Readers are referred to this source for a more detailed\ndescription of how multiple alignments are formed in the ICMAUS framework.\nAt the heart of the function is a process for ﬁnding full alignments and good\npartial alignments between pairs of patterns. This process, described quite fully\nin Wolﬀ[1994], is a version of ‘dynamic programming’ [see Sankoﬀand Kruskall,\n1983] with advantages over standard methods:\n• List processing techniques allow memory to be used eﬃciently so that long\npatterns may be matched.\n• In cases where a pair of patterns can be matched in more than one way, the\nprocess can deliver two or more alternative alignments, each one scored in\nterms of compression.\n• The ‘depth’ or thoroughness of searching—and thus the speed of\nprocessing—can be varied.\nThis matching process is applied iteratively to build alignments containing\ntwo or more rows, one pattern per row. The process stops when no more align-\nments can be found or when compression scores have reached a peak and have\nfallen for two or three cycles.\nOn the ﬁrst cycle, the matching process is applied to ﬁnd alignments, each\none between the CPFN and one of the patterns stored in Old. When this has\nbeen completed, they are sorted in order of their compression scores and several\nof the best are selected for further processing. In general, the alignments that are\nselected are ones that can be treated as if they were a single sequence (pattern)\nof symbols, as described in Section 4.3.3, below.\nIn the second and subsequent cycles of the building process, two or three of\nthe best alignments selected in the previous cycle are chosen as ‘driving’ patterns\nand matched against ‘target’ patterns comprising the current set of patterns in\nOld together with all of the alignments selected in previous cycles (including\nthe driving patterns themselves). Each of the resulting alignments is between\none driving pattern and one target pattern. As on the ﬁrst cycle, several of the\nbest alignments formed are selected for further processing provided they can be\ntreated as if they were single sequences of symbols.\n3Where the word “current” is not required, a pattern from New will be referred as a PFN.\n15\n4.3.1\nCalculation of Compression Scores\nThe compression score for an alignment is calculated as:\nC = Nr −Ne\nwhere Nr is the size, in bits, of the CPFN in its uncompressed ‘raw’ form and Ne\nis the size, in bits, of the code pattern derived from the alignment as indicated\nin Section 3.4.1. Nr is calculated as:\nNr =\ni=n\nX\ni=1\nsi\nwhere si is number of bits required to encode the ith symbol in the sequence of\nn symbols in the CPFN. Ne is calculated in the same way from the symbols in\nthe code pattern.\nWith a qualiﬁcation to be described, the number of bits required to encode\nany given symbol type that appears in New (its ‘encoding cost’) is calculated\nusing the Shannon-Fano-Elias (SFE) method [see Cover and Thomas, 1991].\nThis method is similar to the well-known Huﬀman coding method and gives\nsimilar results—but it has advantages over the Huﬀman method when codes\nare used for the calculation of probabilities [see Wolﬀ, 1999b]. As noted above,\nthe values for the frequencies of symbol types that are required for this method\nare computed (in operation 2 in Figure 2) from the set of patterns in New.\nNotice that the encoding cost of any symbol is totally independent of the size\nof the symbol as it appears to the reader. In general, symbols are represented\nby character strings chosen for reasons of readability or mnemonic value. These\nstrings are quite independent of the number of bits required to discriminate one\nsymbol from another in an eﬃcient manner.\nThe qualiﬁcation mentioned above is that the encoding costs of symbol types\nappearing in New (calculated by the SFE method) are multiplied by a cost\nfactor, normally about 5 or 10.\nThis means that symbols representing the\noriginal raw ‘data’ for the program are treated by the system as if they were\nrelatively large chunks of information—by contrast with other symbols that are\nused to encode the data (added by the system in the course of learning) which\nare not given any additional weighting. The reason for applying this cost factor\nto data symbols is that, without this weighting, the best grammar found for the\nkind of small example that is convenient for experimentation and demonstration\nis often simply a repetition of the original patterns, without any recognition\nof structures within those patterns. If the data symbols are treated as if they\nwere larger chunks of information, then the beneﬁts of recognising substructures\noutweighs the costs of encoding those structures. With larger examples, where\nfrequency values for substructures will normally be higher, this problem should\ndisappear.\nAs learning proceeds, patterns are added to Old (operation 3.6 in Figure\n2).\nWithin those patterns, some of the symbols are derived from New and\ntheir encoding costs are already known. However, other symbols—ID symbols\n16\nand copies of them—are created by the system in operation 3.6 (Figure 2) and\nthe variety of types of these symbols and their frequencies of occurrence are\nconstantly changing. For this reason, it is diﬃcult at this stage to calculate\nencoding costs using the SFE method. Accordingly, the encoding costs of sym-\nbols created by the system are initially set at a ﬁxed arbitrary value. As we\nshall see, more precise values are calculated in the sifting and sorting() phase\nof processing. The approximation at this stage does not seem to be a serious\nimpediment to learning, perhaps because the selection of alignments depends\non relative values for compression scores, not absolute values.\nThe search for alignments that maximise C conforms with MLE principles as\noutlined in Section 3.2. The repository of Old patterns may be taken to be the\ncurrent ‘grammar’ for encoding the CPFN and, for any given CPFN, the size\nof this grammar, G, is constant.4 Since G is constant, the goal of minimising T\nis equivalent to a goal of minimising E. For any given CPFN, E is the same as\nNe. Thus, since Nr is constant for any given CPFN, seeking to minimise E is\nequivalent to the attempt to maximise C.\n4.3.2\nConstraints on Matching when a Given Pattern Appears Two\nor More Times in an Alignment\nAn important point to notice about multiple alignments in the ICMAUS frame-\nwork is that any pattern may appear two or more times in one alignment. For\nexample, in a sentence like The winds from the west are strong, there are two\ninstances of a simple form of noun phrase (the winds and the west) so that, in a\nmultiple alignment parsing of the sentence, there would be two appearances of\nthe pattern representing the structure of that form of noun phrase.\nNotice that multiple appearances of one pattern within an alignment are not\nthe same as multiple copies of the pattern within an alignment. In the latter\ncase, there are two or more distinct patterns and it is quite acceptable for them\nto be fully aligned, one with another. In the former case, there is only a single\npattern so it is not permissible for a symbol in one appearance to be matched\nagainst the corresponding symbol in another appearance—because this would\nmean matching the given symbol with itself. However, it is permissible to form\na match between one symbol within the given pattern and another symbol (in\nanother position) in the same pattern.\nIn any situation where a given pattern is matched—directly or indirectly—\nwith itself, the matching process in the create multiple alignments() function is\nconstrained to prevent any one symbol being matched with itself.\n4As indicated in operation 3.3 of Figure 2 and described in Section 4.4, below, a copy of the\nCPFN is added to Old during the process of building alignments. However, for the purpose\nof encoding the CPFN, the entire copy (with ID symbols that are added to the copy) counts\nas part of Old and thus, for any given CPFN, G is indeed constant.\n17\n4.3.3\nConstraints on “Mismatches” between Patterns and the Treat-\nment of Alignments as Simple Sequences\nA mismatch in an alignment occurs where one or more unmatched symbols in\none pattern appear opposite one or more unmatched symbols in another pattern\nwithin the alignment. Symbols are ‘opposite’ each other if they lie between two\ncolumns of matched symbols or between one column of matched symbols and\nthe beginning or end of the alignment.\nIn the ICMAUS framework, mismatches are illegal if they occur between\npatterns from Old. If any alignment contains that kind of mismatch, it is dis-\ncarded. Notice that a mismatch between the CPFN and any pattern from Old\nis legal and, as we shall see, it is those kinds of mismatches (and, more generally,\nunmatched C symbols within alignments) that drive the learning process.\nIn each of the alignments shown in Figure 3, row 0 contains a pattern from\nNew and rows 1 and 2 contain patterns from Old. Where ‘x’ and ‘y’ lie opposite\neach other (in (a) and (b)), there is an illegal mismatch. Where ‘b’ or ‘c’ lies\nopposite ‘x’ or ‘y’ (or both) (in all four alignments), the mismatch is legal.\n0 a b c 0\n0 a b c 0\n|\n|\n| |\n1 a x c 1\n1 a b x 1\n|\n|\n| |\n2 a y c 2\n2 a b y 2\n(a)\n(b)\n0 a b c 0\n0 a b c 0\n|\n|\n| |\n1 a x c 1\n1 a b\n1\n|\n|\n| |\n2 a\nc 2\n2 a b y 2\n(c)\n(d)\nFigure 3: Alignments illustrating mismatches as discussed in the text.\nAs was noted above, the process of building multiple alignments requires\nthat each alignment created in the intermediate stages can be treated as a single\nsequence of symbols. The critical issue here is whether or not a given alignment\ncontains any illegal mismatches. If it does contain illegal mismatches, it cannot\nbe treated as a single sequence. Otherwise, it can. Notice that mismatches\nbetween the CPFN and other patterns are of no consequence. For example,\nalignment (c) in Figure 3 may be treated as the sequence ‘a x c’ while alignment\n(d) in the same ﬁgure may be treated as ‘a b y’. All unmatched symbols within\nthe CPFN are simply ignored.\n18\n4.4\nCopying the CPFN into Old\nIn its bare essentials, ‘learning’ in SP70 is achieved by the addition of patterns\nto Old. This occurs in two ways: by copying each pattern from New into Old\n(operation 3.3 in Figure 2) and by deriving patterns from alignments in the\nfunction derive patterns() (operation 3.6 in the same ﬁgure). The ﬁrst of these\nis described here and the second is described in the next subsection.\nDuring the matching process in the ﬁrst cycle of the\ncreate multiple alignments() function, the CPFN is copied, one symbol at a time,\ninto Old in such a way that any symbol in the CPFN can be matched with any\nearlier symbol in the copy but it cannot be matched with the corresponding\nsymbol in the copy or any subsequent symbol. When the transfer is complete,\nID symbols are added to the copy to provide a ‘code’ for the pattern, as described\nbelow.\nThe aim here is to detect any redundancy that may exist within each pattern\nfrom New (e.g., the repetition that can be seen in the pattern ‘a b c d x y z a\nb c d’) but to avoid detecting the redundancy resulting from the fact that the\nCPFN has been copied into Old. This constraint is imposed for very much the\nsame reason as the constraint (described in Section 4.3.2, above) which prevents\nany one symbol within an alignment being matched with itself.\nThe reason for copying each pattern from New into Old rather than simply\nmoving it is that each such pattern (with its ID symbols) is a candidate for in-\nclusion in one or more of the best grammars selected by the sifting and sorting()\nfunction and it cannot be evaluated properly unless it is a copy of the corre-\nsponding pattern from New, not the pattern itself (see Section 4.6, below).\nThe ID symbols that are added to the copy of each CPFN comprise left\nand right brackets (‘<’ and ‘>’) at each end of the pattern together with\nsymbols immediately after the left bracket that serve to identify the pattern\nuniquely amongst the patterns in Old. For the sake of consistency with the\nderive patterns() function (see Section 4.5, next), two ID symbols follow the left\nbracket. Thus, for example, a pattern from New like ‘t h a t b o y r u n s’ might\nbecome ‘< %1 9 t h a t b o y r u n s >’ when ID symbols have been added.\n4.5\nDeriving Patterns from Alignments\nIn operation 3.6 in Figure 2, the derive patterns() function is applied to a selec-\ntion of the best alignments formed and, in each case, it looks for sequences of\nunmatched symbols within the alignment and also sequences of matched sym-\nbols.\nConsider the alignment shown in Figure 4. From an alignment like that, the\nfunction ﬁnds the unmatched sequences ‘g i r l’ and ‘b o y’ and, within row 1, it\nalso ﬁnds the matched sequences ‘t h a t’ and ‘r u n s’. With respect to row 1,\nthe focus of interest is the matched and unmatched sequences of C symbols—ID\nsymbols are ignored.\nA copy of each of the four sequences is made, ID symbols are added to\neach copy (as described in Section 4.5.1, below) and the copy is added to Old.\n19\n0\nt h a t g i r l r u n s\n0\n| | | |\n| | | |\n1 < %1 9 t h a t b o y\nr u n s > 1\nFigure 4: A simple alignment from which other patterns may be derived.\nIn addition, another ‘abstract’ pattern is made that records the sequence of\nmatched and unmatched patterns within the alignment. The result in this case\nis ﬁve patterns like those shown in Figure 5.\n< %7 12 t h a t >\n< %9 14 b o y >\n< %9 15 g i r l >\n< %8 13 r u n s >\n< %10 16 < %7 > < %9 > < %8 > >\nFigure 5: Patterns derived from the alignment shown in Figure 4.\nIt should be clear that the set of patterns in Figure 5 is, in eﬀect, a simple\ngrammar for the two sentences in Figure 4, with patterns representing gram-\nmatical rules in much the same style as those shown in Figure 1. The abstract\npattern ‘< %10 220 < %7 > < %9 > < %8 > >’ describes the overall struc-\nture of this kind of sentence with slots that may receive individual words at\nappropriate points in the pattern.\nNotice how the symbol ‘%9’ serves to mark ‘b o y’ and ‘g i r l’ as alternatives\nin the middle of the sentence.\nThis is a grammatical class in the tradition\nof distributional or structural linguistics [see, for example, Fries, 1952, Harris,\n1951].\nWith alignments like this:\n0\nt h e g r e e n a p p l e\n0\n| | |\n| | | | |\n1 < %1 2 t h e\na p p l e > 1\nor this:\n0\nt h e\na p p l e\n0\n| | |\n| | | | |\n1 < %1 2 t h e g r e e n a p p l e > 1\nthe system derives patterns very much as before except that the unmatched\nsequence (‘g r e e n’) is assigned to a class by itself, without any alternative\npattern that may appear in the same context. Arguably, there should be some\nkind of ‘null’ alternative to ‘g r e e n’ in cases like this in order to capture the\nidea that “the apple” and “the green apple” are acceptable variants of the same\nphrase. This is a possible reﬁnement of the model in the future.\n20\nReaders may wonder why the grammar shown in Figure 5 was not simpliﬁed\nto something like this:\n< %9 14 b o y >\n< %9 15 g i r l >\n< %10 16 t h a t < %9 > r u n s >\nThe main reason for adopting the style shown in Figure 5 is that the overall\norganisation of the model is simpler if each newly-derived pattern is automati-\ncally referenced from the contexts or contexts in which it may appear. Another\nreason is that it is anticipated that, with realistically large corpora, most of the\npatterns that will ultimately turn out to be signiﬁcant in terms of MLE princi-\nples will appear in two or more contexts and, in that case, MLE principles are\nlikely to dictate that each pattern should be referenced from each of its contexts\nrather than written out redundantly in each of the two or more places where it\nappears.\n4.5.1\nAssignment of Identiﬁcation Symbols\nApart from the terminating brackets, each pattern in Figure 5 has two ID sym-\nbols:\n• A ‘class’ symbol (e.g., ‘%7’ or ‘%9’) that normally starts with the ‘%’\ncharacter. The class symbol is, in eﬀect, a reference to the context or\ncontexts in which the given pattern may appear. Thus, for example, the\nsymbol ‘%7’ in the ﬁrst pattern in Figure 5 shows that that pattern may\nappear where the matching symbol occurs in the pattern ‘< %10 220 <\n%7 > < %9 > < %8 > >’. Any one pattern may belong to more than\none class and should contain a symbol for each of the classes it belongs to\n(see Section 4.5.2).\n• A ‘discrimination’ symbol (e.g., ‘12’, ‘14’) that serves to distinguish the\npattern from any others that may belong in the same class. At this stage,\nthe discrimination symbol is simply a unique identiﬁer for the given pat-\ntern amongst the other patterns and alignments created by the program.\nWhile alignments are being built and coded patterns are being added to Old,\nnew class symbols and new discrimination symbols are created quite liberally.\nHowever, many of these symbols are weeded out during the sifting and sorting()\nphase of processing and those that remain are renamed in a tidy manner.\n4.5.2\nAvoiding Duplication\nIn the course of deriving patterns from alignments and adding them to Old, it\ncan easily happen that a newly-derived pattern has the same C symbols as one\nthat is already in Old. For this reason, each newly-derived pattern is checked\nagainst patterns already stored in Old and it is discarded if an existing pattern\nis found with the same C symbols.\n21\nAlthough the discarded pattern has the same C symbols as a pre-existing\npattern, it comes from a diﬀerent context. So a new symbol type is created to\nrepresent that context, a copy of the symbol type is added to the pre-existing\npattern, and another copy of the symbol type is added to the abstract pattern in\nthe appropriate position. In this way, any one sequence of C symbols may appear\nin a pattern containing several diﬀerent class symbols, each one representing one\nof the contexts where the C symbols may appear.\nAs the program stands, there is a one-to-one relation between contexts and\nclasses. But it can easily happen that the set of patterns that may appear in\none context is the same as the set of patterns that may appear in another. At\nsome stage, it is intended that the program will be augmented to check for this\nkind of redundancy and to merge classes that turn out to be equivalent.\n4.5.3\nDeriving Patterns from Alignments Containing Three or More\nRows\nFor any given alignment, the derive patterns() function works by looking for\none or more unmatched sequences of symbols in the CPFN, or one or more\nsequences of unmatched C symbols in a pattern from Old, or both these things.\nWhat happens if an alignment contains two or more patterns from Old?\nConsider the alignment shown in Figure 6. In a case like this, it is necessary\nto identify one of the patterns from Old for the purpose of deriving patterns\nfrom the alignment. The pattern that is chosen is the one that is deemed to be\nthe most ‘abstract’ pattern amongst those in the rows below the top row.\n0\nt h e\nr e d\na p p l e\nf a l l s\n0\n| | |\n| | | | |\n| | | | |\n1\n| | |\n| | | | |\n< %4 5 f a l l s >\n1\n| | |\n| | | | |\n| |\n|\n2 < %5 7 < %1\n| | | > < %2 > < %3\n| | | | | > < %4\n> > 2\n| |\n| | | |\n| |\n| | | | | |\n3\n| |\n| | | |\n< %3 3 a p p l e >\n3\n| |\n| | | |\n4\n< %1 0 t h e >\n4\nFigure 6: An alignment (between a pattern from New and four patterns from\nOld) from which other patterns may be derived.\nThe most abstract row in any alignment is the row below the top row that\nstarts furthest to the left within the alignment, e.g., row 5 in Figure 1 and\nrow 2 in Figure 6.\nTypically, this is also the row that ﬁnishes furthest to\nthe right. In general, this is the row within any alignment that, directly or\nindirectly, encodes the largest number of symbols from the CPFN. Of course, if\nan alignment contains only two rows, then row 1 is the most abstract row.\nTo be a suitable candidate for processing by the derive patterns() function,\nthe only row below the top row that may contain unmatched C symbols is\nthe most abstract row. Also, there must be at least one unmatched C symbol\n22\nsomewhere within the CPFN and the most abstract row. Any alignment that\ndoes not meet these conditions is discarded for the purpose of deriving new\npatterns.\nFrom the alignment shown in Figure 6, the function creates patterns like\nthose shown in Figure 7 and adds them to Old.\n< %2 9 r e d >\n< %7 10 < %3 > < %4 > >\n< %8 11 < %1 > < %2 > < %7 > >\nFigure 7: A set of patterns derived from the alignment shown in Figure 6.\nHere, the unmatched sequence ‘r e d’ from the CPFN has been converted into\nthe pattern ‘< %2 9 r e d >’. The system recognises that ‘r e d’ lies opposite\nthe sequence ‘< %2 >’ within row 2 of Figure 6 and that this sequence is a\nreference to the class ‘%2’. Accordingly, the pattern ‘r e d’ has been assigned to\nthat class. If the unmatched sequence opposite ‘r e d’ could not be recognised\nas a reference to a class, or if there was no unmatched sequence opposite ‘r e d’,\nthen the system would create a new class and new patterns in the same manner\nas we saw in the examples at the beginning of Section 4.5.\nThe second pattern in Figure 7 (‘< %7 10 < %3 > < %4 > >’) is derived\nfrom the sequence ‘< %3 > < %4 >’ within the abstract pattern in row 2 of\nFigure 6. The third pattern (‘< %8 11 < %1 > < %2 > < %7 > >’) is a new\nversion of that abstract pattern that references the class of the second pattern\n(‘%7’).\n4.5.4\nRedundancy in Old\nGiven that the system is dedicated to IC, it may seem strange that, at this stage\nof processing, there may be considerable replication of information (redundancy)\namongst the patterns in Old. Patterns are added to Old but, at this stage,\nnothing is removed from Old. In the example just considered, the pattern ‘<\n%7 10 < %3 > < %4 > >’ coexists with the pattern ‘< %5 7 < %1 > < %2\n> < %3 > < %4 > >’ even though they both contain the sequence ‘< %3 > <\n%4 >’. In the example from Figures 4 and 5, the patterns ‘< %7 12 t h a t >’,\n‘< %9 14 b o y >’ and ‘< %8 13 r u n s > coexist with the pattern ‘< %1 9 t\nh a t b o y r u n s >’ despite the obvious duplication of information amongst\nthese patterns.\nThe reason for designing the system in this way is that there is no guarantee\nthat any given pattern derived from an alignment will ultimately turn out to be\n‘correct’ in terms of MLE principles or one’s intuitions about what the correct\ngrammar should be. Indeed, many of the patterns abstracted by the system are\nclearly ‘wrong’ in these terms. Retention of older patterns in the store alongside\npatterns that have been derived from them leaves the door open for the system\nto create ‘correct’ patterns at later stages regardless of whether ‘wrong’ patterns\n23\nhad been created earlier. In eﬀect, the system is able to explore alternative paths\nthrough the abstract space of possible patterns.\n4.6\nSifting and Sorting of Patterns\nIdentiﬁcation of ‘wrong’ patterns occurs in the sifting and sorting() stage of\nprocessing (operation 4 in Figure 2), where the system develops one or more al-\nternative grammars for the patterns in New in accordance with MLE principles.\nFigure 8 shows the overall structure of the sifting and sorting() function.\nEach pattern in Old has an associated frequency of occurrence and, at the\nstart of the function, all these values are set to zero. Then, all the patterns\nin New are reprocessed with the create multiple alignments() function, building\nmultiple alignments as before, each one between one pattern from New and one\nor more patterns in Old.\nThe diﬀerence on this occasion is that, for each CPFN, the best alignments\nare ﬁltered to remove any that contain unmatched symbols in the CPFN or\nunmatched C symbols in any pattern from Old. The remaining ‘full’ alignments\nprovide the basis for further processing.\nSIFTING_AND_SORTING()\n{\n1 For each pattern in Old, set its frequency of occurrence to 0.\n2 While (there are still unprocessed patterns in New)\n{\n2.1 Identify the first or next pattern from New as the CPFN.\n2.2 Apply the function CREATE_MULTIPLE_ALIGNMENTS() to\ncreate multiple alignments, each one between the CPFN\nand one or more patterns from Old.\n2.3 From amongst the best of the multiple alignments formed,\nselect ‘full’ alignments in which all the symbols of\nthe CPFN are matched and all the C symbols are\nmatched in each pattern from Old.\n2.4 For each pattern from Old, count the maximum number of\ntimes it appears in any one of the full alignments\nselected in operation 2.3. Add this count to the\nfrequency of occurrence of the given pattern.\n}\n3 Compute frequencies of symbol types and their encoding costs.\nFrom these values, compute encoding costs of patterns in\nOld and new compression scores for each of the full\nalignments created in operation 2.\n4 Using the alignments created in 2 and the values computed in\noperation 3, COMPILE_ALTERNATIVE_GRAMMARS().\n}\nFigure 8: The organisation of the sifting and sorting() function.\nThe com-\npile alternative grammars() function is described in Section 4.6.1.\nIn this phase of the program, we can be conﬁdent of ﬁnding at least one\nfull alignment for each CPFN because, in the previous phase, each unmatched\n24\nportion of the given pattern from New led to the creation of patterns in Old\nthat would provide an appropriate match in the future.\nWhen all the patterns from New have been processed in this way, there is a\nset A of full alignments, divided into b1...bm disjoint subsets, one for each PFN.\nFrom these alignments, the function computes the frequency of occurrence of\neach of the p1...pn patterns in Old as:\nfi =\nj=m\nX\nj=1\nmax(pi, bj)\nwhere max(pi, bj) is the maximum number of times that pi appears in any\none alignment in subset bj. Using the maximum value for any one alignment\nfor a given PFN is necessary because the alignments in each bj are alternative\nanalyses of the corresponding PFN. If we simply counted the number of times\neach pattern appeared in all the alignments for a given PFN, the frequency\nvalues would be too high.\nThe function also compiles an alphabet of the symbol types, s1...sr, in the\npatterns in Old and, following the principles just described, computes the fre-\nquency of occurrence of each symbol type as:\nFi =\nj=m\nX\nj=1\nmax(si, bj)\nwhere max(si, bj) is the maximum number of times that si appears in any one\nalignment in subset bj.\nFrom these values, the encoding cost of each symbol type is computed using\nthe SFE method as before [Cover and Thomas, 1991]. As before (Section 4.3.1),\nthe encoding cost of each of the ‘data’ symbol types (those that appears in New)\nis weighted so that data symbols behave as if they were relatively large chunks\nof information.\nEach symbol in each pattern in New and Old is then assigned the frequency\nand encoding cost of its type. With these values in place, the compression score\nof each alignment in the set of full alignments is recalculated.\nFinally, in operation 4 of Figure 8, a set of one or more alternative grammars\nis compiled, as described in Section 4.6.1.\nAs the program stands, these alternative grammars are simply presented to\nthe user for inspection. However, it is intended that the patterns in Old should\nbe purged of all its patterns except those in the best grammar that has been\nfound. It is anticipated that the program will be developed so that patterns\nfrom New will be processed in batches and that this kind of purging of Old will\noccur at the end of each batch to remove the ‘rubbish’ and retain only those\npatterns that have proved useful in encoding the cumulative set of patterns from\nNew.\n25\n4.6.1\nCompiling a Set of Alternative Grammars\nThe set of alternative grammars for the patterns in New are derived (in the\ncompile alternative grammars() function) from the full alignments created in\noperation 2 of Figure 8.\nFor any given PFN, a grammar for that pattern can be derived from one\nof its full alignments by simply listing the patterns from Old that appear in\nthat alignment, counting multiple appearances of any pattern as one. Any such\ngrammar may be augmented to cover an additional PFN by selecting one of the\nfull alignments for the second PFN and adding patterns from Old that appear\nwithin that alignment and are not already present in the grammar (counting\nmultiple appearances as one, as before). In this way, taking one PFN at a time,\na grammar may be compiled for all the patterns from New.\nA complication, of course, is that there are often two or more full alignments\nfor any given PFN. This means that, for a given set of patterns from New, one\ncan generate a tree of alternative grammars with branching occurring wherever\nthere are two or more alternative alignments for a given PFN. Without some\nconstraints, this tree can become unmanageably large.\nIn the compile alternative grammars() function, the tree of alternative gram-\nmars is pruned periodically to keep it within reasonable bounds. The tree is\ngrown in successive stages, at each stage processing the alignments for one of\nthe patterns from New and, for each grammar, processing only one alignment\nfor each PFN. Values for G, E and T are calculated for each grammar and, at\neach stage, grammars with high values for T are eliminated.\nFor a given grammar comprising patterns p1...pg, the value of G is calculated\nas:\nG =\ni=g\nX\ni=1\n(\nj=Li\nX\nj=1\nsj)\nwhere Li is the number of symbols in the ith pattern and sj is the encoding\ncost of the jth symbol in that pattern.\nGiven that each grammar is derived from a set a1...an of alignments (one\nalignment for each PFN), the value of E for the grammar is calculated as:\nE =\ni=n\nX\ni=1\nei\nwhere ei is the size, in bits, of the code string derived from the ith alignment\n(as described in Section 3.4.1), calculated as described in Section 4.3.1.\nWhen the set of alternative grammars has been completed, each grammar is\n‘cleaned up’ by removing code symbols that have no function in the grammar\nand by renaming code symbols in a tidy manner (see Section 6.1, below).\nBefore leaving this section, it is worth pointing out a minor anomaly in the\nway values for G and E are calculated. These values depend on the encoding\ncosts of symbols which themselves depend on frequencies of symbol types. At\n26\npresent, these frequencies are derived from the entire set of patterns in Old but\nit would probably be more appropriate if frequency values were derived from\neach grammar individually at each stage in the process of compiling grammars.\nThe results are likely to be similar in both cases since encoding costs depend\non relative frequency values, not absolute values and, for the symbol types\nappearing in any one grammar, it is likely that the ranking of frequency values\nderived from the grammar would be similar to the ranking derived from the\nentire set of patterns in Old.\n5\nEvaluation of the Model\nCriteria that may be used to evaluate a learning model like SP70 include:\n• If applications are scaled up to realistic size, is the model likely to make\nunreasonable demands for processing power or computer memory?\n• Given that the system aims to ﬁnd grammars with relatively small values\nfor T , does it succeed in this regard?\n• Does it produce knowledge structures that look ‘natural’ or ‘reasonable’?\nSince humans are the most ‘powerful’ learning systems on the planet, there\nis some justiﬁcation for using human intuitions about the structuring of\ninformation as a touchstone for success or failure of an artiﬁcial learning\nsystem.\nGiven evidence that human cognition is conditioned by MLE\nprinciples (Section 3.2.1), human intuitions provide an indirect check on\nwhether or not MLE principles have been successfully implemented in the\nmodel.\n• Does it produce knowledge structures that successfully support other op-\nerations such as reasoning, making ‘expert’ judgements, playing chess, and\nso on?\nThe ﬁrst of these criteria is discussed in the next subsection.\nEvidence\nbearing on the second criterion is presented in Section 6.1.2, below, and the\nintuitive plausibility of results obtained from the model is considered at various\npoints in Sections 6 and 7. No attempt has yet been made to evaluate SP70 in\nterms of the fourth criterion.\n5.1\nComputational Complexity\nIn common with other programs for unsupervised learning (and, indeed, other\nprograms for ﬁnding good multiple alignments), SP70 does not attempt to ﬁnd\ntheoretically ideal solutions. This is because the abstract space of possible gram-\nmars (and the abstract space of possible alignments) is, normally, too large to be\nsearched exhaustively. In general, heuristic techniques like hill climbing, genetic\nalgorithms, simulated annealing etc must be used. By using these techniques,\n27\none can normally convert an intractable computation into one with computa-\ntional complexity that is within acceptable limits.\nIn SP70, the critical operation is the formation of multiple alignments (cre-\nate multiple alignments()). Other operations (e.g., the derive patterns() func-\ntion) are, in comparison, quite trivial in their computational demands.\nIn a serial processing environment, the time complexity of the cre-\nate multiple alignments() function has been estimated [Wolﬀ, 1998] to be ap-\nproximately O(log2n × nm), where n is the size of the pattern from New (in\nbits) and m is the sum of the lengths of the patterns in Old (in bits). In a par-\nallel processing environment, the time complexity may approach O(log2n × n),\ndepending on how well the parallel processing is applied. In serial and parallel\nenvironments, the space complexity should be O(m).\nIn SP70, the function is applied (twice) to the set of patterns in New so we\nneed to take account of how many patterns there are in New. It seems reasonable\nto assume that the sizes of patterns in New are approximately constant.5\nOld is initially empty and grows as learning proceeds.\nThe size of Old\n(before purging) is, approximately, a linear function of the size of New. Given\nthis growth in the size of Old, the time required to create alignments for any\ngiven pattern from New will grow as learning proceeds. Again, the relationship\nis approximately linear.\nSo if we ignore operations other than the create multiple alignments() func-\ntion, we may estimate the time complexity of the program (in a serial environ-\nment) to be O(N 2) where N is the number of patterns in New. In a parallel\nprocessing environment, the time complexity may approach O(N), depending\non how well the parallel processing is applied. In serial or parallel environments,\nthe space complexity should be O(N).\nThe time complexity of the program may be improved when it has been\ndeveloped, as envisaged, so that the New patterns are processed in batches,\nwith a purging of Old between each batch to remove all patterns except those\nin the best grammar. In this case, the size of each batch of New patterns will be\napproximately constant and the running time of the program should be roughly\nproportional to the number of batches of New patterns.\n6\nExamples\nThis section presents two examples to illustrate how SP70 works and what it\ncan do. The examples are fairly simple, partly for the sake of clarity and partly\nbecause of shortcomings in the model (discussed in Section 7, below).\n5There is no requirement in the model that patterns in New should, for example, be\ncomplete sentences. They may equally well be arbitrary portions of incoming data, perhaps\nmeasured oﬀby some kind of input buﬀer.\n28\n6.1\nExample 1\nThe four short sentences supplied to SP70 as New for this ﬁrst example are\nshown in Figure 9.\nj o h n r u n s\nm a r y r u n s\nj o h n w a l k s\nm a r y w a l k s\nFigure 9: Four patterns supplied to SP70 as New.\nThe two best grammars found by the program for these sentences (with 10\nas the cost factor) are shown in Figure 10. In (a) the best grammar is shown in\nthe form that it is ﬁrst compiled and in (b) the same grammar is shown after it\nhas been cleaned up. Figure 10 (c) shows the second-best grammar after it has\nbeen cleaned up.\nCleaning up grammars means removing class symbols that have no referents\n(e.g., ‘%7’ and ‘%11’ in the second pattern in Figure 10 (a)) and renumbering\nthe class symbols and the discrimination symbols, starting from 1 for each set.\nThe renumbering is a purely cosmetic matter and makes no diﬀerence to the\nencoding cost calculated for each symbol.\nThe two grammars shown in Figure 10 are both reasonably plausible gram-\nmars for the four original sentences. In the best grammar ‘m a r y’ and ‘j o h\nn’ are picked out as discrete words and assigned to the same grammatical class\n(‘%3’). In a similar way, ‘r u n’ and ‘w a l k’ are each picked out as a discrete\nentity—corresponding to the ‘stem’ of a verb—and both are assigned to the\nclass ‘%1’. The suﬃx for these verb stems (‘s’) is picked out as a distinct entity\nand the overall sentence structure is captured in the pattern ‘< 6 < %3 > <\n%1 > < %2 > >’. The second best grammar is the same except that the suﬃx\nof each verb is not separated from the stem.\n6.1.1\nIntermediate Results\nThe simplicity of the results shown in Figure 10 disguises what the program\nhas done to achieve them. The ﬂavour of this processing may be seen from the\nselection of intermediate results presented here.\nWhen the ﬁrst pattern from New is processed, Old is empty except for a\ncopy of that ﬁrst pattern that is added to Old, one symbol at a time, as the\npattern is processed. The only alignment formed at this stage is shown in Figure\n11. Remember that the symbols in the PFN (‘j o h n r u n s’) must not be\naligned with the corresponding symbols in the pattern in Old because, in eﬀect,\nthis would mean matching each symbol with itself (Section 4.4).\nIt is evident that, at this stage, opportunities to gain any useful insights into\nthe overall structure of the patterns in New are quite limited. From the ‘bad’\nalignment shown in Figure 11 the program abstracts the ‘bad’ patterns ‘< %2\n29\n< %4 15 s >\n< %7 %9 %11 152 m a r y >\n< %9 %14 162 j o h n >\n< %24 406 r u n >\n< %24 %27 407 w a l k >\n< %25 412 < %9 > < %24 > < %4 > >\n(a)\n< %2 1 s >\n< %3 2 m a r y >\n< %3 3 j o h n >\n< %1 4 r u n >\n< %1 5 w a l k >\n< 6 < %3 > < %1 > < %2 > >\n(b)\n< %2 2 m a r y >\n< %2 3 j o h n >\n< %1 1 r u n s >\n< %1 5 w a l k s >\n< 4 < %2 > < %1 > >\n(c)\nFigure 10: Grammars found by SP70 with the four patterns shown in Figure\n9 supplied as New. (a) The best grammar without cleaning up. (b) The same\ngrammar after cleaning up. (c) The second best grammar after cleaning up.\n0 j o h n r u\nn s\n0\n|\n1 < %1 5 j o h n r u n s > 1\nFigure 11: The only alignment formed when the ﬁrst pattern from New is\nprocessed.\n7 n >’, ‘< %3 10 j o h >’, ‘< %3 11 j o h n r u >’, ‘< %4 14 r u n s >’, ‘< %4\n15 s >’ and ‘< %5 19 < %3 > < %2 > < %4 > >’.6\nWhen the next PFN (‘m a r y r u n s’) is processed, the program is able to\n6As applied to alignments and patterns, the words ‘bad’ and ‘good’ are shorthand for\n“bad/good in terms of MLE principles and perhaps also in terms of one’s intuitions about\nwhat is or is not an appropriate grammar for the data”. Quote marks will be dropped in the\nremainder of the paper.\n30\nform more sensible alignments like\n0 m a r y r u n s\n0\n| | | |\n1 < %4 14 r u n s > 1\nand\n0 m a r y\nr u n s\n0\n| | | |\n1 < %1 5 j o h n r u n s > 1.\nThe ﬁrst of these alignments yields the patterns ‘< %7 152 m a r y >’ and\n‘< %8 157 < %7 > < %4 > >’. It would have created a pattern for ‘r u n s’\nbut this is suppressed because the program detects that a pattern with those C\nsymbols already exists (‘< %4 14 r u n s >’).\nFrom the second alignment, the system derives the pattern ‘< %9 162 j o h\nn >’ (assigned to the class ‘%9’) and it would create a pattern for ‘m a r y’ but\nit detects that a pattern with these C symbols already exists (‘< %7 152 m a\nr y >’). However, since ‘j o h n’ and ‘m a r y’ occur in the same context (‘—r\nu n s’), they should be assigned to the same context-deﬁned class. Accordingly,\nthe program adds the class symbol ‘%9’ to the pattern ‘< %7 152 m a r y >’ so\nthat it becomes ‘< %7 %9 152 m a r y >’ and it creates the abstract pattern\n‘< %10 168 < %9 > < %4 > >’, tying the whole structure together.\nAs processing proceeds in the pattern-generation phase (operation 3 in Fig-\nure 2), the program forms good alignments like\n0\nj o h n w a l k s\n0\n| | | |\n|\n1 < %1 5 j o h n r u n\ns > 1\nand relatively bad ones like\n0\nj o h\nn w a l k s\n0\n| | |\n|\n|\n1 < %1 5 j o h n r u n\ns > 1.\nFrom alignments like these it derives correspondingly good and bad patterns.\nBy the time the last PFN is processed (‘m a r y w a l k s’) there are enough\ngood patterns in Old for the program to start forming quite plausible alignments\nlike\n0\nm a r y\nw a l k s\n0\n| | | |\n| | | | |\n1\n| | | |\n< %22 %29 %4 %31 394 w a l k s >\n1\n| | | |\n|\n|\n|\n2 < %8 157 < %7\n| | | | > <\n%4\n> > 2\n| |\n| | | | |\n3\n< %7 %9 %11 152 m a r y >\n3.\n31\nAt the end of this phase of processing, Old contains a variety of patterns\nincluding several good ones and quite a lot of bad ones.\nIn the second sifting and sorting() phase of the program (operation 4 in\nFigure 2) the program compiles a set of alternative grammars the best of which\nare shown in Figure 10.\n6.1.2\nPlotting Values for G, E and T\nThe value of T for the best grammar is 1379 bits before cleaning up and 1348\nbits after cleaning up. For the second-best grammar, the corresponding values\nare 1418 and 1377. By contrast, T is calculated to be 2245 bits for the ‘naive’\ngrammar that comprises the four patterns from New with added ID symbols\n(‘< %1 5 j o h n r u n s >’, ‘< %6 21 m a r y r u n s >’, ‘< %21 212 j o h n w\na l k s >’ and ‘< %33 457 m a r y w a l k s >’).\nFigure 12 shows how the values of G, E and T change as successive pat-\nterns from New are processed when the set of alternative grammars is compiled.\nEach point on each of the lower three graphs represents the relevant value from\nthe best grammar found after the full alignments for a given PFN have been\nprocessed. The top graph shows successive values of T for the ‘naive’ grammar\nmentioned above.\nNotice how the value of G does not change between the third PFN and the\nfourth.\nThis is because there is nothing in the fourth pattern that has not\nalready been found in the ﬁrst three patterns.\n6.2\nExample 2\nWhen New contains the eight sentences shown in Figure 13, the best grammar\nfound by SP70 (after cleaning up) is the one shown in Figure 14.\nThis result looks reasonable but, in the light of the best grammar found in\nExample 1, one may wonder why the terminal ‘s’ of ‘r u n s’ and ‘w a l k s’ has\nnot been identiﬁed as a discrete entity, separate from the verb stems ‘r u n’ and\n‘w a l k’.\nIn the pattern-generation phase of processing, SP70 does form alignments\nlike this\n0\nt h a t b o y w a l k s\n0\n| | | | | | |\n|\n1 < %1 9 t h a t b o y r u n\ns > 1\nwhich clearly recognises the verb stems and ‘s’ as distinct entities.\nBut for\nreasons that are still not entirely clear, the program does not build these entities\ninto plausible versions of the full sentence structure. The program isolates the\npattern ‘< %25 599 t h a t b o y >’ from the alignment just shown but for\nsome reason it fails to ﬁnd internal structure within that pattern—although it\nrecognises ‘t h a t’ and ‘b o y’ in other contexts. This issue is discussed in\nSection 7.1, below.\n32\nFigure 12:\nValues of G,\nE and T\nfor the best grammar found (be-\nfore cleaning up) as successive patterns from New are processed in com-\npile alternative grammars().\nValues for T for the ‘naive’ grammar are also\nshown (marked with ‘*’).\nt h a t b o y r u n s\nt h a t g i r l r u n s\nt h a t b o y w a l k s\nt h a t g i r l w a l k s\ns o m e b o y r u n s\ns o m e g i r l r u n s\ns o m e b o y w a l k s\ns o m e g i r l w a l k s\nFigure 13: Eight sentences supplied to SP70 as New.\n7\nDiscussion\nThis section discusses a selection of issues relating to the SP70 model, especially\nshortcomings of the current version and how they may be overcome.\n33\n< %2 2 s o m e >\n< %2 3 t h a t >\n< %1 5 b o y >\n< %1 6 g i r l >\n< %3 4 r u n s >\n< %3 7 w a l k s >\n< 1 < %2 > < %1 > < %3 > >\nFigure 14: The best grammar found by SP70 (after cleaning up) when New\ncontains the eight sentences shown in Figure 13.\n7.1\nFinding Internal Structure\nThe failure of the model to ﬁnd some of the internal structure within a sentence\nin Example 2 seems to be a manifestation of a more general shortcoming. Al-\nthough the model in its current form can isolate basic segments and tie them\ntogether in an overall abstract structure, it is not good at ﬁnding intermediate\nlevels of abstraction.\nWhat seems to be needed is some kind of additional reprocessing of the pat-\nterns in Old, including the abstract patterns that have been added to Old, to\ndiscover partial matches not detected in the initial processing of the patterns\nfrom New. This should allow the system to detect intermediate levels of struc-\nture such as phrases or clauses or structures that may exist within smaller units\nsuch as words.\n7.2\nFinding Discontinuous Dependencies\nIn the development of the model to date, no attempt has been made to enable\nthe system to detect discontinuous dependencies such as number dependency be-\ntween the subject of a sentence and its main verb (as mentioned in Section 3.4.1\nand illustrated in Figure 1) or gender dependencies in languages like French.\nAlthough this kind of capability may seem like a reﬁnement that we can\naﬀord to do without at this stage of development, a deﬁciency in this area\nseems to have an impact on the program’s performance at an elementary level.\nEven in quite simple structures, dependencies can exist that bridge intervening\nstructure and, in its current form, the program does not encode this kind of\ninformation in a satisfactory manner.\nThere do not seem to be any insuperable obstacles to solving this problem\nwithin the ICMAUS framework:\n• The format for representing knowledge accommodates these kinds of struc-\nture quite naturally.\n• Finding partial matches that bridge intervening structure is bread-and-\nbutter for the create multiple alignments() function.\n34\nWhat seems to be required is some revision of the way in which patterns are\nderived from alignments.\n7.3\nGeneralization of Grammatical Rules and the Correc-\ntion of Overgeneralizations\nA well-documented phenomenon in the way young children learn language is\nthat they say things like “The ball was hitted” or “Look at the gooses”, ap-\nparently applying general rules for constructing words but applying them too\ngenerally. How is it that children eventually learn to avoid these kinds of over-\ngeneralizations? It is tempting to suppose that children are corrected by parents\nor other adults but the weight of empirical evidence is that, while such correc-\ntions may be helpful, they are not actually necessary for language learning.\nMLE principles provide an elegant solution to this puzzle.\nWithout the\nkind of feedback or supplementary information postulated by Gold [1967], it\nis possible to search for grammars that are good in MLE terms and these are\nnormally ones that steer a path between generalizations that are, intuitively,\n‘correct’ and others that appear to be ‘wrong’. This kind of eﬀect has been\ndemonstrated with the SNPR model [Wolﬀ, 1982].\nWhen SP70 is run on the ﬁrst three of the four patterns shown in Figure\n9, the best grammar found is exactly the same as before (Figure 10 (b)). This\ngrammar generates the missing sentence (‘m a r y w a l k s’) as well as the other\nthree sentences, but it does not generate anything else.\nIn this example, the model generalizes in a way that seems intuitively to\nbe correct and avoids creating overgeneralizations that are glaringly wrong.\nHowever, relatively little attention has so far been given to this aspect of the\nmodel and further work is required. In particular, a better understanding is\nneeded of alternative ways in which grammatical rules may be generalized.\n7.4\nOther Developments\nOther areas where further work is planned include:\n• As was indicated in Section 4.6, it is anticipated that the program will be\ndeveloped so that it processes patterns from New in batches, purging bad\npatterns from Old at the end of each batch.\n• As was noted in Section 4.5.2, it is possible for two or more context-deﬁned\nclasses to be the same. The program needs to check for this possibility\nand merge identical classes whenever they are found.\n• At present, the program applies the create multiple alignments() func-\ntion twice to each PFN, once as part of the process of generating pat-\nterns to be added to Old and once in the sifting and sorting() phase. It\nseems possible that the two phases could be integrated so that the cre-\nate multiple alignments() function need only be applied once to each PFN.\n35\n• As was suggested in Section 4.5, there may be a case for introducing a\n‘null’ pattern to allow for the encoding of optional elements in a syntactic\nstructure.\n• Although the computational complexity of the model on a serial machine\nis within acceptable limits, improvements in that area, and higher absolute\nspeeds, may be obtained by the application of parallel processing. If or\nwhen residual problems in the model have been solved, it is envisaged that\nthe system will be developed as a software virtual machine on existing\nhigh-parallel hardware or perhaps on new forms of hardware dedicated to\nthe needs of the model. It may be possible to exploit optical techniques\nto achieve high-parallel matching of patterns in the core of the model.\n7.5\nMotivation and emotion\nAlthough learning has been considered in this article primarily as an engineering\nproblem, the theory that has been described may be viewed as a possible theory\nof learning in people and other animals.\nWe tend to remember things best that are signiﬁcant for us in terms of our\nmotivations and emotions. How would this ﬁt in with the kinds of learning\nmechanisms that have been described?\nThe tentative suggestion here is that motivations and emotions may have an\nimpact when patterns are purged from the system. Other things being equal, we\nmay suppose that MLE principles govern the choice of which patterns should\nbe retained and which should be discarded. But any pattern that represents\nsomething that has special signiﬁcance may be retained by the system even if\nit does not score well in terms of MLE measures.\n8\nConclusion\nAlthough SP70 is still some way short of an ‘industrial strength’ system for\nunsupervised learning, the results obtained so far are good enough to show that\nthe general approach is sound. Problems that have been identiﬁed appear to be\nsoluble.\nA particular attraction of this approach to learning is that the ICMAUS\nframework provides a uniﬁed view of a variety of issues in AI thus facilitating\nthe integration of learning with other aspects of intelligence.\nAcknowledgements\nI am very grateful for constructive comments that I have received from Pat\nLangley.\n36\nReferences\nP. Adriaans, M. Trautwein, and M. Vervoort. Towards high speed grammar\ninduction on large text corpora. In V. Hlav´a¨c, K. G. Jeﬀery, and J. Wieder-\nmann, editors, SOFSEM 2000, volume 1963 of Lecture Notes in Computer\nScience, pages 173–186. Springer, 2000.\nL. Allison, C. S. Wallace, and C. N. Yee.\nMinimum message length encod-\ning, evolutionary trees and multiple-alignment. In Proceedings of the Hawaii\nInternational Conference on Systems Science, HICCS-25, January 1992.\nN. Chater. Reconciling simplicity and likelihood principles in perceptual organ-\nisation. Psychological Review, 103(3):566–581, 1996.\nN. Chater. The search for simplicity: a fundamental cognitive principle? Quar-\nterly Journal of Experimental Psychology, 52 A(2):273–302, 1999.\nN. Chomsky. Syntactic Structures. Mouton, The Hague, 1957.\nA. Clark. Unsupervised induction of stochastic context-free grammars using\ndistributional clustering. In Daelemans and Zajac [2001], pages 105–112.\nT. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley,\nNew York, 1991.\nW. Daelemans and R. Zajac, editors. Proceedings of CoNLL-2001 (at ACL-2001,\nToulouse, France), 2001.\nF. Denis. Learning regular languages from simple positive examples. Machine\nLearning, 44(1/2):37–66, 2001.\nJ. Eisner. Discovering syntactic deep structure via Bayesian statistics. Cognitive\nScience, 26(3):255–268, 2002.\nC. C. Fries. The Structure of English. Harcourt, Brace & World, New York,\n1952.\nM. Gold. Language identiﬁcation in the limit. Information and Control, 10:\n447–474, 1967.\nZ. S. Harris. Methods in Structural Linguistics. University of Chicago Press,\nChicago, 1951.\nP. J. Henrichsen. Grasp: Grammar learning from unlabelled speech corpora.\nIn D. Roth and A. van den Bosch, editors, Proceedings of CoNLL-2002 (at\nCOLING-2002), pages 22–28, 2002.\nM. Johnson and S. Riezler. Statistical models of syntax learning and use. Cog-\nnitive Science, 26:239–253, 2002.\nD. Klein and C. Manning. Distributional phrase structure induction. In Daele-\nmans and Zajac [2001].\n37\nP. Langley and S. Stromsten. Learning context-free grammars with a simplic-\nity bias. In Proceedings of the Eleventh European Conference on Machine\nLearning, pages 220–228, 2000. Barcelona.\nM. Li and P. Vit´anyi. An Introduction to Kolmogorov Complexity and Its Ap-\nplications. Springer-Verlag, New York, 1997.\nC. G. Nevill-Manning and I. H. Witten. Compression and explanation using\nhierarchical grammars. Computer Journal, 40(2/3):103–116, 1997.\nA. L. Oliveira and A. Sangiovanni-Vincentelli. Using the minimum description\nlength principle to infer reduced ordered decision graphs. Machine Learning,\n25(1):23–50, 1996.\nP. E. Rapp, I. D. Zimmerman, E. P. Vining, N. Cohen, A. M. Albano, and M. A.\nJimenez-Montano. The algorithmic complexity of neural spike trains increases\nduring focal seizures. Journal of Neuroscience, 14(8):4731–4739, 1994.\nJ. Rissanen. Modelling by the shortest data description. Automatica-J, IFAC,\n14:465–471, 1978.\nD. Sankoﬀand J. B. Kruskall. Time Warps, String Edits, and Macromolecules:\nthe Theory and Practice of Sequence Comparisons. Addison-Wesley, Reading,\nMA, 1983.\nR. J. Solomonoﬀ. A formal theory of inductive inference. parts I and II. Infor-\nmation and Control, 7:1–22 and 224–254, 1964.\nR. J. Solomonoﬀ. The discovery of algorithmic probability. Journal of Computer\nand System Sciences, 55(1):73–88, 1997.\nS.Watkinson and S. Manandhar. A psychologically plausible and computation-\nally eﬀective approach to learning syntax. In Daelemans and Zajac [2001].\nL. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):\n1134–1142, 1984.\nC. S. Wallace and D. M. Boulton. An information measure for classiﬁcation.\nComputer Journal, 11(2):185–195, 1968.\nJ. G. Wolﬀ. An algorithm for the segmentation of an artiﬁcial language analogue.\nBritish Journal of Psychology, 66:79–90, 1975.\nJ. G. Wolﬀ. The discovery of segments in natural language. British Journal of\nPsychology, 68:97–106, 1977.\nJ. G. Wolﬀ. Language acquisition and the discovery of phrase structure. Lan-\nguage & Speech, 23:255–269, 1980.\nJ. G. Wolﬀ. Language acquisition, data compression and generalization. Lan-\nguage & Communication, 2:57–89, 1982.\n38\nJ. G. Wolﬀ. Learning syntax and meanings through optimization and distribu-\ntional analysis. In Y. Levy, I. M. Schlesinger, and M. D. S. Braine, editors,\nCategories and Processes in Language Acquisition, pages 179–215. Lawrence\nErlbaum, Hillsdale, NJ, 1988.\nJ. G. Wolﬀ. Computing, cognition and information compression. AI Commu-\nnications, 6(2):107–127, 1993.\nJ. G. Wolﬀ. A scaleable technique for best-match retrieval of sequential infor-\nmation using metrics-guided search. Journal of Information Science, 20(1):\n16–28, 1994.\nJ.\nG.\nWolﬀ.\nProbabilistic\nreasoning\nas\ninformation\ncompression\nby\nmultiple\nalignment,\nuniﬁcation\nand\nsearch.\nTechnical\nreport,\nSchool of Informatics,\nUniversity of Wales at Bangor,\n1998.\nCopy:\nwww.cognitionresearch.org.uk/papers/pr/pr.htm.\nJ. G. Wolﬀ. ‘Computing’ as information compression by multiple alignment,\nuniﬁcation and search. Journal of Universal Computer Science, 5(11):777–\n815, 1999a.\nJ. G. Wolﬀ.\nProbabilistic reasoning as information compression by multiple\nalignment, uniﬁcation and search: an introduction and overview. Journal of\nUniversal Computer Science, 5(7):418–462, 1999b.\nJ. G. Wolﬀ. Syntax, parsing and production of natural language in a framework\nof information compression by multiple alignment, uniﬁcation and search.\nJournal of Universal Computer Science, 6(8):781–829, 2000.\nJ. G. Wolﬀ. Information compression by multiple alignment, uniﬁcation and\nsearch as a framework for human-like reasoning. Logic Journal of the IGPL, 9\n(1):205–222, 2001. First published in the Proceedings of the International Con-\nference on Formal and Applied Practical Reasoning (FAPR 2000), September\n2000, ISSN 1469–4166.\nJ.\nG.\nWolﬀ.\nMathematics\nand\nlogic\nas\ninformation\ncompres-\nsion\nby\nmultiple\nalignment,\nuniﬁcation\nand\nsearch.\nTech-\nnical\nreport,\nCognitionResearch.org.uk,\n2002a.\nCopy:\nwww.cognitionresearch.org.uk/papers/cml/cml.htm.\nJ. G. Wolﬀ. Neural mechanisms for information compression by multiple align-\nment, uniﬁcation and search.\nTechnical report, CognitionResearch.org.uk,\n2002b. Available from the author on request.\nJ. G. Wolﬀ. Information compression by multiple alignment, uniﬁcation and\nsearch as a unifying principle in computing and cognition. Artiﬁcial Intelli-\ngence Review, to appear.\n39\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "I.2.4; I.2.6; I.2.7"
  ],
  "published": "2003-02-12",
  "updated": "2003-02-12"
}