{
  "id": "http://arxiv.org/abs/cs/0304027v1",
  "title": "\"I'm sorry Dave, I'm afraid I can't do that\": Linguistics, Statistics, and Natural Language Processing circa 2001",
  "authors": [
    "Lillian Lee"
  ],
  "abstract": "A brief, general-audience overview of the history of natural language\nprocessing, focusing on data-driven approaches.Topics include \"Ambiguity and\nlanguage analysis\", \"Firth things first\", \"A 'C' change\", and \"The empiricists\nstrike back\".",
  "text": "arXiv:cs/0304027v1  [cs.CL]  21 Apr 2003\n“I’m sorry Dave, I’m afraid I can’t do that”: Linguistics, Statistics,\nand Natural Language Processing circa 2001∗\nLillian Lee, Cornell University\nIt’s the year 2000, but where are the ﬂying cars? I was promised ﬂying cars.\n– Avery Brooks, IBM commercial\nAccording to many pop-culture visions of the future, technology will eventually produce the Machine that\nCan Speak to Us. Examples range from the False Maria in Fritz Lang’s 1926 ﬁlm Metropolis to Knight\nRider’s KITT (a talking car) to Star Wars’ C-3PO (said to have been modeled on the False Maria). And,\nof course, there is the HAL 9000 computer from 2001: A Space Odyssey; in one of the ﬁlm’s most famous\nscenes, the astronaut Dave asks HAL to open a pod bay door on the spacecraft, to which HAL responds,\n“I’m sorry Dave, I’m afraid I can’t do that”.\nNatural language processing, or NLP, is the ﬁeld of computer science devoted to creating such machines\n— that is, enabling computers to use human languages both as input and as output. The area is quite\nbroad, encompassing problems ranging from simultaneous multi-language translation to advanced search\nengine development to the design of computer interfaces capable of combining speech, diagrams, and other\nmodalities simultaneously. A natural consequence of this wide range of inquiry is the integration of ideas\nfrom computer science with work from many other ﬁelds, including linguistics, which provides models of\nlanguage; psychology, which provides models of cognitive processes; information theory, which provides\nmodels of communication; and mathematics and statistics, which provide tools for analyzing and acquiring\nsuch models.\nThe interaction of these ideas together with advances in machine learning (see [other chapter]) has re-\nsulted in concerted research activity in statistical natural language processing: making computers language-\nenabled by having them acquire linguistic information directly from samples of language itself. In this essay,\nwe describe the history of statistical NLP; the twists and turns of the story serve to highlight the sometimes\ncomplex interplay between computer science and other ﬁelds.\nAlthough currently a major focus of research, the data-driven, computational approach to language\nprocessing was for some time held in deep disregard because it directly conﬂicts with another commonly-\nheld viewpoint: human language is so complex that language samples alone seemingly cannot yield enough\ninformation to understand it. Indeed, it is often said that NLP is “AI-complete” (a pun on NP-completeness;\nsee [other chapter]), meaning that the most difﬁcult problems in artiﬁcial intelligence manifest themselves\nin human language phenomena. This belief in language use as the touchstone of intelligent behavior dates\nback at least to the 1950 proposal of the Turing Test1 as a way to gauge whether machine intelligence has\nbeen achieved; as Turing wrote, “The question and answer method seems to be suitable for introducing\nalmost any one of the ﬁelds of human endeavour that we wish to include”.\n∗To appear in the National Research Council study on the Fundamentals of Computer Science. This is an April 2003 version.\n1Roughly speaking, a computer will have passed the Turing Test if it can engage in conversations indistinguishable from that of\na human’s.\n1\nThe reader might be somewhat surprised to hear that language understanding is so hard. After all, human\nchildren get the hang of it in a few years, word processing software now corrects (some of) our grammatical\nerrors, and TV ads show us phones capable of effortless translation. One might therefore be led to believe\nthat HAL is just around the corner.\nSuch is not the case, however. In order to appreciate this point, we temporarily divert from describing\nstatistical NLP’s history — which touches upon Hamilton versus Madison, the sleeping habits of colorless\ngreen ideas, and what happens when one ﬁres a linguist — to examine a few examples illustrating why\nunderstanding human language is such a difﬁcult problem.\nAmbiguity and language analysis\nAt last, a computer that understands you like your mother.\n– 1985 McDonnell-Douglas ad\nThe snippet quoted above indicates the early conﬁdence at least one company had in the feasibility of\ngetting computers to understand human language. But in fact, that very sentence is illustrative of the host\nof difﬁculties that arise in trying to analyze human utterances, and so, ironically, it is quite unlikely that the\nsystem being promoted would have been up to the task. A moment’s reﬂection reveals that the sentence\nadmits at least three different interpretations:\n1. The computer understands you as well as your mother understands you.\n2. The computer understands that you like your mother.\n3. The computer understands you as well as it understands your mother.\nThat is, the sentence is ambiguous; and yet we humans seem to instantaneously rule out all the alternatives\nexcept the ﬁrst (and presumably the intended) one. We do so based on a great deal of background knowl-\nedge, including understanding what advertisements typically try to convince us of. How are we to get such\ninformation into a computer?\nA number of other types of ambiguity are also lurking here. For example, consider the speech recog-\nnition problem: how can we distinguish between this utterance, when spoken, and “... a computer that\nunderstands your lie cured mother”? We also have a word sense ambiguity problem: how do we know that\nhere “mother” means “a female parent”, rather than the Oxford English Dictionary-approved alternative of\n“a cask or vat used in vinegar-making”? Again, it is our broad knowledge about the world and the context\nof the remark that allows us humans to make these decisions easily.\nNow, one might be tempted to think that all these ambiguities arise because our example sentence is\nhighly unusual (although the ad writers probably did not set out to craft a strange sentence). Or, one might\nargue that these ambiguities are somehow artiﬁcial because the alternative interpretations are so unrealistic\nthat an NLP system could easily ﬁlter them out. But ambiguities crop up in many situations. For example,\nin “Copy the local patient ﬁles to disk” (which seems like a perfectly plausible command to issue to a\ncomputer), is it the patients or the ﬁles that are local?2 Again, we need to know the speciﬁcs of the situation\nin order to decide. And in multilingual settings, extra ambiguities may arise. Here is a sequence of seven\nJapanese characters:\n2Or, perhaps, the ﬁles themselves are patient? But our knowledge about the world rules this possibility out.\n2\nSince Japanese doesn’t have spaces between words, one is faced with the initial task of deciding what\nthe component words are. In particular, this character sequence corresponds to at least two possible word\nsequences, “president, both, business, general-manager” (= “a president as well as a general manager of\nbusiness”) and “president, subsidiary-business, Tsutomu (a name), general-manager” (= ?). It requires a fair\nbit of linguistic information to choose the correct alternative.3\nTo sum up, we see that the NLP task is highly daunting, for to resolve the many ambiguities that arise in\ntrying to analyze even a single sentence requires deep knowledge not just about language but also about the\nworld. And so when HAL says, “I’m afraid I can’t do that”, NLP researchers are tempted to respond, “I’m\nafraid you might be right”.\nFirth things ﬁrst\nBut before we assume that the only viable approach to NLP is a massive knowledge engineering project, let\nus go back to the early approaches to the problem. In the 1940s and 1950s, one prominent trend in linguistics\nwas explicitly empirical and in particular distributional, as exempliﬁed by the work of Zellig Harris (who\nstarted the ﬁrst linguistics program in the USA). The idea was that correlations (co-occurrences) found in\nlanguage data are important sources of information, or, as the inﬂuential linguist J. R. Firth declared in 1957,\n“You shall know a word by the company it keeps”.\nSuch notions accord quite happily with ideas put forth by Claude Shannon in his landmark 1948 paper\nestablishing the ﬁeld of information theory; speaking from an engineering perspective, he identiﬁed the\nprobability of a message’s being chosen from among several alternatives, rather than the message’s actual\ncontent, as its critical characteristic. Inﬂuenced by this work, Warren Weaver in 1949 proposed treating the\nproblem of translating between languages as an application of cryptography (see [other chapter]), with one\nlanguage viewed as an encrypted form of another. And, Alan Turing’s work on cracking German codes\nduring World War II led to the development of the Good-Turing formula, an important tool for computing\ncertain statistical properties of language.\nIn yet a third area, 1941 saw the statisticians Frederick Mosteller and Frederick Williams address the\nquestion of whether it was Alexander Hamilton or James Madison who wrote some of the pseudonymous\nFederalist Papers. Unlike previous attempts, which were based on historical data and arguments, Mosteller\nand Williams used the patterns of word occurrences in the texts as evidence. This work led up to the famed\nMosteller and Wallace statistical study which many consider to have settled the authorship of the disputed\npapers.\nThus, we see arising independently from a variety of ﬁelds the idea that language can be viewed from\na data-driven, empirical perspective — and a data-driven perspective leads naturally to a computational\nperspective.\nA “C” change\nHowever, data-driven approaches fell out of favor in the late 1950’s. One of the commonly cited factors is\na 1957 argument by linguist (and student of Harris) Noam Chomsky, who believed that language behavior\nshould be analyzed at a much deeper level than its surface statistics. He claimed,\n3To take an analogous example in English, consider the non-word-delimited sequence of letters “theyouthevent”. This corre-\nsponds to the word sequences “the youth event”, “they out he vent”, and “the you the vent”.\n3\nIt is fair to assume that neither sentence (1) [Colorless green ideas sleep furiously] nor (2)\n[Furiously sleep ideas green colorless] ... has ever occurred .... Hence, in any [computed]\nstatistical model ... these sentences will be ruled out on identical grounds as equally “remote”\nfrom English.4 Yet (1), though nonsensical, is grammatical, while (2) is not.\nThat is, we humans know that sentence (1), which at least obeys (some) rules of grammar, is indeed more\nprobable than (2), which is just word salad; but (the claim goes), since both sentences are so rare, they\nwill have identical statistics — i.e., a frequency of zero — in any sample of English. Chomsky’s criticism\nis essentially that data-driven approaches will always suffer from a lack of data, and hence are doomed to\nfailure.\nThis observation turned out to be remarkably prescient: even now, when billions of words of text are\navailable on-line, perfectly reasonable phrases are not present. Thus, the so-called sparse data problem\ncontinues to be a serious challenge for statistical NLP even today. And so, the effect of Chomsky’s claim,\ntogether with some negative results for machine learning and a general lack of computing power at the time,\nwas to cause researchers to turn away from empirical approaches and toward knowledge-based approaches\nwhere human experts encoded relevant information in computer-usable form.\nThis change in perspective led to several new lines of fundamental, interdisciplinary research. For ex-\nample, Chomsky’s work viewing language as a formal, mathematically-describable object has had lasting\nimpact on both linguistics and computer science; indeed, the Chomsky hierarchy, a sequence of increasingly\nmore powerful classes of grammars, is a staple of the undergraduate computer science curriculum. Con-\nversely, the highly inﬂuential work of, among others, Kazimierz Adjukiewicz, Joachim Lambek, David K.\nLewis, and Richard Montague adopted the lambda calculus, a fundamental concept in the study of program-\nming languages, to model the semantics of natural languages.\nThe empiricists strike back\nBy the ’80s, the tide had begun to shift once again, in part because of the work done by the speech recog-\nnition group at IBM. These researchers, inﬂuenced by ideas from information theory, explored the power\nof probabilistic models of language combined with access to much more sophisticated algorithmic and data\nresources than had previously been available. In the realm of speech recognition, their ideas form the core\nof the design of modern systems; and given the recent successes of such software — large-vocabulary\ncontinuous-speech recognition programs are now available on the market — it behooves us to examine how\nthese systems work.\nGiven some acoustic signal, which we denote by the variable a, we can think of the speech recognition\nproblem as that of transcription: determining what sentence is most likely to have produced a. Probabilities\narise because of the ever-present problem of ambiguity: as mentioned above, several word sequences, such\nas “your lie cured mother” versus “you like your mother”, can give rise to similar spoken output. Therefore,\nmodern speech recognition systems incorporate information both about the acoustic signal and the language\nbehind the signal. More speciﬁcally, they rephrase the problem as determining which sentence s maximizes\nthe product P(a|s)×P(s). The ﬁrst term measures how likely the acoustic signal would be if s were actually\nthe sentence being uttered (again, we use probabilities because humans don’t pronounce words the same way\nall the time). The second term measures the probability of the sentence s itself; for example, as Chomsky\nnoted, “colorless green ideas sleep furiously” is intuitively more likely to be uttered than the reversal of the\n4Interestingly, this claim has become so famous as to be self-negating, as simple web searches on “Colorless green ideas sleep\nfuriously” and its reversal will show.\n4\nphrase. It is in computing this second term, P(s), where statistical NLP techniques come into play, since\naccurate estimation of these sentence probabilities requires developing probabilistic models of language.\nThese models are acquired by processing tens of millions of words or more. This is by no means a simple\nprocedure; even linguistically naive models require the use of sophisticated computational and statistical\ntechniques because of the sparse data problem foreseen by Chomsky. But using probabilistic models, large\ndatasets, and powerful learning algorithms (both for P(s) and P(a|s)) has led to our achieving the milestone\nof commercial-grade speech recognition products capable of handling continuous speech ranging over a\nlarge vocabulary.\nBut let us return to our story. Buoyed by the successes in speech recognition in the ’70s and ’80s\n(substantial performance gains over knowledge-based systems were posted), researchers began applying\ndata-driven approaches to many problems in natural language processing, in a turn-around so extreme that it\nhas been deemed a “revolution”. Indeed, now empirical methods are used at all levels of language analysis.\nThis is not just due to increased resources: a succession of breakthroughs in machine learning algorithms\nhas allowed us to leverage existing resources much more effectively. At the same time, evidence from psy-\nchology shows that human learning may be more statistically-based than previously thought; for instance,\nwork by Jenny Saffran, Richard Aslin, and Elissa Newport reveals that 8-month-old infants can learn to di-\nvide continuous speech into word segments based simply on the statistics of sounds following one another.\nHence, it seems that the “revolution” is here to stay.\nOf course, we must not go overboard and mistakenly conclude that the successes of statistical NLP\nrender linguistics irrelevant (rash statements to this effect have been made in the past, e.g., the notorious\nremark, “Every time I ﬁre a linguist, my performance goes up”). The information and insight that linguists,\npsychologists, and others have gathered about language is invaluable in creating high-performance broad-\ndomain language understanding systems; for instance, in the speech recognition setting described above, a\nbetter understanding of language structure can lead to better language models. Moreover, truly interdisci-\nplinary research has furthered our understanding of the human language faculty. One important example of\nthis is the development of the head-driven phrase structure grammar (HPSG) formalism — this is a way of\nanalyzing natural language utterances that truly marries deep linguistic information with computer science\nmechanisms, such as uniﬁcation and recursive data-types, for representing and propagating this information\nthroughout the utterance’s structure. In sum, computational techniques and data-driven methods are now an\nintegral part both of building systems capable of handling language in a domain-independent, ﬂexible, and\ngraceful way, and of improving our understanding of language itself.\nAcknowledgments\nThanks to the members of the CSTB Fundamentals of Computer Science study — and especially Alan\nBiermann — for their helpful feedback. Also, thanks to Alex Acero, Takako Aikawa, Mike Bailey, Regina\nBarzilay, Eric Brill, Chris Brockett, Claire Cardie, Joshua Goodman, Ed Hovy, Rebecca Hwa, John Lafferty,\nBob Moore, Greg Morrisett, Fernando Pereira, Hisami Suzuki, and many others for stimulating discussions\nand very useful comments. Rie Kubota Ando provided the Japanese example. The use of the term “rev-\nolution” to describe the re-ascendance of statistical methods comes from Julia Hirschberg’s 1998 invited\naddress to the American Association for Artiﬁcial Intelligence. I learned of the McDonnell-Douglas ad and\nsome of its analyses from a class run by Stuart Shieber. All errors are mine alone. This paper is based upon\nwork supported in part by the National Science Foundation under ITR/IM grant IIS-0081334 and a Sloan\nResearch Fellowship. Any opinions, ﬁndings, and conclusions or recommendations expressed above are\nthose of the authors and do not necessarily reﬂect the views of the National Science Foundation or the Sloan\nFoundation.\n5\nReferences\nAdjukiewicz, Kazimierz. 1935. Die syntaktische Konnexit¨at. Studia Philosophica, 1:1–27. English trans-\nlation available in Storrs McCall, editor, Polish Logic 1920-1939, Clarendon Press (1967).\nChomsky, Noam. 1957. Syntactic Structures. Number IV in Janua Linguarum. Mouton, The Hague, The\nNetherlands.\nFirth, John Rupert. 1957. A synopsis of linguistic theory 1930–1955. In the Philological Society’s Studies\nin Linguistic Analysis. Blackwell, Oxford, pages 1–32. Reprinted in Selected Papers of J. R. Firth,\nedited by F. Palmer. Longman, 1968.\nGood, Irving J. 1953. The population frequencies of species and the estimation of population parameters.\nBiometrika, 40(3,4):237–264.\nHarris, Zellig. 1951. Methods in Structural Linguistics. University of Chicago Press. Reprinted by Phoenix\nBooks in 1960 under the title Structural Linguistics.\nLambek, Joachim.\n1958.\nThe mathematics of sentence structure.\nAmerican Mathematical Monthly,\n65:154–169.\nLewis, David K. 1970. General semantics. Synth`ese, 22:18–67.\nMontague, Richard. 1974. Formal Philosophy: Selected Papers of Richard Montague. Yale University\nPress. Edited by Richmond H. Thomason.\nMosteller, Frederick and David L. Wallace. 1984. Applied Bayesian and Classical Inference: The Case\nof the Federalist Papers. Springer-Verlag. First edition published in 1964 under the title Inference and\nDisputed Authorship: The Federalist.\nPollard, Carl and Ivan Sag. 1994. Head-driven phrase structure grammar. Chicago University Press and\nCSLI Publications.\nSaffran, Jenny R., Richard N. Aslin, and Elissa L. Newport. 1996. Statistical learning by 8-month-old\ninfants. Science, 274(5294):1926–1928, December.\nShannon, Claude E. 1948. A mathematical theory of communication. Bell System Technical Journal,\n27:379–423 and 623–656.\nTuring, Alan M. 1950. Computing machinery and intelligence. Mind, LIX:433–60.\nWeaver, Warren. 1949. Translation. Memorandum. Reprinted in W.N. Locke and A.D. Booth, eds., Ma-\nchine Translation of Languages: Fourteen Essays, MIT Press, 1955.\nFor further reading\nCharniak, Eugene. 1993. Statistical Language Learning. MIT Press.\nJurafsky, Daniel and James H. Martin. 2000. Speech and Language Processing: An Introduction to Natural\nLanguage Processing, Computational Linguistics, and Speech Recognition. Prentice Hall. Contribut-\ning writers: Andrew Kehler, Keith Vander Linden, and Nigel Ward.\n6\nManning, Christopher D. and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Process-\ning. The MIT Press.\n7\n",
  "categories": [
    "cs.CL",
    "I.2.7"
  ],
  "published": "2003-04-21",
  "updated": "2003-04-21"
}