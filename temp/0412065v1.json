{
  "id": "http://arxiv.org/abs/cs/0412065v1",
  "title": "A Framework for Creating Natural Language User Interfaces for Action-Based Applications",
  "authors": [
    "Stephen Chong",
    "Riccardo Pucella"
  ],
  "abstract": "In this paper we present a framework for creating natural language interfaces\nto action-based applications. Our framework uses a number of reusable\napplication-independent components, in order to reduce the effort of creating a\nnatural language interface for a given application. Using a type-logical\ngrammar, we first translate natural language sentences into expressions in an\nextended higher-order logic. These expressions can be seen as executable\nspecifications corresponding to the original sentences. The executable\nspecifications are then interpreted by invoking appropriate procedures provided\nby the application for which a natural language interface is being created.",
  "text": "arXiv:cs/0412065v1  [cs.CL]  17 Dec 2004\nA Framework for Creating Natural Language User Interfaces\nfor Action-Based Applications∗\nStephen Chong\nCornell University\nIthaca, NY 14853 USA\nschong@cs.cornell.edu\nRiccardo Pucella\nCornell University\nIthaca, NY 14853 USA\nriccardo@cs.cornell.edu\nAbstract\nIn this paper we present a framework for creating natural language interfaces to action-based\napplications. Our framework uses a number of reusable application-independent components,\nin order to reduce the effort of creating a natural language interface for a given application.\nUsing a type-logical grammar, we ﬁrst translate natural language sentences into expressions\nin an extended higher-order logic. These expressions can be seen as executable speciﬁcations\ncorresponding to the original sentences. The executable speciﬁcations are then interpreted by\ninvoking appropriate procedures provided by the application for which a natural language in-\nterface is being created.\n1\nIntroduction\nThe separation of the user interface from the application is regarded as a sound design principle. A\nclean separation of these components allows different user interfaces such as GUI, command-line\nand voice-recognition interfaces. To support this feature, an application would supply an applica-\ntion interface. Roughly speaking, an application interface is a set of “hooks” that an application\nprovides so that user interfaces can access the application’s functionality. A user interface issues\ncommands and queries to the application through the application interface; the application executes\nthese commands and queries, and returns the results back to the user interface. We are interested\nin applications whose interface can be described in terms of actions that modify the application’s\nstate, and predicates that query the current state of the application. We refer to such applications as\naction-based applications.\nIn this paper, we propose a framework for creating natural language user interfaces to action-\nbased applications. These user interfaces will accept commands from the user in the form of natural\nlanguage sentences. We do not address how the user inputs these sentences (by typing, by speak-\ning into a voice recognizer, etc), but rather focus on what to do with those sentences. Intuitively,\nwe translate natural language sentences into appropriate calls to procedures available through the\napplication interface.\n∗A preliminary version of this paper appeared in the Proceedings of the Third International AMAST Workshop on\nAlgebraic Methods in Language Processing, TWLT Report 21, pp. 83-98, 2003.\n1\nOther UI 2\n= Application\nspecific\ncomponent\n= Application\nindependent\ncomponent\nAction Based\nApplication\nNLUI\nScreen\nLexicon\nInterpreter\nParser\nAdapter\nOther UI 1\nApplication\nFigure 1: Architecture\nAs an example, consider the application TOYBLOCKS. It consists of a graphical representation\nof two blocks on a table, that can be moved, and put one on top of the other. We would like to be\nable to take a sentence such as move block one on block two, and have it translated into suitable calls\nto the TOYBLOCKS interface that would move block 1 on top of block 2. (This requires that the\ninterface of TOYBLOCKS supplies a procedure for moving blocks.) While this example is simple,\nit already exposes most of the issues with which our framework must deal.\nOur framework architecture is sketched in Figure 1. The diagram shows an application with\nseveral different user interfaces. The box labeled “NLUI” represents the natural language user in-\nterface that our framework is designed to implement. Our framework is appropriate for applications\nthat provide a suitable application interface, which is described in Section 2. We expect that most\nexistent applications will not provide an interface conforming to our requirements. Thus, an adapter\nmight be required, as shown in the ﬁgure. Other user interfaces can also build on this application\ninterface. The user interface labeled “Other UI 1” (for instance, a command-line interface) does just\nthat. The application may have some user interfaces that interact with the application through other\nmeans, such as the user interface “Other UI 2” (for instance, the native graphical interface of the\napplication).\nThe translation from natural language sentences to application interface calls is achieved in\ntwo steps. The ﬁrst step is to use a categorial grammar [Carpenter 1997] to derive an intermediate\nrepresentation of the semantics of the input sentence. An interesting feature of categorial grammars\nis that the semantics of the sentence is compositionally derived from the meaning of the words in\nthe lexicon. The derived meaning is a formula of higher-order logic [Andrews 1986]. The key\nobservation is that such a formula can be seen as an executable speciﬁcation. More precisely, it\ncorresponds to an expression of a simply-typed λ-calculus [Barendregt 1981]. The second step\nof our translation is to execute this λ-calculus expression via calls to procedures supplied by the\napplication interface.\nWe implement the above scheme as follows. A parser accepts a natural language sentence from\nthe user, and attempts to parse it using the categorial grammar rules and the vocabulary from the\napplication-speciﬁc lexicon. The parser fails if it is not able to provide a unique unambiguous\n2\nparsing of the sentence. Successful parsing results in a formula in our higher-order logic, which\ncorresponds to an expression in an action calculus—a λ-calculus equipped with a notion of action.\nThis expression is passed to the action calculus interpreter, which “executes” the expression by\nmaking appropriate calls to the application via the application interface. The interpreter may report\nback to the screen the results of executing the actions.\nThe main advantage of our approach is its modularity. This architecture contains only a few\napplication-speciﬁc components, and has a number of reusable components. More precisely, the\ncategorial grammar parser and the action calculus interpreter are generic and reusable across differ-\nent applications. The lexicon, on the other hand, provides an application-speciﬁc vocabulary, and\ndescribes the semantics of the vocabulary in terms of a speciﬁc application interface.\nIn Section 2 we describe our requirements for action-based applications. We deﬁne the notion\nof an application interface, and provide a semantics for such an interface in terms of a model of the\napplication. In Section 3 we present an action calculus that can be used to capture the meaning of\nimperative natural language sentences. The semantics of this action calculus are given in terms of\nan action-based application interface and application model; these semantics permit us to evaluate\nexpressions of the action calculus by making calls to the application interface. Section 4 provides\na brief introduction to categorial grammars. Section 5 shows how these components (action-based\napplications, action calculus, and categorial grammar) are used in our framework. We discuss some\nextensions to the framework in Section 6, and conclude in Section 7.\n2\nAction-Based Applications\nOur framework applies to applications that provide a suitable Application Programmer Interface\n(API). Roughly speaking, such an interface provides procedures that are callable from external\nprocesses to “drive” the application. In this section, we describe in detail the kind of interface\nneeded by our approach. We also introduce a model of applications that will let us reason about the\nsuitability of the whole framework.\n2.1\nApplication Interface\nOur framework requires action-based applications to have an application interface that speciﬁes\nwhich externally callable procedures exist in the application. This interface is meant to specify\nprocedures that can be called from programs written in fairly arbitrary programming languages. To\nachieve this, we assume only that the calling language can distinguish between objects (the term\n‘object’ is used is a nontechnical sense, to denote arbitrary data values), and Boolean values tt (true)\nand ff (false).\nAn application interface speciﬁes the existence of a number of different kind of procedures.\n(1) Constants: There is a set of constants representing objects of interest. For TOYBLOCKS, the\nconstants are b1, b2, and table.\n(2) Predicates: There is a set of predicates deﬁned over the states of the application. A predicate\ncan be used to check whether objects satisfy certain properties, dependent on the state of\nthe application. Predicates return truth values. For TOYBLOCKS, we consider the single\npredicate is on(bl, pos), that checks whether a particular block bl is in a particular position\n3\npos (on another block or on the table). Each predicate p has an associated arity, indicating\nhow many arguments it needs.\n(3) Actions: Finally, there is a set of actions deﬁned by the application. Actions essentially effect\na state change. Actions can be given arguments, for example, to effect a change to a particular\nobject. For TOYBLOCKS, we consider a single action, move(bl, pos), which moves block\nbl to position pos (on another block or on the table). As with predicates, each action has an\nassociated arity, which may be 0, indicating that the action is parameterless.\nWe emphasize that the application interface simply gives the names of the procedures that are\ncallable by external processes. It does not actually deﬁne an implementation for these procedures.\nIn order to prevent predicates and actions from being given inappropriate arguments, we need\nsome information about the actual kind of objects associated with constants, and that the predicates\nand actions take as arguments. We make the assumption that every object in the application belongs\nto at least one of many classes of objects. Let C be such a set of classes. Although this termi-\nnology evokes object-oriented programming, we emphasize that an object-oriented approach is not\nnecessary for such interfaces; a number of languages and paradigms are suitable for implementing\napplication interfaces.\nWe associate “class information” to every name in the interface via a map σ. More speciﬁcally,\nwe associate with every constant c a set σ(c) ⊆C representing the classes of objects that can be\nassociated with c. We associate with each predicate p a set σ(p) ⊆Cn (where n is the arity of the\npredicate), indicating for which classes of objects the predicate is deﬁned. Similarly, we associate\nwith each action a a set σ(a) ⊆Cn (again, where n is the arity of the action, which in this case can\nbe 0). As we will make clear shortly, we only require that the application return meaningful values\nfor objects of the right classes.\nFormally, an application interface is a tuple I = (C, P, A, C, σ), where C is a set of constant\nnames, P is a set of predicate names, A is a set of action names, C is the set of classes of the\napplication, and σ is the map associating every element of the interface with its corresponding class\ninformation. The procedures in the interface provide a means for an external process to access\nthe functionality of the application, by presenting to the language a generally accessible version\nof the constants, predicates, and actions. Of course, in our case, we are not interested in having\narbitrary processes invoking procedures in the interface, but speciﬁcally an interpreter that interprets\ncommands written in a natural language.\nIn a precise sense, the map σ describes typing information for the elements of the interface.\nHowever, because we do not want to impose a particular type system on the application (for instance,\nwe do not want to assume that the application is object-oriented), we instead assume a form of\ndynamic typing. More precisely, we assume that there is a way to check if an object belongs to\na given class. This can either be performed through special guard predicates in the application\ninterface (for instance, a procedure is block that returns true if the supplied object is actually a\nblock), or a mechanism similar to Java’s instanceOf operator.\nExample 2.1. As an example, consider the following interface IT for TOYBLOCKS. Let IT =\n(C, P, A, C, σ), where, as we discussed earlier,\nC = {b1, b2, table}\nP = {is on}\nA = {move}.\n4\nWe consider only two classes of objects, block, representing the blocks that can be moved, and\nposition, representing locations where blocks can be located. Therefore, C = {block, position}.\nTo deﬁne σ, consider the way in which the interface could be used. The constant b1 represents an\nobject that is both a block that can be moved, and a position to which the other block can be moved\nto (since we can stack blocks on top of each other). The constant b2 is similar. The constant table\nrepresents an object that is a position only. Therefore, we have:\nσ(b1) = {block, position}\nσ(b2) = {block, position}\nσ(table) = {position}.\nCorrespondingly, we can derive the class information for is on and move:\nσ(is on) = {(block, position)}\nσ(move) = {(block, position)}.\n⊓⊔\n2.2\nApplication Model\nIn order to reason formally about the interface, we provide a semantics to the procedures in the\ninterface. This is done by supplying a model of the underlying application. We make a number of\nsimplifying assumptions about the application model, and discuss relaxing some of these assump-\ntions in Section 6.\nApplications are modeled using four components:\n(1) Interface: The interface, as we saw in the previous section, speciﬁes the procedures that can\nbe used to query and affect the application. The interface also deﬁnes the set C of classes of\nobjects in the application.\n(2) States: A state is, roughly speaking, everything that is relevant to understand how the appli-\ncation behaves. At any given point in time, the application is in some state. We assume that\nan application’s state changes only through explicit actions.\n(3) Objects: This deﬁnes the set of objects that can be manipulated, or queried, in the application.\nAs we already mentioned, we use the term ‘object’ in the generic sense, without implying\nthat the application is implemented through an object-oriented language. Every object is\nassociated with at least one class.\n(4) Interpretation: An interpretation associates with every element of the interface a “meaning”\nin the application model. As we shall see, it associates with every constant an object of\nthe model, with every predicate a predicate on the model, and with every action a state-\ntransformation on the model.\nFormally, an application is a tuple M = (I, S, O, π), where I in an interface (that deﬁnes the\nconstants, predicates, and actions of the application, as well as the classes of the objects), S is the\nset of states of the application, O is the set of objects, and π is the interpretation.\n5\nWe extend the map σ deﬁned in the interface to also provide class information for the objects\nin O. Speciﬁcally, we deﬁne for every object o ∈O a set σ(o) ⊆C of classes to which that object\nbelongs. An object can belong to more than one class.\nThe map π associates with every state and every element in the interface (i.e., every constant,\npredicate and action) the appropriate interpretation of that element at that state. Speciﬁcally, for a\nstate s ∈S, we have π(s)(c) ∈O. Therefore, constants can denote different objects at different\nstates of the applications. For predicates, π(s)(p) is a partial function from O × . . . × O to truth\nvalues tt or ff. This means that predicates are pure, in that they do not modify the state of an\napplication; they are simply used to query the state. For actions, π(s)(a) is a partial function from\nO×. . .×O to S. The interpretation π is subject to the following conditions. For a given predicate p,\nthe interpretation π(s)(p) must be deﬁned on objects of the appropriate class. Thus, the domain of\nthe partial function π(s)(p) must at least consist of {(o1, . . . , on) | σ(o1)×. . .×σ(on)∩σ(p) ̸= ∅}.\nSimilarly, for a given action a, the domain of the partial function π(s)(a) must at least consist of\n{(o1, . . . , on) | σ(o1)×. . .×σ(on)∩σ(a) ̸= ∅}. Furthermore, any class associated with a constant\nmust also be associated with the corresponding object. In other words, for all constants c, we must\nhave σ(c) ⊆σ(π(s)(c)) for all states s.\nExample 2.2. We give a model MT for our sample TOYBLOCKS application, to go with the inter-\nface IT deﬁned in Example 2.1. Let MT = (IT , S, O, π). We will consider only three states in the\napplication, S = {s1, s2, s3}, which can be described variously:\nin state s1, blocks 1 and 2 are on the table\nin state s2, block 1 is on block 2, and block 2 is on the table\nin state s3, block 1 is on the table, and block 2 is on block 1.\nWe consider only three objects in the model, O = {b1, b2, t}, where b1 is block 1, b2 is block 2, and\nt is the table. We extend the map σ in the obvious way:\nσ(b1) = {block, position}\nσ(b2) = {block, position}\nσ(t) = {position}.\nThe interpretation for constants is particularly simple, as the interpretation is in fact independent of\nthe state (in other words, the constants refer to the same objects at all states):\nπ(s)(b1) = b1\nπ(s)(b2) = b2\nπ(s)(table) = t.\nThe interpretation of the is on predicates is straightforward:\nπ(s1)(is on)(x) =\n\u001a tt\nif x ∈{(b1, t), (b2, t)}\nff\nif x ∈{(b1, b1), (b1, b2), (b2, b1), (b2, b2)}\nπ(s2)(is on)(x) =\n\u001a tt\nif x ∈{(b1, b2), (b2, t)}\nff\nif x ∈{(b1, t), (b1, b1), (b2, b1), (b2, b2)}\nπ(s3)(is on)(x) =\n\u001a tt\nif x ∈{(b1, t), (b2, b1)}\nff\nif x ∈{(b1, b1), (b1, b2), (b2, t), (b2, b2)}.\nThe interpretation of move is also straightforward:\n6\nπ(s1)(move)(x) =\n\n\n\ns2\nif x = (b1, b2)\ns3\nif x = (b2, b1)\ns1\nif x ∈{(b1, t), (b1, b1), (b2, t), (b2, b2)}\nπ(s2)(move)(x) =\n\u001a s1\nif x = (b1, t)\ns2\nif x ∈{(b1, b1), (b1, b2), (b2, t), (b2, b1), (b2, b2)}\nπ(s3)(move)(x) =\n\u001a s1\nif x = (b2, t)\ns3\nif x ∈{(b1, t), (b1, b1), (b1, b2), (b2, b1), (b2, b2)}.\nIf a block is unmovable (that is, if there is another block on it), then the state does not change\nfollowing a move operation.\n⊓⊔\n3\nAn Action Calculus\nAction-based application interfaces are designed to provide a means for external processes to access\nthe functionality of an application. In this section we deﬁne a powerful and ﬂexible language that\ncan be interpreted as calls to an application interface. The language we use is a simply-typed λ-\ncalculus extended with a notion of action. It is effectively a computational λ-calculus in the style\nof Moggi [1989], although we give a nonstandard presentation in order to simplify expressing the\nlanguage semantics in terms of an application interface.\nThe calculus is parameterized by a particular application interface and application model. The\napplication interface provides the primitive constants, predicates, and actions, that can be used to\nbuild more complicated expressions, while the application model is used to deﬁne the semantics.\n3.1\nSyntax\nEvery expression in the language is given a type, intuitively describing the kind of values that the\nexpression produces. The types used in this language are given by the following grammar.\nTypes:\nτ ::=\ntype\nObj\nobject\nBool\nboolean\nAct\naction\nτ1 →τ2\nfunction\nThe types τ correspond closely to the types required by the action-based application interfaces we\ndeﬁned in the previous section: the type Bool is the type of truth values, with constants true and\nfalse corresponding to the Boolean values tt and ff, and the type Obj is the type of generic objects.\nThe type Act is more subtle; an expression of type Act represents an action that can be executed to\nchange the state of the application. This is an example of computational type as deﬁned by Moggi\n[1989]. As we shall see shortly, expressions of type Act can be interpreted as calls to the action\nprocedures of the application interface.\nThe classes C deﬁned by the application interface have no corresponding types in this language—\ninstead, all objects have the type Obj. Incorporating these classes as types is an obvious possible\nextension (see Section 6).\nThe syntax of the language is a straightforward extension of that of the λ-calculus.\n7\nSyntax of Expressions:\nv ::=\nvalue\ntrue | false\nboolean\nλx:τ.e\nfunction\nskip\nnull action\ne ::=\nexpression\nx\nvariable\nv\nvalue\nidc()\nconstant\nidp(e1, . . . , en)\npredicate\nida(e1, . . . , en)\naction\ne1 e2\napplication\ne1?e2:e3\nconditional\ne1; e2\naction sequencing\nThe expressions idc(), idp(e1, . . . , en) and ida(e1, . . . , en) correspond to the procedures (respec-\ntively, constants, predicates, and actions) available in the application interface. (Constants are writ-\nten idc() as a visual reminder that they are essentially functions: idc() may yield different values at\ndifferent states, as the semantics will make clear.) So, for TOYBLOCKS, the constants are b1, b2,\nand table; the only predicate is is on; and the only action is move. The expression e1?e2:e3 is a\nconditional expression, evaluating to e2 if e1 evaluates to true, and e3 if e1 evaluates to false. The\nexpression e1; e2 (when e1 and e2 are actions) evaluates to an action corresponding to performing\ne1 followed by e2. The constant skip represents an action that has no effect.\nExample 3.1. Consider the interface for TOYBLOCKS. The expression b1() represents block 1,\nwhile table() represents the table. The expression move(b1(), table()) represents the action\nof moving block 1 on the table. Similarly, the action move(b1(), table()); move(b2(), b1())\nrepresents the composite action of moving block 1 on the table, and then moving block 2 on top of\nblock 1.\n⊓⊔\n3.2\nOperational Semantics\nThe operational semantics is deﬁned with respect to the application model. More precisely, the\nsemantics is given by a transition relation, written (s, e) −→(s′, e′), where s, s′ are states of the\napplication, and e, e′ are expressions. Intuitively, this represents the expression e executing in state\ns, and making a one-step transition to a (possibly different) state s′ and a new expression e′.\nTo accommodate the transition relation, we need to extend the syntax of expressions to account\nfor object values produced during the evaluation. We also include a special value ⋆that represents\nan exception raised by the code. This exception is used to capture various errors that may occur\nduring evaluation.\n8\nAdditional Syntax of Expressions:\nvo ∈O\nobject value\nv ::=\nvalue\n...\nvo\nobject\n⋆\nexception\nThe transition relation is parameterized by the functions δc, δp and δp, given below. These\nfunctions provide a semantics to the constant, predicate, and action procedures respectively, and\nare derived from the interpretation π in the application model. The intuition is that evaluating\nthese functions corresponds to making calls to the appropriate procedures on the given application\ninterface, and returning the result.\nReduction Rules for Interface Elements:\nδc(s, idc) ≜π(s)(idc)\nδp(s, idp, v1, . . . , vn) ≜\n\u001a π(s)(id p)(v1, . . . , vn)\nif σ(v1) × . . . × σ(vn) ∩σ(idp) ̸= ∅\n⋆\notherwise\nδa(s, ida, v1, . . . , vn) ≜\n\u001a π(s)(ida)(v1, . . . , vn)\nif σ(v1) × . . . × σ(vn) ∩σ(ida) ̸= ∅\n⋆\notherwise\nNote that determining whether or not a primitive throws an exception depends on being able to\nestablish the class of an object (via the map σ). We can thus ensure that we never call an action or\npredicate procedure on the application interface with inappropriate objects, and so we guarantee a\nkind of dynamic type-safety with respect to the application interface.\nReduction Rules:\n(Red App 1)\n(s, e1) −→(s, e′\n1)\n(s, e1 e2) −→(s, e′\n1 e2)\n(Red App 2)\n(s, e1) −→(s, ⋆)\n(s, e1 e2) −→(s, ⋆)\n(Red App 3)\n(s, (λx:τ.e1) e2) −→(s, e1{x←e2})\n(Red OCon)\n(s, idc()) −→(s, δc(s, idc))\n(Red PCon 1)\n(s, ei) −→(s, e′\ni)\nfor some i ∈[1..n]\n(s, idp(. . . , ei, . . .)) −→(s, idp(. . . , e′\ni, . . .))\n(Red PCon 2)\n(s, ei) −→(s, ⋆)\nfor some i ∈[1..n]\n(s, idp(e1, . . . , en)) −→(s, ⋆)\n(Red PCon 3)\n(s, idp(v1, . . . , vn)) −→(s, v)\nδp(s, idp, v1, . . . , vn) = v\n(Red If 1)\n(s, e1) −→(s, e′\n1)\n(s, e1?e2:e3) −→(s, e′\n1?e2:e3)\n(Red If 2)\n(s, e1) −→(s, ⋆)\n(s, e1?e2:e3) −→(s, ⋆)\n(Red If 3)\n(s, v?etrue:efalse) −→(s, ev)\n9\n(Red Seq 1)\n(s, e1) −→(s′, e′\n1)\n(s, e1; e2) −→(s′, e′\n1; e2)\n(Red Seq 2)\n(s, ⋆; e) −→(s, ⋆)\n(Red Seq 3)\n(s, skip; e) −→(s, e)\n(Red ACon 1)\n(s, ei) −→(s, e′\ni)\nfor some i ∈[1..n]\n(s, ida(. . . , ei, . . .)) −→(s, ida(. . . , e′\ni, . . .))\n(Red ACon 2)\n(s, ei) −→(s, ⋆)\nfor some i ∈[1..n]\n(s, ida(e1, . . . , en)) −→(s, ⋆)\n(Red ACon 3)\n(s, ida(v1, . . . , vn)) −→(s′, skip)\nδa(s, ida, v1, . . . , vn) = s′\n(Red ACon 4)\n(s, ida(v1, . . . , vn)) −→(s, ⋆)\nδa(s, ida, v1, . . . , vn) = ⋆\nThe operational semantics is a combination of call-by-name and call-by-value semantics. The\nlanguage as a whole is evaluated in a call-by-name fashion. In particular, rule (Red App 3) indicates\nthat application is call-by-name. Actions, on the other hand, are evaluated under what might be\ncalled call-by-value, as indicated by rule (Red Seq 1). Roughly, the ﬁrst term of a sequencing\noperation e1; e2 is fully evaluated before e2 is evaluated. Intuitively, applications are evaluated\nunder call-by-name because premature evaluation of actions could lead to action procedures in the\napplication interface being called inappropriately. For example, under call-by-value semantics, the\nevaluation of the following expression\n(λx:Act.false?x:skip) A\nwould call the action procedure for A, assuming A is an action in the application interface. This\ndoes not agree with the intuitive interpretation of actions. More importantly, the mapping from nat-\nural language sentences to expressions in our calculus naturally yields a call-by-name interpretation.\n3.3\nType System\nWe use type judgments to ensure that expressions are assigned types appropriately, and that the\ntypes themselves are well-formed. Roughly speaking, a type is well-formed if it preserves the sepa-\nration between pure computations (computations with no side-effects) and imperative computations\n(computations that may have side-effects). The type system enforces that pure computations do\nnot change the state of the application. This captures the intuition that declarative sentences—\ncorresponding to pure computations— should not change the state of the world. (This correspon-\ndence between declarative sentences and pure computations is made clear in the next section.) The\nrules for the type well-formedness judgment ⊢τ ok are given in the following table, along with the\nauxiliary judgment ⊢τ pure, stating that a type τ is a pure type (evaluates without side effects).\n10\nJudgments ⊢τ pure and ⊢τ ok:\n(Pure Obj)\n⊢Obj pure\n(Pure Bool)\n⊢Bool pure\n(Pure Fun)\n⊢τ1 →τ2 pure\n(OK Fun Pure)\n⊢τ1 pure\n⊢τ2 pure\n⊢τ1 →τ2 ok\n(OK Fun Act)\n⊢τ →Act ok\nThe judgment Γ ⊢e : τ assigns a type τ to expression e in a well-formed environment Γ.\nAn environment Γ deﬁnes the types of all variables in scope. An environment is of the form x1 :\nτ1, . . . , xn : τn, and deﬁnes each variable xi to have type τi. We require that variables do not repeat\nin a well-formed environment. The typing rules for expressions are essentially standard, with the\nexception of the typing rule for functions, which requires that function types τ →τ ′ be well-formed.\nJudgment Γ ⊢e : τ:\n(Typ Var)\nΓ, x : τ ⊢x : τ\n(Typ Obj)\nΓ ⊢vo : Obj\n(Typ True)\nΓ ⊢true : Bool\n(Typ False)\nΓ ⊢false : Bool\n(Typ Exc)\nΓ ⊢⋆: τ\n(Typ App)\nΓ ⊢e1 : τ →τ ′\nΓ ⊢e2 : τ\nΓ ⊢e1 e2 : τ ′\n(Typ Fun)\nΓ, x : τ ⊢e : τ ′\n⊢τ →τ ′ ok\nΓ ⊢λx:τ.e : τ →τ ′\n(x ̸∈Dom(Γ))\n(Typ If)\nΓ ⊢e1 : Bool\nΓ ⊢e2 : τ\nΓ ⊢e3 : τ\nΓ ⊢e1?e2:e3 : τ\n(Typ Skip)\nΓ ⊢skip : Act\n(Typ Seq)\nΓ ⊢e1 : Act\nΓ ⊢e2 : Act\nΓ ⊢e1; e2 : Act\n(Typ ACon)\nΓ ⊢ei : Obj\n∀i ∈[1..n]\nΓ ⊢ida(e1, . . . , en) : Act\n(Typ OCon)\nΓ ⊢idc() : Obj\n(Typ PCon)\nΓ ⊢ei : Obj\n∀i ∈[1..n]\nΓ ⊢idp(e1, . . . , en) : Bool\nIt is straightforward to show that our type system is sound, that is, that type-correct expressions\ndo not get stuck when evaluating. We write (s, e) −→∗(s′, e′) to mean that there exists a sequence\n(s1, e2), . . . , (sn, en) such that (s, e) −→(s1, e1) −→. . . −→(sn, en) −→(s′, e′).\nTheorem 3.2. If ⊢e : τ, and s is a state, then there exists a state s′ and value v such that\n(s, e) −→∗(s′, v). Moreover, if ⊢τ pure, then s′ = s.\nProof. See Appendix A.\n⊓⊔\nTheorem 3.2 in fact states that the language is strongly normalizing: the evaluation of every\nexpression terminates. This is a very desirable property for the language, since it will form part of\nthe user interface.\nExample 3.3. Consider the following example, interpreted with respect to the application model\nof Example 2.2. In state s1 (where both block 1 and 2 are on the table), let us trace through the\n11\nexecution of the expression (λx:Obj.λy:Obj.move(x, y)) (b1()) (b2()). (We omit the derivation\nindicating how each step is justiﬁed.)\n(s1, (λx:Obj.λy:Obj.move(x, y)) (b1()) (b2())) −→\n(s1, (λy:Obj.move(b1(), y)) (b2())) −→\n(s1, move(b1(), b2())) −→\n(s1, move(b1, b2())) −→\n(s1, move(b1, b2)) −→\n(s2, skip).\nIn other words, evaluating the expression in state s1 leads to state s2, where indeed block 1 is on top\nof block 2.\n⊓⊔\n3.4\nA Direct Interpreter\nThe main reason for introducing the action calculus of this section is to provide a language in\nwhich to write expressions invoking procedures available in the application interface. However,\nthe operational semantics given above rely on explicitly passing around the state of the application.\nThis state is taken from the application model. In the model, the state is an explicit datum that enters\nthe interpretation of constants, predicates and actions. Of course, in the actual application, the state\nis implicitly maintained by the application itself. Invoking an action procedure on the application\ninterface modiﬁes the current state of the application, putting the application in a new state. This\nnew state is not directly visible to the user.\nWe can implement an interpreter based on the above operational semantics but without carrying\naround the state explicitly. To see this, observe that the state is only relevant for the evaluation of\nthe primitives (constants, predicates, and actions). More importantly, it is always the current state\nof the application that is relevant, and only actions are allowed to change the state. We can therefore\nimplement an interpreter by simply directly invoking the procedures in the application interface\nwhen the semantics tells us to reduce via δc, δp, or δa. Furthermore, we need to be able to raise an\nexception ⋆if the objects passed to the interface are not of the right class. This requires querying\nfor the class of an object. As we indicated in Section 2.1, we simply assume that this can be done,\neither through language facilities (an instanceOf operator), or through explicit procedures in the\ninterface that check whether an object is of a given class.\nIn summary, given an application with a suitable application interface, we can write an in-\nterpreter for our action calculus that will interpret expressions by invoking procedures available\nthrough the application interface when appropriate. The interpreter does not require an application\nmodel. The model is useful to establish properties of the interpreter, and if one wants to reason\nabout the execution of expressions via the above operational semantics.\n4\nCategorial Grammars\nIn the last section, we introduced an action calculus that lets us write expressions that can be un-\nderstood via calls to the application interface. The aim of this section is to use this action calculus\nas the target of a translation from natural language sentences. In other words, we describe a way to\ntake a natural language sentence and produce a corresponding expression in our action calculus that\ncaptures the meaning of the sentence. Our main tool is categorial grammars.\n12\nCategorial grammars provide a mechanism to assign semantics to sentences in natural language\nin a compositional manner. As we shall see, we can obtain a compositional translation from natural\nlanguage sentences into the action calculus presented in the previous section, and thus provide a\nsimple natural language user interface for a given application. This section provides a brief exposi-\ntion of categorial grammars, based on Carpenter’s [1997] presentation. We should note that the use\nof categorial grammars is not a requirement in our framework. Indeed, any approach to provide se-\nmantics to natural language sentences in higher-order logic, which can be viewed as a simply-typed\nλ-calculus [Andrews 1986], can be adapted to our use. For instance, Moortgat’s [1997] multimodal\ncategorial grammars, which can handle a wider range of syntactic constructs, can also be used for\nour purposes. To simplify the exposition, we use the simpler categorial grammars in this paper.\nCategorial grammars were originally developed by Ajdukiewicz [1935] and Bar-Hillel [1953],\nand later generalized by Lambek [1958]. The idea behind categorial grammars is simple. We start\nwith a set of categories, each category representing a grammatical function. For instance, we can\nstart with the simple categories np representing noun phrases, pp representing prepositional phrases,\ns representing declarative sentences and a representing imperative sentences. Given categories A\nand B, we can form the functor categories A/B and B\\A. The category A/B represents the\ncategory of syntactic units that take a syntactic unit of category B to their right to form a syntactic\nunit of category A. Similarly, the category B\\A represents the category of syntactic units that take\na syntactic unit of category B to their left to form a syntactic unit of category A.\nConsider some examples. If np is the category of noun phrases and s is the category of declar-\native sentences, then the category np\\s is the category of intransitive verbs (e.g., laughs): they take\na noun phrase on their left to form a sentence (e.g., Alice laughs or the reviewer laughs). Similarly,\nthe category (np\\s)/np represents the category of transitive verbs (e.g., takes): they take a noun\nphrase on their right and then a noun phrase on their left to form a sentence (e.g., Alice takes the\ndoughnut). We also consider the category pp of propositional phrases, as well as the category a of\nimperative sentences.\nThe main goal of categorial grammars is to provide a method of determining the well-formedness\nof natural language. A lexicon associates every word (or complex sequence of words that constitute\na single lexical entry) with one or more categories. The approach described by Lambek [1958] is\nto prescribe a calculus of categories so that if a sequence of words can be assigned a category A\naccording to the rules, then the sequence of words is deemed a well-formed syntactic unit of cate-\ngory A. Hence, a sequence of words is a well-formed noun phrase if it can be shown in the calculus\nthat it has category np. As an example of reduction, we see that if σ1 has category A and σ2 has\ncategory A\\B, then σ1 σ2 has category B. Schematically, A, A\\B ⇒B. Moreover, this goes both\nways, that is, if σ1 σ2 has category B and σ1 can be shown to have category A, then we can derive\nthat σ2 has category A\\B.\nVan Benthem [1986] showed that this calculus could be used to assign a semantics to terms by\nfollowing the derivation of the categories. Assume that every basic category is assigned a type in\nour action calculus, through a type assignment T. A type assignment T can be extended to functor\ncategories by putting T(A/B) = T(B\\A) = T(B) →T(A). The lexicon is extended so that every\nword is now associated with one or more pairs of a category A and an expression α in our action\ncalculus of the appropriate type, that is, ⊢α : T(A).\nWe use the sequent notation α1 : A1, . . . , αn : An ⇒α : A to mean that expressions α1, . . . , αn\nof categories A1, . . . , An can be concatenated to form an expression α of category A. We call α : A\nthe conclusion of the sequent. We use capital Greek letters (Σ, ∆,...) to represent sequences of\n13\nexpressions and categories. (We reserve Γ for typing contexts of the calculus in the last section.)\nWe now give rules that allow us to derive new sequents from other sequents.\nCategorial Grammar Sequent Rules:\n(Seq Id)\nα : A ⇒α : A\n(Seq Cut)\n∆⇒β : B\nΣ1, β : B, Σ2 ⇒α : A\nΣ1, ∆, Σ2 ⇒α : A\n(Seq App Right)\n∆⇒β : B\nΣ1, α(β) : A, Σ2 ⇒γ : C\nΣ1, α : A/B, ∆, Σ2 ⇒γ : C\n(Seq App Left)\n∆⇒β : B\nΣ1, α(β) : A, Σ2 ⇒γ : C\nΣ1, ∆, α : B\\A, Σ2 ⇒γ : C\n(Seq Abs Right)\nΣ, x : A ⇒α : B\nΣ ⇒λx.α : B/A\n(Seq Abs Left)\nx : A, Σ ⇒α : B\nΣ ⇒λx.α : A\\B\nExample 4.1. Consider the following simple lexicon, suitable for the TOYBLOCKS application.\nThe following types are associated with the basic grammatical units:\nT(np) = Obj\nT(pp) = Obj\nT(s) = Bool\nT(a) = Act.\nHere is a lexicon that captures a simple input language for TOYBLOCKS:\nblock one 7→b1() : np\nblock two 7→b2() : np\nthe table 7→table() : np\non 7→(λx:Obj.x) : pp/np\nis 7→(λx:Obj.λy:Obj.is on(y, x)) : (np\\s)/pp\nif 7→(λx:Bool.λy:Act.x?y:skip) : (a/a)/s\nmove 7→(λx:Obj.λy:Obj.move(x, y)) : (a/pp)/np.\nThis is a particularly simple lexicon, since every entry is assigned a single term and category. It is\nalso a very specialized lexicon, for the purpose of illustration; our treatment of is is speciﬁc to the\nTOYBLOCKS example.\nUsing the above lexicon, the sentence move block one on block two can be associated with the string\nof expressions and categories λx:Obj.λy:Obj.move(x, y) : (a/pp)/np, b1() : np, λx:Obj.x :\npp/np, b2() : np. The following derivation shows that this concatenation yields an expression of\n14\ncategory a. (For reasons of space, we have elided the type annotations in λ-abstractions.)\nb2():np ⇒b2():np\nb1():np ⇒b1():np\n(†)\nλx.λy.move(x, y):(a/pp)/np, b1():np, (λx.x) (b2()):pp ⇒\n(λx.λy.move(x, y)) (b1()) ((λx.x) (b2())):a\nλx.λy.move(x, y):(a/pp)/np, b1():np, λx.x:pp/np, b2():np ⇒\n(λx.λy.move(x, y)) (b1()) ((λx.x) (b2())):a\nwhere the subderivation (†) is simply:\n(†) :\n(λx.x) (b2()):pp ⇒\n(λx.x) (b2()):pp\n(λx.λy.move(x, y)) (b1()) ((λx.x) (b2())):a ⇒\n(λx.λy.move(x, y)) (b1()) ((λx.x) (b2())):a\n(λx.λy.move(x, y)) (b1()):a/pp, (λx.x) (b2()):pp ⇒\n(λx.λy.move(x, y)) (b1()) ((λx.x) (b2())):a.\nHence, the sentence is a well-formed imperative sentence. Moreover, the derivation shows that the\nmeaning of the sentence move block one on block two is\n(λx:Obj.λy:Obj.move(x, y)) (b1()) ((λx:Obj.x) (b2())).\nThe execution of this expression, similar to the one in Example 3.3, shows that the intuitive meaning\nof the sentence is reﬂected by the execution of the corresponding expression.\n⊓⊔\nOne might hope that the expressions derived through a categorial grammar derivation are always\nvalid expressions of our action calculus. To ensure that this property holds, we must somewhat\nrestrict the kind of categories that can appear in a derivation. Let us say that a derivation respects\nimperative structure if for every category of the form A\\B or A/B that appears in the derivation,\nwe have ⊢T(A) →T(B) ok. Intuitively, a derivation respects imperative structure if it cannot\nconstruct declarative sentences that depend on imperative subsentences, i.e., a declarative sentence\ncannot have any “side effects.” (For the lexicon in Example 4.1, a derivation respects imperative\nstructure if and only if every category of the form a\\B or B/a that appears in the derivation is\neither a\\a or a/a.) We can show that all such derivations correspond to admissible typing rules in\nthe type system of the last section. (An admissible typing rule is a rule that does not add derivations\nto the type system; anything derivable using the rule can be derived without the rule.)\nTheorem 4.2. If α1 : A1, . . . , αn : An ⇒α : A has a derivation that respects imperative structure,\nthen the rule\nΓ ⊢α1 : T(A1)\n. . .\nΓ ⊢αn : T(An)\nΓ ⊢α : T(A)\nis an admissible typing rule.\nProof. See Appendix A.\n⊓⊔\nNote that if each expression αi : Ai is taken from the lexicon, then we have ⊢αi : T(Ai) by\nassumption, and therefore Theorem 4.2 says that if α1 : A1, . . . , αk : Ak ⇒α : A has a derivation\nthat respects imperative structure, then ⊢α : T(A).\n15\nSo, given a natural language imperative sentence from the user, we use the lexicon to ﬁnd the\ncorresponding expressions and category pairs α1 : A1, . . . , αn : An, and then attempt to parse it,\nthat is, to ﬁnd a derivation for the sequent α1 : A1, . . . , αn : An ⇒α : a that respects imperative\nstructure. If a unique such derivation exists, then we have an unambiguous parsing of the natural\nlanguage imperative sentence, and moreover, the action calculus expression α is the semantics of\nthe imperative sentence.\n5\nPutting It All Together\nWe now have the major components of our framework: a model for action-based applications and\ninterfaces to them; an action calculus which can be interpreted as calls to an application interface;\nand the use of categorial grammars to create expressions in our action calculus from natural language\nsentences.\nLet’s see how our framework combines these components by considering an end-to-end example\nfor TOYBLOCKS. Suppose the user inputs the sentence move block one on block two when blocks\n1 and 2 are both on the table. Our framework would process this sentence in the following steps.\n(1) Parsing: The TOYBLOCKS lexicon is used to parse the sentence. Parsing succeeds only\nif there is a unique parsing of the sentence (via a derivation that respects imperative struc-\nture), otherwise the parsing step fails, because the sentence was either ambiguous, contained\nunknown words or phrases, or was ungrammatical. In this example, there is only a single\nparsing of the sentence (as shown in Example 4.1), and the result is the following expression\nin our action calculus, which has type Act:\n(λx:Obj.λy:Obj.move(x, y)) (b1()) ((λx:Obj.x) (b2())).\n(2) Evaluating: The action calculus expression is evaluated using a direct interpreter imple-\nmenting the operational semantics of Section 3. The evaluation of the expression proceeds as\nfollows.\n(s1, (λx:Obj.λy:Obj.move(x, y)) (b1()) ((λx:Obj.x) (b2()))) −→\n(s1, (λy:Obj.move(b1(), y)) ((λx:Obj.x) (b2()))) −→\n(s1, move(b1(), (λx:Obj.x) (b2()))) −→\n(s1, move(b1, (λx:Obj.x) (b2()))) −→\n(s1, move(b1, (b2()))) −→\n(s1, move(b1, b2)) −→\n(s2, skip).\nIn the process of this evaluation, several calls are generated to the application interface. In\nparticular, calls are made to determine the identity of the object constants b1 and b2 as b1\nand b2 respectively. Then, during the last transition, guard predicates such as is block(b1)\nand is position(b2) may be called to ensure that b1 and b2 are of the appropriate classes\nfor being passed as arguments to move. Since the objects are of the appropriate classes, the\naction move(b1, b2) is invoked via the application interface, and succeeds.\n16\n(3) Reporting: Following the evaluation of the expression, some result must be reported back to\nthe user. Our framework does not detail what information is conveyed back to the user, but\nthey must be informed if an exception was raised during the evaluation of the expression.\nIn this example, no exception was raised, so what to report to the user is at the discretion of\nthe user interface. If the user interface had a graphical depiction of the state of TOYBLOCKS,\nit may now send queries to the application interface to determine the new state of the world,\nand modify its graphical display appropriately.\nLet’s consider what would happen if an exception (⋆) was raised during the evaluation phase. For\nexample, consider processing the sentence move the table on block one. The parsing phase would\nsucceed, as the sentence is grammatically correct. However, prior to calling the action move(t, b1),\nthe evaluation would determine that the object t does not belong to the class block (by a guard\npredicate such as is block(t) returning ff, or by some other mechanism). An exception would\nthus be raised, and some information must be reported back to the user during the reporting phase.\nNote that the framework has ensured that the action move(t, b1) was not invoked on the application\ninterface.\n6\nExtensions\nSeveral extensions to this framework are possible. There is a mismatch of types in our framework.\nThe application model permits a rich notion of types: any object of the application may belong to\none or more classes. By contrast, our action calculus has a very simple notion of types, assigning the\ntype Obj to all objects, and not statically distinguishing different classes of objects. The simplicity\nof our action calculus is achieved at the cost of dynamic type checking, which ensures that actions\nand predicates on the application interface are invoked only with appropriate parameters. It would\nbe straightforward to extend the action calculus with a more reﬁned type system that includes a\nnotion of subtyping, to model the application classes. Not only would this extension remove many,\nif not all, of the dynamic type checks, but it may also reduce the number of possible parses of\nnatural language sentences. The reﬁned type system allows the semantics of the lexicon entries to\nbe ﬁner-grained, and by considering these semantics, some nonsensical parses of a sentence could\nbe ignored. For example, in the sentence pick up the book and the doughnut and eat it the referent\nof it could naively be either the book or the doughnut; if the semantics of eat require an object of the\nclass Food and the classes of the book and the doughnut are considered, then the former possibility\ncould be ruled out.\nAnother straightforward extension to the framework is to allow the user to query the state by\nentering declarative sentences and treating them as yes-no interrogative sentences. For example,\nblock one is on the table? This corresponds to accepting sequents of the form α1 : A1, . . . , αn :\nAn ⇒α : s, and executing the action calculus expression α, which has type Bool. The categorial\ngrammar could be extended to accept other yes-no questions, such as is block two on block one?\nA more interesting extension (which would require a correspondingly more complex application\nmodel) is to allow hypothetical queries, such as if you move block one on block two, is block one\non the table? This corresponds to querying is block one on the table? in the state that would result\nif the action move block one on block two were performed. This extension would bring our higher-\norder logic (that is, our action calculus) closer to dynamic logic [Groenendijk and Stokhof 1991;\n17\nHarel, Kozen, and Tiuryn 2000]. It is not clear, however, how to derive a direct interpreter for such\nan extended calculus.\nIn Section 2.2 we made some simplifying assumptions about the application model. Chief\namong these assumptions was that an application’s state changes only as a result of explicit actions.\nThis assumption may be unrealistic if, for example, the application has multiple concurrent users.\nWe can however extend the framework to relax this assumption. One way of relaxing it is to in-\ncorporate transactions into the application model and application interface: the application model\nwould guarantee that within transactions, states change only as a result of explicit actions, but if no\ntransaction is in progress then states may change arbitrarily. The evaluation of an action calculus\nexpression would then be wrapped in a transaction.\nAnother restriction we imposed was that predicates be pure. It is of course technically possible\nto permit arbitrary state changes during the evaluation of predicates. In fact, we can modify the\noperational semantics to allow the evaluation of any expression to change states. If done properly,\nthe key property is still preserved: the evaluation of constants, predicates or actions rely only on the\ncurrent state, and all other transitions do not rely on the state at all. Thus, the semantics remains\nconsistent with interpreting expressions using calls to the application interface. However, doing this\nwould lose the intuitive meaning of natural language sentences that do not contain actions; they\nshould not change the state of the world.\n7\nConclusion\nWe have presented a framework that simpliﬁes the creation of simple natural language user inter-\nfaces for action-based applications. The key point of this framework is the use of a λ-calculus to\nmediate access to the application. The λ-calculus we deﬁne is used as a semantics for natural lan-\nguage sentences (via categorial grammars), and expressions in this calculus are executed by issuing\ncalls to the application interface. The framework has a number of application-independent compo-\nnents, reducing the amount of effort required to create a simple natural language user interface for\na given application.\nA number of applications have natural language interfaces [Winograd 1971; Price, Rilofff,\nZachary, and Harvey 2000], but they appear to be designed speciﬁcally for the given application,\nrather than being a generic approach. A number of methodologies and frameworks exist for natural\nlanguage interfaces for database queries (see Androutsopoulos et al. [1995] for a survey), but we\nare not aware of a framework for deriving natural language interfaces to general applications in a\nprincipled manner.\nWhile the framework presented here is useful for the rapid development of simple natural lan-\nguage user interfaces, the emphasis is on simple. Categorial grammars (and other techniques that\nuse higher order logic as the semantics of natural language) are limited in their ability to deal with\nthe wide and diverse phenomena that occur in English. For example, additional mechanisms outside\nof the categorial grammar, probably application-speciﬁc, would be required to deal with discourse.\nHowever, categorial grammars are easily extensible, by expanding the lexicon, and many parts of\nthe lexicon of a categorial grammar are reusable in different applications, making it well-suited to a\nframework for rapid development of natural language user interfaces.\nIt may seem that a limitation of our framework is that it is only suitable for applications for\nwhich we can provide an interface of the kind described in Section 2—the action calculus of Sec-\n18\ntion 3 is speciﬁcally designed to be interpreted as calls to an action-based application. However,\nall the examples we considered can be provided with such an interface. It is especially interesting\nto note that our deﬁnition of action-based application interfaces is compatible with the notion of\ninterface for XML web services [Barclay, Gray, Strand, Ekblad, and Richter 2002]. This suggests\nthat it may be possible to derive a natural language interface to XML Web Services using essentially\nthe approach we advocate in this paper.\nAcknowledgments\nThanks to Eric Breck and Vicky Weissman for comments on earlier drafts of this paper. This work\nwas partially supported by NSF under grant CTC-0208535, by ONR under grants N00014-00-1-\n03-41 and N00014-01-10-511, and by the DoD Multidisciplinary University Research Initiative\n(MURI) program administered by the ONR under grant N00014-01-1-0795.\nA\nProofs\nThe soundness and strong normalization (Theorem 3.2) of the calculus in Section 3 can be derived\nusing logical relations, in a fairly standard way [Winskel 1993]. In order to do this, we need some\nlemmas about properties of the operational semantics.\nLemma A.1. If ⊢e : τ and (s, e) −→(s′, e′), then ⊢e′ : τ.\nProof. This is a completely straightforward proof by induction on the height of the typing derivation\nfor ⊢e : τ.\n⊓⊔\nLemma A.2. If ⊢e : τ, (s, e) −→(s′, e′), and ⊢τ pure, then s′ = s.\nProof. This result follows essentially by examination of the operational semantics rules, proceeding\nby induction on the structure of e.\n- Case e = x: This case cannot arise, since ⊢e : τ cannot hold with an empty context when e\nis a variable.\n- Case e = v: An inspection of the operational semantics rules shows that this case cannot\narise, since there is no s′ and e′ such that (s, e) −→(s′, e′) if e is a value.\n- Case e = idc(): By (Red OCon), we have (s, e) −→(s, δc(idc)), and the state is unchanged,\nirrespectively of τ.\n- Case e = idp(e1, . . . , en): By examination of the operational semantics rules, two cases\narise. If every ei is a value vi, then (s, e) −→(s, δp(s, idp, v1, . . . , vn)), with τ = Bool and\n⊢τ pure, and the state is unchanged during the transition, as required. Otherwise, there is at\nleast one ei that is not a value, and (s, e) −→(s, idp(. . . , e′, . . . )) or (s, e) −→(s, ⋆). Again,\nτ = Bool, so that ⊢τ pure, and the state is unchanged during the transition, as required.\n19\n- Case e = ida(e1, . . . , en): If ⊢e : τ, then τ = Act, which is not a pure type, so there is\nnothing to show for this case.\n- Case e = e1 e2: By examination of the operational semantics rules, two cases arise. If e1 is a\nvalue, then it must be ⋆or an abstraction λx:τ ′.e′. In the former case, (s, e) −→(s, ⋆). In the\nlatter case, (s, e) −→(s, e′{x←e2}). In both cases, the state is unchanged, irrespectively of\nthe type τ. If e1 is not a value, then from rule (Red App 1), we get (s, e1 e2) −→(s, e′\n1 e2)\nor (s, e1 e2) −→(s, ⋆), and the state is unchanged, irrespectively of the type τ.\n- Case e = e1?e2:e3: By examination of the operational semantics rules, we consider two cases.\nIf e1 is a value, then it must be ⋆or a Boolean value. In the former case, (s, e) −→(s, ⋆).\nIn the latter case, (s, e) −→(s, e2) or (s, e) −→(s, e3), depending on whether e1 is true or\nfalse. In both cases, the state is unchanged, irrespectively of the type τ. If e1 is not a value,\nthen from rule (Red If 1), we get (s, e) −→(s, e′\n1?e2:e3) or (s, e) −→(s, ⋆), and the state is\nunchanged, irrespectively of the type τ.\n- Case e = e1; e2: If ⊢e : τ, then τ = Act, which is not a pure type, so there is nothing to\nshow for this case.\nThis completes the induction.\n⊓⊔\nWe deﬁne, for each type τ, a set Rτ of terms which terminate in all states. Formally, for a base\ntype b, either Obj, Bool, or Act, we take\nRb = {e |\n⊢e : t, ∀s∃v∃s′.(e, s) −→∗(v, s′)}.\nFor a function type τ1 →τ2, we take\nRτ1→τ2 = {e |\n⊢e : τ1 →τ2, ∀s∃v∃s′.(e, s) −→∗(v, s′), ∀e′ ∈Rτ1.(e e1) ∈Rτ2}.\nWe deﬁne a substitution operator γ to be a partial map from variables to expressions of the\naction calculus. Let dom(γ) be the domain of deﬁnition of the partial map γ. Given a context Γ,\nwe write γ |= Γ if the domains of γ and Γ are equal (a context Γ can be understood as a partial map\nfrom variables to types), and for all x ∈dom(γ), γ(x) ∈RΓ(x), where Γ(x) is the type associated\nwith x in the context Γ. We extend γ to expressions, by taking ˆγ(e) to be the expression resulting\nfrom replacing every variable x in e by the expression γ(x). Formally,\nˆγ(x) =\n(\nγ(x)\nif x ∈dom(γ)\nx\notherwise\nˆγ(true) = true\nˆγ(false) = false\nˆγ(λx:τ.e) = λx:τ.ˆγx(e)\nˆγ(skip) = skip\nˆγ(vo) = vo\nˆγ(⋆) = ⋆\nˆγ(id c()) = idc()\n20\nˆγ(idp(e1, . . . , en) = idp(ˆγ(e1), . . . , ˆγ(en))\nˆγ(ida(e1, . . . , en) = ida(ˆγ(e1), . . . , ˆγ(en))\nˆγ(e1 e2) = ˆγ(e1) ˆγ(e2)\nˆγ(e1?e2:e3) = ˆγ(e1)?ˆγ(e2):ˆγ(e3)\nˆγ(e1; e2) = ˆγ(e1); ˆγ(e2)\nwhere ˆγx is the same substitution map as γ, except that it is undeﬁned on variable x.\nLemma A.3. If Γ ⊢e : τ and γ |= Γ, then ⊢ˆγ(e) : τ.\nProof. This is a straightforward proof by induction on the height of the typing derivation for Γ ⊢e :\nτ.\n⊓⊔\nLemma A.4. If ⊢e : τ, and for all s there exists s′ and e′ ∈Rτ such that (s, e) −→∗(s′, e′), then\ne ∈Rτ.\nProof. We prove this by induction on the structure of τ. For a base type b, the result is immediate\nby the deﬁnition of Rb. For τ = τ1 →τ2, assume ⊢e : τ1 →τ2, and for all s, there exists the\nrequired s′, e′. For an arbitrary state s, let s′, e′ be such that (s, e) −→∗(s′, e′); since e′ ∈Rτ1→τ2,\nwe have (s′, e′) −→∗(s′′, v) for some s′′ and value v. Thus, (s, e) −→∗(s′′, v). Finally, it\nremains to show that for all e′′ ∈Rτ1, we have (e e′′) ∈Rτ2. By assumption, we have (e′ e′′) ∈\nRτ2. To apply the induction hypothesis and get (e e′) ∈Rτ2, we show that for all s, we have\n(s, e e′′) −→∗(s, e′ e′′). We proceed by induction on the length of the derivation (s, e) −→∗(s′, e′).\nFirst, note that because ⊢e : τ1 →τ2, which is a pure type, a straightforward induction on the\nlength of the derivation using Lemma A.2 shows that s′ = s. If the length is 0, then e = e′,\nso the result is immediate. If the length is non-zero, then (s, e) −→∗(s, e′′′) −→(s, e′). By\nthe induction hypothesis, (s, e e′′) −→∗(s, e′′′ e′′). Since (s, e′′′) −→(s, e′), by rule (Red App\n1), (s, e′′′ e′′) →(s, e′ e′′), so that (s, e e′′) −→∗(s, e′ e′′), as required. This establishes that\n(e e′′) ∈Rτ2.\n⊓⊔\nWe can now prove the main result.\nTheorem 3.2.\nIf ⊢e : τ, and s is a state, then there exists a state s′ and value v such that\n(s, e) −→∗(s′, v). Moreover, if ⊢τ pure, then s′ = s.\nProof. Clearly, it is sufﬁcient to show that ⊢e : τ implies e ∈Rτ. To use induction, we prove the\nmore general statement that Γ ⊢e : τ and γ |= Γ implies ˆγ(e) ∈Rτ. (The desired result follows\nby taking γ to be the empty substitution, and Γ the empty context.) We prove the general result by\ninduction on the structure of e.\n- Case e = x: Assume Γ ⊢x : τ, and γ |= Γ. We need to show that ˆγ(x) ∈Rτ. Since x\nis a variable, τ = Γ(x), and thus x is in the domain of Γ. Since γ |= Γ, γ(x) ∈Rτ, and\nˆγ(x) = γ(x) implies ˆγ(x) ∈Rτ, as required.\n- Case e = true, false, skip, ⋆, vo: Assume Γ ⊢e : b, for the appropriate base type b,\nand γ |= Γ. We need to show that ˆγ(e) = e ∈Rb. Since e is a value, than for all s,\n(s, e) −→∗(s, e), so e ∈Rb, as required.\n21\n- Case e = λx:τ.e′: This is the difﬁcult case. Assume that Γ ⊢λx:τ.e′ : τ →τ ′, and\nγ |= Γ. We need to show that ˆγ(λx:τ.e′) = λx:τ.ˆγ(e′) ∈Rτ→τ ′. This involves, following\nthe deﬁnition of Rτ→τ ′, establishing three facts. First, by Lemma A.3, ⊢ˆγ(λx:τ.e′). Since\nˆγ(λ:τ.e′) = λx:.ˆγ(e′) is a value, we immediately have that (s, λx:.ˆγ(e′)) reduces to a value\nfor all states s. Finally, we need to show that for all e′′ ∈Rτ, we have ((λx:.ˆγ(e′)) e′′) ∈Rτ ′.\nGiven e′′ ∈Rτ. By (Red App 3), for all s, (s, (λx:τ.ˆγ(e′)) e′′) −→(s, e′{x←e′′}). By\nLemma A.4, it sufﬁces to show that e′{x←e′′} ∈Rτ ′ to show that ((λx:.ˆγ(e′)) e′′) ∈Rτ ′\n(by taking s′ = s).\nDeﬁne γ′\nx = γx[x 7→e′′] = γ[x 7→e′′] (since γx is just γ expect undeﬁned on variable x).\nClearly, ˆγ(e′){x←e′′} = ˆγ′\nx(e′). By assumption, we have Γ ⊢λx:τ.e′ : τ →τ ′, which\nmeans that Γ, x : τ ⊢e′ : τ ′. Now, γ′\nx |= Γ, x : τ, since γ |= Γ and γ′\nx(x) = e′′ ∈Rτ, by\nassumption. Applying the induction hypothesis yields that ˆγ′\nx(e′) ∈Rτ ′, as required.\n- Case e = idc(): Assume Γ ⊢idc() : Obj, and γ |= Γ. We need to show that ˆγ(idc()) =\nidc() ∈RObj. For all s, (s, id c()) −→∗(s, δc(s, idc)) by (Red OCon), so idc() ∈RObj, as\nrequired.\n- Case e = idp(e1, . . . , en): Assume Γ ⊢idp(e1, . . . , en) : Bool, and γ |= Γ. We need to show\nthat ˆγ(idp(e1, . . . , en)) = idp(ˆγ(e1), . . . , ˆγ(en)) ∈RBool. Since Γ |= idp(e1, . . . , en) :\nBool, we have Γ |= ei : Obj for all i. Applying the induction hypothesis, we get that ei ∈\nRObj for all i, and thus for all s, we can construct a derivation (s, idp(ˆγ(e1), . . . , ˆγ(en))) −→∗\n(s, idp(v1, . . . , ˆγ(en))) −→∗. . . −→∗(s, idp(v1, . . . , vn)) −→(s, δp(s, v1, . . . , vn)) by re-\npeated applications of (Red PCon 1) and (Red PCon 2), and a ﬁnal application of (Red Pcon\n3). (Alternatively, a derivation that reduces to ⋆is also possible.) Therefore, idp(ˆγ(e1), . . . , ˆγ(en)) ∈\nRBool, as required.\n- Case e = ida(e1, . . . , en): This case is exactly like the case for idp(e1, . . . , en), replacing\nBool by Act where appropriate.\n- Case e = e1 e2: Assume Γ ⊢e1 e2 : τ, and γ |= Γ. We need to show that ˆγ(e1 e2) =\nˆγ(e1) ˆγ(e2) ∈Rτ. Since Γ ⊢e1 e2 : τ, we know that Γ ⊢e1 : τ ′ →τ, and Γ ⊢e2 : τ ′, for\nsome τ ′. Applying the induction hypothesis, we get ˆγ(e1) ∈Rτ ′→τ and ˆγ(e2) ∈Rτ ′. By the\ndeﬁnition of Rτ ′→τ, we get that ˆγ(e1) ˆγ(e2) ∈Rτ, as required.\n- Case e = e1?e2:e3: Assume Γ ⊢e1?e2:e3 : τ, and γ |= Γ.\nWe need to show that\nˆγ(e1?e2:e3) = ˆγ(e1)?ˆγ(e2):ˆγ(e3) ∈Rτ. Since Γ ⊢e1?e2:e3 : τ, we know that Γ ⊢e1 : Obj,\nΓ ⊢e2 : τ, and Γ ⊢e3 : τ, for some τ. Applying the induction hypothesis, we get ˆγ(e1) ∈\nRBool, ˆγ(e2) ∈Rτ, and ˆγ(e3) ∈Rτ. Therefore, for all s, we can construct either the deriva-\ntion (s, ˆγ(e1)?ˆγ(e2):ˆγ(e3)) −→∗(s, true?ˆγ(e2):ˆγ(e3)) −→(s, ˆγ(e2)) −→∗(s, v2) or the\nderivation (s, ˆγ(e1)?ˆγ(e2):ˆγ(e3)) −→∗(s, false?ˆγ(e2):ˆγ(e3)) −→(s, ˆγ(e3)) −→∗(s, v3),\nusing (Red If 1), (Red If 2), (Red If 3), depending on the Boolean value that (s, ˆγ(e1)) reduces\nto. (Alternatively, a derivation that reduces to ⋆is also possible.) Therefore, ˆγ(e1)?ˆγ(e2):ˆγ(e3) ∈\nRτ, as required.\n- Case e = e1; e2: Assume Γ ⊢e1; e2 : Act, and γ |= Γ. We need to show that ˆγ(e1; e2) =\nˆγ(e1); ˆγ(e2) ∈RAct. Since Γ ⊢e1; e2 : Act, we know that Γ ⊢e1 : Act and Γ ⊢e2 :\n22\nAct. Applying the induction hypothesis, we get ˆγ(e1) ∈RAct and ˆγ(e2) ∈RAct. There-\nfore, for all s, we can construct the derivation (s, ˆγ(e1); ˆγ(e2)) −→∗(s′, skip; ˆγ(e2)) −→\n(s′, ˆγ(e2)) −→∗(s′′, skip), by applications of (Red Seq 1), (Red Seq 2), (Red Seq 3). (Al-\nternatively, a derivation that reduces to ⋆is also possible.) Therefore, we have ˆγ(e1); ˆγ(e2) ∈\nRAct, as required.\nIf ⊢τ pure, a straightforward induction on the length of the derivation (s, e) −→∗(s′, v), via\nLemma A.2, establishes that s′ = s.\n⊓⊔\nTheorem 4.2. If α1 : A1, . . . , αn : An ⇒α : A has a derivation that respects imperative structure,\nthen the rule\nΓ ⊢α1 : T(A1)\n. . .\nΓ ⊢αn : T(An)\nΓ ⊢α : T(A)\nis an admissible typing rule.\nProof. We proceed by induction on the height of the derivation for α1 : A1, . . . , αn : An ⇒α : A.\nFirst, some notation: if ∆is a sequence α1 : A1, . . . , αk : Ak and Γ is a typing context, we write\nΓ[[∆]] for the sequence of judgments Γ ⊢α1 : T(A1), . . . , Γ ⊢αk : T(Ak). For the base case, we\nhave α : A ⇒α : A, and clearly, the typing rule\nΓ ⊢α : T(A)\nΓ ⊢α : T(A)\nis admissible. For the induction step, consider a number of cases, one for each possible last rule of\nthe derivation. In the case (Seq Cut), the last rule of the derivation is of the form\n∆⇒β : B\nΣ1, β : B, Σ2 ⇒α : A\nΣ1, ∆, Σ2 ⇒α : A.\nApplying the induction hypothesis, both\nΓ[[∆]]\nΓ ⊢β : T(B)\nand\nΓ[[Σ1]]\nΓ ⊢β : T(B)\nΓ[[Σ2]]\nΓ ⊢α : T(A)\nare admissible typing rules. Composing these two admissible rules yields the admissible rule:\nΓ[[Σ1]]\nΓ[[∆]]\nΓ ⊢β : T(B)\nΓ[[Σ2]]\nΓ ⊢α : T(A).\nIn the case (Seq App Right), the last rule of the derivation is of the form\n∆⇒β : B\nΣ1, γ(β) : C, Σ2 ⇒α : A\nΣ1, γ : C/B, ∆, Σ2 ⇒α : A\n23\nApplying the induction hypothesis, both\nΓ[[∆]]\nΓ ⊢β : T(B)\nand\nΓ[[Σ1]]\nΓ ⊢γ(β) : C\nΓ[[Σ2]]\nΓ ⊢α : T(A)\nare admissible typing rules. Composing them yields the following admissible rule, upon noting that\nT(C/B) = T(B) →T(C):\nΓ[[Σ1]]\nΓ ⊢γ : T(B) →T(C)\nΓ[[∆]]\nΓ ⊢β : T(B)\nΓ ⊢γ(β) : T(C)\nΓ[[Σ2]]\nΓ ⊢α : T(A).\nThe case for (Seq App Left) is similar.\nFinally, in the case (Seq Abs Right), where we have α = λx.β and A = B/C, the last rule of\nthe derivation is of the form\nΣ, x : C ⇒β : B\nΣ ⇒λx.β : B/C\nApplying the induction hypothesis, for Γ of the form Γ′, x : T(C), the typing rule\n(Γ′, x : T(C))[[Σ]]\nΓ′, x : T(C) ⊢x : T(C)\nΓ′, x : T(A) ⊢β : T(B)\nis admissible. Noting that T(B/C) = T(C) →T(B), in order to derive an admissible typing\nrule using (Typ Fun), we need to check that ⊢T(C) →T(B) ok. But this is exactly what the\nassumption that the derivation respects imperative structure gives us. We can therefore derive the\nfollowing admissible typing rule:\n(Γ′, x : T(C))[[Σ]]\nΓ′, x : T(C) ⊢x : T(C)\nΓ′, x : T(C) ⊢β : T(B)\n⊢T(C) →T(B) ok\nΓ′ ⊢λx:T(C).β : T(C) →T(B).\nThe case for (Seq Abs Left) is similar.\n⊓⊔\nReferences\nAjdukiewicz, K. (1935). Die syntaktische Konnexit¨at. Studia Philosophica 1, 1–27.\nAndrews, P. B. (1986). An Introduction to Mathematical Logic and Type Theory: To Truth\nthrough Proof. Academic Press.\nAndroutsopoulos, I., G. Ritchie, and P. Thanisch (1995). Natural language interfaces to\ndatabases—an introduction. Journal of Language Engineering 1(1), 29–81.\n24\nBar-Hillel, Y. (1953). A quasi-arithmetical notation for syntactic description. Language 29, 47–\n58.\nBarclay, T., J. Gray, E. Strand, S. Ekblad, and J. Richter (2002, June). TerraService.NET: An\nintroduction to web services. Technical Report MS–TR–2002–53, Microsoft Research.\nBarendregt, H. P. (1981). The Lambda Calculus, Its Syntax and Semantics. Studies in Logic.\nNorth-Holland.\nBenthem, J. van (1986). The semantics of variety in categorial grammar. In W. Buszkowski,\nJ. van Benthem, and W. Marciszewski (Eds.), Categorial Grammar, Number 25 in Linguis-\ntics and Literary Studies in Eastern Europe, pp. 37–55. John Benjamins. Previously appeared\nas Report 83-29, Department of Mathematics, Simon Fraser University (1983).\nCarpenter, B. (1997). Type-Logical Semantics. MIT Press.\nGroenendijk, J. and M. Stokhof (1991). Dynamic predicate logic. Linguistics and Philoso-\nphy 14(1), 39–100.\nHarel, D., D. Kozen, and J. Tiuryn (2000). Dynamic Logic. MIT Press.\nLambek, J. (1958). The mathematics of sentence structure. The American Mathematical\nMonthly 65, 154–170.\nMoggi, E. (1989). Computational lambda-calculus and monads. In Proc. 4th Annual IEEE Sym-\nposium on Logic in Computer Science (LICS’89), pp. 14–23. IEEE Computer Society Press.\nMoortgat, M. (1997). Categorial type logics. In J. van Benthem and A. ter Meulen (Eds.), Hand-\nbook of Logic and Language, Chapter 2, pp. 93–177. The MIT Press / Elsevier.\nPrice, D., E. Rilofff, J. L. Zachary, and B. Harvey (2000). NaturalJava: a natural language inter-\nface for programming in Java. In Intelligent User Interfaces, pp. 207–211.\nWinograd, T. (1971). Procedures as a representation for data in a computer program for under-\nstanding natural languages. Project MAC technical report MAC-TR-84, MIT.\nWinskel, G. (1993). The Formal Semantics of Programming Languages. MIT Press.\n25\n",
  "categories": [
    "cs.CL",
    "cs.HC",
    "H.5.2; I.2.7; F.4.2; F.4.1"
  ],
  "published": "2004-12-17",
  "updated": "2004-12-17"
}