{
  "id": "http://arxiv.org/abs/cs/0504063v1",
  "title": "Selection in Scale-Free Small World",
  "authors": [
    "Zs. Palotai",
    "Cs. Farkas",
    "A. Lorincz"
  ],
  "abstract": "In this paper we compare the performance characteristics of our selection\nbased learning algorithm for Web crawlers with the characteristics of the\nreinforcement learning algorithm. The task of the crawlers is to find new\ninformation on the Web. The selection algorithm, called weblog update, modifies\nthe starting URL lists of our crawlers based on the found URLs containing new\ninformation. The reinforcement learning algorithm modifies the URL orderings of\nthe crawlers based on the received reinforcements for submitted documents. We\nperformed simulations based on data collected from the Web. The collected\nportion of the Web is typical and exhibits scale-free small world (SFSW)\nstructure. We have found that on this SFSW, the weblog update algorithm\nperforms better than the reinforcement learning algorithm. It finds the new\ninformation faster than the reinforcement learning algorithm and has better new\ninformation/all submitted documents ratio. We believe that the advantages of\nthe selection algorithm over reinforcement learning algorithm is due to the\nsmall world property of the Web.",
  "text": "arXiv:cs/0504063v1  [cs.LG]  14 Apr 2005\nSelection in Scale-Free Small World\nZs. Palotai1, Cs. Farkas2, A. L˝orincz1∗\n1 Eotvos Lorand University, Department of Information Systems, Pazmany Peter se-\ntany 1/c, Budapest, H1117, Hungary\n2 University of South Carolina, Department of Computer Sciences and Engineering,\nColumbia, SC 29208, USA\nAbstract\nIn this paper we compare the performance characteristics of our selec-\ntion based learning algorithm for Web crawlers with the characteristics\nof the reinforcement learning algorithm. The task of the crawlers is to\nﬁnd new information on the Web. The selection algorithm, called weblog\nupdate, modiﬁes the starting URL lists of our crawlers based on the found\nURLs containing new information. The reinforcement learning algorithm\nmodiﬁes the URL orderings of the crawlers based on the received rein-\nforcements for submitted documents. We performed simulations based on\ndata collected from the Web. The collected portion of the Web is typical\nand exhibits scale-free small world (SFSW) structure. We have found that\non this SFSW, the weblog update algorithm performs better than the re-\ninforcement learning algorithm. It ﬁnds the new information faster than\nthe reinforcement learning algorithm and has better new information/all\nsubmitted documents ratio. We believe that the advantages of the selec-\ntion algorithm over reinforcement learning algorithm is due to the small\nworld property of the Web.\n1\nIntroduction\nThe largest source of information today is the World Wide Web.\nThe estimated\nnumber of documents nears 10 billion. Similarly, the number of documents changing\non a daily basis is also enormous. The ever-increasing growth of the Web presents a\nconsiderable challenge in ﬁnding novel information on the Web.\nIn addition, properties of the Web, like scale-free small world (SFSW) structure\n[1, 12] may create additional challenges. For example the direct consequence of the\nscale-free small world property is that there are numerous URLs or sets of interlinked\nURLs, which have a large number of incoming links. Intelligent web crawlers can be\neasily trapped at the neighborhood of such junctions as it has been shown previously\n[13, 15].\nWe have developed a novel artiﬁcial life (A-life) method with intelligent individ-\nuals, crawlers, to detect new information on a news Web site. We deﬁne A-life as\n∗Corresponding author. email: lorincz@inf.elte.hu\n1\na population of individuals having both static structural properties, and structural\nproperties which may undergo continuous changes, i.e., adaptation. Our algorithms\nare based on methods developed for diﬀerent areas of artiﬁcial intelligence, such as\nevolutionary computing, artiﬁcial neural networks and reinforcement learning. All ef-\nforts were made to keep the applied algorithms as simple as possible subject to the\nconstraints of the internet search.\nEvolutionary computing deals with properties that may be modiﬁed during the cre-\nation of new individuals, called ’multiplication’. Descendants may exhibit variations\nof population, and diﬀer in performance from the others. Individuals may also termi-\nnate. Multiplication and selection is subject to the ﬁtness of individuals, where ﬁtness\nis typically deﬁned by the modeler. For a recent review on evolutionary computing, see\n[7]. For reviews on related evolutionary theories and the dynamics of self-modifying\nsystems see [8, 4] and [11, 5], respectively. Similar concepts have been studied in other\nevolutionary systems where organisms compete for space and resources and cooperate\nthrough direct interaction (see, e.g., [19] and references therein.)\nSelection, however, is a very slow process and individual adaptation may be neces-\nsary in environments subject to quick changes. The typical form of adaptive learning\nis the connectionist architecture, such as artiﬁcial neural networks. Multilayer percep-\ntrons (MLPs), which are universal function approximators have been used widely in\ndiverse applications. Evolutionary selection of adapting MLPs has been in the focus\nof extensive research [32, 33].\nIn a typical reinforcement learning (RL) problem the learning process [27] is mo-\ntivated by the expected value of long-term cumulated proﬁt. A well-known example\nof reinforcement learning is the TD-Gammon program of Tesauro [29]. The author\napplied MLP function approximators for value estimation. Reinforcement learning has\nalso been used in concurrent multi-robot learning, where robots had to learn to forage\ntogether via direct interaction [16]. Evolutionary learning has been used within the\nframework of reinforcement learning to improve decision making, i.e., the state-action\nmapping called policy [25, 18, 30, 14].\nIn this paper we present a selection based algorithm and compare it to the well-\nknown reinforcement learning algorithm in terms of their eﬃciency and behavior. In\nour problem, ﬁtness is not determined by us, but ﬁtness is implicit. Fitness is jointly\ndetermined by the ever changing external world and by the competing individuals\ntogether. Selection and multiplication of individuals are based on their ﬁtness value.\nCommunication and competition among our crawlers are indirect. Only the ﬁrst sub-\nmitter of a document may receive positive reinforcement. Our work is diﬀerent from\nother studies using combinations of genetic, evolutionary, function approximation, and\nreinforcement learning algorithms, in that i) it does not require explicit ﬁtness func-\ntion, ii) we do not have control over the environment, iii) collaborating individuals\nuse value estimation under ‘evolutionary pressure’, and iv) individuals work without\ndirect interaction with each other.\nWe performed realistic simulations based on data collected during an 18 days long\ncrawl on the Web. We have found that our selection based weblog update algorithm\nperforms better in scale-free small world environment than the RL algorithm, even-\nthough the reinforcement learning algorithm has been shown to be eﬃcient in ﬁnding\nrelevant information [15, 21]. We explain our results based on the diﬀerent behaviors of\nthe algorithms. That is, the weblog update algorithm ﬁnds the good relevant document\nsources and remains at these regions until better places are found by chance. Individu-\nals using this selection algorithm are able to quickly collect the new relevant documents\nfrom the already known places because they monitor these places continuously. The\n2\nreinforcement learning algorithm explores new territories for relevant documents and\nif it ﬁnds a good place then it collects the existing relevant documents from there. The\ncontinuous exploration of RL causes that it ﬁnds relevant documents slower than the\nweblog update algorithm. Also, crawlers using weblog update algorithm submit more\ndiﬀerent documents than crawlers using the RL algorithm. Therefore there are more\nrelevant new information among documents submitted by former than latter crawlers.\nThe paper is organized as follows.\nIn Section 2 we review recent works in the\nﬁeld of Web crawling. Then we describe our algorithms and the forager architecture\nin Section 3.\nAfter that in Section 4 we present our experiment on the Web and\nthe conducted simulations with the results. In Section 5 we discuss our results on\nthe found diﬀerent behaviors of the selection and reinforcement learning algorithms.\nSection 6 concludes our paper.\n2\nRelated work\nOur work concerns a realistic Web environment and search algorithms over this envi-\nronment. We compare selective/evolutionary and reinforcement learning methods. It\nseems to us that such studies should be conducted in ever changing, buzzling, wabbling\nenvironments, which justiﬁes our choice of the environment. We shall review several of\nthe known search tools including those [13, 15] that our work is based upon. Readers\nfamiliar with search tools utilized on the Web may wish to skip this section.\nThere are three main problems that have been studied in the context of crawlers.\nRungsawang et al.\n[23] and references therein and Menczer [17] studied the topic\nspeciﬁc crawlers.\nRisvik et al.\n[22] and references therein address research issues\nrelated to the exponential growth of the Web. Cho and Gracia-Molina [3], Menczer\n[17] and Edwards et. al [6] and references therein studies the problem of diﬀerent\nrefresh rates of URLs (possibly as high as hourly or as low as yearly).\nRungsawang and Angkawattanawit [23] provide an introduction to and a broad\noverview of topic speciﬁc crawlers (see citations in the paper). They propose to learn\nstarting URLs, topic keywords and URL ordering through consecutive crawling at-\ntempts. They show that the learning of starting URLs and the use of consecutive\ncrawling attempts can increase the eﬃciency of the crawlers. The used heuristic is\nsimilar to the weblog algorithm [9], which also ﬁnds good starting URLs and period-\nically restarts the crawling from the newly learned ones. The main limitation of this\nwork is that it is incapable of addressing the freshness (i.e., modiﬁcation) of already\nvisited Web pages.\nMenczer [17] describes some disadvantages of current Web search engines on the\ndynamic Web, e.g., the low ratio of fresh or relevant documents.\nHe proposes to\ncomplement the search engines with intelligent crawlers, or web mining agents to\novercome those disadvantages. Search engines take static snapshots of the Web with\nrelatively large time intervals between two snapshots. Intelligent web mining agents\nare diﬀerent: they can ﬁnd online the required recent information and may evolve\nintelligent behavior by exploiting the Web linkage and textual information.\nHe introduces the InfoSpider architecture that uses genetic algorithm and reinforce-\nment learning, also describes the MySpider implementation of it. Menczer discusses\nthe diﬃculties of evaluating online query driven crawler agents. The main problem\nis that the whole set of relevant documents for any given query are unknown, only a\nsubset of the relevant documents may be known. To solve this problem he introduces\ntwo new metrics that estimate the real recall and precision based on an available sub-\n3\nset of the relevant documents. With these metrics search engine and online crawler\nperformances can be compared. Starting the MySpider agent from the 100 top pages\nof AltaVista the agent’s precision is better than AltaVista’s precision even during the\nﬁrst few steps of the agent.\nThe fact that the MySpider agent ﬁnds relevant pages in the ﬁrst few steps may\nmake it deployable on users’ computers. Some problems may arise from this kind of\nagent usage. First of all there are security issues, like which ﬁles or information sources\nare allowed to read and write for the agent. The run time of the agents should be\ncontrolled carefully because there can be many users (Google answered more than 100\nmillion searches per day in January-February 2001) using these agents, thus creating\nhuge traﬃc overhead on the Internet.\nOur weblog algorithm uses local selection for ﬁnding good starting URLs for\nsearches, thus not depending on any search engines.\nDependence on a search en-\ngine can be a suﬀer limitation of most existing search agents, like MySpiders. Note\nhowever, that it is an easy matter to combine the present algorithm with URLs oﬀered\nby search engines. Also our algorithm should not run on individual users’s computers.\nRather it should run for diﬀerent topics near to the source of the documents in the\ngiven topic – e.g., may run at the actual site where relevant information is stored.\nRisvik and Michelsen [22] mention that because of the exponential growth of the\nWeb there is an ever increasing need for more intelligent, (topic-)speciﬁc algorithms\nfor crawling, like focused crawling and document classiﬁcation. With these algorithms\ncrawlers and search engines can operate more eﬃciently in a topically limited document\nspace. The authors also state that in such vertical regions the dynamics of the Web\npages is more homogenous.\nThey overview diﬀerent dimensions of web dynamics and show the arising problems\nin a search engine model. They show that the problem of rapid growth of Web and\nfrequent document updates creates new challenges for developing more and more eﬃ-\ncient Web search engines. The authors deﬁne a reference search engine model having\nthree main components: (1) crawler, (2) indexer, (3) searcher. The main part of the\npaper focuses on the problems that crawlers need to overcome on the dynamic Web.\nAs a possible solution the authors propose a heterogenous crawling architecture. They\nalso present an extensible indexer and searcher architecture. The crawling architec-\nture has a central distributor that knows which crawler has to crawl which part of the\nweb. Special crawlers with low storage and high processing capacity are dedicated to\nweb regions where content changes rapidly (like news sites). These crawlers maintain\nup-to-date information on these rapidly changing Web pages.\nThe main limitation of their crawling architecture is that they must divide the web\nto be crawled into distinct portions manually before the crawling starts. A weblog like\ndistributed algorithm – as suggested here – my be used in that architecture to overcome\nthis limitation.\nCho and Garcia-Molina [3] deﬁne mathematically the freshness and age of doc-\numents of search engines.\nThey propose the Poisson process as a model for page\nrefreshment. The authors also propose various refresh policies and study their eﬀec-\ntiveness both theoretically and on real data. They present the optimal refresh policies\nfor their freshness and age metrics under the Poisson page refresh model. The authors\nshow that these policies are superior to others on real data, too.\nThey collected about 720000 documents from 270 sites. Although they show that\nin their database more than 20 percent of the documents are changed each day, they\ndisclosed these documents from their studies. Their crawler visited the documents\nonce each day for 5 months, thus can not measure the exact change rate of those\n4\ndocuments. While in our work we deﬁnitely concentrate on these frequently changing\ndocuments.\nThe proposed refresh policies require good estimation of the refresh rate for each\ndocument. The estimation inﬂuences the revisit frequency while the revisit frequency\ninﬂuences the estimation. Our algorithm does not need explicit frequency estimations.\nThe more valuable URLs (e.g., more frequently changing) will be visited more often\nand if a crawler does not ﬁnd valuable information around an URL being in it’s\nweblog then that URL ﬁnally will fall out from the weblog of the crawler. However\nfrequency estimations and refresh policies can be easily integrated into the weblog\nalgorithm selecting the starting URL from the weblog according to the refresh policy\nand weighting each URL in the weblog according to their change frequency estimations.\nMenczer [17] also introduces a recency metric which is 1 if all of the documents\nare recent (i.e., not changed after the last download) and goes to 0 as downloaded\ndocuments are getting more and more obsolete.\nTrivially immediately after a few\nminutes run of an online crawler the value of this metric will be 1, while the value for\nthe search engine will be lower.\nEdwards et al. [6] present a mathematical crawler model in which the number of\nobsolete pages can be minimized with a nonlinear equation system. They solved the\nnonlinear equations with diﬀerent parameter settings on realistic model data. Their\nmodel uses diﬀerent buckets for documents having diﬀerent change rates therefore does\nnot need any theoretical model about the change rate of pages. The main limitations\nof this work are the following:\n• by solving the nonlinear equations the content of web pages can not be taken into\nconsideration. The model can not be extended easily to (topic-)speciﬁc crawlers,\nwhich would be highly advantageous on the exponentially growing web [23], [22],\n[17].\n• the rapidly changing documents (like on news sites) are not considered to be in\nany bucket, therefore increasingly important parts of the web are disclosed from\nthe searches.\nHowever the main conclusion of the paper is that there may exist some eﬃcient\nstrategy for incremental crawlers for reducing the number of obsolete pages without\nthe need for any theoretical model about the change rate of pages.\n3\nForager architecture\nThere are two diﬀerent kinds of agents: the foragers and the reinforcing agent (RA).\nThe ﬂeet of foragers crawl the web and send the URLs of the selected documents to\nthe reinforcing agent. The RA determines which forager should work for the RA and\nhow long a forager should work. The RA sends reinforcements to the foragers based\non the received URLs.\nWe employ a ﬂeet of foragers to study the competition among individual foragers.\nThe ﬂeet of foragers allows to distribute the load of the searching task among diﬀerent\ncomputers. A forager has simple, limited capabilities, like limited number of starting\nURLs and a simple, content based URL ordering. The foragers compete with each\nother for ﬁnding the most relevant documents. In this way they eﬃciently and quickly\ncollect new relevant documents without direct interaction.\nAt ﬁrst the basic algorithms are presented. After that the reinforcing agent and\nthe foragers are detailed.\n5\n3.1\nAlgorithms\n3.1.1\nWeblog algorithm and starting URL selection\nA forager periodically restarts from a URL randomly selected from the list of starting\nURLs. The sequence of visited URLs between two restarts forms a path. The starting\nURL list is formed from the ST ART SIZE = 10 ﬁrst URLs of the weblog. In the\nweblog there are W EBLOG SIZE = 100 URLs with their associated weblog values\nin descending order.\nThe weblog value of a URL estimates the expected sum of\nrewards during a path after visiting that URL. The weblog update algorithm modiﬁes\nthe weblog before a new path is started (Algorithm 1). The weblog value of a URL\nalready in the weblog is modiﬁed toward the sum of rewards in the remaining part of\nthe path after that URL. A new URL has the value of actual sum of rewards in the\nremaining part of the path. If a URL has a high weblog value it means that around\nthat URL there are many relevant documents. Therefore it may worth it to start a\nsearch from that URL.\nAlgorithm 1 Weblog Update. β was set to 0.3\ninput\nvisitedURLs ←the steps of the given path\nvalues ←the sum of rewards for each step in the given path\noutput\nstarting URL list\nmethod\ncumV alues ←cumulated sum of values in reverse order\nnewURLs ←visitedURLs not having value in weblog\nrevisitedURLs ←visitedURLs having value in weblog\nfor each URL ∈newURLs\nweblog(URL) ←cumV alues(URL)\nendfor\nfor each URL ∈revisitedURLs\nweblog(URL) ←(1 −β) weblog(URL) +\nβ cumV alues(URL)\nendfor\nweblog ←descending order of values in weblog\nweblog ←truncate weblog after the WEBLOG SIZEth\nelement\nstarting URL list ←ﬁrst ST ART SIZE elements of weblog\nWithout the weblog algorithm the weblog and thus the starting URL list remains\nthe same throughout the searches. The weblog algorithm is a very simple version of\nevolutionary algorithms. Here, evolution may occur at two diﬀerent levels: the list of\nURLs of the forager is evolving by the reordering of the weblog. Also, a forager may\nmultiply, and its weblog, or part of it may spread through inheritance. This way, the\nweblog algorithm incorporates most basic features of evolutionary algorithms. This\n6\nsimple form shall be satisfactory to demonstrate our statements.\n3.1.2\nReinforcement Learning and URL ordering\nA forager can modify its URL ordering based on the received reinforcements of the sent\nURLs. The (immediate) proﬁt is the diﬀerence of received rewards and penalties at any\ngiven step. Immediate proﬁt is a myopic characterization of a step to a URL. Foragers\nhave an adaptive continuous value estimator and follow the policy that maximizes the\nexpected long term cumulated proﬁt (LTP) instead of the immediate proﬁt.\nSuch\nestimators can be easily realized in neural systems [27, 28, 24].\nPolicy and proﬁt\nestimation are interlinked concepts: proﬁt estimation determines the policy, whereas\npolicy inﬂuences choices and, in turn, the expected LTP. (For a review, see [27].)\nHere, choices are based on the greedy LTP policy: The forager visits the URL, which\nbelongs to the frontier (the list of linked but not yet visited URLs, see later) and has\nthe highest estimated LTP.\nIn the particular simulation each forager has a k(= 50) dimensional probabilistic\nterm-frequency inverse document-frequency (PrTFIDF) text classiﬁer [10], generated\non a previously downloaded portion of the Geocities database.\nFifty clusters were\ncreated by Boley’s clustering algorithm [2] from the downloaded documents.\nThe\nPrTFIDF classiﬁers were trained on these clusters plus an additional one, the (k+1)th,\nrepresenting general texts from the internet. The PrTFIDF outputs were non-linearly\nmapped to the interval [-1,+1] by a hyperbolic-tangent function. The classiﬁer was\napplied to reduce the texts to a small dimensional representation. The output vector\nof the classiﬁer for the page of URL A is state(A) = (state(A)1, . . . , state(A)k). (The\n(k+1)th output was dismissed.) This output vector is stored for each URL (Algorithm\n2).\nAlgorithm 2 Page Information Storage\ninput\npageURLs ←URLs of pages to be stored\noutput\nstate ←the classiﬁer output vectors for pages of pageURLs\nmethod\nfor each URL ∈pageURLs\npage ←text of page of URL\nstate(URL) ←classiﬁer output vector for page\nendfor\nA linear function approximator is used for LTP estimation. It encompasses k pa-\nrameters, the weight vector weight = (weight1, . . . , weightk). The LTP of document\nof URL A is estimated as the scalar product of state(A) and weight: value(A) =\nPk\ni=1 weighti state(A)i. During URL ordering the URL with highest LTP estimation\nis selected. The URL ordering algorithm is shown in Algorithm 3.\nThe weight vector of each forager is tuned by Temporal Diﬀerence Learning [26,\n28, 24]. Let us denote the current URL by URLn, the next URL to be visited by\n7\nAlgorithm 3 URL Ordering\ninput\nfrontier ←the set of available URLs\nstate ←the stored vector representation of the URLs\noutput\nbestURL ←URL with maximum LTP value\nmethod\nfor each URL ∈frontier\nvalue(URL) ←Pk\ni=1 state(URL)i weighti\nendfor\nbestURL ←URL with maximal LTP value\nURLn+1, the output of the classiﬁer for URLj by state(URLj) and the estimated\nLTP of a URL URLj by value(URLj) = Pk\ni=1 wegihti state(URLj)i. Assume that\nleaving URLn to URLn+1 the immediate proﬁt is rn+1. Our estimation is perfect if\nvalue(URLn) = value(URLn+1) + rn+1. Future proﬁts are typically discounted in\nsuch estimations as value(URLn) = γvalue(URLn+1) + rn+1, where 0 < γ < 1. The\nerror of value estimation is\nδ(n, n + 1) = rn+1 + γvalue(URLn+1) −value(URLn).\nWe used throughout the simulations γ = 0.9. For each step URLn →URLn+1 the\nweights of the value function were tuned to decrease the error of value estimation\nbased on the received immediate proﬁt rn+1. The δ(n, n + 1) estimation error was\nused to correct the parameters. The ith component of the weight vector, weighti, was\ncorrected by\n∆weighti = α δ(n, n + 1) state(URLn)i\nwith α = 0.1 and i = 1, . . . , k. These modiﬁed weights in a stationary environment\nwould improve value estimation (see, e.g, [27] and references therein).\nThe URL\nordering update is given in Algorithm 4.\nWithout the update algorithm the weight vector remains the same throughout the\nsearch.\n3.1.3\nDocument relevancy\nA document or page is possibly relevant for a forager if it is not older than 24 hours\nand the forager has not marked it previously. Algorithm 5 shows the procedure of\nselecting such documents.\nThe selected documents are sent to the RA for further\nevaluation.\n3.1.4\nMultiplication of a forager\nDuring multiplication the weblog is randomly divided into two equal sized parts (one\nfor the original and one for the new forager). The parameters of the URL ordering\n8\nAlgorithm 4 URL Ordering Update\ninput\nURLn+1 ←the step for which the reinforcement is received\nURLn ←the previous step before URLn+1\nrn+1 ←reinforcement for visiting URLn+1\noutput\nweight ←the updated weight vector\nmethod\nδ(n, n + 1) ←rn+1 + γvalue(URLn+1) −value(URLn)\nweight ←weight + α δ(n, n + 1) state(URLn)\nAlgorithm 5 Document Relevancy at a forager\ninput\npages ←the pages to be examined\noutput\nrelevantPages ←the selected pages\nmethod\npreviousPages ←previously selected relevant pages\nrelevantPages ←all pages from pages which are\nnot older than 24 hours and\nnot contained in previousPages\npreviousPages ←add relevantPages to previousPages\nalgorithm (the weight vector of the value estimation) are either copied or new random\nparameters are generated. If the forager has a URL ordering update algorithm then\nthe parameters are copied. If the forager does not have any URL ordering update\nalgorithm then new random parameters are generated, as shown in Algorithm 6.\n3.2\nReinforcing agent\nA reinforcing agent controls the ”life” of foragers. It can start, stop, multiply or delete\nforagers. RA receives the URLs of documents selected by the foragers, and responds\nwith reinforcements for the received URLs. The response is REW ARD = 100 (a.u.)\nfor a relevant document and PENALT Y = −1 (a.u.) for a not relevant document. A\ndocument is relevant if it is not yet seen by the reinforcing agent and it is not older\nthan 24 hours. The reinforcing agent maintains the score of each forager working for\nit. Initially each forager has INIT SCORE = 100 score. When a forager sends a\nURL to the RA, the forager’s score is decreased by SCORE−= 0.05. After each\nrelevant page sent by the forager, the forager’s score is increased by SCORE+ = 1\n(Algorithm 7).\n9\nAlgorithm 6 Multiplication\ninput\nweblog\nweight vector of URL ordering\noutput\nnewWeblog\nnewWeight\nmethod\nnewWeblog ←WEBLOG SIZE/2 randomly selected\nURLs and values from weblog\nweblog ←delete newWeblog from weblog\nif forager has URL ordering update algorithm\nnewWeight ←copy the weight vector of URL ordering\nelse\nnewWeight ←generate a new random weight vector\nendif\nWhen the forager’s score reaches MAX SCORE = 200 and the number of foragers\nis smaller than MAX FORAGER = 16 then the forager is multiplied. That is a new\nforager is created with the same algorithms as the original one has, but with slightly\ndiﬀerent parameters. When the forager’s score goes below MIN SCORE = 0 and the\nnumber of foragers is larger than MIN FORAGER = 2 then the forager is deleted\n(Algorithm 8). Note that a forager can be multiplied or deleted immediately after it\nhas been stopped by the RA and before the next forager is activated.\nForagers on the same computer are working in time slices one after each other.\nEach forager works for some amount of time determined by the RA. Then the RA\nstops that forager and starts the next one selected by the RA. The pseudo-code of the\nreinforcing agent is given in Algorithm 9.\n3.3\nForagers\nA forager is initialized with parameters deﬁning the URL ordering, and either with\na weblog or with a seed of URLs (Algorithm 10). After its initialization a forager\ncrawls in search paths, that is after a given number of steps the search restarts and\nthe steps between two restarts form a path.\nDuring each path the forager takes\nMAX ST EP = 100 number of steps, i.e., selects the next URL to be visited with\na URL ordering algorithm. At the beginning of a path a URL is selected randomly\nfrom the starting URL list. This list is formed from the 10 ﬁrst URLs of the weblog.\nThe weblog contains the possibly good starting URLs with their associated weblog\nvalues in descending order. The weblog algorithm modiﬁes the weblog and so thus the\nstarting URL list before a new path is started. When a forager is restarted by the RA,\nafter the RA has stopped it, the forager continues from the internal state in which it\nwas stopped. The pseudo code of step selection is given in Algorithm 11.\nThe URL ordering algorithm selects a URL to be the next step from the frontier\n10\nAlgorithm 7 Manage Received URL\ninput\nURL, forager ←received URL from forager\noutput\nreinforcement to forager\nupdated forager score\nmethod\nrelevants ←relevant pages seen by the RA\npage ←get page of URL\ndecrease forager’s score with SCORE−\nif page ∈relevants or page date is older than 24 hours\nsend PENALT Y to forager\nelse\nrelevants ←add page to relevants\nsend REWARD to forager\nincrease forager’s score with SCORE+\nendif\nURL set. The selected URL is removed from the frontier and added to the visited\nURL set to avoid loops. After downloading the pages, only those URLs (linked from\nthe visited URL) are added to the frontier which are not in the visited set.\nIn each step the forager downloads the page of the selected URL and all of the\npages linked from the page of selected URL. It sends the URLs of the possibly relevant\npages to the reinforcing agent. The forager receives reinforcements on any previously\nsent but not yet reinforced URLs and calls the URL ordering update algorithm with\nthe received reinforcements. The pseudo code of a forager is shown in Algorithm 12.\n11\nAlgorithm 8 : Manage Forager\ninput\nforager ←the forager to be multiplied or deleted\noutput\npossibly modiﬁed list of foragers\nmethod\nif (forager’s score ≥MAX SCORE and\nnumber of foragers < MAX FORAGER)\nweblog, URLordering ←call forager’s\nMultiplication, Alg. 6\nforager may modify it’s own weblog\nnewForager ←create a new forager with the received\nweblog and URLordering\nset the two foragers’ score to INIT SCORE\nelse if (forager’s score ≤MIN SCORE and\nnumber of foragers > MIN FORAGER)\ndelete forager\nendif\nAlgorithm 9 : Reinforcing Agent\ninput\nseed URLs\noutput\nrelevants ←found relevant documents\nmethod\nrelevants ←empty set /*set of all observed relevant pages\ninitialize MIN FORAGER foragers with the seed URLs\nset one of them to be the next\nrepeat\nstart next forager\nreceive possibly relevant URL\ncall Manage Received URL, Alg. 7 with URL\nstop forager if its time period is over\ncall Manage Forager, Alg. 8 with this forager\nchoose next forager\nuntil time is over\n12\nAlgorithm 10 Initialization of the forager\ninput\nweblog or seed URLs\nURL ordering parameters\noutput\ninitialized forager\nmethod\nset path step number to MAX ST EP + 1 /*start new path\nset the weblog\neither with the input weblog\nor put the seed URLs into the weblog with 0 weblog value\nset the URL ordering parameters in URL ordering algorithm\nAlgorithm 11 URL Selection\ninput\nfrontier ←set of URLs available in this step\nvisited ←set of visited URLs in this path\noutput\nstep ←selected URL to be visited next\nmethod\nif path step number ≤MAX ST EP\nstep ←selected URL by URL Ordering, Alg. 3\nincrease path step number\nelse\ncall the Weblog Update, Alg. 1 to update the weblog\nstep ←select a random URL from the starting URL list\nset path step number to 1\nfrontier ←empty set\nvisited ←empty set\nendif\n13\nAlgorithm 12 Forager\ninput\nfrontier ←set of URLs available in the next step\nvisited ←set of visited URLs in the current path\noutput\nsent documents to the RA\nmodiﬁed frontier and visited\nmodiﬁed weblog and URL ordering weight vector\nmethod\nrepeat\nstep ←call URL Selection, Alg. 11\nfrontier ←remove step from frontier\nvisited ←add step to visited\npage ←download the page of step\nlinkedURLs ←links of page\nnewURLs ←linkedURLs which are not visited\nfrontier ←add newURLs to frontier\ndownload pages of linkedURLs\ncall Page Information Storage, Alg. 2 with newURLs\nrelevantPages ←call Document Relevancy, Alg. 5 for\nall pages\nsend relevantPages to reinforcing agent\nreceive reinforcements for sent but not yet reinforced pages\ncall URL Ordering Update, Alg. 4 with\nthe received reinforcements\nuntil time is over\n14\n4\nExperiments\nWe conducted an 18 day long experiment on the Web to gather realistic data. We used\nthe gathered data in simulations to compare the weblog update (Section 3.1.1) and\nreinforcement learning algorithms (Section 3.1.2). In Web experiment we used a ﬂeet\nof foragers using combination of reinforcement learning and weblog update algorithms\nto eliminate any biases on the gathered data. First we describe the experiment on the\nWeb then the simulations. We analyze our results at the end of this section.\n4.1\nWeb\nWe ran the experiment on the Web on a single personal computer with Celeron 1000\nMHz processor and 512 MB RAM. We implemented the forager architecture (described\nin Section 3) in Java programming language.\nIn this experiment a ﬁxed number of foragers were competing with each other to\ncollect news at the CNN web site. The foragers were running in equal time intervals in\na predeﬁned order. Each forager had a 3 minute time interval and after that interval\nthe forager was allowed to ﬁnish the step started before the end of the time interval.\nWe deployed 8 foragers using the weblog update and the reinforcement learning based\nURL ordering update algorithms (8 WLRL foragers). We also deployed 8 other for-\nagers using the weblog update algorithm but without reinforcement learning (8 WL\nforagers). The predeﬁned order of foragers was the following: 8 WLRL foragers were\nfollowed by the 8 WL foragers.\nWe investigated the link structure of the gathered Web pages.\nAs it is shown\nin Fig.\n1 the links have a power-law distribution (P(k) = kγ) with γ = −1.3 for\noutgoing links and γ = −2.57 for incoming links. That is the link structure has the\nscale-free property. The clustering coeﬃcient [31] of the link structure is 0.02 and the\ndiameter of the graph is 7.2893. We applied two diﬀerent random permutations to\nthe origin and to the endpoint of the links, keeping the edge distribution unchanged\nbut randomly rewiring the links. The new graph has 0.003 clustering coeﬃcient and\n8.2163 diameter. That is the clustering coeﬃcient is smaller than the original value\nby an order of magnitude, but the diameter is almost the same. Therefore we can\nconclude that the links of gathered pages form small world structure.\nThe data storage for simulation is a centralized component. The pages are stored\nwith 2 indices (and time stamps). One index is the URL index, the other is the page\nindex. Multiple pages can have the same URL index if they were downloaded from\nthe same URL. The page index uniquely identiﬁes a page content and the URL from\nwhere the page was download. At each page download of any foragers we stored the\nfollowings (with a time stamp containing the time of page download):\n1. if the page is relevant according to the RA then store “relevant”\n2. if the page is from a new URL then store the new URL with a new URL index\nand the page’s state vector with a new page index\n3. if the content of the page is changed since the last download then store the\npage’s state vector with a new page index but keep the URL index\n4. in both previous cases store the links of the page as links to page indices of the\nlinked pages\n(a) if a linked page is from a new URL then store the new URL with a new\nURL index and the linked page’s state vector with a new page index\n15\n10\n0\n10\n2\n10\n4\n10\n−6\n10\n−4\n10\n−2\n10\n0\nlog(k)\nlog(P(k))\nγ ~−1.3\nγ ~−2.57\nFigure 1: Scale-free property of the Internet domain. Log-log scale distri-\nbution of the number of (incoming and outgoing) links of all URLs found during\nthe time course of investigation. Horizontal axis: number of edges (log k). Ver-\ntical axis: relative frequency of number of edges at diﬀerent URLs (log P(k)).\nDots and dark line correspond to outgoing links, crosses and gray line correspond\nto incoming links.\n(b) if the content of the linked page is changed since the last check then store\nthe page’s state vector with a new page index but same URL index\n4.2\nSimulation\nFor the simulations we implemented the forager architecture in Matlab. The foragers\nwere simulated as if they were running on one computer as described in the previous\nsection.\n4.2.1\nSimulation speciﬁcation\nDuring simulations we used the Web pages that we gathered previously to generate a\nrealistic environment (note that the links of pages point to local pages (not to pages\non the Web) since a link was stored as a link to a local page index):\n• Simulated documents had the same state vector representation for URL ordering\nas the real pages had\n• Simulated relevant documents were the same as the relevant documents on the\nWeb\n• Pages and links appeared at the same (relative) time when they were found in\nthe Web experiment - using the new URL indices and their time stamps\n• Pages and links are refreshed or changed at the same relative time as the changes\nwere detected in the Web experiment – using the new page indices for existing\nURL indices and their time stamps\n• Simulated time of a page download was the average download time of a real\npage during the Web experiment.\nWe conducted simulations with two diﬀerent kinds of foragers. The ﬁrst case is\nwhen foragers used only the weblog update algorithm without URL ordering update\n16\nTable 1: Investigated parameters\ndownloaded\nis the number of downloaded documents\nsent\nis the number of documents sent to the RA\nrelevant\nis the number of found relevant documents\nfound URLs\nis the number of found URLs\ndownload eﬃciency\nis the ratio of relevant to downloaded documents in 3 hour time\nwindow throughout the simulation.\nsent eﬃciency\nis the ratio of relevant to sent documents in 3 hour time window\nthroughout the simulation.\nrelative found URL\nratio of found URLs to downloaded at the end of the simulation\nfreshness\nis the ratio of the number of current found relevant documents\nand the number of all found relevant documents [3].\nA stored\ndocument is current, up-to-date, if its content is exactly the same\nas the content of the corresponding URL in the environment.\nage\nA stored current document has 0 age, the age of an obsolete page\nis the time since the last refresh of the page on the Web [3].\n(WL foragers). The second case is when foragers used only the reinforcement learn-\ning based URL ordering update algorithm without the weblog update algorithm (RL\nforagers). Each WL forager had a diﬀerent weight vector for URL value estimation –\nduring multiplication the new forager got a new random weight vector. RL foragers\nhad the same weblog with the ﬁrst 10 URLs of the gathered pages – that is the start-\ning URL of the Web experiment and the ﬁrst 9 visited URLs during that experiment.\nIn both cases initially there were 2 foragers and they were allowed to multiply until\nreaching the population of 16 foragers. The simulation for each type of foragers were\nrepeated 3 times with diﬀerent initial weight vectors for each forager. The variance\nof the results show that there is only a small diﬀerence between simulations using the\nsame kind of foragers, even if the foragers were started with diﬀerent random weight\nvectors in each simulation.\n4.2.2\nSimulation measurements\nTable 1 shows the investigated parameters during simulations.\nParameter ‘download eﬃciency’ is relevant for the site where the foragers should\nbe deployed to gather the new information while parameter ‘sent eﬃciency’ is relevant\nfor the RA. Note that during simulations we are able to immediately and precisely\ncalculate freshness and age values. In a real Web experiment it is impossible to calcu-\nlate these values precisely, because of the time needed to download and compare the\ncontents of all of the real Web pages to the stored ones.\n4.2.3\nSimulation analysis\nThe values in Table 2 are averaged over the 3 runs of each type of foragers.\nFrom Table 2 we can conclude the followings:\n• RL and WL foragers have similar download eﬃciency, i.e., the eﬃciencies from\nthe point of view of the news site are about the same.\n17\nTable 2: Simulation results. The 3rd and 5th columns contain the standard\ndeviation of the individual experiment results from the average values.\ntype\nRL\nstd RL\nWL\nstd WL\ndownloaded\n540636\n9840\n669673\n9580\nsent\n9747\n98\n6345\n385\nrelevant\n2419\n45\n3107\n60\nfound URLs\n31092\n1050\n33116\n3370\ndownload eﬃciency\n0.0045\n0.0001\n0.0046\n0.0001\nsent eﬃciency\n0.248\n0.003\n0.49\n0.031\nrelative found URL\n0.058\n0.001\n0.05\n0.006\nfreshness\n0.7\n0.006\n0.74\n0.011\nage (in hours)\n1.79\n0.04\n1.56\n0.08\n• WL foragers have higher sent eﬃciencies than RL foragers, i.e., the eﬃciency\nfrom the point of view of the RA is higher. This shows that WL foragers divide\nthe search area better among each other than RL foragers. Sent eﬃciency would\nbe 1 if none of two foragers have sent the same document to the RA.\n• RL foragers have higher relative found URL value than WL foragers. RL foragers\nexplore more than WL foragers and RL found more URLs than WL foragers did\nper downloaded page.\n• WL foragers ﬁnd faster the new relevant documents in the already found clusters.\nThat is freshness is higher and age is lower than in the case of RL foragers.\n0\n0.005\n0.01\n0.015\n0.02\n1\n2\n3\n4\n5\n6\n7\n8\n0\n0.005\n0.01\n0.015\n0.02\nDays\nFigure 2: Eﬃciency. Horizontal axis: time in days. Vertical axis: download\neﬃciency, that is the number of found relevant documents divided by number\nof downloaded documents in 3 hour time intervals.\nUpper ﬁgure shows RL\nforagers’ eﬃciencies, lower ﬁgure shows WL foragers’ eﬃciencies. For all of the\n3 simulation experiments there is a separate line.\nFig.\n2 shows other aspects of the diﬀerent behaviors of RL and WL foragers.\nDownload eﬃciency of RL foragers has more, higher, and sharper peaks than the\n18\ndownload eﬃciency of WL foragers has. That is WL foragers are more balanced in\nﬁnding new relevant documents than RL foragers. The reason is that while the WL\nforagers remain in the found good clusters, the RL foragers continuously explore the\nnew promising territories. The sharp peaks in the eﬃciency show that RL foragers\nﬁnd and recognize new good territories and then quickly collect the current relevant\ndocuments from there.\nThe foragers can recognize these places by receiving more\nrewards from the RA if they send URLs from these places.\n0.4\n0.6\n0.8\n1\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n0\n1\n2\nDays\nFigure 3: Freshness and Age. Horizontal axis: time in days. Upper vertical\naxis: freshness of found relevant documents in 3 hour time intervals. Lower\nvertical axis: age in hours of found relevant documents in 3 hour time intervals.\nDotted lines correspond to weblog foragers, continuous lines correspond to RL\nforagers.\nThe predeﬁned order did not inﬂuence the working of foragers during the Web\nexperiment.\nFrom Fig.\n2 it can be seen that foragers during the 3 independent\nexperiments did not have very diﬀerent eﬃciencies.\nOn Fig.\n3 we show that the\nforagers in each run had a very similar behavior in terms of age and freshness, that is\nthe values remains close to each other throughout the experiments. Also the results\nfor individual runs were close to the average values in Table 2 (see the standard\ndeviations).\nIn each individual run the foragers were started with diﬀerent weight\nvectors, but they reached similar eﬃciencies and behavior. This means that the initial\nconditions of the foragers did not inﬂuence the later behavior of them during the\nsimulations. Furthermore foragers could not change their environment drastically (in\nterms of the found relevant documents) during a single 3 minute run time because of\nthe short run time intervals and the fast change of environment – large number of new\npages and often updated pages in the new site. During the Web experiment foragers\nwere running in 8 WLRL, 8 WL, 8 WLRL, 8 WL, . . . temporal order. Because of the\nfact that initial conditions does not inﬂuence the long term performance of foragers\nand the fact that the foragers can not change their environment fully we can start to\nexamine them after the ﬁrst run of WLRL foragers. Then we got the other extreme\norder of foragers, that is the 8 WL, 8 WLRL, 8 WL, 8 WLRL, . . . temporal ordering.\nFor the overall eﬃciency and behavior of foragers it did not really matter if WLRL or\nWL foragers run ﬁrst and one could use mixed order in which after a WLRL forager\na WL forager runs and after a WL forager a WLRL forager comes.\nHowever, for\n19\nhigher bandwidths and for faster computers, random ordering may be needed for such\ncomparisons.\n5\nDiscussion\nOur ﬁrst conjecture is that selection is eﬃcient on scale-free small world structures.\nL˝orincz and K´okai [15] and Rennie et al. [21] showed that RL is eﬃcient in the task\nof ﬁnding relevant information on the Web. Here we have shown experimentally that\nthe weblog update algorithm, selection among starting URLs, is at least as eﬃcient as\nthe RL algorithm. The weblog update algorithm ﬁnds as many relevant documents as\nRL does if they download the same amount of pages. WL foragers in their ﬂeet select\nmore diﬀerent URLs to send to the RA than RL foragers do in their ﬂeet, therefore\nthere are more relevant documents among those selected by WL foragers then among\nthose selected by RL foragers. Also the freshness and age of found relevant documents\nare better for WL foragers than for RL foragers.\nFor the weblog update algorithm, the selection among starting URLs has no ﬁne\ntuning mechanism. Throughout its life a forager searches for the same kind of docu-\nments – goes into the same ‘direction’ in the state space of document states – deter-\nmined by its ﬁxed weight vector. The only adaptation allowed for a WL forager is to\nselect starting URLs from the already seen URLs. The WL forager can not modify\nits (‘directional’) preferences according goes newly found relevant document supply,\nwhere relevant documents are abundant. But a WL forager ﬁnds good relevant doc-\nument sources in its own direction and forces its search to stay at those places. By\nchance the forager can ﬁnd better sources in its own direction if the search path from\na starting URL is long enough. On Fig. 2 it is shown that the download eﬃciency of\nthe foragers does not decrease with the multiplication of the foragers. Therefore the\nnew foragers must found new and good relevant document sources quickly after their\nappearances.\nThe reinforcement learning based URL ordering update algorithm is capable to\nﬁne tune the search of a forager by adapting the forager’s weight vector. This feature\nhas been shown to be crucial to adapt crawling in novel environments [13, 15]. An\nRL forager goes into the direction (in the state space of document states) where the\nestimated long term cumulated proﬁt is the highest. Because the local environment of\nthe foragers may changes rapidly during crawling, it seems desirable that foragers can\nquickly adapt to the found new relevant documents. Relevant documents may appear\nlonely, not creating a good relevant document source, or do not appear at the right\nURL by a mistake. This noise of the Web can derail the RL foragers from good regions.\nThe forager may “turn” into less valuable directions, because of the fast adaptation\ncapabilities of RL foragers.\nOur second conjecture is that selection ﬁts SFSW better than RL. We have shown\nin our experiments that selection and RL have diﬀerent behaviors. Selection selects\ngood information sources, which are worth to revisit, and stays at those sources as\nlong as better sources are not found by chance.\nRL explores new territories, and\nadapts to those. This adaptation can be a disadvantage when compared with the more\nrigid selection algorithm, which sticks to good places until ‘provably’ better places are\ndiscovered. Therefore WL foragers, which can not be derailed and stay in their found\n‘niches’ can ﬁnd new relevant documents faster in such already known terrains than\nRL foragers can. That is, freshness is higher and age is lower for relevant documents\nfound by WL foragers than for relevant documents found by RL foragers. Also, by\n20\nﬁnding good sources and staying there, WL foragers divide the search task better than\nRL foragers do, this is the reason for the higher sent eﬃciency of WL foragers than of\nRL foragers.\nWe have rewired the network as it was described in Section 4.1. This way a scale-\nfree (SF) but not so small world was created. Intriguingly, in this SF structure, RL\nforagers performed better than WL ones.\nClearly, further work is needed to com-\npare the behavior of the selective and the reinforcement learning algorithms in other\nthen SFSW environments. Such ﬁndings should be of relevance in the deployment of\nmachine learning methods in diﬀerent problem domains.\nFrom the practical point of view, we note that it is an easy matter to combine\nthe present algorithm with URLs oﬀered by search engines. Also, the values reported\nby the crawlers about certain environments, e.g., the environment of the URL oﬀered\nby search engines represent the neighborhood of that URL and can serve adaptive\nﬁltering. This procedure is, indeed, promising to guide individual searches as it has\nbeen shown elsewhere [20].\n6\nConclusion\nWe presented and compared our selection algorithm to the well-known reinforcement\nlearning algorithm. Our comparison was based on ﬁnding new relevant documents on\nthe Web, that is in a dynamic scale-free small world environment. We have found that\nthe weblog update selection algorithm performs better in this environment than the\nreinforcement learning algorithm, eventhough the reinforcement learning algorithm\nhas been shown to be eﬃcient in ﬁnding relevant information [15, 21]. We explain our\nresults based on the diﬀerent behaviors of the algorithms. That is the weblog update\nalgorithm ﬁnds the good relevant document sources and remains at these regions\nuntil better places are found by chance.\nIndividuals using this selection algorithm\nare able to quickly collect the new relevant documents from the already known places\nbecause they monitor these places continuously. The reinforcement learning algorithm\nexplores new territories for relevant documents and if it ﬁnds a good place then it\ncollects the existing relevant documents from there. The continuous exploration and\nthe ﬁne tuning property of RL causes that RL ﬁnds relevant documents slower than\nthe weblog update algorithm.\nIn our future work we will study the combination of the weblog update and the\nRL algorithms. This combination uses the WL foragers ability to stay at good regions\nwith the RL foragers ﬁne tuning capability. In this way foragers will be able to go to\nnew sources with the RL algorithm and monitor the already found good regions with\nthe weblog update algorithm.\nWe will also study the foragers in a simulated environment which is not a small\nworld. The clusters of small world environment makes it easier for WL foragers to\nstay at good regions. The small diameter due to the long distance links of small world\nenvironment makes it easier for RL foragers to explore diﬀerent regions. This work\nwill measure the extent at which the diﬀerent foragers rely on the small world property\nof their environment.\n21\n7\nAcknowledgement\nThis material is based upon work supported by the European Oﬃce of Aerospace\nResearch and Development, Air Force Oﬃce of Scientiﬁc Research, Air Force Research\nLaboratory, under Contract No. FA8655-03-1-3036. This work is also supported by\nthe National Science Foundation under grants No. INT-0304904 and No. IIS-0237782.\nAny opinions, ﬁndings and conclusions or recommendations expressed in this material\nare those of the author(s) and do not necessarily reﬂect the views of the European\nOﬃce of Aerospace Research and Development, Air Force Oﬃce of Scientiﬁc Research,\nAir Force Research Laboratory.\nReferences\n[1] A.L. Barab´asi, R. Albert, and H. Jeong, Scale-free characteristics of random\nnetworks: The topology of the world wide web, Physica A 281 (2000), 69–77.\n[2] D.L. Boley, Principal direction division partitioning, Data Mining and Knowledge\nDiscovery 2 (1998), 325–244.\n[3] J. Cho and H. Garcia-Molina, Eﬀective page refresh policies for web crawlers,\nACM Transactions on Database Systems 28 (2003), no. 4, 390–426.\n[4] C.W. Clark and M. Mangel, Dynamic state variable models in ecology: Methods\nand applications., Oxford University Press, Oxford UK, 2000.\n[5] V. Cs´anyi, Evolutionary systems and society: A general theory of life, mind, and\nculture, Duke University Press, Durham, NC, 1989.\n[6] J. Edwards, K. McCurley, and J. Tomlin, An adaptive model for optimizing per-\nformance of an incremental web crawler, Proceedings of the tenth international\nconference on World Wide Web, 2001, pp. 106–113.\n[7] A. E. Eiben and J.E. Smith, Introduction to evolutionary computing, Springer,\n2003.\n[8] J.M. Fryxell and P. Lundberg, Individual behavior and community dynamics.,\nChapman and Hall, London, 1998.\n[9] B. G´abor, Zs. Palotai, and A. L˝orincz, Value estimation based computer-assisted\ndata mining for surﬁng the internet, Int. Joint Conf. on Neural Networks (Piscat-\naway, NJ 08855-1331), IEEE Operations Center, 26-29 July, Budapest, Hungary\n2004, pp. Paper No. 1035., IEEE Catalog Number: 04CH37541C, IJCNN2004\nCD–ROM Conference Proceedings.\n[10] Thorsten Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF\nfor text categorization, Proceedings of ICML-97, 14th International Conference on\nMachine Learning (Nashville, US) (Douglas H. Fisher, ed.), Morgan Kaufmann\nPublishers, San Francisco, US, 1997, pp. 143–151.\n[11] G. Kampis, Self-modifying systems in biology and cognitive science: A new frame-\nwork for dynamics, information and complexity, Pergamon, Oxford UK, 1991.\n[12] J. Kleinberg and S. Lawrence, The structure of the web, Science 294 (2001),\n1849–1850.\n[13] I. K´okai and A. L˝orincz, Fast adapting value estimation based hybrid architecture\nfor searching the world-wide web, Applied Soft Computing 2 (2002), 11–23.\n22\n[14] T. Kondo and K. Ito, A reinforcement learning with evolutionary state recruitment\nstrategy for autonomous mobile robots control, Robotics and Autonomous Systems\n46 (2004), 11–124.\n[15] A. L˝orincz, I. K´okai, and A. Meretei, Intelligent high-performance crawlers used\nto reveal topic-speciﬁc structure of the WWW, Int. J. Founds. Comp. Sci. 13\n(2002), 477–495.\n[16] Maja J. Mataric, Reinforcement learning in the multi-robot domain, Autonomous\nRobots 4 (1997), no. 1, 73–83.\n[17] F. Menczer, Complementing search engines with online web mining agents, Deci-\nsion Support Systems 35 (2003), 195–212.\n[18] D.E. Moriarty, A.C. Schultz, and J.J. Grefenstette, Evolutionary algorithms for\nreinforcement learning, Journal of Artiﬁcial Intelligence Research 11 (1999), 199–\n229.\n[19] E. Pachepsky, T. Taylor, and S. Jones, Mutualism promotes diversity and stability\nin a simple artiﬁcial ecosystem., Artiﬁcial Life 8 (2002), no. 1, 5–24.\n[20] Zs. Palotai, B. G´abor, and A. L˝orincz, Adaptive highlighting of links to assist\nsurﬁng on the internet, Int. J. of Information Technology and Decision Making\n(2005), (to appear).\n[21] J. Rennie, K. Nigam, and A. McCallum, Using reinforcement learning to spider\nthe web eﬃciently, Proc. 16th Int. Conf. on Machine Learning (ICML), Morgan\nKaufmann, San Francisco, 1999, pp. 335–343.\n[22] K. M. Risvik and R. Michelsen, Search engines and web dynamics, Computer\nNetworks 32 (2002), 289–302.\n[23] A. Rungsawang and N. Angkawattanawit, Learnable topic-speciﬁc web crawler,\nComputer Applications xx (2004), xxx–xxx.\n[24] W. Schultz, Multiple reward systems in the brain., Nature Review of Neuroscience\n1 (200), 199–207.\n[25] A. Stafylopatis and K. Blekas, Autonomous vehicle navigation using evolutionary\nreinforcement learning, European Journal of Operational Research 108 (19998),\n306–318.\n[26] R. Sutton, Learning to predict by the method of temporal diﬀerences, Machine\nLearning 3 (1988), 9–44.\n[27] R. Sutton and A.G. Barto, Reinforcement learning: An introduction, MIT Press,\nCambridge, 1998.\n[28] I. Szita and A. L˝orincz, Kalman ﬁlter control embedded into the reinforcement\nlearning framework., Neural Computation 16 (2004), 491–499.\n[29] G. J. Tesauro, Temporal diﬀerence learning and td-gammon, Communication of\nthe ACM 38 (1995), 58–68.\n[30] K. Tuyls, D. Heytens, A. Nowe, and B. Manderick, Extended replicator dynamics\nas a key to reinforcement learning in multi-agent systems, ECML 2003, LNAI\n2837 (N. Lavrac et al., ed.), Springer-Verlag, Berlin, 2003, pp. 421–431.\n[31] D. J. Watts and S. H. Strogatz, Collective dynamics of ‘small-world’ networks,\nNature 393 (1998), no. 6684, 440–442.\n23\n[32] X. Yao, Review of evolutionary artiﬁcial neural networks, International Journal\nof Intelligent Systems 8 (1993), 539–567.\n[33]\n, Evolving artiﬁcial neural networks, Proceedings of the IEEE, vol. 87,\n1999, pp. 1423–1447.\n24\n",
  "categories": [
    "cs.LG",
    "cs.IR",
    "H.3.3"
  ],
  "published": "2005-04-14",
  "updated": "2005-04-14"
}