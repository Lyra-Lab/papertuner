{
  "id": "http://arxiv.org/abs/cs/0703138v1",
  "title": "Reinforcement Learning for Adaptive Routing",
  "authors": [
    "Leonid Peshkin",
    "Virginia Savova"
  ],
  "abstract": "Reinforcement learning means learning a policy--a mapping of observations\ninto actions--based on feedback from the environment. The learning can be\nviewed as browsing a set of policies while evaluating them by trial through\ninteraction with the environment. We present an application of gradient ascent\nalgorithm for reinforcement learning to a complex domain of packet routing in\nnetwork communication and compare the performance of this algorithm to other\nrouting methods on a benchmark problem.",
  "text": "arXiv:cs/0703138v1  [cs.LG]  28 Mar 2007\nReinforcement Learning for Adaptive Routing\nLeonid Peshkin (pesha@ai.mit.edu)\nVirginia Savova (savova@jhu.edu)\nMIT Artiﬁcial Intelligence Lab.\nJohns Hopkins University\nCambridge, MA 02139\nBaltimore, MD 21218\nAbstract - Reinforcement learning means learning\na policy—a mapping of observations into actions—\nbased on feedback from the environment. The learn-\ning can be viewed as browsing a set of policies while\nevaluating them by trial through interaction with the\nenvironment. We present an application of gradient\nascent algorithm for reinforcement learning to a com-\nplex domain of packet routing in network communica-\ntion and compare the performance of this algorithm\nto other routing methods on a benchmark problem.\nI. Introduction\nSuccessful telecommunication requires eﬃcient re-\nsource allocation that can be achieved by developing\nadaptive\ncontrol\npolicies.\nReinforcement\nlearning\n(rl) [10], [17] presents a natural framework for the\ndevelopment of such policies by trial and error in the\nprocess of interaction with the environment.\nIn this\nwork we apply the rl algorithm to network routing.\nEﬀective network routing means selecting the optimal\ncommunication paths.\nIt can be modeled as a multi-\nagent rl problem.\nIn a sense, learning the optimal\ncontrol for network routing could be thought of as\nlearning in some traditional for rl episodic task, like\nmaze searching or pole balancing, but repeating trials\nmany times in parallel with interaction among trials.\nUnder this interpretation, an individual router is an\nagent which makes its routing decisions according to an\nindividual policy. The parameters of this policy are ad-\njusted according to some measure of the global perfor-\nmance of the network, while control is determined by\nlocal observations. Nodes do not have any information\nregarding the topology of network or their position in it.\nThe initialization of each node, as well as the learning\nalgorithm it follows, are identical to that of every other\nnode and independent of the structure of the network.\nThere is no notion of orientation in space or other se-\nmantics of actions. Our approach allows us to update the\nlocal policies while avoiding the necessity for centralized\ncontrol or global knowledge of the networks structure.\nThe only global information required by the learning al-\ngorithm is the network utility expressed as a reward sig-\nnal distributed once in an epoch and dependent on the\n0Errata at www.ai.mit.edu/~pesha/papers.html\naverage routing time. This learning multi-agent system\nis biologically plausible and could be thought of as neu-\nral network in which each neuron only performs simple\ncomputations based on locally available quantities [?].\nII. Domain\nWe test our algorithm on a domain adopted from\nBoyan and Littman [4].\nIt is a discrete time simula-\ntor of communication networks with various topologies\nand dynamic structure. A communication network is an\nabstract representation of real-life systems such as the\nInternet or a transport network. It consists of a homo-\ngeneous set of nodes and edges between them represent-\ning links (see Figure 1). Nodes linked to each other are\ncalled neighbors. Links may be active (”up”) or inac-\ntive (”down”). Each node can be the origin or the ﬁnal\ndestination of packets, or serve as a router.\nPackets are periodically introduced into the network\nwith a uniformly random node of origin and destination.\nThey travel to their destination node by hopping on inter-\nmediate nodes. No packets are generated being destined\nto the node of origin. Sending a packet down a link in-\ncurs a cost that could be thought of as time in transition.\nThere is an added cost to waiting in the queue of a par-\nticular node in order to access the router’s computational\nresource—a queue delay. Both costs are assumed to be\nuniform throughout the network.\nIn our experiments,\neach is set to be a unit cost. The level of network traﬃc\nis determined by the number of packets in the network.\nOnce a packet reaches its destination, it is removed. If a\npacket has been traveling around the network for a long\ntime it is also removed as a hopeless case. Multiple pack-\nets line up at nodes in an fifo (ﬁrst in ﬁrst out) queue\nlimited in size. The node must forward the top packet in\nthe fifo queue to one of its neighbors.\nIn the terminology of rl, the network represents the\nenvironment whose state is determined by the number\nand relative position of nodes, the status of links be-\ntween them and the dynamics of packets. The destina-\ntion of handled packets and the status of local links form\nthe node’s observation. Each node is an agent who has\na choice of actions. It decides where to send the packet\naccording to a policy. The policy computed by our algo-\nrithm is stochastic, as opposed to deterministic, i.e. it\nsends packets bound for the same destination down diﬀer-\nent links, according to some distribution. The policy con-\nsidered in our experiments does not determine whether\nor not to accept a packet (admission control), how many\npackets to accept from each neighbor, or which packets\nshould be assigned priority.\nThe node updates the parameters of its policy based\non the reward. The reward comes in the form of a sig-\nnal distributed through the network by acknowledgment\npackets once a packet has reached its ﬁnal destination.\nThe reward depends on the total delivery time for the\npacket. We measure the performance of the algorithm\nby the average delivery time for packets once the system\nhas settled on a policy (ordinate axes on ﬁgure 2). We\napply policy shaping by explicitly penalizing loops in the\nroute. Each packet is assumed to carry some elements\nof its routing history in addition to obvious destination\nand origin information. They include the time when the\npacket was generated, the time the packet last received\nattention from some router, the trace of recently visited\nnodes and the number of hops performed so far. In case\na packet is detected to have spent too much time in the\nnetwork failing to reach its destination, such packet is dis-\ncarded and the network is penalized accordingly. Thus,\na deﬁning factor in our simulation is weather the num-\nber of hops performed by a packet is more than a total\nnumber of nodes in the network.\nIII. Algorithmic details\nWilliams introduced the notion of policy search via\ngradient ascent for reinforcement learning in his rein-\nforce algorithm [18], [19], which was generalized to a\nbroader class of error criteria by Baird and Moore [1], [2].\nThe general idea is to adjust parameters in the direction\nof the empirically estimated gradient of the aggregate re-\nward. We assume standard Markov decision process mdp\nsetup [10]. Let us consider the case of a single agent in-\nteracting with a partially observable mdp (pomdp). The\nagent’s policy µ is a so-called reactive policy represented\nby a lookup table with a value θoa for each observation-\naction (destination/link) pair.\nThe policy deﬁnes the\nprobability of an action given past history as a continuous\ndiﬀerentiable function of a set of parameters θ according\nto a softmax rule, where Ξ is a temperature parameter:\nµ(a, o, θ) = Pr\n\u0000a(t)=a\n\f\fo(t)=o, θ\n\u0001\n=\nexp(θoa/Ξ)\nP\na′ exp(θoa′/Ξ) > 0.\nThis rule assures that for any destination o any link\na′ available at the node is sometimes chosen with some\nsmall probability dependent on the temperature Ξ.\nWe denote by Ht the set of all possible experience se-\nquences h = ⟨o(1), a(1), r(1), . . ., o(t), a(t), r(t), o(t+1)⟩of\nlength t.\nIn order to specify that some element is a\npart of the history h at time τ, we write, for exam-\nple, r(τ,h) and a(τ, h) for the τ th reward and action\nin the history h. We will also use hτ to denote a pre-\nﬁx of the sequence h ∈Ht truncated at time τ ≤t :\nhτ def\n= ⟨o(1), a(1), r(1), . . . , o(τ), a(τ), r(τ), o(τ +1)⟩. The\nvalue of following a policy µ with parameters θ is the ex-\npected cumulative discounted (by a factor of γ ∈[0, 1))\nreward that can be written as\nV (θ) =\n∞\nX\nt=1\nγt X\nh∈Ht\nPr(h | θ)r(t, h) .\nIf we could calculate the derivative of V (θ) for each\nθoa, it would be possible to do an exact gradient ascent\non value V () by making updates ∆θoa = α\n∂\n∂θoa V (θ) for\nsome step size α. Let us analyze the derivative for each\nweight θoa,\n∂V (θ)\n∂θoa\n=\nP∞\nt=1γtP\nh∈Ht\nh\nr(t, h) ∂Pr(h|θ)\n∂θoa\ni\n=\nP∞\nt=1 γt P\nh∈Ht Pr(h | θ)r(t, h)\n×\nPt\nτ=1\n∂ln Pr(a(τ,h)|hτ−1,θ)\n∂θoa\n.\nHowever, in the spirit of reinforcement learning, we as-\nsume no knowledge of a world model that would allow the\nagent to calculate Pr(h|θ), so we must retreat to stochas-\ntic gradient ascent instead. We sample from the distribu-\ntion of histories by interacting with the environment, and\ncalculate during each trial an estimate of the gradient, ac-\ncumulating the quantities: γtr(t, h) Pt\nτ=1\n∂ln µ(a,o,θ)\n∂θoa\n, for\nall t. For a particular policy architecture, this can be\nreadily translated into a gradient ascent algorithm guar-\nanteed to converge to a local optimum θ∗of V (θ). Under\nour chosen policy encoding we get:\n∂ln µ(a, o, θ)\n∂θo′a′\n=\n\n\n\n0\nif o′ ̸= o,\n−1\nΞµ (a′, o, θ)\nif o′ = o, a′ ̸= a,\n1\nΞ\n\u0002\n1 −µ(a, o, θ)\n\u0003\nif o′ = o, a′ = a.\nApplying this algorithm in a network of connected con-\ntrollers basically constitutes the algorithm of routing by\ndistributed gradient ascent policy search (gaps).\nWe compare the performance of our distributed gaps\nalgorithm to three others, as follows. “Best” is a static\nrouting scheme based on the shortest path counting each\nlink as a single unit of routing cost. We include this al-\ngorithm because it provides the basis for most current\nindustry routing heuristics [3], [8]. “Bestload”performs\nrouting according to the shortest path while taking into\naccount queue sizes at each node. It is close to the theo-\nretical optimum among deterministic routing algorithms\neven though the actual best possible routing scheme re-\nquires not simply computing the shortest path based on\nnetwork loads, but also analyzing how loads change in\ntime according to routing decisions.\nSince calculating\nthe shortest path at every single step of the simulation\nwould be prohibitively costly in terms of computational\nresources, we implemented “Bestload” by readjusting the\nrouting policy only after a notable change of loads in the\nnetwork. We consider 50 successfully delivered packets to\nconstitute a notable load change. Finally, “Q-routing” is\na distributed rl algorithm applied speciﬁcally to this do-\nmain by Littman and Boyan [4]. While our algorithm is\nstochastic and performs policy search, Q-routing is a de-\nterministic, value search algorithm. Note that our imple-\nmentation of the network routing simulation is based on\nthe software Littman and Boyan used to test Q-routing.\nEven so,the results of our simulation of “Q-routing” and\n“Best” on the “6x6” network diﬀer slightly from Littman\nand Boyan’s due to certain modiﬁcations in traﬃc mod-\neling conventions.\nFor instance, we consider a packet\ndelivered and ready for removal only after it has passed\nthrough the queue of the destination node and accessed\nits computational resources, and not merely when the\npacket is successfully routed to the destination node by\nan immediate neighbor, as in the original simulation.\nWe undertake the comparison between gaps and the\naforementioned algorithms with one important caveat.\nThe gaps algorithm explores the class of stochastic poli-\ncies while all other methods pick deterministic routing\npolicies. Consequently, it is natural to expect gaps to\nbe superior for certain types of network topologies and\nloads, where the optimal policy is stochastic. Later, we\nshow that our experiments conﬁrm this expectation.\nWe implement the distributed gaps in pomdp. In par-\nticular, we represent each router as a pomdp, where the\nstate contains the sizes of all queues, destinations of all\npackets, state of links (up or down); the environment\nstate transition function is a law of the dynamics of net-\nwork traﬃc; an observation o consists of the destination\nof the packet; an action a corresponds to sending the\npacket down a link to an adjacent node; and ﬁnally, the\nreward signal is the average number of packets delivered\nper unit of time. Each agent is using a gaps rl algorithm\nto move parameterization values down the gradient of the\naverage reward. It has been shown [14] that an applica-\ntion of distributed gaps causes the system as a whole\nto converge to local optimum under stationarity assump-\ntions. This algorithm is essentially the one described in\nchapter 3 and developed in chapter 5 of Peshkin’s disser-\ntation [13].\nPolicies were initialized in two diﬀerent ways: ran-\ndomly and based on shortest paths.\nWe tried initial-\nization with random policy uniformly chosen over pa-\nrameter space. With such initialization results are very\nsensitive to the learning rate. High learning rate often\ncauses the network to stick to local optima in combined\npolicy space, with very poor performance. Low learning\nrate results in a slow convergence. What constitutes high\nor low learning rate depends on the speciﬁcs of each net-\nwork and we did not ﬁnd any satisfactory heuristics to set\nit. Obviously, such features as average number of hops\nnecessary to deliver a packet under the optimal policy as\nwell as learning speed crucially depend on the particular\ncharacteristics of each network such as number of nodes,\nconnectivity and modularity.\nThese considerations led us to a diﬀerent way of ini-\ntializing controllers.\nNamely, we begin by computing\nshortest path and set controllers to route most of the\ntraﬃc down the shortest path, while occasionally send-\ning a packet to explore an alternative link. We call this\n“ǫ-greedy routing”. In our experiments, ǫ is set to .01.\nWe believe that this parameter would not qualitatively\nchange the outcome of our experiments since it only in-\nﬂuences exploratory behaviour in the beginning.\nThe exploration capacity of the algorithm is regulated\nin a diﬀerent way as well. Both temperature and learning\nrate are simply kept constant both for considerations of\nsimplicity and for maintaining the controllers’ ability to\nadjust to changes in the network, such as links failure.\nHowever, our experiments indicate that having a schedule\nfor reducing learning rate after a key initial period of\nlearning would improve performance.\nAlternatively, it\nwould be interesting to explore diﬀerent learning rates for\nthe routing parameters on one hand, and the encoding\nof topological features on the other.\nIV. Empirical results\nWe compared the routing algorithms on several net-\nworks with various number of nodes and degrees of con-\nnectivity and modularity, including 116-node “lata”\ntelephone network. On all networks, the gaps algorithm\nperformed comparably or better than other routing al-\ngorithms. To illustrate the principal diﬀerences in the\nbehavior of algorithms and the key advantages of dis-\ntributed gaps, we concentrate on the analysis of two\nrouting problems on networks which diﬀer in a single\nlink location.\nFigure 1.left presents the irregular 6x6 grid network\ntopology used by Boyan and Littman [4] in their experi-\nments. The network consists of two well connected com-\nponents with a bottleneck of traﬃc falling on two bridg-\ning links. The resulting dependence of network perfor-\nmance on the load is depicted in ﬁgure 2.left. All graphs\nrepresent performance after the policy has converged, av-\neraged over ﬁve runs. We tested the network on loads\nranging from .5 to 3.5, to compare with the results ob-\n20\n30\n32\n25\n26\n19\n18\n24\n12\n13\n14\n8\n7\n6\n0\n1\n2\n3\n4\n5\n11\n10\n9\n15\n16\n17\n23\n22\n21\n28\n29\n27\n35\n34\n33\n31\n30\n32\n25\n26\n18\n24\n12\n7\n0\n1\n2\n3\n4\n5\n11\n10\n9\n15\n16\n17\n23\n22\n21\n28\n29\n27\n35\n34\n33\n31\n20\n14\n13\n6\n8\n19\nFig. 1\nLeft: Original 6x6 network. Right: Modified 6x6 network favoring stochastic policies.\ntained by Littman and Boyan. The load corresponds to\nthe value of the parameter of Poisson arrival process for\nthe average number of packets injected per time unit. On\nthis network topology, gaps is slightly inferior to other\nalgorithms on lower loads, but does at least as well as\nBestload on higher loads, outperforming both Q-routing\nand Best. The slightly inferior performance on low loads\nis due to exploratory behaviour of gaps — some fraction\nof packets is always sent down random link.\nTo illustrate the diﬀerence between the algorithms\nmore explicitly, we altered the network by moving just\none link from connecting nodes 32 and 33, to connecting\nnodes 20 and 27 as illustrated by ﬁgure 1.right. Since\nnode 20 obviously represents a bottleneck in this conﬁgu-\nration, the optimal routing policy is bound to be stochas-\ntic. The resulting dependence of network performance on\nthe load is presented in ﬁgure 2.right. gaps is clearly su-\nperior to other algorithms at high loads. It even outper-\nforms “Bestload” that has all the global information in\nchoosing a policy, but is bound to deterministic policies.\nNotice how the deterministic algorithms get frustrated at\nmuch lower loads in this network conﬁguration than in\nthe previous one since from their perspective, the bridge\nbetween highly connected components gets twice thinner\n(compare left and right of Figure 2).\nThe gaps algorithm successfully adapts to changes in\nthe network conﬁguration. Under increased load, the pre-\nferred route from the left part of the network to the right\nbecomes evenly split between the two “bridges” at node\n20. By using link 20 −27, the algorithm has to pay a\npenalty of making a few extra hops compared to link\n20 −21, but as the size of the queue at node 21 grows,\nthis penalty becomes negligible compared to the waiting\ntime.\nExploratory behavior helps gaps discover when\nlinks go down and adjust the policy accordingly.\nWe\nhave experimented with giving each router a few bits of\nmemory in ﬁnite state controller [13], [?] but found that\nthis does not improve performance and slows down the\nlearning somewhat.\nV. Related Work\nThe application of machine learning techniques to the\ndomain of telecommunications is a rapidly growing area.\nThe bulk of problems ﬁt into the category of resource\nallocation, e.g. bandwidth allocation, network routing,\ncall admission control (cac) and power management. rl\nappears promising in attacking all of these problems, sep-\narately or simultaneously.\nMarbach, Mihatsch and Tsitsiklis [11] have applied an\nactor-critic (value-search) algorithm to address resource\nallocation within communication networks by tackling\nboth routing and call admission control. They adopt a\ndecompositional approach, representing the network as\nconsisting of link processes, each with its own diﬀeren-\ntial reward. Unfortunately, the empirical results even on\nsmall networks, 4 and 16 nodes, show little advantage\nover heuristic techniques.\nCarlstr¨om [7] introduces another rl strategy based\non decomposition called predictive gain scheduling. The\ncontrol problem of admission control is decomposed into\na time-series prediction of near-future call arrival rates\nand precomputation of control policies for Poisson call\narrival processes. This approach results in faster learn-\ning without performance loss. Online convergence rate\nincreases 50 times on a simulated link with capacity\n24 units/sec.\nGenerally speaking, value-search algorithms have been\nmore extensively investigated than policy search ones\nin the domain of communications.\nValue-search (Q-\nlearning) algorithms have arrived at promising results.\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n5\n8\n11\n14\n17\n20\nLoad\nAverage delivery time\nQ−route\nBest\nBestLoad\nGAPS\n0.5\n1\n1.5\n2\n2.5\n3\n5\n8\n11\n14\n17\n20\nLoad\nAverage delivery time\nQ−route\nBest\nBestLoad\nGAPS\nFig. 2\nPerformance of routing algorithms on the original 6x6 network (left) and the modified 6x6 network\n(right).\nBoyan and Littman’s [4] algorithm - Q-routing, proves\nsuperior to non-adaptive techniques based on shortest\npath, and robust with respect to dynamic variations in\nthe simulation on a variety of network topology, including\nan irregular 6×6 grid and 116-node lata phone network.\nIt regulates the trade-oﬀbetween the number of nodes a\npacket has to traverse and the possibility of congestion.\nWolpert, Tumer and Frank [20] construct a formalism\nfor the so-called Collective Intelligence (coin)neural net\napplied to Internet traﬃc routing. The approach involves\nautomatically initializing and updating the local utility\nfunctions of individual rl agents (nodes) from the global\nutility and observed local dynamics.\nTheir simulation\noutperforms a Full Knowledge Shortest Path Algorithm\non a sample network of seven nodes. Coin networks em-\nploy a method similar in spirit to the research presented\nhere. They rely on a distributed rl algorithm that con-\nverges on local optima without endowing each agent node\nwith explicit knowledge of network topology. However,\ncoin diﬀers form our approach in requiring the introduc-\ntion of preliminary structure into the network by dividing\nit into semi-autonomous neighborhoods that share a local\nutility function and encourage cooperation. In contrast,\nall the nodes in our network update their algorithms di-\nrectly from the global reward.\nThe work presented in this paper focuses on packet\nrouting using policy search.\nIt resembles the work of\nTao, Baxter and Weaver [12] who apply a policy-gradient\nalgorithm to induce cooperation among the nodes of a\npacket switched network in order to minimize the aver-\nage packet delay. While their algorithm performs well in\nseveral network types, it takes many (tens of millions)\ntrials to converge on a network of just a few nodes.\nApplying reinforcement learning to communication of-\nten involves optimizing performance with respect to mul-\ntiple criteria. For a recent discussion on this challenging\nissue see Shelton [15]. In the context of wireless com-\nmunication it was addressed by Brown [5] who considers\nthe problem of ﬁnding a power management policy that\nsimultaneously maximizes the revenue earned by provid-\ning communication while minimizing battery usage. The\nproblem is deﬁned as a stochastic shortest path with dis-\ncounted inﬁnite horizon, where discount factor varies to\nmodel power loss. This approach resulted in signiﬁcant\n(50%) improvement in power usage.\nGelenbe et al. [9] also compute the reward as a\nweighted combination of the probability of packet loss\nand packet delay. The packets themselves are agents con-\ntrolling routing and ﬂow control in a Cognitive Packet\nNetwork. They split packets into three types: ”smart”,\n”dumb” and ”acknowledgment”.\nA small number of\nsmart packets learn the most eﬃcient ways of navigat-\ning through the network, dumb packets simply follow the\nroute taken by the smart packets, while acknowledgment\npackets travel on the inverse route of smart packets to\nprovide source routing information to dumb packets. The\ndivision between smart and dumb packet is an explicit\nrepresentation of the explore/exploit dilemma.\nSmart\npacket allow the network to adapt to structural changes\nwhile the dumb packets exploit the relative stability be-\ntween those changes. Promising results are obtained both\non a simulation network of 100 nodes and on a physical\nnetwork of 6 computers.\nSubramanian, Druschel and Chen [16] adopt an ap-\nproach from ant colonies that is very similar in spirit.\nThe individual hosts in their network keep routing tables\nwith the associated costs of sending a packet to other\nhosts (such as which routers it has to traverse and how\nexpensive they are). These tables are periodically up-\ndated by ”ants”-messages whose function is to assess the\ncost of traversing links between hosts. The ants are di-\nrected probabilistically along available paths. They in-\nform the hosts along the way of the costs associated with\ntheir travel. The hosts use this information to alter their\nrouting tables according to an update rule. There are\ntwo types of ants. Regular ants use the routing tables of\nthe hosts to alter the probability of being directed along\na certain path. After a number of trials, all regular ants\non the same mission start using the same routes. Their\nfunction is to allow the host tables to converge on the\ncorrect cost ﬁgure in case the network is stable. Uniform\nants take any path with equal probability. They are the\nones who continue exploring the network and assure suc-\ncessful adaptation to changes in link status or link cost.\nVI. Discussion\nAdmittedly, the simulation of network routing process\npresented here is far from being realistic. A more realis-\ntic model could include such factors as non-homogeneous\nnetworks with regard to links bandwidth and routing\nnodes buﬀer size limits, collisions of packets, packet or-\ndering constraints, various costs associated with say, par-\nticular links chosen from commercial versus government\nsubnetworks, minimal Quality of Service requirements.\nIntroducing priorities for individual packets brings up yet\nanother set of optimization issues. However, the learning\nalgorithm we applied shows promise in handling adap-\ntive telecommunication protocol and there are several\nobvious ways to develop this research.\nIncorporating\ndomain knowledge into controller structure is one such\ndirection.\nIt would involve classifying nodes into sub-\nnetworks and routing packets in a hierarchical fashion.\nOne step further down this line is employing learning\nalgorithms for routing in ad-hoc networks. Ad-hoc net-\nworks are networks where nodes are being dynamically\nintroduced and terminated from the system, as well as\nexisting active nodes are moving about, loosing some con-\nnections and establishing new ones. Under the realistic\nassumption that physical variations is the network are\nslower than traﬃc routing and evolution, adaptive rout-\ning protocol should deﬁnitely outperform any heuristic\npre-deﬁned routines. We are currently pursuing this line\nof research.\nReferences\n[1]\nL. C. Baird. Reinforcement Learning Through Gradient De-\nscent.\nPhD thesis, Carnegie Mellon University, Pittsburgh,\nPA 15213, 1999.\n[2]\nL. C. Baird and A. W. Moore. Gradient descent for general\nreinforcement learning.\nIn Advances in Neural Information\nProcessing Systems, volume 11. The MIT Press, 1999.\n[3]\nR. Bellman.\nDynamic Programming.\nPrinceton University\nPress, Princeton, New Jersey, 1957.\n[4]\nJ. Boyan and M. L. Littman. Packet routing in dynamically\nchanging networks: a reinforcement learning approach. In Ad-\nvances in Neural Information Processing Systems, volume 7,\npages 671–678, 1994.\n[5]\nT. X. Brown. Low power wireless communication via reinforce-\nment learning. In Advances in Neural Information Processing\nSystems, volume 12, pages 893–899, 1999.\n[6]\nT. X. Brown, H. Tong, and S. P. Singh. Optimizing admis-\nsion control while ensuring quality of service in multimedia\nnetworks via reinforcement learning. In Advances in Neural\nInformation Processing Systems, volume 12, pages 982–988,\n1999.\n[7]\nJ. Carlstrom.\nReinforcement Learning for Admission Con-\ntrol and Routing. PhD thesis, Uppsala University, Uppsala,\nSweden, May 2000.\n[8]\nE. Dijkstra.\nA note on two problems in connection with\ngraphs. Numerical Mathematics, 1:269–271, 1959.\n[9]\nZ. X. E. Gelenbe, Ricardo Lent. Design and analysis of cogni-\ntive packet networks. Performance Evaluation, pages 155–176.\n[10] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforce-\nment learning: A survey. Journal of AI Research, 4:237–277,\n1996.\n[11] P. Marbach, O. Mihatsch, M. Schulte, and J. N. Tsitsiklis.\nReinforcement learning for call admission control and routing\nin integrated service networks. In Advances in Neural Infor-\nmation Processing Systems, volume 11, 1998.\n[12] J. B. Nigel Tao and L. Weaver. A multi-agent, policy gradient\napproach to network routing. In Proceedings of the Eighteenth\nInternational Conference on Machine Learning, 2001.\n[13] L. Peshkin. Reinforcement Learning by Policy Search. PhD\nthesis, Brown University, Providence, RI 02912, 2001.\nin\npreparation.\n[14] L. Peshkin, K.-E. Kim, N. Meuleau, and L. P. Kaelbling.\nLearning to cooperate via policy search. In Sixteenth Confer-\nence on Uncertainty in Artiﬁcial Intelligence, pages 307–314,\nSan Francisco, CA, 2000. Morgan Kaufmann.\n[15] C. R. Shelton. Importance Sampling for Reinforcement Learn-\ning with Multiple Objectives. PhD thesis, MIT, 2001.\n[16] D. Subramanian, P. Druschel, and J. Chen. Ants and rein-\nforcement learning: A case study in routing in dynamic net-\nworks.\nIn Proceedings of the Fifteenth International Joint\nConference on Artiﬁcial Intelligence, volume 2, pages 832–\n839, 1997.\n[17] R. S. Sutton and A. G. Barto. Reinforcement Learning: An\nIntroduction.\nThe MIT Press, Cambridge, Massachusetts,\n1998.\n[18] R. J. Williams. A class of gradient-estimating algorithms for\nreinforcement learning in neural networks. In Proceedings of\nthe IEEE First International Conference on Neural Networks,\nSan Diego, California, 1987.\n[19] R. J. Williams.\nSimple statistical gradient-following algo-\nrithms for connectionist reinforcement learning.\nMachine\nLearning, 8(3):229–256, 1992.\n[20] D. H. Wolpert, K. Tumer, , and J. Frank. Using collective\nintelligence to route internet traﬃc. In Advances in Neural\nInformation Processing Systems-11, pages 952–958, Denver,\n1998.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NI",
    "C.2.1; C.2.2; C.2.4; C.2.6; F.1.1; I.2.6; I.2.8; I.2.9"
  ],
  "published": "2007-03-28",
  "updated": "2007-03-28"
}