{
  "id": "http://arxiv.org/abs/0707.3972v1",
  "title": "Learning Probabilistic Models of Word Sense Disambiguation",
  "authors": [
    "Ted Pedersen"
  ],
  "abstract": "This dissertation presents several new methods of supervised and unsupervised\nlearning of word sense disambiguation models. The supervised methods focus on\nperforming model searches through a space of probabilistic models, and the\nunsupervised methods rely on the use of Gibbs Sampling and the Expectation\nMaximization (EM) algorithm. In both the supervised and unsupervised case, the\nNaive Bayesian model is found to perform well. An explanation for this success\nis presented in terms of learning rates and bias-variance decompositions.",
  "text": "arXiv:0707.3972v1  [cs.CL]  26 Jul 2007\nLEARNING PROBABILISTIC MODELS\nOF WORD SENSE DISAMBIGUATION\nApproved by:\nDr. Dan Moldovan\nDr. Rebecca Bruce\nDr. Weidong Chen\nDr. Frank Coyle\nDr. Margaret Dunham\nDr. Mandyam Srinath\nLEARNING PROBABILISTIC MODELS\nOF WORD SENSE DISAMBIGUATION\nA Dissertation Presented to the Graduate Faculty of the\nSchool of Engineering and Applied Science\nSouthern Methodist University\nin\nPartial Fulﬁllment of the Requirements\nfor the degree of\nDoctor of Philosophy\nwith a\nMajor in Computer Science\nby\nTed Pedersen\n(B.A., Drake University)\n(M.S., University of Arkansas)\nMay 16, 1998\nACKNOWLEDGMENTS\nI am indebted to Dr. Rebecca Bruce for sharing freely of her time, knowledge,\nand insights throughout this research. Certainly none of this would have been possible\nwithout her.\nDr. Weidong Chen, Dr. Frank Coyle, Dr. Maggie Dunham, Dr. Dan Moldovan,\nand Dr. Mandyam Srinath have all made important contributions to this dissertation.\nThey are also among the main reasons why my time at SMU has been both happy\nand productive.\nI am also grateful to Dr. Janyce Wiebe, Lei Duan, Mehmet Kayaalp, Ken McK-\neever, and Tom O’Hara for many valuable comments and suggestions that inﬂuenced\nthe direction of this research.\nThis work was supported by the Oﬃce of Naval Research under grant number\nN00014-95-1-0776.\niii\nPedersen, Ted\nB.A., Drake University\nM.S., University of Arkansas\nLearning Probabilistic Models\nof Word Sense Disambiguation\nAdvisor: Professor Dan Moldovan\nDoctor of Philosophy degree conferred May 16, 1998\nDissertation completed May 16, 1998\nSelecting the most appropriate sense for an ambiguous word is a common\nproblem in natural language processing. This dissertation pursues corpus–based ap-\nproaches that learn probabilistic models of word sense disambiguation from large\namounts of text.\nThese models consist of a parametric form and parameter esti-\nmates.\nThe parametric form characterizes the interactions among the contextual\nfeatures and the sense of the ambiguous word.\nParameter estimates describe the\nprobability of observing diﬀerent combinations of feature values. These models dis-\nambiguate by determining the most probable sense of an ambiguous word given the\ncontext in which it occurs.\nThis dissertation presents several enhancements to existing supervised methods\nof learning probabilistic models of disambiguation from sense–tagged text. A new\nsearch strategy, forward sequential, guides the selection process through the space\nof possible models. Each model considered for selection is judged by a new class of\nevaluation metric, the information criteria. The combination of forward sequential\nsearch and Akaike’s Information Criteria is shown to consistently select highly ac-\ncurate models of disambiguation. The same search strategy and evaluation criterion\nalso serve as the basis of the Naive Mix, a new supervised learning algorithm that\nis shown to be competitive with leading machine learning methodologies. In these\ncomparisons the Naive Bayesian classiﬁer also fares well which seems surprising since\nit is based on a model where the parametric form is simply assumed. However, an\niv\nexplanation for this success is presented in terms of learning rates and bias–variance\ndecompositions of classiﬁcation error.\nUnfortunately, sense–tagged text only exists in small quantities and is expensive\nto create. This substantially limits the portability of supervised learning approaches\nto word sense disambiguation. This bottleneck is addressed by developing unsuper-\nvised methods that learn probabilistic models from raw untagged text.\nHowever,\nsuch text does not contain enough information to automatically select a parametric\nform. Instead, one must simply be assumed. Given a form, the senses of ambiguous\nwords are treated as missing data and their values are imputed via the Expecta-\ntion Maximization algorithm and Gibbs Sampling. Here the parametric form of the\nNaive Bayesian classiﬁer is employed. However, this methodology is appropriate for\nany parametric form in the class of decomposable models.\nSeveral local–context,\nfrequency–based feature sets are also developed and shown to be appropriate for\nunsupervised learning of word senses from raw untagged text.\nv\nTABLE OF CONTENTS\nACKNOWLEDGMENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niii\nLIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nx\nLIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\nCHAPTER\n1. INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1. Word Sense Disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2. Learning from Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2.1.\nSupervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.2.2.\nUnsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.3. Basic Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.4. Chapter Summaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2. PROBABILISTIC MODELS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.1. Inferential Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.1.1.\nMaximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.1.2.\nBayesian Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.2. Decomposable Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.2.1.\nExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.2.2.\nDecomposable Models as Classiﬁers. . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3. SUPERVISED LEARNING FROM SENSE–TAGGED TEXT . . . . . . . . . . .\n24\n3.1. Sequential Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.1.1.\nSearch Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.1.2.\nEvaluation Criteria. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nvi\n3.1.2.1.\nSigniﬁcance Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.1.2.2.\nInformation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n3.1.3.\nExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n3.1.3.1.\nFSS AIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n3.1.3.2.\nBSS AIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.2. Naive Mix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n3.3. Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n4. UNSUPERVISED LEARNING FROM RAW TEXT . . . . . . . . . . . . . . . . . . . .\n45\n4.1. Probabilistic Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n4.1.1.\nEM Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n4.1.1.1.\nGeneral Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n4.1.1.2.\nNaive Bayes description . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4.1.1.3.\nNaive Bayes example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n4.1.2.\nGibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n4.1.2.1.\nGeneral Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n4.1.2.2.\nNaive Bayes description . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n4.1.2.3.\nNaive Bayes example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n4.2. Agglomerative Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n4.2.1.\nWard’s minimum–variance method . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n4.2.2.\nMcQuitty’s similarity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n5. EXPERIMENTAL DATA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n5.1. Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n5.2. Feature Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n5.2.1.\nSupervised Learning Feature Set. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\nvii\n5.2.2.\nUnsupervised Learning Feature Sets . . . . . . . . . . . . . . . . . . . . . . . .\n80\n5.2.3.\nFeature Sets and Event Distributions . . . . . . . . . . . . . . . . . . . . . . .\n83\n6. SUPERVISED LEARNING EXPERIMENTAL RESULTS . . . . . . . . . . . . . .\n92\n6.1. Experiment 1: Sequential Model Selection . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n6.1.1.\nOverall Accuracy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n6.1.2.\nModel Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n6.1.3.\nModel Selection as a Robust Process. . . . . . . . . . . . . . . . . . . . . . . .\n96\n6.1.4.\nModel selection for Noun interest . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n6.2. Experiment 2: Naive Mix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3. Experiment 3: Learning Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n6.4. Experiment 4: Bias Variance Decomposition . . . . . . . . . . . . . . . . . . . . . . . 113\n7. UNSUPERVISED LEARNING EXPERIMENTAL RESULTS . . . . . . . . . . . 119\n7.1. Assessing Accuracy in Unsupervised Learning . . . . . . . . . . . . . . . . . . . . . . 120\n7.2. Analysis 1: Probabilistic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n7.2.1.\nMethodological Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n7.2.2.\nFeature Set Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n7.3. Analysis 2: Agglomerative Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n7.3.1.\nMethodological Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.3.2.\nFeature Set Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.4. Analysis 3: Gibbs Sampling and McQuitty’s Similarity Analysis . . . 145\n8. RELATED WORK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n8.1. Semantic Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n8.2. Machine Readable Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n8.3. Parallel Translations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\nviii\n8.4. Sense–Tagged Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8.5. Raw Untagged Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n9. CONCLUSIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n9.1. Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n9.1.1.\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n9.1.2.\nFuture Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n9.2. Unsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n9.2.1.\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\n9.2.2.\nFuture Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\nREFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\nix\nLIST OF FIGURES\nFigure\nPage\n2.1. Saturated Model (CV RTS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.2. Decomposable Model (CSV )(RST) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.3. Model of Independence (C)(V )(R)(T)(S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.4. Naive Bayes Model (CS)(RS)(TS)(V S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.1. E–Step Iteration 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n4.2. M–Step Iteration 1: ˆp(S), ˆp(F1|S), ˆp(F2|S) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n4.3. E–Step Iteration 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n4.4. E–Step Iteration 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n4.5. M–Step Iteration 2: ˆp(S), ˆp(F1|S), ˆp(F2|S) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n4.6. E–Step Iteration 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n4.7. E–Step Iteration 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n4.8. Stochastic E–Step Iteration 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n4.9. Stochastic M–step Iteration 1: ˆp(S), ˆp(F1|S), ˆp(F2|S) . . . . . . . . . . . . . . . . . .\n65\n4.10. E–Step Iteration 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n4.11. Stochastic E–Step Iteration 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n4.12. Stochastic M–step Iteration 2: ˆp(S), ˆp(F1|S), ˆp(F2|S) . . . . . . . . . . . . . . . . . .\n68\n4.13. Stochastic E–Step Iteration 3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n4.14. Stochastic E–Step Iteration 3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n4.15. Matrix of Feature Values, Dissimilarity Matrix . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nx\n6.1. Robustness of Selection Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n6.2. BSS (top) and FSS (bottom) accuracy for Noun interest . . . . . . . . . . . . . . . 100\n6.3. BSS (top) and FSS (bottom) recall for Noun interest . . . . . . . . . . . . . . . . . . 101\n6.4. Learning Rate for Adjectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.5. Learning Rate for Nouns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.6. Learning Rate for Verbs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n6.7. Classiﬁcation Error Correlation, m=400 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.8. Bias Correlation, m=400 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.9. Variance Correlation, m=400 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n7.1. Human Labeled Senses for line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n7.2. Unlabeled Sense Groups for line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n7.3. Thirty Percent Accuracy Mapping of line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n7.4. Seventy Percent Accuracy Mapping of line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n7.5. Probabilistic Model Correlation of Accuracy for all words . . . . . . . . . . . . . . 126\n7.6. Probabilistic Model Correlation of Accuracy for Nouns . . . . . . . . . . . . . . . . . 126\n7.7. concern - Feature Set A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n7.8. interest - Feature Set B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n7.9. help - Feature Set C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.10. Agglomerative Clustering Correlation of Accuracy for all words . . . . . . . . 137\n7.11. Agglomerative Clustering Correlation of Accuracy for Nouns . . . . . . . . . . . 137\n7.12. concern - Feature Set A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.13. interest - Feature Set B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.14. help - Feature Set C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n7.15. Gibbs and McQuitty’s Correlation of Accuracy for all words. . . . . . . . . . . . 147\n7.16. Gibbs and McQuitty’s Correlation of Accuracy for Adjectives . . . . . . . . . . 147\nxi\n7.17. Gibbs and McQuitty’s Correlation of Accuracy for Nouns . . . . . . . . . . . . . . 148\n7.18. Gibbs and McQuitty’s Correlation of Accuracy for Verbs . . . . . . . . . . . . . . . 148\n8.1. Simple Semantic Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nxii\nLIST OF TABLES\nTable\nPage\n2.1. Maximum Likelihood Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.2. Sense–tagged text for bill . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.1. Model Selection Example Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n3.2. Model Selection Example: FSS AIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.3. Model Selection Example: BSS AIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n3.4. Sequence of Models for Naive Mix created with FSS . . . . . . . . . . . . . . . . . . .\n41\n4.1. Unsupervised Learning Example Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n5.1. Adjective Senses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n5.2. Noun Senses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n5.3. Verb Senses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n5.4. Supervised Co–occurrence features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n5.5. Unsupervised Co–occurrence Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n5.6. Event Distribution for Adjective chief . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n5.7. Event Distribution for Adjective common . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n5.8. Event Distribution for Adjective last. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.9. Event Distribution for Adjective public . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.10. Event Distribution for Noun bill . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n5.11. Event Distribution for Noun concern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n5.12. Event Distribution for Noun drug. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\nxiii\n5.13. Event Distribution for Noun interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n5.14. Event Distribution for Noun line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n5.15. Event Distribution for Verb agree. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n5.16. Event Distribution for Verb close . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n5.17. Event Distribution for Verb help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n5.18. Event Distribution for Verb include . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n6.1. Sequential Model Selection Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n6.2. Complexity of Selected Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n6.3. Naive Mix and Machine Learning Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.4. Naive Bayes Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.5. Bias Variance Estimates, m = 400 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n7.1. Unsupervised Accuracy of EM and Gibbs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n7.2. Unsupervised Accuracy of Agglomerative Clustering . . . . . . . . . . . . . . . . . . . 136\n7.3. Unsupervised Accuracy of Gibbs and McQuitty’s . . . . . . . . . . . . . . . . . . . . . . 146\nxiv\nCHAPTER 1\nINTRODUCTION\nThis dissertation is about computational methods that resolve the meanings of\nambiguous words in natural language text. Here, disambiguation is deﬁned as the\nselection of the intended sense of an ambiguous word from a known and ﬁnite set of\npossible meanings. This choice is based upon a probabilistic model that tells which\nmember of the set of possible meanings is the most likely given the context in which\nthe ambiguous word occurs.\nResolving ambiguity is a routine process for a human; it requires little conscious\neﬀort since a broad understanding of both language and the real–world are utilized\nto make decisions about the intended sense of a word. For a human, the context in\nwhich an ambiguous word occurs includes a wealth of knowledge beyond that which\nis contained in the text. Modeling this vast amount of information in a representation\na computer program can access and make inferences from is an, as yet, unachieved\ngoal of Artiﬁcial Intelligence. Given the lack of such resources, this dissertation does\nnot attempt to duplicate the process a human uses to resolve ambiguity.\nInstead, corpus–based methods are employed which make disambiguation deci-\nsions based on probabilistic models learned from large quantities of naturally occur-\nring text. In these approaches, context is deﬁned in a very limited way and consists\nof information that can easily be extracted from the sentence in which an ambiguous\nword occurs; no deep understanding of the linguistic structure or real–world under-\npinnings of a text is required. This results in methods that take advantage of the\nabundance of text available online and do not require the availability of rich sources\nof real–world knowledge.\n1\n1.1. Word Sense Disambiguation\nMost words have multiple possible senses, each of which is appropriate in certain\ncontexts. Such ambiguity can result in the misunderstanding of a sentence. For ex-\nample, the newspaper headline Drunk Gets 9 Years in Violin Case causes momentary\nconfusion due to word sense ambiguity. Does this imply that someone has been sen-\ntenced to spend 9 years in a box used to store a musical instrument? Or has someone\nhas been sentenced to prison for 9 years for a crime involving a violin? Clearly the\nlatter interpretation is intended. The key to making this determination is resolving\nthe intended sense of case. This is not terribly diﬃcult for a human since it is widely\nknown that people are not imprisoned in violin cases. However, a computer program\nthat attempts to resolve this same ambiguity will have a more challenging task since\nit is not likely to have this particular piece of knowledge available.\nThe diﬃculty of resolving word sense ambiguity with a computer program was\nﬁrst noted by Yehoshua Bar–Hillel, an early researcher in machine translation. In [3]\nhe presented the following example:\nLittle John was looking for his toy box. Finally, he found it. The box was\nin the pen. John was very happy.\nBar-Hillel assumed that pen can have two senses: a writing instrument or an enclosure\nwhere small children can play. He concluded that:\n. . . no existing or imaginable program will enable an electronic computer\nto determine that the word pen in the given sentence within the given\ncontext has the second of the above meanings.\nDisambiguating pen using a knowledge–based approach requires rather esoteric\npieces of information; “toy boxes are smaller than play pens” and “toy boxes are\nlarger than writing pens,” plus some mechanism for making inferences given these\nfacts. To have this available for all potential ambiguities is indeed an impossibility.\nIn that regard Bar–Hillel is correct.\nHowever, while such approaches require an\n2\nimpractical amount of real–world knowledge, corpus–based methods that learn from\nlarge amounts of naturally occurring text oﬀer a viable alternative.\nComputational approaches that automatically perform word sense disambigua-\ntion have potentially wide application. Resolving ambiguity is an important issue\nin machine translation, document categorization, information retrieval, and language\nunderstanding.\nConsider an example from machine translation. The noun bill can refer to a\npiece of legislation that is not yet law or to a statement requesting payment for services\nrendered. However, in Spanish these two senses of bill have two distinct translations;\nproyecto de ley and cuenta. To translate The Senate bill is being voted on tomorrow\nfrom English to Spanish, the intended sense of bill must be resolved. Even a simple\nword by word translation to Spanish is not possible without resolving this ambiguity.\nDocument classiﬁcation can also hinge upon the interpretation of an ambiguous\nword.\nSuppose that there are two documents where the word bill occurs a large\nnumber of times. If a classiﬁcation decision is made based on this fact and the sense\nof bill is not known, it is possible that Peterson’s Field Guide to North American\nBirds and the Federal Register will be considered the same type of document as both\ncontain frequent usages of bill.\n1.2. Learning from Text\nThis dissertation focuses on corpus–based approaches to learning probabilistic\nmodels that resolve the meaning of ambiguous words. These models indicate which\nsense of an ambiguous word is most probable given the context in which it occurs. In\nthis framework disambiguation consists of classifying an ambiguous word into one of\nseveral predetermined senses.\nThese probabilistic models are learned via supervised and unsupervised ap-\nproaches. If manually disambiguated examples are available to serve as training data\nthen supervised learning is most eﬀective. These examples take the form of sense–\ntagged text which is created by collecting a large number of sentences that contain\n3\na particular ambiguous word. Each instance of the ambiguous word is manually an-\nnotated to indicate the most appropriate sense for that usage. Supervised learning\nbuilds a generalized model from this set of examples and uses this model to disam-\nbiguate instances of the ambiguous word found in test data that is separate from the\ntraining data.\nIf there are no training examples available then learning is unsupervised and is\nbased upon raw or untagged text. An unsupervised algorithm divides all the usages\nof an ambiguous word into a speciﬁed number of groups based upon the context in\nwhich each instance of the word occurs. There is no separation of the data into a\ntraining and test sample.\nBefore either kind of learning can take place, a feature set must be developed.\nThis deﬁnes the context of the ambiguous word and consists of those properties of both\nthe ambiguous word and the sentence in which it occurs that are relevant to making a\nsense distinction. These properties are generally referred to as contextual features or\nsimply features. Human intuition and linguistic insight are certainly desirable at this\nstage. The development of a feature set is a subjective process; given the complexity\nof human language there are a huge number of possible contextual features and it is\nnot possible to empirically examine even a fraction of them. This dissertation uses\nan existing feature set for supervised learning and develops several new feature sets\nappropriate for unsupervised learning.\nRegardless of whether a probabilistic model is learned via supervised or unsu-\npervised techniques, the nature of the resulting model is the same. These models\nconsist of a parametric form and parameter estimates. The parametric form shows\nwhich contextual features aﬀect the values of other contextual features as well as\nwhich contextual features aﬀect the sense of the ambiguous word. The parameter\nestimates tell how likely certain combinations of values for the contextual features\nare to occur with a particular sense of an ambiguous word.\nThus, there are two steps to learning a probabilistic model of disambiguation.\nFirst, the parametric form must either be speciﬁed by the user or learned from sense–\n4\ntagged text.\nSecond, parameter estimates are made based upon evidence in the\ntext. The following sections summarize how each of these steps is performed during\nsupervised and unsupervised learning.\nMore details of the learning processes are\ncontained in Chapters 3 and 4. Empirical evaluation of these methods is presented\nin Chapters 6 and 7.\n1.2.1. Supervised Learning\nThe supervised approaches in this dissertation generally follow the model se-\nlection method introduced by Bruce and Wiebe (e.g, [10], [11], and [12]).\nTheir\nmethod learns both the parametric form and parameter estimates of a special class of\nprobabilistic models, decomposable log–linear models. This dissertation extends their\napproach by identifying alternative criteria for evaluating the suitability of a model\nfor disambiguation and also identiﬁes an alternative strategy for searching the space\nof possible models.\nThe approach of Bruce and Wiebe and the extensions described in this disser-\ntation all have the objective of learning a single probabilistic model that adequately\ncharacterizes a training sample for a given ambiguous word. However, this disserta-\ntion shows that diﬀerent models selected by diﬀerent methodologies often result in\nsimilar levels of disambiguation performance. This suggests that model selection is\nsomewhat uncertain and that a single “best” model may not exist for a particular\nword. A new variation on the sequential model selection methodology, the Naive Mix,\nis introduced and addresses this type of uncertainly. The Naive Mix is an averaged\nprobabilistic model that is based on an entire sequence of models found during a se-\nlection process rather than just a single model. Empirical comparison shows that the\nNaive Mix improves on the disambiguation performance of a single selected model\nand is competitive with leading machine learning algorithms.\nThe Naive Bayesian classiﬁer is a supervised learning method where the para-\nmetric form is assumed and only parameter estimates are learned from sense–tagged\ntext. Despite some history of success in word sense disambiguation and other appli-\n5\ncations, the behavior of Naive Bayes has been poorly understood. This dissertation\nincludes an analysis that oﬀers an explanation for its ability to perform at relatively\nhigh levels of accuracy.\n1.2.2. Unsupervised Learning\nA general limitation of supervised learning approaches to word sense disam-\nbiguation is that sense–tagged text is not available for most domains. While sense–\ntagged text is not as complicated to create as more elaborate representations of real–\nworld knowledge, it is still a time–consuming activity and limits the portability of\nmethods that require it. In order to overcome this diﬃculty, this dissertation develops\nknowledge–lean approaches that learn probabilistic models from raw untagged text.\nRaw text only consists of the words and punctuation that normally appear in\na document; there are no manually attached sense distinctions to ambiguous words\nnor is any other kind of information augmented to the raw text. Even without sense–\ntagged text it is still possible to learn a probabilistic model using an unsupervised\napproach. In this case the parametric form must be speciﬁed by the user and then\nparameter estimates can be made from the text. Based on its success in supervised\nlearning, this dissertation uses the parametric form of the Naive Bayesian classiﬁer\nwhen performing unsupervised learning of probabilistic models. However, estimating\nparameters is more complicated in unsupervised learning than in the supervised case.\nThe parametric form of any probabilistic model of disambiguation must include\na feature representing the sense of the ambiguous word; however, raw text contains no\nvalues for this feature. The sense is treated as a latent or missing feature. Two diﬀer-\nent approaches to estimating parameters given missing data are evaluated; the EM al-\ngorithm and Gibbs Sampling. The probabilistic models that result are also compared\nto two well–known agglomerative clustering algorithms, Ward’s minimum–variance\nmethod and McQuitty’s similarity analysis. The application of these methodologies\nto word sense sense disambiguation is an important development since it eliminates\nthe requirement for sense–tagged text made by supervised learning algorithms.\n6\n1.3. Basic Assumptions\nThere are several assumptions that underly both the supervised and unsuper-\nvised approaches to word sense disambiguation presented in this dissertation:\n1. A separate probabilistic model is learned for each ambiguous word.\n2. Any part–of–speech ambiguity is resolved prior to sense disambiguation.1\n3. Contextual features are only deﬁned within the boundaries of the sentence in\nwhich an ambiguous word occurs. In other words, only information that occurs\nin the same sentence is used to resolve the meaning of an ambiguous word.\n4. The possible senses of a word are deﬁned by a dictionary and are known prior\nto disambiguation. In this dissertation Longman’s Dictionary of Contemporary\nEnglish [75] and WordNet [60] are the sources of word meanings.\nThe relaxation or elimination of any of these assumptions presents opportunities\nfor future work that will be discussed further in Chapter 9.\n1.4. Chapter Summaries\nChapter 2 develops background material regarding probabilistic models and\ntheir use as classiﬁers. Particular emphasis is placed on the class of decomposable\nmodels since they are used throughout this dissertation.\nChapter 3 discusses supervised learning approaches to word sense disambigua-\ntion. The statistical model selection method of Bruce and Wiebe is outlined here and\nalternatives to their model evaluation criteria and search strategy are presented. The\nNaive Mix is introduced. This is a new supervised learning algorithm that extends\nmodel selection from a process that selects a single probabilistic model to one that\nﬁnds an averaged model based on a sequence of probabilistic models. Each succeeding\n1For example, share can be used as a noun, I have a share of stock, or as a verb, It would be nice\nto share your stock.\n7\nmodel in the sequence characterizes the training data increasingly well. The Naive\nBayesian classiﬁer is also presented.\nChapter 4 addresses unsupervised learning of word senses from raw, untagged\ntext. This chapter shows how the EM algorithm and Gibbs Sampling can be em-\nployed to estimate the parameters of a model given the parametric form and the\nsystematic absence of data; in this case the sense of an ambiguous word is treated as\nmissing data. Two agglomerative clustering algorithms, Ward’s minimum–variance\nmethod and McQuitty’s similarity analysis, are also presented and used as points of\ncomparison.\nChapter 5 describes the words that are disambiguated as part of the empirical\nevaluation of the methods described in Chapters 3 and 4. The possible senses for each\nword are deﬁned and an empirical study of the distributional characteristics of each\nword is presented. Four feature sets are also discussed. The feature set for supervised\nlearning is due to Bruce and Wiebe. There are three new feature sets introduced for\nunsupervised learning.\nChapter 6 presents an empirical evaluation of the supervised learning algorithms\ndescribed in Chapter 3. There are four principal experiments. The ﬁrst compares\nthe overall accuracy of a range of sequential model selection methods. The second\ncompares the accuracy of the Naive Mix to several leading machine learning algo-\nrithms. The third determines the learning rate of the most accurate methods from\nthe ﬁrst two experiments. The fourth decomposes the classiﬁcation errors of the most\naccurate methods into more fundamental components.\nChapter 7 makes several comparisons among the unsupervised learning meth-\nods presented in Chapter 5. The ﬁrst is between the accuracy of probabilistic models\nwhere the parametric form is assumed and parameter estimates are made via the EM\nalgorithm and Gibbs Sampling. The second employs two agglomerative clustering al-\ngorithms, Ward’s minimum–variance method and McQuitty’s similarity analysis, and\ndetermines which is the more accurate. Finally, the two most accurate approaches,\nGibbs Sampling and McQuitty’s similarity analysis, are compared.\n8\nChapter 8 reviews related work in word sense disambiguation. Methodologies\nare grouped together based upon the type of knowledge source or data they require to\nperform disambiguation. There are discussions of work based on semantic networks,\nmachine readable dictionaries, parallel translations of text, sense–tagged text, and\nraw untagged text.\nChapter 9 summarizes the contributions of this dissertation and provides a\ndiscussion of future research directions.\n9\nCHAPTER 2\nPROBABILISTIC MODELS\nThis chapter introduces the basics of probabilistic models and shows how such\nmodels can be used as classiﬁers to perform word sense disambiguation. Particular\nattention is paid to a special class of probabilistic model known as decomposable\nlog–linear models [26] since they are well suited for use with the supervised and\nunsupervised learning methodologies described in Chapters 3 and 4.\n2.1. Inferential Statistics\nThe purpose of inferential statistics is to learn something about a population\nof interest. The characteristics of a population are described by parameters. Since\nit is generally not possible to exhaustively study a population, estimated values for\nparameters are learned from randomly selected samples of data from the population.\nEach parameter is associated with a distinct event that can occur in the popu-\nlation. An event is the state of a process at a particular moment in time. A common\nexample is coin tossing. This is a binomial process since there are only two possible\nevents; the coin toss comes up heads or tails. A process with more than two possible\nevents is multinomial. Tossing a die is an example since there are 6 possible events.\nThe events in this dissertation are sentences in which an ambiguous word occurs.\nEach sentence is represented by a combination of discrete values for a set of random\nvariables. Each random variable represents a property or feature of the sentence.\nThe dependencies among these features are characterized by the parametric form of\na probabilistic model.\nA feature vector is a particular instantiation of the random variables.\nEach\nfeature vector represents an observation or an instance of an event, i.e., a sentence\n10\nwith an ambiguous word. The exhaustive collection of all possible events given a set\nof feature variables deﬁnes the event space.\nThe joint probability distribution of a set of feature variables indicates how likely\neach event in the event space is to occur. The probability of observing a particular\nevent is described by a parameter. In addition to the parametric form, a probabilistic\nmodel also includes estimated values for all of these parameters.\nSuppose that in a random sample of events from a population there are N\nobservations of q distinct events, i.e., feature vectors, where each observation is de-\nscribed by n discrete feature variables (F1, F2, . . . , Fn−1, Fn). Let fi and θi be the\nfrequency and probability of the ith feature vector, respectively. Then the data sam-\nple D = (f1, f2, . . . , fq) has a multinomial distribution with parameters (N; Θ), where\nΘ = (θ1, θ2, . . . , θq) deﬁnes the joint probability distribution of the feature variables\n(F1, F2, . . . , Fn−1, Fn).\nThe parameters of a probabilistic model can be estimated using a number of\napproaches; maximum likelihood and Bayesian estimation are described in the follow-\ning sections. The model selection methodologies described in Chapter 3 and the EM\nalgorithm from Chapter 4 employ maximum likelihood estimates. Gibbs Sampling,\nalso described in Chapter 4, makes use of Bayesian estimates.\n2.1.1. Maximum Likelihood Estimation\nValues for the parameters of a probabilistic model can be estimated using max-\nimum likelihood estimates such that ˆθi = fi\nN . In this framework, a parameter can only\nbe estimated if the associated event is observed in a sample of data.\nA maximum likelihood estimate maximizes the probability of obtaining the data\nsample that was observed, D, by maximizing the likelihood function, p(D|Θ). The\nlikelihood function for a multinomial distribution is deﬁned as follows:1\n1Other distributions will have diﬀerent formulations of the likelihood function.\n11\np(D|Θ) =\nN!\nQq\ni=1 fi!\nq\nY\ni=1\nˆθfi\ni\n(2.1)\nImplicit in the multinomial distribution is the assumption that all the features of\nan event are dependent. When this is the case the value of any single feature variable\nis directly aﬀected by the values of all the other feature variables. A probabilistic\nmodel where all features are dependent is considered saturated.\nThe danger of relying on a saturated probabilistic model is that reliable param-\neter estimates may be diﬃcult to obtain. When using maximum likelihood estimates,\nany event that is not observed in the data sample will have a zero–valued parameter\nestimate associated with it. This is undesirable since the model regards the associated\nevent as an impossibility. It is more likely that the event is simply unusual and that\nthe sample is not large enough to gather adequate information regarding rare events\nwhen using a saturated model.\nHowever, if the event space is very small it may be reasonable to assume that\nall feature variables are dependent on one another and that every possible event\ncan be observed in a data sample. For example, if an event space is deﬁned by two\nbinary feature variables, (F1, F2), then the saturated model has four parameters, each\nrepresenting the probability of observing one of the four possible events. Table 2.1\nshows a scenario where a sample consists of N = 150 events. The frequency counts of\nthese events are shown in column freq(F1, F2), and the resulting maximum likelihood\nestimates are calculated and displayed in column MLE.\nIt is more often the case in real world problems that the number of possible\nevents is somewhat larger than four. The number of parameters needed to represent\nthese events in a probabilistic model is determined by the number of dependencies\namong the feature variables. If the model is saturated then all of the features are\ndependent on one another and the number of parameters in the probabilistic model\nis equal to the number of possible events in the event space.\n12\nTable 2.1. Maximum Likelihood Estimates\nF1\nF2\nfreq(F1, F2)\nMLE\n0\n0\n21\nˆθ1 =\n21\n150 = .14\n0\n1\n38\nˆθ2 =\n38\n150 = .25\n1\n0\n60\nˆθ3 =\n60\n150 = .40\n1\n1\n31\nˆθ4 =\n31\n150 = .21\nSuppose that an event space is deﬁned by a set of 20 binary feature variables\n(F1, F2, · · ·, F20). The joint probability distribution of this feature set consists of 220\nparameters. Unless the number of observations in the data sample is greater than 220,\nit is inevitable that there will be a great many parameter estimates with zero values.\nIf q < 220, where q represents the number of distinct events in a sample, then 220 −q\nevents will have zero estimates. This situation is exacerbated if the distribution of\nevents in the data sample is skewed, i.e., q ≪N. Unfortunately, it is often the case\nin natural language that the distribution of events is quite skewed (e.g. [74], [101]).\nAn alternative to using a saturated model is to ﬁnd a probabilistic model with\nfewer dependencies among the feature variables that still maintains a good ﬁt to the\ndata sample. Such a model is more parsimonious and yet retains a reasonably close\ncharacterization of the data. Given such a model, the joint probability distribution\ncan be expressed in terms of a smaller number of parameters.\nDependencies among feature variables can be eliminated if a pair of variables are\nidentiﬁed as conditionally independent. Feature variables F1 and F2 are conditionally\nindependent given S if:\np(F1 = f1|F2 = f2, S = s)\n=\np(F1 = f1|S = s)\n(2.2)\nor:\n13\np(F2 = f2|F1 = f1, S = s)\n=\np(F2 = f2|S = s)\n(2.3)\nIn Equation 2.2, the probability of observing feature variable F1 with value f1\nis not aﬀected by the value of feature variable F2 if it is already known that feature\nvariable S has value s. A similar interpretation applies to Equation 2.3.2\nAn automatic method for selecting probabilistic models with fewer dependencies\namong the feature variables is described by Bruce and Wiebe (e.g., [10], [11], [12]).\nThis method selects models from the class of decomposable log–linear models and\nwill be described in greater detail in Chapter 3.\n2.1.2. Bayesian Estimation\nBayesian estimation of parameters is an alternative to maximum likelihood es-\ntimation. Such an estimate is the product of the likelihood function, p(D|Θ), and\nthe prior probability, p(Θ). This product deﬁnes the posterior probability function,\np(Θ|D), deﬁned by Bayes Rule as:\np(Θ|D) = p(D|Θ)p(Θ)\np(D)\n(2.4)\nThe posterior function represents the probability of estimating the parameters,\nΘ, given the observed sample, D. The likelihood function, p(D|Θ), represents the\nprobability of observing the sample, D, given that it comes from the population\ncharacterized by the parameters, Θ. The prior probability function, p(Θ), represents\nthe unconditional probability that the parameters have values Θ. This is a subjective\nprobability that is estimated prior to sampling. Finally, p(D) is the probability of\nobserving a sample, D, regardless of the actual value of the parameters, Θ.\n2In the remainder of this dissertation, a simpliﬁed notation will be employed where feature\nvariable names are not speciﬁed when they can be inferred from the feature values. For example,\nin p(f1|f2, s) = p(f1|s) it is understood that the lower case letters refer to particular values for a\nfeature variable of the same name.\n14\nWhen making a Bayesian estimate some care must be taken in specifying the\ndistribution of the prior probability p(Θ).\nThe nature of the likelihood function\nmust be taken into account, otherwise the product of the likelihood function and the\nprior function may lead to invalid results. Prior probabilities whose distributions lend\nthemselves to fundamentally sound computation of the posterior probability from the\nlikelihood function are known as conjugate priors. A prior probability is a conjugate\nprior if it is related to the events represented by the likelihood function in such a\nway that both the posterior and prior probabilities are members of the same family\nof distributions.\nFor example, suppose a binomial process such as coin tossing is being modeled,\nwhere the observations in a sample are classiﬁed into two mutually exclusive cate-\ngories; heads or tails. The beta distribution is known to be conjugate to observations\nin a binomial process. If the prior probability of observing a heads or tails is assigned\nvia a beta distribution, then the posterior probability will also be a member of the\nbeta family.\nThe multinomial distribution is the n–event generalization of the 2–event bi-\nnomial distribution. The Dirichlet distribution is the n–event generalization of the\n2–event beta distribution. Since the beta distribution is the conjugate prior of the\nbinomial distribution, it follows that the Dirichlet distribution is the conjugate prior\nof the multinomial distribution. When the likelihood function is multinomial and\nthe prior function is speciﬁed using the Dirichlet distribution, the resulting posterior\nprobability function is expressed in terms of the Dirichlet distribution.\n2.2. Decomposable Models\nDecomposable models [26] are a subset of the class of Graphical Models [93]\nwhich is in turn a subset of the class of log-linear models [5]. Decomposable models\ncan also be categorized as the class of models that are both Bayesian Networks [67]\nand Graphical Models. They were ﬁrst applied to natural language processing and\nword sense disambiguation by Bruce and Wiebe (e.g., [10], [11], [12]).\n15\nIn any Graphical Model, feature variables are either dependent or conditionally\nindependent of one another. The parametric form of these models have a graphical\nrepresentation such that each feature variable in the model is represented by a node in\nthe graph, and there is an undirected edge between each pair of nodes corresponding\nto dependent feature variables. Any two nodes that are not directly connected by an\nedge are conditionally independent given the values of the nodes on the path that\nconnects them.\nThe graphical representation of a decomposable model corresponds to an undi-\nrected chordal graph whose set of maximal cliques deﬁnes the joint probability dis-\ntribution of the model. A graph is chordal if every cycle of length four or more has a\nshortcut, i.e., a chord. A maximal clique is the largest set of nodes that are completely\nconnected, i.e., dependent.\nIn general, parameter estimates are based on suﬃcient statistics. These provide\nall the information from the data sample that is needed to estimate the value of a\nparameter. The suﬃcient statistics of the parameters of a decomposable model are\nthe marginal frequencies of the events represented by the feature variables that form\nmaximal cliques in the graphical representation. Each maximal clique is made up of a\nsubset of the feature variables that are all dependent. Together these features deﬁne a\nmarginal event space. The probability of observing any speciﬁc instantiation of these\nfeatures, i.e., a marginal event, is deﬁned by the marginal probability distribution.\nThe joint probability distribution of a decomposable model is expressed as the\nproduct of the marginal distributions of the variables in the maximal cliques of the\ngraphical representation, scaled by the marginal probability distributions of feature\nvariables common to two or more of these maximal sets. Because their joint distri-\nbutions have such closed–form expressions, the parameters of a decomposable model\ncan be estimated directly from the data sample without the need for an iterative\nﬁtting procedure as is required, for example, to estimate the parameters of maximum\nentropy models (e.g., [4]).\n16\nTable 2.2. Sense–tagged text for bill\nSense–tagged sentences\nFeature vectors\nC\nV\nR\nT\nS\nI paid the bill/pay at the restaurant.\nno\nno\nyes\nno\npay\nCongress overrode the veto of that bill/law.\nyes\nyes\nno\nno\nlaw\nCongress passed a new bill/law today.\nyes\nno\nno\nno\nlaw\nThe restaurant bill/pay does not include the tip.\nno\nno\nyes\nyes\npay\nThe bill/law was killed in committee.\nno\nno\nno\nno\nlaw\n2.2.1. Examples\nTo clarify these concepts, both the graphical representation and parameter es-\ntimates associated with several examples of decomposable models are presented in\nterms of a simple word sense disambiguation example. The task is to disambiguate\nvarious instances of bill by selecting one of two possible senses; a piece of pending\nlegislation or a statement requesting payment for services rendered.\nEach sentence containing bill is represented using ﬁve binary feature variables.\nThe classiﬁcation variable S represents the sense of bill.\nFour contextual feature\nvariables indicate whether or not a given word has occurred in the sentence with the\nambiguous use of bill. The presence or absence of Congress, veto, restaurant and tip,\nare represented by binary variables C, V, R and T, respectively. These variables have\na value of yes if the word occurs in the sentence and no if it does not.\nA sample of N sentences that contain bill is collected. The instances of bill\nare manually annotated with sense values by a human judge. These sense–tagged\nsentences are converted by a feature extractor into the feature vectors shown in Table\n2.2.\nGiven ﬁve binary feature variables, there are 32 possible events in the event\nspace. If the parametric form is the saturated model then there are also 32 parameters\nto estimate.\nFor this example the saturated model is notated (CV RTS) and its\n17\nC\nV\nR\nS\nT\nFigure 2.1. Saturated Model (CV RTS)\ngraphical representation is shown in Figure 2.1.\nThis model is decomposable as\nthere is a path of length one between any two feature variables in the graphical\nrepresentation.\nIn order to estimate values for all the parameters of the saturated model, every\npossible event must be observed in the sample data. Let the parameter estimate\nˆθF1,F2,...,Fn−1,S\nf1,f2,...,fn−1,s\nrepresent the probability that a feature vector (f1, f2, . . . , fn−1, s) is ob-\nserved in the data sample where each sentence is represented by the random variables\n(F1, F2, . . . , Fn−1, S). The parameter estimates of the saturated model are calculated\nas follows:\nˆθC,V,R,T,S\nc,v,r,t,s\n=\nˆp(c, v, r, t, s)\n=\nfreq(c, v, r, t, s)\nN\n(2.5)\nHowever, an alternative to the saturated model is to use the model selection\nprocess described in Chapter 3 to ﬁnd a more parsimonious probabilistic model that\ncontains only the most important dependencies among the feature variables. This\nmodel can then be used as a classiﬁer to disambiguate subsequent occurrences of the\nambiguous word.\nSuppose that the model selection process ﬁnds that the model (CSV )(RST),\nshown in Figure 2.2, is an adequate characterization of the data sample.\nThere\nare a number of properties of the model revealed in the graphical representation.\n18\nC\nV\nR\nS\nT\nFigure 2.2. Decomposable Model (CSV )(RST)\nFirst, it is a decomposable model since all cycles of length four or more have a\nchord.\nSecond, conditional independence relationships can be read oﬀthe graph.\nFor example, the values of features R and V are conditionally independent given the\nvalue of S; p(r|v, s) = p(r|s), or p(v|r, s) = p(v|s). Third, (CSV ) and (RST) are\nthe maximal cliques. The variables in each clique are all dependent and each clique\ndeﬁnes a marginal distribution. Each marginal distribution deﬁnes a marginal event\nspace with eight possible events. Thus the total number of parameters needed to\ndeﬁne the joint probability distribution reduces from 32 to 16 when using this model\nrather than the saturated model.\nThe maximum likelihood estimates for the parameters of the joint probability\ndistribution are expressed in terms of the parameters of the decomposable model. The\nsuﬃcient statistics of a decomposable model are the marginal frequencies of the vari-\nables represented in the maximal cliques of the graphical representation. Given the\nparametric form (CSV )(RST), the suﬃcient statistics are the marginal frequencies\nfreq(c, s, v) and freq(r, s, t). The parameters of the decomposable model are ˆθC,S,V\nc,s,v\nand ˆθR,S,T\nr,s,t .\nThese represent the probability that the marginal events (c, s, v) and\n(r, s, t) will be observed in a data sample. These estimates are made by normalizing\nthe marginal frequencies by the sample size N:\n19\nˆθCSV\nc,s,v\n=\nˆp(c, s, v) =\nfreq(c, s, v)\nN\n(2.6)\nand\nˆθRST\nr,s,t\n=\nˆp(r, s, t) =\nfreq(r, s, t)\nN\n(2.7)\nEach parameter of the joint probability distribution can be expressed in terms of\nthese decomposable model parameters. The joint probability of observing the event\n(c, v, r, t, s) is expressed as the product of the marginal probabilities of observing\nmarginal events (c, s, v) and (r, s, t):\nˆθCV RTS\nc,v,r,t,s =\nˆθCSV\nc,s,v × ˆθRST\nr,s,t\nˆθS\ns\n(2.8)\nWhile the denominator ˆθS\ns represents an estimate of a marginal distribution, it is\nnot technically a parameter since it is completely determined by the numerator. The\ndenominator does not add any new information to the model, it simply factors out\nany marginal distributions that occur in more than one of the marginal distributions\nfound in the numerator.\nIn contrast to the saturated model, the model of independence assumes that\nthere are no dependencies among any of the feature variables. For this example the\nmodel of independence is notated (C)(V )(R)(T)(S) and the graphical representation\nis shown in Figure 2.3. This model has ﬁve maximal cliques, each containing one node\nand no dependencies. This deﬁnes ﬁve marginal distributions, each of which has two\npossible values. The number of parameters needed to deﬁne the joint probability\ndistribution is reduced to 10. These parameters are estimated as follows:\nˆθC,V,R,T,S\nc,v,r,t,s\n= ˆθC\nc × ˆθV\nv × ˆθR\nr × ˆθT\nt × ˆθS\ns\n(2.9)\n20\nC\nV\nR\nS\nT\nFigure 2.3. Model of Independence (C)(V )(R)(T)(S)\nThis model indicates that the probability of observing a particular value for a\nfeature variable is not inﬂuenced by the values of any of the other feature variables.\nNo features aﬀect the values of any other features. The model of independence is\ntrivially decomposable as there are no cycles in the graphical representation of the\nmodel.\nDespite its simplicity, the model of independence is used throughout the\nexperimental evaluation described in Chapter 6. It serves as the basis of the majority\nclassiﬁer, a probabilistic model that assigns the most frequent sense of an ambiguous\nword in a sample of data to every instance of the ambiguous word it subsequently\nencounters.\nThe Naive Bayesian classiﬁer [33] also plays a role later in this dissertation. This\nis a decomposable model that has a signiﬁcant history in natural language processing\nand a range of other applications. This model assumes that all of the contextual\nfeatures are conditionally independent given the value of the classiﬁcation variable.\nFor the example in this chapter, the parametric form of Naive Bayes is notated\n(CS)(RS)(TS)(V S) and has a graphical representation as shown in Figure 2.4. In\nthis model there are four maximal cliques, each with two nodes and one dependency.\nThe variables are binary so each of the four marginal distributions represents four\npossible events. The parameter estimates for Naive Bayes are computed as follows:\n21\nS\nC\nR\n T\nV\nFigure 2.4. Naive Bayes Model (CS)(RS)(TS)(V S)\nˆθC,V,R,T,S\nc,v,r,t,s\n=\nˆθC,S\nc,s × ˆθV,S\nv,s × ˆθR,S\nr,s × ˆθT,S\nt,s\nˆθSs × ˆθSs × ˆθSs\n(2.10)\nBy applying the following identities,\nˆθX,S\nx,s\n=\nˆp(x, s)\n=\nˆp(x|s) × ˆp(s)\nand\nˆθS\ns\n=\nˆp(s)\n(2.11)\nthe Naive Bayesian classiﬁer can also be expressed in its more traditional representa-\ntion:\nˆp(c, v, r, t, s) = ˆp(s) × ˆp(c|s) × ˆp(v|s) × ˆp(r|s) × ˆp(t|s)\n(2.12)\n2.2.2. Decomposable Models as Classiﬁers\nA probabilistic model consists of a parametric form that describes the depen-\ndencies among the features and parameter estimates that tell how likely each possible\nevent is to occur.\nSuch a model can be used as a classiﬁer to identify the most\nprobable sense of an ambiguous word given the context in which it appears.\nFor example, suppose a sentence that contains an ambiguous word is represented\nby the following feature vector:\n22\n(C = c, V = v, R = r, T = t, S =?)\n(2.13)\nThe variable S represents the sense of an ambiguous word. Variables C, V ,\nR, and T are the features that represent the context in which the ambiguous word\noccurs. The values of the feature variables are known while the value of S is unknown.\nGiven x possible values of S, where each possible sense is notated sx, there are\nx possible events associated with the incomplete feature vector in Equation 2.13.\nA probabilistic classiﬁer determines which of these possible events has the highest\nassociated probability according to the probabilistic model, i.e., it maximizes the\nprobability of S conditioned on the values of the observed feature variables.\nThus, disambiguation is performed via a simple maximization function. Given\nvalues for the observed contextual features, a probabilistic classiﬁer determines which\nvalue of S is associated with the most probable event:\nS =\nargmax\nsx\np(sx|c, v, r, t) =\nargmax\nsx\np(c, v, r, t, sx)\np(c, v, r, t)\n(2.14)\nThe denominator in Equation 2.14 acts as a constant since it does not include S.\nAs such it can be dropped and the maximization operation is simpliﬁed to ﬁnding the\nvalue of S that maximizes the joint probability distribution of the feature variables\nC, V, R, T, and S:\nS =\nargmax\nsx\np(c, v, r, t, sx) =\nargmax\nsx\nˆθC,V,R,T,S\nc,v,r,t,sx\n(2.15)\nThis chapter has shown how estimates of the joint probability distribution can\nbe expressed in terms of more parsimonious decomposable models. The following\nchapter shows how decomposable models can be automatically selected from sense–\ntagged text.\n23\nCHAPTER 3\nSUPERVISED LEARNING FROM SENSE–TAGGED TEXT\nWhen applied to classiﬁcation problems, supervised learning is a methodology\nwhere examples of properly classiﬁed events are used to train an algorithm that\nwill classify subsequent instances of similar events. For word sense disambiguation,\nmanually disambiguated usages of an ambiguous word serve as training examples.\nThis sense–tagged text is used to learn a probabilistic model that determines the\nmost probable sense of an ambiguous word, given the context in which it occurs.\nIn this dissertation, the objective of supervised learning is to select the paramet-\nric form of a decomposable model that represents the important dependencies among\na set of feature variables exhibited in a particular sample of sense–tagged text. Given\nthis form, the joint probability distribution of this set of feature variables can be ex-\npressed in terms of the marginal probability distributions of the decomposable model.\nOnce the values of these parameters are estimated, the probabilistic model is complete\nand can be used as a classiﬁer to perform disambiguation.\nThe challenge in learning probabilistic models is to locate a parametric form\nthat is both a speciﬁc representation of the important dependencies in the training\nsample and yet general enough to successfully disambiguate previously unobserved\ninstances of the ambiguous word. A model is too complex if a substantial number\nof parameters in the joint probability distribution have zero–valued estimates; this\nindicates that the available data sample simply does not contain enough information\nto support the estimates required by the model. However, a model is too simple if\nrelevant dependencies among features are not represented. In other words, the model\nshould achieve an appropriate balance between model complexity and model ﬁt.\n24\nThere are several supervised learning methodologies discussed in this chapter.\nSequential model selection learns a single parametric form that is judged to achieve\nthe best balance between model complexity and ﬁt for a given sample of sense–tagged\ntext. This methodology is extended by the Naive Mix, which learns an averaged prob-\nabilistic model from the sequence of parametric forms generated during a sequential\nmodel selection process. An alternative to learning the parametric form is to simply\nassume one. In this case no search of parametric forms is conducted; the form is\nspeciﬁed by the user and the sense–tagged text is only utilized to make parameter\nestimates. This is the methodology of the Naive Bayesian classiﬁer [33], often simply\nreferred to as Naive Bayes.\nThe degree to which a probabilistic model successfully balances complexity and\nﬁt is determined by its accuracy in disambiguating previously unobserved instances\nof an ambiguous word. The methods discussed in this chapter are subjected to such\nan evaluation in Chapter 6.\n3.1. Sequential Model Selection\nSequential model selection integrates a search strategy and an evaluation crite-\nrion. Since the number of possible parametric forms is exponential in the number of\nfeatures, an exhaustive search of the possible forms is usually not tractable. A search\nstrategy determines which parametric forms, from the set of all possible parametric\nforms, will be considered during the model selection process. The evaluation crite-\nrion is the ultimate judge of which parametric form achieves the most appropriate\nbalance between complexity and ﬁt, where complexity is deﬁned by the number of\ndependencies in the model, i.e., the number of edges in its graphical representation.\nThe search strategies employed here are greedy and result in the evaluation of\nmodels of steadily increasing or decreasing levels of complexity. A number of candidate\nmodels are generated at each level of complexity. The evaluation criterion determines\nwhich candidate model results in the best ﬁt to the training sample; this model is\ndesignated as the current model. Another set of candidate models is generated by\n25\nincreasing or decreasing the complexity of the current model by one dependency.\nThe process of evaluating candidates, selecting a current model, and generating new\ncandidate models from the current model is iterative and continues until a model is\nfound that achieves the best overall balance of complexity and ﬁt. This is the selected\nmodel and is the ultimate result of the sequential model selection process.\nA selected model is parsimonious in that it has as few dependencies as are nec-\nessary to characterize or ﬁt the training sample. When using maximum likelihood\nestimates, the saturated model exactly ﬁts the distribution of the observed events\nin the training sample. However, the number of parameters is equal to the number\nof events in the event space and obtaining non–zero estimates for large numbers of\nparameters is usually diﬃcult. A parsimonious model should capture the important\ndependencies among the features in a training sample and yet allow the joint proba-\nbility distribution to be expressed relatively simply in terms of a smaller number of\ndecomposable model parameters.\nAs formulated in this dissertation, the model selection process also performs\nfeature selection.\nIf a model is selected where there is no dependency between a\nfeature variable and the sense variable, then that feature is removed from the model\nand will not impact disambiguation.\n3.1.1. Search Strategy\nTwo sequential search strategies are employed in this dissertation: backward\nsequential search [92] and forward sequential search [28].\nThese methods are also\nknown as backward elimination and forward inclusion.\nBackward sequential search for probabilistic models of word sense disambigua-\ntion was introduced by Bruce and Wiebe (e.g., [10], [11], [12]). This dissertation\nintroduces forward sequential search. Forward searches evaluate models of increasing\ncomplexity based on how much candidate models improve upon the ﬁt of the current\nmodel, while backward searches evaluate candidate models based on how much they\ndegrade the ﬁt of the current model.\n26\nA forward sequential search begins by designating the model of independence\nas the current model. The level of complexity is zero since there are no edges in the\ngraphical representation of this model. The set of candidate models is generated from\nthe model of independence and consists of all possible one edge decomposable models.\nThese are individually evaluated for ﬁt by an evaluation criterion.\nThe one edge\nmodel that exhibits the greatest improvement in ﬁt over the model of independence\nis designated as the new current model. A new set of candidate models is generated by\nadding an edge to the current model and consists of all possible two edge decomposable\nmodels. These models are evaluated for ﬁt and the two edge decomposable model\nthat most improves on the ﬁt of the one edge current model becomes the new current\nmodel. A new set of three edge candidate models is generated by adding one edge at\na time to the two edge current model. This sequential search continues until:\n1. none of the candidate decomposable models of complexity level i + 1 results in\nan appreciable improvement in ﬁt over the current model of complexity level i,\nas deﬁned by the evaluation criterion, or\n2. the current model is the saturated model.\nIn either case the current model becomes the selected model and the search ends.\nIn general then, during a forward search the current model is reset to the de-\ncomposable model of complexity level i that most improves on the ﬁt of the current\ndecomposable model of complexity level i −1. All possible decomposable models of\ncomplexity level i+ 1 that are generated from the current model of complexity level i\nare considered as candidate models and then evaluated for ﬁt. The candidate model\nthat most improves on the ﬁt of the current model of complexity level i is designated\nthe new current model. This process continues until either there is no decomposable\nmodel of complexity level i + 1 that results in an appreciable improvement in ﬁt over\nthe current model or the current model of complexity level i is the saturated model.\nIn either case the current model is selected and the search ends.\n27\nFor the sparse and skewed samples typical of natural language data [101], for-\nward sequential search is a natural choice. Early in the search the models are of\nlow complexity and the number of parameters in the model is relatively small. This\nresults in few zero–valued estimates and ensures that the model selection process is\nbased upon the best available information from the training sample.\nA backwards sequential search begins by designating the saturated model as\nthe current model. If there are n feature variables then the number of edges in the\nsaturated model is n(n−1)\n2\n. As an example, given 10 feature variables there are 45\nedges in a saturated model. The set of candidate models consists of each possible\ndecomposable model with 44 edges generated by removing a single edge from the\nsaturated model. These candidates are evaluated for ﬁt and the 44 edge model that\nresults in the least degradation in ﬁt from the saturated model becomes the new\ncurrent model. Each possible 43 edge candidate decomposable model is generated by\nremoving a single edge from the 44 edge current model and then evaluated for ﬁt.\nThe 43 edge decomposable candidate model that results in the least degradation in\nﬁt from the 44 edge current model becomes the new current model. Each possible 42\nedge candidate decomposable model is generated by removing a single edge from the\ncurrent 43 edge model and then evaluated for ﬁt. This sequential search continues\nuntil:\n1. every candidate decomposable model of complexity level i −1 results in an\nappreciable degradation in ﬁt from the current model of complexity level i, as\ndeﬁned by the evaluation criterion, or\n2. the current model is the model of independence.\nIn either case the current model is selected and the search ends.\nIn general then, during a backward search the current model is reset to the\ndecomposable model of complexity level i that results in the least degradation in ﬁt\nfrom the current model of complexity level i + 1. Each possible decomposable model\nof complexity level i−1 is generated by removing a single edge from the current model\n28\nof complexity level i and evaluated for ﬁt. This process continues until either every\ndecomposable model of complexity level i−1 results in an appreciable degradation in\nﬁt from the current model of complexity level i or the current model has complexity\nlevel zero, i.e., the model of independence. In either case the current model is selected\nand the search ends.\nThe backward search in this dissertation diﬀers slightly from that of Bruce and\nWiebe. Their backward search begins with the saturated model and generates a se-\nries of models of steadily decreasingly complexity where the minimal or concluding\nmodel is Naive Bayes. All models in this sequence are evaluated via a test of pre-\ndictive accuracy; the model that achieves the best balance between complexity and\nﬁt is the model that achieves the highest disambiguation accuracy. Here, backward\nsequential search begins with the saturated model and generates a series of models\nthat concludes with the one that best balances complexity and ﬁt, as judged by an\nevaluation criterion. If no such model is found then the model of independence is the\nconcluding model in the sequence.\nFor sparse and skewed data samples, backward sequential search should be used\nwith care. Backward search begins with the saturated model where the number of\nparameters equals the number of events in the event space. Early in the search the\nmodels are of high complexity. Parameter estimates based on the saturated model or\nother complex models are often unreliable since many of the marginal events required\nto make maximum likelihood estimates are not observed in the training sample.\n3.1.2. Evaluation Criteria\nThe degradation and improvement in ﬁt of candidate models relative to the cur-\nrent model is assessed by an evaluation criterion. Two diﬀerent varieties of evaluation\ncriteria are employed in this dissertation; signiﬁcance tests and information criteria.\nThe use of signiﬁcance tests as evaluation criteria during sequential searches\nfor probabilistic models of word sense disambiguation was introduced by Bruce and\nWiebe (e.g., [10], [11], [12]). They employ the log–likelihood ratio G2 and assign\n29\nsigniﬁcance values to this test statistic using an asymptotic distribution and an exact\nconditional distribution. This dissertation expands the range of evaluation criteria\navailable for model selection by introducing two information criteria, Akaike’s Infor-\nmation Criterion (AIC) [1] and the Bayesian Information Criterion (BIC) [86].\n3.1.2.1. Signiﬁcance Testing\nIn signiﬁcance testing, a model is a hypothesized representation of the popula-\ntion from which a training sample was drawn. The adequacy of this model is evaluated\nvia a test statistic that measures the ﬁt of the model to the training sample.1 The ﬁt\nof the hypothesized model is judged acceptable if it diﬀers from the training sample\nby an amount that is consistent with sampling error, where that error is deﬁned by\nthe distribution of the test statistic.\nThe log–likelihood ratio G2 is a frequently used test statistic:\nG2 = 2 ×\nq\nX\ni=1\nfi × logfi\nei\n(3.1)\nwhere fi and ei are the observed and expected counts of the ith feature vector. The\nobserved count fi is calculated directly from the training sample while the expected\ncount ei is calculated assuming that the model under evaluation ﬁts the sample, i.e.,\nthat the null hypothesis is true. This statistic measures the deviation between what\nis observed in the training sample and what would be expected in that sample if the\nhypothesized model is an accurate representation of the population.\nThe distribution of G2 is asymptotically approximated by the χ2 distribution\n[94] with adjusted degrees of freedom (dof) equal to the number of parameters that\nhave non–zero estimates given the data in the sample. The degrees of freedom are\nadjusted to remove those parameters in the hypothesized model that can not be\n1When using maximum likelihood estimates, the training sample is exactly characterized by the\nsaturated model.\nThus the ﬁt of the hypothesized model to the training sample is assessed by\nmeasuring the ﬁt of the model to the saturated model.\n30\nestimated from the training sample. These are parameters whose suﬃcient statistics\nhave a value of zero since the marginal events they are associated with do not occur in\nthe training sample. The statistical signiﬁcance of a model is equal to the probability\nof observing its associated G2 value in the χ2 distribution with appropriate degrees of\nfreedom. If this probability is less than a pre–deﬁned cutoﬀvalue, α, then the deviance\nof the hypothesized model from the training sample is less than would be expected\ndue to sampling error. This suggests that the hypothesized model is a reasonable\nrepresentation of the population from which the training sample was taken.\nDuring backward sequential search a signiﬁcance test determines if a candidate\nmodel results in a signiﬁcantly worse ﬁt than the current model. During forward\nsearch a signiﬁcance test determines if a candidate model results in a signiﬁcantly\nbetter ﬁt than the current model. This is a diﬀerent formulation than the signiﬁcance\ntest described above, where the hypothesized or candidate model is always ﬁtted to\nthe saturated model. This dissertation treats sequential search as a series of local\nevaluations, where the ﬁt of candidate models is made relative to current models\nthat have one more or one less dependency, depending on the direction of the search.\nThis is in contrast to a global evaluation where the ﬁt of candidate models is always\nrelative to the saturated model or some other ﬁxed model.\nThe degree to which a candidate model improves upon or degrades the ﬁt of the\ncurrent model is measured by the diﬀerence between the G2 values for the candidate\nand current model, ∆G2. Like G2, the distribution of ∆G2 is approximated by a χ2\ndistribution with adjusted degrees of freedom equal to the diﬀerence in the adjusted\ndegrees of freedom of the candidate and current model, ∆dof [5].\nDuring backward search a candidate model does not result in a signiﬁcant degra-\ndation in ﬁt from the current model if the probability, i.e., signiﬁcance, of its ∆G2\nvalue is above a pre–determined cutoﬀ, α, that deﬁnes the allowable sampling error.\nThis error is deﬁned by the asymptotic distribution of ∆G2, which is in turn deﬁned\nby the χ2 distribution with degrees of freedom equal to ∆dof. If the error is small\nthen the candidate model is an adequate representations of the population.\n31\nA candidate model of complexity level i −1 inevitably results in a degradation\nin ﬁt from the current model of complexity level i. The objective of backward search\nis to select the candidate model that results in the least degradation in ﬁt from the\ncurrent model. Thus, the candidate model of complexity level i −1 with the lowest\nsigniﬁcance value less than α is selected as the current model of complexity level\ni −1. The degradation in ﬁt is judged acceptable if the value of ∆G2 is statistically\ninsigniﬁcant, according to a χ2 distribution with degrees of freedom equal to ∆dof.\nIf the signiﬁcance of ∆G2 is unacceptably large for all candidate models the selection\nprocess stops and the current model becomes the ultimately selected model.\nDuring forward search the candidate model has one more edge than the current\nmodel. A candidate model of complexity level i + 1 inevitably improves upon the\nﬁt of the current model of complexity level i. The objective of forward search is to\nselect the candidate model that results in the greatest increase in ﬁt from the current\nmodel. The candidate model of complexity level i + 1 with the largest signiﬁcance\nvalue greater than α is selected as the current model of complexity level i+1. This is\nthe model that results in the largest improvement in ﬁt when moving from a model\nof complexity level i to one of i + 1. The improvement in ﬁt is judged acceptable if a\nsigniﬁcance test shows that the value of ∆G2 is statistically signiﬁcant. If all candidate\nmodels result in insigniﬁcant levels of improvement in ﬁt then model selection stops\nand selects the current model of complexity level i.\nWhile it is standard to use a χ2 distribution to assess the signiﬁcance of G2 or\n∆G2, it is known that this approximation may not hold when the data is sparse and\nskewed [80]. An alternative to using an asymptotic approximation to the distribution\nof test statistics such as G2 and ∆G2 is to deﬁne their exact distribution. There are\ntwo ways to deﬁne the exact distribution of a test statistic:\n1. enumerate all elements of that distribution as in Fisher’s Exact Test [35] or\n2. sample from that distribution using a Monte Carlo sampling scheme [81].\n32\nThe signiﬁcance of G2 and ∆G2 based on the exact conditional distribution does\nnot rely on an asymptotic approximation and is accurate for sparse and skewed data\nsamples. Sequential model selection using the exact conditional test is developed for\nword sense disambiguation in [10]. The exact conditional test is also applied to the\nidentiﬁcation of signiﬁcant lexical relationships in [74].\nThis dissertation employs sequential model selection using both the asymptotic\napproximation of the signiﬁcance of G2 values as well as the exact conditional distri-\nbution. The forward and backward sequential search procedures remain the same for\nboth methods; the distinction is in how signiﬁcance is assigned. The asymptotic as-\nsumption results in the assignment of signiﬁcance values from a χ2 distribution while\nthe exact conditional test assigns signiﬁcance based upon a Monte Carlo sampling\nscheme.2\n3.1.2.2. Information Criteria\nTwo information criteria are employed as evaluation criteria in this dissertation;\nAkaike’s Information Criteria (AIC) and the Bayesian Information Criteria (BIC).\nThese criteria are formulated as follows for use during sequential model selection:\nAIC = ∆G2 −2 × ∆dof\n(3.2)\nBIC = ∆G2 −log(N) × ∆dof\n(3.3)\nwhere ∆G2 again measures the deviation in ﬁt between the candidate model and\nthe current model. However, here ∆G2 is treated as a raw score and not assigned\nsigniﬁcance. ∆dof represents the diﬀerence between the adjusted degrees of freedom\nfor the current and candidate models. Like ∆G2, it is treated as a raw score and is not\n2The freely available software package CoCo [2] implements the Monte Carlo sampling scheme\ndescribed in [48].\n33\nused to assign signiﬁcance. In Equation 3.3, N represents the number of observations\nin the training sample.\nThe information criteria are alternatives to using a pre–deﬁned signiﬁcance level,\nα, to judge the acceptability of a model. AIC and BIC explicitly balance model ﬁt\nand complexity; ﬁt is determined by the value of ∆G2 while complexity is expressed\nin terms of the diﬀerence in the adjusted degrees of freedom of the two models, ∆dof.\nSmall values of ∆G2 imply that the ﬁt of the candidate model to the training data\ndoes not deviate greatly from the ﬁt obtained by the current model. Likewise, small\nvalues for the adjusted degrees of freedom, ∆dof, suggest that the candidate and\ncurrent models do not diﬀer greatly in regards to complexity.\nDuring backward search the candidate model with the lowest negative AIC\nvalue is selected as the current model of complexity level i −1. This is the model\nthat results in the least degradation in ﬁt when moving from a model of complexity\nlevel i to one of i −1. This degradation is judged acceptable if the AIC value for the\ncandidate model of complexity level i −1 is negative. If there are no such candidate\nmodels then the degradation in ﬁt is unacceptably large and model selection stops\nand the current model of complexity level i becomes the selected model.\nDuring forward search the candidate model with the largest positive AIC value\nis selected as the current model of complexity level i + 1. This is the model that\nresults in the largest improvement in ﬁt when moving from a model of complexity\nlevel i to one of i + 1. This improvement is judged acceptable if the AIC value for\nthe model of complexity level i + 1 is positive. If there are no such models then the\nimprovement in ﬁt is unacceptably small and model selection stops and the current\nmodel of complexity level i becomes the selected model.\nThe information criteria have a number of appealing properties that make them\nparticularly well suited for sequential model selection. First, they do not require that\na pre–determined cutoﬀpoint be speciﬁed to stop the model selection process; a\nmechanism to stop model selection is inherent in the formulation of the statistic.\nSecond, the balance between model complexity and ﬁt is explicit in the statistic and\n34\nTable 3.1. Model Selection Example Data\nA\nB\nC\nfreq(A, B, C)\n0\n0\n0\n0\n0\n0\n1\n1\n0\n1\n0\n5\n0\n1\n1\n12\n1\n0\n0\n0\n1\n0\n1\n3\n1\n1\n0\n2\n1\n1\n1\n1\ncan be directly controlled by adjusting the constant that precedes ∆dof. As this\nvalue increases the selection process results in models of decreasing complexity.3\n3.1.3. Examples\nFor clarity, the sequential model selection process is illustrated with two simple\nexamples; one using forward search in combination with AIC and the other using\nbackward search and AIC. These methodologies are abbreviated as FSS AIC and BSS\nAIC, respectively. Both examples learn a parametric form from the 24 observation\ntraining sample shown in Table 3.1, where the feature set consists of three binary\nvariables, A, B, and C. There are eight possible events in the event space. The\nfrequency with which each event occurs in the sample is shown by freq(A, B, C).\n3.1.3.1. FSS AIC\nDuring forward search, the candidate models are evaluated relative to how much\nthey improve upon the ﬁt of the current model. Such an improvement is expected\nsince the candidate model has one more dependency than the current model.\n3In general, BIC selects models of lower complexity than does AIC. This is discussed further in\nChapter 6.\n35\nThe value of ∆G2 measures the amount of deviance between the candidate\nmodel and the current model; a large value implies that the candidate model greatly\nincreases the ﬁt of the model. Only candidate models that have positive AIC values\nimprove upon the ﬁt of the current model suﬃciently to merit designation as the\nnew current model. A negative value for AIC during forward search indicates that\nthe increase in ﬁt is outweighed by the resulting increase in complexity and will not\nresult in a model that attains an appropriate balance of complexity and ﬁt.\nThe steps in sequential model selection using FSS AIC are shown in Table\n3.2.\nThe G2 values for the current and candidate models are shown, as is their\ndiﬀerence, ∆G2. The steps of the sequential search using forward search and AIC\nare shown in Table 3.2.\nThe value of ∆G2 measures the improvement in the ﬁt\nwhen a dependency is added to the current model. During forward search, ∆G2 is\ncalculated by subtracting the G2 value associated with the candidate model from the\nG2 associated with the current model:\n∆G2 = G2\ncurrent −G2\ncandidate\n(3.4)\nThis diﬀerence shows the degree to which the candidate model improves upon the ﬁt\nof the current model. A large value of ∆G2 shows that the ﬁt of the candidate model\nto the training sample is considerably better than that of the current model.\nThe degree to which complexity is increased by the addition of a dependency to\nthe candidate model is measured by the diﬀerence in the adjusted degrees of freedom\nfor the two models, ∆dof:\n∆dof = dofcandidate −dofcurrent\n(3.5)\nStep 1: Forward search begins with the model of independence, (A)(B)(C), as\nthe current model. The set of one edge decomposable candidate models is generated\nby adding an edge to the model of independence.\nThe candidate models include\n(AC)(B), (A)(BC), and (AB)(C). These are all evaluated via AIC with the result\n36\nTable 3.2. Model Selection Example: FSS AIC\nCurrent\nG2\nCandidate\nG2\n∆G2\n∆dof\nAIC\nStep 1\n(A)(B)(C)\n10.14\n(AC)(B)\n10.08\n0.06\n1\n-1.94\n(A)(B)(C)\n10.14\n(A)(BC)\n7.07\n3.07\n1\n1.07\n(A)(B)(C)\n10.14\n(AB)(C)\n4.56\n5.58\n1\n3.58\nStep 2\n(AB)(C)\n4.56\n(AB)(AC)\n4.50\n0.06\n1\n-1.94\n(AB)(C)\n4.56\n(AB)(BC)\n1.48\n3.08\n1\n1.08\nStep 3\n(AB)(BC)\n1.48\n(ABC)\n0.00\n1.48\n1\n-0.52\nSelected: (AB)(BC)\nthat (AB)(C) has the greatest positive AIC value. This candidate model exhibits the\ngreatest deviance from the current model and therefore most increases the ﬁt. Thus,\n(AB)(C) becomes the new current model.\nStep 2: A new set of candidate models is generated by adding an edge to the cur-\nrent model, (AB)(C). The candidate models are (AB)(AC) and (AB)(BC). These\nare each evaluated relative to the current model (AB)(C). The model (AB)(BC) has\nthe greatest positive AIC value associated with it and thus most increases the ﬁt over\nthe current model. The new current model is now (AB)(BC).\nStep 3: The set of candidate models is generated by adding an edge to the\ncurrent model (AB)(BC).\nThe only resulting candidate is (ABC), the saturated\nmodel. However, when evaluated relative to the current model it has a negative AIC\nvalue associated with it; this suggests that the increase in ﬁt is not suﬃcient to merit\nfurther increases in the complexity of the model. Thus, the current model (AB)(BC)\nbecomes the selected model and is the ultimate result of the selection process.\n3.1.3.2. BSS AIC\nDuring backward search, candidate models are evaluated based upon how much\nthey degrade the ﬁt of the current model.\nSince the candidate models have one\n37\nTable 3.3. Model Selection Example: BSS AIC\nCandidate\nG2\nCurrent\nG2\n∆G2\n∆dof\nAIC\nStep 1\n(AC)(BC)\n7.00\n(ABC)\n0.00\n7.00\n1\n5.00\n(AB)(AC)\n4.49\n(ABC)\n0.00\n4.49\n1\n2.49\n(AB)(BC)\n1.48\n(ABC)\n0.00\n1.48\n1\n-0.52\nStep 2\n(A)(BC)\n7.07\n(AB)(BC)\n1.48\n5.59\n1\n3.59\n(AB)(C)\n4.56\n(AB)(BC)\n1.48\n3.08\n1\n1.08\nSelected: (AB)(BC)\nfewer dependency than the current model, it is inevitable that there will be some\ndegradation in ﬁt.\nDuring backward search only candidate models that have negative AIC values\nare eligible to be designated current models. A positive AIC suggests that the degra-\ndation in model ﬁt that occurs due to removal of a dependency is too large and oﬀsets\nthe beneﬁts of reducing the complexity of the current model.\nThe steps of the sequential search using backward search and AIC are shown\nin Table 3.3. The value of ∆G2 measures the degradation in ﬁt when a dependency\nis removed from the current model:\n∆G2 = G2\ncandidate −G2\ncurrent\n(3.6)\nThe degree to which complexity is decreased by the removal of a dependency from\nthe candidate model is shown by the diﬀerence in the degrees of freedom for the two\nmodels, ∆dof:\n∆dof = dofcurrent −dofcandidate\n(3.7)\nStep 1: A backward search begins with the saturated model, (ABC), as the\ncurrent model. The set of candidate models consists of all two edge models that\n38\nare generated by removing a single edge from the saturated model.\nThe models\n(AC)(BC), (AB)(AC), and (AB)(BC) are evaluated relative to the saturated model.\n(AC)(BC) has the lowest negative AIC value and becomes the current model.\nStep 2: The candidate models are all the one edge models generated by remov-\ning a single edge from the current model. The models (A)(BC) and (AB)(C) are\nevaluated and both have positive AIC values. Both result in a degradation in ﬁt that\nis not oﬀset by an appropriate reduction in model complexity. Thus, model selection\nstops and the current model, (AB)(BC), becomes the selected model.\n3.2. Naive Mix\nThis dissertation introduces the Naive Mix, a new supervised learning algorithm\nthat extends the sequential model selection methodology.\nThe usual objective of\nmodel selection is to ﬁnd a single model that achieves the best representation of\nthe training sample both in terms of complexity and ﬁt. However, empirical results\ndescribed in Chapter 6 show that it is often the case that very diﬀerent models can\nresult in nearly identical levels of disambiguation accuracy. This suggests that there\nis an element of uncertainty in model selection and that a single best model may not\nalways exist.\nThe Naive Mix is based on the premise that each of the models that serve as\na current model during a sequential search have important information that can be\nexploited for word sense disambiguation. The Naive Mix is an averaged probabilistic\nmodel based upon the average of the parameter estimates for all of the current models\ngenerated during a sequential model selection process.\nSequential methods of model selection result in a sequence of decomposable\nmodels (m1, m2,. . ., mr−1, mr) where m1 is the initial current model and mr is the\nselected model. Each model mi is designated as the current model at the ith step\nin the search process. During forward search m1 is the model of independence and\nduring backward search m1 is the saturated model.\n39\nEach model mi has a parametric form that expresses the dependencies among\nthe feature variables (F1, F2, . . ., Fn−1, S), where the sense of the ambiguous word\nis represented by S and (F1, F2, . . .,Fn−1) represent the feature variables. The joint\nprobability distribution of this set of feature variables can be expressed in terms of\nthe marginal probability distributions deﬁned by each decomposable model mi.\nGiven a sequence of current models found during a sequential search, the param-\neters of the joint probability distribution of the set of feature variables are estimated\nbased upon the marginal distributions of each of these models. In other words, r dif-\nferent estimates for the joint probability distribution of a set of feature variables are\nmade. These r estimates are averaged and the resulting joint probability distribution\nis a Naive Mix:\nˆθ(F1,F2,...,Fn−1,S)average\n=\n1\nr ×\nr\nX\ni=1\nˆθ(F1,F2,...,Fn−1,S)mi\n(3.8)\nwhere ˆθ(F1,F2,...,Fn−1,S)mi represents the parameter estimates given that the parametric\nform is mi.\nA Naive Mix can be created using either forward or backward search. However,\nthere are a number of advantages to formulating a Naive Mix with a forward search.\nFirst, the inclusion of very simple models in the mix eliminates the problem of zero–\nvalued parameter estimates in the averaged probabilistic model. The ﬁrst model in\nthe Naive Mix is the model of independence which acts as a majority classiﬁer and\nhas estimates associated with it for for every event in the event space. Second, for-\nward search incrementally builds on the strongest dependencies among features while\nbackward search incrementally removes the weakest dependencies. Thus a Naive Mix\nformulated with backward search can potentially contain many irrelevant dependen-\ncies while a forward search only includes the most important dependencies.\nConsider an example that formulates a Naive Mix with a forward search; this\nexample follows the notation of the earlier bill example. Suppose that the sequence of\nmodels shown in Table 3.4 are found to be the best ﬁtting models by some evaluation\n40\nTable 3.4. Sequence of Models for Naive Mix created with FSS\ncurrent model\nmixed model\nm1\n(C)(V)(R)(T)(S)\n(S)\nm2\n(CS)(V)(R)(T)\n(CS)\nm3\n(CS)(ST)(V)(R)\n(CS)(ST)\nm4\n(CS)(ST)(SV)(R)\n(CS)(ST)(SV)\nm5\n(CSV)(ST)(R)\n(CSV)(ST)\nm6\n(CSV)(ST)(TR)\n(CSV)(ST)\nm7\n(CSV)(RST)\n(CSV)(RST)\ncriterion at each step of the forward search. These represent the set of current models.\nAny marginal distributions of the current models that do not include S, the\nsense variable, can be eliminated from the model included in the mix. Such marginal\ndistributions simply act as constants in a probabilistic classiﬁer and can be removed\nfrom the mix without aﬀecting the ﬁnal result.\nIn Table 3.4, the models in the\ncolumn mixed model are used to make the parameter estimates of the joint probability\ndistributions that are included in the Naive Mix.\nThe parameters of the joint probability distribution of each decomposable model\nmi are expressed as the product of the marginal distributions of each current model.\nThe parameters of the joint distributions are averaged across all of the models to cre-\nate the Naive Mix. For example, the averaged parameter θ(C,V,R,T,S)average\nc,v,r,t,s\nis estimated\nas follows:\nθ(C,V,R,T,S)average\nc,v,r,t,s\n=\n1\n7(ˆθ\n(C,V,R,T,S)m1\nc,v,r,t,s\n+ ˆθ\n(C,V,R,T,S)m2\nc,v,r,t,s\n+ ˆθ\n(C,V,R,T,S)m3\nc,v,r,t,s\n+\nˆθ\n(C,V,R,T,S)m4\nc,v,r,t,s\n+ ˆθ\n(C,V,R,T,S)m5\nc,v,r,t,s\n+ ˆθ\n(C,V,R,T,S)m6\nc,v,r,t,s\n+ ˆθ\n(C,V,R,T,S)m7\nc,v,r,t,s\n)\n(3.9)\nwhere\n41\nˆθ\n(C,V,R,T,S)m1\nc,v,r,t,s\n= ˆθS\ns , ˆθ\n(C,V,R,T,S)m2\nc,v,r,t,s\n= ˆθC,S\nc,s\n,\nˆθ\n(C,V,R,T,S)m3\nc,v,r,t,s\n=\nˆθC,S\nc,s × ˆθS,T\ns,t\nˆθS\ns\nθ\n(C,V,R,T,S)m4\nc,v,r,t,s\n=\nˆθC,S\nc,s × ˆθS,T\ns,t × ˆθS,V\ns,V\nˆθS\ns × ˆθS\ns\n,\nˆθ\n(C,V,R,T,S)m5\nc,v,r,t,s\n=\nˆθC,S,V\nc,s,v\n× ˆθS,T\ns,t\nˆθS\ns\nˆθ\n(C,V,R,T,S)m6\nc,v,r,t,s\n=\nˆθC,S,V\nc,s,v\n× ˆθS,T\ns,t\nˆθSs\n,\nθ\n(C,V,R,T,S)m7\nc,v,r,t,s\n=\nˆθC,S,V\nc,s,v\n× ˆθR,S,T\nr,s,t\nˆθSs\nOnce the parameter estimates are made and averaged, the resulting probabilis-\ntic model can be used as a classiﬁer to perform disambiguation. Suppose that the\nfollowing feature vector represents a sentence containing an ambiguous use of bill. S\nrepresents the sense of the ambiguous word and the other variables represent observed\nfeatures in the sentence:\n(C = c, V = v, R = r, T = t, S =?)\n(3.10)\nThe value of S that maximizes ˆθ(C,V,R,T,S)average\nc,v,r,t,sx\nis determined to be the sense of\nbill. Here again disambiguation reduces to ﬁnding the value of S that is most probable\nin a particular context as deﬁned by the observed values of the feature variables.\nS\n=\nargmax\nsx\nˆθ(C,V,R,T,S)average\nc,v,r,t,sx\n=\nargmax\nsx\np(sx|c, v, r, t)\n(3.11)\nThe Naive Mix addresses the uncertainty that exists in model selection. Similar\ndiﬃculties in selecting single best models have been noted elsewhere; in fact, there is\na general trend in model selection research away from the selection of such models\n(e.g., [53]). A similar movement exists in machine learning, based on the premise that\nno learning algorithm is superior for all tasks [83]. This has lead to hybrid approaches\n42\nthat combine diverse learning paradigms (e.g., [31]) and approaches that select the\nmost appropriate learning algorithm based on the characteristics of the training data\n(e.g., [8]).\n3.3. Naive Bayes\nNaive Bayes diﬀers from the models learned via sequential model selection and\nthe Naive Mix since the parametric form of Naive Bayes is always the same and does\nnot have to be learned. Naive Bayes assumes that all feature variables are condi-\ntionally independent given the value of the classiﬁcation variable. Examples of both\nthe graphical representation of this parametric form and the associated parameter\nestimates are shown in Chapter 2.\nIn disambiguation, feature variables represent contextual properties of the sen-\ntence in which an ambiguous word occurs. The classiﬁcation variable represents the\nsense of the ambiguous word. Thus, Naive Bayes assumes that the values of any two\ncontextual features in a sentence do not directly aﬀect each other. In general this\nis not a likely representation of the dependencies among features in language. For\nexample, one kind of feature used in this dissertation represents the part–of–speech\nof words that surround the ambiguous word. It is typically the case that the part–\nof–speech of the i + 1th word in a sentence is dependent on the part–of–speech of the\nith word. When an article occurs in the ith position, one can predict that a noun or\nadjective is more likely to occur at the i + 1th position than is a verb, for example.\nHowever, despite the fact that Naive Bayes does not correspond to intuitions regard-\ning the dependencies among features, experimental results in Chapter 6 show that\nNaive Bayes performs at levels comparable to models with learned parametric forms.\nIf the contextual features of a sentence are represented by variables (F1, F2, . . .,\nFn−1) and the sense of the ambiguous word is represented by S, then the parameter\nestimates of Naive Bayes are calculated as follows:\n43\nˆθF1,F2,...,Fn−1,S\nf1,f2,...,fn−1,s\n= θS\ns ×\nn−1\nY\ni=1\nˆθFi,S\nfi,s\nˆθS\ns\n(3.12)\nSeveral alternative but equivalent formulations are shown in Chapter 2.\nEven with a large number of features, the number of parameters in Naive Bayes\nis relatively small. For a problem with n feature variables, each having l possible\nvalues, and a classiﬁcation variable with s possible values, the number of parameters\nin Naive Bayes is n∗l ∗s. More complicated models often require that an exponential\nnumber of parameters be learned. For example, a saturated model given the same\nscenario will have nl ∗s parameters.\n44\nCHAPTER 4\nUNSUPERVISED LEARNING FROM RAW TEXT\nThe main limitation of the supervised learning methods presented in Chapter\n3 is the need for sense–tagged text to serve as training examples. The creation of\nsuch text is time–consuming and proves to be a signiﬁcant bottleneck in porting and\nscaling the supervised approaches to new and larger domains of text.\nUnsupervised learning presents an alternative that eliminates this dependence\non sense–tagged text. The object of unsupervised learning is to determine the clas-\nsiﬁcation of each instance in a sample without using training examples.\nIn word\nsense disambiguation, this corresponds to grouping instances of an ambiguous word\ninto some pre–speciﬁed number of sense groups, where each group corresponds to a\ndistinct sense of the ambiguous word. This is done strictly based on information ob-\ntained from raw untagged text; no external knowledge sources are employed. While\nthis increases the portability of these approaches, it also imposes an important limita-\ntion. Since no knowledge beyond the raw text is employed, the unsupervised learning\nalgorithms do not have access to the sense inventory for a word. Thus, while they\ncreate sense groups based on the features observed in the text, these groups are not\nlabeled with a deﬁnition or any other meaningful tag.\nIf such labels are desired,\nthey must be attached after unsupervised learning has created the sense groups. One\nmeans of attaching such labels is discussed in Chapter 7.\nThis chapter describes a methodology by which probabilistic models can be\nlearned from raw text. It requires that the variable values associated with the sense\nof an ambiguous word be treated as missing or unobserved data in the sample. Given\nthat these values are never present in the data sample, it is not possible to conduct a\nsystematic search for the parametric form of a model; one must simply be assumed.\n45\nIn this framework, the Expectation Maximization (EM) algorithm [29] and Gibbs\nSampling [39] are used to estimate the parameters of a probabilistic model. As a\npart of this process, values are imputed, i.e., ﬁlled–in, for the sense variable. This\neﬀectively assigns instances of an ambiguous word to a particular sense group.\nAn alternative to this probabilistic methodology is to use an agglomerative\nclustering algorithm that forms sense groups of untagged instances of an ambiguous\nword by minimizing a distance measure between the instances of an ambiguous word\nin each sense group. Two agglomerative algorithms are explored here, McQuitty’s\nsimilarity analysis [55] and Ward’s minimum–variance method [91].\n4.1. Probabilistic Models\nIn supervised learning, given the parametric form of a decomposable model,\nmaximum likelihood estimates of parameters are simple to compute. The suﬃcient\nstatistics of these parameters are the frequency counts of marginal events that are\ndeﬁned by the marginal distributions of the model. These counts are obtained directly\nfrom the training data. However, in unsupervised learning, parameter estimation is\nmore diﬃcult since direct estimates from the sample are not possible given that data\nis missing.\nTo illustrate the problem, the bill example from the previous chapter is recast\nas a problem in unsupervised learning. Suppose that (CV S)(RTS) is the parametric\nform of a decomposable model. In supervised learning, maximum likelihood estimates\nof the parameters of the joint distribution are made by observing the frequency of\nthe marginal events (CV S) and (RTS). However, when sense–tagged text is not\navailable this estimate can not be computed directly since the value of S is unknown.\nThere is no way, for example, to directly count the occurrences of the marginal events\n(C = yes, S = 1, V = no) and (C = yes, S = 2, V = no) in an untagged sample\nof text; the only observed marginal event is (C = yes, S =?, V = no). However,\nboth the EM algorithm and Gibbs Sampling impute values for this missing data and\nthereby make parameter estimation possible.\n46\nHere the assumption is made that the parametric form of the model is Naive\nBayes. In this model, all features are conditionally independent given the value of\nthe classiﬁcation feature, i.e., the sense of the ambiguous word. This assumption is\nbased on the success of the Naive Bayes model when applied to supervised word–sense\ndisambiguation (e.g. [11], [37], [51], [62], [70], [73]).\nIn these discussions, the sense of an ambiguous word is represented by a feature\nvariable, S, whose value is missing. The observed contextual features are represented\nby Y = (F1, F2, . . . , Fn).\nThe complete data sample is then D = (Y, S) and the\nparameters of the model are represented by the vector Θ.\n4.1.1. EM Algorithm\nThe EM algorithm is an iterative estimation procedure in which a problem with\nmissing data is recast to make use of complete data estimation techniques. The EM\nalgorithm formalizes a long–standing method of making estimates for the parameters\nof a model, Θ, when data is missing. A high–level description of the algorithm is as\nfollows:\n1. Randomly estimate initial values for the parameters Θ. Call this set of estimates\nΘold.\n2. Replace the missing values of S by their expected values given the parameter\nestimates Θold.\n3. Re–estimate parameters based on the ﬁlled–in values for the missing variable\nS. Call these parameter estimates Θnew.\n4. Have Θold and Θnew converged? If not, rename Θnew as Θold and go to step 2.\n4.1.1.1. General Description\nAt the heart of the EM Algorithm lies the Q-function. This is the expected\nvalue of the log of the likelihood function for the complete data sample, D = (Y, S),\n47\nwhere Y is the observed data and S is the missing sense value:\nQ(Θnew|Θold)\n=\nE[ln p(Y, S|Θnew)|Θold, Y )]\n(4.1)\nHere, Θold is the previous value of the maximum likelihood estimates of the parameters\nand Θnew is the improved estimate; p(Y, S|Θnew) is the likelihood of observing the\ncomplete data given the improved estimate of the model parameters.\nWhen approximating the maximum of the likelihood function, the EM algorithm\nstarts from a randomly generated initial estimate of the model parameters and then\nreplaces Θold by the Θnew which maximizes Q(Θnew|Θold). This is a two step process,\nwhere the ﬁrst step is known as the expectation step, i.e., the E–step, and the second\nis the maximization step, i.e., the M–step. The E–step ﬁnds the expected values of the\nsuﬃcient statistics of the complete model using the current estimates of the model\nparameters. For decomposable models these suﬃcient statistics are the frequency\ncounts of events deﬁned by the marginal distributions of the model. The M–step\nmakes maximum likelihood estimates of the model parameters using the suﬃcient\nstatistics from the E–step. These steps iterate until the parameter estimates Θold and\nΘnew converge.\nThe M–step is usually easy, assuming it is easy for the complete data prob-\nlem. As shown in Chapter 2, making parameter estimates for decomposable models\nis straightforward. In the general case the E–step may be complex. However, for de-\ncomposable models the E–step simpliﬁes to the calculation of the expected marginal\nevent counts deﬁned by a decomposable model, where the expectation is with respect\nto Θold. The M–step simpliﬁes to the calculation of new parameter estimates from\nthese counts. Further, these expected counts can be calculated by multiplying the\nsample size N by the probability of the complete data within each marginal distri-\nbution, given Θold and the observed data within each marginal Ym. This simpliﬁes\nto:\n48\nfreqnew(Sm, Ym) = p(Sm|Ym) × freq(Ym)\n(4.2)\nwhere freqnew is the current estimate of the expected count and p(Sm|Ym) is formu-\nlated using Θold.\n4.1.1.2. Naive Bayes description\nThe expectation and maximization steps of the EM algorithm are outlined here.\nIt is assumed that the parametric form is Naive Bayes, although this discussion ex-\ntends easily to any decomposable model [50]. Given that the parametric form is Naive\nBayes, it follows that:\np(F1, F2, . . . , Fn, S) = p(S) ×\nn\nY\ni=1\np(Fi|S)\n(4.3)\nwhere p(S) and p(Fi|S) are the model parameters. This is equivalent to treating\np(Fi, S) as the model parameters since p(Fi, S) = p(Fi|S)\np(S) . However, the conditional\nrepresentation lends itself to developing certain analogies between the EM algorithm\nand Gibbs Sampling.\nE–step: The expected values of the suﬃcient statistics of the Naive Bayes\nmodel are computed. These are the frequency counts of marginal events of the form\n(Fi, S) and are notated freq(Fi, S). Since S is unobserved, values for it must be\nimputed before the marginal events can be counted. During the ﬁrst iteration of the\nEM algorithm, values for S are imputed by random initialization. Thereafter, S is\nimputed with values that maximize the probability of observing a particular sense for\nan ambiguous word in a given context:\nS =\nargmax\nsx\nˆp(S|f1, f2, . . . , fn−1, fn)\n(4.4)\nFrom p(a|b) = p(a,b)\np(b) it follows that:\n49\nˆp(S|f1, f2, . . . , fn−1, fn) = ˆp(f1, f2, . . . , fn−1, fn, S)\nˆp(f1, f2, . . . , fn−1, fn)\n(4.5)\nAnd from p(a, b) =\nP\nc p(a, b, c) it follows that:\nˆp(f1, f2, . . . , fn−1, fn) =\nX\nS\nˆp(f1, f2, . . . , fn, S)\n(4.6)\nThus,\nS =\nargmax\nsx\nˆp(S) × Qn\ni=1 ˆp(fi|S)\nP\nS ˆp(f1, f2, . . . , fn, S)\n(4.7)\nThis calculation determines the value of S to impute for each possible combi-\nnation of observed feature values. Given imputed values for S, the expected values of\nthe marginal event counts, freq(Fi, S), are determined directly from the data sample\nfollowing Equation 4.2. These counts are the suﬃcient statistics for the Naive Bayes\nmodel.\nM–Step: The suﬃcient statistics from the E–step are used to re–estimate the\nmodel parameters. This new set of estimates is designated Θnew while the previous\nset of parameter estimates is called Θold. The model parameters p(S) and p(Fi|S) are\nestimated as follows:\nˆp(S) = freq(S)\nN\nˆp(Fi|S) = freq(Fi, S)\nfreq(S)\n(4.8)\nConvergence?: If the diﬀerence between the parameter estimates obtained in\nthe previous and current iteration is less than some pre–speciﬁed value ǫ, i.e.,:\n||Θold −Θnew|| < ǫ\n(4.9)\nthen the parameter estimates have converged and the EM algorithm stops. If this\ndiﬀerence is greater than ǫ, Θnew is renamed Θold and the EM algorithm continues.\n50\nTable 4.1. Unsupervised Learning Example Data\nF1\nF2\nS\n1\n2\n?\n1\n2\n?\n2\n2\n?\n2\n2\n?\n1\n2\n?\n1\n1\n?\n1\n1\n?\n1\n1\n?\n1\n2\n?\n2\n2\n?\nThe EM algorithm is guaranteed to converge [29], however if the likelihood\nfunction is very irregular it may converge to a local maxima and not ﬁnd the global\nmaximum. In this case, an alternative is to use the more computationally expensive\nmethod of Gibbs Sampling which is guaranteed to converge to a global maximum.\n4.1.1.3. Naive Bayes example\nThe step by step operation of the EM algorithm is illustrated with a simple\nexample where the parametric form is assumed to be Naive Bayes. Suppose that there\nis a data sample where events are described by 3 random variables. The variables\nF1 and F2 are observed and have two possible values. The variable S represents the\nclass of the event but is unobserved. Given this formulation, the model parameters\nare p(S), p(F1|S) and p(F2|S), following Equation 4.3. The data sample used in this\nexample has ten observations and is shown in Table 4.1.\nE–Step Iteration 1: The EM algorithm begins by randomly assigning values\nto S. Such an assignment is shown on the left side of Figure 4.1. Given these random\nassignments, expected values for the suﬃcient statistics of the Naive Bayes model are\n51\nF1\nF2\nS\n1\n2\n1\n1\n2\n3\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n3\n1\n1\n1\n1\n1\n2\n1\n2\n2\n2\n2\n1\nF1\n1\n2\n1\n3\n1\n4\nS\n2\n2\n2\n4\n3\n2\n0\n2\n7\n3\n10\nF2\n1\n2\n1\n1\n3\n4\nS\n2\n1\n3\n4\n3\n1\n1\n2\n7\n3\n10\nFigure 4.1. E–Step Iteration 1\ndetermined by counting the marginal events deﬁned by Naive Bayes, i.e., freq(F1, S)\nand freq(F2, S). These marginal event counts are conveniently represented in a cross–\nclassiﬁcation or contingency table, as appears in the center and right of Figure 4.1.\nFor example, the center table shows that:\nfreq(F1 = 1, S = 1) = 3\nfreq(F1 = 2, S = 1) = 1\nfreq(F1 = 1, S = 2) = 2\nfreq(F1 = 2, S = 2) = 2\nfreq(F1 = 1, S = 3) = 2\nfreq(F1 = 2, S = 3) = 0\nM–Step Iteration 1: Maximum likelihood estimates for the parameters of\nNaive Bayes are made from the marginal event counts found during the E–step.\nGiven the marginal event counts in Figure 4.1, the parameter estimates are computed\nfollowing Equation 4.8. The values for these estimates found during iteration 1 are\nshown in the contingency tables in Figure 4.2. For example, the center table shows\nthat:\n52\n1\n0.4\nS\n2\n0.4\n3\n0.2\nF1\n1\n2\n1\n0.75\n0.25\n1.0\nS\n2\n0.50\n0.50\n1.0\n3\n1.00\n0.00\n1.0\nF2\n1\n2\n1\n0.25\n0.75\n1.0\nS\n2\n0.25\n0.75\n1.0\n3\n0.50\n0.50\n1.0\nFigure 4.2. M–Step Iteration 1: ˆp(S), ˆp(F1|S), ˆp(F2|S)\nˆp(F1 = 1|S = 1) = 0.75\nˆp(F1 = 2|S = 1) = 0.25\nˆp(F1 = 1|S = 2) = 0.50\nˆp(F1 = 2|S = 2) = 0.50\nˆp(F1 = 1|S = 3) = 1.00\nˆp(F1 = 2|S = 3) = 0.00\nE–Step Iteration 2: After the ﬁrst iteration of the EM algorithm, all subse-\nquent iterations ﬁnd the expected values of the marginal event counts by imputing\nnew values for S that maximize the following conditional distribution:\nS =\nargmax\ns\nˆp(S|F1, F2) = ˆp(S) × ˆp(F1|S) × ˆp(F2|S)\nˆp(F1, F2)\n(4.10)\nThe estimates of the parameters required by Equation 4.10 are the estimates\nmade in the M–step of the previous iteration, shown in Figure 4.2. The computation\nin Equation 4.10 results in the value of S that maximize the conditional probability\ndistribution where S is conditioned on the values of the observed features F1 and F2.\nThe values of ˆp(S|F1, F2) are shown in Figure 4.3. The maximum estimate for each\ngiven pair of values for the features (F1, F2) are shown in bold face. The value of S\nassociated with each of these maximum probabilities is imputed for each observation\nin the data sample that shares the same values for the observed feature values. For\n53\nF1\nF2\nS\nˆp(S|F1, F2)\n1\n1\n1\n.333\n1\n1\n2\n.222\n1\n1\n3\n.444\n1\n2\n1\n.474\n1\n2\n2\n.316\n1\n2\n3\n.211\n2\n1\n1\n.333\n2\n1\n2\n.667\n2\n1\n3\n.000\n2\n2\n1\n.333\n2\n2\n2\n.667\n2\n2\n3\n.000\nFigure 4.3. E–Step Iteration 2\nexample, if (F1 = 1, F2 = 1, S =?) is an observation in the data sample, then the\nvalue of 3 is imputed for S since ˆp(S = 3|F1 = 1, F2 = 1) is greater than both\nˆp(S = 2|F1 = 1, F2 = 1) and ˆp(S = 1|F1 = 1, F2 = 1).\nThe data sample that results from these imputations for S is shown on the left of\nFigure 4.4. The expected counts of the marginal events in that updated data sample\nare shown in contingency table form in the center and right of this same ﬁgure.\nM–Step Iteration 2: Given the expected values of the marginal event counts\nfrom the previous E–step, values for the model parameters are re–estimated. Figure\n4.5 shows the values for the parameter estimates ˆp(S), ˆp(F1|S), and ˆp(F2|S).\nAt this point, two iterations of the EM algorithm have been performed. From\nthis point forward, at the conclusion of each iteration a check for convergence is made.\nThe parameters estimated during the previous iteration are Θold and those estimated\nduring the current iteration are Θnew. For example, during iterations 1 and 2 the\nfollowing estimates have been made:\n54\nF1\nF2\nS\n1\n2\n1\n1\n2\n1\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n3\n1\n1\n3\n1\n1\n3\n1\n2\n1\n2\n2\n2\nF1\n1\n2\n1\n4\n0\n4\nS\n2\n0\n3\n3\n3\n3\n0\n3\n7\n3\n10\nF2\n1\n2\n1\n0\n4\n4\nS\n2\n0\n3\n3\n3\n3\n0\n3\n3\n7\n10\nFigure 4.4. E–Step Iteration 2\n1\n0.4\nS\n2\n0.3\n3\n0.3\nF1\n1\n2\n1\n1.0\n0.0\n1.0\nS\n2\n0.0\n1.0\n1.0\n3\n1.0\n0.0\n1.0\nF2\n1\n2\n1\n0.0\n1.0\n1.0\nS\n2\n0.0\n1.0\n1.0\n3\n1.0\n0.0\n1.0\nFigure 4.5. M–Step Iteration 2: ˆp(S), ˆp(F1|S), ˆp(F2|S)\n55\nF1\nF2\nS\nˆp(S|F1, F2)\n1\n1\n1\n0.0\n1\n1\n2\n0.0\n1\n1\n3\n1.0\n1\n2\n1\n1.0\n1\n2\n2\n0.0\n1\n2\n3\n0.0\n2\n1\n1\n0.0\n2\n1\n2\n0.0\n2\n1\n3\n0.0\n2\n2\n1\n0.0\n2\n2\n2\n1.0\n2\n2\n3\n0.0\nFigure 4.6. E–Step Iteration 3\nΘold = {.4, .4, .2, .75, .25, .5, .5, 1.0, 0.0, .25, .75, .25, .75, .5, .5}\nΘnew = {.4, .3, .3, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0}\nThe diﬀerence between Θold and Θold is considerable and certainly more than a\ntypical value of ǫ, i.e., .01 or .001. Thus, the EM algorithm continues for at least one\nmore iteration.\nE–step Iteration 3: The expected values for the marginal event counts are\ndetermined as in the previous iteration. First, the values of S that maximize the\nconditional probability distribution of ˆp(S|F1, F2) are determined. This maximization\nis based on the parameter estimates, shown in Figure 4.5, which were determined\nduring the M–step of the previous iteration.\nFigure 4.6 shows the estimated values for ˆp(S|F1, F2), the conditional distribu-\ntion of S given the values of the observed feature values. The maximum probability\nfor each given pair of feature values is shown in bold face. The values of S that\n56\nF1\nF2\nS\n1\n2\n1\n1\n2\n1\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n3\n1\n1\n3\n1\n1\n3\n1\n2\n1\n2\n2\n2\nFigure 4.7. E–Step Iteration 3\nresult in this maximized conditional distribution are imputed for the missing data of\nobservations in the sample that share the same observed feature values. The data\nset that results from this imputation is shown in Figure 4.7. However, note that the\nvalues found during the third iteration prove to be identical to those found during\nthe second iteration.\nThe expected counts of marginal events and the parameter estimates found\nduring iteration 3 are identical to those found during iteration 2. Given this, the\ndiﬀerence between Θold and Θnew is zero and the parameter estimates have converged.\nThe values for the missing variable S are assigned as shown in Figure 4.7. This is\nan intuitively appealing result that can be interpreted in terms of assigning events to\nclasses. The event (F1 = 1, F2 = 1) belongs to class 3, event (F1 = 2, F2 = 2) belongs\nto class 2, and event (F1 = 1, F2 = 2) belongs to class 1.\n4.1.2. Gibbs Sampling\nGibbs Sampling is a more general tool than the EM algorithm in that it is\nnot restricted to handling missing data; it is a special case of Markov Chain Monte\n57\nCarlo methods for approximate inference. These methods were ﬁrst used for appli-\ncations in statistical physics in the 1950’s; perhaps the most notable example being\nthe Metropolis algorithm [58]. Gibbs Sampling was originally presented in the con-\ntext of an image restoration problem but has since been applied to a wide range of\napplications.\nIn general, Gibbs Sampling provides a means of approximating complex prob-\nabilistic models. In unsupervised learning probabilistic models are complex because\nthere is missing data, i.e., the sense of the ambiguous word is unknown. Gibbs Sam-\npling approximates the distribution of the parameters of a model as if the missing\ndata were observed. By contrast, the EM algorithm simply maximizes the estimated\nvalues for the parameters of a model, again by acting as if the missing data were\nobserved.\n4.1.2.1. General Description\nGibbs Sampling has a Bayesian orientation in that it naturally incorporates\nprior distributions, p(Θ), into the sampling process. When a prior distribution is\nspeciﬁed in conjunction with an observed data sample, Gibbs Sampling approximates\nthe posterior probability function, p(Θ|D), by taking a large number of samples from\nit.\nIf a prior distribution is not utilized then Gibbs Sampling still takes a large\nnumber of samples, however, they are drawn from the likelihood function p(D|Θ). In\nthis dissertation, non–informative prior distributions are employed and the sampling\nis from the posterior distribution function.\nA Gibbs Sampler creates Markov Chains of parameter estimates and values for\nmissing data whose stationary distributions approximate the posterior distribution,\np(Θ|D), by simulating a random walk in the space of Θ. A Markov Chain is a series\nof random variables (X0, X1, . . .) in which the inﬂuence of the values of (X0, . . . , Xn)\non the distribution of Xn+1 is mediated entirely by the value of Xn.\nLet the values of the observed contextual feature variables be represented by\nY = (F1, F2, · · · , Fn−1, Fn) and let S represent the unknown sense of an ambiguous\n58\nword. Given that the parametric form of the model is known, random initial values\nare generated for the missing data S0 = (S0\n1, S0\n2, . . . , S0\nN) and the unknown parameter\nestimates of the assumed model Θ0 = (θ0\n1, θ0\n2, . . . , θ0\nq). S0 is a vector containing a value\nfor each instance of the missing sense data, Θ0 is a vector containing the parameters\nof the model, N is the number of observations in the data sample, and q is the number\nof parameters in the model.\nA Gibbs Sampler performs the following loop, where j is the iteration counter,\nuntil convergence is detected:\nSj+1 ∼p(S|θj\n1, θj\n2, · · ·, θj\nq, Y )\nθj+1\n1\n∼p(θ1|θj\n2, · · ·, θj\nq, Y, Sj+1)\nθj+1\n2\n∼p(θ2|θj+1\n1\n, θj\n3, · · ·, θj\nq, Y, Sj+1)\n...\nθj+1\nq\n∼p(θq|θj+1\n1\n, · · ·, θj+1\nq−1, Y, Sj+1)\nEach iteration of the Gibbs Sampler samples values for the missing data and for\nthe unknown parameter estimates. The values for the missing data are conditioned\non the values of the parameters and the observed data. The parameter estimates\nare conditioned on the previously estimated values of the other parameters and the\nmissing data as well as the observed data.\nA chain of values is constructed for each missing value and parameter esti-\nmate via this sampling loop. Each chain is monitored for convergence. A range of\ntechniques for monitoring convergence are discussed in [88]; this dissertation uses\nGeweke’s method [40]. In this approach, a chain is divided into two windows, one\nat the beginning and the other at the end. Each window contains about 10% of the\ntotal number of iterations in the chain. If the entire chain has reached a stationary\ndistribution, then these two windows, one early in the chain and the other late, will\n59\nhave approximately the same mean values. If they do not then the parameters have\nnot yet converged to a stationary distribution.\nThe early iterations of Gibbs Sampling produce chains of values with very high\nvariance. It is standard to discard some portion of the early iterations; this process\nis commonly known as a burn–in. The general procedure followed here is to have a\n500 iteration burn–in followed by 1000 iterations that are monitored for convergence.\nIf the chains do not converge after 1000 iterations then additional iterations in in-\ncrements of 500 are performed until they do. This procedure was designed following\nrecommendations by [79].\nA proof that convergence on the posterior probability distribution is guaranteed\nduring Gibbs Sampling is given in [39]. Once convergence occurs, the approximation\nto the posterior probability function can be summarized like any other probability\nfunction. Also, the median value in each chain of sampled values for missing data\nbecomes the sense group to which an instance of an ambiguous word is ultimately\nassigned.\n4.1.2.2. Naive Bayes description\nGibbs Sampling is developed in further detail, given the assumption that the\nparametric form is Naive Bayes. However, this discussion is easily extended to any\ndecomposable model. As in the previous example, the parameters of the model are\np(S) and p(Fi|S), following Equation 4.3.\nA Gibbs Sampler generates chains of values for each missing instance of S in\nthe data sample and also for each of the parameters ˆp(S) and ˆp(Fi|S). Each of these\nchains will eventually converge to a stationary distribution.\nIn this dissertation the observed data sample is multinomial, i.e., each instance\nin the data sample is described by a combination of discrete feature values.\nAs\nwas shown in Chapter 2, such a sample can be formally deﬁned by a multinomial\ndistribution with parameters (N; θ1, θ2, . . . , θq).\nHowever, here the distribution of\nmultinomial data is represented by the frequency counts for each possible event. This\n60\nis notated as M(f1, f2, . . . , fq), where q is the number of possible events and fi is the\nfrequency of the ith event. For example, M(f1, f2, . . . , fq) represents a multinomial\ndistribution with q possible events where the ith event occurs fi times.\nThe conjugate prior to the multinomial distribution is the Dirichlet distribution,\ndescribed by D(α1, α2, . . . , αq), where αi represents the prior frequency of the ith\nevent. If all αi are set to 1 then a non–informative prior has been properly speciﬁed\n[38]. For example, if q = 3, D(1, 1, 1) describes a non–informative Dirichlet prior\ndistribution.\nFollowing Bayes Rule, the product of a prior distribution and the likelihood\ndistribution results in a posterior probability distribution. If the prior distribution\nand the likelihood function are conjugate, then the posterior distribution has the same\ndistribution as the prior. Here, since the observed data is described by a multinomial\ndistribution and the prior is speciﬁed in terms of a Dirichlet distribution, the resulting\nposterior distribution is Dirichlet.\nA multinomial distribution and a Dirichlet distribution are multiplied by adding\nthe frequency counts associated with each possible event in the multinomial with the\nprior frequency count as speciﬁed by the Dirichlet. The resulting sums specify the\nparameters that describe a posterior Dirichlet distribution [38].\nD(f1 + g1, . . . , fq−1 + gq−1, fq + gq) = M(f1, . . . , fq−1, fq) + D(g1, . . . , gq−1, gq)\nThis deﬁnes a distribution from which values can be sampled to approximate the\nposterior distribution of the parameter estimates.\nIn this discussion, Gibbs Sampling is cast as a non–deterministic version of the\nEM algorithm.\nThis treatment is similar in spirit to that of [15], where the EM\nalgorithm is treated as a deterministic version of Gibbs Sampling.\n1. Stochastic E–Step: The expected values of the suﬃcient statistics are cal-\nculated. These are the counts of the marginal events deﬁned by the marginal\n61\ndistributions of the Naive Bayes model, (Fi, S). However, before the marginal\nevents can be counted, values for S must be imputed for each instance in the\ndata sample. In the EM algorithm these values are obtained by ﬁnding the\nvalue of S that maximizes ˆp(S|F1, F2, . . . , Fn−1, Fn). In Gibbs Sampling, these\nvalues are imputed via sampling from that same conditional distribution:\nS ∼ˆp(S|f1, f2, . . . , fn−1, fn) =\nˆp(S) × Qn\ni ˆp(Fi|S)\nˆp(f1, f2, . . . , fn−1, fn)\n(4.11)\nThis conditional distribution is based upon values for ˆp(S) and ˆp(Fi|S) that\nare arrived at via sampling during the previous iteration of the stochastic M–\nstep. If this is the ﬁrst iteration of the Gibbs Sampler, then these values come\nabout as the result of random initialization. After values for S are imputed via\nsampling, the marginal events are counted and the stochastic E–step concludes.\n2. Stochastic M–Step: The expected values of the suﬃcient statistics found\nduring the stochastic E-step are now used to re–estimate the parameters of the\nmodel. The EM algorithm makes maximum likelihood estimates directly from\nthese marginal event counts. However, in Gibbs Sampling these marginal event\ncounts are used to describe a multinomial distribution that is multiplied by\na Dirichlet prior distribution to create a Dirichlet posterior distribution from\nwhich values of the model parameters are sampled.\nThe observed frequency counts of marginal events are used to describe a multi-\nnomial distribution from which samples for the model parameters can be drawn.\nTo approximate the conditional distribution of ˆp(Fi|S) via sampling, suppose\nthere are 2 possible events when the value of S is ﬁxed. The frequency count of\neach event is represented by f1 and f2 and the multinomial distribution of this\ndata can be described by M(f1, f2). Further suppose that a non–information\nprior Dirichlet distribution is speciﬁed, i.e., D(1, 1). These two distributions\n62\nare multiplied to create a posterior Dirichlet distribution from which values for\nˆp(Fi|S) are sampled:\nˆp(Fi|S) ∼D(f1 + 1, f2 + 1) = M(f1, f2) × D(1, 1)\n(4.12)\nValues for ˆp(S) are sampled along similar lines.\nAfter values for the model\nparameters have been sampled, the stochastic M–step concludes.\n3. Convergence?: After j iterations there are chains of length j for both the\nsampled values for each model parameter and for each of the N missing sense\nvalues in the data sample. After some set number of iterations, these chains are\nchecked for convergence.\nOnce convergence is detected, the median values in the chains created during\nsampling are regarded as the estimates of parameters and missing data. For\nexample, suppose that (1, 1, 1, 2, 2, 2, 2, 2, 2, 3) is a chain that represents the\nvalues for a missing sense value sampled for a particular instance in the data\nsample. The median sense value is 2 and this value is imputed for S for that\nobservation in the sample.\n4.1.2.3. Naive Bayes example\nThe same example used to demonstrate the EM algorithm is employed here\nwith Gibbs Sampling. The data sample is shown in Table 4.1 and the parametric\nform of the model is Naive Bayes with model parameters p(S), p(F1|S), and p(F2|S).\nStochastic E–Step Iteration 1: Like the EM algorithm, Gibbs Sampling\nbegins by randomly assigning values to S. Assume that the random assignment of\nvalues to S is as shown on the left of Figure 4.8 and the expected counts of marginal\nevents, as represented in the contingency tables, are as shown in the center and right\nof that ﬁgure.\n63\nF1\nF2\nS\n1\n2\n1\n1\n2\n3\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n1\n3\n1\n1\n1\n1\n1\n2\n1\n2\n2\n2\n2\n1\nF1\n1\n2\n1\n3\n1\n4\nS\n2\n2\n2\n4\n3\n2\n0\n2\n7\n3\n10\nF2\n1\n2\n1\n1\n3\n4\nS\n2\n1\n3\n4\n3\n1\n1\n2\n7\n3\n10\nFigure 4.8. Stochastic E–Step Iteration 1\nStochastic M–Step Iteration 1: In the EM algorithm the marginal event\ncounts are used directly to make maximum likelihood estimates for the model param-\neters. However, in Gibbs Sampling no maximum likelihood estimates are computed;\ninstead, the frequency counts of observed marginal events combine with a speciﬁed\nprior distribution to describe a posterior distribution from which values for the model\nparameters are sampled.\nEach row in the contingency tables shown in Figure 4.8 represents counts of\nmarginal events where the value of S is ﬁxed. These counts can be thought of as\ndescribing a multinomial distribution that represents a conditional probability of the\nform ˆp(Fi|S). This conditional distribution is multiplied by a prior distribution to\ndeﬁne a posterior distribution from which estimated values for the model parameters\nare sampled. For example, given that S has a ﬁxed value of 1, the distribution of\nˆp(F1|S = 1) is described by M(3, 1). Thus, based on the expected counts of the\nmarginal events in Figure 4.8 and the assumption that all priors are non–informative,\nthe following sampling scheme is devised:\n64\n1\n0.03\nS\n2\n0.51\n3\n0.47\nF1\n1\n2\n1\n0.64\n0.36\n1.0\nS\n2\n0.54\n0.46\n1.0\n3\n0.76\n0.24\n1.0\nF2\n1\n2\n1\n0.37\n0.63\n1.0\nS\n2\n0.09\n0.91\n1.0\n3\n0.73\n0.27\n1.0\nFigure 4.9. Stochastic M–step Iteration 1: ˆp(S), ˆp(F1|S), ˆp(F2|S)\nˆp(F1|S = 1) ∼D(4, 2) = M(3, 1) × D(1, 1)\nˆp(F1|S = 2) ∼D(3, 3) = M(2, 2) × D(1, 1)\nˆp(F1|S = 3) ∼D(3, 1) = M(2, 0) × D(1, 1)\nˆp(F2|S = 1) ∼D(2, 4) = M(1, 3) × D(1, 1)\nˆp(F2|S = 2) ∼D(2, 4) = M(1, 3) × D(1, 1)\nˆp(F2|S = 3) ∼D(2, 2) = M(1, 1) × D(1, 1)\nˆp(S) ∼D(5, 5, 3) = M(4, 4, 2) × D(1, 1, 1)\nThe parameter estimates shown in Figure 4.9 are the result of this sampling\nplan.\nStochastic E–Step Iteration 2: After the ﬁrst iteration of Gibbs Sampling,\nall subsequent iterations arrive at new values for the marginal event counts by sam-\npling new values for S from the following conditional distribution:\nS ∼ˆp(S|F1, F2) = ˆp(S) × ˆp(F1|S) × ˆp(F2|S)\nˆp(F1, F2)\n(4.13)\nThe estimates of this conditional distribution are based upon the estimates of\nthe model parameters shown in Figure 4.9; these were obtained via sampling during\n65\nF1\nF2\nS\nˆp(S|F1, F2)\n1\n1\n1\n.024\n1\n1\n2\n.085\n1\n1\n3\n.891\n1\n2\n1\n.033\n1\n2\n2\n.699\n1\n2\n3\n.267\n2\n1\n1\n.100\n2\n1\n2\n.525\n2\n1\n3\n.375\n2\n2\n1\n.028\n2\n2\n2\n.852\n2\n2\n3\n.120\nFigure 4.10. E–Step Iteration 2\nthe stochastic M–step of the previous iteration. The estimated values of ˆp(S|F1, F2)\nare shown in Figure 4.10.\nRather than simply imputing the value of S that maximizes ˆp(S|F1, F2), as is the\ncase in the EM algorithm, values of S are sampled from the conditional distributions\nˆp(S|F1, F2). The result of this sampling process is an imputed value for S for a given\npair of feature values. The resulting data sample after imputation of S is shown on\nthe left in Figure 4.11. As an example of how Gibbs Sampling diﬀers from the EM\nalgorithm, note that the ﬁrst two observations in the data sample have the same\nobserved feature values, (F1 = 1, F2 = 2). However, the stochastic E–step imputes\ndiﬀerent values of S for these observations. This occurs because a value of 2 is imputed\nfor S with a probability of 70% while a value of 1 is imputed with a probability of\n27%. In the E–step of the EM algorithm, only the value of S that maximizes the\nconditional probability is imputed.\nFigure 4.11 shows the contingency tables of marginal event counts that result\nafter values for S are imputed.\n66\nF1\nF2\nS\n1\n2\n2\n1\n2\n3\n2\n2\n2\n2\n2\n2\n1\n2\n3\n1\n1\n3\n1\n1\n3\n1\n1\n3\n1\n2\n2\n2\n2\n2\nF1\n1\n2\n1\n0\n0\n0\nS\n2\n2\n3\n5\n3\n5\n0\n5\n7\n3\n10\nF2\n1\n2\n1\n0\n0\n0\nS\n2\n0\n5\n5\n3\n3\n2\n5\n3\n7\n10\nFigure 4.11. Stochastic E–Step Iteration 2\nStochastic M–Step Iteration 2 Given the marginal event counts found dur-\ning the stochastic E–step, shown in Figure 4.11, sampling from the Dirichlet posterior\nof the model parameters is performed according to the following scheme:\nˆp(F1|S = 1) ∼D(1, 1) = M(0, 0) × D(1, 1)\nˆp(F1|S = 2) ∼D(3, 4) = M(2, 3) × D(1, 1)\nˆp(F1|S = 3) ∼D(6, 1) = M(5, 0) × D(1, 1)\nˆp(F2|S = 1) ∼D(1, 1) = M(0, 0) × D(1, 1)\nˆp(F2|S = 2) ∼D(1, 6) = M(0, 5) × D(1, 1)\nˆp(F2|S = 3) ∼D(4, 3) = M(3, 2) × D(1, 1)\nˆp(S) ∼D(1, 6, 6) = M(0, 5, 5) × D(1, 1, 1)\nFigure 4.12 shows the sampled estimates for ˆp(S), ˆp(F1|S), and ˆp(F2|S). This\nconcludes the second iteration of the Gibbs Sampler.\n67\n1\n0.06\nS\n2\n0.54\n3\n0.40\nF1\n1\n2\n1\n0.25\n0.75\n1.0\nS\n2\n0.68\n0.32\n1.0\n3\n0.93\n0.06\n1.0\nF2\n1\n2\n1\n0.16\n0.84\n1.0\nS\n2\n0.05\n0.95\n1.0\n3\n0.52\n0.48\n1.0\nFigure 4.12. Stochastic M–step Iteration 2: ˆp(S), ˆp(F1|S), ˆp(F2|S)\nStochastic E–step Iteration 3 Given the parameter estimates from the pre-\nvious stochastic M–step, shown in Figure 4.12, conditional distributions for S given\nthe values of the observed features are deﬁned, per Equation 4.13.\nThe resulting distributions are shown in Figure 4.13. From those distributions\nimputed values for S are obtained via sampling. The updated data sample is shown\nin Figure 4.14.\nOnce again, the expected counts of marginal events are represented in contin-\ngency tables. These will be used to describe multinomial distributions that will be\nused in conjunction with a non–informative prior to create the posterior distributions\nfrom which new estimates of the model parameters will be sampled.\nNormally Gibbs Sampling performs hundreds of iterations before it is checked\nfor convergence. However, in the interest of brevity no further calculations or sampling\noperations will be shown. Unlike the EM algorithm, Gibbs Sampling does not stop\nitself. It must be told how many iterations to perform and then the resulting chains\nof parameter estimates and chains of missing values are checked for convergence. If\nconvergence does not occur then some ﬁxed number of additional iterations must be\nperformed and then, once again, the resulting chains must be checked for convergence.\n68\nF1\nF2\nS\nˆp(S|F1, F2)\n1\n1\n1\n0.009\n1\n1\n2\n0.085\n1\n1\n3\n0.906\n1\n2\n1\n0.024\n1\n2\n2\n0.645\n1\n2\n3\n0.331\n2\n1\n1\n0.250\n2\n1\n2\n0.321\n2\n1\n3\n0.429\n2\n2\n1\n0.181\n2\n2\n2\n0.759\n2\n2\n3\n0.056\nFigure 4.13. Stochastic E–Step Iteration 3\nF1\nF2\nS\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n1\n2\n3\n1\n1\n3\n1\n1\n3\n1\n1\n3\n1\n2\n2\n2\n2\n2\nFigure 4.14. Stochastic E–Step Iteration 3\n69\n4.2. Agglomerative Clustering\nIn general, clustering methods rely on the assumption that classes of events\noccupy distinct regions in a feature space. The distance between two points in a\nmulti–dimensional space can be measured using any of a wide variety of metrics (see,\ne.g.\n[30]).\nObservations are grouped in the manner that minimizes the distance\nbetween the members of each cluster. When applied to word sense disambiguation,\neach cluster represents a particular sense group of an ambiguous word.\nWard’s minimum–variance clustering and McQuitty’s similarity analysis are\nagglomerative clustering algorithms that only diﬀer in regards to their distance mea-\nsures. All agglomerative algorithms begin by placing each observation in a unique\ncluster, i.e. a cluster of one. The two closest clusters are merged to form a new cluster\nthat replaces the two merged clusters. Merging of the two closest clusters continues\nuntil some pre–speciﬁed number of clusters remain.\nHowever, natural language data does not immediately lend itself to a distance–\nbased interpretation. Typical features represent part–of–speech tags, morphological\ncharacteristics, and word co-occurrence; such features are nominal and their values\ndo not have scale. However, suppose that the values of a part–of–speech feature are\nrepresented numerically such that noun = 1, verb = 2, adjective = 3, and adverb = 4.\nWhile distance measures could be computed using this representation, they would be\nmeaningless since the fact that a noun has a smaller value than an adverb is purely\narbitrary and reﬂects nothing about the relationship between nouns and adverbs.\nThus, before a clustering algorithm is employed, the data must be converted\ninto a form where spatial distances actually convey a meaningful relationship between\nobservations. In this dissertation this is done by representing the data sample as a\ndissimilarity matrix. Given N observations in a data sample, this can be represented\nin a N × N dissimilarity matrix such that the value in cell (i, j), where i represents\nthe row number and j represents the column, is equal to the number of features in\nobservations i and j that do not match.\n70\nnoun\nverb\ncar\nadjective\nverb\ndefeat\nadverb\nverb\ncar\nnoun\nverb\ncar\n0\n2\n1\n0\n2\n0\n2\n2\n1\n2\n0\n1\n0\n2\n1\n0\nFigure 4.15. Matrix of Feature Values, Dissimilarity Matrix\nFor example, in Figure 4.15 the matrix on the left represents a data sample\nconsisting of four observations, where each observation has three nominal features.\nThis sample is converted into a 4 × 4 dissimilarity matrix that is shown on the left\nin this ﬁgure. In the dissimilarity matrix, cells (1, 2) and (2, 1) have the value 2,\nindicating that the ﬁrst and second observations in the matrix of feature values have\ndiﬀerent values for two of the three features. A value of 0 indicates that observations\ni and j are identical.\nWhen clustering this data, each observation is represented by its corresponding\nrow (or column) in the dissimilarity matrix. Using this representation, observations\nthat fall close together in feature space are likely to belong to the same class and are\ngrouped together into clusters. In this dissertation, Ward’s and McQuitty’s methods\nare used to form clusters of observations; each cluster corresponds to a sense group\nof related instances of an ambiguous word.\n4.2.1. Ward’s minimum–variance method\nIn Ward’s method, the internal variance of a cluster is the sum of squared\ndistances between each observation in the cluster and the mean observation for that\ncluster, i.e., the average of all the observations in the cluster. At each step in Ward’s\nmethod, a new cluster, CKL, with the smallest possible internal variance, is created\nby merging the two clusters, CK and CL, that have the minimum variance between\nthem. The variance between CK and CL is computed as follows:\n71\nVKL = ||xK −xL||2\n1\nNK +\n1\nNL\n(4.14)\nwhere xK is the mean observation for cluster CK, NK is the number of observations\nin CK, and xL and NL are deﬁned similarly for CL.\nImplicit in Ward’s method is the assumption that the sample comes from a\nmixture of normal distributions [91].\nNatural language data is typically not well\ncharacterized by a normal distribution. However, when such data is converted into\na dissimilarity matrix there is reason to believe that a normal approximation is ad-\nequate. The number of features employed here is relatively small, thus the number\nof possible feature mismatches between observations is limited. This tends to have\na smoothing eﬀect on data that may be quite sparse and skewed when represented\nstrictly as a matrix of feature values.\n4.2.2. McQuitty’s similarity analysis\nIn McQuitty’s method, clusters are based on a simple averaging of the number\nof dissimilar features as represented in the dissimilarity matrix.\nAt each step in McQuitty’s method, a new cluster, CKL, is formed by merging\nthe clusters CK and CL that have the fewest number of dissimilar features between\nthem. Put another way, these are the clusters that have the most number of features\nin common. The clusters to be merged, CK and CL, are identiﬁed by ﬁnding the cell\n(l, k) (or (k, l)), where k ̸= l, that has the minimum value in the dissimilarity matrix.\nOnce the new cluster CKL is created, the dissimilarity matrix is updated to\nreﬂect the number of dissimilar features between CKL and all other existing clusters.\nThe dissimilarity between any existing cluster CI and CKL is computed as:\nDKL−I = DKI + DLI\n2\n(4.15)\nwhere DKI is the number of dissimilar features between clusters CK and CI and DLI\nis similarly deﬁned for clusters CL and CI. This is simply the average number of\n72\nmismatches between each component of the new cluster and the components of the\nexisting cluster.\nUnlike Ward’s method, McQuitty’s method makes no assumptions concerning\nthe underlying distribution of the data sample [55].\n73\nCHAPTER 5\nEXPERIMENTAL DATA\n5.1. Words\nIn addition to having many possible meanings, words are also ambiguous syn-\ntactically in that they can serve as multiple possible parts–of–speech. For instance,\nline can be used as a noun, Cut the telephone line, or as a verb, I line my pockets\nwith cash. This dissertation does not address syntactic ambiguity; it is assumed that\nthis has been resolved for each of the 13 words studied here. Those words and their\npart–of–speech are as follows:\n• Adjectives: chief, common, last, and public.\n• Nouns: bill, concern, drug, interest, and line.\n• Verbs: agree, close, help, and include.\nThe line data [51] is from the ACL/DCI Wall Street Journal corpus [54] and\nthe American Printing House for the Blind corpus and tagged with WordNet [59]\nsenses. The remaining twelve words [13] are from the ACL/DCI Wall Street Journal\ncorpus and tagged with senses from the Longman Dictionary of Contemporary English\n[75]. The text that occurs with these twelve words is tagged with part–of–speech\ninformation using the Penn TreeBank tag set1.\nThe possible senses for each word are shown in Tables 5.1, 5.2, and 5.3. The\ndistribution of senses in the supervised and unsupervised learning experiments is also\n1The line data is excluded from the supervised experiments since the text from the American\nPrinting House for the Blind is not part–of–speech tagged.\n74\nshown. The sense inventories for the latter are reduced in order to eliminate very\nsmall minority senses.\nMaking ﬁne grained sense distinctions using unsupervised\ntechniques is not considered in this dissertation and remains a challenging problem\nfor future work.\nIn the supervised and unsupervised experiments a separate model is learned\nfor each word. Only sentences that contain the ambiguous word for which a model\nis being constructed are included in the learning process. This group of sentences\nis referred to as a “word–corpus”. The number of sentences in each word–corpus is\nshown in Table 5.1, 5.2, and 5.3 in the row “total count”.\n5.2. Feature Sets\nEach sentence containing an ambiguous word is reduced to a vector of feature\nvalues. One set of features is employed in the supervised learning experiments and\nthree are used in the unsupervised. All of these features occur within the sentence in\nwhich the ambiguous word occurs. Extending the features beyond sentence bound-\naries is a potential area for future work.\n5.2.1. Supervised Learning Feature Set\nThe feature set used in the supervised experiments was developed by Bruce and\nWiebe and is described in [10], [11], [12], and [13]. In subsequent discussion this is re-\nferred to as feature set BW. This feature set has one morphological feature describing\nthe ambiguous word, four part–of-speech features describing the surrounding words,\nand three co–occurrence features that indicate if certain key words occur anywhere\nwithin the sentence.\nMorphology: This feature represents the morphology of the ambiguous word.\nIt is binary for an ambiguous noun and indicates if it is singular or plural. It shows\nthe tense of an ambiguous verb and has up to 7 possible values. This feature is not\nused for adjectives. It is represented by variable M.\n75\nTable 5.1. Adjective Senses\nsupervised\nunsupervised\nchief:\nhighest in rank:\n86%\n86%\nmost important; main:\n14%\n14%\ntotal count:\n1048\n1048\ncommon:\nas in the phrase ‘common stock’:\n80%\n84%\nbelonging to or shared by 2 or more:\n7%\n8%\nhappening often; usual:\n8%\n8%\nwidely known; general; ordinary:\n3%\nof no special quality; ordinary:\n1%\nsame relationship to 2 more or quantities:\n< 1%\ntotal count:\n1113\n1060\nlast:\non the occasion nearest in the past:\n93%\n94%\nafter all others:\n6%\n6%\nleast desirable:\n< 1%\ntotal count:\n3187\n3154\npublic:\nconcerning people in general:\n56%\n68%\nconcerning the government and people:\n16%\n19%\nnot secret or private:\n11%\n13%\nfor the use of everyone:\n8%\nto become a company:\n6%\nknown to all or many:\n3%\nas in public TV or public radio\n1%\ntotal count:\n871\n715\n76\nTable 5.2. Noun Senses\nsupervised\nunsupervised\nbill:\na proposed law under consideration:\n68%\n68%\na piece of paper money or treasury bill:\n22%\n22%\na list of things bought and their price:\n10%\n10%\ntotal count:\n1341\n1341\nconcern:\na business; ﬁrm:\n64%\n67%\nworry; anxiety:\n32%\n33%\na matter of interest or importance\n3%\nserious care or interest\n2%\ntotal count:\n1490\n1429\ndrug:\na medicine; used to make medicine:\n57%\n57%\na habit-forming substance:\n43%\n43%\ntotal count:\n1217\n1217\ninterest:\nmoney paid for the use of money:\n53%\n59%\na share in a company or business:\n21%\n24%\nreadiness to give attention:\n15%\n17%\nadvantage, advancement or favor:\n8%\nactivity, etc. that one gives attention to:\n3%\nquality of causing attention to be given to:\n< 1%\ntotal count:\n2367\n2113\nline:\na wire connecting telephones:\n37%\na cord; cable:\n32%\nan orderly series:\n30%\ntotal count:\n0\n1149\n77\nTable 5.3. Verb Senses\nsupervised\nunsupervised\nagree:\nto concede after disagreement:\n74%\n74%\nto share the same opinion:\n26%\n26%\nto be happy together; get on well together:\n< 1%\ntotal count:\n1115\n1109\nclose:\nto (cause to) end:\n68%\n77%\nto (cause to) stop operation:\n20%\n23%\nto close a deal:\n6%\nto (cause to) shut:\n2%\nto (cause to) be not open to the public:\n2%\nto come together by making less space between:\n2%\ntotal count:\n1535\n1354\nhelp:\nto enhance - inanimate object:\n75%\n79%\nto assist - human object:\n20%\n21%\nto make better - human object:\n4%\nto avoid; prevent; change - inanimate object:\n1%\ntotal count:\n1398\n1328\ninclude:\nto contain in addition to other parts:\n91%\n91%\nto be a part of - human subject:\n9%\n9%\ntotal count:\n1526\n1526\n78\nTable 5.4. Supervised Co–occurrence features\nC1\nC2\nC3\nagree\nmillion\nthat\nto\nbill\nauction\ndiscount\ntreasury\nchief\neconomist\nexecutive\noﬃcer\nclose\nat\ncents\ntrading\ncommon\nmillion\nsense\nshare\nconcern\nabout\nmillion\nthat\ndrug\ncompany\nFDA\ngeneric\nhelp\nhim\nnot\nthen\ninclude\nare\nbe\nin\ninterest\nin\npercent\nrate\nlast\nmonth\nweek\nyear\npublic\ngoing\noﬀering\nschool\nPart of Speech: These features represent the part–of–speech of words within\n±i positions of the ambiguous word. Feature set BW contains features that indicate\nthe part of speech of words 1 and 2 positions to the left (–) and right (+) of the\nambiguous word. Each feature has one of 25 possible values which are derived from\nthe ﬁrst letter of the Penn TreeBank tag contained in the ACL/DCI WSJ corpus.\nThese features are represented by variables P−2, P−1, P+1, and P+2.\nCo–occurrences: These are binary features that indicate whether or not a\nparticular word occurs in the sentence with the ambiguous word.\nThe values of\nthese features are selected from among the 400 words that occur most frequently in\neach word–corpus. The three words chosen are most indicative of the sense of the\nambiguous word as judged by a test for independence. These features are represented\nby variables C1, C2, and C3 and the words whose occurrence they represent are shown\nin Table 5.4.\n79\n5.2.2. Unsupervised Learning Feature Sets\nThere are three diﬀerent feature sets employed in the unsupervised experiments.\nThis dissertation evaluates the eﬀect that diﬀerent types of features have on the\naccuracy of unsupervised learning algorithms; particular attention is paid to features\nthat occur in close proximity to the ambiguous word, i.e., “local context” features.\nAs the amount of context is increased the size of the associated event space grows and\nunsupervised methods require increasing amounts of computational time and space.\nThe unsupervised learning feature sets are designated A, B, and C. They are\ncomposed of combinations of the following ﬁve types of features.\nMorphology: This feature represents the morphology of ambiguous nouns and\nverbs. It is the same as the morphology feature in set BW.\nPart of Speech: As in feature set BW, these features represent the part–of–\nspeech of words that occur within 1 and 2 positions of the ambiguous word. However,\nin the unsupervised experiments the range of possible values for these features is\nreduced to ﬁve: noun, verb, adjective, adverb, or other. These crude distinctions are\nmade with the rule–based part–of–speech tagger incorporated in the Unix command\nstyle [19]. The tags available in the ACL/DCI WSJ corpus are not used since such\nhigh–quality, detailed tagging is not generally available for raw text. These features\nare represented by variables P5−2, P5−1, P5+1, and P5+2.\nCo–occurrences: These binary features represent whether or not certain high\nfrequency words in the sentence with the ambiguous word.\nThese features diﬀer\nfrom the co–occurrence features in set BW since sense–tagged text is not available to\nselect their values via a test of independence. Rather, the words whose occurrences\nare represented are determined by the most frequent content words2 that occur in each\nword–corpus. Three such features are used. CF1 represents the most frequent content\nword, CF2 the second most frequent, and CF3 the third. The words represented by\nthese features are shown in Table 5.5.\n2Content words are deﬁned here to include nouns, pronouns, verbs, adjectives and adverbs.\n80\nTable 5.5. Unsupervised Co–occurrence Features\nword\nCF1\nCF2\nCF3\nchief\noﬃcer\nexecutive\npresident\ncommon\nshare\nmillion\nstock\nlast\nyear\nweek\nmillion\npublic\noﬀering\nmillion\ncompany\nbill\ntreasury\nbillion\nhouse\nconcern\nmillion\ncompany\nmarket\ndrug\nfda\ncompany\ngeneric\ninterest\nrate\nmillion\ncompany\nline\nhe\nit\ntelephone\nagree\nmillion\ncompany\npay\nclose\ntrading\nexchange\nstock\nhelp\nit\nsay\nhe\ninclude\nmillion\ncompany\nyear\nUnrestricted Collocations: These features represent the most frequent words\nthat occur within ±2 positions of the ambiguous word. These features have 21 possible\nvalues. Nineteen correspond to the 19 most frequent words that occur in that ﬁxed\nposition in the word–corpus.\nThere is also a value, (none), that indicates when\nthe position i to the left or right is occupied by a word that is not among the 19\nmost frequent, and a value, (null), indicating that the position ±i falls outside the\nsentence boundary. These features are represented by variables UC−2, UC−1, UC+1,\nand UC+2. For example, the values of the unrestricted collocation features for concern\nare as follows:\n• UC−2: and, the, a, of, to, ﬁnancial, have, because, an, ’s, real, cause, calif.,\nyork, u.s., other, mass., german, jersey, (null), (none)\n• UC−1 : the, services, of, products, banking, ’s, pharmaceutical, energy, their,\nexpressed, electronics, some, biotechnology, aerospace, environmental, such,\n81\njapanese, gas, investment, (null), (none)\n• UC+1: about, said, that, over, ’s, in, with, had, are, based, and, is, has, was,\nto, for, among, will, did, (null), (none)\n• UC+2: the, said, a, it, in, that, to, n’t, is, which, by, and, was, has, its, possible,\nnet, but, annual, (null), (none)\nContent Collocations: These features represent high frequency content words\nthat occur within 1 position of the ambiguous word. The values of these features\nare determined by the most frequent content words that occur on either side of the\nambiguous word in the word–corpus.\nThese features are represented by variables\nCC−1 and CC+1. The content collocations associated with concern are as follows:\n• CC−1: services, products, banking, pharmaceutical, energy, expressed, electron-\nics, biotechnology, aerospace, environmental, japanese, gas, investment, food,\nchemical, broadcasting, u.s., industrial, growing, (null), (none)\n• CC+1: said, had, are, based, has, was, did, owned, were, regarding, have, de-\nclined, expressed, currently, controlled, bought, announced, reported, posted,\n(null), (none)\nThere is a limitation to frequency based features such as the co–occurrences and\ncollocations previously described; they contain little information about low frequency\nminority senses and are skewed towards the majority sense. Consider the values of the\nco–occurrence features associated with chief: oﬃcer, executive and president. Chief\nhas a majority class distribution of 86% and, not surprisingly, these three content\nwords are all indicative of “highest in rank”, the majority sense.\nHowever, when\nusing raw text it isn’t clear how features that are indicative of minority senses can be\nidentiﬁed. This remains an interesting question for future work.\n82\n5.2.3. Feature Sets and Event Distributions\nThe 4 feature sets used in this dissertation are designated BW, A, B, and C.\nThe supervised experiments are conducted with feature set BW and the unsupervised\nwith A, B, and C. Each of these feature sets results in a diﬀerent event space, i.e.,\nthe set of possible marginal events. The formulation of each feature set as well as the\nmaximum size of the event spaces associated with the saturated model and the Naive\nBayes model are as follows:\n• BW: M, P−2, P−1, P+1, P+2, C1, C2, C3\nSaturated Event Space: 15,857,856\nNaive Bayes Event Space: 534\n• A: M, P5−2, P5−1, P5+1, P5+2, CF1, CF2, CF3\nSaturated Event Space: 105,000\nNaive Bayes Event Space: 99\n• B: M, UC−2, UC−1, UC+1, UC+2\nSaturated Event Space: 4,084,101\nNaive Bayes Event Space: 273\n• C: M, P5−2, P5−1, P5+1, P5+2, CC−1, CC+1\nSaturated Event Space: 5,788,125\nNaive Bayes Event Space: 207\nThe minimum size of the event space depends on the number of possible senses\nand the value of the morphological feature. It also varies if possible values of a feature\nvariable do not occur in the training data. For example, if there are 20 possible values\nfor a feature and only 5 are observed in the training data the parameter estimates\nassociated with the 15 non–occurring events will be zero. The degrees of freedom of\nmodels are adjusted to eliminate zero estimates and reduce the size of the event space\nfurther.\n83\nTables 5.6 through 5.18 contrast the size of the event spaces associated with the\nsaturated model and the Naive Bayes model. This illustrates that event distributions\nare very skewed under the saturated model and that this skewness is reduced, but not\neliminated, with the Naive Bayes model. The reduction in model complexity results\nin a smaller number of marginal events that must be observed to make parameter\nestimates. The number of marginal events given the saturated model and Naive Bayes\nare shown in the row “total events”.\nFor example, as shown in Table 5.13, the number of marginal events for interest\nunder the saturated model and feature set BW is approximately 16,000,000 while\nunder Naive Bayes it is 534. Given such a large number of marginal events under the\nsaturated model it is inevitable that most of these will not be observed and parameter\nestimates will be zero since the training sample sizes are so small by comparison.\nHowever, when the model is simpliﬁed the number of marginal events is reduced and\nthe percentage of marginal events that are observed in the training data increases.\nFor interest, 99.9% of the marginal events under the saturated model for feature set\nBW are unobserved while under Naive Bayes only 36.9% are never observed. The\ndistribution of the event space for all words is smoothed and results in more reliable\nparameter estimates since the majority of possible marginal events are observed in\nthe training data.\n84\nTable 5.6. Event Distribution for Adjective chief\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n99.1\n99.9\n99.9\n19.4\n13.0\n29.5\n26.3\n1–5\n≪0.1\n0.8\n0.1\n0.1\n32.7\n22.2\n39.2\n36.8\n6–10\n≪0.1\n0.1\n0.0\n0.0\n7.1\n5.6\n9.0\n6.1\n11–50\n≪0.1\n0.1\n0.0\n0.0\n16.3\n14.8\n11.4\n11.4\n51–100\n≪0.1\n0.0\n0.0\n0.0\n9.2\n5.6\n4.2\n6.1\n101–1000\n0.0\n0.0\n0.0\n0.0\n15.3\n38.9\n6.6\n13.2\n1000+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\ntotal events\n1.4×105\n1.6×104\n6.7×105\n6.4×105\n98\n54\n166\n114\nTable 5.7. Event Distribution for Adjective common\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n99.1\n99.9\n99.9\n42.1\n7.1\n40.5\n42.2\n1–5\n≪0.1\n0.8\n≪0.1\n≪0.1\n30.6\n28.6\n27.8\n25.5\n6–10\n≪0.1\n0.1\n0.0\n0.0\n8.5\n6.0\n10.7\n4.7\n11–50\n≪0.1\n0.1\n0.0\n0.0\n11.7\n26.2\n12.3\n15.1\n51–100\n0.0\n0.0\n0.0\n0.0\n2.7\n15.5\n5.2\n6.3\n101–1000\n0.0\n0.0\n0.0\n0.0\n4.4\n16.7\n3.6\n6.3\n1000+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\ntotal events\n1.6×106\n3.0×104\n1.1×106\n1.7×106\n366\n84\n252\n192\n85\nTable 5.8. Event Distribution for Adjective last\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n95.5\n99.8\n99.9\n33.3\n1.9\n27.9\n30.5\n1–5\n≪0.1\n3.3\n0.2\n0.1\n23.4\n17.3\n12.2\n10.2\n6–10\n≪0.1\n0.4\n0.0\n0.0\n4.7\n3.8\n6.4\n7.8\n11–50\n≪0.1\n0.7\n0.0\n0.0\n16.7\n11.5\n35.5\n22.7\n51–100\n0.0\n0.1\n0.0\n0.0\n4.2\n15.4\n7.0\n10.2\n101–1000\n0.0\n0.0\n0.0\n0.0\n14.1\n32.7\n8.7\n14.1\n1000+\n0.0\n0.0\n0.0\n0.0\n3.6\n17.3\n2.3\n4.7\ntotal events\n1.0×106\n8.0×103\n4.1×105\n4.8×105\n192\n52\n172\n128\nTable 5.9. Event Distribution for Adjective public\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n99.2\n99.9\n99.9\n38.3\n8.3\n28.5\n38.3\n1–5\n≪0.1\n0.8\n≪0.1\n≪0.1\n32.0\n20.2\n37.8\n25.9\n6–10\n≪0.1\n0.0\n0.0\n0.0\n10.7\n9.5\n15.7\n9.0\n11–50\n≪0.1\n0.0\n0.0\n0.0\n12.9\n25.0\n10.8\n13.9\n51–100\n0.0\n0.0\n0.0\n0.0\n3.2\n17.9\n4.8\n6.5\n101–1000\n0.0\n0.0\n0.0\n0.0\n2.9\n19.0\n2.4\n6.5\n1000+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\ntotal events\n2.3×106\n3.0×104\n1.0×106\n1.9×106\n441\n84\n249\n201\n86\nTable 5.10. Event Distribution for Noun bill\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n99.2\n99.9\n99.9\n15.4\n10.0\n30.3\n38.3\n1–5\n≪0.1\n0.8\n≪0.1\n≪0.1\n24.4\n12.2\n25.7\n13.4\n6–10\n≪0.1\n0.0\n0.0\n0.0\n9.5\n6.7\n10.7\n10.9\n11–50\n≪0.1\n0.0\n0.0\n0.0\n27.4\n21.1\n23.4\n17.9\n51–100\n0.0\n0.0\n0.0\n0.0\n11.4\n17.8\n5.7\n8.0\n101–1000\n0.0\n0.0\n0.0\n0.0\n11.9\n32.2\n4.2\n11.4\n1000+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\ntotal events\n2.2×106\n6.0×104\n2.6×106\n3.5×106\n201\n90\n261\n201\nTable 5.11. Event Distribution for Noun concern\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n97.7\n99.9\n99.9\n25.4\n3.6\n22.8\n29.0\n1–5\n≪0.1\n2.1\n0.1\n≪0.1\n30.6\n3.6\n19.1\n13.7\n6–10\n≪0.1\n0.1\n0.0\n0.0\n8.6\n7.1\n17.9\n16.1\n11–50\n≪0.1\n0.1\n0.0\n0.0\n19.4\n25.0\n27.8\n17.7\n51–100\n0.0\n0.0\n0.0\n0.0\n4.1\n7.1\n2.5\n2.4\n101–1000\n0.0\n0.0\n0.0\n0.0\n11.9\n53.6\n9.9\n21.0\n1000+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\ntotal events\n2.9×106\n2.0×104\n6.1×105\n1.0×106\n268\n56\n162\n124\n87\nTable 5.12. Event Distribution for Noun drug\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n98.6\n99.9\n99.9\n6.6\n3.4\n15.1\n25.8\n1–5\n≪0.1\n1.3\n0.1\n≪0.1\n17.6\n10.3\n21.1\n18.0\n6–10\n≪0.1\n0.1\n0.0\n0.0\n11.0\n5.2\n19.9\n12.5\n11–50\n≪0.1\n0.1\n0.0\n0.0\n33.1\n19.0\n34.3\n19.5\n51–100\n0.0\n0.0\n0.0\n0.0\n14.0\n15.5\n1.8\n5.5\n101–1000\n0.0\n0.0\n0.0\n0.0\n17.6\n46.6\n7.8\n18.8\n1000+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\ntotal events\n2.3×106\n3.0×104\n9.6×105\n1.6×106\n136\n58\n166\n128\nTable 5.13. Event Distribution for Noun interest\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n98.7\n99.9\n99.9\n36.9\n6.9\n22.5\n33.3\n1–5\n≪0.1\n1.1\n≪0.1\n0.1\n26.2\n8.0\n25.7\n5.2\n6–10\n≪0.1\n0.1\n0.0\n0.0\n6.0\n4.6\n11.6\n3.0\n11–50\n≪0.1\n0.1\n0.0\n0.0\n16.7\n16.1\n26.1\n20.0\n51–100\n0.0\n0.0\n0.0\n0.0\n6.2\n17.2\n5.6\n11.1\n101–1000\n0.0\n0.0\n0.0\n0.0\n7.5\n42.5\n8.0\n25.9\n1000+\n0.0\n0.0\n0.0\n0.0\n0.6\n4.6\n0.4\n1.5\ntotal events\n1.6×107\n4.5×104\n1.4×106\n6.8×105\n534\n87\n249\n135\n88\nTable 5.14. Event Distribution for Noun line\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\nna\n97.9\n99.9\n99.9\nna\n2.4\n22.4\n34.4\n1–5\nna\n2.0\n0.1\n≪0.1\nna\n9.5\n34.6\n20.4\n6–10\nna\n0.1\n0.0\n0.0\nna\n4.8\n14.2\n7.5\n11–50\nna\n0.0\n0.0\n0.0\nna\n19.0\n18.7\n12.9\n51–100\nna\n0.0\n0.0\n0.0\nna\n22.6\n2.0\n9.7\n101–1000\nna\n0.0\n0.0\n0.0\nna\n41.7\n8.1\n15.1\n1000+\nna\n0.0\n0.0\n0.0\nna\n0.0\n0.0\n0.0\ntotal events\nna\n3.0×104\n9.6×105\n1.5 ×106\nna\n84\n246\n186\nTable 5.15. Event Distribution for Verb agree\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n99.4\n99.9\n99.9\n36.2\n3.1\n16.5\n12.3\n1–5\n≪0.1\n0.5\n≪0.1\n≪0.1\n23.2\n10.9\n29.4\n34.6\n6–10\n≪0.1\n0.1\n0.0\n0.0\n6.8\n6.3\n16.5\n7.7\n11–50\n≪0.1\n0.0\n0.0\n0.0\n18.4\n28.1\n24.7\n21.5\n51–100\n0.0\n0.0\n0.0\n0.0\n3.9\n14.1\n7.1\n8.5\n101–1000\n0.0\n0.0\n0.0\n0.0\n11.1\n37.5\n5.9\n15.4\n1000+\n0.0\n0.0\n0.0\n0.0\n0.5\n0.0\n0.0\n0.0\ntotal events\n5.1×106\n2.8×106\n1.8×106\n2.8×106\n207\n64\n170\n130\n89\nTable 5.16. Event Distribution for Verb close\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n99.4\n99.9\n99.9\n28.6\n1.5\n25.3\n22.8\n1–5\n≪0.1\n0.6\n≪0.1\n≪0.1\n36.2\n16.7\n23.0\n17.6\n6–10\n≪0.1\n0.0\n0.0\n0.0\n7.4\n6.1\n9.6\n11.0\n11–50\n≪0.1\n0.0\n0.0\n0.0\n18.6\n24.2\n29.8\n25.0\n51–100\n0.0\n0.0\n0.0\n0.0\n3.6\n15.2\n4.5\n8.8\n101–1000\n0.0\n0.0\n0.0\n0.0\n5.7\n36.4\n7.9\n14.7\n1000+\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\ntotal events\n9.5×106\n7.0×104\n2.5×106\n3.7×106\n420\n66\n178\n136\nTable 5.17. Event Distribution for Verb help\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n98.6\n99.9\n99.9\n28.7\n0.0\n8.6\n11.2\n1–5\n≪0.1\n1.4\n≪0.1\n≪0.1\n27.6\n1.6\n31.6\n28.4\n6–10\n≪0.1\n0.0\n0.0\n0.0\n8.1\n3.2\n23.0\n13.4\n11–50\n≪0.1\n0.0\n0.0\n0.0\n21.3\n33.9\n25.9\n23.1\n51–100\n0.0\n0.0\n0.0\n0.0\n6.6\n17.7\n3.4\n7.5\n101–1000\n0.0\n0.0\n0.0\n0.0\n7.0\n43.5\n7.5\n16.4\n1000+\n0.0\n0.0\n0.0\n0.0\n0.7\n0.0\n0.0\n0.0\ntotal events\n6.9×106\n4.8×104\n2.0×106\n2.6×106\n272\n62\n174\n134\n90\nTable 5.18. Event Distribution for Verb include\nevent\nSaturated\nNaive Bayes\ncount\nBW\nA\nB\nC\nBW\nA\nB\nC\n0\n99.9\n98.8\n99.9\n99.9\n13.7\n1.6\n25.9\n23.1\n1–5\n≪0.1\n1.1\n≪0.1\n≪0.1\n26.0\n12.5\n18.4\n23.1\n6–10\n≪0.1\n0.1\n0.0\n0.0\n6.8\n6.3\n15.5\n14.9\n11–50\n0.0\n0.0\n0.0\n0.0\n26.7\n28.1\n26.4\n17.9\n51–100\n0.0\n0.0\n0.0\n0.0\n10.3\n7.8\n5.2\n3.7\n101–1000\n0.0\n0.0\n0.0\n0.0\n15.1\n39.1\n8.0\n15.7\n1000+\n0.0\n0.0\n0.0\n0.0\n1.4\n0.0\n0.6\n1.5\ntotal events\n3.9×106\n6.0×104\n2.0×106\n3.2×106\n146\n61\n174\n134\n91\nCHAPTER 6\nSUPERVISED LEARNING EXPERIMENTAL RESULTS\nThe theoretical foundations of sequential model selection and the Naive Mix\nare introduced in Chapter 3. This chapter discusses four experiments that evaluate\nthese methods.1 The principal measure is disambiguation accuracy, the percentage\nof ambiguous words in a held–out test sample that are disambiguated correctly.\nThe ﬁrst experiment measures the accuracy of models selected using various\ncombinations of search strategy and evaluation criterion. The second compares the\naccuracy of the Naive Mix to several leading machine learning algorithms. The third\nexperiment studies the learning rate of the most accurate methods from the ﬁrst two\nexperiments. The ﬁnal experiment decomposes the overall classiﬁcation error of two\nof the most accurate methods into more fundamental components.\n6.1. Experiment 1: Sequential Model Selection\nIn the ﬁrst experiment, each of the eight possible combinations of search strategy\nand evaluation criterion as described in Chapter 3 are utilized to select a probabilistic\nmodel of disambiguation for each word.\nThe accuracy of each model is evaluated via 10–fold cross validation. All of the\nsense–tagged examples for a word are randomly shuﬄed and divided into 10 equal\nfolds. Nine folds are used as the training sample and the remaining fold acts as a\nheld–out test set. This process is repeated 10 times so that each fold serves as the\ntest set once. The disambiguation accuracy for each word is the average accuracy\nacross all 10 test sets.\n1The freely available software package CoCo [2] was used in conjunction with the Class.3.0 clas-\nsiﬁer [68] for all the model selection experiments.\n92\nWithin this ﬁrst experiment there are four separate analyses performed. The\nﬁrst examines the overall disambiguation accuracy of the selected models for each\nword. The second evaluates model complexity by comparing the number of interac-\ntions in the various models. The third assesses the robustness of the selection process\nrelative to changes in the search strategy. The fourth and ﬁnal analysis is a case\nstudy of the model selection process for a single word.\nAll of these evaluations assume that the overall objective of sequential model\nselection is to automatically stop the search process at an accurate model of disam-\nbiguation. However, there are alternatives to this orientation. For example, Bruce\nand Wiebe (e.g., [10], [11], and [12]) use backward search and the exact conditional\ntest to generate a sequence of models beginning with the saturated model and con-\ncluding with Naive Bayes. The most accurate model is selected from this sequence\nusing a test of predictive accuracy.\n6.1.1. Overall Accuracy\nTable 6.1 shows the accuracy and standard deviation of models selected using\neach evaluation criterion with forward (F) and backward (S) sequential search. The\naccuracy of Naive Bayes and the majority classiﬁer are also reported since they serve\nas simple benchmarks; neither performs a model search but rather rely upon assumed\nparametric forms. When averaged over all twelve words, Naive Bayes and FSS AIC\nare the most accurate approaches.2 However, the diﬀerences between Naive Bayes,\nFSS AIC, and BSS AIC are not statistically signiﬁcant for any word. Throughout\nthis evaluation, judgments regarding the signiﬁcance of diﬀerences are made using a\ntwo–sided pairwise t–test where p = .01.\nA reasonable lower bound on supervised disambiguation algorithms is the ac-\ncuracy attained by the majority classiﬁer. The majority classiﬁer is based on the\n2Each combination of strategy and criterion is sometimes referred to in an abbreviated form. For\nexample, the combination of a forward sequential search and Akaike’s Information Criteria is called\nFSS AIC.\n93\nTable 6.1. Sequential Model Selection Accuracy\nMajority\nNaive\nG2 ∼χ2\nexact\nAIC\nBIC\nBayes\nagree\n.777 .032\n.930 .026\nB\n.914 .018\n.915 .019\n.909 .026\n.924 .023\nF\n.916 .026\n.896 .030\n.911 .026\n.921 .024\nbill\n.681 .044\n.865 .026\nB\n.634 .073\n.612 .022\n.836 .036\n.850 .034\nF\n.778 .048\n.625 .039\n.851 .029\n.851 .041\nchief\n.862 .026\n.943 .015\nB\n.945 .020\n.895 .033\n.945 .020\n.936 .020\nF\n.926 .027\n.891 .043\n.939 .020\n.943 .021\nclose\n.680 .033\n.817 .023\nB\n.687 .039\n.739 .029\n.806 .029\n.742 .031\nF\n.773 .035\n.646 .037\n.810 .040\n.763 .040\ncommon\n.802 .029\n.832 .034\nB\n.843 .030\n.802 .058\n.850 .019\n.815 .030\nF\n.848 .023\n.747 .066\n.846 .023\n.815 .030\nconcern\n.639 .054\n.859 .037\nB\n.565 .055\n.753 .033\n.838 .038\n.767 .031\nF\n.820 .044\n.618 .106\n.830 .025\n.864 .038\ndrug\n.575 .033\n.807 .036\nB\n.695 .091\n.806 .040\n.792 .043\n.784 .041\nF\n.790 .052\n.527 .048\n.800 .037\n.784 .041\nhelp\n.753 .032\n.780 .033\nB\n.770 .049\n.769 .037\n.777 .036\n.797 .030\nF\n.793 .035\n.779 .037\n.798 .033\n.797 .030\ninclude\n.912 .024\n.944 .021\nB\n.922 .019\n.938 .020\n.912 .030\n.949 .016\nF\n.953 .014\n.735 .142\n.950 .012\n.950 .019\ninterest\n.529 .026\n.763 .016\nB\n.476 .040\n.498 .034\n.751 .018\n.676 .025\nF\n.713 .037\n.441 .032\n.757 .026\n.734 .020\nlast\n.933 .014\n.919 .011\nB\n.895 .023\n.849 .018\n.931 .015\n.920 .011\nF\n.898 .021\n.849 .021\n.927 .021\n.915 .012\npublic\n.560 .055\n.593 .054\nB\n.610 .048\n.551 .042\n.600 .047\n.597 .053\nF\n.616 .049\n.472 .095\n.614 .053\n.602 .050\naverage\n.725\n.838\nB\n.746\n.761\n.829\n.813\nF\n.819\n.712\n.836\n.828\n94\nmodel of independence and classiﬁes each instance of an ambiguous word with the\nmost frequent sense in the training data.\nNeither AIC nor BIC ever selects a model that results in accuracy signiﬁcantly\nless than the lower bound. This is true for both forward and backward searches.\nHowever, FSS exact conditional has accuracy signiﬁcantly less than the lower bound\nfor four words and BSS exact conditional has accuracy below the lower bound for two\nwords. FSS G2 ∼χ2 and BSS G2 ∼χ2 are signiﬁcantly less accurate than the lower\nbound for one and two words respectively. These cases are italicized in Table 6.1.\nThis behavior is suggestive of a diﬃculty in using signiﬁcance tests as evaluation\ncriteria when the objective of model selection is to automatically stop the search\nprocess at an accurate model of disambiguation. The value of α determines when\nthe selection process will stop; unfortunately there is no single value of α that leads\nto consistent results. In this experiment the α values .01, .05, .001, and .0001 are\nevaluated and .0001 is found to select the most accurate models overall. However,\nthere is considerable variation from word to word and improved results are likely if α\nis adjusted for each word.\nIt is generally expected that the accuracy of the majority classiﬁer will be im-\nproved upon by more sophisticated models. The majority classiﬁer does not take into\naccount any of the available contextual information when performing disambiguation;\npresumably a model that does will prove to be more accurate.\nHowever, there are four words where no method ever signiﬁcantly improves\nupon the majority classiﬁer; help, include, last, and public. Two of these words, last\nand include, have majority senses of over 90% so signiﬁcant improvement over the\nlower bound is not likely. But the majority senses of public and help are 56% and 75%\nso there is certainly room for improvement. However, the most accurate models for\nthese words have 4 and 6 interactions and disregard most of the features in set BW.\nFor these two words the set of features may need to be modiﬁed to disambiguate at\nhigher levels of accuracy.\n95\n6.1.2. Model Complexity\nThe number of interactions in a model is a general indicator of the complexity of\nthe model. Given n features, the saturated model has n2−n\n2\ninteractions, Naive Bayes\nhas n interactions, and the model of independence has 0. Table 6.2 shows the number\nof interactions in the models selected by each combination of evaluation criterion and\nsearch strategy. The number of interactions in Naive Bayes is included as a point of\ncomparison although this is not a selected model.\nThis table shows that BIC and G2 ∼χ2 often select models with fewer interac-\ntions than either AIC or the exact conditional test. However, these models also result\nin reduced accuracy when compared to AIC. Since BIC assesses a greater penalty on\ncomplexity than AIC, it has a stronger bias towards less complex models. As a result,\nBSS BIC is more aggressive in removing interactions than BSS AIC; similarly FSS\nBIC is more conservative than FSS AIC in adding them.\nThe comparable levels of accuracy among models selected with the information\ncriteria and Naive Bayes are curious since Table 6.2 shows that these methods select\nmodels of diﬀering complexity. For example, the model selected for bill by FSS AIC\nhas 20 interactions, the model selected by FSS BIC has 11, and Naive Bayes has only\n9. However, the accuracy of these 3 models is nearly identical.\nThe fact that models of diﬀering levels of complexity yield similar levels of\naccuracy demonstrates that model selection is an uncertain enterprise. In other words,\nthere is not a single model for a word that will result in overall superior disambiguation\naccuracy. This motivates the development of the Naive Mix, an extension to the\nsequential model selection process that is evaluated later in this chapter.\n6.1.3. Model Selection as a Robust Process\nA model selection process is robust when models of similar accuracy are se-\nlected as a result of both a forward and a backward search using the same evaluation\ncriterion. In general the information criteria result in a robust selection process while\nthe signiﬁcance tests do not.\n96\nTable 6.2. Complexity of Selected Models\nNaive\nG2 ∼χ2\nexact\nAIC\nBIC\nBayes\n.0001\n.0001\nagree\n9\nB\n8\n10\n15\n9\nF\n12\n15\n13\n7\nbill\n9\nB\n22\n25\n26\n7\nF\n20\n28\n20\n11\nchief\n8\nB\n6\n17\n14\n6\nF\n6\n18\n14\n7\nclose\n9\nB\n12\n13\n13\n3\nF\n13\n19\n10\n3\ncommon\n8\nB\n4\n10\n7\n2\nF\n4\n16\n7\n2\nconcern\n9\nB\n5\n15\n16\n6\nF\n17\n24\n13\n9\ndrug\n9\nB\n10\n7\n14\n9\nF\n10\n19\n12\n9\nhelp\n9\nB\n7\n6\n6\n4\nF\n3\n9\n4\n4\ninclude\n9\nB\n6\n3\n16\n8\nF\n6\n22\n9\n9\ninterest\n9\nB\n24\n24\n21\n6\nF\n22\n32\n15\n4\nlast\n8\nB\n8\n9\n14\n9\nF\n15\n18\n14\n2\npublic\n8\nB\n7\n9\n8\n3\nF\n6\n11\n6\n3\naverage\n9\nB\n10\n12\n14\n6\nF\n11\n19\n11\n6\n97\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFSS\nBSS\nAIC ✸\n✸\n✸\n✸\n✸\n✸\n✸✸\n✸\n✸\n✸\n✸\n✸\nBIC +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nBSS=FSS\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFSS\nBSS\nexact ✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\nG2 ∼χ2 +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nBSS=FSS\nFigure 6.1. Robustness of Selection Process\n98\nIn Figure 6.1 each point represents the accuracy of models selected using the\nspeciﬁed evaluation criterion with backward and forward search. The BSS coordinate\nis the accuracy attained by the model selected during backward search while the FSS\ncoordinate is the accuracy resulting from forward search. Points that fall close to the\nline BSS = FSS represent an evaluation criterion that selects models of similar\naccuracy regardless of search strategy. Figure 6.1 (top) shows that, in general, the\ninformation criteria select models of similar accuracy using either forward or backward\nsearch.\nHowever, Figure 6.1 (bottom) shows that the signiﬁcance tests are sensitive to\nchanges in search strategy. For example, BSS exact conditional is more accurate than\nFSS exact conditional. FSS G2 ∼χ2 is slightly more accurate than BSS G2 ∼χ2.\nThis suggests that the value of α may need to be adjusted depending on the direction\nof the search strategy. The following section shows that this is due to changes in the\nbias of the evaluation criteria that are caused by changing the search strategy.\n6.1.4. Model selection for Noun interest\nThe model selection process for interest is discussed in some detail here. Figures\n6.2 and 6.3 show the accuracy and recall3 of the model at each level of complexity\nduring the selection process. The rightmost point on each plot is the measure as-\nsociated with the model ultimately selected by the evaluation criterion and search\nstrategy.\nAs shown in Table 6.2, BIC selects models that have fewer interactions than the\nother criteria. Table 6.1 indicates that this often results in less accurate classiﬁcation\nthan AIC. In Figure 6.2, BSS BIC (top) removes too many interactions and goes\npast the more accurate model selected by AIC, while FSS BIC (bottom) does not\nadd enough interactions and stops short of selecting a highly accurate model. For\n3The percentage of ambiguous words in a held out test sample that are disambiguated, correctly\nor not. A word is not disambiguated if any of the model parameters needed to assign a sense tag\ncannot be estimated from the training sample.\n99\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n5\n10\n15\n20\n25\n30\n35\n%\n# of interactions in model\nAIC ⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\nBIC ✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\nExact α = .0001 △\n△\n△\n△\n△\n△\n△\n△△\n△\nG2 ∼χ2 α = .0001\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n5\n10\n15\n20\n25\n30\n35\n%\n# of interactions in model\nAIC ⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\nBIC ✸\n✸\n✸\n✸\n✸\n✸\nExact α = .0001 △\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\nG2 ∼χ2 α = .0001\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\nFigure 6.2. BSS (top) and FSS (bottom) accuracy for Noun interest\n100\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n5\n10\n15\n20\n25\n30\n35\n%\n# of interactions in model\nAIC ⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\nBIC ✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\nExact α = .0001 △\n△\n△\n△\n△\n△\n△\n△△\n△\nG2 ∼χ2 α = .0001\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n5\n10\n15\n20\n25\n30\n35\n%\n# of interactions in model\nAIC ⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\n⋆\nBIC ✸\n✸\n✸\n✸\n✸\n✸\nExact α = .0001 △\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\n△\nG2 ∼χ2 α = .0001\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\n❜\nFigure 6.3. BSS (top) and FSS (bottom) recall for Noun interest\n101\ninterest, BSS BIC selects a model of 6 interactions while FSS BIC selects a model of\n4 interactions. The other combinations of strategy and criterion select models with\nbetween 15 and 32 interactions and result in higher levels of accuracy. As mentioned\npreviously, the bias of BIC towards models with small numbers of interactions is not\nsurprising given the large penalty that it assesses to complexity.\nThe exact conditional test suﬀers from the reverse problem in that it selects\nmodels with too many interactions. BSS exact conditional removes a small number\nof interactions while FSS exact conditional adds a great many; in both cases the\nresulting models have lower accuracy than the other approaches.\nFigure 6.3 shows that for both forward and backward search, models of relatively\nlow recall are selected by the exact conditional test. This suggests that the selected\nmodels are overly complex and contain many parameters that can not be estimated\nfrom the training data.\nThe contrast between the exact conditional test and the other criteria is stark\nduring backward search. Figures 6.2 (top) and 6.3 (top) show that the exact condi-\ntional test remains at low levels of accuracy and recall while the other criteria rapidly\nincrease both recall and accuracy. However, during forward search Figures 6.2 (bot-\ntom) and 6.3 (bottom) show there is little diﬀerence among the criteria; all select\nhigh recall models that achieve high accuracy early in the search.\nA backward search begins with the saturated model. For feature set BW sat-\nurated models have millions of parameters to estimate.\nThe information criteria\nremove the interactions that result in models with the highest degrees of freedom in\nthe early stages of the search. This is a consequence of the complexity penalty that\nboth AIC and BIC assess. The signiﬁcance test G2 ∼χ2 also targets interactions with\nhigh degrees of freedom for removal. When G2 values are assigned signiﬁcance there\nis an implicit weighting for complexity. Larger degrees of freedom result in smaller\nsigniﬁcance values due to the nature of the χ2 distribution. Interactions that result\nin models with very high degrees of freedom and are more likely to be removed early\nin the course of a backward search since they have smaller signiﬁcance values.\n102\nThe bias of the information criteria and G2 ∼χ2 towards removing interactions\nwith high degrees of freedom results in a rapid reduction in the number of model\nparameters to estimate. This in turn increases the percentage of model parameters\nthat can be estimated from the training data. In Figure 6.3 (top), recall increases\nrapidly as interactions with high degrees of freedom are removed by the information\ncriteria. Most of the model parameters can be estimated from the training data early\nin the search process and this results in a rapid increase in accuracy.\nOn the other hand, the exact conditional test does not take degrees of freedom\ninto account when evaluating models. Interactions are removed or added via signif-\nicance values that are based upon the distribution of randomly generated values of\nG2 relative to the observed value of G2. Thus, the exact conditional test may result\nin the removal of interactions with relatively small degrees of freedom in cases where\nthe information criteria and G2 ∼χ2 would remove interactions with higher degrees\nof freedom. During a backward search the exact conditional test has a diﬀerent bias;\nthe other criteria tend to remove interactions that result in models with large degrees\nof freedom while the exact conditional test removes interactions that maintain the ﬁt\nof the model to the training data.\nHowever, during forward search the exact conditional test achieves approxi-\nmately the same levels of recall and accuracy as do the other criteria. Forward search\nbegins with the model of independence. The information criteria add those interac-\ntions that most increase the ﬁt of the model; in the early stages of forward search this\nresults in a bias towards interactions that have lower degrees of freedom. The same\noccurs with G2 ∼χ2 since it seeks to add those interactions that have the largest\nsigniﬁcance value.\nInteractions with smaller degrees of freedom will tend to have\nhigher signiﬁcance values due to the nature of the χ2 distribution. Thus, during a\nforward search all of the criteria are biased towards the inclusion of interactions with\nlower degrees of freedom. This results in models that have relatively small numbers\nof model parameters; both recall and accuracy are likely to be high.\n103\nThe only diﬀerence among the criteria during forward search is that the signiﬁ-\ncance tests tend to add too many interactions to the model. During a forward search\nwhere the evaluation criteria is a signiﬁcance test, the value of α should be larger than\nin the backward search in order to stop the selection process sooner. This has the\nadded beneﬁt of reducing the computational complexity of the exact conditional test\nsince the number of random tables that must be generated to assess the signiﬁcance\nof each interaction is 1/α [48].\n6.2. Experiment 2: Naive Mix\nThe Naive Mix is deﬁned in Section 3.2 and extends sequential model selection\nmethods by allowing for the incorporation of uncertainty in the development of the\nmodel. Rather than attempting to ﬁnd a single most accurate model, the Naive Mix\nforms an averaged probabilistic model from the sequence of models generated during\nFSS AIC. In this experiment the Naive Mix is compared to the following machine\nlearning algorithms:\nPEBLS [23]: A k nearest–neighbor algorithm where classiﬁcation is performed\nby assigning an ambiguous word to the majority class of the k–nearest training ex-\namples. In these experiments each ambiguous word is assigned the sense of the single\nmost similar training example, i.e., k = 1.\nC4.5 [78]: A decision tree learner in which classiﬁcation rules are formulated\nby recursively partitioning the training sample. Each nested partition is based on the\nfeature value that provides the greatest increase in the information gain ratio for the\ncurrent partition.\nCN2 [22]: A rule induction algorithm that selects classiﬁcation rules that cover\nthe largest possible subsets of the training sample as measured by the Laplace error\nestimate.\nThe Naive Mix, C4.5, CN2, and any model selection method using forward\nsequential search all perform general–to–speciﬁc searches that add features to the\nlearned representation of the training sample based on some measure of information\n104\ncontent increase. These methods all perform feature selection and have a bias towards\nsimpler models. All of these methods can suﬀer from fragmentation when learning\nfrom sparse training data. Fragmentation occurs when the model is complex, in-\ncorporating a large number of feature values to describe a small number of training\ninstances. When this occurs, there is inadequate support in the training data for\nthe inference being speciﬁed by the model. The Naive Mix is designed to reduce the\neﬀects of fragmentation in a general–to–speciﬁc search by averaging the distributions\nof high complexity models with those of low complexity models that include only the\nmost relevant features.\nThe nearest–neighbor algorithm PEBLS shares a number of traits with Naive\nBayes. Neither perform a search to create a representation of the training sample.\nNaive Bayes assumes the form of a model in which all features are regarded as relevant\nto disambiguation but, as in PEBLS, their interdependencies are not considered.\nWeights are assigned to features via parameter estimates from the training sample.\nThese weights allow some discounting of less relevant features. Here, PEBLS stores\nall instances of the training sample and treats each feature independently and equally,\nmaking it susceptible to irrelevant features.\nTable 6.3 shows the accuracy of the Naive Mix, Naive Bayes, the majority\nclassiﬁer, C4.5, CN2, and PEBLS. The accuracies of the sequential model selection\nmethods from the ﬁrst experiment are directly comparable to these since both are\nevaluated via 10–fold cross validation using all of the sense–tagged examples for each\nword.\nBased on a word by word comparison, this experiment shows that the Naive\nMix improves upon the accuracy of models selected by FSS AIC. However, in general\nthere prove to be few signiﬁcant diﬀerences between the accuracy of the Naive Mix,\nNaive Bayes, C4.5, PEBLS, and CN2.\nThe success of Naive Bayes in the ﬁrst two experiments is a bit surprising since\nit neither performs feature selection nor a model search. Despite this, it is among the\nmost accurate of the methods considered in this study.\n105\nTable 6.3. Naive Mix and Machine Learning Accuracy\nMajority\nNaive\nPEBLS\nNaive\nclassiﬁer\nBayes\nk=1\nC4.5\nCN2\nMix\nagree\n.777 .032\n.930 .026\n.928 .030\n.947 .031\n.947 .031\n.948 .017\nbill\n.681 .044\n.865 .026\n.855 .034\n.878 .029\n.873 .035\n.897 .026\nchief\n.862 .026\n.943 .015\n.945 .018\n.947 .020\n.945 .013\n.951 .016\nclose\n.680 .033\n.817 .023\n.843 .042\n.853 .021\n.834 .036\n.831 .033\ncommon\n.802 .029\n.832 .034\n.853 .019\n.871 .030\n.803 .029\n.853 .024\nconcern\n.639 .054\n.859 .037\n.840 .036\n.852 .042\n.859 .033\n.846 .039\ndrug\n.575 .033\n.807 .036\n.778 .034\n.798 .038\n.777 .069\n.815 .041\nhelp\n.753 .032\n.780 .033\n.710 .047\n.790 .039\n.779 .045\n.796 .038\ninclude\n.912 .024\n.944 .021\n.939 .015\n.954 .019\n.951 .018\n.956 .018\ninterest\n.529 .026\n.763 .016\n.768 .020\n.793 .019\n.729 .034\n.800 .019\nlast\n.933 .014\n.919 .011\n.947 .012\n.945 .008\n.935 .013\n.940 .016\npublic\n.560 .055\n.593 .054\n.536 .039\n.598 .047\n.579 .057\n.615 .055\naverage\n.725\n.838\n.829\n.852\n.834\n.854\n106\nTable 6.4. Naive Bayes Comparison\nAverage\nwin–tie–loss\nAccuracy\nNaive Mix\n.854\n0–9–3\nC4.5\n.852\n0–9–3\nNaive Bayes\n.838\nFSS AIC\n.836\n0–12–0\nBSS AIC\n.829\n0–12–0\nPEBLS\n.829\n1–10–1\nFSS BIC\n.828\n1–11–0\nFSS G2\n.819\n1–11–0\nBSS BIC\n.813\n3–9–0\nBSS exact\n.761\n6–6–0\nBSS G2\n.746\n5–7–0\nMajority\n.725\n8–4–0\nFSS exact\n.712\n9–3–0\nTable 6.4 summarizes the accuracy of Naive Bayes relative to the other methods\nemployed in the ﬁrst two experiments. The accuracy reported is based on 10–fold\ncross validation. The win–tie–loss measure is also shown; this indicates the number\nof times Naive Bayes is signiﬁcantly more–equally–less accurate than the competing\nmethod. The win–tie–loss record 1–10–1 associated with PEBLS means that Naive\nBayes is signiﬁcantly more accurate than PEBLS for 1 word, not signiﬁcantly diﬀerent\nthan PEBLS for 10 words, and signiﬁcantly less accurate than PEBLS for 1 word.\nThis measure shows that there are only 7 out of a possible 144 cases where Naive Bayes\nis signiﬁcantly less accurate than a competing method. The cases where a competing\nmethod is signiﬁcantly more accurate than Naive Bayes are shown in Table 6.3 in\nbold face. There is no such case in Table 6.1; Naive Bayes is never signiﬁcantly less\naccurate than a sequential model selection method in the ﬁrst experiment.\n107\nThe success of Naive Bayes in these experiments conﬁrms the results of previous\nstudies of disambiguation. For instance, [51] compares a neural network, Naive Bayes,\nand a content vector when disambiguating six senses of line.4 They report that all\nthree methods are equally accurate. The line data is utilized again in [62] with an\neven wider range of methods. Naive Bayes, a perceptron, a decision tree learner,\na nearest–neighbor classiﬁer, a logic based disjunctive normal form learner, a logic\nbased conjunctive normal form learner, and a decision list learner are compared. Naive\nBayes and the perceptron are found to be the most accurate approaches. Finally, [63]\ncompare PEBLS and Naive Bayes and ﬁnds them to be of comparable accuracy when\ndisambiguating the Defence Science Organization sense–tagged corpus [64]. However,\nall of these studies diﬀer from this dissertation in that they employ a feature set that\nconsists of thousands of binary co–occurrence features, each of which represents the\noccurrence of a particular word within some ﬁxed distance of the ambiguous word.\nThis feature set is commonly known as bag–of–words.\nThe relatively high accuracy achieved by Naive Bayes in disambiguation is some-\ntimes explained as a consequence of the bag–of–words feature set, e.g., [70]. Given\nso many features, the assumptions of conditional independence made by Naive Bayes\nare potentially valid and may result in a model that ﬁts the training data reasonably\nwell. However, this explanation does not apply to feature set BW since a previous\nstudy [13] shows that all of these features are good indicators of the sense of the\nambiguous word. In addition, Naive Bayes is successful in a number of other domains\nwhere the bag–of–words explanation is not relevant.\nFor example, [22] compare Naive Bayes, a rule induction system, and a deci-\nsion tree learner. They ﬁnd that Naive Bayes performs as accurately as these more\nsophisticated methods in various medical diagnosis problems.\nA more extensive study of Naive Bayes appears in [49]. They compare Naive\nBayes and a decision tree learner using data from the University of California at\n4This data is described in Chapter 5, Experimental Data.\n108\nIrvine (UCI) Machine Learning repository [57]. For 4 of 5 naturally occurring data\nsets they report that Naive Bayes is the more accurate. They also present an average\ncase analysis of Naive Bayes that is veriﬁed empirically using artiﬁcial data.\nNaive Bayes and more elaborate Bayesian networks that diagnose the cause of\nacute abdominal pain are compared in [76]. They argue that simple classiﬁcation\nmodels will often outperform more detailed ones if the domain is complex and the\namount of data available is relatively small. Their experiment consists of 1270 cases,\neach of which has 169 features.\nThey ﬁnd that the Naive Bayes model with 169\ninteractions is more accurate than a Bayesian network that has 590 interactions.\nA software agent that learns to rate Web pages according to a user’s level of\ninterest is discussed in [66]. They construct a proﬁle using examples of pages that a\nuser likes and dislikes. They compare Naive Bayes, a nearest–neighbor algorithm, a\nterm–weighting method from information retrieval, a perceptron, and a multi–layer\nneural network and ﬁnd that Naive Bayes is most accurate at predicting Web pages\na user will ﬁnd interesting.\nFinally, [32] compare the accuracy of Naive Bayes with a decision tree learner,\na nearest–neighbor algorithm, and a rule induction system. They report that Naive\nBayes is at least as accurate as the rule induction system and nearest–neighbor algo-\nrithm for 22 of 28 UCI data sets and at least as accurate as the decision tree learner\nfor 20 of 28 data sets. They also present an extensive analysis of the conditions under\nwhich Naive Bayes is an optimal classiﬁer even when the conditional independence\nassumptions are not valid.\n6.3. Experiment 3: Learning Rate\nThe ﬁrst two experiments suggest that Naive Bayes may be an eﬀective gen-\neral purpose method of disambiguation. However, these experiments only study the\ndisambiguation accuracy of models that are learned from relatively large amounts of\ntraining data, i.e, 90% of the total available sense–tagged text for a word.\n109\nThe third experiment diﬀerentiates among the most accurate methods in the\nﬁrst two experiments by training each algorithm with steadily increasing amounts of\ntraining data and studying the learning rate. This shows the relationship between\naccuracy and the number of training examples. A “slow” learning rate implies that\naccuracy gradually increases with the number of training examples. A “fast” learning\nrate suggests an algorithm that attains high accuracy with a very small number of\nexamples.\nThis experiment compares the learning rates of the Naive Mix, Naive\nBayes, C4.5, and FSS AIC.\nA variant of 10–fold cross validation is employed. Each word–corpus is divided\ninto 10 folds; the desired number of training examples are sampled from 9 folds and\nthe remaining fold is held out as the test set. Each algorithm learns a model from the\ntraining data and uses this to disambiguate the test set. This is repeated until each\nfold serves as the test set once. The accuracy reported is averaged over all 10 folds.\nThis procedure is repeated with increasing quantities of training data. In this\nexperiment the number of training examples is ﬁrst 10, then 50, and then 100. There-\nafter the number of examples is incremented 100 at a time until all the available\ntraining data is used. For each amount of training data the accuracy attained for all\nthe words belonging to a particular part–of–speech are averaged. These values are\nplotted in Figures 6.4 through 6.6 and show the learning rate for each method for\neach part–of–speech. Also included is the learning rate of the majority classiﬁer. This\nproves to be constant since it classiﬁes every held–out instance of an ambiguous word\nwith the most frequent sense in the training data. This is easy to correctly identify\neven with very small amounts of training data due to the skewed sense distributions.\nAdjectives C4.5, FSS AIC, and the Naive Mix achieve nearly the same level of\naccuracy learning from 10 examples as they do 900 examples. Naive Bayes has a slower\nlearning rate; accuracy is low with a small number of examples but improves with\nthe addition of training data. Naive Bayes achieves approximately the same accuracy\nas C4.5, FSS AIC, and the Naive Mix after 300 training examples. However, none of\nthe methods signiﬁcantly exceeds the accuracy of the majority classiﬁer.\n110\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\nNaive Bayes\nC4.5\nNaive Mix\nFSS AIC\nMajority\nFigure 6.4. Learning Rate for Adjectives\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n200\n400\n600\n800\n1000\nNaive Bayes\nC4.5\nNaive Mix\nFSS AIC\nMajority\nFigure 6.5. Learning Rate for Nouns\n111\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n200\n400\n600\n800\n1000\n1200\nNaive Bayes\nC4.5\nNaive Mix\nFSS AIC\nMajority\nFigure 6.6. Learning Rate for Verbs\nNouns C4.5, FSS AIC, and the Naive Mix are nearly as accurate as the ma-\njority classiﬁer after only learning from 10 training examples. However, unlike the\nadjectives, accuracy increases with additional training data and signiﬁcantly exceeds\nthe majority classiﬁer. Like the adjectives, Naive Bayes begins at very low accuracy\nbut reaches the same level as C4.5, FSS AIC, and the Naive Mix when approximately\n300 training examples are available.\nVerbs As is the case with adjectives and nouns, Naive Bayes begins at a very low\nlevel of accuracy while C4.5, FSS AIC, and the Naive Mix nearly match the accuracy\nof the majority classiﬁer after only 10 training examples. All methods exceed the\nmajority classiﬁer and perform at nearly exactly the same level of accuracy after\nlearning from approximately 600 examples.\nThe main distinction among these approaches is that Naive Bayes has a slower\nlearning rate; C4.5, FSS AIC, and the Naive Mix achieve at least the accuracy of\nthe majority classiﬁer after just 10 or 50 examples. However, after 300–600 examples\n112\nall of the methods perform at roughly the same level of accuracy and no method\nshows signiﬁcant improvement in accuracy when given more training examples. This\nsuggests that high levels of accuracy in word sense disambiguation are attainable with\nrelatively small quantities of training data.\nThe fast learning rates of C4.5, FSS AIC, and the Naive Mix are largely due\nto skewed sense distributions, especially for adjectives and verbs. A small number\nof examples is suﬃcient to correctly determine the majority sense. With only 10 or\n50 examples to learn from, a decision tree or probabilistic model consists of a few\nfeatures and relies upon knowledge of the majority sense to perform disambiguation.\nGiven the large majority senses that exist, most models are able to attain high levels\nof accuracy with very small numbers of examples. However, Naive Bayes estimates\nmodel parameters that involve all of the contextual features even when there is only a\nvery small amount of training data. In these cases it becomes an inaccurate classiﬁer\nsince it is easily mislead by spurious relationships in the data that do not hold true\nin larger samples.\n6.4. Experiment 4: Bias Variance Decomposition\nThe success of Naive Bayes may seem a bit mysterious. It is a simple approach\nthat does not perform feature selection nor does it engage in a systematic search\nfor a model. It simply assumes a parametric form that is usually not an accurate\nrepresentation of the interactions among contextual features. Despite this, it performs\nas accurately as any other method except when the amount of training data is small.\nThe decomposition of classiﬁcation error, i.e., (1 −accuracy), into bias and\nvariance components oﬀers an explanation for this behavior. This experiment shows\nthat diﬀerent representations of the same training data attain similar levels of accu-\nracy due to the diﬀering degrees with which bias and variance contribute to overall\nclassiﬁcation error.5\n5There are two diﬀerent senses of bias used in this dissertation.\nOne indicates a preference\nexhibited by a learning algorithm. The other refers to a component of classiﬁcation error. Hopefully,\n113\nThe estimated bias of an algorithm reﬂects how often the average classiﬁcations,\nacross multiple training samples, of a learned model fail to correspond to the actual\nclassiﬁcations in the held–out test data. Variance estimates the degree to which the\nclassiﬁcations predicted by the learned model vary across multiple training samples.\nAn algorithm that always makes the same classiﬁcation regardless of the training data\nwill have variance of 0.\nDecision tree learners are inherently unstable in that they produce very dif-\nferent models across multiple samples of training data, even if there are only minor\ndiﬀerences in the samples [7]. These models result in diﬀerent levels of accuracy when\napplied to a held–out test set. Such algorithms are said to have low bias and high\nvariance.\nNaive Bayes is more robust in that it is relatively unaﬀected by minor changes\nin the training data. Naive Bayes is not particularly representative of the training\ndata since the parametric form is assumed rather than learned. Naive Bayes is an\nexample of a high bias and low variance algorithm.\nThis experiment estimates the degree to which bias and variance contribute\nto the classiﬁcation error made by Naive Bayes and the decision tree learner MC46.\nNaive Bayes represents a high bias and low variance approach while MC4 represents\na low bias and high variance algorithm. The estimates of bias and variance reported\nhere are made following the sampling procedure described in [47]:\n1. Randomly divide the data into two sets, D and E. D serves as a super–set of\nthe training sample while E is the held–out test set.\n2. Generate T samples of size m from D. Let the size of D = 2m so that there\nare\n\u0012\n2m\nm\n\u0013\npossible training sets. For even a small value of m this will ensure\nthat there are relatively few duplicate training sets sampled from D.\nthe context will be suﬃcient to allow for immediate disambiguation.\n6The MLC++[46] version of C4.5.\n114\n3. Run a learning algorithm on the T training samples. Classify each observation\nin the test–set E using each of the T learned models. Store these results in an\narray called R.\n4. Array R and the correct classiﬁcations of E serve as the input to the procedure\ndescribed in [47] that estimates bias and variance.\nAs discussed in [47], the reliability of the bias and variance estimates depends\nupon having as large a test set as possible.\nTherefore, a training sample size of\nm = 400 is employed since this is generally the lowest number of examples where\nthe decision tree learner and Naive Bayes perform at comparable levels of accuracy.\nThe size of the D is set to 800 and T = 1000 diﬀerent training samples are randomly\nselected.\nTable 6.5 shows the bias and variance estimates for MC4 and Naive Bayes using\na training sample size of 400. The size of the test set is also listed. The results in\nthis table are divided into three groups. In the ﬁrst group, Naive Bayes has higher\nbias and lower variance than MC4. This corresponds to what would be expected; a\ndecision tree learner should have lower bias since it learns a more representative model\nof the training data than Naive Bayes. The second group has very similar bias and\nvariance for Naive Bayes and MC4. Only drug falls into this group; for this word both\nNaive Bayes and a learned decision tree have approximately the same representational\npower. In the third group of results MC4 has higher bias and lower variance than\nNaive Bayes. This is the reverse of what is expected and initially appears somewhat\ncounter–intuitive.\nHowever, the words in this third group have appeared together in a previous ex-\nperiment; they are the same words where no supervised learning method signiﬁcantly\nimproves upon the accuracy of the majority classiﬁer: help, last, include, and public.\nFor these words disambiguation accuracy is based almost entirely on knowledge of\nthe majority sense; the contextual features provide little information helpful in dis-\nambiguation. However, Naive Bayes assumes a parametric form that includes all of\n115\nTable 6.5. Bias Variance Estimates, m = 400\nword\ntest\nNaive Bayes\nMC4\nsize\nbias\nvar\nerror\nbias\nvar\nerror\nagree\n550\n.063\n.014\n.077\n.048\n.021\n.069\nbill\n540\n.133\n.027\n.160\n.110\n.039\n.149\nchief\n240\n.060\n.004\n.064\n.037\n.020\n.057\nclose\n730\n.137\n.028\n.165\n.110\n.054\n.164\ncommon\n310\n.140\n.027\n.167\n.116\n.027\n.143\nconcern\n790\n.112\n.027\n.139\n.101\n.035\n.136\ninterest\n1560\n.212\n.051\n.263\n.189\n.089\n.278\ndrug\n420\n.192\n.027\n.219\n.192\n.017\n.208\nhelp\n590\n.182\n.037\n.219\n.207\n.028\n.235\nlast\n2380\n.044\n.013\n.057\n.051\n.006\n.057\ninclude\n760\n.055\n.010\n.065\n.086\n.007\n.093\npublic\n70\n.359\n.071\n.430\n.381\n.024\n.405\nthe contextual features. This accounts for the higher amounts of variance since these\nirrelevant features are included in the model and aﬀect disambiguation. However, the\ndecision tree learner disregards features that are not relevant and essentially becomes\na majority classiﬁer where variance is very low and bias is the main source of error.\nThe data in Table 6.5 is presented again as correlation plots in Figures 6.7, 6.8,\nand 6.9. The classiﬁcation error, bias, and variance for each word are represented by\na point in each plot. The x coordinate represents the estimate associated with Naive\nBayes and the y coordinate is associated with MC4. Thus, points on or near x = y\nare associated with measures that have nearly identical estimates for Naive Bayes and\nMC4 for a particular word.\nFigure 6.7 conﬁrms that the two methods result in approximately the same level\nof classiﬁcation error. Figure 6.8 shows that the bias error of Naive Bayes is at least\nslightly higher for 8 of 12 words. And Figure 6.9 conﬁrms that the variance error\ntends to be greater for MC4.\n116\n0\n0.1\n0.2\n0.3\n0.4\n0\n0.1\n0.2\n0.3\n0.4\nMC4\nNaive Bayes\nnoun ✸\n✸\n✸\n✸\n✸\nadjective +\n+\n+\n+\n+\nverb ✷\n✷\n✷\n✷\n✷\nFigure 6.7. Classiﬁcation Error Correlation, m=400\n0\n0.1\n0.2\n0.3\n0.4\n0\n0.1\n0.2\n0.3\n0.4\nMC4\nNaive Bayes\nnoun ✸\n✸\n✸\n✸✸\nadjective +\n+\n+\n+\n+\nverb ✷\n✷\n✷\n✷\n✷\nFigure 6.8. Bias Correlation, m=400\n117\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nMC4\nNaive Bayes\nnoun ✸\n✸✸\n✸\n✸\nadjective +\n+\n+\n+\n+\nverb ✷\n✷\n✷\n✷\n✷\nFigure 6.9. Variance Correlation, m=400\n118\nCHAPTER 7\nUNSUPERVISED LEARNING EXPERIMENTAL RESULTS\nThis chapter contains an experimental evaluation of the unsupervised learning\nmethodologies described in Chapter 4. These approaches diﬀer from the supervised\nlearning methods in that no sense–tagged text is required; only raw untagged text is\nused to perform disambiguation.\nA probabilistic model of disambiguation is learned from raw text by treating the\nsense of an ambiguous word as missing data. As discussed in Chapter 4, the learning\nis restricted to parameter estimation since the parametric form must be speciﬁed\nrather than learned. These experiments assume that the parametric form is Naive\nBayes and use two diﬀerent methods to estimate parameter values; the EM algorithm\nand Gibbs Sampling. Two agglomerative clustering algorithms are also considered,\nWard’s minimum–variance method and McQuitty’s similarity analysis. As discussed\nin Chapter 4, these methods perform disambiguation based upon measures of distance\nthat are derived from a dissimilarity matrix representation of the raw untagged text.1\nThere is one experiment discussed in this chapter.\nThirteen words are dis-\nambiguated using four unsupervised methodologies where the context in which the\nambiguous word occurs is represented by three diﬀerent feature sets. This results in\ndisambiguation by 156 possible combinations of word, method, and feature set. Each\npossible combination is repeated 25 times in order to measure the deviation intro-\nduced by randomly selecting initial parameter estimates for the EM algorithm and\n1The freely available software package Bugs [41] was used for all Gibbs Sampling experiments.\nVarious freely available implementations of the EM algorithm were employed; AutoClass [18],\nGAMES [89], and CoCo [2].\nThe commercial software package SAS [82], was used to perform\nthe agglomerative clustering methods.\n119\nGibbs Sampling, and randomly selecting among equally distant clusters when using\nWard’s and McQuitty’s algorithms. The results of this experiment are evaluated in\nterms of disambiguation accuracy. However, as will be outlined in Section 7.1, the\nevaluation methodology for unsupervised learning is somewhat diﬀerent than in the\nsupervised case.\nThere are three pairwise analyses of the four unsupervised algorithms presented.\nFirst, the accuracy of the probabilistic models where the parameter estimates are\nlearned with the EM algorithm and Gibbs Sampling are compared.\nSecond, the\naccuracy of agglomerative clustering performed by Ward’s and McQuitty’s methods\nare compared. The ﬁnal analysis compares the two most accurate methods from the\nﬁrst two pairwise comparisons, McQuitty’s similarity analysis and Gibbs Sampling.\nEach analysis contains two discussions. First, a methodological comparison is\nmade that highlights any signiﬁcant diﬀerences in accuracy between two unsuper-\nvised learning algorithms when both use the same feature set. Second, a feature set\ncomparison is made that focuses on the variations in accuracy for an unsupervised\nlearner as diﬀerent feature sets are used.\n7.1. Assessing Accuracy in Unsupervised Learning\nIn the supervised learning experiments, accuracy is the rate of agreement be-\ntween the sense tags automatically assigned by a learned model and the sense tags\nassigned by a human judge. This deﬁnition of accuracy is also employed in the un-\nsupervised experiment. However, the means of arriving at that evaluation measure\nare diﬀerent and in fact point to some important diﬀerences between supervised and\nunsupervised learning.\nIn supervised learning accuracy is fairly easy to measure. The supervised algo-\nrithm learns from examples where a human judge has assigned sense tags to ambigu-\nous words that refer to speciﬁc entries in the sense inventory of a word. For example,\ngiven multiple instances of line and the sense inventory (telephone, series, cable),\na human judge tags some instances with the telephone sense, others with the cable\n120\nsense, and still others with the series sense. From these examples, the supervised\nalgorithm learns how to assign these same meaningful tags to other instances of the\nambiguous word.\nIn the supervised framework, the act of sense–tagging richly augments the\nknowledge contained in a text by creating a link from the manually disambiguated\ninstances of a word to a sense inventory provided by a dictionary or other lexical\nresource. These links are critical for evaluation relative to a human judge since they\nconnect the text to the same sense inventory that the human tagger used. However,\nin unsupervised learning no such links exist. The text is disconnected from the sense\ninventory. No human tagger is involved and the only information available to the\nunsupervised learner is the raw text which has no links to a sense inventory or any\nother external knowledge source.\nAn unsupervised algorithm is limited to creating sense groups. A sense group\nis simply a number of instances of an ambiguous word that are considered to belong\nto the same sense. However, there is no link from the members of the sense group\nto a sense inventory. The sense group is labeled by the unsupervised learner but this\nlabel has no relation to the sense inventory nor does it describe the contents of the\ngroup; it essentially meaningless.\nIn order to evaluate the accuracy of the unsupervised algorithm relative to a\nhuman judge, a mapping between the uninformative labels attached to sense groups\nand the sense inventory for a word must be established. In supervised learning these\nmappings are automatic since the human tagger provides the link between the text\nand the sense inventory. In unsupervised learning this mapping must be made as a\nsecond step after the sense groups are learned.2\nConsider the following example. Suppose there are 10 instances of line to be\ndisambiguated.\nA human tagger is told that there are three entries in the sense\n2The separation of disambiguation into a two step process is discussed in [85]. There the act of\ncreating sense groups is termed sense discrimination and the process of attaching meaningful labels\nto these groups is called sense labeling.\n121\ncable\nT\nT\nT\nT\n S\nS\ntelephone\nseries\n C\n C\n C\n C\nFigure 7.1. Human Labeled Senses for line\ninventory, (telephone, series, cable). The human assigns these sense tags as shown in\nFigure 7.1. An unsupervised learner is given these same 10 instances and told that\nthere are three possible senses. It divides the usages of the ambiguous word into the\n3 sense groups shown in Figure 7.2.\nAt this point there is not an immediately apparent means to assess the agree-\nment between the sense group assignments made by the unsupervised learner and the\nsense tags assigned by the human judge. In order to determine accuracy, the sense\ngroups must be linked to the entries in the sense inventory. For this example there are\n6 possible mappings between (1,2,3) and (telephone, series, cable).3 Each possible\nmapping is examined to determine which results in the closest agreement with the\nhuman judge.\nOne possible mapping is shown in Figure 7.3. Sense group 1 is assigned sense\ntag series, sense group 2 is assigned cable, and sense group 3 is assigned telephone.\nThe shaded instances show where the sense group label matches the sense tag assigned\nby the human judge. In this ﬁgure, 3 of 10 instances agree and result in unsupervised\naccuracy of 30%.\nA second possible mapping is shown in Figure 7.4. Sense group 1 is assigned\nseries, 2 is assigned telephone, and 3 is assigned cable. Here 7 of 10 instances agree\n3Given n sense groups to be assigned n meaningful sense tags, there are n! possible mappings to\nbe considered.\n122\n3\n1\n2\nFigure 7.2. Unlabeled Sense Groups for line\n1/series\n2/cable\n3/telephone\naccuracy = 3/10\nT\n T\nT\nC\n C\n C\n C\nS\nS\n T\nFigure 7.3. Thirty Percent Accuracy Mapping of line\n1/series\n2/telephone\n3/cable\naccuracy = 7/10\nT\nT\nT\nT\nC\nC\n C\nS\nS\nC\nFigure 7.4. Seventy Percent Accuracy Mapping of line\n123\nwith the human judge so accuracy is 70%. The other four possible mappings are\nevaluated and the maximum accuracy is reported as the unsupervised accuracy.\nGiven this evaluation methodology, a convenient means of determining a lower\nbound for unsupervised disambiguation accuracy emerges. An unsupervised learner\ncan achieve accuracy equal to the percentage of the majority sense by not performing\nany disambiguation at all. In other words, a lower bound classiﬁer for unsupervised\nlearning simply assigns every instance of an ambiguous word to the same sense group.\nThis is somewhat analogous to supervised learning where the lower bound is estab-\nlished by assigning every instance of an ambiguous word to the most frequent sense\nin the training data, i.e., the majority sense. However, in supervised learning it is\nrelatively easy to exceed this lower bound. The same does not prove to be true for\nunsupervised learning.\n7.2. Analysis 1: Probabilistic Models\nThe ﬁrst analysis of this experiment compares the accuracy of a probabilistic\nmodel where the parametric form is assumed to be Naive Bayes and the parameter\nestimates are learned by the EM algorithm and Gibbs Sampling. Table 7.1 shows the\naverage unsupervised disambiguation accuracy and standard deviation for each com-\nbination of word, feature set, and parameter estimation method over 25 trials, where\neach trial begins with a diﬀerent random initialization of the parameter estimates.\nIn this table, signiﬁcant diﬀerences in the disambiguation accuracy of a word\nusing the EM algorithm and Gibbs Sampling for a given feature set are shown in bold\nface. These diﬀerences are discussed in Section 7.2.1, the methodological comparison.\nThe highest overall accuracy for a word using either the EM algorithm or Gibbs\nSampling and any of the three feature sets is shown in parenthesis. Any other values\nthat are not signiﬁcantly less than the maximum accuracy are underlined. These\nresults are discussed in Section 7.2.2, the feature set comparison.4\n4As in the supervised learning experiments, judgments as to the signiﬁcance of diﬀerences in\naccuracy are made by a two tailed t–test where p = .01.\n124\nTable 7.1. Unsupervised Accuracy of EM and Gibbs\nFeature Set A\nFeature Set B\nFeature Set C\nMaj.\nGibbs\nEM\nGibbs\nEM\nGibbs\nEM\nchief\n.861\n.719 .01\n(.729 .06)\n.648 .00\n.646 .01\n.728 .04\n.697 .06\ncommon\n.842\n.522 .00\n.521 .00\n.507 .07\n.464 .06\n(.670 .01)\n.543 .09\nlast\n.940\n.900 .00\n.903 .00\n(.912 .00)\n.909 .00\n.908 .00\n.874 .07\npublic\n.683\n.514 .00\n.473 .03\n.478 .04\n.411 .03\n(.578 .00)\n.507 .03\nadjectives\n.832\n.663\n.657\n.636\n.608\n.721\n.655\nbill\n.681\n.590 .04\n.537 .05\n(.705 .10)\n.624 .08\n.592 .04\n.569 .04\nconcern\n.638\n(.842 .00)\n(.842 .00)\n.819 .01\n.840 .02\n.785 .01\n.758 .09\ndrug\n.567\n(.676 .00)\n.658 .03\n.543 .04\n.551 .05\n.674 .06\n.652 .04\ninterest\n.593\n.627 .08\n.616 .06\n(.652 .04)\n.615 .05\n.617 .05\n.649 .09\nline\n.373\n.446 .02\n.457 .01\n(.477 .03)\n.474 .03\n.457 .01\n.458 .01\nnouns\n.570\n.636\n.622\n.639\n.621\n.625\n.617\nagree\n.740\n.609 .07\n.631 .08\n(.714 .14)\n.683 .14\n.685 .14\n.685 .14\nclose\n.771\n.564 .09\n.560 .08\n(.714 .05)\n.672 .06\n.636 .05\n.648 .05\nhelp\n.780\n.658 .04\n.586 .05\n.524 .00\n.526 .00\n(.696 .05)\n.602 .03\ninclude\n.910\n.734 .08\n.725 .02\n(.833 .03)\n.783 .07\n.551 .06\n.535 .00\nverbs\n.800\n.641\n.626\n.696\n.666\n.632\n.618\noverall\n.734\n.646\n.634\n.657\n.631\n.659\n.629\n125\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nEM\nGibbs Sampling\nFeature Set A ✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\nFeature Set B +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\nFigure 7.5. Probabilistic Model Correlation of Accuracy for all words\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nEM\nGibbs Sampling\nFeature Set A ✸\n✸\n✸\n✸\n✸\n✸\nFeature Set B +\n+\n+\n+\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\n✷\nFigure 7.6. Probabilistic Model Correlation of Accuracy for Nouns\n126\n7.2.1. Methodological Comparison\nThis comparison studies the eﬀect on disambiguation accuracy of learning pa-\nrameter estimates for a probabilistic model using Gibbs Sampling and the EM algo-\nrithm. The motivation for the comparison is that while the EM algorithm is easy to\nimplement and generally quick to converge, it is also prone to converging at local max-\nima rather than a global maximum. However, while Gibbs Sampling is guaranteed\nto converge at the global maximum, it does so at greater computational expense.\nThis experiment shows that there are only a few cases where a probabilistic\nmodel with parameter estimates learned by Gibbs Sampling results in signiﬁcantly\ndiﬀerent disambiguation than the model arrived at with the EM algorithm. Of the 39\npossible pairwise comparisons (13 words × 3 feature sets) between the EM algorithm\nand Gibbs Sampling, only 7 result in signiﬁcant diﬀerences. Of those, all favor Gibbs\nSampling. Those cases are shown in bold face in Table 7.1.\nThe lack of signiﬁcant diﬀerences between the EM algorithm and Gibbs Sam-\npling is somewhat surprising given that the EM algorithm can converge to local max-\nima when the distribution of the likelihood function is not well approximated by the\nnormal distribution. However, in this experiment the EM algorithm does not appear\nto have great diﬃculty with local maxima, often converging within 20 iterations to\nessentially the same estimates obtained by Gibbs Sampling.\nThe use of Naive Bayes as the parametric form of the probabilistic model pro-\nvides at least a partial explanation for the comparable results obtained with the EM\nalgorithm and Gibbs Sampling.\nChapter 5 presents the distribution of the event\ncounts in the experimental data when the parametric form of the model is Naive\nBayes. These distributions prove to be relatively smooth for the three unsupervised\nfeature sets and are not dominated by events that are never observed in the data; in\nfact, a majority of the possible events for each word are observed. If the parametric\nform of the model were more complex than Naive Bayes, it would certainly be the\ncase that the distribution of event counts would be more skewed and that the EM\nalgorithm would be more susceptible to becoming trapped at a local maxima. How-\n127\never, the speciﬁcation of Naive Bayes as the parametric form of the model seems to\navoid this diﬃculty.\nWhile the number of signiﬁcant diﬀerences between the EM algorithm and Gibbs\nSampling is small, the correlation plot comparing the accuracy of the two methods\nin Figure 7.5 reveals a consistent increase in the accuracy of Gibbs Sampling relative\nto the EM algorithm. Each point on this plot shows the accuracy attained by the\nprobabilistic model where parameter estimates are learned by the EM algorithm and\nGibbs Sampling for a given word and feature set. Figure 7.6 again shows the correla-\ntion of accuracy, but only for the nouns. This shows the comparable performance of\nboth methods and suggests that Gibbs Sampling has a particular advantage over the\nEM algorithm for the adjectives and verbs. This is not surprising since the adjectives\nand verbs have the most skewed distributions of senses and are more likely to cause\ndiﬃculty for the EM algorithm than are the nouns.\nThe standard deviations associated with the two approaches also prove to be\nsimilar, generally falling between .03 and .10. A standard deviation of .00 indicates\nthat the exact same sense group is created by each of the 25 trials of the algorithm.\nThe larger the deviation the more variation there is in the sense groups created from\ntrial to trial. The standard deviation observed is somewhat larger than expected,\nparticularly since neither method has substantial diﬃculties with local maxima. This\nsuggests that there is some degree of noise in the data that is obscuring sense distinc-\ntions and causing the variance in the results from trial to trial.\nThere are several possible sources of noise in this data. The very crude part–\nof–speech distinctions made in feature sets A and C may not provide suﬃcient infor-\nmation to distinguish among senses. In addition, this tagging was performed without\nthe beneﬁt of training data and is likely to contain inaccuracies.\nThe frequency based co–occurrences in feature sets B and C include a value\nthat signiﬁes that the word at a speciﬁed position relative to the ambiguous word is\nnot among the 19 most frequent words that occur at this position with all instances\nof the ambiguous word; this lumps together a great many words into a single feature\n128\nvalue and may further blur the ability of the learning algorithm to accurately make\nsense distinctions.\nThe diﬃculty in unsupervised learning is that features must be selected from\nraw untagged text.\nThe availability of accurately part–of–speech tagged text can\nnot be assumed, nor can the ability to select feature values that are indicative of\nminority senses.\nRaw untagged text generally does not lend itself to ﬁne grained\nfeature values that are able to identify particular senses; generally speaking features\nmust be selected simply based on frequency counts and lead to a certain amount of\nnoise in the data.\nA noteworthy result is that only the nouns are disambiguated with accuracy\ngreater than the discussed lower bound for unsupervised learning. The accuracy of\nthe probabilistic models is less than the lower bound when the percentage of the\nmajority sense exceeds 68%. However, even in cases where the accuracy of the EM\nalgorithm and Gibbs Sampling is less than the lower bound, these methods are often\nstill providing high accuracy disambiguation. For example, Gibbs Sampling is able\nto achieve 91% accuracy for last and 83% accuracy for include.\nThe relative success of noun disambiguation is at least partially explained by the\nfact that, as a class, the nouns have the most uniform distribution of senses. However,\nthe distribution of senses is not the only factor aﬀecting disambiguation accuracy; the\nperformance of the EM algorithm and Gibbs Sampling is quite diﬀerent for bill and\npublic despite having roughly the same sense distributions.\nIt is diﬃcult to quantify the eﬀect of the distribution of senses on a learning\nalgorithm, particularly when using naturally occurring data. In previous unsupervised\nexperiments with interest, using a feature set similar to A, an increase of 36 percentage\npoints over the accuracy of the lower bound was achieved when the 3 senses were\nevenly distributed in the training data [71]. Here, the most accurate performance\nusing larger samples and a natural distribution of senses is only an increase of 20\npercentage points over the accuracy of the lower bound.\n129\nThe actual distribution of senses does not closely correspond to the distribution\nof senses discovered by either method. As examples, the distribution of senses dis-\ncovered by the EM algorithm and Gibbs Sampling relative to the known distribution\nof senses is illustrated in Figures 7.7, 7.8 and 7.9. These show the confusion matrices\nassociated with the disambiguation of concern, interest, and help, using feature sets\nA, B, and C, respectively. A confusion matrix shows the number of cases where the\nsense discovered by the algorithm agrees with the manually assigned sense along the\nmain diagonal; disagreements are shown in the rest of the matrix. The row totals\nshow the actual distribution of senses while the column totals show the discovered\ndistributions.\nIn general, these matrices show that the EM algorithm and Gibbs Sampling\nresult in distributions of senses that are more balanced than those of the actual\ndistribution. This is at least partially due to the assumption made prior to learning\nby the unsupervised methods that each possible sense is equally likely. Adjusting this\nprior assumption could result in the discovery of less balanced distributions of senses\nand is an interesting direction for future research.\nThe fact that the EM algorithm and Gibbs Sampling often arrive at similar\nresults suggests that a combination of the these methods might be appropriate for this\ndata. It is proposed in [56] that the Gibbs Sampler be initialized with the parameters\nthat the EM algorithm converges upon rather than with randomly selected values.\nIf the EM algorithm has found a local maxima then the Gibbs Sampler can escape\nit and ﬁnd the global maximum. However, if the EM algorithm has already found\nthe global maximum then the Gibbs Sampler will converge quickly and conﬁrm this\nresult.\n7.2.2. Feature Set Comparison\nWhile there is little variation between the EM algorithm and Gibbs Sampling\ngiven a particular word and feature set, there are diﬀerences in the accuracy attained\nfor each method as they are used with diﬀerent feature sets. In general, variation\n130\nDiscovered\nActual\nworry\nbusiness\nworry\n384\n63\n447\nbusiness\n132\n656\n788\n516\n719\n1235\nEM - 1040 correct\nDiscovered\nActual\nworry\nbusiness\nworry\n384\n63\n447\nbusiness\n132\n656\n788\n516\n719\n1235\nGibbs - 1040 correct\nFigure 7.7. concern - Feature Set A\n131\nDiscovered\nActual\nattention\nshare\nmoney\nattention\n127\n230\n4\n361\nshare\n134\n364\n2\n500\nmoney\n320\n124\n808\n1252\n581\n718\n814\n2113\nEM - 1299 correct\nDiscovered\nActual\nattention\nshare\nmoney\nattention\n152\n205\n4\n361\nshare\n134\n364\n2\n500\nmoney\n297\n94\n861\n1252\n583\n663\n867\n2113\nGibbs - 1377 correct\nFigure 7.8. interest - Feature Set B\n132\nDiscovered\nActual\nassist\nenhance\nassist\n119\n160\n279\nenhance\n344\n644\n988\n463\n804\n1267\nEM - 763 correct\nDiscovered\nActual\nassist\nenhance\nassist\n169\n110\n279\nenhance\n276\n712\n988\n445\n822\n1267\nGibbs - 881 correct\nFigure 7.9. help - Feature Set C\n133\nin the accuracy of a method when using diﬀerent feature sets suggests that certain\ntypes of features are more or less appropriate for particular words. Table 7.1 shows\nthe maximum accuracy for each word in parenthesis. Any accuracies that are not\nsigniﬁcantly less than this are underlined.\nThere are a number of cases where the same method attains very diﬀerent levels\nof accuracy when used with diﬀerent feature sets. For example, the accuracies of the\nEM algorithm and Gibbs Sampling for bill and include are much higher with feature\nset B than with A or C. Less extreme examples of the same behavior are shown\nby agree and close. However, the accuracies for drug and help are much lower with\nfeature set B than with A or C. A less extreme example is chief.\nThe separation of behavior between feature sets A and C and feature set B is\ndue to the nature of the features in these sets. A and C both include part–of–speech\nfeatures while feature set B does not.\nIt appears that the usefulness of part–of–\nspeech features for disambiguation varies considerably from word to word. The fact\nthat 3 of 4 verbs perform at higher levels of accuracy with feature set B suggests that\npart–of–speech features may not be helpful when disambiguating verbs.\nWhen using either the EM algorithm or Gibbs Sampling, line, interest, and last\nresult in very similar disambiguation accuracy regardless of the feature set. In fact,\nfor interest and line there are no signiﬁcant diﬀerences among any combination of\nmethod and feature set. This lack of variation shows that the diﬀerent feature sets\nare not able to make suﬃcient distinctions among all words. It may also point to\nlimitations in Naive Bayes. While it performs well in general, there may be certain\nwords and feature sets for which the assumptions it makes are not appropriate.\nThe performance of concern is slightly unusual; it disambiguates most accu-\nrately with feature sets A and B. However, the only feature in common between sets\nA and B is the morphological feature; it seems unlikely that this accounts for the\nhigh disambiguation accuracy achieved using both sets. A more likely explanation\nis that the part–of–speech features from set A somewhat duplicate the information\ncontained in the unrestricted co–occurrences of set B. Unrestricted co–occurrences\n134\ncan be largely dominated by non–content words that provide more syntactic rather\nthan semantic information. This same explanation may apply in cases such as line\nand interest where the results for feature sets A, B, and C are very similar. Feature\nsets A and C rely heavily upon part–of–speech features which largely convey syntactic\ninformation. If the co–occurrences in feature set B are also essentially representing\nsyntactic information, then similar performance across all feature sets is possible.\nThe highest average accuracy achieved for adjectives occurs when Gibbs Sam-\npling is used in combination with feature set C. The adjectives have the most skewed\nsense distributions and set C has the largest dimensionality of the feature sets. Given\nthis combination of circumstances, it appears that the EM algorithm gets trapped at\nlocal maxima for common and public; Gibbs Sampling ﬁnds a global maximum and\nresults in signiﬁcantly better accuracy in these cases.\nFeature set B appears to be well suited for bill but fares poorly with drug.\nOtherwise there is not a clear pattern as to which feature set is most accurate for\nthe nouns. The relatively similar behavior across the feature sets suggests that cer-\ntain features are either essentially duplicating one another or that there are features\nincluded in these sets that are simply not useful for the disambiguation of nouns.\nWhile it is clear that the part–of–speech and co–occurrence features are contributing\nto disambiguation accuracy, it is less certain that the morphological and collocation\nfeatures make signiﬁcant contributions.\n7.3. Analysis 2: Agglomerative Clustering\nThe second analysis of this experiment compares the accuracy of two agglomer-\native clustering algorithms, Ward’s minimum variance method and McQuitty’s sim-\nilarity analysis.\nTable 7.2 shows the average accuracy and standard deviation of\ndisambiguation over 25 random trials for each combination of word, method, and\nfeature set. The repeated trials are necessary to determine the impact of randomly\nbreaking ties during clustering. As in the ﬁrst analysis, both methodological and\nfeature set comparisons are presented.\n135\nTable 7.2. Unsupervised Accuracy of Agglomerative Clustering\nFeature Set A\nFeature Set B\nFeature Set C\nMaj.\nMcQuitty\nWard\nMcQuitty\nWard\nMcQuitty\nWard\nchief\n.861\n.844 .05\n.721 .01\n.831 .06\n.611 .01\n(.856 .00)\n.673 .03\ncommon\n.842\n.648 .12\n.513 .08\n.797 .04\n.444 .04\n(.799 .06)\n.561 .05\nlast\n.940\n(.791 .12)\n.598 .09\n.541 .11\n.659 .03\n.636 .07\n.601 .08\npublic\n.683\n.560 .08\n.450 .05\n.558 .07\n.461 .03\n(.628 .05)\n.488 .04\nadjectives\n.832\n.711\n.571\n.682\n.544\n.730\n.581\nbill\n.681\n.669 .08\n.647 .11\n(.753 .05)\n.600 .04\n.561 .10\n.515 .04\nconcern\n.638\n.629 .07\n.741 .04\n.679 .04\n.697 .02\n.614 .08\n(.758 .04)\ndrug\n.567\n.530 .03\n.557 .06\n.521 .01\n.528 .00\n.573 .06\n(.632 .06)\ninterest\n.593\n.601 .04\n.619 .04\n(.653 .06)\n.552 .06\n.651 .02\n.615 .04\nline\n.373\n.420 .03\n(.441 .03)\n.403 .02\n.428 .03\n.410 .02\n.427 .02\nnouns\n.570\n.570\n.601\n.602\n.561\n.562\n.589\nagree\n.740\n.610 .08\n.547 .03\n.678 .08\n.613 .04\n(.685 .07)\n.601 .00\nclose\n.771\n.616 .09\n.531 .02\n.667 .07\n.664 .00\n(.720 .11)\n.645 .04\nhelp\n.780\n(.713 .05)\n.591 .05\n.636 .11\n.519 .01\n.700 .06\n.570 .03\ninclude\n.910\n(.880 .06)\n.707 .08\n.767 .09\n.770 .06\n.768 .17\n.558 .04\nverbs\n.800\n.705\n.594\n.687\n.642\n.718\n.593\noverall\n.734\n.655\n.589\n.653\n.580\n.662\n.588\n136\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nWard\nMcQuitty\nFeature Set A ✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸✸\n✸\n✸\nFeature Set B +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\nFigure 7.10. Agglomerative Clustering Correlation of Accuracy for all words\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nWard\nMcQuitty\nFeature Set A ✸\n✸\n✸\n✸\n✸\n✸\nFeature Set B +\n+\n+\n+\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\n✷\nFigure 7.11. Agglomerative Clustering Correlation of Accuracy for Nouns\n137\n7.3.1. Methodological Comparison\nWard’s and McQuitty’s methods are both agglomerative clustering algorithms\nand diﬀer only in the distance measure each uses to determine if an instance of an\nambiguous word belongs in a particular sense group. Since distance is not implicit\nin the features used in this experiment, the data representing the instances of an\nambiguous word must be converted into a form where distance can be measured. In\nthis dissertation that representation is a dissimilarity matrix.\nWard’s method is based on a classical measure, Euclidean distance, while Mc-\nQuitty’s method employs a simple count of the number of dissimilar features to es-\ntablish group membership. The dramatic diﬀerence in the nature of these distance\nmeasures motivates their inclusion in this study.\nUnlike the probabilistic models, there are signiﬁcant diﬀerences in the accuracy\nof the two agglomerative clustering algorithms given a particular feature set.\nOf\nthe 39 possible pairwise comparisons, 17 result in signiﬁcant diﬀerences. There are 2\ncases where Ward’s method is signiﬁcantly more accurate and 15 favoring McQuitty’s\nsimilarity analysis. The signiﬁcant diﬀerences are shown in bold face in Table 7.2.\nThe plot of the correlation of accuracy between Ward’s and McQuitty’s meth-\nods in Figure 7.10 shows that McQuitty’s method generally is the more accurate.\nHowever, in Figure 7.11 the correlation plot is restricted to the nouns and Ward’s\nmethod is shown to be slightly more accurate. This is also illustrated in Table 7.2,\nwhere there are only three signiﬁcant diﬀerences among the nouns; in two of those\ncases Ward’s method is the most accurate. Thus, it is only for verbs and adjectives\nthat McQuitty’s method shows a decisive advantage.\nAs is the case with the probabilistic models, only the nouns are consistently\ndisambiguated with accuracy greater than the majority sense. However, McQuitty’s\nmethod achieves accuracy comparable to the majority sense for a few of the adjectives\nand verbs when the standard deviation is taken into account. This occurs for chief\nwith all feature sets, common for sets B and C, close for set C, and include for set A.\n138\nThere appears to be signiﬁcant relationships among the actual sense distribu-\ntions of the ambiguous words, the inherent biases of the agglomerative clustering\nalgorithms, and the accuracy attained by each method. Ward’s method has a well–\nknown bias towards ﬁnding balanced distributions of sense groups [91]. However,\nMcQuitty’s method has no such bias since there are no underlying parametric models\nor distributional assumptions that inﬂuence the algorithm.5 The tendencies of both\nagglomerative methods are illustrated in Figures 7.12, 7.13, and 7.14. These show the\nconfusion matrices for the same word and feature set combinations that are discussed\nin the ﬁrst analysis. These illustrate the bias of Ward’s method towards the discovery\nof balanced sense distributions while also showing that McQuitty’s similarity analysis\ntends to ﬁnd more skewed distributions.\nThe bias of Ward’s method towards balanced distributions of senses results in\naccurate disambiguation of the nouns but also leads to rather poor performance with\nadjectives and verbs. As the actual distribution of senses grows more skewed, Ward’s\nmethod becomes less accurate. By contrast, McQuitty’s method performs fairly well\nwith words that have skewed distributions of senses. It has no bias regarding the\ndistribution of senses it discovers and is able to learn very unbalanced distributions.\nThe standard deviations in Table 7.2 measure the impact of randomly breaking\nties during the clustering process. A standard deviation of .00 indicates that no ties\noccurred during clustering; in this case the agglomerative algorithm is deterministic\nand the sense groups discovered from trial to trial are identical. As clustering becomes\nmore inﬂuenced by random breaking of ties, the standard deviation will increase since\nthe sense groups created will vary from trial to trial.\nOverall, the standard deviation for McQuitty’s similarity analysis is greater than\nthat of Ward’s method. This is not surprising given the simplicity of McQuitty’s\napproach; distances are based on a count of the dissimilar features between two\n5In this regard McQuitty’s method is unique among the four unsupervised approaches discussed\nin this dissertation.\nRecall that both the EM algorithm and Gibbs Sampling also tend to ﬁnd\nbalanced distributions of senses.\n139\nDiscovered\nActual\nworry\nbusiness\nworry\n166\n281\n447\nbusiness\n181\n607\n788\n347\n888\n1235\nMcQuitty - 773 correct\nDiscovered\nActual\nworry\nbusiness\nworry\n288\n159\n447\nbusiness\n155\n633\n788\n443\n792\n1235\nWard - 921 correct\nFigure 7.12. concern - Feature Set A\n140\nDiscovered\nActual\nattention\nshare\nmoney\nattention\n53\n6\n302\n361\nshare\n58\n187\n255\n500\nmoney\n108\n4\n1140\n1252\n219\n197\n1697\n2113\nMcQuitty - 1380 correct\nDiscovered\nActual\nattention\nshare\nmoney\nattention\n280\n3\n78\n361\nshare\n240\n197\n63\n500\nmoney\n559\n0\n693\n1252\n1079\n200\n834\n2113\nWard - 1170 correct\nFigure 7.13. interest - Feature Set B\n141\nDiscovered\nActual\nassist\nenhance\nassist\n45\n234\n279\nenhance\n146\n842\n988\n191\n1076\n1267\nMcQuitty - 887 correct\nDiscovered\nActual\nassist\nenhance\nassist\n88\n191\n279\nenhance\n354\n634\n988\n442\n825\n1267\nWard - 722 correct\nFigure 7.14. help - Feature Set C\n142\ninstances of an ambiguous word. Ties are common given these sets since they contain\nrelatively small numbers of features; A has 8, B has 5, and C has 7.\nHowever, Ward’s method computes Euclidean distances in n–space. While this\nmore detailed measure results in fewer ties, there are still enough to cause relatively\nlarge amounts of deviation for some words. This suggests that the conversion of raw\ntext into a dissimilarity matrix representation results in a reduction in the discrimi-\nnating power of the feature set to the point where ties are still common.\nIn general, these standard deviations suggest that the feature sets need to be\nexpanded to provide more distinctions between instances of an ambiguous word when\nusing the agglomerative clustering algorithms.\n7.3.2. Feature Set Comparison\nWhen using probabilistic models, feature sets A and C result in similar perfor-\nmance and often have an inverse relationship to the accuracy attained with feature\nset B. For example, if set B results in high accuracy then A and C may not. When\nfeature set A and C result in high accuracy then set B often does not. These patterns\nhold for 7 of 13 words when disambiguating with probabilistic models. This suggests\nthat the features common to sets A and C, the part–of–speech of surrounding words,\nare a main contributor to disambiguation accuracy when using probabilistic models.\nHowever, when an agglomerative approach is employed these patterns are much\nless pronounced. The only cases where results from set A and set C are more accurate\nthan those from set B are when McQuitty’s method is used to disambiguate help and\nwhen Ward’s method is used for concern. The only case where feature set B is most\naccurate is when McQuitty’s method is used to disambiguate bill.\nThis change in behavior suggests that the data representation employed by\nthe agglomerative methods blurs some distinctions that are present in the frequency\ncount data used by the probabilistic models. While both the probabilistic models and\nagglomerative methods use the same feature sets, the agglomerative methods convert\nthe data into a dissimilarity matrix representation that shows how many features\n143\ndiﬀer between instances of an ambiguous word; however, it makes no distinctions as\nto which features diﬀer. Thus the part–of–speech distinctions that appear to have\nsigniﬁcant impact on disambiguation accuracy when using probabilistic models are\nnot distinguishable from other features in the dissimilarity matrix representation.\nOverall the combination of McQuitty’s similarity analysis with feature set C\nresults in consistently high accuracy for the adjectives and verbs. Feature set C has\nthe highest dimensionality of the three feature sets. Thus, the number of dissimilar\nfeatures between two instances of an ambiguous word will likely be fairly high most of\nthe time since there are a large number of possible values for the features. This results\nin the creation of a large sense group where all members have fairly high dissimilarity\ncounts. The distribution of discovered senses in this case will be skewed and likely\ncorrespond fairly well with the actual distributions.\nThe only cases where McQuitty’s method and feature set C fares poorly for the\nverbs and adjectives are for last and include. These are interesting exceptions in that\nthese two words have the largest majority senses, .94% and .91% respectively. They\nare both most accurately disambiguated with feature set A; no other combination of\nfeature set and method results in comparable performance. Agglomerative clustering\nbased on dissimilarity counts of the features in set A is particularly eﬀective with\nthese words. This suggests that the combination of a low dimensional feature set\nwith a word that has an extremely skewed distribution of senses may be appropriate.\nThere is not a clear pattern as to which feature sets lead to accurate disam-\nbiguation of nouns. For concern, Ward’s method in conjunction with feature sets A\nand C achieves the highest accuracy. Ward’s method is also most accurate for drug\nwhen used with feature set C. The success of Ward’s method for these two nouns is\nrelated to the general tendency of Ward’s method to ﬁnd balanced distributions of\nsenses. As is the case in the probabilistic models, interest and line do not show great\nvariation from one feature set to the next. This again suggests that the dissimilarity\nmatrix may be reducing the granularity of the information available to the clustering\nalgorithm by reducing the distinctions that the feature sets are able to represent.\n144\n7.4. Analysis 3: Gibbs Sampling and McQuitty’s Similarity Analysis\nThe ﬁrst analysis in Section 7.2 shows that Gibbs Sampling oﬀers some im-\nprovement over the EM algorithm, particularly for adjectives and verbs. The second\nanalysis in Section 7.3 shows that McQuitty’s similarity analysis is often more ac-\ncurate than Ward’s method; primarily when disambiguating adjectives and verbs.\nThis ﬁnal analysis compares McQuitty’s method and Gibbs Sampling. This section\nonly contains a methodological comparison since the feature set comparisons for Mc-\nQuitty’s method and Gibbs Sampling are included in the ﬁrst two analyses.\nTable 7.3 reformats the accuracies reported in the previous two analyses for\neasy comparison. As before, signiﬁcant diﬀerences between the two methods for a\nparticular word and feature set are shown in bold face. The maximum accuracy for\na word is in parenthesis and any accuracies that are not signiﬁcantly less than this\nmaximum are underlined.\nThere are a relatively large number of signiﬁcant diﬀerences between Gibbs\nSampling and McQuitty’s similarity analysis given a particular feature set. Of the\n39 pairwise comparisons, 19 show signiﬁcant diﬀerences.\nGibbs Sampling is more\naccurate in 10 of those cases and McQuitty’s method is more accurate in 9. These\nsigniﬁcant diﬀerences are shown in bold face in Table 7.3.\nThe correlation of accuracy between Gibbs Sampling and McQuitty’s method\nis shown in Figure 7.15. Since there is not a clear pattern associated with the per-\nformance of the methods, this data is broken down into separate correlation plots for\nadjectives, nouns, and verbs in Figures 7.16, 7.17, and 7.18.\nFigure 7.16 shows that McQuitty’s method is generally more accurate for ad-\njectives, the exception being last for all three feature sets. Figure 7.17 suggests that\nGibbs Sampling is more accurate for the nouns. Figure 7.18 shows that McQuitty’s\nmethod is generally more accurate for the verbs, although not so dramatically as is\nthe case with the adjectives.\n145\nTable 7.3. Unsupervised Accuracy of Gibbs and McQuitty’s\nFeature Set A\nFeature Set B\nFeature Set C\nMaj.\nGibbs\nMcQuitty\nGibbs\nMcQuitty\nGibbs\nMcQuitty\nchief\n.861\n.719 .01\n.844 .05\n.648 .00\n.831 .06\n.728 .04\n(.856 .00)\ncommon\n.842\n.522 .00\n.648 .12\n.507 .07\n.797 .04\n.670 .11\n(.799 .06)\nlast\n.940\n.900 .00\n.791 .12\n(.912 .00)\n.541 .11\n.908 .00\n.636 .07\npublic\n.683\n.514 .00\n.560 .08\n.478 .04\n.558 .07\n.578 .00\n(.628 .05)\nadjectives\n.832\n.663\n.711\n.636\n.682\n.721\n.730\nbill\n.681\n.590 .04\n.669 .08\n.705 .10\n(.753 .05)\n.592 .04\n.561 .10\nconcern\n.638\n(.842 .00)\n.629 .07\n.819 .01\n.679 .04\n.785 .01\n.614 .08\ndrug\n.567\n(.676 .00)\n.530 .03\n.543 .04\n.521 .01\n.674 .06\n.573 .06\ninterest\n.593\n.627 .08\n.601 .04\n.652 .04\n(.653 .06)\n.617 .05\n.651 .02\nline\n.373\n.446 .02\n.420 .03\n(.477 .03)\n.403 .02\n.457 .01\n.410 .02\nnouns\n.570\n.636\n.570\n.639\n.602\n.625\n.562\nagree\n.740\n.609 .07\n.610 .08\n(.714 .14)\n.678 .08\n.685 .14\n.685 .07\nclose\n.771\n.564 .09\n.616 .09\n.714 .05\n.667 .07\n.636 .05\n(.720 .11)\nhelp\n.780\n.658 .04\n(.713 .05)\n.524 .00\n.636 .11\n.696 .05\n.700 .06\ninclude\n.910\n.734 .08\n(.880 .06)\n.833 .03\n.767 .09\n.551 .06\n.768 .17\nverbs\n.800\n.641\n.705\n.696\n.687\n.632\n.718\naverage\n.734\n.646\n.655\n.657\n.653\n.659\n.662\n146\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMcQuitty\nGibbs Sampling\nFeature Set A ✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\n✸\nFeature Set B +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\n✷\nFigure 7.15. Gibbs and McQuitty’s Correlation of Accuracy for all words\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMcQuitty\nGibbs Sampling\nFeature Set A ✸\n✸\n✸\n✸\n✸\nFeature Set B +\n+\n+\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\nFigure 7.16. Gibbs and McQuitty’s Correlation of Accuracy for Adjectives\n147\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMcQuitty\nGibbs Sampling\nFeature Set A ✸\n✸\n✸\n✸\n✸\n✸\nFeature Set B +\n+\n+\n+\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\n✷\nFigure 7.17. Gibbs and McQuitty’s Correlation of Accuracy for Nouns\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMcQuitty\nGibbs Sampling\nFeature Set A ✸\n✸\n✸\n✸\n✸\nFeature Set B +\n++\n+\n+\nFeature Set C ✷\n✷\n✷\n✷\n✷\nFigure 7.18. Gibbs and McQuitty’s Correlation of Accuracy for Verbs\n148\nOf the 19 signiﬁcant diﬀerences, 10 occur among the adjectives, 7 occur among\nthe nouns, and 2 occur among the verbs. The distribution of senses plays a role in\nthese results, particularly for the adjectives. Of the 10 signiﬁcant diﬀerences among\nthe adjectives, 7 favor McQuitty’s method. Given the tendency of McQuitty’s method\nto discover skewed sense distributions this is not surprising. Of the 7 signiﬁcant diﬀer-\nences observed among the nouns, all favor Gibbs Sampling. Again, this is somewhat\nexpected given the bias of Gibbs Sampling towards discovering balanced distribu-\ntions of senses. Finally, the two signiﬁcant diﬀerences in the verbs favor McQuitty’s\nmethod. Despite having rather skewed sense distributions, McQuitty’s method and\nGibbs Sampling perform at comparable levels of accuracy for the verbs. This indi-\ncates that the greater granularity of the data representation used by Gibbs Sampling\nis sometimes suﬃcient to oﬀset the bias of McQuitty’s method towards discovering\nskewed sense distributions.\nDirect comparison of McQuitty’s method and Gibbs Sampling must take into\naccount the diﬀerences in the data representations employed. McQuitty’s similarity\nanalysis is based upon counts of the number of dissimilar features between multiple\ninstances of the ambiguous word.\nA probabilistic model is based upon frequency\ncounts of the marginal events as deﬁned by its parametric form. Given these rather\ndiﬀerent representations, it is not surprising that the accuracies for the two meth-\nods for a given word and feature set are often quite diﬀerent. However, this makes\nthe few cases where the two methods achieve nearly identical results all the more\nintriguing. For example, agree with feature sets A and C, interest with feature set\nB, and help with feature set C, all achieve very similar levels of accuracy despite the\ndiﬀerences in representation. Understanding the conditions that lead to these results\nis an interesting area for future work.\nIn general, these results suggest that the characteristics of a feature set must\nbe compatible with the learning algorithm in order to achieve good results. Gibbs\nSampling beneﬁts from feature sets that contain a small number of features that\neach have a limited number of possible values. This allows for accurate parameter\n149\nestimation, particularly when the parametric form is a simple model such as Naive\nBayes. For example, the ability learn reliable parameter estimates contributes to the\nhigh accuracy that Gibbs Sampling achieves with feature set A for the nouns. By\ncontrast, McQuitty’s method generally beneﬁts from larger numbers of features and\nhigher dimensional spaces. Given such data, a dissimilarity matrix becomes a richer\nsource of information that can be used to make more ﬁne grained distinctions than\nis the case with a small number of features. This data representation contributes to\nthe overall high accuracy attained with feature set C for adjectives and verbs.\n150\nCHAPTER 8\nRELATED WORK\nMuch of the early work in word sense disambiguation relied on the use of rich,\nmanually–crafted knowledge sources such as semantic networks and concept hier-\narchies (e.g., [43], [87], [95]). While these systems were very successful in limited\ndomains, they tended to be diﬃcult to scale up or port to new domains.\nAs the diﬃculty in creating knowledge–rich resources for larger domains became\napparent, research shifted to exploiting online lexical resources that were already con-\nstructed such as dictionaries, thesaruses, and encyclopedias (e.g., [52], [97]). While\nthese are rich sources of knowledge that oﬀer relatively broad coverage of both lan-\nguage and topic, they are not designed for use with a mechanical inferencing algo-\nrithm. Rather, these resources are intended for a human user who will apply their own\ninferencing methods to ﬁnd and understand the information in the lexical resource.\nRecent work in disambiguation has been geared towards corpus–based, statisti-\ncal methods (e.g., [11], [12], [64], [63], [70], [73], [96]). These approaches often employ\nsupervised learning algorithms and require the availability of manually created train-\ning examples from which to learn. However, sense–tagged generally does not exist in\nlarge quantities and it proves expensive to create.\nThe diﬃculties in building semantic networks, the lack of automatic inferencing\nalgorithms appropriate for lexical resources designed for human use, and the time\nconsuming nature of manually sense–tagging text; all these factors lead to the real-\nization that the only truly broad–coverage knowledge resource currently available for\nword sense disambiguation is raw untagged text. However, the lack of any systematic\nstructure and the absence of points of reference to external knowledge sources makes\nuntagged text a very challenging resource from which to learn.\n151\nIt is diﬃcult to precisely quantify the degree of structure and richness in a\nknowledge source for word sense disambiguation. The following is an approximate\nand subjective ranking, beginning with the richest and most structured sources of\nknowledge and ending with raw untagged text, the most impoverished and unstruc-\ntured source considered here.\n1. semantic networks, concept hierarchies\n2. machine readable dictionaries, thesaruses\n3. parallel translations\n4. sense–tagged corpora\n5. raw untagged corpora\nThis chapter discusses representative approaches to word sense disambiguation\nthat employ each of these diﬀerent kinds of knowledge resources.\n8.1. Semantic Networks\nA semantic network is a highly structured knowledge source where nodes repre-\nsent concepts and related concepts are connected by links of various types. Common\nexamples of links include is–a, has–part, and is–made–of.\nSemantic networks are often used to model and enforce selectional restrictions,\na concept that ﬁnds its roots in Case Grammar [34]. This is a lexically based linguistic\nformalism where verbs are deﬁned based on the roles, i.e, case frames, of the words\nthat they may be validly used with. As a simple example, suppose that the verb hit\nis deﬁned as follows:\nhit :: [AGENT:human] [OBJECT:projectile] [INSTRUMENT:club]\nAGENT, OBJECT and INSTRUMENT are just a few examples of possible case\nframes. The selectional restrictions on these frames are speciﬁed in lower case letters.\n152\nBOY \nHIT\nBALL\n BAT\nHUMAN\n   PROJECTILE \n    CLUB\n       HIT\nIS-A \nIS-A\nIS-A\nIS-A\nINSTRUMENT\n OBJECT\nAGENT\nFigure 8.1. Simple Semantic Network\nThis deﬁnition tells us that the verb hit expects that the AGENT who performs the\nhitting is a human, that the OBJECT that AGENT hits is a projectile, and that the\nINSTRUMENT the AGENT uses to hit the OBJECT with is a club.\nIn these approaches nouns are often deﬁned in terms of subsuming relations as\nshown in a IS-A hierarchy. A bat is a club, a boy is a human, and a ball is a projectile.\nA Case Grammar parser will accept the sentence The boy hit the ball with a bat since\nall of the selectional restrictions imposed by the verb are honored.\nCase Grammar and selectional restrictions are conveniently mapped onto a se-\nmantic network. The nodes of the network represent concepts and the links between\nnodes enforce the selectional restrictions. A semantic network representation of the\nCase Grammar for hit appears in Figure 8.1. Here the nouns and verbs in the sentence\nare shown in boxes, the concepts are in ovals, and the links are labeled appropriately.\nOnce a semantic network is constructed, word sense disambiguation can be\nperformed using marker passing as an inference mechanism.\nMarker passing was\nintroduced in [77] as a means of spreading activation on semantic memory. Marker\npassing was extended to serve as an inferencing mechanism by [17].\nMarkers are able to travel through the semantic network, visiting the nodes\nand moving along the links. Markers are restricted as to what types of links they\nmay travel along. Inferencing is achieved by propagating markers from the concepts\n153\nof interest and determining at what concepts they intersect in the network. These\npoints of intersection will reveal some kind of relationship between the two concepts\nthat may not have been previously realized.\nMarker passing has been widely used in language processing for various infer-\nencing problems, including word sense disambiguation (e.g. [21], [43], [61], [65], and\n[100]). Generally the words in a sentence activate the concepts that they are linked to\nby passing a marker. The activated concepts continue to propagate markers to other\nconcepts until the network eventually stabilizes. This stabilized network represents\nthe disambiguated sentence.\nMarker passing oﬀers tremendous opportunities to exploit parallel computer\narchitectures. It is also an intuitively appealing approach that may ultimately allow\nfor the development of reasonable cognitive models of disambiguation. However, the\nquestion of how to construct the underlying representations remains problematic. One\napproach that has proven successful is to learn selectional constraints via interactive\ntraining with a user (e.g. [16], [45]). Another option is to automatically construct\nthese representations from existing resources such as a machine readable dictionary\n(e.g. [14], [20], [90]).\n8.2. Machine Readable Dictionaries\nMachine readable dictionaries were ﬁrst applied to word sense disambiguation\nin [52]. There, pine cone was disambiguated based on the dictionary deﬁnitions of\npine and cone. It was noted that the deﬁnitions of pine and cone both contained\nreferences to the concept of a tree:\npine: any of a genus of coniferous evergreen trees which have slender elon-\ngated needles and some of which are valuable timber trees or ornamentals\ncone: a mass of ovule-bearing or pollen-bearing scales or bracts in trees\nof the pine family or in cycads that are arranged usually on a somewhat\nelongated axis\n154\nBy unifying these references to tree a computer program inferred that pine cone is the\nfruit of a tree rather than an edible receptacle for a pine tree. Experimental results\nfor this approach are reported at 50%–70% accuracy for short passages from Pride\nand Prejudice and an Associated Press news story.\n[97] presents an approach where ambiguous words that occur in encyclopedia\nentries are disambiguated with respect to categories deﬁned in Roget’s Thesaurus.\nA Naive Bayes model is developed that contains 100 feature variables and a single\nvariable representing the sense of the ambiguous word. The feature variables are the\n50 words to the left and right of the ambiguous word. The parameter estimates for\nthis model are made using category information from Roget’s Thesaurus. There are\n1042 categories in Roget’s. Typical examples of categories include tools–machinery\nor animal–insect, and each category is described by a broad set of relations (similar\nto those represented by links in a semantic network) that typically consist of over\n3,000 words. After the parameter estimates are made from the entries describing\nRoget’s categories, this probabilistic model is used to disambiguate instances of twelve\nambiguous words found in the June 1991 version of Grolier’s Encyclopedia. Accuracy\nis reported at above 90% for 11 of 12 words with between 2 and 6 possible senses.\nWhile machine readable dictionaries are a promising resource for disambigua-\ntion, it can sometimes be the case that dictionary entries are too brief to provide all\nof the salient collocations or other clues that might identify the sense of an ambigu-\nous word. However, as online dictionaries grow more extensive their usefulness as a\nknowledge source in corpus–based language processing will likewise increase.\n8.3. Parallel Translations\nGiven the expense of manually tagging ambiguous words with senses, it is natu-\nral to ask if there are clever means of obtaining sense–tagged text that avoid the need\nfor manual intervention. In fact, the use of parallel translations is such an approach.\nThis methodology relies upon the premise that while a word may be ambiguous in\none language, the various senses may have distinct word forms in another language.\n155\nConsider the word bill in English. It has many possible senses, among them\npending legislation and statement requesting payment. In Spanish these two senses\nhave distinct word forms, proyecto de ley and cuenta. Suppose the following usages\nof bill are found in parallel English and Spanish text:\n1E) The bill is too much.\n1S) La cuenta es demasiado.\n2E) The bill to save the banks is good.\n2S) El proyecto de ley para salvar los bancos es bueno.\nFrom the Spanish text it is clear that usage of bill in sentence 1E) refers to\na statement requesting payment while the usage in sentence 2E) refers to pending\nlegislation.\nThus, the sense distinction made in Spanish is utilized to assign the\nappropriate sense–tags to bill in English.\nThis approach to creating sense–tagged text has been pursued mainly in French\nand English due to the availability of parallel translations of the Canadian Parlia-\nmentary Proceedings, i.e., the Hansards, (e.g. [9], [37]). Once the sense–tags are\nobtained from a parallel translation, supervised learning methods can be employed\nas if the tagging had been performed manually. Naive Bayes with a large window of\ncontext is employed in [37] while [9] identify a single binary feature that makes the\nsense distinction.\nHowever, given the nature of the Hansards, it is in fact rather diﬃcult to locate\nmany words that are truly ambiguous within that domain. For example,[37] point\nout that while bank is highly ambiguous in general text, in the Hansards it nearly\nalways is used to refer to a ﬁnancial institution. Indeed, the location of more diverse\nparallel bilingual texts remains the main obstacle to wider use of this approach.\nA related method described in [24] ﬁnds translations between Hebrew and En-\nglish using co–occurrence statistics from independent Hebrew and English corpora.\nThis approach is somewhat more ﬂexible in that it does not require the availability\nof diverse parallel bilingual corpora.\n156\nAn unanswered question is key to determining the viability of parallel corpora\napproaches; how large is the set of words that are ambiguous in both languages? If it\nis small then this approach is certainly viable. If not, then it may suﬀer from scaling\nproblems much like other resources.\n8.4. Sense–Tagged Corpora\nThe earliest use of sense–tagged text to create models of word sense disambigua-\ntion may have been that of [44]. They built 1,815 models of disambiguation manually,\nfocusing on words that occur at least 20 times in a corpus of 510,976 words. Their\nmodels consist of sets of rules and use features that are found within four positions of\nthe ambiguous word. These features include the part–of–speech of surrounding words,\nthe morphology of the ambiguous word, and membership of surrounding words into\none of sixteen possible semantic categories: Animate, Human, Collective, Abstract\nNoun, Social Place, Body Part, Political, Economic, Color, Communications, Emo-\ntions, Frequency, Evaluative Adjective, Dimensionality Adjective, Position Adjective,\nand Degree Adverb.\nAn early automatic approach where models are learned from sense–tagged text\nis presented in [6]. Two–thousand sense–tagged instances for each of ﬁve words were\ncreated, where each word had three or four possible senses. A decision tree learner\nwas provided with 1,500 training examples for each word, where each example was\ncharacterized by 81 binary features representing the presence or absence of certain\n“contextual categories”.\nThere are three varieties of contextual category; subject\ncategories from Longman’s Dictionary of Contemporary English, the 41 words that\noccur most frequently within two positions of the ambiguous word, and the 40 content\nwords that occur most frequently in the sentence with an ambiguous word. It was\nfound that the dictionary categories resulted in 47% accuracy, the 41 most frequent\nwords resulted in 72% accuracy, and the 40 most frequent content words resulted in\n75% accuracy.\n157\nEarly probabilistic approaches typically attempted to identify and exploit a\nsingle very powerful contextual feature to perform disambiguation. For example [9],\n[25], and [98] all present methods for identifying a single feature that is suﬃcient to\nmake highly accurate disambiguation decisions. In [98] for example, it is reported\nthat a single collocation feature, content–word–to–the–right, results in accuracy well\nover 90% for binary sense distinctions.\nIn order to utilize probabilistic models with more complicated interactions\namong feature variables, [12] introduced the use of sequential model selection and\ndecomposable models for word sense disambiguation. Prior to this, statistical analy-\nsis of natural language data was often limited to the application of standard models,\nsuch as n-grams and Naive Bayes. They developed a sequential model selection pro-\ncedure using backward search and the exact conditional test in combination with a\ntest for model predictive power. In their procedure, the exact conditional test is used\nto guide the generation of new models and a test of model predictive power was used\nto select the ﬁnal model from among those generated during the search.\nThe supervised learning portion of this dissertation largely consists of exten-\nsions to the work of Bruce and Wiebe. As such, their methods are discussed rather\nextensively in Chapter 3 and their feature set and sense–tagged text is described in\nChapter 5.\nWhat emerges throughout the literature of corpus–based approaches to word\nsense disambiguation is considerable variation in the methodologies, a wide range of\nfeature sets, and a great variety in the types of text that have been disambiguated.\nUnfortunately, comparative studies of these approaches have been relatively rare.\nAs mentioned in Chapter 6, [51] compare a neural network, a Naive Bayes\nclassiﬁer, and a content vector when disambiguating six senses of line. It is reported\nthat all three methods are equally accurate. This same data is utilized by [62] and\napplied to an even wider range of approaches; a Naive Bayes classiﬁer, a perceptron,\na decision–tree, a nearest–neighbor classiﬁer, a logic based Disjunctive Normal Form\nlearner, a logic based Conjunctive Normal Form learner, and a decision list learner\n158\nare all compared. It is found that the Naive Bayes classiﬁer and the perceptron prove\nto be the most accurate of these approaches.\nBoth studies employ the same feature set for the line data. It consists of binary\nfeatures that represent the occurrence of all words within approximately a 50 word\nwindow of the ambiguous word, resulting in nearly 3,000 binary features.\nGiven\nthe vast size of the event space, representations of training data created by simple\napproaches such as Naive Bayes and the perceptron capture the same information as\nthose created by more sophisticated methods.\nA comparative study of the nearest neighbor classiﬁer PEBLS and the backward\nsequential model selection method of [12] is presented by [64]. They compare the\nperformance of the two methods at disambiguating 6 senses of interest. They report\nthat PEBLS achieves accuracy of 87% while [12] report accuracy of 78%.1\nThey\nexpand upon the feature set used in [12] (feature set BW) by including collocation\nfeatures and verb–object relationships. Their feature set consists of the following:\n1. collocations that occur within one word of the ambiguous word\n2. part–of–speech of words ± 3 positions of ambiguous word\n3. morphology of the ambiguous word\n4. unordered set of surrounding key–words, i.e., co–occurrences\n5. verb–object syntactic relations\n[64] evaluate the relative contribution of each type of feature to the overall\ndisambiguation accuracy. They report that the collocations provide nearly all of the\ndisambiguation accuracy, while the part–of–speech and morphological information\nalso prove useful.\nThe unordered sets of surrounding key–words and verb–object\nsyntactic relations tended to contribute very little to disambiguation accuracy. Thus,\n1The same data is employed in this dissertation and the highest accuracy attained is 76±2%.\n159\nthe improvement in accuracy that they report may be due to their use of collocations\nfeatures.\nThe fundamental limitation of supervised learning approaches to word sense\ndisambiguation is the availability of sense–tagged text. The largest available source\nof sense–tagged text is the Defense System Organization 192,800 sense–tagged word\ncorpus [64]. There are 191 diﬀerent nouns and verbs that are sense–tagged. The\naverage number of senses per noun is 7.8 and 12.0 senses per verb. The only other\nlarge source of sense–tagged text that is widely available is a 100,000 word subset of\nthe Brown Corpus [36]. Both of these corpora are tagged with WordNet senses. By\nway of speculation, if all of the “privately held” sense–tagged text was added to the\n300,000 words provided by the two corpora above, it seems unlikely that the total\nnumber of sense–tagged instances would exceed one–million words.\n8.5. Raw Untagged Corpora\nThere are in fact relatively few “pure” unsupervised methodologies for word\nsense disambiguation that rely strictly on raw untagged text (e.g., [69], [72], [84],\n[85]). More typically, bootstrapping approaches have been employed. The ﬁrst such\nexample is described in [42]. There a supervised learning algorithm is trained with a\nsmall amount of manually sense–tagged text and applied to a held out test set. Those\nexamples in the test set that are most conﬁdently disambiguated are added to the\ntraining sample and the supervised learning algorithm is re–trained with this larger\ncollection of examples.\n[99] describes a more recent bootstrapping approach. This method takes advan-\ntage of the one sense per collocation hypothesis put forth in [98], where it is observed\nthat words have a strong tendency to be used only in one sense in a given colloca-\ntion. This is an extension of the observation made in [37] that words tend to be used\nonly in one sense in a given discourse or document, i.e., the one sense per discourse\nhypothesis.\n160\nThis algorithm requires a small number of training examples to serve as a seed.\nThere are a variety of options discussed for automatically selecting seeds; one is to\nidentify collocations that uniquely distinguish between senses. For plant, the collo-\ncations manufacturing plant and living plant make such a distinction. Based on 106\nexamples of manufacturing plant and 82 examples of living plant this algorithm is\nable to distinguish between two senses of plant for 7,350 examples with 97 percent ac-\ncuracy. Experiments with 11 other words using collocation seeds result in an average\naccuracy of 96 percent where each word had two possible senses.\nThere are relatively few approaches that attempt to perform disambiguation\nonly using information found in raw untagged text. One of the ﬁrst such eﬀorts is\ndescribed [84]. There words are represented in terms of the co-occurrence statistics\nof four letter sequences. This representation uses 97 features to characterize a word,\nwhere each feature is a linear combination of letter four-grams formulated by a sin-\ngular value decomposition of a 5000 by 5000 matrix of letter four-gram co-occurrence\nfrequencies. The weight associated with each feature reﬂects all usages of the word in\nthe sample. A context vector is formed for each occurrence of an ambiguous word by\nsumming the vectors of the contextual words. The set of context vectors for the word\nto be disambiguated are then clustered, and the clusters are manually sense-tagged.\nA related method is described in [85].\nHowever, here ambiguous words are\nclustered into sense groups based on second–order co–occurrences; two instances of\nan ambiguous word are assigned to the same sense if the words that they co–occur with\nlikewise co–occur with similar words in the training data. In the previous approach\nthe assignment to sense groups was based on ﬁrst–order co–occurrences where an\nambiguous word was represented by the four–grams it directly occurs with.\nIt is\nreported that second–order co–occurrences reduce sparsity and allow for the use of\nsmaller matrices of co–occurrence frequencies.\nIn this approach the evaluation is\nperformed relative to information retrieval tasks that utilize the sense group and do\nnot require sense–tags. This results in a fully automatic approach where no manual\nintervention is required.\n161\nWhile similar in spirit, the unsupervised work in this dissertation and that of\nSch¨utze are somewhat distinct. The features employed in this dissertation occupy a\nmuch smaller event space and rely mainly on collocations, part–of–speech and mor-\nphological information. While he also employs agglomerative clustering to form sense\ngroups, the data is represented in terms of context vectors while the data here is\nrepresented in terms of dissimilarity matrices.\n162\nCHAPTER 9\nCONCLUSIONS\nThis dissertation presents methods of learning probabilistic models of word sense\ndisambiguation that use both supervised and unsupervised techniques. This chapter\nsummarizes the contributions of this research and outlines directions for future work.\n9.1. Supervised Learning\nSupervised learning approaches to word sense disambiguation depend upon the\navailability of sense–tagged text. While the amount of such text is still limited, there\nhas been a deﬁnite increase in quantity in recent years. The largest contribution to\nthis has been the release of the DSO corpus, discussed in the previous chapter. Given\nthe likelihood that even larger amounts of sense–tagged text will become available,\ncontinuing to develop and improve supervised learning approaches for word sense\ndisambiguation is an important issue.\nIndeed, while the cost of manually tagging text with senses is high, it is still a\nless expensive enterprise than creating the resources utilized by knowledge–intensive\napproaches to disambiguation. These more elaborate representations of knowledge\nbring with them an additional problem; suitable inferencing mechanisms must also\nbe developed to reason from this data. When viewed against these alternatives, the\ncost of manually annotating text is actually quite modest.\n9.1.1. Contributions\nThis dissertation advances the state of supervised learning as applied to word\nsense disambiguation in the following ways:\n163\nThe information criteria are introduced as evaluation criteria for sequential\nmodel selection as applied to word sense disambiguation. These are alternatives to\nsigniﬁcance tests that result in a fully automatic selection process. The information\ncriteria do not require manually tuned values to stop the model selection process;\nsuch a mechanism is inherent in their formulation.\nIn particular, Akaike’s Information Criteria is shown to result in a model se-\nlection process that automatically selects accurate models of disambiguation using\neither backward or forward search.\nForward sequential search is introduced as a search strategy for sequential model\nselection as applied to word sense disambiguation. This is an alternative to backward\nsearch that is especially well suited for the sparse data typical in language process-\ning. Forward sequential search has the advantage that the search process starts with\nmodels of very low complexity. This results in candidate models that have a small\nnumber of parameters whose estimates are well supported even in relatively small\nquantities of training data. This ensures that the selection process makes decisions\nbased upon the best available information at the time.\nThis dissertation also introduces the Naive Mix, a new supervised learning al-\ngorithm for word sense disambiguation. The Naive Mix averages an entire sequence\nof decomposable models generated during a sequential selection process to create a\nprobabilistic model. It is more typical that model selection methods only ﬁnd a single\nbest model. However, this dissertation shows that there are usually several diﬀerent\nmodels that result in similar levels of accuracy; this suggests a degree of uncertainly\nin model selection that is accommodated by the Naive Mix.\nEmpirically, the Naive Mix is shown to result in improved accuracy over single\nbest selected models and also proves to be competitive with leading machine learning\nalgorithms. It is also observed that the learning rate of the Naive Mix is very fast. It\noften learns models of high accuracy using small amounts of training data, sometimes\nwith as few as 10 or 50 sense–tagged examples.\n164\nDespite making rather broad assumptions about the dependencies among fea-\ntures in models of disambiguation, Naive Bayes consistently results in accuracy that\nis competitive with a host of other methods. This dissertation presents an analysis\nof Naive Bayes that includes a study of the learning rate as well as a bias–variance\ndecomposition of classiﬁcation error.\nThe learning rate reveals that Naive Bayes has poor accuracy when the training\nsample sizes are small. Given its ﬁxed parametric form it is easily mislead by spurious\npatterns in very small amounts of sense–tagged text. However, as the amount of\ntraining data is increased, it quickly achieves levels of accuracy comparable to methods\nthat build more representative models of the training data.\nThis behavior is analyzed via a bias variance decomposition and reveals that the\nnature of the errors made by the Naive Bayes model are substantially diﬀerent than\nthose made by a more representative model of the training data, here represented as\na decision tree. The bulk of classiﬁcation errors made by Naive Bayes are due to the\nassumptions conveyed in the parametric form of the model. However, it also tends\nto be very robust to diﬀerences between the test and training data. By contrast,\nthe errors made by a decision tree learner are largely due to a failure to generalize\nwell enough to accommodate diﬀerences between test and training instances. How-\never, despite these diﬀerent sources of error, the total level of classiﬁcation accuracy\nachieved by both methods is comparable.\n9.1.2. Future Work\nThe continued viability of supervised learning for word sense disambiguation is\nlargely dependent on the availability of sense–tagged text. Thus, the creation of such\ntext at relatively low cost must be a high priority; future improvments in supervised\nlearning methodologies will be of little interest if suﬃcient quantities of training data\nare not readily available.\nIn general, supervised learning is a well developed area of research. However,\nnatural language poses peculiar problems that have not necessarily been accounted\n165\nfor in previous work. Continued reﬁnement of supervised learning methodologies as\napplied to natural language processing problems is an important area of future work.\nCreation of Sense–Tagged Text: The manual annotation of text with sense tags\nis the clearest route to expanding the current pool of sense–tagged text. Results from\nthis dissertation suggest that even relatively small amounts of sense–tagged text can\nresult in high levels of disambiguation accuracy. This is encouraging news, suggesting\nthat even small additions to the available quantity of sense–tagged text will prove to\nbe a valuable resource for word sense disambiguation.\nTraditional manual annotation eﬀorts will beneﬁt greatly from the development\nof tools that provide some degree automated assistance. As an example, the Alembic\nworkbench [27], provides support for discourse process tagging tasks. A similar tool\ndevoted to word sense disambiguation would considerably ease the burden of manual\nannotation. If a human tagger noticed a particularly salient co–occurrence or col-\nlocation, such a tool could allow for the rapid tagging of a large number of similar\ninstances. For example, suppose that a human tagger notes that any time interest\nrate occurs, it is nearly certain that interest refers to the cost of borrowing money.\nAfter the ﬁrst such instance is manually sense–tagged, an annotation tool locates all\nthe sentences in a corpus where interest rate occurs and applies that same sense tag\nautomatically.\nWhen a commitment is made to manual annotation, there are related questions\nthat arise.\nWhich sense inventory should be used for a particular domain?\nAre\nthe sense distinctions in any dictionary clear enough so that only one sense can be\nassigned to a particular word in a particular context? How can tagger uncertainty\nbe incorporated into sense tagging? How large a factor is human error in manual\nannotation eﬀorts? All of these questions open up new areas of future research.\nAs an alternative to manual sense–tagging, the large amount of text linked\ntogether via the World Wide Web can be viewed as a source of alternative sources of\nknowledge to apply to language processing problems. A hyperlink connecting a word\nor a phrase to a related web page is not nearly as precise a source of information\n166\nas is the link from a word to a sense inventory, i.e., a sense tag.\nHowever, this\ndiversity brings richness; hyperlinks from mallard could lead to photos, stories from\nduck watching expeditions, or maps showing migratory patterns. Short summaries\ngenerated from these various resources (or provided by the web page creator by way\nof a title or introductory comment) can then serve as deﬁnitions or descriptions of\nthe word or phrase in the referring web page. This process ultimately results in an\nabstracted and simpliﬁed version of the relevant portion of the Web that can then be\ntreated as a knowledge representation structure from which inferences about other\nbodies of text can be made.\nVarying Search Strategies: To date only backward and forward search strategies\nhave been utilized with sequential model selection for word sense disambiguation.\nHowever, these are greedy approaches that conduct very focused searches that can\nbypass models that are worthy of consideration. Developing approaches that combine\nbackward and forward search is a potential solution to this problem.\nGiven the success of Naive Bayes, an alternative strategy is to begin forward\nsearches at Naive Bayes rather than the model of independence. However, this strat-\negy presumes that all the features are relevant to disambiguation and disables the\nability to perform feature selection. In order to allow model selection to disregard\nirrelevant features, the process could begin at Naive Bayes and perform a backward\nsearch to determine if any dependencies can safely be removed. The model that results\nfrom this backward search then serves as the starting point for a forward search. At\nvarious intervals the strategy could be reversed from forward to backward, backward\nto forward, and so on, before arriving at a selected model.\nAn alternative to starting the forward searches at Naive Bayes is to generate a\nmodel of moderate complexity randomly and then search backward for some number\nof steps, then forward, and so on until until a model is selected. This entire process\nis repeated some number of times so that a variety of random starting models are\nemployed. The models that are ultimately selected presumably diﬀer somewhat and\ncould be averaged together in a randomized variant of the Naive Mix.\n167\nIf a reversible search strategy is adopted then the information criteria present\ncertain advantages over the signiﬁcance tests as evaluation criteria. The information\ncriteria perform at roughly the same levels of accuracy during backward and forward\nsearch and do not require any adjustment when changing search direction. However,\nthe signiﬁcance tests are somewhat sensitive and require that the pre–determined\ncutoﬀvalue, α, be reset as the direction of the search changes.\nExtending Feature Sets: The feature set employed for supervised learning in this\ndissertation relies upon part–of–speech of the surrounding words, morphology of the\nambiguous word, and collocations that occur anywhere in the sentence. A potential\nextension to this feature set is to incorporate co–occurrence features. Preliminary\nexperimental results with feature sets made up entirely of co–occurrences that occur\nwithin 1 or 2 positions of an ambiguous word result in disambiguation accuracy that\nis at least comparable to that of the supervised learning feature set. This result,\nmentioned brieﬂy in [71], largely inspired the use of co–occurrence features in the\nunsupervised learning experiments.\nThe feature set could also be extended beyond the sentence boundary to include\nfeatures that occur in the same paragraph or even the same document as the am-\nbiguous word. This would allow for the inclusion of features that provide information\nabout earlier occurrences of a word and the sense it was determined to have in that\nprevious context. For example, if an instance of bill is being disambiguated and it is\nknown that two sentences earlier bill refers to a bird jaw then it seems unlikely that\nthe current occurrence is being used in the sense of pending legislation.\n9.2. Unsupervised Learning\nThe development and improvement of unsupervised learning techniques is an\nimportant issue in natural language processing given the diﬃculty in obtaining train-\ning data for supervised learning. The lack of sense–tagged text poses a considerable\nbottleneck when porting supervised learning methods to new domains and unsuper-\nvised methods oﬀer a way to eliminate this need for sense–tagged text.\n168\n9.2.1. Contributions\nThe contributions of this dissertation to unsupervised learning of word senses\nare as follows:\nSeveral feature sets appropriate for unsupervised learning of word senses from\nraw text are developed. Feature sets designed for use with supervised approaches are\nnot directly applicable in an unsupervised setting since they often contain features\nwhose values are based on information only available in sense–tagged text.\nThe local context features developed for unsupervised learning are co–occurrences\nthat occur within a few positions of the ambiguous word. It is more common for un-\nsupervised approaches learning from raw text to rely upon a much wider window\nof context. However, such approaches result in high dimensional event spaces that\ncan press the limits of computing resources.\nThe use of local context features in\nthis dissertation has led to acceptable levels of disambiguation accuracy while still\nmaintaining a relatively modest event space.\nThis dissertation develops probabilistic models for word sense disambiguation\nwithout utilizing sense–tagged text. The EM algorithm and Gibbs Sampling are used\nto estimate the parameters of probabilistic models of disambiguation based strictly\nupon information available in raw untagged text.\nEmpirical comparison shows that Gibbs Sampling results in limited improve-\nment over the accuracy of models learned via the EM algorithm. The similar results\nare somewhat surprising given the tendency of the EM algorithm to ﬁnd local max-\nima. However, the combination of local context features and the parametric form of\nNaive Bayes results in relatively small event spaces where parameter estimation is\nstill fairly reliable.\nDespite its widespread popularly in a wide range of other applications, Gibbs\nSampling has not previously been applied to word sense disambiguation. The intro-\nduction of this technique is an important contribution since it is a general purpose\nmethodology that can be used in a variety of language processing problems.\n169\nFinally, the comparable accuracy of the EM algorithm and Gibbs Sampling sug-\ngests that rather than competing methodologies these should be treated as compli-\nmentary. The EM algorithm appears to provide a reasonably good and very eﬃcient\nﬁrst pass through untagged data. It may be reasonable to approach unsupervised\nlearning using the EM algorithm ﬁrst and then allowing Gibbs Sampling to continue\nfrom there. A similar suggestion is made in [56].\nMcQuitty’s similarity analysis has not been applied to word sense disambigua-\ntion previously.\nIt is a simple agglomerative clustering algorithm that makes no\nassumptions about the nature of the data it is processing and yet results in accurate\ndisambiguation in an unsupervised setting. This approach requires that the data to\nbe disambiguated be converted into a dissimilarity matrix representation that shows\nthe number of mismatched features between observations.\nDespite the simplicity of both the algorithm and the data representation, Mc-\nQuitty’s method is shown to consistently result in more accurate disambiguation than\na well–known agglomerative clustering algorithm, Ward’s minimum–variance method.\nIt also outperforms the EM algorithm and Gibbs Sampling when disambiguating\nwords with very skewed sense distributions such as the adjectives and verbs in these\nexperiments.\n9.2.2. Future Work\nUnsupervised approaches to word sense disambiguation are of interest because\nthey eliminate the need for sense–tagged text.\nDisambiguation can be performed\nbased solely on information found in raw untagged text. However, the lack of sense–\ntagged text impacts much more than the learning algorithm itself. Both the feature\nsets and the evaluation methodology must be formulated somewhat diﬀerently than\nin the case of supervised learning.\nMeaningful Labeling of Sense Groups: In the process of eliminating the need\nfor sense–tagged text to learn probabilistic models of disambiguation, unsupervised\napproaches also remove the link between the text and a sense inventory established\n170\nby a dictionary or some other lexical resource.\nThus, the sense groups that are\ncreated by an unsupervised learner do not have meaningful sense labels or deﬁnitions\nautomatically attached to them; the sense groups are tagged with meaningless names.\nThis poses a problem if the evaluation of the unsupervised learner is relative to\nhuman sense judgments which are in turn based on knowledge of an established sense\ninventory.\nThis dissertation addresses this problem by developing an evaluation method-\nology where a post–processing step is performed that maps sense groups to entries\nin a sense inventory via sense–tagged text. While this allows for very exact measure-\nments of the agreement between the unsupervised learner and a human judge, it also\nimposes a requirement for sense–tagged text on the evaluation methodology.\nA more automatic alternative is to generate some form of sense descriptions\nfrom the sense groups themselves. It is unlikely that deﬁnitions as precise as those\nfound in a dictionary could be created. However, some meaningful labeling of the\nsense groups based on the content of the sentences assigned to the sense group is\npossible and might provide enough additional information to make a link to a known\nentry in a sense inventory.\nThis is perhaps best viewed as another manifestation of a text summarization\nproblem. Given the sentences that make up a sense group, generate a statement that\nsummarizes those sentences. Suppose the following usages of the ambiguous word\nbank are found in a sense group:\nI went to the bank to deposit the money.\nThe bank extended a loan to the Martinez family.\nChase Manhattan bought my bank.\nThe Federal Reserve Bank controls the money supply.\nWhile it seems improbable that a formal dictionary deﬁnition could be generated\nfrom these examples, it is possible to imagine the creation of a generalized description\nsuch as An entity concerned with ﬁnancial matters. This description can then be used\n171\nto choose from the entries in a sense inventory for bank, resulting in the selection of\nan entry that includes some mention of ﬁnances, for example.\nThere are several potential problems in this approach.\nFirst, creating these\ngeneralized descriptions implies that some external knowledge source is available. It\nis possible that this sort of external knowledge will be just as diﬃcult to acquire as\nsense–tagged text. Second, incorrectly grouped instances could cause a description\nto become overly general, i.e., An entity concerned with objects.\nFeature Selection: The frequency based features developed for unsupervised\nlearning of word senses result in reasonably accurate performance, however they do\nnot tend to provide much information about minority senses.\nWhen using raw text features, values are usually selected based on frequency\nof occurrence. This results in features that are often skewed towards the majority\nsense, particularly if the majority sense is a large one. The development of feature\nselection methods that pick out values indicative of minority senses is a key issue for\nimproving the performance of unsupervised approaches.\nPart–of–Speech Ambiguity: Since the early work of Kelly and Stone, sense dis-\nambiguation has been detached from the problem of part–of–speech ambiguity. It\nhas generally been assumed that part–of–speech ambiguity is resolved before sense\ndisambiguation is performed.\nIn supervised learning this is a reasonable assump-\ntion since a human tagger must make a part–of–speech judgment before assigning a\nsense–tag. However, in unsupervised learning where no such examples are employed,\nthe decoupling of sense and part–of–speech ambiguity may in fact gloss over the fact\nthat reliable part–of–speech information may not be available in a truly unsupervised\nsetting.\nThere are two alternatives.\nIn this dissertation part–of–speech ambiguity is\nresolved by a rule based part–of–speech tagger that is applied to the text before\nunsupervised learning begins. However, the quality of this tagging is dubious, even\nthough the part–of–speech distinctions are rather coarse. The second option is to\nsimply assume that part–of–speech information will not be available for unsupervised\n172\nlearning problems. This requires disambiguation using a wider range of possible senses\nthat will cross over multiple parts–of–speech. This assumption would also suggest that\nunsupervised disambiguation be based only on collocations, co–occurrences, and any\nother immediately apparent lexical feature. In fact, one of the feature sets in this\ndissertation (B) takes this approach and performs as well as those features sets that\nrely more heavily on syntactic information.\n173\nREFERENCES\n[1] Akaike, H. A new look at the statistical model identiﬁcation. IEEE Trans-\nactions on Automatic Control AC-19, 6 (1974), 716–723.\n[2] Badsberg, J. An Environment for Graphical Models. PhD thesis, Aalborg\nUniversity, 1995.\n[3] Bar-Hillel, Y. The present status of automatic translation of languages. In\nAdvances in Computers. Vol. 1., F. Alt, Ed. Academic Press, New York, NY,\n1960, pp. 91–163.\n[4] Berger, A., Della Pietra, S., and Della Pietra, V. A maximum\nentropy approach to natural language processing. Computational Linguistics\n22, 1 (1996), 39–71.\n[5] Bishop, Y., Fienberg, S., and Holland, P. Discrete Multivariate Analy-\nsis. The MIT Press, Cambridge, MA, 1975.\n[6] Black, E. An experiment in computational discrimination of English word\nsenses. IBM Journal of Research and Development 32, 2 (1988), 185–194.\n[7] Breiman, L. The heuristics of instability in model selection. Annals of Statis-\ntics 24 (1996), 2350–2383.\n[8] Brodley, C. Recursive automatic bias selection for classiﬁer construction.\nMachine Learning 20 (1995), 63–94.\n[9] Brown, P., Della Pietra, S., and Mercer, R. Word sense disambigua-\ntion using statistical methods. In Proceedings of the 29th Annual Meeting of\nthe Association for Computational Linguistics (1991), pp. 264–304.\n[10] Bruce, R. A Statistical Method for Word-Sense Disambiguation. PhD thesis,\nDept. of Computer Science, New Mexico State University, 1995.\n[11] Bruce, R., and Wiebe, J. A new approach to word sense disambiguation.\nIn Proceedings of the ARPA Workshop on Human Language Technology (1994),\npp. 244–249.\n174\n[12] Bruce, R., and Wiebe, J. Word-sense disambiguation using decomposable\nmodels.\nIn Proceedings of the 32nd Annual Meeting of the Association for\nComputational Linguistics (1994), pp. 139–146.\n[13] Bruce, R., Wiebe, J., and Pedersen, T. The measure of a model. In\nProceedings of the Conference on Empirical Methods in Natural Language Pro-\ncessing (1996), pp. 101–112.\n[14] Bruce, R., Wilks, Y., Guthrie, L., Slator, B., and Dunning, T.\nNounsense - a disambiguated noun taxonomy with a sense of humour. Tech.\nRep. MCCS-92-246, New Mexico State University - Computing Research Lab-\noratory, August 1992.\n[15] Buntine, W. Operations for learning with graphical models. Journal of Ar-\ntiﬁcial Intelligence Research 2 (1994), 159–225.\n[16] Cardie, C.\nA case-based approach to knowledge acquisition for domain-\nspeciﬁc sentence analysis. In Proceedings of the 11th National Conference on\nArtiﬁcial Intelligence (AAAI-93) (1993), pp. 798–803.\n[17] Charniak, E. Passing markers: A theory of contextual inﬂuence in language\ncomprehension. Cognitive Science 7 (1983).\n[18] Cheeseman, P., and Stutz, J. Bayesian classiﬁcation (AutoClass): Theory\nand results. In Advances in Knowledge Discovery and Data Mining, U. Fayyad,\nG. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, Eds. AAAI Press/MIT\nPress, 1996.\n[19] Cherry, L. PARTS - a system for assigning word classes to English text.\nTech. rep., AT &T Bell Laboratories, June 1978.\n[20] Chodorow, M., Byrd, R., and Heidorn. Extracting semantic hierarchies\nfrom a large on-line dictionary. In Proceedings of the 23rd Annual Meeting of the\nAssociation for Computational Linguistics (Chicago, IL, 1993), pp. 299–304.\n[21] Chung, M., and Moldovan, D. Parallel memory-based parsing on a marker-\npassing computer. Tech. Rep. PKPL 93-4, University of Southern California,\nAugust 1993.\n[22] Clark, P., and Niblett, T. The CN2 induction algorithm. Machine Learn-\ning 3, 4 (1989), 261–283.\n[23] Cost, S., and Salzberg, S.\nA weighted nearest neighbor algorithm for\nlearning with symbolic features. Machine Learning 10, 1 (1993), 57–78.\n[24] Dagan, I., and Itai, A. Word sense disambiguation using a second language\nmonolingual corpus. Computational Linguistics 20 (1994), 563–596.\n175\n[25] Dagan, I., Itai, A., and Schwall, U. Two languages are more informative\nthan one. In Proceedings of the 29th Annual Meeting of the Association for\nComputational Linguistics (1991), pp. 130–137.\n[26] Darroch, J., Lauritzen, S., and Speed, T. Markov ﬁelds and log-linear\ninteraction models for contingency tables. The Annals of Statistics 8, 3 (1980),\n522–539.\n[27] Day, D., Aberdeen, J., Hirschman, L., Kozierok, R., Robinson, P.,\nand Vilain, M. Mixed initiative development of language processing systems.\nIn Proceedings of the Fifth Conference on Applied Natural Language Processing\nSystems (Washington, DC, April 1997), pp. 348–355.\n[28] Dempster, A. Covariance selection. Biometricka 28 (1972), 157–175.\n[29] Dempster, A., Laird, N., and Rubin, D. Maximum likelihood from in-\ncomplete data via the EM algorithm. Journal of the Royal Statistical Society B\n39 (1977), 1–38.\n[30] Devijver, P., and Kittler, J. Pattern Classiﬁcation: A Statistical Ap-\nproach. Prentice Hall, Englewood Cliﬀs, NJ, 1982.\n[31] Domingos, P. Unifying instance–based and rule–based induction. Machine\nLearning 24 (1996), 141–168.\n[32] Domingos, P., and Pazzani, M. On the optimality of the simple Bayesian\nclassiﬁer under zero–one loss. Machine Learning (forthcoming).\n[33] Duda, R., and Hart, P. Pattern Classiﬁcation and Scene Analysis. Wiley,\nNew York, NY, 1973.\n[34] Fillmore, C. The case for case. In Universals in Linguistic Theory, E. Bach\nand R. Harms, Eds. Holt, New York, NY, 1968.\n[35] Fisher, R. The Design of Experiments. Oliver and Boyd, London, 1935.\n[36] Francis, W. N., and Kucera, H. Frequency Analysis of English Usage:\nLexicon and Grammar. Houghton Miﬄin, 1982.\n[37] Gale, W., Church, K., and Yarowsky, D. A method for disambiguating\nword senses in a large corpus. Computers and the Humanities 26 (1992), 415–\n439.\n[38] Gelman, A., Carlin, J., Stern, H., and Rubin, D.\nBayesian Data\nAnalysis. Chapman & Hall, New York, 1995.\n176\n[39] Geman, S., and Geman, D. Stochastic relaxation, Gibbs distributions and\nthe Bayesian restoration of images. IEEE Transactions on Pattern Analysis\nand Machine Intelligence 6 (1984), 721–741.\n[40] Geweke, J. Evaluating the accuracy of sampling–based approaches to cal-\nculating posterior moments. In Bayesian Statistics 4, J. Bernardo, J. Berger,\nA. Dawid, and A. Smith, Eds. Oxford University Press, Oxford, 1992.\n[41] Gilks, W., Thomas, A., and Spiegelhalter, D. A language and program\nfor complex Bayesian modeling. The Statistician 43, 1 (1994), 169–177.\n[42] Hearst, M. Noun homograph disambiguation using local context in large text\ncorpora. In Proceedings of the 7th Annual Conference of the UW Centre for the\nNew OED and Text Research: Using Corpora (Oxford, 1991).\n[43] Hirst, G. Semantic Interpretation and the Resolution of Ambiguity. Cam-\nbridge University Press, 1987.\n[44] Kelly, E., and Stone, P. Computer Recognition of English Word Senses.\nNorth–Holland, Amsterdam, 1975.\n[45] Kim, J., and Moldovan, D. PALKA: A system for linguistic knowledge\nacquisition. Tech. Rep. PKPL 92-8, Dept. of Electrical Engineering-Systems,\nUniversity of Southern California, 1992.\n[46] Kohavi, R., Sommerfield, D., and Dougherty, J. Data mining using\nMLC++: A machine learning library in C++. In Tools with Artiﬁcial Intelli-\ngence (1996), pp. 234–245. http://www.sgi.com/Technology/mlc.\n[47] Kohavi, R., and Wolpert, D. Bias plus variance decomposition for zero–\none loss functions. In Proceedings of the Thirteenth International Conference\non Machine Learning (Bari, Italy, July 1996), pp. 275–283.\n[48] Kreiner, S. Analysis of multidimensional contingency tables by exact condi-\ntional tests: Techniques and strategies. Scandinavian Journal of Statistics 14\n(1987), 97–112.\n[49] Langley, P., Iba, W., and Thompson, K. An analysis of Bayesian classi-\nﬁers. In Proceedings of the 10th National Conference on Artiﬁcial Intelligence\n(San Jose, CA, 1992), pp. 223–228.\n[50] Lauritzen, S. The EM algorithm for graphical association models with miss-\ning data. Computational Statistics and Data Analysis 19 (1995), 191–201.\n[51] Leacock, C., Towell, G., and Voorhees, E. Corpus-based statistical\nsense resolution. In Proceedings of the ARPA Workshop on Human Language\nTechnology (March 1993), pp. 260–265.\n177\n[52] Lesk, M. Automatic sense disambiguation using machine readable dictionaries:\nHow to tell a pine cone from a ice cream cone. In Proceedings of SIGDOC ’86\n(1986).\n[53] Madigan, D., and Raftery, A. Model selection and accounting for model\nuncertainty in graphical models using Occam’s Window. Journal of American\nStatistical Association 89 (1994), 1535–1546.\n[54] Marcus, M., Santorini, B., and Marcinkiewicz, M. Building a large\nannotated corpus of English: The Penn Treebank. Computational Linguistics\n19, 2 (1993), 313–330.\n[55] McQuitty, L. Similarity analysis by reciprocal pairs for discrete and contin-\nuous data. Educational and Psychological Measurement 26 (1966), 825–831.\n[56] Meng, X., and van Dyk, D. The EM algorithm – an old folk–song sung to\na new fast tune (with discussion). Journal of Royal Statistics Society, Series B\n59, 3 (1997), 511—567.\n[57] Merz, C., Murphy, P., and Aha, D. UCI Repository of Machine Learning\nDatabases, 1997. http://www.ics.uci.edu/mlearn/MLRepository.html.\n[58] Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., and\nTeller, E. Equations of state calculations by fast computing machines. Jour-\nnal of Chemical Physics 21 (1953), 1087–1091.\n[59] Miller, G. WordNet: A lexical database. Communications of the ACM 38,\n11 (November 1995), 39–41.\n[60] Miller, G., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K.\nFive papers on WordNet, March 1993. CSL Report 43, Princeton, NJ.\n[61] Moldovan, D., Lee, W., Lin, C., and Chung, M. SNAP: parallel pro-\ncessing applied to AI. Computer (May 1992).\n[62] Mooney, R. Comparative experiments on disambiguating word senses: An\nillustration of the role of bias in machine learning. In Proceedings of the Confer-\nence on Empirical Methods in Natural Language Processing (May 1996), pp. 82–\n91.\n[63] Ng, H. Exemplar–based word sense disambiguation: Some recent improve-\nments. In Proceedings of the Second Conference on Empirical Methods in Nat-\nural Language Processing (Providence, RI, August 1997), pp. 208–213.\n[64] Ng, H., and Lee, H. Integrating multiple knowledge sources to disambiguate\nword sense: An exemplar-based approach. In Proceedings of the 34th Annual\nMeeting of the Society for Computational Linguistics (1996), pp. 40–47.\n178\n[65] Norvig, P. Marker passing as a weak method for text inferencing. Cognitive\nScience 13 (1989).\n[66] Pazzani, M., Muramatsu, J., and Billsus, D. Syskill & Webert: Identify-\ning interesting web sites. In Proceedings of the Thirteenth National Conference\non Artiﬁcial Intelligence (Portland, OR, 1996), pp. 54–61.\n[67] Pearl, J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\nInference. San Mateo, CA, Morgan Kaufmann, 1988.\n[68] Pedersen, T. Class.3.0: A probabilistic classiﬁer using decomposable models.\nTech. Rep. 97-CSE-14, Southern Methodist University, August 1997.\n[69] Pedersen, T., and Bruce, R. Distinguishing word senses in untagged text.\nIn Proceedings of the Second Conference on Empirical Methods in Natural Lan-\nguage Processing (Providence, RI, August 1997), pp. 197–207.\n[70] Pedersen, T., and Bruce, R. A new supervised learning algorithm for word\nsense disambiguation. In Proceedings of the Fourteenth National Conference on\nArtiﬁcial Intelligence (Providence, RI, July 1997), pp. 604–609.\n[71] Pedersen, T., and Bruce, R. Unsupervised text mining. Tech. Rep. 97-\nCSE-9, Southern Methodist University, June 1997.\n[72] Pedersen, T., and Bruce, R. Knowledge lean word sense disambiguation.\nIn Proceedings of the Fifteenth National Conference on Artiﬁcial Intelligence\n(Madison, WI, July 1998).\n[73] Pedersen, T., Bruce, R., and Wiebe, J. Sequential model selection for\nword sense disambiguation. In Proceedings of the Fifth Conference on Applied\nNatural Language Processing (Washington, DC, April 1997), pp. 388–395.\n[74] Pedersen, T., Kayaalp, M., and Bruce, R. Signiﬁcant lexical relation-\nships. In Proceedings of the Thirteenth National Conference on Artiﬁcial Intel-\nligence (Portland, OR, August 1996), pp. 455–460.\n[75] Procter, P., Ed. Longman Dictionary of Contemporary English. Longman\nGroup Ltd., Essex, UK, 1978.\n[76] Provan, G., and Singh, M. Data mining and model simplicity: A case\nstudy in diagnosis. In Proceedings of the Second International Conference on\nKnowledge Discovery and Data Mining (Portland, OR, 1996).\n[77] Quillian, M. The teachable language comprehender: A simulation program\nand theory of language. Communications of the ACM 12, 8 (August 1969),\n459–476.\n179\n[78] Quinlan, J. C4.5: Programs for Machine Learning. Morgan Kaufmann, San\nMateo, CA, 1992.\n[79] Raftery, A., and Lewis, S. How many iterations in the Gibbs Sampler?\nIn Bayesian Statistics 4, J. Bernardo, J. Berger, A. Dawid, and A. Smith, Eds.\nOxford University Press, Oxford, 1992, pp. 763–773.\n[80] Read, T., and Cressie, N. Goodness of ﬁt Statistics for Discrete Multivari-\nate Data. Springer-Verlag, New York, NY, 1988.\n[81] Ripley, B. Stochastic Simulation. John Wiley, New York, NY, 1987.\n[82] SAS Institute, I. SAS/STAT User’s Guide, Version 6. SAS Institute Inc.,\nCary, NC, 1990.\n[83] Schaffer, C. A conservation law for generalization performance. In Proceed-\nings of the Eleventh International Machine Learning Conference (1994).\n[84] Schutze, H. Dimensions of meaning. In Proceedings of Supercomputing ’92\n(Minneapolis, MN, 1992), pp. 787–796.\n[85] Schutze, H. Automatic word sense discrimination. Computational Linguistics\n24, 1 (1998), 97–123.\n[86] Schwarz, G. Estimating the dimension of a model. The Annals of Statistics\n6, 2 (1978), 461–464.\n[87] Small, S., and Rieger, C. Parsing and comprehending with word experts\n(a theory and its realizations. In Strategies for Natural Language Processing,\nW. Lehnert and M. Ringle, Eds. Lawrence Erlbaum Associates, Hillsdale, NJ,\n1982.\n[88] Tanner, M. Tools for Statistical Inference: Methods for the Exploration of\nPosterior Distributions and Likelihood Functions. Springer–Verlag, New York,\nNY, 1993.\n[89] Thiesson, B. Users’s guide to GAMES, ver. 1.0. Tech. rep., Department of\nMathematics and Computer Science, Aalborg University, Denmark, 1996.\n[90] Veronis, J., and Ide, N. Word sense disambiguation with very large neu-\nral networks extracted from machine readable dictionaries. In Proceedings of\nthe 13th International Conference on Computational Linguistics (COLING-90)\n(Helsinki, Finland, 1990), pp. 389–394.\n[91] Ward, J. Hierarchical grouping to optimize an objective function. Journal of\nthe American Statistical Association 58 (1963), 236–244.\n180\n[92] Wermuth, N.\nModel search among multiplicative models.\nBiometrics 32\n(June 1976), 253–263.\n[93] Whittaker, J. Graphical Models in Applied Multivariate Statistics. John\nWiley, New York, 1990.\n[94] Wilks, S. The large–sample distribution of the likelihood ratio for testing\ncomposite hypotheses. The Annals of Mathematical Statistics 9 (1938), 60–62.\n[95] Wilks, Y. An intelligent analyzer and understander of English. Communica-\ntions of the ACM 18, 5 (1975), 264–274.\n[96] Wilks, Y., and Stevenson, M. The grammar of sense: Using part–of–\nspeech tags as a ﬁrst step in semantic disambiguation. Natural Language Engi-\nneering 1 (1997).\n[97] Yarowsky, D. Word-sense disambiguation using statistical models of Roget’s\ncategories trained on large corpora. In Proceedings of the 14th International\nConference on Computational Linguistics (Nantes, France, July 1992), pp. 454–\n460.\n[98] Yarowsky, D. One sense per collocation. In Proceedings of the ARPA Work-\nshop on Human Language Technology (1993), pp. 266–271.\n[99] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised\nmethods.\nIn Proceedings of the 33rd Annual Meeting of the Association for\nComputational Linguistics (Cambridge, MA, 1995), pp. 189–196.\n[100] Yu, Y., and Simmons, R. Truly parallel understanding of text. In Proceedings\nof the 8th National Conference on Artiﬁcial Intelligence (AAAI-90) (1990).\n[101] Zipf, G. The Psycho-Biology of Language. Houghton Miﬄin, Boston, MA,\n1935.\n181\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2007-07-26",
  "updated": "2007-07-26"
}