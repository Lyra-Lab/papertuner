{
  "id": "http://arxiv.org/abs/0904.0546v1",
  "title": "Eligibility Propagation to Speed up Time Hopping for Reinforcement Learning",
  "authors": [
    "Petar Kormushev",
    "Kohei Nomoto",
    "Fangyan Dong",
    "Kaoru Hirota"
  ],
  "abstract": "A mechanism called Eligibility Propagation is proposed to speed up the Time\nHopping technique used for faster Reinforcement Learning in simulations.\nEligibility Propagation provides for Time Hopping similar abilities to what\neligibility traces provide for conventional Reinforcement Learning. It\npropagates values from one state to all of its temporal predecessors using a\nstate transitions graph. Experiments on a simulated biped crawling robot\nconfirm that Eligibility Propagation accelerates the learning process more than\n3 times.",
  "text": " \n1\n  \nAbstract— A mechanism called Eligibility Propagation is \nproposed to speed up the Time Hopping technique used for \nfaster Reinforcement Learning in simulations. Eligibility \nPropagation provides for Time Hopping similar abilities to \nwhat eligibility traces provide for conventional Reinforcement \nLearning. It propagates values from one state to all of its \ntemporal predecessors using a state transitions graph. \nExperiments on a simulated biped crawling robot confirm that \nEligibility Propagation accelerates the learning process more \nthan 3 times. \n \nI. INTRODUCTION \nEINFORCEMENT learning (RL) algorithms [16] address \nthe problem of learning to select optimal actions when \nlimited feedback (usually in the form of a scalar \nreinforcement function) from the environment is available.  \nGeneral RL algorithms like Q-learning [17], SARSA and \nTD(λ) [15] have been proved to converge to the globally \noptimal solution (under certain assumptions) [1][17]. They \nare very flexible, because they do not require a model of the \nenvironment, and have been shown to be effective in solving \na variety of RL tasks. This flexibility, however, comes at a \ncertain cost: these RL algorithms require extremely long \ntraining to cope with large state space problems. \nMany different approaches have been proposed for \nspeeding up the RL process. One possible technique is to use \nfunction approximation [8], in order to reduce the effect of \nthe “curse of dimensionality”. Unfortunately, using function \napproximation creates instability problems when used with \noff-policy learning. \nSignificant \nspeed-up \ncan \nbe \nachieved \nwhen \na \ndemonstration of the goal task is available [3], as in \nApprenticeship Learning [7]. Although there is a risk of \nrunning dangerous exploration policies in the real world [10], \nthere are successful implementations of apprenticeship \nlearning for aerobatic helicopter flight [11]. \nAnother possible technique for speeding up RL is to use \nsome form of hierarchical decomposition of the problem [4]. \nA prominent example is the “MAXQ Value Function \n \nManuscript submitted March 31, 2009. This work was supported in part \nby the Japanese Ministry of Education, Culture, Sports, Science and \nTechnology (MEXT). \nP. S. Kormushev, F. Dong and K. Hirota are with the Department of \nComputational Intelligence and Systems Science, Tokyo Institute of \nTechnology, Yokohama, 226-8502, Japan. (phone: +81-45-924-5686/5682; \nfax: +81-45-924-5676; e-mail: {petar, tou, hirota}@hrt.dis.titech.ac.jp). \nK. Nomoto is with the Industrial Design Center, Mitsubishi Electric \nCorporation, Tokyo, Japan. \n(e-mail: Nomoto.Kohei@dw.MitsubishiElectric.co.jp) \nDecomposition” \n[2]. \nHybrid \nmethods \nusing \nboth \napprenticeship learning and hierarchical decomposition have \nbeen successfully applied to quadruped locomotion [14][18]. \nUnfortunately, decomposition of the target task is not always \npossible, and sometimes it may impose additional burden on \nthe users of the RL algorithm. \nA state-of-the-art RL algorithm for efficient state space \nexploration is E3 [6]. It uses active exploration policy to visit \nstates whose transition dynamics are still inaccurately \nmodeled. Because of this, running E3 directly in the real \nworld might lead to a dangerous exploration behavior. \nInstead of executing RL algorithms in the real world, \nsimulations are commonly used. This approach has two main \nadvantages: speed and safety. Depending on its complexity, a \nsimulation can run many times faster than a real-world \nexperiment. Also, the time needed to set up and maintain a \nsimulation experiment is far less compared to a real-world \nexperiment. The second advantage, safety, is also very \nimportant, especially if the RL agent is a very expensive \nequipment (e.g. a fragile robot), or a dangerous one (e.g. a \nchemical plant). Whether the full potential of computer \nsimulations has been utilized for RL, however, is an open \nquestion. \nA new trend in RL suggests that this might not be the case. \nFor example, two techniques have been proposed recently to \nbetter utilize the potential of computer simulations for RL: \nTime Manipulation [12] and Time Hopping [13]. They share \nthe concept of using the simulation time as a tool for speeding \nup the learning process. The first technique, called Time \nManipulation, \nsuggests \nthat \ndoing \nbackward \ntime \nmanipulations inside a simulation can significantly speed up \nthe learning process and improve the state space exploration. \nApplied to failure-avoidance RL problems, such as the \ncart-pole balancing problem, Time Manipulation has been \nshown to increase the speed of convergence by 260% [12].  \nThis paper focuses on the second technique, called Time \nHopping, which can be applied successfully to continuous \noptimization problems. Unlike the Time Manipulation \ntechnique, which can only perform backward time \nmanipulations, the Time Hopping technique can make \narbitrary “hops” between states and traverse rapidly \nthroughout the entire state space. It has been shown to \naccelerate the learning process more than 7 times on some \nproblems [13]. Time Hopping possesses mechanisms to \ntrigger time manipulation events, to make prediction about \npossible future rewards, and to select promising time hopping \ntargets. \nThis paper proposes an additional mechanism called \nEligibility Propagation to be added to the Time Hopping \nEligibility Propagation to Speed up Time \nHopping for Reinforcement Learning \nPetar S. Kormushev, Kohei Nomoto, Fangyan Dong, and Kaoru Hirota \nR\n \n2\ntechnique, in order to provide similar abilities to what \neligibility traces provide for conventional RL. Eligibility \ntraces are easy to implement for conventional RL methods \nwith sequential time transitions, but in the case of Time \nHopping, due to its non-sequential nature, a number of \nobstacles have to be overcome. \nThe following Section II makes a brief overview of the \nTime Hopping technique and its components. Section III \nexplains why it is important (and not trivial) to implement \nsome form of eligibility traces for Time Hopping and \nproposes the Eligibility Propagation mechanism to do this. \nSection IV presents the results from experimental evaluation \nof \nEligibility \nPropagation \non \na \nbenchmark \ncontinuous-optimization problem: a biped crawling robot.  \n \nII. OVERVIEW OF TIME HOPPING \nA. Basics of Time Hopping \nTime Hopping is an algorithmic technique which allows \nmaintaining higher learning rate in a simulation environment \nby hopping to appropriately selected states [13]. For \nexample, let us consider a formal definition of a RL problem, \ngiven by the Markov Decision Process (MDP) on Fig. 1. Each \nstate transition has a probability associated with it. State 1 \nrepresents situations of the environment that are very \ncommon and learned quickly. The frequency with which state \n1 is being visited is the highest of all. As the state number \nincreases, the probability of being in the corresponding state \nbecomes lower. State 4 represents the rarest situations and \ntherefore the most unlikely to be well explored and learned. \n \nFig. 1.  An example of a MDP with uneven state probability distribution. \nTime Hopping can create “shortcuts in time” (shown with dashed lines) \nbetween otherwise distant states, i.e. states connected by a very \nlow-probability path. This allows even the lowest-probability state 4 to be \nlearned easily. \n \nWhen applied to such a MDP, Time Hopping creates \n“shortcuts in time” by making hops (direct state transitions) \nbetween very distant states inside the MDP. Hopping to \nlow-probability states makes them easier to be learned, while \nat the same time it helps to avoid unnecessary repetition of \nalready well-explored states [13]. The process is completely \ntransparent for the underlying RL algorithm. \n \nB. Components of Time Hopping \nWhen applied to a conventional RL algorithm, Time \nHopping consists of 3 components: \n1) Hopping trigger – decides when the hopping starts; \n2) Target selection – decides where to hop to; \n3) Hopping – performs the actual hopping. \nThe flowchart on Fig. 2 shows how these 3 components of \nTime Hopping are connected and how they interact with the \nRL algorithm. \nWhen the Time Hopping trigger is activated, a target state \nand time have to be selected, considering many relevant \nproperties of the states, such as probability, visit frequency, \nlevel of exploration, connectivity to other states (number of \nstate transitions), etc. After a target state and time have been \nselected, hopping can be performed. It includes setting the RL \nagent and the simulation environment to the proper state, \nwhile at the same time preserving all the acquired knowledge \nby the agent. \n \nFig. 2.  Time Hopping technique applied to a conventional RL algorithm. \nThe lower group (marked with a dashed line) contains the conventional RL \nalgorithm main loop, into which the Time Hopping components (the upper \ngroup) are integrated. \n \nIII. ELIGIBILITY PROPAGATION \nA. The role of eligibility traces \nEligibility traces are one of the basic mechanisms for \ntemporal credit assignment in reinforcement learning [16]. \nAn eligibility trace is a temporary record of the occurrence of \nan event, such as the visiting of a state or the taking of an \naction. When a learning update occurs, the eligibility trace is \nused to assign credit or blame for the received reward to the \nmost appropriate states or actions. For example, in the \npopular TD(λ) algorithm, the λ refers to the use of an \neligibility trace. Almost any temporal-difference (TD) \nmethods, e.g., Q-learning and SARSA, can be combined with \neligibility traces to obtain a more general method that may \nlearn more efficiently. This is why it is important to \nimplement some form of eligibility traces for Time Hopping \nas well, in order to speed up its convergence. \nEligibility traces are usually easy to implement for \nconventional RL methods. In the case of Time Hopping, \nhowever, due to its non-sequential nature, it is not trivial to do \nso. Since arbitrary hops between states are allowed, it is \nimpossible to directly apply linear eligibility traces. Instead, \nwe propose a different mechanism called Eligibility \nPropagation to do this. \n \nSelect action \nExecute action \nNo \nYes \nRL initialization \nHopping\ntrigger \nTarget selection \nHopping \nGet reward \nChange state \nTTiim\nmee  H\nHooppppiinngg  \nR\nRLL  aallggoorriitthhm\nm  \nm\nmaaiinn  lloooopp  \n1\n2\n3\n1\n2\n3\n4\n0.9 \n0.9\n0.9\n0.1 \n0.1 \nStart\n“Shortcuts in time” created by Time Hopping \n0.9\n0.1 \n0.1 \n \n3\nB. Eligibility Propagation mechanism \nTime Hopping is guaranteed to converge when an \noff-policy RL algorithm is used [13], because the learned \npolicy is independent of the policy followed during learning. \nThis means that the exploration policy does not converge to \nthe optimal policy. In fact, Time Hopping deliberately tries to \navoid convergence of the policy in order to maintain high \nlearning rate and minimize exploration redundancy. This \nposes \na \nmajor \nrequirement \nfor \nany \npotential \neligibility-trace-mechanism: it has to be able to learn from \nindependent non-sequential state transitions spread sparsely \nthroughout the state space. \nThe proposed solution is to construct an oriented graph \nwhich represents the state transitions with their associated \nactions and rewards and use this data structure to propagate \nthe learning updates. Because of the way Time Hopping \nworks, the graph might be disconnected, consisting of many \nseparate connected components. \nRegardless of the actual order in which Time Hopping \nvisits the states, this oriented graph contains a record of the \ncorrect chronological sequence of state transitions. For \nexample, each state transition can be considered to be from \nstate St to state St+1, and the information about this state \ntransition is independent from what happened before it and \nwhat will happen after it. This allows to efficiently collect the \nseparate pieces of information obtained during the \nrandomized hopping, and to process them uniformly using the \ngraph structure. \nOnce this oriented graph is available, it is used to \npropagate state value updates in the opposite direction of the \nstate transition edges. This way, the propagation logically \nflows backwards in time, from state St to all of its temporal \npredecessor states St-1, St-2 and so on. The propagation stops \nwhen the value updates become sufficiently small. The \nmechanism is illustrated on Fig. 3. \n \n \nFig. 3.  Eligibility Propagation mechanism applied to the oriented graph of \nstate transitions. \n \nIn summary, an explicit definition for the proposed \nmechanism is as follows: \nEligibility Propagation is an algorithmic mechanism for \nTime Hopping to efficiently collect, represent and propagate \ninformation about states and transitions. It uses a state \ntransitions graph and a wave-like propagation algorithm to \npropagate state values from one state to all of its temporal \npredecessor states. \nA concrete implementation of this mechanism within the \nTime Hopping technique is given in the following section. \n \nC. Implementation of Eligibility Propagation \nThe proposed implementation of Eligibility Propagation \ncan be called “reverse graph propagation”, because values are \npropagated inside the graph in reverse (opposite) direction of \nthe state transitions’ directions. The process is similar to the \nwave-like propagation of a BFS (breadth-first search) \nalgorithm. \nIn order to give a more specific implementation \ndescription, Q-learning is used as the underlying RL \nalgorithm. The following is the pseudo-code for the proposed \nEligibility Propagation mechanism: \n \n1. Construct an ordered set (queue) of state transitions \ncalled PropagationQueue and initialize it with the \ncurrent state transition\n1\n,\nt\nt\nS S +\nin this way: \n     \n1\n,\n.\nt\nt\nPropagationQueue\nS S +\n=\n \n(1) \n2.  Take the first state transition  \n1\n,\nt\nt\nS S\nPropagationQueue\n+\n∈\n and remove it from the \nqueue. \n3.  Let \nmax\nQ\n be the current maximum Q-value of state St: \n \n{\n}\nmax\n,\nmax\n,\ntS A\nA\nQ\nQ\n=\n \n(2) \n     where the transition from state St to state St+1 is done by \nexecuting action A, and the reward\n,\ntS A\nR\n is received. \n4. Update the Q-value for making the state transition \n1\n,\nt\nt\nS S +\nusing the update rule: \n \n{\n}\n1\n,\n,\n, '\n'\nmax\n.\nt\nt\nt\nS A\nS A\nS\nA\nA\nQ\nR\nQ\nγ\n+\n=\n+\n \n(3) \n5. Let \nmax\n'\nQ\n be the new maximum Q-value of state St, \ncalculated using formula (2). \n6.  If \nmax\nmax\n'\n,\nQ\nQ\nε\n−\n>\n  \n(4) \n     then construct the set of all immediate predecessor state \ntransitions of state St: \n \n{\n}\n1\n1\n,\n |  \n,\ntransitions graph ,\nt\nt\nt\nt\nS\nS\nS\nS\n−\n−\n∈\n  \n(5) \n     and append it to the end of the\n.\nPropagationQueue  \n7.  If PropagationQueue ≠\n then go to step 2. \n8.  Stop. \n \nThe decision whether further propagation is necessary is \nmade in step 6. The propagation continues one more step \nbackwards in time only if there is a significant difference \nbetween the old maximum Q-value and the new one, \naccording to formula (4). This formula is based on the fact \nthat \nmax\n'\nQ\n might be different than \nmax\nQ\n in exactly 3 out of 4 \npossible cases, which are: \n- \nThe transition \n1\n,\nt\nt\nS S +\nwas the one with the highest \nvalue for state St and its new (bigger) value needs to be \npropagated backwards to its predecessor states. \nSt+1\nPredecessor states\nNext state\n St\nReward\nAction\nSt-1\nEligibility \nPropagation\nCurrent state\nSt-1\nSt-1 \n \n4\n- \nThe transition \n1\n,\nt\nt\nS S +\nwas the one with the highest \nvalue but it is not any more, because its value is \nreduced. Propagation of the new maximum value \n(which belongs to a different transition) is necessary. \n- \nThe transition \n1\n,\nt\nt\nS S +\nwas not the one with the \nhighest value but now it became one, so its value needs \npropagation. \n \nThe only case when propagation is not necessary is when \nthe transition \n1\n,\nt\nt\nS S +\nwas not the one with the highest value \nand it is still not the one after the update. In this case, \nmax\n'\nQ\n \nis equal to \nmax\nQ\n and formula (4) correctly detects it and skips \npropagation. \nIn the previous 3 cases the propagation is performed, \nprovided that there is a significant change of the value, \ndetermined by the ε  parameter. When ε  is smaller, the \nalgorithm tends to propagate further the value changes. When \nε  is bigger, it tends to propagate only the biggest changes \njust a few steps backwards, skipping any minor updates. \nThe depth of propagation also depends on the discount \nfactor γ . The bigger γ  is, the deeper the propagation is, \nbecause longer-term reward accumulation is stimulated. Still, \ndue to the exponential attenuation of future rewards, the γ  \ndiscount factor prevents the propagation from going too far \nand reduces the overall computational cost. \n \n \nFig. 4.  Eligibility Propagation integrated as a 4th component in the \nTime-Hopping technique. \n \nThe described Eligibility Propagation mechanism can be \nencapsulated as a single component and integrated into the \nTime Hopping technique as shown on Fig. 4. It is called \nimmediately after a state transition takes place, in order to \npropagate any potential Q-value changes, and before a time \nhopping step occurs. \n \nIV. APPLICATION OF ELIGIBILITY PROPAGATION TO BIPED \nCRAWLING ROBOT \nIn order to evaluate the efficiency of the proposed \nEligibility Propagation mechanism, experiments on a \nsimulated biped crawling robot are conducted. The goal of \nthe learning process is to find a crawling motion with the \nmaximum speed. The reward function for this task is defined \nas the horizontal displacement of the robot after every action. \n \nA. THEN experimental environment \nA dedicated experimental software system called THEN \n(Time Hopping ENvironment) was developed for the \npurpose of this evaluation. A general view of the environment \nis shown on Fig. 5. THEN has a built-in physics simulation \nengine, implementation of the Time Hopping technique, \nuseful visualization modules (for the simulation, the learning \ndata and the state transitions graph) and most importantly – a \nprototype implementation of the Eligibility Propagation \nmechanism. To facilitate the analysis of the algorithm \nbehavior, THEN displays detailed information about the \ncurrent state, the previous state transitions, a visual view of \nthe simulation, and allows runtime modification of all \nimportant parameters of the algorithms and the simulation. \nThere is a manual and automatic control of the Time Hopping \ntechnique, as well as visualization of the accumulated data in \nthe form of charts. \n \n \nFig. 5. General view of THEN (Time Hopping ENvironment). The built-in \nphysics engine is running a biped crawling robot simulation. \n \nB. Description of the crawling robot \nThe experiments are conducted on a physical simulation of \na biped crawling robot. The robot has 2 limbs, each with 2 \nsegments, for a total of 4 degrees of freedom (DOF). Every \nDOF is independent from the rest and has 3 possible actions \nat each time step: to move clockwise, to move anti-clockwise, \nor to stand still. Fig. 6 shows a typical learned crawling \nsequence of the robot as visualized in the simulation \nenvironment constructed for this task. \n \n \nFig. 6.  Crawling robot with 2 limbs, each with 2 segments for a total of 4 \nDOF. Nine different states of the crawling robot are shown from a typical \nlearned crawling sequence. \nSelect action \nExecute action\nNo \nYes \nRL initialization \nHopping\ntrigger\nTarget selection \nHopping \nEligibility \npropagation \nGet reward \nChange state \nTTiim\nmee--H\nHooppppiinngg  \nR\nRLL  aallggoorriitthhm\nm  \nm\nmaaiinn  lloooopp  \n1\n2\n3\n4\n \n5\n \nWhen all possible actions of each DOF of the robot are \ncombined, assuming that they can all move at the same time \nindependently, it produces an action space with size \n34 - 1 = 80 (we exclude the possibility that all DOF are \nstanding still). Using appropriate discretization for the joint’s \nangles (9 for the upper limbs and 13 for the lower limbs), the \nstate space becomes divided into (9 x 13)2 = 13689 states. \nFor better analysis of the crawling motion, each limb has been \ncolored differently and only the “skeleton” of the robot is \ndisplayed. \n \nC. Description of the experimental method \nThe conducted experiments are divided in 3 groups: \nexperiments using conventional Q-learning, experiments \nusing only the Time Hopping technique applied to Q-learning \n(as described in [13]), and experiments using Time Hopping \nwith Eligibility Propagation. The implementations used for \nthe Time Hopping components are shown in Table I. \n \nThe experiments from all three groups are conducted in \nexactly the same way, using the same RL parameters (incl. \ndiscount factor γ, learning rate α, and the action selection \nmethod parameters). The initial state of the robot and the \nsimulation environment parameters are also equal. The robot \ntraining continues up to a fixed number of steps (45000), and \nthe achieved crawling speed is recorded at fixed checkpoints \nduring the training. This process is repeated 10 times and the \nresults are averaged, in order to ensure statistical \nsignificance. \n \nD. Evaluation of Eligibility Propagation \nThe evaluation of Eligibility Propagation is done using 3 \nmain experiments.  \nIn the first experiment, the learning speed of conventional \nQ-learning, Time Hopping, and Time Hopping with \nEligibility Propagation is compared based on the best \nsolution found (i.e. the fastest achieved crawling speed) for \nthe same number of training steps. The comparison results are \nshown in Fig. 7. It shows the duration of training needed by \neach of the 3 algorithms to achieve a certain crawling speed. \nThe achieved speed is displayed as percentage from the \nglobally optimal solution. \nThe results show that Time Hopping with Eligibility \nPropagation is much faster than Time Hopping alone, which \nin turn is much faster than conventional Q-learning.  \n \nFig. 7.  Speed-of-learning comparison of conventional Q-learning, Time \nHopping, and Time Hopping with Eligibility Propagation. It is based on the \nbest solution achieved relative to the duration of training. The achieved \ncrawling speed is measured as a percentage of the globally optimal solution, \ni.e. the fastest possible crawling speed of the robot. \n \nCompared to Time Hopping alone, Eligibility Propagation \nachieves significant speed-up of the learning process. For \nexample, an 80%-optimal crawl is learned in only 5000 steps \nwhen Eligibility Propagation is used, while Time Hopping \nalone needs around 20000 steps to learn the same, i.e. in this \ncase Eligibility Propagation needs 4 times fewer training \nsteps to achieve the same result. The speed-up becomes even \nhigher as the number of training steps increases. For example, \nTime Hopping with Eligibility Propagation reaches \n90%-optimal solution with 12000 steps, while Time Hopping \nalone needs more than 50000 steps to do the same. \nCompared \nto \nconventional \nQ-learning, \nEligibility \nPropagation achieves even higher speed-up. For example, it \nneeds only 4000 steps to achieve a 70%-optimal solution, \nwhile conventional Q-learning needs 36000 steps to learn the \nsame, i.e. in this case Eligibility Propagation is 9 times faster. \nTime Hopping alone also outperforms conventional \nQ-learning by a factor of 3 in this case (12000 steps vs. 36000 \nsteps). \nIn the second experiment, the real computational time of \nconventional Q-learning, Time Hopping, and Time Hopping \nwith Eligibility Propagation is compared. The actual \nexecution time necessary for each of the 3 algorithms to reach \na certain crawling speed is measured. \nThe comparison results are shown in Fig. 8.  \n \nFig. 8.  Computational-time comparison of conventional Q-learning, Time \nHopping, and Time Hopping with Eligibility Propagation. It is based on the \nreal computational time of each algorithm required to reach a certain quality \nof the solution, i.e. certain crawling speed. \n \nTABLE I \nIMPLEMENTATION USED FOR EACH TIME HOPPING COMPONENT \n \nComponent name \nImplementation used \n1 \nHopping trigger \nGamma pruning \n2 \nTarget selection \nLasso target selection \n3 \nHopping \nBasic hopping \n4 \nEligibility propagation \nReverse graph propagation \n \n6\nThe results show that Time Hopping with Eligibility \nPropagation achieves 99% of the maximum possible speed \nalmost 3 times faster than Time Hopping alone, and more \nthan 4 times faster than conventional Q-learning. This \nsignificant speed-up of the learning process is achieved \ndespite the additional computational overhead of maintaining \nthe transitions graph. The reason for this is the improved \nGamma-pruning based on more precise future reward \npredictions, as confirmed by the third experiment. \nThe goal of this third experiment is to provide insights \nabout the state exploration and Q-value distribution, in order \nto explain the results from the previous two experiments. \nConventional Q-learning, Time Hopping, and Time Hopping \nwith Eligibility Propagation are compared based on the \nmaximum Q-values achieved for all explored states after \n45000 training steps. The Q-values are sorted in decreasing \norder and represent the distribution of Q-values within the \nexplored state space. Fig. 9 shows the comparison results. \n \n \nFig. 9.  State-exploration comparison of conventional Q-learning, Time \nHopping, and Time Hopping with Eligibility Propagation. It shows the \nsorted sequence of maximum Q-values of all explored states after 45000 \nsteps of training. Time Hopping with Eligibility Propagation has managed to \nfind much higher maximum Q-values for the explored states. The \nconventional Q-learning has explored more states, but has found lower \nQ-values for them. \n \nFirstly, the results show that Time Hopping with Eligibility \nPropagation has managed to find significantly higher \nmaximum Q-values for the explored states compared to both \nconventional Q-learning and Time Hopping. The reason for \nthis is that Eligibility Propagation manages to propagate well \nthe state value updates among all explored states, therefore \nraising their maximum Q-values. \nSecondly, the results show that both Time Hopping and \nTime Hopping with Eligibility Propagation have explored \nmuch fewer states than conventional Q-learning. The reason \nfor this is the Gamma-pruning component of Time Hopping. \nIt focuses the exploration of Time Hopping to the most \npromising branches and avoids unnecessary exploration. \nConventional Q-learning does not have such a mechanism \nand therefore it explores more states, but finds lower \nQ-values for them. \nAlso, Time Hopping with Eligibility Propagation has \nexplored slightly fewer states than Time Hopping alone. The \nreason for this is that while both algorithms concentrate the \nexploration on the most promising parts of the state space, \nonly the Eligibility Propagation manages to propagate well \nthe Q-values among the explored states. This improves the \naccuracy of the future reward estimation performed by the \nGamma-pruning component of Time Hopping, which in its \nturn detects better unpromising branches of exploration and \ntriggers a time hopping step to avoid them. \nThe more purposeful exploration and better propagation of \nthe acquired state information help Eligibility Propagation to \nmake the best of every single exploration step. This is a very \nimportant advantage of the proposed mechanism, especially \nif the simulation involved is computationally expensive. In \nthis case, Eligibility Propagation can save real computational \ntime by reducing the number of normal transition (simulation) \nsteps in favor of Time Hopping steps. \n \nV. CONCLUSION \nThe Eligibility Propagation mechanism is proposed to \nprovide for Time Hopping similar abilities to what eligibility \ntraces provide for conventional RL.  \nDuring operation, Time Hopping completely changes the \nnormal sequential state transitions into a rather randomized \nhopping behavior throughout the state space. This poses a \nchallenge how to efficiently collect, represent and propagate \nknowledge about actions, rewards, states and transitions. \nSince using sequential eligibility traces is impossible, \nEligibility Propagation uses the transitions graph to obtain all \npredecessor states of the updated state. This way, the \npropagation logically flows backwards in time, from one state \nto all of its temporal predecessor states.  \nThe proposed mechanism is implemented as a fourth \ncomponent of the Time Hopping technique. This maintains \nthe clear separation between the 4 Time Hopping components \nand makes it straightforward to experiment with alternative \ncomponent implementations. \nThe biggest advantage of Eligibility Propagation is that it \ncan speed up the learning process of Time Hopping more than \n3 times. This is due to the improved Gamma-pruning ability \nbased on more precise future reward predictions. This, in \nturn, increases the exploration efficiency by better avoiding \nunpromising branches and selecting more appropriate \nhopping targets. \nThe conducted experiments on a biped crawling robot also \nshow that the speed-up is achieved using significantly fewer \ntraining steps. As a result, the speed-up becomes even higher \nwhen the simulation is computationally more expensive, due \nto the more purposeful exploration. This property makes \nEligibility Propagation very suitable for speeding up complex \nlearning tasks which require costly simulation. \nAnother advantage of the proposed implementation of \nEligibility Propagation is that no parameter tuning is \nnecessary during the learning, which makes the mechanism \neasy to use. \nFinally, an important drawback of the proposed technique \nis that it needs additional memory to store the transitions \ngraph data. In other words, the speed-up is achieved by using \nmore memory. \nREFERENCES \n[1] P. Dayan and T. J. Sejnowski, “TD(λ) converges with probability 1,” \nMach. Learn.,  vol. 14, no. 3, pp. 295–301, 1994.  \n[2] T. G. Dietterich, \"Hierarchical Reinforcement Learning with the \nMAXQ Value Function Decomposition\", J. Artif. Intell. Res., vol. 13, \npp. 227-303, 2000. \n \n7\n[3] A. Coates, P. Abbeel, and A. Ng, “Learning for Control from Multiple \nDemonstrations”, ICML, vol. 25, 2008.  \n[4] A. Barto and S. Mahadevan, “Recent Advances in Hierarchical \nReinforcement Learning”, Discrete Event Dynamic Systems, vol. 13, \npp. 341-379, 2003. \n[5] M. Humphrys, “Action Selection methods using Reinforcement \nLearning,“ PhD Thesis, University of Cambridge, June 1997.  \n[6] M. Kearns and S. Singh, “Near-optimal reinforcement learning in \npolynomial time”, Machine Learning, 2002. \n[7] A. Ng, “Reinforcement Learning and Apprenticeship Learning for \nRobotic Control”, Lecture Notes in Computer Science, vol. 4264, pp. \n29-31, 2006. \n[8] D. \nPrecup, \nR.S. \nSutton, \nand \nS. \nDasgupta, \n\"Off-policy \ntemporal-difference learning with function approximation,\" In \nProceedings of the Eighteenth Conference on Machine Learning \n(ICML 2001), ed. M. Kaufmann, pp.417-424, 2001. \n[9] B. Price and C. Boutilier, “Accelerating Reinforcement Learning \nthrough Implicit Imitation,” Journal of Artificial Intelligence \nResearch, vol. 19, pp. 569-629, 2003. \n[10] P. Abbeel and A. Ng, “Exploration and apprenticeship learning in \nreinforcement learning”, ICML, 2005. \n[11] P. Abbeel, A. Coates, M. Quigley, and A. Ng, “An application of \nreinforcement learning to aerobatic helicopter flight”, NIPS, vol. 19, \n2007. \n[12] P. Kormushev, K. Nomoto, F. Dong, and K. Hirota, “Time \nmanipulation technique for speeding up reinforcement learning in \nsimulations”, International Journal of Cybernetics and Information \nTechnologies, vol. 8, no. 1, pp. 12-24, 2008. \n[13] P. Kormushev, K. Nomoto, F. Dong, and K. Hirota, “Time Hopping \ntechnique for faster reinforcement learning in simulations”, IEEE \nTransactions on Systems, Man and Cybernetics part B, submitted, \n2009. \n[14] J. Kolter, M. Rodgers, and A. Ng, “A Control Architecture for \nQuadruped Locomotion Over Rough Terrain”, IEEE International \nConference on Robotics and Automation, 2008. \n[15] R.S. Sutton, “Learning to predict by the methods of temporal \ndifference,” Mach. Learn., vol. 3, pp. 9-44, 1988. \n[16] R. S. Sutton and A. G. Barto, Reinforcement Learning: An \nIntroduction. Cambridge, MA: MIT Press, 1998. \n[17] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Mach. Learn., vol. 8, \npp. 279-292, 1992. \n[18] J. Kolter, P. Abbeel, and A. Ng, “Hierarchical Apprenticeship \nLearning, with Application to Quadruped Locomotion”, Neural \nInformation Processing Systems, vol. 20, 2007. \n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.RO"
  ],
  "published": "2009-04-03",
  "updated": "2009-04-03"
}