{
  "id": "http://arxiv.org/abs/0905.2347v1",
  "title": "Combining Supervised and Unsupervised Learning for GIS Classification",
  "authors": [
    "Juan-Manuel Torres-Moreno",
    "Laurent Bougrain",
    "Frdéric Alexandre"
  ],
  "abstract": "This paper presents a new hybrid learning algorithm for unsupervised\nclassification tasks. We combined Fuzzy c-means learning algorithm and a\nsupervised version of Minimerror to develop a hybrid incremental strategy\nallowing unsupervised classifications. We applied this new approach to a\nreal-world database in order to know if the information contained in unlabeled\nfeatures of a Geographic Information System (GIS), allows to well classify it.\nFinally, we compared our results to a classical supervised classification\nobtained by a multilayer perceptron.",
  "text": "Combining Supervised and Unsupervised\nLearning for GIS Classi\u001ccation\nJuan-Manuel Torres-Moreno1, Laurent Bougrain2, and Frédéric Alexandre2\n1 Laboratoire Informatique d'Avignon\nUniversité d'Avignon et des Pays de Vaucluse\nBP 1228 84911 Avignon Cedex 09, France\n2 Équipe Cortex INRIA/LORIA Campus Scienti\u001cque\nBP 239 54506 Vand÷uvre-lès-Nancy, Cedex, France\njuan-manuel.torres@univ-avignon.fr\nAbstract. This paper presents a new hybrid learning algorithm for un-\nsupervised classi\u001ccation tasks. We combined Fuzzy c-means learning al-\ngorithm and a supervised version of Minimerror to develop a hybrid\nincremental strategy allowing unsupervised classi\u001ccations. We applied\nthis new approach to a real-world database in order to know if the in-\nformation contained in unlabeled features of a Geographic Information\nSystem (GIS), allows to well classify it. Finally, we compared our results\nto a classical supervised classi\u001ccation obtained by a multilayer percep-\ntron.\nKeywords : Minimerror, Hybrid methods, Classi\u001ccation, Unsupervised learn-\ning, Geographic Information System.\n1\nSupervised and Unsupervised Learnings\nFor a classi\u001ccation task, the learning is supervised if the labels of the classes of\nthe input patterns are given a priori by a professor. A cost function calculates the\ndi\u001berence between desired and real outputs produced by a network, then, this\ndi\u001berence is minimized modifying the network's weights by a learning rule. A\nsupervised learning set L is constitued by P couples (ξµ, τ µ), µ = 1, ..., P, where\nξµ is the input pattern µ and τ µ = ±1 its class. ξµ is a N-dimension vector,\nwith numeric or categoric values. If labels τ µ are not present in L, it may be\nused as unsupervised learning. Learning is unsupervised when the object's class\nis not known in advance. This learning is performed by extraction of intrinsic\nregularities of patterns presented to the network. The number of neurons of\nthe output layer corresponds to the desired number of categories. Therefore,\nthe network develops its own representation of input patterns, retaining the\nstatistically redundant traits.\narXiv:0905.2347v1  [cs.LG]  14 May 2009\n2\nSupervised Minimerror\nMinimerror algorithm [1] performs correctly in binary problems of high dimen-\nsionality [3, 4, 10]. The supervised version of Minimerror performs a binary clas-\nsi\u001ccation using the minimization of the cost function:\nE = 1\n2\nP\nX\nµ=1\nV\n\u0012τ µw · ξµ\n2T\n√\nN\n\u0013\n(1)\nwith\nV (x) = 1 −tanh(x)\n(2)\nTemperature T de\u001cnes an e\u001bective window width on both sides of the separat-\ning hyperplane de\u001cned by w. The derivative dV (x)\ndx\nis vanishingly small outside\nthis window. Therefore, if the minimum cost (1) is searched through a gradient\ndescent, only the patterns µ at a\n|γµ| ≡|w · ξµ|\n√\nN\n< 2T\n(3)\ndistance will contribute signi\u001ccantly to learning [1, 2]. Minimerror algorithm im-\nplements this minimization starting at high temperature. The weights are ini-\ntialized with Hebb's rule, which is the minimum of (1) in the high temperature\nlimit. Then, T is slowly decreased upon the successive iterations of the gradient\ndescent by a deterministic annealing, so that only the patterns within the nar-\nrowing window of width 2T are e\u001bectively taken into account for calculating the\ncorrection\nδw = −ϵ ∂E\n∂w\n(4)\nat each time step, where ϵ is the learning rate. Thus, the search of the hyperplane\nbecomes more and more local as the number of iterations increases. In practical\nimplementations, it was found that convergence is considerably speeded-up if\npatterns already learned are considered at a lower temperature TL than the\nnot learned ones, TL < T. Minimerror algorithm has three free parameters: the\nlearning rate ϵ of the gradient descent, the temperature ratio TL/T, and the\nannealing rate δT at which temperature is decreased. At convergence, a last\nminimization with TL = T is performed. This algorithm has been coupled with\na incremental heuristics, NetLS [2,5], which adds neurons in one hidden layer\nas learning function. Several results [2\u00154] show that NetLS is very powerful and\ngives small generalization errors comparable to other methods.\n3\nUnsupervised Minimerror\nA variation of Minimerror, Minimerror-S [2, 3], allows to obtain spherical sepa-\nrations on input's space. The spherical separation used the same cost function\n(1), but a spherical stability γs is de\u001cned by:\nγs = ||w −ξ|| −ρ2\n(5)\nwhere ρ is a hyperspherical's radius centered on w. The pattern's class is τ = −1\ninside the sphere and τ = 1 elsewhere. Spherical separations make it possible\nto consider unsupervised learning using the Minimerror's separating qualities.\nThus, a strategy of unsupervised growing was developed in Loria. The algorithm\nstarts by obtaining the distances between the patterns. The Euclidean distance\ncan be used to calculate them. Once the established distances, we started to\n\u001cnd the pair µ and ν of patterns with the smallest distance ρ. This creates the\n\u001crst incremental kernel. We located the hypersphere's center w0 at the middle\nof patterns µ et ν:\nw0 = (ξµ + ξν)\n2\n(6)\nThe initial radius is \u001cxed\nρ0 = 3ρ\n2\n(7)\nto make enter a certain number of patterns in growing kernel. Then, patterns are\nlabeled τ = −1 if they are inside or in the border of the initial sphere, and τ = 1\nif elsewhere. Minimerror-S \u001cnds the hypersphere {ρ∗, w∗} that better separates\npatterns. The internal representations are σ = −1 if\n−\n1\ncosh2(γµ) < 1\n2\nelse σ = 1. This makes it possible to check if there are patterns with τ = 1\noutside but su\u001eciently close to the sphere (ρ∗\n1, w∗\n1). In this case, then it makes\nτ = −1 for these patterns and it learns them again, repeating the procedure for\nall patterns of L. At this time, it passes to another growing kernel which will\nform a second class w2, calculating with Minimerror-S (ρ∗\n2, w∗\n2), and repeating\nthe procedure until there is no more patterns to classify. Finally it obtains K\nclasses. A pruning procedure can avoid having too many classes by eliminating\nthose with few elements (less than one number \u001cxed in advance). It is possible to\nintroduce conditions at the border, which are restrictions that prevent locating\nthe hypersphere center outside of the input's space. For certain problems this\nstrategy can be interesting. These restrictions are however optional: if it makes\ntoo many learning errors, the algorithm decides to neglect them and the center\nand radius of separating spheres can diverge.\n4\nThe Unsupervised Algorithm Fuzzy c-means\nThis algorithm [6, 7] allows us to obtain a clusterisation of patterns with a fuzzy\napproach. Fuzzy c-means minimizes the sum of the squared errors with the\nfollowing conditions:\nc\nX\nk=1\nmik = 1;\nn\nX\ni=1\nmik > 0; mik ∈0, 1\n(8)\ni = 1, 2, . . . , n; k = 1, 2, . . . , c\n(9)\nThe objective function is de\u001cned by\nJ =\nn\nX\ni=1\nc\nX\nk=1\nmφ\nikd2(ξi, ck)\n(10)\nwhere n is the number of patterns, c is the desired number of classes, ck is the\ncentroid vector of class K, ξi is a pattern i and d2(ξi, ck) is the square of the\ndistance between patterns ξi and ck, in agreement with a de\u001cnition of unspeci\u001ced\ndistance, which to simplify, we will indicate by d2(ξi, ck). φ is a fuzzy parameter,\na value in [2, ∞), which determines the fuzzy\u001ccation of the \u001cnal solution, i.e.,\nit controls the overlapping between the classes. If φ = 1, the solution is a hard\npartition. If φ →∞the solution approaches the maximum of fuzzy\u001ccation and\nall the classes are likely to merge in only one. The minimization of the objective\nfunction J provides the solution for the membership function (6):\nmik =\nd2/φ−1\nik\nPc\nj=1 d2/φ−1\nij\n; i = 1, . . . , n; k = 1, . . . , c;\n(11)\nwhere:\nck =\nPn\ni=1 mφ\nikxi\nPn\ni=1 mφ\nik\n; k = 1, . . . , c\n(12)\nThe fuzzy c-means algorithm is:\n1. Let the class number k, with 1 < k < n.\n2. Let a value of fuzzy parameter f > 2.\n3. To choix a suitable distance de\u001cnition in input's space. That may be eu-\nclidean distance and then d2(xi, ck) = ||xi −ck||2.\n4. To choix a value for stop criterium ϵ (ϵ = 0.001 is a suitable convergence).\n5. Let M = M0, for pattern with random values or with values from a hard\npartition of k-means.\n6. In iteration t = 1, 2, 3, ... (re) calculate C = Ct using 12 and Mt−1.\n7. Re-calculate M = Mt using equation 10 and Ct.\n8. To compare Mt and Mt−1 with a suitable matrix norme. If ||Mt −Mt−1|| < ϵ\nthen stop else go to 6.\n5\nA Hybrid Strategy\nIn spite of the supervised Minimerror's simplicity, the number of classes obtained\nis sometimes too high. Thus, we chose a combined strategy: a \u001crst unsupervised\nhidden layer calculates the centroids with Fuzzy c-means algorithm. As input we\nhave P unlabeled patterns of learning set L. Then Supervised Minimerror \u001cnds\nspherical separations well adapted to maximize the stability of the patterns. The\ninput is the same L set, but labeled by Fuzzy c-means. In this way, the number\nof classes can be selected in advance.\n6\nDeposit Prospection Experiment\nThe mineral resources division of the French geological survey (BRGM [8]) devel-\nops continent-scale Geographic Information System (GIS), which support metal-\nlogenic research. This di\u001ecult real-world problem constitutes a tool for decision\nmaking. The understanding of the formation of metals such as gold, copper or\nsilver is not good enough and a lot of patterns describing a site are available\nincluding the size of the deposit for various metals. In this study, we will focus\non a GIS which covers all the Andes and two classes : deposit and barren. A\ndeposit is an economically exploitable mineral concentration [9]. The concentra-\ntion factor corresponds to the rate of enrichment in a chemical element, i.e. to\nthe relationship between its average content of exploitation and its abundance in\nthe earth's crust. Geologists oppose to the concept of deposit the one of barren.\nActually, for the interpretation of the results of generalization, it is necessary to\nenter the number of sites well classi\u001ced in each category to be able to answer the\nquestion: Is this a deposit or a barren ? In our study, a deposit will be de\u001cned as\na site (represented by a pattern) that contains at least one metal and a barren by\na site without any metal. Then, the classes deposit and barren will be used from\nnow on. The database we used contains 641 patterns, 398 examples of deposits\nand 343 examples of barrens.\n6.1\nStudy of the Attributes\nThe original databases have 25 attributes, 8 qualitative and 17 quantitative, such\nas the position of a deposit, the type and age of the country rock hosting the\ndeposit, the proximity of the deposit to a fault zone distinguished by its orien-\ntation in map view, density and focal depth of earthquakes immediately below\nthe deposit, proximity of active volcanoes, geometry of the subduction zone etc.\nWe made a statistical study to determine the importance of each variable. We\ncalculated for each attribute the average of deposit and barren patterns, in order\nto determine which attributes were relevant for discriminating the patterns (\u001cg-\nure 1). There are some attributes (15, 16, 17 or 22, among others) that are not\nrelevant. On the other hand, the attributes 3, 5, 6 and 25 are rather discriminat-\ning. It is interesting to know how the choice of attributes in\u001duences the learning\nand specially the generalization tasks. Therefore, we created 11 databases with\ndi\u001berent combinations of attributes. Table 1 shows the number of qualitative\nand quantitative attributes, and the dimension for each database used.\n6.2\nData Preprocessing and deposit/barren Approach\nThe range of the attributes is extremely broad. In order to homogenize them,\na standardization of quantitative attributes is suitable. A data preprocessing is\nneeded for the correct functioning of the neural network. Thus, for each continu-\nous variable, the standardization calculates the average and standard deviation.\nThen, the variable was centered and the values divided by the standard devia-\ntion. The qualitative attributes are not modi\u001ced. The standardized corpus was\n0\n5\n10\n15\n20\n25\n0,0\n0,1\n0,2\n0,3\n0,4\n0,5\nDifference\nAttribute\nFig. 1. Mean squared di\u001berences of the average patterns.\nDatabase Attributes Used\nQual. Quant. N\nI\n1 to 25\n8\n17\n25\nII\n1 to 8\n8\n0\n8\nIII\n9 to 25\n0\n17\n17\nIV\n11,12,13,14\n0\n4\n4\nV\n11,12,13,25\n0\n4\n4\nVI\n3,5,6,7\n4\n0\n4\nVII\n11,12,13,14,25\n0\n5\n5\nVIII\n11,12,13,20,25\n0\n5\n5\nIX\n3,5,6,7,11,12,13,25\n4\n4\n8\nX\n11,12,13,14,18,19,20,21,23,24\n0\n10\n10\nXI\n11,12,13,14,18,19,20,21,23,24,25\n0\n11\n11\nTable 1. Andes GIS learning databases used.\ndivided in learning and test sets. The sets consist of randomly selected patterns\nfrom the whole corpus. Learning sets of 10% (64 patterns) to 95% (577 patterns)\nof the original database (641 patterns) were generated. The complement was se-\nlected as test set. There are N input neurons in the network, depending on the\ndatabase dimension. The unsupervised part of the network, Fuzzy c-means, must\n\u001cnd two classes: deposit and barren. Minimerror will \u001cnd the best hyperspherical\nseparator for each class. In the same condition, a multilayer perceptron with 10\nneurons on a single hidden layer obtains up to 77% of correct classi\u001ccation.\n7\nResults\nClassi\u001ccation performance corresponded to the percentage of well classi\u001ced sit-\nuations. Learning and generalization discrimination of deposit and barren were\nobtained for all learning databases. Database VII (including only few quantita-\ntive attributes) had the best learning and generalization performances in com-\nparison to the other databases. When using all the attributes, the performances\nfell. Figure 2 shows some results of this behavior. Based on this information,\nwe kept this database to perform 100 random tests. The capacity of discrimina-\ntion between deposit and barren, according to the percentage of learned patterns\nis shown in \u001cgure 3. The deposit class detection is quite higher than the barren\nclass. We note that the detection of gold, argent and copper remain quite precise,\nbet, that of the molybdenum is rather poor. This can be explained according to\nthe weak presence of this metal.\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n40\n50\n60\n70\n80\nI (all attributes)\nII (1 to 8)\nIV (11,12,13,14)\nVII (11,12,13,14,25)\n% Generalization Performance\n% Patterns\nFig. 2. Generalization performances according to the learning set size obtained by the\nhybrid model with various databases.\n8\nConclusion\nWe developed a variation of Minimerror for unsupervised classi\u001ccation with hy-\nperspherical separations. The hybrid combination of Minimerror and Fuzzy c-\nmeans proved to be the most promising. This strategy applied to real-world\ndatabase, allowed us to predict in a rather satisfactory way if a site could be\nidenti\u001ced or not as a deposit. The 75% value obtained for the well classi\u001ced pat-\nterns with this unsupervised/supervised algorithm is comparable to the values\nobtained with other classical supervised methods. This also shows the discrimi-\nnating capacity of the descriptive attributes that we selected as the most suitable\nfor this two-class problem. Finally, according to the \u001cgure 3, we should be able\nto obtain a signi\u001ccant improvement of the performance just increasing the num-\nber of examples. Additional studies must be made to determine more accurately\nother relevant attributes, as well as to perform hybrid learning multi-class tasks.\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\ndatabase VII (11,12,13,14,25)\ngeneralization\nbarren\ndeposit\n% Performance\n% Patterns\nFig. 3. deposit/barren discrimination performances in generalization according to the\nlearning set size (100 tests) obtained by the hybrid model with the database VII.\nAcknowledgement\nThis research was supported by the Bureau des Recherches Géologique et Minière\n(BRGM), France.\nReferences\n1. Gordon M., Grempel, D.: Learning with a temperature dependant algorithm. Eu-\nrophysics Letters 29 (1990) 275\u0015262\n2. Torres-Moreno, J.M.: Apprentissage et généralisation par des réseaux de neurones:\nétude de nouveaux algorithmes constructifs. Thèse INPG, France, (1997)\n3. Torres Moreno, J.M., Gordon, M.B.: E\u001ecient Adaptative Learning for Classi\u001ccation\ntasks with Binary Units. Neural Computation 10(4) (1998) 1007\u00151030\n4. Torres Moreno, J.M., Gordon, M.B.: Characterization of the Sonar Signals Bench-\nmark. Neural Processing Letters 7(1) (1998) 1\u00154\n5. Dreyfus, G., et al.: Réseaux de Neurones. Méthodologie et applications. Eyrolles,\nParis (2002)\n6. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms.\nPlenum Press, New York (1981)\n7. deGruijter, J.J., McBratney, A.B.: A modi\u001ced fuzzy k-means for predictive classi-\n\u001ccation. In: Bock,H.H.(ed) Classi\u001ccation and Related Methods of Data Analysis.\nElsevier Science, Amsterdam. (1988) 97\u0015104\n8. http://www.brgm.fr\n9. Michel, H., Permingeat, F., Routhier, P., Pélissonnier, H.: Propositions concernant\nla dé\u001cnition des unités métallifères. Comm. Scienti\u001cque à la Commission de la Carte\ngéologique du monde. 22th Int. Geol. Congre. New Dehli (1964) 149\u0015153\n10. Torres-Moreno, J. M., Aguilar, J. C., Gordon, M. B.: The Minimum Number of\nErrors in the N-Parity and its Solution with an Incremental Neural Network. Neural\nProcessing Letters 16(3) (2002) 201\u0015210\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2009-05-14",
  "updated": "2009-05-14"
}