{
  "id": "http://arxiv.org/abs/0906.5151v1",
  "title": "Unsupervised Search-based Structured Prediction",
  "authors": [
    "Hal Daumé III"
  ],
  "abstract": "We describe an adaptation and application of a search-based structured\nprediction algorithm \"Searn\" to unsupervised learning problems. We show that it\nis possible to reduce unsupervised learning to supervised learning and\ndemonstrate a high-quality unsupervised shift-reduce parsing model. We\nadditionally show a close connection between unsupervised Searn and expectation\nmaximization. Finally, we demonstrate the efficacy of a semi-supervised\nextension. The key idea that enables this is an application of the predict-self\nidea for unsupervised learning.",
  "text": "Unsupervised Search-based Structured Prediction\nHal Daum´e III\nme@hal3.name\nSchool of Computing, University of Utah, Salt Lake City, UT 84112\nAbstract\nWe describe an adaptation and application\nof a search-based structured prediction al-\ngorithm “Searn” to unsupervised learning\nproblems.\nWe show that it is possible to\nreduce unsupervised learning to supervised\nlearning and demonstrate a high-quality un-\nsupervised shift-reduce parsing model.\nWe\nadditionally show a close connection between\nunsupervised Searn and expectation maxi-\nmization. Finally, we demonstrate the eﬃ-\ncacy of a semi-supervised extension. The key\nidea that enables this is an application of the\npredict-self idea for unsupervised learning.\n1. Introduction\nA prevalent and useful version of unsupervised learn-\ning arises when both the observed data and the la-\ntent variables are structured.\nExamples range from\nhidden alignment variables in speech recognition (Ra-\nbiner, 1989) and machine translation (Brown et al.,\n1993; Vogel et al., 1996), to latent trees in unsuper-\nvised parsing (Paskin, 2001; Klein & Manning, 2004;\nSmith & Eisner, 2005; Titov & Henderson, 2007), and\nto pose estimation in computer vision (Ramanan et al.,\n2005). These techniques are all based on probabilistic\nmodels. Their applicability hinges on the tractability\nof (approximately) computing latent variable expecta-\ntions, thus enabling the use of EM (Dempster et al.,\n1977). In this paper we show that a recently-developed\nsearch-based algorithm, Searn (Daum´e III et al., 2009\nto appear) (see Section 2.2), can be utilized for unsu-\npervised structured prediction (Section 3). We show:\n(1) that under an appropriate construction, Searn can\nimitate the expectation maximization (Section 4); (2)\nthat unsupervised Searn can be used to obtain com-\npetitive performance on an unsupervised dependency\nparsing task (Section 6); and (3) that unsupervised\nAppearing in Proceedings of the 26 th International Confer-\nence on Machine Learning, Montreal, Canada, 2009. Copy-\nright 2009 by the author(s)/owner(s).\nSearn naturally extends to a semi-supervised setting\n(Section 7). The key insight that enables this work is\nthat we can consider the prediction of the (observed)\ninput to be, itself, a structured prediction problem.\n2. Structured Prediction\nThe supervised structured prediction problem is the\ntask of mapping inputs x to complex structured out-\nputs y (e.g., sequences, trees, etc.). Formally, let X\nbe an arbitrary input space and Y be structure output\nspace. Y is typically assumed to decompose over some\nsmaller substructures (e.g., labels in a sequence). Y\ncomes equipped with a loss function, often assumed\nto take the form of a Hamming loss over the sub-\nstructures.\nFeatures are deﬁned over pairs (x, y) in\nsuch a way that they obey the substructures (e.g., one\nmight have features over adjacent label pairs in a se-\nquence). Under strong assumptions on the structures,\nthe loss function and the features (essentially “local-\nity” assumptions), a number of learning algorithms\ncan be employed: for example, conditional random\nﬁelds (Laﬀerty et al., 2001) or max-margin Markov\nnetworks (Taskar et al., 2005).\nA key diﬃculty in structured prediction occurs when\nthe output space Y, the features, or the loss, does not\ndecompose nicely. All of these issues can lead to in-\ntractable computations at either training or prediction\ntime (often both). An attractive approach for deal-\ning with this intractability is to employ a search-based\nalgorithm.\nThe key idea in search-based structured\nprediction is to ﬁrst decompose the output y into a se-\nquence of (dependent) smaller predictions y1, . . . , yT .\nThese may each be predicted in turn, with later pre-\ndictions dependent of previous decisions.\n2.1. Search-based Structured Prediction\nA recently proposed algorithm for solving the struc-\ntured prediction problem is Searn (Daum´e III et al.,\n2009 to appear). Searn operates by considering each\nsubstructure prediction y1, . . . , yT as a classiﬁcation\nproblem. A classiﬁer h is trained so that at time t,\narXiv:0906.5151v1  [cs.LG]  28 Jun 2009\nUnsupervised Search-based Structured Prediction\ngiven a feature vector, it predict the best value for yt.\nThe feature vector can be based on any part of the\ninput x and any previous decision y1, . . . , yt−1. This\nintroduces a chicken-and-egg problem. h should ide-\nally be trained so that it makes the best decision for yt\ngiven that h makes all past decisions y1, . . . , yt−1 and\nall future decisions yt+1, . . . , yT . Of course, at train-\ning time we do not have access to h (we are trying\nto construct it). The solution is to use an iterative\nscheme.\n2.2. Searn\nThe presentation we give here diﬀers slightly from the\noriginal presentation of the Searn algorithm.\nOur\nmotivation for straying from the original formulation is\nbecause our presentation makes more clear the connec-\ntion between our unsupervised variant of Searn and\nmore standard unsupervised learning methods (such\nas standard algorithms on hidden Markov models).\nLet DSP denote a distribution over pairs (x, y) drawn\nfrom X × Y, and let ℓ(y, ˆy) be the loss associated with\npredicting ˆy when the true answer is y. We assume\nthat y ∈Y can be decomposed into atomic predictions\ny1, . . . , yT , where each yt is drawn from a discrete set\nY . A policy, π, is a (possibly stochastic) function that\nmaps tuples (x, y1, . . . , yt−1) to atomic predictions yt.\nThe key ingredient in Searn is to use the loss func-\ntion ℓand a “current” policy π to turn DSP into a dis-\ntribution over cost-sensitive (multiclass) classiﬁcation\nproblems (Beygelzimer et al., 2005). A cost-sensitive\nclassiﬁcation example is given by an input x and a cost\nvector c = ⟨c1, . . . , cK⟩, where ck is the cost of predict-\ning class k on input x. Deﬁne by Searn(DSP, ℓ, π) a\ndistribution over cost-sensitive classiﬁcation problems\nderived as follows. To sample from this induced dis-\ntribution, we ﬁrst sample an example (x, y) ∼DSP.\nWe then sample t uniformly from [1, T] and run π for\nt −1 steps on (x, y). This yields a partial prediction\n(ˆy1, . . . , ˆyt−1). The input for the cost sensitive classiﬁ-\ncation problem is then the tuple (x, ˆy1, . . . , ˆyt−1). The\ncosts are derived as follows. For each possible choice k\nof ˆyt, we deﬁned ck as the expected loss if π were run,\nbeginning at (ˆy1, . . . , ˆyt−1, k) on input x. Formally:\nck = Eˆyt+1,...,ˆyT ∼πℓ(y, (ˆy1, . . . , ˆyt−1, k, ˆyt+1, . . . , ˆyT ))\n(1)\nSearn assumes access to an “initial policy” π∗(some-\ntimes called the “optimal policy”). Given an input x,\na true output y and a preﬁx of predictions ˆy1, . . . , ˆyt−1,\nπ∗produces a best next-action, ˆyt. It should be con-\nstructed so that the choice ˆyt is optimal (or close to\noptimal) with respect to the problem-speciﬁc loss func-\ntion. For example, if the loss function is Hamming loss,\nAlgorithm Searn-Learn(A, DSP, ℓ, π∗, β)\n1: Initialize π = π∗\n2: while not converged do\n3:\nSample: D ∼Searn(DSP, ℓ, π)\n4:\nLearn: h ←A(D)\n5:\nUpdate: π ←(1 −β)π + βh\n6: end while\n7: Return π without reference to π∗\nFigure 1. The complete Searn algorithm. It’s parameters\nare: a cost-sensitive classiﬁcation algorithm A, a distribu-\ntion over structured problems DSP, a loss function ℓ, an\ninitial policy π∗and an interpolation parameter β.\nthe π∗will always produce ˆyt = yt. For more complex\nloss functions, computing π∗may be more involved.\nGiven these ingredients, Searn operates according the\nalgorithm given in Figure 1. Operationally, the sam-\npling step is typically implemented by generating ev-\nery example from a ﬁxed structured prediction train-\ning set. The costs (expected losses) are computed by\nsampling with tied randomness (Ng & Jordan, 2000).\nIf β = 1/T 3, one can show (Daum´e III et al., 2009 to\nappear) that after at most 2T 3 ln T iterations, Searn\nis guaranteed to ﬁnd a solution π with structured pre-\ndiction loss bounded as:\nL(π) ≤L(π∗) + 2ℓavgT ln T + c(1 + ln T)/T\n(2)\nwhere L(π∗) is the loss of the initial policy (typically\nzero), T is the length of the longest example, c is the\nworse-case per-step loss and ℓavg is the average multi-\nclass classiﬁcation loss. This shows that the structured\nprediction algorithm learned by Searn is guaranteed\nto be not-much-worse than that produced by the initial\npolicy, provided that the created classiﬁcation prob-\nlems are easy (i.e., that ℓavg is small). Note that one\ncan use any classiﬁcation algorithm one likes.\n3. Unsupervised Searn\nIn unsupervised structured prediction, we no longer re-\nceive an pair (x, y) but instead observes only an input\nx. Our job is to construct a classiﬁer that produces y,\neven though we have never observed it.\n3.1. Reduction for Unsupervised to Supervised\nThe key idea—one that underlies much work in unsu-\npervised learning—is that a good y is one that enables\nus to easily recover x. This is precisely the intuition\nwe build in to our model. The observation that makes\nthis practical is that there is nothing in the theory\nor application of Searn that says that π∗cannot be\nUnsupervised Search-based Structured Prediction\nstochastic. Moreover, there is not requirement that the\nloss function depend on all components of the predic-\ntion. Our model will essentially ﬁrst predict y and then\npredict x based on y. Importantly, the loss function is\nagnostic to y (since we do not have true outputs).\nThe general construction is as follows. Let Dunsup be a\ndistribution over inputs x ∈X and let Y be the space\nof desired latent structures (e.g., trees). We deﬁne a\ndistribution Dsup over X ×(Y ×X) by deﬁning a sam-\npling procedure. To sample from Dsup, we ﬁrst sample\nx ∼Dunsup. We then sample uniformly from the set\nof all Y that are valid structures for x. Finally, we re-\nturn the pair (x, (y, x)). We deﬁne a loss function L by\nL((y, x), (ˆy, ˆx)) = Linput(x, ˆx) where Linput is any loss\nfunction on the input space (e.g., Hamming loss). We\napply Searn to the supervised structured prediction\nproblem Dsup, and implicitly learn latent structures.\n3.2. Sequence Labeling Example\nTo gain insight into the operation of Searn in the\nunsupervised setting, it is useful to consider a sequence\nlabeling example. That is, our input x is a sequence\nof length T and we desire a label sequence y of length\nT drawn from a label space of size K.\nWe convert\nthis into a supervised learning problem by considering\nthe “true” structured output to be a label sequence\nof length 2T, with the ﬁrst T components drawn from\nthe label space of size K and the second T components\ndrawn from the input vocabulary. The loss function\ncan then be anything that depends only on the last\nT components. For simplicity, we can consider it to\nbe Hamming loss.\nThe construction of the optimal\npolicy in this case is straightforward. For the ﬁrst T\ncomponents, π∗may behave arbitrarily (e.g., it may\nproduce a uniform distribution over the K labels). For\nthe second T components, π∗always predicts the true\nlabel (which is known, because it is part of the input).\nAn important aspect of the model is the construction\nof the feature vectors. It is most useful to consider this\nconstruction as having two parts. The ﬁrst part has\nto do with predicting the hidden structure (the ﬁrst T\ncomponents). The second part has to do with predict-\ning the observed structure (the second T components).\nFor the ﬁrst part, we are free to use whatever features\nwe desire, so long as they can be computed based on\nthe input x and a partial output. For instance, in the\nHMM case, we could use the two most recent label\npredictions and windowed features from x.\nThe construction of the features for the second part is,\nhowever, also crucial. For instance, if the feature vec-\ntor corresponding to “predict the tth component of x”\ncontains the t component of x, then this learning prob-\nlem is trivial—but also renders the latent structure\nuseless. The goal of the designer of the feature space\nis to construct features for predicting xt that crucially\ndepend on getting the latent structure y correct. That\nis, the ideal feature set is one for which you can predict\nxt accurately if an only if we have found the correct\nlatent structure (more on this in Section 5). For in-\nstance, in the HMM case, we may predict xt based\nonly on the corresponding label yt, or maybe on the\nbasis of yt−1, yt, yt+1. (Note that we are not limited\nto the Markov assumption, as in the case of HMMs.)\nIn the ﬁrst iteration of Searn, all costs for the predic-\ntion of the latent structure are computed with respect\nto the initial policy. Recalling that the initial policy\nbehaves randomly when predicting the latent labels\nand correctly when predicting the words, we can see\nthat these costs are all zero. Thus, for the latent struc-\nture actions, Searn will not induce any classiﬁcation\nexamples (because the cost of all actions is equal).\nHowever, it will create example for predicting the x\ncomponent. For predicting the xs, the cost will be zero\nfor the correct word and one for any incorrect word.\nThese examples will have associated features: we will\npredict word xt based exclusively on yt. Remember:\nyt was generated randomly by the initial policy.\nIn the second iteration, the behavior is diﬀerent.\nSearn returns to creating examples for the latent\nstructure components.\nHowever, in this iteration,\nsince the current policy is not longer optimal, the fu-\nture cost estimates may be non-zero. Consider gen-\nerating an example corresponding to a (latent) state\nyt. For some small percentage (as dictated by β) of the\n“generate x” decisions, the previously learned classiﬁer\nwill ﬁre. If this learned classiﬁer does well, then the\nassociated cost will be low. However, if the learned\nclassiﬁer does poorly, the the associated cost will be\nhigh. Intuitively, the learned classiﬁer will do well if\nand only if the action that labels yt is “good” (i.e.,\nconsistent with what was learned previously). This, in\nthe second pass through the data, Searn does create\nclassiﬁcation examples speciﬁc to the latent decisions.\nAs Searn iterates, more and more of the latent pre-\ndiction decisions are made according to the learned\nclassiﬁers and not with respect to the random policy.\n4. Comparison to EM\nIn this section, we show an equivalence between ex-\npectation maximization in directed probabilistic struc-\ntures and unsupervised Searn.\nWe use mixture of\nmultinomials as a motivating example (primarily for\nsimplicity), but the results easily extend to more com-\nUnsupervised Search-based Structured Prediction\nplicated models (e.g., HMMs: see Section 4.3).\n4.1. EM for Mixture of Multinomials\nIn the mixture of multinomials problem, we are given\nN documents d1, . . . , dN, where dn is a vector of word\ncounts over a vocabulary of size V ; that is, dn,v is\nthe number of times word v appeared in document n.\nThe mixture of multinomials is a probabilistic cluster-\ning model, where we assume an underlying set of K\nclusters (multinomials) that generated the documents.\nDenote by θk the multinomial parameter associated\nwith cluster k, ρk the prior probability of choosing\ncluster k, and let zn be an indicator vector associat-\ning document n with the unique cluster k such that\nzn,k = 1. The probabilistic model has the form:\np(d | θ, ρ) =\nY\nn\n(P\nv dn,v)!\nQ\nv dn,v!\nX\nzn\nY\nk\n\"\nρk\nY\nv\nθdn,v\nk,v\n#zn,k\n(3)\nExpectation maximization in this model involves ﬁrst\ncomputing expectations over the z vectors and then\nupdating the model parameters θ:\nE-step:\nzn,k ∝ρk\nY\nv\nθ\ndn,v\nk,v\n(4)\nM-step:\nθk,v ∝\nX\nn\nzn,kdn,v\n;\nρk ∝\nX\nn\nzn,k\n(5)\nIn both cases, the constant of proportionality is chosen\nso that the variables sum to one over the last compo-\nnent. These updates are repeated until convergence of\nthe incomplete data likelihood, Eq (3).\n4.2. An Equivalent Model in Searn\nNow, we show how to construct an instance of unsu-\npervised Searn that eﬀectively mimics the behavior\nof EM on the mixture of multinomials problem. The\ningredients are as follows:\n• The input space X is the space of documents, repre-\nsented as word count vectors.\n• The (latent) output space Y is a single discrete vari-\nable in the range [1, K] that speciﬁes the cluster.\n• The feature set for predicting y (document counts).\n• The feature set for predicting x is the label y and the\ntotal number of words in the document. The predic-\ntions for a document are estimated word probabilities,\nnot the words themselves.\n• The loss function ignores the prediction y and returns\nthe log loss of the true document x under the word\nprobabilities predicted.\n• The cost-sensitive learning algorithm is diﬀerent de-\npending on whether the latent structure y is being\npredicted or if the document x is being predicted:\n– Structure: The base classiﬁer is a multinomial\nna¨ıve Bayes classiﬁer, parameterized by (say) hm\n– Document:\nThe base classiﬁer is a collection\nof independent maximum likelihood multinomial\nestimators for each cluster.\nConsider the behavior of this setup. In particular, con-\nsider the distribution Searn(DSP, ℓ, π). There are two\n“types” of examples drawn from this distribution: (1)\nlatent structure examples and (2) document examples.\nThe claim is that both classiﬁers learned are identical\nto the mixture of multinomials model from Section 4.1.\nConsider the generation of a latent structure exam-\nple. First, a document n is sampled uniformly from\nthe training set. Then, for each possible label k of this\ndocument, a cost E ˆ\nd∼πl((y, dn), (k, ˆd)) is computed.\nBy deﬁnition, the ˆd that is computed is exactly the\nprediction according to the current multinomial esti-\nmator, hm. Interpreting the multinomial estimator in\nterms of the EM parameters, the costs are precisely the\nzn,ks from EM (see Eq (4)). These latent structure ex-\namples are fed in to the multinomial na¨ıve Bayes clas-\nsiﬁer, which re-estimates a model exactly as per the\nM-step in EM (Eq (5)).\nNext, consider the generation of the document exam-\nples. These examples are generated by π ﬁrst choos-\ning a cluster according to the structure classiﬁer. This\ncluster id is then used as the (only) feature to the “gen-\nerate document” multinomial. As we saw before, the\nprobability that π will select label k for document n\nis precisely zn,k from Eq (4). Thus, the multinomial\nestimator will eﬀectively receive weighted examples,\nweighted by these zn,ks, thus making the maximum\nlikelihood estimate exactly the same as the M-step\nfrom EM (Eq (5)).\n4.3. Synthetic experiments\nTo demonstrate the advantages of the generality of\nSearn, we report here the result of some experiments\non synthetic data.\nWe generate synthetic data ac-\ncording to two diﬀerent HMMs.\nThe ﬁrst HMM is\na ﬁrst-order model. The initial state probabilities, the\ntransition probabilities, and the observation probabil-\nities are all drawn uniformly. The second HMM is a\nsecond-order model, also will all probabilities drawn\nuniformly. The lengths of observations are given by a\nPoisson with a ﬁxed mean.\nIn our experiments, we consider the following learn-\ning algorithms: EM, Searn with HMM features and\na na¨ıve Bayes classiﬁer, and Searn with a logistic\nregression classiﬁer (and an enhanced feature space:\npredicting yt depends on xt−1:t+1. The ﬁrst Searn\nUnsupervised Search-based Structured Prediction\nTable 1. Error rates on ﬁrst- and second-order Markov data with 2, 5 or 10 latent states.\nModels are the true data\ngenerating distribution (approximated by a ﬁrst-order Markov model in the case of HMM2), a model learned by EM, one\nlearned by Searn with a na¨ıve Bayes base classiﬁer, and one learned by Searn with a logistic regression base classiﬁer.\nStandard deviations are given in small text. The best results by row are bolded; the results within the standard deviation\nof the best results are italicized.\nModel\nStates\nTruth\nEM\nSearn -NB\nSearn -LR\n1st order HMM\nK = 2\n0.227 ±0.107\n0.275 ±0.128\n0.287 ±0.138\n0.276 ±0.095\n1st order HMM\nK = 5\n0.687 ±0.043\n0.678 ±0.026\n0.688 ±0.025\n0.672 ±0.022\n1st order HMM\nK = 10\n0.806 ±0.035\n0.762 ±0.021\n0.771 ±0.019\n0.755 ±0.019\n2nd order HMM\nK = 2\n0.294 ±0.072\n0.396 ±0.057\n0.408 ±0.056\n0.271 ±0.057\n2nd order HMM\nK = 5\n0.651 ±0.068\n0.695 ±0.027\n0.710 ±0.016\n0.633 ±0.018\n2nd order HMM\nK = 10\n0.815 ±0.032\n0.764 ±0.021\n0.771 ±0.015\n0.705 ±0.019\nshould mimic EM, but by using sampling rather than\nexact expectation computations. The models are all\nﬁrst-order, regardless of the underlying process.\nWe run the following experiment. For a given number\nof states (which we will vary), we generate 10 random\ndata sets according to each model. Each data set con-\nsists of 5 examples with mean example length of 40 ob-\nservations. The vocabulary size of the observed data is\nalways 10. We compute error rates by matching each\npredicted label to the best-matching true label and the\ncompute Hamming loss. Forward-backward is initial-\nized randomly. We run experiments with the number\nof latent states equal to 2, 5 and 10.1\nThe results of the experiments are shown in Ta-\nble 1. The observations show two things. When the\ntrue model matches the model we attempt to learn\n(HMM1), there is essentially no statistically signiﬁ-\ncant diﬀerence between any of the algorithms. Where\nonce sees a diﬀerence is when the true model does not\nmatch the learned model (HMM2). In this case, we see\nthat Searn-LR obtains a signiﬁcant advantage over\nboth EM and Searn-NB, due to its ability to employ\na richer set of features.\nThese results hold over all\nvalues of K.\nThis is encouraging, since in the real\nworld our model is rarely (if ever) right. The (not sta-\ntistically signiﬁcant) diﬀerence in error rates between\nEM and Searn-NB are due to a sampling versus ex-\nact computation of expectations. Many of the models\noutperform “truth” because likelihood and accuracy\ndo not necessarily correlate (Liang & Klein, 2008).\n5. Analysis\nThere are two keys to success in unsupervised-Searn.\nThe ﬁrst key is that the features on the Y-component\nof the output space be descriptive enough that it be\n1We ran experiments varying the number of samples\nSearn uses in {1, 2, 5}; there was no statistically signiﬁcant\ndiﬀerence. The results we report are based on 2 samples.\nlearnable. One way of thinking of this constraint is\nthat if we had labeled data, then we would be able to\nlearn well. The second key is that the features on the\nX-component of the output space be intrinsically tied\nto the hidden component. Ideally, these features will\nbe such that X can be predicted with high accuracy if\nand only if Y is predicted accurately.\nThe general–though very trivial–result is that if we\ncan guarantee that the loss on Y is bounded by some\nfunction f of the loss on X, then the loss on Y is\nguaranteed after learning to be bounded by f(L(π∗)+\n2ℓavgTmaxlnTmax + c(1 + lnTmax)/Tmax), where all the\nconstants now depend on the induced structured pre-\ndiction problem; see Eq 2.\nOne can see the unsupervised Searn analysis as jus-\ntifying a small variant on “Viterbi training”–the pro-\ncess of performing EM where the E-step is approxi-\nmated with a delta function centered at the maximum.\nOne signiﬁcant issue with Viterbi training is that it is\nnot guaranteed to converge. However, Viterbi training\nis recovered as a special case of unsupervised Searn\nwhere the interpolation parameter is ﬁxed at 1. While\nthe Searn theorem no longer applies in this degen-\nerate case, any algorithm that uses Viterbi training\ncould easily be retroﬁtted to simply make some de-\ncisions randomly. In doing so, one would obtain an\nalgorithm that does have theoretical guarantees.\n6. Unsupervised Dependency Parsing\nThe dependency formalism is a practical and linguis-\ntically interesting model of syntactic structure. One\ncan think of a dependency structure for a sentence of\nlength T as a directed tree over a graph over T + 1\nnodes: one node for each word plus a unique root\nnode. Edges point from heads to dependents. An ex-\nample dependency structure for a T = 7 word sentence\nis shown in Figure 2 . To date, unsupervised depen-\ndency parsing has only been viewed in the context of\nUnsupervised Search-based Structured Prediction\nFigure 2. Dependency parse of a T = 7 word sentence.\nglobal probabilistic models speciﬁed over dependency\npairs (Paskin, 2001) or spanning trees (Klein & Man-\nning, 2004; Smith & Eisner, 2005).\nHowever, there\nis an alternative, popular method for producing de-\npendency trees in a supervised setting: shift-reduce\nparsing (Nivre, 2003; Sagae & Lavie, 2005).\n6.1. Shift-reduce dependency parsing\nShift-reduce dependency parsing (Nivre, 2003) is a left-\nto-right parsing algorithm that operates by maintain-\ning three state variables: a stack S, a current posi-\ntion i and a set of arcs A. The algorithm begins with\n⟨S, i, A⟩= ⟨∅, 1, ∅⟩: the stack and arcset are empty and\nthe current index is 1 (the ﬁrst word). The algorithm\nthen proceeds through a series of actions until a ﬁnal\nstate is reached. A ﬁnal state is one in which i = T, at\nwhich point the set A contains all dependency edges\nfor the parse. Denote by i|I a stack with i at the head\nand stack I at the tail. There are four actions:\nLeftArc: ⟨t|S, i, A⟩−→⟨S, i, (i, t)|A⟩, so long as there\ndoes not exist an arc (·, t) ∈A. (Adds a left depen-\ndency to the arc set between the word t at the top of\nthe stack and the word i at the current index.)\nRightArc: ⟨t|S, i, A⟩−→⟨i|t|s, i + 1, (t, i)|A⟩, so long as\nthere is no arc (·, i) ∈A. (Adds a right dependency\nbetween the top of the stack and the next input.)\nReduce: ⟨t|S, i, A⟩−→⟨S, i, A⟩, so long as there does ex-\nist an arc (·, t) ∈A. (Removes a word from the stack.)\nShift: ⟨S, i, A⟩−→⟨n|S, i + 1, A⟩. (Place item on stack.)\nThis algorithm is guaranteed to terminate in at most\n2T steps with a valid dependency tree (Nivre, 2003),\nunlike standard probabilistic algorithms that have a\ntime-complexity that is cubic in T (McDonald & Satta,\n2007). The advantage of the shift-reduce framework is\nthat it ﬁts nicely into Searn. However, until now, it\nhas been an open question how to train a shift-reduce\nmodel in an unsupervised fashion. The techniques de-\nscribed in this paper give a solution to this problem.\n6.2. Experimental setup\nWe follow the same experimental setup as (Smith &\nEisner, 2005), using data from the WSJ10 corpus (sen-\ntences of length at most ten from the Penn Treebank\n(Marcus et al., 1993)). The data is stripped of punctu-\nation and parsing depends on the part-of-speech tags,\nTable 2. Accuracy on training and test data, plus number\nof iterations for a variety of dependency parsing algorithms\n(all unsupervised except for the last two rows).\nAlgorithm\nAcc-Tr\nAcc-Tst\n# Iter\nRand-Gen\n23.5 ±0.9\n23.5 ±1.3\nRand-Searn\n21.3 ±0.2\n21.0 ±0.6\nK+M:Rand-Init\n23.6 ±3.8\n23.6 ±4.3\n63.3\nK+M:Smart-Init\n35.2 ±6.6\n35.2 ±6.0\n64.1\nS+E:Length\n33.8 ±3.6\n33.7 ±5.9\n173.1\nS+E:DelOrTrans1\n47.3 ±6.0\n47.1 ±5.9\n132.2\nS+E:Trans1\n48.8 ±0.9\n49.0 ±1.5\n173.4\nSearn: Unsup\n45.8 ±1.6\n45.4 ±2.2\n27.6\nS+E: Sup\n79.9 ±0.2\n78.6 ±0.8\n350.5\nSearn: Sup\n81.0 ±0.3\n81.6 ±0.4\n24.4\nnot the words. We use the same train/dev/test split\nas Smith and Eisner: 5301 sentences of training data,\n531 sentences of development data and 530 sentences\nof blind test data. All algorithm development and tun-\ning was done on the development data.\nWe use a slight modiﬁcation to SearnShell to facili-\ntate the development of our algorithm together with a\nmultilabel logistic regression classiﬁer, MegaM.2 Our\nalgorithm uses the following features for the tree-based\ndecisions (inspired by (Hall et al., 2006)), where t is\nthe top of the stack and i is the next token: the parts-\nof-speech within a window of 2 around t and i; the pair\nof tokens at t and i; the distance (discretized) between\nt and i; and the part-of-speech at the head (resp. tail)\nof any existing arc pointing to (resp.\nfrom) t or i.\nFor producing word i, we use the part of speech of i’s\nparent, grandparent, daughters and aunts.\nWe use Searn with a ﬁxed β = 0.1.\nOne sample\nis used to approximate expected losses.\nThe devel-\nopment set is used to tune the scale of the prior vari-\nances for the logistic regression (diﬀerent variances are\nallowed for the “produce tree” and “produce words”\nfeatures). The initial policy makes uniformly random\ndecisions. Accuracy is directed arc accuracy.\n6.3. Experimental results\nThe baseline systems are: two random baselines (one\ngenerative, one given by the Searn initial policy),\nKlein and Manning’s model (Klein & Manning, 2004)\nEM-based model (with and without clever initializa-\ntion), and three variants of Smith and Eisner’s model\n(Smith & Eisner, 2005) (with random initialization,\nwhich seems to be better for most of their mod-\n2SearnShell and MegaM are available at http://searn.\nhal3.name and http://hal3.name/megam, respectively.\nUnsupervised Search-based Structured Prediction\nels). We also report an “upper bound” performance\nbased on supervised training, for both the probabilistic\n(Smith+Eisner model) as well as supervised Searn.\nThe results are reported in Table 2: accuracy on the\ntraining data, accuracy on the test data and the num-\nber of iterations required. These are all averaged over\n10 runs; standard deviations are shown in small print.\nMany of the results (the non-Searn results) are copied\nfrom (Smith & Eisner, 2005). The stopping criteria\nfor the EM-based models is that the log likelihood\nchanges by less than 10e −5. For the Searn-based\nmethods, the stopping criteria is that the development\naccuracy ceases to increase (on the individual classiﬁ-\ncation tasks, not on the structured prediction task).\nAll learned algorithms outperform the random algo-\nrithms (except Klein+Manning with random inits).\nK+M with smart initialization does slightly better\nthan the worst of the S+E models, though the diﬀer-\nence is not statistically signiﬁcant. It does so need-\ning only about a third of the number of iterations\n(moreover, a single S+E iteration is slower than a sin-\ngle K+M iteration). The other two S+E models do\nroughly comparably in terms of performance (strictly\ndominating the previous methods). One of them (“De-\nlOrTrans1”) requires about twice as many iterations as\nK+M; the other (“Trans1”) requires about three times\n(but has much high performance variance). Unsuper-\nvised Searn performs halfway between the best K+M\nmodel and the best S+E model (it is within the error\nbars for “DelOrTrans1” but not “Trans1”).\nNicely, it takes signiﬁcantly fewer iterations to con-\nverge (roughly 15%). Moreover, each iteration is quite\nfast in comparison to the EM-based methods (a com-\nplete run took roughly 3 hours on a 3.8GHz Opteron\nusing SearnShell). Finally, we present results for the\nsupervised case. Here, we see that the Searn-based\nmethod converges much more quickly to a better solu-\ntion than the S+E model. Note that this comparison\nis unfair since the Searn-based model uses additional\nfeatures (though it is a nice property of the Searn-\nbased model that it can make use of additional fea-\ntures). Nevertheless we provide it so as to give a sense\nof a reasonable upper-bound. We imagine that includ-\ning more features would shift the upper-bound and the\nunsupervised algorithm performance up.\n7. A Semi-Supervised Version\nThe unsupervised learning algorithm described above\nnaturally extends to the case where some labeled data\nis available. In fact, the only modiﬁcation to the al-\ngorithm is to change the loss function. In the unsu-\n1\n1.5\n2\n2.5\n3\n3.5\n20\n30\n40\n50\n60\n70\n80\n90\nNumber of examples (log10)\nUnlabeled Arc Accuracy\n \n \nSemi−supervised\nSupervised\nUnsupervised\nFigure 3. Parsing accuracy for semi-supervised, supervised\nand unsupervised Searn. X-axis is: (semi/sup) # of la-\nbeled examples; (unsup) # of unlabeled examples.\npervised case, the loss function completely ignores the\nlatent structure, and returns a loss dependent only on\nthe “predict self” task. In the semi-supervised version,\none plugs in a natural loss function for the “latent”\nstructure prediction for the labeled subset of the data.\nIn Figure 3, we present results on dependency pars-\ning. We show learning curves for unsupervised, fully\nsupervised and semi-supervised models.\nThe x-axis\nshows the number of examples used; in the unsuper-\nvised and supervised cases, this is the total number of\nexamples; in the semi-supervised case, it is the num-\nber of labeled examples. Error bars are two standard\ndeviations. Somewhat surprisingly, with only ﬁve la-\nbeled examples, the semi-supervised approach achieves\nan accuracy of over 70%, only about 10% behind the\nfully supervised approach with 5182 labeled examples.\nEventually the supervised model catches up (at about\n250 labeled examples). The performance of the unsu-\npervised model continues to grow as more examples\nare provided, but never reaches anywhere close to the\nsupervised or semi-supervised models.\n8. Conclusions\nWe have described the application of a search-based\nstructured prediction algorithm, Searn, to unsuper-\nvised learning. This answers positively an open ques-\ntion in the ﬁeld of learning reductions (Beygelzimer\net al., 2005): can unsupervised learning be reduced\nto supervised learning?\nWe have shown a near-\nequivalence between the resulting algorithm and the\nforward-backward algorithm in hidden Markov mod-\nels. We have shown an application of this algorithm\nto unsupervised dependency parsing in a shift-reduce\nframework. This provides the ﬁrst example of unsu-\npervised learning for dependency parsing in a non-\nUnsupervised Search-based Structured Prediction\nprobabilistic model and shows that unsupervised shift-\nreduce parsing is possible. One obvious extension of\nthis work is to structured prediction problems with\nadditional latent structure, such as in machine trans-\nlation. Instead of using the predict-self methodology,\none could directly apply a predict-target methodology.\nThe view of “predict the input” for unsupervised\nlearning is implicit in many unsupervised learning ap-\nproaches, including standard models such as restricted\nBoltzmann machines and Markov random ﬁelds. This\nis made most precise in the wake-sleep algorithm (Hin-\nton et al., 1995), which explicitly trains a neural net-\nwork to reproduce its own input. The wake-sleep al-\ngorithm consists of two phases: the wake phase, where\nthe latent layers are produced, and the sleep phase,\nwhere the input is (re-)produced. These two phases\nare analogous to the predict-structure phase and the\npredict-words phase in unsupervised Searn.\nAcknowledgements.\nThanks for Ryan McDonald\nand Joakim Nivre for discussions related to depen-\ndency parsing algorithms.\nComments from 5 (!)\nanonymous reviewers were incredibly helpful.\nThis\nwas partially supported by NSF grant IIS-0712764.\nReferences\nBeygelzimer, A., Dani, V., Hayes, T., Langford, J.,\n& Zadrozny, B. (2005).\nError limiting reductions\nbetween classiﬁcation tasks.\nProc. Int’l Conf. on\nMachine Learning (pp. 49–56).\nBrown, P., Della Pietra, S., Della Pietra, V., & Mercer,\nR. (1993). The mathematics of statistical machine\ntranslation: Parameter estimation. Computational\nLinguistics, 19, 263–311.\nDaum´e III, H., Langford, J., & Marcu, D. (2009 (to\nappear)). Search-based structured prediction. Ma-\nchine Learning J..\nDempster, A., Laird, N., & Rubin, D. (1977). Max-\nimum likelihood from incomplete data via the EM\nalgorithm. J. of the Royal Statistical Society, B39,\n1–38.\nHall, J., Nivre, J., & Nilsson, J. (2006). Discrimina-\ntive classiﬁers for determining dependency parsing.\nProc. Conf. of the Assoc. for Computational Lin-\nguistics (pp. 316–323).\nHinton, G., Dayan, P., Frey, B., & Neal, R. (1995).\nThe wake-sleep algorithm for unsupervised neural\nnetworks. Science, 26, 1158–1161.\nKlein, D., & Manning, C. (2004). Corpus-based in-\nduction of syntactic structure:\nModels of depen-\ndency and constituency. Proc. Conf. of the Assoc.\nfor Computational Linguistics (pp. 478–485).\nLaﬀerty, J., McCallum, A., & Pereira, F. (2001). Con-\nditional random ﬁelds: Probabilistic models for seg-\nmenting and labeling sequence data.\nProc. Int’l\nConf. on Machine Learning (pp. 282–289).\nLiang, P., & Klein, D. (2008). Analyzing the errors of\nunsupervised learning. Proc. Assoc. for Computa-\ntional Linguistics (pp. 879–887).\nMarcus, M., Marcinkiewicz, M. A., & Santorini, B.\n(1993). Building a large annotated corpus of En-\nglish: The Penn Treebank. Computational Linguis-\ntics, 19, 313–330.\nMcDonald, R., & Satta, G. (2007). On the complexity\nof non-projective data-driven dependency parsing.\nInt’l Wk. on Parsing Technologies (pp. 121–132).\nNg, A., & Jordan, M. (2000). PEGASUS: A policy\nsearch method for large MDPs and POMDPs. Proc.\nConverence on Uncertainty in Artiﬁcial Intelligence\n(pp. 406–415).\nNivre, J. (2003). An eﬃcient algorithm for projective\ndependency parsing. Int’l Wk. on Parsing Technolo-\ngies (pp. 149–160).\nPaskin, M. A. (2001). Grammatical bigrams. Advances\nin Neural Info. Processing Systems (pp. 91–97).\nRabiner, L. (1989). A tutorial on hidden Markov mod-\nels and selected applications in speech recognition.\nProc. IEEE (pp. 257–285).\nRamanan, D., Forsyth, D., & Zisserman, A. (2005).\nStrike a pose: Tracking people by ﬁnding stylized\nposes.\nComputer Vision and Pattern Recognition\n(pp. 271–278).\nSagae, K., & Lavie, A. (2005). A classiﬁer-based parser\nwith linear run-time complexity. Int’l Wk. on Pars-\ning Technologies.\nSmith, N. A., & Eisner, J. (2005). Guiding unsuper-\nvised grammar induction using contrastive estima-\ntion. IJCAI Wk. on Grammatical Inference Apps\n(pp. 73–82).\nTaskar, B., Chatalbashev, V., Koller, D., & Guestrin,\nC. (2005). Learning structured prediction models: A\nlarge margin approach. Proc. Int’l Conf. on Machine\nLearning (pp. 897–904).\nTitov, I., & Henderson, J. (2007).\nA latent vari-\nable model for generative dependency parsing. Int’l\nConf. on Parsing Technologies.\nVogel, S., Ney, H., & Tillmann, C. (1996). HMM-based\nword alignment in statistical translation. Proc. Int’l\nConf. on Computational Linguistics (pp. 836–841).\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2009-06-28",
  "updated": "2009-06-28"
}