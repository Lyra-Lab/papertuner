{
  "id": "http://arxiv.org/abs/0909.3052v1",
  "title": "Cross-Validation for Unsupervised Learning",
  "authors": [
    "Patrick O. Perry"
  ],
  "abstract": "Cross-validation (CV) is a popular method for model-selection. Unfortunately,\nit is not immediately obvious how to apply CV to unsupervised or exploratory\ncontexts. This thesis discusses some extensions of cross-validation to\nunsupervised learning, specifically focusing on the problem of choosing how\nmany principal components to keep. We introduce the latent factor model, define\nan objective criterion, and show how CV can be used to estimate the intrinsic\ndimensionality of a data set. Through both simulation and theory, we\ndemonstrate that cross-validation is a valuable tool for unsupervised learning.",
  "text": "CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nA DISSERTATION\nSUBMITTED TO THE DEPARTMENT OF STATISTICS\nAND THE COMMITTEE ON GRADUATE STUDIES\nOF STANFORD UNIVERSITY\nIN PARTIAL FULFILLMENT OF THE REQUIREMENTS\nFOR THE DEGREE OF\nDOCTOR OF PHILOSOPHY\nPatrick O. Perry\nSeptember 2009\narXiv:0909.3052v1  [stat.ME]  16 Sep 2009\nc⃝Copyright by Patrick O. Perry 2009\nAll Rights Reserved\nii\nI certify that I have read this dissertation and that, in my opinion, it\nis fully adequate in scope and quality as a dissertation for the degree\nof Doctor of Philosophy.\n(Art B. Owen)\nPrincipal Adviser\nI certify that I have read this dissertation and that, in my opinion, it\nis fully adequate in scope and quality as a dissertation for the degree\nof Doctor of Philosophy.\n(Iain M. Johnstone)\nI certify that I have read this dissertation and that, in my opinion, it\nis fully adequate in scope and quality as a dissertation for the degree\nof Doctor of Philosophy.\n(Jonathan E. Taylor)\nApproved for the University Committee on Graduate Studies.\niii\niv\nAbstract\nCross-validation (CV) is a popular method for model-selection. Unfortunately, it is\nnot immediately obvious how to apply CV to unsupervised or exploratory contexts.\nThis thesis discusses some extensions of cross-validation to unsupervised learning,\nspeciﬁcally focusing on the problem of choosing how many principal components to\nkeep. We introduce the latent factor model, deﬁne an objective criterion, and show\nhow CV can be used to estimate the intrinsic dimensionality of a data set. Through\nboth simulation and theory, we demonstrate that cross-validation is a valuable tool\nfor unsupervised learning.\nv\nvi\nAcknowledgments\nThis work could not have been done alone. I would like to thank:\n• Art Owen for always having an open door and for providing endless advice and\nencouragement;\n• Gunnar Carlsson, Trevor Hastie, Iain Johnstone, and Jonathan Taylor for valu-\nable feedback;\n• Mollie Biewald, Ray, Kerry, Clare, and Erin Perry for unwavering support;\n• the entire Stanford Statistics Department for community and education.\nI would also like to thank Mark Churchland for providing the motor cortex data. This\nwork was supported by a Stanford Graduate Fellowship and by the National Science\nFoundation under Grant DMS-0652743.\nvii\nviii\nContents\nAbstract\nv\nAcknowledgments\nvii\n1\nIntroduction\n1\n2\nMultivariate statistics background\n3\n2.1\nThe multivariate normal and Wishart distributions\n. . . . . . . . . .\n3\n2.2\nClassical asymptotics . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3\nModern asymptotics\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.3.1\nThe bulk of the spectrum\n. . . . . . . . . . . . . . . . . . . .\n13\n2.3.2\nThe edges of the spectrum . . . . . . . . . . . . . . . . . . . .\n17\n2.3.3\nEigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3\nBehavior of the SVD\n25\n3.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.2\nAssumptions, notation, and main results . . . . . . . . . . . . . . . .\n27\n3.2.1\nAssumptions and notation . . . . . . . . . . . . . . . . . . . .\n27\n3.2.2\nMain results . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.2.3\nNotes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.3\nPreliminaries\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.3.1\nChange of basis . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.4\nThe secular equation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3.4.1\nOutline of the proof . . . . . . . . . . . . . . . . . . . . . . . .\n34\nix\n3.5\nAnalysis of the secular equation . . . . . . . . . . . . . . . . . . . . .\n35\n3.6\nSolutions to the secular equation\n. . . . . . . . . . . . . . . . . . . .\n43\n3.6.1\nAlmost sure limits\n. . . . . . . . . . . . . . . . . . . . . . . .\n43\n3.6.2\nSecond-order behavior\n. . . . . . . . . . . . . . . . . . . . . .\n47\n3.7\nSingular values and singular vectors . . . . . . . . . . . . . . . . . . .\n51\n3.7.1\nSingular values\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n3.7.2\nRight singular vectors\n. . . . . . . . . . . . . . . . . . . . . .\n53\n3.7.3\nLeft singular vectors\n. . . . . . . . . . . . . . . . . . . . . . .\n54\n3.8\nResults for γ ∈(0, 1) . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n3.9\nRelated work, extensions, and future work . . . . . . . . . . . . . . .\n56\n3.9.1\nRelated work\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.9.2\nExtensions and future work\n. . . . . . . . . . . . . . . . . . .\n57\n4\nAn intrinsic notion of rank for factor models\n59\n4.1\nThe latent factor model\n. . . . . . . . . . . . . . . . . . . . . . . . .\n60\n4.1.1\nThe spiked model . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n4.1.2\nMore general matrix models . . . . . . . . . . . . . . . . . . .\n62\n4.2\nAn intrinsic notion of rank . . . . . . . . . . . . . . . . . . . . . . . .\n62\n4.3\nLoss behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n4.4\nSimulations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n4.5\nRelation to the scree plot . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n4.6\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n4.7\nSummary and future work . . . . . . . . . . . . . . . . . . . . . . . .\n78\n5\nCross-validation for unsupervised learning\n79\n5.1\nAssumptions, and notation . . . . . . . . . . . . . . . . . . . . . . . .\n82\n5.2\nCross validation strategies . . . . . . . . . . . . . . . . . . . . . . . .\n83\n5.2.1\nNaive hold-outs . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n5.2.2\nWold hold-outs . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n5.2.3\nGabriel hold-outs . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n5.2.4\nRotated cross-validation . . . . . . . . . . . . . . . . . . . . .\n88\n5.3\nMissing-value SVDs . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\nx\n5.4\nSimulations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n5.4.1\nPrediction error estimation . . . . . . . . . . . . . . . . . . . .\n94\n5.4.2\nRank estimation\n. . . . . . . . . . . . . . . . . . . . . . . . .\n97\n5.5\nReal data example\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\n5.6\nSummary and future work . . . . . . . . . . . . . . . . . . . . . . . .\n105\n6\nA theoretical analysis of BCV\n107\n6.1\nAssumptions and notation . . . . . . . . . . . . . . . . . . . . . . . .\n109\n6.2\nMain results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n113\n6.3\nThe SVD of the held-in block\n. . . . . . . . . . . . . . . . . . . . . .\n116\n6.4\nThe prediction error estimate\n. . . . . . . . . . . . . . . . . . . . . .\n119\n6.5\nSummary and future work . . . . . . . . . . . . . . . . . . . . . . . .\n126\nA Properties of random projections\n129\nA.1 Uniformly distributed orthonormal k-frames . . . . . . . . . . . . . .\n129\nA.1.1\nGenerating random elements . . . . . . . . . . . . . . . . . . .\n130\nA.1.2\nMixed moments . . . . . . . . . . . . . . . . . . . . . . . . . .\n130\nA.2 Uniformly distributed projections . . . . . . . . . . . . . . . . . . . .\n134\nA.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\nA.3.1\nProjections of orthonormal k-frames\n. . . . . . . . . . . . . .\n137\nA.3.2\nA probabilistic interpretation of the Frobenius norm . . . . . .\n139\nB Limit theorems for weighted sums\n141\nReferences\n145\nxi\nxii\nChapter 1\nIntroduction\nCross-validation (CV) is a popular method for model selection. It works by estimat-\ning the prediction error of each model under consideration and then choosing the\nmodel with the best performance. In unsupervised contexts, though, there is no clear\nnotion as to what exactly “prediction error” is. Therefore, it is diﬃcult to employ\ncross-validation for model selection in unsupervised or exploratory contexts. In this\nthesis, we take a look at some extensions of cross-validation to unsupervised learning.\nWe focus speciﬁcally on the problem of choosing how many components to keep for\nprincipal component analysis (PCA), but many of the concepts we introduce are more\nbroadly applicable.\nBefore we can do anything, we need a solid theoretical foundation. To this end,\nChapter 2 gives a survey of relevant results from multivariate statistics and ran-\ndom matrix theory.\nThen, Chapter 3 derives the behavior of the singular value\ndecomposition (SVD) for “signal-plus-noise” matrix models. These two chapters are\ncomplemented by Appendices A and B, which collect properties of random orthog-\nonal matrices and give some limit theorems for weighted sums of random variables.\nCollectively, this work provides the groundwork for the rest of the thesis.\nIn Chapter 4, we introduce the latent factor model. This is a generative model\nfor signal-plus-noise matrix data that expands the setup of PCA to include correlated\nfactor loadings. We motivate loss functions for estimating the signal part, and then\nshow how the SVD performs with respect to these criteria.\n1\n2\nCHAPTER 1. INTRODUCTION\nThe next chapter (Chapter 5) focuses on cross-validation strategies. It covers both\nWold-style “speckled” hold-outs as well as Gabriel-style “blocked” hold-outs.\nWe\ndeﬁne model error and prediction error for the latent factor model, and present the\ntwo cross-validation methods as estimators of prediction error. The chapter includes\na comparison of CV methods with parametric model-selection procedures, showing\nthrough simulation that CV is much more robust to violations in model assumptions.\nIn situations where parametric assumptions are unreasonable, cross-validation proves\nto be an attractive method for model selection.\nChapter 6, the ﬁnal chapter, contains a theoretical analysis of Gabriel-style cross-\nvalidation for the SVD, also known as bi-cross-validation (BCV). This chapter shows\nthat BCV is in general a biased estimator of model error, with an explicit expression\nfor the bias. Despite this bias, though, the procedure can still be used successfully for\nmodel selection, provided the leave-out sizes are chosen appropriately. The chapter\nshows how to choose the leave-out sizes and proves a weak form of consistency.\nCross-validation is a valuable and ﬂexible procedure for model selection. Through\ntheory and simulation, this thesis demonstrates the applicability and utility of cross-\nvalidation as applied to principal component analysis.\nMany of the ideas in the\nfollowing chapters generalize to other unsupervised learning procedures. This thesis\nshows that cross-validation can successfully be used for model selection in a variety\nof contexts.\nChapter 2\nMultivariate statistics background\nMultivariate statistics will prove to be a central tool for this thesis. We use this\nchapter to gather the relevant deﬁnitions and results, concentrating mainly on the\neigenvalues and eigenvectors from sample covariance matrices. The literature in this\narea spans over ﬁfty years. We give the basic deﬁnitions and properties of the mul-\ntivariate normal and Wishart distributions in Section 2.1. Then, in Section 2.2, we\nsurvey classical results about sample covariance matrices when the number of di-\nmensions, p, is ﬁxed and the sample size, n, grows to inﬁnity. This material is by\nnow standard and can be found in any good multivariate statistics book (e.g. Muir-\nhead [61]). Lastly, in Section 2.3, we survey modern asymptotics, where n →∞and\np grows with n. Modern multivariate asymptotics is still an active research topic,\nbut today it is possible to give a reasonably-complete description of the objects of\ninterest.\n2.1\nThe multivariate normal and Wishart distri-\nbutions\nWe start with the deﬁnition of the multivariate normal distribution and some basic\nproperties, which can be found, for example, in Muirhead [61][Chapters 1–3].\nDeﬁnition 2.1 (Multivariate Normal Distribution). For mean vector\n¯\nµ ∈Rp and\n3\n4\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\npositive-semideﬁnite covariance matrix Σ ∈Rp×p, a random vector ¯X ∈Rp is said\nto be distributed from the multivariate normal distribution, denoted ¯X ∼N\n\u0000¯\nµ, Σ\n\u0001\nif\nfor every ﬁxed vector ¯a ∈Rp, the vector ¯aT\n¯X has a univariate normal distribution\nwith mean ¯aT\n¯\nµ and variance ¯aTΣ¯a.\nThe multivariate normal distribution is deﬁned for any positive-semideﬁnite covari-\nance matrix Σ, but it only has a density when Σ is strictly positive-deﬁnite.\nProposition 2.2. If ¯X ∈Rp follows a multivariate normal distribution with mean\n¯\nµ\nand positive-deﬁnite covariance matrix Σ, then its components have density\nf(¯x) = (2π)−p/2|Σ|−1/2 exp\n\u0012\n−1\n2(¯x −\n¯\nµ)TΣ−1(¯x −µ)\n\u0013\n.\n(2.1)\nA basic fact about the multivariate normal is the following:\nProposition 2.3. Let ¯a ∈Rp, C ∈Rq×p, and ¯X ∼N\n\u0000¯\nµ, Σ\n\u0001\n. Deﬁne ¯Y = C ¯X + ¯a.\nThen ¯Y ∼N\n\u0000C\n¯\nµ + ¯a, CΣCT\u0001\n.\nTwo immediate corollaries are:\nCorollary 2.4. Suppose that ¯X ∼N (¯0, σ2Ip) and that O ∈Rp×p is an orthogonal\nmatrix. Then O¯X\nd= ¯X.\nCorollary 2.5. If ¯X ∼N (¯0, Σ) and Σ = CCT for a matrix C ∈Rp×p, and if\n¯Z ∼N (¯0, Ip) , then C¯Z\nd= ¯X.\nWe are often interested in estimating the underlying parameters from multivariate\nnormal data. The suﬃcient statistics are the standard estimates.\nProposition 2.6. Say that ¯X1, ¯X2, . . . , ¯Xn are independent draws from a N\n\u0000¯\nµ, Σ\n\u0001\ndistribution. Then the sample mean\n¯\n¯Xn ≡1\nn\nn\nX\ni=1 ¯Xi\n(2.2)\n2.1. THE MULTIVARIATE NORMAL AND WISHART DISTRIBUTIONS\n5\nand the sample covariance\nSn ≡\n1\nn −1\nn\nX\ni=1\n\u0000¯Xi −¯\n¯Xn\n\u0001 \u0000¯Xi −¯\n¯Xn\n\u0001T\n(2.3)\nare suﬃcient statistics for\n¯\nµ and Σ.\nTo describe the distribution of Sn, we need to introduce the Wishart distribution.\nDeﬁnition 2.7 (Wishart Distribution). Let ¯X1, ¯X2, . . . , ¯Xn ∈Rp be an iid sequence\nof random vectors, each distributed as N (¯0, Σ) . Then the matrix\nA =\nn\nX\ni=1 ¯Xi ¯XT\ni\nis said to have the Wishart distribution with n degrees of freedom and scale parameter\nΣ. We denote this by A ∼Wp (n, Σ) .\nWhen n ≥p and Σ is positive-deﬁnite, the elements of a Wishart matrix have a\ndensity.\nProposition 2.8. Suppose that A ∼Wp (n, Σ). If n ≥p and Σ is positive-deﬁnite,\nthen the elements of A have a density over the space of positive-deﬁnite matrices,\ngiven by\nf(A) =\n|A|\nn−p−1\n2\n2\nnp\n2 |Σ|\nn\n2 Γp\n\u0000 n\n2\n\u0001 exp\n\u0012\n−1\n2 tr\n\u0000Σ−1A\n\u0001\u0013\n,\n(2.4)\nwhere Γp (·) is the multivariate gamma function, computed as\nΓp\n\u0010n\n2\n\u0011\n= πp(p−1)/4\np\nY\ni=1\nΓ\n\u0012n + 1 −i\n2\n\u0013\n.\n(2.5)\nWe can now characterize the distributions of the suﬃcient statistics of a sequence\nof iid multivariate normal random vectors.\nProposition 2.9. Let ¯\n¯Xn and Sn be deﬁned as in Proposition 2.6. Then ¯\n¯Xn and Sn\nare independent with ¯\n¯Xn ∼N\n\u0000µ, 1\nnΣ\n\u0001\nand (n −1)Sn ∼Wp (n −1, Σ).\n6\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nWhite Wishart matrices—those with scale parameter Σ = σ2Ip are of particular\ninterest. We can characterize their distribution in terms of eigenvalues and eigenvec-\ntors.\nProposition 2.10. Suppose that A ∼Wp (n, σ2Ip) with n ≥p and let A = nOLOT\nbe the spectral decomposition of A, with L = diag (l1, l2, . . . , lp) and l1 > l2 > · · · >\nlp > 0. Then O and L are independent, with O Haar-distributed over the group of\np × p orthogonal matrices and the elements of L having density\n\u0012 1\n2σ2\n\u0013np/2\nπp2/2\nΓp\n\u0000 n\n2\n\u0001\nΓp\n\u0000 p\n2\n\u0001\np\nY\ni<j\n|li −lj|\np\nY\ni=1\nl(n−p−1)/2\ni\ne−li\n2σ2 .\n(2.6)\nIn the random matrix theory literature, with σ2 = 1 the eigenvalue density above is\nsometimes referred to as the Laguerre Orthogonal Ensemble (LOE).\n2.2\nClassical asymptotics\nIn this section we present results about sample covariance matrices when the sample\nsize, n, tends to inﬁnity, with the number of dimensions, p a ﬁxed constant.\nA\nstraightforward application of the strong law of large numbers gives us the limits of\nthe sample mean and covariance.\nProposition 2.11. Let ¯X1, ¯X2, . . . , ¯Xn be a sequence of iid random vectors in Rp\nwith E [¯X1] =\n¯\nµ and E\nh\u0000¯X1 −\n¯\nµ\n\u0001 \u0000¯X1 −\n¯\nµ\n\u0001Ti\n= Σ. Then, as n →∞,\n¯\n¯Xn ≡1\nn\nn\nX\ni=1 ¯Xi\na.s.\n→\n¯\nµ\nand\nSn ≡\n1\nn −1\nn\nX\ni=1\n\u0000¯Xi −¯\n¯Xn\n\u0001 \u0000¯Xi −¯\n¯Xn\n\u0001T a.s.\n→Σ.\nTo simplify matters, for the rest of the section we will mostly work in a setting\nwhen the variables have been centered. In this case, the sample covariance matrix\n2.2. CLASSICAL ASYMPTOTICS\n7\ntakes the form Sn =\n1\nn\nPn\ni=1 ¯Xi ¯XT\ni . To see that centering the variables does not\nchange the theory in any essential way, we provide the following proposition.\nProposition 2.12. Let ¯X1, ¯X2, . . . , ¯Xn be a sequence of random observations in Rp\nwith mean vector\n¯\nµ and covariance matrix Σ.\nLet ¯\n¯Xn =\n1\nn\nPn\ni=1 ¯Xi and Sn =\n1\nn−1\nPn\ni=1\n\u0000¯Xi −¯\n¯Xn\n\u0001\u0000¯Xi −¯\n¯Xn\n\u0001T be the sample mean and covariance, respectively. De-\nﬁne the centered variables ¯\n˜Xi = ¯Xi −\n¯\nµ. Then\nSn = 1\nn\nn\nX\ni=1 ¯\n˜Xi ¯\n˜XT\ni + OP\n\u00121\nn\n\u0013\n.\nIn particular, this implies that\n√n (Sn −Σ) = √n\n \n1\nn\nn\nX\ni=1 ¯\n˜Xi ¯\n˜XT\ni −Σ\n!\n+ OP\n\u0012 1\n√n\n\u0013\n.\nProof. We can write\nSn =\n1\nn −1\nn\nX\ni=1 ¯\n˜Xi ¯\n˜XT\ni +\nn\nn −1\n\u0000¯\n¯Xn −\n¯\nµ\n\u0001 \u0000¯\n¯Xn −\n¯\nµ\n\u0001T\n=\n\u00121\nn +\n1\nn(n −1)\n\u0013\nn\nX\ni=1 ¯\n˜Xi ¯\n˜XT\ni +\nn\nn −1\n\u0000¯\n¯Xn −\n¯\nµ\n\u0001 \u0000¯\n¯Xn −\n¯\nµ\n\u0001T\nThe result follows since ¯\n˜Xi ¯\n˜XT\ni = OP (1) and ¯\n¯Xn −\n¯\nµ = OP\n\u0010\n1\n√n\n\u0011\n.\nThe next fact follows directly from the multivariate central limit theorem.\nProposition 2.13. Suppose that ¯X1, ¯X2, . . . , ¯Xn is a sequence of iid random vectors\nin Rp with\nE\n\u0002\n¯X1 ¯XT\n1\n\u0003\n= Σ,\nand that for all 1 ≤i, j, i′, j′ ≤p there exists ﬁnite Γiji′j′ with\nE [(X1iX1j −Σij) (X1i′X1j′ −Σi′j′)] = Γiji′j′.\n8\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nIf Sn = 1\nn\nPn\ni=1 ¯Xi ¯XT\ni , then √n vec (Sn −Σ)\nd→vec (G) , where G is a random p × p\nsymmetric matrix with vec (G) a mean-zero multivariate normal having covariance\nCov (Gij, Gi′j′) = Γiji′j′.\nIn the previous proposition, vec is an operator that stacks the columns of an n × p\nmatrix to create an np-dimensional vector.\nIf the elements of ¯X1 have vanishing ﬁrst and third moments (for instance if the\ndistribution of ¯X1 is symmetric about the origin, i.e. ¯X1\nd= −¯X1), and if E\n\u0002\n¯X1 ¯XT\n1\n\u0003\n=\ndiag (λ1, λ2, . . . , λp) , then Γiji′j′ simpliﬁes to\nΓiiii = E\n\u0002\nX4\n1i\n\u0003\n−λ2\ni\nfor 1 ≤i ≤p,\n(2.7a)\nΓijij = Γijji = E\n\u0002\nX2\n1iX2\n1j\n\u0003\nfor 1 ≤i, j ≤p, i ̸= j;\n(2.7b)\nall other values of Γiji′j′ are 0. In particular, this implies that the elements of vec (G)\nare independent. If we also have that ¯X1 is multivariate normal, then\nΓiiii = 2λ2\ni\nfor 1 ≤i ≤p, and\n(2.8a)\nΓijij = Γijji = λiλj\nfor 1 ≤i, j ≤p, i ̸= j.\n(2.8b)\nIt is inconvenient to study the properties of the sample covariance matrix when the\npopulation covariance Σ is not diagonal. By factorizing Σ = ΦΛΦT for orthogonal\nΦ and diagonal Λ, we can introduce ¯\n˜Xi ≡ΦT\n¯Xi to get E\nh\n¯\n˜Xi ¯\n˜XT\ni\ni\n= Λ and Sn =\nΦ\n\u0010\n1\nn\nPn\ni=1 ¯\n˜Xi ¯\n˜XT\ni\n\u0011\nΦT. With this transformation, we can characterize the distribution\nof Sn completely in terms of 1\nn\nPn\ni=1 ¯\n˜Xi ¯\n˜XT\ni .\nThe next result we present is about the sample principal components. It is moti-\nvated by Proposition 2.13 and is originally due to Anderson [1].\nTheorem 2.14. For n →∞and p ﬁxed, let S1, S2, . . . , Sn be a sequence of ran-\ndom symmetric p × p matrices with √n vec (Sn −Λ)\nd→vec (G) , for a determin-\nistic Λ = diag (λ1, λ2, . . . , λp) having λ1 > λ2 > · · · > λp and a random symmet-\nric matrix G.\nLet Sn = U nLnU T\nn be the eigendecomposition of Sn, with Ln =\ndiag (ln,1, ln,2, . . . , ln,p) and ln,1 ≥ln,2 ≥· · · ≥ln,p. If G = OP (1), and the signs of U n\n2.2. CLASSICAL ASYMPTOTICS\n9\nare chosen so that Un,ii ≥0 for 1 ≤i ≤p, then the elements of U n and Ln converge\njointly as\n√n (Un,ii −1)\nP→0\nfor 1 ≤i ≤p,\n(2.9a)\n√nUn,ij\nd→−\nGij\nλi −λj\nfor 1 ≤i, j ≤p with i ̸= j, and\n(2.9b)\n√n (ln,i −λi)\nd→Gii\nfor 1 ≤i ≤p.\n(2.9c)\nMore generally, Anderson treats the case when the λi are not all unique. The key\ningredient to Anderson’s proof is a perturbation lemma, which we state and prove\nbelow.\nLemma 2.15. For n →∞and ﬁxed p let S1, S2, . . . , Sn ∈Rp×p be a sequence of\nsymmetric matrices of the form\nSn = Λ + 1\n√nHn + o\n\u0012 1\n√n\n\u0013\n,\nwhere Λ = diag (λ1, λ2, . . . , λp) with λ1 > λ2 > · · · > λp and Hn = O (1) . Let\nSn = U nLnU T\nn be the eigendecomposition of Sn, with Ln = diag (ln,1, ln,2, . . . , ln,p) .\nFurther suppose that Un,ii ≥0 for 1 ≤i ≤p and ln,1 > ln,2 > · · · > ln,p. Then for all\n1 ≤i, j ≤p and i ̸= j we have\nUn,ii = 1 + o\n\u0012 1\n√n\n\u0013\n,\n(2.10a)\nUn,ij = −1\n√n\nHn,ij\nλi −λj\n+ o\n\u0012 1\n√n\n\u0013\n,\nand\n(2.10b)\nln,i = λi + Hn,ii\n√n + o\n\u0012 1\n√n\n\u0013\n.\n(2.10c)\n10\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nProof. Deﬁne p × p matrices En, F n, and ∆n so that\nEn = diag (Un,11, Un,22, . . . , Un,pp) ,\n(2.11)\nF n = √n (U n −En) ,\n(2.12)\n∆n = √n (Ln −Λ) ,\n(2.13)\ngiving\nU n = En + 1\n√nF n,\nand\nLn = Λ + 1\n√n∆n.\nWe have that\nSn = Λ + 1\n√nHn + o\n\u0012 1\n√n\n\u0013\n= U nLnU T\nn\n= EnΛET\nn + 1\n√n\n\u0000En∆nET\nn + F nΛET\nn + EnΛF T\nn\n\u0001\n+ 1\nnM n\n(2.14)\nwhere the elements of M n are sums of O (p) terms, with each term a product of\nelements taken from En, F n, Λ, and ∆n. Also,\nIp = U nU T\nn\n= EnET\nn + 1\n√n\n\u0000F nET\nn + EnF T\nn\n\u0001\n+ 1\nnW n,\n(2.15)\nwhere W n = F nF T\nn. From (2.15) we see that for 1 ≤i, j ≤p and i ̸= j we must\nhave\n1 = E2\nn,ii + 1\nnWn,ii,\nand\n(2.16a)\n0 = En,iiFn,ji + Fn,ijEn,jj + 1\n√nWn,ij.\n(2.16b)\n2.2. CLASSICAL ASYMPTOTICS\n11\nSubstituting E2\nn,ii = 1 −1\nnWn,ii into equation (2.14), we get\nHn,ii = En,ii∆n,iiEn,ii + 1\n√n (Mn,ii −λiWn,ii) + o (1) ,\nand\n(2.17a)\nHn,ij = λjEn,jjFn,ij + λiFn,jiEn,ii + 1\n√nMn,ij + o (1) .\n(2.17b)\nEquations (2.16a)–(2.17b) admit the solution\nEn,ii = 1 + o\n\u0012 1\n√n\n\u0013\n,\n(2.18a)\nFn,ij = −Hn,ij\nλi −λj\n+ o (1) ,\nand\n(2.18b)\n∆n,ii = Hn,ii + o (1) .\n(2.18c)\nThis completes the proof.\nAn application of the results of this section is the following theorem, which de-\nscribes the behavior of principal component analysis for large n and ﬁxed p.\nTheorem 2.16. Let ¯X1, ¯X2, . . . , ¯Xn be a sequence of iid N\n\u0000¯\nµ, Σ\n\u0001\nrandom vectors\nin Rp, with sample mean ¯\n¯Xn and sample covariance Sn. Let Σ = ΦΛΦT be the\neigendecomposition of Σ, with Λ = diag (λ1, λ2, . . . , λp) and λ1 > λ2 > · · · > λp > 0.\nSimilarly, let Sn = U nLnU T\nn be the eigendecomposition of Sn, likewise with Ln =\ndiag (ln,1, ln,2, . . . , ln,p), ln,1 > ln,2 > · · · > ln,p, and signs chosen so that\n\u0000ΦTU n\n\u0001\nii ≥0\nfor 1 ≤i ≤p. Then\n(i) ln,i\na.s.\n→λi for 1 ≤i ≤p, and U n\na.s.\n→Φ .\n(ii) After appropriate cenering and scaling, {ln,i}p\ni=1 and U n converge jointly in\ndistribution and their limits are independent. For all 1 ≤i ≤p\n√n (ln,i −λi)\nd→N\n\u00000, 2λ2\ni\n\u0001\n,\nand\n√n\n\u0000ΦTU n −Ip\n\u0001\nd→F ,\n12\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nwhere F is a skew-symmetric matrix with elements above the diagonal indepen-\ndent of each other and distributed as\nFij ∼N\n \n0,\nλiλj\n(λi −λj)2\n!\n,\nfor all 1 ≤i < j ≤p.\nProof. Part (i) is a restatement of Proposition 2.11. Part (ii) follows from Proposi-\ntion 2.13 and Theorem 2.15.\n2.3\nModern asymptotics\nWe now present some results about sample covariance matrices when both the sample\nsize, n, and the dimensionality, p, go to inﬁnity. Speciﬁcally, most of these results\nsuppose that n →∞, p →∞, and n\np →γ for a ﬁxed constant γ ∈(0, ∞) . There\nis no widely-accepted name for γ, but we will sometimes adopt the terminology of\nMarˇcenko and Pastur [56] and call it the concentration.\nMost of the random matrix theory literature concerning sample covariance ma-\ntrices is focused on eigenvalues.\nGiven a sequence of sample covariance matrices\nS1, S2, . . . , Sn, with Sn ∈Rp×p and p = p(n) these results generally come in one of\ntwo forms. If we label the eigenvalues of Sn as ln,1, ln,2, . . . , ln,p, with ln,1 ≥ln,2 ≥\n· · · ≥ln,p, then we can deﬁne a random measure\nF Sn = 1\np\np\nX\ni=1\nδln,i.\n(2.19)\nThis measure represents a random draw from the set of eigenvalues of Sn that puts\nequal weight on each eigenvalue. It is called the spectral measure of Sn. Results\nabout F Sn are generally called results about the “bulk” of the spectrum.\nThe second major class of results is concerned with the behavior of the extreme\neigenvalues ln,1 and ln,p. Results of this type are called “edge” results.\n2.3. MODERN ASYMPTOTICS\n13\n2.3.1\nThe bulk of the spectrum\nTo work in a setting where the dimensionality p, grows with the sample size, n, we\nintroduce a triangular array of sample vectors. The sample covariance matrix Sn is\nof dimension p × p and is formed from row n of a triangular array of independent\nrandom vectors, ¯Xn,1, ¯Xn,2, . . . , ¯Xn,n. Speciﬁcally, Sn = 1\nn\nPn\ni=1 ¯Xn,i ¯XT\nn,i. We let Xn\nbe the n × p matrix Xn =\n\u0010\n¯Xn,1\n¯Xn,2\n· · ·\n¯Xn,n\n\u0011T\n, so that Sn = 1\nnXT\nnXn. Most\nasymptotic results about sample covariance matrices are expressed in terms of Xn\nrather than Sn. For example, the next theorem about the spectral measure of a large\ncovariance matrix is stated this way.\nTheorem 2.17. Let X1, X2, . . . , Xn be a sequence of random matrices of increasing\ndimension as n →∞, so that Xn ∈Rn×p and p = p(n). Deﬁne Sn = 1\nnXT\nnXn. If\nthe elements of Xn are iid with E|Xn,11 −EXn,11|2 = 1 and n\np →γ > 0, then the\nempirical spectral measure F Sn almost surely converges in distribution to a determin-\nistic probability measure. This measure, denoted F MP\nγ\n, is called the Marˇcenko-Pastur\nLaw. For γ ≥1 it has density\nf MP\nγ\n(x) =\nγ\n2πx\nq\n(x −aγ)(bγ −x),\naγ ≤x ≤bγ,\n(2.20)\nwhere aγ =\n\u0010\n1 −\n1\n√γ\n\u00112\nand bγ =\n\u0010\n1 +\n1\n√γ\n\u00112\n. When γ < 1, there is an additional\npoint-mass of (1 −γ) at the origin.\nFigure 2.1 shows the density f MP\nγ\n(x) for diﬀerent values of γ. The reason for choosing\nthe name “concentration” to refer to γ becomes apparent in that for larger values of\nγ, F MP\nγ\nbecomes more and more concentrated around its mean.\nThe limiting behavior of the empirical spectral measure of a sample covariance\nmatrix was originally studied by Marˇcenko and Pastur [56] in 1967.\nSince then,\nseveral papers have reﬁned these results, including Grenander and Silverstein [37],\nWachter [92], Jonsson [47], Yin and Krishnaiah [98], Yin [96], Silverstein and Bai [82],\nand Silverstein [81]. These papers either proceed via a combinatorial argument in-\nvolving the moments of the matrix elements, or else they employ a tool called the\nStieltjes transform. Theorem 2.17 is a simpliﬁed version of Silverstein and Bai’s main\n14\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nQuantile\nDensity\n0.0\n0.5\n1.0\n1.5\n0\n2\n4\n6\n8\nConcentration\n4\n1\n0.25\nFigure 2.1: The Marˇcenko-Pastur law. Density, f MP\nγ\n(x), plotted against quan-\ntile, x, for concentration γ = 0.25, 1, and 4. Concentration is equal to the number of\nsamples per dimension. For γ < 1, there is an addition point-mass of size (1 −γ) at\nx = 0.\nresult, which more generally considers complex-valued random variables and allows\nthe columns of Xn to have heterogeneous variances.\nThe meaning of the phrase “F Sn converges almost surely in distribution to F MP\nγ\n”\nis that for all x which are continuity points of F MP\nγ\n,\nF Sn (x)\na.s.\n→F MP\nγ\n(x) .\n(2.21)\nEquivalently, Theorem 2.17 can be stated as a strong law of large numbers.\nCorollary 2.18 (Wishart LLN). Let Xn and {ln,i}p\ni=1 be as in Theorem 2.17. Let\n2.3. MODERN ASYMPTOTICS\n15\ng : R →R be any continuous bounded function. Then\n1\np\np\nX\ni=1\ng (ln,i)\na.s.\n→\nZ\ng(x) dF MP\nγ\n(x).\n(2.22)\nConcerning convergence rates for the quantities in Theorem 2.17, Bai et al. [4]\nstudy the total variation distance between F Sn and F MP\nγ\n. Under suitable conditions\non Xn,11 and γ, they show that ∥F Sn −F MP\nγ\n∥TV = OP\n\u0000n−2/5\u0001\nand that with proba-\nbility one ∥F Sn −F MP\nγ\n∥TV = O\n\u0000n−2/5+ε\u0001\nfor any ε > 0. Guionnet and Zeitouni [39]\ngive concentration of measure results. If Xn,11 satisﬁes a Poincar´e inequality and g is\nLipschitz, they show that for δ large enough,\n−1\nn2 log P\n\u001a\f\f\f\f\nZ\ng(x) dF Sn(x) −\nZ\ng(x) dF MP\nγ\n(x)\n\f\f\f\f > δ\n\u001b\n= O\n\u0000δ2\u0001\n,\n(2.23)\nwith an explicit bound on the error. If one is willing to assume that the elements\nof Xn are Gaussian, then Hiai and Petz [41] give an exact value for the quantity in\n(2.23). Guionnet [38] gives a survey of other large deviations results.\nIt is interesting to look at the scaled behavior in equation (2.22) when the quan-\ntities are scaled by p. Indeed one can prove a Central Limit Theorem (CLT) for\nfunctionals of eigenvalues.\nTheorem 2.19 (Wishart CLT). Let X1, X2, . . . , Xn be a sequence of random n × p\nmatrices with p = p(n). Assume that Xn has iid elements and that E [Xn,11] = 0,\nE\n\u0002\nX2\nn,11\n\u0003\n= 1, and E\n\u0002\nX4\nn,11\n\u0003\n< ∞. Deﬁne Sn = 1\nnXT\nnXn and let F Sn be its spectral\nmeasure. If n →∞, n\np →γ and g1, g2, . . . , gk are real-valued functions analytic on\nthe support of F MP\nγ\n, then the sequence of random vectors\np ·\n\u0010 Z\ng1(x) dF Sn(x) −\nZ\ng1(x) dF MP(x),\nZ\ng2(x) dF Sn(x) −\nZ\ng2(x) dF MP(x), . . . ,\nZ\ngk(x) dF Sn(x) −\nZ\ngk(x) dF MP(x)\n\u0011\n16\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nis tight. Moreover, if E\n\u0002\nX4\nn,11\n\u0003\n= 3, then the sequence converges in distribution to a\nmultivariate normal with mean\n¯\nµ and covariance Σ, where\nµi = gi(aγ) + gi(bγ)\n4\n−1\n2π\nZ bγ\naγ\ngi(x)\np\n(x −aγ)(bγ −x)\ndx\n(2.24)\nand\nΣij = −1\n2π2\nZZ\ngi(z1)gj(z2)\n(m(z1) −m(z2))2\nd\ndz1\nm(z1) d\ndz2\nm(z2)dz1dz2.\n(2.25)\nThe integrals in (2.25) are contour integrals enclosing the support of F MP\nγ\n, and\nm(z) = −(z + 1 −γ−1) +\np\n(z −aγ)(z −bγ)\n2z\n,\n(2.26)\nwith the square root deﬁned to have positive imaginary part when ℑz > 0.\nThe case when E\n\u0002\nX4\nn,11\n\u0003\n= 3 is of particular interest because it arises when Xn,11 ∼\nN (0, 1).\nThis theorem was proved by Bai and Silverstein [6], and can be considered a\ngeneralization of the work by Johnsson [47]. In computing the variance integral (2.25),\nit is useful to know that m satisﬁes the identities\nm(¯z) = m(z),\nand\nz = −\n1\nm(z) +\nγ−1\nm(z) + 1.\nBai and Silverstein show how to compute the limiting means and variances for g(x) =\nlog x and g(x) = xr. They also derive a similar CLT when the elements of Xn are\ncorrelated. Pastur and Lytova [66] have recently relaxed some of the assumptions\nmade by Bai and Silverstein.\n2.3. MODERN ASYMPTOTICS\n17\n2.3.2\nThe edges of the spectrum\nWe now turn our attention to the extreme eigenvalues of a sample covariance matrix.\nIt seems plausible that if F Sn\nd→F MP\nγ\n, then the extreme eigenvalues of Sn should\nconverge to the edges of the support of F MP\nγ\n. Indeed, under suitable assumptions,\nthis is exactly what happens. For the largest eigenvalue, work on this problem started\nwith Geman [34], and his assumptions were further weakened by Jonsson [48] and Sil-\nverstein [76]. Yin et al. [97] prove the result under the weakest possible conditions [7].\nTheorem 2.20. Let X1, X2, . . . , Xn be a sequence of random matrices of increasing\ndimension, with Xn of size n × p, p = p(n), n →∞, and n\np →γ ∈(0, ∞). Let\nSn = 1\nnXT\nnXn and denote its eigenvalues by ln,1 ≥ln,2 ≥· · · ≥ln,p. If the elements\nof Xn are iid with EXn,11 = 0, EX2\nn,11 = 1, and EX4\nn,11 < ∞, then\nln,1\na.s.\n→\n\u00001 + γ−1/2\u00012 .\nFor the smallest eigenvalue, the ﬁrst work was by Silverstein [78], who gives a result\nwhen Xn,11 ∼N (0, 1). Bai and Yin [10] proved a theorem that mirrors Theorem 2.20.\nTheorem 2.21. Let Xn, n, p, and {ln,i}p\ni=1 be as in Theorem 2.20. If EX4\nn,11 < ∞\nand γ ≥1, then\nln,p\na.s.\n→\n\u00001 −γ−1/2\u00012 .\nWith the same moment assumption on Xn,11, if 0 < γ < 1, then\nln,p−n+1\na.s.\n→\n\u00001 −γ−1/2\u00012 .\nFor the case when the elements of Xn are correlated, Bai and Silverstein [5] give a\ngeneral result that subsumes Theorems 2.20 and 2.21.\nAfter appropriate centering and scaling, the largest eigenvalue of a white Wishart\nmatrix converges weakly to a random variable with known distribution.\nJohans-\nson [44] proved this statement and identiﬁed the limiting distribution for complex\nwhite Wishart matrices. Johnstone [45] later provided an analogous result for real\nmatrices, which we state below.\n18\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nTheorem 2.22. Let X1, X2, . . . , Xn be a sequence of random matrices of increasing\ndimension, with Xn ∈Rn×p, p = p(n), and n →∞with n\np →γ ∈(0, ∞). Deﬁne\nSn = 1\nnXT\nnXn and label its eigenvalues ln,1 ≥ln,2 ≥· · · ≥ln,p. If the elements of Xn\nare iid with Xn,11 ∼N (0, 1) , then\nln,1 −µn,p\nσn,p\nd→W1 ∼F TW\n1\n,\nwhere\nµn,p = 1\nn\n\u0010p\nn −1/2 +\np\np −1/2\n\u0011\n,\nσn,p = 1\nn\n\u0010p\nn −1/2 +\np\np −1/2\n\u0011  \n1\np\nn −1/2\n+\n1\np\np −1/2\n!1/3\n,\nand F TW\n1\nis the Tracy-Widom law of order 1.\nEl Karoui [28] extended this result to apply when γ = 0 or γ = ∞. With appropriate\nmodiﬁcations to µn,p and σn,p, he later gave a convergence rate of order (n ∧p)2/3\nfor complex-valued data [29]. Ma [53] gave the analogous result for real-valued data.\nFor correlated complex normals, El Karoui [30] derived a more general version of\nTheorem 2.22.\nThe Tracy-Widom distribution, which appears in Theorem 2.22, was established\nto be the limiting distribution (after appropriate scaling) of the maximum eigenvalue\nfrom an n × n symmetric matrix with independent entries distributed as N (0, 2)\nalong the main diagonal and N (0, 1) otherwise [89] [90]. To describte F TW\n1\n, let q(x)\nsolve the Painlev´e II equation\nq′′(x) = xq(x) + 2q3(x),\nwith boundary condition q(x) ∼Ai(x) as x →∞and Ai(x) the Airy function. Then\nit follows that\nF TW\n1\n(x) = exp\n\u001a\n−1\n2\nZ ∞\ns\nq(x) + (x −s)q2(x) dx\n\u001b\n.\n2.3. MODERN ASYMPTOTICS\n19\nQuantile\nDensity\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n−4\n−2\n0\n2\n4\nFigure 2.2:\nThe Tracy-Widom law. Limiting density of the largest eigenvalue\nfrom a white Wishart matrix after appropriate centering and scaling.\nHastings and McLeod [40] study the tail behavior of q(x). Using their analysis, one\ncan show (see, e.g. [70]) that for s →−∞,\nF TW\n1\n(s) ∼exp\n\u0012\n−|s|3\n24\n\u0013\n,\nwhile for s →∞,\n1 −F TW\n1\n(s) ∼s−3/4\n4√π exp\n\u0012\n−2\n3s3/2\n\u0013\n.\nThe density of F TW\n1\nis shown in Figure 2.2.\nA result like Theorem 2.22 holds true for the smallest eigenvalue. We deﬁne the\nReﬂected Tracy-Widom Law to have distribution function GTW\n1\n(s) = 1 −F TW\n1\n(−s).\nThen we have\n20\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nTheorem 2.23. With the same assumptions as in Theorem 2.22, if γ ∈(1, ∞) then\nln,p −µ−\nn,p\nσ−\nn,p\nd→W1 ∼GTW\n1\n,\nwhere\nµ−\nn,p = 1\nn\n\u0010p\nn −1/2 −\np\np −1/2\n\u0011\n,\nσ−\nn,p = 1\nn\n\u0010p\nn −1/2 −\np\np −1/2\n\u0011  \n1\np\np −1/2\n−\n1\np\nn −1/2\n!1/3\n.\nIf γ ∈(0, 1), then\nln,p−n+1 −µ−\np,n\nσ−\np,n\nd→W1 ∼GTW\n1\n.\nWe get the result for γ ∈(0, 1) by reversing the role of n and p. Baker et al [13] proved\nthe result for complex data. Paul [67] extended the result to real data when γ →∞.\nMa [53] gives convergence rates. In practice, log ln,p converges in distribution faster\nthan ln,p. Ma recommends appropriate centering and scaling constants for log ln,p to\nconverge in distribution to a GTW random variable at rate (n ∧p)2/3.\nTheorem 2.23 does not apply when n\np →1. Edelman [26] derived the limiting\ndistribution of the smallest eigenvalue when n = p. It is not known if his result holds\nmore generally when n\np →1.\nTheorem 2.24. Let Xn and {ln,i}p\ni=1 be as in Theorem 2.22. If p(n) = n, then for\nt ≥0,\nP {n ln,p ≤t} →\nZ t\n0\n1 + √x\n√x\ne−(x/2+√x) dx.\nIn addition to the extreme eigenvalues, it is possible to study the joint distribution\nof top or bottom k sample eigenvalues for ﬁxed k as n →∞. In light of Theorem 2.17,\nfor ﬁxed k we must have that the top (respectively, bottom) sample eigenvalues\nconverge almost surely to the same limit. Furthermore, Soshnikov [83] showed that\napplying the centering and scaling from Theorem 2.22 to the top k sample eigenvalues\ngives a speciﬁc limiting distribution.\n2.3. MODERN ASYMPTOTICS\n21\nIt is natural to ask if the limiting eigenvalue distributions are speciﬁc to Wishart\nmatrices, or if they apply to non-Gaussian data as well. There is compelling evidence\nthat the Tracy-Widom law is universal. Soshnikov [83] extended Theorem 2.22 to\nmore general Xn under the assumption that Xn,11 is sub-Gaussian and n −p =\nO(p1/3). P´ech´e [69] later removed the restriction on n −p. Tao and Vu [87] showed\nthat Theorem 2.24 applies for general Xn,11 with EXn,11 = 0 and EX2\nn,11 = 1.\n2.3.3\nEigenvectors\nRelatively little attention has been focused on the eigenvectors of sample covariance\nmatrices. While many results are known, as of yet there is no complete character-\nization of the eigenvectors from a general sample covariance matrix. Most of the\ndiﬃculty in tackling the problem is that it is hard to describe convergence properties\nof U n, the p × p matrix of eigenvectors, when n and p go to inﬁnity. The individual\np2 elements of U n do not converge in any meaningful way, so the challenge is to come\nup with relevant macroscopic characteristics of U n.\nSilverstein [74] was perhaps the ﬁrst to study the eigenvectors of large-dimensional\nsample covariance matrices. He hypothesized that for sample covariance matrices of\nincreasing dimension, the eigenvector matrix becomes more and more “Haar-like”. A\nrandom matrix U ∈Rp×p is said to be Haar-distributed over the orthogonal group\nif for every ﬁxed p × p orthogonal matrix O, the rotated matrix OU has the same\ndistribution as U. That is, OU\nd= U. Silverstein’s conjecture was that as n →∞, U n\nbehaves more and more like a Haar-distributed matrix. The next theorem displays\none aspect of Haar-like behavior.\nTo state the theorem, we need to deﬁne the extension of a scalar function g : R 7→\nR to symmetric matrix arguments. If S = ULU T is the eigendecomposition of the\nsymmetric matrix S ∈Rp×p, with L = diag(l1, l2, . . . , lp), then we deﬁne\ng (S) = U\n\u0010\ndiag\n\u0000g(l1), g(l2), . . . , g(lp)\n\u0001\u0011\nU T.\nWith this notion, we can state Silverstein’s result.\n22\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nTheorem 2.25. Let X1, X2, . . . , Xn be a sequence of random matrices of increasing\ndimension, with Xn ∈Rn×p, p = p(n), and Xn having iid elements with EXn,11 = 0,\nEX2\nn,11 = 1, and EX4\nn,11 < ∞. Deﬁne Sn = 1\nnXT\nnXn. Let ¯a1,¯a2, . . . ,¯an be a sequence\nof nonrandom unit vectors with ¯an in Rp and let g : R →R be a continuous bounded\nfunction. If n →∞and n\np →γ ∈(0, ∞), then\n¯aT\nng (Sn)¯an\na.s.\n→\nZ\ng(x) dF MP\nγ\n(x).\nSilverstein [74] proves the result for convergence in probability and a speciﬁc class of\nXn. Bai et al. [3] strengthen the result to a larger class of Xn and proves almost-sure\nconvergence. They also consider dependence in Xn.\nIt may not be immediately obvious how Theorem 2.25 is related to eigenvectors.\nIf Sn = U nLnU n is the eigendecomposition of Sn, with Ln = diag (ln,1, ln,2, . . . , ln,p) ,\nthen g(Ln) = diag\n\u0000g(ln,1), g(ln,2), . . . , g(ln,p)\n\u0001\nand g(Sn) = U ng(Ln)U T\nn. We let ¯bn =\nU n¯an. Then,\n¯aT\nng(Sn)¯an =\np\nX\ni=1\nb2\nn,i g(ln,i).\n(2.27)\nIf U n is Haar-distributed, then ¯bn will be distributed uniformly over the unit sphere in\nRp, and the average in (2.27) will put about weight 1\np on each eigenvalue. If U n puts\nbias in any particular direction then the average will put extra weight on particular\neigenvalues.\nSilverstein investigated second-order behavior of eigenvectors in [75], [77], [79],\nand [80]. He demonstrated that certain second-order behavior of U n depends in a\ncrucial way on the fourth moment of Xn,11. This greatly restricts the class of Xn for\nwith the eigenvectors of Sn are Haar-like.\nTheorem 2.26. Let Xn, Sn, and ¯an be as in Theorem 2.25.\nSuppose also that\nEX4\nn,11 = 3. Let g1, g2, . . . , gk be real-valued functions analytic on the support of F MP\nγ\n.\n2.3. MODERN ASYMPTOTICS\n23\nThen, the random vector\n√p ·\n\u0012\n¯aT\nng1(Sn)¯an −\nZ\ng1(x) dF MP\nγ\n(x),\n¯aT\nng2(Sn)¯an −\nZ\ng2(x) dF MP\nγ\n(x), . . . ,\n¯aT\nngk(Sn)¯an −\nZ\ngk(x) dF MP\nγ\n(x)\n\u0013\nconverges in distribution to a mean-zero multivariate normal with covariance between\nthe ith and jth components equal to\nZ\ngi(x)gj(x) dF MP\nγ\n(x) −\nZ\ngi(x) dF MP\nγ\n(x) ·\nZ\ngj(x) dF MP\nγ\n(x).\nBai et al. [3] give a similar result for complex-valued and correlated Xn.\nSilver-\nstein [79] showed that if g1(x) = x and g2(x) = x2, then the condition EX4\nn,11 = 3 is\nnecessary for the random vector in Theorem 2.26 to converge in distribution for all\n¯an. However, for the speciﬁc choice of ¯an =\n\u0010\n1\n√p,\n1\n√p, . . . ,\n1\n√p\n\u0011\n, he later showed that\nthe conclusions of Theorem 2.26 hold more generally when Xn,11 is symmetric and\nEX4\nn,11 < ∞[80].\n24\nCHAPTER 2. MULTIVARIATE STATISTICS BACKGROUND\nChapter 3\nBehavior of the SVD in low-rank\nplus noise models\n3.1\nIntroduction\nMany modern data sets involve simultaneous measurements of a large number of\nvariables.\nSome ﬁnancial applications, such as portfolio selection, involve looking\nat the market prices of hundreds or thousands of stocks and their evolution over\ntime [57]. Microarray studies involve measuring the expression levels of thousands\nof genes simultaneously [73]. Text processing involves counting the appearances of\nthousands of words on thousands of documents [55]. In all of these applications, it is\nnatural to suppose that even though the data are high-dimensional, their dynamics\nare driven by a relatively small number of latent factors.\nUnder the hypothesis that one ore more latent factors explain the behavior of a\ndata set, principal component analysis (PCA) [46] is a popular method for estimating\nthese latent factors. When the dimensionality of the data is small relative to the sam-\nple size, Anderson’s 1963 paper [1] gives a complete treatment of how the procedure\nbehaves. Unfortunately, his results do not apply when the sample size is comparable\nto the dimensionality.\nA further complication with many modern data sets is that it is no longer appro-\npriate to assume the observations are iid. Also, sometimes it is diﬃcult to distinguish\n25\n26\nCHAPTER 3. BEHAVIOR OF THE SVD\nbetween “observation” and “variable”. We call such data “transposable”. A microar-\nray study involving measurements of p genes for n diﬀerent patients can be considered\ntransposable: we can either treat each gene as a measurement of the patient, or we can\ntreat each patient as a measurement of the gene. There are correlations both between\nthe genes and between the patients, so in fact both interpretations are relevant [27].\nOne can study latent factor models in a transpose-agnostic way by considering\ngenerative models of the form X = UDV T + E. Here, X is the n × p observed data\nmatrix. The unobserved row and column factors are given by U and V , respectively,\nmatrices with k orthonormal columns each, and k ≪min(n, p). The strengths of\nthe factors are given in the k × k diagonal matrix D, and E is a matrix of noise. A\nnatural estimator for the latent factor term UDV T can be constructed by truncating\nthe singular value decomposition (SVD) [36] of X. The goal of this paper is to study\nthe behavior of the SVD when n and p both tend to inﬁnity, with their ratio tending\nto a nonzero constant.\nIn an upcoming paper, Onatski [64] gives a thorough treatment of latent factor\nmodels. He assumes that the elements of E are iid Gaussians, that √n(U TU −Ik)\ntends to a multivariate Gaussian distribution, and that V and D are both non-\nrandom.\nThe contributions of this chapter are twofold.\nFirst, we work under a\ntranspose-agnostic generative model that allows randomness in all three of U, D,\nand V . Second, we give a more complete picture of the almost-sure limits of the\nsample singular vectors, taking into account the signs of the dot products between\nthe population and sample vectors.\nWe describe the main results in Section 3.2. Sections 3.3–3.8 are dedicated to\nproving the two main theorems. Finaly, we discuss related work and extensions in\nSection 3.9. We owe a substantial debt to Onatski’s work. Although most of the\ndetails below are diﬀerent, the general outline and the main points of the argument\nare the same.\n3.2. ASSUMPTIONS, NOTATION, AND MAIN RESULTS\n27\n3.2\nAssumptions, notation, and main results\nHere we make explicit what the model and assumptions are, and we present our main\nresults.\n3.2.1\nAssumptions and notation\nWe will work sequences of matrices indexed by n, with\nXn = √n U nDnV T\nn + En.\n(3.1)\nWe denote by √n U nDnV T\nn the “signal” part of the matrix Xn and En the “noise”\npart. We will often refer to U n and V n as the left and right factors of Xn, and the\nmatrix Dn will be called the matrix of normalized factor strengths. The ﬁrst two\nassumptions concern the signal part:\nAssumption 3.1. The factors U n and V n are random matrices of dimensions n×k\nand p × k, respectively, normalized so that U T\nnU n = V T\nnV n = Ik. The number of\nfactors, k, is a ﬁxed constant. The aspect ratio satisﬁes n\np = γ + o\n\u0010\n1\n√n\n\u0011\nfor a ﬁxed\nnonzero constant γ ∈(0, ∞).\nAssumption 3.2. The matrix of factor strengths, Dn, is of size k × k and diagonal,\nwith Dn = diag (dn,1, dn,2, . . . , dn,k) and dn,1 > dn,2 > · · · > dn,k > 0. The matrix Dn\nconverges almost surely to a deterministic matrix D = diag(µ1/2\n1 , µ1/2\n2 , . . . , µ1/2\nk ) with\nµ1 > µ2 > · · · > µk > 0. Moreover, the vector √n(d2\nn,1 −µ1, d2\nn,1 −µ2, . . . , d2\nn,K −µk)\nconverges in distribution to a mean-zero multivariate normal with covariance matrix\nΣ having entries Σij = σij (possibly degenerate).\nThe next assumption concerns the noise part:\nAssumption 3.3. The noise matrix En is an n × p matrix with entries En,ij inde-\npendent N(0, σ2) random variables, also independent of U n, Dn, and V n.\nFor analyzing the SVD of Xn, we need to introduce some more notation. We\ndenote the columns of U n and V n by ¯un,1,¯un,2, . . . ,¯un,k and ¯vn,1,¯vn,2, . . . ,¯vn,k, respec-\ntively. We let √n ˆU n ˆ\nDn ˆV\nT\nn be the singular value decomposition of Xn truncated\n28\nCHAPTER 3. BEHAVIOR OF THE SVD\nto k terms, where ˆ\nDn = diag(ˆµ1/2\nn,1, ˆµ1/2\nn,2, . . . , ˆµ1/2\nn,k) and the columns of ˆU and ˆV are\ngiven by ¯ˆun,1,¯ˆun,2 . . . ,¯ˆun,k and ¯ˆvn,1,¯ˆvn,2, . . . ,¯ˆvn,k, respectively.\n3.2.2\nMain results\nWe are now in a position to say what the results are. There are two main theorems, one\nconcerning the sample singular values and the other concerning the sample singular\nvectors. First we give the result about the singular values.\nTheorem 3.4. Under Assumptions 3.1 – 3.3, the vector ˆ\n¯\nµn = (ˆµn,1, ˆµn,2, . . . , ˆµn,k)\nconverges almost surely to\n¯\n¯µ = (¯µ1, ¯µ2, . . . , ¯µk), where\n¯µi =\n\n\n\n(µi + σ2)\n\u0010\n1 + σ2\nγµi\n\u0011\nwhen µi > σ2\n√γ,\nσ2 \u0010\n1 +\n1\n√γ\n\u00112\notherwise.\n(3.2)\nMoveover, √n(\n¯\nˆµ−\n¯\n¯µ) converges in distribution to a (possibly degenerate) multivariate\nnormal with covariance matrix ¯Σ whose ij element is given by\n¯σij ≡\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nσij\n\u0012\n1 −σ4\nγµ2\ni\n\u0013 \u0012\n1 −σ4\nγµ2\nj\n\u0013\n+ δij2σ2\n\u0012\n2µi + (1 + γ−1) σ2\n\u0013\u0012\n1 −σ4\nγµ2\ni\n\u0013\nwhen µi, µj > σ2\n√γ,\n0\notherwise.\n(3.3)\nWhen σii = 2µ2\ni , and µi >\nσ2\n√γ, the variance of the ith component simpliﬁes to ¯σii =\n2(µi + σ2)2 \u0010\n1 −σ4\nγµ2\ni\n\u0011\n.\nNext, we give the result for the singular vectors:\nTheorem 3.5. Suppose Assumptions 3.1 – 3.3 hold. Then the k × k matrix Θn ≡\nV T\nn ˆV n converges almost surely to a matrix Θ = diag(θ1, θ2, . . . , θk), where\nθ2\ni =\n\n\n\n\u0010\n1 −σ4\nγµ2\ni\n\u0011 \u0010\n1 + σ2\nγµi\n\u0011−1\nwhen µi > σ2\n√γ,\n0\notherwise.\n(3.4)\n3.2. ASSUMPTIONS, NOTATION, AND MAIN RESULTS\n29\nAlso, the k × k matrix Φn ≡U T\nn ˆU n converges almost surely to a matrix Φ =\ndiag(ϕ1, ϕ2, . . . , ϕk), where\nϕ2\ni =\n\n\n\n\u0010\n1 −σ4\nγµ2\ni\n\u0011 \u0010\n1 + σ2\nµi\n\u0011−1\nwhen µi > σ2\n√γ,\n0\notherwise.\n(3.5)\nMoreover, θi and ϕi almost surely have the same sign.\n3.2.3\nNotes\nSome discussion of the assumptions and the results are in order:\n1. Assumptions 3.1 and 3.2 are simpler than the assumptions given in many other\npapers while still being quite general. For example, Paul’s “spiked” covariance\nmodel has data of the form\nX = ZΞT + E,\nwhere Ξ is an p×k matrix of factors and Z is an n×k matrix of factor loadings\nwhose rows are iid multivariate N(0, C) random variables for covariance matrx\nC having eigen-decomposition C = QΛQT. Letting Z = √n ˆ\nP ˆΛ\n1/2 ˆ\nQ\nT be the\nSVD of Z, Anderson’s results [1] give us that ˆΛ and ˆ\nQ converge almost surely to\nΛ and Q, respectively, and that the diagonal elements of √n(ˆΛ−Λ) converge to\na mean-zero multivariate normal whenever C has no repeated eigenvalues. If we\ndeﬁne U = ˆ\nP , D = ˆΛ\n1/2, and V = Ξ ˆ\nQ, then X = √n UDV T + E, where the\nfactors satisfy Assumptions 3.1 and 3.2. Dropping the normality assumption on\nthe rows of Z poses no problem. Moreover, we can suppose instead of iid that\nthe rows of Z are a martingale diﬀerence array with well-behaved low-order\nmoments and still perform a transformation of the variables to get factors of\nthe form we need for Theorems 3.4 and 3.5.\n2. There is a sign-indeterminancy in the sample and population singular vectors.\nWe choose them arbitrarily.\n30\nCHAPTER 3. BEHAVIOR OF THE SVD\n3. If instead of almost-sure convergence, Dn converges in probability to D, then\nthe theorems still hold with\n¯\nˆµn, Θn, and Φn converging in probability.\n4. The assumption that √n (d2\n1 −µ1, d2\n2 −µ2, . . . d2\nk −µk) converges weakly is only\nnecessary for determining second-order behavior; the ﬁrst order results still\nhold without this assumption. If the limiting distribution of the vector of factor\nstrengths √n (d2\n1 −µ1, d2\n2 −µ2, . . . d2\nk −µk) is non-normal, one can still get at\nthe second-order behavior of the SVD through a small adaptation of the proof.\n5. Most of the results in Theorems 3.4 and 3.5 can be gotten from Onatski’s results\n[64]. However, Onatski does not show that √n(ˆµi −¯µi)\nP→0 when µi is below\nthe critical threshold. Furthermore, Onatski proves convergence in probability,\nnot almost sure convergence. Lastly, Onatski does not get at the joint behavior\nbetween Θn and Φn.\n3.3\nPreliminaries\nWithout loss of generality we will assume that σ2 = 1. Until Section 3.8 , we will also\nassume that γ ≥1.\n3.3.1\nChange of basis\nA convenient choice of basis will make it easier to study the SVD of Xn. Deﬁne\nU n,1 = U n, and choose U n,2 so that\n\u0010\nU n,1\nU n,2\n\u0011\nis an orthogonal matrix. Similarly,\nput V n,1 = V n and choose V n,2 so that\n\u0010\nV n,1\nV n,2\n\u0011\nis orthogonal. If we deﬁne\n˜\nEn,ij = U T\nn,iEnV n,j and Xn,ij = U T\nn,iXnV n,j, then in block form,\n \nU T\nn,1\nU T\nn,2\n!\nXn\n\u0010\nV n,1\nV n,2\n\u0011\n=\n √nDn + ˜\nEn,11\n˜\nEn,12\n˜\nEn,21\n˜\nEn,22\n!\n.\n3.3. PRELIMINARIES\n31\nBecause Gaussian white noise is orthogonally invariant, the matrices ˜\nEn,ij are all\nindependent with iid N(0, 1) elements. Let\n˜\nEn,22 = √n\n\u0010\nOn,1\nOn,2\n\u0011  \nΛ1/2\nn\n0\n!\nP T\nn\n(3.6)\nbe the SVD of ˜\nEn,22, with Λn = diag (λn,1, λn,2, . . . λn,p−k) . Note that ˜\nE\nT\nn,22 ˜\nEn,22 ∼\nWp−k (n −k, Ip−k) . Deﬁne\n˜\nXn =\n\n\n\n\nIk\n0\n0\nOT\nn,1\n0\nOT\nn,2\n\n\n\n\n √nDn + ˜\nEn,11\n˜\nEn,12\n˜\nEn,21\n˜\nEn,22\n!  \nIk\n0\n0\nP n\n!\n=\n\n\n\n\n√nDn + En,11\nEn,12\nEn,21\n√nΛ1/2\nn\nEn,31\n0\n\n\n\n,\n(3.7)\nwhere En,11 = ˜\nEn,11, En,12 = ˜\nEn,12P n, En,21 = OT\nn,1 ˜\nEn,21, and En,31 = OT\nn,2 ˜\nEn,31.\nLet ˜U n ˜\nDn ˜V n be the SVD of ˜\nXn, truncated to k terms. Lastly, put the left and right\nsingular vectors in block form as\n˜U n =\n ˜U n,1\n˜U n,2\n!\n(3.8)\nand\n˜V n =\n ˜V n,1\n˜V n,2\n!\n,\n(3.9)\nwhere ˜U n,1 and ˜V n,1 both k × k matrices.\nWe have gotten to ˜\nXn via an orthogonal change of basis applied to Xn.\nBy\ncarefully choosing this basis, we have assured that:\n1. The blocks of ˜\nXn are all independent.\n2. The elements of the matrices En,ij are iid N (0, 1).\n32\nCHAPTER 3. BEHAVIOR OF THE SVD\n3. The elements {nλn,1, nλn,2, . . . , nλn,p−k} are eigenvalues from a white Wishart\nmatrix with n −k degrees of freedom.\n4.\n˜\nXn and Xn have the same singular values. This implies that ˆ\nDn = ˜\nDn.\n5. The left singular vectors of Xn can be recovered from the left singular vectors\nof ˜\nXn via multiplication by an orthogonal matrix. The same holds for the right\nsingular vectors.\n6. The dot product matrix U T\nn ˆU n is equal to ˜U n,1. Similarly, V T\nn ˆV n = ˜V n,1.\nThis simpliﬁed form of the problem makes it much easier to analyze the SVD of Xn.\n3.4\nThe secular equation\nWe set Sn = 1\nn = ˜\nX\nT\nn ˜\nXn. The eigenvalues and eigenvectors of Sn are the squares\nof the singular values of\n1\n√n ˜\nXn and its right singular vectors, respectively. In block\nform, we have\nSn =\n \nSn,11\nSn,12\nSn,21\nSn,22\n!\n,\n(3.10)\nwhere\nSn,11 = D2\nn + 1\n√n\n\u0000DnEn,11 + ET\nn,11Dn\n\u0001\n+ 1\nn\n\u0000ET\nn,11En,11 + ET\nn,21En,21 + ET\nn,31En,31\n\u0001\n,\n(3.11a)\nand\nSn,12 =\n1\n√n\n\u0010\nDnEn,12 + ET\nn,21Λ1/2\nn\n\u0011\n+ 1\nnET\nn,11En,12,\n(3.11b)\nSn,21 =\n1\n√n\n\u0010\nET\nn,12Dn + Λ1/2\nn En,21\n\u0011\n+ 1\nnET\nn,12En,11,\n(3.11c)\nSn,22 = Λn + 1\nnET\nn,12En,12.\n(3.11d)\n3.4. THE SECULAR EQUATION\n33\nNow we study the eigendecomposition of Sn. If ¯v =\n \n¯v1\n¯v2\n!\nis an eigenvector of Sn\nwith eigenvalue µ, then\n \nSn,11\nSn,12\nSn,21\nSn,22\n!  \n¯v1\n¯v2\n!\n= µ\n \n¯v1\n¯v2\n!\n.\nIf µ is not an eigenvalue of Sn,22, then we get\n¯v2 = −(Sn,22 −µIp−k)−1 Sn,21¯v1,\nand\n(3.12a)\n\u0000Sn,11 −µIk −Sn,12 (Sn,22 −µIp−k)−1 Sn,21\n\u0001\n¯v1 = 0.\n(3.12b)\nConversely, if (µ,¯v1) is a pair that solves (3.12b) and ¯v1 ̸= 0, then\n¯v =\n \n¯v1\n−(Sn,22 −µIp−k)−1 Sn,21¯v1\n!\n(3.13)\nis an eigenvector of Sn with eigenvalue µ.\nWe deﬁne\nT n(z) = Sn,11 −zIk −Sn,12 (Sn,22 −zIp−k)−1 Sn,21\n(3.14)\nfn(z,¯x) = T n(z)¯x,\n(3.15)\nand refer to fn(z,¯x) = 0 as the secular equation. This terminology comes from numer-\nical linear algebra, where a secular equation is analogous to a characteristic equation;\nit is an equation whose roots are eigenvalues of a matrix. Typically, secular equations\narise in eigenvalue perturbation problems. The name comes from the fact that they\noriginally appeared studying secular perturbations of planetary orbits. A more stan-\ndard use of the term “secular equation” would involve the equation det T n(z) = 0.\nHowever, for our purposes it is more convenient to work with fn.\n34\nCHAPTER 3. BEHAVIOR OF THE SVD\n3.4.1\nOutline of the proof\nAlmost surely Sn and Sn,22 have no eigenvalues in common, so every eigenvalue-\neigenvector pair of Sn is a solution to the secular equation. To study these solutions,\nwe ﬁrst focus on T n(z).\nIt turns out that when z > (1 + γ−1/2)2, we can ﬁnd a perturbation expansion for\nT n(z). In Section 3.5, we show that for z above this threshold, we can expand\nT n(z) = T 0(z) + 1\n√nT n,1(z),\nwhere T 0(z) is deterministic and T n,1(z) converges in distribution to a matrix-valued\nGaussian process indexed by z. With this expansion, in Section 3.6 we study se-\nquences of solutions to the equation fn(zn,¯xn) = 0. Using a Taylor series expansion\nfor zn > (1 + γ−1/2)2, we write\nzn = z0 + 1\n√nzn,1 + oP\n\u0012 1\n√n\n\u0013\n,\n¯xn = ¯x0 + 1\n√n¯xn,1 + oP\n\u0012 1\n√n\n\u0013\n,\nwhere (z0,¯x0) is the limit of (zn,¯xn) as n →∞and (zn,1,¯xn,1) is the order- 1\n√n ap-\nproximation error.\nIn Section 3.7 we get the singular values and singular vectors of\n1\n√n ˜\nXn. From\nevery solution pair (zn,¯xn) satisfying fn(zn,¯xn) = 0, we can construct an eigenvalue\nand an eigenvector of Sn using equation (3.13). The eigenvalues are squares of sin-\ngular values of\n1\n√n ˜\nXn, and the eigenvectors are right singular vectors. We can get\nthe corresponding left singular vectors by multiplying the right singular vectors by\n˜\nXn and scaling appropriately. For z values above the critical threshold, we use the\nperturbation results of the previous two sections. Below the threshold, we use a more\ndirect approach involving the ﬂuctuations of the top eigenvalues of Λn.\nFinally, in Section 3.8 we show that the results still hold when γ < 1. For parts\nof the proof, we will need some limit theorems for weighted sums from Appendix B.\n3.5. ANALYSIS OF THE SECULAR EQUATION\n35\n3.5\nAnalysis of the secular equation\nWe devote this section to ﬁnding a simpliﬁed formula for T n(z) for certain values of\nz. By a bit of algebra and analysis, we ﬁnd the ﬁrst- and second-order behavior of\nthe secular equation.\nFirst, we employ the Sherman-Morrison-Woodbury formula [36] to get an expres-\nsion for (Sn,22 −zIp−k)−1. As a reminder, this identity states that for matrices A,\nB, and C with compatible dimensions, the following formula for the inverse holds:\n(A + BC)−1 = A−1 −A−1B\n\u0000I + CA−1B\n\u0001−1 CA−1.\nUsing this, we can write\n(Sn,22 −zIp−k)−1 =\n\u0012\n(Λn −zIp−k) + 1\nnET\nn,12En12\n\u0013−1\n= (Λn −zIp−k)−1\n−1\nn (Λn −zIp−k)−1\n· ET\nn,12\n\u0012\nIk + 1\nnEn,12 (Λn −zIp−k)−1 ET\nn,12\n\u0013−1\nEn,12\n· (Λn −zIp−k)−1 .\n(3.16)\nNext, we deﬁne ˜\nDn = Dn +\n1\n√nEn,11, so that\nSn,12\n\u0010\nSn,22 −zIp−k\n\u0011−1\nSn,21\n= 1\nn\n\u0010\n˜\nD\nT\nnEn,12 + ET\nn,21Λ1/2\nn\n\u0011\n· (Sn,22 −zIp−k)−1\n·\n\u0010\nET\nn,12 ˜\nDn + Λ1/2\nn En,21\n\u0011\n.\n(3.17)\nThere are three important terms coming out of equations (3.16) and (3.17) that\ninvolve Λn. These are En,12 (Λn −zIp−k)−1 ET\nn,12, ET\nn,21 (Λn −zIp−k)−1 En,21, and\n36\nCHAPTER 3. BEHAVIOR OF THE SVD\nEn,12 (Λn −zIp−k)−1 Λ1/2\nn En,21. Each term can be written as a weighted sum of outer\nproducts.\nThere is dependence between the weights, but the outer products are iid. For\nexample, with En,12 =\n\u0010\n¯En,12,1\n¯En,12,2\n· · ·\n¯En,12,p−k\n\u0011\n, we have\nEn,12 (Λn −zIp−k)−1 ET\nn,12, =\np−k\nX\nα=1\n1\nλn,α −z · ¯En,12,α ¯ET\nn,12,α.\nFrom the Central Limit Theorem and the Strong Law of Large Numbers we know\nthat\n1\np−k\nPp−k\nα=1 ¯En,12,α ¯ET\nn,12,α\na.s.\n→Ik and that √p −k\n\u0010\n1\np−k\nPp−k\nα=1 ¯En,12,α ¯ET\nn,12,α −Ik\n\u0011\nconverges in distribution to mean-zero symmetric matrix whose elements are jointly\nmultivariate normal. In the limiting distribution, the elements have variance 2 along\nthe diagonal and variance 1 oﬀof it; aside from the matrix being symmetric, the\nunique elements are all uncorrelated. The diﬃculty in analyzing these terms comes\nfrom the dependence between the weights.\nWhen z is in the support of F MP\nγ\n, the weights behave erratically, but other-\nwise they have some nice properties. First of all, Λn is independent of En,12 and\nEn,21. Secondly, the Wishart LLN (Corollary 2.18) and Theorem 2.20 ensure that\nfor z\n>\n(1 + γ−1/2)2,\n1\np−k\nPp−k\nα=1\n1\nλn,α−z\na.s.\n→\nR\n1\nt−zdF MP\nγ\n(t). Moreover, the Wishart\nCLT (Theorem 2.19) guarantees that the error is of size OP( 1\nn). These properties in\ncombination with the limit theorems for weighted sums in Appendix B allow us to\nget the behavior of En,12 (Λn −zIk)−1 ET\nn,12 and its cousins.\nThe function\nm(z) ≡\nZ\n1\nt −zdF MP\nγ\n(t)\n= γ ·\n−(z −1 + γ−1) +\nq\u0000z −bγ\n\u0001\u0000z −aγ\n\u0001\n2z\n(3.18)\nis the Stieltjes transform of F MP\nγ\n, where aγ =\n\u00001 −γ−1/2\u00012 and bγ =\n\u00001 + γ−1/2\u00012 .\nWhen restricted to the complement of the support of F MP\nγ\n, m has a well-deﬁned\n3.5. ANALYSIS OF THE SECULAR EQUATION\n37\ninverse\nz(m) = −1\nm +\n1\n1 + γ−1 m.\n(3.19)\nAlso, m is strictly increasing and convex outside the support of F MP\nγ\n. This function\nappears frequently in the remainder of the chapter.\nLemma 3.6. If z > bγ, then\n1\nnEn,12 (Λn −zIp−k)−1 ET\nn,12\na.s.\n→γ−1m(z) · Ik,\n(3.20a)\n1\nnET\nn,21 (Λn −zIp−k)−1 En,21\na.s.\n→γ−1m(z) · Ik,\n(3.20b)\nand\n1\nnEn,12 (Λn −zIp−k)−1 Λ1/2\nn En,21\na.s.\n→0.\n(3.20c)\nProof. We prove the result for the ﬁrst quantity and the other derivations are analo-\ngous. For each 1 ≤i, j ≤k, we have that\n\u00121\nnEn,12 (Λn −zIp−k)−1 ET\nn,12\n\u0013\nij\n= p −k\nn\n·\n1\np −k\np−k\nX\nα=1\n(En,12)iα (En,12)jα\nλn,α −z\n.\nLet N = p−k, deﬁne weights WN,α = (λn,α−z)−1, and let YN,α = (En,12)iα (En,12)jα .\nThe function\ng(t) =\n\n\n\n1\nt−z\nif t ≤bγ,\n1\nbγ−z\notherwise,\nis bounded and continuous. Moreover, since λn,1\na.s.\n→bγ, with probability one g(λn,α)\nis eventually equal to WN,α for all α. The Wishart LLN (Corollary 2.18) gives us\nthat\n1\nN\nPN\nα=1 WN,α\na.s.\n→\nR\n1\nt−zdF MP\nγ\n(t) = m(z). Since |WN,α| ≤WN,1\na.s.\n≤bγ, the fourth\nmoments of the weights are uniformly bounded in N. The YN,α are all iid with EYN,α =\nδij and EY 4\nN,α < ∞. Applying these results, the weighed SLLN (Proposition B.2) gives\nus that 1\nN\nPN\nα=1 WN,αYN,α\na.s.\n→m(z)δij. Since p−k\nn\n→γ−1, this completes the proof.\nLemma 3.7. Considered as functions of z, the quantities in Lemma 3.6 and their\nderivatives converge uniformly over any closed interval [u, v] ⊂\n\u0000bγ, ∞\n\u0001\n.\n38\nCHAPTER 3. BEHAVIOR OF THE SVD\nProof. We show this for the ﬁrst quantity and the other proofs are similar. Deﬁning\nen,ij(z) =\n\u00121\nnEn,12 (Λn −zIp−k)−1 ET\nn,12\n\u0013\nij\n,\nwe will show that for all (i, j) with 1 ≤i, j ≤k, supz∈[u,v] |en,ij(z) −γ−1m(z)δij|\na.s.\n→0.\nLet ε > 0 be arbitrary. Note that for z > bγ, m′(z) > 0 and m′′(z) < 0. We let\nd = m′(u) and choose a grid u = z1 < z2 < · · · < zM−1 < zM = v, with |zl−zl+1| = γε\n2d.\nThen |γ−1m(zl) −γ−1m(zl+1)| < ε\n2. From Lemma 3.6, we can ﬁnd N large enough\nsuch that for n > N, maxl∈{1,...,M} |en,ij(zl) −γ−1m(zl)δij| < ε\n2. Also guarantee that N\nis large enough so that λn,1 < u (this is possible since λn,1\na.s.\n→bγ < u). Let z ∈[u, v]\nbe arbitrary and ﬁnd l such that zl ≤z ≤zl+1. Observe that en,ij(z) is monotone\nfor z > λn,1. Thus, either en,ij(zl) ≤en,ij(z) ≤en,ij(zl+1) or en,ij(zl+1) ≤en,ij(z) ≤\nen,ij(zl).\nIf i ̸= j, we have for n > N, −ε\n2 < en,ij(z) < ε\n2. Otherwise, when i = j, en,ij(z)\nis monotonically increasing and γ−1m(zl) −ε\n2 < en,ij(z) < γ−1m(zl+1) + ε\n2, so that\nγ−1m(z) −ε < en,ij(z) < γ−1m(z) + ε. In either case, |en,ij(z) −γ−1m(z)δij| < ε.\nSince\nd\ndz\n\u0002\n1\nλ−z\n\u0003\n= −\n1\n(λ−z)2, which is monotone for z > λ, the same argument applies\nto show that the derivatives converge uniformly.\nLemma 3.8. If z1, z2, . . . , zl > bγ, then jointly for z ∈{z1, z2, . . . , zl}\n√n\n\u00121\nnEn,12 (Λn −zIp−k)−1 ET\nn,12 −γ−1m(z)Ik\n\u0013\n≡F n(z)\nd→F (z),\n(3.21a)\n√n\n\u00121\nnEn,12 (Λn −zIp−k)−1 Λ1/2\nn En,21\n\u0013\n≡Gn(z)\nd→G(z),\n(3.21b)\n√n\n\u00121\nnET\nn,21 (Λn −zIp−k)−1 En,21 −γ−1m(z)Ik\n\u0013\n≡Hn(z)\nd→H(z),\n(3.21c)\nwhere the elements of F (z), G(z), and H(z) jointly deﬁne a multivariate Gaussian\n3.5. ANALYSIS OF THE SECULAR EQUATION\n39\nprocess indexed by z, with each matrix independent of the others. The nonzero co-\nvariances are deﬁned by\nCov\n\u0010\nFij(z1), Fij(z2)\n\u0011\n= γ−1 (1 + δij) m(z1) −m(z2)\nz1 −z2\n,\n(3.22a)\nCov\n\u0010\nGij(z1), Gij(z2)\n\u0011\n= γ−1 z1 m(z1) −z2 m(z2)\nz1 −z2\n,\n(3.22b)\nCov\n\u0010\nHij(z1), Hij(z2)\n\u0011\n= γ−1 (1 + δij) m(z1) −m(z2)\nz1 −z2\n,\n(3.22c)\nwith the interpretation when z1 = z2 that\nm(z1) −m(z2)\nz1 −z2\n= m′(z1),\nand\nz1 m(z1) −z2 m(z2)\nz1 −z2\n= m(z1) + z1 m′(z1).\nProof. We will need the Wishart CLT (Theorem 2.19) and the strong weighted mul-\ntivariate CLT (Corollary B.5). To save space we only give the argument for the joint\ndistribution of F (z1) and F (z2).\nPut N = p −k and consider the 2k2-dimensional vector ¯YN,α =\n \n¯\n˜YN,α\n¯\n˜YN,α\n!\n, where\n¯\n˜YN,α = vec\n\u0000¯En,12,α ¯ET\nn,12,α\n\u0001\nand En,12 =\n\u0010\n¯En,12,1\n¯En,12,2\n· · ·\n¯En,12,N\n\u0011\n. Deﬁne the\n2k2-dimensional weight vector ¯WN,α =\n \n¯WN,α,1\n¯WN,α,2\n!\n, where ¯WN,α,i =\n1\nλα−zi¯1. We have\nthat for α = 1, 2, . . . , N, the ¯YN,α are iid with\nE [¯YN,1] =\n¯\nµY =\n \n¯\n˜µY\n¯\n˜µY\n!\nand\n¯\n˜µY = vec (Ik). Also, we have\nE\nh\u0000¯YN,1 −\n¯\nµY \u0001 \u0000¯YN,1 −\n¯\nµY \u0001Ti\n= ΣY =\n ˜Σ\nY\n˜Σ\nY\n˜Σ\nY\n˜Σ\nY\n!\n,\nwhere ˜Σ\nY = E\nh\u0010\nvec\n\u0000¯En,12,1¯ET\nn,12,1 −Ik\n\u0001\u0011\u0010\nvec\n\u0000¯En,12,1¯ET\nn,12,1 −Ik\n\u0001\u0011Ti\nis a k2 × k2\n40\nCHAPTER 3. BEHAVIOR OF THE SVD\nmatrix deﬁned by the relation\nE\nh\u0000¯En,12,1¯ET\nn,12,1 −Ik\n\u0001\nij\n\u0000¯En,12,1¯ET\nn,12,1 −Ik\n\u0001\ni′j′\ni\n= δ(i,j)=(i′,j′) + δ(i,j)=(j′,i′).\nThat is, the diagonal elements of ¯En,12,1¯ET\nn,12,1 −Ik have variance 2 and the oﬀ-\ndiagonal elements have variance 1.\nAside from the matrix being symmetric, the\nunique elements are all uncorrelated.\nLetting\nµW\ni\n=\nZ dF MP\nγ\n(t)\nt −zi\n= m(zi),\nand\nσW\nij =\nZ\ndF MP\nγ\n(t)\n(t −zi)(t −zj) = m(zi) −m(zj)\nzi −zj\n,\nthe Wishart LLN combined with the truncation argument of Lemma 3.6 gives us that\n1\nN\nPN\nα=1\n1\nλn,α−zi\na.s.\n→µW\ni\nand\n1\nN\nPN\nα=1\n1\n(λn,α−zi)(λn,α−zj)\na.s.\n→σW\nij . Thus,\n1\nN\nN\nX\nα=1 ¯WN,α\na.s.\n→\n¯\nµW =\n \nµW\n1 ¯1\nµW\n2 ¯1\n!\n,\nand\n1\nN\nN\nX\nα=1 ¯WN,α ¯W T\nN,α\na.s.\n→ΣW =\n \nσW\n11¯1¯1T\nσW\n12¯1¯1T\nσW\n21¯1¯1T\nσW\n11¯1¯1T\n!\n.\nMoreover, the Wishart CLT tells us that the error in each of the sums is of size\nOP\n\u0000 1\nN\n\u0001\n.\nAs in Lemma 3.6, the fourth moments of ¯WN,α and ¯YN,α are all well-behaved.\nFinally, we can invoke the strong weighted CLT (Corollary B.5) to get that the\nweighted sum\n√\nN\n\u0010\n1\nN\nPN\nα=1 ¯WN,α • ¯YN,α −\n¯\nµW •\n¯\nµT\u0011\nconverges in distribution to a\nmean-zero multivariate normal with covariance\nΣW • ΣY =\n \nσW\n11 ˜Σ\nσW\n12 ˜Σ\nσW\n21 ˜Σ\nσW\n22 ˜Σ\n!\n.\n3.5. ANALYSIS OF THE SECULAR EQUATION\n41\nThis completes the proof since\n√n vec\n\u00121\nnEn,12 (Λn −ziIp−k)−1 ET\nn,12 −γ−1m(µ)Ik\n\u0013\n= γ−1\nr\nn\np −k ·\n√\nN\n \n1\nN\nN\nX\nα=1 ¯WN,α,i • ¯\n˜YN,α −\n¯\nµW\ni •\n¯\n˜µY\ni\n!\n(with\n¯\nµW\ni\n= µW\ni ¯1), and γ−1q\nn\np−k →γ−1/2.\nRemark 3.9. It is possible to show that the sequences in Lemma 3.8 are tight by\nan argument similar to the one used in [64]. This implies that the convergence is\nuniform in z. For our purposes, we only need that the ﬁnite-dimensional distributions\nconverge.\nWith Lemmas 3.6–3.8, we can get a perturbation expansion of T n(z) for z > bγ.\nLemma 3.10. If [u, v] ⊂\n\u0000bγ, ∞\n\u0001\n, then T n(z)\na.s.\n→T 0(z) uniformly on [u, v], where\nT 0(z) =\n1\n1 + γ−1m(z)D2 +\n1\nm(z)Ik.\n(3.23)\nLemma 3.11. If z > bγ, then\nT n(z) = T 0(z) + 1\n√nT n,1(z),\n(3.24)\nwhere\nT n,1(z) = −\n\u00001 + γ−1m(z)\n\u0001−2 · DF n(z)D\n+\n\u00001 + γ−1m(z)\n\u0001−1\n·\nn√n\n\u0000D2\nn −D2\u0001\n+ D\n\u0000En,11 −Gn(z)\n\u0001\n+\n\u0000En,11 −Gn(z)\n\u0001TD\no\n−z Hn(z) + √n\n\u00121\nnET\nn,31En,31 −(1 −γ−1)Ik\n\u0013\n+ oP (1) .\n(3.25)\n42\nCHAPTER 3. BEHAVIOR OF THE SVD\nProof. First, we have\nSn,11 = D2\nn + Ik + 1\n√n\n\u0000DEn,11 + ET\nn,11D\n\u0001\n+\n\u00121\nn\n\u0000ET\nn,11En,11 + ET\nn,21En,21 + ET\nn,31En,31\n\u0001\n−Ik\n\u0013\n.\nNext, we compute\n \nIk + 1\nnEn,12\n\u0010\nΛn −zIp−k\n\u0011−1\nET\nn,12\n!−1\n=\n\u0012\nIk + γ−1m(z)Ik + 1\n√nF n(z)\n\u0013−1\n=\n\u00001 + γ−1m(z)\n\u0001−1Ik −1\n√n\n\u00001 + γ−1m(z)\n\u0001−2F n(z) + oP\n\u0012 1\n√n\n\u0013\n.\nUsing this, after a substantial amount of algebra we get\nSn,12\n\u0010\nSn,22 −zIp−k\n\u0011−1\nSn,21\n= z m(z) Ik +\nγ−1m(z)\n1 + γ−1m(z)D2\nn\n+ 1\n√n\n(\n1\n1 + γ−1m(z)\n\u0000DGn(z) + GT\nn(z)D\n\u0001\n+\nγ−1m(z)\n1 + γ−1m(z)\n\u0000DEn,11 + ET\nn,11D\n\u0001\n+\n\u0012\n1\n1 + γ−1m(z)\n\u00132\nDF n(z)D + z Hn(z)\n)\n+ 1\nnET\nn,21En,21 + oP\n\u0012 1\n√n\n\u0013\n.\nThe equations for T 0 and T n,1 follow. To simplify the form of T 0, we use the identity\nz ·\n\u0010\n1 + γ−1m(z)\n\u0011\n= −\n1\nm(z) + (1 −γ−1).\n3.6. SOLUTIONS TO THE SECULAR EQUATION\n43\n3.6\nSolutions to the secular equation\nWe will now study the solutions to fn(z,¯x) = 0, deﬁned in equation (3.15). If (z,¯x) is\na solution, then so is (z, α ¯x) for any scalar α. We restrict our attention to solutions\nwith ∥¯x∥2 = 1. We also impose a restriction on the sign of ¯x, namely we require that\nthe component with the largest magnitude is positive, i.e.\nmax\ni\nxi = max\ni\n|xi|.\n(3.26)\n3.6.1\nAlmost sure limits\nFirst we look at the solutions of f0(z,¯x) ≡T 0(z)¯x, the limit of fn(z,¯x) for z > bγ.\nLemma 3.12. Letting ¯k = max{i : µi > γ−1/2}, if µ1, µ2, . . . , µk are all distinct, then\nthere are exactly ¯k solutions to the equation f0(z,¯x) = 0. They are given by\n(¯µi,¯ei),\ni = 1, . . . , ¯k,\nwhere ¯µi is the unique solution\nm(¯µi) =\n1\nµi + γ−1,\nand ¯ei is the ith standard basis vector.\nProof. We have that\nT 0(z) =\n1\n1 + γ−1m(z)D2 +\n1\nm(z)Ik.\nSince this is diagonal, the equation f0(z,¯x) = 0 holds iﬀthe ith diagonal element of\nT 0(z) is zero and ¯x = ¯ei. The ith diagonal is zero when\nµi\n1 + γ−1m(z) +\n1\nm(z) = 0,\n44\nCHAPTER 3. BEHAVIOR OF THE SVD\nequivalently\nm(z) = −\n1\nµi + γ−1.\nNote that m(z) > −\n\u0000γ−1/2 + γ−1\u0001−1 and m′(z) > 0 on\n\u0000bγ, ∞\n\u0001\n. Hence, a unique\nsolution exists exactly when µi > γ−1/2.\nGiven a solution of f0(z,¯x) = 0, it is not hard to believe that there is a sequence\nof solutions (zn,¯xn) such that fn(zn,¯xn) = 0, with zn and ¯xn converging to z and ¯x,\nrespectively. We dedicate the rest of this section to making this statement precise.\nLemma 3.13. If µ > γ−1/2 occurs l times on the diagonal of D2, then with probability\none there exist sequences zn,1, . . . , zn,l such that for n large enough:\n1. zn,i ̸= zn,j for i ̸= j\n2. det T n(zn,i) = 0 for i = 1, . . . , l.\n3. zn,i →z0 = m−1 \u0010\n1\nµ+γ−1\n\u0011\n.\nThe proof involves a technical lemma, which we state and prove now.\nLemma 3.14. Let gn(z) be a sequence of continuous real-valued functions that con-\nverge uniformly on (u, v) to g0(z). If g0(z) is analytic on (u, v), then for n large\nenough, gn(z) and g0(z) have the same number of zeros in (u, v) (counting multiplic-\nity).\nProof. Since |g0(z)| is bounded away from zero outside the neighborhoods of its zeros,\nwe can assume without loss of generality that g0(z) has a single zero z0 ∈(u, v) of\nmultiplicity l. Deﬁne ˜gn(z) =\ngn(z)\n|z−z0|l−1. The function ˜g0(z) is bounded and continuous,\nand has a simple zero at z0. For r small enough, ˜g0(z0+r) and ˜g0(z0−r) have diﬀering\nsigns. Without loss of generality, say that the ﬁrst is positive.\nThe sequence ˜gn(z) converges uniformly to ˜g0(z) outside of a neighborhood of z0.\nThus, for n large enough, ˜gn(z −r) < 0 and ˜gn(z + r) > 0. Also, either gn(z) has a\nzero at z0, or else for a small enough neighborhood around z0, sgn ˜gn(z) is constant.\nSince ˜gn(z) is continuous outside of a neighborhood of z0, ˜gn(z) and gn(z) must have\n3.6. SOLUTIONS TO THE SECULAR EQUATION\n45\na zero in (z0 −r, z0 + r) ⊂(u, v). Call this zero zn,1. Since r is arbitrary, zn,1 →z0,\nand\ngn(z)\nz−zn,1 →g0(z)\nz−z0 uniformly on (u, v). We can now proceed inductively, since g0(z)\nz−z0\nhas a zero of multiplicity l −1 at z0.\nProof of Lemma 3.13. Let z0 be the unique solution of m(z0) = (µ + γ−1)−1. Deﬁne\ngn(z) = det T n(z). Since det is a continuous function, gn(z) converges uniformly to\ng0(z) in any neighborhood of z0. Noting that g0(z) has a zero of multiplicity l at z0,\nby Lemma 3.14 we get that for large enough n, gn(z) has l zeros in a neighborhood\nof z0. By a lemma of Okamoto [63], the zeros of gn(z) are almost surely simple.\nThe last thing we need to do is show that for each sequence zn,i solving the\nequation det T n(zn,i) = 0, there is a corresponding sequence of vectors ¯xn,i with\nfn(zn,i,¯xn,i) = 0. Since det T n(zn,i) = 0, there exists an ¯xn,i with T n(zn,i)¯xn,i = 0.\nWe need to show that the sequence of vectors has a limit.\nEvery solution pair (zn,i,¯xn,i) determines a unique eigenvalue-eigenvector pair\nthrough equation (3.13).\nSince the eigenvalues of Sn are almost surely unique,\nwith the identiﬁability restriction of (3.26) we must have that zn,i uniquely deter-\nmines ¯xn,i. Suppose that zn,i is the ith largest solution of det T n(z) = 0, and that\nfn(zn,i,¯xn,i) = 0. We will now show that ¯xn,i\na.s.\n→¯ei.\nLemma 3.15. Suppose that fn(zn,i,¯xn,i) = 0, that ¯xn,i satisﬁes the identiﬁability\nrestriction (3.26), and that zn,i\na.s.\n→¯µi. If µi ̸= µj for all j ̸= i, then ¯xn,i\na.s.\n→¯ei.\nWe will use a perturbation lemma, which follows from the sin Θ theorem (see\nStewart and Sun [85][p. 258]).\nLemma 3.16. Let (z,¯x) be an approximate eigenpair of the k × k matrix A (in the\nsense that A¯x ≈z¯x), with ∥¯x∥2 = 1. Let ¯r = A¯x −z¯x. Suppose that there is a set L\nof k −1 eigenvalues of A such that\nδ = min\nl∈L |l −z| > 0.\nThen there is an eigenpair (z0,¯x0) of A with ∥¯x0∥2 = 1 satisfying\n¯xT\n¯x0 ≥\nr\n1 −∥¯r∥2\n2\nδ2\n46\nCHAPTER 3. BEHAVIOR OF THE SVD\nand\n|z −z0| ≤∥¯r∥2.\nProof of Lemma 3.15. We have that zn,i\na.s.\n→¯µi and that T n(zn,i)¯xn,i = 0.\nSince\nT n(zn,i)\na.s.\n→T 0(¯µi), we get\n∥T 0(¯µi)¯xn,i∥2 = ∥\n\u0000T n(zn,i −T 0(¯µi)\n\u0001\n¯xn,i∥2\n≤∥T n(zn,i −T 0(¯µi)∥F\na.s.\n→0.\nSince µi is distinct, 0 is a simple eigenvalue of T 0(¯µi). Thus, all other eigenvalues\nhave magnitude at least δ > 0 for some δ. Deﬁne ¯rn = T 0(¯µi)¯xn,i. By Lemma 3.16,\nthere exists an eigenpair (z0,¯x0) of T 0(¯µi) with |z0| ≤∥¯rn∥2 and ¯xT\nn,i ¯x0 ≥\nq\n1 −∥¯r∥2\n2\nδ2 .\nSince ∥¯rn∥\na.s.\n→0, for n large enough we must have z0 = 0 and ¯x0 = ¯ei or −¯ei. Lastly,\nnoting that ¯x0 and ¯xn,i are both unit vectors, we get\n∥¯xn,i −¯x0∥2\n2 = 2 −2¯xT\nn,i ¯x0\na.s.\n→0.\nWith the identiﬁability restriction, this forces ¯xn,i\na.s.\n→¯ei.\nFinally, we show that eventually the points described in Lemma 3.13 are the only\nzeros of fn(z,¯x) having z > bγ. Since fn(z,¯x) = 0 implies det T n(z) = 0, it suﬃces\nto show that T n(z) has no other zeros.\nLemma 3.17. For n large enough, almost surely the equation det T n(z) = 0 has\nexactly ¯k solutions in\n\u0000bγ, ∞\n\u0001\n(namely, the ¯k points described in Lemma 3.13).\nProof. By Lemma 3.14, for n large enough det T n(z) and det T 0(z) have the same\nnumber of solutions in (u, v) ⊂\n\u0000bγ, ∞\n\u0001\n. Thus, we only need to show that the solutions\nof det T n(z) are bounded. Since every solution is an eigenvalue of Sn, this amounts to\nshowing that the eigenvalues of Sn are bounded. Using the Courant-Fischer min-max\n3.6. SOLUTIONS TO THE SECULAR EQUATION\n47\ncharacterization of eigenvalues [36], we have\n∥Sn∥2 =\n1\n√n∥Xn∥2\n2\n≤\n\u0012\n∥U nDnV T\nn∥2 + 1\n√n∥En∥2\n\u00132\na.s.\n→\n\u0010√µ1 +\np\nbγ\n\u00112\n.\nThus, the solutions of det T n(z) = 0 are almost surely bounded.\n3.6.2\nSecond-order behavior\nTo ﬁnd the second-order behavior of the solutions to the secular equation, we use\na Taylor-series expansion of fn(z,¯x) around the limit points. That is, if (zn,¯xn) →\n(z0,¯x0), we let Dfn be the derivative of fn and write\n0 = fn(zn,¯xn) ≈fn(z0,¯x0) + Dfn(z0,¯x0)\n \nzn −z0\n¯xn −¯x0\n!\n.\nWe now want to solve for zn −z0 and ¯xn −¯x0. Without the identiﬁability constraint,\nthere are k equations and k + 1 unknowns, but as soon as we impose the condition\n∥¯xn∥2 = ∥¯x0∥2 = 1, the system becomes well-determined.\nTo make this precise, we ﬁrst compute\nDfn(z,¯x) =\n\u0010\nT ′\n0(z)¯x\nT 0(z)\n\u0011\n+ OP\n\u0012 1\n√n\n\u0013\n,\n(3.27)\nwith pointwise convergence in z. Then, we write\nfn(zn,¯xn) = fn(z0,¯x0) + Dfn(z0,¯x0)\n \nzn −z0\n¯xn −¯x0\n!\n+ OP\n\u0000(zn −z0)2 + ∥¯xn −¯x0∥2\n2\n\u0001\n.\n48\nCHAPTER 3. BEHAVIOR OF THE SVD\nIf fn(zn,¯xn) = 0 and f0(z0,¯x0) = 0, then we get\n0 =\n1\n√nfn,1(z0,¯x0) + Dfn(z0,¯x0)\n \nzn −z0\n¯xn −¯x0\n!\n+ OP\n\u0000(zn −z0)2 + ∥¯xn −¯x0∥2\n2\n\u0001\n.\nIf (zn,¯xn)\na.s.\n→(z0,¯x0), then the diﬀerences zn−z0 and ¯xn−¯x0 must be of size OP\n\u0010\n1\n√n\n\u0011\nand the error term in the Taylor expanson is size OP\n\u0000 1\nn\n\u0001\n. The ﬁnal simpliﬁcation we\ncan make is from the length constraint on ¯xn and ¯x0. We have\n1 = ¯xT\nn¯xn\n=\n\u0000¯x0 + (¯xn −¯x0)\n\u0001T\u0000¯x0 + (¯xn −¯x0)\n\u0001\n= 1 + 2¯xT\n0 (¯xn −¯x0) + ∥¯xn −¯x0∥2\n2,\nso that ¯xT\n0 (¯xn −¯x0) = OP\n\u0000 1\nn\n\u0001\n. With a little more eﬀort, we can solve for zn −z0 and\n¯xn −¯x0.\nLemma 3.18. If (zn,¯xn) is a sequence converging almost surely to (¯µi,¯ei), such that\nfn(zn,¯xn) = 0 and µi ̸= µj for i ̸= j, then:\n(i)\n√n(zn −¯µi) = −\n\u0000T n,1(¯µi)\n\u0001\nii\n\u0000T ′\n0(¯µi)\n\u0001\nii\n+ oP(1),\n(3.28)\n(ii)\n√n (xn,i −1) = oP (1) ,\nand\n(3.29)\n(iii)\n√n xn,j = −\n\u0000T n,1(¯µi)\n\u0001\nji\n\u0000T 0(¯µi)\n\u0001\njj\n+ oP(1)\nfor i ̸= j.\n(3.30)\nProof. We have done most of the work in the exposition above. In particular, we\nalready know that (ii) holds. Using the Taylor expansion, we have\n0 =\n1\n√nT n,1(¯µi)¯ei +\n\u0010\nT ′\n0(¯µi)¯ei\nT 0(¯µi)\n\u0011  \nzn −¯µi\n¯xn −¯ei\n!\n+ oP\n\u0012 1\n√n\n\u0013\n.\n3.6. SOLUTIONS TO THE SECULAR EQUATION\n49\nSince T 0 and T ′\n0 are diagonal, using (ii) we get\n0 =\n1\n√n\n\u0000T n,1(¯µi)\n\u0001\nii +\n\u0000T ′\n0(¯µi)\n\u0001\nii(zn −¯µi) + oP\n\u0012 1\n√n\n\u0013\n,\n0 =\n1\n√n\n\u0000T n,1(¯µi)\n\u0001\nji +\n\u0000T 0(¯µi)\n\u0001\njj xn,j + oP\n\u0012 1\n√n\n\u0013\nfor i ̸= j.\nTherefore, (i) and (iii) follow.\nAt this point, it behooves us to do some simpliﬁcation. For µi, µj > γ−1/2, we\nhave\nm(¯µi) = −\n1\nµi + γ−1.\nUsing (3.19), this implies\n¯µi = µi + 1 + γ−1 + γ−1\nµi\n=\n\u0010\nµi + 1\n\u0011 \u0012µi + γ−1\nµi\n\u0013\n.\nNote that this agrees with the deﬁnition of ¯µi in Theorem 3.4. Now,\nm(¯µi) −m(¯µj)\n¯µi −¯µj\n=\n1\n(µi + γ−1)(µj + γ−1) ·\nµiµj\nµiµj −γ−1,\nso that\nm′(¯µi) =\n1\n(µi + γ−1)2 ·\nµ2\ni\nµ2\ni −γ−1.\nAlso,\n¯µi m(¯µi) −¯µj m(¯µj)\n¯µi −¯µj\n=\n1\nµi µj −γ−1.\nWe can compute\n\u0000T 0(¯µi)\n\u0001\njj = (µi −µj)\n\u0012µi + γ−1\nµi\n\u0013\nfor j ̸= i,\n\u0000T ′\n0(¯µi)\n\u0001\nii = −\n\u0012\nµ2\ni\nµ2\ni −γ−1\n\u0013 \u0012µi + γ−1\nµi\n\u0013\n.\n50\nCHAPTER 3. BEHAVIOR OF THE SVD\nSince\n\u00001 + γ−1m(¯µi)\n\u0001−1 = µi+γ−1\nµi\n, we get\nT n,1(¯µi) =\n\u0012 1\n√nET\nn,31En,31 −√n(1 −γ−1)Ik\n\u0013\n−\n\u0012µi + γ−1\nµi\n\u00132\nDF n(¯µi)D\n+\n\u0012µi + γ−1\nµi\n\u0013 \u0010√n(D2\nn −D) + D\n\u0000En,11 −Gn(¯µi)\n\u0001\n+\n\u0000En,11 −Gn(¯µi)\n\u0001TD\n\u0011\n−(µi + 1)\n\u0012µi + γ−1\nµi\n\u0013\nHn(¯µi).\nThe ﬁrst term converges in distribution to a mean-zero multivariate normal with\nvariance 2 (1 −γ−1) along the diagonal and variance 1 −γ−1 otherwise; the elements\nare all uncorrelated except for the obvious symmetry. Also, we have\nCov\n\u0000Fij(¯µ1), Fij(¯µ2)\n\u0001\n= γ−1(1 + δij) ·\n1\n(µ1 + γ−1)(µ2 + γ−1) ·\nµ1µ2\nµ1µ2 −γ−1,\nCov\n\u0000Gij(¯µ1), Gij(¯µ2)\n\u0001\n= γ−1 ·\n1\nµ1µ2 −γ−1,\nCov\n\u0000Hij(¯µ1), Hij(¯µ2)\n\u0001\n= γ−1(1 + δij) ·\n1\n(µ1 + γ−1)(µ2 + γ−1) ·\nµ1µ2\nµ1µ2 −γ−1.\nTherefore, for j ̸= i we have variances\nVar\n\u0000\u0000T n,1(¯µi)¯ei\n\u0001\ni\n\u0001\n= σii ·\n\u0012µi + γ−1\nµi\n\u00132\n+ 2\n\u00002µi + 1 + γ−1\u0001 (µi + γ−1)2\nµ2\ni −γ−1\n+ o(1),\n(3.31a)\nVar\n\u0010\u0000T n,1(¯µi)¯ei\n\u0001\nj\n\u0011\n=\n\u00002µi + 1 + γ−1\u0001 (µi + γ−1)2\nµ2\ni −γ−1\n+ o(1),\n(3.31b)\nand nontrivial covariances\nCov\n\u0010\u0000T n,1(¯µi)¯ei\n\u0001\ni,\n\u0000T n,1(¯µj)¯ej\n\u0001\nj\n\u0011\n= σij · µi + γ−1\nµi\n· µj + γ−1\nµj\n+ o(1)\n(3.32a)\n3.7. SINGULAR VALUES AND SINGULAR VECTORS\n51\nand\nCov\n\u0010\u0000T n,1(¯µi)¯ei\n\u0001\nj,\n\u0000T n,1(¯µj)¯ej\n\u0001\ni\n\u0011\n=\n\u0000µi + µj + 1 + γ−1\u0001\n· (µi + γ−1)(µj + γ−1)\nµiµj −γ−1\n+ o(1).\n(3.32b)\nAll other covariances between the elements of T n,1(¯µi)¯ei and T n,1(¯µj)¯ej are zero.\n3.7\nSingular values and singular vectors\nThe results about solutions to the secular equation translate directly to results about\nthe singular values and right singular vectors of\n1\n√n ˜\nXn.\n3.7.1\nSingular values\nEvery value z with fn(z,¯x) = 0 for some ¯x is the square of a singular value of\n1\n√n ˜\nXn.\nTherefore, Section 3.6 describes the behavior of the top ¯k singular values. To complete\nthe proof of Theorem 3.4 for γ ≥1, we only need to describe what happens to the\nsingular values corresponding to the indices i with µi ≤γ−1/2.\nLemma 3.19. If µi ≤γ−1/2 then the ith eigenvalue of Sn, converges almost surely\nto bγ.\nProof. From Lemma 3.17, we know that for n large enough and ε small, there are\nexactly ¯k = max{i : µi > γ−1/2} eigenvalues of Sn in\n\u0000bγ +ε, ∞). From the eigenvalue\ninterleaving inequalities, we know that the ith eigenvalue of Sn is at least as big as\nthe ith eigenvalue of Sn,22.\nDenote by ˆµn,i the ith eigenvalue of Sn, with ¯k < i ≤k. Then almost surely,\nlim\nn→∞λn,i ≤lim\nn→∞\nˆµn,i ≤lim\nn→∞ˆµn,i ≤bγ + ε.\nSince λn,i\na.s.\n→bγ and ε is arbitrary, this forces ˆµn,i\na.s.\n→bγ.\n52\nCHAPTER 3. BEHAVIOR OF THE SVD\nLemma 3.20. If µi ≤γ−1/2, then √n(ˆµi −¯µi)\nP→0, where ˆµi is the ith eigenvalue of\nSn.\nProof. We use the same notation as in Lemma 3.19.\nRecall that in the present\nsituation, ¯µi = bγ. Since λn,i = bγ + OP(n−2/3), we have that\n√n\n\u0000λn,i −bγ\n\u0001 P→0.\nThis means that\nlim\nn→∞\n√n\n\u0000ˆµn,i −bγ\n\u0001\n≥oP(1).\nThe upper bound is a little more delicate. By the Courant-Fischer min-max charac-\nterization, ˆµ1/2\ni\nis bounded above by ˜µ1/2\ni\n, the ith singular value of\n1\n√nXn + √n\n\u0000(γ−1/2 + ε)1/2 −µ1/2\ni\n\u0001\n¯un,i¯vT\nn,i\nfor any ε > 0. From the work in Section 3.6, we know that\n˜µi =\n\u00001 + γ−1/2 + ε\n\u0001 \u0012\n1 +\n1\nγ1/2 + γε\n\u0013\n+ OP\n\u0012 ˜σi\n√n\n\u0013\n= bγ + ε2 γ1/2 −1\n1 + γ1/2ε + OP\n\u0012 ˜σi\n√n\n\u0013\n,\nwhere\n˜σ2\ni = σii\n\u0012\n1 −\n1\n1 + 2γ1/2ε + γε2\n\u00132\n+ 2\n\u00002(γ−1/2 + ε) + 1 + γ−1\u0001 \u0012\n1 −\n1\n1 + 2γ1/2ε + γε2\n\u0013\n= O(ε).\nTherefore, for all 0 < ε < 1, we have\n√n\n\u0012\nˆµn,i −bγ −ε2 γ1/2 −1\n1 + γ1/2ε\n\u0013\n≤OP\n\u0000ε1/2\u0001\n.\n3.7. SINGULAR VALUES AND SINGULAR VECTORS\n53\nLetting ε →0, we get\nlim\nn→∞\n√n\n\u0000ˆµn,i −bγ\n\u0001\n≤oP(1).\nTogether with the lower bound, this implies √n\n\u0000ˆµn,i −bγ\n\u0001 P→0.\n3.7.2\nRight singular vectors\nFor µi > γ−1/2, we can get a right singular vector of\n1\n√n ˜\nXn from the sequence of\nsolution pairs (zn,i,¯xn,i) satisﬁying fn(zn,i,¯xn,i) = 0 and zn,i\na.s.\n→¯µi. The vector is\nparallel to\n¯˜xn,i =\n \n¯xn\n−(Sn,22 −znIp−k)−1Sn,21 ¯xn\n!\n.\n(3.33)\nWe just need to normalize this vector to have unit length. The length of ¯˜xn is given\nby\n∥¯˜xn∥2\n2 = ¯xT\nn\n\u0000Ik + Sn,12(Sn,22 −znIp−k)−2Sn,21\n\u0001\n¯xn.\n(3.34)\nIt is straightforward to show that for z > bγ,\nIk + Sn,12(Sn,22 −zIp−k)−2Sn,21\na.s.\n→−T ′\n0(z)\n=\nγ−1m′(z)\n\u00001 + γ−1m(z)\n\u00012D2 +\nm′(z)\n\u0000m(z)\n\u00012Ik\nuniformly for z in any compact subset of\n\u0000bγ, ∞\n\u0001\n. It is also not hard to compute for\nµi > γ−1/2 that\n−T ′\n0(¯µi) =\nγ−1\nµ2\ni −γ−1D2 +\nµ2\ni\nµ2\ni −γ−1Ik.\nTherefore, if µi > γ−1/2 and (zn,i,¯xn,i)\na.s.\n→(¯µi,¯ei), then\n∥¯˜xn,i∥2\n2\na.s.\n→µi(µi + γ−1)\nµ2\ni −γ−1\n=\n1 +\n1\nγµi\n1 −\n1\nγµ2\n.\nThe behavior of the right singular vectors when µi ≤γ1/2 is a little more diﬃcult to\n54\nCHAPTER 3. BEHAVIOR OF THE SVD\nget at. We will use a variant of an argument from Paul [68] to show that ∥¯˜x∥2\na.s.\n→∞,\nwhich implies that\n¯xn,i\n∥¯˜xn,i∥2\na.s.\n→0. We can do this by showing the smallest eigenvalue of\nSn,12(Sn,22 −zn,iIp−k)−2Sn,21 goes to ∞.\nWrite\nSn,12(Sn,22 −zn,iIp−k)−2Sn,21 =\np−k\nX\nα=1\n¯sn,α¯sT\nn,α\n(˜λn,α −zn,i)2,\nwhere ¯sn,1,¯sn,2, . . . ,¯sn,p−k are the eigenvectors of Sn,22 multiplied by Sn,12 = ST\nn,21,\nand ˜λn,1, ˜λn,2, . . . , ˜λn,p−k are the eigenvalues of Sn,22. For ε > 0, deﬁne the event\nJn(ε) = {zn,i < bγ + ε} . From the interleaving inequality, we have zn,i ≥˜λn,i. With\nrespect to the ordering on positive-deﬁnite matrices, we have\np−k\nX\nα=1\n¯sn,α¯sT\nn,α\n(˜λn,α −zn,i)2 ⪰\np−k\nX\nα=i\n¯sn,α¯sT\nn,α\n(˜λn,α −zn,i)2\n⪰\np−k\nX\nα=i\n¯sn,α¯sT\nn,α\n(bγ + ε −zn,i)2\non Jn(ε).\nIt is not hard to show that\n\r\r\rPi−1\nα=1\n¯sn,α¯sT\nn,α\n(bγ+ε−zn,i)2\n\r\r\r\na.s.\n→0. Therefore,\np−k\nX\nα=i\n¯sn,α¯sT\nn,α\n(bγ + ε −zn,i)2\na.s.\n→\nγ−1m′\u0000bγ + ε\n\u0001\n1 + γ−1m\n\u0000bγ + ε\n\u0001D2 +\nm′\u0000bγ + ε\n\u0001\n\u0002\nm\n\u0000bγ + ε\n\u0001\u00032Ik.\nAs n →∞, we have P\n\u0000Jn(ε)\n\u0001\n→1. So, since m′\u0000bγ+ε\n\u0001\n≥\nC\n√ε for some constant C, let-\nting ε →0 we must have that the smallest eigenvalue of Sn,12(Sn,22 −zn,iIp−k)−2Sn,21\ngoes to ∞.\n3.7.3\nLeft singular vectors\nWe can get the left singular vectors from the right from multiplication by\n1\n√n ˜\nXn.\nSpeciﬁcally, if ¯˜vn,i is a right singular vector of\n1\n√n ˜\nXn with singular value z1/2\nn,i , then\n¯˜un,i, the corresponding left singular vector, is deﬁned by\nz1/2\nn,i ¯˜un,i =\n1\n√n\n˜\nXn¯˜vn,i.\n3.8. RESULTS FOR γ ∈(0, 1)\n55\nWe are only interested in the ﬁrst k components of ¯˜un,i. If\n¯˜vn,i =\n1\n∥¯˜xn,i∥2\n \n¯xn,i\n−(Sn,22 −zn,iIp−k)−1Sn,21 ¯xn,i\n!\n,\nthen these are given by\n1\n∥¯˜xn,i∥2Rn(zn,i)¯xn,i, where\nRn(z) = Dn + 1\n√nEn,11 −1\n√nEn,12 (Sn,22 −zIp−k)−1 Sn,21.\nIt is not hard to show that Rn(z)\na.s.\n→R0(z), uniformly for z > bγ, and Rn(z) =\nR0(z) +\n1\n√nRn,1(z) + oP\n\u0010\n1\n√n\n\u0011\npointwise for z > bγ. Here,\nR0(z) =\n1\n1 + γ−1m(z)D,\nand\nRn,1(z) =\n\u00001 + γ−1m(z)\n\u0001−1\u0010√n (Dn −D) + En,11\n\u0011\n−\n\u00001 + γ−1m(z)\n\u0001−2F n(z)D −Gn(z).\nLet yn,i =\n1\n∥¯˜xn,i∥2Rn(zn,i)¯xn,i. A straightforward calculation shows the following:\nLemma 3.21. If µi > γ−1/2 and (zn,i,¯xn,i)\na.s.\n→(¯µi,¯ei), then\n¯\nyn,i\na.s.\n→\n 1 −\n1\nγµ2\ni\n1 + 1\nµi\n!1/2\n¯ei.\nIf µi ≤γ−1/2 and zn,i\na.s.\n→bγ, then\n¯\nyn,i\na.s.\n→0.\n3.8\nResults for γ ∈(0, 1)\nRemarkably the formulas for the limiting quantities still hold when γ < 1. To see\nthis, we can get the behavior for γ < 1 by taking the transpose of Xn and applying\n56\nCHAPTER 3. BEHAVIOR OF THE SVD\nthe theorem for γ ≥1. We switch the roles of n and p, replace µi by µ′\ni = γµi, and\nreplace γ by γ′ = γ−1. Then, for instance, the critical cutoﬀbecomes\nµ′\ni >\n1\n√γ′,\ni.e.\nγµi > γ1/2,\nwhich is the same formula for γ ≥1. The almost-sure limit of the square of the ith\neigenvalue of 1\npXnXT\nn becomes\n¯µ′\ni = (γµi + 1)\n\u0012\n1 + γ\nγµi\n\u0013\n= γ(µi + 1)\n\u0012\n1 + 1\nγµ\n\u0013\n= γ¯µi.\nThe formulas for the other quantities also still remain true.\n3.9\nRelated work, extensions, and future work\nWith the proofs completed, we now discuss some extensions and related work.\n3.9.1\nRelated work\nWhen Johnstone [45] worked out the distribution of the largest eigenvalue in the\nnull (no signal) case, he proposed studying “spiked” alternative models. Spiked data\nconsists of vector-valued observations with population covariance of the form\nΣ = diag(µ1, µ2, . . . , µk, 1, . . . , 1).\nWork on the spiked model started with Baik et al. [11], Baik & Silverstein [12], and\nPaul [68]. Baik et al. showed that for complex Gaussian data, a phase transition\n3.9. RELATED WORK, EXTENSIONS, AND FUTURE WORK\n57\nphenomenon exists, depending on the relative magnitude of the spike. Baik & Sil-\nverstein gave the almost-sure limits of the eigenvalues from the sample covariance\nmatrix without assuming Gaussianity.\nPaul worked with real Gaussian data and\ngave the limiting distributions when γ > 1. Paul also gives some results about the\neigenvectors.\nAfter this initial work, Bai & Yao [8] [9] derived the almost-sure limits and prove a\ncentral limit theorem for eigenvalues for a general class of data that includes colored\nnoise and non-Gaussianity. Chen et al [19] consider another type of spiked model\nwith correlation. Nadler [62] derived the behavior of the ﬁrst eigenvector in a spiked\nmodel with one spike.\nThe above authors all consider data of the form X = ZΣ1/2. Onatski [64], like\nus, examines data of the form X = UDV T +E. With slightly diﬀerent assumptions\nthan ours, he is able to give the probability limits of the top eigenvalues, along with\nthe marginal distributions of the scaled eigenvalues and singular vectors. However,\nOnatski does not work in a “transpose-agnostic” framework like we do, so his methods\ndo not allow getting at the joint distribution of the left and right singular vectors.\n3.9.2\nExtensions and future work\nWe have stopped short of computing the second-order behavior of the singular vectors,\nbut no additional theory is required to get at these quantities. Anyone patient enough\ncan use our results to compute the joint distribution of ∥¯˜xn,i∥2, ¯xn,i, and\n¯\nyn,i. This in\nturn will give the joint behavior of the singular vectors.\nMost of the proof remains unchanged for complex Gaussian noise, provided trans-\npose (T) is replaced by conjugate-transpose (H). The variance formulas need a small\nmodiﬁcation, since the fourth-moment of a real Gaussian is 3 and that of a complex\nGaussian is 2.\nFor colored or non-Gaussian noise, we no longer have orthogonal invariance, so\nthe change of basis in Section 3.3 is a little trickier. It is likely that comparable\nresults can still be found, perhaps using results on the eigenvectors of general sample\ncovariance matrices from Bai et al. [3].\n58\nCHAPTER 3. BEHAVIOR OF THE SVD\nChapter 4\nAn intrinsic notion of rank for\nfactor models\nAs Moore’s Law progresses, data sets measuring on the order of hundreds or thousands\nof variables are becoming increasingly more common. Making sense of data of this\nsize is simply not tractable without imposing a simplifying model.\nOne popular\nsimpliﬁcation is to posit existence of a small number of common factors that drive\nthe dynamics of the data, which are usually estimated by principal component analysis\n(PCA) or some variation thereof. The n × p data matrix X is approximated as a\nlow-rank product X ≈UV T, where U and V are n × k and p × k, respectively, with\nk much smaller than n and p.\nThe number of algorithms for approximating matrices by low-rank products has\nexploded in recent years. These algorithms include archetypal analysis [21], the semi-\ndiscrete decomposition (SDD) [49], the non-negative matrix factorization (NMF) [52],\nthe plaid model [51], the CUR decomposition [25], and regularized versions thereof.\nThey also include some clustering methods, in particular k-means and fuzzy k-means\n[14].\nA prevailing question is: How many common factors underlie a data set? Alter-\nnately, how should one choose k? In general, the answer to this question is application-\nspeciﬁc. If we are trying to use X to predict a response, y, then the optimal k is\nthe one that gives the best prediction error for y. The situation is not always this\n59\n60 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\nsimple, though. For exploratory analysis, there is no external response, and we want\nto choose a k that is “intrinsic” to X. For other applications, we don’t have a single\nresponse, y, we have many responses y1, y2, . . . , ym. We may not even know all of the\nyi when we are processing X. We want a k that has good average-case or worst-case\nprediction properties for a large class of responses.\nIn this chapter, we develop a precise notion of intrinsic rank for “latent factor” ma-\ntrix data. This choice of k is the one that minimizes average- or worst-case prediction\nerror over all bilinear statistics. We speciﬁcally work with the singular value decom-\nposition (SVD), but our deﬁnition can be extended to other matrix factorizations as\nwell.\nSection 4.1 introduces the latent factor model, the data model from which we\nbase our constructions. Section 4.2 deﬁnes intrinsic rank as the minimizer of a loss\nfunction. We give a theoretical analysis of the behavior of some natural loss functions\nin Section 4.3, followed by simulations in Section 4.4. In Section 4.5, we examine the\nconnection between intrinsic rank and the scree plot. Finally, Section 4.6 discusses\nsome extensions and Section 4.7 gives a summary of the chapter.\n4.1\nThe latent factor model\nWe start by describing a model for data generated by a small number of latent factors\nand additive noise. Suppose that we have n multivariate observations ¯x1,¯x2, . . . ,¯xn ∈\nRp. In a microarray setting, we will have about n = 50 arrays measuring the activa-\ntions of around p = 5000 genes (or 50000, or even millions of genes in the case of exon\narrays). Alternatively, for ﬁnancial applications ¯xi will measure the market value of\non the order of p = 1000 assets on day i, and we may be looking at data from the\nlast three years, so n ≈1000. In these situations and others like them, we can often\nconvince ourselves that there aren’t really 5000 or 1000 diﬀerent things going on in\nthe data. Probably, there are a small number, k of unobserved factors driving the\ndynamics of the data. Typically, in fact, we think k is on the order of around 5 or 10.\nTo be speciﬁc about this intuition, for genomics applications, we don’t really\nthink that all p = 5000 measured genes are behaving independently. On the contrary,\n4.1. THE LATENT FACTOR MODEL\n61\nwe think that there are a small number of biological processes that determine how\nmuch of each protein gets produced. In ﬁnance, while it is true that the stock prices\nof individual companies have a certain degree of independence, often macroscopic\neﬀects like industry- and market-wide trends can explain a substantial portion of the\nvalue.\n4.1.1\nThe spiked model\nOne way to model latent eﬀects is to assume that the ¯xi are iid and that their\ncovariance is “spiked”. We think that ¯xi is a weighted combination of k latent factors\ncorrupted by additive white noise. In this case, ¯xi can be decomposed as\n¯xi =\nk\nX\nj=1 ¯ajsi,j + ¯εi\n= A¯si + ¯εi,\n(4.1)\nwhere A =\n\u0010\n¯a1\n¯a2\n· · ·\n¯ak\n\u0011\nis a p × k matrix of latent factors common to all\nobservations and ¯si ∈Rk is a vector of loadings for the ith observation. We assume\nthat the noise vector ¯εi is distributed as N(0, σ2Ip). If the loadings have mean zero\nand covariance ΣS ∈Rk×k, and if they are also independent of the noise, then ¯xi has\ncovariance\nΣ ≡E\n\u0002\n¯xi ¯xT\ni\n\u0003\n= AΣSAT + σ2Ip.\n(4.2)\nThe decomposition in (4.2) can be reparametrized as\nΣ = QΛQT + σ2Ip,\n(4.3)\nwhere QTQ = Ik and Λ = diag (λ1, λ2, . . . , λk) , with λ1 ≥λ2 ≥· · · ≥λk ≥0.\nEquation (4.3) makes it apparent that Σ is “spiked”, in the sense that most of its\neigenvalues are equal, but k eigenvalues stand out above the bulk. The ﬁrst k eigen-\nvalues are λ1 + σ2, λ2 + σ2, . . . , λk + σ2, and the remaining p −k eigenvalues are equal\nto σ2.\n62 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\n4.1.2\nMore general matrix models\nWe can introduce a model more general than the spiked one by specifying a distri-\nbution for the n × p data matrix X =\n\u0010\n¯x1\n¯x2\n· · ·\n¯xn\n\u0011T\nthat includes dependence\nbetween the rows. In the spiked model, the distribution of X can be described as\nX\nd= ZΛ1/2QT + E,\n(4.4)\nwhere E =\n\u0010\n¯ε1\n¯ε2\n· · ·\n¯εn\n\u0011T\nand Z is an n × k matrix of independent N (0, 1)\nelements. More generally, we can consider data of the form\nX\nd= √n UDV T + E,\n(4.5)\nwhere U TU = V TV = Ik, and D = diag(d1, d2, . . . , dk) with d1 ≥d2 ≥· · · ≥dk ≥0.\nWe can get (4.5) from (4.4) by letting ZΛ1/2 = √n UDCT be the SVD of ZΛ1/2 and\ndeﬁning V = QC. Unlike the spiked model, (4.5) can model dependence between\nvariables as well as dependence between observations.\n4.2\nAn intrinsic notion of rank\nWith Section 4.1’s latent factor model in mind, we turn our attention to deﬁning the\nintrinsic rank of a data set. This deﬁnition will be motivated both by the generative\nmodel for X and by predictive power considerations. When X = √n UDV T + E,\nwe think of √n UDV T as “signal” and E as “noise”. We make a distinction between\nthe generative rank and the eﬀective rank.\nDeﬁnition 4.1. If the n × p matrix X is distributed as X = √n UDV T + E,\nwhere U TU = V TV = Ik0, D is a k0 × k0 diagonal matrix with positive diagonal\nentries, and E is a noise matrix independent of the signal term whose elements are\niid N(0, σ2), then we denote by k0 the generative rank of X.\nIntuitively the generative rank is the rank of the signal part of X.\nThe eﬀective rank is deﬁned in terms of how well the ﬁrst terms of the SVD\nof X approximates the signal √n UDV T.\nWe let X = √n ˆU ˆ\nD ˆV\nT be the full\n4.2. AN INTRINSIC NOTION OF RANK\n63\nSVD of X, where ˆU =\n\u0010\n¯ˆu1\n¯ˆu2\n· · ·\n¯ˆun∧p\n\u0011\n, ˆV =\n\u0010\n¯ˆv1\n¯ˆv2\n· · ·\n¯ˆvn∧p\n\u0011\n, and ˆ\nD =\ndiag\n\u0010\nˆd1, ˆd2, . . . , ˆdn∧p\n\u0011\n. If we let ˆ\nD(k) = diag\n\u0010\nˆd1, ˆd2, . . . , ˆdk, 0, 0, . . . , 0\n\u0011\n, then the\nSVD truncated to k terms is ˆ\nX(k) = √n ˆU ˆ\nD(k) ˆV\nT. We are now in a position to\ndeﬁne eﬀective rank\nDeﬁnition 4.2. Given a loss function L : Rn×p × Rn×p →R, the eﬀective rank of X\nwith respect to L is equal to\nk∗\nL ≡argmin\nk\nL\n\u0010√nUDV T, ˆ\nX(k)\n\u0011\n.\n(4.6)\nThe eﬀective rank depends on the choice of loss function. In some settings it\nis preferable to choose an application-speciﬁc loss function.\nOften, we appeal to\nsimplicity and convenience and choose squared Frobenius loss. Speciﬁcally,\nLF(A, A′) = ∥A −A′∥2\nF.\n(4.7)\nOne way to motivate this loss function is that it measures average squared error over\nall bilinear statistics of the form ¯αTA\n¯\nβ,\n1\nnp ∥A −A′∥2\nF =\nZ\n∥¯α∥2=1,\n∥\n¯\nβ∥2=1\n\u0000¯αTA\n¯\nβ −¯αTA′\n¯\nβ\n\u00012 d¯α d\n¯\nβ\n(see Section A.3.2 for details). In the context of the latent factor model, the eﬀective\nrank with respect to LF is the rank that gives the best average-case predictions of\nbilinear statistics of the signal part (with respect to squared-error loss).\nA common alternative to Frobenius loss is spectral loss, given by\nL2(A, A′) = ∥A −A′∥2\n2.\n(4.8)\nThis can be interpreted as worst-case squared error over the class of all bilinear\n64 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\nstatistics,\n∥A −A′∥2\n2 =\nsup\n∥¯α∥2=1,\n∥\n¯\nβ∥2=1\n\u0000¯αTA\n¯\nβ −¯αTA′\n¯\nβ\n\u00012 .\nIn the sequel, we denote the optimal ranks with respect to Frobenius and spectral\nloss as k∗\nF and k∗\n2, respectively.\n4.3\nLoss behavior\nIn this section we investigate the behavior of the loss functions introduced in Sec-\ntion 4.2. First, we need to be more precise about our working assumptions on the\ndata matrices. The theory is easier if we work in an asymptotic setting, introducing\na sequence of data matrices X1, X2, . . . , Xn, where Xn ∈Rn×p, p = p(n), n →∞,\nand n\np →γ ∈(0, ∞). We will need three assumptions.\nAssumption 4.3. The matrix Xn ∈Rn×p can be decomposed as\nXn = √n U nDnV T\nn + En.\n(4.9)\nHere, U n ∈Rn×k0, Dn ∈Rk0×k0, and V n ∈Rp×k0. The left and right factors U n\nand V n satisfy U T\nnU n = V T\nnV n = Ik0. The aspect ratio satisﬁes n\np = γ + o\n\u0010\n1\n√n\n\u0011\nfor\na constant γ ∈(0, ∞). The number of factors k0 is ﬁxed.\nAssumption 4.4. The matrix of normalized factor strengths is diagonal with Dn =\ndiag (dn,1, dn,2, . . . , dn,k0) . For 1 ≤i ≤k0, the diagonal elements satisfy d2\nn,i\na.s.\n→µi for\ndeterministic µi satisﬁying µ1 > µ2 > · · · > µk0 > 0.\nAssumption 4.5. The noise matrix En has iid elements independent of U n, Dn,\nand V n, with En,11 ∼N(0, σ2).\nThese assumptions allow us to apply the results of Chapter 3 to get the ﬁrst-order be-\nhavior of the SVD of Xn. As before, we let ¯un,1,¯un,2, . . . ,¯un,k0 and ¯vn,1,¯vn,2, . . . ,¯vn,k0\ndenote the columns of U n and V n, respectively. We set Xn = √n ˆU n ˆ\nDn ˆV\nT\nn to\nbe the SVD of Xn, where the columns of ˆU n and ˆV n are ¯ˆun,1,¯ˆun,2, . . . ,¯ˆun,n∧p and\n4.3. LOSS BEHAVIOR\n65\n¯ˆvn,1,¯ˆvn,2, . . . ,¯ˆvn,n∧p, respectively.\nWith ˆ\nDn = diag\n\u0010\nˆµ1/2\nn,1, ˆµ1/2\nn,2, . . . , ˆµ1/2\nn,n∧p\n\u0011\n, we set\nˆ\nDn(k) = diag\n\u0010\nˆµ1/2\nn,1, ˆµ1/2\nn,2, . . . , ˆµ1/2\nn,k, 0, 0, . . . , 0\n\u0011\nso that ˆ\nXn(k) = √n ˆU n ˆ\nDn(k) ˆV\nT\nn is\nthe SVD of Xn truncated to k terms.\nWe can decompose the columns of ˆV n into sums of two terms, with the ﬁrst term\nin the subspace spanned by V n, and the second term orthogonal to it. By setting\nΘn = V T\nn ˆV n and taking the QR decomposition of ˆV n −V nΘn, the matrix of right\nfactors can be expanded as\nˆV n = V nΘn + ¯V n ¯Θn,\n(4.10)\nwith ¯V n ∈Rp×(p−k0) satisfying ¯V\nT\nn ¯V n = Ip−k0 and V T\nn ¯V n = 0. We choose the signs\nso that ¯Θn has non-negative diagonal entries. Note that\nIk = ˆV\nT\nn ˆV n = ΘT\nnΘn + ¯Θ\nT\nn ¯Θn.\n(4.11)\nIn particular, since ¯Θn is upper-triangular, if Θn converges to a diagonal matrix Θ,\nthen ¯Θn must also, converge to a diagonal matrix,\n\u0000Ik −Θ2\u00011/2 . This makes the\ndecomposition in equation (4.10) very convenient to work with.\nThe same trick applies to ˆU n. We expand\nˆU n = U nΦn + ¯U n ¯Φn,\n(4.12)\nwith ¯U n and ¯Φn deﬁned analogously to ¯V n and ¯Θn. Again, we have that\nIk = ΦT\nnΦn + ¯Φ\nT\nn ¯Φn.\n(4.13)\nLikewise, if Φn converges to a diagonal matrix, then ¯Φn must do the same.\nWe can new get a simpliﬁed formula for ˆ\nXn(k).\nWith the decompositions in\nequations (4.10) and (4.12), we get\nˆ\nXn(k) = √n\n\u0010\nU n\n¯U n\n\u0011  \nΦn ˆ\nDn(k)ΘT\nn\nΦn ˆ\nDn(k) ¯Θ\nT\nn\n¯Φn ˆ\nDn(k)ΘT\nn\n¯Φn ˆ\nDn(k) ¯Θ\nT\nn\n!  \nV T\nn\n¯V\nT\nn\n!\n.\n(4.14)\n66 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\nWe can get the asymptotic limits of the quantities above. For 1 ≤i ≤k0, we set\n¯µi =\n\n\n\n(µi + σ2)\n\u0010\n1 + σ2\nγµi\n\u0011\nwhen µi > σ2\n√γ,\nσ2 \u0010\n1 +\n1\n√γ\n\u00112\notherwise,\n(4.15a)\nθi =\n\n\n\n\n\nr\u0010\n1 −σ4\nγµ2\ni\n\u0011 \u0010\n1 + σ2\nγµi\n\u0011−1\nwhen µi > σ2\n√γ,\n0\notherwise,\n(4.15b)\nϕi =\n\n\n\n\n\nr\u0010\n1 −σ4\nγµ2\ni\n\u0011 \u0010\n1 + σ2\nµi\n\u0011−1\nwhen µi > σ2\n√γ,\n0\notherwise,\n(4.15c)\nwhile for i > k0, we set ¯µi = σ2 \u0010\n1 +\n1\n√γ\n\u00112\nand θi = ϕi = 0. For i ≥1, we deﬁne\n¯θi =\nq\n1 −θ2\ni ,\n(4.15d)\n¯ϕi =\nq\n1 −ϕ2\ni .\n(4.15e)\nWith\nD(k) = diag\n\u0010\n¯µ1/2\n1 , ¯µ1/2\n2 , . . . , ¯µ1/2\nk , 0, 0, . . . , 0\n\u0011\n∈Rn×p\n(4.16a)\nand\nΘ = diag (θ1, θ2, . . . , θp) ,\n(4.16b)\nΦ = diag (ϕ1, ϕ2, . . . , ϕn) ,\n(4.16c)\n¯Θ = diag\n\u0000¯θ1, ¯θ2, . . . , ¯θp\n\u0001\n,\n(4.16d)\n¯Φ = diag ( ¯ϕ1, ¯ϕ2, . . . , ¯ϕn) ,\n(4.16e)\n4.3. LOSS BEHAVIOR\n67\nTheorems 3.4 and 3.5 give us that for ﬁxed k as n →∞,\nΦn ˆ\nDn(k)ΘT\nn\na.s.\n→ΦD(k)ΘT,\nΦn ˆ\nDn(k) ¯Θ\nT\nn\na.s.\n→ΦD(k) ¯Θ\nT,\n¯Φn ˆ\nDn(k)ΘT\nn\na.s.\n→¯ΦD(k)ΘT,\n¯Φn ˆ\nDn(k) ¯Θ\nT\nn\na.s.\n→¯ΦD(k) ¯Θ\nT.\nThis result makes it easy to analyze the loss behavior. Letting µi = 0 for i > k0,\nputting ¯µi(k) = ¯µi 1{i ≤k} and\nF i(k) =\n \nµ1/2\ni\n−ϕi ¯µ1/2\ni\n(k) θi\n−ϕi ¯µ1/2\ni\n(k) ¯θi\n−¯ϕi ¯µ1/2\ni\n(k) θi\n−¯ϕi ¯µ1/2\ni\n(k) ¯θi\n!\n(4.17)\nwe have that for ∥· ∥being spectral or Frobenius norm, for ﬁxed k as n →∞,\n1\n√n\n\r\r√n U nDnV T\nn −ˆ\nXn(k)\n\r\r a.s.\n→\n\r\r diag\n\u0000F 1(k), F 2(k), . . . , F k∨k0(k)\n\u0001\r\r.\nThus,\n1\nn\n\r\r√n U nDnV T\nn −ˆ\nXn(k)\n\r\r2\nF\na.s.\n→\nk∨k0\nX\ni=1\n\r\rF i(k)\n\r\r2\nF\n(4.18)\nand\n1\nn\n\r\r√n U nDnV T\nn −ˆ\nXn(k)\n\r\r2\n2\na.s.\n→\nmax\n1≤i≤k∨k0\n\r\rF i(k)\n\r\r2\n2.\n(4.19)\nSimilar results can be gotten for other orthogonally-invariant norms. A straightfor-\nward calculation shows\nF T\ni (k) F i(k)\n=\n \nµi −2ϕiµ1/2\ni\nθi¯µ1/2\ni\n(k) + θ2\ni ¯µi(k)\n−ϕi¯θiµ1/2\ni\n¯µ1/2\ni\n(k) + θi¯θi¯µi(k)\n−ϕi¯θiµ1/2\ni\n¯µ1/2\ni\n(k) + θi¯θi¯µi(k)\n¯θ2\ni ¯µi(k)\n!\n,\n68 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\nso that\ntr\n\u0000F T\ni (k) F i(k)\n\u0001\n= µi −2ϕiθiµ1/2\ni\n¯µ1/2\ni\n(k) + ¯µi(k),\ndet\n\u0000F T\ni (k) F i(k)\n\u0001\n= ¯ϕ2\ni ¯θ2\ni µi¯µi(k).\nWhen µi > σ2\n√γ, we can use the identities ϕiθi¯µ1/2\ni\nµ−1/2\ni\n= 1 −\nσ4\nγµ2\ni and ¯ϕ2\ni ¯θ2\ni =\nσ4\nγµ2\ni to\nget\ntr\n\u0000F T\ni (k) F i(k)\n\u0001\n=\n\n\n\nσ2\nγµi\n\u00003σ2 + (γ + 1)µi\n\u0001\nif i ≤k,\nµi\notherwise,\n(4.20a)\ndet\n\u0000F T\ni (k) F i(k)\n\u0001\n=\n\n\n\n\u0010\nσ2\nγµi\n\u00112\n(µi + σ2)(γµi + σ2)\nif i ≤k\n0\notherwise.\n(4.20b)\nWhen µi ≤σ2\n√γ, we have\ntr\n\u0000F T\ni (k) F i(k)\n\u0001\n=\n\n\n\nµi + σ2 \u0010\n1 +\n1\n√γ\n\u00112\nif i ≤k\nµi\notherwise,\n(4.21a)\ndet\n\u0000F T\ni (k) F i(k)\n\u0001\n=\n\n\n\nµi σ2 \u0010\n1 +\n1\n√γ\n\u00112\nif i ≤k\n0\notherwise.\n(4.21b)\nWe can use these expressions to compute\n∥F i(k)∥2\nF = tr\n\u0000F T\ni (k) F i(k)\n\u0001\n,\n(4.22)\n∥F i(k)∥2\n2 = 1\n2\nn\ntr\n\u0000F T\ni (k) F i(k)\n\u0001\n+\nq\u0002\ntr\n\u0000F T\ni (k) F i(k)\n\u0001\u00032 −4 det\n\u0000F T\ni (k) F i(k)\n\u0001 o\n.\n(4.23)\nThe expression for the limit of 1\nn∥√n U nDnV T\nn −ˆ\nXn(k)∥2\n2 is fairly complicated. In\nthe Frobenius case, we have\n4.3. LOSS BEHAVIOR\n69\nProposition 4.6. For ﬁxed k as n →∞, we have\n1\nn∥√n U nDnV T\nn −ˆ\nXn(k)∥2\nF\na.s.\n→\nk\nX\ni=1\nαiµi +\nk0\nX\ni=k+1\nµi + σ2\n\u0012\n1 + 1\n√γ\n\u00132\n· (k −k0)+,\n(4.24)\nwhere\nαi =\n\n\n\nσ2\nγµ2\ni\n\u00003σ2 + (γ + 1)µi\n\u0001\nif µi > σ2\n√γ,\n1 + σ2\nµi\n\u0010\n1 +\n1\n√γ\n\u00112\notherwise.\n(4.25)\nFigure 4.1 shows αi as a function of µi. It is beneﬁcial to include the ith term\nwhen αi < 1, or equivalently µi > µ∗\nF, with\nµ∗\nF ≡σ2\n\n1 + γ−1\n2\n+\ns\u00121 + γ−1\n2\n\u00132\n+ 3\nγ\n\n.\n(4.26)\nThis gives us the next Corollary.\nCorollary 4.7. As n →∞,\nk∗\nF\na.s.\n→max {i : µi > µ∗\nF} ,\nprovided no µk is exactly equal to µ∗\nF.\nThe theory in Chapter 3 tells us that when µi > σ2\n√γ, the ith signal term is “detectable”\nin the sense that the ith sample singular value and singular vectors are correlated with\nthe population quantities. Proposition 4.6 tells us that when µi ∈\n\u0010\nσ2\n√γ, µ∗\nF\n\u0011\n, the ith\nsignal term is detectable, but it is not helpful (in terms of Frobenius norm) to include\nin the estimate of √n U nDnV T\nn. Only when µi surpasses the inclusion threshold µ∗\nF\nis it beneﬁcial to include the ith term.\nFigure 4.2 shows the detection and inclusion thresholds as functions of γ.\n70 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\nSignal Strength\nFrobenius Loss Penalty\n2−4\n2−2\n20\n22\n24\n26\n0\n5\n10\n15\n20\n25\nAspect Ratio\n16\n4\n1\n0.25\n0.0625\nFigure 4.1: Frobenius loss penalty. Relative penalty for including the ith factor\nin the SVD approximation of √n U nDnV T\nn, with respect to squared Frobenius loss.\nWhen the ith factor has signal strengh µi, the cost for excluding the ith term of the\nSVD is µi, and the cost for including it is αi · µi. Here, we plot αi as a function of µi\nfor various aspect ratios γ = n\np. The units are chosen so that σ2 = 1.\n4.3. LOSS BEHAVIOR\n71\nAspect Ratio\nSignal Strength\n5\n10\n15\n20\n25\n2−4\n2−2\n20\n22\n24\nThreshold\nInclusion\nDetection\nFigure 4.2: Signal strength thresholds. Detection threshold γ−1/2 and inclu-\nsion threshold µ∗\nF = 1+γ−1\n2\n+\nr\u0010\n1+γ−1\n2\n\u00112\n+ 3\nγ plotted against the aspect ratio γ = n\np.\nWhen the normalized signal strength\nµi\nσ2 is above the detection threshold, the ith\nsample SVD factors are correlated with the population factors.\nWith respect to\nFrobenius loss, when the normalized signal strength is above the inclusion threshold,\nit is beneﬁcial to include the ith term in the SVD approximation of √n U nDnV T\nn.\n72 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\n4.4\nSimulations\nWe conﬁrm the theory of the previous section with a Monte Carlo simulation. We\ngenerate a matrix X as follows:\n1. The concentration γ is one of {0.25, 1.0, 4.0}.\n2. The size of the matrix s is one of {144, 400, 1600, 4900}. We set the number of\nrows and columns in the matrix as n = √s γ and p =\np\ns/γ, respectively. This\nensures that γ = n\np and s = n p.\n3. The noise level is set at σ2 = 1. The noise matrix E is an n × p matrix with\niid N (0, 1) elements.\n4. The generative rank, k0, is ﬁxed at 5. The normalized factor strengths are set at\n(µ1, µ2, µ3, µ4, µ5) = (4µ∗\nF, 2µ∗\nF, µ∗\nF, 1\n2µ∗\nF, 1\n4µ∗\nF), and the factor strength matrix\nis D = diag\n\u0010\nµ1/2\n1 , µ1/2\n2 , . . . , µ1/2\n5\n\u0011\n.\n5. The left and right factor matrices U and V are of sizes n × 5 and p × 5,\nrespectively. We choose these matrices uniformly at random over the Stiefel\nmanifold according to Algorithm A.1 in Appendix A.\n6. We set X = √n UDV T + E and let ˆ\nX(k) be the SVD of X truncated to k\nterms.\nAfter generating X, we compute the squared Frobenius loss 1\nn∥√nUDV T −ˆ\nX(k)∥2\nF\nand the squared spectral loss 1\nn∥√nUDV T −ˆ\nX(k)∥2\n2 as functions of the rank, k. In\nFigures 4.3 and 4.4, we plot the results over 500 replicates. For the Frobenius case,\nwe should expect the loss to decrease until k = 2, stay ﬂat at k = 3, and then increase\nthereafter. This is conﬁrmed by the simulations. The spectral norm behaves similarly\nto the Frobenius norm.\n4.4. SIMULATIONS\n73\nRank\nSquared Frobenius Loss\n20\n25\n30\n35\n40\n45\n50\n10\n15\n20\n6\n8\n10\n12\n144\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n400\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n1600\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n4900\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n0.25\n1\n4\nFigure 4.3: Simulated Frobenius loss. Squared Frobenius loss 1\nn∥√nUDV T −\nˆ\nX(k)∥2\nF as a function of the rank, k, for X generated according the procedure de-\nscribed in Section 4.4 and\nˆ\nX(k) equal to the SVD of X truncated to k terms.\nThe concentration γ =\nn\np is one of {0.25, 1.0, 4.0} and the size s = np is one of\n{144, 400, 1600, 4900}. The solid lines show the means over 500 replicates with the\nerror bars showing one standard deviation. The dashed lines show the predictions\nfrom Proposition 4.6. We can see that as the size increases, the simulation agrees\nmore and more with the theory.\n74 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\nRank\nSquared Spectral Loss\n10\n15\n20\n25\n4\n6\n8\n10\n12\n2\n3\n4\n5\n6\n144\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n400\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n1600\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n4900\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0 1 2 3 4 5 6\n0.25\n1\n4\nFigure 4.4:\nSimulated spectral loss. Squared spectral loss\n1\nn∥√nUDV T −\nˆ\nX(k)∥2\n2 as a function of the rank, k, with X and ˆ\nX(k) as in Figure 4.3. Sample size\ns = np is shown in the columns and concentration γ = n\np is shown in the rows. Solid\nlines show the means over 500 replicates with the error bars showing one standard\ndeviation; the dashed lines show the predictions from Section 4.3, speciﬁcally from\nequations (4.19), (4.20a–4.21b), and (4.23). The simulations agrees quite well with\nthe theory, especially for large sample sizes.\n4.5. RELATION TO THE SCREE PLOT\n75\n4.5\nRelation to the scree plot\nCattell’s scree plot [18] is a popular device for choosing the truncation rank in prin-\ncipal component analysis. One plots the square of the singular value, ˆd2\nk, against the\ncomponent number, k. Typically such a plot exhibits an “elbow”, where the slope\nchanges noticeably. This is the point at which Cattell recommends truncating the\nSVD of the matrix.\nIn some circumstances, the elbow is close to the Frobenius and spectral loss mini-\nmizers, k∗\nF and k∗\n2. In Figure 4.5, we generate a matrix X = √n UDV T +E ∈Rn×p,\nwith n = p = 100, such that elements of E are iid N(0, 1). In the ﬁrst column, we set\nD2 = diag(5.0, 4.75, 4.5, . . . , 0.5, 0.25). The ﬁgure shows the scree plot along with\n∥√n UDV T −ˆ\nX(k)∥2 for Frobenius and spectral norms, where ˆ\nX(k) is the SVD\nof X truncated to k terms. There is substantial ambiguity in determining the loca-\ntion of the most pronounced elbow. Despite this subtlety, there is indeed an elbow\nrelatively close to k∗\nF and k∗\n2, the minimizers of the two loss functions.\nWe can easily manipulate the simulation to get a scree plot with a more pro-\nnounced elbow in the wrong place.\nIn the second column of Figure 4.5, we aug-\nment the factor strength matrix with three additional large values, so that D2 =\ndiag(20.0, 15.0, 10.0, 5.0, 4.75, 4.5, . . . , 0.5, 0.25). With this modiﬁcation, there is a\nclear elbow at k = 4. However, compared to the optimal values, truncating the SVD\nat k = 4 gives about a 25% worse error with respect to squared Frobenius loss and\nabout 50% worse with respect to squared spectral loss.\nIn general, we cannot make any assurances about how close the elbow is to k∗\nF\nor k∗\n2. Through a simulation study, Jackson [42] provides evidence that if the latent\nfactors are strong enough to be easily distinguished from noise, then the elbow is a\nreasonable estimate of the loss minimizers (he does not actually compute the loss\nbehavior, but this seems likely). However, when there are both well-separated factors\nand factors near the critical strength level µ∗\nF, the second example here illustrates\nthat the elbow might be a poor estimate.\n76 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\nRank\n      Resid. Spec. Sq.          Resid. Frob. Sq.       Singular Value Sq.\n0.01\n0.02\n0.03\n0.04\n0.05\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\nG\nG\nG\nG\nG\nG\nGGGGG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nGG\nGG\nG\nGG\nG\nG\nGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n0\n20\n40\n60\n80\n100\nRank\n      Resid. Spec. Sq.          Resid. Frob. Sq.       Singular Value Sq.\n0.02\n0.04\n0.06\n0.08\n0.10\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n1\n2\n3\n4\n5\n6\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\n0\n20\n40\n60\n80\n100\nFigure 4.5:\nScree plots and loss functions. We generate a matrix as X =\n√n UDV T + E and set ˆ\nX(k) equal to the SVD of X truncated to k terms. The left\nand right columns show diﬀerent choices of D, described in the text. The top row\ncontains scree plots, where the square of the kth singular value of X, ˆd2\nk, is plotted\nagainst component number, k, with units are chosen so that P\nk ˆd2\nk = 1. The next\ntwo rows show ∥√n UDV T −ˆ\nX(k)∥2\nF and ∥√n UDV T −ˆ\nX(k)∥2\n2, as functions of\nthe rank, k with units chosen so that the minimum value is 1.0. Dashed lines show\nthe elbow of the scree plot and the minimizers of the two loss functions. The elbow of\nthe scree plot (which is ﬁt by eye) gives a reasonable estimate of the loss minimizers\nin the ﬁrst column, but not in the second.\n4.6. EXTENSIONS\n77\n4.6\nExtensions\nWe have focused speciﬁcally on truncating the singular value decomposition because\nthat is the case for which the theory has been most developed. We have also focused\nspeciﬁcally on Frobenius and spectral losses. One could easily extend our work to look\nat the nuclear norm loss (sum of singular values) [32] or Stein’s loss [43]. Alternatively,\nit is not hard to adapt our analysis to study forms like ∥ˆU ˆ\nD(k)2 ˆU\nT −UD2U T∥.\nFor matrix decompositions beyond the SVD, extension of our work is more diﬃcult,\nmainly because very little scholarship has been devoted to their theoretical properties.\nWe have not examined shrinking the singular values at all, but in some situations\nthis may be beneﬁcial.\nFor example, the Frobenius norm of the F i matrix from\nSection 4.3, which is involved in the Frobenius loss ∥√n UDV T −ˆ\nX(k)∥2\nF, converges\nto\nµi −2ϕiθiµ1/2\ni\n¯µ1/2\ni\n+ ¯µi\nwhenever k ≥i. Recall that ¯µi is the almost-sure limit of the ˆd2\ni , the square of the\nith singular value of\n1\n√nX. If we shrink the ith singular value, replacing ˆdi with f( ˆdi)\nfor continuous f(·), then the Frobenius penalty for including the ith term converges\nto\nµi −2ϕiθiµ1/2\ni\nf(¯µ1/2\ni\n) + f 2(¯µ1/2\ni\n).\nThis quantity is minimized when f(¯µ1/2\ni\n) = µ1/2\ni\nϕiθi = ¯µ−1/2\ni\n\u0010\nµi −σ4\nγµi\n\u0011\n. After some\nalgebra, the optimal f takes the form\nf( ˆdi) =\n\n\n\n\u0010\nˆd2\ni −2\n\u00001 + 1\nγ\n\u0001\nσ2 +\n\u00001 −1\nγ\n\u00012 σ4\nˆd2\ni\n\u00111/2\nwhen ˆdi > σ\n\u0010\n1 +\n1\n√γ\n\u0011\n,\n0\notherwise.\n(4.27)\nWith this shrinkage, it is always beneﬁcial (in terms of Frobenius norm) to include\nthe ith term.\n78 CHAPTER 4. AN INTRINSIC NOTION OF RANK FOR FACTOR MODELS\n4.7\nSummary and future work\nWe have described a plausible model for data generated by a small number of latent\nfactors corrupted by additive white noise. With this model in mind, we have moti-\nvated two loss functions, the squared Frobenius norm and squared spectral norm, as\nmeasuring average- or worst-case quadratic error over the class of all bilinear statis-\ntics. These loss functions in turn determine the intrinsic ranks k∗\nF and k∗\n2. We have\nshown how the losses and the ranks behave for the truncated SVD, both theoretically\nand through simulation. Finally, we have explored the relation between intrinsic rank\nand the scree plot, and then discussed some extensions.\nWe did not describe a way to estimate the error from truncating an SVD, nor\ndid we propose a practical method for choosing k. For now, our hope is that this\nwork is useful in developing intuition for how the SVD behaves and that it provides a\nsuitable starting point for designing and evaluating such procedures. In later chapters,\nwe explicitly discuss estimation and rank selection.\nChapter 5\nCross-validation for unsupervised\nlearning\nThe problem unsupervised learning (UL) tries to address is this: given some data,\ndescribe its distribution.\nMany estimation problems can be cast as unsupervised\nlearning, including mean and density estimation. However, more commonly unsuper-\nvised learning refers to either clustering or manifold learning. A canonical example\nis principal component analysis (PCA). In PCA, we are given some high-dimensional\ndata, and we look for a lower-dimensional subspace that explains most of the variation\nin the data. The lower-dimensional subspace describes the distribution of the data. In\nclustering, the estimated cluster centers give us information about the distribution of\nthe data. The output of every UL method is a summary statistic designed to convey\ninformation about the process which generated the data.\nMany UL problems involve model selection. For example, in principal component\nanalysis we need to choose how many components to keep. For clustering, we need\nto choose the number of clusters in the data. Many manifold learning techniques\nrequire choosing a bandwidth or a kernel. Often in these contexts, model-selection is\ndone in an ad-hoc manner. Rules of thumb and manual inspection guide most choices\nfor how many components to keep, how many clusters are present, and what is an\nappropriate kernel. Such informal selection rules can be problematic when diﬀerent\nresearchers come to diﬀerent conclusions about what the right model is. Moreover,\n79\n80\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\neven when there is an obvious “natural” model to human eyes, it may be hard to pick\nout in computer-automated analysis. For objectivity and eﬃciency, it is desirable to\nhave a well-speciﬁed automatic model selection procedure.\nFor concreteness, in this chapter we focus on principal components, though many\nof the ideas generalize to other methods. We suppose that the data, X, is an n × p\nmatrix generated according to the signal-plus-noise model X = √n UDV T +E. We\nconsider the ﬁrst term to be the “signal” matrix, and the second term to be “noise”.\nOften the signal term is a low-rank product. Our goal is to estimate this term as best\nas possible by truncating the singular value decomposition (SVD) of X. Here “best”\nmeans with respect to the metrics introduced in Chapter 4. We are interested in the\nmodel selection problem where each model is deﬁned by the number of terms we keep\nfrom the SVD of X.\nWe would like our model selection procedure to be non-parametric, if possible.\nTo work in a variety of contexts, the selection procedure cannot assume Gaussianity\nor independence across samples. We would like the procedure to be driven by the\nempirical distribution of the data. Cross-validation (CV) is a popular approach to\nmodel selection that generally meets these criteria. Therefore, we seek to adapt CV\nto our purposes.\nCV prescribes dividing a data set into a “test” set and a “train” set, ﬁtting a model\nto the training set, and then evaluating the model’s performance on the test set. We\nrepeat the ﬁt/evaluate procedure multiple times over diﬀerent test/train partitions,\nand then average over all replicates. Traditionally, the partitions are chosen so that\neach datum occurs in exactly one test set. As for terminology, for a particular replicate\nthe test set is commonly referred to as the held-out or left-out set, and likewise the\ntrain set is often called the held-in or left-in set.\nMost often, cross-validation is applied in supervised contexts. In supervised learn-\ning (SL) the data consists of a sequence of (¯x,\n¯\ny) predictor-response pairs. Broadly\nconstrued, the goal of supervised learning is to describe the conditional distribution\nof\n¯\ny given ¯x. This is usually for prediction or classiﬁcation. In the supervised context,\n81\nfor a particular CV replicate there are four parts of data:\n \nXtrain\nY train\nXtest\nY test\n!\n.\nImplicit in the description of cross-validation is that the replicates use Xtest to predict\nY test. So, the held-in data looks like\n \nXtrain\nY train\nXtest\n∗\n!\n.\nWe extrapolate from Xtest to predict Y test.\nIt is not immediately obvious how to apply cross-validation to unsupervised learn-\ning. In unsupervised learning there is no Y ; we instead have the two-way partition\n \nXtrain\nXtest\n!\n.\nThere is nothing to predict! Renaming X to Y does not make the problem any\nbetter, for then the division becomes:\n \nY train\nY test\n!\n,\nwith hold-in\n \nY train\n∗\n!\n.\nNow, there is nothing to extrapolate from to predict Y test! For cross-validation to\nwork in unsupervised learning, we need to consider more general hold-outs.\nWe look at two diﬀerent types of hold-outs in this chapter. The ﬁrst, due to Wold,\nis “speckled”: we leave out random elements of the matrix X and use a missing data\nalgorithm like expectation-maximization (EM) for prediction. The second type of\nhold-out is “blocked”. This type, due to Gabriel, randomly partitions the columns of\n82\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nX into “predictor” and “response” sets and then performs the SL version of cross-\nvalidation.\n5.1\nAssumptions, and notation\nWe will generally assume we have data X ∈Rn×p generated according to the latent\nfactor model\nX = √n UDV T + E.\n(5.1)\nHere, U ∈Rn×k0, V ∈Rn×k0, and D = diag(d1, d2, . . . , dk0), with U TU = Ik0\nV TV = Ik0, and d1 ≥d2 ≥· · · ≥dk0 > 0. We call √n UDV T the signal part and E\nthe noise part. In the spirit of data-driven analysis, we avoid putting distributional\nassumptions on U, D, V , and E. This makes the terms unidentiﬁable. While this\nindeterminacy can (and should!) bother some readers, for now we will plod on.\nWe denote the SVD of X by\nX = √n ˆU ˆ\nD ˆV\nT,\n(5.2)\nwith ˆU ∈Rn×n∧p, ˆV ∈Rp×n∧p, and ˆ\nD = diag( ˆd1, ˆd2, . . . , ˆdn∧p).\nHere, ˆU\nT ˆU =\nˆV\nT ˆV = In∧p and the singular values are ordered ˆd1 ≥ˆd2 ≥· · · ≥ˆdn∧p ≥0. We set\nˆ\nD(k) = diag( ˆd1, ˆd2, . . . , ˆdk, 0, . . . , 0) ∈Rn∧p×n∧p so that\nˆ\nX(k) = √n ˆU ˆ\nD(k) ˆV\nT\n(5.3)\nis the SVD of X truncated to k terms.\nSimilarly, we deﬁne ˆU(k) ∈Rn×k and\nˆV (k) ∈Rp×k to be the ﬁrst k rows of ˆU and ˆV , respectively.\nWe focus on estimating the squared Frobenius model error\nME(k) = ∥√n UDV T −ˆ\nX(k)∥2\nF\n(5.4)\nor its minimizer,\nk∗\nME = argmin\nk\nME(k).\n(5.5)\n5.2. CROSS VALIDATION STRATEGIES\n83\nHere, ∥· ∥2\nF is the sum of squares of the elements.\nAnother kind of error relevant to cross-validation is prediction error. We let E′\nbe a matrix independent of E but having the same distribution conditionally on U,\nD, and V T. We set X′ = √n UDV T + E′ and deﬁne the prediction error\nPE(k) = E∥X′ −ˆ\nX(k)∥2\nF.\n(5.6)\nLikewise, we set\nk∗\nPE = argmin\nk\nPE(k).\n(5.7)\nIf E is independent of U, D, and V , then\nPE(k) = E∥√n UDV T −ˆ\nX(k) + E′∥2\nF\n= E∥√n UDV T −ˆ\nX(k)∥\n+ 2 E\nh\ntr\n\u0010\u0000√n UDV T −ˆ\nX(k)\n\u0001TE′\u0011i\n+ E∥E′∥2\nF\n= E\n\u0002\nME(k)\n\u0003\n+ E∥E∥2\nF.\n(5.8)\nThe prediction error is thus equal to the sum of the expected model error and an\nirreducible error term.\nFinally, we should note that our deﬁnitions of prediction error and model error\nare motivated by the deﬁnitions given by Breiman [15] for cross-validating linear\nregression.\n5.2\nCross validation strategies\nIn this section we describe the various hold-out strategies for getting a cross-validation\nestimate of PE(k). It is possible to get an estimate of ME(k) from the estimate of\nPE(k) by subtracting an estimate of the irreducible error. For now, though, we choose\nto focus just on estimating PE(k).\nFor performing K-fold cross-validation on the matrix X, we partition its elements\ninto K hold-out sets. For each of K replicates and for each value of the rank, k, we\n84\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nleave out one of the hold-out sets, ﬁt a k-term SVD to the left-in set, and evaluate its\nperformance on the left-out set. We thus need to describe the hold-out set, how to ﬁt\nan SVD to the left-in data, and how to make a prediction of the left-out data. After a\nbrief discussion of why the usual (naive) way of doing hold-outs won’t work, we survey\nboth speckled hold-outs (Wold-style), as well as blocked hold-outs (Gabriel-style).\n5.2.1\nNaive hold-outs\nThe ordinary hold-out strategy will not work for estimating prediction error. Suppose\nwe leave out a subset of the rows of X.\nAfter permutation, the rows of X are\npartitioned as\n \nX1\nX2\n!\n, where X1 ∈Rn1×p and X2 ∈Rn2×p, and n1 + n2 = n. The\nonly plausible prediction of X2 based on truncating the SVD of X1 is the following:\n1. Let X1 = √n ˆU 1 ˆ\nD1 ˆV\nT\n1 be the SVD of X1, with ˆ\nD1 = diag( ˆd(1)\n1 , ˆd(1)\n2 , . . . , ˆd(1)\nn1∧p).\n2. Let ˆ\nD1(k) = diag( ˆd(1)\n1, ˆd(1)\n2, . . . , ˆd(1)\nk, 0, . . . , 0) so that ˆ\nX1(k) ≡√n ˆU 1 ˆ\nD1(k) ˆV\nT\n1 is\nthe SVD of X1 truncated to k terms. Similarly, denote by ˆV 1(k) ∈Rp×k the\nﬁrst k columns of ˆV 1.\n3. Let (+) denote pseudo-inverse and predict the held out rows as\nˆ\nX2(k) =\nX2 ˆ\nX1(k)T\u0000 ˆ\nX1(k) ˆ\nX1(k)T\u0001+ ˆ\nX1(k) = X2 ˆV 1(k) ˆV 1(k)T.\nThe problem with this procedure is that ∥X2 −ˆ\nX2(k)∥2\nF decreases with k regardless\nof the true model for X. So, it cannot possibly give us a good estimate of the error\nfrom truncating the full SVD of X.\nA similar situation arrises if we leave out only a subset of the columns of X.\nTo get a reasonable cross-validation estimate, it is therefore necessary to consider\nmore-general hold-out sets.\n5.2.2\nWold hold-outs\nA Wold-style speckled leave-out is perhaps the most obvious attempt at a more general\nhold-out. We leave out a subset of the elements of the X, then use the left-in elements\nto predict the rest.\n5.2. CROSS VALIDATION STRATEGIES\n85\nFirst we need to introduce some more notation. Let I denote the set of indices of\nthe elements of X, so that I = {(i, j) : 1 ≤i ≤n, 1 ≤j ≤p}. For a subset I ⊂I,\nlet ¯I denote its complement I \\ I. We use the symbol ∗to denote a missing value,\nand let R∗= R ∪{∗}. For I ⊂I, we deﬁne the matrix XI ∈Rn×p\n∗\nwith elements\nXI,ij =\n\n\n\nXij\nif (i, j) ∈I,\n∗\notherwise.\n(5.9)\nSimilarly X ¯I, has elements\nX¯I,ij =\n\n\n\nXij\nif (i, j) /∈I,\n∗\notherwise.\n(5.10)\nFinally, for A ∈Rn×p\n∗\n, we deﬁne\n∥A∥2\nF,I =\nX\n(i,j)∈I\nA2\nij.\n(5.11)\nThis notation allows us to describe matrices with missing entries.\nA Wold-style speckled hold-out is an unstructured random subset I ⊂I. The held-\nin data is the matrix X ¯I and the held-out data is the matrix XI. We use a missing\nvalue SVD algorithm to ﬁt a k-term SVD to X ¯I. This gives us an approximation\nU kDkV T\nk , where U k ∈Rn×k and V k ∈Rp×k have orthonormal columns and D ∈\nRk×k is diagonal. We evaluate the SVD on the held-out set by ∥U kDkV T\nk −XI∥2\nF,I.\nAside from the algorithm for getting U kDkV T\nk from X ¯I, this is a full description of\nthe CV replicate.\nWhen Wold introduced this form of hold-out in 1978 [95], he suggested using an\nalgorithm called nonlinear iterative partial least squares (NIPALS) to ﬁt an SVD\nto the held-in data. This algorithm, attributed to Fisher and Mackenzie [33] and\nrediscovered by Wold and Lyttkens [94], never seems to have gained much prominence.\nWe suggest instead using an expectation-maximization (EM) as is consistent with\ncurrent mainstream practice in Statistics. Either way, there are some subtle issues in\n86\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\ntaking the SVD of a matrix with missing values. We explore these issues further in\nSection 5.3, and present a complete algorithm for estimating the factors U kDkV T\nk .\n5.2.3\nGabriel hold-outs\nA Gabriel-style hold-out works by transforming the unsupervised learning problem\ninto a supervised one. We take a subset of the columns of X and denote them as the\nresponse columns; the rest are denoted predictor columns. Then we take a subset of\nthe rows and consider them as test rows; the rest are train rows. This partitions the\nelements of X into four blocks. With permutation matrices P and Q, we can write\nP TXQ =\n \nX11\nX12\nX21\nX22\n!\n.\n(5.12)\nHere, X11 consists of the train-predictor block, X12 is the train-response block, X21\nis the test-predictor block, and X22 is the test-response block. It is beneﬁcial to think\nof the blocks as\n \nX11\nX12\nX21\nX22\n!\n=\n \nXtrain\nY train\nXtest\nY test\n!\n.\nA Gabriel-style replicate has hold-in\n \nX11\nX12\nX21\n∗\n!\nand hold-out X22.\nTo ﬁt a model to the hold-in set, we take the SVD of X11 and ﬁt a regression\nfunction from the predictor columns to the response columns. Normally, the regres-\nsion is ordinary least squares regression from the principal component loadings to the\nresponse columns. Then, to evaluate the function on the hold-out set, we apply the\nestimated regression function to X21 to get a prediction ˆ\nX22.\nIn precise terms, suppose that there are p1 predictor columns, p2 response columns,\nn1 train rows, and n2 test rows. Then X11 ∈Rn1×p1 and X22 ∈Rn2×p2 with p1+p2 = p\n5.2. CROSS VALIDATION STRATEGIES\n87\nand n1 + n2 = n. First we ﬁt an SVD to the train-predictor block\nX11 = √n ˆU 1 ˆ\nD1 ˆV\nT\n1 .\nThen, we truncate the SVD to k terms as √n ˆU 1(k) ˆ\nD1(k) ˆV\nT\n1 (k) in the same way as\nis discussed in Section 5.1. This deﬁnes a projection ˆV 1(k) and principal component\nscores\nˆZ1(k) = X11 ˆV 1(k) = √n ˆU 1(k) ˆ\nD1(k).\nSimilarly, we can get the principal components scores for the test-predictor block as\nˆZ2(k) = X21 ˆV 1(k).\nNext, we model the response columns as linear functions of the principal component\nscores. We ﬁt the model X12 ≈ˆZ1(k)B with\nˆ\nB =\n\u0000 ˆZ1(k)T ˆZ1(k)\n\u0001+ ˆZ1(k)TX12 =\n1\n√n\nˆ\nD1(k)+ ˆU 1(k)TX12.\nFinally, we apply the model the test rows to get a prediction for the test-response\nblock\nˆ\nX22 = ˆZ2(k) ˆ\nB = X21\n\u0012 1\n√n\nˆV 1(k) ˆ\nD1(k)+ ˆU 1(k)T\n\u0013\nX12.\n(5.13)\nThis gives a complete description of the CV replicate.\nRemark 5.1. Typically for Gabriel-style CV, we have two partitions, one for the rows\nand one for the columns. If the rows are partitioned into K sets and the columns are\npartitioned into L sets, then we average the estimated prediction error over all KL\npossible hold-out sets. This corresponds to\nn\nn2 ≈K and\np\np2 ≈L. In this situation, we\nsay that we are performing (K, L)-fold Gabriel-style cross-validation.\nWe can get some intuition for why Gabriel-style replicates work by expressing\nthe latent factor decomposition X = √n UDV T + E in block form. We let P =\n\u0010\nP 1\nP 2\n\u0011\nand Q =\n\u0010\nQ1\nQ2\n\u0011\nbe the block-decompositions of the row and column\npermutations so that Xij = P T\ni XQj. We deﬁne U i = P T\ni U, V j = QT\nj V , and\n88\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nEij = P T\ni EQj so that\nP TXQ =\n \nP T\n1\nP T\n2\n!\n\u0000√n UDV T + E\n\u0001 \u0010\nQ1\nQ2\n\u0011\n= √n\n \nU 1DV T\n1\nU 1DV T\n2\nU 2DV T\n1\nU 2DV T\n2\n!\n+\n \nE11\nE12\nE21\nE22\n!\n.\nThus, all four blocks have the same low-rank structure. If the noise is small then\nX22 ≈√n U 2DV T\n2 and\nˆ\nX22 ≈√n U 2D\n\u0000V T\n1 ˆV 1(k)\n\u0001 ˆ\nD(k)+ \u0000 ˆU 1(k)TU 1\n\u0001\nDV T\n2\nIf the noise is exactly zero, k = k0, and rank(X22) = rank(X), then Owen and\nPerry [65] show that ˆ\nX22 = X22. For other types of noise, the next chapter proves a\nmore general consistency result.\n5.2.4\nRotated cross-validation\nWhen the underlying signal UDV T is sparse, it’s possible that we will miss it in the\ntraining set. For example, if U =\n\u0010\n1\n0\n0\n0\n\u0011T\nand V =\n\u0010\n1\n0\n0\n0\n0\n\u0011T\n, then\nUDV T =\n\n\n\n\n\n\nd1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n.\nEither the test set will observe this factor, or the train set, but not both. Only the\nset that contains X11 will be aﬀected by the factor.\nOne way to adjust for sparse factors is to randomly rotate the rows and columns\nof X before performing the cross-validation. We generate P ∈Rn×n and Q ∈Rp×p,\nuniformly random orthogonal matrices, by employing Algorithm A.1. Then, we set\n˜\nX = P XQT and perform ordinary cross-validation on ˜\nX. Regardless of the factor\nstructure in X, the signal part of ˜\nX will be uniformly spread across all elements of\n5.3. MISSING-VALUE SVDS\n89\nthe matrix. We call this procedure rotated cross-validation, abbreviated RCV.\nRCV is similar in spirit to the generalized cross-validation (GCV) described in\nCraven & Wahba [20] and Golub et al. [35]. GCV is an orthogonally-invariant way of\nperforming leave-one-out cross validation for regression. The diﬀerence is that GCV\nrotates to a speciﬁc non-random conﬁguration of the data that gives equal weight to\nall observations in the rotated space, while RCV rotates to a random conﬁguration\nthat gives equal weight in expectation.\n5.3\nMissing-value SVDs\nTo perform a Wold-style cross-validation we need to be able to compute the SVD of\na matrix with missing entries. This is a diﬃcult problem. First of all, the problem\nis not very well deﬁned. If A is a matrix with missing entries, there are potentially\nmany diﬀerent ways of factoring A as an SVD-like product. Often, one attempts to\nforce uniqueness by ﬁnding the complete matrix A′ ∈Rn,p of minimum rank such\nthat A′\nij = Aij for all non-missing elements of A. Even then, A′ may not be unique.\nTake\nA =\n \n1\n∗\n∗\n∗\n!\n.\nThen\n \n1\n0\n0\n0\n!\n,\n \n1\n1\n0\n0\n!\n,\n \n1\n0\n1\n0\n!\n,\nand\n \n1\n1\n1\n1\n!\nare all rank-1 matrices that agree with A on its non-missing entries.\nWe might\ndiscriminate between these by picking the matrix with minimum Frobenius norm. In\nthis case, the matrix\n \n1\n∗\n∗\n1\n!\npresents an interesting problem. We can either complete it as\n \n1\n1\n1\n1\n!\nor\n \n1\n0\n0\n1\n!\n.\n90\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nThe ﬁrst option has rank 1 and Frobenius norm 2. The second option has higher\nrank, 2, but lower Frobenius norm,\n√\n2. Which criterion is more important? It it not\nclear what the “right” SVD of A is.\nWe can alleviate the problem by considering a sequence of SVDs rather than a\nsingle one. For each k = 0, 1, 2, . . ., deﬁne A′\nk to the rank-k matrix of minimum\nFrobenius norm that agrees with A on its non-missing elements. If no such matrix\nexists, let I ⊂I be indices of the non-missing elements of A and deﬁne the candidate\nsets\nAk = {Ak ∈Rn×p : rank(Ak) = k}\nCk = {Ak ∈Ak : ∥A −Ak∥F,I = min\nBk∈Ak ∥A −Bk∥F,I}.\nLastly, put\nA′\nk = argmin\nAk∈Ck\n∥Ak∥F,I.\nWe then deﬁne the rank-k SVD of A to be the equal to the SVD of A′\nk.\nThere are still some problems with these SVDs. First of all, in general there may be\nno relationship between A′\nk and A′\nk+1; the two matrices may be completely diﬀerent\nand have completely diﬀerent SVDs. We lose the nesting property of ordinary SVDs,\nwhere the rank-k SVD is contained in the rank-(k + 1) SVD. Secondly, although it\nseems plausible, we do not have any guarantees that A′\nk is unique. Situations may\narise where two diﬀerent rank-k matrices have the same norms and the same residual\nnorms on the non-missing elements of A. Finally, ﬁnding A′\nk is a non-convex problem.\nThe function ∥A −Ak∥F,I can have more than one local maximum. We need to be\naware of these deﬁciencies.\nRather than get too deep into the missing-value SVD rabbit-hole, we choose in-\nstead to live with an approximation. We acknowledge that computing the best rank-k\napproximation as deﬁned above is computationally infeasible for large n and p. In-\nstead of proving theorems about the global optimum, we focus on an algorithm for\ncomputing a local solution.\n5.3. MISSING-VALUE SVDS\n91\nWe use an EM-algorithm to estimate A′\nk. The inputs to the algorithm are k, a non-\nnegative integer rank, and A ∈Rn×p\n∗\n, a matrix with missing values. The output is A′\nk,\na rank-k approximation of A. The algorithm proceeds by iteratively estimating the\nmissing values of A by the values from the ﬁrst k terms of the SVD of the completed\nmatrix. We give detailed pseudocode for the procedure as Algorithm 5.1. This is\nessentially the same algorithm as the SVDimpute algorithm given in Troyanksaya et\nal. [91], except that we use a diﬀerent convergence criterion. SVDimpute stops when\nsuccessive estimates of the missing values of A diﬀer by less than “the empirically\ndetermined threshold of 0.01.” We instead stop when relative diﬀerence of the residual\nsum of squares (RSS) between the non-missing entries and the rank-k SVD is small\n(usually 1.0 × 10−4 or less).\nOur reason for using a diﬀerent convergence rule is\nthat the analysis of the EM algorithm in Dempster et al. [22] shows that the RSS\ndecreases with each iteration, but makes no assurances about the missing values\nconverging. Regardless of which convergence criterion is used, the algorithm is very\neasy to implement.\n92\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nAlgorithm 5.1 Rank-k SVD approximation with missing values\n1. Let I = {(i, j) : Aij ̸= ∗}.\n2. For 1 ≤j ≤p let µj be the mean of the non-missing values in column j of A,\nor 0 if all of the entries in column j are missing.\n3. Deﬁne A(0) ∈Rn×p by\nA(0)\nij =\n(\nAij\nif (i, j) ∈I,\nµj\notherwise.\n4. Initialize the iteration count N ←0.\n5. (M-Step) Let\nA(N) =\nn∧p\nX\ni=1\nd(N)\ni\n¯u(N)\ni\n¯v(N)T\ni\nbe the SVD of A(N) and let A′(N)\nk\nbe the SVD truncated to k terms, so that\nA′(N)\nk\n=\nk\nX\ni=1\nd(N)\ni\n¯u(N)\ni\n¯v(N)T\ni\n.\n6. (E-Step) Deﬁne A(N+1) ∈Rn×p as\nA(N+1)\nij\n=\n(\nAij\nif (i, j) ∈I,\nA′(N)\nk,ij\notherwise.\n7. Set\nRSS(N) = ∥A −A′(N)\nk\n∥2\nF,I.\nIf |RSS(N) −RSS(N−1)| is small, declare convergence and output A′(N)\nk\nas A′\nk.\nOtherwise, increment N ←N + 1 and go to Step 5.\n5.4. SIMULATIONS\n93\n5.4\nSimulations\nWe performed two sets of simulations to gauge the performance of Gabriel- and Wold-\nstyle cross-validation. In the ﬁrst set of simulations, we compare estimated prediction\nerror with true prediction error. In the second set, we evaluate the methods’ abilities\nto estimate k∗\nPE, the optimal rank.\nEach simulation generates a data matrix X as X = √n UDV T + E. We ﬁx the\ndimensions and number of generating signals as n = 100, p = 50, and k0 = 6. For\n“weak” factors we set D = Dweak = diag (10, 9, 8, 7, 6, 5) and for “strong” factors,\nwe set D = Dstrong = √nDweak. We generate U, V , and E independently of each\nother. To avoid ambiguity in what constitutes “signal” and what constitutes “noise”,\nwe ensure that the elements of E are uncorrelated with each other.\nWe consider two types of factors. For “Gaussian” factors, we put the elements of\nU distributed iid with U11 ∼N\n\u00000, 1\nn\n\u0001\nand the elements of V iid with V11 ∼N\n\u00000, 1\np\n\u0001\n,\nalso independent of U. For “Sparse” factors we use sparsity parameter s = 10% and\nset\nP{U11 = 0} = 1 −s,\nP\nn\nU11 = −\n1\n√s n\no\n= P\nn\nU11 = +\n1\n√s n\no\n= s\n2.\nSimilarly, we put\nP{V11 = 0} = 1 −s,\nP\nn\nV11 = −\n1\n√s p\no\n= P\nn\nV11 = +\n1\n√s p\no\n= s\n2.\nThe scalings in both cases are chosen so that E[U TU] = E[V TV ] = Ik0. Gaussian\nfactors are uniformly spread out in the observations and variables, while sparse factors\nare only observable in a small percentage of the matrix entries (about 1%).\nWe use three types of noise. For “white” noise, we generate the elements of E\niid with E11 ∼N (0, 1). For “heavy” noise, we use iid elements with E11 ∼σ−1\nν\ntν,\nand ν = 3. Here tν is a t random variable with ν degrees of freedom, and σν =\np\nν/(ν −2) is chosen so that E[E2\n11] = 1. Heavy noise is so-called because it has a\nheavy tail. Lastly, for “colored” noise, we ﬁrst generate σ2\n1, . . . , σ2\nn ∼Inverse-χ2(ν1)\n94\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nand τ 2\n1 , . . . , τ 2\np ∼Inverse-χ2(ν2) independently, with ν1 = ν2 = 3. Then we generate\nthe elements of E independently as Eij ∼c−1\nν1,ν2 · N(0, σ2\ni + τ 2\nj ), where cν1,ν2 =\np\n1/(ν1 −2) + 1/(ν2 −2). Again, cν1,ν2 is chosen so that E[E2\nij] = 1. Colored noise\nsimulates heteroscedasticity. All three types of noise are plausible for real-world data.\nObviously, this set of simulations comes with a number of caveats. We are only\nlooking at two choices for the signal strengths, one choice of n and p, and a single\nhold-out size. Moreover, this example uses relatively small n and p, potentially too\nsmall for consistency asymptotics to kick in. Despite these deﬁciencies, the simula-\ntions still convey substantial information about the behavior of the procedures under\nconsideration.\n5.4.1\nPrediction error estimation\nOur goal with the ﬁrst simulation was to get intuition for the behavior of Gabriel-\nand Wold-style cross validation as prediction error estimators. We generated random\ndata of the form X = √nUDV T + E, described above. With ˆ\nX(k) being the SVD\nof X truncated to k terms, the (normalized) true prediction error is given by\nPE(k) = ∥√n UDV T −ˆ\nX(k)∥2\nF + 1.\nCross-validation gives us an estimate c\nPE(k) of the prediction error curve. We wanted\nto see how c\nPE(k) compares to PE(k).\nFigures 5.1 and 5.2 show the true and estimated prediction error curves from\n(2, 2)-fold Gabriel CV and 5-fold Wold CV, along with their RCV variants. The plots\nonly show one set of curves for each factor and noise instance, but other replicates\nshowed similar behavior.\nFor most of the simulations, the cross-validation estimates of prediction error are\ngenerally conservative. The only exception is with weak factors and colored noise,\nperhaps because of the ambiguity in what constitutes “signal” and what constitutes\n“noise” in this simulation.\nThis global upward bias agrees with previous studies\nof cross-validation (e.g. [15] and [17]). The RCV versions of the methods generally\nbrought down the bias. Some authors have observed a downward bias at the minimizer\n5.4. SIMULATIONS\n95\nof c\nPE(k) for ordinary cross-validation ([16], [88]). This bias does not appear to be\npresent here.\nA striking diﬀerence between Wold- and Gabriel-style CV is their behavior for k\ngreater than k∗\nPE. Gabriel-style CV does a better job at estimating the true PE(k),\nwhich is relatively ﬂat. Wold-style CV, on the other hand, increases steeply for k past\nthe minimizer. In some situations, the Wold-style behavior is more desirable, but as\nthe heavy-noise examples in Figure 5.2 illustrate, the steep increase is not always in\nthe right place. Gabriel-style cross-validation is better at conveying ambiguity when\nthe underlying dimensionality of the data is unclear.\nStrong Gaussian Factors\nRank\nPrediction Error\n2\n4\n6\n8\n10\n12\n2\n4\n6\n8\n10\n12\n2\n4\n6\n8\n10\n12\nGabriel CV\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\n0\n5\n10\n15\n20\nWold CV\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G\nG G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG\nG G\nG G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G\nG G\n0\n5\n10\n15\n20\nWhite Noise\nColored Noise\nHeavy Noise\nStrong Sparse Factors\nRank\nPrediction Error\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8\n10\n12\n14\nGabriel CV\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G\n0\n5\n10\n15\n20\nWold CV\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G G G G G G G G\nG G G G\nG G\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG\nG G G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G\nG G\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG\nG G\nG G G\nG\nG\nG G\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G\nG G\nG\n0\n5\n10\n15\n20\nWhite Noise\nColored Noise\nHeavy Noise\nFigure 5.1:\nCross-validation with strong factors.\nEstimated prediction\nerror curves for Gabriel- and Wold-style cross-validation, both original and rotated\n(RCV) versions, with strong factors in the data. The true prediction error is shown in\nblack, the CV curves are red, and the RCV curves are blue. Error bars are computed\nfrom the CV replicates. Despite their upward bias, the methods do well at estimating\nPE(k) for k ≤k∗\nPE.\n96\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nWeak Gaussian Factors\nRank\nPrediction Error\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nGabriel CV\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG G G G G G G G G G G G G G G G G G G\nG\nG\nG G G G G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G G\nG\nG\nG\nG G\nG G G G G G G G G G G G G G G G\nG\nG\nG G G\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG G G G G G G G G G G G G G G G\nG\nG G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\n0\n5\n10\n15\n20\nWold CV\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG G G\nG G G\nG\nG G\nG\nG\nG\nG\nG G\nG\nG G G\nG\nG\nG G G G G\nG G\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G G\nG\nG\nG G\nG G G\nG\nG\nG G\nG G G G G G G G G G\nG\nG\nG G\nG\nG G G G G G G G G G G G G G G G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG\nG G\nG\nG\nG\nG G G\nG G\nG G G G G G G G\nG\nG\nG G G G G G G\nG\nG G\nG\nG\nG G\nG\nG\nG\nG\nG\n0\n5\n10\n15\n20\nWhite Noise\nColored Noise\nHeavy Noise\nWeak Sparse Factors\nRank\nPrediction Error\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nGabriel CV\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG G G G G G G G G G G G G G G G G G G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G G\nG\nG\nG G G G G G G G G G G G G G G G G G G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG G G G G G G G G G G G G G G G G G G\nG G\nG\nG\nG G G G G G G G G G G G G G G G G G G\n0\n5\n10\n15\n20\nWold CV\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG\nG G G G\nG\nG G\nG\nG\nG\nG\nG\nG\nG G\nG\nG G\nG\nG\nG\nG G G G G\nG G\nG\nG\nG\nG G\nG\nG G\nG\nG\nG\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG G G\nG\nG G\nG\nG\nG G\nG G G G G G G G G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG\nG\nG G G G G G G G G G G G G G G G G G\nG\nG G\nG\nG\nG G\nG G G\nG G G G G G\nG G G G G\nG\nG\nG\nG G G G G G G G\nG\nG\nG G\nG\nG\nG G\nG\nG\n0\n5\n10\n15\n20\nWhite Noise\nColored Noise\nHeavy Noise\nFigure 5.2: Cross-validation with weak factors. Estimated prediction error\ncurves for Gabriel- and Wold-style cross-validation, both original and rotated (RCV)\nversions, with weak factors in the data. As in Figure 5.1 the true prediction error is\nshown in black, the CV curves are red, and the RCV curves are blue. Error bars give\nare computed from the CV replicates. The methods have a harder time estimating\nPE(k) and its minimizer.\n5.4. SIMULATIONS\n97\n5.4.2\nRank estimation\nIn the next simulation, we see how well cross-validation works at estimating the\noptimal rank, especially as compared to other rank-selection methods. We generate\ndata in the same manner as before, and record how far oﬀthe minimizer of c\nPE(k) is\nfrom the minimizer of PE(k).\nWe compared the four CV-based rank estimation methods with seven other meth-\nods. They are as follows:\n• AIC: Rao & Edelman’s AIC-based estimator [71].\n• BIC1, BIC2, and BIC3: Bai & Ng’s BIC-based estimators [2].\n• F: Faber & Kowalski’s modiﬁcation of Malinowski’s F-test, with a signiﬁcance\nlevel of 0.05 [31, 54].\n• MDL: Wax & Kailaith’s estimator based on the minimum description length\nprinciple [93] .\n• UIP: Kritchman & Nadler’s estimator based on Roy’s union-intersection prin-\nciple and their background noise estimator, with a signiﬁcance level of 0.001\n[50, 72].\nKritchman and Nadler [50] give concise descriptions of four of the estimators. The\nother estimators, Bai and Ng’s BICs, are deﬁned as the minimizers of\nBIC1(k) = log ∥X −ˆ\nX(k)∥2\nF + k n + p\nn p log n p\nn + p,\n(5.14a)\nBIC2(k) = log ∥X −ˆ\nX(k)∥2\nF + k n + p\nn p log Cn,p,\n(5.14b)\nBIC3(k) = log ∥X −ˆ\nX(k)∥2\nF + k log C2\nn,p\nCn,p\n,\n(5.14c)\nwhere Cn,p = min(√n, √p).\nTables 5.1–5.4 summarize the results of 100 replicates. For the strong factors in\nwhite noise, almost all of the methods correctly estimate the true PE-minimizing\nrank. When the noise is non-white, Wold-style CV seems to be the clear winner.\n98\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nEstimated Rank\nMethod\n−7\n−6\n−5\n−4\n−3\n−2\n−1\n0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n> 7\nWhite Noise\nCV-Gabriel\n99\n1\nRCV-Gabriel\n97\n2\n1\nCV-Wold\n100\nRCV-Wold\n100\nAIC\n99\n1\nBIC1\n100\nBIC2\n100\nBIC3\n100\nF\n100\nMDL\n100\nUIP\n100\nColored Noise\nCV-Gabriel\n32\n42\n18\n5\n2\n1\nRCV-Gabriel\n6\n7\n18\n13\n15\n26\n9\n4\n2\nCV-Wold\n97\n3\nRCV-Wold\n22\n39\n29\n7\n2\n1\nAIC\n2\n9\n19\n70\nBIC1\n14\n26\n31\n20\n4\n4\n1\nBIC2\n23\n41\n24\n9\n1\n2\nBIC3\n1\n1\n3\n4\n5\n3\n4\n79\nF\n4\n11\n29\n27\n15\n12\n1\n1\nMDL\n13\n24\n36\n20\n3\n3\n1\nUIP\n1\n5\n14\n28\n23\n19\n9\n1\nHeavy Noise\nCV-Gabriel\n61\n35\n4\nRCV-Gabriel\n42\n27\n12\n14\n5\nCV-Wold\n99\n1\nRCV-Wold\n68\n26\n5\n1\nAIC\n3\n20\n22\n32\n18\n4\n1\nBIC1\n65\n27\n7\n1\nBIC2\n70\n26\n3\n1\nBIC3\n32\n27\n20\n13\n4\n4\nF\n38\n29\n27\n6\nMDL\n64\n28\n7\n1\nUIP\n18\n35\n27\n18\n2\nTable 5.1:\nRank estimation with strong Gaussian factors.\nDiﬀerence\nbetween the estimated rank and the true minimizer of PE(k) for 100 replicates of\nstrong Gaussian factors with various types of noise.\nHowever, as Figure 5.1 demonstrates, there is not much Frobenius loss penalty for\nslightly overestimating the rank. Therefore, Tables 5.1 and 5.2 may be exaggerating\nthe advantage of Wold-style CV.\nFor the weak factors in white noise, the AIC, F and UIP methods fare well, and\nthe performance of the cross-validation-based methods is mediocre. For non-white\nnoise and weak factors, none of the methods perform very well. This is probably due\nto the inherent ambiguity between what constitutes “signal” and what constitutes\n“noise” in these simulations.\n5.4. SIMULATIONS\n99\nEstimated Rank\nMethod\n−7\n−6\n−5\n−4\n−3\n−2\n−1\n0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n> 7\nWhite Noise\nCV-Gabriel\n8\n73\n11\n7\n1\nRCV-Gabriel\n98\n2\nCV-Wold\n1\n8\n91\nRCV-Wold\n1\n99\nAIC\n100\nBIC1\n1\n99\nBIC2\n1\n99\nBIC3\n100\nF\n100\nMDL\n100\nUIP\n100\nColored Noise\nCV-Gabriel\n4\n34\n19\n24\n8\n6\n5\nRCV-Gabriel\n1\n5\n13\n13\n22\n20\n10\n8\n8\nCV-Wold\n1\n1\n8\n83\n4\n3\nRCV-Wold\n26\n32\n25\n12\n2\n2\n1\nAIC\n2\n10\n19\n69\nBIC1\n17\n23\n29\n20\n3\n4\n2\n2\nBIC2\n24\n36\n27\n9\n2\n1\n1\nBIC3\n3\n2\n2\n4\n7\n2\n80\nF\n4\n9\n32\n28\n14\n11\n2\nMDL\n16\n24\n31\n19\n6\n2\n1\n1\nUIP\n4\n17\n27\n23\n18\n9\n2\nHeavy Noise\nCV-Gabriel\n5\n52\n29\n11\n2\n1\nRCV-Gabriel\n39\n24\n19\n11\n6\n1\nCV-Wold\n1\n1\n11\n83\n4\nRCV-Wold\n66\n28\n5\n1\nAIC\n2\n21\n24\n33\n11\n6\n3\nBIC1\n1\n63\n27\n6\n3\nBIC2\n1\n71\n22\n5\n1\nBIC3\n28\n31\n19\n14\n7\n1\nF\n32\n41\n14\n12\n1\nMDL\n63\n28\n6\n3\nUIP\n20\n32\n27\n15\n6\nTable 5.2: Rank estimation with strong sparse factors. Diﬀerence between\nthe estimated rank and the true minimizer of PE(k) for 100 replicates of strong sparse\nfactors with various types of noise.\n100\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nEstimated Rank\nMethod\n−7\n−6\n−5\n−4\n−3\n−2\n−1\n0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n> 7\nWhite Noise\nCV-Gabriel\n3\n56\n32\n9\nRCV-Gabriel\n5\n66\n20\n8\n1\nCV-Wold\n4\n31\n65\nRCV-Wold\n3\n32\n65\nAIC\n3\n95\n2\nBIC1\n1\n13\n46\n40\nBIC2\n1\n8\n29\n47\n15\nBIC3\n99\n1\nF\n99\n1\nMDL\n6\n44\n50\nUIP\n99\n1\nColored Noise\nCV-Gabriel\n2\n9\n22\n21\n25\n10\n3\n5\n2\n1\nRCV-Gabriel\n2\n4\n11\n16\n14\n12\n19\n9\n13\nCV-Wold\n1\n4\n4\n7\n9\n17\n14\n34\n4\n2\n1\n1\nRCV-Wold\n2\n29\n24\n17\n8\n6\n7\n2\n4\n1\nAIC\n4\n14\n14\n68\nBIC1\n2\n26\n22\n20\n9\n8\n3\n2\n4\n4\nBIC2\n4\n39\n28\n12\n5\n4\n1\n3\n3\n1\nBIC3\n1\n1\n5\n11\n1\n3\n4\n74\nF\n1\n7\n16\n24\n17\n13\n10\n1\n11\nMDL\n1\n25\n23\n21\n8\n9\n4\n1\n4\n4\nUIP\n2\n11\n12\n22\n19\n15\n4\n15\nHeavy Noise\nCV-Gabriel\n1\n1\n2\n10\n51\n25\n8\n1\n1\nRCV-Gabriel\n13\n37\n24\n13\n9\n2\n1\n1\nCV-Wold\n1\n4\n6\n4\n13\n17\n21\n32\n1\n1\nRCV-Wold\n1\n2\n13\n59\n16\n4\n2\n1\n1\n1\nAIC\n7\n15\n28\n25\n14\n7\n1\n2\n1\nBIC1\n1\n2\n16\n58\n16\n3\n2\n1\n1\nBIC2\n4\n15\n23\n46\n8\n1\n1\n1\n1\nBIC3\n38\n21\n23\n8\n4\n4\n1\n1\nF\n45\n22\n19\n11\n1\n1\n1\nMDL\n2\n14\n61\n15\n3\n3\n1\n1\nUIP\n27\n29\n27\n9\n4\n3\n1\nTable 5.3:\nRank estimation with weak Gaussian factors. Diﬀerence be-\ntween the estimated rank and the true minimizer of PE(k) for 100 replicates of weak\nGaussian factors with various types of noise.\n5.4. SIMULATIONS\n101\nEstimated Rank\nMethod\n−7\n−6\n−5\n−4\n−3\n−2\n−1\n0\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n> 7\nWhite Noise\nCV-Gabriel\n8\n16\n39\n27\n8\n2\nRCV-Gabriel\n1\n8\n53\n27\n9\n1\n1\nCV-Wold\n2\n2\n11\n16\n29\n40\nRCV-Wold\n2\n15\n29\n54\nAIC\n6\n77\n16\n1\nBIC1\n1\n4\n21\n45\n29\nBIC2\n1\n4\n14\n27\n37\n17\nBIC3\n1\n2\n86\n11\nF\n85\n15\nMDL\n2\n19\n42\n37\nUIP\n73\n26\n1\nColored Noise\nCV-Gabriel\n2\n5\n16\n19\n19\n12\n9\n3\n10\n3\n2\nRCV-Gabriel\n1\n2\n7\n12\n15\n19\n11\n8\n25\nCV-Wold\n1\n3\n6\n11\n10\n17\n21\n25\n2\n2\n1\n1\nRCV-Wold\n2\n22\n21\n19\n12\n11\n3\n5\n3\n2\nAIC\n2\n8\n9\n81\nBIC1\n1\n22\n23\n20\n10\n9\n4\n2\n3\n6\nBIC2\n1\n7\n37\n17\n13\n9\n7\n1\n3\n4\n1\nBIC3\n2\n2\n2\n4\n8\n7\n75\nF\n4\n6\n21\n22\n14\n13\n6\n14\nMDL\n22\n19\n22\n12\n10\n5\n2\n3\n5\nUIP\n5\n9\n22\n19\n17\n10\n18\nHeavy Noise\nCV-Gabriel\n2\n4\n22\n25\n24\n20\n3\nRCV-Gabriel\n1\n4\n9\n18\n20\n18\n12\n13\n2\n1\n2\nCV-Wold\n1\n1\n4\n10\n16\n19\n29\n18\n1\n1\nRCV-Wold\n2\n19\n51\n7\n16\n3\n1\n1\nAIC\n2\n17\n16\n19\n22\n8\n4\n10\n2\nBIC1\n1\n4\n21\n51\n5\n13\n3\n1\n1\nBIC2\n2\n4\n16\n32\n31\n4\n9\n1\n1\nBIC3\n3\n25\n23\n17\n12\n10\n4\n4\n1\n1\nF\n2\n29\n21\n23\n10\n7\n5\n2\n1\nMDL\n3\n19\n52\n6\n15\n3\n1\n1\nUIP\n20\n23\n22\n13\n12\n3\n5\n1\n1\nTable 5.4:\nRank estimation with weak sparse factors. Diﬀerence between\nthe estimated rank and the true minimizer of PE(k) for 100 replicates of weak sparse\nfactors with various types of noise.\n102\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\n5.5\nReal data example\nWe conclude this chapter with a neuroscience application. The Neural Prosthetic Sys-\ntems Laboratory at Stanford University (NPSL) is interested in studying the motor\ncortex region of the brain. Essentially, they want to know what the correspondence is\nbetween neural activity in that part of the brain and motor activity (movement). Re-\nsearch is still at a very fundamental level, and the basic question of how many things\nare being represented is still unanswered. It is thought that desired position, speed,\nand velocity get expressed as neural activity, but conjectures about the dimensionality\nof the neural responses vary from 7 to 20 or more.\nNPSL has designed and carried out an experiment meant to measure the di-\nmensionality of neural response for a two-dimensional motion task. The experiment\ninvolves measuring the activity in 49 neurons as a monkey performs 27 diﬀerent\nmovement tasks (conditions).\nFor a particular neuron and condition, a simpliﬁed explanation of the experiment\nis as follows:\n1. At time t = 0 ms, start recording neural activity in the monkey.\n2. At time t = 400 ms (target-on), show the monkey a target. The monkey is\nnot allowed to move at this point.\n3. At a random time between time t = 400 ms and time t = 1560 ms, allow the\nmonkey to move.\n4. At time t = 1560 ms (movement), the monkey starts to move and point at the\ntarget.\n5. Record activity up to but not including time t = 2110 ms.\nEach condition includes a target position and a conﬁguration of obstacles. The same\nmonkey is used for every trial. Measurements are taken at 5 ms intervals, so that\nthere are 422 total time points.\nIt is necessary to do some registration, scaling,\nand interpolation before doing more serious data analysis, but the details of those\n5.5. REAL DATA EXAMPLE\n103\nprocesses are not important for our purposes.\nFigure 5.3 shows the preprocessed\nresponses for each neuron and condition.\nTime (ms)\nResponse\n0.0\n0.5\n1.0\n1.5\n0\n500\n1000 1500 2000\nFigure 5.3: Motor cortex data. Response rates in 47 neurons for 27 movement\ntasks. The subplots show the normalized response rates in a single neuron as functions\nof time. Each color corresponds to a diﬀerent movement task. The plot on the left is\na zoomed-in view of the data for the ﬁrst neuron.\nThe data from the NPSL motor cortex experiment can be put into a matrix\nwhere each neuron is thought of as a variable, and the timepoints of each condition\nare thought of as observations. This gives us a matrix X with p = 49 variables and\nn = 27 · 422 = 11394 observations. Of course, the rows of X are nothing like iid, and\nthe noise in X is not white. Parametric methods are not likely to give very reliable\nestimates of the dimensionality of X, but cross-validation stands a reasonable chance.\nAfter centering the columns of X, we performed Wold- and Gabriel-style cross-\nvalidation to estimate the dimensionality of the signal part of X. For Gabriel-style\nCV, we ﬁrst tried both (2, 2)-fold and (2, 49)-fold; both resulting d\nPE(k) curves had\ntheir minima at the maximum k. For 5-fold Wold-style CV, there is a minimum at\nk = 13. The BIC, F, MDL, and UIP estimators all chose k = 48 as the dimensionality,\nwhile the AIC estimator chose k = 47.\nWe show the cross-validation estimated\nprediction curves in Figure 5.4. It is likely that the true dimensionality of X is high.\n104\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\nRank\nEstimated Prediction Error\n0.2\n0.4\n0.6\n0.8\n1.0\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGG\nG\nG\nGG\nG\nG\nGG\nG\nGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGG\nG\nGGGG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n0\n10\n20\n30\n40\nCV Method\nG\nGabriel (2,2)−Fold\nG\nGabriel (2,49)−Fold\nG\nWold 5−Fold\nFigure 5.4:\nMotor cortex estimated prediction error. Prediction error\nas a function of rank, estimated by three cross-validation methods. The units are\nnormalized so that the maximum prediction error is 1.0. Error bars show one standard\nerror, estimated from the folds. The prediction error estimate from Wold-style CV\nshows a minimum at k = 13, but the Gabriel-style CV estimates always decrease with\nk. It is likely that the true dimensionality of the data is high.\n5.6. SUMMARY AND FUTURE WORK\n105\n5.6\nSummary and future work\nWe have described two diﬀerent forms of cross-validation appropriate for model se-\nlection in unsupervised learning. Wold-style CV uses a “speckled” leave-out, and\nGabriel-style CV uses a “blocked” leave-out. We have deﬁned two forms of error\nassociated with SVD/PCA-like models, the prediction error and the model error.\nThrough simulations, we have shown that both forms of CV can be considered to give\nestimates of prediction error. Both methods perform well, but Wold-style CV seems\nto be more robust to badly-behaved noise. We have applied these cross-validation\nmethods to a data analysis problem from a neuroscience experiment.\nWe have focused on latent factor models and the singular value decomposition.\nHowever, it is relatively easy to translate the two styles of cross-validation presented\nhere to other unsupervised learning methods.\nFor Wold-style hold-outs, an EM-\nlike algorithm can usually be applied to the models in many unsupervised learning\ncontexts. The Gabriel-style philosophy of “treat some of the variables as response and\nthe others as predictors” can also be applied more broadly. We are in the process of\ninvestigating both cross-validation strategies for clustering and kernel-based manifold\nlearning.\nThis chapter leaves a number of open questions. Mainly, we have not provided\nany theoretical results here, only simulations. A quote from Downton’s discussion of\nStone’s 1974 paper on cross-validation equally applies to our work:\nA current nine-day wonder in the press concerns the exploits of a Mr. Uri\nGeller who appears to be able to bend metal objects without touching\nthem; [The author] seems to be attempting to bend statistics without\ntouching them. My attitude to both of these phenomena is one of open-\nminded scepticism; I do not believe in either of these prestigious activities,\non the other hand they both deserve serious scientiﬁc examination [86].\nDespite Dowton’s skepticism, cross-validation has proven to be an invaluable tool\nfor supervised learning. It is our hope that with some additional work, CV can be\njust as valuable for unsupervised learning. In the next chapter, we provide some\n106\nCHAPTER 5. CROSS-VALIDATION FOR UNSUPERVISED LEARNING\ntheoretical justiﬁcation for Gabriel-style cross-validation, but analysis of Wold-style\ncross-validation is still an open problem.\nChapter 6\nA theoretical analysis of\nbi-cross-validation\nIn this chapter we will determine the optimal leave out-size for Gabriel-style cross-\nvalidation of an SVD, also known as bi-cross-validation (BCV), along with proving\na weak form of consistency. In Chapter 4, we rigorously deﬁned the rank estimation\nproblem, and in Chapter 5 we introduced Gabriel-style cross validation. Here, we\nprovide theoretic justiﬁcation for Gabriel-style CV.\nFirst, a quick review of the problem. We are given X, an n × p matrix generating\nby a “signal-plus-noise” process,\nX = √n UDV T + E.\nHere, U ∈Rn×k0, D ∈Rk0×k0, V\n∈Rp×k0, and E ∈Rn×p.\nThe ﬁrst term,\n√n UDV T, is the low-rank “signal” part. We call U and V the matrices of left\nand right factors, respectively.\nThey are normalized so that U TU = V TV\n=\nIk0. The factor “strengths” are given in D, a diagonal matrix of the form D =\ndiag(d1, d2, . . . , dk0), with d1 ≥d2 ≥· · · ≥dk0 ≥0. Also, typically k0 is much smaller\nthan n and p. Lastly, E consists of “noise”. Although more general types of noise are\npossible, for simplicity we will assume that E is independent of U, D, and V . We\nthink of the signal part as the important part of X, and the noise part is inherently\n107\n108\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nuninteresting.\nThe rank estimation problem is ﬁnd the optimal number of terms of the SVD\nto keep to estimate the signal part.\nWe let X = √n ˆU ˆ\nD ˆV\nT be the SVD of\nX, where ˆU ∈Rn×n∧p and ˆV\n∈Rp×n∧p have orthonormal columns, and ˆ\nD =\ndiag( ˆd1, ˆd2, . . . , ˆdn∧p) with ˆd1 ≥ˆd2 ≥· · · ≥ˆdn∧p. For 0 ≤k ≤n ∧p, we deﬁne\nˆ\nD(k) = diag( ˆd1, ˆd2, . . . , ˆdk, 0, 0, . . . , 0) so that ˆ\nX(k) ≡√n ˆU ˆ\nD(k) ˆV\nT is the SVD of\nX truncated to k terms. The model error with respect to Frobenius loss is given by\nME(k) = 1\nn p∥√n UDV T −ˆ\nX(k)∥2\nF\nThe optimal rank is deﬁned with respect to this criterion is\nk∗= argmin\nk\nME(k).\nThe problem we consider is how to estimate ME(k) or k∗.\nClosely related to model error is the prediction error. For prediction error, we con-\njure up a noise matrix E′ with the same distribution as E and let X′ = √nUDV T+\nE′. The prediction error is deﬁned as\nPE(k) ≡1\nn pE∥X′ −ˆ\nX(k)∥2\nF,\nwhich can be expressed as\nPE(k) = E[ME(k)] + 1\nn pE∥E∥2\nF.\nThe minimizer of PE is the same as the minimizer of E[ME(k)], and one can get an\nestimate of ME from an estimate of PE by subtracting an estimate of the noise level.\nThe previous chapter suggests using Gabriel-style cross-validation for estimating\nthe optimal rank. Owen & Perry [65] call this procedure bi-cross-validation (BCV).\nFor fold (i, j) of BCV, we permute the rows of X with matrices P (i) and Q(j), then\n6.1. ASSUMPTIONS AND NOTATION\n109\npartition the result into four blocks as\nP (i)TXQ(j) =\n \nX11\nX12\nX21\nX22\n!\n.\nWe take the SVD of the upper-left block and evaluate its predictive performance on\nthe lower-right block. If X11 = √n ˆU 1 ˆ\nD1 ˆV\nT is the SVD of X11 and ˆ\nX11(k) =\n√n ˆU 1 ˆ\nD1(k) ˆV\nT is its truncation to k terms (with ˆ\nD1(k) deﬁned analogously to\nˆ\nD(k)), then the BCV estimate of prediction error from this fold is given by\nc\nPE\n\u0000k; i, j\n\u0001\n=\n1\nn2 p2\n∥X22 −X21 ˆ\nX11(k)+X12∥2\nF.\nHere, + denotes pseudo-inverse and X22 has dimensions n2×p2. For (K, L)-fold BCV,\nthe ﬁnal estimate is the average over all folds:\nc\nPE(k) =\n1\nKL\nK\nX\ni=1\nL\nX\nj=1\nc\nPE\n\u0000k; i, j\n\u0001\n.\nFrom c\nPE(k) we can get an estimate of the optimal rank as ˆk = argmink c\nPE(k).\nIn this chapter, we give a theoretical analysis of c\nPE(k). This allows us to de-\ntermine the bias inherent in c\nPE(k) and its consistency properties for estimating k∗,\nalong with guidance for choosing the number of folds (K and L). Section 6.1 sets\nout our assumptions and notation. Section 6.2 gives our main results. Then, Sec-\ntions 6.3 and 6.4 are devoted to proofs, followed by a discussion in Section 6.5.\n6.1\nAssumptions and notation\nThe theory becomes easier if we work in an asymptotic framework. For that, we\nintroduce a sequence of data matrices indexed by n:\nXn = √n U nDnV T\nn + En.\n(6.1)\n110\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nHere, Xn ∈Rn×p with p = p(n) and n\np →γ ∈(0, ∞). Even though the dimensions\nof Xn grow, we assume that the number of factors is ﬁxed at k0. The ﬁrst set of\nassumptions is as follows:\nAssumption 6.1. We have a sequence of random matrices Xn ∈Rn×p with n →∞\nand p = p(n) also going to inﬁnity. Their ratio converges to a ﬁxed constant γ ∈\n(0, ∞) as n\np = γ + o\n\u0010\n1\n√n\n\u0011\n.\nAssumption 6.2. The matrix Xn is generated as Xn = √n U nDnV T\nn + En. Here,\nU n ∈Rn×k0, Dn ∈Rk0×k0, V n ∈Rp×k0, and En ∈Rn×p. The number of factors, k0,\nis ﬁxed.\nAssumption 6.3. The matrices of left and right factors, U n and V n, have or-\nthonormal columns, i.e.\nU T\nnU n = V T\nnV n = Ik0.\nTheir columns are denoted by\n¯un,1,¯un,2, . . . ,¯un,k0 and ¯vn,1,¯un,2, . . . ,¯vn,k0, respectively.\nAssumption 6.4. The matrix of factor strengths is diagonal:\nDn = diag(dn,1, dn,2, . . . , dn,k0).\n(6.2)\nThe strengths converge as d2\nn,i\na.s.\n→µi and d2\nn,i −µi = OP\n\u0010\n1\n√n\n\u0011\n, strictly ordered as\nµ1 > µ2 > · · · > µk0 > 0.\nAssumption 6.5. The noise matrix En is independent of U n, Dn, and V n. Its\nelements are iid with En,11 ∼N(0, σ2).\nThese assumptions are standard for latent factor models.\nWe can apply the work of Chapter 4 to get the behavior of the model error. We\nlet Xn = √n ˆU n ˆ\nDn ˆV\nT\nn be the SVD of Xn and let ˆ\nXn(k) = √n ˆU n ˆ\nDn(k) ˆV\nT\nn be its\ntruncation to k terms. Then the model error is\nMEn(k) = 1\nn p\n\r\r√n U nDnV T\nn −ˆ\nXn(k)\n\r\r2\nF.\n(6.3)\nWith Assumtions 6.1–6.5, we can apply Proposition 4.6 to get that for ﬁxed k as\n6.1. ASSUMPTIONS AND NOTATION\n111\nn →∞,\np · MEn(k)\na.s.\n→\nk\nX\ni=1\nαiµi +\nk0\nX\ni=k+1\nµi + σ2\n\u0012\n1 + 1\n√γ\n\u00132\n· (k −k0)+,\n(6.4)\nwhere\nαi =\n\n\n\nσ2\nγµ2\ni\n\u00003σ2 + (γ + 1)µi\n\u0001\nif µi > σ2\n√γ,\n1 + σ2\nµi\n\u0010\n1 +\n1\n√γ\n\u00112\notherwise.\n(6.5)\nDeﬁning k∗\nn as the minimizer of MEn(k), we also get that\nk∗\nn\na.s.\n→max {i : µi > µcrit} ,\n(6.6)\nwhere\nµcrit ≡σ2\n\n1 + γ−1\n2\n+\ns\u00121 + γ−1\n2\n\u00132\n+ 3\nγ\n\n,\n(6.7)\nprovided no µi is exactly eqaul to µcrit.\nWe therefore know how MEn(k) and its\nminimizer behave.\nTo study the bi-cross-validation estimate of prediction error, we need to introduce\nsome more assumptions and notation. As we are only analyzing ﬁrst-order behavior,\nwe can restrict our analysis to the prediction error estimate from a single fold. We\nlet P n ∈Rn×n and Qn ∈Rp×p be permutation matrices for the fold, partitioned as\nP n =\n\u0010\nP n,1\nP n,2\n\u0011\nand Qn =\n\u0010\nQn,1\nQn,2\n\u0011\n, with P n,1 ∈Rn×n1, P n,2 ∈Rn×n2,\nQn,1 ∈Rp×p1, and Qn,2 ∈Rp×p2. Note that n = n1 + n2 and that p = p1 + p2. We\ndeﬁne Xn,ij = P T\nn,iXQn,j, En,ij = P T\nn,iEQn,j, U n,i = P T\nn,iU n, and V n,j = QT\nn,jV n.\nThen in block form,\nP T\nnXnQn =\n \nXn,11\nXn,12\nXn,21\nXn,22\n!\n= √n\n \nU n,1DnV T\nn,1\nU n,1DnV T\nn,2\nU n,2DnV T\nn,1\nU n,2DnV T\nn,2\n!\n+\n \nEn,11\nEn,12\nEn,21\nEn,22\n!\n(6.8)\nThis is the starting point of our analysis.\n112\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nNow we look at the estimate of prediction error. We let Xn,11 = √n ˆU n,1 ˆ\nDn,1 ˆV\nT\nn,1\nbe the SVD of Xn,11. Here,\nˆU n,1 =\n\u0010\n¯ˆu(1)\nn,1\n¯ˆu(1)\nn,2\n· · ·\n¯ˆu(1)\nn,n1∧p1\n\u0011\n,\n(6.9a)\nˆV n,1 =\n\u0010\n¯ˆv(1)\nn,1\n¯ˆv(1)\nn,2\n· · ·\n¯ˆv(1)\nn,n1∧p1\n\u0011\n,\n(6.9b)\nand\nˆ\nDn,1 = diag\n\u0010\nˆd(1)\nn,1, ˆd(1)\nn,2, . . . , ˆd(1)\nn,n1∧p1\n\u0011\n.\n(6.9c)\nFor convenience, we deﬁne ˆµ(1)\nn,i =\n\u0000 ˆd(1)\nn,i\n\u00012. For 0 ≤k ≤n1 ∧p1, we let\nˆ\nDn,1(k) = diag\n\u0010\nˆd(1)\nn,1, ˆd(1)\nn,2, . . . , ˆd(1)\nn,k, 0, 0, . . . , 0\n\u0011\n(6.10)\nso that ˆ\nXn,11(k) ≡√n ˆU n,1 ˆ\nDn,1(k) ˆV\nT\nn,1 is the SVD of Xn,11 truncated to k terms.\nThis matrix has pseudo-inverse ˆ\nXn,11(k)+ =\n1\n√n ˆV n,1 ˆ\nDn,1(k)+ ˆU\nT\nn,1. Therefore, the\nBCV rank-k prediction of X22 is\nˆ\nXn,22(k) = Xn,21 ˆ\nX\n+\nn,11(k)Xn,12\n=\n\u0000√n U n,2DnV T\nn,1 + En,21\n\u0001\u0000 ˆ\nXn,11(k)+\u0001\u0000√n U n,1DnV T\nn,2 + En,12\n\u0001\n= √n U n,2DnV T\nn,1 ˆV n,1 ˆ\nDn,1(k)+ ˆU\nT\nn,1U n,1DnV T\nn,2\n+ U n,2DnV T\nn,1 ˆV n,1 ˆ\nDn,1(k)+ ˆU\nT\nn,1En,12\n+ En,21 ˆV n,1 ˆ\nDn,1(k)+ ˆU\nT\nn,1U n,1DnV T\nn,2\n+ 1\n√n En,21 ˆV n,1 ˆ\nDn,1(k)+ ˆU\nT\nn,1En,12\n= √n U n,2DnΘn,1 ˆ\nDn,1(k)+ΦT\nn,1DnV T\nn,2\n+ U n,2DnΘn,1 ˆ\nDn,1(k)+ ˜\nEn,12\n+ ˜\nEn,21 ˆ\nDn,1(k)+ΦT\nn,1DnV T\nn,2\n+ 1\n√n\n˜\nEn,21 ˆ\nDn,1(k)+ ˜\nEn,12,\n(6.11)\n6.2. MAIN RESULTS\n113\nwhere Θn,1 = V T\nn,1 ˆV n,1, Φn,1 = U T\nn,1 ˆU n,1, ˜\nEn,12 = ˆU\nT\nn,1En,12, and ˜\nEn,21 = En,21 ˆV n,1.\nNote that ˜\nEn,12 and ˜\nEn,21 have iid N(0, σ2) entries, also independent of the other\nterms that make up ˆ\nXn,22(k) and Xn,22. By conditioning on ˜\nEn,12 and ˜\nEn,21, we can\nsee that ˆ\nX22(k) is in general a biased estimate of √n U n,2DnV T\nn,2.\nTo analyze ˆ\nX22(k), we need to impose some additional assumptions. The ﬁrst\nassumption is fairly banal and involves the leave-out sizes.\nAssumption 6.6. There exist ﬁxed K, L ∈(0, ∞) (not necessarily integers), such\nthat\nn\nn2 = K + o\n\u0010\n1\n√n\n\u0011\nand\np\np2 = L + o\n\u0010\n1\n√n\n\u0011\n.\nThe next assumption is not quite so innocent and involves the distribution of the\nfactors.\nAssumption 6.7. The departure from orthogonality for the held-in factors U n,1 and\nV n,1 is of order\n1\n√n. Speciﬁcally,\nsup\nn E\n\r\r\r√n1\n\u0010\nU T\nn,1U n,1 −n1\nn Ik0\n\u0011\r\r\r\n2\nF < ∞,\nand\nsup\nn E\n\r\r\r√p1\n\u0010\nV T\nn,1V n,1 −p1\np Ik0\n\u0011\r\r\r\n2\nF < ∞.\nThis assumption is there so that we can apply the theory in Chapter 3 to get at the\nbehavior of the SVD of Xn,11. It is satisﬁed, for example, if we are performing ro-\ntated cross-validation (see subsection 5.2.4) or if the factors are generated by certain\nstationary processes. Proposition A.5 in Appendix A is helpful for verifying Assump-\ntion 6.7. It is likely that the theory presented below holds under a weaker condition,\nbut a detailed analysis is beyond the scope of this chapter.\n6.2\nMain results\nThe BCV estimate of prediction error from a single replicate is given by\nc\nPEn(k) =\n1\nn2 p2\n∥Xn,22 −ˆ\nXn,22(k)∥2\nF.\n(6.12)\n114\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nIt turns out that E\nh\nc\nPEn(k)\ni\nis dominated by the irreducible error. However, if we\nhave a √np-consistent estimate of σ2, then we can get an expression for the scaled\nlimit of the estimated model error. This expression is the main result of the chapter.\nTheorem 6.8. Suppose that ˆσ2\nn is a sequence of √np-consistent estimators of σ2\nsatisfying E\n\u0002√np (ˆσ2\nn −σ2)\n\u0003\n→0. Deﬁne the BCV estimate of model error from a\nsingle replicate as\nd\nMEn(k) = c\nPEn(k) −ˆσ2\nn.\n(6.13)\nThen, for ﬁxed k as n →∞,\nE\nh\np · d\nMEn(k)\ni\n→\nk∧k0\nX\ni=1\nβi µi +\nk0\nX\ni=k+1\nµi + η · (k −k0)+,\n(6.14)\nwhere\nβi =\n\n\n\n\n\n\n\nσ2\nγµ2\ni (3σ2+(γ K−1\nK + L−1\nL )µi)\nρ+(γ K−1\nK + L−1\nL ) σ2\nγµi + σ4\nγµ2\ni\nwhen µi >\nσ2\n√ρ γ,\n1 + η\nµi\notherwise,\n(6.15a)\nρ = K −1\nK\n· L −1\nL\n,\n(6.15b)\nand\nη =\nσ2\n\u0010√γ +\nq\nK\nK−1 ·\nq\nL−1\nL\n\u00112.\n(6.15c)\nIt is interesting to compare βi with the expression for αi in equation (6.5), which\nappears in the scaled limit of the true model error, p · MEn(k). Although the BCV\nestimator of model error is biased, this bias is small for large µi, K, and L.\nA corollary gives the behavior of the minimizer of the expected model error esti-\nmate. We let ˆkn be the rank that minimizes E\nh\nd\nMEn(k)\ni\n. Note that ˆkn is a deter-\nministic quantity. The next two results follow from Theorem 6.8.\n6.2. MAIN RESULTS\n115\nCorollary 6.9. As n →∞,\nˆkn →max\n\u001a\ni : µi > σ2\n√γ ·\nr2\nρ\n\u001b\n,\n(6.16)\n(provided no µi is exactly equal to\nσ2\n√γ ·\nq\n2\nρ, in which case the limit is ambiguous).\nWe can use Corollary 6.9 to guide our choice of K and L. If we choose them\ncarefully, then ˆkn and k∗\nn will converge to the same value.\nCorollary 6.10. If\n√ρ =\n√\n2\n√¯γ + √¯γ + 3,\n(6.17)\nwhere\n¯γ =\n\u0012γ1/2 + γ−1/2\n2\n\u00132\n,\n(6.18)\nthen ˆkn and k∗\nn converge to the same value (provided no µi is exactly equal to σ2\n√γ ·\nq\n2\nρ).\nInterestingly, the ﬁrst-order-optimal choices of K and L do not depend on the\naspect ratio of the original matrix. All that matters is the ratio of the number of\nelements in Xn,11 to the number of elements in Xn.\nFor a square matrix, γ = ¯γ = 1, and the optimal ρ is 2\n9. If we choose K = L, then\nthis requires\nK −1\nK\n=\n√\n2\n3 ,\nso that\nK =\n \n1 −\n√\n2\n3\n!−1\n≈1.89.\nFor general aspect ratios (γ ̸= 1), this requires\nK =\n3\n3 −2\n\u0000√¯γ + 3 −√¯γ\n\u0001.\nFor very large or very small aspect ratios (γ →0 or γ →∞), K →1. In these\nsituations one should leave out almost all of the matrix when performing bi-cross-\nvalidation.\n116\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nThe remainder of the chapter is devoted to proving Theorem 6.8.\n6.3\nThe SVD of the held-in block\nThe ﬁrst step in analyzing the BCV estimate of prediction error is to see how the SVD\nof Xn,11 behaves. With Assumptions 6.1–6.7, we can start this analysis. Our strategy\nis to use Assumption 6.7 to apply a matrix perturbation argument in combination\nwith Theorems 3.4 and 3.5.\nWe show that we can apply Theorem 3.5 to √n U n,1DnV T\nn,1 + En,11 even though\nU n,1 and V n,1 do not have orthogonal columns. First we set\nγ1 = K −1\nK\n·\nL\nL −1 · γ\n(6.19)\nand note that n1\np1 = n1\nn · p\np1 · n\np = γ1 + o\n\u0010\n1\n√n1\n\u0011\n. Next, for 1 ≤i ≤k0, we deﬁne\nµ1,i = L −1\nL\n· µi,\n(6.20a)\n¯µ1,i =\n\n\n\nK−1\nK\n· (µ1,i + σ2)\n\u0010\n1 +\nσ2\nγ1µ1,i\n\u0011\nwhen µ1,i >\nσ2\n√γ1,\nK−1\nK\n· σ2 \u0010\n1 +\n1\n√γ1\n\u00112\notherwise.\n(6.20b)\nθ1,i =\n\n\n\n\n\nq\nL−1\nL ·\nr\u0010\n1 −\nσ4\nγ1µ2\n1,i\n\u0011 \u0010\n1 +\nσ2\nγ1µ1,i\n\u0011−1\nwhen µ1,i >\nσ2\n√γ1,\n0\notherwise,\n(6.20c)\nϕ1,i =\n\n\n\n\n\nq\nK−1\nK\n·\nr\u0010\n1 −\nσ4\nγ1µ2\n1,i\n\u0011 \u0010\n1 + σ2\nµ1,i\n\u0011−1\nwhen µ1,i >\nσ2\n√γ1,\n0\notherwise.\n(6.20d)\nFor i > k0, we put ¯µ1,i = K−1\nK ·σ2 \u0010\n1 +\n1\n√γ1\n\u00112\n. Now, for k ≥1 we let Θ1(k) and Φ1(k)\nbe k0 × k matrices with entries\nθ(k)\n1,ij =\n\n\n\nθ1,i\nif i = j,\n0\notherwise,\nand\nϕ(k)\n1,ij =\n\n\n\nϕ1,i\nif i = j,\n0\notherwise,\n(6.21)\n6.3. THE SVD OF THE HELD-IN BLOCK\n117\nrespectively. In this section, we prove the following:\nProposition 6.11. For ﬁxed k as n →∞, the ﬁrst k columns of Θ1,n and Φ1,n\nconverge in probability to Θ1(k) and Φ1(k), respectively. Likewise, for 1 ≤i ≤k,\nˆd(1)\nn,i\nP→¯µ1/2\n1,i .\nWe prove the proposition by leveraging the work of Chapter 3. We have that\nXn,11 = √n U n,1DnV T\nn,1 + En,11. The ﬁrst term does not satisfy the conditions\nof Theorems 3.4 and 3.5 since U n,1 and V n,1 do not have orthonormal columns.\nMoreover, the scaling is √n instead of √n1.\nWe introduce scaling constants and\ngroup the terms as\nXn,11 = √n1\n\u0010r n\nn1\nU n,1\n\u0011\u0010rp1\np Dn\n\u0011\u0010r p\np1\nV n,1\n\u0011T\n+ En,11.\n(6.22)\nWith this scaling,\nE\n\u0014\u0010r n\nn1\nU n,1\n\u0011T\u0010r n\nn1\nU n,1\n\u0011\u0015\n= E\n\u0014\u0010r p\np1\nV n,1\n\u0011T\u0010r p\np1\nV n,1\n\u0011\u0015\n= Ik0,\nand the diagonal elements of\n\u0010q\np1\np Dn\n\u0011\nconverge to µ1/2\n1,1 , µ1/2\n1,2 , . . . , µ1/2\n1,k0. So, Propo-\nsition 6.11 should at least be plausible.\nWe prove the result by showing that\n\u0010q\nn\nn1U n,1\n\u0011\u0010q\np1\np Dn\n\u0011\u0010q\np\np1V n,1\n\u0011T\nis almost\nan SVD. We denote its k0-term SVD by ˜U n,1 ˜\nDn,1 ˜V\nT\nn,1, and demonstrate the following:\nLemma 6.12. Three properties hold:\n(1) For 1 ≤i ≤k0, |p1\np d2\nn,i −˜d2\nn,i| = OP\n\u0010\n1\n√n\n\u0011\n.\n(2) For any sequence of vectors ¯x1,¯x2, . . . ,¯xn with ¯xn ∈Rn,\n\r\r\r\r\n\u0010r n\nn1\nU n,1\n\u0011T\n¯xn −˜U\nT\nn,1 ¯xn\n\r\r\r\r\n2\n= OP\n\u0012∥¯xn∥2\n√n\n\u0013\n.\n(3) For any sequence of vectors\n¯\ny1,\n¯\ny2, . . . ,\n¯\nyn with\n¯\nyn ∈Rp,\n\r\r\r\r\n\u0010r p\np1\nV n,1\n\u0011T\n¯\nyn −˜V\nT\nn,1 ¯\nyn\n\r\r\r\r\n2\n= OP\n\u0012∥\n¯\nyn∥2\n√n\n\u0013\n.\n118\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nAbove, ˜dn,i is the ith diagonal entry of ˜\nDn, which are assumed to be sorted in de-\nscending order.\nProposition 6.11 is then a direct consequence of Lemma 6.12 combined with Theo-\nrems 3.4 and 3.5.\nProof of Lemma 6.12. First, let\nq\nn\nn1U n,1 = ¯U n,1Rn be a QR-decomposition so that\n¯U n,1 ∈Rn1×k0 has orthonormal columns and Rn is an upper-triangular matrix. Deﬁne\nRn,1 = √n(Rn,1 −Ik0) so that Rn = Ik0 +\n1\n√nRn,1. We can write\nn\nn1\nU T\nn,1U n,1 = Ik0 + 1\n√n\n\u0000Rn,1 + RT\nn,1\n\u0001\n+ 1\nnRT\nn,1Rn,1.\nBy Assumption 6.7,\nn\nn1U T\nn,1U n,1 −Ik0 = OP\n\u0010\n1\n√n\n\u0011\n. Therefore, Rn,1 = OP (1).\nThe same argument applies to show that there exits a ¯V n,1 ∈Rp1×k0 with or-\nthonormal columns such that\nr p\np1\nV n,1 = ˜V n,1\n\u0012\nIk0 + 1\n√nSn,1\n\u0013\n,\nand Sn,1 is upper-triangular with Sn,1 = OP(1).\nWe now look at\n\u0010\nIk0 + 1\n√nRn,1\n\u0011\u0010\nDn\n\u0011\u0010\nIk0 + 1\n√nSn,1\n\u0011T\n= Dn + 1\n√n\n\u0000Rn,1Dn + DnST\nn,1\n\u0001\n+ 1\nnRn,1DnST\nn,1.\nSince the diagonal elements of Dn are distinct, we can apply Lemma 2.15 twice to\nget that there exist k0 × k0 matrices ¯\nRn,1, ¯Sn,1 and ∆n of size OP(1) such that\n\u0010\nIk0 + 1\n√nRn,1\n\u0011\u0010\nDn\n\u0011\u0010\nIk0 + 1\n√nSn,1\n\u0011T\n=\n\u0010\nIk0 + 1\n√n\n¯\nRn,1\n\u0011\u0010\nDn + 1\n√n∆n\n\u0011\u0010\nIk0 + 1\n√n\n¯Sn,1\n\u0011T\n,\nwith Ik0 +\n1\n√n ¯\nRn,1 and Ik0 +\n1\n√n ¯Sn,1 both orthogonal matrices, and ∆n diagonal.\n6.4. THE PREDICTION ERROR ESTIMATE\n119\nWe deﬁne ˜U n,1 = ¯U n,1\n\u0010\nIk0 +\n1\n√n ¯\nRn,1\n\u0011\n, ˜V n,1 = ¯V n,1\n\u0010\nIk0 +\n1\n√n ¯Sn,1\n\u0011\n, and ˜\nDn =\nq\np1\np\n\u0010\nDn +\n1\n√n∆\n\u0011\n. Now, as promised, ˜U n,1 ˜\nDn ˜V\nT\nn,1 is the SVD of\n\u0010r n\nn1\nU n,1\n\u0011\u0010rp1\np Dn\n\u0011\u0010r p\np1\nV n,1\n\u0011T\n.\nWe can see immediately the property (1) holds. For property (2), note that\n˜U n,1 = ¯U n,1\n\u0010\nIk0 + 1\n√n\n¯\nRn,1\n\u0011\n=\nr n\nn1\nU n,1\n\u0010\nIk0 + 1\n√nRn,1\n\u0011−1\u0010\nIk0 + 1\n√n\n¯\nRn,1\n\u0011\n=\nr n\nn1\nU n,1\n\u0010\nIk0 + 1\n√n\n˜\nRn,1\n\u0011\nfor some ˜\nRn,1 = OP(1). Therefore,\n˜U\nT\nn,1¯xn −U T\nn,1¯xn =\n1\n√n\n˜\nR\nT\nn,1U T\nn,1¯xn\nso that\n∥˜U\nT\nn,1¯xn −U T\nn,1¯xn∥2 ≤\n1\n√n∥˜\nRn,1∥F · ∥U n,1∥F · ∥¯xn∥2\n= OP\n\u0012∥¯xn∥2\n√n\n\u0013\n.\nA similar argument applies to show that property (3) holds.\n6.4\nThe prediction error estimate\nIn this section, we study the estimate of prediction error\nc\nPEn(k) =\n1\nn2 p2\n∥Xn,22 −ˆ\nXn,22(k)∥2\nF\n120\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nWe can expand this as\nc\nPEn(k) =\n1\nn2 p2\ntr\n\u0010\u0000Xn,22 −ˆ\nXn,22(k)\n\u0001\u0000Xn,22 −ˆ\nXn,22(k)\n\u0001T\u0011\n=\n1\nn2 p2\ntr\n\u0010\u0000√n U n,2DnV T\nn,2 + En,22 −ˆ\nXn,22(k)\n\u0001\n·\n\u0000√n U n,2DnV T\nn,2 + En,22 −ˆ\nXn,22(k)\n\u0001T\u0011\n=\n1\nn2 p2\n\u0012\r\r√n U n,2DnV T\nn,2 −ˆ\nXn,22(k)\n\r\r2\nF\n+ 2 tr\n\u0010\nEn,22\n\u0000√n U n,2DnV T\nn,2 −ˆ\nXn,22(k)\n\u0001T\u0011\n+ ∥En,22∥2\nF\n\u0013\n.\n(6.23)\nIt has expectation\nE\nh\nc\nPEn(k)\ni\n= E\n\u0014\n1\nn2 p2\n\r\r√n U n,2DnV T\nn,2 −ˆ\nXn,22(k)\n\r\r\n\u0015\n+ σ2.\n(6.24)\nThe ﬁrst term is the expected model approximation error and the second term is the\nirreducible error.\nThe expected model approximation error expands into four terms. We have\nE\n\r\r√n U n,2DnV T\nn,2 −ˆ\nXn,22(k)\n\r\r2\nF\n= E\nh\ntr\n\u0010\u0000√n U n,2(Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn)V T\nn,2\n−U n,2DnΘn,1 ˆ\nDn,1(k)+ ˜\nEn,12\n−˜\nEn,21 ˆ\nDn,1(k)+ΦT\nn,1DnV T\nn,2\n−1\n√n\n˜\nEn,21 ˆ\nDn,1(k)+ ˜\nEn,12\n\u0001\n·\n\u0000√n U n,2(Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn)V T\nn,2\n−U n,2DnΘn,1 ˆ\nDn,1(k)+ ˜\nEn,12\n−˜\nEn,21 ˆ\nDn,1(k)+ΦT\nn,1DnV T\nn,2\n−1\n√n\n˜\nEn,21 ˆ\nDn,1(k)+ ˜\nEn,12\n\u0001T\u0011i\n.\n(6.25)\n6.4. THE PREDICTION ERROR ESTIMATE\n121\nBy ﬁrst conditioning on everything but ˜\nEn,12 and ˜\nEn,21, the cross-terms cancel and\nwe get\nE\n\r\r√n U n,2DnV T\nn,2 −ˆ\nXn,22(k)\n\r\r2\nF\n= E\n\r\r√n U n,2(Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn)V T\nn,2\n\r\r2\nF\n+ E\n\r\rU n,2DnΘn,1 ˆ\nDn,1(k)+ ˜\nEn,12\n\r\r2\nF\n+ E\n\r\r ˜\nEn,21 ˆ\nDn,1(k)+ΦT\nn,1DnV T\nn,2\n\r\r2\nF\n+ E\n\r\r 1\n√n\n˜\nEn,21 ˆ\nDn,1(k)+ ˜\nEn,12\n\r\r2\nF.\n(6.26)\nSince ˜\nEn,12 and ˜\nEn,21 are made of iid N(0, σ2) random variables, the last three terms\nare fairly easy to analyze. We can use the following lemma:\nLemma 6.13. Let Z ∈Rm×n be a random matrix with uncorrelated elements, all\nhaving mean 0 and variance 1 (but not necessarily from the same distribution). If\nA ∈Rn×p is independent of Z, then\nE ∥ZA∥2\nF = m · E ∥A∥2\nF .\nProof. The square of the ij element of the product is given by\n(ZA)2\nij =\n\u0010\nn\nX\nα=1\nZiαAαj\n\u00112\n=\nn\nX\nα=1\nZ2\niαA2\nαj +\nX\nα̸=β\nZiαAαjZiβAβj\nThis has expectation\nE\nh\n(ZA)2\nij\ni\n=\nn\nX\nα=1\nE\n\u0002\nA2\nαj\n\u0003\n,\n122\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nso that\nE ∥ZA∥2\nF =\nm\nX\ni=1\np\nX\nj=1\nE\nh\n(ZA)2\nij\ni\n=\nm\nX\ni=1\np\nX\nj=1\nn\nX\nα=1\nE\n\u0002\nA2\nαj\n\u0003\n= m · E ∥A∥2\nF .\nWe also need a technical result to ensure that after appropriate scaling, the ex-\npectations are ﬁnite.\nLemma 6.14. For ﬁxed k as n →∞, ˆd(1)\nn,k is almost surely bounded away from zero.\nProof. We have that ˆd(1)\nn,k is the kth singular value of\n1\n√nXn,11 = U n,1DnV T\nn,1 +\n1\n√nEn,11. This is the same as the kth eigenvalue of\n1\nnXn,11XT\nn,11 = U n,1DnV T\nn,1V n,1DnU T\nn,1 + 1\n√nU n,1DnV T\nn,1ET\nn,11\n+ 1\n√nEn,11V n,1DnU T\nn,1 + 1\nnEn,11ET\nn,11.\nFor each n, we choose On =\n\u0010\nOn,1\nOn,2\n\u0011\n∈Rn×n to be an orthogonal matrix with\nOn,2 ∈Rn×n−k0 and OT\nn,2U n,1 = 0. Then, with λk(·) denoting the kth eigenvalue, we\nhave\nˆd(1)\nn,k = λk\n\u00121\nnXn,11XT\nn,11\n\u0013\n= λk\n\u0012\nOT\nn\n\u00101\nnXn,11XT\nn,11\n\u0011\nOn\n\u0013\n= λk\n  \nAn,11\nAn,12\nAn,21\nAn,22\n!!\n,\nfor An,ij = 1\nnOT\nn,iXT\nn,11Xn,11On,j. Note that An,22 = 1\nnOT\nn,2En,11ET\nn,11On,2 is an (n −\nk) × (n −k) matrix with iid N(0, σ2) entries.\nDeﬁne Gn = λk(An,22) By the eigenvalue interlacing inequality [36], we have that\n6.4. THE PREDICTION ERROR ESTIMATE\n123\nˆd(1)\nn,k ≥Gn. Moreover, Theorems 2.17 and 2.20 give us that Gn\na.s.\n→σ2 \u0010\n1 +\n1\n√γ1\n\u00112\n.\nHence, almost surely for n large enough ˆd(1)\nn,k ≥Gn ≥σ2.\nWith these lemmas, we can prove the following:\nLemma 6.15. The terms in the expected model approximation error converge as:\nE\n\u0014\np\nn2 p2\n\r\r\r\n√n U n,2(Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn)V T\nn,2\n\r\r\r\n2\nF\n\u0015\n→\nk∧k0\nX\ni=1\nµi\n\u0012\n1 −\nr µi\n¯µ1,i\nθ1,iϕ1,i\n\u00132\n+\nk0\nX\ni=k+1\nµi,\n(6.27a)\nE\n\u0014\np\nn2 p2\n\r\r\rU n,2DnΘn,1 ˆ\nDn,1(k)+ ˜\nEn,12\n\r\r\r\n2\nF\n\u0015\n→σ2\nγ ·\nk∧k0\nX\ni=1\nµi\n¯µ1,i\nθ2\n1,i,\n(6.27b)\nE\n\u0014\np\nn2 p2\n\r\r\r ˜\nEn,21 ˆ\nDn,1(k)+ΦT\nn,1DnV T\nn,2\n\r\r\r\n2\nF\n\u0015\n→σ2 ·\nk∧k0\nX\ni=1\nµi\n¯µ1,i\nϕ2\n1,i,\n(6.27c)\nand\nE\n\u0014\np\nn2 p2\n\r\r\r 1\n√n\n˜\nEn,21 ˆ\nDn,1(k)+ ˜\nEn,12\n\r\r\r\n2\nF\n\u0015\n→σ2\nγ ·\nk\nX\ni=1\nσ2\n¯µ1,i\n.\n(6.27d)\nProof. The squared Frobenius norm in the ﬁrst term is equal to\ntr\n\u0010\u0000√n U n,2(Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn)V T\nn,2\n\u0001\n·\n\u0000√n U n,2(Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn)V T\nn,2\n\u0001T\u0011\n= n · tr\n\u0010\u0000Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn\n\u0001\n·\n\u0000V T\nn,2V n,2\n\u0001\n·\n\u0000Dn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn\n\u0001\n·\n\u0000U T\nn,2U n,2\n\u0001\u0011\n.\n124\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nNow, U T\nn,2U n,2\nP→1\nKIk0, V T\nn,2V n,2\nP→1\nLIk0, and\nDn −DnΘn,1 ˆ\nDn,1(k)+Φn,1Dn\nP→diag\n\u0010\nµ1/2\n1\n−µ1/2\n1 θ1,1¯µ−1/2\n1,1 (k) ϕ1,1µ1/2\n1 ,\nµ1/2\n2\n−µ1/2\n2 θ1,2¯µ−1/2\n1,2 (k) ϕ1,2µ1/2\n2 ,\n. . . ,\nµ1/2\nk0 −µ1/2\nk0 θ1,k0 ¯µ−1/2\n1,k0 (k) ϕ1,k0µ1/2\nk0\n\u0011\n,\nwhere\n¯µ1,i(k) =\n\n\n\n¯µ1,i\nif i ≤k,\n0\notherwise.\nWe can apply the Bounded Convergence Theorem to get the result for the ﬁrst\nterm since the elements of U n,2, V n,2, Θn,1 and Φn,1 are bounded by 1 and since\nLemma 6.14 ensures that the elements of ˆ\nDn,1(k)+ are bounded as well.\nThe last three terms can be gotten similarly by applying Lemmas 6.13 and 6.14.\nWe can now get an expression for the limit of the estimated model approximation\nerror. Speciﬁcally,\nE\n\u0014\np\nn2 p2\n\r\r\r\n√n U n,2DnV T\nn,2 −ˆ\nXn,22(k)\n\r\r\r\n2\nF\n\u0015\n=\nk∧k0\nX\ni=1\n(\nµi\n\u0012\n1 −\nr µi\n¯µ1,i\nθ1,iϕ1,i\n\u00132\n+ σ2µi\n¯µi\n\u0000γ−1θ2\n1,i + ϕ2\n1,i\n\u0001\n+\nσ4\nγ¯µ1,i\n)\n+\nk0\nX\ni=k+1\nµi + σ2\nγ ·\nk\nX\ni=k0+1\nσ2\n¯µ1,i\n=\nk∧k0\nX\ni=1\nβi µi +\nk0\nX\ni=k+1\nµi + σ2\nγ\n\u0012\n1 +\n1\n√γ1\n\u0013−2\n· (k −k0)+,\n(6.28)\n6.4. THE PREDICTION ERROR ESTIMATE\n125\nwhere\nβi =\n\u0012\n1 −\nr µi\n¯µ1,i\nθ1,iϕ1,i\n\u00132\n+ σ2\n¯µ1,i\n\u0000γ−1θ2\n1,i + ϕ2\n1,i\n\u0001\n+ 1\nµi\nσ4\nγ¯µ1,i\n= 1 −2\nr µi\n¯µ1,i\nθ1,iϕ1,i + µi\n¯µ1,i\nθ2\n1,iϕ2\n1,i + σ2\n¯µ1,i\n\u0000γ−1θ2\n1,i + ϕ2\n1,i\n\u0001\n+ 1\nµi\nσ4\nγ¯µ1,i\n.\n(6.29)\nIf µ1,i ≤\nσ2\n√γ1, then θ1,i = ϕ1,i = 0 and ¯µ1,i = K−1\nK\n· σ2 \u0010\n1 +\n1\n√γ1\n\u00112\n, so that\nβi = 1 + 1\nγ · σ2\nµi\n·\n\u0012\n1 +\n1\n√γ1\n\u0013−2\n.\nIn the opposite situation (µ1,i >\nσ2\n√γ1), we deﬁne\nρ = L −1\nL\n· K −1\nK\nand get the simpliﬁcations\n¯µ1,i\nµi\n= ρ ·\n\u0012\n1 + (1 + γ−1\n1 ) σ2\nµ1,i\n+ σ4\nγ1µ2\ni\n\u0013\n,\nθ1,iϕ1,i = ρ ·\nr µi\n¯µ1,i\n·\n\u0012\n1 −\nσ4\nγ1µ2\n1,i\n\u0013\n,\nso that\n−2\nr µi\n¯µ1,i\nθ1,iϕ1,i + µi\n¯µi\nθ2\n1,iϕ2\n1,i\n= −ρ2\n\u0012 µi\n¯µ1,i\n\u00132 \u0012\n1 −\nσ4\nγ1µ2\n1,i\n\u0013 \u0012\n1 + 2(1 + γ−1\n1 ) σ2\nµ1,i\n+ 3 σ4\nγ1µ2\n1,i\n\u0013\n,\nσ2\n¯µ1,i\n\u0000γ−1θ2\n1,i + ϕ2\n1,i\n\u0001\n= ρ2\n\u0012 µi\n¯µ1,i\n\u00132 \u0012\n1 −\nσ4\nγ1µ2\n1,i\n\u0013 \u0012\n(1 + γ−1\n1 ) σ2\nµ1,i\n+ 2 σ4\nγ1µ2\n1,i\n\u0013\n,\nand\n1\nµi\nσ4\nγ¯µ1,i\n= ρ2\n\u0012 µi\n¯µ1,i\n\u00132 \u0012 σ4\nγ1µ2\n1,i\n\u0013 \u0012\n1 + (1 + γ−1\n1 ) σ2\nµ1,i\n+\nσ4\nγ1µ2\n1,i\n\u0013\n.\n126\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nPutting it all together, we get\nβi = 1 + ρ2\n\u0012 µi\n¯µ1,i\n\u00132\n·\n\u001a \u0012\n1 −\nσ4\nγ1µ2\n1,i\n\u0013 \u0012\n−1 −(1 + γ−1\n1 ) σ2\nµ1,i\n−\nσ4\nγ1µ2\n1,i\n\u0013\n+\n\u0012 σ4\nγ1µ2\n1,i\n\u0013 \u0012\n1 + (1 + γ−1\n1 ) σ2\nµ1,i\n+\nσ4\nγ1µ2\n1,i\n\u0013 \u001b\n= 1 + ρ2\n\u0012 µi\n¯µ1,i\n\u00132 \u0012\n2 σ4\nγ1µ2\n1,i\n−1\n\u0013 \u0012\n1 + (1 + γ−1\n1 ) σ2\nµ1,i\n+\nσ4\nγ1µ2\n1,i\n\u0013\n= 1 +\n2\nσ4\nγ1µ2\n1,i −1\n1 + (1 + γ−1\n1 ) σ2\nµ1,i +\nσ4\nγ1µ2\n1,i\n=\n3\nσ4\nγ1µ2\n1,i + (1 + γ−1\n1 ) σ2\nµ1,i\n1 + (1 + γ−1\n1 ) σ2\nµ1,i +\nσ4\nγ1µ2\n1,i\n=\nσ2\nγ1µ2\n1,i (3σ2 + (γ1 + 1)µ1,i)\n1 + (1 + γ−1\n1 ) σ2\nµ1,i +\nσ4\nγ1µ2\n1,i\n.\n(6.30)\nIn particular, note that βi < 1 when 2\nσ4\nγ1µ2\n1,i −1 < 0, or equivalently µi >\nσ2\n√γ ·\nq\n2\nρ.\nGetting the ﬁnal expressions for βi and η in Theorem 6.8 is a matter of routine\nalgebra.\n6.5\nSummary and future work\nWe have provided an analysis of the ﬁrst-order behavior of bi-cross-validation. This\nanalysis has shown that BCV gives a biased estimate of prediction error, with an\nexplicit expression for the bias. Fortunately, the bias is not too bad when the signal\nstrength is large and the leave-out sizes are small. Importantly, our analysis gives\nguidance as to how the leave-out sizes should be chosen. Our theoretical analysis\nagrees with the simulations done by Owen & Perry [65], who observed that despite\nbias in prediction error estimates, larger hold-out sizes tend to perform better at\nestimating k∗\nF.\nThe form of consistency we give is rather weak since we did not analyze the\n6.5. SUMMARY AND FUTURE WORK\n127\nvariance of the BCV prediction error estimate. As a follow-up, it would be worthwhile\nto address this limitation. For now, though, we can get some comfort from empirical\nobservations in [65] that the variance of the estimator is not too large. Indeed, based\non these simulations, it is entirely possible that the variance becomes negligible for\nlarge n and p.\n128\nCHAPTER 6. A THEORETICAL ANALYSIS OF BCV\nAppendix A\nProperties of random projections\nWe use this section to present some results about random projection matrices. We\ncall a symmetric p × p matrix P a projection if its eigenvalues are in the set {0, 1}.\nAny projection matrix of rank k ≤p can be decomposed as P = V V T for some V\nsatisfying V TV = Ik. We call the set\nVk (Rp) =\n\b\nV ∈Rp×k : V TV = Ik\n\t\n⊆Rp×k\n(A.1)\nthe rank-k Stiefel manifold of Rp. It is the set of orthonormal k-frames in Rp. Simi-\nlarly, we call the set\nGk (Rp) =\n\b\nV V T : V ∈Vk (Rp)\n\t\n⊆Rp×p\n(A.2)\nthe rank-k Grassmannian of Rp; this is the set of rank k projection matrices.\nIf\n\u0010\nV\n¯V\n\u0011\nis an p × p Haar-distributed orthogonal matrix and V is p × k, then we say\nthat V is uniformly distributed over Vk(Rp) and that V V T is uniformly distributed\nover Gk(Rp).\nA.1\nUniformly distributed orthonormal k-frames\nWe ﬁrst present some results about a matrix V distributed uniformly over Vk(Rp).\nWe denote this distribution by V ∼Unif\n\u0000Vk(Rp)\n\u0001\n. In the special case of k = 1, the\n129\n130\nAPPENDIX A. PROPERTIES OF RANDOM PROJECTIONS\ndistribution is equivalent to drawing a random vector uniformly from the unit sphere\nin Rp. We denote this distribution by ¯V ∼Unif\n\u0000Sp−1\u0001\n.\nA.1.1\nGenerating random elements\nThe easiest way to generate a random element of Vk(Rp) is to let Z be a p×k matrix\nof iid N(0, 1) random variables, take the QR decomposition Z = QR, and let V be\nequal to the ﬁrst k columns of Q. In practice, there is a bias in the way standard\nQR implementations choose the signs of the columns of Q. To get around this, we\nrecommend using Algorithm A.1, below.\nAlgorithm A.1 Generate a random orthonormal k-frame\n1. Draw Z, a random p×k matrix whose elements are iid N(0, 1) random variables.\n2. Compute Z = QR, the QR-decomposition of Z. Set Q1 to be the p×k matrix\ncontaining the ﬁrst k columns of Q.\n3. Draw S, a random k × k diagonal matrix with iid diagonal entries such that\nP {S11 = −1} = P {S11 = +1} = 1\n2.\n4. Return V = Q1S.\nThis algorithm has a time complexity of O (pk2). Diaconis and Shahshahani [23]\npresent an alternative approach called the subgroup algorithm which can be used to\ngenerate V as a product of k Householder reﬂections. Their algorithm has time com-\nplexity O (pk). Mezzadri [60] gives a simple description of the subgroup algorithm.\nA.1.2\nMixed moments\nSince we can ﬂip the sign of any row or column of V and not change its distribution,\nthe mixed moments of the elements of V vanish unless the number of elements from\nany row or column is even (counting multiplicity). For example, E [V 2\n11V21] = 0 since\nwe can ﬂip the sign of the second row of V to get\nV 2\n11V21\nd= V 2\n11(−V21) = −V 2\n11V21.\nA.1. UNIFORMLY DISTRIBUTED ORTHONORMAL K-FRAMES\n131\nThis argument does not apply to V11V12V22V21 or V 2\n11V 2\n22.\nIn the special case when k = 1, the vector\n\u0000V 2\n11, V 2\n21, . . . , V 2\np1\n\u0001\nis distributed as\nDirichlet\n\u0000 1\n2, 1\n2, . . . , 1\n2\n\u0001\n. This follows from the fact that if Y1, Y2, . . . , Yp are indepen-\ndently distributed Gamma random variables and Yi has shape ai and scale s, then\nwith S = Pp\ni=1 Yi, the vector\n\u0010\nY1\nS , Y2\nS , . . . , Yp\nS\n\u0011\nis distributed Dirichlet (a1, a2, . . . , ap).\nUsing the standard formulas for Dirichlet variances and covariances, we get the mixed\nmoments up to fourth order. They are summarized in the next lemma.\nLemma A.1. If V ∼Unif\n\u0000V1(Rp)\n\u0001\n, then\nE\n\u0002\nV 2\n11\n\u0003\n= 1\np,\n(A.3a)\nE [V11V21] = 0,\n(A.3b)\nE\n\u0002\nV 4\n11\n\u0003\n=\n3\np (p + 2),\n(A.3c)\nE\n\u0002\nV 2\n11V 2\n21\n\u0003\n=\n1\np (p + 2).\n(A.3d)\nThe odd mixed moments are all equal to zero.\nUsing Theorem 4 from Diaconis and Shahshahani [24], which gives the moments\nof the traces of Haar-distributed orthogonal matrices, we can compute the mixed\nmoments of V for more general k. Meckes [59] gives an alternative derivation of\nthese results.\nLemma A.2. If V ∼Unif\n\u0000Vk(Rp)\n\u0001\nand k > 1, then the nonzero mixed moments of\n132\nAPPENDIX A. PROPERTIES OF RANDOM PROJECTIONS\nthe elements V up to fourth order are deﬁned by\nE\n\u0002\nV 2\n11\n\u0003\n= 1\np,\n(A.4a)\nE\n\u0002\nV 4\n11\n\u0003\n=\n3\np (p + 2),\n(A.4b)\nE\n\u0002\nV 2\n11V 2\n21\n\u0003\n=\n1\np (p + 2),\n(A.4c)\nE\n\u0002\nV 2\n11V 2\n22\n\u0003\n=\np + 1\np (p −1)(p + 2),\n(A.4d)\nE [V11V12V22V21] =\n−1\np (p −1)(p + 2).\n(A.4e)\nProof. The ﬁrst three equations follow directly from the previous lemma. We can\nget the other moments from the moments of O, a Haar-distibuted p × p orthogonal\nmatrix. For the fourth equation, we use that\nE [tr(O)]4 =\nX\nr,s,t,u\nE [OrrOssOttOuu] .\nOnly the terms with even powers of Oii are nonzero. Thus, we have\nE [tr(O)]4 =\n\u0012p\n1\n\u0013\nE\n\u0002\nO4\n11\n\u0003\n+\n\u0012p\n2\n\u0013\u00124\n2\n\u0013\nE\n\u0002\nO2\n11O2\n22\n\u0003\n= p E\n\u0002\nO4\n11\n\u0003\n+ 3p (p −1) E\n\u0002\nO2\n11O2\n22\n\u0003\n.\nTheorem 4 of Diaconis and Shahshahani [24] gives that E [tr(O)]4 = 3. Combined\nwith Lemma A.1, we get that\nE\n\u0002\nO2\n11O2\n22\n\u0003\n=\n1\n3p (p −1)\n\b\nE [tr(O)]4 −p E\n\u0002\nO4\n11\n\u0003\t\n=\n1\n3p (p −1)\n\u001a\n3 −p ·\n3\np (p + 1)\n\u001b\n=\np + 1\np (p −1)(p + 2).\nA.1. UNIFORMLY DISTRIBUTED ORTHONORMAL K-FRAMES\n133\nFor the last equation, we use E\n\u0002\ntr(O4)\n\u0003\n. We have that\n(O4)ij =\nX\nr,s,t\nOirOrsOstOtj.\nWe would like to compute E\n\u0002\n(O4)11\n\u0003\n. Note that unless r = s = t = 1, there are only\nthree situations when E [O1rOrsOstOt1] ̸= 0. Two of them are demonstrated visually\nby the conﬁgurations\n1\ns\n1\ns\n\n\n\n\n\n\n\nO1r\n· · ·\nOrs\n· · ·\n...\n...\n...\nOt1\n· · ·\nOst\n· · ·\n...\n...\n\n\n\n\n\n\n\nr = 1, s = t, s ̸= 1\nand\n1\nr\n1\nr\n\n\n\n\n\n\n\nOt1\n· · ·\nO1r\n· · ·\n...\n...\n...\nOst\n· · ·\nOrs\n· · ·\n...\n...\n\n\n\n\n\n\n\nt = 1, r = s, r ̸= 1\n.\nThe other nonzero term is when s = 1, r = t, and r ̸= 1, so that O1rOrsOstOt1 =\nO2\n1rO2\nr1. In all other conﬁgurations there is a row or a column that only contains\none of {O1r, Ors, Ost, Ot1}. Since we can multiply a row or a column of O by −1\nand not change the distribution of O, for this choice of r, s, and t we must have\nO1rOrsOstOt1\nd= −O1rOrsOstOt1. This in turn implies that E [O1rOrsOstOt1] = 0.\nWith these combinatorics in mind, we have that\nE\n\u0002\n(O4)11\n\u0003\n=\nX\ns̸=1\nE [O11O1sOssOs1] +\nX\nr̸=1\nE [O1rOrrOr1O11] +\nX\nr̸=1\nE\n\u0002\nO2\n1rO2\nr1\n\u0003\n+ E\n\u0002\nO4\n11\n\u0003\n= 2(p −1)E [O11O12O22O21] + (p −1)E\n\u0002\nO2\n12O2\n21\n\u0003\n+ E\n\u0002\nO4\n11\n\u0003\n= 2(p −1)E [O11O12O22O21] + (p −1)E\n\u0002\nO2\n11O2\n22\n\u0003\n+ E\n\u0002\nO4\n11\n\u0003\nAgain applying Theorem 4 of [24], we have that E\n\u0002\ntr(O4)\n\u0003\n= 1. Combined with\n134\nAPPENDIX A. PROPERTIES OF RANDOM PROJECTIONS\nLemma A.1, we have\nE [O11O12O22O21] =\n1\n2(p −1)\n\u001a1\npE\n\u0002\ntr(O4)\n\u0003\n−E\n\u0002\nO4\n11\n\u0003\n−(p −1)E\n\u0002\nO2\n11O2\n22\n\u0003\u001b\n=\n1\n2(p −1)\n\u001a1\np −\n3\np (p + 2) −(p −1)\np + 1\np (p −1)(p + 2)\n\u001b\n=\n−1\np (p −1)(p + 2).\nA.2\nUniformly distributed projections\nWhen P ∈Rp×p is chosen uniformly over the set of rank-k p × p projection matrices,\nwe say P ∼Unif\n\u0000Gk(Rp)\n\u0001\n. With the results of the previous section, we can derive the\nmoments of random projection matrices.\nLemma A.3. If P ∼Unif\n\u0000Gk(Rp)\n\u0001\n, then\nE [P ] = k\npIp.\n(A.5)\nProof. Write P = V V T, where V ∼Unif\n\u0000Vk(Rp)\n\u0001\n. For 1 ≤i, j ≤p we have\nPij =\nk\nX\nr=1\nVirVjr,\nso\nE [Pij] =\n\n\n\nk E [V 2\n11] ,\nwhen i = j,\nk E [V11V21] ,\notherwise.\nThe result now follows from Lemma A.1.\nLemma A.4. Let P ∼Unif\n\u0000Gk(Rp)\n\u0001\n. If 1 ≤i, j, i′, j′ ≤p, then\nCov\n\u0002\nPij, Pi′j′\u0003\n=\n1\n(p −1)(p + 2)\n\u0012k\np\n\u0013 \u0012\n1 −k\np\n\u0013\n·\n\u0010\np δ(i,j)=(i′,j′) + p δ(i,j)=(j′,i′) −2 δ(i,i′)=(j,j′)\n\u0011\n.\n(A.6)\nA.2. UNIFORMLY DISTRIBUTED PROJECTIONS\n135\nThis gives us that aside from the obvious symmetry (Pij = Pji), the oﬀ-diagonal\nelements of P are uncorrelated with each other and with the diagonal elements.\nProof. We need to perform six computations. As before, we use the representation\nP = V V T, where V ∼Unif\n\u0000Vk(Rp)\n\u0001\n. We have\nE\n\u0002\nP 2\n11\n\u0003\n= E\n\u0014\u0010\nk\nX\ni=1\nV 2\n1i\n\u00112\u0015\n= E\n\u0014\nk\nX\ni=1\nV 4\n1i +\nX\ni̸=j\nV 2\n1iV 2\n1j\n\u0015\n= k ·\n3\np (p + 2) + k (k −1) ·\n1\nn (p + 2)\n= k (k + 2)\np (p + 2)\n=\n2\np + 2\n\u0012k\np\n\u0013 \u0012\n1 −k\np\n\u0013\n+\n\u0012k\np\n\u00132\n,\nwhich gives us that\nVar [P11] =\n2\np + 2\n\u0012k\np\n\u0013 \u0012\n1 −k\np\n\u0013\n.\nNext,\nE\n\u0002\nP 2\n12\n\u0003\n= E\n\u0014\u0010\nk\nX\ni=1\nV1iV2i\n\u00112\u0015\n= E\n\u0014\nk\nX\ni=1\nV 2\n1iV 2\n2i +\nX\ni̸=j\nV1iV2iV1jV2j\n\u0015\n= k ·\n1\np (p + 2) + k (k −1) ·\n−1\np (p −1)(p + 2)\n=\np\n(p −1)(p + 2)\n\u0012k\np\n\u0013 \u0012\n1 −k\np\n\u0013\n,\nso that\nVar [P12] =\np\n(p −1)(p + 2)\n\u0012k\np\n\u0013 \u0012\n1 −k\np\n\u0013\n.\n136\nAPPENDIX A. PROPERTIES OF RANDOM PROJECTIONS\nAlso,\nE [P11P22] = E\n\u0014\u0010\nk\nX\ni=1\nV 2\n1i\n\u0011\u0010\nk\nX\nj=1\nV 2\n2j\n\u0011\u0015\n= E\n\u0014\nk\nX\ni=1\nV 2\n1iV 2\n2i +\nk\nX\ni=1\nX\nj̸=i\nV 2\n1iV 2\n2j\n\u0015\n= k ·\n1\np (p + 2) + k (k −1) ·\np + 1\np (p −1)(p + 2)\n=\nk\np (p + 2)\n\u0012\n1 + (p + 1)k −1\np −1\n\u0013\n=\n−2\n(p −1)(p + 2)\n\u0012k\np\n\u0013 \u0012\n1 −k\np\n\u0013\n+\n\u0012k\np\n\u00132\n,\nso that\nCov [P11, P22] =\n−2\n(p −1)(p + 2)\n\u0012k\np\n\u0013 \u0012\n1 −k\np\n\u0013\n.\nSince P is symmetric, we have\nCov [P12, P21] = Var [P12] .\nThe other covariances are all zero. This is because\nE [P11P12] = E\n\u0014 X\ni,j\nV 2\n1i V1jV2j\n\u0015\n,\nE [P12P23] = E\n\u0014 X\ni,j\nV1iV2iV2jV3j\n\u0015\n,\nand\nE [P12P34] = E\n\u0014 X\ni,j\nV1iV2iV3jV4j\n\u0015\n.\nEach term in these sums has an element that appears only once in a row. Thus, the\nexpectations are all 0.\nA.3. APPLICATIONS\n137\nA.3\nApplications\nWe now present two applications of the results in this section.\nA.3.1\nProjections of orthonormal k-frames\nSuppose we have U ∈Rp×k, an orthonormal k-frame, and we randomly project the\ncolumns of U into Rq, with q < p. If we denote the projection matrix by V T and set\n˜U = V TU, it is natural to ask how close ˜U is to being an orthonormal k-frame. We\ncan prove the following:\nProposition A.5. Suppose U ∈Rp×k satisﬁes U TU = Ik. Let V ∼Unif\n\u0000Vq(Rp)\n\u0001\nwith k ≤q ≤p and set ˜U =\np\np/qV TU. Then there exists a decomposition ˜U =\n˜U 0 +\n1\n√q ˜U 1 such that ˜U\nT\n0 ˜U 0 = Ik and\nE∥˜U 1∥2\nF ≤1\n2 k (k + 1)\n\u0012q\np\n\u00132 \u0012\n1 −q\np\n\u0013\n.\n(A.7)\nIn particular, this implies that E∥˜U 1∥2\nF ≤\n2\n27 k (k + 1).\nThe main ingredients of the proof are a perturbation lemma and a result about ˜U\nT ˜U,\nstated below.\nLemma A.6. Suppose U ∈Rp×k satisﬁes U TU = Ik. Let V ∼Unif\n\u0000Vq(Rp)\n\u0001\nwith\nk ≤q ≤p and set ˜U =\np\np/qV TU. Then E\nh\n˜U\nT ˜U\ni\n= Ik and\nE\n\r\r\r√q\n\u0000 ˜U\nT ˜U −Ik\n\u0001\r\r\r\n2\nF ≤k (k + 1)\n\u0012q\np\n\u00132 \u0012\n1 −q\np\n\u0013\n,\n(A.8a)\nwith a matching lower bound of\nE\n\r\r\r√q\n\u0000 ˜U\nT ˜U −Ik\n\u0001\r\r\r\n2\nF ≥k (k + 1)\n\u0012q\np\n\u00132 \u0012\n1 −q\np\n\u0013 \u0012\np\np + 2\n\u0013\n.\n(A.8b)\n138\nAPPENDIX A. PROPERTIES OF RANDOM PROJECTIONS\nProof. Set P = V V T so that ˜U\nT ˜U = p\nq U TP U. Now,\nE\nh\n˜U\nT ˜U\ni\n= p\nq U TE [P ] U = Ik.\nAlso, since OTP O\nd= P for any p × p orthogonal matrix O, we must that U TP U\nhas the same distribution as the upper k × k submatrix of P . This implies that\nE\n\r\r\r ˜U\nT ˜U −Ik\n\r\r\r\n2\nF =\nk\nX\ni=1\nk\nX\nj=1\nVar [P ij]\n= q\np\n\u0012\n1 −q\np\n\u0013 \u001a\nk ·\n2\np + 2 + k (k −1) ·\np\n(p −1)(p + 2)\n\u001b\n=\np k (k + 1)\n(p −1)(p + 2)\n\u0012q\np\n\u0013 \u0012\n1 −q\np\n\u0013 \u0012\n1 −\n2\np (k + 1)\n\u0013\n.\nThe lower and upper bounds follow.\nThe next ingredient is a perturbation theorem due to Mirsky, which we take from\nStewart [84] and state as a lemma.\nLemma A.7 (Mirsky). If A and A + E are in Rn×p, then\nn∧p\nX\ni=1\n\u0000σi(A + E) −σi(A)\n\u00012 ≤∥E∥2\nF,\n(A.9)\nwhere σi(·) denotes the ith singular value.\nWe can now proceed to the rest of the proof.\nProof of Proposition A.5. We have that ˜U\nT ˜U = Ik + E, where E∥√qE∥2\nF ≤C and\nC is given in Lemma A.6. We can apply Lemma A.7 to get\nk\nX\ni=1\n\u0000σi( ˜U\nT ˜U) −1\n\u00012 ≤∥E∥2\nF.\nSetting εi = σi( ˜U\nT ˜U) −1, we have σi( ˜U) = √1 + εi.\nNote that E [q P\ni ε2\ni ] ≤\nE∥√q E∥2\nF ≤C. Let R and S be p × k and k × k matrices with orthonormal columns\nA.3. APPLICATIONS\n139\nsuch that\n˜U = R(Ik + ∆)ST\nis the SVD of ˜U, where ∆= diag(∆1, ∆2, . . . , ∆k), and ∆i = √1 + εi −1. Set\n˜U 0 = RST and ˜U 1 = √q R∆ST. Then,\n˜U\nT\n0 ˜U 0 = SRTRST = SST = Ik\nand\nE∥˜U 1∥2\nF =\nk\nX\ni=1\nE\n\u0002\nq ∆2\ni\n\u0003\n≤1\n2\nk\nX\ni=1\nE\n\u0002\nq ε2\ni\n\u0003\n≤C\n2 ,\nwhere we have used that √1 + εi ≥1 + 1\n2εi −1\n2ε2\ni .\nA.3.2\nA probabilistic interpretation of the Frobenius norm\nAs another application of the results in this section, we give a probabilistic represen-\ntation of the Frobenius norm of a matrix. It is commonly known that for any n × p\nmatrix A,\nsup\n∥¯x∥2=1,\n∥\n¯\ny∥2=1\n\u0000¯xTA\n¯\ny\n\u00012 = ∥A∥2\n2,\n(A.10)\nwhere ∥· ∥2 is the spectral norm, equal largest singular value (see, e.g. [36]). The\nfunction f(¯x,\n¯\ny) = ¯xTA\n¯\ny is a general bilinear form on Rn × Rp. The square of the\nspectral norm of A gives the maximum value of\n\u0000f(¯x,\n¯\ny)\n\u00012 when ¯x and\n¯\ny are both\nunit vectors. It turns out that the Frobenius norm of A is related to the average\nvalue of\n\u0000f(¯X, ¯Y )\n\u00012 when ¯X and ¯Y are random unit vectors.\nProposition A.8. If A ∈Rn×p, then\nZ\n∥¯x∥2=1,\n∥\n¯\ny∥2=1\n\u0000¯xTA\n¯\ny\n\u00012 d¯x d\n¯\ny = 1\nnp∥A∥2\nF.\n(A.11)\nProof. There are two steps to the proof. First, we show that if ¯X ∼Unif\n\u0000Sn−1\u0001\nand\n140\nAPPENDIX A. PROPERTIES OF RANDOM PROJECTIONS\n¯a is arbitrary, then\nE\n\u0002\n¯XT\n¯a\n\u00032 = 1\nn∥¯a∥2\n2.\nNext, we show that if ¯Y ∼Unif\n\u0000Sp−1\u0001\n, then\nE∥A ¯Y ∥2\n2 = 1\np∥A∥2\nF.\nThe result follows from these two facts. To see the ﬁrst part, since ¯X is orthogonally\ninvariant we have\n¯XT\n¯a\nd= ¯XT (∥¯a∥2¯e1) = ∥¯a∥2 ¯X1.\nTo see the second part, let A = UΣV T be the SVD of A and write\n∥A ¯Y ∥2\n2 = ∥ΣV T\n¯Y ∥2\n2\nd= ∥Σ¯Y ∥2\n2 =\nn∧p\nX\ni=1\nσ2\ni (A) Y 2\ni .\nAppendix B\nLimit theorems for weighted sums\nIn this appendix we state and prove some limit theorems for weighted sums of iid\nrandom variables. First, we give a weak law of large numbers (WLLN):\nProposition B.1 (Weighted WLLN). Let Xn,1, Xn,2, . . . , Xn,n be a triangular ar-\nray of random variables, iid across each row, with EXn,1 = µ and EX2\nn,1 uniformly\nbounded in n.\nAlso, let Wn,1, Wn,2, . . . , Wn,n be another triangular array of ran-\ndom variables independent of the Xn,i (but not necessarily of each other). Deﬁne\n¯Wn = 1\nn\nPn\ni=1 Wn,i. If ¯Wn\nP→¯W and E\n\u0002 1\nn\nPn\ni=1 W 2\nn,i\n\u0003\nis uniformly bounded in n, then\n1\nn\nn\nX\ni=1\nWn,i Xn,i\nP→¯Wµ.\n(B.1)\nWe do not give a proof of Proposition B.1, but instead derive a strong law of large\nnumbers (SLLN) below. The proof of the weak law is similar. Here is the strong law:\nProposition B.2 (Weighted SLLN). Let Xn,1, Xn,2, . . . , Xn,n be a triangular array of\nrandom variables, iid across each row, with EXn,1 = µ and EX4\nn,1 uniformly bounded\nin n. Also, let Wn,1, Wn,2, . . . , Wn,n be another triangular array of random variables\nindependent of the Xn,i (but not necessarily of each other). Deﬁne ¯Wn = 1\nn\nPn\ni=1 Wn,i.\nIf ¯Wn\na.s.\n→¯W and E\n\u0002 1\nn\nPn\ni=1 W 4\nn,i\n\u0003\nis uniformly bounded in n, then\n1\nn\nn\nX\ni=1\nWn,i Xn,i\na.s.\n→¯Wµ.\n(B.2)\n141\n142\nAPPENDIX B. LIMIT THEOREMS FOR WEIGHTED SUMS\nProof. Deﬁne Sn = Pn\ni=1 Wn,i Xn,i and let FW\nn\n= σ(Wn,1, Wn,2, . . . , Wn,n). We have\nthat\n1\nnSn −¯Wn µ = 1\nn\nn\nX\ni=1\nWn,i (Xn,i −µ)\nNote that\nE\n\"\n1\nn2\n n\nX\ni=1\nWn,i (Xn,i −µ)\n!4 \f\f\f\f\f FW\nn\n#\n= 1\nn2\n \nE [Xn,1 −µ]4\nn\nX\ni=1\nW 4\nn,i + 3 E\n\u0002\n(Xn,1 −µ)2(Xn,2 −µ)2\u0003 X\ni̸=j\nW 2\nn,iW 2\nn,j\n!\n≤C1\n\"\n1\nn\nn\nX\ni=1\nW 2\nn,i\n#2\nfor some constant C1 bounding E [Xn,1 −µ]4 and 3 E [(Xn,1 −µ)2(Xn,2 −µ)2]. There-\nfore, the full expectation is bounded by some other constant C2. We have just shown\nthat\nE\n\u00141\nnSn −¯Wn µ\n\u00154\n≤C\nn2\nfor some constant C. Applying Chebyschev’s inequality, we get\nP\n\u001a\f\f\f\f\n1\nnSn −¯Wn µ\n\f\f\f\f > ε\n\u001b\n≤\nC\nn2ε4.\nInvoking the ﬁrst Borel-Cantelli Lemma, see the sum converges almost surely.\nNext, we derive a central limit theorem (CLT). To prove it we will need a CLT\nfor dependent variables, which we take from McLeish [58]:\nLemma B.3. Let Xn,i, Fn,i, i = 1, . . . , n be a martingale diﬀerence array.\nIf the\nLindeberg condition Pn\ni=1 E\n\u0002\nX2\nn,i; |Xn,i| > ε\n\u0003\n→0 is satisﬁed and Pn\ni=1 X2\nn,i\nP→σ2\nthen Pn\ni=1 Xn,i\nd→N(0, σ2).\nWith this Lemma, we can prove a CLT for weighted sums.\nProposition B.4 (Weighted CLT). Let ¯Xn,i, i = 1, . . . , n be a triangular array of\nrandom vectors in Rp with ¯Xn,i iid such that E¯Xn,1 =\n¯\nµX, Cov[¯Xn,1] = ΣX, and all\n143\nmixed fourth moments of the elements of ¯Xn,1 are uniformly bounded in n. Let ¯Wn,i,\ni = 1, . . . , n be another triangular array of random vectors in Rp, independent of the\n¯Xn,i but not necessarily of each other. Assume that\n1\nn\nPn\ni=1 ¯Wn,i ¯W T\nn,i\nP→ΣW, and\nthat for all sets of indices j1, j2, j3, j4, with each index between 1 and p, we have that\nE\n\u0002 1\nn\nPn\ni=1 Wn,ij1Wn,ij2Wn,ij3Wn,ij4\n\u0003\nis uniformly bounded in n. Then\n√n\n\"\n1\nn\nn\nX\ni=1 ¯Wn,i • ¯Xn,i −1\nn\nn\nX\ni=1 ¯Wn,i •\n¯\nµX\n#\nd→N\n\u00000, ΣW • ΣX\u0001\n,\n(B.3)\nwhere • denotes Hadamard (elementwise) product.\nProof. Let\n¯Sn = √n\n\"\n1\nn\nn\nX\ni=1 ¯Wn,i • ¯Xn,i −1\nn\nn\nX\ni=1 ¯Wn,i •\n¯\nµX\n#\n.\nWe will use the Cram´er-Wold device. Let ¯θ ∈Rp be arbitrary. Deﬁne\nYn,i =\n1\n√n\np\nX\nj=1\nθjWn,ij (Xn,ij −µX\nj ),\nso that ¯θT\n¯Sn = Pn\ni=1 Yn,i. Also, with Fn,i = σ(Yn,1, Yn,2, . . . , Yn,i−1), the collection\n{Yn,i, Fn,i} is a martingale diﬀerence array. We will use Lemma B.3 to prove the\nresult. First, we compute the variance as\nn\nX\ni=1\nY 2\nn,i = 1\nn\nn\nX\ni=1\n\"\np\nX\nj=1\nθjWn,ij (Xn,ij −µX\nj )\n#2\n= 1\nn\nn\nX\ni=1\nn\nX\nj=1\np\nX\nk=1\nθjθkWn,ijWn,ik(Xn,ij −µX\nj )(Xn,ik −µX\nk )\nP→\np\nX\nj=1\np\nX\nk=1\nθjθkΣW\njkΣX\njk\n= ¯θT \u0000ΣW • ΣX\u0001\n¯θ,\nwhere we have used the fourth moment assumptions and Proposition B.1 to get the\n144\nAPPENDIX B. LIMIT THEOREMS FOR WEIGHTED SUMS\nconvergence. Lastly we check the Lindeberg condition. We have that\nn\nX\ni=1\nE\n\u0002\nY 2\nn,i; |Yn,i| > ε\n\u0003\n≤1\nε2\nn\nX\ni=1\nE\n\u0002\nY 4\nn,i\n\u0003\n=\n1\nε2n2\nn\nX\ni=1\nX\nj1,...,j4\nn\nθj1θj2θj3θj4 · E [Wn,ij1Wn,ij2Wn,ij3Wn,ij4]\n· E\nh\n(Xn,ij1 −µX\nj1)(Xn,ij2 −µX\nj2)(Xn,ij3 −µX\nj3)(Xn,ij4 −µX\nj4)\nio\n≤C1\nε2n\nX\nj1,...,j4\nE\n\"\n1\nn\nn\nX\ni=1\nWn,ij1Wn,ij2Wn,ij3Wn,ij4\n#\n≤C1C2p4\nε2n\n→0\nwhere C1 is a constant bounding |θj|4 and the centered fourth moments of Xn,ij, and\nC2 bounds the fourth moments of the weights.\nFor some classes of weights, we can get a stronger result.\nCorollary B.5. With the same assumptions as in Proposition B.4, if the mean of\nthe weights converges suﬃciently fast as √n\n\u0002 1\nn\nPn\ni=1 ¯Wn,i −\n¯\nµW\u0003 P→0, then\n√n\n\"\n1\nn\nn\nX\ni=1 ¯Wn,i • ¯Xn,i −\n¯\nµW •\n¯\nµX\n#\nd→N\n\u00000, ΣW • ΣX\u0001\n.\n(B.4)\nReferences\n[1] T.W. Anderson.\nAsymptotic theory for principal component analysis.\nAnn.\nMath. Stat., 34(1):122–148, 1963.\n[2] J. Bai and S. Ng.\nDetermining the number of factors in approximate factor\nmodels. Econometrica, 70(1):191–221, 2002.\n[3] Z.D. Bai, B.Q. Miao, and G.M. Pan. On asymptotics of eigenvectors of large\nsample covariance matrix. Ann. Probab., 35(4):1532–1572, 2007.\n[4] Z.D. Bai, B.Q. Miao, and J.F. Yao.\nConvergence rates of the spectral dis-\ntributions of large sample covariance matrices. SIAM J. Matrix Anal. Appl.,\n25(1):105–127, 2003.\n[5] Z.D. Bai and J.W. Silverstein. No eigenvalues outside the support of the limit-\ning spectral distribution of large dimensional sample covariance matrices. Ann.\nProbab., 26(1):316–345, 1998.\n[6] Z.D. Bai and J.W. Silverstein.\nCLT for linear spectral statistics of large-\ndimensional sample covairance matrices. Ann. Probab., 32(1A):553–605, 2004.\n[7] Z.D. Bai, J.W. Silverstein, and Y.Q. Yin. A note on the largest eigenvalue of a\nlarge dimensional sample covariance matrix. J. Multivariate Anal., 26(2):166–\n168, 1988.\n[8] Z.D. Bai and J.F. Yao. Central limit theorems for eigenvalues in a spiked popu-\nlation model. Ann. I. H. Poincar´e – PR, 44(3):447–474, 2008.\n145\n146\nREFERENCES\n[9] Z.D. Bai and J.F. Yao. Limit theorems for sample eigenvalues in a generalized\nspiked population model. arXiv:0806.1141v1 [math.ST], 2008.\n[10] Z.D. Bai and Y.Q. Yin. Limit of the smallest eigenvalue of a large dimensional\nsample covariance matrix. Ann. Probab., 21(3):1275–1294, 1993.\n[11] J. Baik, G. Ben Arous, and S. P´ech´e. Phase transition of the largest eigenvalue\nfor nonnull complex sample covariance matrices. Ann. Probab., 33(5):1643–1697,\n2005.\n[12] J. Baik and J.W. Silverstein. Eigenvalues of large sample covariance matrices of\nspiked population models. J. Multivariate Anal., 97(6):1382–1408, 2006.\n[13] T.H. Baker, P.J. Forrester, and P.A. Pearce. Random matrix ensembles with\nan eﬀective extensive external charge. J. Phys. Math. Gen., 31(29):6087–6102,\n1998.\n[14] J. C. Bezdek. Fuzzy Mathematics in Pattern Classiﬁcation. PhD thesis, Cornell\nUniversity, 1973.\n[15] L. Breiman. The little bootstrap and other methods for dimensionality selection\nin regression: X-ﬁxed prediction error. J. Am. Stat. Assoc., 87(419):738–754,\n1992.\n[16] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression\nTrees. Wadsworth, 1984.\n[17] L. Breiman and P. Spector. Submodel selection and evaluation in regression.\nThe X-random case. Int. Stat. Rev., 60(3):291–319, 1992.\n[18] R.B. Cattell. The scree test for the number of factors. Multivariate Behav. Res.,\n1(2):245–276, 1966.\n[19] K. Chen, D. Paul, and J.L. Wang. Properties of principal component analysis\nfor correlated data. Unpublished manuscript, 2009.\nREFERENCES\n147\n[20] P. Craven and G. Wahba. Smoothing noisy data with spline functions: estimating\nthe correct degree of smoothing by the method of generalized cross-validation.\nNumer. Math., 31(4):377–403, 1979.\n[21] A. Cutler and L. Breiman. Archetypal analysis. Technometrics, 36(4):338–347,\n1994.\n[22] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incom-\nplete data via the EM algorithm. J. Roy. Stat. Soc. B, 39(1):1–38, 1977.\n[23] P. Diaconis and M. Shahshahani. The subgroup algorithm for generating uniform\nrandom variables. Prob. Eng. Inform. Sc., 1(1):15–32, 1987.\n[24] P. Diaconis and M. Shahshahani. On the eigenvalues of random matrices. J.\nAppl. Probab., 31A:49–62, 1994.\n[25] P. Drineas, R. Kannan, and M.W. Mahoney. Fast Monte Carlo algorithms for\nmatrices III: Computing a compressed approximate matrix decomposition. SIAM\nJ. Comput., 36(1):184–206, 2006.\n[26] A. Edelman. Eigenvalues and condition numbers of random matrices. SIAM J.\nMatrix Anal. Appl., 9(4):543–560, 1988.\n[27] B. Efron. Are a set of microarrays independent of each other?\nUnpublished\nmanuscript, 2008.\n[28] N. El Karoui. On the largest eigenvalue of Wishart matrices with identity co-\nvariance when n, p and p/n tend to inﬁnity. arXiv:math/0309355v1 [math.ST],\n2003.\n[29] N. El Karoui. A rate of convergence result for the largest eigenvalue of complex\nwhite Wishart matrices. Ann. Probab., 34(6):2077–2117, 2006.\n[30] N. El Karoui. Tracy-Widom limit for the largest eigenvalue of a large class of\ncomplex sample covariance matrices. Ann. Probab., 35(2):663–714, 2007.\n148\nREFERENCES\n[31] K. Faber and B.R. Kowalski. Critical evaluation of two F-tests for selecting the\nnumber of factors in abstract factor analysis. Anal. Chim. Acta, 337(1):57–71,\n1997.\n[32] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford\nUniversity, 2002.\n[33] R.A. Fisher and W.A. Mackenzie. Studies in crop variation. II. The manurial\nresponse of diﬀerent potato varieties. J. Agric. Sci., 13:311–320, 1923.\n[34] S. Geman. A limit theorem for the norm of random matrices. Ann. Probab.,\n8(2):252–261, 1980.\n[35] G.H. Golub, M. Heath, and G. Wahba. Generalized cross-validation as a method\nfor choosing a good ridge parameter. Technometrics, 21(2):215–223, 1979.\n[36] G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University\nPress, 1996.\n[37] U. Grenander and J.W. Silverstein. Spectral analysis of networks with random\ntopologies. SIAM J. Appl. Math., 32(2):499–519, 1977.\n[38] A. Guionnet. Large deviations and stochastic calculus for large random matrices.\nProbab. Surv., 1:72–172, 2004.\n[39] A. Guionnet and O. Zeitouni. Concentration of the spectral measure for large\nmatrices. Electro. Comm. Probab., 5:119–136, 2000.\n[40] S. P. Hastings and J. B. McLeod. A boundary value problem associated with\nthe second Painlev´e transcendent and the Korteweg-de Vries equation. Archive\nRat. Mech. Anal., 73(1):31–51, 1980.\n[41] F. Hiai and D. Petz. Eigenvalue density of the Wishart matrix and large devia-\ntions. Inf. Dim. Anal. Quantum Probab. Rel. Top, 1(4):633–646, 1998.\n[42] D.A. Jackson. Stopping rules in principal components analysis: a comparison of\nheuristical and statistical approaches. Ecology, 74(8):2204–2214, 1993.\nREFERENCES\n149\n[43] W. James and C. Stein. Estimation with quadratic loss. In Proc. Fourth Berkeley\nSymp. Math. Statist. Prob., pages 361–380. Univ. of California Press, 1960.\n[44] K. Johansson. Shape ﬂuctuations and random matrices. Comm. Math. Phys.,\n209(2):437–476, 2000.\n[45] I.M. Johnstone. On the distribution of the largest eigenvalue in principal com-\nponents analysis. Ann. Statist., 29(2):295–327, 2001.\n[46] I.T. Jolliﬀe. Principal Component Analysis. Springer New York, 2002.\n[47] D. Jonsson.\nSome limit theorems for the eigenvalues of a sample covariance\nmatrix. J. Multivariate Anal., 12(1):1–38, 1982.\n[48] D. Jonsson. On the largest eigenvalue of a sample covariance matrix. In P.R.\nKrishnaiah, editor, Multivariate Analysis VI. North-Holland, 1983.\n[49] T.G. Kolda and D.P. O’Leary. A semidiscrete matrix decomposition for latent\nsemantic indexing information retrieval. ACM Trans. Inform. Syst., 16(4):322–\n346, 1998.\n[50] S. Kritchman and B. Nadler. Determining the number of components in a factor\nmodel from limited noisy data. Chemometr. Intell. Lab. Syst., 94(1):19–32, 2008.\n[51] L. Lazzeroni and A. Owen. Plaid models for gene expression data. Stat. Sinica,\n12(1):61–86, 2002.\n[52] D.D. Lee and H.S. Seung. Learning the parts of objects by non-negative matrix\nfactorization. Nature, 401(6755):788–791, 1999.\n[53] Z. Ma. Accuracy of the Tracy-Widom limit for the largest eigenvalue in white\nWishart matrices. arXiv:0810.1329v1 [math.ST], 2008.\n[54] E.R. Malinowski. Statistical F-tests for abstract factor analysis and target test-\ning. J. Chemometr., 3(1):49–69, 1989.\n150\nREFERENCES\n[55] C.D. Manning, H. Sch¨utze, and MIT Press. Foundations of Statistical Natural\nLanguage Processing. MIT Press, 1999.\n[56] V.A. Marˇcenko and L.A. Pastur. Distribution of eigenvalues for some sets of\nrandom matrices. Sb. Math., 1(4):457–483, 1967.\n[57] H. Markowitz. Portfolio selection. J. Finance, 7(1):77–91, 1952.\n[58] D.L. McLeish. Dependent central limit theorems and invariance principles. Ann.\nProbab., 2(4):620–628, 1974.\n[59] E. Meckes. An Inﬁnitesimal Version of Stein’s Method of Exchangeable Pairs.\nPhD thesis, Stanford University, 2006.\n[60] F. Mezzadri.\nHow to generate random matrices from the classical compact\ngroups. Notices Amer. Math. Soc., 54(5):592, 2007.\n[61] R.J. Muirhead. Aspects of Multivariate Statistical Theory. John Wiley & Sons,\n1982.\n[62] B. Nadler. Finite sample approximation results for principal component analysis:\nA matrix perturbation approach. Ann. Statist., 36(6):2791–2817, 2008.\n[63] M. Okamoto. Distinctness of the eigenvalues of a quadratic form in a multivariate\nsample. Ann. Statist., 1(4):763–765, 1973.\n[64] A. Onatski. Asymptotic distribution of the principal components estimator of\nlarge factor models when factors are relatively weak. Unpublished manuscript,\n2009.\n[65] A.B. Owen and P.O. Perry. Bi-cross-validation of the SVD and the non-negative\nmatrix factorization. Ann. Appl. Statist., 3(2):564–594, 2009.\n[66] L. Pastur and A. Lytova. Central limit theorem for linear eigenvalue statistics of\nrandom matrices with independent entries. arXiv:0809.4698v1 [math.PR], 2008.\nREFERENCES\n151\n[67] D. Paul. Distribution of the smallest eigenvalue of a Wishart(N,n) when N/n →\n0. Unpublished manuscript, 2006.\n[68] D. Paul. Asymptotics of sample eigenstructure for a large dimensional spiked\ncovariance model. Stat. Sinica, 17(4):1617–1642, 2007.\n[69] S. P´ech´e. Universality results for largest eigenvalues of some sample covariance\nmatrix ensembles. arXiv:0705.1701v2 [math.PR], 2008.\n[70] P.O. Perry and P.J. Wolfe.\nMinimax rank estimation for subspace tracking.\narXiv:0906.3090v1 [stat.ME], 2009.\n[71] N.R. Rao and A. Edelman.\nSample eigenvalue based detection of high-\ndimensional signals in white noise using relatively few samples. IEEE Trans.\nSignal Process., 56(7 Part 1):2625–2638, 2008.\n[72] S.N. Roy. On a heuristic method of test construction and its use in multivariate\nanalysis. Ann. Math. Stat., 24(2):220–238, 1953.\n[73] M. Schena, D. Shalon, R.W. Davis, and P.O. Brown. Quantitative monitoring\nof gene expression patterns with a complementary DNA microarray. Science,\n270(5235):467–470, 1995.\n[74] J.W. Silverstein. On the randomness of eigenvectors generated from networks\nwith random topologies. SIAM J. Appl. Math., 37(2):235–245, 1979.\n[75] J.W. Silverstein.\nDescribing the behavior of eigenvectors of random matri-\nces using sequences of measures on orthogonal groups. SIAM J. Math. Anal.,\n12(2):274–281, 1981.\n[76] J.W. Silverstein. On the largest eigenvalue of a large dimensional sample covari-\nance matrix. Unpublished manuscript, 1984.\n[77] J.W. Silverstein. Some limit theorems on the eigenvectors of large dimensional\nsample covariance matrices. J. Multivariate Anal., 15(3):295–324, 1984.\n152\nREFERENCES\n[78] J.W. Silverstein. The smallest eigenvalue of a large dimensional Wishart matrix.\nAnn. Probab., 13(4):1364–1368, 1985.\n[79] J.W. Silverstein. On the eigenvectors of large dimensional sample covariance\nmatrices. J. Multivariate Anal., 30(1):1–16, 1989.\n[80] J.W. Silverstein. Weak convergence of random functions deﬁned by the eigen-\nvectors of sample covariance matrices. Ann. Probab., 18(3):1174–1194, 1990.\n[81] J.W. Silverstein. Strong convergence of the emperical distribution of eigenvalues\nof large dimensional random matrices.\nJ. Multivariate Anal., 55(2):331–229,\n1995.\n[82] J.W. Silverstein and Z.D. Bai. On the empirical distribution of eigenvalues of a\nclass of large dimensional random matrices. J. Multivariate Anal., 54(2):175–192,\n1995.\n[83] A. Soshnikov. A note on universality of the distribution of the largest eigenvalues\nin certain sample covariance matrices. J. Stat. Phys., 108(5):1033–1056, 2002.\n[84] G.W. Stewart. Perturbation theory for the singular value decomposition. Tech-\nnical Report CS-TR-2539, University of Maryland, September 1990.\n[85] G.W. Stewart and J. Sun. Matrix Perturbation Theory. Academic Press, 1990.\n[86] M. Stone. Cross-validatory choice and assessment of statistical predictions. J.\nRoy. Stat. Soc. B, 36(2):111–147, 1974.\n[87] T. Tao and V. Vu. Random matrices: The distribution of the smallest singular\nvalues. arXiv:0903.0614v1 [math.PR], 2009.\n[88] R. Tibshirani and R. Tibshirani. A bias correction for the minimum error rate\nin cross-validation. Ann. Appl. Statist., 3(2):822–829, 2009.\n[89] C.A. Tracy and H. Widom.\nLevel-spacing distributions and the Airy kernel.\nComm. Math. Phys., 159(1):151–174, 1994.\nREFERENCES\n153\n[90] C.A. Tracy and H. Widom. On orthogonal and symplectic matrix ensembles.\nComm. Math. Phys., 177(3):727–754, 1996.\n[91] O. Troyanskaya, M. Cantor, G. Sherlock, P. Brown, T. Hastie, R. Tibshirani,\nD. Botstein, and R.B. Altman.\nMissing value estimation methods for DNA\nmicroarrays. Bioinformatics, 17(6):520–525, 2001.\n[92] K.W. Wachter. The strong limits of random matrix spectra for sample matrices\nof independent elements. Ann. Probab., 6(1):1–18, 1978.\n[93] M. Wax and T. Kailath. Detection of signals by information theoretic criteria.\nIEEE Trans. Acoust., Speech, Signal Process., 33(2):387–392, 1985.\n[94] H. Wold and E. Lyttkens. Nonlinear iterative partial least squares (NIPALS)\nestimation procedures. In Bull. Intern. Statist. Inst: Proc. 37th Session, London,\npages 1–15, 1969.\n[95] S. Wold. Cross-validatory estimation of the number of components in factor and\nprincipal components models. Technometrics, 20(4):397–405, 1978.\n[96] Y.Q. Yin.\nLimiting spectral distribution for a class of random matrices.\nJ.\nMultivariate Anal., 20(1):50–68, 1986.\n[97] Y.Q. Yin, Z.D. Bai, and P.R. Krishnaiah. On the limit of the largest eigenvalue\nof the large dimensional sample covariance matrix. Probab. Theor. Relat. Field.,\n78(4):509–521, 1988.\n[98] Y.Q. Yin and P.R. Krishnaiah. A limit theorem for the eigenvalues of product\nof two random matrices. J. Multivariate Anal., 13(4):489–507, 1983.\n",
  "categories": [
    "stat.ME",
    "math.ST",
    "stat.TH"
  ],
  "published": "2009-09-16",
  "updated": "2009-09-16"
}