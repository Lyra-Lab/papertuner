{
  "id": "http://arxiv.org/abs/0912.1820v1",
  "title": "Parsing of part-of-speech tagged Assamese Texts",
  "authors": [
    "Mirzanur Rahman",
    "Sufal Das",
    "Utpal Sharma"
  ],
  "abstract": "A natural language (or ordinary language) is a language that is spoken,\nwritten, or signed by humans for general-purpose communication, as\ndistinguished from formal languages (such as computer-programming languages or\nthe \"languages\" used in the study of formal logic). The computational\nactivities required for enabling a computer to carry out information processing\nusing natural language is called natural language processing. We have taken\nAssamese language to check the grammars of the input sentence. Our aim is to\nproduce a technique to check the grammatical structures of the sentences in\nAssamese text. We have made grammar rules by analyzing the structures of\nAssamese sentences. Our parsing program finds the grammatical errors, if any,\nin the Assamese sentence. If there is no error, the program will generate the\nparse tree for the Assamese sentence",
  "text": "IJCSI International Journal of Computer Science Issues, Vol. 6, No. 1, 2009 \nISSN (Online): 1694-0784 \nISSN (Print): 1694-0814 \n \n28\nParsing of part-of-speech tagged Assamese Texts \nMirzanur Rahman1, Sufal Das1 and Utpal Sharma2 \n \n 1 Department of Information Technology, Sikkim Manipal Institute of Technology \nRangpo, Sikkim-737136, India \n \n \n2 Department of Computer Science & Engineering, Tezpur University  \nTezpur, Assam-784028, India \n \n \n \nAbstract \nA natural language (or ordinary language) is a language that is \nspoken, written, or signed by humans for general-purpose \ncommunication, as distinguished from formal languages (such as \ncomputer-programming languages or the \"languages\" used in the \nstudy of formal logic). The computational activities required for \nenabling a computer to carry out information processing using \nnatural language is called natural language processing. We have \ntaken Assamese language to check the grammars of the input \nsentence. Our aim is to produce a technique to check the \ngrammatical structures of the sentences in Assamese text. We \nhave made grammar rules by analyzing the structures of \nAssamese sentences. Our parsing program finds the grammatical \nerrors, if any, in the Assamese sentence. If there is no error, the \nprogram will generate the parse tree for the Assamese sentence \n \nKeywords: Context-free Grammar, Earley’s Algorithm, Natural \nLanguage Processing, Parsing, Assamese Text. \n1. Introduction \nNatural language processing, a branch of artificial \nintelligence that deals with analyzing, understanding and \ngenerating the languages that humans use naturally in \norder to interface with computers in both written and \nspoken contexts using natural human languages instead of \ncomputer languages. It studies the problems of automated \ngeneration and understanding of natural human languages. \nWe have taken Assamese language for information \nprocessing i.e. to check the grammars of the input \nsentence. Parsing process makes use of two components. \nA parser, which is a procedural component and a \ngrammar, which is declarative. The grammar changes \ndepending on the language to be parsed while the parser \nremains unchanged. Thus by simply changing the \ngrammar, the system would parsed a different language. \nWe have taken Earley’s Parsing Algorithm for parsing \nAssamese Sentence according to a grammar which is \ndefined for Assamese language. \n2. Related Works \n2.1 Natural Language Processing  \nThe term “natural” languages refer to the languages that \npeople speak, like English, Assamese and Hindi etc. The \ngoal of the Natural Language Processing (NLP) group is \nto design and build software that will analyze, understand, \nand generate languages that humans use naturally. The \napplications of Natural Language can be divided into two \nclasses [2] \n• \nText \nbased \nApplications: \nIt \ninvolves \nthe \nprocessing of written text, such as books, \nnewspapers, reports, manuals, e-mail messages \netc. These are all reading based tasks. \n• \nDialogue based Applications: It involves human \n– machine communication like spoken language. \nAlso includes interaction using keyboards. From \nan end-user’s perspective, an application may \nrequire NLP for either processing natural \nlanguage input or producing natural language \noutput, or both. Also, for a particular application, \nonly some of the tasks of NLP may be required, \nand depth of analysis at the various levels may \nvary. Achieving human like language processing \ncapability is a difficult goal for a machine. \nThe difficulties are: \n• \nAmbiguity \n• \nInterpreting partial information \n• \nMany inputs can mean same thing \n \n2.2 Knowledge Required for Natural Language  \nA Natural Language system uses the knowledge about the \nstructure of the language itself, which includes words and \nIJCSI International Journal of Computer Science Issues, Vol. 6, No. 1, 2009 \n \n \n29\nhow words combine to form sentences, about the word \nmeaning and how word meanings contribute to sentence \nmeanings and so on. The different forms of knowledge \nrelevant for natural language are [2]: \n• \nPhonetic \nand \nphonological \nknowledge: \nIt \nconcerns how words are related to the sounds that \nrealize them. \n• \nMorphological knowledge: It concerns how \nwords are constructed from more basic meaning \nunits called morphemes. A morpheme is the \nprimitive unit of meaning in a language (for \nexample, the meaning of the word friendly is \nderivable from the meaning of the noun friend \nand suffix -ly, which transforms a noun into an \nadjective. \n• \nSyntactic Knowledge: It concerns how words can \nbe put together to form correct sentences and \ndetermine what structure role each word plays in \nthe sentence. \n• \nSemantic knowledge: It concerns what words \nmean and how these meanings combine in \nsentences to form sentence meanings. \n• \nPragmatic Knowledge: It concerns how sentences \nare used in different situations and how use \naffects the interpretation of the sentence. \n• \nDiscourse Knowledge: It concerns how the \nimmediately preceding sentences affect the \ninterpretation of the next sentence. \n• \nWord Knowledge: It includes what each language \nuser must know about the other user’s beliefs and \ngoals. \n2.3 Earley’s Parsing Algorithm \nThe Earley’s Parsing Algorithm [8, 9] is basically a top \ndown parsing algorithm where all the possible parses are \ncarried simultaneously. Earley’s algorithm uses dotted \nContext-free Grammar (CFG) rules called items, which \nhas a dot in its right hand side. \nLet the input sentence be- “0 I 1 saw 2 a 3 man 4 in 5 the 6 \npark 7”. Here the numbers appeared between words are \ncalled position numbers.  \nFor CFG rule S →NP VP we will have three types of \ndotted items- \n• [ S→ .NP VP,0,0 ] \n• [ S→ NP.VP,0,1 ] \n• [ S→ NP VP.,0,4 ] \n \n \nHere \nS   → Starting Symbol \nNP → Noun Phrase \nVP → Verb Phrase \n \n1. The first item indicates that the input sentence is \ngoing to be parsed applying the rule S→ NP VP \nfrom position 0. \n2. The second item indicates the portion of the input \nsentence from the position number 0 to 1 has \nbeen parsed as NP and the remainder left to be \nsatisfied as VP. \n3. The third item indicates that the portion of input \nsentence from position number 0 to 4 has been \nparsed as NP VP and thus S is accomplished.  \n \nEarley’s algorithm uses 3 phases \n• \nPredictor \n• \nScanner \n• \nCompleter \nLet α, β, γ are sequence of terminal or nonterminal \nsymbols and S, A, B are non terminal symbols.  \n \nPredictor Operation \nFor an item of the form [A → α.Bβ,i,j] create [B→.γ,j,j] \nfor each production of the [B→γ] It is called predictor \noperation because we can predict the next item.  \n \nCompleter Operation  \nFor an item of the form [B→γ.,j,k] create [A→αB.β,i,k] \n(i<j<k) for each item in the form of [A→α.Bβ,i,j] if exists. \nIt is called completer because it completes an operation. \n \nScanner Operation \nFor \nan \nitem \nof \nthe \nform \n[A→α.wβ,i,j] \ncreate \n[A→αw.β,i,j+1], if w is a terminal symbol appeared in the \ninput sentence between j and j+1. \n \nEarley’s parsing algorithm \n \n1. For each production S→α, create [S→.α,0,0] \n2. For j=0 to n do (n is the length of the input \nsentence) \n3. For each item in the form of [A→α.Bβ,i,j] apply \nPredictor operation while a new item is created.  \n4. For each item in the form of [B→γ.i,j] apply \nCompleter operation while a new item is created.  \n5. For each item in the form of [A→α.wβ,i,j] apply \nScanner operation \n \nIf we find an item of the form [S→α.,0,n] then we accept \nit. \n \nLet us take an example.. \n”0 I 1 saw 2 a 3 man 4”. \nConsider the following grammar: \n1. S → NP VP \n2. S → S PP \n3.  NP → n \nIJCSI International Journal of Computer Science Issues, Vol. 6, No. 1, 2009 \n \n30\n4.  NP → art n \n5.  NP → NP PP \n6.  PP → p NP \n7.  VP → v NP \n8.   n → I \n9.   n → man \n11. v → saw \n12. art → a \n \nNow parse the sentence using Earley’s parsing technique. \n1. \n[S→.NP VP,0,0] \nInitialization \n2. \n[S→.S PP,0,0] \nApply Predictor to step 1 and \nstep 2 \n3. \n[NP→.n,0,0] \n \n4. \n[NP→.art n,0,0] \n \n5. \n[NP→.NP PP,0,0] \nApply Predictor to step 3 \n6. \n[n→.“I”,0,0] \nApply scanner to 6 \n7. \n[n→ “I”.,0,1] \nApply Completer to step 7 with \nstep 3 \n8. \n[NP→n.,0,1] \nApply Completer to step 8 with \nstep 1 and step 5 \n9. \n[S→NP.VP,0,1] \n \n10. \n[NP→NP.PP,0,1] \nApply Predictor to step 9 \n11. \n[VP→.v NP,1,1] \nApply Predictor to step 11 \n12. \n[v→.“saw”,1,1] \nApply Predictor to step 10 \n13. \n[PP→.p NP,1,1] \nApply Scanner to step 12 \n \n14. \n[v→ “saw”.1,2] \nApply Completer to step 14 \nwith  step 11 \n15. \n[VP→v.NP,1,2] \nApply Predictor to step 15 \n16. \n[NP→.n,2,2] \n \n17. \n[NP→.art n,2,2] \n \n18. \n[NP→.NP PP 2,2] \nApply Predictor to step 17 \n19. \n[art → .“a”, 2,2] \nApply Scanner  to step 19 \n20. \n[art → “a”.,2,3] \nApply Completer to step 20 \nwith step 17 \n21. \n[NP → art .n,2,3] \nApply Predictor to step 21 \n22. \n[n → .“man”, 3,3 \nApply Scanner to step 22 \n23. \n[n → “man”.,3,4] \nApply Completer to step 23 \nwith step 21 \n24. \n[NP → art n.,2,4] \nApply Completer to 24 with 15 \n25. \n[VP → v NP.,1,4] \nApply Completer to 25 with 9 \n26. \n[S → NP VP.,0,4] \nComplete \n \n                                                                       \n. When applying Predictor operation Earley’s algorithm \noften creates a set of similar items such as-step 3,4,5 and \n16,17,18 expecting NP in future. \n3. Properties and problems of parsing \nalgorithm  \nParsing algorithms are usually designed for classes of \ngrammar rather than for some individual grammars. There \nare some important properties [6] that make a parsing \nalgorithm practically useful. \n• \nIt should be sound with respect to a given \ngrammar and lexicon \n• \nIt should be complete so that it assign to an input \nsentence and all the analyses it    can have with \nrespect to the current grammar and lexicon. \n• \nIt should also be efficient so that it take minimum \nof computational work. \nAlgorithm should be robust, behaving in a reasonably \nsensible way when presented with sentence that it is \nunable to fully analyze successfully. \nThe main problem of Natural Language is its \nambiguity. The sentences of Natural Languages are \nambiguous in meaning. There are different meanings for \none sentence. So all the algorithms for parsing can not be \nused for Natural Language processing. There are many \nparsing technique used in programming languages (like C \nlanguage).These techniques easy to use, because in \nprogramming language, meaning of the words are fixed. \nBut in case of NLP we can not used this technique for \nparsing, because of ambiguity. \n \nFor example: “I saw a man in the park with a telescope”.  \n \nThis sentence has at least three meanings- \n• \nUsing a telescope I saw the man in the park. \n• \nI saw the man in the park that has a telescope. \n• \nI saw the man in the park standing behind the \ntelescope which is placed in the park. \nSo, this sentence is ambiguous and no algorithm can \nresolve the ambiguity. An algorithm will be the best \nalgorithm, which produces all the possible analyses. \nTo begin with, we look for algorithm that can take care of \nambiguity of smaller components such as ambiguity of \nwords and phrases. \n4. Proposed grammar and algorithm for \nAssamese Texts \nSince it is impossible to cover all types of sentences in \nAssamese language, we have taken some portion of the \nsentence and try to make grammar for them. Assamese is \nfree-word-order language [10]. As an example we can take \nthe following Assamese sentence. \n \nIJCSI International Journal of Computer Science Issues, Vol. 6, No. 1, 2009 \n \n \n31\nThis sentence can be written as. \n \nHere we see that one sentence can be written in different \nforms for the same meaning, i.e. the positions of the tags \nare not fixed. So we can not restrict the grammar rule for \none sentence. The grammar rule may be very long, but we \nhave to accept it. The grammar rule we have tried to make, \nmay not work for all the sentences in Assamese language. \nBecause we have not considered all types of sentences. \nSome of the sentences are shown below, which are used to \nmake the grammar rule [3, 4]. \n \n \n \nOur proposed grammars for Assamese sentences \n \n1. S   → PP VP | PP \n2. PP → PN NP | NP PN | ADJ NP | NP ADJ | NP |   \n                 ADJ | IND NP | PN | ADV NP | ADV \n3. NP → NP PP | PP NP | ADV NP |  PP | ART NP |  \n                        NP ART | IND PN | PN IND \n \nHere..... \nNP → Noun \nPN → Pronoun \nVP → Verb \nADV → Adverb \nADJ → Adjective \nART → Article \nIND → Indeclinable \n \n \n4.1 Modification of Earley’s Algorithm for Assamese \nText Parsing \nWe know that Earley’s algorithm uses three operations, \nPredictor, Scanner and Completer. We add Predictor and \nCompleter in one phase and Scanner operation in another \nphase.  \nLet α, β, γ, PP, VP are sequence of terminal or \nnonterminal symbols and S, B are non terminal symbols. \n \nPhase 1:(Predictor+Completer) \nFor an item of the form [S →α .Bβ,i,j] , create  \n[S →α.γβ,i,j] for each production of the [B→γ] \n \nPhase 2 :( Scanner) \nFor \nan \nitem \nof \nthe \nform \n[S→α.wβ,i,j] \ncreate \n[S→αw.β,i,j+1], if w is a terminal symbol appeared in the \ninput sentence between j and j+1. \n \nOur Algorithm \nInput: Tagged Assamese Sentence \nOutput: Parse Tree or Error message \nStep 1: If Verb is present in the sentence then create \n            [S→ .PP VP ,0,0]  \n             Else create  \n            [S→ .PP ,0,0] \nStep 2: Do the following steps in a loop until there is a     \n            success or error \nStep 3: For each item of the form of [S→α.Bβ,i,j], apply  \n            phase 1 \nStep 4: For each item of the form of [S→ .αwβ,i,j], apply  \n             phase 2 \nStep 5: If we find an item of the form [S→α. ,0,n], then  \n             we accept the sentence as success else error   \n             message. Where n is the length of input sentence.   \n             And then come out from the loop. \nStep 6: Generate the parse trees for the successful  \n            sentences. \n \nSome other modifications of Earley’s algorithm: \n \n1. \nEarley’s algorithm blocks left recursive rules \n[NP→ .NP PP ,0,0], when applying Predictor \noperation. Since Assamese Language is a Free-\nWord-Order language. We are not blocking this \ntype of rules. \n2. \nEarley’s algorithm creates new items for all \npossible productions, if there is a non terminal in \nthe left hand side rule. But we reduce these \nproductions \nby \nremoving \nsuch \ntype \nof \nproductions, which create the number of total \nIJCSI International Journal of Computer Science Issues, Vol. 6, No. 1, 2009 \n \n32\nproductions in the stack, greater then total tag \nlength of the input sentence. \n3. \nAnother restriction we used in our algorithm for \ncreating new item is that, if the algorithm \ncurrently analyzing the last word of the sentence, \nthen it selects only the single production in the \nright hand side (example [PP→NP]). The other \nrules (which have more then one production rules \nin right hand side (example [PP→PN NP])) are \nignored by the algorithm. \n \n4.2 Parsing Assamese text using proposed grammar \nand algorithm \nLet us take an Assamese sentence. \n \nNow the position number for the words are placed \naccording to which word will be parsed first. \n \n \nWe consider the following grammar rule \n1. S      → PP VP | PP \n2. PP    → PN NP | NP PN | ADJ NP | NP ADJ | NP  \n                    | ADJ | IND NP | PN | ADV NP | ADV \n3. NP    → NP PP | PP NP | ADV NP |  PP | ART   \n                     NP | NP ART | IND PN | PN IND \n4. PN    → “mai” \n5. PN    → “si” \n6. IND  → “Aru” \n7. ADV → “ekelge” \n8. NP    → “gharalE” \n9. VP    → “jAm” \n \nParsing process will proceed as follows \n \n1 \n[S → .PP VP , 0,0] \nApply Phase 1 \n2 \n[S → .NP VP,0 ,0] \nApply Phase 1 \n3 \n[S → .PP NP VP,0,0] \nApply Phase 1 \n4 \n[S → .PN NP NP VP,0,0] \nApply Phase 1 \n5 \n[S → .“mai” NP NP VP,0,0] \nApply Phase 2 \n6 \n[S → .“mai” .NP NP VP,0,1] \nApply Phase 1 \n7 \n[S → “mai” .IND PN NP VP,0,1] \nApply Phase 1 \n8 \n[S → “mai” . “Aru” PN NP \nVP,0,1] \nApply Phase 2 \n9 \n[S → “mai” “Aru” .PN NP VP,0,2] \nApply Phase 1 \n10 \n[S → “mai” “Aru” .“si” NP \nVP,0,2] \nApply Phase 2 \n11 \n[S → “mai” “Aru” “si” .NP \nVP,0,3] \nApply Phase 1 \n12 \n[S → “mai” “Aru” “si” .ADV NP \nVP,0,3] \nApply Phase 1 \n13 \n[S → “mai” “Aru” “si” .“ekelge” \nNP VP,0,3] \nApply Phase 2 \n14 \n[S → “mai” “Aru” “si” “ekelge” \n.NP VP,0,4] \nApply Phase 1 \n15 \n[S → “mai” “Aru” “si” “ekelge” \n.“gharalE” .VP,0,5] \nApply Phase 2 \n16 \n[S → “mai” “Aru” “si” “ekelge” \n“gharalE” .VP,0,5] \nApply Phase 1 \n17 \n[S → “mai” “Aru” “si” “ekelge” \n“gharalE” .“jAm”,0,5] \nApply Phase 2 \n18 \n[S → “mai” “Aru” “si” “ekelge” \n“gharalE” “jAm”.,0,6] \nComplete \n \nIn the above example, we have shown only the steps \nwhich proceeds to the goal. The other steps are ignored.  \n5. Implementation and Result Analysis \n5.1 Different Stages of the Program \nIn the program there are 3 stages. \n• \nLexical Analysis \n• \nSyntax Analysis \n• \nTree Generation \nIn Lexical Analysis stage, program finds the correct tag \nfor each word in the sentence by searching the database. \nThere are seven databases (NP, PN, VP, ADJ, ADV, ART, \nIND) for tagging the words. \nIn Syntax Analysis stage the program tries to \nanalyze whether the given sentence is grammatically \ncorrect or not. \nIn Tree Generation stage, the program finds all \nthe production rules which lead to success and generates \nparse tree for those rules. If there are more then one path \nto success, this stage can generates more then on parse \ntrees. It also displays the words of the sentences with \nproper tags. The following shows a parse tree generate by \nthe program. \nIJCSI International Journal of Computer Science Issues, Vol. 6, No. 1, 2009 \n \n \n33\n \n \nThe original parse tree for the above sentence is. \n \n5.2 Result Analysis  \nAfter implementation of Earley’s algorithm using our \nproposed grammar, it has been seen that the algorithm can \neasily generates parse tree for a sentence if the sentence \nstructure satisfies the grammar rules. For example we take \nthe following Assamese sentence  \n \nThe structure of the above sentence is NP-ART-ADJ-NP. \nThis is a correct sentence according to the Assamese \nliterature. According to our proposed grammar a possible \ntop down derivation for the above sentence is \n \n \n1. \nS \n[Handle] \n2. \n>>PP \n[S→PP] \n3. \n>> NP \n[PP→NP] \n4. \n>>NP PP \n[NP→NP PP] \n5. \n>>NP ART PP \n[NP → NP ART] \n6. \n>>gru ART PP \n[NP → gru \n7. \n>>gru ebidh PP \n[ART→ ebidh \n8. \n>>gru ebidh  ADJ NP \n[PP→ ADJ NP] \n9. \n>>gru ebidh  upakArI NP \n[ADJ→upakArI] \n10. \n>>gru ebidh  upakArI za\\ntu \n[NP→za\\ntu] \n \nFrom the above derivation it has been seen that the \nAssamese sentence is correct according to the proposed \ngrammar.  So our parsing program generates a parse tree \nsuccessfully as follows. \n \n \n \nOur program tests only the sentence structure according to \nthe proposed grammar rules. So if the sentence structure \nsatisfies the grammar rule, program recognizes the \nsentence as a correct sentence and generates parse tree. \nOtherwise it gives output as an error. \n6. Conclusion and Future Work \nWe have developed a context free grammar for simple \nAssamese sentences.  Different natural languages present \ndifferent challenges in computational processing. We have \nstudied the issues that arise in parsing Assamese sentences \nand produce an algorithm suitable for those issues. This \nalgorithm is a modification of Earley’s Algorithm. We \nfound that Earley’s parsing algorithms is simple and \neffective. \n \nIJCSI International Journal of Computer Science Issues, Vol. 6, No. 1, 2009 \n \n34\nIn this work we have considered limited number of \nAssamese sentences to construct the grammar rules. We \nalso have considered only seven main tags. In future work \nwe have to consider as many sentences as we can and \nsome more tags for constructing the grammar rules. \nBecause Assamese language is a free-word-order \nlanguage. Word position for one sentence may not be \nsame in the other sentences. So we can not restrict the \ngrammar rules for some limited number of sentences. \n \nReferences \n[1] Alfred V. Aho and Jeffrey D. Ullman. Principles of \nCompiler Design. Narosa publishing  House, 1995. \n[2] James \nAllen. \nNatural \nLanguage \nUnderstanding. \nPearson Education, Singapur, second edition, 2004. \n[3] Hem Chandra Baruah. Assamiya Vyakaran. Hemkush \nPrakashan, Guwahati, 2003. \n[4] D. Deka and B. Kalita. Adhunik Rasana Bisitra. Assam \nBook Dipo, Guwahati, 7th edition, 2007. \n[5] H. Numazaki and H. Tananaka. A new parallel \nalgorithm for generalized lr parsing, 1990. \n[6] Stephen G. Pulman. Basic parsing techniques: an \nintroductory survey, 1991. \n[7] Utpal \nSharma. \nNatural \nLanguage \nProcessing. \nDepartment of Computer Science and Information \nTechnology, \nTezpur \nUniversity, \nTezpur-784028, \nAssam,India. \n[8] Hozumi Tanaka. Current trends on parsing - a survey, \n1993. \n[9] Jay Earley. An efficient context free parsing algorithm, \nCommunications of the ACM. Volume 13, no 2, \nFebruary-1970 \n[10] Navanath Saharia, Dhrubajyoti Das, Utpal Sharma, \nJugal Kalita. Part of Speech Tagger for Assamese Text, \nACL-IJCNLP 2009, 2-7 August 2009, Singapore. \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2009-12-09",
  "updated": "2009-12-09"
}