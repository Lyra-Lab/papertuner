{
  "id": "http://arxiv.org/abs/1011.5951v1",
  "title": "Reinforcement Learning in Partially Observable Markov Decision Processes using Hybrid Probabilistic Logic Programs",
  "authors": [
    "Emad Saad"
  ],
  "abstract": "We present a probabilistic logic programming framework to reinforcement\nlearning, by integrating reinforce-ment learning, in POMDP environments, with\nnormal hybrid probabilistic logic programs with probabilistic answer set\nseman-tics, that is capable of representing domain-specific knowledge. We\nformally prove the correctness of our approach. We show that the complexity of\nfinding a policy for a reinforcement learning problem in our approach is\nNP-complete. In addition, we show that any reinforcement learning problem can\nbe encoded as a classical logic program with answer set semantics. We also show\nthat a reinforcement learning problem can be encoded as a SAT problem. We\npresent a new high level action description language that allows the factored\nrepresentation of POMDP. Moreover, we modify the original model of POMDP so\nthat it be able to distinguish between knowledge producing actions and actions\nthat change the environment.",
  "text": "Reinforcement Learning in Partially Observable \n \nMarkov Decision Processes using Hybrid \n \nProbabilistic Logic Programs \n \nEmad  Saad \n \nDepartment  of  Computer  Science \n \nGulf University for Science and Technology \nMishref, Kuwait \n \nsaad.e@gust.edu.kw \n \n \n \nAbstract—We present a probabilistic logic programming \nframework to reinforcement learning, by integrating reinforce-\nment learning, in POMDP environments, with normal hybrid \nprobabilistic logic programs with probabilistic answer set seman-\ntics, that is capable of representing domain-specific knowledge. \nWe formally prove the correctness of our approach. We show \nthat the complexity of finding a policy for a reinforcement \nlearning problem in our approach is NP-complete. In addition, \nwe show that any reinforcement learning problem can be \nencoded as a classical logic program with answer set semantics. \nWe also show that a reinforcement learning problem can be \nencoded as a SAT problem. We present a new high level action \ndescription language that allows the factored representation of \nPOMDP. Moreover, we modify the original model of POMDP so \nthat it be able to distinguish between knowledge producing \nactions and actions that change the environment. \n \nI.   INTRODUCTION \n \nReinforcement learning is the problem of learning to act by \ntrial and error interaction in dynamic environments. \nReinforcement learning problems can be represented as \nMarkov Decision Processes (MDP), under the assumption that \naccurate and complete model of the environment is known. \nThis assumption requires the agent to have perfect sensing and \nobservation abilities. \n \nHowever, complete and perfect observability is unrealistic \nfor many real-world reinforcement learning applications, al-\nthough necessary for learning optimal policies in MDP \nenvironments. Therefore, different model is needed to \nrepresent and solve reinforcement learning problems with \npartial observability. This model is Partially Observable \nMarkov Decision Processes (POMDP). Similar to MDP, \nPOMDP requires the model of the environment to be known, \nhowever states of the world are not completely known. \nConsequently, \nthe \nagent \nperforms \nactions \nto \nmake \nobservations about the states of the worlds. These \nobservations can be noisy due to imperfect agent’s sensors. \nSimilar to MDP, dynamic programming methods, by value \niteration, has been used to learn the optimal policy for a \nreinforcement learning problem in POMDP environment. \n \nA logical framework to reinforcement learning in MDP \nenvironment has been developed in [30], which relies on tech- \n \n \n \nniques \nfrom \nprobabilistic \nreasoning \nand \nknowledge \nrepresentation by normal hybrid probabilistic logic programs \n[34]. The normal hybrid probabilistic logic programs framework \nof [30] has been proposed upon observing that dynamic \nprogramming methods to reinforcement learning in general and \nvalue iteration in particular are incapable of exploiting domain-\nspecific knowledge of the reinforcement learning problem \ndomains to improve the efficiency of finding the optimal policy. \nIn addition, these dynamic programming methods use primitive \nrepresentation of states and actions as this representation does not \ncapture the relationship between states [22] and makes it difficult \nto represent domain-specific knowledge. However, using richer \nknowledge representation frameworks for MDP and POMDP \nallow efficiently finding optimal policies in more complex \nstochastic domains and lead to develop methods to find optimal \npolicies with larger domains sizes [22]. \n \nThe choice of normal hybrid probabilistic logic programs \n(NHPLP) to solve reinforcement learning problems in MDP \nenvironment is based on that; NHPLP is nonmonotonic, there-\nfore more suitable for knowledge representation and reasoning \nunder uncertainty; NHPLP subsumes classical normal logic \nprograms with classical answer set semantics [7], a rich \nknowl-edge representation and reasoning framework, and \ninherits \nits \nknowledge \nrepresentation \nand \nreasoning \ncapabilities including the ability to represent and reason about \ndomain-specific knowledge; NHPLP has been shown \napplicable to a variety of fundamental probabilistic reasoning \nproblems including probabilistic planning [28], contingent \nprobabilistic planning [31], the most probable explanation in \nbelief networks, the most likely trajectory in probabilistic \nplanning, and Bayesian reasoning [29]. \n \nIn this view, we integrate reinforcement learning in POMDP \nenvironment with NHPLP, providing a logical framework that \novercomes \nthe \nrepresentational \nlimitations \nof \ndynamic \nprogramming method to reinforcement learning in POMDP and \nis capable of representing its domain-specific knowledge. In \naddition, the proposed framework extends the logical framework \nof reinforcement learning in MDP of [30] with partial \nobservability. We show that any reinforcement learning problem \nin POMDP environment can be encoded as a SAT\nproblem. The importance of that is reinforcement learning \nproblems in POMDP environment can be now solved as SAT \nproblems. \n \nII.   SYNTAX  AND  SEMANTICS  OF  NHPLP \n \nWe introduce a class of NHPLP [34], namely NHPLP\n \nthat is sufficient to represent POMDP. \n \nA.   The  Language  of  NHPLP\n \n \n   Let \n be a first-order language with finitely many \npredicate symbols, constants, and infinitely many variables. \nThe Herbrand base of \n is denoted by \n. Probabilities are \nassigned to atoms in \n as values from \n. An annotation, \n, is either a constant in \n, a variable (annotation \nvariable) ranging over \n, or \n (called \nannotation function) where \n is a representation of a \ncomputable \ntotal \nfunction \n \nand \n are annotations. Let \n. Then we \nsay that \n iff \n. A normal probabilistic logic \nprogram (np-program) in NHPP\n is a pair \n \nwhere \n is a finite set of normal probabilistic rules (np-rules) \nand  is a mapping \n, where \n is a set of \ndisjunctive probabilistic strategies (p-strategies) whose \ncomposition \nfunctions \n, \nare \nmappings \n. A composition function of a \ndisjunctive p-strategy returns the probability of a disjunction \nof two events given the probability values of its components \nAn np-rule is an expression of the form \n \nwhere \n \nare \natoms \nand \n are annotations. Intuitively, the \nmeaning of an np-rule is that if for each \n, the \nprobability of \n is at least \n (w.r.t. \n) and for each \n, it is not believable that the probability of \n \nis at least \n, then the probability of \n is \n. The mapping  \nassociates to each atom \n a disjunctive p-strategy that will be \nemployed to combine the probability values obtained from \ndifferent np-rules having \n in their heads. An np-program is \nground if no variables appear in any of its p-rules. \n  \nB.   Probabilistic  Answer  Set  Semantics  of  NHPLP\n \n \n    A probabilistic interpretation (p-interpretation), , is a \nmapping from \n to \n. Let \n be a ground np-\nprogram,  be a p-interpretation, and  be an np-rule as \nabove. Then, we say \n \nThe probabilistic reduct \n of  w.r.t.  is an np-program \nwithout negation, \n, where: \n \nand \n A probabilistic model (p-\nmodel) of an np-program \n is a p-interpretation of \n that \nsatisfies \n. We say that a p-interpretation \n of \n is a \nprobabilistic answer set of \n if  is the minimal p-model of \nthe probabilistic reduct, \n, of \n w.r.t. . \n  \nIII.   PARTIALLY  OBSERVABLE  MARKOV  DECISION \n \nPROCESSES \n \nWe review finite-horizon POMDP [12] with stationary tran-\nsition functions, stationary bounded reward functions, and \nstationary policies. \n \nA.   POMDP  Definition \n \nPOMDP \nis \na \ntuple \nof \nthe \nform \n where: \n is a finite set of \nstates; \n is the initial state distribution; \n is a finite set of \nstochastic actions; \n is stationary transition function \n, \nwhere \nfor \nany \n which determines the expected sum of discounted rewards \nresulting from executing the policy \n starting from \n. \nBecause of the agent is unable to completely observe the \nstates of the world and with reliability, it keeps what is called \na belief state. An agent's belief state is a probability \ndistribution over the possible world states the agent may think \nit is in. Therefore, an action causes a transition from a belief \nstate to another belief state. Given  is a believe state and  is \nan action, then executing  in the belief state  results a new \nbelief state , where the probability of a state, \n, in  and the \nvalue function of executing a policy  in  are given by:  \n \n \n \n \nThe optimal policy over the agent's belief states can \nconstructed from the optimal value function over the agent's \nbelief states which is given by \n. \n \nB.   Discussion \n \nThe original model of POMDP does not distinguish \nbetween knowledge producing (sensing) actions and actions \nthat affects and change the environment (non-sensing actions). \nThis means that it treats sensing and non-sensing actions \nequally in the sense that, like non-sensing actions, a sensing \naction affects and changes the environment as well as \nproducing \nknowledge \nresulting \nfrom \nobserving \nthe \nenvironment. However, [36] proved that sensing actions \nproduce knowledge (make observations) and does not change \nthe state of the world. Therefore, actions that change the state \nof the world are different from the knowledge producing \nactions. In addition, the value function described above makes \nthe agent observing the environment at every step of its life \nwith each action it takes. However, this is not necessary to be \nalways the case, since it is possible for the agent to start with \nobserving the environment then performing a sequence of \nactions, or the agent could start with performing a sequence of \nactions then observing the environment. To overcome these \nlimitations, we define the value function of n-step finite \nhorizon POMDP with respect to an initial state \n as: \n \n \nwhere \n is the probability of observing the state\n, where for some \n is observed in \n. Notice that \n \nis treated as a mapping \n, where  is \nthe set of sensing actions. For any\n \nis the probability distribution over states resulting from \nexecuting\n. As in the \noriginal model of POMDP, is a mapping \n, where  is the set of non-sensing \nactions. Extension to infinite horizon POMDP can be \nachieved in a similar manner. This definition of POMDP \ndistinguishes between knowledge producing actions and \nactions that change the environment. In this view, the optimal \npolicy \n is given by: \n \n \n \nIV.   \n  AN  ACTION  LANGUAGE  FOR  POMDP \n \nWe introduce an action language for POMDP, \n. The \nproposed action language extends both the action language, \n, [30] for representing and reasoning about MDP, and \nthe action language, \n, [31] for representing and reasoning \nabout imperfect sensing actions with probabilistic outcomes. \nAn action theory in \n is capable of representing the initial \nstate distribution, the executability conditions of actions, the \ndiscount factor, the reward received from executing actions in \nstates, and makes it clear the distinction between sensing and \nnon-sensing actions. \nA.   Language  syntax \n \n   A fluent is a predicate, which may contain variables. Given \nthat \n is a set of fluents and \n is a set of actions that can \ncontain variables, a fluent literal is either a fluent \n or\n. A conjunction of fluent literals of the form \n is \nconjunctive fluent formula, where \n are fluent literals. \nSometimes we abuse the notation and refer to a conjunctive \nfluent formula as a set of fluent literals (\n). An \naction theory, \n is a tuple \n, \nwhere \n is a proposition of the form (1), \n is a set of \npropositions from (2-4), and \n is a discount factor as \nfollows: \n \nwhere \n \nare \nconjunctive \nfluent \nformulas, \n. The set of all ground \n must be exhaustive and mutually exclusive. \n \nThe initial agent’s belief state—a probability distribution \nover the possible initial states, is represented by (1), that says \neach possible initial state \n holds with probability \n. \nExecutability condition is represented by (2). A non-sensing \naction, \n, is represented by (3), which says that for each \n,  causes \n to hold with probability \n and reward \n is received in a successor state to a state in which  is \nexecuted and \n holds. A sensing action, , is represented by \n(4), which says that for each \n, whenever a correlated \n is known to be true,  causes any of \n to be known true \nwith probability \n and reward \n is received in a successor \nstate to a state in which  is executed, where the literals in \n \ndetermine what the agent is observing and literals in \n \ndetermine what the sensor reports on. Similar to [4], when a \nproperty of the world cannot be directly sensed by the sensor, \nanother correlated property of the world, that can be sensed by \nthe sensor, can be used instead. An action theory is ground if \nit does not contain any variables. \n In the sequel, we represent an action  in (3) as a set of the \nform \n, where each \n corresponds to \n, \n, \n, and \n. For each \n, (3) can be represented as \n. \nSimilarly, \n(4) \ncan \nbe \nrepresented as \n. \n \nExample 1: Consider the tiger domain from [20]. A tiger is \nbehind left \n or tight \n door with equal probability 0.5. \nIf left door is opened and \n, punishment of -100 is received, \nbut a reward of 10 is received if \n and the other way \naround. The sensing action listen used for hearing the tiger \nbehind left door \n, a correlated property to \n. But, the \nagent’s hearing is not perfect and costs -1. If the agent listens \nto \n, then it reports \n with 0.85 and erroneously reports \n with 0.15. Similarly for listening to the right door. This is \nrepresented by the action theory \n, where \n \n \n \n \n \nB.   Semantics \nA set of ground literals  is consistent if it does not contain a \npair of complementary literals. If a literal  belongs to , then \nwe say  is true in , and  is false in  if \n is in . A set of \nliterals  is true in  if  is contained in . A state  is a \ncomplete and consistent set of literals that describes the world at \na certain time point. \nDefinition 2: Let \n be a ground action theory in \n,  be a state, \n \nbe in \n, and \n be an action, where each \n \ncorresponds to \n, and \n (similarly for\n). Then, the state \nresulting from executing \n is: \n \n    \nDefinition  2: Let  be a state, and \n \n(similarly \n) \nbe in propositions. Then, the transition probability distribution \nafter executing \n is given by \n \nThe reward received in a state  after executing \n is  \n \n   Definition 3: Let \n be an initial state, \n be states, and  be \na policy in \n. Then, the value function of n-step remaining, \n, of  is given by: \nExecuting sensing or non-sensing action, \n in  causes a \ntransition to a set of states, \n. Let \n \ndenotes the set of actions \n executed in \nthe states \n respectively. Notice that if \n is a \nsingleton, i.e., the same action is executed in every state in , \nthen this corresponds to executing an action in a belief state \n. Since executing \n in  produces \nanother set of states\n, then executing \n causes a transition \nfrom a belief state to another belief state. \nFor finite horizon POMDP, a policy \n can be \nrepresented as a set of ordered pairs, starting from the initial \nbelief state \n (the set of initial states in \n), as\nwhere \nfor \n represents a belief state (a set of states) \nresulting from executing \n in \n. This set \nrepresentation of finite horizon policies in POMDP leads to \nview a policy as a set of trajectories, where each trajectory \nis \n \nwhere \n is an initial state in \n and for all \n and\n, such that for any\n. Let \n be a policy for a \nfinite horizon POMDP and \n be the set of trajectories \nrepresentation of , given the trajectory view of , the value \nfunction of  can be now described as: \nwhere \nThus, the optimal policy\n, the maximum value function \namong all policies, is given by \n. \n \n    V.   REINFORCEMENT LEARNING IN NHPLP\n \n \nThis section uses NHPLP\n to solve reinforcement \nlearning problems, by encoding an action theory, \n in\n, into an np-program, \n. The probabilistic answer \nsets of \n correspond to valid trajectories in \nwith \nassociated value function. The np-program encoding of an \naction theory in \n follows related encoding described in \n[30], [31], [37]. We assume that the length of the optimal \npolicy that we are looking for is known and finite. We use \nthe following predicates: \n for literal \n holds at \ntime moment \n for action  executes at time \n,\n for a state of the world at time \n, \nfor the reward received at time \n is , \n for the \nvalue function of a state at time \n is \n, and \n for \nthe discount factor . If an atom appears in an np-rule in \nwith no annotation it is assumed to be associated with the \nannotation 1. We use \n to denote \n for \nis a predicate and\n. Let \n be \nthe np-program encoding\n, where \n is the \nset of the following np-rules. \n \nEach fluent \n is encoded as a fact of the form\n. Fluent literals are encoded as \nTo specify that fluents \n and \n are contrary literals, we \nuse the following np-rules. \n• The initial belief state \n is \nrepresented in \n as follows. Let \n be the set \nof \npossible \ninitial \nstates, \nwhere \nfor \neach \n, and the initial probability \ndistribution \nbe \n. \nMoreover, \nlet\n, \n, \n. Let \n is a sensor \nreport literal  be the set of all sensor report literals in all \n. Let \n. \nIntuitively, \nis the same as  after excluding the set of \nsensor report literals \n. Let \n be the set \nof all pairs \n are sets of literals \ncontained in \n is the set of sensor reading \nliterals and \n is the set of sensor report literals appearing \nin \n. The set of all possible initial states are generated as \nfollows: for each \n  \n \nwhich represents a fact that holds in every possible initial \nstate. It says that the literal  holds at time moment 0. In \naddition, for each \n includes \n \nThese np-rules say  (similarly \n  holds at time moment \n0, if \n (similarly ) does not hold at the time moment 0. \nFor each \n, let \n, then for \neach \n includes \n \nThe initial probability distribution over the initial states is \nencoded as follows, which says that the probability of a \nstate at time 0 is \n  hold at the time 0. \n \n \n• Each executability condition of an action of the form (2) \nis encoded for each \n as \n \n \nFor \neach \nnon-sensing \naction \nproposition\n, \nin \n. \nThen, \nincludes  \n \nIf \n occurs at time \n and \n holds at the same time \nmoment, then the \n. Then, we \nhave \n \nwhere \n is an annotation variable ranging over \n acts \nas a place holder. This np-rule states that if \n holds in a \nstate at time \n, whose probability is \n, and in which  is \nexecutable, then the probability of a successor state at time \n is \n holds. \n \n \nFor \neach \nsensing \naction \nproposition \n, \nin \n and \n. Then, \n includes \n \nwhere first p- rule says that executing the sensing action  \nat time  in which \n holds causes \n to be observed to be \nknown true at the same moment \n, and second p-rule \nstates that if  occurs at time \n and the literals in \n are \nobserved to be known true at the same moment, then the \nliterals \n are known to hold at the time moment\n. \n \nThe above np-rule says that the probability of a state at \ntime \n is \n become known true at the same \nmoment, after executing \n in a state at time \n, whose \nprobability is \n, in which the literals in \n are observed \ntrue. \n  \n• The reward \n received at time \n  after executing  \nin a state at time  is encoded as \n \n \n•  The value function \n steps away from the initial \nstate given the value function \n steps away from the \ninitial state in a given episode is encoded as \n \n-- if  is a non-sensing action \n \n-- if  is a sensing action \n \nwhere the variables \n, \n, and \n. These np-rules state that the \nvalue function at time \n is equal to the value function \nat time  added to the product of the reward \n received in \na state at \n and the probability of a state at time \n \ndiscounted by \n.  \n The following np-rule asserts that a literal \n holds at \n if it holds at \n and its contrary does not hold at \n. \n \n The literal, \n, and its negation, \n, cannot hold at the \nsame time, where \n is a literal that does not \nappear in \n. \n \n Actions are generated once at a time by the np-rules: \n \n The goal expression \n is encoded as \n \n \n \n \nIn this section we prove that the probabilistic answer sets \nof the np-program encoding of an action theory, \n, \ncorrespond to trajectories in \n, with associated value \nfunction. Moreover, we show that the complexity of \nfinding a policy for \n in our approach is NP-complete. \nLet the domain of \n be \n. Let  be a \n transition \nfunction associated with \n  \n be a possible initial state, \nand \n be a set of actions in \n. Recall, any \naction \n  can be represented as \n. \nTherefore, \na \ntrajectory \n in \n can be \nalso represented as \n for \n \nand\n, such that \n is a state, \n is \nan action, \n, \n, and \n. \n \nTheorem 1: Let \n be an action theory in \n be \na policy in \n be the set of trajectories in \n. \nThen, \nis \na \ntrajectory in \n \nis true in a probabilistic answer set of \n. \n \n \nIntuitively, an action theory,  \n, can be \nencoded to an np-program, \n, whose probabilistic \nanswer sets correspond trajectories in \n. \n \nTheorem 2:   Let  be a probabilistic answer set of \n be \na policy in \n be the set of trajectories in \n. Let \n \nbe \na \nset \nthat \ncontains \n, \n iff \n, \n. Then, \n \nTheorem (2) states that the summation of the values \n, \nappearing in \n that is satisfied by a probabilistic \nanswer set  in which \n \nis satisfied is equal to the expected sum of discounted rewards \nafter executing a policy  starting from a state \n. \nThe np-program encoding of the reinforcement learning \nproblems, in finite-horizon POMDP, finds optimal policies \nusing the flat representation of the problem domains. Flat \nrepresentation is the explicit enumeration of world states [23]. \nHence, Theorem 4 follows directly from Theorem 3 \nTheorem 3 ([23]): The stationary policy existence problem \nfor finite-horizon POMDP in the flat representation is NP-\ncomplete. \nTheorem 4: The policy existence problem for a rein-\nforcement learning problem in POMDP environment using \nNHPLP\n with probabilistic answer set semantics is NP-\ncomplete. \n \nVII.   REINFORCEMENT  LEARNING  USING  ANSWER  SET \nPROGRAMMING \n \nReinforcement learning problems in POMDP can be also \nencoded as classical normal logic programs with classical an-\nswer set semantics [7]. Excluding the np-rules (15), (18), (21) \n– (24) from the np-program encoding, \n, of \n, results \nnp-program, denoted by\n, with only annotations of the \nform 1. As shown in [34], the syntax and semantics of this \nclass of np-programs is equivalent to classical normal logic \nprograms with classical answer set semantics. \n \nTheorem  5:  Let \n be the normal logic program \nresulting after deleting the np-rules (15), (18), (21) -- (24) \nfrom \n. \nThen, \n is a trajectory in \n is true in an \nanswer set of \n. \nTheorem 5 shows that classical normal logic programs with \nanswer set semantics can be used to solve reinforcement \nlearn-ing problems in POMDP in two steps. First, a \nreinforcement learning problem, \n, is encoded to a classical \nnormal logic program whose answer sets correspond to valid \ntrajectories in \n. From the answer sets of the normal logic \nprogram encoding of \n, we can determine the set of \ntrajectories \n for a policy  in \n. Second, the value of the \npolicy  is calculated using (5). Moreover, any reinforcement \nlearning problem in POMDP environment can be encoded as a \nSAT problem. Hence, state-of-the-art SAT solvers can be \nused to solve reinforcement learning problems. Any normal \nlogic program, \n, can be translated into a SAT formula, S, \nwhere the models of S are equivalent to the answer sets of \n[19]. Therefore, the normal logic program encoding of a \nreinforcement learning problem \n can be translated into an \nequivalent SAT formula, where the models of S correspond to \nvalid trajectories in \n. \n \nTheorem 6: Let \n be an action theory and \n be the \nnormal logic program encoding of \n. Then, the models of \nthe SAT encoding of \n are equivalent to valid \ntrajectories in \n. \n \nReinforcement learning problems can be directly encoded to \nSAT [32]. This is shown by following corollary. \nVI.   CORRECTNESS \nCorollary 1: Let \n be an action theory. Then, \n can be \ndirectly encoded as a SAT formula S where the models of S \nare equivalent to valid trajectories in \n. \nVIII.   CONCLUSIONS  AND  RELATED  WORK \n \nWe described a new high level action language, \n, that \nallows the factored representation of POMDP. Moreover, we \npresented a new reinforcement learning framework by relating \nreinforcement learning in POMDP to NHPLP. The translation \nfrom an action theory representation of a reinforcement \nlearning problem in \n into an NHPLP program is based on \na similar translation from probabilistic planning into NHPLP \n[28].  The difference between \n and the action languages \n[1], [2], [5], [11], and [17]  is that \n is a hight level \nlanguage and allows the factored specification of POMDP. \n  The approaches for solving POMDP to find the optimal \npolicies can be categorized into two main approaches; \ndynamic programming approaches and the search-based \napproaches (a detailed survey on these approaches can be \nfound in [2]). However, dynamic programming approaches \nuse primitive domain knowledge representation. Moreover, \nthe search-based approaches mainly rely on search heuristics \nwhich have limited knowledge representation capabilities to \nrepresent and use domain-specific knowledge. \nIn [22], a logical approach for solving POMDP, for \nprobabilistic contingent planning, has been presented which \nconverts a POMDP specification of a probabilistic contingent \nplanning problem into a stochastic satisfiability problem and \nsolving the stochastic satisfiability problem instead. Our \napproach is similar in spirit to [22] in the sense that both \napproaches are logic based approaches. However, it has been \nshown in [29] that NHPLP is more expressive than stochastic \nsatisfiability from the knowledge representation point of view. \nIn [15], based on first-order logic programs without \nnonmonotonic negation, a first-order logic representation of \nMDP has been described. Similar to the first-order \nrepresentation of MDP in [15], AMD allows objects and \nrelations. However, unlike APO, [15] finds policies in the \nabstract level. But, NHPLP allows objects and relations. [3] \npresented a more expressive first-order representation of MDP \nthan [15] that is a probabilistic extension to Reiter’s situation \ncalculus. However, it is more complex than [15]. \n \nREFERENCES \n \n[1] C. Baral, N. Tran, L. C. Tuan. Reasoning about actions in a \nprobabilistic setting. In AAAI, 2002.  \n \n[2] C. Boutilier, T. Dean, and S. Hanks. Decision-theoretic planning: \nstruc-tural assumptions and computational leverage. Journal of AI \nResearch, 11:1–94, 1999.  \n \n[3] C. Boutilier, R. Reiter, and B. Price. Symbolic dynamic \nprogramming for first-order MDPs. In 17th IJCAI, 2001.  \n \n[4] D. Draper, S. Hanks, and D. Weld. Probabilistic planning with \ninformation gathering and contingent execution. In Proc. of the 2nd \nInternational Conference on Artificial Intelligence Planning Systems \n, pages 31–37, 1994.  \n \n[5] T. Eiter and T. Lukasiewicz. Probabilistic reasoning about actions in \nnonmonotonic causal theories. In 19th Conference on Uncertainty in \nArtificial Intelligence , 2003.  \n \n[6] T. Eiter et al. Declarative problem solving in dlv. In Logic Based \nArtificial Intelligence, 2000.  \n \n[7] M. Gelfond and V. Lifschitz. The stable model semantics for logic \nprogramming. ICSLP, 1988, MIT Press.  \n \n[8] M. Gelfond and V. Lifschitz. Classical negation in logic programs \nand disjunctive databases. New Generation Computing, 9(3-4):363-\n385, 1991.  \n \n[9] M. Gelfond and V. Lifschitz Representing action and change by \nlogic programs. Journal of Logic Programming, 17:301–321, 1993.  \n \n[10] . S. Holldobler, E. Karabaev, and O. Skvortsova. FluCap: A heuristic \nsearch planner for first-order MDPs. JAIR, 27:419–439, 2006.  \n [11] L. Iocchi, T. Lukasiewicz, D. Nardi, and R. Rosati. Reasoning about \nactions with sensing under qualitative and probabilistic uncertainty. In  \n16th  European  Conference  on  Artificial  Intelligence ,  2004. \n \n \n[12] L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in \npartially observable stochastic domains. Artificial Intelligence , 101: \n99-134, 1998.  \n \n[13] L. Kaelbling, M. Littman, and A. Moore. Reinforcement Learning: A \nSurvey. Journal of Artificial Intelligence Research , 4:237-285, \n1996.  \n \n[14] H. Kautz and B. Selman. Pushing the envelope: planning, \npropositional logic, and stochastic search. In 13th National \nConference on Artificial Intelligence, 1996.  \n \n[15] K. Kersting and L. De Raedt. Logical Markov decision programs and \nthe convergence of logical TD(λ). In 14th International Conference \non Inductive Logic Programming, 2004.  \n \n[16] K. Kersting, M. van Ottterlo, and L. De Raedt. Bellman goes \nrelational. In ICML, 2004.  \n \n[17] N. Kushmerick, S. Hanks, and D. Weld. An algorithm for \nprobabilistic planning. Artificial Intelligence , 76(1-2):239-286, \n1995.  \n[18] V.  Lifschitz.  Answer  set  planning.  In  ICLP,  1999.  \n \n[19] F. Lin and Y. Zhao. ASSAT: Computing answer sets of a logic \nprogram by SAT solvers. Artificial Intelligence , 157(1-2):115-137, \n2004.  \n \n[20] M. Littman, A. Cassandra, and L. Kaelbling. Learning policies for \npartially observable environments: scaling up. In 12th ICML, 1995.  \n \n[21] S. Majercik and M. Littman. MAXPLAN: A new approach to proba-\nbilistic planning. In 4th International Conference on Artificial \nIntelligence Planning, pages 86–93, 1998.  \n \n[22] S. Majercik and M. Littman. Contingent planning under uncertainty \nvia stochastic satisfiability. Artificial Intelligence , 147(1–2):119–\n162, 2003.  \n \n[23] M. Mundhenk, J. Goldsmith, C. Lusena, and E. Allender. \nComplexity of finite-horizon Markov decision process problems. \nJournal of the ACM, 2000.  \n \n[24] I. Niemela and P. Simons. Efficient implementation of the well-\nfounded and stable model semantics. In Joint International \nConference and Symposium on Logic Programming, 289-303, 1996.  \n \n[25] E.  Saad.   Incomplete  knowlege  in  hybrid  probabilistic  logic  \nprograms.  \n \nIn  10th  European  Conference  on  Logics  in  Artificial  Intelligence ,  \n2006.  \n \n[26] E. Saad. Towards the computation of the stable probabilistic model \nsemantics. In 29th Annual German Conference on Artificial \nIntelligence , June 2006.  \n \n[27] E. Saad. A logical approach to qualitative and quantitative reasoning. \nIn 9th European Conference on Symbolic and Quantitative \nApproaches to Reasoning with Uncertainty, 2007.  \n \n[28] E. Saad. Probabilistic planning in hybrid probabilistic logic \nprograms. In 1st International Conference on Scalable Uncertainty \nManagement, 2007.  \n \n[29] E. Saad On the relationship between hybrid probabilistic logic \nprograms and stochastic satisfiability. In 10th International \nSymposium on Artificial Intelligence and Mathematics, 2008.  \n \n[30] E. Saad. A logical framework to reinforcement learning using hybrid \nprobabilistic logic programs. In Proceedings of the 2nd International \nConference on Scalable Uncertainty Management (SUM’08), 2008.  \n \n[31] E. Saad. Probabilistic planning with imperfect sensing actions using \nhybrid probabilistic logic programs. In SUM, 2009.  \n \n[32] E. Saad. Probabilistic reasoning by SAT solvers. In Tenth \nECSQARU, 2009.  \n \n[33] E. Saad and E. Pontelli. Towards a more practical hybrid \nprobabilistic logic programming framework. In Practical Aspects of \nDeclarative Languages, 2005.  \n \n[34] E. Saad and E. Pontelli. A new approach to hybrid probabilistic logic \nprograms Annals of Mathematics and Artificial Intelligence Journal , \n48(3-4):187-243, 2006.  \n \n[35] S. Sanner and C. Boutilier. Practical solution techniques for first-\norder MDPs. AIJ, 173:748–788, 2009.  \n \n[36] R. Scherl and H. Levesque. The frame problem and knowledge \nproduc-ing actions. In AAAI, 1993.  \n \n[37] T. Son, C. Baral, T. Nam, and S. McIlraith. Domain-dependent \nknowl-edge in answer set planning. ACM Transactions on \nComputational Logic, 7(4):613–657, 2006.  \n \n[38] V. S. Subrahmanian and C. Zaniolo. Relating stable models and AI \nplanning \ndomains. \nIn \nInternational \nConference \nof \nLogic \nProgramming, 233-247, 1995.  \n \n[39]R. Sutton and A. Barto. Reinforcement Learning: An Introduction. \nMIT Press, Cambridge, MA, 1998. \n \n \n \nIX.   APPENDIX:  EXAMPLE \n \nExample 2: The np-program encoding of the tiger domain \npresented in Example 1 is given by \n, where \n consists of the following np-rules, \nin addition to the np-rules (7), (8), (9), (10), (25), (26), \n(27), (28): \n \n \n \n \n \nThe value function is encoded in $R$ by the np-rules: \n \n \n \n \n \n",
  "categories": [
    "cs.AI"
  ],
  "published": "2010-11-27",
  "updated": "2010-11-27"
}