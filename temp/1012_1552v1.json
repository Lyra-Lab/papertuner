{
  "id": "http://arxiv.org/abs/1012.1552v1",
  "title": "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework",
  "authors": [
    "Emad Saad"
  ],
  "abstract": "Knowledge Representation is important issue in reinforcement learning. In\nthis paper, we bridge the gap between reinforcement learning and knowledge\nrepresentation, by providing a rich knowledge representation framework, based\non normal logic programs with answer set semantics, that is capable of solving\nmodel-free reinforcement learning problems for more complex do-mains and\nexploits the domain-specific knowledge. We prove the correctness of our\napproach. We show that the complexity of finding an offline and online policy\nfor a model-free reinforcement learning problem in our approach is NP-complete.\nMoreover, we show that any model-free reinforcement learning problem in MDP\nenvironment can be encoded as a SAT problem. The importance of that is\nmodel-free reinforcement",
  "text": " \nBridging the Gap between Reinforcement Learning and \nKnowledge Representation: A Logical Off- and On-Policy \nFramework \n \nEmad Saad 1 \n \nAbstract. Knowledge Representation is important issue in \nreinforcement learning. In this paper, we bridge the gap between \nreinforcement learning and knowledge representation, by providing \na rich knowledge representation framework, based on normal logic \nprograms with answer set semantics, that is capable of solving \nmodel-free reinforcement learning problems for more complex do-\nmains and exploits the domain-specific knowledge. We prove the \ncorrectness of our approach. We show that the complexity of \nfinding an offline and online policy for a model-free reinforcement \nlearning problem in our approach is NP-complete. Moreover, we \nshow that any model-free reinforcement learning problem in MDP \nenvironment can be encoded as a SAT problem. The importance of \nthat is model-free reinforcement learning problems can be now \nsolved as SAT problems. \n \n1     Introduction \n \nReinforcement learning is the problem of learning to act by trial and \nerror interaction in dynamic environments. Under the assumption that a \ncomplete model of the environment is known, a reinforcement learning \nproblem is modeled as a Markov Decision Process (MDP), in which an \noptimal policy can be learned. Operation research methods, in \nparticular dynamic programming by value iteration, have been \nextensively used to learn the optimal policy for a reinforcement learn-\ning problem in MDP environment. However, an agent may not know \nthe model of the environment. In addition, an agent may not be able to \nconsider all possibilities and use its knowledge to plan ahead, because \nof the agent’s limited computational abilities to consider all states \nsystematically [4]. Therefore, Q-learning [4] and SARSA [17] are \nproposed as model-free reinforcement learning algorithms that learn \noptimal policies without the need for the agent to know the model of \nthe environment. \n \nQ-learning and SARSA are incremental dynamic programming \nalgorithms, that learns optimal policy from actual experience from \ninteraction with the environment, where to guarantee convergence the \nfollowing assumptions must hold; the action-value function is \nrepresented as a look-up table; the environment is a deterministic MDP; \nfor each starting state and action, there are an infinite number of \nepisodes; and the learning rate is decreased appropriately over time. \nHowever, these assumptions imply that all actions are tried in every \npossible state and every state must be visited infinitely many times, \nwhich leads to a slow convergence, although, it is sufficient for the \nagent to try all possible actions in every possible state only \n \n1 Department of Computer Science, Gulf University for Science and Tech-\nnology, Kuwait, email: saad.e@gust.edu.kw  \n \nonce to learn about the reinforcements resulting from executing \nactions in states. In addition, in some situations it is not possible for \nthe agent to visit a state more than once. Consider a deer that eats in \nan area where a cheetah appears and the deer flees and survived. If \nthe deer revisits this area again it will be eaten and does not learn \nanymore. This is unavoidable in Q-learning and SARSA because of \nthe iterative dynamic programming approach they adopt and their \nconvergence assumptions. Moreover, dynamic programming meth-\nods use primitive representation of states and actions and do not ex-\nploit domain-specific knowledge of the problem domain, in \naddition they solve MDP with relatively small domain sizes [16]. \nHowever, using richer knowledge representation frameworks for \nMDP allow to efficiently find optimal policies in more complex \nand larger domains. \n \nA logical framework to model-based reinforcement learning has \nbeen proposed in [19] that overcomes the representational \nlimitations of dynamic programming methods and capable of \nrepresenting domain specific knowledge. The framework in [19] is \nbased on the integration of model-based reinforcement learning in \nMDP environment with normal hybrid probabilistic logic programs \nwith probabilistic answer set semantics [23] that allows \nrepresenting and reasoning about a variety of fundamental \nprobabilistic reasoning problems including probabilistic planning \n[18], contingent probabilistic planning [21], the most probable \nexplanation in belief networks, and the most likely trajectory [20]. \n \nIn this paper we integrate model-free reinforcement learning with \nnormal logic programs with answer set semantics and SAT, providing a \nlogical framework to model-free reinforcement learning using Q-\nlearning and SARSA update rules to learn the optimal off- and on-\npolicy respectively. This framework is considered a model-free \nextension to the model-based reinforcement learning framework of \n[19]. The importance of the proposed framework is twofold. First, the \nproposed framework overcomes the representational limitations of \ndynamic programming methods to model-free reinforcement learning \nand capable of representing domain-specific knowledge, and hence \nbridges the gap between reinforcement learning and knowledge \nrepresentation. Second, it eliminates the requirement of visiting every \nstate infinitely many times which is required for the convergence of the \nQ-learning and SARSA. \n \nThis integration is achieved by encoding the representation of a \nmodel-free reinforcement learning problem in a new high level action \nlanguage we develop in this paper called, \n, into normal logic \nprogram with answer set semantics, where all actions are tried in every \nstate only once. We show the correctness of the translation. We prove \nthat the complexity of finding an off- and on-policy in our ap- \n \nproach is NP-complete. In addition, we show that any model-free \nreinforcement learning problem in MDP environment can be \nencoded as SAT problem. \n \n2     Preliminaries \n \nAs in the underlying assumptions of the original Q-learning and \nSARSA, the subsequent results in the rest of this paper assume that \nthe considered MDPs are deterministic. Normal logic programs [7] \nand Q-learning [4] and SARSA [17] are reviewed in this section. \n \n2.1     Normal Logic Programs \n \nLet \n be a first-order language with finitely many predicate \nsymbols, function symbols, constants, and infinitely many \nvariables. The Herbrand base of \n is denoted by \n. A Herbrand \ninterpretation is a subset of the Herbrand base \n. A normal logic \nprogram is a finite set of rules of the form \n \nWhere \n are atoms and \n is the \nnegation-as-failure. A normal logic program is ground if no \nvariables appear in any of its rules. Let \n be a ground normal logic \nprogram and  be a Herbrand interpretation, then, we say that the \nabove rule is satisfied by  iff \n, whenever \n \nand \n, \nor \nfor \nsome \n, \n \n \nA Herbrand model of \n is a Herbrand interpretation that satisfies \nevery rule in \n. A Herbrand interpretation \n of a normal logic pro-\ngram \n is said to be an answer set of \n if  is the minimal Herbrand \nmodel (with respect to the set inclusion) of the reduct, denoted by \n, where \n \n \n \n  \n2.2     Q-learning and SARSA \n \nQ-learning learns the optimal Q-function,\n, from the agent’s ex-\nperience (set of episodes) by repeatedly estimating the optimal Q-\nvalue for every state-action pair \n. The Q-value, \n, \ngiven a policy (a mapping from states to actions), is defined as the \nexpected sum of discounted rewards resulting from executing the \naction  in a state  and then following the policy thereafter. Given \n, an optimal policy, \n, can be determined by identifying the \noptimal action in every state, where  is optimal in a state , i.e., \n \nand \n \nis \nexecutable in . An episode is an exploration of the environment \nwhich is a sequence of state-action-reward-state of the form \n, where for each \n means that an agent executed \naction \n in state \n and rests in state \n where it received reward \n. \n denotes an initial state and \n is a terminal (goal) state. Given \nthat the agent sufficiently explored the environment, the optimal Q-\nvalues are repeatedly estimated by the following algorithm: \n \n initialize \n arbitrary \nRepeat forever for each episode \n \nSelect the initial state  of an episode \n \nRepeat \nChoose an action \n for the current state\n \nExecute the action \n in  \n \nObserve the subsequent state \n \n \nReceive an immediate reward \n \n \nSet \n \n \nUntil \n is the end of an episode \n \nwhere \n is the learning rate, \n is the discount factor, and \n is the reward received in \n from executing \n in \n. Q-learning is an offline algorithm that learns the optimal Q-\nfunction while executing another policy. Under the same conver-\ngence assumptions as in Q-learning, SARSA [17] has been devel-\noped as an online model-free reinforcement learning algorithm, that \nlearns optimal Q-function while exploring the environment. Similar \nto Q-learning, SARSA is an iterative dynamic programming algo-\nrithm whose update rule is given by: \n \n \nIn addition, SARSA converges slowly to Q∗, since it requires ev-\nery state to be visited infinitely many times with all actions are \ntried. Although, it is sufficient for an agent to try all possible \nactions in every possible state only once to learn about the \nreinforcements resulting from executing every possible action in \nevery possible state. This assumption could not be eliminated in Q-\nlearning and SARSA, since both are iterative dynamic \nprogramming algorithms. However, under the assumption that the \nenvironment is finite-horizon Markov decision process with finite \nlength episodes, estimating the optimal Q-function, \n for Q-\nlearning, can be simply computed recursively as: \n \n \n \nSimilarly, the estimate of the optimal Q-function for SARSA can \nbe described as: \n \n \nEquations (1) and (2) show that it is sufficient to consider the \nrewards collected from the set of all episodes, \n, only once to \ncalculate estimate of the optimal Q-function, \n, which eliminates \nthe need to visit every possible state infinitely many times. \n \nUnlike Q-learning, our estimate of \n, can be computed online as \nwell as offline. It can be computed online by accumulating estimate of \n during the exploration of the environment. On the other hand, it can \nbe computed offline by first exploring the environment and \n \ncollecting the set of all possible episodes, then computing estimate \nof \n. \n3     Action Language \n \n \nThis section develops the syntax and semantics of the action lan-\nguage, \n, that allows the representation of model-free reinforce-\nment learning problems, which extends the action language \n [8]. \n \n3.1     Language syntax \n \nA fluent is a predicate, which may contain variables, that describes \na property of the environment. Let \n be a set of fluents and \n be a \nset of action names that can contain variables. A fluent literal is \neither a fluent \n, the negation of . Conjunctive fluent \nformula is a conjunction of fluent literals of the form \n, \nwhere \n are fluent literals. Sometimes we abuse the \nnotation and refer to a conjunctive fluent formula as a set of fluent \nliterals (\n). An action theory, \n is a tuple of \nthe form \n, where \n is a proposition of the form (3), \n is a set of propositions from (4-6), and \n is a discount \nfactor as follows: \n \n \n \nwhere  is a fluent literal,  \n are conjunctive fluent \nformulas, \n is an action, and  is a real number in . \n \nProposition (3) represents the set of possible initial states. \nProposition (4) states that an action \n is executable in any state in \nwhich  holds, where each variable that appears in  also appears in . \nIndirect effect of action is described by proposition (5), which says that \n holds in every state in which  also holds. A proposition of the form \n \n(6) represents the conditional effects of an action  along with the \nrewards received in a state resulting from executing . All variables \nthat appear in  also appear in  and . Proposition (6) says that  \ncauses  to hold with reward  is received in a successor state to a \nstate in which \n is executed and \n holds. An action theory is \nground if it does not contain any variables. \n \nExample 1 Consider an elevator of n-story building domain \nadapted from [5] that is represented by an action theory, \n is described by (7) (  is a \nparticular value in \n) and \n \nis represented by (8)-(14). \n \n \n  \nThe actions in the elevator domain are \n for move up to floor \n, \n for move down to floor \n, and \n for closing the \nelevator door. The predicates\n  \nare \n \nfluents represent respectively that the elevator current floor is \n, \nlight of floor \n is on, and elevator door is opened. The target is to \nget all floors serviced and \n is true for all \n. \n \n3.2     Semantics \n \nWe say a set of ground literals  is consistent if it does not contain \na pair of complementary literals. If a literal \n, then we say  \nholds in , and  does not hold in  if \n. A set of literals  \nholds in  if  is contained in , otherwise,  does not hold in . \nWe say that a set of literals  satisfies an indirect effect of action of \nthe form (5), if  belongs to  whenever  is contained in  or  is \nnot contained in . Let \n be an action theory in \n and  be a set \nof literals. Then \n is the smallest set of literals that contains  \nand satisfies all indirect effects of actions propositions in \n. A state \n is a complete and consistent set of literals that satisfies all the \nindirect effects of actions propositions in \n. \n \nDefinition 1 Let \n be a ground action theory in \n, \n be a state, \n be a proposition in \n. Then, \n is the state resulting from executing  in , given \nthat  is executable in , where \n is defined as: \n \n \nwhere the reward received in \n. \n \nAn episode in \n is an expression of the form \n, \nwhere \nfor \neach \nDefinition 2 Let \n be a ground action theory and \n \nbe \nthe \nset \nof \nall \nepisodes \nin \n. \nThen, \nfor \n, the optimal Q-function, \n, for Q-learning and SARSA are respectively estimated by \n \n \n \nConsidering SARSA, the optimal Q-function can be computed in-\ncrementally as follows. For any episode in \n, the optimal Q-value \nfor the initial state-action pair is estimated by  \n \n \n \nthat  is  calculated  online  during  the  exploration of the \nenvironment. Then, for any state-action pair, \n, in the \nepisode, \n, is calculated from \n by \n \n \n \nHowever, for Q-learning, \n can be computed incrementally as \nwell by first computing \n incrementally using (15), then (16) is \nused as an update rule only once, where for \n \n \n \n \nNotice that, unlike [4], by using (15) and (16), Q-learning can be \ncomputed online during the exploration of the environment as well \nas offline. \n \n4 Off- and On-Policy Model-free Reinforcement \nLearning Using Answer Set Programming  \n \nWe provide a translation from any action theory \n, a \nrepresentation of a model-free reinforcement learning problem into \na normal logic program with answer set semantics \n, where the \nrules in \n encode (1) the set of possible initial states \n, (2) the \ntransition function \n, (3) the set of propositions in \n, (4) and the \ndiscount factor . The answer sets of \n correspond to episodes in \n, with associated estimated optimal Q-values. This translation \nfollows some related translations described in [24, 18, 19]. \n \nWe assume the environment is a finite-horizon Markov decision \nprocess, where the length of each episode is known and finite. We use \nthe predicates;  \n to represent a literal \n holds at time \nmoment \n; \n for action \n executes at time \n ; \n for reward received at time \n after executing  is ; \n says the estimate of the optimal Q-value of the initial state-\naction pair, in a given episode, \n steps from the initial state is \n; and \n for the discount factor. We use lower case letters to \nrepresent constants and upper case letters to represent variables. \n \nLet \n be the normal logic program translation of \n that contains a set of rules described as follows. To \nsimplify the presentation, given  is a predicate and \n be a set of literals, we use \n to denote \n \n \n For each action \n includes the set of facts \n \n \n \n Literals describe states of the world are encoded by \n \n \n \nwhere \n is a set of facts that describe the properties of the \nworld. To specify that \n are contrary literals the \nfollowing rules are added to \n. \n \n \n The reward  received at time \n after executing  at time \n \ngiven that  is executable is encoded by \n \n \n \n Estimate of the optimal Q-value of an initial state-action pair, in a \ngiven episode, \n steps away from the initial state, is equal to \nthe estimate of the optimal Q-value of the same initial state-action \npair, in the same episode, \n steps away from the initial state added \nto the discounted reward (by \n) received at time \n, where \n  \n \n \n \n \n \n \n   \n The following rule says that  holds at the time moment \n if \nit holds at the time moment \n and its contrary does not hold at the \ntime moment \n.  \n \n \n  \n A literal \n and its negation \n cannot hold at the same time is \nencoded in \n by \n \n \n \n Rules that generate actions occurrences once at a time are \nencoded by \n \n \n  \n The set of initial states, \n, is encoded as \nfollows. Let \n be the set of initial states, where for \n. Moreover, let \n, \nIntuitively, for any literal \n  belongs to , then  \n \ncontains only . For each literal \n includes \n \n \n \nwhich says  holds at time 0. Literals in  belong to every initial \nstate. For each \n includes \n \nwhich says that  (similarly \n) holds at time 0, if \n (similarly ) \ndoes not hold at the time 0. \n \n Each proposition of the form (4) is encoded in \n as \n \n \n \n Let  \n be a goal expression, then  is encoded \nin \n as \n \n \n \nEstimates of the optimal Q-value of initial state-action pair, \n, is represented in \n, \nwhere  \n represents the estimate of \n at the end \nof episode of length \n. These Q-values, \n, can be \ncomputed online during the exploration of the environment as well \nas offline after the exploration of the environment. Moreover, the \naction generation rules (31) and (32) in our translation, choose \nactions greedily at random. However, other action selection \nstrategies can be encoded instead. \n \nExample 2 The normal logic program encoding, \n, of the \nelevator domain described in Example 1 is given as follows, where \n consists of the following rules, along with the rules (18), (19), \n(20), (21), (29), (30), (31), (32): \n  \n Each \n is encoded as \n \n \n \n, which says that if  occurs at time \n holds at the same time moment, then  holds at time \n \n \nfor \n The atoms \n \ndescribe properties of the world that for\n are encoded \nas \n \n \n \n \nThe initial state is encoded as follows, where\n , for \n and for some \n \n \n \n \nThe executability conditions of actions, for \n, are \nencoded as \n \n \n  \nEffects, rewards, and the Q-value of the initial state-action pair \nresulting after executing the actions \n and \n, \nfor\n, are given by \n \n \n  \n \nwhere \n is a fact, \n, and \n \n \nEffects of the \n action is given by \n \n \nThe reward received after executing \n is given by \n \n \n \nQ-value of the initial state-action pair is given by the following \nrule, where\n is a fact. \n \n \n \nThe goal is encoded by the following rule for some \n \n \n \n \n5     Correctness \n \nThis section shows the correctness of our translation. We prove that \nthe answer sets of the normal logic program translation of an ac-\ntion theory, \n in \n, correspond to episodes in \n, \nassociated with estimates of the optimal Q-values. Moreover, we \nshow that the complexity of finding a policy for \n in our approach \nis NP-complete. Let the domain of \n be \n. Let \n be a \ntransition function associated with \n is an initial state, and \n be a set of actions in \n. An episode in \n is state-\naction-reward-state \nsequence \nof \nthe \nform \n, \nsuch \nthat \n, \n \nare \nstates, \n \nis \nan \naction, \n, and \n. \n \nTheorem 1 Let \n be an action theory representing a model-free \nreinforcement learning problem in \n. Then, \n is an episode in \n iff \n  \n \nTheorem 1 says that an action theory, \n, in \n, can be translated \ninto a normal logic program, \n, such that an answer set of \n is \nequivalent to an episode in \n. \n \nTheorem 2 Let \n be an action theory in \n be an answer set of \n be the set of all episodes in \n. Let \n be a set such \nthat \n iff \n \n \n Theorem 2 asserts that, given an action theory \n and by \nconsidering Q-learning update rule, the expected sum of discounted \nrewards resulting after executing an action \n in a state \n and \nfollowing the optimal policy thereafter, \n, is equal to the \nmaximum over , appearing in \n which is satisfied by \nevery answer set \n for which \n \n is \nalso satisfied. However, by considering the update rule of SARSA, \n is equal to  in \n that is satisfied by some \nanswer set of \n for which  is also satisfied. For any \n and  in \nSARSA, \n is calculated from \n by (15), where \n But, for Q-learning, \nfor any \n and , \n is calculated from \n by (15), \nthen (16) is used as an update rule only once.  \nIn addition, we show that any model-free reinforcement learning \nproblem in MDP environment can be encoded as SAT problem. \nHence, state-of-the-art SAT solvers can be used to solve model-free \nreinforcement learning problems. Any normal logic program,\n, \ncan be translated into a SAT problem, \n, where the models of \n \nare equivalent to the answer sets of \n [13]. Hence, the normal logic \nprogram encoding of a model-free reinforcement learning problem \n can be translated into an equivalent SAT problem, where the \nmodels of S correspond to episodes in \n. \n \nTheorem 3 Let \n be an action theory in \n and \n be the normal \nlogic program encoding of \n. Then, the models of the SAT \nencoding of \n are equivalent to valid episodes in \n. \n \nFor SAT encoding, the optimal Q-function is computed in a similar \nway as in the normal logic program encoding of model-free \nreinforcement learning problems. The transformation step from \nnor-mal logic program encoding of a model-free reinforcement \nlearning problem into SAT can be avoided, by encoding a model-\nfree reinforcement learning problem directly into SAT [22]. The \nfollowing corollary shows any model-free reinforcement learning \nproblem can be encoded directly as SAT problem. \nCorollary 1 Let \n be an action theory in \n. Then, \n can be \ndirectly encoded as a SAT formula \n where the models of \n are \nequivalent to valid episodes in \n. \nNormal logic programs with answer set semantics find optimal policies \nfor model-free reinforcement learning problems in finite horizon MDP \nenvironments using the flat representation of the problem domains.\n \nThe flat representation of reinforcement learning problem domains \nis the explicit enumeration of world states [14]. Hence, Theorem 5 \nfollows directly from Theorem 4 [14]. \n \nTheorem 4 The stationary policy existence problem for finite-\nhorizon MDP in the flat representation is NP-complete. \n \nTheorem 5 The policy existence problem for a model-free \nreinforcement learning problem in MDP environment using normal \nlogic pro-grams with answer set semantics and SAT is NP-complete. \n \n6     Conclusions and Related Work \n \nWe described a high level action language called \n that allows \nthe representation of model-free reinforcement learning problems \nin MDP environments. In addition, we introduced online and \noffline logical framework to model-free reinforcement learning by \nrelating model-free reinforcement learning in MDP environment to \nnormal logic programs with answer set semantics and SAT. \n \nThe translation from an action theory in \n into a normal logic \nprogram builds on similar translations described in [24, 18, 19]. The \nliterature is rich with action languages that are capable of represent-ing \nand reasoning about MDPs and actions with probabilistic effects, which \ninclude [1, 2, 6, 9, 12]. The main difference between these languages \nand \n is that \n allows the factored characterization of MDP for \nmodel-free reinforcement learning. \n \nMany approaches for solving MDP to find the optimal policy for \nboth reinforcement learning and probabilistic planning have been \npresented. These approaches can be classified into two main cat-\negories of approaches; dynamic programming approaches and the \nsearch-based approaches (a detailed survey on these approaches can \nbe found in [10, 2]). However, dynamic programming approaches \nuse primitive domain knowledge representation. On the other hand, \nthe search-based approaches mainly rely on search heuristics which \nhave limited knowledge representation capabilities to represent and \nuse domain-specific knowledge. \n \nA logic based approach for solving MDP, for probabilistic plan-ning, \nhas been presented in [15]. The approach of [15] converts MDP \nspecification of a probabilistic planning problem into a stochastic \nsatisfiability problem and solving the stochastic satisfiability problem \ninstead. First-order logic representation of MDP for model-based \nreinforcement learning has been described in [11] based on first-order \nlogic programs without nonmonotonic negations. Similar to the first-\norder representation of MDP in [11], \n allows objects and relations. \nHowever, unlike \n, [11] finds policies in the abstract level. A more \nexpressive first-order representation of MDP than [11] has been \npresented in [3] that is a probabilistic extension to Reiter’s situation \ncalculus. Although more expressive, it is more complex than [11]. \nUnlike the logical model-based reinforcement learning frame-work of \n[19] that uses normal hybrid probabilistic logic programs to encode \nmodel-based reinforcement learning problems, normal logic program \nwith answer set semantics is used to encode model-free reinforcement \nlearning problems. \n \nREFERENCES \n \n[1] C. Baral, N. Tran, and L. C. Tuan, ‘Reasoning about actions in a prob-\nabilistic setting’, in AAAI, (2002).  \n \n[2] C. Boutilier, T. Dean, and S. Hanks, ‘Decision-theoretic planning: \nstructural assumptions and computational leverage’, Journal of AI Re-\nsearch, 11(1), (1999).  \n[3] C. Boutilier, R. Reiter, and B. Price, ‘Symbolic dynamic \nprogramming for first-order mdps’, in 17th IJCAI, (2001).  \n \n[4] Christopher C. Watkins, Learning from delayed rewards, Ph.D. \ndissertation, University of Cambridge, 1989.  \n \n[5] R. Crites and A. Barto, ‘Improving elevator performance using rein-\nforcement learning’, in Advances in Neural Information Processing, \n(1996).  \n \n[6] T. Eiter and T. Lukasiewicz, ‘probabilistic reasoning about actions in \nnonmonotonic causal theories’, in 19th Conference on UAI, (2003).  \n \n[7] M. Gelfond and V. Lifschitz, ‘The stable model semantics for logic \nprogramming’, in ICSLP. MIT Pres, (1988).  \n \n[8] M. Gelfond and V. Lifschitz, ‘Action languages’, Electronic Transac-\ntions on AI, 3(16), 193–210, (1998).  \n[9] L. Iocchi, T. Lukasiewicz, D. Nardi, and R. Rosati, ‘Reasoning about \nactions with sensing under qualitative and probabilistic uncertainty’, in  \n16th European Conference on Artificial Intelligence , (2004).  \n \n[10] L. Kaelbling, M. Littman, and A. Moore, ‘Reinforcement learning: A \nsurvey’, JAIR, 4, 237–285, (1996).  \n[11] K. Kersting and L. De Raedt, ‘Logical markov decision programs and \nthe convergence of logical td(λ)’, in 14th International Conference on \nInductive Logic Programming, (2004).  \n \n[12] N. Kushmeric, S. Hanks, and D. Weld, ‘An algorithm for probabilistic \nplanning’, Artificial Intelligenc , 76(1-2), 239–286, (1995).  \n[13] F. Lin and Y. Zhao, ‘Assat: Computing answer sets of a logic program \nby sat solvers’, Artificial Intelligence , 157(1-2), 115–137, (2004).  \n[14] M. Littman, J. Goldsmith, and M. Mundhenk, ‘The computational \ncom-plexity of probabilistic planning’, Journal of Artificial \nIntelligence Re-search, 9, 1–36, (1998).  \n \n[15] S. Majercik and M. Littman, ‘Maxplan: A new approach to \nprobabilistic planning’, in Fourth International Conference on \nArtificial Intelligence Planning, pp. 86–93, (1998).  \n \n[16] S. Majercik and M. Littman, ‘Contingent planning under uncertainty \nvia stochastic satisfiability’, Artificial Intelligence , 147(1-2), 119–\n162, (2003).  \n \n[17] G. Rummery and M. Niranjan, ‘Online q-learning using connection-\nist systems’, Technical report, CUED/F-INFENG/TR166, Cambridge \nUniversity, (1994).  \n \n[18] E. Saad, ‘Probabilistic planning in hybrid probabilistic logic \nprograms’, in 1st International Conference on Scalable Uncertainty \nManagement, (2007).  \n \n[19] E. Saad, ‘A logical framework to reinforcement learning using hybrid \nprobabilistic logic programs’, in Second International Conference on \nScalable Uncertainty Management, (2008).  \n \n[20] E. Saad, ‘On the relationship between hybrid probabilistic logic pro-\ngrams and stochastic satisfiability’, in Second International \nConference on Scalable Uncertainty Management, (2008).  \n \n[21] E. Saad, ‘Probabilistic planning with imperfect sensing actions using \nhybrid probabilistic logic programs’, in Third International \nConference on Scalable Uncertainty Management, (2009).  \n \n[22] E. Saad, ‘Probabilistic reasoning by sat solvers’, in Tenth ECSQARU, \n(2009).  \n \n[23] E. Saad and E. Pontelli, ‘A new approach to hybrid probabilistic logic \nprograms’, Annals of Mathematics and Artificial Intelligence , 48(3-\n4), 187–243, (2006).  \n \n[24] T. Son, C. Baral, T. Nam, and S. McIlraith, ‘Domain-dependent \nknowl-edge in answer set planning’, ACM Transactions on \nComputational Logic, 7(4), 613–657, (2006).  \n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.LO"
  ],
  "published": "2010-12-07",
  "updated": "2010-12-07"
}