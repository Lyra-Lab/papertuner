{
  "id": "http://arxiv.org/abs/1108.3757v1",
  "title": "Self-Organizing Mixture Networks for Representation of Grayscale Digital Images",
  "authors": [
    "Patryk Filipiak"
  ],
  "abstract": "Self-Organizing Maps are commonly used for unsupervised learning purposes.\nThis paper is dedicated to the certain modification of SOM called SOMN\n(Self-Organizing Mixture Networks) used as a mechanism for representing\ngrayscale digital images. Any grayscale digital image regarded as a\ndistribution function can be approximated by the corresponding Gaussian\nmixture. In this paper, the use of SOMN is proposed in order to obtain such\napproximations for input grayscale images in unsupervised manner.",
  "text": "arXiv:1108.3757v1  [cs.AI]  18 Aug 2011\nSamoorganizujące się sieci mieszankowe w reprezentacji fotograﬁi\ncyfrowych w skali szarości\nPatryk Filipiak\nInstytut Informatyki, Uniwersytet Wrocławski\npatryk.ﬁlipiak@ii.uni.wroc.pl\n4 września 2018\nStreszczenie\nSieci Kohonena (SOM) są najczęściej wykorzystywanym narzędziem w celu tzw. uczenia bez nadzoru.\nDlatego też doczekały się wielu modyﬁkacji i adaptacji. Niniejsza praca poświęcona jest samoorganizu-\njącym się sieciom mieszankowym (SOMN), będącym istotnym rozwinięciem pierwotnej idei Kohonena.\nZdolność SOMN do efektywnego uczenia się dowolnego rozkładu statystycznego ukazana została na przy-\nkładzie fotograﬁi cyfrowych w skali szarości. Dowolny obraz cyfrowy w skali szarości może być przybliżony\nza pomocą skończonej mieszanki gaussowskiej, której parametry dobierane są automatycznie w procesie\nuczenia SOMN. W niniejszej publikacji przedstawiona została grupa przykładów takiego wykorzystania\nSOMN przy użyciu zaimplementowanej w tym celu aplikacji.\n1\nWstęp\nReprezetacja w skali szarości obrazu o rozdzielczości M × N pikseli sprowadza się do przypisania każdemu\nz M · N punktów wartości natężenia jego jasności. Wielkość tę zwyczajowo poddajemy dyskretyzacji do\nwartości całkowitych z przedziału od 0 do 255, co umożliwia przechowanie jej w dokładnie jednym bajcie\npamięci komputera.\nOkreślamy funkcję jasności l : {0, . . . , M −1} × {0, . . . , N −1} −→{0, . . . , 255}, która każdemu pikselowi\nobrazu (x, y) ∈{0, . . ., M −1} × {0, . . ., N −1} przyporządkowuje dyskretną wartość natężenia jego jasności.\nNiech:\nL =\nM−1\nX\nx=0\nN−1\nX\ny=0\nl(x, y).\nWówczas funkcja l′ =\nl\nL jest dyskretyzacją funkcji gęstości pewnego rozkładu wektora losowego w prze-\nstrzeni dwuwymiarowej. Możemy zatem postrzegać obraz w kryteriach rozkładu statystycznego.\nNiech x będzie wektorem losowym w przestrzeni d-wymiarowej Ω⊆Rd (d ⩾1). Mówimy, że wektor\nlosowy x ma rozkład w postaci skończonej mieszanki (ang. ﬁnite mixture distribution), jeżeli funkcja\ngęstości jego rozkładu jest następująca:\np(x) = p1(x)P1 + . . . + pK(x)PK\n(x ∈Ω, K ⩾1),\n(1)\ngdzie\nPi ⩾0, i = 1, . . . , K;\nK\nX\ni=1\nPi = 1\noraz\npi(·) ⩾0, i = 1, . . . , K;\nZ\nΩ\npi(x) dx = 1.\n1\nZmienne P1, . . . , PK nazywać będziemy wagami, zaś funkcje p1(·), . . . , pK(·) — składnikami mieszanki.\nDla mieszanek jednorodnych (tzn. takich, których składniki są funkcjami gęstości tego samego typu) wygodnie\nbędzie zapisać Równanie 1 w postaci\np(x|Θ) =\nK\nX\ni=1\npi(x|θi)Pi,\n(2)\ngdzie θi są wektorami parametrów i-tego rozkładu, zaś Θ = (θ1, . . . , θK) .\nW niniejszej pracy rozważać będziemy mieszanki gaussowskie zadane wzorem\n∀i=1,...,K\npi(x|θi) =\n1\n(2π)\nd\n2 · |Σi|\n1\n2 · exp\n\u0014\n−1\n2(x −mi)T Σ−1\ni (x −mi)\n\u0015\n,\n(3)\ngdzie θi = {θi1, θi2} = {mi, Σi} to (kolejno) wektor wartości średnich oraz macierz kowariancji rozkładu\nnormalnego.\n2\nKlasteryzacja za pomocą sieci Kohonena (SOM)\nNiech d ⩾1 oraz {x ; x = (x1, . . . , xd) ∈Ω⊆Rd} będzie próbką z pewnego rozkładu prawdopodobień-\nstwa (ciągłego lub dyskretnego), zaś {m} = {mi ∈Rd ; i = 1, . . . , K} ustalonym zbiorem tzw. wektorów\nkotwicowych.\nTeselacją Voronoi’a [1] przestrzeni Ω⊆Rd nazywamy procedurę jej podziału na K wypukłych pod-\nzbiorów Vi({m}) wyznaczonych przez wektory mi ∈Rd następująco:\n∀i=1,...,K\nVi({m}) = {x ∈Ω; ∀j̸=i ∥x −mi∥< ∥x −mj∥},\n(4)\ngdzie ∥· ∥oznacza normę euklidesową. Uzyskany podział nazywamy mozaiką Voronoi’a.\nZauważmy, jakie znaczenie odgrywa odpowiedni dobór wektorów kotwicowych. Aby wynikowa mozaika\nVoronoi’a była reprezentatywna dla wyjściowego rozkładu p(x) i użyteczna dla potrzeb klasteryzacji, musimy\nzadbać o to, by gęstość rozmieszczenia wektorów mi była proporcjonalna do gęstości p(x) w danym regionie.\nInnymi słowy — chcemy, aby:\n∀i̸=j\nP(x ∈Vi({m})) = P(x ∈Vj({m})).\nSieci Kohonena (SOM, od ang. Self-organizing Maps [3, 4]) realizują koncepcję tzw. samoorganizacji\ntopologicznej. Nacisk kładziony jest bowiem nie tylko na rozmieszczenie wektorów kotwicowych mi ∈Rd\ntraktowanych niezależnie, ale również ich wzajemne położenie.\nNiech G będzie dowolnym grafem spójnym, którego wierzchołkami są wszystkie K wektory kotwicowe mi.\nNiech ponadto d(i, j) będzie pewną grafową miarą odległości w G. Określmy funkcję eh : R+ ∪{0} −→[0, 1]\no własnościach:\n(a) eh jest ściśle malejąca,\n(b) eh(0) = 1,\n(c) limz→∞eh(z) = 0.\nDla tak zadanego odwzorowania eh zdeﬁniujmy funkcję sąsiedztwa (ang. neighbourhood function) h : {(i, j) ; i, j =\n1, . . . , K, i ̸= j} −→R następująco:\n∀i̸=j\nh(i, j) = eh\n\u0012d(i, j)\nσ\n\u0013\n,\n(5)\ngdzie σ > 0 jest pewnym parametrem algorytmu. W każdej iteracji wektor x leżący w i-tej komórce Voronoi’a\nporusza wszystkie j-te wektory kotwicowe, dla których d(i, j) < σ.\n2\n \nx1\nx2\nmi\nRysunek 1: Przykładowa mozaika Voronoi’a dla d = 2. Rysunek zaczerpnięto z [1].\n2.1\nAlgorytm SOM\n• Dane: Liczba K ∈N, skończony zbiór wektorów losowych x ∈Ω⊆Rd zgodny z rozkładem prawdopo-\ndobieństwa p(x), funkcja sąsiedztwa h(i, j) oraz parametry σ, η.\n• Wynik: Zbiór wektorów kotwicowych {m} = {mi ∈Rd ; i = 1, . . . , K} tworzących strukturę grafu,\nreprezentatywny dla p(x).\nWybierz losowo wszystkie wektory mi ∈Rd, a następnie powtarzaj:\n1. Wylosuj wektor x ∈Ωzgodnie z rozkładem p(x).\n2. Znajdź komórkę Voronoi’a zawierającą x, to znaczy wyznacz indeks i taki, że:\n∀j̸=i\n∥x −mi∥< ∥x −mj∥.\n3. Przesuń wszystkie wektory kotwicowe mj w stronę x według następującej formuły:\nmj ←mj + η(x −mj)h(i, j).\n(6)\n3\nSamoorganizujące się sieci mieszankowe\nSamoorganizujące się sieci mieszankowe (ang. Self-Organizing Mixture Networks, w skrócie SOMN ) [6] są\nuogólnieniem pojęcia Bayesian Self-Organizing Maps (BSOM ) [5]. Cechuje je dwuwartstwowa struktura,\npozwalająca połączyć zalety sieci Kohonena (SOM) z możliwością uczenia macierzy kowariancji oraz wag dla\nkażdej ze składowych mieszanki.\n3.1\nArchitektura sieci\nNa Rysunku 2 przedstawiono schematycznie dwuwarstwową strukturę sieci. Niższa warstwa funkcjonuje na\nzasadach bardzo zbliżonych do sieci Kohonena. Wyższa natomiast powstaje poprzez sumowanie wszystkich\nwęzłów z uwzględnieniem wag Pi, z jakimi węzły te występują w wynikowej mieszance (por. Równanie 2),\ndając w wyniku wartość p(x|Θ) dla dowolnego x ∈Ω.\n3\n \np(x|Θ)\nΣ\nPi\nw\nRysunek 2: Schematyczne przedstawienie SOMN zaczerpnięte z [6].\nW praktycznym zastosowaniu SOMN do reprezentacji obrazów cyfrowych, przekształcenie funkcji gęstości\nrozkładu na wartość jasności każdego kolejnego piksela jest niewystarczające. Ignoruje ono bowiem informację\no średniej jasności całego obrazu, traktując jednakowo obrazy utrzymane w jasnych jak i ciemnych odcieniach,\nodwzorowując jedynie kontrast pomiędzy najjaśniejszym i najciemniejszym pikselem. Prowadzić to może do\nbłędnej reprezentacji wyjściowego obrazu (por. Rysunek 3).\nCelem uniknięcia powyższego błędu, należy dla wejściowego obrazu o rozdzielczości M × N wyznaczyć\nsumę jasności wszystkich M · N pikseli. Pamiętając o wspomnianej wcześniej kwestii rozbieżności w repre-\nzentacji odcieni, tzn. l(·, ·) = 0 oznacza odcień najciemniejszy, zaś gęstość p(·|·) = 0 — odcień najjaśniejszy\n— obliczymy w istocie sumę różnic postaci:\nL′ =\nM−1\nX\nx=0\nN−1\nX\ny=0\n(255 −l(x, y)).\n3.2\nUczenie SOMN\nUczenie SOMN jest procesem iteracyjnym. W chwili inicjalizacji sieć składa się z eK węzłów, gdzie eK ⩾K.\nJeżeli wartość K jest znana a priori, przyjmujemy zwykle eK = K. W przeciwnym razie szacujemy ją z góry,\naby uniknąć błędu wynikającego ze zbyt niewielkiej liczby składników mieszanki, co w oczywisty sposób\nobniżałoby dokładność przybliżenia rozkładu.\nW każdym z T ∈N kroków uczenia sieci losujemy próbkę x(t) zgodnie z przybliżanym rozkładem p(x),\nw konsekwencji czego nieznacznie modyﬁkujemy jej stan. Aktualną postać mieszanki oznaczmy jako:\nˆp(x|ˆΘ) =\ne\nK\nX\ni=1\nˆpi(x|ˆθi) ˆPi,\n(7)\ngdzie ˆΘ = (ˆθ1, . . . , ˆθ e\nK).\n4\n(a)\n(b)\n(c)\nRysunek 3: Odwzorowanie jasności obrazu: (a) obraz wyjściowy, (b) i (c) przykłady niepoprawnego odwzo-\nrowania.\nNiech t = 1, . . . , T będzie numerem bieżącej iteracji. Wektorowi x(t) przyporządkowujemy węzeł, dla\nktórego wartość prawdopodobieństwa warunkowego (a posteriori) obliczanego według formuły:\nˆP(i|x) =\nˆPiˆpi(x|ˆθi)\nˆp(x|ˆΘ)\n(8)\njest największa. Węzeł taki zwyczajowo nazywamy zwycięzcą (ang. winner). Na Rysunku 2 przedstawiony\njest jako największe niewypełnione koło oznaczone literą w. Pozostałe niewypełnione koła symbolizują węzły\nznajdujące się w bezpośrednim sąsiedztwie zwycięzcy.\n3.3\nAlgorytm SOMN\n• Dane: Liczba eK ∈N, skończony zbiór wektorów losowych x ∈Ω⊆Rd zgodny z rozkładem prawdopo-\ndobieństwa p(x), odległość grafowa d(i, j), ciąg nierosnący (at)∞\nt=1 zbieżny do zera oraz skończony ciąg\nniemalejący (δt)T\nt=1 o tej własności, że δT = 0.\n• Wynik: Graf węzłów sieci (z parametrami ˆPi, ˆθi, gdzie ˆθi = { ˆmi, ˆΣi} dla i = 1, . . . , eK) reprezentatywny\ndla p(x).\nZainicjuj dowolną metodą wszystkie parametry ˆPi, ˆθi, a następnie dla t = 1, . . . , T powtarzaj:\n1. Wylosuj wektor x ∈Ωzgodnie z rozkładem p(x).\n2. Znajdź indeks i ∈{1, . . ., eK} węzła, dla którego wartość:\nˆP(i|x) =\nˆPiˆpi(x|ˆθi)\nˆp(x|ˆΘ)\njest największa.\n3. Dla wszystkich węzłów j ∈{1, . . . , eK} takich, że d(i, j) ⩽δ(t) przypisz:\nˆmj\n←\nˆmj + a(t) ˆP(j|x) [x −ˆmj] ,\nˆΣj\n←\nˆΣj + a(t) ˆP (j|x)\nn\n[x −ˆmj] [x −ˆmj]T −ˆΣj\no\n,\nˆPj\n←\nˆPj + a(t)\nh\nˆP(j|x) −ˆPj\ni\n.\n5\n4. Dla wszystkich węzłów j = 1, . . . , eK wykonaj normalizację:\nˆPj(t) ←\nˆPj(t)\nP e\nK\nk=1 ˆPk(t)\n.\n4\nPrzykłady\nTesty zostały wykonane przy wykorzystaniu programu (stanowiącego integralną cześć pracy [2]) implementu-\njącego opisany wariant algorytmu SOMN. Program ten dostarcza możliwość skonﬁgurowania następujących\nparametrów uczenia sieci: Suwak Learn pozwala wybrać wartość z przedziału 0, 01 ÷ 1, 00 z dokładnością\n0, 01. Wyrazy ciągu współczynników sterujących tempem uczenia dla wektorów średnich ˆmi oraz macierzy\nkowariancji ˆΣi określone są wzorem: a(t) = Cooling factor(t)·Learn. Dla zniwelowania efektu zbyt gwałtow-\nnych wahań wag węzłów SOMN w procesie uczenia, wprowadzono dodatkowy parametr Weight z przedziału\n0, 00001 ÷ 0, 00100 określany z dokładnością 0, 00001. Ciąg współczynników determinujących tempo uczenia\nwag węzłów wyrażony jest więc wzorem: α(t) = a(t) · Weight.\nW pierwszej kolejności posłużymy się fotograﬁą gmachu węgierskiego parlamentu w Budapeszcie:\nRysunek 4: Obraz wejściowy w skali szarości (640 × 480 pikseli).\nZdjęcie przesycone jest detalami architektonicznymi, dlatego jego poprawne odwzorowanie wymaga pre-\ncyzyjnego dobrania parametrów. Rysunek 5(a) pokazuje, że 100 tysięcy iteracji to zdecydowanie zbyt mało,\nby oddać szczegóły budowli. Obraz jest mocno rozmyty i nieczytelny. Z kolei Rysunek 5(b) przedstawia wy-\nnik działania SOMN po pięciu milionach iteracji z parametrami: Learn = 0, 15 oraz Weight = 0, 00005,\nktóre w przypadku tego obrazu okazały się być zbyt wysokie. Zważywszy, że uzyskanie wyniku w niniejszym\nprzypadku pochłonęło nieco ponad 7 godzin, widzimy, że właściwe dobranie parametrów staje się procesem\nczasochłonnym.\n6\n(a)\n(b)\nRysunek 5: Sieć rozmiaru 100 × 100 węzłów: (a) po 100 tys. iteracji, (b) po 5 mln. iteracji ze zbyt wysokimi\nparametrami uczenia.\nTabela 1 przedstawia zestawienie najlepszych uzyskanych wyników. Rezultat dla sieci 200 × 200 wygląda\nsatysfakcjonująco, jednak należy zwrócić uwagę, że 5 milionów iteracji algorytmu pochłonęło znacznie ponad\njedną dobę.\n7\nRozmiar sieci\nLiczba iteracji\n100 × 100\n200 × 200\n1 000 000\n1 godz. 22 min.\n6 godz. 29 min.\n5 000 000\n7 godz. 16 min.\n31 godz. 21 min.\nTabela 1: Zestawienie rezultatów i czasów wykonywania algorytmu SOMN dla podanych rozmiarów sieci oraz\nliczby iteracji na przykładzie fotograﬁi gmachu węgierskiego parlamentu.\nZupełnie inny charakter cechuje kolejną fotograﬁę przedstawiającą paryską bazylikę Sacré-Cœur (Rysunek\n6). Zdjęcie zostało wykonane nocą, zatem odwzorowanie ciemnego nieba skontrastowanego z jasną elewacją\nkościoła stanowi kompletnie odmienne zadanie dla SOMN w stosunku do poprzedniego przykładu.\nTrudność w odwzorowaniu dużych pól jednakowego odcienia skutkuje występowaniem artefaktów, wi-\ndocznych na poniższym rysunku:\nArtefakty szczególnie widoczne są w przypadku sieci o rozmiarach 100×100 węzłów, mniej zaś dla rozmiaru\n200 × 200 (por. Tabela 2).\n8\nRysunek 6: Obraz wejściowy w skali szarości (460 × 480 pikseli).\nRysunek 7: Artefakty widoczne na dużych polach tego samego odcienia.\nJako ostatni przykład weźmiemy pod uwagę fotograﬁę pomnika Mikołaja Kopernika w Toruniu (Rysunek\n8). Podobnie jak w poprzednich przypadkach, najlepsze odwzorowanie detali uzyskujemy po pięciu milionach\niteracji algorytmu dla sieci o rozmiarze 200 × 200 węzłów. Odwzorowanie detali rysów twarzy, jak również\nliści drzew na drugim planie jest bliskie oryginałowi. Brak rozległych pól jednakowego odcienia powoduje,\nże na uzyskanym obrazie nie odnajdujemy niepożądanych artefaktów. Zestawienie uzyskanych rezultatów\nprzedstawiono w Tabeli 3.\n5\nKonkluzje\nNiniejsza publikacja oparta jest w znacznej mierze na pracy magisterskiej autora [2] napisanej pod kierunkiem\nprof. Tomasza Schreibera na Uniwersytecie Mikołaja Kopernika w Toruniu.\nJako dane wejściowe dla procesu klasteryzacji z użyciem SOMN wykorzystane zostały obrazy cyfrowe\nw skali szarości. Warto zwrócić uwagę, że reprezentacja obrazu przy pomocy sieci neuronów jest bardzo\nużyteczna dla potrzeb analizowania wzorców (ang. pattern matching), eksploracji danych (ang. data mining)\nczy kompresji. Przedstawione przykłady pokazują, że odpowiednio nauczone SOMN mogą być z powodzeniem\n9\nRozmiar sieci\nLiczba iteracji\n100 × 100\n200 × 200\n1 000 000\n1 godz. 4 min.\n5 godz. 57 min.\n5 000 000\n6 godz. 32 min.\n28 godz. 11 min.\nTabela 2: Zestawienie rezultatów i czasów wykonywania algorytmu SOMN dla podanych rozmiarów sieci oraz\nliczby iteracji na przykładzie fotograﬁi bazyliki Sacré-Cœur.\nRysunek 8: Obraz wejściowy w skali szarości (640 × 480 pikseli).\n10\nRozmiar sieci\nLiczba iteracji\n100 × 100\n200 × 200\n1 000 000\n1 godz. 31 min.\n6 godz. 27 min.\n5 000 000\n7 godz. 8 min.\n31 godz. 55 min.\nTabela 3: Zestawienie rezultatów i czasów wykonywania algorytmu SOMN dla podanych rozmiarów sieci oraz\nliczby iteracji na przykładzie fotograﬁi pomnika Mikołaja Kopernika.\nwykorzystane do reprezentacji obrazów cyfrowych w skali szarości.\nLiteratura\n[1] A. C. C. Coolen, R. Kühn, and P. Sollich. Theory of Neural Information Processing Systems. Oxford\nUniversity Press, 2005.\n[2] P. Filipiak. Samoorganizujące się sieci mieszankowe w reprezentacji obrazów cyfrowych. Master’s thesis,\nUniwersytet Mikołaja Kopernika w Toruniu, 2010.\n[3] T. Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9):1464–1478, 1990.\n[4] T. Kohonen. Self-Organizing Maps. Springer, 2001.\n[5] H. Yin and N. M. Allinson. Bayesian learning for self-organizing map. Electronic Letters, 33(4):304–305,\n1997.\n[6] H. Yin and N. M. Allinson. Self-organizing mixture networks for probability density estimation. IEEE\nTransactions on Neural Networks, 12(2):405–411, 2001.\n11\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2011-08-18",
  "updated": "2011-08-18"
}