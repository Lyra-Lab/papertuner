{
  "id": "http://arxiv.org/abs/1108.3848v1",
  "title": "Language understanding as a step towards human level intelligence - automatizing the construction of the initial dictionary from example sentences",
  "authors": [
    "Chitta Baral",
    "Juraj Dzifcak"
  ],
  "abstract": "For a system to understand natural language, it needs to be able to take\nnatural language text and answer questions given in natural language with\nrespect to that text; it also needs to be able to follow instructions given in\nnatural language. To achieve this, a system must be able to process natural\nlanguage and be able to capture the knowledge within that text. Thus it needs\nto be able to translate natural language text into a formal language. We\ndiscuss our approach to do this, where the translation is achieved by composing\nthe meaning of words in a sentence. Our initial approach uses an inverse lambda\nmethod that we developed (and other methods) to learn meaning of words from\nmeaning of sentences and an initial lexicon. We then present an improved method\nwhere the initial lexicon is also learned by analyzing the training sentence\nand meaning pairs. We evaluate our methods and compare them with other existing\nmethods on a corpora of database querying and robot command and control.",
  "text": "Language understanding as a step towards human level intelligence - automatizing\nthe construction of the initial dictionary from example sentences\nChitta Baral\nSchool of Computing, Informatics and DSE\nArizona State University\nchitta@asu.edu\nJuraj Dzifcak\nSchool of Computing, Informatics and DSE\nArizona State University\njuraj.dzifcak@asu.edu\nAbstract\nFor a system to understand natural language, it needs to be\nable to take natural language text and answer questions given\nin natural language with respect to that text; it also needs to\nbe able to follow instructions given in natural language. To\nachieve this, a system must be able to process natural lan-\nguage and be able to capture the knowledge within that text.\nThus it needs to be able to translate natural language text\ninto a formal language. We discuss our approach to do this,\nwhere the translation is achieved by composing the mean-\ning of words in a sentence. Our initial approach uses an in-\nverse lambda method that we developed (and other methods)\nto learn meaning of words from meaning of sentences and an\ninitial lexicon. We then present an improved method where\nthe initial lexicon is also learned by analyzing the training\nsentence and meaning pairs. We evaluate our methods and\ncompare them with other existing methods on a corpora of\ndatabase querying and robot command and control.\nIntroduction and Motivation\nWe consider natural language understanding as an important\naspect of human level intelligence. But what do we mean by\n“language understanding”. In our view a system that under-\nstands language can among other attributes (i) take natural\nlanguage text and then answer questions given in natural lan-\nguage with respect to that text and (ii) take natural language\ninstructions and execute those instructions as a human would\ndo.\nA system that can do the above must have several func-\ntional capabilities, such as: (a) It must be able to process lan-\nguage; (b) It must be able to capture knowledge expressed\nin the text; (c) It must be able to reason, plan and in gen-\neral do problem solving and for that it may need to do efﬁ-\ncient searching of solutions; (d) It must be able to do high\nlevel execution and control as per given directives and (e)\nTo scale, it must be able to learn new language aspects (for\ne.g., new words). These functional capabilities are often\ncompartmentalized to different AI research topics. How-\never, good progress in each of these areas (over the last few\ndecades) provides an opportunity to use results and systems\nfrom them and build up on that to develop a natural language\nunderstanding system.\nCopyright c⃝2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nOver the last two decades our group has been focusing\nin the research of developing suitable knowledge represen-\ntation languages. The research by a broader community has\nled to KR languages and systems that allow us to represent\nvarious kinds of knowledge and the KR systems allow us to\nreason, plan and do declarative problem solving using them.\nVarious search techniques are embedded in some of these\nsystems and one such system from Potsdam (CLASP)1 has\nbeen doing very well in SAT competitions2. Similarly, var-\nious languages and systems have been developed that can\ntake directives in a formal language and use it in high level\nexecution and control. These cover the aspects (c) and (d)\nmentioned above.\nIn our current research we use the existing results on (c)\nand (d) and develop an overall architecture that addresses\nthe aspects (a), (b) and (e) to lead to a natural language un-\nderstanding framework.\nThe ﬁrst key aspect of our approach and our language un-\nderstanding framework is to translate natural language to\nappropriate formal languages.\nOnce that is achieved we\nachieve (b) and then together with the (c) and (d) compo-\nnents we achieve (a).\nThe second key aspect of our ap-\nproach and our language understanding framework is that\nwe can reason and learn about how to translate new words\nand phrases. This allows our overall system to scale up to\nlarger vocabularies and thus we achieve (e).\nIn this paper we ﬁrst give a brief presentation of our sys-\ntem and framework which was reported in an earlier limited\naudience conference/workshop. We then present some orig-\ninal work to enhance what was done then.\nTranslating English to Formal languages\nOur approach to translate English to formal languages is in-\nspired by Montague’s path-breaking thesis (Montague 1974)\nof viewing English as a formal language. We consider each\nword to be characterized by one or more λ-calculus for-\nmulas and the translation to be obtained by composing ap-\npropriate λ-calculus formulas of the words as dictated by a\nPCCG (Probabilistic Combinatorial Categorial Grammars).\nThe big challenge in this approach is to be able to come up\nwith the right λ-calculus formulas for various words. Our\n1http://www.cs.uni-potsdam.de/clasp/\n2http://www.satcompetition.org/\narXiv:1108.3848v1  [cs.CL]  18 Aug 2011\napproach, initially presented in (Baral et al. 2011), utilizes\ninverse λ-calculus operators and generalization to obtain se-\nmantic representations of words and learning techniques to\ndistinguish in between them. The system architecture of our\napproach is given in ﬁgure 1. The left block shows an overall\nsystem to translate a sentence into a target formal language\nusing the PCCG grammar and the lexicon, while the right\nblock shows the learning module to learn the meaning of\nnew words (via Inverse λ and generalization methods) and\nassigning weights to multiple meaning of words. We now\nelaborate on some important parts of the system.\nInverse λ computation\nThe composition semantics of λ-calculus basically com-\nputes the meaning of a phrase “a b” by α(β) or β(α) depend-\ning on the CCG parse. Now suppose we know the meaning\n“a b” to be γ and also know the meaning of “a” as α. By\ninverse λ, we refer to the obtaining of β given α and γ. De-\npending on whether γ is α(β) or β(α) we have two inverse\noperators: InverseR and InverseL. We now give a quick\nglimpse of InverseR as given in (Baral et al. 2011). Further\ndetails are given in (Gonzalez 2010).\n• Let\nG,\nH\nrepresent\ntyped\nλ-calculus\nformulas,\nJ1,J2,...,Jn represent typed terms, v1 to vn, v and\nw represent variables and σ1,...,σn represent typed\natomic terms.\n• Let f() represent a typed atomic formula. Atomic formu-\nlas may have a different arity than the one speciﬁed and\nstill satisfy the conditions of the algorithm if they contain\nthe necessary typed atomic terms.\n• Typed terms that are sub terms of a typed term J are de-\nnoted as Ji.\n• If the formulas we are processing within the algorithm\ndo not satisfy any of the if conditions then the algorithm\nreturns null.\nDeﬁnition 1 Consider two lists of typed λ-elements A and\nB, (ai, ..., an) and (bj, ..., bn) respectively and a formula H.\nThe result of the operation H(A : B) is obtained by replac-\ning ai by bi, for each appearance of A in H.\nDeﬁnition 2 The function InverseR(H, G), is deﬁned as:\nGiven G and H:\n1. If G is λv.v@J, set F = InverseL(H, J)\n2. If J is a sub term of H and G is λv.H(J : v) then F = J\n3. G is not λv.v@J, J is a sub term of H and G is\nλw.H(J(J1, ..., Jm) : w@Jp, ..., @Jq) with 1 ≤p,q,s ≤\nm then F = λv1, ..., vs.J(J1, ..., Jm : vp, ..., vq).\nTo illustrate InverseR assume that in the example given\nin table 2 the semantics of the word “in” is not known.\nWe can use the Inverse operators to obtain it as follows.\nUsing the semantic representation of the whole sentence,\nanswer(river(loc2(stateid(′arkansas′)))),\nand\nthe\nsemantics of the word “Name”, λx.answer(x), we can\nuse the respective operators to obtain the semantics of “the\nrivers in Arkansas” as river(loc2(stateid(′arkansas′))).\nRepeating\nthis\nprocess\nrecursively\nwe\nobtain\nλy.y@loc2(stateid(′arkansas′))\nas\nthe\nrepresentation\nof “in Arkansas” and λx.λy.y@loc2(x) as the desired\nmeaning of “in”. 3\nGeneralization and trivial solution\nUsing INV ERSE L and INV ERSE R, we are able to\nobtain new semantic representations of particular words in\nthe training sentences. To go beyond that, we use a no-\ntion of generalization that we developed. For example, con-\nsider the non-transitive verb “ﬂy” who category as per a\nCCG4 (Steedman 2000) is S\\NP.\nLets assume we ob-\ntain a new semantic expression for “ﬂy” as λx.fly(x) using\nINV ERSE L and INV ERSE R. Generalization looks\nup all the words of the same syntactic category, S\\NP. It\nthen identiﬁes the part of the semantic expression in which\n“ﬂy” is involved. In our particular case, it’s the subexpres-\nsion fly. We can then assign the expression λx.w(x) to the\nwords w of the same category. For example, for the verb\n“swim”, we could add λx.swim(x) to the dictionary. This\nprocess can be performed “en masse”, by going through the\ndictionary and expanding the entries of as many words as\npossible or “on demand”, by looking up the words of the\nsame categories when a semantic representation of a word in\na sentence is required. Even with generalization, we might\nstill be missing large amounts of semantics information to be\nable to use INV ERSEL and INV ERSER. To make up\nfor this, we allow trivial solutions, where words or phrases\nare assigned the meaning λx.x, λx.λy.(y@x) or similarly\nsimple representations, which basically mean that this word\nmay be ignored. The trivial solutions are used as a last resort\napproach if neither inverse nor generalization are sufﬁcient.\nTranslation and the Overall Learning Algorithm\nEarlier we mentioned that a sentence is translated to a rep-\nresentation in a formal language by composing the meaning\nof the words in that sentences as dictated by a CCG. How-\never, in presence of multiple meaning of words probabilistic\nCCG is used where the probabilities of a particular transla-\ntion is computed using weights associated with each word\nand meaning pair. For a given sentence the translation that\nhas the higher probability is picked. This raises the question\nof how does one obtain the weights. The weights are ob-\ntained using standard parameter estimation approaches with\nthe goal that the weights should be such that they maximize\nthe overall probability of translating each of the sentences\n3A very brief review of the λ representation is as follows. The\nformula λx.answer(x) basically means that x is an input and\nwhen that input is given then it replaces x in the rest of the for-\nmula. This application of a given input in expressed via the symbol\n@. Thus λx.answer(x)@a reduces to answer(a).\n4In a combinatorial categorial grammar (CCG) words are asso-\nciated with categories. The meaning of the category S\\NP is that\nif a word of category NP comes from the left then by combining\nit with a word of category S\\NP we get a phrase of category S.\nFor example, if the word “a” has a category S\\NP and the word\n“b” has category NP then the two words can be combined to the\nphrase “b a” which will have the category S. Similarly, the cate-\ngory S/NP means that a word of category NP has to come from\nthe right for us to be able to combine.\nFigure 1: Overall system architecture\nin the training set (of sentences and their desired meaning)\nto their desired meaning. We now present our overall learn-\ning algorithm that combines inverse λ, generalization and\nparameter estimation.\n•\nInput: A set of training sentences with their corresponding desired representations S\n=\n{(Si, Li) : i = 1...n} where Si are sentences and Li are desired expressions. Weights\nare given an initial value of 0.1. An initial feature vector Θ0.\n•\nOutput: An updated lexicon LT +1. An updated feature vector ΘT +1.\n•\nAlgorithm:\n–\nSet L0 = INIT IAL DICT IONARY (S)\n–\nFor t = 1 . . . T\n–\nStep 1: (Lexical generation)\n–\nFor i = 1...n.\n∗\nFor j = 1...n.\n∗\nParse sentence Sj to obtain Tj\n∗\nTraverse Tj\n·\napply INV ERSE L, INV ERSE R and GENERALIZED to ﬁnd new\nλ-calculus expressions of words and phrases α.\n∗\nSet Lt+1 = Lt ∪α\n–\nStep 2: (Parameter Estimation)\n–\nSet Θt+1 = UP DAT E(Θt, Lt+1)5\n•\nreturn GENERALIZE(LT , LT ), Θ(T )\nAutomatic generation of initial dictionary\nIn tables 6 and 7 we compare the performance of our sys-\ntems INVERSE, INVERSE+, and INVERSE+(i) with other\nsystems that have similar goals. However, although other\nsystems had other issues, we were not happy that our sys-\ntems required a manually created initial dictionary consist-\ning of λ-calculus representations of a set of words. In the\nrest of the paper we present an approach to overcome that by\nautomatically coming up with candidates for the initial dic-\ntionary and letting the parameter estimation module ﬁgure\nout the correct meaning. In particular we present methods\nto automatically come up with possible λ-calculus represen-\ntation of nouns and various other words that are part of the\ninitial vocabulary in (Baral et al. 2011). Unlike (Baral et\nal. 2011), where each of the word in the initial vocabulary is\ngiven a unique λ-calculus representation, our approach does\nnot necessarily come up with a single λ-calculus representa-\ntion of the words that are in the initial vocabulary in (Baral\n5For details on Θ computation, please see (Zettlemoyer and\nCollins 2005)\net al. 2011) but sometimes may come up with multiple pos-\nsibilities.\nWe will now illustrate our approach in obtaining the initial\ndictionary and the use of CCG and λ-calculus in obtaining\nsemantic representations of sentences on the Geoquery cor-\npus at http://www.cs.utexas.edu/users/ml/geo.html. Table 1\nshows several examples of sentences with their desired rep-\nresentations while table 2 shows a sample CCG parse with\nit’s corresponding semantic derivation.\nTo be able to automatically create the entries in the ini-\ntial dictionary as given by (Baral et al. 2011), we need to\nanswer the following two questions. How do we ﬁnd the ex-\npression λx.answer(x) and how do we assign it to the word\n“Name”?. The word “answer” isn’t given anywhere by the\nsentence. Similarly, How do we know that the semantic ex-\npression for “Arkansas” should be stateid(′arkansas′)?.\nThe ﬁrst question can be answered by looking at several\npossible semantic representations as given in table 1. They\nshare one common aspect, which is that they all contain\nthe predicate answer as the outermost expression. Thus,\nwe can assume that λx.answer(x) should be part of any\nderivation as given by table 2. In general, using the grammar\nderivations for the meaning representations, we can compare\nvarious representations and look for common parts, which\nwe will refer to as common structures. We identify these\ncommon parts and assign them to certain relevant words\nin the sentence, such as assigning the common expression\nλx.answer(x) to the word “Name”. To answer the sec-\nond question, we again look at the grammar derivations for\nnouns, and analyze them to be able to obtain the semantic\nexpression for “Arkansas” as stateid(′arkansas′).\nTable 2 shows an example syntactic and semantic deriva-\ntion for the sentence “Name the rivers in Arkansas.”. The\nsyntactic categories for each are given by the upper part\nof the table.\nThese are then combined using combi-\nnatorial rules (Steedman 2000) to obtain the rest of the\nsyntactic categories.\nFor example, the word “Arkansas”\nof category N is combined with the word “in” of cate-\ngory (NP\\N)/N, to obtain the syntactic category of “in\nArkansas”, NP\\N. The lower portion of the table lists the\nSentence\nRepresentation\nName the rivers in Arkansas.\nanswer(river(loc2(stateid(′arkansas′))))\nHow many people are there in New York?\nanswer(population1(stateid(′newyork′)))\nHow high is Mount McKinley?\nanswer(elevation1(placeid(′mountmckinley′)))\nName all the lakes of US.\nanswer(lake(loc2(countryid(′usa′))))\nName the states which have no surrounding states.\nanswer(exclude(state(all), nextto2(state(all))))\nTable 1: Example translations.\nName\nthe\nrivers\nin\nArkansas.\nS/NP\nNP/NP\nN\n(NP \\N)/N\nN\nS/NP\nNP/NP\nN\nNP \\N\nS/NP\nNP/NP\nNP\nS/NP\nNP\nS\nName\nthe\nrivers\nin\nArkansas.\nλx.answer(x)\nλx.x\nλx.river(x)\nλx.λy.y@loc2(x)\nstateid(′arkansas′)\nλx.answer(x)\nλx.x\nλx.river(x)\nλy.y@loc2(stateid(′arkansas′))\nλx.answer(x)\nλx.x\nriver(loc2(stateid(′arkansas′)))\nλx.answer(x)\nriver(loc2(stateid(′arkansas′)))\nanswer(river(loc2(stateid(′arkansas′))))\nTable 2: CCG and λ-calculus derivation for “Name the rivers in Arkansas.”\nsemantic representations of each words using λ-calculus.\nThese are combined by applying the formulas one to an-\nother, following the syntactic parse tree.\nFor example,\nthe semantics of “Arkansas”, stateid(′arkansas′), is ap-\nplied onto the semantics of “in”, λx.λy.y@loc2(x), yielding\nλy.y@loc2(stateid(′arkansas′)).\nLet us ﬁrst discuss the common structures of a logical\nform. For example, for the Geoquery corpus, as shown in\ntable 1, many queries are of the form answer(X) where X\nis a structure corresponding to the actual query. Similarly, by\nanalyzing the Robocup corpus, we realize that all the queries\nare of the form ((A) (doB)) , (definer C (B)) or (definec\nC (B)), where C is an identiﬁer and A and B are some other\nconstructs in the given language. The main attribute of these\nexpressions is that they deﬁne the structure(s) of the desired\nmeaning representation.\nThe second component of the dictionaries were the se-\nmantic representations of nouns. Unlike the common struc-\ntures, these need to be generated for as many nouns as possi-\nble to ensure that the system is capable to learn the missing\nsemantic representations. For example, in GeoQuery, a noun\n“Arkansas” is represented as stateid(′arkansas′).\n6 For\nRobocup, a compound noun “player 5” can be represented\nas (player our {5}).\nThus our task in being able to automatically obtain these\nis two fold. We ﬁrst need to identify the common structures\nand ﬁnd the appropriate λ-calculus formulas and, pick the\nwords to which we will assign them. The second part of our\ngoal is to ﬁnd the corresponding λ-calculus expressions for\nnouns and compound nouns.\nWe will assume this process is done on the training data\nand full syntactic parse of the sentences, as well as the parse\nof the desired formal representation are given.\nCommon structures\nIn order to look for the common\nstructures, we will compare the derivation structures of var-\nious formulas and look for common structures in them. To\n6We are using the funql representation, although the same ap-\nproach is applicable for the prolog one.\nlimit the potential search, and with respect to our previous\nexperience, we will only look for the common parts at top\nparts of the derivation. Also, in order to be more precise and\nkeep the computation within reasonable bounds, instead of\nlooking at the whole grammar for meaning representations,\nwe will look at the derivations of the meaning representa-\ntions of the training data. This is a reasonable assumption,\nas in general the amount of structures in the target language\ncan be assumed to be less than the amount of training data\nas in the case of Geoquery and CLANG.\nDeﬁnition 3 Given a context free grammar G with an initial\nsymbol S, a set of non-terminals N, a set of terminals T, a\nset of production rules P and a string w = x1, ..., xn, where\nxis are terminal or non-terminal symbols, a production d is\na transformation x1, ..., xn ⇒x1, ..., xi−1, A, xi+1, ..., xn\nsuch that xi →A is in P. We will say that xi →A corre-\nsponds to d.\nGiven a sequence of productions d∗= d1, ..., dn a deriva-\ntion tree t corresponding to d∗is given as:\n- If n = 1, let X →X1, X2, ..., Xn be the rule corre-\nsponding to d1. Then t is a tree with X as the root node,\nwhich has n children, in order, left to right, X1, X2, ..., Xn.\n- If t′ is a derivation tree corresponding to d1, ..., dn−1\nand X →X1, X2, ..., Xn is the rule corresponding to dn,\nthen t is given as t′ with n children added, in order, left to\nright, X1, X2, ..., Xn, to the left most leaf X of t′.\nA λ tree is a pair (V, t), where V is a list of λ bound\nvariables and t is a tree, where each interior node of t is\na non terminal symbol from N and each leaf node of t is a\nterminal symbol from T or a variable from V .\nGiven two sequences of productions d1 and d2 with their\ncorresponding derivation trees t1 and t2, a λ tree (V, tc) is a\ncommon template of t1 and t2 iff there exists two sequences\nof applications s1 = X1, ..., Xn and s2 = Y1, ..., Yn such\nthat when we apply each Xi to each vi, i = 1, ..., n, in tc we\nobtain a subtree of t1 and when we apply each Yi to each vi,\ni = 1, ..., n, in tc we obtain a subtree of t2.\nS\nS\nCIT Y\n↙\n↓\n↘\n↙\n↓\n↘\n↙\n↓\n↘\nanswer(\nRIV ER\n)\nanswer(\nP LACE\n)\ncity(\nCIT Y\n)\n↙\n↓\n↘\n↙\n↓\n↘\n↙\n↓\n↘\nriver(\nRIV ER\n)\nlake(\nP LACE\n)\nloc2(\nST AT E\n)\n↙\n↓\n↘\n↙\n↓\n↘\n↙\n↓\n↘\nloc2(\nST AT E\n)\nloc2(\nCOUNT RY\n)\nstateid(\nST AT ENAME\n)\n↓\n′virginia′\nTable 3: Sample derivation trees\nExample derivation trees and a common template are\ngiven in tables 3 and 4.\nS\nλv.\n↙\n↓\n↘\nanswer(\nv\n)\nTable 4: Sample common template.\nThus, based on the above deﬁnitions, to look for\nthe common structures in the desired meaning repre-\nsentations,\nwe will look for common trees between\nderivations\nwhich\nare\nrooted\nat\nthe\ninitial\nsymbol.\nAs an example,\nconsider the following parts of the\nderivation,\nobtained directly from the Geoquery cor-\npus, for answer(river(loc2(stateid(′arkansas′)))) and\nanswer(lake(loc2(countryid(′usa′)))).\n•\nS →(1a) answer(RIV ER)\n•\n→(2a) answer(river(RIV ER))\n•\n→(3a) answer(river(loc2(ST AT E)))\n•\nS →(1b) answer(P LACE)\n•\n→(2b) answer(lake(P LACE))\n•\n→(3b) answer(lake(loc2(COUNT RY )))\nStarting from the initial non-terminal S, we can see that\nthe rules (1a) and (1b) are already different. They share a\ncommon part in having the terminal symbols answer( and\n). Thus, if we replace all the non-terminals in the common\nparts of the derivation with λ bound variables, we obtain the\ncommon part of the derivations as λv.answer(v), where v\nis the new λ bound variable.\nIn general, having a derivation, we start at the initial sym-\nbol and follow the derivation tree level by level while com-\nparing the nodes in the derivation tree. We then collect all\nthe common terminals from this subtree, and replace all the\ndifferent non-terminals with λ bound variables. Note that\nthere might be multiple such structures, as in the case of\nRobocup corpus. In that case we would store and use all of\nthem and the learning part of the system would take care of\npicking the proper ones.\nAfter ﬁnding the common structures between the deriva-\ntions, we need to ﬁnd the words to which we assign them\nto. Since the structures are supposed to deﬁne the common\nstructures of the desired representations, it is reasonable to\ntry to assign them to words which, in a sense, “deﬁne” the\nsentences. In our case, we look for words that are usually\nlast to combine in the CCG derivation. The reasoning is that\nwhen looking for the common structures, we looked at the\ntop parts of the derivation of meaning representations. Thus\nit is reasonable to try to assign them to words which are in\nthe top parts of the derivation in the syntactic parse of the\nsentence. Note that these words might not be the ones with\nmost complex categories. In practice, such words are usu-\nally verbs, wh-words or some adverbs.\nDeﬁnition 4 Given a CCG parse tree T of a sentence s and\na word w from s, a word w is a top word if there is no other\nword w′ from s, such that level(w′) < level(w).\nGiven a set of training pairs (Si, Li), i = 1, ..., k, where\nSi is a sentence and Li is the corresponding desired logical\nform, together with a syntactic parse of Si and the derivation\nof Li, we can obtain the candidate common structures using\nthe following algorithm, denoted as INITIALC.\n•\nInput:\nA set of training sentences with their corresponding desired representations S = {(Si, Li) :\ni = 1...n} where Si are sentences and Li are desired expressions. A CCG grammar G for\nsentences Si. A CFG grammar G’ for representations Li.\n•\nOutput:\nAn initial lexicon L0.\n•\nAlgorithm:\n–\nStep 1: (Word selection)\n–\nFor i = 1...n.\n∗\nParse Si using the CCG grammar G to obtain parse tree ti. Find all the top words of ti\nand store them in Wi.\n–\nStep 2: (λ-expression generation)\n–\nFor i = 1...n.\n∗\nFor j = 1...n.\n∗\nParse derivations Li and Lj using the CFG grammar G’ to obtain the derivation trees\nTi and Tj.\n∗\nStarting from roots, compare Ti and Tj and ﬁnd the largest common template (V, T ),\nsuch that T that is rooted at the initial symbol of the grammar, S.\n∗\nConcatenate all the leafs of T together to form a λ-expression γ. For each v ∈V , add\nλv. in front of γ.\n∗\nAdd γ as semantic expression to each of the words in Wi and Wj.7\n–\nSet L0 = ∪iWi\n–\nreturn L0\nNouns\nIn order to derive potential λ-expression candi-\ndates for nouns, instead of looking at the top of the deriva-\ntion trees and ﬁnding words, we match the nouns with the\nterminals in the leafs of the derivation tree and then look\n7This step exhaustively assigns the new semantics to all the top\nwords. While not optimal, the learning part of the overall algorithm\ntakes care of ﬁguring out the proper assignment.\nfor non-terminals which can produce it. As we traverse up-\nwards towards the root, we look for other terminals which\nare produced by the non-terminals we encounter. At each\nencountered non-terminal, we generate potential candidate\nλ-expressions by analyzing the current subtree and store\nthem. As in the previous case, we leave it to the parame-\nter learning part of the overall algorithm to ﬁgure out the\nproper ones. Our approach can be illustrated as follows.\nLet\nus\nlook\nat\nan\nexample\nof\nrules\nderiving\n(city(loc2(stateid(′virginia′))))\nfrom\nthe\nsentence\n“Give me the cities in Virginia.”, also given by table 3.\n•\nCIT Y →1f city(CIT Y )\n•\nCIT Y →2f loc2(ST AT E)\n•\nST AT E →3f stateid(ST AT ENAME)\n•\nST AT ENAME →4f ′virginia′\nLet us assume that the noun we are interested in is “Vir-\nginia”. First, we will attempt to match it to a terminal in\nthe derivation, which in this case is ′virginia′. We will\nthen traverse the tree upwards. In this case, we ﬁrst reach\nthe non-terminal STATENAME. Since ′virginia′ is the\nonly child, we add ′virginia′ as the potential candidate rep-\nresentation of “Virginia”. Continuing recursively, we arrive\nat the non-terminal STATE. It has additional terminal sym-\nbols as children, stateid( and ). We try to match these with\nthe sentence and after being unsuccessful, we concatenate\non the leafs of the current subtree to generate another poten-\ntial candidate, which yields stateid(′virginia′). Continu-\ning to traverse we arrive at the non-terminal CITY in the\nrule (2f). As in the previous case, it has terminal symbols\nloc2( and ) as children, and we are unable to match them\nonto the sentence. Thus we again concatenate at the leaves,\nleading to loc2(stateid(′virginia′)) as a potential represen-\ntation candidate for the word “Virginia”. Continuing up-\nwards in the tree, we reach the non-terminal symbol CITY\ngiven by the rule (1f). In this case, we can match one of it’s\nchildren, the terminal city(, with some words in the sentence\nand we stop. This approach produces three possible repre-\nsentations for “Virginia”, ′virginia′, stateid(′virginia′),\nloc2(stateid(′virginia′)).\nHowever, during the training\nprocess the ﬁrst one does not yield any new semantic data\nusing the inverse lambda operators, while the third one is too\nspeciﬁc and can only be used in very few sentences. Con-\nsequently, their weights are very low and they are not used,\nleaving stateid(′virginia′) as the relevant representation.\nWe will now deﬁne an algorithm to obtain the candi-\ndate noun expressions from the training set, denoted by\nINITIALN. For our experiments, maxlevel was set to\n2 and accuracy was set to 0.7.\n•\nInput: A set of training sentences with their corresponding desired representations S\n=\n{(Si, Li) : i = 1...n} where Si are sentences and Li are desired expressions. A CCG\ngrammar G for sentences Si. A CFG grammar G’ for representations Li.\nF N(t) - given a CCG parse tree t, returns all the nouns in t nMAT CH(w) - returns a\nset of terminal symbols partially matching the string w with accuracy a. Returns a single non\nterminal if w is a single word. The accuracy for nMAT CH(w) is given by the partial string\nmatching, given as the percentage of similar parts in between the strings MCY K(X) - given\na set of terminal and non terminal symbols, ﬁnds the non-terminal symbol which can yield all of\nthem using a modiﬁed CYK algorithm.\nmaxlevel M - maximum number of levels allowed to traverse in the derivation trees\n•\nOutput: An initial lexicon L′\n0.\n•\nAlgorithm:\n•\nStep 1: (λ-expression generation)\n•\nFor i = 1...n.\n–\nParse Si using the CCG grammar to obtain ti.\n–\nParse Li using the CFG grammar to obtain Ti.\n–\nSet W = F N(ti).\n–\nFor each wj ∈W :\n∗\nSet X = nMAT CH(w)\n∗\nRepeat a maximum of M times\n·\nSet N = MCY K(X).\n·\nSet T to be a subtree of Ti rooted at the N.\n·\nFor each leaf node n of T which is a match of some word w′ of the sentence Si, if\nthe path from n to N contains a non-terminal symbol, replace n with a new λ bound\nvariable v and add λv. to Γ\n·\nConcatenate all the leaf nodes of T to form Γ′.\n·\nSet Γ = Γ. Γ′, where ’.’ represents string concatenation\n·\nAdd (wj, Γ) to L′\n0.\n·\nIf N has two or more non-terminal children, break.\n·\nIf N has a child which terminal symbol can be matched to any word of Si but wj,\nbreak.\n·\nSet N = MCY K(N).\n•\nreturn L′\n0.\nThe algorithm stops when it encounters other terminals\nbecause we are looking for the representations of speciﬁc\nwords. We assume each word is represented as a lambda cal-\nculus formula. Once we encounter a terminal corresponding\nto some other word of the sentence, we assume that word\nhas it’s own representation which we do not want to add to\nthe representation of the current noun we are investigating.\nThe algorithm produces results such as λx, answer(x) for\nthe words list, name, what and stateid(′virginia′) for the\nword V irginia. In case of CLANG corpus, some of the re-\nsults are λx.λy.(x)(do y), λx.λy.definer ′x′ y for each of\nthe words call, let, if.\nCombining the output of both algorithms yields an initial\nlexicon which can be used by the system. Some of the results\nobtained by the algorithms are given in table 5.\nWord\nObtained representations\nlist\nλx, answer(x)\nVirgina\nstateid(′virginia′)\nwhat\nλx, answer(x)\nMississippi\nstateid(′mississippi′), riverid(′mississippi′)\nif\nλx.λy.(x)(do y)\nλx.λy.definer ′x′ y\nlet\nλx.λy.(x)(do y)\nλx.λy.definer ′x′ y\nplayer 5\n(player our {5})\nmidﬁeld\nλx.(x midfield)\nTable 5: Examples of learned initial representa-\ntions.\nEvaluation\nSimilarly to (Zettlemoyer and Collins 2009), we used the\nstandard GEOQUERY and CLANG corpora for evalua-\ntion.The GEOQUERY corpus contained 880 English sen-\ntences with their respective database queries in funql lan-\nguage. The CLANG corpus contained 300 entries specify-\ning rules, conditions and deﬁnitions in CLANG.\nIn all the experiments, we used the C&C parser of (Clark\nand Curran 2007) to obtain syntactic parses for sentences.\nIn case of CLANG, most compound nouns including num-\nbers were pre-processed. We used the standard 10 fold cross\nvalidation and proceeded as follows. A set of training and\ntesting examples was generated from the respective corpus.\nThese were parsed using the C&C parser to obtain the syn-\ntactic tree structure.\nNext, the syntactic parses plus the\ngrammar derivations of the desired representations for the\ntraining data were used to create a corresponding initial dic-\ntionary. These together with the training sets containing the\ntraining sentences with their corresponding semantic repre-\nsentations (SRs) were used to train a new dictionary with\ncorresponding parameters. Note that it is possible that many\nof the words were still missing their SRs, however note that\nour generalization approach was also applied when comput-\ning the meanings of the test data. This dictionary was then\nused to parse the test sentences and the highest scoring parse\nwas used to determine precision and recall.\nSince many\nwords might have been missing their SRs, the system might\nnot have returned a proper complete semantic parse. To mea-\nsure precision and recall, we adopted the measures given by\n(Wong and Mooney 2007) and (Ge and Mooney 2009). Pre-\ncision denotes the percentage of of returned SRs that were\ncorrect, while Recall denotes the percentage of test examples\nwith pre-speciﬁed SRs returned. F-measure is the standard\nharmonic mean of precision and recall. For database query-\ning, a SR was correct if it retrieved the same answer as the\nstandard query. For CLANG, an SR was correct if it was an\nexact match of the desired SR, except for argument ordering\nof conjunctions and other commutative predicates.\nTo evaluate our system, a comparison with the perfor-\nmance results of several alternative systems with available\ndata is given. In many cases, the performance data given\nby (Ge and Mooney 2009) are used.\nWe compared our\nsystem with the following ones: The SYN0, SYN20 and\nGOLDSYN systems by (Ge and Mooney 2009), the system\nSCISSOR by (Ge and Mooney 2005), an SVM based system\nKRIPS by (Kate and Mooney 2006), a synchronous gram-\nmar based system WASP by (Wong and Mooney 2007), the\nCCG based system by (Zettlemoyer and Collins 2007), the\nwork by (Lu et al. 2008) and the INVERSE and INVERSE+\nsystems given by (Baral et al. 2011). The results for differ-\nent copora, if available, are given by the tables 6 and 78. The\nwork by (Percy, Michael, and Dan 2011) reports a 91.1% re-\ncall on geoquery corpus but uses a 600 to 280 split.\nPrecision\nRecall\nF-measure\nA-INVERSE+\n94.58\n90.22\n92.35\nINVERSE+\n93.41\n89.04\n91.17\nINVERSE\n91.12\n85.78\n88.37\nGOLDSYN\n91.94\n88.18\n90.02\nWASP\n91.95\n86.59\n89.19\nZ&C\n91.63\n86.07\n88.76\nSCISSOR\n95.50\n77.20\n85.38\nKRISP\n93.34\n71.70\n81.10\nLu at al.\n89.30\n81.50\n85.20\nTable 6: Performance on GEOQUERY.\nPrecision\nRecall\nF-measure\nA-INVERSE+\n87.05\n79.28\n82.98\nINVERSE+(i)\n87.67\n79.08\n83.15\nINVERSE+\n85.74\n76.63\n80.92\nGOLDSYN\n84.73\n74.00\n79.00\nSYN20\n85.37\n70.00\n76.92\nSYN0\n87.01\n67.00\n75.71\nWASP\n88.85\n61.93\n72.99\nKRISP\n85.20\n61.85\n71.67\nSCISSOR\n89.50\n73.70\n80.80\nLu at al.\n82.50\n67.70\n74.40\nTable 7: Performance on CLANG.\n8 The INV ERSE + (i) and A −INV ERSE + (i) denotes\nevaluation where “(deﬁnec” and “(deﬁner” at the start of SRs were\ntreated as being equal.\nThe results of our experiments indicate that our approach\noutperforms the existing parsers in F-measure and illustrate\nthat our approach scales well and is applicable for sentences\nwith various lengths. In particular, it is even capable of out-\nperforming the manually created initial dictionaries given by\n(Baral et al. 2011). The main reason seems to be that unlike\nin (Wong and Mooney 2007), our approach actually beneﬁts\nfrom a more simpliﬁed nature of funql compared to PRO-\nLOG. The resulting λ-calculus expressions are often sim-\npler, as they do not have to account for variables and mul-\ntiple predicates. The increase in accuracy mainly resulted\nfrom the decrease of number of possible semantic expres-\nsions of words. As we understand the work by (Baral et al.\n2011) would sometimes include many meanings of words.\nOur approach reduces this number. A decrease was caused\nby not being able to automatically generate some expres-\nsions that were manually added in Baral et al 2011. The\nautomatically obtained dictionary contained around 32% of\nthe semantic data of the manually created one.\nMost of the failures of our system can be attributed to the\nlack of data in the training set. In particular, new syntactic\ncategories, or semantic constructs rarely seen in the training\nset usually result in complete inability to parse those sen-\ntences. In addition, given the syntactic parses, a complex\nsemantic representations in lambda calculus are produced,\nwhich are then often propagated via generalization and can\nproduce bad translation and interfere with learning. Addi-\ntionally, many of the words will have several possible rep-\nresentations and the training set distribution might not prop-\nerly represent the desired one. The C&C parser that we used\nwas primarily trained on news paper text, (Clark and Curran\n2007), and thus did have some problems with these differ-\nent domains and in some cases resulted in complex semantic\nrepresentations of words. This could be improved by using\na different parser, or by simply adjusting some of the parse\ntrees.\nIn the previous paragraphs we compared our system with\nsimilar systems in terms of performance. We now give a\nqualitative comparison of our approach with other learning\nbased approaches that can potentially translate natural lan-\nguage text to formal representation languages (Zettlemoyer\nand Collins 2005), (Kate and Mooney 2006), (Wong and\nMooney 2006), (Wong and Mooney 2007), (Lu et al. 2008),\n(Zettlemoyer and Collins 2007), (Ge and Mooney 2009),\n(Kwiatkowski et al.\n2010), (Kwiatkowski et al.\n2011),\n(Percy, Michael, and Dan 2011). (Zettlemoyer and Collins\n2005) uses a set of hand crafted rules to learn syntactic cat-\negories and semantic representations of words using combi-\nnatorial categorial grammar (CCG), (Steedman 2000), and\nλ-calculus formulas, (Gamut 1991).\nThe same approach\nis adopted in (Zettlemoyer and Collins 2007). (Kanazawa\n2001), (Kanazawa 2003) and (Kanazawa 2006) focuses on\ncomputing the missing λ-expressions, but do not provide a\ncomplete system. In (Ge and Mooney 2009), a word align-\nment approach is adopted to obtain the semantic lexicon and\nrules, which allow semantic composition, are learned. Com-\npared to (Ge and Mooney 2009), we do not generate word\nalignments for the sentences and their semantic representa-\ntions. We only use a limited form of pattern matching to\ninitialize our approach with several basic semantic represen-\ntations. We focus on the simplest cases, the top and bottom\nof the trees, rather than performing a complete analysis of\nthe trees. We assign each word a λ-calculus formula as it’s\nsemantics and use the native λ-calculus application, @, to\ncombine them rather than computed composition rules. The\nlearning process then ﬁgures out which of the candidate se-\nmantics to use. We use a different syntactic parser which\ndictates the direction of the semantic composition. Both ap-\nproaches use a similar learning model based on (Zettlemoyer\nand Collins 2005). The work by (Kwiatkowski et al. 2010)\nuses higher-order uniﬁcation. Instead of using inverse, they\nperform a split operation which can break a λ expression\ninto two. However, this approach is not capable of learn-\ning more complex λ calculus formulas and lacks general-\nization. (Percy, Michael, and Dan 2011) uses dependency-\nbased compositional semantics(DCS) with lexical triggers\nwhich loosely correspond to our initial dictionaries.\nConclusion and Discussion\nIn this work we presented an approach to translate natural\nlanguage sentences into semantic representations. Using a\ntraining set of sentences with their desired semantic repre-\nsentations our system is capable of learning the meaning\nrepresentations of words. It uses the parse of desired se-\nmantic representations under an unambiguous grammar to\nobtain an initial dictionary, inverse λ operators and gener-\nalization techniques to automatically compute the semantic\nrepresentations based on the syntactic structure of the syn-\ntactic parse tree and known semantic representations without\nany human supervision. Statistical learning approaches are\nused to distinguish the various potential semantic represen-\ntations of words and prefer the most promising one. In this\nwork, we are able to overcome some of the deﬁciencies of\nour initial work in (Baral et al. 2011). Our approach here is\nfully automatic and it generates a set of potential candidate\nwords for each noun based solely on the context free gram-\nmar of the target language and the training data. The result-\ning method is capable of outperforming many of the existing\nsystems on the standard copora of Geoquery and CLANG.\nThere are many possible extensions to our work. One of\nthe possible direction is to experiment with additional cor-\npora which uses temporal logic as a target language. Other\ndirections include the improvements in inverse lambda com-\nputation and application of other learning methods such as\nsparse learning.\nReferences\n[Baral et al. 2011] Baral, C.; Gonzalez, M.; Dzifcak, J.; and\nZhou, J. 2011. Using inverse λ and generalization to trans-\nlate english to formal languages. In Proceedings of the Inter-\nnational Conference on Computational Semantics, Oxford,\nEngland, January 2011.\n[Clark and Curran 2007] Clark, S., and Curran, J. R. 2007.\nWide-coverage efﬁcient statistical parsing with ccg and log-\nlinear models. Computational Linguistics 33.\n[Gamut 1991] Gamut, L. 1991. Logic, Language, and Mean-\ning. The University of Chicago Press.\n[Ge and Mooney 2005] Ge, R., and Mooney, R. J. 2005. A\nstatistical semantic parser that integrates syntax and seman-\ntics. In Proceedings of CoNLL., 9–16.\n[Ge and Mooney 2009] Ge, R., and Mooney, R. J.\n2009.\nLearning a compositional semantic parser using an existing\nsyntactic parser. In Proceedings of ACL-IJCNLP., 611–619.\n[Gonzalez 2010] Gonzalez, M. A. 2010. An inverse lambda\ncalculus algorithm for natural language processing. Master’s\nthesis, Arizona State University.\n[Kanazawa 2001] Kanazawa, M. 2001. Learning word-to-\nmeaning mappings in logical semantics. In Proceedings of\nthe Thirteenth Amsterdam Colloquium, 126–131.\n[Kanazawa 2003] Kanazawa, M.\n2003.\nComputing word\nmeanings by interpolation. In Proceedings of the Fourteenth\nAmsterdam Colloquium, 157–162.\n[Kanazawa 2006] Kanazawa, M.\n2006.\nComputing inter-\npolants in implicational logics.\nAnn. Pure Appl. Logic\n142(1-3):125–201.\n[Kate and Mooney 2006] Kate, R. J., and Mooney, R. J.\n2006. Using string-kernels for learning semantic parsers.\nIn Proceedings of COLING., 439–446.\n[Kwiatkowski et al. 2010] Kwiatkowski, T.; Zettlemoyer, L.;\nGoldwater, S.; and Steedman, M. 2010. Inducing proba-\nbilistic ccg grammars from logical form with higher-order\nuniﬁcation. In In Proceedings of EMNLP.\n[Kwiatkowski et al. 2011] Kwiatkowski, T.; Zettlemoyer, L.;\nGoldwater, S.; and Steedman, M. 2011. Lexical generaliza-\ntion in ccg grammar induction for semantic parsing. In In\nProceedings of EMNLP.\n[Lu et al. 2008] Lu, W.; Ng, H. T.; Lee, W. S.; and Zettle-\nmoyer, L. S. 2008. A generative model for parsing natu-\nral language to meaning representations. In Proceedings of\nEMNLP-08.\n[Montague 1974] Montague, R.\n1974.\nFormal Philoso-\nphy. Selected Papers of Richard Montague. Yale University\nPress.\n[Percy, Michael, and Dan 2011] Percy, L.; Michael, J.; and\nDan, K. 2011. Learning dependency-based compositional\nsemantics. In Proceedings of ACL-HLT, 590–599.\n[Steedman 2000] Steedman, M. 2000. The syntactic process.\nMIT Press.\n[Wong and Mooney 2006] Wong, Y. W., and Mooney, R. J.\n2006. Learning for semantic parsing with statistical machine\ntranslation. In Proceedings of HLT/NAACL., 439–446.\n[Wong and Mooney 2007] Wong, Y. W., and Mooney, R. J.\n2007. Learning synchronous grammars for semantic parsing\nwith lambda calculus. In Proceedings of ACL., 960–967.\n[Zettlemoyer and Collins 2005] Zettlemoyer,\nL.,\nand\nCollins, M.\n2005.\nLearning to map sentences to logical\nform: Structured classiﬁcation with probabilistic categorial\ngrammars. In AAAI, 658–666.\n[Zettlemoyer and Collins 2007] Zettlemoyer,\nL.,\nand\nCollins, M. 2007. Online learning of relaxed ccg gram-\nmars for parsing to logical form.\nIn Proceedings of\nEMNLP-CoNLL, 678–687.\n[Zettlemoyer and Collins 2009] Zettlemoyer,\nL.,\nand\nCollins, M. 2009. Learning context-dependent mappings\nfrom sentences to logical form. In ACL.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2011-08-18",
  "updated": "2011-08-18"
}