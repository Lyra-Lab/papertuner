{
  "id": "http://arxiv.org/abs/1112.3714v1",
  "title": "Nonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction",
  "authors": [
    "Youngmin Cho",
    "Lawrence K. Saul"
  ],
  "abstract": "We show how to incorporate information from labeled examples into nonnegative\nmatrix factorization (NMF), a popular unsupervised learning algorithm for\ndimensionality reduction. In addition to mapping the data into a space of lower\ndimensionality, our approach aims to preserve the nonnegative components of the\ndata that are important for classification. We identify these components from\nthe support vectors of large-margin classifiers and derive iterative updates to\npreserve them in a semi-supervised version of NMF. These updates have a simple\nmultiplicative form like their unsupervised counterparts; they are also\nguaranteed at each iteration to decrease their loss function---a weighted sum\nof I-divergences that captures the trade-off between unsupervised and\nsupervised learning. We evaluate these updates for dimensionality reduction\nwhen they are used as a precursor to linear classification. In this role, we\nfind that they yield much better performance than their unsupervised\ncounterparts. We also find one unexpected benefit of the low dimensional\nrepresentations discovered by our approach: often they yield more accurate\nclassifiers than both ordinary and transductive SVMs trained in the original\ninput space.",
  "text": "Machine Learning manuscript No.\n(will be inserted by the editor)\nNonnegative Matrix Factorization for Semi-supervised\nDimensionality Reduction\nYoungmin Cho · Lawrence K. Saul\nReceived: date / Accepted: date\nAbstract We show how to incorporate information from labeled examples into non-\nnegative matrix factorization (NMF), a popular unsupervised learning algorithm for\ndimensionality reduction. In addition to mapping the data into a space of lower di-\nmensionality, our approach aims to preserve the nonnegative components of the data\nthat are important for classiﬁcation. We identify these components from the sup-\nport vectors of large-margin classiﬁers and derive iterative updates to preserve them\nin a semi-supervised version of NMF. These updates have a simple multiplicative\nform like their unsupervised counterparts; they are also guaranteed at each iteration\nto decrease their loss function—a weighted sum of I-divergences that captures the\ntrade-off between unsupervised and supervised learning. We evaluate these updates\nfor dimensionality reduction when they are used as a precursor to linear classiﬁcation.\nIn this role, we ﬁnd that they yield much better performance than their unsupervised\ncounterparts. We also ﬁnd one unexpected beneﬁt of the low dimensional representa-\ntions discovered by our approach: often they yield more accurate classiﬁers than both\nordinary and transductive SVMs trained in the original input space.\nKeywords Nonnegative matrix factorization · Semi-supervised learning\n1 Introduction\nIn many applications of machine learning, high dimensional data must be mapped\ninto a lower dimensional space where it becomes easier to store, manipulate, and\nYoungmin Cho\nDepartment of Computer Science and Engineering, University of California, San Diego, La Jolla, CA,\nUSA\nE-mail: yoc002@cs.ucsd.edu\nLawrence K. Saul\nDepartment of Computer Science and Engineering, University of California, San Diego, La Jolla, CA,\nUSA\nE-mail: saul@cs.ucsd.edu\n2\nYoungmin Cho, Lawrence K. Saul\nmodel. Unsupervised algorithms for dimensionality reduction have the ability to an-\nalyze unlabeled examples—a potentially compelling advantage when the number of\nunlabeled examples exceeds the number of labeled ones. However, unsupervised al-\ngorithms also have a corresponding weakness: they do not always preserve the struc-\nture in the data that is important for classiﬁcation.\nThe above issues highlight the important role of semi-supervised learning algo-\nrithms for dimensionality reduction. Such algorithms are designed to analyze data\nfrom a mix of labeled and unlabeled examples. From the labeled examples, they\naim to preserve those components of the input space that are needed to distinguish\ndifferent classes of data. From the unlabeled examples, they aim to prune those com-\nponents of the input space that do not account for much variability. The question, in\nboth theory and in practice, is how to balance these competing criteria (Chapelle et al.\n2006; Zhu and Goldberg 2009).\nIn this paper, we propose a semi-supervised framework for nonnegative matrix\nfactorization (NMF), one of the most popular learning algorithms for dimensional-\nity reduction (Lee and Seung 1999). Widely used for unsupervised learning of text,\nimages, and audio, NMF is especially well-suited to the high-dimensional feature\nvectors that arise from word-document counts and bag-of-feature representations.\nWe study the setting where NMF is used as a precursor to large-margin clas-\nsiﬁcation by linear support vector machines (SVMs) (Cortes and Vapnik 1995). In\nthis setting, we show how to modify the original updates of NMF to incorporate in-\nformation from labeled examples. Our approach can be used for problems in binary,\nmultiway, and multilabel classiﬁcation. Experiments on all these problems reveal sig-\nniﬁcant (and sometimes unexpected) beneﬁts from semi-supervised dimensionality\nreduction.\nThe organization of this paper is as follows. In section 2, we introduce our model\nfor semi-supervised NMF and compare it to previous related work (Lee et al. 2010;\nLiu and Wu 2010). In section 3, we present our experimental results, evaluating a\nnumber of competing approaches for semi-supervised learning and dimensionality\nreduction. Finally in section 4, we review our most signiﬁcant ﬁndings and conclude\nby discussing directions for future work.\n2 Model\nNonnegative matrix factorization (NMF) is a method for discovering low dimensional\nrepresentations of nonnegative data (Lee and Seung 1999). Let X denote the d × n\nmatrix formed by adjoining n nonnegative column vectors (inputs) of dimensional-\nity d. NMF seeks to discover a low-rank approximation\nX ≈VH,\n(1)\nwhere V and H are respectively d×r and r×n matrices, and typically r \u0004 min(d, n).\nThe columns of V can be viewed as basis vectors; the columns of H, as reconstruc-\ntion coefﬁcients. Section 2.1 reviews the original formulation of NMF, while sec-\ntion 2.2 describes our extension for semi-supervised learning. Finally, section 2.3\nreviews related work.\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n3\n2.1 NMF for unsupervised learning\nThe error of the approximation in eq. (1) can be measured in different ways. Here we\nconsider the I-divergence (Csiszar 1975):\nD(X, VH) =\n\u0001\nij\n\u0002\nXij log\nXij\n(V H)ij\n−Xij + (V H)ij\n\u0003\n.\n(2)\nThe right hand side of eq. (2) is generally positive, vanishing only if the low-rank\napproximation VH perfectly reconstructs the matrix X. Other penalties (e.g., sum of\nsquared error) can also be used to derive NMF algorithms (Lee and Seung 2001), but\nwe do not consider them here. The error in eq. (2) can be minimized by iterating the\nmultiplicative updates:\nVik ←Vik\n\u0004\u0005\nj HkjXij/(V H)ij\n\u0005\nj Hkj\n\u0006\n,\n(3)\nHkj ←Hkj\n\u0002\u0005\ni VikXij/(V H)ij\n\u0005\ni Vik\n\u0003\n.\n(4)\nSimple to implement, these updates are guaranteed to decrease the approximation\nerror at each iteration. They also provably converge to a stationary point—typically,\na local minimum—of eq. (2).\nUnsupervised NMF can be viewed as mapping the high-dimensional column vec-\ntors of X into the low-dimensional column vectors of H. Often this mapping has\ninteresting properties, yielding sparse distributed representations of the inputs (Lee\nand Seung 1999). However, this mapping can also lead to a severe distortion of in-\nner products and distances in the low dimensional space. The distortion arises due to\nthe non-orthogonality of the basis vectors in V. In particular, even an exact low-rank\nfactorization X = VH does not imply that X\u0001X = H\u0001H. We can modify the re-\nsults of NMF in a simple way to obtain a more geometrically faithful mapping. In\nparticular, let\nZ = (V\u0001V)1/2H.\n(5)\nThe matrix Z is the same size as H, and its column vectors provide a similarly low\ndimensional representation of the inputs X (though not one that is constrained to\nbe nonnegative). Unlike the matrix H, however, we observe that Z\u0001Z ≈X\u0001X if\nX ≈VH; thus this mapping preserves inner products in addition to reconstructing\nthe data matrix by a low-rank approximation. This property of Z provides a useful\nsanity check when NMF is performed as a preprocessing step for classiﬁcation by\nlinear SVMs (as we consider in the next section). In particular, it ensures that for\nsufﬁciently large r, SVMs trained on the low-dimensional columns of Z obtain es-\nsentially the same results as SVMs trained on the full d-dimensional columns of X.\n4\nYoungmin Cho, Lawrence K. Saul\nBinary classiﬁcation\nw\nw\n+\nw\n-\nw\n(a)\n(b)\nMultiway classiﬁcation\n(c)\n(d)\nFig. 1 Left: (a) large-margin hyperplane, with normal vector w, separating positively and negatively la-\nbeled examples in the nonnegative orthant; (b) decomposing w = w+ −w−, where the nonnegative\ncomponents w+ and w−are derived respectively from support vectors with positive and negative labels.\nRight: (c) important directions in the data for multiway classiﬁcation, as estimated from the hyperplane\ndecision boundaries between classes; (d) nonnegative components derived from the support vectors in\neach classiﬁer.\n2.2 NMF for semi-supervised learning\nThe updates in eqs. (3–4) are guaranteed to decrease the reconstruction error in\neq. (2), but they are not guaranteed to preserve directions in the data that are im-\nportant for classiﬁcation. This weakness is not peculiar to NMF: it is the bane of all\npurely unsupervised algorithms for dimensionality reduction.\n2.2.1 Binary classiﬁcation\nWe begin by addressing this weakness in the simple case of binary classiﬁcation.\nLet xi denote the ith column of the matrix X, and suppose (without loss of general-\nity) that the ﬁrst m ≤n inputs have binary labels yi ∈{−1, +1}. For nonnegative\ndata, all these inputs will lie in the same orthant. Fig. 1 (left) shows a hyperplane\nseparating positively and negatively labeled examples in this orthant. The normal\nvector w to this hyperplane identiﬁes an important direction for classiﬁcation. How\ncan we modify NMF so that it preserves this component of the data? This problem\nis complicated by the fact that the weight vector w may not itself lie inside the non-\nnegative orthant. In particular, as shown in the ﬁgure, it may have both positive and\nnegative projections onto the orthogonal coordinate axes of the input space. This di-\nrection cannot therefore be preserved by any trivial modiﬁcation of NMF.\nWe can make progress by appealing to a dual representation of the weight vec-\ntor w. In such a representation, the vector w is expressed as a weighted sum of labeled\nexamples,\nw =\nm\n\u0001\ni=1\nαiyixi,\n(6)\nwith nonnegative coefﬁcients αi ≥0. Such a representation arises naturally in large-\nmargin classiﬁers, where the nonzero coefﬁcients identify support vectors, and in\nperceptrons, where they count the mistakes made by an online classiﬁer. The dual\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n5\nrepresentation in eq. (6) suggests a simple decomposition of the weight vector w into\nnonnegative components. In particular, we deﬁne:\nw+ =\n\u0001\nyi=+1\nαixi,\nw−=\n\u0001\nyi=−1\nαixi,\n(7)\nwhere the sums in eq. (7) range over positively and negatively labeled examples,\nrespectively. Note that both w+ and w−lie in the same orthant as the inputs xi. In\nterms of these components, the weight vector is given by the difference:\nw = w+ −w−.\n(8)\nFig. 1 (b) illustrates this decomposition for a large-margin hyperplane, where the\nnon-zero coefﬁcients αi > 0 identify support vectors.\nThough the weight vector w ∈\rd may not lie in the same orthant as the inputs,\nwe can modify NMF to preserve the individual components w± ∈\rd\n+ (which are\nnecessarily nonnegative). For ease of notation, we deﬁne the nonnegative vectors\nα+ ∈\rn\n+ and α−∈\rn\n+ with elements:\n[α±]i =\n\u0007 αi max(0, ±yi) if i ≤m,\n0\nif i > m.\n(9)\nIn terms of these elements, we can rewrite the nonnegative components in eq. (7) as:\nw+ = Xα+,\nw−= Xα−.\n(10)\nHow well are these components preserved by the dimensionality reduction X ≈VH\nin eq. (1) of NMF? Analogous to eq. (10), let ˆw+ and ˆw−indicate the reconstructed\ncomponents:\nˆw+ = VHα+,\nˆw−= VHα−.\n(11)\nThe nonnegative components of the weight vector w will be preserved if both w+ ≈ˆw+\nand w−≈ˆw−. On the other hand, if these approximations do not hold—if these\ncomponents are not preserved—then the dimensionality reduction from NMF is likely\nto shrink the margin of separation between different classes.\nIntuitively, we can view the components w± as additional inputs that must be ac-\ncurately modeled when NMF is used as a preprocessing step for linear classiﬁcation.\nEq. (10) shows how these additional inputs are constructed from labeled examples\nin the original input space. Of course, the same labeled examples may not emerge\nas support vectors when a linear SVM is retrained on the low dimensional outputs\nof NMF. Indeed, the smaller the number of labeled examples, the weaker the hints\nthey provide for semi-supervised dimensionality reduction. The crucial question in\nthis regime is how to best exploit the few labeled examples that are available. In\nour case, the answer to this question must also take into account that the resulting\nlow dimensional representations will be used as features for linear classiﬁcation. Our\napproach—to preserve the direction perpendicular to the large margin hyperplane—is\ninspired by the statistical learning theory of SVMs. We view the large margin hyper-\nplane as a particularly robust “hint” that can be extracted from a small number of\nlabeled examples in the high dimensional input space.\n6\nYoungmin Cho, Lawrence K. Saul\nWe can bias NMF to preserve the components w± by adding extra terms to its\nloss function. Speciﬁcally, for problems in binary classiﬁcation, we imagine that a\nweight vector w has been found by training a perceptron or SVM on m ≤n labeled\nexamples. Then, from the dual representation of w in eq. (6), we compute the non-\nnegative components w± in eq. (7). Finally, to derive a semi-supervised version of\nNMF, we seek a factorization X = VH that minimizes the loss function\nL = D(X,VH) + λ [D(w+, ˆw+) + D(w−, ˆw−)] ,\n(12)\nwhere the reconstructed components ˆw± are deﬁned from eq. (11). The ﬁrst term\nin eq. (12) is simply the usual I-divergence for unsupervised NMF, while the other\nterms penalize large divergences between the nonnegative components w± and their\nreconstructions ˆw±. The parameter λ > 0 determines the balance between unsuper-\nvised and supervised learning.\nWith a slight change in notation, we can rewrite the loss function in eq. (12) in a\nmore symmetric form. In particular, let\nS =\n\b\nα+ α−\n\t\n(13)\ndenote the n×2 matrix obtained by adjoining the nonnegative vectors α+ and α−of\nsupport vector coefﬁcients. (The last n−m rows of this matrix are necessarily zero,\nsince unlabeled examples cannot appear as support vectors.) In terms of the matrix S,\nwe can rewrite eq. (12) as:\nL = D(X, VH) + λD(XS, VHS).\n(14)\nThe goal of semi-supervised NMF is to minimize the right hand side of eq. (14) with\nrespect to the matrices V and H. Note that we do not update the coefﬁcients α± that\nappear in the matrix S; these coefﬁcients are estimated prior to NMF by training a\nlinear classiﬁer on the subset of m labeled examples. Once estimated in this way, the\nmatrix S is assumed to be ﬁxed.\nThe weighted combination of I-divergences in eq. (14) can be minimized by sim-\nple multiplicative updates. These updates take the form:\nVik ←Vik\n\n\n\u0005\nj\nXijHkj\n(V H)ij + λ \u0005\n\f\n(XS)i\u0004(HS)k\u0004\n(V HS)i\u0004\n\u0005\nj Hkj + λ \u0005\n\f(HS)k\f\n\n,\n(15)\nHkj ←Hkj\n\n\n\u0005\ni\nXijVik\n(V H)ij + λ \u0005\ni\f\n(XS)i\u0004VikSj\u0004\n(V HS)i\u0004\n\u0005\ni Vik + λ \u0005\ni\f VikSj\f\n\n.\n(16)\nThe updates require similar matrix computations as eqs. (3–4) and reduce to their\nunsupervised counterparts when λ = 0. In the appendix, we show how to derive\nthese updates from an auxiliary function and prove that they monotonically decrease\nthe loss function in eq. (14). We call this model as NMF-α due to the important role\nplayed by the dual coefﬁcients in eq. (6).\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n7\n2.2.2 Multiway and multilabel classiﬁcation\nThe framework in the previous section extends in a straightforward way to preserve\nmultiple directions that are important for classiﬁcation. The need to preserve multiple\ndirections arises in problems of multiway and multilabel classiﬁcation. For multiway\nclassiﬁcation, important directions in the data can be estimated by training c(c−1)/2\nlinear classiﬁers to distinguish all pairs of c classes. Then, just as in the binary case,\nthe normal to each hyperplane decision boundary between classes can be decom-\nposed into a difference of nonnegative components w±. Fig. 1 (d) illustrates the six\nnonnegative components that arise in this way from a problem with c=3 classes. For\nmultilabel classiﬁcation, we can train as many linear classiﬁers as there are labels.\nThen, from each of these classiﬁers, we can extract nonnegative components w± to\nbe preserved by NMF.\nAs in section 2.2.1, we imagine that these important directions are estimated\nprior to NMF using whatever labeled examples are available for this purpose. Let\nτ ∈{1, 2, ..., p} index the multiple linear SVMs that are trained using these exam-\nples. As before, from each SVM, using eqs. (6–9), we can extract the support vector\ncoefﬁcients α(τ)\n± . Finally, let\nS =\n\b\nα1\n+ α2\n+ . . . αp\n+ α1\n−α2\n−. . . αp\n−\n\t\n(17)\ndenote the n × 2p matrix that adjoins all the column vectors α(τ)\n± . Deﬁning the ma-\ntrix S in this way, we can perform semi-supervised NMF using the same objective\nand updates as eqs. (14–16).\n2.3 Related work\nSeveral previous authors have modiﬁed NMF to incorporate information from la-\nbeled examples, though not exactly in the same way. Closest to our approach is the\nwork by Lee et al. (2010), who adapt NMF so that the low dimensional representa-\ntions H in eq. (1) are also predictive of the classes of labeled examples. In addition\nto the nonnegative input matrix X, they construct a binary label matrix Y whose ele-\nments Ykl indicate whether or not the lth training example (l ≤m) belongs to the kth\nclass. Then they attempt to model the relationship between Y and H1:m (the ﬁrst m\ncolumns of H) by a linear regression Y ≈UH1:m. Combining the sum of squared\nerrors from this regression and the original NMF, they consider the loss function\nL(V, H, U) = \u0010X −VH\u00102 + λ\u0010Y −UH1:m\u00102,\n(18)\nwhere the parameter λ > 0 balances the trade-off between unsupervised and super-\nvised learning, just as in eq. (12). Lee et al. (2010) also show how to minimize eq. (18)\nusing simple multiplicative updates. The model in eq. (18) has obvious parallels to\nour approach; we therefore evaluate it as an additional baseline in our experiments.\n(See section 3.)\nAnother line of related work is the model for constrained nonnegative matrix fac-\ntorization (CNMF) proposed by Liu and Wu (2010). The main idea in CNMF is to\n8\nYoungmin Cho, Lawrence K. Saul\nrequire the data points xi and xj (i.e., the ith and jth columns of X) to have the same\nlow-dimensional representation hi = hj if they belong to the same class. Specif-\nically, H is ﬁrst separated into two parts: H1:m (labeled) and Hm+1:n (unlabeled).\nThen the reconstruction coefﬁcients for labeled examples are constrained to be gener-\nated by the binary label matrix Y: in other words, CNMF requires that H1:m = QY\nfor some nonnegative matrix Q. The reconstruction coefﬁcients Hm+1:n for unla-\nbeled examples are not constrained except to be generally nonnegative. Both these\nconditions can be expressed by writing\nH = PA\nwhere\nP =\n\u000e\nQ\nHm+1:n\n\u000f\nand A =\n\u0010 Y\n0\n0\nIn−m\n\u0011\n.\n(19)\nHere the lower-right sub-block In−m of the matrix A denotes an identity matrix of\nsize (n−m) × (n−m). By plugging H in eq. (19) into the sum of squared errors of\nthe original NMF, they obtain the loss function:\nL(V, P) = \u0010X −VPA\u0010,\n(20)\nwhich is minimized by simple multiplicative updates for V and P. Note that the loss\nfunction in eq. (20) does not involve a tuning parameter, such as the parameter λ that\nappears in eqs. (12) and (18). We also evaluate the model in eq. (20) as an additional\nbaseline in our experiments.\nIn addition to these two baselines, we brieﬂy mention other related approaches.\nWang et al. (2004) and Heiler and Schn¨orr (2006) modiﬁed NMF based on ideas from\nlinear discriminant analysis, adapting the reconstruction coefﬁcients in H to optimize\nintra-class and inter-class variances. Chen et al. (2007) proposed a nonnegative tri-\nfactorization of Gram matrices for clustering with side information. Wang et al. (2009)\nproposed a semi-supervised version of NMF based on ideas from graph embedding,\nrepresenting the similarity between examples (both labeled and unlabeled) by the\nedges of a weighted graph. In comparison to the work by Lee et al. (2010) and\nLiu and Wu (2010), we found these other methods to be either less competitive or\nconsiderably more difﬁcult to implement, involving many tuning parameters and/or\nmore expensive optimizations. Thus in this paper we do not include their results.\nNote that we are interested in semi-supervised NMF for extremely large data sets\nof sparse, high dimensional inputs (e.g., word-document counts). We do not expect\nsuch inputs to be densely sampled from a low dimensional manifold, a common as-\nsumption behind many graph-based semi-supervised algorithms. Our own approach\nexploits the dual representation of linear SVMs and perceptrons because these algo-\nrithms scale very well to massively large data sets. These considerations have also\ninformed our survey of previous work.\n3 Experiments\nWe experimented with our approach on three publicly available data sets. For each\ndata set, we separated the examples into training, validation, and test sets, as shown in\nTable 1. We evaluated the beneﬁts of semi-supervised learning by varying the number\nof training examples whose labels were available for dimensionality reduction and\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n9\nTable 1 Composition of data sets for binary, multiway, and multilabel classiﬁcation.\nMNIST\nNews\nAviation\nExamples (training)\n5000\n9007\n12912\nExamples (validation)\n6791\n2262\n8607\nExamples (test)\n1991\n7505\n7077\nInput dimensionality\n784\n53975\n28426\nClasses or labels\n2\n20\n22\nlarge-margin classiﬁcation. For each experiment, we averaged our results over ﬁve\ndifferent splits of training and validation examples. However, we used the same test\nsets across all experiments; this was done to be consistent with previous benchmarks,\nwhich speciﬁcally designated the examples that appeared in the test set.\nWe chose the data sets in Table 1 mainly to experiment on three different types of\ntasks. The ﬁrst task—in binary classiﬁcation—was to distinguish grayscale images\nof 4s versus 9s from the MNIST data set of handwritten digits (LeCun and Cortes\n1998). The second task—in multiway classiﬁcation—was to label postings from the\n20-Newsgroups1 data set. Finally, the third task—in multilabel classiﬁcation—was\nto recognize aviation safety reports2 from 22 non-exclusive categories. For the ﬁrst\ntwo tasks, we measured performance by classiﬁcation accuracy; for the third task,\nin multilabel classiﬁcation, we computed the average of macro-averaged and micro-\naveraged F-measures.\nThe News and Aviation data sets in Table 1 typify the sorts of applications we\nimagine for semi-supervised NMF: these data sets consist of sparse, high dimensional\ninputs, of the sort that naturally arise from word-document counts and bag-of-feature\nrepresentations. The MNIST data set does not have this structure, but as we shall\nsee, it is useful for visualizing the effect of label information on the basis vectors\ndiscovered by NMF.\n3.1 Methodology\nEach of our experiments was designed to measure the performance when dimension-\nality reduction of some kind is followed by large-margin classiﬁcation. We compared\nsix methods for dimensionality reduction: principal component analysis (PCA), lin-\near discriminant analysis (LDA), unsupervised NMF with the I-divergence in eq. (2),\nsemi-supervised NMF (Lee et al. 2010) and constrained NMF (Liu and Wu 2010)\nwith the loss functions (respectively) in eqs. (18) and (20), and our own approach\nNMF-α. All these methods except LDA3 operated on the entire set of n training ex-\namples, but only the semi-supervised approaches made special use of the m ≤n\nlabeled examples. For NMF-α, we began by training one or more linear SVMs on the\nm labeled training examples, as described in section 2.2. The hyperplane decision\n1 http://www.ai.mit.edu/people/jrennie/20Newsgroups/\n2 http://web.eecs.utk.edu/events/tmw07/\n3 Only the m labeled examples were used in LDA.\n10\nYoungmin Cho, Lawrence K. Saul\nλ = 0\nλ = 104\nλ = 105\nFig. 2 Comparison of r = 64 basis vectors for images of handwritten digits (4s and 9s) learned by\nunsupervised and semi-supervised NMF. As λ increases from left to right, the basis vectors evolve from\nprimitive strokes to confusable examples. The image on the left is reconstructed by the highlighted basis\nvectors in each panel.\nboundaries in these SVMs were then fed to the semi-supervised updates in eqs. (15–\n16). Finally, we used the results of PCA, LDA, and NMF to map all the inputs into a\nspace of dimensionality r ≤d. (Note that once the basis vectors from these methods\nare known, no labels are required to compute this mapping on test examples.) For\nNMF, we also performed the corrective mapping described at the end of section 2.1.\nNext we measured the effects of dimensionality reduction on large-margin classi-\nﬁcation. Using only the same m labeled examples in the training set, we evaluated the\nperformance of linear SVMs on the reduced-dimensional test examples from PCA,\nLDA, and NMF. We used the labeled examples in the validation set to tune the margin\npenalty parameter in SVMs as well as the trade-off parameter λ in eqs. (14) and (18).\nFor reference, we also tested linear SVMs and transductive SVMs (Joachims 1999b)\non the raw inputs (i.e., without any dimensionality reduction). The transductive SVMs\nmade use of all n examples in the training set, both labeled and unlabeled. We used\nthe LIBLINEAR package (Fan et al. 2008) to train the baseline SVMs in these exper-\niments and SVMlight (Joachims 1999a) to train the transductive SVMs.\nFor each of the data sets in Table 1, we experimented with different numbers\nof basis vectors r ≤d for dimensionality reduction and different numbers of la-\nbeled training examples m ≤n for semi-supervised learning. The following sections\npresent a representative subset of our results.\n3.2 Qualitative results\nWe begin by illustrating qualitatively how the results from NMF change when the\nalgorithm is modiﬁed to incorporate information from labeled examples. Fig. 2 com-\npares the r = 64 basis vectors of MNIST handwritten digits (4s and 9s) learned for\ndifferent values of λ in eq. (14). The visual differences in these experiments are strik-\ning even though only 2% of the training examples were labeled and the corresponding\nbasis vectors in different panels were initialized in the same way. Note how for λ = 0\n(i.e., purely unsupervised NMF), the basis vectors resemble cursive strokes (Lee and\nSeung 1999), whereas for very large λ, they resemble actual training examples (e.g.,\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n11\nMNIST (4 versus 9)\n16 64\n144\n256\n400\n88\n90\n92\n94\n96\n98\nSVM\nTSVM\n# basis vectors (r)\nAccuracy (%)\n# labeled examples = 100\n16 64\n144\n256\n400\n88\n90\n92\n94\n96\n98\nSVM\nTSVM\n# basis vectors (r)\nAccuracy (%)\n# labeled examples = 500\n16 64\n144\n256\n400\n88\n90\n92\n94\n96\n98\nSVM\n# basis vectors (r)\nAccuracy (%)\n# labeled examples = 5000\nNEWS (20 groups)\n10 50 100\n300\n50\n60\n70\n80\nSVM\nTSVM\n# basis vectors (r)\nAccuracy (%)\n# labeled examples = 1800\n10 50 100\n300\n50\n60\n70\n80\nSVM\nTSVM\n# basis vectors (r)\nAccuracy (%)\n# labeled examples = 3600\n10 50 100\n300\n50\n60\n70\n80\nSVM\n# basis vectors (r)\nAccuracy (%)\n# labeled examples = 9007\nAviation (22 labels)\n10 50 100\n300\n0.4\n0.45\n0.5\n0.55\n0.6\nSVM\nTSVM\n# basis vectors (r)\n(F−macro + F−micro) / 2\n# labeled examples = 646\n10 50 100\n300\n0.4\n0.45\n0.5\n0.55\n0.6\nSVM\nTSVM\n# basis vectors (r)\n(F−macro + F−micro) / 2\n# labeled examples = 1292\n10 50 100\n300\n0.4\n0.45\n0.5\n0.55\n0.6\nSVM\n# basis vectors (r)\n(F−macro + F−micro) / 2\n# labeled examples = 12912\nFig. 3 Effect of dimensionality reduction by NMF-α on the performance of linear SVMs. Also shown\nas baselines are the performance of ordinary and transductive SVMs on the original (raw) inputs. (The\nlatter is indicated by TSVM.) Each sub-ﬁgure plots the classiﬁcation accuracy or F-measure on the test set\nversus the numbers of basis vectors used for dimensionality reduction. The hollow black square indicates\nthe model that performed best on the validation set. Top row: Binary classiﬁcation on the MNIST data set\n(4s vs. 9s) when 2% (left), 10% (middle), and 100% (right) of training examples were labeled. Middle row:\nMultiway classiﬁcation on the 20-Newsgroups data set when 20% (left), 40% (middle), and 100% (right)\nof training examples were labeled. Bottom row: Multilabel classiﬁcation on the Aviation data set when 5%\n(left), 10% (middle), and 100% (right) of training examples were labeled.\nsupport vectors). The middle panel shows the results from the intermediate value of\nλ whose basis vectors yielded the best-performing classiﬁer.\n3.3 Beneﬁts of dimensionality reduction\nNext we examine the interplay between classiﬁcation performance, semi-supervised\nlearning, and dimensionality reduction. To begin, we compare NMF-α to two other\napproaches that do not involve dimensionality reduction. Fig. 3 shows the results\nfrom NMF-α from experiments in binary classiﬁcation, multiway classiﬁcation, and\nmultilabel classiﬁcation. We report the performance in terms of classiﬁcation accu-\nracy or F-measure on the test examples as we vary both the number of labeled training\nexamples (along different plots in the same row) and the number of basis vectors for\ndimensionality reduction (along the horizontal axes within each plot). Also shown\nas baselines are the performance of ordinary and transductive SVMs on the original\n(raw) inputs.\n12\nYoungmin Cho, Lawrence K. Saul\n−10\n−5\n0\n5\n10\n−10\n−5\n0\n5\n10\nNMF\nNMF-α\n−10\n−5\n0\n5\n10\n−10\n−5\n0\n5\n10\nPCA\nNMF-α\nFig. 4 Comparing the effects on linear classiﬁcation of dimensionality reduction by NMF-α versus purely\nunsupervised methods. The plots compare the results from NMF-α in Fig. 3 to analogous results from or-\ndinary NMF (left) and PCA (right); each point represents a pair of experiments on the same data set\n(indicated by color), for the same number of labeled examples (indicated by shape), and with the same\nnumber of basis vectors (not indicated in the plot). The axes delineate the classiﬁcation accuracy (%) or\n100×F-measure relative to the baseline result obtained by linear SVMs in the original input space. Thus\npoints in the upper half-space reveal cases where dimensionality reduction by NMF-α improved classiﬁ-\ncation over the baseline SVMs; points in the right half-space reveal cases where dimensionality reduction\nby NMF or PCA improved classiﬁcation over the baseline SVMs; ﬁnally, points above the diagonal line\nreveal cases where the dimensionality reduction by NMF-α yielded better performance than purely unsu-\npervised methods. For better visualization, we omit some points where NMF-α performs much better than\nunsupervised approaches, but the opposite cases are all shown.\nThe results in Fig. 3 reveal an interesting trend: the representations from NMF-α\noften yield better performance than SVMs (and even transductive SVMs) trained on\nthe raw inputs. In particular, note how in many plots, one or more results from NMF-\nα top the performance of the baseline and transductive SVM in the original input\nspace. This trend4 suggests that the updates for NMF-α are not only preserving dis-\ncriminative directions in the input space, but also pruning away noisy, irrelevant sub-\nspaces that would otherwise compromise the performance of large-margin classiﬁca-\ntion.\n3.4 Beneﬁts of semi-supervised learning\nNext we compare the effects on linear classiﬁcation by different methods of dimen-\nsionality reduction. Fig. 4 compares NMF-α in this role to the purely unsupervised\nmethods of NMF and PCA. These latter methods derive basis vectors from all the\ntraining examples, but they do not exploit the extra information in the m labeled\nexamples. To produce Fig. 4, we collected the results for ordinary NMF and PCA\n4 Similar results were observed on the validation sets, suggesting cross-validation as one way to select\nthe number of basis vectors. The hollow black square in each plot indicates the best classiﬁer on the\nvalidation set.\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n13\nTable 2 Comparing the effects on linear classiﬁcation of dimensionality reduction by NMF-α versus the\npurely supervised method of LDA. See text for details.\nMNIST\n20 Newsgroups\nAviation\nNMF-α\nLDA\nNMF-α\nLDA\nNMF-α\nLDA\nm \u0003 n\n93.3\n88.5\n70.1\n65.3\n48.5\n43.6\nm < n\n96.9\n92.7\n75.3\n69.8\n51.0\n47.9\nm = n\n98.2\n94.2\n78.1\n73.5\n57.2\n56.4\nanalogous to those in Fig. 3 for NMF-α ; we then plotted them against the results for\nNMF-α . At a high level, points above the diagonal line indicate comparable models\n(i.e., those with the same number of basis vectors) where NMF-α outperforms either\nNMF (left) and PCA (right) when these methods are used as a precursor for linear\nclassiﬁcation. (See the caption for more detail.) Note that the vast majority of points\nlie above the diagonal line, often by a signiﬁcant margin. Thus this ﬁgure shows\nclearly the beneﬁts of semi-supervised learning for dimensionality reduction.\nWe also compared NMF-α to another canonical method for dimensionality reduc-\ntion: linear discriminant analysis (LDA). LDA can be regarded as a purely supervised\nmethod that computes informative directions in the data from labeled examples (but\nnot unlabeled ones). LDA is typically used to extract a number of basis vectors equal\nto one less than the number of classes. Table 2 compares the results in linear classi-\nﬁcation by SVMs when LDA and NMF-α were used for dimensionality reduction.\nThe results for NMF-α in this table are from the models that obtained the best aver-\nage performance on the validation set—i.e., those corresponding to the hollow black\nsquares in Fig. 3. Note that LDA is limited by the small number of basis vectors that\nit can extract. Again these results reveal the beneﬁts of semi-supervised learning for\ndimensionality reduction: NMF-α outperforms LDA in every experimental setting.\n3.5 Comparison to other semi-supervised approaches\nFinally we compare the effects on linear classiﬁcation by dimensionality reduction\nby NMF-α to other semi-supervised versions of NMF. Fig. 5 compares NMF-α in\nthis role to the semi-supervised methods described in section 2.3. To produce Fig. 4,\nwe evaluated the methods of Lee et al. (2010) and Liu and Wu (2010) with the same\nnumbers of basis vectors and labeled examples as shown in Fig. 3; we then plotted\nthe results for these methods against the results for NMF-α . One clear trend from\nthese plots is that NMF-α works better in the regime m \u0004 n where there are very\nfew labeled examples. (Note how nearly all the squares lie above the diagonal line.)\nThe plots validate our intuition—based on results in purely supervised learning—that\nmargin-based approaches are better suited to this regime than linear regression. From\nthe plots, we conclude that NMF-α is generally exploiting more robust information in\nthe labeled examples—and this is particularly true when there are very few of them.\n14\nYoungmin Cho, Lawrence K. Saul\n−10\n−5\n0\n5\n10\n−10\n−5\n0\n5\n10\nLee et al. (2010)\nNMF-α\n−10\n−5\n0\n5\n10\n−10\n−5\n0\n5\n10\nLiu & Wu (2010)\nNMF-α\nFig. 5 Comparing the effects on linear classiﬁcation of dimensionality reduction by NMF-α versus other\nsemi-supervised versions of NMF: Lee et al. (2010) (left) and Liu & Wu (2010) (right). The format is the\nsame as Fig. 4.\n4 Discussion\nIn this paper we have investigated a novel form of NMF for semi-supervised di-\nmensionality reduction. The updates for our approach are simple to implement and\nguaranteed to converge. Empirically, we have observed several beneﬁts when these\nupdates are used as a precursor to linear classiﬁcation. First, as shown in Fig. 3, the\nlower dimensional representations discovered by NMF-α often lead to better perfor-\nmance than SVMs and transductive SVMs trained in the original input space; this\nimprovement was observed in both semi-supervised and fully supervised settings.\nSecond, as shown in Fig. 4 and Table 2, NMF-α performs better in this role than\nother canonical methods for dimensionality reduction, such as PCA, ordinary NMF,\nand LDA. Third, as shown in Fig. 5, NMF-α seems especially well suited to the\nregime where the number of unlabeled examples greatly exceeds the number of la-\nbeled ones. In particular, NMF-α appears to perform better in this regime than other\nsemi-supervised versions of NMF, presumably because its margin-based approach is\nmore robust than least-squares methods. All these results suggest a useful role for\nNMF-α in problems of high dimensional classiﬁcation—especially those problems\nwhere examples are plentiful but labels are scarce.\nWhile this paper focuses on semi-supervised NMF, many other authors have\nexplored how to incorporate information from labeled examples in related mod-\nels of matrix factorization. Blei and McAuliffe (2008) proposed a variant of la-\ntent Dirichlet allocation for supervised topic modeling of word-document matrices.\nRish et al. (2008) used shared basis components from exponential family PCA to\nbuild a joint model over inputs and labels, whereas Goldberg et al. (2010) considered\ndirectly completing a joint matrix of inputs and labels while minimizing its rank.\nMairal et al. (2009) showed how to adapt sparse coding for supervised dictionary\nlearning: the dictionary entries in this scheme are optimized for classiﬁcation as well\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n15\nas reconstruction accuracy. Our work has similar goals, but by starting in a differ-\nent place, we are able to capitalize on the many desirable properties (e.g., ease of\nimplementation) of NMF.\nMotivated by our results in this area, as well as those of others, we are investigat-\ning several directions for future work. For very large-scale problems, where training\none or more linear SVMs is not feasible, we are exploring faster online algorithms —\nperceptrons and recent variants thereof (Dredze et al. 2008) — to estimate which di-\nrections in the data should be preserved for classiﬁcation. We are also considering to\njointly optimize the NMF loss functions and the support vector coefﬁcients α for bet-\nter modeling (Gupta and Xiao 2011). We note that the computations in eqs. (15–16)\nare easily parallelized; in future work, we hope to exploit this parallelizable struc-\nture with GPU programming. Others have noted that NMF can be kernelized (Zhang\net al. 2006); we are looking into a kernelized version of our algorithm whose low-\nrank factorizations support more scalable implementations of nonlinear SVMs (Fine\nand Scheinberg 2001). Finally, we are interested in applying the ideas in this paper to\nslightly different settings, such as domain adaptation and transfer learning (Ando and\nZhang 2005; Blitzer et al. 2007), where hybrid schemes for dimensionality reduction\nhave proven extremely useful. These directions and others are left for future work.\nA Derivation of multiplicative updates\nIn this appendix we show how to derive the multiplicative updates in eqs. (15–16).\nWe also sketch the proof that they converge to a stationary point of the loss function\nin eq. (14). The full proof is based on constructing an auxiliary function that provides\nan upper bound on the loss function. Our construction closely mimics the one used by\nLee and Seung (2001) in their study of NMF for unsupervised learning. Accordingly,\nwe omit many details where the two proofs coincide.\nWe begin by bounding the individual I-divergences that appear in eq. (14). From\nJensen’s inequality, we know that for any nonnegative numbers zk:\nlog\n\u0001\nk\nzk ≥\n\u0001\nk\nθk log zk\nθk\n,\n(21)\nwhere θk > 0 and \u0005\nk θk = 1. To bound the I-divergences in eq. (14), we apply this\ninequality to the implied sums in the matrix products VH and VHS. Speciﬁcally,\nwe can write:\nlog(V H)ij ≥\n\u0001\nk\nηijk log VikHkj\nηijk\n,\n(22)\nlog(V HS)i\f ≥\n\u0001\njk\nψi\fjk log VikHkjSj\f\nψi\fjk\n,\n(23)\nwhere ηijk and ψi\fjk are positive numbers (chosen later, to achieve the tightest pos-\nsible bounds) that satisfy \u0005\nk ηijk = \u0005\njk ψi\fjk = 1. Substituting these inequalities\n16\nYoungmin Cho, Lawrence K. Saul\ninto eq. (14), we obtain the upper bound:\nD(X, VH) + λD(XS, VHS) ≤\n\u0001\nijk\nXijηijk log Xijηijk\nVikHkj\n+\n\u0001\nij\n\b\n(V H)ij −Xij\n\t\n+\nλ\n\u0001\ni\fjk\n(XS)i\fψi\fjk log (XS)i\fψi\fjk\nVikHkjSj\f\n+ λ\n\u0001\ni\f\n\b\n(V HS)i\f −(XS)i\f\n\t\n.\n(24)\nThe iterative updates in eqs. (15–16) are derived by minimizing the upper bound in\neq. (24) in lieu of the original loss function. By setting its partial derivatives to zero,\nthe bound can be minimized in closed form with respect to either of the nonnegative\nmatrices V or H. With respect to V, the minimum is given by:\nVik =\n\u0005\nj Xijηijk + λ \u0005\nj\f(XS)i\fψi\fjk\n\u0005\nj Hkj + λ \u0005\n\f(HS)k\f\n.\n(25)\nThe question remains how to set ηijk and ψi\fjk in this expression. In brief, it can be\nshown that by setting them appropriately, we can ensure that the update in eq. (25)\nmonotonically decreases the original loss function in eq. (14). Speciﬁcally, the fol-\nlowing choices — in terms of the current parameter estimates for V and H — lead\nto this property:\nηijk = VikHkj/(V H)ij,\n(26)\nψi\fjk = VikHkjSj\f/(V HS)i\f.\n(27)\nSubstituting these values into eq. (25) yields the update rule for V in eq. (15). The\nupdate rule for H can be derived in an analogous fashion.\nReferences\nAndo, R. K., & Zhang, T. (2005). A framework for learning predictive structures\nfrom multiple tasks and unlabeled data. Journal of Machine Learning Research,\n6, 1817–1853.\nBlei, D., & McAuliffe, J. (2008). Supervised topic models. In Platt, J. C., Koller,\nD., Singer, Y., & Roweis, S. (Eds.), Advances in Neural Information Processing\nSystems 20 (pp. 121–128). Cambridge, MA: MIT Press.\nBlitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, Bollywood, boom-boxes\nand blenders: domain adaptation for sentiment classiﬁcation. In Proceedings of\nthe 45th Annual Meeting of the Association of Computational Linguistics (ACL-\n07) (pp. 440–447). Prague, Czech Republic.\nChapelle, O., Sch¨olkopf, B., & Zien, A. (Eds.). (2006). Semi-Supervised Learning.\nMIT Press.\nChen, Y., Rege, M., Dong, M., & Hua, J. (2007). Incorporating user provided con-\nstraints into document clustering. In Proceedings of the Seventh IEEE Interna-\ntional Conference on Data Mining (ICDM-07) (pp. 103–112). Washington, DC,\nUSA.\nNonnegative Matrix Factorization for Semi-supervised Dimensionality Reduction\n17\nCortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20,\n273–297.\nCsiszar, I. (1975). I-divergence geometry of probability distributions and minimiza-\ntion problems. The Annals of Probability, 3(1), 146–158.\nDredze, M., Crammer, K., & Pereira, F. (2008). Conﬁdence-weighted linear clas-\nsiﬁcation.\nIn Proceedings of the 25th International Conference on Machine\nLearning (ICML-08) (pp. 264–271). Helsinki, Finland.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). LIBLIN-\nEAR: A library for large linear classiﬁcation. Journal of Machine Learning\nResearch, 9, 1871–1874.\nFine, S., & Scheinberg, K. (2001). Efﬁcient SVM training using low-rank kernel\nrepresentations. Journal of Machine Learning Research, 2, 243–264.\nGoldberg, A., Zhu, X., Recht, B., Xu, J., & Nowak, R. (2010). Transduction with\nmatrix completion: Three birds with one stone. In Lafferty, J., Williams, C.\nK. I., Shawe-Taylor, J., Zemel, R., & Culotta, A. (Eds.), Advances in Neural\nInformation Processing Systems 23 (pp. 757–765).\nGupta, M. D., & Xiao, J. (2011).\nNon-negative matrix factorization as a feature\nselection tool for maximum margin classiﬁers. In Proceedings of the Twenty-\nFourth IEEE Conference on Computer Vision and Pattern Recognition (CVPR-\n11) (pp. 2841–2848).\nHeiler, M., & Schn¨orr, C. (2006). Learning sparse representations by non-negative\nmatrix factorization and sequential cone programming.\nJournal of Machine\nLearning Research, 7, 1385–1407.\nJoachims, T. (1999a). Making large-scale SVM learning practical. In B. Sch¨olkopf,\nC. Burges and A. Smola (Eds.), Advances in Kernel Methods - Support Vector\nLearning chapter 11, (pp. 169–184). Cambridge, MA: MIT Press.\nJoachims, T. (1999b). Transductive inference for text classiﬁcation using support vec-\ntor machines. In Proceedings of the 16th International Conference on Machine\nLearning (ICML-99) (pp. 200–209). Bled, Slovenia.\nLeCun, Y., & Cortes, C. (1998). The MNIST database of handwritten digits. http:\n//yann.lecun.com/exdb/mnist/.\nLee, D. D., & Seung, H. S. (1999). Learning the parts of objects by non-negative\nmatrix factorization. Nature, 401(6755), 788–791.\nLee, D. D., & Seung, H. S. (2001). Algorithms for non-negative matrix factoriza-\ntion. In Leen, T. K., Dietterich, T. G., & Tresp, V. (Eds.), Advances in Neural\nInformation Processing Systems 13 (pp. 556–562). Cambridge, MA: MIT Press.\nLee, H., Yoo, J., & Choi, S. (2010). Semi-supervised nonnegative matrix factoriza-\ntion. IEEE Signal Processing Letters, 17(1), 4–7.\nLiu, H., & Wu, Z. (2010). Non-negative Matrix Factorization with Constraints. In\nProceedings of the Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence\n(AAAI-10) (pp. 506–511).\nMairal, J., Bach, F., Ponce, J., Sapiro, G., & Zisserman, A. (2009). Supervised dic-\ntionary learning. In Koller, D., Schuurmans, D., Bengio, Y., & Bottou, L. (Eds.),\nAdvances in Neural Information Processing Systems 21 (pp. 1033–1040).\nRish, I., Grabarnik, G., Cecchi, G., Pereira, F., & Gordon, G. J. (2008). Closed-\nform supervised dimensionality reduction with generalized linear models. In\n18\nYoungmin Cho, Lawrence K. Saul\nProceedings of the 25th International Conference on Machine Learning (ICML-\n08) (pp. 832–839). Helsinki, Finland.\nWang, C., Yan, S., Zhang, L., & Zhang, H. (2009). Non-negative semi-supervised\nlearning. In Proceedings of the 12th International Conference on Artiﬁcial In-\ntelligence and Statistics.\nWang, Y., Jia, Y., Hu, C., & Turk, M. (2004). Fisher non-negative matrix factorization\nfor learning local features. In Asian Conference on Computer Vision (pp. 806–\n811).\nZhang, D., Zhou, Z.-H., & Chen, S. (2006). Non-negative matrix factorization on\nkernels. In PRICAI 2006: Trends in Artiﬁcial Intelligence (pp. 404–412).\nZhu, X., & Goldberg, A. B. (2009). Introduction to Semi-Supervised Learning. Mor-\ngan & Claypool Publishers.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2011-12-16",
  "updated": "2011-12-16"
}