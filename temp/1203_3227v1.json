{
  "id": "http://arxiv.org/abs/1203.3227v1",
  "title": "Generalisation of language and knowledge models for corpus analysis",
  "authors": [
    "Anton Loss"
  ],
  "abstract": "This paper takes new look on language and knowledge modelling for corpus\nlinguistics. Using ideas of Chaitin, a line of argument is made against\nlanguage/knowledge separation in Natural Language Processing. A simplistic\nmodel, that generalises approaches to language and knowledge, is proposed. One\nof hypothetical consequences of this model is Strong AI.",
  "text": "Generalisation of language and \nknowledge models for corpus analysis \nAnton Loss (avl@gmx.co.uk) \nABSTRACT. This paper takes new look on language and knowledge modelling \nfor corpus linguistics. Using ideas of Chaitin, a line of argument is made against \nlanguage/knowledge separation in Natural Language Processing. A simplistic \nmodel, that generalises approaches to language and knowledge, is proposed. \nOne of hypothetical consequences of this model is Strong AI. \nIntroduction \nAs Plungyan(2009) noticed, corpus linguistics made it possible to treat language impartially. \nCorpus, which is a large collection of human written texts, gives opportunity to depart from \nsubjective imposing of what is right and wrong in both knowledge and language. When \nsomething is illogical, or syntactically incorrect, but it is found in corpus – then it is not \ndiscarded. This paper shows that when treating language in such impartial manner, then there \nis no major difference between syntax and knowledge models. \nReasons for syntax separation today \nComplete separation of syntax from analysis of meaning or knowledge was done in the middle \nof 20 century (Plungyan, 2009). Possibly strongest reason for separation was subconscious \nfeeling that many grammatical forms can be associated with the same meaning. It is impossible \nthough to definitely prove that, for instance, “This is Mary‟s pony” and “This pony belongs to \nMary” mean exactly the same. \nToday language syntax is modelled separate from knowledge and logic. Two popular \napproaches will be used here as examples of each type of model. Analysis of language (syntax) \nwill be represented by Chomsky‟s generative grammar. Analysis of knowledge will be \nrepresented by a programming language Prolog. It will be shown that both methods are \nindividual cases of more general approach. A brief description of how Prolog and Generative \nGrammar are viewed in this paper follows. \nProlog \nProlog is computer implementation of formal logic. It can be reduced to the following: \nSeveral true statements, such as “Mary likes ponies” or “Mary is a girl”, are given. These \nstatements are put into Prolog syntax: \nLIKES (MARY , PONY) \nGIRL(MARY) \nLogical statements can also be given. It is possible to define logic such as “any girl likes ponies”. \n \nLIKES(X, PONY) :- GIRL(X) \nAfter finite amount of statements is given to Prolog it is possible to obtain a list of all valid \nstatements, which could be derived from them. Following statements are given: \nGIRL(LINDA) \nGIRL(MARY) \nLIKES(X, PONY) :- GIRL(X) \nProlog will produce following: \nGIRL(LINDA) \nGIRL(MARY) \nLIKES (MARY , PONY) \nLIKES(LINDA, PONY) \nGenerative Grammar \n(For simplicity only context free grammar is considered here.) \nGenerative grammar can be essentially reduced to generating stings by unconditional \nsubstitution. The starting symbol(S) and rules of substitution are defined, and from them \nstrings are generated. For instance rules can have following form: \nS → SS, \nS → A, \nS → B, \nThis will produce strings as: \n S \n SS \n SSS \n SSA \n SBA \n… \nIn this particular case strings of all possible combinations of “S”, “a” and “b” are produced. \nChaitin argument against language/knowledge separation \nKolmogorov-Chaitin complexity of a string is defined relative to the length of a shortest \nprogram that can produce such a string (Chaitin 1987).  \nIn his later works Chaitin(2006) argues that comprehension is essentially the process of \ncompression. So for instance to understand the string “abababababababababababab” would be \nto describe it as “12 times ab”. As second string is only 11 symbols long, while original is 26, \nChaitin (2006) argues that to produce the second string is same as to understand the first one. \nIf instead of one string, a set of stings is described, then this idea can relate to understanding of \ncorpus. \nComplexity in Prolog \n(In following paragraphs term „corpus‟ is slightly misused. And it means a set of statements of \nany sort. „National corpus‟ is used instead to refer to a large collection of human created texts.) \nIt is possible treat corpus as collection of separate sentences. Those sentences are compressed to \nproduce a program that produces those sentences back. Let‟s call this program Compressed \nCorpus (CC). Length of CC should be shorter or equal to length of original corpus. Addition or \nremoval of sentences from original corpus will be changing the length of CC. \nIt is possible to show that, at least for the case of Prolog, addition of some sentences will \nincrease length of CC, while addition of others will decrease it. Consider that corpus consists \npurely of Prolog statements, and that CC is also written in Prolog. For instance: \nFATHER_CHILD(TOM, SALLY). \nFATHER_CHILD(TOM, ERICA). \nFATHER_CHILD(TOM, JAMES). \nFATHER_CHILD(TOM, POLY). \n \nSIBLING(SALLY, ERICA). \nSIBLING(SALLY, JAMES). \n... \n SIBLING(JAMES, SALLY). \nSIBLING(ERICA, SALLY). \n \nIf in these statements all possible siblings are identified, then this can be compressed into the \nfollowing CC: \nFATHER_CHILD(TOM, SALLY). \nFATHER_CHILD(TOM, ERICA). \nFATHER_CHILD(TOM, JAMES). \nFATHER_CHILD(TOM, POLY). \n \nSIBLING(X, Y) :- FATHER_CHILD(TOM, X), FATHER_CHILD(TOM, Y). \n \nThe second program will produce exactly same statements as the first one. The list of siblings \nwas simply replaced with a rule, to generate such list. \nIf one sibling relation is removed from original corpus, it is no longer possible to use old CC to \nproduce it, as old CC will produce one extra relation which is no longer part of the corpus. So \nafter removal of one relation, new CC will become longer. Adding that statement back will, in \nturn, decrease size of CC. \nIf a statement without any connections to other statements is added, then CC would increase in \nlength. It will decrease when it is removed again. \nThis shows compression aspect to the formal logic and Prolog in particular - many valid \nstatements are compressed into fewer statements (some of which are rules). \nCC limited in size \nAssume that size of CC is limited. In this case there would be times when it won‟t be possible to \ncompress all statements of original corpus. So trade-offs would have to be made to fit CC into \ncertain size: either not all original statements will be compressed, or some extra statements will \nbe compressed. Let‟s call those two tradeoffs Completeness and Accuracy. \nA CC limited in size balances between Accuracy and Completeness. Two extremes could be \nidentified. Imagine size of the CC is very limited. Then there would be two ways to compress a \nlarge corpus: \nNear 100% accuracy with near 0% completeness \n This would be several statements from original corpus in their original form. Except for \nthose few all other statements are not compressed and lost. \nNear 0% accuracy with near 100% completeness \n This would be when near all possible statements are generated without regards to \noriginal corpus. So all original statements will be preserved, but it will be impossible to \ndistinguish them from other random sequences of words. It is not obvious that this \nextreme compression is as useless as the other one (Borges, 1941).  \nComplexity in Generative Grammar \nGenerative grammar uses what appears to be different, but essentially the same approach. As \nwith Prolog it is possible to encode sentences “as is” with it: \nS → TOM IS SALLY’S FATHER \nS → TOM IS ERICA’S FATHER \nS → TOM IS JAMES’S FATHER \n \n S → SALLY AND ERICA ARE SIBLINGS \nS → SALLY AND JAMES ARE SIBLINGS \n… \nThis would reproduce initial statements without any compression. On the other hand it is \npossible to achieve the same with different rules. \nS → TOM IS C FATHER \nC →SALLY \nC →ERICA \nC →JAMES \n \n \nS →C AND C ARE SIBLINGS \nIf a National Corpus is compressed correctly with Generative Grammar, then theoretically \nresulted CC should have near 100% completeness, with measurable positive accuracy. The fact \nthat Generative Grammar usually produces infinite number of statements can be overcome by \nlimiting sentence length to the longest sentence in National Corpus. \nProlog and Generative Grammar compared \nIn essence both Prolog, and Generative Grammar use substitution for compression. In a way \nthey both work towards understanding of a language. Their success of understanding language \ncan be compared to each other. Assume that there is some simple function that translates Prolog \nstatements into human readable, grammatically correct form. \nThe following graph demonstrates how Prolog and Generative Grammar can be compared on \nthe chart. Y-axis measures what percentage of sentences in national corpus can be produces by \nparticular method. X-axis measures percentage of sentences produced by the method that can \nbe found in national corpus. So Y-axis is completeness and X-axis is accuracy. \nSet a maximum program size, and try to find such Prolog program and Generative Grammar \nrules no bigger than that size, that will be generating corpus sentences as completely and \naccurately as possible. Then reduce allowed size and try to repeat this process. It is possible to \nplot this on graph. \n \nLine p indicates Prolog. \n \nLine g indicates Generative Grammar. \nThe direction of the lines indicate decreasing limit of the program. \n \nExtra points are added for comparison: \n \na is half of the sentences from national corpus \n \nb is half of national corpus sentences with addition of same amount of sentences not \nfound in national corpus \n \nc is national corpus itself \n \nSet of statements produced by particular method is called M. Set of statements in national \ncorpus is C.  \nACCURACY = |(M ∩ C)|/|M| \nCOMPLETENESS = |(M ∩ C)|/|C| \nLast graph demonstrates how different methods can be compared. Actual lines might look \ndifferently. In fact for each size limit there won‟t be just one point, but a number of points, most \nefficient of which can be connected into a line. \nLet‟s say that limit is fixed for each compression method at some value. For each fixed \ncompleteness and size there would program with maximum accuracy, and vice-versa. So it is \npossible to plot completeness against accuracy for fixed size. Once again, following graph presents \nonly concept of comparing Prolog and Generative Grammar at fixed program size, actual lines \ncould look much different. \n \nBoth Prolog and Generative Grammar can be called methods of loose compression (LC). LC can \nbe related to understanding of any structure that can be expressed through separate statements \nthat consist of finite alphabet. LC is similar to finding some order in a set of such statements in \norder to compress it into a smaller set of statements, which in turn can be used to loosely \nproduce back original set. \nIt seems that many methods for LC could exist. LC should be able to vary between accuracy, \ncompleteness and size. This means that good LC method should be capable of storing statements \n“as is”, to produce 100% accuracy and 100% completeness when size of CC is limited to the size of \noriginal set. It should be flexible to maximise its accuracy when completeness and size are set, and \nvice-versa. \nAn attempt to find most general and flexible LC method is presented below. \nBracket Compression \nBracket Compression (BC) is a general loose compression method. It facilitates compression of \nstrings of any kind in any form: sentence parts, sentences, paragraphs, and texts. BC syntax \nconsists only of well formed brackets and natural language words (or symbols of any kind) \nbetween them. \nOnly rule for processing statements with brackets is to replace brackets with the ending of \nanother statement that starts with the content of the brackets and does not contain brackets \nitself. For example: \n[GIRL] LIKES PONIES \nGIRL MARY \nThis will produce:  \n[GIRL] LIKES PONIES \nGIRL MARY \nMARY LIKES PONIES \nIf the brackets contain a full existing statement then they are removed (replaced with nothing). \nIf there are several brackets containing same fragment, they all are replaced the same way. This \nsimulates effect of variables without the need for actual variables. If brackets are empty they are \nreplaced with any complete statement, all empty brackets are replaced the same way. \nNAME MARY \nMARY IS A GIRL \nNAME TOM \nTOM IS A BOY \n [NAME] LIKES PONIES [[NAME] IS A GIRL] \nAmong other statements this will produce \n \nMARY LIKES PONIES [MARY IS A GIRL] \n \nTOM LIKES PONIES [TOM IS A GIRL] \nBecause there is no statement to replace “[Tom is a girl]”, only one sentence without brackets \nwill be generated: \n \nMARY LIKES PONIES \nIt is possible to simulate Chomsky context-free grammar with BC. Take for instance grammar \nwith following production rules  \nS → ASA, \nS → BSB, \nS → Ε, \n \nThe corresponding BC would be: \nS→A[S→]A \nS→B[S→]B \nS→ \nThis time “→” was kept, but it has no special meaning in BC, it could‟ve been any other \nsymbol; except for brackets, meaning of everything else is defined within the compression itself. \nBecause multiple brackets with same content in BC are processed as variables, multiple non-\nterminals are numbered. Consider following Generative Grammar rule: \nS→SS \nThis can be achieved with two BC statements. \n S→[S→][S1→] \nS1→[S→] \nIt is also possible to imitate Prolog statements: \n \nGIRL (MARY) \n \nLIKES(X, PONIES) :- GIRL(X) \nThis becomes: \n \nGIRL MARY \n \n[GIRL] LIKES PONIES \nAlternatively this can become: \n \nGIRL MARY \n \nMARY \n \n[] LIKES PONIES [GIRL [] ] \nMost statements can be translated from Prolog to BC. \nAdvantages of BC for Corpus analysis \n \nIt is possible to establish connection between form and meaning. For instance if text is \nabout monarch, plural pronoun might be used instead of singular. \n \n Instead of trying to model the meaning of the sentence, its connection to other sentences \nis captured, which makes work of scientist more impartial. \n \nIt is possible to express illusive logic such as “Mary likes some ponies” also means “She \nis probably not fond of some other ponies”. \n \nMeaning of the word or word combinations within some specific context could be \ncaptured. Some phrases could for instance mean slightly different thing in some \nparticular context. \n \nIt is possible to find a place of negative examples in the language. Starred sentences \n(examples of incorrect sentences) are now part of the language, as they are part of \nnational corpuses. Other models would fail to address them by definition. \n \nIt is possible to analyze information that is not considered a part of language, but plays \nsome ambiguous role, for instance emoticons. \n \nBC has only one type of statements and minimum artificial syntax and logic to it. This \nmeans that the logic expressed with it, is the logic that comes from the language itself. \nFormal definition of Bracket Compression \nBracket Compression (BC) is a simplified compression method. All statements are of the same \nform, and can be generated with the following context-free grammar: \nG = ({S},{WORDS, [, ] },S,P) \nS → S S \nS → [S] \nS → WORD1 | WORD2 | WORD3 | … \nEvery set of N statements in BC can be expanded into N or more statements. The expansion is \nmade by replacement of the words in the brackets, with the ending of the statement that starts \nwith these words, and does not contain brackets itself. Statements with brackets can be \ncompared to logical statements in Prolog. In Generative Grammar they can be compared to \nrules, or strings with non-terminals. \n \nA [B] \nB C \nCan be expanded to \n \nA [B] \nB C \nA C \n If several brackets contain the same sequence of words they are replaced the same way: \n \nA [B] [B] \nB C \nB D \nThis can be expanded to \n \nA [B] [B] \nB C \nB D \nA C C \nA D D \nNotice that neither A C D nor A D C can be obtained from the original set. \nEmpty brackets are replaced with a complete existing statement. All empty brackets within the \nstatement are replaced the same way. \n \nA [] [ [] C] \nB \nProduces: \n \nA[]  [ [] C] \nB \nA B [B C] \nUsage examples \nOne possible application would be to find a rule that connects two sentences. And then \ngenerate large amount of such pairs and look if there are texts where they stand together. \nSufficient amount of trials might help discover connections between seemingly unconnected \nsentences. Or show connection between sentences that seem to be connected. Such rule can \nhave following form: \n[NAME] WAS WEARING [ACCESSORY] [SOME OTHER SENTENCES] \n [PR] TOOK OFF [ACCESSORY] [PRONOUN OF [NAME] IS [PR]] \nThis statement parts mean the following: \n[[NAME] WAS WEARING [ACCESSORY]] – produces first sentence. Name generates name. \nAccessory is an accessory. \nMary was wearing necklace \n[SOME OTHER SENTENCES] – produces sentences (or words) in between. In the \nimplementation no actual sentences are generated. This just denotes that something can \nbe between first and second sentence. \n[PR] TOOK OFF [ACCESSORY] – Produces second sentence, where Pr is pronoun, and \naccessory is the same accessory. \nShe took off necklace \n[PRONOUN OF [NAME] IS [PR]] – matches a name with the pronoun used in the main \nsentence. \nPronoun of Mary is she \nThis would find a text where “Mary was wearing necklace” is followed by “She took off \nnecklace” (articles are missing for simplification). Also text with “He took off tie” after “John \nwas wearing a tie” would be found. \nThis demonstrates everyday kind of logic – “what is taken off was once worn”. This kind of \nlogic is otherwise is hard to capture. Also this captures that when something is worn, the person \nwho is wearing it is addressed by name, and when the person takes it off, then the person is \naddressed by a pronoun. This is probably not true – but some relations of this sort might exist. \nA simple search in Google shows that this method indeed can be applied to real texts, for \ninstance search for: \n\"she was wearing\" \"she took off\" \nThis finds several texts, where item worn is the same item that is taken off. \nAddition statements \nWith BC it is possible to recursively specify addition. \nAFTER 0 IS 1 \nAFTER 1 IS 2 \nAFTER 2 IS 3 \netc.. \nNUMBER 0 \nNUMBER [AFTER [NUMBER] IS] \nBEFORE [NUMBER] IS [ANOTHER NUMBER] [AFTER [ANOTHER NUMBER] IS [NUMBER]] \nANOTHER NUMBER [NUMBER] \n[NUMBER] + 0 = [NUMBER] \n[NUMBER] + [ANOTHER NUMBER] = [[AFTER [NUMBER] IS] + [BEFORE [ANOTHER NUMBER] IS]] \nThis would be sufficient to recursively generate all “n + m = k” statements. It is very unlikely \nthat in natural language such amount of recursion would be found, but this could be suitable \nfor generating scientific part of the corpus. \nMake compression with NOT \nTOM IS A GIRL, NOT! \nMARY IS A GIRL \nNAME MARY \nNAME TOM \n[NAME] LIKES PONIES [[NAME] IS A GIRL] \nThis would produce a negative statement without logic specified. \n \nMARY LIKES PONIES \n \nTOM LIKES PONIES, NOT! \nThis might help to better understand double negatives in some languages. This also shows the \npossibility that logic needs not to be artificially modelled. Also there is some evidence in the \nother NLP (neuro-linguistic programming) that humans are easily confused with multiple \nnegations (Erickson, 1976). This could be related the possibility that human do not have “logical \nnegation” in Computer Science sense. \nText compression \nWith BC it is theoretically possible to compress a whole text. For instance: \nONCE UPON A TIME THERE LIVED [NAME OF THE [HERO]]. [PRONOUN OF [HERO]] WAS \n[POSITIVE QUALITY OF [GENDER OF [HERO]]]. ONE DAY [NAME OF THE [HERO]] \n[OUTSTANDING ACTION]. etc.. \nIt is possible to represent in this form any particular fairytale. Theoretically it is possible to find \na template for all fairytales, though that is probably almost as hard as to find a common \ntemplate for all texts. \nAI \nIt might be possible to define AI from BC. AI would be such a compression of national corpus \nthat its size is reasonable small, its accuracy is reasonably low, and its completeness is \nreasonably high.  It might be possible to estimate those parameters without actual construction \nof AI. For instance they could look like this: \nSize: 100MB \nAccuracy: 0.00002% \nCompleteness: 99.98% \nThis would give at least some idea to scientists what AI machine might look like. If this \nnumbers could‟ve been found, it would be possible to argue that this is an improvement over \nTuring Test. \nAccuracy is set so low because it can be thought that there is some bigger, theoretical corpus, \nwhich includes all conceivable texts (including dialogs during Turing Test). Real corpus is only \nsmallest fraction of all possibilities, though it should be sufficient to give enough evidence for \ncreating correct rules. Accuracy is probably many magnitudes smaller in reality that estimated \nabove. \nText generating \nCompressing example sentences with low accuracy could be used to produce pseudo-random \ntexts, as an alternative to Markov Chains. \nPartial understanding \nThis model can provide partial understanding of the language, some logical observations “out \nof the apparently monolithic problem of natural language understanding” (Abney, 1996). It will \nbe possible to statistically test those relations using national corpuses. \nPossible developments \nFunctions \nIt might be sensible to add system functions to BC. Comparison function would aid practical \ncompression and help increase accuracy.  This function would be automatically generated by \nthe system hosting BC and would iterate through all possible words and sentences telling \nwhich are different. Produced rules could have following form: \n \nDIFFERENT A FROM B \n \nDIFFERENT A FROM C \n \nDIFFERENT B FROM C D \n \netc. \nThough it is possible in some cases to compress even this, it seems to be quite complex. \nConclusion \nIn this paper it was analysed that in fact two common approaches of analysing language and \nknowledge are variations of a more common approach. This common approach was defined. \nThis approach uses language itself as the basis, and establishes connections between real \nlanguage sentences, instead of modelling it. This approach might gain popularity in modern \nlinguistics due to appearance of corpuses, as they make it possible to utilise this method. Also \nthis model provides alternative view on Strong AI. \nBibliography: \nAbney, S. (1996) Part-of-Speech Tagging and Partial Parsing \nBorges, J. L. (1941) La biblioteca de Babel, El Jardín de senderos que se bifurcan \nChaitin, G. (1987) Algorithmic Information Theory, Cambridge University Press. \nChaitin, Gregory (2006), The Limits Of Reason  \nCristian S. Calude (2002). Information and Randomness: An Algorithmic Perspective, second edition. \nSpringer. ISBN 3-5404-3466-6 \nErickson M.H., Rossi E.L. (1976) Hypnotic realities \nPlungyan,(2009). Why modern linguistics should be corpus linguistics? (Public lecture in Russian) \nhttp://www.polit.ru/article/2009/10/23/corpus/ \n \n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2012-03-14",
  "updated": "2012-03-14"
}