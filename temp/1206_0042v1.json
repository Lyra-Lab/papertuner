{
  "id": "http://arxiv.org/abs/1206.0042v1",
  "title": "Language Acquisition in Computers",
  "authors": [
    "Megan Belzner",
    "Sean Colin-Ellerin",
    "Jorge H. Roman"
  ],
  "abstract": "This project explores the nature of language acquisition in computers, guided\nby techniques similar to those used in children. While existing natural\nlanguage processing methods are limited in scope and understanding, our system\naims to gain an understanding of language from first principles and hence\nminimal initial input. The first portion of our system was implemented in Java\nand is focused on understanding the morphology of language using bigrams. We\nuse frequency distributions and differences between them to define and\ndistinguish languages. English and French texts were analyzed to determine a\ndifference threshold of 55 before the texts are considered to be in different\nlanguages, and this threshold was verified using Spanish texts. The second\nportion of our system focuses on gaining an understanding of the syntax of a\nlanguage using a recursive method. The program uses one of two possible methods\nto analyze given sentences based on either sentence patterns or surrounding\nwords. Both methods have been implemented in C++. The program is able to\nunderstand the structure of simple sentences and learn new words. In addition,\nwe have provided some suggestions regarding future work and potential\nextensions of the existing program.",
  "text": "Language Acquisition in Computers\nMegan Belzner\nbelzner@mit.edu\nSean Colin-Ellerin\nseancolinellerin@gmail.com\nJorge H. Roman\njhr@lanl.gov\nApril 2012\nAbstract\nThis project explores the nature of language acquisition in computers, guided by\ntechniques similar to those used in children. While existing natural language processing\nmethods are limited in scope and understanding, our system aims to gain an under-\nstanding of language from ﬁrst principles and hence minimal initial input. The ﬁrst\nportion of our system was implemented in Java and is focused on understanding the\nmorphology of language using bigrams. We use frequency distributions and diﬀerences\nbetween them to deﬁne and distinguish languages. English and French texts were an-\nalyzed to determine a diﬀerence threshold of 55 before the texts are considered to be\nin diﬀerent languages, and this threshold was veriﬁed using Spanish texts. The second\nportion of our system focuses on gaining an understanding of the syntax of a language\nusing a recursive method. The program uses one of two possible methods to analyze\ngiven sentences based on either sentence patterns or surrounding words. Both methods\nhave been implemented in C++. The program is able to understand the structure of\nsimple sentences and learn new words. In addition, we have provided some suggestions\nregarding future work and potential extensions of the existing program.\n1\narXiv:1206.0042v1  [cs.CL]  31 May 2012\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nContents\n1\nIntroduction\n3\n1.1\nHistory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.1.1\nStatistical Parsers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.1.2\nOther NLP Programs . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.2\nCommon Language Properties . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.2.1\nBigrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n1.2.2\nRecursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.3\nLinguistic Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n1.4\nProject Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2\nMorphology\n11\n2.1\nAlgorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.2\nLimitations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3\nSyntax\n17\n3.1\nAlgorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.2\nLimitations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4\nAnalysis\n23\n4.1\nComparison to Existing Programs . . . . . . . . . . . . . . . . . . . . . . . .\n23\n5\nExtension\n25\n5.1\nOther Sentence Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n5.2\nSemantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n6\nConclusion\n31\nA Sample Texts\n34\nB Morphology Code\n35\nC Syntax Code\n37\nPage 2 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n1\nIntroduction\nNatural language processing is a wide and varied subﬁeld of artiﬁcial intelligence.\nThe\nquestion of how best to give a computer an intuitive understanding of language is one with\nmany possible answers, which nonetheless has not yet been answered satisfactorily. Most\nexisting programs work only within a limited scope, and in most cases it cannot realistically\nbe said that the computer actually understands the language in question.\nThis project seeks to give a computer a truly intuitive understanding of a given language,\nby developing methods which allow the computer to learn the language with a minimum of\noutside input, on its own terms. We have developed methods to teach the computer both\nthe morphology and the syntax of a language, and have provided some suggestions regarding\nlanguage acquisition at the semantic level.\n1.1\nHistory\nThe idea of natural language processing originates from Alan Turing, a British computer\nscientist, who formulated a hypothetical test, known as the “Turing Test”. The “Turing\nTest” proposes that the question “Can machines think?” can be answered if a computer\nis indistinguishable from a human in all facets of thought, such as conversation; object\nidentiﬁcation based on given properties; and so forth [1]. After Turing’s proposition, many\nattempts have been made to create natural language processing software, particularly using\nsound recognition, which is currently used in cell-phones, most proﬁciently in the iPhone\n4S Siri system. However, most of these programs do not have high-level semantic abilities,\nrather they have a very limited set of operations, for which keywords are assigned. For\nexample, the Siri system can send an email or text message. When told to send an email\nor text message, the software uses these keywords to open a blank e-mail or text message\nand when told what is to be written in the e-mail or text message, there is no semantic\nPage 3 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nunderstanding of the message, simply a transcription of the words using voice recognition\n[2]. Similarly, there is a lot of software, such as LingPipe, that is able to determine the origin\nand basic ‘signiﬁcance’ of a term or sentence by searching the term(s) on a database for other\nuses of the term(s). These programs do not, however, gain a semantic understanding of the\nterm(s), rather they simply collect patterns of information with association to the term(s)\n[3].\n1.1.1\nStatistical Parsers\nThere have also been some more technical, less commercially eﬃcacious attempts at natural\nlanguage processing, such as statistical language parsers, which have been intricately devel-\noped by many educational institutions. Parsers are a set of algorithms that determine the\nparts of speech of the words in a given sentence. Current parsers use a set of human-parsed\nsentences that creates a probability distribution, which is then used as a statistical model for\nparsing other sentences. Stanford University and the University of California Berkeley use\nprobabilistic context-free grammars (PCFG) statistical parsers, which are the most accurate\nstatistical parsers currently used, with 86.36% and 88% accuracy, respectively [4] [5]. The\ndiﬀerent parts of speech are separated as in Figure 1.\nIn Figure 1, NN = noun, NP = noun phrase, S = subject, VP = verb phrase, and the\nother symbols represent more speciﬁc parts of speech. One can see that the parser splits the\nsentence into three parts: the subject noun phrase, the verb phrase, and the noun phrase.\nEach of these parts is then split into more parts and those parts into parts, ﬁnally arriving\nat individual qualiﬁcations for each word. The assignment of a given part of speech for a\nword is determined by tentatively allocating the part of speech that is most probable for\nthat word, which is then tested within its phrase (i.e. subject noun phrase, or verb phrase,\netc.), and if the probability remains high then that part of speech is set for the word. These\nPage 4 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nFigure 1: Statistical parser tree [4]\nparsers are called context-free because the parse of a word is not aﬀected by the other words\nin the sentence other than those in its phrase, while the less accurate parsers obtain an\noverall probability for a sentence and adjust their parsing accordingly [6].\n1.1.2\nOther NLP Programs\nIn addition to statistical parsers, which only determine the syntax of a sentence, some ele-\nmentary programs have been written for evaluating the semantics of a given body of text.\nThere is a system called FRUMP that organises news stories by ﬁnding key words in an\narticle that match a set of scripts and then assigns the article to a certain category, in\nPage 5 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nwhich it is grouped with other articles of similar content. SCISOR summarizes a given news\narticle by analyzing the events, utilizing three diﬀerent sets of knowledge: semantic knowl-\nedge, abstract knowledge, and event knowledge. As the program sifts through the body\nof text, certain words trigger diﬀerent pieces of knowledge, which are compiled to gain the\nbest understanding of that word or sequence of words. The resultant meanings can then be\norganized and rewritten using similar meanings, equally balanced among the three sets of\nknowledge as the original piece of information. Similarly, TOPIC summarizes a given text\nby distinguishing the nouns and noun phrases and then analyzing their meaning through a\n“thesaurus-like ontological knowledge base”, after which the program uses these equivalent\nmeanings to rewrite the text [7].\n1.2\nCommon Language Properties\nCertain elements of language are commonly used both in natural language processing and the\ngeneral analysis of language. These properties include bigrams and recursion, two properties\nwhich play a signiﬁcant role in this project.\n1.2.1\nBigrams\nAn n-gram is a sequence of n letters.\nThe most commonly used forms of n-grams are\nbigrams and trigrams because these oﬀer a speciﬁc indication for a set of information, without\nsignifying extremely rare and complex aspects of the subject. A good example of this is the\nuse of bigrams in cryptography. A common method of decoding a message that has been\nencoded using a keyword, like the Vigenere Cipher encryption, is to calculate the distance\nin letters between two of the same bigrams in order to determine the length of the keyword,\nand then the keyword itself [8]. If any n-grams are used, where n is greater than or equal\nto 4 or even in some cases if n is equal to 3, then the number of same n-grams for a given\nPage 6 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nmessage would be very rare and make determining the length of the keyword increasingly\ndiﬃcult.\nSimilarly, in natural language processing, bigrams are used for word discrimination, which\nis the understanding of an unknown word based upon bigram correspondence with a reference\nlist of known words. In addition, word bigrams are used in some lexical parsers, the Markov\nModel for bag generation, and several text categorization tools [9].\n1.2.2\nRecursion\nThe principle of recursion is an essential aspect of human language, and is considered one\nof the primary ways in which children learn a language. For a sentence with a particular\npattern, a word with a speciﬁc part of speech can be exchanged for another word of the same\npart of speech, indicating that the two words have the same part of speech. For example,\ngiven the sentence: “The boy wears a hat”, it can be determined that “the” and “a” are the\nsame part of speech by reconstructing the sentence as “A boy wears the hat”. In addition,\nthe word “boy” can be exchanged for the word “girl”, indicating that these are also the same\ntype of word, thereby expanding the lexicon of the child.\nIn addition, the words of a sentence can remain unchanged, while the pattern changes,\nthereby introducing a new part of speech. If we have the sentence “The boy wears a hat”\nor “A B C A B” if represented as a pattern of parts of speech, we can add a “D” part of\nspeech by creating a new sentence, “The boy wears a big hat” (A B C A D B). The child\nascertains that “big” must be a new part of speech because no words have previously been\nplaced between an “A” and a “B”. This method can be repeated for any new part of speech,\nas well as embedded clauses such as “The boy, who is very funny, wears a big hat.”\nFinally, recursion can be used to indicate the grammatical structures of a language. Let\nus examine the following poem:\nPage 7 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nWhen tweetle beetles ﬁght,\nit’s called\na tweetle beetle battle.\nAnd when they\nbattle in a puddle,\nit’s a tweetle\nbeetle puddle battle.\nAND when tweetle beetles\nbattle with paddles in a puddle,\nthey call it a tweetle\nbeetle puddle paddle battle.\n- Excerpt from Dr. Seuss’ Fox in Socks [10]\nThe author uses the recursive principle to indicate that “tweetle beetle” can be both a\nnoun and an adjective, and then repeats this demonstration with “puddle” and “paddle”.\nFurther, the correct placement of the new noun acting as an adjective is shown to be between\nthe old string of nouns acting as adjectives and the object noun. Conversely, the poem\nillustrates that a noun acting as an adjective can be rewritten as a preposition and an added\nclause, e.g “a tweetle beetle puddle battle” can be rephrased as “a tweetle beetle battle in a\npuddle” [11]. Thus, the principle of recursion can allow a child to acquire new vocabulary,\nnew types of parts of speech, and new forms of grammar.\n1.3\nLinguistic Interpretation\nThere is a basic three-link chain in the structure of language. Phonetics is the most basic\nstructure, which is formed into meaning by units, known as words. Units are then arranged\nsyntactically to form sentences, which in turn forms a more extensive meaning, formally\ncalled semantics [12].\nIt is fundamental in learning a language that a computer understand the connections in\nPage 8 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nthe phonetics-syntax-semantics chain, and the structure and computations of each, with the\nexception of phonetics. Phonetics and their formulation can be disregarded because these\nrefer more greatly to the connections of the brain to external sounds, than the core structure\nof language. In fact, the entire external world can be ignored, as there need not be an infer-\nence between external objects in their human sensory perception, and their representation\nas language, formally known as pragmatics or in Chomskyan linguistics as E-language (Ex-\nternal Language). Instead, the relations between words and meaning, intricately augmented\nby the semantics created by their syntactically accurate formed sentences, is the only form\nof language necessary, denominated contrastingly as I-language (Internal Language) [12].\nNoam Chomsky argues that I-Language is the only form of language that can be studied\nscientiﬁcally because it represents the innate structure of language [7]. Language similar\nto this form can be found in humans who are literate in a language, yet can not speak it.\nTherefore, due to the increase in structure, the lack of external representation in natural\nlanguage processing may be more advantageous than one might think.\n1.4\nProject Deﬁnition\nThis project examines the nature of language acquisition in computers by implementing\ntechniques similar to those used by children to acquire language. We have focused primarily\non morphology and syntax, developing methods to allow a computer to gain knowledge of\nthese aspects of language. We have developed programs in both C++ and Java.\nRegarding morphology, the program is able to analyze the word structure of given lan-\nguages and distinguish between languages in diﬀerent samples of text using bigram frequen-\ncies, and we have examined the usefulness and limitations of this method in the context of\nexisting methods. Using this technique we have developed computationally understandable\ndeﬁnitions of English, French and Spanish morphologies. We have also described and par-\nPage 9 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\ntially implemented a novel technique for understanding the syntax of a language using a\nminimum of initial input and recursive methods of learning both approximate meanings of\nwords and valid sentence structures. Finally, we provide suggestions for future work regard-\ning the further development of our methods for understanding syntax as well as potential\nmethods for gaining a rudimentary understanding of semantics.\nPage 10 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n2\nMorphology\nTo analyze the morphology of a given language, bigrams can be used to deﬁne and compare\nlanguages. Since the frequency distribution of a set of bigrams is unique to a given language\n(across a large enough sample text), this can be used as an accurate identiﬁer of a language\nwith minimal eﬀort.\n2.1\nAlgorithm\nThe program was initially developed in C++ then translated to Java to take advantage\nof non-standard characters, and is set up in two main portions.\nThe ﬁrst step involves\ngenerating a table of frequency values from a ﬁle, and the second step is to compare the two\ntables and determine the level of similarity.\nTo generate a frequency table, a two-dimensional numerical array is created from the set\nof valid characters such that the array includes a space for every possible bigram, with the\ninitial value for each being set to zero. For each word from the input ﬁle, the program checks\neach pair of letters, adding one to the corresponding position in the bigram frequency table.\nOnce the end of the ﬁle is reached, each frequency count is divided by the total number of\nbigrams found times 100 to give a percentage frequency. This process is shown in Figure 2.\nThis produces an array similar to Table 1, which can be analyzed separately to examine\ncommon and uncommon bigrams for a given language, or compared with another text’s table\nto distinguish between languages as detailed below.\nAfter the frequency tables are created for each ﬁle, the two must be compared to deter-\nmine the level of similarity between the languages of the two ﬁles. This is done by ﬁnding\nthe absolute values of the diﬀerences between corresponding frequencies for the two ﬁles,\nthen ﬁnding the sum of these diﬀerences as seen in Figure 3.\nThis gives an approximate measure of how diﬀerent the two ﬁles are in terms of the\nPage 11 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nFigure 2: Learning morphology with bigrams\na\nb\nc\n. . .\na\n0\n0.204\n0.509\n. . .\nb\n0.080\n0\n0\n. . .\nc\n0.298\n0\n0.269\n. . .\n...\n...\n...\n...\n...\nTable 1: Sample of frequency array\nfrequency of given bigrams. As each language tends to have a unique frequency distribution,\na large net diﬀerence suggests a diﬀerent language for each ﬁle while a smaller net diﬀerence\nsuggests the same language. The threshold dividing a determination of ‘same language’ or\n‘diﬀerent language’ was experimentally determined to be approximately 55.\nFigure 3: Calculating morphology diﬀerences between sample texts\nPage 12 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n2.2\nLimitations\nThis method does have certain limitations, however. Since the program deals with bigrams\n(though it can be easily made to use n-grams for any n greater than 1), single-letter words\nare not taken into account. While this does not have a large overall eﬀect, it produces some\ninaccuracies in the analysis of frequencies for a given language.\nA more signiﬁcant limitation is the requirement that all “legal” characters be deﬁned\nbefore the program is run. Although it would be relatively straightforward to dynamically\ndetermine the character set based on the input ﬁles, this creates issues where the character\nsets for each ﬁle are not the same, making it diﬃcult, if not impossible, to accurately compare\nthe two ﬁles. Even ignoring this, varying the number of characters may produce variations\nin the threshold used to determine language similarity. The program is also eﬀective only\nfor ﬁles of considerable length to allow for a large enough sample size.\n2.3\nResults\nFigure 4: Distribution of English bigrams\nPage 13 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nThe program was run using a series of ﬁles in English, French and Spanish.\nInitial\nfrequency tables for analysis of individual languages were created using the EU Charter in\neach respective language [13], producing the frequency distributions shown in Figures 4, 5,\nand 6 for English, French, and Spanish, respectively.\nFigure 5: Distribution of French bigrams\nThese frequency graphs show that each language has a handful of extremely common\nbigrams (in addition to some which appear little or not at all). In English, this includes\n“th” and “he” with percentage frequencies of 2.9 and 2.8, respectively, along with “on” and\n“ti” also both above 2.5%. This data is slightly skewed by the text used, though “th” and\n“he” are indeed the most common bigrams in English. A study conducted using a sample of\n40,000 words [14] gave the two frequencies of 1.5 and 1.3, respectively, though the next most\ncommon bigrams in the sample text are not as common in the English language as a whole\nas their frequencies here would suggest. This is largely due to an inherent bias in the text,\nas words such as “protection” or “responsibilities” appear frequently in the EU Charter.\nFrench resulted in “es” and “on” as the most common bigrams, followed by “de” and\nPage 14 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nFigure 6: Distribution of Spanish bigrams\n“le”. In Spanish, the most common bigram by far was “de”, followed by “cl”, “en”, “er”,\nand “es”. Again, however, these likely suﬀer from slight biases due to the nature of the text.\nThe comparison threshold was initially determined using a series of randomized Wikipedia\narticles of considerable length in English and French. The same threshold was also used\nto compare English and Spanish texts and French and Spanish texts with continued high\naccuracy for texts of considerable length, showing that this method does not vary notably\nwith diﬀerent languages. The outputs of these tests are shown in Table 2.\nphilosophy\nencyclopedia\nfrance (fr)\ncapitalism (fr)\njazz (es)\nnyc (es)\nphilosophy\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nencyclopedia\n33.13\n. . .\n. . .\n. . .\n. . .\n. . .\nfrance (fr)\n73.83\n75.31\n. . .\n. . .\n. . .\n. . .\ncapitalism (fr)\n73.94\n79.62\n30.15\n. . .\n. . .\n. . .\njazz (es)\n67.68\n69.56\n64.76\n67.76\n. . .\n. . .\nnyc (es)\n71.76\n73.42\n66.41\n70.46\n28.95\n. . .\nTable 2: Sample of sum diﬀerences\nPage 15 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nFinally, this method was tested with ﬁles of varying lengths. For the set of two English\ntexts which were tested with decreasing word counts, the point at which this method was\nno longer accurate was between 400 and 200 words. For other ﬁles this is likely to vary,\nand could be lessened with further ﬁne-tuning of the threshold. At a certain point, however,\nthe diﬀerence values begin overlapping due to variation and bias from the words used in the\ntext, making accuracy impossible.\nPage 16 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n3\nSyntax\nTo analyze the syntax of a language, a “recursive learning” method is implemented using\nC++. Since the program would ideally require an absolute minimum of initial information,\nthis method takes a small initial set of words and builds on this by alternately using known\nwords to learn new sentence structures and using known sentence structures to learn new\nwords as seen below.\n{cat, man, has} →“The man has a cat” →“The x y a z” →“The man wore a hat” →\n{cat, man, has, wore, hat}\n3.1\nAlgorithm\nThere are two main elements to understanding the recursive learning system, namely un-\nderstanding both how the information is represented and the methods used to analyze new\ninformation.\nThe information this program gathers can be split into two pieces, information on the\nwords themselves and information on valid sentence structures. For words, the program\nkeeps track of the word itself and the word’s type. Only two speciﬁc types are deﬁned in\nthe program, “noun” and “verb”, and the actual meaning of each is deﬁned by context.\nAny word which does not ﬁt these deﬁnitions is deﬁned relative to these deﬁnitions. For\nsentences, the program keeps track of the number of words in a given sentence pattern and\nthe type of the word in each position (in an array). These structures are shown in Figure 7.\nThe methods used by the program to analyze new information can also be split into two\npieces. Both methods require that some information is already known about the sentence\nin question, but are used in slightly diﬀerent ways. The ﬁrst method analyzes new words\nbased on the structure of the sentence, by selecting the most applicable of existing patterns\nPage 17 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nFigure 7: Word and sentence pattern datatypes\nbased on correspondence with known information. The program keeps track of how many\n“matches” there are between the sentence and the known pattern, taking the one with the\nmost matches (if greater than half the word count) and using it to set unknown word types.\nThis method is seen in Figure 8\nFigure 8: Sentence structure method of learning\nThis method is particularly useful for learning new nouns and verbs, in situations where\nother words in the sentence are primarily known grammatical particles. The other method\nuses the surrounding words to deﬁne the type of any unknown words. The program notes the\ntypes of the words before and after the unknown word, and the type of the unknown word\nis then stated as “a<type of previous word> b<type of next word>”. For example, a word\nwith type “anoun bverb” would be one which tends to come after nouns and before verbs.\nPage 18 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nThis type could also be analyzed and modiﬁed to give further insight into other types of\nwords such as adjectives, as detailed in Section 5.1. The program can redeﬁne known words\nif new information is found on their positioning, using the new deﬁnition if it is shorter, and\nthus more general, than the previous deﬁnition. This is seen in Figure 9.\nThis method is best used when most of the nouns and verbs in a sentence are known,\nbut other words exist which are not known. The program would be able to dynamically\nselect between the two methods based on what information is already known. As a rule, the\nprogram would default to the ﬁrst method initially as this is more likely to provide accurate\nresults.\nFigure 9: Word-based method of learning\n3.2\nLimitations\nWith proper development this method could be used to learn many types of sentences.\nHowever, it still has several limitations. Although it is suﬃcient for simpler grammatical\nconstructs, complicated words and patterns could cause confusion.\nIn particular, words\nwhich have multiple meanings with diﬀerent parts of speech would confuse the program\nPage 19 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\ntremendously, and at present it also has no way of connecting related words (such as plural\nor possessive forms of nouns).\nIn addition, this method requires carefully crafted training sentences. Although it can\nwork with a minimum of initial information unlike most existing systems, it still has to learn\nthe constructs somewhat sequentially and avoid having too many new concepts introduced\nat once. This could be partially remedied by implementing a method by which the program\nstores any sentences it does not yet have the tools to analyze to be recalled later, so that a\nmore general text could be used as training material.\n3.3\nResults\nAs a proof of concept for these methods, a series of three training sentences were input\nand analyzed by the program to learn a handful of new words and concepts. Although the\nprogram begins with far less information than many existing programs, it nonetheless needs\nsome initial input - in this case three words which will, together with “a” and “the”, make\nup the initial sentence, as seen in Table 3.\nWord\nType\nhas\nverb\nhat\nnoun\nman\nnoun\nTable 3: Initial program input\nThe ﬁrst sentence input into the program is “the man has a hat,” which is analyzed\nusing the word-based second method. From here, two new words are learned - namely “a”\nand “the” which are deﬁned based on the surrounding words. The sentence pattern is also\ncatalogued, and the known information reads as in Table 4.\nAt present, although “the” must only appear before a noun, “a” is assumed to require a\nPage 20 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nWord\nType\nhas\nverb\nhat\nnoun\nman\nnoun\nthe\nbnoun\na\naverb bnoun\n5 bnoun—noun—verb—averb bnoun—noun\nTable 4: Program knowledge after run one\npreceding verb. To correctly deﬁne “a”, the next sentence reads “a man has the hat.” While\nusing the same words, the two known grammatical particles are reversed. The program\nredeﬁnes the word “a” with a more general deﬁnition (i.e. simply requires a succeeding noun),\nhowever the deﬁnition of “the” remains the same as the program determines that replacing\nthe deﬁnition would add more constraints, which is counterproductive. The sentence pattern\nis again catalogued, and the known information reads as in Table 5.\nWord\nType\nhas\nverb\nhat\nnoun\nman\nnoun\nthe\nbnoun\na\nbnoun\n5 bnoun—noun—verb—averb bnoun—noun\n5 bnoun—noun—verb—bnoun—noun\nTable 5: Program knowledge after run two\nAlthough the two existing sentence patterns are functionally identical, and the ﬁrst should\nactually be redeﬁned as the second, both are kept to demonstrate the ﬁrst method based on\nsentence patterns. For this, the sentence “the dog ate a biscuit” is used, having the same\nstructure as existing sentences but a diﬀerent set of nouns and verbs. Although some matches\nexist with the ﬁrst pattern, the redeﬁnition of “a” results in only two matches. Instead, the\nprogram ﬁnds this to be the same as the second sentence pattern, as the number of words\nmatches as do the types of the words “the” and “a”. Hence, the program deﬁnes the unknown\nPage 21 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nwords based on this pattern, resulting in the set of information shown in Table 6.\nWord\nType\nhas\nverb\nhat\nnoun\nman\nnoun\nthe\nbnoun\na\nbnoun\ndog\nnoun\nate\nverb\nbiscuit\nnoun\n5 bnoun—noun—verb—averb bnoun—noun\n5 bnoun—noun—verb—bnoun—noun\nTable 6: Program knowledge after run three\nPage 22 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n4\nAnalysis\nIn this project we have developed methods for allowing a computer to understand and learn\nboth the morphology and the syntax of a language. Using novel techniques or applications,\nwe have designed and implemented these methods and tested their capabilities for learning\nlanguage.\nAlthough the use of bigrams in language analysis is not a new idea, we have implemented\nit in a novel way by working to develop it as a deﬁning quality and learning mechanism for\nnatural language processing. The method proves very useful for understanding the mor-\nphology of a language, though only to a point. It is extremely eﬀective when using a large\nenough sample text, but with smaller sample texts it is no longer able to accurately compare\nlanguages. Hence, although it creates a computationally eﬀective “deﬁnition” of a language,\nits actual ability as a learning mechanism is limited. Used in tandem with other methods,\nthe bigram method could prove extremely powerful.\nThe recursive learning method implemented for gaining an understanding of syntax proves\nvery useful and has great potential to be developed further. Both of its submethods work\nwith high accuracy for simple sentences, and hence it is able to develop a growing model of\nsentence construction. Even more particularly, it is able to do this with a very small amount\nof initial input and with methods which could be applied to many types of languages.\n4.1\nComparison to Existing Programs\nThe use of bigrams to understand and analyze diﬀerent parts of a given language has been\nstudied and implemented substantially.\nFor example, there are programs that calculate\nbigram frequencies to evaluate a language’s morphology.\nHowever, unlike our program,\nnone to date have utilized the diﬀerences in bigram frequencies between two languages to\ndistinguish one language from the next.\nPage 23 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nThe system in the program that was used for determining the parts of speech in a sentence\nhas rarely been attempted, and when it has been used, only partially and in conjunction\nwith other methods. Most natural language processing programs have been designed to be\nas the eﬃcient and eﬀective as possible. As a result, many use large banks of initial data,\nwhich the program then analyzes and uses for subsequent input. As discussed previously, the\nmost common and successful programs of this sort are statistical parsers. On the contrary,\nour program uses the recursive principle to acquire new vocabulary and forms of syntax for\na given language, provided only a very small initial set of data. In practice, our model only\nrequired one sentence with verb and noun indicated to determine the parts of speech of all\nother words, although not denoting them in linguistic terms (article, preposition, etc.), as\nwell as learn new words and, in theory, to learn new sentence patterns. Despite the extensive\npower of the recursive method, it has rarely been used in the history of natural language\nprocessing.\nThe results of our program illustrate the potential abilities of the recursive\nmethod that have not been seen previously.\nPage 24 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n5\nExtension\nThe program as it stands now can learn and understand a range of language elements,\nincluding morphology and simple sentence patterns. However, there is still signiﬁcant room\nfor further exploration both by developing the techniques for learning syntax to allow for a\nmore complete range of sentence patterns, and by developing methods for a computational\nunderstanding of semantics.\n5.1\nOther Sentence Structures\nIn addition to existing sentence structures and constructs, the syntax program can learn\nother common constructs. Although some may be understandable at present, others may\nrequire some additions to the program to fully understand. Below are some examples of\nother simple sentence constructs and how the program would interpret them.\n“The man has a blue hat.”\nHere, the only unknown word with present knowledge is “blue”. The program at present\nwould interpret it using the word-context method, resulting in a type of “abnoun bnoun”.\nContinuing in this manner would quickly lead to complications, however, so the program\ncould be extended to understand words based on the form of types which are not nouns or\nverbs. The word directly before blue has the type “bnoun”, so “blue” would be interpreted\nin this manner as a noun, resulting in two nouns in a row. The program could additionally\nbe edited to interpret two like words in a row as a “noun phrase” or “verb phrase”, which\nwould diﬀerentiate adjectives from nouns and adverbs from verbs in a way more intuitive to\nthe computer.\n“The hat is blue.”\nThis sentence presents a diﬀerent use of an adjective eﬀectively in the place of a noun.\nAssuming “is” had been previously learned as a verb, this sentence would be more-or-less\nPage 25 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nreadily understandable as “blue” is still interpreted as related to nouns. This introduces\nanother common sentence construct as well, namely that some nouns can appear directly\nafter verbs without a “bnoun” word in between. Here, it may become worthwhile to add\nsome indicator to new nouns about whether they can appear directly after a verb or not.\n“The man has one hat.”\nThis sentence would introduce numerical words, which would be readily understandable\nas “bnoun” words similar to “a” or “the”. This is suﬃcient and accurate for most cases,\nthough as the program expands into semantics this construct may require more speciﬁc\ndeﬁnition.\n“The man has four hats.”\nHere, the concept of plurality is introduced.\nThis sentence would simply be another\nexample of the above sentence with regards to syntax alone—the word “hats” would just\nbe considered a new noun. However, this would likely be the most problematic concept\nregarding semantics. Without an external concept of meaning or some other indication,\nplurality would have to be learned simply by similarity at the word level. In many cases\nthis would be suﬃcient, such as “hats”, but some words do not follow standard plurality\nrules such as “mice” versus “mouse”. Similar issues apply to verb tenses, though here the\nrules are even less standardized. This issue could be at least partially remedied by creating\nan artiﬁcial semblance of external understanding, though this would likely prove diﬃcult as\nwell.\nMany other sentence constructs are built from these sorts of basic patterns. For example,\nanother common sentence construct involves prepositions such as the sentence “the man\nthrew the hat in the trash”. Assuming prior knowledge of the nouns present, the program\ncould interpret “in” as verb-like, appearing between two noun phrases. This is, again, a\nsuﬃcient interpretation in most cases. A marker indicating what would eﬀectively be two full\nPage 26 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nconstructs could be implemented as well. Embedded clauses would be interpreted similarly,\nwhere the program would ﬁnd the pattern of the outside clause and then the pattern of\nthe inside clause. An understanding of punctuation would make such sentences more easily\ninterpretable.\nDespite some issues, the program can, currently or with only minor modiﬁcations, un-\nderstand many common sentence constructs using the two main forms of learning and still\na fairly small amount of initial information. The program proves fairly versatile, though of\ncourse not at the full level of human understanding.\n5.2\nSemantics\nSemantics is the meanings of words or sentences, not only determined by direct deﬁnition,\nbut also by connotation and context. To gain a computational understanding of semantics in\na given language, without external representation for words, we have devised an associative\napproach. For a noun, diﬀerent adjectives and verbs can be used when employing the noun\nin a sentence. The particular set of adjectives and verbs for that noun are representative of\nthe nature of the object. A second noun will have some shared verbs and adjectives, and\nthe degree to which this is true will indicate the similarities of the two objects. Conversely,\na verb would be understood based on the diﬀerent objects associated with it, especially\nthe order of the nouns, i.e the subjects and objects used with that noun. From a set of\nsentences, a threshold of similarity could be experimentally determined, which would result\nin the categorization of the diﬀerent nouns and verbs. The sequence of sentences could be\nas follows:\n1. The man wears the big hat.\n2. The man throws the small hat.\nPage 27 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n(“hat” is known to be something that can be “big”, “small”, “worn”, and “thrown”)\n3. The man throws the big ball.\n4. The man bounces the small ball.\n(“ball” is like “hat” in having the ability to be “big” or “small” and can be “thrown”,\nhowever it has not yet be found that it can be “worn”, and “hat” has not be shown to\nbe able to be “bounced”)\n5. The man wears the big shoes.\n6. The man throws the small shoes.\n(“shoes” is then understood as the same type of object as “hat”)\n7. The man uses the telephone.\n8. The man answers the telephone.\nLet us stop here because the fundamental logic of the sequence of sentences can now\nbe seen. Using these characteristics of nouns, a complex associative web would be formed,\nwhere objects have meaning based on their relation to a set of other objects, which also\nhave meaning in relations to another set objects that the ﬁrst may not. This web may be\nrepresented visually by a venn diagram similar to the one in Figure 10.\nFor the verbs “wear”, “throw”, “bounce” etc., the program would interpret the above\nsentences as suggesting that only “man” can conduct these actions, but cannot be the recip-\nient of these actions, while certain nouns can be the recipient of these actions. Further, it\nshould be noted that the training sentences do not require a strict order because a charac-\nteristic of a particular noun would be stored until another noun was found to have the same\ncharacteristic, and the two nouns would then share a link of the web. Similarly, a verb would\nhave tentative subjects and objects associated with it until more were found. Nevertheless,\nPage 28 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nFigure 10: Sample semantic web\nthis method is limited by the requirement of large amounts of sentences in order to gain any\nsigniﬁcant understanding of a given word.\nTo understand a concept such as time, which is essential for semantics and acknowledging\ntenses, the program would require some initial speciﬁcation, as well as using a time indicator\nin all sentences referring to the past or present. It would start with the conditional parameter\nthat if a sentence has the word “yesterday”, “ago”, “tomorrow”, “later”, “past”, or “future”,\nthen a time diﬀerent from the current moment is being referenced, and the verb used is similar\nto another verb (the present form, which is the ﬁrst form the program learns), but with a\nslightly diﬀerent morphology. Yet, a problem arises because “last” and “next” applied in\n“last/next week” or “last/next month” cannot be used strictly as indicators of time, as they\ncan be used in other contexts, such as “He ate the last cookie in the jar” or “He is next\nin line at the supermarket”. Thus, in certain cases, time would present a diﬃculty for the\nsemantic understanding of the program. The same applies for the use of negation, as in the\nPage 29 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nsentence “The man is not big”, whereby “not”, “never”, “none”, “no longer”, and “no more”\nwould have to be explained prior and would result in the noun or verb being categorized as\ndiﬀerent from other nouns and verbs with shared associative terms.\nIn addition, the associative method only allows for a deﬁnitional understanding of a given\nword, which can be substantially limited due to the possible changes in the meaning of a\nword as a result of its context. To allow a computer to have an understanding of context,\na system could be implemented to keep track of previously learned information. Cognitive\nscientist Marvin Minsky suggests that a series of “frames” which represent generalized situ-\nations can be used to represent cognition computationally. These general frames could then\nbe modiﬁed with situation-speciﬁc variables [15]. This idea could prove useful in natural\nlanguage processing to give a computer an understanding of context. For example, if a series\nof sentences read “John is playing soccer. He kicked the ball,” the program would be able to\nselect a general frame which it could use to keep track of relevant variables, such as “John”\nbeing the subject of this action—hence linking this to the “he” in the next sentence.\nAnother issue that might arise is the issue of connotative meaning of words rather than\nmerely denotative meaning. This is also related to the idea of symbolism, another element of\nlanguage which can prove diﬃcult for a computer to understand. Here, a method similar to\nthe associative approach above could be implemented after the initial denotative associations\nwere formed. Here, the training sentences would be ones using symbolism rather than literal\nones as above. If a word is suddenly associated with a word which is not within the proper\ncategorization, such as “a heart of stone,” it could be interpreted by the computer as a\nconnotative association. This would allow the computer to examine characteristics related\nonly to one or the other, hence gaining an understanding of the symbolic associations of\nwords.\nPage 30 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n6\nConclusion\nUsing certain principles of language, we have designed a novel method by which a computer\ncan gain an intuitive understanding of language rather than simply an artiﬁcial understand-\ning. We have developed techniques by which a computer can learn and analyze the mor-\nphology of any given language, and hence understand diﬀerences between two languages.\nWe have also developed a recursive learning system for understanding sentence patterns and\nconstructs, which uses a minimum of initial information. At present, the program can in-\nterpret many basic sentences, and we have also provided possibilities and suggestions for\nextending the capabilities of the program. This approach is unique compared to common\nnatural language processing systems because of this lack of need for signiﬁcant initial in-\nput and its recursive design, and could have great potential in the ﬁeld of natural language\nprocessing.\nPage 31 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nReferences\n[1] E. Reingold. The Turing Test. University of Toronto Department of Psychol-\nogy. Available at http://www.psych.utoronto.ca/users/reingold/courses/ai/\nturing.html.\n[2] Learn More About Siri. Apple Inc. Available at http://www.apple.com/iphone/\nfeatures/siri-faq.html.\n[3] LingPipe. Alias-I. Available at http://alias-i.com/lingpipe/.\n[4] D. Klein and C.D. Manning. Accurate Unlexicalized Parsing. In: Proceedings of the 41st\nAnnual Meeting on Association for Computational Linguistics, Vol. 1 (2003), 423–430.\n[5] M. Bansal and D. Klein. Simple, Accurate Parsing with an All-Fragments Grammar. In:\nProceedings of the 48th Annual Meeting on Association for Computational Linguistics\n(2010).\n[6] E. Charniak. Statistical Parsing with a Context-Free Grammar and Word Statistics.\nIn: Proceedings of the Fourteenth National Conference on Artiﬁcial Intelligence. AAAI\nPress/MIT Press, Menlo Park, CA (1997), 598–603.\n[7] C.M. Powell. From E-Language to I-Language: Foundations of a Pre-Processor for the\nConstruction Integration Model. Oxford Brookes University (2005).\n[8] R. Morelli. The Vigenere Cipher. Trinity College Department of Computer Science.\nAvailable at http://www.cs.trincoll.edu/~crypto/historical/vigenere.html.\n[9] H.H. Chen and Y.S. Lee. Approximate N-Gram Markov Model Natural Language Gen-\neration. National Taiwan University Department of Computer Science and Information\nEngineering (1994).\n[10] Dr. Seuss. Fox in Socks. Available at http://ai.eecs.umich.edu/people/dreeves/\nFox-In-Socks.txt.\n[11] M.H. Christiansen and N. Chater. Constituency and Recursion in Language. In: M.A.\nArbib. The Handbook of Brain Theory and Neural Networks. 2nd ed. MIT Press, Cam-\nbridge, MA (2003), 267–271.\nPage 32 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n[12] B.C. Lust. Child Language: Acquisition and Growth. Cambridge University Press, Cam-\nbridge, UK (2006).\n[13] The Charter of Fundamental Rights of the European Union. European Parliament.\nAvailable at http://www.europarl.europa.eu/charter/default_en.htm.\n[14] Digraph Frequency. Cornell University Department of Mathematics. Available at http:\n//www.math.cornell.edu/~mec/2003-2004/cryptography/subs/digraphs.html.\n[15] M. Minsky. A Framework for Representing Knowledge. In: J. Haugeland, editor. Mind\nDesign. The MIT Press, Cambridge, MA (1981), 95–128.\nPage 33 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nA\nSample Texts\nSample texts for the morphology-analysis system were primarily selected from randomized\nWikipedia articles in the respective languages, and were selected primarily for length. The\nfollowing sample texts were used for testing:\nEnglish EU Charter (available at http://www.europarl.europa.eu/charter/pdf/text_en.pdf)\nFrench EU Charter (available at http://www.europarl.europa.eu/charter/pdf/text_fr.pdf)\nSpanish EU Charter (available at http://www.europarl.europa.eu/charter/pdf/text_es.pdf)\nEnglish Wikipedia article on “Philosophy” (available at http://en.wikipedia.org/wiki/Philosophy)\nEnglish Wikipedia article on “Encyclopedias” (available at http://en.wikipedia.org/wiki/Encyclopedia)\nEnglish Wikipedia article on “Peace Dollars” (available at http://en.wikipedia.org/wiki/Peace_\ndollar)\nEnglish Wikipedia article on “Buﬀalo Nickels” (available at http://en.wikipedia.org/wiki/Buffalo_\nnickel)\nFrench Wikipedia article on “France” (available at http://fr.wikipedia.org/wiki/France)\nFrench Wikipedia article on “Capitalism” (available at http://fr.wikipedia.org/wiki/Capitalisme)\nFrench Wikipedia article on “Karl Marx” (available at http://fr.wikipedia.org/wiki/Karl_Marx)\nFrench Wikipedia article on “Democracy” (available at http://fr.wikipedia.org/wiki/Dmocratie)\nSpanish Wikipedia article on “Jazz” (available at http://es.wikipedia.org/wiki/Jazz)\nSpanish Wikipedia article on “New York City” (available at http://es.wikipedia.org/wiki/New_\nYork_City)\nSpanish Wikipedia article on “the Sassanid Empire” (available at http://es.wikipedia.org/wiki/\nImperio_sasnida)\nSpanish Wikipedia article on “the Crown of Castile” (available at http://es.wikipedia.org/wiki/\nCorona_de_Castilla)\nPage 34 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nB\nMorphology Code\npackage\nngrams;\nimport\njava.io. BufferedReader ;\nimport\njava.io.File;\nimport\njava.io.FileWriter;\nimport\njava.io.IOException;\nimport\njava.io. InputStreamReader ;\nimport\njava.io.PrintWriter;\nimport\njava.util.Scanner;\npublic\nclass\nNgrams {\n// Main\nprogram\npublic\nstatic\nvoid main(String [] args) throws\nIOException {\nString\nfile1 = \"\", file2 = \"\";\nfloat\ndiff = 0; // Total\nfrequency\ndifference\nbetween\ntwo files\nchar [] alpha = {a,`a,^a,¨a,b,c,¸c,d,e,`e,´e,^e,¨e,f,g,h,i,^ı,¨ı,j,k,l,m,n,o,^o,\np,q,r,s,t,u,`u,^u,¨u,v,w,x,y,¨y,z}; // Valid\ncharacters\nSystem.out.print(\"Input\nFirst\nFile Name: \");\nfile1 = read(file1 ); // Reads\ntext from file\nSystem.out.print(\"Input\nSecond\nFile Name: \");\nfile2 = read(file2 );\nfloat bi1 [][] = count(file1 , alpha ); // Creates\nfrequency\ntable for file\nfloat bi2 [][] = count(file2 , alpha );\n// Calculates\ntotal\nfrequency\ndifference\nfor (int i = 0; i < alpha.length; i++)\nfor (int j = 0; j < alpha.length; j++)\ndiff += Math.abs(bi1[i][j] - bi2[i][j]);\nSystem.out.println(diff );\nif (diff < 55) // Experimentally\ndetermined\nthreshold\nSystem.out.println(\"Same\nLanguage\");\nelse\nSystem.out.println(\"Different\nLanguage\");\n}\n// Create\nfrequency\ntables\npublic\nstatic\nfloat [][]\ncount(String file ,char [] alpha)\nthrows\nIOException { // 2-d frequency\ntable of all\npossible\nbigrams\nfloat\nbigram [][] = new float[alpha.length ][ alpha.length ];\nfor (int i = 0; i < alpha.length; i++)\nfor (int j = 0; j < alpha.length; j++)\nbigram[i][j] = 0; // Initialize\nwith\nfrequency =0\nint a = alpha.length +1, b = alpha.length +1;\nfloat\ntotal = 0; // Total\nnumber of bigrams\nScanner in = new\nScanner(new File(file+\".txt\"));\nwhile(in.hasNext(\"\\\\S+\")) { // Read file word by word\nPage 35 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nString\nword = in.next(\"\\\\S+\");\nword.toLowerCase ();\nfor (int k = 0; k < word.length ()-1; k++)\n{ // For each pair of letters\na = alpha.length +1;\nb = alpha.length +1;\nfor (int m = 0; m < alpha.length; m++)\n{ // Locates\neach\nletter\nwithin\nalpha\nlist\nif (word.charAt(k) == alpha[m])\na = m;\nif (word.charAt(k+1) == alpha[m])\nb = m;\n}\nif (a < alpha.length && b < alpha.length)\n{ // Adds\nvalid\nbigrams to frequency\nlist\nbigram[a][b]++;\ntotal ++;\n}\n}\n}\n// Create new file with\nfrequency\ntable\nPrintWriter\nout = new\nPrintWriter(new\nFileWriter(file+\"_tab.csv\"));\nfor (int p = 0; p < alpha.length; p++) {\nfor (int q = 0; q < alpha.length; q++) {\nbigram[p][q] = (bigram[p][q] / total) * 100;\n// Convert to decimal\nfrequency\nout.print(bigram[p][q]);\nout.print(\",\");\n}\nout.print(\"\\n\");\n}\nout.close ();\nreturn\nbigram;\n}\n// Read text from file\npublic\nstatic\nString\nread(String str) {\nString\nstr2 = \"\";\nBufferedReader\nread = new\nBufferedReader (new\nInputStreamReader (System.in));\ntry {\nstr2 = read.readLine ();\n} catch (IOException\nioe) {\nSystem.out.println(\"Error: Cannot\nRead\nInput\\n\");\nread(str);\n}\nreturn\nstr2;\n}\n}\nPage 36 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\nC\nSyntax Code\nTo begin, types must be created and initial input deﬁned, and the sentence must be divided\ninto its component words which are checked against known information:\n// Type\ndefinitions\nstruct\nword {\nstring\nword;\nstring\ntype;\n} list [50];\nstruct\npattern {\nint wcount;\nstring\npatt [10];\n} list2 [10];\n// Initial\ninput - First\nword must be \"NULL\" to avoid\nerrors\nlist [0]. word = \"NULL\";\nlist [0]. type = \"NULL\";\nlist [1]. word = \"has\";\nlist [1]. type = \"verb\";\nlist [2]. word = \"hat\";\nlist [2]. type = \"noun\";\nlist [3]. word = \"man\";\nlist [3]. type = \"noun\";\n// Split\nsentence\ninto\ncomponent\nwords\nfor (i = 0; i < sen.length (); i++) {\nif (sen.at(i) ==\n)\nsen_count ++;\n}\nfor (i = 0; i < sen.length (); i++) {\nif (sen.at(i) ==\n) {\nwords[pos] = sen.substr(temp ,i-temp );\ntemp = i+1;\npos ++;\n}\n}\n// Find\nwords in \"list\" array\nnum2 = num;\nfor (i = 0; i < sen_count; i++) { // For each word\nfor (j = 0; j < num; j++) {\nif (words[i] == list[j]. word) {\ninarray[i] = j; // For known words , track\nposition\n}\n}\nif (inarray[i] == 0) {\ninarray[i] = num2; // For\nunknown words , set new\nposition\nnum2 ++;\nPage 37 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n}\n}\nUnknown words are then deﬁned using either the word-context method:\n// For words not in \"list\" array , define\n// Define\nunknown\nwords by surrounding\nwords\nfor (i = 0; i < sen_count; i++) { // For each word\nif (inarray[i] >= num)\n{ // If unknown , position\nwill be greater\nthan num (number of words)\nlist[inarray[i]]. word = words[i];\nlist[inarray[i]]. type = \"\";\nif (i > 0) // If not the first\nword in the\nsentence\nlist[inarray[i]]. type =\nlist[inarray[i]]. type + \"a\" + list[inarray[i -1]]. type + \" \";\nif (i < sen_count -1) // If not the last word in the\nsentence\nlist[inarray[i]]. type =\nlist[inarray[i]]. type + \"b\" + list[inarray[i+1]]. type + \" \";\n}\nelse if (list[inarray[i]]. type != \"noun\" &&\nlist[inarray[i]]. type != \"verb\")\n{ // If known , but not a noun or verb , checks if should be redefined\ntype_temp = \"\";\nif (i > 0)\ntype_temp = type_temp + \"a\" + list[inarray[i -1]]. type + \" \";\nif (i < sen_count -1)\ntype_temp = type_temp + \"b\" + list[inarray[i+1]]. type + \" \";\nif (type_temp.length ()\n<= list[inarray[i]]. type.length ())\n// The\nshorter\none is the type\nlist[inarray[i]]. type = type_temp;\n}\n}\nnum = num2; // Reset\nknown\nnumber of words\n// Create new\nsentence\npattern\nlist2[runs ]. wcount = sen_count;\nfor (i = 0; i < sen_count; i++) { // For each word\nlist2[runs ]. patt[i] = list[inarray[i]]. type;\n}\nOr by the sentence pattern method:\n// Define\nunknown\nwords by sentence\nstructure\nfor (j = 0; j < 2; j++) { // For each\nexisting\npattern\nmatches = 0;\nfor (i = 0; i < sen_count; i++) { // For each word\nif (inarray[i] < num)\nif (list[inarray[i]]. type == list2[j]. patt[i])\nmatches ++;\nPage 38 of 39\nLanguage Acquisition in Computers\nMegan Belzner and Sean Colin-Ellerin\n}\nif (sen_count == list2[j]. wcount) // If word\ncounts\nmatch\nmatches ++;\nif (matches > sen_count /2)\n{ // If enough\nmatches exist , unknown\nwords are\ndefined\nfor (i = 0; i < sen_count; i++) {\nif (inarray[i] >= num) {\nlist[inarray[i]]. word = words[i];\nlist[inarray[i]]. type = list2[j]. patt[i];\n}\n}\n}\n}\nnum = num2; // Reset\nknown\nnumber of words\nPage 39 of 39\n",
  "categories": [
    "cs.CL",
    "I.2.6; I.2.7"
  ],
  "published": "2012-05-31",
  "updated": "2012-05-31"
}