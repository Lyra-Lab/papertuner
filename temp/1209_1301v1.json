{
  "id": "http://arxiv.org/abs/1209.1301v1",
  "title": "Evaluation of Computational Grammar Formalisms for Indian Languages",
  "authors": [
    "Nisheeth Joshi",
    "Iti Mathur"
  ],
  "abstract": "Natural Language Parsing has been the most prominent research area since the\ngenesis of Natural Language Processing. Probabilistic Parsers are being\ndeveloped to make the process of parser development much easier, accurate and\nfast. In Indian context, identification of which Computational Grammar\nFormalism is to be used is still a question which needs to be answered. In this\npaper we focus on this problem and try to analyze different formalisms for\nIndian languages.",
  "text": "Evaluation of Computational Grammar Formalisms for Indian Languages \n \n \nNisheeth Joshi [1], Iti Mathur [2] \n \n[1] [2] Department of Computer Science, Apaji Institute, Banasthali University, Rajasthan, India \nnisheeth.joshi@rediffmail.com [1], mathur_iti@rediffmail.com [2] \n \n \nABSTRACT \n \nNatural Language Parsing has been the most prominent \nresearch area since the genesis of Natural Language \nProcessing. Probabilistic Parsers are being developed to \nmake the process of parser development much easier, \naccurate and fast. In Indian context, identification of which \nComputational Grammar Formalism is to be used is still a \nquestion which needs to be answered. In this paper we \nfocus on this problem and try to analyze different \nformalisms for Indian languages. \nIndex Terms— Indian Languages, Computational \nGrammars, Linguistic Theories, Syntactic Structures, \nEvaluation1 \n \n1. INTRODUCTION \n \nNatural Language Parsing has been an important activity in \nNatural Language Processing (NLP) development. But, \neven since the introduction of machine learning techniques \ninto NLP application development, the scenario changed \ndrastically. This new approach appeared to be very \npromising, as it helped in rapid prototype development of \nNLP systems. In this technique, large amount of data was \nused, onto which various models like Hidden Markov \nModel (HMM), Conditional Random Fields (CRF), Neural \nNetworks (NN), Support Vector Machines (SVM) etc. were \napplied. These approaches are also termed as statistical \napproaches or Statistical Natural Language Processing \n(SNLP). This was a very effective way of application \ndevelopment, with applications attaining 60-75% accuracy \nwith very little effort. Unfortunately, this approach soon \nlost its shine as after a point of optimized performance, they \nbecome very less helpful in improvement of the systems[1]. \nMoreover, it failed to implement broad coverage parsing or \ndeep parsing. \n                                                \nProc. of International Conference in Computer Engineering \nand Technology, 2012, Organized by Jodhpur Institute of \nEngineering and Technology, Jodhpur. Sponsored by IEEE, \nUSA and Institution of Engineers (India), Kolkatta. \n \n \n \nDue to this reason, NLP researchers, in order to \nimprove performance of their systems, tried a new \napproach. They initially stared with a rule based \n(traditional) approach. This was called the seed data. Once \nthis was done, it was then supplied to machine learning \ntechniques. This approach was termed as hybrid approach \n(partially rule based and partially statistical). We can find \nevidence of improved systems in literature which used this \napproach[2][3]. This approach even helped in development \nof probabilistic parsers like Stanford parser[4], Charniak \nparser[5], MaltParser[6]. These all parsers where supplied \nwith different computational grammars formalisms or with \ntreebanks, which were developed using manually parsed \nsentences, based on one of the formalisms, for example \nPenn \nTreebank[7] \nTIGER \nTreebank[8] \nParaguay \nDependency Treebank[9]. In one or the other way, \ngrammar formalisms were used for development of deep \nparsers.  In this paper, we attempt to study the performance \nof some of the popular computational grammar formalism \ntechniques, which could be used in development of deep \nlanguage processing applications like a deep parser, \nmachine translators, semantic role labeler etc. \n \nThe motivation for this study came from the fact that \nfree word order is one of the areas were grammar \nformalism has not yet reached the level of good accuracy. \nIn this area there have been numerous claims to prove \nsuperiority \nof \ndependency \ngrammar \nover \nother \nformalisms[10][11]. But, often discussions based on this \nformalism ignore more practical aspects like usability and \nexpressivity.  In order to examine free word order approach, \nwe conducted our study on Hindi. Since all other Indian \nlanguages follow the same phenomena. The approach \nsuggested in this study can be applied to other languages. \n \n2. COMPUTATIONAL GRAMMAR FORMALISMS \n \nIn general computational grammars can be divided into \nthree categories based on their functionality. They are \nPhrase Structure Grammars, Dependency Grammars, and \nHybrid Grammars. A phrase structure grammar is the one \nwhich uses the approach shown by transformational \ngrammars where specific tree positions are associated with \nassignments of various syntactic roles, such as subject and \nobject. This concept is the motivation for having elements \nappear in various positions in the tree in the process of \nderiving the final syntactic structure. Some of the popular \nformalisms of this category are Tree Adjoining Grammar \n(TAG) [12], Head Driven Phrase Structure (HPSG) [13] \nGrammar. Hybrid grammar augments phrase structure \ngrammar by expressing non-projective syntactic relations, \nwhile maintaining a more formally defined architecture \nthen phrase structure grammar. Lexical Functional \nGrammar (LFG)[14] is an example of hybrid grammar. \n \nA dependency grammar consists of a set of words and a \nset of directed binary dependency relations between words, \nsuch that  \n \nNo words depends on itself \n \nEach dependent has one and only one head \n \nA head may have many dependents \n \nThere is one distinguished word which is the head of \nthe sentence and depends on no other word \n \nAll other words in a sentence are dependents such that \nthe whole sentence is connected. \n \nDependency \ngrammars \nhave \nbeen \nstudied \nby \nGaifman[15] who studied linear precedence in dependency \nrelations, Hudson[16] who introduced word grammar, \nStarosta [17] who studied lexicase grammar and Bharti et \nal[18] who showed the similarities between Paninian \nGrammar (PG) and dependency grammar and the \nsuitability of PG in Indian context. \n \n3. METHODOLODY \n \nIn order to understand the pros and corns of different \ngrammar formalism, we tested all three types of grammar \nformalisms. From phrase structure stable, we selected TAG, \nLFG from hybrid and PG from dependency framework. \n \nWe developed parallel grammars for all three \nframeworks and took a detailed note of development \nprocess and variations in syntactic structures. We recorded \ntime taken to construct each sentence, the total time taken \nto complete the task and the average time taken for the \ntask. We also noted the difficulty level with which each \ngrammar was developed. \n \nSince all three grammar formalism are somewhat \ndistinct in nature, it was very much necessary to develop a \nmechanism which would not be biased towards one \ngrammar and penalize others. To ensure the equivalence, \nwe tested each grammar using the same set of sentences. \nThe test case contained grammatical as well as \nungrammatical sentences. Each grammar was required to \ndistinguish between the two categories. Moreover each \ngrammar was required to provide predicate, arguments and \nmodifiers for each sentence which was parsed. Figure 1 and \n2 give a brief idea of the type of sentences used. Various \ntypes of sentences used in the test case were: \n \nBasic sentences with auxiliary verbs \n \nSentences having case assigning post positions \n \nSentences marking subjects \n \nAdpositional sentences \n \nSentences with generative constructions \n \nSentences with descriptive adjectives \n \nSentences with predicative adjectives \n \nSentences with relative/co-relative constructions \n \n(a) लडकȧ ने  लडके को  मारा \nladkii ne  ladke ko   mara \ngirl-Erg   boy-Acc   hit \n(b) मारा लडकȧ ने लडके को \nmara ladkii ne ladke ko \n(c) लडके को लडकȧ ने मारा \nladke ko ladkii ne mara \n(d) मारा लडके को लडकȧ ने \nmara ladke ko ladkii ne \n(e) लडके को मारा लडकȧ ने \nladke ko mara ladkii ne \n(f) लडकȧ ने मारा लडके को \nladkii ne mara ladke ko \n \nFigure 1: Simple Hindi Test Sentence with various variations [19] \n \n(a) जो  खड़ी         है   वो         लडकȧ लंबी    है \njo     khari       hai  vo          ladkii  lambii hai \nRel   standing  be   Co-Rel  girl      tall     is  \n(b) जो लडकȧ खड़ी है वो लंबी है \njo ladkii khari hai vo lambii hai \n(c) वो लडकȧ लंबी है जो खड़ी है \nvo ladkii khari hai jo lambii hai \n(d) * वो लडकȧ लंबी है जो लडकȧ खड़ी है \nvo ladkii lambi hai jo ladkii khari hai \n(e) * वो लंबी है जो लडकȧ खड़ी है \nvo lambi hai jo ladkii khari hai  \n(f) वो लडकȧ जो खड़ी है लंबी है \nvo ladkii jo khari hai lambii hai \n(g) * वो जो लडकȧ खड़ी है लंबी है \nvo jo ladkii khari hai lambii hai \n \nFigure 2: Simple Hindi Test Sentence with various variations [19] \n \nLFG and DG had no problems to handle these type of \nsentences. We used Lexicalized TAG (LTAG) which is the \nmodification of TAG and can handle word order variation.  \n \nWe did this study using ten grammar writes, which \nwere provided with the sentences and were asked to \nconstruct the grammars for each sentence. In order to \nunderstand the usability of each grammar, we provided \neach writer with 35 sentences from various categories, as \ndiscussed above. Each writer was provided with a short \ntutorial of each grammar. Shortly after the tutorial of a \nparticular grammar, the writers were asked to implement \nthe sentences for the said grammar. Their performance was \ncalculated on the measure discussed above. \n \nWriter \nPG \nTAG \nLFG \nDur.\nAcc.\nDur.\nAcc.\nDur.\nAcc.\nW1 \n9 min \n74% \n28 \nmin \n71% \n41 \nmin \n61% \nW2 \n8 min \n80% \n31 \nmin \n84% \n43 \nmin \n73% \nW3 \n8 min \n79% \n27 \nmin \n83% \n48 \nmin \n64% \nW4 \n10 \nmin \n60% \n35 \nmin \n88% \n56 \nmin \n66% \nW5 \n13 \nmin \n98% \n36 \nmin \n88% \n61 \nmin \n73% \nW5 \n17 \nmin \n93% \n37 \nmin \n94% \n67 \nmin \n71% \nW7 \n9 min \n83% \n38 \nmin\n97% \n71 \nmin\n79 % \nW8 \n11 \nmin \n87% \n39 \nmin \n93% \n59 \nmin \n67% \nW9 \n11 \nmin\n95% \n35 \nmin\n89% \n55 \nmin\n69% \nW10 \n7 min \n70% \n34 \nmin \n73% \n59 \nmin \n62% \nAverage \n10.3 \nmin\n81.9\n%\n34 \nmin\n86.0\n%\n56 \nmin\n68.5\n%\nTable 1: Total Time Taken and Accuracy Achieved by Each \nGrammar Writer \n \nTask \nDifficulty \nPG\nTAG\nLFG\nBasic Sentences \n1 (8) \n2 (7) \n3(9) \nAuxiliary Verbs \n3 (9) \n4 (8) \n2 (9) \nCase Assigning PPs \n4 (7) \n3 (8) \n4 (6) \nAdpositional Sentences\n4 (6)\n3 (9)\n4 (7)\nDescriptive Adjectives \n3 (8) \n2 (7) \n4 (7) \nGenitive Case \n2 (9) \n3 (7) \n4 (6) \nPredictive Adjectives \n2 (8) \n3 (7) \n4 (7) \nRelative Clause\n5 (8)\n4 (7)\n5 (8)\nTable 2: Highest Voted Ranks by Grammar Writers for each \ngrammar \n \n \n4. RESULTS \n \nWe calculated the results for total and average time taken to \ncomplete the task, accuracy with which the task was \ncompleted, difficulty ratings provided by each writer, for \neach formalism, on different categories of sentences. The \ntypes of errors committed. The results of the study are \nprovided in the following sections. \n \n4.1 Time Taken and Accuracy \n \nTable 1 summarizes the average time taken to complete the \ntask by each writer and the accuracy with which they did it. \nLooking at the data, it is clearly seen that time taken to \ncomplete the task was least in PG and most in LFG, TAG \nwas in between the two.  \n \nThe average time taken by the writers to complete the \ntask for PG, TAG and LFG is 10.3 min, 34 min and 56 min \nrespectively. PG took least time with which the sentences \nwere completed. We also measured average accuracy of \neach writer. Here TAG scored more accuracy then the other \nformalisms. \n \n4.2 Difficulty Rating \n \nAfter the task, we provided the questionnaire to the writers. \nWe asked them to provide us with the difficulty rating for \neach type of sentence, for each grammar. We asked them to \nranks the difficulty of sentences between 1 and 5, where 1 \nbeing the easiest and 5 being the toughest. As it was not \npossible to provide results for all the writes here. In Table \n2, we provide the ranks given majority of writers. The sores \nwithout brackets are the ranks given and the ones in bracket \nare the no. of writers who gave this rank.  \n \nWe can see that PG sores very well in simple, \ngenerative and predictive adjective cases, but do not \nperform well on other categories of the sentences. TAG on \nthe other hand performs moderately well. It sores highest in \nfour categories of sentences. LFG scores highest in just one. \n \n4.3 Error Analysis \n \nWe also examined the types of errors committed by \ndifferent formalisms, as we wanted to know, why writer had \ngreat difficulty with LFG as compared to PG or TAG.  \n \nIn Paninian Grammar, we analyzed that writers faced \ngreat difficulty in assigning relationships to dependency \nstructures. This could be due to the notational convention of \nthe formalism or due to the difficulty with the concepts of \nhead and dependents. We also saw that whenever a \ndirectional error was made, the correct rule was framed for \nimplementing the dependency. This shows that the \ndifficulty was with the notion and not with the concepts. \nMoreover PG although being least rigorous out of the three, \nshowed some sluggishness while dealing with complex \nsentences. \n \nIn Tree Adjoining Grammar, we saw that most errors \nwere in the formation of the derived trees. Between \nadjunction and substitution operations, adjunction proved to \nbe more error prone. Almost 80% errors were made due to \nincorrect adjunction operation. This shows that writers had \ngreat difficulty understanding the adjunction operation.  \n \nIn Lexical Functional Grammar, we saw that grammar \nwriter’s had great difficulty in associating features with \nconstituent structures. In some cases the writers got \nconfused as to use noun phrase and verb phrase in \nconstituent structure or to use subject and predicate in \nfeature structure. Although this formalism is the most \nperfect in terms of linguistic phenomena as it captures all \nthe aspects of the language’s grammar, it is also fairly \ndifficulty to understand, as it takes time for the grammar \nwrite to understand and implement grammar using it. \n \n5. CONCLUSION \n \nWe wanted to study the applicability of different grammar \nformalism on Indian languages, so that different NLP tasks \nlike development of a deep probabilistic parser or \ndevelopment of a Treebank could be under taken. In doing \nso, we gathered insights into the different formalisms and \nunderstood the merits and demerits of each. \n \nWe found out that though Paninain Grammar was \npreferred for simple sentences, overall performance of TAG \nwas good. It scored better in the average accuracy attained \nto write the sentences. Although this is a preliminary study \nand more in-depth evaluations are required before making \nany sound conclusions. But, with some confidence we can \nsay that TAG can perform better in most of the difficult \ncases as compared to dependency grammar. \n \n6. REFERENCES \n \n[1]. \nJ.G. Neal, E.L. Feit and C.A. Montgomery, “Benchmark \nInvestigation/Identification \nProject,” \nMachine \nTranslation, Springer, Germany, Vol 8, No. 1-2, pp77-84, \n1993. \n[2]. \nT. Baldwin, J. Beavers, E.M. Bender, D. Flickinger, A. \nKim, and S. Oepen, “Beauty and the beast: What running \na broad-coverage precision grammar over thee bnc taught \nus about the grammar — and the corpus,” Linguistic \nEvidence: Empirical, Theoretical, and Computational \nPerspectives, Mouton de Gruyter, Berlin, Germany, pp \n49–70., 2005. \n[3]. \nS. A. Waterman, “Distributed parse mining,” In \nProceedings of the NAACL Workshop on Software \nEngineering, Testing, and Quality Assurance for Natural \nLanguage Processing, USA, 2009. \n[4]. \nC.D. Manning and H Schütze, “Foundations of Statistical \nNatural Language Processing”, MIT Press, USA, 1999. \n[5]. \nE. Charniak, “A Maximum Entropy Inspired Parser”, In \nProceedings of NAACL, USA, 2000. \n[6]. \nJ. Nivre, J. Hall and J. Nilsson, “MaltParser: A Data-\nDriven Parser-Generator for Dependency Parsing,” \nIn Proceedings of the fifth international conference on \nLanguage Resources and Evaluation, Genoa, Italy, pp. \n2216-2219, May, 2006.  \n[7]. \nA Taylor, A. Warner and B. Santorini, “The Penn \nTreebank: An Overview”, Treebanks: Building and Using \nParsed \nCorpora, \nKluwer \nAcademic \nPublishers, \nNetherlands, 2003. \n[8]. \nS. Brants, S. Dipper, P. Eisenberg, S. Hansen, E. König, \nW. Lezius, C. Rohrer, G. Smith, and H. Uszkoreit, \n“TIGER: Linguistic interpretation of a German corpus,” \nResearch on Language and Computation, Springer, \nGermany, Vol 9, No. 2, pp 597-620, 2004. \n[9]. \nJ Hajič, B Hladká and P. Pajas, “ The Prague \nDependency \nTreebank: \nAnnotation \nStructure \nand \nSupport,” In Proceedings of the IRCS Workshop on \nLinguistic Databases, Pennsylvania, USA, pp 105-114, \n2001. \n[10]. \nM. Covington, “Parsing Discontinuous Constituents \nDependency in Dependency Grammar,” Computational \nLinguistics, MIT Press, USA, Vol 16, No. 4, pp-234-236, \n1990 \n[11]. \nR. Sangal and V Chaitanya, “An Intermediate Language \nfor Machine Translation: An approach based on Sanskrit \nusing conceptual graph notation”, Journal of Computer \nSociety of India, Mumbai, India, Vol 17, pp 9-21, 1987. \n[12]. \n A.K. Joshi, “An Introduction to Tree Adjoining \nGrammars,” Mathematics of Language, John Benjamins, \nNetherlands, 1987.  \n[13]. \nI.A. Sag, T Wasow and E.M. Bender, “Syntactic Theory,” \n2 Edition, CSLI Publications, USA, 2001. \n[14]. \nM. Dalrymple, “Lexical Functional Grammar: Syntax and \nSemantics”, Academic Press, USA, 2001. \n[15]. \nH. Gaifman, “Dependency systems and phrase structure \nsystems,” Information and Control, USA, Vol 8, pp 304-\n337, 1965. \n[16]. \nJ. Hudson, “Word Grammar,” Basil Blackwell, England, \n1984. \n[17]. \nS. Starosta, “The Case for Lexicase: An Outline of \nLexicase Grammatical Theory,” Cassell, London, 1988. \n[18]. \nA. Bharti, V. Chaitanya, R. Sangal, “Natural Language \nProcessing: A Paninian Perspective,” PHI, India, 1999. \n[19]. \nV. \nDwivedi, \n“Tropicalization \nin \nHindi \nand the \nCorrelative Construction,” Theoretical Perspectives on \nWord \nOrder \nin \nSouth \nAsian \nLanguages, \nCSLI \nPublications, USA, 1994. \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2012-08-19",
  "updated": "2012-08-19"
}