{
  "id": "http://arxiv.org/abs/1212.2145v1",
  "title": "A Scale-Space Theory for Text",
  "authors": [
    "Shuang-Hong Yang"
  ],
  "abstract": "Scale-space theory has been established primarily by the computer vision and\nsignal processing communities as a well-founded and promising framework for\nmulti-scale processing of signals (e.g., images). By embedding an original\nsignal into a family of gradually coarsen signals parameterized with a\ncontinuous scale parameter, it provides a formal framework to capture the\nstructure of a signal at different scales in a consistent way. In this paper,\nwe present a scale space theory for text by integrating semantic and spatial\nfilters, and demonstrate how natural language documents can be understood,\nprocessed and analyzed at multiple resolutions, and how this scale-space\nrepresentation can be used to facilitate a variety of NLP and text analysis\ntasks.",
  "text": "arXiv:1212.2145v1  [cs.IR]  10 Dec 2012\nA Scale-Space Theory for Text∗\nShuang Hong Yang\nCollege of Computing\nGeorgia Tech\nshy@gatech.edu\nAbstract\nScale-space theory has been established pri-\nmarily by the computer vision and signal pro-\ncessing communities as a well-founded and\npromising framework for multi-scale process-\ning of signals (e.g., images). By embedding\nan original signal into a family of gradually\ncoarsen signals parameterized with a contin-\nuous scale parameter, it provides a formal\nframework to capture the structure of a signal\nat different scales in a consistent way. In this\npaper, we present a scale space theory for text\nby integrating semantic and spatial ﬁlters, and\ndemonstrate how natural language documents\ncan be understood, processed and analyzed at\nmultiple resolutions, and how this scale-space\nrepresentation can be used to facilitate a vari-\nety of NLP and text analysis tasks.\n1\nIntroduction\nPhysical objects in the world appear differently\ndepending on the scale of observation/measurement.\nTake the tree as an example, meaningful obser-\nvations range from molecules at the scale of\nnanometers, to leaves at centimeters, to branches at\nmeters, and to forest at kilometers. This inherent\nproperty is ubiquitous and holds equally true for\nnatural language.\nOn the one hand, concepts are\nmeaningful only at the right resolution, for instance,\nnamed entities usually range from unigram (e.g.,\n“new”) to bigram (e.g., “New York”), to multigram\n(e.g., “New York Times”), and even to a whole\nlong sequence (e.g., a song name “ Another Lonely\nNight In New York”). On the other hand, our under-\nstanding of natural language depends critically on\nthe scale at which it is examined, for example, de-\npending on how much detailed we would like to get\n∗1st submitted: Jan. 15 2012; revised: Mar. 28 2012.\ninto a document, our knowledge could range from\na collection of “keywords”, to a sentence sketch\nnamed “title”, to a paragraph summary named\n“abstract”, to a page long “introduction” and ﬁnally\nto the entire content. The notion of scale is funda-\nmental to the understanding of natural language, yet\nit was largely ignored by existing models for text\nrepresentation, which include simple bag-of-word\n(BOW) or unigram language model (LM), n-gram\nor higher order LMs, and other more advanced\ntext/language\nmodels\n(Iyer and Ostendorf, 1996;\nManning and Schuetze, 1999;\nMetzler and Croft, 2005).\nOne\nkey\nproblem\nwith many of these models is their inﬂexibility —\nthey capture the semantic structure rather rigidly\nat only a single resolution (e.g., n-gram with a\nsingle ﬁxed value of n). However, which scale is\nappropriate for a speciﬁc task is usually unknown\na priori and in many cases even not homogeneous\n(e.g., a document may contain named entities of\ndifferent length), making it impossible to capture\nthe right meanings with a ﬁxed single scale.\nScale space theory is a well-established and\npromising framework for multi-resolution represen-\ntation, developed primarily by the computer vision\nand signal processing communities with compli-\nmentary motivations from physics and bio-vision.\nThe key idea is to embed a signal into the scale\nspace, i.e., to represent it as a family of progres-\nsively smoothed signals parameterized by a continu-\nous variable of scale, where ﬁne-resolution detailed\nstructures are progressively suppressed by the con-\nvolution of the original signal with a smoothing ker-\nnel (i.e., a low pass ﬁlter with certain properties)\n(Witkin, 1983; Lindeberg, 1994).\nIn this paper, we adapt the scale-space model\nfrom image to text signals, proposing a novel frame-\nwork that enables multi-resolution representation for\ndocuments. The adaptation poses substantial chal-\nlenges as the structure of the semantic domain is\nnontrivially complicated than the spatial domains in\ntraditional image scale space.\nWe show how this\ncan be made possible with a set of assumptions and\nsimpliﬁcations. The scale-space model for text not\nonly provides new perspectives for how text analy-\nsis tasks can be formulated and addressed, but also\nenables well-established computer vision tools to be\nadapted and applied to text processing, e.g., match-\ning, segmentation, description, interests points de-\ntection, and classiﬁcation. To stimulate further in-\nvestigation in this promising direction, we initiate\na couple of instantiations to demonstrate how this\nmodel can be used in a variety of NLP and text anal-\nysis tasks to make things easier, better, and most im-\nportantly, scale-invariant.\n2\nScale Space Representation\nThe notion of scale space is applicable to signals of\narbitrary dimensions. Let us consider the most com-\nmon case, where it is applied to 2-dimensional sig-\nnals such as images. Given an image f(x1, x2), its\nscale-space representation γ(x1, x2, s) is deﬁned by:\nγ(x1, x2, s) = f(x1, x2) ∗ℓ(x1, x2, s)\n(1)\n=\nZ\nR2 f(x1 −u1, x2 −u2)ℓ(u1, u2, s)du1du2,\nwhere ∗denotes the convolution operator, and ℓ:\nR2×R+ →R is a smoothing kernel (i.e., a low pass\nﬁlter) with a set of desired properties (i.e., the scale-\nspace axioms (Lindeberg, 1994)).\nThe bandwidth\nparameter s is referred to as scale parameter since\nas s increases, the derived image will become grad-\nually smoother (i.e., blurred) and consequently more\nand more ﬁne-scale structures will be suppressed.\nIt has been shown that the Gaussian kernel is the\nunique option that satisﬁes the conditions for linear\nscale space:\nℓ(x, s) =\n1\n√\n2πs\ne−(x2\n1+x2\n2)/2s.\n(2)\nThe resultant linear scale space representation\nγ(x1, x2, s) can be obtained equivalently as a solu-\ntion to the diffusion (heat) equation\n∂sγ = 1\n2△γ\n(3)\nwith initial condition γ(x, 0) = f(x), where △de-\nnotes the Laplace operator which in a 2-dimensional\nspatial space corresponds to\n∂2\n∂x2\n1 + ∂2\n∂x2\n2. If we view\nγ as a heat distribution, the equation essentially de-\nscribes how it diffuses from initial value, f, in a ho-\nmogeneous media with uniform conductivity over\ntime s.\nAs we can imagine, the distribution will\ngradually approach uniform and consequently the\nﬁne-scale structure of f will be lost.\nScale-space theory provides a formal framework\nfor handling the multi-scale nature of both the phys-\nical world and the human perception. Since its in-\ntroduction in 1980s, it has become the foundation of\nmany computer vision techniques and been widely\napplied to a large variety of vision/image processing\ntasks. In this paper, we show how this powerful tool\ncan be adapted and applied to natural language texts.\n3\nScale Space Model for Text\n3.1\nWord-level 2D Image Analogy of Text\nA straightforward step towards textual sale space\nwould be to represent texts in the way as image sig-\nnal. In this section, we show how this can be made\npossible. Other alternative signal formulations will\nbe discussed in the followed section.\nLet V = {v1, v2, . . . , vM} be our vocabulary con-\nsisting of M words, given a document d comprised\nof a ﬁnite N-word sequence d = w1w2 . . . wN,\nwithout any information loss, we can characterize d\nas a 2D N × M binary matrix f, with the (x, y)-\nth entry f(x, y) indicates whether or not the y-th\nvocabulary word vy is observed at the x-th posi-\ntion, i.e.: f(x, y) = δ(wx, vy), where δ(a, b) = 1\nif a = b and 0 otherwise. Hereafter, we will re-\nfer to the x-axis as spatial domain (i.e., positions in\nthe document, x ∈X = {1, . . . , N}), and y-axis\nas the semantic axis (i.e., indices in the vocabulary,\ny ∈Y = V). This representation provides an image\nanalogy to text, i.e., a document f is equivalent to a\nblack-and-white image except that here we have one\nspatial and one semantic domains, (x, y), instead of\ntwo spatial domains, (x1, x2).\nInterestingly, scale-space representation can also\nbe motivated by this binary model from a slightly\ndifferent perspective, as a way of robust density es-\ntimation. We have the following deﬁnition:\nDEFINITION 1.\nA 2D text model f ∈RN×M\n+\nis\na probabilistic distribution over the joint spatial-\nsemantic space: X × Y →R+, 0 ⩽f(x, y) ⩽1,\nR\nx\nR\ny f(x, y)dxdy = 1.\nThis 2D text model deﬁnes the probability of ob-\nserving a semantic word y at a spatial position x.\nThe binary matrix representation (after normaliza-\ntion) can be understood as an estimation of f with\nkernel density estimators:\nf(x, ·) = 1\nN\nXN\ni=1 δ(wx −wi)e⊤\nx ,\n(4)\nf(·, y) = 1\nM\nXM\nj=1 δ(vy −vj)ey,\n(5)\nwhere ei is the i-th column vector of an iden-\ntity matrix, f(x, ·) denotes the x-th row vector\nand f(·, y) the y-th column vector.\nNote that\nhere the Dirac impulse kernels δ is used, i.e.,\nwords are unrelated either spatially or semanti-\ncally.\nThis contradicts the common knowledge\nsince neighboring words in text are highly corre-\nlated both semantically (Mei et al., 2008) and spa-\ntially (Lebanon et al., 2007). For instance, observ-\ning the word “New” indicates a high likelihood\nof seeing the other word “York” at the next posi-\ntion.\nAs a result, it motivates more reliable esti-\nmate of f by using smooth kernels such as Gaussian\n(Witkin, 1983; Lindeberg, 1994), which, as we will\nsee, leads exactly to the Gaussian ﬁltering used in\nthe linear scale-space theory.\n3.2\nTextual Signals\nThe 2D binary matrix described above is not the only\noption we can work with in scale space. Generally\nspeaking, any vector, matrix or even tensor represen-\ntation of a document can be used as a signal upon\nwhich scale space ﬁltering can be applied. In partic-\nular, we use the following in the current paper:\n• Word-level 2D signal, f(x, y), is the binary ma-\ntrix we described in §3.1. It records the spatial\nposition for each word, and is deﬁned on the\njoint spatial-semantic domains.\n• Bag-of-word 1D signal is the BOW represen-\ntation f(y) = P\nx f(x, y), i.e., the 2D matrix\nis collapsed to a 1D vector. Since the spatial\naxis is wiped out, this signal is deﬁned on the\nsemantic domain alone.\n• Sentence-level 2D signal is a compromise be-\ntween word-level 2D and the BOW signals. In-\nstead of collapsing the spatial dimension for the\nwhole document, we do it for each sentence.\nAs a result, this signal, f(x, y), records the po-\nsition of each sentence; for a ﬁxed position x0,\nf(x = x0, y) records the BOW of the corre-\nsponding sentence.\n• Topic 1D signal, f(x), is composed of the topic\nembedding of each sentence and deﬁned on the\nspatial domain only. Assume we have trained\na topic model (e.g., Latent Dirichlet Alloca-\ntion) on a universal corpus in advance, this sig-\nnal is obtained by applying topic inference to\neach sentence and recording the topic embed-\nding θx ∈Rk, where k ≪M is the dimen-\nsionality of the topic space. Topic embedding\nis beneﬁcial since it endows us the ability to ad-\ndress synonyms and polysemy. Also note that\nthe semantic correlation may have been elimi-\nnated and consequently semantic smoothing is\nno longer necessary. In other words, although\nf(x) is a matrix, we would rather treat it as a\nvector-variate 1D signal.\nAll these textual signals involve either a semantic\ndomain or both semantic and spatial domains. In the\nfollowing, we investigate how scale-space ﬁltering\ncan be applied to these domains respectively.\n3.3\nSpatial Filtering\nSpatial ﬁltering has long been popularized in signal\nprocessing\n(Witkin, 1983;\nLindeberg, 1994),\nand\nwas\nrecently\nexplored\nin\nNLP\nby\n(Lebanon et al., 2007;\nYang and Zha, 2010).\nIt\ncan be achieved by convolution of the signal with a\nlow-pass spatial ﬁlter, i.e., γ(x, s) = f(x) ∗ℓ(x, s).\nFor texts, this amounts to borrowing the occurrence\nof words at one position from its neighbor-\ning positions, similar to what was done by a\ncache-based language model (Jelinek et al., 1991;\nBeeferman et al., 1999).\nIn order not to introduce spurious information, the\nﬁlter ℓneed to satisfy a set of scale-space axioms\n(Lindeberg, 1994). If we view the positions in a text\nas a spatial domain, the Gaussian kernel ℓ(x, s) =\n1\n√\n2πs exp(−x2/2s) or its discrete counterpart\nℓ(n, s) = e−sIn(s)\n(6)\nare singled out as the unique options that satisfy\nthe set of axioms1 leading to the linear scale space,\nwhere In(t) denotes the modiﬁed Bessel functions\nof integer order. Alternatively, if we view the po-\nsition x as a time variable as in the Markov lan-\nguage models, a Poisson kernel ℓ(n, s) = e−ssn/n!\nis more appropriate as it retains temporal causality\n(i.e., inaccessibility of future data).\n3.4\nSemantic Filtering\nSemantic ﬁltering attempts to smooth the probabil-\nities of seeing words that are semantically corre-\nlated.\nIn contrast to the spatial domain, the se-\nmantic domain has some unique properties.\nThe\nﬁrst thing we notice is that, as semantic coordi-\nnates are nothing but indices to the dictionary, we\ncan permute them without changing the seman-\ntic meaning of the representation.\nWe refer to\nthis property as permutation invariance. Semantic\nsmoothing has been extensively explored in natural\nlanguage processing (Manning and Schuetze, 1999;\nZhai and Lafferty, 2004).\nClassical\nsmoothing\nmethods, e.g., Laplacian and Dirichlet smoother,\nusually shrink the original distributions to a prede-\nﬁned reference distribution.\nRecent advances ex-\nplored local smoothing where correlated words are\nsmoothed according to their interrelations deﬁned\nby a semantic network (Mei et al., 2008).\nGiven a semantic graph Gv, where two correlated\nwords vy and vz are connected with weight µyz, se-\nmantic smoothing can be formulated as solving a\ngraph-based optimization problem:\nmin\nγ\n(1 −λ)\nXM\ny=1 µy(γy −fy)2\n+ λ\nXM\ny=1\nXM\nz=1 µyz(γy −γz)2,\n(7)\nwhere 0 ⩽λ ⩽1 deﬁnes the tradeoff, µy weights\nthe importance of the node vy. Interestingly, the so-\nlution to Eqn.(7) is simply the convolution of the\noriginal signal with a speciﬁc kernel2, i.e., γ = f ∗ℓ.\n1Including linearity, shift-invariance, semi-group structure,\nnon-enhancement of local extrema (i.e., monotonicity), scale-\ninvariance, etc.; see (Lindeberg, 1994) for details and proofs.\n2This can be proven by the ﬁrst-order optimality of Eq(7).\nCompared with spatial ﬁltering, semantic ﬁlter-\ning is, however, more challenging.\nIn particular,\nthe semantic domain is heterogeneous and not shift-\ninvariant — the degree of correlation µyz depends on\nboth coordinates y and z rather than their difference\n(y −z). As a result, kernels that provably satisfy\nscale-space axioms are no longer feasible. To this\nend, we simply set aside these requirements and de-\nﬁne kernels in terms of the dissimilarity dyz between\na pair of words y and z rather than their direct differ-\nence (y −z), that is, ℓy(y, z; s) = ℓx(dyz, s), where\nwe use ℓy to denote semantic kernel to distinguish\nfrom spatial kernels ℓx. For Gaussian, this means\nℓy(y, z; s) =\n1\n√\n2πs exp(−d2\nyz/2s).\n3.5\nText Scale Space\nScale is vital for the understanding of natural lan-\nguage, yet it is nontrivial to determine which scale is\nappropriate for a speciﬁc task at hand in advance. As\na matter of fact, the best choice usually varies from\ntask to task and from document to document. Even\nwithin one document, it could be heterogeneous,\nvarying from paragraph to paragraph and sentence\nto sentence. For the purpose of automatic modeling,\nthere is no way to decide a priori which scale ﬁts\nthe best. More importantly, it might be impossible\nto capture all the right meanings at a single scale.\nTherefore, the only reasonable way is to simulta-\nneously represent the document at multiple scales,\nwhich is exactly the notion of scale space.\nScale space representation embeds a textual sig-\nnal into a continuous scale-space, i.e., by a family\nof progressively smoothed signals parameterized by\ncontinuous scale parameters. In particular, for a 2D\ntextual signal f(x, y), we have:\nγ(x, y; sx, sy) = f(x, y) ∗ℓ(x, y; sx, sy),\n(8)\nwhere the 2D smoothing kernel ℓis separable be-\ntween spatial and semantic domains, i.e.,\nℓ(x, y; sx, sy) = ℓx(x, sx)ℓy(y, sy).\n(9)\nNote that we have two continuous scale parameters,\nthe spatial scale sx ∈R+ and the semantic scale\nsy ∈R+. The case for 1D signals are even simpler\nas they only involve one type of kernels (spatial or\nsemantic). For a 1D spatial signal f(x), we have ℓ=\nℓx, and for a semantic signal f(y), ℓ= ℓy. And if\nFigure 1: Samples from the scale space representation of\nthe example text “New York Times offers free iPhone 3G\nas gifts for new customers in New York” at scales (from\nleft to right): s = (0, 0), (1, 1), (4, 4), and (64, 64).\nf is a vector-variate signal, we just apply smoothing\nto each of its dimensions independently.\nExample. As an example, Figure 1 shows four sam-\nples, {γ(x, y; s = si), i = 1, 2, 3, 4}, from the\nscale-space representation γ(x, y; s) of a synthetic\nshort text “New York Times offers free iPhone 3G\nas gifts for new customers in New York”, where s =\n(sx, sy), the two scales are set equal sx = sy for ease\nof explanation and γ is obtained based on the word-\nlevel 2D signal.\nWe use a vocabulary containing\n12 words (in order): “new”, “york”, “time”, “free”,\n“iPhone”, “gift”, “customer”, “apple”, “egg”, “city”,\n“service” and “coupon”, where the last four words\nare chosen because of their strong correlations with\nthose words that appear in this text. The semantic\ngraph is constructed based on pairwise mutual in-\nformation scores estimated on the RCV1-V2 corpus\nas well as a large set of Web search queries. The\n(0,0)-scale sample, or the original signal, is a 12×10\nbinary matrix, recording precisely which word ap-\npears at which position. The smoothed signals at\n(1,1), (2,2) and (8,8)-scales, on the other hand, cap-\nture not only short-range spatial correlations such as\nbi-gram, tri-gram and even higher orders (e.g., the\nnamed entities “New York” and “New York Times”),\nbut also long-range semantic dependencies as they\nprogressively boost the probability of latent but se-\nmantically related topics, e.g., “iPhone” →“ap-\nple”, “customer” →“service”, “free” and “gift” →\n“coupon”, “new” and “iPhone” →“egg” (due to the\nonline electronics store newegg.com).\n4\nScale Space Applications\nThe scale-space representation creates a new dimen-\nsion for text analysis. Besides providing a multi-\nscale representation that allows texts to be analyzed\nin a scale-invariant fashion, it also enables well-\nestablished computer vision tools to be adapted and\napplied to analyzing texts. The scale space model\ncan be used in NLP and text mining in a variety of\nways. To stimulate further research in this direction,\nwe initiate a couple of instantiations.\n4.1\nScale-Invariant Text Classiﬁcation\nIn this section, we show how to make text classi-\nﬁcation scale-invariant by exploring the notion of\nscale-invariant text kernel (SITK). Given a pair of\ndocuments, d and d′, at any ﬁxed scale s, the repre-\nsentation γ induces a single-scale kernel ks(d, d′) =\n⟨γs, γ′\ns⟩, where ⟨·, ·⟩denotes any inner product\n(e.g., Frobenius product, Gaussian RBF similarity,\nJensen-Shannon divergence).\nThis kernel can be\nmade scale-invariant via the expectation:\nk(d, d′) = Eq[ks(d, d′)] =\nZ ∞\n0\nks(d, d′)q(s)ds,\n(10)\nwhere q is a probabilistic density over the scale\nspace R+ with 0 ⩽q(s) ⩽1 and\nR ∞\n0 q(s)ds = 1,\nwhich in essence characterizes the distribution of the\nmost appropriate scale. q can be learned from data\nvia a EM procedure or in a Bayesian framework if\nour belief about the scale can be encoded into a prior\ndistribution q0(s). As an example, we show below\none possible formulation.\nGiven a training corpus D = {di, yi}n\ni=1, where d\nis a document and y its label, our goal in text clas-\nsiﬁcation is to minimize the expected classiﬁcation\nerror. To simplify matters, we assume a paramet-\nric form for q. Particularly, we use the Gamma dis-\ntribution q(s; k, θ) = θksk−1e−θs/Γ(k) due to its\nﬂexibility. Moreover, we propose a formulation that\neliminates the dependence on the choice of the clas-\nsiﬁer, which approximately minimizes the Bayes er-\nror rate (Yang and Hu, 2008) , i.e.:\nmaxk,θ\nXn\ni=1 Eq[hi(s)]\n(11)\nwhere hi(s) = △s(di, dm\ni ) −△s(di, dh\ni ) is a heuris-\ntic margin; dh\ni , called “nearest-hit”, is the nearest\nneighbor of di with the same class label, whereas\ndm\ni , the “nearest-miss”, is the nearest neighbor of di\nwith a different label, and the distance △s(d, d′) =\np\nks(d, d) + ks(d′, d′) −2ks(d, d′).\nThis above\nformulation can be solved via a EM procedure. Al-\nternatively, we can discretize the scale space (prefer-\nably in log-scale), i.e., S = {s1, . . . , sm}, and opti-\nmize a discrete distribution qj = q(sj) directly from\nthe same formulation. In particular, if we regularize\nthe ℓ2-norm of q, Eq(11) will become a convex opti-\nmization with a close-form solution that is extremely\nefﬁcient to obtain:\nq = (¯h)+/||(¯h)+||\n(12)\nwhere q = [q1, . . . , qm]⊤, the average margin vector\n¯h = [¯h1, . . . , ¯hm]⊤with entry ¯hj = 1\nn\nPn\ni=1 hi(sj),\nand (·)+ denotes the positive-part operator.\nExperiments.\nWe test the scale-invariant text\nkernels (SITK) on the RCV1-v2 corpus with fo-\ncus on the 161,311 documents from ten leaf-\nnode topics: C11, C24, C42, E211, E512,\nGJOB, GPRO, M12, M131 and M142.\nEach\ntext is stop-worded and stemmed.\nThe top 20K\nwords with the highest DFs (document frequencies)\nare selected as vocabulary; all other words are dis-\ncarded. The semantic network is constructed based\non pairwise mutual information scores estimated on\nthe whole RCV1 corpus as well as a large scale\nrepository of web search queries, and further spar-\nsiﬁed with a cut-off threshold. We implemented the\nsentence-level 2D, the LDA 1D signals and BOW\n1D for this task. For the ﬁrst two, the documents are\nnormalized to the length of the longest one in the\ncorpus via bi-linear interpolation.\nWe examined the classiﬁcation performance of\nthe SVM classiﬁers that are trained on the one-vs-\nall splits of the training data, where three types of\nkernels (i.e., linear (Frobenius), RBF Gaussian and\nJensen-Shannon kernels) were considered. The av-\nerage test accuracy (i.e., Micro-averaged F1) scores\nare reported in Table 1.\nAs a reference, the re-\nsults by BOW representations with TF or TFIDF at-\ntributes are also included. For all the three kernel\noptions, the scale-space based SITK models signiﬁ-\ncantly (according to t-test at 0.01 level) outperform\nthe two BOW baselines, while the sentence level\nSITK performs substantially the best with 7.8% ac-\ncuracy improvement (i.e., 56% error reduction).\n4.2\nScale-Invariant Document Retrieval\nCapturing users’ information need from their in-\nput queries is crucially important to information re-\ntrieval, yet notoriously challenging because the in-\nformation conveyed by a short query is far more\nvague and subtle than a BOW model can capture. It\nTable 1: Text classiﬁcation test accuracy. We compared\nﬁve models: the bag-of-word vector space models with\nTF or TFIDF attributes, and the scale-invariant text ker-\nnels with BOW 1D (SITK.BOW), LDA 1D (SITK.LDA)\nand Sentence-level 2D (SITK.Sentence) textual signal.\nBest results are highlighted in bold.\nModel\\Kernel\nLinear\nRBF\nJ-S\nTF\n0.8789\n0.9087\n0.8901\nTFIDF\n0.8821\n0.9099\n0.9016\nSITK.BOW\n0.8917\n0.9143\n0.9076\nSITK.LDA\n0.9284\n0.9312\n0.9239\nSITK.Sentence\n0.9473\n0.9525\n0.9496\nis therefore desirable to base search on more effec-\ntive text representations than BOW. We show here\nhow scale space model, together with interest point\ndetection techniques, can be used to make a retrieval\nmodel scale-invariant and more effective.\nGiven a set of documents {d} and a query Q, our\ngoal is to rank the documents according to their rel-\nevance w.r.t. Q. The key to text retrieval is a rele-\nvance model r(Q, d). We deﬁne r in the same spirit\nas we develop the SITK. In particular, if we normal-\nize the representations of Q and d to the same di-\nmension, e.g., via bi-linear interpolations3, then at\nany ﬁxed scale s, the scale space model induces a\nrelevance function r(Q, d|s) = ⟨γQ, γd⟩(e.g., via\nKL-divergence, Jessen-Shannon score). This rele-\nvance model can be made scale invariant by deﬁning\na distribution q over the scale space and using:\nr(Q, d) = Eq[r(Q, d|s)],\n(13)\nwhich is referred to as scale-invariant language\nmodel (SILM). As in §4.1, q can be learned through\na Bayesian framework or via a EM procedure. As\nan example, assume q is again a Gamma distribution\nwith parameter (k, θ). Moreover, assume we have\na training corpus containing a set of queries {Q},\nand for each Q a set of documents {dQ\nj } along with\ntheir relevance judgements. We have the following\npairwise preference learning formulation:\nmax\nq\nX\nQ\nX\ndQ\ni ≻dQ\nj\nEq[h(Q, i, j|s)]\n(14)\n3In the case of the sentence-level 2D or LDA 1D signals, γQ\nis a vector and γd is a matrix, this simply amounts to replicat-\ning γQ to the same dimension as γd, or equivalently applying a\nsentence-level sliding-window to d, calculating r at each point\nand summating the relevance scores.\nwhere\nthe\npairwise\nmargin\nh(Q, i, j|s)\n=\nr(Q, dQ\ni |s) −r(Q, dQ\nj |s), and dQ\ni ≻dQ\nj means dQ\ni\nis more relevant to Q than dQ\nj . This formulation can\nbe solved efﬁciently via a similar EM procedure,\nand again in the discrete case with ℓ2-regularization\nhas an efﬁcient close-form solution:\nq = (¯h)+/||(¯h)+||\n(15)\nwhere the average margin ¯h = [¯h1, . . . , ¯hm]⊤with\n¯hl = P\nQ\nP\ndQ\ni ≻dQ\nj h(Q, i, j|sl), l = 1, . . . , m.\nMore interestingly, scale-space model can also be\nused, together with techniques for interest point de-\ntection (Lowe, 2004), to address passage retrieval\n(PR) in a scale-invariant manner, i.e., to determine\nnot only which documents are relevant but also\nwhich parts of them are relevant.\nPR is partic-\nularly advantageous when documents are substan-\ntially longer than queries or when they span a large\nvariety of topic areas, for example, when retrieving\nbooks. A key challenge in PR is how to effectively\nnarrow our attention to a small part of a long docu-\nment. Existing approaches mostly employ a sliding-\nwindow style exhaustive search, i.e., scan through\nevery possible passage, compute relevance scores\nand rank all of them (Tellex et al., 2003). These ap-\nproaches suffer from computational efﬁciency issues\nsince the number of possible passages could be quite\nlarge for long documents. Here we propose a new\nidea which employs interest point detection (IPD)\nalgorithms to quickly focus our attentions to a small\nset of potentially relevant passages. In particularly,\nfor a given (Q, d) pair, we ﬁrst apply IPD (without\nnormalization) to both γQ and γd in scale space, then\nmatch them locally between region pairs centered at\neach interest point and calculate the relevance scores\nthere.\nExperiments.\nWe evaluated SILM on a text re-\ntrieval task based on the OHSUMED data set, a\ncollection of 348,566 documents, 106 queries and\n16,140 relevance judgements. Similar preprocess-\ning steps as in §4.1 were implemented. For SILM,\nstandard Kullback-Leibler divergence was used as\nrelevance function.\nFor comparison, the unigram\nlanguage model (i.e., 1-LM) was used as baselines.\nThe results are reported in Table 2 in terms of three\nstandard IR evaluation measures, i.e., the Mean-\nAverage-Precision (MAP), Precision at N with N=5\nTable 2: Text retrieval performance. We evaluate four\nmodels: the Unigram Language Model (1-LM) and the\nScale-Invariant Language Models with three textual sig-\nnal options (referred to as SILM.BOW, SILM.LDA and\nSILM.Sentence respectively).\nModel\\Measure\nMAP\nP@5\nP@10\n1-LM\n0.2699\n0.4812\n0.4659\nSILM.BOW\n0.2807\n0.5076\n0.4762\nSILM.LDA\n0.2839\n0.5154\n0.4981\nSILM.Sentence\n0.3099\n0.5447\n0.5108\nand 10 (i.e., P@5 and P@10).\nWe observe that\nSILM models outperform the uni-gram LM amaz-\ningly by (up to) 15% in terms of MAP, 13% in P@5\nand 10% in P@10. All these improvements are sig-\nniﬁcant based on a Wilcoxon test at the level of\n0.01. Again, the best performance is obtained by\nthe sentence-level 2D based SILM model.\n4.3\nHierarchical Document Keywording\nThe extrema (i.e., maxima and minima) of a signal\nand its ﬁrst a few derivatives contain important infor-\nmation for describing the structure of the signal, e.g.,\npatches of signiﬁcance, boundaries, corners, ridges\nand blobs in an image. Scale space model provides\na convenient framework to obtain the extrema of a\nsignal at different scales. In particular, the extrema\nin the (k−1)-th derivative of a signal is given by the\nzero-crossing in the k-the derivative, which can be\nobtained at any scale in the scale space conveniently\nvia the convolution of the original signal with the\nderivative of the Gaussian kernel, i.e.:\n∂k\n∂xk γ = f ∗∂k\n∂xk ℓ.\n(16)\nSince Gaussian kernel is inﬁnitely differentiable, the\nscale-space model makes it possible to obtain lo-\ncal extrema/derivatives of a signal to arbitrary or-\nders even when the signal itself is undifferentiable.\nMoreover, due to the “non-enhancement of local ex-\ntrema” property, local extrema are created monoton-\nically as we decrease the scale parameter s. In this\nsection, we show how this can be used to detect\nkeywords from a document in a hierarchical fash-\nion. The idea is to work with the word-level 2D sig-\nnal (other options are also possible) and track the\nextrema (i.e., patterns of signiﬁcance) of the scale-\nspace model γ through the zero-crossing of its ﬁrst\nderivative γ′ = 0 to see how extrema progressively\nemerge as the scale s goes from coarse to ﬁner lev-\nels. This process reduces the scale-space represen-\ntation to a simple ternary tree in the scale space, i.e.,\nthe so-called “interval tree” in (Witkin, 1983). Since\nf deﬁnes a probability over the spatial-semantic\nspace, it is straightforward to interpret the identi-\nﬁed intervals as keywords. This algorithm therefore\nyields a keyword tree that deﬁnes topics we could\nperceive at different levels of granularities from the\ndocument.\nExperiments. As an illustrative example, we ap-\nply the hierarchical keywording algorithm described\nabove to the current paper.\nThe keywords that\nemerged in order are as follows: “scale space” →\n“kernel”, “signal”, “text” →“smoothing”, “spatial”,\n“semantic”, “domains”, “Gaussian”, “ﬁlter”, “text\nanalysis”, “natural language”, “word” →. . . .\n4.4\nHierarchical Text Segmentation\nIn the previous section, we show how semantic key-\nwords can be extracted from a text in a hierarchi-\ncal way by tracking the extrema of its scale space\nmodel γ. In the same spirit, here we show how topic\nboundaries in a text can be identiﬁed by tracking the\nextrema of the ﬁrst derivative γ′.\nText segmentation is an important topic in\nNLP and has been extensively investigated previ-\nously (Beeferman et al., 1999). Many existing ap-\nproaches, however, are only able to identify a ﬂat\nstructure, i.e., all the boundaries are identiﬁed at a\nﬂat level. A more challenging task is to automat-\nically identify a hierarchical table-of-content style\nstructure for a text, that is, to organize boundaries\nof different text units in a tree structure according\nto their topic granularities, e.g., chapter boundaries\nat the top-level, followed in order by boundaries of\nsections, subsections, paragraphs and sentences as\nthe level of depth increases. This can be achieved\nconveniently by the interval tree and coarse-to-ﬁne\ntracking idea presented in (Witkin, 1983). In partic-\nular, if we keep tracking the extrema of the 1st order\nderivatives (i.e., rate of changes) by looking at the\npoints satisfying:\n∂2\n∂2xγ = 0, while ∂3\n∂3xγ ̸= 0.\n(17)\n0  \n0.5\n1\n0\n10\n20\n30\n40\n50\n60\n70\nposition\nscale\n0\n1\n0.5\n0.7\n0.8\n0.9\n1\nposition\nmagnitude of derivative\nFigure 2: Hierarchical text segmentation in scale space.\nDue to the monotonicity nature of scale space repre-\nsentation, such contours are closed above but open\nbelow in the scale space. They naturally illustrate\nhow topic boundaries appear progressively as scale s\ngoes ﬁner. And the exact localization of a boundary\ncan be obtained by tracking back to the scale s = 0.\nAlso note that this algorithm, unlike many existing\nones, does not require any supervision information.\nExperiments. As an example, we apply the hierar-\nchical segmentation algorithm to the current paper.\nWe use the sentence level 2D signal. Let γ(x, sx)\ndenote the vector γ(x, ·, sx, sy = C), where the se-\nmantic scale sy is ﬁxed to a constant C, and the se-\nmantic index y enumerates through the whole vo-\ncabulary {y = v1, . . . , vM}.\nWe identify hier-\narchical boundaries by tracking the zero contours\n|| ∂2\n∂x2 γ(x, sx)||2 = 0 (where || · ||2 denotes ℓ2-norm)\nto the scale s = 0, where the length of the projec-\ntion in scale space (i.e., the vertical span) reﬂects\neach contour line’s topic granularity, as plotted in\nFigure 2 (top). As a reference, the velocity mag-\nnitude curve (bottom) || ∂\n∂xγ(x, sx)||2, and the true\nboundaries of sections (red-dashed vertical lines in\ntop ﬁgure) and subsections (green-dashed) are also\nplotted. As we can see, the predictions match the\nground truths with satisfactorily high accuracy.\n5\nSummary\nThis paper presented scale-space theory for text,\nadapting concepts, formulations and algorithms that\nwere originally developed for images to address the\nunique properties of natural language texts. We also\nshow how scale-space models can be utilized to fa-\ncilitate a variety of NLP tasks. There are a lot of\npromising topics along this line, for example, al-\ngorithms that scale up the scale-space implementa-\ntions towards massive corpus, structures of the se-\nmantic networks that enable efﬁcient or even close-\nform scale-space kernel/relevance model, and effec-\ntive scale-invariant descriptors (e.g., named entities,\ntopics, semantic trends in text) for texts similar to\nthe SIFT feature for images (Lowe, 2004).\nReferences\n[Beeferman et al.1999] D. Beeferman, A. Berger, and\nJ. Lafferty. 1999. Statistical models for text segmen-\ntation. Machine Learning, 34:177–210.\n[Iyer and Ostendorf1996] R.\nIyer and\nM. Ostendorf.\n1996.\nModeling long distance dependence in lan-\nguage: Topic mixtures vs. dynamic cache models.\nIEEE Transactions on Speech and Audio Processing,\n7(1):30–39.\n[Jelinek et al.1991] F. Jelinek, B. Merialdo, S. Roukos,\nand M. Strauss. 1991. A dynamic language model\nfor speech recognition. HLT ’1991, pages 293–295.\n[Lebanon et al.2007] G. Lebanon, Y. Mao, and J. Dillon.\n2007. The locally weighted bag of words framework\nfor document representation. JMLR, 8:2405–2441.\n[Lindeberg1994] T. Lindeberg. 1994. Scale-space the-\nory: A basic tool for analysing structures at different\nscales. Journal of Applied Statistics, 21(2):224–270.\n[Lowe2004] D. Lowe. 2004. Distinctive image features\nfrom scale-invariant keypoints. IJCV, 60(2):91–110.\n[Manning and Schuetze1999] C.\nManning\nand\nH. Schuetze.\n1999.\nFoundations of Statistical\nNatural Language Processing. MIT Press.\n[Mei et al.2008] Q. Mei, D. Zhang, and C. Zhai. 2008.\nA general optimization framework for smoothing lan-\nguage models on graph structures. In SIGIR ’2008,\npages 611–618.\n[Metzler and Croft2005] D. Metzler and W. Croft. 2005.\nA markov random ﬁeld model for term dependencies.\nIn SIGIR ’2005.\n[Tellex et al.2003] S. Tellex, B. Katz, J. Lin, A. Fernan-\ndes, and G. Marton. 2003. Quantitative evaluation\nof passage retrieval algorithms for question answering.\nIn SIGIR ’2003.\n[Witkin1983] A. Witkin. 1983. Scale-space ﬁltering. In\nIJCAI ’1983, pages 1019–1022.\n[Yang and Hu2008] S. Yang and B. Hu. 2008. Feature\nselection by nonparametric bayes error minimization.\nIn PAKDD ’2008, pages 417–428.\n[Yang and Zha2010] S. Yang and H. Zha.\n2010.\nLan-\nguage pyramid and multi-scale text analysis. In CIKM\n’2010, pages 639–648.\n[Zhai and Lafferty2004] C. Zhai and J. Lafferty. 2004. A\nstudy of smoothing methods for language models ap-\nplied to information retrieval. ACM TOIS, 22(2):179–\n214.\n",
  "categories": [
    "cs.IR",
    "cs.CL"
  ],
  "published": "2012-12-10",
  "updated": "2012-12-10"
}