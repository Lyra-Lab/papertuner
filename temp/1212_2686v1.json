{
  "id": "http://arxiv.org/abs/1212.2686v1",
  "title": "Joint Training of Deep Boltzmann Machines",
  "authors": [
    "Ian Goodfellow",
    "Aaron Courville",
    "Yoshua Bengio"
  ],
  "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.",
  "text": "arXiv:1212.2686v1  [stat.ML]  12 Dec 2012\nJoint Training of Deep Boltzmann Machines for Classiﬁcation\nIan J. Goodfellow\nAaron Courville\nYoshua Bengio\nUniversit´e de Montr´eal\nAbstract\nWe introduce a new method for training deep\nBoltzmann machines jointly. Prior methods\nrequire an initial learning pass that trains the\ndeep Boltzmann machine greedily, one layer\nat a time, or do not perform well on classiﬁ-\ncation tasks.\n1\nDeep Boltzmann machines\nA\ndeep\nBoltzmann\nmachine\n(Salakhutdinov and Hinton, 2009) is a probabilistic\nmodel consisting of many layers of random variables,\nmost of which are latent. Typically, a DBM contains\na set of D input features v that are called the visible\nunits because they are always observed during both\ntraining and evaluation. The DBM is usually applied\nto classiﬁcation problems and thus often represents\nthe class label with a one-of-k code in the form of\na discrete-valued label unit y.\ny is observed (on\nexamples for which it is available) during training.\nThe DBM also contains several hidden units, which\nare usually organized into L layers h(i)\nof size\nNi, i = 1, . . . , L,with each unit in a layer conditionally\nindependent of the other units in the layer given the\nneighboring layers.\nThese conditional independence\nproperties allow fast Gibbs sampling because an entire\nlayer of units can be sampled at a time.\nLikewise,\nmean ﬁeld inference with ﬁxed point equations is fast\nbecause each ﬁxed point equation gives a solution to\nan entire layer of variational parameters.\nA DBM deﬁnes a probability distribution by exponen-\ntiating and normalizing an energy function\nP(v, h, y) = 1\nZ exp (−E(v, h, y))\nPreliminary work presented to Bruno Olshausen’s lab and\nGoogle Brain, December 2012.\nwhere\nZ =\nX\nv′,h′,y′\nexp (−E(v′, h′, y′)) .\nZ, the partition function, is intractable, due to the\nsummation over all possible states.\nMaximum like-\nlihood learning requires computing the gradient of\nlog Z. Fortunately, the gradient can be estimated us-\ning an MCMC procedure (Younes, 1999; Tieleman,\n2008). Block Gibbs sampling of the layers makes this\nprocedure eﬃcient.\nThe structure of the interactions in h determines\nwhether further approximations are necessary. In the\npathological case where every element of h is con-\nditionally independent of the others given the visi-\nble units, the DBM is simply an RBM and logZ is\nthe only intractable term of the log likelihood.\nIn\nthe general case, interactions between diﬀerent ele-\nments of h render the posterior P(h | v, y) intractable.\nSalakhutdinov and Hinton (2009) overcome this by\nmaximizing the lower bound on the log likelihood\ngiven by the mean ﬁeld approximation to the poste-\nrior rather than maximizing the log likelihood itself.\nAgain, block mean ﬁeld inference over the layers makes\nthis procedure eﬃcient.\nAn interesting property of the DBM is that the train-\ning procedure thus involves feedback connections be-\ntween the layers. Consider the simple DBM consisting\nof all binary valued units, with the energy function\nE(v, h) = −vT W (1)h(1) −h(1)T W (2)h(2).\nApproximate inference in this model involves repeat-\nedly applying two ﬁxed-point update equations to\nsolve for the mean ﬁeld approximation to the poste-\nrior. Essentially it involves running a recurrent net in\norder to obtain approximate expectations of the latent\nvariables.\nBeyond their theoretical appeal as a deep model that\nadmits simultaneous training of all components using a\ngenerative cost, DBMs have achieved excellent perfor-\nmance in practice. When they were ﬁrst introduced,\nDBMs set the state of the art on the permutation-\ninvariant version of the MNIST handwritten digit\nManuscript under review by AISTATS 2013\nrecognition task at 0.95. (By permutation-invariant,\nwe mean that permuting all of the input pixels prior to\nlearning the network should not cause a change in per-\nformance, so using synthetic image distortions or con-\nvolution to engineer knowledge about the structure of\nthe images into the system is not allowed). Recently,\nnew techniques were used in conjunction with DBM\npretraining to set a new state of the art of 0.79 % test\nerror (Hinton et al., 2012).\n2\nThe joint training problem\nUnfortunately,\nit\nis\nnot\npossible\nto\ntrain\na\ndeep Boltzmann machine using only the varational\nbound and approximate gradient described above.\nSalakhutdinov and Hinton (2009) found that instead\nit must be trained one layer at a time, where each layer\nis trained as an RBM. The RBMs can then be modiﬁed\nslightly, assembled into a DBM, and the DBM may be\ntrained with the learning rule described above.\nIn this paper, we propose a method that enables the\ndeep Boltzmann machine to be jointly trained.\n2.1\nMotivation\nAs a greedy optimization procedure, layerwise training\nmay be suboptimal. Recent small-scale experimental\nwork has demonstrated this to be the case for deep\nbelief networks (Arnold and Ollivier, 2012).\nIn general, for layerwise training to be optimal, the\ntraining procedure for each layer must take into ac-\ncount the inﬂuence that the deeper layers will provide.\nThe standard training procedure simply does not at-\ntempt to be optimal, while the procedure advocated\nby (Arnold and Ollivier, 2012) makes an optimistic as-\nsumption that the deeper layers will be able to im-\nplement the best possible prior on the current layer’s\nhidden units. This approach does not work for deep\nBoltzmann machines because the interactions between\ndeep and shallow units are symmetrical.\nMoreover,\nmodel architectures incorporating design features such\nas sparse connections, pooling, or factored multilinear\ninteractions make it diﬃcult to predict how best to\nstructure one layer’s hidden units in order for the next\nlayer to make good use of them.\nMontavon and M¨uller (2012) showed that reparame-\nterizing the DBM to improve the condition number\nof the Hessian results in succesful generative training\nwithout a greedy layerwise pretraining step. However,\nthis method has never been shown to have good clas-\nsiﬁcation performance, possibly because the reparam-\neterization makes the features never be zero from the\npoint of view of the ﬁnal classiﬁer.\n2.2\nObstacles\nMany obstacles make DBM training diﬃcult.\nAs\nshown by Montavon and M¨uller (2012), the condition\nnumber of the Hessian is poor when the model is pa-\nrameterized as having binary states.\nMany other obstacles exist. The intractable objective\nfunction and the great expense of methods of approx-\nimating it such as AIS makes it too costly do line\nsearches or early stopping. The standard means of ap-\nproximating the gradient are based on stateful MCMC\nsampling, so any optimization method that takes large\nsteps makes the Markov chain and thus the subsequent\ngradient estimates invalid.\n3\nThe JDBM criterion\nOur basic approach is to use a deterministic criterion\nso that each of the above obstacles ceases to be a prob-\nlem.\nOur speciﬁc deterministic criterion we call the Joint\nDBM inpainting criterion, given by\nJ(v, θ) =\nX\ni\nlog Q∗\ni (vSi)\nwhere\nQ∗(Si) = argminQDKL (Q(vSi)∥P(h | v−Si)) .\nThis can be viewed as a mean ﬁeld approximation\nto the generalized pseudolikelihood.\nWe backprop\nthrough the minimization of Q, so this can be viewed\nas training a family of recurrent nets that all share\nparameters but each optimize a diﬀerent task.\nWhile both pseudolikelihood and likelihood are asymp-\ntotically consistent estimators, their behavior in the\nlimited data case is diﬀerent.\nMaximum likelihood\nshould be better for drawing samples, but general-\nized pseudolikelihood can often be better for training\na model to answer queries conditioning on sets similar\nto the Si used during training. We view our work as\nsimilar to (Stoyanov et al., 2011). The idea is to train\nthe DBM to be a general question answering machine,\nusing the same approximations at train time as will be\nrequired at test time, rather than to train it to be a\ngood at generating MCMC samples that resemble the\ntraining data.\nWe train using nonlinear conjugate gradient descent\non large minibatches of data. For each data point, in\nthe minibatch, we sample only one subset Si to train\non, rather than attempting to sum over all subsets\nSi. We choose each variable in the model to be con-\nManuscript under review by AISTATS 2013\nditioned on independently from the others with prob-\nability p. High values of p work best, since the mean\nﬁeld assumption is applied to the variables that are not\nselected to be conditioned on, and the more of those\nthere are the worse the mean ﬁeld assumption is.\n3.1\nMNIST experiments\nWe used the MNIST dataset as a benchmark to com-\npare our training method to the layerwise method\nproposed by Salakhutdinov and Hinton (2009).\nIn\norder to replicate their technique as closely as pos-\nsible\nwe\nrefer\nto\nthe\naccompanying demo\ncode\n(http://www.mit.edu/ rsalakhu/DBM.html) rather\nthan the paper itself. Since many important details\nof the code are not included in the paper, we provide\na summary of the code here.\n3.1.1\nPrior method\nThe demo code trains a DBM consisting of v, h(1),\nh(2), and y. This is accomplished in three steps: 1)\nTraining an RBM consisting of v and h(1) to maximize\nthe likelihood of v. 2) Training an RBM consisting of\nh(1), h(2), and y to maximize the likelihood of y and\nh(1) when h(1) is drawn from the ﬁrst RBM’s posterior.\n3) Assembling the RBMs into a DBM and training it to\nmaximize the variational lower bound on log P(v, y).\nThus far the model has only been trained generatively,\nthough the labels y are included.\nIts discriminative\nperformance–its ability to predict y from v is thus\nsomewhat limited.\nWe used mean ﬁeld inference to\napproximate P(y | v) in the trained model and ob-\ntained a test set error of 2.15 %.\nIn order to obtain better discriminative performance,\nthe DBM is used to deﬁne a feature extractor / clas-\nsiﬁer pipeline.\nFirst, the dataset is augmented with features φ. φ is\ncomputed once at the start of discriminative training\nand then ﬁxed, i.e., the discriminative learning does\nnot change the value of φ. φ(v) is deﬁned to be the\nmean ﬁeld parameter vector ˆh(2) obtained by running\nmean ﬁeld on v with ˆy clamped to 0. No explanation\nis given for clamping ˆy to 0 in the code or the paper,\nbut we observe that it greatly improves generalization\nperformance, even though it does not correspond to a\nstandard probabilistic operation like marginalizing out\ny.\nNext, these features are fed into a multilayer percep-\ntron that resembles one more step of inference:\nˆh(1)′ = σ\n\u0010\nvT A + f T B + b(1)\u0011\nˆh(2)′ = σ\n\u0010\nˆh(1)′T C + b(2)\u0011\nˆy = softmax\n\u0010\nˆh(2)′T D\n\u0011\nA, B, C, and D are initialized to W (1), W (2)T , W (2),\nand W (3), respectively. They are then treated as in-\ndependent parameters, i.e., C is not constrained to re-\nmain equal to the transpose of D during learning. The\nMLP is ﬁnally trained to maximize the log probability\nof y under ˆy using 100 epochs of nonlinear conjugate\ngradient descent.\n3.2\nOur method\nWe follow the pre-existing procedure as closely as pos-\nsible. The diﬀerences are as follows:\n1. We do not have a layerwise pretraining phase.\n2. When training the DBM over v, h(1), h(2) and y,\nwe use the JDBM inpainting criterion instead of\nPCD.\n3. Rather than running training for a hard-coded\nnumber of epochs as in the DBM demo, we use\nearly stopping based on the validation set er-\nror. We use the ﬁrst 50,000 training examples for\ntraining and the last 10,000 for validation.\nAf-\nter the validation set error starts to increase, we\ntrain on the entire MNIST training set until the\nlog likelihood on the last 10,000 examples matches\nthe log likelihood on the ﬁrst 50,000 at the time\nthat the validation set error began to rise.\nWe obtain a test set accuracy of 1.19 % on MNIST.\nWe observe that a DBM trained with layerwise RBM\npretraining followed by standard DBM variational\nlearning obtains a lower inpainting error on the train-\ning set than our models jointly trained using the in-\npainting criterion.\nThis suggests that our criterion\ncorrectly ranks models according to their value as a\nclassiﬁer, but that our optimization procedure needs\nto be improved.\nFor comparison, our best result using standard DBM\nvariational learning but without layerwise pretraining\nwas 1.69 % test error. Using the centering trick, this\nincreased to 2.03 %. Both of these numbers are likely\nto improve somewhat with more hyperparameter ex-\nploration.\nManuscript under review by AISTATS 2013\nReferences\nArnold, L. and Ollivier, Y. (2012). Layer-wise learning\nof deep generative models. ArXiv e-prints.\nHinton,\nG.\nE.,\nSrivastava,\nN.,\nKrizhevsky,\nA.,\nSutskever, I., and Salakhutdinov, R. (2012). Improv-\ning neural networks by preventing co-adaptation of\nfeature detectors. CoRR, abs/1207.0580.\nMontavon, G. and M¨uller, K.-R. (2012). Learning fea-\nture hierarchies with cented deep Boltzmann ma-\nchines. CoRR, abs/1203.4416.\nSalakhutdinov, R. and Hinton, G. (2009). Deep Boltz-\nmann machines. In Proceedings of the Twelfth In-\nternational Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS 2009), volume 8.\nStoyanov, V., Ropson, A., and Eisner, J. (2011). Em-\npirical risk minimization of graphical model param-\neters given approximate inference, decoding, and\nmodel structure. In Proceedings of the 14th Inter-\nnational Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS), volume 15 of JMLR Work-\nshop and Conference Proceedings, pages 725–733,\nFort Lauderdale. Supplementary material (4 pages)\nalso available.\nTieleman, T. (2008). Training restricted Boltzmann\nmachines using approximations to the likelihood\ngradient.\nIn W. W. Cohen, A. McCallum, and\nS. T. Roweis, editors, ICML 2008, pages 1064–1071.\nACM.\nYounes, L. (1999).\nOn the convergence of marko-\nvian stochastic algorithms with rapidly decreasing\nergodicity rates. Stochastics and Stochastic Reports,\n65(3), 177–228.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2012-12-12",
  "updated": "2012-12-12"
}