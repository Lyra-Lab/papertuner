{
  "id": "http://arxiv.org/abs/1301.2840v4",
  "title": "Unsupervised Feature Learning for low-level Local Image Descriptors",
  "authors": [
    "Christian Osendorfer",
    "Justin Bayer",
    "Sebastian Urban",
    "Patrick van der Smagt"
  ],
  "abstract": "Unsupervised feature learning has shown impressive results for a wide range\nof input modalities, in particular for object classification tasks in computer\nvision. Using a large amount of unlabeled data, unsupervised feature learning\nmethods are utilized to construct high-level representations that are\ndiscriminative enough for subsequently trained supervised classification\nalgorithms. However, it has never been \\emph{quantitatively} investigated yet\nhow well unsupervised learning methods can find \\emph{low-level\nrepresentations} for image patches without any additional supervision. In this\npaper we examine the performance of pure unsupervised methods on a low-level\ncorrespondence task, a problem that is central to many Computer Vision\napplications. We find that a special type of Restricted Boltzmann Machines\n(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple\nbinarization scheme produces compact representations that perform better than\nseveral state-of-the-art descriptors.",
  "text": "Unsupervised Feature Learning for low-level Local\nImage Descriptors\nChristian Osendorfer, Justin Bayer, Sebastian Urban, Patrick van der Smagt\nTechnische Universit¨at M¨unchen\n{osendorf, bayerj, surban, smagt}@in.tum.de\nAbstract\nUnsupervised feature learning has shown impressive results for a wide range of in-\nput modalities, in particular for object classiﬁcation tasks in computer vision. Us-\ning a large amount of unlabeled data, unsupervised feature learning methods are\nutilized to construct high-level representations that are discriminative enough for\nsubsequently trained supervised classiﬁcation algorithms. However, it has never\nbeen quantitatively investigated yet how well unsupervised learning methods can\nﬁnd low-level representations for image patches without any additional supervi-\nsion. In this paper we examine the performance of pure unsupervised methods on\na low-level correspondence task, a problem that is central to many Computer Vi-\nsion applications. We ﬁnd that a special type of Restricted Boltzmann Machines\n(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple\nbinarization scheme produces compact representations that perform better than\nseveral state-of-the-art descriptors.\n1\nIntroduction\nIn this paper we tackle a recent computer vision dataset [2] from the viewpoint of unsupervised\nfeature learning. Why yet another dataset? There are already enough datasets that serve well for\nevaluating feature learning algorithms. In particular for feature learning from image data, several\nwell-established benchmarks exist: Caltech-101 [10], CIFAR-10 [19], NORB [23], to name only a\nfew. Notably, these benchmarks are all object classiﬁcation tasks. Unsupervised learning algorithms\nare evaluated by considering how well a subsequent supervised classiﬁcation algorithm performs\non high-level features that are found by aggregating the learned low-level representations [8]. We\nthink that mingling these steps makes it difﬁcult to assess the quality of the unsupervised algorithms.\nA more direct way is needed to evaluate these methods, preferably where a subsequent supervised\nlearning step is completely optional.\nWe are not only at odds with the methodology of evaluating unsupervised learning algorithms. Gen-\neral object classiﬁcation tasks are always based on orientation- and scale-rectiﬁed pictures with\nobjects or themes ﬁrmly centered in the middle. We are looking for a dataset where it is possible\nto show that unsupervised feature learning is beneﬁcial to the wide range of Computer Vision tasks\nbeyond object classiﬁcation, like tracking, stereo vision, panoramic stitching or structure from mo-\ntion. One might argue, that object classiﬁcation acts as a good proxy for all these other tasks but\nthis hypothesis has not shown to be correct either theoretically or through empirical evidence. In-\nstead, we chose the most general and direct task that can be be applied to low-level representations:\nmatching these representations, i.e. determining if two data samples are similar given their learned\nrepresentation.\nMatching image descriptors is a central problem in Computer Vision, so hand-crafted descriptors\nare always evaluated with respect to this task [28]. Given a dataset of labeled correspondences,\nsupervised learning approaches will ﬁnd representations and the accompanying distance metric that\n1\narXiv:1301.2840v4  [cs.CV]  25 Apr 2013\nare optimized with respect to the induced similarity measure. It is remarkable that hand-engineered\ndescriptors perform well under this task without the need to learn such a measure for their represen-\ntations in a supervised manner.\nTo the best of our knowledge it has never been investigated whether any of the many unsupervised\nlearning algorithms developed over the last couple of years can match this performance without\nrelying on any supervision signals. While we propose an additional benchmark for unsupervised\nlearning algorithms, we do not introduce a new learning algorithm. We rather investigate the per-\nformance of the Gaussian RBM (GRBM) [39], its sparse variant (spGRBM) [29] and the mean\ncovariance RBM (mcRBM) [33] without any supervised learning with respect to the matching task.\nAs it turns out, the mcRBM performs comparably to hand-engineered feature descriptors. In fact\nusing a simple heuristic, the mcRBM produces a compact binary descriptor that performs better than\nseveral state-of-the-art hand-crafted descriptors.\nWe begin with a brief description of the dataset used for evaluating the matching task, followed by\na section on details of the training procedure. In section 4 we present our results, both quantitatively\nand qualitatively and also mention other models that were tested but not further analyzed because\nof overall bad performance. Section 5 concludes with a brief summary and an outlook for future\nwork. A review of GRBMs, spGRBMs and mcRBMs is provided in the appendix, section 6, for\ncompleteness.\nRelated work\nMost similar in spirit to our work are [6, 20, 22]: Like us, [6, 22] are interested in\nthe behavior of unsupervised learning approaches without any supervised steps afterwards. Whereas\nboth investigate high-level representations. [20] learns a compact, binary representation with a\nvery deep autoencoder in order to do fast content-based image search (semantic hashing, [36]).\nAgain, these representations are studied with respect to their capabilities to model high-level object\nconcepts. Additionally, various algorithms to learn high-level correspondences have been studied\n[4, 37, 16] in recent years.\nFinding (compact) low-level image descriptors should be an excellent machine learning task: Even\nhand-designed descriptors have many free parameters that cannot (or should not) be optimized man-\nually. Given ground truth data for correspondences, the performance of supervised learning algo-\nrithms is impressive [2]. Very recently, boosted learning with image gradient-based weak learners\nhas shown excellent results [43, 42] on the same dataset used in this paper. See section 2 of [43] for\nmore related work in the space of supervised metric learning.\n2\nDataset\nAt the heart of this paper is a recently introduced dataset for discriminative learning of local image\ndescriptors [2]. It attempts to foster learning optimal low-level image representations using a large\nand realistic training set of patch correspondences. The dataset is based on more than 1.5 million\nimage patches (64 × 64 pixels) of three different scenes: the Statue of Liberty (about 450,000\npatches), Notre Dame (about 450,000 patches) and Yosemite’s Half Dome (about 650,000 patches).\nThe patches are sampled around interest points detected by Difference of Gaussians [27] and are\nnormalized with respect to scale and orientation1. As shown in Figure 1, the dataset has a wide\nvariation in lighting conditions, viewpoints, and scales.\nThe dataset contains also approximately 2.5 million image correspondences. Correspondences be-\ntween image patches are established via dense surface models obtained from stereo matching (stereo\nmatching, with its epipolar and multi-view constraints, is a much easier problem than unconstrained\n2D feature matching). The exact procedure to establish correspondences is more involved and de-\nscribed in detail in [2, Section II]. Because actual 3D correspondences are used, the identiﬁed 2D\npatch correspondences show substantial perspective distortions resulting in a much more realistic\ndataset than previous approaches [24, 28]. The dataset appears very similar to an earlier benchmark\nof the same authors [47], yet the correspondences in the novel dataset resemble a much harder prob-\nlem. The error rate at 95% detection of correct matches for the SIFT descriptor [27] raises from 6%\nto 26%, the error rate for evaluating patch similarity in pixel space (using normalized sum squared\ndifferences) raises from 20% to at least 48% (all numbers are take from [47] and [2] respectively),\n1A similar dataset of patches centered on multi-scale Harris corners is also available.\n2\nFigure 1: Patch correspondences from the Liberty dataset. Note the wide variation in lighting,\nviewpoint and level of detail. The patches are centered on interest points but otherwise can be\nconsidered random, e.g. there is no reasonable notion of an object boundary possible. Figure taken\nfrom [2].\nfor example. In order to facilitate comparison of various descriptor algorithms a large set of prede-\ntermined match/non-match patch pairs is provided. For every scene, sets comprising between 500\nand 500,000 pairs (with 50% matching and 50% non-matching pairs) are available.\nWe don’t argue that this dataset subsumes or substitutes any of the previously mentioned bench-\nmarks. Instead, we think that it can serve to complement those. It constitutes an excellent testbed\nfor unsupervised learning algorithms: Experiments considering self-taught learning [32], effects of\nsemi-supervised learning, supervised transfer learning over input distributions with a varying de-\ngree of similarity (the scenes of Statue of Liberty and Notredame show architectural structures,\nwhile Half Dome resembles a typical natural scenery) and the effect of enhancing the dataset with\narbitrary image patches around keypoints can all be conducted in a controlled environment. Further-\nmore, end-to-end trained systems for (large) classiﬁcation problems (like [21, 5]) can be evaluated\nwith respect to this type of data distribution and task.\n3\nTraining Setup\nDifferent to [2], our models are trained in an unsupervised fashion on the available patches. We train\non one scene (400,000 randomly selected patches from this scene) and evaluate the performance on\nthe test set of every scene. This allows us to investigate the self-taught learning paradigm [32]. We\nalso train on all three scenes jointly (represented by 1.2 million image patches) and then evaluate\nagain every scene individually.\n3.1\nGRBM/spGRBM\nThe GRBM and spGRBM (see Appendix, section 6.2) only differ in the setting of the sparsity\npenalty λsp, all other settings are the same. We use CD1 [13] to compute the approximate gradient\nof the log-likelihood and the recently proposed rmsprop [41] method as gradient ascent method.\nCompared to standard minibatch gradient ascent, we ﬁnd that rmsprop is a more efﬁcient method\nwith respect to the training time necessary to learn good representations: it takes at most half of the\ntraining time necessary for standard minibatch gradient ascent.\nBefore learning the parameters, we ﬁrst scale all image patches to 16×16 pixels. Then we preprocess\nall training samples by subtracting the vectors’ mean and dividing by the standard deviation of its\nelements. This is a common practice for visual data and corresponds to local brightness and contrast\nnormalization. [39, Section 2.2] gives also a theoretical justiﬁcation for why this preprocessing step\nis necessary to learn a reasonable precision matrix Λ. We ﬁnd that this is the only preprocessing\nscheme that allows GRBM and spGRBM to achieve good results. In addition, it is important to learn\n3\nΛ—setting it to the identity matrix, a common practice [14], also produces dissatisfying error rates.\nNote that originally it was considered that learning Λ is mostly important when one wants to ﬁnd a\ngood density (i.e. generative) model of the data.\nBoth GRBM and spGRBM have 512 hidden units. The elements of W are initialized according to\nN(0, 0.1), the biases are initialized to 0. rmsprop uses a learning rate of 0.001, the decay factor is\n0.9, the minibatch size is 128. We train both models for 10 epochs (this takes about 15 minutes on\na consumer GPU for 400000 patches). For the spGRBM we use a sparsity target of ρ = 0.05 and a\nsparsity penalty of λsp = 0.2. spGRBM is very sensitive to settings of λsp [38]—setting it too high\nresults in dead representations (samples that have no active hidden units) and the results deteriorate\ndrastically.\n3.2\nmcRBM\nmcRBM (see Appendix, section 6.3) training is performed using the code from [33]. We resam-\nple the patches to 16 × 16 pixels. Then the samples are preprocessed by subtracting their mean\n(patchwise), followed by PCA whitening, which retains 99% of the variance. The overall training\nprocedure (with stochastic gradient descent) is identical to the one described in [33, Section 4]. We\ntrain all architectures for a total of 100 epochs, however updating P is only started after epoch\n50. We consider two different mcRBM architectures: The ﬁrst has 256 mean units, 512 factors\nand 512 covariance units. P is not constrained by any ﬁxed topography. We denote this architec-\nture by mcRBM(256, 512/512). The second architecture is concerned with learning more compact\nrepresentations: It has 64 mean units, 576 factors and 64 covariance units. P is initialized with a\ntwo-dimensional topography that takes 5 × 5 neighborhoods of factors with a stride equal to 3. We\ndenote this model by mcRBM(64, 576/64). On a consumer grade GPU it takes 6 hours to train the\nﬁrst architecture on 400000 samples and 4 hours to train the second architecture on the same number\nof samples.\n4\nResults\nFor the results presented in this section (Table 1) we follow the evaluation procedure of [2]: For\nevery scene (Liberty (denoted by LY), Notredame (ND) and Half Dome (HD)), we use the labeled\ndataset with 100,000 image pairs to assess the quality of a trained model on this scene. In order to\nsave space we do not present ROC curves and only show the results in terms of the 95% error rate\nwhich is the percent of incorrect matches when 95% of the true matches are found: After computing\nthe respective distances for all pairs in a test set, a threshold is determined such that 95% of all\nmatching pairs have a distance below this threshold. Non-matching pairs with a distance below this\nthreshold are considered incorrect matches.\nTable 1 consists of two subtables. Table 1a presents the error rates for GRBM, spGRBM and\nmcRBM when no limitations on the size of representations are placed. Table 1b only considers\ndescriptors that have an overall small memory footprint. For GRBM and spGRBM we use the\nactivations of the hidden units given a preprocessed input patch v as descriptor D(v) (see eq. 5,\nsection 6.1):\nD(v) = σ(vT Λ\n1\n2 W + b)\nFor the mcRBM a descriptor is formed by using the activations of the latent covariance units alone,\nsee eq. 8, section 6.3:\nD(v) = σ(P T (CT v)2 + c)\nThis is in accordance with manually designed descriptors. Many of these rely on distributions (i.e.\nhistograms) of intensity gradients or edge directions [27, 28, 1], structural information which is\nencoded by the covariance units (see also [35, Section 2])2.\n4.1\nDistance metrics\nAs we explicitly refrain from learning a suitable (with respect to the correspondence task) distance\nmetric with a supervised approach, we have to resort to standard distance measures. The Euclidean\n2Extending the descriptor with mean units degrades results.\n4\nTest set\nMethod\nTraining set\nLY\nND\nHD\nSIFT\n–\n28.1 20.9 24.7\nLY\n47.6 33.5 41.4\nGRBM\nND\n50.0 33.4 42.5\n(L1ℓ1)\nHD\n49.0 34.0 41.5\nLY/ND/HD\n48.7 33.5 42.1\nLY\n37.9 26.9 34.3\nspGRBM ND\n40.0 28.0 35.4\n(L1ℓ1)\nHD\n39.1 27.9 34.9\nLY/ND/HD\n37.5 26.6 33.6\nLY\n31.3 25.1 34.5\nmcRBM\nND\n34.0 25.6 33.0\n(L1ℓ2)\nHD\n31.2 22.3 25.7\nLY/ND/HD\n30.8 24.8 33.3\nLY\n34.7 24.2 38.6\nmcRBM\nND\n33.3 24.8 44.9\n(JSD)\nHD\n29.9 22.7 37.6\nLY/ND/HD\n30.0 23.1 39.8\n(a)\nTest set\nMethod\nTraining set\nLY\nND\nHD\nSIFT\n–\n31.7 22.8 25.6\nBRIEF\n–\n59.1 54.5 54.9\nBRISK\n–\n79.3 74.8 73.2\nSURF\n–\n54.0 45.5 43.5\nLY\n–\n16.9 22.8\nBinBoost\nND\n20.4\n–\n18.9\n(8 bytes)\nHD\n21.6 14.5\n–\nLY\n–\n31.1 34.4\nITQ-SIFT ND\n37.0\n–\n34.3\n(8 bytes)\nHD\n37.3 30.5\n–\nLY\n–\n43.1 47.2\nD-Brief\nND\n46.2\n–\n51.3\n(4 bytes)\nHD\n53.3 43.9\n–\nLY\n36.2 39.9 64.9\nmcRBM\nND\n46.2 34.5 56.1\n(8 bytes)\nYM\n43.4 37.4 53.0\nLY/ND/HD\n40.5 36.6 55.4\n(b)\nTable 1: Error rates, i.e. the percent of incorrect matches when 95% of the true matches are found.\nAll numbers for GRBM, spGRBM and mcRBMs are given within ±0.5%. Every subtable, indicated\nby an entry in the Method column, denotes a descriptor algorithm. Descriptor algorithms that do not\nrequire learning (denoted by – in the column Training set) are represented by one line. The numbers\nin the columns labeled LY, ND and HD are the error rates of a method on the respective test set for\nthis scene. Supervised algorithms are not evaluated (denoted by –) on the scene they are trained on.\nThe Training set LY/ND/HD encompasses 1.2 million patches of all three scenes; this setting is only\npossible for unsupervised learning methods. (a) Error rates for several unsupervised algorithms\nwithout restricting the size of the learned representation. GRBM, spGRBM and mcRBM learn\ndescriptors of dimensionality 512. (L1ℓ1) denotes that the error rates for a method are with respect\nto ℓ1 normalization of the descriptor under the L1 distance. (b) Results for compact descriptors.\nBRIEF (32 bytes) [3] and BRISK (64 bytes) [25] are binary descriptors, SURF [1] is a real valued\ndescriptor with 64 dimensions. BinBoost [42], ITQ-SIFT [12] and D-Brief [44] learn compact\nbinary descriptors with supervision. Numbers for BRIEF, BRISK, SURF, BinBoost and ITQ-SIFT\nare from [42].\ndistance is widely used when comparing image descriptors. Yet, considering the generative nature of\nour models we follow the general argumentation of [17] and choose the Manhattan distance, denoted\nin this text by L1. We also consider two normalization schemes for patch representations, ℓ1 and ℓ2\n(i.e. after a feature vector x is computed, its length is normalized such that ∥x∥1 = 1 or ∥x∥2 = 1).\nGiven a visible input both (sp)GRBM and mcRBM compute features that resemble parameters of\n(conditionally) independent Bernoulli random variables. Therefore we consider the Jensen-Shannon\ndivergence (JSD) [26] as an alternative similarity measure. Finally, for binary descriptors, we use\nthe Hamming distance.\n4.2\nSIFT Baseline\nSIFT [27] (both as interest point detector and descriptor) was a landmark for image feature matching.\nBecause of its good performance it is one of the most important basic ingredients for many different\nkinds of Computer Vision algorithms. It serves as a baseline for evaluating our models. We use\nvlfeat [45] to compute the SIFT descriptors.\n5\nThe performance of the SIFT descriptor, ℓ1-normalized, is reported (using L1 distance) in Table 1a,\nﬁrst entry. ℓ1 normalization provides better results than ℓ2 normalization or no normalization at all.\nSIFT performs descriptor sampling at a certain scale relative to the Difference of Gaussians peak.\nIn order to achieve good results, it is essential to optimize this scale parameter [2, Figure 6] on\nevery dataset. Table 1b is concerned with evaluating compact descriptors: the ﬁrst entry shows the\nperformance of SIFT when used as a 128-byte descriptor (i.e. no normalization applied, but again\noptimized for the best scale parameter) with L1 distance.\n4.3\nQuantitative analysis\nTable 1a shows that SIFT performs better than all three unsupervised methods.\nmcRBM(256,\n512/512) performs similar to SIFT when trained on Half Dome, albeit at the cost of a 4.5 times\nlarger descriptor representation. The compact binary descriptor (the simple binarization scheme\nis described below) based on mcRBM(64, 576/64) performs remarkably well, comparable or even\nbetter than several state-of-the-art descriptors (either manually designed or trained in a supervised\nmanner), see Table 1b, last entry. We discuss in more detail several aspects of the results in the\nfollowing paragraphs.\nGRBM and spGRBM\nspGRBM performs considerably better than its non-sparse version (see\nTable 1a, second and third entries). This is not necessarily expected: Unlike e.g. in classiﬁcation\n[8] sparse representations are considered problematic with respect to evaluating distances directly.\nLifetime sparsity may be after all beneﬁcial in this setting compared to strictly enforced population\nsparsity. We plan to investigate this issue in more detail in future work by comparing spGRBM to\nCardinality restricted boltzman machines [38] on this dataset.\nSelf-taught paradigm\nWe would expect that the performance of a model trained on the Liberty\ndataset and evaluated on the Notre Dame scene (and vice versa) should be noticeably better than\nthe performance of a model trained on Half Dome and evaluated on the two architectural datasets.\nHowever, this is not what we observe. In particular for the mcRBM (both architectures) it is the\nopposite: Training on the natural scene data leads to much better performance than the assumed\noptimal setting.\nJensen-Shannon Divergence\nBoth GRBM and spGRBM perform poorly under the Jensen-\nShannon divergence similarity (overall error rates are around 60%), therefore we don’t report these\nnumbers in the table. Similar, results for mcRBM under JSD are equally bad. However, if one scales\ndown P by a constant (we found the value of 3 appropriate), the results with respect to JSD improve\nnoticeably, see Table 1a, the last entry. The performance on the Half Dome dataset is still not good\n– the scaling factor should be learned [9], which we also plan for future work.\nCompact binary descriptor\nWe were not successful in ﬁnding a good compact representa-\ntion with either GRBM or spGRBM.\nFinding compact representations for any kind of input\ndata should be done with multiple layers of nonlinearities [20]. But even with only two layers\n(mcRBM(64, 576/64)) we learn relatively good compact descriptors. If features are binarized, the\nrepresentation can be made even more compact (64 bits, i.e. 8 bytes). In order to ﬁnd a suitable\nbinarization threshold we employ the following simple heuristic: After training on a dataset is ﬁn-\nished we histogram all activations (values between 0 and 1) of the training set and use the median\nof this histogram as the threshold.\n4.4\nQualitative analysis\nWe brieﬂy comment on the developed ﬁlters (Figure 2). Unsurprisingly, spGRBM (Figure 2a) and\nmcRBM (Figure 2b—these are columns from C) learn Gabor like ﬁlters. At a closer look we make\nsome interesting observations: Figure 2c shows the diagonal elements of Λ\n1/2 from a spGRBM.\nWhen computing a latent representation, the input v is scaled (elementwise) by this matrix, which,\nvisualized as a 2D image, resembles a Gaussian that is dented at the center, the location of the\nkeypoint of every image patch. The mcRBM also builds ﬁlters around the keypoint: Figure 2d shows\nsome unusual ﬁlters from C. They are centered around the keypoint and bear a strong resemblance\nto discriminative projections (Figure 2e) that are learned in a supervised way on this dataset [2,\n6\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 2: (a) Typical ﬁlters learned with spGRBM. (b) Filters from an mcRBM. (c) The pixelwise\ninverted standard deviations learned with a spGRBM plotted as a 2D image (darker gray intensities\nresemble lower numerical values). An input patch is elementwise multiplied with this image when\ncomputing the latent representation. This ﬁgure is generated by training on 32 × 32 patches for\nbetter visibility, but the same qualitative results appear with 16 × 16 patches. (d) The mcRBM also\nlearns some variants of log-polar ﬁlters centered around the DoG keypoint. These are very similar\nto ﬁlters found when optimizing for the correspondence problem in a supervised setting. Several\nof such ﬁlters are shown in subﬁgure (e), taken from [2, Figure 5]. Finally (f), the basic keypoint\nﬁlters are combined with Garbor ﬁlters, if these are placed close to the center; the Garbor ﬁlters get\nsystematically arranged around the keypoint ﬁlters.\nFigure 5]. Qualitatively, the ﬁlters in Figure 2d resemble log-polar ﬁlters that are used in several\nstate-of-the-art feature designs [28]. The very focused keypoint ﬁlters (ﬁrst column in Figure 2d)\nare often combined with Gabor ﬁlters placed in the vicinity of the center – the Garbor ﬁlters appear\non their own, if they are too far from the center. If an mcRBM is trained with a ﬁxed topography for\nP , one sees that the Gabor ﬁlters get systematically arranged around the keypoint (Figure 2f).\n4.5\nOther models\nWe also trained several other unsupervised feature learning models: GRBM with nonlinear rectiﬁed\nhidden units3 [30], various kinds of autoencoders (sparse [7] and denoising [46] autoencoders), K-\n3Our experiments indicate that rmsprop is in this case also beneﬁcial with respect to the ﬁnal results: It\nlearns models that perform about 2-3% better than those trained with stochastic gradient descent.\n7\nmeans [7] and two layer models (stacked RBMs, autoencoders with two hidden layers, cRBM [34]).\nNone of these models performed as good as the spGRBM.\n5\nConclusion\nWe start this paper suggesting that unsupervised feature learning should be evaluated (i) without us-\ning subsequent supervised algorithms and (ii) more directly with respect to its capacity to ﬁnd good\nlow-level image descriptors. A recently introduced dataset for discriminatively learning low-level\nlocal image descriptors is then proposed as a suitable benchmark for such an evaluation scheme that\ncomplements nicely the existing benchmarks. We demonstrate that an mcRBM learns real-valued\nand binary descriptors that perform comparably or even better to several state-of-the-art methods on\nthis dataset.\nIn future work we plan to evaluate deeper architectures [20], combined with sparse convolutional\nfeatures [18] on this dataset. Moreover, ongoing work investigates several algorithms [4, 37] for\nsupervised correspondence learning on the presented dataset.\nReferences\n[1] H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robust features. In Proc. ECCV, 2006.\n[2] M. Brown, G. Hua, and S. Winder. Discriminative learning of local image descriptors. IEEE PAMI, 2010.\n[3] M. Calonder, V. Lepetit, M. Ozuysal, T. Trzcinski, C. Strecha, and P. Fua. Brief: Computing a local binary\ndescriptor very fast. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(7):1281–1298,\n2012.\n[4] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to\nface veriﬁcation. In Proc. CVPR, 2005.\n[5] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\nIn Proc. CVPR, 2012.\n[6] A. Coates, A. Karpathy, and A. Ng.\nEmergence of object-selective features in unsupervised feature\nlearning. In Proc. NIPS, 2012.\n[7] A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning. In\nProc. AISTATS, 2011.\n[8] A. Coates and A. Ng. The importance of encoding versus training with sparse coding and vector quanti-\nzation. In Proc. ICML, 2011.\n[9] G. Dahl, M. Ranzato, A. Mohamed, and G. Hinton. Phone recognition with the mean-covariance restricted\nboltzmann machine. In Proc. NIPS, 2010.\n[10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An\nincremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand-\ning, 2007.\n[11] Y. Freund and D. Haussler. Unsupervised learning of distributions on binary vectors using two layer\nnetworks. Technical report, University of California, Santa Cruz, 1994.\n[12] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. Iterative quantization: A procrustean approach to\nlearning binary codes for large-scale image retrieval. Pattern Analysis and Machine Intelligence, 2012.\n[13] G. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,\n14(8):1771–1800, 2002.\n[14] G. Hinton. A practical guide to training restricted boltzmann machines. Technical report, University of\nToronto, 2010.\n[15] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,\n313(5786):504–507, 2006.\n[16] G. Huang, M. Mattar, H. Lee, and E. Learned-Miller. Learning to align from scratch. In Proc. NIPS,\n2012.\n[17] Y. Jia and T. Darrell. Heavy-tailed distances for gradient based image descriptors. In Proc. NIPS, 2011.\n[18] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor, M. Mathieu, and Y. LeCun. Learning convolutional\nfeature hierarchies for visual recognition. In Proc. NIPS, 2010.\n[19] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of\nToronto, 2009.\n8\n[20] A. Krizhevsky and G. Hinton. Using very deep autoencoders for content-based image retrieval. In Proc.\nESANN, 2011.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-\nworks. In Proc. NIPS, 2012.\n[22] Q. Le, R. Monga, M. Devin, G. Corrado, K. Chen, M. Ranzato, J. Dean, and A. Ng. Building high-level\nfeatures using large scale unsupervised learning. In Proc. ICML, 2012.\n[23] Y. LeCun, F. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to\npose and lighting. In Proc. CVPR, 2004.\n[24] V. Lepetit and P. Fua. Keypoint recognition using randomized trees. IEEE PAMI, 28(9):1465–1479, 2006.\n[25] S. Leutenegger, M. Chli, and R. Siegwart. Brisk: Binary robust invariant scalable keypoints. In Proc.\nICCV, 2011.\n[26] J. Lin. Divergence measures based on the shannon entropy. Information Theory, IEEE Transactions on,\n37(1):145–151, 1991.\n[27] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer\nVision, 60(2):91–110, 2004.\n[28] K. Mikolajczyk and C. Schmid. A performance evaluation of local descriptors. IEEE PAMI, 2005.\n[29] V. Nair and G. Hinton. 3-d object recognition with deep belief nets. In Proc. NIPS, 2009.\n[30] V. Nair and G. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proc. ICML,\n2010.\n[31] R. Neal. Probabilistic inference using markov chain monte carlo methods. Technical report, University\nof Toronto, 1993.\n[32] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng. Self-taught learning: Transfer learning from unlabeled\ndata. In Proc. ICML, 2007.\n[33] M. Ranzato and G. Hinton. Modeling pixel means and covariances using factorized third-order boltzmann\nmachines. In Proc. CVPR, 2010.\n[34] M. Ranzato, A. Krizhevsky, and G. Hinton. Factored 3-way restricted boltzmann machines for modeling\nnatural images. In Proc. AISTATS, 2010.\n[35] M. Ranzato, V. Mnih, and G. Hinton. Generating more realistic images using gated mrf’s. In Proc. NIPS,\n2010.\n[36] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning,\n2008.\n[37] J. Susskind, R. Memisevic, G. Hinton, and M. Pollefeys. Modeling the joint density of two images under\na variety of transformations. In Proc. CVPR, 2011.\n[38] K. Swersky, D. Tarlow, I. Sutskever, R. Salakhutdinov, R. Zemel, and R. Adams. Cardinality restricted\nboltzmann machines. In Proc. NIPS, 2012.\n[39] Y. Tang and A. Mohamed. Multiresolution deep belief networks. In Proc. AISTATS, 2012.\n[40] T. Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In\nProc. ICML, 2008.\n[41] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent\nmagnitude. COURSERA: Neural Networks for Machine Learning, 2012.\n[42] T. Trzcinski, M. Christoudias, P. Fua, and V. Lepetit. Boosting binary image descriptors. Technical report,\nEPFL, 2012.\n[43] T. Trzcinski, M. Christoudias, V. Lepetit, and P. Fua. Learning image descriptors with the boosting-trick.\nIn Proc. NIPS, 2012.\n[44] T. Trzcinski and V. Lepetit. Efﬁcient discriminative projections for compact binary descriptors. In Proc.\nECCV, 2012.\n[45] A. Vedaldi and B. Fulkerson. Vlfeat: An open and portable library of computer vision algorithms. In\nProceedings of the International Conference on Multimedia, 2010.\n[46] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing robust features with\ndenoising autoencoders. In Proc. ICML, 2008.\n[47] S. Winder and M. Brown. Learning local image descriptors. In Proc. CVPR, 2007.\n9\n6\nAppendix\n6.1\nGaussian-Binary Restricted Boltzmann Machine\nThe Gaussian-Binary Restricted Boltzmann Machine (GRBM) is an extension of the Binary-Binary\nRBM [11] that can handle continuous data [15, 39]. It is a bipartite Markov Random Field over a\nset of visible units, v ∈RNv, and a set of hidden units, h ∈{0, 1}Nh. Every conﬁguration of units\nv and units h is associated with an energy E(v, h), deﬁned as\nE(v, h; θ) = 1\n2vT Λv −vT Λa −hT b −vT ΛW h\n(1)\nwith θ = (W ∈RNv×Nh, a ∈RNv, b ∈RNh, Λ ∈RNv×Nv), the model parameters. W rep-\nresents the visible-to-hidden symmetric interaction terms, a and b represent the visible and hidden\nbiases respectively and Λ is the precision matrix of v, taken to be diagonal. E(v, h) induces a\nprobability density function over v and h:\np(v, h; θ) = exp\n\u0000−E(v, h; θ)\n\u0001\nZ(θ)\n(2)\nwhere Z(θ) is the normalization partition function, Z(θ) =\nR P\nh exp\n\u0000−E(v, h; θ)\n\u0001\ndv.\nLearning the parameters θ is accomplished by gradient ascent in the log-likelihood of θ given N\ni.i.d. training samples. The log-probability of one training sample is\nlog p(v) = −1\n2vT Λv + vT Λa +\nNh\nX\nj\nlog\n \n1 + exp\n Nv\nX\ni\nvT\ni (Λ\n1\n2 W )ij + bj\n!!\n−Z(θ)\n(3)\nEvaluating Z(θ) is intractable, therefore algorithms like Contrastive Divergence (CD) [13] or per-\nsistent CD (PCD) [40] are used to compute an approximation of the log-likelihood gradient. The\nbipartite nature of an (G)RBM is an important aspect when using these algorithms: The visible units\nare conditionally independent given the hidden units. They are distributed according to a diagonal\nGaussian:\np(v | h) ∼N(Λ−1\n2 W h + a, Λ−1)\n(4)\nSimilarly, the hidden units are conditionally independent given the visible units. The conditional\ndistribution can be written compactly as\np(h | v) = σ(vT Λ\n1\n2 W + b)\n(5)\nwhere σ denotes the element-wise logistic sigmoid function, σ(z) = 1/(1 + e−z).\n6.2\nSparse GRBM\nIn many tasks it is beneﬁcial to have features that are only rarely active [29, 8]. Sparse activation of a\nbinary hidden unit can be achieved by specifying a sparsity target ρ and adding an additional penalty\nterm to the log-likelihood objective that encourages the actual probability of unit j of being active,\nqj, to be close to ρ [29, 14]. This penalty is proportional to the negative KL divergence between the\nhidden unit marginal qj = 1\nN\nP\nn p(hj = 1 | vn) and the target sparsity:\nλsp\n\u0000ρ log qj + (1 −ρ) log(1 −qj)\n\u0001\n,\n(6)\nwhere λsp represents the strength of the penalty. This term enforces sparsity of feature j over the\ntraining set, also referred to as lifetime sparsity. The hope is that the features for one training sample\nare then encoded by a sparse vector, corresponding to population sparsity. We denote a GRBM with\na sparsity penalty λsp > 0 as spGRBM.\n10\n6.3\nMean-Covariance Restricted Boltzmann Machine\nIn order to model pairwise dependencies of visible units gated by hidden units, a third-order RBM\ncan be deﬁned with a weight wijk for each triplet vi, vj, hk. By factorizing and tying these weights,\nparameters can be reduced to a ﬁlter matrix C ∈RNv×F and a pooling matrix P ∈RF ×Nh. C\nconnects the input to a set of factors and P maps factors to hidden variables. The energy function\nfor this cRBM [34] is\nEc(v, hc; θ) = −(vT CT )2P hc −cT hc\n(7)\nwhere (·)2 denotes the element-wise square operation and θ = {C, P , c}. Note that P has to be\nnon-positive [34, Section 5]. The hidden units of the cRBM are still conditionally independent given\nthe visible units, so inference remains simple. Their conditional distribution (given visible state v)\nis\np(hc | v) = σ(P T (CT v)2 + c)\n(8)\nThe visible units are coupled in a Markov Random Field determined by the setting of the hidden\nunits:\np(v | hc) ∼N(0, Σ)\n(9)\nwith\nΣ−1 = Cdiag(−P hc)CT\n(10)\nAs equation 9 shows, the cRBM can only model Gaussian inputs with zero mean. For general\nGaussian-distributed inputs the cRBM and the GRBM can be combined into the mean-covariance\nRBM (mcRBM) by simply adding their respective energy functions:\nEmc(v, hm, hc; θ, θ′) = Em(v, hm; θ) + Ec(v, hc, θ′)\n(11)\nEm(v, hm; θ) denotes the energy function of the GRBM (see eq. 1) with Λ ﬁxed to the identity\nmatrix. The resulting conditional distribution over the visible units, given the two sets of hidden\nunits hm (mean units) and hc (covariance units) is\np(v | hm, hc) ∼N(ΣW hm, Σ)\n(12)\nwith Σ deﬁned as in eq. 10. The conditional distributions p(hm|v) and p(hc|v) are still as in eq. 5\nand eq. 7 respectively. The parameters θ, θ′ can again be learned using approximate Maximum\nLikelihood Estimation, e.g. via CD or PCD. These methods require to sample from p(v|hm, hc),\nwhich involves an expensive matrix inversion (see eq. 10). Instead, samples are obtained by using\nHybrid Monte Carlo (HMC) [31] on the mcRBM free energy [33].\n11\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2013-01-14",
  "updated": "2013-04-25"
}