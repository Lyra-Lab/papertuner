{
  "id": "http://arxiv.org/abs/1301.5734v1",
  "title": "Reinforcement learning from comparisons: Three alternatives is enough, two is not",
  "authors": [
    "Benoit Laslier",
    "Jean-Francois Laslier"
  ],
  "abstract": "The paper deals with the problem of finding the best alternatives on the\nbasis of pairwise comparisons when these comparisons need not be transitive. In\nthis setting, we study a reinforcement urn model. We prove convergence to the\noptimal solution when reinforcement of a winning alternative occurs each time\nafter considering three random alternatives. The simpler process, which\nreinforces the winner of a random pair does not always converges: it may cycle.",
  "text": "arXiv:1301.5734v1  [math.OC]  24 Jan 2013\nReinforcement learning from comparisons:\nThree alternatives is enough, two is not.\nBenoˆıt Laslier\nInstitut Camille Jordan, Universit´e Claude Bernard Lyon 1,\n43 boulevard du 11 novembre 1918, 69622 Villeurbanne, France\nJean-Fran¸cois Laslier\nCNRS and ´Ecole polytechnique,\n91128 Palaiseau, France\njean-francois.laslier@polytechnique.edu\nOctober 11, 2018\nAbstract\nThe paper deals with the problem of ﬁnding the best alternatives\non the basis of pairwise comparisons when these comparisons need not\nbe transitive. In this setting, we study a reinforcement urn model.\nWe prove convergence to the optimal solution when reinforcement of\na winning alternative occurs each time after considering three random\nalternatives.\nThe simpler process, which reinforces the winner of a\nrandom pair does not always converges: it may cycle.\n1\nIntroduction\nIn a pairwise comparison problem, one is given a set of alternatives, with\ndata about how they compare the ones to the others. In its purest form, on\nwhich we focus in the present paper, we simply have, for any pair of distinct\nalternatives, the information of which one “beats” the other. Such a data\nset is called a tournament. Basic results on this structure can be found in\nMoon [18].\nFor logical as well as practical reasons, binary relations are at the basis\nof choice theory. Two classical examples are sport competition and majori-\ntarian politics. Many sports involve by deﬁnition two players (or teams),\nso that competition among any number of players must take the form of\n1\nthe aggregation of pairwise comparisons. In majority voting, a candidate is\nsocially preferred to another if and only if a majority of the voters prefer the\nformer to the latter. More generally, the prevalence of that kind of binary\nrelations can be traced back to speciﬁc features of eﬃcient natural languages\n(Rubinstein [25]).\nIf a chess player beats all the other players, he or she is clearly the best.\nIf a candidate cannot be defeated under majority rule by any challenger,\nthat “Condorcet” candidate can claim to be the best according to majority\nrule.1 But if no alternative beats all the others, it is not clear how to deﬁne\nthe best alternatives. The problem of choosing from pairwise comparisons\nhas thus attracted the attention of scholars in various ﬁelds, most often from\nthe axiomatic, normative, point of view (David [5]; Fishburn [6]; Laslier [13];\nBrandt et al. [1]).\nIn the present paper we tackle the same problem from an evolutionary\nperspective instead of an axiomatic one. We consider dynamic processes\naccording to which, at each period in time, a small number (2 or 3) of alter-\nnatives are sampled, the tournament is played among these few alternatives,\nand the winning alternative is reinforced in the sense that it will be sam-\npled with higher probability in the future. Where does such a mechanical\nadaptive process go?\nUsing a standard urn model, where reinforcing an\nalternative is adding a colored ball to an urn, we obtain two results.\n(i) If one samples three alternatives (distinct or not) at each date, the\nprocess is able to discover the optimal solution of the tournament, that is\nthe unique probability distribution p∗which is, in expectation, defeated by\nno alternative. With probability one, the composition of the sampling urn,\nwhich deﬁnes the probability pτ of choosing the various alternatives at time\nτ, tends to p∗when τ tends to inﬁnity.\n(ii) If one samples only two alternatives at each date, the process is not\nable to discover the optimal solution, unless the solution is degenerated,\nwith one alternative defeating all the others.\nWith probability one, the\ncomposition of the sampling urn, which gives the probability pτ of choosing\nthe alternatives, concentrates on the support of the optimal solution p∗.\nThis means that all the alternatives which are played with zero probability\nin the optimal solution are chosen with a probability going to 0. However the\ncomposition of the urn may cycle, staying away from the optimal solution.\nIn some cases we even prove that it cycles with probability one.\nThe negative result (ii) echoes known results about the evolutionary\ninstability of mixed equilibria in evolutionary game theory.\nFor instance\n1This observation does not imply that the majority principle is good for Politics.\n2\ncycling with probability one is proven by Posch [23] in a reinforcement urn\nmodel for 2 × 2 games.\nThe positive result (i) seems more original.\nIn a study of imitation\nprocesses in Matching Pennies games, Hofbauer and Schlag [11] observe that\nplayers end up closer to the equilibrium if they sample several individual\nbefore imitating: there is still cycling, but closer to the equilibrium. Our\nresults might be interpreted in the same spirit: learning slower leads to\nmore stability.\nThe techniques we use to derive these results are standard in the ﬁeld\nof adaptive processes with reinforcement; see Pemantle 2007 [22], and they\nbelong to the family of martingales techniques. The main ingredient of the\nproof is the deﬁnition of a well chosen function of the process whose values\nform a martingale (see (11)). We use the convergence theorem for positive\nmartingales to obtain some global asymptotic information about the process.\nIn the case of three alternatives we get fairly directly the convergence of the\nprocess while for the case of two alternatives the convergence theorem has\nto be complemented with a variance analysis to prove the non-convergence.\nThe paper is organized as follows. Section 2 introduces the necessary\nnotions about tournaments: deﬁnition and notation (2.1), the Markov chain\ninduced by the play of small-size tournaments at each date (2.2), the tour-\nnament game which allows to deﬁne and to prove existence of the optimal\nsolution (2.3), and some further preliminary material (2.4, 2.5). Section 3\nstarts by the deﬁnition of urns and of the adaptive processes (3.1). Then,\nin order to illustrate the argument in a simple way, a toy example is intro-\nduced and treated according to its deterministic approximation (3.2). The\nstatement and proof of our main result on three-alternatives reinforcement is\nfound in (3.3) and two-alternatives reinforcement is treated in (3.4), before\na short conclusion (3.5).\n2\nFramework\n2.1\nTournaments\nLet X be a ﬁnite set. A tournament T on X is a complete and antisym-\nmetric binary relation. For any x and y in X, one and only one of the three\npossibilities occurs: x = y, x T y, or y T x. When x T y we often say that x\nbeats y. Deﬁne the sets :\nT +(x) = {y ∈X : xTy}, T −(x) = {y ∈X : yTx}.\n(1)\n3\nThe binary relation T is ﬁxed throughout this paper. It is sometimes easier\nto use the notation:\nmax{x, y} =\n\u001a x\ny\nif x T y or x = y\nif y T x.\nAn alternative which beats all other alternatives is called a Condorcet win-\nner. A tournament can have a Condorcet winner or not, but cannot have\ntwo. The Top-Cycle of the tournament T is the smallest (by inclusion) set\nY ⊆X such that:\n∀x ∈X \\ Y , ∃y ∈Y : yTx.\nIt is easily seen that such a set is unique and reduces to a singleton {c}\nif and only if c is a Condorcet winner. The literature on tournaments and\nformal political science has shown that the Top-Cycle is usually a very large\nset (McKelvey [17]), and has proposed many reﬁnements of this set (see [13]\nfor a survey).\n2.2\nA Markov chain\nLet ∆(X) be the set of probability distributions on X and let p ∈∆(X).\nThe support of p is denoted by Supp(p). Given p, deﬁne a sequence (p[t])t∈IN\nof probability distributions on X derived from p in the following way :\np[0] = p,\n(2)\np[t+1](x) = p[t](x) · p\n\u0000T +(x) ∪{x}\n\u0001\n+ p[t](T +(x)) · p(x),\n(3)\nfor any t ⩾0, for any x ∈X\nThe interpretation is that p[t] is the distribution of a random variable\nξ(t) ∈X such that ξ(0) is chosen at random according to p and then, given\nthat ξ(t) = x, ξ(t + 1) is the winner (according to T) of the comparison\nbetween x and some alternative y randomly chosen in X according to p.\nTherefore ξ(t + 1) = x either because ξ(t) was already equal to x and y was\nchosen in T +(x)∪{x} (ﬁrst term in the above formula), or because ξ(t) was\nin T +(x) and x was chosen according to p (second term). We call p the\n“sampling” probability.\nThis process is usually considered with p uniform on X (Daniels [4],\nUshakov [28], Levchenkov [16], Slutzky and Volij [27], Chebotarev and\nShamis [2, 3]). We need the general version because, later in this paper,\np will be endogenous.\n4\nGiven p, the stationary distribution for this ﬁnite Markov chain exists\nand is unique;2 we denote it by p[∞]. It is characterized by the fact that\nSupp(p[∞]) ⊆Supp(p) and, for any x in Supp(p),\np[∞](T +(x)) · p(x) = p[∞](x) · p(T −(x)).\n(4)\nNotice that the inclusion Supp(p[∞]) ⊆Supp(p) may be strict; indeed,\np[∞](x) = 0 when p(T −(x)) = 0, that is when x beats no alternative in\nthe support of p. More exactly, Supp(p[∞]) is the Top-Cycle of the restric-\ntion of T to Supp(p); thus Supp(p[∞]) does not exactly really depends on\np but only on Supp(p). If p has full support, for instance in the usual case\nwhere p is uniform, Supp(p[∞]) = TC(T).\n2.3\nThe tournament game\nThe tournament game is the two-player, symmetric, zero-sum game deﬁned\nby the strategy set X and the payoﬀfunction g(x, y) = +1 if x T y, g(x, y) =\n0 if x = y, and g(x, y) = −1 if y T x.\nFor p, q ∈∆(X) two probability\ndistributions on X, write:\ng(p, q) =\nX\ny∈X\ng(x, y)p(x)q(y).\n(5)\nFrom the deﬁnition, g is clearly antisymmetric: g(q, p) = −g(p, q).\nThe tournament game has been studied by graph theorists (Ficher and\nRyan [8, 9, 10]) and has more recently attracted attention of computer sci-\nentists (Rivest and Chen [24]). As a model of majority voting and two-party\nelectoral competition, it studied in Social Choice theory and formal Political\nScience (Moulin [19], Myerson [20, 21], Laslier [14, 15]). Remarkably, such a\ngame has a unique equilibrium. Here is the precise result that will be needed\nin the sequel. (Fisher and Ryan [9] prove this result using linear algebra and\nLaﬀond et al. [12] have a direct proof using a parity argument.)\nProposition 1 There exists a unique p∗∈∆(X) such that g(p∗, q) ⩾0 for\nall q ∈∆(X). This p∗, called the optimal strategy, is also characterized by\nthe following : for all x ∈X,\np∗(x) > 0 ⇐⇒g(x, p∗(x)) = 0\np∗(x) = 0 ⇐⇒g(x, p∗(x)) < 0.\n2We state the results in this section without proofs. They are easily derived from\nelementary theory of ﬁnite Markov chains and have already been noticed for p uniform in\nthe mentioned references.\n5\nThe support of the optimal strategy is called the Bipartisan Set of the\ntournament: Supp(p∗) = BP(T). This set is a subset of the Top Cycle and\nthe inclusion is often strict. For instance, in totally random tournaments,\nthe Top Cycle contains all the alternatives and the Bipartisan Set contains\nonly half of them (Fisher and Reeves [7]).\n2.4\nTwo formulas\nBefore we go further and explain the relation between the game optimal\nstrategy and stationary probabilities, it is useful to state two technical for-\nmulas. The following lemma describes the probabilities p[2] and p[3], obtained\nafter sampling two or three alternatives with the Markov chain deﬁned in\nSection 2.2, in term of the payoﬀfunction g.\nLemma 2 For any x ∈X:\np[2](x) = p(x) · (1 + g(x, p)),\np[3](x) = p(x) ·\n\n1 + 3\n2g(x, p) + 1\n2g(x, p)2 +\nX\ny∈X\np(y)g(x, y)g(y, p)\n\n.\nProof. First let us notice a useful equality. By deﬁnition (1) and (5)\ng(x, p) = p(T +(x)) −p(T −(x)),\n(6)\nand, since p(T +(x)) + p(T −(x)) + p(x) = 1, we get:\n1 + g(x, p) = 2p(T +(x)) + p(x).\n(7)\nLet a and b be chosen according to p and let x = max{a, b}, then:\np[2](x) = Pr[a = x] · Pr[b ∈T +(x) ∪{x}] + Pr[a ∈T +(x)] · Pr[b = x]\n= p(x) ·\n\u00002p\n\u0000T +(x)\n\u0001\n+ p(x)\n\u0001\n= p(x) · (1 + g(x, p)) .\nFor the second formula, let a, b and c be chosen according to p. An\nalternative x appears as x = max {max{a, b}, c} in the two exclusive cases:\nx = max{a, b}, and x = max{x, c} .\nx T max{a, b}, and x = c .\n6\nIn the ﬁrst line, the event x = max{a, b} has probability p[2](x) = p(x)·(1+\ng(x, p)) so the probability of the ﬁrst case is p(x) · (1 + g(x, p)) · p(T +(x) ∪\n{x}). In the second line, the event x T max{a, b} has probability\np[2](T +(x)) =\nX\ny∈T +(x)\np[2](y) =\nX\ny∈T +(x)\np(y)(1 + g(y, p)),\ntherefore the probability p[3] is:\np[3](x) = p(x) · (1 + g(x, p)) ·\n\np(x) +\nX\ny∈T +(x)\np(y)\n\n+ p(x)\nX\ny∈T +(x)\np(y)(1 + g(y, p))\n= p(x)2 [1 + g(x, p)] + p(x)\nX\ny∈T +(x)\np(y) [(2 + g(x, p)) + g(y, p)] .\nUsing the fact that 1+g(x,y)\n2\nis 1 if y ∈T +(x), is 1/2 if y = x, and is 0 if not,\none ﬁnds:\np[3](x)\np(x)\n=\nX\ny\np(y) [(2 + g(x, p)) + g(y, p)] 1 + g(x, y)\n2\n= 1\n2\nX\ny\np(y) [(2 + g(x, p)) + g(y, p) + 2g(x, y) + g(x, p)g(x, y) + g(y, p)g(x, y)]\n= 1 + 1\n2g(x, p) + 1\n2g(p, p) + g(x, p) + 1\n2g(x, p)2 + 1\n2\nX\ny\np(y)g(y, p)g(x, y)\n= 1 + 3\n2g(x, p) + 1\n2g(x, p)2 + 1\n2\nX\ny\np(y)g(y, p)g(x, y),\nwhich is the announced formula.\nQ.E.D.\n2.5\nRelation between optimal strategies and stationary prob-\nabilities\nWe ﬁrst observe that the game optimal strategy p∗satisﬁes a nice ﬁxed-\npoint property if we take p[1] = p∗as the sampling probability to build the\nMarkov chain, and that only an optimal strategy can be such a ﬁxed point.\nProposition 3 Let p∗be the optimal strategy for the tournament game,\nthen (p∗)[2] = (p∗)[∞] = p∗. Conversely, let p be such that p[2] = p, then p is\nthe optimal strategy for the tournament game restricted to the support of p.\n7\nProof. By lemma 2, p∗[2](x) = p∗(x)(1 + g(x, p∗), and, by proposition 1,\neither p∗(x) = 0 or g(x, p∗) = 0.\nConversely if p[2](x) = p(x) = p(x)(1 + g(x, p)) then g(x, p) = 0 as soon\nas p(x) > 0 and p is the optimal strategy on its support.\nQ.E.D.\n3\nLearning\nWith the previous background material in mind, we turn to the main result\nof this paper. Instead of considering re-sampling at each date according to\na constant probability distribution, as is done in the previously described\nMarkov chains, we describe learning processes where winning alternatives\nare reinforced at the level of the sampling probability. These processes can\nbe implemented with random urns.\n3.1\nChoice by reinforcement\nAn urn on X is a list n of strictly positive integers n(x), x ∈X. The integer\nn(x) is the “number of balls of color x in the urn n.” The set of such urns\non X is denoted by N, formally:\nN = INX\n+.\nTo each n ∈N is associated the probability distribution en on X deﬁned by\nen(x) =\nn(x)\nP\ny∈X n(y).\nWhen we write that the alternative x is picked in the urn n, we mean that\nx is picked in X according to the probability en.\nA random urn sequence is a sequence Uτ, τ ∈IN of random variables on\nN such that Uτ+1 is deﬁned conditionally on Uτ. Here are three examples:\n1. Two-alternatives reinforcement. Given a realization nτ ∈N of Uτ, an\nalternative x is picked in X according to the probability distribution\nf\nnτ\n[2], and one ball of color x is added to the urn: nτ+1(w) = nτ(w)+1\nand for all v ̸= w, nτ+1(v) = nτ(v). This means that two alternatives,\nsay a and b are picked independently in the urn nτ, and are compared\naccording to T. The result of the comparison is x = max{a, b}, that is:\nx = a if a = b or if a T b and x = b if b T a. Alternative x is reinforced.\n8\n2. Three-alternatives reinforcement. Same thing as above, with the prob-\nability distribution f\nnτ\n[3]. This means that three alternatives, say a,\nb and c are picked independently in X according to nτ; a, b and\nc are compared according to T in sequence and one ball of color\nx = max {max{a, b}, c} is added to the urn. Remark that there are\nonly two cases : ranked alternatives where we reinforce the top one or\na cycle where we reinforce at random.\n3. Fast reinforcement. Same thing as above, with the probability distri-\nbution f\nnτ\n[∞], the stationary distribution for T when sampling is done\naccording to f\nnτ.\nRemark that the ﬁrst two examples can be concretely implemented easily,\nas described, but fast reinforcement cannot.\n3.2\nA motivational example\nThis section presents a simple non-rigorous argument to justify our focus on\nthree-alternatives reinforcement. Consider the simplest possible non trivial\ntournament : a cycle of three alternatives A, B and C with A T B, B T C,\nC T A. In order to evaluate the long term behavior of two-alternatives and\nthree-alternatives reinforcement, we use a deterministic continuous time mo-\ntion corresponding to the limit of a large number of balls in the urn. We\nwrite a(t), b(t) and c(t) the “number” of balls of each type and ˜a(t) = a(t)/t,\n˜b(t) = b(t)/t and ˜c(t) = c(t)/t the corresponding probabilities.\nFor two-alternatives reinforcement we get :\na′\n=\n˜a2 + 2˜a˜c\nb′\n=\n˜b2 + 2˜b˜a\n(8)\nc′\n=\n˜c2 + 2˜c˜b\nand we remark that\nd\ndt\n\u0010\nln ˜a + ln˜b + ln ˜c\n\u0011\n=\nd\ndt (−3 ln t + ln a + ln b + ln c)\n=\n−3\nt + ˜a2 + 2˜a˜c\na\n+ . . .\n=\n−3\nt + ˜a + 2˜c\nt\n+\n˜b + 2˜a\nt\n+ ˜c + 2˜b\nt\n=\n0\nso (˜a,˜b, ˜c) cannot converge to the optimal probability independently of the\nstate at ﬁnite time.\n9\nIt is then natural to study three-alternatives reinforcement, for which :\na′\n=\n˜a3 + 3˜a2˜c + 3˜a˜c2 + ˜a˜b˜c\nb′\n=\n˜b3 + 3˜b2˜a + 3˜b˜a2 + ˜a˜b˜c\n(9)\nc′\n=\n˜c3 + 3˜c2˜b + 3˜c˜b2 + ˜a˜b˜c\nand for the same quantity\nd\ndt\n\u0010\nln ˜a + ln˜b + ln ˜c\n\u0011\n=\n−3\nt + ˜a2 + 3˜a˜c + 3˜c2 + ˜b˜c\nt\n+ . . .\n=\n4(˜a2 + ˜b2 + ˜c2) + 4(˜a˜c + ˜b˜a + ˜c˜b) −3\nt\n=\n˜a2 + ˜b2 + ˜c2 −2(˜a˜c + ˜b˜a + ˜c˜b)\nt\nand simple calculus shows that this last term is positive expect for ˜a =\n˜b = ˜c = 1/3. Then ln ˜a + ln˜b + ln ˜c is an increasing negative function so it\nconverges. It is not diﬃcult to see, using the divergence of\nR\n1/tdt, that this\nimplies that ˜a2 +˜b2 + ˜c2 −˜a˜c −˜b˜a −˜c˜b converges to 0 and then that (˜a,˜b, ˜c)\nconverges to (1/3, 1/3, 1/3) (the details of the arguments will be given in\nthe rigorous proof of the next section).\nWith this example we can see that two-alternatives reinforcement should\nnot converge to the optimal probability even for a simple tournament and\nwhen we neglect the eﬀect of probabilistic noise while three-alternatives\nreinforcement seems to work in that case. In the next section we will prove\nthat three-alternatives reinforcement actually converges for any tournament.\nWe will use the same idea of computing the variation of ln ˜a+ln˜b+ln ˜c with\ntechnical changes for the general tournament, the discrete time and the\nprobabilistic evolution.\n3.3\nThree-alternatives reinforcement and martingale tech-\nnique\nWe will now prove the result about three-alternatives reinforcement:\nTheorem 4 For any initial urn n0 ∈N, the random urn sequence obtained\nby three-alternatives reinforcement is such that the realization nτ, τ ∈IN\nalmost surely veriﬁes:\nlim\nτ→∞f\nnτ = p∗.\nThe same proof will actually also give the ﬁrst part of result about two-\nalternatives reinforcement, which we thus state now:\n10\nTheorem 5 For any initial urn n0 ∈N, the random urn sequence obtained\nby two-alternatives reinforcement is such that the realization nτ, τ ∈IN\nalmost surely veriﬁes:\n∀x ∈X, p∗(x) = 0 ⇒lim\nτ→∞f\nnτ(x) = 0.\nThe proof relies mainly on the study of a well chosen function of the state\nof the urn. Let LD denotes a discrete logarithm: for integers 0 < a < b,\nLD[a, b] = −\nb−1\nX\ni=a\n1\ni .\n(10)\nRecall that at time τ ∈IN, nτ(w) denotes the number of w-balls in the urn.\nThe total number of balls is increasing by 1 at each time, so P\nw nτ(w) =\nA + τ.\nThe probability of drawing a w-ball is ˜nτ(w) = nτ(w)/(A + τ).\nConsider the quantity\nµτ =\nX\nw∈X\nLD[nτ(w), A + τ] · p∗(w),\n(11)\nthat is the expected value, according to the optimal probability p∗, of the\ndiscrete logarithm at time τ.\nProposition 6 For both two-alternatives and three-alternatives reinforce-\nment, the sequence µτ, τ ∈IN is a negative sub-martingale. More precisely\nwe have, for two-alternatives reinforcement:\nE [µτ+1 −µτ | nτ] = g(p∗, ˜nτ)\nA + τ\n.\nand for three-alternatives reinforcement:\nE [µτ+1 −µτ | nτ]\n=\n1\nA + τ\n \n1\n2g(p∗, ˜n) + 1\n2\nX\nw∈X\ng(w, ˜n)2p∗(w) +\nX\nv\n˜n(v)g(p∗, v)(1 + g(v, ˜n))\n!\n.\nProof. We will write p for ˜nτ and let i denote either 2 or 3. From τ to\nτ + 1, one and only one ball is added. This ball has type w with probability\n11\np[i](w). Thus:\nE [µτ+1 −µτ | nτ]\n= −\nX\nw∈X\np[i](w)\n\nX\nv̸=w\n1\nA + τ · p∗(v) +\n\u0014\n1\nA + τ −\n1\nnτ(w)\n\u0015\n· p∗(w)\n\n\n=\n−1\nA + τ +\nX\nw∈X\np[i](w)\n1\nnτ(w)p∗(w)\n=\n−1\nA + τ +\nX\nw∈X\np[i](w)\np(w)\np∗(w)\nA + τ\nWhere in the last line we used the deﬁnition p(w) = nτ(w)/(A + τ). Using\nthe formula for p[i] of lemma 2, it comes, for two-alternatives:\nE [µτ+1 −µτ | nτ]\n=\n−1\nA + τ +\nX\nw∈X\n(1 + g(w, p)) p∗(w)\nA + τ\n= g(p∗, p)\nA + τ .\nwhich is always non-negative. Furthermore, g(p∗, p) = 0 implies, by the ﬁrst\npart of proposition 1, that Supp(p) ⊆Supp(p∗).\nFor three-alternatives, we have:\nE [µτ+1 −µτ | nτ]\n=\n−1\nA + τ +\nX\nw∈X\n \n1 + 3\n2g(w, p) + 1\n2g(w, p)2 +\nX\nv\np(v)g(w, v)g(v, p)\n!\np∗(w)\nA + τ .\none can re-arrange:\n(A + τ)E [µτ+1 −µτ | nτ]\n= 3\n2g(p∗, p) + 1\n2\nX\nw∈X\ng(w, p)2p∗(w) +\nX\nv\np(v)g(p∗, v)g(v, p)\n= 1\n2g(p∗, p) + 1\n2\nX\nw∈X\ng(w, p)2p∗(w) +\nX\nv\np(v)g(p∗, v)(1 + g(v, p)).\nAnd all the terms in this sum are non-negative. The sum can be 0 only if\nboth Supp(p) ⊆Supp(p∗), and g(w, p) = 0 for all w in the support of p∗,\nwhich implies p = p∗by the uniqueness in Proposition 1.\nQ.E.D.\n12\nWe are now able to prove the two results of the beginning of the section.\nProof of Theorems 4 and 5. We consider for this proof either two or\nthree alternatives reinforcement. We have:\nE[µτ] = E[µ0] + E\n\" τ\nX\nt=1\nµt −µt−1\n#\n(12)\n= E[µ0] + E\n\" τ\nX\nt=1\nE [µt −µt−1|µt−1]\n#\n.\n(13)\nBy Proposition 6, µτ is a negative sub-martingale, so it converges almost\nsurely to an integrable random variable µ∞(see Corollary VII.4.1 and\nVII.4.2 in [26]). Furthermore, the right hand side is an increasing nega-\ntive sequence so it converges to a ﬁnite value. In the left hand side, the sum\nis an increasing sequence of positive random variables so by the monotonous\nconvergence theorem (Theorem II.6.1 in [26]) we can take the limit inside\nthe expectation.\nHence E [P∞\nt=1 E[µt −µt−1|µt−1]] = lim E[µτ] −E[µ0] is\nﬁnite and so P∞\nt=1 E[µt −µt−1|µt−1] is almost surely ﬁnite.\nLet f [2](p) = g(p∗, p) and f [3](p) = g(p, p∗) + P\nw∈X g(w, p)2p∗(x). The\nsimplex ∆(X) is embedded in IRX so we use the L∞distance. With this\ndistance, f [i] is continuous and d(f\nnτ, ]\nnτ+1) ⩽\n1\nA+τ almost surely. Denote by\nB(p, η) the ball of center p and radius η.\nNow consider a single realization of the urn process. Since X is a ﬁnite\nset, ∆(X) is compact, so let f\nn∞be an accumulation point for f\nnτ. We will\nshow that necessarily f [i]( f\nn∞) = 0. Looking for a contradiction, suppose\nf [i]( f\nn∞) > 0.\nSince f [i] is continuous, let ǫ, η > 0 be such that ∀p ∈B( f\nn∞, η), f [i](p) >\nǫ. Let φ be a sub-sequence such that ∀τ, ]\nnφ(τ) ∈B( f\nn∞, η/2) and φ(τ +1) >\n(1 + η)φ(τ). Then:\n∞\nX\nt=0\nE[µt+1 −µt|nt] ⩾\n∞\nX\nt=0\n1\n2(A + t)f [i]( ent)\n(14)\n⩾\n∞\nX\nτ=0\n1\n2(A + φ(τ))\n⌊(1+η/2)φ(τ)⌋\nX\nt=φ(τ)\nf [i]( ent)\n(15)\n⩾\n∞\nX\nτ=0\n1\n2(A + φ(τ))⌊(1 + η/2)φ(τ)⌋ǫ\n(16)\nThe right hand side of the last line is inﬁnite.\nWe already proved that\nthe left hand side is almost surely ﬁnite. This contradiction proves that all\n13\nthe accumulation points of ent are zeros of f [i]. It follows that f [i]( ent) →0\nalmost surely. We have seen in the proof of Proposition 6 that this fact\nimplies exactly the theorems.\nQ.E.D.\n3.4\nTwo-alternatives reinforcement and variance estimates\nIn this section, we study in detail the two-alternatives reinforcement. The\nmain idea is to study the variance of µ∞conditionally on the state at a large\ntime t.\nTheorem 7 For any initial urn n0 ∈N, the random urn sequence obtained\nby two-alternatives reinforcement is such that:\n1. almost surely, for all alternatives x such that p∗(x) = 0, ˜nτ(x) →0\nwhen τ →∞;\n2. with positive probability, the realized sequence ˜nτ, τ ∈IN has no limit\nas τ →∞;\n3. if T is such that ∀x, p∗(x) > 0 (in other words, BP(T) = X) then,\nwith probability one, ˜nτ, τ ∈IN has no limit.\nTo simplify notation, in this section we let the process start at τ ̸= 0\nso that τ always denote the number of ball in the urn (i.e. A = 0). We\nwill also only consider two-alternatives reinforcement in this section. Recall\nthe piece of notation Supp(p∗) = BP.\nAlso recall from the last section\nthat µτ is a negative submartingale so it has an almost sure limit µ∞. Let\nφ = P\nx∈BP p∗(x) log p∗(x) be the value of µ∞when ˜nτ converges to p∗.\nThe ﬁrst point is the following variance estimate :\nLemma 8 Let τ > 0 and let ǫτ(x) = p∗(x)/˜nτ(x) −1. We have\nE[(µτ+1 −µτ)2|Fτ] = 1\nτ 2\nX\nx∈BP\n˜n[2](x)ǫτ(x)2\n(17)\nProof. This is a straightforward computation :\nE[(µτ+1 −µτ)2|Fτ] =\nX\nx\n˜n[2]\nτ (x)\n\u0000−\nX\ny\np∗(y)1\nτ + p∗(x)\n1\nn(x)\n\u00012\n(18)\n=\nX\nx\n˜n[2]\nτ (x) 1\nτ 2 (−1 + p∗(x)\n˜n(x) )2\n(19)\n= 1\nτ 2\nX\nx\n˜n[2](x)ǫτ(x)2\n(20)\n14\nQ.E.D.\nThe main point is the factor\n1\nτ 2 that make the series of those terms\nsummable (once a small control on ǫ is provided). Thus the variance of µ∞\nconditioned on Fτ will be of order ǫ2/τ. Thus with hight probability µ∞\nwill be close to µτ so that if µτ is far enough from φ, then µ∞̸= φ.\nWe will ﬁrst consider the case where BP ̸= X. In this case we have\nE[µτ+1−µτ | Fτ] = g(p∗,˜nτ)\nτ\n⩾g0·˜n(BP c) > 0 (where g0 = infy∈BP c g(p∗, y))\nso we need an estimate of ˜n(BP).\nLemma 9 Suppose that there exists π ∈(0, 1) such that, at each time τ the\nprobability of adding a ball in BP c is inferior to π · ˜nτ(BP c). Then we have\nfor all τ ⩾τ0,\nE[˜nτ(BP c) | Fτ] ⩽\nτY\nt=τ0+1\nt + π −1\nt\nE[˜nτ0]\n⩽\n\u0010τ0\nτ\n\u00111−π\n˜nτ0.\nProof. The ﬁrst line comes from a straightforward induction\nE[˜nτ] = 1\nτ E[nτ−1] + 1\nτ E[nτ −nτ−1]\n= τ−1\nτ E[˜nτ−1] + 1\nτ E\n\u0002\nE[nτ −nτ−1 | Fτ−1]\n\u0003\n⩽τ−1\nτ E[˜nτ−1] + 1\nτ E[π˜nτ−1]\n⩽τ+π−1\nτ\nE[˜nτ−1]\nand for the second line we have\nlog\nτY\nt=τ0+1\nt + π −1\nt\n⩽\nτ\nX\nτ0+1\nlog(1 + π −1\nt\n)\n⩽\nτ\nX\nτ0+1\nπ −1\nt\n⩽(π −1) log( τ\nτ0\n)\nQ.E.D.\nNow we are able to prove the third point of the theorem.\nProof of theorem 7, case BP c ̸= ∅. Remark that for δ > 0 small\nenough and τ0 big enough, the set\nS = {n ∈N s.t n(X) ⩾τ0 and |φ −\nX\nx\np∗(x)LD(n(x), n(X))| ⩽δ}\n15\nveriﬁes :\n• ∀n ∈S, ∀x ∈BP, |p∗(x)/˜n(x) −1| ⩽1\n• ∀n ∈S, ∀x ∈BP c, 1 + g(x, ˜n) ⩽π\nfor some π > 1 −infx∈BP c g(p∗, x).\nLet us assume that ˜nτ0 ∈S, (which clearly happens with positive prob-\nability). Let T be the ﬁrst time after τ0 such that ˜nτ /∈S. T is a stopping\ntime so µτ∧T is still a submartingale, let us call it µ′\nτ. Furthermore, up to\ntime T, we have\nE[µτ+1 −µτ | Fτ] = g(p∗, ˜nτ)\nτ\n⩽˜nτ(BP)\nτ\n,\nand thus\nE[µ′\nτ+1 −µ′\nτ | Fτ0] ⩽1\nτ E[˜nτ(BP)1{T⩽τ} | Fτ0]\n⩽τ 1−π\n0\nτ pi−2˜nτ0(BP).\nNow let µ′\n∞denote the almost sure limit of µτ∧T = µ′\nτ. Remark that if\n|φ −µ∞|(ω) < δ then T(ω) = ∞and thus µ∞(ω) = µ′\n∞(ω). It is therefore\nenough to show that, with positive probability φ −δ < µ′\n∞< φ.\nNote that µ′ is a bounded submartingale, so it also converges in L1 and\nL2 toward µ′\n∞. Thus\nE[µ′\n∞−µ′\nτ0 | Fτ] =\n∞\nX\nt=τ0\nE[µ′\nt+1 −µ′\nt | Fτ0]\n⩽\n∞\nX\nt=τ0\nτ 1−π\n0\ntpi−2˜nτ0(BP)\n⩽C˜nτ0(BP),\n16\nand\nVar(µ′\n∞| Fτ0) ⩽\n∞\nX\nt=τ0\nVar(µ′\nt+1 | Fτ0)\n⩽\n∞\nX\nt=τ0\nE[(µ′\nt+1 −µ′\nt)2 | Fτ0]\n⩽\n∞\nX\nt=τ0\n1\nt2\nX\nx∈BP\n˜n[1](x)ǫt(x)2\n⩽C 1\nτ0\n.\nFinally consider a τ0 large enough so that 1\nτ0 ≪δ. It is clear that with a\npositive probability, ˜nτ0 ≪δ and |µτ0 −φ| is close to δ/2. Under this event\nwe see that |φ −µ′\n∞| is a random variable with expectation close to δ/2 and\nvariance small with respect to δ so |µ′\n∞−φ| has a positive probability to be\nin [δ/4, 3δ/4] which proves the theorem.\nQ.E.D.\nNow we turn to the case where X = BP. The idea will be similar, with\nLemma 8 being the core argument.\nThe main simpliﬁcation comes from\nthe fact that in this case g(p∗, p) = 0 for all p so µ is a martingale and\nLemma 9 will no longer be needed. However, in order to prove that µ∞is\nalmost surely diﬀerent from φ, we will need an almost sure lower bound on\n|φ −µτ| which will come from a careful analysis of the diﬀerence between\ndiscrete and real logarithms. Finally since the almost sure bound that we\nwill get will be much worse than the one we were able to have with positive\nprobability, we will need to be more careful in our use of Lemma 8.\nFirst recall the following well known approximation result :\nProposition 10 Let k > 0, there exists a constant γ (Euler’s constant)\nsuch that\nlog(k + 1) + γ −1\n2\n∞\nX\ni=k+1\n1\ni2 ⩽\nk\nX\ni=1\n1\ni ⩽log(k + 1) + γ −1\n2\n∞\nX\ni=k+2\n1\ni2\n(21)\nFurthermore when k tends to inﬁnity,\n∞\nX\ni=k+1\n1\ni2 ∼1\nk\n(22)\n17\nThis proposition implies the following corollary:\nCorollary 11 Let T be a tournament on the set X such that BP = T.\nThere exists c > 0 such that, for any urn n ∈N,\nX\nx\np∗(x)LD(n(x), τ) ⩽φ −c\nτ .\n(23)\nFurthermore, writing ǫ(x) = p∗(x)/˜n(x) −1, if we restrict ourselves to large\nenough τ (with ǫ staying bounded) the constant c can be taken as close as\nwe want to :\n|X| −1 + P\nx∈BP ǫ(x)\n2\n(24)\nProof. This is a straightforward computation using the deﬁnition of LD.\nQ.E.D.\nWe also need a control of ǫ in term of µ\nLemma 12 Almost surely, for any time τ\nX\nx∈BP\n[˜nτ(x) + p∗(x)/2]ǫτ (x)2 ⩽φ −µτ\n(25)\nProof. We have\nφ −µτ ⩾\nX\nx∈BP\np∗(x)[log p∗−log p]\n(26)\n⩾\nX\nx∈BP\np∗(x)(ǫτ(x) + ǫτ(x)2/2)\n(27)\n⩾\nX\nx∈BP\np∗(x)ǫτ(x)2/2 +\nX\nx∈BP\n˜nτ(x)(1 + ǫτ(x))ǫτ(x)\n(28)\n⩾\nX\nx∈BP\n(p∗(x)/2 + ˜nτ(x))ǫτ(x)2\n(29)\nbecause P\nx∈BP ˜nτ(x)ǫ(x) = P\nx∈BP p∗(x) −˜nτ(x) = 0.\nQ.E.D.\nTogether, the last lemmas have the following consequence\nLemma 13 Let τ0 > 0 large enough such that |φ −µτ0| ⩾\n5\nτ0−1, then there\nexists π > 0 such that, with probability at least π,\n∀τ > τ0, |φ −µτ| ⩾1\nτ0\n.\n18\nProof. Let d = |φ −µτ0| and let T = inf{τ > τ0|φ −µτ ⩾2d or φ −µτ ⩽\nd/5}.\nT is a stopping time so µ′\nτ = µτ∧T is still a sub-martingale ; by\ndeﬁnition it is also bounded so it converges almost surely and in all Lp. Let\nµ′\n∞denote its limit. We will show that P(µ′\n∞∈(d/5, 2d)) > π > 0 for some\nπ. Remark that, since µ makes vanishing steps, we are only interested in\nthe behaviour of µ close to φ, so we can restrict ourself to small ǫ.\nAs long as τ < T, by deﬁnition we have φ −µτ ⩽2d and thus by\nLemma 12, P\nx(p∗(x)/2 + ˜nτ(x))ǫτ(x)2 ⩽2d.\nFor ǫ small enough, this\nimplies P\nx ˜n[1]\nτ ǫ(x)2 ⩽2d and thus, by Lemma 8:\n∀τ > τ0, E[(µ′\nt+1 −µ′\nt)2|F] ⩽2d\nτ 2 .\n(30)\nSumming up to inﬁnity (recall that µ′ has constant expectation):\nVar(µ′\n∞−µ′\nτ0|Fτ0) =\n∞\nX\nτ=τ0\nE[(µ′\nτ+1 −µ′\nτ)2|Fτ0]\n(31)\n⩽\n∞\nX\nτ=τ0\n2d\nτ 2\n(32)\n⩽2d\n1\nτ0 −1\n(33)\n⩽2\n5d2\n(34)\nwhere we used the hypothesis d ⩾\n5\nτ0−1 in the last line.\nRemark that, since µT∧τ is bounded, E[µ′\n∞] = d. Moreover notice that a\nrandom variable with expectation d which never takes value in the interval\n(d/2, 2d) has at least variance d2/2.\nSince Var(µ′\n∞|Fτ) ⩽\n2\n5d2, we have\nP(µ′ ∈(d/2, 2d)) ⩾1/10 and on this event, T = ∞so |φ −µ| has never\nreached d/2 ⩾1\nτ0 .\nQ.E.D.\nProof of Theorem 7, case BP = ∅.\nFirst note that if ˜nτ converges,\nit has to be toward a ﬁxed point. It is easy to see that the ﬁxed points\nof the dynamics are exactly the optimal strategies p∗\nY corresponding to all\nsubtournaments Y ⊆X.\n(This of course includes p∗= p∗\nX itself.)\nFor\nY ⊊X, P\nx p∗\nX(x) log p∗\nY (x) = −∞so by the using the Markov inequality\non µ we see that convergence to those ﬁxed points is impossible. Therefore\nwe only have to rule out convergence towards p∗\nX.\nWe ﬁrst consider the case |X| ⩾12. Then by Corollary 11, for any τ\nlarge enough, µτ almost surely veriﬁes the hypothesis of Lemma 13. Fix any\n19\nsuitable τ0, by Lemma 13, P(˜nτ →p∗) ⩽P(∃τ ⩾τ0||φ−µτ| ⩽1/τ0) ⩽1−π.\nOn the event that |φ−µτ| does reach 1/τ0 at time τ1, we can use Lemma 13\nat time τ1 to get P(˜nτ →p∗) ⩽(1−π)2. By induction we get P(˜nτ →p∗) = 0\nwhich proves the theorem.\nFor the case 3 ⩽|X| ⩽11 (a non trivial tournament has at least 3\nelements), consider any τ0 large enough. By Corollary 11 we have |φ−µτ0| ⩾\n1/τ0.\nLet T = inf{τ > τ0||φ −µτ| ⩽1/2τ0or|φ −µτ| ⩾\n5\nτ0 .\nThe event\n{T = ∞or |φ −µT | ⩾5\nτ0 } has probability at least 1/9 and if |φ −µT | ⩾5\nτ0\nwe can apply Lemma 13 at time T so the conclusion of Lemma 13 is still true\nwith 1/τ0 replaced by 1/2τ0 and we can use the same induction as before to\nprove the theorem.\nQ.E.D.\n3.5\nConclusion\nWe have found the behavior of learning process designed to discover the\n“best” alternatives in a tournament. Learning is achieved through the fol-\nlowing idea. An alternative which is considered as “good” at some date is\nreinforced for the future in the sense that one (slightly, and less and less)\nincreases the probability for this alternative to be considered: reinforcement\nupdates the sampling, or “prior” probability. The test according to which\nan alternative is considered as a good one at time t rests on comparing a\nfew randomly chosen alternatives.\nWe found a very diﬀerent behavior between the processes where rein-\nforcement occurs after sampling two or three alternatives. With three al-\nternatives, the process converges almost surely to a well-deﬁned limit that\nhas a nice interpretation in term of the tournament game: it is the optimal\nstrategy for this game. One can therefore say that this form of learning is\n“successful ”. With two alternatives, the picture is more complicated. The\nlearning process “succeeds ” in ﬁnding the Bipartisan set (a set which has\nbeen argued to be more important in term of social choice than the numeri-\ncal values of the optimal probabilities [14]), but not the optimal probabilities\nthemselves. We conjecture that the almost sure non-convergence happens\nfor all tournaments and not only when BP = X.\nAcknowledgment\nThanks to Bastien Mallein for useful discussions in the early stage of this\nstudy.\n20\nReferences\n[1] Brandt,Felix, Maria Chudnovsky, Ilhee Kim, Gaku Liu, Sergey Norin,\nAlex Scott, Paul Seymour, and Stephan Thomasse (2011) A counter-\nexample to a conjecture of Schwartz. Social Choice and Welfare forth-\ncoming.\n[2] Chebotarev, P. T., Shamis, E. 1998. Characterizations of scoring meth-\nods for preference aggregation. Annals of Operation Research 80: 299—\n332.\n[3] Chebotarev, P. T., Shamis, E. 2006. Preference fusion when the\nnumber of alternatives exeeds two:\nindirect scoring procedures.\narXiv:math/060217v3 [math.OC]\n[4] Daniels, H. E. 1969. Round-robin tournament scores. Biometrika 56:\n295—299.\n[5] David, H. 1963. The Method of Paired Comparisons, Griﬃn’s statistical\nmonographs and courses. Charles Griﬃns, London.\n[6] Fishburn, P.C. 1977. “Condorcet social choice functions“ SIAM Journal\non Applied Mathematics 33: 469—489.\n[7] David C. Fisher and Richard B. Reeves, 1995. Optimal strategies for\nrandom tournament games. Linear Algebra and its Applications, 217:\n83—85.\n[8] Fisher, D., Ryan, J. 1992. Optimal strategies for a generalized ‘Scissors,\nPaper and Stone’ game. American Mathematical Monthly 99: 935—\n942.\n[9] Fisher, D., Ryan, J. 1995a. Tournament games and positive tourna-\nments. Journal of Graph Theory 19: 217—236.\n[10] Fisher, D., Ryan, J. 1995b. Probabilities within optimal strategies for\ntournament games. Discrete Applied Mathematics 56: 87—91.\n[11] Hofbauer, J., Schlag, K. 2000. Sophisticated imitation in cyclic games.\nJournal of Evolutionary Economics 10: 523—543.\n[12] Laﬀond, G., Laslier, J.-F., Le Breton, M. 1993. The Bipartisan set of a\ntournament game. Games and Economic Behavior 5: 182—201.\n21\n[13] Laslier, J.-F. 1997. Tournament Solutions and Majority Voting, Berlin:\nSpringer-Verlag.\n[14] Laslier, J.-F. 2000. Aggregation of preferences with a variable set of\nalternatives. Social Choice and Welfare 17: 241—246.\n[15] Laslier, J.-F. 2000. Interpretation of electoral mixed strategies. Social\nChoice and Welfare 17: 247—267.\n[16] Levchenkov, V. S. 1992. Social choice theory: a new insight. Discussion\npaper, Institute of Systems Analysis, Moscow.\n[17] McKelvey, R. 1979. General conditions for global intransitivities in a\nformal voting model. Econometrica 47: 1085—1112.\n[18] Moon, J.W. 1968. Topics on Tournaments. Holt, Rinehart and Win-\nston, New York.\n[19] Moulin, H. 1986. Choosing from a tournament. Social Choice and Wel-\nfare 3: 271—291.\n[20] Myerson, R. B. 1993. Incentives to cultivate favored minorities under\nalternative electoral systems. American Political Science Review 87:\n856—869.\n[21] Myerson, R. B. 1995. Analysis of democratic institutions: structure,\nconduct and performance. Journal of Economic Perspectives 9: 77—\n89.\n[22] Pemantle, R. 2007. A survey of random processes with reinforcement.\nProbability Surveys 4: 1—79.\n[23] Posch, M. 1997. Cycling in a stochastic learning algorithm for normal\nform games. Journal of Evolutionary Economics 7: 193—207.\n[24] Rivest,\nRonald.L and Emily Shen (2010)\n“An Optimal Single-\nWinner\nPreferential\nVoting\nSystem\nBased\non\nGame\nTheory”\nhttp://people.csail.mit.edu/rivest/gt/latest conf.pdf\n[25] Rubinstein, Ariel 1996. Why are certain properties of binary relations\nrelatively more common in natural languages? Econometrica 64: 343—\n355.\n[26] Shiryaev, A.N. 1995. Probability. Graduate Text in Mathematics,\nSpringer.\n22\n[27] Slutzki, G. , Volij, O. 2006. Scoring of web pages and tournaments —\naxiomatizations. Social Choice and Welfare 26: 75—92.\n[28] Ushakov, I. A. 1976. The problem of choosing the preferred element:\nAn application to sport games. In Management Science in Sports (R. E.\nMachol, S. P. Ladany, and D.G. Morrison, eds.) 153—161. Amsterdam:\nNorth-Holland.\n23\n",
  "categories": [
    "math.OC",
    "cs.LG",
    "math.PR"
  ],
  "published": "2013-01-24",
  "updated": "2013-01-24"
}