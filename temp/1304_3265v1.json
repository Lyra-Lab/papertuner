{
  "id": "http://arxiv.org/abs/1304.3265v1",
  "title": "Extension of hidden markov model for recognizing large vocabulary of sign language",
  "authors": [
    "Maher Jebali",
    "Patrice Dalle",
    "Mohamed Jemni"
  ],
  "abstract": "Computers still have a long way to go before they can interact with users in\na truly natural fashion. From a users perspective, the most natural way to\ninteract with a computer would be through a speech and gesture interface.\nAlthough speech recognition has made significant advances in the past ten\nyears, gesture recognition has been lagging behind. Sign Languages (SL) are the\nmost accomplished forms of gestural communication. Therefore, their automatic\nanalysis is a real challenge, which is interestingly implied to their lexical\nand syntactic organization levels. Statements dealing with sign language occupy\na significant interest in the Automatic Natural Language Processing (ANLP)\ndomain. In this work, we are dealing with sign language recognition, in\nparticular of French Sign Language (FSL). FSL has its own specificities, such\nas the simultaneity of several parameters, the important role of the facial\nexpression or movement and the use of space for the proper utterance\norganization. Unlike speech recognition, Frensh sign language (FSL) events\noccur both sequentially and simultaneously. Thus, the computational processing\nof FSL is too complex than the spoken languages. We present a novel approach\nbased on HMM to reduce the recognition complexity.",
  "text": "EXTENSION OF HIDDEN MARKOV MODEL FOR \nRECOGNIZING LARGE VOCABULARY OF SIGN \nLANGUAGE \nMaher Jebali1, Patrice Dalle2, and Mohamed Jemni1 \n1Research Lab. LaTICE – ESSTT University of Tunis – Tunisia  \nmaher.jbeli@gmail.com \n2Research Lab. IRIT Univ. of Toulouse3 – France  \npatrice.dalle@irit.fr\nABSTRACT \nComputers still have a long way to go before they can interact with users in a truly natural fashion. From a user’s \nperspective, the most natural way to interact with a computer would be through a speech and gesture interface. \nAlthough speech recognition has made significant advances in the past ten years, gesture recognition has been \nlagging behind. Sign Languages (SL) are the most accomplished forms of gestural communication. Therefore, \ntheir automatic analysis is a real challenge, which is interestingly implied to their lexical and syntactic \norganization levels. Statements dealing with sign language occupy a significant interest in the Automatic Natural \nLanguage Processing (ANLP) domain. In this work, we are dealing with sign language recognition, in particular \nof French Sign Language (FSL). FSL has its own specificities, such as the simultaneity of several parameters, the \nimportant role of the facial expression or movement and the use of space for the proper utterance organization. \nUnlike speech recognition, Frensh sign language (FSL) events occur both sequentially and simultaneously. Thus, \nthe computational processing of FSL is too complex than the spoken languages. We present a novel approach \nbased on HMM to reduce the recognition complexity. \n \nKEY WORDS \nSign language recognition, Frensh sign language, Pattern recognition, HMM \n1. INTRODUCTION  \nThe focus of Human-Computer Interaction (HCI) is becoming more significant in our life. With the \nprogress of computer science, the existing devices used on HCI (keyboard, mouse, ) are not satisfying \nour needs nowadays. Many designers attempt to make HCI more natural and more easier. They \nintroduced the technique of Human-to-Human Interaction (HHI) into the domain of HCI, to reach this \npurpose. In this context, using hand gestures is among the richest HHI domain since everyone uses \nmainly hand gestures to explain ideas when he communicates with others. Communication with hand \ngestures becomes very important and much clearer with the consideration of sign language. Sign \nlanguage (SL) is the natural way of communication among deaf persons. As letters and words in \nnatural languages, we find the corresponding elements on sign languages, which are the movements, \ngestures, postures and facial expressions. Many researchers exploited hand gestures in other \napplication fields such as hand tracking [2] and interactive computer graphics [3]. Similarly, several \nstudies have focused automatic sign language recognition [1] [2] so as to facilitate communication \nbetween hearing and deaf persons and to improve Human-Computer Interaction systems. Several \nworks in the domain of automatic sign language recognition have been interested in the manual \ninformation such as the trajectories of both hands. In this context, we are presenting in this paper, our \napproach for modelling the manual information. In forthcoming papers, we will present others \ncomponents of our sign language recognition system, such as non-manual signs (head movement, \ngaze…) and especially the facial expression that gives additional information to convey the true \nmeaning of sign [24]. This article is structured as follows. Section 2 gives an overview of related \nworks in this study. Section 3 gives an overview of sign recognition. Section 4 presents the main \nproblems of automatic sign language recognition. Section 5 details the novel extension of HMM that \nwe proposed for reducing the complexity task. \n \n2. RELATED WORKS \nIn order to recognize human gestures by motion information [4] [28], many methods, based on \ntrajectories and motions, have been proposed to analyze these gestures. In [5], Bobick and Wilson \nadopted a state-based method for gestures recognizing. They used a number of samples, for each \ngesture, to calculate its main curve. Each gesture sample point was mapped to the length of arc at \ncurve. Afterword, they approximated the discritized curve using a uniform length in segments. They \ngrouped the line segments into clusters, and to match the previously learning state of sequences and \nthe current input state, they used the algorithm of Dynamic Time Warping. A 3D vision-based system \nwas described in [6] for American Sign Language (ASL) recognition. They used a method based on \n3D physics tracking to prepare their data samples, which were used to train HMMs. They used the \nextended Kalman filter to predicate the motion of the model. This input of the HMM method was a set \nof translation and rotation parameters. Subsequently, Vogler and Metaxas proposed an extended HMM \ncalled Parallel Hidden Markov Model to overcome the problem of large-scale vocabulary size. In [23], \nthey took into account the sentence spatial structure. They have presented a general sign language \nmodel of sentences that uses the composition of the space of signing as a representation of both the \nmeaning and the realisation of the utterance. Fels and Hinton [7] used in their recognition system a \ndata gloves as an acquisition tool, and they have chosen 5 neural networks to classify 203 signs. For \ncontinuous ASL recognition, Haynes and Jain used a view-based approach [9]. [16] Starner and al. \nextracted 2D features from a sign camera. They obtained 92%. In [20], the authors used many \ncharacteristics to detect the borders of Japanese SL words, like orientation, hand shape and position. \n[26] [27] have developed a language recognition application based on 3D continuous sign. They used \nthe Longest Common Subsequence method for sign language recognition instead of HMM, which is a \ncostly double stochastic process. A method of dynamic programming was used by the authors of [11] \n[12] to recognize continuous CSL. They used a data gloves as acquisition devices and Hidden Markov \nModel as a method of recognition. Their system recognized 94.8%. Previous research on automatic \nrecognition of sign language mainly focused on the domain of the dependent signer. A little work has \nbeen devoted to independent signer. Indeed, the sign language recognition systems based on \nindependent signer have a promising trend in practical systems that can recognize a different signers \nSL. But the problem of the recognition of signer independent is hard to resolve regarding the great \nobstacle from different sign variations. \n \n \n \n \n \nTable 1. Gesture recognition methods \n \n3. SIGN lANGUAGE OVERVIEW \n3.1 Lexical Meaning Expression \nThe linguists of sign language make out the basic components of a sign as corresponding of the \nhandshape, location, hand orientation and movement. Handshape means the fingers configurations, \nlocation means where the hand is localized relatively to a parts of body and orientation means the \ndirection in which the fingers and palm are pointing. The movement of hand draws out a trajectory in \nthe signing space. Stokoe, who proposed the first phonological sign language model, accentuates the \nsimultaneous presentation of these components. The model of Liddell and Johnson [18] accentuated \nthe sequentiality organization of these components. They defined the segments of movement as short \nperiods during which some sign component is in transition. The segments of hold are short periods \nwhen all these components are static. The aim of many novel models is to represent the sequential and \nthe simultaneous signs structure and it would look that the adopted SL recognition system must be \ncapable to model sequential and simultaneous structures. Different researchers used constituent parts \nof signs for classification systems. All signs parts are very important as manifested by the signs unit \nexistence, which differs in only one of the principal components. When signs take place in a \ncontinuous video to product utterances, the hand must change the position from the ending of one sign \nto the starting of the next sign. At the same time, the orientation and the handshape also change from \nthe ending orientation and handshape of one sign to the starting orientation and handshape of the next \nsign. These periods of transition from one position to another are called movement epenthesis and they \ndo not represent any part of signs. The production of continuous signs processes with the same effects \nof the co-articulation in speech language, where the preceding and succeeding signs utter the sign \nappearance. However, these movements epenthesis are not presented in all signs. Hence, the \nmovement epenthesis presents often times during the production of a continuous signs and should be \nanalyzed first, before dealing with the different phonological aspects. Extraction and classification \nfeature methods are affected by some signing aspects, especially for approaches based on vision. First, \nwhen a sign gesture is produced, the hand may be placed in different orientations with consideration to \nthe morphological signers body and a fixed orientation of the hand cannot be supposed. Second, \ndifferent movements types are included in signs production. Generally, the movement represent the \nwhole hand drawing a general 3D trajectory. However, there are many signs whose only represent \nlocal movements, such as changing the orientation of the hand by moving the fingers or deforming the \nwrist. Third, the two hands often occlude or touch each other when captured from only one field of \nview and the hands partially occlude the face in some signs. Hence, handling the occlusion is an \nimportant step. \n3.2 Grammatical Processes \nThe different changes to the sign aspect during the production of continuous signs do not change the \nsign meaning. However, there are other amendments to one or more sign components, which affect the \nmeaning of sign, and these are concisely described in this section. Usually, the meanings transmitted \nthrough these changes are related to the verbs aspects that include recurrence, frequency, permanence, \nintensity and duration. Furthermore, the movement of sign can be changed through its tension, rhythm, \ntrajectory shape and rate. [17] list between 8 and 11 types of possible inflections for temporal aspect. \nThe agreement of person (first person, second person, or third person) is an another type of inflection \nthat can be presented. Here, the verb designates its object and subject by a modification in the \ndirection of movement with corresponding modifications in its start and end position, and the \norientation of hand. Verbs can be changed simultaneously for number agreement and person. Many \nother samples of grammatical aspects which turn out in orderly changes in the appearance of sign \ninvolve categorical derivation of nouns from verbs, inflections, compound signs and numerical \nincorporation. Categorical inflections are used for the accentuation intention and are shown across \nrepetition in the movement of sign, with tension throughout. In a classic system of sign language \nrecognition, data are dealt together in only one block. However, both hands would be represented as a \nsingle block. At this level, us show that this system type of recognition looks not well adapted to \nfeatures of SL. To facilate sign language analysis sentences and to associate lexical level to superior \nlevel, the authors of [25] have proposed a computational model for signing space construction that can \nbe related to the grammar of sign language. \n3.3 Non-manual gestures \nFor non-manual, we mean the movement of the head, eyebrows, eyelids, cheeks, nose, mouth, \nshoulders and torso and the direction of gaze. This section presents the non-manual gestures based on \nthe current knowledge, particularly their linguistic functions, to propose a reflection at all levels of \nsign languages. The non-manual elements, in particular, the facial expression have long been put aside \nby linguists. Subsequently, the non-manual elements evoked on many occasions for their linguistics \nroles but without having been specific studies. The non-manual elements intervene at the lexical level \nin four cases : \n- In the constitution of a standard sign.  \n- The distinction between two signs.  \n- The distinction between the object and its action and/or use.  \n- In the modifiers realization (adjectives, adverbs, etc.).  \nThe non-manual elements intervene also on the utterances. It has a comparable role to the punctuation \nin writing or the tone of the speech. Thus, it is essential to the proper understanding of the final \nmessage. These elements present also an important role in the segmentation of utterances and in \nparticular to delimit syntactic groups. \nThe figure fig.1 shows two signs which have the same manual elements and distinguished through the \nnoon manual elements. \n \nFigure 1: (content) and (pain in the heart) \n4. MANUAL SIGN RECOGNITION ISSUES \n4.1 Lexicon Complexity \nComplexity is a relevant issue in sign language recognition systems when the lexicon has a large size. \nSince a sign consists of numerous entities occurring together, the lexicon of SL can incorporate a large \nnumber of signs [19]. We can obtain HS possible handshapes for each hand, M different movements, \nL locations, and O orientations. Thus HS*L*O*M possible signs. Furthermore, both hands can be \nassociated together, thus we can have a very large lexicon. Modeling a sign language recognition \nsystem for such a lexicon is challenging task. In fact, the signers can not represent all possible \ncombinations. Signs parameters are associated following semantic and grammatical rules. \nNevertheless, the resulting lexicon size remains too large. To figure out this problem, instead \nprocessing signs parameters like only one block, we decide to deal them separately as in [20] [21]. We \ncan process like in speech recognition system (SRS). In SRS, the phonemes are recognized instead \nwhole words. But in speech language, phonemes are always successive, while in the parameters of SL, \nthey are often simultaneous. Therefore, establishing a recognition system remains arduous. \n4.2 Role of both hands in signing space \nThere are three kinds of interactions between right and left hands. Either both hands creates together a \nsign, or only one hand produce a sign, or each hand creates a sign and those signs can be completely \nindependent or have a relationships. The problem is being able to set aside each instance. Thus, we \nwill not recognize a sign produced with both hands when there are two different signs executed at the \nsame time. To differentiate these two cases, the similarity parameters measure between both hands is \nunusable, because Signs produced with two hands, may be entirely synchronous, as well as correlated \nand not having a similarity between parameter. It is necessary to find the two other existing \nrelationships between hands. When both hands produces two different signs with a relationship  of \ndominant / dominated hand, there are points of synchronization between the different signs. However, \nboth hands will be structured in a manner that supply informations. Nevertheless, during a brief time, \nthe two hands are structured in order to supply the spatial informations. Thus, we demand to be \ncapable to capture this especial period in order to represent this spatial relationships. Even if the \ninformation transmitted by the two hands is only spatial, the relationship between both hands is \nspatio-temporal. \n4.3 Translation of non-standard Sign  \nThe third issue concerns the non standard signs translation, and this is a very difficult problem. \nCurrently, only a little part of those signs has been accounted by systems dealing with sign recognition \n[22] [23]. Classifiers, size and shape specifiers represent a part of non-standard signs. The utterance \nand its context give a meaning. Classifiers are frequently associated to a standard sign which has been \npreviously produced, so it will be an easy task to give a sense to the sign. Unluckily, some classifiers \nare linked to signs which are produced after them or they are linked to descriptions made with \nspecifiers. Yet, we can develop a possible set of classifiers for signs. Detecting  a sens for specifier is \nmore difficult, because it is particular to what it describes. Therefore, from one characterization to \nanother, signs can widely vary. Unlike the limited lexicons in oral languages, SL contains many signs \nwhich can be invented in accordance with needs. The signs specifiers parameters are linked to what \nthey describe. Therefore, new signs can appear for each description. Moreover, the way which the \ndescription is done depends on the signers cultural and social context. We need to analyze the \nutterance and its context to understand these non standard signs. Also, we need to analyze different \nparameters of those signs for associating them with what is being described. This is not evident, and \ncan only be dealt at the semantic and syntactic level. Primarily, sign language recognition systems \nwork at the lexical level, thus, they are not capable to deal with such signs which are declined or badly \nrepresented. We need to use a higher level analyzer to deal these stored signs. \n5. HIDDEN MARKOV MODEL \nFrom a set of N states Ci, the transitions from two states can be described, at each time step t, as a \nstochastic process. The probability of transition to reach state Ci in the first time step is mentioned as \nthe probability of transition aij of state Ci to another state Cj only depends on the preceding states, this \nprocess is called Markov chain. The additional hypothesis, that the current transition only depends on \nthe preceding state leads to a first order Markov chain. A second stochastic process can be defined, \nthat produces a symbol vectors x at each time step t. The Using HMM for sign recognition is \nmotivated by the successful application of probability of emission of a vector x wich not depends on \nthe way that the state was attained but only depends on the current state. The density of emission \nprobability yi(x) for vector x at state Ci can either be continuous or discrete. This process is a doubly \nstochastic and called a Hidden Markov Model (HMM) if the sequence of state are not observable, but \nnot the vectors x. A HMM H is defined as follows H = (x, A, B). x represent the vector of the initial \nprobabilities of transition , the NxN matrix A denotes the probabilities of transition a, from state Ci to \nCj and finally, B represents the emission densities vector yi(x) of each state. \n \n \nFigure 2: Hidden Markov Model \n \nUsing HMM for sign recognition is motivated by the successful application of the techniques of \nHidden Markov Model to speech recognition issues. The similar points between speech and sign \nsuggest that effective techniques for one problem may be effective for the other as well. First, like \nspoken languages, gestures vary according to position, social factors, and time. Second, the \nmovements of body, like the sounds in speech, transmit certain meanings. Third, signs regularities \nperformances while speaking are similar to syntactic rules. Therefore, the methods elaborate by \nlinguistic may be used in sign recognition. Sign recognition has its own characteristics and issues. \nMeaningful signs may be complex to deal, containing simultaneous movements. However, the \ncomplicated signs should be facilely specifiable. Generally, signs can be specified either by \ndescription or by example. Earlier, each system has a training stage in which samples of different signs \nare collected to train the models. These models are the all gestures representations that will be \nrecognized by the system. In the latter specification method, a description of each sign is written with \na a formal language of SL, in which the syntax is specified for each sign. Evidently, the description \nmethod has less flexibility than the example method. One potential inconvenience of example \nspecification is difficult to specify the eligible variation between signs of a given class. If the \nparameters of the model were determined by the most likely performance criterion, this problem \nwould be avoided. Because sign is an expressive motion, it is evident to describe a movement across a \nsequential model. Based on these criterions, Hiddem Markov Model is appropriate for sign \nrecognition. A multi-dimensional HMM is capable to deal with multichannel signs which are general \ncases of sign language recognition. \n5.1 Independent channels \nIn speech, phonemes occur in sequence, but in sign languages can occur in parallel. Some \nsigns are produced with two hands, so they must be modelled. Many approaches highlighted \nthe massive complexity of the simultaneous aspects modelling of sign languages. It is \ninfeasible to model all these combinations for two reasons. First, the computational and the \nmodelling viewpoint. Second, it would be impossible to gather sufficient data for all these \ncombinations. The main solution is to find a way to extinguish the simultaneous events from \none another. However, it is viable to seem at only one event at a time. The idea of this \napproach comes from many researchers in linguistic field into sign languages, especially into \nFrench Sign Language (FSL), most of which concentrated on a particular various aspect of the \nfeatures. Each hand had their own features such as the hand shape, orientation or position. \nSplitting the manual and non-manual components within independent channels allows a major \nreduction in the modelling complexity task. Thus, it is not needful to envisage all possible \nphonemes combinations. Instead, we model the phonemes in a single channel. Each channel \ncontain a reduced number of different Hidden Markov Models to represent the phonemes. \nTherefore, phonemes combinations are easy to reliate together at time of recognition, \nespecially in association with the parallel HMM. \n \n5.2 Three parallel channels  \nConventional HMMs are an unsuitable choice for SL modelling for many reasons. First, they are only \nable to model single event. However, they should merging the different channels into a single event, \nforcing them to acquire in a coupled manner, which is poor for many systems that demand modelling \nof different simultaneous processes. For example, if a sign, produced with only one hand, precedes a \nsign performed with both hands, the dominate hand moves often to the position requisite by the sign \nperformed with both hands before the dominant hand starts to make it, at a slightly inconclusive point \nin time. If the different channels wee too closely coupled, the dominate hand movement would be \nimpossible to capture. Unfortunately, this extension cannot solve the key modelling complexity \nproblem of French sign language. This approach needs to train the different interactions between these \nchannels. Thus, there would require to be sufficient training examples usable for all possible \ninteraction between FSL phonemes through channels. Therefore, it is necessary to make a novel \nextension to HMMs that allow phonemes combinations decoupling, and to model them at recognition \nmoment, instead of training moment. We describe now the proposed three channels parallel HMMs as \na possible solution. We model each channel Ci with independent HMM with separate output.  \n \n \nFigure 3: Three channels parallel HMM \nThe separate channels of parallel HMM progress independently from one to another and they have \nautonomous output. However, it is possible to train independently the different channels, and to set the \ndifferent channels together at recognition moment. Using this novel approach, we have to reduce the \nFSL modelling complexity, because, instead to perform all phonemes possible combinations, we are \ninvited to consider each phoneme by itself. Thus, the total number of hidden Markov model needed is \nonly the sum, instead of the product of all phonemes. At recognition time, it is indispensable to \nconsolidate some informations from the HMM representing the three different channels. The \nrecognition algorithm should find the maximum joint probability of the three channels that is : \n \n \n \nWhere Q(c) is the state sequence of channel c, 1 ≤ c ≤ C, with observation sequence O(c) through the \nHMM network λ(c). Recall that this observation sequence corresponds to some unknown sign sequence \nin channel c, which is to be recognized. Because in Parallel HMMs the channels are independent, the \nmerged information consists of the product of the probabilities of the individual channels or the sum of \nthe log probabilities. \n \n \n6. CONCLUSION \nSeeing that the two hands are represented by independent channels of communication produces \ndifferent relationships between signs produced by left hand and those produced by right hand and the \nimportant role of the head to complete the sign meaning. We can not neglect those relationships \nbecause they convey in formations which are very needful to the recognition system. We have \ndecribed a new method for FSL recognition, wich is based on phonological modeling of sign \nlanguage. We have presented Parallel Hidden Markov Model with three independent channels wich \nrepresent respectively the right hand, the left hand and the head. This recognition method has the \npotential to manage a large-scale vocabularies than the majority of current and past research that have \nbeen used into sign language recognition. \n \nREFERENCES \n[1] Vogler, C.P.: American sign language recognition : reducing the complexity of the task with \nphoneme-based modeling and parallel hidden markov models. PhD thesis, University of Pennsylvania, \nPhiladelphia, PA, USA. Supervisor-Dimitris N. Metaxas (2003) \n[2] Lefebvre-Albaret,F.: Automatic processing of videos FSL: Modeling and operating phonological \nconstraints of movement. PhD thesis, University of Toulouse III - Paul Sabatier (2010) \n[3] Freeman, W.T., Anderson, D., Beardsley, P. , Dodge, C. , Kage, H., Kyuma, K., Miyake, Y. : Computer \nvision for interactive computer graphics, IEEE Computer Graphics and Applications 18 (3) 4253 (1998) \n[4] Johansson, G. : a Visual Perception of Biological Motion and a Model for Its Analysis, Perception and \nPsychophysics, vol. 73, no. 2, pp. 201-211 (1973) \n[5] Bobick, A.F., Wilson, A.D. : A State-Based Approach to the Representation and Recognition of Gesture, \nIEEE Trans. Pattern Analysis and Machine Intelligence, vol. 19, no. 12, pp. 1325-1337, Dec (1997) \n[6] Vogler, C., Metaxas, D. : a ASL Recognition Based on a Coupling between HMMs and 3D Motion \nAnalysis, Proc. Sixth IEEE Int’l Conf. Computer Vision, pp. 363- 369, (1998) \n[7] Dempster, A.P., Laird, N.M., Rubin, D.B. : a Maximum Likelihood from Incomplete Data via the EM \nAlgorithm, J. Royal Statistical Soc., vol. 39, no. 1, pp. 1-38, (1977)  \n[8] Fels, S.S., Hinton, G.E. : a Glove-Talk II: A Neural Network Interface which Maps Gestures to Parallel \nFormat Speech Synthesizer Controls, IEEE Trans. Neural Networks, vol. 9, no. 1, pp. 205-212, (1997)  \n[9] Haynes, S., Jain, R. : a Detection of Moving Edges, Computer Vision, Graphics, and Image Understanding, \nvol. 21, no. 3, pp. 345- 367, (1980) [10] Isard, M., Blake, A. : a CONDENSATION Conditional Density \nPropagation for Visual Tracking,o Int’l J. Computer Vision, vol. 29, no. 1, pp. 5-28, (1998)12. \n[11] Leonardis, A., Gupta, A., Bajcsy, R. : a Segmentation as the Search for the Best Description of the Image in \nTerms of Primitives, Proc. Third IEEE Int’l Conf. Computer Vision, pp. 121-125, (1990) \n[12] Marshall, D., Lukacs, G., Martin, R.: a Robust Segmentation of Primitives from Range Data in the \nPresence of Geometric Degeneracy, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 23, no. 3, pp. \n304-314, Mar. (2001) \n[13] Wilson, A.D., Bobick, A.F. : a Parametric Hidden Markov Models for Gesture Recognition, IEEE Trans. \nPattern Analysis and Machine Intelligence, vol. 21, no. 9, pp. 884-900, Sept. (1999) \n[14] Schlenzig, J., Hunter, E., Jain, R. : a Vision Based Hand Gesture Interpretation Using Recursive \nEstimation, Proc. 28th Asilomar Conf. Signals, Systems, and Com- puters, (1994) \n[15] Siskind, J.M., Morris, Q. : A Maximum-Likelihood Approach to Visual Event Clas- sification, Proc. Fourth \nEuropean Conf. Computer Vision, pp. 347-360, (1996) \n[16] Starner, T., Weaver, J., Pentland, A. : a Real-Time American Sign Language Recognition Using Desk and \nWearable Computer Based Video, IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 20, no. 12, pp. \n1371-1375, Dec. (1998) \n[17] Klima, E.S., Bellugi, U. : The Signs of Language. Harvard Univ. Press, (1979)  \n[18] Liddell, S.K., Johnson, R.E. : American Sign Language: The Phonological Base, Sign Language Studies, \nvol. 64, pp. 195-277, (1989)  \n[19] Vogler, C.,Metaxas, D.N.: Toward Scalability in ASL Recognition: Breaking Down Signs into Phonemes. \nIn : Proc. of Gesture Workshop99, Lecture Notes in Artificial Intelligence, Vol. Springer-Verlag, Gif sur Yvette \nFrance, (march 1999), 211- 224. (1739) \n[20] Liang, R.H., Ming, O.: A Sign Language Recognition System Using Hidden Markov Model and Context \nSensitive Search. In : Proc. of ACM Symposium on Virtual Reality Software and Technology96, 59-66. (1996) \n[21] Vogler, C., Metaxas, D.N.: Parallel Hiden Markov Models for American Sign Language Recognition. In : \nProc. of International Conference on Computer Vision, 116-122. (1999) \n[22] Braffort, A.: An Architecture for Sign Language Recognition and Interpretation. In : Proc. of Gesture \nWorkshop96, Harling, P.A., Edwards, A.D.N. (eds.), Springer- Verlag, 17-30. March (1996) \n[23] Lenseigne, B., Dalle, P. : Using Signing Space as a Representation for Sign Language Processing. In the \n6th International Gesture Workshop. pp 25-36. (2006)  \n[24] Mercier, H., Peyras, J., Dalle, P. : Toward an Efficient and Accurate AAM Fitting on Appearance Varying \nFaces. In : International Conference on Automatic Face and Gesture Recognition (FG 2006), Southampton, \nRoyaume-Uni, 10/04/2006 2/04/2006, IEEE, p. 363-368, (2006) \n[25] Dalle, P. : High level models for sign language analysis by a vision system. In Workshop on the \nRepresentation and Processing of Sign Language: Lexicographic Matters and Didactic Scenarios (LREC 2006), \nGnes, Italy, 28/05/2006 28/05/2006, Evaluations and Language resources Distribution Agency (ELDA), p. \n17-20, May (2006) \n[26] Jaballah, K., Jemni,M. : Toward Automatic Sign Language Recognition fromWeb3D Based Scenes, \nICCHP, Lecture Notes in Computer Science Springer Berlin/Heidelberg, pp.205-212 vol.6190, Vienna Austria, \nJuly (2010) \n[27] Jaballah, K., Jemni,M. : Towards Sign language indexing and retrieval, CSUN, San Diego, California \nMarch (2011). \n[28] Zaman Khan, R., Adnan Ibraheem , N., : Hand Gesture Recognition: A Literature Review, International \nJournal of Artificial Intelligence & Applications (IJAIA), July (2012) \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2013-04-11",
  "updated": "2013-04-11"
}