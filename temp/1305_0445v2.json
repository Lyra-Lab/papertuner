{
  "id": "http://arxiv.org/abs/1305.0445v2",
  "title": "Deep Learning of Representations: Looking Forward",
  "authors": [
    "Yoshua Bengio"
  ],
  "abstract": "Deep learning research aims at discovering learning algorithms that discover\nmultiple levels of distributed representations, with higher levels representing\nmore abstract concepts. Although the study of deep learning has already led to\nimpressive theoretical results, learning algorithms and breakthrough\nexperiments, several challenges lie ahead. This paper proposes to examine some\nof these challenges, centering on the questions of scaling deep learning\nalgorithms to much larger models and datasets, reducing optimization\ndifficulties due to ill-conditioning or local minima, designing more efficient\nand powerful inference and sampling procedures, and learning to disentangle the\nfactors of variation underlying the observed data. It also proposes a few\nforward-looking research directions aimed at overcoming these challenges.",
  "text": "arXiv:1305.0445v2  [cs.LG]  7 Jun 2013\nDeep Learning of Representations:\nLooking Forward\nYoshua Bengio\nDepartment of Computer Science and Operations Research\nUniversit´e de Montr´eal, Canada\nAbstract. Deep learning research aims at discovering learning algorithms that discover mul-\ntiple levels of distributed representations, with higher levels representing more abstract con-\ncepts. Although the study of deep learning has already led to impressive theoretical results,\nlearning algorithms and breakthrough experiments, several challenges lie ahead. This paper\nproposes to examine some of these challenges, centering on the questions of scaling deep learn-\ning algorithms to much larger models and datasets, reducing optimization diﬃculties due\nto ill-conditioning or local minima, designing more eﬃcient and powerful inference and sam-\npling procedures, and learning to disentangle the factors of variation underlying the observed\ndata. It also proposes a few forward-looking research directions aimed at overcoming these\nchallenges.\n1\nBackground on Deep Learning\nDeep learning is an emerging approach within the machine learning research community. Deep\nlearning algorithms have been proposed in recent years to move machine learning systems towards\nthe discovery of multiple levels of representation. They have had important empirical successes in\na number of traditional AI applications such as computer vision and natural language processing.\nSee (Bengio, 2009; Bengio et al., 2013d) for reviews and Bengio (2013c) and the other chapters of\nthe book by Montavon and Muller (2012) for practical guidelines. Deep learning is attracting much\nattention both from the academic and industrial communities. Companies like Google, Microsoft,\nApple, IBM and Baidu are investing in deep learning, with the ﬁrst widely distributed products being\nused by consumers aimed at speech recognition. Deep learning is also used for object recognition\n(Google Goggles), image and music information retrieval (Google Image Search, Google Music), as\nwell as computational advertising (Corrado, 2012). A deep learning building block (the restricted\nBoltzmann machine, or RBM) was used as a crucial part of the winning entry of a million-dollar\nmachine learning competition (the Netﬂix competition) (Salakhutdinov et al., 2007; T¨oscher et al.,\n2009). The New York Times covered the subject twice in 2012, with front-page articles.1 Another\nseries of articles (including a third New York Times article) covered a more recent event showing\noﬀthe application of deep learning in a major Kaggle competition for drug discovery (for example\nsee “Deep Learning - The Biggest Data Science Breakthrough of the Decade”2. Much more recently,\nGoogle bought out (“acqui-hired”) a company (DNNresearch) created by University of Toronto\nprofessor Geoﬀrey Hinton (the founder and leading researcher of deep learning) and two of his\nPhD students, Ilya Sutskever and Alex Krizhevsky, with the press writing titles such as “Google\nHires Brains that Helped Supercharge Machine Learning” (Robert McMillan for Wired, March 13th,\n2013).\nThe performance of many machine learning methods is heavily dependent on the choice of data\nrepresentation (or features) on which they are applied. For that reason, much of the actual eﬀort in\ndeploying machine learning algorithms goes into the design of preprocessing pipelines that result in\na hand-crafted representation of the data that can support eﬀective machine learning. Such feature\n1 http://www.nytimes.com/2012/11/24/science/scientists-see-advances-\nin-deep-learning-a-part-of-artiﬁcial-intelligence.html\n2 http://oreillynet.com/pub/e/2538\n2\nY. Bengio\nengineering is important but labor-intensive and highlights the weakness of many traditional learn-\ning algorithms: their inability to extract and organize the discriminative information from the data.\nFeature engineering is a way to take advantage of human ingenuity and prior knowledge to compen-\nsate for that weakness. In order to expand the scope and ease of applicability of machine learning,\nit would be highly desirable to make learning algorithms less dependent on feature engineering, so\nthat novel applications could be constructed faster, and more importantly for the author, to make\nprogress towards artiﬁcial intelligence (AI).\nA representation learning algorithm discovers explanatory factors or features. A deep learning\nalgorithm is a particular kind of representation learning procedure that discovers multiple levels of\nrepresentation, with higher-level features representing more abstract aspects of the data. This area\nof research was kick-started in 2006 by a few research groups, starting with GeoﬀHinton’s group,\nwho initially focused on stacking unsupervised representation learning algorithms to obtain deeper\nrepresentations (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2008). Since\nthen, this area has seen rapid growth, with an increasing number of workshops (now one every year\nat the NIPS and ICML conferences, the two major conferences in machine learning) and even a\nnew specialized conference just created in 2013 (ICLR – the International Conference on Learning\nRepresentations).\nTransfer learning is the ability of a learning algorithm to exploit commonalities between diﬀerent\nlearning tasks in order to share statistical strength, and transfer knowledge across tasks. Among the\nachievements of unsupervised representation learning algorithms are the impressive successes they\nobtained at the two transfer learning challenges held in 2011. First, the Transfer Learning Challenge,\npresented at an ICML 2011 workshop of the same name, was won using unsupervised layer-wise pre-\ntraining (Bengio, 2011; Mesnil et al., 2011). A second Transfer Learning Challenge was held the\nsame year and won by Goodfellow et al. (2011) using unsupervised representation learning. Results\nwere presented at NIPS 2011’s Challenges in Learning Hierarchical Models Workshop.\n2\nQuick Overview of Deep Learning Algorithms\nThe central concept behind all deep learning methodology is the automated discovery of abstraction,\nwith the belief that more abstract representations of data such as images, video and audio signals\ntend to be more useful: they represent the semantic content of the data, divorced from the low-level\nfeatures of the raw data (e.g., pixels, voxels, or waveforms). Deep architectures lead to abstract\nrepresentations because more abstract concepts can often be constructed in terms of less abstract\nones.\nDeep learning algorithms are special cases of representation learning with the property that they\nlearn multiple levels of representation. Deep learning algorithms often employ shallow (single-layer)\nrepresentation learning algorithms as subroutines. Before covering the unsupervised representation\nlearning algorithms, we quickly review the basic principles behind supervised representation learning\nalgorithms such as the good old multi-layer neural networks. Supervised and unsupervised objectives\ncan of course be combined (simply added, with a hyper-parameter as coeﬃcient), like in Larochelle\nand Bengio (2008)’s discriminative RBM.\n2.1\nDeep Supervised Nets, Convolutional Nets, Dropout\nBefore 2006, it was believed that training deep supervised neural networks (Rumelhart et al., 1986)\nwas too diﬃcult (and indeed did not work). The ﬁrst breakthrough in training them happened in\nGeoﬀHinton’s lab with unsupervised pre-training by RBMs (Hinton et al., 2006), as discussed in\nthe next subsection. However, more recently, it was discovered that one could train deep supervised\nnets by proper initialization, just large enough for gradients to ﬂow well and activations to convey\nuseful information (Glorot and Bengio, 2010; Sutskever, 2012).3 Another interesting ingredient in the\n3 and potentially with the use of momentum (Sutskever, 2012)\nDeep Learning of Representations: Looking Forward\n3\nsuccess of training the deep supervised networks of Glorot and Bengio (2010) (and later of Krizhevsky\net al. (2012)) is the presence of rectifying non-linearities (such as max(0, x)) instead of sigmoidal non-\nlinearities (such as 1/(1+exp(−x)) or tanh(x)). See Jarrett et al. (2009); Nair and Hinton (2010) for\nearlier work on rectiﬁer-like non-linearities. We return to this topic in Section 4. These good results\nwith purely supervised training of deep nets seem to be especially clear when large quantities of\nlabeled data are available, and it was demonstrated with great success for speech recognition (Seide\net al., 2011a; Hinton et al., 2012a; Deng et al., 2013) and object recognition (Krizhevsky et al., 2012)\nwith breakthroughs reducing the previous state-of-the-art error rates by 30% to 50% on diﬃcult to\nbeat benchmarks.\nOne of the key ingredients for success in the applications of deep learning to speech, images,\nand natural language processing (Bengio, 2008; Collobert et al., 2011) is the use of convolutional\narchitectures (LeCun et al., 1998b), which alternate convolutional layers and pooling layers. Units\non hidden layers of a convolutional network are associated with a spatial or temporal position and\nonly depend on (or generate) the values in a particular window of the raw input. Furthermore, units\non convolutional layers share parameters with other units of the same “type” located at diﬀerent\npositions, while at each location one ﬁnds all the diﬀerent types of units. Units on pooling layers\naggregate the outputs of units at a lower layer, either aggregating over diﬀerent nearby spatial\npositions (to achieve a form of local spatial invariance) or over diﬀerent unit types. For example, a\nmax-pooling unit outputs the maximum over some lower level units, which can therefore be seen to\ncompete towards sending their signal forward.\nAnother key ingredient in the success of many recent breakthrough results in the area of object\nrecognition is the idea of dropouts (Hinton et al., 2012b; Krizhevsky et al., 2012; Goodfellow et al.,\n2013b). Interestingly, it consists in injecting noise (randomly dropping out units with probability\n1/2 from the neural network during training, and correspondingly multiplying by 1/2 the weights\nmagnitude at test time) that prevents a too strong co-adaptation of hidden units: hidden units must\ncompute a feature that will be useful even when half of the other hidden units are stochastically\nturned oﬀ(masked). This acts like a powerful regularizer that is similar to bagging aggregation\nbut over an exponentially large number of models (corresponding to diﬀerent masking patterns, i.e.,\nsubsets of the overall network) that share parameters.\n2.2\nUnsupervised or Supervised Layer-wise Pre-Training\nOne of the key results of recent years of research in deep learning is that deep compositions of\nnon-linearities – such as found in deep feedforward networks or in recurrent networks applied over\nlong sequences – can be very sensitive to initialization (some initializations can lead much better\nor much worse results after training). The ﬁrst type of approaches that were found useful to reduce\nthat sensitivity is based on greedy layer-wise pre-training (Hinton et al., 2006; Bengio et al., 2007).\nThe idea is to train one layer at a time, starting from lower layers (on top of the input), so that\nthere is a clear training objective for the currently added layer (which typically avoids the need\nfor back-propagating error gradients through many layers of non-linearities). With unsupervised\npre-training, each layer is trained to model the distribution of values produced as output of the\nprevious layer. As a side-eﬀect of this training, a new representation is produced, which can be used\nas input for deeper layers. With the less common supervised pre-training (Bengio et al., 2007; Yu\net al., 2010; Seide et al., 2011b), each additional layer is trained with a supervised objective (as\npart of a one hidden layer network). Again, we obtain a new representation (e.g., the hidden or\noutput layer of the newly trained supervised model) that can be re-used as input for deeper layers.\nThe eﬀect of unsupervised pre-training is apparently most drastic in the context of training deep\nauto-encoders (Hinton and Salakhutdinov, 2006), unsupervised learners that learn to reconstruct\ntheir input: unsupervised pre-training allows to ﬁnd much lower training and test reconstruction\nerror.\n4\nY. Bengio\n2.3\nDirected and Undirected Graphical Models with Anonymous Latent Variables\nAnonymous latent variables are latent variables that do not have a predeﬁned semantics in terms of\npredeﬁned human-interpretable concepts. Instead they are meant as a means for the computer to dis-\ncover underlying explanatory factors present in the data. We believe that although non-anonymous\nlatent variables can be very useful when there is suﬃcient prior knowledge to deﬁne them, anony-\nmous latent variables are very useful to let the machine discover complex probabilistic structure:\nthey lend ﬂexibility to the model, allowing an otherwise parametric model to non-parametrically\nadapt to the amount of data when more anonymous variables are introduced in the model.\nPrincipal components analysis (PCA), independent components analysis (ICA), and sparse coding\nall correspond to a directed graphical model in which the observed vector x is generated by ﬁrst\nindependently sampling some underlying factors (put in vector h) and then obtaining x by Wh\nplus some noise. They only diﬀer in the type of prior put on h, and the corresponding inference\nprocedures to recover h (its posterior P(h | x) or expected value E[h | x]) when x is observed. Sparse\ncoding tends to yield many zeros in the estimated vector h that could have generated the observed\nx. See section 3 of Bengio et al. (2013d) for a review of representation learning procedures based on\ndirected or undirected graphical models.4 Section 2.5 describes sparse coding in more detail.\nAn important thing to keep in mind is that directed graphical models tend to enjoy the property\nthat in computing the posterior, the diﬀerent factors compete with each other, through the celebrated\nexplaining away eﬀect. Unfortunately, except in very special cases (e.g., when the columns of W are\northogonal, which eliminates explaining away and its need), this results in computationally expensive\ninference. Although maximum a posteriori (MAP) inference5 remains polynomial-time in the case\nof sparse coding, this is still very expensive, and unnecessary in other types of models (such as\nthe stacked auto-encoders discussed below). In fact, exact inference becomes intractable for deeper\nmodels, as discussed in section 5.\nAlthough RBMs enjoy tractable inference, this is obtained at the cost of a lack of explaining away\nbetween the hidden units, which could potentially limit the representational power of E[h | x] as a\ngood representation for the factors that could have generated x. However, RBMs are often used as\nbuilding blocks for training deeper graphical models such as the deep belief network (DBN) (Hinton\net al., 2006) and sthe deep Boltzmann machine (DBM) (Salakhutdinov and Hinton, 2009), which\ncan compensate for the lack of explaining away in the RBM hidden units via a rich prior (provided\nby the upper layers) which can introduce potentially complex interactions and competition between\nthe hidden units. Note that there is explaining away (and intractable exact inference) iin DBNs and\nsomething analogous in DBMs.\n2.4\nRegularized Auto-Encoders\nAuto-encoders include in their training criterion a form of reconstruction oerror, such as ||r(x)−x||2,\nwhere r(·) is the learned reconstruction function, often decomposed as r(x) = g(f(x)) where f(·) is\nan encoding function and g(·) a decoding function. The idea is that auto-encoders should have low\nreconstruction error at the training examples, but high reconstruction error in most other conﬁg-\nurations of the input. In the case of auto-encoders, good generalization means that test examples\n(sampled from the same distribution as training examples) also get low reconstruction error. Auto-\nencoders have to be regularized to prevent them from simply learning the identity function r(x) = x,\nwhich would be useless. Regularized auto-encoders include the old bottleneck auto-encoders (like in\nPCA) with less hidden units than input, as well as the denoising auto-encoders (Vincent et al.,\n2008) and contractive auto-encoders (Rifai et al., 2011a). The denoising auto-encoder takes a noisy\nversion N(x) of original input x and tries to reconstruct x, e.g., it minimizes ||r(N(x)) −x||2. The\ncontractive auto-encoder has a regularization penalty in addition to the reconstruction error, trying\nto make hidden units f(x) as constant as possible with respect to x (minimizing the contractive\n4 Directed and undirected: just two diﬀerent views on the semantics of probabilistic models, not mutually\nexclusive, but views that are more convenient for some models than others.\n5 ﬁnding h that approximately maximizes P(h | x)\nDeep Learning of Representations: Looking Forward\n5\npenalty || ∂f(x)\n∂x ||2\nF ). A Taylor expansion of the denoising error shows that it is also approximately\nequivalent to minimizing reconstruction error plus a contractive penalty on r(·) (Alain and Bengio,\n2013). As explained in Bengio et al. (2013d), the tug-of-war between minimization of reconstruction\nerror and the regularizer means that the intermediate representation must mostly capture the vari-\nations necessary to distinguish training examples, i.e., the directions of variations on the manifold\n(a lower dimensional region) near which the data generating distribution concentrates. Score match-\ning (Hyv¨arinen, 2005) is an inductive principle that can be an interesting alternative to maximum\nlikelihood, and several nconnections have been drawn between reconstruction error in auto-encoders\nand score matching (Swersky et al., 2011). It has also been shown that denoising auto-encoders\nand some forms of contractive auto-encoders estimate the score6 of the underlying data generat-\ning distribution (Vincent, 2011; Alain and Bengio, 2013). This can be used to endow regularized\nauto-encoders with a probabilistic interpretation and to sample from the implicitly learned density\nmodels (Rifai et al., 2012b; Bengio et al., 2012; Alain and Bengio, 2013) through some variant of\nLangevin or Metropolis-Hastings Monte-Carlo Markov chains (MCMC). More recently, the results\nfrom Alain and Bengio (2013) have been generalized: whereas the score estimation result was only\nvalid for asymptotically small Gaussian corruption noise, squared reconstruction error, and contin-\nuous inputs, the result from Bengio et al. (2013c) is applicable for any type of input, any form of\nreconstruction loss (so long as it is a negative log-likelihood), any form of corruption (so long as it\nprevents learning the identity mapping) and does not depend on the level of corruption noise going\nto zero.\nEven though there is a probabilistic interpretation to regularized auto-encoders, this interpreta-\ntion does not involve the deﬁnition of intermediate anonymous latent variables. Instead, they are\nbased on the construction of a direct parametrization of an encoding function which immediately\nmaps an input x to its representation f(x), and they are motivated by geometrical considerations in\nthe spirit of manifold learning algorithms (Bengio et al., 2013d). Consequently, there is no issue of\ntractability of inference, even with deep auto-encoders obtained by stacking single-layer ones. This\nis true even in the recently proposed multi-layer versions of the denoising auto-encoders (Bengio\nand Thibodeau-Laufer, 2013) in which noise is injected not just in input, but in hidden units (like\nin the Gibbs chain of a deep Boltzmann machine).\nIt was previously believed (Ranzato et al., 2008), including by the author himself, that recon-\nstruction error should only be small where the estimated density has a peak, e.g., near the data.\nHowever, recent theoretical and empirical results (Alain and Bengio, 2013) show that the reconstruc-\ntion error will be small where the estimated density has a peak (a mode) but also where it has a\ntrough (a minimum). This is because the reconstruction error vector (reconstruction minus input)\nestimates the score ∂log p(x)\n∂x\n, i.e., the reconstruction error is small where || ∂log p(x)\n∂x\n|| is small. This\ncan happen at a local maximum but also at a local minimum (or saddle point) of the estimated\ndensity. This argues against using reconstruction error itself as an energy function,7 which should\nonly be low near high probability points.\n2.5\nSparse Coding and PSD\nSparse coding (Olshausen and Field, 1996) is a particular kind of directed graphical model with\na linear relationship between visible and latent variables (like in PCA), but in which the latent\nvariables have a prior (e.g., Laplace density) that encourages sparsity (many zeros) in the MAP\nposterior. Sparse coding is not actually very good as a generative model, but has been very successful\nfor unsupervised feature learning (Raina et al., 2007; Coates and Ng, 2011; Yu et al., 2011; Grosse\net al., 2007; Jenatton et al., 2009; Bach et al., 2011). See Bengio et al. (2013d) for a brief overview in\nthe context of deep learning, along with connections to other unsupervised representation learning\nalgorithms. Like other directed graphical models, it requires somewhat expensive inference, but the\n6 derivative of the log-density with respect to the data; this is diﬀerent from the usual deﬁnition of score\nin statistics, where the derivative is with respect to the parameters\n7 To deﬁne energy, we write probability as the normalized exponential of minus the energy.\n6\nY. Bengio\ngood news is that for sparse coding, MAP inference is a convex optimization problem for which\nseveral fast approximations have been proposed (Mairal et al., 2009; Gregor and LeCun, 2010a). It\nis interesting to note the results obtained by Coates and Ng (2011) which suggest that sparse coding\nis a better encoder but not a better learning algorithm than RBMs and sparse auto-encoders (none of\nwhich has explaining away). Note also that sparse coding can be generalized into the spike-and-slab\nsparse coding algorithm (Goodfellow et al., 2012), in which MAP inference is replaced by variational\ninference, and that was used to win the NIPS 2011 transfer learning challenge (Goodfellow et al.,\n2011).\nAnother interesting variant on sparse coding is the predictive sparse coding (PSD) algorithm (Kavukcuoglu\net al., 2008) and its variants, which combine properties of sparse coding and of auto-encoders. Sparse\ncoding can be seen as having only a parametric “generative” decoder (which maps latent variable\nvalues to visible variable values) and a non-parametric encoder (ﬁnd the latent variables value that\nminimizes reconstruction error and minus the log-prior on the latent variable). PSD adds a para-\nmetric encoder (just an aﬃne transformation followed by a non-linearity) and learns it jointly with\nthe generative model, such that the output of the parametric encoder is close to the latent variable\nvalues that reconstructs well the input.\n3\nScaling Computations\nFrom a computation point of view, how do we scale the recent successes of deep learning to much\nlarger models and huge datasets, such that the models are actually richer and capture a very large\namount of information?\n3.1\nScaling Computations: The Challenge\nThe beginnings of deep learning in 2006 have focused on the MNIST digit image classiﬁcation\nproblem (Hinton et al., 2006; Bengio et al., 2007), breaking the supremacy of SVMs (1.4% error)\non this dataset.8 The latest records are still held by deep networks: Ciresan et al. (2012) currently\nclaim the title of state-of-the-art for the unconstrained version of the task (e.g., using a convolutional\narchitecture and stochastically deformed data), with 0.27% error.\nIn the last few years, deep learning has moved from digits to object recognition in natural\nimages, and the latest breakthrough has been achieved on the ImageNet dataset.9 bringing down\nthe state-of-the-art error rate (out of 5 guesses) from 26.1% to 15.3% (Krizhevsky et al., 2012)\nTo achieve the above scaling from 28×28 grey-level MNIST images to 256×256 RGB images,\nresearchers have taken advantage of convolutional architectures (meaning that hidden units do not\nneed to be connected to all units at the previous layer but only to those in the same spatial area,\nand that pooling units reduce the spatial resolution as we move from lower to higher layers). They\nhave also taken advantage of GPU technology to speed-up computation by one or two orders of\nmagnitude (Raina et al., 2009; Bergstra et al., 2010, 2011; Krizhevsky et al., 2012).\nWe can expect computational power to continue to increase, mostly through increased parallelism\nsuch as seen in GPUs, multicore machines, and clusters. In addition, computer memory has become\nmuch more aﬀordable, allowing (at least on CPUs) to handle potentially huge models (in terms of\ncapacity).\nHowever, whereas the task of recognizing handwritten digits is solved to the point of achieving\nroughly human-level performance, this is far from true for tasks such as general object recognition,\nscene understanding, speech recognition, or natural language understanding. What is needed to nail\nthose tasks and scale to even more ambitious ones?\n8 for the knowledge-free version of the task, where no image-speciﬁc prior is used, such as image deformations\nor convolutions, where the current state-of-the-art is around 0.8% and involves deep learning (Rifai et al.,\n2011b; Hinton et al., 2012b).\n9 The 1000-class ImageNet benchmark, whose results are detailed here:\nhttp://www.image-net.org/challenges/LSVRC/2012/ results.html\nDeep Learning of Representations: Looking Forward\n7\nAs we approach AI-scale tasks, it should become clear that our trained models will need to\nbe much larger in terms of number of parameters. This is suggested by two observations. First,\nAI means understanding the world around us at roughly the same level of competence as humans.\nExtrapolating from the current state of machine learning, the amount of knowledge this represents is\nbound to be large, many times more than what current models can capture. Second, more and more\nempirical results with deep learning suggest that larger models systematically work better (Coates\net al., 2011; Hinton et al., 2012b; Krizhevsky et al., 2012; Goodfellow et al., 2013b), provided\nappropriate regularization is used, such as the dropouts technique described above.\nPart of the challenge is that the current capabilities of a single computer are not suﬃcient\nto achieve these goals, even if we assume that training complexity would scale linearly with the\ncomplexity of the task. This has for example motivated the work of the Google Brain team (Le\net al., 2012; Dean et al., 2012) to parallelize training of deep nets over a very large number of nodes.\nAs we will see in Section 4, we hypothesize that as the size of the models increases, our current\nways of training deep networks become less and less eﬃcient, so that the computation required to\ntrain larger models (to capture correspondingly more information) is likely to scale much worse than\nlinearly (Dauphin and Bengio, 2013).\nAnother part of the challenge is that the increase in computational power has been mostly coming\n(and will continue to come) from parallel computing. Unfortunately, when considering very large\ndatasets, our most eﬃcient training algorithms for deep learning (such as variations on stochastic\ngradient descent or SGD) are inherently sequential (each update of the parameters requires having\ncompleted the previous update, so they cannot be trivially parallelized). Furthermore, for some tasks,\nthe amount of available data available is becoming so large that it does not ﬁt on a disk or even\non a ﬁle server, so that it is not clear how a single CPU core could even scan all that data (which\nseems necessary in order to learn from it and exploit all of it, if training is inherently sequential).\n3.2\nScaling Computations: Solution Paths\nParallel Updates: Asynchronous SGD. One idea that we explored in Bengio et al. (2003) is that\nof asynchronous SGD: train multiple versions of the model in parallel, each running on a diﬀerent\nnode and seeing diﬀerent subsets of the data (on diﬀerent disks), but with an asynchronous lock-free\nsharing mechanism which keeps the diﬀerent versions of the model not too far from each other. If\nthe sharing were synchronous, it would be too ineﬃcient because most nodes would spend their time\nwaiting for the sharing to be completed and would be waiting for the slowest of the nodes. This idea\nhas been analyzed theoretically (Recht et al., 2011) and successfully engineered on a grand scale\nrecently at Google (Le et al., 2012; Dean et al., 2012). However, current large-scale implementations\n(with thousands of nodes) are still very ineﬃcient (in terms of use of the parallel resources), mostly\nbecause of the communication bottleneck requiring to regularly exchange parameter values between\nnodes. The above papers also take advantage of a way to train deep networks which has been\nvery successful for GPU implementations, namely the use of rather large minibatches (blocks of\nexamples after which an update is performed), making some parallelization (across the examples in\nthe minibatch) easier. One option, explored by Coates et al. (2012) is to use as building blocks for\nlearning features algorithms such as k-means that can be run eﬃciently over large minibatches (or\nthe whole data) and thus parallelized easily on a cluster (they learned 150,000 features on a cluster\nwith only 30 machines).\nAnother interesting consideration is the optimization of trade-oﬀbetween communication cost\nand computation cost in distributed optimization algorithms, e.g., as discussed in Tsianos et al.\n(2012).\nSparse Updates. One idea that we propose here is to change the learning algorithms so as to obtain\nsparse updates, i.e., for any particular minibatch there is only a small fraction of parameters that\nare updated. If the amount of sparsity in the update is large, this would mean that a much smaller\nfraction of the parameters need to be exchanged between nodes when performing an asynchronous\n8\nY. Bengio\nSGD10. Sparse updates could be obtained simply if the gradient is very sparse. This gradient sparsity\ncan arise with approaches that select paths in the neural network. We already know methods which\nproduce slightly sparse updates, such as dropouts (Hinton et al., 2012b),11 maxout (Goodfellow et al.,\n2013b)12 and other hard-pooling mechanisms, such as the recently proposed and very successful\nstochastic pooling (Zeiler and Fergus, 2013). These methods do not provide enough sparsity, but\nthis could be achieved in two ways. First of all, we could choose to only pay attention to the largest\nelements of the gradient vector. Second, we could change the architecture along the lines proposed\nnext.\nConditional Computation. A central idea (that applies whether one parallelizes or not) that we\nput forward is that of conditional computation: instead of dropping out paths independently and at\nrandom, drop them in a learned and optimized way. Decision trees remain some of the most appeal-\ning machine learning algorithms because prediction time can be on the order of the logarithm of\nthe number of parameters. Instead, in most other machine learning predictors, scaling is linear (i.e.,\nmuch worse). This is because decision trees exploit conditional computation: for a given example, as\nadditional computations are performed, one can discard a gradually larger set of parameters (and\navoid performing the associated computation). In deep learning, this could be achieved by com-\nbining truly sparse activations (values not near zero like in sparse auto-encoders, but actual zeros)\nand multiplicative connections whereby some hidden units gate other hidden units (when the gater\noutput is zero it turns oﬀthe output of the gated unit). When a group A of hidden units has a\nsparse activation pattern (with many actual zeros) and it multiplicatively gates other hidden units\nB, then only a small fraction of the hidden units in B may need to be actually computed, because\nwe know that these values will not be used. Such gating is similar to what happens when a decision\nnode of a decision tree selects a subtree and turns oﬀanother subtree. More savings can thus be\nachieved if units in B themselves gate other units, etc. The crucial diﬀerence with decision trees (and\ne.g., the hard mixture of experts we introduced a decade ago (Collobert et al., 2003)) is that the\ngating units should not be mutually exclusive and should instead form a distributed pattern. Indeed,\nwe want to keep the advantages of distributed representations and avoid the limited local general-\nization suﬀered by decision trees (Bengio et al., 2010). With a high level of conditional computation,\nsome parameters are used often (and are well tuned) whereas other parameters are used very rarely,\nrequiring more data to estimate. A trade-oﬀand appropriate regularization therefore needs to be\nestablished which will depend on the amount of training signals going into each parameter. Inter-\nestingly, conditional computation also helps to achieve sparse gradients, and the fast convergence of\nhard mixtures of experts (Collobert et al., 2003) provides positive evidence that a side beneﬁt of\nconditional computation will be easier and faster optimization.\nAnother existing example of conditional computation and sparse gradients is with the ﬁrst layer\nof neural language models, deep learning models for text data (Bengio et al., 2003; Bengio, 2008). In\nthat case, there is one parameter vector per word in the vocabulary, but each sentence only “touches”\nthe parameters associated with the words in the sentence. It works because the input can be seen as\nextremely sparse. The question is how to perform conditional computation in the rest of the model.\nOne issue with the other example we mentioned, hard mixtures of experts (Collobert et al.,\n2003), is that its training mechanism only make sense when the gater operates at the output layer.\nIn that case, it is easy to get a strong and clean training signal for the gater output: one can\njust evaluate what the error would have been if a diﬀerent expert had been chosen, and train the\ngater to produce a higher output for the expert that would have produced the smallest error (or to\nreduce computation and only interrogate two experts, require that the gater correctly ranks their\n10 although the gain would be reduced considerably in a minibatch mode, roughly by the size of the minibatch\n11 where half of the hidden units are turned oﬀ, although clearly, this is not enough sparsity for reaching our\nobjective; unfortunately, we observed that randomly and independently dropping a lot more than half of\nthe units yielded substantially worse results\n12 where in addition to dropouts, only one out of k ﬁlters wins the competition in max-pooling units, and\nonly one half of those survives the dropouts masking, making the sparsity factor 2k\nDeep Learning of Representations: Looking Forward\n9\nprobability of being the best one). The challenge is how to produce training signals for gating units\nthat operate in the middle of the model. One cannot just enumerate all the gating conﬁgurations,\nbecause in a distributed setting with many gating units, there will be an exponential number of\nconﬁgurations. Interestingly, this suggests introducing randomness in the gating process itself, e.g.,\nstochastically choosing one or two choices out of the many that a group of gating units could\ntake. This is interesting because this is the second motivation (after the success of dropouts as a\nregularizer) for re-introducing randomness in the middle of deep networks. This randomness would\nallow conﬁgurations that would otherwise not be selected (if only a kind of “max” dictated the\ngating decision) to be sometimes selected, thus allowing to accumulate a training signal about the\nvalue of this conﬁguration, i.e., a training signal for the gater. The general question of estimating or\npropagating gradients through stochastic neurons is treated in another exploratory article (Bengio,\n2013a), where it is shown that one can obtain an unbiased (but noisy) estimator of the gradient of\na loss through a discrete stochastic decision. Another interesting idea explored in that paper is that\nof adding noise just before the non-linearity (max-pooling (maxi xi) or rectiﬁer (max(0, x))). Hence\nthe winner is not always the same, and when a choice wins it has a smooth inﬂuence on the result,\nand that allows a gradient signal to be provided, pushing that winner closer or farther from winning\nthe competition on another example.\n4\nOptimization\n4.1\nOptimization: The Challenge\nAs we consider larger and larger datasets (growing faster than the size of the models), training error\nand generalization error converge. Furthermore many pieces of evidence in the results of experiments\non deep learning suggest that training deep networks (including recurrent networks) involves a\ndiﬃcult optimization (Bengio, 2013b; Gulcehre and Bengio, 2013; Bengio et al., 2013a). It is not\nyet clear how much of the diﬃculty is due to local minima and how much is due to ill-conditioning\n(the two main types of optimization diﬃculties in continuous optimization problems). It is therefore\ninteresting to study the optimization methods and diﬃculties involved in deep learning, for the sake\nof obtaining better generalization. Furthermore, better optimization could also have an impact on\nscaling computations, discussed above.\nOne important thing to keep in mind, though, is that in a deep supervised network, the top\ntwo layers (the output layer and the top hidden layer) can rather easily be made to overﬁt, simply\nby making the top hidden layer large enough. However, to get good generalization, what we have\nfound is that one needs to optimize the lower layers, those that are far removed from the immediate\nsupervised training signal (Bengio et al., 2007). These observations mean that only looking at the\ntraining criterion is not suﬃcient to assess that a training procedure is doing a good job at optimizing\nthe lower layers well. However, under constraints on the top hidden layer size, training error can be\na good guide to the quality of the optimization of lower layers. Note that supervised deep nets are\nvery similar (in terms of the optimization problem involved) to deep auto-encoders and to recurrent\nor recursive networks, and that properly optimizing RBMs (and more so deep Boltzmann machines)\nseems more diﬃcult: progress on training deep nets is therefore likely to be a key to training the\nother types of deep learning models.\nOne of the early hypotheses drawn from experiments with layer-wise pre-training as well as\nof other experiments (semi-supervised embeddings (Weston et al., 2008) and slow feature analy-\nsis (Wiskott and Sejnowski, 2002a; Bergstra and Bengio, 2009)) is that the training signal provided\nby backpropagated gradients is sometimes too weak to properly train intermediate layers of a deep\nnetwork. This is supported by the observation that all of these successful techniques somehow inject a\ntraining signal into the intermediate layers, helping them to ﬁgure out what they should do. However,\nthe more recent successful results with supervised learning on very large labeled datasets suggest that\nwith some tweaks in the optimization procedure (including initialization), it is sometimes possible\nto achieve as good results with or without unsupervised pre-training or semi-supervised embedding\nintermediate training signals.\n10\nY. Bengio\n4.2\nOptimization: Solution Paths\nIn spite of these recent encouraging results, several more recent experimental results again point to\na fundamental diﬃculty in training intermediate and lower layers.\nDiminishing Returns with Larger Networks. First, Dauphin and Bengio (2013) show that\nwith well-optimized SGD training, as the size of a neural net increases, the “return on investment”\n(number of training errors removed per added hidden unit) decreases, given a ﬁxed number of training\niterations, until the point where it goes below 1 (which is the return on investment that would be\nobtained by a brain-dead memory-based learning mechanism – such as Parzen Windows – which\njust copies an incorrectly labeled example into the weights of the added hidden unit so as to produce\njust the right answer for that example only). This suggests that larger models may be fundamentally\nmore diﬃcult to train, probably because there are now more second-order interactions between the\nparameters, increasing the condition number of the Hessian matrix (of second derivatives of model\nparameters with respect to the training criterion). This notion of return on investment may provide\na useful metric by which to measure the eﬀect of diﬀerent methods to improve the scaling behavior\nof training and optimization procedures for deep learning.\nIntermediate Concepts Guidance and Curriculum. Second, Gulcehre and Bengio (2013) show\nthat there are apparently simple tasks on which standard black-box machine learning algorithms\ncompletely fail. Even supervised and pre-trained deep networks were tested and failed at these tasks.\nThese tasks have in common the characteristic that the correct labels are obtained by the composi-\ntion of at least two levels of non-linearity and abstraction: e.g., the ﬁrst level involves the detection\nof objects in a scene and the second level involves a non-linear logical operation on top of these (such\nas the detecting presence of multiple objects of the same category). On the other hand, the task\nbecomes easily solvable by a deep network whose intermediate layer is ﬁrst pre-trained to solve the\nﬁrst-level sub-task. This raises the question of how humans might learn even more abstract tasks,\nand Bengio (2013b) studies the hypothesis that the use of language and the evolution of culture\ncould have helped humans reduce that diﬃculty (and gain a serious advantage over other less cul-\ntured animals). It would be interesting to explore multi-agent learning mechanisms inspired by the\nthe mathematical principles behind the evolution of culture in order to bypass this optimization\ndiﬃculty. The basic idea is that humans (and current learning algorithms) are limited to “local\ndescent” optimization methods, that make small changes in the parameter values with the eﬀect of\nreducing the expected loss in average. This is clearly prone to the presence of local minima, while\na more global search (in the spirit of both genetic and cultural evolution) could potentially reduce\nthis diﬃculty. One hypothesis is that more abstract learning tasks involve more challenging opti-\nmization diﬃculties, which would make such global optimization algorithms necessary if we want\ncomputers to learn such abstractions from scratch. Another option, following the idea of curriculum\nlearning (Bengio et al., 2009), is to provide guidance ourselves to learning machines (as exempliﬁed\nin the toy example of Gulcehre and Bengio (2013)), by “teaching them” gradually more complex\nconcepts to help them understand the world around us (keeping in mind that we also have to do\nthat for humans and that it takes 20 years to complete).\nChanging the learning procedure and the architecture. Regarding the basic optimization\ndiﬃculty of a single deep network, three types of solutions should be considered. First, there are\nsolutions based on improved general-purpose optimization algorithms, such as for example the recent\nwork on adaptive learning rates (Schaul et al., 2012), online natural gradient (Le Roux et al., 2008;\nPascanu and Bengio, 2013) or large-minibatch second order methods (Martens, 2010).\nAnother class of attacks on the optimization problem is based on changing the architecture (fam-\nily of functions and its parametrization) or the way that the outputs are produced (for example by\nadding noise). As already introduced in LeCun et al. (1998a), changes in the preprocessing, training\nobjective and architecture can change the diﬃculty of optimization, and in particularly improve the\nDeep Learning of Representations: Looking Forward\n11\nconditioning of the Hessian matrix (of second derivatives of the loss with respect to parameters).\nWith gradient descent, training time into a quadratic bowl is roughly proportional to the condition\nnumber of the Hessian matrix (ratio of largest to smallest eigenvalue). For example LeCun et al.\n(1998a) recommends centering and normalizing the inputs, an idea recently extended to hidden lay-\ners of Boltzmann machines with success (Montavon and Muller, 2012). A related idea that may have\nan impact on ill-conditioning is the idea of skip-connections, which forces both the mean output and\nthe mean slope of each hidden unit of a deep multilayer network to be zero (Raiko et al., 2012), a\ncentering idea which originates from Schraudolph (1998).\nThere has also been very successful recent work exploiting rectiﬁer non-linearities for deep su-\npervised networks (Glorot et al., 2011a; Krizhevsky et al., 2012). Interestingly, such non-linearities\ncan produce rather sparse unit outputs, which could be exploited, if the amount of sparsity is suf-\nﬁciently large, to considerably reduce the necessary computation (because when a unit output is 0,\nthere is no need to actually multiply it with its outgoing weights). Very recently, we have discovered\na variant on the rectiﬁer non-linearity called maxout (Goodfellow et al., 2013b) which appears to\nopen a very promising door towards more eﬃcient training of deep networks. As conﬁrmed exper-\nimentally (Goodfellow et al., 2013b), maxout networks can train deeper networks and allow lower\nlayers to undergo more training. The more general principle at stake here may be that when the\ngradient is sparse, i.e., only a small subset of the hidden units and parameters is touched by the\ngradient, the optimization problem may become easier. We hypothesize that sparse gradient vec-\ntors have a positive eﬀect on reducing the ill-conditioning diﬃculty involved in training deep nets.\nThe intuition is that by making many terms of the gradient vector 0, one also knocks oﬀmany\noﬀ-diagonal terms of the Hessian matrix, making this matrix more diagonal-looking, which would\nreduce many of the ill-conditioning eﬀects involved, as explained below. Indeed, gradient descent\nrelies on an invalid assumption: that one can modify a parameter θi (in the direction of the gradient\n∂C\n∂θi ) without taking into account the changes in ∂C\n∂θi that will take place when also modifying other\nparameters θj. Indeed, this is precisely the information that is captured (e.g. with second-order\nmethods) by the oﬀ-diagonal entries\n∂2C\n∂θi∂θj =\n∂\n∂θj\n∂C\n∂θi , i.e., how changing θj changes the gradient\non θi. Whereas second-order methods may have their own limitations13 it would be interesting if\nsubstantially reduced ill-conditioning could be achieved by modifying the architecture and training\nprocedure. Sparse gradients would be just one weapon in this line of attack.\nAs we have argued above, adding noise in an appropriate way can be useful as a powerful\nregularizer (as in dropouts), and it can also be used to make the gradient vector sparser, which\nwould reinforce the above positive eﬀect on the optimization diﬃculty. If some of the activations are\nalso sparse (as our suggestions for conditional computation would require), then more entries of the\ngradient vector will be zeroed out, also reinforcing that beneﬁcial optimization eﬀect. In addition,\nit is plausible that the masking noise found in dropouts (as well as in denoising auto-encoders)\nencourages a faster symmetry-breaking: quickly moving away from the condition where all hidden\nunits of a neural network or a Boltzmann machine do the same thing (due to a form of symmetry\nin the signals they receive), which is a non-attractive ﬁxed point with a ﬂat (up to several orders)\nlikelihood function. This means that gradient descent can take a lot of time to pull apart hidden\nunits which are behaving in a very similar way. Furthermore, when starting from small weights,\nthese symmetry conditions (where many hidden units do something similar) are actually attractive\nfrom far away, because initially all the hidden units are trying to grab the easiest and most salient\njob (explain the gradients on the units at the layer above). By randomly turning oﬀhidden units\nwe obtain a faster specialization which helps training convergence.\nA related concept that has been found useful in understanding and reducing the training diﬃculty\nof deep or recurrent nets is the importance of letting the training signals (back-propagated gradients)\nﬂow, in a focused way. It is important that error signals ﬂow so that credit and blame is clearly\nassigned to diﬀerent components of the model, those that could change slightly to improve the\ntraining loss. The problem of vanishing and exploding gradients in recurrent nets (Hochreiter, 1991;\n13 ﬁrst, practical implementations never come close to actually inverting the Hessian, and second, they often\nrequire line searches that may be computationally ineﬃcient if the optimal trajectory is highly curved\n12\nY. Bengio\nBengio et al., 1994) arises because the eﬀect of a long series of non-linear composition tends to\nproduce gradients that can either be very small (and the error signal is lost) or very large (and the\ngradient steps diverge temporarily). This idea has been exploited to propose successful initialization\nprocedures for deep nets (Glorot and Bengio, 2010). A composition of non-linearities is associated\nwith a product of Jacobian matrices, and a way to reduce the vanishing problem would be to make\nsure that they have a spectral radius (largest eigenvalue) close to 1, like what is done in the weight\ninitialization for Echo State Networks (Jaeger, 2007) or in the carousel self-loop of LSTM (Hochreiter\nand Schmidhuber, 1997) to help propagation of inﬂuences over longer paths. A more generic way\nto avoid gradient vanishing is to incorporate a training penalty that encourages the propagated\ngradient vectors to maintain their magnitude (Pascanu and Bengio, 2012). When combined with a\ngradient clipping14 heuristic (Mikolov, 2012) to avoid the detrimental eﬀect of overly large gradients,\nit allows to train recurrent nets on tasks on which it was not possible to train them before (Pascanu\nand Bengio, 2012).\n5\nInference and Sampling\nAll of the graphical models studied for deep learning except the humble RBM require a non-trivial\nform of inference, i.e., guessing values of the latent variables h that are appropriate for the given\nvisible input x. Several forms of inference have been investigated in the past: MAP inference is\nformulated like an optimization problem (looking for h that approximately maximizes P(h | x));\nMCMC inference attempts to sample a sequence of h’s from P(h | x); variational inference looks for a\nsimple (typically factorial) approximate posterior qx(h) that is close to P(h | x), and usually involves\nan iterative optimization procedure. See a recent machine learning textbook for more details (Bishop,\n2006; Barber, 2011; Murphy, 2012).\nIn addition, a challenge related to inference is sampling (not just from P(h | x) but also from\nP(h, x) or P(x)), which like inference is often needed in the inner loop of learning algorithms for\nprobabilistic models with latent variables, energy-based models (LeCun et al., 2006) or Markov\nRandom Fields (Kindermann, 1980) (also known as undirected graphical models), where P(x) or\nP(h, x) is deﬁned in terms of a parametrized energy function whose normalized exponential gives\nprobabilities.\nDeep Boltzmann machines (Salakhutdinov and Hinton, 2009) combine the challenge of inference\n(for the “positive phase” where one tries to push the energies associated with the observed x down)\nand the challenge of sampling (for the “negative phase” where one tries to push up the energies\nassociated with x’s sampled from P(x)). Sampling for the negative phase is usually done by MCMC,\nalthough some learning algorithms (Collobert and Weston, 2008; Gutmann and Hyvarinen, 2010;\nBordes et al., 2013) involve “negative examples” that are sampled through simpler procedures (like\nperturbations of the observed input). In Salakhutdinov and Hinton (2009), inference for the positive\nphase is achieved with a mean-ﬁeld variational approximation.15\n5.1\nInference and Sampling: The Challenge\nThere are several challenges involved with all of the these inference and sampling techniques.\nThe ﬁrst challenge is practical and computational: these are all iterative procedures that can\nconsiderably slow down training (because inference and/or sampling is often in the inner loop of\nlearning).\n14 When the norm of the gradient is above a threshold τ, reduce it to τ\n15 In the mean-ﬁeld approximation, computation proceeds like in Gibbs sampling, but with stochastic binary\nvalues replaced by their conditional expected value (probability of being 1), given the outputs of the\nother units. This deterministic computation is iterated like in a recurrent network until convergence is\napproached, to obtain a marginal (factorized probability) approximation over all the units.\nDeep Learning of Representations: Looking Forward\n13\nPotentially Huge Number of Modes. The second challenge is more fundamental and has to do\nwith the potential existence of highly multi-modal posteriors: all of the currently known approaches\nto inference and sampling are making very strong explicit or implicit assumptions on the form the\ndistribution of interest (P(h | x) or P(h, x)). As we argue below, these approaches make sense if this\ntarget distribution is either approximately unimodal (MAP), (conditionally) factorizes (variational\napproximations, i.e., the diﬀerent factors hi are approximately independent16 of each other given x),\nor has only a few modes between which it is easy to mix (MCMC). However, approximate inference\ncan be potentially hurtful, not just at test time but for training, because it is often in the inner loop\nof the learning procedure (Kulesza and Pereira, 2008).\nImagine for example that h represents many explanatory variables of a rich audio-visual scene\nwith a highly ambiguous raw input x, including the presence of several objects with ambiguous\nattributes or categories, such that one cannot really disambiguate one of the objects independently\nof the others (the so-called “structured output” scenario, but at the level of latent explanatory\nvariables). Clearly, a factorized or unimodal representation would be inadequate (because these\nvariables are not at all independent, given x) while the number of modes could grow exponentially\nwith the number of ambiguous factors present in the scene. For example, consider a visual scene\nx through a haze hiding most details, yielding a lot of uncertainty. Say it involves 10 objects (e.g.,\npeople), each having 5 ambiguous binary attributes (out of 20) (e.g., how they are dressed) and\nuncertainty between 100 categorical choices for each element (e.g., out of 10000 persons in the\ndatabase, the marginal evidence allows to reduce the uncertainty for each person to about 100\nchoices). Furthermore, suppose that these uncertainties cannot be factorized (e.g., people tend to be\nin the same room with other people involved in the same activity, and friends tend to stand physically\nclose to each other, and people choose to dress in a way that socially coherent). To make life hard\non mean-ﬁeld and other factorized approximations, this means that only a small fraction (say 1%)\nof these conﬁgurations are really compatible. So one really has to consider 1% × (25 × 100)10 ≈1033\nplausible conﬁgurations of the latent variables. If one has to take a decision y based on x, e.g.,\nP(y | x) = P\nh P(y | h)P(h | x) involves summing over a huge number of non-negligible terms of the\nposterior P(h | x), which we can consider as modes (the actual dimension of h is much larger, so we\nhave reduced the problem from (220 × 10000)10 ≈10100 to about 1033, but that is still huge. One\nway or another, summing explicitly over that many modes seems implausible, and assuming single\nmode (MAP) or a factorized distribution (mean-ﬁeld) would yield very poor results. Under some\nassumptions on the underlying data-generating process, it might well be possible to do inference that\nis exact or a provably good approximations, and searching for graphical models with these properties\nis an interesting avenue to deal with this problem. Basically, these assumptions work because we\nassume a speciﬁc structure in the form of the underlying distribution. Also, if we are lucky, a few\nMonte-Carlo samples from P(h | x) might suﬃce to obtain an acceptable approximation for our\ny, because somehow, as far as y is concerned, many probable values of h yield the same answer y\nand a Monte-Carlo sample will well represent these diﬀerent “types” of values of h. That is one\nform of regularity that could be exploited (if it exists) to approximately solve that problem. What\nif these assumptions are not appropriate to solve challenging AI problems? Another, more general\nassumption (and thus one more likely to be appropriate for these problems) is similar to what we\nusually do with machine learning: although the space of functions is combinatorially large, we are\nable to generalize by postulating a rather large and ﬂexible family of functions (such as a deep neural\nnet). Thus an interesting avenue is to assume that there exists a computationally tractable function\nthat can compute P(y | x) in spite of the apparent complexity of going through the intermediate\nsteps involving h, and that we may learn P(y | x) through (x, y) examples. This idea will be\ndeveloped further in Section 5.2.\nMixing Between Modes. What about MCMC methods? They are hurt by the problem of mode\nmixing, discussed at greater length in Bengio et al. (2013b), and summarized here. To make the\n16 this can be relaxed by considering tree-structured conditional dependencies (Saul and Jordan, 1996) and\nmixtures thereof\n14\nY. Bengio\nmental picture simpler, imagine that there are only two kinds of probabilities: tiny and high. MCMC\ntransitions try to stay in conﬁgurations that have a high probability (because they should occur in the\nchain much more often than the tiny probability conﬁgurations). Modes can be thought of as islands\nof high probability, but they may be separated by vast seas of tiny probability conﬁgurations. Hence,\nit is diﬃcult for the Markov chain of MCMC methods to jump from one mode of the distribution\nto another, when these are separated by large low-density regions embedded in a high-dimensional\nspace, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005;\nNarayanan and Mitter, 2010). This hypothesis states that natural classes present in the data (e.g.,\nvisual object categories) are associated with low-dimensional regions17 (i.e., manifolds) near which\nthe distribution concentrates, and that diﬀerent class manifolds are well-separated by regions of\nvery low density. Here, what we consider a mode may be more than a single point, it could be a\nwhole (low-dimensional) manifold. Slow mixing between modes means that consecutive samples tend\nto be correlated (belong to the same mode) and that it takes a very large number of consecutive\nsampling steps to go from one mode to another and even more to cover all of them, i.e., to obtain\na large enough representative set of samples (e.g. to compute an expected value under the sampled\nvariables distribution). This happens because these jumps through the low-density void between\nmodes are unlikely and rare events. When a learner has a poor model of the data, e.g., in the\ninitial stages of learning, the model tends to correspond to a smoother and higher-entropy (closer\nto uniform) distribution, putting mass in larger volumes of input space, and in particular, between\nthe modes (or manifolds). This can be visualized in generated samples of images, that look more\nblurred and noisy18. Since MCMCs tend to make moves to nearby probable conﬁgurations, mixing\nbetween modes is therefore initially easy for such poor models. However, as the model improves and\nits corresponding distribution sharpens near where the data concentrate, mixing between modes\nbecomes considerably slower. Making one unlikely move (i.e., to a low-probability conﬁguration)\nmay be possible, but making N such moves becomes exponentially unlikely in N. Making moves\nthat are far and probable is fundamentally diﬃcult in a high-dimensional space associated with\na peaky distribution (because the exponentially large fraction of the far moves would be to an\nunlikely conﬁguration), unless using additional (possibly learned) knowledge about the structure of\nthe distribution.\n5.2\nInference and Sampling: Solution Paths\nGoing into a space where mixing is easier. The idea of tempering (Iba, 2001) for MCMCs\nis analogous to the idea of simulated annealing (Kirkpatrick et al., 1983) for optimization, and it\nis designed for and looks very appealing to solve the mode mixing problem: consider a smooth\nversion (higher temperature, obtained by just dividing the energy by a temperature greater than\n1) of the distribution of interest; it therefore spreads probability mass more uniformly so one can\nmix between modes at that high temperature version of the model, and then gradually cool to the\ntarget distribution while continuing to make MCMC moves, to make sure we end up in one of the\n“islands” of high probability. Desjardins et al. (2010); Cho et al. (2010); Salakhutdinov (2010b,a)\nhave all considered various forms of tempering to address the failure of Gibbs chain mixing in RBMs.\nUnfortunately, convincing solutions (in the sense of making a practical impact on training eﬃciency)\nhave not yet been clearly demonstrated. It is not clear why this is so, but it may be due to the need to\nspend much time at some speciﬁc (critical) temperatures in order to succeed. More work is certainly\nwarranted in that direction.\nAn interesting observation (Bengio et al., 2013b) which could turn out to be helpful is that\nafter we train a deep model such as a DBN or a stack of regularized auto-encoders, we can observe\nthat mixing between modes is much easier at higher levels of the hierarchy (e.g. in the top-level\nRBM or top-level auto-encoder): mixing between modes is easier at deeper levels of representation.\n17 e.g. they can be charted with a few coordinates\n18 See examples of generated images with some of the current state-of-the-art in learned generative models\nof images (Courville et al., 2011; Luo et al., 2013)\nDeep Learning of Representations: Looking Forward\n15\nThis is achieved by running the MCMC in a high-level representation space and then projecting\nback in raw input space to obtain samples at that level. The hypothesis proposed (Bengio et al.,\n2013b) to explain this observation is that unsupervised representation learning procedures (such as\nfor the RBM and contractive or denoising auto-encoders) tend to discover a representation whose\ndistribution has more entropy (the distribution of vectors in higher layers is more uniform) and that\nbetter “disentangles” or separates out the underlying factors of variation (see next section for a\nlonger discussion of the concept of disentangling). For example, suppose that a perfect disentangling\nhad been achieved that extracted the factors out of images of objects, such as object category,\nposition, foreground color, etc. A single Gibbs step could thus switch a single top-level variable (like\nobject category) when that variable is resampled given the others, a very local move in that top-level\ndisentangled representation but a very far move (going to a very diﬀerent place) in pixel space. Note\nthat maximizing mutual information between inputs and their learned deterministic representation,\nwhich is what auto-encoders basically do (Vincent et al., 2008), is equivalent to maximizing the\nentropy of the learned representation,19 which supports this hypothesis. An interesting idea20 would\ntherefore be to use higher levels of a deep model to help the lower layers mix better, by using them\nin a way analogous to parallel tempering, i.e., to suggest conﬁgurations sampled from a diﬀerent\nmode.\nAnother interesting potential avenue for solving the problem of sampling from a complex and\nrough (non-smooth) distribution would be to take advantage of quantum annealing eﬀects (Rose\nand Macready, 2007) and analog computing hardware (such as produced by D-Wave). NP-hard\nproblems (such as sampling or optimizing exactly in an Ising model) still require exponential time\nbut experimental evidence has shown that for some problems, quantum annealing is far superior\nto standard digital computation (Brooke et al., 2001). Since quantum annealing is performed by\nessentially implementing a Boltzmann machine in analog hardware, it might be the case that drawing\nsamples from a Boltzmann machine is one problem where quantum annealing would be dramatically\nsuperior to classical digital computing.\nLearning a Computational Graph that Does What we Want If we stick to the idea of\nobtaining actual values of the latent variables (either through MAP, factorized variational inference\nor MCMC), then a promising path is based on learning approximate inference, i.e., optimizing a\nlearned approximate inference mechanism so that it performs a better inference faster. This idea is\nnot new and has been shown to work well in many settings. This idea was actually already present in\nthe wake-sleep algorithm (Hinton et al., 1995; Frey et al., 1996; Hinton et al., 2006) in the context of\nvariational inference for Sigmoidal Belief Networks and DBNs. Learned approximate inference is also\ncrucial in the predictive sparse coding (PSD) algorithm (Kavukcuoglu et al., 2008). This approach\nis pushed further with Gregor and LeCun (2010b) in which the parametric encoder has the same\nstructural form as a fast iterative sparse coding approximate inference algorithm. The important\nconsideration in both cases is not just that we have fast approximate inference, but that (a) it\nis learned, and (b) the model is learned jointly with the learned approximate inference procedure.\nSee also Salakhutdinov and Larochelle (2010) for learned fast approximate variational inference in\nDBMs, or Bagnell and Bradley (2009); Stoyanov et al. (2011) for learning fast approximate inference\n(with fewer steps than would otherwise be required by standard general purpose inference) based\non loopy belief propagation.\nThe traditional view of probabilistic graphical models is based on the clean separation between\nmodeling (deﬁning the model), optimization (tuning the parameters), inference (over the latent\nvariables) and sampling (over all the variables, and possibly over the parameters as well in the\nBayesian scenario). This modularization has clear advantages but may be suboptimal. By bringing\nlearning into inference and jointly learning the approximate inference and the “generative model”\nitself, one can hope to obtain “specialized” inference mechanisms that could be much more eﬃcient\nand accurate than generic purpose ones; this was the subject of a recent ICML workshop (Eisner,\n19 Salah Rifai, personal communication\n20 Guillaume Desjardins, personal communication\n16\nY. Bengio\n2012). The idea of learned approximate inference may help deal with the ﬁrst (purely computational)\nchallenge raised above regarding inference, i.e., it may help to speed up inference to some extent,\nbut it generally keeps the approximate inference parameters separate from the model parameters.\nBut what about the challenge from a huge number of modes? What if the number of modes is\ntoo large and/or these are too well-separated for MCMC to visit eﬃciently or for variational/MAP\ninference to approximate satisfactorily? If we stick to the objective of computing actual values of\nthe latent variables, the logical conclusion is that we should learn to approximate a posterior that\nis represented by a rich multi-modal distribution. To make things concrete, imagine that we learn\n(or identify) a function f(x) of the visible variable x that computes the parameters θ = f(x) of an\napproximate posterior distribution Qθ=f(x)(h) but where Qθ=f(x)(h) ≈P(h | x) can be highly multi-\nmodal, e.g., an RBM with visible variables h (coupled with additional latent variables used only to\nrepresent the richness of the posterior over h itself). Since the parameters of the RBM are obtained\nthrough a parametric computation taking x as input,21 this is really a conditional RBM (Taylor\net al., 2007; Taylor and Hinton, 2009). Whereas variational inference is usually limited to a non-\nparametric approximation of the posterior, Q(h) (one that is analytically and iteratively optimized\nfor each given x) one could consider a parametric approximate posterior that is learned (or derived\nanalytically) while allowing for a rich multi-modal representation (such as what an RBM can capture,\ni.e., up to an exponential number of modes).\nAvoiding inference and explicit marginalization over latent variables altogether. We now\npropose to consider an even more radical departure from traditional thinking regarding probabilistic\nmodels with latent variables. It is motivated by the observation that even with the last proposal,\nsomething like a conditional RBM to capture the posterior P(h | x), when one has to actually\nmake a decision or a prediction, it is necessary for optimal decision-making to marginalize over the\nlatent variables. For example, if we want to predict y given x, we want to compute something like\nP\nh P(y | h)P(h | x). If P(h | x) is complex and highly multi-modal (with a huge number of modes),\nthen even if we can represent the posterior, performing this sum exactly is out of the question, and\neven an MCMC approximation may be either very poor (we can only visit at most N modes with N\nMCMC steps, and that is very optimistic because of the mode mixing issue) or very slow (requiring\nan exponential number of terms being computed or a very very long MCMC chain). It seems that we\nhave not really addressed the original “fundamental challenge with highly multi-modal posteriors”\nraised above.\nTo address this challenge, we propose to avoid explicit inference altogether by avoiding to sample,\nenumerate, or represent actual values of the latent variables h. In fact, our proposal is to completely\nskip the latent variables themselves. Instead, if we ﬁrst consider the example of the previous para-\ngraph, one can just directly learn to predict P(y | x). In general, what we seek is that the only\napproximation error we are left with is due to to function approximation. This might be impor-\ntant because the compounding of approximate inference with function approximation could be very\nhurtful (Kulesza and Pereira, 2008).\nTo get there, one may wish to mentally go through an intermediate step. Imagine we had a good\napproximate posterior Qθ=f(x)(h) as proposed above, with parameters θ = f(x). Then we could\nimagine learning an approximate decision model that approximates and skips the intractable sum\nover h, instead directly going from θ = f(x) to a prediction of y, i.e., we would estimate P(y | x)\nby g(f(x)). Now since we are already learning f(x), why learn g(θ) separately? We could simply\ndirectly learn to estimate π(x) = g(f(x)) ≈P(y | x).\nNow that may look trivial, because this is already what we do in discriminant training of deep\nnetworks or recurrent networks, for example. And don’t we lose all the advantages of probabilistic\nmodels, such as, handling diﬀerent forms of uncertainty, missing inputs, and being able to answer\nany “question” of the form “predict any variables given any subset of the others”? Yes, if we stick\n21 for many models, such as deep Boltzmann machines, or bipartite discrete Markov random ﬁelds (Martens\nand Sutskever, 2010), f does not even need to be learned, it can be derived analytically from the form of\nP(h | x)\nDeep Learning of Representations: Looking Forward\n17\nto the traditional deep (or shallow) neural networks like those discussed in Section 2.1.22 But there\nare other options.\nWe propose to get the advantages of probabilistic models without the need for explicitly going\nthrough many conﬁgurations of the latent variables. The general principle of what we propose to\nachieve this is to construct a family of computational graphs which perform the family of tasks we are\ninterested in. A recent proposal (Goodfellow et al., 2013a) goes in this direction. Like previous work\non learned approximate inference (Stoyanov et al., 2011), one can view the approach as constructing\na computational graph associated to approximate inference (e.g. a ﬁxed number of iterations of mean-\nﬁeld updates) in a particular setting (here, ﬁlling missing input with a variational approximation\nover hidden and unclamped inputs). An interesting property is that depending on which input\nvariables are clamped and which are considered missing (either during training or at test time), we\nget a diﬀerent computational graph, while all these computational graphs share the same parameters.\nIn Goodfellow et al. (2013a), training the shared parameters of these computational graph is achieved\nthrough a variational criterion that is similar to a generalized pseudo-likelihood, i.e., approximately\nmaximizing log P(xv | xc) for randomly chosen partitions (v, c) of s.\nThis would be similar to dependency networks (Heckerman et al., 2000), but re-using the same\nparameters for every possible question-answer partition and training the system to answer for any\nsubset of variables rather than singletons like in pseudo-likelihood. For the same reason, it raises the\nquestion of whether the diﬀerent estimated conditionals are coherent with a global joint distribution.\nIn the case where the computational graph is obtained from the template of an inference mechanism\nfor a joint distribution (such as variational inference), then clearly, we keep the property that these\nconditionals are coherent with a global joint distribution. With the mean-ﬁeld variational inference,\nthe computational graph looks like a recurrent neural network converging to a ﬁxed point, and where\nwe stop the iterations after a ﬁxed number of steps or according to a convergence criterion. Such a\ntrained parametrized computational graph is used in the iterative variational approach introduced\nin Goodfellow et al. (2013a) for training and missing value inference in deep Boltzmann machines,\nwith an inpainting-like criterion in which arbitrary subsets of pixels are predicted given the others (a\ngeneralized pseudo-likelihood criterion). It has also been used in a recursion that follows the template\nof loopy belief propagation to ﬁll-in the missing inputs and produce outputs (Stoyanov et al., 2011).\nAlthough in these cases there is still a notion of latent variables (e.g. the latent variables of the\ndeep Boltzmann machine) that motivate the “template” used for the learned approximate inference,\nwhat we propose here is to stop thinking about them as actual latent factors, but rather just as\na way to parametrize this template for a question answering mechanism regarding missing inputs,\ni.e., the “generic conditional prediction mechanism” implemented by the recurrent computational\ngraph that is trained to predict any subset of variables given any other subset. Although Goodfellow\net al. (2013a) assume a factorial distribution across the predicted variables, we propose to investigate\nnon-factorial posterior distributions over the observed variables, i.e., in the spirit of the recent ﬂurry\nof work on structured output machine learning (Tsochantaridis et al., 2005). We can think of this\nparametrized computational graph as a family of functions, each corresponding to answering a\ndiﬀerent question (predict a speciﬁc set of variables given some others), but all sharing the same\nparameters. We already have examples of such families in machine learning, e.g., with recurrent\nneural networks or dynamic Bayes nets (where the functions in the family are indexed by the\nlength of the sequence). This is also analogous to what happens with dropouts, where we have an\nexponential number of neural networks corresponding to diﬀerent sub-graphs from input to output\n(indexed by which hidden units are turned on or oﬀ). For the same reason as in these examples, we\nobtain a form of generalization across subsets. Following the idea of learned approximate inference,\nthe parameters of the question-answering inference mechanism would be taking advantage of the\nspeciﬁc underlying structure in the data generating distribution. Instead of trying to do inference\non the anonymous latent variables, it would be trained to do good inference only over observed\n22 although, using something like these deep nets would be appealing because they are currently beating\nbenchmarks in speech recognition, language modeling and object recognition\n18\nY. Bengio\nvariables or over high-level features learned by a deep architecture, obtained deterministically from\nthe observed input.\nAn even more radically diﬀerent solution to the problem of avoiding explicit latent variables\nwas recently introduced in Bengio et al. (2013c) and Bengio and Thibodeau-Laufer (2013). These\nintroduce training criteria respectively for generalized forms of denoising auto-encoders and for gener-\native stochastic networks, with the property that maximum likelihood training of the reconstruction\nprobabilities yields consistent but implicit estimation of the data generating distribution. These\nGenerative Stochastic Networks (GSNs) can be viewed as inspired by the Gibbs sampling procedure\nin deep Boltzmann machines (or deep belief networks) in the sense that one can construct computa-\ntional graphs that perform similar computation, i.e., these are stochastic computational graphs (or\nequivalently, deterministic computational graphs with noise sources injected in the graph). These\nmodels are not explicitly trained to ﬁll-in missing inputs but simply to produce a Markov chain\nwhose asymptotic distribution estimates the data generating distribution. However, one can show\nthat this chain can be manipulated in order to obtain samples of the estimated conditional distribu-\ntion P(xv | xc), i.e., if one clamps some of the inputs, one can sample from a chain that stochastically\nﬁlls-in from the missing inputs.\nThe approximate inference is not anymore an approximation of something else, it is the deﬁni-\ntion of the model itself. This is actually good news because we thus eliminate the issue that the\napproximate inference may be poor. The only thing we need to worry about is whether the parame-\nterized computational graph is rich enough (or may overﬁt) to capture the unknown data generating\ndistribution, and whether it makes it easy or diﬃcult to optimize the parameters.\nThe idea that we should train with the approximate inference as part of the computational graph\nfor producing a decision (and a loss) was ﬁrst introduced by Stoyanov et al. (2011), and we simply\npush it further here, by proposing to allow the computational graph to depart in any way we care to\nexplore from the template provided by existing inference or sampling mechanisms, i.e., potentially\nlosing the connection and the reference to probabilistic latent variables. Once we free ourselves\nfrom the constraint of interpreting this parametrized question answering computational graph as\ncorresponding to approximate inference or approximate sampling involving latent variables, all kinds\nof architectures and parametrizations are possible, where current approximate inference mechanisms\ncan serve as inspiration and starting points. Interestingly, Bengio and Thibodeau-Laufer (2013)\nprovides a proper training criterion for training any such stochastic computational graph simply\nusing backprop over the computational graph, so as to maximize the probability of reconstructing\nthe observed data under a reconstruction probability distribution that depends on the inner nodes\nof the computational graph. The noise injected in the computational graph must be such that the\nlearner cannot get rid of the noise and obtain perfect reconstruction (a dirac at the correct observed\ninput), just like in denoising auto-encoders. It is quite possible that this new freedom could give rise\nto much better models.\nTo go farther than Bengio and Thibodeau-Laufer (2013); Goodfellow et al. (2013a); Stoyanov\net al. (2011) it would be good to go beyond the kind of factorized prediction common in variational\nand loopy belief propagation inference. We would like the reconstruction distribution to be able to\ncapture multi-modal non-factorial distributions. Although the result from Alain and Bengio (2013)\nsuggests that when the amount of injected noise is small, a unimodal distribution is suﬃcient, it is\nconvenient to accomodate large amounts of injected noise to make training more eﬃcient, as dis-\ncussed by Bengio et al. (2013c). One idea is to obtain such multi-modal reconstruction distributions\nis to represent the estimated joint distribution of the predicted variables (possibly given the clamped\nvariables) by a powerful model such as an RBM or a regularized auto-encoder, e.g., as has been done\nfor structured output predictions when there is complex probabilistic structure between the output\nvariables (Mnih et al., 2011; Li et al., 2013).\nAlthough conditional RBMs have been already explored, conditional distributions provided by\nregularized auto-encoders remain to be studied. Since a denoising auto-encoder can be shown to\nestimate the underlying data generating distribution, making its parameters dependent on some\nother variables yields an estimator of a conditional distribution, which can also be trained by simple\ngradient-based methods (and backprop to obtain the gradients).\nDeep Learning of Representations: Looking Forward\n19\nAll these ideas lead to the question: what is the interpretation of hidden layers, if not directly\nof the underlying generative latent factors? The answer may simply be that they provide a better\nrepresentation of these factors, a subject discussed in the next section. But what about the represen-\ntation of uncertainty about these factors? The author believes that humans and other animals carry\nin their head an internal representation that implicitly captures both the most likely interpretation\nof any of these factors (in case a hard decision about some of them has to be taken) and uncertainty\nabout their joint assignment. This is of course a speculation. Somehow, our brain would be operat-\ning on implicit representations of the joint distribution between these explanatory factors, generally\nwithout having to commit until a decision is required or somehow provoked by our attention mech-\nanisms (which seem related to our tendancy to verbalize a discrete interpretation). A good example\nis foreign language understanding for a person who does not master that foreign language. Until we\nconsciously think about it, we generally don’t commit to a particular meaning for ambiguous word\n(which would be required by MAP inference), or even to the segmentation of the speech in words,\nbut we can take a hard or a stochastic decision that depends on the interpretation of these words if\nwe have to, without having to go through this intermediate step of discrete interpretation, instead\ntreating the ambiguous information as soft cues that may inform our decision. In that example, a\nfactorized posterior is also inadequate because some word interpretations are more compatible with\neach other.\nTo summarize, what we propose here, unlike in previous work on approximate inference, is to\ndrop the pretense that the learned approximate inference mechanism actually approximates the\nlatent variables distribution, mode, or expected value. Instead, we only consider the construction\nof a computational graph (deterministic or stochastic) which produces answers to the questions\nwe care about, and we make sure that we can train a family of computational graphs (sharing\nparameters) whose elements can answer any of these questions. By removing the interpretation of\napproximately marginalizing over latent variables, we free ourselves from a strong constraint and the\npossible hurtful approxiations involved in approximate inference, especially when the true posterior\nwould have a huge number of signiﬁcant modes.\nThis discussion is of course orthogonal to the use of Bayesian averaging methods in order to\nproduce better-generalizing predictions, i.e., handling uncertainty due to a small number of training\nexamples. The proposed methods can be made Bayesian just like neural networks have their Bayesian\nvariants (Neal, 1994), by somehow maintaining an implicit or explicit distribution over parameters.\nA promising step in this direction was proposed by Welling and Teh (2011), making such Bayesian\ncomputation tractable by exploiting the randomness introduced with stochastic gradient descent to\nalso produce the Bayesian samples over the uncertain parameter values.\n6\nDisentangling\n6.1\nDisentangling: The Challenge\nWhat are “underlying factors” explaining the data? The answer is not obvious. One answer could\nbe that these are factors that can be separately controlled (one could set up way to change one but\nnot the others). This can actually be observed by looking at sequential real-world data, where only\na small proportion of the factors typically change from t to t + 1. Complex data arise from the rich\ninteraction of many sources. These factors interact in a complex web that can complicate AI-related\ntasks such as object classiﬁcation. If we could identity and separate out these factors (i.e., disentangle\nthem), we would have almost solved the learning problem. For example, an image is composed of\nthe interaction between one or more light sources, the object shapes and the material properties\nof the various surfaces present in the image. It is important to distinguish between the related but\ndistinct goals of learning invariant features and learning to disentangle explanatory factors. The\ncentral diﬀerence is the preservation of information. Invariant features, by deﬁnition, have reduced\nsensitivity in the directions of invariance. This is the goal of building features that are insensitive to\nvariation in the data that are uninformative to the task at hand. Unfortunately, it is often diﬃcult\nto determine a priori which set of features and variations will ultimately be relevant to the task at\n20\nY. Bengio\nhand. Further, as is often the case in the context of deep learning methods, the feature set being\ntrained may be destined to be used in multiple tasks that may have distinct subsets of relevant\nfeatures. Considerations such as these lead us to the conclusion that the most robust approach to\nfeature learning is to disentangle as many factors as possible, discarding as little information about\nthe data as is practical.\nDeep learning algorithms that can do a much better job of disentangling the underlying factors\nof variation would have tremendous impact. For example, suppose that the underlying factors can be\n“guessed” (predicted) from a simple (e.g. linear) transformation of the learned representation, ideally\na transformation that only depends on a few elements of the representation. That is what we mean\nby a representation that disentangles the underlying factors. It would clearly make learning a new\nsupervised task (which may be related to one or a few of them) much easier, because the supervised\nlearning could quickly learn those linear factors, zooming in on the parts of the representation that\nare relevant.\nOf all the challenges discussed in this paper, this is probably the most ambitious, and success\nin solving it the most likely to have far-reaching impact. In addition to the obvious observation\nthat disentangling the underlying factors is almost like pre-solving any possible task relevant to the\nobserved data, having disentangled representations would also solve other issues, such as the issue of\nmixing between modes. We believe that it would also considerably reduce the optimization problems\ninvolved when new information arrives and has to be reconciled with the world model implicit in the\ncurrent parameter setting. Indeed, it would allow only changing the parts of the model that involve\nthe factors that are relevant to the new observation, in the spirit of sparse updates and reduced\nill-conditioning discussed above.\n6.2\nDisentangling: Solution Paths\nDeeper Representations Disentangle Better. There are some encouraging signs that our cur-\nrent unsupervised representation-learning algorithms are reducing the “entanglement” of the un-\nderlying factors23 when we apply them to raw data (or to the output of a previous representation\nlearning procedure, like when we stack RBMs or regularized auto-encoders).\nFirst, there are experimental observations suggesting that sparse convolutional RBMs and sparse\ndenoising auto-encoders achieve in their hidden units a greater degree of disentangling than in their\ninputs (Goodfellow et al., 2009; Glorot et al., 2011b). What these authors found is that some hidden\nunits were particularly sensitive to a known factor of variation while being rather insensitive (i.e.,\ninvariant) to others. For example, in a sentiment analysis model that sees unlabeled paragraphs\nof customer comments from the Amazon web site, some hidden units specialized on the topic of\nthe paragraph (the type of product being evaluated, e.g., book, video, music) while other units\nspecialized on the sentiment (positive vs negative). The disentanglement was never perfect, so the\nauthors made quantitative measurements of sensitivity and invariance and compared these quantities\non the input and the output (learned representation) of the unsupervised learners.\nAnother encouraging observation (already mentioned in the section on mixing) is that deeper rep-\nresentations were empirically found to be more amenable to quickly mixing between modes (Bengio\net al., 2013b). Two (compatible) hypotheses were proposed to explain this observation: (1) RBMs\nand regularized auto-encoders deterministically transform24 their input distribution into one that\nis more uniform-looking, that better ﬁlls the space (thus creating easier paths between modes), and\n(2) these algorithms tend to discover representations that are more disentangled. The advantage of\na higher-level disentangled representation is that a small MCMC step (e.g. Gibbs) in that space\n(e.g. ﬂipping one high-level variable) can move in one step from one input-level mode to a distant\none, e.g., going from one shape / object to another one, adding or removing glasses on the face of\na person (which requires a very sharp coordination of pixels far from each other because glasses\noccupy a very thin image area), or replacing foreground and background colors (such as going into\na “reverse video” mode).\n23 as measured by how predictive some individual features are of known factors\n24 when considering the features learned, e.g., the P(hi = 1 | x), for RBMs\nDeep Learning of Representations: Looking Forward\n21\nAlthough these observations are encouraging, we do not yet have a clear understanding as to why\nsome representation algorithms tend to move towards more disentangled representations, and there\nare other experimental observations suggesting that this is far from suﬃcient. In particular, Gul-\ncehre and Bengio (2013) show an example of a task on which deep supervised nets (and every other\nblack-box machine learning algorithm tried) fail, on which a completely disentangled input represen-\ntation makes the task feasible (with a maxout network (Goodfellow et al., 2013b)). Unfortunately,\nunsupervised pre-training applied on the raw input images failed to produce enough disentangling\nto solve the task, even with the appropriate convolutional structure. What is interesting is that we\nnow have a simple artiﬁcial task on which we can evaluate new unsupervised representation learn-\ning methods for their disentangling ability. It may be that a variant of the current algorithms will\neventually succeed at this task, or it may be that altogether diﬀerent unsupervised representation\nlearning algorithms are needed.\nGeneric Priors for Disentangling Factors of Variation. A general strategy was outlined in\nBengio et al. (2013d) to enhance the discovery of representations which disentangle the underlying\nand unknown factors of variation: it relies on exploiting priors about these factors. We are most\ninterested in broad generic priors that can be useful for a large class of learning problems of interest\nin AI. We list these priors here:\n• Smoothness: assumes the function f to be learned is s.t. x ≈y generally implies f(x) ≈f(y).\nThis most basic prior is present in most machine learning, but is insuﬃcient to get around the curse\nof dimensionality.\n• Multiple explanatory factors: the data generating distribution is generated by diﬀerent un-\nderlying factors, and for the most part what one learns about one factor generalizes in many con-\nﬁgurations of the other factors. The objective is to recover or at least disentangle these underlying\nfactors of variation. This assumption is behind the idea of distributed representations. More\nspeciﬁc priors on the form of the model can be used to enhance disentangling, such as multiplica-\ntive interactions between the factors (Tenenbaum and Freeman, 2000; Desjardins et al., 2012) or\northogonality of the features derivative with respect to the input (Rifai et al., 2011b, 2012a; Sohn\net al., 2013). The parametrization and training procedure may also be used to disentangle discrete\nfactors (e.g., detecting a shape) from associated continuous-valued factors (e.g., pose parameters),\nas in transforming auto-encoders (Hinton et al., 2011), spike-and-slab RBMs with pooled slab vari-\nables (Courville et al., 2011) and other pooling-based models that learn a feature subspace (Kohonen,\n1996; Hyv¨arinen and Hoyer, 2000).\n• A hierarchical organization of explanatory factors: the concepts that are useful for describ-\ning the world around us can be deﬁned in terms of other concepts, in a hierarchy, with more abstract\nconcepts higher in the hierarchy, deﬁned in terms of less abstract ones. This assumption is exploited\nwith deep representations. Although stacking single-layer models has been rather successful, much\nremains to be done regarding the joint training of all the layers of a deep unsupervised model.\n• Semi-supervised learning: with inputs X and target Y to predict, given X, a subset of the\nfactors explaining X’s distribution explain much of Y , given X. Hence representations that are use-\nful for spelling out P(X) tend to be useful when learning P(Y | X), allowing sharing of statistical\nstrength between the unsupervised and supervised learning tasks. However, many of the factors that\nexplain X may dominate those that also explain Y , which can make it useful to incorporate obser-\nvations of Y in training the learned representations, i.e., by semi-supervised representation learning.\n• Shared factors across tasks: with many Y ’s of interest or many learning tasks in general, tasks\n(e.g., the corresponding P(Y | X, task)) are explained by factors that are shared with other tasks,\nallowing sharing of statistical strength across tasks, e.g. for multi-task and transfer learning or do-\nmain adaptation. This can be achieved by sharing embeddings or representation functions across\ntasks (Collobert and Weston, 2008; Bordes et al., 2013).\n• Manifolds: probability mass concentrates near regions that have a much smaller dimensional-\nity than the original space where the data lives. This is exploited with regularized auto-encoder\nalgorithms, but training criteria that would explicitly take into account that we are looking for a\n22\nY. Bengio\nconcentration of mass in an integral number directions remain to be developed.\n• Natural clustering: diﬀerent values of categorical variables such as object classes are associated\nwith separate manifolds. More precisely, the local variations on the manifold tend to preserve the\nvalue of a category, and a linear interpolation between examples of diﬀerent classes in general in-\nvolves going through a low density region, i.e., P(X | Y = i) for diﬀerent i tend to be well separated\nand not overlap much. For example, this is exploited in the Manifold Tangent Classiﬁer (Rifai et al.,\n2011b). This hypothesis is consistent with the idea that humans have named categories and classes\nbecause of such statistical structure (discovered by their brain and propagated by their culture),\nand machine learning tasks often involves predicting such categorical variables.\n• Temporal and spatial coherence: this prior introduced in Becker and Hinton (1992) is similar\nto the natural clustering assumption but concerns sequences of observations: consecutive (from a\nsequence) or spatially nearby observations tend to be easily predictable from each other. In the spe-\ncial case typically studied, e.g., slow feature analysis (Wiskott and Sejnowski, 2002b), one assumes\nthat consecutive values are close to each other, or that categorical concepts remain either present\nor absent for most of the transitions. More generally, diﬀerent underlying factors change at diﬀerent\ntemporal and spatial scales, and this could be exploited to sift diﬀerent factors into diﬀerent cate-\ngories based on their temporal scale.\n• Sparsity: for any given observation x, only a small fraction of the possible factors are relevant.\nIn terms of representation, this could be represented by features that are often zero (as initially\nproposed by Olshausen and Field (1996)), or more generally by the fact that most of the extracted\nfeatures are insensitive to small variations of x. This can be achieved with certain forms of priors\non latent variables (peaked at 0), or by using a non-linearity whose value is often ﬂat at 0 (i.e., 0\nand with a 0 derivative), or simply by penalizing the magnitude of the derivatives of the function\nmapping input to representation. A variant on that hypothesis is that for any given input, only a\nsmall part of the model is relevant and only a small subset of the parameters need to be updated.\n• Simplicity of Factor Dependencies: in good high-level representations, the factors are related\nto each other through simple, typically linear, dependencies. This can be seen in many laws of\nphysics, and is assumed when plugging a linear predictor on top of a learned representation.\n7\nConclusion\nDeep learning and more generally representation learning are recent areas of investigation in ma-\nchine learning and recent years of research have allowed to clearly identify several major challenges\nfor approaching the performance of these algorithms from that of humans. We have broken down\nthese challenges into four major areas: scaling computations, reducing the diﬃculties in optimizing\nparameters, designing (or avoiding) expensive inference and sampling, and helping to learn repre-\nsentations that better disentangle the unknown underlying factors of variation. There is room for\nexploring many paths towards addressing all of these issues, and we have presented here a few\nappealing directions of research towards these challenges.\nAcknowledgments\nThe author is extremely grateful for the feedback and discussions he enjoyed with collaborators\nIan Goodfellow, Guillaume Desjardins, Aaron Courville, Pascal Vincent, Roland Memisevic and\nNicolas Chapados, which greatly contributed to help form the ideas presented here and ﬁne-tune\nthis manuscript. He is also grateful for the funding support from NSERC, CIFAR, the Canada\nResearch Chairs, and Compute Canada.\nBibliography\nAlain, G. and Bengio, Y. (2013). What regularized auto-encoders learn from the data generating distribution.\nIn International Conference on Learning Representations (ICLR’2013).\nBach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011). Structured sparsity through convex optimiza-\ntion. Technical report, arXiv.1109.2397.\nBagnell, J. A. and Bradley, D. M. (2009). Diﬀerentiable sparse coding. In NIPS’2009, pages 113–120.\nBarber, D. (2011). Bayesian Reasoning and Machine Learning. Cambridge University Press.\nBecker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces in random-dot\nstereograms. Nature, 355, 161–163.\nBengio, Y. (2008). Neural net language models. Scholarpedia, 3(1).\nBengio, Y. (2009). Learning deep architectures for AI . Now Publishers.\nBengio, Y. (2011). Deep learning of representations for unsupervised and transfer learning. In JMLR W&CP:\nProc. Unsupervised and Transfer Learning.\nBengio, Y. (2013a). Estimating or propagating gradients through stochastic neurons. Technical Report\narXiv:1305.2982, Universite de Montreal.\nBengio, Y. (2013b). Evolving culture vs local minima. In Growing Adaptive Machines: Integrating Develop-\nment and Learning in Artiﬁcial Neural Networks, number also as ArXiv 1203.2990v1, pages T. Kowaliw,\nN. Bredeche & R. Doursat, eds. Springer-Verlag.\nBengio, Y. (2013c). Practical recommendations for gradient-based training of deep architectures. In K.-R.\nM¨uller, G. Montavon, and G. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer.\nBengio, Y. and Thibodeau-Laufer, E. (2013). Deep generative stochastic networks trainable by backprop.\nTechnical Report arXiv:1306.1091, Universite de Montreal.\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is\ndiﬃcult. IEEE Transactions on Neural Networks, 5(2), 157–166.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language model. JMLR,\n3, 1137–1155.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise training of deep networks.\nIn NIPS’2006.\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In ICML’09.\nBengio, Y., Delalleau, O., and Simard, C. (2010). Decision trees do not generalize to new variations. Com-\nputational Intelligence, 26(4), 449–467.\nBengio, Y., Alain, G., and Rifai, S. (2012). Implicit density estimation by local moment matching to sample\nfrom auto-encoders. Technical report, arXiv:1207.0057.\nBengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2013a). Advances in optimizing recurrent net-\nworks. In ICASSP’2013.\nBengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013b). Better mixing via deep representations. In\nICML’2013.\nBengio, Y., Li, Y., Alain, G., and Vincent, P. (2013c). Generalized denoising auto-encoders as generative\nmodels. Technical Report arXiv:1305.6663, Universite de Montreal.\nBengio, Y., Courville, A., and Vincent, P. (2013d). Unsupervised feature learning and deep learning: A\nreview and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI).\nBergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex cell-like networks.\nIn NIPS’2009.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley,\nD., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the\nPython for Scientiﬁc Computing Conference (SciPy).\nBergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delalleau, O., Desjardins, G., Warde-Farley,\nD., Goodfellow, I., Bergeron, A., and Bengio, Y. (2011). Theano: Deep learning on gpus with python. In\nBig Learn workshop, NIPS’11.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nBordes, A., Glorot, X., Weston, J., and Bengio, Y. (2013). A semantic matching energy function for learning\nwith multi-relational data. Machine Learning: Special Issue on Learning Semantics.\nBrooke, J. J., Bitko, D., Rosenbaum, T. F., and Aeppli, G. (2001). Quantum annealing of a disordered\nmagnet. Technical Report cond-mat/0105238.\n24\nY. Bengio\nCayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923, UCSD.\nCho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is eﬃcient for learning restricted Boltzmann\nmachines. In IJCNN’2010.\nCiresan, D., Meier, U., and Schmidhuber, J. (2012). Multi-column deep neural networks for image classiﬁ-\ncation. Technical report, arXiv:1202.2745.\nCoates, A. and Ng, A. Y. (2011). The importance of encoding versus training with sparse coding and vector\nquantization. In ICML’2011.\nCoates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in unsupervised feature\nlearning. In AISTATS’2011.\nCoates, A., Karpathy, A., and Ng, A. (2012). Emergence of object-selective features in unsupervised feature\nlearning. In NIPS’2012.\nCollobert, R. and Weston, J. (2008). A uniﬁed architecture for natural language processing: Deep neural\nnetworks with multitask learning. In ICML’2008.\nCollobert, R., Bengio, Y., and Bengio., S. (2003). Scaling large learning problems with hard parallel mixtures.\nInternational Journal of Pattern Recognition and Artiﬁcial Intelligence, 17(3), 349–365.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011). Natural language\nprocessing (almost) from scratch. Journal of Machine Learning Research, 12, 2493–2537.\nCorrado, G. (2012). Deep networks for predicting ad click through rates. In ICML’2012 Online Advertising\nWorkshop.\nCourville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by spike-and-slab RBMs.\nIn ICML’2011.\nDauphin, Y. and Bengio, Y. (2013). Big neural networks waste capacity. Technical Report arXiv:1301.3583,\nUniversite de Montreal.\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M., Senior, A., Tucker,\nP., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep networks. In NIPS’2012.\nDeng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., Seltzer, M., Zweig, G., He, X., Williams, J., Gong,\nY., and Acero, A. (2013). Recent advances in deep learning for speech research at Microsoft. In ICASSP\n2013.\nDesjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tempered Markov chain\nMonte Carlo for training of restricted Boltzmann machine. In AISTATS, volume 9, pages 145–152.\nDesjardins, G., Courville, A., and Bengio, Y. (2012).\nDisentangling factors of variation via generative\nentangling.\nEisner, J. (2012).\nLearning approximate inference policies for fast prediction.\nKeynote talk at ICML\nWorkshop on Inferning: Interactions Between Search and Learning.\nFrey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good density estima-\ntors? In NIPS’95, pages 661–670. MIT Press, Cambridge, MA.\nGlorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward neural networks.\nIn AISTATS’2010.\nGlorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectiﬁer neural networks. In AISTATS.\nGlorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation for large-scale sentiment classiﬁcation:\nA deep learning approach. In ICML’2011.\nGoodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep networks. In NIPS’09,\npages 646–654.\nGoodfellow, I., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding for unsupervised feature\ndiscovery. In NIPS Workshop on Challenges in Learning Hierarchical Models.\nGoodfellow, I., Courville, A., and Bengio, Y. (2012). Large-scale feature learning with spike-and-slab sparse\ncoding. In ICML’2012.\nGoodfellow, I. J., Courville, A., and Bengio, Y. (2013a). Joint training of deep Boltzmann machines for\nclassiﬁcation. In International Conference on Learning Representations: Workshops Track.\nGoodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013b). Maxout networks. In\nICML’2013.\nGregor, K. and LeCun, Y. (2010a).\nLearning fast approximations of sparse coding.\nIn L. Bottou and\nM. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning\n(ICML-10). ACM.\nGregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In ICML’2010.\nGrosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007). Shift-invariant sparse coding for audio classiﬁcation.\nIn UAI’2007.\nDeep Learning of Representations: Looking Forward\n25\nGulcehre, C. and Bengio, Y. (2013). Knowledge matters: Importance of prior information for optimization.\nTechnical Report arXiv:1301.4083, Universite de Montreal.\nGutmann, M. and Hyvarinen, A. (2010).\nNoise-contrastive estimation: A new estimation principle for\nunnormalized statistical models. In AISTATS’2010.\nHeckerman, D., Chickering, D. M., Meek, C., Rounthwaite, R., and Kadie, C. (2000). Dependency networks\nfor inference, collaborative ﬁltering, and data visualization. Journal of Machine Learning Research, 1,\n49–75.\nHinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-encoders. In ICANN’2011.\nHinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath,\nT., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition. IEEE\nSignal Processing Magazine, 29(6), 82–97.\nHinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural networks.\nScience, 313(5786), 504–507.\nHinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsupervised\nneural networks. Science, 268, 1558–1161.\nHinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural\nComputation, 18, 1527–1554.\nHinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012b).\nImproving\nneural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.\nHochreiter, S. (1991).\nUntersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f¨ur\nInformatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.\nHyv¨arinen, A. (2005). Estimation of non-normalized statistical models using score matching. J. Machine\nLearning Res., 6.\nHyv¨arinen, A. and Hoyer, P. (2000). Emergence of phase and shift invariant features by decomposition of\nnatural images into independent feature subspaces. Neural Computation, 12(7), 1705–1720.\nIba, Y. (2001). Extended ensemble monte carlo. International Journal of Modern Physics, C12, 623–656.\nJaeger, H. (2007). Echo state network. Scholarpedia, 2(9), 2330.\nJarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture\nfor object recognition? In ICCV’09.\nJenatton, R., Audibert, J.-Y., and Bach, F. (2009). Structured variable selection with sparsity-inducing\nnorms. Technical report, arXiv:0904.3523.\nKavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008).\nFast inference in sparse coding algorithms with\napplications to object recognition. CBLL-TR-2008-12-01, NYU.\nKindermann, R. (1980). Markov Random Fields and Their Applications (Contemporary Mathematics ; V.\n1). American Mathematical Society.\nKirkpatrick, S., Jr., C. D. G., , and Vecchi, M. P. (1983). Optimization by simulated annealing. Science,\n220, 671–680.\nKohonen, T. (1996). Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map.\nBiological Cybernetics, 75, 281–291. 10.1007/s004220050295.\nKrizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolutional neural\nnetworks. In NIPS’2012.\nKulesza, A. and Pereira, F. (2008). Structured learning with approximate inference. In NIPS’2007.\nLarochelle, H. and Bengio, Y. (2008). Classiﬁcation using discriminative restricted Boltzmann machines. In\nICML’2008.\nLe, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng, A. (2012). Building\nhigh-level features using large scale unsupervised learning. In ICML’2012.\nLe Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural gradient algorithm. In\nNIPS’07.\nLeCun, Y., Bottou, L., Orr, G. B., and M¨uller, K. (1998a). Eﬃcient backprop. In Neural Networks, Tricks\nof the Trade.\nLeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998b). Gradient based learning applied to document\nrecognition. Proc. IEEE.\nLeCun, Y., Chopra, S., Hadsell, R., Ranzato, M.-A., and Huang, F.-J. (2006). A tutorial on energy-based\nlearning. In G. Bakir, T. Hofman, B. Scholkopf, A. Smola, and B. Taskar, editors, Predicting Structured\nData, pages 191–246. MIT Press.\nLee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area V2. In NIPS’07.\n26\nY. Bengio\nLi, Y., Tarlow, D., and Zemel, R. (2013). Exploring compositional high order pattern potentials for structured\noutput learning. In CVPR’2013.\nLuo, H., Carrier, P. L., Courville, A., and Bengio, Y. (2013). Texture modeling with convolutional spike-\nand-slab RBMs and deep extensions. In AISTATS’2013.\nMairal, J., Bach, F., Ponce, J., and Sapiro, G. (2009). Online dictionary learning for sparse coding. In\nICML’2009.\nMartens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and M. Littman, editors,\nProceedings of the Twenty-seventh International Conference on Machine Learning (ICML-10), pages 735–\n742. ACM.\nMartens, J. and Sutskever, I. (2010). Parallelizable sampling of Markov random ﬁelds. In AISTATS’2010.\nMesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins,\nG., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011).\nUnsupervised and transfer\nlearning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning,\nvolume 7.\nMikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno University\nof Technology.\nMnih, V., Larochelle, H., and Hinton, G. (2011). Conditional restricted Boltzmann machines for structure\noutput prediction. In Proc. Conf. on Uncertainty in Artiﬁcial Intelligence (UAI).\nMontavon, G. and Muller, K.-R. (2012). Deep Boltzmann machines and the centering trick. In G. Montavon,\nG. Orr, and K.-R. M¨uller, editors, Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes\nin Computer Science, pages 621–637.\nMurphy, K. P. (2012). Machine Learning: a Probabilistic Perspective. MIT Press, Cambridge, MA, USA.\nNair, V. and Hinton, G. E. (2010).\nRectiﬁed linear units improve restricted Boltzmann machines.\nIn\nICML’10.\nNarayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis. In NIPS’2010.\nNeal, R. M. (1994). Bayesian Learning for Neural Networks. Ph.D. thesis, Dept. of Computer Science,\nUniversity of Toronto.\nOlshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties by learning a\nsparse code for natural images. Nature, 381, 607–609.\nPascanu, R. and Bengio, Y. (2012). On the diﬃculty of training recurrent neural networks. Technical Report\narXiv:1211.5063, Universite de Montreal.\nPascanu, R. and Bengio, Y. (2013).\nRevisiting natural gradient for deep networks.\nTechnical report,\narXiv:1301.3584.\nRaiko, T., Valpola, H., and LeCun, Y. (2012).\nDeep learning made easier by linear transformations in\nperceptrons. In AISTATS’2012.\nRaina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007). Self-taught learning: transfer learning from\nunlabeled data. In ICML’2007.\nRaina, R., Madhavan, A., and Ng, A. Y. (2009). Large-scale deep unsupervised learning using graphics\nprocessors. In L. Bottou and M. Littman, editors, ICML 2009, pages 873–880, New York, NY, USA.\nACM.\nRanzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007). Eﬃcient learning of sparse representations\nwith an energy-based model. In NIPS’2006.\nRanzato, M., Boureau, Y.-L., and LeCun, Y. (2008). Sparse feature learning for deep belief networks. In\nNIPS’07, pages 1185–1192, Cambridge, MA. MIT Press.\nRecht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic\ngradient descent. In NIPS’2011.\nRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive auto-encoders: Explicit\ninvariance during feature extraction. In ICML’2011.\nRifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011b). The manifold tangent classiﬁer. In\nNIPS’2011.\nRifai, S., Bengio, Y., Courville, A., Vincent, P., and Mirza, M. (2012a). Disentangling factors of variation\nfor facial expression recognition. In Proceedings of the European Conference on Computer Vision (ECCV\n6), pages 808–822.\nRifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012b). A generative process for sampling contractive\nauto-encoders. In ICML’2012.\nRose, G. and Macready, W. (2007).\nAn introduction to quantum annelaing.\nTechnical report, D-Wave\nSystems.\nDeep Learning of Representations: Looking Forward\n27\nRumelhart, D., Hinton, G., and Williams, R. (1986). Learning representations by back-propagating errors.\nNature, 323, 533–536.\nSalakhutdinov, R. (2010a). Learning deep Boltzmann machines using adaptive MCMC. In ICML’2010.\nSalakhutdinov, R. (2010b). Learning in Markov random ﬁelds using tempered transitions. In NIPS’2010.\nSalakhutdinov, R. and Hinton, G. (2009). Deep Boltzmann machines. In AISTATS’2009, pages 448–455.\nSalakhutdinov, R. and Larochelle, H. (2010).\nEﬃcient learning of deep Boltzmann machines.\nIn AIS-\nTATS’2010.\nSalakhutdinov, R., Mnih, A., and Hinton, G. (2007).\nRestricted Boltzmann machines for collaborative\nﬁltering. In ICML’2007, pages 791–798.\nSaul, L. K. and Jordan, M. I. (1996). Exploiting tractable substructures in intractable networks. In NIPS’95.\nMIT Press, Cambridge, MA.\nSchaul, T., Zhang, S., and LeCun, Y. (2012). No More Pesky Learning Rates. Technical report, New York\nUniversity, arxiv 1206.1106.\nSchraudolph, N. N. (1998). Centering neural network gradient factors. In G. B. Orr and K.-R. Muller,\neditors, Neural Networks: Tricks of he Trade, pages 548–548. Springer.\nSeide, F., Li, G., and Yu, D. (2011a). Conversational speech transcription using context-dependent deep\nneural networks. In Interspeech 2011, pages 437–440.\nSeide, F., Li, G., and Yu, D. (2011b). Feature engineering in context-dependent deep neural networks for\nconversational speech transcription. In ASRU’2011.\nSohn, K., Zhou, G., and Lee, H. (2013).\nLearning and selecting features jointly with point-wise gated\nBoltzmann machines. In ICML’2013.\nStoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters\ngiven approximate inference, decoding, and model structure. In AISTATS’2011.\nSutskever, I. (2012). Training Recurrent Neural Networks. Ph.D. thesis, CS Dept., U. Toronto.\nSwersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On autoencoders and score\nmatching for energy based models. In ICML’2011. ACM.\nTaylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines for modeling motion\nstyle. In L. Bottou and M. Littman, editors, ICML 2009, pages 1025–1032. ACM.\nTaylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent variables. In\nNIPS’06, pages 1345–1352. MIT Press, Cambridge, MA.\nTenenbaum, J. B. and Freeman, W. T. (2000). Separating style and content with bilinear models. Neural\nComputation, 12(6), 1247–1283.\nTsianos, K., Lawlor, S., and Rabbat, M. (2012). Communication/computation tradeoﬀs in consensus-based\ndistributed optimization. In NIPS’2012.\nTsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. (2005). Large margin methods for structured\nand interdependent output variables. J. Mach. Learn. Res., 6, 1453–1484.\nT¨oscher, A., Jahrer, M., and Bell, R. M. (2009). The bigchaos solution to the netﬂix grand prize.\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation,\n23(7), 1661–1674.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008).\nExtracting and composing robust\nfeatures with denoising autoencoders. In ICML 2008.\nWelling, M. and Teh, Y.-W. (2011).\nBayesian learning via stochastic gradient Langevin dynamics.\nIn\nICML’2011, pages 681–688.\nWeston, J., Ratle, F., and Collobert, R. (2008). Deep learning via semi-supervised embedding. In ICML\n2008.\nWiskott, L. and Sejnowski, T. (2002a). Slow feature analysis: Unsupervised learning of invariances. Neural\nComputation, 14(4), 715–770.\nWiskott, L. and Sejnowski, T. J. (2002b). Slow feature analysis: Unsupervised learning of invariances. Neural\nComputation, 14(4), 715–770.\nYu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured conditional random ﬁelds.\nIEEE Journal of Selected Topics in Signal Processing.\nYu, K., Lin, Y., and Laﬀerty, J. (2011). Learning image representations from the pixel level via hierarchical\nsparse coding. In CVPR’2011.\nZeiler, M. D. and Fergus, R. (2013).\nStochastic pooling for regularization of deep convolutional neural\nnetworks. In International Conference on Learning Representations.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2013-05-02",
  "updated": "2013-06-07"
}