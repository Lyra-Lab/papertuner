{
  "id": "http://arxiv.org/abs/1305.1809v2",
  "title": "Cover Tree Bayesian Reinforcement Learning",
  "authors": [
    "Nikolaos Tziortziotis",
    "Christos Dimitrakakis",
    "Konstantinos Blekas"
  ],
  "abstract": "This paper proposes an online tree-based Bayesian approach for reinforcement\nlearning. For inference, we employ a generalised context tree model. This\ndefines a distribution on multivariate Gaussian piecewise-linear models, which\ncan be updated in closed form. The tree structure itself is constructed using\nthe cover tree method, which remains efficient in high dimensional spaces. We\ncombine the model with Thompson sampling and approximate dynamic programming to\nobtain effective exploration policies in unknown environments. The flexibility\nand computational simplicity of the model render it suitable for many\nreinforcement learning problems in continuous state spaces. We demonstrate this\nin an experimental comparison with least squares policy iteration.",
  "text": "Journal of Machine Learning Research 1 (2014)\nSubmitted ; Published\nCover tree Bayesian reinforcement learning\nNikolaos Tziortziotis\nntziorzi@gmail.com\nDepartment of Computer Science and Engineering\nUniversity of Ioannina\nGR-45110, Greece\nChristos Dimitrakakis\nchristos.dimitrakakis@gmail.com\nDepartment of Computer Science and Engineering\nChalmers university of technology\nSE-41296, Sweden\nKonstantinos Blekas\nkblekas@cs.uoi.gr\nDepartment of Computer Science and Engineering\nUniversity of Ioannina\nGR-45110, Greece\nEditor: -\nAbstract\nThis paper proposes an online tree-based Bayesian approach for reinforcement learning.\nFor inference, we employ a generalised context tree model.\nThis deﬁnes a distribution\non multivariate Gaussian piecewise-linear models, which can be updated in closed form.\nThe tree structure itself is constructed using the cover tree method, which remains eﬃ-\ncient in high dimensional spaces. We combine the model with Thompson sampling and\napproximate dynamic programming to obtain eﬀective exploration policies in unknown en-\nvironments. The ﬂexibility and computational simplicity of the model render it suitable\nfor many reinforcement learning problems in continuous state spaces. We demonstrate this\nin an experimental comparison with a Gaussian process model, a linear model and simple\nleast squares policy iteration.\nKeywords:\nBayesian inference, non-parametric statistics, reinforcement learning.\n1. Introduction\nIn reinforcement learning, an agent must learn how to act in an unknown environment\nfrom limited feedback and delayed reinforcement. Eﬃcient learning and planning requires\nmodels of the environment that are not only general, but can also be updated online with\nlow computational cost. In addition, probabilistic models allow the use of a number of near-\noptimal algorithms for decision making under uncertainty. While it is easy to construct such\nmodels for small, discrete environments, models for the continuous case have so far been\nmainly limited to parametric models, which may not have the capacity to represent the\nenvironment (such as generalised linear models) and to non-parametric models, which do\nnot scale very well (such as Gaussian processes).\nIn this paper, we propose a non-parametric family of tree models, with a data-dependent\nstructure constructed through the cover tree algorithm, introduced by Beygelzimer et al.\n(2006). Cover trees are data structures that cover a metric space with a sequence of data-\n©2014 N. Tziortziotis, C. Dimitrakakis, K. Blekas.\narXiv:1305.1809v2  [stat.ML]  2 May 2014\nTziortztiotis et al.\ndependent partitions. They were initially proposed for the problem of k-nearest neighbour\nsearch, but they are in general a good method to generate ﬁne partitions of a state space,\ndue to their low complexity, and can be applied to any state space, with a suitable choice\nof metric. In addition, it is possible to create a statistical model using the cover tree as a\nbasis. Due to the tree structure, online inference has low (logarithmic) complexity.\nIn this paper, we speciﬁcally investigate the case of a Euclidean state space. For this,\nwe propose a model generalising the context tree weighting algorithm proposed by Willems\net al. (1995), combined with Bayesian multivariate linear models. The overall prior can be\ninterpreted as a distribution on piecewise-linear models. We then compare this model with\na Gaussian process model, a single linear model, and the model-free method least-squares\npolicy iteration in two well-known benchmark problems in combination with approximate\ndynamic programming and show that it consistently outperforms other approaches.\nThe remainder of the paper is organised as follows. Section 1.1 introduces the setting,\nSection 1.2 discusses related work and Section 1.3 explains our contribution. The model\nand algorithm are described in Section 2. Finally, comparative experiments are presented\nin Section 3 and we conclude with a discussion of the advantages of cover-tree Bayesian\nreinforcement learning and directions of future work in Section 4.\n1.1 Setting\nWe assume that the agent acts within a fully observable discrete-time Markov decision\nprocess (MDP), with a metric state space S, for example S ⊂Rm. At time t, the agent\nobserves the current environment state st ∈S, takes an action at from a discrete set A,\nand receives a reward rt ∈R.\nThe probability over next states is given in terms of a\ntransition kernel Pµ(S | s, a) ≜Pµ(st+1 ∈S | st = s, at = a).\nThe agent selects its\nactions using a policy π ∈Π, which in general deﬁnes a conditional distribution Pπ(at |\ns1, . . . , st, a1, . . . , at−1, r1, . . . , rt−1) over the actions, given the history of states and actions.\nThis reﬂects the learning process that the agent undergoes, when the MDP µ is unknown.\nThe agent’s utility is U ≜P∞\nt=0 γtrt, the discounted sum of future rewards, with γ ∈\n(0, 1) a discount factor such that rewards further into the future are less important than\nimmediate rewards. The goal of the agent is to maximise its expected utility:\nmax\nπ∈Π Eπ\nµ U = max\nπ∈Π Eπ\nµ\n∞\nX\nt=0\nγtrt,\n(1.1)\nwhere the value of the expectation depends on the agent’s policy π and the environment µ.\nIf the environment is known, well-known dynamic programming algorithms can be used to\nﬁnd the optimal policy in the discrete-state case (Puterman, 2005), while many approximate\nalgorithms exist for continuous environments (Bertsekas and Tsitsiklis, 1996). In this case,\noptimal policies are memoryless and we let Π1 denote the set of memoryless policies. Then\nMDP and policy deﬁne a Markov chain with kernel P π\nµ (S | s, a) = P\na∈A Pµ(S | s, a)π(a | s).\nHowever, since the environment µ is unknown, the above maximisation is ill-posed. In\nthe Bayesian framework for reinforcement learning, this problem is alleviated by perform-\ning the maximisation conditioned on the agent’s belief about the true environment µ. This\nconverts the problem of reinforcement learning into a concrete, optimisation problem. How-\n2\nCover Tree Bayesian RL\never, this is generally extremely complex, as we must optimise over all history-dependent\npolicies.\nMore speciﬁcally, the main assumption in Bayesian reinforcement learning is that the\nenvironment µ lies in a given set of environments M. In addition, the agent must select\na subjective prior distribution p(µ) which encodes its belief about which environments are\nmost likely. The Bayes-optimal expected utility for p is:\nU∗\np ≜max\nπ∈ΠD Eπ\np U = max\nπ∈ΠD\nZ\nM\n\u0000Eπ\nµ U\n\u0001\ndp(µ).\n(1.2)\nUnlike the known µ case, the optimal policy may not be memoryless, as our belief changes\nover time. This makes the optimisation over the policies signiﬁcantly harder (Duﬀ, 2002), as\nwe have to consider the set of all history-dependent deterministic policies, which we denote\nby ΠD ⊂Π. In this paper, we employ the simple, but eﬀective, heuristic of Thompson\nsampling (Thompson, 1933; Wyatt, 1998; Dearden et al., 1998; Strens, 2000) for ﬁnding\npolicies. This strategy is known by various other names, such as probability matching,\nstochastic dominance, sampling-greedy and posterior sampling. Very recently Osband et al.\n(2013) showed that it suﬀers small Bayes-regret relative to the Bayes-optimal policy for\nﬁnite, discrete MDPs.\nThe second problem in Bayesian reinforcement learning is the choice of the prior distri-\nbution. This can be of critical importance for large or complex problems, for two reasons.\nFirstly, a well-chosen prior can lead to more eﬃcient learning, especially in the ﬁnite-sample\nregime. Secondly, as reinforcement learning involves potentially unbounded interactions,\nthe computational and space complexity of calculating posterior distributions, estimating\nmarginals and performing sampling become extremely important. The choice of priors is\nthe main focus of this paper. In particular, we introduce a prior over piecewise-linear mul-\ntivariate Gaussian models. This is based on the construction of a context tree model, using\na cover tree structure, which deﬁnes a conditional distribution on local linear Bayesian\nmultivariate models. Since inference for the model can be done in closed form, the result-\ning algorithm is very eﬃcient, in comparison with other non-parametric models such as\nGaussian processes. The following section discusses how previous work is related to our\nmodel.\n1.2 Related work\nOne component in our model is the context tree. Context trees were introduced by Willems\net al. (1995) for sequential prediction (see Begleiter et al., 2004, for an overview). In this\nmodel, a distribution of variable order Markov models for binary sequences is constructed,\nwhere the tree distribution is deﬁned through context-dependent weights (for probability of\na node being part of the tree) and Beta distributions (for predicting the next observation).\nA recent extension to switching time priors (van Erven et al., 2008) has been proposed by\nVeness et al. (2012). More related to this paper is an algorithm proposed by Kozat et al.\n(2007) for prediction. This asymptotically converges to the best univariate piecewise linear\nmodel in a class of trees with ﬁxed structure.\nMany reinforcement learning approaches based on such trees have been proposed, but\nhave mainly focused on the discrete partially observable case (Daswani et al., 2012; Veness\n3\nTziortztiotis et al.\net al., 2011; Bellemare et al., 2013; Farias et al., 2010).1\nHowever, tree structures can\ngenerally be used to perform Bayesian inference in a number of other domains (Paddock\net al., 2003; Meila and Jordan, 2001; Wong and Ma, 2010).\nThe core of our model is a generalised context tree structure that deﬁnes a distribution on\nmultivariate piecewise-linear-Gaussian models. Consequently, a necessary component in our\nmodel is a multivariate linear model at each node of the tree. Such models were previously\nused for Bayesian reinforcement learning in (Tziortziotis et al., 2013) and were shown to\nperform well relatively to least-square policy iteration (LSPI) (Lagoudakis and Parr, 2003).\nOther approaches using linear models include (Strehl and Littman, 2008), which proves\nmistake bounds on reinforcement learning algorithms using online linear regression, and\n(Abbeel and Ng, 2005) who use separate linear models for each dimension. Another related\napproach in terms of structure is (Brunskill et al., 2009), which partitions the space into\ntypes and estimates a simple additive model for each type.\nLinear-Gaussian models are naturally generalised by Gaussian processes (GP). Some\nexamples of GP in reinforcement learning include (Rasmussen and Kuss, 2004; Deisenroth\net al., 2009; Deisenroth and Rasmussen, 2011), which focused on a model-predictive ap-\nproach, while the work of Engel et al. (2005) employed GPs for expected utility estimation.\nGPs are computationally demanding, in contrast to our tree-structured prior.\nAnother\nproblem with the cited GP-RL approaches is that they employ the marginal distribution\nin the dynamic programming step. This heuristic ignores the uncertainty about the model\n(which is implicitly taken into account in equations 1.2, 2.11). A notable exception to this\nis the policy gradient approach employed by Ghavamzadeh and Engel (2006) which uses full\nBayesian quadrature. Finally, output dimensions are treated independently, which may not\nmake good use of the data. Methods for eﬃcient dependent GPs such as the one introduced\nby Alvarez et al. (2011) have not yet been applied to reinforcement learning.\nFor decision making, this paper uses the simple idea of Thompson sampling (Thompson,\n1933; Wyatt, 1998; Dearden et al., 1998; Strens, 2000), which has been shown to be near-\noptimal in certain settings (Kaufmann et al., 2012; Agrawal and Goyal, 2012; Osband et al.,\n2013). This avoids the computational complexity of building augmented MDP models (Auer\net al., 2008; Asmuth et al., 2009; Castro and Precup, 2010; Araya et al., 2012), Monte-\nCarlo tree search (Veness et al., 2011), sparse sampling (Wang et al., 2005), stochastic\nbranch and bound (Dimitrakakis, 2010b) or creating lower bounds on the Bayes-optimal\nvalue function (Poupart et al., 2006; Dimitrakakis, 2011). Thus the approach is reasonable\nas long as sampling from the model is eﬃcient.\n1.3 Our contribution\nOur approach is based upon three ideas. The ﬁrst idea is to employ a cover tree (Beygelzimer\net al., 2006) to create a set of partitions of the state space. This avoids having to prespecify\na structure for the tree. The second technical novelty is the introduction of an eﬃcient non-\nparametric Bayesian conditional density estimator on the cover tree structure. This is a\ngeneralised context tree, endowed with a multivariate linear Bayesian model at each node.\nWe use this to estimate the dynamics of the underlying environment. The multivariate\n1. We note that another important work in tree-based reinforcement learning, though not directly related\nto ours, is that of Ernst et al. (2005), which uses trees for expected utility rather than model estimation.\n4\nCover Tree Bayesian RL\nmodels allow for a sample-eﬃcient estimation by capturing dependencies. Finally, we take\na sample from the posterior to obtain a piecewise linear Gaussian model of the dynamics.\nThis can be used to generate policies. In particular, from this, we obtain trajectories of\nsimulated experience, to perform approximate dynamic programming (ADP) in order to\nselect a policy. Although other methods could be used to calculate optimal actions, we\nleave them for future work.\nThe main advantage of our approach is its generality and eﬃciency. The posterior cal-\nculation and prediction is fully conjugate and can be performed online. At the t-th time\nstep, inference takes O(ln t) time. Sampling from the tree, which need only be done infre-\nquently, is O(t). These properties are in contrast to other non-parametric approaches for\nreinforcement learning such as GPs. The most computationally heavy step of our algorithm\nis ADP. However, once a policy is calculated, the actions to be taken can be calculated\nin logarithmic time at each step. The speciﬁc ADP algorithm used is not integral to our\napproach and for some problems it might be more eﬃcient to use an online algorithm.\n2. Cover Tree Bayesian RL\nThe main idea of cover tree Bayesian reinforcement learning (CTBRL) is to construct a cover\ntree from the observations, simultaneously inferring a conditional probability density on the\nsame structure, and to then use sampling to estimate a policy. We use a cover tree due to its\neﬃciency compared with e.g. a ﬁxed sequence of partitions or other dynamic partitioning\nmethods such as KD-trees. The probabilistic model we use can be seen as a distribution\nover piecewise linear-Gaussian densities, with one local linear model for each set in each\npartition. Due to the tree structure, the posterior can be computed eﬃciently online. By\ntaking a sample from the posterior, we acquire a speciﬁc piecewise linear Gaussian model.\nThis is then used to ﬁnd an approximately optimal policy using approximate dynamic\nprogramming.\nAn overview of CTBRL is given in pseudocode in Alg. 1. As presented, the algorithm\nworks in an episodic manner.2 When a new episode k starts at time tk, we calculate a\nnew stationary policy by sampling a tree µk from the current posterior ptk(µ). This tree\ncorresponds to a piecewise-linear model. We draw a large number of rollout trajectories\nfrom µk using an arbitrary exploration policy. Since we have the model, we can use an\ninitial state distribution that covers the space well. These trajectories are used to estimate\na near-optimal policy πk using approximate dynamic programming. During the episode, we\ntake new observations using πk, while growing the cover tree as necessary and updating the\nposterior parameters of the tree and the local model in each relevant tree node.\nWe now explain the algorithm in detail. First, we give an overview of the cover tree\nstructure on which the context tree model is built. Then we show how to perform inference\non the context tree, while Section 2.3 describes the multivariate model used in each node\nof the context tree.\nThe sampling approach and the approximate dynamic method are\ndescribed in Sec. 2.4, while the overall complexity of the algorithm is discussed in Sec. 2.5.\n2. An online version of the same algorithm (still employing Thompson sampling) would move line 6 to just\nbefore line 9. A fully Bayes online version would “simply” take an approximation of the Bayes-optimal\naction at every step.\n5\nTziortztiotis et al.\nAlgorithm 1 CTBRL (Episodic, using Thompson sampling)\n1: k = 0, π0 = Unif (A), prior p0 on M.\n2: for t = 1, . . . , T do\n3:\nif episode-end then\n4:\nk := k + 1.\n5:\nSample model µk ∼pt(µ).\n6:\nCalculate policy πk ≈arg maxπ Eπ\nµk U.\n7:\nend if\n8:\nObserve state st.\n9:\nTake action at ∼πk(· | st).\n10:\nObserve next state st+1, reward rt+1.\n11:\nAdd a leaf node to the tree Tat, containing st.\n12:\nUpdate posterior: pt+1(µ) = pt(µ | st+1, st, at) by updating the parameters of all\nnodes containing st.\n13: end for\n2.1 The cover tree structure\nCover trees are a data structure that can be applied to any metric space and are, among\nother things, an eﬃcient method to perform nearest-neighbour search in high-dimensional\nspaces (Beygelzimer et al., 2006). In this paper, we use cover trees to automatically con-\nstruct a sequence of partitions of the state space. Section 2.1.1 explains the properties of\nthe constructed cover tree. As the formal construction duplicates nodes, in practice we use\na reduced tree where every observed point corresponds to one node in the tree. This is\nexplained in Section 2.1.2. An explanation of how nodes are added to the structure is given\nin Section 2.1.3.\n2.1.1 Cover tree properties.\nTo construct a cover tree T on a metric space (Z, ψ) we require a set of points Dt =\n{z1, . . . , zt}, with zi ∈Z, a metric ψ, and a constant ζ > 1. We introduce a mapping\nfunction [·] so that the i-th tree node corresponds to one point z[i] in this set. The nodes\nare arranged in levels, with each point being replicated at nodes in multiple levels, i.e. we\nmay have [i] = [j] for some i ̸= j. Thus, a point corresponds to multiple nodes in the tree,\nbut to at most one node at any one level. Let Gn denote the set of points corresponding\nto the nodes at level n of the tree and C(i) ⊂Gn−1 the corresponding set of children. If\ni ∈Gn then the level of i is ℓ(i) = n. The tree has the following properties:\n1. Reﬁnement: Gn ⊂Gn−1.\n2. Siblings separation: i, j ∈Gn, ψ(z[i], z[j]) > ζn.\n3. Parent proximity: If i ∈Gn−1 then ∃a unique j ∈Gn such that ψ(z[i], z[j]) ≤ζn and\ni ∈C(j).\nThese properties can be interpreted as follows. Firstly lower levels always contain more\npoints. Secondly, siblings at a particular level are always well-separated. Finally, a child\n6\nCover Tree Bayesian RL\nmust be close to its parent. These properties directly give rise to the theoretical guarantees\ngiven by the cover tree structure, as well as methods for searching and adding points to the\ntree, as explained below.\n2.1.2 The reduced tree\nAs formally the cover tree duplicates nodes, in practice we use the explicit representa-\ntion (described in more detail in Sec. 2 of Beygelzimer et al., 2006). This only stores the\ntop-most tree node i corresponding to a point z[i]. We denote this reduced tree by ˆT . The\ndepth d(i) of node i ∈ˆT is equal to its number of ancestors, with the root node having a\ndepth of 0. After t observations, the set of nodes containing a point z, is:\nˆGt(z) ≜\nn\ni ∈ˆT\n\f\f\f z ∈Bi\no\n,\n(2.1)\nwhere Bi =\n\b\nz ∈Z\n\f\f ψ(z[i], z) ≤ζd(i) \t\nis the neighbourhood of i. Then ˆGt(z) forms a\npath in the tree, as each node only has one parent, and can be discovered in logarithmic\ntime through the Find-Nearest function (Beygelzimer et al., 2006, The. 5). This fact\nallows us to eﬃciently search the tree, insert new nodes, and perform inference.\n2.1.3 Inserting nodes in the cover tree\nThe cover tree insertion we use is only a minor adaptation of the Insert algorithm by\nBeygelzimer et al. (2006). For each action a ∈A, we create a diﬀerent reduced tree ˆTa, over\nthe state space, i.e. Z = S, and build the tree using the metric ψ(s, s′) = ∥s −s′∥1.\nAt each point in time t, we obtain a new observation tuple st, at, st+1. We select the\ntree ˆTat corresponding to the action. Then, we traverse the tree, decreasing d and keeping\na set of nodes Qd ⊂Gd that are ζd-close to st. We stop whenever Qd contains a node that\nwould satisfy the parent proximity property if we insert the new point at d −1, while the\nchildren of all other nodes in Qd would satisfy the sibling separation property. This means\nthat we can now insert the new datum as a child of that node.3 Finally, the next state st+1\nis only used during the inference process, explained below.\n2.2 Generalised context tree inference\nIn our model, each node i ∈ˆT is associated with a particular Bayesian model. The main\nproblem is how to update the individual models and how to combine them. Fortunately,\na closed form solution exists due to the tree structure. We use this to deﬁne a generalised\ncontext tree, which can be used for inference.\nAs with other tree models (Willems et al., 1995; Ferguson, 1974), our model makes\npredictions by marginalising over a set of simpler models. Each node in the context tree is\ncalled a context, and each context is associated with a speciﬁc local model. At time t, given\nan observation st = s and an action at = a, we calculate the marginal (predictive) density\npt of the next observation:\npt(st+1 | st, at) =\nX\nct\npt(st+1 | st, ct)pt(ct | st, at),\n(2.2)\n3. The exact implementation is available in the CoverTree class in Dimitrakakis et al. (2007).\n7\nTziortztiotis et al.\nst\nst+1\nat\nct\nθt\nFigure 1: The generalised context tree graphical model. Blue circles indicate observed vari-\nables.\nGreen dashed circles indicate latent variables.\nRed rectangles indicate\nchoice variables. Arrows indicate dependencies. Thus, the context distribution\nat time t depends on both the state and action, while the parameters depend on\nthe context. The next state depends on the action only indirectly.\nwhere we use the symbol pt throughout for notational simplicity to denote marginal distri-\nbutions from our posterior at time t. Here, ct is such that if pt(ct = i | st, at) > 0, then\nthe current state is within the neighbourhood of i-th node of the reduced cover tree ˆTat, i.e.\nst ∈Bi.\nFor Euclidean state spaces, the i-th component density pt(st+1 | st, ct = i) employs a\nlinear Bayesian model, which we describe in the next section. The graphical structure of\nthe model is shown in simpliﬁed form in Fig. 1. The context at time t depends only on the\ncurrent state st and action at. The context corresponds to a particular local model with\nparameter θt, which deﬁnes the conditional distribution.\nThe probability distribution pt(ct | st, at) is determined through stopping probabilities.\nMore precisely, we set it be equal to the probability of stopping at the i-th context, when\nperforming a walk from the leaf node containing the current observation towards the root,\nstopping at the j-th node with probability wj,t along the way:\npt(ct = i | st, at) = wi,t\nY\nj∈Dt(i)\n(1 −wj,t),\n(2.3)\nwhere Dt(i) are the descendants of i that contain the observation st. This forms a path from\ni to the leaf node containing st. Note that w0,t = 1, so we always stop whenever we reach\nthe root. Due to the eﬀectively linear structure of the relevant tree nodes, the stopping\nprobability parameters w can be updated in closed form, as shown in (Dimitrakakis, 2010a,\nTheorem 1) via Bayes’ theorem as follows:\nwi,t+1 =\npt(st+1 | st, ct = i)wi,t\npt(st+1 | st, ct ∈{i} ∪Dt(i)).\n(2.4)\nSince there is a diﬀerent tree for each action, cti uniquely identiﬁes a tree, the action\ndoes not need to enter in the conditional expressions above. Finally, it is easy to see, by\nmarginalisation and the deﬁnition of the stopping probabilities, that the denominator in\nthe above equation can be calculated recursively:\npt(st+1 | st, ct ∈{i}∪Dt(i)) = wi,tpt(st+1 | st, ct = i)+(1−wi,t)pt(st+1 | st, ct ∈Dt(i)).\n(2.5)\n8\nCover Tree Bayesian RL\nConsequently, inference can be performed with a simple forward-backward sweep through\na single tree path. In the forward stage, we compute the probabilities of the denominator,\nuntil we reach the point where we have to insert a new node. Whenever a new node is\ninserted in the tree, its weight parameter is initialised to 2−d(i). We then go backwards to\nthe root node, updating the weight parameters and the posterior of each model. The only\nremaining question is how to calculate the individual predictive marginal distributions for\neach context i in the forward sweep and how to calculate their posterior in the backward\nsweep. In this paper, we associate a linear Bayesian model with each context, which provides\nthis distribution.\n2.3 The linear Bayesian model\nIn our model we assume that, given ct = i, the next state st+1 is given by a linear transfor-\nmation of the current state and additive noise εi,t:\nst+1 = Aixt + εi,t,\nxt ≜\n\u0012st\n1\n\u0013\n,\n(2.6)\nwhere xt is the current state vector augmented by a unit basis.4 In particular, each context\nmodels the dynamics via a Bayesian multivariate linear-Gaussian model. For the i-th con-\ntext, there is a diﬀerent (unknown) parameter pair (Ai, Vi) where Ai is the design matrix\nand Vi is the covariance matrix. Then the next state distribution is:\nst+1 | xt = x, ct = i ∼N (Aix, Vi).\n(2.7)\nThus, the parameters θt which are abstractly shown in Fig. 1 correspond to the two matrices\nA, V . We now deﬁne the conditional distribution of these matrices given ct = i.\nWe can model our uncertainty about these parameters with an appropriate prior dis-\ntribution p0. In fact, a conjugate prior exists in the form of the matrix inverse-Wishart\nnormal distribution. In particular, given Vi = V , the distribution for Ai is matrix-normal,\nwhile the marginal distribution of Vi is inverse-Wishart:\nAi | Vi = V ∼N (Ai | M, C\n| {z }\nprior parameters\n, V )\n(2.8)\nVi ∼W (Vi |\nz }| {\nW , n).\n(2.9)\nHere N is the prior on design matrices, which has a matrix-normal distribution, conditional\non the covariance and two prior parameters: M, which is the prior mean and C which is\nthe prior covariance of the dependent variable (i.e. the output). Finally, W is the marginal\nprior on covariance matrices, which has an inverse-Wishart distribution with W and n.\nMore precisely, the distributions have the following forms:\nN (Ai | M, C, V ) ∝e−1\n2 tr[(Ai−M)⊤V −1(Ai−M)C]\nW (V | W , n) ∝|V −1W /2|n/2e−1\n2 tr(V −1W ).\n4. While other transformations of st are possible, we do not consider them in this paper.\n9\nTziortztiotis et al.\nEssentially, the model extends the classic Bayesian linear regression model (e.g DeGroot,\n1970) to the multivariate case via vectorisation of the mean matrix.\nSince the prior is\nconjugate, it is relatively simple to calculate the posterior after each observation.\nFor\nsimplicity, and to limit the total number of prior parameters we have to select, we use the\nsame prior parameters (Mi, Ci, Wi, ni) for all contexts in the tree.\nTo integrate this with inference in the tree, we must deﬁne the marginal distribution\nused in the nominator of (2.4).\nThis is a multivariate Student-t distribution, so if the\nposterior parameters for context i at time t are (M t\ni , Ct\ni, W t\ni , nt\ni), then this is:\npt(st+1 | xt = x, ct = i) = Student(M t\ni , W t\ni /zt\ni, 1 + nt\ni),\n(2.10)\nwhere zt\ni = 1 −x⊤(Ct\ni + xx⊤)−1x.\n2.3.1 Regression illustration\n−4\n−2\n0\n2\n4\n−1\n0\n1\nst\nst+1\n103 samples\n−4\n−2\n0\n2\n4\n−1\n0\n1\nst\n104 samples\nE(st+1 | st)\nEpt(st+1 | st)\nEˆµ1(st+1 | st)\nEˆµ2(st+1 | st)\nFigure 2: Regression illustration. We plot the expected value for the real distribution, the\nmarginal, as well as two sampled models ˆµ1, ˆµ2 ∼pt(µ).\nAn illustration of inference using the generalised context tree is given in Fig. 2, where the\npiecewise-linear structure is evident. The st variates are drawn uniformly in the displayed\ninterval, while st+1 | st = s ∼N (sin(s), 0.1), i.e. drawn a normal distribution with mean\nsin(st) and variance 0.1.\nThe plot shows the marginal expectation Ept, as well as the\nexpectation from two diﬀerent models sampled from the posterior pt(µ).\n10\nCover Tree Bayesian RL\n2.4 Approximating the optimal policy with Thompson sampling\nMany algorithms exist for ﬁnding the optimal policy for a speciﬁc MDP µ, or for calculating\nthe expected utility of a given policy for that MDP. Consequently, a simple idea is to draw\nMDP samples µi from the current posterior distribution and then calculate the expected\nutility of each. This can be used to obtain approximate lower and upper bounds on the\nBayes-optimal expected utility by maximising over the set of memoryless policies Π1. Taking\nK samples, allows us to calculate the upper and lower bounds with accuracy O(1/\n√\nK).\nmax\nπ∈Π1 Eπ\np U ≈max\nπ∈Π1\n1\nK\nK\nX\ni=1\nEπ\nµi U ≤1\nK\nK\nX\ni=1\nmax\nπ∈Π1 Eπ\nµi U,\nµi ∼pt(µ).\n(2.11)\nWe consider only the special case K = 1, i.e. when we only sample a single MDP. Then the\ntwo values are identical and we recover Thompson sampling. The main problems we have\nto solve now is how to sample a model and how to calculate a policy for the sampled model.\n2.4.1 Sampling a model from the posterior.\nEach model µ sampled from the posterior corresponds to a particular choice of tree pa-\nrameters. Sampling is done in two steps. The ﬁrst generates a partition from the tree\ndistribution and the second step generates a linear model for each context in the partition.\nThe ﬁrst step is straightforward. We only need to sample a set of weights ˆwi ∈{0, 1}\nsuch that P( ˆwi = 1) = wi,t, as shown in (Dimitrakakis, 2010a, Rem. 2). This creates a\npartition, with one Bayesian multivariate linear model responsible for each context in the\npartition.\nThe second step is to sample a design and covariance matrix pair ( ˆ\nAi, ˆVi) for each context\ni in the partition. This avoids sampling matrices for contexts not part of the sampled tree.\nAs the model suggests, we can ﬁrst sample the noise covariance by plugging the posterior\nparameters in (2.9) to obtain ˆVi. Sampling from this distribution can be done eﬃciently\nusing the algorithm suggested by Smith and Hocking (1972). We then plug in ˆVi into the\nconditional design matrix posterior (2.8) to obtain a design matrix ˆ\nAi by sampling from\nthe resulting matrix-normal distribution.\nThe ﬁnal MDP sample µ from the posterior has two elements. Firstly, a set of contexts\nˆCµ ⊂S\na∈A ˆTa, from all action trees. This set is a partition with associated mapping fµ : S×\nA →ˆCµ. Secondly, a set of associated design and covariance matrices\nn\n(Aµ\ni , V µ\ni )\n\f\f\f i ∈ˆCµ o\nfor each context. Then the prediction of the sampled MDP is:\nPµ(st+1 | st, at) = N (Aµ\nf(st,at)xt, V µ\nf(st,at)),\n(2.12)\nwhere xt is given in (2.6).\n2.4.2 Finding a policy for a sample via ADP\nIn order to calculate an optimal policy π∗(µ) for µ, we generate a large number of trajectories\nfrom µ using a uniform policy. After selecting an appropriate set of basis functions, we then\nemploy a variant of the least-squares policy iteration (LSPI (Lagoudakis and Parr, 2003))\nalgorithm, using least-squares temporal diﬀerences (LSTD Bradtke and Barto (1996)) rather\n11\nTziortztiotis et al.\nthan LSTDQ. This is possible because since we have µ available, we have access to (2.12)\nand it makes LSPI slightly more eﬃcient.\nMore precisely, consider the value function V π\nµ : S →R, deﬁned as:\nV π\nµ (s) ≜Eπ\nµ (U | st = s) .\n(2.13)\nUnfortunately, for continuous S ﬁnding an optimal policy requires approximations. A com-\nmon approach is to make use of the fact that:\nV π\nµ (s) = ρ(s) + γ\nZ\nS\nV π\nµ (s′) dP π\nµ (s′ | s),\n(2.14)\nwhere we assume for simplicity that ρ(s) is the reward obtained at state s. The conditional\nmeasure P π\nµ is the transition kernel on S induced by µ, π, introduced in Section 1.1. We\nthen select a parametric family vω : S →R with parameter ω ∈Ωand minimise:\nh(ω) +\nZ\nS\n\r\r\r\rvω(s) −ρ(s) −γ\nZ\nS\nvω(s′) d ˆP π\nµ (s′|s)\n\r\r\r\r dχ(s),\n(2.15)\nwhere h is a regularisation term, χ is an appropriate measure on S and ˆP π\nµ is an empirical\nestimate of the transition kernel, used to approximate the respective integral that uses P π\nµ .\nAs we can take an arbitrary number of trajectories from µ, π, this can be as accurate as our\ncomputational capacity allows.\nIn practice, we minimise (2.15) with a generalised linear model (deﬁned on an appropri-\nate basis) for vω while χ need only be positive on a set of representative states. Speciﬁcally,\nwe employ a variant of the least-squares policy iteration (LSPI (Lagoudakis and Parr, 2003))\nalgorithm, using the least-squares temporal diﬀerences (LSTD Bradtke and Barto (1996))\nfor the minimisation of (2.15). Then the norm is the euclidean norm and the regularisation\nterm is h(ω) = λ∥ω∥. In order to estimate the inner integral, we take KL ≥1 samples from\nthe model so that\nˆP π\nµ (s′ | s) ≜\n1\nKL\nKL\nX\ni=1\nI\n\b\nsi\nt+1 = s′ | si\nt = s\n\t\n,\n(2.16)\nsi\nt+1 | si\nt = s ∼P π\nµ (· | s),\nwhere I {·} is an indicator function and P π\nµ is decomposable in known terms. Equation\n(2.16) is also used for action selection in order to calculate an approximate expected utility\nqω(s, a) for each state-action pair (s, a):\nqω(s, a) ≜ρ(s) + γ\nZ\nS\nvω(s′) d ˆP π\nµ (s′|s)\n(2.17)\nEﬀectively, this approximates the integral via sampling. This may add a small amount5 of\nadditional stochasticity to action selection, which can be reduced6 by increasing KL.\nFinally, we optimise the policy by approximate policy iteration. At the j-th iteration we\nobtain an improved policy ˆπj(a | s) ∝P[a ∈arg maxa′∈A qωj−1(s, a′)] from ωj−1 and then\nestimate ωj for the new policy.\n5. Generally, this error is bounded by O(K−1/2\nL\n).\n6. We remind the reader that Thompson sampling itself results in considerable exploration by sampling an\nMDP from the posterior. Thus, additional randomness may be detrimental.\n12\nCover Tree Bayesian RL\n2.5 Complexity\nWe now analyse the computational complexity of our approach, including the online com-\nplexity of inference and decision making, and of the sampling and ADP taking place every\nepisode. It is worthwhile to note two facts. Firstly, that the complexity bounds related\nto the cover tree depend on a constant c, which however depends on the distribution of\nsamples in the state space. In the worst case (i.e. a uniform distribution), this is bounded\nexponentially in the dimensionality of the actual state space. While we do not expect this\nto be the case in practice, it is easy to construct a counterexample where this is the case.\nSecondly, that the complexity of the ADP step is largely independent of the model used,\nand mostly depends on the number of trajectories we take in the sampled model and the\ndimensionality of the feature space.\nFirst, we examine the total computation time that is required to construct the tree.\nCorollary 1 Cover tree construction from t observations takes O(t ln t) operations.\nProof\nIn the cover tree, node insertion and query are O(ln t)(Beygelzimer et al., 2006,\nTheorems 5, 6). Then note that Pt\nk=1 ln k ≤Pt\nk=1 ln t = t ln t.\nAt every step of the process, we must update our posterior parameters. Fortunately, this\nalso takes logarithmic time as we only need to perform calculations for a single path from\nthe root to a leaf node.\nLemma 2 If S ⊂Rm, then inference at time step t has complexity O(m3 ln t).\nProof At every step, we must perform inference on a number of nodes equal to the length\nof the path containing the current observation. This is bounded by the depth of the tree,\nwhich is in turn bounded by O(ln t) from (Beygelzimer et al., 2006, Lem. 4.3). Calculating\n(2.4) is linear in the depth. For each node, however, we must update the linear-Bayesian\nmodel, and calculate the marginal distribution. Each requires inverting an m × m matrix,\nwhich has complexity O(m3).\nFinally, at every step we must choose an action through value function look-up. This again\ntakes logarithmic time, but there is a scaling depending on the complexity of the value\nfunction representation.\nLemma 3 If the LSTD basis has dimensionality mL, then taking a decision at time t has\ncomplexity O(KLmL ln t).\nProof\nTo take a decision we merely need to search in each action tree to ﬁnd a corre-\nsponding path. This takes O(ln t) time for each tree. After Thompson sampling, there will\nonly be one linear model for each action tree. LSTD takes KL operations, and requires the\ninner product of two mL-dimensional vectors.\nThe above lemmas give the following result:\nTheorem 4 At time t, the online complexity of CTBRL is O((m3 + KLmL) ln t).\nWe now examine the complexity of ﬁnding a policy. Although this is the most computa-\ntionally demanding part, its complexity is not dependent on the cover tree structure or the\nprobabilistic inference method used. However, we include it here for completeness.\n13\nTziortztiotis et al.\nLemma 5 Thompson sampling at time t is O(tm3).\nProof In the worst case, our sampled tree will contain all the leaf nodes of the reduced tree,\nwhich are O(t). For each sampled node, the most complex operation is Wishart generation,\nwhich is O(m3) (Smith and Hocking, 1972).\nLemma 6 If we use ns samples for LSTD estimation and the basis dimensionality is mL,\nthis step has complexity O(m3\nL + ns(m2\nL + KLmL ln t)).\nProof For each sample we must take a decision according to the last policy, which requires\nO(KLmL ln t) as shown previously. We also need to update two matrices (see Boyan (2002)),\nwhich is O(m2\nL). So, O(ns(m2\nL +KLmL ln t)) computations must be performed for the total\nnumber of the selected samples. Since LSTD requires an mL × mL matrix inversion, with\ncomplexity O(m3\nL), we obtain the ﬁnal result.\nFrom Lemmas 3 and 6 it follows that:\nTheorem 7 If we employ API with KA iterations, the total complexity of calculating a new\npolicy is O(tm3 + KA(m3\nL + ns(m2\nL + KLmL ln t))).\nThus, while the online complexity of CTBRL is only logarithmic in t, there is a sub-\nstantial cost when calculating a new policy. This is only partially due to the complexity of\nsampling a model, which is manageable when the state space has small dimensionality. Most\nof the computational eﬀort is taken by the API procedure, at least as long as t < (mL/m)3.\nHowever, we think this is unavoidable no matter what the model used is.\nThe complexity of Gaussian process (GP) models is substantially higher. In the simplest\nmodel, where each output dimension is modelled independently, inference is O(mt3), while\nthe fully multivariate tree model has complexity O(m3t ln t). Since there is no closed form\nmethod for sampling a function from the process, one must resort to iterative sampling\nof points. For n points, the cost is approximately O(nmt3), which makes sampling long\ntrajectories prohibitive. For that reason, in our experiments we only use the mean of the\nGP.\n3. Experiments\nWe conducted two sets of experiments to analyse the oﬄine and the online performance.\nWe compared CTBRL with the well-known LSPI algorithm (Lagoudakis and Parr, 2003)\nfor the oﬄine case, as well as an online variant (Bu¸soniu et al., 2010) for the online case.\nWe also compared CTBRL with linear Bayesian reinforcement learning (LBRL, Tziortziotis\net al., 2013) and ﬁnally GP-RL, where we simply replaced the tree model with a Gaussian\nprocess. For CTBRL and LBRL we use Thompson sampling. However, since Thompson\nsampling cannot be performed on GP models, we use the mean GP instead. In order to\ncompute policies given a model, all model-based methods use the variant of LSPI explained\nin Section 2.4.2. Hence, the only signiﬁcant diﬀerence between each approach is the model\nused, and whether or not they employ Thompson sampling.\nA signiﬁcant limitation of Gaussian processes is that their computational complexity\nbecomes prohibitive as the number of samples becomes extremely large. In order to make\n14\nCover Tree Bayesian RL\nthe GP model computationally practical, the greedy approximation approach introduced\nby Engel et al. (2002) has been adopted. This is a kernel sparsiﬁcation methodology which\nincrementally constructs a dictionary of the most representative states. More speciﬁcally,\nan approximate linear dependence analysis is performed in order to examine whether a state\ncan be approximated suﬃciently as a linear combination of current dictionary members or\nnot.\nWe used one preliminary run and guidance from the literature to make an initial selection\nof possible hyper-parameters, such as the number of samples and the features used for LSTD\nand LSTD-Q. We subsequently used 10 runs to select a single hyper-parameter combination\nfor each algorithm-domain pair. The ﬁnal evaluation was done over an independent set of\n100 runs.\nFor CTBRL and the GP model, we had the liberty to draw an arbitrary number of\ntrajectories for the value function estimation. We drew 1-step transitions from a set of 3000\nuniformly drawn states from the sampled model (the mean model in the GP case). We used\n25 API iterations on this data.\nFor the oﬄine performance evaluation, we ﬁrst drew rollouts from k = {10, 20, . . . , 50,\n100, . . . , 1000} states drawn from the true environment’s starting distribution, using a uni-\nformly random policy. The maximum horizon of each rollout was set equal to 40. The\ncollected data was then fed to each algorithm in order to produce a policy. This policy was\nevaluated over 1000 rollouts on the environment.\nIn the online case, we simply use the last policy calculated by each algorithm at the\nend of the last episode, so there is no separate learning and evaluation phase. This means\nthat eﬃcient exploration must be performed. For CTBRL, this is done using Thompson\nsampling. For online-LSPI, we followed the approach of (Bu¸soniu et al., 2010), who adopts\nan ϵ-greedy exploration scheme with an exponentially decaying schedule ϵt = ϵt\nd, with\nϵ0 = 1. In preliminary experiments, we found ϵd = 0.997 to be a reasonable compromise.\nWe compared the algorithms online for 1000 episodes.\n3.1 Domains\nWe consider two well-known continuous state, discrete-action, episodic domains. The ﬁrst\nis the inverted pendulum domain and the second is the mountain car domain.\n3.1.1 Inverted pendulum\nThe goal in this domain, is to balance a pendulum by applying forces of a mixed magnitude\n(50 Newtons). The state space consists of two continuous variables, the vertical angle and\nthe angular velocity of the pendulum. There are three actions: no force, left force or right\nforce. A zero reward is received at each time step except in the case where the pendulum\nfalls. In this case, a negative (-1) reward is given and a new episode begins. An episode\nalso ends with 0 reward after 3000 steps, after which we consider that the pendulum is\nsuccessfully balanced. Each episode starts by setting the pendulum in a perturbed state\nclose to the equilibrium point. More information about the speciﬁc dynamics can be found\nat (Lagoudakis and Parr, 2003).\nThe discount factor γ was 0.95.\nThe basis we used\nfor LSTD/LSPI, was equidistant 3 × 3 grid of RBFs over the state space following the\n15\nTziortztiotis et al.\nsuggestions of Lagoudakis and Parr (2003). This was replicated for each action for the\nLSTD-Q algorithm used in LSPI.\n3.1.2 Mountain car\nThe aim in this domain is to drive an underpowered car to the top of a hill. Two continuous\nvariables characterise the vehicle state in the domain, its position and its velocity. The\nobjective is to drive an underpowered vehicle up a steep valley from a randomly selected\nposition to the right hilltop (at position > 0.5) within 1000 steps. There are three actions:\nforward, reverse and zero throttle. The received reward is −1 except in the case where the\ntarget is reached (zero reward). At the beginning of each rollout, the vehicle is positioned to\na new state, with the position and the velocity uniformly randomly selected. The discount\nfactor is set to γ = 0.999. An equidistant 4 × 4 grid of RBFs over the state space plus a\nconstant term is selected for LSTD and LSPI.\n3.2 Results\nIn our results, we show the average performance in terms of number of steps of each method,\naveraged over 100 runs. For each average, we also plot the 95% conﬁdence interval for the\naccuracy of the mean estimate with error bars. In addition, we show the 90% percentile\nregion of the runs, in order to indicate inter-run variability in performance.\nFigure 3(a) shows the results of the experiments in the oﬄine case. For the mountain\ncar, it is clear that CTBRL is signiﬁcantly more stable compared to GPRL and LSPI. In\ncontrast to the other two approaches, CTBRL needs only a small number of rollouts in\norder to discover the optimal policy. For the pendulum domain, the performance of both\nCTBRL and GPRL is almost perfect, as they need only about twenty rollouts in order to\ndiscover the optimal policy. On the other hand, LSPI despite the fact that manages to ﬁnd\nthe optimal policy frequently, around 5% of its runs fail.\nFigure 3(b) shows the results of the experiments in the online case. For the mountain\ncar, CTBRL managed to ﬁnd an excellent policy in the vast majority of runs, while con-\nverging earlier than GPRL and LSPI. Moreover, CTBRL presents a more stable behaviour\nin contrast to the other two. In the pendulum domain, the performance diﬀerence relative\nto LSPI is even more impressive. It becomes apparent that both CTBRL and GPRL reach\nnear optimal performances with an order of magnitude fewer episodes than LSPI, while the\nlatter remains unstable. In this experiment, we see that CTBRL reaches an optimal policy\nslightly before GPRL. Although the diﬀerence is small, it is very consistent.\nThe success of CTBRL over the other approaches can be attributed to a number of\nreasons. Firstly, it could be a better model. Indeed, in the oﬄine results for the mountain\ncar domain, where the starting state distribution is uniform, and all methods have the same\ndata, we can see that CTBRL has a far better performance than everything else.\nThe\nsecond could be the more eﬃcient exploration aﬀorded by Thompson sampling. Indeed, in\nthe mountain car online experiments we see that the LBRL performs quite well (Fig. 3(b)),\neven though its oﬄine performance is not very good (Fig. 3(a)).\nHowever, Thompson\nsampling is not suﬃcient for obtaining a good performance, as seen by both the oﬄine\nresults and the performance in the pendulum domain.\n16\nCover Tree Bayesian RL\n4. Conclusion\nWe proposed a computationally eﬃcient, fully Bayesian approach for the exact inference\nof unknown dynamics in continuous state spaces.\nThe total computation for inference\nafter t steps is O(t ln t), in stark contrast to other non-parametric models such as Gaussian\nprocesses, which scale O(t3). In addition, inference is naturally performed online, with the\ncomputational cost at time t being O(ln t).\nIn practice, the computational complexity is orders of magnitude lower for cover trees\nthan GP, even for these problems.\nWe had to use a dictionary and a lot of tuning to\nmake GP methods work, while cover trees worked out of the box. Another disadvantage of\nGP methods is that it is infeasible to implement Thompson sampling with them. This is\nbecause it is not possible to directly sample a function from the GP posterior. Although\nThompson sampling confers no advantage in the oﬄine experiments (as the data there were\nthe same for all methods), we still see that the performance of CTBRL is signiﬁcantly better\non average and that it is much more stable.\nExperimentally, we showed that cover trees are more eﬃcient both in terms of compu-\ntation and in terms of reward, relative to GP models that used the same ADP method to\noptimise the policy and to a linear Bayesian model which used both the same ADP method\nand the same exploration strategy. We can see that overall the linear model performs sig-\nniﬁcantly worse than both GP-RL and CTBRL, though better than ϵ-greedy LSPI. This\nshows that the main reason for the success of CTBRL is the cover tree inference and not\nthe linear model itself, or Thompson sampling.\nCTBRL is particularly good in online settings, where the exact inference, combined with\nthe eﬃcient exploration provided by Thompson sampling give it an additional advantage.\nWe thus believe that CTBRL is a method that is well-suited for exploration in unknown\ncontinuous state problems. Unfortunately, it is not possible to implement Thompson sam-\npling in practice using GPs, as there is no reasonable way to sample a function from the\nGP posterior. Nevertheless, we found that in both online and oﬄine experiments (where\nThompson sampling should be at a disadvantage) the cover tree method achieved superior\nperformance to Gaussian processes.\nAlthough we have demonstrated the method in low dimensional problems, higher di-\nmensions are not a problem for the cover tree inference itself. The bottleneck is the value\nfunction estimation and ADP. This is independent of the model used, however. For example,\nGP methods for estimating the value function (c.f. Deisenroth et al., 2009) typically have a\nlarge number of hyper-parameters for value function estimation, such as choice of represen-\ntative states and trajectories, kernel parameters and method for updating the dictionary,\nto avoid problems with many observations.\nWhile in practice ADP can be performed in the background while inference is taking\nplace, and although we seed the ADP with the previous solution, one would ideally like to\nuse a more incremental approach for that purpose. One interesting idea would be to employ\na gradient approach in a similar vein to Deisenroth and Rasmussen (2011). An alternative\napproach would be to employ an online method, in order to avoid estimating a policy for\nthe complete space.7 Promising such approaches include running bandit-based tree search\nmethods such as UCT (Kocsis and Szepesv´ari, 2006) on the sampled models.\n7. A suggestion made by the anonymous reviewers.\n17\nTziortztiotis et al.\nAnother direction of future work is to consider more sophisticated exploration policies,\nparticularly for larger problems. Due to the eﬃciency of the model, it should be possible\nto compute near-Bayes-optimal policies by applying the tree search method used by Veness\net al. (2011). Finally, it would be interesting to examine continuous actions. These can\nbe handled eﬃciently both by the cover tree and the local linear models by making the\nnext state directly dependent on the action through an augmented linear model. While\noptimising over a continuous action space is challenging, more recent eﬃcient tree search\nmethods such as metric bandits (Bubeck et al., 2011) may alleviate that problem.\nAn interesting theoretical direction would be to obtain regret bounds for the problem.\nThis could perhaps be done building upon the analyses of Kozat et al. (2007) for context\ntree prediction, and of Ortner and Ryabko (2012) for continuous MDPs. The statistical\neﬃciency of the method could be improved by considering edge-based (rather than node-\nbased) distributions on trees, as was suggested by Pereira and Singer (1999).\nFinally, as the cover tree method only requires specifying an appropriate metric, the\nmethod could be applicable to many other problems.\nThis includes both large discrete\nproblems, and partially observable problems. It would be interesting to see if the approach\nalso gives good results in those cases.\nAcknowledgements\nWe would like to thank the anonymous reviewers, for their careful and detailed comments\nand suggestions, for this and previous versions of the paper, which have signiﬁcantly im-\nproved the manuscript. We also want to thank Mikael K˚ageb¨ack for additional proofreading.\nThis work was partially supported by the Marie Curie Project ESDEMUU “Eﬃcient Se-\nquential Decision Making Under Uncertainty” , Grant Number 237816 and by an ERASMUS\nexchange grant.\nReferences\nP. Abbeel and A.Y. Ng. Exploration and apprenticeship learning in reinforcement learning.\nIn Proceedings of the 22nd international conference on Machine learning (ICML 2005),\n2005.\nS. Agrawal and N. Goyal.\nAnalysis of Thompson sampling for the multi-armed bandit\nproblem. In COLT 2012, 2012.\nM. Alvarez, D. Luengo-Garcia, M. Titsias, and N. Lawrence. Eﬃcient multioutput gaussian\nprocesses through variational inducing kernels. In JMLR: W&CP 9, pages 25–32, 2011.\nM. Araya, V. Thomas, O. Buﬀet, et al. Near-optimal BRL using optimistic local transitions.\nIn ICML, 2012.\nJ. Asmuth, L. Li, M. L. Littman, A. Nouri, and D. Wingate. A Bayesian sampling approach\nto exploration in reinforcement learning. In UAI 2009, 2009.\nPeter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforce-\nment learning. In Proceedings of NIPS 2008, 2008.\n18\nCover Tree Bayesian RL\nR. Begleiter, R. El-Yaniv, and G. Yona. On prediction using variable order Markov models.\nJournal of Artiﬁcial Intelligence Research, pages 385–421, 2004.\nM. Bellemare, J. Veness, and M. Bowling. Bayesian learning of recursively factored envi-\nronments. In ICML 2013, volume 28(3) of JMLR W & CP, pages 1211–1219, 2013.\nD. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.\nA. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In ICML\n2006, 2006.\nJ.A. Boyan. Technical update: Least-squares temporal diﬀerence learning. Machine Learn-\ning, 49(2):233–246, 2002.\nS.J. Bradtke and A.G. Barto. Linear least-squares algorithms for temporal diﬀerence learn-\ning. Machine Learning, 22(1):33–57, 1996.\nE. Brunskill, B. R. Leﬄer, L. Li, M. L. Littman, and N. Roy. Provably eﬃcient learning with\ntype parametric models. Journal of Machine Learning Research, 10:1955–1988, 2009.\nS. Bubeck, R. Munos, G. Stoltz, and C. Szepesv´ari. X-armed bandits. Journal of Machine\nLearning Research, 12:1655–1695, 2011.\nL. Bu¸soniu, D. Ernst, B. De Schutter, and R. Babuˇska. Online least-squares policy iter-\nation for reinforcement learning control. In Proceedings of the 2010 American Control\nConference, pages 486–491, 2010.\nP. Castro and D. Precup. Smarter sampling in model-based Bayesian reinforcement learning.\nMachine Learning and Knowledge Discovery in Databases, pages 200–214, 2010.\nM. Daswani, P. Sunehag, and M. Hutter. Feature reinforcement learning using looping suﬃx\ntrees. In Proceedings of 10th European Workshop on Reinforcement Learning (EWRL),\n2012.\nRichard Dearden, Nir Friedman, and Stuart J. Russell.\nBayesian Q-learning.\nIn\nAAAI/IAAI, pages 761–768, 1998. URL citeseer.ist.psu.edu/dearden98bayesian.\nhtml.\nM. H. DeGroot. Optimal Statistical Decisions. John Wiley & Sons, 1970.\nM. P. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-eﬃcient approach\nto policy search. In International conference on Machine Learning (ICML), Bellevue,\nWA, USA, July 2011.\nM.P. Deisenroth, C.E. Rasmussen, and J. Peters. Gaussian process dynamic programming.\nNeurocomputing, 72(7-9):1508–1524, 2009.\nC. Dimitrakakis.\nBayesian variable order Markov models.\nIn International Conference\non Artiﬁcial Intelligence and Statistics (AISTATS), volume 9 of JMLR : W&CP, pages\n161–168, Chia Laguna Resort, Sardinia, Italy, 2010a.\n19\nTziortztiotis et al.\nC. Dimitrakakis. Complexity of stochastic branch and bound methods for belief tree search\nin Bayesian reinforcement learning. In ICAART 2010, pages 259–264. Springer, 2010b.\nC. Dimitrakakis. Robust bayesian reinforcement learning through tight lower bounds. In\nEuropean Workshop on Reinforcement Learning (EWRL 2011), pages 177–188, 2011.\nC. Dimitrakakis, N. Tziortziotis, and A. Tossou. Beliefbox: A framework for statistical\nmethods in sequential decision making. http://code.google.com/p/beliefbox/, 2007.\nM. Duﬀ. Optimal Learning Computational Procedures for Bayes-adaptive Markov Decision\nProcesses. PhD thesis, University of Massachusetts at Amherst, 2002.\nY. Engel, S. Mannor, and R. Meir. Sparse online greedy support vector regression. In\nEuropean Conference on Machine Learning, pages 84–96, 2002.\nY. Engel, S. Mannor, and R. Meir.\nReinforcement learning with gaussian process.\nIn\nInternational Conference on Machine Learning, pages 201–208, 2005.\nD. Ernst, P. Geurts, and L. Wehenkel.\nTree-based batch mode reinforcement learning.\nJournal of Machine Learning Research, 6:503–556, 2005.\nV. F. Farias, C. C. Moallemi, B. Van Roy, and T. Weissman.\nUniversal reinforcement\nlearning. IEEE Transactions on Information Theory, 56(5):2441–2454, 2010.\nT. S. Ferguson.\nPrior distributions on spaces of probability measures.\nThe Annals of\nStatistics, 2(4):615–629, 1974. ISSN 00905364.\nM. Ghavamzadeh and Y. Engel. Bayesian policy gradient algorithms. In NIPS 2006, 2006.\nE. Kaufmann, N. Korda, and R. Munos.\nThompson sampling: An optimal ﬁnite time\nanalysis. In ALT-2012, 2012.\nL. Kocsis and C. Szepesv´ari. Bandit based Monte-Carlo planning. In Proceedings of ECML-\n2006, 2006.\nS.S. Kozat, A.C. Singer, and G.C. Zeitler. Universal piecewise linear prediction via context\ntrees. Signal Processing, IEEE Transactions on, 55(7):3730–3745, July 2007. ISSN 1053-\n587X. doi: 10.1109/TSP.2007.894235.\nM.G. Lagoudakis and R. Parr. Least-squares policy iteration. The Journal of Machine\nLearning Research, 4:1107–1149, 2003.\nM. Meila and M.I. Jordan.\nLearning with mixtures of trees.\nThe Journal of Machine\nLearning Research, 1:1–48, 2001.\nRonald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous rein-\nforcement learning. In NIPS 2012, 2012.\nI. Osband, D. Russo, and B. Van Roy. (more) eﬃcient reinforcement learning via posterior\nsampling. In NIPS, 2013.\n20\nCover Tree Bayesian RL\nS.M. Paddock, F. Ruggeri, M. Lavine, and M. West. Randomized Polya tree models for\nnonparametric Bayesian inference. Statistica Sinica, 13(2):443–460, 2003.\nF. C. Pereira and Y. Singer. An eﬃcient extension to mixture techniques for prediction and\ndecision trees. In Machine Learning, pages 183–199, 1999.\nP. Poupart, N. Vlassis, J. Hoey, and K. Regan. An analytic solution to discrete Bayesian\nreinforcement learning. In ICML 2006, pages 697–704. ACM Press New York, NY, USA,\n2006.\nM. L. Puterman. Markov Decision Processes : Discrete Stochastic Dynamic Programming.\nJohn Wiley & Sons, New Jersey, US, 2005.\nC.E. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning. In Advances\nin Neural Information Processing Systems 16, pages 751–759, 2004.\nWB Smith and RR Hocking. Wishart variates generator, algorithm AS 53. Applied Statis-\ntics, 21:341–345, 1972.\nA. L. Strehl and M. L. Littman. Online linear regression and its application to model-based\nreinforcement learning. In NIPS 2008, 2008.\nM. Strens. A Bayesian framework for reinforcement learning. In ICML 2000, pages 943–950,\n2000.\nW.R. Thompson. On the likelihood that one unknown probability exceeds another in view\nof the evidence of two samples. Biometrika, 25(3-4):285–294, 1933.\nN. Tziortziotis, C. Dimitrakakis, and K. Blekas. Linear Bayesian reinforcement learning.\nIn Proceedings of the 23rd international joint conference on artiﬁﬁcal intelligence (IJCAI\n2013), 2013.\nT. van Erven, P. D. Gr¨unwald, and S. de Rooij. Catching up faster by switching sooner\n: a prequential solution to the AIC-BIC dilemma. arXiv, 2008. A preliminary version\nappeared in NIPS 2007.\nJoel Veness, Kee Siong Ng, Marcus Hutter, William T. B. Uther, and David Silver. A\nMonte-Carlo AIXI approximation. J. Artif. Intell. Res. (JAIR), 40:95–142, 2011.\nJoel Veness, Kee Siong Ng, Marcus Hutter, and Michael Bowling. Context tree switching.\nIn Data Compression Conference (DCC), 2012, pages 327–336. IEEE, 2012.\nT. Wang, D. Lizotte, M. Bowling, and D. Schuurmans. Bayesian sparse sampling for on-line\nreward optimization. In ICML ’05, pages 956–963, New York, NY, USA, 2005. ACM.\nISBN 1-59593-180-5.\nF.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. The context tree weighting method:\nbasic properties. IEEE Transactions on Information Theory, 41(3):653–664, 1995.\nW.H. Wong and L. Ma. Optional P´olya tree and Bayesian inference. The Annals of Statis-\ntics, 38(3):1433–1459, 2010.\n21\nTziortztiotis et al.\nJ. Wyatt. Exploration and inference in learning from reinforcement. PhD thesis, University\nof Edinburgh. College of Science and Engineering. School of Informatics., 1998.\n22\nCover Tree Bayesian RL\n101\n102\n103\n101.8\n102\n102.2\n102.4\n# Steps\nMountainCar\n101\n102\n103\n0\n1,000\n2,000\n3,000\nPendulum\n(a) Oﬄine results\n100\n101\n102\n103\n0\n200\n400\n600\n800\n1,000\nNumber of Episodes (Rollouts)\n# Steps\nMountain car\n100\n101\n102\n103\n0\n1,000\n2,000\n3,000\nNumber of Episodes (Rollouts)\nPendulum\n(b) Online results\nCTBRL\nLBRL\nLSPI\nGPRL\nFigure 3: Experimental evaluation. The dashed line shows CTBRL, the dotted line shows\nLBRL, the solid line shows LSPI, while the dash-dotted line shows GPRL. The\nerror bars denote 95% conﬁdence intervals for the mean (i.e. statistical signiﬁ-\ncance). The shaded regions denote 90% percentile performance (i.e. robustness)\nacross runs. In all cases, CTBRL converges signiﬁcantly quicker than the other\napproaches. In addition, as the percentile regions show, it is also much more\nstable than LBRL, GPRL and LSPI.\n23\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2013-05-08",
  "updated": "2014-05-02"
}