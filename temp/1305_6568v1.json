{
  "id": "http://arxiv.org/abs/1305.6568v1",
  "title": "Reinforcement Learning for the Soccer Dribbling Task",
  "authors": [
    "Arthur Carvalho",
    "Renato Oliveira"
  ],
  "abstract": "We propose a reinforcement learning solution to the \\emph{soccer dribbling\ntask}, a scenario in which a soccer agent has to go from the beginning to the\nend of a region keeping possession of the ball, as an adversary attempts to\ngain possession. While the adversary uses a stationary policy, the dribbler\nlearns the best action to take at each decision point. After defining\nmeaningful variables to represent the state space, and high-level macro-actions\nto incorporate domain knowledge, we describe our application of the\nreinforcement learning algorithm \\emph{Sarsa} with CMAC for function\napproximation. Our experiments show that, after the training period, the\ndribbler is able to accomplish its task against a strong adversary around 58%\nof the time.",
  "text": "arXiv:1305.6568v1  [cs.LG]  28 May 2013\nReinforcement Learning for the Soccer Dribbling Task\nArthur Carvalho and Renato Oliveira\nAbstract—We propose a reinforcement learning solution to\nthe soccer dribbling task, a scenario in which a soccer agent\nhas to go from the beginning to the end of a region keeping\npossession of the ball, as an adversary attempts to gain possession.\nWhile the adversary uses a stationary policy, the dribbler learns\nthe best action to take at each decision point. After deﬁning\nmeaningful variables to represent the state space, and high-level\nmacro-actions to incorporate domain knowledge, we describe our\napplication of the reinforcement learning algorithm Sarsa with\nCMAC for function approximation. Our experiments show that,\nafter the training period, the dribbler is able to accomplish its\ntask against a strong adversary around 58% of the time.\nI. INTRODUCTION\nSoccer dribbling consists of the ability of a soccer agent\nto go from the beginning to the end of a region keeping\npossession of the ball, while an adversary attempts to gain\npossession. In this work, we focus on the dribbler’s learning\nprocess, i.e., the learning of an effective policy that determines\na good action for the dribbler to take at each decision point.\nWe study the soccer dribbling task using the RoboCup\nsoccer simulator [1]. Speciﬁc details of this simulator increase\nthe complexity of the learning process. For example, besides\nthe adversarial and real-time environment, agents’ perceptions\nand actions are noisy and asynchronous.\nWe model the soccer dribbling task as a reinforcement\nlearning problem. Our solution to this problem combines\nthe Sarsa algorithm with CMAC for function approximation.\nDespite the fact that the resulting learning algorithm is not\nguaranteed to converge to the optimal policy in all cases, many\nlines of evidence suggest that it converges to near-optimal\npolicies (for example, see [2], [3], [4], [5]).\nBesides this introductory section, the rest of this paper is\norganized as follows. In the next section, we describe the\nsoccer dribbling task. In Section 3, we show how to map this\ntask onto an episodic reinforcement learning framework. In\nSection 4 and 5, we present, respectively, the reinforcement\nlearning algorithm and its results against a strong adversary.\nIn Section 6, we review the literature related to our work. In\nSection 7, we conclude and present future research directions.\nII. SOCCER DRIBBLING\nSoccer dribbling is a crucial skill for an agent to become\na successful soccer player. It consists of the ability of a\nsoccer agent, henceforth called the dribbler, to go from the\nbeginning to the end of a region keeping possession of the\nball, while an adversary attempts to gain possession. We can\nsee soccer dribbling as a subproblem of the complete soccer\ndomain. The main simpliﬁcation is that the players involved\nare only focused on speciﬁc goals, without worrying about\nteam strategies or unrelated individual skills (e.g., passing and\nshooting). Nevertheless, a successful policy learned by the\ndribbler can be used in the complete soccer domain whenever\na soccer agent faces a dribbling situation.\nSince our focus is on the dribbler’s learning process, an\nomniscient coach agent is used to manage the play. At the\nbeginning of each trial (episode), the coach resets the location\nof the ball and of the players within a training ﬁeld. The\ndribbler is placed in the center-left region together with the\nball. The adversary is placed in a random position with the\nconstraint that it does not start with possession of the ball. An\nexample of a starting conﬁguration is shown in Figure 1.\nWhenever the adversary gains possession for a set period\nof time or when the ball goes out of the training ﬁeld by\ncrossing either the left line or the top line or the bottom line,\nthe coach declares the adversary as the winner of the episode.\nIf the ball goes out of the training ﬁeld by crossing the right\nline, then the winner is the ﬁrst player to intercept the ball.\nAfter declaring the winner of an episode, the coach resets the\nlocation of the players and of the ball within the training ﬁeld\nand starts a new episode. Thus, the dribbler’s goal is to reach\nthe right line that delimits the training ﬁeld with the ball. We\ncall this task the soccer dribbling task.\nWe argue that the soccer dribbling task is an excellent\nbenchmark for comparing different machine learning tech-\nniques since it involves a complex problem, and it has a\nwell-deﬁned objective, which is to maximize the number of\nepisodes won by the dribbler. We study the soccer dribbling\ntask using the RoboCup soccer simulator [1].\nThe RoboCup soccer simulator operates in discrete time\nsteps, each representing 100 milliseconds of simulated time.\nSpeciﬁc details of this simulator increase the complexity of the\nlearning process. For example, random noise is injected into\nall perceptions and actions. Further, agents must sense and act\nasynchronously. Each soccer agent receives visual information\nabout other objects every 150 milliseconds, e.g., its distance\nFig. 1.\nExample of a starting conﬁguration.\nfrom other players in its current ﬁeld of view. Each agent\nhas also a body sensor, which detects its current “physical\nstatus” every 100 milliseconds, e.g., that agent’s stamina\nand speed. Agents may execute a parameterized primitive\naction every 100 milliseconds, e.g., turn(angle), dash(power),\nand kick(power, angle). Full details of the RoboCup soccer\nsimulator are presented by Chen et al. [6].\nSince possession is not well-deﬁned in the RoboCup soccer\nsimulator, we consider that an agent has possession of the ball\nwhenever the ball is close enough to be kicked, i.e., it is in a\ndistance less than 1.085 meters from the agent.\nIII. THE SOCCER DRIBBLING TASK AS A\nREINFORCEMENT LEARNING PROBLEM\nIn the soccer dribbling task, an episode begins when the\ndribbler may take the ﬁrst action. When an episode ends\n(e.g., when the adversary gains possession for a set period\nof time), the coach starts a new one, thereby giving rise to a\nseries of episodes. Thus, the interaction between the dribbler\nand the environment naturally breaks down into a sequence\nof distinct episodes. This point, together with the fact that\nthe RoboCup soccer simulator operates in discrete time steps,\nallows the soccer dribbling task to be mapped onto a discrete-\ntime, episodic reinforcement-learning framework.\nRoughly speaking, reinforcement learning is concerned with\nhow an agent must take actions in an environment so as to\nmaximize the expected long-term reward [7]. Like in a trial-\nand-error search, the learner must discover which action is the\nmost rewarding one in a given state of the world. Thus, solving\na reinforcement learning problem means ﬁnding a function\n(policy) that maps states to actions so that it maximizes\na reward over the long run. As a way of incorporating\ndomain knowledge, the actions available to the dribbler are\nthe following high-level macro-actions, which are built on the\nsimulator’s primitive actions1:\n• HoldBall(): The dribbler holds the ball close to its body,\nkeeping it in a position that is difﬁcult for the adversary\nto gain possession;\n• Dribble(Θ, k): The dribbler turns its body towards the\nglobal angle Θ, kicks the ball k meters ahead of it, and\nmoves to intercept the ball.\nThe global angle Θ is in the range [0, 360]. In detail, the\ncenter of the training ﬁeld has been chosen as the origin of the\nsystem, where the zero-angle points towards the middle of the\nright line that delimits the training ﬁeld, and it increases in the\nclockwise direction. Those macro-actions are based on high-\nlevel skills used by the UvA Trilearn 2003 team [8]. The ﬁrst\none maps directly onto the primitive action kick. Consequently,\nit usually takes a single time step to be performed. The second\none, however, requires an extended sequence of the primitive\nactions turn, kick, and dash. To handle this situation, we treat\nthe soccer dribbling task as a semi-Markov decision process\n(SMDP) [9].\n1Henceforth, we use the terms action and macro-action interchangeably,\nwhile always distinguishing primitive actions.\nFormally, an SMDP is a 5-tuple < S, A, P, r, F >, where\nS is a countable set of states, A is a countable set of actions,\nP(s′|s, a), for s′, s ∈S, and a ∈A, is a probability distribu-\ntion providing the transition model between states, r(s, a) ∈ℜ\nis a reward associated with the transition (s, a), and F(τ|s, a)\nis a probability distribution indicating the sojourn time in a\ngiven state s ∈S, i.e., the time before transition provided that\naction a was taken in state s.\nLet ai ∈A be the ith macro-action selected by the dribbler.\nThus, several simulator’s time steps may elapse between ai\nand ai+1. Let si+1 ∈S and ri+1 ∈ℜbe, respectively, the\nstate and the reward following the macro-action ai. From the\ndribbler’s point of view, an episode consists of a sequence\nof SMDP steps, i.e., a sequence of states, macro-actions, and\nrewards: s0, a0, r1, s1, . . . , si, ai, ri+1, si+1, . . . , an−1, rn, sn,\nwhere ai is chosen based exclusively on the state si, and sn is\na terminal state in which either the adversary or the dribbler is\ndeclared the winner of the episode by the coach. In the former\ncase, the dribbler receives the reward rn = −1, while in the\nlatter case its reward is rn = 1. The intermediate rewards are\nalways equal to zero, i.e., r1 = r2 = · · · = rn−1 = 0. Thus,\nour objective is to ﬁnd a policy that maximizes the dribbler’s\nreward, i.e., the number of episodes in which it is the winner.\nA. Dribbler\nThe dribbler must take a decision at each SMDP step by\nselecting an available macro-action. Besides the macro-action\nHoldBall, the set of actions available to the dribbler contains\nfour instances of the macro-action Dribble: Dribble(30◦, 5),\nDribble(330◦, 5), Dribble(0◦, 5), and Dribble(0◦, 10). Thus,\nbesides hiding the ball from the adversary, the dribbler can\nkick the ball forward (strongly and weakly), diagonally up-\nward, and diagonally downward. If at some time step the\ndribbler has not possession of the ball and the current state\nis not a terminal state, then it usually means that the dribbler\nchose an instance of the macro-action Dribble before and it is\ncurrently moving to intercept the ball.\nWe turn now to the state representation used by the dribbler.\nIt consists of a set of state variables which are based on\ninformation related to the ball, the adversary, and the dribbler\nitself. Let ang(x) be the global angle of the object x, and\nang(x, y) and dist(x, y) be, respectively, the relative angle\nand the distance between the objects x and y. Further, let w\nand h be, respectively, the width and the height of the training\nﬁeld. Finally, let posY (x) be a function indicating whether the\nobject x is close to (less than 1 meter away from) the top line\nor the bottom line that delimits the training ﬁeld. In the former\ncase, posY (x) = 1, whereas in the latter case posY (x) = −1,\nand otherwise posY (x) = 0. Table 1 shows the state variables\ntogether with their ranges.\nThe ﬁrst three variables help the dribbler to locate itself\nand the adversary inside the training ﬁeld. Together, the last\ntwo variables can be seen as a point describing the position\nof the adversary in a polar coordinate system, where the ball\nis the pole. Thus, these variables are used by the dribbler to\nlocate the adversary with respect to the ball. It is interesting to\nTABLE I\nDESCRIPTION OF THE STATE REPRESENTATION.\nState Variable\nRange\nposY (dribbler)\n{−1, 0, 1}\nang(dribbler)\n[0, 360]\nang(dribbler, adversary)\n[0, 360]\nang(ball, adversary)\n[0, 360]\ndist(ball, adversary)\n[0,\n√\nw2 + h2]\nnote that a more informative state representation can be used\nby adding more state variables, e.g., the current speed of the\nball and the dribbler’s stamina. However, large domains can\nbe impractical due to the “curse of dimensionality”, i.e., the\ngeneral tendency of the state space to grow exponentially in\nthe number of state variables [10]. Consequently, we focus on\na state representation that is as concise as possible.\nB. Adversary\nThe adversary uses a ﬁxed, pre-speciﬁed policy. Thus, we\ncan see it as part of the environment in which the dribbler\nis interacting with. When the adversary has possession of the\nball, it tries to maintain possession for another time step by\ninvoking the macro-action HoldBall. If it maintains possession\nfor two consecutive time steps, then it is the winner of the\nepisode. When the adversary does not have the ball, it uses\nan iterative scheme to compute a near-optimal interception\npoint based on the ball’s position and velocity. Thereafter,\nthe adversary moves to that point as fast as possible. This\nprocedure is the same used by the dribbler when it is moving\nto intercept the ball after invoking the macro-action Dribble.\nMore details about this iterative scheme can be found in the\ndescription of the UvA Trilearn 2003 team [8].\nIV. THE REINFORCEMENT LEARNING ALGORITHM\nOur solution to the soccer dribbling task combines the rein-\nforcement learning algorithm Sarsa with CMAC for function\napproximation. In what follows, we brieﬂy introduce both of\nthem before presenting the ﬁnal learning algorithm.\nA. Sarsa\nThe Sarsa algorithm works by estimating the action-value\nfunction Qπ(s, a), for the current policy π and for all state-\naction pairs (s, a) [7]. The Q-function assigns to each state-\naction pair the expected return from it. Given a quintuple of\nevents, (st, at, rt+1, st+1, at+1), that make up the transition\nfrom the state-action pair (st, at) to the next one, (st+1, at+1),\nthe Q-value of the ﬁrst state-action pair is updated according\nto the following equation:\nQ(st, at) ←Q(st, at) + αδt,\n(1)\nwhere δt is the traditional temporal-difference error,\nδt = rt+1 + λQ(st+1, at+1) −Q(st, at),\n(2)\nα is the learning rate parameter, and λ is a discount rate gov-\nerning the weight placed on future, as opposed to immediate,\nrewards. Sarsa is an on-policy learning method, meaning that\nFig. 2.\nExample of two layers overlaid over a two-dimensional state space.\nAny input vector (state) activates two receptive ﬁelds, one from each layer.\nFor example, the state represented by the black dot activates the highlighted\nreceptive ﬁelds.\nit continually estimates Qπ, for the current policy π, and at the\nsame time changes π towards greediness with respect to Qπ.\nA typical policy derived from the Q-function is an ǫ-greedy\npolicy. Given the state st, this policy selects a random action\nwith probability ǫ and, otherwise, it selects the action with the\nhighest estimated value, i.e., a = argmaxa Q(st, a).\nB. CMAC\nIn tasks with a small number of state-action pairs, we can\nrepresent the action-value function Qπ as a table with one\nentry for each state-action pair. However, this is not the case of\nthe soccer dribbling task. For illustration’s sake, suppose that\nall variables in Table 1 are discrete. If we consider the 5 actions\navailable to the dribbler and a 20m x 20m training ﬁeld, we end\nup with more than 1.9×1010 state-action pairs. This would not\nonly require an unreasonable amount of memory, but also an\nenormous amount of data to ﬁll up the table accurately. Thus,\nwe need to generalize from previously experienced states to\nones that have never been seen. For dealing with this task, we\nuse a technique commonly known as function approximation.\nBy using a function approximation, the action-value func-\ntion Qπ is now represented as a parameterized functional\nform [7]. Now, whenever we make a change in one parameter\nvalue, we also change the estimated value of many state-action\npairs, thus obtaining generalization. In this work, we use the\nCerebellar Model Arithmetic Computer (CMAC) for function\napproximation [11], [12].\nCMAC works by partitioning the state space into multi-\ndimensional receptive ﬁelds, each of which is associated with\na weight. In this work, receptive ﬁelds are hyper-rectangles\nin the state space. Nearby states share receptive ﬁelds. Thus,\ngeneralization occurs between them. Multiple partitions of the\nstate space (layers) are usually used, which implies that any\ninput vector falls within the range of multiple excited receptive\nﬁelds, one from each layer.\nLayers are identical in organization, but each one is offset\nrelative to the others so that each layer cuts the state space in\na different way. By overlapping multiple layers, it is possible\nto achieve quick generalization while maintaining the ability\nto learn ﬁne distinctions. Figure 2 shows an example of two\ngrid-like layers overlaid over a two-dimensional space.\nThe receptive ﬁelds excited by a given state s make up the\nfeature set Fs, with each action a indexing their weights in a\ndifferent way. In other words, each macro-action is associated\nwith a particular CMAC. Clearly, the number of receptive\nﬁelds inside each feature set is equal to the number of layers.\nThe CMAC’s response to a feature set Fs is equal to the sum\nof the weights of the receptive ﬁelds in Fs. Formally, let θa(i)\nbe the weight of the receptive ﬁeld i indexed by the action\na. Thus, the CMAC’s response to Fs is equal to P\ni∈Fs θa(i),\nwhich represents the Q-value Q(s, a).\nCMAC is trained by using the traditional delta rule (also\nknown as the least mean square). In detail, after selecting an\naction a, the weight of an excited receptive ﬁeld i indexed by\na, θa(i), is updated according to the following equation:\nθa(i) ←θa(i) + αδ,\n(3)\nwhere δ is the temporal-difference error. A major issue when\nusing CMAC is that the total number of receptive ﬁelds\nrequired to span the entire state space can be very large.\nConsequently, an unreasonable amount of memory may be\nneeded. A technique commonly used to address this issue\nis called pseudo-random hashing [7]. It produces receptive\nﬁelds consisting of noncontiguous, disjoint regions randomly\nspread throughout the state space, so that only information\nabout receptive ﬁelds that have been excited during previous\ntraining is actually stored.\nC. Linear, Gradient-Descent Sarsa\nOur solution to the soccer dribbling task combines the Sarsa\nalgorithm with CMAC for function approximation. We use\nan ǫ-greedy policy for action selection. Sutton and Barto [7]\nprovide a complete description of this algorithm under the\nname of linear, gradient-descent Sarsa. Our implementation\nfollows the solution proposed by Stone et al. [13]. It consists\nof three routines: RLstartEpisode, to be run by the dribbler\nat the beginning of each episode; RLstep, run on each SMDP\nstep; and RLendEpisode, to be run when an episode ends. In\nwhat follows, we present each routine in detail.\n1) RLstartEpisode: Given an initial state s0, this routine\nstarts by iterating over all available actions. In line 2, it ﬁnds\nthe receptive ﬁelds excited by s0, which compose the feature\nset Fs0. Next, in line 3, the estimated value of each macro-\naction a in s0 is calculated as the sum of the weights of\nthe excited receptive ﬁelds. In line 5, this routine selects a\nmacro-action by following an ǫ-greedy policy and sends it to\nthe RoboCup soccer simulator. Finally, the chosen action and\nthe initial state s0 are stored, respectively, in the variables\nLastAction and LastState.\nAlgorithm 1 RLstartEpisode\n1: for each action a do\n2:\nFs0 ←receptive ﬁelds excited by s0\n3:\nQa ←P\ni∈Fs0 θa(i)\n4: end for\n5: LastAction ←\n\u001a\nargmaxa Qa\nw/ prob. 1 −ǫ\nrandom action\nw/ prob. ǫ\n6: LastState ←s0\n2) RLstep: This routine is run on each SMDP step, when-\never the dribbler has to choose a macro-action. Given the\ncurrent state s, it starts by calculating part of the temporal-\ndifference error (Equation 2), namely the difference between\nthe intermediate reward r and the expected return of the\nprevious SMDP step, QLastAction. In lines 2 to 5, this routine\nﬁnds the receptive ﬁelds excited by s and uses their weights to\ncompute the estimated value of each action a in s. In line 6,\nthe next action to be taken by the dribbler is selected according\nto an ǫ-greedy policy. In line 7, this routine ﬁnishes to\ncompute the temporal-difference error by adding the discount\nrate λ times the expected return of the current SMDP step,\nQCurrentAction. Next, in lines 8 to 10, this routine adjusts the\nweights of the receptive ﬁelds excited in the previous SMDP\nstep by the learning factor α times the temporal-difference\nerror δ (see Equation 3). Since the weights have changed,\nwe must recalculate the expected return of the current SMDP\nstep, QCurrentAction (line 11). Finally, the chosen action\nand the current state are stored, respectively, in the variables\nLastAction and LastState.\nAlgorithm 2 RLstep\n1: δ ←r −QLastAction\n2: for each action a do\n3:\nFs ←receptive ﬁelds excited by s\n4:\nQa ←P\ni∈Fs θa(i)\n5: end for\n6: CurrentAction ←\n\u001a\nargmaxa Qa\nw/ prob. 1 −ǫ\nrandom action\nw/ prob. ǫ\n7: δ ←δ + λQCurrentAction\n8: for each i ∈FLastState do\n9:\nθLastAction(i) ←θLastAction(i) + αδ\n10: end for\n11: QCurrentAction ←P\ni∈Fs θCurrentAction(i)\n12: LastAction ←CurrentAction\n13: LastState ←s\n3) RLendEpisode: This routine is run when an episode\nends. Initially, it calculates the appropriate reward based\non who won the episode. Next, it calculates the temporal-\ndifference error in the action-value estimates (line 6). There\nis no need to add the expected return of the current SMDP\nstep (QCurrentAction) since this value is deﬁned to be 0 for\nterminal states. Lastly, this routine adjusts the weights of the\nreceptive ﬁelds excited in the previous SMDP step.\nAlgorithm 3 RLendEpisode\n1: if the dribbler is the winner then\n2:\nr ←1\n3: else\n4:\nr ←−1\n5: end if\n6: δ ←r −QLastAction\n7: for each i ∈FLastState do\n8:\nθLastAction(i) ←θLastAction(i) + αδ\n9: end for\nV. EMPIRICAL RESULTS\nIn this section, we report our experimental results with\nthe soccer dribbler task. In all experiments, we used the\nstandard RoboCup soccer simulator (version 14.0.3, protocol\n9.3) and a 20m x 20m training region. In that simulator,\nagents typically have limited and noisy visual sensors. For\nexample, each player can see objects within a 90◦view cone,\nand the precision of an object’s sensed location degrades with\ndistance. To simplify the learning process, we removed those\nrestrictions. Both the dribbler and the adversary were given\n360◦of noiseless vision to ensure that they would always have\ncomplete and accurate knowledge of the environment.\nRelated to parameters of the reinforcement learning algo-\nrithm2, we set ǫ = 0.01, α = 0.125, and λ = 1. By no means\ndo we argue that these values are optimal. They were set based\non results of brief, informal experiments.\nThe weights of ﬁrst-time excited receptive ﬁelds were set\nto 0. The bounds of the receptive ﬁelds were set according to\nthe generalization that we desired: angles were given widths\nof about 20 degrees, and distances were given widths of\napproximately 3 meters. We used 32 layers. Each dimension of\nevery layer was offset from the others by 1/32 of the desired\nwidth in that dimension. We used the CMAC implementation\nproposed by Miller and Glanz [14], which uses pseudo-\nrandom hashing. To retain previously trained information in\nthe presence of subsequent novel data, we did not allow hash\ncollisions.\nTo create episodes as realistic as possible, agents were not\nallowed to recover their staminas by themselves. This task\nwas done by the coach after ﬁve consecutive episodes. This\nenabled agents to start episodes with different stamina values.\nWe ran this experiment 5 independent times, each one lasting\n50,000 episodes, and taking, on average, approximately 74\nhours. Figure 3 shows the histogram of the average number\nof episodes won by the dribbler during the training process.\nBins of 500 episodes were used.\nThroughout the training process, the dribbler won, on av-\nerage, 23, 607 episodes (≈47%). From Figure 3, we can see\nthat it greatly improves its average performance as the number\nof episodes increases. At the end of the training process, it is\nwinning slightly less than 53% of the time.\nQualitatively, the dribbler seems to learn two major rules. In\nthe ﬁrst one, when the adversary is at a considerable distance,\nthe dribbler keeps kicking the ball to the opposite side in\nwhich the adversary is located until the angle between them\nis in the range [90, 270], i.e., when the adversary is behind\nthe dribbler. After that, the dribbler starts to kick the ball\nforward. An illustration of this rule can be seen in Figure\n4.\nThe second rule seems to occur when the adversary is\nrelatively close to and in front of the dribbler. Since there is\nno way for the dribbler to move forward or diagonally without\nputting the possession at risk, it then holds the ball until the\n2The\nimplementation\nof\nthe\nlearning\nalgorithm\ncan\nbe\nfound\nat:\nhttp://sites.google.com/site/soccerdribbling/\nFig. 3.\nHistogram of the average number of episodes won by the dribbler\n(success) during the training process (50,000 episodes). Bins of 500 episodes\nwere used, and 5 independent simulations were performed.\nangle between it and the adversary is in the range [90, 270].\nThereafter, it starts to advance by kicking the ball forward. An\nillustration of this rule can be seen in Figure 5.\nAfter the training process, we randomly generated 10,000\ninitial conﬁgurations to test our solution. This time, the\ndribbler always selected the macro-action with the highest\nestimated value, i.e., we set ǫ = 0. Further, the weights of\nthe receptive ﬁelds were not updated, i.e., we set α = 0. We\nused the receptive ﬁelds’ weights resulting from the simulation\nwhere the dribbler obtained the highest success rate. The result\nof this experiment was even better. The dribbler won 5,795\nepisodes, thus obtaining a success rate of approximately 58%.\nFig. 4.\nExample of the ﬁrst major rule learned by the dribbler. (Top Left)\nThe adversary is at a considerable distance from the dribbler. (Top Right) The\ndribbler starts to kick the ball to the opposite side in which the adversary is\nlocated. (Bottom Left) The angle between the adversary and the dribbler is in\nthe range [90, 270]. Consequently, the dribbler starts to kick the ball forward.\n(Bottom Right) The dribbler keeps kicking the ball forward.\nFig. 5.\nExample of the second major rule learned by the dribbler. (Top Left)\nThe adversary is close to and in front of the dribbler. (Top Right) The dribbler\nholds the ball so as not to lose possession. (Bottom Left) The dribbler keeps\nholding the ball. (Bottom Right) The angle between the adversary and the\ndribbler is the range [90, 270]. Consequently, the dribbler starts to advance\nby kicking the ball forward.\nA. One-Dimensional CMACs\nFor comparison’s sake, we repeated the above experiment\nusing the original solution proposed by Stone et al. [13]. It\nconsists of the same learning algorithm presented in Section\n3, but using one-dimensional CMACs. In detail, each layer is\nan interval along a state variable. In this way, the feature set\nFs is now composed by 32 × 5 = 160 excited receptive ﬁelds,\ni.e., 32 excited receptive ﬁelds for each state variable.\nOne of the main advantages of using one-dimensional\nCMACs is that it is possible to circumvent the curse of dimen-\nsionality. In detail, the state space does not grow exponentially\nin the number of state variables because dependence between\nvariables is not taken into account.\nFigure 6 shows the histogram of the average number of\nepisodes won by the dribbler during the training process.\nEach simulation took, on average, approximately 43 hours.\nThroughout the training process, the dribbler won, on average,\n16, 278 episodes (≈33%). From Figure 6, we can see that the\nlearning algorithm converges much faster when using one-\ndimensional CMACs. However, its average performance is\nconsiderably worse. At the end of the training process, the\ndribbler is winning, on average, less than 30% of the time.\nAfter the training process, we tested this solution using\nthe same 10,000 initial conﬁgurations previously generated.\nAgain, we set ǫ = α = 0, and used the receptive ﬁelds’\nweights resulting from the simulation where the dribbler\nobtained the highest success rate. The result of this experiment\nwas slightly better. The dribbler won 3,701 episodes, thus\nFig. 6.\nHistogram of the average number of episodes won by the dribbler\n(success) during the training process (50,000 episodes) when using one-\ndimensional CMACs. Bins of 500 episodes were used, and 5 independent\nsimulations were performed.\nobtaining a success rate of approximately 37%.\nQualitatively, the dribbler seems to learn a rule similar to the\none shown in Figure 4. The major difference is that it always\nkicks the ball to the opposite side in which the adversary is\nlocated, it does not matter its distance from the adversary’s\nlocation. Consequently, it is highly unlikely that the dribbler\nsucceeds when the adversary is close to it.\nWe conjecture that one of the main reasons for such a poor\nperformance of the reinforcement learning algorithm when\nusing one-dimensional CMACs is that it does not take into\naccount dependence between variables, i.e., they are treated\nindividually. Hence, such approach may throw away valuable\ninformation. For example, the variables ang(ball, adversary)\nand dist(ball, adversary) together describe the position of\nadversary with respect to the ball. However, they do not make\nas much sense when considered individually.\nVI. RELATED WORK\nReinforcement learning has long been applied to the robot\nsoccer domain. For example, Andou [15] uses “observational\nreinforcement learning” to reﬁne a function that is used by\nthe soccer agents for deciding their positions on the ﬁeld.\nRiedmiller et al. [16] use reinforcement learning to learn\nlow-level soccer skills, such as kicking and ball-interception.\nNakashima et al. [17] propose a reinforcement learning meth-\nod called “fuzzy Q-learning”, where an agent determines its\naction based on the inference result of a fuzzy rule-based\nsystem. The authors apply the proposed method to the sce-\nnario where a soccer agent learns to intercept a passed ball.\nArguably, the most successful application is due to Stone\net al. [13]. They propose the “keepaway task”, which consists\nof two teams, the keepers and the takers, where the former\ntries to keep control of the ball for as long as possible, while\nthe latter tries to gain possession. Our solution to the soccer\ndribbling task follows closely the solution proposed by those\nauthors to learn the keepers’ behavior. Iscen and Erogul [18]\nuse similar solution to learn a policy for the takers.\nGabel et al. [19] propose a task which is the opposite of the\nsoccer dribbling task, where a defensive player must interfere\nand disturb the opponent that has possession of the ball. Their\nsolution to that task uses a reinforcement learning algorithm\nwith a multilayer neural network for function approximation.\nKalyanakrishnan et al. [20] present the “half-ﬁeld offense\ntask”, a scenario in which an offense team attempts to outplay\na defense team in order to shoot goals. Those authors pose\nthat task as a reinforcement learning problem, and propose a\nnew learning algorithm for dealing with it.\nMore closely related to our work are reinforcement learn-\ning-based solutions to the task of conducting the ball (e.g.,\n[21]), which can be seen as a simpliﬁcation of the dribbling\ntask since it usually does not include adversaries.\nVII. CONCLUSION\nWe proposed a reinforcement learning solution to the soccer\ndribbling task, a scenario in which an agent has to go from\nthe beginning to the end of a region keeping possession\nof the ball, while an adversary attempts to gain possession.\nOur solution combined the Sarsa algorithm with CMAC for\nfunction approximation. Empirical results showed that, after\nthe training period, the dribbler was able to accomplish its\ntask against a strong adversary around 58% of the time.\nAlthough we restricted ourselves to the soccer domain,\ndribbling, as deﬁned in this paper, is also common in other\nsports, e.g., hockey, basketball, and football. Thus, the pro-\nposed solution can be of value to dribbling tasks of other sports\ngames. Furthermore, we believe that the soccer dribbling task\nis an excellent benchmark for comparing different machine\nlearning techniques because it involves a complex problem,\nand it has a well-deﬁned objective.\nThere are several exciting directions for extending this\nwork. From a practical perspective, we intend to analyze\nthe scalability of our solution, i.e., to study how it performs\nwith training ﬁelds of distinct sizes and against different\nadversaries. Further, we are considering schemes to extend\nour solution to the original partially observable environment,\nwhere the available information is incomplete and noisy.\nAs stated before, a more informative state representation\ncould be obtained by using more state variables. The major\nproblem of adding extra variables to our solution is that\nCMAC’s complexity increases exponentially with its dimen-\nsionality. Due to this fact, we are considering other solutions\nwhich use function approximations whose complexity is unaf-\nfected by dimensionality per se, e.g., the Kanerva coding (for\nexample, see Kostiadis and Hu’s work [22]).\nFinally, we note that when modeling the soccer dribbling\ntask as a reinforcement learning problem, we do not directly\nuse intermediate rewards (they are all set to zero). However,\nthey may make the learning process more efﬁcient (for exam-\nple, see [23]). Thus, we intend to investigate the inﬂuence of\nintermediate rewards on the ﬁnal solution in future work.\nACKNOWLEDGMENTS\nWe would like to thank W. Thomas Miller, Filson H. Glanz,\nand others from the Department of Electrical and Computer\nEngineering at the University of New Hampshire for making\ntheir CMAC code available.\nREFERENCES\n[1] I. Noda, H. Matsubara, K. Hiraki, and I. Frank, “Soccer server: A tool for\nresearch on multiagent systems,” Applied Artiﬁcial Intelligence, vol. 12,\nno. 2, pp. 233–250, 1998.\n[2] G. J. Gordon, “Reinforcement learning with function approximation\nconverges to a region,” in Advances in Neural Information Processing\nSystems 13, 2001, pp. 1040–1046.\n[3] R. S. Sutton, “Generalization in reinforcement learning: Successful ex-\namples using sparse coarse coding,” in Advances in Neural Information\nProcessing Systems 8, 1996, pp. 1038–1044.\n[4] J. N. Tsitsiklis and B. V. Roy, “An analysis of temporal-difference\nlearning with function approximation,” IEEE Transactions on Automatic\nControl, vol. 42, no. 5, pp. 674–690, 1997.\n[5] T. J. Perkins and D. Precup, “A convergent form of approximate policy\niteration,” in Advances in Neural Information Processing Systems 15,\n2003, pp. 1595–1602.\n[6] M.\nChen,\nE.\nForoughi,\nF.\nHeintz,\nS.\nKapetanakis,\nK.\nKos-\ntiadis, J. Kummeneje, I. Noda, O. Obst, P. Riley, T. Steffens,\nY. Wang, and X. Yin, Users manual: RoboCup soccer server man-\nual for soccer server version 7.07 and later, 2003, available at\nhttp://sourceforge.net/projects/sserver/.\n[7] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.\nThe MIT Press, 1998.\n[8] R. de Boer and J. R. Kok, “The incremental development of a synthetic\nmulti-agent system: the uva trilearn 2001 robotic soccer simulation\nteam,” Master’s thesis, University of Amsterdam, The Netherlands, 2002.\n[9] M. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic\nProgramming.\nWiley, 2005.\n[10] R. Bellman, Dynamic Programming.\nDover Publications, 2003.\n[11] J. S. Albus, “A Theory of Cerebellar Function,” Mathematical Bio-\nsciences, vol. 10, no. 1-2, pp. 25–61, 1971.\n[12] ——, Brain, Behavior, and Robotics.\nByte Books, 1981.\n[13] P. Stone, R. S. Sutton, and G. Kuhlmann, “Reinforcement Learning for\nRoboCup-Soccer Keepaway,” Adaptive Behavior, vol. 13, no. 3, pp. 165–\n188, 2005.\n[14] W. T. Miller and F. H. Glanz, UNH CMAC Version 2.1: The\nUniversity\nof\nNew\nHampshire\nimplementation\nof\nthe\nCerebel-\nlar\nmodel\narithmetic\ncomputer\n-\nCMAC,\n1994,\navailable\nat\nhttp://www.ece.unh.edu/robots/cmac.htm.\n[15] T. Andou, “Reﬁnement of Soccer Agents’ Positions Using Reinforce-\nment Learning,” in RoboCup-97: Robot Soccer World Cup I, 1998, pp.\n373–388.\n[16] M. A. Riedmiller, A. Merke, D. Meier, A. Hoffman, A. Sinner, O. Thate,\nand R. Ehrmann, “Karlsruhe Brainstormers - A Reinforcement Learning\nApproach to Robotic Soccer,” in RoboCup 2000: Robot Soccer World\nCup IV, 2001, pp. 367–372.\n[17] T. Nakashima, M. Udo, and H. Ishibuchi, “A fuzzy reinforcement\nlearning for a ball interception problem,” in RoboCup 2003: Robot\nSoccer World Cup VII, 2004, pp. 559–567.\n[18] A. Iscen and U. Erogul, “A new perspective to the keepaway soccer:\nthe takers,” in Proceedings of the 7th international joint conference on\nAutonomous agents and multiagent systems, 2008, pp. 1341–1344.\n[19] T. Gabel, M. Riedmiller, and F. Trost, “A Case Study on Improving De-\nfense Behavior in Soccer Simulation 2D: The NeuroHassle Approach,”\nin RoboCup-2008: Robot Soccer World Cup XII, 2009, pp. 61–72.\n[20] S. Kalyanakrishnan, Y. Liu, and P. Stone, “Half ﬁeld offense in RoboCup\nsoccer: A multiagent reinforcement learning case study,” in RoboCup-\n2006: Robot Soccer World Cup X, 2007, pp. 72–85.\n[21] M. Riedmiller, R. Hafner, S. Lange, and M. Lauer, “Learning to dribble\non a real robot by success and failure,” in IEEE International Conference\non Robotics and Automation, 2008, pp. 2207–2208.\n[22] K. Kostiadis and H. Hu, “KaBaGe-RL: Kanerva-based generalisation\nand reinforcement learning for possession football,” in Proceedings\nof the IEEE/RSJ International Conference on Intelligent Robots and\nSystems, 2001, pp. 292–297.\n[23] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward\ntransformations: Theory and application to reward shaping,” in Proceed-\nings of the 16th International Conference on Machine Learning, 1999,\npp. 278–287.\n",
  "categories": [
    "cs.LG",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2013-05-28",
  "updated": "2013-05-28"
}