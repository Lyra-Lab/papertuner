{
  "id": "http://arxiv.org/abs/1306.2759v1",
  "title": "Horizontal and Vertical Ensemble with Deep Representation for Classification",
  "authors": [
    "Jingjing Xie",
    "Bing Xu",
    "Zhang Chuang"
  ],
  "abstract": "Representation learning, especially which by using deep learning, has been\nwidely applied in classification. However, how to use limited size of labeled\ndata to achieve good classification performance with deep neural network, and\nhow can the learned features further improve classification remain indefinite.\nIn this paper, we propose Horizontal Voting Vertical Voting and Horizontal\nStacked Ensemble methods to improve the classification performance of deep\nneural networks. In the ICML 2013 Black Box Challenge, via using these methods\nindependently, Bing Xu achieved 3rd in public leaderboard, and 7th in private\nleaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in\nprivate leaderboard.",
  "text": "Horizontal and Vertical Ensemble with Deep Representation for\nClassiﬁcation\nJingjing Xie\nxiejingjing113@gmail.com\nBing Xu\nantinucleon@gmail.com\nZhang Chuang\nzhangchuang@bupt.edu.cn\nBeijing University of Posts and Telecommunications, 10th Xitucheng Rd., Beijing, China, 100876\nAbstract\nRepresentation learning, especially which by\nusing deep learning, has been widely applied\nin classiﬁcation.\nHowever, how to use lim-\nited size of labeled data to achieve good clas-\nsiﬁcation performance with deep neural net-\nwork, and how can the learned features fur-\nther improve classiﬁcation remain indeﬁnite.\nIn this paper, we propose Horizontal Vot-\ning Vertical Voting and Horizontal Stacked\nEnsemble methods to improve the classiﬁca-\ntion performance of deep neural networks. In\nthe ICML 2013 Black Box Challenge, via us-\ning these methods independently, Bing Xu\nachieved 3rd in public leaderboard, and 7th\nin private leaderboard; Jingjing Xie achieved\n4th in public leaderboard, and 5th in private\nleaderboard.\n1. Introduction\nClassiﬁcation is one of the most important machine\nlearning tasks. Besides classiﬁcation algorithms, the\nperformance of classiﬁer is heavily dependent on the\nset of data representations on which they are applied.\nTraditionally, data representations are hand-crafted,\nwith prior knowledge or hypotheses of the human de-\nsigners. Then the classiﬁers with the designed repre-\nsentations (or features) are trained by ﬁtting the la-\nbeled data, expected to give a good class prediction\non test data inputs.\nHowever, the increasing size of data in real world and\nthe variety of learning tasks bring challenges to this\ntraditional paradigm. Practically, labeled data is rare,\nbut unlabeled data is always abundant.\nAlthough\nPresented at the ICML Workshop on Representation Learn-\ning, Atlanta, Georgia, USA, 2013 Copyright 2013 by the\nauthor(s).\nthere are some less expensive ways to obtain labels, au-\ntomatically learning representations from data would\nbe more eﬃcient.\nFurthermore, it has been proved\nthat in some ﬁelds automatic representation learning\ncan work better, even if human feature engineering\nis still powerful. In the ICML 2013 Black Box Chal-\nlenge1, both labeled and unlabeled data are provided\nto players without prior knowledge about what the\ndata really is. So we resort to deep learning and design\na deep neural network which consists 5 layers of de-\nnoising auto-encoder and 3 maxout layers, with more\nthan 16 million parameters in total. We use all the\n130 thousand unlabeled data to pre-train the stacked\ndenoising auto-encoders and ﬁne-tune the huge deep\nnetwork with only 1,000 training examples.\nAs the\ntask is data classiﬁcation, its natural to ask: how to\nuse so little labeled data to train a large deep network\nwith robust classiﬁcation result? Can the hierarchical\nrepresentations in the deep architecture help improve\nthe performance of classiﬁcation?\nIn this work, we describe our method of training deep\nneural networks for classiﬁcation with both labeled\nand unlabeled data. We also proposed three methods\ncalled Vertical Voting, Horizontal Voting and Horizon-\ntal Stacked Ensemble to improve the classiﬁcation ac-\ncuracy and robustness of deep network. Their perfor-\nmance and combination strategies are also discussed.\n2. Background\nA deep neural network applies combined transforma-\ntions to input data, and produces representations with\nan increasing level of abstraction and complexity. The\narchitecture of a deep neural network is drawn in Fig-\nure 1. Input data are processed in a deep architec-\nture of transformations, and generate desired output\nat the end. Usually, there are pre-training layers on\n1http://www.kaggle.com/c/challenges-in-\nrepresentation-learning-the-black-box-learning-challenge\narXiv:1306.2759v1  [cs.LG]  12 Jun 2013\nHorizontal and Vertical Ensemble with Deep Representation for Classiﬁcation\nInput data\noutput\npre-training layers h0 - h4\nsoftmax layer\nhidden layers\nmaxout layer h5\nmaxout layer h6\nmaxout layer h7\nFigure 1. The architecture of deep neural networks. In this\nexample, the deep network has 5 stacked auto-encoder lay-\ners (h0 - h4) which are represented by a single pre-training\nlayer for simplicity. Upon them are 3 maxout layers (h5 -\nh7) and a softmax layer.\nthe bottom of the architecture, which are built with\nRestricted Boltzmann Machine (RBM) (Smolensky,\n1986; Hinton et al., 2006) or auto-encoder (Le Cun,\n1987; Bourlard & Kamp, 1988; Hinton & Zemel, 1994)\nlayers. The layers above them such as sigmoid, tanh,\nand maxout (Goodfellow et al., 2013) layers, together\nwith pre-training layers, are collectively called hidden\nlayers. At the top is the softmax layer which produces\nprobabilities for each class as output.\nThe training of deep neural networks has two phases\n(Bengio, 2009). The ﬁrst phase is layer-wise unsuper-\nvised pre-training (Hinton et al., 2006; Bengio et al.,\n2007) which makes use of unlabeled data, adjusts the\nparameter of pre-training layers, and initializes the\ndeep neural network to a data-dependent manifold\n(Erhan et al., 2009). In the second phase, all param-\neters in the network are ﬁne-tuned under the super-\nvision of labeled data. The softmax layer on the top\nproduces the probabilities of each class for each ex-\nample. An alternative to obtain class prediction is to\ntrain a standard classiﬁer (such as Random Forest or\nSVM) with learned data representations (Bengio et al.,\n2012).\n3. Vertical Voting, Horizontal Voting\nand Horizontal Stacked Ensemble\nWe proposed a series of methods to improve the per-\nformance of classiﬁcation. These methods are Vertical\nVoting, Horizontal Voting and Horizontal Stacked En-\nAlgorithm 1 Vertical Voting\nInput: training data X, test data x, target y, neu-\nral network N, max epoch E, objective epoch e, se-\nlected hidden layers Ω= {ω1, . . . , ωn}, classiﬁcation\nalgorithm set A\nInitialize SGD trainer for N\nInitialize a list Preds = [ ]\nInitialize iteration = 0\nrepeat\nUse X and y to do one epoch back-propagation\ntraining on N\niteration = iteration + 1\nif iteration = e then\nInput X and x into N, get X and x’s rep-\nresentation pairs set R in each layer ωi ∈Ω:\nR = {(Xω1, xω1), . . . , (Xωn, xωn)}\nfor each pair rj ∈R do\nTrain classiﬁer c using an algorithm a ∈A,\nwith training data Xωj and y\nAdd c’s probabilistic prediction vector pωj on\nxωj to Preds\nend for\nend if\nuntil iteration > E\nPred = Ppωi∈P reds\npωi\npωi\nOutput: argmax(pred)\nsemble.\n3.1. Vertical Voting\nThe softmax layer generates predictions by using the\ntop level data representation. All the lower level rep-\nresentations are discarded. However, lower level rep-\nresentations of data may contribute to classiﬁcation\nthemselves. For example, word is a kind of low level\ndata representation. Some words can be strongly in-\ndicative for a topic, but they may lost when deep neu-\nral network parsing the sentence into a high level repre-\nsentation. To help classiﬁcation, we propose a method\ncalled Vertical Voting. This method ensembles a se-\nries of classiﬁers whose inputs are the representation\nof intermediate layers. A lower error rate is expected\nbecause these features seem diverse.\nThe procedure of Vertical Voting method is shown in\nAlgorithm 1.\n3.2. Horizontal Voting\nIf appropriate network architecture and learning rate\nare chosen, the error rate of classiﬁcation would ﬁrst\ndecline and then tend to be stable with the training\nepoch grows. But when size of labeled training set is\nHorizontal and Vertical Ensemble with Deep Representation for Classiﬁcation\ntoo small, the error rate would oscillate, as shown in\nthe validation set curve Figure 2. Although dropout\n(Hinton et al., 2012) helps a little, it is still overﬁt. So\nit is diﬃcult to choose a “magic” epoch to obtain a\nreliable output. To reduce the instability, we put for-\nward a method called Horizontal Voting. First, net-\nworks trained for a relatively stable range of epoch are\nselected. The predictions of the probability of each la-\nbel are produced by standard classiﬁers with top level\nrepresentation of the selected epoch, and then aver-\naged.\nThe procedure of Horizontal Voting is shown in Algo-\nrithm 2.\nFigure 2. Learning curve of a deep network. 90% of the\ntraining set is kept for training and 10% is for validation.\nAlgorithm 2 Horizontal Voting\nInput: training data X, test data x, target y, neu-\nral network N, max epoch E, selected epoch range\n(L, H)\nInitialize SGD trainer for N\nInitialize iteration = 0\nInitialize a list Preds = [ ]\nrepeat\nUse X and y to do one epoch back-propagation\ntraining on N\niteration = iteration + 1\nif iteration > L and iteration < H then\nPut x into N, get softmax output vector predi\nAdd predi to Preds\nend if\nuntil iteration > E\nPred = Ppredi∈P reds\npredi\npredi\nOutput: argmax(pred)\nAlgorithm 3 Horizontal Stacked Ensemble\nInput: training data X, test data x, target y, neu-\nral network N, max epoch E, selected epoch range\n(L, H), classiﬁcation algorithm set A\nInitialize SGD trainer for N\nInitialize iteration = 0\nInitialize a list Predsx = [ ]\nInitialize a list PredsX = [ ]\nrepeat\nUse X and y to do one epoch back-propagation\ntraining on N\niteration = iteration + 1\nif iteration > L and iteration < H then\nPut x into N, get softmax output vector predi\nAdd predi to Predsx\nPut X into N, get softmax output vector predi\nAdd predi to PredsX\nend if\nuntil iteration > E\nReshape H −L−1 softmax output vectors in PredX\nto a single feature vector FX into a single vector of\n(H −L −1) × num of classes dimension\nReshape H −L−1 softmax output vectors in Predx\nto a single feature vector Fx into a single vector of\n(H −L −1) × num of classes dimension\nTrain classiﬁer c by using an algorithm a ∈A, use\ntraining data FX and y\nPut Fx into c, get ﬁnal prediction Pred\nOutput: argmax(pred)\n3.3. Horizontal Stacked Ensemble\nSergey Yurgenson suggested a non-linear horizontal\nensemble method2 for shallow neural network, which\nhas signiﬁcantly improved the accuracy of classiﬁca-\ntion.\nThis method can be extended to deep neural\nnetworks. Similar to the horizontal voting method in\nsection 3.2, it takes the output of networks within a\ncontinuous range of epoch. The following step is sim-\nilar to Stacked Generalization method. All these out-\nputs are collected to form a new feature space for clas-\nsiﬁcation.\nThe\nprocedure\nof\nHorizontal\nStacked\nEnsemble\nmethod is shown in Algorithm 3.\n4. Models and Experiments\nThe Black Box dataset provided by ICML 2013 Rep-\nresentation Learning Challenge is used. This dataset\n2http://www.kaggle.com/c/challenges-in-\nrepresentation-learning-the-black-box-learning-\nchallenge/forums/t/4674/models?page=2\nHorizontal and Vertical Ensemble with Deep Representation for Classiﬁcation\nprovides 1,000 labeled training examples, 10,000 test\nexamples halved to public and private test, together\nwith 135,735 unlabeled data for algorithms to exploit.\nThe data is in 1,875 dimension, and is required to be\ndivided to 9 classes. No prior knowledge can help since\nthe data is human unreadable.\nThe deep neural network is chosen as the key for this\nchallenge, and 6 models are designed. Model 1 is a sin-\ngle traditional shallow neural network and model 2 is\na deep neural network. Model 2 is used as benchmark\nfor the following models to test the eﬃciency of our\nmethods, and explore the strategies of method combi-\nnation. The models are described below. All of the\nexperiments share same learning rate and momentum.\nMost of the models are trained with open-source tools\nTheano (Bergstra et al., 2010), PyLearn2 (Warde-\nFarley et al., 2011) and scikit-learn (Pedregosa et al.,\n2011).\n4.1. Model 1: Shallow neural network\n(without pre-training)\nModel 1 is a traditional shallow neural network with-\nout pre-training layers.\nThe 1000 labeled data is\nthe input of 3 maxout layers and a softmax layer.\nThe number of neurons is 1875(input)-1500-1500-1500-\n9(output).\n4.2. Model 2: Deep neural network with\nunsupervised pre-training\nModel 2(see Figure 1 for the architecture) add un-\nsupervised pre-training layers to model 1. 5 denois-\ning auto-encoder layers are used to take advantage of\nmore than 130 thousand unlabeled data. The number\nof neurons is 1875(input)-1500-1000-1500-1200-1500-\n1500-1500-1500-9(output).\n4.3. Model 3: Deep neural network with\nVertical Voting\nModel 3 adds Vertical Voting to Model 2 to test its\neﬀectiveness. Representations in the 3 maxout layers\n(h5-h7 in Figure 1) vote for the prediction.\n4.4. Model 4: Deep neural network with\nHorizontal Voting\nIn model 4, Horizontal Voting is introduced to the\nnetwork in model 2. Deep neural networks that are\ntrained from 651 to 850 epoch are averaged.\n4.5. Model 5: Deep neural network combined\nHorizontal and Vertical Voting\nModel 5 implements both Vertical Voting and Horizon-\ntal Voting based on Model 2. For every training epoch,\nRandom Forest’s prediction given by 3 hidden layers\n(h5-h7 in Figure 1) are Vertically Voted. The process\nis repeated over the network of 651 to 850 epoch, then\nthe 200 predictions are Horizontally combined.\n4.6. Model 6: Deep neural network with\nHorizontal Stacked Ensemble\nThe model 6 introduces Horizontal Stacked Ensemble\nto Model 2. Also, 200 networks which are trained for\n651 to 850 epoch are ensembled .\n5. Results and Discussion\nThis section shows the result of each model, and dis-\ncusses their performance. The gap in classiﬁcation ac-\ncuracy between model 1 and model 2 shows the con-\ntribution of pre-training layers. When compared with\nmodel 2, results of model 3 and model 4 may prove\nthe eﬀectiveness of the Vertical and Horizontal Voting\nrespectively. In model 5 we can test the performance\nof both Voting methods.\nModel 6 and model 4 use\ndiﬀerent ensemble method and their performances can\nbe compared.\n5.1. Model 1 and Model 2\nThe classiﬁcation of model 1 and model 2 are imple-\nmented with softmax function. The classiﬁcation accu-\nracy is shown in Table 3. Pre-training layers contribute\nto 20.19% and 19.09% score of model 2 in public and\nprivate test set respectively.\n5.2. Model 3\nFollowing the Vertical Voting method, Random For-\nest(with nestimates = 500) provides predictions for rep-\nresentations generated by hierarchical layers (h5-h7) of\nthe network. The classiﬁcation accuracy of each pre-\ndiction and the voted version are shown in the row\n1-2 of Table 1. Note that the representation in h7 is\nclassiﬁed by Random Forest here, while processed by\nsoftmax in model 2. The row 3-4 of Table 1 shows the\nperformance of Vertical Voting in another deep net-\nwork, where the top maxout layer (h7) is replaced by\na rectiﬁed linear layer.\nBy comparing columns of Table 1, it is observed that\nlower level representations do not play that well as\nhigher level ones as we may had foreseen. However, the\nvoted predictions in row 1-2 of Table 1 do not have the\nHorizontal and Vertical Ensemble with Deep Representation for Classiﬁcation\nhighest accuracy, though the case in row 3-4 meet our\nexpectation. We have three guesses that may be re-\nsponsible for this phenomenon. The ﬁrst guess is that\nrepresentations in diﬀerent layers of the same network\ndo not provide good feature diversity. The second is\nthat since overﬁtting exists in the deep network itself,\nensembling a series of such models may deteriorate the\nperformance. And the third is, adjusting the weight of\nvoting may lead to better result. Validations of these\nguesses are beyond the scope of this paper.\n5.3. Model 4\nTo obtain a smoother learning curve, we choose a\nlearning rate 0.025.\nThen following the Horizontal\nVoting method, a epoch range (650, 850] is chosen.\nThe classiﬁcation error rate statistic of the picked 200\nexamples is listed in Table 2, which indicates a big\noscillation.\nBut by voting the prediction of 200 ex-\namples, the risk brought by a badly chosen epoch is\ngreatly reduced.\nAs the Table 3 shows, the result of horizontally vot-\ning achieved 0.68220 in public test set, and 0.67240\nin private test set, which is the best score among our\nexperiments during the challenge. Improvements are\nalso achieved on networks of diﬀerent structure. That\nshows horizontal Voting method can eﬀectively pro-\nduce a better and more robust performance of a deep\nneural network.\nTable 2. Classiﬁcation error rate statistic of the 200 exam-\nples, calculated on validation set (10% of the training set).\nMin\nMax\nMean\nStandard Error\n0.309999\n0.439999\n0.375427\n0.024364\n5.4. Model 5\nModel 5 applies both Vertical and Horizontal Voting\nmethod to model 2.\nTable 3 lists the classiﬁcation\naccuracy of model 5, also make the performance of\neach model convenient to compare.\nAs we have observed in 5.3, the eﬀectiveness of Ver-\ntical Voting is not stable. The performance of model\n5 is also inﬂuenced, if compared with model 4.\nIn\npublic test set the accuracy improves a little, but in\nprivacy test set the accuracy decreases. On the other\nhand, compared with the result of model 3, model 5\nhas about 3.58% and 1.58% improvement, which is\nbrought by Horizontal Voting method.\nSerious overﬁtting is also observed in the model. The\ndiﬀerence of accuracy between public and private test\nset is 0.0162. So combining Vertical Voting and Hori-\nzontal Voting is not a appropriate strategy, besides it\ncosts much more computation resources.\n5.5. Model 6\nSergey Yurgenson suggests that his non-linear ensem-\nble method achieved a 10% improvement in his shallow\nnetwork. In Model 6, the random forest on 200 soft-\nmax output of deep neural network achieves accuracy\nof 0.68540 on public test set, and 0.67440 on private\ntest set. The improvement is 2.82% and 3.56% com-\npared to Model 2. and the method performs even bet-\nter than model 4. Similar results have been observed\nin our other networks using this method, which indi-\ncates that this ensemble method really learns some-\nthing from probability output of neural network and\nadjusts to a better output.\n6. Conclusion and Future Work\nWe focus on the classiﬁcation problem without prior\nknowledge to the data.\nUsing very limited number\nof labeled data and massive unlabeled data, we have\nachieved a good performance in ICML 2013 Black Box\nLeaning Challenge, by exploiting the power of deep\nneural networks.\nIn this work, we propose Vertical Voting, Horizontal\nVoting and Horizontal Stacked Ensemble method for\ndeep neural network and test performance of them. We\nﬁnd that hierarchical representation in diﬀerent layers\nmay not lead to a better classiﬁcation accuracy as ex-\npected. On the other hand, for representations in hor-\nizontal, both linear Horizontal Voting and Horizontal\nStacked Ensemble methods can robustly improve the\nperformance.\nIf we were provided with more knowledge about the\ndata or more labeled training sets, we could have done\nmore investigations and harvested deeper understand-\ning for the representations in hierarchy. This explo-\nration may be done on other datasets in the future.\nAcknowledgments\nThis research was supported by the Fundamental Re-\nsearch Funds for the Central Universities (Grant No.\n2012RC0129), Important National Science & Technol-\nogy Speciﬁc Projects (Grant No. 2011ZX03002-005-\n01), National Natural Science Foundation of China\n(Grant No.61273217), and 111 Project of China un-\nder Grant No. B08004. Also we thank Ian Goodfel-\nlow’s generous help in pylearn2-dev group. Finally we\nthank all participates in kaggle forum. You share ideas\nHorizontal and Vertical Ensemble with Deep Representation for Classiﬁcation\nTable 1. Classiﬁcation accuracy of Random Forest for layer h5-h7, and the voted result. Row 1-2 is for model 3, and row\n3-4 is for another deep neural network with Vertical Voting method. The best score of each experiment is in bold.\nRF for h5\nRF for h6\nRF for h7\nVoted\nAccuracy(public test set)\n0.62440\n0.66140\n0.66240\n0.65920\nAccuracy(private test set)\n0.62800\n0.65760\n0.65480\n0.65620\nAccuracy(public test set)\n0.63800\n0.67220\n0.67000\n0.67240\nAccuracy(private test set)\n0.62760\n0.65380\n0.65720\n0.65960\nTable 3. Classiﬁcation accuracy of model 1-6. The best scores are in bold.\nModel 1\nmodel 2\nmodel 3\nmodel 4\nmodel 5\nmodel 6\nAccuracy(public test set)\n0.55460\n0.66660\n0.65920\n0.68220\n0.68280\n0.68540\nAccuracy(private test set)\n0.54680\n0.65120\n0.65620\n0.67240\n0.66660\n0.67440\nkindly, which will help everyone in the future.\nReferences\nBengio, Yoshua.\nLearning deep architectures for ai.\nFoundations and Trends R⃝in Machine Learning, 2\n(1):1–127, 2009.\nBengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and\nLarochelle, Hugo. Greedy layer-wise training of deep\nnetworks. Advances in neural information process-\ning systems, 19:153, 2007.\nBengio, Yoshua, Courville, Aaron, and Vincent, Pas-\ncal. Representation learning: A review and new per-\nspectives. arXiv preprint arXiv:1206.5538, 2012.\nBergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric,\nLamblin,\nPascal,\nPascanu,\nRazvan,\nDesjardins,\nGuillaume, Turian, Joseph, Warde-Farley, David,\nand Bengio, Yoshua. Theano: a cpu and gpu math\nexpression compiler. In Proceedings of the Python\nfor Scientiﬁc Computing Conference (SciPy), vol-\nume 4, 2010.\nBourlard, Herv´e and Kamp, Yves. Auto-association by\nmultilayer perceptrons and singular value decompo-\nsition. Biological cybernetics, 59(4-5):291–294, 1988.\nErhan, Dumitru, Manzagol, Pierre-Antoine, Bengio,\nYoshua, Bengio, Samy, and Vincent, Pascal. The\ndiﬃculty of training deep architectures and the ef-\nfect of unsupervised pre-training. In Proceedings of\nThe Twelfth International Conference on Artiﬁcial\nIntelligence and Statistics (AISTATS09), pp. 153–\n160. Citeseer, 2009.\nGoodfellow,\nIan J, Warde-Farley,\nDavid,\nMirza,\nMehdi, Courville, Aaron, and Bengio, Yoshua. Max-\nout networks.\narXiv preprint arXiv:1302.4389v3,\n2013.\nHinton, Geoﬀrey E and Zemel, Richard S. Autoen-\ncoders, minimum description length, and helmholtz\nfree energy. Advances in neural information process-\ning systems, pp. 3–3, 1994.\nHinton, Geoﬀrey E, Osindero, Simon, and Teh, Yee-\nWhye. A fast learning algorithm for deep belief nets.\nNeural computation, 18(7):1527–1554, 2006.\nHinton, Geoﬀrey E, Srivastava, Nitish, Krizhevsky,\nAlex, Sutskever, Ilya, and Salakhutdinov, Rus-\nlan R.\nImproving neural networks by preventing\nco-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580, 2012.\nLe\nCun,\nYann.\nMod`eles\nconnexionnistes\nde\nl’apprentissage. PhD thesis, 1987.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel,\nV., Thirion, B., Grisel, O., Blondel, M., Pretten-\nhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,\nPassos, A., Cournapeau, D., Brucher, M., Perrot,\nM., and Duchesnay, E. Scikit-learn: Machine learn-\ning in Python.\nJournal of Machine Learning Re-\nsearch, 12:2825–2830, 2011.\nSmolensky, Paul. Information processing in dynamical\nsystems: Foundations of harmony theory. 1986.\nWarde-Farley,\nDavid,\nGoodfellow,\nIan,\nLamblin,\nPascal, Desjardins, Guillaume, Bastien, Fr´ed´eric,\nand Bengio, Yoshua.\npylearn2, 2011.\nhttp://\ndeeplearning.net/software/pylearn2.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2013-06-12",
  "updated": "2013-06-12"
}