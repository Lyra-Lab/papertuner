{
  "id": "http://arxiv.org/abs/1307.3941v1",
  "title": "Reinforcement and inference in cross-situational word learning",
  "authors": [
    "Paulo F. C. Tilles",
    "Jose F. Fontanari"
  ],
  "abstract": "Cross-situational word learning is based on the notion that a learner can\ndetermine the referent of a word by finding something in common across many\nobserved uses of that word. Here we propose an adaptive learning algorithm that\ncontains a parameter that controls the strength of the reinforcement applied to\nassociations between concurrent words and referents, and a parameter that\nregulates inference, which includes built-in biases, such as mutual\nexclusivity, and information of past learning events. By adjusting these\nparameters so that the model predictions agree with data from representative\nexperiments on cross-situational word learning, we were able to explain the\nlearning strategies adopted by the participants of those experiments in terms\nof a trade-off between reinforcement and inference. These strategies can vary\nwildly depending on the conditions of the experiments. For instance, for fast\nmapping experiments (i.e., the correct referent could, in principle, be\ninferred in a single observation) inference is prevalent, whereas for\nsegregated contextual diversity experiments (i.e., the referents are separated\nin groups and are exhibited with members of their groups only) reinforcement is\npredominant. Other experiments are explained with more balanced doses of\nreinforcement and inference.",
  "text": "Reinforcement and inference in cross-situational word learning\nPaulo F. C. Tilles and Jos´e F. Fontanari\nInstituto de F´ısica de S˜ao Carlos, Universidade de S˜ao Paulo,\nCaixa Postal 369, 13560-970 S˜ao Carlos, S˜ao Paulo, Brazil\nCross-situational word learning is based on the notion that a learner can determine the referent\nof a word by ﬁnding something in common across many observed uses of that word.\nHere we\npropose an adaptive learning algorithm that contains a parameter that controls the strength of\nthe reinforcement applied to associations between concurrent words and referents, and a parameter\nthat regulates inference, which includes built-in biases, such as mutual exclusivity, and information\nof past learning events. By adjusting these parameters so that the model predictions agree with\ndata from representative experiments on cross-situational word learning, we were able to explain\nthe learning strategies adopted by the participants of those experiments in terms of a trade-oﬀ\nbetween reinforcement and inference. These strategies can vary wildly depending on the conditions\nof the experiments. For instance, for fast mapping experiments (i.e., the correct referent could, in\nprinciple, be inferred in a single observation) inference is prevalent, whereas for segregated contextual\ndiversity experiments (i.e., the referents are separated in groups and are exhibited with members\nof their groups only) reinforcement is predominant. Other experiments are explained with more\nbalanced doses of reinforcement and inference.\nI.\nINTRODUCTION\nA desirable goal of a psychological theory is to oﬀer ex-\nplanations grounded on elementary principles to the data\navailable from psychology experiments [13].\nAlthough\nmost of these quantitative psychological data is related\nto mental chronometry, recent explorations on the human\nperformance to acquire an artiﬁcial lexicon in controlled\nlaboratory conditions have paved the way to the under-\nstanding of the learning strategies humans use to infer a\nword-object mapping [10, 11, 17, 22, 23]. These exper-\niments are based on the cross-situational word-learning\nparadigm which avers that a learner can determine the\nmeaning of a word by ﬁnding something in common\nacross all observed uses of that word [9, 14]. In that sense,\nlearning takes place through the statistical sampling of\nthe contexts in which a word appears in accord with the\nclassical associationist stance of Hume and Locke that\nthe mechanism of word learning is sensitivity to covaria-\ntion - if two events occur at the same time, they become\nassociated [3].\nIn a typical cross-situational word-learning experi-\nment, participants are exposed repeatedly to multiple\nunfamiliar objects concomitantly with multiple spoken\npseudo-words, such that a word and its correct refer-\nent (object) always appear together on a learning trial.\nDiﬀerent trials exhibiting distinct word-object pairs will\neventually allow the disambiguation of the word-object\nassociations and the learning of the correct mapping [22].\nIt should be pointed out, however, that this scenario does\nnot describe the actual word learning process by children\neven in the unambiguous situation where the single novel\nobject is followed by the utterance of its corresponding\npseudo-word. In fact, young children will only make the\nconnection between the object and the word provided\nthey have a reason to believe that they are in presence\nof an act of naming and for this the speaker has to be\npresent [2, 3, 21]. Adults could learn those associations\neither because they were previously instructed by the ex-\nperimenter that they would be learning which words go\nwith which objects or because they could infer that the\ndisembodied voice is an act of naming by a concealed per-\nson. Nevertheless, the sort of statistical learning assumed\nin the cross-situational word-learning scenario may play\na key role in the development of pidgin [7] and it is likely\nthe sole option for the autonomous learning of a lexicon\nby robots [5, 18].\nIn order to learn a word-object mapping within\nthe cross-situational word-learning scenario the learner\nshould be able to (i) recall at least a fraction of the word-\nobject pairings that appeared in the learning trials, (ii)\nregister both co-occurrences and non-co-occurrences of\nwords and objects and (iii) apply the mutual exclusiv-\nity principle which favors the association of novel words\nto novel objects [12]. Although a variety of rule-based\nand associative learning algorithms has been proposed to\nmodel cross-situational word learning [4, 6, 8, 11, 15, 16,\n19], none of them explicitly take into account processes\nthat regulate all three above-mentioned criteria.\nIn this paper we oﬀer an adaptive learning algorithm\nthat comprises two parameters which regulate the as-\nsociative reinforcement of pairings between concurrent\nwords and objects, and the non-associative inference pro-\ncess that handles built-in biases (e.g., mutual exclusivity)\nas well as information of past learning events. By setting\nthe values of these parameters so as to ﬁt a representa-\ntive selection of experimental data presented in Kacher-\ngis et al. [10, 11] we are able to identify and explain the\nlearning strategies adopted by the participants of those\nexperiments in terms of a trade-oﬀbetween reinforcement\nand inference.\narXiv:1307.3941v1  [q-bio.NC]  15 Jul 2013\n2\nII.\nCROSS-SITUATIONAL LEARNING\nSCENARIO\nWe\nassume\nthere\nare\nN\nobjects\no1, . . . , oN,\nN\nwords w1, . . . , wN and a one-to-one mapping between\nwords\nand\nobjects\nrepresented\nby\nthe\nset\nΓ\n=\n{(w1, o1) , . . . , (wN, oN)}. At each learning trial, C word-\nobject pairs are selected from Γ and presented to the\nlearner without providing any clue on which word goes\nwith which object. For instance, pictures of the C objects\nare displayed in a slide while C pseudo-words are spoken\nsequentially such that their spatial and temporal arrange-\nments do not give away the correct word-object associa-\ntions [10, 22]. We refer to the subset of words and their\nreferents (objects) presented to the learner in a learn-\ning trial as the context Ω= {w1, o1, w2, o2, ..., wC, oC}.\nThe context size C is then a measure of the within-trial\nambiguity, i.e., the number of co-occurring word-object\npairs per learning trial.\nThe selection procedure from\nthe set Γ, which may favor some particular subsets of\nword-object pairs, determines the diﬀerent experimental\nsetups discussed in this paper. Although each individual\ntrial is highly ambiguous, repetition of trials with par-\ntially overlapping contexts should in principle allow the\nlearning of the N word-object associations.\nAfter the training stage is completed, which typically\ncomprises about two dozen trials, the learning accuracy\nis measured by instructing the learner to pick the object\namong the N objects on display which the learner thinks\nis associated to a particular target word. The test is re-\npeated for all N words and the average learning accuracy\ncalculated as the fraction of correct guesses [10].\nThis cross-situational learning scenario does not ac-\ncount for the presence of noise, such as the eﬀect of out-\nof-context words. This situation can be modeled by as-\nsuming that there is a certain probability (noise) that\nthe referent of one of the spoken words is not part of the\ncontext (so that word can be said to be out of context).\nAlthough theoretical analysis shows that there is a max-\nimum noise intensity beyond which statistical learning is\nunattainable [20], as yet no experiment was carried out\nto verify the existence of this threshold phenomenon on\nthe learning performance of human subjects.\nIII.\nMODEL\nWe model learning as a change in the conﬁdence with\nwhich the algorithm (or, for simplicity, the learner) as-\nsociates the word wi to an object oj that results from\nthe observation and analysis of the contexts presented in\nthe learning trials.\nMore to the point, this conﬁdence\nis represented by the probability Pt (wi, oj) that wi is\nassociated to oj at learning trial t. This probability is\nnormalized such that P\noj Pt (wi, oj) = 1 for all wi and t,\nwhich then implies that when the word wi is presented to\nthe learner in the testing stage the learning accuracy is\ngiven simply by Pt (wi, oi). In addition, we assume that\nPt (wi, oj) contains information presented in the learning\ntrials up to and including trial t only.\nIf at learning trial t the learner observes the con-\ntext Ωt = {w1, o1, w2, o2, ..., wC, oC} then it can infer\nthe existence of two other informative sets.\nFirst, the\nset of the words (and their referents) that appear for\nthe ﬁrst time at trial t, which we denote by ˜Ωt =\n\b\n˜w1, ˜o1, ˜w2, ˜o2, ..., ˜w ˜\nC, ˜o ˜\nCt\n\t\n. Clearly, ˜Ωt ⊆Ωt and ˜Ct ≤\nC. Second, the set of words (and their referents) that do\nnot appear in Ωt but that have already appeared in the\nprevious trials, ¯Ωt = { ¯w1, ¯o1, ..., ¯wNt−C, ¯oNt−C} where\nNt is the total number of diﬀerent words that appeared in\ncontexts up to and including trial t. Clearly, ¯Ωt∩Ωt = ∅.\nThe update rule of the conﬁdences Pt (wi, oj) depends on\nwhich of these three sets the word wi and the object oj\nbelong to (if i ̸= j they may belong to diﬀerent sets).\nIn fact, our learning algorithm comprises a parameter\nχ ∈[0, 1] that measures the associative reinforcement ca-\npacity and applies only to known words that appear in\nthe current context, and a parameter β ∈[0, 1] that mea-\nsures the inference capacity and applies either to known\nwords that do not appear in the current context or to\nnew words in the current context. Before the experiment\nbegins (t = 0) we set P0 (wi, oj) = 0 for all words wi and\nobjects oj. Next we describe how the conﬁdences are up-\ndated following the sequential presentation of contexts.\nIn the ﬁrst trial (t = 1) all words are new ( ˜C1 = N1 =\nC), so we set\nP1 ( ˜wi, ˜oj) = 1\nC\n(1)\nfor ˜wi, ˜oj ∈˜Ω= Ω. In the second or in an arbitrary trial\nt we expect to observe contexts exhibiting both novel and\nrepeated words. Novel words must go through an infer-\nence preprocessing stage before the reinforcement proce-\ndure can be applied to them. This is so because if ˜wi\nappears for the ﬁrst time at trial t then Pt−1 ( ˜wi, oj) = 0\nfor all objects oj and since the reinforcement is propor-\ntional to Pt−1 ( ˜wi, oj) the conﬁdences associated to ˜wi\nwould never be updated (see eq. (5) and the explanation\nthereafter). Thus when a novel word ˜wi appear at trial\nt ≥2, we redeﬁne its conﬁdence values at the previous\ntrial (originally set to zero) as\nPt−1 ( ˜wi, ˜oj) =\nβ\n˜Ct\n+\n1 −β\nNt−1 + ˜Ct\n,\n(2)\nPt−1 ( ˜wi, oj) =\n1 −β\nNt−1 + ˜Ct\n,\n(3)\nPt−1 ( ˜wi, ¯oj) =\n1 −β\nNt−1 + ˜Ct\n.\n(4)\nOn the one hand, setting the inference parameter β to\nits maximum value β = 1 enforces the mutual exclusivity\nprinciple which requires that the new word ˜wi be asso-\nciated with equal probability to the ˜Ct new objects ˜oj\nin the current context. Hence in the case ˜Ct = 1 the\nmeaning of the new word would be inferred in a single\n3\npresentation. On the other hand, for β = 0 the new word\nis associated with equal probability to all objects already\nseen up to an including trial t, i.e., Nt = Nt−1 + ˜Ct.\nIntermediate values of β describe a situation of imper-\nfect inference. Note that using eqs. (2)-(4) we can eas-\nily verify that P\n˜oj Pt−1 ( ˜wi, ˜oj) + P\noj Pt−1 ( ˜wi, oj) +\nP\n¯oj Pt−1 ( ˜wi, ¯oj) = 1, in accord with the normalization\nconstraint.\nNow we can focus on the update rule of the conﬁdence\nPt (wi, oj) in the case both word wi and object oj appear\nin the context at trial t. The rule applies both to re-\npeated and novel words, provided the conﬁdences of the\nnovel words are preprocessed according to eqs. (2)-(4).\nIn order to fulﬁll automatically the normalization condi-\ntion for word wi, the increase of the conﬁdence Pt (wi, oj)\nwith oj ∈Ωt must be compensated by the decrease of the\nconﬁdences Pt (wi, ¯oj) with ¯oj ∈¯Ωt. This can be imple-\nmented by distributing evenly the total ﬂux of probability\nout of the latter conﬁdences, i.e., P\n¯oj∈¯Ωt Pt−1 (wi, ¯oj),\nover the conﬁdences Pt (wi, oj) with oj ∈Ωt. Hence the\nnet gain of conﬁdence on the association between wi and\noj is given by\nrt−1 (wi, oj) = χPt−1 (wi, oj)\nP\n¯oj∈¯Ωt Pt−1 (wi, ¯oj)\nP\noj∈Ωt Pt−1 (wi, oj)\n(5)\nwhere, as mentioned before, the parameter χ ∈[0, 1]\nmeasures the strength of the reinforcement process. Note\nthat if both oj and ok appear in the context together\nwith wi then the reinforcement procedure should not cre-\nate any distinction between the associations (wi, oj) and\n(wi, ok). This result is achieved provided that the ratio\nof the conﬁdence gains equals the ratio of the conﬁdences\nbefore reinforcement, i.e., rt−1 (wi, oj) /rt−1 (wi, ok) =\nPt−1 (wi, oj) /Pt−1 (wi, ok). This is the reason that the\nreinforcement gain of a word-object association given\nby eq. (5) is proportional to the previous conﬁdence\non that association.\nThe total increase in the conﬁ-\ndences between wi and the objects that appear in the\ncontext, i.e. P\noj∈Ωt rt−1 (wi, oj), equals the product of\nχ and the total decrease in the conﬁdences between wi\nand the objects that do not appear in the context, i.e.\nP\n¯oj∈¯Ωt Pt−1 (wi, ¯oj). So for χ = 1 the conﬁdences asso-\nciated to objects absent from the context are fully trans-\nferred to the conﬁdences associated to objects present in\nthe context. Lower values of χ allows us to control the\nﬂow of conﬁdence from objects in ¯Ωt to objects in Ωt.\nMost importantly, in order to implement the reinforce-\nment process the learner should be able to use the in-\nformation about the previous trials, which is condensed\non the conﬁdence values Pt (wi, oj).\nThe eﬃciency on\nthe usage of this information is quantiﬁed by the word\nand trial dependent quantity αt (wi) ∈[0, 1] that allows\nfor the interpolation between the cases of perfect usage\n(αt (wi) = 1) and complete neglect (αt (wi) = 0) of the\ninformation stored in the conﬁdences Pt (wi, oj). In par-\nticular, we assume that the greatest the certainty on the\nassociation between word wi and its referent, the more\neﬃciently that information is used by the learner.\nA\nquantitative measure of the uncertainty associated to the\nconﬁdences regarding word wi is given by the entropy\nHt (wi) = −\nX\noj∈Ωt∪¯Ωt\nPt (wi, oj) log [Pt (wi, oj)]\n(6)\nwhose maximum (log Nt) is obtained by the uniform dis-\ntribution Pt (wi, oj) = 1/Nt for all oj ∈Ωt ∪¯Ωt, and\nwhose minimum (0) by Pt (wi, oj) = 1 and Pt (wi, ok) = 0\nfor ok ̸= oj. So we deﬁne\nαt (wi) = α0 + (1 −α0)\n\u0014\n1 −Ht (wi)\nlog Nt\n\u0015\n,\n(7)\nwhere α0 ∈[0, 1] is a baseline eﬃciency corresponding to\nthe maximum uncertainty about the referent of a target\nword.\nFinally, recalling that at trial t the learner has access\nto the sets Ωt, ¯Ωt as well as to the conﬁdences at trial\nt −1 we write the update rule\nPt (wi, oj) = Pt−1 (wi, oj) + αt−1 (wi) rt−1 (wi, oj)\n+ [1 −αt−1 (wi)]\n\u0014 1\nNt\n−Pt−1 (wi, oj)\n\u0015\n(8)\nfor wi, oj ∈Ωt. Note that if αt−1 (wi) = 0 the learner\nwould associate word wi to all objects that have appeared\nup to and including trial t with equal probability. This\nsituation happens only if α0 = 0 and if there is complete\nuncertainty about the referent of word wi.\nNow we consider the update rule for the conﬁdence\nPt (wi, ¯oj) in the case that word wi appears in the context\nat trial t but object ¯oj does not. (We recall that object ¯oj\nmust have appeared in some previous trial.) According\nto the reasoning that led to eq. (5) this conﬁdence must\ndecrease by the amount χPt−1 (wi, ¯oj) and so, taking into\naccount the information eﬃciency factor, we obtain\nPt (wi, ¯oj) = Pt−1 (wi, ¯oj) −αt−1 (wi) χPt−1 (wi, ¯oj)\n+ [1 −αt−1 (wi)]\n\u0014 1\nNt\n−Pt−1 (wi, ¯oj)\n\u0015\n(9)\nwhich can be easily seen to satisfy the normalization\nX\noj∈Ωt\nPt (wi, oj) +\nX\n¯oj∈¯Ωt\nPt (wi, ¯oj) = 1.\n(10)\nWe focus now on the update rule for the conﬁdence\nPt ( ¯wi, ¯oj) with ¯wi, ¯oj ∈¯Ωt, i.e., both the word ¯wi and\nthe object ¯oj are absent from the context shown at trial t,\nbut they have already appeared, not necessarily together,\nin previous trials. A similar inference reasoning that led\nto the expressions for the preprocessing of new words\nwould allow the learner to conclude that a word absent\nfrom the context should be associated to an object that is\nalso absent from it. In that sense, conﬁdence should ﬂow\nfrom the associations between ¯wi and objects oj ∈Ωt to\nthe associations between ¯wi and objects ¯oj ∈¯Ωt. Hence,\n4\nignoring the information eﬃciency factor for the moment,\nthe net gain to conﬁdence Pt ( ¯wi, ¯oj) is given by\n¯rt−1 ( ¯wi, ¯oj) = βPt−1 ( ¯wi, ¯oj)\nP\noj∈Ωt Pt−1 ( ¯wi, oj)\nP\n¯oj∈¯Ωt Pt−1 ( ¯wi, ¯oj).\n(11)\nThe direct proportionality of this gain to Pt−1 ( ¯wi, ¯oj) can\nbe justiﬁed by an argument similar to that used to justify\neq. (5) in the case of reinforcement.\nThe information\neﬃciency issue is also handled in a similar manner so the\ndesired update rule reads\nPt( ¯wi, ¯oj) = Pt−1( ¯wi, ¯oj) + αt−1( ¯wi) ¯rt−1( ¯wi, ¯oj)\n+ [1 −αt−1( ¯wi)]\n\u0014 1\nNt\n−Pt−1( ¯wi, ¯oj)\n\u0015\n(12)\nfor ¯wi, ¯oj ∈¯Ωt. To ensure normalization the conﬁdence\nPt ( ¯wi, oj) must decrease by an amount proportional to\nβPt−1 ( ¯wi, oj) so that\nPt( ¯wi, oj) = Pt−1( ¯wi, oj) −αt−1( ¯wi) βPt−1( ¯wi, oj)\n+ [1 −αt−1( ¯wi)]\n\u0014 1\nNt\n−Pt−1( ¯wi, oj)\n\u0015\n(13)\nfor ¯wi ∈¯Ωt and oj ∈Ωt. We can verify that prescriptions\n(12) and (13) satisfy the normalization\nX\n¯oj∈¯Ωt\nPt ( ¯wi, ¯oj) +\nX\noj∈Ωt\nPt ( ¯wi, oj) = 1,\n(14)\nas expected.\nIn summary, before any trial (t = 0) we set all con-\nﬁdence values to zero, i.e. P0 (wi, oj) = 0, and ﬁx the\nvalues of the parameters α0, χ and β. In the ﬁrst trial\n(t = 1) we set the conﬁdences of the words and ob-\njects in Ω1 according to eq. (1), so we have the values\nof P1 (wi, oj) for wi, oj ∈Ω1.\nIn the second trial, we\nseparate the novel words ˜wi ∈˜Ω2 and reset P1 ( ˜wi, oj)\nwith oi ∈Ω2 ∪¯Ω2 according to eqs. (2)-(4). Only then\nwe calculate α1 (wi) with wi ∈Ω1 ∪˜Ω2 using eq. (7).\nThe conﬁdences at trial t = 2 then follows from eqs.\n(8), (9), (12) and (13). As before, in the third trial we\nseparate the novel words ˜wi ∈˜Ω3, reset P2 ( ˜wi, oj) with\noi ∈Ω3 ∪¯Ω3 according to eqs. (2)-(4), calculate α2 (wi)\nwith wi ∈Ω1∪Ω2∪˜Ω3 using eq. (7), and only then resume\nthe evaluation of the conﬁdences at trial t = 3. This pro-\ncedure is repeated until the training stage is completed,\nsay, at t = t∗. At this point, knowledge of the conﬁdence\nvalues Pt∗(wi, oj) allows us to answer any question posed\nin the testing stage.\nIn the next section we evaluate the adequacy of this al-\ngorithm to describe a selection of cross-situational word-\nlearning experiments carried out on adult subjects by\nKachergis et al. [10, 11].\nIV.\nRESULTS\nThe cross-situational word-learning experiments of\nKachergis et al. [10, 11] aimed to understand how word\nsampling frequency (i.e., number of trials a word ap-\npears), contextual diversity (i.e., the co-occurrence of\ndistinct words or groups of words in the learning trials),\nwithin-trial ambiguity (i.e., the context size C), and fast-\nmapping of novel words aﬀect the learning performance\nof adult subjects. In this section we compare the per-\nformance of the algorithm described in the previous sec-\ntion with the performance of adult subjects reported in\nKachergis et al. [10, 11]. In particular, once the condi-\ntions of the training stage are speciﬁed, we carry out 104\nruns of our algorithm for ﬁxed values of the three param-\neters α0, β, χ, and then calculate the average accuracy\nat trial t = t∗over all those runs for that parameter set-\nting. Since the algorithm is deterministic, what changes\nin each run is the composition of the contexts at each\nlearning trial. As our goal is to model the results of the\nexperiments, we search the space of parameters to ﬁnd\nthe setting such that the performance of the algorithm\nmatches that of humans within the error bars (i.e., one\nstandard deviation) of the experiments.\nA.\nWord sampling frequency\nIn these experiments the number of words (and ob-\njects) is N = 18 and the training stage totals t∗= 27\nlearning trials, with each trial comprising the presenta-\ntion of 4 words together with their referents (C = 4).\nFollowing Kachergis et al. [10], we investigate two con-\nditions which diﬀer with respect to the number of times\na word is exhibited in the training stage.\nIn the two-\nfrequency condition, the 18 words are divided into two\nsubsets of 9 words each. The words in the ﬁrst subset\nappear 9 times and those in the second only 3 times. In\nthe three-frequency condition, the 18 words are divided\ninto three subsets of 6 words each. Words in the ﬁrst\nsubset appear 3 times, in the second, 6 times and in the\nthird, 9 times. In these two conditions, the same word\nwas not allowed to appear in two consecutive learning\ntrials.\nFigures 1 and 2 summarize our main results for the\ntwo-frequency and three-frequency conditions, respec-\ntively. The left panels show the regions (shaded areas)\nin the (χ, β) plane for ﬁxed α0 where the algorithm de-\nscribes the experimental data. The middle panels show\nthe accuracy of the best ﬁt as function of the parameter\nα0 and the right panels exhibit the values of χ and β cor-\nresponding to that ﬁt. The broken horizontal lines and\nthe shaded zones around them represent the means and\nstandard deviations of the results of experiments carried\nout with 33 adult subjects [10].\nIt is interesting that although the words sampled more\nfrequently are learned best in the two-frequency condi-\ntion as expected, this advantage practically disappears\nin the three-frequency condition in which case all words\nare learned at equal levels within the experimental error.\nNote that the average accuracy for the words sampled 3\ntimes is actually greater than the accuracy for the words\n5\nà\nààà\nààààà\nààààà\nààààà\nààààà\nààà\nàà\nàà\nààà\nààààà\nààààààà\nàààààààà\nààààààà\nààààààà\nàààààà\nààààà\nààà\nà\nà\nàà\nàààà\nàààààà\nààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààà\nààààà\nàààà\nà\nà\nààà\nààààà\nààààààà\nàààààààà\nàààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nàààààààà\nààààà\nàààà\nà\nàà\nàààà\nààààà\nààààààà\nààààààààà\nààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààà\nàààààààà\nàààààà\nààààà\nàà\nà\nàààà\nààààà\nààààààà\nàààààààà\nààààààààààà\nàààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nààààààààààà\nàààààààà\nààààààà\nàààà\nààà\nà\nà\nààà\nààààà\nàààààà\nààààààààà\nàààààààààà\nàààààààààààà\nààààààààààààà\nààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààà\nààààààà\nàààà\nààà\nà\nà\nààà\nààààà\nàààààà\nààààààà\nààààààààà\nààààààààààà\nàààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nàààààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nàààààààà\nàààààààà\nààààààà\nàààà\nàà\nà\nΑ0=0.98\nΑ0=0.94\nΑ0=0.9\nΑ0=0.86\nΑ0=0.82\nΑ0=0.78\nΑ0=0.74\nΑ0=0.7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΧ\nΒ\n0.6\n0.7\n0.8\n0.9\n1.0\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nΑ0\nAccuracy\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nΧ\nΒ\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nΧ,Β\nFIG. 1: Summary of the results for the two-frequency condition experiment. Left panel: Regions in the plane (χ, β) where the\nalgorithm ﬁts the experimental data for ﬁxed α0 as indicated in the ﬁgure. Middle panel: Average accuracy for the best ﬁt to\nthe experimental results of Kachergis et al. [10] represented by the broken horizontal lines (means) and shaded regions around\nthem (one standard deviation). The blue symbols represent the accuracy for the group of words sampled 9 times whereas the\nred symbols represent the accuracy for the words sampled 3 times. Right panel: Parameters χ and β corresponding to the best\nﬁt shown in the middle panel. The other parameters are N = 18 and C = 4.\nàà\nàààà\nàààà\nààààà\nààààà\nààà\nàààà\nààààà\nààààààà\nààààààà\nàààààààà\nàààààà\nàààà\nà\nà\nàààà\nààààà\nààààààà\nàààààààà\nàààààààà\nàààààààààà\nàààààààààà\nàààààààà\nàààààà\nààààà\nàà\nàà\nààà\nàààààà\nààààààà\nààààààààà\nàààààààààà\nàààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nàààààààà\nàààààà\nàààà\nà\nàà\nàààà\nàààààà\nàààààààà\nàààààààààà\nààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nààààààààààààà\nààààààààààà\nàààààààààà\nààààààà\nààààà\nààà\nàà\nàà\nàààà\nààààà\nàààààààà\nààààààààà\nààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nààààààààààààà\nàààààààààààà\nààààààààààààà\nàààààààààà\nàààààààà\nààààààà\nàààà\nààà\nààà\nàààà\nàààààà\nààààààà\nààààààààà\nààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààà\nààààà\nààà\nà\nààà\nàààà\nààààà\nàààààà\nàààààà\nààààà\nààààà\nààààà\nàààà\nàààà\nàààà\nàààà\nàààà\nààà\nààà\nààà\nààà\nàà\nà\nΑ0=0.98\nΑ0=0.94\nΑ0=0.9\nΑ0=0.86\nΑ0=0.82\nΑ0=0.78\nΑ0=0.74\nΑ0=0.7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΧ\nΒ\n0.6\n0.7\n0.8\n0.9\n1.0\n0.32\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\nΑ0\nAccuracy\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nΧ\nΒ\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nΧ,Β\nFIG. 2: Summary of the results for the three-frequency condition experiment. Left panel: Regions in the plane (χ, β) where\nthe algorithm ﬁts the experimental data for ﬁxed α0 as indicated in the ﬁgure. Middle panel: Average accuracy for the best\nﬁt to the experimental results of Kachergis et al. [10] represented by the broken horizontal lines (means) and shaded regions\naround them (one standard deviation). The blue symbols represent the accuracy for the group of words sampled 9 times, the\ngreen symbols for the words sampled 6 times, and the red symbols for the words sampled 3 times. Right panel: Parameters χ\nand β corresponding to the best ﬁt shown in the middle panel. The other parameters are N = 18 and C = 4.\nsampled 6 times, but this inversion is not statistically sig-\nniﬁcant, although, most surprisingly, the algorithm does\nreproduce it for α0 ∈[0.7, 0.8]. According to Kachergis\net al. [10], the reason for the observed sampling frequency\ninsensitivity might be because the high-frequency words\nare learned quickly and once they are learned subsequent\ntrials containing those words will exhibit an eﬀectively\nsmaller within-trial ambiguity. In this vein, the inversion\ncould be explained if by chance the words less frequently\nsampled were generally paired with the highly sampled\nwords. Thus contextual diversity seems to play a key role\nin cross-situational word learning.\nB.\nContextual diversity and within-trial ambiguity\nIn the ﬁrst experiment aiming to probe the role of con-\ntextual diversity in the cross-situational learning, the 18\nwords were divided in two groups of 6 and 12 words each,\nand the contexts of size C = 3 were formed with words\nbelonging to the same group only. Since the sampling\nfrequency was ﬁxed to 6 repetitions for each word, those\nwords belonging to the more numerous group are ex-\nposed to a larger contextual diversity (i.e., the variety\nof diﬀerent words with which a given word appear in the\ncourse of the training stage). The results summarized in\nFigure 3 indicate clearly that contextual diversity en-\nhances the learning accuracy.\nPerhaps more telling is\n6\nà\nà\nà\nà\nà\nààà\nà\nààà\nàààà\nàà\nà\nàà\nàà\nàà\nà\nà\nà\nà\nà\nàààà\nààà\nàààà\nà\nàààà\nààà\nà\nààà\nààààà\nàà\nàààà\nàà\nààà\nàà\nààà\nàà\nàà\nàà\nà\nàà\nà\nà\nà\nà\nà\nà\nàààà\nà\nàààààààà\nàà\nàààààààà\nà\nààààààà\nàààààààà\nà\nààààà\nà\nàà\nààààà\nààààààà\nàà\nààààà\nàààààà\nààààà\nàà\nàà\nàààààà\nààààà\nààààà\nà\nàààà\nààààà\nàààà\nàààà\nàà\nàààà\nà\nà\nàààà\nàà\nààà\nà\nà\nàà\nà\nà\nà\nà\nà\nààààà\nàà\nààààà\nà\nààààà\nà\nàààààààà\nààààà\nàààààà\nà\nààààààà\nàààààà\nààààà\nàààà\nà\nàààà\nà\nàààààà\nà\nàààà\nààààà\nà\nààààà\nà\nàà\nàà\nààà\nàà\nà\nà\nà\nààà\nà\nà\nà\nà\nà\nàà\nà\nàà\nà\nà\nà\nà\nà\nà\nààààà\nà\nààààààà\nà\nàààà\nàà\nààà\nà\nààààà\nàààà\nààà\nàà\nààà\nà\nàà\nààà\nàà\nΑ0=0.9\nΑ0=0.86\nΑ0=0.82\nΑ0=0.78\nΑ0=0.74\nΑ0=0.7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΧ\nΒ\n0.6\n0.7\n0.8\n0.9\n1.0\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nΑ0\nAccuracy\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nΧ\nΒ\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nΧ,Β\nFIG. 3:\nSummary of the results of the two-level contextual diversity experiment. Left panel: Regions in the plane (χ, β)\nwhere the algorithm ﬁts the experimental data for ﬁxed α0 as indicated in the ﬁgure. Middle panel: Average accuracy for\nthe best ﬁt to the experimental results of Kachergis et al. [10] represented by the broken horizontal lines (means) and shaded\nregions around them (one standard deviation). The blue symbols represent the accuracy for the group of words belonging to the\n12-components subgroup and the red symbols for the words belonging to the 6-components subgroup. All words are repeated\nexactly 6 times during the t∗= 27 learning trials. Right panel: Parameters χ and β corresponding to the best ﬁt shown in the\nmiddle panel. The other parameters are N = 18 and C = 3.\nà\nààà\nàààà\nàààààà\nàààààààà\nààààààààà\nààààààààà\nàààààààà\nàààààààà\nàààààààà\nààààààà\nàààààààà\nààààààà\nààààààà\nàààààà\nààààààà\nàààààà\nà\nàà\nàààà\nàààààà\nàààààààà\nàààààààààà\nàààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààààà\nàààààààà\nààààààààà\nàààààààà\nàààààààà\nà\nààà\nààààà\nààààààà\nààààààààà\nààààààààààà\nààààààààààààà\nààààààààààààààà\nààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nààààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nààààààààà\nààààààààà\nà\nàààà\nàààààà\nàààààààà\nàààààààààà\nààààààààààààà\nààààààààààààààà\nààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààààà\nààààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nààààààààààààààààààààààààà\nààààààààààààààààààààààààà\nààààààààààààààààààààààààà\nààààààààààààààààààààààààà\nàààààààààààààààààààààààà\nààààààààààààààààààààààà\nààààààààààààààààààààààà\nààààààààààààààààààààààà\nààààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nààààààààà\nààà\nààààà\nàààààààà\nàààààààààà\nààààààààààààà\nààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààààà\nààààààààààààààààààààààààà\nààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààà\nààààààààààààààààààààààààààà\nààààààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nààààààààààààààààààààààààà\nààààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nààààààààààààààààààààààà\nàààààààààààààààààààààààà\nààààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nààààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààà\nààààààààà\nàààààààà\nààààààà\nàààààà\nààààà\nàààà\nàààà\nààà\nàà\nàà\nà\nà\nà\nààà\nàààààà\nààààààààà\nàààààààààààà\nààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààà\nààààààààààààààààààààààààààààà\nàààààààààààààààààààààààààààà\nààààààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nàààààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nààààààààààààààààààààààà\nàààààààààààààààààààààà\nààààààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nàààààààààààà\nààààààààààà\nààààààààà\nàààààààà\nààààààà\nàààààà\nààààà\nàààà\nààà\nàà\nà\nà\nΑ0=0.98\nΑ0=0.94\nΑ0=0.9\nΑ0=0.86\nΑ0=0.82\nΑ0=0.78\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΧ\nΒ\nà\nàà\nàààà\nààààà\nàààààà\nààààà\nààààà\nààààà\nààààà\nàààà\nààààà\nàààà\nàà\nàààà\nààààà\nààààààà\nààààààà\nàààààààà\nàààààààà\nààààààà\nààààààà\nààààààà\nàààààà\nààààààà\nàààààà\nàààààà\nàààààà\nàààààà\nàààààà\nààààà\nàààààà\nààààà\nà\nààà\nààààà\nààààààà\nàààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nààààààààà\nààààààààà\nàààààààà\nààààààààà\nààààààààà\nàààààààà\nààààààààà\nàààààààà\nàààààààà\nààààààà\nàààààààà\nààààààà\nààààààà\nààààààà\nàààààà\nààààààà\nàààààà\nàààààà\nà\nààà\nààààà\nààààààà\nààààààààà\nàààààààààà\nàààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààààà\nààààààààà\nàààààààà\nààààààààà\nàààààààà\nàààààààà\nààààààà\nàààààààà\nààààààà\nààààààà\nàà\nààà\nààààà\nààààààà\nààààààààà\nààààààààààà\nààààààààààààà\nààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nàààààààààà\nàààààààààà\nààààààààà\nààààààààà\nàààààààà\nàààààààà\nààààààààà\nàààààààà\nààààààà\nààààààà\nàà\nàààà\nàààààà\nàààààààà\nàààààààààà\nàààààààààààà\nàààààààààààààà\nàààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nàààààààààààààà\nàààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààààà\nàààààààà\nàààààààà\nàààààààà\nàààààààà\nàààààààà\nààààààà\nàààààà\nàààààà\nààààà\nààààà\nààààà\nà\nààà\nààààà\nàààààààà\nàààààààààà\nàààààààààààà\nàààààààààààààà\nàààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nàààààààààààààààààààààààà\nààààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nàààààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààààà\nààààààààààààààààààààà\nàààààààààààààààààààà\nàààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nààààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nààààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nàààààààààààààààà\nààààààààààààààà\nààààààààààààààà\nàààààààààààààà\nàààààààààààààà\nààààààààààààà\nààààààààààààà\nàààààààààààà\nàààààààààààà\nààààààààààà\nààààààààààà\nàààààààààà\nààààààààà\nààààààààà\nàààààààà\nààààààà\nàààààà\nààààà\nàààà\nààà\nàà\nàà\nà\nà\nà\nΑ0=0.98\nΑ0=0.94\nΑ0=0.9\nΑ0=0.86\nΑ0=0.82\nΑ0=0.78\nΑ0=0.74\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΧ\nΒ\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.2\n0.3\n0.4\n0.5\n0.6\nΑ0\nAccuracy\nFIG. 4: Summary of the results of the experiments where all words co-occur without constraint and the N = 18 words are\nrepeated exactly 6 times during the t∗= 27 learning trials. Left panel: Regions in the plane (χ, β) where the algorithm ﬁts\nthe experimental data for ﬁxed α0 and context size C = 3. Middle panel: Same as the left panel but for context size C = 4.\nRight panel: Average accuracy for the best ﬁt to the experimental results of Kachergis et al. [10] represented by the broken\nhorizontal lines (means) and shaded regions around them (one standard deviation). The red symbols are for C = 3 and the\nblue symbols for C = 4.\nthe ﬁnding that incorrect responses are largely due to\nmisassignments to referents whose words belong to the\nsame group of the test word. In particular, Kachergis\net al. [10] found that this type of error accounts for 56%\nof incorrect answers when the test word belongs to the\n6-components subgroup and for 76% when it belongs to\nthe 12-components subgroup. The corresponding statis-\ntics for our algorithm with the optimal parameters set\nat α0 = 0.9 are 43% and 70%, respectively. The region\nin the space of parameters where the model can be said\nto describe the experimental data is greatly reduced in\nthis experiment and even the best ﬁt is barely within\nthe error bars.\nIt is interesting that, contrasting with\nthe previous experiments, in this case the reinforcement\nprocedure seems to play the more important role in the\nperformance of the algorithm.\nThe eﬀect of the context size or within-trial ambiguity\nis addressed by the experiment summarized in Figure\n4, which is similar to the previous experiment, except\nthat the words that compose the context are chosen uni-\nformly from the entire repertoire of N = 18 words. Two\ncontext sizes are considered, namely, C = 3 and C = 4.\nIn both cases, there is a large selection of parameter val-\nues that explain the experimental data, yielding results\nindistinguishable from the experimental average accura-\ncies. This is the reason we do not exhibit a graph akin to\n7\nthose shown in the right panels of the previous ﬁgures.\nSince a perfect ﬁtting can be obtained both for χ > β\nand for χ < β, this experiment is uninformative with re-\nspect to these two abilities. As expected, increase of the\nwithin-trial ambiguity diﬃcilitate learning. In addition,\nthe (experimental) results for C = 3 yield a learning ac-\ncuracy value that is intermediary to those measured for\nthe 6 and 12-components subgroups, which is in agree-\nment with the conclusion that the increase of the contex-\ntual diversity enhances learning, since the mean number\nof diﬀerent co-occurring words is 4.0 in the 6-components\nsubgroup, 9.2 in the 12-components subgroup and 8.8 in\nthe uniformly mixed situation [10].\nC.\nFast mapping\nThe experiments carried out by Kachergis et al. [11]\nwere designed to elicit participants’ use of the mutual\nexclusivity principle (i.e., the assumption of one-to-one\nmappings between words and referents) and to test the\nﬂexibility of a learned word-object association when new\nevidence is provided in support to a many-to-many map-\nping. To see how mutual exclusivity implies fast mapping\nassume that a learner who knows the association (w1, o1)\nis exposed to the context Ω= {w1, o1, w2, o2} in which\nthe word w2 (and its referent) appears for the ﬁrst time.\nThen it is clear that a mutual-exclusivity-biased learner\nwould infer the association (w2, o2) in this single trial.\nHowever, a purely associative learner would give equal\nweights to o1 and o2 if asked about the referent of w2.\nIn the speciﬁc experiment we address in this section,\nN = 12 words and their referents are split up into two\ngroups of 6 words each, say A = {(w1, o1) , . . . , (w6, o6)}\nand B = {(w7, o7) , . . . , (w12, o12)}. The context size is\nset to C = 2 and the training stage is divided in two\nphases. In the early phase, only the words belonging to\ngroup A are presented and the duration of this phase is\nset such that each word is repeated 3, 6 or 9 times. In\nthe late phase, the contexts consist of one word belonging\nto A and one belonging to B forming ﬁxed couples, i.e.,\nwhenever wi appears in a context, wi+6, with i = 1, . . . , 6,\nmust appear too. The duration of the late phase depends\non the number of repetitions of each word that can be 3,\n6, or 9 as in the early phase [11]. The combinations of the\nsampling frequencies yield 9 diﬀerent training conditions\nbut here we will consider only the case that the late phase\ncomprises 6 repetitions of each word.\nThe testing stage comprises the play of a single word,\nsay w1, and the display of 11 of the 12 trained objects\n[11]. Each word was tested twice with a time lag between\nthe tests: once without its corresponding early object (o1\nin the case) and once without its late object (o7 in the\ncase). This procedure requires that we renormalize the\nconﬁdences for each test. For instance, in the case o1 is\nleft out of the display, the renormalization is\nP ′\nt∗(w1, oj) = Pt∗(w1, oj) /\nX\nok̸=o1\nPt∗(w1, ok)\n(15)\nwith j = 2, . . . , 12 so that P\noj̸=o1 P ′\nt∗(w1, oj) = 1. Sim-\nilarly, in the case o7 is left out the renormalization be-\ncomes\nP ′\nt∗(w1, oj) = Pt∗(w1, oj) /\nX\nok̸=o7\nPt∗(w1, ok)\n(16)\nwith j = 1, . . . , 6, 8, . . . , 12 so that P\noj̸=o7 P ′\nt∗(w1, oj) =\n1. We are interested on the (renormalized) conﬁdences\nP ′\nt∗(w1, o1), P ′\nt∗(w1, o7), P ′\nt∗(w7, o7), and P ′\nt∗(w7, o1),\nwhich are shown in Figures 5 and 6 for the conditions\nwhere words wi, i = 1, . . . , 6 are repeated 3 (left panel),\n6 (middle panel), and 9 (right panel) times in the early\nlearning phase, and the words wi, i = 1, . . . , 12 are re-\npeated 6 times in the late phase. The ﬁgures exhibit the\nperformance of the algorithm for the set of parameters\nχ and β that ﬁts best the experimental data of Kacher-\ngis et al. [11] for ﬁxed α0. This optimum set is shown\nin Figure 7 for the 6 early repetition condition, which\nis practically indistinguishable from the optima of the\nother two conditions. The conditions with the diﬀerent\nword repetitions in the early phase intended to produce\ndistinct conﬁdences on the learned association (w1, o1)\nbefore the onset of the late phase in the training stage.\nThe insensitivity of the results to these conditions prob-\nably indicates that association was already learned well\nenough with 3 repetitions only. Finally, we note that,\nthough the testing stage focused on words w1 and w7\nonly, all word pairs wi and wi+6 with i = 1, . . . , 6 are\nstrictly equivalent since they appear the same number of\ntimes during the training stage.\nThe experimental results exhibited in Figure 6 of-\nfer indirect evidence that the participants have resorted\nto mutual exclusivity to produce their word-object map-\npings. In fact, from the perspective of a purely associa-\ntive learner, word w7 should be associated to objects o1 or\no7 only, but since in the testing stage one of those objects\nwas not displayed, such a learner would surely select the\ncorrect referent. However, the ﬁnding that P ′\nt∗(w7, o7)\nis considerably greater than P ′\nt∗(w7, o1) (they should be\nequal for an associative learner) indicates that there is a\nbias against the association (w7, o1) which is motivated,\nperhaps, from the previous understanding that o1 was\nthe referent of word w1.\nIn fact, a most remarkable\nresult revealed by Figure 6 is that P ′\nt∗(w7, o7) < 1.\nSince word w7 appeared only in the late phase context\nΩ= {w1, o1, w7, o7} and object o1 was not displayed in\nthe testing stage, we must conclude that the participants\nproduced spurious associations between words and ob-\njects that never appeared together in a context.\nOur\nalgorithm accounts for these associations through eq. (4)\nin the case of new words and, more importantly, through\neqs. (9) and (13) due to the eﬀect of the information\neﬃciency factor αt (wi). The experimental data is well\ndescribed only in the narrow range α0 ∈[0.85, 0.9].\n8\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nAccuracy\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nAccuracy\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nAccuracy\nFIG. 5: Results of the experiments on mutual exclusivity in the case the late phase of the training process comprises 6 repetitions\nof each word. The blue symbols represent the probability that the algorithm picks object o1 as the referent of word w1 whereas\nthe red symbols represent the probability it picks o7. The broken horizontal lines and the shaded zones around them represent\nthe experimental means and standard deviations [11] represented by the broken horizontal lines (means) and shaded regions\naround them (one standard deviation). The left panel shows the results for 3 repetitions of w1 in the early training phase, the\nmiddle panel for 6 repetitions and the right panel for 9 repetitions. The results correspond to the parameters χ and β that\nbest ﬁt the experimental data for ﬁxed α0.\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nAccuracy\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nAccuracy\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nAccuracy\nFIG. 6: Results of the experiments on mutual exclusivity in the case the late phase of the training process comprises 6\nrepetitions of each word. The green symbols represent the probability that the algorithm picks object o7 as the referent of word\nw7 whereas the orange symbols represent the probability it picks o1. The broken horizontal lines and the shaded zones around\nthem represent the experimental means and standard deviations [11] represented by the broken horizontal lines (means) and\nshaded regions around them (one standard deviation). The left panel shows the results for 3 repetitions of w1 in the early\ntraining phase, the middle panel for 6 repetitions and the right panel for 9 repetitions. The results correspond to the parameters\nχ and β that best ﬁt the experimental data for ﬁxed α0.\nV.\nDISCUSSION\nThe chief purpose of this paper is to understand and\nmodel the mental processes used by human subjects to\nproduce their word-object mappings in the controlled\ncross-situational word-learning scenarios devised by Yu\nand Smith [22] and Kachergis et al. [10, 11]. In other\nwords, we seek to analyze the psychological phenomena\ninvolved in the production of those mappings. Accord-\ningly, we assume that the completion of that task requires\nthe existence of two cognitive abilities, namely, the asso-\nciative capacity to create and reinforce associations be-\ntween words and referents that co-occur in a context, and\nthe non-associative capacity to infer word-object associ-\nations based on previous learning events, which accounts\nfor the mutual exclusivity principle, among other things.\nIn order to regulate the eﬀectiveness of these two capac-\nities we introduce the parameters χ ∈[0, 1], which yields\nthe reinforcement strength, and β ∈[0, 1], which deter-\nmines the inference strength.\nIn addition, since the reinforcement and inference pro-\ncesses require storage, use and transmission of past and\npresent information (coded mainly on the values of the\nconﬁdences Pt (wi, oj)) we introduce a word-dependent\n9\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nà\nΒ\nΧ\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nΑ0\nΧ,Β\nFIG. 7: Parameters χ (reinforcement strength) and β (infer-\nence strength) corresponding to the best ﬁt shown in Figures\n5 and 6 in the case word w1 is repeated 6 times in the early\ntraining phase.\nquantity αt (wi) ∈[0, 1] which measures how eﬃciently\nthe information regarding word wi is processed. In par-\nticular, the greater the certainty about the referent of\nword wi, the more eﬃciently the information regarding\nwi is processed and transmitted from trial to trial. How-\never, there is a baseline eﬃciency α0 ∈[0, 1] used to\nprocess words for which the uncertainty about their ref-\nerents is maximum. The adaptive expression for αt (wi)\ngiven in eq. (7) seems to be critical for the ﬁtting of the\nexperimental data. In fact, our ﬁrst choice was to use a\nconstant information eﬃciency (i.e., αt (wi) = α ∀t, wi)\nwith which we were able to describe only the experiments\nsummarized in Figures 1 and 4 (data not shown). Note\nthat a consequence of prescription (7) is that once the\nreferent of a word is learned with maximum conﬁdence\n(i.e., Pt (wi, oj) = 1 and Pt (wi, ok) = 0 for ok ̸= oj) it is\nnever forgotten.\nThe algorithm described in Section III comprises three\nfree parameters χ, β and α0 which are adjusted so as\nto ﬁt a representative selection of the experimental data\npresented in Kachergis et al. [10, 11]. A robust result\nfrom all experiments is that the baseline information ef-\nﬁciency is in the range 0.7 < α0 < 1.\nActually, the\nfast mapping experiments narrow this interval down to\n0.85 < α0 < 0.9. This is a welcome result because we\ndo not have a clear-cut interpretation for α0 – it encom-\npasses storage, processing and transmission of informa-\ntion – and so the fact that this parameter does not vary\nmuch for wildly distinct experimental settings is evidence\nthat, whatever its meaning, it is not relevant to explain\nthe learning strategies used in the diﬀerent experimental\nconditions. Fortunately, this is not the case for the two\nother parameters χ and β.\nFor instance, in the fast mapping experiments dis-\ncussed in Subsection IV C the best ﬁt of the experimental\ndata is achieved for β ≈1 indicating thus the extensive\nuse of mutual exclusivity, and inference in general, by the\nparticipants of those experiments. Moreover, in that case\nthe best ﬁt corresponds to a low (but nonzero) value of χ,\nwhich is expected since for contexts that exhibit two as-\nsociations (C = 2) only, most of the disambiguations are\nlikely to be achieved solely through inference. This con-\ntrasts with the experiments on variable word sampling\nfrequencies discussed in Subsection IV A, for which the\nbest ﬁt is obtained with intermediate values of β and χ so\nthe participants’ use of reinforcement and inference was\nnot too unbalanced. The contextual diversity experiment\nof Subsection IV B, in which the words are segregated in\ntwo isolated groups of 12 and 6 components, oﬀers an-\nother extreme learning situation, since the best ﬁt corre-\nsponds to χ ≈1 and β ≈0 in that case. To understand\nthis result, ﬁrst we recall that most of the participants’\nerrors were due to misassignments of referents belonging\nto the same group of the test word, and those conﬁdences\nwere strengthened mainly by the reinforcement process.\nSecond, in contrast to the inference process, which cre-\nates and strengthens spurious intergroup associations via\neq. (12), the reinforcement process solely weakens those\nassociations via eq. (9). Thus, considering the learning\nconditions of the contextual diversity experiment it is no\nsurprise that reinforcement was the participants’ choice\nstrategy.\nOur results agree with the ﬁndings of Smith et al. [17]\nthat participants use various learning strategies, which in\nour case are determined by the values of the parameters χ\nand β, depending on the speciﬁc conditions of the cross-\nsituational word-learning experiment. In particular, in\nthe case of low within-trial ambiguity those authors found\nthat participants generally resorted to a rigorous elimina-\ntive approach to infer the correct word-object mapping.\nThis is exactly the conclusion we reached in the analysis\nof the fast mapping experiment for which the within-trial\nambiguity takes the lowest possible value (C = 2).\nAlthough the adaptive learning algorithm presented\nin this paper reproduced the performance of adult par-\nticipants in cross-situational word-learning experiments\nquite successfully, the deterministic nature of the algo-\nrithm hindered somewhat the psychological interpreta-\ntion of the information eﬃciency factor αt (wi). In fact,\nnot only learning and behavior are best described as\nstochastic processes [1] but also the modeling of those\nprocesses requires (and facilitates) a precise interpreta-\ntion of the model parameters, since they are introduced\nin the model as transition probabilities. Work in that\nline is underway.\n10\nAcknowledgement\nThe work of J.F.F. was supported in part by Con-\nselho Nacional de Desenvolvimento Cient´ıﬁco e Tec-\nnol´ogico\n(CNPq)\nand\nP.F.C.T.\nwas\nsupported\nby\ngrant # 2011/11386-1, S˜ao Paulo Research Foundation\n(FAPESP).\n[1] Atkinson, R.C., Bower, G.H., and Crothers, E.J. (1965)\nAn introduction to mathematical learning theory. New\nYork: Wiley.\n[2] Baldwin, D.A., Markman, E.M., Bill, B., Desjardins,\nR.N., Irwin, J.M., and Tidball, G. (1996) Infants’ re-\nliance on a social criterion for establishing word-object\nrelations, Child Dev., 67, 3135-3153.\n[3] Bloom, P. (2000) How children learn the meaning of\nwords. Cambridge, MA: MIT Press.\n[4] Blythe, R. A., Smith, K., and Smith, A. D. M. (2010)\nLearning Times for Large Lexicons Through Cross-\nSituational Learning, Cogn. Sci., 34, 620-642.\n[5] Cangelosi, A., Tikhanoﬀ, V., Fontanari, J.F., and Hour-\ndakis, E. (2007) Integrating Language and Cognition: A\nCognitive Robotics Approach, IEEE Comput. Intell. M.,\n2 , 65-70.\n[6] Fontanari, J. F., Tikhanoﬀ, V.. Cangelosi, A., Ilin, R.,\nand Perlovsky, L. I. (2009) Cross-situational learning of\nobject-word mapping using Neural Modeling Fields, Neu-\nral Networks, 22, 579-585.\n[7] Fontanari,\nJ. F.,\nand Cangelosi,\nA. (2011) Cross-\nsituational and supervised learning in the emergence of\ncommunication, Interact. Stud., 12, 119-133.\n[8] Frank, M. C., Goodman, N. D., and Tenenbaum, J. B.\n(2009) Using speakers’ referential intentions to model\nearly cross-situational word learning, Psychol Sci., 20,\n578-585.\n[9] Gleitman, L. (1990) The structural sources of verb mean-\nings, Lang. Acquis., 1, 1-55.\n[10] Kachergis, G., Yu, C., and Shiﬀrin, R. M. (2009)\nFrequency and Contextual Diversity Eﬀects in Cross-\nSituational Word Learning. In N. Taatgen, H. van Rijn,\nJ. Nerbonne, & L. Schomaker (Eds.), Proceedings of 31st\nAnnual Meeting of the Cognitive Science Society. Cogni-\ntive Science Society, Austin, TX, pp. 755760.\n[11] Kachergis, G., Yu, C., and Shiﬀrin, R. M. (2012) An\nAssociative Model of Adaptive Inference for Learning\nWord-Referent Mappings, Psychonomic Bull. Rev., 19,\n317-324.\n[12] Markman, E.M and Wachtel, G.F. (1988) Children’s use\nof mutual exclusivity to constrain the meanings of words,\nCognitive Psychol., 20,121-157.\n[13] Newell, A. (1994) Uniﬁed Theories of Cognition. Cam-\nbridge, MA: Harvard University Press.\n[14] Pinker, S. (1990) Language learnability and language de-\nvelopment. Cambridge, MA: Harvard University Press.\n[15] Reisenauer, R., Smith, K., and Blythe, R. A. (2013)\nStochastic Dynamics of Lexicon Learning in an Uncertain\nand Nonuniform World, Phys. Rev. Lett., 110, 258701.\n[16] Siskind, J. M. (1996) A computational study of cross-\nsituational techniques for learning word-to-meaning map-\npings, Cognition, 61, 39-91.\n[17] Smith, K., Smith, A. D. M., and Blythe, R. A. (2011)\nCross-situational learning:\nAn experimental study of\nword-learning mechanisms, Cogn. Sci., 35, 480-498.\n[18] Steels, L. (2011) Modeling the cultural evolution of lan-\nguage, Phys. Life Rev., 8, 339-356.\n[19] Tilles, P. F. C., and Fontanari, J. F. (2012a) Minimal\nmodel of associative learning for cross-situational lexicon\nacquisition, J. Math. Psychol., 56, 396-403.\n[20] Tilles, P. F. C., and Fontanari, J. F. (2012b) Critical\nbehavior in a cross-situational lexicon learning scenario,\nEurophys. Lett., 99, 60001.\n[21] Waxman, S. R., and Gelman, S. A. (2009) Early word-\nlearning entails reference, not merely associations, Trends\nCogn. Sci., 13, 258-263.\n[22] Yu, C., and Smith, L. B. (2007) Rapid word learning un-\nder uncertainty via cross-situational statistics, Psychol.\nSci., 18, 414-420.\n[23] Yu,\nC.,\nand Smith,\nL. B. (2012) Modeling cross-\nsituational word-referent learning: Prior questions, Psy-\nchol. Rev., 119, 21-39.\n",
  "categories": [
    "q-bio.NC"
  ],
  "published": "2013-07-15",
  "updated": "2013-07-15"
}