{
  "id": "http://arxiv.org/abs/1308.2029v3",
  "title": "Accuracy of Latent-Variable Estimation in Bayesian Semi-Supervised Learning",
  "authors": [
    "Keisuke Yamazaki"
  ],
  "abstract": "Hierarchical probabilistic models, such as Gaussian mixture models, are\nwidely used for unsupervised learning tasks. These models consist of observable\nand latent variables, which represent the observable data and the underlying\ndata-generation process, respectively. Unsupervised learning tasks, such as\ncluster analysis, are regarded as estimations of latent variables based on the\nobservable ones. The estimation of latent variables in semi-supervised\nlearning, where some labels are observed, will be more precise than that in\nunsupervised, and one of the concerns is to clarify the effect of the labeled\ndata. However, there has not been sufficient theoretical analysis of the\naccuracy of the estimation of latent variables. In a previous study, a\ndistribution-based error function was formulated, and its asymptotic form was\ncalculated for unsupervised learning with generative models. It has been shown\nthat, for the estimation of latent variables, the Bayes method is more accurate\nthan the maximum-likelihood method. The present paper reveals the asymptotic\nforms of the error function in Bayesian semi-supervised learning for both\ndiscriminative and generative models. The results show that the generative\nmodel, which uses all of the given data, performs better when the model is well\nspecified.",
  "text": "arXiv:1308.2029v3  [stat.ML]  25 Mar 2015\nAccuracy of Latent-Variable Estimation\nin Bayesian Semi-Supervised Learning ∗\nKeisuke Yamazaki\nk-yam@math.dis.titech.ac.jp\nDepartment of Computational Intelligence and Systems Science,\nTokyo Institute of Technology\nG5-19 4259 Nagatsuta Midori-ku Yokohama, Japan\nAbstract\nHierarchical probabilistic models, such as Gaussian mixture models, are\nwidely used for unsupervised learning tasks. These models consist of observ-\nable and latent variables, which represent the observable data and the un-\nderlying data-generation process, respectively. Unsupervised learning tasks,\nsuch as cluster analysis, are regarded as estimations of latent variables based\non the observable ones. The estimation of latent variables in semi-supervised\nlearning, where some labels are observed, will be more precise than that in un-\nsupervised, and one of the concerns is to clarify the eﬀect of the labeled data.\nHowever, there has not been suﬃcient theoretical analysis of the accuracy of\nthe estimation of latent variables. In a previous study, a distribution-based\nerror function was formulated, and its asymptotic form was calculated for un-\nsupervised learning with generative models. It has been shown that, for the\nestimation of latent variables, the Bayes method is more accurate than the\nmaximum-likelihood method. The present paper reveals the asymptotic forms\nof the error function in Bayesian semi-supervised learning for both discrim-\ninative and generative models. The results show that the generative model,\nwhich uses all of the given data, performs better when the model is well spec-\niﬁed.\nkeywords: Latent-variable estimation, Generative and discriminative models,\nBayes statistics\n∗This research was partially supported by the Kayamori Foundation of Informational Science\nAdvancement and KAKENHI 23500172 and 24700139.\n1\n1\nIntroduction\nHierarchical statistical models, such as the Gaussian mixture model, are widely em-\nployed in unsupervised learning. They consist of observable and latent variables,\nwhich express the given data and the underlying data-generation process, respec-\ntively. A typical task of unsupervised learning is clustering, in which observable\ndata is used to estimate labels that indicate which cluster the data are from. The\nGaussian mixture model is often used for cluster analysis, and in this model, a Gaus-\nsian component represents a cluster. If the parameter is known, the probabilities\nof the labels for each data point are easily computed. However, in many practical\nsituations, the parameter is unknown, and both it and the latent variable must be\nestimated. Some learning algorithms, such as the expectation–maximization (EM)\nalgorithm (Dempster et al., 1977) and the variational Bayes method (Attias, 1999;\nGhahramani and Beal, 2000; Beal, 2003), have two explicit estimation steps (the E\nstep and the M step). The present paper focuses on the Bayesian approach, which\nuses the posterior distribution to marginalize out the parameter and calculates the\nprobabilities of the latent variables.\nThere are two ways to use hierarchical models: to predict unseen observable data\nor to estimate the latent variables. The accuracy of prediction has been analyzed\ntheoretically. The generalization error measures the accuracy, and, in many cases,\nits asymptotic behavior is well known. When the error function is deﬁned by the\nKullback-Leibler divergence, the asymptotic form of the error is well known in the\nmaximum-likelihood method, and it has been used as a criterion for selecting model\ncomplexity (Akaike, 1974; Takeuchi, 1976; White, 1982).\nIn the Bayes method,\nthe posterior distribution of the parameter plays an important role to determine the\naccuracy; the normalizing factor of the distribution is the marginal likelihood and its\nnegative logarithm has a direct relation with the error function (Levin et al., 1990).\nSince the asymptotic form of the marginal likelihood has been calculated (Schwarz,\n1978; Clarke and Barron, 1990), this relation allows us to derive the asymptotic\ngeneralization error.\nOn the other hand, latent-variable (LV) estimation has not been analyzed suf-\nﬁciently.\nRecently, a distribution-based error function was formulated to deter-\nmine the accuracy of the LV estimation, and its asymptotic form was calculated\n(Yamazaki, 2014, 2015b). The results showed the diﬀerent properties from those of\nthe estimation of observable variables (OVs). For example, the Bayes LV estimation\nis more accurate than the maximum-likelihood estimation under the regularity con-\nditions while these estimation methods have the same accuracy in the OV prediction.\nMoreover, when there are unnecessary labels in the model, the Bayes method au-\ntomatically eliminates the redundant labels. Because of these advantages, we focus\non the Bayes approach in the present paper.\nThere are discriminative and generative approaches to deﬁning the model; the\n2\ndiscriminative approach results in a model that expresses the causal eﬀect of the\nobservable data on the latent variables, while the generative approach results in\na model that explains the data-generation process.\nOur previous study mainly\nanalyzed the generative model (Yamazaki, 2014). The present paper compares these\napproaches in terms of the estimation of LVs.\nIn the estimation of LVs, partially observed labels will improve accuracy because\nthey have information about the targets of the estimation. Learning from a mixture\nof labeled and unlabeled data is referred to as semi-supervised learning (Zhu, 2007;\nYamazaki, 2015a). Its main concerns include clarifying how the supplemental infor-\nmation aﬀects the accuracy of the estimation and developing a learning algorithm\nthat achieves better results based on this advantage.\nThe present paper analyzes the statistical properties of the Bayes method for\nsemi-supervised learning, and the main contributions are as follows:\n1. The asymptotic forms of the error function are derived for both the discrimi-\nnative and generative models.\n2. The generative model asymptotically performs better in the well-speciﬁed case.\nAsymptotic analysis generally assumes that the amount of unlabeled data is suf-\nﬁciently large. If the number of labels that are given is not large, there may be\nno eﬀect on the estimation, or it may be very weak. To magnify the eﬀect of the\nobserved labels, we assume that the number of labeled data points is αn, where\nn is the number of the training data points and α is the ratio of the labels where\n0 < α < 1 (see Fig. 1).\nThe rest of this paper is organized as follows. The next section formalizes the\ndata structure and the model expression. In Section 3, we introduce the Bayesian\nLV estimation and an error function to measure its accuracy. The discriminative and\nthe generative approaches are explained. Section 4 shows the asymptotic analysis of\nthe error function and compares the approaches. Section 5 discusses the magnitude\nrelation of the error function among the approaches and clariﬁes the eﬀect of the\nobservable labels on the accuracy.\n2\nData Structure and Model Expression\n2.1\nFormal Expressions of Data and Model\nThis subsection formulates the settings of the data and the model.\nFig. 1 shows the structure of the data; the observable and latent variable are\nx ∈RM and y ∈{1, . . . , K}, respectively. The data points {(x1, y1), . . . , (xn, yn)} are\nindependent and identically distributed, and αn data points {(x1, y1), . . . , (xαn, yαn)}\n3\n: observable part of data\n: hidden part of data\nFigure 1: Data structure: pairs (xi, yi) are generated. The solid circles represent\nobservable data, which is used for training, and the dashed circles represent hidden\ndata, which are the target of the estimation.\nare labeled, where 0 < α < 1 and αn is an integer. We deﬁne the following data\nsets:\nX1 = {x1, . . . , xαn},\nX2 = {xαn+1, . . . , xn},\nY1 = {y1, . . . , yαn},\nY2 = {yαn+1, . . . , yn},\nXn = {X1, X2},\nY n = {Y1, Y2},\nD = {X1, Y1, X2} = {Xn, Y1},\nwhere X1 is the set of αn observable data points, Y1 is the set of the corresponding\nlabels, and the target of the LV estimation is Y2. The set D contains the available\ndata for the estimation. The number of the labels grows linearly with the total\nnumber of data points n.\nThe generative model represents the underlying process of data generation. In\nthe present paper, we assume that the observable variables are caused by the latent\nvariables.\nThe mathematical expression for this is p(x, y|w) = p(x|y, w)p(y|w),\n4\nwhere w is the parameter. On the other hand, the discriminative model expresses\nthe probability of the latent variable based on the observable variable; the classiﬁer\nof the learning model is deﬁned by p(y|x, w). If the discriminative model is deﬁned\non the basis of the generative one, the model expression is given by p(y|x, w) =\np(x, y|w)/p(x|w), where p(x|w) = P\ny p(x, y|w). According to Fig. 1, the target of\nthe LV estimation is p(Y2|D). There are various ways to deﬁne p(Y2|D), as will be\nshown in the next section. Let q(y|x) and q(x) be the true classiﬁer and the true\ndensity of x, respectively; the data (Xn, Y n) are generated from q(x, y) = q(y|x)q(x).\n2.2\nAn Example of the Model\nFor illustrative purposes, we present the following data source and model:\nExample 1 (Data distribution) Deﬁne x and y so that x ∈R and y ∈{1, 2}.\nThe sample data follow the following distribution:\nq(x, y) = a∗\nyN (x|b∗\ny, σ),\n(1)\nwhere a∗\ny is the mixing ratio and N (x|µ, σ) is the one-dimensional Gaussian distri-\nbution with mean µ ∈R and variance σ ∈R>0. The mixing ratio is expressed as\na∗\n1 = a∗∈(0, 1) and a∗\n2 = 1 −a∗.\nNote that the density of x is also based on q(x, y):\nq(x) =\n2\nX\ny=1\nq(x, y).\nExample 2 (Learning model) The two-component Gaussian mixture learning\nmodel is deﬁned by\np(x|w) =\n2\nX\nk=1\nakN (x|bk, σ),\nwhere 0 ≤a1 ≤1, a2 = 1 −a1, and bk ∈R.\nThe parameter consists of w =\n(a1, b1, b2)⊤. The discriminative model is based on the following mixture:\np(y = k|x, w) =\np(x, y = k, w)\nP2\nj=1 p(x, y = j, w)\n=\nakN (x|bk, σ)\nP2\nj=1 ajN (x|bj, σ)\n.\n(2)\n5\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n-4\n-2\n 0\n 2\n 4\n 0\n 0.02\n 0.04\n 0.06\n 0.08\n 0.1\n 0.12\n 0.14\np(y|x,w)\np(x,y|w)\nx\np(y=1|x,w)\np(x,y|w)\np(x,y=1|w)\np(x,y=2|w)\ndata\nFigure 2: Model shapes with the generative and the discriminative expressions.\nThere exists a true parameter w∗such that q(y|x) = p(y|x, w∗).\nFig. 2 shows\nthe model shapes with the generative and the discriminative expressions in the\ntwo-component Gaussian mixture.\nThe true parameter is w∗= (a∗\n1, b∗\n1, b∗\n2)⊤=\n(0.5, 0, 1.5)⊤. A sample set of data from q(x, y) is shown in the same ﬁgure; the\ndata with y = 1 and y = 2 are on the upper and the lower horizontal lines, respec-\ntively.\n2.3\nRedundant Parameters in the Discriminative Model\nThe classiﬁer p(y|x, w) based on p(x, y|w) does not provide a one-to-one relation\nbetween the model expression and the parameter. Let us consider the case in Ex-\namples 1 and 2. Suppose that a∗\nk ̸= 0, which means that the model has the same\nnumber of clusters as the data distribution. Even though there is no redundancy in\nthe number of clusters, there is redundancy in the parameter:\np(y = 1|x, w) =a1 exp{−(x −b1)2/(2σ2)}\nZm\n=\n1\n1 + f1(x, w),\nwhere the normalizing factor is expressed as\nZm =\n2\nX\ny=1\nay exp{−(x −by)2/(2σ2)},\n6\nand the functions f1 and f2 are written as\nln f1(x, w) = 1\nσ2(b2 −b1)x −1\n2(b2\n2 −b2\n1) + ln a2\na1\n.\nThe coeﬃcients of x and the constant terms contain more elements of the parameter\nthan are needed to express the function. Let us reparameterize f1(x, w) as\nln f1(x, ¯w) =c1x + c2,\nwhere ¯w = (c1, c2)⊤. Considering the case y = 2, we can easily conﬁrm that the\nparameter ¯w is suﬃcient to express p(y = 2|x, w). This means that the essential\ndimension of the parameter is dim ¯w = 2 instead of dim w = 3. To eliminate the\nredundancy, we regard ak as a positive constant and let the reduced parameter be\n¯w, which consists of the elements of w except for ak. For the general dimension\nof data M and the general number of the components K, we can calculate that\ndim ¯w = dim w −M in the Gaussian mixture.\n3\nThe Bayes LV Estimation and Error Function\nThis section introduces the Bayes LV estimation and an error function to measure its\naccuracy. Let L(w, Xn, Y n) be a likelihood function on {Xn, Y n}, and let L(w, D) =\nP\nY2 L(w, Xn, Y n) be one on D. In the Bayesian model, the LV estimation, which\ncorresponds to constructing p(Y2|D), is written as\np(Y2|D) =\nR\nL(w, Xn, Y n)ϕ(w|η)dw\nR\nL(w, D)ϕ(w|η)dw\n,\n(3)\nwhere ϕ(w|η) is a prior distribution and η is the hyperparameter. We will consider\nhere the following three likelihood functions:\n(Model 1)\nL1( ¯w, Xn, Y n) =\nn\nY\ni=1\np(yi|xi, ¯w),\nL1( ¯w, D) =\nαn\nY\ni=1\np(yi|xi, ¯w);\n(Model 2)\nL2(w, Xn, Y n) =\nαn\nY\ni=1\np(yi|xi, w)\nn\nY\ni=αn+1\np(xi, yi|w),\nL2(w, D) =\nαn\nY\ni=1\np(yi|xi, w)\nn\nY\ni=αn+1\np(xi|w);\n7\nlabeled data unlabeled data\ndiscriminative\ngenerative\n(a) Model 1\nlabeled data unlabeled data\ndiscriminative\ngenerative\n(b) Model 2\nlabeled data unlabeled data\ndiscriminative\ngenerative\n(c) Model 3\nFigure 3: The relations between the data sets and the model expressions\n(Model 3)\nL3(w, Xn, Y n) =\nn\nY\ni=1\np(xi, yi|w),\nL3(w, D) =\nαn\nY\ni=1\np(xi, yi|w)\nn\nY\ni=αn+1\np(xi|w).\nThe ﬁrst and third models correspond to the discriminative and generative models,\nrespectively. The second one is a hybrid model; the labeled data are used in the dis-\ncriminative expression, and the unlabeled data are used in the generative expression.\nNote that Model 2 cannot use the reduced parameterization ¯w in the discriminative\nexpression, since it must use w in the generative part.\nThe formulation deﬁned by Eq. 3 has the following equivalent expressions:\np(Y2|D) =\nZ\nn\nY\ni=αn+1\np(yi|xi, w)p(w|D)dw,\n(4)\np(w|D) =\nL(w, D)ϕ(w|η)\nR\nL(w, D)ϕ(w|η)dw,\n(5)\nwhere p(w|D) is the posterior distribution. The ﬁrst deﬁnition of p(Y2|D), Eq. 3,\nis used for theoretical calculations in the asymptotic analysis, and the second one,\nEq. 4, is useful for numerical computations and for comparison of the models. Note\nthat for the both deﬁnitions the parameter w is replaced with ¯w in Model 1. Fig.3\nshows relations between the data sets and the model expressions in the posterior\ndistribution. For example, in Model 2, the main factor of the posterior distribution\nL(w, D) contains the discriminative expression with the labeled data {X1, Y1} and\n8\nthe generative expression with the unlabeled data X2. Note that L( ¯w, D) in Model\n1 contains the discriminative expression only, where the unlabeled data X2 are not\nused for the estimation.\nSince the data are i.i.d., the true joint probability of Y2 is given by\nq(Y2|D) = q(Y2|X2) =\nn\nY\ni=αn+1\nq(yi|xi).\n(6)\nThe error function is deﬁned as the diﬀerence between the true and the estimated\nprobabilities of Y2. Following previous work (Yamazaki, 2014), the error is based on\nthe Kullback–Leibler divergence:\nD(n) =\n1\n(1 −α)nED\n\u0014 X\nY2\nq(Y2|D) ln q(Y2|D)\np(Y2|D)\n\u0015\n,\nwhere the expectation means\nED[f(X1, Y1, X2)] =\nZ X\nY1\nf(X1, Y1, X2)q(Y1|X1)q(X1)q(X2)dX1dX2.\nSince the number of elements in Y2 is (1 −α)n, the error function is the average\ndivergence for one latent variable.\n4\nAsymptotic Analysis of the Error Function\nThis section shows one of the main results of this paper: the asymptotic forms of\nthe error functions of the three models and a comparison between them.\nWe assume the following condition:\n(A1) In the discriminative expression, there exists a true parameter ¯w∗such that\nq(y|x) = p(y|x, w∗) in the support of ϕ( ¯w|η), and the following Fisher infor-\nmation matrix in the neighborhood of ¯w∗exists and is positive deﬁnite:\n{Iy|x( ¯w)}ij =\nZ X\ny\n∂ln p(y|x, ¯w)\n∂¯wi\n∂ln p(y|x, ¯w)\n∂¯wj\np(y|x, ¯w)q(x)dx.\n(A2) In the generative expression, there exists a true parameter w∗such that\nq(x, y) = p(x, y|w∗) in the support of ϕ(w|η), and the following Fisher in-\nformation matrices in the neighborhood of w∗exist and are positive deﬁnite:\n{Ixy(w)}ij =\nZ X\ny\n∂ln p(x, y|w)\n∂wi\n∂ln p(x, y|w)\n∂wj\np(x, y|w)dx,\n{Ix(w)}ij =\nZ ∂ln p(x|w)\n∂wi\n∂ln p(x|w)\n∂wj\np(x|w)dx.\n9\nThese conditions indicate the ideal situation for the estimation, where the estimated\nprobability p(Y2|D) in all models converges to the true one and the model is identi-\nﬁable.\nWhen the discriminative model p(y|x, w) is based on the generative expression,\nsuch as in Example 2, let another Fisher information matrix be deﬁned by\n{I(w)}ij =\nZ X\ny\n∂ln p(y|x, w)\n∂wi\n∂ln p(y|x, w)\n∂wj\np(y|x, w)q(x)dx,\nwhere I(w∗) = Ixy(w∗) −Ix(w∗). Note that here we use the common parameter\nsetting and not the reduced one ¯w; this means that, due to the redundancy of the\nparameters, the rank of I(w∗) is not more than dim w −M in the Gaussian mixture.\nThe parameter redundancy in the discriminative model must be eliminated for the\ncondition (A1).\nThus, we use the notation ¯w to indicate the parameter setting\nsatisfying (A1).\nThe following theorem shows the asymptotic behavior of the error function.\nTheorem 3 Let D1(n), D2(n), and D3(n) be the error functions of Models 1, 2,\nand 3, respectively. Under the conditions (A1) and (A2), it holds that\nD1(n) =dim ¯w\n2\nln 1/α\n1 −α\n1\nn + o\n\u00121\nn\n\u0013\n,\nD2(n) =1\n2\nln det K2(w∗)\n1 −α\n1\nn + o\n\u00121\nn\n\u0013\n,\nD3(n) =1\n2\nln det K3(w∗)\n1 −α\n1\nn + o\n\u00121\nn\n\u0013\n,\nwhere\nK2(w) =\n\u0000Ixy(w) −αIx(w)\n\u0001\u0000αIxy(w) + (1 −2α)Ix(w)\n\u0001−1,\nK3(w) =Ixy(w)\n\u0000αIxy(w) + (1 −α)Ix(w)\n\u0001−1.\nThe proof is in the appendix.\nThe theorem shows that, in all models, the dominant order is 1/n, which is the\nspeed at which the error converges to zero. The accuracy of Model 1 depends on\nthe dimension of ¯w instead of the position of ¯w∗; note that, in the other models, the\ncoeﬃcients of the dominant order are functions of w∗.\nLet us compare these error values. We assume that Model 1 is based on the\ngenerative expression and uses the reduced parameter ¯w. According to Theorem 3,\nthe asymptotic forms of the error functions are expressed as\nDi(n) = ci\nn + o\n\u00121\nn\n\u0013\n,\n10\nwhere ci is a positive constant. When ci < cj, we deﬁne the magnitude relation\nbetween the error functions as\nDi(n) < Dj(n).\nThe following theorem shows the relation among the three error functions;\nTheorem 4 If the nonzero eigenvalues of I(w∗)Ix(w∗)−1 are all non negative, the\nfollowing inequality holds asymptotically:\nD3(n) < D2(n) < D1(n).\nThe proof is in the appendix.\n5\nDiscussions\n5.1\nOn the Magnitude Relation in Theorem 4\nFirst, let us consider the magnitude relation in Theorem 4. A larger amount of\ntraining data obviously increases the accuracy of the estimation, and a high dimen-\nsional parameter allows the model to be expressive and complex. It is known that\nthe asymptotic form of the error function depends on the number of data and the\ndimension of the parameter in many cases, and there is a trade-oﬀbetween them.\nFor example, the generalization error G(n) for the OV estimation is deﬁned by\nG(n) = En\n\u0014 Z\nq(x) ln\nq(x)\np(x|Xn)dx\n\u0015\n,\nwhere Xn = {x1, . . . , xn}, En[·] is the expectation over all training data Xn, and\np(x|Xn) is the predictive distribution. In the Bayes method, the predictive distri-\nbution is given by\np(x|Xn) =\nZ\np(x|w)po(w|Xn)dw,\npo(w|Xn) =\nQn\ni=1 p(xi|w)ϕ(w|η)\nR Qn\ni=1 p(xi|w)ϕ(w|η)dw.\nUnder the condition (A2), the asymptotic form of the generalization error is ex-\npressed as\nG(n) = dim w\n2n\n+ o\n\u00121\nn\n\u0013\n,\n11\nwhere n is the number of data and dim w is the number of the parameter (Schwarz,\n1978; Rissanen, 1986; Clarke and Barron, 1990; Levin et al., 1990). Obviously, the\nprediction is accurate when n is large or dim w is small.\nIn the unsupervised LV estimation with the generative expression, the task is to\nestimate all labels Y n = {Y1, Y2} = {y1, . . . , yn} based on Xn = {X1, X2}. In the\nBayes method, the estimated distribution of Y n is described by\np(Y n|Xn) =\nZ\nn\nY\ni=1\np(xi, yi|w)\np(xi|w) po(w|Xn)dw,\nand the error function is deﬁned by\nDU(n) = 1\nnEn\n\u0014 X\nY n\nq(Y n|Xn) ln q(Y n|Xn)\np(Y n|Xn)\n\u0015\n,\nwhere the true distribution of Y n is given by\nq(Y n|Xn) =\nn\nY\ni=1\nq(xi, yi)\nq(xi) .\nUnder the condition (A2), the error function has the following asymptotic form\n(Yamazaki, 2014),\nDU(n) = 1\n2n ln det Ixy(w∗)Ix(w∗)−1 + o\n\u00121\nn\n\u0013\n.\nSince the rank of Ixy(w∗)Ix(w∗)−1 is determined by the dimension of w, the LV\nestimation is also accurate when n is large or dim w is small.\nLet us compare the three models from the perspective of the parameter dimension\nand the amount of data. Since dim ¯w < dim w, Model 1 has an advantage in the\nparameter dimension. On the other hand, the actual amount of data used for the\nestimation is larger in Models 2 and 3; according to the second deﬁnition of p(Y2|D),\nEq. 4 and Fig. 3-(a), the posterior distribution of Model 1 is constructed by only\n{X1, Y1}, while those of Models 2 and 3 require D = {X1, Y1, X2}. Theorem 4 shows\nthat, in order to improve accuracy, increasing the amount of data is more eﬀective\nthan reducing the dimension of the parameter. Model 1 is thus at a disadvantage.\n5.2\nThe Eﬀect of the Posterior Convergence on the Accu-\nracy\nNext, we discuss convergence of the posterior distribution in Eq. 5 and its eﬀect on\nthe accuracy. The asymptotic form of the error function in Theorem 3 indicates the\n12\n-1.8\n-1.6\n-1.4\n-1.2\n-1\n-0.8\n-0.6\n 1.2\n 1.4\n 1.6\n 1.8\n 2\n 2.2\nModel 1\n(a) Model 1\n-1.8\n-1.6\n-1.4\n-1.2\n-1\n-0.8\n-0.6\n 1.2\n 1.4\n 1.6\n 1.8\n 2\n 2.2\nModel 2\n(b) Model 2\n-1.8\n-1.6\n-1.4\n-1.2\n-1\n-0.8\n-0.6\n 1.2\n 1.4\n 1.6\n 1.8\n 2\n 2.2\nModel 3\n(c) Model 3\nFigure 4: Sampled points from the posterior distribution in the space of ¯w\neﬀect of the posterior. According to its proof in Appendix, the posterior converges\nto the Gaussian distribution N ( ˜w, ˜Σ/n) in law, where ˜w is the maximum-likelihood\nestimator of L(w, D) in Eq. 5, and ˜Σ is given by\n˜Σ−1 =\n\n\n\n\n\nαIy|x( ¯w∗)\nModel 1\nαIxy(w∗) + (1 −2α)Ix(w∗)\nModel 2\nαIxy(w∗) + (1 −α)Ix(w∗)\nModel 3\n.\nThe right-hand side corresponds to the inverse matrix in the coeﬃcient. For example,\nK2(w∗) for Model 2 has the inverse matrix {αIxy(w∗)+(1−2α)Ix(w∗)}−1. Therefore,\nthe variance of the posterior, which shows the convergence speed, is one of the\nessential factor to determine the accuracy. In Model 1, the original form of the\ncoeﬃcient is ln det Iy|x( ¯w∗){αIy|x( ¯w∗)}−1, and then dim ¯w ln 1/α appears instead of\n˜Σ−1.\nAs shown in Eq. 4, the posterior determines the diﬀerence of the models. Then,\nthe magnitude relation in Theorem 4 reﬂects the diﬀerence of the convergence speeds\nof the posterior distributions. In order to visualize this diﬀerence, let us experimen-\ntally compare the posterior distributions in the settings of Examples 1 and 2. The\nMarkov chain Monte Carlo (MCMC) method was employed for obtaining parameter\nsamples from the posterior. The total number of data was n = 400, and the ratio\nof labeled data was α = 0.5. Fig. 4 shows 100 sampled points from the posterior\nin each model. To compare Model 1 with Models 2 and 3, we mapped samples of\nthe parameter w to the space of the reduced parameter ¯w = (c1, c2)⊤. According to\nthe calculation in Section 2.3, the dimension of ¯w is dim ¯w = 2, and the mapping is\ndeﬁned by\nc1 =b2 −b1,\nc2 = −1\n2(b2 −b1)(b2 + b1) + ln 1 −a1\na1\n,\nwhere we assumed σ = 1 for simplicity. We conducted this MCMC sampling in 50\ndiﬀerent data sets, where each data set Xn contains 200 labeled and 200 unlabeled\n13\nmodels\nE[¯µ]\nE[(¯µ −w∗)2]\nModel 1\n(1.55231, −1.16090)⊤\n0.12081\nModel 2\n(1.47616, −1.09729)⊤\n0.06214\nModel 3\n(1.48225, −1.09883)⊤\n0.05043\nTable 1: The convergence of the posterior distribution\ndata. Let ¯µ be the empirical mean of the sampled points, and E[¯µ] be its expectation\nover 50 sets. The true parameter was ¯w∗= (1.5, −1.125)⊤, which was the conver-\ngence point of the posterior at n →∞. Table 1 shows E[¯µ] and E[(¯µ −¯w∗)2]. The\nsmaller E[(¯µ −¯w∗)2] is, the faster the posterior converges. Model 1 had the slowest\nconvergence because its posterior was constructed without the unlabeled data and\nthe amount of the data was much smaller than that of the other models. As Theo-\nrem 4 predicted, the convergence of Model 3 was faster than that of Model 2. The\nposterior distributions of Models 2 and 3 were originally deﬁned in the space of w.\nSince the parameter w was reducible to the lower dimensional ¯w, the discriminative\nexpression for the labeled data Qαn\ni=1 p(yi|xi, w) had less information on the original\nparameter w than the generative expression Qαn\ni=1 p(xi, yi|w). This was the reason\nwhy Model 3 had better results than Model 2.\n5.3\nComparison with the Estimation without Labels\nIn order to clarify the eﬀect of the observable labels, we compare the accuracy\nbetween Model 3 and the estimation of Y2 without the labels Y1. The estimation is\nexpressed as\npNL(Y2|X1, X2) =\nR Qαn\ni=1 p(xi|w) Qn\ni=αn+1 p(xi, yi|w)ϕ(w|η)dw\nR Qn\ni=1 p(xi|w)ϕ(w|η)dw\n,\nwhich is a generative expression. The error function is then given by\nDNL(n) =\n1\n(1 −α)nED\n\u0014\nq(Y2|D) ln\nq(Y2|D)\npNL(Y2|X1, X2)\n\u0015\n.\nThis corresponds to the Type II estimation in Yamazaki (2014), and the asymptotic\nform of the error has been derived as\nDNL(n) =1\n2\nln det K4(w∗)\n1 −α\n1\nn + o\n\u00121\nn\n\u0013\nunder the condition (A2), where\nK4(w) =((1 −α)Ixy(w) + αIx(w))Ix(w)−1.\nThe following lemma shows the quantitative diﬀerence between the estimations\nwith and without the observable labels Y1,\n14\nLemma 5 Let assume that Ixy(w∗)Ix(w∗)−1 has the eigen values λ1, . . . , λd. Under\nthe same conditions of Theorem 4, all eigen values are not less than one. Then, the\nasymptotic error is described by\nDNL(n) =\n1\n2(1 −α)n\nd\nX\ni=1\nln(α + (1 −α)λi) + o\n\u00121\nn\n\u0013\n,\n(7)\nand the magnitude relation to the error of Model 3 is given by\nDNL(n) > D3(n).\nMore precisely, the asymptotic diﬀerence of these error functions is described as\nDNL(n) −D3(n) =\n1\n2(1 −α)n\nd\nX\ni=1\nln\n\u001a\nα(1 −α)\n\u0012\nλi + 1\nλi\n\u0013\n+ α2 + (1 −α)2\n\u001b\n+ o\n\u00121\nn\n\u0013\n,\n(8)\nwhere the coeﬃcient of the dominant term is positive since the factor λi + 1/λi is\nthe convex function with respect to λi and has the minimum value at λi = 1.\nThe proof is in the appendix. Let us focus on the case, where the labels are infor-\nmative and the diﬀerence between the information matrices with and without labels\nis large. The eigen value λi increases from one since λi = 1 for Ixy(w∗) = Ix(w∗).\nThe asymptotic form in Eq. 7 shows how the accuracy is adversely aﬀected by this\nincrease. Therefore, λi indicates the diﬃculty of the task in the unsupervised learn-\ning. According to Eq. 8, the diﬀerence of the error functions is also determined by\nthe eigen values, Because the factor λi + 1/λi is the increasing function for λi ≥1,\nthe accuracy of the semi-supervised learning is signiﬁcantly improved when the task\nis diﬃcult and λi grows.\n6\nConclusion\nIn semi-supervised learning, the given labels are used for the estimation of unob-\nservable labels. Depending on the expression of the labeled data in the likelihood\nfunction, we have three approaches: generative, discriminative, and their hybrid\nmodels. In the present paper, we focus on the Bayes method to estimate the la-\nbels in a distribution-based manner, and derive the asymptotic form of the error\nfunction measuring the accuracy with the Kullback-Leibler divergence. Comparing\nthe asymptotic forms in the three models, we prove that the generative model per-\nforms better when the model is well speciﬁed. The asymptotic error depends on the\n15\namount of data and the dimension of the parameter, and there is a trade-oﬀbetween\nthem. The discriminative model does not require high dimensional parameter due to\nits simple expression while the generative model uses more data for the estimation.\nThe magnitude relation theoretically indicates that increasing the amount of data\nis more eﬀective than reducing the dimension of the parameter in order to improve\naccuracy.\nAppendix\nThis section shows the proofs of the theorems.\nProof of Theorem 3\nFirst, we derive the asymptotic form of D1(n). Deﬁne the free-energy function by\nFy|x(n) =En\n\u0014\nln\nn\nY\ni=1\nq(yi|xi) −ln\nZ\nn\nY\ni=1\np(yi|xi, ¯w)ϕ( ¯w|η)d ¯w\n\u0015\n,\nwhere the expectation is\nEn[f(Xn, Y n)] =\nZ X\nY n\nf(Xn, Y n)q(Y n|Xn)q(Xn)dXn.\nThe error function D1(n) can be rewritten as\n(1 −α)nD1(n) =ED\n\u0014 X\nY2\nq(Y2|D)\n\u001a\nln\nQn\ni=1 q(yi|xi)\nR Qn\ni=1 p(yi|xi, ¯w)ϕ( ¯w|η)d ¯w\n−ln\nQαn\ni=1 q(yi|xi)\nR Qαn\ni=1 p(yi|xi, ¯w)ϕ( ¯w|η)d ¯w}\n\u001b\u0015\n=Fy|x(n) −Fy|x(αn).\n(9)\nIt is suﬃcient to calculate the asymptotic form of Fy|x(n).\nThe maximum-likelihood estimator is deﬁned by\nˆwn = arg max\n¯w\nn\nY\ni=1\np(yi|xi, ¯w).\nDue to (A1), the estimator converges to ¯w∗, which means that the essential param-\neter area for the integration is the neighborhood of ¯w∗and ˆwn. According to the\n16\nTaylor expansion at w = ˆwn,\nEn\n\u0014\nln\nZ\nn\nY\ni=1\np(yi|xi, ¯w)ϕ( ¯w|η)d ¯w\n\u0015\n=En\n\u0014\nln\nZ\nexp\n\u001a\nn1\nn\nn\nX\ni=1\nln p(yi|xi, ¯w)\n\u001b\nϕ( ¯w|η)d ¯w\n\u0015\n=En\n\u0014\nln\nZ\nexp\n\u001a\nn1\nn\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n+ n1\nn( ¯w −ˆwn)⊤∂\n∂¯w\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n+ n1\n2( ¯w −ˆwn)⊤1\nn\n∂2\n∂¯w2\nn\nX\ni=1\nln p(yi|xi, ˆwn)(w −ˆwn) + r1( ¯w)\n\u001b\nϕ( ¯w|η)d ¯w\n\u0015\n,\nwhere r1( ¯w) is the remainder term. Based on the saddle-point approximation,\nEn\n\u0014\nln\nZ\nn\nY\ni=1\np(yi|xi, ¯w)ϕ( ¯w|η)d ¯w\n\u0015\n=En\n\u0014\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n\u0015\n+ En\n\u0014\nln\nZ\nexp(nr1( ¯w))ϕ( ¯w|η)N ( ˆwn, (nIy|x( ¯w∗))−1)d ¯w\n\u0015\n−\n\u001adim ¯w\n2\nln n −dim ¯w\n2\nln 2π + 1\n2 ln det Iy|x( ¯w∗)\n\u001b\n+ o(1)\n=En\n\u0014\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n\u0015\n−dim ¯w\n2\nln n\n+ dim ¯w\n2\nln 2π −1\n2 ln det Iy|x( ¯w∗) + ln ϕ( ¯w∗) + o(1),\nwhere N (µ, Σ) is a dim ¯w-dimensional Gaussian distribution with mean µ ∈Rdim ¯w\nand variance-covariance matrix Σ. Thus, we obtain\nFy|x(n) =En\n\u0014\nn\nX\ni=1\nln\nq(yi|xi)\np(yi|xi, ˆwn)\n\u0015\n+ dim ¯w\n2\nln n −dim ¯w\n2\nln 2π + 1\n2 ln det Iy|x( ¯w∗) −ln ϕ( ¯w∗|η) + o(1).\n17\nAccording to the Taylor expansion at ˆwn,\nEn\n\u0014\nn\nX\ni=1\nln p(yi|xi, ¯w∗)\n\u0015\n=En\n\u0014\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n+ ( ¯w∗−ˆwn)⊤∂\n∂¯w\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n+ 1\n2( ¯w∗−ˆwn)⊤∂2\n∂¯w2\nn\nX\ni=1\nln p(yi|xi, ˆwn)( ¯w∗−ˆwn) + r2( ¯w)\n\u0015\n=En\n\u0014\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n\u0015\n−n\n2En\n\u0014\n( ¯w∗−ˆwn)⊤Iy|x( ¯w∗)( ¯w∗−ˆwn)\n\u0015\n+ o(1),\nwhere r2( ¯w) is the remainder term. The estimator ˆwn has asymptotic normality,\nand it converges to the Gaussian distribution with mean w0 and variance–covariance\nmatrix (nIy|x( ¯w∗)−1)−1. It holds that\nEn\n\u0014\nn\nX\ni=1\nln p(yi|xi, ¯w∗)\n\u0015\n= En\n\u0014\nn\nX\ni=1\nln p(yi|xi, ˆwn)\n\u0015\n−dim ¯w\n2\n+ o(1).\nThe free-energy function has the asymptotic form\nFy|x(n) =dim ¯w\n2\nln n −dim ¯w\n2\nln 2πe + 1\n2 ln det Iy|x( ¯w∗) −ln ϕ( ¯w∗|η) + o(1),\nwhich is consistent with the form derived in (Clarke and Barron, 1990). Based on\nthe relation in Eq. 9, we obtain\n(1 −α)nD1(n) = −dim ¯w\n2\nln α + o(1),\nwhich is the asymptotic form of D1(n).\nIn the similar way, we derive the asymptotic forms of D2(n) and D3(n). Deﬁne\nthe free-energy functions as\nFxy(n) =En\n\u0014\nln\nn\nY\ni=1\nq(xi, yi) −ln\nZ\nn\nY\ni=1\np(xi, yi|w)ϕ(w|η)dw\n\u0015\n,\nFxy,x(n) =En\n\u0014\nln\nαn\nY\ni=1\nq(xi, yi)\nn\nY\ni=αn+1\nq(xi)\n−ln\nZ\nαn\nY\ni=1\np(xi, yi|w)\nn\nY\ni=αn+1\np(xi|w)ϕ(w|η)dw\n\u0015\n.\n18\nThe error function D3(n) is rewritten as\n(1 −α)nD3(n) =ED\n\u0014 X\nY2\nq(Y2|D)\n\u001a\nln\nQn\ni=1 q(xi, yi)\nR Qn\ni=1 p(xi, yi|w)ϕ(w|η)dw\n−ln\nQαn\ni=1 q(xi, yi) Qn\ni=αn+1 q(xi)\nR Qαn\ni=1 p(xi, yi|w) Qn\ni=αn+1 p(xi|w)ϕ(w|η)dw\n\u001b\u0015\n=Fxy(n) −Fxy,x(n).\n(10)\nThe maximum-likelihood estimator is deﬁned by\nˆwxy = arg max\nw\nn\nY\ni=1\np(xi, yi|w).\nDue to (A2), the estimators ˆwxy and ˆw3 converge to w∗, which means that the\nessential parameter area for the integration is the neighborhood of w∗, ˆwxy, and ˆw3.\nAccording to the Taylor expansion at w = ˆwxy,\nEn\n\u0014\nln\nZ\nn\nY\ni=1\np(xi, yi|w)ϕ(w|η)dw\n\u0015\n=En\n\u0014\nln\nZ\nexp\n\u001a\nn1\nn\nn\nX\ni=1\nln p(xi, yi|w)\n\u001b\nϕ(w|η)dw\n\u0015\n=En\n\u0014\nln\nZ\nexp\n\u001a\nn1\nn\nn\nX\ni=1\nln p(xi, yi| ˆwxy)\n+ n1\nn(w −ˆwxy)⊤∂\n∂w\nn\nX\ni=1\nln p(xi, yi| ˆwxy)\n+ n1\n2(w −ˆwxy)⊤1\nn\n∂2\n∂w2\nn\nX\ni=1\nln p(xi, yi| ˆwxy)(w −ˆwxy) + r1(w)\n\u001b\nϕ(w|η)dw\n\u0015\n,\n19\nwhere r1(w) is the remainder term. Based on the saddle-point approximation,\nEn\n\u0014\nln\nZ\nn\nY\ni=1\np(xi, yi|w)ϕ(w|η)dw\n\u0015\n=En\n\u0014\nn\nX\ni=1\nln p(xi, yi| ˆwxy)\n\u0015\n+ En\n\u0014\nln\nZ\nexp(nr1(w))ϕ(w|η)N ( ˆwxy, (nIxy(w∗))−1)dw\n\u0015\n−\n\u001adim w\n2\nln n −dim w\n2\nln 2π + 1\n2 ln det Ixy(w∗)\n\u001b\n+ o(1)\n=En\n\u0014\nn\nX\ni=1\nln p(xi, yi| ˆwxy)\n\u0015\n−dim w\n2\nln n\n+ dim w\n2\nln 2π −1\n2 ln det Ixy(w∗) + ln ϕ(w∗) + o(1),\nwhere N (µ, Σ) is a dim w-dimensional Gaussian distribution with mean µ ∈Rdim w\nand variance-covariance matrix Σ. Then, we obtain\nFxy(n) =En\n\u0014\nn\nX\ni=1\nln p(xi, yi|w∗)\np(xi, yi| ˆwxy)\n\u0015\n+ dim w\n2\nln n −dim w\n2\nln 2π + 1\n2 ln det Ixy(w∗) −ln ϕ(w∗|η) + o(1).\nAccording to the Taylor expansion at ˆwxy,\nEn\n\u0014\nn\nX\ni=1\nln p(xi, yi|w∗)\n\u0015\n=En\n\u0014\nn\nX\ni=1\nln p(xi, yi| ˆwxy)\n+ (w∗−ˆwxy)⊤∂\n∂w\nn\nX\ni=1\nln p(xi, .yi| ˆwxy)\n+ 1\n2(w∗−ˆwxy)⊤∂2\n∂w2\nn\nX\ni=1\nln p(xi, yi| ˆwxy)(w∗−ˆwxy) + r2(w)\n\u0015\n=En\n\u0014\nn\nX\ni=1\nln p(xi, yi| ˆwxy)\n\u0015\n−n\n2En\n\u0014\n(w∗−ˆwxy)⊤Ixy(w∗)(w∗−ˆwxy)\n\u0015\n+ o(1),\nwhere r2(w) is the remainder term. The estimator ˆwxy has asymptotic normality,\nand it converges to the Gaussian distribution with mean w∗and variance–covariance\n20\nmatrix (nIxy(w∗))−1. It holds that\nEn\n\u0014\nn\nX\ni=1\nln p(xi, yi|w∗)\n\u0015\n=En\n\u0014\nn\nX\ni=1\nln p(xi, yi| ˆwxy)\n\u0015\n−dim w\n2\n+ o(1).\nThe free-energy function has the asymptotic form\nFxy(n) = −dim w\n2\n+ dim w\n2\nln n −dim w\n2\nln 2π\n+ 1\n2 ln det Ixy(w∗) −ln ϕ(w∗|η) + o(1).\nIn the same way, we obtain that\nFxy,x(n) = −dim w\n2\n+ dim w\n2\nln n −dim w\n2\nln 2π\n+ 1\n2 ln det{αIxy(w∗) + (1 −α)Ix(w∗)} −ln ϕ(w∗|η) + o(1).\nBased on the relation in Eq. 10, we obtain that\n(1 −α)nD3(n) =1\n2 ln det\n\u001a\nIxy(w∗)\n\u0000αIxy(w∗) + (1 −α)Ix(w∗)\n\u0001−1\n\u001b\n+ o(1),\nwhich is the asymptotic form of D3(n).\nDeﬁne the free-energy functions by\nFy|x,xy(n) =En\n\u0014\nln\nαn\nY\ni=1\nq(yi|xi)\nn\nY\ni=αn+1\nq(xi, yi)\n−ln\nZ\nαn\nY\ni=1\np(yi|xi, w)\nn\nY\ni=αn+1\np(xi, yi|w)ϕ(w|η)dw\n\u0015\n,\nFy|x,x(n) =En\n\u0014\nln\nαn\nY\ni=1\nq(yi|xi)\nn\nY\ni=αn+1\nq(xi)\n−ln\nZ\nαn\nY\ni=1\np(yi|xi, w)\nn\nY\ni=αn+1\np(xi|w)ϕ(w|η)dw\n\u0015\n.\nThe error function can be rewritten as\n(1 −α)nD2(n) =Fy|x,xy(n) −Fy|x,x(n).\n(11)\nThe maximum-likelihood estimator is deﬁned by\nˆwy|x,xy = arg max\nw\nαn\nY\ni=1\np(yi|xi, w)\nn\nY\ni=αn+1\np(xi, yi|w).\n21\nDue to (A2), the estimators ˆwy|x,xy and ˆw2 converge to w∗, which means that the\nessential parameter area for the integration is the neighborhood of w∗, ˆwy|x,xy, and\nˆw2. According to the Taylor expansion and the saddle-point approximation, the\nfree-energy functions have the following asymptotic forms:\nFy|x,xy(n) = −dim w\n2\n+ dim w\n2\nln n −dim w\n2\nln 2π\n+ 1\n2 ln det{Ixy(w∗) −αIx(w∗)} −ln ϕ(w∗|η) + o(1),\nFy|x,xy(n) = −dim w\n2\n+ dim w\n2\nln n −dim w\n2\nln 2π\n+ 1\n2 ln det{αIxy(w∗) + (1 −2α)Ix(w∗)} −ln ϕ(w∗|η) + o(1).\nBased on the relation in Eq. 11, we obtain that\n(1 −α)nD2(n) =1\n2 ln det\n\u001a\nK21(w∗)K22(w∗)−1\n\u001b\n+ o(1),\nK21(w) =Ixy(w) −αIx(w),\nK22(w) =αIxy(w) + (1 −2α)Ix(w),\nwhich is the asymptotic form of D3(n). (End of Proof)\nProof of Theorem 4\nAccording to the condition, let the eigenvalues of I(w∗)Ix(w∗)−1 be\nσ1 ≥σ2 ≥· · · ≥σ ¯d > 0,\nσ ¯d+1 = · · · = σd = 0,\nwhere ¯d = dim ¯w. First, we compare D1(n) and D3(n). Focusing on the factor\nln det K3(w∗) of the dominant term in D3(n), we obtain\nln det K3(w∗) = ln det Ixy(w∗) −ln det\n\b\nαIxy(w∗) + (1 −α)Ix(w∗)\n\t\n= ln det\n\b\nI(w∗) + Ix(w∗)\n\t\n−ln det\n\b\nαI(w∗) + Ix(w∗)\n\t\n= ln det\n\b\nI(w∗)Ix(w∗)−1 + E\n\t\n−ln det\n\b\nαI(w∗)Ix(w∗)−1 + E\n\t\n=\nd\nX\ni=1\nln(σi + 1) −\nd\nX\ni=1\nln(ασi + 1)\n=\n¯d\nX\ni=1\nln σi + 1\nασi + 1\n= ¯d ln 1\nα +\n¯d\nX\ni=1\nln\n\u0012\n1 + 1 −1/α\nσi + 1/α\n\u0013\n,\n22\nwhere E is the d×d unit matrix and the relation I(w) = Ixy(w)−Ix(w) was applied.\nBecause\n1−1/α\nσi+1/α < 0, the second term in the last expression is less than zero. Thus,\nln det K3(w∗) < ¯d ln 1/α,\nwhich shows that\nD3(n) −D1(n) =1\n2\n\b\nln det K3(w∗) −¯d ln 1/α\n\t\n1\n(1 −α)n + o\n\u00121\nn\n\u0013\n< 0.\nNext, we compare D1(n) and D2(n).\nln det K2(w) = ln det\n\b\nIxy(w∗) −αIx(w∗)\n\t\n−ln det\n\b\nαIxy(w∗) + (1 −2α)Ix(w∗)\n\t\n= ln det\n\b\nI(w∗) + (1 −α)Ix(w∗)\n\t\n−ln det\n\b\nαI(w∗) + (1 −α)Ix(w∗)\n\t\n= ln det\n\b\nI(w∗)Ix(w∗)−1 + (1 −α)E\n\t\n−ln det\n\b\nαI(w∗)Ix(w∗)−1 + (1 −α)E\n\t\n=\nd\nX\ni=1\nln(σi + (1 −α)) −\nd\nX\ni=1\nln(ασi + (1 −α))\n= ¯d ln 1\nα +\n¯d\nX\ni=1\nln\nσi + 1 −α\nσi + (1 −α)/α\n= ¯d ln 1\nα +\n¯d\nX\ni=1\nln\n\u0012\n1 + (1 −α)(1 −1/α)\nσi + (1 −α)/α\n\u0013\n= ¯d ln 1\nα +\n¯d\nX\ni=1\nln\n\u0012\n1 +\n1 −1/α\nσi/(1 −α) + 1/α\n\u0013\n.\nBecause\n1−1/α\nσi/(1−α)+1/α < 0, the second term in the last expression is less than zero.\nThus,\nln det K2(w∗) < ¯d ln 1/α,\nwhich shows that\nD2(n) −D1(n) =1\n2\n\b\nln det K2(w) −¯d ln 1/α\n\t\n1\n(1 −α)n + o\n\u00121\nn\n\u0013\n< 0.\nComparing K3(w∗) and K2(w∗), we ﬁnd that\n¯d\nX\ni=1\nln\n\u0012\n1 + 1 −1/α\nσi + 1/α\n\u0013\n<\n¯d\nX\ni=1\nln\n\u0012\n1 +\n1 −1/α\nσi/(1 −α) + 1/α\n\u0013\n.\nTherefore, ln det K3(w∗) < ln det K2(w∗), which shows that D3(n) < D2(n). (End\nof Proof)\n23\nProof of Lemma 5\nFocusing on the factor ln det K4(w∗) of the dominant term in DNL(n), we obtain\nln det K4(w∗) = ln det\n\u0000αE + (1 −α)Ixy(w∗)Ix(w∗)−1\u0001\n=\nd\nX\ni=1\nln(α + (1 −α)λi),\nwhich proves Eq.7. Using the eigen values λi, we rewrite the factor of the dominant\nterm ln det K3(w∗) as\nln det K3(w∗) =Ixy(w∗)(αIxy(w∗) + (1 −α)Ix(w∗))−1\n= −ln det(αE + (1 −α)Ix(w∗)Ixy(w∗)−1)\n= −\nd\nX\ni=1\nln\n\u0012\nα + (1 −α) 1\nλi\n\u0013\n.\nThe diﬀerence of the coeﬃcients in DNL(n) −D3(n) is expressed as\nln det K4(w∗) −ln det K3(w∗) =\nd\nX\ni=1\n\u001a\nln(α + (1 −α)λi) + ln\n\u0012\nα + (1 −α) 1\nλi\n\u0013\u001b\n=\nd\nX\ni=1\nln\n\u0012\nα + (1 −α)λi\n\u0013\u0012\nα + (1 −α) 1\nλi\n\u0013\n=\nd\nX\ni=1\nln\n\u001a\nα(1 −α)\n\u0012\nλi + 1\nλi\n\u0013\n+ α2 + (1 −α)2\n\u001b\n,\nwhich shows the diﬀerence in Eq. 8 and DNL(n) > D3(n). (End of Proof)\nReferences\nAkaike, H. (1974). A new look at the statistical model identiﬁcation. IEEE Trans.\non Automatic Control, 19, 716–723.\nAttias, H. (1999). Inferring parameters and structure of latent variable models by\nvariational Bayes. In Proceedings of Uncertainty in Artiﬁcial Intelligence.\nBeal, M. J. (2003). Variational algorithms for approximate bayesian inference. Tech-\nnical report.\nClarke, B. and Barron, A. R. (1990). Information-theoretic asymptotics of bayes\nmethods. IEEE Transactions on Information Theory, 36, 453–471.\n24\nDempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from\nincomplete data via the em algorithm. Journal of the Royal Statistical Society,\nSeries B, 39(1), 1–38.\nGhahramani, Z. and Beal, M. J. (2000). Graphical models and variational methods.\nIn Advanced Mean Field Methods - Theory and Practice. MIT Press.\nLevin, E., Tishby, N., and Solla, S. (1990). A statistical approaches to learning and\ngeneralization in layered neural networks. Proceedings of IEEE, 78(10), 1568–\n1674.\nRissanen, J. (1986). Stochastic complexity and modeling. Annals of Statistics, 14,\n1080–1100.\nSchwarz, G. E. (1978). Estimating the dimension of a model. Annals of Statistics,\n6 (2), 461–464.\nTakeuchi, K. (1976). Distribution of information statistics and criteria for adequacy\nof models. Mathematical Science, 153, 12–18. in Japanese.\nWhite, H. (1982). Maximum likelihood estimation of misspeciﬁed models. Econo-\nmetrica, 50(1), 1–25.\nYamazaki, K. (2014).\nAsymptotic accuracy of distribution-based estimation for\nlatent variables. Journal of Machine Learning Research, 13, 3541–3562.\nYamazaki, K. (2015a). Accuracy analysis of semi-supervised classiﬁcation when the\nclass balance changes. Neurocomputing. to appear.\nYamazaki, K. (2015b). Asymptotic accuracy of Bayes estimation for latent variables\nwith redundancy. Machine Learning. to appear.\nZhu, X. (2007).\nSemi-supervised learning literature survey.\nTechnical Report\nTR1530, Computer Science, University of Wisconsin Madison.\n25\n",
  "categories": [
    "stat.ML"
  ],
  "published": "2013-08-09",
  "updated": "2015-03-25"
}