{
  "id": "http://arxiv.org/abs/1311.0707v3",
  "title": "Generative Modelling for Unsupervised Score Calibration",
  "authors": [
    "Niko Brümmer",
    "Daniel Garcia-Romero"
  ],
  "abstract": "Score calibration enables automatic speaker recognizers to make\ncost-effective accept / reject decisions. Traditional calibration requires\nsupervised data, which is an expensive resource. We propose a 2-component GMM\nfor unsupervised calibration and demonstrate good performance relative to a\nsupervised baseline on NIST SRE'10 and SRE'12. A Bayesian analysis demonstrates\nthat the uncertainty associated with the unsupervised calibration parameter\nestimates is surprisingly small.",
  "text": "GENERATIVE MODELLING FOR UNSUPERVISED SCORE CALIBRATION\nNiko Br¨ummer∗\nAGNITIO Research\nSomerset West, South Africa\nDaniel Garcia-Romero\nHLTCOE, Johns Hopkins University\nBaltimore, MD, USA\nABSTRACT\nScore calibration enables automatic speaker recognizers to make\ncost-effective accept / reject decisions. Traditional calibration re-\nquires supervised data, which is an expensive resource. We propose\na 2-component GMM for unsupervised calibration and demonstrate\ngood performance relative to a supervised baseline on NIST SRE’10\nand SRE’12. A Bayesian analysis demonstrates that the uncertainty\nassociated with the unsupervised calibration parameter estimates is\nsurprisingly small.\nIndex Terms— calibration, unsupervised learning, Laplace ap-\nproximation, automatic speaker recognition\n1. INTRODUCTION\nAutomatic speaker recognizers map trials to scores. A trial has two\nparts: some speech of a known speaker, and some of an unknown\nspeaker. When the same speaker is present in both parts, we have\na target trial. When the speakers differ, we have a non-target trial.\nThe score is a real number, which is expected to be larger (more\npositive) for target trials and smaller (more negative) for non-target\ntrials. When a speaker recognizer is deployed in a new environment,\nwhich may differ from previously seen environments w.r.t. factors\nlike language, demographics, vocal effort, noise level, microphone,\ntransmission channel, duration, etc., the behaviour of the scores may\nchange. Although the scores can still be expected to discriminate\nbetween targets and non-targets in the new environment, score dis-\ntributions could change between environments.\nIf scores are to be used to make hard decisions, then we need\nto calibrate the scores for the appropriate environment. The ideal\ncalibration of a score, s, would be of the form:\ns →log\nP(s|target, environment)\nP(s|non-target, environment)\nbut of course, we are not given these score distributions. Our only\nresource would be some data collected from the new environment.\nIn special cases (usually involving considerable expense), this data\ncan be supervised, such that each trial is labelled as target or non-\ntarget. In a more realistic scenario however, all or most of this data\nwould be unsupervised.\nTo date, most works on calibration have made use of supervised\ndata. In this paper, we explore the problem of calibration where our\nonly resource is a large database of completely unsupervised scores.\n2. CALIBRATION MODEL\nIn the supervised setting, score calibration can be viewed as a\nstraight-forward, 2-class pattern recognition problem, for which\n∗The experiments were done while attending the CLSP 2013 Speaker\nRecognition Workshop at Johns Hopkins University.\nboth generative and discriminative solutions exist [1, 2, 3, 4, 5].\nFor the unsupervised case, we found the generative approach more\nconvenient.\nHere we introduce the score model for supervised\ncalibration, followed by a generalization to the unsupervised case.\n2.1. Supervised calibration model\nFor the supervised case, we adopt the simple generative model of [4].\nDenoting targets as H1 and non-targets as H2, we model a score\ns ∈R, with class-conditional Gaussian distributions:\nP(s|Hi, C) = N(s|µi, σ2)\n(1)\nwhere µ1, µ2 are class-conditional means and σ2 is the common\nwithin-class variance. We collectively refer to C = (µ1, µ2, σ2)\nas the calibration parameters. This model gives an afﬁne calibration\ntransformation, from score to log-likelihood-ratio, of the form:\nlog R(s|C) = log N(s|µ1, σ2)\nN(s|µ2, σ2) = d′\nσ s + µ2\n2 −µ2\n1\n2σ2\n(2)\nwhere d′, the separation between targets and non-targets [6],\nd′ = µ1 −µ2\nσ\n,\n(3)\nrepresents accuracy, since the theoretical equal-error-rate is EER =\nΦ(−d′\n2 ).1 At complete overlap, d′ = 0 and EER = 0.5. As d′\nincreases, the EER decreases.\nWe refer to R(s|C) as the plug-in LR, because it can be calcu-\nlated only if C is given. In reality, these parameters are not given, so\nthey must be estimated before being plugged into (2). When labelled\ncalibration training scores are given, maximum-likelihood parame-\nter estimates are straight-forward—see [4], where this plug-in recipe\nis shown to give similar accuracy to logistic regression calibration.\n2.2. Unsupervised calibration model\nIn the unsupervised case, we are given a collection of T training\nscores, denoted S = s1, . . . , sT , but we are not given the corre-\nsponding class labels. Denoting these labels as L = ℓ1, . . . , ℓT ,\nwe treat them as hidden variables and our calibration model gener-\nalizes to a 2-component Gaussian mixture model (GMM), for which\nwe need an additional mixture-weight parameter, π1. Letting π2 =\n1 −π1, the GMM likelihood is:\nP(S|M) =\nT\nY\nt=1\n2\nX\ni=1\nπiN(st|µi, σ2)\n(4)\n1Φ is the normal cumulative density, given in terms of the error-function\nas Φ(x) = (1 + erf(x/\n√\n2))/2.\narXiv:1311.0707v3  [stat.ML]  14 Feb 2014\nwhere M = (C, π1) = (µ1, µ2, σ2, π1) are the GMM parameters.\nNow consider a new test score, s′, generated by the same model,\nand with associated hidden class label ℓ′ ∈{H1, H2}. The condi-\ntional dependency structure of all the parameters and variables can\nbe summarized in graphical model [7] notation as:\nπ1 →L →S ←C →s′ ←ℓ′ ←π′\n1\n(5)\nwhere we have introduced an independent prior, π′\n1, for ℓ′. The im-\nportance of this diagram cannot be overstressed. It is used repeatedly\nbelow, to be able to remove irrelevant conditioning terms.2\nOur end-goal will be to infer the value of ℓ′, when S and s′ are\nobserved and π′\n1 is given, but where π1, L, C are unknown nuisance\nvariables. The result could be given as the posterior P(ℓ′|S, s′, π′\n1),\nor equivalently3, as the predictive likelihood-ratio:\nR(s′|S) = P(s′|ℓ′ = H1, S)\nP(s′|ℓ′ = H2, S)\n(6)\nOur end-goal is the calibration of s′ via the mapping:\ns′ →log R(s′|S)\n3. INFERENCE\nHere we discuss computational strategies to compute R(s′|S). The\ncomputation involves summing over all of the hidden labels, L, and\nintegrating out the parameters, M. Unfortunately this cannot be\ndone in closed form. If conjugate priors are used, the parameters can\nbe integrated out in closed form, but this makes the labels depen-\ndent, so that summing them out requires an intractable sum over 2T\nterms. Conversely, if you start with the labels, they can be summed\nout in closed form, using (4), but then the parameters cannot be in-\ntegrated out in closed form. For this work, we shall follow the latter\nroute, because the parameter space is just 4-dimensional, allowing\napproximate integration in this space.\nThe numerator and denominator of (6) are obtained by marginal-\nizing w.r.t. C and simplifying by (5):\nR(s′|S) =\n\nP(s′|C, H1)\n\u000b\nS\n\nP(s′|C, H2)\n\u000b\nS\n(7)\nwhere ⟨⟩S denotes expectation w.r.t. the parameter posterior P(C|S).\nWe show below how to derive P(C|S) from the Laplace approxi-\nmation for P(M|S).\nAlthough we shall use (7) in practice, we develop an interest-\ning alternative form below that helps to theoretically illuminate the\nrelationship between the plug-in and predictive LRs.\n3.1. Plug-in vs predictive LR\nThe predictive likelihoods can be expanded by the product rule as:\nP(s′|S, Hi) = P(s′|C, Hi) ×\nP(C|S)\nP(C|S, s′, Hi)\n(8)\n2Observation at a node with convergent arrows induces dependency be-\ntween variables linked via this node; when not observed, such nodes block\ndependency. Conversely, nodes with divergent or aligned arrows induce de-\npendency when not observed; and block dependency when observed. A node\nis ‘observed’ if it appears to the right of the | in probability notation.\n3To see this, use (5) to ﬁnd: P (H1|S,s′,π′\n1)\nP (H2|S,s′,π′\n1) =\nπ′\n1\n1−π′\n1 × R(s′|S).\nwhere a ratio of parameter posteriors modulates the plug-in like-\nlihood. The numerator is conditioned on the unsupervised scores,\nwhile the denominator is conditioned on one additional supervised\nscore, with assumed label ℓ′ = Hi. Although (8) holds for any value\nof C with non-zero posteriors, we ﬁnd a more convenient form by\ntaking logarithms and the expectation w.r.t. P(C|S) on both sides:\nlog P(s′|S, Hi) =\nZ\nlog P(s′|C, Hi)P(C|S) dC + Di\n(9)\nwhere Di denotes KL-divergence between the posteriors:\nDi =\nZ\nP(C|S) log\nP(C|S)\nP(C|S, s′, Hi) dC\n(10)\nUsing (9) in (6) gives the predictive log-LR as:\nlog R(s′|S) =\n\nlog R(s′|C)\n\u000b\nS + D1 −D2\n(11)\nNotice that:\n\nlog R(s′|C)\n\u000b\nS = s′\nd′\nσ\n\u000b\nS +\n\nµ2\n2 −µ2\n1\n2σ2\n\u000b\nS\n(12)\nwhich remains afﬁne in s′, just like (2). Moreover, if P(C|S) has a\nsharp, dominant4 peak, then\n\nlog R(s′|C)\n\u000b\nS ≈log R(s′| ˆC), where\nˆC is the mode of the dominant peak. Finally, if there are many scores\nin S, then a single additional score s′ that is similar to the scores in\nS, will result in small Di, so that log R(s′|S) ≈log R(s′| ˆC). Only\nif S has very few scores, or s′ is very far away, will the Di cause\nsigniﬁcant non-linearity in log R(s′|S).\nWe already know that we have a large collection of unsupervised\nscores, but it remains to be demonstrated that P(C|S) has a dominant\npeak, which we shall do below, via an experimental exploration of\nthe likelihood. We shall also quantify the sharpness of that peak by\nusing the Laplace approximation.\n3.2. Laplace approximation\nThe Laplace approximation (LA) is ideally suited to approximating\nsharply peaked, low-dimensional posteriors [7, 8]. We have only 4\nparameters and the likelihood is sharply peaked because we have lots\nof data. The only pitfall is that label swapping causes two identical\npeaks in the likelihood. We kill the unwanted peak by assigning a\nprior of the form P(M) ∝u(µ1 −µ2), where u is the unit step\nfunction. We do not need to specify the prior in any more detail, be-\ncause any reasonable prior that we might want to assign here would\nbe effectively constant relative to the sharply peaked likelihood.\nFollowing the standard LA recipe to approximate the posterior\nP(M|S), we deﬁne:\nf(M) = log P(S, M) = log[P(S|M)P(M)]\n(13)\nwhich is computable by (4). Notice P(M|S) ∝ef(M). Let\nˆ\nM\nbe the dominant mode of f and form a 2nd-order Taylor-series ap-\nproximation here. The gradient at the mode is zero, but we need the\nHessian (2nd derivative matrix), denoted Λ. This forms a multivari-\nate Gaussian, approximate posterior:\n˜P(M|S) = N(M| ˆ\nM, −Λ−1)\n(14)\n4By dominant, we mean that the peak contains almost all of the probabil-\nity mass in P(C|S).\nSince (7) calls for P(C|S), we still have to marginalize5 (14) w.r.t.\nπ1. With the Gaussian approximation this is easy. Let C = −Λ−1\ndenote the covariance of ˜P(M|S) = N(M| ˆ\nM, C), then the corre-\nsponding marginal ˜P(C|S) is also multivariate Gaussian, where the\nelements in ˆ\nM and C corresponding to π1 have been removed [7].\nIn summary, we get the 4-dimensional mode and Hessian, invert the\nHessian and then discard one dimension.\n3.2.1. Model parametrization\nThe LA is not invariant to parametrization [8]. Moreover, it is ob-\nvious that the true posterior for the parameters π1 and σ2 cannot be\nGaussian. But, for a sharply peaked posterior, the parametrization is\nnot that important and the behaviour far from the maximum is almost\nirrelevant. As long as f(M) is smooth enough so that a 2nd-order\napproximation is accurate close to the maximum, the posterior peak\nwill be approximately Gaussian. If the likelihood magnitudes are\nlarge, then by the time the 2nd-order approximation becomes inac-\ncurate, this inaccuracy becomes irrelevant because of the effect of\nthe exponentiation. The reader is encouraged to consult Wikipedia,6\nwhere this is graphically illustrated. Our experimental results below\nare reported for the parametrization [µ1, µ2, log(σ2), log(π1)].\n4. EXPERIMENTS\nWe used two score corpora, DAC and ABC:\nThe Domain Adaptation Challenge (DAC)7 has telephone speech\nfrom the LDC’s Switchboard and Mixer databases as well as NIST\nSRE’10 [9]. It has three parts:\nRecognizer: Switchboard was used to train the hyperparameters\nof an i-vector PLDA speaker recognizer [10], which was then used\nto produce the scores below.\nCalibration: About 7 million trials (single enrollment), from\npre-SRE’10 Mixer corpora, provided the unsupervised scores, S,\nwith target proportion about 4% and (empirical) EER = 2.38%.\nEvaluation: About 400 000 trials, composed of pairs of seg-\nments from SRE’10, provided the test scores, s′, with EER =\n5.54%.\nThe ABC corpus used a different speaker recognizer, applied to a\ndifferent data set, drawn from the AGNITIO-BUT-CRIM submis-\nsion to NIST SRE’12 [11, 12]. The mixture of conditions was more\ndiverse than for DAC, having telephone and microphone speech,\nvariable number of enrollment segments, full and truncated test seg-\nments and varied noise levels.\nRecognizer: Switchboard, Fisher and pre-SRE’12 Mixer was\nused to train an i-vector PLDA system.\nCalibration: About 42 million scores, S, from pre-SRE’12\nMixer. The target proportion is about 0.07%, and EER = 2.38%.\nEvaluation: About 9 million test scores, s′, from SRE’12, with\nEER = 3.25%.\n4.1. Exploration of likelihood\nThe success of the whole venture depends critically on the behaviour\nof the GMM likelihood, P(S|M), given by (4). If we are a-priori\n5Recall M = (C, π1). By (5), π1 and C are dependent in P(C, π1|S),\nso we cannot just ignore π1.\n6en.wikipedia.org/wiki/Laplace_approximation\n7www.clsp.jhu.edu/workshops/archive/ws-13/.\nvery uncertain about the proportion of targets in the unsupervised\ndata, and also about the accuracy of the recognizer, it is not at all\nobvious whether there is enough information in the likelihood8 to be\nable to infer calibration parameters with a useful level of accuracy.\nMoreover, since our inference tools (plug-in and LA) both rely on\nﬁnding likelihood optima, it is important to know whether the likeli-\nhood is plagued by local optima.9\nTo learn how the likelihood behaves, we did an exhaustive ex-\nperimental exploration of the parameter space, (µ1, µ2, σ2, π1). To\nfacilitate visual representation, we used a 2-dimensional model rep-\nresentation, namely (d′, log\nπ1\n1−π1 ), which we plotted against log-\nlikelihood, where the remaining two degrees of freedom were opti-\nmized.10 Recall from section 2.1 that d′ represents accuracy, while\nπ1 represents target proportion. If the likelihood has a single dom-\ninant peak for these two critical parameters, then there is hope that\nthe calibration exercise will work.\nWe made such plots for the calibration parts of DAC and ABC.\nThe results are similar. In ﬁgure 1 we show the latter, which we\nbelieve is more challenging, because of the smaller target proportion.\nThe log-likelihood is smooth as a function of d′, but is bi-modal as\na function of π1—a warning that initialization for the EM algorithm\nis important. Although the modes look ﬂat, and similar, in the log-\nlikelihood plot, the normalized11 likelihood plot of ﬁgure 2 reveals\nthat there is just a single, sharp, dominant peak, the location of which\nis given in table 2.\n4.2. Analysis of sharpness\nTo approximate P(M|S), we use the LA recipe of section 3.2. The\nmode is found with the EM-algorithm. Complex-step differentia-\ntion [14] and the Pearlmutter trick [15] are used for the Hessian.\nWe ﬁnd the error-bars (posterior standard deviations [13, 8]) for the\nparameters to be suprisingly small:\nparametrization\nµ1\nµ2\nlog(σ2)\nlog(πi)\nABC error-bars\n0.0226\n0.0004\n0.0002\n0.0071\nDAC error-bars\n0.1183\n0.0192\n0.0006\n0.0022\n4.3. Calibration experiments\nThe sharpness of the parameter posterior shows there is no practi-\ncal difference between the plug-in and predictive likelihood-ratios.\nWe therefore proceed to report our ﬁnal experimental results for\nmaximum-likelihood plug-in calibration. We estimated the param-\neters on the calibration parts of DAC (7 million scores, 4% targets)\nand ABC (42 million scores, 0.07% targets). The performance of\nthese calibrations was tested on the independent evaluation parts of\nthose corpora, in terms of normalized Bayes error-rate, also known\nas normalized DCF [16]:\nnormDCF(π′\n1) = π′\n1Pmiss(π′\n1) + (1 −π1)′Pfa(π′\n1)\nmin(π′\n1, 1 −π′\n1)\n(15)\n8The likelihood function, P(S|M), represents all of the information that\nour chosen model can extract from S.\n9This true even for more sophisticated Bayesian tools, like variational\nBayes and Gibbs sampling [7].\n10Integrating them out using LA would also be feasible, but we found this\nunnecessary—when d′ and π1 are ﬁxed, there remains very little uncertainty\nabout the scale and location. The constrained optimization was done with\na bespoke EM algorithm, where the M-step had to make use of numerical\noptimization.\n11Subtract the maximum over the graph and then exponentiate [13].\n0\n2\n4\n6\n8\n−10\n−5\n0\n5\n−12\n−10\n−8\n−6\nx 10\n7\nlogit π1\nd’\nlog likelihood\nFig. 1. log P(S|M) for ABC\n0\n2\n4\n6\n8\n−10\n−5\n0\n5\n0\n0.5\n1\nlogit π1\nd’\nlikelihood\nFig. 2. normalized P(S|M) for ABC\nwhere Pmiss(π′\n1) and Pfa(π′\n1) are the empirical miss and false-alarm\nerror-rates obtained when using the log-likelihood-ratios to make\nBayes decisions at the theoretical threshold, −logit(π′\n1). The de-\nnominator is the Bayes error-rate for the default decision that always\naccepts, or always rejects, depending only on π′\n1. Smaller values of\nnormDCF are better, while a value of smaller than one shows the\nrecognizer is doing better than the default decision.\nTable 1 reports normDCF for 4 different values of π′\n1. For both\ndatabases, we compare the supervised recipe of [4] against the pro-\nposed unsupervised recipe. The supervised method used the same\ndata as the unsupervised recipe, except that the labels were sup-\nplied. We also report minDCF, which uses an empirically optimized\nthreshold at each operating point, where the optimization makes use\nof the evaluation labels. Finally, for DAC, we also report results for\na supervised calibration trained on the mismatched12, Switchboard\ndata. The high error-rates for this case emphasizes the need for cali-\nbration on matched data.\nSurprisingly, for the DAC database, the unsupervised method\n12Switchboard is at least a decade older than Mixer, during which time\ntelephony changed dramatically [17].\ndoes mostly better than the supervised one. This may be because of\nerrors13 in the labels supplied to the supervised method.\nIn an effort to test whether our method holds up for very low\ntarget proportions, we noticed that we could go as far as removing\nall trials labelled as targets, so that S contained only trials labelled as\nnon-targets. These entries in the table are marked as unsupervised*.\nThe fact that calibration still works in these cases can perhaps also\nbe attributed to labelling errors.\nFor additional insight, tables 2 and 3 compare the estimates of\nmodel and calibration parameters for the supervised and unsuper-\nvised cases.\nπ′\n1\n0.001\n0.01\n0.1\n0.5\nABC supervised\n0.32\n0.22\n0.13\n0.08\nABC unsupervised\n0.33\n0.24\n0.16\n0.11\nABC unsupervised*\n0.32\n0.23\n0.15\n0.10\nABC minDCF\n0.31\n0.21\n0.12\n0.06\nDAC mismatched\n0.73\n0.54\n0.35\n0.21\nDAC supervised\n0.63\n0.44\n0.28\n0.13\nDAC unsupervised\n0.65\n0.43\n0.25\n0.11\nDAC unsupervised*\n0.65\n0.43\n0.24\n0.12\nDAC minDCF\n0.63\n0.42\n0.24\n0.11\nTable 1. Calibration performance in terms of normDCF.\nµ1\nµ2\nσ\nd′\nπ1\nABC super\n8.2\n-5.9\n2.9\n4.9\n6.6e-4\nABC unsup\n9.9\n-5.9\n2.9\n5.5\n5.6e-4\nABC unsup*\n9.6\n-5.9\n2.9\n5.4\n5.1e-5\nDAC super\n34.0\n-169.3\n48.4\n4.2\n3.9E-2\nDAC unsup\n45.9\n-168.7\n48.8\n4.4\n3.4E-2\nDAC unsup*\n72.3\n-169.3\n48.0\n5.0\n1.4E-5\nTable 2. GMM parameter estimates\nscale\noffset\nABC super\n1.7\n-2.0\nABC unsup\n1.9\n-3.8\nABC unsup*\n1.9\n-3.5\nDAC super\n0.087\n5.9\nDAC unsup\n0.090\n5.5\nDAC unsup*\n0.105\n5.1\nTable 3. Calibration parameters\n5. CONCLUSION\nThe outcome of this work held two surprises for us. The ﬁrst is that\nunsupervised calibration works at all. The second is that the miss-\ning labels contribute surprisingly little uncertainty to the parameter\nestimates.\nFor future work on different data, we caution against blind ap-\nplication of the plug-in recipe. We feel that some Bayesian analysis\nsimilar to ours should also be done to illuminate the interaction be-\ntween model and data.\n13Some Mixer subjects registered multiple times, with different PINs,\nthereby causing some target trials to be falsely labelled as non-targets.\n6. REFERENCES\n[1] Daniel\nRamos-Castro,\nJoaquin\nGonzalez-Rodriguez,\nChristophe Champod,\nJulian Fierrez-Aguilar,\nand Javier\nOrtega-Garcia,\n“Between-source modelling for likelihood\nratio computation in forensic biometric recognition,”\nin\nProceedings of the 5th international conference on Audio- and\nVideo-Based Biometric Person Authentication, Berlin, Heidel-\nberg, 2005, AVBPA’05, pp. 1080–1089, Springer-Verlag.\n[2] Niko Br¨ummer and Johan A. du Preez,\n“Application-\nindependent evaluation of speaker detection,”\nComputer\nSpeech and Language, vol. 20, no. 2–3, pp. 230–275, 2006.\n[3] Niko Br¨ummer, Luk´aˇs Burget, Jan “Honza” Luk´aˇs, Ondˇrej\nGlembek, Frantiˇsek Gr´ezl, Martin Karaﬁ´at, David A. van\nLeeuwen, Pavel Matˇejka, Petr Schwarz, and Albert Strasheim,\n“Fusion of heterogenous speaker recognition systems in the\nSTBU submission for the NIST speaker recognition evaluation\n2006,” IEEE Transactions on Audio, Speech, and Language\nProcessing, vol. 15, no. 7, pp. 2072–2084, Sept. 2007.\n[4] David van Leeuwen and Niko Br¨ummer, “The distribution of\ncalibrated likelihood ratios,” in Interspeech, 2013.\n[5] Niko Br¨ummer and George Doddington, “Likelihood-ratio cal-\nibration using prior-weighted proper scoring rules,” in Inter-\nspeech, 2013.\n[6] D.M. Green and J.A. Swets, Signal Detection Theory and Psy-\nchophysics, New York: Wiley, 1966.\n[7] Christopher M. Bishop,\nPattern Recognition and Machine\nLearning (Information Science and Statistics), Springer, 2007.\n[8] David J. C. MacKay,\nInformation Theory, Inference, and\nLearning Algorithms), Cambridge University Press, 2003.\n[9] The National Institute of Standards and Technology,\n“The\nNIST\nyear\n2010\nspeaker\nrecognition\nevaluation\nplan,”\nwww.itl.nist.gov/iad/mig//tests/sre/2010/\nNIST_SRE10_evalplan.r6.pdf, Apr. 2010.\n[10] D. Garcia-Romero and C. Y. Espy-Wilson,\n“Analysis of i-\nvector length normalization in speaker recognition systems,”\nin Interspeech, Florence, Italy, August 2011, pp. 249–252.\n[11] AGNITIO, BUT, and CRIM, “ABC SRE12 presentation,” in\nNIST SRE 2012 Workshop, Orlando, 2012.\n[12] The National Institute of Standards and Technology,\n“The\nNIST\nyear\n2012\nspeaker\nrecognition\nevaluation\nplan,”\nwww.nist.gov/itl/iad/mig/upload/NIST_\nSRE12_evalplan-v17-r1.pdf, 2012.\n[13] D. S. Sivia, Data Analysis: A Bayesian Tutorial, Oxford Uni-\nversity Press, 1996.\n[14] Joaquim R. R. A. Martins, Peter Sturdza, and Juan J. Alonso,\n“The complexstep derivative approximation,” ACM Transac-\ntions on Mathematical Software, p. 262.\n[15] Barak A. Pearlmutter, “Fast exact multiplication by the Hes-\nsian,” Neural Computation, vol. 6, pp. 147–160, 1994.\n[16] Niko Br¨ummer and Edward de Villiers,\n“The BOSARIS\nToolkit: Theory, algorithms and code for surviving the new\nDCF,” in NIST SRE’11 Analysis Workshop, Atlanta, 2011.\n[17] Christopher Cieri, Linda Corson, David Graff, and Kevin\nWalker,\n“Resources for new research directions in speaker\nrecognition: The Mixer 3, 4 and 5 corpora,” in Interspeech,\n2007.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2013-11-04",
  "updated": "2014-02-14"
}