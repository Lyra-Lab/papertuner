{
  "id": "http://arxiv.org/abs/1311.3175v1",
  "title": "Architecture of an Ontology-Based Domain-Specific Natural Language Question Answering System",
  "authors": [
    "Athira P. M.",
    "Sreeja M.",
    "P. C. Reghu Raj"
  ],
  "abstract": "Question answering (QA) system aims at retrieving precise information from a\nlarge collection of documents against a query. This paper describes the\narchitecture of a Natural Language Question Answering (NLQA) system for a\nspecific domain based on the ontological information, a step towards semantic\nweb question answering. The proposed architecture defines four basic modules\nsuitable for enhancing current QA capabilities with the ability of processing\ncomplex questions. The first module was the question processing, which analyses\nand classifies the question and also reformulates the user query. The second\nmodule allows the process of retrieving the relevant documents. The next module\nprocesses the retrieved documents, and the last module performs the extraction\nand generation of a response. Natural language processing techniques are used\nfor processing the question and documents and also for answer extraction.\nOntology and domain knowledge are used for reformulating queries and\nidentifying the relations. The aim of the system is to generate short and\nspecific answer to the question that is asked in the natural language in a\nspecific domain. We have achieved 94 % accuracy of natural language question\nanswering in our implementation.",
  "text": "International Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \nDOI : 10.5121/ijwest.2013.4403                                                                                                                    31 \n \n \nArchitecture of an Ontology-Based Domain-\nSpecific Natural Language Question  \nAnswering System \n \nAthira P. M., Sreeja M. and P. C. Reghuraj \n \nDepartment of Computer Science and Engineering, Government Engineering College, \nSreekrishnapuram, Palakkad Kerala, India, 678633 \n \n \nABSTRACT \n \nQuestion answering (QA) system aims at retrieving precise information from a large collection of \ndocuments against a query. This paper describes the architecture of a Natural Language Question \nAnswering (NLQA) system for a specific domain based on the ontological information, a step towards \nsemantic web question answering. The proposed architecture defines four basic modules suitable for \nenhancing current QA capabilities with the ability of processing complex questions. The first module was \nthe question processing, which analyses and classifies the question and also reformulates the user query. \nThe second module allows the process of retrieving the relevant documents. The next module processes the \nretrieved documents, and the last module performs the extraction and generation of a response. Natural \nlanguage processing techniques are used for processing the question and documents and also for answer \nextraction. Ontology and domain knowledge are used for reformulating queries and identifying the \nrelations. The aim of the system is to generate short and specific answer to the question that is asked in the \nnatural language in a specific domain. We have achieved 94 % accuracy of natural language question \nanswering in our implementation. \n \nKEYWORDS \n \nNatural Language Processing, Question Answering, Ontology, Semantic Role Labeling \n \n1. INTRODUCTION \n \nQuestion Answering is the process of extracting answers to natural language questions. A QA \nsystem takes questions in natural language as input, searches for answers in a set of documents, \nand extracts and frames concise answers. QA systems provide answers to the natural language \nquestions by considering an archive of documents. Instead of providing the precise answers, in \nmost of the current information retrieval systems the users have to select the required information \nfrom a ranked list of documents. Information Extraction (IE) is the name given to any process \nwhich selectively structures and combines data which is found, explicitly stated or implied, in one \nor more texts [5]. After finding the significant documents, the IR system submits those to the \nuser. The scope of the QA has been constrained to domain specific systems, due to the \ncomplications in natural language processing (NLP) techniques [4]. Current search engines can \nreturn ranked lists of documents, but not the answers to the user queries. \n \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n32 \nNovice users may lack adequate knowledge in the domain of search, so the query framed by them \nmay not meet the information needs. Moreover, the query that the users often codify captures \nmany documents that are irrelevant, and also fails to find the knowledge or relationships that are \nhidden in the articles. To overcome this drawback, many systems provide various facilities such \nas relevance feed-back, with which searchers can find out the documents that are of interest to \nthem. With these questions about the current techniques in mind, a new querying approach can be \ndeveloped based on domain specific ontologies and some NLP techniques for better results [7]. \nAlso syntactic analysis based on rules and semantic role labeling can be applied to improve both \nquery construction and answer extraction. With this information we will be able to analyze and \nextract structure and meaning from both questions and candidate sentences, which helps us to \nidentify more relevant and precise answers in a long list of candidate sentences [2]. \n \n2. LITERATURE SURVEY \n \nThere has been an impressive rise in the significance in natural language question answering \nsince the establishment of the Question Answering track in the Text Retrieval Conferences, \nbeginning with TREC-8 in 1999 (Voorhees and Harman, 2000). However, this is not the first time \nthat the QA has been discussed by the NLP researchers.  In fact, in 1965 Simmons published a \nsurvey article ’Answering English Questions by Computer’ and his paper analyses about more \nthan fifteen English language question answering systems implemented in the previous  five years \n[5]. A brief history of QA systems starting from database approaches is briefly described in [4]. \n \nQuestion answering systems have traditionally depended on a variety of lexical resources to \nbridge the surface differences between questions and potential answers. Syntactic structure \nmatching has been applied to passage retrieval (Cui et al., 2005) and answer extraction (Shen and \nKlakow, 2006). The significance of semantic roles in answering complex questions was first \nemphasized by Narayanan and Harabagiu in 2004. <Predicate-argument> structures were \nidentified in their system by consolidating the information on semantic roles from PropBank and \nFrameNet. But, the history of semantic and thematic role labeling dates back to ancient period. \nThe classical Sanskrit grammar Astadhyayi, created by the Indian grammarian Panini at a time \nvariously estimated at 600 or 300 B.C., includes a sophisticated theory of thematic structure that \nremains influential till today [2]. \n \nSun et al. successfully use semantic relations to match candidate answers. FREyA (Damljanovic \net al., 2010) a Feedback Refinement and Extended Vocabulary Aggregation system associates the \nmethod of syntactic parsing with ontological information for decreasing the effort of adaptation. \nIn spite of the rule-based systems, their system encodes the knowledge into ontology for a better \nunderstanding of the question posed by the user. Then for getting a more definite answer, the \nsyntactic parsing is incorporated. \n \nOur work focuses on the analysis of questions using both syntactic and semantic methods, \ndecomposing a single complex query into a set of less complex queries using an ontology and \nmorphological expansion. Our approach is different from the works in that we use semantic role \nlabeling and domain knowledge using ontology to analyze questions as well as to find answer \nphrases. Importance is given to both nouns and verbs by extracting the named entities, noun \nphrases and analyzing then using the Verbnet. \n \n \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n33 \n3. QUESTION ANSWERING SYSTEM \n \nQuestion Answering, the process of extracting answers to natural language questions, is \nprofoundly different from Information Retrieval (IR) or Information Extraction (IE). IR systems \npresent the user with a set of documents that relate to their information need, but do not exactly \nindicate the correct answer. In IR, the relevant documents are obtained by matching the keywords \nfrom user query with a set of index terms from the set of documents. In contrast, IE systems \nextract the information of interest provided the domain of extraction is well defined. In IE \nsystems, the required information is built around in presumed templates, in the form of slot-\nfillers.  \n \nThe QA technology takes both IR and IE a step further, and provides specific and brief answers to \nopen domain questions formulated naturally [9]. Current information retrieval systems allow us to \nlocate documents that might contain the pertinent information, but most of them leave it to the \nuser to extract the useful information from a ranked list [10]. \n \nThe Question Answering systems based on a repository of documents have three main \ncomponents. The first is an information retrieval engine that sits on top of the document \ncollection and handles retrieval requests, i.e. a web search engine. The second component is a \nquery interpretation system that deciphers the natural-language questions into keywords or \nqueries for the search engine for fetching the significant documents from the database. That is, \nthe documents that can potentially answer the question [5]. Fine grained information extraction \ntechniques need to be used for pinpointing answers within likely documents. The third \ncomponent, answer extraction, evaluates these documents and extracts answer snippets from them \n[8]. \n \nThe three essential modules in almost all QA systems are question processing (generate a query \nout of the natural language question), document retrieval (perform document level information \nretrieval), and answer extraction and formulation (pinpointing answers) [8]. The general \narchitecture of a QA System is shown in Figure 1. \n \n \n \n \n \n \n \n \n \n \n \nFigure 1:  General Architecture of a NLQA System \n \n1. QUESTION PROCESSING: The objective of this process is to understand the question \nposed by the user, for which analytical operations are performed for the representation \nand classification of the questions.  \n \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n34 \n2. DOCUMENT EXTRACTION & PROCESSING: This module selects a set of relevant \ndocuments and extracts a set of paragraphs depending on the focus of the question. The \nanswer is in terms of these paragraphs.  \n \n3. ANSWER PROCESSING: This module is responsible for selecting the response \nbased on the relevant fragments of the documents. This needs a pre-processing of \nthe information in order to relate the answers with a given question. \n \nMost of the current system uses either syntactic and semantic analysis or ontology processing for \nanswer retrieval. But our system implements the three modules based on a hybrid approach, a \nstep towards natural language question answering in semantic web. The proposed system \nanalyses both the question and answer processing modules using the syntactic and semantic \napproaches, and also uses the domain ontology for relation extraction and identification. The base \nontology is populated dynamically for each document in the collection. And ontology for a \nspecific domain is created as a by-product of the system, which can be used for future analysis \nand processes. \n \n4. THE PROPOSED ARCHITECTURE \n \nThe proposed architecture of an ontology-based domain-specific NLQA system is depicted in \nFigure 2. The model integrates key components such as Natural Language Processing techniques; \nConceptual Indexing based Retrieval Mechanism, and Ontology Processing. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 2:  Proposed Architecture of the OD-NLQA System \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n35 \n4.1. Question Processing \n \nIn the question processing module, with the help of various components, the following actions \nare performed. \n \n· \nAnalysis of the natural language question  \n· \nQuestion classification  \n· \nReformulation of the user query  \n \nA. Query Analyzer  \n \nThe natural-language question given by the user is analyzed using various natural \nlanguage processing techniques. \n· \nSyntactic Analysis – The question is analyzed syntactically using NLP techniques. Part-of-\nspeech tagging and named entity recognition (NER) are performed. Tools such as Python-\nnltk, OpenNLP, Stanford CoreNLP can be used for this purpose. In the proposed system, we \nused Stanford CoreNLP tool-kit. The CoreNLP processes the document and creates an XML \nfile as output. Shallow parsing is performed to identify the phrasal chunks. The phrasal \nchunks can be identified using the Regular-expression chunker and the Conll-2000 trained \nchunker.  \n \n· \nSemantic Analysis – Semantic role labeling is an important step in this module, which \nenables to find the dependencies or restriction that, can be imposed, after getting the user \nquery [6]. This greatly eliminates the chances of irrelevant set of answers. Semantic roles \nare identified using the verbnet frames.  \n \nB. Question Classification  \n \nThe natural-language question needs to be classified into various sets for extracting more \nprecise sets of answers. The following steps are performed by the proposed system: \n \n· \nFocus Identification - The objective of this is to identify the category of response that the \nuser is searching for. The question focus can be identified by looking at the question word \nor a combination of the question word and its succeeding words. Classification of question \nword to question focus is shown in Table 1. For example, both the question word 'when' or \nthe combination 'what time' indicates a temporal aspect which has to be found in the answer \nset.  \nTable 1: Question to Question focus Classification. \n \n \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n36 \n \n \n· \nPhrase or Clause Detection - The phrases and clauses contains the information relevant to \nthe expected answer and the irrelevant set can be easily eliminated. The question phrase can \nbe determined using shallow parsing or chunking. The NP chunk identifies the clauses that \nare to be looked upon in the documents for obtaining the answer set.  \n \n· \nFrame Detection – The semantic roles are identified and mapped to a semantic frame for \nbetter retrieval. For example: The event E “Who gave a balloon to the kid?” has the roles \n“AGENT verb/give THEME to RECIPIENT”, the semantic frame is identified as \n“has_possession(start(E), Agent, Theme ) has_possession(end(E), Recipient, Theme) \ntransfer(during(E), Theme)”. The frames can also be used for ranking of the retrieved \nanswer set.  \n \nC. Query Reformulation  \n \nThe user queries may be reformulated by adding domain knowledge and ontological \ninformation. \n· \nOntology - Ontology is defined in the basic terms and relations comprising the vocabulary \nof a specific area, as well as rules for combining these terms and relations to define \nextensions vocabularies [1]. The base ontology is created for the specific domain by \nincorporating the classes and object properties. The domain, range, and restrictions on the \nclasses are also specified.  \n \n4.2. Document Retrieval  \n \nThis module selects a set of relevant documents from a domain specific repository. Conceptual \nindexing is used for the retrieval process since the key word based indexing ignores the \nsemantic content of the document collection [5]. Both the documents and queries can be mapped \ninto concepts and these concepts are used as a conceptual indexing space for identifying and \nextracting documents. \n \n4.3 Document Processing \n \nThe retrieved documents are processed for extracting candidate answer set. This module is \nresponsible for selecting the response based on the relevant fragments of the documents. \n \n· \nSyntactic Analysis – The documents analyzed syntactically using the NLP techniques \nsuch as part-of-speech tagging and named-entity recognition. In the syntactic analysis, \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n37 \nfirstly the documents are tokenized into set of sentences. Using the Stanford CoreNLP, \nthe POS tagging and NER is performed. Shallow parsing is performed to identify the \nphrasal chunks. The chunks identified in the question analysis module are matched with \nthose identified in the document and relevant sentences are retrieved. \n· \n \n· \nSemantic Analysis – Shallow parsing can be performed for finding the semantic phrases \nor clauses. The semantic roles are identified and mapped to semantic frames. The \nsentences whose semantic frames map exactly to the semantic frames of the question are \nalso extracted.  \n· \n \n· \nRelation Identification - The base ontology is populated with the domain knowledge \nincrementally as we go through different set of documents. By this method a valid \nknowledge on any specialized discipline can be incorporated to the system. The relations \namong different concepts are identified using the domain knowledge and the ontological \ninformation obtained.  \n \n4.4 Answer Extraction  \n \nThe filtering of candidate answer set and answer generation is performed. The user is supplied \nwith a set of short and specific answers ranked according to their relevance. The different stages \nare: \n \n· \nFiltering – The extracted sentences are filtered and the candidate answer set is produced. \nThis is done by incorporating the information obtained from the question classification \nand document processing modules. The identified focus and frames are matched to get \nthe candidate set.  \n \n· \nAnswer Ranking – The answer set is ranked based on the semantic similarity. Simple \ntemplate matching is not adopted since it neglects the semantic content and domain \nknowledge. Answers are ranked based on the similarity between the question frame and \nthe answer frame. Example: The event E “John gave a balloon to the kid.” has the roles \n“AGENT verb/give THEME to RECIPIENT, the semantic frame is identified as \n“has_possession(start(E), Agent, Theme ) has_possession(end(E), Recipient, Theme) \ntransfer(during(E), Theme)” matches exactly with the question frame.  \n \n· \nAnswer Generation – From the answer set, specific answers have to be generated in case \nthe direct answers are not available. Hidden relations can be identified from the domain \nknowledge gathered from the ontology. Concept of natural language generation can also \nbe utilized for this purpose.  \n \n5. EVALUATION AND TESTING  \n \nThe TREC QA test collections contain newswire articles and the accompanying queries cover a \nwide variety of topics [11]. QA evaluation process simply access these TREC collections for \ntesting the efficiency of the systems. Simply applying the open domain QA evaluation paradigm \nto a restricted domain poses several problems. So a different method for evaluation process is \nused in this system. A random set of documents are collected over a specific domain. Relevant, \ncorrect and complete answers are derived for a set of question from some arbitrarily chosen \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n38 \nunbiased users, and this set is used for the testing purposes [4]. The questions are tested for their: \n \n· \nCorrectness - The answer should be factually correct  \n· \nRelevance - The answer should be a response to the question  \n· \nCompleteness - The answer should complete, i.e. a partial answer should not get full \ncredit.  \n \nThe proposed system tries to find precise answers to factual questions and explanative answers \nare not provided. For multiple answers, ranking is provided based on the semantic matching. \nDirect answers are generated using natural language generation techniques form the candidate set \nof answers. \n \nThe system is tested for efficiency using the notion of recall. Recall for a question answering \nsystem is defined as the “ratio of number of correct answers to the total number of questions \ngiven [5].” Answer precision may be subjective, but we have tried to make it as objective as \npossible. \n \nThe system is tested in a domain of short stories and the results are presented in Table 2 and \nTable 3. For the system testing, the document set (collection of short stories) is retrieved \narbitrarily over the web. For the testing we use the question track consisting of 100 questions of \nvarying type complexity and difficulty. It has to be noted that precise answer here indicates the \none-word answer generated as response to a factual question, but for a question answering \nsystem, the answer can be generated as a single sentence and is indicated as retrieved sentences \ncontaining precise answers. \n \nTable 2: % Recall with retrieved precise answers. \n \nNo. of \nTotal size \nNo. of questions \nNo. of correct \n% Recall \ndocuments \nanswers \n20 \n478KB \n50 \n41 \n82 \n50 \n1.2MB \n120 \n97 \n80.8 \n \nTable 3: % Recall considering retrieved sentences containing precise answers. \n \nNo. of \nTotal size \nNo. of questions \nNo. of correct \n% Recall \ndocuments \nanswers \n20 \n478KB \n50 \n47 \n94 \n50 \n1.2MB \n120 \n112 \n93.3 \n \n6. CONCLUSION AND FUTURE WORK \n \nWe have presented an architecture of ontology-based domain-specific natural language question \nanswering that applies semantics and domain knowledge to improve both query construction and \nanswer extraction. The system presented in this paper is a step towards the ultimate goal of using \nInternational Journal of Web & Semantic Technology (IJWesT) Vol.4, No.4, October 2013 \n39 \nthe web as a comprehensive, self-updating knowledge repository, which can be automatically \nmined to answer a wide range of questions with much less effort than is required by todays search \nengines. The experiments show that our system is able to filter semantically matching sentences \nand their relations effectively and therefore, rank the correct answers higher in the result list. \n \nWe intend to extend the coverage of the system to all possible question types i.e. move from the \nfactual to more complex forms of question, including lists, summarization of contradictory \ninformation, and explanations, including answers to how or why questions, and eventually, what \nif questions. Generating short, coherent and precise answers will be a major research area and \nwill rely massively on progress in information extraction and text summarization. Also new \nresearch should be done to gather more information in various levels of understanding, \neffectiveness and situations. \n \nREFERENCES \n \n[1] Abouenour L.,Bouzouba K., Rosso P., (2010) “An evaluated semantic query expansion and \nstructure-based approach for enhancing Arabic question/answering”, International Journal on \nInformation and Communication Technologies, Vol. 3, June 2010  \n \n[2] Akshar B, Rajeev S, (1993) “Parsing Free Word Order Languages in the Paninian Framework, ” In \nProceedings of the 31st annual meeting on Association for Computational Linguistics  \n \n[3] Allen J F , (2007) “ Natural Language Understanding, ” Pearson Education Inc.  \n \n[4] Hirschman L, Gaizauskas R, (2001) “Natural language question answering: the view from here,”  \n \n          Journal of Natural Language Engineering.  \n \n[5] Kwok C, Etzioni O, Weld D S, (2001) “Scaling Question Answering to the Web,” ACM \nTransactions on Information Systems.  \n \n[6] Pizzato L A, Diego M, (2008) “Indexing on Semantic Roles for Question Answering, ” In \nProceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), \nColing.  \n \n[7] Song M., Allen R.B., (2007) ”Integration of association rules and ontologies for semantic query \nexpansion”, Data Knowledge Engineering, Elsevier pp. 6375.  \n \n[8] Sucunuta M E, Riofrio G E, (2010) “Architecture of a Question-Answering System for a Specific \nRepository of Documents, ” In 2nd International Conference on Software Technology and \nEngineering.  \n \n[9] Surdeanu M, Moldovan D, (2003) “On the role of Information Retrieval and Information Extraction \nin Question Answering Systems, “ Information Extraction in Web Era - Springer. \n \n[10] Tuffis D, (2011) “Natural Language Question Answering in Open Domains, ” Computer Science \nJournal of Moldova.  \n \n[11] Yilmazel O, Diekema A.R, Liddy E.D, (2004) “Evaluation of Restricted Domain Question                                      \nAnswering Systems”, Association for Computational Linguistics. \n",
  "categories": [
    "cs.CL",
    "cs.IR"
  ],
  "published": "2013-11-13",
  "updated": "2013-11-13"
}