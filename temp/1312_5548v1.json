{
  "id": "http://arxiv.org/abs/1312.5548v1",
  "title": "My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013",
  "authors": [
    "Jürgen Schmidhuber"
  ],
  "abstract": "Deep Learning has attracted significant attention in recent years. Here I\npresent a brief overview of my first Deep Learner of 1991, and its historic\ncontext, with a timeline of Deep Learning highlights.",
  "text": "My First Deep Learning System of 1991\n+ Deep Learning Timeline 1962-2013\nJ¨urgen Schmidhuber\nThe Swiss AI Lab IDSIA, Galleria 2, 6928 Manno-Lugano\nUniversity of Lugano & SUPSI, Switzerland\n20 December 2013\nAbstract\nDeep Learning has attracted signiﬁcant attention in recent years. Here I present a brief overview of\nmy ﬁrst Deep Learner of 1991, and its historic context, with a timeline of Deep Learning highlights.\nNote: As a machine learning researcher I am obsessed with proper credit assignment. This draft is the\nresult of an experiment in rapid massive open online peer review. Since 20 September 2013, subsequent\nrevisions published under www.deeplearning.me have absorbed many suggestions for improvements by\nexperts. The abbreviation “TL” is used to refer to subsections of the timeline section.\nFigure 1: My ﬁrst Deep Learning system of 1991 used a deep stack of recurrent neural networks (a Neural\nHierarchical Temporal Memory) pre-trained in unsupervised fashion to accelerate subsequent supervised\nlearning [79, 81, 82].\n1\narXiv:1312.5548v1  [cs.NE]  19 Dec 2013\nIn 2009, our Deep Learning Artiﬁcial Neural Networks became the ﬁrst Deep Learners to win ofﬁcial\ninternational pattern recognition competitions [40, 83] (with secret test sets known only to the organisers);\nby 2012 they had won eight of them (TL 1.13), including the ﬁrst contests on object detection in large\nimages (ICPR 2012) [2, 16] and image segmentation (ISBI 2012) [3, 15]. In 2011, they achieved the\nworld’s ﬁrst superhuman visual pattern recognition results [20, 19]. Others have implemented very similar\ntechniques, e.g., [51], and won additional contests or set benchmark records since 2012, e.g., (TL 1.13, TL\n1.14). The ﬁeld of Deep Learning research is far older though—compare the timeline (TL) further down.\nMy ﬁrst Deep Learner dates back to 1991 [79, 81, 82] (TL 1.7). It can perform credit assignment across\nhundreds of nonlinear operators or neural layers, by using unsupervised pre-training for a stack of recurrent\nneural networks (RNN) (deep by nature) as in Figure 1. Such RNN are general computers more powerful\nthan normal feedforward NN, and can encode entire sequences of input vectors.\nThe basic idea is still relevant today. Each RNN is trained for a while in unsupervised fashion to predict\nits next input. From then on, only unexpected inputs (errors) convey new information and get fed to the\nnext higher RNN which thus ticks on a slower, self-organising time scale. It can easily be shown that no\ninformation gets lost. It just gets compressed (much of machine learning is essentially about compression).\nWe get less and less redundant input sequence encodings in deeper and deeper levels of this hierarchical\ntemporal memory, which compresses data in both space (like feedforward NN) and time. There also is a\ncontinuous variant [84].\nOne ancient illustrative Deep Learning experiment of 1993 [82] required credit assignment across 1200\ntime steps, or through 1200 subsequent nonlinear virtual layers. The top level code of the initially unsu-\npervised RNN stack, however, got so compact that (previously infeasible) sequence classiﬁcation through\nadditional supervised learning became possible.\nThere is a way of compressing higher levels down into lower levels, thus partially collapsing the\nhierarchical temporal memory. The trick is to retrain lower-level RNN to continually imitate (predict)\nthe hidden units of already trained, slower, higher-level RNN, through additional predictive output neu-\nrons [81, 79, 82]. This helps the lower RNN to develop appropriate, rarely changing memories that may\nbridge very long time lags.\nThe Deep Learner of 1991 was a ﬁrst way of overcoming the Fundamental Deep Learning Problem\nidentiﬁed and analysed in 1991 by my very ﬁrst student (now professor) Sepp Hochreiter (TL 1.6): the\nproblem of vanishing or exploding gradients [46, 47, 10]. The latter motivated all our subsequent Deep\nLearning research of the 1990s and 2000s.\nThrough supervised LSTM RNN (1997) (e.g., [48, 32, 39, 36, 37, 40, 38, 83], TL 1.8) we could even-\ntually perform similar feats as with the 1991 system [81, 82], overcoming the Fundamental Deep Learning\nProblem without any unsupervised pre-training. Moreover, LSTM could also learn tasks unlearnable by\nthe partially unsupervised 1991 chunker [81, 82].\nParticularly successful are stacks of LSTM RNN [40] trained by Connectionist Temporal Classiﬁcation\n(CTC) [36]. On faster computers of 2009, this became the ﬁrst RNN system ever to win an ofﬁcial inter-\nnational pattern recognition competition [40, 83], through the work of my PhD student and postdoc Alex\nGraves, e.g., [40]. To my knowledge, this also was the ﬁrst Deep Learning system ever (recurrent or not) to\nwin such a contest (TL 1.10). (In fact, it won three different ICDAR 2009 contests on connected handwrit-\ning in three different languages, e.g., [83, 40], TL 1.10.) A while ago, Alex moved on to Geoffrey Hinton’s\nlab (Univ. Toronto), where a stack [40] of our bidirectional LSTM RNN [39] also broke a famous TIMIT\nspeech recognition record [38] (TL 1.14), despite thousands of man years previously spent on HMM-based\nspeech recognition research. CTC-LSTM also helped to score ﬁrst at NIST’s OpenHaRT2013 evaluation\n[11].\nRecently, well-known entrepreneurs also got interested [43, 52] in such hierarchical temporal memories\n[81, 82] (TL 1.7).\nThe expression Deep Learning actually got coined relatively late, around 2006, in the context of un-\nsupervised pre-training for less general feedforward networks [44] (TL 1.9). Such a system reached 1.2%\nerror rate [44] on the MNIST handwritten digits [54, 55], perhaps the most famous benchmark of Ma-\nchine Learning. Our team ﬁrst showed that good old backpropagation (TL 1.2) on GPUs (with training\npattern distortions [6, 86] but without any unsupervised pre-training) can actually achieve a three times\nbetter result of 0.35% [17] - back then, a world record (a previous standard net achieved 0.7% [86]; a\nbackprop-trained [54, 55] Convolutional NN (CNN or convnet) [29, 30, 54, 55] got 0.39% [70](TL 1.9);\n2\nplain backprop without distortions except for small saccadic eye movement-like translations already got\n0.95%). Then we replaced our standard net by a biologically rather plausible architecture inspired by\nearly neuroscience-related work [29, 49, 30, 54]: Deep and Wide GPU-based Multi-Column Max-Pooling\nCNN (MCMPCNN) [18, 21, 22] with alternating backprop-based [54, 55, 71] weight-sharing convolutional\nlayers [30, 54, 56, 8] and winner-take-all [29, 30] max-pooling [72, 71, 77] layers (see [14] for early\nGPU-based CNN). MCMPCNN are committees of MPCNN [19] with simple democratic output averaging\n(compare earlier more sophisticated ensemble methods [76]). Object detection [16] and image segmenta-\ntion [15] (TL 1.13) proﬁt from fast MPCNN-based image scans [63, 62]. Our supervised GPU-MCMPCNN\nwas the ﬁrst system to achieve superhuman performance in an ofﬁcial international competition (with dead-\nline and secret test set known only to the organisers) [20, 19] (TL 1.12) (compare [85]), and the ﬁrst with\nhuman-competitive performance (around 0.2%) on MNIST [22]. Since 2011, it has won numerous addi-\ntional competitions on a routine basis, e.g., (TL 1.13, TL 1.14).\nOur (multi-column [19]) GPU-MPCNN [18] (TL 1.12) were adopted by the groups of Univ. Toronto\n/ Stanford / Google, e.g., [51, 25] (TL 1.13, TL 1.14). Apple Inc., the most proﬁtable smartphone maker,\nhired Ueli Meier, member of our Deep Learning team that won the ICDAR 2011 Chinese handwriting con-\ntest [22]. ArcelorMittal, the world’s top steel producer, is using our methods for material defect detection\nand classiﬁcation, e.g., [63]. Other users include a leading automotive supplier, recent start-ups such as\ndeepmind (which hired four of my former PhD students/postdocs), and many other companies and leading\nresearch labs. One of the most important applications of our techniques is biomedical imaging [16] (TL\n1.13, TL 1.14), e.g., for cancer prognosis or plaque detection in CT heart scans.\nRemarkably, the most successful Deep Learning algorithms in most international contests since 2009\n(TL 1.10-1.14) are adaptations and extensions of an over 40-year-old algorithm, namely, supervised efﬁ-\ncient backprop [59, 90] (TL 1.2) (compare [60, 53, 75, 12, 67]) or BPTT/RTRL for RNN, e.g., [94, 73, 91,\n68, 80, 61] (exceptions include two 2011 contests specialised on transfer learning [35]—but compare [23]).\nIn particular, as of 2013, state-of-the-art feedforward nets (TL 1.12-1.13) are GPU-based [18] multi-\ncolumn [22] combinations of two ancient concepts: Backpropagation (TL 1.2) applied [55] to Neocognitron-\nlike convolutional architectures (TL 1.3) (with max-pooling layers [72, 71, 77] instead of alternative [29,\n30, 31, 78] local winner-take-all methods). (Plus additional tricks from the 1990s and 2000s, e.g., [65, 64].)\nIn the quite different deep recurrent case, supervised systems also dominate, e.g., [48, 36, 40, 37, 61, 38]\n(TL 1.10, TL 1.14).\nIn particular, most competition-winning or benchmark record-setting Deep Learners (TL 1.10 - TL\n1.14) use one of two supervised techniques developed in my lab: (1) recurrent LSTM (1997) (TL 1.8)\ntrained by CTC (2006) [36], or (2) feedforward GPU-MPCNN (2011) [18] (TL 1.12) (building on earlier\nwork since the 1960s mentioned in the text above). Nevertheless, in many applications it can still be\nadvantageous to combine the best of both worlds - supervised learning and unsupervised pre-training, like\nin my 1991 system described above [79, 81, 82].\n3\n1\nTimeline of Deep Learning Highlights\n1.1\n1962: Neurobiological Inspiration Through Simple Cells and Complex Cells\nHubel and Wiesel described simple cells and complex cells in the visual cortex [49]. This inspired later\ndeep artiﬁcial neural network (NN) architectures (TL 1.3) used in certain modern award-winning Deep\nLearners (TL 1.12-1.14). (The author of the present paper was conceived in 1962.)\n1.2\n1970 ± a Decade or so: Backpropagation\nError functions and their gradients for complex, nonlinear, multi-stage, differentiable, NN-related systems\nhave been discussed at least since the early 1960s, e.g., [41, 50, 13, 27, 12, 93, 4, 26]. Gradient descent [42]\nin such systems can be performed [13, 50, 12] by iterating the ancient chain rule [57, 58] in dynamic\nprogramming style [9] (compare simpliﬁed derivation using chain rule only [27]). However, efﬁcient error\nbackpropagation (BP) in arbitrary, possibly sparse, NN-like networks apparently was ﬁrst described by\nLinnainmaa in 1970 [59, 60] (he did not refer to NN though). BP is also known as the reverse mode of\nautomatic differentiation [41], where the costs of forward activation spreading essentially equal the costs of\nbackward derivative calculation. See early FORTRAN code [59], and compare [66]. Compare the concept\nof ordered derivative [89] and related work [28], with NN-speciﬁc discussion [89] (section 5.5.1), and the\nﬁrst NN-speciﬁc efﬁcient BP of 1981 by Werbos [90, 92]. Compare [53, 75, 67] and generalisations for\nsequence-processing recurrent NN, e.g., [94, 73, 91, 68, 80, 61]. See also natural gradients [5]. As of 2013,\nBP is still the central Deep Learning algorithm.\n1.3\n1979: Deep Neocognitron, Weight Sharing, Convolution\nFukushima’s deep Neocognitron architecture [29, 30, 31] incorporated neurophysiological insights (TL 1.1)\n[49]. It introduced weight-sharing Convolutional Neural Networks (CNN) as well as winner-take-all lay-\ners. It is very similar to the architecture of modern, competition-winning, purely supervised, feedforward,\ngradient-based Deep Learners (TL 1.12-1.14). Fukushima, however, used local unsupervised learning rules\ninstead.\n1.4\n1987: Autoencoder Hierarchies\nIn 1987, Ballard published ideas on unsupervised autoencoder hierarchies [7], related to post-2000 feed-\nforward Deep Learners (TL 1.9) based on unsupervised pre-training, e.g., [44]; compare survey [45] and\nsomewhat related RAAMs [69].\n1.5\n1989: Backpropagation for CNN\nLeCun et al. [54, 55] applied backpropagation (TL 1.2) to Fukushima’s weight-sharing convolutional\nneural layers (TL 1.3) [29, 30, 54]. This combination has become an essential ingredient of many modern,\ncompetition-winning, feedforward, visual Deep Learners (TL 1.12-1.13).\n1.6\n1991: Fundamental Deep Learning Problem\nBy the early 1990s, experiments had shown that deep feedforward or recurrent networks are hard to train\nby backpropagation (TL 1.2). My student Hochreiter [46] discovered and analyzed the reason, namely, the\nFundamental Deep Learning Problem due to vanishing or exploding gradients. Compare [47].\n1.7\n1991: Deep Hierarchy of Recurrent NN\nMy ﬁrst recurrent Deep Learning system (present paper) partially overcame the fundamental problem (TL\n1.6) through a deep RNN stack pre-trained in unsupervised fashion [79, 81, 82] to accelerate subsequent\nsupervised learning. This was a working Deep Learner in the modern post-2000 sense, and also the ﬁrst\nNeural Hierarchical Temporal Memory.\n4\n1.8\n1997: Supervised Deep Learner (LSTM)\nLong Short-Term Memory (LSTM) recurrent neural networks (RNN) became the ﬁrst purely supervised\nDeep Learners, e.g., [48, 33, 39, 36, 37, 40, 38]. LSTM RNN were able to learn solutions to many previ-\nously unlearnable problems (see also TL 1.10, TL 1.14).\n1.9\n2006: Deep Belief Networks / CNN Results\nA paper by Hinton and Salakhutdinov [44] focused on unsupervised pre-training of feedforward NN to\naccelerate subsequent supervised learning (compare TL 1.7). This helped to arouse interest in deep NN\n(keywords: restricted Boltzmann machines; Deep Belief Networks). In the same year, a BP-trained CNN\n(TL 1.3, TL 1.5) by Ranzato et al. [70] set a new record on the famous MNIST handwritten digit recognition\nbenchmark [54], using training pattern deformations [6, 86].\n1.10\n2009: First Competitions Won by Deep Learning\n2009 saw the ﬁrst Deep Learning systems to win ofﬁcial international pattern recognition contests (with\nsecret test sets known only to the organisers): three connected handwriting competitions at ICDAR 2009\nwere won by deep LSTM RNN [40, 83] performing simultaneous segmentation and recognition.\n1.11\n2010: Plain Backpropagation on GPUs Yields Excellent Results\nIn 2010, a new MNIST record was set by good old backpropagation (TL 1.2) in deep but otherwise standard\nNN, without unsupervised pre-training, and without convolution (but with training pattern deformations).\nThis was made possible mainly by boosting computing power through a fast GPU implementation [17]. (A\nyear later, ﬁrst human-competitive performance on MNIST was achieved by a deep MCMPCNN (TL 1.12)\n[22].)\n1.12\n2011: MPCNN on GPU / First Superhuman Visual Pattern Recognition\nIn 2011, Ciresan et al. introduced supervised GPU-based Max-Pooling CNN or Convnets (MPCNN) [18],\ntoday used by most if not all feedforward competition-winning deep NN (TL 1.13, TL 1.14). The ﬁrst\nsuperhuman visual pattern recognition in a controlled competition (trafﬁc signs [87]) was achieved [20, 19]\n(twice better than humans, three times better than the closest artiﬁcial NN competitor, six times better than\nthe best non-neural method), through deep and wide Multi-Column (MC) GPU-MPCNN [18, 19], the\ncurrent gold standard for deep feedforward NN.\n1.13\n2012: First Contests Won on Object Detection and Image Segmentation\n2012 saw the ﬁrst Deep learning system (a GPU-MCMPCNN [18, 19], TL 1.12) to win a contest on visual\nobject detection in large images (as opposed to mere recognition/classiﬁcation): the ICPR 2012 Contest\non Mitosis Detection in Breast Cancer Histological Images [2, 74, 16]. An MC (TL 1.12) variant of a\nGPU-MPCNN also achieved best results on the ImageNet classiﬁcation benchmark [51]. 2012 also saw the\nﬁrst pure image segmentation contest won by Deep Learning (again through a GPU-MCMPCNN), namely,\nthe ISBI 2012 Challenge on segmenting neuronal structures [3, 15]. This was the 8th international pattern\nrecognition contest won by my team since 2009 [1].\n1.14\n2013: More Contests and Benchmark Records\nIn 2013, a new TIMIT phoneme recognition record was set by deep LSTM RNN [38] (TL 1.8, TL 1.10).\nA new record [24] on the ICDAR Chinese handwriting recognition benchmark (over 3700 classes) was set\non a desktop machine by a GPU-MCMPCNN with almost human performance. The MICCAI 2013 Grand\nChallenge on Mitosis Detection was won by a GPU-MCMPCNN [88, 16]. Deep GPU-MPCNN [18] also\nhelped to achieve new best results on ImageNet classiﬁcation [95] and PASCAL object detection [34].\n5\nAdditional contests are mentioned in the web pages of the Swiss AI Lab IDSIA, the University of Toronto,\nNY University, and the University of Montreal.\n2\nAcknowledgments\nDrafts/revisions of this paper have been published since 20 Sept 2013 in my massive open peer review web\nsite www.idsia.ch/˜juergen/ﬁrstdeeplearner.html (also under www.deeplearning.me). Thanks for valuable\ncomments to Geoffrey Hinton, Kunihiko Fukushima, Yoshua Bengio, Sven Behnke, Yann LeCun, Sepp\nHochreiter, Mike Mozer, Marc’Aurelio Ranzato, Andreas Griewank, Paul Werbos, Shun-ichi Amari, Seppo\nLinnainmaa, Peter Norvig, Yu-Chi Ho, Alex Graves, Dan Ciresan, Jonathan Masci, Stuart Dreyfus, and\nothers.\nReferences\n[1] A. Angelica interviews J. Schmidhuber: How Bio-Inspired Deep Learning Keeps Winning Competi-\ntions, 2012. KurzweilAI: http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-\ncompetitions.\n[2] ICPR 2012 Contest on Mitosis Detection in Breast Cancer Histological Images. Organizers: IPAL\nLaboratory, TRIBVN Company, Pitie-Salpetriere Hospital, CIALAB of Ohio State Univ., 2012.\n[3] Segmentation of Neuronal Structures in EM Stacks Challenge - IEEE International Symposium on\nBiomedical Imaging (ISBI), 2012.\n[4] S. Amari. A theory of adaptive pattern classiﬁers. IEEE Trans. EC, 16(3):299–307, 1967.\n[5] S.-I. Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2):251–276,\n1998.\n[6] H. Baird. Document Image Defect Models. In Proceddings, IAPR Workshop on Syntactic and Struc-\ntural Pattern Recognition, Murray Hill, NJ, 1990.\n[7] D. H. Ballard. Modular learning in neural networks. In AAAI, pages 279–284, 1987.\n[8] S. Behnke. Hierarchical Neural Networks for Image Interpretation, volume LNCS 2766 of Lecture\nNotes in Computer Science. Springer, 2003.\n[9] R. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1st edition,\n1957.\n[10] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is\ndifﬁcult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n[11] T. Bluche, J. Louradour, M. Knibbe, B. Moysset, F. Benzeghiba, and C. Kermorvant. The A2iA\nArabic Handwritten Text Recognition System at the OpenHaRT2013 Evaluation. In Submitted to\nDAS 2014, 2013.\n[12] A. Bryson and Y. Ho. Applied optimal control: optimization, estimation, and control. Blaisdell Pub.\nCo., 1969.\n[13] A. E. Bryson. A gradient method for optimizing multi-stage allocation processes. In Proc. Harvard\nUniv. Symposium on digital computers and their applications, 1961.\n[14] K. Chellapilla, S. Puri, and P. Simard. High performance convolutional neural networks for document\nprocessing. In International Workshop on Frontiers in Handwriting Recognition, 2006.\n6\n[15] D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Deep neural networks segment\nneuronal membranes in electron microscopy images. In Advances in Neural Information Processing\nSystems NIPS, 2012.\n[16] D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Mitosis detection in breast cancer\nhistology images using deep neural networks. In MICCAI 2013, 2013.\n[17] D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets for\nhandwritten digit recogntion. Neural Computation, 22(12):3207–3220, 2010.\n[18] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high perfor-\nmance convolutional neural networks for image classiﬁcation. In Intl. Joint Conference on Artiﬁcial\nIntelligence IJCAI, pages 1237–1242, 2011.\n[19] D. C. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. A committee of neural networks for trafﬁc\nsign classiﬁcation. In International Joint Conference on Neural Networks, pages 1918–1921, 2011.\n[20] D. C. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. Multi-column deep neural network for trafﬁc\nsign classiﬁcation. Neural Networks, 2012.\n[21] D. C. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁ-\ncation. Technical report, IDSIA, February 2012. arXiv:1202.2745v1 [cs.CV].\n[22] D. C. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi-\nﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition CVPR 2012, 2012. Long\npreprint arXiv:1202.2745v1 [cs.CV].\n[23] D. C. Ciresan, U. Meier, and J. Schmidhuber. Transfer learning for Latin and Chinese characters\nwith deep neural networks. In International Joint Conference on Neural Networks, pages 1301–1306,\n2012.\n[24] D. C. Ciresan and J. Schmidhuber. Multi-column deep neural networks for ofﬂine handwritten Chi-\nnese character classiﬁcation. Technical report, IDSIA, September 2013. arXiv:1309.0261.\n[25] A. Coates, B. Huval, T. Wang, D. J. Wu, A. Y. Ng, and B. Catanzaro. Deep learning with COTS HPC\nsystems. In Proc. International Conference on Machine learning (ICML’13), 2013.\n[26] S. W. Director and R. A. Rohrer. Automated network design - the frequency-domain case. IEEE\nTrans. Circuit Theory, CT-16:330–337, 1969.\n[27] S. E. Dreyfus. The numerical solution of variational problems. Journal of Mathematical Analysis and\nApplications, 5(1):30–45, 1962.\n[28] S. E. Dreyfus. The computational solution of optimal control problems with time lag. IEEE Transac-\ntions on Automatic Control, 18(4):383–385, 1973.\n[29] K. Fukushima. Neural network model for a mechanism of pattern recognition unaffected by shift in\nposition - neocognitron. In Trans. IECE, 1979.\n[30] K. Fukushima. Neocognitron: A self-organizing neural network for a mechanism of pattern recogni-\ntion unaffected by shift in position. Biological Cybernetics, 36(4):193–202, 1980.\n[31] K. Fukushima. Artiﬁcial vision by multi-layered neural networks: Neocognitron and its advances.\nNeural Networks, 37:103–119, Jan. 2013.\n[32] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM.\nIn Proc. ICANN’99, Int. Conf. on Artiﬁcial Neural Networks, pages 850–855, Edinburgh, Scotland,\n1999. IEE, London.\n7\n[33] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM.\nNeural Computation, 12(10):2451–2471, 2000.\n[34] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. Technical Report arxiv.org/abs/1311.2524, UC Berkeley and ICSI, 2013.\n[35] I. J. Goodfellow, A. C. Courville, and Y. Bengio. Large-scale feature learning with spike-and-slab\nsparse coding. In Proceedings of the 29th International Conference on Machine Learning, 2012.\n[36] A. Graves, S. Fernandez, F. J. Gomez, and J. Schmidhuber. Connectionist temporal classiﬁcation:\nLabelling unsegmented sequence data with recurrent neural nets. In ICML’06: Proceedings of the\nInternational Conference on Machine Learning, 2006.\n[37] A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connec-\ntionist system for improved unconstrained handwriting recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 31(5), 2009.\n[38] A. Graves, A. Mohamed, and G. E. Hinton. Speech recognition with deep recurrent neural networks.\nIn Proceedings of the ICASSP, 2013.\n[39] A. Graves and J. Schmidhuber. Framewise phoneme classiﬁcation with bidirectional lstm and other\nneural network architectures. Neural Networks, 18(5-6):602–610, 2005.\n[40] A. Graves and J. Schmidhuber.\nOfﬂine handwriting recognition with multidimensional recurrent\nneural networks. In Advances in Neural Information Processing Systems 21. MIT Press, Cambridge,\nMA, 2009.\n[41] A. Griewank. Documenta Mathematica - Extra Volume ISMP, pages 389–400, 2012.\n[42] J. Hadamard. M´emoire sur le probl`eme d’analyse relatif `a l’´equilibre des plaques ´elastiques en-\ncastr´ees. M´emoires pr´esent´es par divers savants `a l’Acad´emie des sciences de l’Institut de France:\n´Extrait. Imprimerie nationale, 1908.\n[43] J. Hawkins and D. George. Hierarchical Temporal Memory - Concepts, Theory, and Terminology.\nNumenta Inc., 2006.\n[44] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,\n313(5786):504–507, 2006.\n[45] G. E. Hinton. Connectionist learning procedures. Artiﬁcial intelligence, 40(1):185–234, 1989.\n[46] S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f¨ur In-\nformatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen, 1991. Advisor: J. Schmidhuber.\n[47] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber.\nGradient ﬂow in recurrent nets: the\ndifﬁculty of learning long-term dependencies. In S. C. Kremer and J. F. Kolen, editors, A Field Guide\nto Dynamical Recurrent Neural Networks. IEEE Press, 2001.\n[48] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780,\n1997.\n[49] D. H. Hubel and T. Wiesel. Receptive ﬁelds, binocular interaction, and functional architecture in the\ncat’s visual cortex. Journal of Physiology (London), 160:106–154, 1962.\n[50] H. J. Kelley. Gradient theory of optimal ﬂight paths. ARS Journal, 30(10):947–954, 1960.\n[51] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing Systems (NIPS 2012), 2012.\n[52] R. Kurzweil. How to Create a Mind: The Secret of Human Thought Revealed. 2012.\n8\n[53] Y. LeCun. Une proc´edure d’apprentissage pour r´eseau `a seuil asym´etrique. Proceedings of Cognitiva\n85, Paris, pages 599–604, 1985.\n[54] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Back-\npropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.\n[55] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.\nHandwritten digit recognition with a back-propagation network. In D. S. Touretzky, editor, Advances\nin Neural Information Processing Systems 2, pages 396–404. Morgan Kaufmann, 1990.\n[56] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.\n[57] G. W. Leibniz. Memoir using the chain rule (cited in tmme 7:2&3 p 321-332, 2010). 1676.\n[58] G. F. A. L’Hospital. Analyse des inﬁniment petits, pour l’intelligence des lignes courbes. Paris:\nL’Imprimerie Royale, 1696.\n[59] S. Linnainmaa. The representation of the cumulative rounding error of an algorithm as a Taylor\nexpansion of the local rounding errors. Master’s thesis, Univ. Helsinki, 1970.\n[60] S. Linnainmaa. Taylor expansion of the accumulated rounding error. BIT Numerical Mathematics,\n16(2):146–160, 1976.\n[61] J. Martens and I. Sutskever. Learning recurrent neural networks with Hessian-free optimization. In\nProceedings of the 28th International Conference on Machine Learning, pages 1033–1040, 2011.\n[62] J. Masci, D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with\ndeep max-pooling convolutional neural networks. In Proc. ICIP, 2013.\n[63] J. Masci, A. Giusti, D. C. Ciresan, G. Fricout, and J. Schmidhuber. A fast learning algorithm for\nimage segmentation with max-pooling convolutional networks. In Proc. ICIP, 2013.\n[64] G. Montavon, G. Orr, and K. M¨uller. Neural Networks: Tricks of the Trade. Number LNCS 7700 in\nLecture Notes in Computer Science Series. Springer Verlag, 2012.\n[65] G. Orr and K. M¨uller. Neural Networks: Tricks of the Trade. Number LNCS 1524 in Lecture Notes\nin Computer Science Series. Springer Verlag, 1998.\n[66] G. M. Ostrovskii, Y. M. Volin, and W. W. Borisov. ¨Uber die Berechnung von Ableitungen. Wiss. Z.\nTech. Hochschule f¨ur Chemie, 13:382–384, 1971.\n[67] D. B. Parker. Learning-logic. Technical Report TR-47, Center for Comp. Research in Economics and\nManagement Sci., MIT, 1985.\n[68] B. A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural Computation,\n1(2):263–269, 1989.\n[69] J. B. Pollack. Implications of recursive distributed representations. In NIPS, pages 527–536, 1988.\n[70] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with\nan energy-based model. In J. P. et al., editor, Advances in Neural Information Processing Systems\n(NIPS 2006). MIT Press, 2006.\n[71] M. A. Ranzato, F. Huang, Y. Boureau, and Y. LeCun. Unsupervised learning of invariant feature\nhierarchies with applications to object recognition. In Proc. Computer Vision and Pattern Recognition\nConference (CVPR’07). IEEE Press, 2007.\n[72] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nat. Neurosci.,\n2(11):1019–1025, 1999.\n9\n[73] A. J. Robinson and F. Fallside. The utility driven dynamic error propagation network. Technical\nReport CUED/F-INFENG/TR.1, Cambridge University Engineering Department, 1987.\n[74] L. Roux, D. Racoceanu, N. Lomenie, M. Kulikova, H. Irshad, J. Klossa, F. Capron, C. Genestie, G. L.\nNaour, and M. N. Gurcan. Mitosis detection in breast cancer histological images - an ICPR 2012\ncontest. J. Pathol. Inform., 4:8, 2013.\n[75] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propa-\ngation. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing, volume 1,\npages 318–362. MIT Press, 1986.\n[76] R. E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, 1990.\n[77] D. Scherer, A. M¨uller, and S. Behnke. Evaluation of pooling operations in convolutional architectures\nfor object recognition. In Proc. International Conference on Artiﬁcial Neural Networks (ICANN),\n2010.\n[78] J. Schmidhuber. A local learning algorithm for dynamic feedforward and recurrent networks. Con-\nnection Science, 1(4):403–412, 1989.\n[79] J. Schmidhuber. Neural sequence chunkers. Technical Report FKI-148-91, Institut f¨ur Informatik,\nTechnische Universit¨at M¨unchen, April 1991.\n[80] J. Schmidhuber. A ﬁxed size storage O(n3) time complexity learning algorithm for fully recurrent\ncontinually running networks. Neural Computation, 4(2):243–248, 1992.\n[81] J. Schmidhuber. Learning complex, extended sequences using the principle of history compression.\nNeural Computation, 4(2):234–242, 1992.\n[82] J. Schmidhuber. Netzwerkarchitekturen, Zielfunktionen und Kettenregel. (Network Architectures,\nObjective Functions, and Chain Rule.) Habilitationsschrift (Habilitation Thesis), Institut f¨ur Infor-\nmatik, Technische Universit¨at M¨unchen, 1993.\n[83] J. Schmidhuber, D. Ciresan, U. Meier, J. Masci, and A. Graves. On fast deep nets for AGI vision. In\nProc. Fourth Conference on Artiﬁcial General Intelligence (AGI), Google, Mountain View, CA, 2011.\n[84] J. Schmidhuber, M. C. Mozer, and D. Prelinger. Continuous history compression. In H. H¨uning,\nS. Neuhauser, M. Raus, and W. Ritschel, editors, Proc. of Intl. Workshop on Neural Networks, RWTH\nAachen, pages 87–95. Augustinus, 1993.\n[85] P. Sermanet and Y. LeCun. Trafﬁc sign recognition with multi-scale convolutional networks. In\nProceedings of International Joint Conference on Neural Networks (IJCNN’11), 2011.\n[86] P. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks applied to vi-\nsual document analysis. In Seventh International Conference on Document Analysis and Recognition,\npages 958–963, 2003.\n[87] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. INI German Trafﬁc Sign Recognition Benchmark\nfor the IJCNN’11 Competition, 2011.\n[88] M. Veta, M. Viergever, J. Pluim, N. Stathonikos, and P. J. van Diest. MICCAI 2013 Grand Challenge\non Mitosis Detection (organisers), 2013.\n[89] P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.\nPhD thesis, Harvard University, 1974.\n[90] P. J. Werbos. Applications of advances in nonlinear sensitivity analysis. In Proceedings of the 10th\nIFIP Conference, 31.8 - 4.9, NYC, pages 762–770, 1981.\n10\n[91] P. J. Werbos. Generalization of backpropagation with application to a recurrent gas market model.\nNeural Networks, 1, 1988.\n[92] P. J. Werbos. Backwards differentiation in AD and neural nets: Past links and new opportunities. In\nAutomatic Differentiation: Applications, Theory, and Implementations, pages 15–34. Springer, 2006.\n[93] J. H. Wilkinson, editor. The Algebraic Eigenvalue Problem. Oxford University Press, Inc., New York,\nNY, USA, 1988.\n[94] R. J. Williams. Complexity of exact gradient computation algorithms for recurrent neural networks.\nTechnical Report Technical Report NU-CCS-89-27, Boston: Northeastern University, College of\nComputer Science, 1989.\n[95] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. Technical Report\narXiv:1311.2901 [cs.CV], NYU, 2013.\n11\n",
  "categories": [
    "cs.NE"
  ],
  "published": "2013-12-19",
  "updated": "2013-12-19"
}