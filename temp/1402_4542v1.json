{
  "id": "http://arxiv.org/abs/1402.4542v1",
  "title": "Unsupervised Ranking of Multi-Attribute Objects Based on Principal Curves",
  "authors": [
    "Chun-Guo Li",
    "Xing Mei",
    "Bao-Gang Hu"
  ],
  "abstract": "Unsupervised ranking faces one critical challenge in evaluation applications,\nthat is, no ground truth is available. When PageRank and its variants show a\ngood solution in related subjects, they are applicable only for ranking from\nlink-structure data. In this work, we focus on unsupervised ranking from\nmulti-attribute data which is also common in evaluation tasks. To overcome the\nchallenge, we propose five essential meta-rules for the design and assessment\nof unsupervised ranking approaches: scale and translation invariance, strict\nmonotonicity, linear/nonlinear capacities, smoothness, and explicitness of\nparameter size. These meta-rules are regarded as high level knowledge for\nunsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a\nranking principal curve (RPC) model, which learns a one-dimensional manifold\nfunction to perform unsupervised ranking tasks on multi-attribute observations.\nFurthermore, the RPC is modeled to be a cubic B\\'ezier curve with control\npoints restricted in the interior of a hypercube, thereby complying with all\nthe five meta-rules to infer a reasonable ranking list. With control points as\nthe model parameters, one is able to understand the learned manifold and to\ninterpret the ranking list semantically. Numerical experiments of the presented\nRPC model are conducted on two open datasets of different ranking applications.\nIn comparison with the state-of-the-art approaches, the new model is able to\nshow more reasonable ranking lists.",
  "text": "arXiv:1402.4542v1  [cs.LG]  19 Feb 2014\n1\nUnsupervised Ranking of Multi-Attribute Objects\nBased on Principal Curves\nChun-Guo Li, Xing Mei, and Bao-Gang Hu, Senior Member, IEEE,\nAbstract—Unsupervised ranking faces one critical challenge in evaluation applications, that is, no ground truth is available. When\nPageRank and its variants show a good solution in related subjects, they are applicable only for ranking from link-structure data.\nIn this work, we focus on unsupervised ranking from multi-attribute data which is also common in evaluation tasks. To overcome\nthe challenge, we propose ﬁve essential meta-rules for the design and assessment of unsupervised ranking approaches: scale and\ntranslation invariance, strict monotonicity, linear/nonlinear capacities, smoothness, and explicitness of parameter size. These meta-\nrules are regarded as high level knowledge for unsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a ranking\nprincipal curve (RPC) model, which learns a one-dimensional manifold function to perform unsupervised ranking tasks on multi-attribute\nobservations. Furthermore, the RPC is modeled to be a cubic B´ezier curve with control points restricted in the interior of a hypercube,\nthereby complying with all the ﬁve meta-rules to infer a reasonable ranking list. With control points as the model parameters, one is\nable to understand the learned manifold and to interpret the ranking list semantically. Numerical experiments of the presented RPC\nmodel are conducted on two open datasets of different ranking applications. In comparison with the state-of-the-art approaches, the\nnew model is able to show more reasonable ranking lists.\nIndex Terms—Unsupervised ranking, multi-attribute, strict monotonicity, smoothness, data skeleton, principal curves, B´ezier curves.\n✦\n1\nINTRODUCTION\nF\nROM the viewpoint of machine learning, ranking\ncan be performed in an either supervised or unsu-\npervised way as shown in the hierarchical structure in\nFig. 1. When supervised ranking [1] is able to evaluate\nthe ranking performance from the given ground truth,\nunsupervised ranking seems more challenging because\nno ground truth label is available. Modelers or users will\nencounter a more difﬁcult issue below:\n“How can we insure that the ranking list from the unsu-\npervised ranking is reasonable or proper?”\nFrom the viewpoint of given data types, ranking ap-\nproaches can be further divided into two categories:\nranking based on link structure and ranking based on\nmulti-attribute data. PageRank [2] is one of the repre-\nsentative unsupervised approaches to rank items which\nhave a linking network (e.g. websites). But PageRank\nand its variants do not work for ranking candidates\nwhich have no links. In this paper, we focus on unsuper-\nvised ranking approaches on a set of objects with multi-\nattribute numerical observations.\nTo rank from multi-attribute objects, weighted sum-\nmation of attributes is widely used to provide a scalar\nscore for each object. But different weight assignments\ngive different ranking lists such that ranking results are\nnot convincing enough. The ﬁrst principal component\n• C.-G. Li, X. Mei and B.-G. Hu are with the National Laboratory of Pattern\nRecognition, Institute of Automation Chinese Academy of Sciences, 95\nZhongGuanCun East Road, Beijing 100190, P.R. China.\nEmail: cgli@nlpr.ia.ac.cn, hubg@nlpr.ia.ac.cn\n• C.G. Li is also with Faculty of Mathematics and Computer Science, Hebei\nUniversity, 180 Wusi East Road, Baoding, Hebei 071002, P.R. China.\nanalysis (PCA) provides a weight learning approach [5],\nby which the score for each object is determined by its\nprincipal component on the skeleton of the data distri-\nbution. However, it encounters problems when the data\ndistribution is nonlinearly shaped. Although kernel PCA\n[5] is proposed to attack this problem, the mapping to the\nkernel space is not order-preserving, which is the basic\nrequirement for a ranking function. Neither dimension\nreduction methods [6] nor vector quantization [9] can\nassign scores for multi-attribute observations.\nAs the nonlinear extension of the ﬁrst PCA, principal\ncurves can be used to perform a ranking task [8], [10].\nA principal curve provides an ordering of data points\nby the ordering of threading through their projected\npoints on the curve (illustrated by Fig. 2) which can\nbe regarded as the “ranking skeleton”. However, not\nall of principal curve models are capable of performing\na ranking task. Polyline approximation of a principal\ncurve [11] fails to provide a consistent ranking rule due\nto non-smoothness at connecting points. Besides, it fails\nto guarantee order-preserving. Order-preserving can not\nbe guaranteed either by a general principal curve model\n(e.g. [19]) which is not modeled specially for ranking\ntasks. The problem can be tackled by the constraint of\nstrict monotonicity which is one of the constraints we\npresent for ranking functions in this paper. Example 1\nshows that strict monotonicity is a necessary condition\nfor a ranking function but was neglected by all other\ninvestigations.\nExample 1. Suppose we want to evaluate life qualities of\ncountries with a principal curve based on two attributes:\n2\nFig. 1. Hierarchical diagram of ranking approaches. RPC is an unsupervised ranking approach based on multi-attribute\nobservations for objects.\n(a) Polyline Approximation\n(non-strict monotonicity)\n(b)\nA\nGeneral\nPrincipal\nCurve (non-monotonicity)\nFig. 2. Examples on a monotonicity property for ranking\nwith principal curves.\nLEB1 and GDP2. Each country is a data point in the\ntwo-dimensional plane of LEB and GDP. If the principal\ncurve is approximated by a polyline as in Fig. 2(a), the\npiece of the horizontal line is not strictly monotone.\nIt makes the same ranking solution for x1 = (58, 1.4)\nand x2 = (58, 16.2) but x2 should be ranked higher\nthan x1. For a general principal curve like the curve\nin Fig. 2(b) which is not monotone, two pairs of points\nare ordered unreasonably. The pair, x3 = (74, 40.2) and\nx4 = (82, 40.2), are put in the same place of the ranking\nlist since they are projected to the same point which has\nthe vertical tangent line to the curve. But x4 should be\nranked higher for its higher LEB than x3. Another pair,\nx5 = (75, 62.5) and x6 = (81, 64.8), are also put in the\nsame place but apparently x6 should be ranked higher\nthan x5. With strict monotonicity, these points would be\nin the order that they are.\nFollowing the principle of “let the data speak for them-\nselves” [12], this work tries to attack problems for unsu-\npervised ranking of multi-attribute objects with principal\ncurves . First, ranking performance is taken into account\nfor the design of ranking functions. It is known that\nknowledge of a given task can always improve learning\nperformance [13]. The reason why PageRank produces\na commonly acceptable search result for a query, lies\n1. Life Expectancy at Birth, years\n2. Gross Domestic Product per capita by Purchasing Power Parities,\nK$/person\nFig. 3. Motivation of RPC model for unsupervised rank-\ning.\non that PageRank algorithm is designed by integrating\nthe knowledge about backlinks [2]. For multi-attribute\nobjects with no linking networks, knowledge about rank-\ning functions can be taken into account to make ranking\nfunctions produce reasonable ranking lists. In this work,\nwe present ﬁve essential meta-rules for ranking rules\n(Fig. 3). These meta-rules can be capable of assessing the\nreasonability of ranking lists for unsupervised ranking.\nSecond, principal curves should be modeled to be able\nto serve as ranking functions. As referred in [8], ranking\nwith a principal curve is performed on the learned\nskeleton of data distribution. But not all principal curve\nmodels are capable of producing reasonable ranked lists\nwhen no ranking knowledge is embedded into principal\ncurve models. Motivated by [14], the principal curve can\nbe parametrically designed with a cubic B´ezier curve. We\nwill show in Section 4 that the parameterized principal\ncurve has all the ﬁve meta-rules with constraints on\ncontrol points and that its existence and convergency of\nlearning algorithm are proved theoretically. Therefore,\nthe parameterized principal curve is capable of making\na reasonable ranking list.\nThe following points highlight the main contributions\nof this paper:\n• We propose ﬁve meta-rules for unsupervised rank-\ning, which serve as high-level guidance in the de-\nsign and assessment of unsupervised ranking ap-\nproaches for multi-attribute objects. We justify that\nthe ﬁve meta-rules are essential in applications, but\nunfortunately some or all of them were overlooked\n3\nby most of ranking approaches.\n• A ranking principal curve (RPC) model is presented\nfor unsupervised ranking from multi-attribute nu-\nmerical\nobservations\nof\nobjects,\ndifferent\nfrom\nPageRank which ranks from link structure [2]. The\npresented model can satisfy all of ﬁve meta-rules\nfor ranking tasks, while other existing approaches\n[8] overlooked them.\n• We develop the RPC learning algorithm, and the-\noretically prove the existence of a RPC and con-\nvergency of learning algorithm for given multi-\nattribute objects for ranking. With RPC learning\nalgorithm, reasonable ranking lists for openly ac-\ncessible data illustrate the good performance of the\nproposed unsupervised ranking approaches.\n1.1\nRelated Works\nDomain knowledge can be integrated into leaning mod-\nels to improve learning performance. By coupling do-\nmain knowledge as prior information with network con-\nstructions, Hu et al. [13] and Daniels et al. [15] improve\nthe prediction accuracy of neural networks. Recently,\nmonotonicity is taken into consideration as constraints\nby Kotłowski et al. [16] to improved the ordinal clas-\nsiﬁcation performance. For unsupervised ranking, the\ndomain knowledge of monotonicity can also be taken\ninto account and is capable of assessing the ranking\nperformance, other than evaluation of side-effects [17].\nRanking on manifolds has provided a new ranking\nframework [3], [4], [8], [18], which is different from\ngeneral ranking functions such as ranking aggregation\n[7]. As one-dimensional manifolds, principal curves are\nable to perform unsupervised ranking tasks from multi-\nattribute numerical observations of objects [8]. But not all\nprincipal curve models can serve as ranking functions.\nFor example, Elmap can well portray the contour of a\nmolecular surface [19] but would bring about a biased\nranking list due to no guarantee of order-preserving [8].\nWhat’s more, Elmap is hardly interpretable since the\nparameter size of principal curves is unknown explicitly.\nA B´ezier curve is a parametrical one-dimensional\ncurve which is widely used in ﬁtting [20]. Hu et al.\n[14] proved that in two-dimensional space a cubic B´ezier\ncurve is strictly monotone with end points in the opposite\ncorner and control points in the interior of the square box\nas shown in Fig. 4. To avoid confusion, end points refer\nto the points on both ends of the control polyline (also\nthe end points of the curve) and control points refer to\nthe other vertices of the control polyline in this paper.\n1.2\nPaper Organization\nThe rest of this paper is organized as follows. Back-\ngrounds of this paper are formalized in the next section.\nIn Section 3, ﬁve meta-rules are elaborated for ranking\nfunctions. In Section 4, a ranking model, namely ranking\nprincipal curve (RPC) model, is deﬁned and formulated\nwith a cubic B`ezier curve which is proved to follow all\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nFig. 4.\nFor an increasing monotone function, there are\nfour basic nonlinear shapes [14] of cubic B´ezier curves (in\nblue) which mimic shapes of the control polylines (in red).\nCurve shapes are determined by the locations of control\npoints.\nthe ﬁve meta-rules for ranking functions. RPC learning\nalgorithm is designed to learn the control points of the\ncubic B`ezier curve in Section 5. To illustrate the effective\nperformance of the proposed RPC model, applications\non real world datasets are carried out in Section 6, prior\nto summary of this paper in Section 7.\n2\nBACKGROUNDS\nConsider ranking a set of n objects A = {a1, a2, · · · , an}\naccording to d real-valued attributes (or indicators, fea-\ntures) V = {v1, v2, · · · , vd}. Numerical observations of\none object a ∈A on all the attributes comprise an\nitem which is denoted as a vector x in d-dimensional\nspace Rd. Ranking objects in A is equivalent to ranking\ndata points X = {x1, x2, · · · , xn}. That is, to give the\nordering of ai1 ⪯ai2 ⪯· · · ⪯ain can be achieved\nby discovering the ordering of xi1 ⪯xi2 ⪯· · · ⪯xin\nwhere {i1, i2, · · · , in} is a permutation of {1, 2, · · · , n}\nand xi ⪯xj means that xi precedes xj. As there is no\nlabel to help with ranking, it is an unsupervised ranking\nproblem from multi-attribute data.\nMathematically, ranking task is to provide a list of\ntotally ordered points. A total order is a special partial\norder which requires comparability in addition to the re-\nquirements of reﬂexivity, antisymmetry and transitivity\nfor the partial order [21]. Let x and y are one pair of\npoints in X. For ranking, if x and y are different, they\nhave the ordinal relation of either x ⪯y or y ⪯x. If\nx ⪯y and y ⪯x, then y = x which infers that x and y\nare the same thing.\nRemembering that a partial order is associated with a\nproper cone and that Rd\n+ is a self-dual proper cone [21]\n4\nRd\n+ = {ρ : ρT x ≥0, ∀x ∈Rd\n+}, the order for ranking\ntasks on Rd is deﬁned in this paper to be\nx ⪯y ⇐⇒\n\n\n\n\n\nδ1(y1 −x1)\nδ2(y2 −x2)\n...\nδd(yd −xd)\n\n\n\n\n∈Rd\n+\n(1)\nwhere x = (x1, x2, · · · , xd)T , y = (y1, y2, · · · , yd)T , and\nδj =\n\u001a\n1,\nj ∈E\n−1,\nj ∈F\n.\n(2)\nIt is easy to verify that the order deﬁned by Eq.(1) is a\ntotal order with properties of comparability, reﬂexivity,\nantisymmetry and transitivity. In Eq.(2), E and F are two\nsubsets of {1, 2, · · · , d} such that E S F = {1, 2, · · · , d}\nand E T F = ∅. If let\nα = (δ1, δ2, · · · , δd)T .\n(3)\nα is unique for one given ranking task and varies from\ntask to task. For a given ranking task with deﬁned α, x\nprecedes y for xj < yj(j ∈E) and xj > yj(j ∈F).\nAs R is totally ordered, we prefer to grade each\npoint with a real value to help with ranking. Assume\nϕ : Rd 7→R is the ranking function to assign x a score\nwhich provides the ordering of x. ϕ is required to be\norder-preserving so that ϕ(x) has the same ordering in R\nas x in Rd. In order theory, an order-preserving function\nis also called isotone or monotone [22].\nDeﬁnition 1 ([22]). A function ϕ : Rd 7→R is called\nmonotone (or, alternatively, order-preserving) if\nx ⪯y =⇒ϕ(x) ≤ϕ(y)\n(4)\nand strictly monotone if\nx ⪯y, x ̸= y =⇒ϕ(x) < ϕ(y)\n(5)\nOrder-preserving is the basic requirement for a rank-\ning function. For a partially ordered set, ϕ should assign\na score to x no more than the score to y if x ⪯y.\nMoreover, if x ̸= y also holds, the score assigned to x\nmust be smaller than the score to y. As A is totally\nordered and different points should be assigned with\ndifferent scores, the ranking function is required to be\nstrictly monotone as stated by Eq.(5). Otherwise, the\nranking rule would be meaningless due to breaking the\nordering in original data space Rd.\nExample 2. In addition to the two indicators in Ex-\nample 1, another two indicators are taken to evaluate\nlife qualities of countries: IMR3 and Tuberculosis4. It\nis easily known that the life quality of one country\nwould be higher if it has a higher LEB and GDP while\na lower IMR and Tuberculosis. Let numerical observa-\ntions on four countries to be xI = (2.1, 62.7, 75, 59),\nxM = (11.3, 75.5, 12, 30), xG = (32.1, 79.2, 6, 4), and xN =\n3. Infant Mortality Rate per 1000 born\n4. new cases of infectious Tuberculosis per 100,000 of population\n(47.6, 80.1, 3, 3) respectively. By Eq.(1), they have the\nordering xI ⪯xM ⪯xG ⪯xN with α = (1, 1, −1, −1)T.\nIn this case, E = {1, 2} and F = {3, 4}. Let ϕ(xI) = 0.407,\nϕ(xM) = 0.593, ϕ(xG) = 0.785 and ϕ(xN) = 0.891.\nThen ϕ is a strictly monotone mapping which strictly\npreserves the ordering in R4.\nRecall that a differentiable function f : R 7→R is\nnondecreasing if and only if f ′(x) ≥0 for all x ∈domf,\nand increasing if f ′(x) > 0 for all x ∈domf (but the\nconverse is not true) [23]. They are readily extended to\nthe case of monotonicity in Deﬁnition 1 with respect to\nthe order deﬁned by Eq.(1).\nTheorem 1 ([21]). Let ϕ : Rd 7→R be differentiable. ϕ is\nmonotone if and only if\n∇ϕ(x) ⪰0\n(6)\nwhere 0 is the zero vector. ϕ is strictly monotone if\n∇ϕ(x) ≻0\n(7)\nTheorem 1 provides ﬁrst-order conditions for mono-\ntonicity. Note that ‘≻’ denotes a strict partial order [21].\nLet\n∇ϕ(x) =\n\u0012 ∂ϕ\n∂x1\n, ∂ϕ\n∂x2\n, · · · , ∂ϕ\n∂xd\n\u0013T\n.\n(8)\n∇ϕ(x) ≻0 infers\n∂ϕ\n∂xj > 0 for j ∈E and\n∂ϕ\n∂xj < 0 for\nj ∈F. ∇ϕ(x) ≻0 infers that each component of ∇ϕ(x)\ndoes not equal to zero. By the case of strict monotonicity\nin Theorem 1, ∇ϕ(x) ≻0 infers not only that ϕ is strictly\nmonotone from Rd to R, but also that the value s = ϕ(x)\nis increasing with respect to xj(j ∈E) and decreasing\nwith respect to xj(j ∈F). Vice versa, if\n∂ϕ\n∂xj is bigger\nthan zero for j ∈E and smaller than zero for j ∈F,\n∇ϕ(x) ≻0 holds and infers ϕ is a strictly monotone\nmapping. Lemma 1 can be concluded immediately.\nLemma 1. s = ϕ(x) is strictly monotone if and only if s is\nstrictly monotone along xi with ﬁxed the others xj(j ̸= i).\nFurther more, a strictly monotone mapping infers a\none-to-one mapping that for a value s ∈rangϕ there is\nexactly one point x ∈domϕ such that ϕ(x) = s. If the\npoint x is denoted by x = f(s), f : R 7→Rd is called the\ninverse mapping of ϕ and inherits the property of strict\nmonotonicity of its origin ϕ.\nTheorem 2. Assume ∇ϕ(x) ≻0. There exists an inverse\nmapping denoted by f : rangϕ 7→domϕ such that ∇f(s) ≻\n0 holds for all s ∈rangϕ, that is for ∀s1, s2 ∈rangϕ\ns1 < s2 =⇒f(s1) ⪯f(s2), f(s1) ̸= f(s2).\n(9)\nProof of Theorem 2 can be found in Appendix B.\nThe theorem also holds in the other direction. Assuming\nf : R 7→Rd, if ∇f(s) ≻0, there exists an inverse\nmapping ϕ : rangf 7→domf and ∇ϕ(x) ≻0 holds for\nall x ∈rangf. Because of the one-to-one correspondence,\nf and ϕ share the same geometric properties such as\nscale and translation invariance, smoothness and strict\n5\nmonotonicity [23].\n3\nMETA-RULES\nAs a ranking function for ϕ : Rd 7→R, ϕ(x) outputs\na real value s = ϕ(x) as the ranking score for a given\npoint x. The ranking list of objects would be provided\nby sorting their ranking scores in ascending/descending\norder. Since unsupervised ranking has no label infor-\nmation to verify the ranking list, we restrict ranking\nfunctions with ﬁve essential features to guarantee that\na reasonable ranking list is provided. These features are\ncapable of serving as high-level guidance of modeling\nranking functions. They are also capable of serving as\nhigh-level assessments for unsupervised ranking perfor-\nmance, different from assessments for supervised rank-\ning performance which take qualities of ranking labels.\nAny functions from Rd to R with all the ﬁve features\ncan serve as ranking functions and be able to provide\na reasonable ranking list. These features are rules for\nranking rules, namely meta-rules.\n3.1\nScale and Translation Invariance\nDeﬁnition 2 ([24]). A ranking rule is invariant to scale and\ntranslation if for x ⪯y\nϕ(x) ≤ϕ(y) ⇐⇒ϕ(L(x)) ≤ϕ(L(y)).\n(10)\nwhere L(·) performs scale and translation.\nNumerical observations on different indicators are\ntaken on different dimensions of quantity. In Example\n1, GDP is measured in thousands of dollars while LEB\nranges from 40 to 90 years. They are not in the same\ndimensions of quantity. As a general data preprocessing\ntechnique, scale and translation can take them into the\nsame dimensions (e.g. [0, 1]) while preserving their orig-\ninal ordering. If let L be a linear transformation on Rd,\nwe have x ⪯y ⇐⇒L(x) ⪯L(y) for x, y ∈Rd [24].\nTherefore, a ranking function ϕ(x) should produce the\nsame ranking list before and after scaling and translat-\ning.\n3.2\nStrict Monotonicity\nDeﬁnition 3 ([22]). ϕ(x) is strictly monotone if ϕ(xi) <\nϕ(xj) for xi ⪯xj and xi ̸= xj(i ̸= j) .\nStrict monotonicity in Deﬁnition 1 is speciﬁed here as\none of meta-rules for ranking. For ordinal classiﬁcation\nproblem, monotonicity is a general constraint since two\ndifferent objects would be classiﬁed into the same class\n[16]. But for the ranking problem discussed in this paper,\nit requires the strict monotonicity since different objects\nshould have different scores for ranking. ϕ(xi) = ϕ(xj)\nholds if and only if xi = xj(i ̸= j). In Example 1, x1 ⪯x2\nand xi ̸= xj indicate that a higher score should be\nassigned to x2 than x1. And so do x3 and x4. Therefore,\nthe ranking function ϕ(x) is required to be a strictly\nmonotone mapping. Otherwise, the ranking list would\nbe not convincing. ϕ in Example 2 is to the point referred\nhere.\n3.3\nLinear/Nonlinear Capacities\nDeﬁnition 4. ϕ(x) has the capacities of linearity and nonlin-\nearity if ϕ(x) is able to depict the relationship of both linearity\nand nonlinearity.\nTaking the ranking task in Example 1 for illustration,\none has no knowledge about the relationship between\nLEB and the score. The score might be a either linear\nor nonlinear function of LEB. It is the similar case for\nthe relationship between GDP and the score. Therefore,\nt = ϕ(x) should embody both of the linear and nonlinear\nrelationships between t and xj. For the ranking task in\nExample 1, the ranking function ϕ should be a linear\nfunction of LEB for ﬁxed GDP if LEB is linear with t.\nMeanwhile, ϕ should also be a nonlinear function of\nGDP for ﬁxed LEB if GDP is nonlinear with t.\n3.4\nSmoothness\nDeﬁnition 5 ( [23]). ϕ(x) is smooth if ϕ(x) is C h(h ≥1).\nIn mathematical analysis, a function is called smooth if\nit has derivatives of all orders [23]. Yet a ranking function\nϕ(x) is required to be of class C h where h ≥1. That\nis, ϕ(x) is continuous and has the ﬁrst-order derivative\n∇ϕ(x). The ﬁrst-order derivative ∇ϕ(x) guarantees that\nϕ(x) will exert a consistent ranking rule for all objects\nand the ranking rule would be not abruptly changed for\nsome object. Taking the polyline in Fig. 2 for illustration,\nit is of class C 0 but not of class C 1 because it is continu-\nous but not differentiable at the connecting vertex of the\ntwo lines. This would lead to an unreasonable ranking\nfor those points projected to the vertex.\n3.5\nExplicitness of Parameter Size\nDeﬁnition 6. ϕ(x) has the property of explicitness if ϕ(x)\nhas known parameter size for a fair comparison among ranking\nmodels.\nHu et al. [13] considered that nonparametric ap-\nproaches are a class of “black-box” approaches since\nthey can not be interpreted by our intuition. As a rank-\ning function, ϕ(x) should be semantically interpretable\nso that ϕ(x) has systematical meanings. For example,\nϕ(x) = θTx gives explicitly the linear expression with\nparameter size d which is the dimension of the parameter\nθ. It can be interpreted that the score of x is linear\nwith x and the parameter θ is the allocation proportion\nvector of indicators for ranking. Moreover, if there is\nanother ranking model with the same characteristics,\nϕ(x, θ) would be more applicable if it has a smaller size\nof parameters.\nThese ﬁve meta-rules above is the guidance of de-\nsigning a reasonable and practical ranking function. To\nperform a ranking task, a ranking function should satisfy\nall the ﬁve meta-rules above to produce a convincing\nranking list. Any ranking function that breaks any of\nthem would produce a biased and unreasonable ranking\nlist. In this sense, they can be regarded as high-level\nassessments for unsupervised ranking performance.\n6\nFig. 5. Schematic plots of ranking skeletons (heavy solid\nlines or curves in red). Circle points: observations of\ncountries on two indicators: LEB and GDP.\n4\nRANKING PRINCIPAL CURVES\nIn this section, we propose a ranking principal curve\n(RPC) model to perform an unsupervised ranking task\nwith a principal curve which has all the ﬁve meta-rules.\nThe RPC is parametrically designed to a cubic B´ezier\ncurve with control points restricted in the interior of a\nhypercube.\n4.1\nRPC Model\nThe simplest ranking rule is the ﬁrst PCA which sum-\nmarizes the data in d-dimensional space with the largest\nprincipal component line [25]. The ﬁrst PCA seeks the\ndirection w that explains the maximal variance of the\ndata cloud. Then x is orthogonally projected by wT x\nonto the line passing through the mean µ. The line\ncan be regarded as the ranking skeleton. Projected points\ntake an ordering along the ranking skeleton which is\njust the ordering of their ﬁrst principal components\ncomputed by wT x. Let s = wT x and an ordering of\ns gives the ordering of x. As a ranking function, the\nﬁrst PCA is smooth, explicitly expressed, and invariant\nto scale and translation. It works well for the skeleton of\nslender ellipse distributing data. However, the ﬁrst PCA\ncan hardly depict the skeleton of data distributions like\ncrescents (Fig. 5(a)) such that the produced ranking list\nis not convincing. What’s more, the ﬁrst PCA might be\nnon-strictly monotone when the direction w is parallel\nto one coordinate axis such that it can not discriminate\nthose points like x1 and x2 in Example 1 since they will\nbe projected to the same points if the ﬁrst PCA is on\nthe direction parallel to the horizontal line. The prob-\nlems referred above hinder the ﬁrst PCA from extensive\napplications in comprehensive evaluation.\nRecalling that principal curves are nonlinear exten-\nsions of the ﬁrst PCA [10], we try to summarize mul-\ntiple indicators in the data space with a principal curve\n(Appendix A gives a brief review of principal curves).\nAssuming f(s, θ)(θ ∈Θ) is the principal curve of a\ngiven data cloud, it provides an ordering of projected\npoints on the principal curve, in a way similar to the ﬁrst\nPCA. Intuitively, the principal curve is a good choice to\nperform ranking tasks. On the one hand, unsupervised\nranking could only rely on those numerical observa-\ntions for ranking candidates on given attributes. For the\ndataset with a linking network, PageRank can calculate\na score with backlinks for each point [2]. When there is\nno link between points, a score can still be calculated\naccording to the ranking skeleton, instead of link struc-\nture. On the other hand, the principal curve reconstructs\nx according to x = f(s, θ) + ε, instead of x = µ + sw + ε\nfor the ﬁrst PCA. To perform ranking tasks, a ranking\nfunction assigns a score s to x by s = ϕ(x, θ). Actually,\nnoise is inevitable due to measuring errors and inﬂuence\nfrom exclusive indicators from V. Thus the latent score\nshould be produce after removing noise from x, that is\ns = ϕ(x −ε, θ). As a ranking function, ϕ is assumed to\nbe strictly monotone. Thus, data points and scores are\none-to-one correspondence and there exists an inverse\nfunction f for ϕ such that\nx = f(s, θ) + ε\n(11)\nwhich is the very principal curve model [10]. The inverse\nfunction can be taken as the generating function for\nnumerical observations from the score s which can be\nregarded to be pre-existing.\nAs stated in Section 3, there are ﬁve meta-rules for a\nfunction ϕ(x, θ) to serve as a ranking rule. As ϕ(x, θ)\nis required to be strictly monotone, there exists an in-\nverse function f(s, θ) which is also strictly monotone by\nTheorem 2. Correspondingly, ϕ and its inverse f share\nthe other properties of scale and translation invariance,\nsmoothness, capacities of linearity and nonlinearity, and\nexplicitness of parameter size. A principal curve should\nalso follow all the ﬁve meta-rules to serve as a rank-\ning function. However, polyline approximations of the\nprincipal curve might go against smoothness and strict\nmonotonicity (e.g. Fig. 5(b)). A smooth principal curve\nwould also go against strict monotonicity (e.g. Fig. 5(c)).\nBoth of them would make unreasonable ranking solu-\ntions as illustrated in Example 1. Within the framework\nof Fig. 3, all the ﬁve meta-rules can be modeled as\nconstraints to the ranking function. Since a principal\ncurve is deﬁned to be smooth and invariant to scale\nand translation [10], the constraint of strict monotonicity\nwould make it be capable of performing ranking tasks\n(e.g. Fig. 5(d)). Naturally, the principal curve should\nhave a known parameter size for interpretability reason.\nWe present Deﬁnition 7 for unsupervised ranking with\na principal curve.\nDeﬁnition 7. A curve f(x, θ) in d-dimensional space is\ncalled a ranking principal curve (RPC) if f(x, θ) is a strictly\nmonotone principal curve of given data cloud and it is\nexplicitly expressed with known parameters θ of limited size.\n7\n4.2\nRPC Formulation with B´ezier Curves\nTo perform a ranking task, a principal curve model\nshould follow all the ﬁve meta-rules (Section 3) which\ncan be also similarly deﬁned for f. However, not all\nof principal curve models can perform ranking tasks.\nThe models in [10], [26], [27], [29] lack of explicitness\nand can not make a monotone mapping on Rd (Fig.\n5(c)). Polyline approximation [11], [19], [28] misses the\nrequirements for smoothness and strictly monotonicity\n(Fig. 5(b)). A new principal curve model is needed to\nperform ranking while following all the ﬁve meta-rules.\nIn this paper, an RPC is parametrically modeled with\na B´ezier curve\nf(s) =\nk\nX\nr=0\nBk\nr (s)pr, s ∈[0, 1]\n(12)\nwhich is formulated in terms of Bernstein polynomials\n[31]\nBk\nr (s)\n=\n\u0012k\nr\n\u0013\n(1 −s)k−rsr,\n(13)\n\u0012k\nr\n\u0013\n=\nk!\nr!(k −r)!.\n(14)\nIn Eq.(12), pr ∈Rd are control and end points of the\nB´ezier curve which are in the place of the function\nparameters θ in Eq.(11). Particularly, when k = 3, Eq.(12)\nhas the matrix form of\nf(s) = PMz.\n(15)\nwhere\nP = (p0, p1, p2, p3)\nM =\n\n\n\n\n1\n−3\n3\n−1\n0\n3\n−6\n3\n0\n0\n3\n−3\n0\n0\n0\n1\n\n\n\n, z =\n\n\n\n\n1\ns\ns2\ns3\n\n\n\n\nIn case k > 3, the model would become more complex\nand bring about overﬁtting problem. In case k < 3, the\nmodel is too simple to represent all possible monotonic\ncurves. k = 3 is the most suitable degree to perform the\nranking task.\nA cubic B´ezier curve with constraints on control points\ncan be proved to have all the ﬁve meta-rules. First of\nall, the formulation Eq.(12) is a nonlinear interpolation\nof control points and end points in terms of Bern-\nstein polynomials [31]. These points are the determinant\nparameters of total size d × 4. Different locations of\nthese points would produce different shapes of nonlinear\ncurves besides straight lines [14]. Scale and translation\nto B´ezier curves are applied to these points without\nchanging the ranking score which is contained in z\nΛf(s) + β = ΛPMz + β = (ΛP + β)Mz\n(16)\nwhere Λ is a diagonal matrix with scaling factors to\ndimensions and β is the translation vector. This property\nallows us to put all data into [0, 1]d in order to facilitate\nranking. What’s more, the derivative of f(s) is a lower\norder B´ezier curve\ndf(s)\nds\n= k\nk−1\nX\nj=0\nBk−1\nj\n(s)(pj+1 −pj)\n(17)\nwhich involves the calculation of end points and control\npoints. Its derivatives of all orders exist for all s ∈[0, 1]\nand thus Eq.(12) is smooth enough. Last but not the\nleast, it has been proved that a cubic B´ezier curve can\nperform the four basic types of strict monotonicity in\ntwo-dimensional space [14]. Let end points after scale\nand translation are denoted by p0 =\n1\n2(1 −α) and\np3\n=\n1\n2(1 + α). Control points p2 and p3 are the\ndeterminants for nonlinearity of the cubic B´ezier curve\n(Fig. 4). In two-dimensional space, f(s) is proved to be\nincreasing along each coordinate if control points are\nrestricted in the interior of the hypercube [0, 1]d [14].\nThus, a proposition can be deduced by Lemma 1.\nProposition 1. f(s) is strictly monotone for s ∈[0, 1] with\np0 = 1\n2(1 −α), p3 = 1\n2(1 + α) and p1, p2 ∈(0, 1)d.\nWhat is the most important, there always exists an\nRPC parameterized by a cubic B´ezier curve which is\nstrictly monotone for a group of numerical observations.\nThe existence has failed to be proved in many principal\ncurve models [10], [19], [28].\nTheorem 3. Assume that x is the numerical observation of\na ranking candidate and that E∥x∥2 < ∞. There exists P∗∈\n[0, 1]d such that f ∗(s) = P∗Mz is strictly monotone and\nJ(P∗) = inf\nn\nJ(P) = E\n\u0010\ninf\ns ∥x −PMz∥2\u0011o\n.\n(18)\nProof of Theorem 3 can be found in Appendix C.\n5\nRPC LEARNING ALGORITHM\nTo perform unsupervised ranking from the numerical\nobservations of ranking candidates X = (x1, x2, · · · , xn),\nwe should ﬁrst learn control points of the curve in\nEq.(12). The optimal points achieve the inﬁmum of the\nestimation of J(P) in Eq.(18). By the principal curve\ndeﬁnition proposed by Hastie et al. [10], the RPC is\nthe curve which minimizes the summed residual ε.\nTherefore, the ranking task is formulated as a nonlinear\noptimization problem\nmin\nJ(P, s) =\nn\nX\ni=1\n∥xi −PMzi∥2\n(19)\ns.t.\n\u0012∂PMz\n∂s\n\u0013T\n(xi −PMz)\n\f\f\f\f\f\ns=si\n= 0,\n(20)\ns = (s1, s2, · · · , sn), z = (1, s, s2, s3)T\nP ∈[0, 1]d×4,\nsi ∈[0, 1],\ni = 1, 2, · · · , n\nwhere Eq.(20) determines si to ﬁnd the point on the\ncurve which has the minimum residual to reconstruct\n8\nxi by f(si). Obviously, a local minimizer (P∗, s∗) can be\nachieved in an alternating minimization way\nP(t+1) = arg min\nn\nX\ni=1\n∥xi −PMz(t)\ni ∥2\n(21)\n\u0012∂P(t+1)Mz\n∂s\n\u0013T \u0010\nxi −P(t+1)Mz\n\u0011\f\f\f\f\f\ns=s(t+1)\ni\n= 0\n(22)\nwhere t means the tth iteration.\nThe optimal solution of Eq.(21) has an explicit expres-\nsion. Associate X with Z\nZ =\n\n\n\n\n1\n1\n· · ·\n1\ns1\ns2\n· · ·\nsn\ns2\n1\ns2\n2\n· · ·\ns2\nn\ns3\n1\ns3\n2\n· · ·\ns3\nn\n\n\n\n= (z1, z2, · · · , zn)\n(23)\nand Eq.(19) can be rewritten in matrix form\nJ(P, s)\n=\n∥X −PMZ∥F\n=\ntr(XT X) −2tr(PMZXT )\n+tr(PMZZT MT PT ).\n(24)\nSetting the derivative of J with respect to P to zero\n∂J\n∂P = 2\n\u0000P(MZ)(MZ)T −X(MZ)T \u0001\n= 0\n(25)\nand remembering A+ = AT (AAT )+ [35], we get an\nexplicitly expression for the minimum point of Eq.(19)\nP = X(MZ)T \u0000(MZ)(MZ)T \u0001+ = X(MZ)+\n(26)\nwhere (· )+ takes pseudo-inverse computation. Based\non the tth iterative results Z(t), the optimal solution\ncan be given by substituting Z(t) into Eq.(26) which\nis P(t+1) = X(MZ(t))+. However, (MZ(t))+ is compu-\ntationally expensive in numerical experiments and X\nis always ill-conditioned which has a high condition\nnumber, resulting in that a very small change in Z(t)\nwould produce a tremendous change in P(t+1). Z(t) is\nnot the optimal solution of Eq.(19) but a intermediate\nresult of the iteration, and P(t+1) would thereby go\nfar away from the optimal solution. To settle out the\nproblem, we employ the Richardson iteration [37] with\na preconditioner D which is a diagonal matrix with the\nL2 norm of columns of (MZ(t))(MZ(t))T as its diagonal\nelements. Then P(t+1) is updated according to\nP(t+1)\n=\nP(t) −γ(t)(P(t)(MZ(t))(MZ(t))T\n−X(MZ(t))T )D−1\n(27)\nwhere γ(t) is a scalar parameter such that the sequence\nP(t) converges. In practice, we set\nγ(t) =\n2\nλ(t)\nmin + λ(t)\nmax\n(28)\nwhere λ(t)\nmin and λ(t)\nmax is the minimum and maximum\neigenvalues of (MZ(t))(MZ(t))T respectively [38].\nAfter getting P(t+1), the score vector s(t+1) can be\ncalculated as the solution to Eq.(22). Eq.(22) is a quin-\nAlgorithm 1 Algorithm to learn an RPC.\nInput:\nX: data matrix;\nξ: a small positive value;\nOutput:\nP∗: control points of the learned B´ezier curve\ns∗: the score vector of objects in the set.\n1: Normalize X into [0, 1]d;\n2: Initialize P(0);\n3: while △J > ξ do\n4:\nAdopt GSS to ﬁnd the approximate solution s(t);\n5:\nCompute P(t+1) using a preconditioner;\n6:\nif △J < 0 then\n7:\nbreak;\n8:\nend if\n9: end while\ntic polynomial equation which rarely has explicitly ex-\npressed roots. In [20], si for xi was approximated by Gra-\ndient and Gauss-Newton methods respectively. Jenkins-\nTraub method [32] was also considered to ﬁnd the\nroots of the polynomial equation directly. As Eq.(20) is\ndesigned to ﬁnd the minimum distance of point xi from\nthe curve, we adopt Golden Section Search (GSS) [33] to\nﬁnd the local approximate solution to Eq.(22).\nAlgorithm 1 summarizes the alternative optimization\nprocedure. Before performing the ranking task, numer-\nical observations of objects should be normalized into\n[0, 1]d by\nˆx =\nx −xmin\nxmax −xmin\n(29)\nwhere ˆx is the normalized vector of x, xmin the min-\nimum vector and xmax the maximum vector. Grading\nscores would be unchanged as scaling and translating\nare only performed on control points and end points\n(Eq.(16)) without changing the interpolation values. In\nStep 2, we initialize the end points as p0 = 1\n2(1−α) and\np3 = 1\n2(1 + α), and randomly select samples as control\npoints. During learning procedure, P(t) is automatically\nlearned making a B´ezier curve to be an RPC in numerical\nexperiments. In Step 6, △J < 0 occurs when J begins\nto increase. In this case, the algorithm stops updating\n(P(t), s(t)) and gets a local minimum J. Proposition 2\nguarantees the convergency of the sequence found by\nRPC learning algorithm (proof can be found in Appendix\nD). Therefore, the RPC learning algorithm ﬁnds a con-\nverging sequence of (P(t), s(t)) to achieve the inﬁmum\nin Eq.(18).\nProposition 2. If P(t) →P∗as t →∞, J(P(t), s(t)) is a\ndecaying sequence which converges to J(P∗, s∗) as t →∞.\nAlgorithm 1 converges in limited steps. In each step,\nP is updated in 4 × d size and scores for points are\ncalculated in n size. When iteration stops, ranking scores\nare produced along with P. In summary, the computa-\ntional complexity of RPC unsupervised ranking model\n9\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nA\nB\nC\nA’\ns2\ns1\ns3\ns’1\nFig. 6. A, B and C are three objects to rank. s1, s2 and s3\nare scores given by the RPC (in green) of S-type shape\nin the ﬁgure. A different observation of A (denoted by A′)\nwould give a different RPC (in pink) and thus a different\nordering of objects.\nis O(4d + n). Compared to the ranking rule of weighted\nsummation, ranking with RPC model costs a little more.\nHowever, weighted summation needs weight assign-\nments by a domain expert such that it is more subjective\nbecause weights is diverse expert by expert. But RPC\nmodel needs no expert to assign weight proportions to\nindicators. The learning procedure of RPC model does\nthe whole work for ranking.\nThe RPC learning algorithm learns a ranking func-\ntion in a completely different way from the traditional\nmethods. On the one hand, the ranking function is in\nconstraints of ﬁve meta-rules for ranking rules. Integrat-\ning meta-rules with ranking functions makes the ranking\nrule be more in line with human knowledge about rank-\ning problems. As a high level knowledge, these meta-\nrules are capable of evaluating ranking performance.\nOn the other hand, ranking is carried out following the\nprinciple of unsupervised ranking, “let the data speak\nfor themselves”. For unsupervised ranking, there is no\ninformation for ranking labels to guide the system to\nlearn a ranking function. As a matter of fact, the structure\nof the dataset contains the ordinal information between\nobjects. If all the determining factors of ordinal relations\nare included, the RPC can thread through all the objects\nsuccessively. In practice, the most inﬂuential indicators\nare selected to estimate the order of objects, but the rest\nfactors still affect the numerical observation. In the case\nwe know nothing about the rest factors, we would better\nto minimize the effect which we formulate to be error ε.\nTherefore, minimizing errors is adopted as the learning\nobjection in case no ranking label can be available.\n6\nEXPERIMENTS\n6.1\nComparisons with Ranking Aggregation\nFor ranking task, some researchers prefer to aggregate\nmany different ranking lists of the same set of objects\nin order to get a “better” order. For example, median\nrank aggregation [34] aggregates different orderings into\na median rank with\nκ(i) =\nPm\nj=1 τj(i)\nm\n, i = 1, 2, · · · , n\n(30)\nwhere τj(i) is the location of object i in ranking list τj,\nτj is a permutation of {1, 2, · · · , n} and κ is the order-\ning of median rank aggregation. However, approaches\nof ranking aggregation suffers the difﬁculties of strict\nmonotonicity and smoothness. Therefore, the ranking list\nis not very convincing. What’s more, aggregation merely\ncombines the orderings and ignores the information\ndelivered by numerical observations.\nIn contrast, RPC is modeled following all the ﬁve\nmeta-rules which infers a reasonable ranking list. More-\nover, RPC can detect the ordinal information embed-\nded in the numerical observations, illustrated in Fig.\n6. Consider to rank three objects A, B and C in a\ntwo-dimensional space in Fig. 6. Let their numerical\nobservations on x1 and x2 be values shown in Table 1(a).\nObjects can be ordered along with x1 and x2 respectively.\nMedian rank aggregation [34] produces an ordering\nwhich can not distinguish A and B since they are in\nthe paratactic place of the ranking list. In contrast, the\nRPC model produce the order ABC where A and B are\nin a distinguishable order since RPC ranks objects based\non their original observation data. If there is a different\nobservation for one of objects, a different RPCwould pro-\nduce a different ranking list while RankAgg remains the\nsame. In Table 1(b), a different observation of object A is\nobtained, denoted as A′. A different RPC is learned (the\npink curve in Fig. 6) and gives the order BA′C (the last\ncolumn of Table 1(b)) which is different from the order\nin Table 1(a). In summary, RPC is able to capture the\nordinal information contained not only among ranking\ncandidates but also in the individual observation.\n6.2\nApplications\nUnsupervised ranking of multi-attribute observations of\nobjects has a widely applications. The most signiﬁcant\napplication is to rank countries, journals and universi-\nties. Taking the journal ranking task for illustration, there\nhave been many indices to rank journals, such as impact\nfactor (IF) [39] and Eigenfactor [40]. Different indices\nreﬂect different aspects of journals and provide different\nranking lists for journals. Thus, how to evaluate journals\nin a comprehensive way becomes a tough problem. RPC\nmodel is proposed as a new framework to attack the\nproblem which provides an ordering along the “ranking\nskeleton” of data distribution. In this paper, we perform\nranking tasks with RPCs to produce a comprehensive\nevaluation on three open access datasets of countries\nand journals with the open source software Scilab (5.4.1\nversion) on a Ubuntu 12.04 system with 4GB memory.\nDue to space limitation, we just list parts of their ranking\nlists (the full lists will be available when the paper is\npublished).\n10\nTABLE 1\nRPC model can detect ordinal information contained in numerical observations in Fig. 6.\n(a) A group of bservations and ranking lists by different rules\nObject\nx1\nx2\nRankAgg\nRPC\nValue\nOrder\nValue\nOrder\nScore\nOrder\nA\n0.3\n2\n0.25\n1\n1.5\n0.2329\n1\nB\n0.25\n1\n0.55\n2\n1.5\n0.3304\n2\nC\n0.7\n3\n0.7\n3\n3\n0.7300\n3\n(b) Another group of bservations and ranking lists by different rules\nObject\nx1\nx2\nRankAgg\nRPC\nValue\nOrder\nValue\nOrder\nScore\nOrder\nA′\n0.35\n2\n0.4\n1\n1.5\n0.3708\n2\nB\n0.25\n1\n0.55\n2\n1.5\n0.3431\n1\nC\n0.7\n3\n0.7\n3\n3\n0.7318\n3\nNote: Different observations of objects would produce different ranking lists of objects. In (a), objects A, B and C can be ordered by their values on x1 and x2\nrespectively. Ranking aggregation (RankAgg) then produce a comprehensive ordering by Eq.(30). But it fails to distinguish A and B which have distinguishable\nobservations while RPC can distinguish them. RPC can also detect the minor ordinal difference between objects. In (b), A has a different observation from (a),\nwhich is denoted as A′. Ranking lists keeps the same for RankAgg while RPC provides a different ordering.\nTABLE 2\nPart of the ranking list for life qualities of countries.\nCountry\nGDP1\nLEB2\nIMR3\nTuberculosis4\nElmap [8]\nRPC\nScore\nOrder\nScore\nOrder\nLuxembourg\n70014\n79.56\n6\n4\n0.892\n1\n1.0000\n1\nNorway\n47551\n80.29\n3\n3\n0.647\n2\n0.8720\n2\nKuwait\n44947\n77.258\n11\n10\n0.608\n3\n0.8483\n3\nSingapore\n41479\n79.627\n12\n2\n0.578\n4\n0.8305\n4\nUnited States\n41674\n77.93\n2\n7\n0.575\n5\n0.8275\n5\n...\n...\n...\n...\n...\n...\n...\n...\n...\nMoldova\n2362\n67.923\n63\n17\n0.002\n97\n0.5139\n96\nVanuatu\n3477\n69.257\n37\n31\n0.011\n96\n0.5135\n97\nSuriname\n7234\n68.425\n53\n30\n0.011\n95\n0.5133\n98\nMorocco\n3547\n70.443\n44\n36\n0.002\n98\n0.5106\n99\nIraq\n3200\n68.495\n25\n37\n-0.002\n100\n0.5032\n100\n...\n...\n...\n...\n...\n...\n...\n...\n...\nSouth Africa\n8477\n51.803\n349\n55\n-0.652\n167\n0.0786\n167\nSierra Leone\n790\n46.365\n219\n160\n-0.664\n169\n0.0541\n168\nDjibouti\n1964\n54.456\n330\n88\n-0.655\n168\n0.0524\n169\nZimbabwe\n538\n41.681\n311\n68\n-0.680\n170\n0.0462\n170\nSwaziland\n4384\n44.99\n422\n110\n-0.876\n171\n0\n171\np0\n44713\n81.218\n2\n0\n-\n-\n-\n-\np1\n330\n80.4\n2\n0\n-\n-\n-\n-\np2\n330\n59.7\n33\n43\n-\n-\n-\n-\np3\n1581.824\n41.68\n290\n151\n-\n-\n-\n-\n1 Gross Domestic Product per capita by Purchasing Power Parities, $per person;\n2 Life Expectancy at Birth, years;\n3 Infant Mortality Rate (per 1000 born);\n4 Infectious Tuberculosis, new cases per 100,000 of population, estimated.\n5 pj(j = 0, 1, 2, 3) are control and end points of the RPC.\n6.2.1\nResults on Life Qualities of Countries\nGorban et al. [8] ranked 171 countries by life qualities\nof people with data driven from GAPMINDER5 based\non four indicators as in Example 2. For comparison,\nwe also use the same four GAPMINDER indicators\nin [8]. The RPC learned by Algorithm 1 is shown in\ntwo-dimensional visualization in Fig. 7 and part of the\nranking list is illustrated in Table 2.\nFrom Fig. 7, RPC portrays the data distributing trends\nwith different shapes, including linearity and nonlinear-\n5. http://www.gapminder.org/\nity. For this task, α = [1, 1, −1, −1]T for this task just as\nExample 2. α also discovers the relationship between\nindicators for ranking. GDP is in the same direction\nwith LEB, but in the opposite direction with IMR and\nTuberculosis. In the beginning, a small amount of GDP\nincreasing brings about tremendous increasing of LEB\nand tremendous decreasing of IMR and Tuberculosis.\nWhen GDP exceeds $14300 (0.2 as normalized value in\nFig. 7) per person, increasing GDP does result in little\nLEB increase, so does IMR and Tuberculosis decrease.\nAs a matter of fact, it is hard to improve further LEB,\nIMR and Tuberculosis when they are close to the limit\n11\n0\n1\n0\n8\nGDP\n0\n1\n0\n1\nGDP\nLEB\n0\n1\n0\n1\nGDP\nTuberculosis\n0\n1\n0\n1\nGDP\nIMR\n0\n1\n0\n1\nLEB\nGDP\n0\n1\n0\n2.5\nLEB\n0\n1\n0\n1\nLEB\nTuberculosis\n0\n1\n0\n1\nLEB\nIMR\n0\n1\n0\n1\nTuberculosis\nGDP\n0\n1\n0\n1\nTuberculosis\nLEB\n0\n1\n0\n8\nTuberculosis\n0\n1\n0\n1\nTuberculosis\nIMR\n0\n1\n0\n1\nIMR\nGDP\n0\n1\n0\n1\nIMR\nLEB\n0\n1\n0\n1\nIMR\nTuberculosis\n0\n1\n0\n5\nIMR\nFig. 7. Two-dimensional display of data points and RPC\nfor life qualities of countries. Green points are numerical\nobservations and red curves are 2-dimensional projection\nof RPC.\nof human evolution.\nIn Table 2, control points provided by RPC learning\nalgorithm (Algorithm 1) are listed in the bottom. pi in\nthe bottom is given in the original data space. Although\nthe number of control points are set to two in addition\nto two end points, the number actually needed for each\nindicators is adapted automatically by learning. From\nTable 2, p0 and p1 for IMR and Tuberculosis overlaps\nwhich means that three points are enough for a B´ezier\ncurve to depicts the skeleton of IMR and Tuberculosis.\nTwo-dimensional visualizations in Fig. 7 tally with the\nstatement above.\nGorban et al. [8] provided centered scores for coun-\ntries, which is similar to the ﬁrst PCA. But the zero score\nis assigned to no country such that no country is taken\nas the ranking reference. In addition, rankers would get\ninto trouble to understand the ranking principle due to\nunknown parameter size. Therefore, the ranking list is\nhard to interpret for human understanding. Compared\nwith Elmap [8], the presented RPC model follows all the\nﬁve meta-rules. With these meta-rules as constraints, it\nachieves a better ﬁtting performance in term of Mean\nSquare Error (90% vs 86% of explained variance). It\nproduces scores in [0, 1] where 0 and 1 are the worst and\nthe best reference respectively. Luxembourg with the best\nlife quality provides a developing direction for countries\nbelow. Additionally, the RPC model is interpretable and\neasy to carry out in practice since there are just four\npoints to determine the ranking list.\n6.2.2\nResults on Journal Ranking\nWe also apply RPC model to rank journals with data\naccessable from the Web of Knowledge6 which is afﬁl-\niated to Thomson Reuters. Thomson Reuters publishes\nannually Journal Citation Reports (JCR) which provide\ninformation about academic journals in the sciences and\nsocial sciences. JCR2012 reports citation information with\nindicators of Impact Factor, 5-year Impact Factor, Im-\nmediacy Index, Eigenfactor Score, and Article Inﬂuence\nScore. After journals with data missing are removed\nfrom the data table (58 out of 451), RPC model tries\nto provide a comprehensive ranking list of journals in\nthe categories of computer science: artiﬁcial intelligence,\ncybernetics, information systems, interdisciplinary appli-\ncations, software engineering, theory and methods. Table\n3 illustrates the ranking list of journals produced by RPC\nmodel based on JCR2012. Two-dimensional visualization\nof the RPC is shown in Fig. 8.\nFor this ranking task, a journal will rank higher with\na higher value for each indicator, that is α = [1, 1, 1, 1].\nAmong all the indicators here, 5-year Impact Factor\nshows almost a linear relationship with the others. But\nEigenfactor presents no clear relationship which means\nthat it is calculated in a very different way from the other\nindicator. Actually, Eigenfactor works like PageRank [2]\nwhile the others take frequency count.\nFrom Table 3, IEEE Transactions on\nKnowledge\nand Data Engineering (TKDE) is ranked in a higher\nplace than IEEE Transactions on Systems, Man, and\nCybernetics-Part A (SMCA) although SMCA has a\nhigher IF (2.183) than TKDE (1.892). The lower inﬂuence\nscore (0.767) of SMCA brings it down the ranking list (vs.\n1.129 for TKDE). Therefore, TKDE gets a higher compre-\nhensive evaluating score and wins a higher ranking place\nin the ranking list. This means that one indicator does\nnot tell the whole story of ranking lists. RPC produces a\nranking list for journals taking account several indicators\nof different aspects.\n7\nCONCLUSIONS\nRanking and its tools have and will have an increasing\nimpact on the behavior of human, either positively or\nnegatively. However, those ranking activities are still\nfacing many challenges which have greatly restrained\nto the rational design and utilization of ranking tools.\nGenerally, ranking in practice is an unsupervised task\nwhich encounters a critical challenge that there is no\nground truth to evaluate the provided lists. PageRank [2]\nis an effective unsupervised ranking model for ranking\ncandidates with a link-structure. However, it does not\nwork for numerical observations on multiple attributes\nof objects.\nIt is well known that domain knowledge can always\nimprove the data mining performance. We try to attack\nunsupervised ranking problems by domain knowledge\n6. http://wokinfo.com/\n12\nTABLE 3\nPart of the ranking list for JCR2012 journals of computer sciences.\nTitle\nImpact Factor (IF)\n5-Year IF\nImmediacy Index\nEigenfactor\nInﬂuence Score\nRPC\nScore\nOrder\nScore\nOrder\nScore\nOrder\nScore\nOrder\nScore\nOrder\nScore\nOrder\nIEEE T PATTERN ANAL\n4.795\n7\n6.144\n5\n0.625\n26\n0.05237\n3\n3.235\n6\n1.0000\n1\nENTERP INF SYST UK\n9.256\n1\n4.771\n10\n2.682\n2\n0.00173\n230\n0.907\n86\n0.9505\n2\nJ STAT SOFTW\n4.910\n4\n5.907\n6\n0.753\n18\n0.01744\n20\n3.314\n4\n0.9162\n3\nMIS QUART\n4.659\n8\n7.474\n2\n0.705\n21\n0.01036\n49\n3.077\n7\n0.9105\n4\nACM COMPUT SURV\n3.543\n21\n7.854\n1\n0.421\n56\n0.00640\n80\n4.097\n1\n0.9092\n5\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nDECIS SUPPORT SYST\n2.201\n51\n3.037\n43\n0.196\n169\n0.00994\n52\n0.864\n93\n0.4701\n65\nCOMPUT STAT DATA AN\n1.304\n156\n1.449\n180\n0.415\n61\n0.02601\n11\n0.918\n83\n0.4665\n66\nIEEE T KNOWL DATA EN\n1.892\n82\n2.426\n72\n0.217\n152\n0.01256\n37\n1.129\n55\n0.4616\n67\nMACH LEARN\n1.467\n133\n2.143\n96\n0.373\n70\n0.00638\n81\n1.528\n20\n0.4490\n68\nIEEE T SYST MAN CY A\n2.183\n53\n2.44\n68\n0.465\n46\n0.00728\n69\n0.767\n111\n0.4466\n69\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n0\n1\n0\n6\nIF\n0\n1\n0\n1\nIF\n5IF\n0\n1\n0\n1\nIF\nImmInd\n0\n1\n0\n1\nIF\nEigenfactor\n0\n1\n0\n1\nIF\nIS\n0\n1\n0\n1\n5IF\nIF\n0\n1\n0\n4\n5IF\n0\n1\n0\n1\n5IF\nImmInd\n0\n1\n0\n1\n5IF\nEigenfactor\n0\n1\n0\n1\n5IF\nIS\n0\n1\n0\n1\nImmInd\nIF\n0\n1\n0\n1\nImmInd\n5IF\n0\n1\n0\n8\nImmInd\n0\n1\n0\n1\nImmInd\nEigenfactor\n0\n1\n0\n1\nImmInd\nIS\n0\n1\n0\n1\nEigenfactor\nIF\n0\n1\n0\n1\nEigenfactor\n5IF\n0\n1\n0\n1\nEigenfactor\nImmInd\n0\n1\n0\n10\nEigenfactor\n0\n1\n0\n1\nEigenfactor\nIS\n0\n1\n0\n1\nIS\nIF\n0\n1\n0\n1\nIS\n5IF\n0\n1\n0\n1\nIS\nImmInd\n0\n1\n0\n1\nIS\nEigenfactor\n0\n1\n0\n4\nIS\nFig. 8. Two-dimensional display of data points and RPC\nfor JCR2012. Green points are numerical observations\nand red curves are 2-dimensional projection of RPC.\n(IF:Impact Factor, 5IF:5-Year IF, ImmInd:Immediacy In-\ndex, IS: Inﬂuence Score)\nabout ranking. Motivated by [13], [16], ﬁve meta-rules\nas ranking knowledge are presented and are regarded as\nconstraints to ranking models. They are scale and trans-\nlation invariance, strict monotonicity, linear/nonlinear\ncapacities, smoothness and explicitness of parameter\nsize. They can also be capable of assessing the ranking\nperformance of different models. Enlightened by [8],\n[14], we propose a ranking model with a principal curve\nwhich is parametrically formulated with a cubic B´ezier\ncurve by restricting control points in the interior of\nthe hypercube [0, 1]d. Control points are learned from\nthe data distribution without human interventions. Ap-\nplications in life qualities of countries and journals of\ncomputer sciences show that the proposed RPC model\ncan produce reasonable ranking lists.\nFrom an application view points, there are many\nindicators for ranking objects. RPC can also be used to do\nfeature selection which is one part of our future works.\nAPPENDIX A\nPRINCIPAL CURVES\nGiven a dataset X = (x1, x2, · · · , xn), xi ∈Rd, a prin-\ncipal curve summarizes the data with a smooth curve\ninstead of a straight line in the ﬁrst PCA\nx = f(s) + ε\n(A-1)\nwhere f(s) = (f1(s), f2(s), · · · , fd(s)) ∈Rd and s ∈R.\nThe principal curve f was originally deﬁned by Hastie\nand Stuetzle [10] as a smooth (C∞) unit-speed (∥f ′′∥2 =\n1) one-dimensional manifold in Rd satisfying the self-\nconsistence condition\nf(s) = E (x| sf(x) = s)\nwhere s = sf(x) ∈R is the largest value so that f(s) has\nthe minimum distance from x. Mathematically, sf(x) is\nformulated as [10]\nsf(x) = sup\nn\ns : ∥x −f(s)∥= inf\ns ∥x −f(s)∥\no\n.\n(A-2)\nIn other words, a curve f : R 7→Rd is called a princi-\npal curve if it minimizes the expected squared distance\nbetween x and f which is denoted by [11]\nJ(f) = E\n\u0010\ninf\ns ∥x −f(s)∥2\u0011\n= E∥x −f(sf(x))∥2.\n(A-3)\nAs an one-dimensional principal manifold, the prin-\ncipal curve has a wide applications (e.g. [36]) due to\nits simpleness. Following Hastie and Stuetzle [10], re-\nsearchers afterwards have proposed a variety of princi-\npal curve deﬁnitions and learning algorithms to perform\ndifferent tasks [11], [19], [26], [29]. But most of them\ntried to ﬁrst approximate the principal curve ﬁrst with a\n13\npolyline [11] and then smooth it to meet the requirement\nfor smoothness [10] of the principal curve. Therefore,\nthe expression of the principal curve is not explicit\nand results in a ‘black-box’ which is hard to interpret.\nThe other deﬁnitions of principal curves [27], [30] em-\nployed Gaussian mixture model to generally formulate\nthe principal curve which brings model bias and makes\ninterpretation even harder. When the principal curve is\nused to perform a ranking task, it should be modeled\nto be a ‘white-box’ which can be well interpreted for its\nprovided ranking lists.\nAPPENDIX B\nPROOF OF THEOREM 2\nIf ∇ϕ(x) ≻0, ϕ is strictly monotone by Theorem 1.\nRegarding that the ranking candidates is totally ordered,\nthere is a one-to-one correspondence between ranking\nitems in Rd and rangϕ. Otherwise, s = ϕ(x0) and\ns = ϕ(x0 + △x) both hold for some x0 ∈domϕ. In this\ncase, ∇ϕ(x)|x=x0 = 0 which contradicts the assumption\n∇ϕ(x) ≻0 holds for all x ∈domϕ.\nBy Lemma 1 and the one-to-one correspondence, there\nexists an inverse mapping f : rangϕ 7→domϕ such that\nx = f(s). By strict monotonicity (Eq.(1)) and the one-to-\none correspondence, we have\nx1 ⪯x2,\nx1 ̸= x2 ⇐⇒s1 < s2\n(B-1)\nThus, ∇f(s) ≻0 holds for s ∈rangϕ.\n□\nAPPENDIX C\nPROOF OF RPC EXISTENCE (THEOREM 3)\nProof. Assume U = [0, 1] and C(U) denotes the set of all\ncontinuous function f : U 7→[0, 1]d ⊆Rd embracing all\npossible observations of x. The uniform metric is deﬁned\nas\nD(f, g) = sup\n0≤s≤1\n∥f(s) −g(s)∥,\n∀f, g ∈C(U).\n(C-1)\nIt is easy to see (C(U), D) is a complete metric space\n[21].\nLet Γ = {f(s) : f(s) = PMz, P ∈Θ} ⊆C(U), where\nΘ ∈[0, 1]4 is the convex hull of x. With the Frobenius\nnorm, Θ is a sequentially compact set so that for any\ngiven sequence in Θ there exists a subsequence P(t)\nconverging uniformly to an P∗∈[0, 1]d [21] with\n∥P(t) −P∗∥F\n→0\n(C-2)\nLet p0 = 1\n2(1 −α) and p3 = 1\n2(1 + α). Then we have a\nsequence f (t)(s) converging uniformly to f ∗(s):\nD\n\u0010\nf (t)(s), f ∗(s)\n\u0011\n= sup\n0≤s≤1\n∥f (t)(s) −f ∗(s)∥\n(C-3)\n≤\nsup\n0≤s≤1\n∥P(t) −P∗∥F ∥Mz∥\n(C-4)\n=\n∥P(t) −P∗∥F\n→0\n(C-5)\nwhere ∥Mz∥= 1. By Proposition 1, f (t)(s) is a curve\nsequence of strictly monotonicity and converges to f ∗(s).\nAssuming the converging sequence f (t)(s) makes\nJ(P(t)) ≥J(P∗) for ﬁxed x ∈Rd,\nJ(P(t)) −J(P∗)\n=\n∥x −f (t)(s)∥2 −∥x −f ∗(s)∥2\n(C-6)\n≤\n\u0010\n∥x −f (t)(s)∥+ ∥x −f ∗(s)∥\n\u0011\n∥f (t)(s) −f ∗(s)∥(C-7)\n→0\n(C-8)\nand therefore\nE\n\u0010\nJ(P(t)) −J(P∗)\n\u0011\n→0.\n(C-9)\nFinally, we complete the proof.\n□\nAPPENDIX D\nPROOF OF CONVERGENCE (PROPOSITION 2)\nProof: First of all, P(t) generated by Richardson method\nhas been proved to converge [37]. Assume P(t) →P∗,\nand s(t) and s∗are the corresponding score vectors\ncalculated by Eq.(22). Note that the item P(t+1) −P(t)\nis in the descending direction of J in Eq.(27). So we get\nthat\nJ(P(t), s(t)) ≥J(P(t+1), s(t)).\n(D-1)\nThen with the control points P(t+1), s(t+1) minimizes the\nsummed orthogonal distance\nJ(P(t+1), s(t)) ≥J(P(t+1), s(t+1)).\n(D-2)\nThus we get\nJ(P(t), s(t)) ≥J(P(t+1), s(t+1)).\n(D-3)\nFinally, by Theorem 3 the sequence {J(P(t), s(t))} con-\nverges to its inﬁmum {J(P∗, s∗)} as s →∞.\n□\nACKNOWLEDGEMENT\nThe authors appreciate very much the advice from the\nmachine learning crew in NLPR. This work is supported\nin part by NSFC (No. 61273196) for C.-G. Li and B.-G.\nHu, and NSFC (No. 61271430 and No. 61332017) for X.\nMei.\nREFERENCES\n[1]\nH. Li, “A Short Introduction to Learning to Rank”, IEICE Trans.\nInf. Syst., vol. E94-D, no. 10, pp. 1-9, 2011.\n[2]\nS. Brin and L. Page, “The Anatomy of a Large-Scale Hypertextual\nWeb Search Engine”, Computer Networks, vol. 30, no. 1-7, pp. 107-\n117, 1998.\n[3]\nD. Zhou, J. Weston, A. Gretton, O. Bousquet, and B. Sch¨olkopf,\n“Ranking on Data Manifolds”, Advances in Neural Information\nProcessing Systems 16, S. Thrun, L. Saul, and B. Sch¨olkopf, eds.,\nMIT Press, 2004.\n[4]\nB. Xu, J. Bu, C. Chen, D. Cai, X. He, W. Liu, and J. Luo, “Efﬁcient\nmanifold ranking for image retrieval”, Proc. 34th Int’l ACM SIGIR\nConf. Research and Development in Information Retrieval, pp. 525-534,\n2011.\n[5]\nC. Bishop, Pattern Recognition and Machine Learning, New York:\nSpringer, 2006.\n[6]\nI. Guyon and A. Elisseeff, “An Introduction to Variable and\nFeature Selection”, J. Mach. Learn. Res., vol. 3, pp. 1157-1182, 2003.\n[7]\nA. Klementiev, D. Roth, K. Small, and I. Titov, “Unsupervised\nRank Aggregation with Domain-Speciﬁc Expertise”, Proc. 20th\nInt’l Joint Conf. Artiﬁcal Intell., pp. 1101-1106, 2009.\n14\n[8]\nA.Y. Zinovyev and A.N. Gorban, “Nonlinear Quality of Life\nIndex”[EB/OL], New York, http://arxiv.org/abs/1008.4063, 2010.\n[9]\nA. Vasuki, “A Review of Vector Quantization Techiniques”, IEEE\nPotentials, vol. 25, no. 4, pp. 39-47, 2006.\n[10] T. Hastie and W. Stuetzle, “Principal Curves”, J. Amer. Stat. Assoc.,\nvol. 84, no. 406, pp. 502-516, 1989.\n[11] B. K´egl, A. Krzy˙zak, T. Linder, and K. Zeger, “Learning and\nDesign of Principal Curves”, IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 22, no. 3, pp. 281-297, 2000.\n[12] P. Gould, “Letting the Data Speak for Themselves”, Assoc. Amer.\nGeog. USA, vol. 71, no. 2, 1981.\n[13] B.-G. Hu, H.B. Qu, Y. Wang, and S.H. Yang, “A Generalized-\nConstraint Neural Network model: Associating Partially Known\nRelationships for Nonlinear Regression”, Inf. Sci., vol. 179, pp.\n1929-1943, 2009.\n[14] B.-G. Hu, G.K.I Mann, and R.G. Gosine, “Control curve design\nfor nonlinear (or fuzzy) proportional actions using spline-based\nfunctions”, Automatica, vol. 34, no. 9, pp. 1125-1133, 1998.\n[15] H. Daniels and M. Velikova, “Monotone and Partially Monotone\nNeural Networks”, IEEE Trans. Neural Networks, vol. 21, no. 6, pp.\n906-917, 2010.\n[16] W. Kotłowski and R. Słowi´nski, “On Nonparametric Ordinal\nClassiﬁcation with Monotonicity Constraints”, IEEE Trans. Knowl.\nData Engineering, vol. 25, no. 11, pp. 2576-2589, 2013.\n[17] Y. Zhang, W. Zhang, J. Pei, X. Lin, Q. Lin, and A. Li, “Consensus-\nBased Ranking of Multivalued Objects: A Generalized Borda\nCount Approach”, IEEE Trans. Knowl. Data Engineering, vol. 26,\nno. 1, pp. 83-96, 2014.\n[18] X.Q. Cheng, P. Du, J.F. Guo, X.F. Zhu, and Y.X. Chen, “Ranking\non Data Manifold with Sink Points”, IEEE Trans. Knowl. Data\nEngineering, vol. 25, no. 1, pp. 177-191, 2013.\n[19] A.N. Gorban and A.Y. Zinovyev, “Chapter 2: Principal Graphs\nand Manifolds”, Handbook of Research on Machine Learning Appli-\ncations and Trends: Algorithms, Methods, and Techiniques, E.S. Olivas,\nJ.D.M. Guerrero, M.M. Sober, J.R.M. Benedito, A.J.S. L´opez, eds.,\nNew York: Inf. Sci. Ref., vol. 1, pp. 28-59, 2010.\n[20] T.A. Pastva, “B´ezier Curve Fitting”, master’s thesis, Naval Post-\ngraduate School, 1998.\n[21] S. Boyd and L. Vandenberghe, Convex Optimization, New York:\nCamb. Univ. Press, 2004.\n[22] H.A. Priestley, “Chapter 2: Ordered Sets and Complete Lattices-a\nPrimer for Computer Science”, Algebraic and Coalgebraic Methods\nin the Mathematics of Program Construction, R. Backhouse, R. Crole,\nJ. Jibbons, eds., LNCS 2297, pp. 21-78, 2002.\n[23] P.M. Fitzpatrick, Advanced Calculus, CA: Thomson Brooks/Cole,\n2006.\n[24] A. Cambibi, D.T. Luc, and L. Martein. “Order-Preserving Trans-\nformations and Applications”, J. Optimization Theory Applications,\nvol. 118, no. 2, pp. 275-293, 2003.\n[25] T.W. Anderson, An Introduction to Multivariate Statistical Analysis,\nNew Jersey: John Wiley & Sons, Inc. 2003.\n[26] J.D. Banﬁeld and A.E. Raftery, “Ice Floe Identiﬁcation in Satellite\nImages Using Mathematical Morphology and Clustering about\nPincipal Curves”, J. Amer. Stat. Assoc., vol. 87, no. 417, pp. 7-16,\n1992.\n[27] P. Delicado, “Another look at principal curves and surfaces”, J.\nMultivariate Anal., vol. 77, no. 1, pp. 84-116, 2001.\n[28] K. Chang and J. Ghosh, “A Uniﬁed Model for Probabilistic\nPrincipal Surfaces”, IEEE Trans. Pattern Anal. Mach. Intell., vol.\n23, no. 1, pp. 22-41, 2001.\n[29] J. Einbeck, G. Tutz, and L. Evers, “Local Principal Curves”, Stat.\nand Comput., vol. 15, no. 4, pp. 301-313, 2005\n[30] R. Tibshirani, “Principal Curves Revisited”, Stat. and Comput., vol.\n2, no. 4, pp. 183-190, 1992.\n[31] G. Farin, Curves and Surfaces for Computer Aided Geometric Design\n(4th Edition), California: Acad. Press, Inc., 1997.\n[32] M. A. Jenkins and J. F. Traub, “A Three-Stage Algorithm for Real\nPolynomials Using Quadratic Iteration”, SIAM J. Numer. Anal.,\nvol. 7, no. 44, pp. 545566, 1970.\n[33] M.S. Bazaraa, H.D. Sherali, and C.M. Shetty, Nonlinear Program-\nming: Theory and Algorithms, New Jersey, Hoboken: John Wiley &\nSons, Inc., 2006.\n[34] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar, “Rank Aggre-\ngation Methods for the Web”, Proc. 10th Int’l Conf. World Wide\nWeb, pp. 613-622, 2001.\n[35] R.A. Roger and C.R. Johnson, Matrix Analysis, New York: Camb.\nUniv. Press, 1985.\n[36] J.P. Zhang, X.D. Wang, U.Kruger, F.Y. Wang, “Principal Curve Al-\ngorithms for Partitioning High-Dimensional Data Spaces”, IEEE\nTrans. Neural Networks, vol. 22, no. 3, pp. 367-380, 2011.\n[37] L.F. Richardson, “The approximate arithmetical solution by ﬁnite\ndifferences of physical problems involving differential equations,\nwith an application to the stresses in a masonry dam”, Philos.\nTrans. Roy. Soc. London Ser. A, vol. 210, pp. 307-357, 1910.\n[38] G. H. Golub and C. F. van Loan, Matrix Computations, 3rd ed.,\nBaltimore, MD: Johns Hopkins, 1996.\n[39] E. Garﬁeld, “The History and Meaning of the Journal Impact\nFactor”, J. Amer. Med. Assoc., vol. 295, no. 1, pp. 90-93, 2006.\n[40] C.T. Bergstrom, J.D. West, M.A. Wiseman,“The Eigenfactor Met-\nrics”, J. of Neuroscience, vol. 28, no. 45, pp. 11433-11434, 2008.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2014-02-19",
  "updated": "2014-02-19"
}