{
  "id": "http://arxiv.org/abs/1402.4884v2",
  "title": "Le Cam meets LeCun: Deficiency and Generic Feature Learning",
  "authors": [
    "Brendan van Rooyen",
    "Robert C. Williamson"
  ],
  "abstract": "\"Deep Learning\" methods attempt to learn generic features in an unsupervised\nfashion from a large unlabelled data set. These generic features should perform\nas well as the best hand crafted features for any learning problem that makes\nuse of this data. We provide a definition of generic features, characterize\nwhen it is possible to learn them and provide methods closely related to the\nautoencoder and deep belief network of deep learning. In order to do so we use\nthe notion of deficiency and illustrate its value in studying certain general\nlearning problems.",
  "text": "Le Cam meets LeCun: Deﬁciency and Generic Feature Learning\nBrendan van Rooyen\nBRENDAN.VANROOYEN@ANU.EDU.AU\nRobert C. Williamson\nBOB.WILLIAMSON@ANU.EDU.AU\nAustralian National University and NICTA, Canberra ACT 0200, Australia\nAbstract\n“Deep Learning” methods attempt to learn generic features in an unsupervised fashion from a large\nunlabelled data set. These generic features should perform as well as the best hand crafted features\nfor any learning problem that makes use of this data. We provide a deﬁnition of generic features,\ncharacterize when it is possible to learn them and provide algorithms closely related to the deep\nbelief network and autoencoders of deep learning. In order to do so we use the notion of deﬁciency\ndistance and illustrate its value in studying certain general learning problems.\n1. Introduction\n“Deep” unsupervised feature learning methods (Bengio, 2009; Hinton and Salakhutdinov, 2006;\nVincent et al., 2008; Tenenbaum et al., 2000; LeCun, 2013) present a challenge to learning theory.\nThis paper takes up this challenge, of explaining when and why these techniques work. Is it possible\nto learn generic features from data in an unsupervised fashion that perform well in a multitude of\ntasks, and if so how do we learn these features? Following the work of the statistician Lucien Le\nCam (Lecam, 2011, 1964, 1974) and utilizing the techniques of statistical decision theory, in partic-\nular the comparison of statistical experiments (Blackwell, 1951, 1953; Torgersen, 1991; Ferguson,\n1967) we show (theorem 3) that it is possible to construct generic features Z from data X if and\nonly if one can ﬁnd a encoder/decoder pair\nX\nencoder\n/ Z\ndecoder\n/ X\nwith low probability of reconstruction error. Furthermore, the worst case difference in performance\nof the best decision rule that uses such features versus the best decision rule that uses the raw data\nis bounded above by the probability of reconstruction error. We also show that we can learn this\nencoder/ decoder pairing in a hierarchical fashion and that the probability of reconstruction error of\nsuch a“stacked” system is bounded by the sum of the probability of reconstruction errors of each\nlayer.\nWhile our approach is abstract, the ultimate pay off will be a novel inequality (theorem 2) that pro-\nvides a characterization of when generic feature learning is possible. This inequality coupled with\nthe concept of deﬁciency (Lecam, 2011; Torgersen, 1991) (to be explained in the paper) illuminates\nthe algorithms used in deep learning and provides means to judge the generic quality of the features\nlearnt by such methods.\narXiv:1402.4884v2  [stat.ML]  21 Feb 2014\nVAN ROOYEN WILLIAMSON\n2. The General Learning Problem\nFor all of the following assume that all of the measure spaces Θ, A, X, Y, Z and so on are ﬁnite.\nThis does not restrict any of the results, rather it allows for a cleaner presentation free of measure\ntheoretic technicalities as well as boundedness and existence concerns.\nA learning problem is a quintuple (Θ, X, T, A, L). Θ is a set of possible “true hypothesis” or un-\nknowns. While we cannot observe Θ directly, we can observe data in some set X. T is a relationship\nbetween the two sets Θ and X called the experiment. T(θ) tells us what data we expect to see if θ\nis the true hypotheses. Ultimately we are required to make a decision by choosing an action a ∈A,\nand our performance is measured by a loss function L : Θ×A →R. We view the loss as an integral\npart of a learning problem and as such do not place any restrictions on it other than boundedness.\nAs is usual in statistical learning, for our possible relationships we use markov kernels (conditional\nprobability assignment/stochastic matrices).\nDeﬁnition 1 A Markov kernel T : Θ ⇝X is a function from Θ to P(X), the set of probability\ndistributions on X.\nFor arbitrary sets Y and Z, denote by M(Y, Z), the set of all Markov kernels from Y to Z. As we\ncan represent P(Y ) by vectors in R|Y | with positive entries we have P(Y ) ⊂R|Y |. As such one can\nrepresent a Markov kernel T : Y ⇝Z as an |Z| × |Y | matrix of positive entries where the sum of\nall entries in each column is equal to 1. It is easily veriﬁed that M(Y, Z) is a closed convex subset\nof R|Z|×|Y |, the set of all |Z| × |Y | matrices.\nA function f : Y →Z deﬁnes a Markov kernel F with F(y) = δf(y), a point mass distribution\non f(y). For every measure space X, there are two special Markov kernels, the identity (or com-\npletely informative) Markov kernel from the identity function idX : X →X, and the completely\nuninformative Markov kernel from the function •X : X →• from X to a one element set •.\nFrom a prior distribution π ∈P(Θ) and a Markov kernel T : Θ ⇝X we can construct a joint\ndistribution T ⊗π ∈P(Θ × X). Using the matrix vector representation, this is achieved by post\nmultiplying T with a diagonal matrix with π on the diagonal, T ⊗π = T diag(π). This is no\ndifferent to the standard product rule P(θ, x) = P(x|θ)P(θ).\nGiven a prior distribution π ∈P(Θ) and a Markov kernel T : Θ ⇝X we denote the Markov kernel\nobtained by Bayes rule by T ∗: X ⇝Θ.\nA learning problem can more compactly be represented as the pair (L, T) where Θ, A, X can be\ninferred from the type signatures of L and T. We measure the size of loss functions by ∥L∥∞=\nsupθ,a |L(θ, a)|\n2.1. Decision Rules\nUpon observing data x ∈X we are required to relate x to a set of actions A by some other Markov\nkernel d : X ⇝A known as a (randomized) decision rule.\nΘ\nT\n/ X\nd\n/ A\nWe are judged on the quality of the composed relation d ◦T : Θ ⇝A.\n2\nLE CAM MEETS LECUN\nDeﬁnition (Composition) Suppose T1 : X ⇝Y and T2 : Y ⇝Z are Markov kernels. Then we\ncan compose T1 and T2 yielding T3 : X ⇝Z by matrix multiplication\nT3 = T2 ◦T1 = T2T1.\nA Markov kernel T : X ⇝Y provides a function T : P(X) →P(Y ) by matrix multiplication.\nTo calculate T(π), π ∈P(X) we identify π with a vector and T with a matrix and use matrix\nmultiplication. This function is convex linear.\n2.2. Risk and Value: Ranking Decision Rules and Learning Problems\nGiven a learning problem (Θ, X, T, A, L) one can rank decision rules d : X ⇝A using the full\nBayes risk\nRL : P(Θ) × M(Θ, A) →R\n(π, D) 7→Eθ∼πEa∼D(θ)L(θ, a).\nHere π ∈P(Θ) is a prior distribution on Θ which reﬂects which hypotheses we feel are more or less\nlikely to be true. Note that both P(Θ) and M(Θ, A) are convex sets and that RL is convex bilinear\n(the same as bilinear but restricted to convex combinations). Alternately, taking a supremum over\nthe prior yields the max risk.\nRanking Learning Problems. We also rank the difﬁculty of learning problems. The greatest\nchallenge in a learning problem comes from the fact we can not use an arbitrary decision rule\nD ∈M(Θ, A). Rather, we are restricted to a certain subset of M(Θ, A) that “factors through” T.\nWe are only allowed to use the data we see.\nDeﬁnition 2 (Factoring Through) Suppose we have two Markov kernels T : X ⇝Z and U :\nX ⇝Y . We say that U factors through T (written T|U) if there exists a Markov kernel U/T : Z ⇝\nY such that U = (U/T) ◦T. Denote by\nM(X, Y )T := {U ∈M(X, Y ) : U = (U/T) ◦T for some U/T ∈M(Z, Y )}.\nIf T|U then U can be thought of as T with extra noise U/T. The reader is directed to section 1 of\nthe appendix for more properties of factoring through. In this notation d = D/T. If T|D we have\nRL(π, D) = RL(π, (D/T) ◦T)\n= Eθ∼πEa∼(D/T)◦T(θ)L(θ, a)\n= Eθ∼πEx∼T(θ)Ea∼(D/T)(x)L(θ, a).\nWe assign a value\nV(π, Θ, X, T, A, L) = VL(π, T) :=\ninf\nD∈M(Θ,A)T\nRL(π, D)\nto a learning problem, with lower value being better. Taking a supremum of the value over the prior\nyields the minimax risk. The value is the risk of the best possible decision rule for the learning\nproblem at hand.\n3\nVAN ROOYEN WILLIAMSON\nBayes Decision Rules are Optimal. If we use VL(π, T) to order decision rules then the best\nD ∈M(Θ, A)T is found by using Bayes rule\n(D/T)(x) = arg inf\na\nEθ∼T ∗(x)L(θ, a),\nwith risk\nRL(π, D) = VL(π, T) = Ex∼πX inf\na Eθ∼T ∗(x)L(θ, a) = Ex∼πXL(T ∗(x)),\nwhere L : P(Θ) →R, L(π) = infa Eθ∼πL(θ, a). Hence L is concave. This results allows us to\nparametrize the action set A by P(Θ), by taking\nf : P(Θ) →A\nπ 7→arg inf\na\nEθ∼P L(θ, a)\neffectively properising the loss function. ˆL(θ, π) := L(θ, f(π)) is a proper loss (Reid and Williamson,\n2011; Dawid, 2007; Gr¨unwald and Dawid, 2004; Parry et al., 2012). There are deep connections\nbetween L and ˆL, we review some of these in section two of the appendix.\nConnections to other Information Measures.\nThere are many connections between VL(π, T) and different information measures present in the\nliterature.\nDeﬁnition For a convex f : Rn−1\n+\n→R the f-information of a set of n distributions P1, . . . , Pn ∈\nP(X) is\nIf(P1, . . . , Pn) :=\nZ\nX\nf(dP2\ndP1\n, . . . , dPn\ndP1\n)dP1.\nf-informations are a multi distribution extension of f divergences and are used in certain general-\nizations of rate-distortion theory where they produce better bounds than the standard techniques (Ziv\nand Zakai, 1973; Zakai and Ziv, 1975; Reid and Williamson, 2011; Garcia-Garcia and Williamson,\n2012). For suitable choices of f one can recover more known measures of information such as\nmutual information.\nTheorem For all experiments T, loss functions L and priors π, the gap between then the value of\nT and the least informative experiment •Θ is a f-information for suitable f\nVL(π, •Θ) −VL(π, T) = If(T(θ1), . . . , T(θn)) = If(T)\nWe direct the reader to Reid and Williamson (2011); Garcia-Garcia and Williamson (2012). By\nthe bijections presented in these two papers, one can replace VL(π, T) by these divergences in all\nthat follows. In particular any |VL(π, T) −VL(π, U)| can be replaced with a |If(T) −If(U)| for\nsuitable f, with no effect on the result. The reader is directed to section 3 of the appendix for proof.\n2.3. Examples\nHere we present some examples of familiar learning problems phrased in this more abstract lan-\nguage. Normally there is a distinction between learning algorithms, something that takes a data\nset of n instance-label pairs and produces a classiﬁer, and a decision rule that is the learnt classi-\nﬁer. Here we do not make such a distinction. Both learning algorithms and decision rules produce\nactions, hence we only use the term decision rule.\n4\nLE CAM MEETS LECUN\nExample 1 (Classiﬁcation) Θ = {−1, 1} and a Markov kernel T : Θ ⇝X is then a pair of\ndistributions T(1) = P, T(−1) = Q on X. Normally A = Θ and a decision rule picks the\ncorresponding label for a given observed x ∈X. Different losses could be used, eg the 0-1 loss\nL01.\nExample 2 (Supervised Learning) There is a space of labels Y = {−1, 1} and a space of co-\nvariates Z with X = (Z × Y )n. Θ = P(Z × Y ) the space of joint distributions on Z × Y with\nT : Θ ⇝X the map that sends each distribution to its n-fold product. A is then some set of classi-\nﬁers (eg linear hyperplanes/kernel machines and so on). A decision rule then produces a classiﬁer\na from n pairs (zi, yi). For example empirical risk minimization algorithms, ERM : X →A, pick\nthe classiﬁer that minimizes the empirical loss on the observed training set. Many suitable losses\nexists but normally L(θ, a) is the misclassiﬁcation probability of the classiﬁer a when used against\nthe distribution θ.\nExample 3 (Generalized Supervised Learning) Θ and A are the same as supervised learning,\nalthough the data observed is different. For example in semi-supervised learning we have X =\n(Z × Y )n × Zm, we observe n instance label pairs and m instances. T : Θ ⇝X then maps each\njoint distribution θ to a product of n copies of itself and m copies of its marginal distribution over\ninstances.\nExample 4 (Active Learning) Θ could be anything with X = Zn, length n sequences in some set\nZ. Each active learning policy determines a different T : Θ ⇝X.\n3. Feature Learning\nStarting from a learning problem (Θ, X, T, A, L), feature learning methods aim to extract features\nφ : X ⇝Z. One then bases all decisions on these features.\nΘ\nT\n/ X\nφ\n/ Z\nd\n/ A\nThese methods swap the original learning problem with (Θ, Z, φ ◦T, A, L). Normally the space\nZ is smaller/ of lower dimension than X and aims at presenting a “compressed” view of the infor-\nmation contained in X. Features can be used for several reasons including communication/ storage\nconstraints, increased performance (ie by implementing decision rules based on Z rather than X\ndirectly), knowledge discovery and to avoid “curse of dimensionality” problems.\n3.1. Supervised Feature Learning\nThere has been much attention in the Machine Learning literature on supervised feature learning\ntechniques, were Θ, T, L and the prior π are ﬁxed. These methods construct features by minimizing\nthe feature gap\n∆VL(π, φ, T) := VL(π, φ ◦T) −VL(π, T).\nThere is now a general framework for solving such problems based largely on variations of the\nBlahut-Arimoto Algorithm from Rate Distortion theory (Banerjee et al., 2005; Tishby et al., 1999;\n5\nVAN ROOYEN WILLIAMSON\nCover and Thomas, 2012). For particular choices of T and L these methods reproduce many cluster-\ning methods such as k-means. We review these methods in section 4 of the appendix. These feature\nlearning methods are not general enough for our purposes as they rely on both the experiment and\nloss. For example if we wish to learn T from data given by (θ, x) pairs, then we are required to\nlearn the features after we have learnt the experiment T. Ideally we would like to learn a feature\nmap φ : X ⇝Z independently from T so that learning φ ◦T is just as beneﬁcial as learning T no\nmatter what T is.\n3.2. Generic Feature Learning\nFor many learning problems, a large amount of unlabelled/loosely labelled data X is readily avail-\nable. For example, with any problem involving images one only has to enter some basic search\nqueries into google to be presented with millions of instances. One of the main arguments of the\ndeep learning community is that while this data may not be of direct use in learning classiﬁers, it\ncan be of great use in learning feature representations. There exists many methods in the literature\nto learn features from unlabelled data.\nIn line with these methods we consider the following relaxation of the supervised feature learning\nproblem. We assume that Θ, T, A, L and the prior π are allowed to vary leaving only X ﬁxed,\nwith one restriction. We assume that there is enough unlabelled data collected from the marginal\ndistribution πX on X that we are able to form an accurate estimate of πX. We consider all learning\nproblems and priors π that are consistent with this information about X ie with T(π) = πX. We\nthen seek to ﬁnd features φ so that the value of VL(π, φ ◦T) is as close to VL(π, T) as possible, no\nmatter what what π, Θ, T, A, L are. To ensure that minor differences in value are not exploited by\nmultiplying the loss function by a large constant, we penalize the size of the loss function by ∥L∥∞.\nDeﬁnition 3 (Generic Features) Fix a measure space X and a distribution πX. φ : X ⇝Z are\ngeneric features of quality ϵ for X if for all learning problems (Θ, X, T, A, L) and priors π with\nT(π) = πX we have\n∆VL(π, φ, T) ≤ϵ∥L∥∞.\nIdeally we want to make ϵ as small as possible, and if ϵ is 0 then our features do not ever decrease\nthe value.\nFor our more relaxed problem the value of our features is effectively\nsup\nL:∥L∥∞≤1\nsup\nT\nsup\nπ : T(π)=πX\n∆VL(π, φ, T)\n= sup\nΘ\nsup\nA\nsup\nL∈RΘ×A:∥L∥∞≤1\nsup\nT\nsup\nπ : T(π)=πX\n∆V(Θ, A, X, φ, T, π)\nLuckily supremums like these have been tackled in theoretical statistics particularly in the work\nof Lucien Le Cam (Lecam, 2011, 1964, 1974). In his 1964 paper Le Cam coined the deﬁciency\ndistance as an extension of David Blackwell’s ordering of experiments Blackwell (1953, 1951) and\nas a means to provide an approximate version of the statistical notion of sufﬁciency. This quantity\nwas used in his later work to form a metric not just on probability distributions but on experiments,\nand in particular allows one to calculate supremums over all loss functions and priors with ﬁxed\nΘ, T. We introduce this quantity (the deﬁciency) in the next section.\n6\nLE CAM MEETS LECUN\n4. Approximate Factoring Through and Deﬁciency\nSuppose T : Θ ⇝X and U : Θ ⇝Y are Markov kernels where U does not factor through T.\nWe measure the degree to which U fails to factor through T by the weighted directed deﬁciency\n(Torgersen, 1991).\nδπ(T, U) :=\ninf\nV :X⇝Y Eθ∼π∥U(θ) −V ◦T(θ)∥.\n∥P −Q∥is the variational divergence between the distributions P, Q ∈P(X), a standard metric\non probability distributions (see section 5 of the appendix for properties). Calculating weighted\ndirected deﬁciencies is a convex (actually linear) optimization problem. One has\nf(π, V ) = Eθ∼π∥U(θ) −V ◦T(θ)∥\n= Eθ∼π∥U(θ) −V ◦T(θ)∥1\n= ∥U ⊗π −(V ◦T) ⊗π∥1\n= ∥U diag(π) −V T diag(π)∥1\nis linear in π. Since the variational divergence ||P −Q|| is convex in Q, f is also convex in\nV , because ∥U diag(π) −V T diag(π)∥1 is the composition of a linear function and a convex\nfunction. Hence determining weighted directed deﬁciencies is a l1 minimization problem. Fast\nmethods exist for solving this problem (eg the well known simplex method of linear programming).\nTaking a supremum over the prior π yields the directed deﬁciency,\nδ(T, U) := sup\nπ δπ(T, U) =\ninf\nV :X⇝Y sup\nθ\n∥U(θ) −V ◦T(θ)∥.\nwhere the second follows from the minimax theorem (Komiya, 1988). For the sake of checking\nwhether T|U it sufﬁces to use the weighted directed deﬁciency and a prior that does not put zero\nprobability on any θ. In this case δπ(T, U) = 0 if and only if T|U (Torgersen, 1991). The weighted\ndeﬁciency, and deﬁciency are respectively\n∆π(T, U) := max(δπ(T, U), δπ(U, T))\n∆(T, U) := sup\nπ ∆π(U, T).\n∆(T, U) = 0 if and only if T|U and U|T, when T is isomorphic to U written T ∼= U. The\ndeﬁciency distance is a true metric on the space of experiments (modulo isomorphic experiments).\nA proof of this is included in the appendix.\n4.1. Relation to Risk\nFactoring through and approximate factoring through are deeply related to the worst case difference\nin performance between two learning problems with the same Θ as the loss is varied. Here we state\nthe three theorems that highlight the connections between factoring through and risk (Torgersen,\n1991). Fix Θ and two experiments T : Θ ⇝X and U : Θ ⇝Y .\nTheorem (Information Processing) If T|U then for any loss function L and prior π\nVL(π, T) ≤VL(π, U).\nIn particular the information processing theorem implies that ∆VL(π, φ, T) ≥0.\n7\nVAN ROOYEN WILLIAMSON\nTheorem (Blackwell-Sherman-Stein) T|U if and only if VL(π, T) ≤VL(π, U) for all loss func-\ntions L and priors π.\nTheorem (Randomization) Fix ϵ > 0, T, U and π. VL(π, T) ≤VL(π, U) + ϵ∥L∥∞if and only if\nδπ(T, U) ≤ϵ for all loss functions L.\nThese three theorems allow one to move between decision theoretic notions such as risk and value\nto probability theoretic notions such as factoring through. For example the original deﬁnition of\nsufﬁciency can be interpreted in terms of factoring through.\nTheorem Fix an experiment T : Θ ⇝X and a function f : X →Y . f is a sufﬁcient statistic if\nf ◦T ∼= T.\nBy the Blackwell-Sherman-Stein theorem we have an equivalent condition for sufﬁciency in terms\nof value.\nTheorem Fix an experiment T : Θ ⇝X and a function f : X →Y .Then f is a sufﬁcient statistic\nif VL(π, f ◦T) = VL(π, f ◦T) for all L and π.\nIsomorphic experiments always have the same value, no matter what the loss function or the set\nof actions. Approximately isomorphic experiments, ones where ∆(T, U) is small, always have\napproximately the same value. Due to the similarities between learning features and sufﬁciency\nstatistics, it should be of no surprise that tools for working with approximate sufﬁciency appear in\nfeature learning.\nThe Randomization theorem is an example of an approximate notion in probability theory (here\napproximate sufﬁciency) has a dual approximate notion in terms of risk.\nTheorem 1 For all experiments U, T and all priors π\n∆π(U, T) = sup\nL\n|VL(π, U) −VL(π, T)|\n∥L∥∞\nThis result is an improvement and generalization of the result contained in Liese (2012) that applied\nonly to binary experiments, Θ ∼= {−1, 1}, and held with inequality. The proof is included in the\nappendix. We utilize this theorem and the randomisation theorem heavily in the following section.\n4.2. Reductions via Factoring Through\nApproximate factoring through can be used to transform decision rules for one learning problem to\nrules for another, with a provable bound on the performance of this decision rule. Suppose DU ∈\nM(Θ, A)U is a decision rule used for the learning problem (Θ, Y, U, A, L). For another learning\nproblem (Θ, X, T, A, L), we can construct a decision rule from a Markov kernel V : X ⇝Y\nΘ\nT\n~\nU\n\u001f\nX\nV\n/ Y\nDU/U\n/ A\nDT = (DU/U) ◦V ◦T. Furthermore if ϵ = Eθ∼π∥U(θ) −V ◦T(θ)∥then\nRL(π, DT ) ≤RL(π, DU) + ϵ∥L∥∞.\nBy taking inﬁmums over V we obtain the smallest ϵ in the above.\n8\nLE CAM MEETS LECUN\n5. Analysis of Generic Feature Learning via Deﬁciency\nAssume one has enough data in some measure space X to form a good estimate of the marginal\ndistribution πX. We wish to construct generic features φ : X ⇝Z for X. This is equivalent to\nﬁnding a φ that minimizes supT ∆π(T, φ ◦T). One might imagine that this means ﬁnding for each\nφ the worst T, but this is not the case.\nTheorem 2 For all experiments T : Θ ⇝X, for all measure spaces Z and for all feature maps\nφ : X ⇝Z\n∆π(T, φ ◦T) ≤∆T(π)(idX, φ).\nNo matter which feature map φ we use, the worst learning problem we can pit against it is the\none that asks you to reconstruct X directly from the features. The proof is straightforward and is\nincluded in the appendix. It hinges on the representation of VL(π, T) in terms of average posterior\nBayes risk and the Randomization theorem. By deﬁnition and as idX |φ\n∆πX(idX, φ) = δπX(φ, idX)\n=\ninf\nd:Z⇝X Ex∼πX∥idX(x) −(d ◦φ)(x)∥\n=\ninf\nd:Z⇝X Ex∼πX∥δx −(d ◦φ)(x)∥\n(1)\n= 2\ninf\nd:Z⇝X Ex∼πXEx′∼(d◦φ)(x)1(x′ = x),\n(2)\nwhere from lines (1) to (2) we have used one of the equivalent forms of the variational divergence\nlisted in the appendix. Hence ∆πX(idX, φ) is equal to twice the minimal possible average recon-\nstruction error from the encoder φ and the prior πX. This means that ﬁnding the best generic features\nfor the data X involves ﬁnding the φ that gives the learning problem (X, Z, φ, X, L01) the highest\nvalue VL01(πX, φ). We term this problem the reconstruction problem.\nTheorem 3 Fix a prior πX and an ϵ > 0. φ constitutes generic features of quality ϵ for X if and\nonly if for the reconstruction problem (X, Z, φ, X, L01), one can ﬁnd a decision rule d : Z ⇝X\nwith\nRL01(πX, d ◦φ) ≤ϵ\n2.\nFor a proof see appendix. If we optimize over both the encoder φ and the decoder d, ﬁnding\ninf\nφ,d Ex∼πX∥idX(x) −(d ◦φ)(x)∥\nwe obtain a variant of the popular autoencoder algorithm from deep learning (Vincent et al., 2008).\nOf course one can always take φ = idX in which case no real feature learning is done and no\nperformance is lost. However, it is more instructive to set Z to a measure space of smaller size/\ndimension than X so that the feature learning extracts (provably) useful patterns in πX. Performing\nthe joint minimization is a non-convex problem.\n9\nVAN ROOYEN WILLIAMSON\n5.1. Relation to other Feature Learning Methods\nInfomax. When faced with a choice of feature maps φi : X ⇝Zi, the Infomax principle (Bell\nand Sejnowski, 1995; Linsker, 1989) dictates that you should choose the features that minimize the\nconditional entropy between data and features, H(X|Zi). By the Hellman-Raviv inequality from\ninformation theory (Hellman and Raviv, 1970) we have\n∆πX(idX, φi) ≤H(X|Zi)\nmeaning the Infomax principle is minimizing an upper bound of the reconstruction error.\nManifold Learning. Under the assumption that πX has support on some manifold M ⊆X man-\nifold learning methods aim to extract this manifold and provide a parametrization φ : M →Rn\n(Silva and Tenenbaum, 2002; Belkin and Niyogi, 2003). If we are able to learn this manifold then\nany coordinate system would constitute generic features.\nSparse Coding. Much like manifold learning, sparse coding also attempts to ﬁnd lower dimensional\nstructure in πX (Lee et al., 2006; Olshausen and Field, 1997). Here Z is chosen to have higher\ndimension than X, however image of the feature map φ : X ⇝Z should comprise only of sparse\nvectors, those with few non-zero entries. If φ is injective on the support of X then φ are generic\nfeatures.\n5.2. Learning Feature Hierarchies\nOne of the tenets of the deep learning paradigm is that features should be learnt in a hierarchical\nfashion. One should ﬁrst ﬁnd patterns in πX through a feature map φ and then ﬁnd patterns in\nφ(πX) and so on. We construct a chain of feature maps\nX\nφ1\n/ Z1\nφ2\n/ Z2\nφ3\n/ . . .\nwith ﬁnal feature space Zn and ﬁnal feature map given by the composition of all maps in the chain.\nProceeding in this fashion allows greater control over the feature spaces Zi. For example the ﬁrst\ncould be of similar (but still lower) size than X, perhaps before a big drop off in the middle of the\nchain. If we can learn each of the φi iteratively it also makes searching for features easier. For\nexample, perhaps the ﬁrst three feature maps have low probability of reconstruction error but not\nthe fourth. In this situation at least we still have a good feature map given by the composition of the\nﬁrst three mappings. We have for any chain of feature maps\n∆πX(idX, φn ◦· · · ◦φ2 ◦φ1)\n≤∆πX(idX, φ1) + ∆πX(φ1, φ2 ◦φ1) + · · · + ∆πX(φn−1 ◦· · · ◦φ1, φn ◦· · · ◦φ1)\n(1)\n≤∆πX(idX, φ1) + ∆φ1(πX)(idZ1, φ2) + · · · + ∆φn−1◦···◦φ2◦φ1(πX)(idZn−1, φn)\n(2)\nwhere (1) follows as the weighted deﬁciency satisﬁes a triangle inequality and (2) follows from\nrepeated application of theorem 2. The reconstruction error of the entire system is bounded by the\nsum of reconstruction errors at each step of the chain. This means we can learn a feature mapping\niteratively, by ﬁrst learning patterns in πX, then in φ(πX) and so on. This is exactly the process that\noccurs in a Deep Belief Network (Hinton and Salakhutdinov, 2006)\n10\nLE CAM MEETS LECUN\n5.3. Supervised Feature learning can work when Generic Feature Learning Fails\nWe present two examples where one can not learn generic features, however we can learn experi-\nment/loss speciﬁc features.\nExperiment Speciﬁc Features. Let Θ = R with X = Rn and T(θ) given by the product of n\nnormal distributions with mean θ and variance 1. It is easy to verify that the sample mean φ :\nX →R is a sufﬁcient statistic meaning that at least for this experiment we can greatly compress the\ninformation contained in X. However, if we take as a prior π for Θ a normal distribution of mean\n0 and variance 1, then the marginal distribution πX will not be concentrated on a set of smaller\ndimension nor have any particularly interesting structure. Hence we can not ﬁnd interesting generic\nfeatures in this case.\nExperiment and Loss Speciﬁc Features. Let Θ = {−1, 1} with T(θ) a normal distribution centred\non θ as in the ﬁgure below.\n-3\n0\n3\n-3\n0\n3\nPH-1 xL\nTH1L\nTH-1L\nFigure 1: Figures for Loss Speciﬁc Feature Learning, see text\nFor this experiment ,L01 and a uniform prior π, the best decision d(x) = 1 if x > 0 as P(−1|x) > 1\n2\nand d(x) = −1 otherwise as P(−1|x) ≤1\n2 . It is easy to show that ∆VL01(π, d, T) = 0, all we need\nis the output of d. However if we change the loss to a cost sensitive loss Lc where say misclassifying\na 1 is more costly than a −1, we no longer have ∆VLc(π, d, T) = 0.\n5.4. Alternate Reconstruction Problems\nThe reconstruction problem (X, Z, φ, X, L01) that is required to be solved to construct generic\nfeatures is the most difﬁcult one we can pose. To perform well in this problem we are required to\nreconstruct each x ∈X exactly. This discards other interesting structure the set X may have. For\nexample if X is image data a different loss function L : X × X →R, perhaps one elicited from\npsychological tests of what humans perceive to be different images is more appropriate. While\nthese are valid points, we remind the reader that this is a ﬁrst step in understanding these methods,\nand making any extra assumptions about X and its structure is exactly what we are trying to avoid.\nHowever, the Hellman Raviv inequality does give means of bounding the value VL01(π, φ) with the\nvalue of different reconstruction problems.\n11\nVAN ROOYEN WILLIAMSON\n6. Concluding Remarks and Future Work\nWe have deﬁned generic features and have provided a characterization of when it is possible to\nlearn them. In doing so we have illuminated some popular feature learning methods including\nautoencoders, deep belief networks and the Infomax principle.. We have moved from supervised\nfeature learning methods\ninf\nφ:X⇝Z ∆VL(π, φ, T)\nwhere Θ, A, L, π are all ﬁxed to\ninf\nφ:X⇝Z\nsup\nL:∥L∥∞≤1\nsup\nT\nsup\nπ : T(π)=πX\n∆VL(π, φ, T)\n(⋆)\nwith almost nothing ﬁxed. Equation (⋆) shows how difﬁcult and general ﬁnding generic features is.\nIt is reasonable to argue that in practice on does not require features that work for all experiments, all\nlosses and all priors, which by existing results implies a quantiﬁcation over all f-informations. This\nbegs the question of which experiments, loss functions and priors to consider. We might not require\nall the information in X to be maintained, just enough to suit our purposes. This is analogous to\nthe the problem of formalizing the notion of how much information is contained in an experiment.\nAs argued long ago by Morris Degroot (DeGroot, 1962), even if one is doing an experiment to\n“gain information”, eventually one does something with this “information” by choosing how to act,\nthe consequence of which will be measured by some loss. Hence a more general theory of feature\nlearning needs to be able to control the sensitivity to the loss, allowing one to move from supervised\nfeature learning to generic feature learning.\nA starting point would be to take ﬁxed L and T lying in some subset of M(Θ, X) as occurs in robust\nstatistics (Huber, 2011), or to allowing small perturbations in the loss. Deﬁciency can possibly play\na role in the development of algorithms to learn features when we take these restricted supremums.\nThere is scope to develop new quantities and theorems analogous to those for deﬁciency where\ninstead of a supremum over all losses, one takes a supremum over some restricted subset. At present\nthis is an open and uncharted area of both machine learning and theoretical statistics.\n12\nLE CAM MEETS LECUN\n7.\n7.1. More Properties of Factoring Through\nLemma Factoring through has the following properties. For all sets W, X, Y, Z and Markov kernels\nT1 : X ⇝Y and T2 : Y ⇝W we have\n1. M(X, Z)T1 ⊇M(X, Z)T2◦T1\n2. M(X, Z) = M(X, Z)idX ⊇M(X, Z)T1\n3. M(X, Z)T1 ⊇M(X, Z)•X\nProof For (1) if D ∈M(X, Z)T2◦T1 then we have\nD = (D/(T2 ◦T1)) ◦(T2 ◦T1) = ((D/(T2 ◦T1)) ◦T2) ◦T1\nwhich is obviously in M(X, Z)T1. For (2) note that for any Markov kernel D : X ⇝Y one has\nD ◦idX = D. Hence all kernels factor through the identity. For (3) take any D ∈M(X, Z)•X and\nrecall that •X = •Y ◦T1. Hence\nD = (D/•X) ◦•X = (D/•X) ◦•Y ◦T1\nwhich is obviously in M(X, Z)T1\nNote that M(X, Z)•X comprises the constant Markov kernels, ones that map each x ∈X to the\nsame distribution on Z.\nOne can view factoring through as adding noise. By showing that T|U, we are showing that U is T\ncomposed with extra noise U/T\nFor some intuition on what factoring through looks like below is a plot of four binary experiments\n(Θ = −1, 1).\n-5\n0\n5\n-5\n0\n5\n-5\n0\n5\n-5\n0\n5\n2\n4\n1\n3\n13\nVAN ROOYEN WILLIAMSON\nWe have that:\n• The second factors through the ﬁrst.\n• The third does not factor through the ﬁrst (nor the ﬁrst through the third).\n• The fourth factors through the ﬁrst and vice versa (it is just a shifted version of the ﬁrst).\nSuppose T : Θ ⇝X and U : Θ ⇝Y are two Markov kernels. If T|U and U|T then we say\nthat T is isomorphic to U written T ∼= U. Isomorphic Markov kernels can appear quite different.\nFor example suppose that T : Θ ⇝X is an exponential family distribution (Θ in this case are the\nparameters for the family). If φ : X →Rn is the sufﬁcient statistics for the family T then T ∼= φ◦T\neven though they appear quite different, and that it appears φ may throw away lots of information.\nFrom a statistical point of view isomorphic Markov kernels are the same.\n7.2. f-Information and Value\nTheorem For all experiments T, loss functions L and priors π, the gap between then the value of\nT and the least informative experiment •Θ is a f-information for suitable f\nVL(π, •Θ) −VL(π, T) = If(T(θ1), . . . , T(θn)) = If(T)\nWe repeat the proof presented in Garcia-Garcia and Williamson (2012), which is a variant of one\npresented in DeGroot (1962).\nProof\nVL(π, T) = Ex∼T(π)L(T ∗(x))\nand\nVL(π, •Θ) = L(π)\nwhere L : P(Θ) →R is concave. Let |Θ| = n, πi = π(θi), Pi = T(θi) and M =\nnP\ni=1\nπiPi. Then\nT ∗(x) = (π1 dP1\ndM (x), π2 dP2\ndM (x), . . . , πn dPn\ndM (x)) and\nVL(π, T) =\nZ\nX\nL(π1\ndP1\ndM , π2\ndP2\ndM , . . . , πn\ndPn\ndM )dM\n=\nZ\nX\nL(dP1\ndM (π1, π2\ndP2\ndP1\n, . . . , πn\ndPn\ndP1\n))dM\ndP1\ndP1\n=\nZ\nX\nL(\n1\nπ1 +\nnP\ni=2\nπi dPi\ndP1\n(π1, π2\ndP2\ndP1\n, . . . , πn\ndPn\ndP1\n))(π1 +\nn\nX\ni=2\nπi\ndPi\ndP1\n)\n|\n{z\n}\nφ( dP2\ndP1 ,..., dPn\ndP1 )\ndP1\nWe need to show that φ : Rn−1\n+\n→R is concave. Note that φ = g ◦h where\nh : Rn−1\n+\n→Rn\n+\nv 7→diag(π)(1, v)\n14\nLE CAM MEETS LECUN\nprepends 1 to v and and multiplies by the prior, and\ng : Rn\n+ →R\nv 7→∥v∥1L(\nv\n∥v∥1\n).\nAs h is afﬁne, φ is concave if g is. For all λ ∈[0, 1] and v1, v2 ∈Rn\n+\ng(λv1 + (1 −λ)v2) = ∥λv1 + (1 −λ)v2∥1L(\nλv1 + (1 −λ)v2\n∥λv1 + (1 −λ)v2∥1\n)\n= ∥λv1 + (1 −λ)v2∥1L(\nλ∥v1∥1\n∥λv1 + (1 −λ)v2∥1\nv1\n∥v1∥1\n+\n(1 −λ)∥v2∥1\n∥λv1 + (1 −λ)v2∥1\nv2\n∥v2∥1\n)\n≤λ∥v1∥1L(\nv1\n∥v1∥1\n) + (1 −λ)∥v2∥1L(\nv2\n∥v2∥1\n)\n= λg(v1) + (1 −λ)g(v2)\nwhere we have used the concavity of L. Therefore φ is concave. Finally\nVL(π, •Θ) −VL(π, T) =\nZ\nX\nL(π) −φ(dP2\ndP1\n, . . . , dPn\ndP1\n)dP1 = If(dP2\ndP1\n, . . . , dPn\ndP1\n)\nwith f = L(π) −φ convex. This completes the proof.\nThe converse is also true. The proof follows by similar manipulations and an appeal to the properties\nof proper losses (see Reid and Williamson (2011); Garcia-Garcia and Williamson (2012)), and is\nnot inlcuded.\n7.3. Proper Loss Functions\nHere we review material relating to the construction of proper loss functions (Reid and Williamson,\n2011; Dawid, 2007; Parry et al., 2012; Gr¨unwald and Dawid, 2004).\nDeﬁnition A loss function L : Θ × P(Θ) →R is proper if for all P ∈P(θ)\nP ∈arg inf\nQ∈P(Θ)\nEθ∼P L(θ, Q)\nAny loss function can be properized.\nTheorem Let L : Θ × A →R be a loss. Deﬁne\nf : P(Θ) →A\nP 7→arg inf\na\nEθ∼P L(θ, a)\nwhere we arbitrarily pick an a ∈arg infa Eθ∼P L(θ, a) if there are multiple. Then ˆL(θ, P) =\nL(θ, f(P)) is proper.\n15\nVAN ROOYEN WILLIAMSON\nIt is possible that by using this trick we remove actions a ∈A. However, for the purpose of\ncalculating Bayes risks we do not require these actions. From ˆL, one can deﬁne a regret\nD(P, Q) = Eθ∼P ˆL(θ, Q) −Eθ∼P ˆL(θ, P)\nwhich measures how suboptimal the best action is to play against the distribution Q is when played\nagainst the distribution P. One does not need knowledge of the function f to construct ˆL, rather\none only needs knowledge of the Bayes risk\nL(P) = inf\na Eθ∼P L(θ, a).\nFrom this one can reconstruct ˆL, and hence L for the purposes of calculating Bayes risks. This is\nachieved by taking the 1-homogenous extension of L\n˜L : R|Θ|\n+ →R\nv 7→∥v∥1L(\nv\n∥v∥1\n)\nand differentiating.\nTheorem For a concave L : P(Θ) →R\nL(θ, P) = ⟨δθ, ∇˜L(P)⟩\nis a proper loss.\nThe regret from a proper loss is equal to the Bregman divergence deﬁned by ˜L(P) and L,\nTheorem For all concave L : P(Θ) →R and P, Q ∈P(Θ)\nD(P, Q) = DL(P, Q) = D˜L(P, Q) = ˜L(Q) + ⟨∇˜L(Q), P −Q⟩−˜L(P).\nAll of these properties show that we only need knowledge of L to compute Bayes risks and values.\n7.4. Supervised Feature Learning\nFor a given experiment T and loss function L, supervised feature learning methods aim to minimize\nthe feature gap\n∆VL(π, φ, T)\nThe following two lemmas (Banerjee et al., 2005; Tishby et al., 1999) provide means to do this.\nLemma 1 The feature gap satisﬁes\n∆VL(π, φ, T) = Ex∼πxEz∼φ(x)DL(T ∗(x), (φ ◦T)∗(z))\n= Ex∼πxEz∼φ(x)DL(P(Θ|x), P(Θ|z))\nwhere DL is the regret induced by L.\n16\nLE CAM MEETS LECUN\nProof\nFor the proof we use the more familiar probability theory notation with T ∗(x) = P(Θ|x), (φ◦T)∗=\nP(Θ|z), πX = P(X) and so on. One has\nDL(P, Q) = L(Q) + ⟨∇L(Q), P −Q⟩−L(P), P, Q ∈P(Θ).\nand\nVL(π, T) = Ex∼P(X)L(P(Θ|x))\nVL(π, φ ◦T) = Ex∼P(Z)L(P(Θ|z))\ngiving\nEx∼P(X)Ez∼P(Z|x)DL(P(Θ|x), P(Θ|z))\n=Ex∼P(X)Ez∼P(Z|x)L(P(Θ|z)) + ⟨∇L(P(Θ|z)), P(Θ|x) −P(Θ|z)⟩−L(P(Θ|x))\n=Ez∼P(Z)Ex∼P(X|z)L(P(Θ|z)) + ⟨∇L(P(Θ|z)), P(Θ|x) −P(Θ|z)⟩−L(P(Θ|x))\n=VL(π, φ ◦T) −VL(π, T) + Ez∼P(Z)Ex∼P(X|z)⟨∇L(P(Θ|z)), P(Θ|x) −P(Θ|z)⟩\nNote that ⟨∇L(P(Θ|z)), P(Θ|x) −P(Θ|z)⟩is afﬁne in P(Θ|x) and that\nEx∼P(X|z)P(Θ|x) = P(Θ|z)\nmeaning\nEz∼P(Z)Ex∼P(X|z)⟨∇L(P(Θ|z)), P(Θ|x) −P(Θ|z)⟩= 0\nthis completes the proof.\nLemma 2 For πZ ∈P(Z), ˆU ∈M(Z, Θ)\ninf\nφ Ex∼πXEz∼φ(x)DL(T ∗(x), (φ ◦T)∗(z)) + βI(X, Z)\n= inf\nφ inf\nˆπZ inf\nˆU\nEx∼πXEz∼φ(x)DL(T ∗(x), ˆU(z)) + Ex∼πXDKL(φ(x)||ˆπZ)\nThis lemma is proved by the following theorem from Banerjee et al. (2005).\nTheorem 4 For any concave L : X →R and distribution P ∈P(X)\n¯x ∈arg inf\nx\nEy∼xDL(y, x).\nWe can know prove the lemma\nProof Once again we use the more standard notation from probability theory. Firstly\nI(X; Y ) = Ex∼P(X)DKL(P(Z|x), P(Z)) = inf\nˆP(Z)\nEx∼P(X)DKL(P(Z|x), ˆP(Z)).\n17\nVAN ROOYEN WILLIAMSON\nsince Ex∼P(X)P(Z|x) = P(Z). Secondly\nEx∼P(X)Ez∼P(Z|x)DL(P(Θ|x), P(Θ|z)) = EZ∼P(Z)Ex∼P(X|z)DL(P(Θ|x), P(Θ|z))\n= EZ∼P(Z)\ninf\nˆP(Θ|z)\nEx∼P(X|z)DL(P(Θ|x), ˆP(Θ|z))\n=\ninf\nˆP(Θ|Z)\nEZ∼P(Z)Ex∼P(X|z)DL(P(Θ|x), ˆP(Θ|z))\nsince Ex∼P(X|z)P(Θ|x) = P(Z|Θ). Combining gives\ninf\nP(Z|X) Ex∼P(X)Ez∼P(Z|x)DL(P(Θ|x), P(Θ|z)) + βEx∼P(X)DKL(P(Z|x), P(Z))\n=\ninf\nP(Z|X)\ninf\nˆP(Θ|Z)\ninf\nˆP(Z)\nEx∼P(X)Ez∼P(Z|x)DL(P(Θ|x), ˆP(Θ|z)) + βEx∼P(X)DKL(P(Z|x), ˆP(Z)).\nThis completes the proof.\nThese two lemma’s yield a family of alternating minimization algorithms that attempt to solve the\nsupervised feature learning problem. The term βI(X; Z) can be interpreted as a regularizer that\nfavours φ that throw away information about X.\n7.5. Variational Divergence\nLet P and Q be distributions on a measure space X. The variational divergence has the following\nequivalent forms\n1. ∥P −Q∥= supφ:X→[−1,1] EP φ −EQφ\n2. ∥P −Q∥= infC such that EP φ −EQφ ≤C∥φ∥∞for all bounded φ.\n3. ∥P −Q∥=\nR\nX |P −Q| the l1 distance between the probability distributions P and Q.\n4. ∥P −Q∥= Df(P, Q) =\nR\nX f(dQ\ndP )dP the f-divergence from P to Q for f(x) = |x −1|\n5. ∥P −Q∥= 2 supA∈Σ(X) P(A) −Q(A), A ⊆X\nSince ∥∥is a f-divergence, it also satisﬁes an information processing theorem (Reid and Williamson,\n2011). For any Markov kernel T : X ⇝Y ,\n∥P −Q∥≥∥T(P) −T(Q)∥\nFinally if π ∈P(X) and T, U : X ⇝Y then\n∥π ⊗T −π ⊗U∥= Eθ∼π∥T(θ) −U(θ)∥\n18\nLE CAM MEETS LECUN\n7.6. Proof that Weighted Deﬁciency Satisﬁes the Triangle Inequality\nWe wish to show that\n∆π(T, U) = max(δπ(T, U), δπ(U, T))\nprovides a metric on experiments on Θ (modulo isomorphism), for priors π that do not assign zero\nmass to some θ. It should be fairly obvious that ∆π is both symmetric and non negative. All that is\nleft to prove is the triangle inequality. Fix Markov kernels Ti : Θ ⇝Xi as well as Markov kernels\nV1 and V2 as in the diagram below\nΘ\nT1\nw\nT2\n\u000f\nT3\n'\nX1\nV1\n/ X2\nV2\n/ X3\nWe have ∀θ ∈Θ\n∥T3(θ) −V2 ◦V1 ◦T1(θ)∥≤∥T3(θ) −V2 ◦T2(θ)∥+ ∥V2 ◦T2(θ) −V2 ◦V1 ◦T1(θ)∥\n≤∥T3(θ) −V2 ◦T2(θ)∥+ ∥T2(θ) −V1 ◦T1(θ)∥\nwhere we have used the fact the variational divergence is a metric and that it satisﬁes an information\nprocessing theorem. Averaging with respect to the prior and taking inﬁmums over V1 and V2 yields\nδπ(T1||T3) ≤δπ(T1||T2) + δπ(T2||T3).\nGoing in the opposite direction and taking maximums yields\n∆π(T1, T3) ≤∆π(T1, T2) + ∆π(T2, T3)\nthe desired result. Even if π does give zero mass to some θ, meaning ∆π may not be a metric, the\ntriangle inequality still applies.\n7.7. Proof of the Information Processing Theorem\nTheorem (Blackwell-Sherman-Stein) T|U if and only if VL(π, T) ≤VL(π, U) for all loss func-\ntions L and priors π.\nProof\nBy deﬁnition we have\nVL(π, T) =\ninf\nD∈M(Θ,A)T\nRL(π, D)\n=\ninf\nD∈M(Θ,A)T\nf(D)\nWhere f : M(Θ, A) →R. By property (1) of factoring through we have M(Θ, A)T ⊇M(Θ, A)U\ngiving\nVL(π, T) =\ninf\nD∈M(Θ,A)T\nf(D)\n≤\ninf\nD∈M(Θ,A)U\nf(D)\n= VL(π, U)\n19\nVAN ROOYEN WILLIAMSON\n7.8. Proof of the Blackwell-Sherman-Stein theorem\nTheorem (Blackwell-Sherman-Stein) T|U if and only if VL(π, T) ≤VL(π, U) for all loss func-\ntions L and priors π.\nHere we prove the converse to the information processing theorem.\nProof\nThe condition that VL(π, T) ≤VL(π, U) for all loss functions and priors is equivalent to\n∀L, ∀DU ∈M(Θ, A)U, ∃DT ∈M(Θ, A)T such that RL(δθ, DY ) ≤RL(δθ, dY ), ∀θ\nin words, for any loss and any decision rule based on U there is one based on T that is better.\nTo see this note for each θ0 ∈Θ we can take loss functions that only care about that particular θ0, ie\nL(θ, a) = 0 if θ ̸= θ0. Now ﬁx A. We have that RL(δθ, −) : M(Θ, A) →R is linear. Furthermore\nM(Θ, A)PT and M(Θ, A)U are closed convex subsets of M(Θ, A). As we vary L and θ, RL(δθ, −)\ngives the entire dual space of M(Θ, A), ie by taking L(θ0, a0) = 1 and zero for all other θ and a. By\nthe correspondence between risk functions and the dual of M(Θ, A) means the above is equivalent\nto\ninf\nD∈M(Θ,A)T\n⟨α, D⟩≤\ninf\nD∈M(Θ,A)U\n⟨α, D⟩, ∀α ∈M(Θ, A)∗\nmeaning that the support function of M(Θ, A)T is less than or equal to the support function of\nM(Θ, A)T . Hence\nM(Θ, A)U ⊇M(Θ, A)T\nNote A was arbitrary. Now let A = Y , and note that U ∈M(Θ, Y )U. Hence by the above inclusion,\nthere exists a U/T : X →Y with U = (U/T) ◦T\n7.9. Proof of the Randomization theorem\nTheorem (Randomization) Fix ϵ > 0, T, U and π. VL(π, T) ≤VL(π, U) + ϵ∥L∥∞if and only if\nδπ(T, U) ≤ϵ for all loss functions L.\nFirstly the if direction.\nProof (Forward implication)\nIf δ(T||U) ≤ϵ then there exists a Markov kernel V such that\nEθ∼π∥U(θ) −V ◦T(θ)∥≤ϵ\n20\nLE CAM MEETS LECUN\nFix a decision rule DU ∈M(Θ, A)U. Using V gives a decision rule DT ∈M(Θ, A)T by compo-\nsition, DT = (DU/U) ◦V ◦T. See the diagram below for a better explanation.\nΘ\nT\n~\nU\n\u001f\nX\nV\n/ Y\nDU/U\n/ A\nBy the properties of the variational divergence one has\nRL(π, DT ) −RL(π, DU) = Eθ∼πEa∼DT (θ)L(θ, a) −Eθ∼πEa∼DU(θ)L(θ, a)\n≤Eθ∼π∥DU(θ) −DT (θ)∥∥L∥∞\n= Eθ∼π∥(DU/U) ◦U(θ) −((DU/U) ◦V ◦T)(θ)∥∥L∥∞\n≤Eθ∼π∥U(θ) −(V ◦T)(θ)∥∥L∥∞\n≤ϵ∥L∥∞\nwhere the ﬁrst line is the deﬁnition of risk, the second follows from one deﬁnition of the variational\ndivergence and the third follows from the fact the variational divergence is itself an f-divergence\nand as such satisﬁes an information processing theorem. Note this holds for all DU ∈M(Θ, A)U.\nTaking inﬁmums over DT and DU, one has VL(π, T) ≤VL(π, U) + ϵ∥L∥∞for all loss functions.\nWe now prove the converse.\nProof (Converse)\nFix A, a decision rule DU ∈M(Θ, A)U and a prior π and deﬁne a function\nf(L, DT ) = RL(π, DT ) −RL(π, DU) −ϵ∥L∥∞\nthat takes a decision rule DT ∈M(Θ, A)T and a loss L and returns the difference in Bayes risks of\nDT and DU subtracted by a term that penalizes losses with high magnitude. Note that f is a linear\nin DT and concave in L. By the conditions in the theorem one has\nsup\nL\ninf\nDT f(L, DT ) ≤0.\nBy the minimax theorem (Komiya, 1988) , there exists a saddle point (L∗, D∗\nT ) with\nf(L∗, D∗\nT ) = sup\nL\ninf\nDT f(L, DT ) = inf\nDT sup\nL\nf(L, DT ).\nHence f(L, D∗\nT ) ≤0, ∀L, meaning\nRL(π, D∗\nT ) −RL(π, DU) ≤ϵ∥L∥∞∀L.\nThis implies that\nEθ∼πEa∼D∗\nT (θ)L(θ, a) −Eθ∼πEa∼DU(θ)L(θ, a) ≤ϵ∥L∥∞∀L\n21\nVAN ROOYEN WILLIAMSON\nmeaning\n∥D∗\nT ⊗π −DU ⊗π∥= Eθ∼π∥DT (θ) −DU(θ)∥≤ϵ\nAs DU was arbitrary, we have for all DU ∈M(Θ, A)U there exists a DT ∈M(Θ, A)T with\nEθ∼π∥DT (θ) −DU(θ)∥≤ϵ.\nOnce again, take Y = A and note that U ∈M(Θ, Y )U.\n7.10. Proof of Theorem 1\nTheorem For all experiments U, T and all priors π\n∆π(U, T) = sup\nL\n|VL(π, U) −VL(π, T)|\n∥L∥∞\nFor the proof we require the following very simple lemma.\nLemma 3 For x, y ∈R if ∀ϵ ∈R we have x ≤ϵ ⇔y ≤ϵ then x = y.\nProof Suppose that x ̸= y and without loss of generality assume that x ≤y. Set ϵ = x+y\n2 . Then\nx ≤ϵ and y ≥ϵ, a contradiction.\nNow the theorem.\nProof If ∆π(U, T) ≤ϵ then δπ(U, T) ≤ϵ and δπ(T, U) ≤ϵ. By the randomization theorem\n|VL(π, U) −VL(π, T)| ≤ϵ∥L∥∞, ∀L\nhence\nsup\nL\n|VL(π, U) −VL(π, T)|\n∥L∥∞\n≤ϵ.\nConversely if supL\n|VL(π,U)−VL(π,T)|\n∥L∥∞\n≤ϵ then for all L\nVL(π, T) ≤VL(π, U) + ϵ∥L∥∞\nand\nVL(π, U) ≤VL(π, T) + ϵ∥L∥∞\nwhich by the randomization theorem gives ∆π(U, T) ≤ϵ. Combining these two facts and the\nlemma completes the proof.\n22\nLE CAM MEETS LECUN\n7.11. Proof of Theorem 2\nTheorem For all experiments T : Θ ⇝X, for all measure spaces Z and for all feature maps\nφ : X ⇝Z\n∆π(T, φ ◦T) ≤∆T(π)(idX, φ).\nTo prove this theorem we note that from the randomization theorem we have (Torgersen, 1991)\n∆π(T, U) = sup\nL\n|VL(π, T) −VL(π, U)|\n∥L∥∞\n.\nBy standard manipulations from decision theory one has\nVL(π, T) =\ninf\nd:X⇝A Eθ∼πEx∼T(θ)Ea∼d(x)L(θ, a)\n=\ninf\nd:X⇝A Ex∼πXEθ∼T ∗(x)Ea∼d(x)L(θ, a)\n= Ex∼πX inf\na∈A Eθ∼T ∗(x)L(θ, a)\n= Ex∼πXL(T ∗(x))\ngiving\n∆π(T, U) = sup\nL\n|Ex∼T(π)L(T ∗(x)) −Ey∼U(π)L(U∗(x))|\n∥L∥∞\n.\nDue to the correspondence between loss functions and their Bayes risks (Reid and Williamson,\n2011; Dawid, 2007; Gr¨unwald and Dawid, 2004; Garcia-Garcia and Williamson, 2012) (In particu-\nlar one can recover the loss from its Bayes Risk), and the fact that Bayes risks L : P(Θ) →R are\n“attached directly to Θ”, in feature learning we may wish to move them to objects that are attached\nto X. We are inspired by the diagram below.\nZ\nΘ\nT\n/\nφ◦T\n*\nT\n4\nX\nφ\n>\nidX\n \nX\nThis is achieved by the following lemma.\nLemma 4 Let L(Θ) denote the set of all Bayes risks on Θ (proper concave functions). Fix a Markov\nkernel T : Θ ⇝X and a prior π on Θ. Then the Markov kernel T ∗: Θ ⇝Θ provided by Bayes\nrule gives a function\nL[T] : L(Θ) →L(X)\nL 7→L ◦T ∗\n23\nVAN ROOYEN WILLIAMSON\nfurthermore for a chain of Markov kernels\nΘ\nT\n/\nφ◦T=U\n9\nX\nφ\n/ Z\nwe have that Ez∼U(π)L(U∗(y)) = Ez∼U(π)L[T](L)(φ∗(y))\nProof\nIt should be fairly obvious that L[T](L) is concave as it is the composition of a linear function and\na concave function. Furthermore\nEz∼U(π)L[T](L)(φ∗(z)) = Ez∼U(π)L(T ∗◦φ∗(z))\n= Ez∼U(π)L(U∗(z))\nwe can now prove theorem 2\nProof\n∆π(T, φ ◦T) =\nsup\nL∈L(Θ)\n|Ex∼T(π)L(T ∗(x)) −Ey∼φ◦T(π)L(T ∗◦φ∗(x))|\n∥L∥∞\n(1)\n=\nsup\nL∈L(Θ)\n|Ex∼T(π)L[T](L)(δx) −Ey∼φ◦T(π)L[T](L)(φ∗(y))|\n∥L[T](L)∥π\n(2)\n≤\nsup\nL∈L(X)\n|Ex∼T(π)L(δx) −Ey∼φ◦T(π)L(φ∗(x))|\n∥L∥∞\n(3)\n= ∆T(π)(idX, φ)\n7.12. Proof of Theorem 3\nTheorem Fix a prior πX and an ϵ > 0. φ : X ⇝Z constitutes generic features of quality ϵ\nfor X if and only if for the reconstruction problem (X, Z, φ, X, L01), one can ﬁnd a decision rule\nd : Z ⇝X with\nRL01(πX, d ◦φ) ≤ϵ\n2.\nFirst the if.\nProof (Forward Implication) Let T : Θ ⇝X be an experiment, L a loss and π a prior on Θ with\nT(π) = πX. For any DT ∈M(Θ, A)T consider\nDφ◦T = (DT /T) ◦d ◦φ ◦T ∈M(Θ, A)φ◦T .\n24\nLE CAM MEETS LECUN\nWe wish to calculate\nRL(π, Dφ◦T ) = Eθ∼πEx∼T(θ)Ex′∼(d◦φ)(x)Ea∼(DT /T)(x′)L(Θ, x).\nTo calculate this risk we consider two cases. Either x = x′ with probability 1 −ϵ\n2, giving risk\nRL(π, DT ), or x′ ̸= x with probability ϵ\n2 with risk bounded above by 2∥L∥∞. This upper bound\noccurs when the action chosen by d after seeing x is the best for θ and the action chosen by d after\nseeing x′ is the worst for θ. Combining these two gives\nRL(π, Dφ◦T ) ≤(1 −ϵ\n2)RL(π, DT ) + ϵ\n22∥L∥∞≤RL(π, DT ) + ϵ∥L∥∞.\nNote that T and L are arbitrary. Taking inﬁmums over DT and Dφ◦T gives\nVL(π, φ ◦T) ≤VL(π, T) + ϵ∥L∥∞, ∀L, ∀T, ∀π st T(π) = πX.\nNow the converse.\nProof (Converse)\nIf\nVL(π, φ ◦T) ≤VL(π, T) + ϵ∥L∥∞, ∀L, ∀T, ∀π st T(π) = πX\nthen\n∆π(φ ◦T, T) ≤ϵ, ∀T, ∀π st T(π) = πX\nin particular by taking T = idX and π = πX we have\n∆πX(φ, idx) ≤ϵ.\nBy the deﬁnition of weighted deﬁciency and as idX |φ, ∆πX(φ, idx) = δπ(φ, idX). Therefore there\nexists a markov kernel d : Z ⇝X with\nEx∼πX∥δx −(d ◦φ)(x)∥= 2Ex∼πXEx′∼(d◦φ)(x)1(x′ = x) ≤ϵ\nHence RL01(πX, d ◦φ) ≤ϵ\n2.\nReferences\nArindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh.\nClustering with Bregman\ndivergences. The Journal of Machine Learning Research, 6:1705–1749, 2005.\nMikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.\nNeural computation, 15(6):1373–1396, 2003.\nAnthony J. Bell and Terrence J. Sejnowski. An information-maximization approach to blind separation and\nblind deconvolution. Neural computation, 7(6):1129–1159, 1995.\n25\nVAN ROOYEN WILLIAMSON\nYoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):\n1–127, 2009.\nDavid Blackwell. Comparison of experiments. In Second Berkeley Symposium on Mathematical Statistics\nand Probability, volume 1, pages 93–102, 1951.\nDavid Blackwell. Equivalent comparisons of experiments. The Annals of Mathematical Statistics, 24(2):\n265–272, 1953.\nThomas M. Cover and Jay A. Thomas. Elements of Information Theory. Wiley, 2012.\nA. Phillip Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical Mathematics,\n(April 2006):77–93, 2007.\nMorris H. DeGroot. Uncertainty, information, and sequential experiments. The Annals of Mathematical\nStatistics, 33(2):404–419, 1962.\nThomas Shelburne Ferguson. Mathematical statistics: A decision theoretic approach. Academic Press New\nYork, 1967.\nDario Garcia-Garcia and Robert C. Williamson. Divergences and Risks for Multiclass Experiments. In\nConference on Learning Theory (JMLR: W&CP), volume 23, 2012.\nPeter D. Gr¨unwald and A. Philip Dawid. Game theory, maximum entropy, minimum discrepancy and robust\nBayesian decision theory. The Annals of Statistics, 32(4):1367–1433, 2004.\nMartin Hellman and Josef Raviv. Probability of error, equivocation, and the Chernoff bound. IEEE Transac-\ntions on Information Theory, 16(4):368–372, 1970.\nGeoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural networks.\nScience, 313(5786):504–507, 2006.\nPeter J. Huber. Robust Statistics. Wiley Series in Probability and Statistics. Wiley, 2011.\nHidetoshi Komiya. Elementary proof for Sion’s minimax theorem. Kodai mathematical journal, 11(1):5–7,\n1988. ISSN 0386-5991.\nLucien Lecam.\nSufﬁciency and approximate sufﬁciency.\nThe Annals of Mathematical Statistics, 35(4):\n1419–1455, 1964.\nLucien Lecam. On the Information Contained in Additional Observations. The Annals of Statistics, 2(4):\n630–649, 1974.\nLucien Lecam. Asymptotic Methods in Statistical Decision Theory. Springer London, 2011.\nYann LeCun. Learning Representations: A Challenge for Learning Theory, Plenary talk COLT 2013, 2013.\nHonglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efﬁcient sparse coding algorithms. In Advances\nin neural information processing systems, pages 801–808, 2006.\nFriedrich Liese. phi-divergences, sufﬁciency, Bayes sufﬁciency, and deﬁciency. Kybernetika, 48(4):690–713,\n2012.\nRalph Linsker. An application of the principle of maximum information preservation to linear systems. NIPS,\n1989.\n26\nLE CAM MEETS LECUN\nBruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set: A strategy employed\nby V1? Vision research, 37(23):3311–3325, 1997.\nMatthew Parry, A Philip Dawid, and Steffen Lauritzen. Proper local scoring rules. The Annals of Statistics,\n40(1):561–592, 2012.\nMark D. Reid and Robert C. Williamson. Information, divergence and risk for binary experiments. The\nJournal of Machine Learning Research, 12:731–817, 2011.\nVin D. Silva and Joshua B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction.\nIn Advances in neural information processing systems, pages 705–712, 2002.\nJoshua B. Tenenbaum, Vin De Silva, and John C. Langford. A global geometric framework for nonlinear\ndimensionality reduction. Science, 290(5500):2319–2323, 2000.\nNaftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In Proc. of\nAllerton Conf. on Communication, Control and Computing, volume physics/00, pages 368–377, 1999.\nErik Torgersen. Comparison of Statistical Experiments. Cambridge University Press, 1991.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In Proceedings of the 25th International Conference on\nMachine Learning, pages 1096–1103. ACM, 2008.\nMoshe Zakai and Jacob Ziv. A generalization of the rate-distortion theory and application. Information\nTheory, New Trends and Open Problems, pages 87–123, 1975.\nJacob Ziv and Moshe Zakai. On functionals satisfying a data-processing theorem. IEEE Transactions on\nInformation Theory, I(3):770–772, 1973.\n27\n",
  "categories": [
    "stat.ML"
  ],
  "published": "2014-02-20",
  "updated": "2014-02-21"
}