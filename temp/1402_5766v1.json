{
  "id": "http://arxiv.org/abs/1402.5766v1",
  "title": "No more meta-parameter tuning in unsupervised sparse feature learning",
  "authors": [
    "Adriana Romero",
    "Petia Radeva",
    "Carlo Gatta"
  ],
  "abstract": "We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised\nfeature learning algorithm, which exploits a new way of optimizing for\nsparsity. Experiments on STL-10 show that the method presents state-of-the-art\nperformance and provides discriminative features that generalize well.",
  "text": "arXiv:1402.5766v1  [cs.LG]  24 Feb 2014\nNo more meta-parameter tuning\nin unsupervised sparse feature learning\nAdriana Romero\nADRIANA.ROMERO@UB.EDU\nDepartament de Matem`atica Aplicada i An`alisi, Universitat de Barcelona, Barcelona, Spain.\nPetia Radeva\nPETIA.IVANOVA@UB.EDU\nDepartament de Matem`atica Aplicada i An`alisi, Universitat de Barcelona, Barcelona, Spain.\nCarlo Gatta\nCGATTA@CVC.UAB.ES\nCentre de Visi´o per Computador, Bellaterra, Spain.\nAbstract\nWe propose a meta-parameter free, off-the-shelf,\nsimple and fast unsupervised feature learning al-\ngorithm, which exploits a new way of optimizing\nfor sparsity. Experiments on STL-10 show that\nthe method presents state-of-the-art performance\nand provides discriminative features that gener-\nalize well.\n1. Introduction\nSigniﬁcant effort has been devoted to handcraft appropri-\nate feature representations of data in several ﬁelds. In tasks\nsuch as image classiﬁcation and object recognition, unsu-\npervised learned features have shown to compete well or\neven outperform manually designed ones (Ranzato et al.,\n2006; Yang et al., 2009; Coates et al., 2011).\nUnsuper-\nvised feature learning has also shown to be helpful\nin greedy layerwise pre-training of deep architectures\n(Hinton et al., 2006; Bengio et al., 2006; Larochelle et al.,\n2009; Erhan et al., 2010).\nIn (Bengio, 2009), the author claims that potentially in-\nteresting research involves pre-training algorithms, which\n“[...] would be proﬁcient at extracting good features but\ninvolving an easier optimization problem.” In addition to\nthat, one of the main criticisms to state-of-the-art meth-\nods is that they require a signiﬁcant amount of meta-\nparameters (Bengio et al., 2013). As stated in (Snoek et al.,\n2012), the tuning of these meta-parameters is a labori-\nous task that requires expert knowledge, rules of thumb\nor extensive search and, whose setting can vary for dif-\nferent tasks. Therefore, there is great interest for meta-\nparameter free methods (Ngiam et al., 2011) and automatic\napproaches to optimize the performance of learning algo-\nrithms (Snoek et al., 2012).\nNevertheless, little effort has been devoted to address this\nproblem (see Table 1 for a comparison of meta-parameters\nrequired by unsupervised feature learning methods). To\nthe best of our knowledge, work in this direction includes\nICA (Hyv¨arinen & Oja, 2000; Hyv¨arinen et al., 2000) and\nsparse ﬁltering (Ngiam et al., 2011). Although ICA pro-\nvides good results at object recognition tasks (Le et al.,\n2011; Ngiam et al., 2011), the method scales poorly to\nlarge datasets and high input dimensionality.\nComputational complexity is also a major drawback of\nmany state-of-the-art methods.\nICA requires an expen-\nsive orthogonalization to be computed at each iteration.\nSparse coding has an expensive inference, which requires\na prohibitive iterative optimization. Signiﬁcant amount of\nwork has been done in order to overcome this limitation\n(Lee et al., 2006; Kavukcuoglu et al., 2010).\nPredictive\nSparse Decomposition (PSD) (Kavukcuoglu et al., 2010) is\na successful variant of sparse coding, which uses a predic-\ntor to approximate the sparse representation and solves the\nsparse coding computationally expensive encoding step.\nIn this paper, we aim to solve some of the above-mentioned\nproblems. We propose a meta-parameter free, off-the-\nshelf, simple and fast approach, which exploits a new way\nof optimizing for a sparsity, without explicitly modeling the\ndata distribution. The method iteratively builds an ideally\nsparse target and optimizes the dictionary by minimizing\nthe error between the system output and the ideally sparse\ntarget. Deﬁning sparsity concepts in terms of expected out-\nput allows to exploit a new strategy in unsupervised train-\ning.\nNo more meta-parameter tuning in unsupervised sparse feature learning\nTable 1. Meta-parameters to tune of state-of-the-art unsupervised feature learning methods.\nMethod\nMeta-parameters to tune\nSparse RBM (Hinton et al., 2006; Lee et al., 2008)\nweight decay, sparseness constant,\nsparsity penalty, momentum\nSparse\nauto-encoders (Ranzato et al., 2006)\nweight decay, sparseness constant,\nsparsity penalty\nSparse Coding (Olshausen & Field, 1997)\nsparsity penalty\nRICA (Le et al., 2011)\nreconstruction penalty\nPSD (Kavukcuoglu et al., 2010)\nsparsity penalty, prediction penalty\nOMP-k (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011)\nk (non-zero elements)\nICA (Hyv¨arinen & Oja, 2000; Hyv¨arinen et al., 2000)\n-\nSparse Filtering (Ngiam et al., 2011)\n-\nIt is worth stressing that many optimization strategies can\nbe used to minimize the above-mentioned error and that\nparameters of these optimization techniques must not be\nconsidered as belonging to our approach.\nExperiments on STL-10 dataset show that the method out-\nperforms state-of-the-art methods in single layer image\nclassiﬁcation, providing discriminative features that gener-\nalize well.\nLinear feature extraction methods combined with sparse\ncoding encodings are among best performers on object\nrecognition datasets. The importance of properly combin-\ning training/encoding and encoding/pooling strategies has\nbeen argued in (Coates & Ng, 2011) and (Zeiler & Fergus,\n2013) respectively. Since the goal of this paper is to pro-\npose a new method for unsupervised feature learning, deal-\ning with all the possible combinations of encoding and\npooling could mask the beneﬁts of the method that we pro-\npose. However, for the sake of fair comparison with the\nstate-of-the-art, we test the method with sparse coding and\nsoft-threshold encodings combined with sum pooling, fol-\nlowing the experimental pipeline of (Coates & Ng, 2011).\n2. State-of-the-art\nCommonly used algorithms for unsupervised feature\nlearning include Restricted Boltzmann Machines (RBM)\n(Hinton et al., 2006), auto-encoders (Bengio et al., 2006),\nsparse coding (Raina et al., 2007) and hybrids such as\nPSD (Kavukcuoglu et al., 2010).\nMany other methods\nsuch as ICA (Hyv¨arinen & Oja, 2000; Hyv¨arinen et al.,\n2000),\nReconstruction ICA (RICA) (Le et al., 2011),\nSparse Filtering (Ngiam et al., 2011) and methods re-\nlated to vector quantization such as Orthogonal Matching\nPursuit (OMP-k) (Pati et al., 1993; Blumensath & Davies,\n2007; Coates & Ng, 2011) have also been used in\nthe literature to extract unsupervised feature represen-\ntations.\nThese algorithms could be divided into two\ncategories:\nexplicitly modeling or not the input dis-\ntribution.\nSparse auto-encoders (Ranzato et al., 2006),\nsparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton,\n2010; Goh et al., 2012), sparse coding (Olshausen & Field,\n1997), PSD (Kavukcuoglu et al., 2010), OMP-k (Pati et al.,\n1993; Blumensath & Davies, 2007; Coates & Ng, 2011)\nand Reconstruction ICA (RICA) (Le et al., 2011) explic-\nitly model the data distribution by minimizing the recon-\nstruction error. Although learning a good approximation of\nthe data distribution may be desirable, approaches such as\nsparse ﬁltering (Ngiam et al., 2011) show that this seems\nnot so important if the goal is to have a discriminative\nsparse system. Sparse ﬁltering does not attempt to explic-\nitly model the input distribution but focuses on the proper-\nties of the output distribution instead.\nSparsity is among the desirable properties of a good\noutput representation (Field, 1994; Olshausen & Field,\n1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011;\nNgiam et al., 2011; Bengio et al., 2013). Sparse features\nconsist of a large amount of outputs, which respond rarely\nand provide high responses when they do respond. Sparsity\ncan be described in terms of population sparsity and life-\ntime sparsity (Willmore & Tolhurst, 2001). Both lifetime\nand population sparsity are important properties of the out-\nput distribution. On one hand, lifetime sparsity plays an im-\nportant role in preventing bad solutions such as numerous\ndead outputs. There seems to be a consensus to overcome\nsuch degenerate solutions, which is to ensure similar statis-\ntics among outputs (Field, 1994; Willmore & Tolhurst,\n2001; Ranzato et al., 2006; Ngiam et al., 2011).\nOn the\nother hand, population sparsity helps providing a simple\ninterpretation of the input data such as the ones found in\nearly visual areas. To the best of our knowledge, the deﬁ-\nnition of population sparsity remains ambiguous.\nState-of-the-art methods optimize either for one or both\nsparsity forms in their objective function. The great major-\nity seeks sparsity using the L1 penalty and does not opti-\nmize for an explicit level of sparsity in their outputs. Sparse\nauto-encoders optimize for a target activation allowing to\ndeal with lifetime sparsity; nevertheless, the target acti-\nvation requires tuning and does not explicitly control the\nlevel of population sparsity. OMP-k deﬁnes the level of\npopulation sparsity by setting k to the maximum expected\nNo more meta-parameter tuning in unsupervised sparse feature learning\nnumber of non-zero elements per output code, whereas the\nmethods in (Olshausen & Field, 1997; Ranzato et al., 2006;\nLee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not\nexplicitly deﬁne the proportion of outputs expected to be\nactive at the same time.\n3. Method\nIn this section, we describe how the proposed method\nlearns a sparse feature representation of the data in terms\nof population and lifetime sparsity. The method iteratively\nbuilds an ideally sparse target and optimizes the dictionary\nby minimizing the error between the system output and the\nideally sparse target. Subsection 3.1 highlights the algo-\nrithm to enforce lifetime and population sparsity in the ide-\nally sparse target. Subsection 3.2 provides implementation\ndetails on the system and optimization strategies used to\nminimize the error between the system output and the ide-\nally sparse target.\n3.1. Enforcing Population and Lifetime Sparsity by\ndeﬁning an ideal target\nWe deﬁne population and lifetime sparsity as properties of\nan ideal sparse output. Given N training samples and an\noutput of dimensionality Nh, we deﬁne the ﬁrst property\nof the output as:\n1. Strong Lifetime Sparsity: The output vectors must\nbe composed solely of active and inactive units (no\nintermediate values between two ﬁxed scalars are al-\nlowed) and all outputs must activate for an equal num-\nber of inputs. Activation is exactly distributed among\nthe Nh outputs.\nOur Strong Lifetime Sparsity deﬁnition is a more strict re-\nquirement than the high dispersal concept introduced in\n(Ngiam et al., 2011), since they only require that “the mean\nsquared activations of each feature (output) [...] should be\nroughly the same for all features (outputs)”. While high\ndispersal attempts to diversify the learned bases, it does not\nguarantee the output distribution, in the lifetime sense, to\nbe composed of only a few activations. Furthermore, our\ndeﬁnition ensures the absence of dead outputs.\nGiven our deﬁnition of Strong Lifetime Sparsity, the popu-\nlation sparsity must require that, for each training sample,\nonly one output element is active:\n2. Strong Population Sparsity: For each training sam-\nple only one output must be active.\nThe rationale of our approach is to appropriately generate\nan ideal output target that fulﬁls properties (1) and (2), and\nthen learn the parameters of the system by minimizing the\nL2 error between the output target and the output generated\nby the system during training. In this way, we seek a system\noptimized for both population and lifetime sparsity in an\nexplicit way.\nThe key component of our approach is how to deﬁne the\nideal output target based on the above-mentioned proper-\nties. However, to ensure that the optimization of the system\nparameters converges, we add a third property:\n3. Minimal Perturbation:\nThe ideal output target\nshould be deﬁned as the best approximation of the sys-\ntem output by means of L2 error fulﬁlling properties\n(1) & (2).\nCreating the output target that ensures the above-mentioned\nproperties is analogous to solving an assignment problem.\nThe Hungarian method (Kuhn, 1955) is a combinatorial\noptimization algorithm, which solves the assignment prob-\nlem. However, its computational cost O((NNh)3/2) is pro-\nhibitive. Therefore, in the next section we propose a simple\nand fast O(NNh) algorithm to generate the ideal output\ntarget, which ensures sparsity properties (1) and (2) and\nprovides an approximate solution for minimal perturbation\nproperty (3).\n3.1.1. IDEAL TARGET GENERATION: THE ENFORCING\nPOPULATION AND LIFETIME SPARSITY (EPLS)\nALGORITHM\nLet us assume that we have a system, which produces a\nrow output vector h. We use the notation hj to refer to one\nelement of h. We deﬁne an output matrix H composed of\nNb output vectors of dimensionality Nh, such that Nb ≤\nN. Likewise, we deﬁne an ideal target output matrix T\nof the same size. Algorithm 1 details the EPLS algorithm\nto generate the ideal target T from H. For the sake of\nsimplicity, every step of the algorithm where the subscript\nj appears must be applied ∀j ∈{1, 2, . . ., Nh}.\nAlgorithm 1 EPLS\nRequire: H, a, N\nEnsure: T, a\n1: T = 0\n2: for n = 1 →Nb do\n3:\nhj = Hn,j\n4:\nk = arg maxj (hj −aj)\n5:\nTn,k = 1\n6:\nak = ak + Nh\nN + ǫ\n7: end for\n8: Remap T to active/inactive values of the corresponding func-\ntion.\nStarting with no activation in T (line 1), the algorithm pro-\nceeds as follows. A row vector h from H is processed at\neach iteration (line 3). The crucial step is performed in line\nNo more meta-parameter tuning in unsupervised sparse feature learning\n4: the output k that has to be activated in the nth row of T\nis selected as the one that has the maximal activation value\nhj minus the inhibitor aj. The inhibitor aj can be seen as\nan accumulator that “counts” the number of times an output\nj has been selected, increasing its inhibition progressively\nby Nh/N until reaching maximal inhibition. This prevents\nthe selection of an output that has already been activated\nN/Nh times. The rationale behind the equation in line 4\nis that, while selecting the maximal responses in the matrix\nH, we have to take care to distribute them evenly among\nall outputs (in order to ensure Strong Lifetime Sparsity).\nUsing this strategy, it can be demonstrated that the result-\ning matrix T perfectly fulﬁlls properties (1) and (2). In\nline 5, the algorithm activates the kth element of nth row\nof the target matrix T. By activating the “relative” maxi-\nmum, we approximate property (3). Finally, the inhibitor a\nis updated in line 6.\n3.2. System and Optimization strategies\nLet us assume that we have a system parameterized by Γ =\n{W, b}, with activation function f, which takes as input a\ndata vector d and produces an output vector h = f(d, Γ).\nWe use the same notation as in Section 3 and deﬁne a data\nmatrix D composed of N rows and Nd columns, where Nd\nis the input dimensionality.\nTo compare our training strategy to previous well known\nsystems, we tested our algorithm using\nH = f (DW + b) ,\n(1)\nwhere f is a logistic non-linearity.\n3.2.1. OPTIMIZATION STRATEGY\nThe system might be trained by means of an off-the-shelf\nmini-batch Stochastic Gradient Descent (SGD) method\nwith adaptive learning rates such as variance-based SGD\n(vSGD) (Schaul et al., 2013). Algorithm 2 details the latter\ntraining process. The mini-batch size Nb can be set to any\nvalue, in all the experiments we have set Nb = Nh. Start-\ning with Γ set to small random numbers as in (LeCun et al.,\n1998) (line 1), at each epoch we shufﬂe the samples of the\ntraining set (line 3), reset the EPLS inhibitor a to a ﬂat\nactivation (line 4) and process all mini-batches. For each\nmini-batch b, samples D(b) are selected (line 6). Then, the\noutput H(b) is computed (line 7) and the EPLS is invoked\nto compute T(b) and update a (line 8). After that, the gra-\ndient of the error is computed (line 9) and the learning rate\nη is estimated as in (Schaul et al., 2013) (line 10). The sys-\ntem parameters are then updated to minimize the L2 error\nE(b) = ||H(b)−T(b)||2\n2 (line 11). Finally, the bases W in Γ\nare limited to have unit norm to avoid degenerate solutions\n(line 13). This procedure is repeated until a stop condition\nis met; in our experiments, the training stops when the rel-\native decrement error between epochs is small (< 10−6).\nWhen updating the system parameters, we assume that T\ndoes not depend on Γ, thus ∂T\n∂Γ = 0; we carried out ex-\nperiments that show that this approximation does not sig-\nniﬁcantly inﬂuence the gradient descent convergence nor\nthe quality of the minimization. Moreover, this assumption\nmakes the algorithm faster, since we remove the need of\ncomputing the numerical partial derivatives of T.\nThe mini-batch vSGD allows to scale the algorithm easily,\nespecially with respect to the number of samples N.\nAlgorithm 2 Standard EPLS training\nRequire: D\nEnsure: Γ\n1: Γ = small random values\n2: repeat\n3:\nShufﬂe D randomly\n4:\na = ﬂat activation\n5:\nfor b = 1 →⌊N/Nb⌋do\n6:\nSelect mini-batch samples D(b)\n7:\nH(b) = f(D(b), Γ)\n8:\n(T(b), a) = EPLS(H(b), a, N)\n9:\nG = ∇Γ||H(b) −T(b)||2\n2\n10:\nEstimate learning rate η as in (Schaul et al., 2013)\n11:\nΓ = Γ −ηG\n12:\nend for\n13:\nLimit the bases W in Γ to have unit norm\n14: until stop condition veriﬁed\n4. Experiments\nThe performance of training and encoding strategies in\nsingle layer networks has been extensively analyzed in\nthe literature (Coates et al., 2011; Coates & Ng, 2011;\nNgiam et al., 2011) on STL-101 dataset. STL-10 dataset\nconsists of 96x96 pixels color images belonging to 10 dif-\nferent classes. The dataset is divided into a large unlabeled\ntraining set containing 100K images and smaller labeled\ntraining and test sets, containing 5000 and 8000 images,\nrespectively. It has to be considered that in STL-10, the pri-\nmary challenge is to make use of the unlabeled data (100K\nimages), which is 100 times bigger than the labeled data\nused to train the classiﬁer (1000 images per fold). In this\ncase, the supervised training must strongly rely on the abil-\nity of the unsupervised method to learn discriminative fea-\ntures. Moreover, since the unlabeled dataset contains other\ntypes of animals (bears, rabbits, etc.) and vehicles (trains,\nbuses, etc.) in addition to the ones in the labeled set, the\nunsupervised method should be able to generalize well.\nTo validate our method, we follow the experimental\npipeline of (Coates et al., 2011). We ﬁrst extract random\npatches and normalize them for local brightness and con-\n1http://www.stanford.edu/∼acoates/stl10/\nNo more meta-parameter tuning in unsupervised sparse feature learning\nFigure 1. Random subset of bases learned by EPLS, a receptive\nﬁeld of 10 pixels and Nh = 1600 (better seen in color).\nTable 2. Classiﬁcation accuracy on STL-10.\nAlgorithm\nAccuracy\nSingle-Layer with meta-parameters\nRICA (Le et al., 2011) (1600/Natural)\n52.9%\nOMP-1 (1600/Natural)\n51.8% (0.47%)\nOMP-1 (whitening, 1600/Natural)\n53.1% (0.52%)\nOMP-1 (whitening, 1600x2/Natural)\n54.5% (0.66%)\nOMP-1 (whitening, 1600x2/SC)\n59.0% (0.80%)\nSingle-Layer without meta-parameters\nRaw pixels\n31.8% (0.62%)\nICA (whitening, Complete/Natural)\n48.0% (1.47%)\nK-means-tri (whitening, 1600)\n51.5% (1.73%)\nSparse Filtering (1600/Natural)\n53.5% (0.53%)\nNatural\n(1600)\nNatural\n(1600x2)\nSC\n(1600x2)\nEPLS\n56.6% (0.66%)\n56.9% (0.50%)\n61.0% (0.58%)\ntrast. Note that EPLS does not require any whitening of the\ninput data, since it decorrelates the data during the train-\ning by means of the imposed strong sparsity properties of\nthe output target. Then, we apply the system to retrieve\nsparse features of patches covering the input image, pool\nthem into 4 quadrants and ﬁnally train a L2 SVM for clas-\nsiﬁcation purposes. We tune the SVM parameter using 5-\nfold cross-validation. As in (Ngiam et al., 2011), we use a\nreceptive ﬁeld of 10x10 pixels and a stride of 1. The num-\nber of outputs is set to Nh = 1600 for fair comparison with\nthe other state-of-the-art methods. We also provide the re-\nsults of our method with sign split (Nh = 1600x2, using\nW and −W for encoding as in (Coates & Ng, 2011)) and\nusing the sparse coding (SC) encoder, which (Coates & Ng,\n2011) found to be the best when small number of labeled\ndata is available. For this encoder, we searched over the\nsame set of parameter values as (Coates & Ng, 2011), i.e.,\nλ = {0.5, 0.75, 1.0, 1.25, 1.5}. The parameter λ is tuned\nto consider the use of sparse coding as encoder after the\ntraining and, thus, does not belong to the method that we\npropose.\nTable 2 summarizes the results obtained on this dataset\ncompared to other state-of-the-art methods. When pair-\ning each training method with its associated natural encod-\ning, EPLS outperforms all the other methods. When pair-\ning the training methods with sparse coding, EPLS outper-\nforms the state-of-the-art best performer in single layer net-\nworks as well, achieving 61.0% (0.58%) accuracy. More-\nover, the standard deviation of the folds is lower than the\none provided by OMP-1 with sparse coding encoding. Re-\nsults are even more impressive if we compare them to meta-\nparameter free algorithms.\nFigure 1 shows a subset of 100 randomly selected bases\nlearned by our method, 10x10 pixel receptive ﬁeld and a\nsystem of Nh = 1600 outputs. As shown in the ﬁgure,\nthe method learns not only common bases such as oriented\nedges/ridges in many directions and colors but also corner\ndetectors, tri-banded colored ﬁlters, center surrounds and\nLaplacian of Gaussians among others. This suggests that\nenforcing lifetime sparsity helps the system to learn a set\nof complex, rich and diversiﬁed bases.\n5. Computational complexity\nThe EPLS algorithm requires the computation of T, which\nhas O(NNh) cost, and therefore scales linearly on both\nN and Nh.\nSince we can use vSGD for optimization,\nthe method scales linearly on N given a ﬁxed number of\nepochs. Finally, applying the activation function, the cost\nof computing the derivative is linear with Nd, since we use\na closed form for ∂E\n∂Γ .\nThe memory complexity is related to the mini-batch size\nNb. Consequently, the method can scale gracefully to very\nlarge datasets: theoretically, it requires to store in mem-\nory the mini-batch input data D(b) (NbNd elements), out-\nput H(b) (NbNh elements), target T(b) (NbNh elements)\nand the system parameters to optimize Γ (Nh (Nd + 1) el-\nements); a total amount of Nh (Nd + 1) + Nb (Nd + 2Nh)\nelements.\n6. Discussion\nOur results show that simultaneously enforcing both pop-\nulation and lifetime sparsity helps in learning discrimina-\ntive dictionaries, which reﬂect in better performance, es-\npecially when compared to meta-parameter free methods\n(Ngiam et al., 2011; Le et al., 2011). Experiments suggest\nthat our algorithm is able to extract features that general-\nize well on unseen data. When comparing the performance\nSTL-10 dataset, our algorithm outperforms state-of-the-art\nbest performers. Results suggest that our algorithm helps\nthe classiﬁer in generalizing with a few training examples\n(1% of the dataset), gaining 2% accuracy w.r.t. the state-\nof-the art best performer (OMP-1 paired with sparse cod-\ning) with a lower standard deviation across folds, suggest-\ning more robustness to variations in the training folds.\nIt is important to highlight that OMP-1 can be seen as a\nspecial case of our algorithm, where the activation function\nis |DW| and lifetime sparsity is not taken into account in\nthe optimization process (potentially leading to dead out-\nNo more meta-parameter tuning in unsupervised sparse feature learning\nputs). Our algorithm has several advantages over OMP-1:\n(1) It can use any activation function; (2) by enforcing life-\ntime sparsity it does not suffer of the dead output problem,\nthus not requiring ad-hoc tricks to avoid it; (3) it does not\nrequire whitening, which can be a problem if the input di-\nmensionality is large (Le et al., 2011).\nWith our proposal, we advance in the meta-parameter free\nline of ICA (Hyv¨arinen & Oja, 2000) and sparse ﬁltering\n(Ngiam et al., 2011). It is clear that the advantage of sparse\nﬁltering over ICA comes from removing the orthogonal-\nity constraint, and imposing some sort of “competition”\nbetween outputs, which also permits overcomplete repre-\nsentations. Following this spirit, our algorithm imposes an\neven more strict form of competition to prevent dead out-\nputs by means of Strong Lifetime Sparsity and conﬁrms the\ntrend of (Ngiam et al., 2011; Hyv¨arinen & Oja, 2000) that\ndata reconstruction seems not so important if the goal is to\nhave a discriminative sparse system.\nLast and most importantly, it is worth highlighting ﬁve\ninteresting properties of the EPLS algorithm.\nFirst, the\nmethod is meta-parameter free, which highly simpliﬁes the\ntraining process for practitioners, especially when used as\na greedy pre-training method in deep architectures. Sec-\nond, the method is fast and scales linearly with the number\nof training samples and the input/output dimensionalities.\nThird, EPLS is easy to implement. We implemented the\nEPLS in Algorithm 1 in less than 50 lines of C code. The\nmini-batch vSGD is a general purpose optimizer; our Mat-\nlab implementation of vSGD plus the EPLS mex source\nwill be publicly available after publication. Fourth, the pro-\nposed learning strategy is not limited to perceptrons. Fifth,\nthere is an interest in the literature in avoiding redundancy\nin the image representation by using the algorithms in a\nconvolutional fashion (Kavukcuoglu et al., 2010). For this\npurpose, the EPLS can be slightly modiﬁed to apply the\nprocedure to a whole image at once and consider the mini-\nbatch size to be the image divided into patches. This aspect\nis not considered in the paper and is left for future investi-\ngation.\n7. Conclusion\nIn this paper, we introduced the Enforcing Population and\nLifetime Sparsity method. The algorithm provides a meta-\nparameter free, off-the-shelf, simple and computation-\nally efﬁcient approach for unsupervised sparse feature\nlearning. It seeks both lifetime and population sparsity in\nan explicit way in order to learn discriminative features,\nthus preventing dead outputs.\nResults show that the method signiﬁcantly outperforms\nall state-of-the-art methods on STL-10 dataset with lower\nstandard deviation across folds, suggesting more robust-\nness across training sets.\nReferences\nBengio, Yoshua. Learning deep architectures for AI. Foun-\ndations and Trends in Machine Learning, 2(1):1–127,\n2009.\nBengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and\nLarochelle, Hugo. Greedy layer-wise training of deep\nnetworks. In NIPS, pp. 153–160, 2006.\nBengio, Yoshua, Courville, Aaron C., and Vincent, Pascal.\nRepresentation learning: A review and new perspectives.\nIEEE TPAMI, 35(8):1798–1828, 2013.\nBlumensath, T. and Davies, M. E.\nOn the difference\nbetween orthogonal matching pursuit and orthogonal\nleast squares.\nUnpublished manuscript, 2007.\nURL\nhttp://www.see.ed.ac.uk/˜tblumens/\npapers/BDOMPvsOLS07.pdf.\nCoates, A., Lee, H., and Ng, A. Y. An analysis of single-\nlayer networks in unsupervised feature learning. In AIS-\nTATS, pp. 214–223, 2011.\nCoates, Adam and Ng, Andrew. The importance of encod-\ning versus training with sparse coding and vector quan-\ntization. In ICML, pp. 921–928, 2011.\nErhan, Dumitru, Courville, Aaron, Bengio, Yoshua, and\nVincent, Pascal.\nWhy does unsupervised pre-training\nhelp deep learning?\nIn AISTATS, volume 9, pp. 201–\n208, May 2010.\nField, D. J. What is the goal of sensory coding? Neural\nComputation, 6(4):559–601, July 1994.\nGoh, Hanlin, Thome, Nicolas, Cord, Matthieu, and Lim,\nJoo-Hwee. Unsupervised and supervised visual codes\nwith restricted boltzmann machines. In ECCV, pp. 298–\n311, 2012.\nHinton, G. E.\nA Practical Guide to Training Restricted\nBoltzmann Machines.\nTechnical report, University of\nToronto, 2010.\nHinton, Geoffrey E., Osindero, Simon, and Teh, Yee-\nWhye. A fast learning algorithm for deep belief nets.\nNeural Computation, 18(7):1527–1554, July 2006.\nHyv¨arinen, A. and Oja, E. Independent component analy-\nsis: algorithms and applications. Neural Networks, 13(4-\n5):411–430, 2000.\nHyv¨arinen, A., Karhunen, J., and Oja, E. Independent com-\nponent analysis. Wiley Interscience, 2000.\nNo more meta-parameter tuning in unsupervised sparse feature learning\nKavukcuoglu, Koray, Sermanet, Pierre, Boureau, Y-Lan,\nGregor, Karol, Mathieu, Micha¨el, and LeCun, Yann.\nLearning convolutional feature hierachies for visual\nrecognition. In NIPS, 2010.\nKuhn, Harold W. The hungarian method for the assignment\nproblem. Naval Research Logistics Quarterly, 2:83–97,\n1955.\nLarochelle, Hugo, Bengio, Yoshua, Louradour, J´erˆome,\nand Lamblin, Pascal. Exploring strategies for training\ndeep neural networks. J. Mach. Learn. Res., 10:1–40,\nJune 2009.\nLe, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y. ICA\nwith reconstruction cost for efﬁcient overcomplete fea-\nture learning. In NIPS, pp. 1017–1025, 2011.\nLeCun, Yann, Bottou, Leon, Orr, Genevieve, and M¨uller,\nKlaus. Efﬁcient backprop. In Neural Networks: Tricks\nof the Trade, pp. 9–50. Springer Berlin, 1998.\nLee, H., Battle, A., Raina, R., and Ng, A. Y. Efﬁcient sparse\ncoding algorithms. In NIPS, pp. 801–808, 2006.\nLee, H., Ekanadham, C., and Ng, A. Y. Sparse deep be-\nlief net model for visual area v2. In NIPS, pp. 873–880,\n2008.\nNgiam, J., Koh, P. W., Chen, Z., Bhaskar, S., and Ng, A. Y.\nSparse ﬁltering. In NIPS, pp. 1125–1133, 2011.\nOlshausen, B. and Field, D. J. Sparse coding with an over-\ncomplete basis set: a strategy employed by v1? Vision\nResearch, 37(23):3311–3325, 1997.\nPati, Y. C., Rezaifar, R., and Krishnaprasad, P. S. Orthog-\nonal matching pursuit: recursive function approximation\nwith applications to wavelet decomposition. In ACSSC,\npp. 40–44, November 1993.\nRaina, Rajat, Battle, Alexis, Lee, Honglak, Packer, Ben-\njamin, and Ng, Andrew Y. Self-taught learning: Trans-\nfer learning from unlabeled data. In ICML, pp. 759–766,\n2007.\nRanzato, M. A., Poultney, C., Chopra, S., and Lecun, Y. Ef-\nﬁcient learning of sparse representations with an energy-\nbased model. In NIPS, pp. 1137–1144, 2006.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No More\nPesky Learning Rates. In ICML, 2013.\nSnoek, J., Larochelle, H., and Adams, R. P.\nPractical\nbayesian optimization of machine learning algorithms.\nIn NIPS, pp. 2960–2968, 2012.\nWillmore, B. and Tolhurst, D. J. Characterizing the sparse-\nness of neural codes. Network, 12(12):255–270, January\n2001.\nYang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial\npyramid matching using sparse coding for image classi-\nﬁcation. In IEEE CVPR, pp. 1794–1801, 2009.\nZeiler, Matthew D. and Fergus, Rob. Stochastic pooling\nfor regularization of deep convolutional neural networks.\nCoRR, abs/1301.3557, 2013.\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2014-02-24",
  "updated": "2014-02-24"
}