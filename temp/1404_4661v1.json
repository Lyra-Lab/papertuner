{
  "id": "http://arxiv.org/abs/1404.4661v1",
  "title": "Learning Fine-grained Image Similarity with Deep Ranking",
  "authors": [
    "Jiang Wang",
    "Yang song",
    "Thomas Leung",
    "Chuck Rosenberg",
    "Jinbin Wang",
    "James Philbin",
    "Bo Chen",
    "Ying Wu"
  ],
  "abstract": "Learning fine-grained image similarity is a challenging task. It needs to\ncapture between-class and within-class image differences. This paper proposes a\ndeep ranking model that employs deep learning techniques to learn similarity\nmetric directly from images.It has higher learning capability than models based\non hand-crafted features. A novel multiscale network structure has been\ndeveloped to describe the images effectively. An efficient triplet sampling\nalgorithm is proposed to learn the model with distributed asynchronized\nstochastic gradient. Extensive experiments show that the proposed algorithm\noutperforms models based on hand-crafted visual features and deep\nclassification models.",
  "text": "arXiv:1404.4661v1  [cs.CV]  17 Apr 2014\nLearning Fine-grained Image Similarity with Deep Ranking\nJiang Wang1∗\nYang Song2\nThomas Leung2\nChuck Rosenberg2\nJingbin Wang2\nJames Philbin2\nBo Chen3\nYing Wu1\n1Northwestern University\n2Google Inc.\n3California Institute of Technology\njwa368,yingwu@eecs.northwestern.edu\nyangsong,leungt,chuck,jingbinw,jphilbin@google.com\nbchen3@caltech.edu\nAbstract\nLearning ﬁne-grained image similarity is a challenging\ntask. It needs to capture between-class and within-class\nimage differences.\nThis paper proposes a deep ranking\nmodel that employs deep learning techniques to learn sim-\nilarity metric directly from images. It has higher learning\ncapability than models based on hand-crafted features. A\nnovel multiscale network structure has been developed to\ndescribe the images effectively.\nAn efﬁcient triplet sam-\npling algorithm is proposed to learn the model with dis-\ntributed asynchronized stochastic gradient. Extensive ex-\nperiments show that the proposed algorithm outperforms\nmodels based on hand-crafted visual features and deep\nclassiﬁcation models.\n1. Introduction\nSearch-by-example, i.e. ﬁnding images that are similar\nto a query image, is an indispensable function for modern\nimage search engines. An effective image similarity metric\nis at the core of ﬁnding similar images.\nMost\nexisting\nimage\nsimilarity\nmodels\nconsider\ncategory-level image similarity. For example, in [12, 22],\ntwo images are considered similar as long as they belong\nto the same category. This category-level image similarity\nis not sufﬁcient for the search-by-example image search\napplication. Search-by-example requires the distinction of\ndifferences between images within the same category, i.e.,\nﬁne-grained image similarity.\nOne way to build image similarity models is to ﬁrst ex-\ntract features like Gabor ﬁlters, SIFT [17] and HOG [4],\nand then learn the image similarity models on top of these\nfeatures [2, 3, 22]. The performance of these methods is\nlargely limited by the representation power of the hand-\ncrafted features. Our extensive evaluation has veriﬁed that\nbeing able to jointly learn the features and similarity models\n∗The work was performed while Jiang Wang and Bo Chen interned at\nGoogle.\nQuery\nPositive\nNegative\nFigure 1. Sample images from the triplet dataset. Each column\nis a triplet.\nThe upper, middle and lower rows correspond to\nquery image, positive image, and negative image, where the pos-\nitive image is more similar to the query image that the negative\nimage, according to the human raters. The data are available at\nhttps://sites.google.com/site/imagesimilaritydata/.\nwith supervised similarity information provides great po-\ntential for more effective ﬁne-grained image similarity mod-\nels than hand-crafted features.\nDeep learning models have achieved great success on\nimage classiﬁcation tasks [15].\nHowever, similar image\nranking is different from image classiﬁcation. For image\nclassiﬁcation, “black car”, “white car” and “dark-gray car”\nare all cars, while for similar image ranking, if a query im-\nage is a “black car”, we usually want to rank the “dark gray\ncar” higher than the “white car”. We postulate that image\nclassiﬁcation models may not ﬁt directly to task of distin-\nguishing ﬁne-grained image similarity. This hypothesis is\nveriﬁed in experiments. In this paper, we propose to learn\nﬁne-grained image similarity with a deep ranking model,\nwhich characterizes the ﬁne-grained image similarity rela-\ntionship with a set of triplets. A triplet contains a query\nimage, a positive image, and a negative image, where the\npositive image is more similar to the query image than the\nnegative image (see Fig. 1 for an illustration). The image\nsimilarity relationship is characterized by relative similar-\nity ordering in the triplets. Deep ranking models can em-\nploy this ﬁne-grained image similarity information, which\nis not considered in category-level image similarity models\nor classiﬁcation models, to achieve better performance.\nAs with most machine learning problems, training data\nis critical for learning ﬁne-grained image similarity. It is\nchallenging to collect large data sets, which is required for\ntraining deep networks. We propose a novel bootstrapping\nmethod (section 6.1) to generate training data, which can\nvirtually generate unlimited amount of training data. To\nuse the data efﬁciently, an online triplet sampling algo-\nrithm is proposed to generate meaningful and discriminative\ntriplets, and to utilize asynchronized stochastic gradient al-\ngorithm in optimizing triplet-based ranking function.\nThe impact of different network structures on similar im-\nage ranking is explored. Due to the intrinsic difference be-\ntween image classiﬁcation and similar image ranking tasks,\na good network for image classiﬁcation ( [15]) may not\nbe optimal for distinguishing ﬁne-grained image similarity.\nA novel multiscale network structure has been developed,\nwhich contains the convolutional neural network with two\nlow resolution paths. It is shown that this multi-scale net-\nwork structure can work effectively for similar image rank-\ning.\nThe image similarity models are evaluated on a human-\nlabeled dataset. Since it is error-prone for human labelers\nto directly label the image ranking which may consist tens\nof images, we label the similarity relationship of the images\nwith triplets, illustrated in Fig. 1. The performance of an\nimage similarity model is determined by the fraction of the\ntriplet orderings that agrees with the ranking of the model.\nTo our knowledge, it is the ﬁrst high quality dataset with\nsimilarity ranking information for images from the same\ncategory. We compare the proposed deep ranking model\nwith state-of-the-art methods on this dataset. The experi-\nments show that the deep ranking model outperforms the\nhand-crafted visual feature-based approaches [17, 4, 3, 22]\nand deep classiﬁcation models [15] by a large margin.\nThe main contributions of this paper includes the follow-\ning. (1) A novel deep ranking model that can learn ﬁne-\ngrained image similarity model directly from images is pro-\nposed. We also propose a new bootstrapping way to gen-\nerate the training data. (2) A multi-scale network structure\nhas been developed. (3) A computationally efﬁcient online\ntriplet sampling algorithm is proposed, which is essential\nfor learning deep ranking models with online learning al-\ngorithms. (4) We are publishing an evaluation dataset. To\nour knowledge, it is the ﬁrst public data set with similar-\nity ranking information for images from the same category\n(Fig. 1).\n2. Related Work\nMost prior work on image similarity learning [23, 11]\nstudies the category-level image similarity, where two im-\nages are considered similar as long as they belong to the\nsame category. Existing deep learning models for image\nsimilarity also focus on learning category-level image sim-\nilarity [22]. Category-level image similarity mainly corre-\nsponds to semantic similarity. [6] studies the relationship\nbetween visual similarity and semantic similarity. It shows\nthat although visual and semantic similarities are gener-\nally consistent with each other across different categories,\nthere still exists considerable visual variability within a cat-\negory, especially when the category’s semantic scope is\nlarge. Thus, it is worthwhile to learn a ﬁne-grained model\nthat is capable of characterizing the ﬁne-grained visual sim-\nilarity for the images within the same category.\nThe following works are close to our work in the spirit\nof learning ﬁne-grained image similarity.\nRelative at-\ntribute [19] learns image attribute ranking among the im-\nages with the same attributes. OASIS [3] and local dis-\ntance learning [10] learn ﬁne-grained image similarity rank-\ning models on top of the hand-crafted features. These above\nworks are not deep learning based. [25] employs deep learn-\ning architecture to learn ranking model, but it learns deep\nnetwork from the “hand-crafted features” rather than di-\nrectly from the pixels. In this paper, we propose a Deep\nRanking model, which integrates the deep learning tech-\nniques and ﬁne-grained ranking model to learn ﬁne-grained\nimage similarity ranking model directly from images. The\nDeep Ranking models perform much better than category-\nlevel image similarity models in image retrieval applica-\ntions.\nPairwise ranking model is a widely used learning-to-rank\nformulation. It is used to learn image ranking models in\n[3, 19, 10]. Generating good triplet samples is a crucial\naspect of learning pairwise ranking model. In [3] and [19],\nthe triplet sampling algorithms assume that we can load the\nwhole dataset into memory, which is impractical for a large\ndataset. We design a computationally efﬁcient online triplet\nsampling algorithm that does not require loading the whole\ndataset into memory, which makes it possible to learn deep\nranking models with very large amount of training data.\n3. Overview\nOur goal is to learn image similarity models. We deﬁne\nthe similarity of two images P and Q according to their\nsquared Euclidean distance in the image embedding space:\nD(f(P), f(Q)) = ∥f(P) −f(Q)∥2\n2\n(1)\nwhere f(.) is the image embedding function that maps an\nimage to a point in an Euclidean space, and D(., .) is the\nsquared Euclidean distance in this space. The smaller the\ndistance D(P, Q) is, the more similar the two images P and\nQ are. This deﬁnition formulates the similar image ranking\nproblem as nearest neighbor search problem in Euclidean\nspace, which can be efﬁciently solved via approximate near-\nest neighbor search algorithms.\n2\nWe employ the pairwise ranking model to learn image\nsimilarity ranking models, partially motivated by [3, 19].\nSuppose we have a set of images P, and ri,j = r(pi, pj)\nis a pairwise relevance score which states how similar the\nimage pi ∈P and pj ∈P are. The more similar two images\nare, the higher their relevance score is. Our goal is to learn\nan embedding function f(.) that assigns smaller distance to\nmore similar image pairs, which can be expressed as:\nD(f(pi), f(p+\ni )) < D(f(pi), f(p−\ni )),\n∀pi, p+\ni , p−\ni such that r(pi, p+\ni ) > r(pi, p−\ni )\n(2)\nWe call ti = (pi, p+\ni , p−\ni ) a triplet, where pi, p+\ni , p−\ni are the\nquery image, positive image, and negative image, respec-\ntively. A triplet characterizes a relative similarity ranking\norder for the images pi, p+\ni , p−\ni . We can deﬁne the follow-\ning hinge loss for a triplet: ti = (pi, p+\ni , p−\ni ):\nl(pi, p+\ni , p−\ni ) =\nmax{0, g + D(f(pi), f(p+\ni )) −D(f(pi), f(p−\ni ))}\n(3)\nwhere g is a gap parameter that regularizes the gap between\nthe distance of the two image pairs: (pi, p+\ni ) and (pi, p−\ni ).\nThe hinge loss is a convex approximation to the 0-1 rank-\ning error loss, which measures the model’s violation of the\nranking order speciﬁed in the triplet. Our objective function\nis:\nmin\nX\ni\nξi + λ∥W ∥2\n2\ns.t. : max{0, g + D(f(pi), f(p+\ni )) −D(f(pi), f(p−\ni ))} ≤ξi\n∀pi, p+\ni , p−\ni such that r(pi, p+\ni ) > r(pi, p−\ni )\n(4)\nwhere λ is a regularization parameter that controls the mar-\ngin of the learned ranker to improve its generalization. W\nis the parameters of the embedding function f(.). We em-\nploy λ = 0.001 in this paper. (4) can be converted to an\nunconstrained optimization by replacing ξi = max{0, g +\nD(f(pi), f(p+\ni )) −D(f(pi), f(p−\ni ))}.\nIn this model, the most crucial component is to learn an\nimage embedding function f(.). Traditional methods typi-\ncally employ hand-crafted visual features, and learn linear\nor nonlinear transformations to obtain the image embed-\nding function. In this paper, we employ the deep learning\ntechnique to learn image similarity models directly from\nimages. We will describe the network architecture of the\ntriple-based ranking loss function in (4) and an efﬁcient op-\ntimization algorithm to minimize this objective function in\nthe following sections.\n4. Network Architecture\nA triplet-based network architecture is proposed for the\nranking loss function (4), illustrated in Fig. 2. This net-\nQ\nP\nN\nTriplet Sampling Layer\n....\nImages\n....\nRanking Layer\np i\np i\n-\np i\n+\nf(pi)\nf(pi )\nf(pi)\n+\n-\nFigure 2. The network architecture of deep ranking model.\nwork takes image triplets as input. One image triplet con-\ntains a query image pi, a positive image p+\ni and a negative\nimage p−\ni , which are fed independently into three identi-\ncal deep neural networks f(.) with shared architecture and\nparameters. A triplet characterizes the relative similarity re-\nlationship for the three images. The deep neural network\nf(.) computes the embedding of an image pi: f(pi) ∈Rd,\nwhere d is the dimension of the feature embedding.\nA ranking layer on the top evaluates the hinge loss (3)\nof a triplet. The ranking layer does not have any parame-\nter. During learning, it evaluates the model’s violation of\nthe ranking order, and back-propagates the gradients to the\nlower layers so that the lower layers can adjust their param-\neters to minimize the ranking loss (3).\nWe design a novel multiscale deep neural network archi-\ntecture that employs different levels of invariance at differ-\nent scales, inspired by [8], shown in Fig. 3. The ConvNet\nin this ﬁgure has the same architecture as the convolutional\ndeep neural network in [15]. The ConvNet encodes strong\ninvariance and captures the image semantics. The other two\nparts of the network takes down-sampled images and use\nshallower network architecture. Those two parts have less\ninvariance and capture the visual appearance. Finally, we\nnormalize the embeddings from the three parts, and com-\nbine them with a linear embedding layer. In this paper, The\ndimension of the embedding is 4096.\nWe start with a convolutional network (ConvNet) archi-\ntecture for each individual network, motivated by the recent\nsuccess of ConvNet in terms of scalability and generaliz-\nability for image classiﬁcation [15]. The ConvNet contains\nstacked convolutional layers, max-pooling layer, local nor-\nmalization layers and fully-connected layers. The readers\ncan refer to [15] or the supplemental materials for more de-\ntails.\nA convolutional layer takes an image or the feature maps\nof another layer as input, convolves it with a set of k learn-\nable kernels, and puts through the activation function to\n3\nImage\n225 x 225\nSubSample\nSubSample\nConvolution\nConvolution\n4:1\n8:1\nMax pooling\nMax pooling\n57 X 57\n29 X 29\n8 x 8 x 96\nl2 Normalization\nLinear Embedding\nl2 \nNormalization\n8 x 8: 4x4\n8 x 8: 4x4\n3 x 3: 2x2\n7 x 7: 4x4\n15 x 15 x 96\n4 x 4 x 96\n4 x 4 x 96\n3074\n4096\n4096\n4096\nConvNet\nl2 Normalization\n4096\nFigure 3. The multiscale network structure. Ech input image goes\nthrough three paths. The top green box (ConvNet) has the same\narchitecture as the deep convolutional neural network in [15]. The\nbottom parts are two low-resolution paths that extracts low resolu-\ntion visual features. Finally, we normalize the features from both\nparts, and use a linear embedding to combine them. The number\nshown on the top of a arrow is the size of the output image or\nfeature. The number shown on the top of a box is the size of the\nkernels for the corresponding layer.\ngenerate k feature maps. The convolutional layer can be\nconsidered as a set of local feature detectors.\nA max pooling layer performs max pooling over a local\nneighborhood around a pixel. The max pooling layer makes\nthe feature maps robust to small translations.\nA local normalization layer normalizes the feature map\naround a local neighborhood to have unit norm and zero\nmean. It leads to feature maps that are robust to the differ-\nences in illumination and contrast.\nThe stacked convolutional layers, max-pooling layer and\nlocal normalization layers act as translational and contrast\nrobust local feature detectors. A fully connected layer com-\nputes a non-linear transformation from the feature maps of\nthese local feature detectors.\nAlthough ConvNet achieves very good performance for\nimage classiﬁcation, the strong invariance encoded in its ar-\nchitecture can be harmful for ﬁne-grained image similarity\ntasks. The experiments show that the multiscale network ar-\nchitecture outperforms single scale ConvNet in ﬁne-grained\nimage similarity task.\n5. Optimization\nTraining a deep neural network usually needs a large\namount of training data, which may not ﬁt into the mem-\nory of a single computer. Thus, we employ the distributed\nasynchronized stochastic gradient algorithm proposed in [5]\nwith momentum algorithm [21].\nThe momentum algo-\nrithm is a stochastic variant of Nesterov’s accelerated gra-\ndient method [18], which converges faster than traditional\nstochastic gradient methods.\nBack-propagation scheme is used to compute the gradi-\nent. A deep network can be represented as the composition\nof the functions of each layer.\nf(.) = gn(gn−1(gn−2(· · · g1(.) · · · )))\n(5)\nwhere gl(.) is the forward transfer function of the l-th layer.\nThe parameters of the transfer function gl is denoted as wl.\nThen the gradient ∂f(.)\n∂wl can be written as:\n∂f(.)\n∂gl\n×\n∂gl\n∂wl ,\nand ∂f(.)\n∂gl\ncan be efﬁciently computed in an iterative way:\n∂f(.)\n∂gl+1 × ∂gl+1(.)\n∂gl\n. Thus, we only need to compute the gradi-\nents\n∂gl\n∂wl and\n∂gl\n∂gl−1 for the function gl(.). More details of\nthe optimization can be found in the supplemental materi-\nals.\nTo avoid overﬁtting, dropout [13] with keeping probabil-\nity 0.6 is applied to all the fully connected layers. Random\npixel shift is applied to the input images for data augmenta-\ntion.\n5.1. Triplet Sampling\nTo avoid overﬁtting, it is desirable to utilize a large va-\nriety of images. However, the number of possible triplets\nincreases cubically with the number of images. It is compu-\ntationally prohibitive and sub-optimal to use all the triplets.\nFor example, the training dataset in this paper contains 12\nmillion images. The number of all possible triplets in this\ndataset is approximately (1.2×107)3 = 1.728×1021. This\nis an extermely large number that can not be enumerated.\nIf the proposed triplet sampling algorithm is employed, we\nﬁnd the optimization converges with about 24 million triplet\nsamples, which is a lot smaller than the number of possible\ntriplets in our dataset.\nIt is crucial to choose an effective triplet sampling strat-\negy to select the most important triplets for rank learning.\nUniformly sampling of the triplets is sub-optimal, because\nwe are more interested in the top-ranked results returned by\nthe ranking model. In this paper, we employ an online im-\nportance sampling scheme to sample triplets.\nSuppose we have a set of images P, and their pairwise\nrelevance scores ri,j = r(pi, pj). Each image pi belongs to\na category, denoted by ci. Let the total relevance score of\nan image ri deﬁned as\nri =\nX\nj:cj=ci,j̸=i\nri,j\n(6)\nThe total relevance score of an image pi reﬂects how rele-\nvant the image is in terms of its relevance to the other im-\nages in the same category.\nTo sample a triplet, we ﬁrst sample a query image pi\nfrom P according to its total relevance score. The probabil-\nity of an image being chosen as query image is proportional\nto its total relevance score.\n4\nThen, we sample a positive image p+\ni from the images\nsharing the same categories as pi. Since we are more in-\nterested in the top-ranked images, we should sample more\npositive images p+\ni with high relevance scores ri,i+. The\nprobability of choosing an image p+\ni as positive image is:\nP(p+\ni ) = min{Tp, ri,i+}\nZi\n(7)\nwhere Tp is a threshold parameter, and the normalization\nconstant Zi equals P\ni+ P(p+\ni ) for all the p+\ni sharing the\nthe same categories with pi.\nWe have two types of negative image samples. The ﬁrst\ntype is out-of-class negative samples, which are the negative\nsamples that are in a different category from query image pi.\nThey are drawn uniformly from all the images with differ-\nent categories with pi. The second type is in-class negative\nsamples, which are the negative samples that are in the same\ncategory as pi but is less relevant to pi than p+\ni . Since we\nare more interested in the top-ranked images, we draw in-\nclass negative samples p−\ni with the same distribution as (7).\nIn order to ensure robust ordering between p+\ni and p−\ni in\na triplet ti = (pi, p+\ni , p−\ni ), we also require that the margin\nbetween the relevance score ri,i+ and ri,i−should be larger\nthan Tr, i.e.,\nri,i+ −ri,i−≥Tr, ∀ti = (pi, p+\ni , p−\ni )\n(8)\nWe reject the triplets that do not satisfy this condition. If\nthe number of failure trails for one example exceeds a given\nthreshold, we simply discard this example.\nLearning deep ranking models requires large amount of\ndata, which cannot be loaded into main memory. The sam-\npling algorithms that require random access to all the ex-\namples in the dataset are not applicable. In this section, we\npropose an efﬁcient online triplet sampling algorithm based\non reservoir sampling [7].\nWe have a set of buffers to store images. Each buffer has\na ﬁxed capacity, and it stores images from the same cate-\ngory. When we have one new image pj, we compute its key\nkj = u(1/rj)\nj\n,where rj is its total relevance score deﬁned in\n(6) and uj = uniform(0, 1) is a uniformly sampled number.\nThe buffer corresponding to the image pj’s can be found ac-\ncording to its category cj. If the buffer is not full, we insert\nthe image pj into the buffer with key kj. Otherwise, we ﬁnd\nthe image p′\nj with smallest key k′\nj in the buffer. If kj > k′\nj,\nwe replace the image p′\nj with image pj in the buffer. Other-\nwise, the imgage example pj is discarded. If this replacing\nscheme is employed, uniformly sampling from a buffer is\nequivalent to drawing samples with probability proportional\nto the total relevance score rj.\nOne image pi is uniformly sampled from all the im-\nages in the buffer of category cj as the query image. We\nthen uniformly generate one image p+\ni from all the images\nBuffers for queries\nImage sample\nFind buffer \nof the query\nTriplets\nQuery\nPositive\nNegative\nFigure 4. Illustration of the online triplet sampling algorithm. The\nnegative image in this example is an out-of-class negative. We\nhave one buffer for each category. When we get a new image\nsample, we insert it into the buffer of the corresponding category\nwith prescribed probability. The query and positive examples are\nsampled from the same buffer, while the negative image is sampled\nfrom a different buffer.\nin the buffer of category cj, and accept it with probabil-\nity min(1, ri,i+/ri+), which corresponds to the sampling\nprobability (7). Sampling is continued until one example is\naccepted. This image example acts as the positive image.\nFinally, we draw a negative image sample. If we are\ndrawing out-of-class negative image sample, we draw a im-\nage p−\ni uniformly from all the images in the other buffers.\nIf we are drawing in-class negative image samples, we use\nthe positive example’s drawing method to generate a nega-\ntive sample, and accept the negative sample only if it satis-\nﬁes the margin constraint (8). Whether we sample in-class\nor out-of-class negative samples is controlled by a out-of-\nclass sample ratio parameter. An illustration of this sam-\npling method is shown in Fig. 4 The outline of reservoir\nimportance sampling algorithm is shown in the supplemen-\ntal materials.\n6. Experiments\n6.1. Training Data\nWe use two sets of training data to train our model. The\nﬁrst training data is ImageNet ILSVRC-2012 dataset [1],\nwhich contains roughly 1000 images in each of 1000 cate-\ngories. In total, there are about 1.2 million training images,\nand 50,000 validation images. This dataset is utilized to\nlearn image semantic information. We use it to pre-train the\n“ConvNet” part of our model using soft-max cost function\nas the top layer.\nThe second training data is relevance training data, re-\nsponsible for learning ﬁne-grained visual similarity. The\ndata is generated in a bootstrapping fashion. It is collected\nfrom 100,000 search queries (using Google image search),\nwith the top 140 image results from each query. There are\nabout 14 million images. We employ a golden feature to\ncompute the relevance ri,j for the images from the same\nsearch query, and set ri,j = 0 to the images from different\n5\nqueries. The golden feature is a weighted linear combina-\ntion of twenty seven features. It includes features described\nin section 6.4, with different parameter settings and distance\nmetrics. More importantly, it also includes features learned\nthrough image annotation data, such as features or embed-\ndings developed in [24]. The linear weights are learned\nthrough max-margin linear weight learning using human\nrated data. The golden feature incorporates both visual ap-\npearance information and semantic information, and it is of\nhigh performance in evaluation. However, it is expensive to\ncompute, and ”cumbersome” to develop. This training data\nis employed to ﬁne-tune our network for ﬁne-grained visual\nsimilarity.\n6.2. Triplet Evaluation Data\nSince we are interested in ﬁne-grained similarity, which\ncannot be characterized by image labels, we collect a triplet\ndataset to evaluate image similarity models 1.\nWe started from 1000 popular text queries and sampled\ntriplets (Q, A, B) from the top 50 search results for each\nquery from the Google image search engine. We then rate\nthe images in the triplets using human raters. The raters\nhave four choices: (1) both image A and B are similar to\nquery image Q; (2) both image A and B are dissimilar to\nquery image Q; (3) image A is more similar to Q than B; (4)\nimage B is more similar to Q than A. Each triplet is rated by\nthree raters. Only the triplets with unanimous scores from\nthe three rates enter the ﬁnal dataset. For our application,\nwe discard the triplets with rating (1) and rating (2), because\nthose triplets does not reﬂect any image similarity ordering.\nAbout 14,000 triplets are used in evaluation. Those triplets\nare solely used for evaluation. Fig 1 shows some triplet\nexamples.\n6.3. Evaluation Metrics\nTwo evaluation metrics are used: similarity precision\nand score-at-top-K for K = 30.\nSimilarity precision is deﬁned as the percentage of\ntriplets being correctly ranked.\nGiven a triplet ti\n=\n(pi, p+\ni , p−\ni ), where p+\ni should be more similar to pi than\np−\ni . Given pi as query, if p+\ni is ranked higher than p−\ni , then\nwe say the triplet ti is correctly ranked.\nScore-at-top-K is deﬁned as the number of correctly\nranked triplets minus the number of incorrectly ranked ones\non a subset of triplets whose ranks are higher than K. The\nsubset is chosen as follows. For each query image in the\ntest set, we retrieve 1000 images belonging to the same text\nquery, and rank these images using the learned similarity\nmetric. One triplet’s rank is higher than K if its positive\nimage p+\ni or negative image p−\ni is among the top K near-\nest neighbors of the query image pi. This metric is similar\nto the precision-at-top-K metric, which is widely used to\n1https://sites.google.com/site/imagesimilaritydata/\nevaluate retrieval systems. Intuitively, score-at-top-K mea-\nsures a retrieval system’s performance on the K most rele-\nvant search results. This metric can better reﬂect the perfor-\nmance of the similarity models in practical image retrieval\nsystems, because users pay most of their attentions to the\nresults on the ﬁrst few pages. we set K = 30 in our experi-\nments.\n6.4. Comparison with Hand-crafted Features\nWe ﬁrst compare the proposed deep ranking method with\nhand-crafted visual features. For each hand-crafted feature,\nwe report its performance using its best experimental set-\nting.\nThe evaluated hand-crafted visual features include\nWavelet [9], Color (LAB histoghram), SIFT [17]-like fea-\ntures, SIFT-like Fisher vectors [20], HOG [4], and SPMK\nTaxton features with max pooling [16]. Supervised image\nsimilarity ranking information is not used to obtain these\nfeatures.\nTwo image similarity models are learned on top of the\nconcatenation of all the visual features described above.\n• L1HashKCPA [14]: A subset of the golden features\n(with L1 distance) are chosen using max-margin lin-\near weight learning. We call this set of features “L1\nvisual features”. Weighted Minhash and Kernel prin-\ncipal component analysis (KPCA) [14] are applied on\nthe L1 visual features to learn a 1000-dimension em-\nbedding in an unsupervised fashion.\n• OASIS [3]: Based on the L1HashKCPA feature, an\ntransformation (OASIS transformation) is learnt with\nan online image similarity learning algorithm [3], us-\ning the relevance training data described in Sec. 6.1.\nThe performance comparison is shown in Table 1. The\n“DeepRanking” shown in this table is the deep ranking\nmodel trained with 20% out-of-class negative samples. We\ncan see that any individual feature without learning does\nnot performs very well. The L1HashKCPA feature achieves\nreasonably good performance with relatively low dimen-\nsion, but its performance is inferior to DeepRanking model.\nThe OASIS algorithm can learn better features because it\nexploits the image similarity ranking information in the rel-\nevance training data. By directly learning a ranking model\non images, the deep ranking method can use more informa-\ntion from image than two-step “feature extraction”-“model\nlearning” approach. Thus, it performs better both in terms\nof similarity precision and score-at-top-30.\nThe DeepRanking model performs better in terms of\nsimilarity precision than the golden features, which are used\nto generate relevance training data.\nThis is because the\nDeepRanking model employs the category-level informa-\ntion in ImageNet data and relevance training data to better\ncharacterize the image semantics. The score-at-top-30 met-\nric of DeepRanking is only slightly lower than the golden\nfeatures.\n6\nMethod\nPrecision\nScore-30\nWavelet [9]\n62.2%\n2735\nColor\n62.3%\n2935\nSIFT-like [17]\n65.5%\n2863\nFisher [20]\n67.2%\n3064\nHOG [4]\n68.4%\n3099\nSPMKtexton1024max [16]\n66.5%\n3556\nL1HashKPCA [14]\n76.2%\n6356\nOASIS [3]\n79.2%\n6813\nGolden Features\n80.3%\n7165\nDeepRanking\n85.7%\n7004\nTable 1. Similarity precision (Precision) and score-at-top-30\n(Score-30) for different features.\n6.5. Comparison of Different Architectures\nWe compare the proposed method with the following\narchitectures: (1) Deep neural network for classiﬁcation\ntrained on ImageNet, called ConvNet. This is exactly the\nsame as the model trained in [15]. (2) Single-scale deep\nneural network for ranking. It only has a single scale Con-\nvNet in deep ranking model, but It is trained in the same\nway as DeepRanking model. (3) Train an OASIS model [3]\non the feature output of single-scale deep neural network for\nranking. (4) Train a linear embedding on both the single-\nscale deep neural network and the visual features described\nin the last section. The performance are shown in Table 2.\nIn all the experiments, the Euclidean distance of the embed-\nding vectors of the penultimate layer before the ﬁnal soft-\nmax or ranking layer is exploited as similarity measure.\nFirst, we ﬁnd that the ranking model greatly increases\nthe performance. The performance of single-scale ranking\nmodel is much better than ConvNet. The two networks have\nthe same architecture except single-scale ranking model is\nﬁne-tuned with the relevance training data using ranking\nlayer, while ConvNet is trained solely for classiﬁcation task\nusing logistic regression layer.\nWe also ﬁnd that single-scale ranking performs very well\nin terms of similarity precision, but its score-at-top-30 is\nnot very high. The DeepRanking model, which employs\nmultiscale network architecture, has both better similarity\nprecision and score-at-top-30.\nFinally, although training\nan OASIS model or linear embedding on the top increases\nperformance, their performance is inferior to DeepRanking\nmodel, which uses back-propagation to ﬁne-tune the whole\nnetwork.\nAn illustration of the learned ﬁlters of the multi-scale\ndeep ranking model is shown in Fig. 5. The ﬁlters learned in\nthis paper captures more color information compared with\nthe ﬁlter learned in [15].\nMethod\nPrecision\nScore-30\nConvNet\n82.8%\n5772\nSingle-scale Ranking\n84.6%\n6245\nOASIS on Single-scale Ranking\n82.5%\n6263\nSingle-Scale & Visual Feature\n84.1%\n6765\nDeepRanking\n85.7%\n7004\nTable 2. Similarity precision (Precision) and score at top 30\n(Score-30) for different neural network architectures.\nFigure 5. The learned ﬁlters of the ﬁrst level convolutional layers\nof the multi-scale deep ranking model.\n6.6. Comparison of Different Sampling Methods\nWe study the effect of the fraction of the out-of-class\nnegative samples in online triplet sampling algorithm on\nthe performance of the proposed method. Fig. 6 shows\nthe results. The results are obtained from drawing 24 mil-\nlion triplets samples. We ﬁnd that the score-at-top-30 metric\nof DeepRanking model decreases as we have more out-of-\nclass negative samples. However, having a small fraction\nof out-of-class samples (like 20%) increases the similarity\nprecision metric a lot.\nWe also compare the performance of the weighted sam-\npling and uniform sampling with 0% out-of-class negative\nsamples. In weighted sampling, the sampling probability\nof the images is proportional to its total relevance score rj\nand pairwise relevance score ri,j, while uniform sampling\ndraws the images uniformly from all the images (but the\nranking order and margin constraints should be satisﬁed).\nWe ﬁnd that although the two sampling methods perform\nsimilarly in overall precision, the weighted sampling algo-\nrithm does better in score-at-top-30. Thus, weighted sam-\npling is employed.\n6.7. Ranking Examples\nA comparison of the ranking examples of ConvNet, OA-\nSIS feature (L1HashKPCA features with OASIS learning)\nand Deep Ranking is shown in Fig. 7. We can see that\nConvNet captures the semantic meaning of the images very\nwell, but it fails to take into account some global visual ap-\npearance, such as color and contrast. On the other hand,\n7\n0\n0.2\n0.4\n0.6\n0.8\n1\n6600\n6700\n6800\n6900\n7000\n7100\n7200\nFraction of out−of−class negative samples\nScore at 30\n \n \nweighted sampling\nuniform sampling\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.83\n0.84\n0.85\n0.86\n0.87\nFraction of out−of−class negative samples\nOverall precision\n \n \nweighted sampling\nuniform sampling\nFigure 6. The relationship between the performance of the pro-\nposed method and the fraction of out-of-class negative samples.\nConvNet\nOASIS\nDeep\nRanking\nConvNet\nOASIS\nDeep\nRanking\nQuery\nRanking Results\nFigure 7. Comparison of the ranking examples of ConvNet, Oasis\nFeatures and Deep Ranking.\nOasis features can characterize the visual appearance well,\nbut fall short on the semantics. The proposed deep ranking\nmethod incorporates both the visual appearance and image\nsemantics.\n7. Conclusion\nIn this paper, we propose a novel deep ranking model to\nlearn ﬁne-grained image similarity models. The deep rank-\ning model employs a triplet-based hinge loss ranking func-\ntion to characterize ﬁne-grained image similarity relation-\nships, and a multiscale neural network architecture to cap-\nture both the global visual properties and the image seman-\ntics. We also propose an efﬁcient online triplet sampling\nmethod that enables us to learn deep ranking models from\nvery large amount of training data. The empirical evaluation\nshows that the deep ranking model achieves much better\nperformance than the state-of-the-art hand-crafted features-\nbased models and deep classiﬁcation models. Image sim-\nilarity models can be applied to many other computer vi-\nsion applications, such as exemplar-based object recogni-\ntion/detection and image deduplication. We will explore\nalong these directions.\nReferences\n[1] A. Berg, D. Jia, and L. FeiFei. Large scale visual recognition challenge 2012,\n2012.\n[2] Y.-L. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-level features\nfor recognition. In CVPR, pages 2559–2566. IEEE, 2010.\n[3] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large scale online learning\nof image similarity through ranking. JMLR, 11:1109–1135, 2010.\n[4] N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detec-\ntion. In CVPR, pages 886–893. IEEE, 2005.\n[5] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ran-\nzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale distributed deep\nnetworks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger,\neditors, Advances in Neural Information Processing Systems 25, pages 1232–\n1240. 2012.\n[6] T. Deselaers and V. Ferrari. Visual and semantic similarity in imagenet. In\nCVPR, pages 1777–1784. IEEE, 2011.\n[7] P. S. Efraimidis. Weighted random sampling over data streams. arXiv preprint\narXiv:1012.0256, 2010.\n[8] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical fea-\ntures for scene labeling.\nPattern Analysis and Machine Intelligence, IEEE\nTransactions on, 35(8):1915–1929, 2013.\n[9] A. Finkelstein and D. Salesin. Fast multiresolution image querying. In Pro-\nceedings of the ACM SIGGRAPH Conference on Visualization: Art and Inter-\ndisciplinary Programs, pages 6–11. ACM, 1995.\n[10] A. Frome, Y. Singer, and J. Malik. Image retrieval and classiﬁcation using local\ndistance functions. In NIPS, volume 2, page 4, 2006.\n[11] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. Tagprop: Discrimina-\ntive metric learning in nearest neighbor models for image auto-annotation. In\nICCV, pages 309–316. IEEE, 2009.\n[12] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an\ninvariant mapping. In CVPR, volume 2, pages 1735–1742. IEEE, 2006.\n[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdi-\nnov. Improving neural networks by preventing co-adaptation of feature detec-\ntors. arXiv preprint arXiv:1207.0580, 2012.\n[14] S. Ioffe. Improved consistent sampling, weighted minhash and l1 sketching.\nIn Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages\n246–255. IEEE, 2010.\n[15] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In NIPS, pages 1106–1114, 2012.\n[16] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyra-\nmid matching for recognizing natural scene categories. In CVPR, volume 2,\npages 2169–2178. IEEE, 2006.\n[17] D. G. Lowe. Object recognition from local scale-invariant features. In ICCV,\nvolume 2, pages 1150–1157. IEEE, 1999.\n[18] Y. Nesterov. A method of solving a convex programming problem with conver-\ngence rate o(1/sqr(k)). Soviet Mathematics Doklady, 1983.\n[19] D. Parikh and K. Grauman. Relative attributes. In ICCV, pages 503–510. IEEE,\n2011.\n[20] F. Perronnin, Y. Liu, J. S´anchez, and H. Poirier. Large-scale image retrieval\nwith compressed ﬁsher vectors. In CVPR, pages 3384–3391. IEEE, 2010.\n[21] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initial-\nization and momentum in deep learning. In ICML, 2013.\n[22] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invariance through\nimitation. In CVPR, pages 2729–2736. IEEE, 2011.\n[23] G. Wang, D. Hoiem, and D. Forsyth. Learning image similarity from ﬂickr\ngroups using stochastic intersection kernel machines. In ICCV, pages 428–435.\nIEEE, 2009.\n[24] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: learning\nto rank with joint word-image embeddings. Machine learning, 81(1):21–35,\n2010.\n[25] P. Wu, S. C. Hoi, H. Xia, P. Zhao, D. Wang, and C. Miao. Online multimodal\ndeep similarity learning with application to image retrieval. In Proceedings of\nthe 21st ACM international conference on Multimedia, pages 153–162. ACM,\n2013.\n8\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2014-04-17",
  "updated": "2014-04-17"
}