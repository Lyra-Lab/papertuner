{
  "id": "http://arxiv.org/abs/1406.1853v2",
  "title": "Model-based Reinforcement Learning and the Eluder Dimension",
  "authors": [
    "Ian Osband",
    "Benjamin Van Roy"
  ],
  "abstract": "We consider the problem of learning to optimize an unknown Markov decision\nprocess (MDP). We show that, if the MDP can be parameterized within some known\nfunction class, we can obtain regret bounds that scale with the dimensionality,\nrather than cardinality, of the system. We characterize this dependence\nexplicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is\nthe Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. These\nrepresent the first unified regret bounds for model-based reinforcement\nlearning and provide state of the art guarantees in several important settings.\nMoreover, we present a simple and computationally efficient algorithm\n\\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies\nthese bounds.",
  "text": "arXiv:1406.1853v2  [stat.ML]  31 Oct 2014\nModel-based Reinforcement Learning\nand the Eluder Dimension\nIan Osband\nStanford University\niosband@stanford.edu\nBenjamin Van Roy\nStanford University\nbvr@stanford.edu\nAbstract\nWe consider the problem of learning to optimize an unknown Markov deci-\nsion process (MDP). We show that, if the MDP can be parameterized within\nsome known function class, we can obtain regret bounds that scale with the\ndimensionality, rather than cardinality, of the system. We characterize this\ndependence explicitly as ˜O(√dKdET) where T is time elapsed, dK is the\nKolmogorov dimension and dE is the eluder dimension. These represent\nthe ﬁrst uniﬁed regret bounds for model-based reinforcement learning and\nprovide state of the art guarantees in several important settings. More-\nover, we present a simple and computationally eﬃcient algorithm posterior\nsampling for reinforcement learning (PSRL) that satisﬁes these bounds.\n1\nIntroduction\nWe consider the reinforcement learning (RL) problem of optimizing rewards in an unknown\nMarkov decision process (MDP) [1]. In this setting an agent makes sequential decisions\nwithin its enironment to maximize its cumulative rewards through time. We model the\nenvironment as an MDP, however, unlike the standard MDP planning problem the agent\nis unsure of the underlying reward and transition functions.\nThrough exploring poorly-\nunderstood policies, an agent may improve its understanding of its environment but it may\nimprove its short term rewards by exploiting its existing knowledge [2, 3].\nThe focus of the literature in this area has been to develop algorithms whose performance\nwill be close to optimal in some sense.\nThere are numerous criteria for statistical and\ncomputational eﬃciency that might be considered. Some of the most common include PAC\n(Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It\nKnows) [6] and regret [7]. We will focus our attention upon regret, or the shortfall in the\nagent’s expected rewards compared to that of the optimal policy. We believe this is a natural\ncriteria for performance during learning, although these concepts are closely linked. A good\noverview of various eﬃciency guarantees is given in section 3 of Li et al. [6].\nBroadly, algorithms for RL can be separated as either model-based, which build a generative\nmodel of the environment, or model-free which do not. Algorithms of both type have been\ndeveloped to provide PAC-MDP bounds polynomial in the number of states S and actions\nA [8, 9, 10]. However, model-free approaches can struggle to plan eﬃcient exploration. The\nonly near-optimal regret bounds to time T of ˜O(S\n√\nAT) have only been attained by model-\nbased algorithms [7, 11]. But even these bounds grow with the cardinality of the state and\naction spaces, which may be extremely large or even inﬁnite. Worse still, there is a lower\nbound Ω(\n√\nSAT) for the expected regret in an arbitrary MDP [7].\nIn special cases, where the reward or transition function is known to belong to a certain\nfunctional family, existing algorithms can exploit the structure to move beyond this “‘tabula\nrasa” (where nothing is assumed beyond S and A) lower bound. The most widely-studied\n1\nparameterization is the degenerate MDP with no transitions, the mutli-armed bandit [12,\n13, 14]. Another common assumption is that the transition function is linear in states and\nactions. Papers here establigh regret bounds ˜O(\n√\nT) for linear quadratic control [16], but\nwith constants that grow exponentially with dimension. Later works remove this exponential\ndependence, but only under signiﬁcant sparsity assumptions [17]. The most general previous\nanalysis considers rewards and transitions that are α-H¨older in a d-dimensional space to\nestablish regret bounds ˜O(T (2d+α)/(2d+2α)) [18]. However, the proposed algorithm UCCRL\nis not computationally tractable and the bounds approach linearity in many settings.\nIn this paper we analyse the simple and intuitive algorithm posterior sampling for reinforce-\nment learning (PSRL) [20, 21, 11]. PSRL was initially introduced as a heuristic method [21],\nbut has since been shown to satisfy state of the art regret bounds in ﬁnite MDPs [11] and\nalso exploit the structure of factored MDPs [15]. We show that this same algorithm satisﬁes\ngeneral regret bounds that depends upon the dimensionality, rather than the cardinality, of\nthe underlying reward and transition function classes. To characterize the complexity of this\nlearning problem we extend the deﬁnition of the eluder dimension, previously introduced for\nbandits [19], to capture the complexity of the reinforcement learning problem. Our results\nprovide a uniﬁed analysis of model-based reinforcement learning in general and provide new\nstate of the art bounds in several important problem settings.\n2\nProblem formulation\nWe consider the problem of learning to optimize a random ﬁnite horizon MDP M =\n(S, A, RM, P M, τ, ρ) in repeated ﬁnite episodes of interaction. S is the state space, A is\nthe action space, RM(s, a) is the reward distribution over R and P M(·|s, a) is the transition\ndistribution over S when selecting action a in state s, τ is the time horizon, and ρ the initial\nstate distribution. All random variables we will consider are on a probability space (Ω, F, P).\nA policy µ is a function mapping each state s ∈S and i = 1, . . . , τ to an action a ∈A. For\neach MDP M and policy µ, we deﬁne a value function V :\nV M\nµ,i(s) := EM,µ\n\u0002\nτ\nX\nj=i\nrM(sj, aj)\n\f\f\fsi = s\n\u0003\n(1)\nwhere rM(s, a) := E[r|r ∼RM(s, a)] and the subscripts of the expectation operator indicate\nthat aj = µ(sj, j), and sj+1 ∼P M(·|sj, aj) for j = i, . . . , τ. A policy µ is said to be optimal\nfor MDP M if V M\nµ,i(s) = maxµ′ V M\nµ′,i(s) for all s ∈S and i = 1, . . . , τ. We will associate with\neach MDP M a policy µM that is optimal for M.\nWe require that the state space S is a subset of Rd for some ﬁnite d with a ∥· ∥2-norm\ninduced by an inner product. These result actually extend to general Hilbert spaces, but we\nwill not deal with that in this paper. This allows us to decompose the transition function\nas a mean value in S plus additive noise s′ ∼P M(·|s, a)\n=⇒\ns′ = pM(s, a) + ǫP . At\nﬁrst this may seem to exclude discrete MDPs with S states from our analysis. However,\nwe can represent the discrete state as a probability vector st ∈S = [0, 1]S ⊂RS with a\nsingle active component equal to 1 and 0 otherwise. In fact, the notational convention that\nS ⊆Rd should not impose a great restriction for most practical settings.\nFor any distribution Φ over S, we deﬁne the one step future value function U to be the\nexpected value of the optimal policy with the next state distributed according to Φ.\nU M\ni (Φ) := EM,µM\n\u0002\nV M\nµM,i+1(s)\n\f\fs ∼Φ\n\u0003\n.\n(2)\nOne natural regularity condition for learning is that the future values of similar distributions\nshould be similar. We examine this idea through the Lipschitz constant on the means of\nthese state distributions. We write E(Φ) := E[s|s ∼Φ] ∈S for the mean of a distribution\nΦ and express the Lipschitz continuity for U M\ni\nwith respect to the ∥· ∥2-norm of the mean:\n|U M\ni (Φ) −U M\ni (˜Φ)| ≤KM\ni (D)∥E(Φ) −E(˜Φ)∥2 for all Φ, ˜Φ ∈D\n(3)\nWe deﬁne KM(D) := maxi KM\ni (D) to be a global Lipschitz contant for the future value\nfunction with state distributions from D. Where appropriate, we will condense our notation\n2\nto write KM := KM(D(M)) where D(M) := {P M(·|s, a)|s ∈S, a ∈A} is the set of all\npossible one-step state distributions under the MDP M.\nThe reinforcement learning agent interacts with the MDP over episodes that begin at times\ntk = (k −1)τ + 1, k = 1, 2, . . .. Let Ht = (s1, a1, r1, . . . , st−1, at−1, rt−1) denote the history\nof observations made prior to time t. A reinforcement learning algorithm is a deterministic\nsequence {πk|k = 1, 2, . . .} of functions, each mapping Htk to a probability distribution\nπk(Htk) over policies which the agent will employ during the kth episode. We deﬁne the\nregret incurred by a reinforcement learning algorithm π up to time T to be\nRegret(T, π, M ∗) :=\n⌈T/τ⌉\nX\nk=1\n∆k,\nwhere ∆k denotes regret over the kth episode, deﬁned with respect to the MDP M ∗by\n∆k :=\nZ\ns∈S\nρ(s)\n\u0010\nV M∗\nµ∗,1 −V M∗\nµk,1\n\u0011\n(s)\nwith µ∗= µM∗and µk ∼πk(Htk).\nNote that regret is not deterministic since it can\ndepend on the random MDP M ∗, the algorithm’s internal random sampling and, through\nthe history Htk, on previous random transitions and random rewards. We will assess and\ncompare algorithm performance in terms of regret and its expectation.\n3\nMain results\nWe now review the algorithm PSRL, an adaptation of Thompson sampling [20] to rein-\nforcement learning. PSRL was ﬁrst proposed by Strens [21] and later was shown to satisfy\neﬃcient regret bounds in ﬁnite MDPs [11]. The algorithm begins with a prior distribution\nover MDPs. At the start of episode k, PSRL samples an MDP Mk from the posterior. PSRL\nthen follows the policy µk = µMk which is optimal for this sampled MDP during episode k.\nAlgorithm 1\nPosterior Sampling for Reinforcement Learning (PSRL)\n1: Input: Prior distribution φ for M ∗, t=1\n2: for episodes k = 1, 2, .. do\n3:\nsample Mk ∼φ(·|Ht)\n4:\ncompute µk = µMk\n5:\nfor timesteps j = 1, .., τ do\n6:\napply at ∼µk(st, j)\n7:\nobserve rt and st+1\n8:\nadvance t = t + 1\n9:\nend for\n10: end for\nTo state our results we ﬁrst introduce some notation. For any set X and Y ⊆Rd for d ﬁnite\nlet PC,σ\nX ,Y be the family the distributions from X to Y with mean ∥·∥2-bounded in [0, C] and\nadditive σ-sub-Gaussian noise. We let N(F, α, ∥· ∥2) be the α-covering number of F with\nrespect to the ∥· ∥2-norm and write nF = log(8N(F, 1/T 2, ∥· ∥2)T ) for brevity. Finally we\nwrite dE(F) = dimE(F, T −1) for the eluder dimension of F at precision T −1, a notion of\ndimension specialized to sequential measurements described in Section 4.\nOur main result, Theorem 1, bounds the expected regret of PSRL at any time T .\nTheorem 1 (Expected regret for PSRL in parameterized MDPs).\nFix a state space S, action space A, function families R ⊆PCR,σR\nS×A,R and P ⊆PCP,σP\nS×A,S for\nany CR, CP, σR, σP > 0. Let M ∗be an MDP with state space S, action space A, rewards\nR∗∈R and transitions P ∗∈P. If φ is the distribution of M ∗and K∗= KM∗is a global\nLipschitz constant for the future value function as per (3) then:\nE[Regret(T, πP S, M ∗)] ≤\n\u0002\nCR + CP\n\u0003\n+ ˜D(R) + +E[K∗]\n\u0012\n1 +\n1\nT −1\n\u0013\n˜D(P)\n(4)\n3\nWhere for F equal to either R or P we will use the shorthand:\n˜D(F) := 1 + τCFdE(F) + 8\nq\ndE(F)(4CF +\np\n2σ2\nF log(32T 3)) + 8\np\n2σ2\nFnFdE(F)T.\nTheorem 1 is a general result that applies to almost all RL settings of interest. In particular,\nwe note that any bounded function is sub-Gaussian. To clarify the assymptotics if this bound\nwe use another classical measure of dimensionality.\nDeﬁnition 1. The Kolmogorov dimension of a function class F is given by:\ndimK(F) := lim sup\nα↓0\nlog(N(F, α, ∥· ∥2))\nlog(1/α)\n.\nUsing Deﬁnition 1 in Theorem 1 we can obtain our Corollary.\nCorollary 1 (Assymptotic regret bounds for PSRL in parameterized MDPs).\nUnder the assumptions of Theorem 1 and writing dK(F) := dimK(F):\nE[Regret(T, πP S, M ∗)] = ˜O\n\u0010\nσR\np\ndK(R)dE(R)T + E[K∗]σP\np\ndK(P)dE(P)T\n\u0011\n(5)\nWhere ˜O(·) ignores terms logarithmic in T .\nIn Section 4 we provide bounds on the eluder dimension of several function classes. These\nlead to explicit regret bounds in a number of important domains such as discrete MDPs,\nlinear-quadratic control and even generalized linear systems. In all of these cases the eluder\ndimension scales comparably with more traditional notions of dimensionality. For clarity,\nwe present bounds in the case of linear-quadratic control.\nCorollary 2 (Assymptotic regret bounds for PSRL in bounded linear quadratic systems).\nLet M ∗be an n-dimensional linear-quadratic system with σ-sub-Gaussian noise. If the state\nis ∥· ∥2-bounded by C and φ is the distribution of M ∗, then:\nE[Regret(T, πP S, M ∗)] = ˜O\n\u0010\nσCλ1n2√\nT\n\u0011\n.\n(6)\nHere λ1 is the largest eigenvalue of the matrix Q given as the solution of the Ricatti equations\nfor the unconstrained optimal value function V (s) = −sT Qs [22].\nProof. We simply apply the results of for eluder dimension in Section 4 to Corollary 1 and\nupper bound the Lipschitz constant of the constrained LQR by 2Cλ1, see Appendix D.\nAlgorithms based upon posterior sampling are intimately linked to those based upon opti-\nmism [14]. In Appendix E we outline an optimistic variant that would attain similar regret\nbounds but with high probility in a frequentist sense. Unfortunately this algorithm remains\ncomputationally intractable even when presented with an approximate MDP planner. Fur-\nther, we believe that PSRL will generally be more statistically eﬃcient than an optimistic\nvariant with similar regret bounds since the algorithm is not aﬀected by loose analysis [11].\n4\nEluder dimension\nTo quantify the complexity of learning in a potentially inﬁnite MDP, we extend the existing\nnotion of eluder dimension for real-valued functions [19] to vector-valued functions. For any\nG ⊆PC,σ\nX ,Y we deﬁne the set of mean functions F = E[G] := {f|f = E[G] for G ∈G}. If\nwe consider sequential observations yi ∼G∗(xi) we can equivalently write them as yi =\nf ∗(xi)+ǫi for some f ∗(xi) = E[y|y ∼G∗(xi)] and ǫi zero mean noise. Intuitively, the eluder\ndimension of F is the length d of the longest possible sequence x1, .., xd such that for all i,\nknowing the function values of f(x1), .., f(xi) will not reveal f(xi+1).\nDeﬁnition 2 ((F, ǫ) −dependence).\nWe will say that x ∈X is (F, ǫ)-dependent on {x1, ..., xn} ⊆X\n⇐⇒∀f, ˜f ∈F,\nn\nX\ni=1\n∥f(xi) −˜f(xi)∥2\n2 ≤ǫ2 =⇒∥f(x) −˜f(x)∥2 ≤ǫ.\nx ∈X is (ǫ, F)-independent of {x1, .., xn} iﬀit does not satisfy the deﬁnition for dependence.\n4\nDeﬁnition 3 (Eluder Dimension).\nThe eluder dimension dimE(F, ǫ) is the length of the longest possible sequence of elements\nin X such that for some ǫ′ ≥ǫ every element is (F, ǫ′)-independent of its predecessors.\nTraditional notions from supervised learning, such as the VC dimension, are not suﬃcient to\ncharacterize the complexity of reinforcement learning. In fact, a family learnable in constant\ntime for supervised learning may require arbitrarily long to learn to control well [19]. The\neluder dimension mirrors the linear dimension for vector spaces, which is the length of the\nlongest sequence such that each element is linearly independent of its predecessors, but\nallows for nonlinear and approximate dependencies. We overload our notation for G ⊆PC,σ\nX ,Y\nand write dimE(G, ǫ) := dimE(E[G], ǫ), which should be clear from the context.\n4.1\nEluder dimension for speciﬁc function classes\nTheorem 1 gives regret bounds in terms of the eluder dimension, which is well-deﬁned for\nany F, ǫ. However, for any given F, ǫ actually calculating the eluder dimension may take\nsome additional analysis. We now provide bounds on the eluder dimension for some common\nfunction classes in a similar approach to earlier work for real-valued functions [14]. These\nproofs are available in Appendix C.\nProposition 1 (Eluder dimension for ﬁnite X).\nA counting argument shows that for |X| = X ﬁnite, any ǫ > 0 and any function class F:\ndimE(F, ǫ) ≤X\nThis bound is tight in the case of independent measurements.\nProposition 2 (Eluder dimension for linear functions).\nLet F = {f |f(x) = θφ(x) for θ ∈Rn×p, φ ∈Rp, ∥θ∥2 ≤Cθ, ∥φ∥2 ≤Cφ} then ∀X:\ndimE(F, ǫ) ≤p(4n −1)\ne\ne −1 log\n\" \n1 +\n\u00122CφCθ\nǫ\n\u00132!\n(4n −1)\n#\n+ 1 = ˜O(np)\nProposition 3 (Eluder dimension for quadratic functions).\nLet F = {f |f(x) = φ(x)T θφ(x) for θ ∈Rp×p, φ ∈Rp, ∥θ∥2 ≤Cθ, ∥φ∥2 ≤Cφ} then ∀X:\ndimE(F, ǫ) ≤p(4p −1)\ne\ne −1 log\n\n\n\n1 +\n \n2pC2\nφCθ\nǫ\n!2\n(4p −1)\n\n+ 1 = ˜O(p2).\nProposition 4 (Eluder dimension for generalized linear functions).\nLet g(·) be a component-wise independent function on Rn with derivative in each component\nbounded ∈[h, h] with h > 0.\nDeﬁne r =\nh\nh > 1 to be the condition number.\nIf F =\n{f |f(x) = g(θφ(x)) for θ ∈Rn×p, φ ∈Rp, ∥θ∥2 ≤Cθ, ∥φ∥2 ≤Cφ} then for any X:\ndimE(F, ǫ) ≤p\u0000r2(4n −2) + 1\u0001\ne\ne −1\n\u0012\nlog\n\u0014\u0000r2(4n −2) + 1\u0001 \u0012\n1 +\n\u00102CθCφ\nǫ\n\u00112\u0013\u0015\u0013\n+1 = ˜O(r2np)\n5\nConﬁdence sets\nWe now follow the standard argument that relates the regret of an optimistic or pos-\nterior sampling algorithm to the construction of conﬁdence sets [7, 11].\nWe will use\nthe eluder dimension build conﬁdence sets for the reward and transition which contain\nthe true functions with high probability and then bound the regret of our algorithm by\nthe maximum deviation within the conﬁdence sets.\nFor observations from f ∗∈F we\nwill center the sets around the least squares estimate ˆf LS\nt\n∈arg minf∈F L2,t(f) where\nL2,t(f) := Pt−1\ni=1 ∥f(xt) −yt∥2\n2 is the cumulative squared prediciton error. The conﬁdence\nsets are deﬁned Ft = Ft(βt) := {f ∈F|∥f −ˆf LS\nt\n∥2,Et ≤√βt} where βt controls the growth\nof the conﬁdence set and the empirical 2-norm is deﬁned ∥g∥2\n2,Et := Pt−1\ni=1 ∥g(xi)∥2\n2.\n5\nFor F ⊆PC,σ\nX ,Y, we deﬁne the distinguished control parameter:\nβ∗\nt (F, δ, α) := 8σ2 log(N(F, α, ∥· ∥2)/δ) + 2αt\n\u0010\n8C +\np\n8σ2 log(4t2/δ))\n\u0011\n(7)\nThis leads to conﬁdence sets which contain the true function with high probability.\nProposition 5 (Conﬁdence sets with high probability).\nFor all δ > 0 and α > 0 and the conﬁdence sets Ft = Ft(β∗\nt (F, δ, α)) for all t ∈N then:\nP\n \nf ∗∈\n∞\n\\\nt=1\nFt\n!\n≥1 −2δ\nProof. We combine standard martingale concentrations with a discretization scheme. The\nargument is essentially the same as Proposition 6 in [14], but extends statements about R\nto vector-valued functions. A full derivation is available in the Appendix A.\n5.1\nBounding the sum of set widths\nWe now bound the deviation from f ∗by the maximum deviation within the conﬁdence set.\nDeﬁnition 4 (Set widths).\nFor any set of functions F we deﬁne the width of the set at x to be the maximum L2 deviation\nbetween any two members of F evaluated at x.\nwF(x) := sup\nf,f∈F\n∥f(x) −f(x)∥2\nWe can bound for the number of large widths in terms of the eluder dimension.\nLemma 1 (Bounding the number of large widths).\nIf {βt > 0\n\f\ft ∈N} is a nondecreasing sequence with Ft = Ft(βt) then\nm\nX\nk=1\nτ\nX\ni=1\n1{wFtk (xtk+i) > ǫ} ≤\n\u00124βT\nǫ2\n+ τ\n\u0013\ndimE(F, ǫ)\nProof. This result follows from proposition 8 in [14] but with a small adjustment to account\nfor episodes. A full proof is given in Appendix B.\nWe now use Lemma 1 to control the cumulative deviation through time.\nProposition 6 (Bounding the sum of widths).\nIf {βt > 0\n\f\ft ∈N} is nondecreasing with Ft = Ft(βt) and ∥f∥2 ≤C for all f ∈F then:\nm\nX\nk=1\nτ\nX\ni=1\nwFtk (xtk+i) ≤1 + τCdimE(F, T −1) + 4\np\nβT dimE(F, T −1)T\n(8)\nProof. Once again we follow the analysis of Russo [14] and strealine notation by letting wt =\nwFtk (xtk+i) abd d = dimE(F, T −1). Reordering the sequence (w1, .., wT ) →(wi1, .., wiT )\nsuch that wi1 ≥.. ≥wiT we have that:\nm\nX\nk=1\nτ\nX\ni=1\nwFtk (xtk+i) =\nT\nX\nt=1\nwit ≤1 +\nT\nX\ni=1\nwit1{wit ≥T −1}\n.\nBy the reordering we know that wit > ǫ means that Pm\nk=1\nPτ\ni=1 1{wFtk (xtk+i) > ǫ} ≥t.\nFrom Lemma 1, ǫ ≤\nq\n4βT d\nt−τd. So that if wit > T −1 then wit ≤min{C,\nq\n4βT d\nt−τd}. Therefore,\nT\nX\ni=1\nwit1{wit ≥T −1} ≤τCd+\nT\nX\nt=τd+1\nr\n4βT d\nt −τd ≤τCd+2\np\nβT\nZ T\n0\nr\nd\nt dt ≤τCd+4\np\nβT dT\n6\n6\nAnalysis\nWe will now show reproduce the decomposition of expected regret in terms of the Bellman\nerror [11]. From here, we will apply the conﬁdence set results from Section 5 to obtain\nour regret bounds. We streamline our discussion of P M, RM, V M\nµ,i, U M\ni\nand T M\nµ\nby simply\nwriting ∗in place of M ∗or µ∗and k in place of Mk or µk where appropriate; for example\nV ∗\nk,i := V M∗\n˜µk,i.\nThe ﬁrst step in our ananlysis breaks down the regret by adding and subtracting the imagined\noptimal reward of µk under the MDP Mk.\n∆k =\n\u0000V ∗\n∗,1 −V ∗\nk,1\n\u0001\n(s0) =\n\u0000V ∗\n∗,1 −V k\nk,1\n\u0001\n(s0) +\n\u0000V k\nk,1 −V ∗\nk,1\n\u0001\n(s0)\n(9)\nHere s0 is a distinguished initial state, but moving to general ρ(s) poses no real challenge.\nAlgorithms based upon optimism bound (V ∗\n∗,1 −V k\nk,1) ≤0 with high probability. For PSRL\nwe use Lemma 2 and the tower property to see that this is zero in expectation.\nLemma 2 (Posterior sampling).\nIf φ is the distribution of M ∗then, for any σ(Htk)-measurable function g,\nE[g(M ∗)|Htk] = E[g(Mk)|Htk]\n(10)\nWe introduce the Bellman operator T M\nµ , which for any MDP M = (S, A, RM, P M, τ, ρ),\nstationary policy µ : S →A and value function V : S →R, is deﬁned by\nT M\nµ V (s) := rM(s, µ(s)) +\nZ\ns′∈S\nP M(s′|s, µ(s))V (s′).\nThis returns the expected value of state s where we follow the policy µ under the laws of M,\nfor one time step. The following lemma gives a concise form for the dynamic programming\nparadigm in terms of the Bellman operator.\nLemma 3 (Dynamic programming equation).\nFor any MDP M = (S, A, RM, P M, τ, ρ) and policy µ : S × {1, . . . , τ} →A, the value\nfunctions V M\nµ\nsatisfy\nV M\nµ,i = T M\nµ(·,i)V M\nµ,i+1\n(11)\nfor i = 1 . . . τ, with V M\nµ,τ+1 := 0.\nThrough repeated application of the dynamic programming operator and taking expectation\nof martingale diﬀerences we can mirror earlier analysis [11] to equate expected regret with\nthe cumulative Bellman error:\nE[∆k] =\nτ\nX\ni=1\n(T k\nk,i −T ∗\nk,i)V k\nk,i+1(stk+i)\n(12)\n6.1\nLipschitz continuity\nEﬃcient regret bounds for MDPs with an inﬁnite number of states and actions require some\nregularity assumption. One natural notion is that nearby states might have similar optimal\nvalues, or that the optimal value function function might be Lipschitz. Unfortunately, any\ndiscontinuous reward function will usually lead to discontious values functions so that this\nassumption is violated in many settings of interest.\nHowever, we only require that the\nfuture value is Lipschitz in the sense of equation (3). This will will be satisﬁed whenever the\nunderlying value function is Lipschitz, but is a strictly weaker requirement since the system\nnoise helps to smooth future values.\nSince P has σP -sub-Gaussian noise we write st+1 = pM(st, at) + ǫP\nt in the natural way. We\nnow use equation (12) to reduce regret to a sum of set widths. To reduce clutter and more\nclosely follow the notation of Section 4 we will write xk,i = (stk+i, atk+i).\nE[∆k]\n≤\nE\n\" τ\nX\ni=1\n\b\nrk(xk,i) −r∗(xk,i) + U k\ni (P k(xk,i)) −U k\ni (P ∗(xk,i))\n\t\n#\n≤\nE\n\" τ\nX\ni=1\n\b\n|rk(xk,i) −r∗(xk,i)| + Kk∥pk(xk,i) −p∗(xk,i)∥2\n\t\n#\n(13)\n7\nWhere Kk is a global Lipschitz constant for the future value function of Mk as per (3).\nWe now use the results from Sections 4 and 5 to form the corresponding conﬁdence sets\nRk := Rtk(β∗(R, δ, α)) and Pk := Ptk(β∗(P, δ, α)) for the reward and transition functions\nrespectively. Let A = {R∗, Rk ∈Rk ∀k} and B = {P ∗, Pk ∈Pk ∀k} and condition upon\nthese events to give:\nE[Regret(T, πP S, M ∗)]\n≤\nE\n\" m\nX\nk=1\nτ\nX\ni=1\n\b\n|rk(xk,i) −r∗(xk,i)| + Kk∥pk(xk,i) −p∗(xk,i)∥2\n\t\n#\n≤\nm\nX\nk=1\nτ\nX\ni=1\n\b\nwRk(xk,i) + E[Kk|A, B]wPk(xk,i) + 8δ(CR + CP)\n\t\n(14)\nThe posterior sampling lemma ensures that E[Kk] = E[K∗] so that E[Kk|A, B] ≤\nE[K∗]\nP(A,B) ≤\nE[K∗]\n1−8δ by a union bound on {Ac ∪Bc}. We ﬁx δ = 1/8T to see that:\nE[Regret(T, πP S, M ∗)] ≤(CR + CP) +\nm\nX\nk=1\nτ\nX\ni=1\nwRk(xk,i) + E[K∗]\n\u0010\n1 +\n1\nT −1\n\u0011\nm\nX\nk=1\nτ\nX\ni=1\nwPt(xk,i)\nWe now use equation (7) together with Proposition 6 to obtain our regret bounds. For ease\nof notation we will write dE(R) = dimE(R, T −1) and dE(P) = dimE(P, T −1).\nE[Regret(T, πP S, M ∗)]\n≤\n2 + (CR + CP) + τ(CRdE(R) + CPdE(P)) +\n4\nq\nβ∗\nT (R, 1/8T, α)dE(R)T + 4\nq\nβ∗\nT (P, 1/8T, α)dE(P)T(15)\nWe let α = 1/T 2 and write nF = log(8N(F, 1/T 2, ∥· ∥2)T ) for R and P to complete our\nproof of Theorem 1:\nE[Regret(T, πP S, M ∗)] ≤\n\u0002\nCR + CP\n\u0003\n+ ˜D(R) + E[K∗]\n\u0012\n1 +\n1\nT −1\n\u0013\n˜D(P)\n(16)\nWhere\n˜D(F) is shorthand for 1 + τCFdE(F) + 8\nq\ndE(F)(4CF +\np\n2σ2\nF log(32T 3)) +\n8\np\n2σ2\nFnFdE(F)T. The ﬁrst term [CR + CP] bounds the contribution from missed con-\nﬁdence sets. The cost of learning the reward function R∗is bounded by ˜D(R). In most\nproblems the remaining contribution bounding transitions and lost future value will be\ndominant. Corollary 1 follows from the Deﬁnition 1 together with nR and nP.\n7\nConclusion\nWe present a new analysis of posterior sampling for reinforcement learning that leads to\na general regret bound in terms of the dimensionality, rather than the cardinality, of the\nunderlying MDP. These are the ﬁrst regret bounds for reinforcement learning in such a\ngeneral setting and provide new state of the art guarantees when specialized to several\nimportant problem settings. That said, there are a few clear shortcomings which we do not\naddress in the paper. First, we assume that it is possible to draw samples from the posterior\ndistribution exactly and in some cases this may require extensive computational eﬀort.\nSecond, we wonder whether it is possible to extend our analysis to learning in MDPs without\nepisodic resets. Finally, there is a fundamental hurdle to model-based reinforcement learning\nthat planning for the optimal policy even in a known MDP may be intractable. We assume\naccess to an approximate MDP planner, but this will generally require lengthy computations.\nWe would like to examine whether similar bounds are attainable in model-free learning\n[23], which may obviate complicated MDP planning, and examine the computational and\nstatistical eﬃciency tradeoﬀs between these methods.\nAcknowledgments\nOsband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc. This work\nwas supported in part by Award CMMI-0968707 from the National Science Foundation.\n8\nReferences\n[1] Apostolos Burnetas and Michael Katehakis. Optimal adaptive policies for Markov decision\nprocesses. Mathematics of Operations Research, 22(1):222–255, 1997.\n[2] Tze Leung Lai and Herbert Robbins. Asymptotically eﬃcient adaptive allocation rules. Ad-\nvances in applied mathematics, 6(1):4–22, 1985.\n[3] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A\nsurvey. arXiv preprint cs/9605103, 1996.\n[4] Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142,\n1984.\n[5] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold\nalgorithm. Machine learning, 2(4):285–318, 1988.\n[6] Lihong Li, Michael L Littman, Thomas J Walsh, and Alexander L Strehl. Knows what it\nknows: a framework for self-aware learning. Machine learning, 82(3):399–443, 2011.\n[7] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement\nlearning. The Journal of Machine Learning Research, 99:1563–1600, 2010.\n[8] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.\nMachine Learning, 49(2-3):209–232, 2002.\n[9] Ronen Brafman and Moshe Tennenholtz.\nR-max-a general polynomial time algorithm for\nnear-optimal reinforcement learning. The Journal of Machine Learning Research, 3:213–231,\n2003.\n[10] Alexander Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael Littman. Pac model-\nfree reinforcement learning. In Proceedings of the 23rd international conference on Machine\nlearning, pages 881–888. ACM, 2006.\n[11] Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) Eﬃcient Reinforcement Learning\nvia Posterior Sampling. Advances in Neural Information Processing Systems, 2013.\n[12] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-oﬀs. The Journal of\nMachine Learning Research, 3:397–422, 2003.\n[13] S´ebastien Bubeck, R´emi Munos, Gilles Stoltz, and Csaba Szepesv´ari. X-armed bandits. Journal\nof Machine Learning Research, 12:1587âĂŞ1627, 2011.\n[14] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. CoRR,\nabs/1301.2609, 2013.\n[15] Ian Osband and Benjamin Van Roy. Near-optimal regret bounds for reinforcement learning in\nfactored MDPs. arXiv preprint arXiv:1403.3741, 2014.\n[16] Yassin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari.\nImproved algorithms for linear\nstochastic bandits. Advances in Neural Information Processing Systems, 24, 2011.\n[17] Morteza Ibrahimi, Adel Javanmard, and Benjamin Van Roy. Eﬃcient reinforcement learning\nfor high dimensional linear quadratic systems. In NIPS, pages 2645–2653, 2012.\n[18] Ronald Ortner, Daniil Ryabko, et al. Online regret bounds for undiscounted continuous rein-\nforcement learning. In NIPS, pages 1772–1780, 2012.\n[19] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of opti-\nmistic exploration. In Advances in Neural Information Processing Systems, pages 2256–2264,\n2013.\n[20] William Thompson. On the likelihood that one unknown probability exceeds another in view\nof the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.\n[21] Malcom Strens. A Bayesian framework for reinforcement learning. In Proceedings of the 17th\nInternational Conference on Machine Learning, pages 943–950, 2000.\n[22] Dimitri Bertsekas. Dynamic programming and optimal control, volume 1. Athena Scientiﬁc\nBelmont, MA, 1995.\n[23] Benjamin Van Roy and Zheng Wen. Generalization and exploration via randomized value\nfunctions. arXiv preprint arXiv:1402.0635, 2014.\n9\nA\nConﬁdence sets with high probability\nIn this appendix we will build up to a proof of Proposition 5, that the conﬁdence sets deﬁned by β∗\nin equation 7 hold with high probability. We begin with some elementary results from martingale\ntheory.\nLemma 4 (Exponential Martingale).\nLet Zi ∈L1 be real-calued random variables adapted to Hi. We deﬁne the conditional mean µi =\nE[Zi|Hi−1] and conditional cumulant generating function ψi(λ) = log E[exp (λ(Zi −µi)) |Hi−1],\nthen\nMn(λ) = exp\n \nn\nX\n1\nλ(Zi −µi) −ψi(λ)\n!\nis a martingale with E[Mn(λ)] = 1.\nLemma 5 (Concentration Guarantee).\nFor Zi adapted real L1 random variables adapted to Hi. We deﬁne the conditional mean µi =\nE[Zi|Hi−1] and conditional cumulant generating function ψi(λ) = log E[exp (λ(Zi −µi)) |Hi−1].\nP\n ∞\n[\nn=1\n{\nn\nX\n1\nλ(Zi −µi) −ψi(λ) ≥x}\n!\n≤e−x\nBoth of these lemmas are available in earlier discussion for real-valued variables [14].\nWe now\nspecialize our discussion to the vector space Y ⊆Rd where the inner product < y, y >= ∥y∥2\n2. To\nsimplify notation we will write f ∗\nt := f ∗(xt) and ft = f(xt) for arbitrary f ∈F. We now deﬁne\nZt\n=\n∥f ∗\nt −yt∥2 −∥ft −yt∥2\n=\n< f ∗\nt −yt, f ∗\nt −yt > −< ft −yt, ft −yt, ft −yt >\n=\n−< ft −f ∗\nt , ft −f ∗\nt > +2 < ft −f ∗\nt , yt −f ∗\nt >\n=\n−∥ft −f ∗\nt ∥2 + 2 < ft −f ∗\nt , ǫt >\nso that clearly µt = −∥ft −f ∗\nt ∥2.\nNow since we have said that the noise is σ-sub-Gaussian,\nE[exp (< φ, ǫ >)] ≤exp\n\u0010\n∥φ∥2\n2σ2\n2\n\u0011\n∀φ ∈Y. From here we can deduce that:\nψt(λ)\n=\nlog E[exp (λ(Zt −µt)) |Ht−1]\n=\nlog E[exp(2λ < ft −f ∗\nt , ǫt >)]\n≤\n∥2λ(ft −f ∗\nt )∥2\n2σ2\n2\n.\nWe now write Pt−1\ni=1 Zi = L2,t(f ∗) −L2,t(f) according to our earlier deﬁnition of L2,t. We can\napply Lemma 5 with λ = 1/4σ2, x = log(1/δ) to obtain:\nP{\n\u0010\nL2,t(f) ≥L2,t(f ∗) + 1\n2∥f −f ∗∥2,Et −4σ2 log(1/δ)\n\u0011\n∀t} ≥1 −δ\nsubstituting f = ˆf to be the least squares solution which minimizes L2,t(f) we can remove L2,t( ˆf)−\nL2,t(f ∗) ≤0.\nFrom here we use an α-cover discretization argument to complete the proof of\nProposition 5.\nLet Fα ⊂F be an α-2 cover of F such that ∀f ∈F there is some ∥f α −f∥2 ≤α. We can use a\nunion bound on Fα so that ∀f ∈F:\nL2,t(f) −L2,t(f ∗) ≥1\n2∥f −f ∗∥2,Et −4σ2 log(N(F, α, ∥· ∥2)/δ) + DE(α)\n(17)\nFor DE(α) =\nmin\nfα∈Fα\nn1\n2∥f α −f ∗∥2\n2,Et −1\n2∥f −f ∗∥2\n2,Et + L2,t(f) −L2,t(f α)\no\nWe will now seek to bound this discretization error with high probability.\nLemma 6 (Bounding discretization error).\nIf ∥f α(x) −f(x)∥2 ≤α for all x ∈X then with probability at least 1 −δ:\nDE(α) ≤αt\nh\n8C +\np\n8σ2 log(4t2/δ)\ni\n10\nProof. For non-trivial bounds we will consider the case of α ≤C and note that via Cauchy-Schwarz:\n∥f α(x)∥2\n2 −∥f(x)∥2\n2 ≤max\n∥y∥2≤α ∥f(x) + y∥2\n2 −∥f∥2\n2 ≤2Cα + α2.\nFrom here we can say that\n∥f α(x) −f ∗(x)∥2\n2 −∥f(x) −f ∗(x)∥2\n2 = ∥f α(x)∥2\n2 −∥f(x)∥2\n2 + 2 < f ∗(x), f(x) −f α(x) >≤4Cα\n∥y −f(x)∥2\n2 −∥y −f α(x)∥2\n2 = 2 < y, f α(x) −f(x) > +∥f(x)∥2\n2 −∥f α(x)∥2\n2 ≤2α|y| + 2Cα + α2\nSumming these expressions over time i = 1, .., t −1 and using sub-gaussian high probability bounds\non |y| gives our desired result.\nFinally we apply Lemma 6 to equation 17 and use the fact that ˆf LS\nt\nis the L2,t minimizer to obtain\nthe result that with probability at least 1 −2δ:\n∥ˆf LS\nt\n−f ∗∥2,Et ≤\np\nβ∗\nt (F, α, δ)\nWhich is our desired result.\nB\nBounding the number of large widths\nLemma 1 (Bounding the number of large widths).\nIf {βt > 0\n\f\ft ∈N} is a nondecreasing sequence with Ft = Ft(βt) then\nm\nX\nk=1\nτ\nX\ni=1\n1{wFtk (xtk+i) > ǫ} ≤\n\u00104βT\nǫ2\n+ τ\n\u0011\ndimE(F, ǫ)\nProof. We ﬁrst imagine that wFt(xt) > ǫ and is ǫ-dependent on K disjoint subsequences of\nx1, .., xt−1. If xt is ǫ-dependent on K disjoint subsequences then there exist ∥f −f∥2,Et > Kǫ2. By\nthe triangle inequality ∥f −f∥2,Et ≤2√βt ≤2√βT so that K < 4βT /ǫ2.\nIn the case without episodic delay, Russo went on to show that in any sequence of length l there is\nsome element which is ǫ-dependent on at least\nl\ndimE(F,ǫ) −1 disjoint subsequences [14]. Our analysis\nfollows similarly, but we may lose up to τ −1 proper subsequences due to the delay in updating the\nepisode. This means that we can only say that K ≥\nl\ndimE(F,ǫ) −τ. Considering the subsequence\nwFtk (xtk+i) > ǫ we see that l ≤\u0000 4βT\nǫ2 + τ\u0001\ndimE(F, ǫ) as required.\nC\nEluder dimension for speciﬁc function classes\nIn this section of the appendix we will provide bounds upon the eluder dimension for some canonical\nfunction classes. Recalling Deﬁnition 3, dimE(F, ǫ) is the length d of the longest sequence x1, .., xd\nsuch that for some ǫ′ ≥ǫ:\nwk = sup\n\u001a\n∥(f −f)(xk)∥2\n\f\f\f\f ∥f −f∥2,Et ≤ǫ′\n\u001b\n> ǫ′\n(18)\nfor each k ≤d.\nC.1\nFinite domain X\nAny x ∈X is ǫ-dependent upon itself for all ǫ > 0. Therefore for all ǫ > 0 the eluder dimension of\nF is bounded by |X |.\nC.2\nLinear functions f(x) = θφ(x)\nLet F = {f |f(x) = θφ(x) for θ ∈Rn×p, φ ∈Rp, ∥θ∥2 ≤Cθ, ∥φ∥2 ≤Cφ}. To simplify our notation\nwe will write φk = φ(xk) and θ = θ1 −θ2. From here, we may manipulate the expression\n∥θφ∥2\n2 = φT\nk θT θφk = Tr(φT\nk θT θφk) = Tr(θφkφT\nk θ)\n=⇒wk = sup\nθ\n{∥θφk∥2\n\f\fTr(θΦkθT ) ≤ǫ2} where Φk :=\nk−1\nX\ni=1\nφiφT\ni\nWe next require a lemma which gives an upper bound for trace constrained optimizations.\n11\nLemma 7 (Bounding norms under trace constraints).\nLet θ ∈Rn×p, φ ∈Rp and V ∈Rp×p\n++ , the set of positive deﬁnite p × p matrices, then:\nW 2 = max\nθ\n∥θφ∥2\n2 subject to Tr(θV θT ) ≤ǫ2\nis bounded above by W 2 ≤(2n −1)ǫ2∥φ∥2\nV −1 where ∥φ∥2\nA := φT Aφ.\nProof. We ﬁrst note that ∥θφ∥2\n2 = Tr(θφφTθT ) = Pn\n1 (θφ)2\ni ≤\u0000Pn\n1 (θφ)i\n\u00012 by Jensen’s inequality.\nWe deﬁne ˜Φ ∈Rn×p such that each row of ˜Φ = φT . Then this inequality can be expressed as:\nW 2 = Tr(θφφTθT ) ≤Sum(θ ⊗˜Φ)2\nWhere A ⊗B = C for Cij = AijBij and Sum(C) := P\ni,j Cij We can now obtain an upper bound\nfor our original problem through this convex relaxation:\nmax\nθ\nSum(θ ⊗˜Φ) subject to Tr(θV θT ) ≤ǫ2\nWe can now form the lagrangian L(θ, λ) = −Sum(θ ⊗˜Φ) + λ(Tr(θV θT ) −ǫ2). Solving for ﬁrst\norder optimality ▽θL = 0 =⇒θ∗=\n1\n2λ ˜ΦV −1. From here we form the dual objective\ng(λ) = −Sum( 1\n2λ\n˜ΦV −1 ⊗˜Φ) + Tr( 1\n4λ\n˜ΦV −1 ˜ΦT ) −λǫ2\nHere we solve\nfor\nthe dual-optimal λ∗\n▽λg\n=\n0\n=⇒\n1\n2λ∗\n2Sum( 1\n2λ ˜ΦV −1 ⊗˜Φ) −\n1\n4λ∗\n2Tr( 1\n4λ ˜ΦV −1 ˜ΦT ) = ǫ2.\nFrom the deﬁnition of ˜Φ, Sum(˜ΦV −1 ⊗˜Φ) = nφT V −1φ and\nTr(˜ΦV −1 ˜ΦT ) = φT V −1φ. From this we can simplify our expression to conclude:\nn\n2λ∗2 φT V −1φ −\n1\n4λ∗2 φT V −1φ = ǫ2 =⇒λ∗=\nr\n(n −1/2)\n2ǫ2\n∥φ∥V −1\n=⇒g(λ∗) = −n\n2λ∗∥φ∥2\nV −1 +\n1\n4λ∗∥φ∥2\nV −1 −λ∗ǫ\nstrong duality\n=⇒f(θ∗) = g(λ∗) =\n√\n2n −1ǫ∥φ∥V −1\nFrom here we conclude that the optimal value of W 2 ≤f(θ∗)2 ≤(2n −1)ǫ2∥φ∥2\nV −1.\nUsing this lemma, we will be able to address the eluder dimension for linear functions. Using the\ndeﬁnition of wk from equation 18 together with Φk we may rewrite:\nwk = max\nθ {\np\nTr(θφkφT\nk θ)\n\f\f Tr(θΦkθT ) ≤ǫ2}.\nLet Vk := Φk +\n\u0010\nǫ\n2Cθ\n\u00112\nI so that Tr(θΦkθT ) ≤ǫ2\n=⇒\nTr(θVkθT ) ≤2ǫ2 through a triangle\ninequality. Now applying Lemma 7 we can say that wk ≤ǫ√4n −2∥φk∥V −1\nk\n. This means that if\nwk ≥ǫ then ∥φk∥2\nV −1\nk\n>\n1\n4n−2 > 0.\nWe now imagine that wi ≥ǫ for each i < k. Then since Vk = Vk−1 + φkφT\nk we can use the Matrix\nDeterminant together with the above observation to say that:\ndet(Vk) = det(Vk−1)(1 + φT\nk V −1\nK φk) ≥det(Vk−1)\n\u0010\n1 +\n1\n4n −2\n\u0011\n≥.. ≥λp \u0010\n1 +\n1\n4n −2\n\u0011k−1\n(19)\nfor λ :=\n\u0010\nǫ\n2Cθ\n\u00112\n. To get an upper bound on the determinant we note that det(Vk) is maximized\nwhen all eigenvalues are equal or equivalently:\ndet(Vk) ≤\n\u0012\nTr(Vk)\np\n\u0013p\n≤\n\u0012C2\nφ(k −1)\np\n+ λ\n\u0013p\n(20)\nNow\nusing\nequations\n19\nand\n20\ntogether\nwe\nsee\nthat\nk\nmust\nsatistfy\nthe\ninequality\n\u00001 +\n1\n4n−2\n\u0001(k−1)/p ≤\nC2\nφ(k−1)\nλp\n+ 1. We now write ζ0 =\n1\n4n−2 and α0 =\nC2\nφ\nλ =\n\u0010\n2CφCθ\nǫ\n\u00112\nso that we\ncan epress this as:\n(1 + ζ0)\nk−1\np\n≤α0 k −1\np\n+ 1\n12\nWe now use the result that B(x, α) = max{B\n\f\f (1+x)B ≤αB+1} ≤1+x\nx\ne\ne−1{log(1+α)+log( 1+x\nx )}.\nWe complete our proof of Proposition 2 through computing this upper bound at (ζ0, α0),\ndimE(F, ǫ) ≤p(4n −1)\ne\ne −1 log\n\u0014\u0012\n1 +\n\u00102CφCθ\nǫ\n\u00112\u0013\n(4n −1)\n\u0015\n+ 1 = ˜O(np).\nC.3\nQuadratic functions f(x) = φT (x)θφ(x)\nLet F = {f |f(x) = φ(x)T θφ(x) for θ ∈Rp×p, φ ∈Rp, ∥θ∥2 ≤Cθ, ∥φ∥2 ≤Cφ} then for any X we\ncan say that:\ndimE(F, ǫ) ≤p(4p −1)\ne\ne −1 log\n\" \n1 +\n\u00122pC2\nφCθ\nǫ\n\u00132!\n(4p −1)\n#\n+ 1 = ˜O(p2).\nWhere we have simply applied the linear result with ˜ǫ =\nǫ\npCP . This is valid since if we can identify\nthe linear function g(x) = θφ(x) to within this tolerance then we will certainly know f(x) as well.\nC.4\nGeneralized linear models\nLet g(·) be a component-wise independent function on Rn with derivative in each component\nbounded ∈[h, h] with h > 0. Deﬁne r = h\nh > 1 to be the condition number. If F = {f |f(x) =\ng(θφ(x)) for θ ∈Rn×p, φ ∈Rp, ∥θ∥2 ≤Cθ, ∥φ∥2 ≤Cφ} then for any X :\ndimE(F, ǫ) ≤p\u0000r2(4n −2) + 1\u0001\ne\ne −1\n\u0012\nlog\n\u0014\u0000r2(4n −2) + 1\u0001 \u0012\n1 +\n\u00102CθCφ\nǫ\n\u00112\u0013\u0015\u0013\n+1 = ˜O(r2np)\nThis proof follows exactly as per the linear case, but ﬁrst using a simple reduction on the form of\nequation (18).\nwk\n=\nsup\n\u001a\n∥(f −f)(xk)∥2\n\f\f\f\f ∥f −f∥2,Et ≤ǫ′\n\u001b\n≤\nmax\nθ1,θ2\n(\n∥g(θ1φk) −g(θ2φk)∥2\n\f\f\nk−1\nX\ni=1\n∥g(θ1φi) −g(θ2φi)∥2\n2 ≤ǫ2\n)\n≤\nmax\nθ\n(\nh∥θφk∥2\n\f\f\nk−1\nX\ni=1\nh2∥θφi∥2\n2 ≤ǫ2\n)\nTo which we can now apply Lemma 7 with the ǫ rescaled by r. Following the same arguments as\nfor linear functions now completes our proof.\nD\nBounded LQR control\nWe imagine a standard linear quadratic controller with rewards with x = (s, a) the state-action\nvector. The rewards and transitions are given by:\nR(x) = −xT Ax + ǫR , P(x) = ΠC(Bx + ǫP ),\nwhere A ≽0 is positive semi-deﬁnite and ΠC projects x onto the ∥· ∥2-ball at radius C.\nIn the case of unbounded states and actions the Ricatti equations give the form of the optimal\nvalue function V (s) = −sT Qs for Q ≽0. In this case we can see that the diﬀerence in values of\ntwo states:\n|V (s) −V (s′)| = | −sT Qs + s′T Qs′| = | −(s + s′)T Q(s −s′)| ≤2Cλ1∥s −s′∥2\nwhere λ1 is the largest eigenvalue of Q and C is an upper bound on the ∥· ∥2-norm of both s and\ns′. We note that 2Cλ1 works as an eﬀective Lipshcitz constant when we know what C can bound\ns, s′.\nWe observe that for any projection ΠC(x) = αx for α ∈(0, 1] and that for all positive semi-deﬁnite\nmatrices A ≽0, (αx)T A(αx) = α2xT Ax ≤xT Ax. Using this observation together with reward\nand transition functions we can see that the value function of the bounded LQR system is always\ngreater than or equal to that of the unconstrained value function. The eﬀect of excluding the low-\nreward outer region, but maintaining the higher-reward inner region means that the value function\nbecomes more ﬂat in the bounded case, and so 2Cλ1 works as an eﬀective Lipschitz constant for\nthis problem too.\n13\nE\nUCRL-Eluder\nFor completeness, we explicitly outline an optimistic algorithm which uses the conﬁdence sets in\nour analysis of PSRL to guarantee similar regret bounds with high probability over all MDP M ∗.\nThe algorithm follows the style of UCRL2 [7] so that at the start of the kth episode the algorithm\nform Mk = {M|P M ∈Pk, RM ∈Rk} and then solves for the optimistic policy that attains the\nhighest reward over any M in Mk.\nAlgorithm 2\nUCRL-Eluder\n1: Input:\nConﬁdence parameter δ > 0, t=1\n2: for episodes k = 1, 2, .. do\n3:\nform conﬁdence sets Rk(β∗(R, δ, 1/k2)) and Pk(β∗(P, δ, 1/k2))\n4:\ncompute µk optimistic policy over Mk = {M|P M ∈Pk, RM ∈Rk}\n5:\nfor timesteps j = 1, .., τ do\n6:\napply at ∼µk(st, j)\n7:\nobserve rt and st+1\n8:\nadvance t = t + 1\n9:\nend for\n10: end for\nGenerally, step 4 of this algorithm with not be computationally tractable even when solving for µM\nis possible for a given M.\n14\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2014-06-07",
  "updated": "2014-10-31"
}