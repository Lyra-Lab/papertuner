{
  "id": "http://arxiv.org/abs/1407.2989v1",
  "title": "Hidden Markov Model Based Part of Speech Tagger for Sinhala Language",
  "authors": [
    "A. J. P. M. P. Jayaweera",
    "N. G. J. Dias"
  ],
  "abstract": "In this paper we present a fundamental lexical semantics of Sinhala language\nand a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala\nlanguage. In any Natural Language processing task, Part of Speech is a very\nvital topic, which involves analysing of the construction, behaviour and the\ndynamics of the language, which the knowledge could utilized in computational\nlinguistics analysis and automation applications. Though Sinhala is a\nmorphologically rich and agglutinative language, in which words are inflected\nwith various grammatical features, tagging is very essential for further\nanalysis of the language. Our research is based on statistical based approach,\nin which the tagging process is done by computing the tag sequence probability\nand the word-likelihood probability from the given corpus, where the linguistic\nknowledge is automatically extracted from the annotated corpus. The current\ntagger could reach more than 90% of accuracy for known words.",
  "text": "International Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n10.5121/ijnlc.2014.3302                                                                                                                                   9 \n \n \nHIDDEN MARKOV MODEL BASED PART OF SPEECH \nTAGGER FOR SINHALA LANGUAGE  \n \nA.J.P.M.P. Jayaweera1 and N.G.J. Dias2 \n \n1 Virtusa (Pvt.) Ltd, No 752, Dr. Danister De Silva Mawatha, Colombo 09, Sri Lanka  \n2Department of Statistics & Computer Science, University of Kelaniaya,  \nKelaniya, Sri Lanka \nABSTRACT \n \nIn this paper we present a fundamental lexical semantics of Sinhala language and a Hidden Markov Model \n(HMM) based Part of Speech (POS) Tagger for Sinhala language. In any Natural Language processing \ntask, Part of Speech is a very vital topic, which involves analysing of  the construction, behaviour and the \ndynamics of the language, which the knowledge could utilized in computational linguistics analysis and \nautomation applications. Though Sinhala is a morphologically rich and agglutinative language, in which \nwords are inflected with various grammatical features, tagging is very essential for further analysis of the \nlanguage. Our research is based on statistical based approach, in which the tagging process is done by \ncomputing the tag sequence probability and the word-likelihood probability from the given corpus, where \nthe linguistic knowledge is automatically extracted from the annotated corpus. The current tagger could \nreach more than 90% of accuracy for known words.  \n \nKEYWORDS \n \nPart of Speech tagging, Morphology, Natural Language Processing, Hidden Markov Model, Stochastic \nbased tagging \n \n1. INTRODUCTION \n \nAccording to figures from UNESCO (The United Nations’ Educational, Scientific and Cultural \nOrganization), there are around 6900 spoken languages are exist in this world, only 20 languages \nare spoken by 50% of the world population. Each of these languages are spoken by more than 50 \nmillion speakers. Most of the world population speaks Chinese Mandarin and that is spoken by \naround 1000 million people. Spanish, English, Hindi, Arabic, Portuguese and Russian   are other \ntop most languages spoken by the largest population in this world, and each language is spoken \nby 200 million speakers or more. People who speak those top most languages are spread across \ndifferent geographical regions in multiple countries. Also 50% of the languages are endangered \nand most of them are spoken by small communities and they are always limited to a specific \ngeographical region [1, 2, 3].  \n \nSinhala is also one unique language that speaks only by people in Sri Lanka and more than 17 \nmillion speakers speak Sinhala as their mother tongue. We believe that Sinhala is not an \nendangered language yet, though speakers are limited only to a small geographical region. But we \nthink our mother language need more attention, and need to get more provision to develop the \nlanguage with latest technology trends. So our effort here is to address one pitfall that we have \nidentified in area of computational linguistics and Natural Language Processing (NLP) related to \nSinhala language. \n \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n10 \n \nThough research on NLP, has taken giant leap in the last two decades with the advent of efficient \nmachine learning algorithms and the creation of large annotated corpora for various languages, \nonly few languages in the world have the advantage of having enough lexical resources, such as \nEnglish. NLP researches for Sinhala are still far behind than other South Asian languages. Further \nwe have very limited lexical resources available for Sinhala language. Researches on NLP for \nSinhala language can be pushed by creation of required lexical resources and tools.  So, the \nattempt of this research is to develop a Part of speech Tagger for Sinhala language, which is a \nfundamental need for further computational linguistic analysis for our mother language.  \n \nSinhala is a complex language, morphologically rich and agglutinative in nature, words of which \nare inflected with various grammatical features. Sinhala root noun (lemma) inflects for plural and \nsingular and Sinhala verb specifies almost everything like gender, number and person markings, \nand represents the tense of the activity. \n \nPOS tagging is a well-studied problem in the field of NLP and one of the fundamental processing \nstep for any language in NLP and language automation, i.e., the capability of a computer to \nautomatically POS tag a given sentence. Throughout the history of NLP, different approaches \nhave already been tried out to automate the task of POS tagging of languages such as English, \nGerman, Chinese and few South Asian languages such as Hindi, Tamil and Bengali. \nWords are the fundamental building block of a language. Every human language, spoken, signed \nor written is composed of words [7]. Every area of speech and language processing, from speech \nrecognition to machine translation, text to speech, spelling and grammar checking to language-\nbased information retrieval on the Web, requires extensive knowledge about words that are \nheavily based on the lexical knowledge. In contrast to other data processing systems, language \nprocessing applications use knowledge of the language. \n \nThe basic processing step in tagging consists of assigning POS tags to every token in the text with \na corresponding POS tag like noun, verb, preposition, etc., based both on its definition, as well as \nits context. The number of part of speech tags in a tagger may vary depending on the information \none wants to capture [7]. \n \nIn this paper, we present a fundamental lexical and morphological analysis of Sinhala language, \ntheory of Hidden Markov Model and the algorithm of the implementation. Section 2 of this paper \ngives an idea of history and previous research on NLP and section 3 discusses previous work on \nSinhala language.  Section 4 and 5 give a comprehensive lexical and morphological analysis of \nSinhala language. Section 6 and 7 give details about available lexical resources which we use in \nthis research. Section 8 and 9 describe POS tagging and the Hidden Markov Model \nimplementation algorithm. Section 10 and 11 discuss the Evaluation, testing and the result, and \nsection 12 concludes the paper and describes the future work. \n \n2. PREVIOUS WORK ON NLP \n \nNatural language processing history started from Shanon (1948), Kleen (1951) then Chomsky \n(1956) to Harris (1959), they contributed a lot in early 1950s to formulate the basic concepts and \nprinciples of language processing. In the last 50 years of research in language processing, various \nkinds of knowledge had been captured through the use of small number of formal models or \ntheories. Most of these models and theories are all extracted from the standard toolkit of \nComputer Science, Mathematics and Linguistics. Among the most important elements in these \ntoolkits are state machine, formal rules system, logic as well as probability theory and other \nmachine learning tools [7]. But in the last decade, probabilistic and data-driven models had \nbecome quite standard throughout the natural language processing. \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n11 \n \nFor English, there are many POS taggers available: employing machine learning techniques \n(based on Hidden Markov Models [15]), transformation based error driven learning [10], decision \ntrees [9] and maximum entropy methods [6]. There are some taggers which are hybrid using both \nstochastic and rule-based approaches. Most of the POS taggers have reached a success, between \n92-97 % accuracy. However, these accuracies are aided by the availability of large annotated \ncorpus for English. Further there are few Tagging systems available for South Asian languages \nlike Hindi, Tamil and Bengali [8, 12, 13, 14]. In 2006, a POS tagger was proposed for Hindi, \nwhich uses an annotated corpus of 15,562 words and a decision tree based learning algorithm. \nThey reached an accuracy of 93.45% with a tag set of 23 POS tags [14]. For Bengali, a tagger was \ndeveloped using a corpus based semi-supervised learning algorithm based on HMMs [13]. \n \n3. PREVIOUS WORK ON SINHALA NLP ANALYSIS \n \nThere were some important language analysis work has done for Sinhala language, and created a \nTag set [16] and a corpus of one million words [17], which was an important initiative, that gives \na substantial influence to perform NLP research on Sinhala language. But unfortunately, the \nprogress of computational linguistic analysis on Sinhala language is far behind than other \nlanguages. According to our knowledge, there is no well-known automated POS tagging system \navailable for Sinhala language. \n \n4. MORPHOLOGY IN SINHALA LANGUAGE \n \nSinhala is morphologically rich and agglutinative language, in which root words are inflected in \ndifferent contexts. In Sinhala, words are defined as written stream of letters forming a sensible \nunderstanding to a person that denotes or relation to the physical world or to an abstract concept.  \n \nBasic building blocks of Sinhala words are also sound units not the letters, same as English \nlanguage, which distinguish two broad classes of morphemes: lemma  and affixes . The lemma \n(stem) is the “main” morpheme of the word, supplying the main meaning, while the affixes add \n“additional” meaning of various kinds. Often Sinhala words are postpositionally inflected with \nvarious grammatical features.  Sinhala verb inflects to specifying almost everything like gender, \nsingularity or plurality, person markings and represents the tense. Sinhala nouns inflect and \nspecifying singularity or plurality, gender, person marking and case of the noun [18]. \n \nAccording to tradition, below are four main types of words exist in Sinhala language [4, 5]: \n \n1. Noun - kdu mo.  \n2. Verb - ls%hd mo. \n3. Upasarga – Wmi¾. mo  (no direct matching with English grammar) \n4. Nipatha – ksmd; mo (no direct matching with English grammar) \n \n5. SINHALA WORD CLASSES \n \nTraditionally the definition of POS has been based on morphological and syntactic functions [7]. \nSimilar to most of other languages, POS in Sinhala language also can be divided into two broad \ncategories: closed class type and open class type. Closed classes are those that have relatively \nfixed membership. Closed class words are generally function words: which tend to be very short, \noccur frequently, and play an important role in grammar.  By contrast open class is the type that \nlager numbers of words are belongs in any language, and new words are continually coined or \nborrowed from other languages. The words that are usually containing main content of a sentence \nare belonged to open word class category. \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n12 \n \n \nIn Sinhala, all Nouns and Verbs can be categorized under open word class. But Nipatha and \nUpasarga behave differently in Sinhala grammar. Words belong to Nipatha and Upasarga are not \nchanged according to time and gender, Upasarga always join with nouns and provide additional \n(improve) meaning to the noun, therefore, Upasarga are not categorized under any of word \nclasses, but Nipatha can be categorized as closed class words based on their existence. \nIn-addition to that, Sinhala Pronouns also can be classified as open class words, based on their \nmorphological properties, but also Pronouns can be classified as closed class words, based on \ntheir existence of fixed membership in the language. Sinhala Pronouns are forms of noun \ncommonly referring to person, place or things [11]. \n \n6. POS TAG SET FOR SINHALA LANGUAGE \n \nIn Table I, presents the Tag set defined for Sinhala language, which was developed by UCSC \nunder PAN Localization project in 2005 [16], and this tag set contains 26 tags which are mostly \nbased on morphological and syntactical features of Sinhala language. Currently this is the only \ntag set available for Sinhala Language, and we use this tag set in our research. \n \nHowever, there are few issues that the authors have encountered during the process of defining \nthe tag set, based on the syntactical complexity of Sinhala Language [16]: \n \n1. Separation of Participle1 and Post-positions2. \n2. Separation of Compound Nouns - Combination of multiple nouns act as a single noun. \n3. Multiword - Certain word combination/phrases can function as one grammatical \ncategory. \nTable 1 Sinhala Tag Set \n \n \nTag \nDescription \n1 \nNNR \nCommon Noun Root \n2 \nNNM \nCommon Noun Masculine \n3 \nNNF \nCommon Noun Feminine \n4 \nNNN  \nCommon Noun Neuter \n5 \nNNPA  \nProper Noun Animate \n6 \nNNPI  \nProper Noun Inanimate \n7 \nPRPM  \nPronoun Masculine \n8 \nPRPF  \nPronoun Feminine \n9 \nPRPN  \nPronoun Neuter \n10 \nPRPC  \nPronoun Common \n11 \nQFNUM  \nNumber Quantifier \n12 \nDET  \nDeterminer \n13 \nJJ  \nAdjective \n14 \nRB  \nAdverb \n15 \nRP  \nParticle \n                                                 \n1 Particle   is a word that resembles a preposition. \n \n2 By definition a post-position follows a noun or a noun phrase. \n \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n13 \n \n \nTag \nDescription \n16 \nVFM  \nVerb Finite Main \n17 \nVNF  \nVerb Non Finite \n18 \nVP \nVerb Ptharticiple  \n19 \nVNN  \nVerbal Non Finite Noun \n20 \nPOST  \nPostpositions \n21 \nCC  \nConjunctions \n22 \nNVB  \nNoun in Kriya Mula \n23 \nJVB  \nAdjective in Kriya Mula \n24 \nUH  \nInterjection \n25 \nFRW  \nForeign Word \n26 \nSYM  \nNot Classified \n \n7. SINHALA TEXT CORPUS \n \nCorpus is also an important lexical resource in the field of NLP. In this research we use the Beta \nversion of the Corpus developed by the UCSC under PAN Localization project in 2005 [17], \nwhich contains around 650 000 words and out of which 70000 distinct words, that comprise of \ndata drawn from different kinds of Sinhala newspaper articles. \n \n8. POS TAGGING \n \nPart-of-speech tagging is the process of assigning a part-of-speech or other lexical class marker to \neach word in a sentence [7]. The input to a tagging algorithm is a string of words and a tag set. \nThe output is a single best tag for each word. For example, here is a sample sentence from Sinhala \nText Corpus of a news report about “Silsamadana on a Wesak poya day” which each word tagged \nwith mapping tag using the tag set defined in Table I. \n \nExample: වෙසක්_NNPI ව ෝය_NNN නිමිත්වෙන්_POST මැයි_NNPI 2_NUM ෙැනි_QFNUM \nදා_NNN \n ැෙැති_VP \nශීල_JJ \nව්‍යා ාරයට_NNN \nද_RP \nවදසීයක්_QFNUM \n මණ_RP \nපිරිසක්_NNM සහභාගි_NVB වූහ_VFM ._. [Refer the Sinhala glossary for meaning of Sinhala \nwords] \n \nSinhala is a morphologically rich and agglutinative language, which words are made up of lexical \nroots combined with affixes or prefixes. So automatically assigning a tag to each word in a \nlanguage like Sinhala is very complex. The main challenge in Sinhala POS tagging is solving the \ncomplexity of words. Ambiguity is also adding some complexity in the process of tagging, but \nfortunately most words in Sinhala are unambiguous. The example below shows how a word can \nbe ambiguous in Sinhala language. \n \nAmbiguity is the existence of more than one possible usage of POS in different context. The noun \nbínd (ibba) containing two meanings tortoise and padlock bear intimateness and inanimateness \naccording to the context. The problem of POS tagging is to resolve these ambiguities, choosing \nthe proper tag for the context, not for the word. So this can be resolved by looking at the \nassociated words with the word. \n \n \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n14 \n \nExample: \nbífnd_NNPA fokAfklA bkAkjd (There are two tortoises). \nbífnd_NNPI fokAfklA od,d ;shkjd (Two padlocks have been fixed). \n \nMost of tagging algorithms fall into one of the two classes: rule-based taggers and stochastic \ntaggers [7]. Rule-based taggers generally involve a large database of hand written disambiguation \nrules. Stochastic tagger generally resolves tagging ambiguities by using a training corpus to \ncompute the probability of a given word having a given tag in a given context. In addition to that, \nthere are taggers, which use a hybrid approach, which employees both of the above methods to \nresolve the tagging ambiguity, which is called transformation-based taggers or Brill taggers [7].  \nUnder this research we have tried out the applicability of Stochastic based tagging approach for \nSinhala language. \n \n9. OUR APPROACH \n \nWe next describe the approach and the overall application architecture defined for Sinhala POS \ntagger in this Research.  To find a suitable tagging approach for Sinhala language, we analysed \nmultiple approaches that has already been discussed for other morphologically rich languages and \ndecided to use a well-known stochastic based approach which is known as Hidden Markov Model \nthat has proven evidence of better results for other languages. Probability is the basic principle \nbehind HMM. The model described here follows the concepts given in the reference [7]. \n \nThe intuition behind all stochastic taggers is simple generalization of the “pick the most-likely tag \nfor this word”. For a given sentence or a word sequence, HMM tagger chooses the tag sequence \nthat maximizes: \n \nP (word | tag) *  P (tag | previous n tags). \n \nHMM tagger generally chooses a tag sequence for a given sentence rather than for a single word. \nThis approach assumes that we are trying to compute the most probable tag sequence\n of tags T \n= (t1,t2,…,tn) for a given sequence of words in the sentence W = (w1,w2,…,wn): \n \n \nwhere,   \n \nis the set of values of t for which P(T|W) attains its maximum value. \nBy Bayes law, P (T|W) can be expressed as \n \n \n \nSo we choose the sequence of tags that gives \n \n \nwhere, \n, \n, \n. \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n15 \n \n \n \nis the set of values of t for which (P(T)P(W|T)/P(W)) attains its maximum value. \n \nSince we are looking for the most likely tag sequence for a sentence given a particular word \nsequence, the probability of the word sequence P (W) will be same for each tag sequence and we \ncan ignore it. So we get \n \n \nwhere, P(T) is the Prior probability and P(W|T) is the Likelihood probability. \n \nFrom the chain rule of probability, we get  \n \n \n \nBut for a long sequence of words, calculating probabilities like P (wi|w1t1…wi-1ti-1ti)*P \n(ti|w1t1…wi-1ti-1) is not an easy task, there is no easy way to calculate probability for selecting tag \nto a word given a long sequence of preceding words. We could solve this problem by making \nuseful simplification: we approximate the probability of a word given all previous words. The \nprobability of the word given the single previous word called bigram model. Bigram model \napproximates the probability of a word given all the previous words by the conditional probability \nof the preceding word. \n \nThis assumption that the probability of a word depends only on the previous words is called \nMarkov assumption. Markov models are the class of probabilistic models that assume that we can \npredict the probability of some future unit without looking too far into the past. We can generalize \nthe bigram to the trigram which looks two words into the past [7]. \n \nIn practice, trigram model is always used in NLP applications. So that let us define the \nsimplifying assumptions for this scenario. \n \nFirst make the assumption that the probability of a word depends only on its tag, i.e., \n \n \nNext, we make the assumption that the tag history can be approximated by the most recent two \ntags \n \nFrom (1), (2) and (3), we get \n \n \nThus, the best tag sequence can be choose, so that it maximize \n \n \n(1) \n(2) \n(3) \n. \n. \n, \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n16 \n \n \nNow, as usual we can use maximum likelihood estimation from relative frequency to compute \nthese probabilities. We use corpus to find  counts of  tag sequences of tags ti-2,ti-1,ti and  tags ti-2,ti-\n1, where ti is the tag i and ti-1,ti-2 are previous two tags, and count of witi, where wi is the word i and \nti is the tag assigned to word i. \n \nWe compute the probabilities \n \nand \n \nfor all wi , where 1≤i≤n.  \n \n9.1 The Algorithm \n \nThe algorithm explains below is based on the Viterbi Algorithm [7], which is widely used in the \nNLP applications, that allows considering all the words in the given sentence simultaneously and \ncomputes the most likely tag sequence. More formally, the algorithm searches for the best tag \nsequence  for given an observation sequence W = (w1,w2,…,wn) based on the text corpus. Each \ncell viterbi[t,i] (a two dimension array with i*j elements) of the matrix contains the probability of \nthe path which contains the probability for the first t observations ends in state i. This is the most-\nprobable path out of all possible sequence of the tags of length t-1.  \n \nThe algorithm sets up a probability matrix, with one column for each observation index (t) and \none row for each state (i) in the state graph. The algorithm first creates t+2 columns. The first \ncolumn is the initial observation, which is the start of the sequence, then next corresponds to the \nfirst observation, and so on. Then begin with the first column by setting the probability of the start \nto 1, and other probabilities to 0. For each column of the matrix, that is, for each time index t, \neach cell viterbi[t,j] will contain the probability of the most likely path to end in that cell j. We \ncalculate this probability recursively, by maximizing over the probability of the coming from all \npossible preceding states. Then we move to the next state; for each of the state i, viterbi[0,i] in \ncolumn 0, then compute the probability of moving into each of the cell j   viterbi[1,j] in column 1, \nand finally, the probability for the best path will appear in the final column. Finally back tracing \ncan be done to find the path that gives the best possible tag sequence. \n \n9.2 Overall Application Architecture and the Design \n \nFigure 1 shows the overall architecture of the proposed tagger, which is a two-step process that \nfirst runs through the tagged corpus and extract the linguistic knowledge. Then it runs through the \nrow text inputs and generating the best tag sequence for the sequence of input words based on the \nknowledge that gathered from the corpus. \n \nLexical Parser: Checks boundary conditions of each sentences and words as defined in the \nlexical rules, and prepare for Tokenizing and Pre-processing. \nTokenization: Run through the tagged corpus, separate out the words and tags, prepare for \nprobability calculation. \n \nProbability Calculation:  Calculate the Transition probability and the observation likelihood \nprobability for each pairs of Words, Tag sequences in the corpus as explained in section 9.3 \nbelow. \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n17 \n \n \nViterbi Matrix Analyzer: Prepare a state graph that has all possible state transitions for the \ngiven text input, calculate and assign state transition probability for each transition in the matrix, \nas explained in section A. \n \nTag Sequence Analyzer: Back trace the viterbi matrix, analyse the maximum probability path \nand assign tags to each word in the sentence based on highest probability. \n \n9.3 Train the Tagger \n \nThe next important step is training the tagger. The training method we describe here is based on \nsupervised learning approach. It runs on the corpus, makes use of tagged data and estimates the \nprobabilities of transition, P(tag | previous tag) and observation likelihood P(word | tag) for the \nHMM. \n \nThen the transition probability P(ti|ti-1)  is calculated simply by using the following formula. \n \n \n \nwhere c(ti-1ti)  is the count of tag sequence ti-1,ti in the corpus. \n \nFor calculating observation likelihood probability P(wi|ti), we calculate the unigram (unigram \nmodel uses only one piece of information, which is the one that is considering) of a word along \nwith its tag assigned in the tagged data. The likelihood probability is calculated simply by the \nfollowing formula. \n \n \n \nwhere c(tiwi) is the count of word i (wi) is assigned tag i (ti) in the corpus. \n \n \nFigure 1. The Architecture of the Tagger \n \n \n, \n, \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n18 \n \n \n10. EVALUATION \n \nThe evaluation of the system was mainly driven by train the system using Sinhala text corpus, \nwhich comprised of 2754 sentences and 90551 words. The training data set was selected from the \nSinhala text corpus developed by UCSC and we used only articles which drown from various \nSinhala newspapers. Example given below shows a part of the corpus, in which each word is \nannotated with corresponding mapping tag.  \n \nExample: ඇරිස්ටයිඩ්_NNPA සිය_PRP පාලනයේ_NNN අවසන්_JVB කාලයේ_NNN දී_VNF \nකියා_VNF සිටි_VP පරිදි_POST ,_, ඔහුයේ_PRP ජනාධිපතිත්වයේ_NNN ඉතිරි_NVB කාල_NNN \nසීමාව_NNN තුළ_POST විරුද්ධවාදීන්_NNM සමඟ_CC බලය_NNN යබදා_VNF ගැනීයේ_VNN \nපදනමක්_NNN මත_POST ,_, අර්බුදය_NNN යේ_DET තරමට_NNN යමෝරා_VNF ඒයමන්_VNN \nවළකා_JVB ගත_VP හැකිව_? තිබිණි_VFM ._. \n \nThe testing was performed on a test data extracted from the corpus, and accuracy was calculated \nusing number of correct tags proposed by the system and total number of words in the sentence/s, \nby the following formula. \n \n \n \nThe results were obtained by performing a cross validation over the corpus. The accuracy for \nknown and unknown words was also measured separately.  \n \n11. RESULT AND DISCUSSION \n \nTesting was done under two classifications: first, tested only with known words (which is already \ntagged and the tagger is trained), that gives a very high accuracy close to 95%, secondly tested the \ndata set with few unknown words and that gives a less accuracy. The tagger doesn’t perform after \nreaching an unknown word. \nTable 2 contains part of test results that were obtained by performing tests for evaluating known \nword scenarios. Actual and predicted tag assignment for each word in the sentences is shown in \nthe table. \n \nTable 3 below presents the confusion matrix, which summarized the test results given in Table 2. \nIn this confusion matrix, all correct predictions are located in the diagonal of the table. Only one \ntag assignment has deviated from the actual out of 9 actual NNN tag assignments, system has \npredicted NNN tags for 7 words, NVB tag was assigned for other two words. In this case, the \naccuracy of the system has reached to 90.91% for known words scenarios.  \n \nHence, increasing the size of the training corpus is required to increase the tagging accuracy.  Not \nonly that, it is required to include data from a wide range of domains that makes the corpus more \nunbiased and representative, and also further research are required in increasing and optimizing \nthe tagging accuracy for   known words scenarios. \n \nFurther, tagging data with unknown words is also an essential need to handle in the tagger. When \nthe system reach an unknown word, current tagger fails to propose a tag, since the system is not \ntrained for that word and the tagging algorithm doesn’t have enough intelligence to propose tags \nfor untrained words. So improvements can be suggested to the algorithm by extracting knowledge \nmainly from open class word category, since new words are coined or browed from other \nlanguages more commonly belongs to open word class. Due to fixed number of membership of \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n19 \n \nclosed class word category, we can assume that the words belongs to closed class category are \nwell defined in Sinhala grammar and that is fixed. So improvements of the algorithm can be \nsuggested to focus more on words belongs to sub categories of open class words, such as noun, \nverbs and pronouns. This could be done by incurring some intelligence to the tagger by set of \nhand written disambiguation rules, and follow the hybrid approach in the tagging algorithm. \n \nTable 2. Test Data \n \nTest Data \n1 \nPredicted: ලාාංකිකයින්ට _NNM ආයාචනා _NNN !_.   \n \nActual: ලාාංකිකයින්ට _NNM ආයාචනා _NNN !_. \n2 \nPredicted: බ්‍රිතාන්ය _NNPI ජාතිකයන් _NNM පස් _QFNUM යදනා \n_NNM නිදහස් _NVB !_. \n \nActual: බ්‍රිතාන්ය _NNPI ජාතිකයන් _NNM පස් _QFNUM යදනා \n_NNM නිදහස් _NNN !_. \n3 \nPredicted: නිදහස් _NNN සන්ධානයයන් _NNN ශ්‍රී _NNPI \nලාාංකිකයින්ට _NNM ආයාචනා _NVB !_. \n \nActual: නිදහස් _NNN සන්ධානයයන් _NNN ශ්‍රී _NNPI \nලාාංකිකයින්ට _NNM ආයාචනා _NNN !_. \n4 \nPredicted: බ්‍රිතාන්ය _NNPI මහ _JJ යකොමසාරිස් _NNM උතුරට \n_NNN යයි _VFM ._. \n \nActual: බ්‍රිතාන්ය _NNPI මහ _JJ යකොමසාරිස් _NNM උතුරට \n_NNN යයි _VFM ._. \n5 \nPredicted: දිේ _JJ විජයයන් _NNN ධර්ම _NNN විජය _NNN කරන \n_VP ඇෆ්ගන් _NNPI තයේබාන්වරු _NNPA ._. \n \nActual: දිේ _JJ විජයයන් _NNN ධර්ම _NNN විජය _NNN කරන \n_VP ඇෆ්ගන් _NNPI තයේබාන්වරු _NNPA ._. \n \nFurther, our research opens more areas to continue researches on tagging Sinhala language, which \nleads more work to be carried out on finding optimization techniques and unknown word \nhandling approaches. \n \nTable 3. Confusion Matrix of the Test Result \n \n \nPredicted \nNNM \nNNN \nNNPI \nNVB \nJJ \nVFM \nVP \nActual \nNNM \n5 \n \n \n \n \n \n \nNNN \n \n7 \n \n2 \n \n \n \nNNPI \n \n \n4 \n \n \n \n \nNVB \n \n \n \n0 \n \n \n \nJJ \n \n \n \n \n2 \n \n \nVFM \n \n \n \n \n \n1 \n \nVP \n \n \n \n \n \n \n1 \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n20 \n \n12. CONCLUSION AND FUTURE WORK \n \nIn this research, our effort was mainly focused on giving a push to NLP and computational \nlinguistics analysis for Sinhala language by developing a tagging system (according to our \nknowledge, there is no language specific tagging system available for Sinhala language). In this \npaper, we have described the POS tagging approach that we have developed, which is an \nimplementation of stochastic model approach based on HMM. An algorithm has been produced \nfor the said model. The model was tested against 90551 words, 2754 sentences of Sinhala text \ncorpus, the tagger gave more than 90% accuracy for known words, but the system is not \nperforming well for the text with unknown words yet. So unknown words scenarios are still an \nopen area for further researches. \n \nThough this research produced a tagger for Sinhala language, more research is required in this to \nimprove and optimize the algorithm. Hence, several interesting directions are suggested here for \nfuture work. \n \n \nSince new words are continuously coming into the language, handling the unknown \nwords (Out-Of-Vocabulary) is required. \n \nIn-addition to disambiguation, there are few other complex scenarios exist in Sinhala \nlanguage, which separate particles and post-positions, separation of compound nouns, \nmultiword (combination/ phrases can be function as one grammatical category) and \nseparation of using Nipatha (ksmd;) in different contexts, which  are not handled in this \nresearch. \n \nSmoothing technique can be applied to get a better outcome.  \n \nACKNOWLEDGEMENTS \n \nI express my immense gratitude and many thanks to Mr. Harsha Kumara at University of \nKelaniya for his invaluable support in providing an initiative to NLP in Sinhala language. Many \nthanks to Mrs. Kumudu Gamage at the Department of Linguistics, University of Kelaniya for her \nkind support. \n \n \nGlossary of Sinhala Terms \n \n \nSinhala Term \nEnglish Translation \n1 \nවෙසක් ව ෝය \nVesak Poya \n2 \nනිමිත්වෙන් \ndue to \n3 \nමැයි ෙැනි 2 දා \non May 2nd \n4 \n ැෙැති \nheld \n5 \nශීල ෙයා ාරයට ද \nprogram of observing Sill \n6 \nවදසීයක්  මණ \naround two hundred \n7 \nපිරිසක් \npersons \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n21 \n \n \nSinhala Term \nEnglish Translation \n8 \nසහභාගි වූහ \nparticipated \n9 \nbífnd \ntortoises, padlock \n10 \nfokAfklA \ntwo \n11 \nbkAkjd \nthere are \n12 \nod,d ;shkjd \nhave been fixed \n13 \nඇරිස්ටයිඩ් \nname, Aristed \n14 \nසිය පාලනයේ \nin his rule \n15 \nඅවසන් කාලයේ දී \nduring the last period of \n16 \nකියා සිටි පරිදි \nas told \n17 \nඔහුයේ \nhis \n18 \nජනාධිපතිත්වයේ \npresidency \n19 \nඉතිරි කාල සීමාව තුළ \nduring remaining time period \n20 \nවිරුද්ධවාදීන් \nopposition \n21 \nසමඟ \nwith \n22 \nබලය \npower \n23 \nයබදා ගැනීයේ \ndistribution of \n24 \nපදනමක් \nbase \n25 \nමත \non \n26 \nඅර්බුදය \ntrouble \n27 \nයේ \nතරමට \nයමෝරා \nඒයමන් \nExpanded in to this level   \n28 \nවළකා ගත හැකිව තිබිණි \ncould be avoided \n29 \nලාාංකිකයින්ට \nfor Sri Lankans \n30 \nආයාචනා \nsummoned, called \n31 \nබ්‍රිතාන්ය \nBritish \n32 \nජාතිකයන් \nnationalist \n33 \nපස් යදනා \n5 (5 people) \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n22 \n \n \nSinhala Term \nEnglish Translation \n34 \nනිදහස් \nfreedom, is released \n35 \nසන්ධානයයන් \nunited party \n36 \nමහ යකොමසාරිස් \nhigh commissioner \n37 \nඋතුරට \nto the north \n38 \nයයි \nwent \n39 \nදිේ විජයයන් ධර්ම විජය \nකරන \nruled by religion \n40 \nඇෆ්ගන් තයේබාන්වරු \nAfghan  Talabanish \n41 \nisxy, jHdlrKh \nSinhala grammar \n42 \nNdIdfõ \nof the language \n43 \nksjerÈ jHdlrKh úê \ncorrect grammar in use \n \n \nREFERENCES \n \n[1] \nList of languages by number of native speakers, from w.w.w.wikipedia.org.  \n[2] \nEndangered Language, from www.wikipedia.org. \n[3] \nLanguages of the world, from www.bbc.co.uk/languages/guide /languages.shtml. \n[4] \nAlecx Perera. Sinhala grammer, isxy, jHdlrKh, Wasana publication. Dankotuwa, Sri Lanka, \n2004. \n[5] \nA.A.S. Adikari. Sinhala grammer, isxy, jHdlrKh. Udaya Publications, Niwandama, Ja-ela, Sri \nLanka, 2008. \n[6] \nA. Ratnaparkhi, “A maximum entropy Part of Speech tagger”. Proceedings of EMNLP, 1996. \n[7] \nDaniel Jurafsky and James H. Martin, Speech and Language Processing, Introduction to Natural \nLanguage Processing, Computational Linguistics, and Speech Recognition. Person Education Inc \n(Singapore) Pte. Ltd., 5th Edition, 2005. \n[8] \nV. Dhanalakshmi, M. Anand kumar, K.P. Soman and S. Rajendran. “POS tagger and chunker for \nTamil language”. Proceedings of the 8th Tamil Internet Conference, Cologne, Germany, 2009. \n[9] \nE. Black, “Decision tree models applied to the labeling of text with Parts of Speech”. In Darpa \nworkshop on speech and natural language processing, 1992. \n[10] \nE. Brill, “Transformation based error driven learning and natural language processing”, A case \nstudy in Part of Speech tagging in computational linguistics, 1995. \n[11] \nK.D.C Gunasakara. Sinhala grammer, isxy, NdIdfõ ksjerÈ jHdlrKh úê. Tharanji Prints, \nNavinna, Maharagama, Sri Lanka, 2008. \n[12] \nkshar Bharathi and Prashanth R. Mannen. “Introduction to the shallow parsing contest for South \nAsian languages”. Language technilogy research center, International institute of information \ntechnology Hyderabad, India. \n[13] \nSandipan Dandapat, Sudeshna Sarkar, Anupam Basu, “A Hybrid model for Part of Speech tagging \nand its application to Bengali”. Proceedings of international conference on computational \nintelligence, 2004. \n[14] \nSmriti Singh, Kuhoo Gupta, Manish Shrivastava and Pushpak, “Morphological richness offsets \nresource demand experiences in constructing a POS tagger for Hindi”. Department of computer \nscience and engineering, Indian Institute of Technology, Bombay. \nInternational Journal on Natural Language Computing (IJNLC) Vol. 3, No.3, June 2014 \n23 \n \n[15] \nT. Brants, “A statistical Part of Speech Ttagger”, Proceedings of 6th applied NLP conference, \n2000. \n[16] \nUCSC Tagset, from http: //www.ucsc.cmb.ac.lk. \n[17] \n UCSC/LTRL Sinhala Corpus, from     http://www.ucsc.cmb.ac.lk, Beta Version April 2005. \n[18] \nDulip Herath, Kumudu Gamage and Anuradha Malalasekara,”Research report on Sinhala \nlexicon”. Langugae Technology Research Laboratory, UCSC. \n \nAuthors \nA.J.P.M.P. Jayaweera is graduated from the University of Colombo, Sri Lanka and has \ncompleted Masters in computer science from the University of Kelaniaya, Sri Lanka, and \ncurrently working on a Master of Philosophy degree in the field of Natural Language \nProcessing and Computational Linguistics at the same university. Professionally he is a \nprofessional Software engineer with 11+ years of experience in divers technologies. A \nproven career records in enterprise application development which involved providing \nbusiness critical real time application for leading industries. At present, he is working as a software \nArchitect at Virtusa Pvt Ltd, No 752, Dr. Danister De Silva Mawatha, Colombo 09, Sri Lanka. \n \nDr. N. G. J. Dias is graduated from the University of Colombo, Sri Lanka, specializing \nMathematics as the main subject. He has completed Masters and Doctoral Degrees in \nComputer Science from the Queen’s University of Belfast, Northern Ireland and \nUniversity of Wales, College of Cardiff, Cardiff of the United Kingdom respectively. At \npresent, Dr. Dias is a Professor in Computer Science attached to the Department of \nStatistics & Computer Science of the University of Kelaniya, Sri Lanka. He has been \nworking in the field of Computer Science for the last 30 years and he is the team leader of the Natural \nLanguage Processing and Computational Mathematics research groups of the University. \n",
  "categories": [
    "cs.CL",
    "I.2.7"
  ],
  "published": "2014-07-10",
  "updated": "2014-07-10"
}