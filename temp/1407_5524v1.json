{
  "id": "http://arxiv.org/abs/1407.5524v1",
  "title": "Process-Oriented Parallel Programming with an Application to Data-Intensive Computing",
  "authors": [
    "Edward Givelberg"
  ],
  "abstract": "We introduce process-oriented programming as a natural extension of\nobject-oriented programming for parallel computing. It is based on the\nobservation that every class of an object-oriented language can be instantiated\nas a process, accessible via a remote pointer. The introduction of process\npointers requires no syntax extension, identifies processes with programming\nobjects, and enables processes to exchange information simply by executing\nremote methods. Process-oriented programming is a high-level language\nalternative to multithreading, MPI and many other languages, environments and\ntools currently used for parallel computations. It implements natural\nobject-based parallelism using only minimal syntax extension of existing\nlanguages, such as C++ and Python, and has therefore the potential to lead to\nwidespread adoption of parallel programming. We implemented a prototype system\nfor running processes using C++ with MPI and used it to compute a large\nthree-dimensional Fourier transform on a computer cluster built of commodity\nhardware components. Three-dimensional Fourier transform is a prototype of a\ndata-intensive application with a complex data-access pattern. The\nprocess-oriented code is only a few hundred lines long, and attains very high\ndata throughput by achieving massive parallelism and maximizing hardware\nutilization.",
  "text": "Process-Oriented Parallel Programming with an Application to\nData-Intensive Computing\nEdward Givelberg\nDecember 12, 2021\nAbstract\nWe introduce process-oriented programming as a natural extension of object-oriented programming\nfor parallel computing. It is based on the observation that every class of an object-oriented language\ncan be instantiated as a process, accessible via a remote pointer. The introduction of process pointers\nrequires no syntax extension, identiﬁes processes with programming objects, and enables processes to\nexchange information simply by executing remote methods. Process-oriented programming is a high-level\nlanguage alternative to multithreading, MPI and many other languages, environments and tools currently\nused for parallel computations. It implements natural object-based parallelism using only minimal syntax\nextension of existing languages, such as C++ and Python, and has therefore the potential to lead to\nwidespread adoption of parallel programming. We implemented a prototype system for running processes\nusing C++ with MPI and used it to compute a large three-dimensional Fourier transform on a computer\ncluster built of commodity hardware components. Three-dimensional Fourier transform is a prototype\nof a data-intensive application with a complex data-access pattern. The process-oriented code is only\na few hundred lines long, and attains very high data throughput by achieving massive parallelism and\nmaximizing hardware utilization.\n1\nIntroduction\nThe ﬁrst commercially available microprocessor CPUs appeared in the early 1970s.\nThese were single-\nprocessor devices that operated with a clock rate of less than 1 MHz. Over the course of the following three\ndecades increasingly faster and cheaper CPUs were built. This was achieved in large part by persistently\nincreasing the clock rate. By 2004 the CPU clock rates reached the 3-4 GHz range and heat dissipation\nbecame a major problem. In order to continue improving the performance and the cost, microprocessor\ndesigners turned to parallel computing. Today, nearly all computing devices (servers, tablets, phones, etc.)\nare built using processors with multiple computing cores, whose operating frequency is less than 3.5 GHz.\nThe industry move to parallel computing succeeded because practically every application contains tasks\nthat can be executed in parallel. And yet, a decade later the vast majority of computer programs are still\nbeing written for execution on a single processor, and parallel computation is being realized primarily using\nthreads, sequential processes that share memory.\nParallel programming is generally recognized as diﬃcult, and has been a subject of extensive research\n(see [3], [9], [11] and references therein). The problem with threads is eloquently described in [10]. The\nauthor paints a bleak scenario:\n“If [...] programmers make more intensive use of multithreading, the next generation of computers\nwill become nearly unusable.”\n1\narXiv:1407.5524v1  [cs.PL]  21 Jul 2014\nIn the scientiﬁc computing community parallel programs are typically written in Fortran and C with OpenMP\n[2] and MPI [1]. Dozens of high-level languages for parallel programming have also been developed, but\npresently none of them is widely used.\nEven the so-called embarrassingly parallel computations are not\nembarrassingly easy to implement.\nIn this paper we develop a new framework for parallel programming, which we call process-oriented\nprogramming. It is based on the fundamental observation that any class in an object-oriented language\ncan be instantiated as a process, accessible via a remote pointer [6]. Such a process instantiates an object\nof the class and acts as a server to other processes, remotely executing the class interface methods on\nthis object.\nThe introduction of remote pointers enables a straightforward extension of object-oriented\nprogramming languages to process-oriented programming with hardly any syntax additions. We show that\nprocess-oriented programming is an eﬃcient framework for parallel programming, and we propose it as a\nreplacement for multithreading, MPI and many other languages, environments and tools currently used for\nparallel computations. We implemented a prototype system for running processes using C++ with MPI and\ninvestigated process-oriented programming in the context of data-intensive computing.\nThe rapid growth of generated and collected data in business and academia creates demand for increas-\ningly complex and varied computations with very large data sets. The data sets are typically stored on\nhard drives, so the cost of accessing and moving small portions of the data set is high. Nevertheless, a large\nnumber of hard drives can be used in parallel to signiﬁcantly reduce the amortized cost of data access. In\n[7], we argued that a data-intensive computer can be built, using widely available (“commodity”) hardware\ncomponents, to solve general computational problems involving very large data sets.\nThe primary challenge in the construction of the data-intensive computer lies in software engineering.\nThe software framework must balance programmer productivity with eﬃcient code execution, i.e. big data\napplications with complex data access patterns must be realizable using a small amount of code, and this\ncode must attain high data throughput, using massive parallelism. In this paper we aim to demonstrate that\nprocess-oriented programming is the right framework for the realization of the data-intensive computer.\nWe chose the computation of a large three-dimensional Fourier transform as the subject of our study\nprimarily because it can be considered as a prototype of a diﬃcult data-intensive problem. We show that\nusing processes our application can be realized with only a few hundred lines of code, which are equivalent to\napproximately 15000 lines of C++ with MPI. We also show that even with the complex data access patterns\nrequired for the computation of the 3D Fourier transform, our code attains very high data throughput by\nachieving massive parallelism and maximizing hardware utilization.\nIn section 2 we describe a simple model of storage of a data set as a collection of data pages on mul-\ntiple hard-drives.\nThis model is used in the examples in section 3, where we introduce processes.\nThe\nprocess-oriented implementation of the Fourier transform is described in section 4 and the eﬃciency of the\ncomputation is analyzed in section 5. We conclude with a discussion in section 6.\nOur presentation uses C++, but can be easily applied to any object-oriented language. size t is a large\nnon-negative integer type used in C++ to represent the size of a data object in bytes.\n2\nThe Data Set\n2.1\nData Pages\nWe represent an N1×N2×N3 array of complex double precision numbers as a collection of NP1×NP2×NP3\npages, where each page is a small complex double precision array of size n1 × n2 × n3. First, we deﬁne a\nPage class which stores n bytes of unstructured data:\n2\nclass\nPage\n{\npublic:\nPage(size_t n, unsigned\nchar * data);\n~Page ();\nprotected:\nsize_t n;\nunsigned\nchar * data;\n};\nThe ArrayPage class is derived from the Page class to handle three-dimensional complex double-precision\narray blocks:\nclass\nArrayPage:\npublic\nPage\n{\npublic:\nArrayPage(\nint n1 , int n2 , int n3 ,\ndouble * data\n);\n//\nconstructor\nthat\nallocates\ndata:\nArrayPage(int n1 , int n2 , int n3);\nvoid\ntranspose13 ();\nvoid\ntranspose23 ();\n...\nprivate:\nint n1 , n2 , n3;\n}\nThe ArrayPage class may include functions for local computation with the array page data, such as the trans-\npose functions that are important in the computation of the Fourier transform (see section 4.2). Throughout\nthis paper the arrays have equal dimensions and the distinct variables n1, n2 and n3 are maintained only\nfor the clarity of exposition.\n2.2\nStorage devices\nWe store data pages on hard drives, using a single large ﬁle for every available hard drive. The following\nPageDevice class controls the hard drive I/O.\nclass\nPageDevice\n{\npublic:\nPageDevice(\nstring\nfilename ,\nsize_t\nNumberOfPages ,\nsize_t\nPageSize\n);\n~ PageDevice ();\nvoid\nwrite(Page * p, size_t\nPageIndex);\nvoid\nread(Page * p, size_t\nPageIndex);\nprotected:\nstring\nfilename;\nsize_t\nNumberOfPages ;\nsize_t\nPageSize;\nprivate:\n3\nint\nfile_descriptor ;\n};\nThe implementation of this class creates a ﬁle filename of NumberOfPages * PageSize bytes. Pages of\ndata are stored in the PageDevice object using a PageIndex address, where PageIndex ranges from 0 to\nNumberOfPages - 1. The write method copies a data page of size PageSize to the location with an oﬀset\nPageIndex * PageSize from the beginning of the ﬁle filename. Similarly, the read method reads a page\nof data stored at a given integer address in the PageDevice. Linux direct I/O functions are used in the class\nimplementation. For example, the Linux open function, used with the O DIRECT ﬂag, attempts to minimize\ncache eﬀects of the I/O to and from the speciﬁed ﬁle.\nFor array pages we deﬁne the ArrayPageDevice, which extends PageDevice, as follows:\nclass\nArrayPageDevice :\npublic\nPageDevice\n{\npublic:\nArrayPageDevice (\nstring\nfilename ,\nsize_t\nNumberOfPages ,\nint nn1 , int nn2 , int nn3\n):\nn1(nn1), n2(nn2), n3(nn3),\nPageDevice(\nfilename ,\nNumberOfPages ,\n2 * n1 * n2 * n3 * sizeof(double)\n)\n{}\nvoid\nwrite_transpose13 (Page * p, size_t\nPageIndex);\nvoid\nread_transpose13 (Page * p, size_t\nPageIndex);\nvoid\nwrite_transpose23 (Page * p, size_t\nPageIndex);\nvoid\nread_transpose23 (Page * p, size_t\nPageIndex);\n...\nprivate:\nint n1 , int n2 , int n3;\n};\nThe implementation of the transpose methods is very simple. For example:\nvoid\nArrayPageDevice ::\nread_transpose13 (\nPage * p, size_t\nPageIndex\n)\n{\nread(p, PageIndex);\np-> transpose13 ();\n}\nIn addition to the transpose methods the ArrayPageDevice class may provides various other methods for\ncomputing with n1 × n2 × n3 blocks of complex double precision data. Furthermore, in section 4.3 we\nextend ArrayPageDevice to include caching.\n4\n2.3\nPage map\nSince we use multiple hard drives to store a single large array object, we introduce the PageMap class to\nspecify the storage layout of a given array. The PageMap translates logical array page indices into storage\naddresses. A storage address consists of an id of the server, corresponding to the hard drive where the data\npage is stored, and the page index, indicating the address of the page on that hard drive.\ntypedef\nstruct\n{\nint\nserver_id;\nsize_t\npage_index;\n}\naddress;\nstruct\nPageMap\n{\nvirtual\naddress\nPageAddress (\nint i1 , int i2 , int i3\n);\n};\nIn most applications the PageMap would be a simple, on-the-ﬂy computable function, but it is also possible\nto implement it using an array of pre-computed values.\n3\nProcesses\nIn this section we introduce processes into object-oriented programming languages and show that a complete\nframework for parallel programming with processes is obtained using only very small syntax extension.\n3.1\nProcess creation, destruction and remote pointers\nProgramming objects can be naturally interpreted as processes. Upon creation, such a process instantiates\nthe object and proceeds to act as a server to other processes, remotely executing the class interface methods on\nthis object. For example, a program running on the computing node machine0 can create a new PageDevice\nprocess on machine1, as follows:\nsize_t\nnumber_of_pages = 1024;\nsize_t\npage_size = 32 * 1024 * 1024;\nchar * remote_machine = \"machine1\";\nPageDevice * storage\n= new( remote_machine )\nPageDevice(\n\"pagefile\",\nnumber_of_pages ,\npage_size\n);\nIt can then generate a page of data and store it on the remote machine1 using the remote pointer storage:\nPage * page = GenerateDataPage ();\nsize_t\nPageAddress = 17;\nstorage ->write(page , PageAddress );\n5\nThe new PageDevice process on machine1 acts as a server which listens on a communications port, accepts\ncommands from other processes, executes them and sends results back to the clients.\nThe client-server\nprotocol is generated by the compiler from the class desccription. Remote pointer dereferencing triggers a\nsequence of events, that includes several client-server communications, data transfer and execution of code\non both the client and the server machines.\nProcess semantics and remote pointers extend naturally to simple objects, as shown in the following\nexample:\ndouble * data\n= new( remote_machine ) double [1024];\ndata [7] = 3.1415;\ndouble x = data [2];\nWhen this code is executed on machine0, a new process is created on remote machine.\nThis process\nallocates a block of 1024 doubles and deploys a server that communicates with the parent client running\non machine0. The execution of data[7] = 3.1415; requires communication between the client and the\nserver, including sending the numbers 7 and 3.1415 from the client to the server. Similarly, the execution\nof the following command leads to an assignment of the local variable x with a copy of the remote double\ndata[2] obtained over the network using client-server communications. We emphasize that code execution\nis sequential: each instruction, and all communications associated with it, is completed before the following\ninstruction is executed.\nFinally, we remark that the notion of the class destructor in C++ extends natually to process objects:\ndestruction of a remote object causes termination of the remote process and completion of the correspoding\nclient-server communications.\n3.2\nParallel computation\nThe sequential programming model requires an execution of an instruction to complete before the next\ninstruction is executed. We deﬁned remote method execution to conform to this model, and therefore the\ncalling process is kept idle until it is notiﬁed that the remote method has completed. In order to enable\nparallel computation we introduce the start keyword to indicate that the calling process may proceed with\nthe execution of the next statement without waiting for the current statement to complete.\nExample: A shared memory computation is constructed by providing access to the previously deﬁned\ndata block to several computing processes:\nconst\nint N = 64;\nclass\nComputingProcess ;\nComputingProcess * process_group [N];\nfor (int i = 0;\ni < N;\ni ++)\nstart\nprocess_group [i] = new(machine[i])\nComputingProcess (data);\nThe start keyword can be used also for remote method and remote function calls, as is shown below.\nAn array of remote pointers deﬁnes a group of processes. It is easy to assign ids to the processes and to\nmake them aware of the other processes in the group. This enables subsequent inter-process communication\nby remote method execution. Extending the example above, we assume that ComputingProcess is derived\nfrom the following ProcessGroupMember class.\nclass\nProcessGroupMember\n{\n6\npublic:\nint ID() const\n{ return id; }\nint\nNumberOfProcesses () const\n{ return N; }\nProcessGroupMember\n**\nProcessGroup () const\n{ return\ngroup; }\nprotected:\nvirtual\nvoid\nSetProcessGroup (\nint my_id ,\nint my_N ,\nProcessGroupMember\n**\nmy_group\n);\nprivate:\nint id;\nint N;\nProcessGroupMember\n** group;\n};\nThe parameter my group of the SetProcessGroup method is a remote pointer to an array of remote processes,\nso a shallow copy implementation of SetProcessGroup will result in redundant future communications. The\nfollowing deep copy implementation of SetProcessGroup, which copies the entire remote array of remote\npointers to a local array of remote pointers, is preferable:\nvoid\nProcessGroupMember :: SetProcessGroup (\nint my_id ,\nint my_N ,\nProcessGroupMember\n**\nmy_group\n)\n{\nid = my_id;\nN = my_N;\ngroup = new\nProcessGroupMember * [N];\nfor (int i = 0;\ni < N;\ni ++)\n// remote\ncopy:\ngroup[i] = my_group[i];\n}\nAfter instantiating the processes in process group the master process can form a process group as\nfollows:\nfor (int id = 0;\nid < N;\nid ++)\nprocess_group [i]-> SetProcessGroup (\ni, N, process_group\n);\nAlternatively, the master process can execute the SetProcessGroup calls in parallel, but this requires syn-\nchronizing the processes at the end. A standard way to do this is using barrier functions. A process executing\nthe barrier function call must wait until all other processes in the group execute a barrier function call before\nproceeding with the execution of the next statement:\nfor (int id = 0;\nid < N;\nid ++)\nstart\nprocess_group [i]-> SetProcessGroup (\ni, N, process_group\n);\nprocess_group ->barrier ();\n7\ndual&Intel&Xeon&X5650&\n12&physical&cores&\nmul;threading&\n48&GB&RAM&\n24 x 1 TB \n24 x100 MB/sec \nnode0&\nnetwork&\nswitch&\n10 Gb/sec  \nEthernet  \nnode1&\nnode2&\nnode3&\nFigure 1:\nCluster conﬁguration for data intensive computing: The nodes are interconnected by a high speed\nnetwork (10 Gb/sec Ethernet). Each node has a 12 core Intel Xeon processor with 48 GB of RAM and 24\nattached 1 TB hard-drives. Each hard drive has an I/O throughput in the range of 100-150 MB/sec.\n3.3\nPersistent objects and processes\nWe view a large data set as a collection of persistent processes, which provide access to portions of the data\nset, as well as methods for computing with it. Persistent processes are objects that can be destroyed only\nby explicitly calling the destructor. The runtime system is responsible for storing process representation,\nand activating and de-activating processes, as needed. Processes can be accessed using a symbolic object\naddress, similar to addresses used by the Data Access Protocol (DAP) [5], for example:\nPageDevice * page_device =\n\"http :// data/set/PageDevice /34\";\nBecause persistence is especially important for large data objects, in this paper we assume that all processes\nand all objects are persistent.\n4\nA program to compute the Fourier transform\nIn this section we show how to use processes in a data-intensive application on a cluster, such as the one\nshown in Figure 1. We chose a large three-dimensional Fourier transform as an example of a data-intensive\napplication and we give an extensive description of the process-oriented code. We consider the situation\nwhere the array data set is represented by a group of processes, and the Fourier transform is a separate\napplication whose processes interact with the array processes. In section 4.3 we show how the eﬃciency of\nthe parallel computation can be improved using server-side caching.\n4.1\nThe Array\nWe described a method of storing a large data set on multiple hard drives in section 2. We now deﬁne the\nArray class that is used in the Fourier transform computation.\n8\nThe Array class describes a complex double precision three-dimensional array. It speciﬁes the array\ndomain, its storage layout and data access methods. We ﬁrst deﬁne an auxiliary class to describe rectangular\nthree-dimensional domains:\nclass\nDomain\n{\npublic:\nDomain(\nint N11 , int N12 ,\nint N21 , int N22 ,\nint N31 , int N32\n);\n...\n};\nThe collection of hard drives attached to the cluster nodes can be turned into a distributed disk-based\nrandom access memory by launching a PageDevice process for every hard-drive on the cluster node to which\nthis hard-drive is attached, but for computations with arrays we launch ArrayPageDevice processes that\nenable us to perform some of the array computations “close to the data”:\nArrayPageDevice\n**\npage_server =\nnew\nArrayPageDevice * [ NumberOfServers ];\nfor (\nint i = 0;\ni < NumberOfServers ;\ni ++\n)\npage_server[i] = new(machine[i])\nArrayPageDevice (\nfilename[i],\nNumberOfPages ,\nN1 , N2 , N3\n);\nThe ArrayPageDevice processes could be launched instead of the PageDevice processes, however in most\napplications it would be advantageous to launch them alongside existing PageDevice processes. In this case\nArrayPageDevice must include a constructor that takes a pointer to an existing PageDevice process as\na parameter. There would be essentially no communication overhead when the ArrayPageDevice process\nis launched on the same node as the corresponding PageDevice process. The PageDevice::read method,\nfor example, will copy a page from a hard drive directly into a memory buﬀer that is accessible by the\ncorresponding ArrayPageDevice process.\nArray storage layout is determined by the PageMap object. The assignment of array pages to servers may\naﬀect the degree of parallelism that can be achieved in the computation. We use the circulant map, which\nassigns array page (i1, i2, i3) to the server (i1 + i2 + i3) % NumberOfServers.\nTo complete the\nspeciﬁcation of the PageMap, we assign the ﬁrst available PageIndex address within the target PageServer.\nThe Array class provides methods for a client process to compute over a small array subdomain. The\nclient may use a small (e.g. 4 GB) memory buﬀer to assemble the subdomain from array pages that reside\nwithin page server processes, as determined by the array pagemap.\nclass\nArray\n//\ncomplex\ndouble\n{\npublic:\nArray(\nDomain * ArrayDomain ,\n9\nDomain * PageDomain ,\nint\nNumberOfServers ,\nArrayPageDevice\n** page_server ,\nPageMap * pagemap\n);\n~Array ();\nvoid\nread(Domain * d, double * buffer);\nvoid\nwrite(Domain * d, double * buffer);\nvoid\nread_transpose13 (\nDomain * d, double * buffer\n);\nvoid\nwrite_transpose13 (\nDomain * d, double * buffer\n);\nvoid\nread_transpose23 (\nDomain * d, double * buffer\n);\nvoid\nwrite_transpose23 (\nDomain * d, double * buffer\n);\nprivate:\nDomain * ArrayDomain ;\nDomain * PageDomain;\nint\nNumberOfServers ;\nArrayPageDevice * page_server;\nPageMap * pagemap;\n};\nAn Array object is constructed by a single process, which can then pass the object pointer to any group\nof processes. Because an Array is a persistent object, it can also be accessed using a symbolic address, as\ndescribed in section 3.3.\nTranspose I/O methods are used to compute with long and narrow array subdomains that are not\naligned with the third dimension. In section 4.2 we show an application of the transpose methods to the\ncomputation of the Fourier transform. The transpose of array pages can be computed either on the client\nor on the servers. For example, the implementation of Array::read transpose13 can assemble the data by\nexecuting ArrayPageDevice::read transpose13 on the appropriate page server processes. Alternatively,\nit can execute ArrayPageDevice::read methods on these servers, followed by ArrayPage::transpose13 on\nthe received pages.\n4.2\nFFT Processes\nWe compute the three-dimensional Fourier transform using three separate functions, fft1, fft2 and fft3,\neach performing one-dimensional Fourier transforms along the corresponding dimension.\nThe functions\nfft1 and fft2 are similar to fft3, except that subdomains are read and written using the I/O transpose\noperations of the Array class. Having read and transposed the data into a local memory buﬀer, Fourier\ntransforms are computed along the third dimension. The result is then transposed back and written to the\narray. We therefore restrict our description below to the fft3 function.\nThe computation of the fft3 function is performed in parallel using several FFT3 client processes:\nclass\nFFT3:\npublic\nProcessGroupMember\n{\npublic:\n10\nFFT3(int sign , Array * a);\n~FFT3 () { delete\nbuffer; }\nvoid\nComputeTransform ();\nprivate:\ndouble * buffer;\nint\nsign;\nArray * a;\n};\nWe divide the array into n slabs along the ﬁrst dimension. The master process launches n FFT3 processes,\nassigning each process to a slab.\nFFT3 ** fft = new\nFFT3 * [n];\nfor (int i = 0;\ni < n;\ni ++)\nfft[i] = new(node[i]) FFT3(sign , a);\nfor (int i = 0;\ni < n;\ni ++)\nfft[i]-> SetProcessGroup (i, n, fft);\nfor (int i = 0;\ni < n;\ni ++)\nstart\nfft[i]-> ComputeTransform ();\nEach process maintains a buﬀer that can hold a page line, where a page line is a collection of NP3 pages\nwith page indices\n{(i1, i2, i3) :\ni3 = 0, 1, . . . , NP3 −1} .\nFor complex double precision array of 1283 pages, with each page consisting of 1283 points, the page line\nbuﬀer is 4 GB. Each process computes:\nvoid\nFFT3 :: ComputeTransform ()\n{\nDomain * PageLine;\nfor every PageLine in the slab\n{\na->read(PageLine , buffer);\ncall FFTW(sign , buffer);\na->write(PageLine , buffer);\nProcessGroup () ->barrier ();\n}\n}\nAn fft process assembles a page line by reading the pages from appropriate page servers. For each page line\none FFTW [4] function call computes a set of n1 ×n2 one-dimensional complex double Fourier transforms of\nsize N3 each. The result pages are then sent back to the page servers and stored on hard drives. Although\nthe fft processes do not communicate with each other, they share a common network and a common pool\nof page servers. The barrier synchronization at the end of each iteration is not strictly necessary.\n4.3\nParallel execution; caching\nEach page line read and write operation consists of disk I/O and network transfer, with disk I/O being\nsigniﬁcantly more time consuming. We implement caching on the page server in order to carry out part of\nthe disk I/O in parallel with the FFTW computation.\nclass\nArrayDevice :\npublic\nArrayPageDevice\n11\n{\npublic:\nArrayPageDevice (\nstring\nfilename ,\nint\nNumberOfPages ,\nint n1 , int n2 , int n3 ,\nint Nc\n):\nArrayPageDevice (\nfilename ,\nNumberOfPages ,\nn1 , n2 , n3 ,\n),\nNumberOfCachePages (Nc)\n{}\nvoid\nReadIntoCache (size_t\npage_index);\nvoid\nread(Page * p, size_t\npage_index );\n...\nprivate:\nint\nNumberOfCachePages ;\nPage ** cache;\n};\nAn ArrayDevice server is conﬁgured with a cache that can hold a line of pages. In order to use ArrayDevice\ninstead of ArrayPageDevice we add the following ReadIntoServerCache method to the Array class:\nvoid\nArray :: ReadIntoServerCache (\nDomain * domain\n)\n{\nfor every page (i1 , i2 , i3) in domain\n{\naddress a =\npagemap -> PageAddress (i1 , i2 , i3);\nint id = a.server_id;\nsize_t i = a.page_index;\nstart\nserver[id]-> ReadIntoCache (i);\n}\n}\nThe ArrayDevice:read method executes ArrayPageDevice::read if the requested page is not found in its\ncache; otherwise it returns (sends over the network) the cached page.\nThe transform computation is now reorganized, so that instructions to read the next line of pages are\nsent to the page servers before the FFT of the current page line is started.\nvoid\nFFT3 :: ComputeTransform ()\n{\nDomain * PageLine;\nfor every PageLine in the slab\n{\na->read(PageLine , buffer);\nDomain * NextPageLine = next page line;\nstart a-> ReadIntoServerCache ( NextPageLine );\ncall FFTW(sign , buffer);\na->write(PageLine , buffer);\nProcessGroup () ->barrier ();\n}\n12\n}\nAfter the completion of the FFT the servers are instructed to write pages to hard drives. An execution of\nthe write method will take place only after the server has completed ArrayDevice::ReadIntoCache. The\neﬃciency of server-side caching depends on the relative timing of the FFT computation and the page I/O. If\nnecessary, write and WriteFromCache methods can be implemented in the ArrayDevice class analogously.\n5\nComputation of the Fourier Transform\nIn this section we describe the computation of a large (64 TB) data-intensive Fourier transform on a small\n(8 nodes, 96 cores) cluster. We describe our prototype implementation of the process framework and analyze\nthe performance of the Fourier transform computation.\n5.1\nSoftware Implementation of Processes\nThe Fourier transform application was developed by completing the process-oriented code which is sketched\nin sections 2 and 4, translating it into C++ with MPI and linking it against an auxiliary library of tools\nfor implementation of basic process functionality. We describe the procedure for ArrayDevice processes. In\norder to instantiate ArrayDevice processes, we created an ArrayDevice server class and a C++ program\nﬁle ArrayDevice process.cpp containing the following code:\n// start\nthe\nserver\nand\ndisconnect\nfrom\nparent\nprocess\nint\nmain(int argc , char ** argv)\n{\nMPI_Init (&argc , &argv);\nMPI_Comm\nparent;\nMPI_Comm_get_parent (& parent);\nArrayDevice_server * s =\nnew\nArrayDevice_server ();\nMPI_Comm_disconnect (& parent);\nMPI_Finalize ();\n}\nIn addition, we implemented the function\nvoid\nLaunchProcess (\nconst\nstring & process_file_name ,\nconst\nstring & machine_address ,\nstring & server_address\n);\nLaunchProcess uses MPI Comm spawn to create an MPI process by running the executable process file name\non the remote machine speciﬁed by machine address. The launched process starts a server which uses\nMPI Open port to open a port and return the port address in the server address output parameter. The\nserver process then disconnects from the launching process.\nIn order to implement remote method execution we also created an ArrayDevice client class. The\nArrayDevice client and the ArrayDevice server classes are automatically constructed from the\nArrayDevice class. For the purposes of this example we use a simple name mangling scheme to ﬁrst generate\nthe following ArrayDevice interface class:\nclass\nArrayDevice_interface\n{\n13\n...\npublic:\nvirtual\nvoid\nArrayDevice_create (\n//\nconstructor\nstring\nfilename ,\nsize_t\nnumberofpages ,\nsize_t\npagesize\n);\nvirtual\nvoid\nArrayDevice_destroy ();\n//\ndestructor\nvirtual\nvoid\nArrayDevice_write (Page * p, size_t\npage_index);\nvirtual\nvoid\nArrayDevice_ReadIntoCache (\nint n,\nsize_t * page_index\n);\n//\ncommands\nused in client -server\nprotocol\nenum\nArrayDevice_command\n{\ncreate_CMD = 0,\ndestroy_CMD = 1,\nwrite_CMD = 2,\nReadIntoCache_CMD = 3\n};\n...\n};\nThe ArrayDevice interface class contains meta-information obtained from the ArrayDevice class. Both\nthe ArrayDevice client and the ArrayDevice server classes are derived from ArrayDevice interface.\nArrayDevice client is also derived from a general ProcessClient class, and similarly ArrayDevice server\nis derived from ProcessServer. Jointly, the ArrayDevice client, ArrayDevice server pair implement\nremote procedure calls (RPC) for ArrayDevice. The client-server communications protocol for ArrayDevice\nuses the ArrayDevice interface class and the general-purpose functionality implemented in ProcessClient\nand ProcessServer. The client-server implementation uses a simple object serialization library to send\nmethod parameters and results over the network. This serialization library is also used to implement a\nrudimentary ﬁle-based object persistence mechanism. Information is sent over the network using an MPI-\nbased communications software layer.\nThe translation procedure we implemented for the processes of the Fourier transform application can be\nextended and incorporated into a C++ compiler. It converts several hundred lines of process-oriented code\ninto a C++/MPI application for computing Fourier transform, which is approximately 15000 lines long. A\nvery small subset of MPI functions is used. The following is the almost complete list:\n• MPI Send, MPI Recv\n– for inter-process communication,\n• MPI barrier\n– for process synchronization,\n• MPI Open port, MPI Close port, MPI Comm accept\n– to implement client-server functionality,\n• MPI Comm spawn, MPI Info create, MPI Info set\n– to spawn processes on speciﬁed target ma-\nchines,\n• MPI Comm connect, MPI Comm disconnect, MPI Comm get parent – to manage process connections.\nBoth MPICH and Intel MPI were used in our computation, and with both implementations we encoun-\ntered problems with some MPI functions. The most signiﬁcant bugs were found in MPI Comm spawn and\n14\nMPI Comm disconnect.\n5.2\nFourier Transform Computation\nWe used the process prototype implementation to carry out a computation of the Fourier transform of a\n(16K)3-point array of complex double precision numbers. (We use the notation 1K = 1024, 16K = 16384 =\n128 × 128.) The total size of the array is 64 TB. The computations were carried out on a cluster of 8 nodes,\ninterconnected with a 10 Gb/sec Ethernet, similar to the cluster depicted in Figure 1. Each computational\nnode has an Intel R\n⃝Xeon R\n⃝X5650 12 core CPU with hyperthreading, rated at 2.67GHz, and 24 attached\n1 TB hard drives. The hard drives are manufactured by Samsung, model SpinPoint F3 HD103SJ, with\nmanufacturer listed latency of 4.14 ms and average seek time of 8.9 ms. Benchamark hard drive read and\nwrite throughput is reported at over 100 MB/sec.\nThe (16 K)3-point array was broken up into 1283 ArrayPage pages of 128 × 128 × 128 points each. The\nresulting page size is 32 MB. The choice of the page size is constrained by the latency and seek time of the\nsystem’s hard drives: the smaller the page size, the lower the overall disk I/O throughput. We measured a\ntypical page read/write time in the range of 0.25-0.35 sec.\nWe used 4 of the 8 available nodes to store the Array object, creating 24 ArrayDevice processes on each\nnode, one process for each available hard drive. Because ArrayDevice processes are primarily dedicated to\ndisk I/O it is possible to run 24 processes on a 12-core node. The process framework makes it easy to shift\ncomputation closer to data by extending the ArrayDevice class, and in such case relatively more powerful\nCPUs may be needed to run the server processes.\nWe used the other 4 nodes to run 16 processes of the Fourier transform application, 4 processes per node.\nEach of the 16 Fourier transform processes was assigned an array slab of 8 × 128 × 128 pages. The process\ncomputes the transform of its slab line by line in 8 × 128 = 1024 iterations. Although the 16 processes are\nindependent of each other, they compete among themselves for service from 96 page servers.\nThe wall clock time for a single iteration of ComputeTransform (see section 4.3) generally ranged from 68\nto 78 seconds, with the average of approximately 73 seconds. The total speed of data processing (including\nreading, computing and writing the data) has been therefore close to 1 GB/sec. In the next section we\nanalyze the performance in detail and indicate a number of ways to substantially improve it.\n5.3\nPerformance analysis\nThe computation of the Fourier transform was completed in the course of several long (over 10 hours)\ncontinuous runs. Our implementation of persistence mechanism for processes made it possible to stop and\nrestart the computation multiple times. The primary reason for long runs was to test the stability and\nrobustness of our implementation. We instrumented the code to measure the utilization of the system’s\ncomponents: the network, the hard drives and the processors. Because the results did not vary substantially\nover the course of the computation, we present a detailed analysis of a typical iteration of ComputeTransform.\nThe synchronization of the processes at the end of each iteration is not strictly necessary, but we found\nthat it did not signiﬁcantly aﬀect performance, and made the code easier to analyze. On the other hand, we\nfound that introducing additional barriers within the iteration would slow down the computation signiﬁcantly.\nWe now present detailed measurements of the component phases of the iteration.\nAt the beginning of every iteration each process reads a page line consisting of 128 pages, which are evenly\ndistributed among the 96 servers by the circulant map (see section 4.1). Except for the ﬁrst iteration, the\nrequired pages have already been read from the hard drive and placed in the memory of the corresponding\nservers. Each server has either 21 or 22 pages to send to the clients. In total, 64 GB of data is sent from\n15\nthe servers to the clients. Accordingly, the combined size of the server caches in each of the server nodes is\nslightly more than 16 GB. We timed the parallel reading of page lines by the 16 client processes in a typical\ntime step:\n15:\nArray :: read\n11520 -11647 x 2432 -2559 x 0 -16383:: 4 GB , 17.3638 sec , 235.894\nMB/sec\n12:\nArray :: read\n9216 -9343 x 2432 -2559 x 0 -16383:: 4 GB , 18.6695 sec , 219.395\nMB/sec\n0: Array :: read 0 -127 x 2432 -2559 x 0 -16383:: 4 GB , 21.6104 sec , 189.538\nMB/sec\n1: Array :: read\n768 -895 x 2432 -2559 x 0 -16383:: 4 GB , 22.2821 sec , 183.824\nMB/sec\n11:\nArray :: read\n8448 -8575 x 2432 -2559 x 0 -16383:: 4 GB , 22.4682 sec , 182.302\nMB/sec\n8: Array :: read\n6144 -6271 x 2432 -2559 x 0 -16383:: 4 GB , 22.7035 sec , 180.412\nMB/sec\n14:\nArray :: read\n10752 -10879 x 2432 -2559 x 0 -16383:: 4 GB , 22.9213 sec , 178.698\nMB/sec\n5: Array :: read\n3840 -3967 x 2432 -2559 x 0 -16383:: 4 GB , 27.1039 sec , 151.122\nMB/sec\n6: Array :: read\n4608 -4735 x 2432 -2559 x 0 -16383:: 4 GB , 27.0914 sec , 151.192\nMB/sec\n2: Array :: read\n1536 -1663 x 2432 -2559 x 0 -16383:: 4 GB , 27.2604 sec , 150.255\nMB/sec\n4: Array :: read\n3072 -3199 x 2432 -2559 x 0 -16383:: 4 GB , 28.1986 sec , 145.256\nMB/sec\n3: Array :: read\n2304 -2431 x 2432 -2559 x 0 -16383:: 4 GB , 28.3351 sec , 144.556\nMB/sec\n7: Array :: read\n5376 -5503 x 2432 -2559 x 0 -16383:: 4 GB , 28.9623 sec , 141.425\nMB/sec\n10:\nArray :: read\n7680 -7807 x 2432 -2559 x 0 -16383:: 4 GB , 29.027 sec , 141.11 MB/sec\n9: Array :: read\n6912 -7039 x 2432 -2559 x 0 -16383:: 4 GB , 29.5628 sec , 138.553\nMB/sec\n13:\nArray :: read\n9984 -10111 x 2432 -2559 x 0 -16383:: 4 GB , 30.1746 sec , 135.743\nMB/sec\nThe ﬁrst number in each row is the client process id, followed by the description of the domain, domain\nsize, the time it took to read it and the corresponding throughput.\nThe fastest process completes the\nexecution of the Array::read method (including sending the command and the parameter to page servers)\nin approximately 17.5 seconds, the slowest – in about 30 seconds. The faster processes proceed to start\nthe execution of Array::ReadIntoServerCache immediately after completion of Array::read. The typical\naggregate throughput during the parallel execution of Array::read is therefore signiﬁcantly larger than 2\nGB/sec. The maximal possible throughput to the four nodes computing the transform is approximately 4\nGB/sec.\nIn the Array::ReadIntoServerCache phase of the computation 16 clients send small messages to 96\npage servers with instructions to read a total of 16 × 128 pages. These commands are queued for execution\nin the servers, so that the clients do not wait for the completion of the execution. The time measurements of\nthe parallel execution of start Array::ReadIntoServerCache by the 16 client processes in a typical time\nstep were:\n14:\nArray :: ReadIntoServerCache\n10752 -10879 x 2560 -2687 x 0 -16383::\n9.36148\nsec\n13:\nArray :: ReadIntoServerCache\n9984 -10111 x 2560 -2687 x 0 -16383::\n2.10771\nsec\n10:\nArray :: ReadIntoServerCache\n7680 -7807 x 2560 -2687 x 0 -16383::\n3.25717\nsec\n12:\nArray :: ReadIntoServerCache\n9216 -9343 x 2560 -2687 x 0 -16383::\n9.5796\nsec\n8: Array :: ReadIntoServerCache\n6144 -6271 x 2560 -2687 x 0 -16383::\n9.81484\nsec\n11:\nArray :: ReadIntoServerCache\n8448 -8575 x 2560 -2687 x 0 -16383::\n13.6131\nsec\n9: Array :: ReadIntoServerCache\n6912 -7039 x 2560 -2687 x 0 -16383::\n2.67452\nsec\n5: Array :: ReadIntoServerCache\n3840 -3967 x 2560 -2687 x 0 -16383::\n5.16807\nsec\n2: Array :: ReadIntoServerCache\n1536 -1663 x 2560 -2687 x 0 -16383::\n5.0223\nsec\n4: Array :: ReadIntoServerCache\n3072 -3199 x 2560 -2687 x 0 -16383::\n4.08604\nsec\n0: Array :: ReadIntoServerCache\n0 -127 x 2560 -2687 x 0 -16383::\n10.6741\nsec\n3: Array :: ReadIntoServerCache\n2304 -2431 x 2560 -2687 x 0 -16383::\n3.94899\nsec\n1: Array :: ReadIntoServerCache\n768 -895 x 2560 -2687 x 0 -16383::\n9.98827\nsec\n6: Array :: ReadIntoServerCache\n4608 -4735 x 2560 -2687 x 0 -16383::\n5.16787\nsec\n15:\nArray :: ReadIntoServerCache\n11520 -11647 x 2560 -2687 x 0 -16383::\n14.9185\nsec\n7: Array :: ReadIntoServerCache\n5376 -5503 x 2560 -2687 x 0 -16383::\n3.27696\nsec\nThese results are signiﬁcantly worse than expected. Our implementation of processes repeatedly establishes\nand breaks client-server connections. We found the MPI Comm connect function to be very fast, but with\nincreasing number of client-server connections, it sporadically performed hundreds of times slower than usual.\nAn implementation of caching connections is likely to reduce the total time for this phase of the computation\nto about 3 seconds.\nWe timed the execution of the FFTW function call by every processor.\n16\n14:\nfftw\n10752 -10879 x 2432 -2559 x 0 -16383\n5.53611\nsec\n0: fftw 0 -127 x 2432 -2559 x 0 -16383\n5.86336\nsec\n6: fftw\n4608 -4735 x 2432 -2559 x 0 -16383\n5.91158\nsec\n4: fftw\n3072 -3199 x 2432 -2559 x 0 -16383\n5.99485\nsec\n8: fftw\n6144 -6271 x 2432 -2559 x 0 -16383\n6.01364\nsec\n12:\nfftw\n9216 -9343 x 2432 -2559 x 0 -16383\n6.03876\nsec\n10:\nfftw\n7680 -7807 x 2432 -2559 x 0 -16383\n6.06033\nsec\n2: fftw\n1536 -1663 x 2432 -2559 x 0 -16383\n6.07081\nsec\n11:\nfftw\n8448 -8575 x 2432 -2559 x 0 -16383\n6.38444\nsec\n7: fftw\n5376 -5503 x 2432 -2559 x 0 -16383\n6.38421\nsec\n15:\nfftw\n11520 -11647 x 2432 -2559 x 0 -16383\n6.42032\nsec\n3: fftw\n2304 -2431 x 2432 -2559 x 0 -16383\n6.42975\nsec\n9: fftw\n6912 -7039 x 2432 -2559 x 0 -16383\n14.001\nsec\n13:\nfftw\n9984 -10111 x 2432 -2559 x 0 -16383\n14.5631\nsec\n5: fftw\n3840 -3967 x 2432 -2559 x 0 -16383\n14.5582\nsec\n1: fftw\n768 -895 x 2432 -2559 x 0 -16383\n14.878\nsec\nThe last 4 processes (9,13,5 and 1) ran on the same node. Throughout our computation these 4 processes\nexecuted the FFTW library function call signiﬁcantly slower than the processes running on other nodes.\nAdditional investigation of the conﬁguration of this node is needed to speed up the computation.\nThe reading of the pages on the server side is done concurrently with the FFTW computation. We\ninclude the measurements for a few of the 96 servers:\n...\n34:\nArrayDevice :: ReadIntoCache : 22 pages , 704 MB , 5.59864 sec , 125.745\nMB/sec\n25:\nArrayDevice :: ReadIntoCache : 21 pages , 672 MB , 5.614 sec , 119.701\nMB/sec\n57:\nArrayDevice :: ReadIntoCache : 22 pages , 704 MB , 5.58319 sec , 126.093\nMB/sec\n52:\nArrayDevice :: ReadIntoCache : 22 pages , 704 MB , 5.59719 sec , 125.777\nMB/sec\n40:\nArrayDevice :: ReadIntoCache : 22 pages , 704 MB , 5.61367 sec , 125.408\nMB/sec\n73:\nArrayDevice :: ReadIntoCache : 21 pages , 672 MB , 5.57943 sec , 120.442\nMB/sec\n21:\nArrayDevice :: ReadIntoCache : 22 pages , 704 MB , 5.64487 sec , 124.715\nMB/sec\n33:\nArrayDevice :: ReadIntoCache : 22 pages , 704 MB , 5.63134 sec , 125.015\nMB/sec\n89:\nArrayDevice :: ReadIntoCache : 21 pages , 672 MB , 5.56771 sec , 120.696\nMB/sec\n46:\nArrayDevice :: ReadIntoCache : 22 pages , 704 MB , 5.62876 sec , 125.072\nMB/sec\n...\nThe reading throughput is close to the maximal throughput for this type of hard drives. For every server the\npage reading commands are scheduled before the page writing commands of the last phase of the iteration.\nThe writing of the pages will therefore start only after the completion of the page read commands. The\nclients write pages in parallel, with the typical timing as follows:\n8: Array :: write\n6144 -6271 x 2432 -2559 x 0 -16383:: 4 GB , 22.3279 sec , 183.448\nMB/sec\n4: Array :: write\n3072 -3199 x 2432 -2559 x 0 -16383:: 4 GB , 22.784 sec , 179.776\nMB/sec\n11:\nArray :: write\n8448 -8575 x 2432 -2559 x 0 -16383:: 4 GB , 22.7165 sec , 180.31\nMB/sec\n12:\nArray :: write\n9216 -9343 x 2432 -2559 x 0 -16383:: 4 GB , 23.083 sec , 177.447\nMB/sec\n15:\nArray :: write\n11520 -11647 x 2432 -2559 x 0 -16383:: 4 GB , 23.0278 sec , 177.872\nMB/sec\n7: Array :: write\n5376 -5503 x 2432 -2559 x 0 -16383:: 4 GB , 23.2437 sec , 176.22 MB/sec\n0: Array :: write\n0 -127 x 2432 -2559 x 0 -16383:: 4 GB , 23.7938 sec , 172.145\nMB/sec\n14:\nArray :: write\n10752 -10879 x 2432 -2559 x 0 -16383:: 4 GB , 24.5307 sec , 166.975\nMB/sec\n10:\nArray :: write\n7680 -7807 x 2432 -2559 x 0 -16383:: 4 GB , 24.2152 sec , 169.15\nMB/sec\n6: Array :: write\n4608 -4735 x 2432 -2559 x 0 -16383:: 4 GB , 24.6648 sec , 166.066\nMB/sec\n2: Array :: write\n1536 -1663 x 2432 -2559 x 0 -16383:: 4 GB , 24.7291 sec , 165.635\nMB/sec\n3: Array :: write\n2304 -2431 x 2432 -2559 x 0 -16383:: 4 GB , 25.3921 sec , 161.31 MB/sec\n9: Array :: write\n6912 -7039 x 2432 -2559 x 0 -16383:: 4 GB , 21.8906 sec , 187.113\nMB/sec\n13:\nArray :: write\n9984 -10111 x 2432 -2559 x 0 -16383:: 4 GB , 22.0268 sec , 185.956\nMB/sec\n1: Array :: write\n768 -895 x 2432 -2559 x 0 -16383:: 4 GB , 21.7936 sec , 187.945\nMB/sec\n5: Array :: write\n3840 -3967 x 2432 -2559 x 0 -16383:: 4 GB , 22.2789 sec , 183.851\nMB/sec\nThe results for page writing are fairly uniform, with the total time for each process between 22 and 25.5\nseconds. The aggregate througfhput for this phase is therefore over 2.5 GB/sec. We did not implement\nexplicit caching for writing pages. The implementation of Array::write is analogous to the implementation\nof Array::ReadIntoServerCache (see section 4.3):\n17\nvoid\nArray :: write(Domain * domain)\n{\nfor every page in domain\nstart write page to the appropriate server\n}\nThere is a limited caching eﬀect as a result of the start command: having transmitted the page to the\nserver, the client disconnects and proceeds to transmit pages to other servers, while the server starts writing\nthe page to disk only after the client has disconnected. We found that writing pages to hard drive with the\nO DIRECT ﬂag is about twice as fast as reading, presumably because of buﬀering. The typical throughput\nmeasured was between 230 and 240 MB/sec:\n...\n22:\nArrayDevice :: write: 1 page , 32 MB , 0.136958 sec , 233.648\nMB/sec\n78:\nArrayDevice :: write: 1 page , 32 MB , 0.135615 sec , 235.962\nMB/sec\n73:\nArrayDevice :: write: 1 page , 32 MB , 0.13452 sec , 237.883\nMB/sec\n64:\nArrayDevice :: write: 1 page , 32 MB , 0.136484 sec , 234.46 MB/sec\n8:\nArrayDevice :: write: 1 page , 32 MB , 0.137042 sec , 233.505\nMB/sec\n54:\nArrayDevice :: write: 1 page , 32 MB , 0.136533 sec , 234.376\nMB/sec\n87:\nArrayDevice :: write: 1 page , 32 MB , 0.136293 sec , 234.788\nMB/sec\n...\nThe conclusion of our performance analysis is that the presented computation could be sped up by 25%\nor more, but greater beneﬁts can be derived from a more balanced hardware conﬁguration. The aggregate\nthroughput of the 24 hard drives of a cluster node is about 3 GB/sec, about 3 times the capacity of the\nincoming network connection. Furthermore, the FFTW computation takes only 10-20% of the total iteration\ntime. It appears that a 2–4-fold increase in the network capacity of the present cluster is likely to result\nin a more balanced system with better hardware utilization, and a total runtime of under 20 seconds per\niteration.\n6\nDiscussion and Conclusions\nIn this paper we introduced process-oriented programming as a natural extension of object-oriented pro-\ngramming for parallel computing. We implemented a prototype of the process framework and carried out\na data-intensive computation. We have shown that a complex and eﬃcient application can be built using\nonly a few hundred lines of process-oriented code, which is equivalent to many thousands of lines of object-\noriented code with MPI. The process-oriented code in this paper is an extension of C++, but processes can\nbe introduced into any object-oriented language. The syntax extension is minimal. In C++, for example, it\namounts to adding a parameter to the new operator and introducing the keyword start. Combined with the\nfact that creating a process can be thought of as simply placing an object on a remote machine, it suggests\nthat a lot of existing code can be easily modiﬁed to run in parallel. Potentially the most important impact of\nthe process-oriented extension of languages, such as C++ and Python, is a widespread adoption of parallel\nprogramming, as application developers realize the ability to easily create processes instead of using thread\nlibraries and to place diﬀerent objects on diﬀerent CPU cores.\nThe process-oriented programming model is based on a simple hardware abstraction: the computer\nconsists of a collection of processors, interconnected by a network, where each processor is capable of running\nmultiple processes. The run-time system is responsible for mapping the abstract model onto a concrete\nhardware system, and must provide the programmer with system functions describing the state of the\nhardware.\nThe hardware abstraction of the process-oriented model makes it possible to create portable\nparallel applications and applications that run in the cloud.\n18\nProcesses are accessible by remote pointers.\nSyntactically, executing a method on a remote process\nis not diﬀerent from method execution on an object.\nAny class of an object-oriented language can be\ninterpreted as a process, but even more importantly, in the process-oriented framework only processes that\nare class instances are allowed. We argue that object-based parallelism is a high level abstraction, which is\nnaturally suitable for reasoning about parallelism. Although shared memory and message passing can be\nrealized in a process-oriented language, these are lower implementation-level models. Process inheritance\nis a powerful aspect of object-based parallelism, as it enables the deﬁnition of new processes in terms of\npreviously deﬁned processes. In combination with process pointers, it gives the programmer the ﬂexibility\nto adapt the computation to the hardware by adding simple methods to class deﬁnitions (see the comments\nat the end of section 4.1 about the computation of array page transpose).\nA process-oriented program, like a typical sequential program starts with a single main process. The\nmain process may launch new processes on remote machines, as easily as it can create objects. In contrast\nwith MPI, processes are explicitly managed by the programmer. Process launching is part of the program,\nand is not determined by the runtime command line parameters. Using process pointers and language data\nstructures, the programmer can form groups of processes, assign process ids and perform tasks that in MPI\nwould require using communicators.\nProcesses exchange information by executing remote methods, rather than via shared memory or message\npassing. We’d like to use the analogy that writing programs with message passing today is like writing\nprograms with GOTOs ﬁfty years ago: it is easy to write intractable code. And just like GOTO statements\nare used in assembly languages, message passing is a low-level language construct underlying remote method\nexecution in process-oriented programming.\nWe introduced the start keyword to enable parallel execution of remote methods. In order to use the\nkeyword the programmer must decide whether there is a need to wait for the remote task to complete before\nproceeding with the computation. In general, this decision is easy and intuitive, but keeping track of task\ndependencies is not. Each process executes only one method at a time and remote method execution requests\nare queued. The programmer must keep in mind the state of the execution queue for every process. This is\npossible only for very simple scenarios. We used barriers to synchronize processes. Barrier synchronization\nhelps the programmer to keep track of the execution queues of the processes, but it may reduce the parallelism\nof the computation.\nWe view a large data object as a collection of persistent processes. For a large data object a negligible\namount of additional storage space is needed to store serialized processes alongside the data.\nProcess\npersistence is needed to enable stopping and restarting a computation, and to make a data set accessible to\nseveral simultaneous applications. It is also needed to develop basic mechanisms for fault tolerance. In this\npaper we showed that the process-oriented view of a large data object is very powerful: using only a small\namount of code the programmer can copy and reformat very large data objects, and even carry out complex\noperations, such as the Fourier transform. Yet, software users and application developers tend to see a data\nset as consisting of “just data”, and being independent of a speciﬁc programming language. We stop short\nof suggesting a solution for this problem, but in this context it is worth recalling the CORBA standard [8].\nThe introduction of process-oriented programming was motivated by our research in data-intensive com-\nputing. The data-intensive Fourier transform computation was carried out on a small cluster of 8 nodes\n(96 cores). Our measurements indicate that Petascale data-intensive computations can be eﬃciently carried\nout on a larger cluster, with more nodes and signiﬁcantly increased network bandwidth. Such a cluster can\nserve as a general-purpose data-intensive computer, whose operating system and applications are developed\nas process-oriented programs.\nThe introduction of process-oriented programming in this paper is far from complete. It is merely the\n19\nﬁrst step of an extensive research program. We tested a substantial subset of the process framework in a\nprototype implementation. A full-ﬂedged implementation must include a compiler and a run-time system\nthat substantially expand the basic prototype.\n7\nAcknowledgements\nI am grateful to J. J. Bunn for discussions that signiﬁcantly improved the presentation of the material.\nReferences\n[1] MPI: A Message-Passing Interface Standard, version 3.0. http://www.mpi-forum.org/.\n[2] OpenMP 4.0 Speciﬁcations. http://openmp.org/wp/openmp-specifications/.\n[3] J. Diaz, C. Munoz-Caro, and A. Nino. A survey of parallel programming models and tools in the multi\nand many-core era. Parallel and Distributed Systems, IEEE Transactions on, 23(8):1369–1386, 2012.\n[4] M. Frigo and S. G. Johnson.\nThe design and implementation of ﬀtw3.\nProceedings of the IEEE,\n93(2):216–231, 2005.\n[5] J. Gallagher, N. Potter, T. Sgouros, S. Hankin, and G. Flierl. The data access protocol-DAP 2.0, 2004.\n[6] E. Givelberg. Object-oriented parallel programming. April 2014. arXiv:1404.4666 [cs.PL].\n[7] E. Givelberg, A. Szalay, K. Kanov, and R. Burns. An architecture for a data-intensive computer. In\nProceedings of the ﬁrst international workshop on Network-aware data management, pages 57–64. ACM,\n2011.\n[8] M. Henning. The rise and fall of CORBA. Queue, 4(5):28–34, June 2006.\n[9] M. Herlihy and N. Shavit. The Art of Multiprocessor Programming, Revised Reprint. Elsevier, 2012.\n[10] E. A. Lee. The problem with threads. Computer, 39(5):33–42, 2006.\n[11] T. G. Mattson, B. A. Sanders, and B. L. Massingill.\nPatterns for parallel programming.\nPearson\nEducation, 2004.\n20\n",
  "categories": [
    "cs.PL",
    "cs.DC"
  ],
  "published": "2014-07-21",
  "updated": "2014-07-21"
}