{
  "id": "http://arxiv.org/abs/1407.6099v1",
  "title": "Autonomous requirements specification processing using natural language processing",
  "authors": [
    "S. G. Macdonell",
    "K. Min",
    "A. M. Connor"
  ],
  "abstract": "We describe our ongoing research that centres on the application of natural\nlanguage processing (NLP) to software engineering and systems development\nactivities. In particular, this paper addresses the use of NLP in the\nrequirements analysis and systems design processes. We have developed a\nprototype toolset that can assist the systems analyst or software engineer to\nselect and verify terms relevant to a project. In this paper we describe the\nprocesses employed by the system to extract and classify objects of interest\nfrom requirements documents. These processes are illustrated using a small\nexample.",
  "text": "AUTONOMOUS REQUIREMENTS SPECIFICATION PROCESSING USING \nNATURAL LANGUAGE PROCESSING \n \nProfessor S.G. MacDonell \nSoftware Engineering Research Lab \nAuckland University of Technology \nAuckland, New Zealand \nstephen.macdonell@aut.ac.nz  \nDr Kyongho Min \nSchool of Computer & Info Sciences \nAuckland University of Technology \nAuckland, New Zealand \nkyongho.min@aut.ac.nz \nDr A.M. Connor \nSoftware Engineering Research Lab \nAuckland University of Technology \nAuckland, New Zealand \nandrew.connor@aut.ac.nz \n \n \nAbstract \n \nWe describe our ongoing research that centres on the \napplication of natural language processing (NLP) to \nsoftware engineering and systems development activities.  \nIn particular, this paper addresses the use of NLP in the \nrequirements analysis and systems design processes.  We \nhave developed a prototype toolset that can assist the \nsystems analyst or software engineer to select and verify \nterms relevant to a project.  In this paper we describe the \nprocesses employed by the system to extract and classify \nobjects of interest from requirements documents.  These \nprocesses are illustrated using a small example.  \n \n \n1 \nINTRODUCTION \n \nThis paper describes the architecture of an \nautonomous requirements specification processing system \nthat utilises a limited version of a natural language \nprocessing (NLP) system and an interactive user interface \nsystem. When analyzing requirements artefacts e.g. \nspecification documents, interview transcripts and so on, \nan analyst generally uses their own software engineering \nknowledge, training and experience in combination with \none or more software design tools. In particular, however, \nthe verification of requirements specification analysis \ndepends primarily on the software engineer’s knowledge. \nAs a result, important information such as relationships \nbetween entities in a requirements specification document \ncould possibly be missed. \n \nIt is rather stating the obvious, but the requirements \nanalysis and determination activities are among the most \nimportant \nin \ninformation \nsystems \ndevelopment.  \nInaccuracies that are introduced or omissions that occur in \nthese stages of development, if unchecked, generally \nresult in costly rework in later lifecycle phases. The work \ndescribed in this paper is therefore focused on the \nverification of requirements specification analysis as \nperformed by a software engineer or systems analyst with \na view to producing a design model – a use case diagram, \nan entity-relationship model or similar.  This paper first \ndescribes prior autonomous application research in \nrequirements analysis in section 2.  This is followed by a \ndescription of the proposed system architecture in section \n3.  Section 4 closes the paper with a brief discussion and \nour conclusions to date. \n \n2 \nBACKGROUND \n \nMany of the problems encountered in software \nsystems can be traced back to shortcomings in the \nprocesses and practices used to gather, specify and \nmanage the end product requirements. Typically, these \nshortcomings are due to the use of informal information \ngathering, unstated or implicit functionality, unfounded or \nuncommunicated assumptions, inadequately documented \nrequirements or a casual requirements change process [1]. \nIt has been suggested that between 40 and 60% of \nsoftware defects are related to errors made during the \nrequirements stage [2]. The cost of correcting defects is \noften significantly greater than the cost that would have \nbeen incurred to ensure that the requirements correctly \nrepresented the users’ need.  \n \nWhilst the generation of a complete and non-\nambiguous set of requirements reduces the risk in any \ngiven project, there is still a risk that the requirement set \nis not transformed into an appropriate design. This risk is \ninherent as a result of mis-interpretation of the \nrequirements, particularly due to a lack of shared \nunderstanding [3] or due to poor structuring of the project \nby not conducting architectural design in parallel with \nrequirements capture [4]. \n \nThe use of formal languages or a structured system \ndesign approach can greatly increase the chance that the \nsoftware as constructed will in fact conform to the \ninterpretation of the requirements. Formal languages help \nremove some elements of ambiguity from the process as \nthey use explicit syntax and semantics that define a set of \nrelations and object interactions more consistently than \nthe English language. However, the extraction of entity \nrelationships from a natural language requirements \ndocument is normally conducted manually by a designer \nusing \ntheir \nsoftware \nengineering \nknowledge \nin \nconjunction with a design tool. This introduces the risk of \ninconsistency in approach and also the possibility that \nsome entities, relationships or attributes will be missed \nentirely. \n \nA great deal of research has focused on the \nautomation of aspects of the software engineering \nprocess, namely requirements elicitation, translation and \nanalysis, \nand \nsubsequent \nsoftware \ngeneration, \ndemonstration and test, resulting in a final system artefact. \nTo date there have been few attempts to automate the \ntranslation from a requirements document written in a \nnatural language to one expressed in a formal \nspecification language. One of the major reasons for this \nis the ambiguity of natural language requirements.  \n \nNazlia et al [5] propose new heuristics that assist the \nsemi-automated generation of entity relationship diagrams \nfor database modelling from a natural language \ndescription, with reasonable success. However, the \nlimitation to database systems does imply that the natural \nlanguage documents being processed have particular \nstructure and language and their approach may not be \nextendable to generic software requirement documents. \n \nBras and Toussiant [6] specify a framework for the \nanalysis and mapping of requirements documents, with a \nparticular focus on satellite ground support systems. Such \nsystems tend to be large, take a long time to develop, and \nhave extensive documentation that is all predominantly in \nnatural language. They facilitate requirements traceability \nby building tools to analyze, linguistically map and retain \nas a knowledge base the contents of the requirements \ndocuments. \n \nLee and Bryant [7] developed a system for mapping \nnatural language requirements documents into an object-\noriented formal specification language that utilises \nContextual Natural Language Processing (CNLP) to \novercome the ambiguity in natural language. The \nmapping \nprocess \nrequires \nthat \nthe \nrequirements \nspecification is converted to an XML format which is then \nparsed, with the results added to a knowledge base. The \ncontent of the knowledge based is converted into a Two \nLevel Grammar format which is a formal requirements \nspecification language [8]. Finally, a VDM++ model is \nproduced that describes the software design. \n \nAmbriola and Gervasi [9] describe a system for \nsupporting \nnatural \nlanguage \nrequirements \ngathering, \nelicitation, selection and validation. Central to the work is \nthe idea that requirements are supplemented by a glossary \ndescribing and classifying all the domain and system specific \nterms used in the requirements. Therefore, the NLP engine \nhas a-priori knowledge relevant to the content of the \nrequirements documents. \n \nThe approach detailed in this paper has no a-priori \nknowledge with regards the content of the documents, \nwhich also require no pre-processing. It is applicable to \nall software requirements documents as it is primarily \nused interactively and as such provides a high level of \nconsistency checking to ensure that all requirements are \ncaptured in terms of the relationships between entities. \n \n3 \nSYSTEM DESIGN \n \nIn this section, the architecture of an NL (natural \nlanguage)-based SE tool is described. The system focuses \non the automatic extraction of objects of interest from a \nrequirements specification document that is being \nprocessed by a systems analyst (Figure 1). \n \n \n \nFigure 1: Assisted Requirements Analysis Process \n(as implemented in this research project) \n \n3.1 \nSystem Architecture  \n \nThe system is composed of three modules with a \nuser interface implemented by Common lisp IDE (Figure \n2). The first of the three modules – a tokeniser – reads \nsentences from a document, the second module parses \neach sentence and extracts all unique noun terms (an NLP \ntool), and the third module – a term management system – \nperforms 1) the filtering of unimportant terms, 2) the \nclassification of the remaining terms into one of three \ncategories (function, entity, or attribute), and 3) the \ninsertion of objects of interest into a project knowledge \nbase. \n \nRequirements \nSpecification \nDocument \nNLP Tool \n(Syntactic \nParsing) \nTerm \nManagement \nSystem (UI) \n \nFigure 2: System Architecture \n \n3.2 \nA Parsing System \n \nAfter the sentences in a requirements specification \ndocument are extracted by the tokeniser, each sentence is \nparsed by a syntactic parser based on a chart parsing \ntechnique [10] with a context-free grammar (CFG) that is \naugmented with constraints. The current prototype system \nuses a dictionary with about 32000 entries and 79 rules. A \ncontext free rule is composed of LHS (Left-Hand Side), \nRHS (Right-Hand Side) with well-formedness constraints \nfor the phrasal constituent. For example, there is a rule S \n(i.e. LHS) Æ NP VP (i.e. RHS) with its well-formedness \nconstraint being (number-agreement NP VP). Thus the \nsentence “He see a car in the park” would be filtered out \nas ill-formed because of the number disagreement \nbetween “he” and “see”. \n \nAt present, the syntactic parsing system does not \nrecognise compound noun terms, such as “information \nsystem” and “staff members”, by a systematical \ncompound \nnoun \nrecognition \nsystem. \nThe \nsystem \nrecognises compound noun terms by using a list of \ncompound noun terms and a pattern matching technique. \n \nThe syntactic parser can produce ambiguous parse \ntrees of each sentence. At present, the parser has no \ndisambiguation module – this will be implemented in a \nlater version of the system.  Currently the first parse tree \nis selected as the basis for the extraction of terms for the \nterm management system, terms that will ultimately \nappear in specification and design artefacts such as use \ncase diagrams or data models. For example, the sentence \n“A system requires entry of patient’s information” has the \nfollowing parse tree:  \n \n(S    (NP (DET “A”) (NOUN “system”))  \n(VP  (VERB “requires”)  \n(NP  (NP (NOUN “entry”)) \n(PP (OF “of”) (NP (POSSADJ “patient’s”) \n(NOUN “information”)))))). \n \nFrom the parse tree, terms based on the syntactic \nstructure (noun phrase (NP)) would be extracted.  In the \nexample above this would include (NP (DET “A”) \n(NOUN “system”)), (NP (NOUN “entry”), and (NP \n(POSSADJ \n“patient’s”) \n(NOUN \n“information”)). \nHowever, the NP (“entry of patient’s information”) would \nnot be extracted because the structure includes embedded \nNPs (“entry” “patient’s information”).  \n \nAnother real, complex sentence extracted from a \nrequirements specification document, “Dunedin Podiatry \nrequires an information system that allows entry and \nretrieval of patient's details and their medical histories.” \nresults in two parse trees. From the first parse tree, the \nterm extraction stage retrieves NOUN terms including \n“Dunedin Podiatry”, “information system”, “entry”, \n“retrieval”, “(patient’s) details”, and “(their medical) \nhistories” (Figure 3). \n \nTerm \nlist \npane \nClass \nlist \npane \nRequirement specification document \n \n \nFigure 3: Term Extraction by a Syntactic Parser \n \nFinally, the term extraction process identifies nouns \nin the extracted NPs, in this case nouns such as “system”, \n“entry”, and “information”, and these terms can then be \nclassified into one of the categories relevant to the design \nartefact being produced (e.g. entity, function, attribute) by \na term management system. \n \n3.3 \nTerm Management System \n \nAfter extracting NP terms, the nouns are shown in \nthe term list pane (i.e. left pane) in Figures 3 and 4. The \nfiltering function (enacted by the ‘Filter Entity’ toggle \nbutton, shown in Figure 4) enables the analyst or software \nengineer to remove unimportant terms. The term \nextraction process cannot necessarily determine every \nuseful term automatically. Thus in this stage the user can \nmanually remove further unimportant terms. \n \n \n \nFigure 4: Filtering and categorisation of terms. \n \nThe user can then select terms to create classes of \nobjects of interest (in this example, one of entity, \nattribute, or function) and can manage the term’s addition \nto and deletion from the defined class (via the class list \npane, shown as the middle pane in Figure 3 and the right-\nhand pane in Figure 4). The user can view the currently \nclassified terms in each of the three classes by using a list \npane of classes (i.e. a combo box under the ‘chart-parser’ \nbutton in Figure 4). \n \nBy selecting terms and their class, individual objects \nare created and stored in a project knowledge base using \nthe following data structures: \n \n(OBJECT  (:TYPE FUNCTION) (:VALUE “entry”)); \n(OBJECT  (:TYPE ENTITY) (:VALUE “patient”)); and \n(OBJECT  (:TYPE ATTRIBUTE) (:VALUE “age”)). \n \nFurther documents relevant to the project can then \nbe analysed and the knowledge base updated. Class \nconflicts can be identified by the system and flagged to \nthe user as requiring resolution.  The knowledge base can \nthen be used as the basis for the automatic generation of \nrelevant design artefacts – object models, data models and \nthe like. \n \n4 \nDISCUSSION AND CONCLUSIONS \n \nAt present the prototype parsing system is unable to \nperform the following:  \n \n1. disambiguation of syntactic parse trees \n \n2. compound noun analysis and proper noun processing \n \n3. anaphoric resolution and semantic interpretation of \nterms. \n \nThe next version of the system will be extended to \nimplement the above mentioned functionality in order to \nenhance the process of term extraction and enable term \nrelationship identification. The semantic interpretation of \neach sentence will help in the extraction of useful \nrelationships between the classes. For example, the \nparsing of “patient’s medical histories” will produce in  a \ndata model a one-to-many relationship between “patient” \nand “medical histories”. \n \nThe fully implemented system will utilise NLP to \nassist  systems analysts in selecting and verifying objects \nand relationships of relevance to any given project, then \nenabling these objects and relationships to be depicted in \ndesign artefacts (in either this tool or additional software \nengineering tools). Thus the burden of analysis – \nrequiring that the systems analyst ‘parse’, select and relate \nthe objects of interest from specification documents – can \nbe shifted at least in part to a toolset that is able to \nperform these tasks intelligently and automatically.  \n \n5 \nREFERENCES \n \n[1] Wiegers, K.E. Software Requirements, Microsoft \nPress, 1999 \n \n[2] Leffingwell, \nD. \n“Calculating \nthe \nReturn \non \nInvestment from more Effective Requirements \nManagement”, American Programmer, vol. 10, no. \n4, pp. 13-16, 1997 \n \n[3] Larson, S. & Morrison, B. “Managing software \nrequirements in the context of the scientific \nenterprise”, \nIEEE \nAerospace \nConference \nProceedings, vol. 4, pp 509-522, 2000 \n \n[4] Kazmana, R., In, H.P. & Chem, H-M. “From \nrequirements negotiation to software architecture \ndecisions”, Information and Software Technology, \nvol. 47, issue 8, pp. 511-520, 2005 \n \n[5] Nazlia, O., Hanna, P. & McKevitt, P.  “Heuristics-\nbased entity-relationship modelling through natural \nlanguage processing”, Proc. of the Fifteenth Irish \nConf. on Artificial Intelligence and Cognitive \nScience, pp. 302-313, 2004 \n \n[6] Bras, M. & Toussaint, Y. “Artificial intelligence \ntools for software engineering: Processing natural \nlanguage requirements”, Proc. of the Eighth Intl. \nConf. on Applications of Artificial Intelligence in \nEngineering, pp. 275-290, 1993 \n \n[7] Lee, B.-S. & Bryant, B.R. “Automated conversion \nfrom requirements documentation to an object-\noriented formal specification language”, Proc. of the \n2002 ACM symposium on Applied, pp. 932-936, \n2002 \n \n[8] Bryant, B. & Lee, B.-S., “Two-Level Grammar as an \nObject-Oriented \nRequirements \nSpecification \nLanguage”, Proc. of the 35th Annual Hawaii Intl. \nConf. on System Sciences, vol. 9, pp. 280-290, 2002  \n \n[9] Ambriola, V. and Gervasi, V. “Processing natural \nlanguage requirements”, Proc. 12th IEEE Intl. Conf. \non Automated Software Engineering, pp. 36-45, \n1997 \n \n[10] Earley, J. “An Efficient Context-Free Parsing \nAlgorithm”, CACM, vol. 13, no. 2, pp. 94-102, 1970 \n",
  "categories": [
    "cs.CL",
    "cs.SE"
  ],
  "published": "2014-07-23",
  "updated": "2014-07-23"
}