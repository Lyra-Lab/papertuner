{
  "id": "http://arxiv.org/abs/1409.5718v2",
  "title": "Convolutional Neural Networks over Tree Structures for Programming Language Processing",
  "authors": [
    "Lili Mou",
    "Ge Li",
    "Lu Zhang",
    "Tao Wang",
    "Zhi Jin"
  ],
  "abstract": "Programming language processing (similar to natural language processing) is a\nhot research topic in the field of software engineering; it has also aroused\ngrowing interest in the artificial intelligence community. However, different\nfrom a natural language sentence, a program contains rich, explicit, and\ncomplicated structural information. Hence, traditional NLP models may be\ninappropriate for programs. In this paper, we propose a novel tree-based\nconvolutional neural network (TBCNN) for programming language processing, in\nwhich a convolution kernel is designed over programs' abstract syntax trees to\ncapture structural information. TBCNN is a generic architecture for programming\nlanguage processing; our experiments show its effectiveness in two different\nprogram analysis tasks: classifying programs according to functionality, and\ndetecting code snippets of certain patterns. TBCNN outperforms baseline\nmethods, including several neural models for NLP.",
  "text": "Convolutional Neural Networks over Tree Structures\nfor Programming Language Processing\nLili Mou,1 Ge Li,1∗Lu Zhang,1 Tao Wang,2 Zhi Jin1∗\n1Software Institute, Peking University\n∗Corresponding authors\ndoublepower.mou@gmail.com, {lige,zhanglu,zhijin}@sei.pku.edu.cn\n2Stanford University, twangcat@stanford.edu\nAbstract\nProgramming language processing (similar to natural\nlanguage processing) is a hot research topic in the ﬁeld\nof software engineering; it has also aroused growing in-\nterest in the artiﬁcial intelligence community. However,\ndifferent from a natural language sentence, a program\ncontains rich, explicit, and complicated structural infor-\nmation. Hence, traditional NLP models may be inappro-\npriate for programs. In this paper, we propose a novel\ntree-based convolutional neural network (TBCNN) for\nprogramming language processing, in which a convo-\nlution kernel is designed over programs’ abstract syn-\ntax trees to capture structural information. TBCNN is\na generic architecture for programming language pro-\ncessing; our experiments show its effectiveness in two\ndifferent program analysis tasks: classifying programs\naccording to functionality, and detecting code snippets\nof certain patterns. TBCNN outperforms baseline meth-\nods, including several neural models for NLP.\nIntroduction\nResearchers from various communities are showing growing\ninterest in applying artiﬁcial intelligence (AI) techniques to\nsolve software engineering (SE) problems (Dietz et al. 2009;\nBettenburg and Begel 2013; Hao et al. 2013). In the area of\nSE, analyzing program source code—called programming\nlanguage processing in this paper—is of particular impor-\ntance.\nEven though computers can run programs, they do not\ntruly “understand” programs. Analyzing source code pro-\nvides a way of estimating programs’ behavior, functionality,\ncomplexity, etc. For instance, automatically detecting source\ncode snippets of certain patterns help programmers to dis-\ncover buggy or inefﬁcient algorithms so as to improve code\nquality. Another example is managing large software repos-\nitories, where automatic source code classiﬁcation and tag-\nging are crucial to software reuse. Programming language\nprocessing, in fact, serves as a foundation for many SE tasks,\ne.g., requirement analysis (Ghabi and Egyed 2012), software\ndevelopment and maintenance (Bettenburg and Begel 2013).\nHindle et al. (2012) demonstrate that programming lan-\nguages, similar to natural languages, also contain abundant\nCopyright c⃝2016, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nstatistical properties, which are important for program anal-\nysis. These properties are difﬁcult to capture by humans,\nbut justify learning-based approaches for programming lan-\nguage processing. However, existing machine learning pro-\ngram analysis depends largely on feature engineering, which\nis labor-intensive and ad hoc to a speciﬁc task, e.g., code\nclone detection (Chilowicz, Duris, and Roussel 2009), and\nbug detection (Steidl and Gode 2013). Further, evidence\nin the machine learning literature suggests that human-\nengineered features may fail to capture the nature of data,\nso they may be even worse than automatically learned ones.\nThe deep neural network, also known as deep learning, is\na highly automated learning machine. By exploring multi-\nple layers of non-linear transformation, the deep architec-\nture can automatically learn complicated underlying fea-\ntures, which are crucial to the task of interest. Over the\npast few years, deep learning has made signiﬁcant break-\nthroughs in various ﬁelds, such as speech recognition (Dahl,\nMohamed, and Hinton 2010), computer vision (Krizhevsky,\nSutskever, and Hinton 2012), and natural language process-\ning (Collobert and Weston 2008).\nDespite some similarities between natural languages and\nprogramming languages, there are also obvious differences\n(Pane, Ratanamahatana, and Myers 2001). Based on a for-\nmal language, programs contain rich and explicit structural\ninformation. Even though structures also exist in natural\nlanguages, they are not as stringent as in programs. Pinker\n(1994) illustrates an interesting example, “The dog the stick\nthe ﬁre burned beat bit the cat.” This sentence complies\nwith all grammar rules, but too many attributive clauses\nare nested. Hence, it can hardly be understood by people\ndue to the limitation of human intuition capacity. On the\ncontrary, three nested loops are common in programs. The\nparse tree of a program, in fact, is typically much larger\nthan that of a natural language sentence—there are approx-\nimately 190 nodes on average in our experiment, whereas\na sentence comprises only 20 words in a sentiment analy-\nsis dataset (Socher et al. 2013). Further, the grammar rules\n“alias” neighboring relationships among program compo-\nnents. The statements inside and outside a loop, for exam-\nple, do not form one semantic group, and thus are not se-\nmantically neighboring. On the above basis, we think more\neffective neural models are in need to capture structural in-\nformation in programs.\narXiv:1409.5718v2  [cs.LG]  8 Dec 2015\nIn this paper, we propose a novel Tree-Based Convolu-\ntional Neural Network (TBCNN) based on programs’ ab-\nstract syntax trees (ASTs). We also introduce the notion\nof “continuous binary trees” and apply dynamic pooling to\ncope with ASTs of different sizes and shapes. The TBCNN\nmodel is a generic architecture, and is applied to two SE\ntasks in our experiments—classifying programs by function-\nalities and detecting code snippets of certain patterns. It out-\nperforms baseline methods in both tasks, including the re-\ncursive neural network (Socher et al. 2011b) proposed for\nNLP. To the best of our knowledge, this paper is also the ﬁrst\nto apply deep neural networks to the ﬁeld of programming\nlanguage processing.1\nRelated Work\nDeep neural networks have made signiﬁcant breakthroughs\nin many ﬁelds. Stacked restricted Boltzmann machines and\nautoencoders are successful pretraining methods (Hinton,\nOsindero, and Teh 2006; Bengio et al. 2006). They explore\nthe underlying features of data in an unsupervised manner,\nand give a more meaningful initialization of weights for later\nsupervised learning with deep neural networks. These ap-\nproaches work well with generic data (e.g. data located in a\nmanifold embedded in a certain dimensional space), but they\nmay not be suitable for programming language processing,\nbecause programs contain rich structural information. Fur-\nther, AST structures also vary largely among different data\nsamples (programs), and hence they cannot be fed directly\nto a ﬁxed-size network.\nTo capture explicit structures in data, it may be impor-\ntant and beneﬁcial to integrate human priors to the net-\nworks (Bengio, Courville, and Vincent 2013). One exam-\nple is convolutional neural networks (CNNs, LeCun et al.\n1995; Krizhevsky, Sutskever, and Hinton 2012), which spec-\nify spatial neighboring information in data. CNNs work with\nsignals of a certain dimension (e.g., images); they also fail\nto capture tree-structural information as in programs.\nSocher et al. (2013, 2011b) propose a recursive neural net-\nwork (RNN) for NLP. Although structural information may\nbe coded to some extent in RNNs, the major drawback is\nthat only the root features are used for supervised learn-\ning, which buries illuminating information under a compli-\ncated neural architecture. RNNs also suffer from the difﬁ-\nculty of training due to the long dependency path during\nback-propagation (Bengio, Simard, and Frasconi 1994).\nSubsequent work. After the preliminary version of this\npaper was preprinted on arXiv,2 Zaremba and Sutskever\n(2014) use recurrent neural networks to estimate the output\nof restricted python programs. Piech et al. (2015) build re-\ncursive networks on Hoare triples. Regarding the proposed\nTBCNN, we extend it to process syntactic parse trees of nat-\nural languages (Mou et al. 2015); Duvenaud et al. (2015) ap-\nply a similar convolutional network over graphs to analyze\nmolecules.\n1We make our source code and the collected dataset available\nthrough our website (https://sites.google.com/site/treebasedcnn/).\n2 On 18 September 2014 (http://arxiv.org/abs/1409.5718v1).\nTree-Based Convolutional Neural Network\nProgramming languages have a natural tree representation—\nthe abstract syntax tree (AST). Figure 1a shows the AST of\nthe code snippet “int a=b+3;”.3 Each node in the AST\nis an abstract component in program source code. A node p\nwith children c1, · · · , cn represents the constructing process\nof the component p →c1 · · · cn.\nFigure 1b shows the overall architecture of TBCNN. In\nour model, an AST node is ﬁrst represented as a distributed,\nreal-valued vector so that the (anonymous) features of the\nsymbols are captured. The vector representations are learned\nby a coding criterion in our previous work (Peng et al. 2015).\nThen we design a set of subtree feature detectors, called\nthe tree-based convolution kernel, sliding over the entire\nAST to extract structural information of a program. We\nthereafter apply dynamic pooling to gather information over\ndifferent parts of the tree. Finally, a hidden layer and an out-\nput layer are added. For supervised classiﬁcation tasks, the\nactivation function of the output layer is softmax.\nIn the rest of this section, we ﬁrst explain the coding cri-\nterion for AST nodes’ representation learning, serving as a\npretraining phase of programming language processing. We\nthen describe the proposed TBCNN model, including a cod-\ning layer, a convolutional layer, and a pooling layer. We also\nprovide additional information on dealing with nodes that\nhave varying numbers of child nodes, as in ASTs, by intro-\nducing the notion of continuous binary trees.\nRepresentation Learning for AST Nodes\nVector representations, sometimes known as embeddings,\ncan capture underlying meanings of discrete symbols, like\nAST nodes. We propose in our previous work (Peng et al.\n2015) an unsupervised approach to learn program vector\nrepresentations by a coding criterion, which serves as a way\nof pretraining.\nA\ngeneric\ncriterion\nfor\nrepresentation\nlearning\nis\n“smoothness”—similar symbols have similar feature vec-\ntors (Bengio, Courville, and Vincent 2013). For example,\nthe symbols While and For are similar because both of\nthem are related to control ﬂow, particularly loops. But they\nare different from ID, since ID probably represents some\ndata. In our scenario, we would like the child nodes’ repre-\nsentations to “code” their parent node’s via a single neural\nlayer, during which both vector representations and coding\nweights are learned. Formally, let vec(·) ∈RNf be the fea-\nture representation of a symbol, where Nf is the feature di-\nmension. For each non-leaf node p and its direct children\nc1, · · · , cn, we would like\nvec(p) ≈tanh\n\u0010X\ni liWcode,i · vec(ci) + bcode\n\u0011\n(1)\nwhere Wcode,i\n∈\nRNf ×Nf is the weight matrix corre-\nsponding to the node ci; bcode ∈RNf is the bias. li =\n#leaves under ci\n#leaves under p\nis the coefﬁcient of the weight. (Weights\nWcode,i are weighted by leaf numbers.)\nBecause different nodes may have different numbers of\nchildren, the number of Wcode,i’s is not ﬁxed. To overcome\n3 Parsed by pycparser (https://pypi.python.org/pypi/pycparser/).\nTree-based\n(a)\nDecl\n(b)\nTypeDecl\nBinaryOp\nIdentifierType\nID\nConstant\nVector representation\nsoftmax\nfully\nconnected\nmax\npooling\nconvolution\nand coding\nfully\nconnected\nDynamic\npooling\nHidden\nOutput\nFigure 1: (a) Illustration of an AST, corresponding to the C code snippet “int a=b+3;” It should be notice that our model takes as\ninput the entire AST of a program, which is typically much larger. (b) The architecture of the Tree-Based Convolutional Neural\nNetwork (TBCNN). The main components in our model include vector representation and coding, tree-based convolution and\ndynamic pooling; then a fully-connected hidden layer and an output layer (softmax) are added.\nthis problem, we introduce the “continuous binary tree,”\nwhere only two weight matrices W l\ncode and W r\ncode serve as\nmodel parameters. Wi is a linear combination of the two pa-\nrameter matrices according to the position of node i. Details\nare deferred to the last part of this section.\nThe closeness between vec(p) and its coded vector is\nmeasured by Euclidean distance square, i.e.,\nd =\n\r\r\rvec(p) −tanh\n\u0010X\ni liWcode,i · vec(ci) + bcode\n\u0011\r\r\r\n2\n2\nTo prevent the pretraining algorithm from learning trivial\nrepresentations (e.g., 0’s will give 0 distance but are mean-\ningless), negative sampling is applied like Collobert et al.\n(2011). For each pretraining data sample p, c1, · · · , cn, we\nsubstitute one symbol (either p or one of c’s) with a random\nsymbol. The distance of the negative sample is denoted as\ndc, which should be at least larger than that of the positive\ntraining sample plus a margin ∆(set to 1 in our experiment).\nThus, the pretraining objective is to\nminimize\nW l\ncode,W r\ncode,bcode,vec(·) max {0, ∆+ d −dc}\nCoding Layer\nHaving pretrained the feature vectors for all symbols, we\nwould like to feed them forward to the tree-based convo-\nlutional layer for supervised learning. For leaf nodes, they\nare just the vector representations learned in the pretraining\nphase. For a non-leaf node p, it has two representations: the\none learned in the pretraining phase (left-hand side of Equa-\ntion 1), and the coded one (right-hand side of Equation 1).\nThey are linearly combined before being fed to the convolu-\ntional layer. Let c1, · · · , cn be the children of node p and we\ndenote the combined vector as p. We have\np = Wcomb1 · vec(p)\n+ Wcomb2 · tanh\n\u0010X\ni liWcode,i · vec(xi) + bcode\n\u0011\nwhere Wcomb1, Wcomb2 ∈RNf ×Nf are the parameters for\ncombination. They are initialized as diagonal matrices and\nthen ﬁne-tuned during supervised training.\nTree-based Convolutional Layer\nNow that each symbol in ASTs is represented as a dis-\ntributed, real-valued vector x ∈RNf , we apply a set of\nTOP\nLOWER_LEFT LOWER_RIGHT\n(a)\n(b)\n(c)\nFigure 2: (a) Tree-based convolution. Nodes on the left are\nthe feature vectors of AST nodes. They are either pretrained\nor combined with pretrained and coded vectors. (b) An illus-\ntration of 3-way pooling. (c) An analogy to the continuous\nbinary tree model. In the triangle, the color of a pixel is a\ncombination of three primary colors; in the convolution pro-\ncess, the weight for a node is a combination of three weight\nparameters, namely W t\nconv, W l\nconv, and W r\nconv.\nﬁxed-depth feature detectors sliding over the entire tree, de-\npicted in Figure 2a. The subtree feature detectors can be\nviewed as convolution with a set of ﬁnite support kernels.\nWe call this tree-based convolution.\nFormally, in a ﬁxed-depth window, if there are n nodes\nwith vector representations x1, · · · , xn, then the output of\nthe feature detectors is4\ny = tanh\n\u0010Xn\ni=1 Wconv,i · xi + bconv\n\u0011\nwhere y, bconv∈RNc, Wconv,i∈RNc×Nf . (Nc is the number\nof feature detectors.) 0’s are padded for nodes at the bottom\nthat do not have as many layers as the feature detectors. In\nour experiments, the kernel depth is set to 2.\nNote that, to deal with varying numbers of children, we\nalso adopt the notion of continuous binary tree. In this sce-\nnario, three weight matrices serve as model parameters,\nnamely W t\nconv, W l\nconv, and W r\nconv. Wconv,i is a linear combi-\nnation of these three matrices (explained in detail in the last\npart of this section).\n4 We used tanh as the activation function in TBCNN mainly\nbecause we hope to encode features to a same semantic space\n(−1, 1) during coding. We are grateful to an anonymous reviewer\nfor reminding us of using ReLU in convolution, and we are happy\nto try it in future work.\nDynamic Pooling\nAfter convolution, structural features in an AST are ex-\ntracted, and a new tree is generated. The new tree has ex-\nactly the same shape and size as the original one, which is\nvarying among different programs. Therefore, the extracted\nfeatures cannot be fed directly to a ﬁxed-size neural layer.\nDynamic pooling (Socher et al. 2011a) is applied to deal\nwith this problem.\nThe simplest approach, perhaps, is to pool all features to\none vector. We call this one-way pooling. Concretely, the\nmaximum value in each dimension is taken from the features\nthat are detected by tree-based convolution. We also propose\nan alternative, three-way pooling, where features are pooled\nto 3 parts, TOP, LOWER LEFT, and LOWER RIGHT, ac-\ncording to the their positions in the AST (Figure 2b). As we\nshall see from the experimental results, the simple one-way\npooling just works as well as three-way pooling. Therefore\nwe adopt one-way pooling in our experiments.\nAfter pooling, the features are fully connected to a hid-\nden layer and then fed to the output layer (softmax) for su-\npervised classiﬁcation. With the dynamic pooling process,\nstructural features along the entire AST reach the output\nlayer with short paths. Hence, they can be trained effectively\nby back-propagation.\nThe “Continuous Binary Tree” Model\nAs stated, one problem of coding and convolving is that we\ncannot determine the number of weight matrices because\nAST nodes have different numbers of children.\nOne possible solution is the continuous bag-of-words\nmodel (CBoW, Mikolov et al., 2013),5 but position informa-\ntion will be lost completely. Such approach is also used in\nHermann and Blunsom (2014). Socher et al. (2014) allocate\na different weight matrix as parameters for each position;\nbut this method fails to scale up since there will be a huge\nnumber of different positions in ASTs.\nIn our model, we view any subtree as a “binary” tree, re-\ngardless of its size and shape. That is, we have only three\nweight matrices as parameters for convolution, and two for\ncoding. We call it a continuous binary tree.\nTake convolution as an example. The three parameter ma-\ntrices are W t\nconv, W l\nconv, and W r\nconv. (Superscripts t, l, r refer\nto “top,” “left,” and “right.”) For node xi in a window, its\nweight matrix for convolution Wconv,i is a linear combina-\ntion of W t\nconv, W l\nconv, and W r\nconv, with coefﬁcients ηt\ni, ηl\ni, and\nηr\ni , respectively. The coefﬁcients are computed according to\nthe relative position of a node in the sliding window. Fig-\nure 2c is an analogy to the continuous binary tree model.\nThe equations for computing η’s are listed as follows.\n• ηt\ni = di−1\nd−1 (di: the depth of the node i in the sliding win-\ndow; d: the depth of the window.)\n• ηr\ni = (1 −ηt\ni) pi−1\nn−1 . (pi: the position of the node; n: the\ntotal number of p’s siblings.)\n• ηl\ni = (1 −ηt\ni)(1 −ηr\ni )\n5In their original paper, they do not deal with varying-length\ndata, but their method extends naturally to this scenario. Their\nmethod is also mathematically equivalent to average pooling.\nLikewise, the continuous binary tree for coding has two\nweight matrices W l\ncode and W r\ncode as parameters. The details\nare not repeated here.\nTo sum up, the entire parameter set for TBCNN is Θ =\n{W l\ncode, W r\ncode, Wcomb1, Wcomb2, W t\nconv, W l\nconv, W r\nconv, Whid,\nWout, bcode, bconv, bhid, bout, vec(·)}, where Whid, Wout, bhid,\nand bout are the weights and biases for the hidden and\noutput layers. To set up supervised training, W l\ncode, W r\ncode,\nbcode, and vec(·) are derived from the pretraining phase;\nWcomb1 and Wcomb2 are initialized as diagonal matrices;\nother parameters are initialized randomly. We apply the\ncross-entropy loss and use stochastic gradient descent,\ncomputed by back-propagation.\nExperiments\nWe ﬁrst assess the learned vector representations both quali-\ntatively and quantitatively. Then we evaluate TBCNN in two\nsupervised learning tasks, and conduct model analysis.\nThe dataset of our experiments comes from a pedagogi-\ncal programming open judge (OJ) system.6 There are a large\nnumber of programming problems on the OJ system. Stu-\ndents submit their source code as the solution to a certain\nproblem; the OJ system automatically judges the validity of\nsubmitted source code by running the program. We down-\nloaded the source code and the corresponding programming\nproblems (represented as IDs) as our dataset.\nUnsupervised Program Vector Representations\nWe applied the coding criterion of pretraining to all C code\nin the OJ system, and obtained AST nodes’ vector represen-\ntations.\nQualitative analysis. Figure 3a illustrates the hierarchical\nclustering result based on a subset of AST nodes. As demon-\nstrated, the symbols mainly fall into three categories: (1)\nBinaryOp, ArrayRef, ID, Constant are grouped to-\ngether since they are related to data reference/manipulation;\n(2) For, If, While are similar since they are related to\ncontrol ﬂow; (3) ArrayDecl, FuncDecl, PtrDecl are\nsimilar since they are declarations. The result is quite sen-\nsible because it is consistent with human understanding of\nprograms.\nQuantitative analysis. We also evaluated pretraining’s\neffect on supervised learning by feeding the learned repre-\nsentations to a program classiﬁcation task. (See next subsec-\ntion.) Figure 3b plots the learning curves of both training and\nvalidation, which are compared with random initialization.\nUnsupervised vector representation learning accelerates the\nsupervised training process by nearly 1/3, showing that pre-\ntraining does capture underlying features of AST nodes, and\nthat they can emerge high-level features spontaneously dur-\ning supervised learning. However, pretraining has a limited\neffect on the ﬁnal accuracy. One plausible explanation is that\nthe number of AST nodes is small: the pycparser, we\nuse, distinguishes only 44 symbols. Hence, their representa-\ntions can be adequately tuned in a supervised fashion.\n6 http://programming.grids.cn\n(a)\n(b)\nFigure 3: Analysis of vector representations. (a) Hierarchical\nclustering based on AST nodes’ vector representations. (b)\nLearning curves with and without pretraining.\nStatistics\nMean\nSample std.\n# of code lines\n36.3\n19.0\n# of AST nodes\n189.6\n106.0\nAverage leaf nodes’ depth in an AST\n7.6\n1.7\nMax depth of an AST\n12.3\n3.2\nTable 1: Statistics of our dataset.\nHyperparameter\nValue\nHow is the value chosen?\nInitial learning rate\n0.3\nBy validation\nLearning rate decay\nNone\nEmpirically\nEmbedding dimension\n30\nEmpirically\nConvolutional layers’ dim.\n600\nBy validation\nPenultimate layer’s dim.\n600\nSame as conv layers\nl2 penalty\nNone\nEmpirically\nTable 2: TBCNN’s hyperparameters.\nNonetheless, we think the pretraining criterion is effec-\ntive and beneﬁcial for TBCNN, because training deep neural\nnetworks is usually time-consuming, especially when tuning\nhyperparameters. The pretrained vector representations are\nused throughout the experiments below.\nClassifying Programs by Functionalities\nTask description\nIn software engineering, classifying\nprograms by functionalities is an important problem for var-\nious software development tasks. For example, in a large\nsoftware repository (e.g., SourceForge), software products\nare usually organized into categories, a typical criterion for\nwhich is by functionalities. With program classiﬁcation, it\nbecomes feasible to automatically tag a software component\nnewly added into the repository, which is beneﬁcial for soft-\nware reuse during the development process.\nIn our experiment, we applied TBCNN to classify source\ncode in the OJ system. The target label of a data sample is\none of 104 programming problems (represented as an ID).\nThat is, programs with a same target label have the same\nfunctionality. We randomly chose exactly 500 programs in\neach class, and thus 52,000 samples in total, which were\nfurther randomly split by 3:1:1 for training, validation, and\ntesting. Relevant statistics are shown in Table 1.\nHyperparameters\nTBCNN’s hyperparameters are shown\nin Table 2. Our competing methods include SVM and a deep\nfeed-forward neural network based on hand-crafted features,\nnamely bag-of-words (BoW, the counting of each symbol)\nor bag-of-tree (BoT, the counting of 2-layer subtrees). We\nalso compare our model with the recursive neural network\nGroup\nMethod\nTest Accuracy (%)\nlinear SVM+BoW\n52.0\nSurface\nRBF SVM+BoW\n83.9\nfeatures\nlinear SVM+BoT\n72.5\nRBF SVM+BoT\n88.2\nDNN+BoW\n76.0\nNN-based\nDNN+BoT\n89.7\napproaches\nVector avg.\n53.2\nRNN\n84.8\nOur method\nTBCNN\n94.0\nTable 3: The accuracy of 104-label program classiﬁcations.\n(RNN, Socher et al. 2011b). Hyperparameters for baselines\nare listed as follows.\nSVM. The linear SVM has one hyperparameter C; RBF\nSVM has two, C and γ. They are tuned by validation over\nthe set {· · · , 1, 0.3, 0.1, 0.03, · · · } with grid search.\nDNN. We applied a 4-layer DNN (including input) em-\npirically. The hidden layers’ dimension is 300, chosen from\n{100, 300, 1000}; learning rates are 0.003 for BoW and 0.03\nfor BoT, chosen from {0.003, · · · , 0.3} with granularity 3x.\nℓ2 regularization coefﬁcient is 10−6 for both BoW and BoT,\nchosen from {10−7, · · · , 10−4} with granularity 10x, and\nalso no regularization.\nRNN.\nRecursive\nunits\nare\n600-dimensional,\nas\nin\nour method. The learning rate is chosen from the set\n{· · · 1.0, 0.3, 0.1 · · · }, and 0.3 yields the highest validation\nperformance.\nResults\nTable 3 presents the results in the 104-label\nprogram classiﬁcation experiment. Using SVM with sur-\nface features does distinguish different programs to some\nextent—for example, a program about string manipulation\nis different from, say, matrix operation; also, a difﬁcult pro-\ngramming problem necessitates a more complex program,\nand thus more lines of code and AST nodes. However, their\nperformance is comparatively low.\nWe tried deep feed-forward neural networks on these fea-\ntures, and achieved accuracies of 76.0–89.7%, comparable\nto SVMs. Vector averaging with softmax—another neural\nnetwork-based competing method applied in NLP (Socher et\nal. 2013; Kalchbrenner, Grefenstette, and Blunsom 2014)—\nyields an accuracy similar to a linear classiﬁer built on\nBoW features. This is probably because the number of AST\nsymbols is far fewer than words in natural languages, and\nthus the vector representations (provided non-singular) can\nbe absorbed into the classiﬁer’s weights. Comparing these\napproaches with our method, we deem TBCNN’s perfor-\nmance boost is not merely caused by using a better clas-\nsiﬁer (neural networks versus SVM, say), but also the fea-\nture/representation learning nature, which enables automatic\nstructural feature extraction.\nWe also applied RNN to the program classiﬁcation\ntask7; the RNN’s accuracy is lower than shallow methods\n7We do not use the pretrained vector representations, which are\ninimical to RNN: the weight Wcode codes children’s representation\nto its candidate parent’s; adversely, the high-level nodes in pro-\ngrams (e.g., a function deﬁnition) are typically non-informative.\nClassiﬁer\nFeatures\nAccuracy\nRand/majority\n–\n50.0\nRBF SVM\nBag-of-words\n62.3\nRBF SVM\nBag-of-trees\n77.1\nTBCNN\nLearned\n89.1\nTable 4: Accuracy of detecting bubble sort (in percentage).\nModel Variant\nValidation Acc.\nCoding layer →None\n92.3\n1-way pooling →3-way\n94.3\nContinuous binary tree →CBoW\n93.1\nTBCNN with the best gadgets\n94.4\nTable 5: Effect of coding, pooling, and the continuous binary\ntree.\n(SVM+BoT). Taking into consideration experiments in NLP\n(Socher et al. 2011b; Socher et al. 2013), we observe a\ndegradation of RNN’s performance if the tree structure is\nlarge.\nTBCNN outperforms the above methods, yielding an ac-\ncuracy of 94%. By exploring tree-based convolution, our\nmodel is better at capturing programs’ structural features,\nwhich is important for program analysis.\nDetecting Bubble Sort\nTask description\nTo further evaluate our TBCNN model\nin a more realistic SE scenario, we used it to detect an un-\nhealthy code pattern, bubble sort, which can also be regarded\nas a (binary) program classiﬁcation task. Detecting source\ncode of certain patterns is closely related to many SE prob-\nlems. In this experiment, bubble sort is thought of as un-\nhealthy code because it implements an inefﬁcient algorithm.\nBy identifying such unhealthy code, project managers can\nreﬁne the implementations during the maintenance process.\nBefore the experiment, a volunteer8 annotated, from the\nOJ system, 109 programs that contain bubble sort, and 109\nprograms that do not contain bubble sort. They were split\n1:1 for validation and testing.\nData augmentation\nTo train our TBCNN model, a dataset\nof such scale is insufﬁcient. We propose a simple yet useful\ndata augmentation technique for programs. Concretely, we\nused the source code of 4k programs in the OJ system as the\nnon-bubble sort class. For each program, we randomly sub-\nstituted a fragment of program statements with a pre-written\nbubble sort snippet. Thus we had 8k data samples in total.\nResults\nWe tested our model on the annotated real-world\nprograms. Note that the test samples were written by real-\nworld programmers, and thus the styles and forms of bubble\nsort snippets may differ from the training set, for example,\nsorting an integer array versus sorting a user-deﬁned struc-\nture, and sorting an array versus sorting two arrays simulta-\nneously. As we see in Table 4, bag-of-words features are not\nilluminating in this classiﬁcation and yield a low accuracy of\n8The volunteer has neither authorship nor a conﬂict of interests.\nFigure 4: Validation accuracy versus the number of convo-\nlution units.\n62.3%. Bag-of-trees features are better, and achieve 77.06%.\nOur model outperforms these methods by more than 10%.\nThis experiment also suggests that neural networks can learn\nmore robust features than just counting surface statistics.\nModel Analysis\nWe now analyze each gadget of TBCNN quantitatively, with\nthe 104-label program classiﬁcation as our testbed. We re-\nport validation accuracies throughout this part.\nEffect of coding layer\nIn the proposed TBCNN model for\nprogram analysis, we represent a non-leaf node by combin-\ning its coded representation and its pretrained one. We ﬁnd\nthat, the underneath coding layer can also integrate global\ninformation in addition to merely averaging two homoge-\nneous sources. If we build a tree-based convolutional layer\ndirectly on the pretrained vector representations, all struc-\ntural features are “local,” that is, conﬁned in the convolution\nwindow. The lack of integrating global information leads to\n2% degradation in performance. (See the ﬁrst and last rows\nin Table 5.)\nLayers’ dimensions\nIn our experiments, AST nodes’ vec-\ntor representations are set to be 30-dimensional empirically.\nWe chose this small value because AST nodes have only 44\ndifferent symbols. Hence, the dimension needs to be, intu-\nitively, smaller than words’ vector representations, e.g., 300\nin Mou et al. (2015). The dimension of convolution, i.e.,\nthe number of feature detectors, was chosen by validation\n(Figure 4). We tried several conﬁgurations, among which\n600-dimensional convolution results in the highest valida-\ntion accuracy. This analysis also veriﬁes that programs have\nrich structural information, even though the number of AST\nsymbols is not large. As the rich semantics are emerged by\ndifferent combinations of AST symbols, we are in need of\nmore feature detectors, that is, a larger convolutional layer.\nEffect of pooling layer\nWe tried two pooling methods in\nour TBCNN model, and compare them in Table 5 (the sec-\nond and last rows). 3-way pooling is proposed in hope of\npreserving features from different parts of the tree. However,\nas indicated by the experimental result, the simple 1-way\npooling works just as ﬁne (even 0.1% higher on the valida-\ntion set). This suggests that TBCNN is not sensitive to pool-\ning methods, which mainly serve as a necessity for packing\nvarying sized and shaped data. Further development can be\naddressed in future work.\nEffect of continuous binary tree\nThe continuous binary\ntree is introduced to treat nodes with different numbers of\nchildren, as well as to capture order information of child\nnodes. We also implemented the continuous bag-of-words\n(CBoW) model, where child nodes’ representations are av-\neraged before convolution. Rows 4 and 5 in Table 5 compare\nour proposed continuous binary tree and the above alterna-\ntive. The result shows a boost of 1.3% in considering child\nnodes’ order information.\nConclusion\nIn this paper, we applied deep neural networks to the ﬁeld\nof programming language processing. Due to the rich and\nexplicit tree structures of programs, we proposed the novel\nTree-Based Convolutional Neural Network (TBCNN). In\nour model, program vector representations are learned by\nthe coding criterion; structural features are detected by the\nconvolutional layer; the continuous binary tree and dynamic\npooling enable our model to cope with trees of varying sizes\nand shapes. Experimental results show the superiority of our\nmodel to baseline methods.\nAcknowledgments\nWe would like to thank anonymous reviewers for insight-\nful comments; we also thank Xiaowei Sun for annotating\nbubble sort programs, Yuxuan Liu for data processing, and\nWeiru Liu for discussion on the manuscript. This research\nis supported by the National Basic Research Program of\nChina (the 973 Program) under Grant No. 2015CB352201\nand the National Natural Science Foundation of China un-\nder Grant Nos. 61421091, 61232015, 61225007, 91318301,\nand 61502014.\nReferences\n[2006] Bengio, Y.; Lamblin, P.; Popovici, D.; and Larochelle, H.\n2006. Greedy layer-wise training of deep networks. In NIPS.\n[2013] Bengio, Y.; Courville, A.; and Vincent, P. 2013. Repre-\nsentation learning: A review and new perspectives. IEEE Trans.\nPattern Anal. Mach. Intell. 35(8):1798–1828.\n[1994] Bengio, Y.; Simard, P.; and Frasconi, P. 1994. Learning\nlong-term dependencies with gradient descent is difﬁcult. IEEE\nTrans. Neural Networks 5(2):157–166.\n[2013] Bettenburg, N., and Begel, A.\n2013.\nDeciphering the\nstory of software development through frequent pattern mining.\nIn ICSE, 1197–1200.\n[2009] Chilowicz, M.; Duris, E.; and Roussel, G. 2009. Syntax\ntree ﬁngerprinting for source code similarity detection. In Proc.\nIEEE Int. Conf. Program Comprehension, 243–247.\n[2008] Collobert, R., and Weston, J. 2008. A uniﬁed architec-\nture for natural language processing: Deep neural networks with\nmultitask learning. In ICML.\n[2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.;\nKavukcuoglu, K.; and Kuksa, P. 2011. Natural language pro-\ncessing (almost) from scratch. JRML 12:2493–2537.\n[2010] Dahl, G.; Mohamed, A.; and Hinton, G.\n2010.\nPhone\nrecognition with the mean-covariance restricted Boltzmann ma-\nchine. In NIPS.\n[2009] Dietz, L.; Dallmeier, V.; Zeller, A.; and Scheffer, T. 2009.\nLocalizing bugs in program executions with graphical models. In\nNIPS.\n[2015] Duvenaud, D.; Maclaurin, D.; Aguilera-Iparraguirre, J.;\nG´omez-Bombarelli, R.; Hirzel, T.; Aspuru-Guzik, A.; and\nAdams, R. 2015. Convolutional networks on graphs for learn-\ning molecular ﬁngerprints. arXiv preprint arXiv:1509.09292.\n[2012] Ghabi, A., and Egyed, A. 2012. Code patterns for automat-\nically validating requirements-to-code traces. In ASE, 200–209.\n[2013] Hao, D.; Lan, T.; Zhang, H.; Guo, C.; and Zhang, L. 2013.\nIs this a bug or an obsolete test? In Proc. ECOOP, 602–628.\n[2014] Hermann, K., and Blunsom, P. 2014. Multilingual models\nfor compositional distributed semantics. In ACL, 58–68.\n[2012] Hindle, A.; Barr, E.; Su, Z.; Gabel, M.; and Devanbu, P.\n2012. On the naturalness of software. In ICSE, 837–847.\n[2006] Hinton, G.; Osindero, S.; and Teh, Y. 2006. A fast learning\nalgorithm for deep belief nets. Neural Computation 18(7):1527–\n1554.\n[2014] Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014.\nA convolutional neural network for modelling sentences. In ACL,\n655–665.\n[2012] Krizhevsky, A.; Sutskever, I.; and Hinton, G. 2012. Ima-\ngeNet classiﬁcation with deep convolutional neural networks. In\nNIPS.\n[1995] LeCun, Y.; Jackel, L.; Bottou, L.; Brunot, A.; Cortes, C.;\nDenker, J.; Drucker, H.; Guyon, I.; Muller, U.; and Sackinger, E.\n1995. Comparison of learning algorithms for handwritten digit\nrecognition. In Proc. Int. Conf. Artiﬁcial Neural Networks.\n[2013] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.; and\nDean, J. 2013. Distributed representations of words and phrases\nand their compositionality. In NIPS.\n[2015] Mou, L.; Peng, H.; Li, G.; Xu, Y.; Zhang, L.; and Jin, Z.\n2015.\nDiscriminating neural sentence modeling by tree-based\nconvolution. In EMNLP, 2315–2325.\n[2001] Pane, J.; Ratanamahatana, C.; and Myers, B. 2001. Study-\ning the language and structure in non-programmers’ solutions\nto programming problems.\nInt. J. Human-Computer Studies\n54(2):237–264.\n[2015] Peng, H.; Mou, L.; Li, G.; Liu, Y.; Zhang, L.; and Jin, Z.\n2015. Building program vector representations for deep learn-\ning. In Proc. 8th Int. Conf. Knowledge Science, Engineering and\nManagement, 547–553.\n[2015] Piech, C.; Huang, J.; Nguyen, A.; Phulsuksombati, M.; Sa-\nhami, M.; and Guibas, L. 2015. Learning program embeddings\nto propagate feedback on student code. In ICML.\n[1994] Pinker, S. 1994. The Language Instinct: The New Science\nof Language and Mind. Pengiun Press.\n[2011a] Socher, R.; Huang, E.; Pennin, J.; Manning, C.; and Ng,\nA.\n2011a.\nDynamic pooling and unfolding recursive autoen-\ncoders for paraphrase detection. In NIPS.\n[2011b] Socher, R.; Pennington, J.; Huang, E.; Ng, A.; and Man-\nning, C. 2011b. Semi-supervised recursive autoencoders for pre-\ndicting sentiment distributions. In EMNLP, 151–161.\n[2013] Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C.;\nNg, A.; and Potts, C. 2013. Recursive deep models for semantic\ncompositionality over a sentiment treebank. In EMNLP, 1631–\n1642.\n[2014] Socher, R.; Karpathy, A.; Le, Q.; Manning, C.; and Ng,\nA. Y. 2014. Grounded compositional semantics for ﬁnding and\ndescribing images with sentences. TACL 2:207–218.\n[2013] Steidl, D., and Gode, N. 2013. Feature-based detection of\nbugs in clones. In 7th Int. Workshop on Software Clones, 76–82.\n[2014] Zaremba, W., and Sutskever, I. 2014. Learning to execute.\narXiv preprint arXiv:1410.4615.\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "cs.SE"
  ],
  "published": "2014-09-18",
  "updated": "2015-12-08"
}