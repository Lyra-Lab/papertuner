{
  "id": "http://arxiv.org/abs/1409.7612v1",
  "title": "Semi-supervised Classification for Natural Language Processing",
  "authors": [
    "Rushdi Shams"
  ],
  "abstract": "Semi-supervised classification is an interesting idea where classification\nmodels are learned from both labeled and unlabeled data. It has several\nadvantages over supervised classification in natural language processing\ndomain. For instance, supervised classification exploits only labeled data that\nare expensive, often difficult to get, inadequate in quantity, and require\nhuman experts for annotation. On the other hand, unlabeled data are inexpensive\nand abundant. Despite the fact that many factors limit the wide-spread use of\nsemi-supervised classification, it has become popular since its level of\nperformance is empirically as good as supervised classification. This study\nexplores the possibilities and achievements as well as complexity and\nlimitations of semi-supervised classification for several natural langue\nprocessing tasks like parsing, biomedical information processing, text\nclassification, and summarization.",
  "text": "arXiv:1409.7612v1  [cs.CL]  25 Sep 2014\nSemi-supervised Classiﬁcation for\nNatural Language Processing\nRushdi Shams\nDepartment of Computer Science, University of Western Ontario,\nLondon, ON N6A 5B7, Canada.\nemail: rshams@csd.uwo.ca.\nAbstract—Semi-supervised classiﬁcation is an interesting idea\nwhere classiﬁcation models are learned from both labeled and\nunlabeled data. It has several advantages over supervised clas-\nsiﬁcation in natural language processing domain. For instance,\nsupervised classiﬁcation exploits only labeled data that are ex-\npensive, often difﬁcult to get, inadequate in quantity, and require\nhuman experts for annotation. On the other hand, unlabeled\ndata are inexpensive and abundant. Despite the fact that many\nfactors limit the wide-spread use of semi-supervised classiﬁcation,\nit has become popular since its level of performance is empirically\nas good as supervised classiﬁcation. This study explores the\npossibilities and achievements as well as complexity and limita-\ntions of semi-supervised classiﬁcation for several natural langue\nprocessing tasks like parsing, biomedical information processing,\ntext classiﬁcation, and summarization.\nKeywords: Semi-supervised learning, classiﬁcation, natural\nlanguage processing, data mining.\nI.\nINTRODUCTION\nClassical supervised methods use labeled data to train\ntheir classiﬁer models. These methods are widespread and\nused in many different domains including natural language\nprocessing. The key material used in different natural language\nprocessing tasks is text. The number of text, however, is\nincreasing everyday due to the pervasive use of computing.\nThere are more unlabeled than labeled text since data labeling\nis expensive due to engagement of human for data annotation.\nThe process also consumes time. These difﬁculties have serious\neffects on supervised learning since a good ﬁt of a classiﬁer\nmodel requires as much labeled data as possible for its training\n[1].\nSemi-supervised learning can be a good means to overcome\nthe aforementioned problems. The basic principle of semi-\nsupervised learning is simple: use both unlabeled and labeled\ndata to generate classiﬁer models. This makes semi-supervised\nlearning substantially useful for a domain like natural lan-\nguage processing because of a good note, unlabeled text is\ninexpensive, abundant, and more available than labeled text.\nIn addition, empirically, semi-supervised models have good\nperformance records. On many tasks, they are as good as\nsupervised models and in most cases, they are better than\ncluster-based, unsupervised models [2]. However, many of\nthese results are not conclusive and proper care therefore\nshould be taken due to some serious concerns related to semi-\nsupervised learning.\nThis study explores the use of semi-supervised learning\nfor natural language processing. Interestingly, like traditional\nmachine learning methods, semi-supervised learning can be\nused to solve classiﬁcation, regression, and clustering prob-\nlems. The particular focus of this study, however, is on semi-\nsupervised classiﬁcation. In this study, popular research papers\nand classic books are explored to outline the possibilities and\nachievements of semi-supervised classiﬁcation for natural lan-\nguage processing. As well, thorough investigations are carried\nout, both from theoretical and empirical data, to explain the\ncomplexity and limitations of this method. Natural language\nprocessing is one of the largest research areas of artiﬁcial\nintelligence. The scope of this study is, however, limited to\nmost popular tasks such as parsing, biomedical information\nprocessing, text classiﬁcation, and summarization.\nThe organization of the paper is as follows. Section II\npresents an overview of semi-supervised learning that includes\nlearning problems, and different types of semi-supervised\nalgorithms and learning techniques. Following that Section III\noutlines the use of semi-supervised classiﬁcation for different\nnatural language processing tasks. In Section IV, several\nconsiderations and conclusions are drawn.\nII.\nOVERVIEW OF SEMI-SUPERVISED LEARNING\nUnlike\nsupervised\nand\nunsupervised\nlearning,\nsemi-\nsupervised learning exploits both labeled and unlabeled data.\nTo start with, semi-supervised methods train models with a\nvery little labeled data. Surprisingly, test results show that\nmarginal labeled data are sufﬁcient to train models with good\nﬁt for semi-supervised learning [3]. The generated models are\nthen applied on unlabeled data in an attempt to label them.\nThe conﬁdence of the models in labeling them is measured\nagainst a conﬁdence threshold set a priori by users. Note that\nFig. 1: The overview of semi-supervised learning. The ﬁgure\nalso outlines the scope of transductive and inductive learning.\n(a) Supervised decision boundaries for labeled data.\n(b) Supervised and semi-supervised decision boundaries for\nlabeled and unlabeled data.\nFig. 2: Supervised and semi-supervised decision boundaries drawn by a random classiﬁer for two labeled and 100 unlabeled data\n[2]. In 2a, the supervised decision boundary is in the middle by averaging the values of the data points. In 2b, the supervised\ndecision boundary produces more classiﬁcation errors due to the distribution of the data.\nlearning algorithms often have their own conﬁdence measures\nthat generally depend on their working principles. For instance,\nclass probability values for each data instance are considered\nas conﬁdence measures for Na¨ıve Bayes models [4]. For an\nunlabeled data, if the models reach the pre-set conﬁdence\nthreshold, then the newly labeled data are added to the pool of\noriginally labeled data. This process continues unless (i) the\nmodels’ conﬁdences for the labels stop reaching the threshold,\nor (ii) the models conﬁdently label all the unlabeled data and\nthere are no unlabeled data remaining in the dataset. The\ninteresting cycle of labeling and re-labeling of semi-supervised\nlearning is illustrated in Figure 1.\nA. Learning Problems\nSemi-supervised learning problems can be broadly cat-\negorized into two groups: (i) transductive learning and (ii)\ninductive learning. Transductive learning is like a take-home\nexam. This group of semi-supervised learning tries to evaluate\nthe goodness of a model assumption on unlabeled data after\ntraining a classiﬁer with the available labeled data. Inductive\nlearning, on the contrary, is often seen as an in-class exam—\nit evaluates the goodness of a model assumption on unseen,\nunlabeled test data after training a classiﬁer with both labeled\nand unlabeled data. Figure 1 shows the boundaries between\nthese two types of semi-supervised learning. While the entire\ncycle in the ﬁgure illustrates inductive learning, steps 1—3\ndescribe transductive learning.\nB. Working Principle\nFigure 2 can be referred to understand how semi-supervised\nlearning works with a very few labeled but abundant unlabeled\ndata. Figure 2a shows that based on the position of a positive\n(x = 1) and a negative (x = −1) labeled data, a supervised\ndecision boundary is drawn right at x = 0 based on the average\nof the data points. However, given only these two labeled data\nand 100 unlabeled data (represented by green dots in Figure\n2b), this supervised decision boundary still remains at x = 0.\nIn contrast, had a semi-supervised classiﬁer been used, the\nboundary would have shifted more to the right (say some point\nat x = 0.4) than the supervised decision boundary. This shift\nis due to the distribution of unlabeled data points considering\nthe position of the positive and negative examples. In this\nparticular case, the semi-supervised classiﬁer assumes that the\ngreen dots near to the red cross point form one kind of data\ndistribution while the green dots near to the blue circle form\na different one. Interestingly, semi-supervised learning fails in\nmany intriguing cases, where the distributions of labeled and\nunlabeled data are not as distinguishable as seen in Figure 2.\nC. Types of Algorithms\nThere are several semi-supervised algorithms and most\nof them can be categorized into two groups based on their\nproperties: (i) generative algorithms and (ii) discriminative\nalgorithms. The models generated by these two types of\nalgorithms are therefore called generative and discriminative\nmodels, respectively. The following can explain the key dif-\nference between the two types of models. Say, we are given\na set of speeches given by human presenters. As well, a set\nof languages are provided. The task is to simply classify\nevery speech into one of the languages. This learning problem\ncan be solved in either of the following two ways: ﬁrst, the\nlearner learns each language and then attempts to classify\nthe speeches according to its learning. Second, the learner\nlearns the differences among the speeches according to various\nattributes or features present in them and then attempts to\nclassify the speeches according to its learning. Note that\nfor the second case, the learner does not need to learn all\nthe languages. The prior is called a generative learner and\nthe latter is known as a discriminative learner. Let us take\na look into these two types of algorithms mathematically.\nSay, we are given a set of instances x and their classes y\nin the form of (x, y): (1, 0), (1, 0), (2, 0), (2, 1). Generative\nalgorithms attempt to ﬁnd out the joint probability, p(x, y)\n(a) Joint probability calculated\nby generative algorithms\n(b) Conditional probability cal-\nculated by discriminative algo-\nrithms.\nFig. 3: Probability distribution of the data as seen by generative\nand discriminative algorithms. Generative algorithms calculate\nthe joint probability distribution of the data while discrimina-\ntive algorithms deal with their conditional probability.\nfrom these data (see Figure 3a) while discriminative algorithms\ncalculate their conditional probability, p(x|y) (Figure 3b).\nNow, for supervised algorithms a discriminative model predicts\nthe label y from training example x as follows:\nf(x) = argmax\ny\np(y|x).\n(1)\nHowever, from the Baye’s theorem, we know that\np(y|x) = p(x|y)p(y)\np(x)\n.\n(2)\nHowever, for Equation 1, p(x) can be ignored since it ﬁnds\nthe function, f(x) for the maximum value of y. Therefore,\nignoring p(x) in Equation 2 gives us\nf(x) = argmax\ny\np(x|y)p(y).\n(3)\nInterestingly, Equation 3 is what supervised, generative\nalgorithms use to induce their models. In other words, for\nsupervised algorithms, Equation 1 is used to ﬁnd out class\nboundaries based on the given training instances x and Equa-\ntion 3 is used to generate x for any given y. The latter,\nhowever, is not found as easily for semi-supervised algorithms\nas for supervised algorithms. The ﬁrst and foremost reason\nfor this is that in semi-supervised problems, the algorithms\ncannot completely ignore p(x) because most of what it has are\nthe distributions of training examples (i.e., p(x)). Moreover,\nfor semi-supervised algorithms, a very few class labels are\nprovided (for training examples) and therefore from the few\ngiven y’s, the conditional probabilities, p(x|y) are difﬁcult to\ngenerate. This is a key difference between supervised and\nsemi-supervised algorithms. An example is provided to un-\nderstand the difference better. For semi-supervised algorithms,\nEquation 1 can be substituted by\np(y|x) =\np(x|y)p(y)\nP\ny′ p(x|y′)p(y′),\n(4)\nwhere y′ denotes the classes of the few given training examples\nx. Equation 4 has a probability density function p(x|y) in its\nnumerator. If the distribution of x comes from a Gaussian and\nit is a function of mean vector and covariance matrix of the\nGaussian, then using a Maximum Likelihood Estimate, the\nmean vector and covariance matrix can be tuned to maximize\nthe density function. Thereafter, this tuning can be optimized\nusing an Expectation-Maximization (EM) algorithm. Note that\naccording to the distribution of x, different algorithms use\ndifferent techniques for tuning and optimizing the density\nfunction p(x|y) in Equation 4. Among the semi-supervised\nalgorithms, Transductive Support Vector Machine (TSVM) and\ngraph-based methods are generative algorithms while EM and\nself-learning are discriminative algorithms.\nD. Types of Learning\nThe semi-supervised learning can be broadly categorized\ninto three: (i) self-training, (ii) co-training, and (iii) active\nlearning.\n1) Self-training: In self-training, from a set of initially\nlabeled data L, a classiﬁer, C1 is generated. This classiﬁer is\nthen applied on a set of initially unlabeled data U. According to\na pre-set conﬁdence threshold, the classiﬁcations of unlabeled\ndata are observed. If the classiﬁer’s conﬁdence reaches the\nthreshold, the newly classiﬁed instances are concatenated with\nL to produce a set Lnew and removed from U to produce Unew.\nA second classiﬁer, C2 is generated from Lnew, and thereafter\napplied on Unew. This cycle continues until the classiﬁer\nconverges—which means that either (a) all the unlabeled data\nare conﬁdently labeled by the classiﬁer or (b) the classiﬁer’s\nconﬁdence stops reaching the threshold for several cycles.\nSelf-training is very simple and particularly useful if the\nsupervised algorithm is difﬁcult to modify. Nonetheless, self-\ntraining performs poorly for datasets that contain large number\nof outliers.\n2) Co-training: In contrast to self-training, for co-training,\ntwo partitions L1 and L2 are created from the initially labeled\ndata L. The partitions are based on two different sets of\nattributes or features V1 and V2 (in semi-supervised literature,\nthey are often referred to as views). Then, two classiﬁers\nindependently generates respective models F1 and F2 from L1\nand L2 using V1 and V2. Following that, from the unlabeled\ndata pool U, k most conﬁdent predictions of F1 are added to\nL2 and k most conﬁdent predictions of F2 are added to L1.\nThese added examples are removed from U. F1 is re-trained\nwith L1 and F2 is re-trained with L2. This cycle continues until\nthe classiﬁers converge. Finally, using a voting or averaging\nmethod, test data are classiﬁed. Note that co-training can be\nseen as self-training with two or more classiﬁers. Co-training\nis very useful if the attributes or features naturally split into\ntwo distinguishable sets. However, there are two important\nconditions that should be met for co-training to work. Given\nenough labeled data,\n1)\neach view alone should be sufﬁcient to make good\nclassiﬁcations and\n2)\nthe co-trained algorithms should individually perform\ngood.\n3) Active Learning: Finally, for active learning a model is\ngenerated from labeled data and attempts to classify unlabeled\ninstances. The classiﬁcation it makes is then provided to a\nhuman expert called the oracle for her judgment. The correctly\nlabeled instances according to the oracle are then added to the\npool of labeled data while the instances with incorrect labels\nremain in the unlabeled data pool. This process continues until\nthe unlabeled data pool becomes empty. Active learning is very\nuseful for limited available data (both labeled and unlabeled).\nBecause of the presence of an oracle, this semi-supervised\nlearning is slow and almost always expensive.\nIII.\nSEMI-SUPERVISED CLASSIFICATION FOR NATURAL\nLANGUAGE PROCESSING\nIn this section, different natural language processing ap-\nplications of semi-supervised classiﬁcation are discussed. The\ndiscussion is mainly based on the ﬁndings from several classic\nand state-of-the-art literature from the domain of parsing, text\nclassiﬁcation, text summarization, and biomedical information\nmining.\nA. Parsing\nSteedman et al. [5] found that self-training has very small\neffects on parser improvements. Similar results are reported\nby Clark et al. [6] who applied self-training to part-of-speech\n(POS) tagging. The only works that reported successful ex-\necution of self-training to improve parsers are very few [7]\n[8]. This paper concentrates on the work of McClosky et al.\nbecause they do not adapt the parser in use that because\nadaptation has some drastic effects on self-training. Rather\nthan using an adaptive parser, the Charniak parser used in\ntheir research utilized both labeled and unlabeled data that\ncome from the same source domain. Using of a re-ranker\nbesides the parser is also what makes their work different\nthan many contemporary work. The parser uses third order\nMarkov grammar and ﬁve probability distributions that are\nconditioned with more than ﬁve linguistic attributes. Firstly,\nthe parser produces 50-best parses for the sentences in the\ndatasets. Secondly, a maximum entropy re-ranker with over\na million attributes re-ranks these parses. The experiment is\nextensive: datasets used in this experiment are Penn treebank\nsection 2 −21 for training (approximately 40, 000 wall street\njournal articles), section 23 for testing, and section 24 for held-\nout data. 24 million LA Times articles were used as unlabelled\ndata collected from the North American News Text Corpus\n(NANC). The authors experiment with and without the re-\nranker as they added unlabelled sentences to their labeled data\npool. They found that the parser performs better with the re-\nranker system. The improvement reported is about 1.1% F-\nscore—among which the self-trained parser contributes 0.8%\nand the re-ranker contributes 0.3%). The authors also did\nsome experiments with sentences in section 1, 22, and 24\nto see how the self-trained parser performs at sentence level.\nEach sentence in these sections was labelled as better, no\nchange or worse compared to the baseline F-score for the\nsentences. Interestingly, the outcomes showed that the parser\nhad improvement neither for unknown words nor for prepo-\nsitional phrases. However, there was an explicit improvement\nfor intermediate-length sentences but no improvement for the\nextremes (Goldilocks effect). The parser performs poorly for\nconjunctions.\nZhu [9], however, asserted that in semi-supervised classi-\nﬁcation, unlabeled sentences for which the parser accuracy is\nunusually better than normal should be restricted to be included\nin the pool of labeled data. McClosky et al. [7], however,\nstated that they did not followed this approach particularly. The\nspeed of the semi-supervised Charniak parser is similar to its\nsupervised version but it requires more memory to execute the\ncycles involved in self-training. Also, the labeled and unlabeled\ndata were collected from two different datasets (although they\nare both newspaper sources) that usually limits the success of\nself-training. Nevertheless, the experiment is a success and this\nquestion is unanswered in their paper.\nB. Text Classiﬁcation\nSemi-supervised classiﬁcation has been used widely in\nnatural language processing tasks such as spam classiﬁcation,\nwhich is a form of text classiﬁcation. The results in 2006\nECML/PKDD spam discovery challenge [10] indicated that\nspam ﬁlters based on semi-supervised classiﬁcation outper-\nformed supervised ﬁlters. Extensive experiments showed that\nsemi-supervised ﬁlters work better when source of available\nlabeled examples differs from those to be classiﬁed. Interest-\ningly, Mojdeh and Cormack [11] found completely different\nresults when they re-designed the challenge with different\ncollections of email datasets.\nThe 2006 ECML/PKDD discovery challenge had two in-\nteresting tasks. The ﬁrst task is called the Delayed Feedback\nwhere the ﬁlters are trained with emails T1 and then they\nclassify some test emails t1. In their second cycle of training,\nthey are trained with T1 and t1 combined and the training\ncontinues for the entire dataset for the challenge. The best\n(1−AUC) reported in the challenge is a remarkable 0.01%.\nThe second task for the challenge is called the Cross-user\nTrain where the classiﬁers are trained on one particular set of\nemails and then tested on a completely different set of emails.\nThe best (1−AUC) reported for this task is greater than the ﬁrst\ntask: 0.1%. The best performing ﬁlters in the challenge were all\nsemi-supervised ﬁlters and based on support vector machines\nSVM and TSVM [12], Dynamic Markov Compression (DMC)\n[13], and Logistic regression with self-training (LR) [14]. On\nthe other hand, in 2007 TREC Spam Track Challenge [15], the\nparticipating spam ﬁlters were trained with publicly available\nemails and their model accuracy was tested on emails collected\nfrom user inboxes (i.e., personalized emails). In an attempt\nto see whether semi-supervised ﬁlters perform as good as it\nwas reported in [10], Mojdeh and Cormack [11] reproduced\nthe work by replacing the datasets of ECML/PKDD challenge\nwith TREC challenge datasets. The delayed feedback task\nwas reproduced as follows: ﬁrst, 10, 000 messages were used\nfor training and the next 60, 000 messages were divided into\nsix batches each containing 10, 000 messages. Second, the\nremaining 5, 000 messages were kept for testing the models.\nOn the other hand, to reproduce the Cross-user Train task,\n30, 338 messages from particular user inboxes were used for\ntraining while 45, 081 messages from other users were used\nfor model evaluation.\nThe experimental outcomes showed that for both the\ntasks, the semi-supervised versions of DMC, LR, and TSVM\nunderperformed for LREC dataset. Their respective 1−AUC\nscores for the delayed feedback task were 0.090, 0.046, and\n0.230. On the other hand, the 1−AUC of their supervised\nversions were 0.016, 0.049, and 0.030 for the task. For the\ncross-user task, the 1−AUC of the semi-supervised DMC, LR,\nand TSVM ﬁlters were 9.97, 10.72, and 24.3, respectively.\nFor the same task, their supervised versions performed way\nbetter. The authors also reported a cross-corpus experiment\nto reproduce the results of ECML/PKDD Challenge. Here,\nthe ﬁrst 10, 000 messages from the TREC 2005 dataset were\nconsidered. Besides, the TREC 2007 dataset was split into\n10, 000 message segments. The outcomes again showed that\nself-training is harmful for the ﬁlters. Except the TSVM ﬁlter,\nthe rest of the two semi-supervised ﬁlters failed to perform as\ngood as their supervised versions.\nKeeping the aforementioned results in mind, we can say\nthat semi-supervised classiﬁcation is applicable to text clas-\nsiﬁcation but the performance depends on the labeled and\nunlabeled training data, and the source from which the data\nare derived.\nC. Extractive Text Summarization\nWong et al. [16] have conducted a comparative study\nwhere they produced extractive summaries by using both\nsupervised and semi-supervised classiﬁers. The authors used\nfour traditional groups of attributes to train their classiﬁers: (1)\nsurface (2) relevance (3) event, and (4) content attributes. They\ntried different combinations of the attributes and found that the\nclassiﬁers had produced better summaries when the surface,\nrelevance, and content attributes were combined. The novelty\nof their work is that they used supervised SVM as well as\nits semi-supervised version called probabilistic SVM or PSVM\nto generate classiﬁers and compared their performances. As\nperformance measure they considered ROUGE scores and found\nthat the ROUGE-I score of their SVM classiﬁer is 0.396 while\nthe human ROUGE-I was 0.42 when compared to the gold\nstandard summaries. On the other hand, the co-training with\nthe PSVM and Na¨ıve Bayes classiﬁers produced summaries\nthat have ROUGE-I of 0.366. Although this performance is not\nbetter than what they found with the supervised SVM or human\nsummaries, it was better than supervised PSVM and Na¨ıve\nBayes classiﬁers. Note that as their datasets, the authors used\nthe DUC 2001 dataset1. The dataset contains 30 clusters of\nrelevant documents. Each cluster comes with model summaries\ncreated by the dataset annotators. 50, 100, 200, and 400-word\nsummaries are provided for each cluster. Among the clusters,\n25 are used as training data while the remaining 5 clusters are\nused for testing. The authors also concluded that the ROUGE-I\nscores of their classiﬁer are better if they produce 400-word\nsummaries for the test clusters.\nNevertheless, the reported methodology of the paper has\nsome serious drawbacks. Many of the methods used in this\nresearch are not in line with what had been found by classic\nempirical studies. For instance, the co-training is done on the\nsame attribute space that violates the primary hypothesis of co-\ntraining: two classiﬁers used in co-training should use separate\nviews (see Section II-D2). Secondly, the authors selected the\nset of attributes (surface, relevance, and content attributes) by\nonly considering the performance of PSVM with them and\nignoring the performance of the supervised Na¨ıve Bayes with\nthem.\nD. Biomedical Information Mining\nNow-a-days, there is much impetus for information mining\nfrom biomedical research papers. Researchers put signiﬁcant\neffort to mine secondary information such as protein in-\nteraction relations from biomedical research papers to help\nidentify primary information like DNA replication, genotype-\nphenotype relations, and signaling pathways. The ﬁrst and\nforemost task for protein interaction relation miners is to\nclassify sentences in research papers that describe one or more\nprotein interactions. These sentences are called the candidate\nsentences. A number of supervised tools are developed to\nclassify candidate sentences from biomedical articles (see\nfor example [17], [18], and [19]). However, the ﬁrst semi-\nsupervised approach for the task was reported by Erkan et al.\n[20]. Their approach identiﬁed candidate sentences using sim-\nilarities of the paths present between two protein names found\nfrom the dependency parses of the sentences. What follows are\n1Download at: http://duc.nist.gov\nthe brief descriptions of their method. The authors produced\ndependency trees for each sentence from two given datasets.\nThe paths between two protein names in the parse trees were\nthen analyzed. According to these paths, the sentences were\nlabeled and treated as the gold standard for the tool’s eval-\nuation. Given the paths, two distance-based measures, cosine\nsimilarity and edit distance, were used by their tool to ﬁnd\nout interactions between the proteins. These measures were\nprovided to both supervised and semi-supervised algorithms to\ngenerate models to classify the sentences in the datasets. The\nlabels predicted by the supervised and semi-supervised classi-\nﬁers were then evaluated against the gold standard. According\nto the outcomes, the semi-supervised classiﬁers performed\nbetter than their supervised versions by a wide margin. Four\nalgorithms were used to generate the classiﬁers among which\ntwo are supervised (SVM and K-Nearest Neighbor (KNN))\nand the rest were their respective semi-supervised versions\n(TSVM and Harmonic Functions). The distance-based measures\nwere used to generate attributes for the classiﬁers and were\nextracted from two datasets named AIMED and Christine-\nBrun (CB). The AIMED dataset contains 4, 026 sentences of\nwhich 951 describe protein interactions while the CB dataset is\ncomposed of 4, 056 sentences of which 2, 202 describe protein\ninteractions. Each of the four algorithms then generated a\nclassiﬁer from the two sets of attributes found from the two\ndistance measures. Experimental outcomes show that for the\nAIMED dataset, TSVM with edit distance attributes performed\nthe best with 59.96% F-score. This F-score was signiﬁcantly\nbetter than the F-scores found using the supervised classiﬁers.\nComparisons showed that the F-score with TSVM was signif-\nicantly better than those reported by two contemporary work\n[18] [21]. On the other hand, the tool performed even better\non the CB dataset where its TSVM classiﬁer with edit distance\nbased attributes produced an F-score of 85.20%. Similar to\nthe result found with the AIMED dataset, the performances\nof the supervised classiﬁers were not satisfactory. The authors\nalso examined the effect of the size of the labeled training\ndata for the classiﬁers. In the case of AIMED, the authors\nfound that with small labeled training data, semi-supervised\nalgorithms were better. In addition, SVM performed poorly\nwith less training data but as more data became available for its\ntraining, it started to perform well. On the other hand, for the\nCB dataset, KNN performed poorly with much labeled data.\nInterestingly, SVM performed competitively with the semi-\nsupervised classiﬁers with more labeled data.\nNote that TSVM is susceptible to the distribution of the\nlabeled data. However, the work did not report any test on the\ndata distribution. The AIMED dataset, in addition, has class\nimbalance problem that seriously affects the performance of\nTSVM classiﬁers. This can be seen as the limitation of the work\nsince it did not explain why in their case the TSVM classiﬁer\nperformed better than the rest.\nIV.\nCONCLUSIONS\nThe ﬁndings of empirical research on parsing, text classiﬁ-\ncation, text summarization and biomedical information mining\nare investigated in this study. According to most of them,\nsemi-supervised classiﬁcation has substantial advantages over\nsupervised classiﬁcation when labeled data are difﬁcult to\nmanage and unlabeled data are abundant. This paper also\noutlines the theories behind the success of semi-supervised\nFig. 4: The use of labeled and unlabeled data in semi-\nsupervised classiﬁcation. A dot represents a paper that uses\nsemi-supervised classiﬁcation. Light gray dots mean older\npapers while dark gray dots mean newer papers [9].\nclassiﬁcation. According to the theories, there is no free lunch\nfor semi-supervised classiﬁcation rather its success depends on\nunderlying data distribution, data complexity, model assump-\ntion, choice of proper algorithm, problem in hand, and most of\nall—experience. Surprisingly, the investigation has found that\nthe classic studies often do not consider the do’s and don’ts\nsuggested by the theories. Despite the success reported in the\nempirical studies, it is therefore inconclusive whether semi-\nsupervised classiﬁcation can be really as useful as supervised\nclassiﬁcation.\nThe complexity associated with semi-supervised classiﬁ-\ncation limits its use. This can be seen from the illustration\nin Figure 4. It shows the use of labeled and unlabeled data\nin semi-supervised classiﬁcation. Each dot in the illustration\nrepresents a paper that uses semi-supervised classiﬁcation.\nWhile the light gray dots represent older papers, the dark gray\ndots represent recent papers. We can come to two conclusions\nfrom this data:\n1)\nthere are not much reported work that implement\nsemi-supervised classiﬁcation and a bulk of the re-\nported work are old and\n2)\nalthough the main purpose of using semi-supervised\nclassiﬁcation is the abundance of unlabeled data, the\namount of unlabeled data used in research are at most\n106 so far—in layman’s term, which is just above the\nnumber of people in a stadium.\nNevertheless, semi-supervised classiﬁcation is the only op-\ntion until now to deal the natural language processing problems\nwhere there are more unlabeled than labeled data. This study,\nhowever, points out the following suggestions for dealing with\nsemi-supervised classiﬁcation more effectively:\n1)\nThe model assumption for semi-supervised algo-\nrithms must match the problem in hand. For in-\nstance, if the classes produce well-clustered data\nthen expectation-maximization is a good algorithm\nto choose; if the attribute space can be naturally\nsplit into two sets then co-training is preferred; if\ntwo points with similar attribute values tend to be\nin the same class then graph-based method (not\ndiscussed in this paper) can be a reasonable choice;\nif SVM performs well on labeled data then TSVM is a\nnatural extension; and given the supervised algorithm\nis complicated and difﬁcult to modify, self-training is\nuseful.\n2)\nThe distributions of both labeled and unlabeled data\nneed to be investigated. TSVM, for instance, per-\nforms poorly with unlabeled data that have highly\noverlapped positive and negative distribution since it\nassumes that its decision boundary would go right\nthrough the densest region. Therefore, in this case\na TSVM classiﬁer usually produces a lot of false\npositives and false negatives.\n3)\nThe proportion of labeled and unlabeled data is\nimportant to notice before choosing an algorithm.\nHowever, there is not conclusive remark on how the\nproportion affects the overall classiﬁcation perfor-\nmance.\n4)\nIt has been found empirically that there is an effect\nof dependency among attributes on semi-supervised\nclassiﬁcation. To be more speciﬁc, with fewer labeled\nexamples, the number of dependent attributes should\nbe kept as low as possible.\n5)\nData noises should be investigated as they have effect\non classiﬁcation performance. It is easier to detect\nnoise in the labeled data than in unlabeled data. Note\nthat data noise has less effect on semi-supervised\nclassiﬁcation than supervised classiﬁcation.\n6)\nThe labeled and unlabeled data, usually, are collected\nfrom different sources and this can affect the classiﬁ-\ncation performance. If the labeled and unlabeled data\nare collected from completely different sources and\ntheir properties differ, then rather than using semi-\nsupervised classiﬁcation, transfer learning and self-\ntaught classiﬁcation are encouraged to use [22].\nREFERENCES\n[1]\nL. Shih, J. D. Rennie, Y.-H. Chang, and D. R. Karger, “Text\nbundling: Statistics based data-reduction.” in ICML, T. Fawcett and\nN. Mishra, Eds.\nAAAI Press, 2003, pp. 696–703. [Online]. Available:\nhttp://dblp.uni-trier.de/db/conf/icml/icml2003.html#ShihRCK03\n[2]\nX. Zhu, A. B. Goldberg, R. Brachman, and T. Dietterich, Introduction\nto Semi-Supervised Learning.\nMorgan and Claypool Publishers, 2009.\n[3]\nO. Chapelle, B. Schlkopf, and A. Zien, Semi-Supervised Learning,\n1st ed.\nThe MIT Press, 2010.\n[4]\nG. H. John and P. Langley, “Estimating continuous distributions in\nbayesian classiﬁers,” in Proceedings of the Eleventh Conference on\nUncertainty in Artiﬁcial Intelligence, ser. UAI’95.\nSan Francisco, CA,\nUSA: Morgan Kaufmann Publishers Inc., 1995, pp. 338–345.\n[5]\nM.\nSteedman,\nM.\nOsborne,\nA.\nSarkar,\nS.\nClark,\nR.\nHwa,\nJ. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim, “Bootstrapping\nstatistical parsers from small datasets,” in Proceedings of the Tenth\nConference on European Chapter of the Association for Computational\nLinguistics - Volume 1, ser. EACL ’03.\nStroudsburg, PA, USA:\nAssociation\nfor\nComputational\nLinguistics,\n2003,\npp.\n331–338.\n[Online]. Available: http://dx.doi.org/10.3115/1067807.1067851\n[6]\nS. Clark, J. R. Curran, and M. Osborne, “Bootstrapping pos taggers\nusing unlabelled data,” in Proceedings of the Seventh Conference\non Natural Language Learning at HLT-NAACL 2003 - Volume\n4,\nser.\nCONLL\n’03.\nStroudsburg,\nPA,\nUSA:\nAssociation\nfor\nComputational Linguistics, 2003, pp. 49–55. [Online]. Available:\nhttp://dx.doi.org/10.3115/1119176.1119183\n[7]\nD. Mcclosky, E. Charniak, and M. Johnson, “Effective self-training for\nparsing,” in In Proc. N. American ACL (NAACL, 2006, pp. 152–159.\n[8]\nM.\nBacchiani,\nM.\nRiley,\nB.\nRoark,\nand\nR.\nSproat,\n“Map\nadaptation\nof\nstochastic\ngrammars,”\nComput.\nSpeech\nLang.,\nvol.\n20,\nno.\n1,\npp.\n41–68,\nJan.\n2006.\n[Online].\nAvailable:\nhttp://dx.doi.org/10.1016/j.csl.2004.12.001\n[9]\nX. Zhu, “Semi-supervised learning literature survey,” Computer Sci-\nences, University of Wisconsin-Madison, Tech. Rep. 1530, 2005. [On-\nline]. Available: http://pages.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf\n[10]\nS. Bickel, “Ecml-pkdd discovery challenge 2006 overview,” in Proceed-\nings of the ECML-PKDD Discovery Challenge Workshop, 2006.\n[11]\nM. Mojdeh and G. V. Cormack, “Semi-supervised spam ﬁltering: does it\nwork?” in SIGIR, S.-H. Myaeng, D. W. Oard, F. Sebastiani, T.-S. Chua,\nand M.-K. Leong, Eds. ACM, 2008, pp. 745–746. [Online]. Available:\nhttp://dblp.uni-trier.de/db/conf/sigir/sigir2008.html#MojdehC08\n[12]\nT.\nJoachims,\n“Advances\nin\nkernel\nmethods,”\nB.\nSch¨olkopf,\nC.\nJ.\nC.\nBurges,\nand\nA.\nJ.\nSmola,\nEds.\nCambridge,\nMA,\nUSA: MIT Press, 1999, ch. Making Large-scale Support Vector\nMachine\nLearning\nPractical,\npp.\n169–184.\n[Online].\nAvailable:\nhttp://dl.acm.org/citation.cfm?id=299094.299104\n[13]\nA. Bratko, G. V. Cormack, D. R, B. Filipic, P. Chan, T. R. Lynam, and\nT. R. Lynam, “Spam ﬁltering using statistical data compression models,”\nJournal of Machine Learning Research, vol. 7, pp. 2673–2698, 2006.\n[14]\nG. V. Cormack, “Harnessing unlabeled examples through iterative\napplication of dynamic markov modeling,” in In Proceedings of the\nECML-PKDD Discovery Challenge Workshop, 2006.\n[15]\nG. Cormack, “Trec 2006 spam track overview,” in Proceedings of TREC\n2006, 2006.\n[16]\nK.-F. Wong, M. Wu, and W. Li, “Extractive summarization using\nsupervised and semi-supervised learning,” in Proceedings of the\n22Nd International Conference on Computational Linguistics - Volume\n1, ser. COLING ’08.\nStroudsburg,\nPA, USA: Association\nfor\nComputational Linguistics, 2008, pp. 985–992. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=1599081.1599205\n[17]\nI.\nM.\nDonaldson,\nJ.\nD.\nMartin,\nB.\nde\nBruijn,\nC.\nWolting,\nV.\nLay,\nB.\nTuekam,\nS.\nZhang,\nB.\nBaskin,\nG.\nD.\nBader,\nK.\nMichalickova,\nT.\nPawson,\nand\nC.\nW.\nV.\nHogue,\n“Prebind\nand\ntextomy\n-\nmining\nthe\nbiomedical\nliterature\nfor\nprotein-protein\ninteractions\nusing\na\nsupport\nvector\nmachine.”\nBMC\nBioinformatics,\nvol.\n4,\np.\n11,\n2003.\n[Online].\nAvailable:\nhttp://dblp.uni-trier.de/db/journals/bmcbi/bmcbi4.html#DonaldsonMBWLTZBBMPH\n[18]\nT.\nMitsumori,\nM.\nMurata,\nY.\nFukuda,\nK.\nDoi,\nand\nH.\nDoi,\n“Extracting\nprotein-protein\ninteraction\ninformation\nfrom\nbiomedical\ntext\nwith\nsvm.”\nIEICE\nTransactions,\nvol.\n89-D,\nno.\n8,\npp.\n2464–2466,\n2006.\n[Online].\nAvailable:\nhttp://dblp.uni-trier.de/db/journals/ieicet/ieicet89d.html#MitsumoriMFDD06\n[19]\nK. Sugiyama, K. Hatano, M. Yoshikawa, and S. Uemura, “Extracting\ninformation on protein-protein interactions from biological literature\nbased on machine learning approaches,” Genome Informatics Series,\npp. 699–700, 2003.\n[20]\nG. Erkan, A. ¨Ozg¨ur, and D. Radev, “Semi-supervised classiﬁcation for\nextracting protein interaction sentences using dependency parsing,” in In\nProceedings of the Joint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural Language Learning\n(EMNLP-CoNLL), 2007, pp. 228–237.\n[21]\nA. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii, “Biomedical in-\nformation extraction with predicate-argument structure patterns,” in\nProceedings of the 11th Annual Meeting of the Association for Natural\nLanguage Processing, 2005, pp. 60–69.\n[22]\nR. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng, “Self-taught\nlearning: Transfer learning from unlabeled data,” in Proceedings of the\n24th International Conference on Machine Learning, ser. ICML ’07.\nNew York, NY, USA: ACM, 2007, pp. 759–766. [Online]. Available:\nhttp://doi.acm.org/10.1145/1273496.1273592\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2014-09-25",
  "updated": "2014-09-25"
}