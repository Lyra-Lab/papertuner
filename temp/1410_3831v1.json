{
  "id": "http://arxiv.org/abs/1410.3831v1",
  "title": "An exact mapping between the Variational Renormalization Group and Deep Learning",
  "authors": [
    "Pankaj Mehta",
    "David J. Schwab"
  ],
  "abstract": "Deep learning is a broad set of techniques that uses multiple layers of\nrepresentation to automatically learn relevant features directly from\nstructured data. Recently, such techniques have yielded record-breaking results\non a diverse set of difficult machine learning tasks in computer vision, speech\nrecognition, and natural language processing. Despite the enormous success of\ndeep learning, relatively little is understood theoretically about why these\ntechniques are so successful at feature learning and compression. Here, we show\nthat deep learning is intimately related to one of the most important and\nsuccessful techniques in theoretical physics, the renormalization group (RG).\nRG is an iterative coarse-graining scheme that allows for the extraction of\nrelevant features (i.e. operators) as a physical system is examined at\ndifferent length scales. We construct an exact mapping from the variational\nrenormalization group, first introduced by Kadanoff, and deep learning\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\nOur results suggests that deep learning algorithms may be employing a\ngeneralized RG-like scheme to learn relevant features from data.",
  "text": "arXiv:1410.3831v1  [stat.ML]  14 Oct 2014\nAn exact mapping between the Variational Renormalization Group and Deep Learning\nPankaj Mehta\nDept.\nof Physics, Boston University, Boston, MA\nDavid J. Schwab\nDept.\nof Physics, Northwestern University, Evanston, IL\nDeep learning is a broad set of techniques that uses multiple layers of representation to automat-\nically learn relevant features directly from structured data. Recently, such techniques have yielded\nrecord-breaking results on a diverse set of diﬃcult machine learning tasks in computer vision, speech\nrecognition, and natural language processing. Despite the enormous success of deep learning, rel-\natively little is understood theoretically about why these techniques are so successful at feature\nlearning and compression. Here, we show that deep learning is intimately related to one of the\nmost important and successful techniques in theoretical physics, the renormalization group (RG).\nRG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e.\noperators) as a physical system is examined at diﬀerent length scales. We construct an exact map-\nping from the variational renormalization group, ﬁrst introduced by Kadanoﬀ, and deep learning\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the\nnearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning\nalgorithms may be employing a generalized RG-like scheme to learn relevant features from data.\nA central goal of modern machine learning research\nis to learn and extract important features directly from\ndata. Among the most promising and successful tech-\nniques for accomplishing this goal are those associated\nwith the emerging sub-discipline of deep learning. Deep\nlearning uses multiple layers of representation to learn\ndescriptive features directly from training data [1, 2]\nand has been successfully utilized, often achieving record\nbreaking results, in diﬃcult machine learning tasks in-\ncluding object labeling [3], speech recognition [4], and\nnatural language processing [5].\nIn this work, we will focus on a set of deep learning\nalgorithms known as deep neural networks (DNNs) [6].\nDNNs are biologically-inspired graphical statistical mod-\nels that consist of multiple layers of “neurons”, with units\nin one layer receiving inputs from units in the layer be-\nlow them. Despite their enormous success, it is still un-\nclear what advantages these deep, multi-layer architec-\ntures possess over shallower architectures with a similar\nnumber of parameters. In particular, it is still not well\nunderstood theoretically why DNNs are so successful at\nuncovering features in structured data. (But see [7–9].)\nOne possible explanation for the success of DNN ar-\nchitectures is that they can be viewed as an iterative\ncoarse-graining scheme, where each new high-level layer\nof the neural network learns increasingly abstract higher-\nlevel features from the data [1, 10]. The initial layers of\nthe the DNN can be thought of as low-level feature de-\ntecters which are then fed into higher layers in the DNN\nwhich combine these low-level features into more abstract\nhigher-level features, providing a useful, and at times re-\nduced, representation of the data. By successively apply-\ning feature extraction, DNNs learn to deemphasize irrel-\nevant features in the data while simultaneously learning\nrelevant ones. (Note that in a supervised setting, such\nas classiﬁcation, relevant and irrelevant are ultimately\ndetermined by the problem at hand. Here we are con-\ncerned solely with the unsupervised aspect of training\nDNNs, and the use of DNNs for compression [6].)\nIn\nwhat follows, we make this explanation precise.\nThis successive coarse-graining procedure is reminis-\ncent of one of the most successful and important tools\nin theoretical physics, the renormalization group (RG)\n[11, 12].\nRG is an iterative coarse-graining procedure\ndesigned to tackle diﬃcult physics problems involving\nmany length scales.\nThe central goal of RG is to ex-\ntract relevant features of a physical system for describing\nphenomena at large length scales by integrating out (i.e.\nmarginalizing over) short distance degrees of freedom. In\nany RG sequence, ﬂuctuations are integrated out start-\ning at the microscopic scale and then moving iteratively\non to ﬂuctuations at larger scales.\nUnder this proce-\ndure, certain features, called relevant operators, become\nincreasingly important while other features, dubbed irrel-\nevant operators, have a diminishing eﬀect on the physical\nproperties of the system at large scales.\nIn general, it is impossible to carry out the renormal-\nization procedure exactly.\nTherefore, a number of ap-\nproximate RG procedures have been developed in the\ntheoretical physics community [12–15].\nOne such ap-\nproximate method is a class of variational “real-space”\nrenormalization schemes introduced by Kadanoﬀfor per-\nforming RG on spin systems [14, 16, 17].\nKadanoﬀ’s\nvariational RG scheme introduces coarse-grained auxil-\niary, or “hidden”, spins that are coupled to the physical\nspin systems through some unknown coupling parame-\nters. A parameter-dependent free energy is calculated for\nthe coarse-grained spin system from the coupled system\nby integrating out the physical spins. The coupling pa-\nrameters are chosen through a variational procedure that\nminimizes the diﬀerence between the free energies of the\nphysical and hidden spin systems. This ensures that the\ncoarse-grained system preserves the long-distance infor-\nmation present in the physical system. Carrying out this\n2\nprocedure results in an RG transformation that maps the\nphysical spin system into a coarse-grained description in\nterms of hidden spins. The hidden spins then serve as\nthe input for the next round of renormalization.\nThe introduction of layers of hidden spins is also a cen-\ntral component of DNNs based on Restricted Boltzmann\nMachines (RBMs). In RBMs, hidden spins (often called\nunits or neurons) are coupled to “visible” spins describing\nthe data of interest. (Here we restrict ourselves to binary\ndata.) The coupling parameters between the visible and\nhidden layers are chosen using a variational procedure\nthat minimizes the Kullback-Leibler divergence (i.e. rela-\ntive entropy) between the “true” probability distribution\nof the data and the variational distribution obtained by\nmarginalizing over the hidden spins. Like in variational\nRG, RBMs can be used to map a state of the visible\nspins in a data sample into a description in terms of hid-\nden spins. If the number of hidden units is less than the\nnumber of visible units, such a mapping can be thought\nof as a compression. (Note, however, that dimensional\nexpansions are common [18].) In deep learning, individ-\nual RBMs are stacked on top of each other into a DNN\n[6, 19], with the output of one RBM serving as the input\nto the next. Moreover, the variational procedure is often\nperformed iteratively, layer by layer.\nThe preceding paragraphs suggest an intimate connec-\ntion between RG and deep learning. Indeed, here we con-\nstruct an exact mapping from the variational RG scheme\nof Kadanoﬀto DNNs based on RBMs [6, 19]. Our map-\nping suggests that DNNs implement a generalized RG-\nlike procedure to extract relevant features from struc-\ntured data.\nThe paper is organized as follows.\nWe begin by re-\nviewing Kadanoﬀ’s variational renormalization scheme\nin the context of the Ising Model.\nWe then introduce\nRBMs and deep learning architectures of stacked RBMs.\nWe then show how to map the procedure of variational\nRG to unsupervised training of a DNN. We illustrate\nthese ideas using the one- and two-dimensional nearest-\nneighbor Ising models. We end by discussing the impli-\ncation of our mapping for physics and machine learning.\nI.\nOVERVIEW OF VARIATIONAL RG\nIn statistical physics, one often considers an ensemble\nof N binary spins {vi} that can take the values ±1. The\nindex i labels the position of spin vi in some lattice. In\nthermal equilibrium, the probability of a spin conﬁgura-\ntion is given by the Boltzmann distribution\nP({vi}) = e−H({vi})\nZ\n,\n(1)\nwhere we have deﬁned the Hamiltonian H({vi}), and the\npartition function\nZ = Trvie−H({vi}) ≡\nX\nv1,...vN =±1\ne−H({vi}).\n(2)\nNote throughout the paper we set the temperature equal\nto one, without loss of generality. Typically, the Hamil-\ntonian depends on a set of couplings or parameters,\nK = {Ks}, that parameterizes the set of all possible\nHamiltonians.\nFor example, with binary spins, the K\ncould be the couplings describing the spin interactions of\nvarious orders:\nH[{vi}] = −\nX\ni\nKivi−\nX\nij\nKijvivj−\nX\nijk\nKijkvivjvk+. . . .\n(3)\nFinally, we can deﬁne the free energy of the spin system\nin the standard way:\nF v = −log Z = −log\n\u0010\nTrvie−H({vi})\u0011\n.\n(4)\nThe idea behind RG is to ﬁnd a new coarse-grained\ndescription of the spin system where one has “integrated\nout” short distance ﬂuctuations. To this end, let us intro-\nduce M < N new binary spins, {hj}. Each of these spins\nhj will serve as a coarse-grained degree of freedom where\nﬂuctuations on small scales have been averaged out. Typ-\nically, such a coarse-graining procedure increases some\ncharacteristic length scale describing the system such as\nthe lattice spacing. For example, in the block spin renor-\nmalization picture introduced by Kadanoﬀ, each hi rep-\nresents the state of a local block of physical spins, vi.\nFigure 1 shows such a block-spin procedure for a two-\ndimensional spin system on a square lattice, where each\nhi represents a 2 × 2 block of visible spins. The result\nof such a coarse-graining procedure is that the lattice\nspacing is doubled at each step of the renormalization\nprocedure.\nIn general, the interactions (statistical correlations)\nbetween the {vi} induce interactions (statistical corre-\nlations) between the coarse-grained spins, {hj}. In par-\nticular, the coarse-grained system can be described by a\nnew coarse-grained Hamiltonian of the form\nHRG[{hj}] = −\nX\ni\n˜Kihi−\nX\nij\n˜Kijhihj−\nX\nijk\n˜Kijkhihjhk+. . . ,\n(5)\nwhere { ˜K} describe interactions between the hidden\nspins, {hj}. In the physics literature, such a renormal-\nization transformation is often represented as mapping\nbetween couplings, {K} →{ ˜K}. Of course, the exact\nmapping depends on the details of the RG scheme used.\nIn the variational RG scheme proposed by Kadanoﬀ,\nthe coarse graining procedure is implemented by con-\nstructing a function, Tλ({vi}, {hj}), that depends on a\nset of variational parameters {λ} and encodes (typically\npairwise) interactions between the physical and coarse-\ngrained degrees of freedom. After coupling the auxiliary\nspins {hj} to the physical spins {vi}, one can then inte-\ngrate out (marginalize over) the visible spins to arrive at a\ncoarse-grained description of the physical system entirely\nin terms of the {hj}. The function Tλ({vi}, {hj}) then\nnaturally deﬁnes a Hamiltonian for the {hj} through the\n3\nFIG. 1. Block spin renormalization. In block spin renormalization [14], a physical system is coarse grained by introducing\nnew “block” variables which describe some “eﬀective” behavior of a block of spins. For example, in the ﬁgure, four adjacent\nspins are grouped into 2 x 2 blocks. The system is then described in terms of these new block variables. This scheme is then\niterated to create even new block variables that average over an even larger set of the original spins. Notice the lattice spacing\ndoubles after each iteration.\nexpression\ne−HRG\nλ\n[{hj}] ≡TrvieTλ({vi},{hj})−H({vi}).\n(6)\nWe can also deﬁne a free energy for the coarse grained\nsystem in the usual way\nF h\nλ = −log\n\u0010\nTrhie−HRG\nλ({hi})\u0011\n.\n(7)\nThus far we have ignored the problem of choosing the\nvariational parameters λ that deﬁne our RG transfor-\nmation Tλ({vi}, {hj}). Intuitively, it is clear we should\nchoose λ to ensure that the long-distance physical observ-\nables of the system are invariant to this coarse graining\nprocedure. This is done by choosing the parameters λ\nto minimize the free energy diﬀerence, ∆F = F h\nλ −F v,\nbetween the physical and coarse grained systems. Notice\nthat\n∆F = 0 ⇐⇒TrhjeTλ({vi},{hj}) = 1\n(8)\nThus, for any exact RG transformation, we know that\nTrhjeTλ({vi},{hj}) = 1\n(9)\nIn general, it is not possible to choose the parame-\nters λ to satisfy the condition above and various varia-\ntional schemes (e.g. bond moving) have been proposed\nto choose λ to minimize this ∆F.\nII.\nRBMS AND DEEP NEURAL NETWORKS\nWe will show below that this variational RG procedure\nhas a natural interpretation as a deep learning scheme\nbased on a powerful class of energy-based models called\nRestricted Boltzmann Machines (RBMs) [6, 20–23]. We\nwill restrict our discussion to RBMs acting on binary data\n[6] drawn from some probability distribution, P({vi}),\nwith {vi} binary spins labeled by an index i = 1 . . . N.\nFor example, for black and white images each spin vi\nencodes whether a given pixel is on or oﬀand the distri-\nbution P({vi}) encodes the statistical properties of the\nensemble of images (e.g the set of all handwritten digits\nin the MNIST dataset).\nTo model the data distribution, RBMs introduce new\nhidden spin variables, {hj} (j = 1 . . . M) that couple to\nthe visible units. The interactions between visible and\nhidden units are modeled using an energy function of the\nform\nE({vi}, {hj}) =\nX\ni\nbjhj +\nX\nij\nviwijhj +\nX\ni\ncivi, (10)\nwhere λ = {bj, wij, ci} are variational parameters of the\nmodel. In terms of this energy function, the joint prob-\nability of observing a conﬁguration of hidden and visible\nspins can be written as\npλ({vi}, {hj}) = e−E({vi},{hj})\nZ\n.\n(11)\nThis joint distribution also deﬁnes a variational distribu-\ntion for the visible spins\npλ({vi}) =\nX\n{hj}\npλ({vi}, {hj}) = Trhjpλ({vi}, {hj})\n(12)\n4\nas well as a marginal distribution for hidden spins them-\nselves:\npλ({hj}) =\nX\n{vj}\npλ({vi}, {hj}) = Trvipλ({vi}, {hj}).\n(13)\nFinally, for future reference it will be helpful to deﬁne a\n“variational” RBM Hamiltonian for the visible units:\npλ({vi}) ≡e−HRBM\nλ\n[{vi}]\nZ\n,\n(14)\nand an RBM Hamiltonian for the hidden units:\npλ({hj}) ≡e−HRBM\nλ\n[{hj}]\nZ\n.\n(15)\nSince the objective of the RBM for our purposes is\nunsupervised learning, the parameters in the RBM are\nchosen to minimize the Kullback-Leibler divergence be-\ntween the true distribution of the data P({vi}) and the\nvariational distribution pλ({vi}):\nDKL(P({vi})||pλ({vi}) =\nX\n{vi}\nP({vi}) log\n\u0012 P({vi})\npλ({vi})\n\u0013\n.\n(16)\nFurthermore, notice that when the RBM exactly repro-\nduces the visible data distribution\nDKL(P({vi})||pλ({vi})) = 0.\n(17)\nIn general it not possible to explicitly minimize the\nDKL(P({vi})||pλ({vi})) and this minimization is usually\nperformed using approximate numerical methods such as\ncontrastive divergence [24]. Note that if the number of\nhidden units is restricted (i.e. less than 2N), the RBM\ncannot be made to match an arbitrary distribution ex-\nactly [9].\nIn a DNN, RBMs are stacked on top of each other so\nthat, once trained, the hidden layer of one RBM serves as\nthe visible layer of the next RBM. In particular, one can\nmap a conﬁguration of visible spins to a conﬁguration\nin the hidden layer via the conditional probability distri-\nbution, pλ({hj}|{vi}). Thus, after training an RBM, we\ncan treat the activities of the hidden layer in response\nto each visible data sample as data for learning a second\nlayer of hidden spins, and so on.\nIII.\nMAPPING VARIATIONAL RG TO DEEP\nLEARNING\nIn variational RG, the couplings between the hid-\nden and visible spins are encoded by the operators\nTλ({vi}, {hj}).\nIn RBMs, an analogous role is played\nby the joint energy function E({vi}, {hj}). In fact, as we\nwill show below, these objects are related through the\nequation,\nT({vi}, {hj}) = −E({vi}, {hj}) + H[{vi}],\n(18)\nwhere H[{vi}] is the Hamiltonian deﬁned in Eq. 3 that\nencodes the data probability distribution P({vi}). This\nequation deﬁnes a one-to-one mapping between the vari-\national RG scheme and RBM based DNNs.\nUsing this deﬁnition, it is easy to show that the Hamil-\ntonian HRG\nλ\n[{hj}], originally deﬁned in Eq.\n6 as the\nHamiltonian of the coarse-grained degrees of freedom\nafter performing RG, also describes the hidden spins\nin the RBM. This is equivalent to the statement that\nthe marginal distribution pλ({hj}) describing the hidden\nspins of the RBM is of the Boltzmann form with a Hamil-\ntonian HRG\nλ\n[{hj}]. To prove this, we divide both sides of\nEq. 6 by Z to get\ne−HRG\nλ\n[{hj}]\nZ\n= TrvieTλ({vi},{hj})−H({vi})\nZ\n.\n(19)\nSubstituting Eq. 18 into this equation yields\ne−HRG\nλ\n[{hj}]\nZ\n= Trvi\ne−E({vi},{hj})\nZ\n= pλ({hj}).\n(20)\nSubstituting Eq. 15 into the right-hand side yields the\ndesired result\nHRG\nλ\n[{hj}] = HRBM\nλ\n[{hj}].\n(21)\nThese results also provide a natural interpretation for\nvariational RG entirely in the language of probability\ntheory. The operator Tλ({vi}, {hj}) can be viewed as\na variational approximation for the conditional probabil-\nity of the hidden spins given the visible spins. To see\nthis, notice that\neT({vi},{hj}) = e−E({vi},{hj})+H[{vi}]\n= pλ({vi}, {hj})\npλ({vi})\neH[{vi}]−HRBM\nλ\n[{vi}]\n= pλ({hj}|{vi})eH[{vi}]−HRBM\nλ\n[{vi}] (22)\nwhere in going from the ﬁrst the line to the second\nline we have used Eqs.\n11 and 14.\nThis implies that\nwhen an RG can be performed exactly (i.e.\nthe RG\ntransformation satisﬁes the equality TrhjeTλ({vi},{hj}) =\n1), the variational Hamiltonian is identical to the true\nHamiltonian describing the data, H[{vi}] = HRBM\nλ\n[{vi}]\nand T({vi}, {hj}) is exactly the conditional proba-\nbility.\nIn the language of probability theory, this\nmeans that the variational distribution pλ({vi}) ex-\nactly reproduces the true data distribution P({vi}) and\nDKL(P({vi})||pλ({vi}) = 0.\nIn general, it is not possible to perform the variational\nRG transformation exactly.\nInstead, one constructs a\nfamily of variational approximations for the exact RG\ntransform [14, 16, 17].\nThe discussion above makes it\nclear that these variational distributions work at the\nlevel of the Hamiltonians and Free Energies.\nIn con-\ntrast, in the Machine Learning literature, these varia-\ntional approximations are usually made by minimizing\nthe KL divergence DKL(P({vi})||pλ({vi}) = 0. Thus,\n5\nJ  J  J  J  J  J  J  J  \nJ  (1)\nJ  (1)\nJ  (1)\nJ  (1)\nJ  (2)\nJ  (2)\n. .\n. .\n. .\n. . \n. .\n. . \n0\n1\n2\nNumber of Decimations\n. .\n. .\n. .\n. . \n. .\n. . \nJ  J  \nJ  \nJ  J  \nJ  \nJ  \nJ  J  \nJ  \nJ  (1) J  (1)\nJ  (1) J  (1)\nJ  (1)\nJ  (1)\n0\n1\n2\nLayer\nDecimation\nDeep Architecture\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nJ =   0\nStable\nUnstable\nJ  ∞\n=\nA\nB\nC\ntanh[J]     \n0\n1\n2\n3\n4\ntanh[J        ]     \n(n+1)  = tanh [J      ] \n2\n(n)\ntanh[J]     \ntanh[J        ]     \n(n+1)  = tanh [J      ] \n2\n(n)\nFIG. 2. RG and deep learning in the one-dimensional Ising Model. (A) A decimation based renormalization trans-\nformation for the ferromagnetic 1-D Ising model. At each step, half the spins are decimated, doubling the eﬀective lattice\nspacing. After, n successive decimations, the spins can be described using a new 1-D Ising models with a coupling Jn between\nspins. Couplings at a given layer are related to couplings at a previous layer through the square of the hyberbolic tangent\nfunction. (B) Decimation-based renormalization transformations can also be realized using the deep architecture where the\nweights between the n + 1 and n-th hidden layer are given by Jn.\n(C) Visualizing the renormalization group ﬂow of the\ncouplings for 1-D Ferromagnetic Ising model. Under four successive decimations or equivalently as we move up four layers in\nthe deep architecture, the couplings (marked by red dots) get smaller. Eventually, the couplings are attracted to stable ﬁxed\npoint J = 0.\nthe two approaches employ distinct variational approxi-\nmation schemes for coarse graining. Finally, notice that\nthe correspondence does not rely on the explicit form of\nthe energy E({hj}, {vj}) and hence holds for any Boltz-\nmann Machine.\nIV.\nEXAMPLES\nTo gain intuition about the mapping between RG\nand deep learning, it is helpful to consider some sim-\nple examples in detail. We begin by examining the one-\ndimensional nearest-neighbor Ising model where the RG\ntransformation can be carried out exactly. We then nu-\nmerically explore the two-dimensional nearest-neighbor\nIsing model using an RBM-based deep learning architec-\nture.\nA.\nOne dimensional Ising Model\nThe one-dimensional Ising model describes a collection\nof binary spins {vi} organized along a one-dimensional\nlattice with lattice spacing a. Such a system is described\nby a Hamiltonian of the form\nH = −J\nX\ni\nvivi+1,\n(23)\nwhere J is a ferromagnetic coupling that energetically\nfavors conﬁgurations where neighboring spins align. To\nperform a RG transformation, we decimate (marginalize\nover) every other spin. This doubles the lattice spacing\na →2a and results in a new eﬀective interaction J(1) be-\ntween spins (see Figure 2). If we denote the coupling af-\nter performing n successive RG transformations by J(n),\nthen a standard calculation shows that these coeﬃcients\nsatisfy the RG equations\ntanh [J(n+1)] = tanh2 [J(n)],\n(24)\nwhere we have deﬁned J(0) = J [14].\nThis recursion\nrelationship can be visualized as a one-dimensional ﬂow\nin the coupling space J from J = ∞to J = 0. Thus,\nafter performing RG the interactions become weaker and\nweaker and J →0 as n →∞.\nThis RG transformation also naturally gives rise to the\ndeep learning architecture shown in Figure 2. The spins\nat a given layer of the DNN have a natural interpretation\nas the decimated spins when performing the RG trans-\nformation in the layer below. Notice that the coupled\nspins in the bottom two layers of the DNNs in Fig. 2B\nform an “eﬀective” one-dimensional chain isomorphic to\nthe original spin chain. Thus, marginalizing over spins in\nthe bottom layer in the DNN is identical to decimating\nevery other spin in the original spin systems. This im-\nplies that the “hidden” spins in the second layer of the\nDNN are also described by the RG transformed Hamil-\ntonian with a coupling J(1) between neighboring spins.\nRepeating this argument for spins coupled between the\nsecond and third layers and so on, one obtains the deep\nlearning architecture shown in Fig. 2B which implements\ndecimation.\nThe advantage of the simple deep architecture pre-\nsented here is that it is easy to interpret and requires no\ncalculations to construct. However, an important short-\ncoming is that it contains no information about half of\nthe visible spins, namely the spins that do not couple to\nthe hidden layer.\n6\n1600\n400\n100\n25\nSamples\nReconstructions\nA\nB\nC\nD\nE\nLayers/ RG Iterations\n \n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n  \n \n−20\n−15\n−10\n−5\n0\n5\n10\n15\n20\n25\n30\n25\n100\n400\nFIG. 3. Deep learning the 2D Ising model A Deep Neural Network with four layers of size 1600, 400, 100, and 25 spins was\ntrained using samples drawn from a 2D Ising model slightly above the critical temperature, J/(kBT ) = 0.408. B Visualization\nof the eﬀective receptive ﬁelds for the top layer of spins. Each 40 by 40 pixel image depicts the eﬀective receptive ﬁeld of one of\nthe 25 spins in the top layer (see material and methods)C Visualization of eﬀective receptive ﬁelds for each of the 100 spins in\nthe middle layer calculated as in B. D The eﬀective receptive ﬁelds get larger as one moves up the Deep Neural Network. This\nis consistent with what is expected from the successive application of block renormalization. E Three representative samples\ndrawn from the 2D Ising model at J = 0.408 and their reconstruction from the trained DNN. Samples were reconstructed from\nDNNs as in [6].\nB.\nTwo dimensional Ising Model\nWe next applied deep learning techniques to numeri-\ncally coarse-grain the two-dimensional nearest-neighbor\nIsing model on a square lattice. This model is described\nby a Hamiltonian of the form\nH[{vi}] = −J\nX\n⟨ij⟩\nvivj,\n(25)\nwhere ⟨ij⟩indicates that i and j are nearest neigh-\nbors and J is a ferromagnetic coupling that favors\nconﬁgurations where neighboring spins align.\nUnlike\nthe one-dimensional Ising model, the two dimensional\nIsing model has a phase transition that occurs when\nJ/(kBT ) = 0.4352 (recall we have set β = T −1 = 1).\nAt the phase transition, the characteristic length scale of\nthe system, the correlation length, diverges. For this rea-\nson, near a critical point the system can be productively\ncoarse-grained using a procedure similar to Kadanoﬀ’s\nblock spin renormalization (see Fig. 1) [14].\nInspired by our mapping between variational RG and\nDNNs, we applied standard deep learning techniques to\nsamples generated from the 2D Ising model for J = 0.408,\njust above the critical temperature. 20, 000 samples were\ngenerated from a periodic 40 × 40 2D Ising model using\nstandard equilibrium Monte Carlo techniques and served\nas input to an RBM-based deep neural network of four\nlayers with 1600, 400, 100, and 25 spins respectively (see\nFig. 3A). We furthermore imposed an L1 penalty on the\nweights between layers in the RBM and trained the net-\nwork using contrastive divergence [24] (see Materials and\nMethods). The L1 penalty serves as a sparsity promot-\ning regularizer that encourages weights in the RBM to\nbe zero and prevents overﬁtting due to the ﬁnite num-\nber of samples. In practice, it ensures that visible and\nhidden spins interact with only a small subset of all the\nspins in an RBM. (Note that we did not use a convolu-\ntional network that explicitly builds in spatial locality or\ntranslational invariance.)\nThe architecture of the resulting DNN suggests that it\n7\nis implementing a coarse-graining scheme similar to block\nspin renormalization (see Fig. 3). Each spin in a hidden\nlayer couples to a local block of spins in the layer below.\nThis iterative blocking is consistent with Kadanoﬀ’s in-\ntuitive picture of how coarse-graining should be imple-\nmented near the critical point.\nMoreover, the size of\nthe blocks coupling to each hidden unit in a layer are of\napproximately the same size (Fig. 3B,C), and the char-\nacteristic size is increasing with layer (Fig.\n3D). Sur-\nprisingly, this local block spin structure emerges from the\ntraining process, suggesting the DNN is self-organizing\nto implement block spin renormalization. Furthermore,\nas shown in Fig.\n3E, reconstructions from the coarse\ngrained DNN can qualitatively reproduce the macro-\nscopic features of individual samples despite having only\n25 spins in the top layer, a compression ratio of 64.\nV.\nDISCUSSION\nDeep learning is one of the most successful paradigms\nfor unsupervised learning to emerge over the last ten\nyears. The enormous success of deep learning techniques\nat a variety of practical machine learning tasks ranging\nfrom voice recognition to image classiﬁcation raises nat-\nural questions about its theoretical underpinnings. Here,\nwe have demonstrated that there is a one-to-one map-\nping between RBM-based Deep Neural Networks and\nthe variational renormalization group.\nWe illustrated\nthis mapping by analytically constructing a DNN for the\n1D Ising model and numerically examining the 2D Ising\nmodel. Surprisingly, we found that these DNNs self or-\nganize to implement a coarse-graining procedure remi-\nniscent of Kadanoﬀblock renormalization. This suggests\nthat deep learning may be implementing a generalized\nRG-like scheme to learn important features from data.\nRG plays a central role in our modern understanding\nof statistical physics and quantum ﬁeld theory. A cen-\ntral ﬁnding of RG is that the long distance physics of\nmany disparate physical systems are dominated by the\nsame long distance ﬁxed points. This gives rise to the\nidea of universality – many microscopically dissimilar sys-\ntems exhibit macroscopically similar properties at long\ndistances Physicists have developed elaborate technical\nmachinery for exploiting ﬁxed points and universality to\nidentify the salient long distance features of physics sys-\ntems. It will be interesting to see, what, if any of this\nmore complex machinery can be imported to deep learn-\ning. A potential obstacle for importing ideas from physics\ninto the deep learning framework is that RG is commonly\napplied to physical systems with many symmetries. This\nis in contrast to deep learning which is often applied to\ndata with limited structure.\nRecently, it was suggested that modern RG techniques\ndeveloped in the context of quantum systems such as ma-\ntrix product states and tensor networks have a natural\ninterpretation in terms of variational RG [17]. These new\ntechniques exploit ideas such as entanglement entropy\nand disentanglers which create a features with a mini-\nmum amount of redundancy. It is an open question to see\nwhether these ideas can be imported into deep learning\nalgorithms. Our mapping also suggests a route for apply-\ning real space renormalization techniques to complicated\nphysical systems. Real space renormalization techniques\nsuch as variational RG have often been limited by their\ninability to make good approximations. Techniques from\ndeep learning may represent a possible route for overcom-\ning these problems.\nAppendix A: Learning Deep Architecture for the\nTwo-dimensional Ising Model\nDetails are given in the SI Materials and Meth-\nods.\nStacked RBMs were trained with a variant\nof the code from [6].\nThis code is available at\nhttps://code.google.com/p/matrbm/.\nIn\nparticular,\nonly the unsupervised learning phase was performed. In-\ndividual RBMs were trained with contrastive divergence\nfor 200 epochs, with momentum 0.5 using mini-batches of\nsize 100 on 40, 000 total samples from the 2D Ising model\nwith J = 0.408. Additionally, L1 regularization was im-\nplemented, with strength 2 × 10−4, instead of weight de-\ncay. This L1 regularization strength was chosen to ensure\nthat one could not have all-to-all couplings between lay-\ners in the DNN. Reconstructions were performed as in [6].\nSee Supplementary ﬁles for a Matlab variable containing\nthe learned model.\nAppendix B: Visualizing Eﬀective Receptive Fields\nThe eﬀective receptive ﬁeld is a way to visualize which\nspins in the visible layer that coupled to a given spin in\none of the hidden layers. We denote the eﬀective recep-\ntive ﬁeld matrix of layer l by r(l) and the number of spins\nin layer l by n(l), with the visible layer corresponding to\nl = 0. Each column in r(l) is a vector that encodes the\nreceptive ﬁeld of a single spin in hidden layer l. It can be\ncomputed by convoluting the weight matrices W (l) en-\ncoding the weights wij between the spins in layers l −1\nand l .\nTo compute r(l) ﬁrst we set r(1) = W (1) and\nused the recursion relationship rl = r(l−1)W (l) for l > 1.\nThus, the eﬀective receptive ﬁeld of a spin is a measure\nof how much that hidden spin inﬂuences the spins in the\nvisible layer.\nACKNOWLEDGMENTS\nPM is grateful to Charles K. Fisher for useful conver-\nsations. We are also grateful to Javad Noorbakhsh and\nAlex Lang for comments on the manuscript. This work\nwas partially supported by Simons Foundation Investiga-\ntor Award in the Mathematical Modeling of Living Sys-\n8\ntems and a Sloan Research Fellowship (to P.M). DJS was\npartially supported by NIH Grant K25 GM098875.\n[1] Y. Bengio, Foundations and trends R\n⃝in Machine Learn-\ning, 2, 1 (2009).\n[2] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen,\nG. Corrado, J. Dean,\nand A. Ng, International Con-\nference in Machine Learning (2012).\n[3] A. Krizhevsky, I. Sutskever,\nand G. E. Hinton, Ad-\nvances in Neural Information Processing Systems 25,\n1097 (2012).\n[4] G.\nHinton,\nL.\nDeng,\nD.\nYu,\nG.\nDahl,\nA.\nMo-\nhamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,\nT. Sainath, and B. Kingsbury, Signal Processing Maga-\nzine, 29, 82 (2012).\n[5] R. Sarikaya, G. Hinton, and A. Deoras, IEEE Transac-\ntions on Audio Speech and Language Processing (2014).\n[6] G. E. Hinton and R. R. Salakhutdinov, Science, 313, 504\n(2006).\n[7] Y. Bengio and L. Yann, Large-scale kernel machines, 34,\n1 (2007).\n[8] N. Le Roux and Y. Bengio, Neural Computation, 22,\n2192 (2010).\n[9] N. Le Roux and Y. Bengio, Neural Computation, 20,\n1631 (2008).\n[10] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle,\nAdvances in neural information processing systems, 19,\n153 (2007).\n[11] K. G. Wilson and J. Kogut, Physics Reports, 12, 75\n(1974).\n[12] K. G. Wilson, Reviews of Modern Physics, 55, 583\n(1983).\n[13] J. Cardy, Scaling and renormalization in statistical\nphysics, Vol. 5 (Cambridge University Press, 1996).\n[14] L. P. Kadanoﬀ, Statics, Dynamics and Renormalization\n(World Scientiﬁc, 2000).\n[15] N. Goldenfeld, (1992).\n[16] L. P. Kadanoﬀ, A. Houghton, and M. C. Yalabik, Journal\nof Statistical Physics, 14, 171 (1976).\n[17] E. Efrati, Z. Wang, A. Kolan, and L. P. Kadanoﬀ, Re-\nviews of Modern Physics, 86, 647 (2014).\n[18] Y. Bengio, A. Courville, and P. Vincent, Pattern Analy-\nsis and Machine Intelligence, IEEE Transactions on, 35,\n1798 (2013).\n[19] G. Hinton, S. Osindero, and Y.-W. Teh, Neural compu-\ntation, 18, 1527 (2006).\n[20] R. Salakhutdinov, A. Mnih,\nand G. Hinton, in Pro-\nceedings of the 24th international conference on Machine\nlearning (ACM, 2007) pp. 791–798.\n[21] H. Larochelle and Y. Bengio, in Proceedings of the 25th\ninternational conference on Machine learning (ACM,\n2008) pp. 536–543.\n[22] P. Smolensky, (1986).\n[23] Y. W. Teh and G. E. Hinton, Advances in neural infor-\nmation processing systems, 908 (2001).\n[24] G. E. Hinton, Neural computation, 14, 1771 (2002).\n",
  "categories": [
    "stat.ML",
    "cond-mat.stat-mech",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2014-10-14",
  "updated": "2014-10-14"
}