{
  "id": "http://arxiv.org/abs/1412.6597v4",
  "title": "An Analysis of Unsupervised Pre-training in Light of Recent Advances",
  "authors": [
    "Tom Le Paine",
    "Pooya Khorrami",
    "Wei Han",
    "Thomas S. Huang"
  ],
  "abstract": "Convolutional neural networks perform well on object recognition because of a\nnumber of recent advances: rectified linear units (ReLUs), data augmentation,\ndropout, and large labelled datasets. Unsupervised data has been proposed as\nanother way to improve performance. Unfortunately, unsupervised pre-training is\nnot used by state-of-the-art methods leading to the following question: Is\nunsupervised pre-training still useful given recent advances? If so, when? We\nanswer this in three parts: we 1) develop an unsupervised method that\nincorporates ReLUs and recent unsupervised regularization techniques, 2)\nanalyze the benefits of unsupervised pre-training compared to data augmentation\nand dropout on CIFAR-10 while varying the ratio of unsupervised to supervised\nsamples, 3) verify our findings on STL-10. We discover unsupervised\npre-training, as expected, helps when the ratio of unsupervised to supervised\nsamples is high, and surprisingly, hurts when the ratio is low. We also use\nunsupervised pre-training with additional color augmentation to achieve near\nstate-of-the-art performance on STL-10.",
  "text": "Accepted as a workshop contribution at ICLR 2015\nAN ANALYSIS OF UNSUPERVISED PRE-TRAINING IN\nLIGHT OF RECENT ADVANCES\nTom Le Paine*, Pooya Khorrami*, Wei Han, Thomas S. Huang\n∗Beckman Institute for Advanced Science and Technology\nUniversity of Illinois at Urbana-Champaign\nUrbana, IL 61801\npaine1,pkhorra2,weihan3,t-huang1@illinois.edu\nABSTRACT\nConvolutional neural networks perform well on object recognition because of a\nnumber of recent advances: rectiﬁed linear units (ReLUs), data augmentation,\ndropout, and large labelled datasets. Unsupervised data has been proposed as an-\nother way to improve performance. Unfortunately, unsupervised pre-training is\nnot used by state-of-the-art methods leading to the following question: Is unsu-\npervised pre-training still useful given recent advances? If so, when? We answer\nthis in three parts: we 1) develop an unsupervised method that incorporates ReLUs\nand recent unsupervised regularization techniques, 2) analyze the beneﬁts of un-\nsupervised pre-training compared to data augmentation and dropout on CIFAR-10\nwhile varying the ratio of unsupervised to supervised samples, 3) verify our ﬁnd-\nings on STL-10. We discover unsupervised pre-training, as expected, helps when\nthe ratio of unsupervised to supervised samples is high, and surprisingly, hurts\nwhen the ratio is low. We also use unsupervised pre-training with additional color\naugmentation to achieve near state-of-the-art performance on STL-10.\n1\nINTRODUCTION\nWe analyze the beneﬁts of unsupervised pre-training in the context of recent deep learning inno-\nvations including: rectiﬁed linear units, data augmentation, and dropout. Recent work shows that\nconvolutional neural networks (CNNs) can achieve state-of-the-art performance for object classiﬁ-\ncation (Krizhevsky et al. (2012)) and object detection (Girshick et al. (2013)), when there is enough\ntraining data. However, in many cases there is a dearth of labeled data. In these cases regularization\nis necessary for good results. The most common types of regularization are data augmentations\n(Krizhevsky et al. (2012); Dosovitskiy et al. (2014)) and dropout (Hinton et al. (2012)). Another\nform of regularization, unsupervised pre-training (Hinton et al. (2006); Bengio et al. (2007); Erhan\net al. (2010)), has recently fallen out of favor.\nWhile there has been signiﬁcant work in unsupervised learning, most of these works came before\nrectiﬁed linear units, which signiﬁcantly help training deep supervised neural networks, and before\nsimpler regularization schemes for unsupervised learning, such as zero-bias with linear encoding for\nauto-encoders (Memisevic et al. (2014)).\nWe train an unsupervised method that takes advantage of these improvements we call Zero-bias\nConvolutional Auto-encoders (CAEs). Previous work showed that pre-trained tanh CAEs achieved\nan increase in performance over randomly initialized tanh CNNs. We conduct this experiment with\nour zero-bias CAE and observe a larger boost in performance.\nWe analyze the effectiveness of our technique when combined with the popular regularization tech-\nniques used during supervised training on CIFAR-10 while varying the ratio of unsupervised to\nsupervised samples. We do this comparing against randomly initialized CNNs without any addi-\ntional regularization. We ﬁnd that, when ratio is large, unsupervised pre-training provides useful\nregularization, increasing test set performance. When the ratio is small, we ﬁnd that unsupervised\npre-training hurts performance.\n∗- Authors contributed equally to this work.\n1\narXiv:1412.6597v4  [cs.CV]  10 Apr 2015\nAccepted as a workshop contribution at ICLR 2015\nWe verify our ﬁnding that unsupervised pre-training can boost performance when the ratio of unsu-\npervised to supervised samples is high by running our algorithm on the STL-10 dataset, which has a\nratio of 100:1. As expected, we observe an improvement (3.87%). When combined with additional\ncolor augmentation, we achieve near state-of-the-art results. Our unsupervised regularization still\nyields an improvement of (1.69%).\nWe will begin by reviewing related work on fully-connected and convolutional auto-encoders. In\nSection 3, we will present our method and how it is trained both during unsupervised pre-training\nand supervised ﬁne-tuning. We present our results on the CIFAR-10 and STL-10 datasets in Section\n4, and in Section 5 we conclude the paper.\n2\nRELATED WORK\nMany methods have used unsupervised learning to learn parameters, which are subsequently used\nto initialize a neural network to be trained on supervised data. These are called unsupervised pre-\ntraining, and supervised ﬁne-tuning respectively. We will highlight some of the unsupervised learn-\ning methods related to our work.\n2.1\nAUTO-ENCODERS\nOne of the most widely-used models for unsupervised learning, an auto-encoder is a model that\nlearns a function that minimizes the squared error between the input x ∈Rn and its reconstruction\nr(x):\nL = ∥x −r(x)∥2\n2\n(1)\nr(x) = W T\nd f(Wex + b) + c\n(2)\nIn the above equation, We represents the weight matrix that transforms the input, x into some hid-\nden representation, b is vector of biases for each hidden unit and f(·) is some nonlinear function.\nCommonly chosen examples for f(·) include the sigmoid and hyperbolic tangent functions. Mean-\nwhile, Wd is the weight matrix that maps back from the hidden representation to the input space\nand c is a vector of biases for each input (visible) unit. These parameters are commonly learned by\nminimizing the loss function over the training data via stochastic gradient descent.\nWhen no other constraints are imposed on the loss function, the auto-encoder weights tend to learn\nthe identity function. To combat this, some form of regularization must imposed upon the model so\nthat the model can uncover the underlying structure in the data. Some forms of regularization include\nadding noise to the input units (Vincent et al. (2010)) and requiring the hidden unit activations\nbe sparse (Coates et al. (2011)) or have small derivatives (Rifai et al. (2011)). These models are\nknown as de-noising, sparse, and contractive auto-encoders respectively. A more recent work by\nMemisevic et al. (2014) showed that training an auto-encoder with rectiﬁed linear units (ReLU)\ncaused the activations to form tight clusters due to having negative bias values. They showed that\nusing thresholded linear (TLin) or thresholded rectiﬁer (TRec) activations with no bias can allow\none to train an auto-encoder without the need for additional regularization.\n2.2\nCONVOLUTIONAL AUTO-ENCODERS\nWhile the aforementioned fully-connected techniques have shown impressive results, they do not di-\nrectly address the structure of images. Convolutional neural networks (CNNs) (LeCun et al. (1998);\nLee et al. (2009)) present a way to reduce the number of connections by having each hidden unit only\nbe responsible for a small local neighborhood of visible units. Such schemes allow for dense feature\nextraction followed by pooling layers which when stacked could allow the network to learn over\nlarger and larger receptive ﬁelds. Convolutional auto-encoders (CAEs) combined aspects from both\nauto-encoders and convolutional neural nets making it possible to extract highly localized patch-\nbased information in an unsupervised fashion. There have been several works in this area including\nJarrett et al. (2009) and Zeiler et al. (2010). Both rely on sparse coding to force their unsupervised\nlearning to learn non-trival solutions. Zeiler et al. (2011) extended this work by introducing pool-\ning/unpooling and visualizing how individual feature maps at different layers inﬂuenced speciﬁc\nportions of the reconstruction. These sparse coding approaches had limitations because they used\n2\nAccepted as a workshop contribution at ICLR 2015\nan iterative procedure for inference. A later work by Masci et al. (2011) trained deep feed forward\nconvolutional auto-encoders, using only max-pooling and saturating tanh non-linearities as a form\nof regularization, while still showing a modest improvement over randomly initialized CNNs. While\ntanh was a natural choice at the time, Krizhevsky et al. (2012) showed that ReLUs are more suitable\nfor learning given their non-saturating behavior.\n3\nOUR APPROACH\nOur method’s training framework can be broken up into two phases: (i) unsupervised pre-training\nand (ii) supervised ﬁne-tuning. We describe those in more detail below.\n3.1\nUNSUPERVISED PRE-TRAINING\nOur method incorporates aspects of previous unsupervised learning methods in order to learn salient\nfeatures, yet be efﬁcient to train. Our model is similar to the deconvolutional network in Zeiler et al.\n(2011) where the cost we minimize at each layer is the mean square error on the original image.\nHowever, unlike the network in Zeiler et al. (2011), our method does not use any form of sparse\ncoding. Our model also is similar to that of Masci et al. (2011), however we improve upon it by\nintroducing regularization in the convolutional layers through the use of zero-biases and ReLUs as\ndiscussed in Memisevic et al. (2014).\nWe now describe the model architecture in detail. Like the previous work described above, our\nmodel involves several encoding modules followed by several decoding modules. A single encoding\nmodule El(·) consists of a convolution layer Fl, a nonlinearity f(·), followed by a pooling layer Psl\nwith switches sl.\nEl(x) = Pslf(Flx)\n(3)\nEach encoding module has an associated decoding module Dl, which unpools using El pooling\nswitches sl and deconvolves with El’s ﬁlters, (i.e. F T\nl ).\nDl(x) = F T\nl Uslx\n(4)\nA two layer network can be written as:\nr(x) = D1(D2(E2(E1(x))))\n(5)\nWe train each encoder/decoder pair in a greedy fashion (i.e. ﬁrst a 1 layer CAE, then a 2 layer CAE,\netc.) while keeping the parameters of previous layers ﬁxed. Like Zeiler et al. (2011), we compute the\ncost by taking the mean squared error between the original image and the network’s reconstruction\nof the input. Thus, the costs for a one layer network (C1(x)) and two layer network (C2(x)) would\nbe expressed in the following manner:\nC1(x) = ∥x −D1(E1(x))∥2\n2\n(6)\nC2(x) = ∥x −D1(D2(E2(E1(x))))∥2\n2\n(7)\nWe regularize our learned representation by ﬁxing the biases of our convolutional and deconvolu-\ntional layers at zero and using ReLUs as our activation function during encoding. We use linear\nactivations for our decoders. Unlike the work by Memisevic et al. (2014) which analyzes fully-\nconnected auto-encoders, our work is the ﬁrst, to our knowledge, that trains zero-bias CAEs for\nunsupervised learning.\n3.1.1\nUNSUPERVISED WEIGHT INITIALIZATION\nWeight initialization is often a key component of successful neural network training. For ReLU’s it\nis important to ensure the input to the ReLU is greater than 0. This can be achieved by setting the\nbias appropriately. This cannot be done for zero-bias auto-encoders. Instead we use two methods\n3\nAccepted as a workshop contribution at ICLR 2015\nfor initializing the weights to achieve this 1) in the ﬁrst layer, we initialize each of the ﬁlters to be a\nrandomly drawn patch from the dataset, 2) on the later layers, we sample weights from a Gaussian\ndistribution and ﬁnd the nearest orthogonal matrix by taking the singular value decomposition (SVD)\nof the weight matrix and setting all of the singular values to one. For CNNs we must take into\naccount the additive effect of overlapping patches thus we weight each ﬁlter by a 2D hamming\nwindow to prevent intensity build-up.\n3.2\nSUPERVISED FINE-TUNING\nAfter the weights of the CAE have been trained, we remove all of the decoder modules and leave just\nthe encoding modules. We add an additional fully-connected layer and a softmax layer to the pre-\ntrained encoding modules. The weights of these layers are drawn from a Gaussian distribution with\nzero mean and standard deviation of k/√NF AN IN, where k is drawn uniformly from [0.2, 1.2].\n3.3\nTRAINING\nFor both unsupervised and supervised training we use stochastic gradient descent with a constant\nmomentum of 0.9, and a weight decay parameter of 1e-5. We select the highest learning rate that\ndoesn’t explode for the duration of training. For these experiments we do not anneal the learning\nrate. The only pre-processing we do to each patch is centering (i.e. mean subtraction) and scaling to\nunit variance.\n4\nEXPERIMENTS AND ANALYSIS\n4.1\nDATASETS\nWe run experiments on two natural image datasets, CIFAR-10 (Krizhevsky and Hinton (2009))\nand STL-10 (Coates et al. (2011)). CIFAR-10 is a common benchmark for object recognition.\nMany unsupervised and supervised neural network approaches have been tested on it. It consists\nof 32x32 pixel color images drawn from 10 object categories. It has 50,000 training images, and\n10,000 testing images. STL-10 is also an object recognition benchmark, but was designed to test\nunsupervised learning algorithms, so it has a relatively small labeled training set of 500 images per\nclass, and an additional unsupervised set which contains 100,000 unlabeled images. The test set\ncontains 800 labeled images per class. All examples are 96x96 pixel color images.\n4.2\nCIFAR-10\nOn CIFAR-10, we train a network with structure similar to Masci et al. (2011), so that we can directly\nshow the beneﬁts of our modiﬁcations. The network consists of three convolutional layers with 96,\n144, and 192 ﬁlters respectively. The ﬁlters in the ﬁrst two layers are of size 5x5 while the ﬁlters in\nthe third layer are of size 3x3. We also add 2x2 max pooling layers after the ﬁrst two convolutional\nlayers. There is also a full-connected layer with 300 hidden units followed by a softmax layer with\n10 output units. All of our nets were trained using our own open source neural network library 1.\nAs stated in the methods section, we ﬁrst train our unsupervised model on 100% of the training\nimages, do supervised ﬁne-tuning, and report overall accuracy on the test set. We 1) present quali-\ntative results of unsupervised learning, 2) show our zero-bias convolutional auto-encoder performs\nwell compared to previous convolutional auto-encoder work by Masci et al. (2011) developed before\nthe popularization of rectiﬁed linear units, and zero-bias auto-encoders, 3) we show our analysis of\nvarious regularization techniques, and vary the ratio of unsupervised to supervised data, 4) for com-\npleteness we report our best results when training on the full CIFAR-10 dataset, however this is not\nthe main point of this work.\n4.2.1\nQUALITATIVE RESULTS\nOne way in which we ensure the quality of our learned representation is by inspecting the ﬁrst layer\nﬁlters. We visualize the ﬁlters learned by our model in Figure 1. So that we can directly compare\n1https://github.com/ifp-uiuc/anna\n4\nAccepted as a workshop contribution at ICLR 2015\nwith the ﬁlters presented in Masci et al. (2011), we trained an additional zero-bias convolutional\nauto-encoder with ﬁlters of size 7x7x3 (instead of 5x5x3) in the ﬁrst layer. From Figure 1, we can\nsee that, indeed, our model is able to capture interpretable patterns such as Gabor-like oriented edges\n(both color and intensity) and center-surrounds.\n4.2.2\nUNSUPERVISED PRE-TRAINING FOR TANH CAES AND ZERO-BIAS CAES\nFigure 1: First layer ﬁlters learned by our zero-\nbias convolutional auto-encoder. Each ﬁlter has\ndimension 7x7x3. (Best viewed in color.) For di-\nrect comparison with tanh CAE please see Masci\net al. (2011) Figure 2c.\nFor our quantitative experiments, we ﬁrst com-\npare the performance of the tanh CAE proposed\nby Masci et al. (2011) with our zero-bias CAE.\nIn their paper, Masci et al. (2011) trained a tanh\nCNN from a random initialization and com-\npared it with one pre-trained using a tanh CAE.\nThey also added 5% translations as a form of\ndata augmentation. We re-conduct this experi-\nment using a zero-bias CNN trained from a ran-\ndom initialization, and compare it to one pre-\ntrained using our zero-bias CAE.\nIn Table 1 we compare the improvements of our\nmodel with that of Masci et al. (2011)’s, on var-\nious subsets of CIFAR-10. As expected, the\nzero-bias CNN (a ReLU CNN without bias parameters) performs signiﬁcantly better than the tanh\nCNN (2.53%, 8.53%, 5.23%). More interestingly, notice that on each subset, compared to Masci\net al. (2011) our pre-trained model shows similar or better performance over the randomly initialized\nCNN. When the ratio of unsupervised to supervised data is high, we experience an 8.44% increase\nin accuracy as opposed to Masci et al. (2011)’s 3.22% increase.\n4.2.3\nANALYSIS OF REGULARIZATION METHODS\nNext, we analyze how different supervised regularization techniques affect our model’s perfor-\nmance. Speciﬁcally, we consider the effects of dropout, data augmentation (via translations and\nhorizontal ﬂips), unsupervised pre-training (with our zero-bias CAE) and their combinations. We\ncompare each regularization technique to a zero-bias CNN trained from random initialization with-\nout any regularization (labeled CNN in Figure 2). Figure 2 shows the classiﬁcation accuracy im-\nprovement over CNN for each type of regularization both individually and together.\nWe perform this analysis for subsets of CIFAR-10 with different unsupervised to supervised sample\nratios ranging from 50:1 to 1:1, by ﬁxing the unsupervised data size, and varying the number of\nsupervised examples. It is important to note that as this ratio approaches 1:1, the experimental setup\nfavors data augmentation and dropout because the number of virtual supervised samples is larger\nthan number of unsupervised samples.\nIn Figure 2a, where the ratio of unsupervised to supervised samples is 50:1, there are three notable\neffects: (i) unsupervised pre-training alone yields a larger improvement (4.09%) than data augmen-\ntation (2.67%) or dropout (0.59%), (ii) when unsupervised pre-training is combined with either data\naugmentation or dropout, the improvement is greater than the sum of the individual contributions,\n(iii) we experience the largest gains (15.86%) when we combine all three forms of regularization.\nTable 1: Comparison between Tanh CAE (Masci et al. (2011)) and our model on various subsets of\nCIFAR-10.\nUnsupervised to supervised ratio\n(Samples per Class)\n50:1\n(100)\n10:1\n(500)\n5:1\n(1000)\n1:1\n(5000)\nTanh CNN - Masci et al. (2011)\n44.48 %\n—\n64.77 %\n77.50 %\nTanh CAE - Masci et al. (2011)\n47.70 %\n—\n65.65 %\n78.20 %\nZero-bias CNN\n47.01 %\n64.76 %\n73.30 %\n82.73 %\nZero-bias CAE\n55.45 %\n68.42 %\n74.06 %\n83.64 %\n5\nAccepted as a workshop contribution at ICLR 2015\n \n \n+A\n+D\n+U\n+AD\n+AU\n+DU +ADU\nAdditional regularization\nClassiﬁcation\naccuracy\nimprovement\nover CNN\n(absolute %)\n16\n14\n12\n10\n8\n6\n4\n2\n0\nCNN\n(a) 50:1 unsupervised to supervised sample ratio\n(100 samples per class), baseline CNN: 44.3%\n \n \n+A\n+D\n+U\n+AD\n+AU\n+DU +ADU\nAdditional regularization\n16\n14\n12\n10\n8\n6\n4\n2\n0\nCNN\n(b) 10:1 unsupervised to supervised sample ratio\n(500 samples per class), baseline CNN: 62.0%\n \n \n+A\n+D\n+U\n+AD\n+AU\n+DU +ADU\nAdditional regularization\nClassiﬁcation\naccuracy\nimprovement\nover CNN\n(absolute %)\n16\n14\n12\n10\n8\n6\n4\n2\n0\nCNN\n(c) 5:1 unsupervised to supervised sample ratio\n(1000 samples per class), baseline CNN: 67.8%\n \n \n+A\n+D\n+U\n+AD\n+AU\n+DU +ADU\nAdditional regularization\n16\n14\n12\n10\n8\n6\n4\n2\n-2\n0\nCNN\n(d) 1:1 unsupervised to supervised sample ratio\n(5000 samples per class), baseline CNN: 80.2%\nFigure 2: Analysis of the effects of different types of regularization (A: data augmentation, D:\ndropout, U: unsupervised learning), individually and jointly, on different subsets of CIFAR-10.\nWe see that effect (ii) is also observed in the case where the ratio of unsupervised to supervised\nsamples is 10:1 (Figure 2b), and to a lesser extent when the ratio is 5:1 (Figure 2c). Unfortunately,\neffects (i) and (iii) are not observed when the ratio of unsupervised to supervised samples decreases.\nWe will elaborate on effect (i) below.\nIn Figure 3, we observe that the improvement in performance from unsupervised learning decreases\nrapidly as the ratio of unsupervised to supervised samples decreases. Surprisingly, when the ratio is\n1:1, we see that unsupervised learning actually hurts performance (-0.67%).\n4.2.4\nCOMPARISON WITH EXISTING METHODS\nWe also compare the performance of our algorithm on the full CIFAR-10 dataset with other tech-\nniques in Table 2, though we show above our method performs worse when the ratio of unsupervised\nto supervised samples is 1:1. We outperform all methods that use unsupervised pre-training (Masci\net al. (2011), Coates et al. (2011), Dosovitskiy et al. (2014), Lin and Kung (2014)), however we\nare not competitive with supervised state-of-the-art. We include some representative supervised\nmethods in Table 2.\n4.3\nSTL-10\nNext, we assess the effects of unsupervised pre-training on STL-10. From the CIFAR-10 exper-\niments, it is clear unsupervised pre-training can be beneﬁcial if the unsupervised dataset is much\nlarger than the supervised dataset. STL-10 was designed with this in mind, and has a ratio of unsu-\npervised to supervised data of 100:1. So we experimentally show this beneﬁt.\n6\nAccepted as a workshop contribution at ICLR 2015\nWe design our network to have structure similar to Dosovitskiy et al. (2014), to ease comparison.\nThe network used consists 3 convolutional layers with 64, 128, and 256 ﬁlters in each layer, a\nfully-connected layer with 512 units, and a softmax layer with 10 output units. We also apply max-\npooling layers of size 2x2 after the ﬁrst two convolutional layers and quadrant pooling after the third\nconvolutional layer.\nUnsupervised to supervised sample ratio\n(Supervised samples per class)\n50:1\n(100)\n10:1\n(500)\n5:1\n(1000)\n1:1\n(5000)\nClassiﬁcation\naccuracy\nimprovement\nover CNN\n(absolute %)\n5\n4\n3\n2\n1\n0\n-1\nFigure 3: The beneﬁts of unsupervised learning vs. un-\nsupervised to supervised sample ratio. When the ratio\nis 50:1, we see a 4.09% increase in performance. But\nthe beneﬁt shrinks as the ratio decreases. When the\nratio is 1:1, there is a penalty for using unsupervised\npre-training.\nWe train the zero-bias CAE on 100,000\nunlabeled images. We then ﬁne-tune the\nnetwork on each of the 10 provided splits\nof training set, each consisting of 1000\nsamples (100 samples per class), and eval-\nuate all of them on the test set. The accu-\nracies are subsequently averaged to obtain\nthe ﬁnal recognition accuracy. Similar to\nour CIFAR-10 experiments, we also train\na zero-bias CNN with the same structure\nas our zero-bias CAE on each of the splits\nto further highlight the beneﬁts of unsu-\npervised learning.\nTable 3 presents our results on the STL-\n10 dataset and compares them with other\nmethods. As expected, unsupervised pre-\ntraining gives a 3.87% increase over the\nrandomly initialized CNN.\n4.3.1\nADDITIONAL\nDATA AUGMENTATION:\nCOLOR AND CONTRAST\nThe current best result on STL-10 (Dosovitskiy et al. (2014)) makes extensive use of additional data\naugmentation including: scaling, rotation, color and two forms of contrast. They do not perform\nthese augmentations during supervised training, but during a discriminative unsupervised feature\nlearning period. We test the regularizing effects of these additional augmentations when applied\ndirectly to supervised training, and test how these regularization effects hold up when combined\nwith with unsupervised pre-training. To do this, we use some of these additional data-augmentations\nduring our supervised training: color augmentation and contrast augmentation.\nColor augmentation: The images are represented in HSV color space (h, s, v). Here we generate a\nsingle random number for each image and add it to the hue value for each pixel like so:\na\n∼\nUniform(−0.1, 0.1)\n(8)\nh\n=\nh + a\n(9)\nTable 2: Quantitative comparison with other methods on CIFAR-10 (A: Data Augmentation, D:\nDropout, U: Unsupervised Learning).\nAlgorithm\nAccuracy\nConvolutional Auto-encoders - Masci et al. (2011)\n79.20 %\nSingle layer K-means - Coates et al. (2011)\n79.60 %\nConvolutional K-means Networks - Coates and Ng (2011)\n82.00 %\nExemplar CNN - Dosovitskiy et al. (2014)\n82.00 %\nConvolutional Kernel Networks - Mairal et al. (2014)\n82.18 %\nNOMP - Lin and Kung (2014)\n82.90 %\nMax-Out Networks - Goodfellow et al. (2013b)\n90.65 %\nNetwork In Network - Lin et al. (2013)\n91.20 %\nDeeply-Supervised Nets - Lee et al. (2014)\n91.78 %\nZero-bias CNN +ADU\n86.44 %\nZero-bias CNN +AD\n86.70 %\n7\nAccepted as a workshop contribution at ICLR 2015\nTable 3: Quantitative comparison with other methods on STL-10 (A: Data Augmentation, D:\nDropout, C: Color Augmentation, U: Unsupervised Learning).\nAlgorithm\nAccuracy\nConvolutional K-means Networks - Coates and Ng (2011)\n60.1 % ± 1.0 %\nConvolutional Kernel Networks - Mairal et al. (2014)\n62.32 %\nHierarchical Matching Pursuit (HMP) - Bo et al. (2013)\n64.5 % ± 1.0 %\nNOMP - Lin and Kung (2014)\n67.9 % ± 0.6 %\nMulti-task Bayesian Optimization - Swersky et al. (2013)\n70.1 % ± 0.6 %\nExemplar CNN - Dosovitskiy et al. (2014)\n72.8 % ± 0.4 %\nZero-bias CNN +AD\n62.01 % ± 1.9 %\nZero-bias CNN +ADU\n65.88 % ± 0.9 %\nZero-bias CNN +ADC\n68.51 % ± 0.8 %\nZero-bias CNN +ADCU\n70.20 % ± 0.7 %\nContrast augmentation: Here we generate six random numbers for each image, with the following\ndistributions:\na, d\n∼\nUniform(0.7, 1.4)\n(10)\nb, e\n∼\nUniform(0.25, 4)\n(11)\nc, f\n∼\nUniform(−0.1, 0.1)\n(12)\nAnd use them to modify the saturation and value for every pixel in the image, like so:\ns\n=\nasb + c\n(13)\nv\n=\ndse + f\n(14)\nWe ﬁnd that a) additional data-augmentation is incredibly helpful, increasing accuracy by 6.5%, b)\nunsupervised pre-training still maintains an advantage (1.69%).\n5\nCONCLUSIONS\nWe present a new type of convolutional auto-encoder that has zero-bias and ReLU activations and\nachieves superior performance to previous methods. We conduct thorough experiments on CIFAR-\n10 to analyze the effects of unsupervised pre-training as a form of regularization when used in\nisolation and in combination with supervised forms of regularization such as data augmentation and\ndropout. We observe that, indeed, unsupervised pre-training can provide a large gain in performance\nwhen the ratio of unsupervised to supervised samples is large. Finally, we verify our ﬁndings by\napplying our model to STL-10, a dataset with far more unlabeled samples than labeled samples\n(100:1). We ﬁnd that with additional regularization, via color augmentation, our method is able to\nachieve nearly state-of-the-art results.\nCODE\nAll experiments were run using our own open source library Anna, which can be found at: https:\n//github.com/ifp-uiuc/anna\nCode to reproduce the experiments can be found at: https://github.com/ifp-uiuc/\nan-analysis-of-unsupervised-pre-training-iclr-2015\nACKNOWLEDGMENTS\nThis material is based upon work supported by the National Science Foundation under Grant No.\n392 NSF IIS13-18971. The two Tesla K40 GPUs used for this research were donated by the NVIDIA\nCorporation. We would like to acknowledge Theano (Bergstra et al. (2010)) and Pylearn2 (Good-\nfellow et al. (2013a)), on which our code is based. Also, we would like to thank Shiyu Chang for\nmany helpful discussions and suggestions.\n8\nAccepted as a workshop contribution at ICLR 2015\nREFERENCES\nYoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. Greedy layer-wise training\nof deep networks. Advances in neural information processing systems, 19:153, 2007.\nJames Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume\nDesjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU\nmath expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference\n(SciPy), June 2010. Oral Presentation.\nLiefeng Bo, Xiaofeng Ren, and Dieter Fox. Unsupervised feature learning for rgb-d based object\nrecognition. In Experimental Robotics, pages 387–402. Springer, 2013.\nAdam Coates and Andrew Y Ng. Selecting receptive ﬁelds in deep networks. In Advances in Neural\nInformation Processing Systems, pages 2528–2536, 2011.\nAdam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-\nvised feature learning. In International Conference on Artiﬁcial Intelligence and Statistics, pages\n215–223, 2011.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional neural networks. In Z. Ghahramani, M. Welling,\nC. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Pro-\ncessing Systems 27, pages 766–774. Curran Associates, Inc., 2014.\nDumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and\nSamy Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine\nLearning Research, 11:625–660, 2010.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accu-\nrate object detection and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013.\nIan J Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan\nPascanu, James Bergstra, Fr´ed´eric Bastien, and Yoshua Bengio. Pylearn2: a machine learning\nresearch library. arXiv preprint arXiv:1308.4214, 2013a.\nIan J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout\nnetworks. arXiv preprint arXiv:1302.4389, 2013b.\nGeoffrey Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural computation, 18(7):1527–1554, 2006.\nGeoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov.\nImproving neural networks by preventing co-adaptation of feature detectors.\nCoRR,\nabs/1207.0580, 2012.\nKevin Jarrett, Koray Kavukcuoglu, M Ranzato, and Yann LeCun. What is the best multi-stage ar-\nchitecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference\non, pages 2146–2153. IEEE, 2009.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Com-\nputer Science Department, University of Toronto, Tech. Rep, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Advances in neural information processing systems, pages 1097–1105,\n2012.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nChen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.\nDeeply-\nsupervised nets. arXiv preprint arXiv:1409.5185, 2014.\n9\nAccepted as a workshop contribution at ICLR 2015\nHonglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng.\nConvolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations. In Proceedings of the\n26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,\n2013.\nTsung-Han Lin and H. T. Kung. Stable and efﬁcient representation learning with nonnegativity\nconstraints. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Con-\nference on Machine Learning (ICML-14), pages 1323–1331. JMLR Workshop and Conference\nProceedings, 2014.\nJulien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.\nConvolutional kernel net-\nworks. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems 27, pages 2627–2635. Curran Associates,\nInc., 2014.\nJonathan Masci, Ueli Meier, Dan Cires¸an, and J¨urgen Schmidhuber. Stacked convolutional auto-\nencoders for hierarchical feature extraction. In Artiﬁcial Neural Networks and Machine Learning–\nICANN 2011, pages 52–59. Springer, 2011.\nRoland Memisevic, Kishore Konda, and David Krueger. Zero-bias autoencoders and the beneﬁts of\nco-adapting features. arXiv preprint arXiv:1402.3337, 2014.\nSalah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-\nencoders: Explicit invariance during feature extraction. In Proceedings of the 28th International\nConference on Machine Learning (ICML-11), pages 833–840, 2011.\nKevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. In Advances\nin Neural Information Processing Systems, pages 2004–2012, 2013.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.\nMatthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Robert Fergus. Deconvolutional net-\nworks. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages\n2528–2535. IEEE, 2010.\nMatthew D Zeiler, Graham W Taylor, and Rob Fergus. Adaptive deconvolutional networks for mid\nand high level feature learning. In Computer Vision (ICCV), 2011 IEEE International Conference\non, pages 2018–2025. IEEE, 2011.\n10\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2014-12-20",
  "updated": "2015-04-10"
}