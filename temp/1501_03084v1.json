{
  "id": "http://arxiv.org/abs/1501.03084v1",
  "title": "Deep Learning with Nonparametric Clustering",
  "authors": [
    "Gang Chen"
  ],
  "abstract": "Clustering is an essential problem in machine learning and data mining. One\nvital factor that impacts clustering performance is how to learn or design the\ndata representation (or features). Fortunately, recent advances in deep\nlearning can learn unsupervised features effectively, and have yielded state of\nthe art performance in many classification problems, such as character\nrecognition, object recognition and document categorization. However, little\nattention has been paid to the potential of deep learning for unsupervised\nclustering problems. In this paper, we propose a deep belief network with\nnonparametric clustering. As an unsupervised method, our model first leverages\nthe advantages of deep learning for feature representation and dimension\nreduction. Then, it performs nonparametric clustering under a maximum margin\nframework -- a discriminative clustering model and can be trained online\nefficiently in the code space. Lastly model parameters are refined in the deep\nbelief network. Thus, this model can learn features for clustering and infer\nmodel complexity in an unified framework. The experimental results show the\nadvantage of our approach over competitive baselines.",
  "text": "Deep Learning with Nonparametric Clustering\nGang Chen\nJanuary 14, 2015\nAbstract\nClustering is an essential problem in machine learning and data mining. One vital factor that\nimpacts clustering performance is how to learn or design the data representation (or features).\nFortunately, recent advances in deep learning can learn unsupervised features eﬀectively, and\nhave yielded state of the art performance in many classiﬁcation problems, such as character\nrecognition, object recognition and document categorization. However, little attention has been\npaid to the potential of deep learning for unsupervised clustering problems. In this paper, we\npropose a deep belief network with nonparametric clustering. As an unsupervised method, our\nmodel ﬁrst leverages the advantages of deep learning for feature representation and dimension\nreduction. Then, it performs nonparametric clustering under a maximum margin framework –\na discriminative clustering model and can be trained online eﬃciently in the code space. Lastly\nmodel parameters are reﬁned in the deep belief network. Thus, this model can learn features for\nclustering and infer model complexity in an uniﬁed framework. The experimental results show\nthe advantage of our approach over competitive baselines.\n1\nIntroduction\nClustering methods, such as k-means, Gaussian mixture model (GMM), spectral clustering and\nnon-parametrical Bayesian methods, have been widely used in machine learning and data mining.\nAmong various clustering methods, nonparametric Bayesian model is one of promising approaches\nfor data clustering, because of its ability to infer the model complexity from the data automatically.\nTo mine clusters or patterns from data, we can group them based on some notion of similarity. In\ngeneral, calculating the clustering similarity is dependent on the features describing data. Thus,\nfeature representation is vital for successful clustering. Just as common for other clustering methods,\nthe presence of noisy and irrelevant features can degrade clustering performance, making feature\nrepresentation an important factor in cluster analysis. Moreover, diﬀerent features may be relevant\nor irrelevant in the high dimensional data, suggesting the need for feature learning.\nRecent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction\n[9, 27] and classiﬁcation problems [10, 15, 23]. The advantages of deep learning are that they give\nmappings which can capture meaningful structure information in the code space and introduce\nbias towards conﬁgurations of the parameter space that are helpful for unsupervised learning [6].\nMore speciﬁcally, it learns the composition of multiple non-linear transformations (such as stacked\n1\narXiv:1501.03084v1  [cs.LG]  13 Jan 2015\nrestricted Boltzmann machines), with the purpose to yield more abstract and ultimately more useful\nrepresentations [3]. In addition, deep learning with gradient descent scales linearly in time and space\nwith the number of train cases, which makes it possible to apply to large scale data sets [9].\nUnfortunately, little work has been done to leverage the advantages of deep learning for unsupervised\nclustering problems. Moreover, unsupervised clustering also presents a challenge in the deep learning\nframework, compared to supervised methods in the ﬁnal ﬁne-tuning process. Another important\nresearch topic in clustering analysis is how to adapt model complexity for increasing volumes in the\nera of big data [21, 4, 24]. However, most approaches are generative models and have restrictions\non the prior base measures.\nIn this paper, we are interested in clustering problems and propose a deep belief network (DBN)\nwith nonparametric clustering. This approach is an unsupervised clustering method, inspired by\nthe advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models\n[1, 7, 4]. On the one hand, clustering performance depends heavily on data representation, which\nimplies the need for feature learning in clustering. On the other hand, while the nonparametric\nBayesian model can perform model selection and data clustering, it is intractable for non-conjugate\nprior; furthermore, it may not perform well on high-dimensional data, especially in terms of space\nand time complexity. Thus, we propose the deep learning with nonparametric maximum margin\nmodel for clustering analysis. Essentially, we ﬁrst pre-train DBN for feature learning and dimension\nreduction. Then, we will learn the clustering weights discriminatively with nonparametric maximum\nmargin clustering (NMMC), which can be updated online eﬃciently. Finally, we ﬁne-tune the model\nparameters in the deep belief network. Refer to Fig. (1) for visual understanding to our model.\nHence, our framework can handle high-dimensional input features with nonlinear mapping, and\ncluster large scale data sets with model selection using the online nonparametric clustering method.\nOur contributions can be mainly summarized as: (1) leveraging unsupervised feature learning with\nDBN for clustering analysis; (2) a discriminative approach for nonparametric clustering under max-\nimum margin framework. The experimental results show advantages of our model over competitive\nbaselines.\n2\nRelated work\nClustering has been an interesting research topic for decades, including a wide range of techniques,\nsuch as generative/discriminative and parametric/nonparametric approaches. As an discriminative\nmethod, maximum margin clustering (MMC) treats the label of each instance as a latent variable\nand uses SVM for clustering with large margins. However, they [2, 28] either cannot learn parameters\nonline eﬃciently or need to deﬁne the number of clusters like other clustering approaches, such\nas k-means, Gaussian mixture model (GMM) and spectral clustering. Considering the weakness\nof parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been\nproposed to handle the model complexity problems. One of the widely used nonparametric models\nfor clustering is Dirichlet process mixture (DPM) [1, 7]. DPM can learn the number of mixture\ncomponents without speciﬁed in advance, which can grow as new data come in.\nHowever, the\nbehavior of the model is sensitive to the choice of prior base measure G0. In addition, DPM of\nGaussians need to calculate mean and covariance for each component, and update covariance with\nCholesky decomposition, which may lead to high space and time complexity in high-dimensional\n2\nh1 \nW1 \nV \nhL-1 \nWL-1 \nWL \nΘ \nhL \nFeature Learning with DBN \nNMMC \nFine-tuning Process \nFigure 1: In this DBN, L indicates the total number of hidden layers, Wi is the weight between\nadjacent layers, for i = {1, .., L} and Θ is the weight for clustering learned with NMMC. This graph\ndemonstrates 3 steps in our model: (1) Feature learning with deep belief network (DBN), with\nweights learned layer by layer as described above; (2) Perform clustering analysis with NMMC,\nwhich can assign a cluster label for each element in the data; (3) Update the model parameters with\nﬁne-tuning process (only for WL and Θ).\ndata. Unsupervised feature learning with deep structures was ﬁrst proposed in [9] for dimension\nreduction. Later, this unsupervised approach was developed into semi-supervised embedding [27]\nand supervised mapping [16] scenarios. Many other supervised approaches also exploit deep learning\nfor feature extraction and then learn a discriminative classiﬁer with objectives, e.g., square loss [9],\nlogistic regression [15] or support vector machine (SVM) [13, 23] for classiﬁcation in the code space.\nThe success behind deep learning is that it can learn useful information for data visualization and\nclassiﬁcation [6, 3]. Thus, it is desirable to leverage deep learning for clustering analysis, because the\nperformance for clustering depends heavily on data representation. Unfortunately, little attention\nhas been paid to leveraging deep learning for unsupervised clustering problems.\nA recent interesting approach is the implicit mixture of RBMs [17].\nInstead of modeling each\ncomponent with Gaussian distribution, it models each component with RBM. It is formulated as a\nthird-order Boltzmann machine with cluster label as the hidden variable for each instance. However,\nit also requires the number of clusters speciﬁed as input.\nIn this paper, we are interested in deep learning for unsupervised clustering problems.\nIn our\nframework, we take advantage of deep learning for representation learning, which is helpful for\nclustering analysis. Moreover, we take an discriminative approach, namely nonparametric maximum\nmargin clustering to infer model complexity online, without the prior measure assumption as DPM.\n3\nDeep learning with nonparametric maximum margin clustering\nIn this section, we will ﬁrst review RBM and DBN for feature learning. Then, we will introduce\nnonparametric maximum margin clustering (NMMC) method given the feature learned from DBN.\n3\nFinally, we will ﬁne-tune our model given the clustering labels for the data.\n3.1\nFeature learning with deep belief network\nAssume that we have a training set D = {vi}N\ni=1, where vi ∈Rd. An RBM with n hidden units is a\nparametric model of the joint distribution between a layer of hidden variables h = (h1, ..., hn) and\nthe observations v = (v1, ..., vd). The RBM joint likelihood takes the form:\np(v, h) ∝e−E(v,h)\n(1)\nwhere the energy function is\nE(v, h) = −hT W1v −bT v −cT h\n(2)\nAnd we can compute the following conditional likelihood:\np(v|h) =\nY\ni\np(vi|h)\n(3a)\np(vi = 1|h) = logistic(bi +\nX\nj\nW1(i, j)hj)\n(3b)\np(hi = 1|v) = logistic(ci +\nX\nj\nW1(j, i)vj)\n(3c)\nwhere logistic(x) = 1/(1 + e−x). To learn RBM parameters, we need to optimize the negative log\nlikelihood −logp(v) on training data D, the parameters updating can be calculated with a eﬃcient\nstochastic descent method, namely contrastive divergence (CD) [10].\nA Deep Belief Network (DBN) is composed of stacked RBMs [9] learned layer by layer greedily,\nwhere the top layer is an RBM and the lower layers can be interpreted as a directed sigmoid belief\nnetwork [3], shown in Fig. (1). Suppose the DBN used here has L layers, and the weight for each\nlayer is indicated as Wi for i = {1, .., L}. Speciﬁcally, we think RBM is a 1-layer DBN, with weight\nW1. Thus, DBN can learn parametric nonlinear mapping from input v to output x, f : v →x.\nFor example, for 1-layer DBN, we have x = logistic(W1T v + c). After we learn the representation\nfor the data, we use NMCC for clustering analysis to model the data distribution.\n3.2\nNonparametric maximum margin clustering\nNonparametric maximum margin clustering (NMMC) is a discriminative clustering model for clus-\ntering analysis. Given the nonlinear mapping with DBN, we can ﬁrst map the original training data\nD = {vi}N\ni=1 into codes X = {xi}N\ni=1 in the embedding space. Then, with X = {xi}N\ni=1 and its the\ncluster indicators z = {zi}N\ni=1, we propose the following conditional probability for nonparametric\nclustering:\nP(z, {θk}K\nk=1|X) ∝p(z)\n\u0014 N\nY\ni=1\np(xi|θzi)\n\u0015 K\nY\nk=1\np(θk)\n(4)\n4\nwhere K is the number of clusters, p(xi|θzi) is the likelihood term deﬁned in Sec. 3.2.1 and p(θk)\ncan be thought as the Gaussian prior for k = [1, ..., K]. Note that the prior p(θk) will be used in\nthe maximum margin learning in Eq. (12). p(z) = Γ(α) QK\nk=1 Γ(nk+α/K)\nΓ(n+α)Γ(α/K)K\nis the symmetric Dirichlet\nprior, where nk is the number of element in the cluster k, and α is the concentration parameter.\nRecall that Dirichlet process mixture (DPM) [1, 7] is the widely used nonparametric Bayesian\napproach for clustering analysis and model learning, speciﬁed with DP prior measure G0 and α. As\na joint likelihood model, it has to model p(X), which is intractable for non-conjugate prior. The\nessential diﬀerence between our model and DPM is that we maximize a conditional probability,\ninstead of joint probability as in DPM [14]. Moreover, our approach is a discriminative clustering\nmodel with component parameters learned under maximum margin framework.\nTo maximizing the objective function in Eq. (4), we hope the higher within-cluster correlation and\nlower correlation between diﬀerent clusters. Given z, we will need to learn {θk}K\nk=1 to keep each\ncluster as compact as possible, which in turn will help infer better K. In other words, to keep the\nobjective climbing, we need higher likelihood p(xi|θzi) with higher correlation within-cluster, which\ncan be addressed with discriminative clustering. Given the component parameters, {θk}K\nk=1, we\nneed to decide the label for each element for better K. For each round (on the instance level), we\nuse Gibbs sampling to infer zi for each instance xi, which in turn can be used to estimate {θk}K\nk=1\nwith online maximum margin learning. For each iteration (on the whole dataset), we also update\nα with adaptive rejection sampling [18].\n3.2.1\nGibbs sampling\nGiven the data points X = {xi}N\ni=1 and its the cluster indicators z = {zi}N\ni=1, the Gibbs sampling\ninvolves iterations that alternately draw samples from conditional probability while keeping other\nvariables ﬁxed. For each indicator variable zi, we can derive its conditional posterior as follows:\np(zi = k|z−i, xi, {θk}K\nk=1, α, λ)\n(5)\n=\np(zi = k|xi, z−i, {θk}K\nk=1)\n(6)\n∝\np(zi = k|z−i, {θk}K\nk=1)p(xi|zi = k, {θk}K\nk=1)\n(7)\n=\np(zi = k|z−i, α)p(xi|θk)\n(8)\nwhere the subscript −i indicates all indices except for i, p(zi = k|z−i, α) is determined by Chinese\nrestaurant process, and p(xi|θk) is the likelihood for the current observation xi. For DPM, we need\nto maximize the conditional posterior to compute θk, which depends on observations belonging to\nthis cluster and prior G0.\nIn our conditional likelihood model, we deﬁne the following likelihood for instance xi\np(xi|θk) ∝exp(xT\ni θk −λ||θk||2)\n(9)\nwhere λ is a regularization constant to control weights between the two terms above. By default,\nthe prediction function should be proportional to arg maxk(xT\ni θk), for k ∈[1, K]. In other words,\nhigher correlation between xi and θk indicates higher probability that xi belongs to cluster k, which\nfurther leads to higher objective in Eq. (4). In our likelihood deﬁnition, we also subtract λ||θk||2\n5\nin Eq. (9), which can keep the maximum margin beneﬁcial properties in the model to separate\nclusters as far away as possible. Another understanding for the above likelihood is that Eq. (9)\nsatisﬁes the general form of exponential families, which are functions solely of the chosen suﬃcient\nstatistics [22]. Thus, such probability assumption in Eq. (9) make it general to real applications.\nPlug Eq. (9) into Eq. (8), we get the ﬁnal Gibbs sampling strategy for our model\np(zi = k|z−i, xi, {θk}K\nk=1, α, λ)\n∝p(zi = k|z−i, α)exp(xT\ni θk −λ||θk||2)\n(10)\nWe will introduce online maximum margin learning for component parameters {θk}K\nk=1 in Sec 3.2.2.\nFor the newly created cluster, we assume θK+1 is sampled from multivariate t-distribution.\n3.2.2\nOnline maximum margin learning\nWe follow the passive aggressive algorithm (PA) [5] below in order to learn component parameters\nin our discriminative model with maximum margins [25].\nWe denote the instance presented to the algorithm on round t by xt ∈Rn, which is associated with a\nunique label zt ∈[1, K]. Note that the label zt is determined by the above Gibbs sampling algorithm\nin Eq. (10). We shall deﬁne Θ = [θ1, ..., θK] a parameter vector by concatenating all the parameters\n{θk}K\nk=1 (that means Θzt is zt-th block in Θ, or says Θzt = θzt), and Φ(xt, zt) is a feature vector\nrelating input xt and output zt, which is composed of K blocks, and all blocks but the zt-th are set\nto be the zero vector while the zt-th block is set to be xt. We denote by Θt the weight vector used\nby the algorithm on round t, and refer to the term γ(Θt; (xt, zt)) = Θt · Φ(xt, zt) −Θt · Φ(xt, ˆzt)\nas the (signed) margin attained on round t. In this paper, we use the hinge-loss function, which is\ndeﬁned by the following,\nℓ(Θ; (xt, zt)) =\n\u001a 0\nif γ(Θt; (xt, zt)) ≥1\n1 −γ(Θt; (xt, zt))\notherwise\n(11)\nFollowing the passive aggressive (PA) algorithm [5], we optimize the objective function:\nΘt+1 = arg min\nΘ\n1\n2||Θ −Θt||2 + Cξ\ns.t. ℓ(Θ; (xt, zt)) ≤ξ\n(12)\nwhere the l2 norm of Θ on the right hand size can be thought as Gaussian prior in Eq. (4). If\nthere’s loss, then the updates of PA-1 has the following closed form\nΘzt\nt+1 = Θzt\nt + τtxt,\nΘ ˆzt\nt+1 = Θ ˆzt\nt −τtxt,\n(13)\nwhere ˆzt is the label prediction for xt, and τt = min{C, ℓ(Θt;(xt,zt))\n||xt||2\n}. Note that the Gibbs sampling\nstep can decide the indicator variable zt for xt. Given the cluster label (the ground truth assignment)\nfor xt, we update our parameter Θ using the above Eq. (13). For convergence analysis and time\ncomplexity, refer to [5].\n6\n3.3\nFine-tuning the model\nHaving determined the number of clusters and labels for all training data, we can take the ﬁne-tuning\nprocess to reﬁne the DBN parameters. Note that the objective function in Eq. (12) takes the l1\nhinge loss as in [23]. Thus, one possible way is that we can take the sub-gradient and backpropagate\nthe error to update DBN parameters. In our approach, we employ another method and only update\nthe top layer weights WL and Θ in the deep structures. This ﬁne-tuning process is inspired by\nthe classiﬁcation RBM [15] for model reﬁning. Basically, we assume the top DBN layer weight WL\nand SVM weight Θ can be combined into a classiﬁcation RBM as in [15] by maximizing the joint\nlikelihood p(x, z) after we infer the cluster labels for all instances with NMMC. Note that there is\nmapping from SVM’s scores to probabilistic outputs with logistic function [19], which can maintain\nlabel consistency between the SVM classiﬁer and the softmax function. Thus, the SVM weight Θ\ncan be used to initialize the weight of the softmax function in the classiﬁcation RBM. After the\nﬁne-tuning process, we can maxz p(z|v) for z ∈[1, K] to label the unknown data v. For 1-layer\nDBN, we can get the following classiﬁcation probability:\np(z|v) =\nedz Qn\nj=1\n\u00001 + ecj+Θjz+P\ni W1(i,j)vi\u0001\nP\nz∗edz∗Qn\nj=1\n\u00001 + ecj+Θjz∗+P\ni W1(i,j)vi\u0001\n(14)\nwhere dz for z ∈[1, K] is the bias of clustering labels, and cj for j ∈[1, n] are biases of the hidden\nunits. Note that Θ has been reshaped into n×K matrix before updating in the ﬁne-tuning process.\nFor the deep neural network with more than one layer, we ﬁrst project v into the coding space x,\nthen use the above equation for classiﬁcation.\nIn our algorithm, we only ﬁne-tune in the top layer because of the following reasons: (1) the objective\nfunction in Eq. (4) with deep feature learning is non-convex, which can be easily trapped into local\nminimum with L-BFGS [9]; (2) if there was clustering error in the top layer, it could be easily\npropagated in the backpropagation stage; (3) To only update the top layer can eﬀectively handle\nthe overﬁtting problem.\n4\nExperimental Results\nIn order to analyze our model, we performed clustering analysis on two types of data: images and\ndocuments, and compared our results to competitive baselines. For all experiments, including pre-\ntraining and ﬁne-tuning, we set the learning rate as 0.1, the maximum epoch to be 100, and used\nCD-1 to learn the weights and biases in the deep belief network. We used the adjusted Rand Index\n[11, 20] to evaluate all the clustering results.\nClustering on MNIST dataset: The MNIST dataset1 consists of 28 × 28-size images of hand-\nwriting digits from 0 through 9 with a training set of 60,000 examples and a test set of 10,000\nexamples, and has been widely used to test character recognition methods. In the experiment, we\nrandomly sample 5000 images from the training sets for parameter learning and 1000 examples from\nthe testing sets to test our model. After learning the features with DBN in the pre-training stage,\nwe used NMMC for clustering, with setting α = 4, λ = 15 and C = 0.001. In the experiment, λ\n1http://yann.lecun.com/exdb/mnist/\n7\n(a)\n(b)\nFigure 2: The visualization of learned weights in the pre-training and ﬁne-tuning stages respectively\nwith 1-layer DBN for n = 100 on the MNIST dataset.\n0\n50\n100\n150\n200\n250\n300\n350\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nthe number of dimensions\nrand Index\nThe rand Index changes with dimensions\n \n \nrand Index (train+pretrain)\nrand Index (train+finetune)\nrand Index (test+pretrain)\nrand Index (test+finetune)\n0\n1\n2\n3\n4\n5\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nthe number of layers\nrand Index\nThe rand Index changes with depth\n \n \nrand Index (train+pretrain)\nrand Index (train+finetune)\nrand Index (test+pretrain)\nrand Index (test+finetune)\n(a)\n(b)\nFigure 3: How the dimensionality and structural depth inﬂuence performance on MNIST dataset.\n(a) how the Rand Index changes with the encoded data dimension; (b) how the Rand Index changes\nwith the depth of deep structures. It demonstrates the ﬁne-tuning process is helpful to improve\nclustering performance.\nIt also shows that complex deep structures cannot improve clustering\naccuracy.\nplays a vital role on the ﬁnal number of clusters. Higher λ, larger number of clusters generated. To\nmake an fair comparison, we basically tuned parameters to keep the number of generated clusters\nclose to the groundtruth in the training stage. For example, in the MNIST experiment, we keep it\naround 5 to 20 in the training set for both NMMC and DPM. The results from baselines such as\nk-means and GMM should be conceived as upper bound (specify the number of clusters K = 10).\nThe clustering performance of our method (DBN+NMMC) is shown in Table (1), where “pre-\ntrain” and “ﬁne-tune” indicate how the accuracy changes before and after the ﬁne-tuning process\nfor the same parameter setting on the same dataset. The results with 2-layer DBN in Table (1)\n8\ndemonstrate that our method signiﬁcantly outperforms baselines. It also shows that ﬁne-tuning\nprocess can greatly improve accuracy, especially on the testing data. In Table (1), we think the\nlargest train/test diﬀerence for the least complex model is caused by biases between before and\nafter ﬁnetuning. In other words, the ﬁne-tuning step can learn better biases via classiﬁcation RBM\nand improve testing performance. We also visualize how the weights change before and after the\nﬁne-tuning process in Fig. (2).\nWe also evaluate how the depth and dimensionality of deep structures inﬂuence clustering accuracy.\nFig. 3(a) shows how adjusted Rand Index changes with the number of dimensions for 1-layer DBN\n(or RBM), and it demonstrates that higher dimensionality does not mean higher performance. In\nFig. 3(a), we can see ﬁne-tuning severely hurt performance on the training set on higher dimension\ncoding space, we guess it is caused by overﬁtting problem in the complex model. In other words,\nthe wrong clustering prediction will deteriorate the clustering performance even further through\nﬁne-tuning. That makes sense because we treat the wrong labeling as the correct one in the ﬁne-\ntuning stage. It also veriﬁes that it is reasonable by just ﬁne-tuning the model in the top layer,\ninstead of the whole network, with the purpose to reduce the overﬁtting problem. Fig. 3(b) shows\nthat given the 100 hidden nodes in the top layer, how the performance changes with the depth of\nDBN structure. It seems that the deeper complex model cannot guarantee better performance.\nTo verify whether our NMMC is eﬀective for data clustering and model selection, we also compare\nour NMMC to DPM given the same DBN for feature learning. The results in Fig. (4) demonstrates\nthat NMMC outperforms DPM signiﬁcantly and also shows that our NMMC can always converge\nafter 100 iterations. The time complexity comparison between our method and DPM is shown in\nFig. 5 in the DBN projection space. It shows that our method is signiﬁcantly eﬃcient, compared\nto DPM. To manifest how eﬀective our method is, we also show the upper bound DBN+GMM,\nwith 2 layers n = [400, 100] in Table (1). It shows that features learned with DBN are helpful for\nclustering, compared to raw data. It also shows that our method yields better clustering results\nthan the upper bound.\nClustering on 20 newsgroup: We also evaluated our model on 20 newsgroup datasets for docu-\nment categorization. This document dataset has 20 categories, which has been widely used in text\ncategorization and document classiﬁcation. In the experiment, we tested our model on the binary\nversion of the 20 newsgroup dataset2. We used the training set for training and tested the model\non the testing dataset. After we learned features in the DBN, we used NMMC for clustering, with\nsetting α = 4, λ = 30 and C = 0.001. To make an fair comparison, we basically took a similar\nsetting as in the MNIST dataset, for both NMMC and DPM in order to generate the number of\nclusters which is comparable for both methods. Baselines such as k-means and GMM should be\nthought of as upper bound because they need to specify the number of clusters K = 20.\nThe clustering performance of our method (DBN+NMMC) on 20 newsgroups is shown in Table.\n(2). It also demonstrates that the ﬁne-tuning process can greatly improve accuracy, especially on the\ntesting data. Although our model cannot beat baselines on the training set, our model can achieve\nbetter evaluation performance on the testing set (better than GMM and k-means on the raw data\nclustering). To verify whether our NMMC is eﬀective for data clustering and model selection, we\nalso compare our NMMC to DPM given the same DBN for feature learning. The results in Fig.\n2http://www.cs.toronto.edu/~larocheh/public/datasets/20newsgroups/20newsgroups_{train,valid,\ntest}_binary_5000_voc.txt\n9\nModel\nrand Index\nF-value\ntrain\ntest\ntrain\ntest\nDBN+NMMC (pre-train, n = 100)\n0.363 ± 0.038\n0.181 ± 0.07\n0.442 ± 0.032\n0.285 ± 0.063\nDBN+NMMC (ﬁne-tune, n = 100)\n0.371 ± 0.039\n0.392 ± 0.043\n0.447 ± 0.033\n0.467 ± 0.036\nDBN+NMMC (pre-train, n = [400, 100])\n0.419 ± 0.022\n0.232 ± 0.09\n0.483 ± 0.02\n0.319 ± 0.07\nDBN+NMMC (ﬁne-tune, n = [400, 100])\n0.428 ± 0.021\n0.453 ± 0.02\n0.492 ± 0.02\n0.513 ± 0.016\nDBN+NMMC (pre-train, n = [400, 400, 100])\n0.302 ± 0.017\n0.218 ± 0.055\n0.394 ± 0.014\n0.317 ± 0.046\nDBN+NMMC (ﬁne-tune, n = [400, 400, 100])\n0.309 ± 0.015\n0.326 ± 0.015\n0.40 ± 0.012\n0.415 ± 0.02\nDBN+NMMC (pre-train, n = [400, 300, 200, 100])\n0.334 ± 0.05\n0.31 ± 0.08\n0.423 ± 0.04\n0.40 ± 0.07\nDBN+NMMC (ﬁne-tune, n = [400, 300, 200, 100])\n0.34 ± 0.051\n0.364 ± 0.054\n0.433 ± 0.04\n0.45 ± 0.045\nPCA+NMMC (n = 100)\n0.381 ± 0.02\n0.251 ± 0.022\n0.452 ± 0.02\n0.353 ± 0.02\nIMRBM [17] (n = 100, K = 10)\n0.13 ± 0.04\n0.10 ± 0.03\n0.23 ± 0.02\n0.22 ± 0.02\nk-means (K = 10)\n0.356 ± 0.029\n0.367 ± 0.03\n0.446 ± 0.026\n0.451 ± 0.026\nGMM (K = 10)\n0.356 ± 0.029\n0.394 ± 0.04\n0.446 ± 0.025\n0.465 ± 0.026\nSpectral Clustering (K = 10)\n0.354 ± 0.057\n0.359 + 0.035\n0.423 ± 0.045\n0.423 ± 0.03\nDBN + kmeans (K = 10)\n0.411 ± 0.016\n0.316 ± 0.027\n0.473 ± 0.015\n0.401 ± 0.019\nDBN + GMM (K = 10)\n0.411 ± 0.016\n0.406 ± 0.022\n0.473 ± 0.015\n0.467 ± 0.024\nTable 1: The experimental comparison on MNIST dataset, where “train” means the training data,\n“test” indicates the testing data, n speciﬁes the number of hidden variables for each layer (for\nexample, n = [400, 100] indicates DBN has two layers, the ﬁrst layer has 400 hidden nodes, and the\nsecond layer has 100 hidden nodes). For PCA+NMMC, we ﬁrst use PCA project the data into 100\ndimensions, then perform NMMC for clustering. It demonstrates that the ﬁne-tuning process in\nour model can improve clustering performance greatly, and our method (DBN+NMMC) beats the\nbaselines remarkably when n = [400, 100].\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nthe number of iterations\nrand Index\nThe rand Index changes with Gibbs sampling steps\n \n \nDBN(n=100)+DPM\nDBN(n=100)+NMMC\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nthe number of iterations\nrand Index\nThe rand Index changes with Gibbs sampling steps\n \n \nDBN(n=[400 100])+DPM\nDBN(n=[400 100])+NMMC\n(a)\n(b)\nFigure 4: The performance comparison between DPM and NMMC on the MNIST dataset with the\nsame DBN structure for feature learning. (a) it is a 1-layer DBN (or RBM) with the number of\nhidden nodes n = 100; (b) it is a 2-layers DBN, with n = [400, 100] for each layer. It demonstrates\nthat with the same DBN for feature learning, NMMC outperforms DPM remarkably.\n(6) demonstrate that NMMC outperforms DPM remarkably. To test how time complexity changes\nwith respect to the number of dimensions in the projected space, we tried diﬀerent coding spaces\nand compared our method with DPM, with results shown in Fig. 5. Again, it demonstrates our\n10\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n0\n500\n1000\n1500\n2000\n2500\nthe number of training samples\nthe time of training (seconds)\ntime changes with the number of training samples\n \n \nDPM\nNMMC\n0\n50\n100\n150\n200\n250\n300\n350\n400\n0\n1000\n2000\n3000\n4000\n5000\n6000\nthe data dimensionality\nthe time of training (seconds)\ntime changes with the number of dimensions\n \n \nDPM\nNMMC\n(a)\n(b)\nFigure 5: The complexity comparison between DPM and NMMC in the data projection space. (a)\nshows how the time complexity varies with the number of training data on the MNIST data set,\nunder the 1-layer DBN with 100 hidden nodes; (b) shows how the time complexity changes with\nthe number of hidden nodes on the 20 newsgroup dataset, under the 1-layer DBN. It shows that\nour method is more eﬃcient than DPM on the data clustering.\nModel\nrand Index\nF-value\ntrain\ntest\ntrain\ntest\nDBN+NMMC (pre-train, n = 200)\n0.059 ± 0.02\n0.034 ± 0.016\n0.131 ± 0.017\n0.11 ± 0.012\nDBN+NMMC (ﬁne-tune, n = 200)\n0.069 ± 0.023\n0.065 ± 0.025\n0.142 ± 0.019\n0.141 ± 0.02\nDBN+NMMC (pre-train, n = [1000, 200])\n0.048 ± 0.014\n0.028 ± 0.007\n0.109 ± 0.005\n0.098 ± 0.007\nDBN+NMMC (ﬁne-tune, n = [1000, 200])\n0.047 ± 0.015\n0.043 ± 0.013\n0.108 ± 0.006\n0.104 ± 0.004\nPCA+NMMC (n = 200)\n0.036 ± 0.005\n0.016 ± 0.012\n0.11 ± 0.005\n0.087 ± 0.010\nIMRBM [17] (n = 200, K = 20)\n0.015 ± 0.005\n0.013 ± 0.002\n0.096 ± 0.004\n0.093 ± 0.004\nk-means (K = 20)\n0.075 ± 0.02\n0.032 ± 0.004\n0.140 ± 0.019\n0.109 ± 0.016\nGMM (K = 20)\n0.075 ± 0.021\n0.051 ± 0.006\n0.140 ± 0.019\n0.114 ± 0.016\nSpectral Clustering (K = 20)\n0.058 ± 0.02\n0.061 ± 0.017\n0.126 ± 0.013\n0.129 ± 0.006\nDBN + Kmeans (K = 20)\n0.237 ± 0.007\n0.06 ± 0.036\n0.279 ± 0.008\n0.119 ± 0.026\nDBN + GMM (K = 20)\n0.239 ± 0.009\n0.125 ± 0.056\n0.281 ± 0.006\n0.185 ± 0.045\nTable 2: The experimental comparison on the 20 newsgroup dataset, where “train” means for\ntraining data, “test” indicates testing data. It demonstrates that the ﬁne-tuning process in our\nmodel can improve clustering performance. We compare the performances between our method\nand other baselines. It demonstrates that our method (DBN+NMMC) yields clustering accuracy\ncomparable to baselines, and performs better on the testing sets with 1-layer DBN.\nmethod is more eﬃcient in practice.\nTo sum up, our model can converge well after 100 iterations from the experiments above. Moreover,\nthe ﬁne-tuning process in our model can greatly improve the performance on the test sets. Thus, it\nalso shows that the parameters learned with NMMC can be embedded well in the deep structures.\n11\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\nthe number of iterations\nrand Index\nThe rand Index changes with Gibbs sampling steps\n \n \nDBN(n=200)+DPM\nDBN(n=200)+NMMC\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nthe number of iterations\nrand Index\nThe rand Index changes with Gibbs sampling steps\n \n \nDBN(n=[1000 200])+DPM\nDBN(n=[1000 200])+NMMC\n(a)\n(b)\nFigure 6: The performance comparison between DPM and NMMC with the same DBN structure\nfor feature learning on 20 newsgroups. (a) it is a 1-layer DBN (or RBM) with the number of hidden\nnodes n = 200; (b) it is a 2-layers DBN, with n = [1000, 200] for each layer. It demonstrates that\nwith the same DBN for feature learning, NMMC outperforms DPM remarkably.\nConclusion\nClustering is an important problem in machine learning and its performance highly depends on\ndata representation. And, how to adapt the model complexity with data also pose a challenge.\nIn this paper, we propose a deep belief network with nonparametric maximum margin clustering.\nThis approach is inspired by recent advances of deep learning for representation learning.\nAs\nan unsupervised method, our model leverages deep learning for feature learning and dimension\nreduction. Moreover, our approach with nonparametric maximum margin clustering (NMMC) is a\ndiscriminative clustering method, which can adapt model size automatically when data grows. In\naddition, the ﬁne-tuning process can incorporate NMMC well in the deep structures. Thus, our\napproach can learn features for clustering and infer model complexity in an uniﬁed framework. We\ncurrently use DBN [10] instead of deep autoencoders [9] for fast feature learning because the latter\nis time-consuming for dimension reduction. In future work, we will explore deep autoencoders to\nlearn better feature representation for clustering analysis. Another interesting topic to be explored\nis how to optimize the depth of deep learning structures in order to improve clustering performance.\nReferences\n[1] Antoniak, C.E.: Mixtures of dirichlet processes with applications to bayesian nonparametric\nproblems. Annals of Statistics (1974)\n[2] Ben-Hur, A., Horn, D., Siegelmann, H.T., Vapnik, V.: Support vector clustering. J. Mach.\nLearn. Res. 2, 125–137 (2001)\n12\n[3] Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new perspectives.\nPAMI pp. 1798–1828 (2013)\n[4] Blei, D.M., Jordan, M.I.: Variational inference for dirichlet process mixtures. Bayesian Analysis\n1, 121–144 (2005)\n[5] Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., Singer, Y.: Online passive-aggressive\nalgorithms. JMLR pp. 551–585 (2006)\n[6] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.A., Vincent, P., Bengio, S.: Why does\nunsupervised pre-training help deep learning? J. Mach. Learn. Res. 11, 625–660 (Mar 2010),\nhttp://dl.acm.org/citation.cfm?id=1756006.1756025\n[7] Ferguson, T.S.: A Bayesian analysis of some nonparametric problems. The Annals of Statistics\n1(2), 209–230 (1973)\n[8] Hannah, L.A., Blei, D.M., Powell, W.B.: Dirichlet process mixtures of generalized linear mod-\nels. J. Mach. Learn. Res. pp. 1923–1953 (2011)\n[9] Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural networks.\nScience 313(5786), 504–507 (Jul 2006)\n[10] Hinton, G.E., Osindero, S., Teh, Y.W.: A fast learning algorithm for deep belief nets. Neural\nComput. 18(7), 1527–1554 (jul 2006)\n[11] Hubert, L., Arabie, P.: Comparing partitions. Journal of classiﬁcation 2(1), 193–218 (1985)\n[12] Knowles, D.A., Palla, K., Ghahramani, Z.: A nonparametric variable clustering model. In:\nNIPS. pp. 2996–3004 (2012)\n[13] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep convolutional\nneural networks. In: Advances in Neural Information Processing Systems (2012)\n[14] Kurihara, K., Welling, M., Teh, Y.: Collapsed variational Dirichlet process mixture models.\nIn: Proc. Int. Jt. Conf. Artif. Intell. vol. 20, pp. 2796–2801 (2007)\n[15] Larochelle, H., Mandel, M., Pascanu, R., Bengio, Y.: Learning algorithms for the classiﬁcation\nrestricted boltzmann machine. J. Mach. Learn. Res. 13(1), 643–669 (Mar 2012)\n[16] Min, M.R., van der Maaten, L., Yuan, Z., Bonner, A.J., Zhang, Z.:\nDeep supervised t-\ndistributed embedding. In: Frnkranz, J., Joachims, T. (eds.) ICML. pp. 791–798. Omnipress\n(2010)\n[17] Nair, V., Hinton, G.E.: Implicit mixtures of restricted boltzmann machines. In: Koller, D.,\nSchuurmans, D., Bengio, Y., Bottou, L. (eds.) NIPS. pp. 1145–1152. Curran Associates, Inc.\n(2008)\n[18] Neal, R.M.: Markov chain sampling methods for dirichlet process mixture models. JOURNAL\nOF COMPUTATIONAL AND GRAPHICAL STATISTICS pp. 249–265 (2000)\n[19] Platt, J.C.: Probabilistic outputs for support vector machines and comparisons to regularized\nlikelihood methods. In: ADVANCES IN LARGE MARGIN CLASSIFIERS. pp. 61–74 (1999)\n13\n[20] Rand, W.: Objective criteria for the evaluation of clustering methods. Journal of the American\nStatistical Association 66(336), 846–850 (1971)\n[21] Rasmussen, C.E.: The inﬁnite gaussian mixture model. In: NIPS12. pp. 554–560. MIT Press\n(2000)\n[22] Sudderth, E.B.: Graphical models for visual object recognition and tracking. Ph.D. thesis, MIT\n(2006), http://www.cs.brown.edu/~sudderth/papers/sudderthPhD.pdf\n[23] Tang, Y.: Deep learning using support vector machines. In: Workshop on Representational\nLearning, ICML 2013. vol. abs/1306.0239 (2013)\n[24] Teh, Y.W.: Dirichlet processes. In: Encyclopedia of Machine Learning. Springer (2010)\n[25] Vapnik, V.N.: The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc.\n(1995)\n[26] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A.: Stacked denoising au-\ntoencoders: Learning useful representations in a deep network with a local denoising crite-\nrion. J. Mach. Learn. Res. 11, 3371–3408 (Dec 2010), http://dl.acm.org/citation.cfm?id=\n1756006.1953039\n[27] Weston, J., Ratle, F.: Deep learning via semi-supervised embedding. In: International Confer-\nence on Machine Learning (2008)\n[28] Xu, L., Neufeld, J., Larson, B., Schuurmans, D.: Maximum margin clustering. In: NIPS17. pp.\n1537–1544 (2005)\n14\n",
  "categories": [
    "cs.LG",
    "68T10",
    "I.2.6"
  ],
  "published": "2015-01-13",
  "updated": "2015-01-13"
}