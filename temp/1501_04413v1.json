{
  "id": "http://arxiv.org/abs/1501.04413v1",
  "title": "Statistical-mechanical analysis of pre-training and fine tuning in deep learning",
  "authors": [
    "Masayuki Ohzeki"
  ],
  "abstract": "In this paper, we present a statistical-mechanical analysis of deep learning.\nWe elucidate some of the essential components of deep learning---pre-training\nby unsupervised learning and fine tuning by supervised learning. We formulate\nthe extraction of features from the training data as a margin criterion in a\nhigh-dimensional feature-vector space. The self-organized classifier is then\nsupplied with small amounts of labelled data, as in deep learning. Although we\nemploy a simple single-layer perceptron model, rather than directly analyzing a\nmulti-layer neural network, we find a nontrivial phase transition that is\ndependent on the number of unlabelled data in the generalization error of the\nresultant classifier. In this sense, we evaluate the efficacy of the\nunsupervised learning component of deep learning. The analysis is performed by\nthe replica method, which is a sophisticated tool in statistical mechanics. We\nvalidate our result in the manner of deep learning, using a simple iterative\nalgorithm to learn the weight vector on the basis of belief propagation.",
  "text": "arXiv:1501.04413v1  [stat.ML]  19 Jan 2015\nJournal of the Physical Society of Japan\nStatistical-mechanical analysis of pre-training and ﬁne tuning in deep\nlearning\nMasayuki Ohzeki ∗\nDepartment of Systems Science, Graduate School of Informatics, Kyoto University, 36-1\nYoshida Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan\nIn this paper, we present a statistical-mechanical analysis of deep learning. We elucidate some\nof the essential components of deep learning—pre-training by unsupervised learning and ﬁne\ntuning by supervised learning. We formulate the extraction of features from the training data\nas a margin criterion in a high-dimensional feature-vector space. The self-organized classiﬁer\nis then supplied with small amounts of labelled data, as in deep learning. Although we employ\na simple single-layer perceptron model, rather than directly analyzing a multi-layer neural\nnetwork, we ﬁnd a nontrivial phase transition that is dependent on the number of unlabelled\ndata in the generalization error of the resultant classiﬁer. In this sense, we evaluate the eﬃcacy\nof the unsupervised learning component of deep learning. The analysis is performed by the\nreplica method, which is a sophisticated tool in statistical mechanics. We validate our result\nin the manner of deep learning, using a simple iterative algorithm to learn the weight vector\non the basis of belief propagation.\n1.\nIntroduction\nDeep learning is a promising technique in the ﬁeld of machine learning, with its outstand-\ning performance in pattern recognition applications, in particular, being extensively reported.\nThe aim of deep learning is to eﬃciently extract important structural information directly\nfrom the training data to produce a high-precision classiﬁer.1) The technique essentially con-\nsists of three parts. First, a large number of hidden units are introduced by constructing a\nmulti-layer neural network, known as a deep neural network (DNN). This allows the im-\nplementation of an iterative coarse-grained procedure, whereby each high-level layer of the\nneural network extracts abstract information from the input data. In other words, we introduce\nsome redundancy for feature extraction and dimensional reduction (a kind of sparse represen-\n∗mohzeki@i.kyoto-u.ac.jp\n1/13\nJ. Phys. Soc. Jpn.\ntation) of the given data. The second part is pre-training by unsupervised learning. This is a\nkind of self-organization.2) To accomplish self-organization in the DNN, we provide plenty\nof unlabelled data. The network learns the structure of the input data by tuning the weight\nvectors (often termed the network parameters) assigned to each layer of the neural network.\nThe procedure of updating each weight vector on the basis of the gradient method, i.e., back\npropagation, takes a relatively long time3) and its regularization by L1 norm and greedy algo-\nrithm.4–6) This is because many local minima are found during the optimization of the DNN.\nInstead, techniques such as the auto-encoder have been proposed to make the pre-training\nmore eﬃcient and push up the basins of attraction of the minima via a better generalization\nof the training data.7–9) The third component of deep learning involves ﬁne tuning the weight\nvectors using supervised learning to elaborate DNN into a highly precise classiﬁer. This com-\nbination of unsupervised and supervised learning enables the architecture of deep learning to\nobtain better generalization, eﬀectively improving the classiﬁcation under a semi-supervised\nlearning approach.10,11)\nIn the present study, we focus on the latter two parts of deep learning. The ﬁrst is ne-\nglected because it simply highlights a way of implementing the deep learning algorithm. A\nrecent study has formulated a theoretical basis for the relationship between the recursive ma-\nnipulation of variational renormalization groups and the multi-layer neural network in deep\nlearning.12) Indeed, it is conﬁrmed that the renormalization group indeed can mitigate the\ncomputational cost in the learning without any signiﬁcant degradation.13) Furthermore, the\ndirect evaluation of multi-layer neural networks is too complex to fully clarify the early stages\nof our theoretical understanding of deep learning. Although most of the DNN is constructed\nby a Boltzmann machine with hidden units, we simplify the DNN to a basic perceptron. This\nsimpliﬁcation, which is just for our analysis, enables us to shed light on the fundamental\norigin of the outstanding performance of deep learning and the eﬃciency of pre-training by\nunsupervised learning.\nThe steady performance of the classiﬁer constructed by the deep learning algorithm can\nbe assessed in terms of the generalization error using a statistical-mechanical analysis based\non the replica method.14) We consequently ﬁnd nontrivial behaviour involved in the emer-\ngence of the metastable state of the generalization error, a result of the combination of unsu-\npervised and supervised learning. This is analogous to the metastable state in classical spin\nmodels, which leads to the hysteresis eﬀect in magnetic ﬁelds. Following the actual process\nof deep learning, we numerically test our result by successively implementing the unsuper-\nvised learning of the pre-training procedure and the supervised learning for ﬁne tuning. We\n2/13\nJ. Phys. Soc. Jpn.\nthen demonstrate the eﬀect of being trapped in the metastable state, which worsens the gen-\neralization error. This justiﬁes the need for ﬁne tuning by several sets of labelled data after\nthe pre-training stage of deep learning.\nThe remainder of this paper is organized as follows. In the next section, we formulate our\nsimpliﬁed model to represent unsupervised and supervised learning with structured data, and\nanalyze the Bayesian inference process for the weight vectors. In Section 3, we investigate the\nnontrivial behaviour of the generalization error in our model. We demonstrate that the gen-\neralization error can be signiﬁcantly improved by the use of suﬃcient amounts of unlabelled\ndata. Finally, in Section 4, we summarize the present work.\n2.\nAnalysis of combination of unsupervised and supervised learning\n2.1\nProblem setting\nWe deal with a simple two-class labelled-unlabelled classiﬁcation problem. We assume\nthat the N-dimensional feature vectors xµ ∈RN obey the following distribution function\nconditioned on the binary label yµ = ±1 for each datum µ and a predetermined weight vector\nw0:\nPg(xµ|yµ, w0) ∝Θ\n yµ\n√\nN\nxT\nµw0 −g\n!\n,\n(1)\nwhere g is a margin, which resembles the structure of the feature vectors of the given data,\nand\nΘ(x) =\n\n1\nx > 0\n0\nx ≤0\n.\n(2)\nThe labelled data (xµ, yµ) (µ\n=\n1, 2, · · · , L) are generated from the joint probability\nPg(xµ|yµ, w0)P(yµ), where L is the number of labelled data. The unlabelled data (xµ) (µ =\nL +1, L +2, · · · , L +U), where U is the number of unlabelled data, follow the marginal prob-\nability Pg(xµ|w0) = P\nyµ Pg(xµ|yµ, w0)P(yµ). In the following, we assume the large-N limit\nand a huge number of data L, U ∼O(N), as well as a symmetric distribution for the label\nP(yµ) = 1/2.\nThe likelihood function for the dataset is deﬁned as\nPg(D|w0) =\nL\nY\nµ=1\nPg(xµ|yµ, w0)P(yµ)\nL+U\nY\nµ=L+1\nPg(xµ|w0),\n(3)\nwhere D denotes the dataset consisting of labelled data and unlabelled data. When the feature\nvector g has a margin value of zero, unsupervised learning is no longer meaningful, because\nthe marginal distribution becomes ﬂat. However, nonzero values of the margin elucidate the\n3/13\nJ. Phys. Soc. Jpn.\nstructure of the feature vectors through the unsupervised learning. The actual data in im-\nages and sounds have many inherent structures that must be represented by high-dimensional\nweight vectors in the multi-layer neural networks of DNN. In the present study, we simplify\nthis aspect of the actual data to give an artiﬁcial model with a margin that follows the simple\nperceptron. This allows us to assess certain nontrivial aspects of deep learning.\n2.2\nBayesian inference and replica method\nFor readers unfamiliar with deep learning, we sketch the procedure of the deep learning\nhere. The ﬁrst step of the deep learning algorithm is to conduct pre-training. Following the\nunsupervised learning, the weight vector learns the features of the training data without any\nlabels. As a simple strategy, we often estimate the weight vector to maximize the likelihood\nfunction only for the unlabelled data as\nwPT = arg max\nw\nlog\nL+U\nY\nµ=L+1\nPh(xµ|w)\n.\n(4)\nWe use a diﬀerent margin value h from one in Eq. (3) in order to evaluate a generic case\nbelow. When we know a priori the structure of the data, one may set g = h. We may utilize\nthe hidden units to prepare some redundancy to represent the feature of the given data. In the\npresent study, we omit this aspect to simplify the following analysis. In other words, we have\na coarse-graining picture of DNN only by a single layer with a weight vector w, the input xµ\nand output yµ. In the second step, termed as the ﬁne tuning step, we estimate the weight vector\nto precisely classify the training data. For instance, the maximum likelihood estimation can\nbe a candidate to estimate the weight vector as\nwFT = arg max\nw\nlog\nL\nY\nµ=1\nPh(xµ|yµ, w)P(yµ)\nL+U\nY\nµ=L+1\nPh(xµ|w)\n.\n(5)\nWe notice an important thing of the deep learning architecture. In this procedure, we use the\nresult of the pre-training wPT as an initial condition for the gradient method to obtain wFT. The\npurpose of the deep learning is just obtain the weight vector to classify the newly-generated\ndata with better performance simply from some strategy as in Eq. (5). The computational\ncost of the often-employed methods (e.g. back propagation3)) becomes extremely longer in\ngeneral. However if we have some adequate initial condition to manipulate the estimation,\nwe can mitigate harmful computation and reach a better estimation of the weight vector.8,9)\nIn order to evaluate the theoretical limitation of the deep learning, instead of the maximum\nlikelihood estimation, we employ an optimal procedure based on the framework of Bayesian\n4/13\nJ. Phys. Soc. Jpn.\ninference. The posterior distribution can be given by the Bayes’ formula as\nPh(w|D) =\nPh(D|w)P(w)\nR\ndw′Ph(D|w′)P(w′)\n.\n(6)\nWe assume that the prior distribution for the weight vector is P(w) ∝δ\n\u0010\n|w|2 −N\n\u0011\n. The poste-\nrior mean given by this posterior distribution provides an estimator for the quantity related to\nthe weight vector:\nEw|D[f (w)] =\nZ\ndw f (w)\nPh(D|w)P(w)\nR\ndw′Ph(D|w′)P(w′)\n.\n(7)\nThe typical value is evaluated by averaging over the randomness of the dataset as\nED[Ew|D[g(w)]] =\n\nZ\ndwg(w)\nPh(D|w)P(w)\nR\ndw′Ph(D|w′)P(w′)\n\nD\n,\n(8)\nwhere\n[· · · ]D =\nZ\ndDdw0Pg(D|w0)P(w0) × · · · .\n(9)\nThe average quantity is given by the derivative of the characteristic function, namely the free\nenergy, which is deﬁned as\n−F = lim\nN→∞\n1\nN\n\"\nlog\nZ\ndwPh(D|w)P(w)\n#\nD\n.\n(10)\nIn particular, as shown below, the derivative of the free energy yields a kind of self-consistent\nequations for the physically-relevant quantities. In this problem, we compute the overlap\nbetween the estimated w and the original weight vectors w0 and the variance of the weight\nvectors, which quantify the precision of the learning. Following spin glass theory,14) we apply\nthe replica method to evaluate the free energy. We deﬁne the replicated partition function as\nΞn =\n Z\ndwPh(D|w)P(w)\n!n\n.\n(11)\nThe (density of) free energy can be calculated from the replicated partition function through\nthe replica method as\n−F\n=\nlim\nn→0\n∂\n∂n lim\nN→∞\n1\nN log [Ξn]D .\n(12)\nWe exchange the order of the operations on n and the thermodynamic limit N →∞, and\nassume that the replica number n is temporarily a natural number in the evaluation of [Ξn]D.\nWe introduce the following constraints to simplify the calculation dependent on wa:\nZ\ndQ\nY\na≥b\nδ\n \nQab −1\nN wawb\n! Y\na=0\nδ\n \nQ0a −1\nN w0wa\n!\n.\n(13)\n5/13\nJ. Phys. Soc. Jpn.\nThe free energy is then given by solving an extremization problem:\n−F = sup\nQ\n[G(Q) −I(Q)] ,\n(14)\nwhere\nG(Q)\n=\nα log\nΘ (u0 −g)\nn\nY\na=1\nΘ (uα −h)\n\nu\n+ β log\nΦ(u0, g)\nn\nY\na=1\nΦ(ua, h)\n\nu\n(15)\nI(Q)\n=\nsup\n˜Q\n\nX\na≥b\nQab ˜Qab +\nn\nX\na=1\nQ0a ˜Q0a −log M( ˜Q)\n\n(16)\nM( ˜Q)\n=\nEw\nexp\n\nX\na≥b\n˜Qabwawb +\nn\nX\na=1\n˜Q0aw0wa\n\n.\n(17)\nHere, α = L/N, β = U/N, and\nΦ(u, h) = 1\n2Θ (u −h) + 1\n2Θ (−u −h) .\n(18)\nThe expectation is taken over the distribution Qn\na=0 P(wa). We introduce auxiliary parameters\n˜Qab to give an integral representation of the Kronecker’s delta. We use [· · · ]u to denote the\naverage with respect to the (n+1)-multivariate Gaussian random variables {ua} with vanishing\nmean and covariance [uaub]u = δab + Qab(1 −δab).\n2.3\nReplica-symmetric solution\nLet us evaluate the replica-symmetric solution by imposing invariant symmetry for Qab\nand ˜Qab under permutation of the replica index as\nQaa = 1\nQab = q\nQ0a = m\n˜Qaa = ˜Q\n˜Qab = ˜q\n˜Q0a = ˜m.\n(19)\nThen, the Gaussian random variables can be written as ua = √qz +\np\n1 −qta for a > 0 and\nu0 =\np\nm2/qz+\np\n1 −m2/qt0 using the auxiliary normal Gaussian random variables {ta} and z\nwith vanishing mean and unit variance. Under the RS assumption, we obtain an explicit form\nfor the free energy by solving the saddle-point equation for ˜Q, ˜q, and ˜m:\n−F\n=\nα\nZ\nDzH\n\nmz + √qg\np\nq −m2\nlog H\n\n√qz + h\np\n1 −q\n\n+β\nZ\nDzGg(m, √q) logGh( √q, 1) + 1\n2 log(1 −q) + q −m2\n2(1 −q),\n(20)\nwhere Dz = dz exp(−z2/2), H(x) =\nR ∞\nx Dt, and\nGh(a, b)\n=\n1\n2\n(\nH\n az + bh\n√\nb2 −a2\n!\n+ H\n az −bh\n√\nb2 −a2\n!)\n.\n(21)\n6/13\nJ. Phys. Soc. Jpn.\nThe partial derivatives of the free energy (20) with respect to m and q lead to the saddle-\npoint equations for the physically-relevant RS order parameters, namely the overlap m and\nthe variance w of the weight vector:\nα\nZ\nDzH′\n\nmz + √qg\np\nq −m2\n\n\nH′\n √qz+h\n√\n1−q\n!\nH\n √qz+h\n√\n1−q\n!\n\n+β\nZ\nDzG′\ng(m, √q)\n G′\nh( √q, 1)\nGh( √q, 1)\n!\n=\nm\n1 −q,\n(22)\nα\nZ\nDzH\n\nmz + √qg\np\nq −m2\n\n\nH′\n √qz+h\n√\n1−q\n!\nH\n √qz+h\n√\n1−q\n!\n\n2\n+β\nZ\nDzGg(m, √q)\n G′\nh( √q, 1)\nGh( √q, 1)\n!2\n= q −m2\n(1 −q)2,\n(23)\nwhere H′(x) = −exp(−z2/2)/\n√\n2π and\nG′\nh(a, b)\n=\n1\n2\n(\nH′\n az + bh\n√\nb2 −a2\n!\n−H′\n az −bh\n√\nb2 −a2\n!)\n.\n(24)\nThe RS solution always satisﬁes q = m under the condition g = h (the Bayes-optimal solu-\ntion). The above saddle-point equations are then reduced to the following single equation for\nq:\nα\nZ\nDz\n \nH′\n √qz+h\n√\n1−q\n!!2\nH\n √qz+h\n√\n1−q\n!\n+ β\nZ\nDz\n\u0010\nG′\nh( √q, 1)\n\u00112\nGh( √q, 1)\n=\nq\n1 −q.\n(25)\nThe order parameter q is closely related to the generalization error, which is deﬁned as the\nprobability of disagreement between the labelled data and the classiﬁer outputs for the newly\ngenerated example after the classiﬁer has been trained. In the case of an input–output relation\ngiven by a simple perceptron, the generalization error is expressed as:14)\nǫ = 1\nπ cos−1 q.\n(26)\nWe will evaluate this quantity to validate the performance of the classiﬁer generated from the\ncombination of unsupervised and supervised learning.\n7/13\nJ. Phys. Soc. Jpn.\n0 100 200 300 400 500 600 700 800 900 1000\nβ\nα=10\n0\n100 200 300 400 500 600 700 800 900 1000\n−8\n−7\n−6\n−5\n−4\n−3\n−2\n−1\nlogε\nβ\n−8\n−7\n−6\n−5\n−4\n−3\n−2\n−1\nlogε\nα＝10\nα＝1\nFig. 1.\n(color online) Generalization errors for h = 0.1, 0.05, 0.03, 0.02, and 0.01 (curves from left to right).\nThe left panel shows the results for α = 1, and the right one represents α = 10. Both cases exhibit multiple\nsolutions for the same value of β.\n3.\nSaddle point and numerical veriﬁcation\nIn Fig. 1, we plot the logarithm of the generalization error with respect to the number of\nsupervised learning data for several values of h. Each plot shows the results for a diﬀerent\nvalue of α. Note that when there is no ﬁne tuning through supervised learning (i.e., α = 0),\nthe generalization error does not exhibit any nontrivial behaviour. However, for nonzero α,\nwe ﬁnd nontrivial curves, which give multiple solutions for the same β, in the β −ǫ plane.\nThis is a remarkable result for the combination of unsupervised and supervised learning. The\nnontrivial curves imply the existence of a metastable state, similar to several classical spin\nmodels.15) As h decreases, the spinodal point βsp (the point at which the multiple solutions\ncoalesce) moves to larger values of β. This is because decreasing h leads to diﬃculties in the\nclassiﬁcation of the input data. In other words, we need a vast number of unlabelled data to\nattain the lower-error state for a ﬁxed number of labelled data. However, the metastable state\nremains up to a large value of β, causing the computational cost to become very expensive.\nWe therefore need an extremely long computational time to reach the lower-error solution, or\nﬁnd good initial conditions nearby. On the other hand, increasing α causes the spinodal points\nto move to lower values of β. Although this conﬁrms an improvement in the generalization\nerror for the higher-error state, there is no quantitative change in that for the lower-error state.\nIn this sense, pre-training is an essential part of the architecture of deep learning if we wish to\nachieve the lower-error state—this is the origin of deep learning’s remarkable performance.\nIn contrast, the emergence of the metastable state causes the computational cost to increase\n8/13\nJ. Phys. Soc. Jpn.\ndrastically. Several special techniques could be incorporated into the architecture of deep\nlearning to avoid this weak point, eﬀectively preparing good initial conditions that enable the\nlower-error state to be reached.8,9)\nThe asymptotic form of H(x) ∼Θ(x) exp(−x2/2)/|x| for x →∞leads to the exponent of\nthe learner curve,14) which characterizes the decrease in the generalized error in α ≫1 and\nβ ≫1 as ǫg ∼(cαα2 + cαβ + cββ2)−1. Here, cα, cβ, and c are the constants evaluated by the\nGaussian integrals. Thus, there is no quantitative change in the exponent of the learning curve\nin this formulation compared with that of the perceptron with ordinary supervised learning.\nNext, let us consider the eﬀect of ﬁne tuning in the context of deep learning. If we plot\nthe saddle-point solutions in the α −ǫ plane, we ﬁnd that multiple solutions appear in a\ncertain region. Increasing the number of unlabelled data again leads to an improvement in the\ngeneralization error. A gradual increase in the number of labelled data allows us to escape\nfrom the metastable state. In this sense, ﬁne tuning by supervised learning is necessary to\nachieve the lower-error state and mitigate the diﬃculties in reaching the desired solution. We\nshould emphasize that the emergence of the metastable state does not come from the multi-\nlayer neural networks in DNN, but from the combination of unsupervised and supervised\nlearning. This observation was also noted in a previous study.16)\nTo verify our analysis, we conduct numerical experiments using the so-called approxi-\nmate message passing algorithm.17) On the basis of the reference in the modern fashion,18)\nwe can construct an iterative algorithm to infer the weight vector using both the unlabelled\nand labelled data. The update equations are\nat+1\nµ\n=\nN\nX\nk=1\nxµkwk −1\nκtCµ\n \nat\nµ, 1\nκt , h\n!\n(27)\nκt+1\n=\n1\n2\n1 + 4\nN\nN\nX\nk=1\n\u0000at\nk\n\u00012\n\n(28)\nat+1\nk\n=\nL+U\nX\nµ=1\nxµkCµ\n \nat\nµ, 1\nκt , h\n!\n+ Btat\nk\n(29)\nBt+1\n=\n1\nN\nL+U\nX\nµ=1\nDµ\n \nat\nµ, 1\nκt , h\n!\n,\n(30)\nwhere\nCµ(a, b, h)\n=\n\nyµ\nexp(−z2\n−/2)\n√\n2πbH(z−)\n(µ ≤L)\nexp(−z2\n−/2) −exp(−z2\n+/2)\n√\n2πb(H(z−) + H(z+))\n(µ > L)\n(31)\n9/13\nJ. Phys. Soc. Jpn.\nDµ(a, b, h)\n=\n\nC2\nµ(a, b, h) −yµ\nz−Cµ(a, b, h)\n√\nb\n(µ ≤L)\nC2\nµ(a, b, h) + z−exp(−z2\n−/2) + z+ exp(−z2\n+/2)\n√\n2πb(H(z−) + H(z+))\n(µ > L).\n(32)\nHere xµk is the kth component of the feature vector of the datum µ and wk is the kth component\nof the weight vector. We use the abbreviation z± = (h±a)/\n√\nb, and estimate the weight vector\nfrom w = at/κt. In the numerical experiments, we ﬁrst estimate the weight vector using only\nthe unlabelled data, i.e., α = 0. We then gradually increase the number of labelled data while\nestimating the weight vector. The system size is set to N = 100, and the number of samples\nNsam = 1000. The maximum iteration number for ﬁne tuning is set to 20. In Fig. 2, we plot\nthe average generalization error over Nsam independent runs starting from the randomized ini-\ntial conditions. As theoretically predicted, our results conﬁrm the water-falling phenomena\nfor several cases with h = 0.5. Increasing the number of labelled data in the ﬁne tuning step\nallows us to escape from the metastable state. Therefore, ﬁne tuning is a necessary compo-\nnent in the remarkable performance of deep learning. However, the diﬃculty of classiﬁcation,\nrepresented by h, demands a large number of training data. Therefore, we require the initial\ncondition to be as good as possible in the ﬁne tuning to reach the lower-error state. Several\nempirical studies of the deep learning algorithm have revealed that special techniques such as\nthe auto-encoder can provide initial conditions that are suﬃciently good to improve the per-\nformance after ﬁne tuning.9) In future work, we intend to clarify that such speciﬁc techniques\ndo indeed overcome the degradation in performance caused by the metastable state.\n4.\nConclusion\nWe have analyzed the simpliﬁed perceptron model under a combination of unsupervised\nand supervised learning for data with a margin. The margin imitates the structure of the\ntraining data. We have found nontrivial behaviour in the generalization error of the classiﬁer\nobtained by this hybrid of unsupervised and supervised learning. First, we conﬁrmed the\nremarkable improvement in the generalization error by increasing the number of unlabelled\ndata. In this sense, the pre-training step in deep learning is essential when few labelled data\nare available. In addition, our result reveals the existence of the metastable solution, which\nhampers the ordinary gradient-based iteration to pursue the optimal estimation. In the deep\nlearning algorithm, the pre-training technique is crucial in reducing the computation time\nand attaining good performance, because good initial conditions allow the algorithm to reach\nthe lower-error state. Instead of focusing on the specialized pre-training technique, we have\ninvestigated a nontrivial behaviour involved in the metastable state and the existence of the\n10/13\nJ. Phys. Soc. Jpn.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n−7\n−6\n−5\n−4\n−3\n−2\n−1\n0\nlogε\nα\nβ＝200\nβ＝100\nFig. 2.\n(Color online) Numerical test using approximate message passing. We illustrate the case with h = 0.05\nfor β = 100 (blue) and β = 200 (red). Error bars are shown for each plot over Nsam = 1000 samples.\nlower-error state, which is used in the deep learning. In addition, we have analyzed the role\nof ﬁne tuning by changing the number of labelled data. This also conﬁrmed the nontrivial\nbehaviour in the generalization error. Our numerical experiments demonstrated the water-\nfalling phenomena involved in the existence of the metastable state and conﬁrms that after\nﬁne tuning we reach the lower-error state.\nWe make a remark on the statistical-mechanical analysis for a similar problem setting,\nnamely that of semi-supervised learning. A previous analysis also revealed the existence of\nthe metastable state.16) The present study suggests that the metastable state is essential in the\ncombination of unsupervised and supervised learning. In this sense, for the sake of the further\ndevelopment to eﬃciently perform the deep learning, we should invent some techniques to\nescape from the metastable state,\nOur present work is one instance in which a simpliﬁed model can demonstrate the essence\nof deep learning and clarify certain theoretical aspects. We hope that future studies will “ex-\ntract the features” of the architecture of deep learning.\nAcknowledgments\nThe author would like to thank Muneki Yasuda, Jun-ichi Inoue, and Tomoyuki Obuchi\nfor fruitful discussions. The present work has been performed with ﬁnancial support from\nthe JST-CREST, MEXT KAKENHI Grant Nos. 25120008 and 24740263, and the Kayamori\n11/13\nJ. Phys. Soc. Jpn.\nFoundation of Informational Science Advancement.\n12/13\nJ. Phys. Soc. Jpn.\nReferences\n1) G. E. Hinton, S. Osindero, and Y.-W. Teh: Neural Comput. 18 (2006) 1527.\n2) T. Kohonen: Self-Organizing Maps (Physics and astronomy online library. Springer\nBerlin Heidelberg, 2001), Physics and astronomy online library.\n3) C. M. Bishop: Pattern recognition and machine learning (springer New York, 2006),\nVol. 1.\n4) J. Sohl-Dickstein, P. B. Battaglino, and M. R. DeWeese: Phys. Rev. Lett. 107 (2011)\n220601.\n5) A. Decelle and F. Ricci-Tersenghi: Phys. Rev. Lett. 112 (2014) 070603.\n6) S. Yamanaka, M. Ohzeki, and A. Decelle: Journal of the Physical Society of Japan 84\n(2015) 024801.\n7) G. E. Hinton and R. R. Salakhutdinov: Science 313 (2006) 504.\n8) Y. Bengio: Found. Trends Mach. Learn. 2 (2009) 1.\n9) D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio: Journal\nof Machine Learning Research 11 (2010) 625.\n10) M. Seeger: Learning with Labeled and Unlabeled Data (2000).\n11) X. Zhu: Semi-supervised learning literature survey (2008).\n12) P. Mehta and D. J. Schwab: arXiv e-prints:1410.3831 (2014).\n13) K. Tanaka, S. Kataoka, M. Yasuda, and M. Ohzeki: arXiv e-prints:1501.00834 (2015).\n14) H. Nishimori: Statistical physics of spin glasses and information processing: an intro-\nduction (Oxford University Press, 2001).\n15) H. Nishimori and G. Ortiz: Elements of Phase Transitions and Critical Phenomena (Ox-\nford Graduate Texts) (Oxford University Press, USA, 2011).\n16) T. Tanaka: Journal of Physics: Conference Series 473 (2013) 012001.\n17) Y. Kabashima: Journal of Physics A: Mathematical and General 36 (2003) 11111.\n18) Y. Xu, Y. Kabashima, and L. Zdeborov: Journal of Statistical Mechanics: Theory and\nExperiment 2014 (2014) P11015.\n13/13\n",
  "categories": [
    "stat.ML",
    "cond-mat.dis-nn",
    "cond-mat.stat-mech",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2015-01-19",
  "updated": "2015-01-19"
}