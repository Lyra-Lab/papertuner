{
  "id": "http://arxiv.org/abs/1501.06237v1",
  "title": "Deep Transductive Semi-supervised Maximum Margin Clustering",
  "authors": [
    "Gang Chen"
  ],
  "abstract": "Semi-supervised clustering is an very important topic in machine learning and\ncomputer vision. The key challenge of this problem is how to learn a metric,\nsuch that the instances sharing the same label are more likely close to each\nother on the embedded space. However, little attention has been paid to learn\nbetter representations when the data lie on non-linear manifold. Fortunately,\ndeep learning has led to great success on feature learning recently. Inspired\nby the advances of deep learning, we propose a deep transductive\nsemi-supervised maximum margin clustering approach. More specifically, given\npairwise constraints, we exploit both labeled and unlabeled data to learn a\nnon-linear mapping under maximum margin framework for clustering analysis.\nThus, our model unifies transductive learning, feature learning and maximum\nmargin techniques in the semi-supervised clustering framework. We pretrain the\ndeep network structure with restricted Boltzmann machines (RBMs) layer by layer\ngreedily, and optimize our objective function with gradient descent. By\nchecking the most violated constraints, our approach updates the model\nparameters through error backpropagation, in which deep features are learned\nautomatically. The experimental results shows that our model is significantly\nbetter than the state of the art on semi-supervised clustering.",
  "text": "Deep Transductive Semi-supervised Maximum Margin\nClustering\nGang Chen\nAugust 27, 2018\nAbstract\nSemi-supervised clustering is an very important topic in machine learning and computer vision. The\nkey challenge of this problem is how to learn a metric, such that the instances sharing the same label are\nmore likely close to each other on the embedded space. However, little attention has been paid to learn\nbetter representations when the data lie on non-linear manifold. Fortunately, deep learning has led to\ngreat success on feature learning recently. Inspired by the advances of deep learning, we propose a deep\ntransductive semi-supervised maximum margin clustering approach.\nMore speciﬁcally, given pairwise\nconstraints, we exploit both labeled and unlabeled data to learn a non-linear mapping under maximum\nmargin framework for clustering analysis. Thus, our model uniﬁes transductive learning, feature learning\nand maximum margin techniques in the semi-supervised clustering framework. We pretrain the deep\nnetwork structure with restricted Boltzmann machines (RBMs) layer by layer greedily, and optimize our\nobjective function with gradient descent. By checking the most violated constraints, our approach updates\nthe model parameters through error backpropagation, in which deep features are learned automatically.\nThe experimental results shows that our model is signiﬁcantly better than the state of the art on semi-\nsupervised clustering.\n1\nIntroduction\nIn this paper, we investigate the semi-supervised clustering with side information in the form of pairwise\nconstraints. In general, a pairwise constraint between two examples indicates whether they belong to the same\ncluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes\nthat the pair of instances should be partitioned into the same cluster, while a diﬀerent-label (or cannot-link)\nconstraint speciﬁes that the pair of instances should be assigned into diﬀerent clusters [32, 23, 30].\nSemi-supervised learning with pairwise constraints, has received considerable attention recently, especially for\nclassiﬁcation and clustering [32, 3, 11, 23, 6, 30, 34]. On the one hand, it is relatively easy to decide whether\ntwo items are similar or not from human in the loop because it often involves little eﬀort from users. On\nthe other hand, the maximum margin techniques have shown promising performance on classiﬁcation tasks,\nand thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34]. In general, traditional\nsemi-supervised clustering approaches either learn a distance metric based on the pairwise constraints, or\nleverage discriminative methods, such as k-nearest neighbor (kNN) and support vector machines (SVM) for\nbetter clustering performance. However, to collapse examples that belong to the same cluster approximately\ninto a single point cannot always be achieved with simple linear transformations, especially when the data\nlie on an non-linear manifold. Although kernel methods are widely used for non-linear cases, it is a shallow\napproach and needs to specify hyper parameters in most situations [27, 30]. Fortunately, recent advances in\n1\narXiv:1501.06237v1  [cs.LG]  26 Jan 2015\nthe training of deep networks provide a way to learn non-linear transformations of data, which are useful for\nsupervised/unsupervised tasks [8, 2].\nInspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach,\nwhich inherits both advantages from deep learning and maximum margin methods. Our method can learn\nfeatures automatically from observation, kind of learning a metric as in [31]. However, unlike the linear\nmapping, e.g. Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which\nis helpful for clustering and classiﬁcation [2]. With the learned features as the input to the semi-supervised\nmaximum margin clustering framework, we can learn the clustering weights. To leverage the unlabeled data,\nwe also incorporate transductive learning to improve the clustering analysis.\nThrough backpropagation,\nour approach can learn discriminative features via maximum margin techniques. Hence, our model uniﬁes\nmaximum margin, semi-supervised information and deep learning in an joint framework. We pre-train our\nmodel with stacked RBMs for feature representations ﬁrstly.\nAnd then we compute the gradient w.r.t.\nparameters and optimize our objective function in an alternative manner: data representation and model\nweights optimization with gradient descent. We test our model over a bunch of data sets and show that it\nyields accuracy signiﬁcantly better than the state of the art.\nThe outline of this paper is as follows. In Section 2, we review the related work. Then, we present the model\nin Section 3. Section 4 present results of our experiments with the new techniques on a few widely used data\nsets. Finally we conclude the paper.\n2\nRelated work\nThe semi-supervised clustering with partial labels generally explores two directions to improve performance:\n(1) leverage more sophisticated classiﬁcation models, such as maximum margin techniques [25, 30]; (2) learn\na better distance metric [23, 30].\nThe maximum margin clustering (MMC) aims to ﬁnd the hyperplanes that can partition the data into\ndiﬀerent clusters over all possible labels with large margins [33, 26, 35]. Nevertheless, the accuracy of the\nclustering results by MMC may not be good sometimes due to the nature of its unsupervised learning [14].\nThus, it is interested to incorporate semi-supervised information, e.g.\nthe pairwise constraints, into the\nrecently proposed maximum margin clustering framework. Recent research demonstrates the advantages by\nleveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3]. In particular,\nCOPKmeans [11] is a semi-supervised variant of Kmeans, by following the same clustering procedure of\nKmeans while avoiding violations of pairwise constraints. MPCKmeans [3] extended Kmeans and utilized\nboth metric learning and pairwise constraints in the clustering process. More recently, [20] show that they\ncan improve classiﬁcation with pairwise constraints under maximum margin framework. [34] leverage the\nmargin-based approach on the semi-supervised clustering problems, and yield competitive results.\nHow to learn a good metric over input space is critical for a successful semi-supervised clustering approach.\nHence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reﬂect the\nunderlying relationships between the input instance pairs. The pseudo-metric [23] parameterized by positive\nsemi-deﬁnite matrices (PSD) is learned with an online updating rule, that alternates between projections onto\nPSD and onto half-space constraints imposed by the instance pairs. [32] proposed to learn a distance metric\n(Mahalanobis) that respects pairwise constraints for clustering. In [6], an information-theoretic approach to\nlearning a Mahalanobis distance function via LogDet divergence is proposed. Recently, a supervised approach\nto learn Mahalanobis metric is also proposed in [30], by minimizing the pairwise distances between instances\nin the same cluster, while increasing the separation between data points with dissimilar classes. To handle\nthe data that lies on non-linear manifolds, kernel methods are widely used. Unfortunately, these non-linear\nembedding algorithms for use is shallow methods.\nOn the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension\n2\nreduction [13, 31] and classiﬁcation problems [12, 19].\nIn a sense, the success of deep learning lies on\nlearned features, which are useful for supervised/unsupervised tasks [8, 2]. For example, the binary hidden\nunits in the discriminative Restricted Boltzmann Machines (RBMs) [18, 9] can model latent features of\nthe data that improve classiﬁcation. The deep learning for semi-supervised embedding [31] extends shallow\nsemi-supervised learning techniques such as kernel methods with deep neural networks, and yield promising\nresults. The work of [24] is most related to our proposed algorithm. It presented deep learning with support\nvector machines, which can learn features under discriminative learning framework automatically with labeled\ndata. However, their approach is totally supervised and for classiﬁcation problems, while our model is for\nsemi-supervised clustering problems. Compared to conventional methods, our model consider both feature\nlearning and transductive principles in our semi-supervised clustering model, so that it can handles complex\ndata distribution and learns a better non-linear mapping to improve clustering performance.\n3\nDeep Transductive Semi-supervised Maximum Margin Cluster-\ning\nIn this section, we will introduce the transductive semi-supervised maximum margin clustering, with deep\nfeatures learned simultaneously in an uniﬁed framework.\n3.1\nOverview of our approach\nLet X = {xi}N\ni=1 (xi ∈RD) be a set of N examples, which belongs to K clusters called Z. In addition\nto the unlabeled data, there is additional partially labeled data in the form of pairwise constraints C =\n{(xi, xj, δ(zi = zj)}, which is a kind of side information to provide whether the two instances (xi, xj) are\nfrom the same cluster or not (indicated by the δ function). Most methods attempt to learn weights wk ∈RD,\nfor each cluster k = [1, K], to make these constraints satisﬁed as much as possible.\nInstead of learning a linear mapping or Mahalanobis metric [23, 30], we are interested in a non-linear mapping\nfunction. To make it easy to understand, suppose we have learned a nonlinear mapping function f : RD →Rd.\nThen, for each instance x ∈X, we can get its embedding code h = f(x) (note that the pairwise constrains\nalso are kept in the coding space). Then given the learned features h, we leverage semi-supervised maximum\nmargin clustering to partition the data. Just like the multi-class classiﬁcation problems [25], we use the joint\nfeature representation Φ(h, z) for each (h, z) ∈X × Z\nΦ(h, z) =\n\n\nh · δ(z = 1)\n· · ·\nh · δ(z = K)\n\n\n(1)\nwhere δ is the indicator function (1 if the equation holds, otherwise 0). Correspondently, the hyperplanes\nfor the K clusters can be parameterized by the weight vector W ∈R(K×d)×1, which is the concatenation of\nweights wk, for k = {1, ..., K}. In other words, W[(k −1) × d + 1 : k × d] = wk. The clustering of testing\nexamples is done in the same manner as the multiclass SVM [25],\nmax\nz∈[1,K] WT Φ(f(x), z)\n(2)\nFor inference, we ﬁrst project data into hidden space with function f and then do clustering analysis. The\nproblem left is how to learn the weight parameter W and the projection function f.\n3\n3.2\nObjective function\nWe would like to extend semi-supervised clustering with deep feature learning. Deep learning consists of\nlearning a model with several layers of non-linear mapping. As mentioned before, h ∈Rd is the mapping\ncode with function f, which is non-linear mappings deﬁned with L-layers neural network, s.t.\nhi = f(xi) = fL ◦fL−1 ◦· · · ◦f1\n|\n{z\n}\nL times\n(xi)\n(3)\nwhere ◦indicates the function composition, and fl is logistic function with the weight parameter θl respec-\ntively for each layer l = {1, .., L}, refer further to Sec. 3.3 for more details. With a little abuse of symbols,\nfor any input x, If we denote the output of the l-th layer as f1→l(x), then we can get h = f1→L(x).\nIn a similar manner as in [20, 34], we will incorporate the pairwise constraint information into the margin-\nbased clustering framework. In addition, we leverage the unlabeled data to separate clusters in large margins,\nby following transductive learning. Speciﬁcally, given the pairwise constraint set C = {(xi, xj, δ(zi = zj))},\nwe ﬁrst project the dataset X into embedded space and minimize the following transductive semi-supervised\nobjective function\nmin\nW,Θ\nλ\n2 ||W||2 + 1\nn+\nX\ni\nη+\ni + 1\nn−\nX\nj\nη−\nj +\nβ\nUK\nX\ni∈U\nξi\n(4)\ns.t.\n∀si1, si2 ∈Z, si1 ̸= si2; if (hi1, hi2, δ(zi1, zi2)) ∈C+\nmax\nzi1=zi2 WT Φ(hi1, hi2, zi1, zi2)−\nWT Φ(hi1, hi2, si1, si2) ≥1 −ηi, ηi ≥0\n(5)\n∀sj1, sj2 ∈Z, sj1 = sj2; if (hj1, hj2, δ(zj1, zj2)) ∈C−\nmax\nzj1̸=zj2 WT Φ(hj1, hj2, zj1, zj2)−\nWT Φ(hj1, hj2, sj1, sj2) ≥1 −ηj, ηj ≥0\n(6)\n∀i ∈U, ∀si ̸= zi ∈Z\nmax\nzi WT Φ(hi, zi) −WT Φ(hi, si) ≥1 −ξi\n(7)\nwhere W is the clustering weight in the over the learned feature space, Θ = {θl}L\nl=1 are the weights for each\nlayer in the deep architecture, and hi is the mapping code from xi via Eq. 3; C+ = {(hi, hj, δ(zi = zj))|zi =\nzj} are the same label pairs, with the total number of pairwise constraints n+ = |C+|, C−= {(hi, hj, δ(zi =\nzj))|zi ̸= zj} are diﬀerent-label pairs, with n−= |C−|. U is the number of the unlabeled data (instances),\nnot belong to any pairwise constrains. For convenience, we deﬁne Φ(hi, hj, zi, zj) = Φ(hi, zi) + Φ(hj, zj),\nwhich means the mapping of a pairwise constraint as the sum of the individual example-label mappings. The\nmulti-layers non-linear mapping function f projects xi into hi, for i ∈[1, N]. Instead of a linear mapping,\nthe advantage of using a deep network to parametrize the function f is that a multi-layer network is better at\nlearning a non-linear function that is presumably required to collapse classes in the latent space, in particular\nwhen the data consists of very complex non-linear structures.\nEqs. 5 and 6 specify the conditions that need to be satisﬁed, which means that the score for the most possible\nassigning scheme satisfying the constraints should be greater than that for any other assigning scheme with\nlarge margins. More speciﬁcally, for any pair (hi, hj, 1) ∈C+, it requires that the largest score for assigning\n(hi, hj) into the same cluster should be greater than that for assigning the pair into diﬀerent clusters by at\nleast 1 (soft margin can be applied here too). Analogously, for any dissimilar pair (hi, hj, 0) ∈C−, the score\nthat they are assigned into the most two diﬀerent clusters should be greater than that for partitioning them\ninto the same cluster.\n4\nEq. 7 is from the principles of transductive learning, which indicates that the score of the most assigned\ncluster label is greater at least 1 than that of the runner up from the rest clusters.\nThe constrained optimization problem in Eq. 4 is hard to solve because the ﬁrst inequality Eq. 5 and the\nsecond inequality Eq. 6 impose all the possible combinations of two clusters for each pairwise constraint.\nThus, we transform it into the following equivalent unconstrained function which it is generally easier to\nsolve\nmin λ\n2 ||W||2\n+ 1\nn+\n\u001a\n1 −\n\u0014\nmax\nzi1=zi1\n(hi1,hi2,1)∈C+\nWT Φ(hi1, hi2, zi1, zi2)−\nmax\nsi1̸=si1 WT Φ(hi1, hi2, si1, si2)\n\u0015\u001b\n+\n(8a)\n+ 1\nn−\n\u001a\n1 −\n\u0014\nmax\nzj1̸=zj2\n(hj1,hj2,0)∈C−\nWT Φ(hj1, hj2, zj1, zj2)−\nmax\nsj1=sj2 WT Φ(hj1, hj2, sj1, sj2)\n\u0015\u001b\n+\n(8b)\n+\nβ\nUK\nX\ni∈U\n\b\n1 −[max\nzi WT Φ(hi, zi) −max\nsi̸=zi WT Φ(hi, si)]\n\t\n+\n(8c)\nwhere {x}+ = max(x, 0) and hi is the projected code of xi using Eq. 3. The formula 8a speciﬁes the condition\nthat need to be satisﬁed for the same label pairwise constrains, while formula 8b denotes the conditions for\ndiﬀerent-label pairs. The last equation is corresponding to transductive constraints in Eq. 4.\nIn the objective function, we need to estimate the parameters, the weight W, as well as the weights θl for\neach layer l ∈[1, L] in the deep network. From the objective function, we can compute the gradients w.r.t. W\nand θl for l ∈[1, L] (via backpropagation) respectively, and gradient-based methods can be used to optimize\nit. Note that h (we ignore the subscript for convenience) in the objective function Eq. 8 is the non-linear\nembedding code from x. Thus, the objective function is not convex anymore, and we can only ﬁnd a local\nminimum. In practice, we ﬁnd L-BFGS cannot work well and easily trap into a bad local minimum. In our\nwork, we use (sub)gradient descent to optimize the objective function, by projecting the training data with\nf and optimizing the objective function in an alternative manner.\n3.3\nParameter learning\nWe learn the parameters in an alternative manner: (1) data projection, given the model parameters; (2) and\nthen optimize model parameters with gradient descent. To compute the gradients of the parameters, we need\nto ﬁnd the most violated constraints ﬁrst. For the same label pairs, we have the following most violated set:\nA+ =\n\u001a\n(hi, hj, δ(zi1 = zi2)) ∈C+| max\nzi1=zi1 WT Φ(hi1, hi2, zi1, zi2) −max\nsi1̸=si1 WT Φ(hi1, hi2, si1, si2) < 1\n\u001b\n(9)\nFor the diﬀerent-label pairs, we denote the most violated set as\nA−=\n\u001a\n(hi, hj, δ(zj1 = zj2)) ∈C−| max\nzj1̸=zj2 WT Φ(hj1, hj2, zj1, zj2) −max\nsj1=sj2 WT Φ(hj1, hj2, sj1, sj2) < 1\n\u001b\n(10)\n5\nThen, we compute the gradient w.r.t. W\ndW = λW+\n−1\nn+\nX\n(hi1,hi2,1)∈A+\n\u0014\nΦ(hi1, hi2, z+\ni1, z+\ni2) −Φ(hi1, hi2, z−\ni1, z−\ni2)\n\u0015\n−1\nn−\nX\n(hj1,hj2,0)∈A−\n\u0014\nΦ(hj1, hj2, z−\nj1, z−\nj2) −Φ(hj1, hj2, z+\nj1, z+\nj2)\n\u0015\n−\nX\ni∈U\nβ\nUK\n\u0014\nΦ(hi, z+\ni ) −Φ(hi, s+\ni )\n\u0015\n,\n(11)\nwhere (z+\ni1, z+\ni2) = maxzi1=zi2 WT Φ(hi1, hi2, zi1, zi2), (z−\ni1, z−\ni2) = maxzi1̸=zi1 WT Φ(hi1, hi2, zi1, zi2); and for\nthe unlabeled set z+\ni = maxzi Φ(hi, zi) and s+\ni = maxsi̸=z+\ni Φ(hi, si)\nIn order to learn discriminative features, we also need to estimate the weights in the multi-layer network.\nNote that for each pair (hi, hj), if it violates the constraints in Eqs. 5 and 6, then we can compute the\ngradient w.r.t. hi and hj respectively, which will be used to calculate the gradients of θl for l ∈[1, L] in the\ndeep network. We use H = [h1, h2, ..., hN] as the concatenation of all the hidden codes, where H ∈Rd×N,\nwith each column H(:, i) = hi.\nFor the positive pairs, we have\ndhi1 = −1\nn+\nX\ni1\n[Wz+\ni1 −Wz−\ni1]\n(12a)\ndhi2 = −1\nn+\nX\ni2\n[Wz+\ni1 −Wz−\ni2]\n(12b)\nwhere Wz+\ni1 indicates the weight vector corresponding to the cluster label z+\ni1 in the whole weight matrix W.\nMore speciﬁcally, Wz+\ni1 = W[(z+\ni1 −1) × d + 1 : z+\ni1 × d]\nFor the negative pairs, we can get\ndhj1 = −1\nn−\nX\nj1\n[Wz−\nj1 −Wz+\nj1]\n(13a)\ndhj2 = −1\nn−\nX\nj2\n[Wz−\nj2 −Wz+\nj1]\n(13b)\nFor the unlabeled instances, we have\ndhi = −β\nUK\nX\ni\n[Wz+\ni −Ws+\ni ]\n(14)\nGiven the gradient of dhi for each hidden code, we can get the gradient w.r.t. H as\ndH(:, i) = dhi\n(15)\nwhere dhi can be calculated according to Eqs.\n12 and 13.\nThen, we can calculate the gradients w.r.t.\nlower level weights with back-propagation. For example dθL = dH ×\n\u0000f1→L(X) · (1 −f1→L(X))\n\u0001\n, where ×\nrepresentas matrix multiplication, and · indicates pointwise product.\nInitialization: We used stacked RBMs to initialize the weights layer by layer greedily in the deep network,\nwith contrastive divergence [12] (we used CD-1 in our experiments). Note that we used gaussian RBMs for\nthe continuous data in the ﬁrst layer, otherwise we used binary RBMs.\n6\nAlgorithm 1\n1: Input: the training data X, pairwise constraints C, the number of clusters K, the number of iterations\nT, λ, and β;\n2: Initialize W;\n3: Initialize wl for l = {1, ..., L} layer-by-layer greedily;\n4: for i = 1; i <= T; i + + do\n5:\nif the objective in Eq. 8 has no signiﬁcant changes, break;\n6:\nproject all training data X into latent space via Eq. 3;\n7:\nﬁnd the most violated constrains according to Eqs. 9 and 10\n8:\ncompute the gradient w.r.t. W via Eq. 11;\n9:\ncompute the gradient w.r.t. H via Eq. 15;\n10:\ncompute the gradient w.r.t. Θ = {θl}L\nl=1 with backpropagation;\n11:\nupdate the parameters with gradient descent via Eq. 16;\n12: end for\n13: Return model parameters W and {wl}L\nl=1, as well as average accuracy;\nIn our deep model, the weights from the layers 1 to L are θl respectively, for l = {1, .., L}, and the top\nlayer L has weight θL. We ﬁrst pre-train the L-layer deep structure with RBMs layer by layer greedily.\nThus, our deep network can learn parametric nonlinear mapping from input x to output h, f : x →h.\nSpeciﬁcally, we think RBM is a 1-layer deep network, with weight θ1. For example, for 1-layer DBN, we have\nh = f1(x) = logistic(θT\n1 [x, 1]), where we extend x ∈RD into [x, 1] ∈R(D+1) in order to handle bias in the\nnon-linear mapping. Given the output of the current layer as the input to the next layer, we can learn each\nlayer weight greedily.\nAs for the clustering weight W, we take a similar strategy as in [34] to initialize it.\nParameter updating: In our model, we use the gradient descent to update the model parameters. We also\ntried L-BFGS [4, 22] to update model parameters, but it did not perform well. In our model, we can update\nthe model parameters as follows,\nW ←W −γWdW,\nθl ←θl −γθldθl, l ∈{1, ..., L}\n(16)\nwhere γW is the learning rate for the clustering weight W, and γθl is the learning rate for weights θl in the\ndeep neural network. Thus, our method alternates between data projection and parameter optimization. For\nmore details, refer to algorithm 1.\nAfter we learned the model parameters, we can do cluster analysis according to Eq. 2.\n4\nExperiments\nIn this section, we presented a set of experiments comparing our method to the state of the art semi-supervised\nclustering methods on a wide range of data sets, including UCI data sets and the Reuters dataset in Table 1,\nas well as the MNIST digits, COIL-20 and COIL-100 datasets. We also evaluated whether the transductive\nconstraint in Eq. (7) is helpful or not in the clustering analysis.\n4.1\nExperimental setup\nIn the experiments, we compared our method to the state of the art semi-supervised clustering approaches,\nincluding Xing [32], ITML [6], KISSME [17] and CMMC [34]. Note that Xing, ITML and KISSME are the\n7\nDataset\nDescription\n#vectors\ndim\n#clusters\nGlass\n214\n9\n7\nWdbc\n569\n32\n2\nWine\n178\n13\n3\nSonar\n208\n60\n2\nImage Segmentation\n2310\n19\n7\nReuters\n8293\n18933\n65\nTable 1: Descriptions of the UCI datasets and the Reuters dataset.\nMethods\nAccuracy (%)\nAdjusted Rand Index\nGlass\nWdbc\nWine\nSonar\nSegmentation\nReuters\nGlass\nWdbc\nWine\nSonar\nSegmentation\nReuters\nXing [32]\n46.2\n91.9\n81.5\n53.4\n28.5\n44.3\n0.214\n0.70\n0.584\n0.02\n0.12\n0.14\nITML [6]\n47.4\n92.1\n70.2\n69.2\n30.0\n45.0\n0.223\n0.71\n0.520\n0.14\n0.14\n0.15\nKISSME [17]\n36.5\n77.9\n65.2\n67.3\n27.6\n49.7\n0.07\n0.287\n0.466\n0.12\n0.09\n0.17\nCMMC [34]\n43.2\n89.5\n97.1\n72.1\n51.4\n66.5\n0.217\n0.620\n0.918\n0.191\n0.35\n0.22\nOur method\n50.9\n91.5\n98.8\n72.6\n57.1\n72.7\n0.219\n0.689\n0.965\n0.20\n0.41\n0.56\nTable 2: The experimental comparison on the UCI data sets and the Reuters data set. For the real UCI\ndatasets, our method outperforms other methods signiﬁcantly, except on the Glass and Wdbc data sets. Our\nmethod is remarkably better on the Reuters dataset, with both accuracy and Rand index.\nsemi-supervised approaches for metric learning (Mahalanobis). Thus, we used those methods to learn the\nmetric and calculate the distances between all instances, then we used the kernel k-means [7] for clustering.\nOur method and CMMC are similar, which can be directly optimized for clustering.\nAs for parameter setting, we set λ = 0.02 and β = 1. The learning rate γW decreases in the iterations in our\nmodel, by setting γW =\n1\nλ×(i+1), where i is the index for iterations; while the learning rate for weighs in the\ndeep network ﬁxed, with γθl = 0.01, for l = {1, ..., L}. Without other speciﬁcation, our model used the one\nhidden layer with 100 units on most data sets, except on the MNIST and UCI data sets.\nWe tested our method on two tasks: pairwise classiﬁcation and clustering analysis. As for pairwise classiﬁca-\ntion, we randomly sampled 200 pairs of constraints (around 100 must-links and 100 cannot links), of which we\nused 100 pairwise constraints (50 must-links and 50 cannot links) as the training set, and the rest 100 pairs\nas the testing sets. Then we used the receiver operating characteristic (ROC) to evaluate the performance.\nAs for the clustering analysis, we used the pairwise constraints sampled to train our model, then we use\nthe learned model for clustering analysis. We used the accuracy (the most possible matching between the\nobtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our\nmethod in all the experiments.\n4.2\nResults\nUCI data sets: In the experiment, we selected the ﬁve widely used data sets from the UCI machine learning\nrepository1, which has diﬀerent dimension and categories, shown in Table 1. As for the number of hidden\nunits in our model, we set the number of hidden nodes to be 100 on the sonar data set and 64 on the\nother UCI data sets. For the each data set, we randomly sampled 200 pairwise constraints, of which 100\npairs were used for training and the rest to test the pairwise classiﬁcation performance. While for clustering\nperformance, we test the model on all the data elements. We compared our method to the state of the art\nmethods, and clustering results are shown in Table. (2). It demonstrates that our method outperforms other\n1https://archive.ics.uci.edu/ml/datasets.html\n8\nmethods on almost all the data sets, especially for the data with the larger number of classes. We also show\nthe performance of our method on the pairwise classiﬁcation task in Fig. 1. Except on the Wdbc and glass\ndata sets, our method yields completive and even better results than other methods.\nReuters data set: We used the Reuters215782, which has the total 8293 documents with 18933 dimensional\nfeatures for each document, belonging to 65 categories. Because the Reuters data set has high dimension, we\nﬁrst projected it into 400 dimensions with PCA. Then we set the number of hidden nodes to be 100 in our\nmodel. The clustering performance is shown in Table. (2). It demonstrates that our method is signiﬁcantly\nbetter than other methods. Again, our method yields remarkably better pairwise classiﬁcation result, shown\nin the right bottom of Fig. 1.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nROC:wine\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nROC: glass\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nROC: segmentation\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nROC: sonar\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nROC: wdbc\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nROC: Reuters\nTrue Positive Rate (TPR)\nFalse Positive Rate (FPR)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\nFigure 1: The pairwise classiﬁcation results on the ﬁve UCI data sets and the Reuters data set (the right\nbottom).\n2http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html\n9\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ny\nThe number of pairwise constraints\nThe clustering accuracy (%)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nj\nThe number of pairwise constraints\nThe adjusted Rand Index\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\nFigure 2: The clustering performance comparison on the MNIST digits by varying the number of training\npairs, evaluated with accuracy and rand index respectively. It demonstrates that our method is signiﬁcantly\nbetter than other methods for clustering analysis.\nMNIST dataset: The MNIST digits3 consists of 28 × 28-size images of handwriting digits from 0 through\n9 with a training set of 60,000 examples and a test set of 10,000 examples, and has been widely used to test\ncharacter recognition methods. In the experiment, we randomly sampled a subset with 5000 images from the\ntraining sets to test our method and other baselines. In the experiment, we use a three-layer deep structure\nfor MNIST digits, with hidden nodes [400 200 100] respectively on each layer. We tested how the clustering\nperformance changes when the number of pairwise constraints varies. The experimental comparisons between\nour method and other baselines are shown in Fig. 2. It demonstrates that the clustering accuracy is increasing\nwith more pairwise constraints. And it also shows our method is better than other baselines in most cases\nwhen varying the number of training pairs.\nTo evaluate whether the transductive constraint in our model in Eq. 4 is helpful or not for clustering, we\nset β = 0 to get rid of the transductive condition in Eq. 7, and the experimental results are shown in Fig.\n3. We argue that the result in Fig. 3 is consistent with common sense. The smaller the number of pairwise\nconstraints, the higher uncertainty when we do inference. Thus, transductive learning has no advantage\nwhen the number of constraints is small. But it performs better with more constraints in Fig. 3. When\nmore and more pairwise constraints are available, there’s no need to incorporate transductive principles in\nthe model. To sum up, it demonstrates that the transductive constraint in our model is remarkably helpful\nfor the semi-supervised clustering analysis.\nCOIL data set: We test our method on both COIL-20 and COIL-100 image data sets. The COIL-20 data\nset4 has total 1440 images, with size 128 × 128. It is divided into 20 classes of objects, with 72 images for\neach object. In our experiments, we used the processed version, which contains images for all of the objects\nin which the background has been discarded, and furthermore we resized all the images into 32×32 for space\nand time concern. The COIL-100 data set consists of 7200 images, partitioned into 100 classes. Similarly,\nwe also resized the images into 32 × 32 before learning clustering model.\nThe clustering performance is shown in Fig. 5, and it demonstrates that our method is better than other\nmethods with both stability and clustering accuracy. Where’s the performance gain from in Fig. 5? deep\nlearning or transductive learning? To answer this question, we evaluated whether transductive training is\nhelpful when the number of pairwise constraints is limited. In Figs. 4(a) and (b), it shows the results on\n20 classes, and demonstrates that transductive learning is helpful when the number of training pairs is in\nrange [400 2000]. But with more and more training constraints, transductive learning cannot improve the\nperformance too much. In Figs. 4(c) and (d), it shows the results on COIL with 100 classes, and indicates\n3http://yann.lecun.com/exdb/mnist/\n4http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php\n10\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ny\nThe number of pairwise constraints\nThe clustering accuracy (%)\n \n \nNo Transduction\nWith Transduction\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nj\nThe number of pairwise constraints\nThe adjusted Rand Index\n \n \nNo Transduction\nWith Transduction\nFigure 3: The comparison between with and without transductive principles for our method. (a) and (b)\nshow the results (evaluated with accuracy and rand index respectively) on the MNIST data set.\nthat transductive learning cannot improve the performance much when the number of classes is large. We\nthink the reason that transductive learning cannot perform well in Figs. 4(c) and (d) is that it cannot infer\nlabel well with large margin on the dataset with a larger number of clusters. We argue when we have more\ndata and more clusters, it is harder to partition the data well, and more diﬃcult to ﬁnd a better hyperplane\nto separate one cluster well from the others with large margin. In other words, it is harder to satisfy the\ncondition in Eq. 7. Compared to the COIL dataset, transductive learning yields a larger gain on the MNIST\ndata set in Fig. 3. Thus, transductive learning is helpful when the number of classes is small and the data\nis well distributed (compact within the same cluster, and separated between diﬀerent clusters).\nThus, the most performance gain on the COIL-100 data set in Fig. 5 is from deep learning, according to the\nabove analysis.\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAccuracy on the COIL−20 dataset\nThe number of pairwise constraints\nThe clustering accuracy (%)\n \n \nNo Transduction\nWith Transduction\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nThe adjusted Rand Index on the COIL−20 dataset\nThe number of pairwise constraints\nThe adjusted Rand Index\n \n \nNo Transduction\nWith Transduction\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nAccuracy on the Coil−100 dataset\nThe number of pairwise constraints\nThe clustering accuracy (%)\n \n \nNo Transduction\nWith Transduction\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nThe adjusted Rand Index on the Coil−100 dataset\nThe number of pairwise constraints\nThe adjusted Rand Index\n \n \nNo Transduction\nWith Transduction\n(a)\n(b)\n(c)\n(d)\nFigure 4: The comparison between with and without transductive principles for our method. (a) and (b)\nshow the results (evaluated with accuracy and rand index respectively) on the COIL-20 data set; (c) and (d)\nare the results on the COIL-100 dataset, with accuracy and rand index respectively. For the 20 classes, it\nshows that the transductive learning is helpful when the number of training pairs is small. However, for the\n100 classes, the transductive learning cannot improve the performance. It demonstrates that it is better to\nleverage transductive principles when the number of classes is relative smaller.\n5\nConclusions\nIn this paper, we propose a deep transductive semi-supervised maximum margin clustering approach. One\nthe one hand, we leverage deep learning to learn non-linear representations, which can be used as the input\nto the semi-supervised clustering model. On the other hand, we incorporate the non-label instances into\n11\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nAccuracy on the Coil−100 dataset\nThe number of pairwise constraints\nThe clustering accuracy (%)\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\n50\n100\n200\n400\n500\n800\n1000\n2000\n5000\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nThe adjusted Rand Index on the Coil−100 dataset\nThe number of pairwise constraints\nThe adjusted Rand Index\n \n \nXing\nITML\nKISSME\nCMMC\nOur method\nFigure 5: The clustering comparison on the COIL-100 data set by varying the number of training pairs. It\nshows that our method outperforms other methods signiﬁcantly.\nour semi-supervised clustering framework.\nThus, our model uniﬁes transductive learning, deep learning,\nmaximum margin and semi-supervised clustering in one framework. Compared to conventional methods, our\napproach can learn non-linear mappings as well as leveraging transductive information to improve clustering\nperformance. We pretrain the deep structure with stacked restricted Boltzmann machines layer by layer\ngreedily for feature representations and optimize our objective function with gradient decent. We demonstrate\nthe advantages of our model over the state of the art in the experiments.\nReferences\n[1] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning distance functions using equivalence\nrelations.\nIn In Proceedings of the Twentieth International Conference on Machine Learning, pages\n11–18, 2003.\n[2] Y. Bengio, A. Courville, and P. Vincent.\nRepresentation learning: A review and new perspectives.\nPAMI, pages 1798–1828, 2013.\n[3] M. Bilenko, S. Basu, and R. J. Mooney. Integrating constraints and metric learning in semi-supervised\nclustering. In Proceedings of the Twenty-ﬁrst International Conference on Machine Learning, ICML ’04,\npages 11–, New York, NY, USA, 2004. ACM.\n[4] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained opti-\nmization. SIAM J. Sci. Comput., 16(5):1190–1208, Sept. 1995.\n[5] D. Cohn, R. Caruana, and A. Mccallum.\nSemi-supervised clustering with user feedback.\nTechnical\nreport, 2003.\n[6] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.\nInformation-theoretic metric learning.\nIn\nProceedings of the 24th International Conference on Machine Learning, ICML ’07, pages 209–216, New\nYork, NY, USA, 2007. ACM.\n[7] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means, spectral clustering and normalized cuts. In KDD,\n2004.\n[8] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised\npre-training help deep learning? J. Mach. Learn. Res., 11:625–660, Mar. 2010.\n[9] A. Gelfand, Y. Chen, L. van der Maaten, and M. Welling.\nOn herding and the perceptron cycling\ntheorem. In J. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances\nin Neural Information Processing Systems 23, pages 694–702, 2010.\n12\n[10] A. Globerson and S. T. Roweis. Metric learning by collapsing classes. In NIPS, 2005.\n[11] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In\nAdvances in Neural Information Processing Systems 17, pages 513–520. MIT Press, 2004.\n[12] G. E. Hinton, S. Osindero, and Y.-W. Teh.\nA fast learning algorithm for deep belief nets.\nNeural\nComput., 18(7):1527–1554, jul 2006.\n[13] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.\nScience, 313(5786):504–507, July 2006.\n[14] Y. Hu, J. Wang, N. Yu, and X.-S. Hua. Maximum margin clustering with pairwise constraints. In ICDM,\npages 253–262. IEEE Computer Society, 2008.\n[15] L. Hubert and P. Arabie. Comparing partitions. Journal of classiﬁcation, 2(1):193–218, 1985.\n[16] D. Klein, S. D. Kamvar, and C. D. Manning. From instance-level constraints to space-level constraints:\nMaking the most of prior knowledge in data clustering. In Proceedings of the Nineteenth International\nConference on Machine Learning, ICML ’02, pages 307–314, San Francisco, CA, USA, 2002. Morgan\nKaufmann Publishers Inc.\n[17] M. Koestinger, M. Hirzer, P. Wohlhart, P. M. Roth, and H. Bischof. Large scale metric learning from\nequivalence constraints. In Proc. IEEE Intern. Conf. on Computer Vision and Pattern Recognition,\n2012.\n[18] H. Larochelle and Y. Bengio. Classiﬁcation using discriminative restricted boltzmann machines. In\nICML, pages 536–543, New York, NY, USA, 2008. ACM.\n[19] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio. Learning algorithms for the classiﬁcation re-\nstricted boltzmann machine. J. Mach. Learn. Res., 13(1):643–669, Mar. 2012.\n[20] N. Nguyen and R. Caruana. Improving classiﬁcation with pairwise constraints: A margin-based ap-\nproach. In ECML/PKDD (2), pages 113–124, 2008.\n[21] W. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical\nAssociation, 66(336):846–850, 1971.\n[22] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computa-\ntion and Machine Learning). The MIT Press, 2005.\n[23] S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online and batch learning of pseudo-metrics. In Proceedings\nof the Twenty-ﬁrst International Conference on Machine Learning, ICML ’04, pages 94–, New York, NY,\nUSA, 2004. ACM.\n[24] Y. Tang. Deep learning using support vector machines. In Workshop on Representational Learning,\nICML 2013, volume abs/1306.0239, 2013.\n[25] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and\ninterdependent output variables. JMLR, pages 1453–1484, 2005.\n[26] H. Valizadegan and R. Jin. Generalized maximum margin clustering and unsupervised kernel learning.\nIn B. Schlkopf, J. Platt, and T. Hoﬀman, editors, NIPS, pages 1417–1424. MIT Press, 2006.\n[27] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc., 1995.\n[28] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders:\nLearning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res.,\n11:3371–3408, Dec. 2010.\n[29] K. Wagstaﬀ, C. Cardie, S. Rogers, and S. Schr¨odl. Constrained k-means clustering with background\nknowledge. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,\npages 577–584, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.\n[30] K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classiﬁca-\ntion. J. Mach. Learn. Res., 10:207–244, June 2009.\n[31] J. Weston and F. Ratle. Deep learning via semi-supervised embedding. In International Conference on\nMachine Learning, 2008.\n13\n[32] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning, with application to clustering\nwith side-information. In ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 15,\npages 505–512. MIT Press, 2003.\n[33] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In NIPS17, pages\n1537–1544, 2005.\n[34] H. Zeng and Y. ming Cheung. Semi-supervised maximum margin clustering with pairwise constraints.\nIEEE Trans. Knowl. Data Eng., 24(5):926–939, 2012.\n[35] J. Zhang and R. Yan. On the value of pairwise constraints in classiﬁcation and consistency. In Proceedings\nof the 24th International Conference on Machine Learning, ICML ’07, pages 1111–1118, New York, NY,\nUSA, 2007. ACM.\n14\n",
  "categories": [
    "cs.LG",
    "68T10",
    "I.2.6"
  ],
  "published": "2015-01-26",
  "updated": "2015-01-26"
}