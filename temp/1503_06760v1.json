{
  "id": "http://arxiv.org/abs/1503.06760v1",
  "title": "Unsupervised POS Induction with Word Embeddings",
  "authors": [
    "Chu-Cheng Lin",
    "Waleed Ammar",
    "Chris Dyer",
    "Lori Levin"
  ],
  "abstract": "Unsupervised word embeddings have been shown to be valuable as features in\nsupervised learning problems; however, their role in unsupervised problems has\nbeen less thoroughly explored. In this paper, we show that embeddings can\nlikewise add value to the problem of unsupervised POS induction. In two\nrepresentative models of POS induction, we replace multinomial distributions\nover the vocabulary with multivariate Gaussian distributions over word\nembeddings and observe consistent improvements in eight languages. We also\nanalyze the effect of various choices while inducing word embeddings on\n\"downstream\" POS induction results.",
  "text": "Unsupervised POS Induction with Word Embeddings\nChu-Cheng Lin\nWaleed Ammar\nChris Dyer\nLori Levin\nSchool of Computer Science\nCarnegie Mellon University\n{chuchenl,wammar,cdyer,lsl}@cs.cmu.edu\nAbstract\nUnsupervised word embeddings have been\nshown to be valuable as features in supervised\nlearning problems; however, their role in un-\nsupervised problems has been less thoroughly\nexplored. In this paper, we show that embed-\ndings can likewise add value to the problem\nof unsupervised POS induction. In two repre-\nsentative models of POS induction, we replace\nmultinomial distributions over the vocabulary\nwith multivariate Gaussian distributions over\nword embeddings and observe consistent im-\nprovements in eight languages. We also ana-\nlyze the effect of various choices while induc-\ning word embeddings on “downstream” POS\ninduction results.\n1\nIntroduction\nUnsupervised POS induction is the problem of as-\nsigning word tokens to syntactic categories given\nonly a corpus of untagged text. In this paper we\nexplore the effect of replacing words with their vec-\ntor space embeddings1 in two POS induction mod-\nels: the classic ﬁrst-order HMM (Kupiec, 1992) and\nthe newly introduced conditional random ﬁeld au-\ntoencoder (Ammar et al., 2014).\nIn each model,\ninstead of using a conditional multinomial distribu-\ntion2 to generate a word token wi ∈V given a POS\ntag ti ∈T, we use a conditional Gaussian distribu-\ntion and generate a d-dimensional word embedding\nvwi ∈Rd given ti.\n1Unlike Yatbaz et al. (2014), we leverage easily obtainable\nand widely used embeddings of word types.\n2Also known as a categorical distribution.\nOur ﬁndings suggest that, in both models, sub-\nstantial improvements are possible when word em-\nbeddings are used rather than opaque word types.\nHowever, the independence assumptions made by\nthe model used to induce embeddings strongly de-\ntermines its effectiveness for POS induction: em-\nbedding models that model short-range context are\nmore effective than those that model longer-range\ncontexts.\nThis result is unsurprising, but it illus-\ntrates the lack of an evaluation metric that measures\nthe syntactic (rather than semantic) information in\nword embeddings. Our results also conﬁrm the con-\nclusions of Sirts et al. (2014) who were likewise\nable to improve POS induction results, albeit using a\ncustom clustering model based on the the distance-\ndependent Chinese restaurant process (Blei and Fra-\nzier, 2011).\nOur contributions are as follows: (i) reparameter-\nization of token-level POS induction models to use\nword embeddings; and (ii) a systematic evaluation\nof word embeddings with respect to the syntactic in-\nformation they contain.\n2\nVector Space Word Embeddings\nWord embeddings represent words in a language’s\nvocabulary as points in a d-dimensional space such\nthat nearby words (points) are similar in terms of\ntheir distributional properties.\nA variety of tech-\nniques for learning embeddings have been proposed,\ne.g., matrix factorization (Deerwester et al., 1990;\nDhillon et al., 2011) and neural language modeling\n(Mikolov et al., 2011; Collobert and Weston, 2008).\nFor the POS induction task, we speciﬁcally\nneed embeddings that capture syntactic similarities.\narXiv:1503.06760v1  [cs.CL]  23 Mar 2015\nTherefore we experiment with two types of embed-\ndings that are known for such properties:\n• Skip-gram embeddings (Mikolov et al., 2013)\nare based on a log bilinear model that predicts\nan unordered set of context words given a target\nword. Bansal et al. (2014) found that smaller con-\ntext window sizes tend to result in embeddings\nwith more syntactic information. We conﬁrm this\nﬁnding in our experiments.\n• Structured skip-gram embeddings (Ling et al.,\n2015) extend the standard skip-gram embeddings\n(Mikolov et al., 2013) by taking into account the\nrelative positions of words in a given context.\nWe use the tool word2vec3 and Ling et al. (2015)’s\nmodiﬁed version4 to generate both plain and struc-\ntured skip-gram embeddings in nine languages.\n3\nModels for POS Induction\nIn this section, we brieﬂy review two classes\nof models used for POS induction (HMMs and\nCRF autoencoders), and explain how to generate\nword embedding observations in each class.\nWe\nwill represent a sentence of length ℓas w\n=\n⟨w1, w2, . . . , wℓ⟩∈V ℓand a sequence of tags as\nt = ⟨t1, t2, . . . , tℓ⟩∈T ℓ. The embeddings of word\ntype w ∈V will be written as vw ∈Rd.\n3.1\nHidden Markov Models\nThe hidden Markov model with multinomial emis-\nsions is a classic model for POS induction. This\nmodel makes the assumption that a latent Markov\nprocess with discrete states representing POS cate-\ngories emits individual words in the vocabulary ac-\ncording to state (i.e., tag) speciﬁc emission distri-\nbutions. An HMM thus deﬁnes the following joint\ndistribution over sequences of observations and tags:\np(w, t) =\nℓY\ni=1\np(ti | ti−1) × p(wi | ti)\n(1)\nwhere distributions p(ti | ti−1) represents the transi-\ntion probability and p(wi | ti) is the emission prob-\nability, the probability of a particular tag generating\nthe word at position i.5\n3https://code.google.com/p/word2vec/\n4https://github.com/wlin12/wang2vec/\n5Terms for the starting and stopping transition probabilities\nare omitted for brevity.\nWe consider two variants of the HMM as base-\nlines:\n• p(wi | ti) is parameterized as a “na¨ıve multino-\nmial” distribution with one distinct parameter for\neach word type.\n• p(wi | ti) is parameterized as a multinomial lo-\ngistic regression model with hand-engineered fea-\ntures as detailed in (Berg-Kirkpatrick et al., 2010).\nGaussian Emissions.\nWe now consider incorpo-\nrating word embeddings in the HMM. Given a tag\nt ∈T, instead of generating the observed word\nw ∈V , we generate the (pre-trained) embedding\nvw ∈Rd of that word. The conditional probabil-\nity density assigned to vw | t follows a multivariate\nGaussian distribution with mean µt and covariance\nmatrix Σt:\np(vw; µt, Σt) = exp\n\u0000−1\n2(vw −µt)⊤Σ−1\nt (vw −µt)\n\u0001\np\n(2π)d|Σt|\n(2)\nThis parameterization makes the assumption that\nembeddings of words which are often tagged as t\nare concentrated around some point µt ∈Rd, and\nthe concentration decays according to the covariance\nmatrix Σt.6\nNow, the joint distribution over a sequence of ob-\nservations v = ⟨vw1, vw2 . . . , vwℓ⟩(which corre-\nsponds to word sequence w = ⟨w1, w2, . . . , wℓ, ⟩)\nand a tag sequence t = ⟨t1, t2 . . . , tℓ⟩becomes:\np(v, t) =\nℓY\ni=1\np(ti | ti−1) × p(vwi; µti, Σti)\nWe use the Baum–Welch algorithm to ﬁt the µt\nand Σti parameters. In every iteration, we update\nµt∗as follows:\nµnew\nt∗\n=\nP\nv∈T\nP\ni=1...ℓp(ti = t∗| v) × vwi\nP\nv∈T\nP\ni=1...ℓp(ti = t∗| v)\n(3)\nwhere T is a data set of word embedding sequences\nv each of length |v| = ℓ, and p(ti = t∗| v) is the\n6“essentially, all models are wrong, but some are useful” –\nGeorge E. P. Box\nposterior probability of label t∗at position i in the\nsequence v. Likewise the update to Σt∗is:\nΣnew\nt∗\n=\nP\nv∈T\nP\ni=1...ℓp(ti = t∗| v) × δδ⊤\nP\nv∈T\nP\ni=1...ℓp(ti = t∗| v)\n(4)\nwhere δ = vwi −µnew\nt∗.\n3.2\nConditional Random Field Autoencoders\nThe second class of models this work extends is\ncalled CRF autoencoders, which we recently pro-\nposed in (Ammar et al., 2014). It is a scalable family\nof models for feature-rich learning from unlabeled\nexamples.\nThe model conditions on one copy of\nthe structured input, and generates a reconstruction\nof the input via a set of interdependent latent vari-\nables which represent the linguistic structure of in-\nterest. As shown in Eq. 5, the model factorizes into\ntwo distinct parts: the encoding model p(t | w) and\nthe reconstruction model p( ˆ\nw | t); where w is the\nstructured input (e.g., a token sequence), t is the lin-\nguistic structure of interest (e.g., a sequence of POS\ntags), and ˆ\nw is a generic reconstruction of the input.\nFor POS induction, the encoding model is a linear-\nchain CRF with feature vector λ and local feature\nfunctions f.\np( ˆ\nw,t | w) = p(t | w) × p( ˆ\nw | t)\n∝p( ˆ\nw | t) × exp λ ·\n|w|\nX\ni=1\nf(ti, ti−1, w)\n(5)\nIn (Ammar et al., 2014), we explored two kinds\nof reconstructions ˆ\nw:\nsurface forms and Brown\nclusters (Brown et al., 1992), and used “stupid\nmultinomials” as the underlying distributions for re-\ngenerating ˆ\nw.\nGaussian Reconstruction.\nIn this paper,\nwe\nuse d-dimensional word embedding reconstructions\nˆwi = vwi ∈Rd, and replace the multinomial dis-\ntribution of the reconstruction model with the mul-\ntivariate Gaussian distribution in Eq. 2. We again\nuse the Baum–Welch algorithm to estimate µt∗and\nΣt∗similar to Eq. 3. The only difference is that\nposterior label probabilities are now conditional on\nboth the input sequence w and the embeddings se-\nquence v, i.e., replace p(ti = t∗| v) in Eq. 2 with\np(ti = t∗| w, v).\n4\nExperiments\nIn this section, we attempt to answer the following\nquestions:\n• §4.1:\nDo syntactically-informed word embed-\ndings improve POS induction? Which model per-\nforms best?\n• §4.2: What kind of word embeddings are suitable\nfor POS induction?\n4.1\nChoice of POS Induction Models\nHere, we compare the following models for POS in-\nduction:\n• Baseline: HMM with multinomial emissions (Ku-\npiec, 1992),\n• Baseline: HMM with log-linear emissions (Berg-\nKirkpatrick et al., 2010),\n• Baseline: CRF autoencoder with multinomial re-\nconstructions (Ammar et al., 2014),7\n• Proposed: HMM with Gaussian emissions, and\n• Proposed: CRF autoencoder with Gaussian recon-\nstructions.\nData.\nTo train the POS induction models, we used\nthe plain text from the training sections of the\nCoNLL-X shared task (Buchholz and Marsi, 2006)\n(for Danish and Turkish), the CoNLL 2007 shared\ntask (Nivre et al., 2007) (for Arabic, Basque, Greek,\nHungarian and Italian), and the Ukwabelana corpus\n(Spiegler et al., 2010) (for Zulu). For evaluation,\nwe obtain the corresponding gold-standard POS tags\nby deterministically mapping the language-speciﬁc\nPOS tags in the aforementioned corpora to the corre-\nsponding universal POS tag set (Petrov et al., 2012).\nThis is the same set up we used in (Ammar et al.,\n2014).\nSetup.\nIn this section, we used skip-gram (i.e.,\nword2vec) embeddings with a context window\nsize =\n1 and with dimensionality d\n=\n100,\ntrained with the largest corpora for each language\nin (Quasthoff et al., 2006), in addition to the plain\n7We use the conﬁguration with best performance which re-\nconstructs Brown clusters.\nArabic\nBasque\nDanish\nGreek\nHungarian\nItalian\nTurkish\nZulu\nAverage\nV−measure\n0.0\n0.2\n0.4\n0.6\n0.8\nMultinomial HMM\nMultinomial Featurized HMM\nMultinomial CRF Autoencoder\nGaussian HMM\nGaussian CRF Autoencoder\nArabic\nBasque\nDanish\nGreek\nHungarian\nItalian\nTurkish\nZulu\nAverage\nV−measure\n0.0\n0.2\n0.4\n0.6\n0.8\nHMM (standard skip−gram)\nCRF Autoencoder (standard skip−gram)\nHMM (structured skip−gram)\nCRF Autoencoder (structured skip−gram)\nFigure 1: POS induction results. (V-measure, higher is better.) Window size is 1 for all word embeddings.\nLeft: Models which use standard skip-gram word embeddings (i.e., Gaussian HMM and Gaussian CRF\nAutoencoder) outperform all baselines on average across languages. Right: comparison between standard\nand structured skip-grams on Gaussian HMM and CRF Autoencoder.\ntext used to train the POS induction models.8\nIn\nthe proposed models, we only show results for es-\ntimating µt, assuming a diagonal covariance ma-\ntrix Σt(k, k) = 0.45∀k ∈{1, . . . , d}.9 While the\nCRF autoencoder with multinomial reconstructions\nwere carefully initialized as discussed in (Ammar et\nal., 2014), CRF autoencoder with Gaussian recon-\nstructions were initialized uniformly at random in\n[−1, 1]. All HMM models were also randomly ini-\ntialized. We tuned all hyperparameters on the En-\nglish PTB corpus, then ﬁxed them for all languages.\nEvaluation.\nWe use the V-measure evaluation\nmetric (Rosenberg and Hirschberg, 2007) to eval-\nuate the predicted syntactic classes at the token\nlevel.10\nResults.\nThe results in Fig. 1 (left) clearly sug-\ngest that we can use word embeddings to improve\nPOS induction. Surprisingly, the feature-less Gaus-\nsian HMM model outperforms the strong feature-\n8We\nused\nthe\ncorpus/tokenize-anything.sh\nscript in the cdec decoder (Dyer et al., 2010) to tokenize the\ncorpora from (Quasthoff et al., 2006). The other corpora were\nalready tokenized.\nIn Arabic and Italian, we found a lot of\ndiscrepancies between the tokenization used for inducing word\nembeddings and the tokenization used for evaluation.\nWe\nexpect our results to improve with consistent tokenization.\n9Surprisingly, we found that estimating Σt signiﬁcantly de-\ngrades the performance. This may be due to overﬁtting (Shi-\nnozaki and Kawahara, 2007). Possible remedies include using\na prior (Gauvain and Lee, 1994).\n10We found the V-measure results to be consistent with the\nmany-to-one evaluation metric (Johnson, 2007). We only show\none set of results for brevity.\nrich baselines: Multinomial Featurized HMM and\nMultinomial CRF Autoencoder. One explanation is\nthat our word embeddings were induced using larger\nunlabeled corpora than those used to train the POS\ninduction models. The best results are obtained us-\ning both word embeddings and feature-rich models\nusing the Gaussian CRF autoencoder model. This\nset of results suggest that word embeddings and\nhand-engineered features play complementary roles\nin POS induction. It is worth noting that the CRF au-\ntoencoder model with Gaussian reconstructions did\nnot require careful initialization.11\n4.2\nChoice of Embeddings\nStandard skip-gram vs.\nstructured skip-gram.\nOn Gaussian HMMs, structured skip-gram embed-\ndings score moderately higher than standard skip-\ngrams. And as the context window size gets larger\nthe gap widens. The reason may be that structured\nskip-gram embeddings give each position within the\ncontext window its own project matrix, so the smear-\ning effect is not as pronounced as the window grows\nwhen compared to the standard embeddings. How-\never the best performance is still obtained when the\nwindow is small.12\n11In (Ammar et al., 2014), we found that careful initialization\nfor the CRF autoencoder model with multinomial reconstruc-\ntions is necessary.\n12In preliminary experiments, we also compared standard\nskip-gram embeddings to SENNA embeddings (Collobert et al.,\n2011) (which are trained in a semi-supervised multi-task learn-\ning setup, with one task being POS tagging) on a subset of\nthe English PTB corpus. As expected, the induced POS tags\n1\n2\n4\n8\n16\nWindow size\navg. V−measure\n0.30\n0.45\nstandard skip−gram\nstructured skip−gram\nFigure 2: Effect of window size and embeddings\ntype on POS induction over the languages in Fig. 1.\nd = 100. The model is HMM with Gaussian emis-\nsions.\nDimensions = 20 vs.\n200.\nWe also varied the\nnumber of dimensions in the word vectors (d ∈\n{20, 50, 100, 200}). The best V-measure we obtain\nis 0.504 (d = 20) and the worst is 0.460 (d = 100).\nHowever, we did not observe a consistent pattern as\nshown in Fig. 3.\nWindow size = 1 vs.\n16.\nFinally, we varied\nthe window size for the context surrounding target\nwords (w ∈{1, 2, 4, 8, 16}).\nw = 1 yields the\nbest average V-measure across the eight languages\nas shown in Fig. 2. This is true for both standard\nand structured skip-gram models. Notably, larger\nwindow sizes appear to produce word embeddings\nwith less syntactic information. This result conﬁrms\nthe observations of Bansal et al. (2014).\n4.3\nDiscussion\nWe have shown that (re)generating word embed-\ndings does much better than generating opaque word\ntypes in unsupervised POS induction.\nAt a high\nlevel, this conﬁrms prior ﬁndings that unsupervised\nword embeddings capture syntactic properties of\nwords, and shows that different embeddings capture\nmore syntactically salient information than others.\nAs such, we contend that unsupervised POS induc-\ntion can be seen as a diagnostic metric for assessing\nthe syntactic quality of embeddings.\nTo get a better understanding of what the multi-\nvariate Gaussian models have learned, we conduct a\nhill-climbing experiment on our English dataset. We\nare much better when using SENNA embeddings, yielding a V-\nmeasure score of 0.57 compared to 0.51 for skip-gram embed-\ndings. Since SENNA embeddings are only available in English,\nwe did not include it in the comparison in Fig. 1.\n20\n50\n100\n200\nDimension size\nV−measure\n0.30\n0.45\nFigure 3: Effect of dimension size on POS induction\non a subset of the English PTB corpus. w = 1. The\nmodel is HMM with Gaussian emissions.\nseed each POS category with the average vector of\n10 randomly sampled words from that category and\ntrain the model. Seeding unsurprisingly improves\ntagging performance. We also ﬁnd words that are\nthe nearest to the centroids generally agree with the\ncorrect category label, which validate our assump-\ntion that syntactically similar words tend to cluster\nin the high-dimensional embedding space. It also\nshows that careful initialization of model parameters\ncan bring further improvements.\nHowever we also ﬁnd that words that are close\nto the centroid are not necessarily representative\nof what linguists consider to be prototypical. For\nexample, Hopper and Thompson (1983) show that\nphysical, telic, past tense verbs are more prototyp-\nical with respect to case marking, agreement, and\nother syntactic behavior. However, the verbs near-\nest our centroid all seem rather abstract. In English,\nthe nearest 5 words in the verb category are entails,\naspires, attaches, foresees, deems. This may be be-\ncause these words seldom serve functions other than\nverbs; and placing the centroid around them incurs\nless penalty (in contrast to physical verbs, e.g. bite,\nwhich often also act as nouns). Therefore one should\nbe cautious in interpreting what is prototypical about\nthem.\n5\nConclusion\nWe propose using a multivariate Gaussian model to\ngenerate vector space representations of observed\nwords in generative or hybrid models for POS induc-\ntion, as a superior alternative to using multinomial\ndistributions to generate categorical word types. We\nﬁnd the performance from a simple Gaussian HMM\ncompetitive with strong feature-rich baselines. We\nfurther show that substituting the emission part of\nthe CRF autoencoder can bring further improve-\nments.\nWe also conﬁrm previous ﬁndings which\nsuggest that smaller context windows in skip-gram\nmodels result in word embeddings which encode\nmore syntactic information. It would be interesting\nto see if we can apply this approach to other tasks\nwhich require generative modeling of textual obser-\nvations such as language modeling and grammar in-\nduction.\nAcknowledgements\nThis work was sponsored by the U.S. Army Re-\nsearch Laboratory and the U.S. Army Research\nOfﬁce under contract/grant numbers W911NF-11-\n2-0042 and W911NF-10-1-0533.\nThe statements\nmade herein are solely the responsibility of the au-\nthors.\nReferences\n[Ammar et al.2014] Waleed Ammar, Chris Dyer, and\nNoah A. Smith. 2014. Conditional random ﬁeld au-\ntoencoders for unsupervised structured prediction. In\nNIPS.\n[Bansal et al.2014] Mohit Bansal, Kevin Gimpel, and\nKaren Livescu. 2014. Tailoring continuous word rep-\nresentations for dependency parsing. In Proc. of ACL.\n[Berg-Kirkpatrick et al.2010] Taylor\nBerg-Kirkpatrick,\nAlexandre Bouchard-Cˆot´e, John DeNero, and Dan\nKlein.\n2010.\nPainless unsupervised learning with\nfeatures. In Proc. of NAACL.\n[Blei and Frazier2011] David M. Blei and Peter I. Fra-\nzier.\n2011.\nDistance dependent Chinese restaurant\nprocesses. JMLR.\n[Brown et al.1992] Peter F. Brown, Peter V. deSouza,\nRobert L. Mercer, Vincent J. Della Pietra,\nand\nJenifer C. Lai. 1992. Class-based n-gram models of\nnatural language. Computational Linguistics.\n[Buchholz and Marsi2006] Sabine Buchholz and Erwin\nMarsi. 2006. CoNLL-X shared task on multilingual\ndependency parsing. In CoNLL-X.\n[Collobert and Weston2008] Ronan Collobert and Jason\nWeston. 2008. A uniﬁed architecture for natural lan-\nguage processing: Deep neural networks with multi-\ntask learning. In Proc. of ICML.\n[Collobert et al.2011] Ronan Collobert, Jason Weston,\nL´eon Bottou, Michael Karlen, Koray Kavukcuoglu,\nand Pavel Kuksa. 2011. Natural language processing\n(almost) from scratch. JMLR, 12:2493–2537, Novem-\nber.\n[Deerwester et al.1990] Scott Deerwester, Susan T. Du-\nmais, George W. Furnas, Thomas K. Landauer, and\nRichard Harshman. 1990. Indexing by latent semantic\nanalysis. Journal of the American society for informa-\ntion science, 41(6):391–407.\n[Dhillon et al.2011] Paramveer S. Dhillon, Dean Foster,\nand Lyle Ungar. 2011. Multi-view learning of word\nembeddings via CCA. In NIPS, volume 24.\n[Dyer et al.2010] Chris Dyer, Adam Lopez, Juri Gan-\nitkevitch, Johnathan Weese, Ferhan Ture, Phil Blun-\nsom, Hendra Setiawan, Vladimir Eidelman, and Philip\nResnik. 2010. cdec: A decoder, alignment, and learn-\ning framework for ﬁnite-state and context-free transla-\ntion models. In Proc. of ACL.\n[Gauvain and Lee1994] J. Gauvain and Chin-Hui Lee.\n1994. Maximum a posteriori estimation for multivari-\nate Gaussian mixture observations of Markov chains.\nSpeech and Audio Processing, IEEE Transactions on,\n2(2):291–298, Apr.\n[Hopper and Thompson1983] Paul Hopper and Sandra\nThompson. 1983. The iconicity of the universal cat-\negories “noun” and “verb”. In John Haiman, editor,\nIconicity in Syntax: Proceedings of a symposium on\niconicity in syntax.\n[Johnson2007] Mark Johnson. 2007. Why doesn’t EM\nﬁnd good HMM POS-taggers? In Proc. of EMNLP.\n[Kupiec1992] Julian Kupiec.\n1992.\nRobust part-of-\nspeech tagging using a hidden Markov model. Com-\nputer Speech and Language, 6:225–242.\n[Ling et al.2015] Wang Ling, Chris Dyer, Alan Black, and\nIsabel Trancoso. 2015. Two/too simple adaptations of\nword2vec for syntax problems. In Proc. of NAACL.\n[Mikolov et al.2011] Tomas Mikolov, Stefan Kombrink,\nAnoop Deoras, Lukar Burget, and J Cernocky. 2011.\nRNNLM — recurrent neural network language mod-\neling toolkit. In Proc. of the 2011 ASRU Workshop,\npages 196–201.\n[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg\nCorrado, and Jeffrey Dean. 2013. Efﬁcient estima-\ntion of word representations in vector space. ArXiv\ne-prints, January.\n[Nivre et al.2007] Joakim Nivre,\nJohan Hall,\nSandra\nKubler, Ryan McDonald, Jens Nilsson, Sebastian\nRiedel, and Deniz Yuret.\n2007.\nThe CoNLL 2007\nshared task on dependency parsing.\nIn Proc. of\nCoNLL.\n[Petrov et al.2012] Slav Petrov, Dipanjan Das, and Ryan\nMcDonald. 2012. A universal part-of-speech tagset.\nIn Proc. of LREC, May.\n[Quasthoff et al.2006] Uwe Quasthoff, Matthias Richter,\nand Christian Biemann.\n2006.\nCorpus portal for\nsearch in monolingual corpora.\nIn Proc. of LREC,\npages 1799–1802.\n[Rosenberg and Hirschberg2007] Andrew Rosenberg and\nJulia Hirschberg.\n2007.\nV-measure: A conditional\nentropy-based external cluster evaluation measure. In\nEMNLP-CoNLL.\n[Shinozaki and Kawahara2007] T.\nShinozaki\nand\nT. Kawahara.\n2007.\nHMM training based on\nCV-EM and CV gaussian mixture optimization.\nIn\nProc. of the 2007 ASRU Workshop, pages 318–322,\nDec.\n[Sirts et al.2014] Kairit Sirts, Jacob Eisenstein, Micha El-\nsner, and Sharon Goldwater.\n2014.\nPOS induction\nwith distributional and morphological information us-\ning a distance-dependent Chinese restaurant process.\nIn Proc. of ACL.\n[Spiegler et al.2010] Sebastian Spiegler, Andrew van der\nSpuy, and Peter A. Flach. 2010. Ukwabelana: An\nopen-source morphological Zulu corpus. In Proc. of\nCOLING, pages 1020–1028.\n[Yatbaz et al.2014] Mehmet Ali Yatbaz, Enis Rıfat Sert,\nand Deniz Yuret. 2014. Unsupervised instance-based\npart of speech induction using probable substitutes. In\nProc. of COLING.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2015-03-23",
  "updated": "2015-03-23"
}