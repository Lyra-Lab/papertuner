{
  "id": "http://arxiv.org/abs/1504.00641v1",
  "title": "A Probabilistic Theory of Deep Learning",
  "authors": [
    "Ankit B. Patel",
    "Tan Nguyen",
    "Richard G. Baraniuk"
  ],
  "abstract": "A grand challenge in machine learning is the development of computational\nalgorithms that match or outperform humans in perceptual inference tasks that\nare complicated by nuisance variation. For instance, visual object recognition\ninvolves the unknown object position, orientation, and scale in object\nrecognition while speech recognition involves the unknown voice pronunciation,\npitch, and speed. Recently, a new breed of deep learning algorithms have\nemerged for high-nuisance inference tasks that routinely yield pattern\nrecognition systems with near- or super-human capabilities. But a fundamental\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\nfor understanding, analyzing, and synthesizing deep learning architectures has\nremained elusive. We answer this question by developing a new probabilistic\nframework for deep learning based on the Deep Rendering Model: a generative\nprobabilistic model that explicitly captures latent nuisance variation. By\nrelaxing the generative model to a discriminative one, we can recover two of\nthe current leading deep learning systems, deep convolutional neural networks\nand random decision forests, providing insights into their successes and\nshortcomings, as well as a principled route to their improvement.",
  "text": "A Probabilistic Theory of Deep Learning\nAnkit B. Patel, Tan Nguyen, Richard G. Baraniuk\nDepartment of Electrical and Computer Engineering\nRice University\n{abp4, mn15, richb}@rice.edu\nApril 2, 2015\nA grand challenge in machine learning is the development of computational al-\ngorithms that match or outperform humans in perceptual inference tasks such\nas visual object and speech recognition. The key factor complicating such tasks\nis the presence of numerous nuisance variables, for instance, the unknown\nobject position, orientation, and scale in object recognition or the unknown\nvoice pronunciation, pitch, and speed in speech recognition. Recently, a new\nbreed of deep learning algorithms have emerged for high-nuisance inference\ntasks; they are constructed from many layers of alternating linear and nonlin-\near processing units and are trained using large-scale algorithms and massive\namounts of training data. The recent success of deep learning systems is im-\npressive — they now routinely yield pattern recognition systems with near-\nor super-human capabilities — but a fundamental question remains: Why do\nthey work? Intuitions abound, but a coherent framework for understanding,\nanalyzing, and synthesizing deep learning architectures has remained elusive.\nWe answer this question by developing a new probabilistic framework for deep\nlearning based on a Bayesian generative probabilistic model that explicitly cap-\ntures variation due to nuisance variables. The graphical structure of the model\nenables it to be learned from data using classical expectation-maximization\ntechniques. Furthermore, by relaxing the generative model to a discriminative\none, we can recover two of the current leading deep learning systems, deep\nconvolutional neural networks (DCNs) and random decision forests (RDFs),\nproviding insights into their successes and shortcomings as well as a princi-\npled route to their improvement.\n1\narXiv:1504.00641v1  [stat.ML]  2 Apr 2015\nContents\n1\nIntroduction\n4\n2\nA Deep Probabilistic Model for Nuisance Variation\n5\n2.1\nThe Rendering Model: Capturing Nuisance Variation . . . . . . . . . . . . . .\n6\n2.2\nDeriving the Key Elements of One Layer of a Deep Convolutional\nNetwork from the Rendering Model . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3\nThe Deep Rendering Model: Capturing Levels of Abstraction . . . . . . . . . .\n12\n2.4\nInference in the Deep Rendering Model . . . . . . . . . . . . . . . . . . . . .\n15\n2.4.1\nWhat About the SoftMax Regression Layer?\n. . . . . . . . . . . . . .\n17\n2.5\nDCNs are Probabilistic Message Passing Networks . . . . . . . . . . . . . . .\n17\n2.5.1\nDeep Rendering Model and Message Passing . . . . . . . . . . . . . .\n17\n2.5.2\nA Uniﬁcation of Neural Networks and Probabilistic Inference\n. . . . .\n18\n2.5.3\nThe Probabilistic Role of Max-Pooling\n. . . . . . . . . . . . . . . . .\n18\n2.6\nLearning the Rendering Models\n. . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.6.1\nEM Algorithm for the Shallow Rendering Model . . . . . . . . . . . .\n20\n2.6.2\nEM Algorithm for the Deep Rendering Model . . . . . . . . . . . . . .\n21\n2.6.3\nWhat About DropOut Training? . . . . . . . . . . . . . . . . . . . . .\n23\n2.7\nFrom Generative to Discriminative Classiﬁers . . . . . . . . . . . . . . . . . .\n23\n2.7.1\nTransforming a Generative Classiﬁer into a Discriminative One\n. . . .\n24\n2.7.2\nFrom the Deep Rendering Model to Deep Convolutional Networks . . .\n26\n3\nNew Insights into Deep Convolutional Networks\n27\n3.1\nDCNs Possess Full Probabilistic Semantics\n. . . . . . . . . . . . . . . . . . .\n27\n3.2\nClass Appearance Models and Activity Maximization . . . . . . . . . . . . . .\n27\n3.3\n(Dis)Entanglement: Supervised Learning of Task Targets Is\nIntertwined with Unsupervised Learning of Latent Task Nuisances . . . . . . .\n30\n4\nFrom the Deep Rendering Model to Random Decision Forests\n30\n4.1\nThe Evolutionary Deep Rendering Model: A Hierarchy of Categories\n. . . . .\n32\n4.2\nInference with the E-DRM Yields a Decision Tree . . . . . . . . . . . . . . . .\n33\n4.2.1\nWhat About the Leaf Histograms? . . . . . . . . . . . . . . . . . . . .\n34\n4.3\nBootstrap Aggregation to Prevent Overﬁtting Yields A Decision\nForest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.4\nEM Learning for the E-DRM Yields the InfoMax Principle . . . . . . . . . . .\n36\n2\n5\nRelation to Prior Work\n36\n5.1\nRelation to Mixture of Factor Analyzers . . . . . . . . . . . . . . . . . . . . .\n36\n5.2\ni-Theory: Invariant Representations Inspired by Sensory Cortex\n. . . . . . . .\n37\n5.3\nScattering Transform: Achieving Invariance via Wavelets . . . . . . . . . . . .\n38\n5.4\nLearning Deep Architectures via Sparsity\n. . . . . . . . . . . . . . . . . . . .\n38\n5.5\nGoogle FaceNet: Learning Useful Representations with DCNs . . . . . . . . .\n39\n5.6\nRenormalization Theory\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.7\nSummary of Key Distinguishing Features of the DRM\n. . . . . . . . . . . . .\n40\n6\nNew Directions\n41\n6.1\nMore Realistic Rendering Models\n. . . . . . . . . . . . . . . . . . . . . . . .\n41\n6.2\nNew Inference Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n6.2.1\nSoft Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n6.2.2\nTop-Down Convolutional Nets: Top-Down Inference via the DRM . . .\n43\n6.3\nNew Learning Algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n6.3.1\nDerivative-Free Learning . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n6.3.2\nDynamics: Learning from Video . . . . . . . . . . . . . . . . . . . . .\n44\n6.3.3\nTraining from Labeled and Unlabeled Data . . . . . . . . . . . . . . .\n44\nA Supplemental Information\n46\nA.1\nFrom the Gaussian Rendering Model Classiﬁer to Deep DCNs . . . . . . . . .\n46\nA.2\nGeneralizing to Arbitrary Mixtures of Exponential Family\nDistributions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nA.3\nRegularization Schemes: Deriving the DropOut Algorithm . . . . . . . . . . .\n49\n3\n1\nIntroduction\nHumans are expert at a wide array of complicated sensory inference tasks, from recognizing\nobjects in an image to understanding phonemes in a speech signal, despite signiﬁcant variations\nsuch as the position, orientation, and scale of objects and the pronunciation, pitch, and volume\nof speech. Indeed, the main challenge in many sensory perception tasks in vision, speech, and\nnatural language processing is a high amount of such nuisance variation. Nuisance variations\ncomplicate perception, because they turn otherwise simple statistical inference problems with\na small number of variables (e.g., class label) into much higher-dimensional problems. For ex-\nample, images of a car taken from different camera viewpoints lie on a highly curved, nonlinear\nmanifold in high-dimensional space that is intertwined with the manifolds of myriad other ob-\njects. The key challenge in developing an inference algorithm is then how to factor out all of\nthe nuisance variation in the input. Over the past few decades, a vast literature that approaches\nthis problem from myriad different perspectives has developed, but the most difﬁcult inference\nproblems have remained out of reach.\nRecently, a new breed of machine learning algorithms have emerged for high-nuisance in-\nference tasks, resulting in pattern recognition systems with sometimes super-human capabili-\nties (1). These so-called deep learning systems share two common hallmarks. First, architec-\nturally, they are constructed from many layers of alternating linear and nonlinear processing\nunits. Second, computationally, their parameters are learned using large-scale algorithms and\nmassive amounts of training data. Two examples of such architectures are the deep convolu-\ntional neural network (DCN), which has seen great success in tasks like visual object recogni-\ntion and localization (2), speech recognition (3), and part-of-speech recognition (4), and random\ndecision forests (RDFs) (5) for image segmentation. The success of deep learning systems is\nimpressive, but a fundamental question remains: Why do they work? Intuitions abound to ex-\nplain their success. Some explanations focus on properties of feature invariance and selectivity\ndeveloped over multiple layers, while others credit raw computational power and the amount\nof available training data (1). However, beyond these intuitions, a coherent theoretical frame-\nwork for understanding, analyzing, and synthesizing deep learning architectures has remained\nelusive.\nIn this paper, we develop a new theoretical framework that provides insights into both the\nsuccesses and shortcomings of deep learning systems, as well as a principled route to their de-\nsign and improvement. Our framework is based on a generative probabilistic model that explic-\nitly captures variation due to latent nuisance variables. The Rendering Model (RM) explicitly\nmodels nuisance variation through a rendering function that combines the task-speciﬁc vari-\nables of interest (e.g., object class in an object recognition task) and the collection of nuisance\nvariables. The Deep Rendering Model (DRM) extends the RM in a hierarchical fashion by ren-\n4\ndering via a product of afﬁne nuisance transformations across multiple levels of abstraction. The\ngraphical structures of the RM and DRM enable inference via message passing, using, for ex-\nample, the sum-product or max-sum algorithms, and training via the expectation-maximization\n(EM) algorithm. A key element of the framework is the relaxation of the RM/DRM generative\nmodel to a discriminative one in order to optimize the bias-variance tradeoff.\nThe DRM unites and subsumes two of the current leading deep learning based systems as\nmax-sum message passing networks. That is, conﬁguring the DRM with two different nuisance\nstructures — Gaussian translational nuisance or evolutionary additive nuisance — leads directly\nto DCNs and RDFs, respectively. The intimate connection between the DRM and these deep\nlearning systems provides a range of new insights into how and why they work, answering\nseveral open questions. Moreover, the DRM provides insights into how and why deep learning\nfails and suggests pathways to their improvement.\nIt is important to note that our theory and methods apply to a wide range of different in-\nference tasks (including, for example, classiﬁcation, estimation, regression, etc.) that feature a\nnumber of task-irrelevant nuisance variables (including, for example, object and speech recog-\nnition). However, for concreteness of exposition, we will focus below on the classiﬁcation\nproblem underlying visual object recognition.\nThis paper is organized as follows. Section 2 introduces the RM and DRM and demonstrates\nstep-by-step how they map onto DCNs. Section 3 then summarizes some of the key insights\nthat the DRM provides into the operation and performance of DCNs. Section 4 proceeds in a\nsimilar fashion to derive RDFs from a variant of the DRM that models a hierarchy of categories.\nSection 6 closes the paper by suggesting a number of promising avenues for research, including\nseveral that should lead to improvement in deep learning system performance and generality.\nThe proofs of several results appear in the Appendix.\n2\nA Deep Probabilistic Model for Nuisance Variation\nThis section develops the RM, a generative probabilistic model that explicitly captures nuisance\ntransformations as latent variables. We show how inference in the RM corresponds to oper-\nations in a single layer of a DCN. We then extend the RM by deﬁning the DRM, a rendering\nmodel with layers representing different scales or levels of abstraction. Finally, we show that,\nafter the application of a discriminative relaxation, inference and learning in the DRM corre-\nspond to feedforward propagation and back propagation training in the DCN. This enables us to\nconclude that DCNs are probabilistic message passing networks, thus unifying the probabilistic\nand neural network perspectives.\n5\n2.1\nThe Rendering Model: Capturing Nuisance Variation\nVisual object recognition is naturally formulated as a statistical classiﬁcation problem.1 We\nare given a D-pixel, multi-channel image I of an object, with intensity I(x, ω) at pixel x and\nchannel ω (e.g., ω ={red, green, blue}). We seek to infer the object’s identity (class) c ∈C,\nwhere C is a ﬁnite set of classes.2 We will use the terms “object” and “class” interchangeably.\nGiven a joint probabilistic model p(I, c) for images and objects, we can classify a particular\nimage I using the maximum a posteriori (MAP) classiﬁer\nˆc(I) = argmax\nc∈C\np(c|I) = argmax\nc∈C\np(I|c)p(c),\n(1)\nwhere p(I|c) is the image likelihood, p(c) is the prior distribution over the classes, and p(c|I) ∝\np(I|c)p(c) by Bayes’ rule.\nObject recognition, like many other inference tasks, is complicated by a high amount of\nvariation due to nuisance variables, which the above formation ignores. We advocate explicitly\nmodeling nuisance variables by encapsulating all of them into a (possibly high-dimensional)\nparameter g ∈G, where G is the set of all nuisances. In some cases, it is natural for g to be a\ntransformation and for G to be endowed with a (semi-)group structure.\nWe now propose a generative model for images that explicitly models the relationship be-\ntween images I of the same object c subject to nuisance g. First, given c, g, and other auxiliary\nparameters, we deﬁne the rendering function R(c, g) that renders (produces) an image. In image\ninference problems, for example, R(c, g) might be a photorealistic computer graphics engine\n(c.f., Pixar). A particular realization of an image is then generated by adding noise to the output\nof the renderer:\nI|c, g = R(c, g) + noise.\n(2)\nWe assume that the noise distribution is from the exponential family, which includes a large\nnumber of practically useful distributions (e.g., Gaussian, Poisson). Also we assume that the\nnoise is independent and identically distributed (iid) as a function of pixel location x and that\nthe class and nuisance variables are independently distributed according to categorical distri-\nbutions.3 With these assumptions, Eq. 2 then becomes the probabilistic (shallow) Rendering\n1Recall that we focus on object recognition from images only for concreteness of exposition.\n2The restriction for C to be ﬁnite can be removed by using a nonparametric prior such as a Chinese Restaurant\nProcess (CRP) (6)\n3Independence is merely a convenient approximation; in practice, g can depend on c. For example, humans\nhave difﬁculty recognizing and discriminating upside down faces (7).\n6\nModel (RM)\nc ∼Cat({πc}c∈C),\ng ∼Cat({πg}g∈G),\nI|c, g ∼Q(θcg).\n(3)\nHere Q(θcg) denotes a distribution from the exponential family with parameters θcg, which\ninclude the mixing probabilities πcg, natural parameters η(θcg), sufﬁcient statistics T(I) and\nwhose mean is the rendered template µcg = R(c, g).\nAn important special case is when Q(θcg) is Gaussian, and this deﬁnes the Gaussian Ren-\ndering Model (GRM), in which images are generated according to\nI|c, g ∼N(I|µcg = R(c, g), Σcg = σ21),\n(4)\nwhere 1 is the identity matrix. The GRM generalizes both the Gaussian Na¨ıve Bayes Classiﬁer\n(GNBC) and the Gaussian Mixture Model (GMM) by allowing variation in the image to depend\non an observed class label c, like a GNBC, and on an unobserved nuisance label g, like a GMM.\nThe GNBC, GMM and the (G)RM can all be conveniently described as a directed graphical\nmodel (8). Figure 1A depicts the graphical models for the GNBC and GMM, while Fig. 1B\nshows how they are combined to form the (G)RM.\nFinally, since the world is spatially varying and an image can contain a number of different\nobjects, it is natural to break the image up into a number of (overlapping) subimages, called\npatches, that are indexed by spatial location x. Thus, a patch is deﬁned here as a collection of\npixels centered on a single pixel x. In general, patches can overlap, meaning that (i) they do not\ntile the image, and (ii) an image pixel x can belong to more than one patch. Given this notion of\npixels and patches, we allow the class and nuisance variables to depend on pixel/patch location:\ni.e., local image class c(x) and local nuisance g(x) (see Fig. 2A). We will omit the dependence\non x when it is clear from context.\nThe notion of a rendering operator is quite general and can refer to any function that maps\na target variable c and nuisance variables g into a pattern or template R(c, g). For example, in\nspeech recognition, c might be a phoneme, in which case g represents volume, pitch, speed, and\naccent, and R(c, g) is the amplitude of the acoustic signal (or alternatively the time-frequency\nrepresentation). In natural language processing, c might be the grammatical part-of-speech, in\nwhich case g represents syntax and grammar, and R(c, g) is a clause, phrase or sentence.\nTo perform object recognition with the RM via Eq. 1, we must marginalize out the nuisance\nvariables g. We consider two approaches for doing so, one conventional and one unconven-\ntional. The Sum-Product RM Classiﬁer (SP-RMC) sums over all nuisance variables g ∈G and\n7\nI\nμL\nμL–1\nμ1\n...\ngL\ngL–1\ng1\nA\nC\nB\nI\nc\ng\nI\nI\nc\ng\nNaive Bayes \nClassifier\nMixture \nModel\nRendering \nModel\nDeep Rendering \nModel\nFigure 1. Graphical depiction of the Naive Bayes Classiﬁer (A, left), Gaussian Mixture Model (A, right),\nthe shallow Rendering Model (B) and the Deep Rendering Model (C). All dependence on pixel location\nx has been suppressed for clarity.\nthen chooses the most likely class:\nˆcSP(I) = argmax\nc∈C\n1\n|G|\nX\ng∈G\np(I|c, g)p(c)p(g)\n= argmax\nc∈C\n1\n|G|\nX\ng∈G\nexp⟨η(θcg)|T(I)⟩,\n(5)\nwhere ⟨·|·⟩is the bra-ket notation for inner products and in the last line we have used the def-\ninition of an exponential family distribution. Thus the SP-RM computes the marginal of the\nposterior distribution over the target variable, given the input image. This is the conventional\napproach used in most probabilistic modeling with latent variables.\nAn alternative and less conventional approach is to use the Max-Sum RM Classiﬁer (MS-\nRMC), which maximizes over all g ∈G and then chooses the most likely class:\nˆcMS(I) = argmax\nc∈C\nmax\ng∈G p(I|c, g)p(c)p(g)\n= argmax\nc∈C\nmax\ng∈G ⟨η(θcg)|T(I)⟩.\n(6)\nThe MS-RMC computes the mode of the posterior distribution over the target and nuisance\nvariables, given the input image. Equivalently, it computes the most likely global conﬁguration\nof target and nuisance variables for the image. Intuitively, this is an effective strategy when\nthere is one explanation g∗∈G that dominates all other explanations g ̸= g∗. This condition\n8\nis justiﬁed in settings where the rendering function is deterministic or nearly noise-free. This\napproach to classiﬁcation is unconventional in both the machine learning and computational\nneuroscience literatures, where the sum-product approach is most commonly used, although it\nhas received some recent attention (9).\nBoth the sum-product and max-sum classiﬁers amount to applying an afﬁne transformation\nto the input image I (via an inner product that performs feature detection via template match-\ning), followed by a sum or max nonlinearity that marginalizes over the nuisance variables.\nThroughout the paper we will assume isotropic or diagonal Gaussian noise for simplicity,\nbut the treatment presented here can be generalized to any distribution from the exponential\nfamily in a straightforward manner. Note that such an extension may require a non-linear trans-\nformation (i.e. quadratic or logarithmic T(I)), depending on the speciﬁc exponential family.\nPlease see Supplement Section A.2 for more details.\n2.2\nDeriving the Key Elements of One Layer of a Deep Convolutional\nNetwork from the Rendering Model\nHaving formulated the Rendering Model (RM), we now show how to connect the RM with deep\nconvolutional networks (DCNs). We will see that the MS-RMC (after imposing a few additional\nassumptions on the RM) gives rise to most commonly used DCN layer types.\nOur ﬁrst assumption is that the noise added to the rendered template is isotropically Gaus-\nsian (GRM) i.e. each pixel has the same noise variance σ2 that is independent of the conﬁg-\nuration (c, g). Then, assuming the image is normalized ∥I∥2 = 1, Eq. 6 yields the max-sum\nGaussian RM classiﬁer (see Appendix A.1 for a detailed proof)\nˆcMS(I) = argmax\nc∈C\nmax\ng∈G\n\u001c 1\nσ2µcg\n\f\f\fI\n\u001d\n−\n1\n2σ2∥µcg∥2\n2 + ln πcπg\n≡argmax\nc∈C\nmax\ng∈G ⟨wcg|I⟩+ bcg,\n(7)\nwhere we have deﬁned the natural parameters η ≡{wcg, bcg} in terms of the traditional pa-\nrameters θ ≡{σ2, µcg, πc, πg} according to4\nwcg ≡1\nσ2µcg = 1\nσ2R(c, g)\nbcg ≡\n1\n2σ2∥µcg∥2\n2 + ln πcπg.\n(8)\nNote that we have suppressed the parameters’ dependence on pixel location x.\n4Since the Gaussian distribution of the noise is in the exponential family, it can be reparametrized in terms of\nthe natural parameters. This is known as canonical form.\n9\nWe will now demonstrate that the sequence of operations in the MS-RMC in Eq. 7 coincides\nexactly with the operations involved in one layer of a DCN (or, more generally, a max-out neural\nnetwork (10)): image normalization, linear template matching, thresholding, and max pooling.\nSee Fig. 2C. We now explore each operation in Eq. 7 in detail to make the link precise.\nFirst, the image is normalized. Until recently, there were several different types of normal-\nization typically employed in DCNs: local response normalization, and local contrast normal-\nization (11,12). However, the most recent highly performing DCNs employ a different form of\nnormalization, known as batch-normalization (13). We will come back to this later when we\nshow how to derive batch normalization from a principled approach. One implication of this is\nthat it is unclear what probabilistic assumption the older forms of normalization arise from, if\nany.\nSecond, the image is ﬁltered with a set of noise-scaled rendered templates wcg. The size\nof the templates depends on the size of the objects of class c and the values of the nuisance\nvariables g. Large objects will have large templates, corresponding to a fully connected layer in\na DCN (14), while small objects will have small templates, corresponding to a locally connected\nlayer in a DCN (15). If the distribution of objects depends on the pixel position x (e.g., cars are\nmore likely on the ground while planes are more likely in the air) then, in general, we will need\ndifferent rendered templates at each x. In this case, the locally connected layer is appropriate.\nIf, on the other hand, all objects are equally likely to be present at all locations throughout the\nentire image, then we can assume translational invariance in the RM. This yields a global set of\ntemplates that are used at all pixels x, corresponding to a convolutional layer in a DCN (14) (see\nAppendix A.2 for a detailed proof). If the ﬁlter sizes are large relative to the scale at which the\nimage variation occurs and the ﬁlters are overcomplete, then adjacent ﬁlters overlap and waste\ncomputation. In these case, it is appropriate to use a strided convolution, where the output of the\ntraditional convolution is down-sampled by some factor; this saves some computation without\nlosing information.\nThird, the resulting activations (log-probabilities of the hypotheses) are passed through a\npooling layer; i.e., if g is a translational nuisance, then taking the maximum over g corresponds\nto max pooling in a DCN.\nFourth, recall that a given image pixel x will reside in several overlapping image patches,\neach rendered by its own parent class c(x) and the nuisance location g(x) (Fig. 2A). Thus\nwe must consider the possibility of collisions: i.e. when two different parents c(x1) ̸= c(x2)\nmight render the same pixel (or patch). To avoid such undesirable collisions, it is natural to\nforce the rendering to be locally sparse: i.e. we must enforce that only one renderer in a local\nneighborhood can be “active”.\nTo formalize this, we endow each parent renderer with an ON/OFF state via a switching\nvariable a ∈A ≡{ON, OFF}. If a = ON = 1, then the rendered image patch is left untouched,\n10\n1\t\r  \n2\t\r  \n3\t\r  \n4\t\r  \ngl+1\nON\nOFF al+1\n1\t\r  \n2\t\r  \n4\t\r  \n3\t\r  \nxl\ncl\nxl+1\ncl+1\nµ\n!\ncl+1\"\nµ\n!\ncl\"\n= ⇤l+1 !\ngl+1\"\nµ\n!\ncl+1\"\nnoise\n...\n1\t\r  \n2\t\r  \n4\t\r  \n3\t\r  \nxl\ncl\nxl+1\ncl+1\n...\nDCN Convolution\nMaxPool + RELU\nInput\nFeature Map\nOutput\nFeature Map\nIl+1\nInference \nDiscriminative  \nCounterpart \nProbabilistic Model \n(Deep Rendering Model) \nA\nFactor Graph \n(Inference via Max-Sum) \nB\nNeural Network \n(Deep Convolutional \nNetwork) \nC\n1\t\r  \n2\t\r  \n4\t\r  \n3\t\r  \nxl\ncl\nxl+1\ncl+1\nmax\ngl+1 {·}\nµ\n!\ncl+1\"\n|µ\n!\ncl\"\n, gl+1\n...\nMax −Sum\nMessage\nMax −Sum\nMessage\nFactor\nIl+1\nhW l+1|·i\n1\t\r  \n2\t\r  \n3\t\r  \n4\t\r  \ngl+1\nON\nOFF al+1\nIl\nIl\nRendering\nInference\nInference\n...\n...\n...\nI0\nI0\nI0\nFigure 2. An example of mapping from the Deep Rendering Model (DRM) to its corresponding factor\ngraph to a Deep Convolutional Network (DCN) showing only the transformation from level ℓof the\nhierarchy of abstraction to level ℓ+ 1. (A) DRM generative model: a single super pixel xℓ+1 at level\nℓ+ 1 (green, upper) renders down to a 3 × 3 image patch at level ℓ(green, lower), whose location\nis speciﬁed by gℓ+1 (red). (B) Factor graph representation of the DRM model that supports efﬁcient\ninference algorithms such as the max-sum message passing shown here. (C) Computational network that\nimplements the max-sum message passing algorithm from (B) explicitly; its structure exactly matches\nthat of a DCN.\n11\nwhereas if a = OFF = 0, the image patch is masked with zeros after rendering. Thus, the\nswitching variable a models (in)active parent renderers.\nHowever, these switching variables have strong correlations due to the crowding out effect:\nif one is ON, then its neighbors must be OFF in order to prevent rendering collisions. Although\nnatural for realistic rendering, this complicates inference. Thus, we employ an approxima-\ntion by instead assuming that the state of each renderer ON or OFF completely at random and\nthus independent of any other variables, including the measurements (i.e., the image itself).\nOf course, an approximation to real rendering, but it simpliﬁes inference, and leads directly\nto rectiﬁed linear units, as we show below. Such approximations to true sparsity have been\nextensively studied, and are known as spike-and-slab sparse coding models (16,17).\nSince the switching variables are latent (unobserved), we must max-marginalize over them\nduring classiﬁcation, as we did with nuisance variables g in the last section (one can think of a\nas just another nuisance). This leads to (see Appendix A.3 for a more detailed proof)\nˆc(I) = argmax\nc∈C\nmax\ng∈G max\na∈A\n\u001c 1\nσ2aµcg\n\f\f\fI\n\u001d\n−\n1\n2σ2(∥aµcg∥2\n2 + ∥I∥2\n2)) + ln πcπgπa\n≡argmax\nc∈C\nmax\ng∈G max\na∈A a(⟨wcg|I⟩+ bcg) + bcga\n= argmax\nc∈C\nmax\ng∈G ReLU (⟨wcg|I⟩+ bcg) ,\n(9)\nwhere bcga and bcg are bias terms and ReLu(u) ≡(u)+ = max{u, 0} denotes the soft-thresholding\noperation performed by the Rectiﬁed Linear Units (ReLU) in modern DCNs (18). In the last\nline, we have assumed that the prior πcg is uniform so that bcga is independent of c and g and\ncan be dropped.\n2.3\nThe Deep Rendering Model: Capturing Levels of Abstraction\nThe world is summarizable at varying levels of abstraction, and Hierarchical Bayesian Models\n(HBMs) can exploit this fact to accelerate learning. In particular, the power of abstraction allows\nthe higher levels of an HBM to learn concepts and categories far more rapidly than lower levels,\ndue to stronger inductive biases and exposure to more data (19). This is informally known as\nthe Blessing of Abstraction (19). In light of these beneﬁts, it is natural for us to extend the RM\ninto an HBM, thus giving it the power to summarize data at different levels of abstraction.\nIn order to illustrate this concept, consider the example of rendering an image of a face, at\ndifferent levels of detail ℓ∈{L, L−1, . . . , 0}. At level ℓ= L (the coarsest level of abstraction),\nwe specify only the identity of the face cL and its overall location and pose gL without specifying\nany ﬁner-scale details such as the locations of the eyes or type of facial expression. At level\nℓ= L −1, we specify ﬁner-grained details, such as the existence of a left eye (cL−1) with a\n12\nFigure 3. This sculpture by Henri Matisse illustrates the Deep Rendering Model (DRM). The sculpture\nin the leftmost panel is analogous to a fully rendered image at the lowest abstraction level ℓ= 0. Moving\nfrom left to right, the sculptures become progressively more abstract, until the in the rightmost panel we\nreach the highest abstraction level ℓ= 3. The ﬁner-scale details in the ﬁrst three panels that are lost\nin the fourth are the nuisance parameters g, whereas the coarser-scale details in the last panel that are\npreserved are the target c.\ncertain location, pose, and state (e.g., gL−1 = open or closed), again without specifying any\nﬁner-scale parameters (such as eyelash length or pupil shape). We continue in this way, at\neach level ℓadding ﬁner-scaled information that was unspeciﬁed at level ℓ−1, until at level\nℓ= 0 we have fully speciﬁed the image’s pixel intensities, leaving us with the fully rendered,\nmulti-channel image I0(xℓ, ωℓ). Here xℓrefers to a pixel location at level ℓ.\nFor another illustrative example, consider The Back Series of sculptures by the artist Henri\nMatisse (Fig. 3). As one moves from left to right, the sculptures become increasingly abstract,\nlosing low-level features and details, while preserving high-level features essential for the over-\nall meaning: i.e. (cL, gL) = “woman with her back facing us.” Conversely, as one moves from\nright to left, the sculptures become increasingly concrete, progressively gaining ﬁner-scale de-\ntails (nuisance parameters gℓ, ℓ= L−1, . . . , 0) and culminating in a rich and textured rendering.\nWe formalize this process of progressive rendering by deﬁning the Deep Rendering Model\n(DRM). Analogous to the Matisse sculptures, the image generation process in a DRM starts at\nthe highest level of abstraction (ℓ= L), with the random choice of the object class cL and overall\npose gL. It is then followed by generation of the lower-level details gℓ, and a progressive level-\nby-level (ℓ→ℓ−1) rendering of a set of intermediate rendered “images” µℓ, each with more\ndetailed information. The process ﬁnally culminates in a fully rendered D0 ≡D-dimensional\n13\nimage µ0 = I0 ≡I (ℓ= 0). Mathematically,\ncL ∼Cat(π(cL)),\ncL ∈CL,\ngℓ+1 ∼Cat(π(gℓ+1)),\ngℓ+1 ∈Gℓ+1,\nℓ= L −1, L −2, . . . , 0\nµ(cL, g) = Λ(g)µ(cL) ≡Λ1(g1) · · · ΛL(gL) · µ(cL),\ng = {gℓ}L\nℓ=1\nI(cL, g) = µ(cL, g) + N(0, σ21D) ∈RD.\n(10)\nHere Cℓ, Gℓare the sets of all target-relevant and target-irrelevant nuisance variables at level\nℓ, respectively. The rendering path is deﬁned as the sequence (cL, gL, . . . , gℓ, . . . , g1) from\nthe root (overall class) down to the individual pixels at ℓ= 0. µ(cL) is an abstract template\nfor the high-level class cL, and Λ(g) ≡Q\nℓΛℓ(gℓ) represents the sequence of local nuisance\ntransformations that renders ﬁner-scale details as one moves from abstract to concrete. Note\nthat each Λℓ(gℓ) is an afﬁne transformation with a bias term α(gℓ) that we have suppressed\nfor clarity5. Figure 2A illustrates the corresponding graphical model. As before, we have\nsuppressed the dependence of cℓ, gℓon the pixel location xℓat level ℓof the hierarchy.\nWe can cast the DRM into an incremental form by deﬁning an intermediate class cℓ≡\n(cL, gL, . . . , gℓ+1) that intuitively represents a partial rendering path up to level ℓ. Then, the\npartial rendering from level ℓ+ 1 to ℓcan be written as an afﬁne transformation\nµ(cℓ) = Λℓ+1(gℓ+1) · µ(cℓ+1) + α(gℓ+1) + N(0, Ψℓ+1),\n(11)\nwhere we have shown the bias term α explicitly and introduced noise6 with a diagonal co-\nvariance Ψℓ+1. It is important to note that cℓ, gℓcan correspond to different kinds of target\nrelevant and irrelevant features at different levels. For example, when rendering faces, c1(x1)\nmight correspond to different edge orientations and g1(x1) to different edge locations in patch\nx1, whereas c2(x2) might correspond to different eye types and g2(x2) to different eye gaze\ndirections in patch x2.\nThe DRM generates images at intermediate abstraction levels via the incremental rendering\nfunctions in Eq. 11 (see Fig. 2A). Hence the complete rendering function R(c, g) from Eq. 2\nis a composition of incremental rendering functions, amounting to a product of afﬁne trans-\nformations as in Eq. 10. Compared to the shallow RM, the factorized structure of the DRM\nresults in an exponential reduction in the number of free parameters, from D0 |CL| Q\nℓ|Gℓ| to\n|CL| P\nℓDℓ|Gℓ| where Dℓis the number of pixels in the intermediate image µℓ, thus enabling\nmore efﬁcient inference and learning, and most importantly, better generalization.\n5This assumes that we are using an exponential family with linear sufﬁcient statistics i.e. T(I) = (I, 1)T .\nHowever, note that the family we use here is not Gaussian, it is instead a Factor Analyzer, a different probabilistic\nmodel.\n6We introduce noise for two reasons: (1) it will make it easier to connect later to existing EM algorithms for\nfactor analyzers and (2) we can always take the noise-free limit to impose cluster well-separatedness if needed.\nIndeed, if the rendering process is deterministic or nearly noise-free, then the latter is justiﬁed.\n14\nThe DRM as formulated here is distinct from but related to several other hierarchical models,\nsuch as the Deep Mixture of Factor Analyzers (DMFA) (20) and the Deep Gaussian Mixture\nModel (21), both of which are essentially compositions of another model — the Mixture of\nFactor Analyzers (MFA) (22). We will highlight the similarities and differences with these\nmodels in more detail in Section 5.\n2.4\nInference in the Deep Rendering Model\nInference in the DRM is similar to inference in the shallow RM. For example, to classify images\nwe can use either the sum-product (Eq. 5) or the max-sum (Eq. 6) classiﬁer. The key difference\nbetween the deep and shallow RMs is that the DRM yields iterated layer-by-layer updates, from\nﬁne-to-coarse abstraction (bottom-up) and from coarse-to-ﬁne abstraction (top-down). In the\ncase we are only interested in inferring the high-level class cL, we only need the ﬁne-to-coarse\npass and so we will only consider it in this section.\nImportantly, the bottom-up pass leads directly to DCNs, implying that DCNs ignore poten-\ntially useful top-down information. This maybe an explanation for their difﬁculties in vision\ntasks with occlusion and clutter, where such top-down information is essential for disambiguat-\ning local bottom-up hypotheses. Later on in Section 6.2.2, we will describe the coarse-to-ﬁne\npass and a new class of Top-Down DCNs that do make use of such information.\nGiven an input image I0, the max-sum classiﬁer infers the most likely global conﬁguration\n{cℓ, gℓ}, ℓ= 0, 1, . . . , L by executing the max-sum message passing algorithm in two stages: (i)\nfrom ﬁne-to-coarse levels of abstraction to infer the overall class label ˆcL\nMS and (ii) from coarse-\nto-ﬁne levels of abstraction to infer the latent variables ˆcℓ\nMS and ˆgℓ\nMS at all intermediate levels ℓ.\nAs mentioned above, we will focus on the ﬁne-to-coarse pass. Since the DRM is an RM with\na hierarchical prior on the rendered templates, we can use Eq. 7 to derive the ﬁne-to-coarse\n15\nmax-sum DRM classiﬁer (MS-DRMC) as:\nˆcMS(I) = argmax\ncL∈C\nmax\ng∈G ⟨η(cL, g)|Σ−1|I0⟩\n= argmax\ncL∈C\nmax\ng∈G ⟨Λ(g)µ(cL)|(Λ(g)Λ(g)T)†|I0⟩\n= argmax\ncL∈C\nmax\ng∈G ⟨µ(cL)|\n1\nY\nℓ=L\nΛℓ(gℓ)†|I0⟩\n= argmax\ncL∈C\n⟨µ(cL)|\n1\nY\nℓ=L\nmax\ngℓ∈GℓΛℓ(gℓ)†|I0⟩\n= argmax\ncL∈C\n⟨µ(cL)| max\ngL∈GL ΛL(gL)† · · · max\ng1∈G1 Λ1(g1)†|I0\n|\n{z\n}\n≡I1\n⟩\n≡argmax\ncL∈C\n⟨µ(cL)| max\ngL∈GL ΛL(gL)† · · · max\ng2∈G2 Λ2(g2)†|I1\n|\n{z\n}\n≡I2\n⟩\n≡argmax\ncL∈C\n⟨µ(cL)| max\ngL∈GL ΛL(gL)† · · · max\ng3∈G3 Λ3(g3)†|I2⟩\n...\n≡argmax\ncL∈C\n⟨µ(cL)|IL⟩,\n(12)\nwhere Σ ≡Λ(g)Λ(g)T is the covariance of the rendered image I and ⟨x|M|y⟩≡xTMy. Note\nthe signiﬁcant change with respect to the shallow RM: the covariance Σ is no longer diagonal\ndue to the iterative afﬁne transformations during rendering (Eq. 11), and so we must decorrelate\nthe input image (via Σ−1I0 in the ﬁrst line) in order to classify accurately.\nNote also that we have omitted the bias terms for clarity and that M † is the pseudoinverse\nof matrix M. In the fourth line, we used the distributivity of max over products7 and in the last\nlines deﬁned the intermediate quantities\nIℓ+1 ≡\nmax\ngℓ+1∈Gℓ+1⟨(Λℓ+1(gℓ+1))†\n|\n{z\n}\n≡W ℓ+1\n|Iℓ⟩\n=\nmax\ngℓ+1∈Gℓ+1⟨W ℓ+1(gℓ+1)|Iℓ⟩\n≡MaxPool(Conv(Iℓ)).\n(13)\nHere Iℓ= Iℓ(xℓ, cℓ) is the feature map output of layer ℓindexed by channels cℓand η(cℓ, gℓ) ∝\nµ(cℓ, gℓ) are the natural parameters (i.e., intermediate rendered templates) for level ℓ.\n7 For a > 0, max{ab, ac} = a max{b, c}.\n16\nIf we care only about inferring the overall class of the image cL(I0), then the ﬁne-to-coarse\npass sufﬁces, since all information relevant to determining the overall class has been integrated.\nThat is, for high-level classiﬁcation, we need only iterate Eqs. 12 and 13. Note that Eq. 12\nsimpliﬁes to Eq. 9 when we assume sparse patch rendering as in Section 2.2.\nComing back to DCNs, we have see that the ℓ-th iteration of Eq. 12 or Eq. 9 corresponds to\nfeedforward propagation in the ℓ-th layer of a DCN. Thus a DCN’s operation has a probabilistic\ninterpretation as ﬁne-to-coarse inference of the most probable global conﬁguration in the DRM.\n2.4.1\nWhat About the SoftMax Regression Layer?\nIt is important to note that we have not fully reconstituted the architecture of modern a DCN\nas yet. In particular, the SoftMax regression layer, typically attached to the end of network, is\nmissing. This means that the high-level class cL in the DRM (Eq. 12) is not necessarily the\nsame as the training data class labels ˜c given in the dataset. In fact, the two labels ˜c and cL are\nin general distinct.\nBut then how are we to interpret cL? The answer is that the most probable global con-\nﬁguration (cL, g∗) inferred by a DCN can be interpreted as a good representation of the input\nimage, i.e., one that disentangles the many nuisance factors into (nearly) independent compo-\nnents cL, g∗.\n8 Under this interpretation, it becomes clear that the high-level class cL in the\ndisentangled representation need not be the same as the training data class label ˜c.\nThe disentangled representation for cL lies in the penultimate layer activations: ˆaL(In) =\nln p(cL, g∗|In). Given this representation, we can infer the class label ˜c by using a simple linear\nclassiﬁer such as the SoftMax regression9. Explicitly, the Softmax regression layer computes\np(˜c|ˆaL; θSoftmax) = φ(W L+1ˆaL + bL+1), and then chooses the most likely class. Here φ(·) is the\nsoftmax function and θSoftmax ≡{W L+1, bL+1} are the parameters of the SoftMax regression\nlayer.\n2.5\nDCNs are Probabilistic Message Passing Networks\n2.5.1\nDeep Rendering Model and Message Passing\nEncouraged by the correspondence identiﬁed in Section 2.4, we step back for a moment to\nreinterpret all of the major elements of DCNs in a probabilistic light. Our derivation of the\nDRM inference algorithm above is mathematically equivalent to performing max-sum message\npassing on the factor graph representation of the DRM, which is shown in Fig. 2B. The factor\n8In this sense, the DRM can be seen as a deep (nonlinear) generalization of Independent Components Analysis\n(23).\n9Note that this implicitly assumes that a good disentangled representation of an image will be useful for the\nclassiﬁcation task at hand.\n17\ngraph encodes the same information as the generative model but organizes it in a manner that\nsimpliﬁes the deﬁnition and execution of inference algorithms (24). Such inference algorithms\nare called message passing algorithms, because they work by passing real-valued functions\ncalled messages along the edges between nodes. In the DRM/DCN, the messages sent from\nﬁner to coarser levels are the feature maps Iℓ(xℓ, cℓ). However, unlike the input image I0, the\nchannels cℓin these feature maps do not refer to colors (e.g, red, green, blue) but instead to\nmore abstract features (e.g., edge orientations or the open/closed state of an eyelid).\n2.5.2\nA Uniﬁcation of Neural Networks and Probabilistic Inference\nThe factor graph formulation provides a powerful interpretation that the convolution, Max-\nPooling and ReLu operations in a DCN correspond to max-sum inference in a DRM. Thus,\nwe see that architectures and layer types commonly used in today’s DCNs are not ad hoc; rather\nthey can be derived from precise probabilistic assumptions that entirely determine their struc-\nture. Thus the DRM uniﬁes two perspectives — neural network and probabilistic inference. A\nsummary of the relationship between the two perspectives is given in Table 1.\n2.5.3\nThe Probabilistic Role of Max-Pooling\nConsider the role of max-pooling from the message passing perspective. We see that it can\nbe interpreted as the “max” in max-sum, thus executing a max-marginalization over nuisance\nvariables g. Typically, this operation would be intractable, since there are exponentially many\nconﬁgurations g ∈G. But here the DRM’s model of abstraction — a deep product of afﬁne\ntransformations — comes to the rescue.\nIt enables us to convert an otherwise intractable\nmax-marginalization over g into a tractable sequence of iterated max-marginalizations over\nabstraction levels gℓ(Eqs. 12, 13).10 Thus, the max-pooling operation implements probabilistic\nmarginalization, so is absolutely essential to the DCN’s ability to factor out nuisance variation.\nIndeed, since the ReLu can also be cast as a max-pooling over ON/OFF switching variables,\nwe conclude that the most important operation in DCNs is max-pooling. This is in conﬂict with\nsome recent claims to the contrary (27).\n2.6\nLearning the Rendering Models\nSince the RM and DRM are graphical models with latent variables, we can learn their pa-\nrameters from training data using the expectation-maximization (EM) algorithm (28). We ﬁrst\ndevelop the EM algorithm for the shallow RM from Section 2.1 and then extend it to the DRM\nfrom Section 2.3.\n10This can be seen, equivalently, as the execution of the max-product algorithm (26).\n18\nAspect!\nNeural'Nets'Perspective'!\nDeep$Convnets$(DCNs)!\nProbabilistic+Perspective+!\nDeep$Rendering$Model$(DRM)!\nModel!\nWeights and biases of filters at a \ngiven layer  \nPartial Rendering at a given \nabstraction level/scale \n!\nNumber of Layers \nNumber of Abstraction Levels \n!\nNumber of Filters in a layer \nNumber of Clusters/Classes at a \ngiven abstraction level  \n!\nImplicit in network weights; can be \ncomputed by product of weights over \nall layers or by activity maximization \nCategory prototypes are finely \ndetailed versions of coarser-scale \nsuper-category prototypes. Fine \ndetails are modeled with affine \nnuisance transformations. \nInference!\nForward propagation thru DCN \nExact bottom-up inference via Max-\nSum Message Passing (with Max-\nProduct for Nuisance Factorization). \n!\nInput and Output Feature Maps \nProbabilistic Max-Sum Messages \n(real-valued functions of variables \nnodes) \n!\nTemplate matching at a given layer \n(convolutional, locally or fully \nconnected) \nLocal computation at factor node (log-\nlikelihood of measurements) \n!\nMax-Pooling over local pooling region \nMax-Marginalization over Latent \nTranslational Nuisance \ntransformations \n!\nRectified Linear Unit (ReLU). \nSparsifies output activations. \nMax-Marginalization over Latent \nSwitching state of Renderer. Low prior \nprobability of being ON. \nLearning!\nStochastic Gradient Descent \nBatch Discriminative EM Algorithm \nwith Fine-to-Coarse E-step + Gradient \nM-step. No coarse-to-fine pass in E-\nstep. \n!\nN/A \nFull EM Algorithm \n!\nBatch-Normalized SGD (Google state-\nof-the-art [BN]) \nDiscriminative Approximation to Full \nEM (assumes Diagonal Pixel \nCovariance) \nTable 1. Summary of probabilistic and neural network perspectives for DCNs. The DRM provides an\nexact correspondence between the two, providing a probabilistic interpretation for all of the common\nelements of DCNs relating to the underlying model, inference algorithm, and learning rules. [BN] =\nreference (25).\n19\n2.6.1\nEM Algorithm for the Shallow Rendering Model\nGiven a dataset of labeled training images {In, cn}N\nn=1, each iteration of the EM algorithm con-\nsists of an E-step that infers the latent variables given the observed variables and the “old” pa-\nrameters ˆθold\ngen from the last M-step, followed by an M-step that updates the parameter estimates\naccording to\nE-step:\nγncg = p(c, g|In; ˆθold\ngen),\n(14)\nM-step:\nˆθ = argmax\nθ\nX\nn\nX\ncg\nγncgL(θ).\n(15)\nHere γncg are the posterior probabilities over the latent mixture components (also called the\nresponsibilities), the sum P\ncg is over all possible global conﬁgurations (c, g) ∈C × G, and\nL(θ) is the complete-data log-likelihood for the model.\nFor the RM, the parameters are deﬁned as θ ≡{πc, πg, µcg, σ2} and include the prior prob-\nabilities of the different classes πc and nuisance variables πg along with the rendered templates\nµcg and the pixel noise variance σ2. If, instead of an isotropic Gaussian RM, we use a full-\ncovariance Gaussian RM or an RM with a different exponential family distribution, then the\nsufﬁcient statistics and the rendered template parameters would be different (e.g. quadratic for\na full covariance Gaussian).\nWhen the clusters in the RM are well-separated (or equivalently, when the rendering intro-\nduces little noise), each input image can be assigned to its nearest cluster in a “hard” E-step,\nwherein we care only about the most likely conﬁguration of the cℓand gℓgiven the input I0. In\nthis case, the responsibility γℓ\nncg = 1 if cℓand gℓin image In are consistent with the most likely\nconﬁguration; otherwise it equals 0. Thus, we can compute the responsibilities using max-sum\nmessage passing according to Eqs. 12 and 14. In this case, the hard EM algorithm reduces to\nHard E-step:\nγℓ\nncg = J(c, g) = (c∗\nn, g∗\nn)K\n(16)\nM-step:\nˆN ℓ\ncg =\nX\nn\nγℓ\nncg\nˆπℓ\ncg =\nˆN ℓ\ncg\nN\nˆµℓ\ncg =\n1\nˆN ℓ\ncg\nX\nn\nγℓ\nncgIℓ\nn\n(ˆσ2\ncg)ℓ=\n1\nˆN ℓ\ncg\nX\nn\nγℓ\nncg∥Iℓ\nn −µℓ\ncg∥2\n2,\n(17)\nwhere we have used the Iversen bracket to denote a boolean expression, i.e., JbK ≡1 if b is true\nand JbK ≡0 if b is false.\n20\n2.6.2\nEM Algorithm for the Deep Rendering Model\nFor high-nuisance tasks, the EM algorithm for the shallow RM is computationally intractable,\nsince it requires recomputing the responsibilities and parameters for all possible conﬁgurations\nτ ≡(cL, gL, . . . g1).\nThere are exponentially many such conﬁgurations ( |CL| Q\nℓ|Gℓ|), one for each possible\nrendering tree rooted at cL. However, the crux of the DRM is the factorized form of the rendered\ntemplates (Eq. 11), which results in a dramatic reduction in the number of parameters. This\nenables us to efﬁciently infer the most probable conﬁguration exactly11 via Eq. 12 and thus\navoid the need to resort to slower, approximate sampling techniques (e.g. Gibbs sampling),\nwhich are commonly used for approximate inference in deep HBMs (20, 21). We will exploit\nthis realization below in the DRM E-step.\nGuided by the EM algorithm for MFA (22), we can extend the EM algorithm for the shallow\nRM from the previous section into one for the DRM. The DRM E-step performs inference,\nﬁnding the most likely rendering tree conﬁguration τ ∗\nn ≡(cL\nn, gL\nn, . . . , g1\nn)∗given the current\ntraining input I0\nn. The DRM M-step updates the parameters in each layer — the weights and\nbiases — via a responsibility-weighted regression of output activations off of input activations.\nThis can be interpreted as each layer learning how to summarize its input feature map into a\ncoarser-grained output feature map, the essence of abstraction.\nIn the following it will be convenient to deﬁne and use the augmented form12 for certain\nparameters so that afﬁne transformations can be recast as linear ones. Mathematically, a single\n11Note that this is exact for the spike-n-slab approximation to the truly sparse rendering model where only one\nrenderer per neighborhood is active, as described in Section 2.2. Technically, this approximation is not a tree, but\ninstead a so-called polytree. Nevertheless, max-sum is exact for trees and polytrees (29).\n12y = mx + b ≡˜mT ˜x, where ˜m ≡[m|b] and ˜x ≡[x|1] are the augmented forms for the parameters and input.\n21\nEM iteration for the DRM is then deﬁned as\nE-step:\nγnτ = Jτ = τ ∗\nnK where τ ∗\nn ≡argmax\nτ\n{ln p(τ|In)}\n(18)\nE\n\u0002\nµℓ(cℓ)\n\u0003\n= Λℓ(gℓ)†(Iℓ−1\nn\n−αℓ(gℓ)) ≡W ℓ(gℓ)Iℓ−1\nn\n+ bℓ(gℓ)\n(19)\nE\n\u0002\nµℓ(cℓ)µℓ(cℓ)T\u0003\n= 1 −Λℓ(gℓ)†Λℓ(gℓ)+\nΛℓ(gℓ)†(Iℓ−1\nn\n−αℓ(gℓ))(Iℓ−1\nn\n−αℓ(gℓ))T(Λℓ(gℓ)†)T\n(20)\nM-step:\nπ(τ) = 1\nN\nX\nn\nγnτ\n(21)\n˜Λℓ(gℓ) ≡\n\u0002\nΛℓ(gℓ) | αℓ(gℓ)\n\u0003\n=\n X\nn\nγnτIℓ−1\nn\nE\n\u0002\n˜µℓ(cℓ)\n\u0003T\n!  X\nn\nγnτE\n\u0002\n˜µℓ(cℓ)˜µℓ(cℓ)T\u0003\n!−1\n(22)\nΨℓ= 1\nN diag\n(X\nn\nγnτ\n\u0010\nIℓ−1\nn\n−˜Λℓ(gℓ)E\n\u0002\n˜µℓ(cℓ)\n\u0003\u0011\n(Iℓ−1\nn\n)T\n)\n,\n(23)\nwhere Λℓ(gℓ)† ≡Λℓ(gℓ)T(Ψℓ+ Λℓ(gℓ)(Λℓ(gℓ))T)−1 and E\n\u0002\n˜µℓ(cℓ)\n\u0003\n=\n\u0002\nE\n\u0002\nµℓ(cℓ)\n\u0003\n| 1\n\u0003\n. Note\nthat the nuisance variables gℓcomprise both the translational and the switching variables that\nwere introduced earlier for DCNs.\nNote that this new EM algorithm is a derivative-free alternative to the back propagation\nalgorithm for training DCNs that is fast, easy to implement, and intuitive.\nA powerful learning rule discovered recently and independently by Google (25) can be\nseen as an approximation to the above EM algorithm, whereby Eq. 18 is approximated by\nnormalizing the input activations with respect to each training batch and introducing scaling\nand bias parameters according to\nE\n\u0002\nµℓ(cℓ)\n\u0003\n= Λℓ(gℓ)†(Iℓ−1\nn\n−αℓ(gℓ))\n≈Γ · ˜Iℓ−1\nn\n+ β\n≡Γ ·\n\u0012Iℓ−1\nn\n−¯IB\nσB\n\u0013\n+ β.\n(24)\nHere ˜Iℓ−1\nn\nare the batch-normalized activations, and ¯IB and σB are the batch mean and standard\ndeviation vector of the input activations, respectively. Note that the division is element-wise,\n22\nsince each activation is normalized independently to avoid a costly full covariance calculation.\nThe diagonal matrix Γ and bias vector β are parameters that are introduced to compensate\nfor any distortions due to the batch-normalization. In light of our EM algorithm derivation\nfor the DRM, it is clear that this scheme is a crude approximation to the true normalization\nstep in Eq. 18, whose decorrelation scheme uses the nuisance-dependent mean α(gℓ) and full\ncovariance Λℓ(gℓ)†. Nevertheless, the excellent performance of the Google algorithm bodes\nwell for the performance of the exact EM algorithm for the DRM developed above.\n2.6.3\nWhat About DropOut Training?\nWe did not mention the most common regularization scheme used with DCNs — DropOut (30).\nDropOut training consists of units in the DCN dropping their outputs at random. This can be\nseen as a kind of noise corruption, and encourages the learning of features that are robust to\nmissing data and prevents feature co-adaptation as well (18, 30). DropOut is not speciﬁc to\nDCNs; it can be used with other architectures as well. For brevity, we refer the reader to the\nproof of the DropOut algorithm in Appendix A.7. There we show that DropOut can be derived\nfrom the EM algorithm.\n2.7\nFrom Generative to Discriminative Classiﬁers\nWe have constructed a correspondence between the DRM and DCNs, but the mapping deﬁned\nso far is not exact. In particular, note the constraints on the weights and biases in Eq. 8. These\nare reﬂections of the distributional assumptions underlying the Gaussian DRM. DCNs do not\nhave such constraints — their weights and biases are free parameters. As a result, when faced\nwith training data that violates the DRM’s underlying assumptions (model misspeciﬁcation),\nthe DCN will have more freedom to compensate. In order to complete our mapping and create\nan exact correspondence between the DRM and DCNs, we relax these parameter constraints,\nallowing the weights and biases to be free and independent parameters. However, this seems an\nad hoc approach. Can we instead theoretically motivate such a relaxation?\nIt turns out that the distinction between the DRM and DCN classiﬁers is fundamental: the\nformer is known as a generative classiﬁer while the latter is known as a discriminative clas-\nsiﬁer (31, 32). The distinction between generative and discriminative models has to do with\nthe bias-variance tradeoff. On the one hand, generative models have strong distributional as-\nsumptions, and thus introduce signiﬁcant model bias in order to lower model variance (i.e., less\nrisk of overﬁtting). On the other hand, discriminative models relax some of the distributional\nassumptions in order to lower the model bias and thus “let the data speak for itself”, but they do\nso at the cost of higher variance (i.e., more risk of overﬁtting) (31,32). Practically speaking, if\na generative model is misspeciﬁed and if enough labeled data is available, then a discriminative\n23\nmodel will achieve better performance on a speciﬁc task (32). However, if the generative model\nreally is the true data-generating distribution (or there is not much labeled data for the task),\nthen the generative model will be the better choice.\nHaving motivated the distinction between the two types of models, in this section we will\ndeﬁne a method for transforming one into the other that we call a discriminative relaxation. We\ncall the resulting discriminative classiﬁer a discriminative counterpart of the generative classi-\nﬁer.13 We will then show that applying this procedure to the generative DRM classiﬁer (with\nconstrained weights) yields the discriminative DCN classiﬁer (with free weights). Although we\nwill focus again on the Gaussian DRM, the treatment can be generalized to other exponential\nfamily distributions with a few modiﬁcations (see Appendix A.6 for more details).\n2.7.1\nTransforming a Generative Classiﬁer into a Discriminative One\nBefore we formally deﬁne the procedure, some preliminary deﬁnitions and remarks will be\nhelpful. A generative classiﬁer models the joint distribution p(c, I) of the input features and\nthe class labels. It can then classify inputs by using Bayes Rule to calculate p(c|I) ∝p(c, I) =\np(I|c)p(c) and picking the most likely label c. Training such a classiﬁer is known as generative\nlearning, since one can generate synthetic features I by sampling the joint distribution p(c, I).\nTherefore, a generative classiﬁer learns an indirect map from input features I to labels c by\nmodeling the joint distribution p(c, I) of the labels and the features.\nIn contrast, a discriminative classiﬁer parametrically models p(c|I) = p(c|I; θd) and then\ntrains on a dataset of input-output pairs {(In, cn)}N\nn=1 in order to estimate the parameter θd. This\nis known as discriminative learning, since we directly discriminate between different labels c\ngiven an input feature I. Therefore, a discriminative classiﬁer learns a direct map from input\nfeatures I to labels c by directly modeling the conditional distribution p(c|I) of the labels given\nthe features.\nGiven these deﬁnitions, we can now deﬁne the discriminative relaxation procedure for con-\nverting a generative classiﬁer into a discriminative one. Starting with the standard learning\nobjective for a generative classiﬁer, we will employ a series of transformations and relaxations\n13The discriminative relaxation procedure is a many-to-one mapping: several generative models might have the\nsame discriminative model as their counterpart.\n24\nto obtain the learning objective for a discriminative classiﬁer. Mathematically, we have\nmax\nθ\nLgen(θ) ≡max\nθ\nX\nn\nln p(cn, In|θ)\n(a)\n= max\nθ\nX\nn\nln p(cn|In, θ) + ln p(In|θ)\n(b)\n= max\nθ,˜θ:θ=˜θ\nX\nn\nln p(cn|In, θ) + ln p(In|˜θ)\n(c)\n≤max\nθ\nX\nn\nln p(cn|In, θ)\n|\n{z\n}\n≡Lcond(θ)\n(d)\n=\nmax\nη:η=ρ(θ)\nX\nn\nln p(cn|In, η)\n(e)\n≤max\nη\nX\nn\nln p(cn|In, η)\n|\n{z\n}\n≡Ldis(η)\n,\n(25)\nwhere the L’s are the generative, conditional and discriminative log-likelihoods, respectively.\nIn line (a), we used the Chain Rule of Probability. In line (b), we introduced an extra set of\nparameters ˜θ while also introducing a constraint that enforces equality with the old set of gen-\nerative parameters θ. In line (c), we relax the equality constraint (ﬁrst introduced by Bishop,\nLaSerre and Minka in (31)), allowing the classiﬁer parameters θ to differ from the image gener-\nation parameters ˜θ. In line (d), we pass to the natural parametrization of the exponential family\ndistribution I|c, where the natural parameters η = ρ(θ) are a ﬁxed function of the conventional\nparameters θ. This constraint on the natural parameters ensures that optimization of Lcond(η)\nyields the same answer as optimization of Lcond(θ). And ﬁnally, in line (e) we relax the nat-\nural parameter constraint to get the learning objective for a discriminative classiﬁer, where the\nparameters η are now free to be optimized.\nIn summary, starting with a generative classiﬁer with learning objective Lgen(θ), we com-\nplete steps (a) through (e) to arrive at a discriminative classiﬁer with learning objective Ldis(η).\nWe refer to this process as a discriminative relaxation of a generative classiﬁer and the resulting\nclassiﬁer is a discriminative counterpart to the generative classiﬁer.\nFigure 4 illustrates the discriminative relaxation procedure as applied to the RM (or DRM).\nIf we consider a Gaussian (D)RM, then θ simply comprises the mixing probabilities πcg and\nthe mixture parameters λcg, and so that we have θ = {πcg, µcg, σ2}. The corresponding relaxed\ndiscriminative parameters are the weights and biases ηdis ≡{wcg, bcg}.\nIntuitively, we can interpret the discriminative relaxation as a brain-world transformation\napplied to a generative model. According to this interpretation, instead of the world generating\n25\nI\nc g\n⇡cg\nλcg\nI\nc g\n✓\nX\"\ndiscrimina+ve\"\nrelaxa+on\"\nX\"\n⇢(·)\nA\nB\n✓⌘✓brain\n⌘brain\n˜✓⌘✓world\nFigure 4. Graphical depiction of discriminative relaxation procedure. (A) The Rendering Model (RM) is\ndepicted graphically, with mixing probability parameters πcg and rendered template parameters λcg. The\nbrain-world transformation converts the RM (A) to an equivalent graphical model (B), where an extra\nset of parameters ˜θ and constraints (arrows from θ to ˜θ to η) have been introduced. Discriminatively\nrelaxing these constraints (B, red X’s) yields the single-layer DCN as the discriminative counterpart to\nthe original generative RM classiﬁer in (A).\nimages and class labels (Fig. 4A), we instead imagine the world generating images In via the\nrendering parameters ˜θ ≡θworld while the brain generates labels cn, gn via the classiﬁer param-\neters ηdis ≡ηbrain (Fig. 4B). Note that the graphical model depicted in Fig. 4B is equivalent to\nthat in Fig. 4A, except for the relaxation of the parameter constraints (red ×’s) that represent\nthe discriminative relaxation.\n2.7.2\nFrom the Deep Rendering Model to Deep Convolutional Networks\nWe can now apply the above to show that the DCN is a discriminative relaxation of the DRM.\nFirst, we apply the brain-world transformation (Eq. 25) to the DRM. The resulting classiﬁer is\nprecisely a deep MaxOut neural network (10) as discussed earlier. Second, we impose trans-\nlational invariance at the ﬁner scales of abstraction ℓand introduce switching variables a to\nmodel inactive renderers. This yields convolutional layers with ReLu activation functions, as in\nSection 2.1. Third, the learning algorithm for the generative DRM classiﬁer — the EM algo-\nrithm in Eqs. 18–23 — must be modiﬁed according to Eq. 25 to account for the discriminative\nrelaxation. In particular, note that the new discriminative E-step is only ﬁne-to-coarse and cor-\nresponds to forward propagation in DCNs. As for the discriminative M-step, there are a variety\nof choices: any general purpose optimization algorithm can be used (e.g., Newton-Raphson,\nconjugate gradient, etc.). Choosing gradient descent this leads to the classical back propagation\nalgorithm for neural network training (33). Typically, modern-day DCNs are trained using a\nvariant of back propagation called Stochastic Gradient Descent (SGD), in which gradients are\ncomputed using one mini-batch of data at a time (instead of the entire dataset). In light of our\ndevelopments here, we can re-interpret SGD as a discriminative counterpart to the generative\nbatch EM algorithm (34,35).\nThis completes the mapping from the DRM to DCNs. We have shown that DCN classi-\n26\nﬁers are a discriminative relaxation of DRM classiﬁers, with forward propagation in a DCN\ncorresponding to inference of the most probable conﬁguration in a DRM.14 We have also re-\ninterpreted learning: SGD back propagation training in DCNs is a discriminative relaxation of\na batch EM learning algorithm for the DRM. We have provided a principled motivation for\npassing from the generative DRM to its discriminative counterpart DCN by showing that the\ndiscriminative relaxation helps alleviate model misspeciﬁcation issues by increasing the DRM’s\nﬂexibility, at the cost of slower learning and requiring more training data.\n3\nNew Insights into Deep Convolutional Networks\nIn light of the intimate connection between DRMs and DCNs, the DRM provides new insights\ninto how and why DCNs work, answering many open questions. And importantly, DRMs also\nshow us how and why DCNs fail and what we can do to improve them (see Section 6). In this\nsection, we explore some of these insights.\n3.1\nDCNs Possess Full Probabilistic Semantics\nThe factor graph formulation of the DRM (Fig. 2B) provides a useful interpretation of DCNs: it\nshows us that the convolutional and max-pooling layers correspond to standard message pass-\ning operations, as applied inside factor nodes in the factor graph of the DRM. In particular, the\nmax-sum algorithm corresponds to a max-pool-conv neural network, whereas the sum-product\nalgorithm corresponds to a mean-pool-conv neural network. More generally, we see that archi-\ntectures and layer types used commonly in successful DCNs are neither arbitrary nor ad hoc;\nrather they can be derived from precise probabilistic assumptions that almost entirely determine\ntheir structure. A summary of the two perspectives — neural network and probabilistic — are\ngiven in Table 1.\n3.2\nClass Appearance Models and Activity Maximization\nOur derivation of inference in the DRM enables us to understand just how trained DCNs distill\nand store knowledge from past experiences in their parameters. Speciﬁcally, the DRM generates\nrendered templates µ(cL, g) ≡µ(cL, gL, . . . , g1) via a product of afﬁne transformations, thus\nimplying that class appearance models in DCNs (and DRMs) are stored in a factorized form\nacross multiple levels of abstraction. Thus, we can explain why past attempts to understand\nhow DCNs store memories by examining ﬁlters at each layer were a fruitless exercise: it is the\n14As mentioned in Section 2.4.1, this is typically followed by a Softmax Regression layer at the end. This layer\nclassiﬁes the hidden representation (the penultimate layer activations ˆaL(In)) into the class labels ˜cn used for\ntraining. See Section 2.4.1 for more details.\n27\nproduct of all the ﬁlters/weights over all layers that yield meaningful images of objects. Indeed,\nthis fact is encapsulated mathematically in Eqs. 10, 11. Notably, recent studies in computational\nneuroscience have also shown a strong similarity between representations in primate visual\ncortex and a highly trained DCN (36), suggesting that the brain might also employ factorized\nclass appearance models.\nWe can also shed new light on another approach to understanding DCN memories that\nproceeds by searching for input images that maximize the activity of a particular class unit\n(say, cat) (37), a technique we call activity maximization. Results from activity maximization\non a high performance DCN trained on 15 million images from (37) is shown in Fig. 5. The\nresulting images are striking and reveal much about how DCNs store memories. We now derive\na closed-form expression for the activity-maximizing images as a function of the underlying\nDRM model’s learned parameters. Mathematically, we seek the image I that maximizes the\nscore S(c|I) of a speciﬁc object class. Using the DRM, we have\nmax\nI\nS(cℓ|I) = max\nI\nmax\ng∈G ⟨1\nσ2µ(cℓ, gℓ)|I⟩\n∝max\ng∈G max\nI\n⟨µ(cℓ, g)|I⟩\n= max\ng∈G max\nIP1\n· · · max\nIPp\n⟨µ(cℓ, g)|\nX\nPi∈P\nIPi⟩\n= max\ng∈G\nX\nPi∈P\nmax\nIPi\n⟨µ(cℓ, g)|IPi⟩\n= max\ng∈G\nX\nPi∈P\n⟨µ(cℓ, g)|I∗\nPi(cℓ, g)⟩\n=\nX\nPi∈P\n⟨µ(cℓ, g)|I∗\nPi(cℓ, g∗\nPi⟩,\n(26)\nwhere\nI∗\nPi(cℓ, g)\n≡\nargmaxIPi\n⟨µ(cℓ, g)|IPi⟩\nand\ng∗\nPi\n=\ng∗(cℓ, Pi)\n≡\nargmaxg∈G ⟨µ(cℓ, g)|I∗\nPi(cℓ, g)⟩.\nIn the third line, the image I is decomposed into P\npatches IPi of the same size as I, with all pixels outside of the patch Pi set to zero. The\nmaxg∈G operator ﬁnds the most probable g∗\nPi within each patch. The solution I∗of the activity\nmaximization is then the sum of the individual activity-maximizing patches\nI∗≡\nX\nPi∈P\nI∗\nPi(cℓ, g∗\nPi) ∝\nX\nPi∈P\nµ(cℓ, g∗\nPi).\n(27)\nEq. 27 implies that I∗contains multiple appearances of the same object but in various poses.\nEach activity-maximizing patch has its own pose (i.e. g∗\nPi), in agreement with Fig. 5. Such\nimages provide strong conﬁrming evidence that the underlying model is a mixture over nuisance\n(pose) parameters, as is expected in light of the DRM.\n28\ndumbbell \ncup \ndalmatian \nbell pepper \nlemon \nhusky \nwashing machine \ncomputer keyboard \nkit fox \ngoose \nlimousine \nostrich \nFigure 5. Results of activity maximization on ImageNet dataset. For a given class c, activity-maximizing\ninputs are superpositions of various poses of the object, with distinct patches Pi containing distinct poses\ng∗\nPi, as predicted by Eq. 27. Figure adapted from (37) with permission from the authors.\n29\n3.3\n(Dis)Entanglement: Supervised Learning of Task Targets Is\nIntertwined with Unsupervised Learning of Latent Task Nuisances\nA key goal of representation learning is to disentangle the factors of variation that contribute\nto an image’s appearance. Given our formulation of the DRM, it is clear that DCNs are dis-\ncriminative classiﬁers that capture these factors of variation with latent nuisance variables g. As\nsuch, the theory presented here makes a clear prediction that for a DCN, supervised learning\nof task targets will lead inevitably to unsupervised learning of latent task nuisance variables.\nFrom the perspective of manifold learning, this means that the architecture of DCNs is designed\nto learn and disentangle the intrinsic dimensions of the data manifold.\nIn order to test this prediction, we trained a DCN to classify synthetically rendered images\nof naturalistic objects, such as cars and planes. Since we explicitly used a renderer, we have\nthe power to systematically control variation in factors such as pose, location, and lighting. Af-\nter training, we probed the layers of the trained DCN to quantify how much linearly separable\ninformation exists about the task target c and latent nuisance variables g. Figure 6 shows that\nthe trained DCN possesses signiﬁcant information about latent factors of variation and, further-\nmore, the more nuisance variables, the more layers are required to disentangle the factors. This\nis strong evidence that depth is necessary and that the amount of depth required increases with\nthe complexity of the class models and the nuisance variations.\nIn light of these results, when we talk about training DCNs, the traditional distinction be-\ntween supervised and unsupervised learning is ill-deﬁned at worst and misleading at best. This\nis evident from the initial formulation of the RM, where c is the task target and g is a latent vari-\nable capturing all nuisance parameters (Fig. 1). Put another way, our derivation above shows\nthat DCNs are discriminative classiﬁers with latent variables that capture nuisance variation.\nWe believe the main reason this was not noticed earlier is probably that latent nuisance variables\nin a DCN are hidden within the max-pooling units, which serve the dual purpose of learning\nand marginalizing out the latent nuisance variables.\n4\nFrom the Deep Rendering Model to Random Decision\nForests\nRandom Decision Forests (RDF)s (5, 38) are one of the best performing but least understood\nclassiﬁers in machine learning. While intuitive, their structure does not seem to arise from a\nproper probabilistic model. Their success in a vast array of ML tasks is perplexing, with no\nclear explanation or theoretical understanding. In particular, they have been quite successful in\nreal-time image and video segmentation tasks, the most prominent example being their use for\npose estimation and body part tracking in the Microsoft Kinect gaming system (39). They also\n30\n0\n1\n2\n3\n4\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy Rate\nAccuracy of Classifying Classes and Latent Variables vs Layer\nobj_accu_rate\nslant_accu_rate\ntilt_accu_rate\nlocx_accu_rate\nlocy_accu_rate\nlocz_accu_rate\nenergy_accu_rate\n0\n1\n2\n3\n4\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy Rate\nAccuracy of Classifying Classes and Latent Variables vs Layer\nobj_accu_rate\nslant_accu_rate\ntilt_accu_rate\nlocx_accu_rate\nlocy_accu_rate\nFigure 6. Manifold entanglement and disentanglement as illustrated in a 5-layer max-out DCN trained\nto classify synthetically rendered images of planes (top) and naturalistic objects (bottom) in different\nposes, locations, depths and lighting conditions. The amount of linearly separable information about\nthe target variable (object identity, red) increases with layer depth while information about nuisance\nvariables (slant, tilt, left-right location, depth location) follows an inverted U-shaped curve. Layers\nwith increasing information correspond to disentanglement of the manifold — factoring variation into\nindependent parameters — whereas layers with decreasing information correspond to marginalization\nover the nuisance parameters. Note that disentanglement of the latent nuisance parameters is achieved\nprogressively over multiple layers, without requiring the network to explicitly train for them. Due to\nthe complexity of the variation induced, several layers are required for successful disentanglement, as\npredicted by our theory.\n31\nhave had great success in medical image segmentation problems (5,38), wherein distinguishing\ndifferent organs or cell types is quite difﬁcult and typically requires expert annotators.\nIn this section we show that, like DCNs, RDFs can also be derived from the DRM model,\nbut with a different set of assumptions regarding the nuisance structure. Instead of translational\nand switching nuisances, we will show that an additive mutation nuisance process that generates\na hierarchy of categories (e.g., evolution of a taxonomy of living organisms) is at the heart of\nthe RDF. As in the DRM to DCN derivation, we will start with a generative classiﬁer and then\nderive its discriminative relaxation. As such, RDFs possess a similar interpretation as DCNs in\nthat they can be cast as max-sum message passing networks.\nA decision tree classiﬁer takes an input image I and asks a series of questions about it. The\nanswer to each question determines which branch in the tree to follow. At the next node, another\nquestion is asked. This pattern is repeated until a leaf b of the tree is reached. At the leaf, there\nis a class posterior probability distribution p(c|I, b) that can be used for classiﬁcation. Different\nleaves contain different class posteriors. An RDF is an ensemble of decision tree classiﬁers\nt ∈T . To classify an input I, it is sent as input to each decision tree t ∈T individually,\nand each decision tree outputs a class posterior p(c|I, b, t). These are then averaged to obtain\nan overall posterior p(c|I) = P\nt p(c|I, b, t)p(t), from which the most likely class c is chosen.\nTypically we assume p(t) = 1/|T |.\n4.1\nThe Evolutionary Deep Rendering Model: A Hierarchy of Categories\nWe deﬁne the evolutionary DRM (E-DRM) as a DRM with an evolutionary tree of categories.\nSamples from the model are generated by starting from the root ancestor template and randomly\nmutating the templates. Each child template is an additive mutation of its parent, where the spe-\nciﬁc mutation does not depend on the parent (see Eq. 29 below). Repeating this pattern at each\nchild node, an entire evolutionary tree of templates is generated. We assume for simplicity that\nwe are working with a Gaussian E-DRM so that at the leaves of the tree a sample is generated\nby adding Gaussian pixel noise. Of course, as described earlier, this can be extended to handle\nother noise distributions from the exponential family. Mathematically, we have\ncL ∼Cat(π(cL)),\ncL ∈CL,\ngℓ+1 ∼Cat(π(gℓ+1)),\ngℓ+1 ∈Gℓ+1,\nℓ= L −1, L −2, . . . , 0\nµ(cL, g) = Λ(g)µ(cL) ≡Λ1(g1) · · · ΛL(gL) · µ(cL)\n= µ(cL) + α(gL) + · · · + α(g1),\ng = {gℓ}L\nℓ=1\nI(cL, g) = µ(cL, g) + N(0, σ21D) ∈RD.\n(28)\n32\nHere, Λℓ(gℓ) has a special structure due to the additive mutation process: Λℓ(gℓ) = [1 | α(gℓ)],\nwhere 1 is the identity matrix. As before, Cℓ, Gℓare the sets of all target-relevant and target-\nirrelevant nuisance variables at level ℓ, respectively. (The target here is the same as with the\nDRM and DCNs — the overall class label cL.) The rendering path represents template evo-\nlution and is deﬁned as the sequence (cL, gL, . . . , gℓ, . . . , g1) from the root ancestor template\ndown to the individual pixels at ℓ= 0. µ(cL) is an abstract template for the root ancestor\ncL, and P\nℓα(gℓ) represents the sequence of local nuisance transformations, in this case, the\naccumulation of many additive mutations.\nAs with the DRM, we can cast the E-DRM into an incremental form by deﬁning an inter-\nmediate class cℓ≡(cL, gL, . . . , gℓ+1) that intuitively represents a partial evolutionary path up to\nlevel ℓ. Then, the mutation from level ℓ+ 1 to ℓcan be written as\nµ(cℓ) = Λℓ+1(gℓ+1) · µ(cℓ+1) = µ(cℓ+1) + α(gℓ+1),\n(29)\nwhere α(gℓ) is the mutation added to the template at level ℓin the evolutionary tree.\nAs a generative model, the E-DRM is a mixture of evolutionary paths, where each path starts\nat the root and ends at a leaf species in the tree. Each leaf species is associated with a rendered\ntemplate µ(cL, gL, . . . , g1).\n4.2\nInference with the E-DRM Yields a Decision Tree\nSince the E-DRM is an RM with a hierarchical prior on the rendered templates, we can use\nEq. 7 to derive the E-DRM inference algorithm as:\nˆcMS(I) = argmax\ncL∈CL max\ng∈G ⟨η(cL, g)|I0⟩\n= argmax\ncL∈CL max\ng∈G ⟨Λ(g)µ(cL)|I0⟩\n= argmax\ncL∈CL\nmax\ng1∈G1 · · · max\ngL∈GL ⟨µ(cL) + α(gL) + · · · + α(g1)|I0⟩\n= argmax\ncL∈CL\nmax\ng1∈G1 · · ·\nmax\ngL−1∈GL−1 ⟨µ(cL) + α(gL∗)\n|\n{z\n}\n≡µ(cL,gL∗)=µ(cL−1)\n+ · · · + α(g1)|I0⟩\n= argmax\ncL∈CL\nmax\ng1∈G1 · · ·\nmax\ngL−1∈GL−1 ⟨µ(cL−1) + α(gL−1) + · · · + α(g1)|I0⟩\n...\n≡argmax\ncL∈CL ⟨µ(cL, g∗)|I0⟩,\n(30)\nNote that we have explicitly shown the bias terms here, since they represent the additive muta-\ntions. In the last lines, we repeatedly use the distributivity of max over sums, resulting in the\n33\niteration\ngℓ+1(cℓ+1)∗≡argmax\ngℓ+1∈Gℓ+1⟨µ(cℓ+1, gℓ+1)\n|\n{z\n}\n≡W ℓ+1\n|I0⟩\n= argmax\ngℓ+1∈Gℓ+1⟨W ℓ+1(cℓ+1, gℓ+1)|I0⟩\n≡ChooseChild(Filter(I0)).\n(31)\nNote the key differences from the DRN/DCN inference derivation in Eq. 12: (i) the input\nto each layer is always the input image I0, (ii) the iterations go from coarse-to-ﬁne (from root\nancestor to leaf species) rather than ﬁne-to-coarse, and (iii) the resulting network is not a neural\nnetwork but rather a deep decision tree of single-layer neural networks. These differences\nare due to the special additive structure of the mutational nuisances and the evolutionary tree\nprocess underlying the generation of category templates.\n4.2.1\nWhat About the Leaf Histograms?\nThe mapping to a single decision tree is not yet complete; the leaf label histograms (5,38) are\nmissing. Analogous to the missing SoftMax regression layers with DCNs (Sec 2.4.1), the high-\nlevel representation class label cL inferred by the E-DRM in Eq. 30 need not be the training\ndata class label ˜c. For clarity, we treat the two as separate in general.\nBut then how do we understand cL? We can interpret the inferred conﬁguration τ ∗=\n(cL∗, g∗) as a disentangled representation of the input, wherein the different factors in τ ∗, in-\ncluding cL, vary independently in the world. In contrast to DCNs, the class labels ˜c in a decision\ntree are instead inferred from the discrete evolutionary path variable τ ∗through the use of the\nleaf histograms p(˜c|τ ∗). Note that decision trees also have label histograms at all internal (non-\nleaf) nodes, but that they are not needed for inference. However, they do play a critical role in\nlearning, as we will see below.\nWe are almost ﬁnished with our mapping from inference in Gaussian E-DRMs to decision\ntrees. To ﬁnish the mapping, we need only apply the discriminative relaxation (Eq. 25) in order\nto allow the weights and biases that deﬁne the decision functions in the internal nodes to be\nfree. Note that this is exactly analogous to steps in Section 2.7 for mapping from the Gaussian\nDRM to DCNs.\n4.3\nBootstrap Aggregation to Prevent Overﬁtting Yields A Decision\nForest\nThus far we have derived the inference algorithm for the E-DRM and shown that its discrimi-\nnative counterpart is indeed a single decision tree. But how to relate to this result to the entire\n34\nforest? This is important, since it is well known that individual decision trees are notoriously\ngood at overﬁtting data. Indeed, the historical motivation for introducing a forest of decision\ntrees has been in order to prevent such overﬁtting by averaging over many different models,\neach trained on a randomly drawn subset of the data. This technique is known as bootstrap ag-\ngregation or bagging for short, and was ﬁrst introduced by Breiman in the context of decision\ntrees (38). For completeness, in this section we review bagging, thus completing our mapping\nfrom the E-DRM to the RDF.\nIn order to derive bagging, it will be necessary in the following to make explicit the de-\npendence of learned inference parameters θ on the training data DCI ≡{(cn, In)}N\nn=1, i.e.\nθ = θ(DCI). This dependence is typically suppressed in most work, but is necessary here as\nbagging entails training different decision trees t on different subsets Dt ⊂D of the full training\ndata. In other words, θt = θt(Dt).\nMathematically, we perform inference as follows: Given all previously seen data DCI and\nan unseen image I, we classify I by computing the posterior distribution\np(c|I, DCI) =\nX\nA\np(c, A|I, DCI)\n=\nX\nA\np(c|I, DCI, A)p(A)\n≡EA[p(c|I, DCI, A)]\n(a)\n≈1\nT\nX\nt∈T\np(c|I, DCI, At)\n(b)\n= 1\nT\nX\nt∈T\nZ\ndθt p(c|I, θt) p(θt|DCI, At)\n(c)\n≈\n1\nT\nX\nt∈T\np(c|I, θ∗\nt )\n|\n{z\n}\nDecision Forest Classiﬁer\n,\nθ∗\nt ≡max\nθ\np(θ|DCI(At)).\n(32)\nHere At ≡(atn)N\nn=1 is a collection of switching variables that indicates which data points are\nincluded, i.e., atn = 1 if data point n is included in dataset Dt ≡DCI(At). In this way, we have\nrandomly subsampled the full dataset DCI (with replacement) T times in line (a), approximating\nthe true marginalization over all possible subsets of the data. In line (b), we perform Bayesian\nModel Averaging over all possible values of the E-DRM/decision tree parameters θt. Since this\nis intractable, we approximate it with the MAP estimate θ∗\nt in line (c). The overall result is\nthat each E-DRM (or decision tree) t is trained separately on a randomly drawn subset Dt ≡\nDCI(At) ⊂DCI of the entire dataset, and the ﬁnal output of the classiﬁer is an average over the\nindividual classiﬁers.\n35\n4.4\nEM Learning for the E-DRM Yields the InfoMax Principle\nOne approach to train an E-DRM classiﬁer is to maximize the mutual information between the\ngiven training labels ˜c and the inferred (partial) rendering path τ ℓ≡(cL, gL, . . . , gl) at each\nlevel. Note that ˜c and τ ℓare both discrete random variables.\nThis Mutual Information-based Classiﬁer (MIC) plays the same role as the Softmax regres-\nsion layer in DCNs, predicting the class labels ˜c given a good disentangled representation τ ℓ∗\nof the input I. In order to train the MIC classiﬁer, we update the classiﬁer parameters θMIC in\neach M-step as the solution to the optimization:\nmax\nθ\nMI(˜c, (cL, gL, . . . , g1)) = max\nθ1 · · · max\nθL\n1\nX\nl=L\nMI(˜c, gl\nn|gl+1\nn ; θl)\n=\n1\nX\nl=L\nmax\nθl\nMI(˜c, gl\nn|gl+1\nn ; θl)\n=\n1\nX\nl=L\nmax\nθl\nH[˜c] −H[˜c|gl\nn; θl]\n|\n{z\n}\n≡Information Gain\n.\n(33)\nHere MI(·, ·) is the mutual information between two random variables, H[·] is the entropy of a\nrandom variable, and θℓare the parameters at layer ℓ. In the ﬁrst line, we have used the layer-\nby-layer structure of the E-DRM to split the mutual information calculation across levels, from\ncoarse to ﬁne. In the second line, we have used the max-sum algorithm (dynamic programming)\nto split up the optimization into a sequence of optimizations from ℓ= L →ℓ= 1. In the third\nline, we have used the information-theoretic relationship MI(X, Y ) ≡H[X] −H[Y |X]. This\nalgorithm is known as InfoMax in the literature (5).\n5\nRelation to Prior Work\n5.1\nRelation to Mixture of Factor Analyzers\nAs mentioned above, on a high level, the DRM is related to hierarchical models based on the\nMixture of Factor Analyzers (MFA) (22). Indeed, if we add noise to each partial rendering step\nfrom level ℓto ℓ−1 in the DRM, then Eq. 11 becomes\nIℓ−1 ∼N\n\u0000Λℓ(gℓ)µℓ(cℓ) + αℓ(gℓ), Ψℓ\u0001\n,\n(34)\nwhere we have introduced the diagonal noise covariance Ψℓ. This is equivalent to the MFA\nmodel. The DRM and DMFA both employ parameter sharing, resulting in an exponential re-\nduction in the number of parameters, as compared to the collapsed or shallow version of the\nmodels. This serves as a strong regularizer to prevent overﬁtting.\n36\nDespite the high-level similarities, there are several essential differences between the DRM\nand the MFA-based models, all of which are critical for reproducing DCNs. First, in the DRM\nthe only randomness is due to the choice of the gℓand the observation noise after rendering.\nThis naturally leads to inference of the most probable conﬁguration via the max-sum algorithm,\nwhich is equivalent to max-pooling in the DCN. Second, the DRM’s afﬁne transformations Λℓ\nact on multi-channel images at level ℓ+ 1 to produce multi-channel images at level ℓ. This\nstructure is important, because it leads directly to the notion of (multi-channel) feature maps in\nDCNs. Third, a DRM’s layers vary in connectivity from sparse to dense, as they give rise to\nconvolutional, locally connected, and fully connected layers in the resulting DCN. Fourth, the\nDRM has switching variables that model (in)active renderers (Section 2.1). The manifestation\nof these variables in the DCN are the ReLus (Eq. 9). Thus, the critical elements of the DCN\narchitecture arise directly from aspects of the DRM structure that are absent in MFA-based\nmodels.\n5.2\ni-Theory: Invariant Representations Inspired by Sensory Cortex\nRepresentational Invariance and selectivity (RI) are important ideas that have developed in\nthe computational neuroscience community. According to this perspective, the main purpose\nof the feedforward aspects of visual cortical processing in the ventral stream are to compute\na representation for a sensed image that is invariant to irrelevant transformations (e.g., pose,\nlighting etc.) (40,41). In this sense, the RI perspective is quite similar to the DRM in its basic\nmotivations. However, the RI approach has remained qualitative in its explanatory power until\nrecently, when a theory of invariant representations in deep architectures — dubbed i-theory\n— was proposed (42, 43). Inspired by neuroscience and models of the visual cortex, it is the\nﬁrst serious attempt at explaining the success of deep architectures, formalizing intuitions about\ninvariance and selectivity in a rigorous and quantitatively precise manner.\nThe i-theory posits a representation that employs group averages and orbits to explicitly\ninsure invariance to speciﬁc types of nuisance transformations. These transformation must pos-\nsess a mathematical semi-group structure; as a result, the invariance constraint is relaxed to a\nnotion of partial invariance, which is built up slowly over multiple layers of the architecture.\nAt a high level, the DRM shares similar goals with i-theory in that it attempts to capture\nexplicitly the notion of nuisance transformations. However, the DRM differs from i-theory in\ntwo critical ways. First, it does not impose a semi-group structure on the set of nuisance trans-\nformations. This provides the DRM the ﬂexibility to learn a representation that is invariant to a\nwider class of nuisance transformations, including non-rigid ones. Second, the DRM does not\nﬁx the representation for images in advance. Instead, the representation emerges naturally out\nof the inference process. For instance, sum- and max-pooling emerge as probabilistic marginal-\n37\nization over nuisance variables and thus are necessary for proper inference. The deep iterative\nnature of the DCN also arises as a direct mathematical consequence of the DRM’s rendering\nmodel, which comprises multiple levels of abstraction.\nThis is the most important difference between the two theories. Despite these differences,\ni-theory is complementary to our approach in several ways, one of which is that it spends a good\ndeal of energy focusing on questions such as: How many templates are required for accurate\ndiscrimination? How many samples are needed for learning? We plan to pursue these questions\nfor the DRM in future work.\n5.3\nScattering Transform: Achieving Invariance via Wavelets\nWe have used the DRM, with its notion of target and nuisance variables, to explain the power\nof DCN for learning selectivity and invariance to nuisance transformations. Another theoretical\napproach to learning selectivity and invariance is the Scattering Transform (ST) (44,45), which\nconsists of a series of linear wavelet transforms interleaved by nonlinear modulus-pooling of\nthe wavelet coefﬁcients. The goal is to explicitly hand-design invariance to a speciﬁc set of\nnuisance transformations (translations, rotations, scalings, and small deformations) by using\nthe properties of wavelet transforms.\nIf we ignore the modulus-pooling for a moment, then the ST implicitly assumes that images\ncan be modeled as linear combinations of pre-determined wavelet templates. Thus the ST ap-\nproach has a maximally strong model bias, in that there is no learning at all. The ST performs\nwell on tasks that are consistent with its strong model bias, i.e., on small datasets for which\nsuccessful performance is therefore contingent on strong model bias. However, the ST will be\nmore challenged on difﬁcult real-world tasks with complex nuisance structure for which large\ndatasets are available. This contrasts strongly with the approach presented here and that of the\nmachine learning community at large, where hand-designed features have been outperformed\nby learned features in the vast majority of tasks.\n5.4\nLearning Deep Architectures via Sparsity\nWhat is the optimal machine learning architecture to use for a given task? This question has\ntypically been answered by exhaustively searching over many different architectures. But is\nthere a way to learn the optimal architecture directly from the data? Arora et al. (46) provide\nsome of the ﬁrst theoretical results in this direction. In order to retain theoretical tractability,\nthey assume a simple sparse neural network as the generative model for the data. Then, given the\ndata, they design a greedy learning algorithm that reconstructs the architecture of the generating\nneural network, layer-by-layer.\n38\nThey prove that their algorithm is optimal under a certain set of restrictive assumptions.\nIndeed, as a consequence of these restrictions, their results do not directly apply to the DRM or\nother plausible generative models of natural images. However, the core message of the paper\nhas nonetheless been inﬂuential in the development of the Inception architecture (13), which\nhas recently achieved the highest accuracy on the ImageNet classiﬁcation benchmark (25).\nHow does the sparse reconstruction approach relate to the DRM? The DRM is indeed also a\nsparse generative model: the act of rendering an image is approximated as a sequence of afﬁne\ntransformations applied to an abstract high-level class template. Thus, the DRM can potentially\nbe represented as a sparse neural network. Another similarity between the two approaches is the\nfocus on clustering highly correlated activations in the next coarser layer of abstraction. Indeed\nthe DRM is a composition of sparse factor analyzers, and so each higher layer ℓ+ 1 in a DCN\nreally does decorrelate and cluster the layer ℓbelow, as quantiﬁed by Eq. 18.\nBut despite these high-level similarities, the two approaches differ signiﬁcantly in their over-\nall goals and results. First, our focus has not been on recovering the architectural parameters;\ninstead we have focused on the class of architectures that are well-suited to the task of factor-\ning out large amounts of nuisance variation. In this sense the goals of the two approaches are\ndifferent and complementary. Second, we are able to derive the structure of DCNs and RDFs\nexactly from the DRM. This enables us to bring to bear the full power of probabilistic analy-\nsis for solving high-nuisance problems; moreover, it will enable us to build better models and\nrepresentations for hard tasks by addressing limitations of current approaches in a principled\nmanner.\n5.5\nGoogle FaceNet: Learning Useful Representations with DCNs\nRecently, Google developed a new face recognition architecture called FaceNet (47) that illus-\ntrates the power of learning good representations. It achieves state-of-the-art accuracy in face\nrecognition and clustering on several public benchmarks. FaceNet uses a DCN architecture, but\ncrucially, it was not trained for classiﬁcation. Instead, it is trained to optimize a novel learning\nobjective called triplet ﬁnding that learns good representations in general.\nThe basic idea behind their new representation-based learning objective is to encourage\nthe DCN’s latent representation to embed images of the same class close to each other while\nembedding images of different classes far away from each other, an idea that is similar to the\nNuMax algorithm (48). In other words, the learning objective enforces a well-separatedness\ncriterion. In light of our work connecting DRMs to DCNs, we will next show how this new\nlearning objective can be understood from the perspective of the DRM.\nThe correspondence between the DRM and the triplet learning objective is simple. Since\nrendering is a deterministic (or nearly noise-free) function of the global conﬁguration (c, g), one\n39\nexplanation should dominate for any given input image I = R(c, g), or equivalently, the clus-\nters (c, g) should be well-separated. Thus, the noise-free, deterministic, and well-separated\nDRM are all equivalent. Indeed, we implicitly used the well-separatedness criterion when\nwe employed the Hard EM algorithm to establish the correspondence between DRMs and\nDCNs/RDFs.\n5.6\nRenormalization Theory\nGiven the DRM’s notion of irrelevant (nuisance) transformations and multiple levels of abstrac-\ntion, we can interpret a DCN’s action as an iterative coarse-graining of an image, thus relating\nour work to another recent approach to understanding deep learning that draws upon an analogy\nfrom renormalization theory in physics (49). This approach constructs an exact correspondence\nbetween the Restricted Boltzmann Machine (RBM) and block-spin renormalization — an iter-\native coarse-graining technique from physics that compresses a conﬁguration of binary random\nvariables (spins) to a smaller conﬁguration with less variables. The goal is to preserve as much\ninformation about the longer-range correlations as possible, while integrating out shorter-range\nﬂuctuations.\nOur work here shows that this analogy goes even further as we have created an exact map-\nping between the DCN and the DRM, the latter of which can be interpreted as a new real-space\nrenormalization scheme. Indeed, the DRM’s main goal is to factor out irrelevant features over\nmultiple levels of detail, and it thus bears a strong resemblance to the core tenets of renormal-\nization theory. As a result, we believe this will be an important avenue for further research.\n5.7\nSummary of Key Distinguishing Features of the DRM\nThe key features that distinguish the DRM approach from others in the literature can be summa-\nrized as: (i) The DRM explicitly models nuisance variation across multiple levels of abstraction\nvia a product of afﬁne transformations. This factorized linear structure serves dual purposes:\nit enables (ii) exact inference (via the max-sum/max-product algorithm) and (iii) it serves as a\nregularizer, preventing overﬁtting by a novel exponential reduction in the number of parame-\nters. Critically, (iv) the inference is not performed for a single variable of interest but instead\nfor the full global conﬁguration. This is justiﬁed in low-noise settings, i.e., when the rendering\nprocess is nearly deterministic, and suggests the intriguing possibility that vision is less about\nprobabilities and more about inverting a complicated (but deterministic) rendering transforma-\ntion.\n40\n6\nNew Directions\nWe have shown that the DRM is a powerful generative model that underlies both DCNs and\nRDFs, the two most powerful vision paradigms currently employed in machine learning. De-\nspite the power of the DRM/DCN/RDF, it has limitations, and there is room for improvement.\n(Since both DCNs and RDFs stem from DRMs, we will loosely refer to them both as DCNs in\nthe following, although technically an RDF corresponds to a kind of tree of DCNs.)\nIn broad terms, most of the limitations of the DCN framework can be traced back to the\nfact that it is a discriminative classiﬁer whose underlying generative model was not known.\nWithout a generative model, many important tasks are very difﬁcult or impossible, including\nsampling, model reﬁnement, top-down inference, faster learning, model selection, and learning\nfrom unlabeled data. With a generative model, these tasks become feasible. Moreover, the\nDCN models rendering as a sequence of afﬁne transformations, which severely limits its ability\nto capture many important real-world visual phenomena, including ﬁgure-ground segmentation,\nocclusion/clutter, and refraction. It also lacks several operations that appear to be fundamental\nin the brain: feed-back, dynamics, and 3D geometry. Finally, it is unable to learn from unlabeled\ndata and to generalize from few examples. As a result, DCNs require enormous amounts of\nlabeled data for training.\nThese limitations can be overcome by designing new deep networks based on new model\nstructures (extended DRMs), new message-passing inference algorithms, and new learning\nrules, as summarized in Table 2. We now explore these solutions in more detail.\n6.1\nMore Realistic Rendering Models\nWe can improve DCNs by designing better generative models incorporating more realistic as-\nsumptions about the rendering process by which latent variables cause images. These assump-\ntions should include symmetries of translation, rotation, scaling (44), perspective, and non-rigid\ndeformations, as rendered by computer graphics and multi-view geometry.\nIn order to encourage more intrinsic computer graphics-based representations, we can en-\nforce these symmetries on the parameters during learning (50,51). Initially, we could use local\nafﬁne approximations to these transformations (52). For example, we could impose weight ty-\ning based on 3D rotations in depth. Other nuisance transformations are also of interest, such\nas scaling (i.e., motion towards or away from a camera). Indeed, scaling-based templates are\nalready in use by the state-of-the-art DCNs such as the Inception architectures developed by\nGoogle (13), and so this approach has already shown substantial promise.\nWe can also perform intrinsic transformations directly on 3D scene representations. For ex-\nample, we could train networks with depth maps, in which a subset of channels in input feature\nmaps encode pixel z-depth. These augmented input features will help deﬁne useful higher-level\n41\nArea \nProblem (DCN) \nProposed Solution (DRM) \nModel \nRendering model applies nuisance \ntransformations to extrinsic \nrepresentation (2D images or \nfeature maps). \nModify DRM so that rendering applies \nnuisance transformations to intrinsic \nrepresentations (e.g. 3D geometry). \n \nDifficulty handling occlusion, \nclutter and classifying objects that \nare slender, transparent, metallic.  \nModify DRM to include intrinsic \ncomputer-graphics-based \nrepresentations, transformations and \nphototrealistic rendering.  \n \nModel is static and thus cannot \nlearn from videos. \nIncorporate time into the DRM \n(Dynamic DRM). \nInference \nInfers most probable global \nconfiguration (max-product), \nignoring alternative hypotheses. \nUse softer message-passing, i.e. \nhigher temperature or sum-product, to \nencode uncertainty. \n \nNo top-down inference/feedback is \npossible, so vision tasks involving \nlower-level variables (e.g. clutter, \nocclusion, segmentation) are \ndifficult. \nCompute contextual priors as top-\ndown messages for low-level tasks. \nLearning \nHard-EM Algorithm and its \ndiscriminative relaxation tend to \nconfuse signal and noise \nUse Soft-EM or Variational Bayes-EM. \n \nNuisance variation makes learning \nintrinsic latent factors difficult. \nUse Dynamic DRM with movies to \nlearn that only a few nuisance \nparameters change per frame. \n \nDiscriminative models cannot \nlearn from unlabeled data. \nUse DRM to do hybrid generative-\ndiscriminative learning that \nsimultaneously incorporates labeled, \nunlabeled, and weakly labeled data. \nTable 2. Limitations of current DCNs and potential solutions using extended DRMs.\n42\nfeatures for 2D image features, and thereby transfer representational beneﬁts even to test im-\nages that do not provide depth information (53). With these richer geometric representations,\nlearning and inference algorithms can be modiﬁed to account for 3D constraints according to\nthe equations of multi-view geometry (53).\nAnother important limitation of the DCN is its restriction to static images. There is no notion\nof time or dynamics in the corresponding DRM model. As a result, DCN training on large-scale\ndatasets requires millions of images in order to learn the structure of high-dimensional nuisance\nvariables, resulting in a glacial learning process. In contrast, learning from natural videos should\nresult in an accelerated learning process, as typically only a few nuisance variables change from\nframe to frame. This property should enable substantial acceleration in learning, as inference\nabout which nuisance variables have changed will be faster and more accurate (54). See Sec-\ntion 6.3.2 below for more details.\n6.2\nNew Inference Algorithms\n6.2.1\nSoft Inference\nWe showed above in Section 2.4 that DCNs implicitly infer the most probable global interpre-\ntation of the scene, via the max-sum algorithm (55). However, there is potentially major com-\nponent missing in this algorithm: max-sum message passing only propagates the most likely\nhypothesis to higher levels of abstraction, which may not be the optimal strategy, in general,\nespecially if uncertainty in the measurements is high (e.g., vision in a fog or at nighttime). Con-\nsequently, we can consider a wider variety of softer inference algorithms by deﬁning a temper-\nature parameter that enables us to smoothly interpolate between the max-sum and sum-product\nalgorithms, as well as other message-passing variants such as the approximate Variational Bayes\nEM (56). To the best of our knowledge, this notion of a soft DCN is novel.\n6.2.2\nTop-Down Convolutional Nets: Top-Down Inference via the DRM\nThe DCN inference algorithm lacks any form of top-down inference or feedback. Performance\non tasks using low-level features is then suboptimal, because higher-level information informs\nlow-level variables neither for inference nor for learning. We can solve this problem by using\nthe DRM, since it is a proper generative model and thus enables us to implement top-down\nmessage passing properly.\nEmploying the same steps as outlined in Section 2, we can convert the DRM into a top-down\nDCN, a neural network that implements both the bottom-up and top-down passes of inference\nvia the max-sum message passing algorithm. This kind of top-down inference should have a\ndramatic impact on scene understanding tasks that require segmentation such as target detection\n43\nwith occlusion and clutter, where local bottom-up hypotheses about features are ambiguous. To\nthe best of our knowledge, this is the ﬁrst principled approach to deﬁning top-down DCNs.\n6.3\nNew Learning Algorithms\n6.3.1\nDerivative-Free Learning\nBack propagation is often used in deep learning algorithms due to its simplicity. We have\nshown above that back propagation in DCNs is actually an inefﬁcient implementation of an\napproximate EM algorithm, whose E-step consists of bottom-up inference and whose M-step\nis a gradient descent step that fails to take advantage of the underlying probabilistic model\n(the DRM). To the contrary, our above EM algorithm (Eqs. 18–23) is both much faster and\nmore accurate, because it directly exploits the DRM’s structure. Its E-step incorporates bottom-\nup and top-down inference, and its M-step is a fast computation of sufﬁcient statistics (e.g.,\nsample counts, means, and covariances). The speed-up in efﬁciency should be substantial, since\ngenerative learning is typically much faster than discriminative learning due to the bias-variance\ntradeoff (32); moreover, the EM-algorithm is intrinsically more parallelizable (57).\n6.3.2\nDynamics: Learning from Video\nAlthough deep NNs have incorporated time and dynamics for auditory tasks (58–60), DCNs\nfor visual tasks have remained predominantly static (images as opposed to videos) and are\ntrained on static inputs. Latent causes in the natural world tend to change little from frame-\nto-frame, such that previous frames serve as partial self-supervision during learning (61). A\ndynamic version of the DRM would train without external supervision on large quantities of\nvideo data (using the corresponding EM algorithm). We can supplement video recordings of\nnatural dynamic scenes with synthetically rendered videos of objects traveling along smooth\ntrajectories, which will enable the training to focus on learning key nuisance factors that cause\ndifﬁculty (e.g., occlusion).\n6.3.3\nTraining from Labeled and Unlabeled Data\nDCNs are purely discriminative techniques and thus cannot beneﬁt from unlabeled data. How-\never, armed with a generative model we can perform hybrid discriminative-generative train-\ning (31) that enables training to beneﬁt from both labeled and unlabeled data in a principled\nmanner. This should dramatically increase the power of pre-training, by encouraging rep-\nresentations of the input that have disentangled factors of variation. This hybrid generative-\ndiscriminative learning is achieved by the optimization of a novel objective function for learn-\ning, that relies on both the generative model and its discriminative relaxation. In particular, the\n44\nlearning objective will have terms for both, as described in (31). Recall from Section 2.7 that\nthe discriminative relaxation of a generative model is performed by relaxing certain parameter\nconstraints during learning, according to\nmax\nθ\nLgen(θ; DCI) = max\nη:η=ρ(θ) Lnat(η; DCI)\n≤max\nη:η=ρ(θ) Lcond(η; DC|I)\n≤max\nη\nLdis(η; DC|I),\n(35)\nwhere the L’s are the model’s generative, naturally parametrized generative, conditional, and\ndiscriminative likelihoods. Here η are the natural parameters expressed as a function of the\ntraditional parameters θ, DCI is the training dataset of labels and images, and DC|I is the training\ndataset of labels given images. Although the discriminative relaxation is optional, it is very\nimportant for achieving high performance in real-world classiﬁers as discriminative models\nhave less model bias and, therefore, are less sensitive to model mis-speciﬁcations (32). Thus,\nwe will design new principled training algorithms that span the spectrum from discriminative\n(e.g., Stochastic Gradient Descent with Back Propagation) to generative (e.g., EM Algorithm).\nAcknowledgments\nThanks to CJ Barberan for help with the manuscript and to Mayank Kumar, Ali Mousavi,\nSalman Asif and Andreas Tolias for comments and discussions. Thanks to Karen Simonyan\nfor providing the activity maximization ﬁgure. A special thanks to Xaq Pitkow whose keen\ninsight, criticisms and detailed feedback on this work have been instrumental in its develop-\nment. Thanks to Ruchi Kukreja for her unwavering support and her humor and to Raina Patel\nfor providing inspiration.\n45\nA\nSupplemental Information\nA.1\nFrom the Gaussian Rendering Model Classiﬁer to Deep DCNs\nProposition A.1 (MaxOut NNs). The discriminative relaxation of a noise-free GRM classiﬁer\nis a single layer NN consisting of a local template matching operation followed by a piecewise\nlinear activation function (also known as a MaxOut NN (10)).\nProof. In order to teach the reader, we prove this claim exhaustively. Later claims will have\nsimple proofs that exploit the fact that the RM’s distribution is from the exponential family.\nˆc(I) ≡argmax\nc∈C\np(c|I)\n= argmax\nc∈C\n{p(I|c)p(c)}\n= argmax\nc∈C\n(X\nh∈H\np(I|c, h)p(c, h)\n)\n(a)\n= argmax\nc∈C\n\u001a\nmax\nh∈H p(I|c, h)p(c, h)\n\u001b\n= argmax\nc∈C\n\u001a\nmax\nh∈H exp (ln p(I|c, h) + ln p(c, h))\n\u001b\n(b)\n= argmax\nc∈C\n(\nmax\nh∈H exp\n X\nω\nln p(Iω|c, h) + ln p(c, h)\n!)\n(c)\n= argmax\nc∈C\n(\nmax\nh∈H exp\n \n−1\n2\nX\nω\n\nIω −µω\nch|Σ−1\nch |Iω −µω\nch\n\u000b\n+ ln p(c, h) −D\n2 ln |Σch|\n!)\n= argmax\nc∈C\n(\nmax\nh∈H exp\n X\nω\n⟨wω\nch|Iω⟩+ bω\nch\n!)\n(d)\n≡argmax\nc∈C\n\u001a\nexp\n\u0012\nmax\nh∈H {wch ⋆LC I}\n\u0013\u001b\n= argmax\nc∈C\n\u001a\nmax\nh∈H {wch ⋆LC I}\n\u001b\n= Choose {MaxOutPool(LocalTemplateMatch(I))}\n= MaxOut-NN(I; θ).\nIn line (a), we take the noise-free limit of the GRM, which means that one hypothesis (c, h)\ndominates all others in likelihood. In line (b), we assume that the image I consists of multi-\nple channels ω ∈Ω, that are conditionally independent given the global conﬁguration (c, h).\n46\nTypically, for input images these are color channels and Ω≡{r, g, b} but in general Ωcan\nbe more abstract (e.g. as in feature maps). In line (c), we assume that the pixel noise covari-\nance is isotropic and conditionally independent given the global conﬁguration (c, h), so that\nΣch = σ2\nx1D is proportional to the D × D identity matrix 1D. In line (d), we deﬁned the locally\nconnected template matching operator ⋆LC , which is a location-dependent template matching\noperation.\nNote that the nuisance variables h ∈H are (max-)marginalized over, after the application\nof a local template matching operation against a set of ﬁlters/templates W ≡{wch}c∈C,h∈H\nLemma A.2 (Translational Nuisance →d DCN Convolution). The MaxOut template match-\ning and pooling operation (from Proposition A.1) for a set of translational nuisance variables\nH ≡GT reduces to the traditional DCN convolution and max-pooling operation.\nProof. Let the activation for a single output unit be yc(I). Then we have\nyc(I) ≡max\nh∈H {wch ⋆LC I}\n= max\ng∈GT {⟨wcg|I⟩}\n= max\ng∈GT {⟨Tgwc|I⟩}\n= max\ng∈GT {⟨wc|T−gI⟩}\n= max\ng∈GT {(wc ⋆DCN I)g}\n= MaxPool(wc ⋆DCN I).\nFinally, vectorizing in c gives us the desired result y(I) = MaxPool(W ⋆DCN I).\nProposition A.3 (Max Pooling DCNs with ReLu Activations). The discriminative relaxation\nof a noise-free GRM with translational nuisances and random missing data is a single convo-\nlutional layer of a traditional DCN. The layer consists of a generalized convolution operation,\nfollowed by a ReLu activation function and a Max-Pooling operation.\nProof. We will model completely random missing data as a nuisance transformation a ∈A ≡\n{keep, drop}, where a = keep = 1 leaves the rendered image data untouched, while a =\ndrop = 0 throws out the entire image after rendering. Thus, the switching variable a models\nmissing data. Critically, whether the data is missing is assumed to be completely random and\nthus independent of any other task variables, including the measurements (i.e. the image itself).\nSince the missingness of the evidence is just another nuisance, we can invoke Proposition A.1\nto conclude that the discriminative relaxation of a noise-free GRM with random missing data is\nalso a MaxOut-DCN, but with a specialized structure which we now derive.\n47\nMathematically, we decompose the nuisance variable h ∈H into two parts h = (g, a) ∈\nH = G × A, and then, following a similar line of reasoning as in Proposition A.1, we have\nˆc(I) = argmax\nc∈C\nmax\nh∈H p(c, h|I)\n= argmax\nc∈C\n\u001a\nmax\nh∈H {wch ⋆LC I}\n\u001b\n(a)\n= argmax\nc∈C\n\u001a\nmax\ng∈G max\na∈A\n\b\na(⟨wcg|I⟩+ bcg) + b′\ncg + ba + b′\nI\n\t\u001b\n(b)\n= argmax\nc∈C\n\u001a\nmax\ng∈G {max{(wc ⋆DCN I)g, 0} + b′\ncg + b′\ndrop + b′\nI}\n\u001b\n(c)\n= argmax\nc∈C\n\u001a\nmax\ng∈G {max{(wc ⋆DCN I)g, 0} + b′\ncg}\n\u001b\n(d)\n= argmax\nc∈C\n\u001a\nmax\ng∈G {max{(wc ⋆DCN I)g, 0}}\n\u001b\n= Choose {MaxPool(ReLu(DCNConv(I)))}\n= DCN(I; θ).\nIn line (a) we calculated the log-posterior\nln p(c, h|I) = ln p(c, g, a|I)\n= ln p(I|c, g, a) + ln p(c, g, a)\n=\n1\n2σ2\nx\n⟨aµcg|I⟩−\n1\n2σ2\nx\n(∥aµcg∥2\n2 + ∥I∥2\n2)) + ln p(c, g, a)\n≡a(⟨wcg|I⟩+ bcg) + b′\ncg + ba + b′\nI,\nwhere a ∈{0, 1}, ba ≡ln p(a), b′\ncg ≡ln p(c, g), b′\nI ≡−1\n2σ2x∥I∥2\n2. In line (b), we use Lemma A.2\nto write the expression in terms of the DCN convolution operator, after which we invoke the\nidentity max{u, v} = max{u −v, 0} + v ≡ReLu(u −v) + v for real numbers u, v ∈R. Here\nwe’ve deﬁned b′\ndrop ≡ln p(a = keep) and we’ve used a slightly modiﬁed DCN convolution\noperator ⋆DCN deﬁned by wcg ⋆DCN I ≡wcg ⋆I + ln\n\u0010\np(a=keep)\np(a=drop)\n\u0011\n. Also, we observe that all\nthe primed constants are independent of a and so can be pulled outside of the maxa. In line(c),\nthe two primed constants that are also independent of c, g can be dropped due to the argmaxcg.\nFinally, in line (d), we assume a uniform prior over c, g. The resulting sequence of operations\ncorresponds exactly to those applied in a single convolutional layer of a traditional DCN.\nRemark A.4 (The Probabilistic Origin of the Rectiﬁed Linear Unit). Note the origin of\nthe ReLu in the proof above: it compares the relative (log-)likelihood of two hypotheses\n48\na = keep and a = drop, i.e. whether the current measurements (image data I) are avail-\nable/relevant/important or instead missing/irrelevant/unimportant for hypothesis (c, g). In this\nway, the ReLu also promotes sparsity in the activations.\nA.2\nGeneralizing to Arbitrary Mixtures of Exponential Family\nDistributions\nIn the last section, we showed that the GRM – a mixture of Gaussian Nuisance Classiﬁers –\nhas as its discriminative relaxation a MaxOut NN. In this section, we generalize this result to an\narbitrary mixture of Exponential family Nuisance classiﬁers. For example, consider a Laplacian\nRM (LRM) or a Poisson RM (PRM).\nDeﬁnition A.5 (Exponential Family Distributions). A distribution p(x; θ) is in the exponential\nfamily if it can be written in the form\np(x; θ) = h(x) exp(⟨η(θ)|T(x)⟩−A(η)),\nwhere η(θ) is the vector of natural parameters, T(x)is the vector of sufﬁcient statistics,\nA(η(θ)) is the log-partition function.\nBy generalizing to the exponential family, we will see that derivations of the discriminative\nrelations will simplify greatly, with the key roles being played by familiar concepts such as nat-\nural parameters, sufﬁcient statistics and log-partition functions. Furthermore, most importantly,\nwe will see that the resulting discriminative counter parts are still MaxOut NNs. Thus MaxOut\nNNs are quite a robust class, as most E-family mixtures have MaxOut NNs as d-counterparts.\nTheorem A.6 (Discriminative Counterparts to Exponential Family Mixtures are MaxOut Neu-\nral Nets). Let Mg be a Nuisance Mixture Classiﬁer from the Exponential Family. Then the\ndiscriminative counterpart Md of Mg is a MaxOut NN.\nProof. The proof is analogous to the proof of Proposition A.1, except we generalize by using\nthe deﬁnition of an exponential family distribution (above). We simply use the fact that all\nexponential family distributions have a natural or canonical form as described above in the\nDeﬁnition A.5. Thus the natural parameters will serve as generalized weights and biases, while\nthe sufﬁcient statistic serves as the generalized input. Note that this may require a non-linear\ntransformation i.e. quadratic or logarithmic, depending on the speciﬁc exponential family.\nA.3\nRegularization Schemes: Deriving the DropOut Algorithm\nDespite the large amount of labeled data available in many real-world vision applications of\ndeep DCNs, regularization schemes are still a critical part of training, essential for avoiding\n49\noverﬁtting the data. The most important such scheme is DropOut (30) and it consist of training\nwith unreliable neurons and synapses. Unreliability is modeled by a ‘dropout’ probability pd\nthat the neuron will not ﬁre (i.e. output activation is zero) or that the synapse won’t send its\noutput to the receiving neuron. Intuitively, downstream neurons cannot rely on every piece of\ndata/evidence always being there, and thus are forced to develop a robust set of features. This\nprevents the co-adaptation of feature detectors that undermines generalization ability.\nIn this section, we answer the question: Can we derive the DropOut algorithm from the\ngenerative modeling perspective? Here we show that the answer is yes. Dropout can be de-\nrived from the GRM generative model via the use of the EM algorithm under the condition of\n(completely random) missing data.\nProposition A.7. The discriminative relaxation of a noise-free GRM with completely random\nmissing data is a DropOut DCN (18) with Max-Pooling.\nProof. Since we have data that is missing completely at random, we can use the EM algorithm\nto train the GRM (56). Our strategy is to show that a single iteration of the EM-algorithm\ncorresponds to a full epoch of DropOut DCN training (i.e. one pass thru the entire dataset).\nNote that typically an EM-algorithm is used to train generative models; here we utilize the EM-\nalgorithm in a novel way, performing a discriminative relaxation in the M-step. In this way, we\nuse the generative EM algorithm to deﬁne a discriminative EM algorithm (d-EM).\nThe d-E-step is equivalent to usual generative E-step. Given the observed data X and the\ncurrent parameter estimate ˆθt, we will compute the posterior of the latent variables Z = (H, A)\nwhere A is the missing data indicator matrix i.e. Anp = 1 iff the p-th feature (e.g. pixel\nintensity) of the input data In (e.g. natural image) is available. H contains all other latent\nnuisance variables (e.g. pose) that are important for the classiﬁcation task. Since we assume a\nnoise-free GRM, we will actually execute a hybrid E-step: hard in H and soft in A. The hard-E\nstep will yield the Max-Sum Message Passing algorithm, while the soft E-step will yield the\nensemble average that is the characteristic feature of Dropout (18).\nIn the d-M-step, we will start out by maximizing the complete-data log-likelihood\nℓ(θ; H, A, X), just as in the usual generative M-step. However, near the end of the deriva-\ntion we will employ a discriminative relaxation that will free us from the rigid distributional\nassumptions of the generative model θg and instead leave us with a much more ﬂexible set of\nassumptions, as embodied in the discriminative modeling problem for θd.\nMathematically, we have a single E-step and M-step that leads to a parameter update as\n50\nfollows:\nℓ(ˆθnew) ≡max\nθ\nn\nEZ|X[ℓ(θ; Z, X)]\no\n= max\nθ\nn\nEAEH|X[ℓ(θ; H, A, X)]\no\n= max\nθ\nn\nEAEH|X\nh\nℓ(θ; C, H|I, A) + ℓ(θ; I) + ℓ(θ; A)\nio\n= max\nθd∼dθg\nn\nEAEH|X\nh\nℓ(θd; C, H|I, A) + ℓ(θg; I)\nio\n≤max\nθd\nn\nEAEH|X\nh\nℓ(θd; C, H|I, A)\nio\n= max\nθd\nn\nEAMH|X\nh\nℓ(θd; C, H|I, A)\nio\n≡max\nθd\nn\nEA\nh\nℓ(θd; C, H∗|I, A)\nio\n= max\nθd\nn X\nA\np(A) · ℓ(θd; C, H∗|I, A)\no\n≈max\nθd\nn X\nA∈T\np(A) · ℓ(θd; C, H∗|I, A)\no\n= max\nθd\nn X\nA∈T\np(A) ·\nX\nn∈Ddropout\nCI\nln p(cn, h∗\nn|Idropout\nn\n; θd)\no\n.\nHere we have deﬁned the conditional likelihood ℓ(θ; D1|D2) ≡ln p(D1|D2; θ), and D =\n(D1, D2) is some partition of the data.\nThis deﬁnition allows us to write ℓ(θ; D)\n=\nℓ(θ; D1|D2) + ℓ(θ; D2) by invoking the conditional probability law p(D|θ) = p(D1|D2; θ) ·\np(D2|θ).\nThe symbol MH|X[f(H)]\n≡\nmaxH{p(H|X)f(H)} and the reduced dataset\nDdropout\nCI\n(A) is simply the original dataset of labels and features less the missing data (as speciﬁed\nby A).\nThe ﬁnal objective function left for us to optimize is a mixture of exponentially-many dis-\ncriminative models, each trained on a different random subset of the training data, but all sharing\nparameters (weights and biases). Since the sum over A is intractable, we approximate the sums\nby Monte Carlo sampling of A (the soft part of the E-step), yielding an ensemble E ≡{A(i)}.\nThe resulting optimization corresponds exactly to the DropOut algorithm.\n51\nReferences and Notes\n1. J. Schmidhuber, “Deep learning in neural networks: An overview,” Neural Networks,\nvol. 61, pp. 85–117, 2015.\n2. M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional networks,” in\nComputer Vision–ECCV 2014.\nSpringer, 2014, pp. 818–833.\n3. A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh,\nS. Sengupta, A. Coates et al., “Deepspeech: Scaling up end-to-end speech recognition,”\narXiv preprint arXiv:1412.5567, 2014.\n4. H. Schmid, “Part-of-speech tagging with neural networks,” in Proceedings of the 15th\nConference on Computational Linguistics - Volume 1, ser. COLING ’94.\nStroudsburg,\nPA, USA: Association for Computational Linguistics, 1994, pp. 172–176. [Online].\nAvailable: http://dx.doi.org/10.3115/991886.991915\n5. A. Criminisi and J. Shotton, Decision Forests for Computer Vision and Medical Image\nAnalysis, ser. Advances in Computer Vision and Pattern Recognition.\nSpringer London,\n2013. [Online]. Available: https://books.google.com/books?id=F6a-NAEACAAJ\n6. D. Grifﬁths and M. Tenenbaum, “Hierarchical topic models and the nested chinese restau-\nrant process,” Advances in neural information processing systems, vol. 16, p. 17, 2004.\n7. J. H. Searcy and J. C. Bartlett, “Inversion and processing of component and spatial-\nrelational information in faces.” Journal of experimental psychology. Human perception\nand performance, vol. 22, no. 4, pp. 904–915, Aug. 1996.\n8. M. I. Jordan and T. J. Sejnowski, Graphical models: Foundations of neural computation.\nMIT press, 2001.\n9. Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review and new per-\nspectives,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8,\npp. 1798–1828, 2013.\n10. I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, “Maxout net-\nworks,” arXiv preprint arXiv:1302.4389, 2013.\n11. A. Krizhevsky, I. Sutskever, and G. Hinton, “ImageNet Classiﬁcation with Deep Convolu-\ntional Neural Networks,” NIPS, pp. 1–9, Nov. 2012.\n52\n12. K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun, “What is the best multi-stage\narchitecture for object recognition?” in Computer Vision, 2009 IEEE 12th International\nConference on.\nIEEE, 2009, pp. 2146–2153.\n13. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,\nand A. Rabinovich, “Going deeper with convolutions,” arXiv preprint arXiv:1409.4842,\n2014.\n14. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to docu-\nment recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n15. Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the gap to human-level\nperformance in face veriﬁcation,” in Computer Vision and Pattern Recognition (CVPR),\n2014 IEEE Conference on.\nIEEE, 2014, pp. 1701–1708.\n16. J. L¨ucke and A.-S. Sheikh, “Closed-form em for sparse coding and its application to source\nseparation,” in Latent Variable Analysis and Signal Separation.\nSpringer, 2012, pp. 213–\n221.\n17. I. Goodfellow, A. Courville, and Y. Bengio, “Large-scale feature learning with spike-and-\nslab sparse coding,” arXiv preprint arXiv:1206.6407, 2012.\n18. G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural networks for lvcsr\nusing rectiﬁed linear units and dropout,” in Acoustics, Speech and Signal Processing\n(ICASSP), 2013 IEEE International Conference on.\nIEEE, 2013, pp. 8609–8613.\n19. J. B. Tenenbaum, C. Kemp, T. L. Grifﬁths, and N. D. Goodman, “How to grow a mind:\nStatistics, structure, and abstraction,” science, vol. 331, no. 6022, pp. 1279–1285, 2011.\n20. Y. Tang, R. Salakhutdinov, and G. Hinton, “Deep mixtures of factor analysers,” arXiv\npreprint arXiv:1206.4635, 2012.\n21. A. van den Oord and B. Schrauwen, “Factoring variations in natural images with deep\ngaussian mixture models,” in Advances in Neural Information Processing Systems, 2014,\npp. 3518–3526.\n22. Z. Ghahramani, G. E. Hinton et al., “The em algorithm for mixtures of factor analyzers,”\nTechnical Report CRG-TR-96-1, University of Toronto, Tech. Rep., 1996.\n23. A. Hyv¨arinen, J. Karhunen, and E. Oja, Independent component analysis.\nJohn Wiley &\nSons, 2004, vol. 46.\n53\n24. F. R. Kschischang, B. J. Frey, and H.-A. Loeliger, “Factor graphs and the sum-product\nalgorithm,” Information Theory, IEEE Transactions on, vol. 47, no. 2, pp. 498–519, 2001.\n25. S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by\nreducing internal covariate shift,” arXiv preprint arXiv:1502.03167, 2015.\n26. P. F. Felzenszwalb and D. P. Huttenlocher, “Efﬁcient belief propagation for early vision,”\nInternational journal of computer vision, vol. 70, no. 1, pp. 41–54, 2006.\n27. G. Hinton, “What’s wrong with convolutional nets?” 2014, available from the MIT TechTV\nwebsite.\n28. S. Roweis and Z. Ghahramani, “Learning nonlinear dynamical systems using the\nexpectation–maximization algorithm,” Kalman ﬁltering and neural networks, p. 175, 2001.\n29. T. V´amos, “Judea pearl: Probabilistic reasoning in intelligent systems,” Decision Support\nSystems, vol. 8, no. 1, pp. 73–75, 1992.\n30. G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Im-\nproving neural networks by preventing co-adaptation of feature detectors,” arXiv preprint\narXiv:1207.0580, 2012.\n31. C. M. Bishop, J. Lasserre et al., “Generative or discriminative? getting the best of both\nworlds,” Bayesian Statistics, vol. 8, pp. 3–24, 2007.\n32. A. Jordan, “On discriminative vs. generative classiﬁers: A comparison of logistic regression\nand naive bayes,” Advances in neural information processing systems, vol. 14, p. 841, 2002.\n33. B. M. Wilamowski, S. Iplikci, O. Kaynak, and M. ¨O. Efe, “An algorithm for fast conver-\ngence in training neural networks,” in Proceedings of the international joint conference on\nneural networks, vol. 2, 2001, pp. 1778–1782.\n34. O. Capp´e and E. Moulines, “Online em algorithm for latent data models,” Journal of the\nRoyal Statistical Society, 2008.\n35. M. Jordan, Learning in Graphical Models, ser. Adaptive computation and machine\nlearning.\nLondon,\n1998. [Online]. Available:\nhttps://books.google.com/books?id=\nzac7L4LbNtUC\n36. D. L. Yamins, H. Hong, C. F. Cadieu, E. A. Solomon, D. Seibert, and J. J. DiCarlo,\n“Performance-optimized hierarchical models predict neural responses in higher visual cor-\ntex,” Proceedings of the National Academy of Sciences, vol. 111, no. 23, pp. 8619–8624,\n2014.\n54\n37. K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional networks: Visu-\nalising image classiﬁcation models and saliency maps,” arXiv preprint arXiv:1312.6034,\n2013.\n38. L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp. 5–32, 2001.\n39. N.-Q. Pham, H.-S. Le, D.-D. Nguyen, and T.-G. Ngo, “A study of feature combination in\ngesture recognition with kinect,” in Knowledge and Systems Engineering.\nSpringer, 2015,\npp. 459–471.\n40. N. Pinto, D. D. Cox, and J. J. DiCarlo, “Why is Real-World Visual Object Recognition\nHard?” PLoS Computational Biology, vol. 4, no. 1, p. e27, 2008.\n41. J. J. DiCarlo, D. Zoccolan, and N. C. Rust, “Perspective,” Neuron, vol. 73, no. 3, pp. 415–\n434, Feb. 2012.\n42. F. Anselmi, J. Mutch, and T. Poggio, “Magic Materials,” Proceedings of the National\nAcademy of Sciences, vol. 104, no. 51, pp. 20 167–20 172, Dec. 2007.\n43. F. Anselmi, L. Rosasco, and T. Poggio, “On invariance and selectivity in representation\nlearning,” arXiv preprint arXiv:1503.05938, 2015.\n44. J. Bruna and S. Mallat, “Invariant scattering convolution networks,” Pattern Analysis and\nMachine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1872–1886, 2013.\n45. S. Mallat, “Group invariant scattering,” Communications on Pure and Applied Mathemat-\nics, vol. 65, no. 10, pp. 1331–1398, 2012.\n46. S. Arora, A. Bhaskara, R. Ge, and T. Ma, “Provable bounds for learning some deep repre-\nsentations,” arXiv preprint arXiv:1310.6343, 2013.\n47. F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A uniﬁed embedding for face recog-\nnition and clustering,” arXiv preprint arXiv:1503.03832, 2015.\n48. C. Hegde, A. Sankaranarayanan, W. Yin, and R. Baraniuk, “A convex approach for learning\nnear-isometric linear embeddings,” preparation, August, 2012.\n49. P. Mehta and D. J. Schwab, “An exact mapping between the variational renormalization\ngroup and deep learning,” arXiv preprint arXiv:1410.3831, 2014.\n50. X. Miao and R. P. Rao, “Learning the lie groups of visual invariance,” Neural computation,\nvol. 19, no. 10, pp. 2665–2693, 2007.\n55\n51. F. Anselmi, J. Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, and T. Poggio, “Unsuper-\nvised learning of invariant representations in hierarchical architectures,” arXiv preprint\narXiv:1311.4158, 2013.\n52. J. Sohl-Dickstein, J. C. Wang, and B. A. Olshausen, “An unsupervised algorithm for learn-\ning lie group transformations,” arXiv preprint arXiv:1001.1027, 2010.\n53. R. Hartley and A. Zisserman, Multiple view geometry in computer vision.\nCambridge\nuniversity press, 2003.\n54. V. Michalski, R. Memisevic, and K. Konda, “Modeling sequential data using higher-order\nrelational features and predictive training,” arXiv preprint arXiv:1402.2333, 2014.\n55. J. Pearl, “Probabilistic reasoning in intelligent systems: Networks of plausible inference.\nmorgan kauffman pub,” 1988.\n56. C. M. Bishop et al., Pattern recognition and machine learning.\nspringer New York, 2006,\nvol. 4, no. 4.\n57. N. Kumar, S. Satoor, and I. Buck, “Fast parallel expectation maximization for gaussian mix-\nture models on gpus using cuda,” in High Performance Computing and Communications,\n2009. HPCC’09. 11th IEEE International Conference on.\nIEEE, 2009, pp. 103–109.\n58. S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9,\nno. 8, pp. 1735–1780, 1997.\n59. A. Graves, A.-R. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural\nnetworks,” in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International\nConference on.\nIEEE, 2013, pp. 6645–6649.\n60. A. Graves, N. Jaitly, and A.-R. Mohamed, “Hybrid speech recognition with deep bidi-\nrectional lstm,” in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE\nWorkshop on.\nIEEE, 2013, pp. 273–278.\n61. L. Wiskott, “How does our visual system achieve shift and size invariance,” JL van Hemmen\nand TJ Sejnowski, editors, vol. 23, pp. 322–340, 2006.\n56\n",
  "categories": [
    "stat.ML",
    "cs.CV",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2015-04-02",
  "updated": "2015-04-02"
}