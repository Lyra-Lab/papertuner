{
  "id": "http://arxiv.org/abs/1504.01716v3",
  "title": "An Empirical Evaluation of Deep Learning on Highway Driving",
  "authors": [
    "Brody Huval",
    "Tao Wang",
    "Sameep Tandon",
    "Jeff Kiske",
    "Will Song",
    "Joel Pazhayampallil",
    "Mykhaylo Andriluka",
    "Pranav Rajpurkar",
    "Toki Migimatsu",
    "Royce Cheng-Yue",
    "Fernando Mujica",
    "Adam Coates",
    "Andrew Y. Ng"
  ],
  "abstract": "Numerous groups have applied a variety of deep learning techniques to\ncomputer vision problems in highway perception scenarios. In this paper, we\npresented a number of empirical evaluations of recent deep learning advances.\nComputer vision, combined with deep learning, has the potential to bring about\na relatively inexpensive, robust solution to autonomous driving. To prepare\ndeep learning for industry uptake and practical applications, neural networks\nwill require large data sets that represent all possible driving environments\nand scenarios. We collect a large data set of highway data and apply deep\nlearning and computer vision algorithms to problems such as car and lane\ndetection. We show how existing convolutional neural networks (CNNs) can be\nused to perform lane and vehicle detection while running at frame rates\nrequired for a real-time system. Our results lend credence to the hypothesis\nthat deep learning holds promise for autonomous driving.",
  "text": "1\nAn Empirical Evaluation of Deep Learning on\nHighway Driving\nBrody Huval∗, Tao Wang∗, Sameep Tandon∗, Jeff Kiske∗, Will Song∗, Joel Pazhayampallil∗,\nMykhaylo Andriluka∗, Pranav Rajpurkar∗, Toki Migimatsu∗, Royce Cheng-Yue†,\nFernando Mujica‡, Adam Coates§, Andrew Y. Ng∗\n∗Stanford University\n†Twitter\n‡Texas Instruments\n§Baidu Research\nAbstract—Numerous groups have applied a variety of deep\nlearning techniques to computer vision problems in highway\nperception scenarios. In this paper, we presented a number of\nempirical evaluations of recent deep learning advances. Com-\nputer vision, combined with deep learning, has the potential\nto bring about a relatively inexpensive, robust solution to au-\ntonomous driving. To prepare deep learning for industry uptake\nand practical applications, neural networks will require large\ndata sets that represent all possible driving environments and\nscenarios. We collect a large data set of highway data and apply\ndeep learning and computer vision algorithms to problems such\nas car and lane detection. We show how existing convolutional\nneural networks (CNNs) can be used to perform lane and vehicle\ndetection while running at frame rates required for a real-time\nsystem. Our results lend credence to the hypothesis that deep\nlearning holds promise for autonomous driving.\nI. INTRODUCTION\nSince the DARPA Grand Challenges for autonomous vehi-\ncles, there has been an explosion in applications and research\nfor self-driving cars. Among the different environments for\nself-driving cars, highway and urban roads are on opposite\nends of the spectrum. In general, highways tend to be more\npredictable and orderly, with road surfaces typically well-\nmaintained and lanes well-marked. In contrast, residential or\nurban driving environments feature a much higher degree of\nunpredictability with many generic objects, inconsistent lane-\nmarkings, and elaborate trafﬁc ﬂow patterns. The relative\nregularity and structure of highways has facilitated some of the\nﬁrst practical applications of autonomous driving technology.\nMany automakers have begun pursuing highway auto-pilot\nsolutions designed to mitigate driver stress and fatigue and\nto provide additional safety features; for example, certain\nadvanced-driver assistance systems (ADAS) can both keep\ncars within their lane and perform front-view car detection.\nCurrently, the human drivers retain liability and, as such,\nmust keep their hands on the steering wheel and prepare to\ncontrol the vehicle in the event of any unexpected obstacle or\ncatastrophic incident. Financial considerations contribute to a\nsubstantial performance gap between commercially available\nauto-pilot systems and fully self-driving cars developed by\nGoogle and others. Namely, today’s self-driving cars are\nequipped with expensive but critical sensors, such as LIDAR,\nradar and high-precision GPS coupled with highly detailed\nmaps.\nIn today’s production-grade autonomous vehicles, critical\nsensors include radar, sonar, and cameras. Long-range vehicle\nFig. 1: Sample output from our neural network capable of lane\nand vehicle detection.\ndetection typically requires radar, while nearby car detection\ncan be solved with sonar. Computer vision can play an\nimportant a role in lane detection as well as redundant object\ndetection at moderate distances. Radar works reasonably well\nfor detecting vehicles, but has difﬁculty distinguishing between\ndifferent metal objects and thus can register false positives on\nobjects such as tin cans. Also, radar provides little orientation\ninformation and has a higher variance on the lateral position\nof objects, making the localization difﬁcult on sharp bends.\nThe utility of sonar is both compromised at high speeds and,\neven at slow speeds, is limited to a working distance of about\n2 meters. Compared to sonar and radar, cameras generate a\nricher set of features at a fraction of the cost. By advancing\ncomputer vision, cameras could serve as a reliable redundant\nsensor for autonomous driving. Despite its potential, computer\nvision has yet to assume a signiﬁcant role in today’s self-\ndriving cars. Classic computer vision techniques simply have\nnot provided the robustness required for production grade\nautomotives; these techniques require intensive hand engineer-\ning, road modeling, and special case handling. Considering\nthe seemingly inﬁnite number of speciﬁc driving situations,\nenvironments, and unexpected obstacles, the task of scaling\nclassic computer vision to robust, human-level performance\nwould prove monumental and is likely to be unrealistic.\nDeep learning, or neural networks, represents an alternative\narXiv:1504.01716v3  [cs.RO]  17 Apr 2015\n2\napproach to computer vision. It shows considerable promise\nas a solution to the shortcomings of classic computer vision.\nRecent progress in the ﬁeld has advanced the feasibility\nof deep learning applications to solve complex, real-world\nproblems; industry has responded by increasing uptake of such\ntechnology. Deep learning is data centric, requiring heavy\ncomputation but minimal hand-engineering. In the last few\nyears, an increase in available storage and compute capabilities\nhave enabled deep learning to achieve success in supervised\nperception tasks, such as image detection. A neural network,\nafter training for days or even weeks on a large data set, can\nbe capable of inference in real-time with a model size that is\nno larger than a few hundred MB [9]. State-of-the-art neural\nnetworks for computer vision require very large training sets\ncoupled with extensive networks capable of modeling such\nimmense volumes of data. For example, the ILSRVC data-set,\nwhere neural networks achieve top results, contains 1.2 million\nimages in over 1000 categories.\nBy using expensive existing sensors which are currently\nused for self-driving applications, such as LIDAR and mm-\naccurate GPS, and calibrating them with cameras, we can\ncreate a video data set containing labeled lane-markings and\nannotated vehicles with location and relative speed. By build-\ning a labeled data set in all types of driving situations (rain,\nsnow, night, day, etc.), we can evaluate neural networks on this\ndata to determine if it is robust in every driving environment\nand situation for which we have training data.\nIn this paper, we detail empirical evaluation on the data set\nwe collect. In addition, we explain the neural network that we\napplied for detecting lanes and cars, as shown in Figure 1.\nII. RELATED WORK\nRecently, computer vision has been expected to player a\nlarger role within autonomous driving. However, due to its\nhistory of relatively low precision, it is typically used in\nconjunction with either other sensors or other road models\n[3], [4], [6], [7]. Cho et al. [3] uses multiple sensors, such\nas LIDAR, radar, and computer vision for object detection.\nThey then fuse these sensors together in a Kalman ﬁlter using\nmotion models on the objects. Held et al. [4], uses only a\ndeformable parts based model on images to get the detections,\nthen uses road models to ﬁlter out false positives. Caraﬁi et al.\n[6] uses a WaldBoost detector along with a tracker to generate\npixel space detections in real time. Jazayeri et al. [7] relies on\ntemporal information of features for detection, and then ﬁlters\nout false positives with a front-view motion model.\nIn contrast to these object detectors, we do not use any\nroad or motion-based models; instead we rely only on the\nrobustness of a neural network to make reasonable predictions.\nIn addition, we currently do not rely on any temporal features,\nand the detector operates independently on single frames from\na monocular camera. To make up for the lack of other sensors,\nwhich estimate object depth, we train the neural network to\npredict depth based on labels extracted from radar returns.\nAlthough the model only predicts a single depth value for\neach object, Eigen et al. have shown how a neural network\ncan predict entire depth maps from single images [12]. The\nnetwork we train likely learns some model of the road for\nobject detection and depth predictions, but it is never explicitly\nengineered and instead learns from the annotations alone.\nBefore the wide spread adoption of Convolutional Neu-\nral Networks (CNNs) within computer vision, deformable\nparts based models were the most successful methods for\ndetection [13]. After the popular CNN model AlexNet [9]\nwas proposed, state-of-the-art detection shifted towards CNNs\nfor feature extraction [1], [14], [10], [15]. Girshick et al.\ndeveloped R-CNN, a two part system which used Selective\nSearch [16] to propose regions and AlexNet to classify them.\nR-CNN achieved state-of-the-art on Pascal by a large margin;\nhowever, due to its nearly 1000 classiﬁcation queries and\ninefﬁcient re-use of convolutions, it remains impractical for\nreal-time implementations. Szegedy et al. presented a more\nscalable alternative to R-CNN, that relies on the CNN to\npropose higher quality regions compared to Selective Search.\nThis reduces the number of region proposals down to as low as\n79 while keeping the mAP competitive with Selective Search.\nAn even faster approach to image detection called Overfeat\nwas presented by Sermanet et al. [1]. By using a regular\npattern of “region proposals”, Overfeat can efﬁciently reuse\nconvolution computations from each layer, requiring only a\nsingle forward pass for inference.\nFor our empirical evaluation, we use a straight-forward\napplication of Overfeat, due to its efﬁciencies, and combine\nthis with labels similar to the ones proposed by Szegedy et al..\nWe describe the model and similarities in the next section.\nIII. REAL TIME VEHICLE DETECTION\nConvolutional Neural Networks (CNNs) have had the\nlargest success in image recognition in the past 3 years [9],\n[17], [18], [19]. From these image recognition systems, a\nnumber of detection networks were adapted, leading to further\nadvances in image detection. While the improvements have\nbeen staggering, not much consideration had been given to\nthe real-time detection performance required for some appli-\ncations. In this paper, we present a detection system capable\nof operating at greater than 10Hz using nothing but a laptop\nGPU. Due to the requirements of highway driving, we need\nto ensure that the system used can detect cars more than\n100m away and can operate at speeds greater than 10Hz; this\ndistance requires higher image resolutions than is typically\nused, and in our case is 640 × 480. We use the Overfeat\nCNN detector, which is very scalable, and simulates a sliding\nwindow detector in a single forward pass in the network by\nefﬁciently reusing convolutional results on each layer [1].\nOther detection systems, such as R-CNN, rely on selecting\nas many as 1000 candidate windows, where each is evaluated\nindependently and does not reuse convolutional results.\nIn our implementation, we make a few minor modiﬁcations\nto Overfeat’s labels in order to handle occlusions of cars, pre-\ndictions of lanes, and accelerate performance during inference.\nWe will ﬁrst provide a brief overview of the original imple-\nmentation and will then address the modiﬁcations. Overfeat\nconverts an image recognition CNN into a “sliding window”\ndetector by providing a larger resolution image and trans-\nforming the fully connected layers into convolutional layers.\n3\nThen, after converting the fully connected layer, which would\nhave produced a single ﬁnal feature vector, to a convolutional\nlayer, a grid of ﬁnal feature vectors is produced. Each of the\nresulting feature vectors represents a slightly different context\nview location within the original pixel space. To determine\nthe stride of this window in pixel space, it is possible to\nsimply multiply the strides on each convolutional or pool layer\ntogether. The network we used has a stride size of 32 pixels.\nEach ﬁnal feature vector in this grid can predict the presence\nof an object; once an object is detected, those same features are\nthen used to predict a single bounding box through regression.\nThe classiﬁer will predict “no-object” if it can not discern any\npart of an object within its entire input view. This causes large\nambiguities for the classiﬁer, which can only predict a single\nobject, as two different objects could can easily appear in the\ncontext view of the ﬁnal feature vector, which is typically\nlarger than 50% of the input image resolution.\nThe network we used has a context view of 355×355 pixels\nin size. To ensure that all objects in the image are classiﬁed\nat least once, many different context views are taken of the\nimage by using skip gram kernels to reduce the stride of the\ncontext views and by using up to four different scales of the\ninput image. The classiﬁer is then trained to activate when an\nobject appears anywhere within its entire context view. In the\noriginal Overfeat paper, this results in 1575 different context\nviews (or ﬁnal feature vectors), where each one is likely to\nbecome active (create a bounding box).\nThis creates two problems for our empirical evaluation. Due\nto the L2 loss between the predicted bounding box and actual\nbounding proposed by Sermanet et al., the ambiguity of having\ntwo valid bounding box locations to predict when two objects\nappear, is incorrectly handled by the network by predicting a\nbox in the center of the two objects to minimize its expected\nloss. These boxes tend to cause a problem for the bounding\nbox merging algorithm, which incorrectly decides that there\nmust be a third object between the two ground truth objects.\nThis could cause problems for an ADAS system which falsely\nbelieves there is a car where there is not, and emergency\nbreaking is falsely applied. In addition, the merging algorithm,\nused only during inference, operates in O(n2) where n is the\nnumber of bounding boxes proposed. Because the bounding\nbox merging is not as easily parallelizable as the CNN, this\nmerging may become the bottleneck of a real-time system in\nthe case of an inefﬁcient implementation or too many predicted\nbounding boxes.\nIn our evaluations, we use a mask detector as described\nin Szegedy et al. [10] to improve some of the issues with\nOverfeat as described above. Szegedy et al. proposes a CNN\nthat takes an image as input and outputs an object mask\nthrough regression, highlighting the object location. The idea\nof a mask detector is shown in Fig 2. To distinguish multiple\nnearby objects, different part-detectors output object masks,\nfrom which bounding boxes are then extracted. The detector\nthey propose must take many crops of the image, and run\nmultiple CNNs for each part on every crop. Their resulting\nimplementation takes roughly 5-6 seconds per frame per class\nusing a 12-core machine, which would be too slow for our\napplication.\nCNN\nFig. 2: mask detector\nWe combine these ideas by using the efﬁcient “sliding\nwindow” detector of Overfeat to produce an object mask and\nperform bounding box regression. This is shown in Fig 3.\nIn this implementation, we use a single image resolution of\n640 × 480 with no skip gram kernels. To help the ambiguity\nproblem, and reduce the number of bounding boxes predicted,\nwe alter the detector on the top layer to only activate within\na 4 × 4 pixel region at the center of its context view, as\nshown in the ﬁrst box in Fig 3. Because it’s highly unlikely\nthat any two different object’s bounding boxes appear in a\n4 × 4 pixel region, compared to the entire context view with\nOverfeat, the bounding box regressor will no longer have to\narbitrarily choose between two valid objects in its context\nview. In addition, because the requirement for the detector to\nﬁre is stricter, this produces many fewer bounding boxes which\ngreatly reduces our run-time performance during inference.\nAlthough these changes helped, ambiguity was still a com-\nmon problem on the border of bounding boxes in the cases of\nocclusion. This ambiguity results in a false bounding box being\npredicted between the two ground truth bounding boxes. To\nﬁx this problem, the bounding boxes were ﬁrst shrunk by 75%\nbefore creating the detection mask label. This added the addi-\ntional requirement that the center 4×4-pixel region of the de-\ntector window had to be within the center region of the object\nbefore activating. The bounding box regressor however, still\npredicts the original bounding box before shrinking. This also\nfurther reduces the number of active bounding boxes as input\nto our merging algorithm. We also found that switching from\nL2 to L1 loss on the bounding box regressions results in better\nperformance. To merge the bounding boxes together, we used\nOpenCV’s efﬁcient implementation of groupRectangles,\nwhich clusters the bounding boxes based on a similarity metric\nin O(n2) [8].\nThe lower layers of our CNN we use for feature extraction\nis similar to the one proposed by Krizhevsky et al. [9]. Our\nmodiﬁcations to the network occurs on the dense layers which\nare converted to convolution, as described in Sermanet et\nal. [1]. When using our larger image sizes of 640 × 480\nthis changes the previous ﬁnal feature response maps of size\n1 × 1 × 4096 to 20 × 15 × 4096. As stated earlier, each of\nthese feature vectors sees a context region of 355×355 pixels,\nand the stride between them is 32 × 32 pixels; however, we\nwant each making predictions at a resolution of 4 × 4 pixels,\nwhich would leave gaps in our input image. To ﬁx this, we use\neach 4096 feature as input to 64 softmax classiﬁers, which are\narranged in an 8×8 grid each predicting if an object is within\na different 4×4 pixel region. This allows for the 4096 feature\nvector to cover the full stride size of 32 × 32 pixels; the end\n4\nMask Detector \nResult\nBounding Box \nRegression\nDetector\nContext\n“Sliding Window”\nFig. 3: overfeat-mask\nresult is a grid mask detector of size 160 × 120 where each\nelement is 4×4 pixels which covers the entire input image of\nsize 640 × 480.\nA. Lane Detection\nThe CNN used for vehicle detection can be easily extended\nfor lane boundary detection by adding an additional class.\nWhereas the regression for the vehicle class predicts a ﬁve\ndimensional value (four for the bounding box and one for\ndepth), the lane regression predicts six dimensions. Similar to\nthe vehicle detector, the ﬁrst four dimensions indicate the two\nend points of a local line segment of the lane boundary. The\nremaining two dimensions indicate the depth of the endpoints\nwith respect to the camera. Fig 4 visualizes the lane boundary\nground truth label overlaid on an example image. The green\ntiles indicate locations where the detector is trained to ﬁre,\nand the line segments represented by the regression labels are\nexplicitly drawn. The line segments have their ends connected\nto form continuous splines. The depth of the line segments\nare color-coded such that the closest segments are red and\nthe furthest ones are blue. Due to our data collection methods\nfor lane labels, we are able to obtain ground truth in spite of\nobjects that occlude them. This forces the neural network to\nlearn more than a simple paint detector, and must use context\nto predict lanes where there are occlusions.\nSimilar to the vehicle detector, we use L1 loss to train\nthe regressor. We use mini-batch stochastic gradient descent\nfor optimization. The learning rate is controlled by a variant\nof the momentum scheduler [11]. To obtain semantic lane\ninformation, we use DBSCAN to cluster the line segments\ninto lanes. Fig 5 shows our lane predictions after DBSCAN\nclustering. Different lanes are represented by different colors.\nSince our regressor outputs depths as well, we can predict the\nlane shapes in 3D using inverse camera perspective mapping.\nIV. EXPERIMENTAL SETUP\nA. Data Collection\nOur Research Vehicle is a 2014 Inﬁniti Q50. The car\ncurrently uses the following sensors: 6x Point Grey Flea3 cam-\neras, 1x Velodyne HDL32E lidar, and 1x Novatel SPAN-SE\nFig. 4: Example of lane boundary ground truth\nFig. 5: Example output of lane detector after DBSCAN\nclustering\nReceiver. We also have access to the Q50 built-in Continental\nmid-range radar system. The sensors are connected to a Linux\nPC with a Core i7-4770k processor.\nOnce the raw videos are collected, we annotate the 3D\nlocations for vehicles and lanes as well as the relative speed\nof all the vehicles. To get vehicle annotations, we follow the\nconventional approach of using Amazon Mechanical Turk to\nget accurate bounding box locations within pixel space. Then,\nwe match bounding boxes and radar returns to obtain the\ndistance and relative speed of the vehicles.\n5\nUnlike vehicles that can be annotated with bounding boxes,\nhighway lane borders often need to be annotated as curves\nof various shapes. This makes frame-level labelling not only\ntedious and inefﬁcient, but also prone to human errors. For-\ntunately, lane markings can be considered as static objects\nthat do not change their geolocations very often. We follow\nthe process descried in [5] to create LIDAR maps of the\nenvironment using the Velodyne and GNSS systems. Using\nthese maps, labeling is straight forward. First, we ﬁlter the\n3D point clouds based on lidar return intensity and position\nto obtain the left and right boundaries of the ego-lane. Then,\nwe replicate the left and right ego-lane boundaries to obtain\ninitial guesses for all the lane boundaries. A human annotator\ninspects the generated lane boundaries and makes appropriate\ncorrections using our 3D labelling tool. For completeness, we\ndescribe each of these steps in details.\n1) Ego-lane boundary generation: Since we do not change\nlanes during data collection drives, the GPS trajectory of our\nresearch vehicle already gives a decent estimate of the shape\nof the road. We can then easily locate the ego-lane boundaries\nusing a few heuristic ﬁlters. Noting that lane boundaries on\nhighways are usually marked with retro-reﬂective materials,\nwe ﬁrst ﬁlter out low-reﬂectivity surfaces such as asphalt\nin our 3D point cloud maps and only consider points with\nhigh enough laser return intensities. We then ﬁlter out other\nreﬂective surfaces such as cars and trafﬁc signs by only consid-\nering points whose heights are close enough the ground plane.\nLastly, assuming our car drives close to the center of the lane,\nwe ﬁlter out ground paint other than the ego-lane boundaries,\nsuch as other lane boundaries, car-pool or directional signs,\nby only considering markings whose absolute lateral distances\nfrom the car are smaller than 2.2 meters and greater than 1.4\nmeters. We can also distinguish the left boundary from the\nright one using the sign of the lateral distance. After obtaining\nthe points in the left and right boundaries, we ﬁt a piecewise\nlinear curve similar to the GPS trajectory to each boundary.\n2) Semi-automatic generation of multiple lane boundaries:\nWe observe that the width of lanes during a single data\ncollection run stays constant most of the time, with occasional\nexceptions such as merges and splits. Therefore, if we prede-\nﬁne the number of lanes to the left and right of the car for a\nsingle run, we can make a good initial guess of all the lane\nboundaries by shifting the auto-generated ego-lane boundaries\nlaterally by multiples of the lane width. We will then rely on\nhuman annotators to ﬁx the exception cases.\nB. Data Set\nAt the time of this writing our annotated data-set consists\nof 14 days of driving in the San Francisco Bay Area during\nthe months of April-June for a few hours each day. The\nvehicle annotated data is sampled at 1/3Hz and contains nearly\n17 thousand frames with 140 thousand bounding boxes. The\nlane annotated data is sampled at 5Hz and contains over 616\nthousand frames. During training, translation and 7 different\nperspective distortions are applied to the raw data sets. Fig 6\nshows an example image after perspective distortions are\napplied. Note that we apply the same perspective distortion\nFig. 6: Image after perspective distortion\nto the ground truth labels so that they match correctly with\nthe distorted image.\nC. Results\nThe detection network used is capable of running at 44Hz\nusing a desktop PC equipped with a GTX 780 Ti. When using\na mobile GPU, such as the Tegra K1, we were capable of\nrunning the network at 2.5Hz, and would expect the system\nto run at 5Hz using the Nvidia PX1 chipset.\nOur lane detection test set consists of 22 video clips\ncollected using both left and right cameras during 11 different\ndata collection runs, which correspond to about 50 minutes of\ndriving. We evaluate detection results for four lane boundaries,\nnamely, the left and right boundaries of the ego lane, plus\nthe outer boundaries of the two adjacent lanes. For each of\nthese lane boundaries, we further break down the evaluation\nby longitudinal distances, which range from 15 to 80 meters\nahead of the car, spaced by 5 meters. Thus, there are at\nmaximum 4 × 14 = 56 positions at which we evaluate the\ndetection results. We pair up the prediction and ground truth\npoints at each of these locations using greedy nearest neighbor\nmatching. True positives, false positives and false negatives are\naccumulated at every evaluation location in a standard way:\nA true positive is counted when the matched prediction and\nground truth differ by less than 0.5 meter. If the matched\nprediction and ground truth differ by more than 0.5 meter,\nboth false positive and false negative counts are incremented.\nFig 7 shows a visualization of this evaluation method on\none image. The blue dots are true positives. The red dots are\nfalse positives, and the yellow ones are false negatives. Fig 8\nshows the aggregated precision, recall and F1 score on all test\nvideos. For the ego-lane boundaries, we obtain 100% F1 score\nup to 50 meters. Recall starts to drop fast beyond 65 meters,\nmainly because the resolution of the image cannot capture the\nwidth of the lane markings at that distance. For the adjacent\nlanes, recall is low for the nearest point because it is outside\nthe ﬁeld of view of the camera.\nThe vehicle detection test set consists of 13 video clips\ncollected from a single day, which corresponds to 1 hour\n6\nFig. 7: Left: lane prediction on test image. Right: Lane\ndetection evaluated in 3D\n(a)\n(b)\n(c)\n(d)\nFig. 8: Lane detection results on different lateral lanes. (a)\nEgo-lane left border. (b) Ego-lane right border. (c) Left adja-\ncent lane left border. (d) Right adjacent lane right border.\nand 30 mins of driving. The accuracy of the vehicle bound-\ning box predictions were measured using Intersection Over\nUnion (IOU) against the ground truth boxes from Amazon\nMechanical Turk (AMT). A bounding box prediction matched\nwith ground truth if IOU≥0.5. The performance of our\ncar detection as a function of depth can be seen in Fig 9.\nNearby false positives can cause the largest problems for\nADAS systems which could cause the system to needlessly\napply the brakes. In our system, we found overpasses and\nshading effects to cause the largest problems. Two examples\nof these situations are shown in Fig 10.\nAs a baseline to our car detector, we compared the detection\nresults to the Continental mid-range radar within our data\ncollection vehicle. While matching radar returns to ground\ntruth bounding boxes, we found that although radar had nearly\n100% precision, false positives were being introduced through\nerrors in radar/camera calibration. Therefore, to ensure a fair\ncomparison we matched every radar return to a ground truth\nbounding box even if IOU< 0.5, giving our radar returns 100%\nprecision. This comparison is shown in Fig 11, the F1 score\nfor radar is simply the recall.\nIn addition to the bounding box locations, we measured\nthe accuracy of the predicted depth by using radar returns as\nFig. 9: Car Detector Bounding Box Performance\n(a) FP: tree\n(b) FP: overpass\nFig. 10: Vehicle False Positives\nground truth. The standard error in the depth predictions as a\nfunction of depth can be seen in Fig 12.\nFor a qualitative review of the detection system, we have\nuploaded a 1.5 hour video of the vehicle detector ran on our\ntest set. This may be found at youtu.be/GJ0cZBkHoHc. A\nshort video of our lane detector may also be found online\nat youtu.be/__f5pqqp6aM. In these videos, we evaluate the\ndetector on every frame independently and display the raw\ndetections, without the use of any Kalman ﬁlters or road\nmodels. The red locations in the video correspond to the mask\ndetectors that are activated. This network was only trained on\nthe rear view of cars traveling in the same direction, which is\nwhy cars across the highway barrier are commonly missed.\nWe have open sourced the code for the vehicle and lane\nFig. 11: Radar Comparison to Vehicle Detector\n7\nFig. 12: Car Detector Depth Performance\ndetector online at github.com/brodyh/caffe. Our repository was\nforked from the original Caffe code base from the BVLC\ngroup [20].\nV. CONCLUSION\nBy using Camera, Lidar, Radar, and GPS we built a highway\ndata set consisting of 17 thousand image frames with vehicle\nbounding boxes and over 616 thousand image frames with\nlane annotations. We then trained on this data using a CNN\narchitecture capable of detecting all lanes and cars in a single\nforward pass. Using a single GTX 780 Ti our system runs\nat 44Hz, which is more than adequate for real-time use. Our\nresults show existing CNN algorithms are capable of good\nperformance in highway lane and vehicle detection. Future\nwork will focus on acquiring frame level annotations that will\nallow us to develop new neural networks capable of using\ntemporal information across frames.\nACKNOWLEDGMENT\nThis research was funded in part by Nissan who generously\ndonated the car used for data collection. We thank our col-\nleagues Yuta Yoshihata from Nissan who provided technical\nsupport and expertise on vehicles that assisted the research.\nIn addition, the authors would like to thank the author of\nOverfeat, Pierre Sermanet, for their helpful suggestions on\nimage detection.\nREFERENCES\n[1] Sermanet, Pierre, et al. ”Overfeat: Integrated recognition, localization and\ndetection using convolutional networks.” arXiv preprint arXiv:1312.6229\n(2013).\n[2] Rothengatter, Talib Ed, and Enrique Carbonell Ed Vaya. ”Trafﬁc and\ntransport psychology: Theory and application.” International Conference\nof Trafﬁc and Transport Psychology, May, 1996, Valencia, Spain. Perga-\nmon/Elsevier Science Inc, 1997.\n[3] Cho, Hyunggi, et al. ”A multi-sensor fusion system for moving object\ndetection and tracking in urban driving environments.” Robotics and\nAutomation (ICRA), 2014 IEEE International Conference on. IEEE,\n2014.\n[4] Held, David, Jesse Levinson, and Sebastian Thrun. ”A probabilistic\nframework for car detection in images using context and scale.” Robotics\nand Automation (ICRA), 2012 IEEE International Conference on. IEEE,\n2012.\n[5] Levinson, Jesse, et al. ”Towards fully autonomous driving: systems and\nalgorithms.” Intelligent Vehicles Symposium, 2011.\n[6] Carafﬁ, Claudio, et al. ”A system for real-time detection and tracking of\nvehicles from a single car-mounted camera.” Intelligent Transportation\nSystems (ITSC), 2012 15th International IEEE Conference on. IEEE,\n2012.\n[7] Jazayeri, Amirali, et al. ”Vehicle detection and tracking in car video based\non motion model.” Intelligent Transportation Systems, IEEE Transactions\non 12.2 (2011): 583-595.\n[8] Bradski, Gary. ”The opencv library.” Doctor Dobbs Journal 25.11 (2000):\n120-126.\n[9] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ”Imagenet\nclassiﬁcation with deep convolutional neural networks.” Advances in\nneural information processing systems. 2012.\n[10] Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. ”Deep neural\nnetworks for object detection.” Advances in Neural Information Process-\ning Systems. 2013.\n[11] Sutskever, Ilya, et al. ”On the importance of initialization and momentum\nin deep learning.” Proceedings of the 30th International Conference on\nMachine Learning (ICML-13). 2013.\n[12] Eigen, David, Christian Puhrsch, and Rob Fergus. ”Depth map prediction\nfrom a single image using a multi-scale deep network.” Advances in\nNeural Information Processing Systems. 2014.\n[13] Felzenszwalb, Pedro F., et al. ”Object detection with discriminatively\ntrained part-based models.” Pattern Analysis and Machine Intelligence,\nIEEE Transactions on 32.9 (2010): 1627-1645.\n[14] Szegedy, Christian, et al. ”Scalable, High-Quality Object Detection.”\narXiv preprint arXiv:1412.1441 (2014).\n[15] Girshick, Ross, et al. ”Rich feature hierarchies for accurate object\ndetection and semantic segmentation.” Computer Vision and Pattern\nRecognition (CVPR), 2014 IEEE Conference on. IEEE, 2014.\n[16] Uijlings, Jasper RR, et al. ”Selective search for object recognition.”\nInternational journal of computer vision 104.2 (2013): 154-171.\n[17] Szegedy, Christian, et al. ”Going deeper with convolutions.” arXiv\npreprint arXiv:1409.4842 (2014).\n[18] He,\nKaiming,\net\nal.\n”Delving\nDeep\ninto\nRectiﬁers:\nSurpassing\nHuman-Level Performance on ImageNet Classiﬁcation.” arXiv preprint\narXiv:1502.01852 (2015).\n[19] Simonyan, Karen, and Andrew Zisserman. ”Very deep convolutional net-\nworks for large-scale image recognition.” arXiv preprint arXiv:1409.1556\n(2014).\n[20] Jia, Yangqing, et al. ”Caffe: Convolutional architecture for fast feature\nembedding.” Proceedings of the ACM International Conference on Mul-\ntimedia. ACM, 2014.\n",
  "categories": [
    "cs.RO",
    "cs.CV"
  ],
  "published": "2015-04-07",
  "updated": "2015-04-17"
}