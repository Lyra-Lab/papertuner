{
  "id": "http://arxiv.org/abs/1504.02518v2",
  "title": "Unsupervised Feature Learning from Temporal Data",
  "authors": [
    "Ross Goroshin",
    "Joan Bruna",
    "Jonathan Tompson",
    "David Eigen",
    "Yann LeCun"
  ],
  "abstract": "Current state-of-the-art classification and detection algorithms rely on\nsupervised training. In this work we study unsupervised feature learning in the\ncontext of temporally coherent video data. We focus on feature learning from\nunlabeled video data, using the assumption that adjacent video frames contain\nsemantically similar information. This assumption is exploited to train a\nconvolutional pooling auto-encoder regularized by slowness and sparsity. We\nestablish a connection between slow feature learning to metric learning and\nshow that the trained encoder can be used to define a more temporally and\nsemantically coherent metric.",
  "text": "Accepted as a workshop contribution at ICLR 2015\nUNSUPERVISED FEATURE LEARNING FROM TEMPO-\nRAL DATA\nRoss Goroshin1, Joan Bruna1,2, Jonathan Tompson1, David Eigen1, Yann LeCun1,2\n1Courant Institute of Mathematical Science 719 Broadway 12th Floor, New York, NY 10003\n2Facebook AI Research, 770 Broadway, New York, NY 10003\n{goroshin,bruna,tompson,deigen,yann}@cims.nyu.edu\nCurrent state-of-the-art classiﬁcation and detection algorithms rely on supervised training. In this\nwork we study unsupervised feature learning in the context of temporally coherent video data. We\nfocus on feature learning from unlabeled video data, using the assumption that adjacent video frames\ncontain semantically similar information. This assumption is exploited to train a convolutional pool-\ning auto-encoder regularized by slowness and sparsity. We establish a connection between slow\nfeature learning to metric learning and show that the trained encoder can be used to deﬁne a more\ntemporally and semantically coherent metric.\nOur main assumption is that data samples that are temporal neighbors are also likely to be neighbors\nin the latent space. For example, adjacent frames in a video sequence are more likely to be seman-\ntically similar than non-adjacent frames. This assumption naturally leads to the slowness prior on\nfeatures which was introduced in SFA (Wiskott & Sejnowski (2002)).\nTemporal coherence can be exploited by assuming a prior on the features extracted from the temporal\ndata sequence. One such prior is that the features should vary slowly with respect to time. In the\ndiscrete time setting this prior corresponds to minimizing an Lp norm of the difference of feature\nvectors for temporally adjacent inputs. Consider a video sequence with T frames, if zt represents the\nfeature vector extracted from the frame at time t then the slowness prior corresponds to minimizing\nPT\nt=1 ∥zt −zt−1∥p. To avoid the degenerate solution zt = z0 for t = 1...T, a second term is\nintroduced which encourages data samples that are not temporal neighbors to be separated by at\nleast a distance of m-units in feature space, where m is known as the margin. In the temporal setting\nthis corresponds to minimizing max(0, m −∥zt −zt′∥p), where |t −t′| > 1. Together the two\nterms form the loss function introduced in Hadsell et al. (2006) as a dimension reduction and data\nvisualization algorithm known as DrLIM. Assume that there is a differentiable mapping from input\nspace to feature space which operates on individual temporal samples. Denote this mapping by G\nand assume it is parametrized by a set of trainable coefﬁcients denoted by W. That is, zt = GW (xt).\nThe per-sample loss function can be written as:\nL(xt, xt′, W) =\n\u001a\n∥GW (xt) −GW (xt′)∥p,\nif |t −t′| = 1\nmax(0, m −∥GW (xt) −GW (xt′)∥p)\nif |t −t′| > 1\n(1)\nThe second contrastive term in Equation 1 only acts to avoid the degenerate solution in which GW is\na constant mapping, it does not guarantee that the resulting feature space is informative with respect\nto the input. This discriminative criteria only depends on pairwise distances in the representation\nspace which is a geometrically weak notion in high dimensions. We propose to replace this con-\ntrastive term with a term that penalizes the reconstruction error of both data samples. Introducing a\nreconstruction terms not only prevents the constant solution but also acts to explicitly preserve infor-\nmation about the input. This is a useful property of features which are obtained using unsupervised\nlearning; since the task to which these features will be applied is not known a priori, we would like\nto preserve as much information about the input as possible.\nWhat is the optimal architecture of GW for extracting slow features? Slow features are invariant to\ntemporal changes by deﬁnition. In natural video and on small spatial scales these changes mainly\ncorrespond to local translations and deformations. Invariances to such changes can be achieved us-\ning appropriate pooling operators Bruna & Mallat (2013); LeCun et al. (1998). Such operators are\nat the heart of deep convolutional networks (ConvNets), currently the most successful supervised\nfeature learning architectures Krizhevsky et al. (2012). Inspired by these observations, let GWe be a\ntwo stage encoder comprised of a learned, generally over-complete, linear map (We) and rectifying\nnonlinearity f(·), followed by a local pooling. Let the N hidden activations, h = f(Wex), be sub-\n1\narXiv:1504.02518v2  [cs.CV]  15 Apr 2015\nAccepted as a workshop contribution at ICLR 2015\n(a)\n(b)\nFigure 1: Pooled decoder dictionaries learned without (a) and with (b) the L1 penalty using (2).\n L₁\n L₁\nReconstruction\nReconstruction\nL₂-Pooling\nL₂-Pooling\nDecoder\n  Filters\nEncoder\n  Filters\nXt\nXt₊₁\nht\nht₊₁\nDecoder\n  Filters\nEncoder\n  Filters\n \nSlowness\nFigure 2: Block diagram of the Siamese convolutional model trained on pairs of frames.\ndivided into K potentially overlapping neighborhoods denoted by Pi. Note that biases are absorbed\nby expressing the input x in homogeneous coordinates. Feature zi produced by the encoder for the\ninput at time t can be expressed as Gi\nWe(t) = ∥ht∥Pi\np\n=\n\u0010P\nj∈Pi hp\ntj\n\u0011 1\np . Training through a local\npooling operator enforces a local topology on the hidden activations, inducing units that are pooled\ntogether to learn complimentary features. In the following experiments we will use p = 2. Although\nit has recently been shown that it is possible to recover the input when We is sufﬁciently redundant,\nreconstructing from these coefﬁcients corresponds to solving a phase recovery problem Bruna et al.\n(2014) which is not possible with a simple inverse mapping, such as a linear map Wd. Instead of\nreconstructing from z we reconstruct from the hidden representation h. This is the same approach\ntaken when training group-sparse auto-encoders Kavukcuoglu et al. (2009). In order to promote\nsparse activations in the case of over-complete bases we additionally add a sparsifying L1 penalty\non the hidden activations. Including the rectifying nonlinearity becomes critical for learning sparse\ninference in a hugely redundant dictionary, e.g. convolutional dictionaries Gregor & LeCun (2010).\nThe complete loss functional is:\nL(xt, xt′, W) =\nX\nτ={t,t′}\n\u0000∥Wdhτ −xτ∥2 + α|hτ|\n\u0001\n+ β\nK\nX\ni=1\n\f\f∥ht∥Pi −∥ht′∥Pi\f\f\n(2)\nFigure 2 shows a convolutional version of the proposed architecture and loss. By replacing all linear\noperators in our model with convolutional ﬁlter banks and including spatial pooling, translation\ninvariance need not be learned LeCun et al. (1998). In all other respects the convolutional model is\nconceptually identical to the fully connected model described in the previous section.\nREFERENCES\nBengio, Yoshua, Courville, Aaron C., and Vincent, Pascal. Representation learning: A review and new per-\nspectives. Technical report, University of Montreal, 2012.\nBromley, Jane, Bentz, James W, Bottou, L´eon, Guyon, Isabelle, LeCun, Yann, Moore, Cliff, S¨ackinger, Eduard,\nand Shah, Roopak. Signature veriﬁcation using a siamese time delay neural network. International Journal\nof Pattern Recognition and Artiﬁcial Intelligence, 7(04):669–688, 1993.\n2\nAccepted as a workshop contribution at ICLR 2015\nBruna, Joan and Mallat, St´ephane. Invariant scattering convolution networks. Pattern Analysis and Machine\nIntelligence, IEEE Transactions on, 35(8):1872–1886, 2013.\nBruna, Joan, Szlam, Arthur, and LeCun, Yann. Signal recovery from pooling representations. In ICML, 2014.\nCadieu, Charles F. and Olshausen, Bruno A. Learning intermediate-level representations of form and motion\nfrom natural movies. Neural Computation, 2012.\nGoodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net-\nworks. In ICML, 2013.\nGoroshin, Rostislav and LeCun, Yann. Saturating auto-encoders. In ICLR, 2013.\nGregor, Karol and LeCun, Yann. Learning fast approximations of sparse coding. In ICML’2010, 2010.\nHadsell, Raia, Chopra, Soumit, and LeCun, Yann. Dimensionality reduction by learning an invariant mapping.\nIn CVPR, 2006.\nHyv¨arinen, Aapo, Karhunen, Juha, Oja, and Erkki. Independent component analysis, volume 46. John Wiley\n& Sons, 2004.\nHyv¨arinen, Aapo, Hurri, Jarmo, and V¨ayrynen, Jaakko. Bubbles: a unifying framework for low-level statistical\nproperties of natural image sequences. JOSA A, 20(7):1237–1252, 2003.\nKavukcuoglu, Koray, Ranzato, MarcAurelio, Fergus, Rob, and LeCun, Yann.\nLearning invariant features\nthrough topographic ﬁlter maps. In CVPR, 2009.\nKayser, Christoph, Einhauser, Wolfgang, Dummer, Olaf, Konig, Peter, and Kding, Konrad. Extracting slow\nsubspaces from natural videos leads to complex cells. In ICANN’2001, 2001.\nKrizhevsky, Alex. Learning multiple layers of features from tiny images. Master’s thesis, University of Toronto,\nApril 2009.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional\nneural networks. In NIPS, volume 1, pp. 4, 2012.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.\nProc. IEEE, 86(11):2278–2324, 1998.\nLies, Jorn-Philipp, Hafner, Ralf M, and Bethge, Matthias. Slowness and sparseness have diverging effects on\ncomplex cell learning. 10, 2014.\nMobahi, Hossein, Collobert, Ronan, and Weston, Jason. Deep learning from temporal coherence in video. In\nICML, 2009.\nRifai, Salah, Vincent, Pascal, Muller, Xavier, Galrot, Xavier, and Bengio, Yoshua. Contractive auto-encoders:\nExplicit invariance during feature extraction. In ICML, 2011.\nVincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and composing\nrobust features with denoising autoencoders. Technical report, University of Montreal, 2008.\nWiskott, Laurenz and Sejnowski, Terrence J. Slow feature analysis: Unsupervised learning of invariances.\nNeural Computation, 2002.\nZou, Will, Zhu, Shenghuo, Yu, Kai, and Ng, Andrew Y. Deep learning of invariant features via simulated\nﬁxations in video. In Advances in Neural Information Processing Systems, pp. 3212–3220, 2012.\n3\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2015-04-09",
  "updated": "2015-04-15"
}