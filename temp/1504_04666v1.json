{
  "id": "http://arxiv.org/abs/1504.04666v1",
  "title": "Unsupervised Dependency Parsing: Let's Use Supervised Parsers",
  "authors": [
    "Phong Le",
    "Willem Zuidema"
  ],
  "abstract": "We present a self-training approach to unsupervised dependency parsing that\nreuses existing supervised and unsupervised parsing algorithms. Our approach,\ncalled `iterated reranking' (IR), starts with dependency trees generated by an\nunsupervised parser, and iteratively improves these trees using the richer\nprobability models used in supervised parsing that are in turn trained on these\ntrees. Our system achieves 1.8% accuracy higher than the state-of-the-part\nparser of Spitkovsky et al. (2013) on the WSJ corpus.",
  "text": "Unsupervised Dependency Parsing: Let’s Use Supervised Parsers\nPhong Le and Willem Zuidema\nInstitute for Logic, Language, and Computation\nUniversity of Amsterdam, the Netherlands\n{p.le,zuidema}@uva.nl\nAbstract\nWe present a self-training approach to unsu-\npervised dependency parsing that reuses exist-\ning supervised and unsupervised parsing algo-\nrithms. Our approach, called ‘iterated rerank-\ning’ (IR), starts with dependency trees gener-\nated by an unsupervised parser, and iteratively\nimproves these trees using the richer proba-\nbility models used in supervised parsing that\nare in turn trained on these trees. Our system\nachieves 1.8% accuracy higher than the state-\nof-the-part parser of Spitkovsky et al. (2013)\non the WSJ corpus.\n1\nIntroduction\nUnsupervised dependency parsing and its super-\nvised counterpart have many characteristics in com-\nmon: they take as input raw sentences, produce\ndependency structures as output, and often use the\nsame evaluation metric (DDA, or UAS, the percent-\nage of tokens for which the system predicts the cor-\nrect head).\nUnsurprisingly, there has been much\nmore research on supervised parsing – producing a\nwealth of models, datasets and training techniques\n– than on unsupervised parsing, which is more dif-\nﬁcult, much less accurate and generally uses very\nsimple probability models. Surprisingly, however,\nthere have been no reported attempts to reuse super-\nvised approaches to tackle the unsupervised parsing\nproblem (an idea brieﬂy mentioned in Spitkovsky et\nal. (2010b)).\nThere are, nevertheless, two aspects of supervised\nparsers that we would like to exploit in an unsuper-\nvised setting. First, we can increase the model ex-\npressiveness in order to capture more linguistic reg-\nularities. Many recent supervised parsers use third-\norder (or higher order) features (Koo and Collins,\n2010; Martins et al., 2013; Le and Zuidema, 2014)\nto reach state-of-the-art (SOTA) performance.\nIn\ncontrast, existing models for unsupervised parsing\nlimit themselves to using simple features (e.g., con-\nditioning on heads and valency variables) in order\nto reduce the computational cost, to identify consis-\ntent patterns in data (Naseem, 2014, page 23), and\nto avoid overﬁtting (Blunsom and Cohn, 2010). Al-\nthough this makes learning easier and more efﬁcient,\nthe disadvantage is that many useful linguistic regu-\nlarities are missed: an upper bound on the perfor-\nmance of such simple models – estimated by us-\ning annotated data – is 76.3% on the WSJ corpus\n(Spitkovsky et al., 2013), compared to over 93% ac-\ntual performance of the SOTA supervised parsers.\nSecond, we would like to make use of informa-\ntion available from lexical semantics, as in Bansal\net al. (2014), Le and Zuidema (2014), and Chen and\nManning (2014). Lexical semantics is a source for\nhandling rare words and syntactic ambiguities. For\ninstance, if a parser can identify that “he” is a depen-\ndent of “walks” in the sentence “He walks”, then,\neven if “she” and “runs” do not appear in the train-\ning data, the parser may still be able to recognize\nthat “she” should be a dependent of “runs” in the\nsentence “she runs”. Similarly, a parser can make\nuse of the fact that “sauce” and “John” have very\ndifferent meanings to decide that they have different\nheads in the two phrases “ate spaghetti with sauce”\nand “ate spaghetti with John”.\nHowever, applying existing supervised parsing\narXiv:1504.04666v1  [cs.CL]  18 Apr 2015\ntechniques to the task of unsupervised parsing is,\nunfortunately, not trivial. The reason is that those\nparsers are optimally designed for being trained on\nmanually annotated data. If we use existing unsuper-\nvised training methods (like EM), learning could be\neasily misled by a large amount of ambiguity natu-\nrally embedded in unannotated training data. More-\nover, the computational cost could rapidly increase\nif the training algorithm is not designed properly.\nTo overcome these difﬁculties we propose a frame-\nwork, iterated reranking (IR), where existing super-\nvised parsers are trained without the need of manu-\nally annotated data, starting with dependency trees\nprovided by an existing unsupervised parser as ini-\ntialiser. Using this framework, we can employ the\nwork of Le and Zuidema (2014) to build a new sys-\ntem that outperforms the SOTA unsupervised parser\nof Spitkovsky et al. (2013) on the WSJ corpus.\nThe contribution of this paper is twofold. First,\nwe show the beneﬁt of using lexical semantics for\nthe unsupervised parsing task. Second, our work is\na bridge connecting the two research areas unsuper-\nvised parsing and its supervised counterpart. Before\ngoing to the next section, in order to avoid confusion\nintroduced by names, it is worth noting that we use\nun-trained existing supervised parsers which will be\ntrained on automatically annotated treebanks.\n2\nRelated Work\n2.1\nUnsupervised Dependency Parsing\nThe ﬁrst breakthrough was set by Klein and Man-\nning (2004) with their dependency model with va-\nlence (DMV), the ﬁrst model to outperform the\nright-branching baseline on the DDA metric: 43.2%\nvs 33.6% on sentences up to length 10 in the WSJ\ncorpus. Nine years later, Spitkovsky et al. (2013)\nachieved much higher DDAs: 72.0% on sentences\nup to length 10, and 64.4% on all sentences in sec-\ntion 23. During this period, many approaches have\nbeen proposed to attempt the challenge.\nNaseem and Barzilay (2011), Tu and Honavar\n(2012), Spitkovsky et al. (2012), Spitkovsky et al.\n(2013), and Marecek and Straka (2013) employ ex-\ntensions of the DMV but with different learning\nstrategies. Naseem and Barzilay (2011) use seman-\ntic cues, which are event annotations from an out-\nof-domain annotated corpus, in their model during\ntraining. Relying on the fact that natural language\ngrammars must be unambiguous in the sense that\na sentence should have very few correct parses, Tu\nand Honavar (2012) incorporate unambiguity regu-\nlarisation to posterior probabilities. Spitkovsky et al.\n(2012) bootstrap the learning by slicing up all input\nsentences at punctuation. Spitkovsky et al. (2013)\npropose a complete deterministic learning frame-\nwork for breaking out of local optima using count\ntransforms and model recombination. Marecek and\nStraka (2013) make use of a large raw text corpus\n(e.g., Wikipedia) to estimate stop probabilities, us-\ning the reducibility principle.\nDiffering from those works, Bisk and Hocken-\nmaier (2012) rely on Combinatory Categorial Gram-\nmars with a small number of hand-crafted general\nlinguistic principles; whereas Blunsom and Cohn\n(2010) use Tree Substitution Grammars with a hi-\nerarchical non-parametric Pitman-Yor process prior\nbiasing the learning to a small grammar.\n2.2\nReranking\nOur work relies on reranking which is a technique\nwidely used in (semi-)supervised parsing. Rerank-\ning requires two components: a k-best parser and a\nreranker. Given a sentence, the parser generates a\nlist of k best candidates, the reranker then rescores\nthose candidates and picks the one that has the high-\nest score. Reranking was ﬁrst successfully applied to\nsupervised constituent parsing (Collins, 2000; Char-\nniak and Johnson, 2005). It was then employed in\nthe supervised dependency parsing approaches of\nSangati et al. (2009), Hayashi et al. (2013), and Le\nand Zuidema (2014).\nClosest to our work is the work series on semi-\nsupervised constituent parsing of McClosky and col-\nleagues, e.g.\nMcClosky et al. (2006), using self-\ntraining. They use a k-best generative parser and\na discriminative reranker to parse unannotated sen-\ntences, then add resulting parses to the training\ntreebank and re-train the reranker. Different from\ntheir work, our work is for unsupervised dependency\nparsing, without manually annotated data, and uses\niterated reranking instead of single reranking.\nIn\naddition, both two components, k-best parser and\nreranker, are re-trained after each iteration.\n3\nThe IR Framework\nExisting training methods for the unsupervised de-\npendency task, such as Blunsom and Cohn (2010),\nGillenwater et al. (2011), and Tu and Honavar\n(2012), are hypothesis-oriented search with the EM\nalgorithm or its variants: training is to move from\na point which represents a model hypothesis to an-\nother point. This approach is feasible for optimising\nmodels using simple features since existing dynamic\nprogramming algorithms can compute expectations,\nwhich are sums over all possible parses, or to ﬁnd\nthe best parse in the whole parse space with low\ncomplexities.\nHowever, the complexity increases\nrapidly if rich, complex features are used. One way\nto reduce the computational cost is to use approx-\nimation methods like sampling as in Blunsom and\nCohn (2010).\n3.1\nTreebank-oriented Greedy Search\nBelieving that the difﬁculty of using EM is from\nthe fact that treebanks are ‘hidden’, leading to the\nneed of computing sum (or max) overall possible\ntreebanks, we propose a greedy local search scheme\nbased on another training philosophy:\ntreebank-\noriented search. The key idea is to explicitly search\nfor concrete treebanks which are used to train pars-\ning models.\nThis scheme thus allows supervised\nparsers to be trained in an unsupervised parsing set-\nting since there is a (automatically annotated) tree-\nbank at any time.\nGiven S a set of raw sentences, the search space\nconsists of all possible treebanks D = {d(s)|s ∈S}\nwhere d(s) is a dependency tree of sentence s. The\ntarget of search is the optimal treebank D∗that is as\ngood as human annotations. Greedy search with this\nphilosophy is as follows: starting at an initial point\nD1, we pick up a point D2 among its neighbours\nN(D1) such that\nD2 = arg max\nD∈N(D1)\nfD1(D)\n(1)\nwhere fD1(D) is an objective function measuring\nthe goodness of D (which may or may not be con-\nditioned on D1). We then continue this search until\nsome stop criterion is satisﬁed. The crucial factor\nhere is to deﬁne N(Di) and fDi(D). Below are two\nspecial cases of this scheme.\nSemi-supervised parsing using reranking\n(Mc-\nClosky et al., 2006). This reranking is indeed one-\nstep greedy local search. In this scenario, N(D1) is\nthe Cartesian product of k-best lists generated by a\nk-best parser, and fDi(D) is a reranker.\nUnsupervised\nparsing\nwith\nhard-EM\n(Spitkovsky et al., 2010b) In hard-EM, the tar-\nget is to maximise the following objective function\nwith respect to a parameter set Θ\nL(S|Θ) =\nX\ns∈S\nmax\nd∈Dep(s) log PΘ\n\u0000d\n\u0001\n(2)\nwhere Dep(s) is the set of all possible dependency\nstructures of s. The two EM steps are thus\n• Step 1: Di+1 = arg maxD PΘi(D)\n• Step 2: Θi+1 = arg maxΘ PΘ(Di+1)\nIn this case, N(Di) is the whole treebank space and\nfDi(D) = PΘi(D) = Parg maxΘ PΘ(Di)(D).\n3.2\nIterated Reranking\nWe instantiate the greedy search scheme by iterated\nreranking which requires two components: a k-best\nparser P, and a reranker R.\nFirstly, D1 is used\nto train these two components, resulting in P1 and\nR1. The parser P1 then generates a set of lists of k\ncandidates kD1 (whose Cartesian product results in\nN(D1)) for the set of training sentences S. The best\ncandidates, according to reranker R1, are collected\nto form D2 for the next iteration. This process is\nhalted when a pre-deﬁned stop criterion is met.1\nIt is certain that we can, as in the work of\nSpitkovsky et al. (2010b) and many bootstrapping\napproaches, employ only parser P. Reranking, how-\never, brings us two beneﬁts. First, it allows us to em-\nploy very expressive models like the ∞-order gen-\nerative model proposed by Le and Zuidema (2014).\nSecond, it embodies a similar idea to co-training\n(Blum and Mitchell, 1998): P and R play roles as\ntwo views of the data.\n1It is worth noting that, although N(Di) has the size O(kn)\nwhere n is the number of sentences, reranking only needs to\nprocess O(k × n) parses if these sentences are assumed to be\nindependent.\n3.3\nMulti-phase Iterated Reranking\nTraining in machine learning often uses starting big\nwhich is to use up all training data at the same time.\nHowever, Elman (1993) suggests that in some cases,\nlearning should start by training simple models on\nsmall data and then gradually increase the model\ncomplexity and add more difﬁcult data.\nThis is\ncalled starting small.\nIn unsupervised dependency parsing, starting\nsmall is intuitive. For instance, given a set of long\nsentences, learning the fact that the head of a sen-\ntence is its main verb is difﬁcult because a long sen-\ntence always contains many syntactic categories. It\nwould be much easier if we start with only length-\none sentences, e.g “Look!”, since there is only\none choice which is usually a verb. This training\nscheme was successfully applied by Spitkovsky et\nal. (2010a) under the name: Baby Step.\nWe adopt starting small to construct the multi-\nphase iterated reranking (MPIR) framework.\nIn\nphase 0, a parser M with a simple model is trained\non a set of short sentences S(0) as in traditional ap-\nproaches. This parser is used to parse a larger set\nof sentences S(1) ⊇S(0), resulting in D(1)\n1 . D(1)\n1\nis\nthen used as the starting point for the iterated rerank-\ning in phase 1. We continue this process until phase\nN ﬁnishes, with S(i) ⊇S(i−1) (i = 1..N). In gen-\neral, we use the resulting reranker in the previous\nphase to generate the starting point for the iterated\nreranking in the current phase.\n4\nLe and Zuidema (2014)’s Reranker\nLe and Zuidema (2014)’s reranker is an exception\namong supervised parsers because it employs an ex-\ntremely expressive model whose features are ∞-\norder2. To overcome the problem of sparsity, they\nintroduced the inside-outside recursive neural net-\nwork (IORNN) architecture that can estimate tree-\ngenerating models including those proposed by Eis-\nner (1996) and Collins (2003a).\n4.1\nThe ∞-order Generative Model\nLe and Zuidema (2014)’s reranker employs the gen-\nerative model proposed by Eisner (1996).\nIntu-\nitively, this model is top-down: starting with ROOT,\n2In fact, the order is ﬁnite but unbound.\nFigure 1: Inside-Outside Recursive Neural Network\n(IORNN). Black/white rectangles correspond to in-\nner/outer representations.\nwe generate its left dependents and its right de-\npendents.\nWe then generate dependents for each\nROOT’s dependent. The generative process recur-\nsively continues until there is no dependent to gen-\nerate. Formally, this model is described by the fol-\nlowing formula\nP(d(H)) =\nL\nY\nl=1\nP\n\u0000HL\nl |C(HL\nl )\n\u0001\nP\n\u0000d(HL\nl )\n\u0001\n×\nR\nY\nr=1\nP\n\u0000HR\nr |C(HR\nr )\n\u0001\nP\n\u0000d(HR\nr )\n\u0001\n(3)\nwhere H is the current head, d(N) is the fragment\nof the dependency parse rooted at N, and C(N)\nis the context to generate N. HL, HR are respec-\ntively H’s left dependents and right dependents, plus\nEOC (End-Of-Children), a special token to inform\nthat there are no more dependents to generate. Thus,\nP(d(ROOT)) is the probability of generating the\nentire dependency structure d.\nLe and Zuidema’s ∞-order generative model is\ndeﬁned as Eisner’s model in which the context\nC∞(D) to generate D contains all of D’s generated\nsiblings, its ancestors and their siblings. Because\nof very large fragments that contexts are allowed to\nhold, traditional count-based methods are imprac-\ntical (even if we use smart smoothing techniques).\nThey thus introduced the IORNN architecture to es-\ntimate the model.\n4.2\nEstimation with the IORNN\nAn IORNN (Figure 1) is a recursive neural network\nwhose topology is a tree. What make this network\ndifferent from traditional RNNs (Socher et al., 2010)\nis that each tree node u caries two vectors: iu - the\ninner representation, represents the content of the\nphrase covered by the node, and ou - the outer repre-\nsentation, represents the context around that phrase.\nIn addition, information in an IORNN is allowed to\nﬂow not only bottom-up as in RNNs, but also top-\ndown. That makes IORNNs a natural tool for esti-\nmating top-down tree-generating models.\nApplying the IORNN architecture to dependency\nparsing is straightforward, along the generative story\nof the ∞-order generative model. First of all, the\n“inside” part of this IORNN is simpler than what\nis depicted in Figure 1: the inner representation of\na phrase is assumed to be the inner representation\nof its head. This approximation is plausible since\nthe meaning of a phrase is often dominated by the\nmeaning of its head.\nThe inner representation at\neach node, in turn, is a function of a vector repre-\nsentation for the word (in our case, the word vectors\nare initially borrowed from Collobert et al. (2011)),\nthe POS-tag and capitalisation feature.\nWithout loss of generality and ignoring directions\nfor simplicity, they assume that the model is generat-\ning dependent u for node h conditioning on context\nC∞(u) which contains all of u’s ancestors (includ-\ning h) and theirs siblings, and all of previously gen-\nerated u’s sisters. Now there are two types of con-\ntexts: full contexts of heads (e.g., h) whose depen-\ndents are being generated, and contexts to generate\nnodes (e.g., C∞(u)). Contexts of the ﬁrst type are\nclearly represented by outer representations. Con-\ntexts of the other type are represented by partial\nouter representations, denoted by ¯ou. Because the\ncontext to generate a node can be constructed recur-\nsively by combining the full context of its head and\nits previously generated sisters, they can compute ¯ou\nas a function of oh and the inner representations of\nits previously generated sisters. On the top of ¯ou,\nthey put a softmax layer to estimate the probability\nP(x|C∞(u)).\nTraining this IORNN is to minimise the cross en-\ntropy over all dependents. This objective function is\nindeed the negative log likelihood P(D) of training\ntreebank D.\n4.3\nThe Reranker\nLe and Zuidema’s (generative) reranker is given by\nd∗= arg max\nd∈kDep(s)\nP(d)\nwhere P (Equation 3) is computed by the ∞-order\ngenerative model which is estimated by an IORNN;\nand kDep(s) is a k-best list.\n5\nComplete System\nOur system is based on the multi-phase IR. In gen-\neral, any third-party parser for unsupervised depen-\ndency parsing can be used in phase 0, and any third-\nparty parser that can generate k-best lists can be used\nin the other phases. In our experiments, for phase 0,\nwe choose the parser using an extension of the DMV\nmodel with stop-probability estimates computed on\na large corpus proposed by Marecek and Straka\n(2013). This system has a moderate performance3\non the WSJ corpus: 57.1% vs the SOTA 64.4% DDA\nof Spitkovsky et al. (2013). For the other phases, we\nuse the MSTParser4 (with the second-order feature\nmode) (McDonald and Pereira, 2006).\nOur system uses Le and Zuidema (2014)’s\nreranker (Section 4.3). It is worth noting that, in\nthis case, each phase with iterated reranking could\nbe seen as an approximation of hard-EM (see Equa-\ntion 2) where the ﬁrst step is replaced by\nDi+1 = arg max\nD∈N(Di)\nPΘi(D)\n(4)\nIn other words, instead of searching over the tree-\nbank space, the search is limited in a neighbour set\nN(Di) generated by k-best parser Pi.\n5.1\nTuning Parser P\nParser Pi trained on Di deﬁnes neighbour set N(Di)\nwhich is the Cartesian product of the k-best lists in\nkDi. The position and shape of N(Di) is thus deter-\nmined by two factors: how well Pi can ﬁt Di, and k.\nIntuitively, the lower the ﬁtness is, the more N(Di)\ngoes far away from Di; and the larger k is, the larger\n3Marecek and Straka (2013) did not report any experimental\nresult on the WSJ corpus. We use their source code at http:\n//ufal.mff.cuni.cz/udp with the setting presented in\nSection 6.1. Because the parser does not provide the option to\nparse unseen sentences, we merge the training sentences (up to\nlength 15) to all the test sentences to evaluate its performance.\nNote that this result is close to the DDA (55.4%) that the authors\nreported on CoNLL 2007 English dataset, which is a portion of\nthe WSJ corpus.\n4http://sourceforge.net/projects/\nmstparser/\nN(Di) is. Moreover, the diversity of N(Di) is in-\nversely proportional to the ﬁtness. When the ﬁtness\ndecreases, patterns existing in the training treebank\nbecome less certain to the parser, patterns that do not\nexist in the training treebank thus have more chances\nto appear in k-best candidates. This leads to high di-\nversity of N(Di). We blindly set k = 10 in all of\nour experiments.\nWith the MSTParser,\nthere are two hyper-\nparameters: itersMST, the number of epochs, and\ntraining-kMST, the k-best parse set size to cre-\nate constraints during training. training-kMST\nis always 1 because constraints from k-best parses\nwith almost incorrect training parses are useless.\nBecause itersMST controls the ﬁtness of the\nparser to training treebank Di, it, as pointed out\nabove, determines the distance from N(Di) to Di\nand the diversity of the former. Therefore, if we\nwant to encourage the local search to explore more\ndistant areas, we should set itersMST low. In our\nexperiments, we test two strategies: (i) MaxEnc,\nitersMST = 1, maximal encouragement, and (ii)\nMinEnc, itersMST = 10, minimal encouragement.\n5.2\nTuning Reranker R\nTuning the reranker R is to set values for dimIORNN,\nthe dimensions of inner and outer representations,\nand itersIORNN, the number of epochs to train the\nIORNN. Because the ∞-order model is very expres-\nsive and feed-forward neural networks are universal\napproximators (Cybenko, 1989), the reranker is ca-\npable of perfectly remembering all training parses.\nIn order to avoid this, we set dimIORNN = 50, and\nset itersIORNN = 5 for very early stopping.\n5.3\nTuning multi-phase IR\nBecause Marecek and Straka (2013)’s parser does\nnot distinguish training data from test data, we pos-\ntulate S0 = S1. Our system has N phases such that\nS0, S1 contain all sentences up to length l1 = 15,\nSi (i = 2..N) contains all sentences up to length\nli = li−1 + 1, and SN contains all sentences up to\nlength 25. Phase 1 halts after 100 iterations whereas\nall the following phases run with one iteration. Note\nthat we force the local search in phase 1 to run in-\ntensively because we hypothesise that most of the\nimportant patterns for dependency parsing can be\nfound within short sentences.\n6\nExperiments\n6.1\nSetting\nWe use the Penn Treebank WSJ corpus: sections\n02-21 for training, and section 23 for testing. We\nthen apply the standard pre-processing5 for unsu-\npervised dependency parsing task (Klein and Man-\nning, 2004): we strip off all empty sub-trees, punc-\ntuation, and terminals (tagged # and $) not pro-\nnounced where they appear; we then convert the re-\nmaining trees to dependencies using Collins’s head\nrules (Collins, 2003b). Both word forms and gold\nPOS tags are used. The directed dependency accu-\nracy (DDA) metric is used for evaluation.\nThe vocabulary is taken as a list of words occur-\nring more than two times in the training data. All\nother words are labelled ‘UNKNOWN’ and every\ndigit is replaced by ‘0’. We initialise the IORNN\nwith the 50-dim word embeddings from Collobert et\nal. (2011) 6 , and train it with the learning rate 0.1,\n6.2\nResults\nWe compare our system against recent systems (Ta-\nble 1 and Section 2.1). Our system with the two en-\ncouragement levels, MinEnc and MaxEnc, achieves\nthe highest reported DDAs on section 23: 1.8% and\n1.2% higher than Spitkovsky et al. (2013) on all sen-\ntences and up to length 10, respectively. Our im-\nprovements over the system’s initialiser (Marecek\nand Straka, 2013) are 9.1% and 4.4%.\n6.3\nAnalysis\nIn this section, we analyse our system along two as-\npects. First, we examine three factors which deter-\nmine the performance of the whole system: encour-\nagement level, lexical semantics, and starting point.\nWe then search for what IR (with the MaxEnc op-\ntion) contributes to the overall performance by com-\nparing the quality of the treebank resulted in the end\nof phase 1 against the quality of the treebank given\nby its initialier, i.e. Marecek and Straka (2013).\nThe effect of encouragement level\nFigure 2 shows the differences in DDA between\nusing MaxEnc and MinEnc in each phase: we com-\n5http://www.cs.famaf.unc.edu.ar/\n˜francolq/en/proyectos/dmvccm\n6http://ml.nec-labs.com/senna/.\nThese word\nembeddings were unsupervisedly learnt from Wikipedia.\nSystem\nDDA (@10)\nBisk and Hockenmaier (2012)\n53.3 (71.5)\nBlunsom and Cohn (2010)\n55.7 (67.7)\nTu and Honavar (2012)\n57.0 (71.4)\nMarecek and Straka (2013)\n57.1 (68.8)\nNaseem and Barzilay (2011)\n59.4 (70.2)\nSpitkovsky et al. (2012)\n61.2 (71.4)\nSpitkovsky et al. (2013)\n64.4 (72.0)\nOur system (MinEnc)\n66.2 (72.7)\nOur system (MaxEnc)\n65.8 (73.2)\nTable 1: Performance on section 23 of the WSJ cor-\npus (all sentences and up to length 10) for recent sys-\ntems and our system. MinEnc and MaxEnc denote\nitersMST = 10 and itersMST = 1 respectively.\nFigure 2: DDAMaxEnc −DDAMinEnc of all phases\non the their training sets (e.g., phase 3 with S(3) con-\ntaining all training sentences up to length 17).\npute DDAMaxEnc −DDAMinEnc of each phase on its\ntraining set (e.g., phase 3 with S(3) containing all\ntraining sentences up to length 17). MinEnc outper-\nforms MaxEnc within phases 1, 2, 3, and 4. How-\never, from phase 5, the latter surpasses the former. It\nsuggests that exploring areas far away from the cur-\nrent point with long sentences is risky. The reason\nis that long sentences contain more ambiguities than\nshort ones; thus rich diversity, high difference from\nthe current point, but small size (i.e., small k) could\neasily lead the learning to a wrong path.\nThe performance of the system with the two en-\ncouragement levels on section 23 (Table 1) also sug-\ngests the same. MaxEnc strategy helps the system\nachieve the highest accuracy on short sentences (up\nto length 10). However, it is less helpful than Mi-\nnEnc when performing on long sentences.\nFigure 3: DDA of phase 1 (MaxEnc), with and with-\nout the word embeddings (denoted by w/ sem and\nwo/ sem, respectively), on training sentences up to\nlength 15 (i.e. S(1)).\nFigure 4: DDA of phase 1 (MaxEnc) before and af-\nter training with three different starting points pro-\nvided by three parsers used in phase 0: MS (Mare-\ncek and Straka, 2013), GGGPT (Gillenwater et al.,\n2011), and Harmonic (Klein and Manning, 2004).\nThe role of lexical semantics\nWe examine the role of the lexical semantics,\nwhich is given by the word embeddings.\nFig-\nure 3 shows DDAs on training sentences up to\nlength 15 (i.e.\nS(1)) of phase 1 (MaxEnc) with\nand without the word-embeddings. With the word-\nembeddings, phase 1 achieves 71.11%. When the\nword-embeddings are not given, i.e.\nthe IORNN\nuses randomly generated word vectors, the accuracy\ndrops 4.2%. It shows that lexical semantics plays a\ndecisive role in the performance of the system.\nHowever, it is worth noting that, even without that\nknowledge (i.e., with the ∞-order generative model\nalone), the DDA of phase 1 is 2% higher than before\nbeing trained (66.89% vs 64.9%). It suggests that\nphase 1 is capable of discovering some useful de-\npendency patterns that are invisible to the parser in\nphase 0. This, we conjecture, is thanks to high-order\nfeatures captured by the IORNN.\nThe importance of the starting point\nStarting point is claimed to be important in lo-\ncal search. We examine this by using three differ-\nent parsers in phase 0: (i) MS (Marecek and Straka,\nFigure 5: Precision (top) and recall (bottom) over\nbinned HEAD distance of iterated reranking (IR)\nand its initializer (MS) on the training sentences in\nphase 1 (≤15 words).\n2013), the parser used in the previous experiments,\n(ii) GGGPT (Gillenwater et al., 2011)7 employing\nan extension of the DMV model and posterior reg-\nularization framework for training, and (iii) Har-\nmonic, the harmonic initializer proposed by Klein\nand Manning (2004).\nFigure 4 shows DDAs of phase 1 (MaxEnc)\non training sentences up to length 15 with three\nstarting-points given by those parsers. Starting point\nis clearly very important to the performance of the\niterated reranking: the better the starting point is,\nthe higher performance phase 1 has.\nHowever, a\nremarkable point here is that the iterated reranking\nof phase 1 always ﬁnds out more useful patterns for\nparsing whatever the starting point is in this experi-\nment. It is certainly due to the high order features\nand lexical semantics, which are not exploited in\nthose parsers.\nThe contribution of Iterated Reranking\nWe compare the quality of the treebank resulted in\nthe end of phase 1 against the quality of the treebank\ngiven by the initialier Marecek and Straka (2013).\nFigure 5 shows precision (top) and recall (bottom)\n7code.google.com/p/pr-toolkit\nover binned HEAD distance. IR helps to improve\nthe precision on all distance bins, especially on the\nbins corresponding to long distances (≥3). The re-\ncall is also improved, except on the bin correspond-\ning to ≥7 (but the F1-score on this bin is increased).\nWe attribute this improvement to the ∞-order model\nwhich uses very large fragments as contexts thus be\nable to capture long dependencies.\nFigure 6 shows the correct-head accuracies over\nPOS-tags. IR helps to improve the accuracies over\nalmost all POS-tags, particularly nouns (e.g. NN,\nNNP, NNS), verbs (e.g. VBD, VBZ, VBN, VBG)\nand adjectives (e.g. JJ, JJR). However, as being af-\nfected by the initializer, IR performs poorly on con-\njunction (CC) and modal auxiliary (MD). For in-\nstance, in the treebank given by the initializer, al-\nmost all modal auxilaries are dependents of their\nverbs instead of the other way around.\n7\nDiscussion\nOur system is different from the other systems\nshown in Table 1 as it uses an extremely expressive\nmodel, the ∞-order generative model, in which con-\nditioning contexts are very large fragments. Only\nthe work of Blunsom and Cohn (2010), whose re-\nsulting grammar rules can contain large tree frag-\nments, shares this property. The difference is that\ntheir work needs a pre-deﬁned prior, namely hierar-\nchical non-parametric Pitman-Yor process prior, to\navoid large, rare fragments and for smoothing. The\nIORNN of our system, in contrast, does that auto-\nmatically. It learns by itself how to deal with dis-\ntant conditioning nodes, which are often less infor-\nmative than close conditioning nodes on computing\nP(x|C∞(u)). In addition, smoothing is given free:\nrecursive neural nets are able to map ‘similar’ frag-\nments onto close points (Socher et al., 2010) thus\nan unseen fragment tends to be mapped onto a point\nclose to points corresponding to ‘similar’ seen frag-\nments.\nAnother difference is that our system exploits lex-\nical semantics via word embeddings, which were\nlearnt unsupervisedly. By initialising the IORNN\nwith these embeddings, the use of this knowledge\nturns out easy and transparent.\nSpitkovsky et al.\n(2013) also exploit lexical semantics but in a limited\nway, using a context-based polysemous unsuper-\nFigure 6: Correct-head accuracies over POS-tags (sorted in the descending order by frequency) of iterated\nreranking (IR) and its initializer (MS) on the training sentences in phase 1 (≤15 words).\nvised clustering method to tag words. Although their\napproach can distinguish polysemes (e.g., ‘cool’ in\n‘to cool the selling panic’ and in ‘it is cool’), it is not\nable to make use of word meaning similarities (e.g.,\nthe meaning of ‘dog’ is closer to ‘animal’ than to\n‘table’). Naseem and Barzilay (2011)’s system uses\nsemantic cues from an out-of-domain annotated cor-\npus, thus is not fully unsupervised.\nWe have showed that IR with a generative\nreranker is an approximation of hard-EM (see Equa-\ntion 4). Our system is thus related to the works of\nSpitkovsky et al. (2013) and Tu and Honavar (2012).\nHowever, what we have proposed is more than\nthat: IR is a general framework that we can have\nmore than one option for choosing k-best parser and\nreranker. For instance, we can make use of a gener-\native k-best parser and a discriminative reranker that\nare used for supervised parsing. Our future work is\nto explore this.\nThe experimental results reveal that starting point\nis very important to the iterated reranking with the\n∞-order generative model. On the one hand, that\nis a disadvantage compared to the other systems,\nwhich use uninformed or harmonic initialisers. But\non the other hand, that is an innovation as our ap-\nproach is capable of making use of existing systems.\nThe results shown in Figure 4 suggest that if phase 0\nuses a better parser which uses less expressive model\nand/or less external knowledge than our model, such\nas the one proposed by Spitkovsky et al. (2013), we\ncan expect even a higher performance. The other\nsystems, except Blunsom and Cohn (2010), how-\never, might not beneﬁt from using good existing\nparsers as initializers because their models are not\nsigniﬁcantly more expressive than others 8.\n8\nConclusion\nWe have proposed a new framework, iterated rerank-\ning (IR), which trains supervised parsers without the\nneed of manually annotated data by using a unsu-\npervised parser as an initialiser. Our system, em-\nploying Marecek and Straka (2013)’s unsupervised\nparser as the initialiser, the k-best MSTParser, and\nLe and Zuidema (2014)’s reranker, achieved 1.8%\nDDA higher than the SOTA parser of Spitkovsky et\nal. (2013) on the WSJ corpus. Moreover, we also\nshowed that unsupervised parsing beneﬁts from lex-\nical semantics through using word-embeddings.\nOur future work is to exploit other existing super-\nvised parsers that ﬁt our framework. Besides, taking\ninto account the fast development of the word em-\nbedding research (Mikolov et al., 2013; Pennington\net al., 2014), we will try different word embeddings.\nAcknowledgments\nWe thank Remko Scha and three anonymous re-\nviewers for helpful comments.\nLe thanks Milo˘s\nStanojevi´c for helpful discussion.\n8In an experiment, we used the Marecek and Straka (2013)’s\nparser as an initializer for the Gillenwater et al. (2011)’s parser.\nAs we expected, the latter was not able to make use of this.\nReferences\nMohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.\nTailoring continuous word representations for depen-\ndency parsing. In Proceedings of the Annual Meeting\nof the Association for Computational Linguistics.\nYonatan Bisk and Julia Hockenmaier. 2012. Simple ro-\nbust grammar induction with combinatory categorial\ngrammars. In AAAI.\nAvrim Blum and Tom M. Mitchell. 1998. Combining\nlabeled and unlabeled sata with co-training. In COLT,\npages 92–100.\nPhil Blunsom and Trevor Cohn. 2010. Unsupervised in-\nduction of tree substitution grammars for dependency\nparsing. In Proceedings of the 2010 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1204–1213. Association for Computational Lin-\nguistics.\nEugene Charniak and Mark Johnson. 2005. Coarse-to-\nﬁne n-best parsing and maxent discriminative rerank-\ning. In ACL.\nDanqi Chen and Christopher D Manning.\n2014.\nA\nfast and accurate dependency parser using neural net-\nworks.\nIn Empirical Methods in Natural Language\nProcessing (EMNLP).\nMichael Collins. 2000. Discriminative reranking for nat-\nural language parsing. In ICML, pages 175–182.\nMichael Collins. 2003a. Head-driven statistical models\nfor natural language parsing. Computational linguis-\ntics, 29(4):589–637.\nMichael Collins. 2003b. Head-driven statistical models\nfor natural language parsing. Computational Linguis-\ntics, 29(4):589–637.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\nNatural language processing (almost) from scratch.\nThe Journal of Machine Learning Research, 12:2493–\n2537.\nGeorge Cybenko. 1989. Approximation by superposi-\ntions of a sigmoidal function. Mathematics of control,\nsignals and systems, 2(4):303–314.\nJason M Eisner.\n1996.\nThree new probabilistic mod-\nels for dependency parsing: An exploration. In Pro-\nceedings of the 16th conference on Computational\nlinguistics-Volume 1, pages 340–345. Association for\nComputational Linguistics.\nJeffrey L Elman. 1993. Learning and development in\nneural networks: The importance of starting small.\nCognition, 48(1):71–99.\nJennifer Gillenwater, Kuzman Ganchev, Jo˜ao Grac¸a, Fer-\nnando Pereira, and Ben Taskar. 2011. Posterior spar-\nsity in unsupervised dependency parsing. The Journal\nof Machine Learning Research, 12:455–490.\nKatsuhiko Hayashi, Shuhei Kondo, and Yuji Matsumoto.\n2013. Efﬁcient stacked dependency parsing by forest\nreranking. Transactions of the Association for Com-\nputational Linguistics, 1(1):139–150.\nDan Klein and Christopher D. Manning. 2004. Corpus-\nbased induction of syntactic structure: Models of de-\npendency and constituency. In ACL, pages 478–485.\nTerry Koo and Michael Collins. 2010. Efﬁcient third-\norder dependency parsers. In Proceedings of the 48th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1–11. Association for Computa-\ntional Linguistics.\nPhong Le and Willem Zuidema.\n2014.\nThe inside-\noutside recursive neural network model for depen-\ndency parsing. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning. Association for Computational Linguistics.\nDavid Marecek and Milan Straka.\n2013.\nStop-\nprobability estimates computed on a large corpus im-\nprove unsupervised dependency parsing. In ACL (1),\npages 281–290.\nAndr´e FT Martins, Miguel B Almeida, and Noah A\nSmith. 2013. Turning on the turbo: Fast third-order\nnon-projective turbo parsers. In Proc. of ACL.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Effective self-training for parsing. In Proceed-\nings of the main conference on human language tech-\nnology conference of the North American Chapter of\nthe Association of Computational Linguistics, pages\n152–159. Association for Computational Linguistics.\nRyan T. McDonald and Fernando C. N. Pereira. 2006.\nOnline learning of approximate dependency parsing\nalgorithms. In EACL.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representations\nof words and phrases and their compositionality. In\nAdvances in Neural Information Processing Systems,\npages 3111–3119.\nTahira Naseem and Regina Barzilay. 2011. Using se-\nmantic cues to learn syntax. In AAAI.\nTahira Naseem.\n2014.\nLinguistically Motivated Mod-\nels for Lightly-Supervised Dependency Parsing. Ph.D.\nthesis, Massachusetts Institute of Technology.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. Proceedings of the Empiricial Methods in\nNatural Language Processing (EMNLP 2014), 12.\nFederico Sangati, Willem Zuidema, and Rens Bod. 2009.\nA generative re-ranking model for dependency pars-\ning. In Proceedings of the 11th International Confer-\nence on Parsing Technologies, pages 238–241. Asso-\nciation for Computational Linguistics.\nRichard Socher, Christopher D. Manning, and Andrew Y.\nNg.\n2010.\nLearning continuous phrase representa-\ntions and syntactic parsing with recursive neural net-\nworks. In Proceedings of the NIPS-2010 Deep Learn-\ning and Unsupervised Feature Learning Workshop.\nValentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-\nrafsky. 2010a. From Baby Steps to Leapfrog: How\n“Less is More” in unsupervised dependency parsing.\nIn Proc. of NAACL-HLT.\nValentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,\nand Christopher D. Manning. 2010b. Viterbi training\nimproves unsupervised dependency parsing. In Pro-\nceedings of the Fourteenth Conference on Computa-\ntional Natural Language Learning (CoNLL-2010).\nValentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-\nrafsky. 2012. Bootstrapping dependency grammar in-\nducers from incomplete sentence fragments via austere\nmodels. In Proceedings of the 11th International Con-\nference on Grammatical Inference.\nValentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-\nrafsky.\n2013.\nBreaking out of local optima with\ncount transforms and model recombination: A study\nin grammar induction. In EMNLP, pages 1983–1995.\nKewei Tu and Vasant Honavar. 2012. Unambiguity reg-\nularization for unsupervised learning of probabilistic\ngrammars. In Proceedings of the 2012 Joint Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and Computational Natural Language Learn-\ning, pages 1324–1334. Association for Computational\nLinguistics.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2015-04-18",
  "updated": "2015-04-18"
}