{
  "id": "http://arxiv.org/abs/1504.04716v1",
  "title": "Gap Analysis of Natural Language Processing Systems with respect to Linguistic Modality",
  "authors": [
    "Vishal Shukla"
  ],
  "abstract": "Modality is one of the important components of grammar in linguistics. It\nlets speaker to express attitude towards, or give assessment or potentiality of\nstate of affairs. It implies different senses and thus has different\nperceptions as per the context. This paper presents an account showing the gap\nin the functionality of the current state of art Natural Language Processing\n(NLP) systems. The contextual nature of linguistic modality is studied. In this\npaper, the works and logical approaches employed by Natural Language Processing\nsystems dealing with modality are reviewed. It sees human cognition and\nintelligence as multi-layered approach that can be implemented by intelligent\nsystems for learning. Lastly, current flow of research going on within this\nfield is talked providing futurology.",
  "text": "arXiv:1504.04716v1  [cs.CL]  18 Apr 2015\nGap Analysis of Natural Language Processing\nSystems with respect to Linguistic Modality\nVishal Shukla\nAbstract\nModality is one of the important components of grammar in linguistics. It\nlets speaker to express attitude towards, or give assessment or potentiality\nof state of aﬀairs. It implies diﬀerent senses and thus has diﬀerent percep-\ntions as per the context. This paper presents an account showing the gap\nin the functionality of the current state of art Natural Language Processing\n(NLP) systems. The contextual nature of linguistic modality is studied. In\nthis paper, the works and logical approaches employed by Natural Language\nProcessing systems dealing with modality are reviewed. It sees human cog-\nnition and intelligence as multi-layered approach that can be implemented\nby intelligent systems for learning. Lastly, current ﬂow of research going on\nwithin this ﬁeld is talked providing futurology.\nKeywords:\nmodality, NLP, gap, shortcomings, multi-layered, learning\n1. Introduction\nResearch in NLP is gaining interest as its applications are becoming more\nsigniﬁcant. Natural language is highly ambiguous and understanding it ef-\nfectively can be considered as a primary concern. Modality in language is\nassociated with contextual understanding and implied perceptions. Deﬁning\nmodality from a computational linguistics perspective is somewhat diﬃcult\nbecause several concepts are used to refer to phenomena that are related to\nmodality, depending on the task at hand and the speciﬁc singularities that\nV. Shukla\nSystems Engineer\nInfosys, Bangalore, India.\nContact no.: +91-97394 99580\nE-mail: vishal shukla04@infosys.com\nthe speaker addresses. There are diﬀerent senses that can be articulated by\nmodality. Understanding precise sense conveyed is important.\nThe major tasks involved in dealing with modality in text are: detecting\nthe occurrence of modality, categorizing the type of modality and perceiving\nthe sense conveyed through it. Parsers and language processing tools iden-\ntiﬁes the occurrence of modals by Part of Speech (POS) tagging. Talking\nabout type of modality, in the literature, standard classiﬁcation of types is\nnot available as various types are deﬁned. But we consider the classiﬁcation\nbroadly as in two types: epistemic and deontic. According to Palmer (2014),\nepistemic is used by the speakers to express their judgment about the factual\nstatus of the proposition. Whereas deontic modality is concerned with the\nspeakers directive attitude towards an action to be carried out. It relates\nto obligation or permission and to conditional factors that are external to\nthe relevant individual. The senses of necessity and possibility are incorpo-\nrated by epistemic type while those of permission and obligation by deontic.\nModality type categorization and the sense recognition has been carried out\nby annotation approaches.\nThis paper will closely observe the methods applied for the abovemen-\ntioned tasks and also review the works in the subject ﬁeld.\nIt will then\nhighlight shortcomings of the state of art natural language systems in con-\ntext to linguistic modality that forming the gap in their functionality. Latter\nsections will be covering the directions for the scope of improvement in the\nsame context.\n2. Context Review\nAs tagging and annotating are the two key aspects, lets summarize them\nregarding to the language processing. The process of tagging involves assign-\ning tags to each word in the corpus corresponding to the part of speech that\nit embodies. The Part of Speech tagging is an essential subtask in language\nprocessing and is very useful for subsequent phases like syntactic parsing.\nThe tags into which a token can possibly classiﬁed depends on the tagset\nadopted for this task. That is, it depends upon the Treebank taken into use\nwhich deﬁnes the directory of the tags.\nAnnotation, on the other hand, uses machine learning approach. It uses\npre-annotated corpus to learn and annotate plain text.\nThe annotations\nmade are markups usually representing extended features. The extended fea-\ntures may include polarity, certainty, subjectivity, sense and type of modality,\n2\nevent mentions, etc. POS tagging is also carried out with machine learn-\ning methods in a similar way as annotation (Manning and Sch¨utze, 1999;\nRatnaparkhi et al, 1996; Toutanova and Manning, 2000).\n2.1. Detecting occurrence\nA number of diﬀerent expressions in language can have modal meanings.\nVon Fintel (2006) discusses on a subset of variety of modal expressions. Tak-\ning into account the tagging output of Stanford NLP parser of few sample\nsentences with diﬀerent expressions, some points will be highlighted.\nIn case of modal auxiliaries used, the parser tags the same with MD i.e.\nmodal.\n1. Sandy must be home.\nSandy/NNP\nmust/MD\nbe/VB\nhome/NN\n./.\nThus any explicit occurrence of modal auxiliaries such as must, should/shall,\nmight, may, could/can will be detected well and clear.\nWhereas semi-modals like has to, need to, ought to follow diﬀerent treat-\nment. Sometimes ought to is considered as modal and sometimes as semi-\nmodal because of diﬀerence in its syntax. Anyhow it is tagged as a modal.\nRest of the semi-modals are not tagged as modal; but auxiliary verbs and\nstructure of sentence can be identiﬁed by parsers.\n2. Sandy has to be home.\nSandy/NNP\nhas/VBZ\nto/TO\nbe/VB\nhome/NN\n./.\n3. Sandy ought to be home.\nSandy/NNP\nought/MD\nto/TO\nbe/VB\nhome/NN\n./.\nApart from modal auxiliaries and semi-modals, modal meaning can also\nbe conveyed using adverbs (perhaps, probably, etc.), nouns (possibility, ne-\ncessity, etc.), adjectives (bound, certain, etc.), and also conditional constructs\n(if.. then..).\n3\n4. It is far from necessary that Sandy is home.\nIt/PRP\nis/VBZ\nfar/RB\nfrom/IN\nnecessary/JJ\nthat/IN\nSandy/NNP\nis/VBZ\nhome/NN\n./.\n5. There is a slight possibility that Sandy is home.\nThere/EX\nis/VBZ\na/DT\nslight/JJ\npossibility/NN\nthat/IN\nSandy/NNP\nis/VBZ\nhome/NN\n./.\n6. Perhaps Sandy is home.\nPerhaps/RB\nSandy/NNP\nis/VBZ\nhome/NN\n./.\n7. If the light is on, Sandy is home.\nIf/IN\nthe/DT\nlight/NN\nis/VBZ\non/IN\n,/,\nSandy/NNP\nis/VBZ\nhome/NN\n./.\nAlthough, the advances in calibration of parsers has improved the ability\nto tag words accurately, but above certain point, the mechanism seem to\nbecome insuﬃcient to gather underlying information that is not superﬁcial\nor apparent.\n2.2. Type Categorization and Sense Perception\nCategorization of type of modality in text and the identiﬁcation of sense\nconveyed can be done by developing annotation schemes. There are works\nthat accommodate annotating of those features, but not necessarily they are\norganized with study of languages perspective; which makes it diﬃcult to\nsummarize and separate out the relevant points of interest. Upon review-\ning the literature, it can be seen that various annotating schemes has been\nconstructed over time for marking annotations of diﬀerent components.\nBaker et al (2012) described a modality/negation (MN) annotation scheme\nwhich isolates three components of modality and negation: a trigger (that is\nsource of modality or negation), a target (action associated with modality or\nnegation) and a holder (the experiencer of modality). Moreover they have\nconstructed MN lexicon and two automated MN taggers using the annotation\nscheme.\nRuppenhofer and Rehbein (2012) presents annotation scheme that anno-\ntates type of English modals in MPQA corpus. The modal verbs targeted\n4\nwere can/could, may/might, must, ought, shall/should. The annotation in-\nvolved categorization of the modals in to six types: epistemic, deontic, dy-\nnamic, optative, concessive, conditional.\nPakray et al (2012) experimented on QA4MRE data sets to identify modal-\nity and negation in text and assign labels mod, neg, neg-mod, none for oc-\ncurrences of modal, negation, modality and negation, and absence of both\nmodality and negation respectively. And the detected modals were catego-\nrized into epistemic and deontic.\nHendrickx et al (2012) presents a scheme for annotation of modality in\nPortuguese, using MMAX2 tool. The components annotated were: trigger\n(the element conveying the modal value), target (the expression in the scope\nof the trigger), source of the event mention (speaker or writer), and source\nof the modality (agent or experiencer). Also, for trigger, two attributes were\nspeciﬁed: modal value and polarity. They stated thirteen diﬀerent types into\nwhich the modal value could be categorized.\nRubinstein et al (2013) proposed ﬁne-grained annotation approach utiliz-\ning MPQA corpus and MMAX2 tool. It was said to be ﬁne-grained as it ex-\ntends some of the previous works with a number of novel features to improve\ndetection and interpretation of modals in text. The features annotated were:\nmodality type, polarity, propositional arguments, source, background, modi-\nﬁed element, degree indicator, outscoping quantiﬁer and lemma. The types\ninto which modality was categorized are: ﬁne grained types: epistemic, cir-\ncumstantial, ability, deontic, bouletic, teleological, and bouletic/teleological;\nand the coarse grained types: epistemic or circumstantial, ability or circum-\nstantial, and priority.\nOn surveying other studies carried out using annotations, apparently it\nseems that more and more attributes were annotated to make text under-\nstanding precise. Certain works moved in the direction of subjectivity anal-\nysis; some in certainty analysis; whereas others focused on time, events and\ntemporal analysis; all of them being implicitly useful in the study of type\nand/or sense understanding of modality.\nDiﬀerent types of subjectivity is implied in discourse by diﬀerent types,\nepistemic and deontic modals. The relationship between subjectivity and\nmodality is elaborately discussed in Sanders and Spooren (1997).\nRubin et al.\n(2004; 2006) presented a certainty categorization model\nbased on four hypothesized dimensions and tested the model on a sample of\nnews articles. Rubin (2010) identiﬁes that certainty can be seen as a variety\nof epistemic modality expressed in form of markers like probably, perhaps,\n5\nundoubtedly, etc.\nNissim et al (2013) proposed modality annotation model that they say\nto be two layered. Factuality and speakers attitude being two components\nmarked, they also plan to make the model more coherent by annotating\nstrength of modality.\nMatsuyoshi et al (2010) clearly draws attention to the point that recent\ndevelopments in language processing has improved precision but is insuf-\nﬁcient for applications such as information extraction, question answering\nand recognizing textual entailment. Such applications require more informa-\ntion such as modality, polarity, and other associated information collectively\nreferred as extended modality. Matsuyoshi proposes an annotation scheme\nthat represents extended modality and consists of seven components: source,\ntime, conditional, primary modality type, actuality, evaluation, and focus.\nUtilizing the work, they also constructed an annotated corpus in Japanese.\nEmphasizing upon event modality, Saur´ı et al. (2006a) says that modality\nis an important component of discourse together with other levels of infor-\nmation such as argument structure and temporal information. That made\nan apparent need for a more sophisticated approach that is sensitive to such\nadditional information. They worked on annotation scheme that annotates\nevent modality and also identifying its scope using TimeML. Saur´ı (2006b)\nhas also worked on SlinkET attempting the construction of modal parser for\nevents. Pustejovsky et al. (2003) built the TimeBank corpus which is an-\nnotated with information like modals, events, times, relation between events\nand temporal expressions. Saur´ı and Pustejovsky (2009) built the FactBank\non the basis of TimeBank, where events are assigned with diﬀerent degrees of\nfactuality according to their source-introducing predicates (SIPS) and source.\nDiﬀerent degrees of factuality are determined by diﬀerent degrees of certainty\nand polarity axes. Degrees of certainty include certain, probable, possible and\nunknown. Whereas polarity axis contained positive, negative and underspec-\niﬁed.\n3. Gap and Futurology\nLinguistic modality is one of the components of discourse that is asso-\nciated with context, sense and meaning, mental and real spaces, and force\ndynamics.\nAlso there is no proper well deﬁned classiﬁcation of diﬀerent types and\nsenses of modality in linguistic literature. This lack of taxonomy has con-\n6\ntrived enormous confusions regarding types and senses. Considering the fact\nthat the section is complex and modality in discourse has many aspects as-\nsociated with it, and its wider scope, even language scientists can contribute\ntowards it.\nFirst point noted is that diﬀerent kinds of expressions can be used to\nconvey modal meanings. And we saw that the tagging approach is limited\nto tag modals explicit use in text. Moreover, taggers don't put any further\nlight on the type and sense of the modality. Though attempts have made on\ntype and sense classiﬁcation using annotation methods, but due to ﬂexibility\nof meanings, it is diﬃcult to standardize. Flexibility of meaning means the\nmodal verb has diﬀerent meaning according to context. Modality in language\nhas contextual meanings and implied perceptions. And mechanisms fail to\nidentify perspective, aspect and contextuality.\nAnother drawback is dependency in both methods; tagging as well as\nannotating. The process of tagging is dependent on the tagset and the an-\nnotation scheme is limited by its own training corpus. Although this lexicon\ndependency of the available approaches are useful for preliminary passes of\nprocessing but not an eﬀective way for understanding natural language which\nincludes cognition and perception.\nAt this point, inspiring from human information processing mechanisms\nwould be seen appropriate. In this context understanding of human cognitive\nprocess can deﬁnitely enlighten the path of development to make our systems\nartiﬁcially intelligent. Computational models of cognition, creative insight,\nskill acquisition and the design of instructional software, as well as other top-\nics in higher cognition needs to be reconsidered. It is noteworthy to see the\nhumans perceptive systems, visual or speech/audio, etc., both are essentially\nlayered and hierarchical in structure. Thus, it is natural to believe that the\nstate-of-the-art can be advanced in processing these types of natural lan-\nguage if structurally eﬃcient and eﬀective learning model can be developed.\nThe layered structure of human learning shown in Kaplan & Sadock’s Com-\nprehensive Textbook of Psychiatry (Sadock, 2000, ﬁg. 24-1) is instrumental\nin visualization of complexity and multi-stage structure involved. Moreover,\nthe interconnection and synergy between the layers is equally vital.\nReal systems are dynamic in nature and reform continuous shift that\nproduce perceptual diﬀerence. If the shift is in upward direction (in hierar-\nchical multi-layered model) that results in high dimensionality in nature of\nexpression. Thus the expression becomes nonspeciﬁc and losses subjectiv-\nity. Nonspeciﬁc discourse are too complex to interpret and to reach to any\n7\nconclusion is very diﬃcult. From viewing the software dealing with language\nprocessing looks in direction and trends accordance to the subject Artiﬁcial\nIntelligence (AI) and Robotics that attempts to mimic the dynamic and be-\nhavioral output from human. In context of natural language processing with\nspecial reference to modality processing noticeable development observed in\nstatic format of expressions and expression of contextuality also attempted\nwithin one document as in MMAX2 from German NLP group.\nNeural network hidden layer processing and continuous modiﬁcation were\nsuccessfully executed in mechanical output in ﬁeld of machine learning. The\ndirections were explored with annotated titles like deep learning and lay-\nered approach handling are most popular among research works in language\nprocessing.\nAs per the objectives of a system, some key features that should be consid-\nered such as; relevant well-structured knowledge base with improved feature\nspace should be formed upon each processing. And this knowledge base must\nbe dynamically updated (active learning that means continuous updatation\nof the knowledge base by each layer of the model) so that it can eﬀectively\nbe useful in applications of the system.\nMachine learning has been a dominant tool in NLP for many years. How-\never, the use of traditional machine learning in NLP has been mostly lim-\nited to numerical optimization of weights for human designed representations\nand features from the text data. The goal of representation learning is to\nautomatically develop features or representations from the raw text material\nappropriate for a wide range of NLP tasks.\nDeep learning is gaining popularity very recently as it provides levels of\nabstraction. The multi-layered architecture formed due to the levels ensure\nnatural progression from low level to high level structure as seen in natural\ncomplexity. Deep learning works on the principle of formation of learning\nrepresentations.\nThe essence of deep learning is to automate the process\nof discovering eﬀective features or representations for any machine learning\ntask, including automatically transferring knowledge from one task to an-\nother concurrently. In regard to NLP, deep learning develops and makes use\nan important concept called embedding, which refers to the representation\nof symbolic information in natural language text at word-level, phrase-level,\nand even sentence-level in terms of continuous-valued vectors.\nAnother concept of multi-task learning has also shown improvements in\nlearning approaches. Multi-task learning is a machine learning approach that\nlearns to solve several related problems at the same time, using a shared\n8\nrepresentation. It can be regarded as one of the two major classes of transfer\nlearning or learning with knowledge transfer, which focuses on generalizations\nacross distributions, domains, or tasks. The other major class of transfer\nlearning is adaptive learning, where knowledge transfer is carried out in a\nsequential manner, typically from a source task to a target task.\n3.1. Recent works with deep architectures in NLP\nVariety of deep architectures like neural networks, deep belief networks\nand others has shown signiﬁcant performance in various applications of lan-\nguage processing including other ﬁelds.\nCollobert et al (2011) provide a comprehensive review on ways of apply-\ning uniﬁed neural network architectures and related deep learning algorithms\nto solve NLP problems from scratch, meaning that no traditional NLP meth-\nods are used to extract features. The recent work by Mikolov et al (2013a)\nderives word embeddings by simplifying the Neural Network Language Model\n(NNLM). It is found that the NNLM can be successfully trained in two\nsteps. Yet another deep learning approach to machine translation appeared\nin Mikolov et al (2013b).\nOne most interesting NLP task recently tackled by deep learning meth-\nods is that of knowledge base (ontology) completion, which is instrumental in\nquestion-answering and many other NLP applications. An early work in this\nspace came from Bordes et al (2011), where a process is introduced to auto-\nmatically learn structured distributed embeddings of knowledge bases. The\nproposed representations in the continuous-valued vector space are compact\nand can be eﬃciently learned from large-scale data of entities and relations.\nA specialized neural network architecture is used. In the follow-up work that\nfocuses on multi-relational data (Bordes et al, 2014), the semantic matching\nenergy model is proposed to learn vector representations for both entities\nand relations.\nOther recent works Socher et al (2013) and Bowman (2013), adopts an\napproach, based on the use of neural tensor networks, to attack the problem\nof reasoning over a large joint knowledge graph for relation classiﬁcation.\nThe knowledge graph is represented as triples of a relation between two en-\ntities, and the authors aim to develop a neural network model suitable for\ninference over such relationships. The model they presented is a neural ten-\nsor network, with one layer only but it would be encouraged to work further\non multi-layered network models. The network is used to represent entities\n9\nin ﬁxed-dimensional vectors, which are created separately by averaging pre-\ntrained word embedding vectors. It then learns the tensor with the newly\nadded relationship element that describes the interactions among all the la-\ntent components in each of the relationships. Experimentally, Socher et al.,\nshows that this tensor model can eﬀectively classify unseen relationships in\nWordNet and FreeBase. Thus, models built on tensors can contribute upto\ncertain extent for reasoning over relationships between entities enhancing\nknowledge bases for improved performance. Works utilizing Recursive Neu-\nral Networks (RNN) for syntactic parsing and word representations has been\nperformed in Luong et al (2013); Socher et al (2010). Deep neural networks\nhave been popular and are well performing as they are intrinsically multi-\nlayered in structure.\nDeep learning is a hot area of research and there is still much potential\nfor signiﬁcant advances. It can be said that the paradigm of deep learning\narchitectures can improve the results of our models upto quite a certain ex-\ntent; but there is still a limit to it considering the whole problem statement.\nThis is because the deep neural networks are yet a kind of black box model\nin terms of functionality. By revising models and designs enhancement in\nperformance of deep learning algorithms can surely be made as per speciﬁc\napplication domain but there would be a bound to the possible improvements\nand the available approaches wouldnt provide enough means to the desired\nlevel of Artiﬁcial Intelligence. Approaches that are multi-layered and prefer-\nably white box models would be essentially important for organization and\ncontrol of intermediate layers functionalities.\nAs mentioned above, not only for application in NLP but for any ap-\nplication that deals with dynamics of real world conditions, development\nof complex system using combination of several simple modular and multi-\nlayered hierarchal architectures with the key features will be helpful. And\nhopefully such models can attain better linguistic understanding accuracy\nthat would be contributory to the application expertise.\nReferences\nBaker K, Bloodgood M, Dorr BJ, Callison-Burch C, Filardo NW, Piatko C,\nLevin L, Miller S (2012) Modality and negation in simt use of modality and\nnegation in semantically-informed syntactic mt. Computational Linguistics\n38(2):411–438\n10\nBordes A, Weston J, Collobert R, Bengio Y, et al (2011) Learning structured\nembeddings of knowledge bases. In: AAAI\nBordes A, Glorot X, Weston J, Bengio Y (2014) A semantic matching en-\nergy function for learning with multi-relational data. Machine Learning\n94(2):233–259\nBowman SR (2013) Can recursive neural tensor networks learn logical rea-\nsoning? arXiv preprint arXiv:13126192\nCollobert R, Weston J, Bottou L, Karlen M, Kavukcuoglu K, Kuksa P (2011)\nNatural language processing (almost) from scratch. The Journal of Ma-\nchine Learning Research 12:2493–2537\nHendrickx I, Mendes A, Mencarelli S (2012) Modality in text: a proposal for\ncorpus annotation. In: LREC, pp 1805–1812\nLuong MT, Socher R, Manning C (2013) Better word representations with\nrecursive neural networks for morphology. CoNLL-2013 104\nManning CD, Sch¨utze H (1999) Foundations of statistical natural language\nprocessing. MIT press\nMatsuyoshi S, Eguchi M, Sao C, Murakami K, Inui K, Matsumoto Y (2010)\nAnnotating event mentions in text with modality, focus, and source infor-\nmation. In: LREC\nMikolov T, Chen K, Corrado G, Dean J (2013a) Eﬃcient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:13013781\nMikolov T, Le QV, Sutskever I (2013b) Exploiting similarities among lan-\nguages for machine translation. arXiv preprint arXiv:13094168\nNissim M, Pietrandrea P, Sanso A, Mauri C (2013) Cross-linguistic anno-\ntation of modality: a data-driven hierarchical model. In: Workshop on\nInteroperable Semantic Annotation, p 7\nPakray P, Bhaskar P, Banerjee S, Bandyopadhyay S, Gelbukh AF (2012) An\nautomatic system for modality and negation detection. In: CLEF (Online\nWorking Notes/Labs/Workshop), Citeseer\nPalmer FR (2014) Modality and the English modals. Routledge\n11\nPustejovsky J, Hanks P, Sauri R, See A, Gaizauskas R, Setzer A, Radev\nD, Sundheim B, Day D, Ferro L, et al (2003) The timebank corpus. In:\nCorpus linguistics, vol 2003, p 40\nRatnaparkhi A, et al (1996) A maximum entropy model for part-of-speech\ntagging. In: Proceedings of the conference on empirical methods in natural\nlanguage processing, Philadelphia, PA, vol 1, pp 133–142\nRubin VL (2010) Epistemic modality: From uncertainty to certainty in the\ncontext of information seeking as interactions with texts. Information Pro-\ncessing & Management 46(5):533–540\nRubin VL, Kando N, Liddy ED (2004) Certainty categorization model. In:\nAAAI spring symposium: Exploring attitude and aﬀect in text: Theories\nand applications, Stanford, CA\nRubin VL, Liddy ED, Kando N (2006) Certainty identiﬁcation in texts: Cat-\negorization model and manual tagging results. In: Computing attitude and\naﬀect in text: Theory and applications, Springer, pp 61–76\nRubinstein A, Harner H, Krawczyk E, Simonson D, Katz G, Portner P (2013)\nToward ﬁne-grained annotation of modality in text. In: Proceedings of the\nTenth International Conference for Computational Semantics (IWCS 2013)\nRuppenhofer J, Rehbein I (2012) Yes we can!?\nannotating the senses of\nenglish modal verbs. In: Proceedings of the 8th International Conference\non Language Resources and Evaluation (LREC), pp 24–26\nSadock BJ (2000) Kaplan & Sadock’s Comprehensive Textbook of Psychiatry\n(2 Volume Set). Lippincott Williams & Wilkins\nSanders J, Spooren W (1997) Perspective, sulijectivity, and modality from\na cognitive. inguis ic point of view. Discourse and perspective in cognitive\nlinguistics 151:85\nSaur´ı R, Pustejovsky J (2009) Factbank: A corpus annotated with event\nfactuality. Language resources and evaluation 43(3):227–268\nSaur´ı R, Verhagen M, Pustejovsky J (2006a) Annotating and recognizing\nevent modality in text. In: Proceedings of 19th International FLAIRS\nConference\n12\nSaur´ı R, Verhagen M, Pustejovsky J (2006b) Slinket: a partial modal parser\nfor events. In: In Language Resources and Evaluation Conference, LREC\n2006, Citeseer\nSocher R, Manning CD, Ng AY (2010) Learning continuous phrase represen-\ntations and syntactic parsing with recursive neural networks. In: Proceed-\nings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning\nWorkshop, pp 1–9\nSocher R, Chen D, Manning CD, Ng A (2013) Reasoning with neural tensor\nnetworks for knowledge base completion. In: Advances in Neural Informa-\ntion Processing Systems, pp 926–934\nToutanova K, Manning CD (2000) Enriching the knowledge sources used in a\nmaximum entropy part-of-speech tagger. In: Proceedings of the 2000 Joint\nSIGDAT conference on Empirical methods in natural language processing\nand very large corpora: held in conjunction with the 38th Annual Meeting\nof the Association for Computational Linguistics-Volume 13, Association\nfor Computational Linguistics, pp 63–70\nVon Fintel K (2006) Modality and language\n13\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2015-04-18",
  "updated": "2015-04-18"
}