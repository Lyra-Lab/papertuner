{
  "id": "http://arxiv.org/abs/1504.08215v1",
  "title": "Lateral Connections in Denoising Autoencoders Support Supervised Learning",
  "authors": [
    "Antti Rasmus",
    "Harri Valpola",
    "Tapani Raiko"
  ],
  "abstract": "We show how a deep denoising autoencoder with lateral connections can be used\nas an auxiliary unsupervised learning task to support supervised learning. The\nproposed model is trained to minimize simultaneously the sum of supervised and\nunsupervised cost functions by back-propagation, avoiding the need for\nlayer-wise pretraining. It improves the state of the art significantly in the\npermutation-invariant MNIST classification task.",
  "text": "Lateral Connections in Denoising Autoencoders\nSupport Supervised Learning\nAntti Rasmus\nAalto University, Finland\nHarri Valpola\nZenRobotics Ltd., Finland\nTapani Raiko\nAalto University, Finland\nAbstract\nWe show how a deep denoising autoencoder with lateral connections can be used\nas an auxiliary unsupervised learning task to support supervised learning. The\nproposed model is trained to minimize simultaneously the sum of supervised and\nunsupervised cost functions by back-propagation, avoiding the need for layer-\nwise pretraining. It improves the state of the art signiﬁcantly in the permutation-\ninvariant MNIST classiﬁcation task.\n1\nIntroduction\nCombining an auxiliary task to help train a neural network was proposed by Suddarth and Kergosien\n(1990). By sharing the hidden representations among more than one task, the network generalizes\nbetter. Hinton and Salakhutdinov (2006) proposed that this auxiliary task could be unsupervised\nmodelling of the inputs. Ranzato and Szummer (2008) used autoencoder reconstruction as auxiliary\ntask for classiﬁcation but performed the training layer-wise.\nSietsma and Dow (1991) proposed to corrupt network inputs with noise as a regularization method.\nDenoising autoencoders (Vincent et al., 2010) use the same principle to create unsupervised models\nfor data. Rasmus et al. (2015) showed that modulated lateral connections in denoising autoencoder\nchange its properties in a fundamental way making it more suitable as an auxiliary task for super-\nvised training:\n• Lateral connections allow the detailed information to ﬂow directly to the decoder relieving\nthe pressure of higher layers to represent all information and allowing them to concentrate\non more abstract features. In contrast to a deep denoising autoencoder, encoder can discard\ninformation on the way up similarly to typical supervised learning tasks discard irrelevant\ninformation.\n• With lateral connections, the optimal model shape is pyramid like, i.e. the dimensionality\nof the top layers is lower than the bottom layers, which is also true for typical supervised\nlearning tasks, as opposed to traditional denoising autoencoders which prefer layers that\nare equal in size.\nThis paper builds on top the previous work and shows that using denoising autoencoder with lateral\nconnections as an auxiliary task for supervised learning improves network’s generalization capa-\nbility as hypothesized by Valpola (2015). The proposed method achieves state-of-the-art results in\npermutation invariant MNIST classiﬁcation task.\n2\nProposed Model\nThe encoder of the autoencoder acts as the multilayer perceptron network for the supervised task\nso that the prediction is made in the highest layer of the encoder as depicted in Figure 1. For the\n1\narXiv:1504.08215v1  [cs.LG]  30 Apr 2015\n˜x\nˆx\nz(2)\nz(1)\nˆz(1)\nˆz(2)\ny\nz(3)\nˆz(3)\nh(1)\nh(2)\nu(2)\nu(1)\nu(0)\nFigure 1: The conceptual illustration of the model when L = 3. Encoder path from ˜x →y is a\nmultilayer perceptron network, bold arrows indicating fully connected weights W(1) . . . W(3) up-\nwards and V(3) . . . V(1) downwards and thin arrows neuron-wise connections. z(l) are normalized\npreactivations, ˆz(l) their denoised versions, and ˆx denoised reconstruction of the input. u(l) are\nprojections of ˆz(l+1) in the dimensions of z(l). h(l) are the activations and y the class prediction.\ndecoder, we follow the model by Rasmus et al. (2015) but with more expressive decoder function\nand other minor modiﬁcations described in Section 2.2.\n2.1\nEncoder and Classiﬁer\nWe follow Ioffe and Szegedy (2015) to apply batch normalization to each preactivation including\nthe topmost layer in L-layer network to ensure fast convergence due to reduced covariate shift.\nFormally, when input h(0) = ˜x and l = 1 . . . L\nz(l) = NB(W(l)h(l−1))\nh(l)\ni\n= φ(γ(l)\ni (z(l)\ni\n+ β(l)\ni ))\nwhere NB is a component-wise batch normalization NB(xi) =\nxi−ˆµxi\nˆσxi\n, where ˆµxi and ˆσxi are\nestimates calculated from the minibatch, γ(l)\ni\nand β(l)\ni\nare trainable parameters, and φ(·) = max(0, ·)\nis the rectiﬁcation nonlinearity, which is replaced by the softmax for the output y = h(L).\nAs batch normalization is reported to reduce the need of dropout-style regularization, we only add\nisotropic Gaussian noise n to the inputs, ˜x = h(0) = x + n.\nThe supervised cost is average negative log probability of the targets t(n) given the inputs x(n)\nCclass = −1\nN\nN\nX\nn=1\nlog P(Y = t(n) | x(n)).\n2.2\nDecoder for Unsupervised Auxiliary Task\nThe unsupervised auxiliary task performs denoising similar to traditional denoising autoencoder,\nthat is, it tries to match the reconstruction ˆx with the original x.\nLayer sizes in the decoder are symmetric to the encoder and corresponding decoder layer ˆz(l) is\ncalculated from lateral connection z(l) and vertical connection ˆz(l+1). Lateral connections are re-\nstricted so that each unit i in an encoder layer is connected to only one unit i in the corresponding\n2\ndecoder layer, but vertical connections are fully connected and projected to the same space as z(l)\nby\nu(l) = V(l+1)ˆz(l+1),\nand lateral neuron-wise connection for the ith neuron is\nˆzi = ai1zi + ai2σ(ai3zi + ai4) + ai5,\naij = cijui + dij,\nwhere superscripts (l) are dropped to avoid clutter, σ(·) is the sigmoid nonlinearity, and c(l)\nij and d(l)\nij\nare the trainable parameters. This type of parametrization allows the network to use information\nfrom higher layer for any a(l)\nij . The highest layer L has u(L) = 0 and the lowest layer ˆx = ˆz(0) and\nz(0) = ˜x.\nValpola (2015, Section 4.1) discusses how denoising functions represent corresponding distribu-\ntions. The proposed parametrization suits many different distributions, e.g. super- and sub-Gaussian,\nand multimodal. Parameter ai2 deﬁnes the distance of peaks in multimodal distributions (also the\nratio of variances if the distribution is a mixture of two distributions with the same mean but dif-\nferent variance). Moreover, this kind of decoder function is able to emulate both the additive and\nmodulated connections that were analyzed by Rasmus et al. (2015).\nThe cost function for unsupervised path is the mean squared error, nx being the dimensionality of\nthe data\nCreconst = −1\nN\nN\nX\nn=1\n1\nnx\n||ˆx(n) −x(n)||2\nThe training criterion is a combination of the two such that multiplier η determines how much the\nauxiliary cost is used, and the case η = 0 corresponds to pure supervised learning:\nC = Cclass + ηCreconst\nThe parameters of the model include W(l), γ(l), and β(l) for the encoder, and V(l), c(l)\nj , and d(l)\nj\nfor the decoder. The encoder and decoder have roughly the same number of parameters because the\nmatrices V(l) equal to W(l) in size. The only difference comes from per-neuron parameters, which\nencoder has only two (γi and βi), but the decoder has ten (cij and dij, j = 1 . . . 5).\n3\nExperiments\nIn order to evaluate the impact of unsupervised auxiliary cost to the generalization performance, we\ntested the model with MNIST classiﬁcation task. We randomly split the data into 50.000 examples\nfor training and 10.000 examples for validation. The validation set was used for evaluating the\nmodel structure and hyperparameters and ﬁnally to train model for test error evaluation. To improve\nstatistical reliability, we considered the average of 10 runs with different random seeds. Both the\nsupervised and unsupervised cost functions use the same training data.\nModel training took 100 epochs with minibatch size of 100, equalling to 50.000 weight updates.\nWe used Adam optimization algorithm (Kingma and Ba, 2015) for weight updates adjusting the\nlearning rate according to a schedule where the learning rate is linearly reduced to zero during the\nlast 50 epochs starting from 0.002. We tested two models with layer sizes 784-1000-500-10 and 784-\n1000-500-250-250-250-10, of which the latter worked better and is reported in this paper. The best\ninput noise level was σ = 0.3 and chosen from {0.1, 0.3, 0.5}. There are plenty of hyperparameters\nand various model structures left to tune but we were satisﬁed with the reported results.\n3.1\nResults\nFigure 2 illustrates how auxiliary cost impacts validation error by showing the error as a function of\nthe multiplier η. The auxiliary task is clearly beneﬁcial and in this case the best tested value for η is\n500.\nThe best hyperparameters were chosen based on the validation error results and then retrained 10\ntimes with all 60.000 samples and measured against the test data. The worst test error was 0.72 %,\n3\n0\n1000\n2000\n3000\n4000\nUnsupervised cost multiplier, η\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nError (%)\nMNIST classiﬁcation error\nAvg validation error\nAvg test error\nFigure 2: Average validation error as a function of unsupervised auxiliary cost multiplier η and\naverage test error for the cases η = 0 and η = 500 over 10 runs. η = 0 corresponds to pure super-\nvised training. Error bars show the sample standard deviation. Training included 50.000 samples\nfor validation but for test error all 60.000 labeled samples were used.\nMethod\nTest error\nSVM\n1.40 %\nMP-DBM Goodfellow et al. (2013)\n0.91 %\nThis work, η = 0\n0.89 %\nManifold Tangent Classiﬁer Rifai et al. (2011)\n0.81 %\nDBM pre-train + Do Srivastava et al. (2014)\n0.79 %\nMaxout + Do + adv Goodfellow et al. (2015)\n0.78 %\nThis work, η = 500\n0.68 %\nTable 1: A collection of previously reported MNIST test errors in permutation-invariant setting. Do:\nDropout, adv: Adversarial training, DBM: deep Boltzmann machine.\ni.e. 72 misclassiﬁed examples, and the average 0.684 % which is signiﬁcantly lower than the previ-\nously reported 0.782 %. For comparison, we computed the average test error for the η = 0 case, i.e.\nsupervised learning with batch normalization, and got 0.89 %.\n4\nRelated Work\nMulti-prediction deep Boltzmann machine (MP-DBM) (Goodfellow et al., 2013) is a way to train a\nDBM with back-propagation through variational inference. The targets of the inference include both\nsupervised targets (classiﬁcation) and unsupervised targets (reconstruction of missing inputs) that\nare used in training simultaneously. The connections through the inference network are somewhat\nanalogous to our lateral connections. Speciﬁcally, there are inference paths from observed inputs to\nreconstructed inputs that do not go all the way up to the highest layers. Compared to our approach,\nMP-DBM requires an iterative inference with some initialization for the hidden activations, whereas\nin our case, the inference is a simple single-pass feedforward procedure.\n5\nDiscussion\nWe showed that a denoising autoencoder with lateral connections is compatible with supervised\nlearning using the unsupervised denoising task as an auxiliary training objective, and achieved good\nresults in MNIST classiﬁcation task with a signiﬁcant margin to the previous state of the art. We\nconjecture that the good results are due to supervised and unsupervised learning happening concur-\nrently which means that unsupervised learning can focus on the features which supervised learning\nﬁnds relevant.\nThe proposed model is simple and easy to implement with many existing feedforward architectures,\nas the training is based on back-propagation from a simple cost function. It is quick to train and\n4\nthe convergence is fast, especially with batch normalization. The proposed architecture implements\ncomplex functions such as modulated connections without a signiﬁcant increase in the number of\nparameters.\nThis work can be further improved and extended in many ways. We are currently studying the impact\nof adding noise also to z(l) and including auxiliary layer-wise reconstruction costs ||ˆz(l)−z(l)||2, and\nworking on extending these preliminary experiments to larger datasets, to semi-supervised learning\nproblems, and convolutional networks.\nReferences\nGoodfellow, I., Mirza, M., Courville, A., and Bengio, Y. (2013). Multi-prediction deep Boltzmann\nmachines. In Advances in Neural Information Processing Systems, pages 548–556.\nGoodfellow, I., Shlens, J., and Szegedy, C. (2015). Explaining and harnessing adversarial examples.\narXiv:1412.6572.\nHinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786), 504–507.\nIoffe, S. and Szegedy, C. (2015).\nBatch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv:1502.03167.\nKingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. arXiv:1412.6980.\nRanzato, M. A. and Szummer, M. (2008). Semi-supervised learning of compact document repre-\nsentations with deep networks. In Proceedings of the 25th International Conference on Machine\nLearning, ICML ’08, pages 792–799. ACM.\nRasmus, A., Raiko, T., and Valpola, H. (2015).\nDenoising autoencoder with modulated lateral\nconnections learns invariant representations of natural images. arXiv:1412.7210.\nRifai, S., Dauphin, Y. N., Vincent, P., Bengio, Y., and Muller, X. (2011). The manifold tangent\nclassiﬁer. In Advances in Neural Information Processing Systems, pages 2294–2302.\nSietsma, J. and Dow, R. J. (1991).\nCreating artiﬁcial neural networks that generalize.\nNeural\nnetworks, 4(1), 67–79.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout:\nA simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning\nResearch, 15(1), 1929–1958.\nSuddarth, S. C. and Kergosien, Y. (1990). Rule-injection hints as a means of improving network\nperformance and learning time. In Neural Networks, pages 120–129. Springer.\nValpola, H. (2015). From neural PCA to deep unsupervised learning. In Advances in Independent\nComponent Analysis and Learning Machines. Elsevier. Preprint available as arXiv:1411.7783.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising criterion.\nThe Journal of Machine Learning Research, 11, 3371–3408.\n5\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2015-04-30",
  "updated": "2015-04-30"
}