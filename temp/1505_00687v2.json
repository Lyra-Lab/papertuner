{
  "id": "http://arxiv.org/abs/1505.00687v2",
  "title": "Unsupervised Learning of Visual Representations using Videos",
  "authors": [
    "Xiaolong Wang",
    "Abhinav Gupta"
  ],
  "abstract": "Is strong supervision necessary for learning a good visual representation? Do\nwe really need millions of semantically-labeled images to train a Convolutional\nNeural Network (CNN)? In this paper, we present a simple yet surprisingly\npowerful approach for unsupervised learning of CNN. Specifically, we use\nhundreds of thousands of unlabeled videos from the web to learn visual\nrepresentations. Our key idea is that visual tracking provides the supervision.\nThat is, two patches connected by a track should have similar visual\nrepresentation in deep feature space since they probably belong to the same\nobject or object part. We design a Siamese-triplet network with a ranking loss\nfunction to train this CNN representation. Without using a single image from\nImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train\nan ensemble of unsupervised networks that achieves 52% mAP (no bounding box\nregression). This performance comes tantalizingly close to its\nImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We\nalso show that our unsupervised network can perform competitively in other\ntasks such as surface-normal estimation.",
  "text": "Unsupervised Learning of Visual Representations using Videos\nXiaolong Wang, Abhinav Gupta\nRobotics Institute, Carnegie Mellon University\nAbstract\nIs strong supervision necessary for learning a good\nvisual representation?\nDo we really need millions of\nsemantically-labeled images to train a Convolutional Neu-\nral Network (CNN)? In this paper, we present a simple yet\nsurprisingly powerful approach for unsupervised learning\nof CNN. SpeciÔ¨Åcally, we use hundreds of thousands of un-\nlabeled videos from the web to learn visual representations.\nOur key idea is that visual tracking provides the supervi-\nsion. That is, two patches connected by a track should have\nsimilar visual representation in deep feature space since\nthey probably belong to the same object or object part. We\ndesign a Siamese-triplet network with a ranking loss func-\ntion to train this CNN representation. Without using a sin-\ngle image from ImageNet, just using 100K unlabeled videos\nand the VOC 2012 dataset, we train an ensemble of un-\nsupervised networks that achieves 52% mAP (no bound-\ning box regression). This performance comes tantalizingly\nclose to its ImageNet-supervised counterpart, an ensemble\nwhich achieves a mAP of 54.4%. We also show that our\nunsupervised network can perform competitively in other\ntasks such as surface-normal estimation.\n1. Introduction\nWhat is a good visual representation and how can we\nlearn it? At the start of this decade, most computer vision\nresearch focused on ‚Äúwhat‚Äù and used hand-deÔ¨Åned features\nsuch as SIFT [32] and HOG [5] as the underlying visual\nrepresentation. Learning was often the last step where these\nlow-level feature representations were mapped to seman-\ntic/3D/functional categories. However, the last three years\nhave seen the resurgence of learning visual representations\ndirectly from pixels themselves using the deep learning\nand Convolutional Neural Networks (CNNs) [28, 24, 23].\nAt the heart of CNNs is a completely supervised learning\nparadigm. Often millions of examples are Ô¨Årst labeled us-\ning Mechanical Turk followed by data augmentation to cre-\nate tens of millions of training instances. CNNs are then\ntrained using gradient descent and back propagation. But\none question still remains: is strong-supervision necessary\nfor training these CNNs? Do we really need millions of\nsemantically-labeled images to learn a good representation?\n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \nLearning to Rank \nConv \nNet \nConv \nNet \nConv \nNet \nQuery \n(First Frame) \nTracked \n(Last Frame) \nNegative  \n(Random) \n(a) Unsupervised Tracking in Videos  \nùê∑ \n, \nùê∑ \n, \nùê∑ \n, \nùê∑ \n, \nùê∑: Distance in deep feature space \n(b) Siamese-triplet Network \n(c) Ranking Objective  \nFigure 1. Overview of our approach. (a) Given unlabeled videos,\nwe perform unsupervised tracking on the patches in them. (b)\nTriplets of patches including query patch in the initial frame of\ntracking, tracked patch in the last frame, and random patch from\nother videos are fed into our siamese-triplet network for train-\ning. (c) The learning objective: Distance between the query and\ntracked patch in feature space should be smaller than the distance\nbetween query and random patches.\nIt seems humans can learn visual representations using little\nor no semantic supervision but our approaches still remain\ncompletely supervised.\nIn this paper, we explore the alternative: how we can ex-\nploit the unlabeled visual data on the web to train CNNs\n(e.g. AlexNet [24])? In the past, there have been several at-\ntempts at unsupervised learning using millions of static im-\nages [26, 44] or frames extracted from videos [56, 48, 34].\nThe most common architecture used is an auto-encoder\nwhich learns representations based on its ability to recon-\nstruct the input images [35, 3, 49, 37]. While these ap-\nproaches have been able to automatically learn V1-like Ô¨Ål-\nters given unlabeled data, they are still far away from su-\npervised approaches on tasks such as object detection. So,\nwhat is the missing link? We argue that static images them-\nselves might not have enough information to learn a good\n1\narXiv:1505.00687v2  [cs.CV]  6 Oct 2015\nvisual representation. But what about videos? Do they have\nenough information to learn visual representations? In fact,\nhumans also learn their visual representations not from mil-\nlions of static images but years of dynamic sensory inputs.\nCan we have similar learning capabilities for CNNs?\nWe present a simple yet surprisingly powerful approach\nfor unsupervised learning of CNNs using hundreds of thou-\nsands of unlabeled videos from the web. Visual tracking is\none of the Ô¨Årst capabilities that develops in infants and often\nbefore semantic representations are learned1. Taking a leaf\nfrom this observation, we propose to exploit visual track-\ning for learning CNNs in an unsupervised manner. SpeciÔ¨Å-\ncally, we track millions of ‚Äúmoving‚Äù patches in hundreds of\nthousands of videos. Our key idea is that two patches con-\nnected by a track should have similar visual representation\nin deep feature space since they probably belong to the same\nobject. We design a Siamese-triplet network with ranking\nloss function to train the CNN representation. This ranking\nloss function enforces that in the Ô¨Ånal deep feature space\nthe Ô¨Årst frame patch should be much closer to the tracked\npatch than any other randomly sampled patch. We demon-\nstrate the strength of our learning algorithm using exten-\nsive experimental evaluation. Without using a single image\nfrom ImageNet, just 100K unlabeled videos and VOC 2012\ndataset, we train an ensemble of AlexNet networks that\nachieves 52% mAP (no bounding box regression). This per-\nformance is similar to its ImageNet-supervised counterpart,\nan ensemble which achieves 54.4% mAP. We also show that\nour network trained using unlabeled videos achieves simi-\nlar performance to its completely supervised counterpart on\nother tasks such as surface normal estimation. We believe\nthis is the Ô¨Årst time an unsupervised-pretrained CNN has\nbeen shown so competitive; that too on varied datasets and\ntasks. SpeciÔ¨Åcally for VOC, we would like to put our re-\nsults in context: this is the best results till-date by using\nonly PASCAL-provided annotations (next best is scratch at\n44%).\n2. Related Work\nUnsupervised learning of visual representations has a\nrich history starting from original auto-encoders work of\nOlhausen and Field [35]. Most of the work in this area\ncan be broadly divided into three categories.\nThe Ô¨Årst\nclass of algorithms focus on learning generative models\nwith strong priors [20, 46]. These algorithms essentially\ncapture co-occurrence statistics of features.\nThe second\nclass of algorithms use manually deÔ¨Åned features such as\nSIFT or HOG and perform clustering over training data\nto discover semantic classes [42, 38]. Some of these re-\ncent algorithms also focus on learning mid-level repre-\nsentations rather than discovering semantic classes them-\n1http://www.aoa.org/patients-and-public/good-vision-throughout-\nlife/childrens-vision/infant-vision-birth-to-24-months-of-age\nselves [41, 6, 7]. The third class of algorithms and more\nrelated to our paper is unsupervised learning of visual rep-\nresentations from the pixels themselves using deep learning\napproaches [21, 26, 44, 39, 29, 47, 9, 33, 2, 49, 8]. Starting\nfrom the seminal work of Olhausen and Field [35], the goal\nis to learn visual representations which are (a) sparse and\n(b) reconstructive. Olhausen and Field [35] showed that us-\ning this criteria they can learn V1-like Ô¨Ålters directly from\nthe data. However, this work only focused on learning a sin-\ngle layer. This idea was extended by Hinton and Salakhut-\ndinov [21] to train a deep belief network in an unsuper-\nvised manner via stacking layer-by-layer RBMs. Similar to\nthis, Bengio et al. [3] investigated stacking of both RBMs\nand autoencoders. As a next step, Le et al. [26] scaled up\nthe learning of multi-layer autoencoder on large-scale unla-\nbeled data. They demonstrated that although the network is\ntrained in an unsupervised manner, the neurons in high lay-\ners can still have high responses on semantic objects such\nas human heads and cat faces. Sermanet et al. [39] applied\nconvolutional sparse coding to pre-train the model layer-by-\nlayer in unsupervised manner. The model is then Ô¨Åne-tuned\nfor pedestrian detection. In a contemporary work, Doersch\net al. [8] explored to use spatial context as a cue to perform\nunsupervised learning for CNNs.\nHowever, it is not clear if static images is the right way\nto learn visual representations. Therefore, researchers have\nstarted focusing on learning feature representations using\nvideos [11, 53, 27, 43, 56, 16, 48, 34, 45].\nEarly work\nsuch as [56] focused on inclusion of constraints via video\nto autoencoder framework. The most common constraint is\nenforcing learned representations to be temporally smooth.\nSimilar to this, Goroshin et al. [16] proposed to learn auto-\nencoders based on the slowness prior. Other approaches\nsuch as Taylor et al. [48] trained convolutional gated RBMs\nto learn latent representations from pairs of successive im-\nages. This was extended in a recent work by Srivastava et\nal. [43] where they proposed to learn a LSTM model in an\nunsupervised manner to predict future frames.\nFinally, our work is also related to metric learning via\ndeep networks [51, 31, 4, 17, 15, 22, 54]. For example,\nChopra et al. [4] proposed to learn convolutional networks\nin a siamese architecture for face veriÔ¨Åcation.\nWang et\nal. [51] introduced a deep triplet ranking network to learn\nÔ¨Åne-grained image similarity. Zhang et al. [55] optimized\nthe max-margin loss on triplet units to learn deep hashing\nfunction for image retrieval. However, all these methods\nrequired labeled data.\nOur work is also related to [30],\nwhich used CNN pre-trained on ImageNet classiÔ¨Åcation\nand detection dataset as initialization, and performed semi-\nsupervised learning in videos to tackle object detection in\ntarget domain. However, in our work, we propose an unsu-\npervised approach instead of semi-supervised algorithm.\n3. Overview\nOur goal is to train convolutional neural networks using\nhundreds of thousands of unlabeled videos from the Inter-\nnet. We follow the AlexNet architecture to design our base\nnetwork. However, since we do not have labels, it is not\nclear what should be the loss function and how we should\noptimize it. But in case of videos, we have another supervi-\nsory information: time. For example, we all know that the\nscene does not change drastically within a short time in a\nvideo and same object instances appear in multiple frames\nof the video. So, how do we exploit this information to train\na CNN-based representation?\nWe sample millions of patches in these videos and track\nthem over time. Since we are tracking these patches, we\nknow that the Ô¨Årst and last tracked frames correspond to the\nsame instance of the moving object or object part. There-\nfore, any visual representation that we learn should keep\nthese two data points close in the feature space. But just us-\ning this constraint is not sufÔ¨Åcient: all points can be mapped\nto a single point in feature space. Therefore, for training our\nCNN, we sample a third patch which creates a triplet. For\ntraining, we use a loss function [51] that enforces that the\nÔ¨Årst two patches connected by tracking are closer in feature\nspace than the Ô¨Årst one and a random one.\nTraining a network with such triplets converges fast since\nthe task is easy to overÔ¨Åt to. One way is to increase the\nnumber of training triplets. However, after initial conver-\ngence most triplets satisfy the loss function and therefore\nback-propagating gradients using such triplets is inefÔ¨Åcient.\nInstead, analogous to hard-negative mining, we select the\nthird patch from multiple patches that violates the constraint\n(loss is maximum).\nSelecting this patch leads to more\nmeaningful gradients for faster learning.\n4. Patch Mining in Videos\nGiven a video, we want to extract patches of interest\n(patches with motion in our case) and track these patches to\ncreate training instances. One obvious way to Ô¨Ånd patches\nof interest is to compute optical Ô¨Çow and use the high mag-\nnitude Ô¨Çow regions. However, since YouTube videos are\nnoisy with a lot of camera motion, it is hard to localize\nmoving objects using simple optical Ô¨Çow magnitude vec-\ntors. Thus we follow a two-step approach: in the Ô¨Årst step,\nwe obtain SURF [1] interest points and use Improved Dense\nTrajectories (IDT) [50] to obtain motion of each SURF\npoint. Note that since IDT applies a homography estimation\n(video stabilization) method, it reduces the problem caused\nby camera motion. Given the trajectories of SURF inter-\nest points, we classify these points as moving if the Ô¨Çow\nmagnitude is more than 0.5 pixels. We also reject frames\nif (a) very few (< 25%) SURF interest points are classiÔ¨Åed\nas moving because it might be just noise; (b) majority of\nSURF interest points (> 75%) are classiÔ¨Åed as moving as\n‚Ä¶ \n‚Ä¶ \nQuery \n(First Frame) \nTracked \n(Last Frame) \nSliding Window Searching \nTracking \nSmall Motion \nCamera Motion \nFigure 2.\nGiven the video about buses (the ‚Äúbus‚Äù label are not\nutilized), we perform IDT on it. red points represents the SURF\nfeature points, green represents the trajectories for the points. We\nreject the frames with small and large camera motions (top pairs).\nGiven the selected frame, we Ô¨Ånd the bounding box containing\nmost of the moving SURF points. We then perform tracking. The\nÔ¨Årst and last frame of the track provide pair of patches for training\nCNN.\nit corresponds to moving camera. Once we have extracted\nmoving SURF interest points, in the second step, we Ô¨Ånd the\nbest bounding box such that it contains most of the moving\nSURF points. The size of the bounding box is set as h √ó w,\nand we perform sliding window with it in the frame. We\ntake the bounding box which contains the most number of\nmoving SURF interest points as the interest bounding box.\nIn the experiment, we set h = 227, w = 227 in the frame\nwith size 448 √ó 600. Note that these patches might contain\nobjects or part of an object as shown in Figure 2.\nTracking.\nGiven the initial bounding box, we perform\ntracking using the KCF tracker [19]. After tracking along 30\nframes in the video, we obtain the second patch. This patch\nacts as the similar patch to the query patch in the triplet.\nNote that the KCF tracker does not use any supervised in-\nformation except for the initial bounding box.\n5. Learning Via Videos\nIn the previous section, we discussed how we can use\ntracking to generate pairs of patches. We use this procedure\nto generate millions of such pairs (See Figure 3 for exam-\nples of pairs of patches mined). We now describe how we\nuse these as training instances for our visual representation\nlearning.\n5.1. Siamese Triplet Network\nOur goal is to learn a feature space such that the query\npatch is closer to the tracked patch as compared to any other\nrandomly sampled patch. To learn this feature space we de-\nsign a Siamese-triplet network. A Siamese-triplet network\nconsist of three base networks which share the same param-\nQuery \n(First Frame) \nTracked \n(Last Frame) \nQuery \n(First Frame) \nTracked \n(Last Frame) \nPatch \nPairs \nPatch \nPairs \nFigure 3. Examples of patch pairs we obtain via patch mining in the videos.\neters (see Figure 4). For our experiments, we take the image\nwith size 227 √ó 227 as input. The base network is based\non the AlexNet architecture [24] for the convolutional lay-\ners. Then we stack two fully connected layers on the pool5\noutputs, whose neuron numbers are 4096 and 1024 respec-\ntively. Thus the Ô¨Ånal output of each single network is 1024\ndimensional feature space f(¬∑). We deÔ¨Åne the loss function\non this feature space.\n5.2. Ranking Loss Function\nGiven the set of patch pairs S sampled from the video,\nwe propose to learn an image similarity model in the form\nof CNN. SpeciÔ¨Åcally, given an image X as an input for the\nnetwork, we can obtain its feature in the Ô¨Ånal layer as f(X).\nThen, we deÔ¨Åne the distance of two image patches X1, X2\nbased on the cosine distance in the feature space as,\nD(X1, X2) = 1 ‚àí\nf(X1) ¬∑ f(X2)\n‚à•f(X1)‚à•‚à•f(X2)‚à•.\n(1)\nWe want to train a CNN to obtain feature representation\nf(¬∑), so that the distance between query image patch and the\ntracked patch is small and the distance between query patch\nand other random patches is encouraged to be larger. For-\nmally, given the patch set S, where Xi is the original query\npatch (Ô¨Årst patch in tracked frames), X+\ni is the tracked patch\nand X‚àí\ni is a random patch from a different video, we want\nto enforce D(Xi, X‚àí\ni ) > D(Xi, X+\ni ). Therefore, the loss\nof our ranking model is deÔ¨Åned by hinge loss as,\nL(Xi, X+\ni , X‚àí\ni ) = max{0, D(Xi, X+\ni ) ‚àíD(Xi, X‚àí\ni ) + M},\n(2)\nwhere M represents the gap parameters between two dis-\ntances. We set M = 0.5 in the experiment. Then our objec-\ntive function for training can be represented as,\nmin\nW\nŒª\n2 ‚à•W ‚à•2\n2 +\nN\nX\ni=1\nmax{0, D(Xi, X+\ni ) ‚àíD(Xi, X‚àí\ni ) + M},\n(3)\nwhere W is the parameter weights of the network, i.e., pa-\nrameters for function f(¬∑). N is the number of the triplets of\nsamples. Œª is a constant representing weight decay, which\nis set to Œª = 0.0005.\n5.3. Hard Negative Mining for Triplet Sampling\nOne non-trivial part for learning to rank is the process of\nselecting negative samples. Given a pair of similar images\nXi, X+\ni , how can we select the patch X‚àí\ni , which is a nega-\ntive match to Xi, from the large pool of patches? Here we\nÔ¨Årst select the negative patches randomly, and then Ô¨Ånd hard\nexamples (in a process analogous to hard negative mining).\nRandom Selection:\nDuring learning, we perform\nmini-batch Stochastic Gradient Descent (SGD). For each\nXi, X+\ni , we randomly sample K negative matches in the\nsame batch B, thus we have K sets of triplet of samples.\nFor every triplet of samples, we calculate the gradients over\nthree of them respectively and perform back propagation.\nNote that we shufÔ¨Çe all the images randomly after each\nepoch of training, thus the pair of patches Xi, X+\ni can look\nat different negative matches each time.\nHard Negative Mining: While one can continue to sam-\nple random patches for creating the triplets, it is more efÔ¨Å-\ncient to search the negative patches smartly. After 10 epochs\nof training using negative data selected randomly, we want\nto make the problem harder to get more robust feature rep-\nresentations. Analogous to hard-negative mining procedure\nin SVM, where gradient descent learning is only performed\non hard-negatives (not all possible negative), we search for\nùëãùëñ\n+ \nùëãùëñ\n‚àí \nùëãùëñ \nùëì(ùëãùëñ\n+) \nùëì(ùëãùëñ\n‚àí) \nùëì(ùëãùëñ) \nRanking  \nLoss \nLayer \nShared Weights \nShared Weights \n96 256 384 \n384 \n256 \n4096 \n1024 \nFigure 4.\nSiamese-triplet network. Each base network in the\nSiamese-triplet network share the same architecture and parameter\nweights. The architecture is rectiÔ¨Åed from AlexNet by using only\ntwo fully connected layers. Given a triplet of training samples,\nwe obtain their features from the last layer by forward propagation\nand compute the ranking loss.\nFigure 5.\nTop response regions for the pool5 neurons of our\nunsupervised-CNN. Each row shows top response of one neuron.\nthe negative patch such that the loss is maximum and use\nthat patch to compute and back propagate gradients.\nSpeciÔ¨Åcally, the sampling of negative matches is similar\nas random selection before, except that this time we select\naccording to the loss(Eq. 2). For each pair Xi, X+\ni , we cal-\nculate the loss of all other negative matches in batch B, and\nselect the top K ones with highest losses. We apply the loss\non these K negative matches as our Ô¨Ånal loss and calculate\nthe gradients over them. Since the feature of each sample\nis already computed after the forward propagation, we only\nneed to calculate the loss over these features, thus the extra\ncomputation for hard negative mining is very small. For the\nexperiments, we use K = 4. Note that while some of the\nnegatives might be semantically similar patches, our em-\nbedding constraint only requires same instance examples to\nbe closer than category examples (which can be closer than\nother negatives in the space).\n5.4. Adapting for Supervised Tasks\nGiven the CNN learned by using unsupervised data, we\nwant to transfer the learned representations to the tasks with\nsupervised data. In our experiments, we apply our model\nto two different tasks including object detection and sur-\nface normal estimation. In both tasks we take the base net-\nwork from our Siamese-triplet network and adjust the fully\nconnected layers and outputs accordingly.\nWe introduce\ntwo ways to Ô¨Åne-tune and transfer the information obtained\nfrom unsupervised data to supervised learning.\nOne straight forward approach is directly applying our\nranking model as a pre-trained network for the target task.\nMore speciÔ¨Åcally, we use the parameters of the convolu-\ntional layers in the base network of our triplet architecture\nas initialization for the target task. For the fully connected\nlayers, we initialize them randomly. This method of trans-\nferring feature representation is very similar to the approach\napplied in RCNN [14]. However, RCNN uses the network\npre-trained with ImageNet ClassiÔ¨Åcation data. In our case,\nthe unsupervised ranking task is quite different from object\ndetection and surface normal estimation. Thus, we need\nto adapt the learning rate to the Ô¨Åne-tuning procedure in-\ntroduced in RCNN. We start with the learning rate with\nœµ = 0.01 instead of 0.001 and set the same learning rate\nfor all layers. This setting is crucial since we want the pre-\ntrained features to be used as initialization of supervised\nlearning, and adapting the features to the new task.\nIn this paper,\nwe explore one more approach to\ntransfer/Ô¨Åne-tune the network. SpeciÔ¨Åcally, we note that\nthere might be more juice left in the millions of unsuper-\nvised training data (which could not be captured in the ini-\ntial learning stage).\nTherefore, we use an iterative Ô¨Åne-\ntuning scheme. Given the initial unsupervised network, we\nÔ¨Årst Ô¨Åne-tune using the PASCAL VOC data. Given the new\nÔ¨Åne-tuned network, we use this network to re-adapt to rank-\ning triplet task. Here we again transfer convolutional pa-\nrameters for re-adapting. Finally, this re-adapted network is\nÔ¨Åne-tuned on the VOC data yielding a better trained model.\nWe show in the experiment that this circular approach gives\nimprovement in performance. We also notice that after two\niterations of this approach the network converges.\n5.5. Model Ensemble\nWe proposed an approach to learn CNNs using unlabeled\nvideos. However, there is absolutely no limit to generating\ntraining instances and pairs of tracked patches (YouTube\nhas more than billions of videos). This opens up the possi-\nbility of training multiple CNNs using different sets of data.\nOnce we have trained these CNNs, we append the fc7 fea-\ntures from each of these CNNs to train the Ô¨Ånal SVM. Note\nthat the ImageNet trained models also provide initial boost\nfor adding more networks (See Table 1).\n5.6. Implementation Details\nWe apply mini-batch SGD in training. As the 3 networks\nshare the same parameters, instead of inputting 3 samples\nto the triplet network, we perform the forward propagation\nfor the whole batch by a single network and calculate the\nloss based on the output feature. Given a pair of patches\nXi, X+\ni , we randomly select another patch X‚àí\ni ‚ààB which\nis extracted in a different video from Xi, X+\ni . Given their\nfeatures from forward propagation f(Xi), f(X+\ni ), f(X‚àí\ni ),\nwe can compute the loss as Eq. 2.\nFor unsupervised learning, we download 100K videos\nfrom YouTube using the URLs provided by [30]. [30] used\nthousands of keywords to retrieve videos from YouTube.\nNote we drop the labels associated with each video. By per-\nforming our patch mining method on the videos, we obtain\n8 million image patches. We train three different networks\nseparately using 1.5M, 5M and 8M training samples. We\nreport numbers based on these three networks. To train our\nsiamese-triplet networks, we set the batch size as |B| = 100,\n(a) Unsupervised Pre-trained \n(b) Fine-tuned  \nFigure 6.\nConv1 Ô¨Ålters visualization. (a) The Ô¨Ålters of the Ô¨Årst\nconvolutional layer of the siamese-triplet network trained in unsu-\npervised manner. (b) By Ô¨Åne-tuning the unsupervised pre-trained\nnetwork on PASCAL VOC 2012, we obtain sharper Ô¨Ålters.\nthe learning rate starting with œµ0 = 0.001. We Ô¨Årst train our\nnetwork with random negative samples at this learning rate\nfor 150K iterations, and then we apply hard negative min-\ning based on it. For training on 1.5M patches, we reduce\nthe learning rate by a factor of 10 at every 80K iterations\nand train for 240K iterations. For training on 5M and 8M\npatches, we reduce the learning rate by a factor of 10 at ev-\nery 120K iterations and train for 350K iterations.\n6. Experiments\nWe demonstrate the quality of our learned visual rep-\nresentations with qualitative and quantitative experiments.\nQualitatively, we show the convolutional Ô¨Ålters learned in\nlayer 1 (See Figure 6). Our learned Ô¨Ålters are similar to V1\nthough not as strong. However, after Ô¨Åne-tuning on PAS-\nCAL VOC 2012, these Ô¨Ålters become quite strong. We also\nshow that the underlying representation learns a reasonable\nnearness metric by showing what the units in Pool5 layers\nrepresent (See Figure 5). Ignoring boundary effects, each\npool5 unit has a receptive Ô¨Åeld of 195 √ó 195 pixels in the\noriginal 227 √ó 227 pixel input. A central pool5 unit has a\nnearly global view, while one near the edge has a smaller,\nclipped support. Each row displays top 6 activations for a\npool5 unit. We have chosen 5 pool5 units for visualization.\nFor example, the Ô¨Årst neuron represents animal heads, sec-\nond represents potted plant, etc. This visualization indicates\nthe nearness metric learned by the network since each row\ncorresponds to similar Ô¨Åring patterns inside the CNN. Our\nunsupervised networks are available for download.\n6.1. Unsupervised CNNs without Fine-tuning\nFirst, we demonstrate that the unsupervised-CNN rep-\nresentation learned using videos (without Ô¨Åne-tuning) is\nreasonable.\nWe perform Nearest Neighbors (NN) using\nground-truth (GT) windows in VOC 2012 val set as query.\nThe retrieval-database consists of all selective search win-\ndows (more than 0.5 overlap with GT windows) in VOC\n2012 train set. See Figure 7 for qualitative results. Our\nunsupervised-CNN is far superior to a random AlexNet ar-\nchitecture and the results are quite comparable to AlexNet\ntrained on ImageNet.\nQuantitatively, we measure the retrieval rate by counting\nnumber of correct retrievals in top-K (K=20) retrievals. A\nretrieval is correct if the semantic class for retrieved patch\nand query patch are the same. Using our unsupervised-CNN\n(Pool5 features) without Ô¨Åne-tuning and cosine distance, we\nobtain 40% retrieval rate. Our performance is signiÔ¨Åcantly\nbetter as compared to 24% by ELDA [18] on HOG and\n19% by AlexNet with random parameters (our initializa-\ntion). This clearly demonstrates our unsupervised network\nlearns a good visual representation compared to a random\nparameter CNN. As a baseline, ImageNet CNN performs\n62% (but note it already learns on semantics).\nWe also evaluate our unsupervised-CNN without Ô¨Åne-\ntuning for scene classiÔ¨Åcation task on MIT Indoor 67 [36].\nWe train a linear classiÔ¨Åer using softmax loss.\nUsing\npool5 features from unsupervised-CNN without Ô¨Åne-tuning\ngives 41% classiÔ¨Åcation accuracy compared to 21% for\nGIST+SVM and 16% for random AlexNet.\nImageNet-\ntrained AlexNet has 54% accuracy. We also provide object\ndetection results without Ô¨Åne-tuning in the next section.\n6.2. Unsupervised CNNs with Fine-tuning\nNext, we evaluate our approach by transferring the fea-\nture representation learned in unsupervised manner to the\ntasks with labeled data. We focus on two challenging prob-\nlems: object detection and surface normal estimation.\n6.2.1\nObject Detection\nFor object detection, we perform our experiments on PAS-\nCAL VOC 2012 dataset [10].\nWe follow the detection\npipeline introduced in RCNN [14], which borrowed the\nCNNs pre-trained on other datasets and Ô¨Åne-tuned on it with\nthe VOC data. The Ô¨Åne-tuned CNN was then used to extract\nfeatures followed by training SVMs for each object class.\nHowever, instead of using ImageNet pre-trained network as\ninitialization in RCNN, we use our unsupervised-CNN. We\nÔ¨Åne-tune our network with the trainval set (11540 images)\nand train SVMs with them. Evaluation is performed in the\nstandard test set (10991 images).\nAt the Ô¨Åne-tuning stage, we change the output to 21\nclasses and initialize the convolutional layers with our unsu-\npervised pre-trained network. To Ô¨Åne-tune the network, we\nstart with learning rate as œµ = 0.01 and reduce the learning\nrate by a factor of 10 at every 80K iterations. The network\nis Ô¨Åne-tuned for 200K iterations. Note that for all the exper-\niments, no bounding box regression is performed.\nQuery \n(a) Random AlexNet \n(b) Imagenet AlexNet \n(c) Unsupervised AlexNet \nFigure 7. Nearest neighbors results. Given the query object from VOC 2012 val, we retrieve the NN from VOC 2012 train via calculating\nthe cosine distance on pool5 feature space. We compare the results of 3 different models: (a) AlexNet with random parameters; (b) AlexNet\ntrained with Imagenet data; (c) AlexNet trained using our unsupervised method on 8M data.\nTable 1. mean Average Precision (mAP) on VOC 2012. ‚Äúexternal‚Äù column shows the number of patches used to pre-train unsupervised-CNN.\nVOC 2012 test\nexternal aero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmAP\nscratch\n0\n66.1 58.1 32.7 23.0\n21.8\n54.5 56.4 50.8\n21.6\n42.2 31.8 49.2\n49.8\n61.6\n52.1\n25.1\n52.6\n31.3 50.0\n49.1\n44.0\nscratch (3 ensemble)\n0\n68.7 61.2 36.1 25.7\n24.3\n58.9 58.8 55.3\n24.4\n43.5 36.7 53.0\n53.8\n65.6\n54.3\n27.3\n53.5\n38.3 54.6\n51.8\n47.3\nunsup + ft\n1.5M\n68.8 62.1 34.7 25.3\n26.6\n57.7 59.6 56.3\n22.0\n42.6 33.8 52.3\n50.3\n65.6\n53.9\n25.8\n51.5\n32.3 51.7\n51.8\n46.2\nunsup + ft\n5M\n69.0 64.0 37.1 23.6\n24.6\n58.7 58.9 59.6\n22.3\n46.0 35.1 53.3\n53.7\n66.9\n54.1\n25.4\n52.9\n31.2 51.9\n51.8\n47.0\nunsup + ft\n8M\n67.6 63.4 37.3 27.6\n24.0\n58.7 59.9 59.5\n23.7\n46.3 37.6 54.8\n54.7\n66.4\n54.8\n25.8\n52.5\n31.2 52.6\n52.6\n47.5\nunsup + ft (2 ensemble)\n6.5M\n72.4 66.2 41.3 26.4\n26.8\n61.0 61.9 63.1\n25.3\n51.0 38.7 58.1\n58.3\n70.0\n56.2\n28.6\n56.1\n38.5 55.9\n54.3\n50.5\nunsup + ft (3 ensemble)\n8M\n73.4 67.3 44.1 30.4\n27.8\n63.3 62.6 64.2\n27.7\n51.1 40.6 60.8\n59.2\n71.2\n58.5\n28.2\n55.6\n39.4 58.0\n56.1\n52.0\nunsup + iterative ft\n5M\n67.7 64.0 41.3 25.3\n27.3\n58.8 60.3 60.2\n24.3\n46.7 34.4 53.6\n53.8\n68.2\n55.7\n26.4\n51.1\n34.3 53.4\n52.3\n48.0\nRCNN 70K\n72.7 62.9 49.3 31.1\n25.9\n56.2 53.0 70.0\n23.3\n49.0 38.0 69.5\n60.1\n68.2\n46.4\n17.5\n57.2\n46.2 50.8\n54.1\n50.1\nRCNN 70K (2 ensemble)\n75.3 68.3 53.1 35.2\n27.7\n59.6 54.7 73.4\n26.5\n53.0 42.2 73.1\n66.1\n71.0\n48.5\n21.7\n59.2\n50.8 55.2\n58.0\n53.6\nRCNN 70K (3 ensemble)\n74.6 68.7 54.9 35.7\n29.4\n61.0 54.4 74.0\n28.4\n53.6 43.0 74.0\n66.1\n72.8\n50.3\n20.5\n60.0\n51.2 57.9\n58.0\n54.4\nRCNN 200K (big stepsize)\n73.3 67.1 46.3 31.7\n30.6\n59.4 61.0 67.9\n27.3\n53.1 39.1 64.1\n60.5\n70.9\n57.2\n26.1\n59.0\n40.1 56.2\n54.9\n52.3\nWe compare our method with the model trained from\nscratch as well as using ImagNet pre-trained network. No-\ntice that the results for VOC 2012 reported in RCNN [14]\nare obtained by only Ô¨Åne-tuning on the train set without\nusing the val set. For fair comparison, we Ô¨Åne-tuned the\nImageNet pre-trained network with VOC 2012 trainval set.\nMoreover, as the step size of reducing learning rate in\nRCNN [14] is set to 20K and iterations for Ô¨Åne-tuning is\n70K, we also try to enlarge the step size to 50K and Ô¨Åne-\ntune the network for 200K iterations. We report the results\nfor both of these settings.\nSingle Model. We show the results in Table 1. As a\nbaseline, we train the network from scratch on VOC 2012\ndataset and obtain 44% mAP. Using our unsupervised net-\nwork pre-trained with 1.5M pair of patches and then Ô¨Åne-\ntuned on VOC 2012, we obtain mAP of 46.2% (unsup+ft,\nexternal data = 1.5M). However, using more data, 5M\nand 8M patches in pre-training and then Ô¨Åne-tune, we can\nachieve 47% and 47.5% mAP. These results indicate that\nour unsupervised network provides a signiÔ¨Åcant boost as\ncompared to the scratch network. More importantly, when\nmore unlabeled data is applied, we can get better perfor-\nmance ( 3.5% boost compared to training from scratch).\nModel Ensemble. We also try combining different mod-\nels using different sets of unlabeled data in pre-training. By\nensembling two Ô¨Åne-tuned networks which are pre-trained\nusing 1.5M and 5M patches, we obtained a boost of 3.5%\ncomparing to the single model, which is 50.5%(unsup+ft\n(2 ensemble)). Finally, we ensemble all three different net-\nworks pre-trained with different sets of data, whose size are\n1.5M, 5M and 8M respectively. We get another boost and\nreach 52% mAP (unsup+ft (3 ensemble)).\nBaselines. We compare our approach with RCNN [14]\nwhich uses ImageNet pre-trained models. Following the\nprocedure in [14], we obtain 50.1% mAP (RCNN 70K) by\nsetting the step size to 20K and Ô¨Åne-tuning for 70K itera-\ntions. To generate a model ensemble, the CNNs are Ô¨Årst\ntrained on the ImageNet dataset separately, and then they\nare Ô¨Åne-tuned with the VOC 2012 dataset. The result of\nensembling two of these networks is 53.6% mAP (RCNN\n70K (2 ensemble)). If we ensemble three networks, we get\na mAP of 54.4%. For fair of comparison, we also Ô¨Åne-\ntuned the ImageNet pre-trained model with larger step size\n(50K) and more iterations (200K). The result is 52.3% mAP\n(RCNN 200K (big stepsize)). Note that while ImageNet\nnetwork shows diminishing returns with ensembling since\nthe training data remains similar, in our case since every\nnetwork in the ensemble looks at different sets of data, we\nget huge performance boosts.\nExploring a better way to transfer learned represen-\ntation. Given our Ô¨Åne-tuned model using 5M patches in\npre-training (unsup+ft, external = 5M), we use it to re-learn\nand re-adapt to the unsupervised triplet task. After that, the\nnetwork is re-applied to Ô¨Åne-tune on VOC 2012. The Ô¨Ånal\nTable 2. Results on NYU v2 for per-pixel surface normal estimation, eval-\nuated over valid pixels.\n(Lower Better)\n(Higher Better)\nMean\nMedian 11.25‚ó¶22.5‚ó¶30‚ó¶\nscratch\n38.6\n26.5\n33.1\n46.8 52.5\nunsup + ft\n34.2\n21.9\n35.7\n50.6 57.0\nImageNet + ft\n33.3\n20.8\n36.7\n51.7 58.1\nUNFOLD [13]\n35.1\n19.2\n37.6\n53.3 58.9\nDiscr. [25]\n32.5\n22.4\n27.4\n50.2 60.2\n3DP (MW) [12]\n36.0\n20.5\n35.9\n52.0 57.8\nresult for this single model is 48% mAP (unsup + iterative\nft), which is 1% better than the initial Ô¨Åne-tuned network.\nUnsupervised network without Ô¨Åne-tuning: We also\nperform object detection without Ô¨Åne-tuning on VOC 2012.\nWe extract pool5 features using our unsupervised-CNN and\ntrain SVM on top of it. We obtain mAP of 26.1% using our\nunsupervised network (training with 8M data). The ensem-\nble of two unsupervised-network (training with 5M and 8M\ndata) gets mAP of 28.2%. As a comparison, Imagenet pre-\ntrained network without Ô¨Åne-tuning gets mAP of 40.4%.\n6.2.2\nSurface Normal Estimation\nTo illustrate that our unsupervised representation can be\ngeneralized to different tasks, we adapt the unsupervised\nCNN to the task of surface normal estimation from a RGB\nimage.\nIn this task, we want to estimate the orienta-\ntion of the pixels.\nWe perform our experiments on the\nNYUv2 dataset [40], which includes 795 images for train-\ning and 654 images for testing. Each image is has corre-\nsponding depth information which can be used to generate\ngroundtruth surface normals. For evaluation and generating\nthe groundtruth, we adopt the protocols introduced in [12]\nwhich is used by different methods [12, 25, 13] on this task.\nTo apply deep learning to this task, we followed the same\nform of outputs and loss function as the coarse network\nmentioned in [52]. SpeciÔ¨Åcally, we Ô¨Årst learn a codebook\nby performing k-means on surface normals and generate 20\ncodewords. Each codeword represents one class and thus\nwe transform the problem to 20-class classiÔ¨Åcation for each\npixel. Given a 227 √ó 227 image as input, our network gen-\nerates surface normals for the whole scene. The output of\nour network is 20 √ó 20 pixels, each of which is represented\nby a distribution over 20 codewords. Thus the dimension of\noutput is 20 √ó 20 √ó 20 = 8000.\nThe network architecture for this task is also based on\nthe AlexNet. To relieve over-Ô¨Åtting, we only stack two fully\nconnected layers with 4096 and 8000 neurons on the pool5\nlayer. During training, we initialize the network with the\nunsupervised pre-trained network (single network using 8M\nexternal data). We use the same learning rate 1.0 √ó 10‚àí6\nas [52] and Ô¨Åne-tune with 10K iterations given the small\nnumber of training data. Note that unlike [52], we do not\nutilize any data from the videos in NYU dataset for training.\nFigure 8. Surface normal estimation results on NYU dataset. For\nvisualization, we use green for horizontal surface, blue for facing\nright and red for facing left, i.e., blue ‚ÜíX; green ‚ÜíY; red ‚ÜíZ.\nFor comparison, we also trained networks from scratch\nas well as using ImageNet pre-trained. For evaluation, we\nreport mean and median error (in degrees). We also report\npercentage of pixels with less than 11.25, 22.5 and 30 de-\ngree errors. We show our qualitative results in in Figure 8.\nand quantitative results in Table 2. Our approach (unsup +\nft) is signiÔ¨Åcantly better than network trained from scratch\nand comes very close to Imagenet-pretrained CNN (‚àº1%).\n7. Discussion and Conclusion\nWe have presented an approach to train CNNs in an un-\nsupervised manner using videos. SpeciÔ¨Åcally, we track mil-\nlions of patches and learn an embedding using CNN that\nkeeps patches from same track closer in the embedding\nspace as compared to any random third patch. Our unsuper-\nvised pre-trained CNN Ô¨Åne-tuned using VOC training data\noutperforms CNN trained from scratch by 3.5%. An ensem-\nble version of our approach outperforms scratch by 4.7%\nand comes tantalizingly close to an Imagenet-pretrained\nCNN (within 2.5%). We believe this is an extremely sur-\nprising result since until recently semantic supervision was\nconsidered a strong requirement for training CNNs. We be-\nlieve our successful implementation opens up a new space\nfor designing unsupervised learning algorithms for CNN\ntraining.\nAcknowledgement: This work was partially supported by ONR MURI\nN000141010934 and NSF IIS 1320083. This material was also based on\nresearch partially sponsored by DARPA under agreement number FA8750-\n14-2-0244. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright nota-\ntion thereon. The views and conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily representing the ofÔ¨Å-\ncial policies or endorsements, either expressed or implied, of DARPA or\nthe U.S. Government. The authors would like to thank Yahoo! and Nvidia\nfor the compute cluster and GPU donations respectively.\nReferences\n[1] H. Bay, T. Tuytelaars, and L. V. Gool.\nSurf: Speeded up robust\nfeatures. In ECCV, 2006. 3\n[2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A\nreview and new perspectives. TPAMI, 35(8):1798‚Äì1828, 2013. 2\n[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-\nwise training of deep networks. In NIPS, 2007. 1, 2\n[4] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity met-\nric discriminatively, with application to face veriÔ¨Åcation. In CVPR,\n2005. 2\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human\ndetection. In CVPR, 2005. 1\n[6] C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element\ndiscovery as discriminative mode seeking. In NIPS, 2013. 2\n[7] C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory sig-\nnal: Discovering objects with predictable context. In ECCV, 2014.\n2\n[8] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual repre-\nsentation learning by context prediction. In ICCV, 2015. 2\n[9] S. M. A. Eslami, N. Heess, and J. Winn. The shape boltzmann ma-\nchine: a strong model of object shape. In CVPR, 2012. 2\n[10] M. Everingham, L. V. Gool, C. K. Williams, J. Winn, , and A. Zis-\nserman.\nThe pascal visual object classes (voc) challenge.\nIJCV,\n88(2):303‚Äì338, 2010. 6\n[11] P. Foldiak. Learning invariance from transformation sequences. Neu-\nral Computation, 3(2):194‚Äì200, 1991. 2\n[12] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3D primitives\nfor single image understanding. In ICCV, 2013. 8\n[13] D. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor origami\nworld. In ECCV, 2014. 8\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014. 5, 6, 7\n[15] Y. Gong, Y. Jia, T. K. Leung, A. Toshev, and S. Ioffe. Deep con-\nvolutional ranking for multilabel image annotation. In ICLR, 2007.\n2\n[16] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Un-\nsupervised learning of spatiotemporally coherent metrics.\nCoRR,\nabs/1412.6056, 2015. 2\n[17] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by\nlearning an invariant mapping. In CVPR, 2006. 2\n[18] B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrela-\ntion for clustering and classiÔ¨Åcation. In ECCV, 2012. 6\n[19] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-speed\ntracking with kernelized correlation Ô¨Ålters. TPAMI, 2015. 3\n[20] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal.\nThe‚Äù\nwake-sleep‚Äù algorithm for unsupervised neural networks. Science,\n268(5214):1158‚Äì1161, 1995. 2\n[21] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality\nof data with neural networks. Science, 313:504‚Äì507, 2006. 2\n[22] E. Hoffer and N. Ailon. Deep metric learning using triplet network.\nCoRR, /abs/1412.6622, 2015. 2\n[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. CoRR, /abs/1408.5093, 2014. 1\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiÔ¨Åca-\ntion with deep convolutional neural networks. In NIPS, 2012. 1,\n4\n[25] L. Ladick¬¥y, B. Zeisl, and M. Pollefeys.\nDiscriminatively trained\ndense surface normal estimation. In ECCV, 2014. 8\n[26] Q. V. Le, M. A. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Cor-\nrado, J. Dean, and A. Y. Ng. Building high-level features using large\nscale unsupervised learning. In ICML, 2012. 1, 2\n[27] Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learning hierar-\nchical invariant spatio-temporal features for action recognition with\nindependent subspace analysis. In CVPR, 2011. 2\n[28] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard, W. Hub-\nbard, and L. D. Jackel. Handwritten digit recognition with a back-\npropagation network. In NIPS, 1990. 1\n[29] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep\nbelief networks for scalable unsupervised learning of hierarchical\nrepresentations. In ICML, 2009. 2\n[30] X. Liang, S. Liu, Y. Wei, L. Liu, L. Lin, and S. Yan. Computational\nbaby learning. CoRR, abs/1411.2861, 2014. 2, 5\n[31] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, X. Cao, and\nS. Yan. Matching-cnn meets knn: Quasi-parametric human parsing.\nIn CVPR, 2015. 2\n[32] D. Lowe.\nDistinctive Image Features from Scale-Invariant Key-\npoints. IJCV, 60(2):91‚Äì110, 2004. 1\n[33] P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via deep\nlearning. In CVPR, 2012. 2\n[34] H. Mobahi, R. Collobert, and J. Weston. Deep learning from tempo-\nral coherence in video. In ICML, 2009. 1, 2\n[35] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete\nbasis set: A strategy employed by v1? Vision research, 1997. 1, 2\n[36] A. Quattoni and A.Torralba. Recognizing indoor scenes. In CVPR,\n2009. 6\n[37] M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsu-\npervised learning of invariant feature hierarchies with applications to\nobject recognition. In CVPR, 2007. 1\n[38] B. C. Russell, A. A. Efros, J. Sivic, W. T. Freeman, and A. Zisserman.\nUsing multiple segmentations to discover objects and their extent in\nimage collections. In CVPR, 2006. 2\n[39] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian\ndetection with unsupervised multi-stage feature learning. In CVPR,\n2013. 2\n[40] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmen-\ntation and support inference from RGBD images. In ECCV, 2012.\n8\n[41] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of\nmid-level discriminative patches. In ECCV, 2012. 2\n[42] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman.\nDiscovering objects and their location in images. In ICCV, 2005. 2\n[43] N. Srivastava, E. Mansimov, and R. Salakhutdinov.\nUnsuper-\nvised learning of video representations using lstms.\nCoRR,\nabs/1502.04681, 2015. 2\n[44] N. Srivastava and R. R. Salakhutdinov. Multimodal learning with\ndeep boltzmann machines. In NIPS, 2012. 1, 2\n[45] D. Stavens and S. Thrun. Unsupervised learning of invariant features\nusing video. In CVPR, 2010. 2\n[46] E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S. Willsky.\nDescribing visual scenes using transformed dirichlet processes. In\nNIPS, 2005. 2\n[47] Y. Tang, R. Salakhutdinov, and G. Hinton. Robust boltzmann ma-\nchines for recognition and denoising. In CVPR, 2012. 2\n[48] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Convolutional\nlearning of spatio-temporal features. In ECCV, 2010. 1, 2\n[49] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol.\nExtract-\ning and composing robust features with denoising autoencoders. In\nICML, 2008. 1, 2\n[50] H. Wang and C. Schmid. Action recognition with improved trajecto-\nries. In ICCV, 2013. 3\n[51] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\nB. Chen, and Y. Wu. Learning Ô¨Åne-grained image similarity with\ndeep ranking. In CVPR, 2014. 2, 3\n[52] X. Wang, D. F. Fouhey, and A. Gupta. Designing deep networks for\nsurface normal estimation. In CVPR, 2015. 8\n[53] L. Wiskott and T. J. Sejnowski. Slow feature analysis:unsupervised\nlearning of invariances. Neural Computation, 14:715‚Äì770, 2002. 2\n[54] P. Wohlhart and V. Lepetit. Learning descriptors for object recogni-\ntion and 3d pose estimation. In CVPR, 2015. 2\n[55] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang. Bit-scalable deep\nhashing with regularized similarity learning for image retrieval and\nperson re-identiÔ¨Åcation. TIP, 24(12):4766‚Äì4779, 2015. 2\n[56] W. Y. Zou, S. Zhu, A. Y. Ng, and K. Yu. Deep learning of invariant\nfeatures via simulated Ô¨Åxations in video. In NIPS, 2012. 1, 2\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2015-05-04",
  "updated": "2015-10-06"
}