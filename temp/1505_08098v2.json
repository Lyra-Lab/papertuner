{
  "id": "http://arxiv.org/abs/1505.08098v2",
  "title": "CURL: Co-trained Unsupervised Representation Learning for Image Classification",
  "authors": [
    "Simone Bianco",
    "Gianluigi Ciocca",
    "Claudio Cusano"
  ],
  "abstract": "In this paper we propose a strategy for semi-supervised image classification\nthat leverages unsupervised representation learning and co-training. The\nstrategy, that is called CURL from Co-trained Unsupervised Representation\nLearning, iteratively builds two classifiers on two different views of the\ndata. The two views correspond to different representations learned from both\nlabeled and unlabeled data and differ in the fusion scheme used to combine the\nimage features. To assess the performance of our proposal, we conducted several\nexperiments on widely used data sets for scene and object recognition. We\nconsidered three scenarios (inductive, transductive and self-taught learning)\nthat differ in the strategy followed to exploit the unlabeled data. As image\nfeatures we considered a combination of GIST, PHOG, and LBP as well as features\nextracted from a Convolutional Neural Network. Moreover, two embodiments of\nCURL are investigated: one using Ensemble Projection as unsupervised\nrepresentation learning coupled with Logistic Regression, and one based on\nLapSVM. The results show that CURL clearly outperforms other supervised and\nsemi-supervised learning methods in the state of the art.",
  "text": "1\nCURL: Co-trained Unsupervised Representation\nLearning for Image Classiﬁcation\nSimone Bianco, Gianluigi Ciocca, and Claudio Cusano\nAbstract—In this paper we propose a strategy for semi-\nsupervised image classiﬁcation that leverages unsupervised\nrepresentation learning and co-training. The strategy, that\nis called CURL from Co-trained Unsupervised Represen-\ntation Learning, iteratively builds two classiﬁers on two\ndifferent views of the data. The two views correspond to\ndifferent representations learned from both labeled and\nunlabeled data and differ in the fusion scheme used to\ncombine the image features.\nTo assess the performance of our proposal, we conducted\nseveral experiments on widely used data sets for scene\nand object recognition. We considered three scenarios (in-\nductive, transductive and self-taught learning) that differ\nin the strategy followed to exploit the unlabeled data.\nAs image features we considered a combination of GIST,\nPHOG, and LBP as well as features extracted from a Con-\nvolutional Neural Network. Moreover, two embodiments of\nCURL are investigated: one using Ensemble Projection as\nunsupervised representation learning coupled with Logistic\nRegression, and one based on LapSVM. The results show\nthat CURL clearly outperforms other supervised and semi-\nsupervised learning methods in the state of the art.\nIndex Terms—Image classiﬁcation, machine learning\nalgorithms, pattern analysis, semi-supervised learning.\nI. INTRODUCTION\nSemi-supervised learning [1] consists in taking into\naccount both labeled and unlabeled data when training\nmachine learning models. It is particularly effective when\nthere is plenty of training data, but only a few instances\nare labeled. In the last years, many semi-supervised\nlearning approaches have been proposed including gen-\nerative methods [2], [3], graph-based methods [4], [5],\nand methods based on Support Vector Machines [6],\n[7]. Co-training is another example of semi-supervised\ntechnique [8]. It consists in training two classiﬁers\nindependently which, on the basis of their level of\nconﬁdence on unlabeled data, co-train each other trough\nthe identiﬁcation of good additional training examples.\nS. Bianco, and G. Ciocca are with the Department of Informatics\nSystems and Communications, University of Milano-Bicocca, Mi-\nlano, 20126 Italy, e-mail: {bianco, ciocca}@disco.unimib.it.\nC. Cusano is with the Dipartimento di Ingegneria Industriale e\ndell’Informazione, University of Pavia, Pavia, 27100 Italy. e-mail:\nclaudio.cusano@unipv.it\nThe difference between the two classiﬁers is that they\nwork on different views of the training data, often\ncorresponding to two feature vectors. Pioneering works\non co-training identiﬁed the conditional independence\nbetween the views as the main reason of its success.\nMore recently, it has been observed that conditional\nindependence is a sufﬁcient, but not necessary condition,\nand that even a single view can be considered, provided\nthat different classiﬁcation techniques are used [9].\nIn this work we propose a semi-supervised image\nclassiﬁcation strategy which exploits unlabeled data in\ntwo different ways: ﬁrst two image representations are\nobtained by unsupervised representation learning (URL)\non a set of image features computed on all the avail-\nable training data; then co-training is used to enlarge\nthe labeled training set of the corresponding co-trained\nclassiﬁers (C). The difference between the two image\nrepresentations is that one is built on the combination of\nall the image features (early fusion), while the other is\nthe combination of sub-representations separately built\non each feature (late fusion). We call the proposed\nstrategy CURL: Co-trained Unsupervised Representation\nLearning (from the combination of C and URL compo-\nnents). The schema of CURL is illustrated in Fig. 1.\nIn standard co-training each classiﬁer is built on a\nsingle view, often corresponding to a single feature.\nHowever, the combination of multiple features is often\nrequired to recognize complex visual concepts [10]–\n[12]. Both the classiﬁers built by CURL exploit all\nthe available image features in such a way that these\nconcepts can be accurately recognized. We argue that\nthe use of two different fusion schemes together with the\nnon-linear transformation produced by the unsupervised\nlearning procedure, makes the two image representations\nuncorrelated enough to allow an effective co-training of\nthe classiﬁers.\nThe proposed strategy is built on two base compo-\nnents: URL (the unsupervised representation learning)\nand C (the classiﬁer used in co-training). By changing\nthese two components we can have different embodi-\nments of CURL that can be experimented and evaluated.\nTo assess the merits of our proposal we conducted\nseveral experiments on widely used data sets: the 15-\narXiv:1505.08098v2  [cs.LG]  11 Sep 2015\n2\nFig. 1. Schema of the proposed strategy.\nscene data set, the Caltech-101 object classiﬁcation data\nset, and the ILSVCR 2012 data set which contains 1000\ndifferent classes. We considered a variety of scenarios\nincluding transductive learning (i.e. unlabeled test data\navailable during training), inductive learning (i.e. test\ndata not available during training), and self-taught learn-\ning (i.e. test and training data coming from two different\ndata sets). In order to verify the efﬁcacy of the CURL\nclassiﬁcation strategy, we also tested two embodiments:\none that uses Ensemble Projection unsupervised repre-\nsentation coupled with Logistic Regression classiﬁcation,\nand one based on LapSVM semi-supervised classiﬁca-\ntion. Moreover different variants of the embodiments are\nevaluated as well. The results show that CURL clearly\noutperforms other semi-supervised learning methods in\nthe state of the art.\nII. RELATED WORK\nThere is a large literature on semi-supervised learning.\nFor the sake of brevity, we discuss only the paradigms\ninvolved in the proposed strategy. More information\nabout these and other approaches to semi-supervised\nlearning can be found in the book by Chapelle et al. [1].\nA. Co-training\nBlum and Mitchell proposed co-training in 1998 [8]\nand veriﬁed its effectiveness for the classiﬁcation of web\npages. The basic idea is that two classiﬁers are trained\non separate views (features) and then used to train each\nother. More precisely, when one of the classiﬁers is very\nconﬁdent in making a prediction for unlabeled data, the\npredicted labels are used to augment the training set of\nthe other classiﬁer. The concept has been generalized\nto three [13] or more views [14], [15]. Co-training\nhas been used in several computer vision applications\nincluding video annotation [16], action recognition [17],\ntrafﬁc analysis [18], speech and gesture recognition [19],\nimage annotation [20], biometric recognition [21], image\nretrieval [22], image classiﬁcation [23], object detec-\ntion [18], [24], and object tracking [25].\nAccording to Blum and Mitchell, a sufﬁcient condition\nfor the effectiveness of co-training is that, beside being\nindividually accurate, the two classiﬁers are condition-\nally independent given the class label. However, condi-\ntional independence is not a necessary condition. In fact,\nWhang and Zhou [26] showed that co-training can be\neffective when the diversity between the two classiﬁers\nis larger than their errors; their results provided a theo-\nretical support to the success of single-view co-training\nvariants [27]–[29] (the reader may refer to an updated\nstudy from the same authors [30] for more details about\nnecessary and sufﬁcient conditions for co-training).\nB. Unsupervised representation learning\nIn the last years, as a consequence of the success\nof deep learning frameworks we observed an increased\ninterest in methods that make use of unlabeled data to\nautomatically learn new representations. In fact, these\nhave been demonstrated to be very effective for the\npre-training of large neural networks [31], [32]. Re-\nstricted Boltzmann Machines [33] and auto-encoder net-\nworks [34] are notable examples of this kind of methods.\nThe tutorial by Bengio covers in detail this family of\napproaches [35].\nA conceptually simpler approach consists in using\nclustering algorithms to identify frequently occurring\npatterns in unlabeled data that can be used to deﬁne ef-\nfective representations. The K-means algorithm has been\nwidely used for this purpose [36]. In computer vision this\napproach is very popular and lead to the many variants of\nbag-of-visual-words representations [37], [38]. Brieﬂy,\nclustering on unlabeled data is used to build a vocabulary\nof visual words. Given an image, multiple local features\nare extracted and for each of them the most similar visual\nword is searched. The ﬁnal representation is a histogram\ncounting the occurrences of the visual words. Sparse cod-\ning can be seen as an extension of this approach, where\neach local feature is described as a sparse combination\nof multiple words of the vocabulary [39]–[41].\nAnother strategy for unsupervised feature learning is\nrepresented by Ensemble Projection (EP) [42]. From\n3\nall the available data (labeled and unlabeled) Ensemble\nProjection samples a set of prototypes. Discriminative\nlearning is then used to learn projection functions tuned\nto the prototypes. Since a single set of projections could\nbe too noisy, multiple sets of prototypes are sampled to\nbuild an ensemble of projection functions. The values\ncomputed according to these functions represent the\ncomponents of the learned representations.\nLapSVM [7] can be seen as an unsupervised rep-\nresentation learning method as well. In this case the\nlearned representation is not explicit but it is implicitly\nembedded in a kernel learned from unlabeled data.\nC. Fusion schemes\nCombining multimodal information is an important\nissue in pattern recognition. The fusion of multimodal\ninputs can bring complementary information from var-\nious sources, useful for improving the quality of the\nimage retrieval and classiﬁcation performance [43]. The\nproblem arises in deﬁning how these modalities are to\nbe combined or fused. In general, the existing fusion\napproaches can be categorized as early and late fusion\napproaches, which refers to their relative position from\nthe feature comparison or learning step in the whole\nprocessing chain. Early fusion usually refers to the\ncombination of the features into a single representation\nbefore comparison/learning. Late fusion refers to the\ncombination, at the last stage, of the responses obtained\nafter individual features comparison or learning [44],\n[45]. There is no universal conclusion as to which\nstrategy is the preferred method for for a given task.\nFor example, Snoek et al. [44] found that late fusion is\nbetter than early fusion in the TRECVID 2004 seman-\ntic indexing task, while Ayache et al. [46] stated that\nearly fusion gets better results than late fusion on the\nTRECVID 2006 semantic indexing task. A combination\nof these approaches can also be exploited as hybrid\nfusion approach [47].\nAnother form of data fusion is Multiple Kernel Learn-\ning (MKL). MKL has been introduced by Lanckriet et\nal. [48] as extension of the support vector machines\n(SVMs). Instead of using a single kernel computed on\nthe image representation as in standard SVMs, MKL\nlearns distinct kernels. The kernels are combined with a\nlinear or non linear function and the function’s parame-\nters can be determined during the learning process. MKL\ncan be used to learn different kernels on the same image\nrepresentation or by learning different kernels each one\non a different image representation [49]. The former\ncorresponds to have different notion of similarity, and\nto choose the most suitable one for the problem and\nrepresentation at hand. The latter corresponds to have\nmultiple representations each with a, possibly, different\ndeﬁnition of similarity that must be combined together.\nThis kind of data fusion, in [45], is termed intermediate\nfusion.\nIII. THE PROPOSED STRATEGY: CURL\nIn the semi-supervised image classiﬁcation setup\nthe training data consists of both labeled examples\n{Xl, Y} = {(xi, yi)}L\ni=1 and unlabeled ones Xu\n=\n{xi}L+U\ni=L+1, where xi denotes the feature vector of image\ni, yi ∈{1, . . . , K} is its label, and K is the number of\nclasses.\nIn this work, for each image i a set of S different\nimage features x(s)\ni , s = 1, . . . , S is considered. Two\nviews are then generated by using two different fusion\nstrategies: early and late fusion. In case of Early Fusion\n(EF), the image features are concatenated and then used\nto learn a new representation xEF\ni\n= ϕ({[x(1)\ni , . . . , x(S)\ni\n]}\nin an unsupervised way, where ϕ(·) is a projection\nfunction. In case of Late Fusion (LF), an unsupervised\nrepresentation ϕs(x(s)\ni ) is independently learned for each\nimage feature and then the representations are concate-\nnated to obtain xLF\ni\n= [ϕ1(x(1)\ni ), . . . , ϕS(x(S)\ni\n)].\nUsing the learned EF and LF unsupervised representa-\ntions, the two views are built: X EF\nl\n= {xEF\ni }L\ni=1, X EF\nu\n=\n{xEF\ni }L+U\ni=L+1 and X LF\nl\n= {xLF\ni }L\ni=1, X LF\nu\n= {xLF\ni }L+U\ni=L+1.\nFurthermore, two label sets Y EF and Y LF are initialized\nequal to Y.\nOnce the two views are generated, our method iter-\natively co-trains two classiﬁers φEF and φLF on them\n[8]. SVMs, logistic regressions, or any other similar\ntechnique can be used to obtain them. The idea of\niterative co-training is that one can use a small labeled\nsample to train the initial classiﬁers over the respective\nviews (i.e. φEF : X EF\nl\n7→Y EF and φLF : X LF\nl\n7→\nY LF), and then iteratively bootstrap by taking unlabeled\nexamples for which one of the classiﬁers is conﬁdent\nbut the other is not. The conﬁdent classiﬁer determines\npseudo-labels [50] that are then used as if they were\ntrue labels to improve the other classiﬁer [51]. Given\nthe classiﬁer conﬁdence scores wEF\ni\n= φEF(xEF\ni ) and\nwLF\ni\n= φLF(xLF\ni ), the pseudo-labels ˆyEF\ni\nand ˆyLF\ni\nare\nrespectively obtained as:\nˆy\nEF\ni\n= arg max\nj=1,...,K w\nEF\ni [j]\n(1)\nˆy\nLF\ni\n= arg max\nj=1,...,K w\nLF\ni [j]\n(2)\nIn each round of co-training, the classiﬁer φLF chooses\nsome examples in X EF\nu\nto pseudo-label for φEF, and vice\nversa. For each class k, let us call X⋆the set of candidate\n4\nunlabeled examples to be pseudo-labeled for φEF. Each\nx⋆∈X⋆must belong to the unlabeled set, i.e. x⋆∈X EF\nu ,\nhas not to be already used for training, i.e. x⋆/∈X EF\nl\n,\nand its pseudo-label has to be ˆyLF\n⋆\n= k. Furthermore,\nφLF should be more conﬁdent on the classiﬁcation of\nx⋆than φEF, and its conﬁdence should be higher than a\nﬁxed threshold t1:\n∀x⋆∈X⋆: w\nEF\n⋆[k] < w\nLF\n⋆[k], w\nLF\n⋆[k] >t1\n(3)\nIf no x⋆satisfying Eq. 3 are found, then the constraints\nare relaxed:\n∀x⋆∈X⋆: w\nLF\n⋆[k] >t2, with t2 < t1\n(4)\nNon-maximum suppression is applied to add one sin-\ngle pseudo-labeled example for each class by extracting\nthe most conﬁdent x⋆∈X⋆:\nﬁnd x⋆∈X⋆: w\nLF\n⋆[k] = argmax\nj\nw\nLF\nj [k]\n(5)\nThe selected x⋆and its corresponding pseudo-label ˆy⋆\nare added to X LF\nl\nand Y LF respectively. If no x⋆satis-\nfying Eq. 4 are found, then nothing is added to X LF\nl\nand\nY LF.\nSimilarly, the classiﬁer φEF chooses some examples\nin X LF\nu\nto pseudo-label for φLF. At the next co-training\nround, two new classiﬁers φEF and φLF are trained on\nthe respective views, that now contain both labeled and\npseudo-labeled examples. The complete procedure of the\nCURL method is outlined in Algorithms 1-3.\nIV. EXPERIMENTS\nCURL is parametric with respect to the projection\nfunction ϕ used in the unsupervised representation learn-\ning URL, and the supervised classiﬁcation technique C\nused during to co-train φEF and φLF. As ﬁrst embodiment\nof CURL, we used Ensemble Projection [42] for the\nformer and logistic regression for the latter. Another\nembodiment, based on LapSVM [7] is presented in\nSection V-C.\nA. Data sets\nWe evaluated our method on two data sets: Scene-\n15 (S-15) [38], and Caltech-101 (C-101) [52]. Scene-\n15 data set contains 4485 images divided into 15 scene\ncategories with both indoor and outdoor environments.\nEach category has 200 to 400 images. Caltech-101\ncontains 8677 images divided into 101 object categories,\neach having 31 to 800 images. Furthermore, we collected\na set of random images by sampling 20,000 images from\nthe ImageNet data set [53] to evaluate our method on the\nAlgorithm 1: CURL\nData: Labeled data {Xl, Y}, unlabeled data Xu\nResult: Classiﬁers φEF(·) and φLF(·)\nbegin\n[X EF\nl ,X EF\nu ,X LF\nl ,X LF\nu ] = computeURL(Xl, Xu)\nY EF = Y LF = Y\ntrain classiﬁer φEF : X EF\nl\n7→Y EF\ntrain classiﬁer φLF : X LF\nl\n7→Y LF\nfor co-training round c = 1 : C do\ninitialize W EF = W LF = ˆY EF = ˆY LF = ∅\nforeach xEF\ni\n∈X EF\nu\ndo\nadd wEF\ni\n= φEF(xEF\ni ) to W EF\nadd ˆy\nEF\ni\n= arg max\nj=1,...,K w\nEF\ni [j] to ˆY EF\nforeach xLF\ni\n∈X LF\nu\ndo\nadd wLF\ni\n= φLF(xLF\ni ) to W LF\nadd ˆy\nLF\ni\n= arg max\nj=1,...,K w\nLF\ni [j] to ˆY LF\nfor class number k = 1 : K do\nfor (v1, v2) ∈{(EF,LF), (LF,EF)} do\nﬁnd {X⋆, ˆYv2\n⋆} with\nX⋆⊂X v1\nu , X⋆∩X v1\nl\n=∅, ˆYv2\n⋆⊂ˆYv2 s.t.:\n∀x⋆∈X⋆and ∀ˆyv2\n⋆∈ˆYv2\n⋆, hold:\nˆyv2\n⋆= k, wv1\n⋆[k] < wv2\n⋆[k], wv2\n⋆[k] >t1\nif X⋆= ∅then\nﬁnd {X⋆, ˆYv2\n⋆} with\nX⋆⊂X v1\nu , X⋆∩X v1\nl\n=∅, ˆYv2\n⋆⊂ˆYv2\ns.t.:\n∀x⋆∈X⋆and ∀ˆyv2\n⋆∈ˆYv2\n⋆, hold:\nˆyv2\n⋆= k, wv2\n⋆[k] > t2\n[X⋆, ˆYv2\n⋆]=nonMaxSuppr(X⋆, ˆYv2\n⋆,Wv2)\nX v1\nl\n= X v1\nl\n∪X⋆\nYv1 = Yv1 ∪ˆYv2\n⋆\ntrain classiﬁer φEF : X EF\nl\n7→Y EF\ntrain classiﬁer φLF : X LF\nl\n7→Y LF\nAlgorithm 2: compute URL\nData: Labeled data Xl and unlabeled data Xu\nResult: Unsup. representations X EF\nl\n, X EF\nu\n, X LF\nl\n, X LF\nu\nbegin\nLearn EF representation ϕ on {[x(1)\ni , . . . , x(S)\ni\n]}L+U\ni=1\nX EF\nl\n= ϕ({[x(1)\ni , . . . , x(S)\ni\n]}L\ni=1)\nX EF\nu\n= ϕ({[x(1)\ni , . . . , x(S)\ni\n]}L+U\ni=L+1)\nLearn LF representations ϕs on {x(s)\ni }L+U\ni=1\nX LF\nl\n= {[ϕ1(x(1)\ni ), . . . , ϕS(x(S)\ni\n)]}L\ni=1\nX LF\nu\n= {[ϕ1(x(1)\ni ), . . . , ϕS(x(S)\ni\n)]}L+U\ni=L+1\n5\nAlgorithm 3: non-maximum suppression\nData: X, ˆY, W, k\nResult: X⋆, ˆY⋆\nbegin\nﬁnd {X⋆, ˆY⋆} with X⋆∈X, ˆY⋆∈ˆY s.t.:\nw⋆[k] = argmax\nj\nwj[k], with wj ∈W\ntask of self-taught image classiﬁcation. Since the current\nversion of ImageNet has 21841 synsets (i.e. categories)\nand a total of more than 14 millions images, there is a\nsmall probability that the random images and images\nin the two considered data sets come from the same\ndistribution.\nB. Image features\nIn our experiments we used the following three fea-\ntures: GIST [54], Pyramid of Histogram of Oriented Gra-\ndients (PHOG) [55], and Local Binary Patterns (LBP)\n[56]. GIST was computed on the rescaled images of\n256×256 pixels, at 3 scales with 4, 8 and 8 orienta-\ntions respectively. PHOG was computed with a 2-layer\npyramid and in 8 directions. Uniform LBP with radius\nequal to 1, and 8 neighbors was used.\nIn Section V-B we also investigate the use of features\nextracted from a CNN [57] in combination with the\nprevious ones.\nC. Ensemble projection\nDifferently from others semi-supervised methods that\ntrain a classiﬁer from labeled data with a regularization\nterm learned from unlabeled data, Ensemble Projection\n[42] learns a new image representation from all known\ndata (i.e. labeled and unlabeled data), and then trains a\nplain classiﬁer on it.\nEnsemble Projection learns knowledge from T dif-\nferent prototype sets Pt = {(st\ni, ct\ni)}rn\ni=1, with t ∈\n{1, . . . , T} where st\ni ∈{1, . . . , L + U} is the index of\nthe i−th chosen image, ct\ni ∈{1, . . . , r} is the pseudo-\nlabel indicating to which prototype st\ni belong to. r is\nthe number of prototypes in Pt, and n is the number of\nimages sampled for each prototype. For each prototype\nset, m hypotheses are randomly sampled, and the one\ncontaining images having the largest mutual distance is\nkept.\nA set of discriminative classiﬁers φt(·) is learned on\nPt, one for each prototype set, and the projected vectors\nφt(xi) are obtained. The ﬁnal feature vector is obtained\nby concatenating these projected vectors.\nFollowing [42] we set T = 300, r = 30, n = 6,\nm = 50, using Logistic Regression (LR) as discrimina-\ntive classiﬁer φt(·) with C = 15.\nWithin CURL, Ensemble Projection is used to learn\nboth Early Fusion and Late Fusion unsupervised repre-\nsentations. In the case of Early Fusion (EF), the feature\nvector xi is obtained concatenating the S different fea-\ntures available xi = [x(1)\ni , . . . , x(S)\ni\n], s = 1, . . . , S. In the\ncase of Late Fusion (LF), the feature vector xi is made by\nconsidering just one single feature at time xi = x(s)\ni . For\nboth EF and LF, the same number T of prototypes is used\nin order to assure that the unsupervised representations\nhave the same size.\nD. Experimental settings\nWe conducted two kinds of experiments: (1) com-\nparison of our strategy\nwith competing methods for\nsemi-supervised image classiﬁcation; (2) evaluation of\nour method at different number of co-training rounds.\nWe considered three scenarios corresponding to three\ndifferent ways of using unlabeled data. In the inductive\nlearning scenario 25% of the unlabeled data is used\ntogether with the labeled data for the semi-supervised\ntraining of the classiﬁer; the remaining 75% is used\nas an independent test set. In the transductive learning\nscenario all the unlabeled data is used during both\ntraining and test. In the self-taught learning scenario the\nset of unlabeled data is taken from an additional data set\nfeaturing a different distribution of image content (i.e. the\n20,000 images from ImageNet); all the unlabeled data\nfrom the original data set is used as an independent test\nset.\nAs evaluation measure we followed [42] and used\nthe multi-class average precision (MAP), computed as\nthe average precision over all recall values and over all\nclasses. Different numbers of training images per class\nwere tested for both Scene-15 and Caltech-101 (i.e. 1, 2,\n3, 5, 10, and 20). All the reported results represent the\naverage performance over ten runs with random labeled-\nunlabeled splits.\nThe performance of the proposed strategy\nare\ncompared with those of other supervised and semi-\nsupervised baseline methods. As supervised classi-\nﬁers we considered Support Vector Machines (SVM).\nAs semi-supervised classiﬁers, we used LapSVM [7],\n[58]. LapSVM extend the SVM framework including\na smoothness penalty term deﬁned on the Laplacian\nadjacency graph built from both labeled and unlabeled\ndata. For both SVM and LapSVM we experimented\nwith the linear, RBF and χ2 kernels computed on the\nconcatenation of the three available image features as\n6\nin [42]. The parameters of SVM and LapSVM have\nbeen determined by a greedy search with a three-fold\ncross validation on the training set. We also compared\nthe present embodiment\nof CURL against Ensemble\nProjection coupled with a logistic regression classiﬁer\n(EP+LR) as in [42].\nV. EXPERIMENTAL RESULTS\nAs a ﬁrst experiment we compared CURL against\nEP+LR, and against SVMs and LapSVMs with different\nkernels. Speciﬁcally, we tested the two co-trained classi-\nﬁers operating on early-fused and late-fused representa-\ntions, both employing EP for URL and LR as classiﬁer C,\nthat we call CURL-EF(EP+LR) and CURL-LF(EP+LR)\nrespectively. We also included a variant of the proposed\nmethod. It differs in the number of pseudo-labeled ex-\namples that are added at each co-training round. The\nvariant skips the non-maximum suppression step, and at\neach round, adds all the examples satisfying Eq. 3. We\ndenote the two co-trained classiﬁers of the variant as\nCURL-EFn(EP+LR) and CURL-LFn(EP+LR).\nFig. 2 shows the classiﬁcation performance with dif-\nferent numbers of labeled training images per class, in\nthe three learning scenarios for both the Scene-15 and\nCaltech-101 data sets. For the CURL-based methods\nwe considered ﬁve co-training rounds, and the reported\nperformance correspond to the last round. For SVM and\nLapSVM only the results using χ2 kernel are reported,\nsince they consistently showed the best performance\nacross all the experiments. Detailed results for all the\ntested baseline methods, and for the CURL variants\nacross the co-training rounds are available in Tables I, II\nand III.\nThe behavior of the methods is quite stable with\nrespect to the three learning scenarios, with slightly\nlower MAP obtained in the case of self-taught learning.\nIt is evident that our strategy outperformed the other\nmethods in the state of the art included in the comparison\nacross all the data sets and all the scenarios consid-\nered. Among the variants considered, CURL-LF(EP+LR)\ndemonstrated to be the best in the case of a small number\nof labeled images, while CURL-LFn(EP+LR) obtained\nthe best results when more labeled data is available. Clas-\nsiﬁers obtained on early-fused representations performed\ngenerally worse than the corresponding ones obtained on\nlate-fused representations, but they are still uniformly\nbetter than the original EP+LR Ensemble Projection\nwhich can be considered as their non-cotrained version.\nSVMs and LapSVMs performed poorly on the Scene-15\ndata set, but they outperformed EP+LR and some of the\nCURL variants on the Caltech-101 data set.\nTABLE I\nMEAN AVERAGE PRECISION (MAP) OF THE BASELINE\nALGORITHMS, VARYING THE NUMBER OF LABELED IMAGES PER\nCLASS IN THE THREE LEARNING SCENARIOS CONSIDERED:\nINDUCTIVE (IND), TRANSDUCTIVE (TRD), AND SELF-TAUGHT\n(ST).\nScene-15\n# img\nmethod\nIND\nTRD\nST\n1\nSVMlin\n22.7\n22.3\n22.3\nSVMrbf\n26.5\n25.8\n25.8\nSVMχ2\n29.5\n28.7\n28.7\nLapSVMlin\n28.1\n29.1\n26.4\nLapSVMrbf\n28.8\n29.8\n26.7\nLapSVMχ2\n32.3\n33.7\n30.2\nEP+LR\n38.3\n39.3\n32.4\n2\nSVMlin\n27.4\n26.9\n26.9\nSVMrbf\n32.9\n31.3\n31.3\nSVMχ2\n35.4\n34.9\n34.9\nLapSVMlin\n33.7\n34.9\n31.2\nLapSVMrbf\n34.6\n35.7\n32.5\nLapSVMχ2\n38.1\n39.6\n35.7\nEP+LR\n44.6\n47.3\n41.0\n3\nSVMlin\n30.0\n30.2\n30.2\nSVMrbf\n36.5\n36.7\n36.7\nSVMχ2\n39.9\n39.3\n39.3\nLapSVMlin\n37.6\n38.6\n36.4\nLapSVMrbf\n37.7\n38.9\n37.2\nLapSVMχ2\n42.8\n43.9\n41.6\nEP+LR\n50.8\n53.2\n48.5\n5\nSVMlin\n35.5\n35.2\n35.2\nSVMrbf\n43.5\n42.8\n42.8\nSVMχ2\n46.5\n46.1\n46.1\nLapSVMlin\n43.4\n44.5\n43.5\nLapSVMrbf\n43.8\n44.7\n44.0\nLapSVMχ2\n49.2\n50.5\n49.3\nEP+LR\n55.6\n58.6\n55.2\n10\nSVMlin\n43.8\n43.7\n43.7\nSVMrbf\n51.9\n51.3\n51.3\nSVMχ2\n55.5\n55.2\n55.2\nLapSVMlin\n51.4\n52.5\n52.3\nLapSVMrbf\n52.4\n53.1\n53.0\nLapSVMχ2\n56.6\n57.4\n57.2\nEP+LR\n62.8\n64.4\n62.9\n15\nSVMlin\n50.1\n50.3\n50.3\nSVMrbf\n57.4\n57.1\n57.1\nSVMχ2\n60.7\n60.6\n60.6\nLapSVMlin\n55.1\n56.0\n55.5\nLapSVMrbf\n57.8\n58.7\n58.3\nLapSVMχ2\n60.9\n61.6\n61.2\nEP+LR\n66.0\n67.9\n67.3\n20\nSVMlin\n54.0\n53.5\n53.5\nSVMrbf\n60.4\n60.3\n60.3\nSVMχ2\n64.2\n64.1\n64.1\nLapSVMlin\n58.3\n58.7\n58.6\nLapSVMrbf\n60.8\n61.4\n61.2\nLapSVMχ2\n64.5\n65.4\n65.1\nEP+LR\n67.8\n69.7\n69.1\nCaltech-101\n# img\nmethod\nIND\nTRD\nST\n1\nSVMlin\n6.0\n6.3\n6.3\nSVMrbf\n7.1\n7.3\n7.3\nSVMχ2\n9.5\n9.6\n9.6\nLapSVMlin\n9.2\n9.6\n9.1\nLapSVMrbf\n9.8\n10.2\n9.6\nLapSVMχ2\n10.2\n10.7\n10.0\nEP+LR\n8.3\n8.7\n8.4\n2\nSVMlin\n9.1\n9.2\n9.2\nSVMrbf\n10.1\n9.8\n9.8\nSVMχ2\n14.1\n13.7\n13.7\nLapSVMlin\n12.3\n12.8\n12.1\nLapSVMrbf\n12.7\n13.3\n12.4\nLapSVMχ2\n14.6\n14.9\n14.5\nEP+LR\n12.6\n13.1\n12.5\n3\nSVMlin\n10.7\n10.8\n10.8\nSVMrbf\n11.7\n11.6\n11.6\nSVMχ2\n16.7\n16.3\n16.3\nLapSVMlin\n13.8\n14.3\n13.5\nLapSVMrbf\n14.0\n14.6\n13.9\nLapSVMχ2\n17.9\n18.3\n17.6\nEP+LR\n15.5\n15.7\n15.3\n5\nSVMlin\n13.4\n13.3\n13.3\nSVMrbf\n14.9\n14.6\n14.6\nSVMχ2\n20.7\n20.5\n20.5\nLapSVMlin\n16.3\n16.6\n16.0\nLapSVMrbf\n16.8\n17.1\n16.6\nLapSVMχ2\n21.7\n22.1\n21.4\nEP+LR\n19.4\n20.0\n19.5\n10\nSVMlin\n-\n17.3\n17.3\nSVMrbf\n-\n19.2\n19.2\nSVMχ2\n-\n26.0\n26.0\nLapSVMlin\n-\n20.5\n20.1\nLapSVMrbf\n-\n21.6\n21.2\nLapSVMχ2\n-\n28.1\n27.5\nEP+LR\n-\n26.0\n25.2\n15\nSVMlin\n-\n19.7\n19.7\nSVMrbf\n-\n22.1\n22.1\nSVMχ2\n-\n29.1\n29.1\nLapSVMlin\n-\n22.9\n22.5\nLapSVMrbf\n-\n24.0\n23.5\nLapSVMχ2\n-\n31.3\n30.9\nEP+LR\n-\n29.5\n29.0\n20\nSVMlin\n-\n21.5\n21.5\nSVMrbf\n-\n24.2\n24.2\nSVMχ2\n-\n31.5\n31.5\nLapSVMlin\n-\n24.6\n24.0\nLapSVMrbf\n-\n26.8\n26.1\nLapSVMχ2\n-\n33.4\n32.7\nEP+LR\n-\n31.9\n31.0\nCo-training allows to make good use of the early\nfusion representations that otherwise lead to worse re-\nsults than late fusion representations. In our opinion\nthis happens because the two views capture different\nrelationships among data. This fact is visible in Fig. 3,\nwhich shows 2D projections obtained by applying the\nt-SNE [59] method to GIST, PHOG, LBP features,\ntheir concatenation, and their learnt early- and late-fused\nrepresentations. Unsupervised representation learning al-\nlows t-SNE to identify groups of images of the same\nclass. Moreover, representations based on early and late\nfusion induce different relationships among the classes.\nFor instance, in the second row of Fig. 3f the blue\nand the light green classes have been placed close to\neach other on the bottom right; in Fig. 3e, instead,\nthe two classes are well separated. The difference in\n7\nScene-15 inductive\nScene-15 transductive\nScene-15 self-taught\nCaltech-101 inductive\nCaltech-101 transductive\nCaltech-101 self-taught\nFig. 2. Mean Average Precision (MAP) varying the number of labeled images per class, obtained on the Scene-15 data set (ﬁrst row), and\non the Caltech-101 data set (second row). Three scenarios are considered: inductive learning (left column), transductive learning (middle\ncolumn) and self-taught learning (third column). Note that inductive learning on the Caltech-101 data set is limited to 5 labeled images per\nclass because otherwise for some classes there wouldn’t be enough unlabeled data left for both training and evaluation.\n(a) GIST\n(b) PHOG\n(c) LBP\n(d) concatenation\n(e) early fusion\n(f) late fusion\nFig. 3. t-SNE 2D projections for the different features used. They are relative to the Scene-15 (top row) and Caltech-101 (bottom row) data\nsets. Different classes are represented in different colors, and the same class with the same color across the row.\nthe two representations explains the effectiveness of\nco-training and justiﬁes the difference in performance\nbetween CURL-EF(EP+LR) and CURL-LF(EP+LR).\nAs further investigation, we also combined the two\nclassiﬁers produced by the co-training procedure obtain-\ning two other variants of CURL that we denoted as\nCURL-EF&LF(EP+LR) and CURL-EF&LFn(EP+LR).\nHowever, in our experiments, these variants did not\ncaused any signiﬁcant improvement when compared to\nCURL-LF(EP+LR).\n8\nTABLE II\nMEAN AVERAGE PRECISION (MAP) OF THE CURL VARIANTS, IN THE (EP+LR) EMBODIMENT, VARYING THE NUMBER OF LABELED\nIMAGES PER CLASS AT THE DIFFERENT CO-TRAINING ROUNDS OBTAINED ON THE SCENE-15 DATA SET IN THE THREE LEARNING\nSCENARIOS CONSIDERED: INDUCTIVE (LEFT), TRANSDUCTIVE (MIDDLE), AND SELF-TAUGHT (RIGHT). FOR CLARITY, THE (EP+LR)\nSUFFIXES HAVE BEEN OMITTED.\n# co-train round\n# img\nmethod\n0\n1\n2\n3\n4\n5\n1\nCURL-EF\n38.3\n41.0\n41.1\n41.1\n41.1\n41.2\nCURL-LF\n40.0\n42.4\n43.8\n44.0\n44.3\n44.4\nCURL-EF&LF\n-\n43.3\n43.7\n43.8\n44.0\n44.0\nCURL-EFn\n38.3\n38.4\n38.5\n38.4\n38.3\n38.2\nCURL-LFn\n40.0\n39.9\n39.9\n39.7\n39.6\n39.4\nCURL-EF&LFn\n-\n40.4\n40.4\n40.3\n40.1\n40.0\n2\nCURL-EF\n44.6\n46.3\n46.4\n46.5\n46.6\n46.6\nCURL-LF\n48.5\n50.2\n50.7\n50.9\n51.1\n51.3\nCURL-EF&LF\n-\n49.7\n50.0\n50.1\n50.2\n50.3\nCURL-EFn\n44.6\n44.3\n44.7\n44.7\n44.5\n44.2\nCURL-LFn\n48.5\n48.8\n48.8\n48.4\n48.0\n47.3\nCURL-EF&LFn\n-\n48.0\n48.0\n47.8\n47.4\n46.9\n3\nCURL-EF\n50.8\n52.1\n52.2\n52.3\n52.4\n52.4\nCURL-LF\n54.1\n55.2\n56.2\n56.4\n56.6\n56.8\nCURL-EF&LF\n-\n55.3\n55.7\n55.9\n56.0\n56.1\nCURL-EFn\n50.8\n51.6\n52.5\n52.6\n52.0\n51.8\nCURL-LFn\n54.1\n55.3\n55.3\n55.1\n54.7\n54.2\nCURL-EF&LFn\n-\n55.0\n55.2\n55.0\n54.4\n54.0\n5\nCURL-EF\n55.6\n56.1\n56.2\n56.2\n56.2\n56.2\nCURL-LF\n58.3\n59.0\n59.3\n59.5\n59.5\n59.6\nCURL-EF&LF\n-\n59.0\n59.1\n59.2\n59.3\n59.4\nCURL-EFn\n55.6\n56.5\n57.7\n57.7\n57.7\n57.6\nCURL-LFn\n58.3\n59.7\n59.8\n60.0\n59.8\n59.8\nCURL-EF&LFn\n-\n59.6\n59.9\n59.9\n59.7\n59.6\n10\nCURL-EF\n62.8\n63.1\n63.2\n63.2\n63.2\n63.2\nCURL-LF\n66.2\n66.5\n66.6\n66.9\n67.0\n67.1\nCURL-EF&LF\n-\n66.3\n66.4\n66.5\n66.6\n66.7\nCURL-EFn\n62.8\n64.7\n65.3\n65.5\n65.8\n65.8\nCURL-LFn\n66.2\n67.6\n67.8\n68.2\n68.2\n68.3\nCURL-EF&LFn\n-\n67.5\n67.8\n68.0\n68.0\n68.0\n15\nCURL-EF\n66.0\n66.1\n66.1\n66.1\n66.1\n66.1\nCURL-LF\n69.2\n69.4\n69.6\n69.8\n69.8\n69.9\nCURL-EF&LF\n-\n69.3\n69.3\n69.4\n69.4\n69.5\nCURL-EFn\n66.0\n67.6\n68.3\n68.5\n68.7\n68.8\nCURL-LFn\n69.2\n70.6\n70.9\n71.0\n71.0\n70.9\nCURL-EF&LFn\n-\n70.4\n70.8\n70.8\n70.9\n70.9\n20\nCURL-EF\n67.8\n67.9\n67.9\n67.9\n67.9\n67.9\nCURL-LF\n71.1\n71.3\n71.3\n71.5\n71.5\n71.6\nCURL-EF&LF\n-\n71.2\n71.2\n71.2\n71.2\n71.3\nCURL-EFn\n67.8\n69.2\n69.8\n70.1\n70.2\n70.3\nCURL-LFn\n71.1\n72.0\n72.4\n72.5\n72.5\n72.5\nCURL-EF&LFn\n-\n71.9\n72.2\n72.4\n72.4\n72.4\n# co-train round\n# img\nmethod\n0\n1\n2\n3\n4\n5\n1\nCURL-EF\n39.3\n41.7\n41.9\n41.9\n41.9\n41.9\nCURL-LF\n39.8\n42.6\n43.8\n44.1\n44.2\n44.4\nCURL-EF&LF\n-\n43.5\n43.9\n44.1\n44.1\n44.2\nCURL-EFn\n39.3\n38.9\n38.9\n38.9\n38.7\n38.7\nCURL-LFn\n39.8\n39.9\n39.9\n39.7\n39.6\n39.3\nCURL-EF&LFn\n-\n40.4\n40.4\n40.4\n40.2\n40.0\n2\nCURL-EF\n47.3\n49.2\n49.5\n49.6\n49.7\n49.7\nCURL-LF\n48.7\n50.6\n51.5\n51.9\n52.0\n52.1\nCURL-EF&LF\n-\n51.2\n51.7\n51.9\n52.0\n52.1\nCURL-EFn\n47.3\n47.4\n47.8\n47.5\n47.1\n46.9\nCURL-LFn\n48.7\n49.3\n49.0\n48.8\n48.5\n47.8\nCURL-EF&LFn\n-\n49.6\n49.5\n49.2\n48.8\n48.2\n3\nCURL-EF\n53.2\n54.4\n54.5\n54.6\n54.7\n54.7\nCURL-LF\n54.8\n55.8\n56.6\n57.2\n57.4\n57.5\nCURL-EF&LF\n-\n56.4\n56.8\n57.2\n57.3\n57.4\nCURL-EFn\n53.2\n53.4\n53.7\n53.5\n52.7\n52.2\nCURL-LFn\n54.8\n55.4\n55.4\n54.9\n54.4\n53.7\nCURL-EF&LFn\n-\n55.8\n55.7\n55.2\n54.5\n53.9\n5\nCURL-EF\n58.6\n59.1\n59.2\n59.2\n59.2\n59.2\nCURL-LF\n60.3\n61.3\n61.6\n61.7\n61.9\n61.9\nCURL-EF&LF\n-\n61.5\n61.6\n61.7\n61.8\n61.8\nCURL-EFn\n58.6\n59.6\n60.4\n60.3\n60.0\n59.7\nCURL-LFn\n60.3\n61.8\n62.0\n61.9\n61.7\n61.5\nCURL-EF&LFn\n-\n61.9\n62.1\n61.9\n61.6\n61.3\n10\nCURL-EF\n64.4\n64.6\n64.6\n64.6\n64.7\n64.7\nCURL-LF\n66.6\n67.0\n67.2\n67.3\n67.5\n67.6\nCURL-EF&LF\n-\n67.1\n67.1\n67.2\n67.3\n67.4\nCURL-EFn\n64.4\n66.2\n67.0\n67.3\n67.4\n67.5\nCURL-LFn\n66.6\n68.5\n68.8\n69.0\n69.0\n69.0\nCURL-EF&LFn\n-\n68.4\n68.8\n69.0\n69.0\n69.0\n15\nCURL-EF\n67.9\n68.0\n68.0\n68.0\n68.0\n68.0\nCURL-LF\n70.1\n70.5\n70.6\n70.7\n70.8\n71.0\nCURL-EF&LF\n-\n70.5\n70.6\n70.6\n70.7\n70.8\nCURL-EFn\n67.9\n69.5\n70.4\n70.7\n70.8\n70.9\nCURL-LFn\n70.1\n71.7\n71.9\n72.2\n72.3\n72.3\nCURL-EF&LFn\n-\n71.7\n72.1\n72.3\n72.4\n72.4\n20\nCURL-EF\n69.7\n69.8\n69.8\n69.8\n69.8\n69.8\nCURL-LF\n72.1\n72.2\n72.3\n72.4\n72.5\n72.6\nCURL-EF&LF\n-\n72.3\n72.3\n72.4\n72.4\n72.4\nCURL-EFn\n69.7\n71.2\n71.7\n72.1\n72.2\n72.3\nCURL-LFn\n72.1\n73.3\n73.6\n73.7\n73.7\n73.7\nCURL-EF&LFn\n-\n73.3\n73.6\n73.7\n73.8\n73.8\n# co-train round\n# img\nmethod\n0\n1\n2\n3\n4\n5\n1\nCURL-EF\n32.4\n34.8\n35.2\n35.3\n35.3\n35.3\nCURL-LF\n36.7\n38.6\n39.6\n39.7\n39.8\n39.9\nCURL-EF&LF\n-\n38.4\n38.9\n38.9\n39.0\n39.0\nCURL-EFn\n32.4\n32.4\n32.8\n32.8\n32.8\n32.3\nCURL-LFn\n36.7\n34.9\n34.8\n34.8\n34.6\n34.2\nCURL-EF&LFn\n-\n35.5\n35.5\n35.6\n35.4\n35.0\n2\nCURL-EF\n41.0\n42.4\n42.5\n42.6\n42.6\n42.7\nCURL-LF\n45.4\n47.1\n47.6\n47.5\n47.7\n47.7\nCURL-EF&LF\n-\n46.4\n46.5\n46.6\n46.7\n46.7\nCURL-EFn\n41.0\n41.2\n42.7\n42.5\n41.6\n41.4\nCURL-LFn\n45.4\n44.3\n44.2\n43.7\n43.1\n42.3\nCURL-EF&LFn\n-\n44.8\n45.1\n44.7\n43.7\n43.0\n3\nCURL-EF\n48.5\n49.4\n49.5\n49.5\n49.5\n49.6\nCURL-LF\n53.4\n54.0\n54.5\n54.4\n54.5\n54.5\nCURL-EF&LF\n-\n53.5\n53.6\n53.6\n53.6\n53.6\nCURL-EFn\n48.5\n49.9\n50.6\n50.6\n49.8\n49.3\nCURL-LFn\n53.4\n52.0\n52.3\n52.0\n51.6\n51.1\nCURL-EF&LFn\n-\n52.8\n52.9\n52.6\n51.9\n51.3\n5\nCURL-EF\n55.2\n55.4\n55.5\n55.5\n55.5\n55.4\nCURL-LF\n59.4\n59.8\n60.0\n60.1\n60.1\n60.1\nCURL-EF&LF\n-\n59.3\n59.4\n59.4\n59.4\n59.4\nCURL-EFn\n55.2\n56.6\n57.5\n57.5\n57.2\n57.1\nCURL-LFn\n59.4\n59.5\n59.6\n59.6\n59.4\n59.2\nCURL-EF&LFn\n-\n59.8\n59.8\n59.7\n59.3\n59.1\n10\nCURL-EF\n62.9\n63.1\n63.1\n63.1\n63.1\n63.1\nCURL-LF\n66.0\n66.4\n66.7\n66.7\n66.9\n67.0\nCURL-EF&LF\n-\n66.6\n66.7\n66.7\n66.7\n66.8\nCURL-EFn\n62.9\n64.7\n65.7\n66.0\n66.1\n66.3\nCURL-LFn\n66.0\n67.1\n67.3\n67.5\n67.6\n67.6\nCURL-EF&LFn\n-\n67.2\n67.6\n67.7\n67.8\n67.8\n15\nCURL-EF\n67.3\n67.5\n67.5\n67.5\n67.5\n67.5\nCURL-LF\n70.5\n70.8\n70.9\n71.1\n71.2\n71.3\nCURL-EF&LF\n-\n71.0\n71.0\n71.1\n71.2\n71.2\nCURL-EFn\n67.3\n69.2\n69.8\n70.1\n70.2\n70.2\nCURL-LFn\n70.5\n70.8\n71.1\n71.3\n71.5\n71.5\nCURL-EF&LFn\n-\n71.3\n71.5\n71.7\n71.8\n71.8\n20\nCURL-EF\n69.1\n69.2\n69.2\n69.2\n69.2\n69.2\nCURL-LF\n72.3\n72.4\n72.4\n72.5\n72.6\n72.8\nCURL-EF&LF\n-\n72.6\n72.6\n72.7\n72.7\n72.8\nCURL-EFn\n69.1\n70.8\n71.5\n71.7\n71.8\n71.9\nCURL-LFn\n72.3\n72.4\n72.7\n72.8\n72.8\n72.8\nCURL-EF&LFn\n-\n72.9\n73.2\n73.2\n73.3\n73.3\nA. Performance across co-training rounds\nHere we analyze in more details the performance\nof our strategy across the ﬁve co-training rounds. Re-\nsults are reported in Fig. 4 with lines of increasing\ncolor saturation corresponding to rounds one to ﬁve.\nCURL-LF(EP+LR) is reported in red lines, while CURL-\nLFn(EP+LR) in blue. Results are reported in terms of\nMAP improvements with respect to EP+LR, which, we\nrecall, corresponds to CURL-EF(EP+LR) with zero co-\ntraining rounds. For CURL-LF(EP+LR), performances\nalways increase with the number of rounds. For CURL-\nLFn(EP+LR), this is not true on the Scene-15 data set\nwith a small number of labeled examples. In CURL-\nLFn(EP+LR) each round of co-training adds all the\npromising unlabeled samples, with a high chance of\nincluding some of them with the wrong pseudo-label.\nThis may result in a ‘concept drift’, with the classiﬁers\nbeing pulled away from the concepts represented by the\nlabeled examples. This risk is lower on the Caltech-101\n(which tends to have more homogeneous classes than\nScene-15) and when there are more labeled images. The\noriginal CURL-LF(EP+LR) is more conservative, since\neach of its co-training rounds adds a single image per\nclass. As a result, increasing the rounds usually increases\nMAP and never decreases it by an appreciable amount.\nWe\nobserved\nthe\nsame\nbehavior\nfor\nCURL-\nEF(EP+LR) and CURL-EFn(EP+LR). We omit the\nrelative ﬁgures for sake of brevity.\nThe plots conﬁrm that CURL-LF(EP+LR) is better\nsuited for small sets of labeled images, while CURL-\nLFn(EP+LR) is to be preferred when more labeled\nexamples are available. The representation learned from\nlate fused features explains part of the effectiveness of\nCURL. In fact, even CURL-LF(EP+LR) without co-\ntraining (zero rounds) outperforms the baseline repre-\nsented by Ensemble Projection.\nB. Leveraging CNN features in CURL\nIn this further experiment we want to test if the\nproposed classiﬁcation strategy works when more pow-\nerful features are used. Recent results indicate that the\ngeneric descriptors extracted from pre-trained Convolu-\ntional Neural Networks (CNN) are able to obtain con-\nsistently superior results compared to the highly tuned\nstate of the art systems in all the visual classiﬁcation\ntasks on various datasets [57]. We extract a 4096-\n9\nTABLE III\nMEAN AVERAGE PRECISION (MAP) OF THE CURL VARIANTS, IN THE (EP+LR) EMBODIMENT, VARYING THE NUMBER OF LABELED\nIMAGES PER CLASS AT THE DIFFERENT CO-TRAINING ROUNDS OBTAINED ON THE CALTECH-101 DATA SET IN THE THREE LEARNING\nSCENARIOS CONSIDERED: INDUCTIVE (LEFT), TRANSDUCTIVE (MIDDLE), AND SELF-TAUGHT (RIGHT). FOR CLARITY, THE (EP+LR)\nSUFFIXES HAVE BEEN OMITTED.\n# co-train round\n# img\nmethod\n0\n1\n2\n3\n4\n5\n1\nCURL-EF\n8.3\n10.4\n10.5\n10.5\n10.5\n10.5\nCURL-LF\n10.1\n11.6\n11.8\n11.8\n11.8\n11.8\nCURL-EF&LF\n-\n11.5\n11.6\n11.6\n11.6\n11.6\nCURL-EFn\n8.3\n8.3\n8.5\n8.5\n8.8\n8.8\nCURL-LFn\n10.1\n10.3\n10.3\n10.6\n10.7\n10.7\nCURL-EF&LFn\n-\n9.5\n9.6\n9.8\n10.0\n10.1\n2\nCURL-EF\n12.6\n14.2\n14.3\n14.3\n14.3\n14.2\nCURL-LF\n15.3\n16.3\n16.3\n16.3\n16.2\n16.2\nCURL-EF&LF\n-\n15.9\n15.9\n15.8\n15.8\n15.8\nCURL-EFn\n12.6\n12.6\n13.4\n13.5\n13.9\n14.1\nCURL-LFn\n15.3\n15.7\n15.9\n16.1\n16.2\n16.3\nCURL-EF&LFn\n-\n14.6\n15.1\n15.3\n15.5\n15.7\n3\nCURL-EF\n15.5\n16.8\n16.9\n16.8\n16.9\n16.9\nCURL-LF\n18.8\n19.4\n19.6\n19.6\n19.7\n19.6\nCURL-EF&LF\n-\n18.9\n19.0\n18.9\n18.9\n18.9\nCURL-EFn\n15.5\n15.9\n16.8\n16.9\n17.1\n17.1\nCURL-LFn\n18.8\n19.5\n19.8\n20.0\n20.0\n20.0\nCURL-EF&LFn\n-\n18.5\n19.0\n19.2\n19.3\n19.2\n5\nCURL-EF\n19.4\n20.2\n20.4\n20.4\n20.4\n20.4\nCURL-LF\n23.2\n23.4\n23.7\n23.7\n23.7\n23.7\nCURL-EF&LF\n-\n22.9\n23.0\n23.0\n23.0\n23.0\nCURL-EFn\n19.4\n20.3\n21.0\n21.2\n21.3\n21.3\nCURL-LFn\n23.2\n24.2\n24.4\n24.6\n24.6\n24.6\nCURL-EF&LFn\n-\n23.3\n23.7\n23.9\n23.9\n24.0\n10\nCURL-EF\n-\n-\n-\n-\n-\n-\nCURL-LF\n-\n-\n-\n-\n-\n-\nCURL-EF&LF\n-\n-\n-\n-\n-\n-\nCURL-EFn\n-\n-\n-\n-\n-\n-\nCURL-LFn\n-\n-\n-\n-\n-\n-\nCURL-EF&LFn\n-\n-\n-\n-\n-\n-\n15\nCURL-EF\n-\n-\n-\n-\n-\n-\nCURL-LF\n-\n-\n-\n-\n-\n-\nCURL-EF&LF\n-\n-\n-\n-\n-\n-\nCURL-EFn\n-\n-\n-\n-\n-\n-\nCURL-LFn\n-\n-\n-\n-\n-\n-\nCURL-EF&LFn\n-\n-\n-\n-\n-\n-\n20\nCURL-EF\n-\n-\n-\n-\n-\n-\nCURL-LF\n-\n-\n-\n-\n-\n-\nCURL-EF&LF\n-\n-\n-\n-\n-\n-\nCURL-EFn\n-\n-\n-\n-\n-\n-\nCURL-LFn\n-\n-\n-\n-\n-\n-\nCURL-EF&LFn\n-\n-\n-\n-\n-\n-\n# co-train round\n# img\nmethod\n0\n1\n2\n3\n4\n5\n1\nCURL-EF\n8.7\n11.1\n11.2\n11.2\n11.2\n11.2\nCURL-LF\n10.5\n12.2\n12.4\n12.4\n12.4\n12.4\nCURL-EF&LF\n-\n12.1\n12.2\n12.2\n12.2\n12.2\nCURL-EFn\n8.7\n8.8\n8.8\n8.8\n8.9\n8.9\nCURL-LFn\n10.5\n10.5\n10.5\n10.6\n10.6\n10.6\nCURL-EF&LFn\n-\n9.8\n9.8\n9.9\n9.9\n9.9\n2\nCURL-EF\n13.1\n14.4\n14.5\n14.6\n14.5\n14.5\nCURL-LF\n15.7\n16.5\n16.6\n16.6\n16.6\n16.6\nCURL-EF&LF\n-\n16.0\n16.1\n16.1\n16.1\n16.0\nCURL-EFn\n13.1\n13.1\n13.8\n13.9\n14.1\n14.2\nCURL-LFn\n15.7\n16.2\n16.4\n16.5\n16.6\n16.6\nCURL-EF&LFn\n-\n15.2\n15.5\n15.7\n15.8\n15.8\n3\nCURL-EF\n15.7\n16.8\n16.9\n16.9\n17.0\n17.0\nCURL-LF\n19.0\n19.5\n19.8\n19.8\n19.8\n19.8\nCURL-EF&LF\n-\n18.9\n19.0\n19.0\n19.0\n19.0\nCURL-EFn\n15.7\n16.2\n17.2\n17.3\n17.5\n17.5\nCURL-LFn\n19.0\n20.0\n20.2\n20.3\n20.3\n20.3\nCURL-EF&LFn\n-\n18.8\n19.3\n19.4\n19.5\n19.5\n5\nCURL-EF\n20.0\n20.6\n20.7\n20.8\n20.7\n20.7\nCURL-LF\n23.7\n24.0\n24.2\n24.3\n24.3\n24.3\nCURL-EF&LF\n-\n23.2\n23.3\n23.4\n23.4\n23.4\nCURL-EFn\n20.0\n20.9\n21.8\n21.9\n22.0\n22.0\nCURL-LFn\n23.7\n24.9\n25.1\n25.2\n25.2\n25.2\nCURL-EF&LFn\n-\n23.8\n24.3\n24.4\n24.4\n24.4\n10\nCURL-EF\n26.0\n26.3\n26.4\n26.4\n26.4\n26.4\nCURL-LF\n30.2\n30.2\n30.4\n30.4\n30.5\n30.5\nCURL-EF&LF\n-\n29.6\n29.6\n29.6\n29.7\n29.7\nCURL-EFn\n26.0\n27.3\n27.8\n27.9\n27.9\n27.9\nCURL-LFn\n30.2\n31.6\n31.8\n31.8\n31.8\n31.8\nCURL-EF&LFn\n-\n30.7\n31.0\n31.1\n31.0\n31.0\n15\nCURL-EF\n29.5\n29.7\n29.8\n29.8\n29.8\n29.8\nCURL-LF\n34.0\n34.0\n34.1\n34.2\n34.2\n34.3\nCURL-EF&LF\n-\n33.4\n33.5\n33.5\n33.5\n33.5\nCURL-EFn\n29.5\n30.7\n31.1\n31.3\n31.3\n31.3\nCURL-LFn\n34.0\n35.7\n35.9\n36.0\n36.0\n36.0\nCURL-EF&LFn\n-\n34.7\n35.0\n35.1\n35.1\n35.1\n20\nCURL-EF\n31.9\n32.1\n32.1\n32.1\n32.1\n32.1\nCURL-LF\n36.4\n36.4\n36.5\n36.6\n36.6\n36.6\nCURL-EF&LF\n-\n36.1\n36.1\n36.1\n36.0\n36.0\nCURL-EFn\n31.9\n33.4\n33.8\n33.9\n34.0\n34.0\nCURL-LFn\n36.4\n38.1\n38.4\n38.5\n38.6\n38.6\nCURL-EF&LFn\n-\n37.5\n37.8\n37.9\n37.9\n37.9\n# co-train round\n# img\nmethod\n0\n1\n2\n3\n4\n5\n1\nCURL-EF\n8.4\n9.3\n9.3\n9.2\n9.2\n9.2\nCURL-LF\n10.7\n11.4\n11.3\n11.2\n11.1\n11.1\nCURL-EF&LF\n-\n10.9\n10.8\n10.7\n10.7\n10.7\nCURL-EFn\n8.4\n8.4\n8.4\n8.4\n8.4\n8.4\nCURL-LFn\n10.7\n10.7\n10.7\n10.7\n10.7\n10.7\nCURL-EF&LFn\n-\n10.1\n10.1\n10.1\n10.1\n10.1\n2\nCURL-EF\n12.5\n13.3\n13.3\n13.4\n13.4\n13.4\nCURL-LF\n15.4\n16.0\n16.0\n15.9\n15.9\n15.9\nCURL-EF&LF\n-\n15.5\n15.4\n15.4\n15.4\n15.4\nCURL-EFn\n12.5\n12.5\n12.5\n12.5\n12.5\n12.5\nCURL-LFn\n15.4\n15.3\n15.3\n15.4\n15.4\n15.3\nCURL-EF&LFn\n-\n14.7\n14.7\n14.7\n14.7\n14.7\n3\nCURL-EF\n15.3\n15.9\n16.0\n16.0\n16.0\n16.0\nCURL-LF\n18.5\n18.8\n18.9\n18.8\n18.8\n18.8\nCURL-EF&LF\n-\n18.4\n18.4\n18.4\n18.4\n18.4\nCURL-EFn\n15.3\n15.4\n15.4\n15.4\n15.5\n15.5\nCURL-LFn\n18.5\n18.5\n18.5\n18.6\n18.6\n18.6\nCURL-EF&LFn\n-\n17.9\n18.0\n18.0\n18.0\n18.0\n5\nCURL-EF\n19.5\n19.9\n19.9\n19.9\n19.9\n19.9\nCURL-LF\n23.3\n23.4\n23.6\n23.6\n23.6\n23.6\nCURL-EF&LF\n-\n22.9\n23.0\n22.9\n22.9\n22.9\nCURL-EFn\n19.5\n19.6\n19.7\n19.8\n19.8\n19.8\nCURL-LFn\n23.3\n23.4\n23.5\n23.6\n23.6\n23.5\nCURL-EF&LFn\n-\n22.8\n22.8\n22.9\n22.9\n22.9\n10\nCURL-EF\n25.2\n25.4\n25.5\n25.5\n25.5\n25.5\nCURL-LF\n29.9\n29.9\n29.9\n29.9\n29.9\n29.9\nCURL-EF&LF\n-\n29.3\n29.3\n29.3\n29.3\n29.3\nCURL-EFn\n25.2\n25.6\n25.9\n26.0\n26.1\n26.2\nCURL-LFn\n29.9\n30.4\n30.5\n30.6\n30.6\n30.6\nCURL-EF&LFn\n-\n29.7\n29.9\n30.0\n30.0\n30.0\n15\nCURL-EF\n29.0\n29.3\n29.3\n29.3\n29.3\n29.3\nCURL-LF\n33.8\n34.0\n34.1\n34.1\n34.1\n34.1\nCURL-EF&LF\n-\n33.6\n33.7\n33.6\n33.6\n33.7\nCURL-EFn\n29.0\n29.7\n29.9\n30.1\n30.2\n30.3\nCURL-LFn\n33.8\n34.3\n34.6\n34.7\n34.8\n34.8\nCURL-EF&LFn\n-\n34.0\n34.3\n34.4\n34.5\n34.5\n20\nCURL-EF\n31.0\n31.2\n31.2\n31.2\n31.2\n31.2\nCURL-LF\n36.0\n36.2\n36.3\n36.3\n36.3\n36.3\nCURL-EF&LF\n-\n35.9\n35.9\n35.9\n35.9\n35.9\nCURL-EFn\n31.0\n31.9\n32.2\n32.4\n32.5\n32.5\nCURL-LFn\n36.0\n36.6\n36.9\n37.0\n37.1\n37.1\nCURL-EF&LFn\n-\n36.5\n36.8\n36.9\n37.0\n37.0\ndimensional feature vector from each image using the\nCaffe [60] implementation of the deep CNN described\nby Krizhevsky et al. [61]. The CNN was discriminatively\ntrained on a large dataset (ILSVRC 2012) with image-\nlevel annotations to classify images into 1000 different\nclasses. Brieﬂy, a mean-subtracted 227×227 RGB image\nis forward propagated through ﬁve convolutional layers\nand two fully connected layers. Features are obtained\nby extracting activation values of the last hidden layer.\nMore details about the network architecture can be found\nin [60], [61].\nWe leverage the CNN features in CURL using them as\na fourth feature in addition to the three used in Section\nIV. The discriminative power of these CNN features\nalone can be seen in Fig. 5, where their 2D projections\nobtained applying the t-SNE [59] method are reported.\nThe experimental results using the four features, are\nreported in Fig. 6, for both the Scene-15 and Caltech-\n101 data sets. We report the results in the transductive\nscenario only. It can be seen that the results using the\nfour features are signiﬁcantly better than those using\nonly three features mainly due to the discriminative\npower of the CNN features. Furthermore, the CURL\nvariants achieve better results than the baselines. This\nsuggests that CURL is able to effectively leverage both\nlow/mid level features as LBP, PHOG and GIST, and\nmore powerful features as CNN.\nC. Second embodiment of CURL using LapSVM\nIn this Section we want to evaluate the CURL per-\nformance in a different embodiment. Speciﬁcally, we\nsubstitute the EP and LR components with LapSVM-\nbased ones. In the LapSVM, ﬁrst, an unsupervised geo-\nmetrical deformation of the feature kernel is performed.\nThis deformed kernel is then used for classiﬁcation by a\nstandard SVM thus by-passing an explicit deﬁnition of\na new feature representation. In this CURL embodiment\nwe exploit the unsupervised step as surrogate of the URL\ncomponent, and SVM as C component. The EF view\nis obtained concatenating the GIST, PHOG, LBP and\nCNN features and generating the corresponding kernel,\nwhile the LF one is obtained by a linear combination\nof the four kernels computed on each feature. This is\nsimilar to what is done in multiple kernel learning [49].\nDue to its performance in the previous experiments,\nthe χ2 kernel is used for both views. The experimental\n10\nScene-15 inductive\nScene-15 transductive\nScene-15 self-taught\nCaltech-101 inductive\nCaltech-101 transductive\nCaltech-101 self-taught\nFig. 4. Performance obtained by CURL-LF(EP+LR) and CURL-LFn(EP+LR) varying the number of co-training rounds. Performance are\nreported in terms of MAP improvement with respect to Ensemble Projection. Due to the small cardinality of some classes, inductive learning\non the Caltech-101 has been limited to ﬁve labeled images per class.\nScene-15\nCaltech-101\nFig. 5.\n2D projections for the CNN features on the two data sets\nused: Scene-15 (left) and Caltech-101 (right). Different classes are\nrepresented in different colors.\nresults on the Scene-15 and Caltech-101 data sets in the\ntransductive scenario, are reported in Fig. 7. We named\nthe variants of this CURL embodiment by adding the\nsufﬁx (LapSVM). It can be seen that the behavior of the\ndifferent methods is the same of the previous plots, with\nthe LapSVM-based CURL outperforming the standard\nLapSVM. The plots conﬁrm that CURL-LF(LapSVM)\nis better suited for small sets of labeled images, while\nCURL-LFn(LapSVM) is to be preferred when more\nlabeled examples are available.\nIn Fig. 9 and 10 qualitative results for the ‘Panda’\nclass of the Caltech-101 data set are reported: the results\nare relative to the case in which a single instance is\navailable for training and one single example is added\nat each co-training round (i.e. each pair of rows corre-\nspond to CURL-EF(LapSVM) and CURL-LF(LapSVM)\nrespectively). The left part of Fig. 9 contains the training\nexamples that are added by the CURL-EF(LapSVM) and\nCURL-LF(LapSVM) at each co-training round, while the\nright part and Fig. 10 contain the ﬁrst 40 test images\nordered by decreasing classiﬁcation conﬁdence. Samples\nbelonging to the current class are surrounded by a green\nbounding box, while a red one is used for samples\nbelonging to other classes.\nIn the sets of training images, it is possible to see that\nafter the ﬁrst co-training round, CURL-LF(LapSVM)\nselects new examples to add to the training set, while\nCURL-EF(LapSVM) adds examples seleted by CURL-\n11\nScene-15 transductive\nCaltech-101 transductive\nFig. 6. Mean Average Precision (MAP) varying the number of labeled images per class, obtained on the Scene-15 data set (left), and on\nthe Caltech-101 data set (right). Results are obtained using GIST, PHOG, LBP and CNN features.\nScene-15 transductive\nCaltech-101 transductive\nFig. 7. Mean Average Precision (MAP) varying the number of labeled images per class, obtained on the Scene-15 data set (left), and on\nthe Caltech-101 data set (right). Results are obtained using GIST, PHOG, LBP and CNN features.\nLF(LapSVM) in the previous round. This is a pattern\nthat we found to occur also in other categories when\nvery small training sets are used.\nIn the sets of test images, it is possible to see that\nmore and more positive images are recovered. Moreover,\nwe can see how the images belonging to the correct\nclass tends to be classiﬁed with increasing conﬁdence\nand move to the left, while the conﬁdences of images\nbelonging to other classes decrease and are pushed to\nthe right.\nD. Large scale experiment\nIn this experiment we want to test the proposed clas-\nsiﬁcation strategy on a large scale data set, namely the\nILSVRC 2012 which contains a total of 1000 different\nclasses. The experiment is run on the ILSVRC 2012\nvalidation set since the training set was used to learn the\nCNN features. The ILSVRC 2012 validation set, which\ncontains a total of 50 images for each class, has been\nrandomly divided into a training and a test set containing\neach 25 images per class. Again, different numbers of\ntraining images per class were tested (i.e. 1, 2, 3, 5, 10,\nand 20). The second embodiment of CURL is used in\nthis experiment.\nThe experimental results are reported in Fig. 8 and\nrepresent the average performance over ten runs with\nrandom labeled-unlabeled feature splits.\nGiven the large range of MAP values, the plot of MAP\nimprovements with respect to LapSVM baseline is also\nreported. It can be seen that the behavior is similar to that\nof the previous plots, with the LapSVM-based CURL\nvariants outperforming the LapSVM. As for the previous\ndata sets, the plots show that CURL-EF(LapSVM) and\nCURL-LF(LapSVM) are better suited for small sets of\nlabeled images, while CURL-EFn(LapSVM)and CURL-\nLFn(LapSVM) are to be preferred when more labeled\nexamples are available. It is remarkable that the proposed\nclassiﬁcation strategy is able to improve the results of\nthe LapSVM, since the CNN features were speciﬁcally\n12\nlearned for the ILSVRC 2012.\nVI. CONCLUSIONS\nIn this work we have proposed CURL, a semi-\nsupervised image classiﬁcation strategy which exploits\nunlabeled data in two different ways: ﬁrst two image\nrepresentations are obtained by unsupervised learning;\nthen co-training is used to enlarge the labeled training\nset of the corresponding classiﬁers. The two image rep-\nresentations are built using two different fusion schemes:\nearly fusion and late fusion.\nThe proposed strategy has been tested on the Scene-\n15, Caltech-101, and ILSVRC 2012 data sets, and\ncompared with other supervised and semi-supervised\nmethods in three different experimental scenarios: in-\nductive learning, transductive learning, and self-taught\nlearning. We tested two embodiments of CURL and\nseveral variants differing in the co-trained classiﬁer used\nand in the number of pseudo-labeled examples that are\nadded at each co-training round. The experimental results\nshowed that the CURL embodiments outperformed the\nother methods in the state of the art included in the\ncomparisons. In particular, the variants that add a single\npseudo-labeled example per class at each co-training\nround, resulted to perform best in the case of a small\nnumber of labeled images, while the variants adding\nmore examples at each round obtained the best results\nwhen more labeled data are available.\nMoreover, the results of CURL using a combination\nof low/mid and high level features (i.e. LBP, PHOG,\nGIST, and CNN features) outperform those obtained on\nthe same features by state of the art methods. This\nmeans that CURL is able to effectively leverage less\ndiscriminative features (i.e. LBP, PHOG, GIST) to boost\nthe performance of more discriminative ones (i.e. CNN\nfeatures).\nREFERENCES\n[1] O. Chapelle, B. Sch¨olkopf, A. Zien et al., Semi-supervised\nlearning.\nMIT press, 2006.\n[2] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell, “Text\nclassiﬁcation from labeled and unlabeled documents using em,”\nMachine learning, vol. 39, no. 2-3, pp. 103–134, 2000.\n[3] A. Fujino, N. Ueda, and K. Saito, “A hybrid genera-\ntive/discriminative approach to semi-supervised classiﬁer de-\nsign,” in Proc. of the National Conf. on Artiﬁcial Intelligence,\n2005, pp. 764–769.\n[4] A. Blum and S. Chawla, “Learning from labeled and unlabeled\ndata using graph mincuts,” in Proc. 18th Int’l Conf. on Machine\nLearning, 2001, pp. 19–26.\n[5] O. Chapelle, J. Weston, and B. Sch¨olkopf, “Cluster kernels for\nsemi-supervised learning,” in Advances in neural information\nprocessing systems, 2002, pp. 585–592.\n[6] T. Joachims, “Transductive inference for text classiﬁcation using\nsupport vector machines,” in Proc. 16th Int’l Conf. on Machine\nLearning, vol. 99, 1999, pp. 200–209.\n[7] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regu-\nlarization: A geometric framework for learning from labeled\nand unlabeled examples,” The Journal of Machine Learning\nResearch, vol. 7, pp. 2399–2434, 2006.\n[8] A. Blum and T. Mitchell, “Combining labeled and unlabeled\ndata with co-training,” in Proc. of the 11th annual Conf. on\nComputational learning theory, 1998, pp. 92–100.\n[9] Z.-H. Zhou and M. Li, “Semi-supervised learning by disagree-\nment,” Knowledge and Information Systems, vol. 24, no. 3, pp.\n415–439, 2010.\n[10] G. Iyengar and H. J. Nock, “Discriminative model fusion\nfor semantic concept detection and annotation in video,” in\nProceedings of the eleventh ACM international conference on\nMultimedia, 2003, pp. 255–258.\n[11] P. Gehler and S. Nowozin, “On feature combination for mul-\nticlass object classiﬁcation,” in Computer Vision, 2009 IEEE\n12th International Conference on, 2009, pp. 221–228.\n[12] P. Natarajan, S. Wu, S. Vitaladevuni, X. Zhuang, S. Tsakalidis,\nU. Park, and R. Prasad, “Multimodal feature fusion for robust\nevent detection in web videos,” in Computer Vision and Pattern\nRecognition (CVPR), 2012 IEEE Conference on, 2012, pp.\n1298–1305.\n[13] Z.-H. Zhou and M. Li, “Tri-training: Exploiting unlabeled data\nusing three classiﬁers,” Knowledge and Data Engineering, IEEE\nTransactions on, vol. 17, no. 11, pp. 1529–1541, 2005.\n[14] M. Li and Z.-H. Zhou, “Improve computer-aided diagnosis\nwith machine learning techniques using undiagnosed samples,”\nSystems, Man and Cybernetics, Part A: Systems and Humans,\nIEEE Transactions on, vol. 37, no. 6, pp. 1088–1098, 2007.\n[15] Z.-H. Zhou, “When semi-supervised learning meets ensemble\nlearning,” Frontiers of Electrical and Electronic Engineering in\nChina, vol. 6, no. 1, pp. 6–16, 2011.\n[16] M. Wang, X.-S. Hua, and Y. Dai, L-R.and Song, “Enhanced\nsemi-supervised learning for automatic video annotation,” in\nIEEE Int’l Conf. on Multimedia and Expo, 2006, pp. 1485–\n1488.\n[17] S. Gupta, J. Kim, K. Grauman, and R. Mooney, “Watch, listen &\nlearn: Co-training on captioned images and videos,” in Machine\nLearning and Knowledge Discovery in Databases, 2008, pp.\n457–472.\n[18] A. Levin, P. Viola, and Y. Freund, “Unsupervised improvement\nof visual detectors using cotraining,” in Proc. of IEEE Int’l\nConf. on Computer Vision, 2003, pp. 626–633.\n[19] C. Christoudias, K. Saenko, L. Morency, and T. Darrell, “Co-\nadaptation of audio-visual speech and gesture classiﬁers,” in\nProc. of the Int’l Conf. on Multimodal interfaces, 2006, pp.\n84–91.\n[20] H. Feng and T.-S. Chua, “A bootstrapping approach to anno-\ntating large image collection,” in Proc. of the ACM SIGMM\nInt’l Workshop on Multimedia Information Retrieval, 2003, pp.\n55–62.\n[21] H. Bhatt, S. Bharadwaj, R. Singh, M. Vatsa, A. Noore, and\nA. Ross, “On co-training online biometric classiﬁers,” in Int’l\nJoint Conf. on Biometrics, 2011, pp. 1–7.\n[22] S. Tong and E. Chang, “Support vector machine active learning\nfor image retrieval,” in Proc. of ACM Int’l Conf. on Multimedia,\n2001, pp. 107–118.\n[23] M. Guillaumin, J. Verbeek, and C. Schmid, “Multimodal semi-\nsupervised learning for image classiﬁcation,” in IEEE Conf. on\nComputer Vision and Pattern Recognition, 2010, pp. 902–909.\n[24] O. Javed, S. Ali, and M. Shah, “Online detection and clas-\nsiﬁcation of moving objects using progressively improving\ndetectors,” in Computer Vision and Pattern Recognition, 2005.\n13\nILSVRC 2012 transductive\nILSVRC 2012 transductive\nFig. 8.\nMean Average Precision (MAP) varying the number of labeled images per class, obtained on the ILSVRC 2012 data set: MAP\nvalues (left) and MAP improvements over LapSVM baseline (right). Results are obtained using GIST, PHOG, LBP and CNN features.\nCVPR 2005. IEEE Computer Society Conference on, vol. 1.\nIEEE, 2005, pp. 696–701.\n[25] F. Tang, S. Brennan, Q. Zhao, and H. Tao, “Co-tracking using\nsemi-supervised support vector machines,” in Computer Vision,\n2007. ICCV 2007. IEEE 11th International Conference on.\nIEEE, 2007, pp. 1–8.\n[26] W. Wang and Z. Zhou, “Analyzing co-training style algorithms,”\nin Proc. of the European Conf. on Machine Learning, 2007, pp.\n454–465.\n[27] S. Goldman and Y. Zhou, “Enhancing supervised learning with\nunlabeled data,” in Proc. of the Int’l Conf on Machine Learning,\n2000, pp. 327–334.\n[28] M. Chen, Y. Chen, and K. Q. Weinberger, “Automatic feature\ndecomposition for single view co-training,” in Proceedings of\nthe 28th International Conference on Machine Learning (ICML-\n11), 2011, pp. 953–960.\n[29] W. Wang and Z.-H. Zhou, “Co-training with insufﬁcient views,”\nin Asian Conference on Machine Learning, 2013, pp. 467–482.\n[30] W. Wang and Z. Zhou, “A new analysis of co-training,” in Proc.\nof the Int’l Conf on Machine Learning, 2010, pp. 1135–1142.\n[31] G. Hinton, S. Osindero, and Y. Teh, “A fast learning algorithm\nfor deep belief nets,” Neural computation, vol. 18, no. 7, pp.\n1527–1554, 2006.\n[32] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun, “What\nis the best multi-stage architecture for object recognition?” in\nIEEE Int’l Conf. on Computer Vision, 2009, pp. 2146–2153.\n[33] G. Hinton, “Training products of experts by minimizing con-\ntrastive divergence,” Neural computation, vol. 14, no. 8, pp.\n1771–1800, 2002.\n[34] H. Bourlard and Y. Kamp, “Auto-association by multilayer\nperceptrons and singular value decomposition,” Biological cy-\nbernetics, vol. 59, no. 4-5, pp. 291–294, 1988.\n[35] Y. Bengio, “Learning deep architectures for ai,” Foundations\nand trends in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.\n[36] A. Coates and A. Y. Ng, “Learning feature representations with\nk-means,” in Neural Networks: Tricks of the Trade, 2012, pp.\n561–580.\n[37] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray, “Vi-\nsual categorization with bags of keypoints,” in ECCV Workshop\non statistical learning in computer vision, 2004, pp. 1–2.\n[38] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of fea-\ntures: Spatial pyramid matching for recognizing natural scene\ncategories,” in IEEE Conf. on Computer Vision and Pattern\nRecognition, vol. 2, 2006, pp. 2169–2178.\n[39] B. A. Olshausen and D. Field, “Sparse coding with an overcom-\nplete basis set: A strategy employed by v1?” Vision research,\nvol. 37, no. 23, pp. 3311–3325, 1997.\n[40] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online learning\nfor matrix factorization and sparse coding,” The J. of Machine\nLearning Research, vol. 11, pp. 19–60, 2010.\n[41] M. Lewicki and T. Sejnowski, “Learning overcomplete repre-\nsentations,” Neural computation, vol. 12, no. 2, pp. 337–365,\n2000.\n[42] D. Dai and L. V. Gool, “Ensemble projection for semi-\nsupervised image classiﬁcation,” in Computer Vision (ICCV),\n2013 IEEE International Conference on.\nIEEE, 2013, pp.\n2072–2079.\n[43] P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. Kankan-\nhalli, “Multimodal fusion for multimedia analysis: a survey,”\nMultimedia systems, vol. 16, no. 6, pp. 345–379, 2010.\n[44] C. G. Snoek, M. Worring, and A. W. Smeulders, “Early versus\nlate fusion in semantic video analysis,” in Proceedings of\nthe 13th annual ACM international conference on Multimedia.\nACM, 2005, pp. 399–402.\n[45] W. S. Noble et al., “Support vector machine applications\nin computational biology,” Kernel methods in computational\nbiology, pp. 71–92, 2004.\n[46] S. Ayache, G. Qu´enot, and J. Gensel, Classiﬁer fusion for SVM-\nbased multimedia semantic indexing.\nSpringer, 2007.\n[47] Z. Wu, L. Cai, and H. Meng, “Multi-level fusion of audio\nand visual features for speaker identiﬁcation,” in Advances in\nBiometrics.\nSpringer, 2005, pp. 493–499.\n[48] G. R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and\nM. I. Jordan, “Learning the kernel matrix with semideﬁnite\nprogramming,” The Journal of Machine Learning Research,\nvol. 5, pp. 27–72, 2004.\n[49] M. G¨onen and E. Alpaydın, “Multiple kernel learning algo-\nrithms,” The Journal of Machine Learning Research, vol. 12,\npp. 2211–2268, 2011.\n[50] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-\nsupervised learning method for deep neural networks,” in Work-\nshop on Challenges in Representation Learning, ICML, 2013.\n[51] M.-F. Balcan, A. Blum, and K. Yang, “Co-training and expan-\nsion: Towards bridging theory and practice,” in Advances in\nneural information processing systems, 2004, pp. 89–96.\n[52] L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual\nmodels from few training examples: An incremental bayesian\napproach tested on 101 object categories,” Computer Vision and\nImage Understanding, vol. 106, no. 1, pp. 59–70, 2007.\n14\nround 0\nEF\nLF\nround 1\nEF\nLF\nround 2\nEF\nLF\nround 3\nEF\nLF\nround 4\nEF\nLF\nround 5\nEF\nLF\nTraining images\nTest images\nFig. 9.\nQualitative results of the proposed strategy for the ‘Panda’ class of the Caltech-101 data set over ﬁve co-training rounds. Train\nimages are on the left, the ﬁrst 17 test images, ordered by decreasing classiﬁcation conﬁdence are on the right. Test images from 18 to 40\nare reported in Fig. 10.\n[53] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei, “Imagenet: A large-scale hierarchical image database,” in\nComputer Vision and Pattern Recognition, 2009. CVPR 2009.\nIEEE Conference on.\nIEEE, 2009, pp. 248–255.\n[54] A. Oliva and A. Torralba, “Modeling the shape of the scene:\nA holistic representation of the spatial envelope,” International\njournal of computer vision, vol. 42, no. 3, pp. 145–175, 2001.\n[55] A. Bosch, A. Zisserman, and X. Muoz, “Image classiﬁcation\nusing random forests and ferns,” in Computer Vision, 2007.\nICCV 2007. IEEE 11th International Conference on, Oct 2007,\npp. 1–8.\n[56] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution\ngray-scale and rotation invariant texture classiﬁcation with local\nbinary patterns,” Pattern Analysis and Machine Intelligence,\nIEEE Transactions on, vol. 24, no. 7, pp. 971–987, 2002.\n[57] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson,\n“Cnn features off-the-shelf: an astounding baseline for recogni-\ntion,” in Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2014 IEEE Conference on.\nIEEE, 2014, pp. 512–\n519.\n[58] V. Sindhwani, P. Niyogi, and M. Belkin, “Beyond the point\ncloud: from transductive to semi-supervised learning,” in Pro-\nceedings of the 22nd international conference on Machine\nlearning.\nACM, 2005, pp. 824–831.\n[59] L. Van der Maaten and G. Hinton, “Visualizing data using t-\nsne,” Journal of Machine Learning Research, vol. 9, no. 2579-\n2605, p. 85, 2008.\n[60] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional\narchitecture for fast feature embedding,” in Proceedings of the\nACM International Conference on Multimedia.\nACM, 2014,\npp. 675–678.\n[61] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-\nﬁcation with deep convolutional neural networks,” in Advances\nin neural information processing systems, 2012, pp. 1097–1105.\n15\nTest images\nFig. 10.\nQualitative results of the proposed strategy for the ‘Panda’ class of the Caltech-101 data set over ﬁve co-training rounds. The\nimages are ordered by decreasing classiﬁcation conﬁdence. Training image, and test images from 1 to 17 are reported in Fig. 9.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML",
    "I.2.6"
  ],
  "published": "2015-05-29",
  "updated": "2015-09-11"
}