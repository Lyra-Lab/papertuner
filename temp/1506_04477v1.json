{
  "id": "http://arxiv.org/abs/1506.04477v1",
  "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy",
  "authors": [
    "Sang-Woo Lee",
    "Min-Oh Heo",
    "Jiwon Kim",
    "Jeonghee Kim",
    "Byoung-Tak Zhang"
  ],
  "abstract": "The online learning of deep neural networks is an interesting problem of\nmachine learning because, for example, major IT companies want to manage the\ninformation of the massive data uploaded on the web daily, and this technology\ncan contribute to the next generation of lifelong learning. We aim to train\ndeep models from new data that consists of new classes, distributions, and\ntasks at minimal computational cost, which we call online deep learning.\nUnfortunately, deep neural network learning through classical online and\nincremental methods does not work well in both theory and practice. In this\npaper, we introduce dual memory architectures for online incremental deep\nlearning. The proposed architecture consists of deep representation learners\nand fast learnable shallow kernel networks, both of which synergize to track\nthe information of new data. During the training phase, we use various online,\nincremental ensemble, and transfer learning techniques in order to achieve\nlower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image\nrecognition tasks, the proposed dual memory architectures performs much better\nthan the classical online and incremental ensemble algorithm, and their\naccuracies are similar to that of the batch learner.",
  "text": "arXiv:1506.04477v1  [cs.LG]  15 Jun 2015\nDual Memory Architectures for Fast Deep Learning of Stream Data\nvia an Online-Incremental-Transfer Strategy\nSang-Woo Lee\nSLEE@BI.SNU.AC.KR\nMin-Oh Heo\nMOHEO@BI.SNU.AC.KR\nSchool of Computer Science and Engineering, Seoul National University\nJiwon Kim\nG1.KIM@NAVERCORP.COM\nJeonghee Kim\nJEONGHEE.KIM@NAVERCORP.COM\nNaver Labs\nByoung-Tak Zhang\nBTZHANG@BI.SNU.AC.KR\nSchool of Computer Science and Engineering, Seoul National University\nAbstract\nThe online learning of deep neural networks is\nan interesting problem of machine learning be-\ncause, for example, major IT companies want\nto manage the information of the massive data\nuploaded on the web daily, and this technology\ncan contribute to the next generation of lifelong\nlearning. We aim to train deep models from new\ndata that consists of new classes, distributions,\nand tasks at minimal computational cost, which\nwe call online deep learning. Unfortunately, deep\nneural network learning through classical online\nand incremental methods does not work well in\nboth theory and practice. In this paper, we in-\ntroduce dual memory architectures for online in-\ncremental deep learning. The proposed architec-\nture consists of deep representation learners and\nfast learnable shallow kernel networks, both of\nwhich synergize to track the information of new\ndata. During the training phase, we use various\nonline, incremental ensemble, and transfer learn-\ning techniques in order to achieve lower error of\nthe architecture. On the MNIST, CIFAR-10, and\nImageNet image recognition tasks, the proposed\ndual memory architectures performs much better\nthan the classical online and incremental ensem-\nble algorithm, and their accuracies are similar to\nthat of the batch learner.\nICML workshop on Deep Learning 2015, Lille, France, 2015.\nCopyright 2015 by the author(s).\n1. Introduction\nLearning deep neural networks on new data from a poten-\ntially non-stationary stream is an interesting problem in the\nmachine learning ﬁeld for various reasons. From the engi-\nneering perspective, major IT companies may want to up-\ndate their services based on deep neural networks from the\ninformation of massive data uploaded to the web in real\ntime. From the artiﬁcial intelligence perspective, for exam-\nple, we argue that online deep learning is the next probable\nstep towards realizing the next generation of lifelong learn-\ning algorithms. Lifelong learning is a problem of learning\nmultiple consecutive tasks, and it is very important for cre-\nation of intelligent, general-purpose, and ﬂexible machines\n(Thrun & O’ Sullivan, 1996; Ruvolo & Eaton, 2013). On-\nline deep learning can have good properties from the per-\nspective of lifelong learning because deep neural networks\nshow good performance on recognition problems, and their\ntransfer and multi-task learning problem (Heigold et al.,\n2013; Donahue et al., 2014; Yosinski et al., 2014).\nHowever, it is difﬁcult to train deep models in an online\nmanner for several reasons. Most of all, the objective func-\ntion of neural networks is not convex, thus online stochastic\nlearning algorithms cannot guarantee convergence. Learn-\ning new data through neural networks often results in a loss\nof all previously acquired information, which is known as\ncatastrophic forgetting. Because it is a disadvantageous\nconstraint to learn one instance and then discard it in online\nlearning, we can alleviate the constraint by memorizing a\nmoderate amount of data (e.g., 10K). We discover the on-\nline parameter of neural networks with an amount of data,\nwhich works reasonably for stationary data, but does not\nwork well for non-stationary data. On the other hand, if\nwe have sufﬁcient memory capacity, we can instead make\nDual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy\nan incremental ensemble of neural networks. Incremen-\ntal ensemble learning refers to making a weak learner us-\ning new parts of an online dataset, and combining multi-\nple weak learners to obtain better predictive performance.\nThere are several studies that use the incremental ensem-\nble approach (Polikar et al., 2001; Oza & Russell, 2001).\nIn practice, however, a part of entire data is not sufﬁcient\nfor learning highly expressive representations of deep neu-\nral networks; therefore, the incremental ensemble approach\nalone does not work well, as illustrated in Section 3.\nTo solve this problem, we use both online parametric and\nincremental structure learning. Because it is neither triv-\nial nor easy to combine two approaches, we apply trans-\nfer learning to intermediate online and parameter learning.\nThis strategy, which we call an online-incremental-transfer\nstrategy, is one of the key ideas for our proposed architec-\nture. For online incremental deep learning, we introduce\nthe dual memory architecture that consists of the follow-\ning two learning policies, and not simply a group of learn-\ning algorithms. First, this architecture trains two memo-\nries – one is an ensemble of deep neural networks, and the\nother are shallow kernel networks on deep neural networks.\nTwo memories are designed for the different strategies. The\nensemble of deep neural networks learns new information\nin order to adapt its representation, whereas the shallow\nkernel networks aim to manage non-stationary distribution\nand new classes in new data more rapidly. Second, we use\nboth online and incremental ensemble learning through the\ntransfer learning technique. In particular, for example, we\ncontinually train a general model of the entire data seen in\nan online manner, and then, transfer to speciﬁc modules in\norder to incrementally generate an ensemble of neural net-\nworks. In our approach, online and incremental learning\nwork together to achieve a lower error bound for the archi-\ntecture.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 brieﬂy introduces the concept of the dual memory\narchitecture.\nIn Section 3 and 4, we propose and vali-\ndate three speciﬁc examples of learning algorithms that\nsatisfy the policies of the dual memory architecture. On\nthe MNIST, CIFAR-10, and ImageNet image recognition\ntasks, the proposed algorithms performs much better than\nthe classical online and incremental ensemble algorithm,\nand their accuracies are similar to that of the batch learner.\nIn Section 5, we summarize our arguments.\n2. Dual Memory Architectures\nIn addition to the policies described in the previous section,\nwe explain in general terms what dual memory architec-\ntures means, and discuss the type of algorithms that could\nbe included in this framework. However, this description\nis not restricted and can be extended beyond the given ex-\nFigure 1. An dual memory architecture.\nplanation in follow-up studies. Dual memory architecture\nis the learnable system that consists of deep and fast mem-\nory, both of which are trained concurrently by using online,\nincremental, and transfer learning.\n1. Dual memory architecture consists of an ensemble of\nneural networks and shallow kernel networks. We call\nthe former as “deep memory,” and the latter as “fast\nmemory” (Figure 1).\n2. Deep memory learns from new data in an online and\nincremental manner. In deep memory learning, ﬁrst,\na general model is trained on the entire data it has\nseen in an online manner (ﬁrst layer in Figure 1). Sec-\nond, the knowledge or parameter of the general model\nis transferred to incrementally generate an ensemble;\nweak neural network in the ensemble is speciﬁc for\neach data at a speciﬁc time (second layer in Figure 1)\nas clariﬁed in Section 3.\n3. Fast memory is on the deep memory. In other words,\nthe inputs of the shallow kernel network are the hid-\nden nodes of the higher layer of deep neural networks\n(third layer in Figure 1). The deep memory transfers\nits knowledge to the fast memory. The fast memory\nlearns from the new data in an online manner without\nmuch loss of accuracy compared with the batch learn-\ning process. However, batch learning, because of low\ncomputational cost in the parameter learning of shal-\nlow networks, can be used when higher accuracy is\nrequired.\nWhen new instances – potentially a part of which has new\ndistributions and additional classes – arrive gradually, two\nmemories ideally work as follows. First, the weights of the\nfast memory are updated online with scant loss of the ac-\ncuracy of the entire training data; for example, in the case\nof linear regression, no loss exists. In this process, because\nDual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy\nof the transferability of the deep memory, the fast mem-\nory has remarkable performance, especially for new distri-\nbutions and additional classes, as though the fast memory\nhad already trained from many new instances with the same\nclass and similar style (Donahue et al., 2014). Second, rep-\nresentations of the deep memory also learn separately and\nmore slowly from a stored moderate amount of data (e.g.,\n10K), especially because, when we need more data in order\nto make a new weak neural learner for an ensemble. After\na new weak neural learner is made, the fast memory makes\nnew kernels that are functions of hidden values of both old\nand new weak learners. In this procedure, the fast structure\nlearning of the explicit kernel is particularly used in the pa-\nper. As explained above, learning fast and slow is one of\nthe mechanisms how the dual memory architectures work.\nThe other mechanism, online-incremental-transfer strat-\negy, using both online stochastic and incremental learning\nthrough transfer learning technique, is explained in detail\nwith examples. In section 3, we discuss two speciﬁc al-\ngorithms for deep memory. In section 4, we discuss one\nspeciﬁc algorithm for fast memory.\n3. Online Incremental Learning Algorithms\nfor Deep Memory\nFor practical online learning from a massive amount of\ndata, it is good to store a reasonable number of instances\nand discard those that appear less important for learning in\nthe near future. We refer to online learning as a parameter\nﬁne-tuning for new instances without retraining new model\nfrom an entire dataset that the model has seen ever. As a\ntype of practical online learning setting, we consider the\n“mini-dataset-shift learning problem,” which allows keep-\ning at most Nsubset training examples in a storage for on-\nline learning (Algorithm 1).\nAlgorithm 1 Mini-Dataset-Shift Learning Problem\nInitialize a model θ randomly.\nrepeat\nGet new data Dnew.\nMerge Dnew into the storage D (i.e.\nD\n←\nD S Dnew).\nThrow away some data in the storage to make |D| ≤\nNsubset.\nTrain a model θ with D.\nuntil forever\nTo solve this problem, many researchers study incremen-\ntal ensemble learning. We refer to incremental learning as\nstructure learning for new instances; following the infor-\nmation of new data, a new structure is made, and useless\nparts of the structure are removed. Incremental ensemble\nlearning, a type of both incremental and online learning, is\nreferred to as combining multiple weak learners, each of\nwhich is trained on a part of that online dataset. In this\npaper, our proposed algorithms are compared to the simple\nbagging algorithm or “na¨ıve incremental ensemble.” In this\nna¨ıve algorithm, for example, we train the ﬁrst weak learner\nor neural network on the 1 – 10,000th data. After that, the\nsecond neural network learns the 10,001 – 20,000th data.\nThen, the third neural network learns the 20,001 – 30,000th\ndata, and so on (if Nsubset is 10,000). As mentioned later,\nhowever, this algorithm does not work well in our experi-\nments.\n3.1. Mini-Batch-Shift Gradient Descent Ensemble\nFirst, we begin from an alternative approach – online learn-\ning – to complement the simple incremental ensemble ap-\nproach. The ﬁrst step of our ﬁrst algorithm involves us-\ning mini-batch gradient descent at each epoch with recent\nNsubset training examples for accommodating Nnew new\ndata. We refer to this procedure as “mini-batch-shift gradi-\nent descent.” In this algorithm, for example, we ﬁrst train\non the 1 – 10,000th data with mini-batch gradient descent\nwith sufﬁcient epochs. After that, the model learns the 501\n– 10,500th instances with one epoch. Then, the model\nlearns the 1,001 – 11,000th instances with one epoch, and\nso on (if Nsubset is 10,000 and Nnew is 500).\nAlgorithm 2 Mini-Batch-Shift Gradient Descent Ensemble\nCollect ﬁrst Nsubset new data Dfirst.\nLearn a neural network C with Dfirst with enough\nepochs.\nPut Dfirst in the storage D (i.e. D ←Dfirst).\nrepeat\nCollect Nnew new data Dnew such that Nnew <\nNsubset.\nThrow away the oldest Nnew instances in D.\nMerge Dnew into D (i.e. D ←D S Dnew).\nTrain a general neural network C with D with one\nepoch.\nif D is disjoint to the data used in Wprev then\nInitialize a new weak neural network Wnew by pa-\nrameters of C.\nTrain Wnew with D until converge.\nCombine Wnew to a model θ (i.e.\nθ\n←\nθ S{Wnew}).\nRefer to Wnew as Wprev (i.e. Wprev ←Wnew).\nend if\nuntil forever\nIn Section 3.3, we show that mini-batch-shift gradient de-\nscent works well and outperforms the na¨ıve incremental\nensemble. Encouraged by this result, we apply mini-batch-\nshift gradient descent to incremental ensemble learning. To\ncombine online and incremental learning properly, we use\nDual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy\nthe transfer learning technique. Similar to the na¨ıve incre-\nmental ensemble, we train each neural network on each part\nof the online dataset. Unlike the na¨ıve incremental ensem-\nble, we transfer to each neural network from one trained on\nthe entire data seen in an online manner. We refer to the\nneural network trained in an online manner for the entire\ndata as the general neural network C, whereas each weak\nneural network trained in a batch manner for each part of\nthe online dataset is a weak neural network W.\nTo transfer from a general neural network C to each weak\nneural network W, we use the initialize and ﬁne-tune ap-\nproach suggested in (Yosinski et al., 2014). The method we\nuse is as follows: 1) initialize a target neural network with\nall parameters without the last softmax layer of a source\nneural network 2) ﬁne-tune the entire target neural network.\nUsing this method, (Yosinski et al., 2014) achieved 2.1%\nimprovement for transfer learning from one 500-classes to\nanother 500-classes image classiﬁcation task on the Ima-\ngeNet dataset. In the mini-batch-shift gradient descent en-\nsemble, a general neural network C trained by mini-batch-\nshift gradient descent is transferred to each weak neural\nnetwork W (Algorithm 2) and the ensemble of each weak\nlearner W is used for inference. In mini-batch-shift gra-\ndient descent, we use one general neural network C for\ninference, and do not make other neural networks.\n3.2. Neural Prior Ensemble\nDual memory architecture is not just a speciﬁc learning\nprocedure, but a framework for learning data streams. We\nintroduce “neural prior ensemble,” another learning algo-\nrithm for deep memory. In neural prior ensemble, a lastly\ntrained weak neural network Wprev takes the role of the\ngeneral neural network C used in the mini-batch-shift gra-\ndient descent, and it is transferred to a new weak neural\nnetwork Wnew (Algorithm 3). We refers to “neural prior”\nas the strategy for using the last neural network Wnew for\ninference, and neglect the previous neural networks in the\nnext experiments section.\nAlgorithm 3 Neural Prior Ensemble\nrepeat\nCollect Nsubset new data Dnew.\nInitialize a new neural network Wnew by parameters\nof Wprev.\nTrain Wnew with Dnew.\nCombine a weak learner Wnew to a model. θ (i.e. θ ←\nθ S{Wnew})\nRefer to Wnew as Wprev. (i.e. Wprev ←Wnew)\nuntil forever\nFigure 2 illustrates and summarizes ensemble algorithms\nfor deep memory.\nThere is no knowledge transfer in\nna¨ıve incremental learning. In mini-batch-shift gradient de-\nFigure 2. Ensemble algorithms in the paper.\nscent ensemble, a general neural network C transfers their\nknowledge (ﬁrst layer in Figure 2 (c)) to each weak neu-\nral network W (second layer in Figure 2 (c)). In neural\nprior ensemble, a lastly trained weak neural network Wprev\ntransfers their knowledge to a newly constructed neural net-\nwork Wnew.\n3.3. Experiments\nWe evaluate the performance of the proposed algorithm on\nthe MNIST, CIFAR-10, and ImageNet image object clas-\nsiﬁcation dataset. MNIST consists of 60,000 training and\n10,000 test images, from 10 digit classes. CIFAR-10 con-\nsists of 50,000 training and 10,000 test images, from 10 dif-\nferent object classes. ImageNet contains 1,281,167 labeled\ntraining images and 50,000 test images, with each image\nlabeled with one of the 1,000 classes. In experiments on\nImageNet, however, we only use 500,000 images, which\nwill be increased in future studies. Thus, our experiments\non ImageNet in the paper is somewhat disadvantageous be-\ncause online incremental learning algorithms do worse if\ndata is scarce in general. We run various size of deep con-\nvolutional neural networks for each dataset using the demo\ncode in MatConvNet, which is a MATLAB toolbox of con-\nvolutional neural networks (Vedaldi & Lenc, 2014). In our\nexperiments, we do not aim to optimize performance, but\nrather to study online learnability on a standard architec-\nture.\nIn the running of the mini-batch-shift gradient descent, we\nset the learning rate proportional to 1/\n√\nt, where t is a\nvariable proportional to the number of entire data that the\nmodel has ever seen. In the other training algorithms, in-\ncluding the batch learning and the neural prior, we ﬁrst set\nthe learning rate 10−2 and drop it by a constant factor – in\nour experiments, 10 – at some prediﬁned steps. In entire ex-\nperiments, we exploit the momentum of the fast training of\nneural networks; without momentum, we could not reach\nthe reasonable local minima within a moderate amount of\nepochs in our experiments.\nDual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.96\n0.965\n0.97\n0.975\n0.98\n0.985\n0.99\n0.995\n1\n# of online dataset\naccuracy\nMNIST, 10−split\n \n \nBatch\nNeural Prior\nNeural Prior Ensemble\nNaive Incremental Ensemble\nMini−Batch−Shift Gradient Descent\nMini−Batch−Shift Gradient Descent Ensemble\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n# of online dataset\naccuracy\nCIFAR−10, 10−split\n \n \nBatch\nNeural Prior\nNeural Prior Ensemble\nNaive Incremental Ensemble\nMini−Batch−Shift Gradient Descent\nMini−Batch−Shift Gradient Descent Ensemble\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n# of online dataset\naccuracy (Top5)\nImageNet\n \n \nBatch\nNeural Prior\nNeural Prior Ensemble\nNaive Incremental Ensemble\nMini−Batch−Shift Gradient Descent\nFigure 3. Results of 10-split experiments on MNIST, CIFAR-10,\nand ImageNet.\nThe main results on deep memory models are shown in Fig-\nure 3. We randomly split the entire training data into the 10\nonline dataset to make the distribution of the data stream\nstationary; we call this setting ‘10-split experiments’. In\nthis setting, we maintain 1/10 of each entire dataset as the\nnumber of training examples Nmemory in the storage.\nFirst, these results show that mini-dataset-shift learning al-\ngorithms with a single general neural network – i.e. the\nmini-batch-shift gradient descent and the neural prior – out-\nperform the na¨ıve incremental ensemble. In other words,\nthe online learning of a neural network with an amount\n(Nmemory) of stored data is better than simply bagging\neach weak neural network with the same amount of data.\nOur experiments show that learning a part of the entire data\nis not sufﬁcient to make highly expressive representations\nof deep neural networks.\nMeanwhile, the lower accuracies in the early phase of the\nmini-batch-shift gradient descent are conspicuous in each\nﬁgure because we remain as a relatively high learning rate\nthat prevents efﬁcient ﬁne-tuning. We improved the perfor-\nmance of the early phase with batch-style learning of the\nﬁrst online dataset without loss of the accuracy of the latter\nphase in other experiments not shown in the ﬁgures. The\nﬁgure also illustrates that ensemble algorithms for deep\nmemory – i.e. mini-batch-shift gradient descent ensemble\nand neural prior ensemble – perform better than algorithms\nwith a single neural network. Regardless of the improve-\nment, it is a burden to increase the memory and inference\ntime proportional to data size in the ensemble approach.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.68\n0.69\n0.7\n0.71\n0.72\n0.73\n0.74\n0.75\n0.76\n0.77\n0.78\nproportion of source dataset\naccuracy\n \n \nSource Dataset\nTarget Dataset\nEnsemble\nFigure 4. Results of two-split experiments on CIFAR-10\nWhen the data distribution is stationary, however, we found\nthat maintaining a small number of neural networks does\nnot decrease accuracy signiﬁcantly. In our experiment, for\nexample, selecting three over ten neural networks at the end\nof learning in the neural prior ensemble simply decreases\nthe absolute error to less than 1%.\nThe performances of the proposed online learner may seem\ninsufﬁcient compared with the batch learner. However, by\nalleviating the condition, the entire dataset is divided into\nDual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy\ntwo online datasets, the performance losses of the proposed\nensemble decrease. Figure 4 show the results on CIFAR-\n10 split into two online datasets with various proportions of\nthe source and target parts.\n4. Online Incremental Learning Algorithms\nfor Fast Memory\n4.1. Shallow Kernel Networks on the Neural Networks\nWe introduce the fast memory; shallow kernel networks\non the neural networks. In dual memory architectures, the\ninput features of shallow kernel networks we used as fast\nmemory are the activation of deep neural networks. Com-\nplementing the dual memory, the fast memory plays two\nimportant roles for treating stream data. First, a fast mem-\nory integrates the information distributed in each neural\nnetworks of ensemble. On the non-stationary data stream,\nnot only proposed mini-dataset-shift learning algorithm of\na single neural network but also ensemble learning algo-\nrithm for deep memory does not work well. Training fast\nmemory with entire training data makes much better per-\nformance than deep memory alone, in particular, when new\ndata includes new distributions and additional classes. It is\nquite practical, because of low computational costs on pa-\nrameter learning of shallow networks. Second, fast mem-\nory can be updated from each one new instance, with a\nsmall amount of calculation until the features remain un-\nchanged. It does not require without much gain of loss\nfunction comparing to the batch counterpart; in case of the\nlinear regression, loseless. Learning deep memory needs\nexpensive computational costs on inference and backprop-\nagation in deep neural networks, even if deep memory is\ntrained through the online learning algorithm we proposed.\n4.2. Multiplicative Hypernetworks\nIn this section, we introduce a multiplicative hypernetwork\n(mHN) as an example of fast memory. This model is in-\nspired by the sparse population coding model (Zhang et al.,\n2012) and it is revised to be ﬁt to the classiﬁcation task\nwe want to solve. We choose mHNs for their good online\nlearnability via sparse well-shared kernels among classes.\nHowever, there are alternative choices, e.g., a support\nvector machine (SVM) (Liu et al., 2008), and an efﬁcient\nlifelong learning algorithm (ELLA) (Zhou et al., 2012),\namong which SVM is our comparative model. mHNs are\nshallow kernel networks that use a multiplicative function\nas a explicit kernel φ = {φ(1), ..., φ(P )}T where\nφ(p)(v, y) = (v(p,1) × ... × v(p,Kp)) & δ(y).\n× denotes the scalar multiplication and δ denotes the in-\ndicator function. v is the input feature of mHNs, which is\nalso the activation of deep neural networks, and y is the\ntarget class. {v(p,1), ..., v(p,Kp)} is the set of variables used\nin pth kernel. Kp is the order, or the number of variable\nused in pth kernel; in this paper Kp = 2. In the training of\nparameters that correspond to kernels, we obtain weights\nby least-mean-square or linear regression formulation. We\nuse one-vs.-rest strategy for classiﬁcation; i.e., the number\nof linear regressions is the same as that of the class, and\nthe score of each linear regression model is evaluated. This\nsetting guarantees loseless weight update until the features\nremain unchanged.\nP0 = I, B0 = 0\nPt = Pt−1[I −\nφtφT\nt Pt−1\n1+φT\nt Pt−1φt ]\nBt = Bt−1 + φT\nt yt\nw∗\nt = PtBt\nWhere yt is the Boolean scalar whether the class is true\nor false (i.e., 0 or 1), and φt is a kernel vector of tth in-\nstance, the form of kernel φ can have various features, and\nthe search space of the set of kernels is an exponential of\nan exponential. To tackle this problem, we use evolutionary\napproach to ﬁnd a near optimal set of kernels. We randomly\nmake new kernels and discard some kernels less relevant.\nAlgorithm 4 explains the online learning procedure of mul-\ntiplicative hypernetworks.\nAlgorithm 4 Learning Multiplicative Hypernetworks\nrepeat\nGet a new instance dnew.\nif dnew includes new raw feature then\nMake new kernels φnew including the values of new\nfeature explicitly.\nMerge φnew into kernels of model φ.\nFine-tune weights of kernels W of φ with the stor-\nage D.\nDiscard some kernels in φ which seem to be less\nrelevant to target value.\nend if\nUpdate W with dnew.\nCombine dnew to D (i.e. D ←D S{dnew}).\nThrow away some data in the storage seem to be less\nimportant for learning in the near future.\nuntil forever\n4.3. Experiments\nWe evaluate the performance of the proposed fast mem-\nory learning algorithm with convolutional neural networks\n(CNNs) and mHNs on CIFAR-10 dataset. In this setting,\nwe split the entire training data into the 10 online datasets\nwith non-stationary distribution of the class. In particular,\nthe ﬁrst online dataset consists of 40% of class 1, 40% of\nclass 2, and 20% of class 3 data. The second online dataset\nconsists of 40% of class 1, and 20% of class 2 – 5 data.\nDual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n# of online dataset\naccuracy\n \n \nBatch\nNaive Incremental Ensemble\nNeural Prior Ensemble\nSVMs on the CNNs\nmHNs on the CNNs\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nx 10\n4\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n# of instances\naccuracy\nFigure 5. Experimental results on CIFAR-10. (Top) the accuracy\nof various learning algorithms on non-stationary data. (Bottom)\nthe accuracy of the mHN on the CNNs plotted at the every time\nthe one new instance comes.\nThe third online dataset consists of 20% each of class 1 –\n5 data. The fourth online dataset consists of 20% each of\nclass 2 – 6 data, and so on. We maintain 1/10 of entire\ndataset as the number of training examples Nmemory in the\nstorage. We mainly validate mHNs on the deep neural net-\nworks where the neural prior ensemble is used for learning\ndeep memory. We train mHNs in strictly online manner\nuntil new weak learner of ensemble is added; otherwise we\nallow the model to use previous data it has ever seen. It is\nlimitation of our works and will be discussed and improved\nin follow-up studies.\nThe main experiment results on the fast memory models are\nshown in Figure 5. We use neural prior ensemble for deep\nmemory when we validate the fast memory algorithms. Al-\nthough not illustrated in the ﬁgure, the mini-batch-shift gra-\ndient descent and neural prior converge rapidly with the\nnew online dataset and forget the information of old online\ndatasets, as indicated by the research on catastrophic for-\ngetting. Thus, the performance of the deep memory algo-\nrithm on a single neural network does not exceed 50% be-\ncause each online dataset does not include more than 50%\nof the classes. The accuracy of the neural prior ensemble\nexceeds 60%, but it is not sufﬁcient compared with that of\nthe batch learner. The fast memory algorithms – the mHNs\non the CNNs, the SVMs on the CNNs – work better than\na single deep memory algorithm. A difference of the per-\nformance between mHNs and SVMs in the latter phase is\nconspicuous in the ﬁgure, whose meaning and generality is\ndiscussed in follow-up studies.\nThe bottom subﬁgure of Figure 5 shows the performance of\nthe mHNs on the CNNs plotted at the exact time that one\nnew instance arrives. Small squares note the points that\nbefore and after a new weak neural network is made by\nthe neural prior ensemble algorithm. The ﬁgure shows not\nonly that fast memory rapidly learns from each instance of\nthe data stream, but also that the learning of the weak deep\nneural networks is also required. In our experiments, learn-\ning mHNs is approximately 100 times faster than learning\nweak neural networks on average.\n5. Conclusion\nWe introduced dual memory architectures to train deep rep-\nresentative systems without much loss of online learnabil-\nity. In this paper, we studied some properties of online deep\nlearning. First, deep neural networks have online learn-\nability on large-scale object classiﬁcation tasks for station-\nary data stream. Second, for extreme non-stationary data\nstream, deep neural networks forget what they learned pre-\nviously; therefore, making a new module incrementally can\nalleviate this problem. Third, by transferring knowledge\nfrom an old module to a new module, the performance of\nonline learning systems is increased. Fourth, by placing\nshallow kernel networks on deep neural networks, the on-\nline learnability of the architecture is enhanced.\nIn this paper, numerous practical and theoretical issues are\nrevealed, which will be soon discovered in our follow-up\nstudies. We hope these issues will be discussed in the work-\nshop.\nAcknowledgments\nThis work was supported by the Naver Labs.\nThis\nwork was partly supported by the NRF grant funded\nby the Korea government (MSIP) (NRF-2010-0017734-\nVideome) and the IITP grant funded by the Korea gov-\nernment (MSIP) (R0126-15-1072-SW.StarLab, 10035348-\nmLife, 10044009-HRI.MESSI).\nDual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy\nReferences\nS. Thrun and J. O’Sullivan. Discovering structure in multi-\nple learning tasks: The TC algorithm. In ICML, 1996.\nP. Ruvolo and E. Eaton.\nELLA: An Efﬁcient Lifelong\nLearning Algorithm. In ICML, 2013.\nG. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. Ran-\nzato, M. Devin, and J. Dean. Multilingual acoustic mod-\nels using distributed deep neural networks. In ICASSP,\n2013.\nJ. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E.\nTzeng, and T. Darrell. DeCAF: A Deep Convolutional\nActivation Feature for Generic Visual Recognition. In\nICML, 2014.\nJ. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-\nferable are features in deep neural networks? In NIPS,\n2014.\nR. Polikar, L. Udpa, and S. S. Udpa. Learn++: An Incre-\nmental Learning Algorithm for Supervised Neural Net-\nworks.\nIEEE TRANSACTIONS ON SYSTEMS, MAN,\nAND CYBERNETICS PART C: APPLICATIONS AND\nREVIEWS, 31(4):497:508, 2001.\nN. C. Oza and S. Russell. Online Bagging and Boosting.\nIn AISTATS, 2001.\nA. Vedaldi and K. Lenc.\nMatConvNet – Convolutional\nNeural Networks for MATLAB. arXiv:1412.4564,2014.\nB.-T. Zhang, J.-W. Ha, and M. Kang.\nSparse popula-\ntion code models of word learning in concept drift. In\nCogSci, 2012.\nX. Liu, G. Zhang, Y. Zhan, and E. Zhu. An Incremental\nFeature Learning Algorithm Based on Least Square Sup-\nport Vector Machine. Frontiers in Algorithmics, p.330-\n338, 2008.\nG. Zhou, K. Shon, and H. Lee. Online Incremental Feature\nLearning with Denoising Autoencoders.\nIn AISTATS,\n2012.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2015-06-15",
  "updated": "2015-06-15"
}