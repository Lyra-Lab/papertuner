{
  "id": "http://arxiv.org/abs/1507.02672v2",
  "title": "Semi-Supervised Learning with Ladder Networks",
  "authors": [
    "Antti Rasmus",
    "Harri Valpola",
    "Mikko Honkala",
    "Mathias Berglund",
    "Tapani Raiko"
  ],
  "abstract": "We combine supervised learning with unsupervised learning in deep neural\nnetworks. The proposed model is trained to simultaneously minimize the sum of\nsupervised and unsupervised cost functions by backpropagation, avoiding the\nneed for layer-wise pre-training. Our work builds on the Ladder network\nproposed by Valpola (2015), which we extend by combining the model with\nsupervision. We show that the resulting model reaches state-of-the-art\nperformance in semi-supervised MNIST and CIFAR-10 classification, in addition\nto permutation-invariant MNIST classification with all labels.",
  "text": "Semi-Supervised Learning with Ladder Networks\nAntti Rasmus\nThe Curious AI Company, Finland\nHarri Valpola\nThe Curious AI Company, Finland\nMikko Honkala\nNokia Labs, Finland\nMathias Berglund\nAalto University & The Curious AI Company, Finland\nTapani Raiko\nAalto University & The Curious AI Company, Finland\nAbstract\nWe combine supervised learning with unsupervised learning in deep neural net-\nworks. The proposed model is trained to simultaneously minimize the sum of\nsupervised and unsupervised cost functions by backpropagation, avoiding the\nneed for layer-wise pre-training. Our work builds on the Ladder network pro-\nposed by Valpola (2015), which we extend by combining the model with super-\nvision. We show that the resulting model reaches state-of-the-art performance in\nsemi-supervised MNIST and CIFAR-10 classiﬁcation, in addition to permutation-\ninvariant MNIST classiﬁcation with all labels.\n1\nIntroduction\nIn this paper, we introduce an unsupervised learning method that ﬁts well with supervised learning.\nThe idea of using unsupervised learning to complement supervision is not new. Combining an\nauxiliary task to help train a neural network was proposed by Suddarth and Kergosien (1990). By\nsharing the hidden representations among more than one task, the network generalizes better. There\nare multiple choices for the unsupervised task, for example, reconstruction of the inputs at every\nlevel of the model (e.g., Ranzato and Szummer, 2008) or classiﬁcation of each input sample into its\nown class (Dosovitskiy et al., 2014).\nAlthough some methods have been able to simultaneously apply both supervised and unsupervised\nlearning (Ranzato and Szummer, 2008; Goodfellow et al., 2013a), often these unsupervised auxil-\niary tasks are only applied as pre-training, followed by normal supervised learning (e.g., Hinton and\nSalakhutdinov, 2006). In complex tasks there is often much more structure in the inputs than can\nbe represented, and unsupervised learning cannot, by deﬁnition, know what will be useful for the\ntask at hand. Consider, for instance, the autoencoder approach applied to natural images: an aux-\niliary decoder network tries to reconstruct the original input from the internal representation. The\nautoencoder will try to preserve all the details needed for reconstructing the image at pixel level,\neven though classiﬁcation is typically invariant to all kinds of transformations which do not preserve\npixel values. Most of the information required for pixel-level reconstruction is irrelevant and takes\nspace from the more relevant invariant features which, almost by deﬁnition, cannot alone be used\nfor reconstruction.\nOur approach follows Valpola (2015), who proposed a Ladder network where the auxiliary task is\nto denoise representations at every level of the model. The model structure is an autoencoder with\nskip connections from the encoder to decoder and the learning task is similar to that in denoising\nautoencoders but applied to every layer, not just the inputs. The skip connections relieve the pressure\nto represent details in the higher layers of the model because, through the skip connections, the\n1\narXiv:1507.02672v2  [cs.NE]  24 Nov 2015\ndecoder can recover any details discarded by the encoder. Previously, the Ladder network has only\nbeen demonstrated in unsupervised learning (Valpola, 2015; Rasmus et al., 2015a) but we now\ncombine it with supervised learning.\nThe key aspects of the approach are as follows:\nCompatibility with supervised methods. The unsupervised part focuses on relevant details found\nby supervised learning. Furthermore, it can be added to existing feedforward neural networks, for\nexample multi-layer perceptrons (MLPs) or convolutional neural networks (CNNs) (Section 3). We\nshow that we can take a state-of-the-art supervised learning method as a starting point and improve\nthe network further by adding simultaneous unsupervised learning (Section 4).\nScalability resulting from local learning. In addition to a supervised learning target on the top\nlayer, the model has local unsupervised learning targets on every layer, making it suitable for very\ndeep neural networks. We demonstrate this with two deep supervised network architectures.\nComputational efﬁciency. The encoder part of the model corresponds to normal supervised learn-\ning. Adding a decoder, as proposed in this paper, approximately triples the computation during\ntraining but not necessarily the training time since the same result can be achieved faster through\nthe better utilization of the available information. Overall, computation per update scales similarly\nto whichever supervised learning approach is used, with a small multiplicative factor.\nAs explained in Section 2, the skip connections and layer-wise unsupervised targets effectively turn\nautoencoders into hierarchical latent variable models which are known to be well suited for semi-\nsupervised learning. Indeed, we obtain state-of-the-art results in semi-supervised learning in the\nMNIST, permutation invariant MNIST and CIFAR-10 classiﬁcation tasks (Section 4). However,\nthe improvements are not limited to semi-supervised settings: for the permutation invariant MNIST\ntask, we also achieve a new record with the normal full-labeled setting.1\n2\nDerivation and justiﬁcation\nLatent variable models are an attractive approach to semi-supervised learning because they can\ncombine supervised and unsupervised learning in a principled way. The only difference is whether\nthe class labels are observed or not. This approach was taken, for instance, by Goodfellow et al.\n(2013a) with their multi-prediction deep Boltzmann machine. A particularly attractive property of\nhierarchical latent variable models is that they can, in general, leave the details for the lower levels\nto represent, allowing higher levels to focus on more invariant, abstract features that turn out to be\nrelevant for the task at hand.\nThe training process of latent variable models can typically be split into inference and learning, that\nis, ﬁnding the posterior probability of the unobserved latent variables and then updating the under-\nlying probability model to ﬁt the observations better. For instance, in the expectation-maximization\n(EM) algorithm, the E-step corresponds to ﬁnding the expectation of the latent variables over the\nposterior distribution assuming the model ﬁxed and the M-step then maximizes the underlying prob-\nability model assuming the expectation ﬁxed.\nThe main problem with latent variable models is how to make inference and learning efﬁcient. Sup-\npose there are layers l of latent variables z(l). Latent variable models often represent the probability\ndistribution of all the variables explicitly as a product of terms, such as p(z(l) | z(l+1)) in directed\ngraphical models. The inference process and model updates are then derived from Bayes’ rule, typ-\nically as some kind of approximation. The inference is often iterative as it is generally impossible\nto solve the resulting equations in a closed form as a function of the observed variables.\nThere is a close connection between denoising and probabilistic modeling. On the one hand, given\na probabilistic model, you can compute the optimal denoising. Say you want to reconstruct a latent\nz using a prior p(z) and an observation ˜z = z + noise. We ﬁrst compute the posterior distribution\np(z | ˜z), and use its center of gravity as the reconstruction ˆz. One can show that this minimizes\nthe expected denoising cost (ˆz −z)2. On the other hand, given a denoising function, one can draw\n1Preliminary results on the full-labeled setting on a permutation invariant MNIST task were reported in a\nshort early version of this paper (Rasmus et al., 2015b). Compared to that, we have added noise to all layers of\nthe model and further simpliﬁed the denoising function g. This further improved the results.\n2\nsamples from the corresponding distribution by creating a Markov chain that alternates between\ncorruption and denoising (Bengio et al., 2013).\nValpola (2015) proposed the Ladder network, where the inference process itself can be learned by\nusing the principle of denoising, which has been used in supervised learning (Sietsma and Dow,\n1991), denoising autoencoders (dAE) (Vincent et al., 2010), and denoising source separation (DSS)\n(S¨arel¨a and Valpola, 2005) for complementary tasks. In dAE, an autoencoder is trained to reconstruct\nthe original observation x from a corrupted version ˜x. Learning is based simply on minimizing the\nnorm of the difference of the original x and its reconstruction ˆx from the corrupted ˜x; that is the\ncost is ∥ˆx −x∥2.\nWhile dAEs are normally only trained to denoise the observations, the DSS framework is based on\nthe idea of using denoising functions ˆz = g(z) of the latent variables z to train a mapping z = f(x)\nwhich models the likelihood of the latent variables as a function of the observations. The cost\nfunction is identical to that used in a dAE except that the latent variables z replace the observations\nx; that is, the cost is ∥ˆz −z∥2. The only thing to keep in mind is that z needs to be normalized\nsomehow as otherwise the model has a trivial solution at z = ˆz = constant. In a dAE, this cannot\nhappen as the model cannot change the input x.\nFigure 1 depicts the optimal denoising function ˆz = g(˜z) for a one-dimensional bimodal distri-\nbution, which could be the distribution of a latent variable inside a larger model. The shape of\nthe denoising function depends on the distribution of z and the properties of the corruption noise.\nWith no noise at all, the optimal denoising function would be the identity function. In general, the\ndenoising function pushes the values towards higher probabilities, as shown by the green arrows.\nFigure 2 shows the structure of the Ladder network. Every layer contributes to the cost function a\nterm C(l)\nd\n= ∥z(l) −ˆz(l)∥2 which trains the layers above (both encoder and decoder) to learn the\ndenoising function ˆz(l) = g(l)(˜z(l), ˆz(l+1)) which maps the corrupted ˜z(l) onto the denoised estimate\nˆz(l). As the estimate ˆz(l) incorporates all prior knowledge about z, the same cost function term also\ntrains the encoder layers below to ﬁnd cleaner features which better match the prior expectation.\nSince the cost function needs both the clean z(l) and corrupted ˜z(l), during training the encoder is\nrun twice: a clean pass for z(l) and a corrupted pass for ˜z(l). Another feature which differentiates the\nLadder network from regular dAEs is that each layer has a skip connection between the encoder and\ndecoder. This feature mimics the inference structure of latent variable models and makes it possible\nfor the higher levels of the network to leave some of the details for lower levels to represent. Rasmus\net al. (2015a) showed that such skip connections allow dAEs to focus on abstract invariant features\non the higher levels, making the Ladder network a good ﬁt with supervised learning that can select\nwhich information is relevant for the task at hand.\nOne way to picture the Ladder network is to consider it as a collection of nested denoising autoen-\ncoders which share parts of the denoising machinery with each other. From the viewpoint of the\nautoencoder on layer l, the representations on the higher layers can be treated as hidden neurons. In\nother words, there is no particular reason why ˆz(l+i) as produced by the decoder should resemble\nthe corresponding representations z(l+i) as produced by the encoder. It is only the cost function\nC(l+i)\nd\nthat ties these together and forces the inference to proceed in reverse order in the decoder.\nThis sharing helps a deep denoising autoencoder to learn the denoising process as it splits the task\ninto meaningful sub-tasks of denoising intermediate representations.\n3\nImplementation of the Model\nThe steps involved in implementing the Ladder network (Section 3.1) are typically as follows: 1)\ntake a feedforward model which serves supervised learning as the encoder (Section 3.2); 2) add\na decoder which can invert the mappings on each layer of the encoder and supports unsupervised\nlearning (Section 3.3); and 3) train the whole Ladder network by minimizing the sum of all the cost\nfunction terms.\nIn this section, we will go through these steps in detail for a fully connected MLP network and\nbrieﬂy outline the modiﬁcations required for convolutional networks, both of which are used in our\nexperiments (Section 4).\n3\n0\n0\n1\n2\n3\n-1\n-1\n1\n2\n3\n-2\n4\n-2\nCorrupted\nClean\nFigure 1: A depiction of an optimal denoising function for a bimodal distribution. The input for\nthe function is the corrupted value (x axis) and the target is the clean value (y axis). The denoising\nfunction moves values towards higher probabilities as show by the green arrows.\ny\n˜y\ng(1)(·, ·)\ng(0)(·, ·)\nf (1)(·)\nf (1)(·)\nf (2)(·)\nf (2)(·)\nN(0, σ2)\nN(0, σ2)\nN(0, σ2)\nC(2)\nd\nC(1)\nd\nC(0)\nd\n˜z(1)\n˜z(2)\nˆz(2)\nˆz(1)\nz(1)\nz(2)\n˜x\nˆx\nx\nx\nx\ng(2)(·, ·)\nFigure 2: A conceptual illustration of the Ladder network when L = 2. The feedforward path\n(x →z(1) →z(2) →y) shares the mappings f (l) with the corrupted feedforward path, or encoder\n(x →˜z(1) →˜z(2) →˜y). The decoder (˜z(l) →ˆz(l) →ˆx) consists of the denoising functions g(l)\nand has cost functions C(l)\nd\non each layer trying to minimize the difference between ˆz(l) and z(l).\nThe output ˜y of the encoder can also be trained to match available labels t(n).\n4\nAlgorithm 1 Calculation of the output y and cost function C of the Ladder network\nRequire: x(n)\n# Corrupted encoder and classiﬁer\n˜h(0) ←˜z(0) ←x(n) + noise\nfor l = 1 to L do\n˜z(l) ←batchnorm(W(l)˜h(l−1)) + noise\n˜h(l) ←activation(γ(l) ⊙(˜z(l) + β(l)))\nend for\nP(˜y | x) ←˜h(L)\n# Clean encoder (for denoising targets)\nh(0) ←z(0) ←x(n)\nfor l = 1 to L do\nz(l)\npre ←W(l)h(l−1)\nµ(l) ←batchmean(z(l)\npre)\nσ(l) ←batchstd(z(l)\npre)\nz(l) ←batchnorm(z(l)\npre)\nh(l) ←activation(γ(l) ⊙(z(l) + β(l)))\nend for\n# Final classiﬁcation:\nP(y | x) ←h(L)\n# Decoder and denoising\nfor l = L to 0 do\nif l = L then\nu(L) ←batchnorm(˜h(L))\nelse\nu(l) ←batchnorm(V(l+1)ˆz(l+1))\nend if\n∀i : ˆz(l)\ni\n←g(˜z(l)\ni , u(l)\ni ) # Eq. (2)\n∀i : ˆz(l)\ni,BN ←\nˆz(l)\ni\n−µ(l)\ni\nσ(l)\ni\nend for\n# Cost function C for training:\nC ←0\nif t(n) then\nC ←−log P(˜y = t(n) | x(n))\nend if\nC ←C + PL\nl=0 λl\n\r\r\rz(l) −ˆz(l)\nBN\n\r\r\r\n2\n# Eq. (3)\n3.1\nGeneral Steps for Implementing the Ladder Network\nConsider training a classiﬁer,2, or a mapping from input x to output y with targets t, from a training\nset of pairs {x(n), t(n) | 1 ≤n ≤N}. Semi-supervised learning (Chapelle et al., 2006) studies\nhow auxiliary unlabeled data {x(n) | N + 1 ≤n ≤M} can help in training a classiﬁer. It is often\nthe case that labeled data are scarce whereas unlabeled data are plentiful, that is N ≪M.\nThe Ladder network can improve results even without auxiliary unlabeled data but the original\nmotivation was to make it possible to take well-performing feedforward classiﬁers and augment\nthem with an auxiliary decoder as follows:\n1. Train any standard feedforward neural network. The network type is not limited to stan-\ndard MLPs, but the approach can be applied, for example, to convolutional or recurrent\nnetworks. This will be the encoder part of the Ladder network.\n2. For each layer, analyze the conditional distribution of representations given the layer above,\np(z(l) | z(l+1)). The observed distributions could resemble for example Gaussian distri-\nbutions where the mean and variance depend on the values z(l+1), bimodal distributions\nwhere the relative probability masses of the modes depend on the values z(l+1), and so on.\n3. Deﬁne a function ˆz(l) = g(˜z(l), ˆz(l+1)) which can approximate the optimal denoising func-\ntion for the family of observed distributions. The function g is therefore expected to form a\nreconstruction ˆz(l) that resembles the clean z(l) given the corrupted ˜z(l) and the higher-level\nreconstruction ˆz(l+1) .\n4. Train the whole network in a fully-labeled or semi-supervised setting using standard opti-\nmization techniques such as stochastic gradient descent.\n3.2\nFully Connected MLP as Encoder\nAs a starting point we use a fully connected MLP network with rectiﬁed linear units. We follow\nIoffe and Szegedy (2015) and apply batch normalization to each preactivation including the topmost\nlayer in the L-layer network. This serves two purposes. First, it improves convergence as a result\nof reduced covariate shift as originally proposed by Ioffe and Szegedy (2015). Second, as explained\n2Here we only consider the case where the output t(n) is a class label but it is trivial to apply the same\napproach to other regression tasks.\n5\nin Section 2, DSS-type cost functions for all but the input layer require some type of normalization\nto prevent the denoising cost from encouraging the trivial solution where the encoder outputs just\nconstant values as these are the easiest to denoise. Batch normalization conveniently serves this\npurpose, too.\nFormally, batch normalization for the layers l = 1 . . . L is implemented as\nz(l) = NB(W(l)h(l−1))\nh(l) = φ\n\u0000γ(l)(z(l) + β(l))\n\u0001\n,\nwhere h(0) = x, NB is a component-wise batch normalization NB(xi) = (xi−ˆµxi)/ˆσxi, where ˆµxi\nand ˆσxi are estimates calculated from the minibatch, γ(l) and β(l) are trainable parameters, and φ(·)\nis the activation function such as the rectiﬁed linear unit (ReLU) for which φ(·) = max(0, ·). For\noutputs y = h(L) we always use the softmax activation. For some activation functions the scaling\nparameter β(l) or the bias γ(l) are redundant and we only apply them in non-redundant cases. For\nexample, the rectiﬁed linear unit does not need scaling, the linear activation function needs neither\nscaling nor bias, but softmax requires both.\nAs explained in Section 2 and shown in Figure 2, the Ladder network requires two forward passes,\none clean and one corrupted, which produce clean z(l) and h(l) and corrupted ˜z(l) and ˜h(l), respec-\ntively. We implemented corruption by adding isotropic Gaussian noise n to inputs and after each\nbatch normalization:\n˜x\n=\n˜h(0) = x + n(0)\n˜z(l)\npre\n=\nW(l)˜h(l−1)\n˜z(l)\n=\nNB(˜z(l)\npre) + n(l)\n˜h(l)\n=\nφ\n\u0000γ(l)(˜z(l) + β(l))\n\u0001\n.\nNote that we collect the value ˜z(l)\npre here because it will be needed in the decoder cost function in\nSection 3.3.\nThe supervised cost Cc is the average negative log probability of the noisy output ˜y matching the\ntarget t(n) given the inputs x(n)\nCc = −1\nN\nN\nX\nn=1\nlog P(˜y = t(n) | x(n)).\nIn other words, we also use the noise to regularize supervised learning.\nWe saw networks with this structure reach close to state-of-the-art results in purely supervised learn-\ning (see e.g. Table 1), which makes them good starting points for improvement via semi-supervised\nlearning by adding an auxiliary unsupervised task.\n3.3\nDecoder for Unsupervised Learning\nWhen designing a suitable decoder to support unsupervised learning, we had to make a choice as to\nwhat kinds of distributions of the latent variables the decoder would optimally be able to denoise.\nWe ultimately ended up choosing a parametrization that supports the optimal denoising of Gaussian\nlatent variables. We also experimented with alternative denoising functions, more details of which\ncan be found in Appendix B. Further analysis of different denoising functions was recently published\nby Pezeshki et al. (2015).\nIn order to derive the chosen parametrization and justify why it supports Gaussian latent variables,\nlet us begin with the assumption that the noisy value of one latent variable ˜z that we want to denoise\nhas the form ˜z = z + n, where zis the clean latent variable value that has a Gaussian distribution\nwith variance σ2\nz, and n is the Gaussian noise with variance σ2\nn.\nWe now want to estimate ˆz, a denoised version of ˜z, so that the estimate minimizes the squared\nerror of the difference to the clean latent variable values z. It can be shown that the functional form\nof ˆz = g(˜z) has to be linear in order to minimize the denoising cost, with the assumption being\n6\nthat both the noise and the latent variable have a Gaussian distribution (Valpola, 2015, Section 4.1).\nSpeciﬁcally, the result will be a weighted sum of the corrupted ˜z and a prior µ. The weight υ of the\ncorrupted ˜z will be a function of the variance of z and n according to:\nυ =\nσ2\nz\nσ2z + σ2n\nThe denoising function will therefore have the form:\nˆz = g(ˆz) = υ ∗˜z + (1 −υ) ∗µ = (˜z −µ) ∗υ + µ\n(1)\nWe could let υ and µ be trainable parameters of the model, where the model would learn some\nestimate of the optimal weighting υ and prior µ. The problem with this formulation is that it only\nsupports the optimal denoising of latent variables with a Gaussian distribution, as the function g is\nlinear wrt. ˜z.\nWe relax this assumption by making the model only require the distribution of z of a layer to be\nGaussian conditional on the values of the latent variables of the layer above. In a similar vein, in a\nlayer of multiple latent variables we can assume that the latent variables are independent conditional\non the latent variables of the layer above. The distribution of the latent variables z(l) is therefore\nassumed to follow the distribution\np(z(l) | z(l+1)) =\nY\ni\np(z(l)\ni\n| z(l+1))\nwhere p(z(l)\ni\n| z(l+1)) are conditionally independent Gaussian distributions.\nOne interpretation of this formulation is that we are modeling the distribution of z(l) as a mixture of\nGaussians with diagonal covariance matrices, where the value of the above layer z(l+1) modulates\nthe form of the Gaussian that z(l) is distributed as. In practice, we will implement the dependence\nof v and µ on ˆz(l+1) with a batch normalized projection from ˆz(l+1) followed by an expressive\nnonlinearity with trainable parameters. The ﬁnal formulation of the denoising function is therefore\nˆz(l)\ni\n= gi(˜z(l)\ni , u(l)\ni ) =\n\u0010\n˜z(l)\ni\n−µi(u(l)\ni )\n\u0011\nυi(u(l)\ni ) + µi(u(l)\ni )\n(2)\nwhere u(l)\ni\npropagates information from ˆz(l+1) by a batch normalized projection:\nu(l) = NB(V(l+1)ˆz(l+1)) ,\nwhere the matrix V(l) has the same dimension as the transpose of W(l) on the encoder side. The\nprojection vector u(l) therefore has the same dimensionality as z(l). Furthermore, the functions\nµi(u(l)\ni ) and υi(u(l)\ni ) are modeled as expressive nonlinearities:\nµi(u(l)\ni ) = a(l)\n1,isigmoid(a(l)\n2,iu(l)\ni\n+ a(l)\n3,i) + a(l)\n4,iu(l)\ni\n+ a(l)\n5,i\nυi(u(l)\ni ) = a(l)\n6,isigmoid(a(l)\n7,iu(l)\ni\n+ a(l)\n8,i) + a(l)\n9,iu(l)\ni\n+ a(l)\n10,i,\nwhere a(l)\n1,i . . . a(l)\n10,i are the trainable parameters of the nonlinearity for each neuron i in each layer\nl. It is worth noting that in this parametrization, each denoised value ˆz(l)\ni\nonly depends on ˜z(l)\ni\nand\nnot the full ˜z(l). This means that the model can only optimally denoise conditionally independent\ndistributions. While this nonlinearity makes the number of parameters in the decoder slightly higher\nthan in the encoder, the difference is insigniﬁcant as most of the parameters are in the vertical\nprojection mappings W(l) and V(l), which have the same dimensions (apart from transposition).\nNote the slight abuse of the notation here since g(l)\ni\nis now a function of the scalars ˜z(l)\ni\nand u(l)\ni\nrather than the full vectors ˜z(l) and ˆz(l+1). Given u(l), this parametrization is linear with respect to\n˜z(l), and both the slope and the bias depend nonlinearly on u(l), as we hoped.\nFor the lowest layer, ˆx = ˆz(0) and ˜x = ˜z(0) by deﬁnition, and for the highest layer we chose\nu(L) = ˜y. This allows the highest-layer denoising function to utilize prior information about the\n7\nclasses being mutually exclusive, which seems to improve convergence in cases where there are very\nfew labeled samples.\nAs a side note, if the values of z(l) are truly independently distributed Gaussians, there is nothing\nleft for the layer above, ˆz(l+1), to model. In that case, a mixture of Gaussians is not needed to model\nz(l), but a diagonal Gaussian which can be modeled with a linear denoising function with constant\nvalues for υ and µ as in Equation 1, would sufﬁce. In this parametrization all correlations, non-\nlinearities, and non-Gaussianities in the latent variables z(l) have to be represented by modulations\nfrom the layers above for optimal denoising. As the parametrization allows the distribution of z(l)\nto be modulated by z(l+1) through u(l), it encourages the decoder to ﬁnd representations z(l) that\nhave high mutual information with z(l+1). This is crucial as it allows supervised learning to have\nan indirect inﬂuence on the representations learned by the unsupervised decoder: any abstractions\nselected by supervised learning will bias the lower levels to ﬁnd more representations which carry\ninformation about the same abstractions.\nThe cost function for the unsupervised path is the mean squared reconstruction error per neuron, but\nthere is a slight twist which we found to be important. Batch normalization has useful properties,\nas noted in Section 3.2, but it also introduces noise which affects both the clean and corrupted\nencoder pass. This noise is highly correlated between z(l) and ˜z(l) because the noise derives from\nthe statistics of the samples that happen to be in the same minibatch. This highly correlated noise in\nz(l) and ˜z(l) biases the denoising functions to be simple copies3 ˆz(l) ≈˜z(l).\nThe solution we found was to implicitly use the projections z(l)\npre as the target for denoising and scale\nthe cost function in such a way that the term appearing in the error term is the batch normalized z(l)\ninstead. For the moment, let us see how that works for a scalar case:\n1\nσ2 ∥zpre −ˆz∥2\n=\n\r\r\r\r\nzpre −µ\nσ\n−ˆz −µ\nσ\n\r\r\r\r\n2\n= ∥z −ˆzBN∥2\nz\n=\nNB(zpre) = zpre −µ\nσ\nˆzBN\n=\nˆz −µ\nσ\n,\nwhere µ and σ are the batch mean and batch std of zpre, respectively, that were used in batch\nnormalizing zpre into z. The unsupervised denoising cost function Cd is thus\nCd =\nL\nX\nl=0\nλlC(l)\nd\n=\nL\nX\nl=0\nλl\nNml\nN\nX\nn=1\n\r\r\rz(l)(n) −ˆz(l)\nBN(n)\n\r\r\r\n2\n,\n(3)\nwhere ml is the layer’s width, N the number of training samples, and the hyperparameter λl a layer-\nwise multiplier determining the importance of the denoising cost.\nThe model parameters W(l), γ(l), β(l), V(l), a(l)\ni , b(l)\ni , andc(l)\ni\ncan be trained simply by using the\nbackpropagation algorithm to optimize the total cost C = Cc + Cd. The feedforward pass of the\nfull Ladder network is listed in Algorithm 1. Classiﬁcation results are read from the y in the clean\nfeedforward path.\n3.4\nVariations\nSection 3.3 detailed how to build a decoder for the Ladder network to match the fully connected\nencoder described in Section 3.2. It is easy to extend the same approach to other encoders, for\ninstance, convolutional neural networks (CNN). For the decoder of fully connected networks we\nused vertical mappings whose shape is a transpose of the encoder mapping. The same treatment\nworks for the convolution operations: in the networks we have tested in this paper, the decoder has\nconvolutions whose parametrization mirrors the encoder and effectively just reverses the ﬂow of\n3The whole point of using denoising autoencoders rather than regular autoencoders is to prevent skip con-\nnections from short-circuiting the decoder and force the decoder to learn meaningful abstractions which help\nin denoising.\n8\ninformation. As the idea of convolution is to reduce the number of parameters by weight sharing,\nwe applied this to the parameters of the denoising function g, too.\nMany convolutional networks use pooling operations with stride; that is, they downsample the spa-\ntial feature maps. The decoder needs to compensate for this with a corresponding upsampling. There\nare several alternative ways to implement this and in this paper we chose the following options: 1)\non the encoder side, pooling operations are treated as separate layers with their own batch normal-\nization and linear activation function, and 2) the downsampling of the pooling on the encoder side\nis compensated for by upsampling with copying on the decoder side. This provides multiple targets\nfor the decoder to match, helping the decoder to recover the information lost on the encoder side.\nIt is worth noting that a simple special case of the decoder is a model where λl = 0 when l < L.\nThis corresponds to a denoising cost only on the top layer and means that most of the decoder can\nbe omitted. This model, which we call the Γ-model because of the shape of the graph, is useful as it\ncan easily be plugged into any feedforward network without decoder implementation. In addition,\nthe Γ-model is the same for MLPs and convolutional neural networks. The encoder in the Γ-model\nstill includes both the clean and the corrupted paths as in the full ladder.\n4\nExperiments\nWith the experiments with the MNIST and CIFAR-10 datasets, we wanted to compare our method\nto other semi-supervised methods but also show that we can attach the decoder both to a fully\nconnected MLP network and to a convolutional neural network, both of which were described in\nSection 3. We also wanted to compare the performance of the simpler Γ-model (Sec. 3.4) to the\nfull Ladder network and experimented with only having a cost function on the input layer. With\nCIFAR-10, we only tested the Γ-model.\nWe also measured the performance of the supervised baseline models which only included the en-\ncoder and the supervised cost function. In all cases where we compared these directly with Ladder\nnetworks, we did our best to optimize the hyperparameters and regularization of the baseline super-\nvised learning models so that any improvements could not be explained, for example, by the lack of\nsuitable regularization which would then have been provided by the denoising costs.\nWith convolutional networks, our focus was exclusively on semi-supervised learning. The super-\nvised baselines for all labels only intend to show that the performance of the selected network ar-\nchitectures is in line with the ones reported in the literature. We make claims neither about the\noptimality nor the statistical signiﬁcance of these baseline results.\nWe used the Adam optimization algorithm (Kingma and Ba, 2015) for the weight updates. The\nlearning rate was 0.002 for the ﬁrst part of the learning, followed by an annealing phase during\nwhich the learning rate was linearly reduced to zero. The minibatch size was 100. The source\ncode for all the experiments is available at https://github.com/arasmus/ladder unless\nexplicitly noted in the text.\n4.1\nMNIST dataset\nFor evaluating semi-supervised learning, we used the standard 10,000 test samples as a held-out test\nset and randomly split the standard 60,000 training samples into a 10,000-sample validation set and\nused M = 50, 000 samples as the training set. From the training set, we randomly chose N = 100,\n1000, or all labels for the supervised cost.4 All the samples were used for the decoder, which does not\nneed the labels. The validation set was used for evaluating the model structure and hyperparameters.\nWe also balanced the classes to ensure that no particular class was over-represented. We repeated\neach training 10 times, varying the random seed that was used for the splits.\n4In all the experiments, we were careful not to optimize any parameters, hyperparameters, or model choices\non the basis of the results on the held-out test samples. As is customary, we used 10,000 labeled validation\nsamples even for those settings where we only used 100 labeled samples for training. Obviously, this is not\nsomething that could be done in a real case with just 100 labeled samples. However, MNIST classiﬁcation is\nsuch an easy task, even in the permutation invariant case, that 100 labeled samples there correspond to a far\ngreater number of labeled samples in many other datasets.\n9\nTable 1: A collection of previously reported MNIST test errors in the permutation invariant setting\nfollowed by the results with the Ladder network. * = SVM. Standard deviation in parentheses.\nTest error % with # of used labels\n100\n1000\nAll\nSemi-sup. Embedding (Weston et al., 2012)\n16.86\n5.73\n1.5\nTransductive SVM (from Weston et al., 2012)\n16.81\n5.38\n1.40*\nMTC (Rifai et al., 2011)\n12.03\n3.64\n0.81\nPseudo-label (Lee, 2013)\n10.49\n3.46\nAtlasRBF (Pitelis et al., 2014)\n8.10 (± 0.95)\n3.68 (± 0.12)\n1.31\nDGN (Kingma et al., 2014)\n3.33 (± 0.14)\n2.40 (± 0.02)\n0.96\nDBM, Dropout (Srivastava et al., 2014)\n0.79\nAdversarial (Goodfellow et al., 2015)\n0.78\nVirtual Adversarial (Miyato et al., 2015)\n2.12\n1.32\n0.64 (± 0.03)\nBaseline: MLP, BN, Gaussian noise\n21.74 (± 1.77)\n5.70 (± 0.20)\n0.80 (± 0.03)\nΓ-model (Ladder with only top-level cost)\n3.06 (± 1.44)\n1.53 (± 0.10)\n0.78 (± 0.03)\nLadder, only bottom-level cost\n1.09 (±0.32)\n0.90 (± 0.05)\n0.59 (± 0.03)\nLadder, full\n1.06 (± 0.37)\n0.84 (± 0.08)\n0.57 (± 0.02)\nAfter optimizing the hyperparameters, we performed the ﬁnal test runs using all the M = 60, 000\ntraining samples with 10 different random initializations of the weight matrices and data splits. We\ntrained all the models for 100 epochs followed by 50 epochs of annealing. With minibatch size of\n100, this amounts to 75,000 weight updates for the validation runs and 90,000 for the ﬁnal test runs.\n4.1.1\nFully connected MLP\nA useful test for general learning algorithms is the permutation invariant MNIST classiﬁcation task.\nPermutation invariance means that the results need to be invariant with respect to permutation of the\nelements of the input vector. In other words, one is not allowed to use prior information about the\nspatial arrangement of the input pixels. This excludes, among others, convolutional networks and\ngeometric distortions of the input images.\nWe chose the layer sizes of the baseline model to be 784-1000-500-250-250-250-10. The network\nis deep enough to demonstrate the scalability of the method but does not yet represent overkill for\nMNIST.\nThe hyperparameters we tuned for each model are the noise level that is added to the inputs and to\neach layer, and the denoising cost multipliers λ(l). We also ran the supervised baseline model with\nvarious noise levels. For models with just one cost multiplier, we optimized them with a search grid\n{. . ., 0.1, 0.2, 0.5, 1, 2, 5, 10, . . .}. Ladder networks with a cost function on all their layers have a\nmuch larger search space and we explored it much more sparsely. For instance, the optimal model\nwe found for N = 100 labels had λ(0) = 1000, λ(1) = 10, and λ(≥2) = 0.1. A good value for the\nstd of the Gaussian corruption noise n(l) was mostly 0.3 but with N = 1000 labels, 0.2 was a better\nvalue. For the complete set of selected denoising cost multipliers and other hyperparameters, please\nrefer to the code.\nThe results presented in Table 1 show that the proposed method outperforms all the previously\nreported results. Encouraged by the good results, we also tested with N = 50 labels and got a test\nerror of 1.62 % (± 0.65 %).\nThe simple Γ-model also performed surprisingly well, particularly for N = 1000 labels. With\nN = 100 labels, all the models sometimes failed to converge properly. With bottom level or full\ncosts in Ladder, around 5 % of runs result in a test error of over 2 %. In order to be able to estimate\nthe average test error reliably in the presence of such random outliers, we ran 40 instead of 10 test\nruns with random initializations.\n10\nTable 2: CNN results for MNIST\nTest error without data augmentation % with # of used labels\n100\nall\nEmbedCNN (Weston et al., 2012)\n7.75\nSWWAE (Zhao et al., 2015)\n9.17\n0.71\nBaseline: Conv-Small, supervised only\n6.43 (± 0.84)\n0.36\nConv-FC\n0.99 (± 0.15)\nConv-Small, Γ-model\n0.89 (± 0.50)\n4.1.2\nConvolutional networks\nWe tested two convolutional networks for the general MNIST classiﬁcation task but omitted data\naugmentation such as geometric distortions. We focused on the 100-label case since with more\nlabels the results were already so good even in the more difﬁcult permutation invariant task.\nThe ﬁrst network was a straightforward extension of the fully connected network tested in the per-\nmutation invariant case. We turned the ﬁrst fully connected layer into a convolution with 26-by-26\nﬁlters, resulting in a 3-by-3 spatial map of 1000 features. Each of the nine spatial locations was\nprocessed independently by a network with the same structure as in the previous section, ﬁnally re-\nsulting in a 3-by-3 spatial map of 10 features. These were pooled with a global mean-pooling layer.\nEssentially we thus convolved the image with the complete fully connected network. Depooling on\nthe topmost layer and deconvolutions on the layers below were implemented as described in Sec-\ntion 3.4. Since the internal structure of each of the nine almost independent processing paths was\nthe same as in the permutation invariant task, we used the same hyperparameters that were optimal\nfor the permutation invariant task. In Table 2, this model is referred to as Conv-FC.\nWith the second network, which was inspired by ConvPool-CNN-C from Springenberg et al. (2014),\nwe only tested the Γ-model. The MNIST classiﬁcation task can typically be solved with a smaller\nnumber of parameters than CIFAR-10, for which this topology was originally developed, so we\nmodiﬁed the network by removing layers and reducing the number of parameters in the remaining\nlayers. In addition, we observed that adding a small fully connected layer with 10 neurons on top\nof the global mean pooling layer improved the results in the semi-supervised task. We did not tune\nother parameters than the noise level, which was chosen from {0.3, 0.45, 0.6} using the validation\nset. The exact architecture of this network is detailed in Table 4 in Appendix A. It is referred to as\nConv-Small since it is a smaller version of the network used forthe CIFAR-10 dataset.\nThe results in Table 2 conﬁrm that even the single convolution on the bottom level improves the\nresults over the fully connected network. More convolutions improve the Γ-model signiﬁcantly,\nalthough the high variance of the results suggests that the model still suffers from conﬁrmation bias.\nThe Ladder network with denoising targets on every level converges much more reliably. Taken\ntogether, these results suggest that combining the generalization ability of convolutional networks5\nand efﬁcient unsupervised learning of the full Ladder network would have resulted in even better\nperformance but this was left for future work.\n4.2\nConvolutional networks on CIFAR-10\nThe CIFAR-10 dataset consists of small 32-by-32 RGB images from 10 classes. There are 50,000\nlabeled samples for training and 10,000 for testing. Like the MNIST dataset, it has been used for\ntesting semi-supervised learning so we decided to test the simple Γ-model with a convolutional\nnetwork that has been reported to perform well in the standard supervised setting with all labels.\nWe tested a few model architectures and selected ConvPool-CNN-C by Springenberg et al. (2014).\nWe also evaluated the strided convolutional version by Springenberg et al. (2014), and while it\nperformed well with all labels, we found that the max-pooling version overﬁtted less with fewer\nlabels, and thus used it.\n5In general, convolutional networks excel in the MNIST classiﬁcation task. The performance of the fully\nsupervised Conv-Small with all labels is in line with the literature and is provided as a rough reference only\n(only one run, no attempts to optimize, not available in the code package).\n11\nTable 3: Test results for CNN on CIFAR-10 dataset without data augmentation\nTest error % with # of used labels\n4 000\nAll\nAll-Convolutional ConvPool-CNN-C (Springenberg et al., 2014)\n9.31\nSpike-and-Slab Sparse Coding (Goodfellow et al., 2012)\n31.9\nBaseline: Conv-Large, supervised only\n23.33 (± 0.61)\n9.27\nConv-Large, Γ-model\n20.40 (± 0.47)\nThe main differences to ConvPool-CNN-C are the use of Gaussian noise instead of dropout and the\nconvolutional per-channel batch normalization following Ioffe and Szegedy (2015). While dropout\nwas useful with all labels, it did not seem to offer any advantage over additive Gaussian noise with\nfewer labels. For a more detailed description of the model, please refer to model Conv-Large in\nTable 4.\nWhile testing the purely supervised model performance with a limited number of labeled samples\n(N = 4000), we found out that the model overﬁtted quite severely: the training error for most sam-\nples decreased so much that the network effectively learned nothing from them as the network was\nalready very conﬁdent about their classiﬁcation. The network was equally conﬁdent about valida-\ntion samples even when they were misclassiﬁed. We noticed that we could regularize the network\nby stripping away the scaling parameter β(L) from the last layer. This means that the variance of the\ninput to the softmax is restricted to unity. We also used this setting with the corresponding Γ-model\nalthough the denoising target already regularizes the network signiﬁcantly and the improvement was\nnot as pronounced.\nThe hyperparameters (noise level, denoising cost multipliers, and number of epochs) for all models\nwere optimized using M = 40, 000 samples for training and the remaining 10, 000 samples for\nvalidation. After the best hyperparameters were selected, the ﬁnal model was trained with these\nsettings on all the M = 50, 000 samples. All experiments were run with with four different random\ninitializations of the weight matrices and data splits. We applied global contrast normalization and\nwhitening following Goodfellow et al. (2013b), but no data augmentation was used.\nThe results are shown in Table 3. The supervised reference was obtained with a model closer to the\noriginal ConvPool-CNN-C in the sense that dropout rather than additive Gaussian noise was used\nfor regularization.6 We spent some time tuning the regularization of our fully supervised baseline\nmodel for N = 4000 labels and indeed, its results exceed the previous state of the art. This tuning\nwas important to make sure that the improvement offered by the denoising target of the Γ-model is\nnot a sign of a poorly regularized baseline model. Although the improvement is not as dramatic as\nwith the MNIST experiments, it came with a very simple addition to standard supervised training.\n5\nRelated Work\nEarly works on semi-supervised learning (McLachlan, 1975; Titterington et al., 1985) proposed an\napproach where inputs x are ﬁrst assigned to clusters, and each cluster has its class label. Unlabeled\ndata would affect the shapes and sizes of the clusters, and thus alter the classiﬁcation result. This\napproach can be reinterpreted as input vectors being corrupted copies ˜x of the ideal input vectors x\n(the cluster centers), and the classiﬁcation mapping being split into two parts: ﬁrst denoising ˜x into\nx (possibly probabilistically), and then labeling x.\nIt is well known (see, e.g., Zhang and Oles, 2000) that when a probabilistic model that directly\nestimates P(y | x) is being trained, unlabeled data cannot help. One way to study this is to assign\nprobabilistic labels q(y(n)) = P(y(n) | x(n)) to unlabeled inputs x(n) and try to train P(y | x)\nusing those labels: it can be shown (see, e.g., Raiko et al., 2015, Eq. (31)) that the gradient will\nvanish. There are different ways of circumventing this phenomenon by adjusting the assigned labels\nq(y(n)). These are all related to the Γ-model.\n6Same caveats hold for this fully supervised reference result for all labels as with MNIST: only one run, no\nattempts to optimize, not available in the code package.\n12\nLabel propagation methods (Szummer and Jaakkola, 2003) estimate P(y | x), but adjust probabilis-\ntic labels q(y(n)) on the basis of the assumption that the nearest neighbors are likely to have the\nsame label. The labels start to propagate through regions with high-density P(x). The Γ-model\nimplicitly assumes that the labels are uniform in the vicinity of a clean input since corrupted inputs\nneed to produce the same label. This produces a similar effect: the labels start to propagate through\nregions with high density P(x). Weston et al. (2012) explored deep versions of label propagation.\nCo-training (Blum and Mitchell, 1998) assumes we have multiple views on x, say x = (x(1), x(2)).\nWhen we train classiﬁers for the different views, we know that even for the unlabeled data, the true\nlabel is the same for each view. Each view produces its own probabilistic labeling q(j)(y(n)) =\nP(y(n) | x(n)(j)) and their combination q(y(n)) can be fed to train the individual classiﬁers. If we\ninterpret having several corrupted copies of an input as different views on it, we see the relationship\nto the proposed method.\nLee (2013) adjusts the assigned labels q(y(n)) by rounding the probability of the most likely class\nto one and others to zero. The training starts by trusting only the true labels and then gradually\nincreasing the weight of the so-called pseudo-labels. Similar scheduling could be tested with our\nΓ-model as it seems to suffer from conﬁrmation bias. It may well be that the denoising cost which\nis optimal at the beginning of the learning is smaller than the optimal one at later stages of learning.\nDosovitskiy et al. (2014) pre-train a convolutional network with unlabeled data by treating each\nclean image as its own class. During training, the image is corrupted by transforming its location,\nscaling, rotation, contrast, and color. This helps to ﬁnd features that are invariant to the transfor-\nmations that are used. Discarding the last classiﬁcation layer and replacing it with a new classiﬁer\ntrained on real labeled data leads to surprisingly good experimental results.\nThere is an interesting connection between our Γ-model and the contractive cost used by Rifai et al.\n(2011): a linear denoising function ˆz(L)\ni\n= ai˜z(L)\ni\n+ bi, where ai and bi are parameters, turns the\ndenoising cost into a stochastic estimate of the contractive cost.\nRecently Miyato et al. (2015) achieved impressive results with a regularization method that is similar\nto the idea of contractive cost. They required the output of the network to change as little as possible\nclose to the input samples. As this requires no labels, they were able to use unlabeled samples for\nregularization. While their semi-supervised results were not as good as ours with a denoising target\non the input layer, their results with full labels come very close. Their cost function is on the last\nlayer which suggests that the approaches are complementary and could be combined, potentially\nimproving the results further.\nSo far we have reviewed semi-supervised methods which have an unsupervised cost function on\nthe output layer only and therefore are related to our Γ-model. We will now move to other semi-\nsupervised methods that concentrate on modeling the joint distribution of the inputs and the labels.\nThe Multi-prediction deep Boltzmann machine (MP-DBM) (Goodfellow et al., 2013a) is a way\nto train a DBM with backpropagation through variational inference. The targets of the inference\ninclude both supervised targets (classiﬁcation) and unsupervised targets (reconstruction of missing\ninputs) that are used in training simultaneously. The connections through the inference network\nare somewhat analogous to our lateral connections. Speciﬁcally, there are inference paths from\nobserved inputs to reconstructed inputs that do not go all the way up to the highest layers. Compared\nto our approach, MP-DBM requires an iterative inference with some initialization for the hidden\nactivations, whereas in our case, the inference is a simple single-pass feedforward procedure.\nThe Deep AutoRegressive Network (Gregor et al., 2014) is an unsupervised method for learning\nrepresentations that also uses lateral connections in the hidden representations. The connectivity\nwithin the layer is rather different from ours, though: each unit hi receives input from the preceding\nunits h1 . . . hi−1, whereas in our case each unit ˆzi receives input only from zi. Their learning\nalgorithm is based on approximating a gradient of a description length measure, whereas we use a\ngradient of a simple loss function.\nKingma et al. (2014) proposed deep generative models for semi-supervised learning, based on vari-\national autoencoders. Their models can be trained with the variational EM algorithm, stochastic\ngradient variational Bayes, or stochastic backpropagation. They also experimented on a stacked\nversion (called M1+M2) where the bottom autoencoder M1 reconstructs the input data, and the top\n13\nautoencoder M2 can concentrate on classiﬁcation and on reconstructing only the hidden representa-\ntion of M1. The stacked version performed the best, hinting that it might be important not to carry\nall the information up to the highest layers. Compared with the Ladder network, an interesting point\nis that the variational autoencoder computes the posterior estimate of the latent variables with the\nencoder alone while the Ladder network uses the decoder too to compute an implicit posterior ap-\nproximate (the encoder provides the likelihood part, which gets combined with the prior). It will be\ninteresting to see whether the approaches can be combined. A Ladder-style decoder might provide\nthe posterior and another decoder could then act as the generative model of variational autoencoders.\nZeiler et al. (2011) train deep convolutional autoencoders in a manner comparable to ours. They\ndeﬁne max-pooling operations in the encoder to feed the max function upwards to the next layer,\nwhile the argmax function is fed laterally to the decoder. The network is trained one layer at a\ntime using a cost function that includes a pixel-level reconstruction error, and a regularization term\nto promote sparsity. Zhao et al. (2015) use a similar structure and call it the stacked what-where\nautoencoder (SWWAE). Their network is trained simultaneously to minimize a combination of the\nsupervised cost and reconstruction errors on each level, just like ours.\nRecently Bengio (2014) proposed target propagation as an alternative to backpropagation. The idea\nis to base learning not on errors and gradients but on expectations. This is very similar to the\nidea of denoising source separation and therefore resembles the propagation of expectations in the\ndecoder of the Ladder network. In the Ladder network, the additional lateral connections between\nthe encoder and the decoder play an important role and it remains to be seen whether the lateral\nconnections are compatible with target propagation. Nevertheless, it is an interesting possibility that\nwhile the Ladder network includes two mechanisms for propagating information, backpropagation\nof gradients and forward propagation of expectations in the decoder, it may be possible to rely solely\non the latter, thus avoiding problems related to the propagation of gradients through many layers,\nsuch as exploding gradients.\n6\nDiscussion\nWe showed how a simultaneous unsupervised learning task improves CNN and MLP networks\nreaching the state of the art in various semi-supervised learning tasks. In particular, the perfor-\nmance obtained with very small numbers of labels is much better than previous published results,\nwhich shows that the method is capable of making good use of unsupervised learning. However,\nthe same model also achieves state-of-the-art results and a signiﬁcant improvement over the base-\nline model with full labels in permutation invariant MNIST classiﬁcation, which suggests that the\nunsupervised task does not disturb supervised learning.\nThe proposed model is simple and easy to implement with many existing feedforward architectures,\nas the training is based on backpropagation from a simple cost function. It is quick to train and the\nconvergence is fast, thanks to batch normalization.\nNot surprisingly, the largest improvements in performance were observed in models which have a\nlarge number of parameters relative to the number of available labeled samples. With CIFAR-10,\nwe started with a model which was originally developed for a fully supervised task. This has the\nbeneﬁt of building on existing experience but it may well be that the best results will be obtained\nwith models which have far more parameters than fully supervised approaches could handle.\nAn obvious future line of research will therefore be to study what kind of encoders and decoders are\nbest suited to the Ladder network. In this work, we made very small modiﬁcations to the encoders,\nwhose structure has been optimized for supervised learning, and we designed the parametrization of\nthe vertical mappings of the decoder to mirror the encoder: the ﬂow of information is just reversed.\nThere is nothing preventing the decoder from having a different structure than the encoder.\nAn interesting future line of research will be the extension of the Ladder networks to the temporal\ndomain. While datasets with millions of labeled samples for still images exist, it is prohibitively\ncostly to label thousands of hours of video streams. The Ladder networks can be scaled up easily\nand therefore offer an attractive approach for semi-supervised learning in such large-scale problems.\n14\nAcknowledgements\nWe have received comments and help from a number of colleagues who all deserve to be mentioned\nbut we wish to thank especially Yann LeCun, Diederik Kingma, Aaron Courville, Ian Goodfellow,\nSøren Sønderby, Jim Fan, and Hugo Larochelle for their helpful comments and suggestions. The\nsoftware for the simulations for this paper was based on Theano (Bastien et al., 2012; Bergstra et al.,\n2010) and Blocks (van Merri¨enboer et al., 2015). We also acknowledge the computational resources\nprovided by the Aalto Science-IT project. The Academy of Finland has supported Tapani Raiko.\nReferences\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N.,\nand Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and\nUnsupervised Feature Learning NIPS 2012 Workshop.\nBengio, Y. (2014). How auto-encoders could provide credit assignment in deep networks via target\npropagation. arXiv:1407.7906.\nBengio, Y., Yao, L., Alain, G., and Vincent, P. (2013). Generalized denoising auto-encoders as\ngenerative models. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages\n899–907.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-\nFarley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In\nProceedings of the Python for Scientiﬁc Computing Conference (SciPy 2010). Oral Presentation.\nBlum, A. and Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In\nProc. of the Eleventh Annual Conference on Computational Learning Theory (COLT ’98), pages\n92–100.\nChapelle, O., Sch¨olkopf, B., Zien, A., et al. (2006). Semi-supervised learning. MIT Press.\nDosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T. (2014). Discriminative unsuper-\nvised feature learning with convolutional neural networks. In Advances in Neural Information\nProcessing Systems 27 (NIPS 2014), pages 766–774.\nGoodfellow, I., Bengio, Y., and Courville, A. C. (2012). Large-scale feature learning with spike-\nand-slab sparse coding. In Proc. of ICML 2012, pages 1439–1446.\nGoodfellow, I., Mirza, M., Courville, A., and Bengio, Y. (2013a). Multi-prediction deep Boltzmann\nmachines. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 548–\n556.\nGoodfellow, I., Shlens, J., and Szegedy, C. (2015). Explaining and harnessing adversarial examples.\nIn the International Conference on Learning Representations (ICLR 2015). arXiv:1412.6572.\nGoodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013b). Maxout\nnetworks. In Proc. of ICML 2013.\nGregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep autoregressive\nnetworks. In Proc. of ICML 2014, Beijing, China.\nHinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786), 504–507.\nIoffe, S. and Szegedy, C. (2015).\nBatch normalization: Accelerating deep network training by\nreducing internal covariate shift.\nIn International Conference on Machine Learning (ICML),\npages 448–456.\nKingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. In the International\nConference on Learning Representations (ICLR 2015), San Diego. arXiv:1412.6980.\nKingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. (2014). Semi-supervised learning\nwith deep generative models. In Advances in Neural Information Processing Systems 27 (NIPS\n2014), pages 3581–3589.\nLee, D.-H. (2013). Pseudo-label: The simple and efﬁcient semi-supervised learning method for\ndeep neural networks. In Workshop on Challenges in Representation Learning, ICML 2013.\n15\nMcLachlan, G. (1975). Iterative reclassiﬁcation procedure for constructing an asymptotically opti-\nmal rule of allocation in discriminant analysis. J. American Statistical Association, 70, 365–369.\nMiyato, T., Maeda, S., Koyama, M., Nakae, K., and Ishii, S. (2015). Distributional smoothing by\nvirtual adversarial examples. arXiv:1507.00677.\nPezeshki, M., Fan, L., Brakel, P., Courville, A., and Bengio, Y. (2015). Deconstructing the ladder\nnetwork architecture. arXiv:1511.06430.\nPitelis, N., Russell, C., and Agapito, L. (2014). Semi-supervised learning using an unsupervised\natlas. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2014), pages\n565–580. Springer.\nRaiko, T., Berglund, M., Alain, G., and Dinh, L. (2015). Techniques for learning binary stochastic\nfeedforward neural networks. In ICLR 2015, San Diego.\nRanzato, M. A. and Szummer, M. (2008). Semi-supervised learning of compact document repre-\nsentations with deep networks. In Proc. of ICML 2008, pages 792–799. ACM.\nRasmus, A., Raiko, T., and Valpola, H. (2015a). Denoising autoencoder with modulated lateral\nconnections learns invariant representations of natural images. arXiv:1412.7210.\nRasmus, A., Valpola, H., and Raiko, T. (2015b). Lateral connections in denoising autoencoders\nsupport supervised learning. arXiv:1504.08215.\nRifai, S., Dauphin, Y. N., Vincent, P., Bengio, Y., and Muller, X. (2011). The manifold tangent\nclassiﬁer. In Advances in Neural Information Processing Systems 24 (NIPS 2011), pages 2294–\n2302.\nS¨arel¨a, J. and Valpola, H. (2005). Denoising source separation. JMLR, 6, 233–272.\nSietsma, J. and Dow, R. J. (1991).\nCreating artiﬁcial neural networks that generalize.\nNeural\nnetworks, 4(1), 67–79.\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. A. (2014). Striving for simplicity:\nThe all convolutional net. arxiv:1412.6806.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A\nsimple way to prevent neural networks from overﬁtting. JMLR, 15(1), 1929–1958.\nSuddarth, S. C. and Kergosien, Y. (1990). Rule-injection hints as a means of improving network per-\nformance and learning time. In Proceedings of the EURASIP Workshop 1990 on Neural Networks,\npages 120–129. Springer.\nSzummer, M. and Jaakkola, T. (2003). Partially labeled classiﬁcation with Markov random walks.\nAdvances in Neural Information Processing Systems 15 (NIPS 2002), 14, 945–952.\nTitterington, D., Smith, A., and Makov, U. (1985). Statistical analysis of ﬁnite mixture distributions.\nIn Wiley Series in Probability and Mathematical Statistics. Wiley.\nValpola, H. (2015). From neural PCA to deep unsupervised learning. In Adv. in Independent Com-\nponent Analysis and Learning Machines, pages 143–171. Elsevier. arXiv:1411.7783.\nvan Merri¨enboer, B., Bahdanau, D., Dumoulin, V., Serdyuk, D., Warde-Farley, D., Chorowski, J.,\nand Bengio, Y. (2015). Blocks and fuel: Frameworks for deep learning. CoRR, abs/1506.00619.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising criterion.\nJMLR, 11, 3371–3408.\nWeston, J., Ratle, F., Mobahi, H., and Collobert, R. (2012). Deep learning via semi-supervised\nembedding. In Neural Networks: Tricks of the Trade, pages 639–655. Springer.\nZeiler, M. D., Taylor, G. W., and Fergus, R. (2011). Adaptive deconvolutional networks for mid and\nhigh level feature learning. In ICCV 2011, pages 2018–2025. IEEE.\nZhang, T. and Oles, F. (2000). The value of unlabeled data for classiﬁcation problems. In Proc. of\nICML 2000, pages 1191–1198.\nZhao, J., Mathieu, M., Goroshin, R., and Lecun, Y. (2015). Stacked what-where auto-encoders.\narXiv:1506.02351.\n16\nA\nSpeciﬁcation of the convolutional models\nTable 4: ConvPool-CNN-C by Springenberg et al. (2014) and our networks based on it.\nModel\nConvPool-CNN-C\nConv-Large (for CIFAR-10)\nConv-Small (for MNIST)\nInput 32 × 32 or 28 × 28 RGB or monochrome image\n3 × 3 conv. 96 ReLU\n3 × 3 conv. 96 BN LeakyReLU\n5 × 5 conv. 32 ReLU\n3 × 3 conv. 96 ReLU\n3 × 3 conv. 96 BN LeakyReLU\n3 × 3 conv. 96 ReLU\n3 × 3 conv. 96 BN LeakyReLU\n3 × 3 max-pooling stride 2\n2 × 2 max-pooling stride 2 BN\n2 × 2 max-pooling stride 2 BN\n3 × 3 conv. 192 ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 conv. 64 BN ReLU\n3 × 3 conv. 192 ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 conv. 64 BN ReLU\n3 × 3 conv. 192 ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 max-pooling stride 2\n2 × 2 max-pooling stride 2 BN\n2 × 2 max-pooling stride 2 BN\n3 × 3 conv. 192 ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 conv. 128 BN ReLU\n1 × 1 conv. 192 ReLU\n1 × 1 conv. 192 BN LeakyReLU\n1 × 1 conv. 10 ReLU\n1 × 1 conv. 10 BN LeakyReLU\n1 × 1 conv. 10 BN ReLU\nglobal meanpool\nglobal meanpool BN\nglobal meanpool BN\nfully connected 10 BN\n10-way softmax\nHere we describe two model structures, Conv-Small and Conv-Large, that were used for the MNIST\nand CIFAR-10 datasets, respectively. They were both inspired by ConvPool-CNN-C by Springen-\nberg et al. (2014). Table 4 details the model architectures and differences between the models in this\nwork and ConvPool-CNN-C. It is noteworthy that this architecture does not use any fully connected\nlayers, but replaces them with a global mean pooling layer just before the softmax function. The\nmain differences between our models and ConvPool-CNN-C are the use of Gaussian noise instead of\ndropout and the convolutional per-channel batch normalization following Ioffe and Szegedy (2015).\nWe also used 2x2 stride 2 max-pooling instead of 3x3 stride 2 max-pooling. LeakyReLU was used\nto speed up training, as mentioned by Springenberg et al. (2014). We utilized batch normalization\nto all layers, including the pooling layers. Gaussian noise was also added to all layers, instead of\napplying dropout in only some of the layers as with ConvPool-CNN-C.\nB\nFormulation of the Denoising Function\nThe denoising function g tries to map the clean z(l) to the reconstructed ˆz(l), where ˆz(l) =\ng(˜z(l), ˆz(l+1)). The reconstruction is therefore based on the corrupted value and the reconstruction\nof the layer above.\nAn optimal functional form of g depends on the conditional distribution p(z(l) | z(l+1)) that we\nwant the model to be able to denoise. For example, if the distribution p(z(l) | z(l+1)) is Gaussian,\nthe optimal function g, that is, the function that achieves the lowest reconstruction error, is going to\nbe linear with respect to ˜z(l) (Valpola, 2015, Section 4.1). This is the parametrization that we chose\non the basis of preliminary comparisons of different denoising function parametrizations.\nThe proposed parametrization of the denoising function was therefore:\ng(˜z, u) = (˜z −µ(u)) υ(u) + µ(u) .\n(4)\nWe modeled both µ(u) and υ(u) with an expressive nonlinearity 7: µ(u) = a1sigmoid(a2u+a3)+\na4u + a5 and υ(u) = a6sigmoid(a7u + a8) + a9u + a10. We have left out the superscript (l) and\nsubscript i in order not to clutter the equations. Given u, this parametrization is linear with respect\nto ˜z, and both the slope and the bias depended nonlinearly on u.\nIn order to test whether the elements of the proposed function g were necessary, we systematically\nremoved components from g or replaced g altogether and compared the resulting performance to\n7The parametrization can also be interpreted as a miniature MLP network\n17\nthe results obtained with the original parametrization. We tuned the hyperparameters of each com-\nparison model separately using a grid search over some of the relevant hyperparameters. However,\nthe standard deviation of additive Gaussian corruption noise was set to 0.3. This means that the\ncomparison does not include the best-performing models reported in Table 1 that achieved the best\nvalidation errors after more careful hyperparameter tuning.\nAs in the proposed function g, all comparison denoising functions mapped neuron-wise the cor-\nrupted hidden layer pre-activation ˜z(l) to the reconstructed hidden layer activation given one projec-\ntion from the reconstruction of the layer above: ˆz(l)\ni\n= g(˜z(l)\ni , u(l)\ni ).\nTest error % with # of used labels\n100\n1000\nProposed g: Gaussian z\n1.06 (± 0.07)\n1.03 (± 0.06)\nComparison g1: miniature MLP with ˜zu\n1.11 (± 0.07)\n1.11 (± 0.06)\nComparison g2: No augmented term ˜zu\n2.03 (± 0.09)\n1.70 (± 0.08)\nComparison g3: Linear g but with ˜zu\n1.49 (± 0.10)\n1.30 (± 0.08)\nComparison g4: Only the mean depends on u\n2.90 (± 1.19)\n2.11 (± 0.45)\nTable 5: Semi-supervised results from the MNIST dataset. The proposed function g is compared\nto alternative parametrizations. Note that the hyperparameter search was not as exhaustive as in\nthe ﬁnal results, which means that the results of the proposed model deviate slightly from the ﬁnal\nresults presented in Table 1.\nThe comparison functions g1...4 are parametrized as follows:\nComparison g1: Miniature MLP with ˜zu\nˆz = g(˜z, u) = aξ + bsigmoid(cξ)\n(5)\nwhere ξ = [1, ˜z, u, ˜zu]T is an augmented input, a and c are trainable weight vectors, b is a trainable\nscalar weight. This parametrization is capable of learning denoising of several different distributions\nincluding sub- and super-Gaussian and bimodal distributions.\nComparison g2: No augmented term\ng2(˜z, u) = aξ′ + bsigmoid(cξ′)\n(6)\nwhere ξ′ = [1, ˜z, u]T . g2 therefore differs from g1 in that the input lacks the augmented term ˜zu.\nComparison g3: Linear g\ng3(˜z, u) = aξ.\n(7)\ng3 differs from g in that it is linear and does not have a sigmoid term. As this formulation is linear,\nit only supports Gaussian distributions. Although the parametrization has the augmented term that\nlets u modulate the slope and shift of the distribution, the scope of possible denoising functions is\nstill fairly limited.\nComparison g4: u affects only the mean of p(z | u)\ng4(˜z, u) = a1u + a2sigmoid(a3u + a4) + a5˜z + a6sigmoid(a7˜z + a8) + a9\n(8)\ng4 differs from g1 in that the inputs from u are not allowed to modulate the terms that depend on ˜z,\nbut that the effect is additive. This means that the parametrization only supports optimal denoising\nfunctions for a conditional distribution p(z | u) where u only shifts the mean of the distribution of z\nbut otherwise leaves the shape of the distribution intact.\nResults\nAll models were tested in a similar setting as the semi-supervised fully connected MNIST\ntask using N = 1000 labeled samples. We also reran the best comparison model on N = 100 labels.\nThe results of the analyses are presented in Table 5.\nAs can be seen from the table, the alternative parametrizations of g are inferior to the proposed\nparametrization, at least in the model structure we use.\n18\nThese results support the ﬁnding by Rasmus et al. (2015a) that modulation of the lateral connection\nfrom ˜z to ˆz by u is critical for encouraging the development of invariant representations at the higher\nlayers of the model. Comparison function g4 lacked this modulation and it clearly performed worse\nthan any other denoising function listed in Table 5. Even the linear g3 performed very well as long it\nhad the term ˜zu. Leaving the nonlinearity but removing ˜zu in g2 hurt the performance much more.\nIn addition to the alternative parametrizations for the g-function, we ran experiments using a more\nstandard autoencoder structure. In that structure, we attached an additional decoder to the standard\nMLP by using one hidden layer as the input to the decoder and the reconstruction of the clean input\nas the target. The structure of the decoder was set to be the same as the encoder: that is, the number\nand size of the layers from the input to the hidden layer where the decoder was attached were the\nsame as the number and size of the layers in the decoder. The ﬁnal activation function in the decoder\nwas set to be the sigmoid nonlinearity. During training, the target was the weighted sum of the\nreconstruction cost and the classiﬁcation cost.\nWe tested the autoencoder structure with 100 and 1000 labeled samples. We ran experiments for all\npossible decoder lengths: that is, we tried attaching the decoder to all the hidden layers. However,\nwe did not manage to get a signiﬁcantly better performance than the standard supervised model\nwithout any decoder in any of the experiments.\n19\n",
  "categories": [
    "cs.NE",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2015-07-09",
  "updated": "2015-11-24"
}