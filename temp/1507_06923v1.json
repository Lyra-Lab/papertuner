{
  "id": "http://arxiv.org/abs/1507.06923v1",
  "title": "A Reinforcement Learning Approach to Online Learning of Decision Trees",
  "authors": [
    "Abhinav Garlapati",
    "Aditi Raghunathan",
    "Vaishnavh Nagarajan",
    "Balaraman Ravindran"
  ],
  "abstract": "Online decision tree learning algorithms typically examine all features of a\nnew data point to update model parameters. We propose a novel alternative,\nReinforcement Learning- based Decision Trees (RLDT), that uses Reinforcement\nLearning (RL) to actively examine a minimal number of features of a data point\nto classify it with high accuracy. Furthermore, RLDT optimizes a long term\nreturn, providing a better alternative to the traditional myopic greedy\napproach to growing decision trees. We demonstrate that this approach performs\nas well as batch learning algorithms and other online decision tree learning\nalgorithms, while making significantly fewer queries about the features of the\ndata points. We also show that RLDT can effectively handle concept drift.",
  "text": "Reinforcement Learning Based Online Decision Trees\nA Reinforcement Learning Approach to Online Learning of\nDecision Trees\nAbhinav Garlapati ∗\nabhinavg@cse.iitm.ac.in\nAditi Raghunathan ∗\naditirag@cse.iitm.ac.in\nVaishnavh Nagarajan ∗\nvais@cse.iitm.ac.com\nBalaraman Ravindran\nravi@cse.iitm.ac.in\nDeparment of Computer Science and Engineering\nIndian Institute of Technology, Madras\nAbstract\nOnline decision tree learning algorithms typically examine all features of a new data point\nto update model parameters. We propose a novel alternative, Reinforcement Learning-\nbased Decision Trees (RLDT), that uses Reinforcement Learning (RL) to actively examine\na minimal number of features of a data point to classify it with high accuracy. Furthermore,\nRLDT optimizes a long term return, providing a better alternative to the traditional myopic\ngreedy approach to growing decision trees. We demonstrate that this approach performs as\nwell as batch learning algorithms and other online decision tree learning algorithms, while\nmaking signiﬁcantly fewer queries about the features of the data points. We also show that\nRLDT can eﬀectively handle concept drift.\nKeywords:\nDecision Trees, Online Learning, Reinforcement Learning\n1. Introduction\nDecision trees sequentially test the values of various features of a data point to report a class\nlabel. The feature to be inspected at a stage is a decision that depends on the results of the\ntests conducted previously. It is natural to perceive this problem as a sequential decision-\nmaking task, which can be solved using Reinforcement Learning (RL). More importantly,\nwhen compared to traditional decision tree learning, we will see how the diﬀerent constructs\nin RL oﬀer better control over multiple aspects of the learning algorithm.\nIn this work, we present RLDT, an RL-based online decision tree algorithm for classiﬁ-\ncation. Our objective is to design a learner that can achieve high accuracy while simulta-\nneously minimizing the amount of information about data points used for both predicting\nand learning. We show that learning such a minimal tree is equivalent to solving for the\noptimal policy of an unknown Markov Decision Process (MDP).\nA challenge in searching over the space of decision trees is the combinatorial complexity\nof the problem.\nTraditional algorithms address this by taking a greedy decision while\nsplitting any node (e.g., splitting along feature with highest information gain) to grow the\ntree. As shown by Garey and Graham (1974), this can lead to sub-optimal solutions. The\n∗. The authors contributed equally\n1\narXiv:1507.06923v1  [cs.LG]  24 Jul 2015\nGarlapati, Raghunathan, Nagarajan and Ravindran\ndecision making problem could be solved better by maximizing a long-term return rather\nthan a short term gain - which is a typical goal in RL.\nSometimes decision trees can have very few training points in their leaf nodes. This\noverﬁtting is usually avoided by pruning the tree.\nRLDT, however, does not have an\nexplicit pruning stage.\nThe RL formulation provides a neat way of striking a balance\nbetween pruning for generalization and growing deeper trees for higher accuracy.\nWe are interested in a learner that asks as few questions as possible about the features\nof the data point since data can be expensive. Furthermore, diﬀerent features could have\ndiﬀerent costs associated with their extraction. We would want to selectively and eﬃciently\nexamine values subject to these parameters. We will see that the reward formulation of the\nMDP can be naturally tuned to learn in this setup.\nOur interest in studying an RL based approach is also motivated by the fact that online\nclassiﬁcation models face the challenge of concept drift, where the optimal tree changes with\ntime. Online RL algorithms eﬃciently handle such situations with non-stationary optimal\nsolutions.\nFinally, we note that the above formulation is rich in that any of the vast variety of\nRL algorithms can be used to learn the optimal policy which forms our decision tree.\nThus RLDT promises diﬀerent techniques for online decision tree learning within a single\nframework.\nThe rest of the paper is organized as follows. In Section 2, we discuss related work in\nonline decision tree learning. Section 3 formally deﬁnes the problem while Section 4 and 5\npresent the MDP formulation in RLDT. We discuss experiments in Section 6.1 after which\nwe discuss possible directions for future work.\n2. Earlier Work\nTypically, online decision tree learning algorithms such as VFDT (Domingos and Hulten,\n2000) and ID4 (Utgoﬀ, 1989) try to incrementally learn the same tree that batch training\nalgorithms like C4.5 learn. They examine all the features of the input data point to update\ntheir models. However RLDT uses a minimal subset of these features to both classify a new\ndata point and to update its parameters.\nOur work is closely related to that of Hoi et al. (2012) in which the authors study an\nonline linear classiﬁcation algorithm that examines a ﬁxed number of features to classify\nan input point. As the authors note, the constraints of this algorithm are harder than that\nin sparse online learning (Duchi and Singer, 2009; Langford et al., 2009) that has no upper\nbound on the number of features examined. RLDT, in fact, imposes a harder combination\nof the constraints as the learner is encouraged to examine the fewest number of features,\nsubject to an upper bound.\nWe must note how RLDT diﬀers from earlier studies of RL-based decision tree learning.\nBonet and Geﬀner (1998) model the process of learning decision trees as a known partially\nobservable MDP where the states of the underlying MDP are the training sample points.\nThe learner decides to query a feature/report a label based on its belief over these states; the\nbelief encodes the knowledge about the point that has been acquired so far. Preda (2007)\napproach the problem by designing an MDP with states corresponding to every possible\n2\nReinforcement Learning Based Online Decision Trees\npartition of the training data. The action deﬁned at a state is any kind of test (a feature\nquery) that can be conducted on one of the subsets in that partition of the sample points.\nA serious drawback of the above approaches is that the solutions are very rigid with\nrespect to the training data. The algorithms fail to work in an incremental setup because\nthe state spaces change completely by the addition of a new point. RLDT on the other\nhand is diﬀerent in that it uses a state space representation that allows learning from\nnew data incrementally.\nWhile the algorithm presented by Hwang et al. (2006) uses a\nsimilar representation, it is designed for learning from an oﬄine batch of data points and\nan extension to online learning is not obvious. Furthermore, Hwang et al. (2006) propose a\nreward parametrization that is not concerned with minimizing the number of queries.\n3. Online Learning for Classiﬁcation with Query Costs\nWe now formally deﬁne the problem of learning to classify online by querying on features\nselectively. We assume that the input space is X = Zd and the output space is a set of\nlabels Y = {1, 2, . . . K}. We discuss an extension to continuous features in the next section.\nAt every timestep t = 1, 2, . . ., the environment draws an input ⃗xt = (x1, x2, . . . xd) ∈X\nand output yt ∈Y according to an underlying distribution D. The learner is initially not\ninformed of any of the attributes of ⃗xt. Instead, the learner interacts with the environment\nand actively informs itself about the point in two ways. First, it can either query the\nenvironment about the value of a feature with index j – which is denoted as executing\naction Fj. Secondly, the learner can report to the environment an answer ˆyt ∈Y which is\ndenoted as action Rˆyt. The environment then reveals the true label yt, provides a reward\ndepending on whether ˆyt = yt or not, and considers the episode (timestep) terminated. The\nobjective of the learner involves a) achieving a good performance in classiﬁcation and b)\nmaking few queries.\n4. MDP Formulation\nWe formulate the above problem as a Markov Decision Process M such that each episode\nof the MDP processes a new input point. The state space S of the MDP consists of all pos-\nsible states that encode partial information about the data point. This partial information\nconsists of the values of the features that are considered to be known at that state. Thus,\nevery state s ∈S corresponds to a set of known features f(s) which have a conﬁguration\nunique to s. Clearly, for the initial state s0 of the MDP, f(s0) = φ.\nThe action space A consists of the feature query actions Fi for i = 1, 2, . . . d and the\nreport actions Ri for i = 1, 2, . . . K. However, not all query actions are allowed on all states.\nConsider feature j ∈f(s). Fj is disallowed at s. Furthermore, on taking action Fi at this\nstate (i /∈f(s)) a transition is made to another state s′ that includes all partial information\npresent in s and also the information about the value of feature i. Thus f(s′) = f(s) ∪{i}.\nOn the other hand, taking action Rk leads us to a state where the label of the point is\nknown regardless of which report action was taken. While here we consider only queries\nthat result in as many splits as the number of values the feature can take, note that one\ncould also consider comparison queries that result in binary splits.\n3\nGarlapati, Raghunathan, Nagarajan and Ravindran\nThe rewards on the query actions Fi are negative and are proportional to the cost\n−Ci of querying feature i of the point ⃗xt. The reward on the report action Ri is however\ndependent on the actual label yt and the reported label i. If yt = i, the agent assumes that\nthe environment reinforces it with a positive reward, and a negative reward otherwise.\nIn order to discourage the agent from asking many queries we have to set non-zero query\ncosts (Ci > 0) or/and lower values of γ, the discount factor. While γ discourages querying\nindependent of the feature being queried, query costs provide us more ﬂexibility to encode\nthe actual cost associated with accessing a feature.\n5. Solving the MDP\nWe employ Q-learning to solve the MDP M formulated as above. We chose Q-learning (oﬀ-\npolicy method) over SARSA (on-policy method) because we perform multiple path updates\n(described in Section 5.2). The details of the process is described below and experimental\nresults follow.\n5.1 The reward for classiﬁcations and misclassiﬁcation\nWhen the true label is revealed after a report action, the value estimate of the report action\nfor the true label is updated with a target of R+ while the report action for each of the\nother labels is updated with a target of R−. At some state s, for a class k, assume that D is\nsuch that any point that is consistent with the partial information at that state belongs to\nclass k with probability p. Thus, we know that Q(s, Rk) converges to the expected return\nof\npR+ + (1 −p)R−.\nConsider a query action Fi at s for which state t is one of the possible subsequent states.\nGiven state t, it is likely that there exists a class k such that its density pk in state s, is\nless than its density p′\nk in t, since we have more information about the point in t. Thus,\np′\nk > pk. Taking action Fi and reporting k in state t results in an expected return of\n−Ci + γ(p′\nkR+ + (1 −p′\nk)R−).\nThe amount by which this expected return diﬀers from the expected reward on directly\ntaking Rk at state s would be\n−Ci + (γp′\nk −pk)R+ + (−1 + γ −γp′\nk + pk)R−.\nThe above quantity can be made both positive and negative depending on the parameters\nγ, Ci, R+, R−. This is precisely how the tradeoﬀbetween accuracy and number of queries is\naddressed in RLDT. Furthermore, carefully setting the values R+ and R−to be a function\nof the label being correctly/wrongly reported, we can enforce cost-sensitive learning, and\nhandle class imbalance. Class imbalance can also be addressed by setting the value of α to\nbe lower for the majority class thereby under-sampling the class.\n4\nReinforcement Learning Based Online Decision Trees\n5.2 Speeding up Convergence\nWe say that RLDT converges when it has seen suﬃciently many points beyond which the\nalgorithm’s performance does not change in expectation as it has converged to the optimal\npolicy/decision tree.\nIn this section, we discuss two techniques that will help in faster\nconvergence.\nMultiple Path Updates\nThe updates after every episode correspond to a speciﬁc order\nin which the queries were asked. This order corresponds to a single ‘path’ down the MDP.\nHowever, it is crucial to note that we could have made the same set of queries in any other\norder and eﬀected these updates in other paths. We could have also taken shorter paths\nthat made only a subset of these queries before reporting the label. As an example, in\nFigure 1, while the red edges correspond to the path that was taken, value updates can\nbe performed on all the action-values that have been shown. These updates must be done\nexactly once per state-action pair to avoid bias. We will see that this makes convergence\nsigniﬁcantly faster. We note that to avoid enumerating the exponential number of paths,\nwe could sample a constant number of random paths from the space of paths to perform\nthe updates. This would still be speed up convergence when compared to the naive single\npath update.\n(x1, x2, x3)\n(x1, 0, x3)\n(1, 0, x3)\n(1, x2, x3)\nClass +\nx1 =?\nx1 =?\nx2 =?\nx2 =?\nFigure 1: Paths along which updates must be made\nThis technique however results in more frequent updates in states where fewer features\nabout the point are considered to be known i.e., when |f(s)| is small. Thus the value of the\nbest report actions at these states converge faster than possibly better query actions. Since\nϵ-greedy exploration may not suﬃciently explore these query actions, we use optimistic\ninitial values for all the query actions.\nTruncated Exploration\nAssume that we skip taking an exploratory report action Ri at\na state s and choose to make a query instead. At the end of the episode we would always\ntake a report action and know the true label of the input point. By knowing the true label,\nwe would also know the outcome of action Ri that we skipped at state s. Thus, the report\nactions at s can be updated without exploring any of them. Hence, we restrict exploratory\nsteps to only the query actions.\n5\nGarlapati, Raghunathan, Nagarajan and Ravindran\n6. Experiment\n6.1 Mushroom Dataset\nSince it is suﬃcient to either have a low value of γ or a strictly positive query cost, we\nchoose to set γ = 0.8 and all query costs zero. The report rewards are set as R+ = 5\nand R−= −5. Apart from an exploration rate ϵ = 0.01, we initialize all query action\nvalues to an optimistic value of 8 (an arbitrarily chosen value that is slightly higher than\nthe maximum reward of 5). We allow the learner to make at most three queries in order\nto truncate the state space of the MDP. We show that under this restriction RLDT can\nperform better than other algorithms.\nFigure 2 shows the graphs corresponding to the performance of RLDT averaged across\n50 runs on the Mushroom Data Set 1. Note that it is the return, which is a function of\nboth accuracy and the number of queries, that the agent tries to optimize. We see that\nthe learner attains a high accuracy after examining at most 3 out of 22 features of the ﬁrst\n1000 points. Beyond this, the number of queries the agent makes drops to 1.1 queries per\npoint in expectation while maintaining an accuracy of 97.74% ± 3.01. This demonstrates\nthat the discount factor discourages querying without aﬀecting the accuracy undesirably.\nNote that all values are reported with their 95% conﬁdence intervals.\nOn the Nursery Data Set 2 we only present the moving averages to observe the eﬀect\nof the techniques discussed in Section 5.2. We see that multiple path updates speed up\nconvergence signiﬁcantly. Also, the truncated exploration technique results in many queries\nbeing asked on the ﬁrst few points as we would expect. However, a signiﬁcant diﬀerence\nin the convergence rate is not noticeable in this case.\nOn this data set, we also show\nthat increasing query cost reduces the number of queries made during learning (Figure\n4a), although reducing accuracy from 92.53 ± 1.22% to 88.5 ± 0.88%. We also observe\nthat when the number of allowed queries is limited, RLDT can perform better than even a\nbatch learning algorithm like C4.5 that learns from all features. On performing 5-fold cross\nvalidation, RLDT achieves an accuracy of 92.53 ± 1.22% with 2 feature queries per point\nin the testing phase. When C4.5 trees are restricted to a depth of 2 using Reduced Error\nPruning, the accuracy on the same ﬁve folds turns out to be 85.33 ± 0.87%.\nWe use the Electric Power Consumption Data Set (Harries and Wales, 1999) 3 to examine\nthe performance of RLDT under concept drift. As seen in Figure 4b, a suﬃcient learning\nrate α is required to maintain a tree that adapts to a changing distribution. RLDT performs\nbetter than most popular online decision tree learning algorithms (Table 1), while making\nsigniﬁcantly fewer queries.\n7. Future Extensions\nIn this section, we discuss possible ideas for developing RLDT to work with high-dimensional\nand continuous feature spaces.\nDealing with large state and action spaces\nThe MDP corresponding to high di-\nmensional data will have large state and action spaces. Limiting the number of queries\n1. Available at https://archive.ics.uci.edu/ml/datasets/Mushroom\n2. Available at https://archive.ics.uci.edu/ml/datasets/Nursery\n3. Continuous attributes were discretized.\n6\nReinforcement Learning Based Online Decision Trees\nMethod\nFinal Accuracy(%)\nMean Accuracy(%)\nStARMiner Tree(ST)\n81.3\n78.93\nAutomatic StARMiner Tree(AST)\n80.1\n79.15\nVFDT\n75.8\n74.64\nVFDTcNB\n77.3\n77.16\nRLDT\n83.26\n81.15\nTable 1: Performance of standard algorithms on Electric Power Consumption Data Set\n0\n1700\n3400\n5100\n6800\n8500\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n%age\nEpisodes\nAverage Accuracy\n \n \nAccuracy averaged over 50 trials\nMoving average over 100 points\n(a) Average Accuracy\n0\n1700\n3400\n5100\n6800\n8500\n0\n1\n2\n3\n4\n5\nReturn\nEpisodes\nAverage Return\n \n \nReturn averaged over 50 trials\nMoving average over 100 points\n(b) Average Return\n0\n1700\n3400\n5100\n6800\n8500\n0\n1\n2\n3\nNumber of Queries\nEpisodes\nAverage Number of Queries\n \n \nNo of Queries averaged over 50 trials\nMoving average over 100 points\n(c) Average no. of Queries\nFigure 2: Performance on Mushroom Dataset with Multiple Path Updates\n0\n2526\n5052\n7578\n10104\n12630\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n%age\nEpisodes\nAverage Accuracy\n \n \nNaive\nMultiple Path Updates\nMultiple Path updates + Trunc. Expl.\n(a) Average Accuracy\n0\n2526\n5052\n7578\n10104\n12630\n−5\n−3\n−1\n1\n3\n5\nReturn\nEpisodes\nAverage Return\n \n \nNaive\nMultiple Path Updates\nMultiple Path Updates + Trunc. Expl.\n(b) Average Return\n0\n2526\n5052\n7578\n10104\n12630\n0\n1\n2\n3\nNumber of Queries\nEpisodes\nAverage Number of Queries\n \n \nNaive\nMultiple Path Updates\nMultiple Path updates + Trunc. Expl.\n(c) Average No. of Queries\nFigure 3: Performance on Nursery Data Set\nis one solution to truncating a large state space. We could also dynamically prune the\nquery actions at a state if we observe that the actions lead to ‘equivalent’ states where the\nbest action is the same report action. The idea is to explicitly avoid exploration when not\nrequired.\nOn the other hand, we could use function approximation by encoding the states in terms\nof the features that are known/unknown at that state. However, eﬃcient generalization can\nbe done only with appropriate assumptions on the data distribution. For two states s and s′,\nif neither of f(s) and f(s′) is a subset of the other, the distribution in the part of the input\nspace corresponding to s and s′ may diﬀer too signiﬁcantly that we may not be able to draw\n7\nGarlapati, Raghunathan, Nagarajan and Ravindran\n0\n2526\n5052\n7578\n10104\n12630\n0\n1\n2\n3\n4\n5\nNumber of Queries\nEpisodes\nAverage Number of Queries\n \n \nQuery Cost = 0\nQuery Cost = -1\n(a) Varying Query Costs on\nNursery Data Set\n0\n0.9062\n1.8124\n2.7187\n3.6249\n4.5311\nx 10\n4\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n%age\nEpisodes\nAverage Accuracy\n \n \nNaive\nMultiple Path Upd. + decay alpha\nMultiple Path Upd. + const alpha\n(b)\nAverage\nAccuracy\non\nElectricity Data Set\nFigure 4\nexperience from s to s′ or vice versa. Nevertheless, we could still generalize if one of f(s)\nor f(s′) is a subset of the other. That is, we can always draw experience from ‘sub-states’\nwhere more feature values are known, to ‘super-states’ where less values are known. Two\nchallenges arise here. First, we will have to determine how much weight experiences from\ndiﬀerent sub-states are given. Second, we would want to design a function approximation\narchitecture that can (roughly) capture the overlapping experience between sub-states and\nsuper-states.\nIt is also important for the method to scale to large action spaces which might otherwise\nhinder techniques like Q-learning. Policy gradient versions of RLDT with appropriate policy\nparametrization to handle this would be an interesting direction for future work.\nContinuous attribute values\nOne way to handle continuous attribute values would\nbe to discretize the feature space into ﬁnitely many bins. Alternatively, we could directly\nemploy RL techniques that work with continuous action spaces. The value of splitting a\ncontinuous feature as a function of the split point is likely to be a continuous multimodal\ndistribution which can be estimated online using a Gaussian Mixture Model (Agostini and\nCelaya, 2010).\nAnother alternative would be to maintain a ﬁnite number of split points per feature,\neach of which is dynamically updated as described by Chickering et al. (2001). As the\nallowed set of actions changes gradually, the value estimates for the current set of actions\ncan be eﬀectively used to estimate values of a new set of actions.\n8. Conclusion\nWe have presented RLDT, an RL-based online algorithm that actively chooses to know very\nfew features of a data point to both classify it and to learn a better model. The framework\nof the algorithm is rich enough to handle diﬀerent settings such as class imbalance and\nnon-uniformly expensive features using appropriate parameter values. Furthermore, since\nwe can employ any RL algorithm to learn the optimal policy, RLDT promises multiple\nsolution methods to online decision tree learning that can be explored in the future.\n8\nReinforcement Learning Based Online Decision Trees\nReferences\nAlejandro Agostini and Enric Celaya.\nReinforcement learning with a gaussian mixture\nmodel. In International Joint Conference on Neural Networks, IJCNN 2010, pages 1–8,\n2010.\nBlai Bonet and Hector Geﬀner.\nLearning sorting and decision trees with pomdps.\nIn\nJude W. Shavlik, editor, Proceedings of the Fifteenth International Conference on Ma-\nchine Learning (ICML 1998), pages 73–81, 1998.\nDavid Maxwell Chickering, Christopher Meek, and Robert Rounthwaite. Eﬃcient deter-\nmination of dynamic split points in a decision tree. In Proceedings of the 2001 IEEE\nInternational Conference on Data Mining,, pages 91–98, 2001.\nPedro Domingos and GeoﬀHulten. Mining high-speed data streams. In Proceedings of\nthe Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, KDD ’00, pages 71–80, 2000.\nJohn Duchi and Yoram Singer. Eﬃcient online and batch learning using forward backward\nsplitting. J. Mach. Learn. Res., 10:2899–2934, December 2009.\nM. R. Garey and R. L. Graham. Performance bounds on the splitting algorithm for binary\ntesting. Acta Inf., 3(4):347–355, December 1974. ISSN 0001-5903.\nMichael Harries and New South Wales. Splice-2 comparative evaluation: Electricity pricing,\n1999.\nSteven CH Hoi, Jialei Wang, Peilin Zhao, and Rong Jin. Online feature selection for mining\nbig data.\nIn Proceedings of the 1st international workshop on big data, streams and\nheterogeneous source mining: Algorithms, systems, programming models and applications,\npages 93–100. ACM, 2012.\nKao-Shing Hwang, Tsung-Wen Yang, and Chia-Ju Lin. Self organizing decision tree based\non reinforcement learning and its application on state space partition. In Proceedings of\nthe IEEE International Conference on Systems, Man and Cybernetics, Taipei, Taiwan,\nOctober 8-11, 2006, pages 5088–5093. IEEE, 2006.\nJohn Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated gradient.\nJ. Mach. Learn. Res., 10:777–801, June 2009.\nMircea Preda. Adaptive building of decision trees by reinforcement learning. In Proceedings\nof the 7th Conference on 7th WSEAS International Conference on Applied Informatics\nand Communications - Volume 7, AIC’07, pages 34–39, 2007.\nPaul E. Utgoﬀ. Incremental induction of decision trees. Mach. Learn., 4(2):161–186, Novem-\nber 1989.\n9\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2015-07-24",
  "updated": "2015-07-24"
}