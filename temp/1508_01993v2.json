{
  "id": "http://arxiv.org/abs/1508.01993v2",
  "title": "Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures",
  "authors": [
    "Stefan Feuerriegel",
    "Ralph Fehrer"
  ],
  "abstract": "Decision analytics commonly focuses on the text mining of financial news\nsources in order to provide managerial decision support and to predict stock\nmarket movements. Existing predictive frameworks almost exclusively apply\ntraditional machine learning methods, whereas recent research indicates that\ntraditional machine learning methods are not sufficiently capable of extracting\nsuitable features and capturing the non-linear nature of complex tasks. As a\nremedy, novel deep learning models aim to overcome this issue by extending\ntraditional neural network models with additional hidden layers. Indeed, deep\nlearning has been shown to outperform traditional methods in terms of\npredictive performance. In this paper, we adapt the novel deep learning\ntechnique to financial decision support. In this instance, we aim to predict\nthe direction of stock movements following financial disclosures. As a result,\nwe show how deep learning can outperform the accuracy of random forests as a\nbenchmark for machine learning by 5.66%.",
  "text": "IMPROVING DECISION ANALYTICS WITH DEEP\nLEARNING: THE CASE OF FINANCIAL DISCLOSURES\nResearch in Progress\nFeuerriegel, Stefan, University of Freiburg, Freiburg, Germany, stefan.feuerriegel@is.uni-freiburg.de\nFehrer, Ralph, University of Freiburg, Freiburg, Germany, ralphfehrer@gmail.com\nAbstract\nDecision analytics commonly focuses on the text mining of ﬁnancial news sources in order to provide\nmanagerial decision support and to predict stock market movements. Existing predictive frameworks\nalmost exclusively apply traditional machine learning methods, whereas recent research indicates that\nthese methods are not sufﬁciently capable of extracting suitable features and capturing the non-linear\nnature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending\nclassical neural networks with additional hidden layers. Indeed, deep learning often provides a viable\napproach to achieve a high predictive performance. In this paper, we adapt the novel deep learning\ntechnique to ﬁnancial decision support, where we aim to predict the direction of stock movements\nfollowing ﬁnancial disclosures. As a result, our paper shows how deep learning can outperform the\naccuracy of benchmarks for machine learning by 5.66 %.\nKeywords: Decision Analytics, IS Research Methodologies, Financial Prediction, Data Mining, Business\nIntelligence (BI), Text Mining, Information Processing.\n1\nIntroduction\nOrganizations are constantly looking for ways to improve their decision-making processes in core areas,\nsuch as marketing, ﬁrm communication, production and procurement (Turban, 2011). While the classical\napproach relies on having humans devise simple decision-making rules, modern decision support can also\nexploit statistical evidence that originates from analyzing data (Apte, Liu, Pednault, and Smyth, 2002;\nArnott and Pervan, 2005; Asadi Someh and Shanks, 2015; Boylan and Syntetos, 2012; Davenport, 2006;\nVizecky, 2011). This data-driven decision support is nowadays also fueled by the Big Data era (Boyd and\nCrawford, 2012; Chen, Chiang, and Storey, 2012; Halper, 2011; Power, 2014). The term Big Data usually\nrefers to data that is massive in size. In addition, such data comes often in different formats (e. g. video,\ntext), changes quickly and is subject to uncertainty (IBM, 2013).\nCrucial aspects of data-driven decision support systems entail the prediction of future events, such as\nconsumer behavior or stock market reactions to press releases, based on an analysis of historical data\n(Apte, Liu, Pednault, and Smyth, 2002; Vizecky, 2011). Decision analytics thus frequently utilizes\nmodeling, machine learning and data mining techniques from the area of predictive analytics. In fact,\npredictive analytics can be instrumented for “generating new theory, developing new measures, comparing\ncompeting theories, improving existing theories, assessing the relevance of theories, and assessing the\npredictability of empirical phenomena” (Shmueli and Koppius, 2011).\nPredictive analytics frequently contributes to managerial decision support, as is the case when predicting\ninvestor reaction to press releases and ﬁnancial disclosures (Nassirtoussi, Aghabozorgi, Wah, and Ngo,\n2014). In this instance, predictive analytics is typically confronted with massive datasets of heterogeneous\nand mostly textual content, while outcomes are simultaneously of high impact for any business. Until\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n1\narXiv:1508.01993v2  [stat.ML]  4 Jul 2018\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nnow, decision support for ﬁnancial news still predominantly relies on traditional machine learning\ntechniques, such as support vector machines or decision trees (Minev, Schommer, and Grammatikos,\n2012; Nassirtoussi, Aghabozorgi, Wah, and Ngo, 2014; Pang and Lee, 2008).\nThe performance of traditional machine learning algorithms largely depends on the features extracted\nfrom underlying data sources, which has consequently elicited the development and evaluation of feature\nengineering techniques (Arel, Rose, and Karnowski, 2010). Research efforts to automate and optimize\nthe feature engineering process, along with a growing awareness of current neurological research, has\nled to the emergence of a new sub-ﬁeld of machine learning research called deep learning (Hinton and\nSalakhutdinov, 2006). Deep learning takes into account recent knowledge on the way the human brain\nprocesses data and thus enhances traditional neural networks by a series of hidden layers. This series\nof hidden layers allows for deeper knowledge representation, possibly resulting in improved predictive\nperformance. Deep learning methods have been applied to well-known challenges in the machine learning\ndiscipline, such as pattern recognition and natural language processing. The corresponding results indicate\nthat deep learning can outperform classical machine learning methods (which embody only a shallow\nknowledge layer) in terms of accuracy (c. f. Hinton and Salakhutdinov, 2006; Socher, Pennington, Huang,\nNg, and Manning, 2011).\nIn this paper, we want to unleash the predictive power of deep learning to provide decision-support in the\nﬁnancial domain. As a common challenge, we choose the task of predicting stock market movements that\nfollow the release of a ﬁnancial disclosure. We expect that deep learning can learn appropriate features\nfrom the underlying textual corpus efﬁciently and thus surpass other state-of-the-art classiﬁers. However,\nthe successful application of deep learning techniques is not an easy task; deep learning implicitly performs\nfeature extraction through the interplay of different hidden layers, the representation of the textual input\nand the interactions between layers. In order to master this challenge, we apply the recursive autoencoder\nmodel introduced by Socher, Pennington, Huang, Ng, and Manning (2011) and tailor it to the prediction\nof stock price directions based on the content of ﬁnancial materials.\nThe remainder of this paper is structured as follows. Section 2 provides a short overview of related work\nin which we discuss similar text mining approaches and give an overview of relevant deep learning\npublications. We then explain our methodology and highlight the differences between classical and\ndeep learning approaches (Section 3). Finally, Section 4 evaluates both approaches using ﬁnancial news\ndisclosures and discuss the managerial implications.\n2\nRelated Work\nThis Information Systems research is positioned at the intersection between ﬁnance, Big Data, decision\nsupport and predictive analytics. The ﬁrst part of this section discusses traditional approaches of providing\ndecision support based on ﬁnancial news. In the second part, we discuss previous work that focuses on the\nnovel deep learning approach.\n2.1\nDecision analytics for ﬁnancial news\nText mining of ﬁnancial disclosures represents one of the fundamental approaches for decision analytics in\nthe ﬁnance domain. The available work can be categorized by the necessary preprocessing steps, the text\nmining algorithms, the underlying text source (e. g. press releases, ﬁnancial news, tweets) and its focus on\nfacts or opinions (e. g. quarterly reports, analyst recommendations). While Pang and Lee (2008) provide a\ncomprehensive domain-independent survey, other overviews concentrate solely on the ﬁnancial domain\n(Minev, Schommer, and Grammatikos, 2012; Mittermayer and Knolmayer, 2006). In a very recent survey,\nNassirtoussi, Aghabozorgi, Wah, and Ngo (2014) focus speciﬁcally on studies aimed at stock market\nprediction. We structure the discussion of the related research according to the above categories.\nAmong the most popular text mining algorithms are classical machine learning algorithms, such as\nsupport vector machines, regression algorithms, decision trees and Naïve Bayes. In addition, neural\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n2\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nnetwork models have been used more rarely, but are slowly gaining traction, just as in other application\ndomains (Nassirtoussi, Aghabozorgi, Wah, and Ngo, 2014). Furthermore, Bayesian learning can provide\nexplanatory insights by generating domain-dependent dictionaries (Pröllochs, Feuerriegel, and Neumann,\n2015).\nAs part of preprocessing, the ﬁrst step in most text mining approaches is the generation of a set of\nvalues that represent relevant textual features, which can be used as inputs for the subsequent mining\nalgorithms. This usually involves the selection of features based on the raw text sources, some kind of\ndimensionality reduction and the generation of a good feature representation, such as binary vectors.\nA comprehensive discussion of the various techniques used for feature engineering can be found in\nNassirtoussi, Aghabozorgi, Wah, and Ngo (2014) and Pang and Lee (2008).\nThe text sources used for text mining include ﬁnancial news (e. g. Alfano, Feuerriegel, and Neumann, 2015;\nFeuerriegel, Wolff, and Neumann, 2015, 2016; Feuerriegel and Neumann, 2013) and company-speciﬁc\ndisclosures, and range from the less formal, such as tweets (e. g. Bollen, Mao, and Zeng, 2011), to more\nformal texts, such as corporate ﬁlings (e. g. Feuerriegel, Ratku, and Neumann, 2016; Muntermann and\nGuettler, 2007; Pröllochs, Feuerriegel, and Neumann, 2015). Some researchers have focused exclusively\non the headlines of news sources to exclude the noise usually contained in longer texts (Peramunetilleke\nand Wong, 2002).\nNews disclosures with a fact-based focus are especially relevant for investors. As such, German ad hoc\nannouncements in English contain strictly regulated content and a tight logical connection to the stock\nprice, making them an intriguing application in research. The measurable effect of ad hoc news on\nabnormal returns on the day of an announcement have been established by several authors (c. f. Groth\nand Muntermann, 2011; Hagenau, Liebmann, and Neumann, 2013; Muntermann and Guettler, 2007;\nPröllochs, Feuerriegel, and Neumann, 2015). Consequently, we utilize the same corpus in our following\nevaluation.\n2.2\nDeep learning as an emerging trend\nDeep learning1 originally focused on complex tasks, in which datasets are usually high-dimensional\n(Arel, Rose, and Karnowski, 2010; Bengio, 2009). As such, one of the ﬁrst successful deep learning\narchitectures consisted of a so-called autoencoder in combination with a Boltzmann machine, where the\nautoencoder carries out unsupervised pre-training of the network weights (Hinton and Salakhutdinov,\n2006). This model performs well on several benchmarks from the machine learning literature, such\nas image recognition. Moreover, its architecture can be adapted to enhance momentum stock trading\n(Takeuchi and Yu Ying, 2013) as one of the few successful applications of deep learning to ﬁnancial\ndecision support. However, this publication relies only on past stock returns and neglects the predictive\npower of exogenous input, such as ﬁnancial disclosures.\nThe natural language processing community has only recently started to adapt deep learning principles to\nthe speciﬁc requirements of language recognition tasks. For example, Socher, Pennington, Huang, Ng,\nand Manning (2011) utilize a recursive autoencoder to predict sentiment labels based on individual movie\nreview sentences. Further research improved the results on the same dataset by combining a recursive\nneural tensor network with a sentiment treebank (Socher et al., 2013).\n3\nMethodologies for Financial Decision Support\nThis section introduces our research framework to provide ﬁnancial decision support based on news\ndisclosures. In brevity, we introduce a benchmark classiﬁer and our deep learning architecture to predict\nstock movements. Altogether, Figure 1 illustrates how we compare both prediction algorithms. The\nrandom forest and the recursive autoencoder are both trained to predict stock market directions based\n1 For details, see reading list “Deep Learning”. Retrieved April 21, 2015 from http://deeplearning.net/reading-list/.\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n3\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nFeature selection\nNews  \ncorpus\nWord tokens \n(numbers and \npunctuation \nremoved)\nStock market\ndata\nDataset\nTerm-document \nmatrix (tf-idf)\nRepresentation\nSparse entries \nremoval\nDim. reduction\nClassification\nComparison\nAccuracy       \nRecall\nPrecision\nF1-Score\nRandom forest \n(benchmark)\nN-grams \n(implicitely \noptimized)\nNumerical \nvectors\nAutoencoder \n(implicitely \noptimized)\nLogistic \nregression\nFeature selection\nData\nRecursive autoencoder\nheadlines\nAbnormal\nreturns\nFigure 1.\nResearch framework comparing classical machine learning and deep learning.\non the ad hoc announcements and the according abnormal returns. To compare the performance of the\nrecursive autoencoder to the benchmark, we apply the same test set to each of the trained algorithms and\nmeasure the predictive performance in terms of accuracy, precision and recall based on the confusion\nmatrix.\nFollowing Nassirtoussi, Aghabozorgi, Wah, and Ngo (2014), we divide the overall procedure into steps\nfor data generation, feature selection, feature reduction and feature representation. Both approaches, the\nbenchmark algorithm and the recursive autoencoder, differ fundamentally in their preprocessing. The\napplication of a random forest or support vector machine requires traditional feature engineering, whereas\nthe recursive autoencoder, as a remedy, automatically generates a feature representation as part of its\noptimization algorithm. This is indicated in Figure 1 by the extension of the recursive autoencoder box\nover all preprocessing steps.\n3.1\nBenchmark: predicting stock movements with random forests\nIn a ﬁrst step, one transforms the running text into a matrix representation, which subsequently works\nas the input to the actual random forest algorithm. First of all, we remove numbers, punctuations and\nstop words from the running text and then split it into tokens (Manning and Schütze, 1999). Afterwards,\nwe count the frequencies of how often terms occur in each news disclosure, remove sparse entries to\nreduce the dimensionality and store these values in a document-term matrix. The document-term matrix\nthen represents the features. The actual values are weighted (Salton, Fox, and Wu, 1983) by the term\nfrequency-inverse document frequency (tf-idf). This is a common approach in information retrieval to\nadjust the word frequencies by their importance.\nIn the following evaluation, we utilize random forests as a benchmark classiﬁer. Random forests represent\none of the most popular machine learning algorithms due to their favorable predictive accuracy, relatively\nlow computational requirements and robustness (Breiman, 2001; Hastie, Tibshirani, and Friedman, 2009;\nKuhn and Johnson, 2013). Random forests are an ensemble learning method for classiﬁcation and\nregression, which is based on the construction and combination of many de-correlated decision trees.\nGiven a training set X = {xxx1,...,xxxN} with associated responses Y = {y1,...,yN}, the algorithm repeats\nthe following steps B times (we choose B = 500): (1) sample with replacement from X and Y to generate\nnew subsets X′ and Y ′. (2) Train a decision tree tb using X′ and Y ′. The individual decision trees tb,\nb = 1,...,B can be combined to predict a response ˆy for unseen values xxx as follows. One calculates\nindividual predictions tb(xxx) for b = 1,...,B from each tree and then aggregates these predictions by\nsimply using the majority vote to get the ﬁnal response.\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n4\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nx\ny\nz\nf\nf \nReconstruction \nlayer\n Input              \nlayer \nCode              \nlayer\nFigure 2.\nAn autoencoder searches a mapping between input xxx and a lower-dimensional representation\nyyy such that the reconstructed value zzz is similar to the input xxx.\n3.2\nDeep learning architecture: recursive autoencoders\nThis section describes the underlying architecture of our deep learning approach for ﬁnancial disclosures\nbased on so-called autoencoders. The architecture of an autoencoder is illustrated in Figure 2. An\nautoencoder is basically an artiﬁcial neural network, which ﬁnds a lower-dimensional representation of\ninput values. Let xxx ∈[0,1]N denote our input vector, for which we seek a lower-dimensional representation\nyyy ∈[0,1]M with M < N. The mapping f between xxx and yyy is named encoding function and can be, generally\nspeaking, any non-linear function, although a common choice is the sigmoid function\nf(xxx) = σ(Wxxx+bbb) =\n1\n1+exp(Wxxx+bbb) = yyy\nwith parameters W and bbb.\n(1)\nThe key idea of an autoencoder is to ﬁnd a second mapping from yyy to zzz ∈[0,1]N given by f ′(yyy) = σ(W ′yyy+\nbbb′′′), such that zzz is almost equal to the input xxx. Mathematically speaking, we choose the free parameters\nin f and f ′ by minimizing the difference between the original input vector xxx and the reconstructed\nvector zzz. This can be effectively achieved using numerical optimization, such as gradient descent, in\norder to determine the weights W and W ′. Altogether, the representation yyy (often called code) is a lower-\ndimensional representation of the input data; it is frequently used as input features for subsequent learning\nbecause it only contains the most relevant or most discriminating features of the input space.\nThe classical autoencoder works merely with a simple vector as input. In order to incorporate contextual\ninformation, we extend the classical autoencoder, resulting in a so-called recursive autoencoder. Here,\none trains a sequence of autoencoders, where each not only takes a vector xxx as input but also recursively\nthe lower-dimensional code of the previous autoencoder in the sequence. Let us demonstrate this approach\nwith an example as illustrated in Figure 3. We process input in the form of a sequence of words. Each\nword is e. g. given by a binary vector with zeros except for a single entry with 1 representing the current\nword. Then, we train the ﬁrst autoencoder with the input from the ﬁrst two words Company and Ltd. Its\nlower-dimensional code is then input to the second autoencoder together with the vector representation of\nthe word placing. This recursion proceeds up to the ﬁnal autoencoder, which produces as output the code\nrepresentation for the complete sentence. Hence, this recursive approach aims to generate a compact code\nrepresentation of a complete sentence while incorporating contextual information in the code layer. More\nprecisely, this approach can learn from an ordered sequence of words and not only the pure frequencies.\nIn addition, the recursive autoencoder entails an intriguing advantage: it can compress large input vectors\nin an unsupervised fashion without the need for class labels.\nThe basic steps for the recursive autoencoder are as follows: in a ﬁrst step, the individual words of an\ninput sentence (1) are mapped onto vectors of equal length l (2). We initialize the values of the weights by\nsampling from a Gaussian distribution and later continuously updated through backpropagation. Through\na recursive application of the autoencoder algorithm (3), the complete input sentence is then compressed\nbit by bit into a single code representation of length l (4). For this purpose, the ﬁrst autoencoder generates\na code representation of length l from the vectors representing the ﬁrst two words in the sentence. The\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n5\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nCompany\nLtd\nplacing\ncash\n(1) Input sentence\n(2) Vector  \nrepresentation \n(4) Sentence\ncode\n1st auto-\nencoder \n2nd auto-\nencoder \n3rd auto-\nencoder \n(3) Autoencoder\nlayers \nFigure 3.\nA recursive autoencoder is a sequence of autoencoders, where each not only takes a vector as\ninput but also the lower-dimensional code of the previous autoencoder in the sequence.\nsecond autoencoder takes this code representation and the third word vector as inputs, and calculates the\ncode representation of the next level.\nIn order to extract and predict sentiment values, we use an extended variant of the recursive autoencoder\nmodel (Socher, Pennington, Huang, Ng, and Manning, 2011), which includes an additional softmax layer\n(sometimes also referred to as multinomial logit) in each autoencoder. This softmax function estimates\nthe probability that an input vector xxx belongs to a certain class j ∈K via\nP(y = j|xxx) =\nexp\n\u0000xxxTWj\n\u0001\nK\n∑\nk=1\nexp(xxxTWk)\n.\n(2)\nIn order to train this model, we optimize the weights Wk of both the autoencoders and the softmax\nlayers simultaneously with a combined target function. We then utilize the trained weights to classify\nunknown sentences by ﬁrst computing the code representation inside the recursive autoencoder and,\nsecond, calculating the probabilities for each class from the softmax function. Interestingly, the backward\nmapping f ′ is needed for training but is no longer needed for the prediction (that is why black circles\nindicate the vectors only necessary for prediction in Figure 2 and Figure 3).\n4\nEvaluation: Predicting Stock Market Direction from Financial\nNews\nIn this section, we discuss and evaluate our experimental setting for predicting the direction of stock\nmarket movements following ﬁnancial disclosures. We start with describing the steps involved in the\ngeneration of the underlying dataset and then compare classical machine learning with our deep learning\narchitecture.\n4.1\nDataset\nOur news corpus originates from regulated ad hoc announcements2 between January 2004 and the\nend of June 2011 in English. These announcements conform to German regulations that require each\n2 Kindly provided by Deutsche Gesellschaft für Ad-Hoc-Publizität (DGAP).\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n6\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nlisted company in Germany to immediately publish any information with a potentially signiﬁcant effect\non the stock price. With their direct relation to a particular stock, the tightly controlled content and\nthe measurable effect on the stock price on the day of the announcement, ad hoc announcements are\nparticularly well-suited for the development and evaluation of techniques for predictive analytics.\nSince recursive autoencoders work on sentence tokens, we exclusively use the headlines of English ad hoc\nannouncements for the prediction and discard the message body. As previous work (e. g. Peramunetilleke\nand Wong, 2002) has shown, this is not a major disadvantage and can even help in reducing noise, as long\nas the titles concisely represent the content of the text.\nWe gather the ﬁnancial data of the releasing companies from Thomson Reuters Datastream. We retrieve\nthe ﬁrm performance with the help of the International Securities Identiﬁcation Numbers (ISIN) that\nappear ﬁrst in each of the ad hoc announcements. The stock price data before and on the day of the\nannouncement are extracted using the corresponding trading day. These are then used to calculate abnormal\nreturns (Konchitchki and O’Leary, 2011; MacKinlay, 1997; Pröllochs, Feuerriegel, and Neumann, 2015);\nabnormal returns can be regarded as some kind of excess return caused by the news release. In addition,\nwe remove penny stocks with stock prices below $5 for noise reduction. We then label each announcement\ntitle with one of three return direction classes (up, down or steady), according to the abnormal return of\nthe corresponding stock on the announcement day and discard the steady samples for noise reduction.\nThe resulting dataset consists of 8359 headlines from ad hoc announcements with corresponding class\nlabels up or down. Of this complete dataset, we use the samples covering the ﬁrst 80 % of the timeline as\ntraining samples and the remaining 20 % as test samples.3\n4.2\nPreliminary results\nWe can now apply the above methods for predictive analytics to provide decision support regarding how\ninvestors react upon textual news disclosures. By comparing random forests and recursive autoencoders,\nwe can evaluate our hypothesis that deep learning outperforms our benchmark in the current setting.\nThe detailed results are listed in Table 1. We compare the predictive performance on the out-of-sample test\nset in terms of accuracy, precision, recall and the F1-score. The random forest as our benchmark achieves\nan in-sample accuracy of 0.63 and an out-of-sample accuracy of 0.53 at best. In comparison, the recursive\nautoencoder4 as our deep learning architecture results in an accuracy of 0.56. This accounts for a relative\nimprovement of 5.66 %. Similarly, the F1-score increases from 0.52 to 0.56 – a substantial rise of 7.69 %.\nThe higher accuracy, as well as the improved F1-score, of the recursive autoencoder underlines our initial\nassumption that deep learning algorithms can outperform our benchmark from classical machine learning.\nWhen comparing the necessary computational resources, we see that recursive autoencoders (≈23 min)\nrequire less computation time than decision trees (more than 200 min).5 Moreover, recursive autoencoders\nhave an additional advantage: one can simply inject the complete set of news headlines as input without\nthe manual effort of feature engineering. The reason for this is that the calculation and optimization of a\nfeature representation is integrated into the optimization routines of deep learning algorithms.\nThe above results comply with the reported ﬁgures of around 60 % with the full message body from related\nwork (Groth and Muntermann, 2008; Hagenau, Liebmann, and Neumann, 2013). In direct comparison to\nthe benchmarks, our evaluation provides evidence that deep learning is a compelling approach for the\nprediction of stock price movements.\n3 We avoid the use of k-fold cross-validation as this would neglect the timing of disclosures. For instance, we would evaluate our\nmodels with disclosures during the ﬁnancial crisis, while the same models were previously trained with later knowledge of how\nnews were perceived after the happening of the ﬁnancial crisis.\n4 We systematically tried several combinations for the two adjustable parameters embedding size (the length l of the mapped\nfeature vectors) and number-of-iterations (i. e. number of gradient descent iterations). The best result accounts for an accuracy\nof 0.56 on the test-set, with a vector embedding size of 40 and 70 iterations. As expected, increasing the number of iterations\nusually results in better accuracy on the training set and lower accuracy on the test set – a typical indication of over-ﬁtting.\n5 Timings measured on an Intel Core i7-4700MQ CPU running at 2.4 GHz with 8 GB RAM and 64-bit Windows 8.1. Please\nnote that programming languages and matrix algebra libraries vary which makes a fair comparison difﬁcult.\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n7\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nPredictive Analytics Method\nAccuracy\nPrecision\nRecall\nF1-Score\nRandom Forest\n0.53\n0.53\n0.51\n0.52\nRecursive Autoencoder\n0.56\n0.56\n0.56\n0.56\nRelative Improvement\n5.66 %\n5.66 %\n9.80 %\n7.69 %\nTable 1.\nPreliminary results evaluating improvements by utilizing deep learning to predict the direction\nof stock price movements following ﬁnancial disclosures.\n4.3\nDiscussion and implications for practitioners\nTraditional machine learning techniques still represent the default method of choice in predictive analytics.\nHowever, recent research indicates that these methods insufﬁciently capture the properties of complex,\nnon-linear problems. Accordingly, the experiments in this paper show that a deep learning algorithm is\ncapable of implicitly generating a favorable knowledge representation.\nAs a recommendation to practitioners, better results are achievable with deep learning than with classical\nmethods that rely on explicitly generated features. Nevertheless, practitioners must be aware of the\ncomplex architecture of deep learning models. This requires both a thorough understanding and solid\nexperience in order to use such models efﬁciently.\nThe economic impact of our improvement is manifold. A higher predictive performance enables business\nopportunities for automated traders. In addition, corporates can utilize this approach to assess the expected\nmarket response subsequent to disclosures. This works as a safety mechanism to check if the subjective\nperception of investors matches the content of a release.\n5\nConclusion and Research Outlook\nIn the present paper, we show how a novel deep learning algorithm can be applied to provide decision\nsupport for the ﬁnancial domain. Thereby, we contribute to Information Systems research by shedding light\non how to exploit deep learning as a recent trend for managerial decision support. We demonstrate that a\nrecursive autoencoder outperforms our benchmark from traditional machine learning in the prediction of\nstock market movements following ﬁnancial news disclosures. The recursive autoencoder beneﬁts from\nbeing able to automatically generate a deep knowledge representation.\nIn future research, we intend to broaden the preliminary results of this Research-in-Progress paper.\nFirst, our analysis could beneﬁt substantially from incorporating and comparing further algorithms from\npredictive analytics. Second, we want to generalize our results by including further news sources.\nReferences\nAlfano, S. J., S. Feuerriegel, and D. Neumann (2015). “Is News Sentiment More than Just Noise?” In:\n23rd European Conference on Information Systems (ECIS 2015).\nApte, C., B. Liu, E. P. D. Pednault, and P. Smyth (2002). “Business Applications of Data Mining.”\nCommunications of the ACM 45 (8), 49–53.\nArel, I., D. C. Rose, and T. P. Karnowski (2010). “Deep Machine Learning: A New Frontier in Artiﬁcial\nIntelligence Research.” IEEE Computational Intelligence Magazine 5 (4), 13–18. URL: http://web.\neecs.utk.edu/~itamar/Papers/DML_Arel_2010.pdf (visited on 04/19/2015).\nArnott, D. and G. Pervan (2005). “A Critical Analysis of Decision Support Systems Research.” Journal of\nInformation Technology 20 (2), 67–87.\nAsadi Someh, I. and G. Shanks (2015). “How Business Analytics Systems Provide Beneﬁts and Contribute\nto Firm Performance?” In: 23rd European Conference on Information Systems (ECIS 2015).\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n8\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nBengio, Y. (2009). Learning Deep Architectures for AI. URL: http://www.iro.umontreal.ca/~lisa/\npointeurs/TR1312.pdf (visited on 08/10/2014).\nBollen, J., H. Mao, and X. Zeng (2011). “Twitter Mood Predicts the Stock Market.” Journal of Computa-\ntional Science 2 (1), 1–8.\nBoyd, D. and K. Crawford (2012). “Criticial Questions for Big Data.” Information, Communication &\nSociety 15 (5), 662–679.\nBoylan, J. E. and A. A. Syntetos (2012). “Forecasting in Management Science.” Omega 40 (6), 681.\nBreiman, L. (2001). “Random Forests.” Machine Learning 45 (1), 5–32.\nChen, H., R. H. L. Chiang, and V. C. Storey (2012). “Business Intelligence and Analytics: From Big Data\nto Big Impact.” MIS Quarterly 36 (4), 1165–1188.\nDavenport, T. H. (2006). “Competing on Analytics.” Harvard Business Review 134 (1), 98–107.\nFeuerriegel, S, A Ratku, and D Neumann (2016). “Analysis of How Underlying Topics in Financial\nNews Affect Stock Prices Using Latent Dirichlet Allocation.” In: Proceedings of the 49th Hawaii\nInternational Conference on System Sciences (HICSS 2016). IEEE Computer Society.\nFeuerriegel, S, G Wolff, and D Neumann (2015). “Information Processing of Foreign Exchange News:\nExtending the Overshooting Model to Include Qualitative Information from News Sentiment.” In:\nProceedings of the International Conference on Information Systems (ICIS 2015). Association for\nInformation Systems.\n— (2016). “News Sentiment and Overshooting of Exchange Rates.” Applied Economics (forthcoming).\nFeuerriegel, S. and D. Neumann (2013). “News or Noise? How News Drives Commodity Prices.” In:\nProceedings of the International Conference on Information Systems (ICIS 2013). Association for\nInformation Systems.\nGroth, S. and J. Muntermann (2008). “A Text Mining Approach to Support Intraday Financial Decision-\nMaking.” In: Americas Conference on Information Systems (AMCIS 2008).\nGroth, S. S. and J. Muntermann (2011). “An Intraday Market Risk Management Approach Based on\nTextual Analysis.” Decision Support Systems 50 (4), 680–691.\nHagenau, M., M. Liebmann, and D. Neumann (2013). “Automated News Reading: Stock Price Prediction\nbased on Financial News Using Context-Capturing Features.” Decision Support Systems 55 (3), 685–\n697.\nHalper, F. (2011). The Top 5 Trends in Predictive Analytics. URL: http : / / www . information -\nmanagement.com/issues/21_6/the-top-5-trends-in-redictive-an-alytics-10021460-\n1.html (visited on 08/20/2014).\nHastie, T., R. Tibshirani, and J. H. Friedman (2009). The Elements of Statistical Learning: Data Mining,\nInference, and Prediction. 2nd ed. Springer Series in Statistics. New York: Springer.\nHinton, G. E. and R. R. Salakhutdinov (2006). “Reducing the Dimensionality of Data with Neural\nNetworks.” Science 313 (5786), 504–507.\nIBM (2013). The Four V’s of Big Data. URL: http://www.ibmbigdatahub.com/infographic/four-\nvs-big-data (visited on 04/21/2014).\nKonchitchki, Y. and D. E. O’Leary (2011). “Event Study Methodologies in Information Systems Research.”\nInternational Journal of Accounting Information Systems 12 (2), 99–115.\nKuhn, M. and K. Johnson (2013). Applied Predictive Modeling. New York, NY: Springer.\nMacKinlay, A. C. (1997). “Event Studies in Economics and Finance.” Journal of Economic Literature\n35 (1), 13–39.\nManning, C. D. and H. Schütze (1999). Foundations of Statistical Natural Language Processing. 6th Edi-\ntion. Cambridge, MA: MIT Press.\nMinev, M., C. Schommer, and T. Grammatikos (2012). “News and Stock Markets: A Survey on Abnormal\nReturns and Prediction Models.” URL: http://orbilu.uni.lu/bitstream/10993/14176/1/TR.\nSurvey.News.Analytics.pdf (visited on 04/19/2015).\nMittermayer, M.-A. and G. F. Knolmayer (2006). “Text Mining Systems for Market Response to News: A\nSurvey: Working Paper.” SSRN Electronic Journal.\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n9\nFeuerriegel and Fehrer / Deep Learning for Financial Disclosures\nMuntermann, J. and A. Guettler (2007). “Intraday Stock Price Effects of Ad Hoc Disclosures: The German\nCase.” Journal of International Financial Markets, Institutions and Money 17 (1), 1–24.\nNassirtoussi, A. K., S. Aghabozorgi, T. Y. Wah, and D. C. L. Ngo (2014). “Text Mining for Market\nPrediction: A Systematic Review.” Expert Systems with Applications 41 (16), 7653–7670.\nPang, B. and L. Lee (2008). “Opininion Mining and Sentiment Analysis.” Foundations and Trends in\nInformation Retrieval (2), 1–135. (Visited on 04/19/2015).\nPeramunetilleke, D. and R. Wong (2002). “Currency Exchange Rate Forecasting from News Headlines.”\nAustralian Computer Science Communications 24 (2), 131–139.\nPower, D. J. (2014). “Using ‘Big Data’ for Analytics and Decision Support.” Journal of Decision Systems\n23 (2), 222–228.\nPröllochs, N., S. Feuerriegel, and D. Neumann (2015). “Generating Domain-Speciﬁc Dictionaries Using\nBayesian Learning.” In: 23rd European Conference on Information Systems (ECIS 2015).\nSalton, G., E. A. Fox, and H. Wu (1983). “Extended Boolean Information Retrieval.” Communications of\nthe ACM 26 (11), 1022–1036.\nShmueli, G. and O. Koppius (2011). “Predictive Analytics in Information Systems Research.” MIS\nQuarterly 35 (3), 553–572.\nSocher, R., J. Pennington, E. Huang, A. Ng, and C. Manning (2011). “Semisupervised Recursive Autoen-\ncoder.” In: Proceedings of the Conference on Empirical Methods in Natural Language Processing\n(EMNLP 2011), pp. 151–161.\nSocher, R. et al. (2013). “Recursive Deep Models for Semantic Compositionality Over a Sentiment\nTreebank.” In: Proceedings of the Conference on Empirical Methods in Natural Language Processing\n(EMNLP 2013). Vol. 1631.\nTakeuchi, L. and L. Yu Ying (2013). Applying Deep Learning to Enhance Momentum Learning Trading\nStrategies in Stocks. URL: http://cs229.stanford.edu/proj2013/TakeuchiLee-ApplyingDeepLearningT\npdf (visited on 05/03/2015).\nTurban, E. (2011). Business Intelligence: A Managerial Approach. 2nd Edition. Boston, MA: Prentice\nHall.\nVizecky, K. (2011). “Data Mining meets Decision Making: A Case Study Perspective.” In: Americas\nConference on Information Systems (AMCIS 2011), Paper 453.\nTwenty-Fourth European Conference on Information Systems, Istanbul, Turkey, 2016\n10\n",
  "categories": [
    "stat.ML",
    "cs.CL",
    "cs.LG"
  ],
  "published": "2015-08-09",
  "updated": "2018-07-04"
}