{
  "id": "http://arxiv.org/abs/1509.06061v1",
  "title": "A Statistical Theory of Deep Learning via Proximal Splitting",
  "authors": [
    "Nicholas G. Polson",
    "Brandon T. Willard",
    "Massoud Heidari"
  ],
  "abstract": "In this paper we develop a statistical theory and an implementation of deep\nlearning models. We show that an elegant variable splitting scheme for the\nalternating direction method of multipliers optimises a deep learning\nobjective. We allow for non-smooth non-convex regularisation penalties to\ninduce sparsity in parameter weights. We provide a link between traditional\nshallow layer statistical models such as principal component and sliced inverse\nregression and deep layer models. We also define the degrees of freedom of a\ndeep learning predictor and a predictive MSE criteria to perform model\nselection for comparing architecture designs. We focus on deep multiclass\nlogistic learning although our methods apply more generally. Our results\nsuggest an interesting and previously under-exploited relationship between deep\nlearning and proximal splitting techniques. To illustrate our methodology, we\nprovide a multi-class logit classification analysis of Fisher's Iris data where\nwe illustrate the convergence of our algorithm. Finally, we conclude with\ndirections for future research.",
  "text": "A Statistical Theory of Deep Learning via\nProximal Splitting\nNicholas G. Polson\nBooth School of Business\nUniversity of Chicago ∗\nBrandon T. Willard\nBooth School of Business\nUniversity of Chicago †\nMassoud Heidari\nSutton Place\nNew York ‡\nFirst Draft: May 2015\nThis Draft: September 2015\nAbstract\nIn this paper we develop a statistical theory and an implementation of deep\nlearning (DL) models.\nWe show that an elegant variable splitting scheme\nfor the alternating direction method of multipliers (ADMM) optimises a deep\nlearning objective. We allow for non-smooth non-convex regularisation penal-\nties to induce sparsity in parameter weights. We provide a link between tradi-\ntional shallow layer statistical models such as principal component and sliced\ninverse regression and deep layer models. We also deﬁne the degrees of free-\ndom of a deep learning predictor and a predictive MSE criteria to perform\nmodel selection for comparing architecture designs. We focus on deep multi-\nclass logistic learning although our methods apply more generally. Our results\nsuggest an interesting and previously under-exploited relationship between\ndeep learning and proximal splitting techniques. To illustrate our methodol-\nogy, we provide a multi-class logit classiﬁcation analysis of Fisher’s Iris data\nwhere we illustrate the convergence of our algorithm. Finally, we conclude\nwith directions for future research.\nKeywords: Deep Learning, Sparsity, Dropout, Convolutional Neural Nets;\nRegularisation; Bayesian MAP; Image Segmentation; Classiﬁcation; Multi-class\nLogistic regression.\n∗Professor of Econometrics and Statistics at the Chicago Booth School of Business. ngp@chicagobooth.edu.\n†bwillard@uchicago.edu\n‡email: heidari.massoud@gmail.com\n1\narXiv:1509.06061v1  [stat.ML]  20 Sep 2015\n1\nIntroduction\nDeep Learning (DL) provides a powerful tool for high dimensional data reduction.\nMany areas of applications in predictive modeling occur in artiﬁcial intelligence\nand machine learning; including pattern recognition Ripley [1996]; computer vi-\nsion Dean et al. [2012]; image segmentation and scene parsing Farabet et al. [2013];\npredictive diagnostics; intelligent gaming Mnih et al. [2013]. The salient feature\nof a deep learning model is a predictive rule comprised by a layered composition\nof link or activation functions. Deep architectures with at least three layers have\nbeen shown to provide improved predictive performance compared to traditional\nshallow architectures in a number of applications. The challenge for deep learning\nmethodologies, however, are computational: the objective function which mea-\nsures model ﬁt is typically highly multi-modal and hard to optimise efﬁciently.\nWe build on the extensive deep learning literature by showing that proximal\nBayesian optimisation techniques provide a turn-key solution to estimation and\noptimisation of such models and for calculating a regularisation path. We allow for\nthe possibility of irregular non-convex regularisation penalties to induce sparsity\nin the deep layer weights. Proximal operators and the alternating direction method\nof multipliers (ADMM) are the key tools for implementation. This approach sim-\nply re-writes the unconstrained optimisation as a constrained one, with a carefully\nconstructed sequence of auxiliary variables and envelopes, to deal with the asso-\nciated augmented Lagrangian; see Parikh and Boyd [2014], Polson et al. [2015],\nGreen et al. [2015] for recent surveys. Proximal algorithms have achieved great\nsuccess and provide a methodology for incorporating irregular non-differentiable\npenalties; see [Masci et al., 2013] for applications in the areas of computer vision\nand signal processing.\nFrom a statistical perspective, DL models can be viewed as generalised lin-\near models (GLM Davison [2003], Dellaportas and Smith [1993]) with recursively\ndeﬁned link functions. Traditional statistical models commonly use shallow net-\nworks containing at most two layers or hierarchies. For example, reduced rank\nregression can be viewed as a deep learning model with only two layers and linear\nlinks. Support vector machines for classiﬁcation Polson and Scott [2013] use a pre-\ndictive rule based on a rectiﬁed linear (or hinge) link. Recent empirical evidence,\nhowever, suggests improved statistical predictive performance with deep archi-\ntectures of at least three layers. Our focus here will be on developing fast learning\nmethods for deep multi-class logistic models although our methods apply more\ngenerally to recurrent and convolutional neural nets. Although well known in the\nNeural Network and Statistics literature [Knowles and Minka, 2011], efﬁcient es-\ntimation of the cross-entropy/multinomial loss with regularization–outside of the\nℓ2-ridge penalty–has not been a mainstream area of research. See Madigan et al.\n[2005] and Genkin et al. [2007] for applications to large scale multinomial logistic\nmodels. In general, the ℓ2-ridge penalty is commonplace [Poggio and Girosi, 1990,\nOrr, 1995], mostly due to its differentiability. Our methods can therefore be seen\nas related to sparse Bayes MAP techniques; see Titterington [2004], Windle et al.\n[2013], Polson et al. [2015] for high dimensional data reduction.\n2\nMainstream estimation within Deep Learning broadly revolves around gradi-\nent descent methods. The main variation in techniques arises from general con-\nsiderations for computational complexity and introduce more tuning parameters\n[Ngiam et al., 2011]. Such considerations are the basis for Stochastic Gradient De-\nscent (SGD) and back-propagation. For instance, back-propagation uses the chain\nrule for the derivative of the composite of activation functions. This can reduce\nthe order-of-operations dramatically from naive direct evaluation while maintain-\ning high numerical accuracy; however, this says little about the general difﬁcultly\nin estimation of a non-linear objective function. Direct gradient methods can be\npoorly scaled for the estimation of deep layer weights, in contrast to our proximal\nsplitting approach which overcomes this by providing a simultaneous block up-\ndate of parameters at all layers. The largest networks (e.g. Dean et al. [2012]) are\ncurrently trained using asynchronous SGD. Farabet et al. [2013] discusses hard-\nware approaches to faster algorithms. Providing training methodologies is a very\nactive ﬁeld of research.\nThe splitting techniques common in the proximal framework do exist in the AI\nliterature but we believe that their broad applicability and functional coverage has\nmostly been overlooked and under-exploited. This is possibly due to the afore-\nmentioned concerns of computational complexity that makes stochastic gradient\ndescent (SGD) and back-propagation methods so popular, although it’s quite pos-\nsible that the number of iterations to step-complexity can favor (parallel) proximal\nmethods in some cases. We show that our augmented ADMM approach is embar-\nrassingly parallel with block updates for parameters and auxiliary variables being\ndirectly available due to our carefully chosen splitting procedure.\nTraditional approaches to deep learning use Back-propagation LeCun et al.\n[2012], Hinton et al. [2006], Hinton and Salakhutdinov [2006] which rely on the\nchain rule for the derivatives of the composite of the L layers in the network. We\npropose a proximal splitting approach which also lends itself to the inclusion of a\nnon-differentiable regularisation penalty term so as to induce sparsity. Combettes\nand Pesquet [2011] detail multiple splitting methods in the context of proximal\noperators as well as parallel proximal algorithms that handle many splitting vari-\nables. Wang and Carreira-Perpin´an [2012] perform splitting in a similar context\nas ours, but with an ℓ2 loss and quadratic barrier approach to handle the equal-\nity constraints. In contrast, we generalize to non-ℓ2 and detail a general enve-\nlope approach that includes the well known augmented Lagrangian. Our general\nenvelope approach allows one to utilize efﬁcient bounds, such as those found in\nBouchard [2007] and Knowles and Minka [2011].\nThe rest of the paper is outlined as follows. Section 2 introduces deep learning\npredictors. We show that standard statistical models such as principal components\nanalysis, reduced rank and sliced inverse regression can be viewed as shallow ar-\nchitecture models. We also motivate the addition of further layers to aid in the reg-\nularised prediction problem. We illustrate how proximal splitting methods solve\na simple shallow architecture model. Section 3 describes the general deep learn-\ning problem and its solution via proximal splitting methods. Supervised learning\nuses a training dataset to estimate the parameters of each layer of the network. We\n3\ndeﬁne the degrees of freedom of a deep learning predictive rule which quantiﬁes\nthe trade-off between model ﬁt and predictive performance. Section 5 provides a\ncomparison of DL and Neural Network (NN) models in a multi-class deep logistic\nclassiﬁcation model for Fisher’s Iris data. We also illustrate the convergence of our\nalgorithm. Finally, Section 6 concludes with directions for future research.\n2\nDeep Learning Predictors\nLet y ∈S where S = RN for regression and S = {1, . . . , K} for classiﬁcation. Here\ny denotes an observed output associated with a high dimensional input/covariate\nvariable given by X = {Xi}N\ni=1 and xi ∈RM. The generic problem is to ﬁnd a\nnon-linear predictor of the output ˆy(X , W) where W are parameters which will be\nestimated from a training dataset of N outputs and inputs, denoted by {yi, Xi}N\ni=1.\nWhen the dimensionality of X is high, a common approach is to use a data\nreduction technique. This is achieved by introducing a low-dimensional auxiliary\nvariable z ∈Rd where d ≪M and constructing a prediction rule speciﬁed by a\ncomposition of functions,\nˆy = f1( f2(X ))\n= f1(z) where z = f2(X ).\nNow the problem of high dimensional data reduction is to ﬁnd the z-variables\nusing training data and to estimate the layer functions ( f1, f2). One reason for the\nsuccess of deep learning is the regularisation achieved through the hidden layer\nlow-dimensional z variable. A hallmark of deep learning models is also the use\nof nonlinear layers. As such, one can view them as hierarchical nonlinear factor\nmodels or more speciﬁcally as generalised linear models (GLM) with recursively\ndeﬁned nonlinear link functions.\nThe key to a layering approach is to uncover the low-dimensional z-structure\nin a way that doesn’t disregard information about predicting the output y. We will\nshow that our splitting scheme naturally uses the hidden layer z-variables.\nFor example, Principal component analysis (PCA) reduces X using a SVD de-\ncomposition Wold [1956]. This type of dimension reduction is independent of y\nand can easily discard X information that is valuable for predicting the output.\nSliced inverse regression (SIR) Cook and Lee [1999], Cook [2007] overcomes this by\nestimating the layer function f2, independently of f1, using data on both (y, X ).\nDeep learning takes this one step further and jointly estimates f1 and f2 in tandem\nusing training data on both pairs (y, X ).\nA common approach is to introduce parameters, W, at each layer by convo-\nlution with linear maps which we denote by f1(W1z) and f2(W2x) assuming cen-\ntered variables. Statistical models are traditionally shallow architectures with at\nmost two layers. Reduced rank regression corresponds to linear link functions with\nf1(z) = W1z where z = f2(x) = W2x. The dimensionality reduction then becomes\nan eigen-problem for W1 and W2. Radial basis functions/kernel sliced inverse regression\n4\nuses f1(z) = WΦ(z) where Φ is a set of kernels/basis functions. In many cases, we\nwill also add a penalty, φ(W), for parameter estimation. The term φ(W) is a regu-\nlarization term that imposes structure or effects a favorable bias-variance trade-off\nin prediction. We need to be able to account for non-smooth penalties such as lasso\nφ(W) = γ ∑L\nl=1 |Wl| or bridge penalty to induce sparsity, where γ > 0 is a scaling\nparameter that traces out a full regularisation path.\nA deep learning predictor takes the form of an L-layered convolution rule of\nlink functions where\nˆy = f1(. . . fL−1(zL) . . . ) with zL = fL(X ) .\nWe deﬁne the set of layer z-variables are given by zl = Wl fk(zl+1) assuming that\nthe variables are centered.\nThe original motivation for recursive deep learning predictors resides in the\nseminal work of Kolmogorov [1957], Lorenz [1963] and the extensions derived by\nBarron [1993], Poggio and Girosi [1990] and Bach [2014]. See Paige and Butler\n[2001] for a Bayesian approach to neural networks. The motivation behind lay-\nered convolution maps lies in the following completeness result. Given a Gaus-\nsian nonparametric regression with any continuous function, F(x) on [0, 1]M and\nϵ ∼N(0, I) Poggio and Girosi [1990], there exists one dimensional link functions\nfl and fl,m such that\ny = F(X ) + σϵ where F(X ) =\n2M+1\n∑\nl=1\nfl\n \nM\n∑\nm=1\nfl,m(xm)\n!\nThis is a four layer network with a large number of units at the ﬁrst layer. On the\npractical side, one may wish to build a deep network with more layers but less\nunits at each stage. Section 4.2 provides a model selection approach to architecture\ndesign.\nNeural network models can simply be viewed as projection pursuit regression\nF(x) = ∑L\nm=l fl(Wlx) with the only difference being that in a neural network the\nnonlinear link functions, fl, are parameter dependent and learned from training\ndata. For example, a two-layer auto-encoder model with a sigmoid link function\nuses a prediction rule given by functions of the form ∑l W1,l · σ (∑m W2,l,mxm + bm,0).\nThe fl’s are learned via the sigmoid function of the linear combination.\nThe modeling intuition a deep learning architecture can be found within a two\nlayer system. Suppose that we have a ℓ2 loss problem with a two layer rule\nmin\nW ∥y −ˆy(X , W)∥2 where ˆy = W1 f1(W2 f2(X )) .\nThis objective can be highly nonlinear. Finding the optimal parameters is chal-\nlenging as traditional convex optimisation methods deal only with sums rather\nthan composites of objective functions. Our approach will be based on augmented\nADMM methods which still apply by splitting on the variables z1 = W1 f1(z2) and\n5\nz2 = W2 f2(X ). Bertsekas [1976] provides a general discussion of ADMM methods.\nNow we need to solve an augmented Lagrangian problem of the form\nmax\nK\nmin\nW,Z L(W, Z, K)\nwith K = {κ1, κ2} and Z = {z1, z2} where\nL(W, Z, K) =\nN\n∑\ni=1\n\u001a\n∥yi −zi,1∥2 + κi,1 (zi,1 −W1 f1(zi,2)) + µi,1\n2 ∥zi,1 −W1 f1(zi,2)∥2\n+ κi,2 (zi,2 −W2 f2(xi)) + µi,2\n2 ∥zi,2 −W2 f2(xi)∥2\n\u001b\nfor augmentation parameters (µi1, µi2).\nRe-expressing in scaled Lagrangian form with ui,j = κi,j/µi,j, j ∈{1, 2}, gives\nL(Z, W, K) =\nN\n∑\ni=1\n\u001a\n∥yi −zi,1∥2\n+ µi,1\n2 ∥zi,1 + ui,1 −W1 f1(zi,2)∥2 −µi,1\n2 ∥ui,1∥2\n+ µi,2\n2 ∥zi,2 + ui,2 −W2 f2(xi)∥2 −µi,2\n2 ∥ui,2∥2\n\u001b\n.\nIf we add an ℓ2-norm penalty for W, we obtain ridge regressions steps in block\nW updates. The scaled Lagrangian saddle-point is solved via the iterative ADMM\nscheme Polson et al. [2015]. This solves the optimisation of a recursively deﬁned set\nof link functions rather than the traditional sum of convex functions. See Lewis and\nWright [2008] for convergence analysis of these layered optimisation problems.\nAn important feature of our ADMM update for the parameters W = (W1, W2)\nis that it happens in tandem. Moreover, the regularisation steps are properly scaled\nby the current values of the functions of the augmented z-variables. On the other\nhand, a direct gradient-based approach such as back-propagation uses ﬁrst order\ninformation based on the composite derivative ( f1 ◦f2)′. This can easily lead to\nsteps for the second layer parameters, namely W2, that are poorly scaled. Back-\npropagation can be slow to converge as it makes small zig-zag steps in the highly\nmulti-modal objective. More efﬁcient second order Hessian methods would re-\nquire more information and computation. Martens and Sutskever [2011] develops\na Hessian-free Lagrangian approach that is based on an envelope that uses the\nderivative of the composite map l ◦f where l denotes the model measure of ﬁt and\nf a layer function.\nWhilst our augmented Lagrangian uses an ℓ2-barrier, we can use general met-\nrics and still achieve convergence Bertsekas [1976]. A computationally attractive\napproach would be to match the type of ADMM barrier with the nonlinearity in\nthe link function. We leave this for future research.\nWe now turn to our Deep Bayes learning framework. Bayes provides a way of\n6\nTable 1:\nLink (activation) functions. Typical Lp-norms are p = 1 (lasso), p = 2\n(ridge) or ∞(max-norm). RectLU/hinge norm is related to lasso via max(u, 0) =\n1\n2(|u| + u), see Polson et al. [2011]. Max-pooling is the sum of a hinge and lasso\nnorms, max(|u1|, |u2|) = max(|u1| −|u2|, 0) + |u2|.\nfk(u)\nlinear\nAu + b\nsigmoid\n(1 + eu)−1\nsoftmax\neu/ ∑K\nk=1 euk\ntanh\n2(1 + eu)−1 −1\nlog-sum-exp\nlog ∑i eui\nℓp-norm\n(∑i |ui|p)\n1\np\nrectLU/hinge\nmax(0, u)\nmax-pooling\nmax{|uIi|, |uIj|}\nunifying the stochastic and algorithmic approaches to data Breiman [2001].\n3\nDeep Bayes Learning\nA deep learning predictor, ˆy(x, W) ∈RN, depends recursively on a set of link\nfunctions, fk, 1 ≤k ≤L, deﬁned by\nˆyi := ˆy(xi, W) = W1 f1 (W2 f2(. . . ( fL−1(WLXi + bL)) . . . ) + b2) .\nThe link or activation functions fl : RNl →RNl are pre-speciﬁed and part of the\narchitecture design. Speciﬁc choices includes sigmoidal; softmax; tanh; rectLU or log-\nsum-exp see Table 1. We use L to denote the number of layers in the network. The\nparameters bl ∈RNl, with 1 ≤l ≤L and b1 = 0, are off-sets or bias parameters.\nThe parameters W can be decomposed as\nW = (W1, . . . , WL) for Wl ∈RNl×Nl+1 for 1 ≤l < L and NL = M,\nTo construct a posterior distribution, p(W|y), we use a prior distribution, p(W) ∝\nexp (−φ(W)), where φ(W) is a regularisation penalty.\nBayes rule yields the posterior distribution over parameters given data, namely\np(W|X , y) ∝p(y| ˆy(X , W))p(W)\n∝exp (−log p(y| ˆy(X , W)) −φ(W)) .\nThe deep learning estimator, ˆW, is a Bayes MAP estimator and leads to a pre-\n7\ndiction rule\nˆy(X , ˆW) where\nˆW := argmax\nW\np(W|X , y) .\nMaximising the posterior distribution corresponds to ﬁnding the argmin of the\ndeep learning objective function. Taking a Bayesian perspective allows the re-\nsearcher to characterise the full posterior distribution, p(W|y), typically, via sim-\nulation methods such as MCMC. This can aid in ﬁnding parameter uncertainty\nestimates and in determining efﬁcient prediction rules.\nThe log-likelihood can also be interpreted as gauging the accuracy of a pre-\ndiction, ˆy, of outcome, y. The underlying probability model, denoted by p(y| ˆy),\ndetermined this ﬁt via\nl(y, ˆy) = −log p(y| ˆy) .\nThe function l(y, ˆy) is typically a smooth function such as an ℓ2-norm ∥y −ˆy∥2\nor a cross-entropy metric l(y, ˆy) = y log ˆy when dealing with classiﬁcation. The\nstatistical interpretation is as a negative log-likelihood, in machine learning a cost\nfunctional. The major difﬁculty in implementing deep learning models is the high\ndegree of nonlinearity induced by the predictor, ˆy(W, X ), in the likelihood.\nWe will pay particular attention to the multi-class deep logistic learning model.\nThe ﬁrst layer is given by σ(Wx) = eWx/ ∑K\nk eWkx which deﬁnes the vector sigmoid\nfunction σ(x).\nExample 1 (Deep Logistic Learning). Suppose that our observations yt represent a\nmulti-class 1-of-K indicator vector, which we equate with class k via y = k for 1 ≤k ≤K.\nGiven a set of deep layer link functions with zl = fl(·), we have a predictive rule\nˆy(z, W) = p(y = k|z, W) = σk(W1z) .\nThe negative log likelihood is given by\nl(yi, ˆyi) = log p(yi| ˆyi) = −log\nK\n∏\nk=1\n( ˆyi,k)yi,k\n= −\nK\n∑\nk=1\nyi,k log ˆyi,k .\nMinimising the cross-entropy cost functional is therefore equivalent to a multi-class logis-\ntic likelihood function. Genkin et al. [2007] and Madigan et al. [2005] provide analysis of\nlarge-scale multinomial logit models with shallow architectures.\nThe basic deep learning problem is supervised learning with a training dataset\n(yi, Xi)N\ni=1. We ﬁnd the deep network structure via an optimisation of the following\nform\nargmin\nW\n{l(yi, ˆy(xi, W)) + φ(W) + φ(Z)}\nwhere ˆy(xi, W) = W1 f1 (W2 f2(. . . ( fL−1(WLXi + bL)) . . . ) + b2)\n(1)\nAgain l(y, ˆy) is a measure of ﬁt depending implicitly on some observed data y\n8\nand a prediction rule ˆy(W, X ). Here y denotes an N-vector of outcomes and X a\ncorresponding set of N-many M-vector characteristics; for example, pixels in an\nimage or token counts in a topic model for document classiﬁcation.\nTo solve the deep learning objective function with non-differentiable regular-\nisation penalties we use an auxiliary latent variable scheme in the context of an\naugmented Lagrangian. This allows us to write the deep learning optimisation as\na constrained problem\nargmin\nZ,W\nN\n∑\ni=1\nl(yi, z1) + φ(W) + φ(Z)\nwhere zl = Wl fl(zl+1) + bl\n1 ≤l < L\nzL = WLxi + bL\n(2)\nThrough variable splitting we can introduce latent auxiliary variables, zl ∈RNl,\nwhere fl(zl+1) : RNl+1 →RNl+1. Notice that we also allow for regularisation of the\nZ variables so as to include sparsity.\nOur approach follows the alternating pattern of ADMM and Douglas-Rachford\ntype algorithms, which for a split objective given by\nmin f (w) + g(z) where Aw −Bz = 0\nperforms the following iterations\nw(t+1) = argmin\nw\n\u001a\nf (w) + µ\n2\n\r\r\rw −(z(t) −u(t))\n\r\r\r\n2\u001b\nz(t+1) = argmin\nz\n\u001a\ng(z) + µ\n2\n\r\r\rz −(w(t+1) + u(t))\n\r\r\r\n2\u001b\nu(t+1) = u(t) + Aw(t+1) −Bz(t+1) .\nfor some µ > 0.\nRoughly speaking, variable splitting enables one to formulate an iterative solu-\ntion to the original problem that consists of simple ﬁrst-order steps, or independent\nsecond-order steps, since it “decouples” the functions, which in our case are the\npenalty, layers, and loss functions. Now, potentially simpler minimization steps\non the splitting variables and observation are combined, often in a simple linear\nway, to maintain the “coupling” that exists in the original problem. In such a set-\nting, one can craft sub-problems–through the choices of possible splittings–that\nhave good conditioning or that suit the computational platform and restrictions,\nall without necessarily excluding the use of standard and advanced optimization\ntechniques that may have applied to the problem in its original form.\nFor instance, the same Newton-steps and back-propagation techniques can be\napplied on a per-layer basis. For that matter, one can choose to split at the lowest\nlayer (i.e. between the loss function and layers above) and allow f1 to be a compli-\ncated composite function. This separates the problem of minimizing a potentially\n9\ncomplicated loss from a composition of non-linear functions. In general, steps will\nstill need to be taken in the composite f1, but again, currently established methods\ncan be applied.\nSince splitting can occur at any level, a natural question is at which layer should\nthe splitting occur. If there was a penalty function across splitting variables, then\nstandard regularization paths could be computed in fairly low dimension (min-\nimum 2, for a weight on W and Z). This provides a potentially smooth scaling\nacross the layer dimension and/or number of layers (see the regressor selection\nproblems in [Boyd et al., 2011, Section 9.1.1]). By simply re-applying the strict\nequality constraints between splitting variables in some layers, e.g. through an in-\ndicator penalty φ(Z), one could effectively emulate certain model designs; thus,\nthrough a structured approach to regularizing these terms one can perform a type\nof model selection. This approach may also explain–and emulate–the effect of\ndrop-out in a non-stochastic way.\nThe recursive nature of deﬁning layers naturally leads to the construction of z-\nvariables and in Section 4 and 4.1 we show how this provides an under-exploited\nrelationship between these methods and deep learning.\n4\nProximal Splitting\nLet’s start by considering the augmented Lagrangian, L(Z, W, K), which takes the\nfollowing general form for observations {(yi, Xi)}1:N\nmax\nK\nmin\nZ,W\nN\n∑\ni=1\n\u001a\nφ(W) + l(yi, zi,1)\n+ κ⊤\ni,1 (zi,1 −W1 f1(zi,2) −b1) + µi,1\n2 ∥zi,1 −W1 f1(zi,2) −b1∥2\n+\nL−1\n∑\nl=2\n\u0010\nκ⊤\ni,l (zi,l −Wl fl(zi,l+1) −bl) + µi,l\n2 ∥zi,l −Wl fl(zi,l+1) −bl∥2\u0011\n+ κ⊤\ni,L (zi,L −WLxi −bL) + µi,L\n2 ∥zi,L −WLxi −bL∥2\n\u001b\n(3)\nwhere κi,l ∈RNl and κi,l ∈K are Lagrange multipliers and µi,l ∈R+. The form\nhere does not easily highlight the role of each term across observations and layers,\nnor the relationship between terms. As a result, in what follows, we recast the\nLagrangian into forms that clarify the relationship.\nWe make extensive use of vectorization, so many of the result are obtained by\nusing the following identities\nvec(AB) = (I ⊗A) vec(B) = (B⊤⊗I) vec(A)\n10\nwhich, in our case, give\nvec(Wl fl(Zl+1)) = (IN ⊗Wl) fl(zl+1) = ( fl(Zl+1)⊤⊗INl)wl\nwhere wl = vec(Wl), for the stacked by observation terms\nZl = (z1,l . . . zN,l) ∈RNl×N and zl = vec(Zl) ∈RNl·N\nX = (x1 . . . xN) ∈RM×N and µl = (µ1,l . . . µN,l)⊤∈RN\n+ .\nWe also extend Wl and fl(zi,l+1) with ˜Nl = Nl + 1 and\n˜Wl =\n\u0000bl\nWl\n\u0001 ∈RNl× ˜Nl+1,\n˜fl(zi,l+1) =\n\u0012\n1\nfl(zi,l+1)\n\u0013\n∈R ˜Nl+1×1\n˜wl = vec( ˜Wl) ∈RNl· ˜Nl+1,\n˜fl(Zl+1) =\n\u0012\n1⊤\nN\nfl(Zl+1)\n\u0013\n∈R ˜Nl+1×N\nwhich includes fL(zi,L+1) := xi. This means that the bl terms are members of the\nprimal variable set W.\nWe introduce the scaled Lagrange multipliers\nu⊤\nl =\n \nκ⊤\n1,l\nµ1,l\n. . .\nκ⊤\nN,l\nµN,l\n!\n∈RNl·N .\nWe then obtain\nmax\nU\nmin\nW,Z\n\u001a\nφ(W) + L(y, z1) +\nL\n∑\nl=1\n1\n2∥zl −(IN ⊗Wl) fl(zl+1) −(1N ⊗bl) + ul∥2\nΛµl\n−1\n2\nL\n∑\nl=1\n\u0010\n∥u1∥2\nΛµ1\n\u0011\u001b\n=\nmax\nU\nmin\nW,Z\n\u001a\nφ(W) + L(y, z1) +\nL\n∑\nl=1\n1\n2∥zl −\n\u0000IN ⊗˜Wl\n\u0001 ˜fl(zl+1) + ul∥2\nΛµl\n−1\n2\nL\n∑\nl=1\n\u0010\n∥u1∥2\nΛµ1\n\u0011\u001b\n(4)\nwhere IN is the N × N identity matrix and\nΛµl = Diag(µl ⊗1Nl) =\nN\nM\ni=1\nµi,lINl ∈R(N·Nl)×(N·Nl)\n+\n,\ninner-product norm ∥x∥2\nΛ := x⊤Λx and with ⊗denoting the Kronecker product\nand L the direct sum. Λµl is a block diagonal, so it can be factored in “square root”\nfashion.\n11\nNaturally, operations across layers l can also be vectorized. First, let Nw =\nN ∑L\nn=1 ˜Nn, Nz = N ∑L\nn=1 Nn and\nz⊤=\n\u0010\nz⊤\n1 , . . . , z⊤\nL\n\u0011\n∈RNz\nu⊤=\n\u0010\nu⊤\n1 , . . . , u⊤\nL\n\u0011\n∈RNz\nΛµ =\nL\nM\nn=1\nΛµn ∈RNz×Nz .\nThe linear ﬁrst-order difference maps are deﬁned by\n∆˜w : RNz →RNz\n∆˜w := INz −\n\n\n\n\n\n0\n\u0000IN ⊗˜W1\n\u0001\n0\n. . .\n0\n...\n0\n\u0000IN ⊗˜W2\n\u0001\n0\n...\n...\n...\n0\n...\n0\n0\n0\n. . .\n0\n\u0000IN ⊗˜WL\n\u0001\n\n\n\n\n\n˜f\n= I −Ω˜w ˜f\n(5)\nwhere the 0 matrices match in dimension, Ω˜w ∈RNz×(Nw+N·M) and\n˜f ◦z :=\n\u0010\nz⊤\n1 , ˜f1(z⊤\n2 ), . . . , ˜fL−1(z⊤\nL ), vec(X)⊤\u0011⊤\n.\nNow, with P1z = z1, (4) becomes\nmax\nu\nmin\n˜w,z\n\u001a\nφ( ˜w) + L(y, P1z) + 1\n2∥∆˜wz + u∥2\nΛµ −1\n2∥u∥2\nΛµ\n\u001b\n(6)\nIn terms of ˜w, our problem is\nmax\nu\nmin\n˜w,z\n\u001a\nφ( ˜w) + L(y, P1z) + 1\n2∥∆z ˜w −z −u∥2\nΛµ −1\n2∥u∥2\nΛµ\n\u001b\n(7)\nwhere\n∆z :=\nL\nM\nn=1\n\u0010\n˜fn(Z⊤\nn+1) ⊗INn\n\u0011\n∈RNz×Nw .\n(8)\nNote the relationship between the two operators, i.e.\n(I + ∆˜w) z = ∆z ˜w .\nEquations (6) and (7) provides a simple form that shows how our problem dif-\nfers from the problems commonly considered in the basic operator splitting liter-\nature, in which ADMM, Douglas-Rachford and the inexact-Uzawa techniques are\ndeveloped. In these cases the general constraint is usually given as Aw + Bz = c,\n12\nfor linear operators A and B. Our problem involves an operator, ∆˜w, that intro-\nduces a multiplicative relationship between the primal variable ˜w and the dual z.\nThis form could be interpreted as–or related to–a bi-convex problem (for convex\nfl, naturally), especially when ∆˜w is bi-afﬁne Boyd et al. [2011].\n4.1\nProximal Operators and ADMM\nThe proximal operator is deﬁned by\nprox\nΛg\n(x) := argmin\nz\n\u001a\ng(z) + 1\n2∥z −x∥2\nΛ\n\u001b\nfor positive deﬁnite Λ. Normally, Λ = λI and the operator reduces to\nprox\nλg\n(x) := argmin\nz\n\u001a\ng(z) + λ\n2 ∥z −x∥2\n\u001b\n.\nWhen Λ is diagonal and positive, such as Γµl above, one could simply use its in-\nverse to rescale the terms in the problem (effectively g(z) →g(Λ−1/2z) := ˜g(z)\nand x →Λ−1/2x); however, we use the matrix inner-product norm for notational\nconvenience and the implication of wider applicability.\nThe proximal operator enjoys many convenient properties, especially for lower\nsemi-continuous, convex g, and has strong connections with numerous optimiza-\ntion techniques and ﬁxed-point methods [Boyd and Vandenberghe, 2009, Com-\nbettes and Pesquet, 2011].\nThe form of (4) in zl reﬂects the deﬁnition of the proximal operator, and, after\nsome manipulation, the same is true for ˜Wl. This allows us to apply the iterative\ntechniques surrounding proximal operators in what follows.\nOne advantage of our approach is its embarrassingly parallel nature. The aug-\nmented Lagrangian leads to a block update of (W|Z, U). For example, if we add\nthe traditional ℓ2-ridge penalty we have a Bayes ridge regression update for the\nblock ( ˜W1, . . . , ˜WL). Proximal algorithms can also be used for non-smooth penal-\nties. Our method is also directly scalable and the vectorization of our method\nallows implementation in large scale tensor libraries such as Theano or Torch.\n1. Z(t) given\nn\nZ(t−1), W(t−1), U (t−1)o\nThe problem, in vectorized form, is\nargmin\nz\n\u001a\nL(y, P1z) + 1\n2∥∆˜wz + u∥2\nΛµ\n\u001b\nwhich, given the design of ∆˜w, is not a simple proximal operator. Even if\n∆˜w resulted in a simple diagonal matrix, the proximal operator of L(y, P1z)\nmay not be easy to evaluate. Regardless, if this minimum is found using\nproximal approaches, it would likely be through another phase of splitting,\n13\nbe it ADMM for the subproblem, or forward-backward/proximal gradient\niterations.\nTo illustrate the forward-backward approach we let F(z) = 1\n2∥∆˜wz + u∥2\nΛµ\nand note that the Jacobian matrix, D F(z), is given by\nD F(z) =\n\u0012\nI −Ω˜w\n∂˜f\n∂z⊤\n\u0013⊤\nΛµ\n\u0010\n∆˜wz(t) + u(t)\u0011\n=\n\u0012\nI −Ω˜w\n∂˜f\n∂z⊤\n\u0013⊤\nΛµ\n\u0010\nz(t) −Ω˜w ˜f (z(t)) + u(t)\u0011\n(9)\nand use gradient step (note that D f (z) = ∇⊤f (z))\ns = z(t) −γ∇F(z(t))\n(10)\nz(t+1) = s + P⊤\n1\n \nprox\nγL(y,·)\n(P1s) −P1s\n!\n(11)\nSimple forward-backward can’t be expected to work well for all moderate\nto high-dimensional problems, and especially not for functions with more\nextreme non-linearities. Given the composite nature of F(z), the sensitivity\nand condition of this problem could vary drastically, so one will have to tailor\ntheir approach to account for this.\n2. W(t) given\nn\nZ(t), W(t−1), U (t−1)o\nFrom (7), the problem is\nargmin\n˜w\n\u001a\nφ( ˜w) + 1\n2∥∆z ˜w −(z + u)∥2\nΛµ\n\u001b\n.\nIt is easier to see the relationships between terms when operating on a single\nlayer, and since this sub-problem is separable by layer, we proceed condi-\ntionally on l. Let ˜fi,l := ˜fl(zi,l+1) then\nargmin\n˜wl\n\u001a\nφ( ˜wl) +\nN\n∑\ni=1\nµi,l\n2\n\r\r\r\n\u0010\n˜f ⊤\ni,l ⊗INl\n\u0011\n˜wl −(zi,l + ui,l)\n\r\r\r\n2\u001b\n= prox\nΛwl φ\n\u0010\nΛ†\nwl\n\u0000 ˜fl(Zl+1) Diag(µl) ⊗INl\n\u0001 (zl + ul)\n\u0011\n= prox\nΛwl φ\n\u001a\nΛ†\nwl vec\n\u0000(Zl + UL) Diag(µl) ˜fl(Z⊤\nl+1)\n\u0001\u001b\n(12)\n14\nwhere\nΛwl =\nN\n∑\ni=1\nµi,l\n\u0000 ˜fi,l ⊗INl\n\u0001 \u0010\n˜f ⊤\ni,l ⊗INl\n\u0011\n=\nN\n∑\ni=1\nµi,l\n\u0010\n˜fi,l ˜f ⊤\ni,l ⊗INl\n\u0011\n= ˜fl(Zl+1) Diag(µl) ˜fl(Zl+1)⊤⊗INl\nand Λ†\nwl is a right pseudo-inverse. See Appendix A for details.\nThe resulting proximal problem involves a quadratic in ˜wl that is no longer\nstrictly diagonal in its squared term and, thus, isn’t necessarily a simple\nproximal operator to evaluate. The operator splitting that underlies ADMM,\nDouglas-Rachford and similar techniques is a common approach to this type\nof problem. Also, if full decompositions (e.g. SVD, eigen) of Λ†\nwl are reason-\nable to compute at each iteration, one could proceed by working in trans-\nformed ˜wl coordinates; however, the transform will induce a dependency\nbetween components of ˜wl in φ, which may require proximal solutions that\nare themselves difﬁcult to compute. The composite methods in Argyriou\net al. [2013], Chen et al. [2013] are designed for such transformed problems,\nat least when the transform is positive-deﬁnite, but given the dependency on\n˜fl, such conditions are not easy to guarantee in generality.\nOther approaches for solving this sub-problem are forward-backward iter-\nations and ADMM; each approach should be considered in light of ˜fi,l. A\nforward-backward approach may be trivial to implement, especially when\n˜fi,l has a known bound, but convergence can be prohibitively slow for poorly\nconditioned Λ ˜w. Some ADMM approaches can lead to faster convergence\nrates, but at the cost of repeated matrix inversions, which–for structured ma-\ntrix problems–may be trivial.\nFor example, a forward-backward approach for lower semi-continuous, con-\nvex φ would consist of the following ﬁxed-point iterations\n˜wl = prox\nλwφ\n( ˜wl −λw∇F( ˜wl))\nwhere λw ≥0 and\nF( ˜wl) = 1\n2 ˜w⊤\nl Λwl ˜wl −˜w⊤\nl dwl + c ˜wl\nwith ξi,l = zi,l + ui,l and\nd ˜wl =\nN\n∑\ni=1\n\u0000µi,l ˜fi,l ⊗INl\n\u0001\nξi,l,\nc ˜wl = 1\n2\nN\n∑\ni=1\nµi,l∥ξi,l∥2\n15\nIf ∇F( ˜wl) is Lipschitz continuous with constant γw, then λw ≥γw; otherwise,\nline-search can be used to ﬁnd a sufﬁcient γw at every iteration.\nAn ADMM approach could result in a single primal proximal step in F,\nwhich involves inversions of a quantity like λI + Λwl, and, given the form\nof Λwl, may be possible to compute via the well-known identity\n\u0010\nI + uv⊤\u0011−1\n= I −\nuv⊤\n1 + v⊤u .\n3. U (t) given\nn\nZ(t), W(t), U (t−1)o\nThis involves the standard cumulative error\nupdate for the augmented Lagrangian, which is\nu(t) = u(t−1) + ∆(t)\n˜w z(t)\n(13)\nOur approach requires a “consensus” across observations in Step 2, but the\nremaining steps are free to be processed asynchronously in observation dimension\nN. The structure of both ∆˜w and ∆z essentially determine the separability, since\nthey each act as “correlation” matrices between z and ˜w. Observing (5), we see that\nΩ˜w sums “horizontally” across layers, but in independent blocks of observations.\n4.2\nModel Selection and Architecture Design\nOne advantage of viewing a deep learning predictor as a Bayes MAP estimator is\nthat we can seamlessly apply Bayesian model selection tools to determine optimal\narchitecture design. We will provide a model selection criterion for choosing be-\ntween different link functions, size of the number of units together with the num-\nber of layers. Given the probabilistic data generating model, p(y| ˆy) and a deep\nlearning estimator ˆy(X , ˆ\nWL) based on L layers, we propose the use of an infor-\nmation criteria (IC) to gauge model quality. Such measures like AIC, corrected\nAIC Hurvich and Tsai [1991], BIC and others George and Foster [2000] have a long\nhistory as model selection criteria. From a Bayesian perspective, we can deﬁne\nthe Bayes factor as the ratio of marginal likelihoods and also provide an optimal\nmodel averaging approach to prediction.\nAn information measure for a deep learning predictor, ˆy, or equivalently a\ngiven architecture design, falls into a class of the form\nIC( ˆy(L)) = −2 log p(y| ˆy(X , ˆ\nWL) + c · df .\nwhere df is a measure of complexity or so-called degrees of freedom of a model and c\nis its cost. The degrees of freedom term can be deﬁned as d f := σ−2 ∑N\ni=1 Cov(yi, ˆyi)\nwhere σ2 is the model estimation error.\nThe intuition is simple–the ﬁrst term assesses in-sample predictive ﬁt. How-\never, over-ﬁtting is the curse of any nonlinear high dimensional prediction or\n16\nmodeling strategy and the second term penalises for the complexity in architec-\nture design–including the nonlinearities in the links, number of units and layer\ndepth. The combination of terms provides a simple metric for the comparison of\ntwo architecture designs–the best model provides the highest IC value.\nFor suitably stable predictors, ˆy, Stein [1981] provides an unbiased estimator of\nrisk using the identity df = E\n\u0010\n∑N\ni=1 ∂ˆyi/∂yi\n\u0011\n. Given the scalability of our algo-\nrithm, the derivative ∂ˆy/∂y is available using the chain rule for the composition of\nthe L layers and computable using standard tensor libraries such as Torch.\nEfron [1983] provides an alternative motivation for model selection by directly\naddressing the trade-off between minimising out-of-sample predictive error and\nin-sample ﬁt. Consider a nonparametric regression under ℓ2-norm. The in-sample\nmean squared error is err = ||y −ˆy||2 and the out-of-sample predictive MSE is\nErr = Ey⋆\u0000||y⋆−ˆy||2\u0001\nfor a future observation y⋆. In expectation we then have\nE (Err) = E (err + 2var( ˆy, y))\nThe latter term can be written in terms of df as a covariance. Stein’s unbiased risk\nestimate then becomes\nc\nErr = ||y −ˆy||2 + 2σ2\nn\n∑\ni=1\n∂ˆyi\n∂yi\n.\nModels with the best predictive MSE are favoured. This approach also provides\na predictive MSE criterion for optimal hyper-parameter selection in the prior reg-\nularisation penalty φ(W) and allow the researcher to gauge the overall amount\nof regularisation necessary to provide architectures that provide good predictive\nrules. In contrast, the current state-of-the-art is to use heuristic rules such as dropout.\n5\nApplications\nTo illustrate our methodology, we provide an illustration of multi-class deep logis-\ntic learning. We use logit/softmax activation functions which allows us to employ\nsome efﬁcient quadratic envelopes to linearize the functions in the proximal sub-\nproblems within our algorithm. We use Fisher’s Iris data to illustrate the compar-\nison between DL models and traditional classiﬁcation algorithms such as support\nvector machines.\n5.1\nMulti-Class Deep Logistic Regression\nSuppose that we have a multinomial loss with K-classes, where the observations\nare Y ∈RK×N, and fl(z) = σ(z) are all logistic functions. Now, (11) has an explicit\n17\nform as\nL(Y, Z1) =\nN\n∑\ni=1\n(\nlog\n N1\n∑\nk=1\neZk,i,1\n!\n−\nN1\n∑\nk=1\nYi,kZk,i,1\n)\n=\nN\n∑\ni=1\nn\nlog\n\u0010\n1⊤\nN1eZi,1\n\u0011\n−Y⊤\ni Zi,1\no\n= log(1⊤\nN1eZ1)1N −tr(Y⊤Z1)\n= 1⊤\nN log\n\u0010\u0010\nIN ⊗1⊤\nK\n\u0011\nez1\n\u0011\n−y⊤z1\nwith Z1 ∈RN1×N, N1 = K, y = vec(Y) and z1 = vec(Z1). Equation (11) is now\nz1 = argmin\ns\n\u001a\nγL(y, s) + 1\n2∥s −η∥2\nΛµ1\n\u001b\nwhere η = P1 (z −γ∇F(z)) and s ∈RN·N1×1 ′.\nSince this function has a Lipschitz bounded derivative with constant γ1 = 1/4, we\ncan use forward-backward to converge to a solution of the proximal problem. In\nthis case, the sub-problem forward-backward steps are\ns =\nprox\n∥·−η∥2\nΛµ1 /γ1\n\u0012\ns −γ\nγ1\n∇L(y, s)\n\u0013\n=\n\u0000I + Λµ1/γ1\n\u0001−1\n\u0012Λµ1\nγ1\nη + s −γ\nγ1\n∇L(y, s)\n\u0013\n=\nprox\nΛµ1 L(y,·)\n(η)\nwith p = ez1 ⊘(E1ez1 ⊗1K) and\n∇L(y, z1) = p −y\n∇2L(y, z1) = Diag(p) −pp⊤\nfor E1 = IN ⊗1⊤\nK and ⊘signifying element-wise division.\nAll of the methods described here require derivative and/or Hessian informa-\ntion at some stage in the sub-problem minimization steps, and for the logistic trans-\nform those quantities are easily obtained:\nD f (x) = Diag (σ(x) ⊙(1 −σ(x)))\nH f (x) = Diag (σ(x) ⊙(σ(x) −1) ⊙(1 −2σ(x)))\nwhere ⊙is the Hadamard/element-wise product. In the fully vectorized form\nof the model given in Section 4.1, these quantities will need to be augmented by\nthe structural properties of mappings like ˜f (z). Such operations are perhaps best\n18\nsuited for tensor and graph libraries, especially if they’re capable of distributed\nthe operations and handling the sparsity inherent to the model’s design. We do\nnot cover those details here, but our implementations have shown that standard\nsparse matrix libraries can be leveraged to perform such operations without too\nmuch coding effort.\n% non-zero w\nγw\nµl\n1.00\n0\n0.10\n0.54\n0.67\n0.10\n0.40\n1.33\n0.10\n0.40\n2\n0.10\n1.00\n0\n1.00\n0.40\n0.67\n1.00\n0.35\n1.33\n1.00\n0.29\n2\n1.00\n1.00\n0\n1.50\n0.33\n0.67\n1.50\n0.29\n1.33\n1.50\n0.28\n2\n1.50\nTable 2: Percentage of non-zero w entries for the ﬁnal parameter set in the ℓ1 pe-\nnalized model with logit activations and multinomial/softmax loss.\n5.2\nIris Data\nFisher’s Iris ﬂower dataset consists of four measurements from 50 observations\nof three species (K = 3) of Iris. The species are Iris setosa, Iris virginica and Iris\nversicolor and we have 150 total observations. The various Iris species have petals\nand sepals (the green sheaths at the base of the petals) of different lengths which\nare measured in cm and dim(X) = 4. The goal is to predict the label given by\nspecies name as a function of the four characteristics.\nTo illustrate our methodology, we use a deep learning architecture with a multi-\nnomial (or softmax) loss with a hidden logit layer with a softmax link for f1. The\nhidden layer uses 10 units and so we have L = 2 and N2 = 10. Section 5.1 provides\ndetails of the loss structure and objective function. To induce sparsity we use an\nℓ1 penalization. One purpose of our study is to show how a sparse model can per-\nform as well as a dense NN model. The construction of φ( ˜w) includes a sparsity\nparameter γw so that the penalty term is effectively\nφ( ˜w) := γw\nN ˜w\n∑\nj=1\n| ˜wj| .\nBy varying γw we can ﬁnd the full regularisation path for our deep learning model\nmuch in the same way as a traditional lasso regression.\n19\nmu=0.1\nmu=1\nmu=1.5\n50\n100\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\niter\nobjective values\nmeasure\nobj.dual\nobj.pri\ngamma.w\n0\n0.67\n1.33\n2\nFigure 1: Primal and dual objective values for the ℓ1 penalized model with logit\nactivations and multinomial/softmax loss.\nThe parameters are estimated from 70% of the data, i.e. the training sample,\nleaving a hold-out test sample of 30%. For the same training and testing data,\nwe ﬁnd that the neural net library nnet Venables and Ripley [2002] in R gives a\ntest error of 92% on average, which is comparable to the results produced by our\nmodel under the tested conﬁgurations.\nFigure 1 shows the primal and dual objective values across iterations and pa-\nrameter values. Figure 2 shows the classiﬁcation rates. Table 2 lists the percentage\nof non-zero entries in w at the end of each ﬁt across the given parameters. From the\nplot we can see that the ℓ1 penalization has a noticeable effect over the given range\nof γw, and referring back to Figure 2 we see that classiﬁcation rates comparable to\nthe dense w case (i.e. γw = 0) are obtained for sparse w.\n6\nDiscussion\nDeep Learning provides an exciting new area for nonlinear prediction rules in\nmany applications in image processing and machine learning. High dimensional\ndata reduction is achieved using a set of hidden layer variables. Our estimation\nmethodology uses a proximal splitting techniques that leads to efﬁcient imple-\nmentation. Our methods apply to non-convex non-differentiable regularisation\npenalties. The full regularisation path is available as is hyper-parameter selection\nvia a statistical predictive mean squared error criterion.\n20\n0.4\n0.6\n0.8\n1.0\n0.4\n0.6\n0.8\n1.0\n0.7\n0.8\n0.9\nmu=0.1\nmu=1\nmu=1.5\n0\n10\n20\n30\n40\n50\niter\nclassification rates\ngamma.w\n0\n0.67\n1.33\n2\nmeasure\ntest.class.pct\ntrain.class.pct\nFigure 2: Training and testing classiﬁcation rates for the ℓ1 penalized model with\nlogit activations and multinomial/softmax loss.\nThere are many areas of future application. First, there’s a range of models such\nas convolution neural nets where tailored ADMM methods can be constructed.\nAdding acceleration methods such as Nesterov [1983] to proximal operator steps\nand understanding the speed of convergence of these algorithms provides another\nares of fruitful future research. There are also alternative methods for handling the\ngeneral constrained problem in (3). The augmented Lagrangian can be deﬁned in\nterms of a general penalty instead of the customary ℓ2 term. It would be worth-\nwhile to investigate which other penalties work well, if not better, for the problems\ndiscussed here. As well, there is a possible relation between the dropout technique\nin Deep Learning and certain types of regularization on Wl and/or zl. For that\nmatter, regularization on zl hasn’t been explored in this context, yet it may provide\nan automatic means of exploring the space of splitting designs.\nOne fruitful area of research is to develop tailored Markov chain Monte Carlo\n(MCMC) methods to allow one to correctly assess uncertainty bounds and pro-\nvide more efﬁcient predictive rules. For example, a multi-class logistic regression\ncan be implemented in MCMC using a Polya-Gamma data augmentation Polson\net al. [2013] scheme. Another advantage of our regularisation approach frame-\nwork is the application to optimal selection of hyper-parameters deﬁned as the\noverall amount of regularisation applied to the parameter weights. The methods\nin Pereyra [2013] can be used to combine proximal approach with MCMC to obtain\na full description of the uncertainty in parameter estimation of the weights.\n21\nThere are many other areas of future research. For example, tailoring algo-\nrithms for other classes of neural net models such as recurrent neural networks,\nsee Graves and others [2012] is as interesting area of study. There is also a large\nliterature in statistics on non-parametrically estimating the link functions for shal-\nlow architecture models. Until now, deep learning pre-speciﬁes the link and it’s\nan open problem to see whether one can consistently estimate this in a deep layer\nmodel.\n22\nReferences\nAndreas Argyriou, St´ephan Cl´emenccon, and Ruocong Zhang. Learning the graph\nof relations among multiple tasks.\n2013.\nURL https://hal.inria.fr/hal-\n00940321/.\nFrancis Bach. Breaking the curse of dimensionality with convex neural networks,\n2014.\nAndrew R. Barron. Universal approximation bounds for superpositions of a sig-\nmoidal function. Information Theory, IEEE Transactions on, 39(3):930–945, 1993.\nURL http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=256500.\nDimitri P. Bertsekas.\nMultiplier methods:\na survey.\nAutomatica, 12(2):\n133–145, 1976.\nURL http://www.sciencedirect.com/science/article/pii/\n0005109876900777.\nGuillaume Bouchard. Efﬁcient bounds for the softmax function and applications to\napproximate inference in hybrid models. pages 1–9, 2007. URL http://eprints.\npascal-network.org/archive/00003498/.\nStephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge univer-\nsity press, 2009.\nStephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.\nDistributed optimization and statistical learning via the alternating direction\nmethod of multipliers. Foundations and Trends R⃝in Machine Learning, 3(1):1–122,\n2011. URL http://dl.acm.org/citation.cfm?id=2185816.\nLeo Breiman. Statistical modeling: The two cultures (with comments and a re-\njoinder by the author).\nStatistical Science, 16(3):199–231, 2001.\nURL http:\n//projecteuclid.org/euclid.ss/1009213726.\nPeijun Chen, Jianguo Huang, and Xiaoqun Zhang. A primal–dual ﬁxed point al-\ngorithm for convex separable minimization with applications to image restora-\ntion.\nInverse Problems, 29(2):025011, 2013.\nISSN 0266-5611.\ndoi: 10.1088/\n0266-5611/29/2/025011. URL http://stacks.iop.org/0266-5611/29/i=2/a=\n025011?key=crossref.c7722e5930429604a926fdc1944123e0.\nPatrick L Combettes and Jean-Christophe Pesquet. Proximal splitting methods in\nsignal processing. Fixed-point algorithms for inverse problems in science and engi-\nneering, pages 185–212, 2011.\nR. Dennis Cook.\nFisher lecture: Dimension reduction in regression.\nStatistical\nScience, pages 1–26, 2007. URL http://www.jstor.org/stable/27645799.\nR. Dennis Cook and Hakbae Lee.\nDimension reduction in binary response\nregression.\nJournal of the American Statistical Association, 94(448):1187–1200,\n23\n1999.\nURL http://amstat.tandfonline.com/doi/abs/10.1080/01621459.\n1999.10473873.\nAnthony Christopher Davison.\nStatistical models, volume 11.\nCambridge\nUniversity Press, 2003.\nURL https://books.google.com/books?hl=en&lr=\n&id=gQyIGGAiN4AC&oi=fnd&pg=PR9&dq=davison+statistical+models&ots=\n0UbFWpefvz&sig=-NXdEmD8vBq9ct4oF74R6NeW4Jk.\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,\nAndrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and others.\nLarge scale\ndistributed deep networks.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1223–1231, 2012. URL http://papers.nips.cc/paper/4687-large-\nscale-distributed-deep-networks.\nP. Dellaportas and Adrian FM Smith. Bayesian inference for generalized linear\nand proportional hazards models via gibbs sampling. Applied Statistics, pages\n443–459, 1993. URL http://www.jstor.org/stable/2986324.\nBradley Efron. Estimating the error rate of a prediction rule: Improvement on\ncross-validation. Journal of the American Statistical Association, 78(382):316–331,\n1983.\nClement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning\nhierarchical features for scene labeling. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on, 35(8):1915–1929, 2013. URL http://ieeexplore.ieee.org/\nxpls/abs_all.jsp?arnumber=6338939.\nAlexander Genkin, David D. Lewis, and David Madigan. Large-scale bayesian lo-\ngistic regression for text categorization. Technometrics, 49(3):291–304, 2007. URL\nhttp://amstat.tandfonline.com/doi/abs/10.1198/004017007000000245.\nEdwardI George and Dean P. Foster. Calibration and empirical bayes variable se-\nlection. Biometrika, 87(4):731–747, 2000. URL http://biomet.oxfordjournals.\norg/content/87/4/731.short.\nAlex Graves and others. Supervised sequence labelling with recurrent neural networks,\nvolume 385. Springer, 2012. URL http://link.springer.com/content/pdf/10.\n1007/978-3-642-24797-2.pdf.\nP. J. Green, K. \\Latuszy´nski, M. Pereyra, and C. P. Robert. Bayesian computation: a\nperspective on the current state, and sampling backwards and forwards. ArXiv\ne-prints, February 2015.\nGeoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of\ndata with neural networks. Science, 313(5786):504–507, 2006. URL http://www.\nsciencemag.org/content/313/5786/504.short.\n24\nGeoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm\nfor deep belief nets. Neural computation, 18(7):1527–1554, 2006. URL http://www.\nmitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527.\nClifford M. Hurvich and Chih-Ling Tsai. Bias of the corrected AIC criterion for\nunderﬁtted regression and time series models. Biometrika, 78(3):499–509, 1991.\nURL http://biomet.oxfordjournals.org/content/78/3/499.short.\nDavid\nA.\nKnowles\nand\nTom\nMinka.\nNon-conjugate\nvariational\nmes-\nsage passing for multinomial and binary regression.\nIn Advances in\nNeural\nInformation\nProcessing\nSystems,\npages\n1701–1709,\n2011.\nURL\nhttp://papers.nips.cc/paper/4407-non-conjugate-variational-message-\npassing-for-multinomial-and-binary-regression.\nAndreuı Kolmogorov. The representation of continuous functions of many vari-\nables by superposition of continuous functions of one variable and addition.\nDokl. Akad. Nauk SSSR, 114:953–956, 1957.\nYann A. LeCun, L´eon Bottou, Genevieve B. Orr, and Klaus-Robert M¨uller. Efﬁcient\nbackprop. In Neural networks: Tricks of the trade, pages 9–48. Springer, 2012. URL\nhttp://link.springer.com/chapter/10.1007/978-3-642-35289-8_3.\nAdrian S. Lewis and Stephen J. Wright. A proximal method for composite min-\nimization. arXiv preprint arXiv:0812.0423, 2008. URL http://arxiv.org/abs/\n0812.0423.\nEdward N. Lorenz. Deterministic nonperiodic ﬂow. Journal of the atmospheric sci-\nences, 20(2):130–141, 1963.\nURL http://journals.ametsoc.org/doi/abs/10.\n1175/1520-0469(1963)020%3C0130:DNF%3E2.0.CO;2.\nDavid Madigan, Alexander Genkin, David D. Lewis, Dmitriy Fradkin, and oth-\ners. Bayesian multinomial logistic regression for author identiﬁcation. Bayesian\nInference and Maximum Entropy Methods in Science and Engineering, 803:509–516,\n2005.\nURL http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.\n60.2085&rep=rep1&type=pdf.\nJames\nMartens\nand\nIlya\nSutskever.\nLearning\nrecurrent\nneural\nnet-\nworks with hessian-free optimization.\nIn Proceedings of the 28th In-\nternational\nConference\non\nMachine\nLearning\n(ICML-11),\npages\n1033–1040,\n2011.\nURL\nhttp://machinelearning.wustl.edu/mlpapers/paper_files/\nICML2011Martens_532.pdf.\nJonathan Masci, Alessandro Giusti, Dan Ciresan, Gabriel Fricout, and J¨urgen\nSchmidhuber.\nA fast learning algorithm for image segmentation with max-\npooling convolutional networks. In Image Processing (ICIP), 2013 20th IEEE Inter-\nnational Conference on, pages 2713–2717. IEEE, 2013. URL http://ieeexplore.\nieee.org/xpls/abs_all.jsp?arnumber=6738559.\n25\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep re-\ninforcement learning. arXiv preprint arXiv:1312.5602, 2013. URL http://arxiv.\norg/abs/1312.5602.\nYurii Nesterov. A method of solving a convex programming problem with conver-\ngence rate $o(1/kˆ2)$. In Soviet Mathematics Doklady, volume 27, pages 372–376,\n1983.\nJiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, Quoc V. Le, and\nAndrew Y. Ng.\nOn optimization methods for deep learning.\nIn Proceedings\nof the 28th International Conference on Machine Learning (ICML-11), pages 265–\n272, 2011. URL http://machinelearning.wustl.edu/mlpapers/paper_files/\nICML2011Le_210.pdf.\nMark JL Orr. Regularization in the selection of radial basis function centers. Neural\ncomputation, 7(3):606–623, 1995. URL http://www.mitpressjournals.org/doi/\nabs/10.1162/neco.1995.7.3.606.\nRobert L. Paige and Ronald W. Butler.\nBayesian inference in neural networks.\nBiometrika, 88(3):623–641, 2001.\nURL http://biomet.oxfordjournals.org/\ncontent/88/3/623.short.\nNeal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in\nOptimization, 1(3):123–231, 2014. ISSN 2167-3888. doi: 10.1561/2400000003.\nMarcelo Pereyra. Proximal markov chain monte carlo algorithms. arXiv preprint\narXiv:1306.0187, 2013. URL http://arxiv.org/abs/1306.0187.\nTomaso Poggio and Federico Girosi. Networks for approximation and learning.\nProceedings of the IEEE, 78(9):1481–1497, 1990. URL http://ieeexplore.ieee.\norg/xpls/abs_all.jsp?arnumber=58326.\nNicholas G. Polson and James G. Scott. Data augmentation for non-gaussian re-\ngression models using variance-mean mixtures. Biometrika, 100(2):459–471, 2013.\nISSN 00063444. doi: 10.1093/biomet/ass081.\nNicholas G. Polson, Steven L. Scott, and others.\nData augmentation for sup-\nport vector machines.\nBayesian Analysis, 6(1):1–23, 2011.\nURL http://\nprojecteuclid.org/euclid.ba/1339611936.\nNicholas G. Polson, James G. Scott, and Jesse Windle. Bayesian inference for logis-\ntic models using p´olya–gamma latent variables. Journal of the American Statistical\nAssociation, 108(504):1339–1349, 2013. URL http://www.tandfonline.com/doi/\nabs/10.1080/01621459.2013.829001.\nNicholas G. Polson, James G. Scott, and Brandon T. Willard. Proximal algorithms\nin statistics and machine learning. pages 1–43, 2015.\n26\nBrian D. Ripley.\nPattern recognition and neural networks.\nCambridge uni-\nversity\npress,\n1996.\nURL\nhttps://books.google.com/books?hl=en&lr=\n&id=2SzT2p8vP1oC&oi=fnd&pg=PR9&dq=ripley+neural+network&ots=\nJLTUg5dRXL&sig=mTMtffAcI4p1lpotV60k6c3vMWk.\nCharles M. Stein. Estimation of the mean of a multivariate normal distribution.\nThe Annals of Statistics, 9(6):1135–1151, 1981. ISSN 0090-5364. doi: 10.1214/aos/\n1176345632.\nD. M. Titterington. Bayesian methods for neural networks and related models.\nStatistical Science, 19(1):128–139, 2004.\nW. N. Venables and B. D. Ripley. Modern Applied Statistics with S. Springer, New\nYork, fourth edition, 2002. URL http://www.stats.ox.ac.uk/pub/MASS4. ISBN\n0-387-95457-0.\nWeiran Wang and Miguel A. Carreira-Perpin´an. Nonlinear low-dimensional re-\ngression using auxiliary coordinates. In International Conference on Artiﬁcial In-\ntelligence and Statistics, pages 1295–1304, 2012. URL http://machinelearning.\nwustl.edu/mlpapers/paper_files/AISTATS2012_WangC12.pdf.\nJesse Windle, Carlos M. Carvalho, James G. Scott, and Liang Sun. Efﬁcient data\naugmentation in dynamic models for binary and count data.\narXiv preprint\narXiv:1308.0774, 2013. URL http://arxiv.org/abs/1308.0774.\nHerman Wold. Causal inference from observational data: A review of end and\nmeans. Journal of the Royal Statistical Society. Series A (General), pages 28–61, 1956.\nURL http://www.jstor.org/stable/2342961.\nA\nBlock Wl Steps\nIn this section we expand the quadratic term in (12). Using the indexed form of (3)\nand\nfi,l := fl(zi,l+1),\nWl fi,l =\n\u0010\nf ⊤\ni,l ⊗INl\n\u0011\nvec(Wl),\nwl := vec(Wl)\nwe obtain\nN\n∑\ni=1\nµi,l\n2 ∥zi,l −Wl fl(zi,l+1) −bl + ui,l∥2 =\nN\n∑\ni=1\nµi,l\n2\n\r\r\r\n\u0010\nf ⊤\ni,l ⊗INl\n\u0011\nwl −(zi,l + ui,l −bl)\n\r\r\r\n2\nLetting ξi,l = zi,l + ui,l −bl and expanding, we get\n1\n2w⊤\nl Λwlwl −w⊤\nl dwl + cwl\n27\nfor\nΛwl =\nN\n∑\ni=1\nµi,l fi,l f ⊤\ni,l ⊗INl = fl(Zl+1) Diag(µl) fl(Zl+1)⊤⊗INl\ndwl =\nN\n∑\ni=1\n\u0000µi,l fi,l ⊗INl\n\u0001\nξi,l =\n\u0000fl(Zl+1) Diag(µl) ⊗INl\n\u0001\nξl\n= vec\n\u0010\n(Zl + UL) Diag(µl) fl(Z⊤\nl+1)\n\u0011\n−fl(Zl+1)µl ⊗bl\ncwl = 1\n2\nN\n∑\ni=1\nµi,l∥ξi,l∥2 = 1\n2∥ξl∥2\nΛµl\nwhere ξl = zl + ul −(1N ⊗bl). Similarly, we can decompose these terms:\nΛwl = H⊤\nwl Hwl ⊗INl =\n\u0000Hwl ⊗INl\n\u0001⊤\u0000Hwl ⊗INl\n\u0001\ndwl = H⊤\nwl Ml ⊗INl =\n\u0000Hwl ⊗INl\n\u0001⊤\u0000Ml ⊗INl\n\u0001\nwhere Ml := Diag(√µl) ∈RN×N and Hwl := Ml fl(Z⊤\nl+1) ∈RNl+1.\n28\n",
  "categories": [
    "stat.ML"
  ],
  "published": "2015-09-20",
  "updated": "2015-09-20"
}