{
  "id": "http://arxiv.org/abs/1509.08038v1",
  "title": "Deep Trans-layer Unsupervised Networks for Representation Learning",
  "authors": [
    "Wentao Zhu",
    "Jun Miao",
    "Laiyun Qing",
    "Xilin Chen"
  ],
  "abstract": "Learning features from massive unlabelled data is a vast prevalent topic for\nhigh-level tasks in many machine learning applications. The recent great\nimprovements on benchmark data sets achieved by increasingly complex\nunsupervised learning methods and deep learning models with lots of parameters\nusually requires many tedious tricks and much expertise to tune. However,\nfilters learned by these complex architectures are quite similar to standard\nhand-crafted features visually. In this paper, unsupervised learning methods,\nsuch as PCA or auto-encoder, are employed as the building block to learn filter\nbanks at each layer. The lower layer responses are transferred to the last\nlayer (trans-layer) to form a more complete representation retaining more\ninformation. In addition, some beneficial methods such as local contrast\nnormalization and whitening are added to the proposed deep trans-layer networks\nto further boost performance. The trans-layer representations are followed by\nblock histograms with binary encoder schema to learn translation and rotation\ninvariant representations, which are utilized to do high-level tasks such as\nrecognition and classification. Compared to traditional deep learning methods,\nthe implemented feature learning method has much less parameters and is\nvalidated in several typical experiments, such as digit recognition on MNIST\nand MNIST variations, object recognition on Caltech 101 dataset and face\nverification on LFW dataset. The deep trans-layer unsupervised learning\nachieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per\nclass and 75.98% accuracy on 30 samples per class on Caltech 101 dataset,\n87.10% on LFW dataset.",
  "text": "arXiv:1509.08038v1  [cs.NE]  27 Sep 2015\nDeep Trans-layer Unsupervised Networks for\nRepresentation Learning\nWentao Zhua, Jun Miaoa, Laiyun Qingb, Xilin Chena\naKey Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS),\nInstitute of Computing Technology, CAS, Beijing 100190, China\nbSchool of Computer and Control Engineering, University of Chinese Academy of Sciences,\nBeijing 100049, China\nAbstract\nLearning features from massive unlabelled data is a vast prevalent topic for high-\nlevel tasks in many machine learning applications. The recent great improve-\nments on benchmark data sets achieved by increasingly complex unsupervised\nlearning methods and deep learning models with lots of parameters usually re-\nquires many tedious tricks and much expertise to tune. However, ﬁlters learned\nby these complex architectures are quite similar to standard hand-crafted fea-\ntures visually. In this paper, unsupervised learning methods, such as PCA or\nauto-encoder, are employed as the building block to learn ﬁlter banks at each\nlayer. The lower layer responses are transferred to the last layer (trans-layer) to\nform a more complete representation retaining more information. In addition,\nsome beneﬁcial methods such as local contrast normalization and whitening are\nadded to the proposed deep trans-layer networks to further boost performance.\nThe trans-layer representations are followed by block histograms with binary en-\ncoder schema to learn translation and rotation invariant representations, which\nare utilized to do high-level tasks such as recognition and classiﬁcation. Com-\npared to traditional deep learning methods, the implemented feature learning\nmethod has much less parameters and is validated in several typical experiments,\nsuch as digit recognition on MNIST and MNIST variations, object recognition\non Caltech 101 dataset, face veriﬁcation on LFW dataset. The deep trans-layer\nunsupervised learning achieves 99.45 % accuracy on MNIST dataset, 67.11 %\naccuracy on 15 samples per class and 75.98 % accuracy on 30 samples per class\non Caltech 101 dataset, 87.10 % on LFW dataset.\nKeywords:\nUnsupervised feature learning, deep representation learning,\ntrans-layer neural networks, no ﬁne-tuning representation learning\n2010 MSC: 00-01, 99-00\nEmail addresses: wentao.zhu@vipl.ict.ac.cn (Wentao Zhu), jmiao@ict.ac.cn (Jun\nMiao), lyqing@ucas.ac.cn (Laiyun Qing), xlchen@ict.ac.cn (Xilin Chen)\nPreprint submitted to ****\nJune 14, 2021\n1. Introduction\nAlmost all high-layer tasks such as classiﬁcation, recognition and veriﬁcation\nrequire us to design ﬁne representations for their speciﬁc aims. For classiﬁcation\nof images taken from the wild, numerous factors in the environment, such as\ndiﬀerent lighting conditions, occlusions, corruptions and deformations, lead to\nlarge amount of intra-class variability in images. Good representations should\nreduce such non-informative intra-class variability, whilst preserving discrimina-\ntive information across classes. However, designing good feature representations\nis a quite tough and diﬃcult procedure for pattern recognition tasks, which is\na hot topic in machine learning ﬁeld.\nThe research of feature representations mainly contains two aspects, hand-\ncrafted feature designing and automatic feature learning. Researchers and en-\ngineers made enormous eﬀorts to devise robust feature representations at their\nown domains a decade ago.\nMany successful features are proposed such as\nSIFT [1] and HoG [2] features in computer vision domain.\nHowever, these\nhand-crafted features have poor transfer ability over domains. Novel features\nneed to be redesigned elaborately when the domain of application is changed.\nThe other way is representation learning, which is a quite prevalent topic after\ndeep learning coming out [3]. Nevertheless, these fully learned representations\nby multi-layer unsupervised learning followed by a ﬁne-tuning procedure have\ntoo many parameters to be tuned, and require much expertise knowledge and\nsophisticated hardware support to train a long time.\nIn this paper, we demonstrate a novel trans-layer neural network with quite\nsimple and the most classical unsupervised learning method, PCA or auto-\nencoder, as the building block. Diﬀerent from the PCANet [4], a one-by-one\ntwo layer PCA network, the responses of the previous layer of our model are\nconcatenated to that of the last layer to form a more complete representation.\nSuch trans-layer connections make up the rapid information loss in the cascade\nunsupervised learning eﬀectively. In addition, the local contrast normalization\n[5] and whitening are added in our trans-layer unsupervised network to boost\nits learning ability, which are commonly used in deep neural networks [6]. The\ndiﬀerence between the implemented deep trans-layer unsupervised network and\nconventional networks is that the deep trans-layer unsupervised network requires\nno back propagation information to ﬁne-tune the feature banks.\nExperimental results indicate that the implemented trans-layer connection\nscheme boosts the deep trans-layer unsupervised network eﬀectively, and com-\nmonly used local contrast normalization and whitening also contribute to the\nperformances. The demonstrated deep trans-layer unsupervised network is val-\nidated on digit recognition and object recognition tasks.\nQuite surprisingly,\nthe stacked conventional unsupervised learning with trans-layer representations\nachieves 99.45 % accuracy on MNIST dataset, and 67.11 % accuracy on 15\nsamples per class and 75.98 % accuracy on 30 samples per class on Caltech 101\ndataset [7].\nWe will start by reviewing the related works on feature learning and rep-\nresentation in Section 2. Then the idea of the deep trans-layer unsupervised\n2\nnetwork, including the pre-processing and trans-layer unsupervised learning, is\nillustrated detailedly in Section 3. How to use the deep trans-layer unsupervised\nnetwork to extract features and tackle applications is also described in Section\n3. The experimental results and comparative analysis on MNIST, MNIST vari-\nations and Caltech 101 datasets are presented on Section 4. Finally, discussions,\nconclusions and the future work are summarized in Section 5 and Section 6.\n2. Related works\nMuch research has been conducted to pursuit a good representation by man-\nually designing elaborative low-level features, such as LBPH feature [8], SIFT\nfeature [1] and HoG feature [2] in computer vision ﬁeld. However, these hand-\ncrafted features cannot be easily adapted to new conditions and tasks, and\nredesigning them usually requires novel expertise knowledge and tough studies.\nLearning good feature representations is probably a promising way to han-\ndle the required elaborative and expertise problem in devising hand-crafted\nfeatures. Much recent work in machine learning has focused on how to learn\ngood feature representations from massive unlabelled data, and great progresses\nhave been made by the methods [9, 10]. The main idea of deep models is to\nlearn multi-level features at diﬀerent layers. High-level features generated in\nthe upper-layer are expected to extract more complex and abstract semantics of\ndata, and more invariance to intra-class variability, which is quite useful to high-\nlevel tasks. These deep learning methods typically learn multi-level features by\ngreedily “pre-training” each layer using the speciﬁc unsupervised learning, and\nthen ﬁne-tuning the pre-trained features by stochastic gradient descent (SGD)\nmethod with supervised information [3], [5]. However, these deep architectures\nhave numerous parameters such as the number of features to learn and param-\neters of unsupervised learning in each layer. Besides, the SGD also has various\nparameters such as momentum, weight decay rate, learning rate, and extra pa-\nrameters including the Dropout rate or DropConnet rate in recently proposed\nconvolution deep neural networks (ConvNets) [9, 10].\nThere is also some work on conventional unsupervised learning methods\nwith only single layer [6, 11]. The main idea of these methods is to learn over\ncomplete representations with dense features. Although these methods have\nmade much progress on benchmark datasets with almost no hyper parameters,\nthese single layer unsupervised representational learning models require over\ncomplete features of dimensions as high as possible, and the parameters need\nto be elaborately chosen in order to obtain satisfactory results [6].\nA major drawback of deep learning methods with ﬁne-tuning for stacking\nrepresentations is the consuming of expensive computational resources and high\ncomplexities of the models. One intuition is that, since the elaborately learned\nfeatures are quite similar to some conventional unsupervised features, such as\nwavelets, PCA and auto encoder, why not jump over the tough and time-\nconsuming parameter ﬁne-tuning procedure and take those features stacked as\nthe representation directly. Furthermore, more robust invariant features can be\n3\nbetter devised other than various pooling strategies. Wavelet scattering net-\nworks (ScatNet) are such networks with pre-ﬁxed wavelet ﬁlter banks in the\ndeep convolution architectures [12]. The ScatNets have quite solid mathemat-\nical analysis of their rotation and translation invariants at any scale.\nMore\nsurprisingly, superior performance over ConvNet and DNNs is obtained by the\nScatNets with no ﬁne-tuning phase. However, the ScatNet is shown to have\ninferior performance in large intra-class variability including great illumination\nchanging and corruption such as face related tasks [4].\nThe other non-propagation deep network with pre-ﬁxed feature banks is the\nPCANet [4]. The PCANet uses two layer cascaded linear networks with na¨ıve\nPCA ﬁlter banks to extract more complex features.\nThe output of the two\nlayer cascaded PCA network is processed by the quantized histogram units.\nThe PCANet presents a superior or highly comparable performance over other\nmethods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recog-\nnition tasks with large occlusion, illumination, expression and pose changes. In\naddition, the PCANet has quite fewer parameters, much faster learning speed\nand much more reliable than the currently widely researched ConvNets and\nDNNs, which is much more convenient and more practical to applications [4].\nHowever, the cascaded PCA structure in PCANet will face great information\nloss and corruption after multi-layer transformation, which will be illustrated\nin Section 5. The current prevalent deep networks are also probably facing the\nsame problem that lower layer’s discriminative information will be lost after\nlayers’ transformation. This leads to inferior results of PCANet on conventional\nobject recognition tasks.\nThis paper will tackle the multi-layer information loss problem by demon-\nstrating a novel trans-layer structure based on multi-layer conventional unsuper-\nvised ﬁlter banks. The local contrast normalization and whitening operations\nare applied to ameliorate the unsupervised learning in the deep trans-layer net-\nwork. Thus the trans-layer unsupervised network forms a more complete and\neﬀective representation, whilst retaining the advantages such as fewer parame-\nters, faster learning speed and more reliable performance. Also, histogram op-\neration is adopted to preserve translation and rotation invariance after binary\nquantization. Experimental results conﬁrm that the deep trans-layer unsuper-\nvised network boosts the performance of conventional unsupervised learning,\nand it learns eﬀective feature representations that achieve 99.45 % accuracy on\nMNIST dataset, and 67.11 % accuracy on 15 samples per class and 75.98 %\naccuracy on 30 samples per class on Caltech 101 dataset.\n3. Deep trans-layer unsupervised network\nIn this section, we present a novel framework, the deep trans-layer unsu-\npervised network, for feature learning and representation. The framework of\nproposed deep trans-layer unsupervised network is illustrated in Figure 1. The\nprocedures of the deep trans-layer unsupervised network is similar to other com-\nmonly used frameworks in computer vision [14] and feature learning work [15]\n4\nInput\nThe first layer\nThe second layer\nThe output layer\nApplications: Classification, recognition, Ă\nLCN+Whitening\nUnsupervised\nfilter 1\nĂ\nĂ\nLCN+Whitening\nLCN+Whitening\nUnsupervised\nfilter 2\nUnsupervised\nfilter 2\nTrans-layered feature representation\nĂ\nĂ\nBinary quantization\nBlock-wise histogram\nFigure 1: The framework of the deep trans-layer unsupervised network, which is a three-\nlayer neural network, including the ﬁrst unsupervised learning layer, the second unsupervised\nlearning layer and the trans-layer.\nas well. Diﬀerent from the traditional methods, the deep trans-layer unsuper-\nvised network utilizes the unsupervised learning methods, such as the PCA or\nauto encoder, to learn the local receptive ﬁlter banks, and needs no ﬁne-tuning\nprocedure to adjust those local ﬁlter banks. Besides, the previous layer’s unsu-\npervised feature maps are concatenated to the last layer to form a much more\ncompleted representation, which is shown quite eﬀective for the following tasks.\nAs a high level, the proposed deep trans-layer unsupervised performs the\nfollowing steps to learn a feature representation.\nThe ﬁrst layer\n5\n• Extract random patches from the training images.\n• Apply local contrast normalization (LCN) and whitening operations to\neach extracted patches.\n• Learn the ﬁrst layer’s ﬁlter banks by the unsupervised learning, such as\nPCA or auto encoder, on the extracted patches.\n• Use the learned feature banks to generate the feature maps of each training\nimage.\nThe second layer\n• Extract random patches from the obtained feature maps in the ﬁrst layer.\n• Apply local contrast normalization and whitening operations to each ex-\ntracted patches from feature maps.\n• Learn the second layer’s ﬁlter banks by the unsupervised learning on the\nextracted feature map patches.\nGiven test images, we can extract stacked and concatenated feature representa-\ntions and apply them to classiﬁcation.\nThe ﬁrst layer\n• Apply the unsupervised learning feature banks the ﬁrst layer learned to\neach test image to generate its feature maps after local contrast normal-\nization and whitening processing operations.\nThe second layer\n• Apply the unsupervised learning feature banks the second layer learned\nto each ﬁrst layer feature maps after local contrast normalization and\nwhitening processing operations.\nThe output layer\n• Generate the response maps by amalgamating the ﬁrst layer feature maps\nwith the second layer feature maps.\n• Use binary coding operation to facilitate the next histogram procedure.\n• Compose block-wise histogram of the coded response maps of each test\nimage as its feature representation.\n• Apply dimensionality reduction methods or distance metrics to the learned\ndeep trans-layer unsupervised feature representations, or directly train a\nclassiﬁer based the representations to tackle the applications.\nIn the following paragraphs, we will describe the components of the above net-\nwork pipeline in more details.\n6\n3.1. Representation learning\nThe structure of deep trans-layer unsupervised network is partially similar\nto convolution neural network where convolution operations are done in small\npatches. In deep trans-layer unsupervised network, the local ﬁlters are learned\nby unsupervised learning, such as PCA or auto encoder, which requires no ﬁne\ntuning process through error back propagation.\nIn each unsupervised layer (the ﬁrst and second layer), the system begins\nwith extracting a large number of random patches from unlabelled input im-\nages. Suppose the images used here are all gray images. Each patch has a\nreceptive ﬁeld size or dimension of k1-by-k2. If the images are color images with\nd channels, the patch dimension is k1-by-k2-by-d. Just process the other d −1\nchannels the same as following procedures step by step independently. Then a\ndataset of m patches is constructed, X = {x(1), . . . , x(m)}, where x(i) ∈Rk1×k2\nstands for the ith patch extracted from the input images in the ﬁrst layer or\nthe feature maps in the second layer. Sequentially, we apply the preprocessing\nof local contrast normalization (LCN) and whitening, and then unsupervised\nlearning in the ﬁrst and second layer, respectively.\n3.1.1. Local contrast normalization and whitening\nIn the pre-processing of each layer’s unsupervised learning, we perform sev-\neral simple operations eﬀectively in the implemented network.\nThe ﬁrst is local contrast normalization (LCN) [5]. For each local patch x(i)\nin the extracted patch dataset X, we normalize the patch x(i) by subtracting\nits mean and dividing by its standard deviation as,\ny(i)\nj,k = (x(i)j,k −\n1\nk1k2\nk1\nP\nj=1\nk2\nP\nk=1\nx(i)j,k)/\n(\ns\n1\nk1k2\nk1\nP\nj=1\nk2\nP\nk=1\n(x(i)j,k −\n1\nk1k2\nk1\nP\nj=1\nk2\nP\nk=1\nx(i)j,k)2 + C),\nj = 1, · · · , k1; k = 1, · · · , k2; i = 1, · · · , m,\n(1)\nwhere C is a constant integer to make the model more robust, which is commonly\nmanipulated in practice. In our work, C is set to 10.\nThe LCN has explicit explanations both in physics and physiology. The\nmean of local patch stands for local brightness, and the standard deviation rep-\nresents contrast normalization. By LCN, the illumination and material optimal\nproperty eﬀects are removed. On the other hand, the LCN has an eﬀect similar\nto lateral inhibition found in real neurons. The LCN operator inhabits the re-\nsponses within each local patch, whilst activating responses in the same location\nof these patches.\nFollowing the LCN, whitening is the second preprocessing method for each\nunsupervised learning layer. Whitening is commonly used in various applica-\ntion and is a decorrelation operator, which reduces redundant representation of\nimages. The whitening operator transforms the patches as,\n[D, U] = eig(cov(Y))\nz(i) = U(D + diag(ǫ))−1/2UT y(i), i = 1, . . . , m,\n(2)\n7\nwhere Y is formed by m patches y(i), cov() stands for covariance function and\nthe size of output data is k1 ∗k2, eig() is the eigenvalue decomposition function,\nD and U are eigenvalues and eigenvectors respectively, ǫ is set as 0.1 here\nto make the operator more robust.\nThe ZCA whitening also has biological\nexplanation and has been proved its eﬀectiveness by a lot of work.\n3.1.2. Trans-layer unsupervised learning\nAfter pre-processing for each layer, unsupervised learning, such as PCA or\nauto encoder, is used to learn feature banks in the trans-layer network.\nThe ﬁrst unsupervised layer\nAssuming that the number of feature banks in the ﬁrst layer is L1, ﬂatten\neach pre-processed patch z(i)\n1\nextracted from input images, and put the ﬂattened\nvectors together. Extracted patch matrix from the ﬁrst layer will be obtained\nas\nZ1 =\n\u0002\nz1(1),\nz1(2),\n· · · ,\nz1(m) \u0003\n∈Rk1k2×m.\n(3)\nIf the used unsupervised learning is PCA, the PCA unsupervised learning\naims to\nmin\nW1∈RL1×k1k2\n\r\r\rZ1 −W1\nT W1Z1\n\r\r\r\n2\n2\ns.t.\nW1W1\nT = IL1 ,\n(4)\nwhere W1 is the PCA transformation weights, and IL1 is an identity matrix of\nsize L1 × L1. We get the solution of the above PCA constraints as\nW1 =\n\u0002 w11,\nw12,\n· · · ,\nw1L1 \u0003T ,\n(5)\nwhere\nw11,\nw12,\n· · · ,\nw1L1\nare the ﬁrst L1 eigenvectors of Z1ZT\n1 with\nL1 largest variances. Then, we should make these eigenvectors changing back\nto k1 × k2 dimensions to facilitate the next PCA feature mapping operation.\nThe ﬁrst PCA unsupervised layer’s feature maps are generated by applying\nthese L1 ﬁlters to convolute with the input images. For each input image, L1\nfeature maps can be obtained by\nI1\n(i) = I ∗w1i,\ni = 1, · · · , L1 ,\n(6)\nwhere I stands for an input image with zero padded to make I(i)\n1\nhave the same\nsize as the input image, and ∗stands for a convolution operator.\nIf the used unsupervised learning is auto encoder, the auto encoder unsu-\npervised learning aims to\nmin\nW1∈RL1×k1k2 ,b1∈RL1 C\n\r\r\rZ1 −σ\n\u0010\nW1\nT σ\n\u0010\nW1˜Z1 + b1i\n\u0011\n+ b′1i\n\u0011\r\r\r\n2\n2 + ∥W1∥2\n2\n(7)\nwhere W1 is the auto encoder transformation weights, b1 is the encoder bias, C\nis the tradeoﬀbetween errors and model complexity, the used activation function\nσ() is the hyperbolic tangent function, b′1 is the decoder bias, i is a column\n8\nvector of size m full of elements 1, and ˜Z1 is obtained by randomly turning\n10% of the elements in Z1 into 0. Then the encoder weights W1 and bias b1\nare calculated by stochastic gradient decent method. The solution of the above\nde-noising auto encoders is as\nW1 =\n\u0002\nw11,\nw12,\n· · · ,\nw1L1 \u0003T , b1 =\n\u0002\nb1\n1,\nb1\n2,\n· · · ,\nb1\nL1 \u0003T\n(8)\nwhere W1\ni is the weights of the ith neuron in the encoder layer, and bi\n1 stands\nfor the bias of the ith neurons in the encoder layer. Then, we should make\nthese weights changing back to k1 × k2 dimensions to facilitate the next feature\nmapping operation.\nThe ﬁrst de-noising auto encoders unsupervised layer’s feature maps are\ngenerated by applying the encoder layer to convolute with the input images.\nFor each input image, L1 feature maps can be obtained by\nI1\n(i) = σ\n\u0000I ∗w1i + bi\n11\n\u0001\n,\ni = 1, · · · , L1 ,\n(9)\nwhere I stands for an input image with zero padded to make I(i)\n1\nhave the same\nsize as the input image, and 1 is a matrix of the same size as I(i)\n1\nfull of elements\n1.\nThe second unsupervised layer\nPatches should be extracted from feature maps I(i)\n1\nobtained from the ﬁrst\nunsupervised layer.\nThese patches are also the pre-processed by LCN and\nwhitening operations.\nBy ﬂattening these patches, the patch matrix is con-\nstructed as\nZ2 =\n\u0002\nz2(1),\nz2(2),\n· · · ,\nz2(m) \u0003\n∈Rk1k2×m,\n(10)\nwhere z(i)\n2\nstands for ﬂattened patch from the ﬁrst layer’s feature maps after\npre-processing, and m stands for the number of extracted patches, and k1, k2\nstand for the size of extracted patches.\nThe unsupervised learning method is applied to the patch matrix Z2 the\nsame as the ﬁrst layer. Assuming that the number of the second layer’s ﬁlters is\nL2, its solution can be obtained as 5 or 8 by solving 4 or 7. The second layer’s\nfeature maps based on the ﬁrst layer’s feature map are calculated as 6 or 9.\nIf the used unsupervised learning is PCA, the second layer’s unsupervised\nfeature maps are calculated by\nI2\n(i) = I1 ∗w2i,\ni = 1, · · · , L2 ,\n(11)\nwhere I1 is the zero padded image of the ﬁrst layer’s feature map, and wi\n2 is the\nsecond layer’s feature bank weights.\nIf the unsupervised learning is auto encoder, the second layer’s unsupervised\nfeature maps are calculated by\nI2\n(i) = σ\n\u0000I1 ∗w2i + bi\n21\n\u0001\n,\ni = 1, · · · , L2 ,\n(12)\n9\nwhere w2i and bi\n2 are the weights and bias of the ith neuron in the second auto\nencoder layer.\nFor an input image, we get L1 × (L2 + 1) feature maps after the two con-\nvolution layers by concatenating the ﬁrst layer maps to the second layer. That\nis\n{I1\n(1), I1\n(2), · · · , I1\n(L1), I2(1)1, I2(2)1, · · · ,\nI2(L2)1, · · · , I2(1)L1, I2(2)L1, · · · , I2(L2)L1},\n(13)\nwhere I1\n(i) stands for the ith feature map of the ﬁrst layer, and I2(j)i stands\nfor the jth feature map of the second layer for the ith feature map of the ﬁrst\nlayer .\n3.1.3. Block-wise histogram\nThe third layer of the network is illustrated in Figure 2. The third layer is\nquite similar to that of LBPH [8]. For an input training image, the ﬁrst step is\nto encoder the L1 × (L2 + 1) real valued feature maps with binary values, 0 and\n1. The operation converts these feature maps into binary images.\nThe second step is to compress these binary feature maps by quantizing each\nL1 binary feature maps. Here the number of second layer ﬁlters L2 is set as 8,\nand the number of ﬁrst layer ﬁlters L1 is also set as 8. That is, we compress\neach L1 binary feature maps into one feature map, and the compressed feature\nmaps have pixel values from 0 to 255. Then we get L2 + 1 compressed feature\nmaps for each training image as\nn\nIc\n(1), Ic\n(2), · · · , Ic\n(L2), Ic\n(L2+1)o\n,\n(14)\nwhere Ic\n(i) represents the ith compressed feature map.\nThe third step is to construct block-wise histogram illustrated as the third\nprocedure in Figure 2. First, we should partition each compressed feature map.\nAssuming that the size of compressed feature is x × y, and the size of block is\nw1 × w2 with strides s1 × s2, each compressed feature map is partitioned into\n⌊(x −w1)/s1 + 1⌋× ⌊(y −w2)/s2 + 1⌋blocks. For all the L2 + 1 compressed\nfeature maps of each input image, we get blocks as\n\b\nB1, B2, · · · , B(L2+1)×⌊(x−w1)/s1+1⌋×⌊(y−w2)/s2+1⌋\n\t\n,\n(15)\nwhere Bi stands for the ith block. Next step is to build histograms in each of\nthe blocks. In the deep trans-layer PCA network, we set the number of bins\nin the histogram to 2L1. It means that each integer of the pixel values is set\nas a bin and a sparse vector representing the histogram is constructed. Then\nconcatenate these NB = (L2+1)×⌊(x−w1)/s1+1⌋×⌊(y−w2)/s2+1 histograms\nto form a more complete representation of the input image as\nf(Image) =\n[hist(B1)T , hist(B2)T , · · · , hist(BNB)T ]T ∈RNB(2L1),\n(16)\n10\nL1h(L2+1)\nfeature maps     \n……\n……\nL1h(L2+1)\nbinary        \nfeature maps\nB1\nB2\nL2+1 maps\ns1\nx\nw1\ns2\nw2\ny\nNB = (L2+1)h (x-w1)/s1+1 \nh!(y-w2)/s2+1 blocks \nf(Image) = [hist(B1), hist(B2), Ă, hist(B(L2+1)h (x-w1)/s1+1 h!(y-w2)/s2+1 ]\n…\n…\nCompression and partition the blocks\nBinarization\nConcatenating histograms in each block\nFigure 2: Illustration showing the output layer of deep trans-layer unsupervised network.\nWe ﬁrst encoder the output of the trans-layer unsupervised learning. Then compress each\nL1 binarized feature maps into one feature map with pixels in an integer range from 0 to\n2L1 −1. Thus, L2 + 1 compressed feature maps will be obtained for each image. Then use a\nw1 −by −w2 receptive ﬁeld and stride s to generate ⌊(x −w1)/s1 + 1⌋× ⌊(y −w2)/s2 + 1⌋\nblocks for each compressed feature map. For each block, histogram with 2L1 bins is calculated.\nFinally, concatenate the NB histograms to form the feature representation of deep trans-layer\nunsupervised network.\nwhere hist() stands for histogram operators. Then we use the deep trans-layer\nunsupervised network representation of each training image to learn a dimen-\nsional reduction weight, or to train a classiﬁer to tackle the next applications\ndirectly.\n3.2. Feature extraction and classiﬁcation\nAfter the above representation learning phase, the unsupervised convolution\nfeature banks have been learned. Given a test image, the feature extraction\n11\nis to map the image to trans-layer representation with NB(2L1) dimensions by\nthese feature banks.\n3.2.1. Convolutional extraction and block-wise histogram\nGiven a test image, we should zero-pad the image for the sake of keeping\nthe same size between feature maps and input image. Then the pre-processing\nof LCN and whitening is applied to the zero-padded image. Feature maps are\ncalculated by the ﬁrst unsupervised layer with L1 ﬁlters of size k1 × k2.\nFor the second unsupervised layer, the procedure is the same as the ﬁrst\nlayer. First, zero-pad each feature map. Then, pre-process these feature maps.\nNext, put these feature maps into unsupervised learning of L2 feature banks of\nsize k1 ×k2. Finally, merge the ﬁrst layer’s feature maps with the second layer’s\nfeature maps to form a more complete trans-layer representation.\nThe last phase is block-wise histogram. Binary encoding is used to tackle\nthese real valued feature maps. And then form a compact representation by\nquantizing each L1 binary feature maps into one feature map with pixel values\nfrom 0 to 2L1 −1. Then partition these compact feature maps into blocks, and\nconstruct histogram representation within each block. Concatenate these his-\ntograms to form the deep trans-layer unsupervised representation of translation\nand rotation invariance.\n3.2.2. Classiﬁcation\nFor real applications in the next experiments, classiﬁers or dimension re-\nduction methods are following the deep trans-layer unsupervised representa-\ntion. Due to the relative high dimensions of this representation, whitening PCA\n(WPCA) is used to reduce the representation in object recognition tasks. The\nWPCA is conducted by conventional PCA weights weighted by the inverse of\ntheir corresponding squared root energies. Also, the deep trans-layer unsuper-\nvised representation can be directly used to train a classiﬁer to tackle recognition\ntasks. In our experiments of digit recognition and object recognition, a simple\nlinear SVM classiﬁer with no parameter tuned is used following the deep trans-\nlayer unsupervised network. The parameter, the cost factor C, in the used linear\nSVM software kit LIBLINEAR is 1 as default [16].\n4. Experimental results\nIn the experiment, we will validate the performance of deep trans-layer PCA\nnetwork (using PCA unsupervised learning the local receptive features) and deep\ntrans-layer auto encoder network (using de-noising auto encoder unsupervised\nlearning the local receptive features).\nThe proposed deep trans-layer unsu-\npervised network has two key phases, LCN in pre-processing and trans-layer\nconcatenation. We will validate the two phases in the MNIST variations data\nset [17] using the deep tran-layer PCA network. Also, other parameters such as\nblock size and stride size are chosen through cross validation or validation set.\nBenchmark experiments are conducted on digit recognition of MNIST [18] and\n12\nTable 1: Details of the 9 used data sets for digit recognition on MNIST [18] and MNIST\nvariations [17]\nData sets\nRecognition tasks\n#Classes\n#Train-Test\nMNIST\nHandwritten digits from 0 to 9\n10\n60000-10000\nmnist-basic\nSmaller subset of MNIST\n10\n12000-50000\nmnist-rot\nSame as basic with rotation\n10\n12000-50000\nmnist-back-rand\nSame as basic with random background\n10\n12000-50000\nmnist-back-image\nSame as basic with image background\n10\n12000-50000\nmnist-rot-back-image\nSame as basic with rotation and image background\n10\n12000-50000\nrectangles\nTall or wide rectangles\n2\n1200-50000\nrectangles-image\nSame as rect. with image background\n2\n12000-50000\nConvexNonConvex\nConvex or non-convex shapes\n2\n8000-50000\nMNIST variations [17] data sets, and object recognition of Caltech 101 data set\n[7].\n4.1. Eﬀect of local contrast normalization\nWe ﬁrst validate the eﬀect of LCN followed by conventional PCA in our\nnetwork.\nExperiments about deep trans-layer PCA network with LCN and\nwithout LCN are conducted on the MNIST variations data sets.\nThe MNIST data set contains 60,000 training samples and 10,000 test sam-\nples of gray images with size 28 × 28 pixels. The data set is a subset of NIST,\nwhich contains hand-written digits in real world. The recognition targets have\nbeen size-normalized and centered in the images [18]. MNIST variations data\nsets are created by applying simple controllable factor variations on MNIST\ndigits [17]. The data sets are eﬀective ways to study the invariance ability of\nrepresentation learning methods. Details about recognition tasks, numbers of\nclasses and samples are included in Table 1.\nThe parameters of the block size, the stride size and the patch size are\ndetermined on validation set experimentally. The validation sets are typically\npartitioned from the training set in consistence with the related work [17]. On\nthe digit recognition tasks, the ﬁlter size is set to 7 × 7 pixels, the number of\nﬁlters is set to L1=L2=8 and the size of strides is the half size of the block. For\nMNIST, MNIST basic, mnist-rotation, and rectangles-image data sets, the block\nsize is 7 × 7 pixels. For mnist-back-rand, mnist-back-image and mnist-rot-back-\nimage data sets, the block size is 4 × 4 pixels. For rectangles data set, the block\nsize is 14×14 pixels. For convex data set, the block size is 28×28 pixels. These\nvalidated tiny parameters are ﬁxed in the following digit recognition tasks. A\nsimple linear SVM with default parameters is connected to the deep trans-layer\nPCA representation to do recognition task [16].\nTwo groups of experiments are conducted on MNIST basic, mnist-rotation,\nmnist-back-rand, mnist-back-image-rotation data sets to validate the eﬀect of\nLCN. The performance of deep trans-layer PCA Network with and without\nLCN is reported in Table 2.\n13\nTable 2: Comparison of classiﬁcation error rates (%) on MNIST variations data sets using\ndeep trans-layer PCA network with LCN and without LCN.\nMethods\nBasic\nRot.\nBk-rand\nBk-im\nBk-im-rot\nWithout LCN\n1.03\n6.40\n5.09\n10.51\n34.79\nWith LCN\n0.98\n6.36\n5.22\n9.95\n33.55\nTable 3: Comparison of classiﬁcation error rates (%) on MNIST variations data sets using\ndeep trans-layer PCA network with and without trans-layer connection.\nMethods\nBasic\nRot.\nBk-rand\nBk-im\nBk-im-rot\nWithout trans-layer connection\n1.05\n6.62\n5.94\n10.58\n34.58\nWith trans-layer connection\n0.98\n6.36\n5.22\n9.95\n33.55\nFrom Table 2, we observe that our model with LCN achieves better perfor-\nmance on these data sets except for mnist-back-rand data set. The results prove\nthat LCN helps conventional PCA performance, which is explained in section\n3.1.1. The main reason why LCN performs worse in mnist-back-rand data set is\nthat, the mean and standard deviation in the data set are all corrupted noise in-\nformation due to large area random noise in the background, which degrades the\nperformance. The LCN boosts the performance in natural images, which is val-\nidated by the improvements in MNIST basic, mnist-rotation, mnist-back-image\nand mnist-back-image-rotation data sets.\n4.2. Eﬀect of trans-layer connection\nThe eﬀect of trans-layer connection is validated on MNIST basic, mnist-\nrotation, mnist-back-rand, mnist-back-image-rotation data sets in the second\nexperiment. The parameters are set as that of section 4.1. The performance\nof deep trans-layer PCA network with and without trans-layer connection is\nrecorded in Table 3.\nFrom Table 3, we observe that deep trans-layer PCA networks with trans-\nlayer connection have a consistently better performance than those of without\ntrans-layer connection.\nTrans-layer connection boosts almost 0.1 percentage\nperformance even on the hardest task of MNIST basic data set, and 1% per-\nformance for mnist-back-image-rotation data set. The trans-layer connection in\ndeep trans-layer PCA network provides a more complete representation that is\nalways helpful for recognition.\n4.3. Digit recognition on MNIST and MNIST variations data sets\nWe report the performance of the implemented model on MNIST and MNIST\nvariations data sets compared with other methods such as convolution network\n(ConvNet) [5] and ScatNet-2 [12]. The performance of these methods is recorded\nin Table 4 of MNIST data set and Table 5 of MNIST variations data sets re-\nspectively. For fair comparison, the following results do not include the results\nusing augmented samples.\n14\nTable 4: MNIST classiﬁcation error rates (%).\nNote that the compared methods are not\nincluding methods that augment the training data.\nMethods\nMNIST error rates (%)\nK-NN-SCM [19]\n0.63\nK-NN-IDM [20]\n0.54\nCDBN [15]\n0.82\nHSC [13]\n0.77\nConvNet [5]\n0.53\nStochastic Pooling ConvNet [21]\n0.47\nConv. Maxout + Dropout [22]\n0.45\nScatNet-2 (SV Mrbf) [12]\n0.43\nPCANet [4]\n0.66\nDeep Trans-layer PCA Network\n0.58 1\nDeep Trans-layer Autoencoder Network\n0.55 1\nTable 5: Comparison of classiﬁcation error rates (%) on MNIST variations data sets.\nMethods\nBasic\nRot.\nBk-rand\nBk-im\nBk-im-rot\nRect\nRect-im\nCon.\nCAE-2 [23]\n2.48\n9.66\n10.90\n15.50\n45.23\n1.21\n21.54\n-\nTIRBM [24]\n-\n4.20\n-\n-\n35.50\n-\n-\n-\nPGBM\n+DN-1 [25]\n-\n-\n6.08\n12.25\n36.76\n-\n-\n-\nScatNet-2\n(SV Mrbf) [12]\n1.27\n7.48\n18.40\n12.30\n50.48\n0.01\n8.02\n6.50\nPCANet [4]\n1.06\n7.37\n6.19\n10.95\n35.48\n0.24\n14.00\n4.36\nDeep Trans-layer\nPCA Network\n0.98\n6.36\n5.22\n9.95\n33.55\n0.09\n13.18\n3.52\nDeep Trans-layer\nAutoencoder Network\n1.02\n7.56\n5.56\n8.37\n34.43\n0.01\n13.01\n2.90\nFrom Table 4, the results show that the deep trans-layer unsupervised net-\nwork is only inferior to ScatNet-2 and enhanced Convolution Network related\nmethods. It is worthy to mention that the performance of ScatNet-2 is achieved\nby connected with a non-linear SVM with RBF kernels with tuned parameters,\nbut our model is connected with a linear SVM with all default parameters in\nLIBLINEAR software kit [16]. Our model’s performance (0.55) is highly com-\nparable to that of Convolution Network (0.53) on MNIST data set [5]. Because\nMNIST data set contains too many training samples with small intra-class vari-\nability, most of methods work well on this data set and the tiny diﬀerence is\nnot much meaningful statistically. Despite this, the deep trans-layer PCA net-\nwork boosts almost 0.1% performance higher than that of PCA related method,\n1The matlab code can be downloaded from https://github.com/wentaozhu/Deep-trans-layer-unsupervised-network.git\n15\nTable 6: Comparison of classiﬁcation accuracy (%) in terms of mean and stand deviation\nbased on gray-level images of Caltech 101 data set.\nMethods\n15 samples per class (%)\n30 samples per class (%)\nCDBN [15]\n57.70 ± 1.50\n65.40 ± 0.50\nConvNet [26]\n57.60 ± 0.40\n66.30 ± 1.50\nDeconvNet [27]\n58.60 ± 0.70\n66.90 ± 1.10\nChen et al. [28]\n58.20 ± 1.20\n65.80 ± 0.60\nZou et al. [29]\n-\n66.50\nHSC [13]\n-\n74.0\nPCANet [4]\n61.46 ± 0.76\n68.56 ± 1.01\nDeep Trans-layer\nPCA Network\n67.11 ± 0.64\n75.98 ± 0.58\nDeep Trans-layer\nAutoencoder Network\n67.03 ± 0.56\n75.90 ± 0.19\nPCANet.\nFrom Table 5, we observe that the deep trans-layer unsupervised network\nachieves the best performance on six data sets with a simple linear SVM clas-\nsiﬁer. Our model has a highly superior performance than other methods. It\nis suﬃcient to prove that the proposed structure of LCN pre-processing and\ntrans-layer connection work well in the convolution structure with unsupervised\nﬁlters.\n4.4. Object recognition on Caltech 101 data set\nWe also evaluate the model for object recognition task on Caltech 101 data\nset.\nCaltech 101 data set contains color images belonging to 102 categories\nincluding a background class. The number of each class’s images varies from 31\nto 800 [7]. The pre-processing of the data set is to convert the images into gray\nscale, and adjust the longer side of the image to 300 with preserved aspect ratio.\nTwo typical tasks are conducted. One is with a training set of 15 samples per\nclass. The other is with training set of 30 samples per class. The training sets\nare randomly sampled from Caltech 101, and the rest are test set. Five rounds\nof experiments are recorded, and the performance is recorded as the average of\nthe ﬁve rounds of results.\nParameters are set as follows. The ﬁlter size is set to 7×7 pixels, the number\nof ﬁlters is set to L1 = L2 = 8, the block size is set to a quarter of the image size\nand the size of strides is set to the half size of the blocks. The WPCA is used to\nreduce the dimension of each block’s trans-layer representation from 256 to 64.\nLinear SVM with default parameters in LIBLINEAR [16] is used to tackle the\nrecognition task. Comparison results on gray raw images are recorded in Table\n6.\nThe Table 6 shows that the deep trans-layer unsupervised network gets the\nimpressive performance trained by 15 samples per class and 30 samples per class\ntasks respectively by simply using the unsupervised methods. More surprisingly,\n16\nTable 7: Comparison of veriﬁcation rates (%) on LFW-a data set under unsupervised setting\nwith a single descriptor.\nMethods\nVeriﬁcation accuracy (%)\nPOEM [31]\n82.70 ± 0.59\nHigh-dim. LBP [32]\n84.08\nHigh-dim. LE [32]\n84.58\nSFRD [33]\n84.81\nI-LQP [34]\n86.20 ± 0.46\nOCLBP [35]\n86.66 ± 0.30\nPCANet [4]\n86.28 ± 1.14\nDeep Trans-layer PCA Network\n87.10 ± 0.43\nour model with no elaborately tuned parameters gets about 2% upper than that\nof HSC [13] on 30 samples per class task. The proposed simple network really\nmakes a high progress for the data set.\n4.5. Face veriﬁcation on LFW-a data set\nFace veriﬁcation task is conducted with our model on LFW-a data set [30].\nThe LFW data set contains more than 13,000 faces of 5,749 diﬀerent individuals,\nand these images were collected from the web of unconstrained conditions. In\nthe LFW data set, the unsupervised setting is used for the deep trans-layer PCA\nnetwork for suﬃciently validating of representation eﬀectiveness. The LFW-a\ndata set with alignment is used, and we cropped the face images into 150 × 80\npixels. The standard evaluation protocol on LFW is followed for performance\nevaluation. The histogram block size is 15 × 13 with non-overlapping. Other\nparameters are set the same as before. The WPCA is used to reduce the di-\nmension of trans-layer PCA representation to 3,200 after additional square-root\noperation in the data set. Then use NN classiﬁer with cosine distance to tackle\nthe veriﬁcation task. The performance is recorded in Table 7 for unsupervised\nsetting with a single descriptor.\nTable 7 shows that the deep trans-layer PCA network achieves the best\nperformance with the unsupervised setting on the LFW-a data set. Also, the\nboosted performance of our model in contrast to PCANet with the same 3,200\ndimensions reveals that, the trans-layer architecture provides uncorrelated in-\nformation in the increased dimensions, which is beneﬁcial for tasks.\n5. Discussion\nOne of the typical cases to illustrate why the trans-layer representation works\nis that the local information of naevi in our faces may vanish in the upper\nlayer’s unsupervised representation for the size of receptive ﬁelds are increased\nand the local discriminative information may be neglected. However, the naevi\nis of highly discriminative features to recognize each person. In conventional\ndeep neural networks, such discriminative details also hard to preserve with the\n17\n \nFigure 3: Illustration of the second layer’s feature maps on MNIST data set, ﬁve maps per\ncategory. We can hardly recognize most maps for much information loss in the multi-layer\nPCA network.\nincreasing of layer numbers. The trans-layer representation from the bottom\nlayer preserves these details and is helpful for recognition.\nThe trans-layer representation scheme is quite successful for the convolution\nnetwork of PCA and auto encoder ﬁlters, because the number of ﬁlters is rela-\ntively too small to preserve enough information for tasks. And information will\nbe lost rapidly with the increase of layer numbers, as illustrated in Fig. 3. If\nthe model uses only the second layer’s feature maps, there would be much noise\nand lose much useful information. Therefore, the second layer’s feature maps\nare not enough for classiﬁcation, and the trans-layer connection is quite helpful.\nThe deep trans-layer PCA network also has some problems to be improved\ndespite that LCN and trans-layer representation are added into it. The main\nproblem is the high dimensions of the trans-layer representation. Although good\nrepresentation for tasks needs high dimensions, we should make the dimensions\nof representation as low as possible providing good representational ability. So\nthe future work could include that other translation and rotation invariance\npreserving methods to reduce the dimensions of the trans-layer representations\nsuch as pooling operations [5, 18, 21].\n6. Conclusion\nIn this paper, a novel feature learning and representation model, the deep\ntrans-layer unsupervised network, is demonstrated. Several key elements are\nadded to boost the deep unsupervised learning such as the LCN operation and\ntrans-layer representation. Several experiments on digit recognition and object\nrecognition tasks have shown that, the proposed method based on layer-wise\nunsupervised learning the local receptive features also obtains the impressive\nresults by trans-layer representation. The deep trans-layer unsupervised net-\nwork has a quite simple structure with much few parameters, in which only the\ncascaded local receptive ﬁlters need to be learned by unsupervised methods.\nThe structure of trans-layer network also provides a novel direction for us to\nmine.\n18\nReferences\nReferences\n[1] D. G. Lowe, Distinctive image features from scale-invariant keypoints, In-\nternational journal of computer vision 60 (2) (2004) 91–110.\n[2] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection,\nin: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE\nComputer Society Conference on, Vol. 1, IEEE, 2005, pp. 886–893.\n[3] G. E. Hinton, R. R. Salakhutdinov, Reducing the dimensionality of data\nwith neural networks, Science 313 (5786) (2006) 504–507.\n[4] T.-H. Chan, K. Jia, S. Gao, J. Lu, Z. Zeng, Y. Ma, Pcanet: A simple deep\nlearning baseline for image classiﬁcation?, arXiv preprint arXiv:1404.3606.\n[5] K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best multi-\nstage architecture for object recognition?, in: Computer Vision, 2009 IEEE\n12th International Conference on, IEEE, 2009, pp. 2146–2153.\n[6] A. Coates, A. Y. Ng, Learning feature representations with k-means, in:\nNeural Networks: Tricks of the Trade, Springer, 2012, pp. 561–580.\n[7] L. Fei-Fei, R. Fergus, P. Perona, Learning generative visual models from few\ntraining examples: An incremental bayesian approach tested on 101 object\ncategories, Computer Vision and Image Understanding 106 (1) (2007) 59–\n70.\n[8] T. Ojala, M. Pietikainen, T. Maenpaa, Multiresolution gray-scale and ro-\ntation invariant texture classiﬁcation with local binary patterns, Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on 24 (7) (2002)\n971–987.\n[9] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with\ndeep convolutional neural networks, in: Advances in neural information\nprocessing systems, 2012, pp. 1097–1105.\n[10] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, R. Fergus, Regularization of neu-\nral networks using dropconnect, in: Proceedings of the 30th International\nConference on Machine Learning (ICML-13), 2013, pp. 1058–1066.\n[11] Q. V. Le, A. Karpenko, J. Ngiam, A. Y. Ng, Ica with reconstruction cost for\neﬃcient overcomplete feature learning, in: Advances in Neural Information\nProcessing Systems, 2011, pp. 1017–1025.\n[12] J. Bruna, S. Mallat, Invariant scattering convolution networks, Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on 35 (8) (2013)\n1872–1886.\n19\n[13] K. Yu, Y. Lin, J. Laﬀerty, Learning image representations from the pixel\nlevel via hierarchical sparse coding, in:\nComputer Vision and Pattern\nRecognition (CVPR), 2011 IEEE Conference on, IEEE, 2011, pp. 1713–\n1720.\n[14] Y.-L. Boureau, F. Bach, Y. LeCun, J. Ponce, Learning mid-level features\nfor recognition, in: Computer Vision and Pattern Recognition (CVPR),\n2010 IEEE Conference on, IEEE, 2010, pp. 2559–2566.\n[15] H. Lee, R. Grosse, R. Ranganath, A. Y. Ng, Convolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations,\nin: Proceedings of the 26th Annual International Conference on Machine\nLearning, ACM, 2009, pp. 609–616.\n[16] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, C.-J. Lin, Liblinear:\nA library for large linear classiﬁcation, The Journal of Machine Learning\nResearch 9 (2008) 1871–1874.\n[17] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, Y. Bengio, An empirical\nevaluation of deep architectures on problems with many factors of variation,\nin: Proceedings of the 24th international conference on Machine learning,\nACM, 2007, pp. 473–480.\n[18] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning ap-\nplied to document recognition, Proceedings of the IEEE 86 (11) (1998)\n2278–2324.\n[19] S. Belongie, J. Malik, J. Puzicha, Shape matching and object recognition\nusing shape contexts, Pattern Analysis and Machine Intelligence, IEEE\nTransactions on 24 (4) (2002) 509–522.\n[20] D. Keysers, T. Deselaers, C. Gollan, H. Ney, Deformation models for image\nrecognition, Pattern Analysis and Machine Intelligence, IEEE Transactions\non 29 (8) (2007) 1422–1435.\n[21] M. D. Zeiler, R. Fergus, Stochastic pooling for regularization of deep con-\nvolutional neural networks, arXiv preprint arXiv:1301.3557.\n[22] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, Y. Bengio,\nMaxout networks, arXiv preprint arXiv:1302.4389.\n[23] S. Rifai, P. Vincent, X. Muller, X. Glorot, Y. Bengio, Contractive auto-\nencoders: Explicit invariance during feature extraction, in: Proceedings of\nthe 28th International Conference on Machine Learning (ICML-11), 2011,\npp. 833–840.\n[24] K. Sohn, H. Lee, Learning invariant representations with local transforma-\ntions, arXiv preprint arXiv:1206.6418.\n20\n[25] K. Sohn, G. Zhou, C. Lee, H. Lee, Learning and selecting features jointly\nwith point-wise gated {B} oltzmann machines, in: Proceedings of The 30th\nInternational Conference on Machine Learning, 2013, pp. 217–225.\n[26] K. Kavukcuoglu, P. Sermanet, Y.-L. Boureau, K. Gregor, M. Mathieu, Y. L.\nCun, Learning convolutional feature hierarchies for visual recognition, in:\nAdvances in neural information processing systems, 2010, pp. 1090–1098.\n[27] M. D. Zeiler, D. Krishnan, G. W. Taylor, R. Fergus, Deconvolutional net-\nworks, in: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE\nConference on, IEEE, 2010, pp. 2528–2535.\n[28] B. Chen, G. Polatkan, G. Sapiro, L. Carin, D. B. Dunson, The hierarchical\nbeta process for convolutional factor analysis and deep learning, in: Pro-\nceedings of the 28th International Conference on Machine Learning (ICML-\n11), 2011, pp. 361–368.\n[29] W. Zou, S. Zhu, K. Yu, A. Y. Ng, Deep learning of invariant features via\nsimulated ﬁxations in video, in: Advances in Neural Information Processing\nSystems, 2012, pp. 3212–3220.\n[30] L. Wolf, T. Hassner, Y. Taigman, Eﬀective unconstrained face recogni-\ntion by combining multiple descriptors and learned background statistics,\nPattern Analysis and Machine Intelligence, IEEE Transactions on 33 (10)\n(2011) 1978–1990.\n[31] N.-S. Vu, A. Caplier, Enhanced patterns of oriented edge magnitudes for\nface recognition and image matching, Image Processing, IEEE Transactions\non 21 (3) (2012) 1352–1365.\n[32] D. Chen, X. Cao, F. Wen, J. Sun, Blessing of dimensionality:\nHigh-\ndimensional feature and its eﬃcient compression for face veriﬁcation, in:\nComputer Vision and Pattern Recognition (CVPR), 2013 IEEE Confer-\nence on, IEEE, 2013, pp. 3025–3032.\n[33] Z. Cui, W. Li, D. Xu, S. Shan, X. Chen, Fusing robust face region de-\nscriptors via multiple metric learning for face recognition in the wild, in:\nComputer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference\non, IEEE, 2013, pp. 3554–3561.\n[34] S. U. Hussain, T. Napol´eon, F. Jurie, et al., Face recognition using local\nquantized patterns, in: British Machive Vision Conference, 2012.\n[35] O. Barkan, J. Weill, L. Wolf, H. Aronowitz, Fast high dimensional vector\nmultiplication face recognition, in: Computer Vision (ICCV), 2013 IEEE\nInternational Conference on, IEEE, 2013, pp. 1960–1967.\n21\n",
  "categories": [
    "cs.NE",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2015-09-27",
  "updated": "2015-09-27"
}