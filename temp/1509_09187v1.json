{
  "id": "http://arxiv.org/abs/1509.09187v1",
  "title": "Deep Haar Scattering Networks",
  "authors": [
    "Xiuyuan Cheng",
    "Xu Chen",
    "Stephane Mallat"
  ],
  "abstract": "An orthogonal Haar scattering transform is a deep network, computed with a\nhierarchy of additions, subtractions and absolute values, over pairs of\ncoefficients. It provides a simple mathematical model for unsupervised deep\nnetwork learning. It implements non-linear contractions, which are optimized\nfor classification, with an unsupervised pair matching algorithm, of polynomial\ncomplexity. A structured Haar scattering over graph data computes permutation\ninvariant representations of groups of connected points in the graph. If the\ngraph connectivity is unknown, unsupervised Haar pair learning can provide a\nconsistent estimation of connected dyadic groups of points. Classification\nresults are given on image data bases, defined on regular grids or graphs, with\na connectivity which may be known or unknown.",
  "text": "Deep Haar Scattering Networks\nXIUYUAN CHENG∗,\nApplied Mathematics Program, Yale University, CT, USA\nxiuyuan.cheng@yale.edu\nXU CHEN\nElectrical Engineering Department, Princeton University, NJ, USA\nxuchen@princeton.edu\nAND\nST´EPHANE MALLAT\nComputer Science Department, ´Ecole Normale Sup´erieure, Paris, France\nstephane.mallat@ens.fr\nOctober 1, 2015\nAbstract\nAn orthogonal Haar scattering transform is a deep network, computed with a hierarchy of additions, subtractions\nand absolute values, over pairs of coefﬁcients. It provides a simple mathematical model for unsupervised deep network\nlearning. It implements non-linear contractions, which are optimized for classiﬁcation, with an unsupervised pair match-\ning algorithm, of polynomial complexity. A structured Haar scattering over graph data computes permutation invariant\nrepresentations of groups of connected points in the graph. If the graph connectivity is unknown, unsupervised Haar\npair learning can provide a consistent estimation of connected dyadic groups of points. Classiﬁcation results are given\non image data bases, deﬁned on regular grids or graphs, with a connectivity which may be known or unknown.\ndeep\nlearning, neural network, scattering transform, Haar wavelet, classiﬁcation, images, graphs\n2000 Math Subject Classiﬁcation: 68Q32, 68T45, 68Q25, 68T05\n1\nIntroduction\nDeep neural networks appear to provide scalable learning architectures for high-dimensional learning, with impressive\nresults on many different type of data and signals [2]. Despite their efﬁciency, there is still little understanding on the\nproperties of these architectures. Deep neural networks alternate pointwise linear operators, whose coefﬁcients are opti-\nmized with training examples, with pointwise non-linearities. To obtain good classiﬁcation results, strong constraints are\nimposed on the network architecture on the support of these linear operators [15]. These constraints are usually derived\nfrom an experimental trial and error processes.\nSection 2 introduces a simple deep Haar scattering architecture, which only computes the sum of pairs of coefﬁcients,\nand the absolute value of their difference. The architecture preserves some important properties of deep networks, while\nreducing the computational complexity and simplifying their mathematical analysis. Through this architecture, we shall\naddress major questions concerning invariance properties, learning complexity, consistency, and the specialization of such\narchitectures.\nConvolution networks are particular classes of deep networks, which compute translation invariant descriptors of\nsignals deﬁned over uniform grids [15, 27]. Scattering networks were introduced as convolution networks computed\nwith iterated wavelet transforms, to obtain invariants which are stable to deformations [18]. With appropriate architecture\nconstraints on Haar scattering networks, Section 3 deﬁnes locally displacement invariant representations of signals deﬁned\n1\narXiv:1509.09187v1  [cs.LG]  30 Sep 2015\non general graphs. In social, sensor or transportation networks, high dimensional data vectors are supported on a graph\n[28]. In most cases, propagation phenomena require to deﬁne translation invariant representations for classiﬁcation. We\nshow that an appropriate conﬁguration of an orthogonal Haar scattering deﬁnes such a translation invariant representation\non a graph. It is computed with a product of Haar wavelet transforms on the graph, and is thus closely related to non-\northogonal translation invariant scattering transforms [18].\nThe connectivity of graph data is often unknown. In social or ﬁnancial networks, we typically have information on\nindividual agents, without knowing the interactions and hence connectivity between agents. Building invariant repre-\nsentations on such graphs requires to estimate the graph connectivity. Such information can be inferred from unlabeled\ndata, by analyzing the joint variability of signals deﬁned on the unknown graph. This paper studies unsupervised learning\nstrategies, which optimize deep network conﬁgurations, without class label information.\nMost deep neural networks are ﬁghting the curse of dimensionality by reducing the variance of the input data with\ncontractive non-linearities [25, 2]. The danger of such contractions is to nearly collapse together vectors which belong\nto different classes. Learning must preserve discriminability despite this variance reduction resulting from contractions.\nHierarchical unsupervised architectures have been shown to provide efﬁcient learning strategies [1]. We show that unsu-\npervised learning can optimize an average discriminability by computing sparse features. Sparse unsupervised learning,\nwhich is usually NP hard, is reduced to a pair matching problem for Haar scattering. It can thus be computed with a\npolynomial complexity algorithm. For Haar scattering on graphs, it recovers a hierarchical connectivity of groups of\nvertices. Under appropriate assumptions, we prove that pairing problems avoid the curse of dimensionality. It can recover\nan exact connectivity with arbitrary high probability if the training size grows almost linearly with the signal dimension,\nas opposed to exponentially.\nHaar scattering classiﬁcation architectures are numerically tested over image databases deﬁned on uniform grids or\nirregular graphs, whose geometries are either known or estimated by unsupervised learning. Results are compared with\nstate of the art unsupervised and supervised classiﬁcation algorithms, applied to the same data, with known or unknown\ngeometry. All computations can be reproduced with a software available at www.di.ens.fr/data/scattering/haar.\n2\nFree Orthogonal Haar Scattering\n2.1\nOrthogonal Haar ﬁlter contractions\nWe progressively introduce orthogonal Haar scattering by specializing a general deep neural network. We explain the\narchitecture constraints and the resulting contractive properties.\nThe input network is a positive d-dimensional signal x ∈(R+)d, which we write S0x = x. We denote by Sjx the\nnetwork layer at the depth j. A deep neural network computes the next network layer by applying a linear operator Hj to\nSjx followed by a non-linear operator. Particular deep network architectures impose that Hj preserves distances, up to a\nconstant normalization factor λ [21]:\n∥Hjy−Hjy′∥= λ ∥y−y′∥.\nThe network is contractive if it applies a pointwise contraction ρ to each value of the output vector HjS jx. This means\nthat for any (a,b) ∈R2 |ρ(a)−ρ(b)| ≤|a−b|. Rectiﬁcations and sigmoids are examples of such contractions. We use an\nabsolute value ρ(a) = |a| because it preserves the amplitude, and it yields a permutation invariance which will be studied.\nFor any vector y = (y(n))n, the pointwise absolute value is written |y| = (|y(n)|)n. The next network layer is thus:\nSj+1x = |HjSjx| .\n(1)\nThis transform is iterated up to a maximum depth J ≤log2(d) to compute the network output SJx.\nWe shall further impose that each layer Sjx has the same dimension as x, and hence that Hj is an orthogonal operator\nin Rd, up to the scaling factor λ. Geometrically, Sj+1x is thus obtained by rotating Sjx with Hj, and by contracting each\nof its coordinate with the absolute value. The geometry of this contraction is thus deﬁned by the choice of the operator Hj\nwhich adjusts the one-dimensional directions along which the contraction is performed.\n2\nx\nS1x\nS2x\nSJx\nFigure 1: A free Haar scattering network computes a layer Sj+1x by pairing the coefﬁcients of the previous layer S jx, and storing the\nsum of coefﬁcients in each pair and the amplitude of their difference.\nAn orthogonal Haar scattering is implemented with an orthogonal Haar ﬁlter Hj. The vector Hjy regroups the coef-\nﬁcients of y ∈Rd into d/2 pairs and computes their sums and differences. The rotation Hj is thus factorized into d/2\nrotations by π/4 in R2, and multiplications by 21/2. The transformation of each coordinate pair (α,β) ∈R2 is:\n(α,β) −→(α +β , α −β ).\nThe operator |Hj| applies an absolute value to each output coordinate, which has no effect on α +β if α ≥0 and β ≥0,\nbut it removes the sign of their difference:\n(α,β) −→(α +β , |α −β|) .\n(2)\nObserve that this non-linear operator deﬁnes a permutation invariant representation of (α,β). Indeed, the output values\nare not modiﬁed by a permutation of α and β, and the two values of α, β are recovered without order, by\nmax(α,β) = 1\n2\n\u0000α +β +|α −β|\n\u0001\nand min(α,β) = 1\n2\n\u0000α +β −|α −β|\n\u0001\n.\n(3)\nThe operator |Hj| can thus also be interpreted as a calculation of d/2 permutation invariant representations of pairs of\ncoefﬁcients.\nApplying |Hj| to Sjx computes the next layer Sj+1x = |HjSjx|, obtained by regrouping the coefﬁcients of Sjx ∈Rd\ninto d/2 pairs of indices written πj = {πj(2n),πj(2n+1)}0≤n<d/2:\nSj+1x(2n) = Sjx(π j(2n))+Sjx(πj(2n+1)) ,\n(4)\nSj+1x(2n+1) = |Sjx(π j(2n))−S jx(πj(2n+1))| .\n(5)\nThe pairing πj speciﬁes which index π j(2n+1) is paired with π j(2n), but the ordering index n is not important. It speciﬁes\nthe storing position in Sj+1x of the transformed values. For classiﬁcation applications, πj will be optimized with training\nexamples. This deep network computation is illustrated in Figure 1. The network output SJx is calculated with Jd/2\nadditions, subtractions and absolute values. Each coefﬁcient of SJx is calculated by cascading J permutation invariant\noperators over pairs, and thus deﬁnes an invariant over a group of 2J coefﬁcients. The network depth J thus corresponds\nto an invariance scale 2J.\nSince the network is computed by iterating orthogonal linear operators, up to a normalization, and a contractive\nabsolute value, the following theorem proves that it deﬁnes a contractive transform, which preserves the norm, up to a\nnormalization. It also proves that an orthogonal Haar scattering transform SJx is obtained by applying an orthogonal\nmatrix to x, which depends upon x and J.\nTheorem 2.1. For any J ≥0, and any (x,x′) ∈R2d\n∥SJx−SJx′∥≤2J/2∥x−x′∥.\n(6)\n3\nMoreover SJx = 2J/2 Mx,J x where Mx,J is an orthogonal matrix which depends on x and J, and\n∥SJx∥= 2J/2∥x∥.\n(7)\nProof. Since Sj+1x = |HjSjx| where Hj is an orthogonal operator multiplied by 21/2,\n∥Sj+1x−Sj+1x′∥≤∥HjSjx−HjSjx′∥= 21/2∥Sjx−Sjx′∥.\nSince S0x = x, equation (6) is veriﬁed by induction on j. We can also rewrite\nS j+1x = |HjSjx| = Ej,xHjx,\nwhere E j,x is a diagonal matrix where the diagonal entries are ±1, with a sign which depend on Sjx. Since 2−1/2Hj is\northogonal, 2−1/2E j,xHj is also orthogonal so Mx,J = 2−J/2 ∏J\nj=1 Ej,xHj is orthogonal, and depends on x and J. It results\nthat ∥SJx∥= 2J/2 ∥x∥.\n2.2\nComplete representation with bagging\nA single Haar scattering transform looses information since it applies orthogonal operators followed by an absolute value\nwhich looses the information of the sign. However, the following theorem proves that x can be recovered from 2J distinct\northogonal Haar scattering transforms, computed with different pairings π j at each layer.\nTheorem 2.2. There exist 2J different orthogonal Haar scattering transforms such that almost all x ∈Rd can be recon-\nstructed from the coefﬁcients of these 2J transforms.\nThis theorem is proved by observing that a Haar scattering transform is computed with permutation invariants opera-\ntors over pairs. Inverting these operators allows to recover values of signal pairs but not their locations. However, recom-\nbining these values on enough overlapping sets allows one to recover their locations and hence the original signal x. This is\nproved on the following lemma applied to interlaced pairings. We say that two pairings π0 = {π0(2n),π0(2n+1)}0≤n<d/2\nand π1 = {π1(2n),π1(2n + 1)}0≤n<d/2 are interlaced if there exists no strict subset Ωof {1,...,d} such that π0 and π1\nare pairing elements within Ω. The following lemma shows that a single-layer scattering operator is invertible with two\ninterlaced pairings.\nLemma 2.1. If two pairings π0 and π1 of {1,...,d} are interlaced then any x ∈Rd whose coordinates have more than 2\ndifferent values can be recovered from the values of S1x computed with π0 and the values of S1x computed with π1.\nProof. Let us consider a triplet n1,n2,n3 where (n1,n2) is a pair in π0 and (n1,n3) is a pair in π1. From S1x computed\nwith π0 we get\nx(n1)+x(n2),\n|x(n1)−x(n2)|\nand we saw in (3) that it determines the values of {x(n1),x(n2)} up to a permutations. Similarly, {x(n1),x(n3)} are\ndetermined up to a permutation by S1x computed with π1. Then unless x(n1) ̸= x(n2) and x(n2) = x(n3) the three values\nx(n1),x(n2),x(n3) are recovered. The interlacing condition implies that π1 pairs n2 to an index n4 which can not be n3 or\nn1. Thus, the four values of x(n1),x(n2),x(n3),x(x4) are speciﬁed unless x(n4) = x(n1) ̸= x(n2) = x(n3). This interlacing\nargument can be used to extend to {1,...,d} the set of all indices ni for which x(ni) is speciﬁed, unless x takes only two\nvalues.\nProof of Theorem 2.2. Suppose that the 2J Haar scatterings are associated to the J hierarchical pairings (πε1\n1 ,...,πεJ\nJ )\nwhere εj ∈{0,1}, where for each j, π0\nj and π1\nj are two interlaced pairings of d elements. The sequence (ε1,...,εJ) is a\nbinary vector taking 2J different values.\nThe constraint on the signal x is that each of the intermediate scattering coefﬁcients takes more than 2 distinct values,\nwhich holds for x ∈Rd except for a union of hyperplanes which has zero measure. Thus for almost every x ∈Rd, the\ntheorem follows from applying Lemma 2.1 recursively to the j-th level scattering coefﬁcients for J −1 ≥j ≥0.\n4\nLemma 2.1 proves that only two pairings is sufﬁcient to invert one Haar scattering layer. The argument proving that 2J\npairings are sufﬁcient to invert J layers is quite brute-force. It is conjectured that the number of pairings needed to obtain\na complete representation for almost all x ∈Rd does not need to grow exponentially in J but rather linearly. Theorem 2.2\nsuggests to deﬁne a signal representations by aggregating different Haar orthogonal scattering transforms. We shall see\nthat this bagging strategy is indeed improving classiﬁcation accuracy.\n2.3\nSparse unsupervised learning with adaptive contractions\nA free orthogonal Haar scattering transform of depth J is computed with a pairing at each of the J network layers,\nwhich may be chosen freely. We now explain how to optimize these pairings from N unlabeled examples {xi}1≤i≤N. As\npreviously explained, an orthogonal Haar scattering is strongly contractive. Each linear Haar operator rotates the signal\nspace, and the absolute value suppresses the sign of each difference, and hence projects coefﬁcients over a smaller domain.\nOptimizing the network thus amounts to ﬁnd the best directions along which to perform the space compression.\nContractions reduce the space volume and hence the variance of scattering vectors but it may also collapse together\nexamples which belong to different classes. To maximize the “average discriminability” among signal examples, we shall\nthus maximize the variance of the scattering transform over the training set. Following [19], we show that it yields a\nrepresentation whose coefﬁcients are sparsely excited.\nThe network layers are optimized with a greedy layerwise strategy similar to many deep unsupervised learning algo-\nrithms [9, 2], which consists in optimizing the network parameters layer per layer, as the depth j increases. Let us suppose\nthat Haar scattering operators Hℓare computed for 1 ≤ℓ< j. One can thus compute Sjx for any x ∈Rd. We now explain\nhow to optimize Hj to maximize the variance of the next layer Sj+1x. The non-normalized empirical variance of Sj over\nthe training set {xi}i is\nσ2(S jx) = ∑\ni\n∥S jxi∥2 −\n\r\r\r∑\ni\nSjxi\n\r\r\r\n2\n.\nThe following proposition, adapted from [19], proves that the scattering variance decreases as the depth increases, up to a\nfactor 2. It gives a condition on Hj to maximize the variance of the next layer.\nProposition 2.1. For any j ≥0 and x ∈Rd, σ2(2−(j+1)/2Sj+1x) ≤σ2(2−j/2S jx). Maximizing σ2(Sj+1x) given S jx is\nequivalent to ﬁnding Hj which minimizes\n\r\r\r∑\ni\nHjSjxi\n\r\r\r\n2\n= ∑\nn\n\u0010\n∑\ni\n|HjSjxi(n)|\n\u00112\n.\n(8)\nProof. Since Sj+1x = |HjSjx| and ∥HjSjx∥= 21/2∥Sjx∥, we have\nσ2(Sj+1x)\n= ∑\ni\n∥Sj+1xi∥2 −\n\r\r\r∑\ni\nSj+1xi\n\r\r\r\n2\n=\n2\nN\n∑\ni=1\n∥Sjxi∥2 −\n\r\r\r\nN\n∑\ni=1\n|HjSjxi|\n\r\r\r\n2\n.\nOptimizing σ2(Sj+1x) is thus equivalent to minimizing (8). Moreover,\nσ2(Sj+1x)\n=\n2\nN\n∑\ni=1\n∥Sjxi∥2 −\n\r\r\rHj\nN\n∑\ni=1\nSjxi\n\r\r\r\n2\n+\n\r\r\r\nN\n∑\ni=1\nHjSjxi\n\r\r\r\n2\n−\n\r\r\r\nN\n∑\ni=1\n|HjSjxi|\n\r\r\r\n2\n=\n2\nN\n∑\ni=1\n∥S jxi∥2 −2∥\nN\n∑\ni=1\nSjxi∥2 +\n \r\r\r\nN\n∑\ni=1\nHjSjxi\n\r\r\r\n2\n−\n\r\r\r\nN\n∑\ni=1\n|HjSjxi|\n\r\r\r\n2\n!\n≤\n2\nN\n∑\ni=1\n∥Sjxi∥2 −2∥\nN\n∑\ni=1\nSjxi∥2 = 2σ2(Sjx),\nwhich proves the ﬁrst claim of the proposition.\nThis propocsition relies on the energy conservation ∥Hjy∥= 21/2∥y∥. Because of the contraction of the absolute value,\n5\nit proves that the variance of the normalized scattering 2−j/2S jx decreases as j increases. Moreover the maximization of\nσ2(Sj+1x) amounts to minimize a mixed l1 and l2 norm on HjSjxi(n), where the sparsity l1 norm is along the realization\nindex i where as the l2 norm is along the feature index n of the scattering vector.\nMinimizing the ﬁrst l1 norm for n ﬁxed tends to produce a coefﬁcient indexed by n which is sparsely excited across\nthe examples indexed by i. It implies that this feature is discriminative among all examples. On the contrary, the l2 norm\nalong the index n has a tendency to produce l1 sparsity norms which have a uniformly small amplitude. The resulting\n“features” indexed by n are thus uniformly sparse.\nBecause Hj preserves the norm, the total energy of coefﬁcients is conserved:\n∑\nn ∑\ni\n|HjSjxi(n)|2 = 2∑\ni\n∥Sjxi∥2.\nIt results that a sparse representation along the index i implies that HjSjxi(n) is also sparse along n. The same type of\nresult is thus obtained by replacing the mixed l1 and l2 norm (8) by a simpler l1 sparsity norm along both the i and n\nvariables\n∑\nn ∑\ni\n|HjSjxi(n)| .\n(9)\nThis sparsity norm is often used by sparse autoencoders for unsupervised learning of deep networks [2]. Numerical results\nin Section 4 verify that both norms have very close classiﬁcation performances.\nFor Haar operators Hj, the l1 norm leads to a simpler interpretation of the result. Indeed a Haar ﬁltering is deﬁned by\na pairing πj of d integers {1,...,d}. Optimizing Hj amounts to optimize πj, and hence minimize\n∑\nn ∑\ni\n|HjS jxi(n)| = ∑\nn\n\u0010\nSjxi(πj(2n))+Sjxi(π j(2n+1))+|Sjxi(πj(2n))−Sjxi(π j(2n+1))|\n\u0011\n.\nBut ∑n(Sjx(πj(2n))+Sjx(πj(2n+1))) = ∑n Sjx(n) does not depend upon the pairing πj. Minimizing the l1 norm (9) is\nthus equivalent to minimizing\n∑\nn ∑\ni\n|Sjxi(π j(2n))−S jxi(πj(2n+1))|.\n(10)\nIt minimizes the average variation within pairs, and thus tries to regroup pairs having close values.\nFinding a linear operator Hj which minimizes (8) or (9) is a “dictionary learning” problem which is in general an NP\nhard problem. For a Haar dictionary, we show that it is equivalent to a pair matching problem and can thus be solved with\nO(d3) operations. For both optimization norms, it amounts to ﬁnding a pairing πj which minimizes an additive cost\nC(πj) = ∑\nn\nC(πj(2n),πj(2n+1)),\n(11)\nwhere C(π j(2n),π j(2n+1)) = ∑i |HjSjxi(n)| for (9) and C(πj(2n),πj(2n+1)) =\n\u0010\n∑i |HjS jxi(n)|\n\u00112\nfor (8). This linear\npairing cost is minimized exactly by the Blossom algorithm with O(d3) operations. Greedy method obtains a 1/2-\napproximation in O(d2) time [24]. Randomized approximation similar to [12] could also be adapted to achieve a com-\nplexity of O(d logd) for very large size problems.\nTheorem 2.2 proves that several Haar scattering transforms are necessary to obtain a complete signal representation.\nWe learn T Haar scattering transforms by dividing the training set {xi}i in T non-overlapping subsets. A different Haar\nscattering transform is optimized for each training subset. Next section describes a supervised classiﬁer applied to the\nresulting bag of T Haar scattering transforms.\n2.4\nSupervised feature selection and classiﬁcation\nStrong invariants are computed by the supervised classiﬁer which essentially computes adapted linear combinations of\nHaar scattering coefﬁcients. Bagging T orthogonal Haar scattering representations deﬁnes a set of T d scattering coefﬁ-\ncients. A supervised dimension reduction is ﬁrst performed by selecting a subset of scattering coefﬁcients. It is imple-\nmented with an orthogonal least square forward selection algorithm [5]. The ﬁnal supervised classiﬁcation is implemented\n6\nwith a Gaussian kernel SVM classiﬁer applied to this reduced set of coefﬁcients.\nWe select K scattering coefﬁcient to discriminate each class c from all other classes, and decorrelate these features\nbefore applying the SVM classiﬁer. Discriminating a class c from all other classes amounts to approximating the indicator\nfunction\nfc(x) =\n(\n1\nif x belongs to class c\n0\notherwise\n.\nLet us denote by Φx = {φpx}p≤Td the dictionary of T d scattering coefﬁcients to which is added the constant φ0x =\n1. An orthogonal least square linearly approximates fc(x) with a sparse subset of K scattering coefﬁcients {φpk}k≤K\nwhich are greedily selected one at a time. To avoid correlations between selected features, it includes a Gram-Schmidt\northogonalization which decorrelates the scattering dictionary relatively to previously selected features. We denote by\nΦkx = { ˜φ k\npx}p the scattering dictionary, which was orthogonalized and hence decorrelated relatively to the ﬁrst k selected\nscattering features. For k = 0, we have Φ0x = Φx. At the k +1 iteration, we select φ k\npkx ∈Φkx which yields the minimum\nlinear mean-square error over training samples:\n∑\ni\n\u0010\nfc(xi)−\nk\n∑\nℓ=0\nαℓφ ℓ\npℓxi\n\u00112\n(12)\nBecause of the orthonormalization step, the linear regression coefﬁcients are\nαℓ= ∑\ni\nfc(xi)φ ℓ\npℓxi\nand\n∑\ni\n\u0010\nfc(xi)−\nk\n∑\nℓ=0\nαℓφ ℓ\npℓxi\n\u00112\n= ∑\ni\n|fc(xi)|2 −\nk\n∑\nℓ=0\nα2\nℓ.\nThe error (12) is thus minimized by choosing φ k\npk+1x having a maximum correlation:\nαk = ∑\ni\nfc(xi)φ k\npkxi = argmax\np\n\u0010\n∑\ni\nfc(xi)φ k\npxi\n\u0011\n.\nThe scattering dictionary is then updated by orthogonalizing each of its element relatively to the selected scattering feature\nφ k\npkx:\nφ k+1\np\nx = φ k\npx−\n\u0010\n∑\ni\nφ k\npxi φ k\npkxi\n\u0011\nφ k\npkx .\nThis orthogonal least square regression greedily selects the K decorrelated scattering features {φ k\npkx}0≤k<K for each\nclass c. For a total of C classes, the union of all these features deﬁnes a dictionary of size M = KC. They are linear\ncombinations of the original Haar scattering coefﬁcients {φpx}p. In the context of a deep neural network, this dimension\nreduction can be interpreted as a last fully connected network layer, which takes in input T d scattering coefﬁcients and\noutputs a vector of size M. The parameter M optimizes the bias versus variance trade-off. It may be set a priori or adjusted\nby cross validation in order to yield a minimum classiﬁcation error at the output of the Gaussian kernel SVM classiﬁer.\nA Gaussian kernel SVM classiﬁer is applied to the M-dimensional orthogonalized scattering feature vectors. The\nEuclidean norm of this vector is normalized to 1. In the applications of Section 4, M is set to 103 and hence remains large.\nSince the feature vectors lies on a high-dimensional unit sphere, the standard deviation σ of the Gaussian kernel SVM\nmust be of the order of 1. Indeed, a Gaussian kernel SVM performs its classiﬁcation by ﬁtting separating hyperplane over\ndifferent balls of radius of radius σ. If σ ≪1 then the number balls covering the unit sphere grows like σ−M. Since M is\nlarge, σ must remain in the order of 1 to insure that there are enough training samples to ﬁt a hyperplane in each ball.\n3\nOrthogonal Haar Scattering on Graphs\nSignals such as images are sampled on uniform grids. Many classiﬁcation problems are translation invariant, which\nmotivates the calculation of translation invariant representations. A translation invariant representation can be computed\n7\nx\nS1x\nS2x\nSJx\nFigure 2: A structured Haar scattering on a graph computes each layer S j+1x by pairing the rows of the previous layer S jx. For each\npair of rows, it stores their sum and the absolute values of their difference, in a twice bigger row.\nby averaging signal samples, but it removes too much information. Wavelet scattering operators [18] are calculated by\ncascading wavelet transforms and absolute values. Each wavelet transform computes multiscale signal variations on the\ngrid. It yields a large vector of coefﬁcients, whose spatial averaging deﬁnes a rich set of translation invariant coefﬁcients.\nData vectors may be deﬁned on non-uniform graphs [28], for example in social, ﬁnancial or transportation networks.\nA graph displacement moves data samples on the grid but is not equivalent to a uniform grid translation. Orthogonal\nHaar scattering transforms on graphs are computed from local multiscale signal variations on the graph. The calculation\nof displacement invariant features is left to the ﬁnal supervised classiﬁer, which adapts the averaging to the classiﬁcation\nproblem. Section 3.1 introduces this Har scattering on a graph as a particular case of orthogonal Haar scattering. Section\n3.3 proves that an orthogonal Haar scattering on graphs can be written as a product of wavelet transforms, as usual wavelet\nscattering operators. When the graph connectivity is unknown, unsupervised learning can calculate a Haar scattering on\nthe unknown graph and estimate the graph connectivity. The consistency of such estimations is studied in Section 3.4.\n3.1\nStructured orthogonal Haar scattering\nThe free orthogonal Haar scattering transform of Section 2 freely associates any two elements of an internal network\nlayer S jx. A Haar scattering on a graph is constructed by pairing elements according to their position in the graph, which\nrequires to structure the pairing and the network layers. We denote by V the set of d vertices of this graph, and assume\nthat d is a power of 2. The vector Sjx of size d is structured as a two-dimensional array S jx(n,q) of size 2−jd ×2j. For\neach j ≥0, we shall see that n ∈{1,...,2−jd} is a “spatial” index of a set of Vj,n of 2j graph vertices. The 2 j parameters\nq are indexing different permutation invariant coefﬁcients computed from the values of x in Vj,n. The input network layer\nis S0x(n,0) = x(n). We compute S j+1x by pairing the 2−jd rows of Sjx. The row pairing\nπj =\nn\n(πj(2n),πj(2n+1))\no\n0≤n<2−jd.\n(13)\nis pairing each (π j(2n),q) with (π j(2n + 1),q) for 0 ≤q < 2j. It imposes a row structure on the free pairing of Section\n2.1. Applying the absolute Haar ﬁlter (2) to each pair gives\nSj+1x(n,2q) = Sjx(πj(2n),q)+Sjx(π j(2n+1),q)\n(14)\nand\nS j+1x(n,2q+1) = |Sjx(π j(2n),q)−S jx(πj(2n+1),q)| .\n(15)\nApplying these equation for j ≤J deﬁnes a structured Haar network illustrated in Figure 2. If we remove the absolute\nvalue from (15) then these equations iterate linear Haar ﬁlters and deﬁne an orthogonal Walsh transform [6]. The absolute\nvalue completely modiﬁes the properties of this transform but Section 3.3 proves that it can be still be written as a product\nof orthogonal Haar wavelet transforms, alternating with absolute value non-linearities.\nThe following proposition proves that this structured Haar scattering is a transformation on a hierarchical grouping of\nthe graph vertices, derived from the row pairing (13). Let V0,n = {n} for n ∈V. For any j ≥0 and n ∈{1,...,2−j−1d}, we\n8\n(a)\n(b)\n(c)\nFigure 3:\n(a,b): Two different examples of hierarchical partitions of a graph into connected sets Vj,n of size 2 j, for j = 1 (green),\nj = 2 (purple) and j = 3 (red). (c) Hierarchical partitions on a square image grid.\ndeﬁne\nVj+1,n = Vj,πj(2n) ∪Vj,πj(2n+1).\n(16)\nWe verify by induction on j that it deﬁnes a partition V = ∪nVj,n, where each Vj,n is a set of 2j vertices.\nProposition 3.1. The coefﬁcients {SJx(n,q)}0≤q<2 j are computed by applying a Hadamard matrix to the restriction of x\nto VJ,n. This Hadamard matrix depends on x, J and n.\nProof. Theorem 2.1 proves that {SJx(n,q)}0≤q<2J is computed by applying am orthogonal transform to x. To prove that\nit is a Hadamard matrix, it is sufﬁcient to show that its entries are ±1. We verify by induction on j ≤J that Sjx(n,q) only\ndepends on restriction of x to Vj,n, by applying (15) and (14) together with (16). We also verify that each x(v) for v ∈Vj,n\nappears exactly once in the calculation, with an addition or a subtraction. Because of the absolute value, the addition or\nsubtraction which are 1 and −1 in the Hadamard matrix, which therefore depends upon x, J and n\nAn orthogonal Haar scattering on a graph can thus be interpreted as an adaptive Hadamard transform over groups of\nvertices, which outputs positive coefﬁcients. Walsh matrices are particular cases of Hadamard matrices. The induction\n(16) deﬁnes sets Vj,n with connected nodes in the graph if for all j and n, each pair (π j(2n),π j(2n+1)) regroups two sets\nVj,πj(2n) and Vj,πj(2n+1) which are connected. It means that at least one element of Vj,πj(2n) is connected to one element\nof Vj,πj(2n+1). There are many possible connected dyadic partitions of any given graph. Figure 3(a,b) shows two different\nexamples of connected graph partitions.\nFor images sampled on a square grid, a pixel is connected with 8 neighbors. A structured Haar scattering can be\ncomputed by pairing neighbor image pixels, alternatively along rows and columns as the depth j increases. When j is\neven, each Vj,n is then a square group of 2 j pixels, as illustrated in Figure 3(c). Shifting such a partition deﬁnes a new\npartition. Neighbor pixels can also be grouped in the diagonal direction which amounts to rotate the sets Vj,n by π/4 to\n9\ndeﬁne a new dyadic partition. Each of these partitions deﬁne a different structured Haar scattering. Section 4 applies these\nstructured Haar image scattering to image classiﬁcation.\n3.2\nScattering order\nScattering coefﬁcients have very different properties depending upon the number of absolute values which are used to\ncompute them. A scattering coefﬁcient of order m is a coefﬁcient computed by cascading m absolute values. Their\namplitude have a fast decay as the order m increases, and their locations are speciﬁed by the following proposition.\nProposition 3.2. If q = 0 then S jx(n,q) is a coefﬁcient of order 0. Otherwise, Sjx(n,q) is a coefﬁcient of order m ≤j if\nthere exists 0 ≤j1 < ... < jm < j such that\nq =\nm\n∑\nk=1\n2j−jk .\n(17)\nThere are\n\u0000 j\nm\n\u0001\n2−jd coefﬁcients of order m in S jx.\nProof. This proposition is proved by induction on j. For j = 0 all coefﬁcients are of order 0 since S0x(n,0) = x(n). If\nSjx(n,q) is of order m then (14) and (15) imply that Sj+1x(n,2q) is of order m and Sj+1x(n,2q+1) is of order m+1. It\nresults that (17) is valid for j +1 if is valid for j.\nThe number of coefﬁcients Sjx(n,q) of order m corresponds to the number of choices for q and hence for 0 ≤j1 <\n... < jm < j, which is\n\u0000 j\nm\n\u0001\n. This must be multiplied by the number of indices n which is 2−jd.\nThe amplitude of scattering coefﬁcients typically decreases exponentially when the scattering order m increases, be-\ncause of the contraction produced by the absolute value. High order scattering coefﬁcients can thus be neglected. This is\nillustrated by considering a vector x of independent Gaussian random variables of variance 1. The value of Sjx(n,q) only\ndepends upon the values of x in Vj,n. Since Vj,n does not intersect with Vj,n′ if n ̸= n′, we derive that Sj(n,q) and Sj(n′,q)\nare independent. They have same mean and same variance because x is identically distributed. Scattering coefﬁcients are\niteratively computed by adding pairs of such coefﬁcients, or by computing the absolute value of their difference. Adding\ntwo independent random variables multiplies their variance by 2. Subtracting two independent random variables of same\nmean and variance yields a new random variable whose mean is zero and whose variance is multiplied by 2. Taking\nthe absolute value reduces the variance by a factor which depends upon its probability distribution. If this distribution is\nGaussian then this factor is 1−2/π. If we suppose that this distribution remains approximately Gaussian, then applying\nm absolute values reduces the variance by approximately (1 −2/π)m. Since there are\n\u0000J\nm\n\u0001\ncoefﬁcients of order m, their\ntotal normalized variance σ2\nm,J is approximated by\n\u0000J\nm\n\u0001\n(1 −2/π)m. Table 1 shows that\n\u0000J\nm\n\u0001\n(1 −2/π)m is indeed of the\nsame order of magnitude as the value σ2\nm,J computed numerically. This variance becomes much smaller for m > 4. This\nobservation remains valid for large classes of signals x. Scattering coefﬁcients of order m > 4 usually have a negligible\nenergy and are thus removed in classiﬁcation applications.\nm\n1\n2\n3\n4\n5\nσ2\nm,J\n1.8\n1.4\n5.8×10−1\n1.2×10−1\n1.2×10−2\n(1−2\nπ )m ·\n\u0000J\nm\n\u0001\n1.8\n1.3\n4.8×10−1\n8.7×10−2\n6.3×10−3\nTable 1: σ2\nm,J is the normalized variance of all order m coefﬁcients in SJx, computed for a Gaussian white noise x with J = 5. It is\ndecays approximately like (1−2\nπ )m ·\n\u0000J\nm\n\u0001\n.\n3.3\nScattering with orthogonal Haar wavelet bases\nWe now prove that scattering coefﬁcients of order m are obtained by cascading m orthogonal Haar wavelet transforms\ndeﬁned on the graph. Haar wavelets can easily be constructed on graphs [7, 26]. Section 3.1 shows that a Haar scattering\non a graph is constructed over dyadic partitions {Vj,n}n of V, which are obtained by progressively aggregating vertices by\npairing Vj+1,n = Vj,πj(2n) ∪Vj,πj(2n+1). We denote by 1Vj,n(v) the indicator function of Vj,n in V. A Haar wavelet computes\n10\nthe difference between the sum of signal values over two aggregated sets:\nψ j+1,n = 1Vj,πj(2n) −1Vj,πj(2n+1) .\n(18)\nInner products between signals deﬁned on V are written\n⟨x,x′⟩= ∑\nv∈V\nx(v)x′(v).\nFor any 2J < d,\n{1VJ,n}0≤n<2−Jd ∪{ψj,n}0≤n<2−jd,0≤j<J\n(19)\nis a family of d orthogonal Haar wavelets which deﬁne an orthogonal basis of Rd. The following theorem proves that\norder m+1 coefﬁcients are obtained by computing the orthogonal Haar wavelet transform of coefﬁcients of order m. The\nproof is in Appendix A.\nTheorem 3.1. Let q = ∑m\nk=1 2j−jk with j1 < ... < jm ≤j. If jm+1 > jm then for each n ≤2−j−1d\nS jx(n,q+2j−jm+1) =\n∑\np\nVjm+1,p⊂Vj,n\n|⟨S jmx(·,2jm−jq),ψjm+1,p⟩|.\n(20)\nwith\nS jmx(.,q′) =\n2−jmd−1\n∑\nn=0\nSjmx(n,q′)1Vjm,n.\nIf q = ∑m\nk=1 2 j−jk and jm+1 > jm then Sjmx(n,2jm−jq) are coefﬁcients of order m whereas Sjx(n,q + 2j−jm+1) is a\ncoefﬁcient of order m + 1. Equation (20) proves that a coefﬁcient of order m + 1 is obtained by calculating the wavelet\ntransform of scattering coefﬁcients of order m, and summing their absolute values. A coefﬁcient of order m + 1 thus\nmeasures the averaged variations of the m-th order scattering coefﬁcients on neighborhoods of size 2 jm+1 in the graph. For\nexample, if x is constant in a Vj,n then Sℓx(n,q) = 0 if ℓ≤j and q ̸= 0.\n3.4\nLearning graph connectivity by variation minimization\nIn many problems the graph connectivity is unknown. Learning a connected dyadic partitions is easier than learning the\nfull connectivity of a graph, which is typically an NP complete problem. Section 2.3 introduces a polynomial complexity\nalgorithm, which learns pairings in orthogonal Haar scattering networks. For a Haar scattering on a graph, we show that\nthis algorithms amounts to computing dyadic partitions where scattering coefﬁcients have a minimum total variation. The\nconsistency of this pairing algorithm is studied over particular Gaussian stationary processes, and we show that there is\nno curse of dimensionality.\nSection 2.3 introduces two criteria to optimize the pairing πj of a free orthogonal Haar scattering, from a training set\n{xi}i≤N. We concentrate on the l1 norm minimization, which has a simpler expression. For a Haar scattering on a graph,\nthe l1 minimization (10) computes a row pairing πj which minimizes\nN\n∑\ni=1\nd2−(j+1)\n∑\nn=0\n2 j−1\n∑\nq=0\n|Sjxi(π j(2n),q)−S jxi(πj(2n+1),q)|.\n(21)\nThis optimal pairing regroups vertex sets Vj,πj(2n) and Vj,πj(2n+1) whose scattering coefﬁcients have a minimum total\nvariation.\nSuppose that the N training samples xi are independent realizations of a random vector x. To guarantee that this pairing\nﬁnds connected sets we must make sure that the total variation minimization favors regrouping neighborhood points, which\nmeans that x has some form of regularity on the graph. We also need N to be sufﬁciently large so that this minimization\nﬁnds connected sets with high probability, despite statistical ﬂuctuations. Avoiding the curse of dimensionality means that\nN should not grow exponentially with the signal dimension d.\n11\nTo attack this problem mathematically, we consider a very particular case, where signals are deﬁned on a ring graph,\nand are thus d periodic. Two indices n and n′ are connected if |n −n′| = 1 mod d. We study the optimization of the\nﬁrst network layer for j = 0, where S0x(n,q) = x(n). The minimization of (21) amounts to compute a pairing π which\nminimizes\nN\n∑\ni=1\n d/2−1\n∑\nn=0\n|xi(π(2n))−xi(π(2n+1))|\n!\n.\n(22)\nThis pairing is connected if and only if for all n, |π(2n)−π(2n+1)| = 1 mod d.\nFigure 4: Each image pixel gives the probability that the total variation minimization (22) ﬁnds pairs which are all connected, when x\nis a Gaussian stationary vector. It is computed as a function of the dimension d of the vector (horizontal axis) and of the number N of\ntraining samples (vertical axis). Black and white points are probabilities respectively equal to 0 and 1. The blue dotted line corresponds\nto a probability 0.8.\nThe regularity and statistical ﬂuctuations of x(n) are controlled by supposing that x is a circular stationary Gaussian\nprocess. The stationarity implies that its covariance matrix Cov(x(n),x(m)) = Σ(n,m) depends on the distance between\npoints Σ(n,m) = ρ((n−m) mod d). The average regularity depends upon the decay of the correlation ρ(u). We denote\nby ∥Σ∥op the sup operator norm of Σ. The following theorem proves that the training size N must grow like d logd in order\nto compute an optimal pairing with a high probability. The constant is inversely proportional to a normalized “correlation\ngap,” which depends upon the difference between the correlation of neighborhood points and more far away points. It is\ndeﬁned by\n∆=\n s\n1−maxn≥2 ρ(n)\nρ(0)\n−\ns\n1−ρ(1)\nρ(0)\n!2\n.\n(23)\nTheorem 3.2. Given a circular stationary Gaussian process with ∆> 0, the pairing which minimizes the empirical total\nvariation (22) has probability larger than 1−ε to be connected if\nN > π3∥Σ∥op\n2∆\nd\n\u0010\n3logd −logε\n\u0011\n.\n(24)\nThe proof is based on the Gaussian concentration inequality for Lipschitz function [20, 23] and is left to Appendix B .\nFigure 4 displays numerical results obtained with a Gaussian stationary process of dimension d where ρ(1)/ρ(0) = 0.44\nand maxn≥2 ρ(n)/ρ(0) = 0.06. The gray level image gives the probability that a pairing is connected when computing this\npairing by minimizing the total variation (22), as a function of the dimension d and of the number N of training samples.\nThe black and white points correspond to probabilities 0 and 1 respectively. In this example, we see that the optimization\ngives a connected pairing with probability 1−ε for N increasing almost linearly with d, which is illustrated by the nearly\nstraight line of dotted points corresponding to ε = 0.2. The Theorem gives an upper bound which grows like d logd,\nthough the constant involved is not tight.\nFor layer j > 1, Sjx(n,q) is no longer a Gaussian random vector due to the absolute value non-linearity. However, the\nresult can be extended using a Talagrand-type concentration argument instead of the Gaussian concentration. Numerical\nexperiments presented in Section 4 show that this approach does recover the connectivity of high dimensional images\nwith a probability close to 100% for j ≤3, and that the probability decreases as j increases. This seems to be due to the\n12\nFigure 5: MNIST images (left) and images after random pixel permutations (right).\nfact that the absolute value contractions reduce the correlation gap ∆between connected coefﬁcients and more far away\ncoefﬁcients when j increases.\n4\nNumerical classiﬁcation experiments\nHaar scattering representations are tested on classiﬁcation problems, over images sampled on a regular grid or an irregular\ngraph. We consider the cases where the grid or the graph geometry is known a priori, or discovered by unsupervised\nlearning. The efﬁciency of free and structured Haar scattering architectures are compared with state of the art classi-\nﬁcation results obtained by deep neural network learning, when the graph geometry is known or unknown. Although\ncomputations are reduced to additions and subtractions, we show a Haar scattering can get state art results when the graph\ngeometry is unknown even over complex image data bases. For images over a known uniform sampling grid, we show\nthat the simpliﬁcations of a Haar scattering produces an error about 20% larger than state of the art unsupervised learning\nalgorithms.\nA Haar scattering classiﬁcation involves few parameters which are reviewed. The scattering scale 2J ≤d is the\npermutation invariance scale. Scattering coefﬁcients are computed up to the a maximum order m, which is set to 4 in all\nexperiments. Indeed, higher order scattering coefﬁcient have a negligible relative energy, which is below 1%, as explained\nin Section 3.2. The unsupervised learning algorithm computes T different Haar scattering transforms by subdividing the\ntraining set in T subsets. Increasing T decreases the classiﬁcation error but it increases computations. The error decay\nbecomes negligible for T ≥40. The supervised dimension reduction selects a ﬁnal set of M orthogonalized scattering\ncoefﬁcients. We set M = 1000 in all numerical experiments.\n4.1\nClassiﬁcation of image digits in MNIST\nMNIST is a data basis with 6 × 104 hand-written digit images of size d ≤210. There are 10 classes (one per digit)\nwith 5 × 104 images for training and 104 for testing. Examples of MNIST images are shown in Figure 5. To test the\nclassiﬁcation performances of a Haar scattering when the geometry is unknown, we scramble all image pixels with the\nsame unknown random permutations, as shown in Figure 5.\nCNN (Supervised) [15]\n0.53\nSparse Coding (Unsupervised) [13]\n0.59\nGabor Scattering [3]\n0.43\nStructured Haar Scattering\n0.59\n(a) Known Geometry\nMaxout MLP + dropout [8]\n0.94\nDeep convex net. [29]\n0.83\nDBM + dropout [10]\n0.79\nStructured Haar Scattering\n0.90\n(b) Unknown Geometry\nFigure 6: Percentage of errors for the classiﬁcation of MNIST images, obtained by different algorithms.\nWhen the image geometry is known, i.e. using non-scrambled images, the best MNIST classiﬁcation results without\ndata augmentation are given in Table 6a. Deep convolution networks with supervised learning reach an error of 0.53%\n[15], and unsupervised learning with sparse coding have a slightly larger error of 0.59% [13]. A wavelet scattering\ncomputed with iterated Gabor wavelet transforms yields an error of 0.46% [3].\n13\nFor a known image grid geometry, we compute a structured Haar scattering by pairing neighbor image pixels. It builds\nhierachical square subsets Vj,n illustrated in Figure 3(c). The invariance scale is 2J = 26, which corresponds to blocks\nof 8 × 8 pixels. Random shift and rotations of these pairing deﬁne T = 64 different Haar scattering transforms. The\nsupervised classiﬁer of Section 2.4 applied to this structured Haar scattering yields an error of 0.59%.\nMNIST digit classiﬁcation is a relatively simple problem where the main source of variability are due to deformations\nof hand-written image digits. In this case, supervised convolution networks, sparse coding, Gabor wavelet scattering and\northogonal Haar scattering have nearly the same classiﬁcation performances. The fact that a Haar scattering is only based\non additions and subtractions does not affect its efﬁciency.\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\nClassification Error Rate\nNumber of Scattering Tranforms\n \n \nJ = 8\nJ = 9\nJ = 10\nFigure 7: Unsupervised Haar scattering classiﬁcation error for MNIST, as a function of the number T of scattering transforms, for\nnetworks of depth J = 8,9,10.\nFor scrambled images, the connectivity of image pixels is unknown and needs to be learned from data. Table 6b gives\nthe classiﬁcation results of different learning algorithms. The smallest error of 0.79% is obtained with a Deep Belief\noptimized with a supervised backpropagation. Unsupervised learning of T = 50 structured Haar scattering followed by\na feature selection and a supervised SVM classiﬁer produces an error of 0.90%. Figure 7 gives the classiﬁcation error\nrate as a function of T, for different values of maximum scale J. The error rates decrease slowly for T > 10, and do not\nimprove beyond T = 50, which is much smaller than 2J.\nThe unsupervised learning computes connected dyadic partitions Vj,n from scrambled images by optimizing an l1\nnorm. At scales 1 ≤2j ≤23, 100% of these partitions are connected in the original image grid, which proves that the\ngeometry is well estimated at these scales. This is only evaluated on meaningful pixels which do not remain zero on\nall training images. For j = 4 and j = 5 the percentages of connected partitions are respectively 85% and 67%. The\npercentage of connected partitions decreases because long range correlations are weaker.\nA free orthogonal Haar scattering does not impose any condition on pairings. It produces a minimum error of 1% for\nT = 20 Haar scattering transforms, computed up to the depth J = 7. This error rate is higher because the supplement of\nfreedom in the pairing choice increases the variance of the estimation.\n4.2\nCIFAR-10 images\nCIFAR-10 is a data basis of tiny color images of 32 × 32 pixels. It includes 10 classes, such as “dogs”, “cars”, “ships”\nwith a total of 5 × 104 training examples and 104 testing examples. There are much more intra-class variabilities than\nin MNIST digit images, as shown by Figure 8. The 3 color bands are represented with Y,U,V channels, and scattering\ncoefﬁcients are computed independently in each channel.\nWhen the image geometry is known, a structured Haar scattering is computed by pairing neighbor image pixels. The\nbest performance is obtained at the scale 2J = 26 which is below the maximum scale d = 210. Similarly to MNIST, we\ncompute T = 64 connected dyadic partitions for randomly translated and rotated grids. After dimension reduction, the\nclassiﬁcation error is 21.3%. This error is above state of the art results of unsupervised learning algorithms by about\n14\nFigure 8: Examples of CIFAR-10 images in the classes of “cars”, “dogs” and “boats”.\n20%, but it involves no learning. A minimum error rate of 16.9% is obtained by Receptive Field Learning [11]. The Haar\nscattering error is also above the 17.8% error obtaiend by a roto-translation invariant wavelet scattering network [22],\nwhich computes wavelet transforms along translation and rotation parameters. Supervised deep convolution networks\nprovide an important improvement over all unsupervised techniques and reach an error of 9.8%. The study of these\nsupervised networks is however beyound the scope of this paper. Results are summarized in Table 9a.\nWhen the image grid geometry is unknown, because of random scrambling, Table 9a summarizes results with different\nalgorithms. For unsupervised learning with structured Haar scattering, the minimum classiﬁcation error is reached at the\nscale 2J = 27, which maintains some localization information on scattering coefﬁcients. With T = 10 connected dyadic\npartitions, the error is 27.3%. Table 9b shows that it is 10% below previously reported results on this data basis.\nNearly 100% of the dyadic paritions Vj,n computed from scrambled images are connected in the original image grid,\nfor 1 ≤j ≤4, which shows that the multiscale geometry is well estimated at these ﬁne scales. For j = 5,6 and 7,\nthe proportions of connected partitions are 98%, 93% and 83% respectively. As for MNIST images, the connectivity\nestimation becomes less precise at large scales. Similarly to MNIST, a free Haar scattering yields a higher classiﬁcation\nerror of 29.2%, with T = 20 scattering transforms up to layer J = 6.\nCNN (Supervised state-of-the-art) [16]\n9.8\nRFL (Unsupervised state-of-the-art) [11]\n16.9\nRoto-Translation Scattering [22]\n17.8\nStructured Haar Scattering\n21.3\n(a) Known Geometry\nFastfood [14]\n37.6\nFastfood FFT [14]\n36.9\nRandom Kitchen Sinks [14]\n37.6\nStructured Haar Scattering\n27.3\n(b) Unknown Geometry\nFigure 9: Percentage of errors for the classiﬁcation of CIFAR-10 images, obtained by different algorithms.\n4.3\nCIFAR-100 images\nCIFAR-100 also contains tiny color images of the same size as CIFAR-10 images. It has 100 classes containing 600 images\neach, of which 500 are training images and 100 are for testing. Our tests on CIFAR-100 follows the same procedures as\nin Section 4.2. The 3 color channels are processed independently.\nWhen the image grid geometry is known, the results of a structured Haar scattering are summarized in Table 10. The\nbest performance is obtained with the same parameter combination as in CIFAR-10, which is T = 64 and 2J = 26. After\ndimension reduction, the classiﬁcation error is 47.4%. As in CIFAR-10, this error is about 20% larger than state of the\nart unsupervised methods, such as a Nonnegative OMP (39.2%)[17]. A roto-translation wavelet scattering has an error of\n43.7%. Deep convolution networks with supervised training produce again a lower error of 34.6%.\nFor scrambled images of unknown geometry, with T = 10 transforms and a depth J = 7, a structured Haar scattering\nhas an error of 52.7%. A free Haar orthogonal scattering has a higher classiﬁcation error of 56.1%, with T = 10 scattering\ntransforms up to layer J = 6. No such result is reported with another algorithm on this data basis.\nOn all tested image databases, structured Haar scattering has a consistent 7%-10% performance advantage over ‘free’\n15\nHaar scattering, as shown by Table 2. For orthogonal Haar scattering, all reported errors were calculated with an unsuper-\nvised learning which minimizes the l1 norm (9) of scattering coefﬁcients, layer per play. As expected, Table 2 shows that\nminimizing a mixed l1 and l2 norm (8) yields nearly the same results on all data bases.\nCNN (Supervised state-of-the-art) [16]\n34.6\nNOMP (Unsupervised state-of-the-art) [17]\n39.2\nGabor Scattering [22]\n43.7\nStructured Haar Scattering\n47.4\nFigure 10: Percentage of errors for the classiﬁcation of CIFAR-100 images with known geometry, obtained by different algorithms.\nStructured, l1\nStructured, l1/l2\nFree, l1\nFree, l2/l1\nMNIST\n0.91\n0.95\n1.09\n1.02\nCIFAR-10\n28.8\n27.3\n29.2\n29.3\nCIFAR-100\n52.5\n53.1\n56.3\n56.1\nTable 2: Percentage of errors for the classiﬁcation of MNIST, CIFAR-10 and CIFAR-100 images with a structured or a free Haar\nscattering, for unsupervised computed by minimizing a mixed l1/l2 norm or an l1 norm.\n4.4\nImages on a graph over a sphere\nA data basis of irregularly sampled images on a sphere is provided in [4]. It is constructed by projecting the MNIST image\ndigits on d = 4096 points randomly sampled on the 3D sphere, and by randomly rotating these images on the sphere. The\nrandom rotation is either uniformly distributed on the sphere or restricted with a smaller variance (small rotations) [4].\nThe digit ‘9’ is removed from the data set because it can not be distinguished from a ‘6’ after rotation. Examples sphere\ndigits are shown in Figure 11. This geometry of points on the sphere can be described by a graph which connects points\nhaving a sufﬁciently small distance on the sphere.\nThe classiﬁcation algorithms introduced in [4] take advantage of the known distribution of points on the sphere, with\na representation based on the graph Laplacian. Table 3 gives the results reported in [4], with a fully connected neural\nnetwork, and with a spectral graph Laplacian network.\nNearest\nFully\nSpectral\nStructured Haar\nFree Haar\nNeighbors\nConnect.\nNet.[4]\nScattering\nScattering\nSmall rotations\n19\n5.6\n6\n2.2\n1.6\nLarge rotations\n80\n52\n50\n47.7\n55.8\nTable 3: Percentage of errors for the classiﬁcation of MNIST images rotated and sampled on a sphere [4], with a nearest neighbor\nclassiﬁer, a fully connected two layer neural network, a spectral network [4], and an unsupervised Haar scattering.\nAs opposed to these algorithms, the unsupervised structured Haar scattering algorithm does not use this geometric\ninformation and learns the graph information by pairing. Computations are performed on a scrambled set of signal values.\nHaar scattering transforms are calculated up to the maximum scale 2J = d = 212. A total of T = 10 connected dyadic\npartitions are estimated by unsupervised learning, and the classiﬁcation is performed from M = 103 selected coefﬁcients.\nAlthough the graph geometry is unknown, the structured Haar scattering reduces the error rate both for small and large\n3D random rotations.\nIn this case a free orthogonal Haar scattering has a smaller error rate than a structured Haar scattering for small\nrotations, but a larger error for large rotations. It illustrates the tradeoff between the structural bias and the feature\nvariance in the choice of the algorithms. For small rotation, the variability within classes is smaller and a free scattering\ncan take advantage of more degrees of freedom. For large rotations, the variance is too large and dominates the problem.\nTwo points of the sphere of radius 1 are considered to be connected if their geodesic distance is smaller than 0.1. With\nthis convention, over the 4096 points, each point has on average 8 connected neighbors. The unsupervised Haar learning\n16\nFigure 11: Images of digits mapped on a sphere.\nperforms a hierachical pairing of points on the sphere. For small and large rotations, the percentage of connected sets\nVj,n remains above 90% for 1 ≤j ≤4. This is computed over 70% of the points points having a nonneglegible energy. It\nshows that the multiscale geometry on the sphere is well estimated by hierachical pairings.\nAcknowledgment\nThis work was supported by the ERC grant InvariantClass 320959.\nReferences\n[1] F. Anselmi, J. Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, and T. Poggio. Unsupervised learning of invariant\nrepresentations in hierarchical architectures. arXiv preprint arXiv:1311.4158, 2013.\n[2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, 35(8):1798–1828, 2013.\n[3] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Trans. PAMI, 35(8):1872–1886, 2013.\n[4] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and deep locally connected networks on graphs.\nICLR, 2014.\n[5] S. Chen, C. F. Cowan, and P. M. Grant.\nOrthogonal least squares learning algorithm for radial basis function\nnetworks. Neural Networks, IEEE Transactions on, 2(2):302–309, 1991.\n[6] C. Coifman, Y. Meyer, and M. Wickerhauser. Wavelet analysis and signal processing. pages 153–178, 1992.\n[7] M. Gavish, B. Nadler, and R. R. Coifman. Multiscale wavelets on trees, graphs and high dimensional data: Theory\nand applications to semi supervised learning. pages 367–374, 2010.\n[8] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Benjio.\nMaxout networks.\narXiv preprint\narXiv:1302.4389, 2013.\n[9] G. Hinton, S. Osindero, and Y.-W. Teh.\nA fast learning algorithm for deep belief nets.\nNeural computation,\n18(7):1527–1554, 2006.\n[10] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural networks by\npreventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\n[11] Y. Jia, C. Huang, and T. Darrell. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image features.\nCVPR, pages 3370–3377, 2012.\n[12] P. W. Jones, A. Osipov, and V. Rokhlin. Randomized approximate nearest neighbors algorithm. Proceedings of the\nNational Academy of Sciences, 108(38):15679–15686, 2011.\n17\n[13] K. Labusch, E. Barth, and Martinetz. T. Simple method for highperformance digit recognition based on sparse\ncoding. IEEE TNN, 19(11):1985–1989, 2008.\n[14] Q. Le, T. Sarlos, and A. Smola. Fastfood - approximating kernel expansions in loglinear time. ICML, 2013.\n[15] Y. LeCun, K. Kavukvuoglu, and C. Farabet. Convolutional networks and applications in vision. Proc. IEEE Int.\nSump. Circuits and Systems, 2010.\n[16] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply supervised nets. arXiv preprint arXiv:1409.5185, 2014.\n[17] T.-H. Lin and H.-T. Kung. Stable and efﬁcient representation learning with nonnegativity constraints. ICML, 2014.\n[18] S. Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331–1398, 2012.\n[19] S. Mallat and I. Waldspurger. Deep learning by scattering. arXiv preprint arXiv:1306.5532, 2013.\n[20] B. Maurey. Some deviation inequalities. Geometric & Functional Analysis, 1(2):188–197, 1991.\n[21] J. Ngiam, Z. Chen, D. Chia, P. W. Koh, Q. V. Le, and A. Y. Ng. Tiled convolutional neural networks. In Advances\nin Neural Information Processing Systems, pages 1279–1287, 2010.\n[22] E. Oyallon and S. Mallat. Deep roto-translation scattering for object classiﬁcation. arXiv preprint arXiv:1412.8659,\n2014.\n[23] G. Pisier. Probabilistic methods in the geometry of banach spaces. Springer Lecture Notes in Math. 1206, pages\n167–241, 1985.\n[24] R. Preis. Linear time 1/2-approximation algorithm for maximum weighted matching in general graphs. pages\n259–269, 1999.\n[25] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive auto-encoders: Explicit invariance during\nfeature extraction. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages\n833–840, 2011.\n[26] R. Rustamov and L. Guibas. Wavelets on graphs via deep learning. pages 998–1006, 2013.\n[27] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localiza-\ntion and detection using convolutional networks. arXiv preprint arXiv:1312.6229, 2013.\n[28] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. The emerging ﬁeld of signal processing\non graphs: Extending high-dimensional data analysis to networks and other irregular domains. Signal Processing\nMagazine, IEEE, 30(3):83–98, 2013.\n[29] D. Yu and L. Deng. Deep convex net: A scalable architecture for speech pattern classiﬁcation. in Proc. INTER-\nSPEECH, pages 2285–2288, 2011.\n18\nA\nProof of Theorem 3.1\nProof of Theorem 3.1. We derive from the deﬁnition of a scattering transform in equations (3,4) in the text that\nSj+1x(n,2q) = Sjx(πj(2n),q)+Sjx(πj(2n+1),q) = ⟨S jx(·,q),1Vj+1,n⟩,\nSj+1x(n,2q+1) = |Sjx(πj(2n),q)−Sjx(π j(2n+1),q)| = |⟨S jx(·,q),ψj+1,n⟩|.\nwhere Vj+1,n = Vj,π j(2n) ∪Vj,πj(2n+1). Deﬁne κ = 2−jq = ∑m\nk=1 2−jk. Observe that\n2jm+1(κ +2−jm+1) = 2jm+1κ +1 = 2(2jm+1−1κ)+1,\nthus Sjm+1x(n,2 jm+1(κ +2−jm+1)) is calculated from the coefﬁcients S jm+1−1x(n,2jm+1−1κ) of the previous layer with\nSjm+1x(n,2jm+1(κ +2−jm+1)) = |⟨S jm+1−1x(·,2jm+1−1κ),ψjm+1,n⟩|.\n(A.1)\nSince 2 j+1κ = 2 · 2jκ, the coefﬁcient Sjm+1−1x(n,2jm+1−1κ) is calculated from Sjmx(n,2jmκ) by (jm+1 −1 −jm) times\nadditions, and thus\nSjm+1−1x(n,2jm+1−1κ) = ⟨S jmx(·,2jmκ),1Vjm+1−1,n⟩.\n(A.2)\nCombining equations (A.2) and (A.1) gives\nSjm+1x(n,2jm+1(κ +2−jm+1)) = |⟨S jmx(·,2jmκ),ψjm+1,n⟩|.\n(A.3)\nWe go from the depth jm+1 to the depth j ≥jm+1 by computing\nS jx(n,2j(κ +2−jm+1)) = ⟨S jm+1x(·,2jm+1(κ +2−jm+1)),1Vj,n⟩.\nTogether with (A.3) it proves the equation (20) of the proposition. The summation over p, Vjm+1,p ⊂Vj,n comes from the\ninner product ⟨1Vjm+1,p,1Vj,n⟩. This also proves that κ +2−jm+1 is the index of a coefﬁcient of order m+1.\nB\nProof of Theorem 3.2\nThe theorem is proved by analyzing the concentration of the objective function around its expected value as the sample\nnumber N increases. We ﬁrstly introduce the Pisier and Maurey’s version of the Gaussian concentration inequality for\nLipschitz functions.\nProposition B.1 (gaussian concentration for Lipschitz function [23, 20]). Let z1,...zm be i.i.d N(0,1) random variabls,\nand f = f(z1,··· ,zm) a 1-Lipschitz function, then there exists c0 > 0 so that\nPr[f −Ef > t] < exp{−c0t2} and Pr[f −Ef < −t] < exp{−c0t2},\n∀t > 0.\nIn the above proposition, the constant c0 = 2\nπ2 according to [23] and 1/4 in [20].\nTo prove the theorem, recall that the pairing problem is computed by minimizing the l1 norm (22) which up to a\nnormalization amounts to compute:\nπ∗= arg min\nπ∈Πd\nF(π) with F(π) = 1\nN\nN\n∑\ni=1 ∑\n(u,v)∈π\n|xi(u)−xi(v)|,\n(B.1)\nwhere π is a pairing of d elements and we denote by Πd the set of all possible such pairings.\nThe following lemma proves that F(π) is a Lipschitz function of independent gaussian random variables, with a\nLipschitz constant equal to ∥Σd∥1/2\nop , where ∥Σd∥op is the operator norm of the covariance. We prove it on the normalized\nfunction f = N1/2d−1/2F.\n19\nLemma B.1. Let xi = Σ1/2\nd zi with zi = (zi(1),...,zi(d))T ∼N (0,Id) i.i.d. Given any pairing π ∈Πd, deﬁne\nf({(zi(v)}1≤i≤N,1≤v≤d) =\n1\n√\ndN\nN\n∑\ni=1 ∑\n(u,v)∈π\n|xi(u)−xi(v)|,\nthen f is a Lipschitz function with constant\np\n∥Σd∥op, which does not depend on π.\nProof. With slight abuse of notation, denote by v = π(u) if two nodes u and v are paired by π, then we have\n∂f\n∂zi(v′)\n=\n1\n√\ndN ∑\n(u,v)∈π\nSgn(xi(u)−xi(v))\n∂\n∂zi(v′)(xi(u)−xi(v))\n=\n1\n√\ndN\nd\n∑\nu=1\nSgn(xi(u)−xi(π(u)))\n∂\n∂zi(v′)xi(u)\n=\n1\n√\ndN\nd\n∑\nu=1\nSgn(xi(u)−xi(π(u)))(Σ1/2\nd )u,v′\n=\n1\n√\ndN\n(Σ1/2\nd Si)(v′),\nwhere Si := (Sgn(xi(u)−xi(π(u))))d\nu=1 is a vector of length d whose entries are ±1. Then\n∥Σ1/2\nd Si∥≤\nq\n∥Σd∥opd,\nand it follows that\n∥∇z f∥2\n=\nN\n∑\ni=1\nd\n∑\nv′=1\n\f\f\f\f\n∂f\n∂zi(v′)\n\f\f\f\f\n2\n=\nN\n∑\ni=1\n1\ndN ∥Σ1/2\nd Si∥2 ≤∥Σd∥op.\nObserve that the eigenvalues of Σd are the discrete Fourier transform coefﬁcients of the periodic correlation function\nρ(u)\nˆρ(k) =\nd−1\n∑\nj=0\nρ(j)exp{−i2π jk\nd } =\nd−1\n∑\nj=0\nρ(j)cos(2π jk\nd ),\nk = 0,...,d −1.\nObserve that ∑d\nu=1 Si(u) = 0 for each i, that is, Si is orthogonal to the eigenvector of ˆρ(0). So the Lipschitz constant\np\n∥Σd∥op =\np\nmaxk | ˆρ(k)| can be slightly improved to be\np\nmaxk>0 | ˆρ(k)|.\nLet us now prove the claim of Theorem 3.2. Since the pairing has a probability larger than 1 −ε to be connected if\nPr[π∗/∈Π(0)\nd ] < ε, we need to show that under the inequality (24) the probability Pr[π∗/∈Π(0)\nd ] is less than ε. Let us\ndenote\nαu =\nr\n2\nπ ·2(1−ρ(u)), and ¯α2 =\nmin\n2≤u≤d/2αu,\n(B.2)\nand deﬁne\nCρ =\nc0\n∥Σd∥op\n\u00121\n2( ¯α2 −α1)\n\u00132\n.\n(B.3)\nThen Eqn. (24) can be rewritten as\nCρ\nN\nd > 3logd −logε.\n(B.4)\nAs a result of Proposition B.1 and Lemma B.1, if C =\nc0\n∥Σd∥op ·N/d then ∀π ∈Πd,\nPr[F(π)−EF(π) > δ] < exp{−Cδ 2} and Pr[F(π)−EF(π) < −δ] < exp{−Cδ 2},\n∀δ > 0.\n20\nObserve that\nΠd =\nd/2\n[\nm=0\nΠ(m)\nd\nwhere Π(m)\nd\nare the set of pairings which have m non-neighbor pairs. Π(0)\nd\nis the set of pairings which only pair connected\nnodes in the graph, and for the ring graph Π(0)\nd\n= {π(0)\n0 ,π(0)\n1 } the two of which interlace. For any π ∈Π(m)\nd , suppose that\nthere are ml pairs in π so that the distance between the two paired nodes is l, m1 = d/2−m, m2 +···+md/2 = m.\nRecalling the deﬁnition of αk in Eq. (B.2), we verify that\nEF(π) = α1(d\n2 −m)+α2m2 +···+αd/2md/2 ≥α1(d\n2 −m)+ ¯α2m\nwhen m ≥1, and\nEF(π(0)\n0 ) = EF(π(0)\n1 ) = α1\nd\n2.\nThus when m ≥1,\nEF(π)−EF(π(0)\n0 ) ≥( ¯α2 −α1)m,\n∀π ∈Π(m)\nd .\nDeﬁne\nδm = 1\n2( ¯α2 −α1)m,\nm = 1,...,d/2,\nand we have that\nPr[π∗/∈Π(0)\nd ]\n=\nPr[∃π ∈\nd/2\n[\nm=1\nΠ(m)\nd , F(π) < min{F(π(0)\n0 ),F(π(0)\n1 )}]\n≤\nPr[F(π(0)\n0 ) > EF(π(0)\n0 )+δ1]\n+Pr[F(π(0)\n0 ) < EF(π(0)\n0 )+δ1, ∃π ∈\nd/2\n[\nm=1\nΠ(m)\nd , F(π) < F(π(0)\n0 )]\n≤\nPr[F(π(0)\n0 ) > EF(π(0)\n0 )+δ1]\n+Pr[∃π ∈\nd/2\n[\nm=1\nΠ(m)\nd , F(π) < EF(π)−δm], (by that ( ¯α2 −α1)m−δ1 ≥δm)\n≤\nexp{−Cδ 2\n1 }+\nd/2\n∑\nm=1\n|Π(m)\nd |exp{−Cδ 2\nm}\n=\nexp{−Cρ\nN\nd }+\nd/2\n∑\nm=1\n|Π(m)\nd |exp{−Cρ\nN\nd m2},\n(B.5)\nwhere Cρ is as in Eq. (B.3).\nOne can verify the following upper bound for the cardinal number of Π(m)\nd :\n|Π(m)\nd | ≤d2m\n(2m)!.\nWith the crude bound (2m)! ≥1, the above inequality inserted in (B.5) gives\nPr[π∗/∈Π(0)\nd ] ≤exp{−Cρ\nN\nd }+\nd/2\n∑\nm=1\nd2m exp{−Cρ\nN\nd m2}.\n(B.6)\nIf we keep the factor (2m)!, the upper bound for the summation over m can be improved to be\nd2 exp{−CρN/d}\nd/2\n∑\nm=1\n((2m)!)−1 ≤c·d2 exp{−CρN/d}\nwhere c = (e −1)/2 is an absolute constant. By applying this in the ﬁnal bound in the theorem, the constant in front of\n21\nlogd is 2 instead of 3. The constant of the theorem is not tight, while the O(d logd) is believed to be the tight order as d\nincreases.\nTo proceed, deﬁne the function\ng(x) = −Cρ\nN\nd ·x2 +(2logd)·x,\n1 ≤x ≤d\n2,\nand observe that max1≤x≤d/2 g(x) = g(1) whenever\nlogd\nCρN/d < 1,\nwhich holds as long as Eq. (B.4) is satisﬁed. Thus we have\nd/2\n∑\nm=1\nd2m exp{−Cρ\nN\nd m2} ≤\nd/2\n∑\nm=1\nd2 exp{−Cρ\nN\nd } = d3\n2 exp{−Cρ\nN\nd },\nthen the inequality (B.6) becomes\nPr[π∗/∈Π(0)\nd ] ≤\n\u0012d3\n2 +1\n\u0013\nexp{−Cρ\nN\nd } ≤exp{−Cρ\nN\nd +3logd}.\nTo have Pr[π∗/∈Π(0)\nd ] < ε, a sufﬁcient condition is therefore\nexp{−Cρ\nN\nd +3logd} < ε,\n(B.7)\nwhich is reduced to Eq. (B.4) and equivalently Eq. (24).\n22\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2015-09-30",
  "updated": "2015-09-30"
}