{
  "id": "http://arxiv.org/abs/1510.01717v1",
  "title": "Language Segmentation",
  "authors": [
    "David Alfter"
  ],
  "abstract": "Language segmentation consists in finding the boundaries where one language\nends and another language begins in a text written in more than one language.\nThis is important for all natural language processing tasks. The problem can be\nsolved by training language models on language data. However, in the case of\nlow- or no-resource languages, this is problematic. I therefore investigate\nwhether unsupervised methods perform better than supervised methods when it is\ndifficult or impossible to train supervised approaches. A special focus is\ngiven to difficult texts, i.e. texts that are rather short (one sentence),\ncontaining abbreviations, low-resource languages and non-standard language. I\ncompare three approaches: supervised n-gram language models, unsupervised\nclustering and weakly supervised n-gram language model induction. I devised the\nweakly supervised approach in order to deal with difficult text specifically.\nIn order to test the approach, I compiled a small corpus of different text\ntypes, ranging from one-sentence texts to texts of about 300 words. The weakly\nsupervised language model induction approach works well on short and difficult\ntexts, outperforming the clustering algorithm and reaching scores in the\nvicinity of the supervised approach. The results look promising, but there is\nroom for improvement and a more thorough investigation should be undertaken.",
  "text": "UT\nFII\nCDH\nLanguage Segmentation\nAuthor:\nDavid A\nSupervisors:\nProf. Dr. Caroline S\nDr. Sven N\nAugust 18, 2015\nErklärung zur Masterarbeit\nHiermit erkläre ich, dass ich die Masterarbeit selbstständig verfasst und keine ande-\nren als die angegebenen ellen und Hilfsmiel benutzt und die aus fremden ellen\ndirekt oder indirekt übernommenen Gedanken als solche kenntlich gemacht habe.\nDie Arbeit habe ich bisher keinem anderen Prüfungsamt in gleicher oder vergleich-\nbarer Form vorgelegt. Sie wurde bisher nicht veröﬀentlicht.\nDatum\nUnterschri\ni\nAbstract\nLanguage segmentation consists in ﬁnding the boundaries where one lan-\nguage ends and another language begins in a text wrien in more than one lan-\nguage. is is important for all natural language processing tasks.\ne problem can be solved by training language models on language data.\nHowever, in the case of low- or no-resource languages, this is problematic. I\ntherefore investigate whether unsupervised methods perform beer than super-\nvised methods when it is diﬃcult or impossible to train supervised approaches.\nA special focus is given to diﬃcult texts, i.e. texts that are rather short (one\nsentence), containing abbreviations, low-resource languages and non-standard\nlanguage.\nI compare three approaches: supervised n-gram language models, unsuper-\nvised clustering and weakly supervised n-gram language model induction. I de-\nvised the weakly supervised approach in order to deal with diﬃcult text specif-\nically. In order to test the approach, I compiled a small corpus of diﬀerent text\ntypes, ranging from one-sentence texts to texts of about 300 words.\ne weakly supervised language model induction approach works well on\nshort and diﬃcult texts, outperforming the clustering algorithm and reaching\nscores in the vicinity of the supervised approach. e results look promising,\nbut there is room for improvement and a more thorough investigation should be\nundertaken.\nii\nAnowledgements\nMy thanks go to professor Caroline Sporleder for sharing her knowledge with me, for\nher inspiring ideas and for agreeing to supervise my Bachelor’s and Master’s esis\ndespite her busy schedule. It was also thanks to the topic she suggested for my Bach-\nelor’s esis that I met Jürgen Knauth and later was able to get a research assistant\nposition at the SeNeReKo project, collaborating closely with Jürgen.\nWhich brings me to the next person on the list. I would like to thank Jürgen Knauth\nfor the wonderful collaboration, for his patience, for his contagious enthusiasm, and\nall the interesting conversations in passing that always lasted longer than intended.\nI would like to thank Stephan Faber for his insightful comments when I couldn’t\nsee the wood for the trees, for his patience and optimism, for pushing me to go further\nand to persevere.\nI would also like to thank Julian Vaudroz for accompanying me throughout the\ndegree program. We both didn’t know what we were in for when we started, but we\npersevered and it paid oﬀ. It wouldn’t have been the same without you.\nFinally, I would like to thank all the people that volunteered to proofread my thesis\nand all the people that helped me during the writing of this thesis. Unfortunately, I\ncannot list everyone. You know who you are!\niii\nList of Figures\n1\nOut-of-place metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2\nSimple text illustration . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3\nInitial model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4\nInitial model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n5\nModel update\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n6\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n7\nNew model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n8\nMultiple model evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . .\n14\n9\nUpdating relevant model . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n10\nMultiple model evaluation 2\n. . . . . . . . . . . . . . . . . . . . . . . .\n14\n11\nNew model creation 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n12\nProblematic text sample . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n13\nFinding the most similar models . . . . . . . . . . . . . . . . . . . . . .\n16\n14\nMerging most similar models . . . . . . . . . . . . . . . . . . . . . . . .\n16\n15\nWord-Model assignment\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n16\nClustering preprocessor . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n17\nWEKA: Cluster visualization . . . . . . . . . . . . . . . . . . . . . . . .\n28\n18\nELKI: Cluster visualization . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n19\nLanguage model: Distribution 1\n. . . . . . . . . . . . . . . . . . . . . .\n35\n20\nLanguage Model: Distribution 2 . . . . . . . . . . . . . . . . . . . . . .\n35\n21\nLanguage model: Distribution 3\n. . . . . . . . . . . . . . . . . . . . . .\n36\n22\nAlternating language structure . . . . . . . . . . . . . . . . . . . . . . .\n54\niv\nList of Tables\n1\nTraining data: Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2\nUnambiguous encoding: distances . . . . . . . . . . . . . . . . . . . . .\n27\n3\nSimpliﬁed encoding: distances . . . . . . . . . . . . . . . . . . . . . . .\n27\n4\nN-Gram language model results: Latin script . . . . . . . . . . . . . . .\n38\n5\nN-Gram language model results: Mixed script\n. . . . . . . . . . . . . .\n39\n6\nN-Gram language model results: Pali data . . . . . . . . . . . . . . . . .\n40\n7\nN-Gram language model results: Twier data . . . . . . . . . . . . . . .\n41\n8\nTextcat results: Latin script . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n9\nTextcat results: Mixed script . . . . . . . . . . . . . . . . . . . . . . . .\n43\n10\nTextcat results: Pali data\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n11\nTextcat results: Twier data\n. . . . . . . . . . . . . . . . . . . . . . . .\n45\n12\nClustering results: Latin script . . . . . . . . . . . . . . . . . . . . . . .\n46\n13\nClustering results: Mixed script\n. . . . . . . . . . . . . . . . . . . . . .\n47\n14\nClustering results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n15\nClustering results: Twier data . . . . . . . . . . . . . . . . . . . . . . .\n49\n16\nInduction results: Latin script . . . . . . . . . . . . . . . . . . . . . . . .\n50\n17\nInduction results: Mixed script . . . . . . . . . . . . . . . . . . . . . . .\n51\n18\nInduction results: Pali data . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n19\nInduction results: Twier data . . . . . . . . . . . . . . . . . . . . . . .\n53\n20\n‘Twier 3’: Textcat versus Gold clustering\n. . . . . . . . . . . . . . . .\n58\n21\n‘Twier 4’: Textcat versus Gold clustering\n. . . . . . . . . . . . . . . .\n58\nv\nList of Algorithms\n1\nN-gram numerical encoding . . . . . . . . . . . . . . . . . . . . . . . .\n26\n2\nModel induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3\nInitial model creation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n4\nMax model and max score\n. . . . . . . . . . . . . . . . . . . . . . . . .\n32\n5\nModel merger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n6\nDistributional Similarity Calculation . . . . . . . . . . . . . . . . . . . .\n36\nvi\nContents\n1\nIntroduction\n1\n2\nRelated work\n2\n2.1\nN-Grams and rank order statistics . . . . . . . . . . . . . . . . . . . . .\n2\n2.2\nN-Grams and maximum likelihood estimator . . . . . . . . . . . . . . .\n3\n2.3\nTrigrams and short words\n. . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.4\nN-Grams and clustering . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.5\nInclusion detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.6\nClustering and speech . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.7\nMonolingual training data\n. . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.8\nPredictive suﬃx trees . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3\neory\n9\n3.1\nSupervised language model . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.1.1\nN-Gram models . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.1.2\nFormal deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.1.3\nSmoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nUnsupervised clustering\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.3\nWeakly supervised language model induction\n. . . . . . . . . . . . . .\n12\n4\nExperimental setup\n18\n4.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.2\nSupervised language model . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.2.1\nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.2.2\nTraining phase . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.2.3\nApplication of the approach . . . . . . . . . . . . . . . . . . . .\n21\n4.2.4\nTextcat and language segmentation . . . . . . . . . . . . . . . .\n21\n4.3\nUnsupervised clustering\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.3.1\nPreprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.3.2\nDeﬁning features . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.3.3\nMapping features to a common scale . . . . . . . . . . . . . . .\n25\n4.3.4\ne problem of unambiguous encoding . . . . . . . . . . . . . .\n26\n4.3.5\ne clusterer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3.6\nEvaluating clusterings . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.4\nWeakly supervised language model induction\n. . . . . . . . . . . . . .\n31\n4.4.1\nDistributional similarity . . . . . . . . . . . . . . . . . . . . . .\n34\n4.4.2\nEvaluating results . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n4.4.3\nEstimating the parameters . . . . . . . . . . . . . . . . . . . . .\n37\nvii\n5\nResults\n38\n5.1\nN-Gram language model . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.2\nTextcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.3\nClustering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n5.4\nLanguage model induction . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n6\nDiscussion\n54\n6.1\nN-Gram language models . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n6.2\nTextcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n6.3\nClustering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n6.4\nLanguage model induction . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n6.5\nScores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n7\nConclusion\n65\n8\nAppendix\n72\n8.1\nDevelopment data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n8.1.1\nLatin script data . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n8.1.2\nMixed script data . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n8.1.3\nTwier data . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n8.1.4\nPali dictionary data . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n8.2\nTest data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n8.2.1\nLatin script data . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n8.2.2\nMixed script data . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n8.2.3\nTwier data . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n8.2.4\nPali dictionary data . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n8.3\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n8.3.1\nN-Gram Language Models . . . . . . . . . . . . . . . . . . . . .\n80\n8.3.2\nTextcat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n8.3.3\nClustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n8.3.4\nLanguage Model Induction . . . . . . . . . . . . . . . . . . . . . 112\nviii\n1\nIntroduction\nLanguage segmentation and identiﬁcation are important for all natural language pro-\ncessing operations that are language-speciﬁc, such as taggers, parsers or machine\ntranslation (Jain and Bhat, 2014; Zubiaga et al., 2014). Indeed, using “traditional” mono-\nlingual natural language processing components on mixed language data leads to mis-\nerable results (Jain and Bhat, 2014). Even if the results are not terrible, language identi-\nﬁcation and segmentation can improve the overall results. For example, by identifying\nforeign language inclusions in an otherwise monolingual text, parser accuracy can be\nincreased (Alex et al., 2007).\nOne important point that has to be borne in mind is the diﬀerence between lan-\nguage identiﬁcation and language segmentation. Language identiﬁcation is concerned\nwith recognizing the language at hand. It is possible to use language identiﬁcation\nfor language segmentation. Indeed, by identifying the languages in a text, the seg-\nmentation is implicitly obtained. Language segmentation on the other hand is only\nconcerned with identifying language boundaries. No claims about the languages in-\nvolved are made.\nAer giving an overview over related work and diﬀerent approaches that can be\ntaken for language segmentation, I will present the theory behind supervised methods\nas well as unsupervised methods. Finally, I will introduce a weakly supervised method\nfor language segmentation that I developed.\nAer the theoretical part, I will present experiments done with the diﬀerent ap-\nproaches, comparing their eﬀectiveness on the task of language segmentation on dif-\nferent text types. A special focus will be given to diﬃcult text types, such as short texts,\ntexts containing under-resourced languages or texts containing a lot of abbreviations\nor other non-standard features.\nA big advantage of unsupervised methods is language independence. If the ap-\nproach used does not rely on language-speciﬁc details, the approach is more ﬂexible\nas no language resources have to be adapted for the method to work on other lan-\nguages. ese advantages might be especially useful for under-resourced languages.\nWhen there is no or insuﬃcient data available to train a supervised language model,\nan unsupervised approach might yield beer results.\nAnother advantage is that unsupervised methods do not require prior training.\ney are not dependent on training data and thus cannot be skewed by the data. In-\ndeed, supervised approaches that are trained on data are qualitatively tied to their\ntraining data; diﬀerent training data will, in all probability, yield diﬀerent models.\nis thesis aims at answering the question whether unsupervised language seg-\nmentation approaches work beer on diﬃcult text types than supervised language\napproaches.\n1\n2\nRelated work\n2.1\nN-Grams and rank order statistics\nCavnar and Trenkle (1994) use an n-gram language model for language identiﬁcation\npurposes. eir program ‘Textcat’ is intended to classify documents by language. e\nsystem calculates n-grams for 1 ⩽n ⩽5 from training data and orders the n-grams\naccording to inverse frequency, i.e. from the most frequent n-grams to the most infre-\nquent n-grams. e numerical frequency data is then discarded and only inherently\npresent.\nDuring training, the program calculates an n-gram proﬁle consisting of these n-\ngram lists for each category (i.e. language to classify).\nNew data is classiﬁed by ﬁrst calculating the n-gram proﬁle and then comparing\nthe proﬁle to existing proﬁles. e category with the lowest diﬀerence score is taken\nas the category for the document.\ne score they use for classiﬁcation is called out-of-place metric. For each n-gram\nin the document n-gram proﬁle, the corresponding n-gram in the category proﬁle is\nlooked up and the absolute diﬀerence of ranks is taken as score. e sum is calculated\nover all n-grams. More formally, the out-of-place metric moop is calculated as:\nmoop =\nn\n∑\ni=1\n(|r(xi, d) −r(xi, c)|)\n(1)\nWith n the number of n-grams in the document proﬁle, xi the i-th n-gram, r(xi, d)\nthe rank of the i-th n-gram in the document proﬁle, r(xi, c) the rank of the i-th n-gram\nin the category proﬁle.\nFigure 1 illustrates the out-of-place metric.\nCategory proﬁle\nDocument proﬁle\nLeast frequent\nMost frequent\nScore\nAT\nER\nno match →max\nING\nING\n0\nTH\nAT\n2\nWH\nTH\n1\nFigure 1: Out-of-place metric\n2\nIn ﬁgure 1, the document proﬁle has ‘ER’ as most frequent n-gram, at rank 1, fol-\nlowed by ‘ING’ at rank 2, etc. e category proﬁle does not contain the n-gram ‘ER’; in\nthat case, an arbitrary ﬁxed maximum value is assigned. e category proﬁle contains\nthe n-gram ‘ING’ at rank 2, the same rank as in the document proﬁle; the diﬀerence is\n0. e category proﬁle contains the n-gram ‘AT’ at rank 1, while in the document pro-\nﬁle, it occurs at rank 3. e absolute diﬀerence is 2. e out-of-place metric consists\nof the sum of all scores thus calculated.\nCavnar and Trenkle (1994) collected 3713 Usenet texts with a cultural theme in\ndiﬀerent languages. ey ﬁltered out non-monolingual texts and texts that had no\nuseful content for language classiﬁcation. In the end, they had 3478 articles ranging\nfrom a single line of text to 50 KB of text.\neir results indicated that length had no signiﬁcant impact on the classiﬁcation,\ncontrary to what they thought. Also, they found that training the system with 400\nn-grams yielded the best result with a precision of 99.8%.\ney also showed that their approach could be used for subject classiﬁcation of\ntexts in the same language with reasonable precision. is ﬁnding indicates that lan-\nguage and domain are linked to a certain degree.\n2.2\nN-Grams and maximum likelihood estimator\nDunning (1994) also uses an n-gram language model for language identiﬁcation pur-\nposes. e program calculates n-grams and their frequencies from the training data\nand estimates the probability P of a given string using the Maximum Likelihood Esti-\nmator (MLE) with Laplace add-one smoothing.More formally:\nP(wi|w1, . . . , wi−1) =\nC(w1, . . . , wi) + 1\nC(w1, . . . , wi−1) + |V |\n(2)\nwith C(w1, . . . , Ci) the number of times the n-gram w1, . . . , wi occurred,\nC(w1, . . . , Ci−1) the number of times the (n −1)-gram w1, . . . , wi−1 occurred and |V |\nthe size of the vocabulary.\nFor a string S, the string is decomposed into n-grams and the log probability lk is\ncalculated as:\nlk =\n∑\nw1,...,wk∈S\nC(w1, . . . , wk) log P(wk|w1, . . . , wk−1)\n(3)\nwhere k is the order of the n-gram (k = n) used.\nIn order to test the system, Dunning (1994) uses a specially constructed test cor-\npus from a bilingual parallel translated English-Spanish corpus containing English and\nSpanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and\n100 texts varying from 10 to 500 bytes for the test set.\n3\ne results indicate that bigram models perform beer for shorter strings and less\ntraining data while trigram models work beer for larger strings and more training\ndata.\nDunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system\nwould be insensitive to the length of the string to be classiﬁed, as the shortest text they\nclassiﬁed was about 50 words. e system implemented by Dunning (1994) can classify\nstrings of 10 characters in length “moderately well”, while strings of 50 characters or\nmore are classiﬁed “very well”. Accuracies given vary from 92% for 20 bytes of training\ndata to 99.9% for 500 bytes of text.\n2.3\nTrigrams and short words\nGrefenstee (1995) compares trigrams versus short words for language identiﬁcation.\nShort words are oen function words that are typical for and highly frequent in a given\nlanguage.\ne trigram language guesser was trained on one million characters of text in 10\nlanguages: Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese,\nSpanish and Swedish. From the same texts, all words with 5 or less characters were\ncounted for the short-word-strategy.\ne results indicate that the trigram approach works beer for small text fragments\nof up to 15 words, while for any text longer than 15 words, both methods work equally\nwell with reported accuracies of up to 100% in the 11-15 word range.\n2.4\nN-Grams and clustering\nGao et al. (2001) present a system that augments n-gram language models with clus-\ntering techniques. ey cluster words by similarity and use these clusters in order to\novercome the data sparsity problem.\nIn traditional cluster-based n-gram models, the probability P(wi) is deﬁned as the\nproduct of the probability of a word given a cluster ci and the probability of the cluster\nci given the preceding clusters. For a trigram model, the probability P(wi) of a word\nwi is calculated as\nP(wi|wi−2wi−1) = P(wi|ci) × P(ci|ci−2ci−1)\n(4)\ne probability of a word given a cluster is calculated as\nP(wi|ci) = C(wi)\nC(ci)\n(5)\nwith C(wi) the count of the word wi and C(ci) the count of the cluster ci.\n4\ne probability of a cluster given the preceding clusters is calculated using the\nMaximum Likelihood Estimator\nP(ci|ci−2ci−1) = C(ci−2ci−1ci)\nC(ci−2ci−1)\n(6)\nGao et al. (2001) derive from this three ways of using clusters to augment language\nmodels: predictive clustering (7), conditional clustering (8) and combined clustering\n(9).\nP(wi|wi−2wi−1) = P(ci|wi−2wi−1) × P(wi|wi−2wi−1ci)\n(7)\nP(wi|wi−2wi−1) = P(wi|ci−2ci−1)\n(8)\nP(wi|wi−2wi−1) = P(ci|ci−2ci−1) × P(wi|ci−2ci−1ci)\n(9)\nSimilarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in\norder to improve trigram language models. In addition to Gao et al. (2001), they also\nuse information about the subject-verb and verb-object relations of the sentence.\ney show that their model, using clustering, subject-verb information, verb-object\ninformation, and the Porter stemmer outperforms a traditional trigram model.\nCarter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar\nsentences and calculates separate language model parameters for each subcorpus in\norder to capture contextual information. In contrast to other works, Carter (1994)\nclusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al.\n(1994)). Carter (1994) shows that the subdivision into smaller clusters increases the\naccuracy of bigram language models, but not trigram models.\n2.5\nInclusion detection\nBeatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010))\naddresses the problem of English inclusions in mainly non-English texts. For the lan-\nguage pair German-English, inclusions are detected using a German and an English\nlexicon as ﬁrst resource. If a word is found only in the English lexicon, it is tagged\nas unambiguously English. If the word is found in neither lexicon, a web search is\nconducted, restricting the search options to either German or English and counting\nthe number of results. If the German search yields more results, the word is tagged as\nGerman, otherwise as English inclusion. If a word is found in both lexicons, a post-\nprocessing module resolves the ambiguity.\nAlex is mainly concerned with the improvement of parsing results by inclusion\ndetection. For example in (Alex et al., 2007) they report an increase in F-Score of 4.3\n5\nby using inclusion detection when parsing a German text with a parser trained on the\nTIGER corpus (Brants et al., 2002).\n2.6\nClustering and spee\nIn the area of clustering and spoken language identiﬁcation, Yin et al. (2007) present a\nhierarchical clusterer for spoken language. ey cluster 10 languages1 using prosodic\nfeatures and Mel Frequency Cepstral Coeﬃcients (MFCC). MFCC vectors are a way of\nrepresenting acoustic signals (Logan et al., 2000). e signal is ﬁrst divided into smaller\n‘frames’, each frame is passed through the discrete Fourier transform and only the log-\narithm of the amplitude spectrum is retained (Logan et al., 2000). e spectrum is then\nprojected onto the ‘Mel frequency scale’, a scale that maps actual pitch to perceived\npitch, “as apparently the human auditory system does not perceive pitch in a linear\nmanner” (Logan et al., 2000). Finally, a discrete cosine transform is applied to the\nspectrum to get the MFCC representations of the original signal (Logan et al., 2000).\nYin et al. (2007) show that their hierarchical clusterer outperforms traditionalAcous-\ntic Gaussian Mixture Model systems.\nAs spoken language will not be further investigated in this thesis, I will not dive\ndeeper into the maer at this point.\n2.7\nMonolingual training data\nYamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use\nmonolingual training data in order to train a system capable of recognizing the lan-\nguages in a multilingual text.\nYamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to seg-\nment a text by language. eir test data contains fragments of 40 to 160 characters and\nachieves F-scores of 0.94 on the relatively ‘closed’ data set of the Universal Declara-\ntion of Human Rights2 and 0.84 on the more ‘open’ Wikipedia data set. However, the\napproach is computationally intensive, not to say prohibitive; while Yamaguchi and\nTanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 char-\nacters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and\nTanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core\nworkstation.\nKing and Abney (2013) use weakly supervised methods to label the languages of\nwords. ey consider the task as sequence labeling task. ey have limited them-\nselves to bilingual documents with a single language boundary and the task consists\n1e authors do not explicitly list the languages clustered, except for two-leer abbreviations which\nseem to correspond to ISO 639-1. e languages under investigation could have been Vietnamese, Ger-\nman, Farsi, French, Japanese, Spanish, Korean, English, Tamil, and ‘ma’, though it is impossible to tell.\n2http://www.un.org/en/documents/udhr/\n6\nin discriminating between English and non-English text. ey found that a Condi-\ntional Random Field model augmented with Generalized Expectation criteria worked\nbest, yielding accuracies of 88% with as lile as 10 words used for training.\nLui et al. (2014) consider the task as multi-label classiﬁcation task. ey represent\na document as an n-gram distribution of byte sequences in a bag-of-words manner.\ney report F-scores of 0.957 and 0.959. ey note that similar languages will pose\nproblems when trying to identify a language, and solve this problem by identifying a\nset of languages that most probably are correct instead of a single language.\nOne problem that these approaches all have is that they need to know the languages\nthat will occur in the test data (King and Abney, 2013; Lui et al., 2014).\n2.8\nPredictive suﬃx trees\nSeldin et al. (2001) propose a system for automatic unsupervised language segmenta-\ntion and protein sequence segmentation. eir system uses Variable Memory Markov\n(VMM) sources, an alternative to Hidden Markov Models (HMM) implemented as Pre-\ndictive Suﬃx Trees (PST).\nWhereas HMMs require substantial amounts of training data and a deep under-\nstanding of the problem in order to restrict the model architecture, VMMs are simpler\nand less expressive than HMMs, but have been shown to “solve many applications\nwith notable success” (Begleiter et al., 2004). In contrast to n-gram models that es-\ntimate the probability of w as P(w|N) with N the context (typically the n previous\nwords), VMMs can vary N in function of the available context (Begleiter et al., 2004).\nus, they can capture both small and large order dependencies, depending on the\ntraining data (Begleiter et al., 2004).\nere is no single VMM algorithm, but rather a family of related algorithms. One\nof these algorithms is called Predictive Suﬃx Tree (PST) (Ron et al., 1996). A PST is\na tree over an alphabet Σ, with each node either having 0 (leaf nodes) or |Σ| children\n(non-terminal nodes) (Ron et al., 1996). Each node is labeled with the result of the walk\nfrom that node up to the root (Ron et al., 1996). Each edge is labeled by a symbol s ∈Σ\nand the probability for the next symbol being s (Ron et al., 1996).\nBy modifying the Predictive Suﬃx Tree (PST) algorithm using the Minimum De-\nscription Length (MDL) principle, Seldin et al. (2001) end up with a non-parametric\nself-regulating algorithm. e MDL principle avoids overﬁing of the model by favor-\ning low complexity over goodness-of-ﬁt (Grünwald, 2007).\ney embed the algorithm in a deterministic annealing (DA) procedure to reﬁne\nthe results. Finally, they use the Blahut-Arimoto algorithm, a rate-distortion function,\nuntil convergence of the system.\nFor the language segmentation task, they use 150000 leers of text, 30000 from\neach of the following languages: English, German, French, Italian, transliterated Rus-\nsian. ey used continuous language fragments of approximately 100 leers, yielding a\n7\nsynthetic multilingual text that switches language approximately every two sentences.\nOne important point that they note is that “too short segments do not enable reliable\ndiscrimination between diﬀerent models”. erefore, they disallow switching models\naer every word.\ney report very good results on the language segmentation task (and on the pro-\ntein segmentation task). Aer 2000-3000 iterations of the Blahut-Arimoto algorithm,\nthe correct number of languages is identiﬁed and the segmentation is accurate up to a\nfew leers.\n8\n3\neory\n3.1\nSupervised language model\n3.1.1\nN-Gram models\nAmong supervised language models, n-gram models are very popular (Gao et al., 2001).\nAn n-gram is a slice from the original string (Cavnar and Trenkle, 1994). ese slices\ncan be contiguous or not. Non-contiguous n-grams are also called skip-grams (Guthrie\net al., 2006). In skip-grams, an additional parameter k indicates the maximum distance\nthat is allowed between units. In this parlance, contiguous n-grams can be regarded\nas 0-skip-n-grams (Guthrie et al., 2006).\ne following example demonstrates the diﬀerence between (traditional) n-grams\nand skip-grams. Given the following sentence:\nThis\ni s\na sample\nsentence .\nWe can construct, for example, the following word k-skip-n-grams:\n(0-skip-)2-grams: is is, is a, a sample, sample sentence\n2-skip-2-grams: is is, is a, is sample, is a, is sample, is sentence, a sample, a\nsentence, sample sentence\n(0-skip-)3-grams:is is a, is a sample, a sample sentence\n2-skip-3-grams:is is a, is is sample, is is sentence, is a sample, is a sen-\ntence, is sample sentence, is a sample, is a sentence, is sample sentence, a sample\nsentence\ne results for 2-skip-2-grams does not include the skip-gram “is sentence”, as\nthe distance in words between these two words is 3, higher than the allowed k of 2. As\ncan be seen from this example, the number of skip-grams is more than two times higher\nthan the number of contiguous n-grams, and this trend continues the more skips are\nallowed (Guthrie et al., 2006). Skip-grams, unlike n-grams, do not incur the problem\nof data sparseness with an increase of n.\nInstead of using words as unit for n-gram decompositions, we can also choose char-\nacters. Each word is then decomposed into sequences of n characters. For example,\nthe word\nmodel\ncan be decomposed into the 2-grams: mo, de, el. Oen, the word to decompose\nis padded with start and end tags in order to improve the model (Cavnar and Tren-\nkle, 1994). If we pad the word with <w> and </w>, the 2-gram decomposition yields:\n<w>m, mo, de, el, l </w>. e use of paddings allows the model to capture details about\ncharacter distribution with regard to the start and end of words (Cavnar and Trenkle,\n1994). For example, in English the leer ‘y’ occurs more oen at the end of words than\n9\nat the beginning of words, while the leer ‘w’ occurs mainly at the beginning of words\n(Taylor, 2015). A non-padding model cannot capture this distinction, while a padding\nmodel can.\nOne advantage of n-gram models is that the decomposition of a string into smaller\nunits reduces the impact of typing errors (Cavnar and Trenkle, 1994). Indeed, a typ-\ning error only aﬀects a limited number of units (Cavnar and Trenkle, 1994). Due to\nthis property, n-gram models have been shown to be able to deal well with noisy text\n(Cavnar and Trenkle, 1994).\n3.1.2\nFormal deﬁnition\nTraditional n-gram language models predict the next word wi given the previous words\nw1, . . . , wi−1. is prediction uses the conditional probability P(wi|w1, . . . , wi−1). In-\nstead of using the entire history w1, . . . , wi−1, the probability is approximated by using\nonly the n previous words wi−n+1, . . . , wi−1.\nP(wi|w1, . . . , wi−1) = P(wi|wi−n+1, . . . , wi−1)\n(10)\ne probability can be estimated using the Maximum Likelihood Estimation (MLE):\nP(wi|wi−n+1, . . . , wi−1) =\nC(wi−n+1, . . . , wi)\nC(wi−n+1, . . . , wi−1)\n(11)\nWhere C(wi−n+1, . . . , wi) represents the number of times the n-gram sequence\nwi−n+1, . . . , wi occurred in the training corpus and C(wi−n+1, . . . , wi−1) represents the\nnumber of times the (n −1)-gram sequence wi−n+1, . . . , wi−1 was seen in the training\ncorpus.\n3.1.3\nSmoothing\ne problem with MLE is that sequences not seen during training will have a prob-\nability of zero. In order to avoid this problem, diﬀerent smoothing techniques can\nbe used (Chen and Goodman, 1996). e simplest smoothing technique is additive\n(Laplace) smoothing (Chen and Goodman, 1996). Let V be the vocabulary size (i.e. the\ntotal number of unique words in the test corpus). e smoothed probability PLaplace\nbecomes:\nPLaplace(wi|wi−n+1, . . . , wi−1) =\nC(wi−n+1, . . . , wi) + λ\nC(wi−n+1, . . . , wi−1) + λV\n(12)\nWith λ the smoothing factor. If we choose λ = 1, we speak of “add one” smoothing\n(Jurafsky and Martin, 2000). In practice, λ < 1 is oen chosen (Manning and Schütze,\n1999).\n10\nAn important estimation is the Good-Turing estimation (Chen and Goodman, 1996).\nWhile not directly a smoothing method, it estimates the frequency of a given observa-\ntion with\nc∗= (c + 1)Nc+1\nNc\n(13)\nwhere c is the number of times the observation was made, Nc is the number of times\nthe frequency c was observed and Nc+1 the frequency of the frequency c + 1. us,\ninstead of using the actual count c, the count is taken to be c∗(Chen and Goodman,\n1996).\nAnother way to avoid assigning probabilities of zero to unseen sequences is by\nusing back-oﬀmodels. ere are linear and non-linear back-oﬀmodels. In non-linear\nback-oﬀmodels, if the original n-gram probability falls below a certain threshold value,\nthe probability is estimated by the next lowest n-gram model. Katz’s back-oﬀmodel\n(Katz, 1987) for instance calculates probability Pbo using the formula:\nPbo =\n{\ndwi−n+1,...,wi\nC(wi−n+1,...,wi)\nC(wi−n+1,...,wi−1)\nif C(wi−n+1, . . . , wi) > k\nαwi−n+1,...,wi−1Pbo(wi|wi−n+2, . . . , wi−1)\notherwise\n(14)\nWith d and α as smoothing parameters. e parameter k is oen chosen k = 0.\nis means that if the probability given a high-order n-gram model is zero, we back\noﬀto the next lowest model. For tri-gram models, the formula becomes:\nPbo(wi|wi−2, wi−1) =\n\n\n\n\n\nP(wi|wi−2, wi−1)\nif C(wi−2, wi−1) > 0\nα1P(wi|wi−1)\nif C(wi−2, wi−1) = 0 and C(wi−1, wi) > 0\nα2P(wi)\notherwise\n(15)\nIn contrast, linear back-oﬀmodels use an interpolated probability estimate by com-\nbining multiple probability estimates and weighting each estimate. e probability PLI\nfor a tri-gram model is:\nPLI(wi|wi−2, wi−1) = λ3P(wi|wi−2, wi−1) + λ2P(wi|wi−1) + λ1P(wi)\n(16)\nwith ∑λi = 1\n3.2\nUnsupervised clustering\nClustering consists in the grouping of objects based on their mutual similarity (Bie-\nmann, 2006). Objects to be clustered are typically represented as feature vectors (Bie-\nmann, 2006); from the original objects, a feature representation is calculated and used\nfor further processing.\n11\nClustering can be partitional or hierarchical (Yin et al., 2007). Partitional clustering\ndivides the initial objects into separate groups in one step, whereas hierarchical clus-\ntering builds a hierarchy of objects by ﬁrst grouping the most similar objects together\nand then clustering the next level hierarchy with regard to the existing clusters (Yin\net al., 2007).\ne clustering algorithm uses a distance metric to measure the distance between the\nfeature vectors of objects (Biemann, 2006). e distance metric deﬁnes the similarity\nof objects based on the feature space in which the objects are represented (Jain et al.,\n1999). ere are diﬀerent metrics available. A frequently chosen metric is the cosine\nsimilarity that calculates the distance between two vectors, i.e. the angle between them\n(Biemann, 2006).\nIn order for a clustering algorithm to work, features that represent the object to be\nclustered have to be deﬁned (Jain et al., 1999). Features can be quantitative (e.g. word\nlength) or qualitative (e.g. word starts with a capital leer) (Jain et al., 1999).\nMost clustering algorithms, e.g. k-means, need the number of clusters to generate\n(Jain et al., 1999). e question how to best choose this key number has been addressed\nin-depth by Dubes (1987).\nClustering can be soor hard. When hard-clustering, an object can belong to one\nclass only, while in so-clustering, an object can belong to one or more classes, some-\ntimes with diﬀerent probabilities (Jain et al., 1999).\n3.3\nWeakly supervised language model induction\ne main idea behind language model induction is that by inducing language models\nfrom the text itself, the models are highly specialized but the approach is generally\nmore ﬂexible since genre or text speciﬁc issues do not arise.\nis approach is similar in character to the work by Seldin et al. (2001) in that the\ntext itself is used as data set. However, the realization diﬀers greatly. Whereas Seldin\net al. (2001) use predictive suﬃx trees, I use n-gram language models.\ne intuition is to learn the language models from the text itself, in an iterative\nmanner. Suppose we have a document as follows where wi represents the word at\nposition i in the text. Suppose the text contains two languages, marked in red and\nblue.\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nFigure 2: Simple text illustration\nWe take the ﬁrst word and create a language model m1 from that word.\n12\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\ncreate\nFigure 3: Initial model creation\nWe then evaluate the second word using the ﬁrst language model. If the language\nmodel score is high enough, we update the language model with the second word.\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nevaluate\nFigure 4: Initial model evaluation\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nupdate\nFigure 5: Model update\nIf the score is below a certain threshold, the existing language model does not model\nthe word well enough and a new model is created.\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nevaluate\nFigure 6: Evaluation\n13\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nm2\ncreate\nFigure 7: New model creation\nWhen there is more than one language model, each word is evaluated by every\nlanguage model, and the highest scoring model is updated, or a new model is created\nif no language model models the word well enough.\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nm2\nevaluate\nFigure 8: Multiple model evaluation\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nm2\nupdate\nFigure 9: Updating relevant model\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nm2\nevaluate\nFigure 10: Multiple model evaluation 2\n14\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nm2\nm3\ncreate\nFigure 11: New model creation 2\ne last example shows that it is not necessarily the case that exactly one language\nmodel is created per language; it oen is the case that many language models are\ncreated for one language.\nAt the beginning, the models are not very reliable, as they only have a few words\nas basis, but the more text is analyzed, the more reliable the models become.\nHowever, the approach is problematic in that the text structure itself inﬂuences\nthe language models created. If the text starts with a foreign language inclusion, as\nillustrated in ﬁgure 12, the initial model might be too frail to recognize the following\nwords as being a diﬀerent language, updating the ﬁrst model with the second and third\nword and so on. us, the approach would fail at recognizing the foreign language\ninclusion.\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nFigure 12: Problematic text sample\nIf we were to start from the end of the text and work towards the beginning, the\nprobability of having a relatively robust language model for the ‘blue’ language would\nbe high, and so, it would theoretically be easier to recognize the ﬁrst word as not being\n‘blue’.\nerefore, one induction step involves one forward generation and one backwards\ngeneration. is yields two sets, the set of models from the forward generation F =\n{f1, f2, . . . , fn} and the set from the backwards generation B = {b1, b2, . . . , bm}.\nen, from the two sets of models, the most similar models are selected. For this,\nevery model from F is compared to every model from B, as ﬁgure 13 shows. e most\nsimilar models are then merged, as illustrated in ﬁgure 14. Indeed, if both the forward\nand backwards generation yielded a similar language model, it is probable that the\nmodel is correct.\nEven so, both forward and backwards generation can not guarantee ideal results,\nthere is the option to run the generation from a random position. is random induc-\ntion picks a random position in the text and runs one induction step from that position,\nmeaning one forward and one backwards generation. Finally, the most similar models\nare merged as for the general generation.\n15\nf1\nf2\nf3\nf4\nb1\nb2\nb3\nb4\nb5\nb6\nFigure 13: Finding the most similar models\nf1\nf2\nf3\nf4\nb1\nb2\nb3\nb4\nb5\nb6\nMerged model\nFigure 14: Merging most similar models\nis only yields one probable language model, therefore the induction is repeated\nwith the diﬀerence that all probable models are taken into consideration as well. For\neach word, if a probable model models the word well enough, no new model is created,\notherwise a new model is created.\nAt the end of the induction loop, the set of probable models P is examined. As long\nas there are two models that have a similarity score below a certain threshold, the two\nmost similar models are merged.\nFinally, aer the language models have been induced, another pass is made over\nthe text and each word is assigned to the language model which yields the highest\nscore for that word, resulting in a word-to-model assignment as illustrated in ﬁgure\n15.\nw1\nw2\nw3\nw4\nw5\nw6\nw7\nw8\nw9\nw10\n…\nm1\nm2\nm3\nm4\nm5\nm6\nFigure 15: Word-Model assignment\nI have made the approach parametric with parameters being:\n• Induction iterations: Number of induction iterations\n• Random iterations: Number of random iterations\n16\n• Forward/Backwards threshold: reshold for forward/backwards merging\n• Silver threshold: reshold for P model merging\nese parameters can be adapted, in the hope that some parameter conﬁgurations\nwill work beer on certain data sets than other conﬁgurations. Since the approach\nhas parameters that have to be learned from a development set, the approach is said to\nbe weakly supervised; the development set is not used to train any language speciﬁcs,\nonly for the estimation of the parameters of the approach.\n17\n4\nExperimental setup\nIn this chapter I present experiments done using the approaches delineated in the pre-\nvious section in order to ﬁnd out whether there are approaches that work beer on\ncertain types of text.\ne central hypothesis is that unsupervised language segmentation approaches are\nmore successful on diﬃcult data. Diﬃcult data is data for which there is not enough\ndata to train a language model or data which contains a lot of non-standard language\nsuch as abbreviations.\nFirst, I present the data used to test the language segmentation systems and elabo-\nrate on the diﬀerent aspects that had to be considered for the data compilation.\nI then present two supervised language segmentation experiments using n-gram\nlanguage models and Textcat.\nFor unsupervised language segmentation, I will ﬁrst present experiments using\nclustering algorithms before presenting experiments using language model induction.\n4.1\nData\nIn order to test the diﬀerent language segmentation approaches, I compiled diﬀerent\nsets of test data. As I want to focus on short texts, most texts from the test corpus are\nrather small, sometimes consisting of only one sentence. However, in order to test the\ngeneral applicability of the approach, the test corpus also contains larger text samples.\ne test corpus can be subdivided into diﬀerent sub-corpora:\n• Latin-based: Texts consisting of languages using Latin-based scripts, such as\nGerman, English, Finnish or Italian\n• Mixed script: Texts consisting of languages using Latin-based scripts and lan-\nguages using non-Latin-based scripts\n• Twier data: Short texts taken from Twier\n• Pali dictionary data: Unstructured texts containing many diﬀerent language in-\nclusions such as Vedic Sanskrit, Sanskrit, Indogermanic reconstructions, Old Bul-\ngarian, Lithuanian, Greek, Latin, Old Irish, many abbreviations and references\nto text passages\nAs every outcome has to be manually checked, the test corpus is rather small. Every\ncategory consists of ﬁve texts. Each texts consists of two or three languages with the\nexception of the Pali dictionary data that oen contains inclusions from many diﬀerent\nlanguages in the etymological explanations.\nFor each text, I also created a gold standard version with the expected clusters. In\nsome cases it is not clear how to cluster certain objects. In that case, I use a clustering\n18\nthat makes sense to me, but this need not mean that it is the correct or only possible\nclustering.\nFor the parameter estimation of the language model induction approach, I also\ncompiled a set of development data. All texts can be found in the appendix under 8.1\nand 8.2.\n4.2\nSupervised language model\n4.2.1\nImplementation\nFor the supervised language segmentation method, I implemented an n-gram language\nmodel as described by Dunning (1994). e n-gram language model is implemented\nas a character trigram model with non-linear back-oﬀto bigram and unigram models.\ne conditional probability P is calculated using the formula:\nP(wi|wi−2, wi−1) =\n\n\n\n\n\n\n\n\n\n\n\nα1\nC(wi−2,wi−1,wi)\nC(wi−2,wi−1)\nif C(wi−2, wi−1, wi) > 0\nα2\nC(wi−1,wi)\nC(wi−1)\nif C(wi−1, wi) > 0\nα3\nC(wi)\nV\nif C(wi) > 0\nα4\n1\nV +W+X\notherwise\n(17)\nwith α1 = 0.7, α2 = 0.2, α3 = 0.09, α4 = 0.01, V the number of unigrams, W the\nnumber of bigrams and X the number of trigrams.\nEach word is padded by two diﬀerent start symbols and two diﬀerent end symbols.\ne joint probability for a word w of length n is calculated as\nP(w) =\n1\n∑n\ni=2 | log P(wi|wi−2, wi−1)|\n(18)\nIn the denominator, I use the log probability instead of the probability to increase\nnumerical stability. Indeed, multiplying very small numbers can lead to the result\nbeing approximated as zero by the computer when the numbers become too small to\nbe represented as normalized number (Goldberg, 1991). Using the sum of logarithms\navoids this problem and is less computationally expensive (Bürgisser et al., 1997).\nAs the logarithm of a number approaching zero tends to inﬁnity, rare observations\nget a higher score than frequent observations. As such, the denominator can be seen\nas a scale of rarity, with a higher score corresponding to a rarer word. By taking the\ninverse of this scale, we get a score corresponding to the “commonness” (≈frequency)\nof a word.\n4.2.2\nTraining phase\nFirst, models are trained on training data in the relevant languages. I have not included\nthe languages from the Pali dictionary data, as there are too many diﬀerent languages\n19\nLanguage\nSize in MB\nAmharic\n9\nArabic\n747\nChinese\n1005\nEnglish\n2097\nFinnish\n570\nFrench\n2097\nGerman\n2097\nGreek\n464\nItalian\n2097\nPolish\n2097\nRussian\n2097\nSpanish\n2097\nTurkish\n386\nUkrainian\n1456\nTable 1: Training data: Size\nand there are typically only small inclusions of diﬀerent languages in a dictionary en-\ntry; as such, it would not have made sense to train a language model just to recognize\na single word. Another reason for not using the Pali dictionary data languages is that\nsometimes it is not possible to ﬁnd data for a language, e.g. Old Bulgarian or recon-\nstructed Indogermanic. In some cases, it would have been conceivable to train models\non similar languages, but again, the eﬀort of training a model is disproportionately\nhigh compared to the (uncertain) result of recognizing a single inclusion. Instead, an\nadditional catch-all language model is used to capture words that do not seem to belong\nto a trained model.\ne training data consists of Wikipedia dumps from the months June and July 2015;\na dump is a copy of the whole encyclopedia for a given language. Due to the diﬀerence\nin size of the Wikipedia of the diﬀerent languages, I choose the full dump for languages\nwith less than 3 GB of compressed data and limited the amount of data to maximally\n3 GB of compressed data.\ne Wikipedia data was processed using the Wikipedia Extractor3 version 2.8 in\norder to extract the textual content from the article pages. Indeed, the Wikipedia pages\nare wrien using the MediaWiki Markup Language4. While this markup is useful for\nmeta-data annotation and cross-referencing, the encoded information is superﬂuous\nfor language model training and has to be removed before training a model on the\ndata. Table 1 shows the size of the training data per language aer text extraction.\n3http://medialab.di.unipi.it/wiki/Wikipedia_Extractor\n4https://www.mediawiki.org/wiki/Help:Formatting\n20\nAs the test data only contains transliterated Amharic text, the Wikipedia data, writ-\nten in the Ge’ez script, had to be transliterated. e text was transliterated according\nto the EAE transliteration scheme by the Encyclopaedia Aethiopica.\nAs the test data contains transliterated Greek, the Greek data was used once as-is\nand once transliterated according to the ELOT (Hellenic Organization for Standardiza-\ntion) transliteration scheme for Modern monotonic Greek.\nIt should be borne in mind that the training data inﬂuences the quality and accuracy\nof the model. Furthermore, a model might work well on certain text types and less well\non other text types. It is not possible to train a perfect, universal model.\n4.2.3\nApplication of the approa\nIn the second step, an input text is segmented into words. en, each word is evaluated\nby each language model and the model with the highest score is assigned as the word’s\nlanguage model.\ne approach taken consists in classifying words as either belonging to a trained\nlanguage model or to the additional, catch-all model other, which simply means that\nthe word could not be assigned to a trained model class.\n4.2.4\nTextcat and language segmentation\nI also tested how well Textcat is suited to the task of language segmentation. e\napproach is similar to the n-gram approach, with the exception that I do not train any\nmodels and rely on Textcat’s classiﬁer for language prediction.\nIn the ﬁrst step, an input text is segmented into words. en, each word is passed\nto Textcat and the guess made by Textcat is taken as the word’s language.\n4.3\nUnsupervised clustering\nIn order to test the eﬃciency of clustering algorithms on the task of language segmen-\ntation, I looked at various algorithms readily available through WEKA, “a collection of\nmachine learning algorithms for data mining tasks” by the University of Waikato in\nNew Zealand (Hall et al., 2009) and the Environment for Developing KDD-Applications\nSupported by Index-Structures (ELKI), “an open source data mining soware […] with\nan emphasis on unsupervised methods in cluster analysis and outlier detection” by\nthe Ludwig-Maximilians-Universität München (Achtert et al., 2013). I also looked at\nJavaML, “a collection of machine learning and data mining algorithms” (Abeel et al.,\n2009), in order to integrate clusterers into my own code framework. JavaML oﬀers dif-\nferent clustering algorithms and also oﬀers access to WEKA’s clustering algorithms. In\ncontrast to WEKA and ELKI, which can be used in stand-alone mode, JavaML is meant\n21\nto be integrated into bigger programs and provides an application programming inter-\nface (API) that allows the provided algorithms to be accessed in a programmatic way,\ni.e. from inside a program.\n4.3.1\nPreprocessing\nHowever, in order for the clustering algorithms to work, the document to segment has\nto be preprocessed in a number of ways, as shown in ﬁgure 16.\nFigure 16: Clustering preprocessor\nExtract features\nis empty?\nRemove tags\nNormalize\nHas token?\nTokenize input\nRead input\nYes\nNo\nNo\nYes\nFirst of all, the document has to be read in by the program. is step is straightfor-\nward.\n22\ne document then has to be tokenized. Tokenization is not trivial and depends on\nthe deﬁnition of a ‘word’. For this task I have used a whitespace tokenizer that deﬁnes\na word as a continuous sequence of character literals separated by one or more whites-\npace characters. While it can be objected that for scripts that don’t use whitespace to\nseparate words, such as Chinese, tokenization fails, this is not too big a concern. In-\ndeed, if a continuous block of Chinese characters is treated as one word, it is likely to\nbe clustered separately due to the diﬀerent in ”word” length and the diﬀerent charac-\nter set. If, however, a document contains two scripts that do not separate words by\nwhitespace, the approach totally fails. It is beyond the scope of this thesis, and possi-\nbly of any thesis, to implement a universal tokenizer that works regardless of language\nwithout prior knowledge about the languages at hand.\nEach token is then normalized. Normalization of a non-Latin-based input (e.g. Ara-\nbic or Cyrillic script) returns the input without modiﬁcation. Otherwise, the following\nmodiﬁcations are made, if applicable:\n• remove leading and trailing whitespace\n• remove punctuation\n• remove control characters\nControl characters are deﬁned as the set\n(\n[\n]\n)\n\\\nPunctuation is deﬁned as the set\n.\n,\n”\n’\n:\n;\n!\n? −\ne token is then stripped of XML-like tags, if applicable. e following example\nillustrates this step. Let us assume we have the following token:\n<word id =”1” lemma=” go”> goes </ word>\ne token is replaced by the text content of the node, thus the resulting token is ‘goes’.\nIf, aer all these modiﬁcations, the token corresponds to the empty string, we con-\ntinue with the next token. Otherwise, the token is passed on to the feature extraction\nmodule. e algorithm terminates when all tokens have been consumed.\n4.3.2\nDeﬁning features\ne ﬁnal step consists in deﬁning features by which to cluster and implementing fea-\nture extractors that build the feature vectors from the input. Since the features are to\nbe language independent, using features such as ‘occurs in an English lexicon’ cannot\nbe used. e following features were devised:\n23\n1. word length: the length of the word in characters\n2. X tail bigrams: bigrams calculated from the end of the word\n3. Y tail trigrams: trigrams calculated from the end of the word\n4. X ﬁrst bigrams: bigrams calculcated from the beginning of the word\n5. Y ﬁrst trigrams: trigrams calculated from the beginning of the word\n6. latin basic: is the word latin basic?\n7. latin extended: is the word latin extended?\n8. capitalized: is the word capitalized?\n9. contains non-word: does the word contain a non-word?\n10. is non-word: is the word a non-word?\n11. number of latin leers: number of latin leers\n12. number of non-latin leers: number of non-latin leers\n13. vowel ratio: number of vowels divided by the word length\n14. basic latin leer ratio: number of latin leers divided by the word length\n15. max consonant cluster: the longest consonant cluster size in characters\n16. is digit: is the word a digit?\n17. is ideographic: is the word ideographic?\n18. directionality: what directionality does ﬁrst character of the word have?\n19. is BMP codepoint: does the word contain non-BMP characters?\n20. general type: what is the general type of the ﬁrst character of the word?\ne last two features are based on the Java Character class. is class provides\nmethods to check for speciﬁc implementation-based properties of characters.\nWhile most features are rather self-explanatory, a few require further explanation.\nFor the n-grams, the number of n-grams is restricted so as to keep the resulting vec-\ntors the same size. is is important because the clustering algorithm considers one\ndata column as one feature, and having vectors of diﬀerent length would disrupt this\nprecondition. Implementing the comparison of vectors of diﬀerent lengths, or rather\n24\nor vectors containing vectors as features would have been possible, but rather time-\nconsuming. If a word is too short to generate the required number of n-grams, only\nthe possible n-grams are generated and all other positions ﬁlled with 0.\ne ‘latin’ features check whether the word consists only of the basic latin leers\nA-Z and a-z (‘basic’) while the ‘extended’ feature also covers leers derived from the\nlatin leers (e.g. ë, ç, ṃ, ñ).\nNon-words are deﬁned as anything not consisting of leers, such as punctuation\nmarks or digits.\nDirectionality indicates which direction a character should be wrien. While the\nactual list is much more exhaustive, this property basically indicates whether the char-\nacter is wrien from leto right or from right to le. 5\nBMP stands for Basic Multilingual Plane and refers to an encoding unit known as\nplane, which consists of 216 = 65536 codepoints (i.e. encoding slots for characters)\n(e Unicode Consortium, 2014). e BMP is the ﬁrst plane, covering the codepoints\nU+0000 to U+FFFF (e Unicode Consortium, 2014). While it is not important to un-\nderstand the technical details fully, it is interesting to note that most characters are\ncovered by the BMP, including Chinese, Japanese and Korean characters (e Unicode\nConsortium, 2014). e next plane, called Supplementary Multilingual Plane or Plane\n1 contains historic scripts such as Egyptian hieroglyphs and cuneiform scripts, but also\nmusical notation, game symbols and various other scripts and symbols (e Unicode\nConsortium, 2014). ere are 17 planes in total (e Unicode Consortium, 2014).\ne last feature in the list, General Type is also an implementation-related property.\nType can be, for example5, END_PUNCTUATION, LETTER_NUMBER or\nMATH_SYMBOL. ese constants are represented as numbers internally, which are\ntaken as feature for the clustering algorithm.\n4.3.3\nMapping features to a common scale\nAs JavaML requires numerical features, all features were mapped to numerical scales:\n• Binary features were mapped to 0 (false) and 1 (true)\n• Ternary features were mapped to 0 (false), 1 (true) and 99 (not applicable)\n• Numerical features were represented as themselves, either as whole numbers\n(e.g. word length) or as ﬂoating point numbers (e.g. vowel ratio)\n• Java speciﬁc features (18,20) take the underlying numerical value as feature\n• N-grams were encoded numerically using algorithm 1\n5e\nfull\nlist\ncan\nbe\nfound\nunder\nthe\ndocumentation\nof\nthe\nJava\nCharacter\nclass\nhp://docs.oracle.com/javase/7/docs/api/java/lang/Character.html\n25\nAlgorithm 1 N-gram numerical encoding\n1: function (word)\n2:\nsum ←0\n3:\nfor character in word do\n4:\nvalue ←code-point of character\n5:\nsum ←sum + value\n6:\nend for\n7:\nreturn sum\n8: end function\nWhile algorithm 1 does not encode n-grams in an unambiguous way (“en” and “ne”\nare both encoded as 211), it provides a suﬃciently good encoding.\n4.3.4\ne problem of unambiguous encoding\nI have tried using unambiguous encodings. e main problem with unambiguous en-\ncoding is that the notion of “distance” is distorted. e idea behind the unambiguous\nencoding is that each “word” (i.e. string of characters) is encoded numerically so that\nno two “words” are represented as the same number. Besides the encoding of each sep-\narate character, the position of the character inside the string also has to be encoded.\nA possible encoding e for a string w1w2w3 could be\new1w2w3 = n(w1) + x ∗n(w2) + y ∗n(w3)\n(19)\nwith wi the character of the string at position i, n(wi) the numerical encoding of\nthe character wi and x and y parameters. If |A| is the alphabet size of the alphabet A\nin which the word is encoded, the following constraints must be true for the encoding\nto be unambiguous:\nx ≥|A|\n(20)\ny ≥|A|2\n(21)\nIf we take for example the English alphabet with 26 lowercase and 26 uppercase\nleers, not counting punctuation, digits and other characters, it has to be true that\nx ≥52 and y ≥2704. e problem is that we cannot know in advance what size\nthe alphabet will be. If we have English and German texts, the size can be estimated\naround 60. However, if we have English, Russian and Arabic text, the size drastically\nincreases. We could choose any two very big numbers, but if we want to guarantee\nour encoding to be unambiguous, we run the risk of ending up with numbers too big\nto be represented eﬃciently.\n26\nIn this encoding scheme, distance is skewed: changes to the ﬁrst character result\nin linear distance. ‘man’ and ‘nan’ have a distance of 1, because ‘m’ and ‘n’ have a\ndistance of 1. ‘man’ and ‘lan’ have a distance of 2, etc. Changes to the second character\nare multiplied by x. ‘man’ and ‘men’ have a distance of x ∗(distance(a, e)) = 4 ∗x.\nChanges to the third character are scaled by y. For any suﬃciently big x and y, the\ndistances are too skewed to be used for automatic cluster analysis. Let us consider the\nfollowing example with only two characters for simplicity. For this example, let us\nassume x = 1373.\nna\nma\nne\nme\nna\n0\n1\n5492\n5491\nma\n1\n0\n5493\n5492\nne\n5492\n5493\n0\n1\nme\n5491\n5492\n1\n0\nTable 2: Unambiguous encoding: distances\nIt should be apparent from table 2 that the notion of “distance” is distorted. In\ncomparison, table 3 shows the encoding achieved with algorithm 1.\nna\nma\nne\nme\nna\n0\n1\n4\n3\nma\n1\n0\n5\n4\nne\n4\n5\n0\n1\nme\n3\n4\n1\n0\nTable 3: Simpliﬁed encoding: distances\nWhile this encoding is not unambiguous, it is considered suﬃciently good for our\npurposes.\n4.3.5\ne clusterer\nMost clustering algorithms such as k-means need to be passed the number of clusters to\ngenerate. As we want to work as ﬂexibly as possible, I ignored all algorithms that need\nthe number of clusters before clustering. In contrast, the x-means algorithm (Pelleg\nand Moore, 2000) estimates the number of clusters to generate itself. is algorithm\nhas been chosen to perform the language clustering tasks.\nWhile WEKA and ELKI oﬀer a graphical user interface and various graphical rep-\nresentations of the results, the output is not easily interpretable. Indeed, we can get a\nvisualization of a clustering operation as shown in ﬁgures 17 (WEKA) and 18 (ELKI).\nHowever, all data points have to be manually checked by either clicking each point\n27\nin order to get additional information about that data point (WEKA) or by hovering\nover the data points aer having selected the Object Label Tooltip option (ELKI). Fig-\nure 18 shows the information for the lowest orange rectangle data point in the ELKI\nvisualization.\nFigure 17: WEKA: Cluster visualization\n28\nFigure 18: ELKI: Cluster visualization\nerefore, I have decided to embed the x-means clustering algorithm into a custom\nframework. Originally part of the WEKA algorithms, the x-means algorithm has been\nintegrated into a Java program via the JavaML library. e framework takes an input\nﬁle, constructs the aforementioned feature vectors from the input, performs normal-\nization, passes the calculated feature vectors to the clustering algorithm and displays\nthe results in a text-based easily interpretable manner.\nPreliminary analyses have shown that the ﬁrst clustering result oen is not dis-\ncriminating enough. Hence, I perform a ﬁrst clustering analysis, followed by a second\nclustering analysis on the clusters obtained from the ﬁrst analysis.\n4.3.6\nEvaluating clusterings\ne clustering results are evaluated using four common similarity measures used in\nevaluating the accuracy of clustering algorithms. ese methods are based on counting\n29\npairs (Wagner and Wagner, 2007).\nLet us consider the clustering C = {C1, . . . , Ck}. C is a set of non-empty disjoint\nclusters C1, . . . , Ck. Let us consider the reference clustering C′ = {C1, . . . , Cl}. We\ndeﬁne the following sets.\n• S11: set of pairs that are in the same cluster in C and C′\n• S00: set of pairs that are in diﬀerent clusters in C and C′\n• S10: set of pairs that are in the same cluster in C and in diﬀerent clusters in C′\n• S01: set of pairs that are in diﬀerent clusters in C and in the same cluster in C′\nLet nij = |Sij|, with i, j ∈{0, 1} be the size of a given set Sij.\ne Rand Index is deﬁned as\nRI =\nn11 + n00\nn11 + n10 + n01 + n00\n(22)\ne Rand Index measures the accuracy of the clustering given a reference partition\n(Wagner and Wagner, 2007). However, it is criticized for being highly dependent on\nthe number of clusters (Wagner and Wagner, 2007).\ne Jaccard Index measures the similarity of sets. It is similar to the Rand Index,\nbut it disregards S00, the set of pairs that are clustered into diﬀerent clusters in C and\nC′ (Wagner and Wagner, 2007). It is calculated as\nJ =\nn11\nn11 + n10 + n01\n(23)\ne Fowlkes-Mallows Index measures precision. It is calculated as\nFM =\nn11\n√\n(n11 + n10)(n11 + n01)\n(24)\ne Fowlkes-Mallows Index has the undesired property of yielding high values\nwhen the number of clusters is small (Wagner and Wagner, 2007).\nFinally, I will indicate the F-Score. According to Manning et al. (2008), in the context\nof clustering evaluation the F(β) score is deﬁned as\nF(β) = (β2 + 1) ∗P ∗R\n(β2)P + R\n(25)\nwith precision P and recall R deﬁned as\nP =\nn11\nn11 + n10\n(26)\n30\nR =\nn11\nn11 + n01\n(27)\nBy varying β, it is possible to give more weight to either precision (β < 0) or recall\n(β > 1) (Manning et al., 2008). As I value recall higher than precision, I will indicate F1\n(β = 1) and F5 (β = 5) scores. Indeed, I want to penalize the algorithm for clustering\ntogether pairs that are separate in the gold standard while not penalizing the algorithm\nfor spliing pairs that are together in the gold standard.\nAll measures of similarity fall between [0, 1] with 0 being most dissimilar and 1 be-\ning identical. As there is no ultimate measure and all measures of similarity have their\ndrawbacks (Wagner and Wagner, 2007), all measures will be indicated in the results\nsection.\n4.4\nWeakly supervised language model induction\ne language model induction approach works in two stages. In the ﬁrst stage, n-gram\nlanguage models are induced from the text. In the second stage, the text is mapped to\nthe induced models. e algorithm for the language model induction is as follows:\nAlgorithm 2 Model induction\n1: IM\n2: for word in words do\n3:\nmodelAndScore ←MS(word)\n4:\nscore ←modelAndScore.score\n5:\nif score < threshold then\n6:\nmodel ←M(word)\n7:\nmodels.add(model)\n8:\nelse\n9:\nmaxModel ←modelAndScore.model\n10:\nmaxModel.update(word)\n11:\nend if\n12: end for\nFirst of all, an initial language model is created. For each word, the maximum model\nand maximum score is calculated. ese values correspond to the language model that\nyielded the highest probability for the word in question, and the associated probability.\nIf the score falls below a threshold t (i.e. none of the existing language models model\nthe word well enough), a new language model is created on the basis of the word and\nadded to the list of language models. Otherwise, the top scoring language model is\nupdated with the word in question.\n31\nAs the text structure itself inﬂuences the quality of the induced models, the lan-\nguage model induction is run i times (i ⩽1), with one iteration consisting of two\ninduction steps, once forward and once backward, and j times from a random position\n(j ⩽0). e initial model creation thus either picks the ﬁrst word of the text (as shown\nin algorithm 3 line 2), or the last word of the text, or a random word.\nAlgorithm 3 Initial model creation\n1: function IM\n2:\nword ←words.first\n3:\nmodel ←createModel(word)\n4:\nmodels.add(model)\n5: end function\nAlgorithm 4 Max model and max score\n1: function MS(word)\n2:\nmaxScore ←0\n3:\nmaxModel ←none\n4:\nfor model in models do\n5:\nscore ←model.probability(word)\n6:\nif score > maxScore then\n7:\nmaxScore ←score\n8:\nmaxModel ←model\n9:\nend if\n10:\nend for\n11:\nreturn maxModel, maxScore\n12: end function\nAlgorithm 4 returns both the max model and the max score wrapped as a custom\nobject. e individual values can then be read as necessary.\nAer the models have been induced, the most similar models are merged based\non distributional similarity. Distributional similarity is calculated as explained below.\nis merging step only merges one model from the forward induction group with one\nmodel from the backward induction group. e resulting model is added to the set of\nprobable (“silver”) models.\nMerging is performed according to algorithm 5. e merging algorithm only re-\ntains the common set of unigrams from both models, and all resulting bi- and trigrams,\nexcluding any bi- and trigrams that contain character that occur only in one of the\nmodels. e values for the resulting language model are calculated according to one\nof four diﬀerent merge modes.\ne merge modes are:\n32\nAlgorithm 5 Model merger\n1: function (model1,model2, mode)\n2:\nmerged ←∅\n3:\nfor unigram u1 in model1.unigrams do\n4:\nfor unigram u2 in model2.unigrams do\n5:\nif u1 = u2 then\n6:\nv1 ←f(u1)\n◃f(u1) is the frequency of u1\n7:\nv2 ←f(u2)\n8:\nvalue ←mode(v1, v2)\n9:\nunigram ←u1\n◃or u2, since both are equal\n10:\nmerged ←(unigram, value)\n11:\nelse\n12:\nexclude ←u1\n13:\nexclude ←u2\n14:\nend if\n15:\nend for\n16:\nend for\n17:\nfor all bigrams b in model1 and model2 do\n18:\nif not exclude contains any char in b then\n19:\nv1 ←f(b, model1) or 0\n◃frequency of b in model1\n20:\n◃or 0 if it does not exist\n21:\nv2 ←f(b, model2) or 0\n22:\nvalue ←mode(v1, v2)\n23:\nmerged ←(b, value)\n24:\nend if\n25:\nend for\n26:\nfor all trigrams t in model1 and model2 do\n27:\nif not exclude contains any char in t then\n28:\nv1 ←f(t, model1) or 0\n29:\nv2 ←f(t, model2) or 0\n30:\nvalue ←mode(v1, v2)\n31:\nmerged ←(t, value)\n32:\nend if\n33:\nend for\n34:\nreturn merged\n35: end function\n33\n• MAX: use the maximum value (max(v1, v2))\n• MIN: use the minimum value (min(v1, v2))\n• MEAN: use the mean value (v1+v2\n2\n)\n• ADD: use the sum of the values (v1 + v2)\nIf the random iteration count j > 0, a random word is chosen and the induction\nis run once forward and once backward starting from this position. en, the most\nsimilar models from each set are merged and added to the set of probable models.\nIt should be noted that seing the parameter j > 0 will make the algorithm non-\ndeterministic.\ne model induction is then repeated while the iteration count i has not been\nreached or until no more models are induced, with the diﬀerence that for each word,\neach probable model is ﬁrst consulted. If any of the probable models yields a score\nhigher than the threshold value t, it is assumed that the word is already well repre-\nsented by one of the probable models and no models are induced for this word. If the\nscore falls below the threshold value t, induction is run as described.\nAt the end of the induction loop, all probable models are checked against each other.\nWhile there are two models that have a similarity below the silver threshold value s,\nthe two models are merged and added to the set of very probable (“gold”) models.\nIf the set of probable models is not empty aer this merging step, all remaining\nprobable models are added to the set of very probable models.\nIn the second stage, the text is segmented according to the induced “gold” models.\nFor each word, the language model with the highest probability for the word is chosen\nas that word’s hypothetical language model.\n4.4.1\nDistributional similarity\nSuppose we have three models with the distributions of leers as shown in ﬁgures 19,\n20 and 216. Similarity could be calculated based on the occurrence of unigrams/leers\nalone, i.e. if model1 contains the leer ‘a’ and model2 also contains the leer ‘a’, their\nsimilarity increases by 1.\nHowever, if we calculate similarity in such a way, all three models are equally simi-\nlar to each other, as each of the leers occurs at least once in each model. Yet, it should\nbe clear that models 1 and 2 are very similar to each other while model 3 is dissimilar.\nerefore, in order to include the distribution of leers in the similarity measure,\nsimilarity is calculated as shown in algorithm 6.\n6e ﬁgures shown are used for illustration purposes only and do not necessarily reﬂect real language\nmodels.\n34\na\nb\nc\nd\ne\nf\ng\nh\ni\n0\n2\n4\n6\nFigure 19: Language model: Distribution 1\na\nb\nc\nd\ne\nf\ng\nh\ni\n0\n2\n4\n6\nFigure 20: Language Model: Distribution 2\n35\na\nb\nc\nd\ne\nf\ng\nh\ni\n0\n2\n4\n6\nFigure 21: Language model: Distribution 3\nAlgorithm 6 Distributional Similarity Calculation\n1: function (model1,model2)\n2:\nsimilarity ←0\n3:\ndiﬀerence ←1\n◃Initialize diﬀerence to 1 to avoid division by zero\n4:\nfor unigram u1 in model1.unigrams do\n5:\nfor unigram u2 in model2.unigrams do\n6:\nif u1 = u2 then\n◃unigram occurs in both models\n7:\nv1 ←\nf(u1)\nmodel1.size\n◃Normalize value by model size\n8:\nv2 ←\nf(u2)\nmodel2.size\n9:\nq ←|v1−v2|\nv1+v2\n10:\nsimilarity ←similarity +(2 −q)\n11:\nelse\n12:\ndiﬀerence ←diﬀerence +1\n13:\nend if\n14:\nend for\n15:\nend for\n16:\nreturn similarity\ndiﬀerence\n17: end function\nwith f(c) returning the frequency of the character c. e number 2 in (2−q) in line\n10 can be explained as follows: q expresses the dissimilarity of the models with regard\nto a unigram distribution with 0 ⩽q ⩽1, hence (1 −q) expresses the similarity. To\nthis, we add 1, as we increase similarity by 1 due to the match; we augment the simple\nincrease of 1 by the similarity of the distribution.\n36\n4.4.2\nEvaluating results\ne results of this approach can be interpreted as clusters, where each language model\nrepresents one cluster core and all words assigned to that model making up that cluster.\nEvaluation will hence be analogous to the evaluation of the clustering approach.\n4.4.3\nEstimating the parameters\nAs the language model induction can be controlled by parameters, we have to ﬁnd a\ncombination of parameters that works well for our task. e parameters i, j and “merge\nmode” have been estimated on the development set. e development set contains\nsimilar documents to those in the test set. e development set can be found in the\nappendix.\nIt has been found that the parameter combination i = 4, j = 2, ADD yields good\nresults across the development set. Hence, these values have been used for the test set\nevaluation.\n37\n5\nResults\n‘Baseline’ indicates the measurement where all words have been thrown into one clus-\nter, measured against the gold standard. For ‘Baseline 2’, every word has been put into\nits own cluster and this clustering is evaluated against the gold standard. e column\n‘F1’ stands for the F1 score and the ‘F5’ column stands for the F5 score.\nIf any of the ‘runs’ yields a higher score than any of the baseline values, the max-\nimum score is indicated in bold. If a ﬁeld contains ‘n/a’, this means that the value\ncould not be calculated for whatever reason (most oen a division by zero would have\noccurred).\n5.1\nN-Gram language model\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGerman–English\nBaseline\n0.9259\n0.9259\n0.9622\n0.9615\n0.9285\nBaseline 2\n0.0000\n0.0740\nn/a\nn/a\nn/a\nNGLM\n0.5200\n0.4428\n0.6597\n0.6138\n0.9275\nGerman–Finnish–Turkish\nBaseline\n0.3312\n0.3312\n0.5755\n0.4976\n0.3400\nBaseline 2\n0.6721\n0.0103\n0.1015\n0.0204\n0.2132\nNGLM\n0.8104\n0.3872\n0.5615\n0.5582\n0.5081\nEnglish–Fren\nBaseline\n0.7038\n0.7038\n0.8389\n0.8261\n0.7119\nBaseline 2\n0.3064\n0.0145\n0.1207\n0.0287\n0.2777\nNGLM\n0.6246\n0.3540\n0.5322\n0.5229\n0.4459\nEnglish–Transliterated Greek\nBaseline\n0.8809\n0.8809\n0.9385\n0.9385\n0.8850\nBaseline 2\n0.1269\n0.0090\n0.0949\n0.0178\n0.1911\nNGLM\n0.6932\n0.5492\n0.7117\n0.7090\n0.7708\nItalian–German\nBaseline\n0.5807\n0.5807\n0.7620\n0.7347\n0.5902\nBaseline 2\n0.4227\n0.0060\n0.0776\n0.0119\n0.1360\nNGLM\n0.7010\n0.2977\n0.4740\n0.4589\n0.5969\nTable 4: N-Gram language model results: Latin script\n38\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGreek–Russian\nBaseline\n0.5578\n0.5578\n0.7468\n0.7161\n0.5674\nBaseline 2\n0.4440\n0.0034\n0.0584\n0.0068\n0.0817\nNGLM\n0.7597\n0.5108\n0.6762\n0.6762\n0.6694\nEnglish–Greek\nBaseline\n0.9179\n0.9179\n0.9580\n0.9571\n0.9208\nBaseline 2\n0.0946\n0.0136\n0.1167\n0.0269\n0.2643\nNGLM\n0.5665\n0.3867\n0.5586\n0.5577\n0.5877\nEnglish–Spanish–Arabic\nBaseline\n0.3354\n0.3354\n0.5791\n0.5023\n0.3442\nBaseline 2\n0.6682\n0.0109\n0.1044\n0.0215\n0.2227\nNGLM\n0.9204\n0.7489\n0.8573\n0.8564\n0.8936\nEnglish–Chinese\nBaseline\n0.8474\n0.8474\n0.9205\n0.9174\n0.8524\nBaseline 2\n0.1595\n0.0082\n0.0909\n0.0164\n0.1781\nNGLM\n0.6573\n0.4476\n0.6259\n0.6184\n0.7208\nUkrainian–Russian\nBaseline\n0.4950\n0.4950\n0.7035\n0.6622\n0.5048\nBaseline 2\n0.5060\n0.0022\n0.0472\n0.0044\n0.0550\nNGLM\n0.6755\n0.3857\n0.5644\n0.5567\n0.4831\nTable 5: N-Gram language model results: Mixed script\n39\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nPali 1\nBaseline\n0.3131\n0.3131\n0.5595\n0.4768\n0.3216\nBaseline 2\n0.6906\n0.0118\n0.1089\n0.0234\n0.2379\nNGLM\n0.8153\n0.2069\n0.3434\n0.3429\n0.3608\nPali 2\nBaseline\n0.3589\n0.3589\n0.5991\n0.5283\n0.3680\nBaseline 2\n0.6495\n0.0238\n0.1543\n0.0465\n0.3880\nNGLM\n0.7173\n0.1958\n0.3336\n0.3275\n0.3971\nPali 3\nBaseline\n0.4947\n0.4947\n0.7033\n0.6619\n0.5045\nBaseline 2\n0.5075\n0.0045\n0.0676\n0.0091\n0.1067\nNGLM\n0.7874\n0.0816\n0.1692\n0.1508\n0.1064\nPali 4\nBaseline\n0.4000\n0.4000\n0.6324\n0.5714\n0.4094\nBaseline 2\n0.6000\n0.0000\nn/a\nn/a\nn/a\nNGLM\n0.3000\n0.1250\n0.2357\n0.2222\n0.1699\nPali 5\nBaseline\n0.5800\n0.5800\n0.7615\n0.7341\n0.5895\nBaseline 2\n0.4236\n0.0063\n0.0798\n0.0126\n0.1430\nNGLM\n0.4777\n0.2496\n0.4065\n0.3995\n0.4816\nTable 6: N-Gram language model results: Pali data\n40\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nTwitter 1\nBaseline\n0.4615\n0.4615\n0.6793\n0.6315\n0.4712\nBaseline 2\n0.5384\n0.0000\nn/a\nn/a\nn/a\nNGLM\n0.8589\n0.5925\n0.7542\n0.7441\n0.8757\nTwitter 2\nBaseline\n0.5555\n0.5555\n0.7453\n0.7142\n0.5652\nBaseline 2\n0.4444\n0.0000\nn/a\nn/a\nn/a\nNGLM\n0.7485\n0.6090\n0.7591\n0.7570\n0.8121\nTwitter 3\nBaseline\n0.6583\n0.6583\n0.8113\n0.7939\n0.6670\nBaseline 2\n0.3416\n0.0000\nn/a\nn/a\nn/a\nNGLM\n0.6750\n0.4347\n0.6479\n0.6060\n0.8996\nTwitter 4\nBaseline\n0.8750\n0.8750\n0.9354\n0.9333\n0.8792\nBaseline 2\n0.1250\n0.0000\nn/a\nn/a\nn/a\nNGLM\n0.7250\n0.5822\n0.7597\n0.7360\n0.9545\nTwitter 5\nBaseline\n0.4285\n0.4285\n0.6546\n0.6000\n0.4382\nBaseline 2\n0.5714\n0.0000\nn/a\nn/a\nn/a\nNGLM\n0.6666\n0.1250\n0.2672\n0.2222\n0.4561\nTable 7: N-Gram language model results: Twier data\n41\n5.2\nTextcat\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGerman–English\nBaseline\n0.9259\n0.9259\n0.9622\n0.9615\n0.9285\nBaseline 2\n0.0000\n0.0740\nn/a\nn/a\nn/a\nTextcat\n0.8632\n0.8518\n0.9200\n0.9200\n0.9200\nGerman–Finnish–Turkish\nBaseline\n0.3312\n0.3312\n0.5755\n0.4976\n0.3400\nBaseline 2\n0.6721\n0.0103\n0.1015\n0.0204\n0.2132\nTextcat\n0.4095\n0.1903\n0.3823\n0.3198\n0.2124\nEnglish–Fren\nBaseline\n0.7038\n0.7038\n0.8389\n0.8261\n0.7119\nBaseline 2\n0.3064\n0.0145\n0.1207\n0.0287\n0.2777\nTextcat\n0.3890\n0.3211\n0.5476\n0.4861\n0.3411\nEnglish–Transliterated Greek\nBaseline\n0.8809\n0.8809\n0.9385\n0.9385\n0.8850\nBaseline 2\n0.1269\n0.0090\n0.0949\n0.0178\n0.1911\nTextcat\n0.5202\n0.4853\n0.6678\n0.6535\n0.5492\nItalian–German\nBaseline\n0.5807\n0.5807\n0.7620\n0.7347\n0.5902\nBaseline 2\n0.4227\n0.0060\n0.0776\n0.0119\n0.1360\nTextcat\n0.4030\n0.3057\n0.5014\n0.4682\n0.3520\nTable 8: Textcat results: Latin script\n42\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGreek–Russian\nBaseline\n0.5578\n0.5578\n0.7468\n0.7161\n0.5674\nBaseline 2\n0.4440\n0.0034\n0.0584\n0.0068\n0.0817\nTextcat\n0.4468\n0.2971\n0.4769\n0.4581\n0.3644\nEnglish–Greek\nBaseline\n0.9179\n0.9179\n0.9580\n0.9571\n0.9208\nBaseline 2\n0.0946\n0.0136\n0.1167\n0.0269\n0.2643\nTextcat\n0.5357\n0.4933\n0.6730\n0.6607\n0.5619\nEnglish–Spanish–Arabic\nBaseline\n0.3354\n0.3354\n0.5791\n0.5023\n0.3442\nBaseline 2\n0.6682\n0.0109\n0.1044\n0.0215\n0.2227\nTextcat\n0.3956\n0.2832\n0.5042\n0.4414\n0.3052\nEnglish–Chinese\nBaseline\n0.8474\n0.8474\n0.9205\n0.9174\n0.8524\nBaseline 2\n0.1595\n0.0082\n0.0909\n0.0164\n0.1781\nTextcat\n0.5018\n0.4468\n0.6251\n0.6177\n0.5408\nUkrainian–Russian\nBaseline\n0.4950\n0.4950\n0.7035\n0.6622\n0.5048\nBaseline 2\n0.5060\n0.0022\n0.0472\n0.0044\n0.0550\nTextcat\n0.3787\n0.2625\n0.4472\n0.4159\n0.3105\nTable 9: Textcat results: Mixed script\n43\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nPali 1\nBaseline\n0.3131\n0.3131\n0.5595\n0.4768\n0.3216\nBaseline 2\n0.6906\n0.0118\n0.1089\n0.0234\n0.2379\nTextcat\n0.4531\n0.2508\n0.4849\n0.4011\n0.2641\nPali 2\nBaseline\n0.3589\n0.3589\n0.5991\n0.5283\n0.3680\nBaseline 2\n0.6495\n0.0238\n0.1543\n0.0465\n0.3880\nTextcat\n0.4307\n0.2745\n0.5088\n0.4307\n0.2888\nPali 3\nBaseline\n0.4947\n0.4947\n0.7033\n0.6619\n0.5045\nBaseline 2\n0.5075\n0.0045\n0.0676\n0.0091\n0.1067\nTextcat\n0.2032\n0.0704\n0.2502\n0.1315\n0.0736\nPali 4\nBaseline\n0.4000\n0.4000\n0.6324\n0.5714\n0.4094\nBaseline 2\n0.6000\n0.0000\nn/a\nn/a\nn/a\nTextcat\n0.5000\n0.1666\n0.2886\n0.2857\n0.2524\nPali 5\nBaseline\n0.5800\n0.5800\n0.7615\n0.7341\n0.5895\nBaseline 2\n0.4236\n0.0063\n0.0798\n0.0126\n0.1430\nTextcat\n0.5090\n0.3458\n0.5141\n0.5140\n0.5236\nTable 10: Textcat results: Pali data\n44\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nTwitter 1\nBaseline\n0.4615\n0.4615\n0.6793\n0.6315\n0.4712\nBaseline 2\n0.5384\n0.0000\nn/a\nn/a\nn/a\nTextcat\n0.3736\n0.2597\n0.4460\n0.4123\n0.3049\nTwitter 2\nBaseline\n0.5555\n0.5555\n0.7453\n0.7142\n0.5652\nBaseline 2\n0.4444\n0.0000\nn/a\nn/a\nn/a\nTextcat\n0.4678\n0.4347\n0.6158\n0.6060\n0.5207\nTwitter 3\nBaseline\n0.6583\n0.6583\n0.8113\n0.7939\n0.6670\nBaseline 2\n0.3416\n0.0000\nn/a\nn/a\nn/a\nTextcat\n0.6838\n0.6446\n0.8011\n0.7839\n0.6586\nTwitter 4\nBaseline\n0.8750\n0.8750\n0.9354\n0.9333\n0.8792\nBaseline 2\n0.1250\n0.0000\nn/a\nn/a\nn/a\nTextcat\n0.8833\n0.8666\n0.9309\n0.9285\n0.8711\nTwitter 5\nBaseline\n0.4285\n0.4285\n0.6546\n0.6000\n0.4382\nBaseline 2\n0.5714\n0.0000\nn/a\nn/a\nn/a\nTextcat\n0.3333\n0.3333\n0.5773\n0.5000\n0.3421\nTable 11: Textcat results: Twier data\n45\n5.3\nClustering\ne ﬁrst run indicates the value aer one clustering step, and the second run indicates\nthe value aer applying the clustering algorithm to the results of the ﬁrst run.\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGerman–English\nBaseline\n0.9259\n0.9259\n0.9622\n0.9615\n0.9285\nBaseline 2\n0.0000\n0.0740\nn/a\nn/a\nn/a\nFirst run\n0.4102\n0.3929\n0.6069\n0.5642\n0.8549\nSecond\nrun\n0.2336\n0.1970\n0.4199\n0.3291\n0.7712\nGerman–Finnish–Turkish\nBaseline\n0.3312\n0.3312\n0.5755\n0.4976\n0.3400\nBaseline 2\n0.6721\n0.0103\n0.1015\n0.0204\n0.2132\nFirst run\n0.4841\n0.1764\n0.3369\n0.2998\n0.2110\nSecond\nrun\n0.6259\n0.1611\n0.2840\n0.2775\n0.2320\nEnglish–Fren\nBaseline\n0.7038\n0.7038\n0.8389\n0.8261\n0.7119\nBaseline 2\n0.3064\n0.0145\n0.1207\n0.0287\n0.2777\nFirst run\n0.4051\n0.2980\n0.5001\n0.4592\n0.3362\nSecond\nrun\n0.4601\n0.1836\n0.3116\n0.3103\n0.2857\nEnglish–Transliterated Greek\nBaseline\n0.8809\n0.8809\n0.9385\n0.9385\n0.8850\nBaseline 2\n0.1269\n0.0090\n0.0949\n0.0178\n0.1911\nFirst run\n0.5867\n0.3977\n0.5725\n0.5691\n0.6320\nSecond\nrun\n0.5423\n0.3161\n0.4909\n0.4804\n0.5934\nItalian–German\nBaseline\n0.5807\n0.5807\n0.7620\n0.7347\n0.5902\nBaseline 2\n0.4227\n0.0060\n0.0776\n0.0119\n0.1360\nFirst run\n0.4222\n0.2838\n0.4640\n0.4421\n0.3453\nSecond\nrun\n0.4915\n0.2472\n0.4006\n0.3964\n0.3499\nTable 12: Clustering results: Latin script\n46\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGreek–Russian\nBaseline\n0.5578\n0.5578\n0.7468\n0.7161\n0.5674\nBaseline 2\n0.4440\n0.0034\n0.0584\n0.0068\n0.0817\nFirst run\n0.5787\n0.3811\n0.5672\n0.5519\n0.4549\nSecond\nrun\n0.7536\n0.3883\n0.5899\n0.4494\n0.7914\nEnglish–Greek\nBaseline\n0.9179\n0.9179\n0.9580\n0.9571\n0.9208\nBaseline 2\n0.0946\n0.0136\n0.1167\n0.0269\n0.2643\nFirst run\n0.4244\n0.2482\n0.4015\n0.3977\n0.4553\nSecond\nrun\n0.3705\n0.0855\n0.1784\n0.1576\n0.2777\nEnglish–Spanish–Arabic\nBaseline\n0.3354\n0.3354\n0.5791\n0.5023\n0.3442\nBaseline 2\n0.6682\n0.0109\n0.1044\n0.0215\n0.2227\nFirst run\n0.8016\n0.5650\n0.7400\n0.7221\n0.6008\nSecond\nrun\n0.7226\n0.2860\n0.4495\n0.4448\n0.5130\nEnglish–Chinese\nBaseline\n0.8474\n0.8474\n0.9205\n0.9174\n0.8524\nBaseline 2\n0.1595\n0.0082\n0.0909\n0.0164\n0.1781\nFirst run\n0.5480\n0.3356\n0.5087\n0.5025\n0.5866\nSecond\nrun\n0.5138\n0.2584\n0.4361\n0.4107\n0.5957\nUkrainian–Russian\nBaseline\n0.4950\n0.4950\n0.7035\n0.6622\n0.5048\nBaseline 2\n0.5060\n0.0022\n0.0472\n0.0044\n0.0550\nFirst run\n0.5867\n0.1953\n0.3268\n0.3267\n0.3305\nSecond\nrun\n0.5934\n0.1154\n0.2178\n0.2070\n0.2907\nTable 13: Clustering results: Mixed script\n47\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nPali 1\nBaseline\n0.3131\n0.3131\n0.5595\n0.4768\n0.3216\nBaseline 2\n0.6906\n0.0118\n0.1089\n0.0234\n0.2379\nFirst run\n0.4674\n0.2540\n0.4898\n0.4051\n0.2666\nSecond\nrun\n0.7168\n0.2547\n0.4118\n0.4060\n0.3516\nPali 2\nBaseline\n0.3589\n0.3589\n0.5991\n0.5283\n0.3680\nBaseline 2\n0.6495\n0.0238\n0.1543\n0.0465\n0.3880\nFirst run\n0.6738\n0.3026\n0.4777\n0.4646\n0.3825\nSecond\nrun\n0.6646\n0.1865\n0.3147\n0.3144\n0.3021\nPali 3\nBaseline\n0.4947\n0.4947\n0.7033\n0.6619\n0.5045\nBaseline 2\n0.5075\n0.0045\n0.0676\n0.0091\n0.1067\nFirst run\n0.5686\n0.0746\n0.2002\n0.1389\n0.0831\nSecond\nrun\n0.7534\n0.0911\n0.1962\n0.1670\n0.1125\nPali 4\nBaseline\n0.4000\n0.4000\n0.6324\n0.5714\n0.4094\nBaseline 2\n0.6000\n0.0000\nn/a\nn/a\nn/a\nFirst run\n0.5333\n0.3000\n0.5477\n0.4615\n0.3083\nSecond\nrun\n0.3000\n0.3000\n0.5477\n0.4615\n0.3083\nPali 5\nBaseline\n0.5800\n0.5800\n0.7615\n0.7341\n0.5895\nBaseline 2\n0.4236\n0.0063\n0.0798\n0.0126\n0.1430\nFirst run\n0.5294\n0.2472\n0.4111\n0.3965\n0.5242\nSecond\nrun\n0.4666\n0.1214\n0.2524\n0.2166\n0.4117\nTable 14: Clustering results: Pali data\n48\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nTwitter 1\nBaseline\n0.4615\n0.4615\n0.6793\n0.6315\n0.4712\nBaseline 2\n0.5384\n0.0000\nn/a\nn/a\nn/a\nFirst run\n0.8681\n0.7142\n0.8451\n0.8333\n0.7222\nSecond\nrun\n0.8461\n0.6000\n0.7745\n0.7499\n0.9750\nTwitter 2\nBaseline\n0.5555\n0.5555\n0.7453\n0.7142\n0.5652\nBaseline 2\n0.4444\n0.0000\nn/a\nn/a\nn/a\nFirst run\n0.4575\n0.3941\n0.5655\n0.5654\n0.5573\nSecond\nrun\n0.4967\n0.3888\n0.5615\n0.5600\n0.6012\nTwitter 3\nBaseline\n0.6583\n0.6583\n0.8113\n0.7939\n0.6670\nBaseline 2\n0.3416\n0.0000\nn/a\nn/a\nn/a\nFirst run\n0.4571\n0.3595\n0.5525\n0.5289\n0.7215\nSecond\nrun\n0.3523\n0.2093\n0.3997\n0.3461\n0.6428\nTwitter 4\nBaseline\n0.8750\n0.8750\n0.9354\n0.9333\n0.8792\nBaseline 2\n0.1250\n0.0000\nn/a\nn/a\nn/a\nFirst run\n0.9019\n0.8584\n0.9265\n0.9238\n0.8631\nSecond\nrun\n0.6250\n0.5000\n0.6789\n0.6666\n0.8080\nTwitter 5\nBaseline\n0.4285\n0.4285\n0.6546\n0.6000\n0.4382\nBaseline 2\n0.5714\n0.0000\nn/a\nn/a\nn/a\nFirst run\n0.7142\n0.4666\n0.6831\n0.6363\n0.4764\nSecond\nrun\n0.5714\n0.3076\n0.4780\n0.4705\n0.4046\nTable 15: Clustering results: Twier data\n49\n5.4\nLanguage model induction\nIn addition to highlighting results that outperform the baseline values, the following\ntables have been color coded. Results that outperform the clustering algorithm are\nindicated in red and results that outperform both the clustering algorithm and the n-\ngram language model are indicated in blue.7\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGerman–English\nBaseline\n0.9259\n0.9259\n0.9622\n0.9615\n0.9285\nBaseline 2\n0.0000\n0.0740\nn/a\nn/a\nn/a\nInducted\n0.6837\n0.6574\n0.7988\n0.7932\n0.8896\nGerman–Finnish–Turkish\nBaseline\n0.3312\n0.3312\n0.5755\n0.4976\n0.3400\nBaseline 2\n0.6721\n0.0103\n0.1015\n0.0204\n0.2132\nInducted\n0.6438\n0.1771\n0.3057\n0.3009\n0.2588\nEnglish–Fren\nBaseline\n0.7038\n0.7038\n0.8389\n0.8261\n0.7119\nBaseline 2\n0.3064\n0.0145\n0.1207\n0.0287\n0.2777\nInducted\n0.6171\n0.2835\n0.4427\n0.4418\n0.4692\nEnglish–Transliterated Greek\nBaseline\n0.8809\n0.8809\n0.9385\n0.9385\n0.8850\nBaseline 2\n0.1269\n0.0090\n0.0949\n0.0178\n0.1911\nInducted\n0.4436\n0.2398\n0.4277\n0.3868\n0.6382\nItalian–German\nBaseline\n0.5807\n0.5807\n0.7620\n0.7347\n0.5902\nBaseline 2\n0.4227\n0.0060\n0.0776\n0.0119\n0.1360\nInducted\n0.5658\n0.1536\n0.2871\n0.2664\n0.4065\nTable 16: Induction results: Latin script\n7Results that outperform only the n-gram language model would have been indicated in green, but\nthere is no score that outperforms only the n-gram language model.\n50\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nGreek–Russian\nBaseline\n0.5578\n0.5578\n0.7468\n0.7161\n0.5674\nBaseline 2\n0.4440\n0.0034\n0.0584\n0.0068\n0.0817\nInducted\n0.7142\n0.4222\n0.5940\n0.5937\n0.6125\nEnglish–Greek\nBaseline\n0.9179\n0.9179\n0.9580\n0.9571\n0.9208\nBaseline 2\n0.0946\n0.0136\n0.1167\n0.0269\n0.2643\nInducted\n0.4769\n0.3266\n0.5089\n0.4924\n0.6423\nEnglish–Spanish–Arabic\nBaseline\n0.3354\n0.3354\n0.5791\n0.5023\n0.3442\nBaseline 2\n0.6682\n0.0109\n0.1044\n0.0215\n0.2227\nInducted\n0.7783\n0.5677\n0.7534\n0.7242\n0.5773\nEnglish–Chinese\nBaseline\n0.8474\n0.8474\n0.9205\n0.9174\n0.8524\nBaseline 2\n0.1595\n0.0082\n0.0909\n0.0164\n0.1781\nInducted\n0.5657\n0.3343\n0.5258\n0.5011\n0.6953\nUkrainian–Russian\nBaseline\n0.4950\n0.4950\n0.7035\n0.6622\n0.5048\nBaseline 2\n0.5060\n0.0022\n0.0472\n0.0044\n0.0550\nInducted\n0.6289\n0.1000\n0.1935\n0.1818\n0.2659\nTable 17: Induction results: Mixed script\n51\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nPali 1\nBaseline\n0.3131\n0.3131\n0.5595\n0.4768\n0.3216\nBaseline 2\n0.6906\n0.0118\n0.1089\n0.0234\n0.2379\nInducted\n0.7856\n0.1683\n0.2898\n0.2882\n0.3188\nPali 2\nBaseline\n0.3589\n0.3589\n0.5991\n0.5283\n0.3680\nBaseline 2\n0.6495\n0.0238\n0.1543\n0.0465\n0.3880\nInducted\n0.8148\n0.5000\n0.6686\n0.6666\n0.7176\nPali 3\nBaseline\n0.4947\n0.4947\n0.7033\n0.6619\n0.5045\nBaseline 2\n0.5075\n0.0045\n0.0676\n0.0091\n0.1067\nInducted\n0.8492\n0.0569\n0.1083\n0.1078\n0.1186\nPali 4\nBaseline\n0.4000\n0.4000\n0.6324\n0.5714\n0.4094\nBaseline 2\n0.6000\n0.0000\nn/a\nn/a\nn/a\nInducted\n0.6000\n0.0000\n0.0000\nn/a\nn/a\nPali 5\nBaseline\n0.5800\n0.5800\n0.7615\n0.7341\n0.5895\nBaseline 2\n0.4236\n0.0063\n0.0798\n0.0126\n0.1430\nInducted\n0.4033\n0.2082\n0.3504\n0.3446\n0.4134\nTable 18: Induction results: Pali data\n52\nRand\nJaccard\nFowlkes-\nMallows\nF1\nF5\nTwitter 1\nBaseline\n0.4615\n0.4615\n0.6793\n0.6315\n0.4712\nBaseline 2\n0.5384\n0.0000\nn/a\nn/a\nn/a\nInducted\n0.6282\n0.3695\n0.5515\n0.5396\n0.4533\nTwitter 2\nBaseline\n0.5555\n0.5555\n0.7453\n0.7142\n0.5652\nBaseline 2\n0.4444\n0.0000\nn/a\nn/a\nn/a\nInducted\n0.7719\n0.6020\n0.7687\n0.7515\n0.9325\nTwitter 3\nBaseline\n0.6583\n0.6583\n0.8113\n0.7939\n0.6670\nBaseline 2\n0.3416\n0.0000\nn/a\nn/a\nn/a\nInducted\n0.5916\n0.3000\n0.5236\n0.4615\n0.8185\nTwitter 4\nBaseline\n0.8750\n0.8750\n0.9354\n0.9333\n0.8792\nBaseline 2\n0.1250\n0.0000\nn/a\nn/a\nn/a\nInducted\n0.5250\n0.3736\n0.5615\n0.5439\n0.7055\nTwitter 5\nBaseline\n0.4285\n0.4285\n0.6546\n0.6000\n0.4382\nBaseline 2\n0.5714\n0.0000\nn/a\nn/a\nn/a\nInducted\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\nTable 19: Induction results: Twier data\n53\n6\nDiscussion\ne work by Seldin et al. (2001) is similar to the work presented here. ey propose\nan unsupervised language (and protein sequence) segmentation approach that yields\naccurate segmentations. While their work looks promising, it also has its drawbacks.\neir method requires longer monolingual text fragments and a sizable amount of text.\nFurthermore, they disallow switching language models aer each word. is presump-\ntion will fail to detect single-word inclusions and structures as shown in ﬁgure 22,\nwhere the language alternates aer each word.\nw1\nw2\nw3\nw4\nw5\nw6\nw7\n…\nFigure 22: Alternating language structure\nWhile this structure looks very artiﬁcial, such a structure is found, for instance,\nin the ﬁh Pali dictionary text, in the passage “Pacati, [Ved. pacati, Igd. *peqǔō, Av.\npac-;”. In this case, ‘red’ corresponds to Pali, ‘blue’ to (abbreviations in) English and\n‘green’ to reconstructed Indo-european.\n6.1\nN-Gram language models\ne trained n-gram language model approach works well on the Latin script data, man-\naging to single out the German inclusion from the English–German text (even though\nit is classiﬁed as “other” instead of German).\nFor German–Finnish–Turkish, English–French, English–Transliterated Greek and\nItalian–German, the separation of the main languages involved is good, although there\nappear to be some problems when words contain non-word characters such as quotes\nor parentheses.\nSome puzzling misclassiﬁcations happen in the English–Transliterated Greek case:\nagápe is considered English and éros is considered Transliterated Amharic.\nIn the Italian–German text, the Italian language leads to a rather important Spanish\ncluster due to the relatedness of the two Romance languages.\nOn the mixed script data set, the results are more diverse. Greek–Russian, En-\nglish–Spanish–Arabic and Ukrainian–Russian are segmented well, with English– Span-\nish–Arabic having Spanish split into Spanish, French and Italian due to the relatedness\nof the languages.\nIn contrast, the segmentation of English–Greek did not work well at all. Of the two\nGreek words ἀγάπη and ἔρως, ἀγάπη was considered French and ἔρως was considered\nRussian. It must be noted, though, that these words bear polytonic diacritics, whereas\nthe model was trained on monotonic Greek.\n54\nAlso, the segmentation of English–Chinese did not work well. is is probably\ndue to the way the model was trained. Chinese script is wrien without whitespace\ncharacters between words, and the correct segmentation of a text wrien in Chinese\nrequires in-depth knowledge of the language. Some words are wrien with only one\ncharacter, but others are composed of two or more characters, with the meaning oen\nbeing non-compositional; the meaning of a two-character word is diﬀerent from the\nsum of the meaning of the two characters. Sometimes, more than one segmentation\nwould be possible and the context decides on which segmentation is correct. In other\ncases, more than one segmentation might be correct. is problem occurs with all\nscripts that are wrien without whitespace.\nAs with the simpliﬁed assumption in the tokenization of whitespace-scripts, where\nI consider a word to be a character sequence delineated by whitespace, I have treated\neach character as a word. Adapting the method to Chinese and similar scripts would\nhave been possible, but would have introduced the need for large amounts of external\nlinguistic knowledge. Indeed, every possible non-whitespace-script would have to be\nconsidered, and each of the tokenizers would be language dependent, i.e. a tokenizer\nfor Chinese would not work on Korean or Japanese.\ne supervised approach did not work well on the Pali dictionary data. While\nEnglish words could be isolated somewhat successfully, the rest of the data proved\ndiﬃcult to segment. As an example, let us look at the ﬁrst Pali text. e English\ncluster contains almost only English words, but not all, the “other” cluster contains\nmainly marked up words, and the rest is seemingly haphazardly distributed among\nthe other models.\nPali 1: abbha\n• (AR) ., 134., 289.\n• (DE) Miln), imber, dark), Miln\n• (EL) (=, (abbhaŋ\n• (EN) water, mountain, of, free, (used, or, like, referred, (also, A, is, cloudy,\nclouds, later, a, froth, 1, summit, thundering, by, mass, Pv, Oir, obscure, scum,\nthat, water]., thick, As, from, It, is, at, as, the, in, clouds, things, also\n• (ES) (dense, f., sense, expl, rajo\n• (FI) 239., rain;, Lat., Vin, perhaps, SnA\n• (FR) cloud, Dh, adj., point, cloud, Dhs, A), rain, VvA, DhsA, list\n• (IT) \\”dark, &, ambha, 3, 1, 317, J, sunshine, cp., abhra, [Vedic, (megho\n55\n• (PL) 487, =, S, 295, <br, moon–, 249\n• (RU) 348, 53\n• (TR) viz., ambu, Vv\n• (TrAM) 687, PvA, (°sama, 101, (nīl°, (cp., 64;, (nt.), 581, m., Sn, 1064;\n• (TrEL) , Gr., Sk., Idg., to, pabbata, nt.\n• (UK) 12)., 273, 617, 348)., 250;, 251)., 382).\n• (other) <b> –saŋvilāpa </b>, <b> –mua </b>, <smallcaps> vi. </smallcaps>,\n(mahiyā, <smallcaps> iv. </smallcaps>, cloud\\”;, <b> Rāhu </b>, <b> abbhā\n</b>, <b> abbhaŋ, <superscript> 9 </superscript>, marajo </b>, abbhāmua,\nvalāhaka);, <smallcaps> i. </smallcaps>, <b> abbhāmaa </b>, valāhaka–sikhara,\n<superscript> s. </superscript>, <smallcaps> ii. </smallcaps>, <b> dhū-, storm–\ncloud, /><b> –kūṭa </b>, thunder–cloud);, <at> a)fro\\\\s </at>, <b> –paṭala </b>,\n<at>o)/mbros</at>, nīla–megha, <superscript>1</superscript>, *m̊ bhro, \\”dull\\”;,\nacchādesi);, mahikā</b>, <b> –ghana </b>\nOn the Twier data, the supervised approach achieved passable results. While\nthe numbers look great, the actual segmentations do not. For Twier 1, too many\nclusters were generated, for Twier 2 and 3, the recognition of French words worked\nsomewhat, also recognizing English words as French and French words as English.\nFor Twier 4, the Polish inclusion was isolated but recognized as “other”, together\nwith “strawberries”. e recognition of transliterated Amharic worked satisfactorily,\nyielding ‘naw’ to the Polish model.\nAs the number of language models increases, so does the risk of misclassiﬁcation.\nAs can be seen, we already have quite some misclassiﬁcation with only 15 language\nmodels. For example, in our data, the English preposition ‘to’ is oen erroneously clas-\nsiﬁed as ‘transliterated Greek’. e Greek particle το ‘to’ can be either the neuter sin-\ngular accusative or nominative deﬁnite article ‘the’, the masculine singular accusative\nor nominative deﬁnite article ‘the’ or the 3rd person neuter singular nominative/ac-\ncusative weak pronoun ‘it’, and as such is rather frequent in the language. is is\nespecially problematic with the transliterated Greek language model, which tends to\nmisclassify the English preposition ‘to’ as transliterated Greek.\nA quick corpus study using the Corpus of Modern Greek8 and the Corpus of Con-\ntemporary American English9 reveals that the frequency per million words for the\nGreek particle το is 22666, while the English preposition ‘to’ has a frequency per mil-\nlion words of 25193. eir relative frequencies are very close together, and it might\n8http://web-corpora.net/GreekCorpus/\n9http://corpus.byu.edu/coca/\n56\njust have happened that the training data used in this work contained more Greek ‘to’s\nthan English ‘to’s, leading to this misclassiﬁcation.\nOther reasons for misclassiﬁcation include relatedness of the modeled languages\nas in the case of Germanic or Romance language families. Also, the text types used\nfor training and the text types used for testing play an important role, as well as the\namount of training data.\nFor n-gram language models, the quality of the model is dependent on the texts\nused for training and the texts used in evaluation. It is probable that a diﬀerent training\nset would have yielded diﬀerent results. is is also the problem with the supervised\napproach; it is necessary to have language data for training and the trained models\nreﬂect the training data to some extent.\n6.2\nTextcat\nTextcat works well on monolingual texts. However, it fails on multilingual texts and\ndoes not work well on short fragments of text, such as single words. Many of the words\nare tagged as unknown, and if a language has been identiﬁed, the language guess oen\nis not correct. Hence, Textcat cannot be used for language segmentation purposes.\nIndeed, Textcat fails to exceed the baseline values except for two cases: ‘Twier\n3’ and ‘Twier 4’ yield beer values than the baseline values. However, upon closer\ninspection, it is clear that the numerical index values do not give a reliable picture of\nthe quality of the clustering.\nIndeed, while the clustering of ‘Twier 3’ is not nonsensical, it is not very good,\nfailing to extract the French insertion ‘breuvages’. e Rand Index also only shows a\nslightly beer value than the baseline values. It seems that the outstanding score for\n‘Twier 4’ is achieved because both the clustering by Textcat and the gold standard\nhave the same number of clusters.\nTables 20 and 21 show the clusterings side by side. Clearly, Textcat performed\npoorly despite the high numerical index values. A closer inspection of all the Textcat\nresults shows that Textcat performs poorly at the task of language segmentation; oen,\na word cannot be assigned a language and thus is added to the cluster of ‘unknown’\nlanguage words. For the words where a language has been identiﬁed, it most oen\nis not the correct language. While language identiﬁcation is not necessary for the\ntask of language segmentation, it helps to understand why Textcat failed at the task of\nlanguage segmentation.\n57\nTextcat\nGold standard\nCluster 1\n∅\nbreuvages\nCluster 2\n#bilingualism\n#FWWC2015, #bilingualism\nCluster 3\nFood, and, breuvages, in, Ed-\nmonton, are, ready, to, go,\njust, waiting, for, the, fans,\n#FWWC2015\nFood, and, in, Edmonton, are,\nready, to, go, just, waiting, for,\nthe, fans\nTable 20: ‘Twier 3’: Textcat versus Gold clustering\nTextcat\nGold standard\nCluster 1\nstrawberries,\nżubrówka\nCluster 2\nmy, dad, comes, back, from,\npoland, with, two, crates, of,\nżubrówka, and, adidas, jack-\nets, omg\nmy, dad, comes, back, from,\npoland,\nwith,\ntwo,\ncrates,\nof, strawberries, and, adidas,\njackets, omg\nTable 21: ‘Twier 4’: Textcat versus Gold clustering\n6.3\nClustering\ne clustering results are more diﬃcult to interpret. Oen, the ﬁrst distinction made\nseems to be based on case, i.e. words that begin with a capital leer versus words that\nare all lowercase leers. e second run on the ‘mixed script: English – Greek’ data\nshows that the ﬁrst cluster from the ﬁrst run has been separated into a cluster with\nwords that begin with a capital leer and two clusters with words that don’t begin\nwith a capital leer.\nEnglish–Greek: First run: First cluster\n• “intimate, “without, Although, Aquinas, Christians, Corinthians, Socrates, Sym-\nposium, Testament, Whether, aﬀection, ancient, another.”, appreciation, aspires,\naraction, araction.”, becomes, benevolence., biblical, brotherly, chapter,”, char-\nity;, children, children., contemplation, content, continues, contributes, deﬁni-\ntion:, described, existence;, explained, express, feeling, feelings, ﬁnding, further,\nholding, initially, inspired, knowledge, marriage., necessary, non-corporeal, pas-\nsage, passion.”, philosophers, physical, platonic, reﬁned, relationships, returned,\nself-beneﬁt)., sensually, spiritual, subject, suggesting, through, throughout, tran-\nscendence., unconditional, understanding, without, youthful\n58\nEnglish–Greek: Second run: Splitting of ﬁrst cluster\n• aﬀection, ancient, another.”, aspires, becomes, biblical, chapter,”, charity;, chil-\ndren, children., content, deﬁnition:, feeling, feelings, ﬁnding, holding, marriage.,\nnecessary, passage, passion.”, platonic, reﬁned, returned, subject, through, with-\nout\n• Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament,\nWhether\n• “intimate, appreciation, araction, araction.”, benevolence., brotherly, contem-\nplation, continues, contributes, described, existence;, explained, express, further,\ninitially, inspired, knowledge, non-corporeal, philosophers, physical, relation-\nships, self-beneﬁt)., sensually, spiritual, suggesting, throughout, transcendence.,\nunconditional, understanding, youthful\nAnother important distinction seems to be the length of words. Indeed, the results\noen show clusters that clearly are based on the length of the contained words. e\nﬁrst run on the ‘latin script: German – Italian’ data shows that short words have been\nsingled out into the ﬁrst cluster.\nItalian–German: First run: First cluster\n• (il, E, So, a, ad, da, di, e, es, ha, i, il, in, la, le, lo, ma, ne, se, si, un, va, zu\ne clustering works well when the scripts involved are dissimilar, as in the case\nof the English–Chinese text, where the Chinese characters were isolated aer the ﬁrst\nrun, and also the English–Spanish–Arabic example, where the Arabic part was com-\npletely isolated in the ﬁrst run.\ne closer the scripts become, the less well clear cut the results are. For Greek–\nRussian, the results are acceptable, with one mixed cluster. However, the number of\nclusters is too high for the number of languages involved and the separation is only\nachieved aer two consecutive clusterings.\ne clustering of closer scripts, such as Ukrainian–Russian does not work well. e\nclusters, with the exception of the cluster containing the datum ‘9—13’ are all impure,\nconsisting of Ukrainian and Russian words. e second run also fails at improving the\nclustering.\nFinally, clustering of latin based scripts does not perform well unless diacritics are\ninvolved and the diacritics form the most salient distinction. Word containing leers\nwith diacritics are then generally separated from words containing no diacritics, as in\nthe German–Finnish-Turkish example. e ﬁrst run generates a cluster for numbers,\ntwo clusters with diacritics and one cluster without diacritics.\n59\nProbably for this reason, the clustering of Transliterated Greek–English and Greek–\nEnglish worked surprisingly well. In both cases, the ﬁrst run managed to separate\nthe (transliterated) Greek parts from the English words. However, unaccented Greek\nwords such as Agape, erotas or eros were clustered with English.\nEnglish–Transliterated Greek: First run: Transliterated Greek cluster\n• agápe, philía, storgē., éros\nEnglish–Greek: First run: Greek cluster\n• (ἀγάπη, (ἔρως, Agápe, agápē), Éros, érōs), –\ne problem is that when there are other salient distinguishing features besides\ndiacritics, the result is less good, as can be seen on the Pali data.\nPali: abhijjhitar: Second run\n• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., A, M\n• =, l., v.\n• <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, ag., fr., in\n• 265, 287\n• [n.\nIn some cases, the clustering fails at the task of language segmentation, as in the\ncase of the various English–French texts and the English–German example with the\nGerman inclusion. We can thus say that the surface structure or morphology, or in\nother words the basis from which we can extract features, is not suﬃcient to deduce\nrelevant information about ‘language’.\nWhen there are more than two languages that are to be separated, the cluster-\ning also does not work well. Indeed, the most dissimilar objects are separated ﬁrst.\nIn the case of English–Spanish–Arabic, the Arabic part is separated ﬁrst, as well as\nwords with diacritics, while English and Spanish words without diacritics are thrown\ntogether. Subsequent runs show no improvement of the clustering concerning the\nseparation of English and Spanish.\nIn the case of German–Finnish–Turkish, the clustering algorithm seems to cluster\nout Turkish ﬁrst, followed by Finnish. e results are however much less clear-cut\nthan for English–Spanish–Arabic.\n60\n6.4\nLanguage model induction\ne language model induction does not seem to work very well on the Latin script data.\nere are almost only impure clusters, containing more than one language. However,\nthe approach consistently outperforms the clustering approach when we look at the\nF5 score. For the English–French data set, the clustering approach even outperforms\nthe n-gram language model approach. Indeed, the French words are relatively well\nseparated from the English text, with the exception of ‘sucré’, which is still thrown\ntogether with English words.\nLatin script: English–Fren\n• both, “so”, in, English, although, their, is, is, the, opposite, of, “rough”, or, is,\nthe, opposite, of, sweet, only, for, wines, (otherwise, is\n• mou, :, mou, but\n• doux,\n• Doux, (rugueux), Doux\n• while\n• “hard”., used).,\n• translate, as, meaning, very, diﬀerent., ”coarse”, can, also, mean, almost,sucré,\nIn contrast, the approach works well on the mixed script data. Indeed, we achieve\na good separation of the languages by script. However, when there are also Latin\nbased scripts, we encounter the same problems as mentioned above with rather modest\nresults. For example, for the English–Greek text, the approach separates out the Greek\ncharacter words but it fails to separate transliterated Greek and English. Also, for the\nEnglish–Spanish–Arabic text, Arabic is separated out, but English and Spanish are not\nseparated well.\nOne interesting observation can be made in the case of the English–Chinese text.\ne Chinese characters have been isolated, but the Pinyin transcription is thrown to-\ngether with the Chinese characters. Based on the prior observations, this is rather\nunexpected. is raises the question of whether Pinyin ought to be clustered out, or\nclustered together with English or Chinese.\nAgain, the language model induction approach outperforms the clustering approach,\nand also the n-gram language model approach in the case of the English–Greek text.\nOn the larger Pali dictionary entries, the language model induction approach yields\nacceptable results. On the shorter Pali dictionary entries, the language model induction\napproach yields good results.\n61\ne quite low performance must be blamed on the data. Indeed, the Pali dictionary\ndata contain various problematic characters such as ‘comma/dot and whitespace’ as\none character. On such characters, whitespace tokenization fails, yielding big chunks\nof nonsense tokens. For example, the fourth Pali dictionary entry was split into ﬁve\nchunks (while it might not be displayed as such, all commata and all dots are in fact\nnot followed by whitespace, the whitespace is part of the character,10 hence whitespace\ntokenization fails).\nPali: gūhanā: Chunks\n• Gūhanā，（f.）\n• [abstr．fr．gūhati]=gūhanā\n•（q．v.）\n• Pug．19．Cp．pari°．（Page\n• 253）\nFurthermore, the data contains markup, abbreviations, references, typing mistakes\nand signs such as <-> that are diﬃcult to assign to a language.\nOn the Twier data, the language model induction approach works rather well.\nFor example, on the ﬁrst text, separation is not perfect with the Greek cluster still\ncontaining some English words.\nTwitter 1: English–Greek\n• BUSINESS, EXCELLENCE.\n• Μόλις, ψήφισα, αυτή, τη, λύση, Internet, of, στο, διαγωνισμό\n• ings, IT\nFor the third and fourth text, the approach manages to single out the other-language\ninclusions, but not exclusively. Both times, there is one additional item in the cluster\n(the relevant clusters are marked in red).\n10e comma has the Unicode codepoint U+FF0C (FULLWIDTH COMMA) and the dot has the Uni-\ncode codepoint U+FF0E (FULLWIDTH FULL STOP)\n62\nTwitter 3: Fren–English\n• #FWWC2015\n• breuvages, go\n• Food, Edmonton, to, for, the\n• in, waiting, #bilingualism\n• and, are, ready, just, fans\nTwitter 4: English–Polish\n• comes, from, with, two, crates, of, strawberries, jackets, omg\n• my, dad, poland, and, adidas\n• back, żubrówka\ne approach exceeded expectations on the second and ﬁh Twier text. On the\nsecond text, the ‘French’ cluster does not only contain the French words ‘Demain’ and\n‘par’, but also the French way of notating time ‘18h’.\nTwitter 2: Fren–English\n• Keynote, “e, collective, of, science-publish, or, perish;, it, all, that, counts?”\n• Demain, 18h, par\n• #dhiha6, David\n• @dhiparis, dynamics, is\nOn the ﬁh text, an almost perfect result was achieved, with only one additional\nsubdivision of the ‘English’ cluster.\nTwitter 5: Transliterated Amharic–English\n• (coﬀee\n• bread). is, our\n• Buna, dabo, naw\n63\nIt seems that the language model approach does not work very well on longer texts,\nespecially on longer texts in Latin-based scripts, with the chosen parameter set; still,\nthe approach outperforms the clustering approach and achieves scores in the vicinity\nof the scores achieved with the supervised trained n-gram language model approach.\nOn mixed script texts, the approach consistently outperforms the clustering approach\nand we also reach scores in the vicinity of the scores achieved with the supervised\ntrained n-gram language model approach.\nMoreover, on short texts, the approach works rather well. We succeed in outper-\nforming the supervised trained n-gram language model approach on a number of texts,\nand we achieve scores close to the scores achieved with the supervised trained n-gram\nlanguage model approach.\nAlthough the language model induction approach tends to generate too many clus-\nters, it also generally succeeds at separating the languages involved.\n6.5\nScores\nOf the scores I used for evaluation purposes, it seems that a combination of a high\nRand Index and a high F5 score indicate a good language segmentation. A high F5\nscore alone is not signiﬁcant. For example, the clustering algorithm achieves an F5\nscore of 0.7215 on ‘Twier 3’. is score looks good, but the Rand Index score is at\n0.4571, and the segmentation is not good.\nTwitter 3: Cluster analysis\n• Edmonton, Food\n• go, in, to\n• and, are, breuvages, fans, for, just, ready, the, waiting\nSimilarly, a high Rand Index score alone is not signiﬁcant. For example, the clus-\ntering algorithm achieves a Rand Index score of 0.6738 on the ‘Pali 2’ text, but the F5\nscore is at 0.3825 and the clustering is not good.\nPali 2: Cluster analysis\n• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., <smallcaps>i.</smallcaps>, <smallcaps>v.</smallcaps>, =, A, M, ag., fr., in,\nl., v.\n• 265, 287\n• [n.\n64\n7\nConclusion\nIn this thesis, I have asked the question of whether unsupervised approaches to lan-\nguage segmentation perform beer on short and diﬃcult texts than supervised ap-\nproaches by overcoming some of the diﬃculties associated with supervised approaches,\nsuch as the need for (enough and adequate)11 training data, the language-speciﬁcity of\nthe language model or the inﬂexibility of trained language models when it comes to\nspelling variation and abbreviations, unless the training data also contained spelling\nvariation and abbreviations.\nI have given an overview over related work, presenting supervised approaches that\nhave been used in monolingual language identiﬁcation and the amelioration of such\napproaches through unsupervised approaches such as clustering.\nUnfortunately, the body of literature covering the topic of language segmentation\nis sparse. e work by Yin et al. (2007) and the work by Seldin et al. (2001) are closest\nin topic to this thesis. However, Yin et al. (2007) concern themselves with spoken\nlanguage, with requires a diﬀerent approach than dealing with wrien language. As I\nconcentrated on wrien language, their work was not conducive to this thesis.\nIn contrast, Seldin et al. (2001) present a work that looks promising. ey present\na system that ﬁnds language borders in a text with great accuracy using unsuper-\nvised algorithms. However, they restrict their algorithm in such a way that switching\nlanguage models aer each word is disallowed. us, they are unable to detect single-\nword inclusions and cannot handle situations where the language switches every word,\nas has been shown to occur in the test data used in section 4.\nAnother major drawback of the approach is that it also needs longer fragments of\nmonolingual text and an overall longer text. Hence, their approach would not work\nwell on short texts, if at all.\nNext, I have presented the theoretical foundations of a supervised n-gram language\nmodel approach and an unsupervised clustering approach. Finally, I have introduced a\nweakly supervised n-gram language model inducing approach devised by myself. All\nof these approaches can be used for language segmentation. In order to test how well\nthe diﬀerent approaches perform on diﬀerent text types, I have performed experiments.\nSection 4 presents the experiments made. I have ﬁrst compiled a small corpus\nof texts ranging from longer texts with clearly separated languages to one-sentence\nTwier messages containing foreign language inclusions. I have also included a set\nof dictionary entries from the Pali dictionary by the Pali Text Society. Indeed, these\nentries contain a lot of diﬀerent languages and abbreviations, and (unfortunately) are\nnot consistently formaed.\nI have then presented my implementations of the supervised and weakly super-\n11e question of what is to be considered ‘enough’ or ‘adequate’ is another point of contention; the\ndata always inﬂuences the resulting models.\n65\nvised approaches and the choice of the unsupervised clustering algorithms. en, I\nhave presented the results of their application to the data.\nIt can be said that the supervised approach works reasonably well. e drawbacks\nare that the approach needs training data to train the models on. e problems of the\ntraining data and its inﬂuence on the models have been raised more than once.\ne supervised approach failed for non-whitespace scripts. e models would have\nto be adapted for non-whitespace scripts, introducing more complexity. Also, the\ntraining and test texts would have to be split in meaningful ways, introducing the\nneed for a vast array of language-speciﬁc text spliers, should the approach work on\na wide range of languages.\ne unsupervised approach generally succeeded in separating languages by script\nwhen diﬀerent scripts were involved. Other than that, it seems that the chosen mor-\nphological features, or possibly morphological features in general, are insuﬃcient for\nthe algorithm to separate languages eﬀectively.\ne weakly supervised approach worked well on short texts and on diﬃcult short\ntexts, but less well on long texts, while still outperforming the clustering approach\non long texts. e approach consistently outperforms the clustering approach and\nreaches scores in the vicinity of the scores achieved by the supervised approach, even\nsurpassing the supervised approach in some cases. ese results are promising, but\nmore thorough investigations have to be undertaken.\nIn conclusion, it can be said that some unsupervised (or weakly supervised) ap-\nproaches can perform beer on the task of language segmentation on diﬃcult and\nshort texts. e presented weakly supervised approach does not only outperform the\nunsupervised clustering approach, it also achieves scores comparable to the scores\nachieved with the supervised approach.\nFuture work could concentrate on the reduction of the number of generated clus-\nters, ideally geing down to one cluster per language; it would also be thinkable to\nprevent overly frequent language model switching by taking a word’s context into\naccount. Finally, the parameters could conceivably be adapted automatically. With\nan increased interest in the area of multilingual text processing lately, the emergence\nand evolution of the texts themselves will inﬂuence the direction of the work in that\ndirection.\n“Il est venu le temps des cathédrales\nle monde est entré\ndans un nouveau millénaire\nL’homme a voulu monter vers les étoiles\nécrire son histoire\ndans le verre ou dans la pierre”\n— Gringoire\n66\nReferences\nAbeel, T., de Peer, Y. V., and Saeys, Y. (2009). Java-ML: A Machine Learning Library.\nJournal of Machine Learning Research, pages 931–934.\nAchtert, E., Kriegel, H., Schubert, E., and Zimek, A. (2013). Interactive data mining\nwith 3D-parallel-coordinate-trees. In Proceedings of the ACM SIGMOD International\nConference on Management of Data, SIGMOD 2013, New York, NY, USA, June 22-27,\n2013, pages 1009–1012.\nAlex, B. (2005). An unsupervised system for identifying English inclusions in German\ntext. In Proceedings of the 43rd Annual Meeting of the Association for Computational\nLinguistics (ACL 2005), Student Research Workshop, pages 133––138. Association for\nComputational Linguistics.\nAlex, B. (2006). Integrating language knowledge resources to extend the English inclu-\nsion classiﬁer to a new language. In Proceedings of the 5th International Conference\non Language Resources and Evaluation (LREC). European Language Resources Asso-\nciation.\nAlex, B. (2007). Automatic detection of English inclusions in mixed-lingual data with an\napplication to parsing. PhD thesis, University of Edinburgh.\nAlex, B., Dubey, A., and Keller, F. (2007). Using Foreign Inclusion Detection to Improve\nParsing Performance. In EMNLP-CoNLL, pages 151–160.\nAlex, B. and Onysko, A. (2010). Zum Erkennen von Anglizismen im Deutschen: der\nVergleich von einer automatisierten mit einer manuellen Erhebung. In Scherer, C.\nand Holler, A., editors, Strategien der Integration und Isolation nicht-nativer Einheiten\nund Strukturen, pages 223–239. de Gruyter.\nBegleiter, R., El-Yaniv, R., and Yona, G. (2004).\nOn prediction using variable order\nMarkov models. Journal of Artiﬁcial Intelligence Research, pages 385–421.\nBiemann, C. (2006). Chinese whispers: an eﬃcient graph clustering algorithm and\nits application to natural language processing problems. In Proceedings of the ﬁrst\nworkshop on graph based methods for natural language processing, pages 73–80. As-\nsociation for Computational Linguistics.\nBrants, S., Dipper, S., Hansen, S., Lezius, W., and Smith, G. (2002). e TIGER treebank.\nIn Proceedings of the workshop on treebanks and linguistic theories, volume 168.\nBürgisser, P., Clausen, M., and Shokrollahi, M. A. (1997). Algebraic complexity theory,\nvolume 315. Springer.\n67\nCarter, D. (1994). Improving language models by clustering training sentences. In\nProceedings of the fourth conference on Applied natural language processing, pages\n59–64. Association for Computational Linguistics.\nCavnar, W. B. and Trenkle, J. M. (1994). N-gram-based text categorization. In Pro-\nceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information\nRetrieval, pages 161–175.\nChen, S. F. and Goodman, J. (1996). An empirical study of smoothing techniques for\nlanguage modeling. In Proceedings of the 34th annual meeting on Association for Com-\nputational Linguistics, pages 310–318. Association for Computational Linguistics.\nDreyfuss, E., Goodfellow, I., and Baumstarck, P. (2007). Clustering Methods for Improv-\ning Language Models.\nDubes, R. C. (1987). How many clusters are best?-an experiment. Paern Recognition,\n20(6):645–663.\nDunning, T. (1994). Statistical Identiﬁcation of Language. Computing Research Labo-\nratory, New Mexico State University.\nGale, W. and Sampson, G. (1995). Good-turing smoothing without tears. Journal of\nantitative Linguistics, 2(3):217–237.\nGao, J., Goodman, J., Miao, J., et al. (2001). e use of clustering techniques for language\nmodeling–application to Asian languages. International Journal of Computational\nLinguistics and Chinese Language Processing, 6(1):27–60.\nGoldberg, D. (1991). What every computer scientist should know about ﬂoating-point\narithmetic. ACM Computing Surveys (CSUR), 23(1):5–48.\nGoodman, J. and Gao, J. (2000). Language model size reduction by pruning and clus-\ntering. In INTERSPEECH, pages 110–113.\nGoodman, J. T. (2001). A bit of progress in language modeling. Computer Speech and\nLanguage, 15(4):403–434.\nGrefenstee, G. (1995). Comparing two language identiﬁcation schemes. In Proceedings\nof the 3rd International conference on Statistical Analysis of Textual Data. JADT 1995.\nGrünwald, P. D. (2007). e minimum description length principle. MIT press.\nGuthrie, D., Allison, B., Liu, W., Guthrie, L., and Wilks, Y. (2006). A closer look at\nskip-gram modelling. In Proceedings of the 5th international Conference on Language\nResources and Evaluation (LREC-2006), pages 1–4.\n68\nHall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., and Wien, I. H. (2009).\ne WEKA Data Mining Soware: An Update. SIGKDD Explorations, 11.\nJain, A. K., Murty, M. N., and Flynn, P. J. (1999). Data clustering: a review. ACM\ncomputing surveys (CSUR), 31(3):264–323.\nJain, N. and Bhat, R. A. (2014). Language Identiﬁcation in Code-Switching Scenario. In\nProceedings of the Conference on Empirical Methods on Natural Language Processing,\npages 87–93.\nJurafsky, D. and Martin, J. H. (2000). Speech and language processing. An Introduction\nto Natural Language Processing, Computational Linguistics, and Speech Recognition.\nPearson Education India, 2nd edition.\nKatz, S. (1987). Estimation of probabilities from sparse data for the language model\ncomponent of a speech recognizer. Acoustics, Speech and Signal Processing, IEEE\nTransactions on, 35(3):400–401.\nKing, B. and Abney, S. P. (2013). Labeling the Languages of Words in Mixed-Language\nDocuments using Weakly Supervised Methods. In Proceedings of the Conference of\nthe North American Chapter of the Association for Computational Linguistics – Human\nLanguage Technologies, pages 1110–1119.\nLiu, H. and Cong, J. (2013). Language clustering with word co-occurrence networks\nbased on parallel texts. Chinese Science Bulletin, 58(10):1139–1144.\nLogan, B. et al. (2000). Mel frequency cepstral coeﬃcients for music modeling. In\nProceedings of the 1st International Symposium on Music Information Retrieval (ISMIR).\nLui, M., Lau, J. H., and Baldwin, T. (2014). Automatic detection and language identiﬁ-\ncation of multilingual documents. Transactions of the Association for Computational\nLinguistics, 2:27–40.\nManning, C. D., Raghavan, P., and Schütze, H. (2008). Introduction to information re-\ntrieval, volume 1. Cambridge University Press.\nManning, C. D. and Schütze, H. (1999). Foundations of statistical natural language pro-\ncessing. MIT press.\nMarsland, S. (2003). Novelty detection in learning systems. Neural computing surveys,\n3(2):157–195.\nMendizabal, I., Carandell, J., and Horowitz, D. (2014). TweetSafa: Tweet language\nidentiﬁcation. TweetLID @ SEPLN.\n69\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Eﬃcient estimation of word\nrepresentations in vector space. In Proceedings of the International Conference on\nLearning Representations (ICLR) 2013.\nNey, H., Essen, U., and Kneser, R. (1994). On structuring probabilistic dependences in\nstochastic language modelling. Computer Speech & Language, 8(1):1–38.\nPelleg, D. and Moore, A. W. (2000). X-means: Extending K-means with Eﬃcient Es-\ntimation of the Number of Clusters. In Proceedings of the Seventeenth International\nConference on Machine Learning (ICML 2000), pages 727–734.\nPereira, F., Tishby, N., and Lee, L. (1993). Distributional clustering of english words. In\nProceedings of the 31st annual meeting on Association for Computational Linguistics,\npages 183–190. Association for Computational Linguistics.\nPorta, J. (2014). Twier Language Identiﬁcation using Rational Kernels and its potential\napplication to Sociolinguistics. TweetLID @ SEPLN.\nRavi, S., Vassilivitskii, S., and Rastogi, V. (2014). Parallel Algorithms for Unsupervised\nTagging. Transactions of the Association for Computational Linguistics, 2:105–118.\nRon, D., Singer, Y., and Tishby, N. (1996). e power of amnesia: Learning probabilistic\nautomata with variable memory length. Machine learning, 25(2-3):117–149.\nSchölkopf, B., Williamson, R. C., Smola, A. J., Shawe-Taylor, J., and Pla, J. C. (1999).\nSupport vector method for novelty detection. In Advances in Neural Information\nProcessing Systems (NIPS), volume 12, pages 582–588.\nSeldin, Y., Bejerano, G., and Tishby, N. (2001). Unsupervised sequence segmentation\nby a mixture of switching variable memory Markov sources. In Proceedings of the\nSeventeenth International Conference on Machine Learning (ICML), pages 513–520.\nSolorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M., Gohneim, M., Hawwari, A., Al-\nGhamdi, F., Hirschberg, J., Chang, A., et al. (2014). Overview for the First Shared Task\non Language Identiﬁcation in Code-Switched Data. In Proceedings of the Conference\non Empirical Methods on Natural Language Processing, pages 62–72.\nTaylor, D. (2015).\nGraphing the distribution of English leers towards the be-\nginning, middle or end of words.\nhttp://www.prooffreader.com/2014/05/\ngraphing-distribution-of-english.html.\ne Unicode Consortium (2014).\ne Unicode Standard.\nhttp://unicode.org/\nstandard/standard.html. [Online; accessed 21-July-2015].\n70\nUszkoreit, J. and Brants, T. (2008). Distributed word clustering for large scale class-\nbased language modeling in machine translation. In Proceedings of the 46th Annual\nMeeting of the Association for Computational Linguistics, pages 755–762.\nWagner, S. and Wagner, D. (2007). Comparing clusterings: an overview. Universität\nKarlsruhe, Fakultät für Informatik Karlsruhe.\nYamaguchi, H. and Tanaka-Ishii, K. (2012). Text segmentation by language using mini-\nmum description length. In Proceedings of the 50th Annual Meeting of the Association\nfor Computational Linguistics, pages 969–978. Association for Computational Lin-\nguistics.\nYin, B., Ambikairajah, E., and Chen, F. (2007). Hierarchical language identiﬁcation\nbased on automatic language clustering. In INTERSPEECH, pages 178–181.\nYuan, L. (2006). Language model based on word clustering. In Proceedings of the 20th\nPaciﬁc Asia Conference on Language, Information and Computation, pages 394–397.\nZubiaga, A., San Vicente, I., Gamallo, P., Pichel, J. R., Alegria, I., Aranberri, N., Ezeiza,\nA., and Fresno, V. (2014). Overview of TweetLID: Tweet language identiﬁcation at\nSEPLN 2014. TweetLID @ SEPLN.\n71\n8\nAppendix\n8.1\nDevelopment data\n8.1.1\nLatin script data\nKarl Marx anses som en af de ﬁre klassiske sociologer. Marx er epokegørende for den\nhistoriske videnskab. Og Marx spillede en vigtig rolle for den samtidige og eerføl-\ngende arbejderbevægelse.\n1891, nach einer Tuberkuloseerkrankung Hopes, eröﬀnete das Ehepaar ein mod-\nernes Lungensanatorium in Nordrach im Schwarzwald, das sie bis 1893 gemeinsam\nührten. 1895 wurde die Ehe geschieden.\nSources:\nhps://da.wikipedia.org/wiki/Karl_Marx\nhps://de.wikipedia.org/wiki/Hope_Bridges_Adams_Lehmann\n8.1.2\nMixed script data\nCapitalism is an economic system and a mode of production in which trade, industries,\nand the means of production are largely or entirely privately owned. Private ﬁrms and\nproprietorships usually operate in order to generate proﬁt, but may operate as private\nnonproﬁt organizations.\nاو را ولودیا خطاب می​کردند که مخفف ولادمیر است نام اصلی او ولادمیر ایلیچ اولیانوف بود ولی در دنیا به اسم لنین مشهور شد. ولودیا\nیعنی یک سال قبل از کمون پاریس، در یک خانواده مرفه در سیمبریسک۱۸۷۰سومین فرزند از شش فرزند خانواده اولیانوف بود که در سال\nدر ساحل رود ولگا که در ٓان زمان شهرکی بیش نبود ولی بعدها به صورت شهر بزرگی به نام اولیاء نوفسک در ٓامد متولد گردید. پدرش یک\nخرده بورژوای لیبرال و معلم ریاضی و مادرش دختر یک پزشک المانی بود وبه همین جهت لنین در تمام مدت عمر به المانیها و طرز تفکر\nالمانی که مارکس مولود ٓان بود به دیده اغماض می​نگریست. ولودیا در دبیرستان شاگرد خوبی بود و قوه استدلال درخشانی داشت ولی در\nعین حال بچه​ای موذی بود.\nSources:\nhps://en.wikipedia.org/wiki/Capitalism\nhps://fa.wikipedia.org/wiki/ولادیمیر_لنین\n8.1.3\nTwitter data\nTwitter 1\n»Fallo ergo sum«: On being wrong.\nSource:\nRoland Hieber (daniel_bohrer). “»Fallo ergo sum«: On being wrong.”. 26 July 2015,\n16:47. Tweet.\n72\nTwitter 2\nMusic for Airports > le piano en libre-accès dans l’aéroport Charles-de-\nGaulles\nSource:\nYannick Rochat (yrochat). “Music for Airports > le piano en libre-accès dans l’aéroport\nCharles-de-Gaulles”. 26 July 2015, 18:12. Tweet.\n8.1.4\nPali dictionary data\nAll entries have been taken from the Pali Text Society’s Pali-English dictionary (T. W.\nRhys Davids, William Stede, editors, e Pali Text Society’s Pali–English dictionary.\nChipstead: Pali Text Society, 1921–5). 8 parts [738 pp.].)\nHambho\nHambho，（indecl.）[haṁ+bho] a particle expressing surprise or haughti-\nness J.I，184，494．See also ambho．（Page 729）\nUssada\nUssada，[most likely to ud + syad；see ussanna]：this word is beset with\ndiﬃculties，the phrase sa-ussada is applied in all kinds of meanings，evidently the\nresult of an original application & meaning having become obliterated．sa° is taken\nas *sapta（seven）as well as *sava（being），ussada as prominence，protuberance，\nfulness，arrogance．e meanings may be tabulated as follows：（1）prominence（cp．\nSk．utsedha），used in characterisation of the Nirayas，as “projecting，prominent\nhells”，ussadanirayā （but see also below 4）J．I，174；IV，3，422 （pallaṅkaṁ，\nv．l．caturassạṁ，with four corners）；V，266．– adj．prominent A．13 （tej-\nussadehi ariyamaggadhammehi，or as below 4?）．– 2．protuberance，bump，swelling\nJ．IV，188；also in phrase saussada having 7 protuberances，a qualiﬁcation of the\nMahāpurisa D．III，151 （viz．on both hands，feet，shoulders，and on his back）．\n– 3．rubbing in，anointing，ointment；adj．anointed with （-°），in candan° J．III，\n139；IV，60；．1，267；Vv 537；DhA．I，28；VvA．237．– 4．a crowd adj．full\nof （-°）in phrase saussada crowded with（human beings）D．I，87 （cp．DA．I，\n245：aneka-saa-samākiṇṇa；but in same sense BSk．sapt-otsada Divy 620，621）；Pv\nIV．18 （of Niraya = full of beings，expld．by saehi ussanna uparûpari nicita PvA．\n221．– 5．qualiﬁcation，characteristic，mark，aribute，in catussada “having the\nfour qualiﬁcations （of a good village）”J．IV，309 （viz．plenty of people，corn，\nwood and water C．）．e phrase is evidently shaped aer D．I，87 （under 4）．As\n“preponderant quality，characteristic”we ﬁnd ussada used at Vism．103 （cf．Asl．\n267）in combns．lobh°，dos°，moh°，alobh° etc．（quoted from the“Ussadakiana”），\nand similarly at VvA．19 in Dhammapāla’s deﬁnition of manussa（lobh’ādīhi alobh’\nādīhi sahitassa manassa ussannatāya manussā），viz．saā manussa-jātikā tesu lobh’\n‹-› ādayo alobh’ādayo ca ussadā．– 6．（metaph．）self-elevation，arrogance，conceit，\nhaughtiness Vin．I，3；Sn．515，624（an° = taṇhā-ussada-abhāvena SnA 467），783\n73\n（expld．by Nd1 72 under formula saussada；i．e．showing 7 bad qualities，viz．rāga，\ndosa，moha etc．），855．– See also ussādana，ussādeti etc．（Page 157）\n8.2\nTest data\n8.2.1\nLatin script data\nEnglish - German\ne German word Nabelschau means ”navel-gazing” or ”staring\nat your navel”. But in this case, it doesn’t refer to anyone else’s belly buon – just your\nown.\nSource:\nGlass, Nicole (2015): ”German Missions in the United States - Word of the Week”.\nGermany.info.\nEnglish - Fren\ndoux, mou : both translate as ”so” in English, although their mean-\ning is very diﬀerent. Doux is the opposite of ”rough” or ”coarse” (rugueux), while mou\nis the opposite of ”hard”. Doux can also mean sweet, but almost only for wines (oth-\nerwise sucré is used).\nSource:\nMaciamo, (2015): ”French words and nuances that don’t exist in English”. Eupedia.\nEnglish - Transliterated Greek\ne Greek language distinguishes at least four dif-\nferent ways as to how the word love is used. Ancient Greek has four distinct words for\nlove: agápe, éros, philía, and storgē. However, as with other languages, it has been his-\ntorically diﬃcult to separate the meanings of these words when used outside of their\nrespective contexts. Nonetheless, the senses in which these words were generally used\nare as follows.\nSource:\nhps://en.wikipedia.org/wiki/Greek_words_for_love\nItalian - German\nMilano ne custodisce l’esempio più struggente: quel Cenacolo\nche il vinciano aﬀrescò con amore, cura e rivoluzionaria psicologia (il Giuda non vie-\nne privato dell’aureola, ma si condanna da solo, con la consapevolezza del peccato)\ncominciò subito ad autodistruggersi, con un cancro che solo un lunghissimo restauro\nha di recente arginato.\nKaum eine Woche vergeht, in der es keine neue Studie, Umfrage oder Warnung\nzum ema Fachkräemangel in Deutschland gibt.\nCerto, lo faceva per deﬁnire le idee, ma anche perché consapevole che le intuizioni\nsono periture, che la vita stessa va caurata in qualche modo.\n74\nDabei mehren sich letzter Zeit auch Stimmen, die Entwarnung geben. So kam\njüngst eine Studie des Stierverbands ür die Deutsche Wissenschazu dem Ergebnis,\ndass ”ein allgemeiner Fachkräemangel in den MINT-Berufen eher nicht mehr” drohe.\nCome anche i riccioli del Baista richiamano il movimento delle acque, moto che\npoi Leonardo studierà più approfonditamente a Venezia, nelle ricerche sui bacini in\nchiave di difesa anti-Turchi. E si vada alla bellissima Annunciazione, con un occhio\naento alle ali dell’angelo: la delicatezza delle punte all’insù che cosa sono se non\nil barbaglio di un sogno che lo ossessionava da anni, ovvero quello di volare?\nIst das seit Jahren angemahnte Szenario vom drohenden Fachkräemangel bei In-\ngenieuren und Naturwissenschalern also nur ein Mythos?\nSource:\nStalinski, Sandra (2015): ”Ingenieure: Mythos Fachkräemangel?”. tagesschau.de.\nScorranese, Roberta (2015): ”Nelle grandi opere il racconto soﬀerto della natura mor-\ntale”. Archiviostorico.corriere.it.\nGerman - Finnish - Turkish\nDer Sommer ist die wärmste der vier Jahreszeiten in der\ngemäßigten und arktischen Klimazone. Je nachdem, ob er gerade auf der Nord- oder\nSüdhalbkugel herrscht, spricht man vom Nord- oder Südsommer. Der Nordsommer\nﬁndet gleichzeitig mit dem Südwinter sta.\nKesä eli suvi on vuodenaika kevään ja syksyn välissä. Kesä on vuodenajoista läm-\npimin, koska maapallo on silloin kallistunut niin, eä aurinko säteilee maan pinnalle\njyrkemmässä kulmassa kuin muina vuodenaikoina. Pohjoisella pallonpuoliskolla kesä-\nkuukausiksi lasketaan tavallisesti kesä-. heinä- ja elokuu, eteläisellä pallonpuoliskolla\njoulu-, tammi- ja helmikuu.\nYaz, en sıcak mevsimdir. Kuzey Yarım Küre’de en uzun günler yazda gerçekleşir.\nDünya ısıyı depo eiği için en sıcak günler genellikle yaklaşık iki ay sonra ortaya\nçıkar. Sıcak günler Kuzey Yarım Küre’de 21 Haziran ile 22 Eylül arasında, Güney Yarım\nKüre’de ise 22 Aralık ile 21 Mart arasındadır.\nSource:\nhps://ﬁ.wikipedia.org/wiki/Kesä\nhps://de.wikipedia.org/wiki/Sommer\nhps://tr.wikipedia.org/wiki/Yaz\n8.2.2\nMixed script data\nGreek - Russian\nΗ ελληνική γλώσσα είναι μία από τις ινδοευρωπαϊκές γλώσσες.\nΑποτελεί το μοναδικό μέλος ενός ανεξάρτητου κλάδου της ινδοευρωπαϊκής οικο-\nγένειας γλωσσών. Ανήκει επίσης στον βαλκανικό γλωσσικό δεσμό. Στην ελληνική\nγλώσσα, έχουμε γραπτά κείμενα από τον 15ο αιώνα π.Χ. μέχρι σήμερα.\nНа греческом языке на всех этапах его существования была создана богатей-\nшая литература. В Римской империи знание греческого языка считалось обяза-\n75\nтельным для всякого образованного человека. В латинском языке присутствует\nбольшое количество греческих заимствований, а в греческом —значительное\nколичество латинских и романских слов. В новое время древнегреческий язык\nстал (наряду с латинским) источником создания новых научных и технических\nтерминов (так называемая международная лексика). В русский язык греческие\nслова проникали в основном двумя путями —через международную лексику и\nчерез церковнославянский язык.\nSource:\nhps://el.wikipedia.org/wiki/Ελληνική_γλώσσα\nhps://ru.wikipedia.org/wiki/Греческий_язык\nEnglish - Greek - Transliterated Greek\nAgápe (ἀγάπη agápē) means ”love: esp.\nbrotherly love, charity; the love of God for man and of man for God.” Agape is used\nin the biblical passage known as the ”love chapter,” 1 Corinthians 13, and is described\nthere and throughout the New Testament as brotherly love, aﬀection, good will, love,\nand benevolence. Whether the love given is returned or not, the person continues to\nlove (even without any self-beneﬁt). Agape is also used in ancient texts to denote feel-\nings for one’s children and the feelings for a spouse, and it was also used to refer to\na love feast. It can also be described as the feeling of being content or holding one in\nhigh regard. Agape is used by Christians to express the unconditional love of God for\nhis children. is type of love was further explained by omas Aquinas as ”to will\nthe good of another.”\nÉros (ἔρως érōs) means ”love, mostly of the sexual passion.” e Modern Greek\nword ”erotas” means ”intimate love.” It can also apply to dating relationships as well as\nmarriage. Plato reﬁned his own deﬁnition: Although eros is initially felt for a person,\nwith contemplation it becomes an appreciation of the beauty within that person, or\neven becomes appreciation of beauty itself. Plato does not talk of physical araction as\na necessary part of love, hence the use of the word platonic to mean, ”without physical\naraction.”\nIn the Symposium, the most famous ancient work on the subject, Plato has Socrates\nargue that eros helps the soul recall knowledge of beauty, and contributes to an under-\nstanding of spiritual truth, the ideal ”Form” of youthful beauty that leads us humans\nto feel erotic desire – thus suggesting that even that sensually based love aspires to\nthe non-corporeal, spiritual plane of existence; that is, ﬁnding its truth, just like ﬁnd-\ning any truth, leads to transcendence. Lovers and philosophers are all inspired to seek\ntruth through the means of eros.\nSource:\nhps://en.wikipedia.org/wiki/Greek_words_for_love\n76\nEnglish - Spanish - Arabic\nA black ribbon is a symbol of remembrance or mourn-\ning. Wearing or displaying a black ribbon has been used for POW/MIA remembrance,\nmourning tragedies or as a political statement.\nEl crespón negro o lazo negro es un símbolo utilizado por personas, estados, so-\nciedades y organizaciones, representando un sentimiento político-social en señal de\nduelo.\nالرمز يعني الرسم الذي يعبر عن شيء معين وعموما فٔان العلامة ينبغي ٔان تنقل رسالتها بنظرة واحدة دون الحاجة لاية كلمات و من\nالمعروف ٔان قدماء المصريين والٔاغريق ٔاستخدموا العلامات ولكن ٔاكثر من استخدم العلامات هم\nSource:\nhps://es.wikipedia.org/?title=Lazo_negro\nhps://en.wikipedia.org/wiki/Black_ribbon\nhps://ar.wikipedia.org/wiki/رمز\nEnglish - Chinese - (Pinyin)\ne Chinese word for ”crisis” (simpliﬁed Chinese: 危\n机; traditional Chinese: 危機; pinyin: wēijī) is frequently invoked in Western\nmotivational speaking because the word is composed of two Chinese characters that\ncan represent ”danger” and ”opportunity”. Some linguists have criticized this usage\nbecause the component pronounced jī (simpliﬁed Chinese: 机; traditional Chinese:\n機) has other meanings besides ”opportunity”. In Chinese tradition, certain numbers\nare believed by some to be auspicious (吉利) or inauspicious (不利) based on the\nChinese word that the number name sounds similar to. e numbers 0, 6, 8, and 9 are\nbelieved to have auspicious meanings because their names sound similar to words\nthat have positive meanings.\nSource:\nhps://en.wikipedia.org/w/index.php?title=Chinese_word_for_”crisis”\nUkrainian - Russian\nВіддавна на території України існували держави скіфів,\nсарматів, готів та інших народів, але відправним пунктом української\nдержавності й культури вважається Київська Русь 9—13 століття.\nНа юге омывается водами Чёрного и Азовского морей. Имеет сухопутную\nграницу с Россией, Белоруссией, Польшей, Словакией, Венгрией, Румынией и\nМолдавией.\nSource:\nhps://uk.wikipedia.org/wiki/Україна\nSurgut-safari.ru, (2015): ”Страны - Safari Tour”.\n77\n8.2.3\nTwitter data\nTweet 1: Greek – English\nΜόλις ψήφισα αυτή τη λύση Internet of ings, στο\nδιαγωνισμό BUSINESS IT EXCELLENCE.\nSource:\nGaloTyri. ”Μόλις ψήφισα αυτή τη λύση Internet of ings, στο διαγωνισμό\nBUSINESS IT EXCELLENCE.”. 19 June 2015, 12:06. Tweet\nTweet 2: English – Fren\nDemain #dhiha6 Keynote 18h @dhiparis “e collective\ndynamics of science-publish or perish; is it all that counts?” par David @chavalarias\nSource:\nClaudine Moulin (ClaudineMoulin). ”Demain #dhiha6 Keynote 18h @dhiparis ”e\ncollective dynamics of science-publish or perish; is it all that counts?” par David\n@chavalarias”. 10 June 2015, 17:35. Tweet.\nTweet 3: English – Fren\nFood and breuvages in Edmonton are ready to go, just\nwaiting for the fans #FWWC2015 #bilingualism\nSource:\nHBS (HBS_Tweets). ”Food and breuvages in Edmonton are ready to go, just waiting\nfor the fans #FWWC2015 #bilingualism”. 6 June 2015, 23:29. Tweet.\nTweet 4: English – Polish\nmy dad comes back from poland with two crates of\nstrawberries, żubrówka and adidas jackets omg\nSource:\nkatarzyne (wifeyriddim). ”my dad comes back from poland with two crates of\nstrawberries, żubrówka and adidas jackets omg”. 8 June 2015, 08:49. Tweet.\nTweet 5: Transliterated Amharic – English\nBuna dabo naw (coﬀee is our bread).\nSource:\neCodeswitcher. ”Buna dabo naw (coﬀee is our bread).”. 9 June 2015, 02:12. Tweet.\n8.2.4\nPali dictionary data\nAll entries have been taken from the Pali Text Society’s Pali-English dictionary (T. W.\nRhys Davids, William Stede, editors, e Pali Text Society’s Pali–English dictionary.\nChipstead: Pali Text Society, 1921–5. 8 parts [738 pp.].)\nabbha\n(nt.) [Vedic abhra nt. & later Sk. abhra m. ”dark cloud”; Idg. *m̊ bhro, cp. Gr.\n<at>a)fro\\\\s</at> scum, froth, Lat. imber rain; also Sk. ambha water, Gr.\n<at>o)/mbros</at> rain, Oir ambu water]. A (dense & dark) cloud, a cloudy mass A\n<smallcaps>ii.</smallcaps> 53 = Vin <smallcaps>ii.</smallcaps> 295 = Miln 273 in\n78\nlist of to things that obscure moon– & sunshine, viz. <b>abbhaŋ mahikā</b> (mahiyā\nA) <b>dhū- marajo</b> (megho Miln), <b>Rāhu</b> . is list is referred to at SnA\n487 & VvA 134. S <smallcaps>i.</smallcaps> 101 (°sama pabbata a mountain like a\nthunder–cloud); J <smallcaps>vi.</smallcaps> 581 (abbhaŋ rajo acchādesi); Pv\n<smallcaps>iv.</smallcaps> 3 <superscript>9</superscript> (nīl° = nīla–megha PvA\n251). As f. <b>abbhā</b> at Dhs 617 & DhsA 317 (used in sense of adj. ”dull”; DhsA\nexpl <superscript>s.</superscript> by valāhaka); perhaps also in <b>abbhāmaa</b>\n. <br /><b>–kūṭa</b> the point or summit of a storm–cloud 1, 1064; J\n<smallcaps>vi.</smallcaps> 249, 250; Vv 1 <superscript>1</superscript> (=\nvalāhaka–sikhara VvA 12). <b>–ghana</b> a mass of clouds, a thick cloud It 64; Sn\n348 (cp. SnA 348). <b>–paṭala</b> a mass of clouds DhsA 239. <b>–mua</b> free\nfrom clouds Sn 687 (also as abbhāmua Dh 382). <b>–saŋvilāpa</b> thundering S\n<smallcaps>iv.</smallcaps> 289.\nabhijjhitar\n[n. ag. fr. abhijjhita in med. function] one who covets M\n<smallcaps>i.</smallcaps> 287 (T. abhijjhātar, v. l. °itar) = A\n<smallcaps>v.</smallcaps> 265 (T. °itar, v. l. °ātar).\najja\nAjja，& Ajjā （adv.）[Vedic adya & adyā，a + dyā，a° being base of demonstr.\npron. （see a3）and dyā an old Loc. of dyaus （see diva），thus “on this day”]\nto-day，now Sn.75，153，158，970，998；Dh.326；J.I，279；III，425（read bahutaṁ\najjā；not with Kern，Toev. s. v. as “food”）；Pv.I，117 （= idāni PvA.59）；PvA.6，\n23；Mhvs 15，64. ‹-› Freq. in phrase ajjatagge （= ajjato + agge（?）or ajja-tagge，\nsee agga3）from this day onward，henceforth Vin.I，18；D.I，85；DA.I，235.\n–kālaṁ（adv.）this morning J.VI，180；–divasa the present day Mhvs 32，23.\n（Page 10）\ngūhanā\nGūhanā，（f.）[abstr．fr．gūhati]=gūhanā （q．v.）Pug．19．Cp．\npari°．（Page 253）\npacati\nPacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；Obulg．peka to fry，roast，\nLith，kepū bake，Gr．pέssw cook，pέpwn ripe] to cook，boil，roast Vin．IV，264；\nﬁg．torment in purgatory （trs．and intrs．）：Niraye pacitvā aer roasting in N．S．\nII，225，PvA．10，14．– ppr．pacanto tormenting，Gen．pacato （+Caus．\npācayato）D．I，52 （expld at DA．I，159，where read pacato for paccato，by pare\ndaṇḍena pīḷentassa）．– pp．pakka （q．v．）．‹-› Caus．pacāpeti & pāceti （q．v．）．\n– Pass．paccati to be roasted or tormented （q．v．）．（Page 382）\n79\n8.3\nResults\n8.3.1\nN-Gram Language Models\nFor the n-gram language model approach, the identiﬁed language is indicated in\nparentheses. e language abbreviations are:\nAbbreviation\nLanguage\nAR\nArabic\nDE\nGerman\nEL\nGreek\nEN\nEnglish\nES\nSpanish\nFI\nFinnish\nFR\nFrench\nIT\nItalian\nPL\nPolish\nRU\nRussian\nUK\nUkrainian\nTR\nTurkish\nTrAM\nTransliterated Amharic\nTrEL\nTransliterated Greek\nZH\nChinese\nData:\nLatin script: German – English\n• (EN) own., belly, refer, buon, But, it, or, your, at, in, ”staring, anyone, doesn’t,\nelse’s, word, this\n• (FI) –\n• (FR) case, just, means, navel”.\n• (TrAM) e\n• (TrEL) to, German\n• (other) Nabelschau, ”navel-gazing”\n80\nData:\nLatin script: German – Finnish – Turkish\n• (DE) ob, oder, Sommer, und, Nord-, arktischen, der, Der, dem, gemäßigten, mit,\ner, Südsommer., spricht, Jahreszeiten, Südwinter, herrscht, wärmste, vom, die,\nsta., nachdem, auf\n• (EN) ist, Nordsommer, Mart, in\n• (ES) en, depo\n• (FI) joulu-, kevään, suvi, on, eli, vuodenajoista, syksyn, koska, kesä-., kuin, Po-\nhjoisella, man, helmikuu., tammi-, lämpimin, heinä-, niin, maapallo, maan, pin-\nnalle, Kesä, säteilee, tavallisesti, vuodenaika, kallistunut, lasketaan, muina, eiği,\njyrkemmässä, elokuu, välissä., eä, eteläisellä, silloin, ja, kulmassa\n• (FR) vier, Je\n• (PL) aurinko\n• (RU) 22, 21\n• (TR) yaklaşık, ortaya, genellikle, Eylül, Sıcak, çıkar., Yaz, sonra, arasında, Kuzey,\nGüney, Aralık, gerade, ısıyı, gerçekleşir., Küre’de, günler, için, ﬁndet, mevsimdir.,\narasındadır., Haziran, iki, yazda, uzun, ise, ay, sıcak, ile, Yarım, Dünya\n• (TrAM) Der\n• (other) Klimazone., gleichzeitig,kesäkuukausiksi, vuodenaikoina., pallonpuolis-\nkolla,Südhalbkugel\nData:\nLatin script: English – French\n• (EL) ”coarse”\n• (EN) but, both, for, while, wines, almost, sweet, of, although, only, is, ”rough”,\nused)., or, as, meaning, the, in, translate, ”hard”., their, English, also, diﬀerent.,\nvery\n• (ES) can\n• (FI) mean\n• (FR) opposite, Doux, doux, sucré, :\n• (RU) ”so”\n• (TrEL) mou\n• (other) (otherwise, (rugueux)\n81\nData:\nLatin script: English – Transliterated Greek\n• (EN) for, meanings, least, used, been, distinct, love, of, were, are, when, agápe,\nthese, how, and, Greek, word, used., outside, ways, diﬀerent, other, follows.,\nwords, respective, generally, However, is, with, it, at, as, historically, the, in,\nwhich, their\n• (ES) has, separate\n• (FR) language, senses, Ancient, languages, diﬃcult, four\n• (IT) contexts.\n• (TrAM) éros, e, love:\n• (TrEL) to, storgē., philía\n• (other) Nonetheless, distinguishes\nData:\nLatin script: Italian – German\n• (DE) drohe., geben., allgemeiner, Studie, jüngst, ür, Ergebnis, keine, kam, dro-\nhenden, oder, und, letzter, neue, Mythos?, Deutschland, Ist, sich, der, vergeht,\nstudierà, Dabei, Studie, den, dem, auch, Entwarnung, dass, nur, eher, nicht, gibt.,\nUmfrage, Woche, eine, Kaum, Jahren, bei, mehren, Stimmen, Deutsche, das, zum,\nmehr”, angemahnte, ”ein, Zeit, ein, So, vom, zu, die, seit, Warnung, Wissenscha\n• (EL) aﬀrescò\n• (EN) moto, aento, a, in, ad, also\n• (ES) custodisce, cura, subito, Certo, Giuda, lo, del, difesa, con, deﬁnire, restauro,\nse, modo., la, arginato., recente, vada, movimento, Leonardo, Szenario, quel,\ncominciò\n• (FI) va, si, Baista, ema\n• (FR) l’esempio, non, des, acque, perché, un, es, le, sui, condanna\n• (IT) solo, faceva, caurata, chiave, peccato), periture, (il, delicatezza, cancro, pri-\nvato, bellissima, anni, bacini, ovvero, delle, sogno, di, barbaglio, ma, qualche, e,\namore, ricerche, Come, per, richiamano, ne, intuizioni, punte, occhio, struggente:,\nnelle, vita, riccioli, solo, che, volare?, sono, alla, alle, anche, Cenacolo, quello,\ncosa, ali, viene, il, psicologia, vinciano, Venezia\n82\n• (PL) i\n• (TR) ha, più, da\n• (TrAM) Milano, E\n• (TrEL) poi, idee, stessa\n• (other) MINT-Berufen, Fachkräemangel, dell’angelo:, consapevole, anti-Turchi.,\nAnnunciazione, lunghissimo, consapevolezza, ossessionava, dell’aureola, appro-\nfonditamente, autodistruggersi, rivoluzionaria, Stierverbands, all’insù, Natur-\nwissenschalern, Ingenieuren\nData:\nMixed script: Greek – Russian\n• (EL) κείμενα, βαλκανικό, από, το, αιώνα, Αποτελεί, ελληνική, μία, επίσης, στον,\nγλωσσικό, γλωσσών., είναι, Στην, έχουμε, μέλος, ανεξάρτητου, τις, γλώσσες.,\n15ο, Ανήκει, γραπτά, π.Χ., σήμερα., γλώσσα, γλώσσα, κλάδου, οικογένειας, τον,\nτης, δεσμό., μέχρι, μοναδικό, ενός\n• (RU) слов., с, богатейшая, образованного, человека., этапах, значительное,\nзнание, научных, лексика)., называемая, технических, источником, стал,\nлатинских, существования, слова, греческом, всех, —, В, романских, но-\nвых, Римской, и, проникали, в, греческие, терминов, присутствует, грече-\nских, новое, русский, империи, латинском, литература., создана, создания,\nпутями, основном, язык., язык, (так, его, количество, считалось, обязатель-\nным, время, двумя, была, греческого, большое, языке, языка\n• (TrAM) Η\n• (UK) лексику, (наряду, через, всякого, а, На, для, на\n• (other) ινδοευρωπαϊκές, ινδοευρωπαϊκής,латинским), международную, меж-\nдународная, церковнославянский, заимствований, древнегреческий\nData:\nMixed script: English – Greek\n• (DE) Symposium, Modern, being, felt\n• (EL) ”Form”\n83\n• (EN) sensually, platonic, for, holding, existence;, reﬁned, its, explained, arac-\ntion, of, (even, are, spiritual, given, refer, Agape, beauty, or, araction.”, like,\nwithout, not, further, will, own, love, knowledge, will, one’s, most, use, ex-\npress, is, another.”, e, leads, truth, suggesting, dating, relationships, in-\nspired, ”love, mostly, hence, deﬁnition:, regard., appreciation, a, ideal, us, helps,\nseek, Agápe, plane, recall, feeling, within, returned, chapter,”, based, described,\napply, physical, Although, good, by, used, love, God.”, children., his, any, char-\nity;, Socrates, be, work, throughout, and, that, Greek, even, word, agápē), love.”,\nknown, biblical, feelings, does, famous, In, subject, becomes, one, understand-\ning, children, ”love, through, beauty, well, It, was, initially, feast., ﬁnding, itself.,\n13, all, ”without, feel, with, is, it, thus, New, as, the, brotherly, in, is, an, there,\nGod, youthful, necessary, high, Lovers, also, Whether\n• (ES) person, Aquinas, esp., continues, has, omas, truth, can, erotic, sexual,\ndesire\n• (FI) on, –, man, mean\n• (FR) (ἀγάπη, spouse, not, ancient, marriage., soul, person, content, Christians,\nTestament, Éros, just, part, type, passage, means, humans, passion.”, aspires, con-\ntemplation, contributes, argue, aﬀection\n• (IT) texts, 1, ”intimate, Plato, ”to\n• (RU) (ἔρως\n• (TR) talk\n• (TrAM) érōs), ”love:\n• (TrEL) ”erotas”, denote, eros., to, eros\n• (other) non-corporeal, Corinthians, self-beneﬁt)., benevolence., unconditional,\nphilosophers, transcendence.\nData:\nMixed script: English – Spanish – Arabic\n• (AR) ,المصريين, رسالتها, الذي, الرسم, فٔان, دون, لاية, ٔاكثر, ٔاستخدموا, و, ينبغي, العلامة, العلامات, واحدة, قدماء, من, كلمات\nولكن, الرمز, هم, ٔان, شيء, يعني, استخدم, يعبر, عن, بنظرة, المعروف, تنقل, وعموما, والٔاغريق, معين, الحاجة\n• (EN) for, used, been, displaying, of, ribbon, black, or, mourning., statement.,\ntragedies, is, political, a, Wearing, as, mourning\n• (ES) por, has, crespón, sociedades, personas, sentimiento, representando, esta-\ndos, de, El, señal, lazo, símbolo, en, utilizado, y\n84\n• (FR) remembrance, remembrance, un, es\n• (IT) negro, duelo., POW/MIA\n• (TrAM)\n• (TrEL) symbol, o\n• (other) político-social, organizaciones\nData:\nMixed script: English – Chinese\n• (DE) 机;, Chinese:, Western\n• (EL) 機)\n• (EN) Some, for, meanings, by, of, are, 8, positive, speaking, be, composed, or,\nmeanings., tradition, number, and, that, sound, linguists, word, some, this, other,\nIn, have, invoked, criticized, 6, because, e, believed, words, numbers, sounds,\nfrequently, is, pronounced, besides, traditional, the, in, represent, two, motiva-\ntional, usage, their, based\n• (ES) 危機;, has, 危机;, can, Chinese, ”crisis”, similar\n• (FI) on\n• (FR) (吉利), component, ”danger”, characters, (不利), certain, jī\n• (PL) pinyin:\n• (RU) 0, 9, wēijī)\n• (TrEL) to, to., names, name\n• (other) inauspicious, ”opportunity”., (simpliﬁed, auspicious\nData:\nMixed script: Ukrainian – Russian\n• (RU) Польшей, Румынией, Венгрией, юге, границу, с, омывается, Имеет, 9\n—13, Молдавией., Азовского, водами, Россией, Чёрного, Русь, и, пунктом,\nСловакией\n• (TrAM) й\n• (UK) держави, скіфів, України, народів, На, державності, вважається, від-\nправним, території, української, готів, культури, але, сарматів, існували,\nстоліття., Київська, на, Віддавна, інших, та, морей.\n• (other) сухопутную, Белоруссией\n85\nData:\nPali: abbha\n• (AR) ., 134., 289.\n• (DE) Miln), imber, dark), Miln\n• (EL) (=, (abbhaŋ\n• (EN) water, mountain, of, free, (used, or, like, referred, (also, A, is, cloudy,\nclouds, later, a, froth, 1, summit, thundering, by, mass, Pv, Oir, obscure, scum,\nthat, water]., thick, As, from, It, is, at, as, the, in, clouds, things, also\n• (ES) (dense, f., sense, expl, rajo\n• (FI) 239., rain;, Lat., Vin, perhaps, SnA\n• (FR) cloud, Dh, adj., point, cloud, Dhs, A), rain, VvA, DhsA, list\n• (IT) \\”dark, &, ambha, 3, 1, 317, J, sunshine, cp., abhra, [Vedic, (megho\n• (PL) 487, =, S, 295, <br, moon–, 249\n• (RU) 348, 53\n• (TR) viz., ambu, Vv\n• (TrAM) 687, PvA, (°sama, 101, (nīl°, (cp., 64;, (nt.), 581, m., Sn, 1064;\n• (TrEL) , Gr., Sk., Idg., to, pabbata, nt.\n• (UK) 12)., 273, 617, 348)., 250;, 251)., 382).\n• (other) <b> –saŋvilāpa </b>, <b> –mua </b>, <smallcaps> vi. </smallcaps>,\n(mahiyā, <smallcaps> iv. </smallcaps>, cloud\\”;, <b> Rāhu </b>, <b> abbhā\n</b>, <b> abbhaŋ, <superscript> 9 </superscript>, marajo </b>, abbhāmua,\nvalāhaka);, <smallcaps> i. </smallcaps>, <b> abbhāmaa </b>, valāhaka–sikhara,\n<superscript> s. </superscript>, <smallcaps> ii. </smallcaps>, <b> dhū-, storm–\ncloud, /><b> –kūṭa </b>, thunder–cloud);, <at>a)fro\\\\s</at>, <b>–paṭala</b>,\n<at>o)/mbros</at>, nīla–megha, <superscript>1</superscript>, *m̊ bhro, \\”dull\\”;,\nacchādesi);, mahikā</b>, <b> –ghana </b>\n86\nData:\nPali: abhijjhitar\n• (DE) v.\n• (EN) A, one, in, who, covets, med., function]\n• (IT) ag., M, fr.\n• (PL) 287, =\n• (RU) 265\n• (TrAM) l., [n.\n• (TrEL) (T.\n• (other) <smallcaps> v. </smallcaps>, abhijjhātar, abhijjhita, °ātar)., <smallcaps>\ni. </smallcaps>, °itar, °itar)\nData:\nPali: ajja\n• (DE) （see, v., being, Ajjā\n• (EN) of, or, and, not, present, Freq., day, this, “on, from, adyā，a, with, as, the,\nmorning, in, day”], an\n• (ES) bahutaṁ,\n• (FI) 32，23., ajjato\n• (FR) Loc., dyaus, 15，64., dyā, pron.\n• (IT) [Vedic, Mhvs, &, –divasa\n• (PL) （=, +, demonstr., s.\n• (RU) III，425, agge（?）\n• (TR) old, adya, 10）, idāni\n• (TrAM) ‹-›\n• (TrEL) phrase, base\n• (UK) a3）\n• (other) onward，henceforth, ajjā；, DA.I，235., （adv.）, J.I，279；, D.I，85；,\najja-tagge，see, Sn.75，153，158，970，998；, J.VI，180；, PvA.6，23；, –kālaṁ,\ndiva），thus, PvA.59）；, agga3）, Kern，Toev., Pv.I，117, Dh.326；, ajjatagge,\n（read, （Page, Vin.I，18；, dyā，a°, Ajja，&, to-day，now, “food”）；\n87\nData:\nPali: gūhanā\n• (ES) 253）\n• (other) [abstr．fr．gūhati]=gūhanā, Pug．19．Cp．pari°．（Page, （q．v.）,\nGūhanā，（f.）\nData:\nPali: pacati\n• (EL) 382）\n• (EN) for, aer, roasting, read, roasted, be, or, at, tormented, in\n• (FR) pare, D．I，52\n• (IT) &, pacato, purgatory\n• (TrAM) pāceti, ripe]\n• (TrEL) to, daṇḍena\n• (other) bake，Gr．pέssw,（+Caus．pācayato）,（q．v．）．（Page, DA．I，159，where,\nCaus．pacāpeti, intrs．）：Niraye, pacitvā, Pass．paccati,（trs．and, tormenting，\nGen．pacato, pīḷentassa）．–, ﬁg．torment, cook，pέpwn, Pacati，[Ved．pacati，\nIdg．*peqǔō，Av．pac-；, paccato，by, ppr．pacanto, cook，boil，roast, fry，\nroast，Lith，kepū, （q．v．）．–, （expld, Vin．IV，264；, Obulg．peka, pp．\npakka, （q．v．）．‹-›, N．S．II，225，PvA．10，14．–\nData:\nTwier 1 (Greek–English)\n• (DE) Internet\n• (EL) στο, τη, αυτή, διαγωνισμό, λύση, ψήφισα\n• (EN) of, IT, ings\n• (ES) BUSINESS\n• (TrAM) Μόλις\n• (other) EXCELLENCE.\n88\nData:\nTwier 2 (French–English)\n• (EN) David, ”e, is, it, perish;, or, collective, Demain, counts?”, that, of, dynam-\nics, all\n• (FI) 18h\n• (FR) par, Keynote\n• (other) #dhiha6, @dhiparis, science-publish\nData:\nTwier 3 (French–English)\n• (EN) for, Food, waiting, the, in, ready, and, are\n• (ES) go\n• (FI) Edmonton\n• (FR) just, breuvages, fans\n• (TrEL) to\n• (other) #bilingualism, #FWWC2015\nData:\nTwier 4 (English–Polish)\n• (EN) with, back, from, comes, crates, and, poland, two, of, jackets\n• (ES) dad, adidas\n• (TrAM) my\n• (TrEL) omg\n• (other) żubrówka, strawberries\nData:\nTwier 5 (Transliterated Amharic–English)\n• (EN) is, bread).\n• (FR) our\n• (IT) (coﬀee\n• (PL) naw\n• (TrAM) Buna, dabo\n89\n8.3.2\nTextcat\nFor Textcat, the identiﬁed language is indicated in parentheses. As Textcat returns\nunknown for many words, I merely indicate the non-unknown categories to save\nspace and write rest to indicate that all other words of the text have been classiﬁed as\nunknown. e language abbreviations are:\nAbbreviation\nLanguage\nDA\nDanish\nDE\nGerman\nEL\nGreek\nEN\nEnglish\nES\nSpanish\nFI\nFinnish\nFR\nFrench\nHU\nHungarian\nID\nIndonesian\nIT\nItalian\nLT\nLithuanian\nLV\nLatvian\nNL\nDutch\nPT\nPortuguese\nRU\nRussian\nTH\nai\nZH\nChinese\nData:\nLatin script: German – English\n• (HU) “navel-gazing”\n• (ZH) Nabelschau\n• (unknown) rest\nData:\nLatin script: German – Finnish – Turkish\n• (DA) Südsommer., genellikle,\n• (DE) Jahreszeiten, arktischen,\n• (FI) vuodenajoista, kallistunut, tavallisesti,\n90\n• (ZH) gemäßigten, Klimazone., Südhalbkugel, Nordsommer, gleichzeitig, vuoden-\naika, jyrkemmässä, vuodenaikoina., Pohjoisella, pallonpuoliskolla, kesäkuukausik-\nsi, eteläisellä, mevsimdir., gerçekleşir., arasındadır.,\n• (unknown) rest\nData:\nLatin script: English – French\n• (HU) diﬀerent.,\n• (ZH) (rugueux),(otherwise,\n• (unknown) rest\nData:\nLatin script: English – Transliterated Greek\n• (EN) historically, respective,\n• (LT) languages,\n• (ZH) distinguishes, Nonetheless,\n• (unknown) rest\nData:\nLatin script: Italian – German\n• (DE) allgemeiner, angemahnte,\n• (ES) delicatezza,\n• (HU) bellissima,\n• (IT) dell’aureola, consapevole, richiamano, anti-Turchi., ossessionava,\n• (NL) Ingenieuren,\n• (PT) approfonditamente,\n• (ZH) custodisce, struggente:, rivoluzionaria, psicologia, consapevolezza, auto-\ndistruggersi, lunghissimo, Fachkräemangel, Deutschland, intuizioni, Entwar-\nnung, Stierverbands, Wissenscha, MINT-Berufen, Annunciazione, dell’an-\ngelo:, Naturwissenschalern,\n• (unknown) rest\n91\nData:\nMixed script: Greek – Russian\n• (EL) ανεξάρτητου, οικογένειας,\n• (RU) существования, богатейшая, литература., греческого, обязательным,\nобразованного, присутствует, количество, заимствований, значительное,\nисточником, технических, называемая, международная,\n• (TH) латинским),\n• (ZH) ινδοευρωπαϊκές, ινδοευρωπαϊκής, древнегреческий, международную,\nцерковнославянский,\n• (unknown) rest\nData:\nMixed script: English – Greek\n• (DA) deﬁnition:, understanding,\n• (EN) aﬀection, unconditional, suggesting,\n• (FR) relationships, contemplation, appreciation, araction, araction.”, transcen-\ndence.,\n• (HU) benevolence., self-beneﬁt).,\n• (IT) non-corporeal,\n• (PT) contributes,\n• (ZH) Corinthians, throughout, Christians, Symposium, existence;, philosophers,\n• (unknown) rest\nData:\nMixed script: English – Spanish – Arabic\n• (ES) sociedades, organizaciones, sentimiento, político-social,\n• (FR) remembrance, remembrance, statement.,\n• (ID) displaying,\n• (PT) representando,\n• (unknown) rest\n92\nData:\nMixed script: English – Chinese\n• (EN) traditional, motivational, pronounced, tradition„\n• (FR) characters,\n• (ZH) simpliﬁed, frequently, ”opportunity”., criticized, auspicious, inauspicious,\n• (unknown) rest\nData:\nMixed script: Ukrainian – Russian\n• (RU) державності, Словакией, Молдавией.,\n• (TH) вважається,\n• (ZH) відправним, української, сухопутную, Белоруссией,\n• (unknown) rest\nData:\nPali: abbha\n• (DA) storm–cloud, thundering,\n• (HU) marajo</b>, nīla–megha, valāhaka–sikhara,\n• (ZH)\n<at> a)fro\\\\</at>, <at> o)/mbros </at>, <smallcaps> ii. </smallcaps>, mahikā</b>,\n<b> Rāhu </b>, <smallcaps> i. </smallcaps>, thunder–cloud);, <smallcaps> vi.\n</smallcaps>, acchādesi);, <smallcaps> iv. </smallcaps>, <superscript> 9 </su-\nperscript>, <b> abbhā </b>, <superscript> s. </superscript>, valāhaka);, <b>\nabbhāmaa </b>, /><b> –kūṭa </b>, <superscript> 1 </superscript>, <b> –ghana\n</b>, <b> –paṭala </b>, <b> –mua </b>, abbhāmua, <b> –saŋvilāpa </b>\n• (unknown) rest\nData:\nPali: abhijjhitar\n• (ZH) abhijjhita, <smallcaps> i. </smallcaps>, abhijjhātar, <smallcaps> v.\n</smallcaps>,\n• (unknown) rest\n93\nData:\nPali: ajja\n• (ZH) diva），thus, to-day，now, Sn.75，153，158，970，998；, Kern，Toev.,\najja-tagge，see, onward，henceforth,\n• (unknown) rest\nData:\nPali: gūhanā\n• (ZH) Gūhanā，（f.）, [abstr．fr．gūhati]hanā, Pug．19．Cp．pari°．（Page,\n• (unknown) rest\nData:\nPali: pacati\n• (ZH) ﬁg．torment, Pacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；, Obulg．peka,\nfry，roast，Lith，kepū, bake，Gr．pέssw, cook，pέpwn, cook，boil，roast, Vin．\nIV，264；, intrs．）：Niraye, N．S．II，225，PvA．10，14．–, ppr．pacanto,\ntormenting，Gen．pacato,（+Caus．pācayato）, DA．I，159，where, paccato，by,\npīḷentassa）．–,（q．v．）．‹-›, Caus．pacāpeti, Pass．paccati,（q．v．）．（Page,\n• (unknown) rest\nData:\nTwier 1 (Greek–English)\n• (ZH) διαγωνισμό, EXCELLENCE.,\n• (unknown) rest\nData:\nTwier 2 (French–English)\n• (IT) collective,\n• (ZH) science-publish,\n• (unknown) rest\nData:\nTwier 3 (French–English)\n• (ZH) #bilingualism,\n• (unknown) rest\n94\nData:\nTwier 4 (English–Polish)\n• (LV) strawberries,\n• (unknown) rest\nData:\nTwier 5 (Transliterated Amharic–English)\n• (unknown) rest\n8.3.3\nClustering\nClustering the diﬀerent data sets produced the following clusters. e second run\nuses the clusters from the ﬁrst run and possibly subdivides each cluster into two or\nmore clusters.\nData:\nLatin script: German – English\nFirst run\n• “navel-gazing”, doesn’t, else’s\n• “staring, But, German, Nabelschau, anyone, belly, buon, case, just, means,\nnavel”., own., refer, this, word, your\n• at, in, it, or, to\n• –, e\nSecond run\n• doesn’t, else’s\n• “navel-gazing”\n• “staring, But, German, Nabelschau, belly, case, means, navel”., refer, this\n• anyone, buon, just, own., word, your\n• it, or, to\n• at, in\n• –, e\n95\nData:\nLatin script: German – Finnish – Turkish\nFirst run\n• Dünya, Güney, Küre’de, Südhalbkugel, Südsommer., Südwinter, Sıcak, arasında,\ngemäßigten, günler, için, kesäkuukausiksi, lämpimin, säteilee, sıcak, wärmste,\nçıkar., Der\n• Aralık, Eylül, Kesä, Yarım, arasındadır., eteläisellä, eiği, eä, gerçekleşir., heinä-\n, jyrkemmässä, kesä-., kevään, välissä., yaklaşık, ısıyı\n• 21, 22\n• Der, Haziran, Jahreszeiten, Je, Klimazone., Kuzey, Mart, Nord-, Nordsommer,\nPohjoisella, Sommer, Yaz, arktischen, auf, aurinko, ay, dem, depo, der, die, eli,\nelokuu, en, er, ﬁndet, genellikle, gerade, gleichzeitig, helmikuu., herrscht, iki,\nile, in, ise, ist, ja, joulu-, kallistunut, koska, kuin, kulmassa, lasketaan, maan,\nmaapallo, man, mevsimdir., mit, muina, nachdem, niin, ob, oder, on, ortaya, pal-\nlonpuoliskolla, pinnalle, silloin, sonra, spricht, sta., suvi, syksyn, tammi-, taval-\nlisesti, und, uzun, vier, vom, vuodenaika, vuodenaikoina., vuodenajoista, yazda\nSecond run\n• Südhalbkugel, Südsommer., Südwinter, arasında, gemäßigten, kesäkuukausiksi,\nlämpimin, säteilee, wärmste\n• Dünya, Güney, Küre’de, Sıcak, günler, için, sıcak, çıkar., Der\n• arasındadır., eteläisellä, eiği, eä, gerçekleşir., heinä-, jyrkemmässä, kesä-.,\nkevään, välissä., yaklaşık, ısıyı\n• Aralık, Eylül, Yarım\n• Kesä\n• 22\n• 21\n• Der, Haziran, Jahreszeiten, Klimazone., Kuzey, Mart, Nord-, Nordsommer, Po-\nhjoisella, Sommer, Yaz,\n96\n• arktischen, auf, aurinko, dem, depo, der, die, eli, elokuu, ﬁndet, genellikle, gerade,\ngleichzeitig, helmikuu., herrscht, iki, ile, ise, ist, joulu-, kallistunut, koska, kuin,\nkulmassa, lasketaan, maan, maapallo, man, mevsimdir., mit, muina, nachdem,\nniin, oder, ortaya, pallonpuoliskolla, pinnalle, silloin, sonra, spricht, sta., suvi,\nsyksyn, tammi-, tavallisesti, und, uzun, vier, vom, vuodenaika, vuodenaikoina.,\nvuodenajoista, yazda\n• Je, ay, en, er, in, ja, ob, on\nData:\nLatin script: English – French\nFirst run\n• ”coarse”, ”hard”., ”rough”, ”so”, (otherwise, (rugueux), Doux, English, almost,\nalso, although, both, but, can, diﬀerent., doux, for, mean, meaning, mou, only,\nopposite, sucré, sweet, the, their, translate, used)., very, while, wines\n• is, or\n• as, in, of\nSecond run\n• Doux, English,\n• “coarse”, (otherwise, (rugueux), almost, although, diﬀerent., meaning, opposite,\ntranslate\n• “hard”., ”rough”, ”so”, also, both, but, can, doux, for, mean, mou, only, sucré,\nsweet, the, their, used)., very, while, wines\n• or\n• is\n• in\n• of\n• as\n97\nData:\nLatin script: English – Transliterated Greek\nFirst run\n• e\n• agápe, philía, storgē., éros,\n• Ancient, However, Nonetheless, contexts., diﬀerent, diﬃcult, distinct, distin-\nguishes, follows., generally, historically, language, languages, meanings, outside,\nrespective, senses, separate, which, words\n• Greek, and, are, as, at, been, for, four, has, how, in, is, it, least, love, love:, of,\nother, the, their, these, to, used, used., ways, were, when, with, word\nSecond run\n• e\n• philía, storgē.\n• agápe, éros,\n• Ancient, However, Nonetheless, contexts., diﬀerent, diﬃcult, distinct, distin-\nguishes, follows., generally, historically, meanings, respective\n• words\n• language, languages, outside, senses, separate, which\n• and, are, as, at, been, for, four, has, how, in, is, it, least, love, love:, of, other, the,\ntheir, these, to, used, used., ways, were, when, with, word\n• Greek\nData:\nLatin script: German – Italian\nFirst run\n• (il, E, So, a, ad, da, di, e, es, ha, i, il, in, la, le, lo, ma, ne, se, si, un, va, zu\n98\n• “ein , Annunciazione, Baista, Cenacolo, Certo, Come, Dabei, Deutsche, Deutsch-\nland, Entwarnung, Ergebnis, Giuda, Ingenieuren, Ist, Jahren, Kaum, Leonardo,\nMINT-Berufen, Mythos?, Naturwissenschalern, Stierverbands, Stimmen, Stu-\ndie, Studie, Szenario, ema, Umfrage, Venezia, Warnung, Wissenscha, Woche,\nZeit, acque, ali, alla, alle, allgemeiner, also, amore, anche, angemahnte, anni,\nanti-Turchi., approfonditamente, arginato., aento, auch, autodistruggersi, baci-\nni, barbaglio, bei, bellissima, cancro, caurata, che, chiave, con, condanna, consa-\npevole, consapevolezza, cosa, cura, custodisce, das, dass, deﬁnire, del, delicatezza,\ndelle, dem, den, der, des, die, difesa, drohe., drohenden, eher, ein, eine, faceva,\ngeben., gibt., idee, intuizioni, kam, keine, letzter, lunghissimo, mehr”, mehren,\nmodo., moto, movimento, nelle, neue, nicht, non, nur, occhio, oder, ossessiona-\nva, ovvero, peccato), per, periture, poi, privato, psicologia, punte, qualche, quel,\nquello, recente, restauro, riccioli, ricerche, richiamano, rivoluzionaria, seit, sich,\nsogno, solo, solo, sono, stessa, struggente:, subito, sui, und, vada, vergeht, viene,\nvinciano, vita, volare?, vom, zum\n• all’insù, dell’angelo:, dell’aureola, l’esempio, Milano\n• Fachkräemangel, aﬀrescò, cominciò, ür, jüngst, perché, più, studierà\nSecond run\n• a, e, i\n• E\n• So\n• (il, ad, da, di, es, ha, il, in, la, le, lo, ma, ne, se, si, un, va, zu\n• Annunciazione, Baista, Cenacolo, Certo, Come, Dabei, Deutsche, Deutschland,\nEntwarnung, Ergebnis, Giuda, Ingenieuren, Ist, Jahren, Kaum, Leonardo, MINT-\nBerufen, Mythos?, Naturwissenschalern, Stierverbands, Stimmen, Studie, Stu-\ndie, Szenario, ema, Umfrage, Venezia, Warnung, Wissenscha, Woche, Zeit\n• “ein, acque, ali, alla, alle, allgemeiner, also, amore, anche, angemahnte, an-\nni, anti-Turchi., approfonditamente, arginato., aento, auch, autodistruggersi,\nbacini, barbaglio, bei, bellissima, cancro, caurata, che, chiave, con, condanna,\nconsapevole, consapevolezza, cosa, cura, custodisce, das, dass, deﬁnire, del, de-\nlicatezza, delle, dem, den, der, des, die, difesa, drohe., drohenden, eher, ein,\neine, faceva, geben., gibt., idee, intuizioni, kam, keine, letzter, lunghissimo,\nmehr”, mehren, modo., moto, movimento, nelle, neue, nicht, non, nur, oc-\nchio, oder, ossessionava, ovvero, peccato), per, periture, poi, privato, psicologia,\n99\npunte, qualche, quel, quello, recente, restauro, riccioli, ricerche, richiamano, ri-\nvoluzionaria, seit, sich, sogno, solo, solo, sono, stessa, struggente:, subito, sui,\nund, vada, vergeht, viene, vinciano, vita, volare?, vom, zum\n• all’insù, dell’angelo:, dell’aureola, l’esempio, Milano\n• Fachkräemangel\n• aﬀrescò, cominciò, jüngst, perché, studierà\n• ür\n• più\nData:\nMixed script: Greek – Russian\nFirst run\n• 15ο, —, Η\n• το, В, На, а, в, и, на, с\n• (наряду, (так, γλωσσών., γλώσσα, γλώσσες., δεσμό., π.Χ., σήμερα., заимство-\nваний, латинским), лексика)., литература., слов., человека., язык.\n• Ανήκει, Αποτελεί, Στην, έχουμε, αιώνα, ανεξάρτητου, από, βαλκανικό, γλωσ-\nσικό, γλώσσα, γραπτά, είναι, ελληνική, ενός, επίσης, ινδοευρωπαϊκές, ινδο-\nευρωπαϊκής, κείμενα, κλάδου, μέλος, μέχρι, μία, μοναδικό, οικογένειας, στον,\nτης, τις, τον, Римской, богатейшая, большое, была, время, всех, всякого,\nгреческие, греческих, греческого, греческом, двумя, для, древнегреческий,\nего, знание, значительное, империи, источником, количество, латинских,\nлатинском, лексику, международная, международную, называемая, науч-\nных, новое, новых, образованного, обязательным, основном, присутству-\nет, проникали, путями, романских, русский, слова, создана, создания, стал,\nсуществования, считалось, терминов, технических, церковнославянский,\nчерез, этапах, язык, языка, языке\nSecond run\n• 15ο\n• —\n• Η\n100\n• а, в, и, с\n• В\n• το, На, на\n• (наряду, (так\n• γλωσσών., γλώσσα, γλώσσες., δεσμό., π.Χ., σήμερα., заимствований, латин-\nским), лексика)., литература., слов., человека., язык.\n• έχουμε, αιώνα, ανεξάρτητου, από, βαλκανικό, γλωσσικό, γλώσσα, γραπτά, εί-\nναι, ελληνική, ενός, επίσης, ινδοευρωπαϊκές, ινδοευρωπαϊκής, κείμενα, κλάδου,\nμέλος, μέχρι, μία, μοναδικό, οικογένειας, στον, της, τις, τον\n• Ανήκει, Αποτελεί, Στην\n• богатейшая, греческие, греческих, греческого, греческом, древнегреческий,\nзначительное, источником, количество, латинских, латинском, междуна-\nродная, международную, называемая, образованного, обязательным, ос-\nновном, присутствует, проникали, романских, создания, существования,\nсчиталось, терминов, технических, церковнославянский\n• Римской, большое, была, время, всех, всякого, двумя, для, его, знание, им-\nперии, лексику, научных, новое, новых, путями, русский, слова, создана,\nстал, через, этапах, язык, языка, языке\nData:\nMixed script: English – Greek\nFirst run\n• “intimate, “without, Although, Aquinas, Christians, Corinthians, Socrates, Sym-\nposium, Testament, Whether, aﬀection, ancient, another.”, appreciation, aspires,\naraction, araction.”, becomes, benevolence., biblical, brotherly, chapter,”, char-\nity;, children, children., contemplation, content, continues, contributes, deﬁni-\ntion:, described, existence;, explained, express, feeling, feelings, ﬁnding, further,\nholding, initially, inspired, knowledge, marriage., necessary, non-corporeal, pas-\nsage, passion.”, philosophers, physical, platonic, reﬁned, relationships, returned,\nself-beneﬁt)., sensually, spiritual, subject, suggesting, through, throughout, tran-\nscendence., unconditional, understanding, without, youthful\n• (ἀγάπη, (ἔρως, Agápe, agápē), Éros, érōs), –\n101\n• “Form”, “erotas”, “love, “love, “love:, (even, Agape, Greek, Lovers, Modern, Plato,\nis, omas, also, apply, argue, based, beauty, beauty, being, dating, denote,\ndesire, does, eros, eros., erotic, even, famous, feast., feel, felt, given, good, helps,\nhence, high, humans, ideal, itself., just, known, leads, like, love, love, love.”, mean,\nmeans, most, mostly, one’s, part, person, person, plane, recall, refer, regard., seek,\nsexual, soul, spouse, talk, texts, that, there, thus, truth, truth, type, used, well,\nwill, will, with, within, word, work\n• “to, 1, 13, God, God.”, In, It, New, e, a, all, an, and, any, are, as, be, by, can, esp.,\nfor, has, his, in, is, is, it, its, man, not, not, of, on, one, or, own, the, to, us, use,\nwas\nSecond run\n• aﬀection, ancient, another.”, aspires, becomes, biblical, chapter,”, charity;, chil-\ndren, children., content, deﬁnition:, feeling, feelings, ﬁnding, holding, marriage.,\nnecessary, passage, passion.”, platonic, reﬁned, returned, subject, through, with-\nout\n• Although, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament,\nWhether\n• “intimate, appreciation, araction, araction.”, benevolence., brotherly, contem-\nplation, continues, contributes, described, existence;, explained, express, further,\ninitially, inspired, knowledge, non-corporeal, philosophers, physical, relation-\nships, self-beneﬁt)., sensually, spiritual, suggesting, throughout, transcendence.,\nunconditional, understanding, youthful\n• Agápe, agápē), Éros, érōs)\n• (ἀγάπη, (ἔρως\n• –\n• “erotas”, beauty, beauty, dating, denote, desire, erotic, famous, humans, itself.,\nmostly, person, person, recall, regard., sexual, spouse, within\n• “Form”, Agape, Greek, Lovers, Modern, Plato, is, omas, based, being, feast.,\nhence, ideal, leads, means, plane, refer, there\n• apply, felt, helps, high, just, known, most, part, talk, texts, that, thus, truth, truth,\ntype, well, will, will, with, word, work\n• “love, “love, “love:, (even, also, argue, does, eros, eros., even, feel, given, good,\nlike, love, love, love.”, mean, one’s, seek, soul, used\n102\n• 1, 13, In, It\n• “to, a, an, as, be, by, in, is, is, it, of, on, or, to, us\n• God, God.”, New, e, all, and, any, esp., its, own, the\n• are, can, for, has, his, man, not, not, one, use, was\nData:\nMixed script: English – Spanish – Arabic\nFirst run\n• El, POW/MIA, Wearing, a, as, been, black, de, displaying, duelo., en, es, estados,\nfor, has, is, lazo, mourning, mourning., negro, o, of, or, organizaciones, personas,\npolitical, por, remembrance, remembrance, representando, ribbon, sentimiento,\nsociedades, statement., symbol, tragedies, un, used, utilizado, y\n• crespón, político-social, señal, símbolo\n• A\n• ،ٔاستخدموا، ٔاكثر، ٔان، استخدم، الحاجة، الذي، الرسم، الرمز، العلامات، العلامة، المصريين، المعروف، بنظرة، تنقل، دون، رسالتها\nشيء، عن، فٔان، قدماء، كلمات، لاية، معين، من، هم، و، واحدة، والٔاغريق، وعموما، ولكن، يعبر، يعني، ينبغي\nSecond run\n• a, o, y\n• El, as, de, en, es, is, of, or, un\n• Wearing, been, black, displaying, duelo., estados, for, has, lazo, mourning, mourn-\ning., negro, organizaciones, personas, political, por, remembrance, remembrance,\nrepresentando, ribbon, sentimiento, sociedades, statement., symbol, tragedies,\nused, utilizado\n• POW/MIA\n• político-social, símbolo\n• crespón, señal\n• A\n•ٔاستخدموا، استخدم، الحاجة، العلامات، العلامة، المصريين، المعروف، رسالتها، والٔاغريق، وعموما\n•ٔاكثر، الذي، الرسم، الرمز، بنظرة، تنقل، دون، شيء، فٔان، قدماء، كلمات، لاية، معين، واحدة، ولكن، يعبر، يعني، ينبغي\n103\n•ٔان، عن، من، هم\n•و\nData:\nMixed script: English – Chinese\nFirst run\n• “crisis”, “danger”, “opportunity”., (simpliﬁed, Chinese, Chinese:, Western, aus-\npicious, because, believed, besides, certain, characters, component, composed,\ncriticized, frequently, inauspicious, invoked, linguists, meanings, meanings., mo-\ntivational, number, numbers, pinyin:, positive, pronounced, represent, similar,\nsounds, speaking, tradition, traditional, wēijī)\n• (不利), (吉利), 危机;, 危機;, 机;, 機)\n• 0, 6, 8, 9\n• In, Some, e, and, are, based, be, by, can, for, has, have, in, is, jī, name, names,\nof, on, or, other, some, sound, that, the, their, this, to, to., two, usage, word, words\nSecond run\n• Chinese, Chinese:\n• Western\n• “crisis”, “danger”, “opportunity”., (simpliﬁed, auspicious, because, believed, be-\nsides, certain, characters, component, composed, criticized, frequently, inaus-\npicious, invoked, linguists, meanings, meanings., motivational, number, num-\nbers, pinyin:, positive, pronounced, represent, similar, sounds, speaking, tradi-\ntion, traditional, wēijī)\n• (不利), (吉利)\n• 危机;, 危機;\n• 机;, 機)\n• 6, 8, 9\n• 0,\n• Some, e, and, are, based, can, for, has, have, name, names, other, some, sound,\nthat, the, their, this, two, usage, word, words\n104\n• In, be, by, in, is, of, on, or, to, to.\n• jī\nData:\nMixed script: Ukrainian – Russian\nFirst run\n• 9—13\n• Белоруссией, Венгрией, Молдавией., Польшей, Россией, Словакией, мо-\nрей., народів, сарматів, скіфів, століття.\n• Азовского, Віддавна, Київська, Румынией, України, , Чёрного, вважаєть-\nся, відправним, , границу, держави, державності, , культури, омывается,\nпунктом, сухопутную, території, української, існували,\n• Имеет, На, Русь, але, водами, готів, и, й, на, с, та, юге, інших\nSecond run\n• 9—13\n• морей., народів, сарматів, скіфів, століття.\n• Белоруссией, Венгрией, Молдавией., Польшей, Россией, Словакией,\n• Азовского, Віддавна, Київська, Румынией, України, Чёрного, границу,\nдержави, культури, пунктом, існували\n• вважається, відправним, державності, омывается, сухопутную, території,\nукраїнської\n• и, й, с\n• На, на, та\n• але, водами, готів, юге, інших\n• Имеет, Русь\n105\nData:\nPali: abbha\nFirst run\n• (also, (cp., (dense, (megho, (used, (°sama, 1, 1, 101, 1064;, 12)., 134., 239., 249,\n250;, 251)., 273, 289., 295, 3, 317, 348, 348)., 382)., 487, 53, 581, 617, 64;, 687, <at>\na)fro\\\\s </at>, <at> o)/mbros </at>, <smallcaps> i. </smallcaps>, <smallcaps>\nii. </smallcaps>, <smallcaps> iv. </smallcaps>, <smallcaps> vi. </smallcaps>,\n<superscript> 1 </superscript>, <superscript> 9 </superscript>, <superscript>\ns. </superscript>, A, A), As, Dh, Dhs, DhsA, Gr., Idg., It, J, Lat., Miln, Miln),\nOir, Pv, PvA, S, Sk., Sn, SnA, , is, Vin, Vv, VvA, [Vedic, a, abhra, adj., also,\nambha, ambu, as, at, by, cloud, cloud, cloud\\”;, clouds, clouds, cloudy, cp., dark),\nexpl, f., free, from, froth, imber, in, is, later, like, list, m., marajo</b>, mass,\nmoon–, mountain, nt., obscure, of, or, pabbata, perhaps, point, rain, rain;, rajo,\nreferred, scum, sense, storm–cloud, summit, sunshine, that, the, thick, things,\nthunder–cloud);, thundering, to, viz., water, water].\n• &, (=, <b>–ghana</b>, <b>–mua</b>, <br, =, \\”dark, \\”dull\\”;\n• (abbhaŋ, (mahiyā, (nīl°, <b> –saŋvilāpa </b>, <b> Rāhu </b>, <b> abbhā </b>,\n<b> abbhāmaa </b>, abbhāmua, acchādesi);, mahikā </b>, nīla–megha, valā-\nhaka);, valāhaka– sikhara\n• *m̊ bhrocite /><b>–kūṭa</b>, <b>–paṭala</b>, <b>abbhaŋ, <b>dhū-, (nt.)\nSecond run\n• (cp., Dhs, DhsA, Idg., Lat., Miln, Miln), Oir, PvA, SnA, is, Vin, VvA, [Vedic, as,\nat, by, cp., in, is, nt., of, or, to\n• (also, (dense, (megho, (used, (°sama, <at> a)fro\\\\s </at>, <at> o)/mbros </at>,\n<smallcaps> ii. </smallcaps>, <smallcaps> iv. </smallcaps>, <smallcaps> vi.\n</smallcaps>, abhra, adj., also, ambha, ambu, cloud, cloud, cloud\\”;, clouds, clouds,\ncloudy, dark), expl, free, from, froth, imber, later, like, list, marajo </b>, mass,\nmoon–, mountain, obscure, pabbata, perhaps, point, rain, rain;, rajo, referred,\nscum, sense, storm– cloud, summit, sunshine, that, the, thick, things, thunder–\ncloud);, thundering, viz., water, water].\n• 1, 1, 101, 1064;, 12)., 134., 239., 249, 250;, 251)., 273, 289., 295, 3, 317, 348, 348).,\n382)., 487, 53, 581, 617, 64;, 687, <superscript> 1 </superscript>, <superscript> 9\n</superscript>\n• <smallcaps> i. </smallcaps>, <superscript> s. </superscript>, A, A), As, Dh, Gr.,\nIt, J, Pv, S, Sk., Sn, , Vv, a, f., m.\n106\n• <b> –ghana </b>, <b> –mua </b>, <br, \\”dark, \\”dull\\”;\n• &, (=, =\n• (abbhaŋ, (mahiyā, (nīl°, <b> Rāhu </b>, <b> abbhā </b>, nīla–megha\n• <b> –saŋvilāpa </b>, <b> abbhāmaa </b>, abbhāmua, acchādesi);, mahikā\n</b>, valāhaka);, valāhaka–sikhara\n• *m̊ bhro, /><b> –kūṭa </b>, <b> –paṭala </b>, <b> abbhaŋ, <b> dhū-\n• (nt.)\nData:\nPali: abhijjhitar\nFirst run\n• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, =, A, M, ag., fr.,\nin, l., v.\n• 265, 287\n• [n.\nSecond run\n• abhijjhita, abhijjhātar, covets, function], med., one, who, °itar), °itar, °ātar).\n• (T., A, M\n• =, l., v.\n• <smallcaps> i. </smallcaps>, <smallcaps> v. </smallcaps>, ag., fr., in\n• 265, 287\n• [n.\n107\nData:\nPali: ajja\nFirst run\n• –divasa, Freq., Loc., [Vedic, adya, ajjatagge, ajjato, an, and, as, base, being, day,\ndemonstr., dyaus, from, in, morning, not, of, old, or, phrase, present, pron., the,\nthis, with\n• &, +, Mhvs, s., v.\n• –kālaṁ, 10）, 15，64., 32，23., Ajjā, D.I，85；, DA.I，235., Dh.326；, III，425, J.I，\n279；, J.VI，180；, Kern，Toev., Pv.I，117, PvA.59）；, PvA.6，23；, Sn.75，153，\n158，970，998；, Vin.I，18；, a3）, adyā，a, agga3）, agge（?）, ajja-tagge，see,\najjā；, bahutaṁ, day”], diva），thus, dyā, dyā，a°, idāni, onward，henceforth,\nto-day，now,“food”）；,“on, ‹-›, Ajja，&,（=,（Page,（adv.）,（read,（see\nSecond run\n• an, as, in, of, or\n• Freq., Loc., [Vedic\n• –divasa, adya, ajjatagge, ajjato, and, base, being, day, demonstr., dyaus, from,\nmorning, not, old, phrase, present, pron., the, this, with\n• &\n• +\n• Mhvs\n• s., v.\n•“on, ‹-›, Ajja，&, （=, （Page, （adv.）, （read, （see\n• –kālaṁ, 10）, 15，64., 32，23., Ajjā, D.I，85；, DA.I，235., Dh.326；, III，425,\nJ.I，279；, J.VI，180；, Kern，Toev., Pv.I，117, PvA.6，23；, Sn.75，153，158，\n970，998；, Vin.I，18；, a3）, agga3）, ajja-tagge，see, ajjā；, bahutaṁ, day”\n], diva），thus, dyā, idāni, onward，henceforth, to-day，now\n• PvA.59）；, adyā，a, agge（?）, dyā，a°, “food”）；\n108\nData:\nPali: gūhanā\nFirst run\n• 253）, Pug．19．Cp．pari°．（Page, [abstr．fr．gūhati]=gūhanā, Gūhanā，（f.）\n, （q．v.）\nSecond run\n• 253）, Pug．19．Cp．pari°．（Page, [abstr．fr．gūhati]=gūhanā, Gūhanā，（f.）\n, （q．v.）\nData:\nPali: pacati\nFirst run\n• 382）, Caus．pacāpeti, DA．I，159，where, Obulg．peka, Pass．paccati, Vin．\nIV，264；, bake，Gr．pέssw, cook，boil，roast, cook，pέpwn, daṇḍena, ﬁg．\ntorment, fry，roast，Lith，kepū, intrs．）：Niraye, paccato，by, ppr．pacanto,\npp．pakka, pīḷentassa）．–, tormenting，Gen．pacato\n• D．I，52, N．S．II，225，PvA．10，14．–, Pacati，[Ved．pacati，Idg．*peqǔō，\nAv．pac-；, （+Caus．pācayato）, （expld, （q．v．）．–, （q．v．）．‹-›, （q．\nv．）．（Page, （trs．and\n• aer, at, be, for, in, or, pacato, pare, purgatory, read, ripe], roasted, roasting, to,\ntormented\n• &, pacitvā, pāceti\nSecond run\n• Caus．pacāpeti, DA．I，159，where, Obulg．peka, Pass．paccati, Vin．IV，264；\n, bake，Gr．pέssw, cook，boil，roast, cook，pέpwn, daṇḍena, ﬁg．torment, fry，\nroast，Lith，kepū, intrs．）：Niraye, paccato，by, ppr．pacanto, pīḷentassa）．–,\ntormenting，Gen．pacato\n• 382）, pp．pakka\n• D．I，52, N．S．II，225，PvA．10，14．–, （q．v．）．–\n• Pacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；,（+Caus．pācayato）,（expld,\n（q．v．）．‹-›, （q．v．）．（Page, （trs．and\n109\n• for, pacato, pare, read, ripe]\n• aer, purgatory, roasted, roasting, tormented\n• or, to\n• at, be, in\n• &\n• pacitvā, pāceti\nData:\nTwier 1 (Greek–English)\nFirst run\n• αυτή, διαγωνισμό, λύση, στο, τη, ψήφισα, Μόλις\n• BUSINESS, EXCELLENCE., IT, Internet, ings, of\nSecond run\n• Μόλις\n• αυτή, διαγωνισμό, λύση, στο, τη, ψήφισα\n• IT, of\n• Internet, ings,\n• BUSINESS, EXCELLENCE.\nData:\nTwier 2 (French–English)\nFirst run\n• “e, 18h, @dhiparis, David, Demain, Keynote, all, collective, counts?”, dynam-\nics, par, perish;, science-publish, that\n• is, it, of, or\n110\nSecond run\n• “e, @dhiparis, David, Demain, Keynote, all, collective, counts?”, dynamics,\npar, perish;, science-publish, that\n• 18h\n• is, it, or\n• of\nData:\nTwier 3 (French–English)\nFirst run\n• Edmonton, Food\n• go, in, to\n• and, are, breuvages, fans, for, just, ready, the, waiting\nSecond run\n• Edmonton, Food\n• to\n• go, in\n• for, just\n• and, are, breuvages, fans, ready, the, waiting\nData:\nTwier 4 (English–Polish)\nFirst run\n• żubrówka, my\n• adidas, and, back, comes, crates, dad, from, jackets, of, omg, poland, strawberries,\ntwo, with\n111\nSecond run\n• żubrówka, my\n• adidas, comes, dad, of\n• and, back, crates, from, jackets, omg, poland, strawberries, two, with\nData:\nTwier 5 (Transliterated Amharic–English)\nFirst run\n• Buna\n• (coﬀee, bread)., dabo, is, naw, our\nSecond run\n• Buna\n• our\n• (coﬀee, bread)., dabo, is, naw\n8.3.4\nLanguage Model Induction\nFor all language model induction tasks, the threshold value t has been set t = 0.02\nand the silver threshold value s has been set s = 0.1. e other parameters have been\nset to “maximum iteration count” i = 4, “maximum random iteration count” j = 2\nand “merge mode ADD”.\nData:\nLatin script: German–English\n• e, German, word, Nabelschau, means, or, “staring, at, your, But, in, this, it,\ndoesn’t, refer, to, anyone, else’s, buon, just, your, own.,\n• –\n• “navel-gazing”, navel”., case, belly\n112\nData:\nLatin script: German–Finnish–Turkish\n• die, in, und, Klimazone., Je, ob, auf, Südhalbkugel, vom, eli, on, vuodenaika, ja, on,\nvuodenajoista, koska, maapallo, on, silloin, kallistunut, aurinko, maan, pinnalle,\nkulmassa, muina, vuodenaikoina., Pohjoisella, pallonpuoliskolla, lasketaan, ta-\nvallisesti, ja, elokuu, eteläisellä, pallonpuoliskolla, joulu-, ja, helmikuu., en, sı-\ncak, en, yazda, Dünya, depo, en, sıcak, yaklaşık, ay, sonra, ortaya, Sıcak, Haziran,\nEylül, ise, Aralık, arasındadır.\n• Der, ist, wärmste, der, vier, Jahreszeiten, der, arktischen, nachdem, er, der, Nord-,\noder, herrscht, spricht, Nord-, oder, Der, ﬁndet, mit, Südwinter, sta., suvi, läm-\npimin, niin, eä, säteilee, heinä-, Yaz, mevsimdir., Küre’de, Küre’de, 21, 22, ara-\nsında, Küre’de, 22, 21, Mart\n• gemäßigten, gerade, gleichzeitig, kuin, Kuzey, uzun, günler, gerçekleşir., eiği,\niçin, günler, genellikle, iki, günler, Kuzey, ile, ile\n• Sommer, man, Südsommer., Nordsommer, dem, Kesä, kevään, syksyn, välissä.,\nKesä, jyrkemmässä, kesäkuukausiksi, kesä-., tammi-, Yarım, ısıyı, çıkar., Yarım,\nGüney, Yarım\nData:\nLatin script: English–French\n• both, “so”, in, English, although, their, is, is, the, opposite, of, “rough”, or, is,\nthe, opposite, of, sweet, only, for, wines, (otherwise, is\n• mou, :, mou, but\n• doux,\n• Doux, (rugueux), Doux\n• while\n• “hard”., used).,\n• translate, as, meaning, very, diﬀerent., ”coarse”, can, also, mean, almost,sucré,\nData:\nLatin script: English–Transliterated Greek\n• at, least, ways, as, to, is, has, philía, and, storgē., as, has, historically, diﬃcult, to,\nwhich, generally, as\n113\n• e, language, distinguishes, diﬀerent, the, Ancient, distinct, with, languages, it,\nbeen, separate, the, meanings, these, used, outside, their, respective, the, senses,\nin, these, used\n• Greek, how, word, Greek, agápe, éros, However, other, when, were, are\n• four, love, used., four, words, for, love:, of, words, of, contexts., Nonetheless,\nwords, follows.\nData:\nLatin script: Italian–German\n• aﬀrescò, privato, Studie, deﬁnire, periture,Stierverbands, Wissenscha,studierà,\ndifesa, ovvero, Szenario, Naturwissenschalern\n• dell’aureola, da, del, di, der, zum, modo., dem, den, drohe., Come, vom\n• custodisce, quel, es, oder, per, le, idee, stessa, des, dass, delle, E, se, Ist, das, seit\n• più, Cenacolo, vinciano, rivoluzionaria, Giuda, condanna, con, peccato), comin-\nciò, con, cancro, faceva, intuizioni, vita, va, Dabei, Ergebnis, in, i, riccioli, poi,\npiù, bacini, in, Annunciazione, con, ali, la, cosa, barbaglio, anni, bei,\n• ne, struggente:, che, amore, e, non, viene, ma, consapevolezza, ad, che, ha, re-\ncente, Kaum, eine, Woche, vergeht, keine, neue, Umfrage, Warnung, ema,\nFachkräemangel, Deutschland, Certo, ma, anche, consapevole, che, qualche,\nmehren, letzter, Zeit, Stimmen, Entwarnung, geben., kam, jüngst, eine, Deutsche,\n”ein, allgemeiner, Fachkräemangel, eher, mehr”, anche, Baista, che, Leonardo,\napprofonditamente, a, Venezia, nelle, vada, alla, aento, alle, dell’angelo:, deli-\ncatezza, punte, che, non, che, volare?, Jahren, angemahnte, drohenden, Fachkräe-\nmangel, Ingenieuren, ein\n• Milano, l’esempio, psicologia, (il, subito, autodistruggersi, solo, lunghissimo,\nSo, il, movimento, moto, sui, si, bellissima, occhio, all’insù, sono, sogno, lo,\nossessionava, quello, und, also, Mythos?\n• un, ür, MINT-Berufen\n• cura, restauro, arginato., gibt., perché, caurata, sich, auch, zu, nicht, richia-\nmano, acque, ricerche, chiave, anti-Turchi., nur\n114\nData:\nMixed script: Greek–Russian\n• ελληνική, γλώσσα, είναι, μία, από, τις, ινδοευρωπαϊκές, γλώσσες., Αποτελεί, το,\nμοναδικό, μέλος, ενός, ανεξάρτητου, κλάδου, της, ινδοευρωπαϊκής, οικογένειας,\nγλωσσών., Ανήκει, επίσης, στον, βαλκανικό, γλωσσικό, δεσμό., Στην, ελληνική,\nγλώσσα, έχουμε, γραπτά, κείμενα, από, τον, 15ο, αιώνα, μέχρι, σήμερα.\n• На, греческом, на, всех, его, существования, была, создана, богатейшая,\nгреческого, обязательным, всякого, образованного, большое, заимствова-\nний, а, в, греческом, новое, время, (наряду, новых, научных, терминов, на-\nзываемая, международная, слова, в, основном, двумя, через\n• Η, π.Χ.,языке, этапах, литература., В, Римской, империи, знание, языка,\nсчиталось, для, человека., В, латинском, языке, присутствует, количество,\nгреческих, —, значительное, количество, латинских, и, романских, слов., В,\nдревнегреческий, язык, стал, с, латинским), источником, создания, и, тех-\nнических, (так, лексика)., В, русский, язык, греческие, проникали, путями,\n—, международную, лексику, и, церковнославянский, язык.\nData:\nMixed script: English–Greek\n• is, biblical, is, will, is, without, self-beneﬁt)., is, feelings, feelings, it, be, feeling,\nbeing, high, is, by, his, is, by, will, mostly, sexual, ”intimate, well, reﬁned, his,\ndeﬁnition:, is, initially, felt, with, it, beauty, within, beauty, itself., use, ”with-\nout, helps, soul, beauty, spiritual, youthful, beauty, feel, suggesting, sensually,\nspiritual, ﬁnding, its, like, ﬁnding, all, seek\n• (ἀγάπη, (ἔρως\n• Agápe, ”love:, brotherly, love, love, of, God, for, of, for, in, known, ”love, 1, 13,\nthroughout, New, brotherly, love, aﬀection, good, love, love, given, or, not, per-\nson, continues, love, (even, in, for, one’s, for, spouse, refer, love, of, content, or,\nholding, one, in, unconditional, love, of, God, for, of, love, ”to, good, of, Éros,\n”love, of, e, Modern, Greek, word, love.”, own, Although, eros, for, person,\ncontemplation, becomes, of, person, or, even, becomes, of, not, of, of, love, of,\nword, mean, In, Symposium, work, on, subject, eros, knowledge, of, of, ”Form”,\nof, erotic, –, even, love, non-corporeal, of, is, Lovers, philosophers, through, of,\n• agápē), means, esp., charity;, the, man, and, man, God.”, Agape, used, the, pas-\nsage, as, the, chapter,”, Corinthians, and, described, there, and, the, Testament,\nas, and, benevolence., Whether, the, returned, the, to, any, Agape, also, used,\nancient, texts, to, denote, children, and, the, a, and, was, also, used, to, to, a,\nfeast., It, can, also, described, as, the, regard., Agape, used, Christians, to, express,\n115\nthe, children., type, was, further, explained, omas, Aquinas, as, the, another.”,\nérōs), means, the, passion.”, ”erotas”, means, It, can, also, apply, to, dating, re-\nlationships, as, as, marriage., Plato, a, an, appreciation, the, that, appreciation,\nPlato, does, talk, physical, araction, as, a, necessary, part, hence, the, the, pla-\ntonic, to, physical, araction.”, the, the, most, famous, ancient, the, Plato, has,\nSocrates, argue, that, the, recall, and, contributes, to, an, understanding, truth,\nthe, ideal, that, leads, us, humans, to, desire, thus, that, that, based, aspires, to,\nthe, plane, existence;, that, truth, just, any, truth, leads, to, transcendence., and,\nare, inspired, to, truth, the, means, eros.\nData:\nMixed script: English–Spanish–Arabic\n• ،الرمز، يعني، الرسم، الذي، يعبر، عن، شيء، معين، وعموما، فٔان، العلامة، ينبغي، ٔان، تنقل، رسالتها، بنظرة، واحدة، دون\nالحاجة، لاية، كلمات، و، من، المعروف، ٔان، قدماء، المصريين، والٔاغريق، ٔاستخدموا، العلامات، ولكن، ٔاكثر، من، استخدم،\nالعلامات، هم\n• ribbon, symbol, mourning., ribbon, mourning, El, un, y, un, en\n• black, is, a, of, remembrance, or, Wearing, or, displaying, a, black, has, been, used,\nfor, remembrance, tragedies, or, as, a, political, statement., crespón, negro, o, lazo,\nnegro, es, símbolo, utilizado, por, personas, estados, sociedades, organizaciones,\nrepresentando, sentimiento, político-social, señal, de, duelo.\n• A, POW/MIA\nData:\nMixed script: English–Chinese\n• e, Chinese, (simpliﬁed, traditional, Chinese:, invoked, motivational, speaking,\nbecause, the, composed, characters, that, represent, linguists, have, criticized,\nthis, usage, because, the, component, (simpliﬁed, Chinese:, traditional, Chinese:,\nhas, other, besides, Chinese, certain, some, be, based, the, Chinese, that, the, e,\nnumbers, believed, have, because, their, similar, words, that, have, positive\n• (不利)\n• Western, can, and, Some, meanings, In, are, number, name, and, are, meanings,\nnames, meanings.\n• 0, 6, 8, 9\n• ”crisis”, is, auspicious, inauspicious, sounds, sound\n• for, pinyin:, frequently, in, word, of, two, ”danger”, ”opportunity”., pronounced,\ntradition, by, or, on, word, to., to\n• 危机;, 危機;, wēijī), jī, 机;, 機), (吉利)\n116\nData:\nMixed script: Ukrainian–Russian\n• й, Русь, морей., Россией, Белоруссией, Польшей, Словакией, Венгрией, Ру-\nмынией\n• але, 9—13, юге, Имеет, Молдавией.\n• існували, інших\n• Чёрного, Азовского, границу\n• культури\n• території, України, пунктом, української, и, сухопутную, и\n• Віддавна, на, держави, скіфів, сарматів, готів, народів, відправним, держав-\nності, На, водами\n• та, вважається, Київська, століття., омывается, с\nData:\nPali: abbha\n• (nt.), nt., Sk., \\”dark, Idg., cp., Gr., Lat., Sk., water, Gr., water]., dark), at, SnA, S,\nat, It, Sn, (cp., SnA, Sn, S\n• &, A, A), ., J, 251)., 1, 1064;, 249, 250;, 12)., 64;, 348)., 382).\n• viz., 134., 101, 581, f., 289.\n• 53, 295, 273, 487, 3, 617, 317, 348, 239., 687\n• cloud\\”;, also, cloud, cloudy, <smallcaps> ii. </smallcaps>, =, list, is, <smallcaps>\ni. </smallcaps>, (°sama, <smallcaps> vi. </smallcaps>, (abbhaŋ, <smallcaps> iv.\n</smallcaps>, (nīl°, As, Dhs, DhsA, (used, (=, clouds, cloud, (also, as\n• m., adj.\n• abhra, (mahiyā, VvA, acchādesi);, Pv, PvA, \\”dull\\”;, valāhaka);, Vv, valāhaka–\nsikhara\n• <at>a)fro\\\\s</at>, froth, of, <superscript> 9 </superscript>, <superscript> s. </su-\nperscript>, <superscript> 1 </superscript>\n• later, scum, rain;, ambha, rain, a, Miln, (megho, Miln), nīla–megha, sense, expl,\n, Dh\n117\n• *m̊ bhro, <at>o)/mbros</at>, ambu, mass, to, obscure, moon–, <b>abbhaŋ, mahikā\n</b>, <b>dhū-, marajo</b>, <b>Rāhu</b>, pabbata, rajo, <b>abbhā</b>, by,\nperhaps, <b>abbhāmaa</b>, <br, /><b>–kūṭa</b>, or, summit, storm–cloud,\n<b>–ghana</b>, <b>–paṭala</b>, mass, <b>–mua</b>, from, abbhāmua, <b>\n–saŋvilāpa</b>,\n• [Vedic, imber, Oir, (dense, Vin, in, things, that, sunshine, is, referred, moun-\ntain, like, thunder–cloud);, the, point, thick, free, thundering\nData:\nPali: abhijjhitar\n• <smallcaps>i.</smallcaps>, v., l., <smallcaps>v.</smallcaps>\n• abhijjhita, abhijjhātar, °itar), °itar, °ātar).,\n• [n., ag., fr., med., M, 287, (T., =, A, 265\n• in, function], one, who, covets\nData:\nPali: ajja\n• Ajja，&, Ajjā, （adv.）, base, a3）, diva），thus, Dh.326；, ajjā；, v., PvA.59）；,\nPvA.6，23；, phrase, ajjatagge, ajjato, agge（?）, ajja-tagge，see, agga3）,（adv.）\n, the, 32，23., （Page\n• ‹-›, –kālaṁ\n• [Vedic, &, +, being, （see, （see, （read, as, （=, Mhvs, （=, +, Mhvs\n• of, of, “on, “food”）；\n• and, an, old, not\n• adya, adyā，a, dyā，a°, dyā, dyaus, day”], to-day，now, bahutaṁ, with, day,\n–divasa, day\n• demonstr., pron., Loc., this, Kern，Toev., s., Freq., or, from, this, onward，hence-\nforth, this, morning, present\n• Sn.75，153，158，970，998；, J.I，279；, III，425, Pv.I，117, idāni, 15，64., in,\nVin.I，18；, D.I，85；, DA.I，235., J.VI，180；, 10）\n118\nData:\nPali: gūhanā\n• Pug．19．Cp．pari°．（Page\n• Gūhanā，（f.）, [abstr．fr．gūhati]=gūhanā\n• 253）,（q．v.）\nData:\nPali: pacati\n• Vin．IV，264；, N．S．II，225，PvA．10，14．–, D．I，52\n• DA．I，159，where, 382）\n• in\n• at, &\n• cook，pέpwn, cook，boil，roast\n• Pacati，[Ved．pacati，Idg．*peqǔō，Av．pac-；, Obulg．peka, to, fry，roast，\nLith，kepū, ripe], to, ﬁg．torment, purgatory,（trs．and, pacitvā, aer, roasting,\nppr．pacanto, tormenting，Gen．pacato,（+Caus．pācayato）, read, pacato, for,\npaccato，by, pare, pp．pakka, Caus．pacāpeti, pāceti, Pass．paccati, to, roasted,\nor, tormented\n• bake，Gr．pέssw, intrs．）：Niraye,（expld, daṇḍena, pīḷentassa）．–,（q．v．）．\n‹-›, （q．v．）．–, be, （q．v．）．（Page\nNormalized data\n• pacati, peka, pέssw, pέpwn, pacitvā, ppr., pacanto, Gen., pacato, (+Caus., pācay-\nato), pacato, paccato, pare, pīḷentassa)., pp., pakka, Caus., pacāpeti, Pass., paccati\n• *peqǔō, bake\n• pac-;, 264;, 52, &, 382)\n• 10,14.–, 159, –, <->, –\n• fry, Niraye, I, I, by\n• Av., Obulg., Gr., (trs., D., DA., (q.v.)., (q.v.)., (q.v.).\n• [Ved., to, roast, kepū, cook, ripe], to, cook, roast, torment, purgatory, and, aer,\nroasting, tormenting, (expld, at, where, read, for, daṇḍena, pāceti, to, be, roasted,\nor, tormented, (Page\n• Pacati, Idg., Lith, boil, Vin.IV, ﬁg., in, intrs.):, in, N.S.II,225,PvA.\n119\nData:\nTwier 1 (Greek–English)\n• BUSINESS, EXCELLENCE.\n• Μόλις, ψήφισα, αυτή, τη, λύση, Internet, of, στο, διαγωνισμό\n• ings, IT\nData:\nTwier 2 (French–English)\n• Keynote, “e, collective, of, science-publish, or, perish;, it, all, that, counts?”\n• Demain, 18h, par\n• #dhiha6, David\n• @dhiparis, dynamics, is\nData:\nTwier 3 (French–English)\n• #FWWC2015\n• breuvages, go,\n• Food, Edmonton, to, for, the\n• in, waiting, #bilingualism\n• and, are, ready, just, fans\nData:\nTwier 4 (English–Polish)\n• comes, from, with, two, crates, of, strawberries, jackets, omg\n• my, dad, poland, and, adidas\n• back, żubrówka\nData:\nTwier 5 (Transliterated Amharic–English)\n• (coﬀee\n• bread). is, our\n• Buna, dabo, naw\n120\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2015-10-06",
  "updated": "2015-10-06"
}