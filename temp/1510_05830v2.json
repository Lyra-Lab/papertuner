{
  "id": "http://arxiv.org/abs/1510.05830v2",
  "title": "Unsupervised Ensemble Learning with Dependent Classifiers",
  "authors": [
    "Ariel Jaffe",
    "Ethan Fetaya",
    "Boaz Nadler",
    "Tingting Jiang",
    "Yuval Kluger"
  ],
  "abstract": "In unsupervised ensemble learning, one obtains predictions from multiple\nsources or classifiers, yet without knowing the reliability and expertise of\neach source, and with no labeled data to assess it. The task is to combine\nthese possibly conflicting predictions into an accurate meta-learner. Most\nworks to date assumed perfect diversity between the different sources, a\nproperty known as conditional independence. In realistic scenarios, however,\nthis assumption is often violated, and ensemble learners based on it can be\nseverely sub-optimal. The key challenges we address in this paper are:\\ (i) how\nto detect, in an unsupervised manner, strong violations of conditional\nindependence; and (ii) construct a suitable meta-learner. To this end we\nintroduce a statistical model that allows for dependencies between classifiers.\nOur main contributions are the development of novel unsupervised methods to\ndetect strongly dependent classifiers, better estimate their accuracies, and\nconstruct an improved meta-learner. Using both artificial and real datasets, we\nshowcase the importance of taking classifier dependencies into account and the\ncompetitive performance of our approach.",
  "text": "arXiv:1510.05830v2  [cs.LG]  23 Feb 2016\nUnsupervised Ensemble Learning with Dependent Classiﬁers\nAriel Jaﬀe1∗, Ethan Fetaya1, Boaz Nadler1, Tingting Jiang2 and Yuval Kluger2,3\n1Dept. of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot Israel 76100\n2Interdepartmental Program in Computational Biology and Bioinformatics, Yale University, New Haven, CT 06511\n3Dept. of Pathology and Yale Cancer Center, Yale University School of Medicine, New Haven, CT 06520, USA\nAbstract\nIn unsupervised ensemble learning, one obtains predictions from multiple sources or classi-\nﬁers, yet without knowing the reliability and expertise of each source, and with no labeled data\nto assess it. The task is to combine these possibly conﬂicting predictions into an accurate meta-\nlearner. Most works to date assumed perfect diversity between the diﬀerent sources, a property\nknown as conditional independence. In realistic scenarios, however, this assumption is often\nviolated, and ensemble learners based on it can be severely sub-optimal. The key challenges\nwe address in this paper are: (i) how to detect, in an unsupervised manner, strong violations of\nconditional independence; and (ii) construct a suitable meta-learner. To this end we introduce\na statistical model that allows for dependencies between classiﬁers. Our main contributions are\nthe development of novel unsupervised methods to detect strongly dependent classiﬁers, better\nestimate their accuracies, and construct an improved meta-learner. Using both artiﬁcial and\nreal datasets, we showcase the importance of taking classiﬁer dependencies into account and the\ncompetitive performance of our approach.\n1\nIntroduction\nIn recent years unsupervised ensemble learning has become increasingly popular. In multiple appli-\ncation domains one obtains the predictions, over a large set of unlabeled instances, of an ensemble of\ndiﬀerent experts or classiﬁers with unknown reliability. Common tasks are to combine these possibly\nconﬂicting predictions into an accurate meta-learner, as well as assessing the accuracy of the various\nexperts, both without any labeled data.\nA leading example is crowdsourcing, whereby a tedious labeling task is distributed to many\nannotators. Unsupervised ensemble learning is of increasing interest also in computational biology,\nwhere recent works in the ﬁeld propose to solve diﬃcult prediction tasks by applying multiple\nalgorithms and merging their results [1, 3, 7, 14].\nAdditional examples of unsupervised ensemble\nlearning appear, among others, in medicine [12] and decision science [17].\nPerhaps the ﬁrst to address ensemble learning in this fully unsupervised setup were Dawid and\nSkene [5]. A key assumption in their work was of perfect diversity between the diﬀerent classiﬁers.\nNamely, their labeling errors were assumed statistically independent of each other. This property,\nknown as conditional independence is illustrated in the graphical model of Fig. 1 (left). In [5],\nDawid and Skene proposed to estimate the parameters of the model, i.e.\nthe accuracies of the\ndiﬀerent classiﬁers, by the EM procedure on the non-convex likelihood function. With the increasing\npopularity of crowdsourcing and other unsupervised ensemble learning applications, there has been\n∗Email addresses: Ariel Jaﬀe: ariel.jaffe@weizmann.ac.il, Ethan Fetaya: ethan.fetaya@weizmann.ac.il,Boaz\nNadler:\nboaz.nadler@weizmann.ac.il,Tingting\nJiang:\ntingting.jiang@yale.edu,Yuval\nKluger:\nyuval.kluger@yale.edu\n1\na surge of interest in this line of work, and multiple extensions of it [11,18,20,22,23]. As the quality\nof the solution found by the EM algorithm critically depends on its starting point, several recent\nworks derived computationally eﬃcient spectral methods to suggest a good initial guess [2,9,10,15].\nDespite its popularity and usefulness, the model of Dawid and Skene has several limitations.\nOne notable limitation is its assumption that all instances are equally diﬃcult, with each classiﬁer\nhaving the same probability of error over all instances. This issue was addressed, for example, by\nWhitehill et. al. [23] who introduced a model of instance diﬃculty, and also by Tian et. al. [21]\nwho proposed a model where instances are divided into groups, and the expertise of each classiﬁer\nis group dependent.\nA second limitation, at the focus of our work, is the assumption of perfect conditional inde-\npendence between all classiﬁers. As we illustrate below, this assumption may be strongly violated\nin real-world scenarios. Furthermore, as shown in Sec. 5, neglecting classiﬁer dependencies may\nyield quite sub-optimal predictions. Yet, to the best of our knowledge, relatively few works have\nattempted to address this important issue.\nTo handle classiﬁer dependencies, Donmez et. al. [6] proposed a model with pairwise interactions\nbetween all classiﬁer outputs. However, they noted that empirically, their model did not yield more\naccurate predictions. Platanios et. al. [16] developed a method to estimate the error rates of either\ndependent or independent classiﬁers.\nTheir method is based on analyzing the agreement rates\nbetween pairs or larger subsets of classiﬁers, together with a soft prior on weak dependence amongst\nthem.\nThe present work is partly motivated by the ongoing somatic mutation DREAM (Dialogue for\nReverse Engineering Assessments and Methods) challenge, a sequence of open competitions for\ndetecting irregularities in the DNA string. This is a real-world example of unsupervised ensemble\nlearning, where participants in the currently open competition are given access to the predictions\nof more than 100 diﬀerent classiﬁers, over more than 100,000 instances.\nThese classiﬁers were\nconstructed by various labs worldwide, each employing their own biological knowledge and possibly\nproprietary labeled data. The task is to construct, in an unsupervised fashion, an accurate ensemble\nlearner.\nIn ﬁgure 2a we present the empirical conditional covariance matrix between diﬀerent classiﬁers\nin one of the databases of the DREAM challenge, for which ground truth labels have been disclosed.\nUnder the conditional independence assumption, the population conditional covariance between\nevery two classiﬁers should be exactly zero. Figure 2a, in contrast, exhibits strong dependencies\nbetween groups of classiﬁers.\nUnsupervised ensemble learning in the presence of possibly strongly dependent classiﬁers raises\nthe following two key challenges: (i) detect, in an unsupervised manner, strong violations of condi-\ntional independence; and (ii) construct a suitable meta-learner.\nTo cope with these challenges, in Sec. 2 we introduce a new model for the joint distribution of\nall classiﬁers which allows for dependencies between them through an intermediate layer of latent\nvariables. This generalizes the model of Dawid and Skene, and allows for groups of strongly correlated\nclassiﬁers, as observed for example in the DREAM data.\nIn Sec. 3 we devise a simple algorithm to detect subsets of strongly dependent classiﬁers using\nonly their predictions and no labeled data.\nThis is done by exploiting the structural low-rank\nproperties of the classiﬁers’ covariance matrix. Figure 2b shows our resulting estimate for deviations\nfrom conditional independence on the same data as ﬁgure 2a. Comparing the two ﬁgures illustrates\nthe ability of our method to detect strong dependencies with no labeled data.\nIn Sec. 4 we propose methods to better estimate the accuracies of the classiﬁers and construct an\nimproved meta-learner, both in the presence of strong dependencies between some of the classiﬁers.\nFinally, in Sec. 5 we illustrate the competitive performance of the modiﬁed ensemble-learner derived\nfrom our model on both artiﬁcial data, four datasets from the UCI repository and three datasets from\nthe DREAM challenge. These empirical results showcase the limitations of the strict conditional\nindependence model, and highlight the importance of modeling the statistical dependencies between\n2\nY\nf1\nfi\nfm\nψ1, η1\nψi, ηi\nψm, ηm\nY\nα1\nαk\nαK\nf1\nf2\nfi\nfi+1\nfm\nψα\n1 , ηα\n1\nψα\ni , ηα\ni\nψα\nm, ηα\nm\nFig. 1: (Left) The perfect conditional independence model of Dawid and Skene. All classiﬁers are\nindependent given the class label Y ; (Right) The generalized model considered in this work.\ndiﬀerent classiﬁers in unsupervised ensemble learning scenarios.\n2\nProblem Setup\nNotations.\nWe consider the following binary classiﬁcation problem. Let X be an instance space\nwith an output space Y = {−1, 1}. A labeled instance (x, y) ∈X × Y is a realization of the random\nvariable (X, Y ). The joint distribution p(x, y), as well as the marginals pX(x) and pY (y), are all\nunknown. We further denote by b the class imbalance of Y ,\nb = pY (1) −pY (−1).\n(1)\nLet {fi}m\ni=1 be a set of m binary classiﬁers operating on X. As our classiﬁcation problem is binary,\nthe accuracy of the i-th classiﬁer is fully characterized by its sensitivity ψi and speciﬁcity ηi,\nψi = Pr (fi(X) = 1|Y = 1)\nηi = Pr (fi(X) = −1|Y = −1) .\n(2)\nFor future use, we denote by πi its balanced accuracy, given by the average of its sensitivity and\nspeciﬁcity\nπi = 1\n2(ψi + ηi).\n(3)\nNote that when the class imbalance is zero, πi is simply the overall accuracy of the i-th classiﬁer.\nThe classical conditional independence model.\nIn the model proposed by Dawid and Skene\n[5], depicted in Fig. 1(left), all m classiﬁers were assumed conditionally independent given the class\nlabel. Namely, for any set of predictions a1, . . . , am ∈{±1}\nPr(f1 = a1, . . . , fm = am|Y ) =\nY\ni\nPr(fi = ai|Y ).\n(4)\nAs shown in [5], the maximum likelihood estimation (MLE) for y given the parameters ψi, ηi and b\nis linear in the predictions of f1, ..., fm\nˆy = sign\n m\nX\ni=1\nwifi(x) + w0\n!\n, wi = w(ψi, ηi).\n(5)\nHence, the main challenge is to estimate the model parameters ψi and ηi. A simple approach to\ndo so, as described in [9,15], is based on the following insight: A classiﬁer which is totally random\n3\nhas zero correlation with any other classiﬁer. In contrast, a high correlation between the predictions\nof two classiﬁers is a strong indication that both are highly accurate, assuming they are not both\nadversarial.\nIn many realistic scenarios, however, an ensemble may contain several strongly dependent clas-\nsiﬁers.\nSuch a scenario has several consequences: First, the above insight that high correlation\nbetween two classiﬁers implies that both are accurate breaks down completely. Second, as shown\nin Sec. 5, estimating the classiﬁers parameters ψi, ηi as if they were conditionally independent may\nbe highly inaccurate. Third, in contrast to Eq. (5), the optimal ensemble learner is in general\nnon-linear in the m classiﬁers. Applying the linear meta-classiﬁer of Eq. (5) may be suboptimal,\neven when provided with the true classiﬁer accuracies.\nA model for conditionally dependent classiﬁers.\nIn this paper we signiﬁcantly relax the\nconditional independence assumption.\nWe introduce a new model which allows classiﬁers to be\ndependent through unobserved latent variables, and develop novel methods to learn the model\nparameters and construct an improved non-linear meta-learner.\nIn contrast to the 2-layer model of Dawid and Skene, our proposed model, illustrated in Fig.\n1(right), has an additional intermediate layer with K ≤m latent binary random variables {αk}K\nk=1.\nIn this model, the unobserved αk are conditionally independent given the true label Y , whereas each\nobserved classiﬁer depends on Y only through a single and unknown latent variable. Classiﬁers that\ndepend on diﬀerent latent variables are thus conditionally independent given Y , whereas classiﬁers\nthat depend on the same latent variable may have strongly correlated prediction errors.\nEach\nhidden variable can thus be interpreted as a separate unobserved teacher, or source of information,\nand the classiﬁers that depend on it are diﬀerent perturbations of it.\nNamely, even though we\nobserve m predictions for each instance, they are in fact generated by a hidden model with intrinsic\ndimensionality K, where possibly K ≪m.\nLet us now describe in detail our probabilistic model. First, since the latent variables α1, . . . , αK\nfollow the classical model of Dawid and Skene, their joint distribution is fully characterized by the\nclass imbalance b and the 2K probabilities\nPr(αk = 1|Y = 1)\nand\nPr(αk = −1|Y = −1).\nNext, we introduce an assignment function c : [m] →[K], such that if classiﬁer fi depends on αk\nthen c(i) = k. The dependence of classiﬁer fi on the class label Y is only through its latent variable\nαc(i),\nPr(fi|αc(i), Y ) = Pr(fi|αc(i)).\n(6)\nHence, classiﬁers fi, fj with c(i) ̸= c(j) maintain the original conditional independence assump-\ntion of Eq. (4). In contrast, classiﬁers fi, fj with c(i) = c(j) are only conditionally independent\ngiven αc(i),\nPr(fi = ai, fj = aj|αc(i)) = Pr(fi = ai|αc(i)) Pr(fj = aj|αc(i)).\n(7)\nNote that if the number of groups K is equal to the number of classiﬁers, then all classiﬁers are\nconditionally independent, and we recover the original model of Dawid and Skene.\nSince the model now consists of three layers, the remaining parameters to describe it are the\nsensitivity ψα\ni and speciﬁcity ηα\ni of the i-th classiﬁer given its latent variable αc(i),\nψα\ni =Pr(fi = 1|αc(i) = 1),\nηα\ni =Pr(fi =−1|αc(i) =−1).\nBy Eq. (6), the overall sensitivity ψi of the i-th classiﬁer is related to ψα\ni and ηα\ni via\nψi = Pr(αc(i) = 1|Y = 1)ψα\ni + Pr(αc(i) = −1|Y = 1)(1 −ηα\ni ),\n(8)\nwith a similar expression for its overall speciﬁcity ηi.\n4\nRemark on Model Identiﬁability.\nNote that the model depicted in Fig. 1(right) is in general\nnot identiﬁable. For example, the classical model of Dawid and Skene can also be recovered with\na single latent variable K = 1, by having α1 = Y . Similarly, for a latent variable that has only a\nsingle classiﬁer dependent on it, the parameters ψi, ηi and ψα, ηα are non-identiﬁable. Nonetheless,\nthese non-identiﬁability issues do not aﬀect our algorithms, described below.\nProblem Formulation.\nWe consider the following totally unsupervised scenario. Let Z be a\nbinary m × n matrix with entries Zij = fi(xj), where fi(xj) is the label predicted by classiﬁer fi at\ninstance xj. We assume xj are drawn i.i.d. from pX(x). We also assume the m classiﬁers satisfy\nour generalized model, but otherwise we have no prior knowledge as to the number of groups K, the\nassignment function c or the classiﬁer accuracies (sensitivities ψi,ψα\ni and speciﬁcities ηi, ηα\ni ). Given\nonly the matrix Z of binary predictions and no labeled data, we consider the following problems:\n1. Is it possible to detect strongly dependent classiﬁers, and estimate the number of groups and\nthe corresponding assignment function c?\n2. Given a positive answer to the previous question, how can we estimate the sensitivities and\nspeciﬁcities of the m diﬀerent classiﬁers and construct an improved, possibly non-linear, meta\nlearner ?\n3\nEstimating the assignment function\nThe main challenge in our model is the ﬁrst problem of estimating the number of groups K and the\nassignment function c. Once c is obtained, we will see in Section 4 that our second problem can\nbe reduced to the conditional independent case, already addressed in previous works [9,10,15,25].\nIn principle, one could try to ﬁt the whole model by maximum likelihood, however this results in a\nhard combinatorial problem. We propose instead to ﬁrst estimate only K and c. We do so using\nthe low-rank structure of the covariance matrix of the classiﬁers, implied by our model.\nThe covariance matrix.\nLet R denote the m×m population covariance matrix of the m classiﬁers\nrij = E[(fi −E[fi])(fj −E[fj])].\n(9)\nThe following lemma describes its structure. It generalizes a similar lemma, for the standard\nDawid and Skene model, proven in [15]. The proof of this and other lemmas below appear in the\nappendix.\nLemma 1. There exists two vectors von, voff ∈Rm such that for all i ̸= j,\nrij =\n\u001a\nvoff\ni\n· voff\nj\nif c(i) ̸= c(j)\nvon\ni\n· von\nj\nif c(i) = c(j)\n(10)\nThe population covariance matrix is therefore a combination of two rank-one matrices. The\nblock diagonal elements i, j with c(i) = c(j) correspond to the rank-one matrix von(von)T , where on\nstands for on-block, while the oﬀ-block diagonal elements, with c(i) ̸= c(j) correspond to another\nrank-one matrix voff(voff)T . Let us deﬁne the indicator 1c(i, j)\n1c(i, j) =\n(\n1\nc(i) = c(j)\n0\notherwise\n(11)\nThe non-diagonal elements of R can thus be written as follows,\nrij = 1c(i, j)von\ni von\nj\n+ (1 −1c(i, j))voff\ni\nvoff\nj\n.\n(12)\n5\nLearning the model in the ideal setting.\nIt is instructive to ﬁrst examine the case where the\ndata is generated according to our model, and the population covariance matrix R is exactly known,\ni.e. n = ∞. The question of interest is whether it is possible to recover the assignment function in\nthis setting.\nTo this end, let us look at the possible values of the determinant of 2x2 submatrices of R,\nMijkl = det\n\u0012 rij\nril\nrkj\nrkl\n\u0013\n(13)\nDue to the low rank structure described in lemma 1, we have the following result, with the exact\nconditions appearing in the appendix.\nLemma 2. Assume the two vectors von and voff are suﬃciently diﬀerent, then Mijkl = 0 if and only\nif either: (i) Three or more of the indices i, j, k and l belong to the same group or (ii) c(i) ̸= c(j),\nc(j) ̸= c(k), c(k) ̸= c(l) and c(l) ̸= c(i).\nWith details in the appendix, comparing the indices (j, k, l) where M(i1, j, k, l) = 0 with i1 ﬁxed,\nto those where M(i2, j, k, l) = 0, we can deduce, in polynomial time, whether c(i1) = c(i2).\nLearning the model in practice.\nIn practical scenarios, the population covariance matrix R\nis unknown and we can only compute the sample covariance matrix ˆR. Furthermore, our model\nwould typically be only an approximation of the classiﬁers dependency structure. Given only ˆR,\nthe approach to recover the assignment function described above, based on exact matching of the\npattern of zeros of the determinants of various 2x2 submatrices is clearly not applicable.\nIn principle, since E[ ˆR] = R a standard approach would be to deﬁne the following residual\n∆(von, voff, c) =\nX\ni̸=j\n1c(i, j)(von\ni von\nj\n−ˆrij)2 + (1 −1c(i, j))(voff\ni\nvoff\nj\n−ˆrij)2,\n(14)\nand ﬁnd its global minimum. Unfortunately, as stated in the following lemma and proven in the\nappendix, in general this is not a simple task.\nLemma 3. Minimizing the residual of Eq. (14) for a general covariance matrix ˆR is NP-hard.\nIn light of Lemma 3, we now present a tractable algorithm to estimate K and c and provide\nsome theoretical support for it. Our algorithm is inspired by the ideal setting which highlighted\nthe importance of the determinants of 2 × 2 submatrices. To detect pairs of classiﬁers fi, fj that\nstrongly violate the conditional independence assumption, we thus compute the following score\nmatrix ˆS = ˆS( ˆR),\nˆsij =\nX\nk,l̸=i,j\n|ˆrijˆrkl −ˆrilˆrkj|.\n(15)\nThe idea behind the score matrix is the following: Consider the score matrix S computed with the\npopulation covariance R. Lemma 2 characterized the cases where the submatrices in Eq. (15) are\nof rank-one, and hence their determinant is zero. When c(i) ̸= c(j) most submatrices come from\nfour diﬀerent groups, i.e. will have rank one, and thus the sum sij will be small. On the other hand,\nwhen c(i) = c(j) many submatrices will not be rank one and thus sij will be large, assuming no\ndegeneracy between von and voff. As ˆS\nn→∞\n−−−−→S, large values of ˆsij serve as an indication of strong\nconditional dependence between classiﬁers fi and fj.\nThe following lemma provides some theoretical justiﬁcation for the utility of the score matrix S\ncomputed with the population covariance, in recovering the assignment function c. For simplicity, we\nanalyze the ’symmetric’ case where the class imbalance b = 0, Pr(αk = −1|y = −1) = Pr(αk = 1|y =\n6\nAlgorithm 1 Estimating the assignment function c and vectors von, voff\n1: Estimate the covariance matrix R (9).\n2: Obtain the score matrix by (15)\n3: for all 1 < k < m do\n4:\nEstimate c by performing spectral clustering with the Laplacian of the score matrix.\n5:\nUse the clustering function to estimate the two vectors von, voff.\n6:\nCalculate residual by (14).\n7: end for\n8: Pick the assignment function and vectors which yield minimal residual.\n1) and all groups have equal size of m/K. We measure deviation from conditional independence by\nthe following matrices of conditional covariances C+ and C−,\nc+\nij\n=\nE[(fi −E[fi])(fj −E[fj])|Y = 1]\nc−\nij\n=\nE[(fi −E[fi])(fj −E[fj])|Y = −1].\n(16)\nFinally, we assume there is a δ > 0 such that the balanced accuracies of all classiﬁers satisfy\n(2πi −1) > δ > 0.\nLemma 4. Under the assumptions described above, if c(i) = c(j) then\nsij > m2\n\u0012\n1 −3\nK\n\u0013\nδ2|c+\nij| = m2\n\u0012\n1 −3\nK\n\u0013\nδ2|c−\nij|.\n(17)\nIn contrast, if c(i) ̸= c(j) then\nsij < 2m2\nK\n\u0012\n1 −2\nK\n\u0013\n.\n(18)\nAn immediate corollary from lemma 4, is that if the classiﬁers are suﬃciently accurate, and their\ndependencies within each group are strong enough then the score matrix exhibits a clear gap with\nmax\nc(i)̸=c(j) Sij <\nmin\nc(i)=c(j) Sij. In this case, even a simple single-linkage hierarchical clustering algorithm\ncan recover the correct assignment function from S. In practice, as only ˆS is available, we apply\nspectral clustering which is more robust, and works better in practice.\nWe illustrate the usefulness of the score matrix using the DREAM challenge S1 dataset, which\ncontains m = 124 classiﬁers. Fig. 2a shows the matrix of conditional covariance 1\n2(C+ + C−) of Eq.\n(16), computed using the ground truth labels. Fig. 2b shows the score matrix ˆS computed using only\nthe classiﬁers predictions. We also plot the values of the score matrix vs. the conditional covariance\nin ﬁgure 3. Clearly, a high score is a reliable indication for strong conditional dependencies between\nclassiﬁers.\nIt is important to note that the time complexity needed to build the score matrix S is O(m4).\nWhile quartic scaling is usually considered too expensive, in our case as the number m of classiﬁers\nin many real world problems is in the hundreds our algorithm can run on these datasets in less than\nan hour. This can be sped-up, for example, by sampling the elements of S instead of computing the\nfull matrix [8].\nEstimating the assignment function c.\nWe estimate c by spectral clustering the score matrix\nˆS of Eq. (15). As the number of clusters or groups K is unknown, we choose the one which minimizes\nthe residual function deﬁned in Eq. (14). The steps for estimating the number of groups K and the\nassignment function c are summarized in Algorithm 1. Note that retrieving von and voff from the\n7\n \n \n20\n40\n60\n80\n100\n120\n20\n40\n60\n80\n100\n120\n−0.05\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n(a)\nThe\nconditional\ncovariance\nmatrix\n1\n2(C+ + C−)\nof\nthe\nDREAM\ndataset\nS1,\ncomputed using the ground truth labels.\n \n \n20\n40\n60\n80\n100\n120\n20\n40\n60\n80\n100\n120\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n(b) The score matrix\nˆS of the DREAM S1\ndataset, computed from the matrix of classiﬁer\npredictions. For visualization purposes, the up-\nper limit of the above score matrix is ﬁxed at\n300.\nFig. 2\n0.05\n0.1\n0.15\n0.2\n0.25\n50\n100\n150\n200\n250\n300\n1\n2(C+ + C−)\nS\nFig. 3: Values of S vs. the corresponding conditional covariance matrix 1\n2(C++C−) for the DREAM\ndataset S1. The blue dots represent the mean value, the upper and lower red dots represent the\n20th and 80th quantiles, respectively.\ncovariance matrix is a rank-one matrix completion problem, for which several solutions exist, for\nexample see [4]. Also note that while we compute spectral clustering for various number of clusters,\nthe costly eigen-decoposition step only needs to be done once.\n4\nThe latent spectral meta learner\nEstimating the model parameters.\nGiven estimates of K and of the assignment function c,\nestimating the remaining model parameters can be divided into two stages: (i) Estimating the sen-\nsitivity and speciﬁcity of the diﬀerent classiﬁers given the latent variables αk: ψα\ni , ηα\ni (ii) Estimating\nthe probabilities associated with the latent variables, Pr(αk = 1|Y = 1) and Pr(αk = −1|Y = −1).\n8\nAlgorithm 2 Estimate model parameters\n1: Input: Matrix of predictions fi(xj), parameters K and c.\n2: for k = 1, .., K do\n3:\nFind all classiﬁers fi where c(i) = k\n4:\nEstimate ψα\ni , ηα\ni and E[αk]\n5:\nEstimate the latent values αk(xj), ∀j = 1, ..., n\n6: end for\n7: Estimate Pr(αk = 1|Y = 1), Pr(αk = −1|Y = −1)\nThe key observation is that in each of these stages the underlying model follows the classical\nconditional independent model of [5]. In particular, classiﬁers with a common latent variable are\nconditionally independent given its value. Similarly the K latent variables themselves are condition-\nally independent given the true label Y . Thus, we can solve the two stages sequentially by any of\nthe various methods already developed for the Dawid and Skene model. In our implementation, we\nused the spectral meta learner proposed in [9], whose code is publicly available. A pseudo-code for\nthis process appears in Algorithm 2.\nLabel Predictions.\nOnce all the parameters of the model are known, for each instance x we\nestimate its label by maximum likelihood\nˆy = argmax\ny=±1\nPr(f1(x), . . . , fm(x)|y).\n(19)\nFollowing our generative model, Fig.\n1(right), the above probability is a function of the model\nparameters b, ψα\ni , ηα\ni , ψα, ηα, and the assignment function c.\nClassiﬁer selection.\nIn some cases, it is required to construct a sparse ensemble learner which\nuses only a small subset of at most M out of the available m classiﬁers. This problem of selecting\na small subset of classiﬁers, known as ensemble pruning, has mostly been studied in supervised\nsettings, see [13,19,24].\nUnder the conditional independence assumption, the best subset simply consists of the M most\naccurate classiﬁers. In our model, in contrast, the correlations between the classiﬁers have to be\ntaken into account. Assuming the required number of classiﬁers is smaller than the number of groups\nM ≤K, a simple approach is to select the M most accurate classiﬁers under the constraint that\nthey all come from diﬀerent groups. This creates a balance between accuracy and diversity.\n5\nExperiments\nWe demonstrate the performance of the latent variable model on artiﬁcial data, on datasets from\nthe UCI repository and on the ICGA-TCGA dream challenge.\nThroughout our experiments, we compare the performance of the following unsupervised ensem-\nble methods: (1) Majority voting, which serves as a baseline; (2) SML+EM - a spectral meta-learner\nbased on the independence assumption [9] which provides an initial guess, followed by EM iterations;\n(3) Oracle-CI: A linear meta-learner based on Eq. (5), which assumes conditional independence\nbut is given the exact accuracies of all the individual classiﬁers. (4) L-SML (latent SML), the new\nalgorithm presented in this work.\nFor the artiﬁcial data, we also present the performance of its oracle meta-learner, denoted\nOracle-L, which is given the exact structure and parameters of the model, and predicts the la-\nbel Y by maximum likelihood.\n9\n2\n4\n6\n8\n10\n0.6\n0.65\n0.7\n0.75\n0.8\n|G1|\nπens\n \n \nVote\nSML+EM\nOracle−CI\nL−SML\nOracle−L\nFig. 4:\nSimulated data:\nEnsemble learner\nbalanced accuracy vs. the size of group 1.\nMedian\nMax\nVote\nSML\nOracle\nL−SML\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\nπ\nFig. 5: UCI magic dataset, a comparison of\nfour unsupervised ensemble learners.\n5.1\nArtiﬁcial Data\nTo validate our theoretical analysis, we generated artiﬁcial binary data according to our assumed\nmodel, on a balanced classiﬁcation problem with b = 0. We generated an ensemble of m = 20\nclassiﬁers with n = 104 instances. All the parameters of the ensemble were chosen uniformly at\nrandom from the following intervals: Pr(α = 1|Y = 1), Pr(α = −1|Y = −1) ∈[0.5, 0.8], {ψα\ni , ηα\ni } ∈\n[0.7, 0.9]. We consider the case where there is only one group G1 of correlated classiﬁers, with the\nremaining m −|G1| classiﬁers all conditionally independent. The size of the correlated group |G1|\nincreases from 1 to 10. Note that for |G1| = 1 all classiﬁers are conditionally independent. Fig. 4\ncompares the balanced accuracy of the ﬁve unsupervised ensemble learners described above, as a\nfunction of the size of the ﬁrst group, |G1|. As can be seen in Fig. 4, up to |G1| = 6, the ensemble\nlearner based on the concept of correlated classiﬁers achieves similar results to the optimal classiﬁer\n(’oracle-L’). As expected from Lemma 4, as |G1| increases, it is harder to correctly estimate c with\nthe score matrix.\nA complementary graph which presents the probability to recover the correct assignment function\nas a function of |G1| appears in the appendix. As expected, the degradation in performance starts\nwhen the algorithm fails to correctly estimate the model structure.\n5.2\nUCI data sets\nWe applied our algorithms on various binary classiﬁcation problems using 4 datasets from the UCI\nrepository: Magic, Spambase, Miniboo and Musk. Our ensemble of m = 16 classiﬁers consists of\n4 random forests, 3 logistic model trees, 4 SVM and 5 naive Bayes. Each classiﬁer was trained on\na separate, randomly chosen labeled dataset. In our unsupervised ensemble scenario we had access\nonly to their predictions on a large independent test set.\nWe present results for the magic dataset, which contains 19000 instances with 11 attributes. The\ntask is to classify each instance as background or high energy gamma rays. Further details and\nresults on the other datasets appear in the appendix.\nAs seen in Fig. 5, the L-SML improves substantially over the standard SML, and even on the\noracle classiﬁer that assumes conditional independence.\nOur method also outperforms the best\nindividual classiﬁer.\nIn the appendix we show the conditional covariance matrix, Fig. 8, and our assignment, Fig. 9.\nIt can be observed that strongly dependent classiﬁers are indeed grouped together correctly.\n10\nMean\nBest\nVote\nSML+\nEM\nOracle-\nCI\nL-SML\nS1\n6.1\n1.7\n2.8\n1.7\n1.7\n1.6\nS2\n8.7\n1.8\n4.0\n2.8\n2.8\n2.3\nS3\n8.3\n2.5\n4.3\n2.3\n2.3\n1.8\nTable 1: Balanced error of meta-classiﬁers based on the full ensemble. For reference, the ﬁrst two\ncolumns give the mean and smallest balanced error of all classiﬁers.\nVote\nSML+EM\nOracle-CI\nL-SML\nS1\n3.2\n2.3\n1.9\n2.0\nS2\n4.3\n4.1\n2.5\n2.8\nS3\n2.9\n2.9\n2.8\n2.5\nTable 2: Balanced error of sparse meta-classiﬁers.\n5.3\nThe DREAM mutation calling challenge\nThe ICGC-TCGA DREAM challenge is an international eﬀort to improve standard methods for\nidentifying cancer-associated mutations and rearrangements in whole-genome sequencing (WGS)\ndata. This publicly available database contains both real and synthetic in-silico tumor instances.\nThe database contains 14 diﬀerent datasets, each with over 100,000 instances.\nParticipants in the currently open competition are given access to the predictions of about a\nhundred diﬀerent classiﬁers (denoted there as pipe-lines)1. These classiﬁers were constructed by\nvarious labs worldwide, each employing their own biological knowledge and possibly proprietary\nlabeled data. The two current challenges are to construct a meta-learner, by using either (1) all m\nclassiﬁers; or (2) at most ﬁve of them. We evaluate the performance of the diﬀerent meta-classiﬁers\nfmc by their balanced error,\n1 −π = 1\n2(Pr(fmc = 1|y = −1) + Pr(fmc = −1|y = 1)).\nBelow we present results on the datasets S1, S2 and S3 for which ground-truth labels have been\nreleased.\nChallenge I.\nThe balanced errors of the diﬀerent meta-learners, constructed using all m classiﬁers,\nare given in table 1.\nThe L-SML method outperforms the other meta-learners in all the three\ndatasets. On the S3 dataset, it reduces the balanced error by more than 20 % over competing meta\nlearners.\nChallenge II\nHere the goal is to construct a sparse meta-learner based on at most ﬁve individual\nclassiﬁers from the ensemble. For the methods based on the Dawid and Skene model (SML+EM,\nvoting and Oracle-CI), we took the 5 classiﬁers with the highest estimated (or known) balanced\naccuracies. For our model, since the estimated number of groups is larger than ﬁve, we ﬁrst took the\nbest classiﬁer from each group, and then chose the ﬁve classiﬁers with highest estimated balanced\naccuracies.\nFor all methods, the ﬁnal prediction was made by a simple vote of the ﬁve chosen\nclassiﬁers. Though potentially sub-optimal, we nonetheless chose it as our purpose was to compare\nthe diversity of the diﬀerent classiﬁers.\nThe results presented in table 2 show that our method outperforms voting and SML, and are\nsimilar to those achieved by the oracle learner.\n1The data can be downloaded from the challenge website http://dreamchallenges.org/\n11\n6\nAcknowledgments\nThis research was funded in part by the Intel Collaborative Research Institute for Computational\nIntelligence (ICRI-CI). Y.K. is supported by the National Institutes of Health Grant R0-1 CA158167,\nand R0-1 GM086852.\n12\nReferences\n[1] N. Aghaeepour, G. Finak, H. Hoos, T.R. Mosmann, R. Brinkman, R. Gottardo, R.H. Scheuer-\nmann, FlowCAP Consortium, and DREAM Consortium. Critical assessment of automated ﬂow\ncytometry data analysis techniques. Nature methods, 10(3):228–238, 2013.\n[2] A. Anandkumar, R. Ge, D. Hsu, S.M. Kakade, and M. Telgarsky. Tensor decompositions for\nlearning latent variable models. Journal of Machine Learning Research, 15:2773–2832, 2014.\n[3] P.C. Boutros, A.A. Margolin, J.M. Stuart, A. Califano, and G. Stolovitzky.\nToward bet-\nter benchmarking: challenge-based methods assessment in cancer genomics. Genome biology,\n15(9):462, 2014.\n[4] E.J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of\nComputational Mathematics, 9:717–772, 2009.\n[5] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the\nEM algorith. Journal of the Royal Statistical Society. Series C, 28:20–28, 1979.\n[6] P. Donmez, G. Lebanon, and K. Balasubramanian. Unsupervised supervised learning i: Es-\ntimating classiﬁcation and regression errors without labels. Journal of Machine Learning Re-\nsearch, 11:1323–1351, 2010.\n[7] A.D. Ewing, K.E. Houlahan, Y. Hu, K. Ellrott, C. Caloian, T.N. Yamaguchi, J.Ch. Bare,\nC. P’ng, D. Waggott, and V.Y. Sabelnykova. Combining tumor genome simulation with crowd-\nsourcing to benchmark somatic single-nucleotide-variant detection. Nature methods, 12:623,\n2015.\n[8] E. Fetaya, O. Shamir, and S. Ullman. Graph approximation and clustering on a budget. 18th\nconference on artiﬁcial intelligence and statistics, 2015.\n[9] A. Jaﬀe, B. Nadler, and Y. Kluger. Estimating the accuracies of multiple classiﬁers without\nlabeled data. In 18th conference on artiﬁcial intelligence and statistics, pages 407–415, 2015.\n[10] P. Jain and S. Oh. Learning mixtures of discrete product distributions using spectral decom-\npositions. Journal of Machine Learning Research, 35:1–33, 2014.\n[11] D.R. Karger, S. Oh, and D. Shah. Budget-optimal crowdsourcing using low-rank matrix ap-\nproximations. In IEEE Alerton Conference on Communication, Control and Computing, pages\n284–291, 2011.\n[12] J. A. Lee. Click to cure. The Lancet Oncology, 2013.\n[13] G. Martinez-Muoz, D. Hern´andez-Lobato, and A. Suarez. An analysis of ensemble pruning\ntechniques based on ordered aggregation. Pattern Analysis and Machine Intelligence, IEEE\nTransactions on, 31(2):245–259, 2009.\n[14] M. Micsinai, F. Parisi, F. Strino, P. Asp, B.D. Dynlacht, and Y. Kluger. Picking chip-seq peak\ndetectors for analyzing chromatin modiﬁcation experiments. Nucleic acids research, 2012.\n[15] F. Parisi, F. Strino, B. Nadler, and Y. Kluger. Ranking and combining multiple predictors\nwithout labeled data. Proceedings of the National Academy of Sciences, 111:1253–1258, 2014.\n[16] E.A. Platanios, A. Blum, and T. Mitchell.\nEstimating accuracy from unlabeled data.\nIn\nUncertainty in Artiﬁcial Intelligence, 2014.\n13\n[17] A.J. Quinn. Crowdsourcing decision support: frugal human computation for eﬃcient decision\ninput acquisition. PhD thesis, 2014.\n[18] V.C. Raykar, Y. Shipeng, L.H. Zhao, G.H. Valdez, C. Florin, L. Bogoni, and Moy L. Learning\nfrom crowds. J. Machine Learning Research, 11:1297–1322, 2010.\n[19] L. Rokach. Collective-agreement-based pruning of ensembles. Computational Statistics & Data\nAnalysis, 53(4):1015–1026, 2009.\n[20] A. Sheshadri and M. Lease. Square: A benchmark for research on computing crowd concensus.\nIn AAAI conference on human computation and crowdsourcing, 2013.\n[21] Tian Tian and Jun Zhu. Uncovering the latent structures of crowd labeling. In Advances in\nKnowledge Discovery and Data Mining, pages 392–404. Springer, 2015.\n[22] P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds.\nIn Advances in Neural Information Processing Systems 23 (NIPS 2010), 2010.\n[23] J. Whitehill, P. Ruvolo, T. Wu, J Bergsma, and J.R. Movellan. Whose vote should count\nmore: Optimal integration of labels from labelers of unknown expertise. In Advances in Neural\nInformation Processing Systems 22 (NIPS 2009), 2009.\n[24] Xu-Cheng Yin, Kaizhu Huang, Chun Yang, and Hong-Wei Hao. Convex ensemble learning with\nsparsity and diversity. Information Fusion, 2014.\n[25] Y. Zhang, X. Chen, D. Zhou, and M.I. Jordan. Spectral methods meet EM: A provably optimal\nalgorithm for crowdsourcing. In Advances in Neural Information Processing Systems, volume 27,\npages 1260–1268, 2014.\n14\nA\nProof of Lemma 1\nThis proof is based on the following lemma, which appears in [15]:\nIf two classiﬁers fi, fj are conditionally independent given the class label Y , then the covariance\nbetween them is equal to,\nrij = (1 −b2)(ψi + ηi −1)(ψj + ηj −1).\n(20)\nIn our model, if c(i) ̸= c(j), then fi, fj are indeed conditionally independent (Fig. 1,right). The\nﬁrst part of lemma 1 follows directly from Eq. (20), with voff\ni\n=\n√\n1 −b2(ψi + ηi −1).\nTo prove the second part of lemma 1, we note that according to our model, two classiﬁers fi, fj\nwith c(i) = c(j) are conditionally independent given the value of their latent variable α. Therefore,\nwe can treat α as the class label, and apply Eq. (20) with b replaced by the expectation of α, and\nthe sensitivity and speciﬁcity ψi, ηi replaced by ψα\ni , ηα\ni respectively. Hence, Eq. (20) becomes,\nrij = (1 −E[α]2)(ψα\ni + ηα\ni −1)(ψα\nj + ηα\nj −1) = von\ni von\nj ,\n(21)\nwhere von\ni\n=\np\n1 −E[α]2(ψα\ni + ηα\ni −1).\nB\nProof of Lemma 2\nWe assume that von and voff are suﬃciently diﬀerent in the following precise sense: We require that\nfor all 4 distinct indices i, j, k, l, von\ni\n· von\nj\n· von\nk · von\nl\n̸= voff\ni\n· voff\nj\n· voff\nk\n· voff\nl\n.\nNext, we elaborate on the relation between voff\ni\nand von\ni . Let us denote by ψy\nα, ηy\nα the sensitivity\nand speciﬁcity of the latent variable α. Let fi be a classiﬁer that depends on α. Applying Bayes\nrule, its overall sensitivity and speciﬁcity is given by,\nψi = ψy\nαψα\ni + (1 −ψy\nα)(1 −ηα\ni )\nηi = ηy\nαηα\ni + (1 −ηy\nα)(1 −ψα\ni ).\n(22)\nAdding ψi and ηi we get the following,\nψi + ηi −1 = (ψy\nα + ηy\nα −1)(ψα\ni + ηα\ni −1).\n(23)\nIf c(i) = c(j) we have the following dependency between (voff\ni\n, voff\nj\n) and (von\ni , von\nj ),\n\"\nvoff\ni\nvoff\nj\n#\n=\np\n1 −b2(ψy\nα + ηy\nα −1)\n\u0014 (ψα\ni + ηα\ni −1)\n(ψα\nj + ηα\nj −1)\n\u0015\n=\ns\n1 −b2\n1 −E[α]2 (ψy\nα + ηy\nα −1)\n\u0014 von\ni\nvon\nj\n\u0015\n. (24)\nIt follows that two elements voff\ni\n, voff\nj\nwhere c(i) = c(j) are linearly dependent with the correspond-\ning elements of von\ni , von\nj . This fact shall be useful in proving the lemma.\nTo prove lemma 2 we analyze all various possibilities for the group assignments of the four indices\ni, j, k, l of\nM(i, j, k, l) = det\n\u0012 rij\nril\nrkj\nrkl\n\u0013\n.\n1. c(i) = c(j) = c(k) = c(l): In this case M(i, j, k, l) = von\ni von\nj von\nk von\nl\n−von\ni von\nl von\nk von\nj\n= 0.\n2. c(i) ̸= c(j), and c(j) ̸= c(k), and c(k) ̸= c(l) and c(l) ̸= c(i): Here M(i, j, k, l) = voff\ni\nvoff\nj\nvoff\nk\nvoff\nl\n−\nvoff\ni\nvoff\nl\nvoff\nk\nvoff\nj\n= 0.\n15\n3. c(i) = c(l) = c(k) ̸= c(j): M(i, j, k, l) = voff\ni\nvoff\nj\nvon\nk von\nl −von\ni von\nl voff\nk\nvoff\nj\n= voff\nj\nvon\nl\n\u0010\nvoff\ni\nvon\nk −von\ni voff\nk\n\u0011\n.\nFrom the linear dependency shown in Eq. (24).\n\u0010\nvoff\ni\nvon\nk −von\ni voff\nk\n\u0011\n= 0.\n4. c(i) = c(j), c(k) = c(l) and c(i) ̸= c(k): M(i, j, k, l) = von\ni von\nj von\nk von\nl\n−voff\ni\nvoff\nl\nvoff\nk\nvoff\nj\n̸= 0\nfrom our assumption.\nIt can be seen that Mijkl is equal to zero only if either three or more of the indices are equal (cases\n(1) and (2)) or all four pairs which appear in the determinant belong to diﬀerent groups (case (3)).\nC\nAlgorithm for the ideal setting\nAn immediate conclusion from lemma 2, is that the indices i, j, k and l for which M(i, j, k, l) = 0\ndepend only on the assignment function.\nThis means we can compare the pattern of zeros for\nM(i1, j, k, l) and M(i2, j, k, l) to decide if fi1 and fi2 belong to the same group. If c(i1) = c(i2)\nthen M(i1, j, k, l) = 0 ⇐⇒M(i2, j, k, l) = 0. On the other hand if c(i1) ̸= c(i2) and at least one\nof the indices i1 and i2 , w.l.o.g i1, belongs to a group with more than one element, then we can\nﬁnd j, k and l such that M(i1, j, k, l) ̸= 0 but M(i2, j, k, l) = 0. This occurs when c(i1) = c(j), and\nc(i2) ̸= c(j) ̸= c(k) ̸= c(l).\nThis means that by comparing the pattern of zeros, we can recover the assignment function.\nNotice, that according to the algorithm, all singleton classiﬁers, that is, classiﬁers who are condi-\ntionally independent with the rest of the ensemble, are grouped together under a common latent\nvariable. This is not a problem, as our model is not unique and this is an equivalent probabilistic\nmodel, when the latent variable being identical to Y .\nAlgorithm 3 Check if c(i1) = c(i2)\n1: Initialize (m −2) × (m −3) × (m −4) arrays T1, T2 to zero\n2: for j ̸= k ̸= l ̸= i1, i2 do\n3:\nif ri1jrkl −ri1lrkj = 0 then (T1(j, k, l) = 1)\n4:\nend if\n5:\nif ri2jrkl −ri2lrkj = 0 then (T2(j, k, l) = 1)\n6:\nend if\n7: end for\n8: if (T1 = T2) then\n9:\nc(i1) = c(i2).\n10: else\n11:\nc(i1) ̸= c(i2).\n12: end if\nD\nMinimizing ∆is a NP hard problem\nWe prove lemma 3 for the case of K = 2 clusters and known vvvoff,vvvon vectors. Our goal is to ﬁnd a\nminimizer for the following residual:\nˆc = argmin\nc\n∆(c) = argmin\nc\nX\ni,j\n1c(i, j)(von\ni von\nj\n−rij)2 + (1 −1c(i, j))(voff\ni\nvoff\nj\n−rij)2\n(25)\nFor the case of K = 2 we can simplify the residual considerably. Let us deﬁne a vector xxx ∈{−1, 1}m\nwhere xi = 1 if c(i) = 1 and xi = −1 if c(i) = 2. We can replace the indicator function 1(i, j) with\n16\nthe following,\n1(i, j) = (1 + xixj)\n2\n,\n1 −1(i, j) = (1 −xixj)\n2\n.\n(26)\nIn addition, we can replace the minimization over c with a minimization over xxx,\nˆxxx\n=\nargmin\nxxx\nX\ni,j\n(1 + xixj)\n2\n(von\ni von\nj\n−rij)2 + (1 −xixj)\n2\n(voff\ni\nvoff\nj\n−rij)2\n=\nargmin\nxxx\nX\ni,j\n1\n2\n\u0010\n(von\ni von\nj\n−rij)2 + (voff\ni\nvoff\nj\n−rij)2\u0011\n+xixj\n2\n\u0010\n(von\ni von\nj\n−rij)2 + (voff\ni\nvoff\nj\n−rij)2\u0011\n.\n(27)\nThe ﬁrst term does not depend on xxx and we can omit it from the minimization problem. Let us also\ndeﬁne the matrix ˜R,\n˜rij =\n\u0010\n(von\ni von\nj\n−rij)2 + (voff\ni\nvoff\nj\n−rij)2\u0011\n2\n(28)\nWe are left with the following minimization problem:\nˆxxx = argmin\nxxx\nX\ni,j\nxixj ˜rij = argmin\nxxx\nxxx′ ˜Rxxx\n(29)\nIf there is a binary vector whose residual is precisely zero, then it can be found by computing the\neigenvector with smallest eigenvalue of the matrix ˜R. If, however, the minimal residual is not zero,\nthen eq. (29) is a quadratic optimization problem involving discrete variables, which is well known\nto be a NP-hard problem.\nE\nProof of Lemma 4\nWe start by proving the ﬁrst part of the lemma, where c(i) = c(j). The score matrix sij is a sum\nof all possible 2 × 2 determinants,\nsi,j =\nX\nk,l̸=i,j\n|rijrkl −rilrjk| =\nX\nk,l̸=i,j\nskl\nij ,\n(30)\nwhere we deﬁne skl\nij as a single score element. The following table separates the group of skl\nij score\nelements into three types, and states the number of elements in each type.\nElement type\nNumber of elements\nc(i) = c(j) ̸= c(k) ̸= c(l)\nm2 \u00001 −3\nK +\n2\nK2\n\u0001\nc(i) = c(j) = c(k) ̸= c(l)\nm2 \u0000 1\nK −2\nm\n\u0001 \u00001 −1\nm\n\u0001\nc(i) = c(j) = c(k) = c(l)\nm2 \u0000 1\nK −2\nm\n\u0001 \u0000 1\nK −3\nm\n\u0001\nAccording to lemma 1, the contribution to the score from elements of the second and third type\nis exactly 0 (see details in Sec. B). We will therefore focus on analyzing the score elements of the ﬁrst\ntype, where c(i) = c(j) ̸= c(k) ̸= c(l). Recall, that we assume a symmetrical case where b = 0, and\nPr(α = 1|y = 1) = Pr(α = −1|y = −1). These assumptions imply that E[αk] = 0 for all k = 1...K.\nLet us consider Lem. 1 in order to analyze the value of skl\nij,\nskl\nij\n=\n|rijrkl −rijrjk| = |(2πα\ni −1)(2πα\nj −1)(2πk −1)(2πl −1) −(2πi −1)(2πj −1)(2πk −1)(2πl −1)|\n=\n|(2πk −1)(2πl −1)\n\u0000(2πα\ni −1)(2πα\nj −1) −(2πi −1)(2πj −1)\n\u0001\n|\n(31)\n17\nwhere πα\ni = 1\n2(ψα\ni + ηα\ni ). For simplicity of notation, let us denote by γ the ratio of true positives\nand negatives of the latent variables:\nγ = Pr(αk = 1|Y = 1) = Pr(αk = −1|Y = −1)\n(32)\nIt can easily be shown that the following holds:\n(2πi −1) = (2γ −1)(2πα\ni −1)\n(2πj −1) = (2γ −1)(2πα\nj −1)\n(33)\nInserting (33) into (31) we get,\nskl\nij = |(2πk −1)(2πl −1)(2πα\ni −1)(2πα\nj −1)(1 −(2γ −1)2)| =\n|4(2πk −1)(2πl −1)(2πα\ni −1)(2πα\nj −1)(γ(1 −γ))|\n(34)\nLet us now derive the values of the conditional covariance matrices C+, C−. In order to obtain\nC+, we can apply the ﬁrst part of Lem.1, and replace the class imbalance b, which is the mean value\nof Y , with E[α|Y = 1]. A similar argument applies to C−. The value for the conditional expectation\nof α is equal to,\nE[α|Y = 1] = 2γ −1\nE[α|Y = −1] = 1 −2γ\n(35)\nA simple derivation yields the following for both cases,\n(1 −E[α|Y = 1]2) = (1 −E[α|Y = −1]2) = 4γ(1 −γ)\n(36)\nThe value of c+\nij is therefor equal to c−\nij, and both are equal to the following,\nc+\nij = c−\nij = 4γ(1 −γ)(2πα\ni −1)(2πα\nj −1)\n(37)\nInserting (37) into (34) we get the following,\nskl\nij = |(2πk −1)(2πl −1)c+\nij| = |(2πk −1)(2πl −1)c−\nij|\n(38)\nWe will remain with C+ for simplicity, The total score contribution of the ﬁrst type of elements is\ntherefore,\nX\nk,l\nskl\nij = |c+\nij|\nX\nk,l\n|(2πk −1)(2πl −1)|\n(39)\nAssuming (2πi −1) > δ > 0, ∀i, the latter simpliﬁes to,\nsij > |c+\nij|δ2m2(1 −3\nK +\n2\nK2 ) > |c+\nij|δ2m2(1 −3\nK )\n(40)\nWe next turn to proving an upper bound when c(i) ̸= c(j). Once again we can separate the\ndiﬀerent elements into three types,\nElement type\nNumber of elements\nc(i) ̸= c(j) ̸= c(k) ̸= c(l)\nm2 \u00001 −5\nK +\n6\nK2\n\u0001\nc(i) ̸= c(j) = c(k) ̸= c(l)\n2m2 \u0000 1\nK −1\nm\n\u0001 \u00001 −2\nK\n\u0001\nc(i) ̸= c(j) = c(k) = c(l)\nm2 \u0000 1\nK −2\nm\n\u0001 \u0000 1\nK −3\nm\n\u0001\nThe only contribution comes from the second type, as according to our model, if all indices come\nfrom diﬀerent groups, or if three come from the same group, the determinant is equal to 0 (see.\nB). In addition, since (2πi −1) > δ > 0 ∀i, the values of rij are positive for all (i, j) pairs. Since\n0 < rij ≤1 for all score elements skl\nij = |rijrkl −rilrkj| ≤1. The total value of sij is bounded by the\nfollowing\nsij ≤2m2\n\u0012 1\nK −1\nm\n\u0013 \u0012\n1 −2\nK\n\u0013\n< 2m2\nK\n\u0012\n1 −2\nK\n\u0013\n(41)\n18\n2\n4\n6\n8\n10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n|G1|\nPr(ˆc = c)\nFig. 6: Probability of estimating the exact\nassignment function c as a function of the\nsize of the correlated group |G1|\n2\n4\n6\n8\n10\n−0.005\n0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n0.035\n0.04\n|G1|\nMSE({ψ,η}m\ni=1)\n \n \nVote\nSML+EM\nL−SML\nFig. 7: A comparison of the mean squared\nerror in estimating the accuracies of the dif-\nferent classiﬁers in the ensemble.\nF\nAdditional results\nF.1\nArtiﬁcial data\nIn Fig. 6 we present the probability of our spectral clustering based algorithm to recover both the\ncorrect number of classes K and the correct assignment function c, as a function of |G1|. Up to\n|G1| = 6, our algorithm successfully estimates c, with no errors. When |G1| > 7, the algorithm\ncompletely fails. The degradation in performance presented in Fig. 4, corresponds to the point\nwhere the algorithm fails to estimate c correctly.\nIn Fig. 7 we present the mean squared error (MSE) of the sensitivity and speciﬁcity estimation\nfor the ensemble of classiﬁers, as a function of |G1|, deﬁned as\nMSE({ψ, η}m\ni=1) =\n1\n2m\nm\nX\ni=1\n\u0000( ˆψi −ψi)2 + (ˆηi −ηi)2\u0001\n.\n(42)\nWe compare the following three methods: (1) Majority vote ; (2) SML+EM; (3)L-SML. It can\nbe seen that the performance of the SML degrades very fast when the conditional independence\nassumption is violated. The performance of the L-SML is almost perfect up to the point where\n|G1| = 6, where as we have seen in Fig. 6, the model is correctly estimated. The performance is still\nsuperior to other methods, even for large values of |G1|.\nF.2\nUCI results\nFor the magic dataset, Fig. 8 presents the conditional covariance matrix 1\n2(C+ + C−), which is\nunknown to us. The group of SVM classiﬁers (12-16) are highly dependent, as well as the group of\nnaive Bayes classiﬁers (8-11). The groups of random forest classiﬁers and logistic model trees are\nweakly dependent.\nFig. 9 presents an example of the estimated assignment function ˆc for the same dataset. The\ngroups of SVM classiﬁers were assigned together, as well as the naive Bayes classiﬁers. Except for a\nsingle pair, the random forest and logistic model trees were assigned to separate groups.\nIn ﬁgures 10a,10b and 10c we present the results for the following 3 additional datasets from the\nUCI repository:\n• Musk dataset - detection of certain types of molecules.\n• Spam dataset - detection of spam from regular mail.\n19\n \n \n2\n4\n6\n8\n10\n12\n14\n16\n2\n4\n6\n8\n10\n12\n14\n16\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFig. 8: Magic database - conditional covari-\nance matrix 1\n2(C+ + C−).\n \n \n2\n4\n6\n8\n10\n12\n14\n16\n2\n4\n6\n8\n10\n12\n14\n16\n0\n0.2\n0.4\n0.6\n0.8\n1\nFig. 9: Magic database - The estimated group\nreturn by our algorithm.\n• Miniboo dataset - detection of electron neutrinos (signal) from muon neutrinos (background).\nThe base classiﬁers are identical to the ones used for the Magic dataset: (1) 4 random forest (2)\n3 Logistic Model Trees (3) 4 SVM (4) 5 naive Bayes .\nIn ﬁgures 10a-10d, the x-axis is the L-SML balanced error, and the y-axis is the SML balanced\nerror. The results of multiple experiments, each time with the classiﬁers constructed using diﬀerent\nrandom subset of labeled examples, are presented as blue dots, while the red line represents the\ny = x line, i.e. when the error of the L-SML and SML are the same. For the Magic dataset, ﬁgure\n10d, we add two lines which represent 2% and 4% improvement over the standard SML.\nWe can see in the ﬁgures that the improvement due to explicit modeling of possible classiﬁer\ndependencies is consistent across all datasets. The amount of improvement changes, however from\ndataset to dataset. The following table presents a summary of the diﬀerent properties of the datasets\ntogether with the average improvement in the balanced accuracy between the two methods.\nDataset\nNumber of instances\nnumber of features\nMean diﬀerence\nMagic\n19000\n11\n4%\nSpam\n4600\n57\n0.5%\nMiniboo\n130000\n50\n0.2%\nMusk\n6600\n168\n4.7%\n20\n0.1\n0.105\n0.11\n0.115\n0.12\n0.125\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\n1 −πSML\n1 −πL−SML\n(a) UCI Musk dataset, a comparison between the\nbalanced error of the SML and L-SML.\n0.056\n0.058\n0.06\n0.062\n0.045\n0.05\n0.055\n0.06\n0.065\n1 −πSML\n1 −πL−SML\n(b) UCI Spambase dataset, a comparison be-\ntween the balanced error of the SML and L-SML.\n0.1\n0.105\n0.11\n0.115\n0.12\n0.09\n0.095\n0.1\n0.105\n0.11\n0.115\n0.12\n0.125\n1 −πSML\n1 −πL−SML\n(c) UCI Miniboo dataset, a comparison between\nthe balanced error of the SML and L-SML.\n0.19\n0.195\n0.2\n0.205\n0.21\n0.215\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n1 −πens\n1 −πens\n(d) UCI Magic dataset. The magenta and green\nlines represent 2% and 4% balanced accuracy im-\nprovement over the SML results.\nFig. 10\n21\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2015-10-20",
  "updated": "2016-02-23"
}