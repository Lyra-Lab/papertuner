{
  "id": "http://arxiv.org/abs/1511.03855v2",
  "title": "Feature Learning based Deep Supervised Hashing with Pairwise Labels",
  "authors": [
    "Wu-Jun Li",
    "Sheng Wang",
    "Wang-Cheng Kang"
  ],
  "abstract": "Recent years have witnessed wide application of hashing for large-scale image\nretrieval. However, most existing hashing methods are based on hand-crafted\nfeatures which might not be optimally compatible with the hashing procedure.\nRecently, deep hashing methods have been proposed to perform simultaneous\nfeature learning and hash-code learning with deep neural networks, which have\nshown better performance than traditional hashing methods with hand-crafted\nfeatures. Most of these deep hashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application\nscenario with pairwise labels, there have not existed methods for simultaneous\nfeature learning and hash-code learning. In this paper, we propose a novel deep\nhashing method, called deep pairwise-supervised hashing(DPSH), to perform\nsimultaneous feature learning and hash-code learning for applications with\npairwise labels. Experiments on real datasets show that our DPSH method can\noutperform other methods to achieve the state-of-the-art performance in image\nretrieval applications.",
  "text": "Feature Learning based Deep Supervised Hashing with Pairwise Labels\nWu-Jun Li, Sheng Wang and Wang-Cheng Kang\nNational Key Laboratory for Novel Software Technology\nDepartment of Computer Science and Technology, Nanjing University, China\nliwujun@nju.edu.cn, wangs@lamda.nju.edu.cn, kwc.oliver@gmail.com\nAbstract\nRecent years have witnessed wide application of\nhashing for large-scale image retrieval. However,\nmost existing hashing methods are based on hand-\ncrafted features which might not be optimally com-\npatible with the hashing procedure. Recently, deep\nhashing methods have been proposed to perform\nsimultaneous feature learning and hash-code learn-\ning with deep neural networks, which have shown\nbetter performance than traditional hashing meth-\nods with hand-crafted features. Most of these deep\nhashing methods are supervised whose supervised\ninformation is given with triplet labels. For another\ncommon application scenario with pairwise labels,\nthere have not existed methods for simultaneous\nfeature learning and hash-code learning.\nIn this\npaper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH),\nto perform simultaneous feature learning and hash-\ncode learning for applications with pairwise labels.\nExperiments on real datasets show that our DPSH\nmethod can outperform other methods to achieve\nthe state-of-the-art performance in image retrieval\napplications.\n1\nIntroduction\nWith the explosive growing of data in real applications\nlike image retrieval, approximate nearest neighbor (ANN)\nsearch [Andoni and Indyk, 2006] has become a hot research\ntopic in recent years.\nAmong existing ANN techniques,\nhashing has become one of the most popular and effective\ntechniques due to its fast query speed and low memory\ncost [Kulis and Grauman, 2009; Gong and Lazebnik, 2011;\nKong and Li, 2012; Liu et al., 2012; Rastegari et al., 2013;\nHe et al., 2013; Lin et al., 2014; Shen et al., 2015; Kang et\nal., 2016].\nExisting hashing methods can be divided into data-\nindependent methods and data-dependent methods [Gong\nand Lazebnik, 2011; Kong and Li, 2012].\nIn data-\nindependent\nmethods,\nthe\nhash\nfunction\nis\ntypically\nrandomly generated which is independent of any training\ndata. The representative data-independent methods include\nlocality-sensitive hashing (LSH) [Andoni and Indyk, 2006]\nand its variants.\nData-dependent methods try to learn the\nhash function from some training data, which is also called\nlearning to hash (L2H) methods [Kong and Li, 2012].\nCompared with data-independent methods, L2H methods\ncan achieve comparable or better accuracy with shorter hash\ncodes. Hence, L2H methods have become more and more\npopular than data-independent methods in real applications.\nThe L2H methods can be further divided into two cat-\negories [Kong and Li, 2012; Kang et al., 2016]:\nunsu-\npervised methods and supervised methods.\nUnsupervised\nmethods only utilize the feature (attribute) information of\ndata points without using any supervised (label) informa-\ntion during the training procedure. Representative unsuper-\nvised methods include iterative quantization (ITQ) [Gong\nand Lazebnik, 2011], isotropic hashing (IsoHash) [Kong\nand Li, 2012], discrete graph hashing (DGH) [Liu et al.,\n2014], and scalable graph hashing (SGH) [Jiang and Li,\n2015]. Supervised methods try to utilize supervised (label)\ninformation to learn the hash codes. The supervised infor-\nmation can be given in three different forms: point-wise\nlabels, pairwise labels and ranking labels. Representative\npoint-wise label based methods include CCA-ITQ [Gong and\nLazebnik, 2011], supervised discrete hashing (SDH) [Shen\net al., 2015] and the deep hashing method in [Lin et al.,\n2015]. Representative pairwise label based methods include\nsequential projection learning for hashing (SPLH) [Wang\net al., 2010], minimal loss hashing (MLH) [Norouzi and\nFleet, 2011], supervised hashing with kernels (KSH) [Liu et\nal., 2012], two-step hashing (TSH) [Lin et al., 2013], fast\nsupervised hashing (FastH) [Lin et al., 2014], latent factor\nhashing (LFH) [Zhang et al., 2014], convolutional neural\nnetwork hashing (CNNH) [Xia et al., 2014], and column sam-\npling based discrete supervised hashing (COSDISH) [Kang\net al., 2016]. Representative ranking label based methods\ninclude ranking-based supervised hashing (RSH) [Wang et\nal., 2013b], column generation hashing (CGHash) [Li et\nal., 2013], order preserving hashing (OPH) [Wang et al.,\n2013a], ranking preserving hashing (RPH) [Wang et al.,\n2015], and some deep hashing methods [Zhao et al., 2015a;\nLai et al., 2015; Zhang et al., 2015]\nAlthough a lot of hashing methods have been proposed\nas shown above, most existing hashing methods, including\nsome deep hashing methods [Salakhutdinov and Hinton,\n2009; Masci et al., 2014; Liong et al., 2015], are based on\narXiv:1511.03855v2  [cs.LG]  21 Apr 2016\nhand-crafted features.\nIn these methods, the hand-crafted\nfeature construction procedure is independent of the hash-\ncode and hash function learning procedure, and then the\nresulted features might not be optimally compatible with\nthe hashing procedure. Hence, these existing hand-crafted\nfeature based hashing methods might not achieve satisfactory\nperformance in practice. To overcome the shortcoming of\nexisting hand-crafted feature based methods, some feature\nlearning based deep hashing methods [Zhao et al., 2015a;\nLai et al., 2015; Zhang et al., 2015] have recently been\nproposed to perform simultaneous feature learning and hash-\ncode learning with deep neural networks, which have shown\nbetter performance than traditional hashing methods with\nhand-crafted features. Most of these deep hashing methods\nare supervised whose supervised information is given with\ntriplet labels which are a special case of ranking labels.\nFor another common application scenario with pairwise\nlabels, there have appeared few feature learning based\ndeep hashing methods.\nTo the best of our knowledge,\nCNNH [Xia et al., 2014] is the only one which adopts deep\nneural network, which is actually a convolutional neural\nnetwork (CNN) [LeCun et al., 1989], to perform feature\nlearning for supervised hashing with pairwise labels. CNNH\nis a two-stage method. In the ﬁrst stage, the hash codes are\nlearned from the pairwise labels, and then the second stage\ntries to learn the hash function and feature representation\nfrom image pixels based on the hash codes from the ﬁrst\nstage. In CNNH, the learned feature representation in the\nsecond stage cannot give feedback for learning better hash\ncodes in the ﬁrst stage.\nHence, CNNH cannot perform\nsimultaneous feature learning and hash-code learning, which\nstill has limitations. This has been veriﬁed by the authors of\nCNNH themselves in another paper [Lai et al., 2015].\nIn this paper, we propose a novel deep hashing method,\ncalled deep pairwise-supervised hashing (DPSH), for ap-\nplications with pairwise labels. The main contributions of\nDPSH are outlined as follows:\n• DPSH is an end-to-end learning framework which con-\ntains three key components. The ﬁrst component is a\ndeep neural network to learn image representation from\npixels. The second component is a hash function to map\nthe learned image representation to hash codes. And\nthe third component is a loss function to measure the\nquality of hash codes guided by the pairwise labels. All\nthe three components are seamlessly integrated into the\nsame deep architecture to map the images from pixels\nto the pairwise labels in an end-to-end way.\nHence,\ndifferent components can give feedback to each other in\nDPSH, which results in learning better codes than other\nmethods without end-to-end architecture.\n• To the best of our knowledge, DPSH is the ﬁrst method\nwhich can perform simultaneous feature learning and\nhash-code learning for applications with pairwise labels.\n• Experiments on real datasets show that DPSH can out-\nperform other methods to achieve the state-of-the-art\nperformance in image retrieval applications.\n2\nNotation and Problem Deﬁnition\n2.1\nNotation\nWe use boldface lowercase letters like z to denote vectors.\nBoldface uppercase letters like Z are used to denote matrices.\nThe transpose of Z is denoted as ZT . ∥· ∥2 is used to denote\nthe Euclidean norm of a vector. sgn(·) denotes the element-\nwise sign function which returns 1 if the element is positive\nand returns -1 otherwise.\n2.2\nProblem Deﬁnition\nSuppose we have n points (images) X = {xi}n\ni=1 where xi\nis the feature vector of point i. xi can be the hand-crafted\nfeatures or the raw pixels in image retrieval applications.\nThe speciﬁc meaning of xi can be easily determined from\nthe context. Besides the feature vectors, the training set of\nsupervised hashing with pairwise labels also contains a set of\npairwise labels S = {sij} with sij ∈{0, 1}, where sij = 1\nmeans that xi and xj are similar, sij = 0 means that xi and\nxj are dissimilar. Here, the pairwise labels typically refer to\nsemantic labels provided with manual effort.\nThe goal of supervised hashing with pairwise labels is to\nlearn a binary code bi ∈{−1, 1}c for each point xi, where\nc is the code length. The binary codes B = {bi}n\ni=1 should\npreserve the similarity in S. More speciﬁcally, if sij = 1, the\nbinary codes bi and bj should have a low Hamming distance.\nOtherwise if sij = 0, the binary codes bi and bj should have\na high Hamming distance. In general, we can write the binary\ncode as bi = h(xi) = [h1(xi), h2(xi), · · · , hc(xi)]T , where\nh(xi) is the hash function to learn.\n3\nModel and Learning\nMost existing pairwise label based supervised hashing meth-\nods, including SPLH [Wang et al., 2010], MLH [Norouzi\nand Fleet, 2011], KSH [Liu et al., 2012], TSH [Lin et al.,\n2013], FastH [Lin et al., 2014], and LFH [Zhang et al., 2014],\nadopt hand-crafted features for hash function learning. As\nstated in Section 1, these methods cannot achieve satisfactory\nperformance because the hand-crafted features might not\nbe optimally compatible with the hash function learning\nprocedure. CNNH [Xia et al., 2014] adopts CNN to perform\nfeature learning from raw pixels. However, CNNH is a two-\nstage method which cannot perform simultaneous feature\nlearning and hash-code learning in an end-to-end way.\nIn this section, we introduce our model, called deep\npairwise-supervised hashing (DPSH), which can perform\nsimultaneous feature learning and hash-code learning in an\nend-to-end framework.\n3.1\nModel\nFigure 1 shows the end-to-end deep learning architecture for\nour DPSH model, which contains the feature learning part\nand the objective function part.\nFeature Learning Part\nOur DPSH model contains a CNN model from [Chatﬁeld\net al., 2014] as a component. More speciﬁcally, the feature\nlearning part has seven layers which are the same as those of\nCNN-F in [Chatﬁeld et al., 2014]. Other CNN architectures,\nConvolutions\nPooling\nConvolutions Pooling\nWeight Sharing\nPairwise \nSimilarity\nFeature Learning Part\nObjective Function Part\n-1 1-1 1-1 1\n1-1 1-1 1-1\n-1-1 1-1 1-1\n-1 1-1 1-1 1\nBinary Code\nFigure 1: The end-to-end deep learning architecture for DPSH.\nsuch as the AlexNet [Krizhevsky et al., 2012], can also be\nused to substitute the CNN-F network in our DPSH model.\nBut it is not the focus of this paper to study different networks.\nHence, we just use CNN-F for illustrating the effectiveness\nof our DPSH model, and leave the study of other candidate\nnetworks for future pursuit. Please note that there are two\nCNNs (top CNN and bottom CNN) in Figure 1. These two\nCNNs have the same structure and share the same weights.\nThat is to say, both the input and loss function are based on\npairs of images.\nThe detailed conﬁguration of the feature learning part of\nour DPSH model is shown in Table 1.\nMore speciﬁcally,\nit contains 5 convolutional layers (conv 1-5) and 2 fully-\nconnected layers (full 6-7).\nEach convolutional layer is\ndescribed in several aspects: “ﬁlter” speciﬁes the number\nof convolution ﬁlters and their receptive ﬁeld size, denoted\nas “num x size x size”; “stride” indicates the convolution\nstride which is the interval at which to apply the ﬁlters\nto the input; “pad” indicates the number of pixels to add\nto each side of the input; “LRN” indicates whether Local\nResponse Normalization (LRN) [Krizhevsky et al., 2012] is\napplied; “pool” indicates the downsampling factor. “4096”\nin the fully-connected layer indicates the dimensionality of\nthe output.\nThe activation function for all layers is the\nREctiﬁcation Linear Unit (RELU) [Krizhevsky et al., 2012].\nTable 1: Conﬁguration of the feature learning part in DPSH.\nLayer\nConﬁguration\nconv1\nﬁlter 64x11x11, stride 4x4, pad 0, LRN, pool 2x2\nconv2\nﬁlter 256x5x5, stride 1x1, pad 2, LRN, pool 2x2\nconv3\nﬁlter 256x3x3, stride 1x1, pad 1\nconv4\nﬁlter 256x3x3, stride 1x1, pad 1\nconv5\nﬁlter 256x3x3, stride 1x1, pad 1, pool 2x2\nfull6\n4096\nfull7\n4096\nObjective Function Part\nGiven the binary codes B = {bi}n\ni=1 for all the points, we\ncan deﬁne the likelihood of the pairwise labels S = {sij} as\nthat of LFH [Zhang et al., 2014]:\np(sij|B) =\n\u001a σ(Ωij),\nsij = 1\n1 −σ(Ωij),\nsij = 0\nwhere Ωij = 1\n2bT\ni bj, and σ(Ωij) =\n1\n1+e−Ωij . Please note\nthat bi ∈{−1, 1}c.\nBy taking the negative log-likelihood of the observed\npairwise labels in S, we can get the following optimization\nproblem:\nmin\nB J1 = −log p(S|B) = −\nX\nsij∈S\nlog p(sij|B)\n= −\nX\nsij∈S\n(sijΩij −log(1 + eΩij)).\n(1)\nIt is easy to ﬁnd that the above optimization problem can\nmake the Hamming distance between two similar points as\nsmall as possible, and simultaneously make the Hamming\ndistance between two dissimilar points as large as possible.\nThis exactly matches the goal of supervised hashing with\npairwise labels.\nThe problem in (1) is a discrete optimization problem,\nwhich is hard to solve.\nLFH [Zhang et al., 2014] solves\nit by directly relaxing {bi} from discrete to continuous,\nwhich might not achieve satisfactory performance [Kang et\nal., 2016].\nIn this paper, we design a novel strategy which can solve\nthe problem in (1) in a discrete way. First, we reformulate the\nproblem in (1) as the following equivalent one:\nmin\nB,U J2 = −\nX\nsij∈S\n(sijΘij −log(1 + eΘij))\n(2)\ns.t.\nui = bi,\n∀i = 1, 2, · · · , n\nui ∈Rc×1,\n∀i = 1, 2, · · · , n\nbi ∈{−1, 1}c,\n∀i = 1, 2, · · · , n\nwhere Θij = 1\n2uT\ni uj, and U = {ui}n\ni=1.\nTo optimize the problem in (2), we can optimize the follow-\ning regularized problem by moving the equality constraints\nin (2) to the regularization terms:\nmin\nB,U J3 = −\nX\nsij∈S\n(sijΘij −log(1 + eΘij))\n+ η\nn\nX\ni=1\n∥bi −ui∥2\n2,\nwhere η is the regularization term (hyper-parameter).\nDPSH Model\nTo integrate the above feature learning part and objective\nfunction part into an end-to-end framework, we set\nui = WT φ(xi; θ) + v,\nwhere θ denotes all the parameters of the seven layers in\nthe feature learning part, φ(xi; θ) denotes the output of the\nfull7 layer associated with point xi, W ∈R4096×c denotes a\nweight matrix, v ∈Rc×1 is a bias vector. It means that we\nconnect the feature learning part and the objective function\npart into the same framework by a fully-connected layer, with\nthe weight matrix W and bias vector v. After connecting the\ntwo parts, the problem for learning becomes:\nmin\nB,W,v,θ J = −\nX\nsij∈S\n(sijΘij −log(1 + eΘij))\n(3)\n+ η\nn\nX\ni=1\n∥bi −(WT φ(xi; θ) + v)∥2\n2.\nAs a result, we get an end-to-end deep hashing model,\ncalled DPSH, to perform simultaneous feature learning and\nhash-code learning in the same framework.\n3.2\nLearning\nIn the DPSH model, the parameters for learning contain W,\nv, θ and B. We adopt a minibatch-based strategy for learning.\nMore speciﬁcally, in each iteration we sample a minibatch of\npoints from the whole training set, and then perform learning\nbased on these sampled points.\nWe design an alternating method for learning. That is to\nsay, we optimize one parameter with other parameters ﬁxed.\nThe bi can be directly optimized as follows:\nbi = sgn(ui) = sgn(WT φ(xi; θ) + v).\n(4)\nFor the other parameters W, v and θ, we use back-\npropagation (BP) for learning. In particular, we can compute\nthe derivatives of the loss function with respect to ui as\nfollows:\n∂J\n∂ui\n=1\n2\nX\nj:sij∈S\n(aij −sij)uj + 1\n2\nX\nj:sji∈S\n(aji −sji)uj\n+ 2η(ui −bi),\nwhere aij = σ( 1\n2uT\ni uj).\nThen, we can update the parameters W, v and θ by\nutilizing back propagation:\n∂J\n∂W = φ(xi; θ)( ∂J\n∂ui\n)T ,\n(5)\n∂J\n∂v = ∂J\n∂ui\n,\n(6)\n∂J\nφ(xi; θ) = W ∂J\n∂ui\n.\n(7)\nThe whole learning algorithm of DPSH is brieﬂy summa-\nrized in Algorithm 1.\n3.3\nOut-of-Sample Extension\nAfter we have completed the learning procedure, we can only\nget the hash codes for points in the training data. We still need\nto perform out-of-sample extension to predict the hash codes\nfor the points which are not appeared in the training set.\nThe deep hashing framework of DPSH can be naturally\napplied for out-of-sample extension. For any point xq /∈X,\nwe can predict its hash code just by forward propagation:\nbq = h(xq) = sgn(WT φ(xq; θ) + v).\n(8)\n4\nExperiment\nAll our experiments for DPSH are completed with MatCon-\nvNet [Vedaldi and Lenc, 2015] on a NVIDIA K80 GPU\nserver. Our model can be trained at the speed of about 290\nimages per second with a single K80 GPU.\nAlgorithm 1 Learning algorithm for DPSH.\nInput:\nTraining images X = {xi}n\ni=1 and a set of pairwise labels\nS = {sij}.\nOutput:\nThe parameters W, v, θ and B.\nInitialization: Initialize θ with the CNN-F model; Initialize\neach entry of W and v by randomly sampling from a Gaussian\ndistribution with mean 0 and variance 0.01.\nREPEAT\nRandomly sample a minibatch of points from X, and for each\nsampled point xi, perform the following operations:\n• Calculate φ(xi; θ) by forward propagation;\n• Compute ui = WT φ(xi; θ) + v;\n• Compute the binary code of xi with bi = sgn(ui).\n• Compute derivatives for point xi according to (5), (6) and\n(7);\n• Update the parameters W, v, θ by utilizing back propaga-\ntion;\nUNTIL a ﬁxed number of iterations\n4.1\nDatasets and Setting\nWe compare our model with several baselines on two widely\nused benchmark datasets: CIFAR-10 and NUS-WIDE.\nThe CIFAR-10 [Krizhevsky, 2009] dataset consists of\n60,000 32×32 color images which are categorized into 10\nclasses (6000 images per class). It is a single-label dataset in\nwhich each image belongs to one of the ten classes.\nThe NUS-WIDE dataset [Chua et al., 2009; Zhao et al.,\n2015b] has nearly 270,000 images collected from the web. It\nis a multi-label dataset in which each image is annotated with\none or mutiple class labels from 81 classes. Following [Lai et\nal., 2015], we only use the images associated with the 21 most\nfrequent classes. For these classes, the number of images of\neach class is at least 5000.\nWe compare our method with several state-of-the-art hash-\ning methods.\nThese methods can be categorized into ﬁve\nclasses:\n• Unsupervised hashing methods with hand-crafted fea-\ntures, including SH [Weiss et al., 2008] and ITQ [Gong\nand Lazebnik, 2011].\n• Supervised hashing methods with hand-crafted features,\nincluding SPLH [Wang et al., 2010], KSH [Liu et al.,\n2012], FastH [Lin et al., 2014], LFH [Zhang et al.,\n2014], and SDH [Shen et al., 2015].\n• The above unsupervised methods and supervised meth-\nods with deep features extracted by the CNN-F of the\nfeature learning part in our DPSH.\n• Deep hashing methods with pairwise labels, including\nCNNH [Xia et al., 2014].\n• Deep hashing methods with triplet labels, including\nnetwork\nin\nnetwork\nhashing\n(NINH)\n[Lai\net\nal.,\n2015],\ndeep\nsemantic\nranking\nbased\nhashing\n(DSRH)\n[Zhao\net\nal.,\n2015a],\ndeep\nsimilarity comparison hashing (DSCH) [Zhang et\nal., 2015] and deep regularized similarity comparison\nhashing (DRSCH) [Zhang et al., 2015].\nFor hashing methods which use hand-crafted features, we\nrepresent each image in CIFAR-10 by a 512-dimensional\nGIST vector. And we represent each image in NUS-WIDE\nby a 1134-dimensional low level feature vector, including\n64-D color histogram, 144-D color correlogram, 73-D edge\ndirection histogram, 128-D wavelet texture, 225-D block-\nwise color moments and 500-D SIFT features.\nFor deep hashing methods, we ﬁrst resize all images to be\n224×224 pixels and then directly use the raw image pixels\nas input. We adopt the CNN-F network which has been pre-\ntrained on the ImageNet dataset [Russakovsky et al., 2014]\nto initialize the ﬁrst seven layers of our DPSH framework.\nSimilar initialization strategy has also been adopted by other\ndeep hashing methods [Zhao et al., 2015a].\nAs most existing hashing methods, the mean average\nprecision (MAP) is used to measure the accuracy of our\nproposed method and other baselines. The hyper-parameter η\nin DPSH is chosen by a validation set, which is 10 for CIFAR-\n10 and 100 for NUS-WIDE unless otherwise stated.\n4.2\nAccuracy\nFollowing [Xia et al., 2014; Lai et al., 2015], we randomly\nselect 1000 images (100 images per class) as the query set\nin CIFAR-10. For the unsupervised methods, we use the rest\nimages as the training set. For the supervised methods, we\nrandomly select 5000 images (500 images per class) from the\nrest images as the training set. The pairwise label set S is\nconstructed based on the image class labels. That is to say,\ntwo images will be considered to be similar if they share the\nsame class label.\nIn NUS-WIDE, we randomly sample 2100 query images\nfrom 21 most frequent labels (100 images per class) by\nfollowing the strategy in [Xia et al., 2014; Lai et al., 2015].\nFor supervised methods, we randomly select 500 images per\nclass from the rest images as the training set. The pairwise\nlabel set S is constructed based on the image class labels.\nThat is to say, two images will be considered to be similar\nif they share at least one common label. For NUS-WIDE,\nwe calculate the MAP values within the top 5000 returned\nneighbors.\nThe MAP results are reported in Table 2, where DPSH,\nDPSH0, NINH and CNNH are deep methods, and all the\nother methods are non-deep methods with hand-crafted fea-\ntures.\nThe result of NINH, CNNH, KSH and ITQ are\nfrom [Xia et al., 2014; Lai et al., 2015]. Please note that the\nabove experimental setting and evaluation metric is exactly\nthe same as that in [Xia et al., 2014; Lai et al., 2015]. Hence,\nthe comparison is reasonable. We can ﬁnd that our method\nDPSH dramatically outperform other baselines1, including\nunsupervised methods, supervised methods with hand-crafted\nfeatures, and deep hashing methods with feature learning.\n1The accuracy of LFH in Table 2 is much lower than that\nin [Zhang et al., 2014; Kang et al., 2016] because less points\nare adopted for training in this paper.\nPlease note that LFH is\nan efﬁcient method which can be used for training large-scale\nsupervised hashing problems. But the training efﬁciency is not the\nfocus of this paper.\nBoth DPSH and CNNH are deep hashing methods with\npairwise labels.\nBy comparing DPSH to CNNH, we can\nﬁnd that the model (DPSH) with simultaneous feature\nlearning and hash-code learning can outperform the other\nmodel (CNNH) without simultaneous feature learning and\nhash-code learning.\nNINH is a triplet label based method. Although NINH can\nperform simultaneous feature learning and hash-code learn-\ning, it is still outperformed by DPSH. More comparison with\ntriplet label based methods will be provided in Section 4.4.\nTo further verify the importance of simultaneous feature\nlearning and hash-code learning, we design a variant of\nDPSH, called DPSH0, which does not update the parameter\nof the ﬁrst seven layers (CNN-F layers) during learning.\nHence, DPSH0 just uses the CNN-F for feature extraction,\nand then based on the extracted features to learn hash func-\ntions.\nThe hash function learning procedure will give no\nfeedback to the feature extraction procedure. By comparing\nDPSH to DPSH0, we ﬁnd that DPSH can dramatically\noutperform DPSH0. It means that integrating feature learning\nand hash-code learning into the same framework in an end-to-\nend way can get a better solution than that without end-to-end\nlearning.\n4.3\nComparison to Non-Deep Baselines with Deep\nFeatures\nTo further verify the effectiveness of simultaneous feature\nlearning and hash-code learning, we compare DPSH to other\nnon-deep methods with deep features extracted by the CNN-\nF pre-trained on ImageNet.\nThe results are reported in\nTable 3, where “FastH+CNN” denotes the FastH method with\ndeep features and other methods have similar notations. We\ncan ﬁnd that our DPSH outperforms all the other non-deep\nbaselines with deep features.\n4.4\nComparison to Baselines with Ranking Labels\nMost existing deep supervised hashing methods are based on\nranking labels, especially triplet labels. Although the learning\nprocedure of these methods is based on ranking labels, the\nlearned model can also be used for evaluation scenario with\npairwise labels.\nIn fact, most triplet label based methods\nadopt pairwise labels as ground truth for evaluation [Lai et al.,\n2015; Zhang et al., 2015]. In Section 4.2, we have shown that\nour DPSH can outperform NINH. In this subsection, we will\nperform further comparison to other deep hashing methods\nwith ranking labels (triplet labels). These methods include\nDSRH [Zhao et al., 2015a], DSCH [Zhang et al., 2015] and\nDRSCH [Zhang et al., 2015].\nThe experimental setting in DSCH and DRSCH [Zhang et\nal., 2015] is different from that in Section 4.2. To perform fair\ncomparison, we adopt the same setting as that in [Zhang et al.,\n2015] for evaluation. More speciﬁcally, in CIFAR-10 dataset,\nwe randomly sample 10,000 query images (1000 images per\nclass) and use the rest as the training set. In the NUS-WIDE\ndataset, we randomly sample 2100 query images from 21\nmost frequently happened semantic labels (100 images per\nclass), and use the rest as training samples. For NUS-WIDE,\nthe MAP values within the top 50,000 returned neighbors are\nused for evaluation.\nTable 2: Accuracy in terms of MAP. The best MAPs for each category are shown in boldface. Here, the MAP value is calculated based on\nthe top 5000 returned neighbors for NUS-WIDE dataset.\nMethod\nCIFAR-10 (MAP)\nNUS-WIDE (MAP)\n12-bits\n24-bits\n32-bits\n48-bits\n12-bits\n24-bits\n32-bits\n48-bits\nDPSH\n0.713\n0.727\n0.744\n0.757\n0.794\n0.822\n0.838\n0.851\nDPSH0\n0.479\n0.472\n0.470\n0.495\n0.747\n0.751\n0.763\n0.776\nNINH\n0.552\n0.566\n0.558\n0.581\n0.674\n0.697\n0.713\n0.715\nCNNH\n0.439\n0476\n0.472\n0.489\n0.611\n0.618\n0.625\n0.608\nFastH\n0.305\n0.349\n0.369\n0.384\n0.621\n0.650\n0.665\n0.687\nSDH\n0.285\n0.329\n0.341\n0.356\n0.568\n0.600\n0.608\n0.637\nKSH\n0.303\n0.337\n0.346\n0.356\n0.556\n0.572\n0.581\n0.588\nLFH\n0.176\n0.231\n0.211\n0.253\n0.571\n0.568\n0.568\n0.585\nSPLH\n0.171\n0.173\n0.178\n0.184\n0.568\n0.589\n0.597\n0.601\nITQ\n0.162\n0.169\n0.172\n0.175\n0.452\n0.468\n0.472\n0.477\nSH\n0.127\n0.128\n0.126\n0.129\n0.454\n0.406\n0.405\n0.400\nTable 3: Accuracy in terms of MAP. The best MAPs for each category are shown in boldface. Here, the MAP value is calculated based on\nthe top 5000 returned neighbors for NUS-WIDE dataset.\nMethod\nCIFAR-10 (MAP)\nNUSWIDE (MAP)\n12-bits\n24-bits\n32-bits\n48-bits\n12-bits\n24-bits\n32-bits\n48-bits\nDPSH\n0.713\n0.727\n0.744\n0.757\n0.794\n0.822\n0.838\n0.851\nFastH + CNN\n0.553\n0.607\n0.619\n0.636\n0.779\n0.807\n0.816\n0.825\nSDH + CNN\n0.478\n0.557\n0.584\n0.592\n0.780\n0.804\n0.815\n0.824\nKSH + CNN\n0.488\n0.539\n0.548\n0.563\n0.768\n0.786\n0.790\n0.799\nLFH + CNN\n0.208\n0.242\n0.266\n0.339\n0.695\n0.734\n0.739\n0.759\nSPLH + CNN\n0.299\n0.330\n0.335\n0.330\n0.753\n0.775\n0.783\n0.786\nITQ + CNN\n0.237\n0.246\n0.255\n0.261\n0.719\n0.739\n0.747\n0.756\nSH + CNN\n0.183\n0.164\n0.161\n0.161\n0.621\n0.616\n0.615\n0.612\nTable 4: Accuracy in terms of MAP. The best MAPs for each category are shown in boldface. Here, the MAP value is calculated based on\nthe top 50,000 returned neighbors for NUS-WIDE dataset.\nMethod\nCIFAR-10 (MAP)\nNUS-WIDE (MAP)\n16-bits\n24-bits\n32-bits\n48-bits\n16-bits\n24-bits\n32-bits\n48-bits\nDPSH\n0.763\n0.781\n0.795\n0.807\n0.715\n0.722\n0.736\n0.741\nDRSCH\n0.615\n0.622\n0.629\n0.631\n0.618\n0.622\n0.623\n0.628\nDSCH\n0.609\n0.613\n0.617\n0.620\n0.592\n0.597\n0.611\n0.609\nDSRH\n0.608\n0.611\n0.617\n0.618\n0.609\n0.618\n0.621\n0.631\nThe experimental results are shown in Table 4.\nPlease\nnote that the results of DPSH in Table 4 are different from\nthose in Table 2, because the experimental settings are differ-\nent. The results of DSRH, DSCH and DRSCH are directly\nfrom [Zhang et al., 2015]. From Table 4, we can ﬁnd that\nDPSH with pairwise labels can also dramatically outperform\nother baselines with triplet labels. Please note that DSRH,\nDSCH and DRSCH can also perform simultaneously feature\nlearning and hash-code learning in an end-to-end framework.\n4.5\nSensitivity to Hyper-Parameter\nFigure 2 shows the effect of the hyper-parameter η.\nWe\ncan ﬁnd that DPSH is not sensitive to η in a large range.\nFor example, DPSH can achieve good performance on both\ndatasets with 10 ≤η ≤100.\n5\nConclusion\nIn this paper, we have proposed a novel deep hashing meth-\nods, called DPSH, for settings with pairwise labels.\nTo\nthe best of our knowledge, DPSH is the ﬁrst method which\n10\n-1\n10\n0\n10\n1\n10\n2\n10\n3\n0.6\n0.65\n0.7\n0.75\nη\nMAP\n \n \n48 bits\n12 bits\n(a) CIFAR-10\n10\n-1\n10\n0\n10\n1\n10\n2\n10\n3\n0.7\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\nη\nMAP\n \n \n48 bits\n12 bits\n(b) NUS-WIDE\nFigure 2: Sensitivity to hyper-parameter.\ncan perform simultaneous feature learning and hash-code\nlearning for applications with pairwise labels.\nBecause\ndifferent components in DPSH can give feedback to each\nother, DPSH can learn better codes than other methods\nwithout end-to-end architecture. Experiments on real datasets\nshow that DPSH can outperform other methods to achieve the\nstate-of-the-art performance in image retrieval applications.\n6\nAcknowledgements\nThis work is supported by the NSFC (61472182), the Fundamental\nResearch Funds for the Central Universities (20620140510), and the\nTencent Fund (2014320001013613).\nReferences\n[Andoni and Indyk, 2006] Alexandr Andoni and Piotr Indyk. Near-\noptimal hashing algorithms for approximate nearest neighbor in\nhigh dimensions. In FOCS, pages 459–468, 2006.\n[Chatﬁeld et al., 2014] Ken Chatﬁeld, Karen Simonyan, Andrea\nVedaldi, and Andrew Zisserman.\nReturn of the devil in the\ndetails: Delving deep into convolutional nets. In BMVC, 2014.\n[Chua et al., 2009] Tat-Seng Chua, Jinhui Tang, Richang Hong,\nHaojie Li, Zhiping Luo, and Yantao Zheng. NUS-WIDE: A real-\nworld web image database from national university of singapore.\nIn CIVR, 2009.\n[Gong and Lazebnik, 2011] Yunchao Gong and Svetlana Lazebnik.\nIterative quantization: A procrustean approach to learning binary\ncodes. In CVPR, pages 817–824, 2011.\n[He et al., 2013] Kaiming He, Fang Wen, and Jian Sun. K-means\nhashing: An afﬁnity-preserving quantization method for learning\nbinary compact codes. In CVPR, pages 2938–2945, 2013.\n[Jiang and Li, 2015] Qing-Yuan Jiang and Wu-Jun Li.\nScalable\ngraph hashing with feature transformation.\nIn IJCAI, pages\n2248–2254, 2015.\n[Kang et al., 2016] Wang-Cheng Kang, Wu-Jun Li, and Zhi-Hua\nZhou. Column sampling based discrete supervised hashing. In\nAAAI, 2016.\n[Kong and Li, 2012] Weihao Kong and Wu-Jun Li.\nIsotropic\nhashing. In NIPS, pages 1655–1663, 2012.\n[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever, and\nGeoffrey E. Hinton.\nImagenet classiﬁcation with deep\nconvolutional neural networks. In NIPS, pages 1106–1114, 2012.\n[Krizhevsky, 2009] Alex Krizhevsky. Learning multiple layers of\nfeatures from tiny images. Master’s thesis, University of Toronto,\n2009.\n[Kulis and Grauman, 2009] Brian Kulis and Kristen Grauman.\nKernelized locality-sensitive hashing for scalable image search.\nIn ICCV, pages 2130–2137, 2009.\n[Lai et al., 2015] Hanjiang Lai, Yan Pan, Ye Liu, and Shuicheng\nYan. Simultaneous feature learning and hash coding with deep\nneural networks. In CVPR, pages 3270–3278, 2015.\n[LeCun et al., 1989] Yann LeCun, Bernhard E. Boser, John S.\nDenker, Donnie Henderson, R. E. Howard, Wayne E. Hubbard,\nand Lawrence D. Jackel. Backpropagation applied to handwritten\nzip code recognition. Neural Computation, 1(4):541–551, 1989.\n[Li et al., 2013] Xi Li, Guosheng Lin, Chunhua Shen, Anton\nvan den Hengel, and Anthony R. Dick. Learning hash functions\nusing column generation. In ICML, pages 142–150, 2013.\n[Lin et al., 2013] Guosheng Lin, Chunhua Shen, David Suter, and\nAnton van den Hengel. A general two-step approach to learning-\nbased hashing. In ICCV, pages 2552–2559, 2013.\n[Lin et al., 2014] Guosheng Lin, Chunhua Shen, Qinfeng Shi,\nAnton van den Hengel, and David Suter. Fast supervised hashing\nwith decision trees for high-dimensional data. In CVPR, pages\n1971–1978, 2014.\n[Lin et al., 2015] Kevin Lin, Huei-Fang Yang, Jen-Hao Hsiao, and\nChu-Song Chen.\nDeep learning of binary hash codes for fast\nimage retrieval. In CVPR Workshops, pages 27–35, 2015.\n[Liong et al., 2015] Venice Erin Liong, Jiwen Lu, Gang Wang,\nPierre Moulin, and Jie Zhou. Deep hashing for compact binary\ncodes learning. In CVPR, pages 2475–2483, 2015.\n[Liu et al., 2012] Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang,\nand Shih-Fu Chang. Supervised hashing with kernels. In CVPR,\npages 2074–2081, 2012.\n[Liu et al., 2014] Wei Liu, Cun Mu, Sanjiv Kumar, and Shih-Fu\nChang.\nDiscrete graph hashing.\nIn NIPS, pages 3419–3427,\n2014.\n[Masci et al., 2014] Jonathan\nMasci,\nAlex\nM.\nBronstein,\nMichael M. Bronstein, Pablo Sprechmann, and Guillermo\nSapiro. Sparse similarity-preserving hashing. In ICLR, 2014.\n[Norouzi and Fleet, 2011] Mohammad Norouzi and David J. Fleet.\nMinimal loss hashing for compact binary codes. In ICML, pages\n353–360, 2011.\n[Rastegari et al., 2013] Mohammad Rastegari,\nJonghyun Choi,\nShobeir Fakhraei, Daume Hal, and Larry S. Davis. Predictable\ndual-view hashing. In ICML, pages 1328–1336, 2013.\n[Russakovsky et al., 2014] Olga Russakovsky,\nJia Deng,\nHao\nSu, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein,\nAlexander C. Berg, and Fei-Fei Li. Imagenet large scale visual\nrecognition challenge. CoRR, abs/1409.0575, 2014.\n[Salakhutdinov and Hinton, 2009] Ruslan Salakhutdinov and Geof-\nfrey E. Hinton.\nSemantic hashing.\nInternational Journal of\nApproximate Reasoning, 50(7):969–978, 2009.\n[Shen et al., 2015] Fumin Shen, Chunhua Shen, Wei Liu, and\nHeng Tao Shen. Supervised discrete hashing. In CVPR, 2015.\n[Vedaldi and Lenc, 2015] Andrea Vedaldi and Karel Lenc.\nMat-\nConvNet – convolutional neural networks for MATLAB. In ACM\nMM, 2015.\n[Wang et al., 2010] Jun Wang, Sanjiv Kumar, and Shih-Fu Chang.\nSequential projection learning for hashing with compact codes.\nIn ICML, pages 1127–1134, 2010.\n[Wang et al., 2013a] Jianfeng Wang, Jingdong Wang, Nenghai Yu,\nand Shipeng Li.\nOrder preserving hashing for approximate\nnearest neighbor search. In ACM MM, pages 133–142, 2013.\n[Wang et al., 2013b] Jun Wang, Wei Liu, Andy X. Sun, and Yu-\nGang Jiang. Learning hash codes with listwise supervision. In\nICCV, pages 3032–3039, 2013.\n[Wang et al., 2015] Qifan Wang, Zhiwei Zhang, and Luo Si.\nRanking preserving hashing for fast similarity search. In IJCAI,\npages 3911–3917, 2015.\n[Weiss et al., 2008] Yair Weiss, Antonio Torralba, and Robert\nFergus. Spectral hashing. In NIPS, pages 1753–1760, 2008.\n[Xia et al., 2014] Rongkai Xia, Yan Pan, Hanjiang Lai, Cong Liu,\nand Shuicheng Yan. Supervised hashing for image retrieval via\nimage representation learning. In AAAI, pages 2156–2162, 2014.\n[Zhang et al., 2014] Peichao Zhang, Wei Zhang, Wu-Jun Li, and\nMinyi Guo. Supervised hashing with latent factor models. In\nSIGIR, pages 173–182, 2014.\n[Zhang et al., 2015] Ruimao Zhang, Liang Lin, Rui Zhang, Wang-\nmeng Zuo, and Lei Zhang.\nBit-scalable deep hashing with\nregularized similarity learning for image retrieval and person\nre-identiﬁcation.\nIEEE Transactions on Image Processing,\n24(12):4766–4779, 2015.\n[Zhao et al., 2015a] Fang Zhao, Yongzhen Huang, Liang Wang,\nand Tieniu Tan. Deep semantic ranking based hashing for multi-\nlabel image retrieval. In CVPR, pages 1556–1564, 2015.\n[Zhao et al., 2015b] Xueyi Zhao, Xi Li, and Zhongfei (Mark)\nZhang. Multimedia retrieval via deep learning to rank. IEEE\nSignal Processing Letters, 22(9):1487–1491, 2015.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "H.3.1"
  ],
  "published": "2015-11-12",
  "updated": "2016-04-21"
}