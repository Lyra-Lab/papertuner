{
  "id": "http://arxiv.org/abs/1511.06434v2",
  "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
  "authors": [
    "Alec Radford",
    "Luke Metz",
    "Soumith Chintala"
  ],
  "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.",
  "text": "Under review as a conference paper at ICLR 2016\nUNSUPERVISED REPRESENTATION LEARNING\nWITH DEEP CONVOLUTIONAL\nGENERATIVE ADVERSARIAL NETWORKS\nAlec Radford & Luke Metz\nindico Research\nBoston, MA\n{alec,luke}@indico.io\nSoumith Chintala\nFacebook AI Research\nNew York, NY\nsoumith@fb.com\nABSTRACT\nIn recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and unsuper-\nvised learning. We introduce a class of CNNs called deep convolutional generative\nadversarial networks (DCGANs), that have certain architectural constraints, and\ndemonstrate that they are a strong candidate for unsupervised learning. Training\non various image datasets, we show convincing evidence that our deep convolu-\ntional adversarial pair learns a hierarchy of representations from object parts to\nscenes in both the generator and discriminator. Additionally, we use the learned\nfeatures for novel tasks - demonstrating their applicability as general image repre-\nsentations.\n1\nINTRODUCTION\nLearning reusable feature representations from large unlabeled datasets has been an area of active\nresearch. In the context of computer vision, one can leverage the practically unlimited amount of\nunlabeled images and videos to learn good intermediate representations, which can then be used on\na variety of supervised learning tasks such as image classiﬁcation. We propose that one way to build\ngood image representations is by training Generative Adversarial Networks (GANs) (Goodfellow\net al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors\nfor supervised tasks. GANs provide an attractive alternative to maximum likelihood techniques.\nOne can additionally argue that their learning process and the lack of a heuristic cost function (such\nas pixel-wise independent mean-square error) are attractive to representation learning. GANs have\nbeen known to be unstable to train, often resulting in generators that produce nonsensical outputs.\nThere has been very limited published research in trying to understand and visualize what GANs\nlearn, and the intermediate representations of multi-layer GANs.\nIn this paper, we make the following contributions\n• We propose and evaluate a set of constraints on the architectural topology of Convolutional\nGANs that make them stable to train in most settings. We name this class of architectures\nDeep Convolutional GANs (DCGAN)\n• We use the trained discriminators for image classiﬁcation tasks, showing competitive per-\nformance with other unsupervised algorithms.\n• We visualize the ﬁlters learnt by GANs and empirically show that speciﬁc ﬁlters have\nlearned to draw speciﬁc objects.\n1\narXiv:1511.06434v2  [cs.LG]  7 Jan 2016\nUnder review as a conference paper at ICLR 2016\n• We show that the generators have interesting vector arithmetic properties allowing for easy\nmanipulation of many semantic qualities of generated samples.\n2\nRELATED WORK\n2.1\nREPRESENTATION LEARNING FROM UNLABELED DATA\nUnsupervised representation learning is a fairly well studied problem in general computer vision\nresearch, as well as in the context of images. A classic approach to unsupervised representation\nlearning is to do clustering on the data (for example using K-means), and leverage the clusters for\nimproved classiﬁcation scores. In the context of images, one can do hierarchical clustering of image\npatches (Coates & Ng, 2012) to learn powerful image representations. Another popular method\nis to train auto-encoders (convolutionally, stacked (Vincent et al., 2010), separating the what and\nwhere components of the code (Zhao et al., 2015), ladder structures (Rasmus et al., 2015)) that\nencode an image into a compact code, and decode the code to reconstruct the image as accurately\nas possible. These methods have also been shown to learn good feature representations from image\npixels. Deep belief networks (Lee et al., 2009) have also been shown to work well in learning\nhierarchical representations.\n2.2\nGENERATING NATURAL IMAGES\nGenerative image models are well studied and fall into two categories: parametric and non-\nparametric.\nThe non-parametric models often do matching from a database of existing images, often matching\npatches of images, and have been used in texture synthesis (Efros et al., 1999), super-resolution\n(Freeman et al., 2002) and in-painting (Hays & Efros, 2007).\nParametric models for generating images has been explored extensively (for example on MNIST\ndigits or for texture synthesis (Portilla & Simoncelli, 2000)). However, generating natural images\nof the real world have had not much success until recently. A variational sampling approach to\ngenerating images (Kingma & Welling, 2013) has had some success, but the samples often suffer\nfrom being blurry. Another approach generates images using an iterative forward diffusion process\n(Sohl-Dickstein et al., 2015). Generative Adversarial Networks (Goodfellow et al., 2014) generated\nimages suffering from being noisy and incomprehensible. A laplacian pyramid extension to this\napproach (Denton et al., 2015) showed higher quality images, but they still suffered from the objects\nlooking wobbly because of noise introduced in chaining multiple models. A recurrent network\napproach (Gregor et al., 2015) and a deconvolution network approach (Dosovitskiy et al., 2014) have\nalso recently had some success with generating natural images. However, they have not leveraged\nthe generators for supervised tasks.\n2.3\nVISUALIZING THE INTERNALS OF CNNS\nOne constant criticism of using neural networks has been that they are black-box methods, with little\nunderstanding of what the networks do in the form of a simple human-consumable algorithm. In the\ncontext of CNNs, Zeiler et. al. (Zeiler & Fergus, 2014) showed that by using deconvolutions and\nﬁltering the maximal activations, one can ﬁnd the approximate purpose of each convolution ﬁlter in\nthe network. Similarly, using a gradient descent on the inputs lets us inspect the ideal image that\nactivates certain subsets of ﬁlters (Mordvintsev et al.).\n3\nAPPROACH AND MODEL ARCHITECTURE\nHistorical attempts to scale up GANs using CNNs to model images have been unsuccessful. This\nmotivated the authors of LAPGAN (Denton et al., 2015) to develop an alternative approach to it-\neratively upscale low resolution generated images which can be modeled more reliably. We also\nencountered difﬁculties attempting to scale GANs using CNN architectures commonly used in the\nsupervised literature. However, after extensive model exploration we identiﬁed a family of archi-\n2\nUnder review as a conference paper at ICLR 2016\ntectures that resulted in stable training across a range of datasets and allowed for training higher\nresolution and deeper generative models.\nCore to our approach is adopting and modifying three recently demonstrated changes to CNN archi-\ntectures.\nThe ﬁrst is the all convolutional net (Springenberg et al., 2014) which replaces deterministic spatial\npooling functions (such as maxpooling) with strided convolutions, allowing the network to learn\nits own spatial downsampling. We use this approach in our generator, allowing it to learn its own\nspatial upsampling, and discriminator.\nSecond is the trend towards eliminating fully connected layers on top of convolutional features.\nThe strongest example of this is global average pooling which has been utilized in state of the\nart image classiﬁcation models (Mordvintsev et al.). We found global average pooling increased\nmodel stability but hurt convergence speed. A middle ground of directly connecting the highest\nconvolutional features to the input and output respectively of the generator and discriminator worked\nwell. The ﬁrst layer of the GAN, which takes a uniform noise distribution Z as input, could be called\nfully connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional\ntensor and used as the start of the convolution stack. For the discriminator, the last convolution layer\nis ﬂattened and then fed into a single sigmoid output. See Fig. 1 for a visualization of an example\nmodel architecture.\nThird is Batch Normalization (Ioffe & Szegedy, 2015) which stabilizes learning by normalizing the\ninput to each unit to have zero mean and unit variance. This helps deal with training problems that\narise due to poor initialization and helps gradient ﬂow in deeper models. This proved critical to get\ndeep generators to begin learning, preventing the generator from collapsing all samples to a single\npoint which is a common failure mode observed in GANs. Directly applying batchnorm to all layers\nhowever, resulted in sample oscillation and model instability. This was avoided by not applying\nbatchnorm to the generator output layer and the discriminator input layer.\nThe ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output\nlayer which uses the Tanh function. We observed that using a bounded activation allowed the model\nto learn more quickly to saturate and cover the color space of the training distribution. Within the\ndiscriminator we found the leaky rectiﬁed activation (Maas et al., 2013) (Xu et al., 2015) to work\nwell, especially for higher resolution modeling. This is in contrast to the original GAN paper, which\nused the maxout activation (Goodfellow et al., 2013).\nArchitecture guidelines for stable Deep Convolutional GANs\n• Replace any pooling layers with strided convolutions (discriminator) and fractional-strided\nconvolutions (generator).\n• Use batchnorm in both the generator and the discriminator.\n• Remove fully connected hidden layers for deeper architectures.\n• Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n• Use LeakyReLU activation in the discriminator for all layers.\n4\nDETAILS OF ADVERSARIAL TRAINING\nWe trained DCGANs on three datasets, Large-scale Scene Understanding (LSUN) (Yu et al., 2015),\nImagenet-1k and a newly assembled Faces dataset. Details on the usage of each of these datasets\nare given below.\nNo pre-processing was applied to training images besides scaling to the range of the tanh activation\nfunction [-1, 1]. All models were trained with mini-batch stochastic gradient descent (SGD) with\na mini-batch size of 128. All weights were initialized from a zero-centered Normal distribution\nwith standard deviation 0.02. In the LeakyReLU, the slope of the leak was set to 0.2 in all models.\nWhile previous GAN work has used momentum to accelerate training, we used the Adam optimizer\n(Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001,\nto be too high, using 0.0002 instead. Additionally, we found leaving the momentum term β1 at the\n3\nUnder review as a conference paper at ICLR 2016\nFigure 1: DCGAN generator used for LSUN scene modeling. A 100 dimensional uniform distribu-\ntion Z is projected to a small spatial extent convolutional representation with many feature maps.\nA series of four fractionally-strided convolutions (in some recent papers, these are wrongly called\ndeconvolutions) then convert this high level representation into a 64 × 64 pixel image. Notably, no\nfully connected or pooling layers are used.\nsuggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped\nstabilize training.\n4.1\nLSUN\nAs visual quality of samples from generative image models has improved, concerns of over-ﬁtting\nand memorization of training samples have risen. To demonstrate how our model scales with more\ndata and higher resolution generation, we train a model on the LSUN bedrooms dataset containing\na little over 3 million training examples. Recent analysis has shown that there is a direct link be-\ntween how fast models learn and their generalization performance (Hardt et al., 2015). We show\nsamples from one epoch of training (Fig.2), mimicking online learning, in addition to samples after\nconvergence (Fig.3), as an opportunity to demonstrate that our model is not producing high quality\nsamples via simply overﬁtting/memorizing training examples. No data augmentation was applied to\nthe images.\n4.1.1\nDEDUPLICATION\nTo further decrease the likelihood of the generator memorizing input examples (Fig.2) we perform a\nsimple image de-duplication process. We ﬁt a 3072-128-3072 de-noising dropout regularized RELU\nautoencoder on 32x32 downsampled center-crops of training examples. The resulting code layer\nactivations are then binarized via thresholding the ReLU activation which has been shown to be an\neffective information preserving technique (Srivastava et al., 2014) and provides a convenient form\nof semantic-hashing, allowing for linear time de-duplication . Visual inspection of hash collisions\nshowed high precision with an estimated false positive rate of less than 1 in 100. Additionally, the\ntechnique detected and removed approximately 275,000 near duplicates, suggesting a high recall.\n4.2\nFACES\nWe scraped images containing human faces from random web image queries of peoples names. The\npeople names were acquired from dbpedia, with a criterion that they were born in the modern era.\nThis dataset has 3M images from 10K people. We run an OpenCV face detector on these images,\nkeeping the detections that are sufﬁciently high resolution, which gives us approximately 350,000\nface boxes. We use these face boxes for training. No data augmentation was applied to the images.\n4\nUnder review as a conference paper at ICLR 2016\nFigure 2: Generated bedrooms after one training pass through the dataset. Theoretically, the model\ncould learn to memorize training examples, but this is experimentally unlikely as we train with a\nsmall learning rate and minibatch SGD. We are aware of no prior empirical evidence demonstrating\nmemorization with SGD and a small learning rate.\nFigure 3: Generated bedrooms after ﬁve epochs of training. There appears to be evidence of visual\nunder-ﬁtting via repeated noise textures across multiple samples such as the base boards of some of\nthe beds.\n4.3\nIMAGENET-1K\nWe use Imagenet-1k (Deng et al., 2009) as a source of natural images for unsupervised training. We\ntrain on 32 × 32 min-resized center crops. No data augmentation was applied to the images.\n5\nUnder review as a conference paper at ICLR 2016\n5\nEMPIRICAL VALIDATION OF DCGANS CAPABILITIES\n5.1\nCLASSIFYING CIFAR-10 USING GANS AS A FEATURE EXTRACTOR\nOne common technique for evaluating the quality of unsupervised representation learning algo-\nrithms is to apply them as a feature extractor on supervised datasets and evaluate the performance\nof linear models ﬁtted on top of these features.\nOn the CIFAR-10 dataset, a very strong baseline performance has been demonstrated from a well\ntuned single layer feature extraction pipeline utilizing K-means as a feature learning algorithm.\nWhen using a very large amount of feature maps (4800) this technique achieves 80.6% accuracy.\nAn unsupervised multi-layered extension of the base algorithm reaches 82.0% accuracy (Coates &\nNg, 2011). To evaluate the quality of the representations learned by DCGANs for supervised tasks,\nwe train on Imagenet-1k and then use the discriminator’s convolutional features from all layers,\nmaxpooling each layers representation to produce a 4 × 4 spatial grid. These features are then\nﬂattened and concatenated to form a 28672 dimensional vector and a regularized linear L2-SVM\nclassiﬁer is trained on top of them. This achieves 82.8% accuracy, out performing all K-means\nbased approaches. Notably, the discriminator has many less feature maps (512 in the highest layer)\ncompared to K-means based techniques, but does result in a larger total feature vector size due to\nthe many layers of 4 × 4 spatial locations. The performance of DCGANs is still less than that of\nExemplar CNNs (Dosovitskiy et al., 2015), a technique which trains normal discriminative CNNs\nin an unsupervised fashion to differentiate between speciﬁcally chosen, aggressively augmented,\nexemplar samples from the source dataset. Further improvements could be made by ﬁnetuning the\ndiscriminator’s representations, but we leave this for future work. Additionally, since our DCGAN\nwas never trained on CIFAR-10 this experiment also demonstrates the domain robustness of the\nlearned features.\nTable 1: CIFAR-10 classiﬁcation results using our pre-trained model. Our DCGAN is not pre-\ntrained on CIFAR-10, but on Imagenet-1k, and the features are used to classify CIFAR-10 images.\nModel\nAccuracy\nAccuracy (400 per class)\nmax # of features units\n1 Layer K-means\n80.6%\n63.7% (±0.7%)\n4800\n3 Layer K-means Learned RF\n82.0%\n70.7% (±0.7%)\n3200\nView Invariant K-means\n81.9%\n72.6% (±0.7%)\n6400\nExemplar CNN\n84.3%\n77.4% (±0.2%)\n1024\nDCGAN (ours) + L2-SVM\n82.8%\n73.8% (±0.4%)\n512\n5.2\nCLASSIFYING SVHN DIGITS USING GANS AS A FEATURE EXTRACTOR\nOn the StreetView House Numbers dataset (SVHN)(Netzer et al., 2011), we use the features of\nthe discriminator of a DCGAN for supervised purposes when labeled data is scarce. Following\nsimilar dataset preparation rules as in the CIFAR-10 experiments, we split off a validation set of\n10,000 examples from the non-extra set and use it for all hyperparameter and model selection. 1000\nuniformly class distributed training examples are randomly selected and used to train a regularized\nlinear L2-SVM classiﬁer on top of the same feature extraction pipeline used for CIFAR-10. This\nachieves state of the art (for classiﬁcation using 1000 labels) at 22.48% test error, improving upon\nanother modifcation of CNNs designed to leverage unlabled data (Zhao et al., 2015). Additionally,\nwe validate that the CNN architecture used in DCGAN is not the key contributing factor of the\nmodel’s performance by training a purely supervised CNN with the same architecture on the same\ndata and optimizing this model via random search over 64 hyperparameter trials (Bergstra & Bengio,\n2012). It achieves a signﬁcantly higher 28.87% validation error.\n6\nINVESTIGATING AND VISUALIZING THE INTERNALS OF THE NETWORKS\nWe investigate the trained generators and discriminators in a variety of ways. We do not do any\nkind of nearest neighbor search on the training set. Nearest neighbors in pixel or feature space are\n6\nUnder review as a conference paper at ICLR 2016\nTable 2: SVHN classiﬁcation with 1000 labels\nModel\nerror rate\nKNN\n77.93%\nTSVM\n66.55%\nM1+KNN\n65.63%\nM1+TSVM\n54.33%\nM1+M2\n36.02%\nSWWAE without dropout\n27.83%\nSWWAE with dropout\n23.56%\nDCGAN (ours) + L2-SVM\n22.48%\nSupervised CNN with the same architecture\n28.87% (validation)\ntrivially fooled (Theis et al., 2015) by small image transforms. We also do not use log-likelihood\nmetrics to quantitatively assess the model, as it is a poor (Theis et al., 2015) metric.\n6.1\nWALKING IN THE LATENT SPACE\nThe ﬁrst experiment we did was to understand the landscape of the latent space. Walking on the\nmanifold that is learnt can usually tell us about signs of memorization (if there are sharp transitions)\nand about the way in which the space is hierarchically collapsed. If walking in this latent space\nresults in semantic changes to the image generations (such as objects being added and removed), we\ncan reason that the model has learned relevant and interesting representations. The results are shown\nin Fig.4.\n6.2\nVISUALIZING THE DISCRIMINATOR FEATURES\nPrevious work has demonstrated that supervised training of CNNs on large image datasets results in\nvery powerful learned features (Zeiler & Fergus, 2014). Additionally, supervised CNNs trained on\nscene classiﬁcation learn object detectors (Oquab et al., 2014). We demonstrate that an unsupervised\nDCGAN trained on a large image dataset can also learn a hierarchy of features that are interesting.\nUsing guided backpropagation as proposed by (Springenberg et al., 2014), we show in Fig.5 that the\nfeatures learnt by the discriminator activate on typical parts of a bedroom, like beds and windows.\nFor comparison, in the same ﬁgure, we give a baseline for randomly initialized features that are not\nactivated on anything that is semantically relevant or interesting.\n6.3\nMANIPULATING THE GENERATOR REPRESENTATION\n6.3.1\nFORGETTING TO DRAW CERTAIN OBJECTS\nIn addition to the representations learnt by a discriminator, there is the question of what representa-\ntions the generator learns. The quality of samples suggest that the generator learns speciﬁc object\nrepresentations for major scene components such as beds, windows, lamps, doors, and miscellaneous\nfurniture. In order to explore the form that these representations take, we conducted an experiment\nto attempt to remove windows from the generator completely.\nOn 150 samples, 52 window bounding boxes were drawn manually. On the second highest con-\nvolution layer features, logistic regression was ﬁt to predict whether a feature activation was on a\nwindow (or not), by using the criterion that activations inside the drawn bounding boxes are posi-\ntives and random samples from the same images are negatives. Using this simple model, all feature\nmaps with weights greater than zero ( 200 in total) were dropped from all spatial locations. Then,\nrandom new samples were generated with and without the feature map removal.\nThe generated images with and without the window dropout are shown in Fig.6, and interestingly,\nthe network mostly forgets to draw windows in the bedrooms, replacing them with other objects.\n7\nUnder review as a conference paper at ICLR 2016\nFigure 4: Top rows: Interpolation between a series of 9 random points in Z show that the space\nlearned has smooth transitions, with every image in the space plausibly looking like a bedroom. In\nthe 6th row, you see a room without a window slowly transforming into a room with a giant window.\nIn the 10th row, you see what appears to be a TV slowly being transformed into a window.\n6.3.2\nVECTOR ARITHMETIC ON FACE SAMPLES\nIn the context of evaluating learned representations of words (Mikolov et al., 2013) demonstrated\nthat simple arithmetic operations revealed rich linear structure in representation space. One canoni-\ncal example demonstrated that the vector(”King”) - vector(”Man”) + vector(”Woman”) resulted in a\nvector whose nearest neighbor was the vector for Queen. We investigated whether similar structure\nemerges in the Z representation of our generators. We performed similar arithmetic on the Z vectors\nof sets of exemplar samples for visual concepts. Experiments working on only single samples per\nconcept were unstable, but averaging the Z vector for three examplars showed consistent and stable\ngenerations that semantically obeyed the arithmetic. In addition to the object manipulation shown\nin (Fig. 7), we demonstrate that face pose is also modeled linearly in Z space (Fig. 8).\nThese demonstrations suggest interesting applications can be developed using Z representations\nlearned by our models. It has been previously demonstrated that conditional generative models can\nlearn to convincingly model object attributes like scale, rotation, and position (Dosovitskiy et al.,\n2014). This is to our knowledge the ﬁrst demonstration of this occurring in purely unsupervised\n8\nUnder review as a conference paper at ICLR 2016\nFigure 5: On the right, guided backpropagation visualizations of maximal axis-aligned responses\nfor the ﬁrst 6 learned convolutional features from the last convolution layer in the discriminator.\nNotice a signiﬁcant minority of features respond to beds - the central object in the LSUN bedrooms\ndataset. On the left is a random ﬁlter baseline. Comparing to the previous responses there is little to\nno discrimination and random structure.\nFigure 6: Top row: un-modiﬁed samples from model. Bottom row: the same samples generated\nwith dropping out ”window” ﬁlters. Some windows are removed, others are transformed into objects\nwith similar visual appearance such as doors and mirrors. Although visual quality decreased, overall\nscene composition stayed similar, suggesting the generator has done a good job disentangling scene\nrepresentation from object representation. Extended experiments could be done to remove other\nobjects from the image and modify the objects the generator draws.\nmodels. Further exploring and developing the above mentioned vector arithmetic could dramat-\nically reduce the amount of data needed for conditional generative modeling of complex image\ndistributions.\n7\nCONCLUSION AND FUTURE WORK\nWe propose a more stable set of architectures for training generative adversarial networks and we\ngive evidence that adversarial networks learn good representations of images for supervised learning\nand generative modeling. There are still some forms of model instability remaining - we noticed as\nmodels are trained longer they sometimes collapse a subset of ﬁlters to a single oscillating mode.\n9\nUnder review as a conference paper at ICLR 2016\nFigure 7: Vector arithmetic for visual concepts. For each column, the Z vectors of samples are\naveraged. Arithmetic was then performed on the mean vectors creating a new vector Y . The center\nsample on the right hand side is produce by feeding Y as input to the generator. To demonstrate\nthe interpolation capabilities of the generator, uniform noise sampled with scale +-0.25 was added\nto Y to produce the 8 other samples. Applying arithmetic in the input space (bottom two examples)\nresults in noisy overlap due to misalignment.\nFurther work is needed to tackle this from of instability. We think that extending this framework\n10\nUnder review as a conference paper at ICLR 2016\nFigure 8: A ”turn” vector was created from four averaged samples of faces looking left vs looking\nright. By adding interpolations along this axis to random samples we were able to reliably transform\ntheir pose.\nto other domains such as video (for frame prediction) and audio (pre-trained features for speech\nsynthesis) should be very interesting. Further investigations into the properties of the learnt latent\nspace would be interesting as well.\nACKNOWLEDGMENTS\nWe are fortunate and thankful for all the advice and guidance we have received during this work,\nespecially that of Ian Goodfellow, Tobias Springenberg, Arthur Szlam and Durk Kingma. Addition-\nally we’d like to thank all of the folks at indico for providing support, resources, and conversations,\nespecially the two other members of the indico research team, Dan Kuster and Nathan Lintz. Finally,\nwe’d like to thank Nvidia for donating a Titan-X GPU used in this work.\nREFERENCES\nBergstra, James and Bengio, Yoshua. Random search for hyper-parameter optimization. JMLR,\n2012.\nCoates, Adam and Ng, Andrew. Selecting receptive ﬁelds in deep networks. NIPS, 2011.\nCoates, Adam and Ng, Andrew Y. Learning feature representations with k-means. In Neural Net-\nworks: Tricks of the Trade, pp. 561–580. Springer, 2012.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale\nhierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.\nIEEE Conference on, pp. 248–255. IEEE, 2009.\nDenton, Emily, Chintala, Soumith, Szlam, Arthur, and Fergus, Rob. Deep generative image models\nusing a laplacian pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.\nDosovitskiy, Alexey, Springenberg, Jost Tobias, and Brox, Thomas. Learning to generate chairs\nwith convolutional neural networks. arXiv preprint arXiv:1411.5928, 2014.\n11\nUnder review as a conference paper at ICLR 2016\nDosovitskiy, Alexey, Fischer, Philipp, Springenberg, Jost Tobias, Riedmiller, Martin, and Brox,\nThomas. Discriminative unsupervised feature learning with exemplar convolutional neural net-\nworks. In Pattern Analysis and Machine Intelligence, IEEE Transactions on, volume 99. IEEE,\n2015.\nEfros, Alexei, Leung, Thomas K, et al. Texture synthesis by non-parametric sampling. In Computer\nVision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2, pp.\n1033–1038. IEEE, 1999.\nFreeman, William T, Jones, Thouis R, and Pasztor, Egon C. Example-based super-resolution. Com-\nputer Graphics and Applications, IEEE, 22(2):56–65, 2002.\nGoodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua.\nMaxout networks. arXiv preprint arXiv:1302.4389, 2013.\nGoodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair,\nSherjil, Courville, Aaron C., and Bengio, Yoshua. Generative adversarial nets. NIPS, 2014.\nGregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. Draw: A recurrent neural network\nfor image generation. arXiv preprint arXiv:1502.04623, 2015.\nHardt, Moritz, Recht, Benjamin, and Singer, Yoram. Train faster, generalize better: Stability of\nstochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.\nHauberg, Sren, Freifeld, Oren, Larsen, Anders Boesen Lindbo, Fisher III, John W., and Hansen,\nLars Kair. Dreaming more data: Class-dependent distributions over diffeomorphisms for learned\ndata augmentation. arXiv preprint arXiv:1510.02795, 2015.\nHays, James and Efros, Alexei A. Scene completion using millions of photographs. ACM Transac-\ntions on Graphics (TOG), 26(3):4, 2007.\nIoffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nKingma, Diederik P and Ba, Jimmy Lei. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\nKingma, Diederik P and Welling, Max.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nLee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng, Andrew Y. Convolutional deep belief\nnetworks for scalable unsupervised learning of hierarchical representations. In Proceedings of the\n26th Annual International Conference on Machine Learning, pp. 609–616. ACM, 2009.\nLoosli, Ga¨elle, Canu, St´ephane, and Bottou, L´eon. Training invariant support vector machines using\nselective sampling. In Bottou, L´eon, Chapelle, Olivier, DeCoste, Dennis, and Weston, Jason\n(eds.), Large Scale Kernel Machines, pp. 301–320. MIT Press, Cambridge, MA., 2007. URL\nhttp://leon.bottou.org/papers/loosli-canu-bottou-2006.\nMaas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y. Rectiﬁer nonlinearities improve neural\nnetwork acoustic models. In Proc. ICML, volume 30, 2013.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Distributed repre-\nsentations of words and phrases and their compositionality. In Advances in neural information\nprocessing systems, pp. 3111–3119, 2013.\nMordvintsev,\nAlexander,\nOlah,\nChristopher,\nand Tyka,\nMike.\nInceptionism :\nGoing\ndeeper into neural networks.\nhttp://googleresearch.blogspot.com/2015/06/\ninceptionism-going-deeper-into-neural.html. Accessed: 2015-06-17.\nNair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines.\nIn Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807–\n814, 2010.\n12\nUnder review as a conference paper at ICLR 2016\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read-\ning digits in natural images with unsupervised feature learning. In NIPS workshop on deep learn-\ning and unsupervised feature learning, volume 2011, pp. 5. Granada, Spain, 2011.\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and transferring mid-level image represen-\ntations using convolutional neural networks. In CVPR, 2014.\nPortilla, Javier and Simoncelli, Eero P.\nA parametric texture model based on joint statistics of\ncomplex wavelet coefﬁcients. International Journal of Computer Vision, 40(1):49–70, 2000.\nRasmus, Antti, Valpola, Harri, Honkala, Mikko, Berglund, Mathias, and Raiko, Tapani.\nSemi-\nsupervised learning with ladder network. arXiv preprint arXiv:1507.02672, 2015.\nSohl-Dickstein, Jascha, Weiss, Eric A, Maheswaranathan, Niru, and Ganguli, Surya. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.\nSpringenberg, Jost Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller, Martin. Striving for\nsimplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.\nSrivastava, Rupesh Kumar, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, J¨urgen. Under-\nstanding locally competitive networks. arXiv preprint arXiv:1410.1165, 2014.\nTheis, L., van den Oord, A., and Bethge, M.\nA note on the evaluation of generative models.\narXiv:1511.01844, Nov 2015. URL http://arxiv.org/abs/1511.01844.\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Bengio, Yoshua, and Manzagol, Pierre-Antoine.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.\nXu, Bing, Wang, Naiyan, Chen, Tianqi, and Li, Mu. Empirical evaluation of rectiﬁed activations in\nconvolutional network. arXiv preprint arXiv:1505.00853, 2015.\nYu, Fisher, Zhang, Yinda, Song, Shuran, Seff, Ari, and Xiao, Jianxiong. Construction of a large-scale\nimage dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,\n2015.\nZeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional networks. In\nComputer Vision–ECCV 2014, pp. 818–833. Springer, 2014.\nZhao, Junbo, Mathieu, Michael, Goroshin, Ross, and Lecun, Yann.\nStacked what-where auto-\nencoders. arXiv preprint arXiv:1506.02351, 2015.\n13\nUnder review as a conference paper at ICLR 2016\n8\nSUPPLEMENTARY MATERIAL\n8.1\nEVALUATING DCGANS CAPABILITY TO CAPTURE DATA DISTRIBUTIONS\nWe propose to apply standard classiﬁcation metrics to a conditional version of our model, evaluating\nthe conditional distributions learned. We trained a DCGAN on MNIST (splitting off a 10K validation\nset) as well as a permutation invariant GAN baseline and evaluated the models using a nearest\nneighbor classiﬁer comparing real data to a set of generated conditional samples. We found that\nremoving the scale and bias parameters from batchnorm produced better results for both models. We\nspeculate that the noise introduced by batchnorm helps the generative models to better explore and\ngenerate from the underlying data distribution. The results are shown in Table 3 which compares\nour models with other techniques. The DCGAN model achieves the same test error as a nearest\nneighbor classiﬁer ﬁtted on the training dataset - suggesting the DCGAN model has done a superb\njob at modeling the conditional distributions of this dataset. At one million samples per class, the\nDCGAN model outperforms InﬁMNIST (Loosli et al., 2007), a hand developed data augmentation\npipeline which uses translations and elastic deformations of training examples. The DCGAN is\ncompetitive with a probabilistic generative data augmentation technique utilizing learned per class\ntransformations (Hauberg et al., 2015) while being more general as it directly models the data instead\nof transformations of the data.\nTable 3: Nearest neighbor classiﬁcation results.\nModel\nTest Error @50K samples\nTest Error @10M samples\nAlignMNIST\n-\n1.4%\nInﬁMNIST\n-\n2.6%\nReal Data\n3.1%\n-\nGAN\n6.28%\n5.65%\nDCGAN (ours)\n2.98%\n1.48%\nFigure 9:\nSide-by-side illustration of (from left-to-right) the MNIST dataset, generations from a\nbaseline GAN, and generations from our DCGAN .\n14\nUnder review as a conference paper at ICLR 2016\nFigure 10: More face generations from our Face DCGAN.\n15\nUnder review as a conference paper at ICLR 2016\nFigure 11: Generations of a DCGAN that was trained on the Imagenet-1k dataset.\n16\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2015-11-19",
  "updated": "2016-01-07"
}