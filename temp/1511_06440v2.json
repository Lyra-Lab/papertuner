{
  "id": "http://arxiv.org/abs/1511.06440v2",
  "title": "Towards Principled Unsupervised Learning",
  "authors": [
    "Ilya Sutskever",
    "Rafal Jozefowicz",
    "Karol Gregor",
    "Danilo Rezende",
    "Tim Lillicrap",
    "Oriol Vinyals"
  ],
  "abstract": "General unsupervised learning is a long-standing conceptual problem in\nmachine learning. Supervised learning is successful because it can be solved by\nthe minimization of the training error cost function. Unsupervised learning is\nnot as successful, because the unsupervised objective may be unrelated to the\nsupervised task of interest. For an example, density modelling and\nreconstruction have often been used for unsupervised learning, but they did not\nproduced the sought-after performance gains, because they have no knowledge of\nthe supervised tasks.\n  In this paper, we present an unsupervised cost function which we name the\nOutput Distribution Matching (ODM) cost, which measures a divergence between\nthe distribution of predictions and distributions of labels. The ODM cost is\nappealing because it is consistent with the supervised cost in the following\nsense: a perfect supervised classifier is also perfect according to the ODM\ncost. Therefore, by aggressively optimizing the ODM cost, we are almost\nguaranteed to improve our supervised performance whenever the space of possible\npredictions is exponentially large.\n  We demonstrate that the ODM cost works well on number of small and\nsemi-artificial datasets using no (or almost no) labelled training cases.\nFinally, we show that the ODM cost can be used for one-shot domain adaptation,\nwhich allows the model to classify inputs that differ from the input\ndistribution in significant ways without the need for prior exposure to the new\ndomain.",
  "text": "Under review as a conference paper at ICLR 2016\nTOWARDS PRINCIPLED UNSUPERVISED LEARNING\nIlya Sutskever1, Rafal Jozefowicz1, Karol Gregor2, Danilo Rezende2, Tim Lillicrap2, Oriol Vinyals1\nGoogle Brain1 and Google DeepMind2\n{ilyasu,rafalj,karolg,danilor,countzero,vinyals}@google.com\nABSTRACT\nGeneral unsupervised learning is a long-standing conceptual problem in machine\nlearning. Supervised learning is successful because it can be solved by the mini-\nmization of the training error cost function. Unsupervised learning is not as suc-\ncessful, because the unsupervised objective may be unrelated to the supervised\ntask of interest. For an example, density modelling and reconstruction have often\nbeen used for unsupervised learning, but they did not produced the sought-after\nperformance gains, because they have no knowledge of the supervised tasks.\nIn this paper, we present an unsupervised cost function which we name the Out-\nput Distribution Matching (ODM) cost, which measures a divergence between the\ndistribution of predictions and distributions of labels. The ODM cost is appealing\nbecause it is consistent with the supervised cost in the following sense: a perfect\nsupervised classiﬁer is also perfect according to the ODM cost. Therefore, by\naggressively optimizing the ODM cost, we are almost guaranteed to improve our\nsupervised performance whenever the space of possible predictions is exponen-\ntially large.\nWe demonstrate that the ODM cost works well on number of small and semi-\nartiﬁcial datasets using no (or almost no) labelled training cases. Finally, we show\nthat the ODM cost can be used for one-shot domain adaptation, which allows the\nmodel to classify inputs that differ from the input distribution in signiﬁcant ways\nwithout the need for prior exposure to the new domain.\n1\nINTRODUCTION\nSupervised learning is successful for two reasons: it is equivalent to the minimization of the training\nerror, and stochastic gradient descent (SGD) is highly effective at minimizing training error. As a\nresult, supervised learning is robust, reliable, and highly successful in practical applications.\nUnsupervised learning is not as successful, mainly because it is not clear what the unsupervised\ncost function should be. The goal of unsupervised learning is often to improve the performance\nof a supervised learning task for which we do not have a lot of data. Due to the lack of labelled\nexamples, unsupervised cost functions do not know which of the many possible supervised tasks we\ncare about. As a result, it is difﬁcult for the unsupervised cost function to improve the performance\nof the supervised cost function. The disappointing empirical performance of unsupervised learning\nsupports this view.\nIn this paper, we present a cost function that generalizes the ideas of Casey (1986). We illustrate the\nidea in the setting of speech recognition. It is possible to evaluate the quality of a speech recognition\nsystem by measuring the linguistic plausibility of its typical outputs, without knowing whether these\noutputs are correct for their inputs. Thus, we can measure the performance of our system without\nthe use of any input-output examples.\nWe formalize and generalize this idea as follows: in conventional supervised learning, we are trying\nto ﬁnd an unknown function F from X to Y. Each training case (xi, yi) imposes a soft constraint\non F:\nF(xi) = yi\n(1)\nWe solve these equations by local optimization with the backpropagation algorithm (Rumelhart\net al., 1986).\n1\narXiv:1511.06440v2  [cs.LG]  3 Dec 2015\nUnder review as a conference paper at ICLR 2016\nsupervised\ntypical \nunsupervised \ncost\nglobal minimum of supervised cost is unrelated to  \nglobal minimum of unsupervised cost\nsupervised\nunsupervised \nODM cost\nglobal minimum of supervised cost is  \na global minimum of ODM cost\nFigure 1: Existing unsupervised cost functions (left) versus the ODM cost in the limit of inﬁnite data (right).\nThe global minima of typical unsupervised cost functions, such as the reconstruction cost, are unrelated to the\nminima of the supervised cost. The ODM cost. In contrast, the global minimum of the supervised cost function\nis also a global minimum of the ODM cost function whenever there is a function that can perfectly represent\nthe mapping. The implication is not true in reverse because the ODM cost can have additional global minima\nthat are not global minima of the supervised cost. This seemingly surprising property is possible because the\nODM cost uses unlabelled examples from both the input domain and the output domain, while conventional\nunsupervised cost functions ignore the output domain.\nWe now present the unsupervised cost function. Let D be the true data distribution over the input-\noutput pairs (x, y) ∼D. While we do not have access to many labelled samples (x, y) ∼D, we\noften have access to large quantities of unlabelled samples from x ∼D and y ∼D. This assumption\nis valid whenever unlabelled inputs and unlabelled outputs are abundant.\nWe can use uncorrelated samples from x ∼D and y ∼D to impose a valid constraint on F:\nDistr [F(x)] = Distr [y]\n(2)\nwhere Distr [z] denotes the distribution of the random variable z. This constraint is valid in the\nfollowing sense: if we have a function F such that F(xi) = yi is true for every possible training\ncase, then Distr [F(x)] = Distr [y] is satisﬁed as well. Thus, the unsupervised constraint can be\nseen as an additional labelled “training case” that conveys a lot of information whenever the output\nspace is very large.\nThis constraint can be turned into the following cost function\nKL[Distr [y] ∥Distr [F(x)]]\n(3)\nWe call it the output distribution matching (ODM) cost function, the cost literally matches the dis-\ntributions of the outputs.\nFor the constraint Distr [F(x)] = Distr [y] to be highly informative of the optimal parameters, it\nis necessary for the output space Y to be large, since otherwise the ODM cost will be trivial to\noptimize. If the output space is large, then the ODM cost is, in principle, substantially more useful\nthan conventional unsupervised cost functions which are unrelated to the ultimate supervised cost\nfunction. In contrast, the ODM cost is nearly guaranteed to improve the ﬁnal supervised performance\nwhenever there exists a very high performing function, because a supervised function that perfectly\nmaps every input to its desired output is also a global minimum of the ODM cost. See Figure 1. It\nalso means that a practitioner has a high chance of improving their supervised performance if they\nare able to optimize the ODM cost.\nWe also show how the ODM cost can be used for one-shot domain adaptation. Given a test datapoint\nfrom a distribution that is very different from the training one, our model can, in certain settings,\nidentify a nearly unique corresponding datapoint that happens to obey the training data distribu-\ntion, which can then be classiﬁed correctly. We implement this idea using a generative model as a\nmechanism for aligning two distributions without supervision.\nThis allows each new test case to be from signiﬁcantly different distribution, so it is no longer\nnecessary to assume that the test set follows a particular distribution. This capability allows our\nmodels, in principle, to work robustly with unexpected data. However, the ability to perform one-\nshot domain adaptation is not universal, and it can be achieved only in certain restricted settings.\n2\nUnder review as a conference paper at ICLR 2016\n2\nRELATED WORK\nThere has been a number of publications that considered matching statistics as an unsupervised\nlearning signal. The early work of Casey (1986) casts the problem of unsupervised OCR as a prob-\nlem of decoding a cipher, where there is an unknown mapping from images to characters identities,\nwhich must be inferred from the known statistics of language. This idea has been further explored in\nthe context of machine translation (typically in the form of matching bigrams) (Knight et al., 2006;\nHuang et al., 2006; Snyder et al., 2010; Ravi & Knight, 2008), and in unsupervised lexicon induction\n(Fung & McKeown, 1997; Koehn & Knight, 2002; Haghighi et al., 2008; Mikolov et al., 2013b).\nNotably, Knight et al. (2006) discusses the connection between unsupervised learning and language\ndecipherment, and formulates a generative model similar to the one presented in this work.\nSimilar ideas have recently been proposed for domain adaptation where a model learns to map the\nnew distribution back onto the training distribution (Tzeng et al., 2014; Gani et al., 2015). These\napproaches are closely related to the ODM cost, since they are concerned with transforming the new\ndistribution back to the training distribution.\nThere has been a lot of other work on unsupervised learning with neural networks, which is largely\nconcerned with the concept of “pre-training”. In pre-training, we ﬁrst train the model with an un-\nsupervised cost function, and ﬁnish training the model with the supervised cost. This concept was\nintroduced by Hinton et al. (2006) and Hinton & Salakhutdinov (2006) and later by Bengio et al.\n(2007). Other work used the k-means objective for pre-training (Coates et al., 2011), and this list\nof references is far from exhaustive. These ideas have also been used for semi-supervised learning\n(Kingma et al., 2014; Rasmus et al., 2015) and for transfer learning (Mesnil et al., 2012).\nMore recent examples of unsupervised pre-training are the Skip-gram model (Mikolov et al., 2013a)\nand its generalization to sentences, the Skip-thought vectors model of Kiros et al. (2015). These\nmodels use well-motivated unsupervised objective functions that appear to be genuinely useful for\na wide variety of language-processing tasks.\n3\nMETHODS\nIn this section, we present several approaches for optimizing the ODM cost.\n3.1\nODM COSTS AS GENERATIVE MODELS\nThe ODM cost can be formulated as the following generative model. Let P(x) be a model ﬁtted to\nthe marginal x ∼D, and let Pθ(y) be the distribution:\nPθ(y) =\nX\nx\nPθ(y|x)P(x)\n(4)\nThe objective is to ﬁnd a conditional Pθ(y|x) (which corresponds to Fθ(x) where θ are the param-\neters) so that Pθ(y) matches the marginal distribution y ∼D. If P(x) is an excellent model of\nx ∼D, then the cost Ey∼D[−log Pθ(y)] is precisely equivalent to the ODM cost of Eq. 3, modulo\nan additive constant. A similar generative model was presented by Knight et al. (2006).\nIt is desirable to train generative models using the variational autoencoder (VAE) of Kingma &\nWelling (2013). However, VAE training forces y to be continuous, which is undesirable since many\ndomains of interest are discrete. We address this by proposing the following generative model,\nwhich we term the xyh-model:\nPx(x) =\nZ\nh\nPx(x|h)P(h)dh\n(5)\nPx(y) =\nZ\nh\nPy(y|h)P(h)dh\n(6)\nwhose cost function is\nL = Ex∼D[−log Px(x)] + Ey∼D[−log Py(y)]\n(7)\nHere h is continuous while x and y are either continuous or discrete. It is clear that this model can\nalso be trained on labelled (x, y), whenever such data is available.\n3\nUnder review as a conference paper at ICLR 2016\nH\ndomain 1\ndomain 2\ndomain A\ndomain A\ndomain B\ndomain B\nshared\nshared\nnot shared\nnot shared\ndomain A\ndomain A\ndomain B\ndomain B\nshared\nnot shared\nnot shared\nFigure 2: The dual autoencoder. (Left:) The xyh-generative model that motivated the dual autoencoder.\n(Middle:) The dual autoencoder during training. Note that it can be trained in an entirely unsupervised fashion\non data from different domains. The dual autonecoder learns a representation that is compatible with both do-\nmains, in a manner that is entirely unsupervised. (Right:) The dual autoencoder test time. It often successfully\nlearns the correspondence between the domains even though it is not trained for this task.\nAlthough the negative log probability of the xyh-model is not identical to the ODM cost, the two are\nclosely related. Speciﬁcally, whenever the capacity of Px and Py is limited, the xyh-model will be\nforced to represent the structure that is common to Distr [x] and Distr [y] in P(h), while placing the\ndomain speciﬁc structures into Px(x|y) and Py(x|y). For example, in case of speech recognition,\nP(h) could contain the language model, since it is not economical to store the language model in\nPx(x|h) and Py(y|h).\n3.2\nTHE DUAL AUTOENCODER\nWe implemented the xyh-generative model but had difﬁculty getting sensible results in our early\nexperiments. We were able to get better results by designing an novel autoencoder model that is\ninspired by the xyh-model, which we call the dual autoencoder, which is shown in Figure 2. It\nconsists of two autoencoders whose “innermost weights” are shared with each other. More formally,\nthe dual autoencoder has the form\nx′ = f(A0f(WNf(WN−1 . . . f(W1f(B0x)) . . .)))\n(8)\ny′ = f(A1f(WNf(WN−1 . . . f(W1f(B1y)) . . .)))\n(9)\nwhere f is the nonlinearity, and the cost is\nL = Ex∼D [L1(x, x′)] + Ey∼D [L2(y, y′)]\n(10)\nwhere L1 and L2 are appropriate loss functions.\nEqs. 8 and 9 describe two autoencoders that map x to x′ and y to y′, respectively. By sharing the\nweights W1 . . . , WN between the autoencoders, the matrices (A0, B0) and (A1, B1) are encouraged\nto use compatible representations for the two modalities that align x with y in the absence of a direct\nsupervised signal. While not principled, we found this approach to work surprisingly well on simple\nproblems.\n3.3\nGENERATIVE ADVERSARIAL NETWORKS TRAINING\nThe Generative Adversarial Network (Goodfellow et al., 2014) is a procedure for training a “gen-\nerator” to produce samples that are statistically indistinguishable from a desired distribution. The\ngenerator is a neural network G that transforms a source of noise z into samples from some distri-\nbution:\nz →G(z)\n(11)\nThe GAN training algorithm maintains an adversary D(z) →[0, 1] whose goal is to distinguish be-\ntween samples x from the data distribution and samples from the generator G(z), and the generator\nG learns to fool the discriminator. Eventually, if GAN training is successful, G converges to a model\nsuch that the distribution of G(z) is indistinguishable from the target distribution.\nThe generative adversarial network offers a direct way of training generative models, and it had\nenjoyed considerable success in learning models of natural images (Denton et al., 2015). We use the\nGAN training method to train our unsupervised objective Distr [F(x)] = Distr [y] by requiring that\nF produces samples that are indistinguishable from the target distribution.\n4\nUnder review as a conference paper at ICLR 2016\n4\nEXPERIMENTS\n4.1\nDUAL AUTOENCODERS ON MNIST PERMUTATION TASK\nWe begin exploring the ODM cost by selecting a simple artiﬁcial task where the distributions over X\nand Y have rich internal structure but whose relationship is simple. We chose Y to be the distribution\nof MNIST digits, and X to be the distribution of MNIST digits whose pixels are permuted (with the\nsame permutation on all digits). See Figure 2. We call it the MNIST permutation task. The goal of\nthe task is to learn the unknown permutation using no supervised data.\nWe used a dual autoencoder whose architecture is 784-100-100-100-784, where the weights in the\n100-100-100 subnetwork were shared between the two autoencoders. While the results were insen-\nsitive to the learning rate, it was important to use a random initialization that is substantially smaller\nthan is typically prescribed for training neural networks models (namely, a unit Gaussian scaled by\n0.003/√#rows + #cols for each matrix).\nWe found that the dual autoencoder was easily able to recover the permutation without using any\ninput-output examples, as shown in Figure 3.\n1\n2\n3\n4\n5\n6\nFigure 3: Illustrative performance of the\nvarious models on the MNIST permutation\ntask. Column 1: data. Column 2: the per-\nmuted data. Column 3: Supervised Autoen-\ncoder; Column 4: Unsupervised sigmoid\ndual autoencoder; Column 5: Unsupervised\ntanh dual autoencoder; Column 6: Unsuper-\nvised relu Dual Autoencoder. Notice that the\ndual autoencoder that used the Tanh nonlin-\nearity was able to ﬁnd the correct correspon-\ndence while inverting the images. We do not\nunderstand why this happens. The sigmoid\ndual autoencoder is by far the best method\nfor recovering a mapping between two cor-\nresponding datasets.\nCuriously, we found that the sigmoid nonlinearity\nachieved by far the best results on this task, and that\nthe Relu and the Tanh nonlinearity were much less suc-\ncessful for a wide range of hyperparameters. Paradoxi-\ncally, we suspect that the slower learning of the sigmoid\nnonlinearity was beneﬁcial for the dual autoencoder, be-\ncause it caused the “capacity” of the 100-100-100 net-\nwork to grow slowly; it is plausible that the most criti-\ncal learning stage took place when the 100-100-100 net-\nwork had the lowest capacity.\n4.1.1\nDUAL AUTOENCODER FOR CIPHERS\nWe also tested the dual autoencoder on a character and a\nword cipher, tasks that were also considered by Knight\net al. (2006). In this task, we are given two text corpora\nthat follow the same distribution, but where the charac-\nters (or the words) of one of the corpora is scrambled. In\ndetail, we convert a text ﬁle into a list of integers by ran-\ndomly assigning characters (or words) to integers and by\nconsistently using this assignment throughout the ﬁle.\nBy doing this twice, we get two data sources that have\nidentical underlying statistics but different symbols. The\ngoal is to ﬁnd the hidden correspondence between the\nsymbols in both streams. It is not a difﬁcult task since\nit can be accomplished by inspecting the frequency. De-\nspite its simplicity, this task provides us with another\nway to evaluate our model.\nWe used the dual autoencoder architecture as before,\nwhere the input (as well as the the desired output) is rep-\nresented with a bag of 10 consecutive characters from a random sentence from the text corpus. The\nloss function is the softmax cross entropy.\nFor the character-level experiments, we used the architecture 100-25-25-100, and for the word-\nlevel experiments we used 1000-100-100-1000 — where we used a vocabulary of the 1000 most\nfrequent symbols in both data streams. Both architectures achieved perfect identiﬁcation of the\nsymbol mapping in less than 10,000 parameter updates when the sigmoid nonlinearity was used.\nWhile the dual autoencoder was successful on these tasks, its success critically relied on the under-\nlying statistics of the two datasets being very similar. When we trained each autoencoder on data\nfrom a different languages (namely on English and Spanish books from Project Gutenberg), it failed\n5\nUnder review as a conference paper at ICLR 2016\ndiscriminator\nFigure 4: An illustration of the MNIST classiﬁcation setup. The classiﬁer is shown below the dotted line;\nthe discriminator is shown above it. The ﬁrst few layers of the discriminator are convolutional; the remaining\nlayers are fully connected.\nto learn the correct correspondence between words with similar meanings. This indicates that the\ndual autoencoder is sensitive to systematic differences in the two distributions.\n4.2\nGENERATIVE ADVERSARIAL NETWORKS FOR MNIST CLASSIFICATION\nNext, we evaluated the GAN-based model on an artiﬁcial OCR task that was constructed from the\nMNIST dataset. In this task, we used the text from The Origin of Species downloaded from Project\nGutenberg, and arbitrarily replaced each letter with a digit between 0 and 9 in a consistent manner.\nWe call this sequence of numbers the MNIST label ﬁle. We then replaced each label with a random\nimage of an MNIST digit of the same class, thus obtaining a very long sequence of images of MNIST\ndigits which we call the MNIST image sequence.\nThe goal of this problem was to train an MLP F that maps MNIST digits to 10-dimensional vectors\nwithout using any input-output examples. Instead, we train the classiﬁer to match the 20-gram\nstatistics of the Origin of Species, which requires no input-output examples.\nWe trained an MLP F to map each MNIST digit into a 10-dimensional vector representing their\nclassiﬁcation. We used generative adversarial training where the adversary is a 1-d CNN to ensure\nsure that the distribution of 20 consecutive predictions (F(x1), . . . , F(x20)) is statistically indistin-\nguishable from the distribution of 20 consecutive labels from the MNIST label ﬁle (y1, . . . , y20),\nwhere the inputs (x1, . . . , x20) are randomly drawn from the MNIST image sequence.\nA typical architecture of our classiﬁer network was 784-300-300-10. The adversary consist of a 1D\nconvolutional neural network with the following architecture: the ﬁrst three layers are convolutional\nwith a kernel of width 7, whit the following numbers of ﬁlter maps: 10-200-200. It is followed by\nglobal max pooling over time, and the remainder of the architecture consist of the fully connected\nlayers 200-200-1. The nonlinearity was always ReLU, except for the output layer of both networks,\nwhich was the identity.\nIn these experiments, we used the squared loss and not the cross-entropy loss for the supervised\nobjective. If we used the softmax loss, our model’s output layer would have to use the softmax\nlayer, which we found to not work well with GAN training. The squared loss was necessary for\nusing a linear layer for the outputs.\nIt is notable that GAN training was also fragile, and the sensitive hyperparameters are given here:\n• We\ninitialized\neach\nmatrix\nof\nthe\ngenerator\nwith\na\nunit\nGaussian\nscaled\nby\n1.4/√#rows + #cols; for the discriminator we used 1.0/√#rows + #cols\n• The batchsize is 200\n• Total number of parameter updates: 6000\n• Learning rate of generator: 0.1/batchsize for 2000 updates; 0.03/batchsize for another\n2000 updates; and then 0.01/batchsize for 2000 updates\n6\nUnder review as a conference paper at ICLR 2016\n• Learning rate of discriminator: 0.005/batchsize\n• Learning rate of the supervised squared loss: 0.005/batchsize\nWhile learning was extremely rapid, it was highly sensitive to the choice of learning rates.\nWhile GAN training of the ODM cost alone was insufﬁcient to ﬁnd an accurate classiﬁer the prob-\nlem, we were able to achieve 4.7% test error on MNIST classiﬁcation using 4 labelled MNIST\nexamples. Here, each labelled example consists of a single labelled digit, and not a single sequence\nof 20 labelled digits. The result is insensitive to the speciﬁc set of 4 labelled examples. When using\n2 labelled MNIST digits, the classiﬁcation test error increased to 72%.\n5\nONE-SHOT LEARNING AND DOMAIN ADAPTATION\nIf the function that we wish to learn has a small number of parameters, then it should be possible to\ninfer it from a very small number of examples — and in some cases, from a single example.\nSuppose that the training distribution is D, but we are provided with a single test sample y ∼D′\nwhere D′ is an unknown test distribution. If y contains enough information to uniquely identify\na simple function G that maps samples y ∼D′ to samples x ∼D, we will be able to classify\nsamples y ∼D′ without any extra training: we could simply apply our existing classiﬁer to G(y).\nBy inferring the function G from scratch for each test sample from scratch, the model becomes\ncapable at correctly classifying test cases whose statistics are completely different from the training\ndistribution, as well test cases that are as outliers that would have otherwise confused or misled the\nclassiﬁer. This has the potential to make our classiﬁer signiﬁcantly more robust.\nFigure 5: An illustration of one-shot do-\nmain adaptation. Column 1: sample orig-\ninal data; Column 2; 1-data (y); Column\n3: the inferred x; Column 4: the recon-\nstructed y. The bottom row is an exam-\nple of an incorrect reconstruction. About\n5% of the test digits were reconstructed\nincorrectly. Note that we infer the CNN\np(y|x) for each test case separately.\nWe propose to solve these domain adaptation tasks using\nthe generative model of Eq. 4. Let P(x) be a model of the\ndata distribution and let Pθ(y|x) be an unknown likelihood\nfunction. Then, given a test case y from a distribution that\ndiffers from the data distribution, we can try to infer the\nunknown x by solving the following optimization problem:\nx∗= argmaxx,θPθ(y|x)P(x)\n(12)\nWe emphasize that this optimization is run from scratch\nfor each new test case y. This approach can succeed only\nwhen the likelihood Pθ(y|x) has few parameters that can\nbe uniquely determined from the above equation using just\none sample. The conditions under which this is likely to\nhold are discussed in sec. 6.1.\nWe validate these ideas on use the MNIST dataset for train-\ning and the 1-MNIST dataset for testing. The 1-MNIST\ndataset is obtained by replacing the intensity u of each pixel\nwith 1 −u. Thus the 1-MNIST distribution differs very sig-\nniﬁcantly from the MNIST distribution, and as a result, neu-\nral networks trained on MNIST are incapable of correctly\nclassifying instances of 1-MNIST.\nOur concrete model choices are the following: P(x) is im-\nplemented with a next-row-prediction LSTM with three hid-\nden layers that has been trained to ﬁt the MNIST distri-\nbution with the binary cross entropy loss, and P(y|x) is a\nsmall convolutional neural network (CNN) with one hidden\nlayer: its ﬁrst convolution has 5 ﬁlters of size 5×5 and its\nsecond convolution has one ﬁlter of size 5×5.\nGiven a test image y from the 1-MNIST dataset, we optimize log Pθ(y|x)P(x) over x and θ. We\nused signiﬁcant L2 regularization, and optimized this cost with 104 steps of Adagrad with 5 random\nrestarts. The results are illustrated in Fig. 5.\n7\nUnder review as a conference paper at ICLR 2016\nF is spatially local\nLong range correlations\nFigure 6: An setting where the ODM function can recover the true F without help from a supervised objective.\nIf F is incapable of modifying the long-range structure of the signal, then the ODM objective is likely to fully\ndetermine the best performing function F.\nWhile the results obtained in this section are preliminary, they show that in addition to synthesis\nand denoising, generative models can be used for aligning distributions and for expanding the set of\ndistributions to which the model is applied.\n6\nDISCUSSION\n6.1\nWHEN IS SUPERVISION NEEDED?\nWhen does the ODM fully determine the best F? If the function class is too capable, then F can map\nany distribution input distribution to any output distribution, which means that the ODM cost cannot\npossibly recover F. However, since the ODM cost is consistent, it is likely to improve generalization\nby eliminating unsuitable functions from consideration. Further, if the output space Y is small, then\nthe ODM cost will not convey enough information to be of signiﬁcant help to the supervised cost.\nHowever, there is a setting where the ODM cost function could, in principle, completely determine\nthe best supervised function F. It should succeed whenever the input distribution and the output\ndistribution contains long-range dependencies, while F is inherently incapable of modifying the\nlong range structure by being “local”: for example, if the input is a sequence, and we change the\ninput in one timestep, then the function’s output will also change in a small number of timestep.\nThis setting is illustrated in Fig. 6.\n6.2\nLIMITATIONS\nODM-based training has several limitations. The output space must be large, and the “shared hidden\nstructure” of the two space have to be sufﬁciently similar. The simple techniques used in this paper\nare unlikely to successfully learn a mapping between two spaces if their shared structures are insuf-\nﬁciently similar. It is conceivable that as we develop better methods for training generative models,\nit will become possible to beneﬁt from optimizing the ODM cost in a wider range of problems.\n7\nCONCLUSIONS\nIn this paper, we showed that the ODM cost provides a generic approach for unsupervised learning\nthat is entirely consistent with the supervised cost function. Although we were not able to develop\na reliable method that can train, e.g., an attention model Bahdanau et al. (2014) with the ODM ob-\njective, we presented evidence that the ODM objective provides a sensible way of training functions\nwithout the use of input-output examples. We expect better techniques for optimizing the ODM cost\nto make it universally applicable and useful.\nREFERENCES\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473, 2014.\nBengio, Yoshua, Lamblin, Pascal, Popovici, Dan, Larochelle, Hugo, et al. Greedy layer-wise training of deep\nnetworks. Advances in neural information processing systems, 19:153, 2007.\nCasey, Richard G. Text OCR by solving a cryptogram. International Business Machines Incorporated, Thomas\nJ. Watson Research Center, 1986.\n8\nUnder review as a conference paper at ICLR 2016\nCoates, Adam, Ng, Andrew Y, and Lee, Honglak. An analysis of single-layer networks in unsupervised feature\nlearning. In International conference on artiﬁcial intelligence and statistics, pp. 215–223, 2011.\nDenton, Emily, Chintala, Soumith, Szlam, Arthur, and Fergus, Rob. Deep generative image models using a\nlaplacian pyramid of adversarial networks. arXiv preprint arXiv:1506.05751, 2015.\nFung, Pascale and McKeown, Kathleen. A technical word-and term-translation aid using noisy parallel corpora\nacross language groups. Machine translation, 12(1-2):53–87, 1997.\nGani, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Germain, Pascal, Larochelle, Hugo, Laviolette, Franc¸ois,\nMarchand, Mario, and Lempitsky, Victor. Domain-adversarial training of neural networks. arXiv preprint\narXiv:1505.07818, 2015.\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\nAaron, and Bengio, Yoshua. Generative adversarial nets. In Advances in Neural Information Processing\nSystems, pp. 2672–2680, 2014.\nHaghighi, Aria, Liang, Percy, Berg-Kirkpatrick, Taylor, and Klein, Dan. Learning bilingual lexicons from\nmonolingual corpora. In ACL, volume 2008, pp. 771–779, 2008.\nHinton, Geoffrey E and Salakhutdinov, Ruslan R. Reducing the dimensionality of data with neural networks.\nScience, 313(5786):504–507, 2006.\nHinton, Geoffrey E, Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep belief nets.\nNeural computation, 18(7):1527–1554, 2006.\nHuang, Gary, Learned-Miller, Erik G, and McCallum, Andrew. Cryptogram decoding for optical character\nrecognition. University of Massachusetts-Amherst Technical Report, 6(45), 2006.\nKingma, Diederik P and Welling, Max. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\nKingma, Diederik P, Mohamed, Shakir, Rezende, Danilo Jimenez, and Welling, Max. Semi-supervised learning\nwith deep generative models. In Advances in Neural Information Processing Systems, pp. 3581–3589, 2014.\nKiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S, Torralba, Antonio, Urtasun, Raquel, and\nFidler, Sanja. Skip-thought vectors. arXiv preprint arXiv:1506.06726, 2015.\nKnight, Kevin, Nair, Anish, Rathod, Nishit, and Yamada, Kenji. Unsupervised analysis for decipherment prob-\nlems. In Proceedings of the COLING/ACL on Main conference poster sessions, pp. 499–506. Association\nfor Computational Linguistics, 2006.\nKoehn, Philipp and Knight, Kevin. Learning a translation lexicon from monolingual corpora. In Proceedings\nof the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9, pp. 9–16. Association for Compu-\ntational Linguistics, 2002.\nMesnil, Gr´egoire, Dauphin, Yann, Glorot, Xavier, Rifai, Salah, Bengio, Yoshua, Goodfellow, Ian J, Lavoie,\nErick, Muller, Xavier, Desjardins, Guillaume, Warde-Farley, David, et al. Unsupervised and transfer learning\nchallenge: a deep learning approach. ICML Unsupervised and Transfer Learning, 27:97–110, 2012.\nMikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781, 2013a.\nMikolov, Tomas, Le, Quoc V, and Sutskever, Ilya. Exploiting similarities among languages for machine trans-\nlation. arXiv preprint arXiv:1309.4168, 2013b.\nRasmus, Antti, Valpola, Harri, Honkala, Mikko, Berglund, Mathias, and Raiko, Tapani.\nSemi-supervised\nlearning with ladder network. arXiv preprint arXiv:1507.02672, 2015.\nRavi, Sujith and Knight, Kevin. Attacking decipherment problems optimally with low-order n-gram models.\nIn proceedings of the conference on Empirical Methods in Natural Language Processing, pp. 812–819.\nAssociation for Computational Linguistics, 2008.\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J.\nLearning representations by back-\npropagating errors. Nature, 323:533–536, 1986.\nSnyder, Benjamin, Barzilay, Regina, and Knight, Kevin. A statistical model for lost language decipherment. In\nProceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 1048–1057.\nAssociation for Computational Linguistics, 2010.\nTzeng, Eric, Hoffman, Judy, Zhang, Ning, Saenko, Kate, and Darrell, Trevor. Deep domain confusion: Maxi-\nmizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.\n9\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2015-11-19",
  "updated": "2015-12-03"
}