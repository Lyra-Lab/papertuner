{
  "id": "http://arxiv.org/abs/1511.09337v3",
  "title": "Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning",
  "authors": [
    "Yu-An Chung",
    "Hsuan-Tien Lin",
    "Shao-Wen Yang"
  ],
  "abstract": "Deep learning has been one of the most prominent machine learning techniques\nnowadays, being the state-of-the-art on a broad range of applications where\nautomatic feature extraction is needed. Many such applications also demand\nvarying costs for different types of mis-classification errors, but it is not\nclear whether or how such cost information can be incorporated into deep\nlearning to improve performance. In this work, we propose a novel cost-aware\nalgorithm that takes into account the cost information into not only the\ntraining stage but also the pre-training stage of deep learning. The approach\nallows deep learning to conduct automatic feature extraction with the cost\ninformation effectively. Extensive experimental results demonstrate that the\nproposed approach outperforms other deep learning models that do not digest the\ncost information in the pre-training stage.",
  "text": "Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning\nYu-An Chung\nDepartment of CSIE\nNational Taiwan University\nb01902040@ntu.edu.tw\nHsuan-Tien Lin\nDepartment of CSIE\nNational Taiwan University\nhtlin@csie.ntu.edu.tw\nShao-Wen Yang\nIntel Labs\nIntel Corporation\nshao-wen.yang@intel.com\nAbstract\nDeep learning has been one of the most promi-\nnent machine learning techniques nowadays, be-\ning the state-of-the-art on a broad range of applica-\ntions where automatic feature extraction is needed.\nMany such applications also demand varying costs\nfor different types of mis-classiﬁcation errors, but\nit is not clear whether or how such cost information\ncan be incorporated into deep learning to improve\nperformance. In this work, we ﬁrst design a novel\nloss function that embeds the cost information for\nthe training stage of cost-sensitive deep learning.\nWe then show that the loss function can also be in-\ntegrated into the pre-training stage to conduct cost-\naware feature extraction more effectively. Exten-\nsive experimental results justify the validity of the\nnovel loss function for making existing deep learn-\ning models cost-sensitive, and demonstrate that our\nproposed model with cost-aware pre-training and\ntraining outperforms non-deep models and other\ndeep models that digest the cost information in\nother stages.\n1\nIntroduction\nIn many real-world machine learning applications [Tan, 1993;\nChan and Stolfo, 1998; Fan et al., 2000; Zhang and Zhou,\n2010; Jan et al., 2011], classiﬁcation errors may come with\ndifferent costs; namely, some types of mis-classiﬁcation er-\nrors may be (much) worse than others. For instance, when\nclassifying bacteria [Jan et al., 2011], the cost of classify-\ning a Gram-positive species as a Gram-negative one should\nbe higher than the cost of classifying the species as another\nGram-positive one because of the consequence on treatment\neffectiveness. Different costs are also useful for building a\nrealistic face recognition system, where a government staff\nbeing mis-recognized as an impostor causes only little incon-\nvenience, but an imposer mis-recognized as a staff can result\nin serious damage [Zhang and Zhou, 2010]. It is thus impor-\ntant to take into account the de facto cost of every type of\nerror rather than only measuring the error rate and penalizing\nall types of errors equally.\nThe classiﬁcation problem that mandates the learning algo-\nrithm to consider the cost information is called cost-sensitive\nclassiﬁcation. Amongst cost-sensitive classiﬁcation algo-\nrithms, the binary classiﬁcation ones [Elkan, 2001; Zadrozny\net al., 2003] are somewhat mature with re-weighting the\ntraining examples [Zadrozny et al., 2003] being one ma-\njor approach, while the multiclass classiﬁcation ones are\ncontinuing to attract research attention [Domingos, 1999;\nMargineantu, 2001; Abe et al., 2004; Tu and Lin, 2010].\nThis work focuses on multiclass cost-sensitive classiﬁca-\ntion, whose algorithms can be grouped into three categories\n[Abe et al., 2004]. The ﬁrst category makes the predic-\ntion procedure cost-sensitive [Kukar and Kononenko, 1998;\nDomingos, 1999; Zadrozny and Elkan, 2001], generally done\nby equipping probabilistic classiﬁers with Bayes decision the-\nory. The major drawback is that probability estimates can of-\nten be inaccurate, which in term makes cost-sensitive perfor-\nmance unsatisfactory. The second category makes the training\nprocedure cost-sensitive, which is often done by transforming\nthe training examples according to the cost information [Chan\nand Stolfo, 1998; Domingos, 1999; Zadrozny et al., 2003;\nBeygelzimer et al., 2005; Langford and Beygelzimer, 2005].\nHowever, the transformation step cannot take the particular-\nities of the underlying classiﬁcation model into account and\nthus sometimes has room for improvement. The third cate-\ngory speciﬁcally extends one particular classiﬁcation model\nto be cost-sensitive, such as support vector machine [Tu and\nLin, 2010] or neural network [Kukar and Kononenko, 1998;\nZhou and Liu, 2006]. Given that deep learning stands as an\nimportant class of models with its special properties to be dis-\ncussed below, we aim to design cost-sensitive deep learning\nalgorithms within the third category while borrowing ideas\nfrom other categories.\nDeep learning models, or neural networks with deep archi-\ntectures, are gaining increasing research attention in recent\nyears. Training a deep neural network efﬁciently and effec-\ntively, however, comes with many challenges, and different\nmodels deal with the challenges differently. For instance, con-\nventional fully-connected deep neural networks (DNN) gen-\nerally initialize the network with an unsupervised pre-training\nstage before the actual training stage to avoid being trapped\nin a bad local minimal, and the unsupervised pre-training\nstage has been successfully carried out by stacked auto-\nencoders [Vincent et al., 2010; Krizhevsky and Hinton, 2011;\nBaldi, 2012]. Deep belief networks [Hinton et al., 2006;\nLe Roux and Bengio, 2008] shape the network as a gener-\narXiv:1511.09337v3  [cs.LG]  24 May 2016\native model and commonly take restricted Boltzmann ma-\nchines [Le Roux and Bengio, 2008] for pre-training. Convo-\nlutional neural networks (CNN) [LeCun et al., 1998] mimic\nthe visual perception process of human based on special net-\nwork structures that result in less need for pre-training, and\nare considered the most effective deep learning models in\ntasks like image or speech recognition [Ciresan et al., 2011;\nKrizhevsky et al., 2012; Abdel-Hamid et al., 2014].\nWhile some existing works have studied cost-sensitive\nneural networks [Kukar and Kononenko, 1998; Zhou and\nLiu, 2006], none of them have focused on cost-sensitive deep\nlearning to the best of our knowledge. That is, we are the ﬁrst\nto present cost-sensitive deep learning algorithms, with the\nhope of making deep learning more realistic for applications\nlike bacteria classiﬁcation and face recognition. In Section 2,\nwe ﬁrst formalize the cost-sensitive classiﬁcation problem\nand review related deep learning works. Then, in Section 3,\nwe start with a baseline algorithm that makes the predic-\ntion procedure cost-sensitive (ﬁrst category). The features\nextracted from the training procedure of such an algorithm,\nhowever, are cost-blind. We then initiate a pioneering study\non how the cost information can be digested in the training\nprocedure (second category) of DNN and CNN. We design\na novel loss function that matches the needs of neural net-\nwork training while embedding the cost information. Further-\nmore, we argue that for DNN pre-trained with stacked auto-\nencoders, the cost information should not only be used for the\ntraining stage, but also the pre-training stage. We then pro-\npose a novel pre-training approach for DNN (third category)\nthat mixes unsupervised pre-training with a cost-aware loss\nfunction. Experimental results on deep learning benchmarks\nand standard cost-sensitive classiﬁcation settings in Section 4\nveriﬁed that the proposed algorithm based on cost-sensitive\ntraining and cost-aware pre-training indeed yields the best\nperformance, outperforming non-deep models as well as a\nbroad spectrum of deep models that are either cost-insensitive\nor cost-sensitive in other stages. Finally, we conclude in Sec-\ntion 5.\n2\nBackground\nWe will formalize the multiclass cost-sensitive classiﬁcation\nproblem before introducing deep learning and related works.\n2.1\nMulticlass Cost-sensitive Classiﬁcation\nWe ﬁrst introduce the multiclass classiﬁcation problem and\nthen extend it to the cost-sensitive setting. The K-class clas-\nsiﬁcation problem comes with a size-N training set S =\n{(xn, yn)}N\nn=1, where each input vector xn is within an in-\nput space X, and each label yn is within a label space Y =\n{1, 2, ..., K}. The goal of multiclass classiﬁcation is to train a\nclassiﬁer g: X →Y such that the expected error Jy ̸= g(x)K\non test examples (x, y) is small.1\nMulticlass cost-sensitive classiﬁcation extends multiclass\nclassiﬁcation by penalizing each type of mis-classiﬁcation er-\nror differently based on some given costs. Speciﬁcally, con-\nsider a K by K cost matrix C, where each entry C(y, k) ∈\n[0, ∞) denotes the cost for predicting a class-y example as\n1J·K is 1 when the inner condition is true, and 0 otherwise.\nclass k and naturally C(y, y) = 0. The goal of cost-sensitive\nclassiﬁcation is to train a classiﬁer g such that the expected\ncost C(y, g(x)) on test examples is small.\nThe cost-matrix setting is also called cost-sensitive classiﬁ-\ncation with class-dependent costs. Another popular setting is\nto consider example-dependent costs, which means coupling\nan additional cost vector c ∈[0, ∞)K with each example\n(x, y), where the k-th component c[k] denotes the cost for\nclassifying x as class k. During training, each cn that accom-\npanies (xn, yn) is also fed to the learning algorithm to train a\nclassiﬁer g such that the expected cost c[g(x)] is small with\nrespect to the distribution that generates (x, y, c) tuples. The\ncost-matrix setting can be cast as a special case of the cost-\nvector setting by deﬁning the cost vector in (x, y, c) as row y\nof the cost matrix C. In this work, we will eventually propose\na cost-sensitive deep learning algorithm that works under the\nmore general cost-vector setting.\n2.2\nNeural Network and Deep Learning\nThere are many deep learning models that are successful for\ndifferent applications nowadays [Lee et al., 2009; Krizhevsky\nand Hinton, 2011; Ciresan et al., 2011; Krizhevsky et al.,\n2012; Simonyan and Zisserman, 2014]. In this work, we\nﬁrst study the fully-connected deep neural network (DNN)\nfor multiclass classiﬁcation as a starting point of making\ndeep learning cost-sensitive. The DNN consists of H hid-\nden layer and parameterizes each layer i ∈{1, 2, ..., H} by\nθi = {Wi, bi}, where Wi is a fully-connected weight ma-\ntrix and bi is a bias vector that enter the neurons. That is, the\nweight matrix and bias vector applied on the input are stored\nwithin θ1 = {W1, b1}. For an input feature vector x, the H\nhidden layers of the DNN describe a complex feature trans-\nform function by computing φ(x) = s(WH · s(· · · s(W2 ·\ns(W1 ·x+b1)+b2) · · · )+bH), where s(z) =\n1\n1+exp(−z) is\nthe component-wise logistic function. Then, to perform mul-\nticlass classiﬁcation, an extra softmax layer, parameterized\nby θsm = {Wsm, bsm}, is placed after the H-th hidden layer.\nThere are K neurons in the softmax layer, where the j-th neu-\nron comes with weights W(j)\nsm and bias b(j)\nsm and is responsible\nfor estimating the probability of class j given x:\nP(y = j|x) =\nexp(φ(x)T W(j)\nsm + b(j)\nsm)\nPK\nk=1 exp(φ(x)T W(k)\nsm + b(k)\nsm)\n.\n(1)\nBased on the probability estimates, the classiﬁer trained from\nthe DNN is naturally g(x) = argmax1≤k≤KP(y = k|x).\nTraditionally, the parameters {{θi}H\ni=1, θsm} of the DNN\nare optimized by the back-propagation algorithm, which is\nessentially gradient descent, with respect to the negative log-\nlikelihood loss function over the training set S:\nLNLL(S) =\nN\nX\nn=1\n−ln(P(y = yn|xn)).\n(2)\nThe strength of the DNN, through multiple layers of non-\nlinear transforms, is to extract sophisticated features automat-\nically and implement complex functions. However, the train-\ning of the DNN is non-trivial because of non-convex opti-\nmization and gradient diffusion problems, which degrade the\ntest performance of the DNN when adding too many layers.\n[Hinton et al., 2006] ﬁrst proposed a greedy layer-wise pre-\ntraining approach to solve the problem. The layer-wise pre-\ntraining approach performs a series of feature extraction steps\nfrom the bottom (input layer) to the top (last hidden layer) to\ncapture higher level representations of original features along\nthe network propagation.\nIn this work, we shall improve a classical yet effective\nunsupervised pre-training strategy, stacked denoising auto-\nencoders [Vincent et al., 2010], for the DNN. Denoising auto-\nencoder (DAE) is an extension of regular auto-encoder. An\nauto-encoder is essentially a (shallow) neural network with\none hidden layer, and consists of two parameter sets: {W, b}\nfor mapping the (normalized) input vector x ∈[0, 1]d to the\nd′-dimensional latent representation h by h = s(W·x+b) ∈\n[0, 1]d′; {W′, b′} for reconstructing an input vector ˜x from h\nby ˜x = s(W′ · h + b′). The auto-encoder is trained by min-\nimizing the total cross-entropy loss LCE(S) over S, deﬁned\nas\n−\nN\nX\nn=1\nd\nX\nj=1\n\u0010\nxn[j] ln ˜xn[j]+(1−xn[j]) ln(1−˜xn[j])\n\u0011\n, (3)\nwhere xn[j] denotes the j-th component of xn and ˜xn[j] is\nthe corresponding reconstructed value.\nThe DAE extends the regular auto-encoder by randomly\nadding noise to inputs xn before mapping to the latent repre-\nsentation, such as randomly setting some components of xn\nto 0. Several DAEs can then be stacked to form a deep net-\nwork, where each layer receives its input from the latent rep-\nresentation of the previous layer. For the DNN, initializing\nwith stacked DAEs is known to perform better than initializ-\ning with stacked regular auto-encoders [Vincent et al., 2010]\nor initializing randomly. Below we will refer the DNN ini-\ntialized with stacked DAEs and trained (ﬁne-tuned) by back-\npropagation with (2) as the SDAE, while restricting the DNN\nto mean the model that is initialized randomly and trained\nwith (2).\nIn this work, we will also extend another popular deep\nlearning model, the convolutional neural network (CNN), for\ncost-sensitive classiﬁcation. The CNN is based on a locally-\nconnected network structure that mimics the visual percep-\ntion process [LeCun et al., 1998]. We will consider a standard\nCNN structure speciﬁed in Caffe2 [Jia et al., 2014], which\ngenerally does not rely on a pre-training stage. Similar to the\nDNN, we consider the CNN with a softmax layer for multi-\nclass classiﬁcation.\n2.3\nCost-sensitive Neural Network\nFew existing works have studied cost-sensitive classiﬁca-\ntion using neural networks [Kukar and Kononenko, 1998;\nZhou and Liu, 2006]. [Zhou and Liu, 2006] focused on study-\ning the effect of sampling and threshold-moving to tackle\nthe class imbalance problem using neural network as a core\nclassiﬁer rather than proposing general cost-sensitive neu-\nral network algorithms. [Kukar and Kononenko, 1998] pro-\nposed four approaches of modifying neural networks for cost-\nsensitivity. The ﬁrst two approaches train a usual multiclass\n2https://github.com/BVLC/caffe/tree/master/examples/cifar10\nclassiﬁcation neural network, and then make the prediction\nstage of the trained network cost-sensitive by including the\ncosts in the prediction formula; the third approach modiﬁes\nthe learning rate of the training algorithm base on the costs;\nthe fourth approach, called MIN (minimization of the mis-\nclassiﬁcation costs), modiﬁes the loss function of neural net-\nwork training directly. Among the four proposed algorithms,\nMIN consistently achieves the lowest test cost [Kukar and\nKononenko, 1998] and will be taken as one of our competi-\ntors. Nevertheless, none of the existing works, to the best\nof our knowledge, have conducted careful study on cost-\nsensitive algorithms for deep neural networks.\n3\nCost-sensitive Deep Learning\nBefore we start describing our proposed algorithm, we high-\nlight a na¨ıve algorithm. For the DNN/SDAE/CNN that esti-\nmate the probability with (1), when given the full picture of\nthe cost matrix, a cost-sensitive prediction can be obtained\nusing Bayes optimal decision, which computes the expected\ncost of classifying an input vector x to each class and predicts\nthe label that reaches the lowest expected cost:\ng(x) = argmin\n1⩽k⩽K\nK\nX\ny=1\nP(y|x)C(y, k).\n(4)\nWe will denote these algorithms as DNNBayes, SDAEBayes\nand CNNBayes, respectively. These algorithms do not include\nthe costs in the pre-training nor training stages. Also, those\nalgorithms require knowing the full cost matrix, and cannot\nwork under the cost-vector setting.\n3.1\nCost-sensitive Training\nThe DNN essentially decomposes the multiclass classiﬁca-\ntion problem to per-class probability estimation problems\nvia the well-known one-versus-all (OVA) decomposition. [Tu\nand Lin, 2010] proposed the one-sided regression algorithm\nthat extends OVA for support vector machine (SVM) to a\ncost-sensitive SVM by considering per-class regression prob-\nlems. In particular, if regressors rk(x) ≈c[k] can be learned\nproperly, a reasonable prediction can be made by\ngr(x) ≡argmin\n1⩽k⩽K\nrk(x).\n(5)\n[Tu and Lin, 2010] further argued that the loss function of the\nregressor rk with respect to c[k] should be one-sided. That\nis, rk(x) is allowed to underestimate the smallest cost c[y]\nand to overestimate other costs. Deﬁne zn,k = 2Jcn[k] =\ncn[yn]K −1 for indicating whether cn[k] is the smallest\nwithin cn. The cost-sensitive SVM [Tu and Lin, 2010] mini-\nmizes a regularized version of the total one-sided loss ξn,k =\nmax(zn,k·(rk(xn)−cn[k]), 0), where rk are formed by (ker-\nnelized) linear models. With such a design, the cost-sensitive\nSVM enjoys the following property [Tu and Lin, 2010]:\ncn[gr(xn)] ⩽\nK\nX\nk=1\nξn,k.\n(6)\nThat is, an upper bound PK\nk=1 ξn,k of the total cost paid by gr\non xn is minimized within the cost-sensitive SVM.\nIf we replace the softmax layer of the DNN or the CNN\nwith regression outputs (using the identity function instead of\nthe logistic one for outputting), we can follow [Tu and Lin,\n2010] to make DNN and CNN cost-sensitive by letting each\noutput neuron estimate c[k] as rk and predicting with (5).\nThe training of the cost-sensitive DNN and CNN can also be\ndone by minimizing the total one-sided loss. Nevertheless, the\none-sided loss is not differentiable at some points, and back-\npropagation (gradient descent) cannot be directly applied. We\nthus derive a smooth approximation of ξn,k instead. Note that\nthe new loss function should not only approximate ξn,k but\nalso be an upper bound of ξn,k to keep enjoying the bound-\ning property of (6). [Lee and Mangasarian, 2001] has shown a\nsmooth approximation u+ 1\nα·ln(1+exp(−αu)) ≈max(u, 0)\nwhen deriving the smooth SVM. Taking α = 1 leads to\nLHS = ln(1 + exp(u)), which is trivially an upper bound of\nmax(u, 0) because ln(1+exp(u)) > u, and ln(1+exp(u)) >\nln(1) = 0. Based on the approximation, we deﬁne\nδn,k ≡ln(1 + exp(zn,k · (rk(xn) −cn[k]))).\n(7)\nδn,k is not only a smooth approximation of ξn,k that enjoys\nthe differentiable property, but also an upper bound of ξn,k to\nkeep the bounding property of (6) held. That is, we can still\nensure a small total cost by minimizing the newly deﬁned\nsmooth one-sided regression (SOSR) loss over all examples:\nLSOSR(S) =\nN\nX\nn=1\nK\nX\nk=1\nδn,k.\n(8)\nWe will refer to these algorithms, which replace the soft-\nmax layer of the DNN/SDAE/CNN with a regression layer\nparameterized by θSOSR = {WSOSR, bSOSR} and mini-\nmize (8) with back-propagation, as DNNSOSR, SDAESOSR\nand CNNSOSR. These algorithms work with the cost-vector\nsetting. They include costs in the training stage, but not the\npre-training stage.\n3.2\nCost-aware Pre-training\nFor multiclass classiﬁcation, the pre-training stage, either in a\ntotally unsupervised or partially supervised manner [Bengio\net al., 2007], has been shown to improve the performance of\nthe DNN and several other deep models [Bengio et al., 2007;\nHinton et al., 2006; Erhan et al., 2010]. The reason is that pre-\ntraining usually helps initialize a neural network with better\nweights that prevent the network from getting stuck in poor\nlocal minima. In this section, we propose a cost-aware pre-\ntraining approach that leads to a novel cost-sensitive deep\nneural network (CSDNN) algorithm.\nCSDNN is designed as an extension of SDAESOSR. In-\nstead of pre-training with SDAE, CSDNN takes stacked cost-\nsensitive auto-encoders (CAE) for pre-training instead. For\na given cost-sensitive example (x, y, c), CAE tries not only\nto denoise and reconstruct the original input x like DAE, but\nalso to digest the cost information by reconstructing the cost\nvector c. That is, in addition to {W, b} and {W′, b′} for\nDAE, CAE introduces an extra parameter set {W′′, b′′} fed\nto regression neurons from the hidden representation. Then,\nwe can mix the two loss functions LCE and LSOSR with a\nbalancing coefﬁcient β ∈[0, 1], yielding the following loss\nfunction for CAE over S:\nLCAE(S) = (1 −β) · LCE(S) + β · LSOSR(S)\n(9)\nThe mixture step is a widely-used technique for multi-criteria\noptimization [Hillermeier, 2001], where β controls the bal-\nance between reconstructing the original input x and the cost\nvector c. A positive β makes CAE cost-aware during its fea-\nture extraction, while a zero β makes CAE degenerate to\nDAE. Similar to DAEs, CAEs can then be stacked to initial-\nize a deep neural network before the weights are ﬁne-tuned by\nback-propagation with (8). The resulting algorithm is named\nCSDNN, which is cost-sensitive in both the pre-training stage\n(by CAE) and the training stage (by (8)), and can work under\nthe general cost-vector setting. The full algorithm is listed in\nAlgorithm 1.\nAlgorithm 1 CSDNN\nInput: Cost-sensitive training set S = {(xn, yn, cn)}N\nn=1\n1: for each hidden layer θi = {Wi, bi} do\n2:\nLearn a CAE by minimizing (9).\n3:\nTake {Wi, bi} of CAE as θi.\n4: end for\n5: Fine-tune the network parameters {{θi}H\ni=1, θSOSR} by\nminimizing (8) using back-propagation.\nOutput: The ﬁne-tuned deep neural network with (5) as gr.\nCSDNN is essentially SDAESOSR with DAEs replaced by\nCAEs with the hope of more effective cost-aware feature ex-\ntraction. We can also consider SCAEBayes which does the\nsame for SDAEBayes. The CNN, due to its special network\nstructure, generally does not rely on stacked DAEs for pre-\ntraining, and hence cannot be extended by stacked CAEs.\nAs discussed, DAE is a degenerate case of CAE. Another\npossible degeneration is to consider CAE with less complete\ncost information. For instance, a na¨ıve cost vector deﬁned by\nˆcn[k] = Jyn ̸= kK encodes the label information (whether\nthe prediction is erroneous with respect to the demanded la-\nbel) but not the complete cost information. To study whether\nit is necessary to take the complete cost information into\naccount in CAE, we design two variant algorithms that re-\nplace the cost vectors in CAEs with ˆcn[k], which effectively\nmakes those CAEs error-aware. Then, SCAEBayes becomes\nSEAEBayes (with E standing for error); CSDNN becomes\nSEAESOSR.\n4\nExperiments\nIn the previous section, we have derived many cost-sensitive\ndeep learning algorithms, each with its own specialty. They\ncan be grouped into two series: those minimizing with (2) and\npredicting with (4) are Bayes-series algorithms (DNNBayes,\nSDAEBayes, SEAEBayes, SCAEBayes, and CNNBayes);\nthose minimizing with (8) and predicting with (5) are SOSR-\nseries algorithms (DNNSOSR, SDAESOSR, SEAESOSR,\nCSDNN ≡SCAESOSR, CNNSOSR). Note that the Bayes-\nseries can only be applied to the cost-matrix setting while the\nSOSR-series can deal with the cost-vector setting. The two\nseries help understand whether it is beneﬁcial to consider the\ncost information in the training stage.\nWithin each series, CNN is based on a locally-connected\nstructure, while DNN, SDAE, SEAE and SCAE are fully-\nconnected and differ by how pre-training is conducted, rang-\ning from none, unsupervised, error-aware, to cost-aware. The\nwide range helps understand the effectiveness of digesting the\ncost information in the pre-training stage.\nNext, the two series will be compared with the blind-series\nalgorithms (DNNblind, SDAEblind, and CNNblind), which\nare the existing algorithms that do not incorporate the cost in-\nformation at all, to understand the importance of taking the\ncost information into account. The two series will also be\ncompared against two baseline algorithms: CSOSR [Tu and\nLin, 2010], a non-deep algorithm that our proposed SOSR-\nseries originates from; MIN [Kukar and Kononenko, 1998], a\nneural-network algorithm that is cost-sensitive in the training\nstage like the SOSR-series but with a different loss function.\nThe algorithms along with highlights on where the cost infor-\nmation is digested are summarized in Table 1.\n4.1\nSetup\nWe conducted experiments on MNIST, bg-img-rot (the hard-\nest variant of MNIST provided in [Larochelle et al., 2007]),\nSVHN [Netzer et al., 2011], and CIFAR-10 [Krizhevsky and\nHinton, 2009]. The ﬁrst three datasets belong to handwritten\ndigit recognition and aim to classify each image into a digit\nof 0 to 9 correctly; CIFAR-10 is a well-known image recog-\nnition dataset which contains 10 classes such as car, ship and\nanimal. For all four datasets, the training, validation, and test-\ning split follows the source websites; the input vectors in the\ntraining set are linearly scaled to [0, 1], and the input vectors\nin the validation and testing sets are scaled accordingly.\nThe four datasets are originally collected for multiclass\nclassiﬁcation and contain no cost information. We adopt the\nmost frequently-used benchmark in cost-sensitive learning,\nthe randomized proportional setup [Abe et al., 2004], to gen-\nerate the costs. The setup is for the cost-matrix setting. It\nﬁrst generates a K × K matrix C, and sets the diagonal en-\ntries C(y, y) to 0 while sampling the non-diagonal entries\nC(y, k) uniformly from [0, 10 |{n:yn=k}|\n|{n:yn=y}|]. The randomized\nproportional setup generates the cost information that takes\nthe class distribution of the dataset into account, charging\na higher cost (in expectation) for mis-classifying a minority\nclass, and can thus be used to deal with imbalanced classiﬁca-\ntion problems. Note that we take this benchmark cost-matrix\nsetting to give prediction-stage cost-sensitive algorithms like\nthe Bayes-series a fair chance of comparison. We ﬁnd that the\nrange of the costs can affect the numerical stability of the al-\ngorithms, and hence scale all the costs by the maximum value\nwithin C during training in our implementation. The reported\ntest results are based on the unscaled C.\nArguably one of the most important use of cost-sensitive\nclassiﬁcation is to deal with imbalanced datasets. Neverthe-\nless, the four datasets above are somewhat balanced, and the\nrandomized proportional setup may generate similar cost for\neach type of mis-classiﬁcation error. To better meet the real-\nworld usage scenario, we further conducted experiments to\nevaluate the algorithms with imbalanced datasets. In partic-\nular, for each dataset, we construct a variant dataset by ran-\ndomly picking four classes and removing 70% of the exam-\nples that belong to those four classes. We will name these im-\nbalanced variants as MNISTimb, bg-img-rotimb, SVHNimb,\nand CIFAR-10imb, respectively.\nAll experiments were conducted using Theano. For algo-\nrithms related to DNN and SDAE, we selected the hyperpa-\nrameters by following [Vincent et al., 2010]. The β in (9),\nneeded by SEAE and SCAE algorithms, was selected among\n{0, 0.05, 0.1, 0.25, 0.4, 0.75, 1}. As mentioned, for CNN, we\nconsidered a standard structure in Caffe [Jia et al., 2014].\n4.2\nExperimental Results\nThe average test cost of each algorithm along with the stan-\ndard error is shown in Table 2. The best result3 per dataset\namong all algorithms is highlighted in bold.\nIs it necessary to consider costs? DNNblind and SDAEblind\nperformed the worst on almost all the datasets. While\nCNNblind was slightly better than those two, it never reached\nthe best performance for any dataset. The results indicate the\nnecessity of taking the cost information into account.\nIs it necessary to go deep? The two existing cost-sensitive\nbaselines, CSOSR and MIN, outperformed the cost-blind\nalgorithms often, but were usually not competitive to cost-\nsensitive deep learning algorithms. The results validate the\nimportance of studying cost-sensitive deep learning.\nIs it necessary to incorporate costs during training?\nSOSR-series models, especially under the imbalanced sce-\nnario, generally outperformed their Bayes counterparts. The\nresults demonstrate the usefulness of the proposed (7) and (8)\nand the importance of incorporating the cost information dur-\ning the training stage.\nIs it necessary to incorporate costs during pre-training?\nCSDNN outperformed both SEAESOSR and SDAESOSR,\nand SDAESOSR further outperformed DNNSOSR. The re-\nsults show that for the fully-connected structure where pre-\ntraining is needed, our newly proposed cost-aware pre-\ntraining with CAE is indeed helpful in making deep learning\ncost-sensitive.\nWhich is better, CNNSOSR or CSDNN?\nThe last two\ncolumns in Table 2 show that CSDNN is competitive to\nCNNSOSR, with both algorithms usually achieving the best\nperformance. CSDNN is slightly better on two datasets. Note\nthat CNNs are known to be powerful for image recognition\ntasks, which match the datasets that we have used. Hence,\nit is not surprising that CNN can reach promising perfor-\nmance with our proposed SOSR loss (8). Our efforts not only\nmake CNN cost-sensitive, but also result in the CSDNN algo-\nrithm that makes the full-connected deep neural network cost-\nsensitive with the help of cost-aware pre-training via CAE.\nIs the mixture loss necessary? To have more insights on\nCAE, we also conducted experiments to evaluate the perfor-\n3The selected CSDNN that achieved the test cost listed in Ta-\nble 2 is composed of 3 hidden layers, and each hidden layer consists\nof 3000 neurons.\nTable 1: cost-awareness of algorithms (O: cost-aware; E: error-aware; X: cost-blind)\naaaaaaaa\nStage\nAlgorithm\nDNNblind\nSDAEblind\nCNNblind\nCSOSR\nMIN\nDNNBayes\nSDAEBayes\nSEAEBayes\nSCAEBayes\nCNNBayes\nDNNSOSR\nSDAESOSR\nSEAESOSR\nCSDNN\nCNNSOSR\npre-training\nnone\nX\nnone\nnone\nnone\nnone\nX\nE\nO\nnone\nnone\nX\nE\nO\nnone\ntraining\nX\nX\nX\nO\nO\nX\nX\nX\nX\nX\nO\nO\nO\nO\nO\nprediction\nX\nX\nX\nX\nX\nO\nO\nO\nO\nO\nX\nX\nX\nX\nX\nTable 2: Average test cost\naaaaaaaa\nDataset\nAlgorithm\nDNNblind\nSDAEblind\nCNNblind\nCSOSR\nMIN\nDNNBayes\nSDAEBayes\nSEAEBayes\nSCAEBayes\nCNNBayes\nDNNSOSR\nSDAESOSR\nSEAESOSR\nCSDNN\nCNNSOSR\nMNIST\n0.11 ± 0.00\n0.10 ± 0.00\n0.10 ± 0.00\n0.10 ± 0.00\n0.10 ± 0.003\n0.10 ± 0.00\n0.09 ± 0.00\n0.09 ± 0.00\n0.09 ± 0.00\n0.09 ± 0.00\n0.10 ± 0.00\n0.09 ± 0.00\n0.09 ± 0.00\n0.09 ± 0.00\n0.08 ± 0.00\nbg-img-rot\n3.33 ± 0.06\n3.28 ± 0.07\n3.05 ± 0.07\n3.25 ± 0.06\n3.02 ± 0.06\n2.95 ± 0.07\n2.66 ± 0.07\n2.85 ± 0.07\n2.54 ± 0.07\n2.40 ± 0.07\n3.21 ± 0.07\n2.99 ± 0.07\n3.00 ± 0.07\n2.34 ± 0.07\n2.29 ± 0.07\nSVHN\n1.58 ± 0.03\n1.40 ± 0.03\n0.91 ± 0.03\n1.17 ± 0.03\n1.19 ± 0.03\n1.07 ± 0.03\n0.93 ± 0.03\n0.94 ± 0.03\n0.88 ± 0.03\n0.85 ± 0.03\n1.02 ± 0.03\n0.92 ± 0.03\n0.99 ± 0.03\n0.83 ± 0.03\n0.82 ± 0.03\nCIFAR-10\n3.46 ± 0.04\n3.26 ± 0.05\n2.51 ± 0.04\n3.30 ± 0.04\n3.19 ± 0.05\n2.80 ± 0.05\n2.52 ± 0.05\n2.68 ± 0.05\n2.38 ± 0.04\n2.34 ± 0.05\n2.74 ± 0.05\n2.48 ± 0.04\n2.52 ± 0.05\n2.24 ± 0.05\n2.25 ± 0.04\nMNISTimb\n0.32 ± 0.01\n0.31 ± 0.01\n0.19 ± 0.01\n0.26 ± 0.01\n0.27 ± 0.01\n0.23 ± 0.01\n0.20 ± 0.01\n0.20 ± 0.01\n0.18 ± 0.01\n0.18 ± 0.01\n0.22 ± 0.01\n0.20 ± 0.01\n0.19 ± 0.01\n0.17 ± 0.01\n0.17 ± 0.01\nbg-img-rotimb\n15.9 ± 0.70\n13.8 ± 0.70\n5.04 ± 0.67\n8.55 ± 0.70\n8.40 ± 0.69\n7.19 ± 0.69\n5.10 ± 0.70\n4.95 ± 0.70\n4.73 ± 0.70\n4.49 ± 0.68\n6.89 ± 0.70\n4.99 ± 0.69\n4.86 ± 0.69\n4.16 ± 0.68\n4.39 ± 0.69\nSVHNimb\n1.79 ± 0.01\n1.60 ± 0.01\n0.31 ± 0.01\n1.05 ± 0.01\n0.99 ± 0.01\n0.53 ± 0.01\n0.33 ± 0.01\n0.34 ± 0.01\n0.29 ± 0.01\n0.28 ± 0.01\n0.51 ± 0.01\n0.31 ± 0.01\n0.31 ± 0.01\n0.26 ± 0.01\n0.28 ± 0.01\nCIFAR-10imb\n19.1 ± 0.09\n17.7 ± 0.09\n7.29 ± 0.08\n10.1 ± 0.09\n11.2 ± 0.09\n8.16 ± 0.09\n7.48 ± 0.09\n7.25 ± 0.08\n6.97 ± 0.09\n6.81 ± 0.09\n7.86 ± 0.08\n7.44 ± 0.09\n7.14 ± 0.09\n6.48 ± 0.09\n6.63 ± 0.08\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.08\n0.085\n0.09\n0.095\n0.1\n0.105\n0.11\nMNIST\n0\n0.2\n0.4\n0.6\n0.8\n1\n2.2\n2.4\n2.6\n2.8\n3\n3.2\n3.4\nbg−img−rot\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.8\n0.85\n0.9\n0.95\n1\nSVHN\n0\n0.2\n0.4\n0.6\n0.8\n1\n2.2\n2.3\n2.4\n2.5\n2.6\nCIFAR−10\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.16\n0.18\n0.2\n0.22\n0.24\nMNISTimb\n0\n0.2\n0.4\n0.6\n0.8\n1\n4\n4.2\n4.4\n4.6\n4.8\n5\n5.2\nbg−img−rotimb\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.24\n0.26\n0.28\n0.3\n0.32\n0.34\nSVHNimb\n0\n0.2\n0.4\n0.6\n0.8\n1\n6.4\n6.6\n6.8\n7\n7.2\n7.4\n7.6\nCIFAR−10imb\nFigure 1: Relation between β and test cost (note that\nSDAESOSR is the data point with β = 0).\nmance of CSDNN for β ∈[0, 1]. When β = 0, CSDNN de-\ngenerates to SDAESOSR; when β = 1, each CAE of CSDNN\nperforms fully cost-aware pre-training to ﬁt the cost vectors.\nThe results are displayed in Figure 1, showing a roughly U-\nshaped curve. The curve implies that some β ∈[0, 1] that best\nbalances the tradeoff between denoising and cost-awareness\ncan be helpful. The results validate the usefulness of the pro-\nposed mixture loss (9) for pre-training.\n5\nConclusion\nWe proposed a novel deep learning algorithm CSDNN for\nmulticlass cost-sensitive classiﬁcation with deep learning.\nExisting baselines and other alternatives within the blind-\nseries, the Bayes-series and the SOSR-series were exten-\nsively compared with CSDNN carefully to validate the impor-\ntance of each component of CSDNN. The experimental re-\nsults demonstrate that incorporating the cost information into\nboth the pre-training and the training stages leads to promis-\ning performance of CSDNN, outperforming those baselines\nand alternatives. One key component of CSDNN, namely the\nSOSR loss for cost-sensitivity in the training stage, is shown\nto be helpful in improving the performance of CNN. The re-\nsults justify the importance of the proposed SOSR loss for\ntraining and the CAE approach for pre-training.\n6\nAcknowledgement\nWe thank the anonymous reviewers for valuable suggestions.\nThis material is based upon work supported by the Air Force\nOfﬁce of Scientiﬁc Research, Asian Ofﬁce of Aerospace\nResearch and Development (AOARD) under award number\nFA2386-15-1-4012, and by the Ministry of Science and Tech-\nnology of Taiwan under number MOST 103-2221-E-002-\n149-MY3.\nReferences\n[Abdel-Hamid et al., 2014] Ossama Abdel-Hamid, Abdel-\nrahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and\nDong Yu.\nConvolutional neural networks for speech\nrecognition.\nAudio, Speech, and Language Processing,\nIEEE/ACM Transactions on, 22(10):1533–1545, 2014.\n[Abe et al., 2004] Naoki Abe, Bianca Zadrozny, and John\nLangford.\nAn iterative method for multi-class cost-\nsensitive learning. In KDD, 2004.\n[Baldi, 2012] Pierre Baldi.\nAutoencoders, unsupervised\nlearning, and deep architectures. Unsupervised and Trans-\nfer Learning Challenges in Machine Learning, 2012.\n[Bengio et al., 2007] Yoshua Bengio, Pascal Lamblin, Dan\nPopovici, and Hugo Larochelle. Greedy layer-wise train-\ning of deep networks. In NIPS, 2007.\n[Beygelzimer et al., 2005] Alina Beygelzimer, Varsha Dani,\nTom Hayes, John Langford, and Bianca Zadrozny. Error\nlimiting reductions between classiﬁcation tasks. In ICML,\n2005.\n[Chan and Stolfo, 1998] Philip K. Chan and Salvatore J.\nStolfo. Toward scalable learning with non-uniform class\nand cost distributions: A case study in credit card fraud\ndetection. In KDD, 1998.\n[Ciresan et al., 2011] Dan C Ciresan, Ueli Meier, Jonathan\nMasci, and J¨urgen Schmidhuber.\nFlexible, high perfor-\nmance convolutional neural networks for image classiﬁ-\ncation. In IJCAI, 2011.\n[Domingos, 1999] Pedro Domingos.\nMetacost: A general\nmethod for making classiﬁers cost-sensitive.\nIn KDD,\n1999.\n[Elkan, 2001] Charles Elkan.\nThe foundations of cost-\nsensitive learning. In IJCAI, 2001.\n[Erhan et al., 2010] Dumitru Erhan, Yoshua Bengio, Aaron\nCourville, Pierre-Antoine Manzagol, Pascal Vincent, and\nSamy Bengio. Why does unsupervised pre-training help\ndeep learning? JMLR, 11:625–660, 2010.\n[Fan et al., 2000] Wei Fan, Wenke Lee, Salvatore J. Stolfo,\nand Matthew Miller. A multiple model cost-sensitive ap-\nproach for intrusion detection. In ECML, 2000.\n[Hillermeier, 2001] Claus Hillermeier.\nNonlinear multiob-\njective optimization. Birkhauser, 2001.\n[Hinton et al., 2006] Geoffrey E. Hinton, Simon Osindero,\nand Yee-Whye Teh. A fast learning algorithm for deep\nbelief nets. Neural Computation, 18:1527–1554, 2006.\n[Jan et al., 2011] Te-Kang Jan, Hsuan-Tien Lin, Hsin-Pai\nChen, Tsung-Chen Chern, Chung-Yueh Huang, Bing-\nCheng Wen, Chia-Wen Chung, Yung-Jui Li, Ya-Ching\nChuang, Li-Li Li, Yu-Jiun Chan, Juen-Kai Wang, Yuh-Lin\nWang, Chi-Hung Lin, and Da-Wei Wang. Cost-sensitive\nclassiﬁcation on pathogen species of bacterial meningitis\nby Surface Enhanced Raman Scattering. In BIBM, 2011.\n[Jia et al., 2014] Yangqing Jia, Evan Shelhamer, Jeff Don-\nahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser-\ngio Guadarrama, and Trevor Darrell. Caffe: Convolutional\narchitecture for fast feature embedding.\narXiv preprint\narXiv:1408.5093, 2014.\n[Krizhevsky and Hinton, 2009] A. Krizhevsky and G. Hin-\nton. Learning multiple layers of features from tiny im-\nages. Master’s thesis, Department of Computer Science,\nUniversity of Toronto, 2009.\n[Krizhevsky and Hinton, 2011] Alex Krizhevsky and Geof-\nfrey E. Hinton. Using very deep autoencoders for content-\nbased image retrieval. In ESANN, 2011.\n[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever,\nand Geoffrey E. Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In NIPS, 2012.\n[Kukar and Kononenko, 1998] Matjaz\nKukar\nand\nIgor\nKononenko. Cost-sensitive learning with neural networks.\nIn ECAI, 1998.\n[Langford and Beygelzimer, 2005] John Langford and Alina\nBeygelzimer. Sensitive error correcting output codes. In\nCOLT, 2005.\n[Larochelle et al., 2007] Hugo Larochelle, Dumitru Erhan,\nAaron Courville, James Bergstra, and Yoshua Bengio. An\nempirical evaluation of deep architectures on problems\nwith many factors of variation. In ICML, 2007.\n[Le Roux and Bengio, 2008] Nicolas Le Roux and Yoshua\nBengio. Representational power of restricted boltzmann\nmachines and deep belief networks. Neural Computation,\n20:1631–1649, 2008.\n[LeCun et al., 1998] Yann LeCun, L´eon Bottou, Yoshua\nBengio, and Patrick Haffner. Gradient-based learning ap-\nplied to document recognition. Proceedings of the IEEE,\n86:2278–2324, 1998.\n[Lee and Mangasarian, 2001] Yuh-Jye Lee and O. L. Man-\ngasarian. SSVM: A smooth support vector machine. Com-\nputational Optimization and Applications, 20:5–22, 2001.\n[Lee et al., 2009] Honglak Lee, Roger Grosse, Rajesh Ran-\nganath, and Andrew Y. Ng. Convolutional deep belief net-\nworks for scalable unsupervised learning of hierarchical\nrepresentations. In ICML, 2009.\n[Margineantu, 2001] Dragos D. Margineantu. Methods for\ncost-sensitive learning. In IJCAI, 2001.\n[Netzer et al., 2011] Yuval Netzer, Tao Wang, Adam Coates,\nAlessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading\ndigits in natural images with unsupervised feature learn-\ning. In NIPS workshop on deep learning and unsupervised\nfeature learning, 2011.\n[Simonyan and Zisserman, 2014] Karen Simonyan and An-\ndrew Zisserman.\nVery deep convolutional networks\nfor large-scale image recognition.\narXiv preprint\narXiv:1409.1556, 2014.\n[Tan, 1993] Ming Tan. Cost-sensitive learning of classiﬁca-\ntion knowledge and its applications in robotics. Machine\nLearning, 13:7–33, 1993.\n[Tu and Lin, 2010] Han-Hsing Tu and Hsuan-Tien Lin. One-\nsided support vector regression for multiclass cost-\nsensitive classiﬁcation. In ICML, 2010.\n[Vincent et al., 2010] Pascal Vincent, Hugo Larochelle, Is-\nabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Man-\nzagol. Stacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local denoising\ncriterion. JMLR, 11:3371–3408, 2010.\n[Zadrozny and Elkan, 2001] Bianca Zadrozny and Charles\nElkan.\nLearning and making decisions when costs and\nprobabilities are both unknown. In KDD, 2001.\n[Zadrozny et al., 2003] Bianca Zadrozny, John Langford,\nand Naoki Abe.\nCost-sensitive learning by cost-\nproportionate example weighting. In ICDM, 2003.\n[Zhang and Zhou, 2010] Yin Zhang and Zhi-Hua Zhou.\nCost-sensitive face recognition. Pattern Analysis and Ma-\nchine Intelligence, IEEE Transactions on, 32:1758–1769,\n2010.\n[Zhou and Liu, 2006] Zhi-Hua\nZhou\nand Xu-Ying\nLiu.\nTraining cost-sensitive neural networks with methods ad-\ndressing the class imbalance problem.\nKnowledge and\nData Engineering, IEEE Transactions on, 18:63–77, 2006.\n",
  "categories": [
    "cs.LG",
    "cs.NE"
  ],
  "published": "2015-11-30",
  "updated": "2016-05-24"
}