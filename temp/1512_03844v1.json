{
  "id": "http://arxiv.org/abs/1512.03844v1",
  "title": "Efficient Deep Feature Learning and Extraction via StochasticNets",
  "authors": [
    "Mohammad Javad Shafiee",
    "Parthipan Siva",
    "Paul Fieguth",
    "Alexander Wong"
  ],
  "abstract": "Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances.",
  "text": "arXiv:1512.03844v1  [cs.LG]  11 Dec 2015\n1\nEfﬁcient Deep Feature Learning and\nExtraction via StochasticNets\nMohammad Javad Shaﬁee, Parthipan Siva\nPaul Fieguth and Alexander Wong\nAbstract—Deep neural networks are a powerful tool for feature learning\nand extraction given their ability to model high-level abstractions in highly\ncomplex data. One area worth exploring in feature learning and extraction\nusing deep neural networks is efﬁcient neural connectivity formation for\nfaster feature learning and extraction. Motivated by ﬁndings of stochastic\nsynaptic connectivity formation in the brain as well as the brain’s uncanny\nability to efﬁciently represent information, we propose the efﬁcient learning\nand extraction of features via StochasticNets, where sparsely-connected\ndeep neural networks can be formed via stochastic connectivity between\nneurons. To evaluate the feasibility of such a deep neural network ar-\nchitecture for feature learning and extraction, we train deep convolutional\nStochasticNets to learn abstract features using the CIFAR-10 dataset,\nand extract the learned features from images to perform classiﬁcation on\nthe SVHN and STL-10 datasets. Experimental results show that features\nlearned using deep convolutional StochasticNets, with fewer neural con-\nnections than conventional deep convolutional neural networks, can allow\nfor better or comparable classiﬁcation accuracy than conventional deep\nneural networks: relative test error decrease of ∼4.5% for classiﬁcation\non the STL-10 dataset and ∼1% for classiﬁcation on the SVHN dataset.\nFurthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classiﬁcation accu-\nracy even when only 10% of the training data is used for feature learning.\nFinally, it was also shown that signiﬁcant gains in feature extraction speed\ncan be achieved in embedded applications using StochasticNets. As such,\nStochasticNets allow for faster feature learning and extraction performance\nwhile facilitate for better or comparable accuracy performances.\nIndex Terms—Deep feature learning, Deep feature extraction, Random\ngraph, Stochastic neural connectivity\n✦\n1\nINTRODUCTION\nDeep neural networks are a powerful tool for feature learn-\ning and extraction given their ability to represent and model\nhigh-level abstractions in highly complex data. Deep neural\nnetworks have shown considerable capabilities in producing\nfeatures that enable state-of-the-art performance for handling\ncomplex tasks such as speech recognition [1], [2], object recog-\nnition [3], [4], [5], [6], and natural language processing [7], [8].\nRecent advances in improving the performance of deep neural\nnetworks for feature learning and extraction have focused\non areas such as network regularization [9], [10], activation\nfunctions [11], [12], [13], and deeper architectures [6], [14], [15],\nwhere the goal is to learn more representative features with\nrespect to increasing task accuracy.\nDespite the power capabilities of deep neural networks for\nfeature learning and extraction, they are very rarely employed\non embedded devices such as video surveillance cameras,\nsmartphones, and wearable devices. This difﬁcult migration of\ndeep neural networks into embedded applications for feature\nextraction stems largely from the fact that, unlike the highly\npowerful distributed computing systems and GPUs that are\n•\nThe authors are with the Department of Systems Design Engineering, Uni-\nversity of Waterloo, Waterloo, Ontario, Canada.\nE-mail: {mjshaﬁee, a28wong, pﬁeguth}@ uwaterloo.ca\nManuscript received ..., 2015; revised ... .\nFig. 1: An illustrative example of a random graph. All possible\nedge connectivity between the nodes in the graph may occur\nindependently with a probability of pij.\noften leveraged for deep learning networks, the low-power\nCPUs commonly used in embedded systems simply do not\nhave the computational power to make deep neural networks\na feasible solution for feature extraction.\nMuch of the focus on migrating deep neural networks for\nfeature learning and extraction in embedded systems have\nbeen to design custom embedded processing units dedicated\nto accelerating deep neural networks [16], [17], [18] . However,\nsuch an approach greatly limits the ﬂexibility of the type of\ndeep neural network architectures that can be used. Further-\nmore, such an approach requires adding additional hardware,\nwhich adds to the cost and complexity of the embedded\nsystem. On the other hand, improving the efﬁciency of deep\nneural networks for feature learning and extraction is much\nless explored, with considerably fewer strategies proposed so\nfar [19]. In particular, very little exploration has been con-\nducted on efﬁcient neural connectivity formation for efﬁcient\nfeature learning and extraction, which can hold considerable\npromise it achieving highly efﬁcient deep neural network\narchitectures that can be used in embedded applications.\nOne way to address this challenge is to draw inspiration\nfrom the brain which has an uncanny ability to efﬁciently\nrepresent information. In particular, we are inspired by the\nway brain develops synaptic connectivity between neurons.\nRecently, in a pivotal paper by [20], data of living brain tissue\nfrom Wistar rats was collected and used to construct a partial\nmap of a rat brain. Based on this map, Hill et al. came to a\nvery surprising conclusion. The synaptic formation, of speciﬁc\nfunctional connectivity in neocortical neural microcircuits, was\nfound to be stochastic in nature. This is in sharp contrast to the\nway deep neural networks are formed, where connectivity is\nlargely deterministic and pre-deﬁned.\nMotivated by ﬁndings of random neural connectivity for-\nmation and the efﬁcient information representation capabilities\nof the brain, we proposed the learning of efﬁcient feature\nrepresentations via StochasticNets [21], where the key idea is\nto leverage random graph theory [22], [23] to form sparsely-\nconnected deep neural networks via stochastic connectivity\nbetween neurons. The connection sparsity, in particular for\ndeep convolutional networks, allows for more efﬁcient feature\nlearning and extraction due to the sparse nature of receptive\nﬁelds which requires less computation and memory access. We\nwill show that these sparsely-connected deep neural networks,\nwhile computationally efﬁcient, can still maintain the same\naccuracies as the traditional deep neural networks. Further-\n2\nmore, the StochasticNet architecture for feature learning and\nextraction presented in this work can also beneﬁt from all of\nthe same approaches used for traditional deep neural networks\nsuch as data augmentation and stochastic pooling to further\nimprove performance.\nThe paper is organized as follows. First, a review of random\ngraph theory is presented in Section 2. The theory and de-\nsign considerations behind forming StochasticNet as a random\ngraph realizations are discussed in Section 3. Experimental re-\nsults where we train deep convolutional StochasticNets to learn\nabstract features using the CIFAR-10 dataset [24], and extract\nthe learned features from images to perform classiﬁcation on\nthe SVHN [25] and STL-10 [26] datasets is presented in Section\n5. Finally, conclusions are drawn in Section 6.\n2\nREVIEW OF RANDOM GRAPH THEORY\nThe underlying idea of deep feature learning via Stochastic-\nNets is to leverage random graph theory [22], [23] to form the\nneural connectivity of deep neural networks in a stochastic\nmanner such that the resulting neural networks are sparsely\nconnected yet maintaining feature representation capabilities.\nAs such, it is important to ﬁrst provide a general overview of\nrandom graph theory for context. In random graph theory, a\nrandom graph can be deﬁned as the probability distribution\nover graphs [27]. A number of different random graph models\nhave been proposed in literature.\nA commonly studied random graph model is that proposed\nby [22], in which a random graph can be expressed by G(n, p),\nwhere all possible edge connectivity are said to occur indepen-\ndently with a probability of p, where 0 < p < 1. This random\ngraph model was generalized by [28], in which a random graph\ncan be expressed by G(V, pij), where V is a set of vertices and\nthe edge connectivity between two vertices {i, j} in the graph\nis said to occur with a probability of pij, where pij ∈[0, 1].\nTherefore, based on this generalized random graph model,\nrealizations of random graphs can be obtained by starting\nwith a set of n vertices V = {vq|1 ≤q ≤n} and randomly\nadding a set of edges between the vertices based on the set\nof possible edges E = {eij|1 ≤i ≤n, 1 ≤j ≤n , i ̸= j} inde-\npendently with a probability of pij. A number of realizations\nof a random graph are provided in Figure 2 for illustrative\npurposes. It is worth noting that because of the underlying\nprobability distribution, the generated realizations of the ran-\ndom graph often exhibit differing edge connectivity.\nGiven that deep neural networks can be fundamentally\nexpressed and represented as graphs G, where the neurons\nare vertices V and the neural connections are edges E, one\nintriguing idea for introducing stochastic connectivity for the\nformation of deep neural networks is to treat the formation\nof deep neural networks as particular realizations of random\ngraphs, which we will describe in greater detail in the next\nsection.\n3\nSTOCHASTICNETS: DEEP NEURAL NETWORKS AS\nRANDOM GRAPH REALIZATIONS\nLet us represent a deep neural network as a random\ngraph G\n\u0010\nV, p(i →j)\n\u0011\n,\nwhere V\nis\nthe\nset\nof\nneurons\nV = {vi|1 ≥i ≥n}, with vi denoting the ith neuron and n\ndenoting the total number of neurons, in the deep neural net-\nwork and p(i →j) is the probability that a neural connection\noccurs between neuron vi and vj. It is worth noting that since\nneural networks are directed graphs, to this end the probability\np(·) is represented in a directed way from source node to the\ndestination node. As such, one can then form a deep neural\nnetwork as a realization of the random graph G\n\u0010\nV, p(i →j)\n\u0011\nby starting with a set of neurons V, and randomly adding\nneural connections between the set of neurons independently\nwith a probability of p(i →j) as deﬁned above.\nWhile one can form practically any type of deep neural\nnetwork as a random graph realizations, an important design\nconsideration for forming deep neural networks as random\ngraph realizations is that different types of deep neural net-\nworks have fundamental properties in their network architec-\nture that must be taken into account and preserved in the ran-\ndom graph realization. Therefore, to ensure that fundamental\nproperties of the network architecture of a certain type of deep\nneural network is preserved, the probability p(i →j) must\nbe designed in such a way that these properties are enforced\nappropriately in the resultant random graph realization. Let us\nconsider a general deep feed-forward neural network. First, in\na deep feed-forward neural network, there can be no neural\nconnections between non-adjacent layers. Second, in a deep\nfeed-forward neural network, there can be no neural connec-\ntions between neurons on the same layer. Therefore, to enforce\nthese two properties, p(i →j) = 0 when l(i) ̸= l(j) + 1 where\nl(i) encodes the layer number associated to the node i. An\nexample random graph based on this random graph model\nfor representing general deep feed-forward neural networks\nis shown in Figure 3(a), with an example feed-forward neural\nnetwork graph realization is shown in Figure 3(b). Note that\nthe neural connectivity for each neuron may be different due\nto the stochastic nature of neural connection formation.\n4\nFEATURE LEARNING VIA DEEP CONVOLUTIONAL\nSTOCHASTICNETS\nAs one of the most commonly used types of deep neural\nnetworks for feature learning is deep convolutional neural\nnetworks [29], let us investigate the efﬁcacy of efﬁcient feature\nlearning can be achieved via deep convolutional Stochastic-\nNets. Deep convolutional neural networks can provide a gen-\neral abstraction of the input data by applying the sequential\nconvolutional layers to the input data (e.g. input image). The\ngoal of convolutional layers in a deep neural network is to\nextract discriminative features to feed into the classiﬁer such\nthat the fully connected layers play the role of classiﬁcation in\ndeep neural networks. Therefore, the combination of receptive\nﬁelds in convolutional layers can be considered as the feature\nextractor in these models. The receptive ﬁelds’ parameters\nmust be trained to ﬁnd optimal parameters leading to most\ndiscriminative features.\nHowever learning those parameters is not possible every\ntime due to the computational complexity or lake of enough\ntraining data such that general receptive ﬁelds (i.e., convolu-\ntional layers) without learning is desirable. On the other hand,\nthe computational complexity of extracting features is another\nconcern which should be addressed. Essentially, extracting\nfeatures in a deep convolutional neural network is a sequence\nof convolutional processes which can be represented as mul-\ntiplications and summations and the number of operations is\ndependent on the number of parameters of receptive ﬁelds.\n3\nFig. 2: Realizations of a random graph with 50 nodes. The probability for edge connectivity between all nodes in the graph was\nset to pi,j = 0.5 for all nodes i and j. Each diagram demonstrates a different realization of the random graph.\nlayer 1\nlayer 2\nlayer 3\nlayer 1\nlayer 2\nlayer 3\n(a) RG Representation\n(b) FFNN Graph Realization\nFig. 3: Example random graph representing a section of a\ndeep feed-forward neural network (a) and its realization (b).\nEvery neuron i may be connected to neuron j with probability\np(i →j) based on random graph theory. To enforce the prop-\nerties of a general deep feed-forward neural network, there\nis no neural connectivity between nodes that are not in the\nadjacent layers. As shown in (b), the neural connectivity of\nnodes in the realization are varied since they are drawn based\non a probability distribution.\nMotivated by those reasons, sparsifying the receptive ﬁelds\nwhile maintaining the generality of them is highly desirable.\nTo this end, we want to sparsify the receptive ﬁeld mo-\ntivated by the StochasticNet framework to provide efﬁcient\ndeep feature learning. First, in additional to the design con-\nsiderations for p(i →j) presented in the previous section to\nenforce the properties of deep feed-forward neural networks,\nadditional considerations must be taken to preserve the prop-\nerties of deep convolutional neural networks, which is a type\nof deep feed-forward neural network.\nSpeciﬁcally, the neural connectivity for each randomly real-\nized receptive ﬁeld K in the deep convolutional StochasticNet\nis based on a probability distribution, with the neural connec-\ntivity conﬁguration thus being shared amongst different small\nneural collections for a given randomly realized receptive ﬁeld.\nAn example of a realized deep convolutional StochasticNet\nis shown in Figure 4. As seen, the neural connectivity for\nrandomly realized receptive ﬁeld Ki,1 is completely different\nfrom randomly realized receptive ﬁeld Ki,2. The response of\neach randomly realized receptive ﬁeld leads to an output in\nnew channel in layer i + 1.\nA realized deep convolutional StochasticNet can then be\ntrained to learn efﬁcient feature representations via supervised\nlearning using labeled data. One can then use the trained\nStochasticNet for extracting a set of abstract features from input\ndata.\n4.1\nRelationship to Other Methods\nWhile a number of stochastic strategies for improving deep\nneural network performance have been previously intro-\nFig. 4: Forming a deep convolutional StochasticNet. The neural\nconnectivity for each randomly realized receptive ﬁeld (in\nthis case, {Ki,1, Ki,2}) are determined based on a probability\ndistribution, and as such the conﬁguration of each randomly\nrealized receptive ﬁeld may differ. It can be seen that the\nshape and neural connectivity for receptive ﬁeld Ki,1 is com-\npletely different from receptive ﬁeld Ki,2. The response of each\nrandomly realized receptive ﬁeld leads to an output in new\nchannel C. Only one layer of the formed deep convolutional\nStochasticNet is shown for illustrative purposes.\nduced [10], [19] and [30], it is very important to note that\nthe proposed StochasticNets is fundamentally different from\nthese existing stochastic strategies in that StochasticNets main\nsigniﬁcant contributions deals primarily with the formation\nof neural connectivity of individual neurons to construct ef-\nﬁcient deep neural networks that are inherently sparse prior\nto training, while previous stochastic strategies deal with ei-\nther the grouping of existing neural connections to explicitly\nenforce sparsity [19], or removal/introduction of neural con-\nnectivity for regularization during training. More speciﬁcally,\nStochasticNets is a realization of a random graph formed\nprior to training and as such the connectivity in the network\nare inherently sparse, and are permanent and do not change\nduring training. This is very different from Dropout [30] and\nDropConnect [10] where the activations and connections are\ntemporarily removed during training and put back during test\nfor regularization purposes only, and as such the resulting\nneural connectivity of the network remains dense. There is\nno notion of dropping in StochasticNets as only a subset\nof possible neural connections are formed in the ﬁrst place\nprior to training, and the resulting network connectivity of the\n4\nnetwork is sparse.\nStochasticNets are also very different from HashNets [19],\nwhere connection weights are randomly grouped into hash\nbuckets, with each bucket sharing the same weights, to ex-\nplicitly sparsifying into the network, since there is no notion\nof grouping/merging in StochasticNets; the formed Stochas-\nticNets are naturally sparse due to the formation process.\nIn fact, stochastic strategies such as HashNets, Dropout, and\nDropConnect can be used in conjunction with StochasticNets.\n5\nEXPERIMENTAL RESULTS\nTo investigate the efﬁcacy of efﬁcient feature learning via\nStochasticNets, we form deep convolutional StochasticNets\nand train the constructed StochasticNets using the CIFAR-\n10 [24] image dataset for generating generic features. Based on\nthe trained StochasticNets, features are then extracted for the\nSVHN [25] and STL-10 [26] image datasets and image classiﬁ-\ncation performance using these extracted deep features within\na neural network classiﬁer framework are then evaluated in\na number of different ways. It is important to note that the\nmain goal is to investigate the efﬁcacy of feature learning via\nStochasticNets and the inﬂuence of stochastic connectivity pa-\nrameters on feature representation performance, and not to ob-\ntain maximum absolute classiﬁcation performance; therefore,\nthe performance of StochasticNets can be further optimized\nthrough additional techniques such as data augmentation and\nnetwork regularization methods.\n5.0.1\nDatasets\nThe CIFAR-10 image dataset [24] consists of 50,000 training\nimages categorized into 10 different classes (5,000 images per\nclass) of natural scenes. Each image is an RGB image that\nis 32×32 in size. The MNIST image dataset [29] consists of\n60,000 training images and 10,000 test images of handwritten\ndigits. Each image is a binary image that is 28×28 in size,\nwith the handwritten digits are normalized with respect to\nsize and centered in each image. The SVHN image dataset [25]\nconsists of 604,388 training images and 26,032 test images of\ndigits in natural scenes. Each image is an RGB image that is\n32×32 in size. Finally, the STL-10 image dataset [26] consists\nof 5,000 labeled training images and 8,000 labeled test images\ncategorized into 10 different classes (500 training images and\n800 training images per class) of natural scenes. Each image is\nan RGB image that is 96×96 in size. The images were resized\nto 32 × 32 to have the same network conﬁguration for all\nexperimented datasets for consistency purposes. Note that the\n100,000 unlabeled images in the STL-10 image dataset were not\nused in this paper.\n5.0.2\nStochasticNet Conﬁguration\nThe deep convolutional StochasticNets used in this paper\nare realized based on the LeNet-5 deep convolutional neural\nnetwork [29] architecture, and consists of three convolutional\nlayers with 32, 32, and 64 receptive ﬁelds for the ﬁrst, second,\nand third convolutional layers, respectively, and one hidden\nlayer of 64 neurons, with all neural connections being ran-\ndomly realized based on probability distributions. The neural\nconnectivity formation for the deep convolutional Stochastic-\nNet realizations is achieved via acceptance-rejection sampling,\nand can be expressed by:\nei→j exists where\nh\np(i →j) ≥T\ni\n= 1\n(1)\nwhere ei→j is the neural connectivity from node i to node j,\n[·] is the Iverson bracket, and T encodes the sparsity of neural\nconnectivity in the StochasticNet. While it is possible to take\nadvantage of any arbitrary distribution to construct deep con-\nvolutional StochasticNet realizations, for the purpose of this\npaper two different spatial neural connectivity models were\nexplored for the convolutional layers: i) uniform connectivity\nmodel:\np(i →j) =\n(\nU(0, 1)\nj ∈Ri\n0\notherwise.\nand ii) a Gaussian connectivity model:\np(i →j) =\n(\nN(i, σ)\nj ∈Ri\n0\notherwise.\nwhere the mean is at the center of the receptive ﬁeld (i.e., i)\nand the standard deviation σ is set to be a third of the receptive\nﬁeld size. In this study, Ri is deﬁned as a 5 × 5 spatial region\naround node i, which means that neural connectivity of 100 the\nresulting receptive ﬁeld is equivalent to a dense 5 × 5 receptive\nﬁeld used for ConvNets. Finally, for comparison purposes, the\nconventional ConvNet used as a baseline is conﬁgured with\nthe same network architecture using 5 × 5 receptive ﬁelds.\n5.1\nNumber of Neural Connections\nAn experiment was conducted to illustrate the impact of the\nnumber of neural connections on the feature representation\ncapabilities of StochasticNets. Figure 6 demonstrates the train-\ning and test error versus the number of neural connections\nin the network for the STL-10 dataset. The neural connection\nprobability is varied to achieve the desired number of neural\nconnections for testing its effect on feature representation ca-\npabilities.\nFigure 6 demonstrates the training and testing error vs.\nthe neural connectivity percentage relative to the baseline\nConvNet, for two different spatial neural connectivity models:\ni) uniform connectivity model, and ii) Gaussian connectivity\nmodel. It can be observed that classiﬁcation using the features\nfrom the StochasticNet is able to achieve the better or similar\ntest error as using the features from the ConvNet when the\nnumber of neural connections in the StochasticNet is fewer\nthan the ConvNet. In particular, classiﬁcation using the fea-\ntures from the StochasticNet is able to achieve the same test\nerror as using the features from the ConvNet when the number\nof neural connections in the StochasticNet is half that of the\nConvNet. It can be also observed that, although increasing the\nnumber of neural connections resulted in lower training error,\nit does not exhibit reductions in test error, and as such it can\nbe observed that the proposed StochasticNets can improve the\nhandling of over-ﬁtting associated with deep neural networks\nwhile decreasing the number of neural connections, which\nin effect greatly reduces the number of computations and\nthus resulting in more efﬁcient feature learning and feature\nextraction. Finally, it is also observed that there is a notice-\nable difference in the training and test errors when using the\nGaussian connectivity model when compared to the uniform\nconnectivity model, which indicates that the choice of neural\nconnectivity probability distributions can have a noticeable\nimpact on feature representation capabilities.\n5\nNetwork Model\nNetwork Formation\nTraining\nTesting\nDropout\nDrop-Connect\nHashNet\nStochasticNet\nFig. 5: Visual comparison among different related methods to StochasticNet. Constructing a deep neural network can be divided\ninto 4 steps. 1) Network Model: dropout, drop-connect and Hashnet methods start with the conventional convNet where as\nstochastic starts with a random graph. 2) Network Formation: droput and drop-connect approaches utilize the formed network\nfrom previous step while Hashent groups edges to have same weight (shown in same color) and stochastic net samples the\nconstructed random graph. 3)Training: dropout and drop-connect methods drop actiavation function results or the weight\nconnectivities randomly to regularize the training network while the Hashnet and StochasticNet obtain the same conﬁguration\nas previous step (i.e., Network Formation). 4) Testing: in this step dropout, drop-connect and Hashnet approximate all network\nparameters and get the output based on the complete network structure while StochasticNet outputs by the sparse trained\nnetwork.\nNeural Connectivity Percentage\n0\n20\n40\n60\n80\n100\nError\n0.3\n0.4\n0.5\n0.6\n0.7\nTraining Error\nTest Error\nNeural Connectivity Percentage\n0\n20\n40\n60\n80\n100\nError\n0.3\n0.4\n0.5\n0.6\n0.7\nTraining Error\nTest Error\n(a) Gaussian Connectivity Model\n(b) Uniform Connectivity Model\nFig. 6: Training and test error versus the number of neural connections for the STL-10 dataset. Both Gaussian and uniform\nneural connectivity models were evaluated. Note that neural connectivity percentage of 100 is equivalent to ConvNet, since all\nconnections are made. As seen a StochasticNet with fewer neural connectivity compared to the ConvNet provides better than or\nsimilar accuracy to the ConvNet.\n6\nEpoch\n0\n10\n20\n30\n40\n50\nError\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nStochasticNet Training Error\nStochasticNet Test Error\nConvNet Training Error\nConvNet Test Error\nEpoch\n0\n10\n20\n30\n40\n50\nError\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nStochasticNet Training Error\nStochasticNet Test Error\nConvNet Training Error\nConvNet Test Error\n(a) SVHN\n(b) STL-10\nFig. 7: Comparison of classiﬁcation performance between deep features extracted with a standard ConvNet and that extracted\nwith a StochasticNet containing 75% neural connectivity as the ConvNet. The StochasticNet is realized based on a uniform\nconnectivity model. The StochasticNet results in a 0.5% improvement in relative test error for the STL-10 dataset, as well as\nprovides a smaller gap between the training error and test error. The StochasticNet using a uniform connectivity model achieved\nthe same performance as the ConvNet for the SVHN dataset.\nEpoch\n0\n10\n20\n30\n40\n50\nError\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nStochasticNet Training Error\nStochasticNet Test Error\nConvNet Training Error\nConvNet Test Error\nEpoch\n0\n10\n20\n30\n40\n50\nError\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nStochasticNet Training Error\nStochasticNet Test Error\nConvNet Training Error\nConvNet Test Error\n(a) SVHN\n(b) STL-10\nFig. 8: Comparison of classiﬁcation performance between deep features extracted with a standard ConvNet and that extracted\nwith a StochasticNet containing 75% neural connectivity as the ConvNet. The StochasticNet is realized based on a Gaussian\nconnectivity model. The StochasticNet results in a 4.5% improvement in relative test error for the STL-10 dataset, as well as\nprovides a smaller gap between the training error and test error. A 1% relative improvement is also observed for the SVHN\ndataset.\n5.2\nComparisons with ConvNet for Feature Learning\nMotivated by the results shown in Figure 6, a comprehensive\nexperiment were done to investigate the efﬁcacy of feature\nlearning via StochasticNets on CIFAR-10 and utilize them to\nclassify the SVHN and STL-10 image datasets. Deep con-\nvolutional StochasticNet realizations were formed with 75%\nneural connectivity using the Gaussian connectivity model as\nwell as the uniform connectivity model when compared to a\nconventional ConvNet. The performance of the StochasticNets\nand the ConvNets was evaluated based on 25 trials and the\nreported results are based on the best of the 25 trials in terms\nof training error. Figure 7 and Figure 8 shows the training and\ntest error results of classiﬁcation using learned deep features\nfrom CIFAR-10 using the StochasticNets and ConvNets on the\nSVHN and STL-10 datasets via the uniform connectivity model\nand the Gaussian connectivity model, respectively. It can be ob-\nserved that, in the case where the uniform connectivity model\nis used, the test error for classiﬁcation using features learned\nusing StochasticNets, with just 75% of neural connections as\nConvNets, is approximately the same as ConvNets for both the\nSVHN and STL-10 datasets (with ∼0.5% test error reduction\nfor STL-10). It can also be observed that, in the case where\nthe Gaussian connectivity model is used, the test error for\nclassiﬁcation using features learned using StochasticNets, with\njust 75% of neural connections as ConvNets, is approximately\nthe same (∼1% relative test error reduction) as ConvNets for\nthe SVHN dataset. More interestingly, it can also be observed\nthat the test error for classiﬁcation using features learned\nusing StochasticNets, with just 75% of neural connections as\nConvNets, is reduced by ∼4.5% compared to ConvNets for the\nSTL-10 dataset. Furthermore, the gap between the training and\ntest errors of classiﬁcation using features extracted using the\nStochasticNets is less than that of the ConvNets, which would\nindicate reduced overﬁtting in the StochasticNets.\nThese results illustrate the efﬁcacy of feature learning via\nStochasticNets in providing efﬁcient feature learning and ex-\ntraction while preserving feature representation capabilities,\n7\nwhich is particularly important for real-world applications\nwhere efﬁcient feature extraction performance is necessary.\n5.3\nTraining Set Size\nAn experiment was conducted to illustrate the impact of the\nsize of the training set on the feature representation capabilities\nof StochasticNets. To perform this experiment, deep convolu-\ntional StochasticNet realizations were formed with 75% neural\nconnectivity using the Gaussian connectivity model as well\nas the uniform connectivity model, and different percentages\nof the CIFAR-10 dataset was used for feature learning. The\ntrained StochasticNet realizations where then used to perform\nclassiﬁcation on the STL-10 dataset to evaluate training and\ntest error performance analysis.\nFigure 9 demonstrates the training and testing error vs. the\ntraining set size, for the two tested connectivity models. It can\nbe observed that the features extracted using the Stochastic-\nNets can provide comparable classiﬁcation performance even\nwhen only 30% of the training data is used in the case of the\nGaussian connectivity model. Furthermore, it was observed\nthat there was only a 3% drop in test error when only 10%\nof the training data is used. More interest is the case of\nthe uniform connectivity model, where the features extracted\nusing the StochasticNets can provide comparable classiﬁcation\nperformance with no increase in test error even when only 10%\nof the training data, which illustrates the efﬁcacy of feature\nlearning via StochasticNets in situations where the training size\nis small.\n5.4\nRelative Feature Extraction Speed vs. Number of Neu-\nral Connections\nPrevious sections showed that StochasticNets can achieve good\nfeature learning performance relative to conventional Con-\nvNets, while having signiﬁcantly fewer neural connections. We\nnow investigate the feature extraction speed of StochasticNets,\nrelative to the feature extraction speed of ConvNets, with\nrespect to the number of neural connections formed in the\nconstructed StochasticNets. To this end, the convolutions in\nthe StochasticNets are implemented as a sparse matrix dot\nproduct, while the convolutions in the ConvNets are imple-\nmented as a matrix dot product. For fair comparison, both\nimplementations do not make use of any hardware-speciﬁc op-\ntimizations such as Streaming SIMD Extensions (SSE) because\nmany industrial embedded architectures used in applications\nsuch as embedded video surveillance systems do not support\nhardware optimization such as SSE.\nAs with Section 5.3, the neural connection probability is\nvaried in both the convolutional layers and the hidden layer to\nachieve the desired number of neural connections for testing\nits effect on the feature extraction speed of the formed Stochas-\nticNets. Figure 10 demonstrates the relative feature extraction\ntime vs. the neural connectivity percentage, where relative time\nis deﬁned as the time required during the feature extraction\nprocess relative to that of the ConvNet. It can be observed that\nthe relative feature extraction time decreases as the number of\nneural connections decrease, which illustrates the potential for\nStochasticNets to enable more efﬁcient feature extraction.\nInterestingly, it can be observed that speed improvements\nare seen immediately, even at 90% connectivity, which may\nappear quite surprising given that sparse representation of\nmatrices often suffer from high computational overhead when\nrepresenting dense matrices. However, in this case, the number\nof connections in the randomly realized receptive ﬁeld is very\nsmall relative to the typical input image size. As a result,\nthe overhead associated with using sparse representations\nis largely negligible when compared to the speed improve-\nments from the reduced computations gained by eliminating\neven one connection in the receptive ﬁeld. Therefore, these\nresults show that StochasticNets can have signiﬁcant merit\nfor enabling the feature representation power of deep neural\nnetworks to be leveraged for a large number of industrial\nembedded applications.\n6\nCONCLUSIONS\nIn this paper, we proposed the learning of efﬁcient feature\nrepresentations via StochasticNets, where sparsely-connected\ndeep neural networks are constructed by way of stochastic\nconnectivity between neurons. Such an approach facilitates for\nmore efﬁcient neural utilization, resulting in reduced compu-\ntational complexity for feature learning and extraction while\npreserving feature representation capabilities. The effective-\nness of feature learning via StochasticNet was investigated by\ntraining StochasticNets using the CIFAR-10 dataset and using\nthe learned features for classiﬁcation of images in the SVHN\nand STL-10 image datasets. The StochasticNet features were\nthen compared to the features extracted using a conventional\nconvolutional neural network (ConvNet). Experimental results\ndemonstrate that classiﬁcation using features extracted via\nStochasticNets provided better or comparable accuracy than\nfeatures extracted via ConvNets, even when the number of\nneural connections is signiﬁcantly fewer. Furthermore, Stochas-\nticNets, with fewer neural connections than the conventional\nConvNet, can reduce the over ﬁtting issue associating with the\nconventional ConvNet. As a result, deep feature learning and\nextraction via StochasticNets can allow for more efﬁcient fea-\nture extraction while facilitating for better or similar accuracy\nperformances.\nACKNOWLEDGMENT\nThis work was supported by the Natural Sciences and Engi-\nneering Research Council of Canada, Canada Research Chairs\nProgram, and the Ontario Ministry of Research and Innovation.\nThe authors also thank Nvidia for the GPU hardware used in\nthis study through the Nvidia Hardware Grant Program.\nREFERENCES\n[1]\nA. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep-\nspeech: Scaling up end-to-end speech recognition,” arXiv preprint\narXiv:1412.5567, 2014.\n[2]\nG. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pre-\ntrained deep neural networks for large-vocabulary speech recogni-\ntion,” Audio, Speech, and Language Processing, IEEE Transactions on,\nvol. 20, no. 1, pp. 30–42, 2012.\n[3]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural\ninformation processing systems, 2012, pp. 1097–1105.\n[4]\nK. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\nconvolutional networks for visual recognition,” in Computer Vision–\nECCV 2014.\nSpringer, 2014, pp. 346–361.\n[5]\nY. LeCun, F. J. Huang, and L. Bottou, “Learning methods for generic\nobject recognition with invariance to pose and lighting,” in Computer\nVision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004\nIEEE Computer Society Conference on, vol. 2.\nIEEE, 2004, pp. II–97.\n[6]\nK. Simonyan and A. Zisserman, “Very deep convolutional networks\nfor large-scale image recognition,” arXiv preprint arXiv:1409.1556,\n2014.\n8\nTraining Percentage\n0\n20\n40\n60\n80\n100\nError\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTraining Error\nTest Error\nTraining Percentage\n0\n20\n40\n60\n80\n100\nError\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTraining Error\nTest Error\n(a) Gaussian Connectivity Model\n(b) Uniform Connectivity Model\nFig. 9: Training and test error for classifying the STL-10 dataset versus the training set size used to train the network from\nCIFAR-10 dataset. Deep convolutional StochasticNet realizations were formed with 75% neural connectivity via the Gaussian\nconnectivity model as well as the uniform connectivity model. As seen the features extracted using the StochasticNets can\nprovide comparable classiﬁcation performance even when only 30% of the training data is used in the case of the Gaussian\nconnectivity model, and there was only a 3% increase in test error when only 10% of the training data is used. More interest\nis the case of the uniform connectivity model, where the features extracted using the StochasticNets can provide comparable\nclassiﬁcation performance with no increase in test error even when only 10% of the training data.\nNueral Connectivity Percentage\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nRelative Time\n0\n0.2\n0.4\n0.6\n0.8\n1\nFig. 10: Relative feature extraction time versus the number of\nneural connections. Note that neural connectivity percentage of\n100 is equivalent to ConvNet, since all connections are made.\n[7]\nR. Collobert and J. Weston, “A uniﬁed architecture for natural lan-\nguage processing: Deep neural networks with multitask learning,”\nin Proceedings of the 25th international conference on Machine learning.\nACM, 2008, pp. 160–167.\n[8]\nY. Bengio, R. Ducharme, P. Vincent, and C. Janvin, “A neural prob-\nabilistic language model,” The Journal of Machine Learning Research,\nvol. 3, pp. 1137–1155, 2003.\n[9]\nM. D. Zeiler and R. Fergus, “Stochastic pooling for regularization of\ndeep convolutional neural networks,” arXiv preprint arXiv:1301.3557,\n2013.\n[10] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus, “Regularization\nof neural networks using dropconnect,” in Proceedings of the 30th\nInternational Conference on Machine Learning (ICML-13), 2013, pp. 1058–\n1066.\n[11] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training\ndeep feedforward neural networks,” in International conference on\nartiﬁcial intelligence and statistics, 2010, pp. 249–256.\n[12] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural net-\nworks,” in International Conference on Artiﬁcial Intelligence and Statistics,\n2011, pp. 315–323.\n[13] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation,”\narXiv preprint arXiv:1502.01852, 2015.\n[14] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\narXiv preprint arXiv:1409.4842, 2014.\n[15] X. Zhang, J. Zou, K. He, and J. Sun, “Accelerating very deep con-\nvolutional networks for classiﬁcation and detection,” arXiv preprint\narXiv:1505.06798, 2015.\n[16] C. Farabet, B. Martini, P. Akselrod, S. Talay, Y. LeCun, and E. Cu-\nlurciello, “Hardware accelerated convolutional neural networks for\nsynthetic vision systems,” in Circuits and Systems (ISCAS), Proceedings\nof 2010 IEEE International Symposium on.\nIEEE, 2010.\n[17] J. Jin, V. Gokhale, A. Dundar, B. Krishnamurthy, B. Martini, and\nE. Culurciello, “An efﬁcient implementation of deep convolutional\nneural networks on a mobile coprocessor,” in Circuits and Systems\n(MWSCAS), 2014 IEEE 57th International Midwest Symposium on. IEEE,\n2014, pp. 133–136.\n[18] V. Gokhale, J. Jin, A. Dundar, B. Martini, and E. Culurciello, “A 240\ng-ops/s mobile coprocessor for deep neural networks,” in Computer\nVision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Confer-\nence on.\nIEEE, 2014.\n[19] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen,\n“Compressing neural networks with the hashing trick,” arXiv preprint\narXiv:1504.04788, 2015.\n[20] S. L. Hill, Y. Wang, I. Riachi, F. Sch¨urmann, and H. Markram, “Sta-\ntistical connectivity provides a sufﬁcient foundation for speciﬁc func-\ntional connectivity in neocortical neural microcircuits,” Proceedings of\nthe National Academy of Sciences, vol. 109, no. 42, pp. E2885–E2894,\n2012.\n[21] M. J. Shaﬁee, P. Siva, and A. Wong, “Stochasticnet: Forming\ndeep neural networks via stochastic connectivity,” arXiv preprint\narXiv:1508.05463, 2015.\n[22] E. N. Gilbert, “Random graphs,” The Annals of Mathematical Statistics,\npp. 1141–1144, 1959.\n[23] P. Erdos and A. Renyi, “On random graphs i,” Publ. Math. Debrecen,\nvol. 6, pp. 290–297, 1959.\n[24] A. Krizhevsky and G. Hinton, “Learning multiple layers of features\nfrom tiny images,” 2009.\n[25] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Read-\ning digits in natural images with unsupervised feature learning,” in\nNIPS workshop on deep learning and unsupervised feature learning, vol.\n2011, no. 2.\nGranada, Spain, 2011, p. 5.\n[26] A. Coates, A. Y. Ng, and H. Lee, “An analysis of single-layer net-\nworks in unsupervised feature learning,” in International conference on\nartiﬁcial intelligence and statistics, 2011, pp. 215–223.\n[27] B. Bollob´as and F. R. Chung, Probabilistic combinatorics and its applica-\ntions.\nAmerican Mathematical Soc., 1991, vol. 44.\n[28] I. Kovalenko, “The structure of a random directed graph,” Theory of\nProbability and Mathematical Statistics, vol. 6, pp. 83–92, 1975.\n[29] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based\nlearning applied to document recognition,” Proceedings of the IEEE,\nvol. 86, no. 11, pp. 2278–2324, 1998.\n[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: A simple way to prevent neural networks from\noverﬁtting,” The Journal of Machine Learning Research, vol. 15, no. 1,\npp. 1929–1958, 2014.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2015-12-11",
  "updated": "2015-12-11"
}