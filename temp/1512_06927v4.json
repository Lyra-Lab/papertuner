{
  "id": "http://arxiv.org/abs/1512.06927v4",
  "title": "A C++ library for Multimodal Deep Learning",
  "authors": [
    "Jian Jin"
  ],
  "abstract": "MDL, Multimodal Deep Learning Library, is a deep learning framework that\nsupports multiple models, and this document explains its philosophy and\nfunctionality. MDL runs on Linux, Mac, and Unix platforms. It depends on\nOpenCV.",
  "text": "A C++ library for Multimodal Deep Learning\nJian Jin\nDepartment of Computer Science\nJohns Hopkins University\n1\nMultimodal Deep Learning Library\nMDL, Multimodal Deep Learning Library, is a deep learning framework that supports multiple models, and\nthis document explains its philosophy and functionality. MDL runs on Linux, Mac, and Unix platforms. It\ndepends on OpenCV.\n1.1\nModules\nMDL supports the following deep learning models:\n• Restricted Boltzmann Machine\n• Deep Neural Network\n• Deep Belief Network\n• Denoising Autoencoder\n• Deep Boltzmann Machine\n• Bimodal Model\nIt also provides interfaces for reading MNIST, CIFAR, and AVLetters data sets. Multiple options in learning\nand visualization interfaces are also provided.\n1.2\nDocument Content\nChapter 2 introduces three graphical models. Chapters 3 to 7 give descriptions of each deep learning model.\nChapter 8 introduces multimodal deep learning. Chapter 9 describes library architecture. Chapter 10 shows\ntesting results.\n2\nNetwork Survey\nThis chapter includes a brief description of three graphical models that are mentioned in this document and a\nsummary of deep learning models of MDL.\n2.1\nNeural Network\nThe Neural Network is a directed graph that consists of multiple layers of neurons, which is also known as\nunits. Each pair of adjacent layers is connected, but there is no connection within each layer. The ﬁrst layer\nis the input, called the visible layer v. Above the visible layer are multiple hidden layers {h1, h2, ..., hn}.\nThe output of the last hidden layer forms the output layer o.\nIn hidden layers, neurons in layer hi receive input from the previous layer, hi−1 or v, and the output\narXiv:1512.06927v4  [cs.LG]  12 Apr 2016\nof hi is the input to the next layer, hi+1 or o. The value transmitted between layers is called activation or\nemission. The activation is computed as:\na(k)\ni\n= f((\nnk−1\nX\nj=1\na(k−1)\nj\nwk\nij) + b(k)\ni\n) = f(z(k)\ni\n)\n(1)\nwhere f is the activation function that deﬁnes a non-linear form of the output. A typical activation function\nis the sigmoid function. z(k)\ni\nis the total input of the unit i in layer k. It is computed as the weighted sum of\nthe activations of the previous layer. The weight wk\nij and the bias bk\ni are speciﬁed by the model.\nIn forward propagation, activations are computed layer by layer.\nTraining of the neural network uses\nbackpropagation, which is a supervised learning algorithm. It calculates the error based on the network\noutput and the training label, then uses this error to compute the gradients of error with respect to the weights\nand biases. The weights and biases are then updated by gradient descent.\n2.2\nMarkov Random Field\nThe Markov Random Field, also called the Markov Network, is an undirected graphical model in which each\nnode is independent of the other nodes given all the nodes connected to it. It deﬁnes the distribution of the\nvariables in the graph.\nThe Markov Random Field uses energy to decribe the distribution over the graph\nP(u) = 1\nZ e−E(u),\n(2)\nwhere Z is a partition function deﬁned by\nZ =\nX\nu\ne−E(u).\n(3)\nE is the energy speciﬁed by the model, and u is the set of states of all the nodes:\nu = {v1, v2, ..., vn}\n(4)\nwhere vi is the state of node i.\n2.3\nBelief Network\nThe Belief Network, which is also called the Bayesian Network, is a directed acyclic graph for probabilistic\nreasoning. It deﬁnes the conditional dependencies of the model by associating each node X with a condi-\ntional probability P(X|Pa(X)), where Pa(X) denotes the parents of X. Here are two of its conditional\nindependence properties:\n1. Each node is conditionally independent of its non-descendants given its parents.\n2. Each node is conditionally independent of all other nodes given its Markov blanket, which consists of its\nparents, children, and children’s parents.\nThe inference of Belief Network is to compute the posterior probability distribution\nP(H|V ) =\nP(H, V )\nP\nH P(H, V ),\n(5)\nwhere H is the set of the query variables, and V is the set of the evidence variables. Approximate inference\ninvolves sampling to compute posteriors. The Sigmoid Belief Network [17] is a type of the Belief Network\nsuch that\nP(Xi = 1|Pa(Xi)) = σ(\nX\nXj∈P a(Xi)\nWjiXj + bi)\n(6)\nwhere Wji is the weight assigned to the edge from Xj to Xi, and σ is the sigmoid function.\n2\n2.4\nDeep Learning Models\nThere are many models in deep learning [11]. Below are included in MDL.\nThe Restricted Boltzmann Machine is a type of Markov Random Field and is trained in an unsuper-\nvised fashion. It is the building block for other models and could be used for classiﬁcation by adding a\nclassiﬁer on top of it.\nThe Deep Neural Network is a neural network with multiple layers.\nEach layer is initialized by pre-\ntraing a Restricted Boltzmann Machine. Then ﬁne tuning would reﬁne the parameters of the model.\nThe Deep Belief Network is a hybrid of the Restricted Boltzmann Machine and the Sigmoid Belief\nNetwork. It is a generative model but not a feedforward neural network nor a multilayer perceptron even\nthough its training is similar to the Deep Neural Network.\nThe Denoising Autoencoder is a type of neural network that has a symmetric structure.\nIt could re-\nconstruct the input data and if properly trained could denoise the input data.\nHowever, unlike neural\nnetworks, it could be trained with an unsupervised method.\nThe Deep Boltzmann Machine is another type of Markov Random Field.\nIt is pretrained by stacking\nRestricted Boltzmann Machines with adjusted weights and biases as an approximation to undirected graphs.\nIts ﬁne tuning uses a method called mean ﬁeld inference.\n3\nRestricted Boltzmann Machine\n3.1\nLogic of the Restricted Boltzmann Machine\nFigure 3.1: Restricted Boltzmann Machine\nThe Restricted Boltzmann Machine (RBM) [8, 9, 16] is a Markov Random Field consisting of one hidden\nlayer and one visible layer. It is an undirected bipartite graph in which connections are between the hidden\nlayer and the visible layer. Each unit x is a stochastic binary unit such that\nstate(x) =\n\u001a1,\np\n0,\n1 −p\n(7)\nwhere probability p is deﬁned by the model. Figure 3.1 shows a RBM with four hidden units and six visible\nunits.\nAs a Markov Random Field, a Restricted Boltzmann Machine deﬁnes the distribution over the visible\nlayer v and the hidden layer h as\nP(v, h) = 1\nZ e−E(v,h),\n(8)\nwhere Z is the partition function deﬁned as\nZ =\nX\nv,h\ne−E(v,h).\n(9)\n3\nIts energy E(v, h) is deﬁned by\nE(v, h) = −\nX\ni\nbv\ni vi −\nX\nj\nbh\nj hj −\nX\ni\nX\nj\nviwi,jhj,\n(10)\nwhere bv\ni is the bias of the ith visible unit and bh\nj is the bias of the jth hidden unit.\nGiven the conditional independence property of the Markov Random Field, the distribution of one\nlayer given the other layer could be factorized as\nP(h|v) =\nY\nj\nP(hj|v)\n(11)\nP(v|h) =\nY\ni\nP(vi|h).\n(12)\nPlug equation (10) into equation (8):\nP(h|v) =\nP(h, v)\nP\nh′ P(h′, v) =\nexp(P\ni bv\ni vi + P\nj bh\nj hj + P\ni\nP\nj viwi,jhj)\nP\nh′ exp(P\ni bv\ni vi + P\nj bh′\nj h′\nj + P\ni\nP\nj viwi,jh′\nj).\n(13)\nIt is easy to show that\nexp(P\ni bv\ni vi + P\nj bh\nj hj + P\ni\nP\nj viwi,jhj)\nP\nh′ exp(P\ni bv\ni vi + P\nj bh′\nj h′\nj + P\ni\nP\nj viwi,jh′\nj) =\nY\nj\nexp(bh\nj + Pm\ni=1 Wi,jvi)\n1 + exp(bh\nj + Pm\ni=1 wi,jvi).\n(14)\nBy combining equation (11), (13), and (14), we have\nP(hj = 1|v) = σ\n \nbh\nj +\nNv\nX\ni=1\nWi,jvi\n!\n,\n(15)\nwhere σ is a sigmoid function\nσ(t) =\n1\n1 + exp(−t).\n(16)\nSimilarly,\nP(vi = 1|h) = σ\n\nbv\ni +\nNh\nX\nj=1\nWi,jhj\n\n.\n(17)\nA Restricted Boltzmann Machine maximizes the likelihood P(x) of the input data x, which is\nP(x) =\nX\nh\nP(x, h) =\nX\nh\n1\nZ e−E(h,x) = 1\nZ e−F (x),\n(18)\nwhere F(x) is called Free Energy such that\nF(x) = −\nnv\nX\ni=1\nbv\ni xi −\nnh\nX\nj=1\nlog(1 + exp(bh\nj +\nnv\nX\nk=1\nWkjxk)).\n(19)\nIn training, maximizing the likelihood of the training data is achieved by minimizing the negative log-\nlikelihood of the training data. Because the direct solution is intractable, gradient descent is used, in which\nthe weights {Wij}, the biases of the hidden units {bh\nj }, and the biases of the visible units {bv\ni } are updated.\nThe gradient is approximated by an algorithm called Contrastive Divergence.\nThe activation of the hidden layer is\nah = σ(v ∗W + bh),\n(20)\nwhere σ is performed in an element-wise fashion. The set of the hidden states, {hj}, is expressed as a row\nvector h, the set of the hidden biases, {bh\nj }, is expressed as a row vector bh, and the set of the weights,\n{Wij}, is expressed as a matrix W which represents the weight from the visible layer to the hidden layer.\nThe Restricted Boltzmann Machine has tied weights such that\nav = σ(h ∗W T + bv).\n(21)\nIn training, the state of the visible layer is initialized as the training data.\n4\n3.2\nTraining of the Restricted Boltzmann Machine\nIn the training of Restricted Boltzmann Machine, the weights and the biases of the hidden and the visible\nlayers are updated by gradient descent. Instead of stochastic gradient descent, in which each update is based\non each data sample, batch learning is used. In batch learning, each update is based on a batch of training\ndata. There are several epochs in training. Each epoch goes through the training data once.\nFor instance, if the input data has 10,000 samples and the number of batches is 200, then there will\nbe 200 updates in each epoch. For each update, gradients will be based on 50 samples. If the number of\nepochs is 10, commonly there should be a total of 2000 updates in the training process. If the gradients\ncomputed are trivial, this process may stop earlier.\nThe gradients of weights are given by Contrastive Divergence as:\n∇Wij = ⟨vi ∗hj⟩recon −⟨vi ∗hj⟩data,\n(22)\nwhere the angle brackets are expectations under the distribution speciﬁed by the subscript. The expectations\nhere are estimated by data sample mean, therefore\n∇Wij =\nm\nX\nk=1\n((vi ∗hj)reconk −(vi ∗hj)datak)/m,\n(23)\nwhere m is the size of each data batch.\nStates of the visible layer and the hidden layer form a sample in Gibbs sampling, in which the ﬁrst\nsample gives states with the subscript “data” in equation (22) and the second sample gives states with the\nsubscript “recon” in equation (22). Contrastive Divergence states that one step of Gibbs sampling, which\ncomputes the ﬁrst and the second sample, approximates the descent with high accuracy. In RBM, Gibbs\nsampling works in the following manner:\nIn Gibbs sampling, each sample X = (x1, . . . , xn) is made from a joint distribution p(x1, . . . , xn)\nby sampling each component variable from its posterior.\nSpeciﬁcally, in the (i + 1)th sample\nX(i+1) = (x(i+1)\n1\n, . . . , x(i+1)\nn\n), x(i+1)\nj\nis sampled from\np(Xj|x(i+1)\n1\n, . . . , x(i+1)\nj−1 , x(i)\nj+1, . . . , x(i)\nn ),\n(24)\nin which the latest sampled component variables are used to compute posterior. Sampling each component\nvariable xj once forms a sample.\nEach unit of RBM is a stochastic binary unit and its state is either 0 or 1. To sample hj from\nP(hj = 1|v) = σ\n \nbj +\nm\nX\ni=1\nWi,jvi\n!\n,\n(25)\nwe can simply compute aj = σ (bj + Pm\ni=1 Wi,jvi). If aj is larger than a random sample from standard\nuniform distribution, state of hj is 1, otherwise 0. This method works because the probability that a random\nsample u from standard uniform distribution is smaller than aj is aj:\nP(u < aj) = Funiform(aj) = aj.\n(26)\nSo we have\nP(hj = 1|v) = P(u < aj).\n(27)\nThus we could sample by testing if u < aj, since it has probability aj. Each unit has two possible states. If\nu < aj fails, the state is 0.\nIn training, we ﬁrst use the training data to compute the hidden layer posterior with\nP(hj = 1|v) = σ\n \nbj +\nm\nX\ni=1\nWi,jvi\n!\n.\n(28)\nThe hidden layer states and the training data form the ﬁrst sample. Then Gibbs sampling is used to compute\nthe second sample.\n5\nThe gradient of the visible bias is\n∇bv\ni = ⟨vi⟩recon −⟨vi⟩data,\n(29)\nand the gradient of the hidden bias is\n∇bh\nj = ⟨hj⟩recon −⟨hj⟩data\n(30)\n3.3\nTricks in the Training\nDropout\nDropout [2] is a method to prevent neural networks from overﬁtting by randomly blocking emissions from a\nportion of neurons. It is similar to adding noise. In RBM training, a mask is generated and put on the hidden\nlayer.\nFor instance, suppose the hidden states are\nh = {h1, h2, ..., hn}\n(31)\nand the dropout rate is r. Then a mask m is generated by\nmi =\n\u001a1,\nui > r\n0,\nui ≤r\n(32)\nwhere ui is a sample from standard uniform distribution, and i ∈{1, 2, ..., n}. The emission of hidden layer\nwould be\n˜h = h. ∗m\n(33)\nwhere .∗denotes element-wise multiplication of two vectors. ˜h, instead of h, is used to calculate visible states.\nLearning Rate Annealing\nThere are multiple methods to adapt the learning rate in gradient descent. If the learning rate is trivial,\nupdates may tend to get stuck in local minima and waste computation. If it is too large, the updates may\nbound around minima and could not go deeper. In annealing, learning rate decay adjusts the learning rate for\nbetter updates.\nSuppose α is the learning rate. In exponential decay, the annealed rate is\nαa = α ∗e−kt,\n(34)\nwhere t is the index of the current epoch, k is a customized coefﬁcient. In divide decay, the annealed rate is\nαa = α/(1 + kt).\n(35)\nA more common method is step decay:\nαa = α ∗0.5⌊t/5⌋.\n(36)\nwhere learning rate is reduced by half every ﬁve epochs. The coefﬁcients in the decay method should be\ntuned in testings.\nMomentum\nWith momentum ρ, the update step is\n∆t+1W = ρ ∗∆tW −r∇W.\n(37)\nThe update value is a portion of previous update value minus the gradient. Thus, if the gradient is in the\nsame direction of the previous update, the update value will become larger. If the gradient is in the different\ndirection, the variance of the updates will decrease. In this way, time to converge is reduced.\nMomentum is often applied with annealing so that steps of updates will not be too large.\nA feasible\nscheme is\nρ =\n\u001a0.5,\nt < 5\n0.9,\nt ≥5\n(38)\n6\nwhere t is the index of the current epoch.\nWeight Decay\nIn weight decay, a penalty term is added to the gradient as a regularizer. L1 penalty p1 is\np1 = k ×\nX\ni,j\n|Wij|.\n(39)\nL1 penalty causes many weights to become zero and a few weights to become large. L2 penalty p2 is\np2 = k ×\nX\ni,j\nW 2\nij.\n(40)\nL2 penalty causes weights to become more even and smaller. The coefﬁcient k is customized, sometimes 0.5\nwould work. Penalty must be, as well as the gradient, multiplied by the learning rate so that annealing will\nnot change the model trained.\n3.4\nClassiﬁer of the Restricted Boltzmann Machine\nA classiﬁer based on RBM could be built by training a classiﬁer layer with the softmax function on top of\nthe hidden layer. The softmax function takes a vector a = {a1, a2, ..., aq} as input, and outputs a vector\nc = {c1, c2, ..., cq} with the same dimension, where\nci =\neai\nPq\nk=1 eak .\n(41)\nOne-of-K scheme is used to present the class distribution. If there are K classes in the training data, then the\nlabel of a sample in class i is expressed as a vector of length K with only the ith element as 1, the others as\n0. For instance, if the training data has ﬁve classes, a sample in the fourth class has the label\n{0, 0, 0, 1, 0}.\nFor training data in K classes, there are K neurons in the classiﬁer layer. For each data sample, the softmax\nactivation emits a vector of length K. The index of the maximum element in this emission is the label. The\nelements of the softmax activation sum to 1 and the activation is the prediction distribution:\nci = P{The sample is in class i}.\n(42)\nThe input of the softmax function is of K-dimension whereas there may be hundreds of units in the hidden\nlayer. So the projection from the hidden layer to the classiﬁer should be learned. If the hidden layer has nh\nunits and the training data is in K classes, the weight of this projection should be a matrix of dimension\nnh × K.\nBackpropagation with the combination of cross entropy loss and softmax function is used to update\nthis weight. Using t to represent the labels and c to represent the prediction distribution, the cross entropy\nloss is\nL = −\nK\nX\ni=0\ntilog(ci).\n(43)\nBy chain rule:\n∂L\n∂Wij\n=\nK\nX\np=1\n∂L\n∂cp\n∂cp\n∂Wij\n(44)\nand\n∂cp\n∂Wij\n=\nK\nX\nq=1\n∂cp\n∂zq\n∂zq\n∂Wij\n(45)\nwhere c is the output of softmax activation and z is its input. That is to say,\nci =\nexp(zi)\nPK\np=1 exp(zp)\n.\n(46)\n7\nFurthermore\n∂L\n∂cp\n= −tp\ncp\n(47)\n∂cp\n∂zq\n=\n\u001acp(1 −cp)\nif p = q\n−cp × cq\nif p ̸= q\n(48)\n∂zq\n∂Wij\n=\n\u001aah\ni\nif q = j\n0\nif q ̸= j\n(49)\nwhere ah\ni is the activation of the hidden layer. Based on the above equations, for the combination of the cross\nentropy loss and the softmax activation\n∂L\n∂Wij\n= ah\ni (cj −tj).\n(50)\nExpressed in row vectors it is\n∇W = (ah)T × (c −t),\n(51)\nwhere (ah)T is the transpose of row vector ah. This gradient is used in batch learning.\n3.5\nImplementation of the Restricted Boltzmann Machine\nClass rbm is implemented in the header ﬁle rbm.hpp. Here is a selected list of its methods:\nI. void dropout(double i)\n-Set dropout rate as input i.\nII. void doubleTrain(dataInBatch &trainingSet, int numEpoch, int inLayer, ActivationType at = sigmoid t,\nLossType lt = MSE, GDType gd = SGD, int numGibbs = 1)\n-Train an RBM layer in Deep Boltzmann Machine, which is an undirected graph. This part will be explained\nin DBM section.\nIII. void singleTrainBinary(dataInBatch &trainingSet, int numEpoch, ActivationType at = sigmoid t,\nLossType lt = MSE, GDType gd = SGD, int numGibbs = 1);\n-Train an RBM layer with binary units in Deep Belief Networks and RBM.\nIV. void singleTrainLinear(dataInBatch &trainingSet, int numEpoch, ActivationType at= sigmoid t,\nLossType lt = MSE, GDType gd = SGD, int numGibbs = 1);\n-Train an RBM layer with linear units.\nV. void singleClassiﬁer(dataInBatch &modelOut, dataInBatch &labelSet, int numEpoch, GDType gd\n= SGD);\n-Build a classiﬁer layer for RBM.\nThe model could be tested by running runRBM.cpp. First train a RBM with one hidden layer:\nRBM rbm(784, 500, 0);\nrbm.dropout(0.2);\nrbm.singleTrainBinary(trainingData, 6);\ndataInBatch modelOut = rbm.g activation(trainingData);\nThe hidden layer has 500 units, and the index of the RBM is 0. The dropout rate is 0.2. After train-\ning, stack the classiﬁer layer with the softmax function on top of it:\nRBM classiﬁer(500, 10, 0);\nclassiﬁer.singleClassiﬁer(modelOut, trainingLabel, 6);\nclassiﬁcationError e = classifyRBM(rbm, classiﬁer, testingData, testingLabel, sigmoid t);\nMNIST dataset has ten classes, so there are ten units in the classiﬁer layer.\n8\n3.6\nSummary\nRBM is the foundation for several multi-layer models. It is crucial that this component is correctly imple-\nmented and fully understood. The classiﬁer may be trained with the hidden layer at the same time. Separating\nthese two processes facilitates checking problems in the implementation.\n4\nDeep Neural Network\n4.1\nPretraining of the Deep Neural Network\nFigure 4.1: Deep Neural Network\nA Deep Neural Network (DNN) [1] is a neural network with multiple hidden layers. In neural networks,\ninitialization of weights could signiﬁcantly affect the training results.\nPretraining, in which multiple\nRestricted Boltzmann Machines are trained to initialize parameters of each DNN layer, provides weight\ninitialization that saves training time. Backpropagation is a time-consuming process. With pretraining,\nthe time consumed by backpropagation could be signiﬁcantly reduced. Figure 4.1 shows a DNN with two\nhidden layers.\nBelow shows steps to build a Deep Neural Network for classiﬁcation:\nI. Set the architecture of the model, speciﬁcally the size of the visible layer and the hidden layers,\nn0, n1, n2, ..., nN. n0 is the input dimension and input forms a visible layer. nN equals to the number of\nclasses in training data.\nII. Pretrain hidden layers:\nfor i = 1 to N:\n1. Train an RBM with the following settings:\nn(i)\nh = ni\nn(i)\nv\n= ni−1\ndi = ai−1\n9\nwhere\nn(i)\nh = Dimension of hidden layer of RBM trained for layer i,\nn(i)\nv\n= Dimension of visible layer of RBM trained for layer i,\ndi = Input of RBM trained for layer i,\nai−1 = Activation of the (i −1)th DNN layer.\n2. Set\nWi = WRBM,\nbi = bh\nRBM,\nai = aRBM.\nRBM is used to initialize weights and biases of each DNN layer.\nend for.\nIII. Fine Tuning:\nUse backpropagation to reﬁne the weights and the biases of each layer. In backpropagation, one epoch goes\nthrough training data once. A dozen of epochs may sufﬁce.\nClassiﬁcation with Deep Neural Network is similar to RBM. The last layer, which uses softmax acti-\nvation, gives the prediction distribution.\n4.2\nFine Tuning of the Deep Neural Network\nFine tuning uses backpropagation:\nI. Perform a pass through all the layers.\nCompute total inputs of each layer {z(1), ..., z(N)} and acti-\nvations of each layer {a(1), ..., a(N)}. a(i) is the row vector that represents the activation of layer i.\nII. For the last layer, compute δ(N)\ni\nas\nδ(N)\ni\n=\n∂L\n∂z(N)\ni\n(52)\nwhere L is the classiﬁcation error. A row vector δ(N) is computed.\nIII. For l = N −1, ..., 1, compute\nδ(l) =\n\u0010\nδ (W (l))T \u0011\n• g′(z(l))\n(53)\nwhere g is the activation function.\nIV. Compute the gradients in each layer. For l = N, ..., 1, compute\n∇W (l)L = (a(l−1))T δ(l),\n(54)\n∇b(l)L = δ(l).\n(55)\nwhere a(0) is the training data.\nV. Update the weights and biases of each layer using gradient descent.\nIn ﬁne tuning, the training data is repeatedly used to reﬁne the model parameters.\n4.3\nImplementation of Deep Neural Network\nClass dnn is implemented in the header ﬁle dnn.hpp. Class RBMlayer provides interfaces to store architecture\ninformation. Here is a selected list of methods of class dnn:\nI. void addLayer(RBMlayer &l)\n10\nAdd a layer to the current model. This object of class RBMlayer should store information of layer size,\nweight, bias, etc.\nII. void setLayer(std::vector<size t> rbmSize)\nObject of class dnn could automatically initialize random weights and biases of each layer by inputting a\nvector of layer sizes.\nIII. void train(dataInBatch &trainingData, size t rbmEpoch, LossType l = MSE, ActivationType a =\nsigmoid t)\nThis method trains all the layers without classiﬁer. The structure of dnn should be initialized before calling\nthis method.\nIV. void classiﬁer(dataInBatch &trainingData, dataInBatch &trainingLabel, size t rbmEpoch, int pre-\nTrainOpt, LossType l = MSE, ActivationType a = sigmoid t)\nBuild a Deep Neural Network with a classiﬁer layer. This function contains pretraining option preTrainOpt.\nIf preTrainOpt=1, pretrain each layer by training RBMs, else randomly initialize layer parameters without\npretraining.\nV. void ﬁneTuning(dataInBatch &label, dataInBatch &inputSet, LossType l)\nFine tuning step that uses backpropagation.\nVI. classiﬁcationError classify(dataInBatch &testingSet, dataInBatch &testinglabel);\nPerform Classiﬁcation. The result is stored in the format classﬁcationError.\n4.4\nSummary\nThe training of the Deep Neural Network consists of two steps: pretraining by stacking multiple RBMs, and\nﬁne tuning with backpropagation. Pretraining signiﬁcantly reduces time spent on backpropagation.\n5\nDeep Belief Network\n5.1\nLogic of the Deep Belief Network\nFigure 5.1: Deep Belief Network\nA Deep Belief Network (DBN) is a hybrid of a Restricted Boltzmann Machine and a Sigmoid Belief\nNetwork. A Deep Belief Network maximizes the likelihood P(x) of the input x. Figure 5.1 shows a DBN.\n11\nFor a Deep Belief Network with N hidden layers, the distribution over the visible layer (input data)\nand hidden layers is\nP(v, h1, ..., hN) = P(v|h1) ×\n N−2\nY\nk=1\nP(hk|hk+1))\n!\n× P(hN−1, hN).\n(56)\nTo prove this, by chain rule we can write\nP(v, h1, ..., hN) = P(v|h1, ..., hN) ×\n N−2\nY\nk=1\nP(hk|hk+1, ..., hN))\n!\n× P(hN−1, hN).\n(57)\nAdditionally, each node is independent of its ancestors given its parent. So these hold:\nP(v|h1, ..., hN) = P(v|h1),\n(58)\nP(hk|hk+1, ..., hN) = P(hk|hk+1).\n(59)\nThen we have\nP(v, h1, ..., hN) = P(v|h1) ×\n N−2\nY\nk=1\nP(hk|hk+1))\n!\n× P(hN−1, hN),\n(60)\nwhere P(v|h1) ×\n\u0010QN−2\nk=1 P(hk|hk+1))\n\u0011\nis the distribution over the Sigmoid Belief Network and\nP(hN−1, hN) is the distribution over Restricted Boltzmann Machine.\nFor classiﬁcation there should be a layer y on top of the last hidden layer.\nWith layer y that repre-\nsents prediction distribution, the distribution over the Deep Belief Network is\nP(v, h1, ..., hN, y) = P(v|h1) ×\n N−2\nY\nk=1\nP(hk|hk+1))\n!\n× P(hN−1, hN, y).\n(61)\nwhere P(hN−1, hN, y) is the distribution over a RBM which has labels y and the state hN−1 as the input. In\nthe pretraining of the Deep Belief Network, RBMs are stacked. In ﬁne tuning, Up-Down algorithm is used.\n5.2\nTraining of the Deep Belief Network\nIn training, the Deep Belief Network should maximize the likelihood of the training data. With the concavity\nof the logarithm function, the lower bound of the log likelihood of the training data x could be found:\nlog P(x) = log\n X\nh\nQ(h|x)P(x, h)\nQ(h|x)\n!\n≥\nX\nh\nQ(h|x) log P(x, h)\nQ(h|x) ,\n(62)\nand\nX\nh\nQ(h|x) log P(x, h)\nQ(h|x) =\nX\nh\nQ(h|x) log P(x, h) −\nX\nh\nQ(h|x) log Q(h|x),\n(63)\nwhere Q(h|x) is an approximation to the true probability P(h|x) of the model.\nIf Q(h|x) = P(h|x), the right-hand side of (63) could be written as\nX\nh\nP(h|x)(log P(h|x) + log P(x)) −\nX\nh\nP(h|x) log P(h|x)\n=\nX\nh\nP(h|x) log P(x) = log P(x)\nX\nh\nP(h|x) = log P(x).\n(64)\nCombining equation (64), (63) with (62), we could ﬁnd that when Q(h|x) = P(h|x), the lower bound is tight.\nMoreover, the more different Q(h|x) is from P(h|x), the less tight the bound is. The lower bound of\n(62) could be expressed as\nlog P(x) −KL(Q(h|x)||P(h|x)).\n(65)\n12\nIf the approximation Q(h|x) becomes closer to the true posterior P(h|x), their KL divergence will be smaller,\nand the bound will be higher. Unlike the true posterior, the approximation could be factorized as\nQ(h|x) =\nnh\nY\ni=1\nQ(hi|x).\n(66)\nConsequently, in training the goal is to ﬁnd approximation Q(h|x) with high accuracy and at the same time\nmaximizes the bound. This could be done by stacking pretrained RBMs. The lower bound of (62) could be\nfactorized as\nX\nh\nQ(h|x) log P(x, h)\nQ(h|x) =\nX\nh\nQ(h|x)(log P(x|h) + log P(h)) −\nX\nh\nQ(h|x) log Q(h|x)\n(67)\nIn the right-hand side of equation (67), Q(h|x) and P(x|h) are given by the ﬁrst pretrained RBM. Therefore,\nto maximize the lower bound is to maximize\nX\nh\nQ(h|x) log P(h).\n(68)\nSince the RBM maximizes the likelihood of the input data, staking another RBM on top of the ﬁrst hidden\nlayer maximizes the lower bound. Moreover,\nP(h) =\nX\nh(2)\nP(h, h(2)),\n(69)\nwhere h(2) is computed by the second pretrained RBM. The second RBM takes sample made from Q(h|x)\nas input. But it could be trained independently since its parameters do not depend on the parameters of the\nﬁrst pretrained RBM. This is why greedy layer-wise pretraining works.\nIn ﬁne tuning, Up-Down algorithm [4] is used.\nIt is a combination of Contrastive Divergence and\nWake-Sleep algorithm [3]. Wake-Sleep algorithm is used in the learning of the Sigmoid Belief Network.\n5.3\nClassiﬁcation of the Deep Belief Network\nIn training of the Restricted Boltzmann Machine, dropout is used to alleviate overﬁtting.\nThis method\nreminds us that RBM could predict missing values.\nIn a deep belief network, each approximation Q(hk+1|hk) could be computed based on states hk and\nmodel parameters. In a deep belief network with classiﬁer, the top RBM takes labels and hidden layer hN−1\nto compute states of the last hidden layer hN, which is illustrated in Figure 5.2.\nSuppose l is the set of units that represents the prediction distribution. In classiﬁcation, we can ﬁll l with\nzeros, compute approximation Q(hN|l, hN−1), and then use the states hN sampled from Q(hN|l, hN−1) to\ncompute the prediction distribution l with P(l, hN−1|hN).\n5.4\nImplementation of the Deep Belief Network\nClass dbn is in the header ﬁle dbn.hpp. Here is a selected list of its methods:\nI. void addLayer(RBMlayer &l)\nAdd a layer to the current model. This object of class RBMlayer should store information of layer size,\nweight, bias, etc. It could also be modiﬁed after added to the model.\nII. void setLayer(std::vector<size t> rbmSize)\nObject of class dnn could automatically initialize random weights and biases of each layer by inputting a\nvector of layer sizes.\nIII. void train(dataInBatch &trainingData, size t rbmEpoch, LossType l = MSE, ActivationType a =\nsigmoid t)\nThis method trains a dbn without classiﬁer. The architecture of dbn should be initialized before calling this\n13\nFigure 5.2: Top RBM of DBN with classiﬁer\nmethod.\nIV. void classiﬁer(dataInBatch &trainingData, dataInBatch &trainingLabel, size t rbmEpoch, LossType l =\nMSE, ActivationType a = sigmoid t)\nThis method trains a dbn with classiﬁer. The architecture of dbn should be initialized before calling this\nmethod.\nV. void ﬁneTuning(dataInBatch &dataSet, dataInBatch &labelSet, int epoch)\nThe ﬁne tuning uses Up-Down algorithm.\nVI. classiﬁcationError classify(dataInBatch &testingSet, dataInBatch &testinglabel);\nPerform Classiﬁcation.\n5.5\nSummary\nIt is easy to confuse the Deep Belief Network with the Deep Neural Network. Both of them stack RBMs in the\npretraining. However, because their principles and structures are distinct, their ﬁne tunining and classiﬁcation\nmethods are different.\n14\n6\nDenoising Autoencoder\n6.1\nTraining of the Autoencoder\nFigure 6.1: Autoencoder\nAutoencoder(AE) [5] is a type of neural network forming a directed graph. Its symmetricity requires that in\nan autoencoder with (N + 1) layers (including visible layer and output layer), the dimension of each layer is\nconstrained by\nni = nN−i\nfor 0 ≤i ≤N\n(70)\nwhere ni is the dimension of the ith layer, and n0 is the input dimension. Since the output layer and the\nvisible layer are of the same dimension, it is expected that autoencoders could reconstruct the input data.\nThus, the training of autoencoders is unsupervised learning because input data is used as labels in ﬁne tuning,\nand reconstruction errors could be used to access the model. Autoencoders could also be used to construct\nclassiﬁers by adding a classiﬁer layer on top of it. Figure 6.1 shows an Autoencoder.\nBelow is how to construct an autoencoder:\nI. Set the architecture of the model, speciﬁcally the size of each layer, n0, n1, n2, ..., nN.\nII. Pretraining:\nfor i = 1 to N/2:\n1. Train an RBM with the following settings:\nn(i)\nh = ni\nn(i)\nv\n= ni−1\ndi = ai−1\nwhere\nn(i)\nh = Dimension of hidden layer of RBM trained for layer i,\nn(i)\nv\n= Dimension of visible layer of RBM trained for layer i,\ndi = Input of RBM trained for layer i,\nai−1 = Activations of the (i −1)th layer of autoencoder,\n15\n2. Initialize parameters of the current layer\nWi = WRBM,\nbi = bh\nRBM,\nai = aRBM.\nThe parameters of trained RBM are used to initialize the parameters of the layer.\nend for.\nfor i = N/2 + 1 to N:\nInitialize parameters\nWi = W T\nN−i,\nbi = bN−i.\nend for.\nIII. Fine Tuning:\nBackpropagation with Mean Squared Error. The error is computed based on the reconstruction and the\ntraining data.\n6.2\nFine tuning of the Autoencoder\nFine tuning of Autoencoder uses backpropagation, which is:\nI. Perform a forward propagation through all layers that computes layer inputs {z(1), ..., z(N)} and\nactivations {a(1), ..., a(N)}. a(i) is a row vector representing the activation of layer i.\nII. For the last layer, compute δ(N)\ni\nas\nδ(N)\ni\n=\n∂L\n∂z(N)\ni\n(71)\nwhere L is the reconstruction error. This step computes a row vector δ(N).\nIII. For l = N −1, ..., 1, compute\nδ(l) =\n\u0010\nδ(l+1) (W (l))T \u0011\n• g′(z(l))\n(72)\nwhere g is the activation function and here g′ is performed in an element-wise fashion.\nIV. Compute the gradients in each layer. For l = N, ..., 1, compute\n∇W (l)L = (a(l−1))T δ(l),\n(73)\n∇b(l)L = δ(l).\n(74)\nwhere a(0) is the input data of the autoencoder.\nV. Update the weights and biases of each layer with gradient descent.\nThe reconstruction error of Autoencodr is\nL = 1\n2\nnN\nX\ni=1\n(a(N)\ni\n−a(0)\ni )2\n(75)\nwhere\nnN = Dimension of the output/reconstruction,\na(N)\ni\n= Activation of the ith unit in the output layer,\na(0)\ni\n= Activation of the ith unit in the visible layer.\n16\nBackpropagation involves computing {δ(i)}, which is based on the derivatives of activation function and\nerror function. Here is how to compute these two values:\nFor sigmoid activation:\ng′(t) = ∂(1 + e−t)−1\n∂t\n=\n1\n1 + e−t\ne−t\n1 + e−t .\n(76)\nThat is to say\ng′(zi) =\n1\n1 + e−zi (1 −\n1\n1 + e−zi ) = ai(1 −ai).\n(77)\nFor δ(N)\ni\n, by chain rule,\nδ(N)\ni\n=\n∂L\n∂z(N)\ni\n=\nnN\nX\np=1\n∂L\n∂a(N)\np\n∂a(N)\np\n∂z(N)\ni\n=\n∂L\n∂a(N)\ni\n∂a(N)\ni\n∂z(N)\ni\n= (a(N)\ni\n−a(0)\ni ) × a(N)\ni\n(1 −a(N)\ni\n).\n(78)\n6.3\nDenoising Autoencoder\nThe Denoising Autoencoder reconstructs the corrupted data, and could predict missing values. By adding\nnoises to corrupt the training data, we could transform the Autoencoders to the Denoising Autoencoders.\nHere is how to make this transformation:\nFirstly a denoise rate r is chosen, and the mask is made as follows:\nmi =\n\u001a1,\nif Ui > r\n0,\notherwise\n1 ≤i ≤nN\n(79)\nwhere Ui is the ith sample from standard uniform distribution, nN is the size of the last layer, which is also\nthe dimension of input data.\nSecondly compute the corrupted input data\na(c) = a(0) · m =\nnN\nX\ni=1\na(0)\ni mi\n(80)\nFinally use a(c) as the training data to compute the reconstruction and still use the uncorrupted data\na(0) as labels in ﬁne tuning. In reconstruction, ﬁne tuning makes more improvements in the Denoising\nAutoencoder than in the Autoencoder, and is crucial to the Denoising Autoencoder.\n6.4\nImplementation of the Denoising Autoencoder\nThe implementation of DAE is in the header ﬁle autoencoder.hpp.\nI. void addLayer(RBMlayer &l)\nAdd a layer to current DAE. This object of class RBMlayer should store information of layer size, weight,\nbias, etc. It could also be modiﬁed after added to DAE.\nII. void setLayer(std::vector<size t> rbmSize)\nObject of class AutoEncoder could automatically initialize random weights and biases of each layer by\ninputting a vector of layer sizes.\nIII. void train(dataInBatch &trainingData, size t rbmEpoch, LossType l = MSE, ActivationType a =\nsigmoid t)\nThis method trains all the layers without classiﬁer. The structure of this AutoEncoder object should be\ninitialized before calling this method.\nIV. void reconstruct(dataInBatch &testingData)\nThis method gives the reconstruction of testingData. It should be called only after the model has been trained.\n17\nV. dataInBatch g reconstruction()\nGet the reconstruction.\nVI. void ﬁneTuning(dataInBatch &originSet, dataInBatch &inputSet, LossType l)\nUse backpropagation. Unlike DBN, argument LossType should be MSE instead of CrossEntropy.\nVI. void denoise(double dr)\nSet the denoise rate as dr. When model is in training, it will detect if denoise rate is set. So if this method is\ncalled before training, the Denoising Autoencoder will be trained automatically. Otherwise the Autoencoder\nwill be trained.\n6.5\nSummary\nIn pretraining, parameters of half of the layers are acquired by stacking RBMs. The parameters of the other\nhalf are given by the symmetric architecture. Fine tuning is crucial to the Denoising Autoencoder because it\nuses uncorrupted data to modify the model trained with corrupted data. The performance of the Denoising\nAutoencoder is straightforward to assess because one could observe the reconstructed images.\n7\nDeep Boltzmann Machine\n7.1\nLogic of the Deep Boltzmann Machine\nFigure 7.1: Deep Boltzmann Machine\nA Deep Boltzmann Machine(DBM) [14] is a Markov Random Field with multiple layers. Connections\nexist only between adjacent layers. Intuitively, it could incorporate top-down feedback when computing\nbottom-up approximations. Figure 7.1 shows a DBM.\nThe energy function of a Deep Boltzmann Machine with N hidden layers is\nE(v, h(1), ..h(N)) = −vT W (1)h(1) −(h(1))T W (2)h(2) −... −(h(N−1))T W (N)h(N)\n(81)\nwhere W (i) is the weight from the previous layer to the ith hidden layer.\nA Deep Boltzmann Machine maximizes the likelihood of the input data.\nThe gradient of its log like-\nlihood is\n∂log P(v)\n∂W (i)\n= ⟨h(i−1)(h(i))T ⟩data −⟨h(i−1)(h(i))T ⟩model.\n(82)\n18\n7.2\nPretraining of the Deep Boltzmann Machine\nBecause the Deep Boltzmann Machine is an undirected model, the last hidden layer receives input from the\nprevious adjacent layer, and the other hidden layers receive inputs from both directions. So when training\nRestricted Boltzmann Machines, the weights and biases need to be adjusted for better approximations. The\npretraining process is as follows:\nI. Set the architecture of the model, speciﬁcally the size of each layer, n0, n1, n2, ..., nN.\nn0 is the\ndimension of the training data.\nII. Pretrain the ﬁrst hidden layer:\nTrain an RBM, in which the weight from the visible layer v to the hidden layer h1 is 2W1 and the weight\nfrom h1 to v is W T\n1 . W1 is the weight of the ﬁrst DBM hidden layer.\nIII. Pretrain intermediate hidden layers:\nfor i = 2 to N −1:\n1. Train an RBM with the following settings:\nn(i)\nh = ni\nn(i)\nv\n= ni−1\ndi = ai−1\nwhere\nn(i)\nh = Dimension of hidden layer of RBM trained for layer i,\nn(i)\nv\n= Dimension of visible layer of RBM trained for layer i,\ndi = Input of RBM trained for layer i,\nai−1 = Activation of the (i −1)th layer.\n2. Set\nWi = WRBM/2,\nbi = bh\nRBM/2,\nai = aRBM.\nWeights and biases are adjusted here for better approximations.\nIV. Pretrain the last hidden layer:\nTrain an RBM, in which the weight from the hidden layer hN−1 to the hidden layer hN is WN and the\nweight from hN to hN−1 is 2W T\nN. WN is the weight of the last hidden layer of DBM.\n7.3\nMean Field Inference\nThe mean ﬁeld inference [15] of the Deep Boltzmann Machine involves iterative updates of the approxima-\ntions Q. It is performed after pretraining. The algorithm is as follows:\nAlgorithm Mean Field Inference\nInitialize M samples {˜v0,1, ˜h0,1},...,{˜v0,M, ˜h0,M}with the pretrained model. Each sample consists of states\nof the visible layer and all the hidden layers.\nfor t = 0 to T (number of iterations) do\n1. Variational Inference:\nfor each data sample vn, n = 1 to D do\n19\nPerform a bottom-up pass with\nν1\nj = σ\n\u0010 n0\nX\ni=1\n2W 1\nijvi\n\u0011\n,\nν2\nk = σ\n\u0010 n1\nX\nj=1\n2W 2\njkν1\nj\n\u0011\n,\n· · ·\nνN−1\np\n= σ\n\u0010 nN−2\nX\nl=1\n2W N−1\nlp\nνN−2\nl\n\u0011\n,\nνN\nq = σ\n\u0010 nN−1\nX\np=1\nW N\npqνN−1\np\n\u0011\n,\nwhere {Wij} is the set of pretrained weights.\nSet µ = ν and run the mean-ﬁeld updates with:\nµ1\nj ←σ\n\u0010 n0\nX\ni=1\nW 1\nijvi +\nn2\nX\nk=1\nW 2\njkµ2\nk\n\u0011\n,\n· · ·\nµN−1\nj\n←σ\n\u0010 nN−2\nX\ni=1\nW N−1\nij\nµN−2\ni\n+\nnN\nX\nk=1\nW N\njkµN\nk\n\u0011\n,\nµN\nj ←σ\n\u0010 nN−1\nX\ni=1\nW N\nij µN−1\ni\n\u0011\n.\nSet µn = µ.\nend for\n2. Stochastic Approximation:\nfor each sample m = 1 to M do\nRunning one step Gibbs sampling. Get (˜vt+1,m, ˜ht+1,m) from (˜vt,m, ˜ht,m)\nend for\n3. Parameter Update:\nW 1\nt+1 = W 1\nt + αt\n\u0010\n1\nD\nPD\nn=1 vn(µ1\nn)T −\n1\nM\nPM\nm=1 ˜vt+1,m(˜h1\nt+1,m)T \u0011\nW 2\nt+1 = W 2\nt + αt\n\u0010\n1\nD\nPD\nn=1 µ1\nn(µ2\nn)T −\n1\nM\nPM\nm=1 ˜h1\nt+1,m(˜h2\nt+1,m)T \u0011\n· · ·\nW nN\nt+1 = W nN\nt\n+ αt\n\u0010\n1\nD\nPD\nn=1 µnN−1\nn\n(µnN\nn )T −\n1\nM\nPM\nm=1 ˜hnN−1\nt+1,m(˜hnN\nt+1,m)T \u0011\nDecrease αt.\nend for\nTo facilitate computation, M may be chosen as the number of batches.\n7.4\nImplementation of the Deep Boltzmann Machine\nThe implementation of DBM is in the header ﬁle dbm.hpp.\nI. void addLayer(RBMlayer &l)\nAdd a layer to current DBM. This object of class RBMlayer should store information of layer size, weight,\nbias, etc. It could also be modiﬁed after added to DBM.\n20\n(a) Bimodal Deep Belief Network\n(b) Modal Reconstruction Network\nFigure 8.1: Modal Prediction\nII. void setLayer(std::vector<size t> rbmSize)\nObject of class DBM could automatically initialize random weights and biases of each layer by inputting a\nvector of layer sizes.\nIII. void train(dataInBatch &Data, dataInBatch &label, size t rbmEpoch, LossType l = MSE, Activa-\ntionType a = sigmoid t)\nThis method trains DBM as classiﬁer.\nIV. void ﬁneTuning(dataInBatch &data, dataInBatch &label)\nFine tuning with mean ﬁeld inference.\nV. void classify(dataInBatch &data, dataInBatch &label)\nClassify the data with DBM. Tesing label is used to compute classiﬁcation error rate.\n7.5\nSummary\nIn DBM, each layer receives input from all its adjacent layers. Its training is more complicated than other\nmodels.\n8\nMultimodal Learning Model\nFigure 8.2: MRF Multimodal Learning Model\nIt is possible to make a low-dimension representation of two modals by building a Bimodal Deep Belief\nNetwork [13] as in Figure 8.2(a), in which blue nodes represent data from one modal and green nodes\n21\n(a) Bimodal Autoencoder\n(b) Modal Prediction Network\nFigure 8.3: Modal Prediction\nrepresent data from the other modal. In this process, the recognition weights that are used in bottom-up\ncomputation and the generative weights that are used in top-down pass should be learned. If the model is\ntrained with tied weights, half of the memory space could be saved since transposing the weight matrix\nwould transform recognition weights to generative weights. The weights of this model could be used to\nreconstruct data of two modals as in Figure 8.2(b).\nAnother option is to build a Markov Random Field multimodal learning model [18] by combining\ntwo Deep Boltzmann Machines. Figure 8.3 shows such a model. This model is built by ﬁrst building two\nDBMs, of which each is trained on data of one modal, then training an RBM on top of these two DBMs.\nPrediction of data from one modal given data from the other modal could be done by ﬁrst training a\nBimodal Autoencoder in Figure 8.4(a) and then using the modal prediction network in Figure(b) to predict\ndata from two modals. Bimodal Autoencoder should be trained using training data with noises so that it\ncould better predict missing values.\n9\nLibrary Structure\n9.1\nData Reading\n9.1.1\nMNIST\nMNIST [10] is a selected set of samples from NIST data set. It has one training data set, one training label\nset, one testing data set, and one testing label set. The training set has 60,000 samples and the testing set has\n10,000 samples. Each sample data is a 28×28 grey image which is a handwritten integer between 0 and 9. It\nhas 10 classes, so the label is between 0 (inclusive) and 9 (inclusive).\nThe data is stored in big-endian format.\nThe content of data should be read as unsigned characters.\nHeader ﬁle readMNIST.hpp provides functions to read this data set.\nI. cv::Mat imread mnist image(const char* path)\nRead data of MNIST. Each row is a sample.\nII. cv::Mat imread mnist label(const char* path)\nRead labels of MNIST. Each row is a number indicating the class of that sample.\n9.1.2\nCIFAR\nThe CIFAR-10 data set [7] consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\nThere are 5 batches of training images and 1 batch of test images, and each consists of 10000 images.\nIn CIFAR-10 data set, each sample consists of a number indicating its class and the values of the im-\n22\nage pixels. The following function in the header ﬁle readCIFAR.hpp reads them to four OpenCV matrices:\nvoid imread cifar(Mat &trainingData, Mat &trainingLabel, Mat &testingData, Mat &testingLabel)\nEach row of the read OpenCV matrices consists of the label and the data of a sample.\n9.1.3\nAvLetters\nAvLetters [12] is the data set recording audio data and video data of different people uttering letters. The\ndimension of the audio data is 26 and the dimension of the video data is 60×80. The data is stored in\nsingle-precision big-endian format. Each ﬁle is the data of a person uttering a certain letter. For instance, the\nﬁle A1 Anya.mfcc contains the audio data of the person named Anya uttering letter ”A”.\nThe following function in the header ﬁle readAvLetters.hpp reads audio data:\ncv::Mat imread avletters mfcc(const char* path)\nThe output is an OpenCV matrix, of which each row contains data of a sample.\nThe original video\ndata is in MATLAB ﬁle format. The header ﬁle readMat.hpp contains the function\ncv::Mat matRead(const char* ﬁleName, const char* variableName, const char* saveName).\nIt reads the mat ﬁle and at the same time saves it as binary ﬁle named as the argument ”saveName”.\nThis header ﬁle uses the MATLAB/c++ interface provided by MATLAB and requires an environment\nsetting, which is contained as comments in the header ﬁle. There are some problems running the libraries in\nthis interface together with OpenCV. It would be better to transform all MATLAB ﬁles to binary ﬁles before\ntraining models and then read the transformed binary ﬁles. The header ﬁle readDat.hpp provides the function\nto read the transformed binary ﬁles:\ncv::Mat readBinary(const char* ﬁle, int rowSize, int colSize)\nThe output is an OpenCV matrix, of which each row contains data of a sample.\n9.1.4\nData Processing\nHeader ﬁle processData.hpp stores functions processing data.\ndata oneOfK(indexLabel l, int labelDim)\nTransfer index label to one-of-k expression.\ndataInBatch corruptImage(dataInBatch input, double denoiseRate)\nGive corrupted data in batches.\nstd::vector<dataInBatch> dataProcess(dataCollection& reading, int numBatch)\nBuild data batches.\ndataCollection shufﬂeByRow(dataCollection& m)\nShufﬂe the data\ncv::Mat denoiseMask(size t rowSize, size t colSize, double rate)\nGenerate a mask to corrupt data\n9.2\nComputation and Utilities\nactivation.hpp includes multiple activation functions, such as sigmoid, tanh, relu, leaky relu, softmax.\nEach activation function is paired with a function that computes its derivatives to facilitate computation in\nbackpropagation.\ngd.hpp includes functions for adaptive gradient descent and stochastic gradient descent, as well as a\n23\nfunction to anneal the learning rate in which three types of annealing methods are provided.\ninference.hpp includes the mean ﬁeld inference implementation used by DBM.\nloss.hpp includes computation of loss functions.\nMSE, absolute loss, cross entropy, and binary loss\nare provided together with the functions to compute their derivatives.\nmatrix.hpp includes some OpenCV matrix manipulation functions.\nloadData.hpp contains functions to test data loading by visualization.\nvisualization.hpp contains functions of visualization.\n9.3\nModules\nTable 1 shows the ﬁles that contain modules.\nModel\nRBM\nDNN\nDBN\nDAE/AE\nDBM\nHeader(.hpp)\nrbm\ndnn\ndbn\nautoencoder\ndbm\nMain(.cpp)\nrunRBM\nrunDNN\nrunDBN\nrunDAE\nrunDBM\nTable 1: Files that contain modules\n10\nPerformance\n10.1\nRestricted Boltzmann Machine\nRun runRBM.cpp to test the Restricted Boltzmann Machine module. It performs classiﬁcation on MNIST\ndata set, which has 60,000 training samples and 10,000 testing samples. The hidden layer has 500 units.\nThe classiﬁcation error rate is 0.0707 (Classiﬁcation accuracy 92.93%). Multiple deep learning libraries give\nsimilar results.\n10.2\nDeep Neural Network\nRun runDNN.cpp to test the Deep Neural Network module. It performs classiﬁcation on MNIST using a\nDNN with layers of size (28 × 28)-500-300-200-10. Without ﬁne tuning, the classiﬁcation error rate is\n0.093 (Classiﬁcation accuracy 90.7%). With ﬁne tuning, the classiﬁcation error rate is 0.0288 (Classiﬁcation\naccuracy 97.12%). Hinton, Salakhutdinov (2006) claim that DNN with layers of size (28 × 28)-500-500-\n2000-10 could achieve the error rate of 1.2 %. For comparison, a DNN with the same architecture was built\nand the error rate could achieve 0.0182. The source codes of the paper use Conjugate Gradient, which is not\nimplemented in MDL. Also, learning is much more sensitive to the learning rate in the (28 × 28)-500-500-\n2000-10 architecture than in the (28×28)-500-300-200-10 architecture. So ﬁnding a better learning rate may\nalso improve the performance.\n10.3\nDenoising Autoencoder\nRun runDAE.cpp to test the Denoisinig Autoencoder module. It performs reconstruction of MNIST using a\nDenoising Autoencoder and an Autoencoder, each with layers of size (28×28)-500-300-500-(28×28). Fig-\nure 10.1(a) contains the testing data with noises and the reconstruction given by the Denoising Autoencoder.\nFigure 10.1(b) contains the testing data without noises and the reconstruction given by the Autoencoder.\nFine tuninig reduces more reconstruction error in the Denoising Autoencoder than in the Autoen-\ncoder. The reconstruction MSE of the Denoising Autoencoder are: Average error without ﬁne tuning is\n6686.69; Average error after ﬁne tuning is 3256.34.\nThe reconstruction MSE of the Autoencoder are:\nAverage error without ﬁne tuning is 4463.24; Average error after ﬁne tuning is 3182.69. With sufﬁcient ﬁne\ntuning, the reconstruction given by the Denoising Autoencoder is similar to the reconstruction given by the\n24\n(a) Denoising Autoencoder\n(b) Autoencoder\nFigure 10.1: Reconstruction of DAE and AE on MNIST\nAutoencoder. The visualization is comparable to the published results.\nFigure 10.2: Denoising Avletter audio data\nRun runDAE letter.cpp to test the Denoising Autoencoder module with AvLetters data set. Training data\nconsists of the audio data of a person pronouncing the letter “A” to the letter “G”. Testing data consists of the\naudio data of the person pronouncing “H” and “I”. Figure 10.2 shows the result of denosing the testing data.\n10.4\nDeep Belief Network\nRun runDBN.cpp to test the Deep Belief Network module. It performs classiﬁcation on MNIST using DBN\nwith layers of size (28 × 28)-500-300-10. The classiﬁcation error rate on MNIST without ﬁne tuning is\n0.0883 (Classiﬁcation accuracy 91.17%). With the ﬁne tuning using Up-down algorithm, the classiﬁcation\nerror rate could be reduced to 0.0695 (Classiﬁcation accuracy 93.05%). Hinton, Osindero & Teh, (2006)\nclaim that the classiﬁcation error rate could be as low as 1.25%. Factors such as learning rate decay and\ngradient descent method could affect the result.\n25\n10.5\nDeep Boltzmann Machine\nRun runDBM.cpp to test the Deep Boltzmann Machine module. It performs classiﬁcation on MNIST us-\ning DBM with layers of size (28 × 28)-500-500-10. The classiﬁcation error rate on MNIST without ﬁne\ntuning is 0.0937 (Classiﬁcation accuracy 90.63%). Mean Field inference improves the accuracy to 93.47%.\nSalakhutdinov, Hinton (2009) claim that the classiﬁcation error rate could be as low as 0.95%. Its source\ncodes use Conjugate Gradient optimization, which is not implemented in this library. This possibly causes\nthe difference.\n10.6\nModal Prediction\nRun modal prediction.cpp to test the bimodal autoencoder module on AvLetters data set. The training data\nis the audio and the video data of the letters “A” to “G”. The audio data of the letters “H” and “I” is used to\npredict the video data of the letters “H” and “I”. The reconstruction error rate is 10.46%.\nReferences\n[1] Suzanna Becker and Geoffrey E Hinton.\nSelf-organizing neural network that discovers surfaces in\nrandom-dot stereograms. Nature, 355(6356):161–163, 1992.\n[2] Geoffrey Hinton. A practical guide to training restricted boltzmann machines. Momentum, 9(1):926,\n2010.\n[3] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The ”wake-sleep” algorithm for\nunsupervised neural networks. Science, 268(5214):1158–1161, 1995.\n[4] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets.\nNeural computation, 18(7):1527–1554, 2006.\n[5] Geoffrey E Hinton and Ruslan R Salakhutdinov.\nReducing the dimensionality of data with neural\nnetworks. Science, 313(5786):504–507, 2006.\n[6] Harold Hotelling. Relations between two sets of variates. Biometrika, pages 321–377, 1936.\n[7] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009.\n[8] Hugo Larochelle and Yoshua Bengio.\nClassiﬁcation using discriminative restricted boltzmann ma-\nchines.\nIn Proceedings of the 25th international conference on Machine learning, pages 536–543.\nACM, 2008.\n[9] Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann machines and\ndeep belief networks. Neural computation, 20(6):1631–1649, 2008.\n[10] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[11] University of Montreal LISA lab.\nDeep learning tutorial.\nhttp://deeplearning.net/\ntutorial/deeplearning.pdf. Accessed: 2015-11-18.\n[12] Iain Matthews, Timothy F Cootes, J Andrew Bangham, Stephen Cox, and Richard Harvey. Extraction\nof visual features for lipreading. Pattern Analysis and Machine Intelligence, IEEE Transactions on,\n24(2):198–213, 2002.\n[13] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal\ndeep learning. In Proceedings of the 28th international conference on machine learning (ICML-11),\npages 689–696, 2011.\n[14] Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In International Conference\non Artiﬁcial Intelligence and Statistics, pages 448–455, 2009.\n[15] Ruslan Salakhutdinov and Hugo Larochelle. Efﬁcient learning of deep boltzmann machines. In Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics, pages 693–700, 2010.\n[16] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for col-\nlaborative ﬁltering. In Proceedings of the 24th international conference on Machine learning, pages\n791–798. ACM, 2007.\n[17] Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean ﬁeld theory for sigmoid belief net-\nworks. Journal of artiﬁcial intelligence research, 4(1):61–76, 1996.\n26\n[18] Nitish Srivastava and Ruslan R Salakhutdinov. Multimodal learning with deep boltzmann machines. In\nAdvances in neural information processing systems, pages 2222–2230, 2012.\n27\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2015-12-22",
  "updated": "2016-04-12"
}