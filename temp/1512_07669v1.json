{
  "id": "http://arxiv.org/abs/1512.07669v1",
  "title": "Reinforcement Learning: Stochastic Approximation Algorithms for Markov Decision Processes",
  "authors": [
    "Vikram Krishnamurthy"
  ],
  "abstract": "This article presents a short and concise description of stochastic\napproximation algorithms in reinforcement learning of Markov decision\nprocesses. The algorithms can also be used as a suboptimal method for partially\nobserved Markov decision processes.",
  "text": "arXiv:1512.07669v1  [math.OC]  23 Dec 2015\nReinforcement Learning\nStochastic Approximation Algorithms for\nMarkov Decision Processes\nVikram Krishnamurthy\nUniversity of British Columbia,\nVancouver, Canada. V6T 1Z4.\nvikramk@ece.ubc.ca\nDecember 2015\n1\nIntroduction\nStochastic adaptive control algorithms are of two types: direct methods, where\nthe unknown transition probabilities Pij(u) are estimated simultaneously while\nupdating the control policy, and implicit methods (such as simulation based meth-\nods), where the transition probabilities are not directly estimated in order to\ncompute the control policy. Reinforcement learning algorithms are implicit meth-\nods for adaptive control. The aim of this article is to present elementary results\nin stochastic approximation algorithms for reinforcement learning of Markov\ndecision processes. This article is a short summary of a much more detailed\npresentation in the forthcoming book [34].\nRecall that:\n• A Markov Decision Process (MDP) is obtained by controlling the transition\nprobabilities of a Markov chain as it evolves over time.\n• A Hidden Markov Model (HMM) is a noisily observed Markov chain.\n• A partially observed Markov decision process (POMDP) is obtained by con-\ntrolling the transition probabilities and/or observation probabilities of an\nHMM.\nThese relationships are illustrated in Figure 1.1.\nA POMDP specializes to a MDP if the observations are noiseless and equal to\nthe state of the Markov chain. A POMDP specializes to an HMM if the control\nis removed. Finally, an HMM specializes to a Markov chain if the observations\nare noiseless and equal to the state of the Markov chain.\nr\nMarkov\nChain\nHidden Markov\nModel (HMM)\nMarkov Decision\nProcess (MDP)\nPartially Observed\nMarkov Decision Process\n(POMDP)\nNoisy\nmeasurements\nControlled\ntransitions\nControlled transitions\nand observations\nFigure 1.1 Terminology of HMMs, MDPs and POMDPs\nThis article deals with stochastic gradient algorithms for estimating reasonable\n(locally optimal) strategies for MDPs and POMDPs.\nSuppose a decision maker can observe the noisy response yk of a controlled\n2\nIntroduction\nstochastic system to any action uk that it chooses. Let Ik = {u0, y1, . . . , uk−1, yk}\ndenote the history of actions and observed responses up to time k. The decision\nmaker chooses its action as uk = µθ(Ik) where µθ denote a parametrized policy\n(parametrized by a vector θ). Then to optimize its choice of actions, the decision\nmaker needs to compute the optimal parameter θ∗which minimizes the cost\ncriterion E{C(θ, Ik)}. The decision maker uses the following stochastic gradient\nalgorithm to estimate θ∗:\nθk+1 = θk −ǫ ∇θC(θk, Ik),\nk = 0, 1, . . . .\n(1.1)\nHere ∇θC(θk, Ik) denotes the gradient (or estimate of gradient) of the instan-\ntaneous cost with respect to the parameter θ and ǫ denotes a small positive\nstep size. Such algorithms lie within the class of reinforcement learning meth-\nods since the past experience Ik is used to adapt the parameter θk which in\nturn determines the actions; intuitively a good choice of θ would result in good\nperformance which in turn reinforces this choice. This article deals with such\nstochastic gradient algorithms including how to compute the gradient estimate\nand analyze the resulting algorithm.\nChapter 2 discusses gradient estimation for Markov processes via stochastic\nsimulation. This forms the basis of gradient based reinforcement learning.\nChapter 3 presents simulation based stochastic approximation algorithms for\nestimating the optimal policy of MDPs when the transition probabilities are not\nknown. These algorithms are also described in the context of POMDPs. The Q-\nlearning algorithm and gradient based reinforcement learning algorithms are\npresented.\nChapter 4 gives a brief description of convergence analysis of stochastic ap-\nproximation algorithms. Examples given include recursive maximum likeli-\nhood estimation of HMM parameters, the least mean squares algorithm for\nestimating the state of an HMM (which can be used for adaptive control of a\nPOMDP), discrete stochastic optimization algorithms, and mean ﬁeld dynam-\nics for approximating the dynamics of information ﬂow in large scale social\nnetworks.\nFor further details, illustrative examples, proofs of theorem and more in-\nsight, please see [34]. More speciﬁcally, [34] deals extensively with POMDPs\nand structural results. The algorithms in this article can be used in conjunc-\ntion with the structural results resulting in efﬁcient numerical implementations.\nStructural results for POMDPs are developed in [36, 42, 37, 31, 32, 33].\n2\nStochastic Optimization and\nGradient Estimation\nContents\n2.1\nStochastic Gradient Algorithm\npage 4\n2.2\nHow long to simulate a Markov chain?\n7\n2.3\nGradient Estimators for Markov Processes\n8\n2.4\nFinite difference gradient estimators and SPSA\n8\n2.5\nScore Function Gradient Estimator\n10\n2.6\nWeak Derivative Gradient Estimator\n12\n2.7\nBias and Variance of Gradient Estimators\n14\n2.8\nComplements and Sources\n15\nConsider the discounted cost MDP problem:\nCompute µ∗= argmin\nµ\nEµ\n\b ∞\nX\nk=0\nρkc(xk, µ(xk))\n\t\n.\nwhere xk is the controlled state of the system, c(xk, uk) is the cost incurred at\ntime k by choosing action uk = µ(xk) and ρ < 1 is a discount factor. Stochastic\ndynamic programming is used to compute the globally optimal policy µ∗for\nsuch problems.\nIn comparison, this article deals with computing (estimating) local minima\nusing stochastic gradient algorithms. Suppose the action at time k is chosen\naccording to the following parameterized policy: uk = µθ(x) for some pre-\nspeciﬁed function µθ parametrized by the vector θ ∈IRp. Then the aim is:\nCompute θ∗= argmin\nθ\nC(θ),\nC(θ) = Eµθ\n\b ∞\nX\nk=0\nρkc(xk, µθ(xk)\n\t\n.\n(2.1)\nThis will be achieved using a stochastic gradient algorithm of the form\nθk+1 = θk −ǫk b∇θCk(θk).\n(2.2)\nHere ǫk is a scalar step size and b∇θCk(θk) denotes an estimate of gradient\n∇θC(θ) evaluated at θk. These gradient estimates need to be computed using\nthe observed realization of {xk, {c(xk, uk)} since this is the only information\navailable to the decision maker. Since in general, C(θ) is non-convex, at best\none can expect (2.2) to converge (in a sense to be made precise below) to a local\nstationary point of (2.1).\n4\nStochastic Optimization and Gradient Estimation\nEven though stochastic gradient algorithms typically converge to a local sta-\ntionary point, there are several advantages compared to stochastic dynamic\nprogramming. First, in many cases, (2.2) can operate without knowing the tran-\nsition matrices of the MDP, whereas dynamic programming requires complete\nknowledge of these parameters. (This is the basis of reinforcement learning in\nChapter 3.) Second, (2.2) is often substantially cheaper to compute, especially\nfor very large state and action spaces, where dynamic programming can be pro-\nhibitively expensive.\nThis chapter focuses on gradient estimation for Markov processes. Algorithms\ndiscussed include the SPSA algorithm, score function gradient estimator and\nweak derivative gradient estimator. Such gradient estimators form the basis for\nimplementing stochastic gradient algorithms for MDPs and POMDPs. Simula-\ntion based gradient estimation is a mature area; our aim in this chapter is to\npresent a few key algorithms that are useful for MDPs and POMDPs.\n2.1\nStochastic Gradient Algorithm\nThe aim is to solve a continuous-valued stochastic optimization problem. Sup-\npose a parameter vector θ ∈IRp speciﬁes a transition matrix (or more generally,\nkernel) Pθ and stationary distribution πθ from which a Markov process {xk} is\nsimulated. The aim is to compute θ∗∈Θ that minimizes1 the expected cost\nC(θ) = Eπθ{c(x, θ)} =\nZ\nX\nc(x, θ)πθ(x)dx.\n(2.3)\nIn stochastic optimization, the stationary distribution πθ and/or the cost c(x, θ)\nare not known explicitly2; instead the sample path sequence {c(xk, θ), k = 1, 2, . . .}\nof (noisy) costs can be observed for any choice of model θ. Therefore, it is not\npossible to compute the integral in (2.3) explicitly. This is in contrast to a deter-\nministic optimization problem where c(x, θ) and πθ are known so that the right\nhand side of (2.3) can be evaluated explicitly.\nIn stochastic optimization, a widely used algorithm to compute (estimate)\nthe minimizer θ∗of (2.3) is the following stochastic gradient algorithm run over\ntime k = 0, 1, . . .\nθk+1 = θk −ǫk b∇θCk(θk).\n(2.4)\nHere b∇θCk(θk) denotes an estimate of gradient ∇θC(θ) evaluated at θk. Note\nthat b∇θCk(θk) needs to be computed using the observed realization {c(xk, θ)}\nsince this is the only information available.\nIn (2.4), ǫk is a positive scalar at each time k and denotes the step size of the\nalgorithm. There are two general philosophies for choosing the step size.\n1Assume Θ ⊂IRp is compact and C(θ) is a continuous function so that the minimum exists.\n2We assume that Pθ is known but computing πθ in closed form is not tractable. More generally,\nPθ itself may not be known, but for any choice of θ, samples of c(xk, θ) can be simulated.\n2.1 Stochastic Gradient Algorithm\n5\n1. Decreasing step size ǫk = 1/kα, where α ∈(0.5, 1]. Under reasonable condi-\ntions, it can be proved that the stochastic gradient algorithm (2.4) converges\nwith probability 1 to a local stationary point of C(θ).\n2. Constant step size ǫk = ǫ where ǫ is a small positive constant. Under reason-\nable conditions, it can be proved that the stochastic gradient algorithm (2.4)\nconverges weakly to a local stationary point. Such constant step size algo-\nrithms are useful for estimating a time-varying minimum.\nRemark. Robbins Munro Algorithm: The stochastic gradient algorithm (2.4) can be\nviewed as a special case of the following Robbins Munro algorithm. Suppose\none wishes to solve numerically the following equation for θ:\nEπθ{c(x, θ)} = 0, or equivalently\nZ\nX\nc(x, θ) πθ(x)dx = 0.\n(2.5)\nAssume that for any choice of θ, one can obtain random (noisy) samples c(xk, θ).\nThe Robbins Munro algorithm for solving (2.5) is the following stochastic ap-\nproximation algorithm:\nθk+1 = θk + ǫk c(xk, θk).\nNotice that the stochastic gradient algorithm (2.4) can be viewed as an instance\nof the Robbins Munro algorithm for solving Eπθ{∇θCk(θ)} = 0. Chapter 3\nshows that the Q-learning algorithm (reinforcement learning algorithm) for solv-\ning MDPs is also an instance of the Robbins Munro algorithm.\nExamples\nExamples of stochastic gradient algorithms include the following:\n1. Least Mean Square (LMS) algorithm: The LMS algorithm belongs to the class\nof adaptive ﬁltering algorithms and is used widely in adaptive signal process-\ning. The objective to minimize is\nC(θ) = Eπ{(yk −ψ′\nkθ)2}\nwhere yk and ψk are observed at each time k. Then using (2.4) we obtain the\nLMS algorithm\nθk+1 = θk + ǫψk(yk −ψ′\nkθk)\n(2.6)\nNote that this case is a somewhat simple example of a stochastic optimization\nproblem since we know c(x, θ) = (yk −ψ′\nkθ)2 explicitly as a function of x =\n(y, ψ) and θ. Also the measure π does not depend explicitly on θ. In Chapter\n4.6.1, we will discuss the consensus LMS algorithm.\n2. Adaptive Modulation: Consider a wireless communication system where the\npacket error rate of the channel xk evolves according to a Markov process. The\nmodulation scheme is adapted over time depending on the observed (empiri-\ncal) packet error rate yk measured at the receiver, where yk ∈[0, 1]. Suppose at\neach time slot k, one of two modulation schemes can be chosen, namely,\nuk ∈{r1 (low data rate), r2 (high data rate)}.\n6\nStochastic Optimization and Gradient Estimation\nThe modulation scheme uk affects the transition matrix (kernel) P(uk) of the\nerror probability process xk. Deﬁne the instantaneous throughput at time k\nas r(xk, uk) = (1 −xk)uk. The aim is to maximize the average throughput\nlimN→∞\n1\nN+1Eµ{PN\nk=0 r(xk, uk)} where the policy µ maps the available infor-\nmation Ik = {u0, y1, u1, . . . , uk−1, yk} to action uk.\nThe setup is depicted in Figure 2.1. The problem is an average cost POMDP\nand in general intractable to solve.\nr\nAdaptive\nModulator\nNoisy\nChannel\nc(xk, uk)\n(Throughput)\nFeedback channel\nuk\nxk ∼P(uk)\nFigure 2.1 Schematic Setup of Adaptive Modulation Scheme\nConsider the following “simple” parametrized policy for the adaptive mod-\nulation scheme:\nuk+1 = µθ(yk) =\n(\nr1\nif yk ≥θ\nr2\nif yk < θ\n(2.7)\nThis family of policies µθ parametrized by θ is intuitive since the individual\npolicies use a less aggressive modulation scheme (low data rate) when the chan-\nnel quality is poor (large error rate yk), and vice versa. The aim is to determine\nthe value of θ in the adaptive modulation policy µθ which maximizes the aver-\nage throughput.\nLet πθ denote the joint stationary probability distribution of the Markov pro-\ncess (xk, yk−1) induced by the policy µθ. The aim is to determine θ which maxi-\nmizes the average throughput:\nCompute θ∗= argmax\nθ∈Θ\nR(θ),\nwhere R(θ) = Eπθ{r(xk, µ(yk−1))}\n(2.8)\ngiven noisy measurements rk = (1 −yk)uk of the instantaneous throughput.\nAn explicit formula for πθ is not known since it is a complicated function of\nthe modulation scheme, error correction codes, channel coding, medium access\ncontrol protocol, etc. So (2.8) is not a deterministic optimization problem.\nAssuming E{yk} = xk (the empirical observations of the channel are unbi-\nased), then θ∗can be estimated using the stochastic gradient algorithm\nθk+1 = θk + ǫk ( ˆ∇θrk).\nHere ˆ∇θrk is an estimate of the gradient ∇θR(θ) = R r(x, µ(y))∇θπθ(x, y)dxdy.\nIn [52] this approach is described in more detail. In §2.5 and §2.6, two classes of\nsimulation based algorithms are given to estimate the gradient with respect to\nthe stationary distribution of a Markov process. Alternatively, any of the ﬁnite\ndifference methods in §2.4 can be used.\n2.2 How long to simulate a Markov chain?\n7\n2.2\nHow long to simulate a Markov chain?\nSuppose a Markov chain is simulated for n-time points. Let X = {e1, e2, . . . , eX}\ndenote the state space, where ei are the X-dimensional unit vectors. Given h ∈\nIRX, let\nφn = 1\nn\nn−1\nX\nk=0\nh′xk\ndenote the time averaged estimate. By the strong law of large numbers, if P is\nregular, then as n →∞, φn →h′π∞with probability 1 for any initial distribu-\ntion π0. However, in practice, for ﬁnite sample size simulations, one needs to\ndetermine the accuracy of estimate and guidelines for choosing the sample size\nn. For sample size n, deﬁne the bias, variance and mean square deviation of the\nestimate φn as\nBias(φn) = E{φn} −h′π∞\nVar(φn) = E{φn −E{φn}}2\nMSD(φn) = E{φn −h′π∞}2 = Var(φn) + (Bias(φn))2.\nThe main result is as follows; the proof is in [34].\nTHEOREM 2.2.1\nConsider an n-point sample path of a Markov chain {xk} with\nregular transition matrix P. Then\n| Bias(φn)| ≤maxi,j |hi −hj|\nn(1 −ρ)\n∥π0 −π∞∥TV\n(2.9)\nMSD(φn) ≤2 maxi,j |hi −hj|2\nn(1 −ρ)\nX\ni∈X\n(∥ei −π∞∥TV)2 π∞(i) + O( 1\nn2 )\n(2.10)\nP (|φn −E{φn}| > ǫ) ≤2 exp\n\u0012\n−\nǫ2(1 −ρ)2n\nmaxl,m |hl −hm|2\n\u0013\n.\n(2.11)\nHere, for any two pmfs π, ¯π, ∥π −¯π∥TV denotes the variational distance\n∥π −¯π∥TV = 1\n2∥π −¯π∥1 = 1\n2\nX\ni\n|π(i) −¯π(i)|\nand ρ denotes the Dobrushin coefﬁcient of transition matrix P\nρ = 1\n2 max\ni,j\nX\nl∈X\n|Pil −Pjl|.\n2.3\nGradient Estimators for Markov Processes\nConsider the stochastic optimization problem (2.3). To solve (2.3) using the\nstochastic gradient algorithm (2.4) requires estimating the gradient ∇θC(θ). We\nwish to compute the gradient estimate ˆ\n∇θCN(θ) using the observed realization\n8\nStochastic Optimization and Gradient Estimation\nof costs {c(xk, θ)}, k = 0, . . . , N. Here xk is an ergodic Markov process with\ntransition matrix or kernel Pθ. The rest of this chapter discusses two types of\nsimulation based gradient estimators for Markov processes:\n• Finite difference gradient estimators such as the Kiefer Wolfowitz and SPSA\nalgorithms discussed in §2.4. They do not require knowledge of the transition\nkernel of the underlying Markov process. However, ﬁnite difference gradient\nestimators suffer from a bias variance tradeoff.\n• Gradient estimators that exploit knowledge of the Markov transition kernel.\nWe discuss the score function gradient estimator in §2.5 and the weak deriva-\ntive estimator below in §2.6. Unlike the ﬁnite difference estimators, these gra-\ndient estimators are unbiased for random variables.\n§3 shows how these gradient estimators can be used for reinforcement learning\nof MDPs, constrained MDPs, and POMDPs.\n2.4\nFinite diﬀerence gradient estimators and SPSA\nThis section describes two ﬁnite difference gradient estimators that can be used\nwith the stochastic gradient algorithm to minimize (2.3). The algorithms do not\nrequire the transition matrix of the Markov chain to be known.\n2.4.1\nKiefer Wolfowitz algorithm\nThis uses the two sided numerical approximation of a derivative and is de-\nscribed in Algorithm 1. Here ∆n is the gradient step while ǫn is the step size of\nthe stochastic approximation algorithm. These need to be chosen so that\n∆n > 0, ǫn > 0, ∆n →0, ǫn →0,\nX\nn\nǫn = ∞,\nX\nn\nǫ2\nn/∆2\nn < ∞.\n(2.12)\nA disadvantage of the Kiefer Wolfowitz algorithm is that 2p independent sim-\nulations are required to evaluate the gradient along all the possible directions\ni = 1, . . . , p.\n2.4.2\nSimultaneous Perturbation Stochastic Approximation (SPSA)\nThe SPSA algorithm [63] has been pioneered by J. Spall (please see the website\nwww.jhuapl.edu/SPSA/).It overcomes the problem of requiring 2p independent\nsimulations in the Kiefer Wolfowitz algorithm by choosing a single random di-\nrection in IRp along which to evaluate the ﬁnite difference gradient. So only 2\nsimulations are required. The SPSA algorithm is described in Algorithm 2. The\nstep sizes ∆n and ǫn are chosen as in (2.12).\nIn Algorithm 2 the random directions have been chosen from a Bernoulli dis-\ntribution. The random directions dn need to be chosen from a distribution such\nthat the inverse moments E{|1/dn(i)|2+2α} are ﬁnite for some α > 0. Suitable\n2.4 Finite diﬀerence gradient estimators and SPSA\n9\nAlgorithm 1 Kiefer Wolfowitz Algorithm (θn ∈IRp)\nFor iterations n = 0, 1, 2, . . .\n• Evaluate the 2p sampled costs ˆCn(θ + ∆nei) and ˆCn(θ −∆nei), i = 1, 2, . . . , p\nwhere ei is a unit vector with 1 in element i.\n• Compute gradient estimate\nˆ∇θ(i)Cn(θ) =\n1\n2∆n\nh\nˆCn(θ + ∆nei) −ˆCn(θ −∆nei)\ni\n,\ni = 1, . . . , p\n(2.13)\nHere ∆n =\n∆\n(n+1)γ denotes the gradient step size with 0.5 ≤γ ≤1 and ∆> 0.\n• Update model estimate θn via stochastic gradient algorithm\nθn+1 = θn −ǫn ˆ∇C(θn),\nǫn =\nǫ\n(n + 1 + s)ζ ,\n0.5 < ζ ≤1, and ǫ, s > 0.\nchoices (apart from Bernoulli) include segmented uniform and U-shaped den-\nsities; see [63, pp.185] for a detailed description.\nSpall shows that SPSA is asymptotically as efﬁcient as Kiefer Wolfowitz.\nAlgorithm 2 SPSA Algorithm (θn ∈IRp)\nFor iterations n = 0, 1, 2, . . .\n• Simulate the p dimensional vector dn with random elements\ndn(i) =\n(\n−1\nwith probability 0.5\n+1\nwith probability 0.5.\n• Evaluate sample costs ˆCn(θn + ∆ndn) and ˆCn(θn −∆ndn)\n• Compute gradient estimate\nˆ∇C(θn) =\nˆCn(θn + ∆ndn) −ˆCn(θn −∆ndn)\n2∆n\ndn.\nHere ∆n =\n∆\n(n+1)γ denotes the gradient step size with 0.5 ≤γ ≤1 and ∆> 0.\n• Update model estimate θn via stochastic gradient algorithm\nθn+1 = θn −ǫn ˆ∇C(θn),\nǫn =\nǫ\n(n + 1 + s)ζ ,\n0.5 < ζ ≤1, and ǫ, s > 0.\nFinite difference methods such as SPSA suffer from the bias-variance tradeoff.\nThe bias in the gradient estimate is proportional to ∆2. On the other hand, if\nˆCn(θ −∆ei) and ˆCn(θ + ∆ei) are sampled independently, then the variance is\nproportional to 1/∆2. To decrease the bias, one needs a small ∆, but this results\nin an increase in the variance.\n10\nStochastic Optimization and Gradient Estimation\n2.5\nScore Function Gradient Estimator\nThis section describes the score function gradient estimator. Unlike the SPSA\nalgorithm, no ﬁnte difference approximation is used. We assume that the tran-\nsition kernel Pθ of the Markov process xk is known but computing the station-\nary distribution πθ is intractable. The aim is to compute the gradient estimate\nˆ∇θCn(θn) in the stochastic gradient algorithm (2.4) given the realization of costs\n{c(xk, θ)}, k ∈ιn for any choice of θ.\n2.5.1\nScore Function Gradient Estimator for RVs\nTo highlight the main ideas, the score function gradient estimator for random\nvariables is discussed ﬁrst.\nAssuming sufﬁcient regularity to swap the order of integration and gradient,\nwe have\n∇θC(θ) =\nZ\nc(X)∇θπθ(X)dX =\nZ\nc(X)∇θπθ(X)\nπθ(X) πθ(X)dX\nThe score function algorithm proceeds as follows: simulate Xk ∼πθ and com-\npute for any N\nˆ\n∇θCN = 1\nN\nN\nX\nk=1\nc(Xk)∇θπθ(Xk)\nπθ(Xk)\nThe term “score function” stems from the fact that ∇θπθ(X)\nπθ(X)\n= ∇θ log πθ. For any\nN, this is an unbiased estimator of ∇θC(θ). The derivative of the log of density\nfunction is often called the “score function” in statistics.\nExample: If πθ(x) = θe−θx, then ∇θπθ(X)\nπθ(X)\n= ∇θ log πθ = 1\nθ −X.\n2.5.2\nScore Function Gradient Estimator for Markov Process\nWe now describe the score function simulation based gradient estimator for a\nMarkov process. The eventual goal is to solve ﬁnite state MDPs and POMDPs\nvia stochastic gradient algorithms. So to avoid technicalities, we consider a ﬁ-\nnite state Markov chain x and assume that transition matrix Pθ is regular so\nthat there exists a unique stationary distribution πθ. Therefore, for a cost c(x),\nthe expected cost is\nEπθ{c(x)} =\nX\nX\ni=1\nc(i) πθ(i) = c′πθ.\nSuppose that for any choice of θ, one can observe via simulation a sample path\nof the costs c(xk), k = 1, 2, . . . where the Markov chain {xk} evolves with tran-\nsition matrix Pθ. Given such simulated sample paths, the aim is to estimate the\ngradient\n∇θEπθ{c(x)} = ∇θ (c′πθ) .\n2.6 Weak Derivative Gradient Estimator\n11\nThis is what we mean by simulation based gradient estimation. We assume that\nthe transition matrix Pθ is known but the stationary distribution πθ is not known\nexplicitly. Also the cost. c(x) may not be known explicitly and the simulated\nsampled path c(xk) may be observed in zero mean noise.\nClearly as N →∞, for any initial distribution π0, limN→∞c′Pθ\n′Nπ0 →c′πθ.\nSo the ﬁnite sample approximation is (write Pθ as P to simplify notation)\n∇θ (c′πθ) ≈π′\n0\nN−1\nX\nk=0\nP N−k−1 ∇θP P k c\n(2.14)\n=\nX\nx0:N\nc(xN)\n\u0014\n(∇θPx0x1)Px1x2 · · · PxN−1xN + Px0x1(∇θPx1x2) · · · PxN−1xN\n+ · · · + Px0x1Px1x2 · · · (∇θPxN−1xN)\n\u0015\nπx0\n=\nX\nx0:N\nc(xN)\n\u0014∇θPx0x1\nPx0x1\n+ · · · + ∇θPxN−1xN\nPxN−1xN\n\u0015\nPx0x1Px1x2 · · · PxN−1xNπx0\n= Ex0:N\n(\nc(xN)\nN\nX\nk=1\n∇θPθxk−1xk\nPθxk−1xk\n)\nThis leads to the score function gradient estimator in Algorithm 3.\nAlgorithm 3 Score Function Gradient Estimation Algorithm\nStep 1. Simulate Markov chain x0, . . . , xN with transition matrix Pθ.\nStep 2. Compute Sθ\nN = PN\nk=1\n∇θPθxk−1xk\nPθxk−1xk\nThis is evaluated recursively as\nSθ\nk = ∇θPθxk−1xk\nPθxk−1xk\n+ Sθ\nk−1,\nk = 1, . . . , N.\nStep 3. Evaluate the score function gradient estimate via simulation as\nc\n∇θCN(θ) = 1\nN\nN\nX\nk=1\nc(xk)Sθ\nk.\n(Note that this is the average over N samples).\n2.6\nWeak Derivative Gradient Estimator\nThis section describes the weak derivative gradient estimator. Like the score\nfunction estimator, for random variables it provides an unbiased estimator of\nthe gradient.\nA probability distribution Fθ is weakly differentiable at a point θ (on an open\n12\nStochastic Optimization and Gradient Estimation\nsubset of IR) if there exists a signed measure3 denoted as νθ = ∇θF such that\nlim\ns→0\n1\ns\n\u0014Z\nc(x)dFθ+s(x) −\nZ\nc(x)dFθ(x)\n\u0015\n=\nZ\nc(x)dνθ(x)\n(2.15)\nfor any bounded continuous function c(x). The term weak derivative is used\nsince ∇θF(x) may not be a function in the classical sense; it could be a gener-\nalized function (e.g. Dirac delta function). Hence the above deﬁnition involves\nintegration with a test function c(x).\nA well known result in measure theory is that any ﬁnite signed measure ν\ncan be decomposed as\nν = g1µ1 −g2µ2\n(2.16)\nwhere g1 and g2 are constants and µ1 and µ2 are probability measures. In our\ncase, since the signed measure is obtained as the derivative of a probability\nmeasure, i.e., since\nR\ndFθ(x) = 1, therefore\nR\ndνθ(x) = 0, implying that g1 = g2\nin (2.16). So the deﬁnition of weak derivative of a probability distribution can\nbe re-expressed as:\nDEFINITION 2.6.1\nA probability distribution Fθ is weakly differentiable at a point\nθ (on an open subset of IR) if there exist probability distributions ˙Fθ and ¨Fθ and a\nconstant gθ such that for any bounded continuous function c(x), the following holds:\n∇θ\nZ\nc(x)dFθ(x) = gθ\n\u0014Z\nc(x)d ˙Fθ(x) −\nZ\nc(x)d ¨Fθ(x)\n\u0015\n.\nIn more familiar engineering notation using probability density functions, the\ndeﬁnition of the weak derivative is\n∇θ\nZ\nc(x)pθ(x) = gθ\n\u0014Z\nc(x) ˙pθ(x)dx −\nZ\nc(x)¨pθ(x)dx)\n\u0015\n(2.17)\nwhere ˙pθ and ¨pθ are probability density functions, or equivalently,\n∇pθ = gθ( ˙pθ −¨pθ).\n(2.18)\nThe weak derivative is speciﬁed by the triplet (gθ, ˙pθ, ¨pθ). A similar characteri-\nzation holds in terms of probability mass functions.\nIn general the representation (2.18) of the weak derivative is not unique. One\nspeciﬁc representation of interest is obtained via the so called Hahn Jordan de-\ncomposition. This is a deep result in measure theory - for our practical needs,\nthe following simplistic version sufﬁces: A signed measure can be decomposed\nas in (2.17) such that densities ˙pθ and ¨pθ are orthogonal. This means that the set\nof x where density ˙pθ(x) is non-zero coincides with the set of x where density\n¨pθ(x) is zero, and vice versa.\n3Let Ωbe the set of outcomes and A denote a sigma-algebra deﬁned on Ω. Then a signed\nmeasure ν is a real valued function on A which satisﬁes: (i) σ-additive - meaning that if Ai, i =\n1, 2, . . . are disjoint sets (events) in A, then ν(∪iAi) = P\ni ν(Ai). (ii) ν(∅) = 0.\nA signed measure is ﬁnite if ν(Ω) < ∞. Note a non-negative signed measure is called a measure.\nFinally, ν is a probability measure if ν(Ω) = 1.\n2.6 Weak Derivative Gradient Estimator\n13\n2.6.1\nWeak derivative of random variables\nBased on (2.18), the weak derivative gradient estimation for a random variable\nis as follows: Simulate N samples ˙Xk ∼˙pθ and ¨Xk ∼¨pθ, k = 1, 2, . . ., N. Then\nˆ\n∇θCN = 1\nN\nN\nX\nk=1\ngθ\nh\nc( ˙Xk) −c( ¨Xk)\ni\n.\nExamples: Here are examples of the weak derivative of random variables:\n1. Exponential: πθ(x) = θe−θx. Then the Hahn-Jordan decomposition is\n∇θπθ(x) = e−θx(1 −xθ)I(x > 1\nθ) −e−θx(xθ −1)I(x ≤1\nθ )\nSo gθ = θe, ˙pθ(x) = θe−θx(1 −xθ)I(x > 1\nθ), ¨pθ(x) = θe−θx(xθ −1)I(x ≤1\nθ).\n2. Poisson: For a Poisson random variable, the probability mass function is\npθ(x) = eθθx\nx! ,\nx = 0, 1, . . .\nSo clearly, ∇θpθ(x) = pθ(x −1) −pθ(x). So one possible weak derivative imple-\nmentation is\ngθ = 1, ˙pθ = pθ(x −1), ¨pθ = pθ(x).\n2.6.2\nWeak Derivative Gradient Estimator for Markov Process\nLet πθ denote the stationary distribution of a Markov chain with regular transi-\ntion matrix Pθ. Suppose that for any choice of θ, one can observe via simulation\nsample paths {c(xk)} of the costs where Markov chain xk evolves with transi-\ntion matrix Pθ. The aim is to estimate via simulation the gradient ∇θEπθ{c(x)} =\n∇θ (π′\nθc). It is assumed that Pθ is known but πθ is not known explicitly. Also c(x)\nmay not be known explicitly and the simulated sampled path c(xk) may be ob-\nserved in zero mean noise.\nAnalogous to random variable case (2.17), deﬁne the weak derivative of the\ntransition matrix Pθ as follows: A transition probability matrix Pθ is weakly\ndifferentiable at a point θ (on an open subset of IR) if there exists transition\nprobability matrices ˙Pθ and ¨\nPθ and a diagonal matrix gθ such that:\n∇θPθ = gθ( ˙Pθ −¨Pθ),\n(2.19)\nequivalently, ∇θPθij = gθ,i ( ˙Pθij −¨\nPθij),\ni, j ∈{1, 2, . . ., X}.\nSo the weak derivative of transition matrix Pθ is speciﬁed by the triplet ( ˙Pθ, ¨Pθ, gθ).\nObviously, ∇θPθ1 = ∇θ1 = 0 implying that each row of ∇θPθ adds to zero.\nTHEOREM 2.6.2\nFor regular transition matrix Pθ with stationary distribution πθ\nand cost vector c ∈IRX,\n∇θ (π′\nθc) = π′\nθ(∇θPθ)\n∞\nX\nk=0\nPθ\nkc\n(2.20)\n14\nStochastic Optimization and Gradient Estimation\nThe above theorem leads to the following ﬁnite sample estimate:\n∇θ (π′\nθc) ≈π′\n0(Pθ)m(∇θPθ)\nN\nX\nk=0\n(Pθ)kc\n(2.21)\nfor any initial distribution π0. For sufﬁciently large m and N (2.21) will ap-\nproach (2.20). The sample path interpretation of (2.21) leads to the weak deriva-\ntive estimation Algorithm 4. Step 1 simulates π′\n0(Pθ)m, Step 2 implements the\nweak derivative (∇θPθ) and propagates the two chains ˙x and ¨x for N steps.\nFinally Step 3 estimates the right hand side of (2.21). So Step 3 yields that\nb∇θCN(θ) = gθ,xm\nN\nX\nk=m\nc( ˙xk) −c(¨xk).\nNote if the Markov chains ˙x and ¨x are simulated with common random num-\nbers, then at some time point τ, ˙xτ = ¨xτ, and the processes ˙x, ¨x merge and\nevolve identically after time τ. This yields Step 3 of Algorithm 4.\nAlgorithm 4 Weak Derivative based Gradient Estimation Algorithm\nEvaluate triplet ( ˙Pθ, ¨Pθ, gθ) using formula ∇θPθij = gθ,i ( ˙Pθij −¨Pθij).\nStep 1. Simulate the Markov chain x0, . . . xm−1 with transition matrix Pθ.\nStep 2a. Starting with state xm−1, simulate at time m the states ˙xm and ¨xm with\ntransition matrix ˙Pθ and ¨\nPθ, respectively.\nStep 2b. Starting with states ˙xm and ¨xm, respectively, simulate the two Markov\nchains ˙xm+1, ˙xm+2, . . . , ˙xN and ¨xm+1, ¨xm+2, . . . , ¨xN with transition matrix Pθ.\nUse the same random numbers to simulate these Markov chains\nStep 3. Evaluate the weak derivative estimate via simulation as\nb∇θCm,N(θ) = gθ,xm\nm+τ\nX\nk=m\nc( ˙xk) −c(¨xk), where τ = min{k : ˙xk = ¨xk, k ≤N}.\n(2.22)\n2.7\nBias and Variance of Gradient Estimators\nHere we characterize the statistical properties of the score function and weak\nderivative gradient estimators discussed above.\nTHEOREM 2.7.1\nFor a Markov chain with initial distribution π0, regular transition\nmatrix Pθ with coefﬁcient of ergodicity ρ and stationary distribution πθ:\n1. The score function gradient estimator of Algorithm 3 has:\n(a) Bias: E{ c\n∇θCN} −∇θc′πθ = O(1/N).\n(b) Variance: Var{ c\n∇θCN} = O(N)\n2. The weak derivative gradient estimator of Algorithm 4 has:\n2.8 Complements and Sources\n15\n(a) Bias: E{ c\n∇θCm,N} −∇θc′πθ = O(ρm) ∥π0 −πθ∥TV + O(ρN)\n(b) Variance: Var{ c\n∇θCN} = O(1)\nThe proof is in [34]. The result shows that despite the apparent simplicity\nof the score function gradient estimator (and its widespread use), the weak\nderivative estimator performs better in both bias and variance. The variance of\nthe score function estimator actually grows with sample size! We will show in\nnumerical examples for reinforcement learning of MDPs in §3.5, that the weak\nderivative estimator has a substantially smaller variance than the score function\ngradient estimator.\nWhy is the variance of the score function gradient estimator O(N) while the\nvariance of the weak derivative estimator is O(1)? The weak derivative estima-\ntor uses the difference of two sample paths, { ˙xm} and {¨xm}, m = 1, 2 . . . , N. Its\nvariance is dominated by a term of the form PN\nm=1 g′(Pθ\nm)′(π0 −¯π0) where π0\nand ¯π0 are the distributions of ˙x0 and ¨x0. This sum is bounded by constant ×\nPN\nm=1 ρm which is O(1) since ρ < 1 for a regular transition matrix Pθ. In com-\nparison, the score function estimator uses a single sample path and its variance\nis dominated by a term of the form PN\nm=1 g′(Pθ\nm)′π0. This sum grows as O(N).\n2.8\nComplements and Sources\nThis chapter presented an introductory description of simulation based gradi-\nent estimation and is not meant to be a comprehensive account. The proofs and\nfurther details appear in [34]. We have discussed only scalar step size stochastic\napproximation algorithms.\nThe books [27, 59, 61] present comprehensive accounts of adaptive ﬁltering.\nAdaptive ﬁltering constitutes an important class of stochastic approximation\nalgorithms - [12, 48] are the books in the analysis of stochastic approximation\nalgorithms. The SPSA algorithm was pioneered by Spall; see [63] and the web-\nsite http://www.jhuapl.edu/spsa/ for repository of code and references. [56]\nis an excellent book for coverage of simulation based gradient estimation; in-\ndeed, §2.5 and §2.6 are based on [56].\nIn the discussion of gradient estimators we have omitted the important topic\nof inﬁnitesimal perturbation analysis (IPA) and process derivatives.\n3\nReinforcement Learning\nContents\n3.1\nQ-learning Algorithm\n17\n3.2\nPolicy Gradient Reinforcement Learning for MDP\n19\n3.3\nScore Function Policy Gradient Algorithm for MDP\n21\n3.4\nWeak Derivative Gradient Estimator for MDP\n23\n3.5\nNumerical Comparison of Gradient Estimators\n25\n3.6\nPolicy Gradient Reinforcement Learning for Constrained\nMDP (CMDP)\n26\n3.7\nComplements and Sources\n28\nStochastic dynamic programming assumes that the MDP or POMDP model\nis completely speciﬁed. This chapter presents simulation-based stochastic ap-\nproximation algorithms for estimating the optimal policy of MDPs when the\ntransition probabilities are not known. Simulation-based means that although\nthe transition probabilities are unknown, the decision maker can observe the\nsystem trajectory under any choice of control actions. The simulation-based al-\ngorithms given in this chapter also apply as suboptimal methods for solving\nPOMDPs.\nThe following algorithms are presented in this chapter:\n1. The Q-learning algorithm is described in §3.1. It uses the Robbins Munro al-\ngorithm (described in Chapter 2) to estimate the value function for an uncon-\nstrained MDP. It is also shown how a primal-dual Q-learning algorithm can\nbe used for MDPs with monotone optimal policies. The Q-learning algorithm\nalso applies as a suboptimal method for POMDPs.\n2. Policy gradient algorithms are then presented. These use gradient estimation\n(described in Chapter 2) of the cost function together with a stochastic gradi-\nent algorithm to estimate the optimal policy. The policy gradient algorithms\napply to MDPs and constrained MDPs. They also yield a suboptimal policy\nsearch method for POMDPs.\nSome terminology: Determining the optimal policy of an MDP or POMDP\nwhen the parameters are not known falls under the class of stochastic adaptive\ncontrol problems. Stochastic adaptive control algorithms are of two types: di-\nrect methods, where the unknown transition probabilities Pij(u) are estimated\nsimultaneously while updating the control policy, and implicit methods (such as\nsimulation based methods), where the transition probabilities are not directly\n3.1 Q-learning Algorithm\n17\nestimated in order to compute the control policy. In this chapter, we focus on\nimplicit simulation-based algorithms for solving MDPs and POMDPs. These\nare also called reinforcement learning algorithms. One motivation for such implicit\nmethods is that since they are simulation based, only regions of the state space\nvisited by the simulated sample path are used to determine the controller. Effort\nis not wasted on determining parameters for low probability regions which are\nrarely or never visited.\n3.1\nQ-learning Algorithm\nThe Q-learning algorithm is a widely used reinforcement learning algorithm.\nAs described below, the Q-learning algorithm is the simply the Robbins Munro\nstochastic approximation algorithm (2.5) of Chapter 2 applied to estimate the\nvalue function of Bellman’s dynamic programming equation.\n3.1.1\nDiscounted Cost MDP\nBellman’s dynamic programming equation for a discounted cost MDP (with\ndiscount factor ρ) reads\nV (i) = min\nu∈U\n\nc(i, u) + ρ\nX\nj\nPij(u)V (j)\n\n\n= min\nu∈U (c(i, u) + ρ E{V (xk+1)|xk = i, uk = u})\n(3.1)\nFor each state action pair (i, u) deﬁne the Q-factors as\nQ(i, u) = c(i, u) + ρ\nX\nj\nPij(u)V (j),\ni ∈X, u ∈U.\nWe see from (3.1) that the Q-factors Q(i, u) can be expressed as\nQ(i, u) = c(i, u) + ρ\nX\nj\nPij(u) min\nu′ Q(j, u′)\n(3.2)\n= c(i, u) + ρ E{min\nu′ Q(xk+1, u′)|xk = i, uk = u)}.\n(3.3)\nEquation (3.1) has an expectation E inside the minimization, whereas (3.3) has\nthe expectation outside the minimization. It is this crucial observation that forms\nthe basis for using stochastic approximation algorithms to estimate the Q-factors.\nIndeed (3.3) is simply of the form\nE{f(Q)} = 0\n(3.4)\nwhere the random variables f(Q) are deﬁned as\nf(Q) = c(xk, uk) + ρ min\nu′ Q(xk+1, u′) −Q(xk, uk).\n(3.5)\n18\nReinforcement Learning\nThe Robbins Munro algorithm ((2.5) in §2.1) can be used to estimate the solution\nQ∗of (3.4) as follows: Generate a sequence of estimates ˆQk as\nˆQk+1(xk, uk) = ˆQk(xk, uk) + ǫkf( ˆQk)\n(3.6)\nThe step size ǫk is chosen1 as\nǫk =\nǫ\nVisit(i, u, k)\n(3.7)\nwhere ǫ is a positive constant and Visit(i, u, k) is the number of times the state-\naction pair (i, u) has been visited until time k by the algorithm.\nAlgorithm 5 Q-learning Algorithm\nFor n = 0, 1, . . ., (slow time scale):\nUpdate policy as µn(i) = minu∈U ˆQn∆(i, u) for i = 1, 2, . . ., X.\nFor k = n∆, n∆+ 1, . . . , (n + 1)∆−1 (fast time scale)\nGiven state xk, choose action uk = µn(xk).\nSimulate next state xk+1 ∼Pxk,xk+1(uk).\nUpdate Q-factors as (step size ǫk is chosen according to (3.7))\nˆQk+1(xk, uk) = ˆQk(xk, uk) + ǫk\nh\nc(xk, uk) + ρ min\nu′\nˆQk(xk+1, u′) −ˆQk(xk, uk)\ni\nAlgorithm 5 summarizes the entire procedure as a two-time scale stochas-\ntic approximation algorithm. On the fast time scale, the Q factors are updated\napplying the same policy for a ﬁxed period of time slots referred to as update\ninterval, denoted as ∆in Algorithm 5. After that n-th interval, the new policy\nµn+1(i) is chosen based on current Q-factors as µn+1(i) = minu∈U ˆQ(n+1)∆(i, u).\nThis update is done on the slow time scale.\nNote that Q-learning does not require explicit knowledge of the transition\nprobabilities - all that is needed is access to the controlled system so as to mea-\nsure its next state xk+1 when an action uk is applied. For a ﬁnite state MDP,\nQ-learning algorithm converges with probability one to the optimal solution of\nBellman’s equation; see [48, 14] for conditions and proof. Please refer to [15] for\nnovel variations of the Q-learning algorithm for discounted cost MDPs.\n3.1.2\nPrimal-Dual Q-learning for Submodular MDPs\nIf we know the optimal policy of an MDP is monotone, how can Q-learning ex-\nploit this structure? If the Q-factors of a MDP are submodular, then the optimal\npolicy has a monotone structure. Suppose we do not have explicit knowledge\nof the transition matrix, but we know that the costs and transition matrix satisfy\nsufﬁcient conditions (see [34]) so that the Q-factors are submodular. How can the\nQ-learning algorithm be designed to exploit this submodular property of the Q-factors?\n1In general, the decreasing step size of a stochastic approximation algorithm needs to satisfy\nP\nk ǫk = ∞and P\nk ǫ2\nk < ∞\n3.2 Policy Gradient Reinforcement Learning for MDP\n19\nThe submodularity condition is a linear inequality constraint on Q:\nQ(i, u + 1) −Q(i, u) ≤Q(i + 1, u + 1) −Q(i+, u)\n(3.8)\nWrite this as the inequality constraint\nQM ≥0\n(3.9)\nwhere ≥is element-wise, and the deﬁnition of M is obvious from (3.8).\nIn order to incorporate the constraint (3.9), it is convenient to interpret Q-\nlearning as a stochastic gradient algorithm that minimizes an objective function.\nAccordingly, deﬁne g(Q) so that ∇Qg(Q) = −f(Q) where f(·) is deﬁned in (3.5).\nThen we can write (3.4) as E{f(Q)} = ∇QE{g(Q)} = 0. Then Q-learning can be\ninterpreted as a stochastic approximation algorithm to ﬁnd\nQ∗= argmin\nQ\nE{g(Q)}\nFrom (3.2) and (3.5), ∇2\nQf(Q) = −∇Qg(Q) is a diagonal matrix with non-negative\nelements and hence positive semideﬁnite. Therefore g(Q) is convex.\nAs the objective is convex and the constraint set (linear inequality) is convex,\nwe can use the primal-dual stochastic approximation algorithm to estimate Q∗:\nˆQk+1 = ˆQk + ǫ(1)\nk\nh\nf( ˆQk) + λkM\ni\nλk+1 = max[λk −ǫ(1)\nk QkM, 0].\nHere λk ≥0 are interpreted as Lagrange multipliers for the constraint (3.8). The\nstep sizes ǫ(1)\nk\nand ǫ(1)\nk\nare evaluated as in (3.7). For numerical examples see [22].\n3.2\nPolicy Gradient Reinforcement Learning for MDP\nThe Q-learning algorithms described in §3.1 operate in the value space and aim\nto estimate the value function. The rest of this chapter focuses on solving MDPs\nand POMDPs using reinforcement learning algorithms that operate in the pol-\nicy space. That is, with µθ denoting a policy parametrized by θ, the aim is to\nminimize the expected cumulative cost E{Cn(µθ)} with respect to θ by using a\nstochastic gradient algorithm of the form2\nθn+1 = θn −ǫn ˆ∇θCn(µθn).\n(3.10)\nHere Cn(µθn) denotes the observed cumulative cost by the decision maker when\nusing policy µθn and ˆ∇θCn(µθn) denotes the estimated gradient of the cost\nE{Cn(µθ)} evaluated at µθn. The phrase “policy gradient algorithm” applies\nto algorithm (3.10) since it moves along the gradient of the cost in parametrized\npolicy space θ to determine the optimal parametrized policy.\n2We use n to denote the batch time index. The policy gradient algorithm operates on batches of\ndata where each batch comprises of N time points.\n20\nReinforcement Learning\nOne way of implementing the policy gradient algorithm (3.10) is to use the\nﬁnite difference SPSA Algorithm 2. In this section we focus on using the more\nsophisticated score function and weak derivative gradient estimators of Chap-\nter 2 to design policy gradient algorithms for solving MDPs and constrained\nMDPs.\nConsider an average cost unichain MDP [58] where xk ∈X = {1, 2, . . ., X} is\nthe state and uk ∈U = {1, 2, . . ., U} is the action. Then the Markov process\nzk = (xk, uk)\nhas transition matrix given by\nPi,u,j,¯u(θ) = P(xk+1 = j, uk+1 = ¯u | xk = i, uk = u) = θj¯u Pij(u)\nwhere\nθj¯u = P(uk+1 = ¯u|xk+1 = j).\n(3.11)\nThe action probabilities θ deﬁned in (3.11) specify the policy for the MDP. Let\nπθ(i, a) denote the stationary distribution of the Markov process zk. Then we\ncan solve for πθ(i, a) as a linear program and then computed the action proba-\nbilities as θia = πθ(i, a)/ PX\ni=1 πθ(i, a).\nUnfortunately, if the transition probabilities Pij(u) are not known, then the\nLP cannot be solved. Instead, here we consider the following equivalent formu-\nlation for the optimal policy θ∗:\nθ∗= argmin\nθ∈Θ\nC(θ),\nC(θ) = Eπθ{c(x, u)} =\nX\ni∈X\nX\nu∈U\nπθ(i, a)c(i, a).\nΘ = {θia ≥0,\nX\na∈U\nθia = 1}.\n(3.12)\nNote from (3.12) that the optimal policy speciﬁed by θ∗depends on the station-\nary distribution πθ rather than the unknown transition probabilities P(u).\n3.2.1\nParametrizations of Policy\nTo estimate the optimal θ∗in (3.12), we need to ensure that θ∗∈Θ. To this end,\nit is convenient to parametrize the action probabilities θ by some judiciously\nchosen parameter vector ψ so that\nθia(ψ) = P(un = a|xn = i),\na ∈U = {1, 2 . . ., U}, i ∈X = {1, 2, . . ., X}\nThe optimization problem is then\nmin\nψ∈Ψ C(ψ),\nwhere C(ψ) = Eπψ{c(x, u)}\n(3.13)\nNote that the instantaneous cost is independent of ψ but the expectation is with\nrespect to a measure parametrized by ψ.\nWith the above parametrization, we will use the stochastic gradient algorithm\nψn+1 = ψn −ǫn ˆ∇ψCn(ψn)\n(3.14)\n3.3 Score Function Policy Gradient Algorithm for MDP\n21\nto estimate the minimum θ(ψ∗). Here ˆ∇ψCn(θ(ψn)) denotes an estimate of the\ngradient ∇ψC(θ(ψ)) evaluated at ψn. The aim is obtain a gradient estimator\nwhich does not require explicitly knowing the transition matrices P(u) of the\nMDP. The algorithm (3.14) needs to operate recursively on batches of the ob-\nserved system trajectory {(xk, uk), k ∈{nN, (n + 1)N −1}} to yield a sequence\nof estimates {ψn} of the optimal solution ψ∗.\nBefore proceeding with algorithm (3.14) and the gradient estimator, we ﬁrst\nintroduce two useful parametrizations ψ that automatically encode the con-\nstraints (3.12) on the action probabilities θ.\n1. Exponential Parametrization: The exponential parameterization for θ is\nθia(ψ) =\neψia\nP\nu∈U eψiu ,\nψia ∈IR, i ∈X, a ∈U = {1, 2, . . ., U}.\n(3.15)\nIn (3.15), the ψia ∈IR are unconstrained, yet clearly the constraint (3.12) holds.\nNote ψ has dimension UX.\n2. Spherical Coordinate Parametrization: To each value θiu associate the values\nλiu = √θiu. Then (3.12) yields P\nu∈U λ2\niu = 1, and λiu can be interpreted as\nthe coordinates of a vector that lies on the surface of the unit sphere in IRU. In\nspherical coordinates, the angles are ψia, a = 1, . . . U −1, and the radius is unity.\nFor U ≥2, the spherical coordinates parameterization ψ is deﬁned as:\nθiu(ψ) =\n\n\n\n\n\n\n\ncos2(ψi,1)\nif u = 1\ncos2(ψi,u) Qu−1\np=1 sin2(ψi,p)\n2 ≤u ≤U −1\nsin2(ψi,U−1)) QU−2\np=1 sin2(ψi,p)\nu = U\n.\n(3.16)\nTo summarize, the spherical coordinate parameters are\nψia ∈IR,\ni = 1, 2, . . ., X, a = 1, 2, . . . , U −1.\nThe ψia are unconstrained, yet the constraint (3.12) holds.\nNote ψ has dimension (U −1)X. For example, if U = 2, θi1 = cos2 ψi,1 and\nθi2 = sin2 ψi,1 where ψi1 is unconstrained; clearly θ2\ni1 + θ2\ni2 = 1.\n3.3\nScore Function Policy Gradient Algorithm for MDP\nThis section uses the score function gradient estimator of Algorithm 3 on page 11\nto estimate ˆ∇ψCn(ψn). Together with the stochastic gradient algorithm (3.14), it\nconstitutes a reinforcement learning algorithm for estimating the optimal policy\nψ∗for the MDP without requiring knowledge of the transition matrices.\nWe now describe the score function estimator for an MDP. Consider the aug-\nmented Markov process zk = (xk, uk). From the transition probabilities (3.11),\nit follows that\n∇ψPi,u,x,¯u(θ(ψ)) = Pix(u) ∇ψθx¯u(ψ).\n(3.17)\n22\nReinforcement Learning\nThe aim is to estimate the gradient with respect to each component ψxa for the\nexponential and spherical parametrizations deﬁned above.\nFor the exponential parametrization (3.15), ∇ψxaθxa(ψ) = θxa−θ2\nxa and ∇ψxaθx¯u(ψ) =\n−θxaθx¯u for ¯u ̸= a. So for a = 1, 2, . . ., U, Step 2 of Algorithm 3 for the n-th batch\ncomprising of times k ∈{nN + 1, . . . , (n + 1)N} is\nSψxa\nk\n= ∇ψxaPzk−1,zk(θ)\nPzk−1zk(θ)\n+ Sψxa\nk−1\n(3.18)\nwhere\n∇ψx,aPzk−1,zk(θ)\nPzk−1zk(θ)\n=\n\n\n\n\n\n\n\n1 −θxk,uk\nif a = uk, x = xk\n−θxk,a\nif a ∈U −{uk}, x = xk\n0\notherwise.\nIf instead we use the spherical coordinates (3.16), then Step 2 of Algorithm 3 for\na = 1, 2, . . ., U −1 is:\nSψxa\nk\n= ∇ψxaPzk−1,zk(θ)\nPzk−1zk(θ)\n+ Sψxa\nk−1\n(3.19)\n∇ψxaPzk−1,zk(θ)\nPzk−1zk(θ)\n=\n\n\n\n\n\n\n\n2\ntan ψxa\na < uk, x = xk\n−2 tan ψxa\na = uk, x = xk\n0\na > uk+1\nFinally, for either parametrization, Step 3 of Algorithm 3 for the n-th batch\nreads: b∇ψxaCn(θ) =\n1\nN\nPN\nk=1 c(xk, uk)Sψxa\nk\n.\nThe stochastic gradient algorithm (3.14) together with score Sψ\nk constitute a\nparameter free reinforcement learning algorithm for solving a MDP. As can be\nseen from (3.18), explicit knowledge of the transition probabilities P(u) or costs\nc(x, u) are not required; all that is required is that the cost and next state can be\nobtained (simulated) given the current state by action.\nThe main issue with the score function gradient estimator is its large variance\nas described in Theorem 2.7.1. To reduce the variance, [7] replaces Step 2 with\nSψ\nk = ∇ψPxk−1xk(θ)\nPxk−1xk(θ)\n+ β Sψ\nk−1\n(3.20)\nwhere β ∈(0, 1) is a forgetting factor. Other variance reduction techniques [56]\ninclude regenerative estimation, ﬁnite horizon approximations.\n3.4\nWeak Derivative Gradient Estimator for MDP\nThis section uses the weak derivative gradient estimator of Algorithm 4 on\npage 14 to estimate ˆ∇ψCk(θk). Together with the stochastic gradient algorithm\n(3.14), it constitutes a reinforcement learning algorithm for estimating the opti-\nmal policy ψ∗for an MDP.\n3.4 Weak Derivative Gradient Estimator for MDP\n23\nConsider the augmented Markov process zk = (xk, uk). From the transition\nprobabilities (3.11), it follows that\n∇ψPi,u,x,¯u(θ) = Pix(u) ∇ψθx,¯u.\n(3.21)\nOur plan is as follows: Recall from Algorithm 4 on page 14 that the weak\nderivative estimator generates two Markov process ˙z and ¨z. The weak deriva-\ntive representations we will choose below imply that ˙zk = zk for all time k,\nwhere zk is the sample path of the MDP. So we only need to worry about simu-\nlating the process ¨z. It is shown later in this section that ¨z can also be obtained\nfrom the MDP sample path z by using cut-and-paste arguments.\nFor the exponential parameterization (3.15), one obtains from (3.21) the fol-\nlowing derivative with respect to each component ψxa:\n∇ψxaP(i, u, x, ¯u) = θxa(1 −θxa)\n\u0014\nPix(u) I(¯u = a) −Pix(u)\nθx¯u\n1 −θxa\nI(¯u ̸= a)\n\u0015\n.\nFor the spherical coordinate parameterization (3.16), elementary calculus yields\nthat with respect to each component ψxa, a = 1, 2, . . ., U −1\n∇ψxaP(i, u, x, ¯u) = −2θxa tan ψxa\n\u0014\nPix(u) I(¯u = a) −Pix(u)\nθx¯u\nθxa tan2 ψxa\nI(¯u > a)\n\u0015\n.\nComparing this with the weak derivative decomposition (2.19), we can use the\nweak derivative gradient estimator of Algorithm 4. This yields the Algorithm 6\nwhich is the weak derivative estimator an MDP.\nFinally, the stochastic gradient algorithm (3.14) together with Algorithm 6\nresult in a policy gradient algorithm for solving an MDP.\nParameter Free Weak Derivative Estimator\nUnlike the score function gradient estimator (3.18), the weak derivative estima-\ntor requires explicit knowledge of the transition probabilities P(u) in order to\npropagate the process {¨zk} in the evaluation of (3.22) in Algorithm 6.\nHow can the weak derivative estimator in Algorithm 6 be modiﬁed to work\nwithout knowledge of the transition matrix? This is described in [34, 35] We\nneed to propagate ¨zk in Step 2b. This is done by a cut and paste technique orig-\ninally proposed by [28]. Given ¨zm = (xm, ¨um) at time m, deﬁne\nν = min{k > 0 : zm+k = (xm, ¨um)}.\nSince z is unichain, it follows that ν is ﬁnite with probability one. Then ¨z is\nconstructed as follows:\nStep (i): Choose ¨zm+k = zm+ν+k for k = 1, 2, . . . , N where N denotes some pre-\nspeciﬁed batch size.\nStep (ii): Compute the cost differences in (3.22) as\nN−ν\nX\nk=m\nc(zk) −c(¨zk) =\nm+ν−1\nX\nk=m\nc(zk) +\n✚✚✚✚✚✚\nN−ν\nX\nk=m+ν\nc(zk) −\n✚✚✚✚✚✚\nN−ν\nX\nk=m+ν\nc(zk) −\nN\nX\nk=N−ν+1\nc(zk)\n24\nReinforcement Learning\nAlgorithm 6 Weak Derivative Estimator for MDP\nLet k = 0, 1, . . . , denote local time within the n-th batch.\nStep 1. Simulate z0, . . . , zm−1 with transition probability Pi,u,j,¯u(θ(ψ)) deﬁned\nin (3.11).\nStep 2a. Starting with zm−1, choose ˙zm = zm = (xm, um). Choose ¨xm = xm.\nChoose ¨zm = (¨xm, ¨um) where ¨um is simulated with\nP(¨um = ¯u) =\nθxm¯u\n1 −θxma\n,\n¯u ∈U −{a} (exponential parameterization)\nP(¨um = ¯u) =\nθxm ¯u\nθxma tan2 ψxma\n,\n¯u ∈{a + 1, . . . , U} spherical coordinates)\nStep 2b. Starting\nwith\n˙zm\nand\n¨zm,\nsimulate\nthe\ntwo\nMarkov\nchains\n˙zm+1, ˙zm+2, . . . and ¨zm+1, ¨zm+2, . . . with transition probabilities as in Step 1.\n(Note ˙zk = zk for all time k by construction.)\nStep 3. Evaluate the weak derivative estimate for the n-th batch as\nb∇ψxaCn(θ) = gxma\nm+τ\nX\nk=m\nc(zk) −c(¨zk), where τ = min{k : zk = ¨zk},\n(3.22)\ngxa =\n(\nθxa(1 −θxa)\nexponential parametrization\n−2θxa tan ψxa\nspherical coordinates\nAnother possible implementation is\nN\nX\nk=m\nc(zk) −\nN\nX\nk=m+ν\nc(¨zk) −ν ˆc =\nN−ν−1\nX\nk=m\nc(zk) −ν ˆc\n(3.23)\nHere ˆc denotes any estimator that converges as N →∞to C(ψ) a.s., where\nC(ψ) is deﬁned in (3.12). For example, ˆc =\n1\nN\nPN\nm=1 c(zm). Either of the above\nimplementations in (3.22) together with Algorithm 6 results in a policy gradient\nalgorithm (3.14) for solving the MDP without explicit knowledge of the transi-\ntion matrices.\n3.5\nNumerical Comparison of Gradient Estimators\nThis section compares the score function gradient estimator (used in the inﬂu-\nential paper [7]) with the parameter free weak derivative estimator of §3.4. It is\nshown that the weak derivative estimator has substantially smaller variance.\nThe following MDP was simulated: X = {1, 2} (2 states), U = 3 (3 actions),\nP(1) =\n\u00120.9\n0.1\n0.2\n0.8\n\u0013\n, P(2) =\n\u00120.3\n0.7\n0.6\n0.4\n\u0013\n, P(3) =\n\u00120.5\n0.5\n0.1\n0.9\n\u0013\n.\n3.5 Numerical Comparison of Gradient Estimators\n25\nThe action probability matrix (θ(i, u)) and cost matrix (c(i, u)) were chosen as:\n(θ(i, a)) =\n\u00140.2\n0.6\n0.2\n0.4\n0.4\n0.2\n\u0015\n,\n(c(i, a)) = −\n\u001450.0\n200.0\n10.0\n3.0\n500.0\n0.0\n\u0015\nWe work with the exponential parametrization (3.15). First we compute the\nground-truth. By solving the linear program for the optimal parameter ψ∗, we\nobtain the true derivative at ψ∗as\n∇ψ[C(θ(ψ))] =\n\u0012 −9.010\n18.680\n−9.670\n−45.947\n68.323\n−22.377\n\u0013\n.\n(3.24)\nWe simulated the parameter free weak derivative estimator using (3.23) in Al-\ngorithm 6 for the exponential parametrization. For batch sizes N = 100 and\n1000 respectively, the weak derivative gradient estimates are\nd\n∇C\nWD\n100 =\n\u0012 −7.851 ± 0.618\n17.275 ± 0.664\n−9.425 ± 0.594\n−44.586 ± 1.661\n66.751 ± 1.657\n−22.164 ± 1.732\n\u0013\nd\n∇C\nWD\n1000 =\n\u0012 −8.361 ± 0.215\n17.928 ± 0.240\n−9.566 ± 0.211\n−46.164 ± 0.468\n68.969 ± 0.472\n−22.805 ± 0.539\n\u0013\n.\nThe numbers after ± above, denote the conﬁdence intervals at level 0.05 with\n100 batches. The variance of the gradient estimator is shown in Table 3.1, to-\ngether with the corresponding CPU time.\nN = 1000\nVar[d\n∇C\nWD\nN ]\ni = 1\n1.180\n1.506\n1.159\ni = 2\n5.700\n5.800\n7.565\nCPU\n1 unit\nTable 3.1 Variance of weak derivative estimator with exponential parametrization\nWe implemented the score function gradient estimator of [7] with the follow-\ning parameters: forgetting factor 1, batch sizes of N = 1000 and 10000. In both\ncases a total number of 10, 000 batches were simulated. The score function gra-\ndient estimates are\nd\n∇C\nScore\n10000 =\n\u0012 −3.49 ± 5.83\n16.91 ± 7.17\n−13.42 ± 5.83\n−41.20 ± 14.96\n53.24 ± 15.0\n−12.12 ± 12.24\n\u0013\nd\n∇C\nScore\n1000 =\n\u0012 −6.73 ± 1.84\n19.67 ± 2.26\n−12.93 ± 1.85\n−31.49 ± 4.77\n46.05 ± 4.75\n−14.55 ± 3.88\n\u0013\nThe variance of the score function gradient estimates are given Table 3.2.\nNotice that even with substantially larger batch sizes and number of batches\n(and hence computational time), the variance of the score function estimator\nis orders of magnitude larger than the weak derivative estimator. For further\nnumerical results please see [1, 39, 35].\n26\nReinforcement Learning\nN = 1000\nVar[d\n∇C\nScore\nN\n]\ni = 1\n89083\n135860\n89500\ni = 2\n584012\n593443\n393015\nCPU\n687 units\nN = 10000\nVar[d\n∇C\nScore\nN\n]\ni = 1\n876523\n1310900\n880255\ni = 2\n5841196\n5906325\n3882805\nCPU\n6746 units\nTable 3.2 Variance of Score Function estimator\n3.6\nPolicy Gradient Reinforcement Learning for Constrained\nMDP (CMDP)\nThis section describes how the policy gradient algorithms for MDPs described\nin §3.2-3.4 can be extended to solve constrained MDPs (CMDPs). Recall that for\nCMDPs, the optimal policy is randomized. Assuming a unichain CMDP, our\naim is to obtain the optimal parametrized policy:\nCompute θ∗= argmin\nθ\nC(θ) = Eπθ{c(x, u)} =\nX\ni∈X\nX\nu∈U\nπθ(i, u)c(i, u)\n(3.25)\nsubject to Bl(θ) =\nX\ni\nX\nu\nπθ(i, u) βl(i, u) ≤γl,\nl = 1, 2, . . ., L.\n(3.26)\nThe key difference compared to the unconstrained MDP are the L constraints\n(3.26). As in the unconstrained MDP case (previous section), πθ(i, a) is the sta-\ntionary distribution of the controlled Markov process zk = (xk, uk). The optimal\npolicy depends on the action probabilities θ(ψ∗) where θia(ψ) = P(un = a|xn =\ni) and ψ denotes a suitably chosen parametrization of θ(ψ).\nThe optimal policy of a CMDP is randomized for up to L states. Our aim is to\ndevise stochastic approximation algorithms to estimate this randomized policy,\ni.e., optimize (3.25) subject to (3.26).\nAssumption (O): The minima ψ∗of (3.25), (3.26) are regular, i.e., ∇ψBl(ψ∗),\nl = 1, . . . , L are linearly independent. Then ψ∗belongs to the set of Kuhn Tucker\npoints\n\u001a\nψ∗∈Ψ : ∃µl ≥0, l = 1, . . . , L such that\n∇ψC + ∇ψBµ = 0,\nB′µ = 0\n\u001b\nwhere µ = (µ1 . . . , µL)′. Also, ψ∗satisﬁes the second order sufﬁciency condi-\ntion ∇2\nψC(ψ∗) + ∇2\nψB(ψ∗)µ > 0 (positive deﬁnite) on the subspace {y ∈IRL :\n∇ψBl(ψ∗)y = 0} for all l : Bl(ψ∗) = 0, µl > 0.\nThe policy gradient algorithm for solving the CMDP is described in Algo-\nrithm 7. Below we elaborate on the primal-dual algorithm (3.28), (3.29). The\nconstrained optimization problem (3.25), (3.26) is in general non convex in the\nparameter ψ. One solution methodology is to minimize the augmented La-\ngrangian via a primal-dual stochastic gradient algorithm. Such multiplier al-\n3.6 Policy Gradient Reinforcement Learning for Constrained MDP (CMDP)\n27\nAlgorithm 7 Primal-Dual Reinforcement Learning Algorithm for CMDP\nParameters: Cost matrix (c(i, a)), constraint matrix (β(i, a)), batch size N, step\nsize ǫ > 0\nStep 0. Initialize: Set n = 0, initialize ψ(n) and vector λ(n) ∈IRL\n+.\nStep 1. System Trajectory Observation: Observe MDP over batch In\ndefn\n=\n{k ∈\n[nN, (n + 1)N −1]} using randomized policy θ(ψ(n)) of (3.16) and compute\nestimate ˆB(n) of the constraints as\nˆBǫ\nl (n + 1) = ˆBǫ\nl (n) + √ǫ\n \n1\nN\nX\nk∈In\nβl(Zk) −ˆBǫ\nl (n)\n!\n,\nl = 1, . . . , L.\n(3.27)\nStep 2. Gradient Estimation: Compute [\n∇ψC(n), [\n∇ψB(n) over the batch In using\na gradient estimator (such as weak derivative or score function).\nStep 3. Update Policy θ(ψ(n)): Use a penalty function primal dual based stochas-\ntic approximation algorithm to update ψ as follows:\nψn+1 = ψn −ǫ\n\u0012\n[\n∇ψC(n) + [\n∇ψB(n) max\nh\n0, λn + ∆bB(n)\ni\u0013\n(3.28)\nλn+1 = max\nh\u0010\n1 −ǫ\n∆\n\u0011\nλn, λn + ǫ bBn\ni\n.\n(3.29)\nThe “penalization” ∆is a suitably large positive constant and max[·, ·] above is\ntaken element wise.\nStep 4. Set n = n + 1 and go to Step 1.\ngorithms are widely used in deterministic optimization [13, pg 446] with exten-\nsion to stochastic approximation in [47]. First, convert the inequality MDP con-\nstraints (3.26) to equality constraints by introducing the variables z = (z1, . . . , zL) ∈\nIRL, so that Bl(ψ) + z2\nl = 0, l = 1, . . . , L. Deﬁne the augmented Lagrangian,\nL∆(ψ, z, λ)\ndefn\n= C(ψ) +\nL\nX\nl=1\nλl(Bl(ψ) + z2\nl ) + ∆\n2\nL\nX\nl=1\n\u0000Bl(ψ) + z2\nl\n\u00012 .\n(3.30)\nHere ∆denotes a large positive constant. After some further calculations de-\ntailed in [13, pg.396 and 397], the primal-dual algorithm operating on the aug-\nmented Lagrangian reads\nψǫ\nn+1 = ψǫ\nn −ǫ\n\u0012\n∇ψC(ψǫ\nn) + ∇ψB(ψǫ\nn) max\n\u0014\n0, λǫ\nn + ∆B(ψǫ\nn)\n\u0015\u0013\nλǫ\nn+1 = max\nh\u0010\n1 −ǫ\n∆\n\u0011\nλǫ\nn, λǫ\nn + ǫB(ψǫ\nn)\ni\n(3.31)\nwhere ǫ > 0 denotes the step size and max[·, ·] is taken elementwise.\nLEMMA 3.6.1\nUnder Assumption (O), for sufﬁciently large ∆> 0, there exists\n¯ǫ > 0, such that for all ǫ ∈(0, ¯ǫ], the sequence {ψǫ(n), λǫ(n)} generated by the primal-\ndual algorithm (3.31) is attracted to a local Kuhn-Tucker pair (ψ∗, λ∗).\n28\nReinforcement Learning\nProof\nThe proof follows from Proposition 4.4.2 in [13].\nIn [34] we extend the above algorithms to obtain policy gradient algorithms\nfor POMDPs.\n3.7\nComplements and Sources\nThe book [64] is an inﬂuential book on reinforcement learning. [14] (though\npublished in the mid 1990s) is a remarkably clear exposition on reinforcement\nlearning algorithms. [48, 14] present convergence analyses proofs for several\ntypes of reinforcement learning algorithms. [15] has several novel variations of\nthe Q-learning algorithm for discounted cost MDPs. [2] has novel Q-learning\ntype algorithms for average cost MDPs. [3] uses ant-colony optimization for\nQ-learning. [7] is an inﬂuential paper that uses the score function gradient esti-\nmator in a policy gradient algorithm for POMDPs.\nThe area of reinforcement learning is evolving with rapid dynamics. The pro-\nceedings of the Neural Information Processing Systems (NIPS) conference and\nInternational Conference on Machine Learning (ICML) have numerous recent\nadvances in reinforcement learning algorithms.\n4\nStochastic Approximation\nAlgorithms: Examples\nContents\n4.1\nConvergence of Stochastic Approximation Algorithms\n29\n4.2\nExample 1: Recursive Maximum Likelihood Parameter\nEstimation of HMMs\n34\n4.3\nExample 2: HMM State Estimation via LMS Algorithm\n36\n4.4\nExample 3: Discrete Stochastic Optimization for Policy\nSearch\n40\n4.5\nExample 4: Mean Field Population Dynamics Models for\nSocial Sensing\n46\n4.6\nComplements and Sources\n49\nThis chapter, presents case studies of stochastic approximation algorithms in\nstate/parameter estimation and modeling in the context of POMDPs.\nExample 1 discusses online estimation of the parameters of an HMM using\nthe recursive maximum likelihood estimation algorithm. The motivation stems\nfrom classical adaptive control: the parameter estimation algorithm can be used\nto estimate the parameters of the POMDP for a ﬁxed policy.\nExample 2 shows that for an HMM comprised of a slow Markov chain, the\nleast mean squares algorithm can provide satisfactory state estimates of the\nMarkov chain without any knowledge of the underlying parameters. In the\ncontext of POMDPs, once the state estimates are known, a variety of subop-\ntimal algorithms can be used to synthesize a reasonable policy.\nExample 3 shows how discrete stochastic optimization problems can be solved\nvia stochastic approximation algorithms. In controlled sensing, such algorithms\ncan be used to compute the optimal sensing strategy from a ﬁnite set of policies.\nExample 4 shows how large scale Markov chains can be approximated by a\nsystem of ordinary differential equations. This mean ﬁeld analysis is illustrated\nin the context of information diffusion in a social network.\n4.1\nConvergence of Stochastic Approximation Algorithms\nThis section presents a rapid summary of the convergence analysis of stochastic\napproximation algorithms. The books [12, 62, 48] are seminal works that study\nthe convergence of stochastic approximation algorithms under general condi-\ntions.\n30\nStochastic Approximation Algorithms: Examples\nConsider a constant step size stochastic approximation algorithms of the form\nθk+1 = θk + ǫ H(θk, xk),\nk = 0, 1, . . .\n(4.1)\nwhere {θk} is a sequence of parameter estimates generated by the algorithm, ǫ\nis small positive ﬁxed step size, and xk is a discrete time geometrically ergodic\nMarkov process (continuous or discrete state) with transition kernel P(θk) and\nstationary distribution πθk.\nSuch algorithms are useful for tracking time varying parameters and are\nwidely studied in adaptive ﬁltering. Because of the constant step size, conver-\ngence with probability one of the sequence {θk} is ruled out. Instead, under\nreasonable conditions, {θk} converges weakly (in distribution) as will be for-\nmalized below.\nAnalysis of stochastic approximation algorithms is typically of three types:\n1. Mean Square Error Analysis\n2. Ordinary Differential Equation (ODE) Analysis\n3. Diffusion Limit for Tracking Error\nThe mean square analysis seeks to show that for large time k, E∥θk −θ∗∥2 =\nO(ε) where θ∗is the true parameter. §4.3.2 provides a detailed example.\nIn comparison, the ODE analysis and diffusion limit deal with suitable scaled\nsequences of iterates that are treated as stochastic processes rather than random\nvariables. These two analysis methods seek to characterize the behavior of the\nentire trajectory (random process) rather than just at a speciﬁc time k (random\nvariable) that the mean square error analysis does. The price to pay for the ODE\nand diffusion limit analysis is the highly technical machinery of weak conver-\ngence analysis. Below we will provide a heuristic incomplete treatment that\nonly scratches the surface of this elegant analysis tool.\n4.1.1\nWeak Convergence\nFor constant step size algorithms, under reasonable conditions one can show\nthat the estimates generated by the algorithm converge weakly. For a compre-\nhensive treatment of weak convergence of Markov processes please see [23].\nWeak convergence is a function space generalization of convergence in dis-\ntribution of random variables.1 Consider a continuous time random process\nX(t), t ∈[0, T ] which we will denote as X. A sequence of random processes\n{X(n)} (indexed by n = 1, 2, . . .) converges weakly to X if for each bounded\ncontinuous real-valued functional φ,\nlim\nn→∞E{φ(X(n))} = E{φ(X)}.\nEquivalently, a sequence of probability measures {P (n)} converges weakly to\n1A sequence of random variables {Xn} converges in distribution if limn→∞Fn(x) = F (x) for\nall x for which F is continuous. Here Fn and F are the cumulative distribution functions of Xn\nand X, respectively. An equivalent statement is that E{φ(Xn)} →E{φ(X)} for every bounded\ncontinuous function φ.\n4.1 Convergence of Stochastic Approximation Algorithms\n31\nP if\nR\nφ dP (n) →\nR\nφ dP as n →∞. Note that the functional φ maps the entire\ntrajectory of X(n)(t), 0 ≤t ≤T of the random process to a real number. (The\ndeﬁnition specializes to the classical convergence in distribution if X(n) is a\nrandom variable and φ is a function mapping X(n) to a real number.)\nIn the above deﬁnition, the trajectories of the random processes lie in the\nfunction space C[0, T ] (the class of continuous functions on the interval [0, T ]),\nor more generally, D[0, T ] which is the space of piecewise constant functions\nthat are continuous from the right with limit on the left - these are called ‘cadlag’\nfunctions (continue ´a droite, limite ´a gauche) in French.\n4.1.2\nOrdinary Diﬀerential Equation (ODE) Analysis of Stochastic\nApproximation Algorithms\nConsider a generic stochastic approximation algorithm\nθk+1 = θk + ǫ H(θk, xk),\nk = 0, 1, . . . ,\n(4.2)\nwhere {xk} is a random process and θk ∈IRp is the estimate generated by the\nalgorithm at time k = 1, 2, . . ..\nThe ODE analysis for stochastic approximation algorithms was pioneered by\nLjung (see [49, 50]) and subsequently by Kushner and co-workers [46, 48]. It\naims to show that the sample path {θk} generated by the stochastic approxima-\ntion algorithm (4.2) behaves asymptotically according to a deterministic ordi-\nnary differential equation (ODE). Let us make this more precise. First, we rep-\nresent the sequence {θk} generated by the algorithm (4.2) as a continuous time\nprocess since we want to show that its limiting behavior is the continuous time\nODE. This is done by constructing the continuous time trajectory via piecewise\nconstant interpolation of {θk} as\nθǫ(t) = θk\nfor t ∈[kǫ, kǫ + ǫ),\nk = 0, 1, . . . .\n(4.3)\nWe are interested in studying the limit of the continuous time interpolated pro-\ncess θǫ(t) as ǫ →0 over the time interval [0, T ].\nBy using stochastic averaging theory, it can be shown that the following weak\nconvergence result holds; see [48] for proof.2\nTHEOREM 4.1.1 (ODE analysis)\nConsider the stochastic approximation algorithm\n(4.2) with constant step size ǫ. Assume\n(SA1) H(θ, x) is uniformly bounded for all θ ∈IRp and x ∈IRq.\n(SA2) For any ℓ≥0, there exists h(θ) such that\n1\nN\nN+ℓ−1\nX\nk=ℓ\nEl\n\b\nH(θ, xk)\n\t\n→h(θ) in probability as N →∞.\n(4.4)\n2For random variables, convergence in distribution to a constant implies convergence in prob-\nability. The generalization to weak convergence is: Weak convergence to a deterministic trajectory\n(speciﬁed in our case by an ODE) implies convergence in probability to this deterministic trajectory.\nThe statement in the theorem is a consequence of this.\n32\nStochastic Approximation Algorithms: Examples\nwhere El denotes expectation with respect to the σ-algebra generated by {xk, k < l}.\n(SA3) The ordinary differential equation (ODE)\ndθ(t)\ndt\n= h\n\u0000θ(t)\n\u0001\n,\nθ(0) = θ0.\n(4.5)\nhas a unique solution for every initial condition.\nThen the interpolated estimates θǫ(t) deﬁned in (4.3) satisﬁes\nlim\nǫ→0 P\n\u0000sup\n0≤t≤T\n|θǫ(t) −θ(t)| ≥η\n\u0001\n= 0\nfor all T > 0, η > 0\nwhere θ(t) is the solution of the ODE (4.5).\nNumerous variants of Theorem 4.1.1 exist with weaker conditions. Even though\nthe proof of Theorem 4.1.1 is highly technical, the theorem gives remarkable in-\nsight. It says that for sufﬁciently small step size, the entire interpolated trajec-\ntory of estimates generated by the stochastic approximation algorithm is cap-\ntured by the trajectory of a deterministic ODE. Put differently, if the ODE is\ndesigned to follow a speciﬁc deterministic trajectory, then the stochastic approx-\nimation algorithm will converge weakly to this trajectory.\nAssumption (SA1) is a tightness condition and can be weakened considerably\nas shown in the books [12, 48]. More generally, (SA1) can be replaced by uniform\nintegrability of H(θk, xk), namely that for some α > 0,\nsup\nk≤T/ǫ\nE∥H(θk, xk)∥1+α < ∞.\nAssumption (SA2) is a weak law of large numbers. It allows us to work\nwith correlated sequences whose remote past and distant future are asymp-\ntotically independent. Examples include sequences of i.i.d. random variables\nwith bounded variance, martingale difference sequences with ﬁnite second mo-\nments, moving average processes driven by a martingale difference sequence,\nmixing sequences in which remote past and distant future are asymptotically\nindependent, and certain non-stationary sequences such as functions of Markov\nchains.\nExample: Suppose {xk} is a geometrical ergodic Markov process with transi-\ntion probability matrix (kernel) P(θk) and stationary distribution πθk. Then by\nTheorem 4.1.1, the stochastic approximation converges to the ODE\ndθ(t)\ndt\n= h(θ(t))\nwhere h(θ(t) =\nZ\nX\nH(θ(t), x)πθ(t)(x)dx = Eπθ(t){H(θ(t), x)}.\n(4.6)\n4.1.3\nDiﬀusion Limit for Tracking Error\nThe ODE analysis of Theorem 4.1.1 says that the interpolated trajectory of es-\ntimates generated by the algorithm converges weakly to the trajectory of the\n4.1 Convergence of Stochastic Approximation Algorithms\n33\nODE (4.5) over a time interval [0, T ]. What is the rate of convergence? This is\naddressed by the diffusion limit analysis that we now describe.\nDeﬁne the scaled tracking error as the continuous time process\n˜θǫ(t) = 1\n√ǫ(θǫ(t) −θ(t))\n(4.7)\nwhere θ(t) evolves according to the ODE (4.5) and θǫ(t) is the interpolated tra-\njectory of the stochastic approximation algorithm.\nDeﬁne the following continuous time diffusion process:\n˜θ(t) =\nZ t\n0\n∇θh(θ(s)) ˜θ(s) ds +\nZ t\n0\nR1/2(θ(s)) dw(s)\n(4.8)\nwhere h(θ(t)) is deﬁned in the ODE (4.5), the covariance matrix\nR(θ) =\n∞\nX\nn=−∞\ncovθ (H(θ, xn), H(θ, x0)) ,\n(4.9)\nand w(t) is standard Brownian motion. The last term in (4.8) is interpreted as a\nstochastic (Itˆo) integral; see for example, [30].\nThen the main result regarding the limit of the tracking error is the following:\nTHEOREM 4.1.2 (Diffusion Limit of Tracking Error)\nAs ǫ →0, the scaled error\nprocess ˜θǫ deﬁned in (4.7) converges weakly to the diffusion process ˜θ deﬁned in (4.8).\n4.1.4\nInﬁnite Horizon Asymptotics and Convergence Rate\nThe ODE and diffusion analyses outlined above apply to a ﬁnite time interval\n[0, T ]. Often we are interested in the asymptotic behavior as T →∞. In particu-\nlar, we want an expression for the asymptotic rate of convergence of a stochastic\napproximation algorithm. In stochastic analysis, the rate of convergence refers\nto the asymptotic variance of the normalized errors about the limit point.\nUnder stability conditions so that limT →∞limǫ→0 θǫ(t) = limǫ→0 limT →∞θǫ(t),\nit can be shown [44] that the stable ﬁxed points of the ordinary differential equa-\ntion (4.5) coincide with the attractors of the stochastic approximation algorithm.\nSuppose that this limit exists and denote it as θ∗. Then for large t, the diffusion\napproximation (4.8) becomes the linear (Gaussian) diffusion\n˜θ(t) =\nZ t\n0\n∇θh(θ)|θ=θ∗˜θ(s) ds +\nZ t\n0\nR1/2(θ∗) dw(s).\n(4.10)\nIn other words, the scaled error ˜θǫ(t) converges to a Gaussian process – this can\nbe viewed as a functional central limit theorem.\nSuppose that the matrix ∇θh(θ∗) is stable, that is, all its eigenvalues have\nstrictly negative real parts. Then the diffusion process ˜θ(t) has a stationary\nGaussian distribution, that is for large t,\n˜θ(t) ∼N(0, Σ)\n(4.11)\n34\nStochastic Approximation Algorithms: Examples\nwhere the positive deﬁnite matrix Σ satisﬁes the algebraic Lyapunov equation\n∇θh(θ) Σ + Σ ∇′\nθh(θ) + R(θ)\n\f\f\nθ=θ∗= 0.\n(4.12)\nwhere R is deﬁned in (4.9). The covariance matrix Σ, which is the solution of\n(4.12), is interpreted as the asymptotic rate of convergence of the stochastic approx-\nimation algorithm. For large n and small step size ǫ, (4.11) says that the tracking\nerror of the stochastic approximation algorithm (4.2) behaves as\nǫ−1/2(θn −θ∗) ∼N(0, Σ),\nwhere Σ is the solution of the algebraic Lyapunov equation (4.12).\n4.2\nExample 1: Recursive Maximum Likelihood Parameter\nEstimation of HMMs\nLet xk, k = 0, 1, . . . denote a Markov chain on the state space X = {1, 2, . . ., X}\nwhere X is ﬁxed and known. The HMM parameters P(θ) (transition probabil-\nity matrix) and B(θ) (observation probabilities) are functions of the parameter\nvector θ in a compact subset Θ of Euclidean space. Assume that the HMM ob-\nservations yk, k = 1, 2, . . . are generated by a true parameter vector θo ∈Θ\nwhich is not known. The aim is to design a recursive algorithm to estimate the\nHMM parameter vector θo.\nExample: Consider the Markov chain observed in Markov modulated Gaus-\nsian noise via the observation process\nyk = xk + σ(xk)vk\nwhere vk ∼N(0, 1) is i.i.d. and σ(1), . . . , σ(X) are ﬁxed positive scalars in the\ninterval [σ, ¯σ]. One possible parametrization of this Gaussian noise HMM is\nθ = [P11, . . . , PXX, σ(1), σ(2), . . . , σ(X)]′.\nSo Θ = {θ : Pij ≥0, P\nj Pij = 1, σ(i) ∈[σ, ¯σ]}. A more useful parametrization\nthat automatically encodes these constraints on the transition matrix is to use\nspherical coordinates or the exponential parametrization discussed in §3.2.1.\nThe normalized log likelihood of the HMM parameter θ based on the obser-\nvations y1:n = (y1, . . . , yn) is ln(θ) = 1\nn log p(y1:n|θ). It can be expressed as the\narithmetic mean of terms involving the observations and the HMM prediction\nﬁlter as follows:\nln(θ) = 1\nn\nn\nX\nk=1\nlog\nh\n1′Byk(θ)πθ\nk|k−1\ni\n(4.13)\nwhere By(θ) = diag(p(y|x = 1, θ), . . . , p(y|x = X, θ)). Here πθ\nk|k−1 denotes the\nHMM predictor assuming the HMM parameter is θ:\nπθ\nk|k−1 = [πθ\nk|k−1(1), . . . , πθ\nk|k−1(X)]′, πθ\nk|k−1(i) = Pθ(xk = i|y1, . . . .yk−1).\n4.2 Example 1: Recursive Maximum Likelihood Parameter Estimation of HMMs\n35\nThe HMM predictor is computed via the recursion\nπθ\nk+1|k =\nP ′(θ)Byk(θ)πθ\nk|k−1\n1′Byk(θ)πθ\nk|k−1\n.\n(4.14)\nDeﬁne the incremental score vector as\nS(πθ\nk|k−1, yk, θ) = ∇θ log\nh\n1′Byk(θ)πθ\nk|k−1\ni\n.\n(4.15)\nThen the recursive MLE (RMLE) algorithm for online estimation of the HMM\nparameters is a stochastic approximation algorithm of the form\nθk+1 = ΠΘ\n\u0000θk + ǫ S(πθk\nk|k−1, yk, θk)\n\u0001\n(4.16)\nwhere ǫ denotes a positive step size, and ΠΘ denotes the projection of the esti-\nmate to the set Θ.\n4.2.1\nComputation of Score Vector\nThe score vector in the RMLE algorithm (4.16) can be computed recursively by\ndifferentiating the terms within the summation in (4.13) with respect to each\nelement θ(l) of θ, l = 1, . . . , p. This yields the l-th component of S as\nS(l)(πθ\nk|k−1, yk, θ) = 1′Byk(θ) w(l)\nk (θ)\n1′Byk(θ)πθ\nk|k−1\n+\n1′[∇θ(l)Byk(θ)]πθ\nk|k−1\n1′Byk(θ)πθ\nk|k−1\n(4.17)\nwhere w(l)\nk (θ) = ∇θ(l)πθ\nk|k−1 denotes the partial derivative of πθ\nk|k−1 in (4.14)\nwith respect to the lth component of the parameter vector θ. Deﬁne the X × p\nmatrix wk(θ) = ((w(1)\nk (θ), . . . , w(p)\nk (θ)). Clearly wk(θ) belongs to Ξ deﬁned by\nΞ = {w ∈IRX×p : 1′w = 0} since 1′∇θ(l)πθ\nk|k−1 = ∇θ(l)1′πθ\nk|k−1 = ∇θ(l)1 = 0.\nWe need a recursion for evaluating w(l)\nk (θ) in (4.17). Differentiating πθ\nk+1|k\nwith respect to θ(l) yields\nw(l)\nk+1(θ) = ∇θ(l)πθ\nk+1|k = R1(yk, πθ\nk|k−1, θ)w(l)\nk (θ) + R(l)\n2 (yk, πθ\nk|k−1, θ)\n(4.18)\nwhere\nR1(yk, πθ\nk|k−1, θ) = P ′(θ)\n\"\nI −\nByk(θ) πθ\nk|k−11′\n1′Byk(θ) πθ\nk|k−1\n#\nByk(θ)\n1′Byk(θ) πθ\nk|k−1\nR(l)\n2 (zk, πθ\nk|k−1, θ) = P ′(θ)\n\"\nI −\nByk(θ) πθ\nk|k−11′\n1′Byk(θ) πθ\nk|k−1\n#\n1′∇θ(l)Byk(θ) πθ\nk|k−1\n1′Byk(θ) πθ\nk|k−1\n+\n[∇θ(l)P ′(θ)] Byk(θ) πθ\nk|k−1\n1′Byk(θ) πθ\nk|k−1\n.\nTo summarize (4.16), (4.17) and (4.18) constitute the RMLE algorithm for on-\nline parameter estimation of an HMM.\nThe ODE analysis of the above algorithm is in [44]. Recursive prediction and\n36\nStochastic Approximation Algorithms: Examples\nEM type algorithms were developed in the 1990s [40, 21]; see also [19] for a\ndetailed description.\n4.3\nExample 2: HMM State Estimation via LMS Algorithm\nThis section discusses how the least mean squares (LMS) stochastic gradient al-\ngorithm can be used to estimate the underlying state of a slow Markov chain\ngiven noisy observations. Unlike the HMM ﬁlter, the LMS algorithm does not\nrequire exact knowledge of the underlying transition matrix or observation\nprobabilities. In the context of POMDPs, this implies that for slow Markov\nchains, the underlying state can be estimated (with provable performance bounds)\nand then an MDP controller can be run. This approach lies within the class of\nMDP based heuristics for solving POMDPs.\n4.3.1\nFormulation\nLet {yn} be a sequence of real-valued signals representing the observations ob-\ntained at time n, and {xn} be the time-varying true parameter, an IRr-valued\nrandom process. Suppose that\nyn = ϕ′\nnxn + vn, n = 0, 1, . . .,\n(4.19)\nwhere ϕn ∈IRr is the regression vector and {vn} ∈IR is a zero mean sequence.\nNote that (4.19) is a variant of the usual linear regression model, in which, a\ntime-varying stochastic process xn is in place of a ﬁxed parameter. We assume\nthat xn is a slow discrete-time Markov chain.\n(A1) Suppose that there is a small parameter ε > 0 and that {xn} is a Markov chain\nwith states and transition probability matrix given by\nX = {1, 2, . . ., X},\nand\nP ε = I + εQ.\n(4.20)\nHere I denotes the X ×X identity matrix and Q = (Qij) is a X ×X generator\nmatrix of a continuous-time Markov chain (i.e., Q satisﬁes Qij ≥0 for i ̸= j\nand PX\nj=1 Qij = 0 for each i = 1, . . . , X). For simplicity, assume the initial\ndistribution P(x0 = gi) = π0(i) to be independent of ε for each i = 1, . . . , X.\nNote that the small parameter ε > 0 in (A1) ensures that the identity matrix\nI dominates. In fact, Qij ≥0 for i ̸= j thus the small parameter ε > 0 ensures\nthe entries of the transition matrix to be positive since P ε\nij = δij + εQij ≥0\nfor ε > 0 small enough, where δij = 1 if i = j and is 0 otherwise. The use of\nthe generator Q makes the row sum of the matrix P be one since PX\nj=1 P ε\nij =\n1 + ε PX\nj=1 Qij = 1. The essence is that although the true parameter is time\nvarying, it is piecewise constant. In addition, the process does not change too\nfrequently due to the dominating identity matrix in the transition matrix (4.19).\n4.3 Example 2: HMM State Estimation via LMS Algorithm\n37\nIt remains as a constant most of the time and jumps into another state at random\ninstants. Hence the terminology “slow” Markov chain.\nLMS Algorithm.\nThe LMS algorithm will be used to track the Markov chain {xn}. Recall from\n(2.6) that the LMS algorithm generates estimates {bθn} according to\nbθn+1 = bθn + µϕn(yn −ϕ′\nnbθn),\nn = 0, 1, . . . ,\n(4.21)\nwhere µ > 0 is a small constant step size for the algorithm. By using (4.19) with\neθn = bθn −xn, the tracking error satisﬁes\neθn+1 = eθn −µϕnϕ′\nneθn + µϕnvn + (xn −xn+1).\n(4.22)\nThe aim is to determine bounds on the deviation eθn = bθn −xn. This goal is\naccomplished by the following four steps:\n1. Obtain mean square error bounds for E|bθn −xn|2.\n2. Obtain a limit ODE of centered process.\n3. Obtain a weak convergence result of a suitably scaled sequence.\nThe Markov chain xn is called a hypermodel in [12]. While the dynamics of the\nhypermodel xn are used in our analysis, it does not enter the implementation\nof the LMS algorithm (4.21) explicitly.\nAssumptions on the Signals.\nLet Fn be the σ-algebra generated by {(ϕj, vj), j < n, xj, j ≤n}, and denote\nthe conditional expectation with respect to Fn by En. We will use the following\nconditions on the signals.\n(A2) The signal {ϕn, vn} is independent of {xn}. Either {ϕn, vn} is a sequence of\nbounded signals such that there is a symmetric and positive deﬁnite matrix\nB ∈IRr×r such that E{ϕnϕ′\nn} = B\n\f\f\f\n∞\nX\nj=n\nEn{ϕjϕ′\nj −B}\n\f\f\f ≤K,\nand\n\f\f\f\n∞\nX\nj=n\nEn{ϕnvj}\n\f\f\f ≤K,\n(4.23)\nor {ϕn, vn} is a martingale difference satisfying supn E{|ϕn|4+∆} < ∞and\nsupn E{|ϕnvn|2+∆} < ∞for some ∆> 0.\nRemark: Inequalities (4.23) are modeled after mixing processes and are in the\nalmost sure (a.s.) sense with the constant K independent of ω, the sample point.\n[Note, however, we use the same kind of notation as in, for example, the mixing\ninequalities [17, p.166, Eq. (20.4)] and [46, p.82, Eqs. (6.6) and (6.7)].] This allows\nus to work with correlated signals whose remote past and distant future are\nasymptotically independent. To obtain the desired result, the distribution of the\nsignal need not be known. The boundedness is a mild restriction, for example,\none may consider truncated Gaussian processes etc. Moreover, dealing with\n38\nStochastic Approximation Algorithms: Examples\nrecursive procedures in practice, in lieu of (4.21), one often uses a projection or\ntruncation algorithm. For instance, one may use\nbθn+1 = πH[bθn + µϕn(yn −ϕ′\nnbθn)],\n(4.24)\nwhere πH is a projection operator and H is a bounded set. When the iterates are\noutside H, it will be projected back to the constrained set H. Extensive discus-\nsions for such projection algorithms can be found in [48].\n4.3.2\nAnalysis of Tracking Error\nMean Square Error Bounds\nThis section establishes a mean square error estimate for E|bθn−xn|2. It is impor-\ntant to note that the mean square error analysis below holds for small positive\nbut ﬁxed µ and ε. Indeed, let λmin > 0 denote the smallest eigenvalue of the\nsymmetric positive deﬁnite matrix B deﬁned in (A2). Then in the following\ntheorem, it is sufﬁcient to pick out µ and ε small enough so that\nλminµ > O(µ2) + O(εµ);\n(4.25)\nThe phrase “for sufﬁciently large n” in what follows means that there is an\nn0 = n0(ε, µ) such that (4.26) holds for n ≥n0. In fact, (4.26) holds uniformly\nfor n ≥n0.\nTHEOREM 4.3.1\nUnder conditions (A1) and (A2), for sufﬁciently large n, as ε →0\nand µ →0,\nE|eθn|2 = E|bθn −xn|2 = O (µ + ε/µ) exp(µ + ε/µ).\n(4.26)\nThe theorem is proved in the appendix to this chapter.\nIn view of Theorem 4.3.1, it is clear that in order for the adaptive algorithm\nto track the time-varying parameter, due to the presence of the term exp(ε/µ),\nwe need to have at least ε/µ = O(1). Thus, the ratio ε/µ must not be large. A\nglance of the order of magnitude estimate O(µ + ε/µ), to balance the two terms\nµ and ε/µ, we need to choose ε = O(µ2). Therefore, we arrive at the following\ncorollary.\nCOROLLARY 4.3.2\nUnder the conditions of Theorem 4.3.1, if ε = O(µ2), then for\nsufﬁciently large n, E|θn|2 = O(µ).\nMean ODE and Diffusion Approximation\nDue to the form of the transition matrix given by (4.20), the underlying Markov\nchain belongs to the category of two-time-scale Markov chains. For some of\nthe recent work on this subject, we refer the reader to [70] and the references\ntherein. It is assumed that ε = O(µ2) (see Corollary 4.3.2), i.e., the adaptation\nspeed of the LMS algorithm (4.21) is faster than the Markov chain dynamics.\nRecall that the mean square error analysis in §4.3.2 deals with the mean square\nbehavior of the random variable eθn = bθn −xn as n →∞, for small but ﬁxed\n4.3 Example 2: HMM State Estimation via LMS Algorithm\n39\nµ and ε. In contrast, the mean ODE and diffusion approximation analysis of\nthis section deal with how the entire discrete-time trajectory (stochastic pro-\ncess) {eθn : n = 0, 1, 2 . . ., } converges (weakly) to a the limiting continuous-time\nprocess (on a suitable function space) as µ →0 on a time scale O(1/µ). Since\nthe underlying true parameter xn evolves according to a Markov chain (unlike\nstandard stochastic approximation proofs where the parameter is assumed con-\nstant), the proofs of the ODE and diffusion limit are non-standard and require\nuse of the so called “martingale problem” formulation for stochastic diffusions;\nplease see [67, 69] for details.\nMean Ordinary Differential Equation (ODE)\nDeﬁne eθn = bθn −xn and the interpolated process\neθµ(t) = eθn\nfor t ∈[nµ, nµ + µ).\n(4.27)\nWe will need another condition.\n(A3) As n →∞,\n1\nn\nn1+n\nX\nj=n1\nEn1ϕjvj →0, in probability,\n1\nn\nn1+n\nX\nj=n1\nEn1ϕjϕ′\nj →B, in probability.\nTHEOREM 4.3.3\nUnder (A1)–(A3) and assuming that eθ0 = eθµ\n0 converges weakly\nto eθ0, then eθµ(·) deﬁned in (4.27) converges weakly to eθ(·), which is a solution of the\nODE\nd\ndt\neθ(t) = −Beθ(t), t ≥0, eθ(0) = eθ0.\n(4.28)\nThis theorem provides us with the evolution of the tracking errors. It shows\nthat bθn −xn evolves dynamically so that its trajectories follows a deterministic\nODE. Since the ODE is asymptotically stable, the errors decay exponentially fast\nto 0 as time grows.\n4.3.3\nClosing Remark. Tracking Fast Markov Chains\nWhat happens if ǫ = O(µ) so that the Markov chain evolves on the same time\nscale as the LMS algorithm? The analysis is more complex [69] and the main\nresults are as follows:\n• The mean square tracking error is (compare with Theorem 4.3.2)\nE|eθn|2 = E|bθn −xn|2 = O(µ).\n(4.29)\n• The ODE becomes a Markov modulated ODE (compare with Theorem 4.3.3)\nd\ndt\neθ(t) = −Beθ(t) + Qx(t),\nt ≥0, eθ(0) = eθ0.\nwhere x(t) is the interpolated Markov process (constructed as in (4.27)) with\n40\nStochastic Approximation Algorithms: Examples\ntransition rate matrix Q. So stochastic averaging no longer results in a de-\nterministic ODE. Instead the limiting behavior is speciﬁed by a differential\nequation driven by a continuous-time Markov chain with transition rate ma-\ntrix Q, see [69, 65].\n4.4\nExample 3: Discrete Stochastic Optimization for Policy\nSearch\nIn this section we consider a POMDP modeled as a back box: a decision maker\ncan choose one of a ﬁnite number of policies and can measure the noisy costs\nincurred over a horizon. The aim is to solve the following time-varying discrete\nstochastic optimization problem:\nCompute θ∗= argmin\nθ∈Θ\nC(θ),\nwhere C(θ) = E{cn(θ)}.\n(4.30)\nHere\n• Θ = {1, 2, . . ., S} denotes the ﬁnite discrete space of possible policies.\n• θ is a parameter that speciﬁes a policy - for example it could specify how long\nand in which order to use speciﬁc sensors (sensing modes) in a controlled\nsensing problem.\n• cn(θ) is the observed cost incurred when using strategy θ for sensing. Typi-\ncally, this cost is evaluated by running the POMDP over a speciﬁed horizon.\nSo n can be viewed as an index for the n-th run of the POMDP.\n• θ∗∈G where G ⊂Θ denotes the set of global minimizers of (4.30).\nIt is assumed that the probability distribution of the noisy cost cn is not\nknown. Therefore, C(θ) in (4.30) cannot be evaluated at each time n; otherwise\nthe problem is a deterministic integer optimization problem. We can only ob-\nserve noisy samples cn(θ) of the system performance C(θ) via simulation for\nany choice of θ ∈Θ. Given this noisy performance {cn(θ)}, n = 1, 2, . . . , the\naim is to estimate θ∗.\nAn obvious brute force method for solving (4.30) involves an exhaustive enu-\nmeration: For each θ ∈Θ, compute the empirical average\nbcN(θ) = 1\nN\nN\nX\nn=1\ncn(θ),\nvia simulation for large N. Then pick bθ = argminθ∈Θ bcN(θ). Since for any ﬁxed\nθ ∈Θ, {cn(θ)} is an i.i.d. sequence of random variables, by virtue of Kol-\nmogorov’s strong law of large numbers, bcN(θ) →E{cn(θ)} w.p.1, as N →∞.\nThis and the ﬁniteness of Θ imply that\nargmin\nθ∈Θ\nbcN(θ) →argmin\nθ∈Θ\nE {cn(θ)} w.p.1 as N →∞.\n(4.31)\nIn principle, the above brute force simulation method can solve the discrete\nstochastic optimization problem (4.30) and the estimate is consistent, i.e., (4.31)\n4.4 Example 3: Discrete Stochastic Optimization for Policy Search\n41\nholds. However, the method is highly inefﬁcient since bcN(θ) needs to be evalu-\nated for each θ ∈Θ. The evaluations of bcN(θ) for θ ̸∈G are wasted because they\ncontribute nothing to the estimation of bcN(θ), θ ∈G.\nThe idea of discrete stochastic approximation is to design an algorithm that\nis both consistent and attracted to the minimum. That is, the algorithm should\nspend more time obtaining observations cn(θ) in areas of the state space near G\nand less in other areas.\nThe discrete stochastic optimization problem (4.30) is similar in spirit to the\nstochastic bandit problem with the key difference that (4.30) aims to determine\nthe minimizer θ∗while bandits aim to minimize an average observed function\nover a period of time (called the regret).\nIn the remainder of this section we provide two examples of discrete stochas-\ntic optimization algorithms:\n• The smooth best-response adaptive search algorithm.\n• Discrete stochastic search algorithm.\nA numerical example is then given to illustrate the performance of these algo-\nrithms. Also the performance is compared with the upper conﬁdence bound (UCB)\nalgorithm which is a popular multi-armed bandit algorithm.\nWe are interested in situations where the objective function C(θ) in (4.30)\nevolves randomly over a slow time scale and hence the global minimizers evolve\nslowly with time. So the stochastic optimization algorithms described below\nhave constant step sizes to facilitate tracking time varying global minimizers.\n4.4.1\nAlgorithm 1. Smooth Best-Response Adaptive Search\nAlgorithm 8 describes the smooth best-response adaptive search algorithm for\nsolving the discrete stochastic optimization problem (4.30). In this algorithm,\nγ ∈(0, D] denotes the exploration weight where D ≥maxθ C(θ) −minθ C(θ)\nupper bounds the maximum difference in objective function among the feasible\nsolutions.\nThe S-dimensional probability vector bγ(ψn) in Step 1 of Algorithm 8 is ac-\ntually a special case of the following general framework: Let Π denote the unit\nS-dimensional simplex, and int(Π) denote the interior of this simplex. Given\nthe vector ψ = [ψ1, . . . , ψS]′ ∈IRS of beliefs about objective values at different\ncandidate solutions, the smooth best-response sampling strategy bγ ∈Π is\nbγ\u0000ψ\n\u0001\n:= argmin\nσ∈Π\n(σ′ψ −γρ(σ)) , 0 < γ < D,\n(4.33)\nwhere σ ∈Π and the perturbation function ρ(σ) : int(Π) →IR satisfy\ni) ρ(·) is continuously differentiable, strictly concave, and |ρ| ≤1;\nii) ∥∇ρ(σ)∥→∞as σ approaches the boundary of Π where ∥· ∥denotes the\nEuclidean norm.\nIn Algorithm 8, we chose ρ (·) as the entropy function [24] ρ (σ) = −P\ni∈Θ σi log (σi).\n42\nStochastic Approximation Algorithms: Examples\nAlgorithm 8 Smooth Best-Response Adaptive Search\nStep 0. Initialization. Set exploration parameter γ ∈(0, D]. Initialize ψ0 = 0S.\nStep 1. Sampling. Sample θn ∈Θ according to S-dimensional probability vector\nbγ(ψn) = [bγ\n1(ψn), . . . , bγ\nS(ψn)] ,\nwhere bγ\ni\n\u0000ψ\n\u0001\n=\nexp\n\u0000−ψi/γ\n\u0001\nP\nj∈Θ exp\n\u0000−ψj/γ\n\u0001.\nStep 2. Evaluation. Perform simulation to obtain cn(θn).\nStep 3. Belief Update. Update ψn+1 ∈IRS as\nψn+1 = ψn + µ\n\u0002\nf\n\u0000θn, ψn, cn(θn)\n\u0001\n−ψn\n\u0003\n,\n(4.32)\nwhere f = (f1, . . . , fS)′,\nfi\n\u0000θn, ψn, cn(θn)\n\u0001\n= cn(θn)\nbγ\ni\n\u0000ψn\n\u0001I(θn = i).\nStep 4. Recursion. Set n ←n + 1 and go to Step 1.\nThen applying (4.33) yields Step 1 which can be viewed as a Boltzmann explo-\nration strategy with constant temperature\nbγ\ni\n\u0000ψ\n\u0001\n=\nexp\n\u0000−ψi/γ\n\u0001\nP\nj∈Θ exp\n\u0000−ψj/γ\n\u0001.\n(4.34)\nSuch an exploration strategy is used widely in the context of game-theoretic\nlearning and is called logistic ﬁctitious-play [25] or logit choice function [29].\n[55] shows that that the sequence {θn} generated by Algorithm 8 spends more\ntime in the set of global maximizers G than any other point in Θ.\n4.4.2\nAlgorithm 2. Discrete Stochastic Search\nThe second discrete stochastic optimization algorithm that we discuss is dis-\nplayed in Algorithm 9. This random search algorithm was proposed by An-\ndradottir [4, 5] for computing the global minimizer in (4.30). Recall G denotes\nthe set of global minimizers of (4.30) and Θ = {1, 2, . . ., S} is the search space.\nThe following assumption is required:\n(O) For each θ, ˜θ ∈Θ −G and θ∗∈G,\nP(cn(θ∗) < cn(θ)) ≥P(cn(θ) > cn(θ∗))\nP(cn(˜θ) > cn(θ∗)) ≥P(cn(˜θ) > cn(θ))\nAssumption (O) ensures that the algorithm is more likely to jump towards\na global minimum than away from it. Suppose cn(θ) = θ + wn(θ) in (4.30) for\neach θ ∈Θ, where {wn(θ)} has a symmetric probability density function or\nprobability mass function with zero mean. Then assumption (O) holds.\nAlgorithm 9 is attracted to the global minimizer set G in the sense that it\nspends more time in G than at any other candidate in Θ −G. This was proved\nin in [4]; see also [43, 69] for tracking time varying optima.\n4.4 Example 3: Discrete Stochastic Optimization for Policy Search\n43\nAlgorithm 9 Random Search (RS) Algorithm\nStep 0. Initialization. Select θ0 ∈Θ. Set πi,0 = 1 if i = θ0, and 0 otherwise.\nSet θ∗\n0 = θ0 and n = 1.\nStep 1. Sampling. Sample a candidate solution ˜θn uniformly from the set Θ −\nθn−1.\nStep 2. Evaluation. Simulate samples cn(θn−1) and cn(˜θn).\nIf cn(˜θn) < cn(θn−1), set θn = ˜θn, else, set θn = θn−1.\nStep 3. Belief (Occupation Probability) Update.\nπn = πn−1 + µ [eθn −πn−1] ,\n0 < µ < 1,\n(4.35)\nwhere eθn is the unit vector with the θn-th element being equal to one.\nStep 4. Global Optimum Estimate. θ∗\nn ∈argmaxi∈Θ πi,n.\nStep 5. Recursion. Set n ←n + 1 and go to Step 1.\n4.4.3\nAlgorithm 3. Upper Conﬁdence Bound (UCB) Algorithm\nThe ﬁnal algorithm that we discuss is the UCB algorithm. UCB [6] belongs to\nthe family of “follow the perturbed leader” algorithms and is widely used in the\ncontext of multi-armed bandits. Since bandits are typically formulated in terms\nof maximization (rather than minimization), we consider here maximization of\nC(θ) in (4.30) and denote the global maximizers as θ∗. Let B denote an upper\nbound on the objective function and ξ > 0 be a constant. The UCB algorithm is\nsummarized in Algorithm 10. For a static discrete stochastic optimization prob-\nlem, we set µ = 1; otherwise, the discount factor µ has to be chosen in the\ninterval (0, 1). Each iteration of UCB requires O(S) arithmetic operations, one\nmaximization and one simulation of the objective function.\n4.4.4\nNumerical Examples\nThis section illustrates the performance of the above discrete stochastic opti-\nmization algorithms in estimating the mode of an unknown probability mass\nfunction. Let ρ(θ), θ ∈Θ = {0, 1, . . ., S} denote the degree distribution3 of a so-\ncial network (random graph). Suppose that a pollster aims to estimate the mode\nof the degree distribution, namely\nθ∗=\nargmax\nθ∈{0,1,...,S}\nρ(θ).\n(4.36)\nThe pollster does not know ρ(θ). It uses the following protocol to estimate the\nmode. At each time n, the pollster chooses a speciﬁc θn ∈Θ, and then asks a\nrandomly sampled individual in the social network: Is your degree θn? The in-\ndividual replies “yes” (1) or “no” (0). Given these responses {I(θn), n = 1, 2, . . .}\n3The degree of a node is the number of connections or edges the node has to other nodes. The\ndegree distribution ρ(θ) is the fraction of nodes in the network with degree θ, where θ ∈Θ =\n{0, 1, . . .}. Note that P\nθ ρ(θ) = 1 and so the degree distribution is a pmf with support on Θ.\n44\nStochastic Approximation Algorithms: Examples\nAlgorithm 10 Upper conﬁdence bound (UCB) algorithm for maximization of\nobjective C(θ), θ ∈Θ = {1, 2, . . ., S} with discount factor µ ∈(0, 1] and explo-\nration constant ξ > 0. B is an upper bound on the objective function.\nStep 0. Initialization. Simulate each θ ∈Θ = {1, 2, . . ., S} once to obtain c1(θ).\nSet bcθ,S = c1(θ) and mθ,S = 1. Set n = S + 1.\nStep 1a. Sampling. At time n sample candidate solution\nθn = argmax\nθ∈Θ\n\"\nbcθ,n−1 + B\ns\nξ log(Mn−1 + 1)\nmθ,n−1\n#\nwhere bcθ,n−1 =\n1\nmθ,n−1\nPn−1\nτ=1 µn−τ−1cτ(θτ)I(θτ = θ),\nmθ,n−1 =\nn−1\nX\nτ=1\nµn−τ−1I(θτ = θ),\nMn−1 =\nS\nX\ni=1\nmi,n−1\nStep 1b. Evaluation. Simulate to obtain cn(θn).\nStep 2. Global Optimum Estimate. θ∗\nn ∈argmaxi∈Θ bci,n.\nStep 3. Recursion. Set n ←n + 1 and go to Step 1.\nwhere I(·) denotes the indicator function, the pollster aims to solve the discrete\nstochastic optimization problem: Compute\nθ∗=\nargmin\nθ∈{0,1,...,S}\n−E {I(θn)} .\n(4.37)\nClearly the global optimizers of (4.36) and (4.37) coincide. Below, we illustrate\nthe performance of discrete stochastic optimization Algorithm 8 (abbreviated as\nAS), Algorithm 9 (abbreviated as RS), and Algorithm 10 (abbreviated as UCB)\nfor estimating θ∗in (4.37).\nIn the numerical examples below, we simulated the degree distribution as a\nPoisson pmf with parameter λ:\nρ(θ) = λθ exp(−λ)\nθ!\n,\nθ ∈{0, 1, 2, . . ., S}.\nIt is straightforward to show that the mode of the Poisson distribution is\nθ∗= argmax\nθ\nρ(θ) = ⌊λ⌋.\n(4.38)\nIf λ is integer-valued, then both λ and λ−1 are modes of the Poisson pmf. Since\nthe ground truth θ∗is explicitly given by (4.38) we have a simple benchmark\nnumerical study. (The algorithms do not use the knowledge that ρ is Poisson.)\nExample: Static Discrete Stochastic Optimization.\nConsider the case where the Poisson rate λ is a constant. We consider two ex-\namples: i) λ = 1, which implies that the set of global optimizers is G = {0, 1},\nand ii) λ = 10, in which case G = {9, 10}. For each case, we further study the ef-\nfect of the size of search space on the performance of algorithms by considering\n4.4 Example 3: Discrete Stochastic Optimization for Policy Search\n45\nTable 4.1 Example 1: Percentage of Independent Runs of Algorithms that Converged to\nthe Global Optimum Set in n Iterations.\n(a) λ = 1\nIteration\nS = 10\nS = 100\nn\nAS\nRS\nUCB\nAS\nRS\nUCB\n10\n55\n39\n86\n11\n6\n43\n50\n98\n72\n90\n30\n18\n79\n100\n100\n82\n95\n48\n29\n83\n500\n100\n96\n100\n79\n66\n89\n1000\n100\n100\n100\n93\n80\n91\n5000\n100\n100\n100\n100\n96\n99\n10000\n100\n100\n100\n100\n100\n100\n(b) λ = 10\nIteration\nS = 10\nS = 100\nn\nAS\nRS\nUCB\nAS\nRS\nUCB\n10\n29\n14\n15\n7\n3\n2\n100\n45\n30\n41\n16\n9\n13\n500\n54\n43\n58\n28\n21\n25\n1000\n69\n59\n74\n34\n26\n30\n5000\n86\n75\n86\n60\n44\n44\n10000\n94\n84\n94\n68\n49\n59\n20000\n100\n88\n100\n81\n61\n74\n50000\n100\n95\n100\n90\n65\n81\ntwo instances: i) S = 10, and ii) S = 100. Since the problem is static in the sense\nthat G is ﬁxed for each case, one can use the results of [8] to show that if the\nexploration factor γ in (4.33) decreases to zero sufﬁciently slowly, the sequence\nof samples {θn} converges almost surely to the global minimum. We consider\nthe following modiﬁcations to AS Algorithm 8:\n(i) The constant step-size µ in (4.32) is replaced by decreasing step-size µ(n) =\n1\nn;\n(ii) The exploration factor γ in (4.33) is replaced by\n1\nnβ , 0 < β < 1.\nWe chose β = 0.2 and γ = 0.01. Also in Algorithm 10, we set B = 1, and ξ = 2.\nTable 4.1 displays the performance of the algorithms AS, RS and UCB. To\ngive a fair comparison, the performance of the algorithms are compared based\non number of simulation experiments performed by each algorithm. Observe\nthe following from Table 4.1: In all three algorithms, the speed of convergence\ndecreases when either S or λ (or both) increases. However, the effect of increas-\ning λ is more substantial since the objective function values of the worst and\nbest states are closer when λ = 10. Given equal number of simulation experi-\nments, higher percentage of cases that a particular method has converged to the\nglobal optima indicates faster convergence rate.\n46\nStochastic Approximation Algorithms: Examples\nTo evaluate and compare efﬁciency of the algorithms, the sample path of the\nnumber of simulation experiments performed on non-optimal feasible solutions\nis displayed in Figure 4.1, when λ = 1 and S = 100. As can be seen, since\nthe RS method randomizes among all (except the previously sampled) feasible\nsolutions at each iteration, it performs approximately 98% of the simulations\non non-optimal elements. The UCB algorithm switches to its exploitation phase\nafter a longer period of exploration as compared to the AS algorithm.\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n10\n6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nNumber of Simulations n\nProportion of Simulations on Non-Optimal States\n \n \nAS\nRS\nUCB\nFigure 4.1 Example 1: Proportion of simulation effort expended on states outside the\nglobal optima set (λ = 1, S = 100).\nIn [34] we also present examples of how the algorithms perform when the\noptimum evolves according to a Markov chain.\n4.5\nExample 4: Mean Field Population Dynamics Models for\nSocial Sensing\nWe are interested in constructing tractable models for the diffusion of infor-\nmation over a social network comprising of a population of interacting agents.\nAs described in [51], such models arise in a wide range of social phenomena\nsuch as diffusion of technological innovations, ideas, behaviors, trends [26], cul-\ntural fads, and economic conventions [20] where individual decisions are inﬂu-\nenced by the decisions of others. Once a tractable model has been constructed,\nBayesian ﬁlters can be used to estimate the underlying state of nature [38].\nConsider a social network comprised of individuals (agents) who may or may\nnot adopt a speciﬁc new technology (such as a particular brand of smartphone).\nWe are interested in modeling how adoption of this technology diffuses in the\nsocial network. Figure 4.2 shows the schematic setup. The underlying state of\nnature {xk} can be viewed as the market conditions or competing technologies\nthat evolve with time and affect the adoption of the technology. The informa-\ntion (adoption of technology) diffuses over the network – the states of individ-\nual nodes (adopt or don’t adopt) evolve over time as a probabilistic function\n4.5 Example 4: Mean Field Population Dynamics Models for Social Sensing\n47\nof the states of their neighbors and the state of nature {xk}. Let θk denote the\npopulation state vector at time k; as explained below the l-th element of this\nvector, denoted θk(l), is the fraction of the population with k neighbors that\nhas adopted the technology. As the adoption of the new technology diffuses\nthrough the network, its effect is observed by social sensing - such as user sen-\ntiment on a micro-blogging platform like Twitter. The nodes that tweet their\nsentiments act as social sensors. Suppose the state of nature xk changes sud-\ndenly due to a sudden market shock or presence of a new competitor. The goal\nfor a market analyst or product manufacturer is to estimate the state of nature\nxk so as to detect the market shock or new competitor.\nr\nr\nInformation diffusion\nin social network\nSentiment\nestimator\nState of\nnature\nxk\nθk\nPopulation\nstate\nEstimate\nFigure 4.2 Social Sensing of Sentiment\nAs a signal processing problem, the state of nature xk can be viewed as the\nsignal (Markov chain), and the social network can be viewed as a sensor. The\nobserved sentiment can be viewed as an HMM: noisy measurement of θk (pop-\nulation state) which in turn depends on xk (state of nature). The key difference\ncompared to classical signal processing is that the social network (sensor) has\ndynamics due to the information diffusion over a graph. Estimating xk can be\nformulated as a ﬁltering problem, while detecting sudden changes in xk can be\nformulated as a quickest detection problem.\nDealing with large populations results in an combinatorial explosion in the\nsize of the state space for θk. The aim of this section is to construct tractable\nmodels of the population dynamics by using the mean ﬁeld dynamics approxi-\nmation.\n4.5.1\nPopulation Model Dynamics\nLet xk denote the state of nature at time k. Consider a population consisting of\nM agents indexed by m = 1, 2, . . ..M. Each agent m at time k has state s(m)\nk\n∈\n{1, 2 . . ., L}. Let the L dimensional vector θk denote the fraction of agents in the\nL different states at time k. We call θk the population state:\nθk(l) = 1\nM\nM\nX\nm=1\nI(s(m)\nk\n= l),\nl = 1, 2, . . ., L.\n(4.39)\nwhere I(·) denotes the indicator function.\nGiven the states s(m)\nk\n, m = 1, . . . , M and hence population state θk (4.39), the\npopulation evolves as a Markov process in the following 2-steps: At each time k\n48\nStochastic Approximation Algorithms: Examples\nStep 1. An agent mk is chosen uniformly from the M agents. The state s(mk)\nk+1 at\ntime k + 1 of this agent is simulated with transition probability\nPij(xk, θk) = P(sk+1 = j|sk = i, xk, θk)\nwhich depends on both the state of nature and the population state.\nStep 2. Then the population state θk+1 reﬂects this updated state of agent mk:\nθk+1 = θk + 1\nM\n\u0014\nes\n(mk)\nk+1 −es\n(mk)\nk\n\u0015\n(4.40)\nThus the transition probabilities for the population process {θk} are\nP\n\u0000θk+1 = θk + 1\nM [ej −ei] | θk, xk\n\u0001\n= 1\nM Pij(xk, θk)\nNote that {θk} is an\n\u0000M+L−1\nL−1\n\u0001\nstate Markov chain with state space\nΘ = {θ :\nL\nX\nl=1\nθ(l) = 1, θ(l) = n/L for some integer n ≥0}.\nNote also that Θ is a subset of the L −1 dimensional simplex denoted as Π(L).\nWe are interested in modeling the evolution of the population process {θk} in\n(4.40) when the number of agents M is large.\nTHEOREM 4.5.1\nFor a population size of M agents, where each agent has L possible\nstates, the population distribution process θk evolves in Θ ⊂Π(L) as\nθk+1 = θk + 1\nM H(xk, θk) + νk,\nθ0 ∈Θ,\n(4.41)\nwhere H(xk, θk) =\nL\nX\ni=1\nL\nX\nj=1\n(ej −ei)Pij(xk, θk).\nHere νk is an L dimensional ﬁnite state martingale increment process with ∥νk∥2 ≤\nΓ/M for some positive constant Γ.\n4.5.2\nMean Field Dynamics\nThe main result below shows that for large M, the population process θk in\n(4.41) converges to a deterministic difference equation (or equivalently, an ODE)\ncalled the mean ﬁeld dynamics. Thus the ODE method serves the purpose of\nconstructing a tractable model for the population dynamics.\nTHEOREM 4.5.2 (Mean Field Dynamics)\nConsider the deterministic mean ﬁeld\ndynamics process with state ¯θk ∈Π(L) (the L −1 dimensional unit simplex):\n¯θk+1 = ¯θk + 1\nM H(xk, ¯θk),\n¯θ0 = θ0.\n(4.42)\nAssume that H(θ, x) is Lipschitz continuous4 in θ. Then for a time horizon of N points,\n4For any α, β ∈Π(L), ∥H(x, α) −H(x, β)∥∞≤λ∥α −β∥∞for some positive constant λ.\n4.6 Complements and Sources\n49\nthe deviation between the mean ﬁeld dynamics ¯θk in (4.42) and actual population dis-\ntribution θk in (4.41) satisﬁes\nP\n\b\nmax\n0≤k≤N\n\r\r¯θk −θk\n\r\r\n∞≥ǫ\n\t\n≤C1 exp(−C2ǫ2M)\n(4.43)\nproviding N = O(M).\nAs an example, [38] uses the above formulation for the dynamics of informa-\ntion diffusion in a social network.\n4.6\nComplements and Sources\nThe ODE analysis for stochastic approximation algorithms was pioneered by\nLjung (see [49, 50]) and subsequently by Kushner and co-workers [46, 48]. In\nthis chapter we have only scratched the surface of this remarkably powerful\nanalysis tool. Apart from the books listed above, [12, 62, 18] are also excel-\nlent references for analysis of such algorithms. The papers [9, 10] illustrate the\npower of ODE method (and generalizations to differential inclusions) for ana-\nlyzing the dynamics of game-theoretic learning.\n§4.3 uses the LMS algorithm for tracking parameters that jump infrequent but\nby possibly large amounts. In most traditional analyses, the parameter changes\nby small amounts over small intervals of time. As mentioned in §4.3.3, one\ncan also analyze the tracking capability for Markov chains that jump by large\namounts on short intervals of time [69, 65]. In such cases, stochastic averaging\nleads to a Markov modulated ODE (instead of a deterministic ODE).\nIt would be remiss of us not to mention the substantial literature in the analy-\nsis of adaptive ﬁltering algorithms [59, 27]. The proof of Theorem 4.3.1 uses per-\nturbed Lyapunov functions. Solo [62] was inﬂuential in developing discounted\nperturbed Lyapunov function methods for analysis of adaptive ﬁltering algo-\nrithms. The mean ﬁeld dynamics proof of Theorem 4.5.2 is based on [11] and\nuses Azuma-Hoeffding’s inequality. It requires far less mathematical machin-\nery. The monograph [45] contains several deep results in Markov chain approx-\nimations of population processes and dynamics.\nSurvey papers in discrete stochastic optimization include [16]. There has also\nbeen much recent work in using the ODE analysis for ant-colony optimization\nalgorithms and the use of ant-colony optimization in Q-learning [3].\nWe have omitted several important aspects of stochastic approximation al-\ngorithms. One particular intriguing result is Polyak’s iterate averaging [57]. By\nchoosing a larger scalar step size together with an averaging step, it can be\nshown [48], that one can achieve the asymptotic convergence rate of a matrix\nstep size. This is also explored in the context of LMS algorithms in [68, 66, 67].\n50\nStochastic Approximation Algorithms: Examples\n4.6.1\nConsensus Stochastic Approximation Algorithms\nThere has been much recent work on diffusion and consensus stochastic approx-\nimation algorithms, where multiple stochastic approximation algorithms com-\nmunicate with each other over a graph. This area has been pioneered by Sayed\n(see [60] and references therein) and shows remarkable potential in a variety of\ndistributed processing applications.\nIn this section we brieﬂy analyze the consensus stochastic approximation al-\ngorithm. The consensus stochastic approximation algorithm is of the form\nθk+1 = A θk + ǫH(θk, xk),\n(4.44)\nwhere A is a symmetric positive deﬁnite stochastic matrix. For A = I, (4.44)\nbecomes the standard stochastic approximation algorithm.\nOne can analyze the consensus algorithm as follows. Deﬁne the matrix Q\nsuch that A = exp(Qǫ) where exp(·) denotes the matrix exponential. So Q is\nproportional to the matrix logarithm of A. (Since A is positive deﬁnite, the real-\nvalued matrix logarithm always exists.) Indeed Q is a generator matrix with\nQii < 0 and Q1 = 0. Then as ǫ becomes sufﬁciently small (recall the ODE\nanalysis applies for the interpolated process with ǫ →0) since exp(Qǫ) ≈I +\nǫQ + o(ǫ), one can express (4.44) as\nθk+1 = θk + ǫ\n\u0000Qθk + H(θk, xk)\n\u0001\n.\n(4.45)\nTherefore, the consensus ODE associated with (4.44) is\ndθ\ndt = Qθ + Eπθ{H(θ, x)}.\n(4.46)\nTypically Q is chosen such that at the optimum value θ∗, Qθ∗= 0 implying that\nthe consensus ODE and original ODE have identical attractors.\nTo summarize, despite at ﬁrst sight looking different to a standard stochas-\ntic approximation algorithm, the consensus stochastic approximation algorithm\n(4.44) in fact is equivalent to a standard stochastic approximation algorithm\n(4.45) by taking the matrix logarithm of the consensus matrix A. [54, 53] de-\nscribe such algorithms for tracking the equilibria of time varying games. One\ncan also consider consensus in the space of distributions and tracking such a\nconsensus distribution as described in [41]. The variance of the consensus LMS\nusing the diffusion approximation is analyzed in [34].\nReferences\n[1] F. Vazquez Abad and V. Krishnamurthy. Constrained stochastic approximation al-\ngorithms for adaptive control of constrained Markov decision processes. In 42nd\nIEEE Confernce on Decision and Control, pages 2823–2828, 2003.\n[2] J. Abounadi, D. P. Bertsekas, and V. Borkar.\nLearning algorithms for Markov\ndecision processes with average cost. SIAM Journal on Control and Optimization,\n40(3):681–698, 2001.\n[3] M. Dorigo amd M. Gambardella. Ant-q: A reinforcement learning approach to the\ntraveling salesman problem. In Proceedings of 12th International Confeference on Ma-\nchine Learning, pages 252–260, 2014.\n[4] S. Andradottir. A global search method for discrete stochastic optimization. SIAM\nJournal on Optimization, 6(2):513–530, May 1996.\n[5] S. Andradottir. Accelerating the convergence of random search methods for dis-\ncrete stochastic optimization. ACM Transactions on Modelling and Computer Simula-\ntion, 9(4):349–380, Oct. 1999.\n[6] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed\nbandit problem. Machine Learning, 47(2-3):235–256, 2002.\n[7] P. Bartlett and J. Baxter. Estimation and approximation bounds for gradient-based\nreinforcement learning. J. Comput. Syst. Sci., 64(1):133–150, 2002.\n[8] M. Benaim and M. Faure. Consistency of vanishingly smooth ﬁctitious play. Math.\nOper. Res., 38(3):437–450, Aug. 2013.\n[9] M. Benaim, J. Hofbauer, and S. Sorin. Stochastic approximations and differential\ninclusions. SIAM Journal on Control and Optimization, 44(1):328–348, 2005.\n[10] M. Benaim, J. Hofbauer, and S. Sorin. Stochastic approximations and differential\ninclusions, Part II: Applications. Mathematics of Operations Research, 31(3):673–695,\n2006.\n[11] M. Benaim and J. Weibull. Deterministic approximation of stochastic evolution in\ngames. Econometrica, 71(3):873–903, 2003.\n[12] A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Ap-\nproximations, volume 22 of Applications of Mathematics. Springer-Verlag, 1990.\n[13] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA., 2000.\n[14] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc,\nBelmont, MA., 1996.\n[15] D. P. Bertsekas and H. Yu. Q-learning and enhanced policy iteration in discounted\ndynamic programming. Mathematics of Operations Research, 37(1):66–94, 2012.\n52\nReferences\n[16] L. Bianchi, M. Dorigo, L. Gambardella, and W. Gutjahr. A survey on metaheuris-\ntics for stochastic combinatorial optimization. Natural Computing: An International\nJournal, 8(2):239–287, 2009.\n[17] P. Billingsley. Convergence of Probability Measures. John Wiley, N.Y., 1968.\n[18] V. S. Borkar. Stochastic approximation. A Dynamical Systems Viewpoint. Cambridge\nUniversity Press, 2008.\n[19] O. Cappe, E. Moulines, and T. Ryden. Inference in Hidden Markov Models. Springer-\nVerlag, 2005.\n[20] C. Chamley. Rational herds: Economic Models of Social Learning. Cambridge Univer-\nsity Press, 2004.\n[21] I.B. Collings, V. Krishnamurthy, and J.B. Moore. On-line identiﬁcation of hidden\nMarkov models via recursive prediction error techniques. IEEE Transactions on Sig-\nnal Processing, 42(12):3535–3539, December 1994.\n[22] D. Djonin and V. Krishnamurthy. Q-learning algorithms for constrained Markov\ndecision processes with randomized monotone policies: Applications in transmis-\nsion control. IEEE Transactions on Signal Processing, 55(5):2170–2181, 2007.\n[23] S. N. Ethier and T. G. Kurtz. Markov Processes—Characterization and Convergence.\nWiley, 1986.\n[24] D. Fudenberg and D. K. Levine. The Theory of Learning in Games. MIT Press, 1998.\n[25] D. Fudenberg and D.K. Levine. Consistency and cautious ﬁctitious play. Journal of\nEconomic Dynamics and Control, 19(5-7):1065–1089, 1995.\n[26] M. Granovetter. Threshold models of collective behavior. American Journal of Soci-\nology, 83(6):1420–1443, May 1978.\n[27] S. Haykin. Adaptive Filter Theory. Information and System Sciences Series. Prentice\nHall, ﬁfth edition, 2013.\n[28] Y.-C. Ho and X.-R. Cao. Discrete Event Dynamic Systems and Perturbation Analysis.\nKluwer Academic, Boston, 1991.\n[29] J. Hofbauer and W. Sandholm. On the global convergence of stochastic ﬁctitious\nplay. Econometrica, 70(6):2265–2294, Nov. 2002.\n[30] I. Karatzas and S. Shreve. Brownian Motion and Stochastic Calculus. Springer, second\nedition, 1991.\n[31] V. Krishnamurthy. Bayesian sequential detection with phase-distributed change\ntime and nonlinear penalty – a lattice programming POMDP approach. IEEE Trans-\nactions on Information Theory, 57(3):7096–7124, Oct. 2011.\n[32] V. Krishnamurthy.\nQuickest detection POMDPs with social learning: Interac-\ntion of local and global decision makers. IEEE Transactions on Information Theory,\n58(8):5563–5587, 2012.\n[33] V. Krishnamurthy. How to schedule measurements of a noisy Markov chain in de-\ncision making? IEEE Transactions on Information Theory, 59(9):4440–4461, July 2013.\n[34] V. Krishnamurthy. Partially Observed Markov Decision Processes. From Filtering to Con-\ntrolled Sensing. Cambridge University Press, 2016.\n[35] V. Krishnamurthy and F. Vazquez Abad. Gradient based policy optimization of\nconstrained unichain Markov decision processes. In S. Cohen, D. Madan, and T. Siu,\neditors, Stochastic Processes, Finance and Control: A Festschrift in Honor of Robert J.\nElliott. World Scientiﬁc, 2012. http://arxiv.org/abs/1110.4946.\nReferences\n53\n[36] V. Krishnamurthy and D. Djonin. Structured threshold policies for dynamic sensor\nscheduling–a partially observed Markov decision process approach. IEEE Transac-\ntions on Signal Processing, 55(10):4938–4957, Oct. 2007.\n[37] V. Krishnamurthy and D.V. Djonin.\nOptimal threshold policies for multivariate\nPOMDPs in radar resource management. IEEE Transactions on Signal Processing,\n57(10), 2009.\n[38] V. Krishnamurthy, O. Namvar Gharehshiran, and M. Hamdi. Interactive sensing\nand decision making in social networks. Foundations and Trends® in Signal Process-\ning, 7(1-2):1–196, 2014.\n[39] V. Krishnamurthy and F. Vazquez Abad K. Martin. Implementation of gradient es-\ntimation to a constrained Markov decision problem. In IEEE Conference on Decision\nand Control, Maui, Hawaii, 2003.\n[40] V. Krishnamurthy and J.B. Moore. On-line estimation of hidden Markov model\nparameters based on the Kullback-Leibler information measure. IEEE Transactions\non Signal Processing, 41(8):2557–2573, August 1993.\n[41] V. Krishnamurthy, K. Topley, and G. Yin. Consensus formation in a two-time-scale\nMarkovian system. SIAM Journal Multiscale Modeling and Simulation, 7(4):1898–1927,\n2009.\n[42] V. Krishnamurthy and B. Wahlberg. POMDP multiarmed bandits – structural re-\nsults. Mathematics of Operations Research, 34(2):287–302, May 2009.\n[43] V. Krishnamurthy, X. Wang, and G. Yin. Spreading code optimization and adap-\ntation in CDMA via discrete stochastic approximation.\nIEEE Trans. Info Theory,\n50(9):1927–1949, Sept. 2004.\n[44] V. Krishnamurthy and G. Yin.\nRecursive algorithms for estimation of hidden\nMarkov models and autoregressive models with Markov regime. IEEE Transactions\non Information Theory, 48(2):458–476, February 2002.\n[45] T. G. Kurtz. Approximation of population processes, volume 36. SIAM, 1981.\n[46] H. J. Kushner. Approximation and Weak Convergence Methods for Random Processes,\nwith applications to Stochastic Systems Theory. MIT Press, Cambridge, MA, 1984.\n[47] H.J. Kushner and D.S. Clark. Stochastic Approximation Methods for Constrained and\nUnconstrained Systems. Springer-Verlag, 1978.\n[48] H.J. Kushner and G. Yin. Stochastic Approximation Algorithms and Recursive Algo-\nrithms and Applications. Springer-Verlag, 2nd edition, 2003.\n[49] L. Ljung. Analysis of recursive stochastic algorithms. IEEE Transactions on Auto.\nControl, AC-22(4):551–575, 1977.\n[50] L. Ljung and T. S¨oderstr¨om. Theory and practice of recursive identiﬁcation. MIT Press,\n1983.\n[51] D. L´opez-Pintado. Diffusion in complex social networks. Games and Economic Be-\nhavior, 62(2):573–590, 2008.\n[52] A. Misra, V. Krishnamurthy, and R. Schober.\nStochastic learning algorithms for\nadaptive modulation. In Signal Processing Advances in Wireless Communications, 2005\nIEEE 6th Workshop on, pages 756–760. IEEE, 2005.\n[53] O. Namvar, V. Krishnamurthy, and G. Yin. Distributed energy-aware diffusion least\nmean squares: Game-theoretic learning. IEEE Journal of Selected Topics in Signal Pro-\ncessing, 7(5):821, 2013.\n54\nReferences\n[54] O. Namvar, V. Krishnamurthy, and G. Yin. Distributed tracking of correlated equi-\nlibria in regime switching noncooperative games. IEEE Transactions on Automatic\nControl, 58(10):2435–2450, 2013.\n[55] O. Namvar, V. Krishnamurthy, and G. Yin. Adaptive search algorithms for dis-\ncrete stochastic optimization: A smooth best-response approach.\narXiv preprint\narXiv:1402.3354, 2014.\n[56] G. Pﬂug. Optimization of Stochastic Models: The Interface between Simulation and Opti-\nmization. Kluwer Academic Publishers, 1996.\n[57] B.T. Polyak and A.B. Juditsky. Acceleration of stochastic approximation by averag-\ning. SIAM Journal of Control and Optimization, 30(4):838–855, July 1992.\n[58] M. Puterman. Markov Decision Processes. John Wiley, 1994.\n[59] A. Sayed. Adaptive Filters. Wiley, 2008.\n[60] A. H. Sayed. Adaptation, learning, and optimization over networks. Foundations\nand Trends in Machine Learning, 7(4–5):311–801, 2014.\n[61] R. Simmons and S. Konig. Probabilistic navigation in partially observable environ-\nments. In Proceedings of 14th International Joint Conference on Artiﬁcial Intelligence,\npages 1080–1087, Montreal, Canada, 1995. Morgan Kaufman.\n[62] V. Solo and X. Kong. Adaptive Signal Processing Algorithms – Stability and Performance.\nPrentice Hall, N.J., 1995.\n[63] J. Spall. Introduction to Stochastic Search and Optimization. Wiley, 2003.\n[64] R. Sutton and A. Barto. Reinforcement learning: An introduction. MIT Press, 1998.\n[65] G. Yin, C. Ion, and V. Krishnamurthy. How does a stochastic optimization/approx-\nimation algorithm adapt to a randomly evolving optimum/root with jump Markov\nsample paths. Mathematical programming, 120(1):67–99, 2009.\n[66] G. Yin and V. Krishnamurthy. Least mean square algorithms with Markov regime\nswitching limit. IEEE Transactions on Automatic Control, 50(5):577–593, May 2005.\n[67] G. Yin and V. Krishnamurthy. LMS algorithms for tracking slow Markov chains\nwith applications to hidden Markov estimation and adaptive multiuser detection.\nIEEE Transactions on Information Theory, 51(7), July 2005.\n[68] G. Yin, V. Krishnamurthy, and C. Ion. Iterate averaging sign algorithms for adap-\ntive ﬁltering with applications to blind multiuser detection. IEEE Transactions on\nInformation Theory, 48(3):657–671, March 2003.\n[69] G. Yin, V. Krishnamurthy, and C. Ion.\nRegime switching stochastic approxima-\ntion algorithms with application to adaptive discrete stochastic optimization. SIAM\nJournal on Optimization, 14(4):117–1215, 2004.\n[70] G. Yin and Q. Zhang. Discrete-time Markov chains: two-time-scale methods and applica-\ntions, volume 55. Springer, 2006.\n",
  "categories": [
    "math.OC"
  ],
  "published": "2015-12-23",
  "updated": "2015-12-23"
}