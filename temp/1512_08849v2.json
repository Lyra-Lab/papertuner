{
  "id": "http://arxiv.org/abs/1512.08849v2",
  "title": "Learning Natural Language Inference with LSTM",
  "authors": [
    "Shuohang Wang",
    "Jing Jiang"
  ],
  "abstract": "Natural language inference (NLI) is a fundamentally important task in natural\nlanguage processing that has many applications. The recently released Stanford\nNatural Language Inference (SNLI) corpus has made it possible to develop and\nevaluate learning-centered methods such as deep neural networks for natural\nlanguage inference (NLI). In this paper, we propose a special long short-term\nmemory (LSTM) architecture for NLI. Our model builds on top of a recently\nproposed neural attention model for NLI but is based on a significantly\ndifferent idea. Instead of deriving sentence embeddings for the premise and the\nhypothesis to be used for classification, our solution uses a match-LSTM to\nperform word-by-word matching of the hypothesis with the premise. This LSTM is\nable to place more emphasis on important word-level matching results. In\nparticular, we observe that this LSTM remembers important mismatches that are\ncritical for predicting the contradiction or the neutral relationship label. On\nthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the\nstate of the art.",
  "text": "Learning Natural Language Inference with LSTM\nShuohang Wang\nSchool of Information Systems\nSingapore Management University\nshwang.2014@phdis.smu.edu.sg\nJing Jiang\nSchool of Information Systems\nSingapore Management University\njingjiang@smu.edu.sg\nAbstract\nNatural language inference (NLI) is a funda-\nmentally important task in natural language\nprocessing that has many applications. The\nrecently released Stanford Natural Language\nInference (SNLI) corpus has made it possi-\nble to develop and evaluate learning-centered\nmethods such as deep neural networks for nat-\nural language inference (NLI). In this paper,\nwe propose a special long short-term mem-\nory (LSTM) architecture for NLI. Our model\nbuilds on top of a recently proposed neural at-\ntention model for NLI but is based on a sig-\nniﬁcantly different idea. Instead of deriving\nsentence embeddings for the premise and the\nhypothesis to be used for classiﬁcation, our so-\nlution uses a match-LSTM to perform word-\nby-word matching of the hypothesis with the\npremise. This LSTM is able to place more\nemphasis on important word-level matching\nresults.\nIn particular, we observe that this\nLSTM remembers important mismatches that\nare critical for predicting the contradiction or\nthe neutral relationship label. On the SNLI\ncorpus, our model achieves an accuracy of\n86.1%, outperforming the state of the art.\n1\nIntroduction\nNatural language inference (NLI) is the problem of\ndetermining whether from a premise sentence P one\ncan infer another hypothesis sentence H (MacCart-\nney, 2009). NLI is a fundamentally important prob-\nlem that has applications in many tasks including\nquestion answering, semantic search and automatic\ntext summarization.\nThere has been much inter-\nest in NLI in the past decade, especially surround-\ning the PASCAL Recognizing Textual Entailment\n(RTE) Challenge (Dagan et al., 2005). Existing so-\nlutions to NLI range from shallow approaches based\non lexical similarities (Glickman et al., 2005) to ad-\nvanced methods that consider syntax (Mehdad et al.,\n2009), perform explicit sentence alignment (Mac-\nCartney et al., 2008) or use formal logic (Clark and\nHarrison, 2009).\nRecently, Bowman et al. (2015) released the Stan-\nford Natural Language Inference (SNLI) corpus for\nthe purpose of encouraging more learning-centered\napproaches to NLI. This corpus contains around\n570K sentence pairs with three labels: entailment,\ncontradiction and neutral. The size of the corpus\nmakes it now feasible to train deep neural network\nmodels, which typically require a large amount of\ntraining data. Bowman et al. (2015) tested a straight-\nforward architecture of deep neural networks for\nNLI. In their architecture, the premise and the hy-\npothesis are each represented by a sentence embed-\nding vector.\nThe two vectors are then fed into a\nmulti-layer neural network to train a classiﬁer. Bow-\nman et al. (2015) achieved an accuracy of 77.6%\nwhen long short-term memory (LSTM) networks\nwere used to obtain the sentence embeddings.\nA more recent work by Rockt¨aschel et al. (2016)\nimproved the performance by applying a neural at-\ntention model.\nWhile their basic architecture is\nstill based on sentence embeddings for the premise\nand the hypothesis, a key difference is that the em-\nbedding of the premise takes into consideration the\nalignment between the premise and the hypothesis.\nThis so-called attention-weighted representation of\nthe premise was shown to help push the accuracy to\narXiv:1512.08849v2  [cs.CL]  10 Nov 2016\n83.5% on the SNLI corpus.\nA limitation of the aforementioned two models is\nthat they reduce both the premise and the hypoth-\nesis to a single embedding vector before matching\nthem; i.e., in the end, they use two embedding vec-\ntors to perform sentence-level matching. However,\nnot all word or phrase-level matching results are\nequally important. For example, the matching be-\ntween stop words in the two sentences is not likely\nto contribute much to the ﬁnal prediction. Also, for\na hypothesis to contradict a premise, a single word\nor phrase-level mismatch (e.g., a mismatch of the\nsubjects of the two sentences) may be sufﬁcient and\nother matching results are less important, but this in-\ntuition is hard to be captured if we directly match\ntwo sentence embeddings.\nIn this paper, we propose a new LSTM-based ar-\nchitecture for learning natural language inference.\nDifferent from previous models, our prediction is\nnot based on whole sentence embeddings of the\npremise and the hypothesis.\nInstead, we use an\nLSTM to perform word-by-word matching of the\nhypothesis with the premise. Our LSTM sequen-\ntially processes the hypothesis, and at each posi-\ntion, it tries to match the current word in the hy-\npothesis with an attention-weighted representation\nof the premise.\nMatching results that are critical\nfor the ﬁnal prediction will be “remembered” by the\nLSTM while less important matching results will be\n“forgotten.” We refer to this architecture a match-\nLSTM, or mLSTM for short.\nExperiments show that our mLSTM model\nachieves an accuracy of 86.1% on the SNLI cor-\npus, outperforming the state of the art. Furthermore,\nthrough further analyses of the learned parameters,\nwe show that the mLSTM architecture can indeed\npick up the more important word-level matching re-\nsults that need to be remembered for the ﬁnal pre-\ndiction. In particular, we observe that good word-\nlevel matching results are generally “forgotten” but\nimportant mismatches, which often indicate a con-\ntradiction or a neutral relationship, tend to be “re-\nmembered.”\nOur code is available online1.\n1https://github.com/shuohangwang/\nSeqMatchSeq\n2\nModel\nIn\nthis\nsection,\nwe\nﬁrst\nreview\nLSTM.\nWe\nthen review the word-by-word attention model by\nRockt¨aschel et al. (2016), which is their best per-\nforming model. Finally we present our mLSTM ar-\nchitecture for natural language inference.\n2.1\nBackground\nLSTM: Let us ﬁrst brieﬂy review LSTM (Hochre-\niter and Schmidhuber, 1997).\nLSTM is a special\nform of recurrent neural networks (RNNs), which\nprocess sequence data. LSTM uses a few gate vec-\ntors at each position to control the passing of in-\nformation along the sequence and thus improves\nthe modeling of long-range dependencies.\nWhile\nthere are different variations of LSTMs, here we\npresent the one adopted by Rockt¨aschel et al. (2016).\nSpeciﬁcally, let us use X = (x1, x2, . . . , xN) to de-\nnote an input sequence, where xk ∈Rl (1 ≤k ≤\nN). At each position k, there is a set of internal vec-\ntors, including an input gate ik, a forget gate fk, an\noutput gate ok and a memory cell ck. All these vec-\ntors are used together to generate a d-dimensional\nhidden state hk as follows:\nik\n=\nσ(Wixk + Vihk−1 + bi),\nfk\n=\nσ(Wfxk + Vfhk−1 + bf),\nok\n=\nσ(Woxk + Vohk−1 + bo),\nck\n=\nfk ⊙ck−1 + ik ⊙tanh(Wcxk + Vchk−1 + bc),\nhk\n=\nok ⊙tanh(ck),\n(1)\nwhere σ is the sigmoid function, ⊙is the element-\nwise multiplication of two vectors, and all W* ∈\nRd×l,V* ∈Rd×d and b* ∈Rd are weight matrices\nand vectors to be learned.\nNeural Attention Model:\nFor the natural lan-\nguage inference task, we have two sentences Xs =\n(xs\n1, xs\n2, . . . , xs\nM) and Xt\n=\n(xt\n1, xt\n2, . . . , xt\nN),\nwhere Xs is the premise and Xt is the hypothesis.\nHere each x is an embedding vector of the corre-\nsponding word. The goal is to predict a label y that\nindicates the relationship between Xs and Xt. In this\npaper, we assume y is one of entailment, contradic-\ntion and neutral.\nRockt¨aschel et al. (2016) ﬁrst used two LSTMs\nto process the premise and the hypothesis, respec-\ntively, but initialized the second LSTM (for the hy-\npothesis) with the last cell state of the ﬁrst LSTM\n(for the premise). Let us use hs\nj and ht\nk to denote\nthe resulting hidden states corresponding to xs\nj and\nxt\nk, respectively. The main idea of the word-by-word\nattention model by Rockt¨aschel et al. (2016) is to in-\ntroduce a series of attention-weighted combinations\nof the hidden states of the premise, where each com-\nbination is for a particular word in the hypothesis.\nLet us use ak to denote such an attention vector for\nword xt\nk in the hypothesis. Speciﬁcally, ak is de-\nﬁned as follows2:\nak\n=\nM\nX\nj=1\nαkjhs\nj,\n(2)\nwhere αkj is an attention weight that encodes the\ndegree to which xt\nk in the hypothesis is aligned with\nxs\nj in the premise. The attention weight αkj is gen-\nerated in the following way:\nαkj\n=\nexp(ekj)\nP\nj′ exp(ekj′),\n(3)\nwhere\nekj = we · tanh(Wshs\nj + Wtht\nk + Waha\nk−1).\n(4)\nHere · is the dot-product between two vectors, the\nvector we ∈Rd and all matrices W* ∈Rd×d con-\ntain weights to be learned, and ha\nk−1 is another hid-\nden state which we will explain below.\nThe attention-weighted premise ak essentially\ntries to model the relevant parts in the premise with\nrespect to xt\nk, i.e., the kth word in the hypothe-\nsis. Rockt¨aschel et al. (2016) further built an RNN\nmodel over {ak}N\nk=1 by deﬁning the following hid-\nden states:\nha\nk\n=\nak + tanh(Vaha\nk−1),\n(5)\nwhere Va ∈Rd×d is a weight matrix to be learned.\nWe can see that the last ha\nN aggregates all the pre-\nvious ak and can be seen as an attention-weighted\n2We\npresent\nthe\nword-by-word\nattention\nmodel\nby\nRockt¨aschel et al. (2016) in a different way but the underlying\nmodel is the same. Our ha\nk is their rt, our Hs (all of hs\nj) is their\nY, our ht\nk is their ht, and our αk is their αt. Our presentation\nis close to the one by Bahdanau et al. (2015), with our attention\nvectors a corresponding to the context vectors c in their paper.\nrepresentation of the whole premise. Rockt¨aschel et\nal. (2016) then used this ha\nN, which represents the\nwhole premise, together with ht\nN, which can be ap-\nproximately regarded as an aggregated representa-\ntion of the hypothesis3, to predict the label y.\n2.2\nOur Model\nAlthough the neural attention model by Rockt¨aschel\net al. (2016) achieved better results than Bowman\net al. (2015), we see two limitations.\nFirst, the\nmodel still uses a single vector representation of the\npremise, namely ha\nN, to match the entire hypothe-\nsis. We speculate that if we instead use each of the\nattention-weighted representations of the premise\nfor matching, i.e., use ak at position k to match\nthe hidden state ht\nk of the hypothesis while we\ngo through the hypothesis, we could achieve better\nmatching results. This can be done using an RNN\nwhich at each position takes in both ak and ht\nk as its\ninput and determines how well the overall matching\nof the two sentences is up to the current position. In\nthe end the RNN will produce a single vector repre-\nsenting the matching of the two entire sentences.\nThe second limitation is that the model by\nRockt¨aschel et al. (2016) does not explicitly allow\nus to place more emphasis on the more important\nmatching results between the premise and the hy-\npothesis and down-weight the less critical ones. For\nexample, matching of stop words is presumably less\nimportant than matching of content words. Also,\nsome matching results may be particularly critical\nfor making the ﬁnal prediction and thus should be\nremembered.\nFor example, consider the premise\n“A dog jumping for a Frisbee in the snow.”\nand\nthe hypothesis “A cat washes his face and whiskers\nwith his front paw.”\nWhen we sequentially pro-\ncess the hypothesis, once we see that the subject\nof the hypothesis cat does not match the subject of\nthe premise dog, we have a high probability to be-\nlieve that there is a contradiction. So this mismatch\nshould be remembered.\nBased on the two observations above, we propose\nto use an LSTM to sequentially match the two sen-\ntences. At each position the LSTM takes in both ak\n3Strictly speaking, in the model by Rockt¨aschel et al. (2016),\nht\nN encodes both the premise and the hypothesis because the\ntwo sentences are chained. But ht\nN places a higher emphasis on\nthe hypothesis given the nature of RNNs.\nFigure 1: The top ﬁgure depicts the model by Rockt¨aschel et al.\n(2016) and the bottom ﬁgure depicts our model. Here Hs rep-\nresents all the hidden states hs\nj. Note that in the top model each\nha\nk represents a weighted version of the premise only, while\nin our model, each hm\nk represents the matching between the\npremise and the hypothesis up to position k.\nand ht\nk as its input. Figure 1 gives an overview of\nour model in contrast to the model by Rockt¨aschel\net al. (2016).\nSpeciﬁcally, our model works as follows. First,\nsimilar to Rockt¨aschel et al. (2016), we process the\npremise and the hypothesis using two LSTMs, but\nwe do not feed the last cell state of the premise to\nthe LSTM of the hypothesis. This is because we do\nnot need the LSTM for the hypothesis to encode any\nknowledge about the premise but we will match the\npremise with the hypothesis using the hidden states\nof the two LSTMs. Again, we use hs\nj and ht\nk to\nrepresent these hidden states.\nNext, we generate the attention vectors ak simi-\nlarly to Eqn (2). However, Eqn (4) will be replaced\nby the following equation:\nekj = we · tanh(Wshs\nj + Wtht\nk + Wmhm\nk−1).\n(6)\nThe only difference here is that we use a hidden state\nhm instead of ha, and the way we deﬁne hm is very\ndifferent from the deﬁnition of ha.\nOur hm\nk is the hidden state at position k generated\nfrom our mLSTM. This LSTM models the match-\ning between the premise and the hypothesis. Im-\nportant matching results will be “remembered” by\nthe LSTM while non-essential ones will be “forgot-\nten.” We use the concatenation of ak, which is the\nattention-weighted version of the premise for the kth\nword in the hypothesis, and ht\nk, the hidden state for\nthe kth word itself, as input to the mLSTM.\nSpeciﬁcally, let us deﬁne\nmk\n=\n\u0014ak\nht\nk\n\u0015\n.\n(7)\nWe then build the mLSTM as follows:\nim\nk\n=\nσ(Wmimk + Vmihm\nk−1 + bmi),\nf m\nk\n=\nσ(Wmfmk + Vmfhm\nk−1 + bmf),\nom\nk\n=\nσ(Wmomk + Vmohm\nk−1 + bmo),\ncm\nk\n=\nf m\nk ⊙cm\nk−1 + im\nk ⊙tanh(Wmcmk + Vmchm\nk−1\n+bmc),\nhm\nk\n=\nom\nk ⊙tanh(cm\nk ).\n(8)\nWith this mLSTM, ﬁnally we use only hm\nN, the last\nhidden state, to predict the label y.\n2.3\nImplementation Details\nBesides the difference of the LSTM architecture, we\nalso introduce a few other changes from the model\nby Rockt¨aschel et al. (2016). First, we insert a spe-\ncial word NULL to the premise, and we allow words\nin the hypothesis to be aligned with this NULL. This\nis inspired by common practice in machine transla-\ntion. Speciﬁcally, we introduce a vector hs\n0, which\nis ﬁxed to be a vector of 0s of dimension d. This hs\n0\nrepresents NULL and is used with other hs\nj to derive\nthe attention vectors {ak}N\nk=1.\nSecond, we use word embeddings trained from\nGloVe (Pennington et al., 2014) instead of word2vec\nvectors. The main reason is that GloVe covers more\nwords in the SNLI corpus than word2vec4.\nThird, for words which do not have pre-trained\nword embeddings, we take the average of the em-\nbeddings of all the words (in GloVe) surrounding the\nunseen word within a window size of 9 (4 on the left\nand 4 on the right) as an approximation of the em-\nbedding of this unseen word. Then we do not update\n4The SNLI corpus contains 37K unique tokens. Around\n12.1K of them cannot be found in word2vec but only around\n4.1K of them cannot be found in GloVe.\nany word embedding when learning our model. Al-\nthough this is a very crude approximation, it reduces\nthe number of parameters we need to update, and as\nit turns out, we can still achieve better performance\nthan Rockt¨aschel et al. (2016).\n3\nExperiments\n3.1\nExperiment Settings\nData: We use the SNLI corpus to test the effective-\nness of our model. The original data set contains\n570,152 sentence pairs, each labeled with one of the\nfollowing relationships: entailment, contradiction,\nneutral and –, where – indicates a lack of consensus\nfrom the human annotators. We discard the sentence\npairs labeled with – and keep the remaining ones for\nour experiments. In the end, we have 549,367 pairs\nfor training, 9,842 pairs for development and 9,824\npairs for testing. This follows the same data partition\nused by Bowman et al. (2015) in their experiments.\nWe perform three-class classiﬁcation and use accu-\nracy as our evaluation metric.\nParameters: We use the Adam method (Kingma\nand Ba, 2014) with hyperparameters β1 set to 0.9\nand β2 set to 0.999 for optimization.\nThe initial\nlearning rate is set to be 0.001 with a decay ratio\nof 0.95 for each iteration. The batch size is set to\nbe 30. We experiment with d = 150 and d = 300\nwhere d is the dimension of all the hidden states.\nMethods for comparison:\nWe mainly want to\ncompare our model with the word-by-word atten-\ntion model by Rockt¨aschel et al. (2016) because\nthis model achieved the state-of-the-art performance\non the SNLI corpus.\nTo ensure fair comparison,\nbesides comparing with the accuracy reported by\nRockt¨aschel et al. (2016), we also re-implemented\ntheir model and report the performance of our im-\nplementation. We also consider a few variations of\nour model. Speciﬁcally, the following models are\nimplemented and tested in our experiments:\n• Word-by-word attention (d = 150): This is\nour implementation of the word-by-word at-\ntention model by Rockt¨aschel et al. (2016),\nwhere we set the dimension of the hidden states\nto 150.\nThe differences between our imple-\nmentation and the original implementation by\nRockt¨aschel et al. (2016) are the following: (1)\nWe also add a NULL token to the premise for\nground truth\nprediction\nN\nE\nC\nN\n2628\n286\n255\nE\n340\n3005\n159\nC\n250\n77\n2823\nTable 2: The confusion matrix of the results by mLSTM with\nd = 300. N, E and C correspond to neutral, entailment and\ncontradiction, respectively.\nmatching. (2) We do not feed the last cell state\nof the LSTM for the premise to the LSTM for\nthe hypothesis, to keep it consistent with the\nimplementation of our model.\n(3) For word\nrepresentation, we also use the GloVe word\nembeddings and we do not update the word\nembeddings. For unseen words, we adopt the\nsame strategy as described in Section 2.3.\n• mLSTM (d = 150): This is our mLSTM model\nwith d set to 150.\n• mLSTM with bi-LSTM sentence modeling\n(d = 150): This is the same as the model\nabove except that when we derive the hidden\nstates hs\nj and ht\nk of the two sentences, we use\nbi-LSTMs (Graves, 2012) instead of LSTMs.\nWe implement this model to see whether bi-\nLSTMs allow us to better align the sentences.\n• mLSTM (d = 300): This is our mLSTM model\nwith d set to 300.\n• mLSTM with word embedding (d = 300): This\nis the same as the model above except that we\ndirectly use the word embedding vectors xs\nj and\nxt\nk instead of the hidden states hs\nj and ht\nk in our\nmodel. In this case, each attention vector ak is\na weighted sum of {xs\nj}M\nj=1. We experiment\nwith this setting because we hypothesize that\nthe effectiveness of our model is largely related\nto the mLSTM architecture rather than the use\nof LSTMs to process the original sentences.\n3.2\nMain Results\nTable 1 compares the performance of the various\nmodels we tested together with some previously re-\nported results.\nWe have the following observations: (1) First of\nall, we can see that when we set d to 300, our model\nachieves an accuracy of 86.1% on the test data,\nwhich to the best of our knowledge is the highest on\nModel\nd\n|θ|W+M\n|θ|M\nTrain\nDev\nTest\nLSTM [Bowman et al. (2015)]\n100\n10M\n221K\n84.4\n-\n77.6\nClassiﬁer [Bowman et al. (2015)]\n-\n-\n-\n99.7\n-\n78.2\nLSTM shared [Rockt¨aschel et al. (2016)]\n159\n3.9M\n252K\n84.4\n83.0\n81.4\nWord-by-word attention [Rockt¨aschel et al. (2016)]\n100\n3.9M\n252K\n85.3\n83.7\n83.5\nWord-by-word attention (our implementation)\n150\n340K\n340K\n85.5\n83.3\n82.6\nmLSTM\n150\n544K\n544K\n91.0\n86.2\n85.7\nmLSTM with bi-LSTM sentence modeling\n150\n1.4M\n1.4M\n91.3\n86.6\n86.0\nmLSTM\n300\n1.9M\n1.9M\n92.0\n86.9\n86.1\nmLSTM with word embedding\n300\n1.3M\n1.3M\n88.6\n85.4\n85.3\nTable 1: Experiment results in terms of accuracy. d is the dimension of the hidden states. |θ|W+M is the total number of parameters\nand |θ|M is the number of parameters excluding the word embeddings. Note that the ﬁve models in the last section were implemented\nby us while the other results were taken directly from previous papers. Note also that for the ﬁve models in the last section, we do\nnot update word embeddings so |θ|W+M is the same as |θ|M. The three columns on the right are the accuracies of the trained models\non the training data, the development data and the test data, respectively.\nID\nsentence\nlabel\nPremise\nA dog jumping for a Frisbee in the snow.\nExample 1\nAn animal is outside in the cold weather, playing with a plastic toy.\nentailment\nHypothesis\nExample 2\nA cat washed his face and whiskers with his front paw.\ncontradiction\nExample 3\nA pet is enjoying a game of fetch with his owner.\nneutral\nTable 3: Three examples of sentence pairs with different relationship labels. The second hypothesis is a contradiction because it\nmentions a completely different event. The third hypothesis is neutral to the premise because the phrase “with his owner” cannot\nbe inferred from the premise.\nthis data set. (2) If we compare our mLSTM model\nwith our implementation of the word-by-word atten-\ntion model by Rockt¨aschel et al. (2016) under the\nsame setting with d = 150, we can see that our per-\nformance on the test data (85.7%) is higher than that\nof their model (82.6%). We also tested statistical\nsigniﬁcance and found the improvement to be statis-\ntically signiﬁcant at the 0.001 level. (3) The perfor-\nmance of mLSTM with bi-LSTM sentence modeling\ncompared with the model with standard LSTM sen-\ntence modeling when d is set to 150 shows that us-\ning bi-LSTM to process the original sentences helps\n(86.0% vs.\n85.7% on the test data), but the dif-\nference is small and the complexity of bi-LSTM is\nmuch higher than LSTM. Therefore when we in-\ncreased d to 300 we did not experiment with bi-\nLSTM sentence modeling. (4) Interestingly, when\nwe experimented with the mLSTM model using\nthe pre-trained word embeddings instead of LSTM-\ngenerated hidden states as initial representations of\nthe premise and the hypothesis, we were able to\nachieve an accuracy of 85.3% on the test data, which\nis still better than previously reported state of the art.\nThis suggests that the mLSTM architecture coupled\nwith the attention model works well, regardless of\nwhether or not we use LSTM to process the original\nsentences.\nBecause the NLI task is a three-way classiﬁca-\ntion problem, to better understand the errors, we also\nshow the confusion matrix of the results obtained by\nour mLSTM model with d = 300 in Table 2. We\ncan see that there is more confusion between neu-\ntral and entailment and between neutral and contra-\ndiction than between entailment and contradiction.\nThis shows that neutral is relatively hard to capture.\n3.3\nFurther Analyses\nTo obtain a better understanding of how our pro-\nposed model actually performs the matching be-\ntween a premise and a hypothesis, we further con-\nduct the following analyses. First, we look at the\nlearned word-by-word alignment weights αkj to\ncheck whether the soft alignment makes sense. This\nis the same as what was done by Rockt¨aschel et al.\nFigure 2: The alignment weights and the gate vectors of the three examples.\n(2016). We then look at the values of the various\ngate vectors of the mLSTM. By looking at these val-\nues, we aim to check (1) whether the model is able\nto differentiate between more important and less im-\nportant word-level matching results, and (2) whether\nthe model forgets certain matching results and re-\nmembers certain other ones.\nTo conduct the analyses, we choose three ex-\namples and display the various learned parameter\nvalues. These three sentence pairs share the same\npremise but have different hypotheses and different\nrelationship labels. They are given in Table 3. The\nvalues of the alignment weights and the gate vectors\nare plotted in Figure 2.\nBesides using the three examples, we will also\ngive some overall statistics of the parameter values\nto conﬁrm our observations with the three examples.\nWord Alignment\nFirst, let us look at the top-most plots of Fig-\nure 2. These plots show the alignment weights αkj\nbetween the hypothesis and the premise, where a\ndarker color corresponds to a larger value of αkj.\nRecall that αkj is the degree to which the kth word\nin the hypothesis is aligned with the jth word in the\npremise. Also recall that the weights αkj are con-\nﬁgured such that for the same k all the αkj add up\nto 1. This means the weights in the same row in\nthese plots add up to 1. From the three plots we can\nsee that the alignment weights generally make sense.\nFor example, in Example 1, “animal” is strongly\naligned with “dog” and “toy” aligned with “Frisbee.”\nThe phrase “cold weather” is aligned with “snow.”\nIn Example 3, we also see that “pet” is strongly\naligned with “dog” and “game” aligned with “Fris-\nbee.”\nIn Example 2, “cat” is strongly aligned with “dog”\nand “washes” is aligned with “jumping.” It may ap-\npear that these matching results are wrong. How-\never, “dog” is likely the best match for “cat” among\nall the words in the premise, and as we will show\nlater, this match between “cat” and “dog” is actu-\nally a strong indication of a contradiction between\nthe two sentences. The same explanation applies to\nthe match between “washes” and “jumping.”\nWe also observe that some words are aligned\nwith the NULL token we inserted.\nFor example,\nthe word “is” in the hypothesis in Example 1 does\nnot correspond to any word in the premise and is\ntherefore aligned with NULL. The words “face” and\n“whiskers” in Example 2 and “owner” in Example 3\nare also aligned with NULL. Intuitively, if some im-\nportant content words in the hypothesis are aligned\nwith NULL, it is more likely that the relationship la-\nbel is either contradiction or neutral.\nValues of Gate Vectors\nNext, let us look at the values of the learned gate\nvectors of our mLSTM for the three examples. We\nshow these values under the setting where d is set to\n150. Each row of these plots corresponds to one of\nthe 150 dimensions. Again, a darker color indicates\na higher value.\nAn input gate controls whether the input at the\ncurrent position should be used in deriving the ﬁnal\nhidden state of the current position. From the three\nplots of the input gates, we can observe that gener-\nally for stop words such as prepositions and articles\nthe input gates have lower values, suggesting that the\nmatching of these words is less important. On the\nother hand, content words such as nouns and verbs\ntend to have higher values of the input gates, which\nalso makes sense because these words are generally\nmore important for determining the ﬁnal relation-\nship label.\nTo further verify the observation above, we com-\npute the average input gate values for stop words\nand the other content words. We ﬁnd that the former\nhas an average value of 0.287 with a standard devia-\ntion of 0.084 while the latter has an average value of\n0.347 with a standard deviation of 0.116. This shows\nthat indeed generally stop words have lower input\ngate values. Interestingly, we also ﬁnd that some\nstop words may have higher input gate values if they\nare critical for the classiﬁcation task. For example,\nthe negation word “not” has an average input gate\nvalue of 0.444 with a standard deviation of 0.104.\nOverall, the values of the input gates conﬁrm that\nthe mLSTM helps differentiate the more important\nword-level matching results from the less important\nones.\nNext, let us look at the forget gates. Recall that\na forget gate controls the importance of the previ-\nous cell state in deriving the ﬁnal hidden state of the\ncurrent position. Higher values of a forget gate indi-\ncate that we need to remember the previous cell state\nand pass it on whereas lower values indicate that we\nshould probably forget the previous cell. From the\nthree plots of the forget gates, we can see that overall\nthe colors are the lightest for Example 1, which is an\nentailment. This suggests that when the hypothesis\nis an entailment of the premise, the mLSTM tends\nto forget the previous matching results. On the other\nhand, for Example 2 and Example 3, which are con-\ntradiction and neutral, we see generally darker col-\nors. In particular, in Example 2, we can see that the\ncolors are consistently dark starting from the word\n“his” in the hypothesis until the end. We believe the\nexplanation is that after the mLSTM processes the\nﬁrst three words of the hypothesis, “A cat washes,” it\nsees that the matching between “cat” and “dog” and\nbetween “washes” and “jumping” is a strong indica-\ntion of a contradiction, and therefore these matching\nresults need to be remembered until the end of the\nmLSTM for the ﬁnal prediction.\nWe have also checked the forget gates of the other\nsentence pairs in the test data by computing the av-\nerage forget gate values and the standard deviations\nfor entailment, neutral and contradiction, respec-\ntively.\nWe ﬁnd that the values are 0.446±0.123,\n0.507±0.148 and 0.536±0.170, respectively.\nFor\ncontradiction and neutral, the forget gates start to\nhave higher values from certain positions of the hy-\npotheses.\nBased on the observations above, we hypothesize\nthat the way the mLSTM works is as follows. It re-\nmembers important mismatches, which are useful\nfor predicting the contradiction or the neutral re-\nlationship, and forgets good matching results. At\nthe end of the mLSTM, if no important mismatch\nis remembered, the ﬁnal classiﬁer will likely pre-\ndict entailment by default. Otherwise, depending on\nthe kind of mismatch remembered, the classiﬁer will\npredict either contradiction or neutral.\nFor the output gates, we are not able to draw any\nimportant conclusion except that the output gates\nseem to be positively correlated with the input gates\nbut they tend to be darker than the input gates.\n4\nRelated Work\nThere has been much work on natural language in-\nference.\nShallow methods rely mostly on lexical\nsimilarities but are shown to be robust. For example,\nBowman et al. (2015) experimented with a lexical-\nized classiﬁer-based method, which only uses lexi-\ncal information and achieves an accuracy of 78.2%\non the SNLI corpus. More advanced methods use\nsyntactic structures of the sentences to help match-\ning them. For example, Mehdad et al. (2009) ap-\nplied syntactic-semantic tree kernels for recogniz-\ning textual entailment.\nBecause inference is es-\nsentially a logic problem, methods based on for-\nmal logic (Clark and Harrison, 2009) or natural\nlogic (MacCartney, 2009) have also been proposed.\nA comprehensive review on existing work can be\nfound in the book by Dagan et al. (2013).\nThe work most relevant to ours is the recently\nproposed neural attention model-based method by\nRockt¨aschel et al. (2016), which we have detailed\nin previous sections. Neural attention models have\nrecently been applied to some natural language pro-\ncessing tasks including machine translation (Bah-\ndanau et al., 2015), abstractive summarization (Rush\net al., 2015) and question answering (Hermann et\nal., 2015). Rockt¨aschel et al. (2016) showed that\nthe neural attention model could help derive a bet-\nter representation of the premise to be used to match\nthe hypothesis, whereas in our work we also use it to\nderive representations of the premise that are used to\nsequentially match the words in the hypothesis.\nThe SNLI corpus is new and so far it has\nonly been used in a few studies.\nBesides the\nwork by Bowman et al. (2015) themselves and by\nRockt¨aschel et al. (2016), there are two other studies\nwhich used the SNLI corpus. Vendrov et al. (2015)\nused a Skip-Thought model proposed by Kiros et al.\n(2015) to the NLI task and reported an accuracy of\n81.5% on the test data. Mou et al. (2015) used tree-\nbased CNN encoders to obtain sentence embeddings\nand achieved an accuracy of 82.1%.\n5\nConclusions and Future Work\nIn this paper, we proposed a special LSTM ar-\nchitecture for the task of natural language infer-\nence. Based on a recent work by Rockt¨aschel et al.\n(2016), we ﬁrst used neural attention models to de-\nrive attention-weighted vector representations of the\npremise. We then designed a match-LSTM that pro-\ncesses the hypothesis word by word while trying to\nmatch the hypothesis with the premise. The last hid-\nden state of this mLSTM can be used for predicting\nthe relationship between the premise and the hypoth-\nesis. Experiments on the SNLI corpus showed that\nthe mLSTM model outperformed the state-of-the-art\nperformance reported so far on this data set. More-\nover, closer analyses on the gate vectors revealed\nthat our mLSTM indeed remembers and passes on\nimportant matching results, which are typically mis-\nmatches that indicate a contradiction or a neutral re-\nlationship between the premise and the hypothesis.\nWith the large number of parameters to learn, an\ninevitable limitation of our model is that a large\ntraining data set is needed to learn good model pa-\nrameters. Indeed some preliminary experiments ap-\nplying our mLSTM to the SICK corpus (Marelli\net al., 2014), a smaller textual entailment bench-\nmark data set, did not give very good results. We\nbelieve that this is because our model learns ev-\nerything from scratch except using the pre-trained\nword embeddings. A future direction would be to\nincorporate other resources such as the paraphrase\ndatabase (Ganitkevitch et al., 2013) into the learning\nprocess.\nReferences\n[Bahdanau et al.2015] Dzmitry Bahdanau, HyungHyun\nCho, and Yoshua Bengio.\n2015.\nNeural machine\ntranslation by jointly learning to align and translate. In\nProceedings of the International Conference on Learn-\ning Representations.\n[Bowman et al.2015] Samuel R Bowman, Gabor Angeli,\nChristopher Potts, and Christopher D Manning. 2015.\nA large annotated corpus for learning natural language\ninference. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing.\n[Clark and Harrison2009] Peter Clark and Phil Harrison.\n2009. An inference-based approach to recognizing en-\ntailment. In Proceedings of the Text Analysis Confer-\nence.\n[Dagan et al.2005] Ido Dagan,\nOren Glickman,\nand\nBernardo Magnini. 2005. The PASCAL Recognising\nTextual Entailment Challenge. In Proceedings of the\nPASCAL Challenges Workshop on Recognizing Tex-\ntual Entailment.\n[Dagan et al.2013] Ido Dagan, Dan Roth, Mark Sam-\nmons, and Fabio Massimo Zanzotto. 2013. Recog-\nnizing Textual Entailment: Models and Applications.\nSynthesis Lectures on Human Language Technolo-\ngies. Morgan & Claypool Publishers.\n[Ganitkevitch et al.2013] Juri\nGanitkevitch,\nBenjamin\nVan Durme, and Chris Callison-Burch. 2013. PPDB:\nThe paraphrase database. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics.\n[Glickman et al.2005] Oren Glickman, Ido Dagan, and\nMoshe Koppel. 2005. Web based probabilistic textual\nentailment. In Proceedings of the PASCAL Challenges\nWorkshop on Recognizing Textual Entailment.\n[Graves2012] Alex Graves. 2012. Supervised sequence\nlabelling with recurrent neural networks, volume 385.\nSpringer.\n[Hermann et al.2015] Karl Moritz Hermann, Tomas Ko-\ncisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. 2015. Teach-\ning machines to read and comprehend. In Advances in\nNeural Information Processing Systems.\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and\nJ¨urgen Schmidhuber. 1997. Long short-term memory.\nNeural Computation, 9(8):1735–1780.\n[Kingma and Ba2014] Diederik Kingma and Jimmy Ba.\n2014. Adam: A method for stochastic optimization.\nProceedings of the International Conference on Learn-\ning Representations.\n[Kiros et al.2015] Ryan Kiros, Yukun Zhu, Ruslan R\nSalakhutdinov, Richard Zemel, Raquel Urtasun, An-\ntonio Torralba, and Sanja Fidler. 2015. Skip-thought\nvectors. In Advances in Neural Information Process-\ning Systems.\n[MacCartney et al.2008] Bill MacCartney, Michel Galley,\nand Christopher D Manning. 2008. A phrase-based\nalignment model for natural language inference.\nIn\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing.\n[MacCartney2009] Bill MacCartney. 2009. Natural Lan-\nguage Inference. Ph.D. thesis, Stanford University.\n[Marelli et al.2014] Marco\nMarelli,\nStefano\nMenini,\nMarco Baroni, Luisa Bentivogli, Raffaella Bernardi,\nand Roberto Zamparelli. 2014. A SICK cure for the\nevaluation of compositional distributional semantic\nmodels.\nIn Proceedings of the Ninth International\nConference on Language Resources and Evaluation.\n[Mehdad et al.2009] Yashar Mehdad, Alessandro Mos-\nchitti1,\nand Fabio Massiomo Zanzotto.\n2009.\nSemKer: Syntactic/semantic kernels for recognizing\ntextual entailment. In Proceedings of the Text Anal-\nysis Conference.\n[Mou et al.2015] Lili Mou, Men Rui, Ge Li, Yan Xu,\nLu Zhang, Rui Yan, and Zhi Jin. 2015. Recogniz-\ning entailment and contradiction by tree-based convo-\nlution. arXiv preprint arXiv:1512.08422.\n[Pennington et al.2014] Jeffrey\nPennington,\nRichard\nSocher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing.\n[Rockt¨aschel et al.2016] Tim\nRockt¨aschel,\nEdward\nGrefenstette, Karl Moritz Hermann, Tom´aˇs Koˇcisk`y,\nand Phil Blunsom. 2016. Reasoning about entailment\nwith neural attention. In Proceedings of the Interna-\ntional Conference on Learning Representations.\n[Rush et al.2015] Alexander M Rush, Sumit Chopra, and\nJason Weston.\n2015.\nA neural attention model for\nabstractive sentence summarization.\nProceedings of\nthe Conference on Empirical Methods in Natural Lan-\nguage Processing.\n[Vendrov et al.2015] Ivan Vendrov, Ryan Kiros, Sanja\nFidler,\nand Raquel Urtasun.\n2015.\nOrder-\nembeddings of images and language. arXiv preprint\narXiv:1511.06361.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2015-12-30",
  "updated": "2016-11-10"
}