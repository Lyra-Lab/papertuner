{
  "id": "http://arxiv.org/abs/1601.07482v1",
  "title": "Unsupervised Learning in Neuromemristive Systems",
  "authors": [
    "Cory Merkel",
    "Dhireesha Kudithipudi"
  ],
  "abstract": "Neuromemristive systems (NMSs) currently represent the most promising\nplatform to achieve energy efficient neuro-inspired computation. However, since\nthe research field is less than a decade old, there are still countless\nalgorithms and design paradigms to be explored within these systems. One\nparticular domain that remains to be fully investigated within NMSs is\nunsupervised learning. In this work, we explore the design of an NMS for\nunsupervised clustering, which is a critical element of several machine\nlearning algorithms. Using a simple memristor crossbar architecture and\nlearning rule, we are able to achieve performance which is on par with MATLAB's\nk-means clustering.",
  "text": "Unsupervised Learning in Neuromemristive Systems\nCory Merkel and Dhireesha Kudithipudi\nDepartment of Computer Engineering\nRochester Institute of Technology\nRochester, New York 14623-5603\nEmail: {cem1103,dxkeec}@rit.edu\nAbstract—Neuromemristive systems (NMSs) currently repre-\nsent the most promising platform to achieve energy efﬁcient\nneuro-inspired computation. However, since the research ﬁeld\nis less than a decade old, there are still countless algorithms\nand design paradigms to be explored within these systems. One\nparticular domain that remains to be fully investigated within\nNMSs is unsupervised learning. In this work, we explore the\ndesign of an NMS for unsupervised clustering, which is a critical\nelement of several machine learning algorithms. Using a simple\nmemristor crossbar architecture and learning rule, we are able to\nachieve performance which is on par with MATLAB’s k-means\nclustering.\nI.\nINTRODUCTION\nThe present research explores multiple aspects of unsuper-\nvised learning in a class of neurologically-inspired computer\narchitectures referred to as neuromemristive systems (NMSs).\nAlthough they are closely related, NMSs differ from neuro-\nmorphic systems–pioneered by Mead in the late 1980s [1]–\nin two respects: First, they are designed using a mixture of\nCMOS and memristor technologies, which affords levels of\nconnectivity and plasticity that are not achievable in neuromor-\nphic systems. The second distinction, which is more subtle but\nequally important, is that NMSs focus on abstraction, rather\nthan mimicking, of neurological processes. This abstraction\ngenerally improves the efﬁciency of the resulting hardware\nimplementations.\nNMS research and development took off rapidly in the\nlate 2000s, coinciding with a growing interest in two-terminal\nmemristors, which will be described brieﬂy in Section II\nfor unfamiliar readers. The utility of these systems has been\ndemonstrated in various application domains, especially image\nprocessing/analysis. See [2] for a review. Learning in these\nsystems has primarily been supervised. Although there are\nsome examples of unsupervised learning in spike-based sys-\ntems [3], it is relatively unexplored in non-spiking NMSs.\nOne example where unsupervised learning in non-spiking\nnetworks has been demonstrated is in [4], where principal\ncomponent analysis (PCA) is used to reduce the dimensionality\nof images. However, the authors to not discuss any circuit for\nimplementing the PCA algorithm in hardware.\nIn this research, we propose a non-spiking NMS design\nfor unsupervised clustering. The NMS is tested on samples\nfrom the MNIST database of handwritten digits. To the best\nof our knowledge, this is the ﬁrst work that explores general\naspects of clustering in these systems. Given its ability to\nreduce data dimensionality and aid in classiﬁcation, we believe\nthat clustering is a key primitive for future NMSs. Furthermore,\nwe believe this work will advance the state of unsupervised\nlearning in NMSs and help others do the same.\nII.\nBRIEF DESCRIPTION OF MEMRISTORS\nTechnically, a memristor can be deﬁned as any two-\nterminal device with a state-dependent Ohm’s law [5]. More\nconcretely, a memristor is a thin ﬁlm (I) sandwiched between\na top (M) and bottom (M) electrode. The stack is referred to\nas a metal-insulator-metal (MIM) structure because the ﬁlm\nmaterial is nominally insulating. That is, in its stoichiometric\ncrystalline form it will have a large band gap and not enough\nfree carriers to conduct. The ﬁlm is made conductive by\nintroducing defects in the crystalline structure, either through\nfabrication, applying an electric ﬁeld, or both. Defects may be\ninterstitial metallic ions which are oxidized at one electrode\nand then drift to the other, where they are reduced. Defects\nmay also be vacancies such as oxygen vacancies in a TiO2 ﬁlm.\nIn addition, defects may be changes in polarization, such as\nthose in ferroelectric ﬁlms, or even just changes in crystallinity\nas in phase change memory. In some ﬁlms, the defect proﬁle\ncan be gradually adjusted by applying electric ﬁelds for short\ndurations, yielding incremental modiﬁcations to the ﬁlm’s\noverall conductance. In other ﬁlms, only two conductance\nstates can be reached. Moreover, there is usually a minimum\namount of energy required to effect change in the ﬁlm’s defect\nproﬁle. This often translates to a threshold voltage which\nmust be applied across the ﬁlm to change its conductance.\nGiven the constant evolution of memristor technology, it makes\nlittle sense to design an NMS around any speciﬁc memristor\ndevice parameters. Instead, we assume devices will have these\ngeneral characteristics: (1) a large minimum resistance value\n(e.g. in kΩs), (2) a large OFF/ON resistance ratio (at least\n103), (3) high endurance (ability to switch many times before\nfailing), (4) high retention (non-volatility), and (5) incremental\nconductance states that can be reached by applying bipolar\nvoltage pulses above a particular thershold voltage. All of these\nproperties have been demonstrated in various devices. See [6]\nfor a review.\nIII.\nCLUSTERING ALGORITHM DESIGN\nClustering algorithms uncover structure in a set of m\nunlabeled input vectors {u(p)} by identifying M groups, or\nclusters of vectors that are similar in some way. In one common\napproach, each cluster is represented by its centroid, so the\nclustering algorithm is reduced to ﬁnding each of the M\ncentroids. This can be achieved through a simple competitive\nlearning algorithm: Initialize M vectors wi by assigning them\nto randomly-chosen input vectors. These will be referred to\nas weight vectors. Then, for each input vector, move the\narXiv:1601.07482v1  [cs.ET]  27 Jan 2016\nAlgorithm 1 Proposed clustering algorithm.\n1: Map inputs to hypercube vertices.\n2: Initialize weight vectors to random input vectors.\n3: for epoch = 1:Nepochs do\n4:\nfor p = 1:m do\n5:\nd∗\ni,p = wi · u(p) ∀i = 1, 2, . . . , M\n6:\nxi =\n\u001a1,\nd∗\ni,p = max(d∗\ni,p)\n0,\notherwise\n∀i = 1, 2, . . . , M\n7:\n∆wi,j\n=\nαxiu(p)\nj\n∀i\n=\n1, 2, . . . , M ∀j\n=\n1, 2, . . . , m\n8:\nend for\n9: end for\nclosest weight vector a little closer. After several iterations,\nthe algorithm should converge with the weight vectors lying\nat (or close to) the centroids. Of course, there are several\nparameters which must be deﬁned, including a distance metric\nfor measuring closeness. The most obvious choice is the\nℓ2-norm. However, computing this is expensive in terms of\nhardware because it requires units for calculating squares\nand square roots. In addition, as we will discuss later, it is\neasy to use a high-density memristor circuit called a crossbar\nto compute dot products between input and weight vectors.\nTherefore, it is preferred to use a dot product as a distance\nmetric. For example, if all of the vectors are normalized\n(∥u(p)∥= ∥wi∥= 1), then wi∗· u(p) > wi · u(p)∀wi ̸= wi∗,\nwhere wi∗is the closest weight vector to u(p). However, the\nconstraint that ∥u(p)∥= ∥wi∥= 1 creates a large overhead,\nbecause every input vector has to be normalized and every\nweight vector has to be re-normalized each time it is updated.\nWe propose the following solution: Map each input vector\nto the vertex of a hypercube centered about the origin: u(p) ∈\n{−1, 1}N, where N is the dimensionality of the input space.\nNow, wi · u(p) will yield a scalar value d∗\ni,p between −N and\n+N. Moreover, this scalar value can be linearly transformed\nto a distance di,p which is the ℓ1-norm, or Manhattan distance,\nbetween the weight vector and the input:\ndi,p ≡N −d∗\ni,p =\nN\nX\nj=1\n|wi,j −u(p)\nj |.\n(1)\nUsing this distance metric, we don’t ever need to re-\nnormalize the weight vectors. Furthermore, mapping input\nvectors to hypercube vertices can usually be accomplished by\nthresholding. For example, grayscale images can be mapped\nby assigning -1 to pixel values from 0 to 127 and +1 to\npixel values from 128 to 255. Algorithm 1 summarizes the\nalgorithm. The ﬁrst two lines are initialization steps. Within\nthe double for loop xi is 1 when i corresponds to the index\nof the closest vector (called the winner) and 0 otherwise. Then,\nthe weight components of the winner are moved closer to the\ncurrent input vector using a Hebbian update rule. The pre-\nfactor α, which is called the learning rate, determines how far\nthe weight vectors move each time they win. Notice that this\nalgorithm is completely unsupervised, so there are no labeled\ninput vectors.\n𝑢1\n𝑢2\n𝑢𝑁\n…\n…\n𝑥1\n𝑥2\n𝑥𝑀\nDistance\nCalculation\nMemristor\nCrossbar\nWeight \nUpdate\nInputs\nWTA\n…\nFig. 1.\nBlock diagram of proposed NMS for unsupervised clustering.\n𝑖𝑢𝑁\n(𝑝)\n𝑖𝑢2\n(𝑝)\n𝑖𝑢1\n(𝑝)\nMemristor Crossbar for 𝐰𝑖\n…\n…\n𝑅\n𝑅\ntrain_en\n𝑣𝑤𝑖,1\n𝑣𝑤𝑖,2\n𝑣𝑤𝑖,𝑁\n𝑣𝑑𝑖,𝑝\n∗\nInputs\nFig. 2.\nCrossbar and summing ampliﬁer circuit for computing the distance\nbetween the input and a weight vector.\nIV.\nNMS HARDWARE DESIGN\nThe unsupervised clustering algorithm discussed in Section\nIII can be implemented efﬁciently in an NMS by representing\nweight vectors as memristor conductances. A block diagram\nof the proposed design is shown in Figure 1. The inputs,\nwhich are represented as positive and negative currents, are\nfed through M crossbar circuits. Together with a non-inverting\nsumming ampliﬁer, (represented as a circle), each crossbar\ncomputes the distance between the current input and the weight\nvector represented by its memristors’ conductances.\nThe conﬁguration of the crossbar and summing ampliﬁer is\nshown in Figure 2. Memristors in the top row inhibit, or con-\ntribute a negative component to the output, while memristors\nin the bottom row excite, or contribute a positive component\nto the output. Therefore, each crossbar column represents one\ncomponent of one weight vector wi, which can be positive or\nnegative. If we assume that the op amp has a high open loop\ngain and the wire resistances are small, then\nvd∗\ni,p =\nN\nX\nj=1\niu(p)\nj R\n\u0012G2 −G1\nG1 + G2\n\u0013\ni,j\n,\n(2)\nwhere G1 and G2 are the top and bottom memristors in each\ncolumn, respectively. The output of the circuit is a voltage\nrepresentation of the distance between the current input and the\nweight vector represented by the crossbar. The weight vectors\nare modiﬁed by connecting them to write voltages vwi,j using\nFig. 3.\n10 cluster centroids found in a set of 1000 MNIST images using the proposed NMS.\na training enable signal train_en. The write voltages are\ndetermined by the value of ∆wi,j in line 7 of Algorithm 1.\nSpeciﬁcally, if ∆wi,j is negative, then vwi,j will be a negative\nvoltage below the memristor’s write threshold, and if ∆wi,j\nis positive, then vwi,j will be a positive voltage above the\nmemristor’s write threshold. Otherwise, the write voltage is\nzero.\nSo far, we have only discussed the memristor crossbar and\ndistance calculation parts of Figure 1 (line 5 in Algorithm\n1). The winner-takes-all circuit (line 6 in Algorithm 1) can\nbe implemented in a number of ways. In this work, we used\nthe current-mode design described in [7]. Finally, the weight\nupdate (line 7 in Algorithm 1) can be computed using simple\ncombinational logic circuits.\nV.\nCLUSTERING MNIST IMAGES\nOne exciting application of the proposed hardware is auto-\nmatically identifying clusters in sets of images. We took 1000\nimages (m=1000) from the MNIST handwritten digit dataset\nand clustered them using a behavioral model of the NMS\ndescribed in the last section. Each image was originally 20×20\ngrayscale pixels (N=400). They were mapped to hypercube\nvertices using the thresholding approach discussed earlier. In\naddition, we used 10 clusters (M=10), 500 training epochs\n(Ntrain=500), and α=0.005. The results are shown in Figure\n3. Here, we have plotted the weight vectors representing the\ncentroid of each cluster. Figure 4 shows the cost versus the\ntraining epoch, where the cost is deﬁned as\nJ =\nm\nX\np=1\n(min di,p∀i) .\n(3)\nWe see that the cost function for the proposed NMS ap-\nproaches that of MATLAB’s built-in k-means clustering after\n500 epochs.\nVI.\nCONCLUSIONS\nThe goal of this work was to explore both algorithmic and\nhardware design aspects of unsupervised learning in NMSs. To\nthat end, we proposed a clustering algorithm that maps inputs\nto vertices of a hypercube, and then iteratively ﬁnds clusters’\ncentroids using a Hebbian learning rule. We argue (although\nwe haven’t proven) that the proposed algorithm can be im-\nplemented more efﬁciently in an NMS than algorithms that\nuse either ℓ2-norm or cosine similarity as a distance function.\nThe algorithm was implemented in a custom NMS design that\nleverages crossbar circuits to compute the distance between\ninputs and weight vectors. To test our design, we clustered\n1000 MNIST images and found the results to be consistent\nwith MATLAB’s k-means clustering implementation.\nEpoch\n0\n100\n200\n300\n400\n500\nJ\n#10 5\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nMATLAB\nProposed\nFig. 4.\nCost function versus epoch while clustering MNIST images using\nthe proposed NMS.\nREFERENCES\n[1] C. Mead, “Neuromorphic electronic systems,” Proceedings\nof the IEEE, vol. 78, no. 10, pp. 1629–1636, 1990.\n[2] D. Kudithipudi, C. Merkel, M. Soltiz, G. S. Rose, and\nR. Pino, “Design of neuromorphic archtectures with mem-\nristors,” in Network Science and Cybersecurity, R. Pino,\nEd.\nSpringer, 2014, pp. 93–103.\n[3] S. Yu, B. Gao, Z. Fang, H. Yu, J. Kang, and H.-S. P.\nWong, “A low energy oxide-based electronic synaptic\ndevice for neuromorphic visual systems with tolerance to\ndevice variation.” Advanced Materials, vol. 25, no. 12, pp.\n1774–9, Mar. 2013.\n[4] S. Choi, P. Sheridan, and W. D. Lu, “Data Clustering using\nMemristor Networks.” Scientiﬁc reports, vol. 5, p. 10492,\nJan. 2015.\n[5] L. Chua, “Resistance switching memories are memristors,”\nApplied Physics A, vol. 102, no. 4, pp. 765–783, Jan. 2011.\n[6] D. Kuzum, S. Yu, and H.-S. P. Wong, “Synaptic electron-\nics: materials, devices and applications.” Nanotechnology,\nvol. 24, no. 38, p. 382001, Sep. 2013.\n[7] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A.\nMead, “Winner-take-all networks of O(N) complexity,” in\nAdvances in Neural Information Processing Systems, 1988,\npp. 703–711.\n",
  "categories": [
    "cs.ET",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2016-01-27",
  "updated": "2016-01-27"
}