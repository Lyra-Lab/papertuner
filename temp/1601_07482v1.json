{
  "id": "http://arxiv.org/abs/1601.07482v1",
  "title": "Unsupervised Learning in Neuromemristive Systems",
  "authors": [
    "Cory Merkel",
    "Dhireesha Kudithipudi"
  ],
  "abstract": "Neuromemristive systems (NMSs) currently represent the most promising\nplatform to achieve energy efficient neuro-inspired computation. However, since\nthe research field is less than a decade old, there are still countless\nalgorithms and design paradigms to be explored within these systems. One\nparticular domain that remains to be fully investigated within NMSs is\nunsupervised learning. In this work, we explore the design of an NMS for\nunsupervised clustering, which is a critical element of several machine\nlearning algorithms. Using a simple memristor crossbar architecture and\nlearning rule, we are able to achieve performance which is on par with MATLAB's\nk-means clustering.",
  "text": "Unsupervised Learning in Neuromemristive Systems\nCory Merkel and Dhireesha Kudithipudi\nDepartment of Computer Engineering\nRochester Institute of Technology\nRochester, New York 14623-5603\nEmail: {cem1103,dxkeec}@rit.edu\nAbstractâ€”Neuromemristive systems (NMSs) currently repre-\nsent the most promising platform to achieve energy efï¬cient\nneuro-inspired computation. However, since the research ï¬eld\nis less than a decade old, there are still countless algorithms\nand design paradigms to be explored within these systems. One\nparticular domain that remains to be fully investigated within\nNMSs is unsupervised learning. In this work, we explore the\ndesign of an NMS for unsupervised clustering, which is a critical\nelement of several machine learning algorithms. Using a simple\nmemristor crossbar architecture and learning rule, we are able to\nachieve performance which is on par with MATLABâ€™s k-means\nclustering.\nI.\nINTRODUCTION\nThe present research explores multiple aspects of unsuper-\nvised learning in a class of neurologically-inspired computer\narchitectures referred to as neuromemristive systems (NMSs).\nAlthough they are closely related, NMSs differ from neuro-\nmorphic systemsâ€“pioneered by Mead in the late 1980s [1]â€“\nin two respects: First, they are designed using a mixture of\nCMOS and memristor technologies, which affords levels of\nconnectivity and plasticity that are not achievable in neuromor-\nphic systems. The second distinction, which is more subtle but\nequally important, is that NMSs focus on abstraction, rather\nthan mimicking, of neurological processes. This abstraction\ngenerally improves the efï¬ciency of the resulting hardware\nimplementations.\nNMS research and development took off rapidly in the\nlate 2000s, coinciding with a growing interest in two-terminal\nmemristors, which will be described brieï¬‚y in Section II\nfor unfamiliar readers. The utility of these systems has been\ndemonstrated in various application domains, especially image\nprocessing/analysis. See [2] for a review. Learning in these\nsystems has primarily been supervised. Although there are\nsome examples of unsupervised learning in spike-based sys-\ntems [3], it is relatively unexplored in non-spiking NMSs.\nOne example where unsupervised learning in non-spiking\nnetworks has been demonstrated is in [4], where principal\ncomponent analysis (PCA) is used to reduce the dimensionality\nof images. However, the authors to not discuss any circuit for\nimplementing the PCA algorithm in hardware.\nIn this research, we propose a non-spiking NMS design\nfor unsupervised clustering. The NMS is tested on samples\nfrom the MNIST database of handwritten digits. To the best\nof our knowledge, this is the ï¬rst work that explores general\naspects of clustering in these systems. Given its ability to\nreduce data dimensionality and aid in classiï¬cation, we believe\nthat clustering is a key primitive for future NMSs. Furthermore,\nwe believe this work will advance the state of unsupervised\nlearning in NMSs and help others do the same.\nII.\nBRIEF DESCRIPTION OF MEMRISTORS\nTechnically, a memristor can be deï¬ned as any two-\nterminal device with a state-dependent Ohmâ€™s law [5]. More\nconcretely, a memristor is a thin ï¬lm (I) sandwiched between\na top (M) and bottom (M) electrode. The stack is referred to\nas a metal-insulator-metal (MIM) structure because the ï¬lm\nmaterial is nominally insulating. That is, in its stoichiometric\ncrystalline form it will have a large band gap and not enough\nfree carriers to conduct. The ï¬lm is made conductive by\nintroducing defects in the crystalline structure, either through\nfabrication, applying an electric ï¬eld, or both. Defects may be\ninterstitial metallic ions which are oxidized at one electrode\nand then drift to the other, where they are reduced. Defects\nmay also be vacancies such as oxygen vacancies in a TiO2 ï¬lm.\nIn addition, defects may be changes in polarization, such as\nthose in ferroelectric ï¬lms, or even just changes in crystallinity\nas in phase change memory. In some ï¬lms, the defect proï¬le\ncan be gradually adjusted by applying electric ï¬elds for short\ndurations, yielding incremental modiï¬cations to the ï¬lmâ€™s\noverall conductance. In other ï¬lms, only two conductance\nstates can be reached. Moreover, there is usually a minimum\namount of energy required to effect change in the ï¬lmâ€™s defect\nproï¬le. This often translates to a threshold voltage which\nmust be applied across the ï¬lm to change its conductance.\nGiven the constant evolution of memristor technology, it makes\nlittle sense to design an NMS around any speciï¬c memristor\ndevice parameters. Instead, we assume devices will have these\ngeneral characteristics: (1) a large minimum resistance value\n(e.g. in kâ„¦s), (2) a large OFF/ON resistance ratio (at least\n103), (3) high endurance (ability to switch many times before\nfailing), (4) high retention (non-volatility), and (5) incremental\nconductance states that can be reached by applying bipolar\nvoltage pulses above a particular thershold voltage. All of these\nproperties have been demonstrated in various devices. See [6]\nfor a review.\nIII.\nCLUSTERING ALGORITHM DESIGN\nClustering algorithms uncover structure in a set of m\nunlabeled input vectors {u(p)} by identifying M groups, or\nclusters of vectors that are similar in some way. In one common\napproach, each cluster is represented by its centroid, so the\nclustering algorithm is reduced to ï¬nding each of the M\ncentroids. This can be achieved through a simple competitive\nlearning algorithm: Initialize M vectors wi by assigning them\nto randomly-chosen input vectors. These will be referred to\nas weight vectors. Then, for each input vector, move the\narXiv:1601.07482v1  [cs.ET]  27 Jan 2016\nAlgorithm 1 Proposed clustering algorithm.\n1: Map inputs to hypercube vertices.\n2: Initialize weight vectors to random input vectors.\n3: for epoch = 1:Nepochs do\n4:\nfor p = 1:m do\n5:\ndâˆ—\ni,p = wi Â· u(p) âˆ€i = 1, 2, . . . , M\n6:\nxi =\n\u001a1,\ndâˆ—\ni,p = max(dâˆ—\ni,p)\n0,\notherwise\nâˆ€i = 1, 2, . . . , M\n7:\nâˆ†wi,j\n=\nÎ±xiu(p)\nj\nâˆ€i\n=\n1, 2, . . . , M âˆ€j\n=\n1, 2, . . . , m\n8:\nend for\n9: end for\nclosest weight vector a little closer. After several iterations,\nthe algorithm should converge with the weight vectors lying\nat (or close to) the centroids. Of course, there are several\nparameters which must be deï¬ned, including a distance metric\nfor measuring closeness. The most obvious choice is the\nâ„“2-norm. However, computing this is expensive in terms of\nhardware because it requires units for calculating squares\nand square roots. In addition, as we will discuss later, it is\neasy to use a high-density memristor circuit called a crossbar\nto compute dot products between input and weight vectors.\nTherefore, it is preferred to use a dot product as a distance\nmetric. For example, if all of the vectors are normalized\n(âˆ¥u(p)âˆ¥= âˆ¥wiâˆ¥= 1), then wiâˆ—Â· u(p) > wi Â· u(p)âˆ€wi Ì¸= wiâˆ—,\nwhere wiâˆ—is the closest weight vector to u(p). However, the\nconstraint that âˆ¥u(p)âˆ¥= âˆ¥wiâˆ¥= 1 creates a large overhead,\nbecause every input vector has to be normalized and every\nweight vector has to be re-normalized each time it is updated.\nWe propose the following solution: Map each input vector\nto the vertex of a hypercube centered about the origin: u(p) âˆˆ\n{âˆ’1, 1}N, where N is the dimensionality of the input space.\nNow, wi Â· u(p) will yield a scalar value dâˆ—\ni,p between âˆ’N and\n+N. Moreover, this scalar value can be linearly transformed\nto a distance di,p which is the â„“1-norm, or Manhattan distance,\nbetween the weight vector and the input:\ndi,p â‰¡N âˆ’dâˆ—\ni,p =\nN\nX\nj=1\n|wi,j âˆ’u(p)\nj |.\n(1)\nUsing this distance metric, we donâ€™t ever need to re-\nnormalize the weight vectors. Furthermore, mapping input\nvectors to hypercube vertices can usually be accomplished by\nthresholding. For example, grayscale images can be mapped\nby assigning -1 to pixel values from 0 to 127 and +1 to\npixel values from 128 to 255. Algorithm 1 summarizes the\nalgorithm. The ï¬rst two lines are initialization steps. Within\nthe double for loop xi is 1 when i corresponds to the index\nof the closest vector (called the winner) and 0 otherwise. Then,\nthe weight components of the winner are moved closer to the\ncurrent input vector using a Hebbian update rule. The pre-\nfactor Î±, which is called the learning rate, determines how far\nthe weight vectors move each time they win. Notice that this\nalgorithm is completely unsupervised, so there are no labeled\ninput vectors.\nğ‘¢1\nğ‘¢2\nğ‘¢ğ‘\nâ€¦\nâ€¦\nğ‘¥1\nğ‘¥2\nğ‘¥ğ‘€\nDistance\nCalculation\nMemristor\nCrossbar\nWeight \nUpdate\nInputs\nWTA\nâ€¦\nFig. 1.\nBlock diagram of proposed NMS for unsupervised clustering.\nğ‘–ğ‘¢ğ‘\n(ğ‘)\nğ‘–ğ‘¢2\n(ğ‘)\nğ‘–ğ‘¢1\n(ğ‘)\nMemristor Crossbar for ğ°ğ‘–\nâ€¦\nâ€¦\nğ‘…\nğ‘…\ntrain_en\nğ‘£ğ‘¤ğ‘–,1\nğ‘£ğ‘¤ğ‘–,2\nğ‘£ğ‘¤ğ‘–,ğ‘\nğ‘£ğ‘‘ğ‘–,ğ‘\nâˆ—\nInputs\nFig. 2.\nCrossbar and summing ampliï¬er circuit for computing the distance\nbetween the input and a weight vector.\nIV.\nNMS HARDWARE DESIGN\nThe unsupervised clustering algorithm discussed in Section\nIII can be implemented efï¬ciently in an NMS by representing\nweight vectors as memristor conductances. A block diagram\nof the proposed design is shown in Figure 1. The inputs,\nwhich are represented as positive and negative currents, are\nfed through M crossbar circuits. Together with a non-inverting\nsumming ampliï¬er, (represented as a circle), each crossbar\ncomputes the distance between the current input and the weight\nvector represented by its memristorsâ€™ conductances.\nThe conï¬guration of the crossbar and summing ampliï¬er is\nshown in Figure 2. Memristors in the top row inhibit, or con-\ntribute a negative component to the output, while memristors\nin the bottom row excite, or contribute a positive component\nto the output. Therefore, each crossbar column represents one\ncomponent of one weight vector wi, which can be positive or\nnegative. If we assume that the op amp has a high open loop\ngain and the wire resistances are small, then\nvdâˆ—\ni,p =\nN\nX\nj=1\niu(p)\nj R\n\u0012G2 âˆ’G1\nG1 + G2\n\u0013\ni,j\n,\n(2)\nwhere G1 and G2 are the top and bottom memristors in each\ncolumn, respectively. The output of the circuit is a voltage\nrepresentation of the distance between the current input and the\nweight vector represented by the crossbar. The weight vectors\nare modiï¬ed by connecting them to write voltages vwi,j using\nFig. 3.\n10 cluster centroids found in a set of 1000 MNIST images using the proposed NMS.\na training enable signal train_en. The write voltages are\ndetermined by the value of âˆ†wi,j in line 7 of Algorithm 1.\nSpeciï¬cally, if âˆ†wi,j is negative, then vwi,j will be a negative\nvoltage below the memristorâ€™s write threshold, and if âˆ†wi,j\nis positive, then vwi,j will be a positive voltage above the\nmemristorâ€™s write threshold. Otherwise, the write voltage is\nzero.\nSo far, we have only discussed the memristor crossbar and\ndistance calculation parts of Figure 1 (line 5 in Algorithm\n1). The winner-takes-all circuit (line 6 in Algorithm 1) can\nbe implemented in a number of ways. In this work, we used\nthe current-mode design described in [7]. Finally, the weight\nupdate (line 7 in Algorithm 1) can be computed using simple\ncombinational logic circuits.\nV.\nCLUSTERING MNIST IMAGES\nOne exciting application of the proposed hardware is auto-\nmatically identifying clusters in sets of images. We took 1000\nimages (m=1000) from the MNIST handwritten digit dataset\nand clustered them using a behavioral model of the NMS\ndescribed in the last section. Each image was originally 20Ã—20\ngrayscale pixels (N=400). They were mapped to hypercube\nvertices using the thresholding approach discussed earlier. In\naddition, we used 10 clusters (M=10), 500 training epochs\n(Ntrain=500), and Î±=0.005. The results are shown in Figure\n3. Here, we have plotted the weight vectors representing the\ncentroid of each cluster. Figure 4 shows the cost versus the\ntraining epoch, where the cost is deï¬ned as\nJ =\nm\nX\np=1\n(min di,pâˆ€i) .\n(3)\nWe see that the cost function for the proposed NMS ap-\nproaches that of MATLABâ€™s built-in k-means clustering after\n500 epochs.\nVI.\nCONCLUSIONS\nThe goal of this work was to explore both algorithmic and\nhardware design aspects of unsupervised learning in NMSs. To\nthat end, we proposed a clustering algorithm that maps inputs\nto vertices of a hypercube, and then iteratively ï¬nds clustersâ€™\ncentroids using a Hebbian learning rule. We argue (although\nwe havenâ€™t proven) that the proposed algorithm can be im-\nplemented more efï¬ciently in an NMS than algorithms that\nuse either â„“2-norm or cosine similarity as a distance function.\nThe algorithm was implemented in a custom NMS design that\nleverages crossbar circuits to compute the distance between\ninputs and weight vectors. To test our design, we clustered\n1000 MNIST images and found the results to be consistent\nwith MATLABâ€™s k-means clustering implementation.\nEpoch\n0\n100\n200\n300\n400\n500\nJ\n#10 5\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nMATLAB\nProposed\nFig. 4.\nCost function versus epoch while clustering MNIST images using\nthe proposed NMS.\nREFERENCES\n[1] C. Mead, â€œNeuromorphic electronic systems,â€ Proceedings\nof the IEEE, vol. 78, no. 10, pp. 1629â€“1636, 1990.\n[2] D. Kudithipudi, C. Merkel, M. Soltiz, G. S. Rose, and\nR. Pino, â€œDesign of neuromorphic archtectures with mem-\nristors,â€ in Network Science and Cybersecurity, R. Pino,\nEd.\nSpringer, 2014, pp. 93â€“103.\n[3] S. Yu, B. Gao, Z. Fang, H. Yu, J. Kang, and H.-S. P.\nWong, â€œA low energy oxide-based electronic synaptic\ndevice for neuromorphic visual systems with tolerance to\ndevice variation.â€ Advanced Materials, vol. 25, no. 12, pp.\n1774â€“9, Mar. 2013.\n[4] S. Choi, P. Sheridan, and W. D. Lu, â€œData Clustering using\nMemristor Networks.â€ Scientiï¬c reports, vol. 5, p. 10492,\nJan. 2015.\n[5] L. Chua, â€œResistance switching memories are memristors,â€\nApplied Physics A, vol. 102, no. 4, pp. 765â€“783, Jan. 2011.\n[6] D. Kuzum, S. Yu, and H.-S. P. Wong, â€œSynaptic electron-\nics: materials, devices and applications.â€ Nanotechnology,\nvol. 24, no. 38, p. 382001, Sep. 2013.\n[7] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A.\nMead, â€œWinner-take-all networks of O(N) complexity,â€ in\nAdvances in Neural Information Processing Systems, 1988,\npp. 703â€“711.\n",
  "categories": [
    "cs.ET",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2016-01-27",
  "updated": "2016-01-27"
}