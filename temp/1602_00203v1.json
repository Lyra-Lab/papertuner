{
  "id": "http://arxiv.org/abs/1602.00203v1",
  "title": "Greedy Deep Dictionary Learning",
  "authors": [
    "Snigdha Tariyal",
    "Angshul Majumdar",
    "Richa Singh",
    "Mayank Vatsa"
  ],
  "abstract": "In this work we propose a new deep learning tool called deep dictionary\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\na time. This requires solving a simple (shallow) dictionary learning problem,\nthe solution to this is well known. We apply the proposed technique on some\nbenchmark deep learning datasets. We compare our results with other deep\nlearning tools like stacked autoencoder and deep belief network; and state of\nthe art supervised dictionary learning tools like discriminative KSVD and label\nconsistent KSVD. Our method yields better results than all.",
  "text": " \nAbstract—In this work we propose a new deep learning tool – \ndeep dictionary learning. Multi-level dictionaries are learnt in a \ngreedy fashion – one layer at a time. This requires solving a simple \n(shallow) dictionary learning problem; the solution to this is well \nknown. We apply the proposed technique on some benchmark \ndeep learning datasets. We compare our results with other deep \nlearning tools like stacked autoencoder and deep belief network; \nand state-of-the-art supervised dictionary learning tools like \ndiscriminative K-SVD and label consistent K-SVD. Our method \nyields better results than all.   \n \nIndex Terms—Deep Learning, Dictionary Learning, Feature \nExtraction \n \nI. INTRODUCTION \nN recent years there has been a lot of interest in dictionary \nlearning. However the concept of dictionary learning has \nbeen around for much longer. Its application in vision [1] and \ninformation retrieval [2] dates back to the late 90’s. In those \ndays, the term ‘dictionary learning’ had not been coined; \nresearchers were using the term ‘matrix factorization’. The goal \nwas to learn an empirical basis from the data. It basically \nrequired decomposing the data matrix to a basis / dictionary \nmatrix and a feature matrix – hence the name ‘matrix \nfactorization’.  \nThe current popularity of dictionary learning owes to K-SVD \n[3, 4]. K-SVD is an algorithm to decompose a matrix (training \ndata) into a dense basis and sparse coefficients. However the \nconcept of such a dense-sparse decomposition predates K-SVD \n[5]. Since the advent of K-SVD in 2006, there have been a \nplethora of work on this topic. Dictionary learning can be used \nboth for unsupervised problems (mainly inverse problems in \nimage processing) as well as for problems arising in supervised \nfeature extraction.  \nDictionary learning has been used in virtually all inverse \nproblems arising in image processing starting from simple \nimage [6, 7] and video [8] denoising, image inpainting [9], to \nmore complex problems like color image restoration [10], \ninverse half toning [11] and even medical image reconstruction \n[12, 13]. Solving inverse problems is not the goal of this work; \nwe are more interested in dictionary learning from the \nperspective of machine learning. We briefly discussed [6-13] \nfor the sake of completeness. \nMathematical transforms like DCT, wavelet, curvelet, Gabor \netc. have been widely used in image classification problems \n \n \n[14-16]. These techniques used these transforms as a \nsparsifying step followed by statistical feature extraction \nmethods like PCA or LDA before feeding the features to a \nclassifier. Just as dictionary learning is replacing such fixed \ntransforms (wavelet, DCT, curvelet etc.) in signal processing \nproblems, it is also replacing them in feature extraction \nscenarios. Dictionary learning gives researchers the opportunity \nto design dictionaries to yield not only sparse representation \n(like curvelet, wavelet, DCT etc.) but also discriminative \ninformation.  \nInitial techniques proposed naïve approaches which learnt \nspecific dictionaries for each class [17-19]. Later approaches \nincorporated discriminative penalties into the dictionary \nlearning framework. One such technique is to include softmax \ndiscriminative cost function [20-22]; other discriminative \npenalties include Fisher discrimination criterion [23], linear \npredictive classification error [24, 25] and hinge loss function \n[26, 27]. In [28, 29] discrimination is introduced by forcing the \nlearned features to map to corresponding class labels. \nAll prior studies on dictionary learning (DL) are ‘shallow’ \nlearning models just like a restricted boltzman machine (RBM) \n[30] and autoencoder (AE) [31]. DL, RBM and AE – all fall \nunder the broader topic of representation learning. In DL, the \ncost function is Euclidean distance between the data and the \nrepresentation given the learned basis; for RBM it is Boltzman \nenergy; in AE, the cost is the Euclidean reconstruction error \nbetween the data and the decoded representation / features.  \nAlmost at the same time, when dictionary learning started \ngaining popularity, researchers in machine learning observed \nthat better (more abstract and compact) representation can be \nachieved by going deeper. Deep Belief Network (DBN) is \nformed by stacking one RBM after the other [32, 33]. Similarly \nstacked autoencoder (SAE) were created by one AE inside the \nother [34, 35].  \nFollowing the success of DBN and SAE, we propose to learn \nmulti-level deep dictionaries. This is the first work on deep \ndictionary learning. The rest of the paper will be organized into \nseveral sections…. \n \nII. LITERATURE REVIEW \nWe will briefly review prior studies on dictionary learning, \nstacked autoencoders and deep Boltzmann machines.   \nA. Dictionary Learning \nEarly studies in dictionary learning wanted to learn a basis for \nGreedy Deep Dictionary Learning \nSnigdha Tariyal, Angshul Majumdar, Member IEEE, Richa Singh, Senior Member IEEE, and Mayank \nVatsa, Senior Member, IEEE \nI\nrepresentation. There were no constraints on the dictionary \natoms or on the loading coefficients. The method of optimal \ndirections [36] was used to learn the basis: \n2\n,\nmin\nF\nD Z\nX\nDZ\n\n                 (1) \nHere X is the training data, D is the dictionary to be learnt and \nZ consists of the loading coefficients  \nFor problems in sparse representation, the objective is to learn \na basis that can represent the samples in a sparse fashion, i.e. Z \nneeds to be sparse. The KSVD [3, 4] is the most well known \ntechnique for solving this problem. Fundamentally it solves a \nproblem of the form: \n2\n0\n,\nmin\nsuch that \nF\nD Z\nX\nDZ\nZ\n\n\n\n         (2) \nKSVD proceeds in two stages. In the first stage it learns the \ndictionary and in the next stage it uses the learned dictionary to \nsparsely represent the data. Solving the l0-norm minimization \nproblem is NP hard [37]. KSVD employs the greedy (sub-\noptimal) orthogonal matching pursuit (OMP) [38] to solve the \nl0-norm minimization problem approximately. In the dictionary \nlearning stage, KSVD proposes an efficient technique to \nestimate the atoms one at a time using a rank one update. The \nmajor disadvantage of KSVD is that it is a relatively slow \ntechnique owing to its requirement of computing the SVD \n(singular value decomposition) in every iteration. There are \nother efficient optimization based approaches for dictionary \nlearning [39, 40] – these learn the full dictionary instead of \nupdating the atoms separately. \nThe dictionary learning formulation in (2) is unsupervised. As \nmentioned before there is a large volume of work on supervised \ndictionary learning problems. We will briefly discuss the major \nones here. The first work on Sparse Representation based \nClassification (SRC) [41] was not much of a dictionary learning \ntechnique, but was a simple dictionary design problem where \nall the training samples are concatenated in a large dictionary. \nThe assumption is that the training samples for a basis for any \nnew test sample belonging to the correct class. Their proposed \nmodel is: \nx\nXa\n\n                   (3) \nwhere x is the test sample and X is dictionary consisting of all \nthe training samples. \nIt is assumed in [41] that since the correct class only \nrepresents x, the vector a is going to be sparse. Based on this \nassumption they solved a using some sparse recovery \ntechnique. Once a is obtained, the problem is to classify x. This \nis achieved by computing the error between the test image and \nits representation from each class c obtained by Xcac. where c \ndenotes the cth class. The test sample is simply assigned to the \nclass having the lowest error. \nSeveral improvements to the basic SRC formulation was \nproposed in [42-44]. In [42, 43] it was proposed that since a has \na known class structure, one can improve upon the basic sparse \nclassification approach by incorporating group-sparsity. In [44] \na non-linear extension to the SRC was proposed. Later works \nhandled the non-linear extension in a smarter fashion using the \nkernel trick [45-47]. \nThe SRC does exactly fit into the dictionary learning \nparadigm. However [48] proposed a simple extension of SRC – \ninstead of using raw training samples as the basis, they learnt a \nseparate basis for each class and used these dictionaries for \nclassification. This approach is naïve; there is no guarantee that \ndictionaries from different classes would not be similar. In [49] \nthis issue is corrected. Here an additional incoherency penalty \non the dictionaries. This penalty assures that the dictionaries \nfrom different classes look different from each other. The \nformulation is given as: \n\n\n2\n2\n1\n,\n1\nmin\ni\ni\nC\nT\ni\ni\ni\ni\nj\nF\nF\nD Z\ni\ni\nj\nX\nDZ\nZ\nD D\n\n\n\n\n\n\n\n\n\n     (4) \nUnfortunately this formulation does not improve the overall \nresults too much. It learns dictionaries that look different from \neach other but does not produce features that are distinctive; i.e. \nthe feature generated for the test sample from dictionaries of all \nclasses looked more or less the same. \nThe aforesaid issue was rectified in [50]; it combined two \nconcepts. The first one is the discrimination of the learned \nfeatures and the second one is the discrimination of the class \nspecific dictionaries. The second criteria demands that the \nfeatures from a particular class will reconstruct the samples of \nthe same class accurately; however it will not represent samples \nof the other classes. This idea is formulated as follows: \n2\n2\n2\n(\n,\n)\ni\nj\ni\ni\ni\ni\ni\ni\ni\nj\ni\nF\nF\nF\ni\nj\nC X D Z\nX\nDZ\nX\nD Z\nD Z\n\n\n\n\n\n\n (5) \nHere \n\n\n1 |...|\n|...|\nc\nC\nD\nD\nD\nD\n\n is the augmented dictionary \nand Dc are the class specific dictionaries, Xi are the training \nsamples for the ith class, Zi is the representaion over all the \ndictionaries. According to their assumption, only the portion of \nZi pertaining to the correct class should represent the data well \n- this leads to the second term in the expression; the other \ndictionaries should not represent the data well hence the third \nterm.  \nSo far, we have discussed about the discriminative \ndictionaries. As mentioned before, [50] has a second term that \ndiscriminates among the learned features. This term arises from \nthe Fisher Discriminant Analysis - it tries to increase the \ncovariance between the classes and decrease covariance within \nthe class. This is represented by: \n2\n( )\n(\n)\n(\n)\nW\nB\nF\nf Z\ntr S\ntr S\nZ\n\n\n\n\n          (6) \nwhere\n\n\n\n1\nC\nT\nW\ni\nc\ni\nc\nc\ni\nS\nz\nz\nz\nz\n\n\n\n\n\nand \n, \n\n\n\n1\nC\nT\nW\nc\nc\nc\nS\nz\nz\nz\nz\n\n\n\n\n\n; \nthe \nregularization \nhelps \nin \nstabilizing the solution. \nThe complete formulation given in [50] is as follows: \n1\n2\n1\n,\nmin\n(\n, )\n( )\nD Z C XD Z\nZ\nf Z\n\n\n\n\n          (7) \nThe label consistent KSVD is one of the more recent \ntechniques for learning discriminative sparse representation. It \nis simple to understand and implement; it showed good results \nfor face recognition [28, 29]. The first technique called \nDiscriminative K-SVD [28] or LC-KSVD1 [29]; it proposes an \noptimization problem of the following form: \n2\n2\n2\n1\n2\n3\n1\n, ,\nmin\n+\nF\nF\nF\nD Z A X\nDZ\nD\nZ\nQ\nAZ\n\n\n\n\n\n\n\n   (8) \nHere Q is the label of the training samples, it is a canonical basis \nwith a one for the correct class and zeroes elsewhere. A is a \nparameter of the linear classifier.  \nIn [29] a second formulation is proposed that adds another \nterm to penalize classification error. The LC-KSVD2 \nformulation is as follows: \n2\n2\n1\n2\n1\n, , ,W\n2\n2\n3\n4\nmin\n+\nF\nF\nD Z A\nF\nF\nX\nDZ\nD\nZ\nQ\nAZ\nH\nWZ\n\n\n\n\n\n\n\n\n\n\n         (9) \nH is a ‘discriminative’ sparse code corresponding to an input \nsignal sample, if the nonzero values of Hi occur at those indices \nwhere the training sample Xi and the dictionary item dk share \nthe same label. Basically this formulation imposes labels not \nonly on the sparse coefficient vectors Zi’s but also on the \ndictionary atoms.  \nDuring training, the LC-KSVD learns a discriminative \ndictionary D. The dictionary D and the classification weights A \nneed to be normalized. When there is a new test sample, the \nsparse coefficients for the same are learnt using normalized \ndictionary using l1-minimization: \n2\n2\n1\nmin\ntest\ntest\nz\nz\nx\nDz\nz\n\n\n\n\n          (10) \nOnce the sparse representation of the test sample is obtained, \nthe classification task is straightforward – the label of the test \nsample is assigned as: \nargmax(\n)\ntest\nj\nj\nAz\n\n              (11) \nB. Deep Boltzman Machine \n  \n \n \nFig. 1. Restricted Boltzman Machine \n \nRestricted Boltzmann Machines are undirected models that \nuses stochastic hidden units to model the distribution over the \nstochastic visible units. The hidden layer is symmetrically \nconnected with the visible unit and the architecture is \n“restricted” as there are no connections between units of the \nsame layer. Traditionally, RBMs are used to model the \ndistribution of the input data p(x).  \nThe schematic diagram of RBM is shown in Fig. 1. The \nobjective is to learn the network weights (W) and the \nrepresentation (H). This is achieved by optimizing the \nBoltzman cost function given by: \n(\n,\n)\n(\n,\n)\nE W H\np W H\ne\n\n                (12) \nWhere, \n(\n,\n)\n-\nT\nE W H\nH WX\n\nincluding the bias terms.   \nThe conditional distributions are given by (assuming \nindependence) –  \n(\n|\n)\n( | )\np X H\np x h\n\n \n(\n|\n)\n( | )\np H X\np h x\n\n  \nAssuming binary input variable, the probability that a node \nwill be active can be given as follows, \n(\n1| )\n(\n)\nT\np x\nh\nsigm W h\n\n\n  \n(\n1| )\n(\n)\np h\nx\nsigm Wx\n\n\n \nComputing the exact gradient of this loss function is almost \nintractable. However, there is a stochastic approximation to \napproximate the gradient termed as contrastive divergence \ngradient. A sequence of Gibbs sampling based reconstruction, \nproduces an approximation of the expectation of joint energy \ndistribution, using which the gradient can be computed. \nUsually RBM is unsupervised, but there are studies which \ntrained discriminative RBMs by utilizing the class labels [51]. \nThere are also RBMs which are sparse [52]; the sparsity is \ncontrolled by the firing the hidden units only if they are over \nsome threshold. Supervision can also be achieved using sparse \nRBMs by extending it to have similar sparsity structure within \nthe group / class [53]. \nDeep Boltzmann Machines (DBM) [54] is an extension of \nRBM by stacking multiple hidden layers on top of each other \n(Fig. 2). DBM is an undirected learning model and thus it is \ndifferent from the other stacked network architectures that each \nlayer receives feedback from both the top-down and bottom-up \nlayer signals. This feedback mechanism helps in managing \nuncertainty in learning models. While the traditional RBM can \nmodel logistic units, a Gaussian-Bernoulli RBM [55] can be \nused as well with real valued visible units. \n \n \n \nFig. 2. Deep Boltzman Machine \n \nC. Stacked Autoencoder  \n \n \nFig. 3. Single Layer Autoencoder \n \nAn autoencoder consists (as seen in Fig. 3) of two parts – the \nencoder maps the input to a latent representation, and the \nW\nX\nH\nW2\nW1\nX\nH1\nH2\nW \nW’ \nInput Layer \nOutput Layer \nHidden Layer \ndecoder maps the latent representation back to the data. For a \ngiven input vector (including the bias term) x, the latent space \nis expressed as: \nh\nWx\n\n                    (13) \nHere the rows of W are the link weights from all the input nodes \nto the corresponding latent node. Usually a non-linear \nactivation function is used at the output of the hidden nodes \nleading to: \n(\n)\nh\nWx\n\n\n                   (14) \nThe sigmoid function is popular; other non-linear activation \nfunctions (like tanh) can be used as well. Rectifier units and \nlarge neural networks employ linear activation functions \n(identity) – this considerably speeds up training.  \nThe decoder portion reverse maps the latent variables to the \ndata space.  \n' (\n)\nx\nW\nWx\n\n\n                  (15) \nSince the data space is assumed to be the space of real numbers, \nthere is no sigmoidal function here. \nDuring training the problem is to learn the encoding and \ndecoding weights – W and W’. These are learnt by minimizing \nthe Euclidean cost: \n2\n,\n'\nargmin\n' (\n) F\nW W\nX\nW\nWX\n\n\n             (16) \nHere \n1\n[\n|...|\n]\nN\nX\nx\nx\n\n consists all the training sampled stacked \nas columns. The problem (16) is clearly non-convex, but is \nsmooth and hence can be solved by gradient descent techniques; \nthe activation function needs to be smooth and continuously \ndifferentiable. \n \n \nFig. 4. Stacked Autoencoder \n \nThere are several extensions to the basic autoencoder \narchitecture. Stacked autoencoders have multiple hidden layers \n– one inside the other (see Fig. 4). The corresponding cost \nfunction is expressed as follows: \n1\n1\n1\n2\n...\n,\n' ...\n'\nargmin\n(\n)\nL\nL\nF\nW\nW\nW\nW\nX\ng\nf X\n\n\n           (17) \nwhere\n\n\n\n\n1\n2\n'\n'...\n'\n(\n)\nL\ng\nW\nW\nW\nf X\n\n\nand \n\n\n\n\n1\n2\n1\n... (\n)\nL\nL\nf\nW\nW\nW X\n\n\n\n\n\n\n  \nSolving the complete problem (17) is computationally \nchallenging. Also learning so many parameters (network \nweights) lead to over-fitting. To address both these issues, the \nweights are usually learned in a greedy fashion layer by layer \n[32, 34].  \nStacked denoising autoencoder [35] is a variant of the basic \nautoencoder where the input consists of noisy samples and the \noutput consists of clean samples. Here the encoder and decoder \nare learnt to denoise noisy input samples.   \nAnother variation for the basic autoencoder is to regularize \nit, i.e. \n2\n(\n)\nargmin\n(\n)\n(\n,\n)\nF\nW s\nX\ng\nf X\nR W X\n\n\n         (18) \nThe regularization can be a simple Tikhonov regularization \n– however that is not used in practice. It can be a sparsity \npromoting term [56, 57] or a weight decay term (Frobenius \nnorm of the Jacobian) as used in the contractive autoencoder \n[58]. The regularization term is usually chosen so that they are \ndifferentiable and hence minimizable using gradient descent \ntechniques.  \nIII. DEEP DICTIONARY LEARNING \n \n \nFig. 5. Schematic Diagram for Dictionary Learning \n \nIn this section we describe the main contribution of this work. \nA single / shallow level of dictionary learning yields a latent \nrepresentation of data and the dictionary atoms. Here we \npropose to learn deeper latent representation of data by learning \nmulti-level dictionaries. The idea of learning deeper levels of \ndictionaries stems from the recent success of deep learning in \nvarious areas of machine learning. \nThe schematic diagram for dictionary learning is shown in \nFig. 5. X is the data, D is the dictionary and Z is the feature / \nrepresentation of X in D. Dictionary learning follows a \nsynthesis framework, i.e. the dictionary is learnt such that the \nfeatures synthesize the data along with the dictionary.  \nX\nDZ\n\n                    (19) \nThere is also analysis K-SVD, but it cannot be used for feature \nextraction, it can only produce a ‘clean’ version of the data and \nhence is only suitable for inverse problems.  \nIn this work, we propose to extend the shallow (Fig. 3) \ndictionary learning into multiple layers – leading to deep \ndictionary learning (Fig. 6). \n \n \n \nFig. 6. Schematic Diagram for Deep Dictionary Learning \n \nMathematically, the representation at the second layer is \nrepresented as: \nInput Layer \nHidden Layer 1 \nOutput Layer \nHidden Layer L \n……… \nD1\nX\nZ\nD2\nD1\nX\nZ2\n1\n2\n2\nX\nD D Z\n\n                 (20) \nLearning the two dictionaries along with the deepest level \nfeatures is a hard problem for two reasons: \n1) Dictionary learning (19) is a bi-linear (hence non-convex) \nproblem. Learning multiple layers of dictionaries along \nwith the features makes the problem even more difficult to \nsolve. Only recently, studies have proven some \nconvergence guarantees for single level dictionary learning \n[59-63]. These proofs would be very hard to replicate for \nmultiple layers.   \n2) Moreover, the number of parameters required to be solved \nincreases when multiple layers are dictionaries are learnt \nsimultaneously. With limited training data, this could lead \nto over-fitting.  \nHere we propose to learn the dictionaries in a greedy fashion. \nThis is in sync with other deep learning techniques [32-34]. \nMoreover, layer-wise learning will guarantee the convergence \nat each layer. The diagram illustrating layer-wise learning is \nshown in Fig. 5.  \n \n \n \nFig. 7. Greedy Layer-wise Learning \n \nIn a greedy fashion, we start with the first layer, i.e. we solve \nfor D1 and Z1 from –  \n1 1\nX\nD Z\n\n                  (21) \nThe features from the first layer (Z1) acts as input to the second \nlayer. Therefore the second layer learns the weights from –  \n1\n2\n2\nZ\nD Z\n\n                  (22) \nThe learning can be either dense or sparse, i.e. the features / \nrepresentation can be dense or sparse. For dense features, the \nlearning is simple and is given by (23) \n2\n2\n,\nmin\nD Z X\nDZ\n\n                 (23) \nOptimality of solving (23) by alternating minimization has been \nproven in [56]. Therefore we follow the same. The dictionary D \nand the basis Z is learnt by: \n2\n1\n2\nmin\nk\nk\nZ\nZ\nX\nD\nZ\n\n\n\n             (24a) \n2\n2\nmin\nk\nk\nD\nD\nX\nDZ\n\n\n             (24b) \nThis is simply the method of optimal directions [36]. Both (24a) \nand (24b) are simple least square problems having closed form \nsolutions.  \nFor learning sparse features, one just needs to regularize (23) \nby an l1-norm on the features. This is given by: \n2\n2\n1\n,\nmin\nD Z X\nDZ\nZ\n\n\n\n             (25) \nThis too is solved using alternating minimization.  \n2\n1\n1\n2\nmin\nk\nk\nZ\nZ\nX\nD\nZ\nZ\n\n\n\n\n\n         (26a) \n2\n2\nmin\nk\nk\nD\nD\nX\nDZ\n\n\n             (26b) \nAs before, solving (26b) is simple. It is a least square problem \nhaving a closed form solution. The solution to (26a) although \nnot analytic, is well known in signal processing and machine \nlearning literature. It can solved using the Iterative Soft \nThresholding Algorithm (ISTA) [64]. In every iteration, the \nsteps for ISTA are: \n\n\n1\n1\n1\n( )max 0,\n2\nT\nk\nk\nB\nZ\nD\nX\nD\nZ\nZ\nsignum B\nB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \nIn this work, we have used dense dictionary learning for all \nlayers till the penultimate layer and sparse dictionary learning \nonly in the final layer, i.e. for the two layer problem, the first \nlayer (D1, Z1) would be dense and the second layer (D2, Z2) \nwould be sparse.  \nIt must be noted that the two dictionaries cannot be collapsed \ninto a single one. This is because the learning process is non-\nlinear. For example, if the dimensionality of the sample is m \nand the first dictionary is of size m x n1 and the second one is \nn1 x n2, it is not possible to learn a single dictionary of size m x \nn2 and expect the same results as a two-stage dictionary.  \nA. Connection with RBM \nRBM is an undirectional graph, whereas dictionary learning \nis unidirectional. This is evident from figures 1 and 5. In both \ncases, the task is to learn the network weights / atoms and the \nrepresentation given the data. They differ from each other in the \ncost functions used. For RBM it is the Boltzmann function. \nHere one tries to learn the network weight and the output \nfeatures such that the similarity between the projected data (at \nthe input) and the features is maximized.   \nIn dictionary learning, the cost function is different – instead \nof maximizing similarity, we minimize the Euclidean distance \nbetween the data (X) and the synthesis (DZ). RBM has a \nstochastic formulation; dictionary learning is deterministic.  \nRBMs can be formulated for features having values between \n0 and 1. If the values are outside this range, they need to be \nnormalized. In many cases, the normalization does not affect \nthe performance, but there can be scenarios where it suppresses \nimportant information. Dictionary learning can work both on \nreal and complex inputs.    \nB. Connection with Autoencoder \nWe \nmentioned \nbefore \nthat \ndictionary \nlearning \nis \npredominantly modeled as a synthesis problem, i.e. the \ndictionary and the features are learnt such that they can \nsynthesize the data. It is expressed as: X=DSZ where X is the \ndata, DS is the learnt synthesis dictionary and Z are the sparse \ncoefficients.  \nUsually one promotes sparsity in the features and the learning \nrequires minimizing the following, \n2\n1\nF\nS\nX\nD Z\nZ\n\n\n\n               (26) \nThis is the so called synthesis prior formulation where the \nD2\nD1\nX\nZ2\nZ1\ntask is to find a dictionary that will synthesize / generate signals \nfrom sparse features. There is an alternate co-sparse analysis \nprior dictionary learning paradigm [65] where the goal is to \nlearn a dictionary such that when it is applied on the data the \nresulting coefficient is sparse. The model is \nˆ\nA\nD X\nZ\n\n. The \ncorresponding learning problem is framed minimizing: \n2\n1\nˆ\nˆ\nA\nF\nX\nX\nD X\n\n\n\n              (27) \nIf \nwe \ncombine \nanalysis \nand \nsynthesis, \nusing \nˆ\nˆ\n,\nA\nS\nX\nD Z D X\nZ\n\n\nand impute it in (27) we get –  \n2\n1\nˆ\nˆ\nS\nF\nA\nA\nX\nD D X\nD X\n\n\n\n            (28) \nThis is the expression of a sparse denoising autoencoder [54] \nwith linear activation at the hidden layer. If we drop the sparsity \nterm, it becomes –  \n2\nˆ\nA\nS\nF\nX\nD D X\n\n                (29) \nThis formulation is similar to a denoising autoencoder with \nlinear activation.   \nWe can express autoencoder in the lingo of dictionary \nlearning – autoencoder is a model that learnt the analysis and \nthe synthesis dictionaries. To the best of our knowledge, this is \nthe first work which shows the architectural similarity between \nautoencoders and dictionary learning.  \nIV. EXPERIMENTAL EVALUATION \nA. Datasets \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 8. Top to bottom. basic, basic-rot, bg-rand, bg-img, bg-img-rot \n \nWe carried our experiments on several benchmarks datasets. \nThe first one is MNIST dataset which consists of 28x28 images \nof handwritten digits ranging from 0 to 9. The dataset has \n60,000 images for training and 10,000 images for testing. No \npreprocessing has been done on this dataset. \nThe other datasets are variations of MNIST, which are more \nchallenging primarily because they have fewer training samples \n(10,000) and larger number of test samples (50,000).  \n1. basic (smaller subset of MNIST) \n2. basic-rot (smaller subset with random rotations) \n3. bg-rand (smaller subset with uniformly distributed \nnoise in background) \n4. bg-img \n(smaller \nsubset \nwith \nrandom \nimage \nbackground) \n5. bg-img-rot (smaller subset with random image \nbackground plus rotation)  \nSamples for each of the datasets are shown in Fig. 8. \nB. Deep vs Shallow Dictionary Learning \n \n \nFig. 9. First level dictionary for MNIST \n \nIn the first set of results, we show that the multi-level \ndictionaries cannot be collapsed into a single one and expected \nto perform the same. We carried out experiments on the MNIST \nand its variations. In the first case, the number of basis in the \nmulti-level dictionaries are: 300-15-50. In the second case, we \nlearn a shallow dictionary with 50 atoms. The results from these \ntwo would be the same, if the multi-level dictionaries would be \ncollapsible.  \nWe want to show that the representation learnt from a single \nlevel of dictionary and multi-level dictionary are different. To \nshowcase this, we show classification results with a simple K \nNearest Neighbour (K=1). The classification accuracies are \nshown in Table 1.  \nWe use a deterministic initialization for dictionary learning. \nUsually the dictionary atoms are initialized by randomly \nchoosing samples from the training set – but this leads to \nvariability in results. In this work we propose a deterministic \ninitialization based on QR decomposition. Orthogonal vectors \nfrom Q (in order) are used to initialize the dictionary.  \n \n \n \nTABLE I \nDEEP VS SHALLOW \n Dataset \nDeep (300-\n15-50) \nShallow \n(50) \nMNIST \n97.75 \n97.35 \nbasic \n95.80 \n95.02 \nbasic-rot \n87.00 \n84.19 \nbg-rand \n89.35 \n87.19 \nbg-img \n81.00 \n78.86 \nbg-img-rot \n57.77 \n54.40 \n \nThe discrepancy between multi-level dictionary learning and \nsingle level dictionary learning is evident in Table 1. If the \nlearning was linear, it would be possible to collapse multiple \ndictionaries into one; but dictionary learning is inherently non-\nlinear. Hence it is not possible to learn a single layer of \ndictionary in place of multiple levels and expect the same \noutput. \nC. Comparison with other Deep Learning Approaches \nWe compared our results with a stacked autoencoder (SAE) \nand deep belief network (DBN). The implementation for these \nhave been obtained from [66] and [67] respectively. Both SAE \nand DBN has a three layer architecture. The number of nodes is \nhalved in every subsequent layer. This is a standard approach; \nwe tried other configurations but could not improve upon this. \nWe want to compare the representation capability of our \nproposed technique vis-à-vis other deep learning methods. The \nresults for K Nearest Neighbour (KNN) and Support Vector \nMachine (SVM) are shown in Tables 2 and 3.  \n \nTABLE II \nCOMPARISON WITH KNN (K=1) CLASSIFICATION \n Dataset \nDDL \nDBN \nSAE \nMNIST \n97.75 \n97.05 \n97.33 \nbasic \n95.8 \n95.37 \n95.25 \nbasic-rot \n87.00 \n84.71 \n84.83 \nbg-rand \n89.35 \n77.16 \n86.42 \nbg-img \n81.00 \n86.36 \n77.16 \nbg-img-rot \n57.77 \n50.47 \n52.21 \n \nTABLE III \nCOMPARISON WITH SVM CLASSIFICATION \n Dataset \nDDL \nDBN \nSAE \nMNIST \n98.64 \n98.53 \n98.5 \nbasic \n97.284 \n88.44 \n97.4 \nbasic-rot \n90.344 \n76.59 \n79.83 \nbg-rand \n92.38 \n78.59 \n85.34 \nbg-img \n86.17 \n75.22 \n74.99 \nbg-img-rot \n63.85 \n48.53 \n49.14 \n \nWe find that apart from one case each in Tables 1 and 2, our \nproposed method yields better results than DBN and SAE. For \nKNN, our results are slightly better, but for SVM we are doing \nconsiderably better, especially for the more difficult datasets.  \nWe have compared our technique with state-of-the-art \ndictionary learning techniques like D-KSVD [28] and LC-\nKSVD [29]. These were tuned to yield the best possible results. \nComparison is also done with stacked denoising autoencoder \n(SDAE) and deep belief network (DBN) fine tuned with soft-\nmax classifier. We did not run these experiments; these results \nare copied from [35]. \n \nTABLE IV \nCOMPARISON WITH OTHER TECHNIQUES \n Dataset \nDDL-\nSVM \nLC-\nKSVD \nD-\nKSVD \nDBN-\nSM* \nSDAE-\nSM* \nMNIST \n98.64 \n93.30 \n93.6 \n98.76 \n98.72 \nbasic \n97.28 \n92.70 \n92.20 \n96.89 \n97.16 \nbasic-rot \n90.34 \n48.66 \n50.01 \n89.7 \n90.47 \nbg-rand \n92.38 \n87.70 \n87.70 \n93.27 \n89.7 \nbg-img \n86.17 \n80.65 \n81.20 \n83.69 \n83.32 \nbg-img-rot \n63.85 \n75.40 \n75.40 \n52.61 \n56.24 \n*Results from [35] \n \nWe find that the proposed deep dictionary learning \ntechniques always yields better results than shallow dictionary \nlearning (LC-KSVD and D-KSVD). In most cases, we can even \nachieve better accuracy than highly tuned models like DBN and \nSDAE.  \nWe compare our technique with other deep learning \napproaches in terms of speed (training time). All the algorithms \nare run until convergence. SAE, DBN and DDL (proposed) are \nrun until convergence. The machine used is Intel (R) Core(TM) \ni5 running at 3 GHz; 8 GB RAM, Windows 10 (64 bit) running \nMatlab 2014a. The run times for all the smaller MNIST \nvariations are approximately the same. So we only report results \nfor the larger MNIST dataset (60K) and the basic (10K) dataset. \n \nTABLE II \nTRAINING TIME IN SECONDS \n Dataset \nDDL \nDBN \nSAE \nMNIST \n107 \n30071 \n120408 \nbasic \n26 \n \n \n \nWe see that our proposed deep dictionary learning algorithm \nis more than 2 orders of magnitude faster than deep belief \nnetwork and more than 3 orders of magnitude faster than \nstacked autoencoder. This is a huge saving in training time.  \nV. CONCLUSION \nIn this work we propose the idea of deep dictionary learning, \nwhere instead of learning one shallow dictionary – as has been \ndone so far, we learn multiple levels of dictionaries. Learning \nall the dictionaries makes the problem highly non-convex. Also \nlearning so many parameters (atoms of many dictionaries) is \nalways fraught with the problem of over-fitting. To account for \nboth these issues, we learn the dictionaries in a greedy fashion \n– one layer at a time. The representation / feature from one level \nis used as the input to learn the following level. Thus, the basic \nunit of deep dictionary learning is a simple shallow dictionary \nlearning algorithm; which is a well known and solved problem.  \nWe compare the new deep learning tool with the existing \nones like the stacked autoencoder and deep belief network. We \nfind that our method yields better results on benchmark deep-\nlearning datasets. The main advantage of our method is that it \nis few orders of magnitude faster than existing deep learning \ntools like stacked autoencoder and deep belief network.  \nThis is a preliminary work, we will carry out more extensive \nexperimentation in the future. We plan to test the robustness of \ndictionary learning in the presence of missing data, noise and \nlimited number of training sample. \nIn the future, we would also like to apply this technique for \nother practical problems arising, biometrics, vision, speech \nprocessing etc. Also there has been a lot of work on supervised \ndictionary \nlearning; \nour \npreliminary \nformulation \nis \nunsupervised. In future, we expect to improve the results even \nfurther by incorporating techniques from supervised learning.  \nREFERENCES \n[1] B. Olshausen and D. Field, \"Sparse coding with an overcomplete basis \nset: a strategy employed by V1?\", Vision Research, Vol. 37 (23), pp. \n3311-3325, 1997. \n[2] D. D. Lee and H. S. Seung, \"Learning the parts of objects by non-negative \nmatrix factorization\", Nature 401 (6755), pp. 788–791, 1999. \n[3] R. Rubinstein, A. M. Bruckstein and M. Elad, \"Dictionaries for Sparse \nRepresentation Modeling\", Proceedings of the IEEE, Vol. 98 (6), pp. \n1045-1057, 2010 \n[4] M. Aharon, M. Elad and A. Bruckstein, \"K-SVD: An Algorithm for \nDesigning Overcomplete Dictionaries for Sparse Representation\", IEEE \nTransactions on Signal Processing, Vol. 54 (11), pp. 4311-4322, 2006. \n[5] J. Eggert and E. Körner, \"Sparse coding and NMF\", IEEE International \nJoint Conference on Neural Networks, pp. 2529-2533, 2004. \n[6] M. Elad and M. Aharon, \"Image Denoising Via Sparse and Redundant \nRepresentations Over Learned Dictionaries,\" IEEE Transactions on \nImage Processing, Vol.15 (12), pp. 3736-3745, 2006. \n[7] M. Elad and M. Aharon, \"Image Denoising Via Learned Dictionaries and \nSparse representation,\" IEEE Conference on Computer Vision and Pattern \nRecognition, Vol.1, pp. 895-900, 2006. \n[8] M. Protter and M. Elad, \"Image Sequence Denoising via Sparse and \nRedundant Representations,\" IEEE Transactions on Image Processing, \nVol.18 (1), pp. 27-35, 2009. \n[9] K. Min-Sung and E. Rodriguez-Marek, \"Turbo inpainting: Iterative K-\nSVD with a new dictionary,\" IEEE International Workshop on \nMultimedia Signal Processing, pp. 1-6, 2009. \n[10] J. Mairal, M. Elad and G. Sapiro, \"Sparse Representation for Color Image \nRestoration,\" IEEE Transactions on Image Processing, Vol.17 (1), pp. 53-\n69, 2008. \n[11] C.-H. Son and H. Choo, \"Local Learned Dictionaries Optimized to Edge \nOrientation for Inverse Halftoning,\" IEEE Transactions on Image \nProcessing, Vol. 23 (6), pp. 2542-2556, 2014. \n[12] J. Caballero, A. N. Price, D. Rueckert and J. V. Hajnal, \"Dictionary \nLearning and Time Sparsity for Dynamic MR Data Reconstruction,\" \nIEEE Transactions on Medical Imaging, Vol. 33 (4), pp. 979-994, 2014. \n[13] A. Majumdar and R. K. Ward, “Learning Space-Time Dictionaries for \nBlind Compressed Sensing Dynamic MRI Reconstruction”, IEEE \nInternational Conference on Image Processing, 2015. \n[14] A. Majumdar and R. Ward, \"Multiresolution methods in face \nrecognition\", in Recent Advances in Face Recognition, Eds. M. S. \nBartlett, K. Delac and M. Grgic, I-Tech Education and Publishing, \nVienna, Austria, pp. 79-96, 2009. \n[15] V. Dattatray, R. Jadhav and S. Holambe, \"Feature extraction using Radon \nand wavelet transforms with application to face recognition\", \nNeurocomputing, Vol. 72 (7-9), pp. 1951-1959, 2009. \n[16] S. Dabbaghchian, P. M. Ghaemmaghami, A. Aghagolzadeh, “Feature \nextraction using discrete cosine transform and discrimination power \nanalysis with a face recognition technology”, Pattern Recognition, Vol. \n43 (4), pp. 1431-1440, 2010. \n[17] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. \"Discriminative \nlearned dictionaries for local image analysis\". IEEE Conference of \nComputer Vision and Pattern Recognition, 2008. \n[18] L. Yang, R. Jin, R. Sukthankar, and F. Jurie. \"Unifying discriminative \nvisual codebook genearation with classifier training for object category \nrecognition\". IEEE Conference of Computer Vision and Pattern \nRecognition, 2008.  \n[19] W. Jin, L. Wang, X. Zeng, Z. Liu and R. Fu, \"Classification of clouds in \nsatellite \nimagery \nusing \nover-complete \ndictionary \nvia \nsparse \nrepresentation\", Pattern Recognition Letters, Vol. 49 (1), pp. 193-200, \n2014. \n[20] Y. Boureau, F. Bach, Y. LeCun, and J. Ponce. \"Learning mid-level \nfeatures for recognition\". IEEE Conference of Computer Vision and \nPattern Recognition, 2010.  \n[21] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. \"Supervised \ndictionary learning\". Advances in Neural Information Processing \nSystems, 2009.  \n[22] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. \n\"Discriminative sparse image models for class-specific edage detection \nand image interpretation\". European Conference on Computer Vision, \n2008.  \n[23] K. Huang and S. Aviyente. \"Sparse representation for signal \nclassification\". Advances in Neural Information Processing Systems, \n2007.  \n[24] D. Pham and S. Venkatesh. \"Joint learning and dictionary construction for \npattern recognition\". IEEE Conference of Computer Vision and Pattern \nRecognition, 2008.  \n[25] Q. Zhang and B. Li. \"Discriminative k-svd for dictionary learning in face \nrecognition\". IEEE Conference of Computer Vision and Pattern \nRecognition, 2010.  \n[26] J. Yang, K. Yu, and T. Huang. \"Supervised translation-invariant sparse \ncoding\". IEEE Conference of Computer Vision and Pattern Recognition, \n2010.  \n[27] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. \n\"Discriminative sparse image models for class-specific edage detection \nand image interpretation\". European Conference on Computer Vision , \n2008.  \n[28] Q. Zhang and B. Li, \"Discriminative K-SVD for dictionary learning in \nface recognition\". IEEE Conference of Computer Vision and Pattern \nRecognition, 2010. \n[29] Z. Jiang, Z. Lin and L. S. Davis, \"Learning A Discriminative Dictionary \nfor Sparse Coding via Label Consistent K-SVD\", IEEE Transactions on \nPattern Analysis and Machine Intelligence, Vol. 35, pp. 2651-2664, 2013 \n[30] G. E. Hinton and R. R. Salakhutdinov, \"Reducing the Dimensionality of \nData with Neural Networks\", Science, Vol. 313 (5786), pp. 504–507, \n2006.  \n[31] H. Bourlard and Y. Kamp, \"Auto-association by multilayer perceptrons \nand singular value decomposition\". Biological Cybernetics, Vol. 59 (4–\n5), pp. 291–294, 1989. \n[32] Y. Bengio, P. Lamblin, P. Popovici and H. Larochelle, “Greedy Layer-\nWise Training of Deep Networks”, Advances in Neural Information \nProcessing Systems, 2007. \n[33] G. E. Hinton, S. Osindero and Y. W. Teh, “A fast learning algorithm for \ndeep belief nets”, Neural Computation, Vol. 18, pp. 1527-1554, 2006. \n[34] Y. Bengio, “Learning deep architectures for AI”, Foundations and Trends \nin Machine Learning, Vol. 1(2), pp. 1-127, 2009. \n[35] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio and P. A. Manzagol, \n“Stacked denoising autoencoders: Learning useful representations in a \ndeep network with a local denoising criterion”, Journal of Machine \nLearning Research, Vol. 11, pp. 3371-3408, 2010. \n[36] K. Engan, S. Aase, and J. Hakon-Husoy, “Method of optimal directions \nfor frame design,” IEEE International Conference on Acoustics, Speech, \nand Signal Processing, 1999. \n[37] B. K. Natarajan, \"Sparse approximate solutions to linear systems\", SIAM \nJournal on computing, Vol. 24, pp. 227-234, 1995. \n[38] Y. Pati, R. Rezaiifar, P. Krishnaprasad, \"Orthogonal Matching Pursuit : \nrecursive function approximation with application to wavelet \ndecomposition\", Asilomar Conference on Signals, Systems and \nComputers, 1993. \n[39] M. Yaghoobi, T. Blumensath and M. E. Davies, \"Dictionary Learning for \nSparse Approximations With the Majorization Method,\" IEEE \nTransactions on Signal Processing, Vol.57 (6), pp. 2178-2191, 2009. \n[40] A. Rakotomamonjy, \"Applying alternating direction method of \nmultipliers for constrained dictionary learning\", Neurocomputing, Vol. \n106, pp. 126-136, 2013. \n[41] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. “Robust face \nrecognition via sparse representation”. IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, Vol. 31, 2, pp. 210–227, 2009. \n[42] A. Majumdar and R. K. Ward, \"Robust Classifiers for Data Reduced via \nRandom Projections\", IEEE Transactions on Systems, Man, and \nCybernetics, Part B, Vol. 40 (5), pp. 1359 - 1371. \n[43] A. Majumdar and R. K. Ward, \"Fast Group Sparse Classification\", IEEE \nCanadian Journal of Electrical and Computer Engineering, Vol. 34 (4), \npp. 136-144, 2009 \n[44] A. Majumdar and R. K. Ward, \"Improved Group Sparse Classifier\", \nPattern Recognition Letters, Vol. 31 (13), pp. 1959-1964, 2010 \n[45] J. Yin, Z. Liu, Z. Jin and W. Yang, \"Kernel sparse representation based \nclassification\", Neurocomputing, Vol. 77 (1), pp. 120-128, 2012. \n[46] Y. Chen, N. M. Nasrabadi, T. D. Tran, \"Hyperspectral Image \nClassification via Kernel Sparse Representation,\" IEEE Transactions on \nGeoscience and Remote Sensing, Vol. 51 (1), pp. 217-231, 2013. \n[47] L. Zhang, W. D. Zhou, P. C. Chang, J. Liu, Z. Yan, T. Wang and F. Z. Li, \n\"Kernel Sparse Representation-Based Classifier,\" IEEE Transactions on \nSignal Processing, Vol. 60 (4), pp. 1684-1695, 2012 \n[48] M. Yang, L. Zhang, J. Yang, and D. Zhang. metaface learning for sparse \nrepresentation based face recognition. IEEE International Conference on \nImage Processing, 2010. \n[49] I. Ramirez, P. Sprechmann, and G. Sapiro. Classification and clustering \nvia dictionary learning with structured incoherence and shared features. \nIEEE Conference of Computer Vision and Pattern Recognition, 2010. \n[50] M. Yang, L. Zhang, X. Feng, and D. Zhang. Fisher discrimination \ndictionary learning for sparse representation. IEEE International \nConference on Computer Vision, 2011. \n[51] H. Larochelle and Y. Bengio, “Classification using Discriminative \nRestricted Boltzmann Machines”, International Conference on Machine \nLearning, 2008. \n[52] Z. Cui, S. S. Ge, Z. Cao, J. Yang and H. Ren, “Analysis of Different \nSparsity Methods in Constrained RBM for Sparse Representation in \nCognitive Robotic Perception”, Journal of Intelligent Robot and Systems, \npp. 1-12, 2015. \n[53] H. Luo, R. Shen and C. Niu, “Sparse Group Restricted Boltzmann \nMachines”, arXiv:1008.4988v1 \n[54] R. Salakhutdinov and G. Hinton, “Deep Boltzmann Machines”, \nInternational Conference on Artificial Intelligence and Statistics, 2009. \n[55] K. H. Cho, T. Raiko and A. Ilin, \"Gaussian-Bernoulli deep Boltzmann \nmachine,\" IEEE International Joint Conference on Neural Networks, \n2013, pp.1-7, 2013. \n[56] A. Makhzani and B. Frey, \"k-Sparse Autoencoders\", arXiv:1312.5663, \n2013. \n[57] K. Cho, \"Simple sparsification improves sparse denoising autoencoders \nin denoising highly noisy images\", International Conference on Machine \nLearning, 2013. \n[58] S. Rifai, P. Vincent, X. Muller, X. Glorot, Y. Bengio: Contractive Auto-\nEncoders: Explicit Invariance During Feature Extraction, International \nConference on Machine Learning, 2011 \n[59] P. Jain, P. Netrapalli and S. Sanghavi, “Low-rank Matrix Completion \nusing Alternating Minimization”, Symposium on Theory Of Computing, \n2013. \n[60] A. Agarwal, A. Anandkumar, P. Jain and P. Netrapalli, “Learning \nSparsely Used Overcomplete Dictionaries via Alternating Minimization”, \nInternational Conference On Learning Theory, 2014. \n[61] D. A. Spielman, H. Wang and J. Wright, “Exact Recovery of Sparsely-\nUsed Dictionaries”, International Conference On Learning Theory, 2012 \n[62] S. Arora, A. Bhaskara, R. Ge and T. Ma, “More Algorithms for Provable \nDictionary Learning”, arXiv:1401.0579v1 \n[63] C. Hillar and F. T. Sommer, “When can dictionary learning uniquely \nrecover sparse data from subsamples?”, arXiv:1106.3616v3 \n[64] I. Daubechies, M. Defrise, C. De Mol, \"An iterative thresholding \nalgorithm for linear inverse problems with a sparsity constraint\", \nCommunications on Pure and Applied Mathematics, Vol. 57: 1413-1457, \n2004. \n[65] R. Rubinstein, T. Peleg and M. Elad, Analysis K-SVD: A Dictionary-\nLearning Algorithm for the Analysis Sparse Model, IEEE Transactions \non Signal Processing, Vol. 61 (3), pp. 661-677, 2013. \n[66] http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html \n[67] http://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html \n \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2016-01-31",
  "updated": "2016-01-31"
}