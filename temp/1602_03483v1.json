{
  "id": "http://arxiv.org/abs/1602.03483v1",
  "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
  "authors": [
    "Felix Hill",
    "Kyunghyun Cho",
    "Anna Korhonen"
  ],
  "abstract": "Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.",
  "text": "arXiv:1602.03483v1  [cs.CL]  10 Feb 2016\nLearning Distributed Representations of Sentences from Unlabelled Data\nFelix Hill\nComputer Laboratory\nUniversity of Cambridge\nfelix.hill@cl.cam.ac.uk\nKyunghyun Cho\nCourant Institute of\nMathematical Sciences\n& Centre for Data Science\nNew York University\nkyunghyun.cho@nyu.edu\nAnna Korhonen\nDepartment of Theoretical\n& Applied Linguistics\nUniversity of Cambridge\nalk23@cam.ac.uk\nAbstract\nUnsupervised methods for learning distributed\nrepresentations of words are ubiquitous in to-\nday’s NLP research, but far less is known\nabout the best ways to learn distributed phrase\nor sentence representations from unlabelled\ndata. This paper is a systematic comparison\nof models that learn such representations. We\nﬁnd that the optimal approach depends crit-\nically on the intended application.\nDeeper,\nmore complex models are preferable for rep-\nresentations to be used in supervised sys-\ntems, but shallow log-linear models work best\nfor building representation spaces that can\nbe decoded with simple spatial distance met-\nrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to\noptimise the trade-off between training time,\ndomain portability and performance.\n1\nIntroduction\nDistributed representations - dense real-valued vec-\ntors that encode the semantics of linguistic units\n- are ubiquitous in today’s NLP research.\nFor\nsingle-words or word-like entities, there are estab-\nlished ways to acquire such representations from\nnaturally occurring (unlabelled) training data based\non comparatively task-agnostic objectives (such as\npredicting adjacent words).\nThese methods are\nwell understood empirically (Baroni et al., 2014b)\nand theoretically (Levy and Goldberg, 2014).\nThe\nbest word representation spaces reﬂect consistently-\nobserved aspects of human conceptual organisa-\ntion (Hill et al., 2015b), and can be added as features\nto improve the performance of numerous language\nprocessing systems (Collobert et al., 2011).\nBy contrast, there is comparatively little consen-\nsus on the best ways to learn distributed represen-\ntations of phrases or sentences.1\nWith the advent\nof deeper language processing techniques, it is rel-\natively common for models to represent phrases or\nsentences as continuous-valued vectors. Examples\ninclude machine translation (Sutskever et al., 2014),\nimage captioning (Mao et al., 2015) and dialogue\nsystems (Serban et al., 2015). While it has been ob-\nserved informally that the internal sentence repre-\nsentations of such models can reﬂect semantic in-\ntuitions (Cho et al., 2014), it is not known which ar-\nchitectures or objectives yield the ‘best’ or most use-\nful representations. Resolving this question could\nultimately have a signiﬁcant impact on language\nprocessing systems. Indeed, it is phrases and sen-\ntences, rather than individual words, that encode the\nhuman-like general world knowledge (or ‘common\nsense’) (Norman, 1972) that is a critical missing part\nof most current language understanding systems.\nWe address this issue with a systematic compari-\nson of cutting-edge methods for learning distributed\nrepresentations of sentences. We constrain our com-\nparison to methods that do not require labelled data\ngathered for the purpose of training models, since\nsuch methods are more cost-effective and applica-\nble across languages and domains. We also propose\ntwo new phrase or sentence representation learn-\ning objectives - Sequential Denoising Autoencoders\n1See\nthe\ncontrasting\nconclusions\nin\n(Mitchell and Lapata, 2008;\nClark and Pulman, 2007;\nBaroni et al., 2014a; Milajevs et al., 2014) among others.\n(SDAEs) and FastSent, a sentence-level log-linear\nbag-of-words model. We compare all methods on\ntwo types of task - supervised and unsupervised\nevaluations - reﬂecting different ways in which rep-\nresentations are ultimately to be used. In the former\nsetting, a classiﬁer or regression model is applied\nto representations and trained with task-speciﬁc la-\nbelled data, while in the latter, representation spaces\nare directly queried using cosine distance.\nWe observe notable differences in approaches de-\npending on the nature of the evaluation metric. In\nparticular, deeper or more complex models (which\nrequire greater time and resources to train) gener-\nally perform best in the supervised setting, whereas\nshallow log-linear models work best on unsuper-\nvised benchmarks. Speciﬁcally, SkipThought Vec-\ntors (Kiros et al., 2015) perform best on the major-\nity of supervised evaluations, but SDAEs are the top\nperformer on paraphrase identiﬁcation. In contrast,\non the (unsupervised) SICK sentence relatedness\nbenchmark, FastSent, a simple, log-linear variant of\nthe SkipThought objective, performs better than all\nother models. Interestingly, the method that exhibits\nstrongest performance across both supervised and\nunsupervised benchmarks is a bag-of-words model\ntrained to compose word embeddings using dictio-\nnary deﬁnitions (Hill et al., 2015a). Taken together,\nthese ﬁndings constitute valuable guidelines for the\napplication of phrasal or sentential representation-\nlearning to language understanding systems.\n2\nDistributed Sentence Representations\nTo constrain the analysis, we compare neural lan-\nguage models that compute sentence representations\nfrom unlabelled, naturally-ocurring data, as with\nthe predominant methods for word representations.2\nLikewise, we do not focus on ‘bottom up’ models\nwhere phrase or sentence representations are built\nfrom ﬁxed mathematical operations on word vec-\ntors (although we do consider a canonical case -\nsee CBOW below); these were already compared\nby Milajevs et al. (2014). Most space is devoted to\nour novel approaches, and we refer the reader to the\noriginal papers for more details of existing models.\n2This\nexcludes\ninnovative\nsupervised\nsentence-\nlevel\narchitectures\nincluding\n(Socher et al., 2011;\nKalchbrenner et al., 2014) and many others.\n2.1\nExisting Models Trained on Text\nSkipThought Vectors For consecutive sentences\nSi−1, Si, Si+1 in some document, the SkipThought\nmodel (Kiros et al., 2015) is trained to predict target\nsentences Si−1 and Si+1 given source sentence Si.\nAs with all sequence-to-sequence models, in train-\ning the source sentence is ‘encoded’ by a Recurrent\nNeural Network (RNN) (with Gated Recurrent uU-\nnits (Cho et al., 2014)) and then ‘decoded’ into the\ntwo target sentences in turn. Importantly, because\nRNNs employ a single set of update weights at each\ntime-step, both the encoder and decoder are sensitive\nto the order of words in the source sentence.\nFor each position in a target sentence St, the\ndecoder computes a softmax distribution over the\nmodel’s vocabulary. The cost of a training exam-\nple is the sum of the negative log-likelihood of each\ncorrect word in the target sentences Si−1 and Si+1.\nThis cost is backpropagated to train the encoder (and\ndecoder), which, when trained, can map sequences\nof words to a single vector.\nParagraphVector Le and Mikolov (2014) proposed\ntwo log-linear models of sentence representation.\nThe DBOW model learns a vector s for every sen-\ntence S in the training corpus which, together with\nword embeddings vw, deﬁne a softmax distribution\noptimised to predict words w ∈S given S. The\nvw are shared across all sentences in the corpus.\nIn the DM model, k-grams of consecutive words\n{wi . . . wi+k ∈S} are selected and s is combined\nwith {vwi . . . vwi+k} to make a softmax prediction\n(parameterised by additional weights) of wi+k+1.\nWe used the Gensim implementation,3 treating\neach sentence in the training data as a ‘paragraph’ as\nsuggested by the authors. During training, both DM\nand DBOW models store representations for every\nsentence (as well as word) in the training corpus.\nEven on large servers it was therefore only possi-\nble to train models with representation size 200, and\nDM models whose combination operation was aver-\naging (rather than concatenation).\nBottom-Up Methods We train CBOW and Skip-\nGram word embeddings (Mikolov et al., 2013b) on\nthe Books corpus, and compose by elementwise ad-\n3https://radimrehurek.com/gensim/\ndition as proposed by Mitchell and Lapata (2010).4\nWe\nalso\ncompare\nto\nC-\nPHRASE (Pham et al., 2015), an approach that\nexploits a (supervised) parser to infer distributed\nsemantic representations based on a syntactic parse\nof sentences. C-PHRASE achieves state-of-the-art\nresults for distributed representations on several\nevaluations used in this study.5\nNon-Distributed Baseline We implement a TFIDF\nBOW model in which the representation of sentence\nS encodes the count in S of a set of feature-words\nweighted by their tﬁdf in C, the corpus. The feature-\nwords are the 200,000 most common words in C.\n2.2\nModels Trained on Structured Resources\nThe following models rely on (freely-available) data\nthat has more structure than raw text.\nDictRep Hill et al. (2015a) trained neural language\nmodels to map dictionary deﬁnitions to pre-trained\nword embeddings of the words deﬁned by those def-\ninitions. They experimented with BOW and RNN\n(with LSTM) encoding architectures and variants\nin which the input word embeddings were either\nlearned or pre-trained (+embs.) to match the tar-\nget word embeddings. We implement their models\nusing the available code and training data.6\nCaptionRep Using the same overall architecture,\nwe trained (BOW and RNN) models to map cap-\ntions in the COCO dataset (Chen et al., 2015) to\npre-trained vector representations of images. The\nimage representations were encoded by a deep\nconvolutional network (Szegedy et al., 2014) trained\non the ILSVRC 2014 object recognition task\n(Russakovsky et al., 2014). Multi-modal distributed\nrepresentations can be encoded by feeding test sen-\ntences forward through the trained model.\nNMT We consider the sentence representations\nlearned by neural MT models.\nThese models\n4We also tried multiplication but this gave very poor results.\n5Since\ncode\nfor\nC-PHRASE\nis\nnot\npublicly-\navailable\nwe\nuse\nthe\navailable\npre-trained\nmodel\n(http://clic.cimec.unitn.it/composes/cphrase-vectors.html).\nNote this model is trained on 3× more text than others in this\nstudy.\n6https://www.cl.cam.ac.uk/˜fh295/.\nDeﬁnitions\nfrom the training data matching those in the WordNet STS 2014\nevaluation (used in this study) were excluded.\nhave identical architecture to SkipThought, but are\ntrained on sentence-aligned translated texts.\nWe\nused a standard architecture (Cho et al., 2014) on\nall available En-Fr and En-De data from the 2015\nWorkshop on Statistical MT (WMT).7\n2.3\nNovel Text-Based Models\nWe introduce two new approaches designed to ad-\ndress certain limitations with the existing models.\nSequential\n(Denoising)\nAutoencoders\nThe\nSkipThought objective requires training text with\na coherent\ninter-sentence\nnarrative,\nmaking it\nproblematic to port to domains such as social media\nor artiﬁcial language generated from symbolic\nknowledge. To avoid this restriction, we experiment\nwith\na\nrepresentation-learning\nobjective\nbased\non denoising autoencoders (DAEs).\nIn a DAE,\nhigh-dimensional input data is corrupted according\nto some noise function, and the model is trained to\nrecover the original data from the corrupted version.\nAs a result of this process, DAEs learn to repre-\nsent the data in terms of features that explain its\nimportant factors of variation (Vincent et al., 2008).\nTransforming data into DAE representations (as\na ‘pre-training’ or initialisation step) gives more\nrobust (supervised) classiﬁcation performance in\ndeep feedforward networks (Vincent et al., 2010).\nThe original DAEs were feedforward nets applied\nto (image) data of ﬁxed size. Here, we adapt the ap-\nproach to variable-length sentences by means of a\nnoise function N(S|po, px), determined by free pa-\nrameters po, px ∈[0, 1]. First, for each word w in\nS, N deletes w with (independent) probability po.\nThen, for each non-overlapping bigram wiwi+1 in\nS, N swaps wi and wi+1 with probability px. We\nthen train the same LSTM-based encoder-decoder\narchitecture as NMT, but with the denoising objec-\ntive to predict (as target) the original source sentence\nS given a corrupted version N(S|po, px) (as source).\nThe trained model can then encode novel word se-\nquences into distributed representations.\nWe call\nthis model the Sequential Denoising Autoencoder\n(SDAE). Note that, unlike SkipThought, SDAEs can\nbe trained on sets of sentences in arbitrary order.\nWe label the case with no noise (i.e.\npo\n=\npx\n=\n0 and N\n≡\nid) SAE. This set-\n7www.statmt.org/wmt15/translation-task.html\nting matches the method applied to text classi-\nﬁcation tasks by Dai and Le (2015).\nThe ‘word\ndropout’ effect when po\n≥\n0 has also been\nused as a regulariser for deep nets in supervised\nlanguage tasks (Iyyer et al., 2015), and for large\npx the objective is similar to word-level ‘debag-\nging’ (Sutskever et al., 2011).\nFor the SDAE, we\ntuned po, px on the validation set (see Section 3.2).8\nWe also tried a variant (+embs) in which words are\nrepresented by (ﬁxed) pre-trained embeddings.\nFastSent The performance of SkipThought vec-\ntors\nshows\nthat\nrich\nsentence\nsemantics\ncan\nbe inferred from the content of adjacent sen-\ntences.\nThe model could be said to exploit\na type of sentence-level Distributional Hypothe-\nsis (Harris, 1954;\nPolajnar et al., 2015).\nNever-\ntheless, like many deep neural language models,\nSkipThought is very slow to train (see Table 1).\nFastSent is a simple additive (log-linear) sentence\nmodel designed to exploit the same signal, but\nat much lower computational expense.\nGiven a\nBOW representation of some sentence in context,\nthe model simply predicts adjacent sentences (also\nrepresented as BOW) .\nMore formally, FastSent learns a source uw and\ntarget vw embedding for each word in the model vo-\ncabulary. For a training example Si−1, Si, Si+1 of\nconsecutive sentences, Si is represented as the sum\nof its source embeddings si = P\nw∈Si uw. The cost\nof the example is then simply:\nX\nw∈Si−1∪Si+1\nφ(si, vw)\n(1)\nwhere φ(v1, v2) is the softmax function.\nWe also experiment with a variant (+AE) in which\nthe encoded (source) representation must predict its\nown words as target in addition to those of adjacent\nsentences. Thus in FastSent+AE, (1) becomes\nX\nw∈Si−1∪Si∪Si+1\nφ(si, vw).\n(2)\nAt test time the trained model (very quickly) en-\ncodes unseen word sequences into distributed rep-\nresentations with s = P\nw∈S uw.\n8We searched po, px ∈{0.1, 0.2, 0.3} and observed best\nresults with po = px = 0.1.\nOS\nR\nWO\nSD\nWD\nTR\nTE\nS(D)AE\n✓\n2400\n100\n72*\n640\nParagraphVec\n100\n100\n4\n1130\nCBOW\n500\n500\n2\n145\nSkipThought\n✓\n✓\n4800\n620\n336*\n890\nFastSent\n✓\n100\n100\n2\n140\nDictRep\n✓\n✓\n500\n256\n24*\n470\nCaptionRep\n✓\n✓\n500\n256\n24*\n470\nNMT\n✓\n✓\n2400\n512\n72*\n720\nTable 1:\nProperties of models compared in this study\nOS: requires training corpus of sentences in order. R: requires\nstructured resource for training. WO: encoder sensitive to word\norder. SD: dimension of sentence representation. WD: dimen-\nsion of word representation. TR: approximate training time\n(hours) on the dataset in this paper. * indicates trained on GPU.\nTE: approximate time (s) taken to encode 0.5m sentences.\n2.4\nTraining and Model Selection\nUnless stated above, all models were trained on\nthe Toronto Books Corpus,9 which has the inter-\nsentential coherence required for SkipThought and\nFastSent. The corpus consists of 70m ordered sen-\ntences from over 7,000 books.\nSpeciﬁcations of the models are shown in Ta-\nble 1. The log-linear models (SkipGram, CBOW,\nParagraphVec and FastSent) were trained for one\nepoch on one CPU core.\nThe representation di-\nmension d for these models was found after tun-\ning d ∈{100, 200, 300, 400, 500} on the validation\nset.10 All other models were trained on one GPU.\nThe S(D)AE models were trained for one epoch\n(≈8 days). The SkipThought model was trained\nfor two weeks, covering just under one epoch.11 For\nCaptionRep and DictRep, performance was mon-\nitored on held-out training data and training was\nstopped after 24 hours after a plateau in cost. The\nNMT models were trained for 72 hours.\n3\nEvaluating Sentence Representations\nIn previous work, distributed representations of\nlanguage were evaluated either by measuring the\neffect of adding representations as features in\n9http://www.cs.toronto.edu/˜mbweb/\n10For ParagraphVec only d ∈{100, 200} was possible due\nto the high memory footprint.\n11Downloaded from https://github.com/ryankiros/skip-thoughts\nDataset\nSentence 1\nSentence 2\n/5\nNews\nMexico wishes to guarantee citizens’ safety.\nMexico wishes to avoid more violence.\n4\nForum\nThe problem is simpler than that.\nThe problem is simple.\n3.8\nSTS\nWordNet\nA social set or clique of friends.\nAn unofﬁcial association of people or groups.\n3.6\n2014\nTwitter\nTaking Aim #Stopgunviolence #Congress #NRA\nObama, Gun Policy and the N.R.A.\n1.6\nImages\nA woman riding a brown horse.\nA young girl riding a brown horse.\n4.4\nHeadlines\nIranians Vote in Presidential Election.\nKeita Wins Mali Presidential Election.\n0.4\nSICK (test+train)\nA lone biker is jumping in the air.\nA man is jumping into a full pool.\n1.7\nTable 2: Example sentence pairs and ‘similarity’ ratings from the unsupervised evaluations used in this study.\nsome classiﬁcation task - supervised evaluation\n(Collobert et al., 2011;\nMikolov et al., 2013a;\nKiros et al., 2015) - or by comparing with human\nrelatedness\njudgements\n-\nunspervised\nevalu-\nation\n(Hill et al., 2015a;\nBaroni et al., 2014b;\nLevy et al., 2015).\nThe former setting reﬂects\na scenario in which representations are used to\ninject general knowledge (sometimes considered\nas pre-training) into a supervised model.\nThe\nlatter pertains to applications in which the sentence\nrepresentation space is used for direct comparisons,\nlookup or retrieval. Here, we apply and compare\nboth evaluation paradigms.\n3.1\nSupervised Evaluations\nRepresentations are applied to 6 sentence classi-\nﬁcation tasks:\nparaphrase identiﬁcation (MSRP)\n(Dolan et al., 2004),\nmovie\nreview\nsentiment\n(MR)\n(Pang and Lee, 2005),\nproduct\nreviews\n(CR) (Hu and Liu, 2004), subjectivity classiﬁcation\n(SUBJ)\n(Pang and Lee, 2004),\nopinion\npolarity\n(MPQA) (Wiebe et al., 2005) and question type\nclassiﬁcation (TREC) (Voorhees, 2002). We follow\nthe procedure (and code) of Kiros et al. (2015): a\nlogistic regression classiﬁer is trained on top of sen-\ntence representations, with 10-fold cross-validation\nused when a train-test split is not pre-deﬁned.\n3.2\nUnsupervised Evaluations\nWe also measure how well representation spaces\nreﬂect human intuitions of the semantic sen-\ntence relatedness, by computing the cosine dis-\ntance between vectors for the two sentences in\neach test pair,\nand correlating these distances\nwith gold-standard human judgements. The SICK\ndataset (Marelli et al., 2014) consists of 10,000 pairs\nof sentences and relatedness judgements. The STS\n2014 dataset (Agirre et al., 2014) consists of 3,750\npairs and ratings from six linguistic domains. Exam-\nple ratings are shown in Table 2. All available pairs\nare used for testing apart from the 500 SICK ‘trial’\npairs, which are held-out for tuning hyperparameters\n(representation size of log-linear models, and noise\nparameters in SDAE). The optimal settings on this\ntask are then applied to both supervised and unsu-\npervised evaluations.\n4\nResults\nPerformance of the models on the supervised eval-\nuations (grouped according to the data required\nby their objective) is shown in Table 3.\nOverall,\nSkipThought vectors perform best on three of the\nsix evaluations, the BOW DictRep model with pre-\ntrained word embeddings performs best on two, and\nthe SDAE on one. SDAEs perform notably well on\nthe paraphrasing task, going beyond SkipThought\nby three percentage points and approaching state-\nof-the-art performance of models designed speciﬁ-\ncally for the task (Ji and Eisenstein, 2013). SDAE\nis also consistently better than SAE, which aligns\nwith other ﬁndings that adding noise to AEs pro-\nduces richer representations (Vincent et al., 2008).\nResults on the unsupervised evaluations are\nshown in Table 4. The same DictRep model per-\nforms best on four of the six STS categories (and\noverall) and is joint-top performer on SICK. Of\nthe models trained on raw text, simply adding\nCBOW word vectors works best on STS. The best\nperforming raw text model on SICK is FastSent,\nwhich achieves almost identical performance to C-\nPHRASE’s state-of-the-art performance for a dis-\ntributed model (Pham et al., 2015). Further, it uses\nless than a third of the training text and does not\nrequire access to (supervised) syntactic representa-\ntions for training. Together, the results of FastSent\non the unsupervised evaluations and SkipThought\nData\nModel\nMSRP (Acc / F1)\nMR\nCR\nSUBJ\nMPQA\nTREC\nSAE\n74.3 / 81.7\n62.6\n68.0\n86.1\n76.8\n80.2\nSAE+embs.\n70.6 / 77.9\n73.2\n75.3\n89.8\n86.2\n80.4\nUnordered\nSDAE\n76.4 / 83.4\n67.6\n74.0\n89.3\n81.3\n77.6\nSentences\nSDAE+embs.\n73.7 / 80.7\n74.6\n78.0\n90.8\n86.9\n78.4\n(Toronto Books:\nParagraphVec DBOW\n72.9 / 81.1\n60.2\n66.9\n76.3\n70.7\n59.4\n70m sents,\nParagraphVec DM\n73.6 / 81.9\n61.5\n68.6\n76.4\n78.1\n55.8\n0.9B words)\nSkipgram\n69.3 / 77.2\n73.6\n77.3\n89.2\n85.0\n82.2\nCBOW\n67.6 / 76.1\n73.6\n7730\n89.1\n85.0\n82.2\nUnigram TFIDF\n73.6 / 81.7\n73.7\n79.2\n90.3\n82.4\n85.0\nOrdered\nSkipThought\n73.0 / 82.0\n76.5\n80.1\n93.6\n87.1\n92.2\nSentences\nFastSent\n72.2 / 80.3\n70.8\n78.4\n88.7\n80.6\n76.8\n(Toronto Books)\nFastSent+AE\n71.2 / 79.1\n71.8\n76.7\n88.8\n81.5\n80.4\nNMT En to Fr\n69.1 / 77.1\n64.7\n70.1\n84.9\n81.5\n82.8\nOther\nNMT En to De\n65.2 / 73.3\n61.0\n67.6\n78.2\n72.9\n81.6\nstructured\nCaptionRep BOW\n73.6 / 81.9\n61.9\n69.3\n77.4\n70.8\n72.2\ndata\nCaptionRep RNN\n72.6 / 81.1\n55.0\n64.9\n64.9\n71.0\n62.4\nresource\nDictRep BOW\n73.7 / 81.6\n71.3\n75.6\n86.6\n82.5\n73.8\nDictRep BOW+embs.\n68.4 / 76.8\n76.7\n78.7\n90.7\n87.2\n81.0\nDictRep RNN\n73.2 / 81.6\n67.8\n72.7\n81.4\n82.5\n75.8\nDictRep RNN+embs.\n66.8 / 76.0\n72.5\n73.5\n85.6\n85.7\n72.0\n2.8B words\nCPHRASE\n72.2 / 79.6\n75.7\n78.8\n91.1\n86.2\n78.8\nTable 3:\nPerformance of sentence representation models on supervised evaluations (Section 3.1). Bold numbers indicate best\nperformance in class. Underlined indicates best overall.\non the supervised benchmarks provide strong sup-\nport for the sentence-level distributional hypothe-\nsis: the context in which a sentence occurs provides\nvaluable information about its semantics.\nAcross both unsupervised and supervised evalua-\ntions, the BOW DictRep with pre-trained word em-\nbeddings exhibits by some margin the most con-\nsistent performance. Ths robust performance sug-\ngests that DictRep representations may be particu-\nlarly valuable when the ultimate application is non-\nspeciﬁc or unknown, and conﬁrms that dictionary\ndeﬁnitions (where available) can be a powerful re-\nsource for representation learning.\n5\nDiscussion\nMany additional conclusions can be drawn from the\nresults in Tables 3 and 4.\nDifferent objectives yield different representa-\ntions It may seem obvious, but the results conﬁrm\nthat different learning methods are preferable for\ndifferent intended applications (and this variation\nappears greater than for word representations). For\ninstance, it is perhaps unsurprising that SkipThought\nperforms best on TREC because the labels in this\ndataset are determined by the language immediately\nfollowing the represented question (i.e.\nthe an-\nswer) (Voorhees, 2002).\nParaphrase detection, on\nthe other hand, may be better served by a model\nthat focused entirely on the content within a sen-\ntence, such as SDAEs.\nSimilar variation can be\nobserved in the unsupervised evaluations. For in-\nstance, the (multimodal) representations produced\nby the CaptionRep model do not perform particu-\nlarly well apart from on the Image category of STS\nwhere they beat all other models, demonstrating a\nclear effect of the well-studied modality differences\nin representation learning (Bruni et al., 2014).\nThe nearest neighbours in Table 5 give a more\nconcrete sense of the representation spaces.\nOne\nnotable difference is between (AE-style) models\nwhose semantics come from within-sentence rela-\ntionships (CBOW, SDAE, DictRep, ParagraphVec)\nand SkipThought/FastSent, which exploit the con-\ntext around sentences. In the former case, nearby\nsentences generally have a high proportion of words\nin common, whereas for the latter it is the general\nconcepts and/or function of the sentence that is sim-\nilar, and word overlap is often minimal.\nIndeed,\nSTS 2014\nSICK\nModel\nNews\nForum\nWordNet\nTwitter\nImages\nHeadlines\nAll\nTest + Train\nSAE\n17/.16\n.12/.12\n.30/.23\n.28/.22\n.49/.46\n.13/.11\n.12/.13\n.32/.31\nSAE+embs.\n.52/.54\n.22/.23\n.60/.55\n.60/.60\n. 64/.64\n.41/.41\n.42/.43\n.47/.49\nSDAE\n.07/.04\n.11/.13\n.33/.24\n.44/.42\n.44/.38\n.36/.36\n.17/.15\n.46/.46\nSDAE+embs.\n.51/.54\n.29/.29\n.56/.50\n.57/.58\n.59/.59\n.43/.44\n.37/.38\n.46/.46\nParagraphVec DBOW\n.31/.34\n.32/.32\n.53/.5\n.43/.46\n.46/.44\n.39/.41\n.42/.43\n.42/.46\nParagraphVec DM\n.42/.46\n.33/.34\n.51/.48\n.54/.57\n.32/.30\n.46/.47\n.44/.44\n.44/.46\nSkipgram\n.56/.59\n.42/.42\n.73/.70\n.71/.74\n.65/.67\n.55/.58\n.62/.63\n.60/.69\nCBOW\n.57/.61\n.43/.44\n.72/.69\n.71/.75\n.71/.73\n.55/.59\n.64/.65\n.60/.69\nUnigram TFIDF\n.48/.48\n.40/.38\n.60/.59\n.63/.65\n72/.74\n.49/.49\n.58/.57\n.52/.58\nSkipThought\n.44/.45\n.14/.15\n.39/.34\n.42/.43\n.55/.60\n.43/.44\n.27/.29\n.57/.60\nFastSent\n.58/.59\n.41/.36\n.74/.70\n.63/.66\n.74/.78\n.57/.59\n.63/.64\n.61/.72\nFastSent+AE\n.56/ .59\n.41/.40\n.69/.64\n.70/.74\n.63/.65\n.58/.60\n.62/.62\n.60/.65\nNMT En to Fr\n.35/.32\n.18/.18\n.47/.43\n.55/.53\n.44/.45\n.43/.43\n.43/.42\n.47/.49\nNMT En to De\n.47/.43\n.26/.25\n.34/.31\n.49/.45\n.44/.43\n.38/.37\n.40/.38\n.46/46\nCaptionRep BOW\n.26/.26\n.29/.22\n.50/.35\n.37/.31\n.78/.81\n.39/.36\n.46/.42\n.56/.65\nCaptionRep RNN\n.05/.05\n.13/.09\n.40/.33\n.36/.30\n.76/.82\n.30/.28\n.39/.36\n.53/.62\nDictRep BOW\n.62/.67\n.42/.40\n.81/.81\n.62/.66\n.66/.68\n.53/.58\n.62/.65\n.57/.66\nDictRep BOW+embs.\n.65/.72\n.49/.47\n.85/.86\n.67/.72\n.71/.74\n.57/.61\n.67/.70\n.61/.70\nDictRep RNN\n.40/.46\n.26/.23\n.78/.78\n.42/.42\n.56/.56\n.38/.40\n.49/.50\n.49/.56\nDictRep RNN+embs.\n.51/.60\n.29/.27\n.80/.81\n.44/.47\n.65/.70\n.42/.46\n.54/.57\n.49/.59\nCPHRASE\n.69/.71\n.43/.41\n.76/.73\n.60/.65\n.75/.79\n.60/.65\n.65/.67\n.60/.72\nTable 4: Performance of sentence representation models (Spearman/Pearson correlations) on unsupervised (relatedness) evalua-\ntions (Section 3.2). Models are grouped according to training data as indicated in Table 3.\nthis may be a more important trait of FastSent than\nthe marginal improvement on the SICK task. Read-\ners can compare the CBOW and FastSent spaces at\nhttp://45.55.60.98/.\nDifferences between supervised and unsuper-\nvised performance Many of the best performing\nmodels on the supervised evaluations do not per-\nform well in the unsupervised setting.\nIn the\nSkipThought, S(D)AE and NMT models, the cost is\ncomputed based on a non-linear decoding of the in-\nternal sentence representations, so, as also observed\nby (Almahairi et al., 2015), the informative geome-\ntry of the representation space may not be reﬂected\nin a simple cosine distance. The log-linear models\ngenerally perform better in this unsupervised setting.\nDifferences in resource requirements As shown in\nTable 1, different models require different resources\nto train and use. This can limit their possible appli-\ncations. For instance, while it was easy to make an\nonline demo for fast querying of near neighbours in\nthe CBOW and FastSent spaces, it was not practical\nfor other models owing to memory footprint, encod-\ning time and representation dimension.\nThe role of word order is unclear The aver-\nage scores of models that are sensitive to word\norder (76.3) and of those that are not (76.6) are\napproximately the same across supervised evalua-\ntions.\nAcross the unsupervised evaluations, how-\never, BOW models score 0.55 on average compared\nwith 0.42 for RNN-based (order sensitive) models.\nThis seems at odds with the widely held view that\nword order plays an important role in determining\nthe meaning of English sentences. One possibility\nis that order-critical sentences that cannot be dis-\nambiguated by a robust conceptual semantics (that\ncould be encoded in distributed lexical representa-\ntions) are in fact relatively rare. However, it is also\nplausible that current available evaluations do not\nadequately reﬂect order-dependent aspects of mean-\ning (see below). This latter conjecture is supported\nby the comparatively strong performance of TFIDF\nBOW vectors, in which the effective lexical seman-\ntics are limited to simple relative frequencies.\nQuery\nIf he had a weapon, he could maybe take out\nAn annoying buzz started to ring in my ears, becoming\ntheir last imp, and then beat up Errol and Vanessa.\nlouder and louder as my vision began to swim.\nCBOW\nThen Rob and I would duke it out, and every\nLouder.\nonce in a while, he would actually beat me.\nSkip\nIf he could ram them from behind, send them saling over\nA weighty pressure landed on my lungs and my vision blurred\nThought\nthe far side of the levee, he had a chance of stopping them.\nat the edges, threatening my consciousness altogether.\nFastSent\nIsak’s close enough to pick off any one of them,\nThe noise grew louder, the quaking increased as the\nmaybe all of them, if he had his riﬂe and a mind to.\nsidewalk beneath my feet began to tremble even more.\nSDAE\nHe’d even killed some of the most dangerous criminals\nI smile because I’m familiar with the knock,\nin the galaxy, but none of those men had gotten to him like Vitktis.\npausing to take a deep breath before dashing down the stairs.\nDictRep\nKevin put a gun to the man’s head, but even though\nThen gradually I began to hear a ringing in my ears.\n(FF+embs.)\nhe cried, he couldn’t tell Kevin anything more.\nParagraph\nI take a deep breath and open the doors.\nThey listened as the motorcycle-like roar\nVector (DM)\nof an engine got louder and louder then stopped.\nTable 5: Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\nSupervised (combined α = 0.90)\nUnsupervised (combined α = 0.93)\nMSRP\nMR\nCR\nSUBJ\nMPAQ\nTREC\nNews\nForum\nWordNet\nTwitter\nImages\nHeadlines\nAll STS\nSICK\n0.94 (6)\n0.85 (1)\n0.86 (4)\n0.85 (1)\n0.86 (3)\n0.89 (5)\n0.92 (4)\n0.92 (3)\n0.92 (4)\n0.93 (6)\n0.95 (8)\n0.92 (2)\n0.91 (1)\n0.93 (7)\nTable 6: Internal consistency (Chronbach’s α) among evaluations when individual benchmarks are left out of the (supervised or unsuper-\nvised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).\nThe evaluations have limitations The internal con-\nsistency (Chronbach’s α) of all evaluations consid-\nered together is 0.81 (just above ‘acceptable’).12\nTable 6 shows that consistency is far higher (‘ex-\ncellent’) when considering the supervised or unsu-\npervised tasks as independent cohorts.\nThis indi-\ncates that, with respect to common characteristics of\nsentence representations, the supervised and unsu-\npervised benchmarks do indeed prioritise different\nproperties. It is also interesting that, by this met-\nric, the properties measured by MSRP and image-\ncaption relatedness are the furthest removed from\nother evaluations in their respective cohorts.\nWhile these consistency scores are a promis-\ning sign, they could also be symptomatic of a\nset of evaluations that are all limited in the same\nway.\nThe inter-rater agreement is only reported\nfor one of the 8 evaluations considered (MPQA,\n0.72 (Wiebe et al., 2005)), and for MR, SUBJ and\nTREC, each item is only rated by one or two an-\nnotators to maximise coverage. Table 2 illustrates\nwhy this may be an issue for the unsupervised eval-\nuations; the notion of sentential ’relatedness’ seems\nvery subjective.\nIt should be emphasised, how-\never, that the tasks considered in this study are all\nfrequently used for evaluation, and, to our knowl-\nedge, there are no existing benchmarks that over-\n12wikipedia.org/wiki/Cronbach’s_alpha\ncome these limitations.\n6\nConclusion\nAdvances in deep learning algorithms, software and\nhardware mean that many architectures and objec-\ntives for learning distributed sentence representa-\ntions from unlabelled data are now available to NLP\nresearchers.\nWe have presented the ﬁrst (to our\nknowledge) systematic comparison of these meth-\nods.\nWe showed notable variation in the perfor-\nmance of approaches across a range of evaluations.\nAmong other conclusions, we found that the op-\ntimal approach depends critically on whether rep-\nresentations will be applied in supervised or unsu-\npervised settings - in the latter case, fast, shallow\nBOW models can still achieve the best performance.\nFurther, we proposed two new objectives, FastSent\nand Sequential Denoising Autoencoders, which per-\nform particularly well on speciﬁc tasks (MSRP and\nSICK sentence relatedness respectively).13 If the ap-\nplication is unknown, however, the best all round\nchoice may be DictRep: learning a mapping of pre-\ntrained word embeddings from the word-phrase sig-\nnal in dictionary deﬁnitions. While we have focused\non models using naturally-occurring training data,\n13We make all code for training and evaluating these new\nmodels publicly available, together with pre-trained models and\nan online demo of the FastSent sentence space.\nin future work we will also consider supervised ar-\nchitectures (including convolutional, recursive and\ncharacter-level models), potentially training them on\nmultiple supervised tasks as an alternative way to\ninduce the ’general knowledge’ needed to give lan-\nguage technology the elusive human touch.\nAcknowledgments\nThis work was supported by a Google Faculty\nAward to AK and FH and a Google European Doc-\ntoral Fellowship to FH. Thanks also to Marek Rei,\nTamara Polajnar, Laural Rimell, Jamie Ryan Kiros\nand Piotr Bojanowski for helpful discussion and\ncomments.\nReferences\n[Agirre et al.2014] Eneko Agirre, Carmen Banea, Claire\nCardie, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, Weiwei Guo, Rada Mihalcea, German Rigau,\nand Janyce Wiebe. 2014. Semeval-2014 task 10: Mul-\ntilingual semantic textual similarity. In Proceedings of\nthe 8th International Workshop on Semantic Evalua-\ntion (SemEval 2014), pages 81–91.\n[Almahairi et al.2015] Amjad Almahairi, Kyle Kastner,\nKyunghyun Cho, and Aaron Courville. 2015. Learn-\ning distributed representations from reviews for col-\nlaborative ﬁltering. In Proceedings of the 9th ACM\nConference on Recommender Systems, pages 147–\n154. ACM.\n[Baroni et al.2014a] Marco Baroni, Raffaela Bernardi,\nand Roberto Zamparelli.\n2014a.\nFrege in space:\nA program of compositional distributional semantics.\nLinguistic Issues in Language Technology, 9.\n[Baroni et al.2014b] Marco Baroni, Georgiana Dinu, and\nGerm´an Kruszewski.\n2014b.\nDon’t count, pre-\ndict! a systematic comparison of context-counting vs.\ncontext-predicting semantic vectors. In Proceedings\nof the 52nd Annual Meeting of the Association for\nComputational Linguistics, volume 1, pages 238–247.\n[Bruni et al.2014] Elia Bruni, Nam-Khanh Tran, and\nMarco Baroni. 2014. Multimodal distributional se-\nmantics. J. Artif. Intell. Res. (JAIR), 49:1–47.\n[Chen et al.2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin,\nRamakrishna Vedantam, Saurabh Gupta, Piotr Dollar,\nand C Lawrence Zitnick. 2015. Microsoft coco cap-\ntions: Data collection and evaluation server.\narXiv\npreprint arXiv:1504.00325.\n[Cho et al.2014] Kyunghyun Cho, Bart Van Merri¨enboer,\nCaglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014. Learn-\ning phrase representations using rnn encoder-decoder\nfor statistical machine translation. In Proceedings of\nEMNLP.\n[Clark and Pulman2007] Stephen Clark and Stephen Pul-\nman. 2007. Combining symbolic and distributional\nmodels of meaning.\nIn AAAI Spring Symposium:\nQuantum Interaction, pages 52–55.\n[Collobert et al.2011] Ronan Collobert, Jason Weston,\nL´eon Bottou, Michael Karlen, Koray Kavukcuoglu,\nand Pavel Kuksa. 2011. Natural language process-\ning (almost) from scratch.\nThe Journal of Machine\nLearning Research, 12:2493–2537.\n[Dai and Le2015] Andrew M Dai and Quoc V Le. 2015.\nSemi-supervised sequence learning. In Advances in\nNeural Information Processing Systems, pages 3061–\n3069.\n[Dolan et al.2004] Bill Dolan, Chris Quirk, and Chris\nBrockett. 2004. Unsupervised construction of large\nparaphrase corpora:\nExploiting massively parallel\nnews sources. In Proceedings of the 20th international\nconference on Computational Linguistics, page 350.\nAssociation for Computational Linguistics.\n[Harris1954] Zellig S Harris. 1954. Distributional struc-\nture. Word.\n[Hill et al.2015a] Felix Hill, Kyunghyun Cho, Anna Ko-\nrhonen, and Yoshua Bengio. 2015a. Learning to un-\nderstand phrases by embedding the dictionary. Trans-\nactions of the Association for Computational Linguis-\ntics.\n[Hill et al.2015b] Felix Hill, Roi Reichart, and Anna Ko-\nrhonen.\n2015b.\nSimlex-999: Evaluating semantic\nmodels with (genuine) similarity estimation. Compu-\ntational Linguistics.\n[Hu and Liu2004] Minqing Hu and Bing Liu. 2004. Min-\ning and summarizing customer reviews. In Proceed-\nings of the tenth ACM SIGKDD international confer-\nence on Knowledge discovery and data mining, pages\n168–177. ACM.\n[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jordan\nBoyd-Graber, and Hal Daum´e III. 2015. Deep un-\nordered composition rivals syntactic methods for text\nclassiﬁcation. Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics.\n[Ji and Eisenstein2013] Yangfeng Ji and Jacob Eisenstein.\n2013. Discriminative improvements to distributional\nsentence similarity. In EMNLP, pages 891–896.\n[Kalchbrenner et al.2014] Nal\nKalchbrenner,\nEdward\nGrefenstette, and Phil Blunsom. 2014. A convolu-\ntional neural network for modelling sentences.\nIn\nProceedings of EMNLP.\n[Kiros et al.2015] Ryan Kiros, Yukun Zhu, Ruslan R\nSalakhutdinov, Richard Zemel, Raquel Urtasun, An-\ntonio Torralba, and Sanja Fidler. 2015. Skip-thought\nvectors. In Advances in Neural Information Process-\ning Systems, pages 3276–3284.\n[Le and Mikolov2014] Quoc V Le and Tomas Mikolov.\n2014.\nDistributed representations of sentences and\ndocuments. In Proceedings of ICML.\n[Levy and Goldberg2014] Omer Levy and Yoav Gold-\nberg. 2014. Neural word embedding as implicit ma-\ntrix factorization. In Advances in Neural Information\nProcessing Systems, pages 2177–2185.\n[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido\nDagan.\n2015.\nImproving distributional similarity\nwith lessons learned from word embeddings. Transac-\ntions of the Association for Computational Linguistics,\n3:211–225.\n[Mao et al.2015] Junhua Mao, Wei Xu, Yi Yang, Jiang\nWang, and Alan Yulle. 2015. Deep captioning with\nmultimodal recurrent neural networks (m-rnn).\nIn\nProceedings of ICLR.\n[Marelli et al.2014] Marco\nMarelli,\nStefano\nMenini,\nMarco Baroni, Luisa Bentivogli, Raffaella Bernardi,\nand Roberto Zamparelli. 2014. A sick cure for the\nevaluation of compositional distributional semantic\nmodels.\nIn Proceedings of LREC, pages 216–223.\nCiteseer.\n[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg\nCorrado, and Jeffrey Dean. 2013a. Efﬁcient estima-\ntion of word representations in vector space.\narXiv\npreprint arXiv:1301.3781.\n[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,\nKai Chen, Greg S Corrado, and Jeff Dean.\n2013b.\nDistributed representations of words and phrases and\ntheir compositionality. In Advances in neural infor-\nmation processing systems, pages 3111–3119.\n[Milajevs et al.2014] Dmitrijs Milajevs, Dimitri Kartsak-\nlis, Mehrnoosh Sadrzadeh, and Matthew Purver. 2014.\nEvaluating neural word representations in tensor-based\ncompositional settings. In Proceedings of EMNLP.\n[Mitchell and Lapata2008] Jeff Mitchell and Mirella Lap-\nata. 2008. Vector-based models of semantic composi-\ntion. In ACL, pages 236–244.\n[Mitchell and Lapata2010] Jeff Mitchell and Mirella La-\npata. 2010. Composition in distributional models of\nsemantics. Cognitive science, 34(8):1388–1429.\n[Norman1972] Donald A Norman.\n1972.\nMemory,\nknowledge, and the answering of questions.\n[Pang and Lee2004] Bo Pang and Lillian Lee. 2004. A\nsentimental education: Sentiment analysis using sub-\njectivity summarization based on minimum cuts. In\nProceedings of the 42nd annual meeting on Associa-\ntion for Computational Linguistics, page 271. Associ-\nation for Computational Linguistics.\n[Pang and Lee2005] Bo Pang and Lillian Lee. 2005. See-\ning stars: Exploiting class relationships for sentiment\ncategorization with respect to rating scales. In Pro-\nceedings of the 43rd Annual Meeting on Association\nfor Computational Linguistics, pages 115–124. Asso-\nciation for Computational Linguistics.\n[Pham et al.2015] Nghia The Pham, Germ´an Kruszewski,\nAngeliki Lazaridou, and Marco Baroni. 2015. Jointly\noptimizing word representations for lexical and sen-\ntential tasks with the c-phrase model. In Proceedings\nof ALC.\n[Polajnar et al.2015] Tamara Polajnar, Laura Rimell, and\nStephen Clark. 2015. An exploration of discourse-\nbased sentence spaces for compositional distributional\nsemantics. In Workshop on Linking Models of Lexical,\nSentential and Discourse-level Semantics (LSDSem),\npage 1.\n[Russakovsky et al.2014] Olga Russakovsky, Jia Deng,\nHao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,\nMichael Bernstein, et al. 2014. Imagenet large scale\nvisual recognition challenge. International Journal of\nComputer Vision, pages 1–42.\n[Serban et al.2015] Iulian V Serban, Alessandro Sordoni,\nYoshua Bengio, Aaron Courville, and Joelle Pineau.\n2015.\nBuilding end-to-end dialogue systems using\ngenerative hierarchical neural network models. arXiv\npreprint arXiv:1507.04808.\n[Socher et al.2011] Richard Socher, Jeffrey Pennington,\nEric H Huang, Andrew Y Ng, and Christopher D Man-\nning. 2011. Semi-supervised recursive autoencoders\nfor predicting sentiment distributions. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing, pages 151–161. Association for\nComputational Linguistics.\n[Sutskever et al.2011] Ilya Sutskever, James Martens, and\nGeoffrey E Hinton. 2011. Generating text with recur-\nrent neural networks. In Proceedings of the 28th Inter-\nnational Conference on Machine Learning (ICML-11),\npages 1017–1024.\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc VV Le. 2014. Sequence to sequence learning\nwith neural networks. In Advances in neural informa-\ntion processing systems, pages 3104–3112.\n[Szegedy et al.2014] Christian\nSzegedy,\nWei\nLiu,\nYangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and\nAndrew Rabinovich.\n2014.\nGoing deeper with\nconvolutions. arXiv preprint arXiv:1409.4842.\n[Vincent et al.2008] Pascal Vincent, Hugo Larochelle,\nYoshua Bengio, and Pierre-Antoine Manzagol. 2008.\nExtracting and composing robust features with denois-\ning autoencoders. In Proceedings of the 25th interna-\ntional conference on Machine learning, pages 1096–\n1103. ACM.\n[Vincent et al.2010] Pascal Vincent, Hugo Larochelle, Is-\nabelle Lajoie, Yoshua Bengio, and Pierre-Antoine\nManzagol.\n2010.\nStacked denoising autoencoders:\nLearning useful representations in a deep network with\na local denoising criterion. The Journal of Machine\nLearning Research, 11:3371–3408.\n[Voorhees2002] Ellen M Voorhees. 2002. Overview of\nthe trec 2001 question answering track. NIST special\npublication, pages 42–51.\n[Wiebe et al.2005] Janyce Wiebe, Theresa Wilson, and\nClaire Cardie. 2005. Annotating expressions of opin-\nions and emotions in language. Language resources\nand evaluation, 39(2-3):165–210.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2016-02-10",
  "updated": "2016-02-10"
}