{
  "id": "http://arxiv.org/abs/1602.06183v1",
  "title": "Node-By-Node Greedy Deep Learning for Interpretable Features",
  "authors": [
    "Ke Wu",
    "Malik Magdon-Ismail"
  ],
  "abstract": "Multilayer networks have seen a resurgence under the umbrella of deep\nlearning. Current deep learning algorithms train the layers of the network\nsequentially, improving algorithmic performance as well as providing some\nregularization. We present a new training algorithm for deep networks which\ntrains \\emph{each node in the network} sequentially. Our algorithm is orders of\nmagnitude faster, creates more interpretable internal representations at the\nnode level, while not sacrificing on the ultimate out-of-sample performance.",
  "text": "Node-By-Node Greedy Deep Learning for Interpretable Features\nKe Wu\nWUK3@RPI.EDU\nDepartment of Computer Science, 110 8th Street, Troy, NY 12180 USA\nMalik Magdon-Ismail\nMAGDON@GMAIL.COM\nDepartment of Computer Science, 110 8th Street, Troy, NY 12180 USA\nAbstract\nMultilayer networks have seen a resurgence un-\nder the umbrella of deep learning. Current deep\nlearning algorithms train the layers of the net-\nwork sequentially, improving algorithmic perfor-\nmance as well as providing some regularization.\nWe present a new training algorithm for deep net-\nworks which trains each node in the network se-\nquentially. Our algorithm is orders of magnitude\nfaster, creates more interpretable internal repre-\nsentations at the node level, while not sacriﬁcing\non the ultimate out-of-sample performance.\n1. Introduction\nMultilayer neural networks have gone through ups and\ndowns since their arrival in (Rosenblatt, 1958; Widrow,\n1960; Hoff Jr, 1962). The resurgence in “deep” networks is\nlargely due to the efﬁcient greedy layer by layer algorithms\nfor training, that create meaningful hierarchical representa-\ntions of the data. Particularly in the era of “big data” from\ndiverse applications, efﬁcient training to create data rep-\nresentations that provide insight into the complex features\ncaptured by the neurons are important. We explore these\ntwo dimensions of training a deep network. Assume a stan-\ndard machine learning from data setup (Abu-Mostafa et al.,\n2012), with N datapoints (x1, y1), . . . , (xN, yN); xn ∈Rd\nand yn ∈{0, 1, . . . , c −1} (multi-class setting).\nDeep Network\nWe refer to Abu-Mostafa et al. (2012,\nChapter 7) for the basics of multilayer net-\nworks, including notation which we very\nquickly summarize here. On the right, we\nshow a feedforward network architecture.\nSuch a network is “deep” because it has\nmany (≫2) layers.\nWe assume that a\nnetwork architecture has been ﬁxed. The\nnetwork implements a function whereby in\neach layer (ℓ), the output of the previous\nlayer (ℓ−1) is transformed into the output of the layer ℓ\nLearn W(1)\nLearn W(2)\nLearn W(3)\nFine tuning\nFigure 1. Layer-by-layer greedy deep learning algorithm.\nuntil one reaches the ﬁnal layer on top, which is the output\nof the network. The function implemented by layer ℓis\nx(ℓ) = tanh(W(ℓ)x(ℓ−1)),\nwhere x(ℓ) is the output of layer ℓ, and the weight-matrix\nW(ℓ) (of appropriate dimensions to map a vector from layer\nℓ−1 to a vector in ℓ) are parameters to be learned from\ndata. The training phase uses the data to identify all the\nparameters {W(1), W(2), . . . , W(L)} of the deep network.\nBackpropagation which trains all the weights simultane-\nously, allowing for maximum ﬂexibility, was the popu-\nlar approach to training a deep network (Rumelhart et al.,\n1986).\nThe current approach is layer-by-layer:\ntrain\nthe ﬁrst layer weights W(1); then train the second layer\nweights W(2), keeping the ﬁrst layer weights ﬁxed; and so\non until all the weights are learned. In practice, once all\nthe weights have been learned in the greedy-layer-by-layer\nmanner (often referred to as pre-training), the best results\nare obtained by ﬁne tuning all the weights using a few iter-\nations of backpropagation (Figure 1).\nHow one should train the internal layers? The two popular\napproaches are: (1) Each layer is an unsupervised nonlin-\near auto-encoder (Cottrell & Munro, 1988; Bengio et al.,\n2007); this approach is appealing to build meaningful hi-\nerarchical representations of the data in the internal lay-\ners. (2) Each layer is a supervised encoder; this approach\nprimarly targets performance. Deep learning enjoys suc-\ncess in several applications and hence considerable effort\narXiv:1602.06183v1  [cs.LG]  19 Feb 2016\nNode-By-Node Greedy Deep Learning for Interpretable Features\nLayer-by-Layer Unsupervised Autoencoder\nOur Greedy Node-by-Bode Unsupervised Algorithm (GN)\nFigure 2. Features of nine nodes in the ﬁrst internal layer for unsupervised pre-training.\nhas been expended in optimizing the pre-training. The two\nmain considerations are:\n1. Pre-training time and training/test performance of the\nﬁnal solution.\nThe greedy layer-by-layer pre-training is\nsigniﬁcantly more efﬁcient than full backpropagation, and\nappears to be better at avoiding bad local minima (Erhan\net al., 2010). Our algorithms will show an order of magni-\ntude speed gain over greedy layer-by-layer pre-training.\n2. Interpretability of the feature representations in the in-\nternal layers. We use the USPS digits data (10 classes)\nas a strawman benchmark to illustrate our approach. The\nweights going into each node of the ﬁrst layer identify the\npixels contributing to the “high-level” feature represented\nby that hidden node. Figure 2 compares our features with\nthe layer-by-layer features in the unsupervised setting, and\nFigure 4 compares the features in the supervised setting.\nThe layer-by-layer features do not capture the essence of\nthe digits as do our features. This has to do with the simul-\ntaneous training of all the hidden nodes in the layer. Our\napproach can be viewed as a nonlinear extension of PCA,\nwhich ”greedily” constructs each linear features. (In the\nsupplementary material, we show the features captured by\nlinear PCA; they are comparable to our features.)\nGreedy Node-by-Node Pre-Training.\nThe thrust of our\napproach is to learn the weights into each node in a se-\nquential greedy manner: greedy-by-node (GN) for the un-\nsupervised setting and greedy-by-class-by-node (GCN) for\nthe supervised setting. Figure 3 illustrates the ﬁrst 5 steps\nfor a network. Our approach mimics a human who hardly\nW(1)\n1\nW(1)\n2\nW(1)\n3\nW(1)\n4\nW(2)\n1\nFigure 3. Node-by-node greedy deep learning algorithm. In each\nstep the weights feeding into one node are learned (red).\nbuilds all features at once from all objects. Instead, features\nare built sequentially, while processing the data. Our algo-\nrithm learns one feature at a time, using a part of the data\nto learn each feature. Our contributions are\n1. The speciﬁc algorithm to train each internal node.\n2. How to select the training data for each internal node.\nWe do not improve the accuracy of deep learning. Rather,\nwe improve efﬁciency and the interpretability of features,\nwhile maintaining accuracy. A standard deep learning al-\ngorithm uses every data point to process every weight in\nthe network. Our algorithm uses only a subset of the data\nto process a particular weight. By training each node using\n”relevant” data, our algorithm produces more interpretable\nfeatures. Our algorithm gets more intuitive features, in the\nunsupervised and supervised setting (Figures 2 and 4).\nNode-By-Node Greedy Deep Learning for Interpretable Features\nLayer-by-Layer Supervised Encoder\nOur Greedy Node-by-Node Supervised Algorithm (GCN)\nFigure 4. Features of nine nodes in the ﬁrst internal layer for supervised pre-training.\nRelated Work.\nTo help motivate our approach, it helps\nto start back at the very beginning of neural networks,\nwith Rosenblatt (1958) and Widrow (1960). They intro-\nduced the adaline, the adaptive linear (hard threshold el-\nement), and the combination of multiple elements came\nin Hoff Jr (1962), the madaline (the precursor to the mul-\ntilayer perceptron).\nThings cooled off a little because\nﬁtting data with multiple hard threshold elements was a\ncombinatorial nightmare.\nThere is no doubt softening\nthe hard threshold to a sigmoid and the arrival of a new\nefﬁcient training algorithm, backpropagation (Rumelhart\net al., 1986), was a huge part of the resurgence of neural\nnetworks in the 1980s/1990s. But, again, neural networks\nreceded, taking a back seat to modern techniques like the\nsupport vector machine (Vapnik, 2000). In part, this was\ndue to the facts that multilayer feedforward networks were\nstill hard to train iteratively due to convergence issues and\nmultiple local minima (Gori & Tesi, 1992; Fukumizu &\nAmari, 2000), are extremely powerful (Hornik et al., 1989)\nand easy to overﬁt to data. For these reasons, and despite\nthe complexity theoretic advantages of deep networks (see\nfor example the short discussion in (Bengio et al., 2007)),\napplication of neural networks was limited mostly to shal-\nlow two layer networks.\nMulti-layer (deep) neural net-\nworks are back in the guise of deep learning/deep networks,\nand again because of a leap in the methods used to train\nthe network (Hinton et al., 2006). In a nutshell, rather that\naddress the full problem of learning the weights in the net-\nwork all at once, train each layer of the network sequen-\ntially. In so doing, training becomes manageable (Hinton\net al., 2006; Bengio et al., 2007), the local minima prob-\nlem when training a single layer is signiﬁcantly diminished\nas compared to the whole network and the restriction to\nlayer by layer learning reigns in the power of the network,\nhelping with regularizing it(Erhan et al., 2010). A side ben-\neﬁt has also emerged, which is that each layer successively\nhas the potential to learn hierarchical representations (Er-\nhan et al., 2010; Lee et al., 2009a). As a result of these\nalgorithmic advances, deep networks have found a host\nof modern applications, ranging from sentiment classiﬁca-\ntion (Glorot et al., 2011), to audio (Lee et al., 2009b), to\nsignal and information processing (Yu & Deng, 2011), to\nspeech (Deng et al., 2013), and even to the unsupervised\nand transfer settings (Bengio, 2012). Optimization of such\ndeep networks is also an active area, for example (Ngiam\net al., 2011; Martens, 2010).\nMost work has focused on better representations of the in-\nput data. Besides the original deep belief network (Hin-\nton et al., 2006) and autoencoder (Bengio et al., 2007),\nthe stacked denoising autoencoder (Vincent et al., 2010;\n2008) has been widely used as a variant to the classic au-\ntoencoder, where corrupted data is used in pre-training.\nSparse encoding (Boureau et al., 2008; Poultney et al.,\n2006) has also been used to prevent the system from acti-\nvating the same subset of nodes constantly (Poultney et al.,\n2006). This approach has been shown capable of learn-\nNode-By-Node Greedy Deep Learning for Interpretable Features\ning local and meaningful features, however the efﬁciency\nis worse. For images, convolutional neural networks (Le-\nCun & Bengio, 1995) and the deep convolutional neu-\nral network\n(Krizhevsky et al., 2012) are widely used,\nhowever the computation cost is signiﬁcantly more, and\nthe feature learning is based on the local ﬁlter deﬁned by\nthe modeler. Our methods apply to a general deep net-\nwork.\nOur algorithms can be roughly seen as simulta-\nneously clustering the data while extracting the features.\nDeep networks have been used for unsupervised clustering\n(Chen, 2015) and clustering has been used in classic deep\nnetworks by Weston et al. (2012) which are targeted for\nsemi-supervised learning. Using clusters from K-means to\ntrain a deep network was reported (Faraoun & Boukelif,\n2006), which improves the training speed while ignoring\nsome data - not recommended in the supervised setting with\nscarce data. Pre-clustering may have efﬁciency-advantages\nin large-scale systems, but it may not be an effective way\nto learn good representations (Coates & Ng, 2012).\nIn this paper, we are proposing a new algorithmic enhance-\nment to the deep network which is to consider each node\nseparately. It not only explicitly achieves the sparsity of\nnode activation but also requires much shorter computation\ntime. We have found no such approaches in the literature.\n2. Greedy Node-by-Node Deep Learning\nThe basic step for pre-training is to train a two layer net-\nwork. The network is trained to reproduce the input (un-\nsupervised) or the ﬁnal target (supervised). The two algo-\nrithms are very similar in structure. For concreteness, we\ndescribe unsupervised pre-training (auto-encoding).\nIn a classic auto-encoder with one hidden layer using\nstochastic gradient descent (SGD), the number of opera-\ntions (fundamental arithmetic operations) p for one training\nexample is:\np = (2d1d2 + d1 + 2d2) + (3d1d2 + 2d1 + 3d2)\n(Forward Propagation)\n(Backpropagation)\n= 5d1d2 + 3d1 + 5d2,\n(1)\nwhere d1 is the dimension of the input layer, (d1+1 includ-\ning the bias) and d2 is the dimension of the second layer.\nForward propagation computes the output and backpropa-\ngation computes the gradient of the loss w.r.t. the weights\n(see Abu-Mostafa et al. (2012, Chapter 7)). We use the Eu-\nclidean distance between the reconstructed input ˆx and the\noriginal input x (auto-encoder target) as loss,\nloss = ∥x −ˆx∥2.\nFor N training example and E epochs of SGD, the total\nrunning time is O(NpE) = O(NEd1d2).\nIn our algorithm, the basic step is also to train a 2-layer net-\nwork. However, we train each node sequentially in a greedy\nfashion as illustrated in Figure 5. The red (middle) layer is\nFigure 5. Greedy training of a 2-layer network\nbeing trained (it has dimension d2). The inputs come from\nthe outputs of the previous layer, having dimension d1. The\noutput-dimension is also d1 (auto-encoder). We use linear\noutput-nodes and SGD optimizing the auto-encoder (the al-\ngorithm is easy to adapt to sigmoid output-nodes).\nThe standard layer-by-layer algorithm trains all the weights\nat the same time, using the whole data set. We are going to\nuse a fraction of the data to learn one feature at a time; dif-\nferent features are learned on different data. So, the training\nof each layer is done in multiple stages. At each stage, we\nonly update the weights corresponding to one node. After\nall the features are obtained (all the weights learned) for\na layer, a forward propagation with all data computes the\noutputs of the layer, for use in training the next layer (as in\nstandard pre-training). To make this greedy learning algo-\nrithm work, we must address three questions:\n1. How to learn features sequentially?\n2. How to distribute the training data into each node?\n3. How to obtain non-overlapping features?\nLet us address the question 1, assuming we have already\ncreated d2 subsets of the data of size K, S1, . . . , Sd2. These\nsubsets need not be disjoint, but our discussion is for the\ncase of disjoint subsets, so Kd2 = N. If each node is fed\na random subset of K data points, then each node will be\nroughly learning the same feature, that is close to the top\nprinciple component. We will address this issue is ques-\ntions 2 and 3 which encourage learning different features\nby implementing a form of orthogonality and using differ-\nent data for each node.\nA simple idea which works but is inefﬁcient: use data Si\nto train the weights into hidden node i (assuming weights\ninto nodes 1, . . . , (i −1) have been trained). The situation\nis illustrated in Figure 5. The weights into and out of node\n1 are ﬁxed (black). Node 2 is being trained (the red weights\ninto and out of node 2). We use data S2 to train these red\nweights, so we forward propagate the data through both\nnodes, but we only need to backpropagate through the red\nnodes. Effectively, we are forward propagating through a\nnetwork of i hidden nodes, but we have K data points, so\nNode-By-Node Greedy Deep Learning for Interpretable Features\nthe run-time is of just the forward propagations is\nd2\nX\ni=1\nO(KEd1i) = O(KEd1d2\n2).\nSince d2 could be large, it is inefﬁcient to have a quadratic\nin d2 run-time. There is a simple ﬁx which relies on the fact\nthat the outputs of the hidden layer are linearly summed\nbefore feeding into the output layer. This means that after\nthe weights for node 1 are learned (and ﬁxed), one can do a\nsingle forward propagation of all the data once through this\nnode and store a running sum of the signal being fed into\neach output-layer node, which is the sunning contribution\nfrom all previously trained nodes in the hidden layer. This\nrestores the linear dependence on d2 as in equation (1).\nDistributing data into nodes: We now address the 2nd\nquestion. We propose two methods corresponding to the\ntwo new algorithms GN and GCN. There are many ways\nto extend these two basic methods, so we present only the\nrudimentary forms. Each node i is trained on subset Si.\nIn the unsupervised setting (GN), we train node 1 to learn\na “global feature”, and use this single global feature to re-\nconstruct all the data. The reconstruction error from this\nsingle feature is then used to rank the data. A small re-\nconstruction error means the data point is captured well by\nthe current feature. Large reconstruction errors mean a new\nfeature is needed for those data. So the reconstruction error\ncan be used as an approximate proxy for partitioning the\ndata into different features: data with drastically different\nreconstruction error correspond to different features, so that\ndata will be used to train its own feature. After sorting the\ndata according to reconstruction error, the ﬁrst N/(d2 −1)\nwill be used to train node 2, the next N/(d2 −1) to train\nnode 3 and so on up to node d2. One may do more so-\nphisticated things, like cluster the reconstruction errors into\nd2 −1 disjoint clusters and use those clusters as the subsets\nS2, . . . , Sd2. We present the simplest approach.\nIn the supervised setting (GCN), the only change is to mod-\nify the distribution of the data into nodes using the class\nlabels. If there are c classes, then d2/c of the nodes will be\ndedicated to each class, and the data points in each class are\ndistributed evenly among the nodes dedicated to that class.\nEnsuring Non-Overlapping Features.\nIf every node is\ntrained independently on all or a random sample of the\ndata, then each node will learn more-or-less the same fea-\nture. Our method of distributing the data among the nodes\nto some extent breaks this tie. We also introduce an ex-\nplicit coordination mechanism between the nodes which\nwe call the amnesia factor – when training the next node,\nhow much of the prior training is “forgotten”. Recall that\nfrom the previous i−1 (already trained) nodes, we store the\nrunning contribution to the output. SGD is applied based\non the distance between the input and the sum of the cur-\nrent output and running stored value from the previous i−1\nnodes. The running stored value can be viewed as a con-\nstraint on the training of node i, forcing the newly learned\nfeature to be ”orthogonal” to the previous ones. This is\nbecause the previous features are encoded in the running\nstored value. The current value produced by node i will try\nto contribute additional information toward reconstruct the\ndata in Si. Since, in our algorithms, the weights from the\nprevious nodes have been learned and ﬁxed, due to their\noptimization, it may prematurely saturate the output layer\nand make the new ith node redundant. The amnesia fac-\ntor gives the ability to add just the right amount of coor-\ndination between the training of node i and the nodes that\nhave already been trained, leading to stability of the fea-\ntures while at the same time maintaining the ability to get\nnon-redundant features. Our implementation of amnesia is\nsimple. The output value, OB, used for backpropagation to\ntrain the weights into node i are the stored (already learned)\nrunning output, O(1:i−1), scaled by the amnesia factor plus\nthe output from the currently being trained node Oi,\nOB = A · O(1:i−1) + Oi.\nThe amnesia factor controls how “orthogonal” each suc-\ncessive feature will be to the previously trained features. A\nhigher amnesia factor results in strongly coupled features\nthat are more orthogonal. A zero amnesia factor means in-\ndependent training of the nodes which is likely to result in\nredundant features. Here is a high-level summary of the full\nalgorithm. Detailed pseudo-code is presented in the supple-\nmentary materials.\n1: Distribute the data into subsets S1, . . . , Sd2\n2: Train hidden node i on data Si using amnesia factor A\n3: Perform one forward propagation with Si on nodes\n1, . . . , i after training, and add the output values to the\nrunning output value.\nTheorem 1 GN and GCN run in O(NEd1+Nd1d2) time.\n(The detailed derivation of the running time is in the sup-\nplementary materials.) The run-time of the classic deep-\nnetwork algorithm is O(NEd1d2). As d2 and E (the num-\nber of iterations of SGD on each data point) increase, the\nefﬁciency gain increases, and can be orders of magnitude.\n3. Results and Discussion\nWe use a variety of data sets to compare our new algo-\nrithms GN (unsupervised) and GCN (supervised) against\nsome standard deep-network training algorithms. Our aim\nis to demonstrate that\n• Features from GN and GCN are more interpretable.\n• The classiﬁcation performance of our algorithms is\ncomparable to layer-by-layer training, despite being\norders of magnitude more efﬁcient.\nNode-By-Node Greedy Deep Learning for Interpretable Features\nAE\nGN\nFigure 6. Outcomes of four inner nodes using AE and GN algorithm for 5-class handwritten digit data. (The output of the 1st node for\nboth cases is very close to the ﬁrst principal component (See supplementary information).\n3.1. Quality of Features\nFirst, we appeal to the results on the USPS digits data in\nthe Figures 2 and 4 to illustrate the qualitative difference\nbetween our greedy node-by-node features and the fea-\ntures produced by the standard layer-by-layer pre-training.\nWhen you learn all the features in a layer simultaneously,\nthere are several local minima that do not correspond to the\nnatural intuitive features that might be present. Because all\nthe nodes are being trained simultaneously, multiple fea-\ntures can get mixed within one node and this mixing can\nbe compensated for in other nodes. This results in some-\nwhat counter-intuitive features as are obtained in Figures 2\nand 4 for the layer-by-layer approach. The node-by-node\napproaches do not appear to suffer from this problem, as\na simple visual inspection of the features implemented at\neach node yield recognizable digits. It is clear that our new\nalgorithms ﬁnd more interpretable features.\nEven though the visual inspection of the features is quite\ndifference, we may investigate more closely the dimension\nreduction from the original pixel-space to the new feature\nspace. A common approach is to use scatter plots of one\nfeature against another, and we give such scatter plots in\nFigure 6, for the unsupervised setting.\nFor this experi-\nment, we consider a smaller handwritten digit data, gener-\nated from the scikit-learn package (Pedregosa et al., 2011).\nReaders can refer to the supplementary information for the\ndetails on the generation procedure. The simpliﬁed hand-\nwritten digit data is a 8 by 8 pixel handwritten digits. It\nis known that this data admits good classiﬁcation accuracy\nusing the top-2 PCA features. This means that we should\nsee good class separation from a scatter plot of the features\nof one node against another. (The higher dimensional digits\ndata cannot effectively be visualized in 2-dimensions.)\nIn Figure 6, the top row shows scatter plots of one node’s\nfeatures against another node’s features for the standard\nlayer-by-layer algorithm. Good class separation is obtained\neven after projecting to 2-dimensions (there are 5 classes).\nThe bottom row shows the similar plots for our unsuper-\nvised algorithms. The similarity between the scatter plots\nindicates that our features and layer-by-layer features are\ncomparable in terms of classiﬁcation accuracy (we will in-\nvestigate this in depth in the next section).\nSummary:\nOur node-by-node features are more inter-\npretable and appear as effective as layer-by-layer features.\n3.2. Efﬁciency and Classiﬁcation Performance\nWe have argued theoretically that our algorithms are more\nefﬁcient than layer-by-layer pre-training. The rationale is\nNode-By-Node Greedy Deep Learning for Interpretable Features\nTable 1. Running-time and performance.\nSV (supervised) and\nUSV (unsupervised) are the standard layer-by-layer algorithms;\nGN (unsupervised) and GCN (supervised) are our algorithms.\nTYPE\nRT (S)\nPT (S)\nTRAINING\nSCORE\nTEST\nSCORE\nSV\n1363\n1293\n1.000\n0.939\nUSV\n4112\n4042\n0.999\n0.933\nGN\n674\n604\n0.999\n0.931\nGCN\n409\n339\n0.998\n0.923\nthat the training is distributed onto each inner node by the\npartitioning of the data. We ﬁrst compare the running time\nin practice of supervised and unsupervised pre-training for\nour algorithms (GN and GCN) with the standard layer-by-\nlayer algorithms. For a concrete comparison, we use a ﬁxed\nnumber of iterations (300) for pre-training with an initial\nlearning rate 0.001; we use 500 iterations to train the ﬁnal\noutput layer with logistic regression, with an initial learning\nrate 0.002; and ﬁnally, we use 20 iterations of backpropa-\ngation to ﬁne-tune all the weights with ﬁxed learning rate\n0.001. We do not use signiﬁcant learning rate decay.\nAn L2 regularization term is used with all the algorithms\nto help with overﬁtting, with a regularization parameter of\nλ = 1. It should be noted that this ”weight decay” regu-\nlarization term is not always necessary with deep learning\nsince the ﬂexibility of the network is already signiﬁcantly\ndiminished. In this paper, we are not trying to optimize\nall hyperparameters to get the highest possible test perfor-\nmance for certain data sets. Instead we try to compare the\nfour algorithm with one predeﬁned set of parameters. Our\ngoal is to see whether there is a deterioration in classiﬁca-\ntion accuracy due to additional constraints placed by train-\ning the deep-network using greedy node-by-node learning.\nTable 1 shows the results for running time. A 256-200-\n150-10 network structure was used. RT and PT are the\ntimes for the entire algorithm and for only the pre-training\nrespectively (the entire algorithm includes ﬁne-tuning and\ndata pre-processing). 7291 data are in the training set and\n2007 are in the test set. All the experiment was done using\na single Intel i-7 3370 3.4GHz CPU. As shown in Table 1,\nall algorithms show comparable training and test perfor-\nmance, however our algorithms can give an order of mag-\nnitude speed-up.\nImpact of Amnesia The amnesia factor A is used to train\nnode i, given the stored results of forward propagation for\nthe previous i −1 nodes. The stored output is similar to\nkeeping a “memory” of previous learned knowledge within\nthe same representation layer. Node i should learn the new\nfeature with the previous information “in mind”. We found\nthat best results are obtained with an amnesia factor be-\ntween 0 (no memory) and 1.0 (no loss of memory). This\nsuggests that some coordination between nodes in a layer\nis useful, but too much coordination adds too many con-\nstraints. Our results indicate that amnesia is very important\nfor both our new algorithms, and it is not a surprise. Table 2\nshows the result of a 256-200-150-10 network with learn-\ning rate 0.001 and a variety of amnesia factors. All the\nother settings not changed from the previous experiment.\nTable 2. Effect of the amnesia factor (learning rate: 0.001).\nAMNESIA\nFACTOR\nTRAINING\nSCORE\nTEST\nSCORE\n1.0\n0.998\n0.923\n0.9\n0.999\n0.922\n0.8\n0.999\n0.926\n0.7\n0.999\n0.932\n0.6\n0.999\n0.927\n0.5\n0.999\n0.931\n0.4\n0.999\n0.934\n0.0\n0.974\n0.915\nThe results in Table 2 suggest that a resonable choice for\nthe amnesia factor is A ∈[0.4, 0.5], which is far from both\n1 (full coordination) and 0 (no memory that can result in\nredundant features). A non-zero amnesia factor applies a\nbalance between two types of representations. A higher\nAF will lean to generative model of inputs but induce nu-\nmerical issue for training. A lower AF will learn a local\nmodel of inputs but result in loss of information through\nredundant features. The optimal amnesia factor may de-\npend on size and depth of the layer. We did not investigate\nthis issue, keeping A constant for each layer. We also did\nnot investigate different ways to distribute the data among\nthe nodes. Our simple method works well and is efﬁcient.\nFinally, we give an extensive comparison of the out-of-\nsample performance between the four algorithms on sev-\neral additional data sets in Table 3. Nine data sets obtained\nfrom UCI machine learning repository (Lichman, 2013)\nwere used to compare the algorithms. Test performance is\ncomputed using a validation set which is typically approxi-\nmately 30% of the data unless the test set is provided by the\nrepository. For GCN, it is convenient to set the number of\nnodes in each hidden layer to be divisible by the number of\nclasses. We used the same architecture for all algorithms.\nHere are some relevant information for each data set.\nNode-By-Node Greedy Deep Learning for Interpretable Features\nTable 3. Comparison between algorithms for multiple data sets showing training / test accuracy.\nLAYER-BY-LAYER\nGREEDY NODE-BY-NODE\nDATA SET\nNETWORK\nSV\nUSV\nGN\nGCN\nMUSK\n120-80\n1.000 / 0.991\n1.000 / 0.993\n1.000 / 0.984\n1.000 / 0.989\nISOLET\n260-78\n0.997 / 0.946\n0.999 / 0.953\n1.000 / 0.944\n1.000 / 0.935\nCNAE-9\n594-396\n1.000 / 0.954\n0.996 / 0.944\n0.995 / 0.940\n0.994 / 0.921\nMADELON\n800-400-200\n1.000 / 0.560\n1.000 / 0.550\n1.000 / 0.645\n1.000 / 0.652\nCANCER\n50-40\n0.984 / 0.973\n0.987 / 0.957\n0.979 / 0.968\n0.979 / 0.973\nBANK\n30-20\n0.949 / 0.886\n0.912 / 0.905\n0.910 / 0.902\n0.913 / 0.896\nNEWS\n50-40\n0.708 / 0.632\n0.673 / 0.646\n0.689 / 0.646\n0.692 / 0.637\nPOKER\n40-40\n0.718 / 0.579\n0.631 / 0.622\n0.650 / 0.620\n0.629 / 0.624\nCHESS\n60-40-40\n1.000 / 0.962\n1.000 / 0.867\n0.890 / 0.867\n0.972 / 0.936\nDATA SET\nN\nd\nη\nA\nMUSK\n6598\n166\n0.01\n0.4\nISOLET\n6238\n617\n0.0001\n0.4\nCNAE-9\n1080\n856\n0.01-0.1\n0.4\nMADELON\n4400\n500\n0.01\n0.4\nCANCER\n699\n10\n0.01\n0.4\nBANK\n4521\n16\n0.01\n0.4\nNEWS\n39797\n60\n0.01\n0.4\nPOKER\n78211\n10\n0.01\n0.4\nCHESS\n9149\n6\n0.01\n0.4\n(N = # data points; d = dimension (# attributes);\nη = initial learning rate of SGD; A = amnesia factor.)\nMUSK (version 2) is a molecule classiﬁcation task with\ndata on molecules that are judged by experts as musks\nand non-musks. ISOLET contains audio information for\n26 letters spoken by human speakers. CNAE-9 contains\n1080 documents of text business descriptions for Brazilian\ncompanies categorized into 9 categories. CNAE-9 is very\nsparse, so a learning rate up to 0.1 can produce a reason-\nable performance for our new algorithms while the classi-\ncal algorithms use a learning rate of 0.01. Better perfor-\nmance results if one specializes the learning rate to the al-\ngorithm (for example, GCN with learning rate of 0.15 has\nthe better test score of 0.935). MADELON is an artiﬁcial\ndataset used in the NIPS 2003 feature selection challenge.\nCANCER is the Wisconsin breast cancer data. BANK con-\ntains bank marketing data. NEWS contains news popular-\nity data. POKER is part of the poker hand data set (class\n0 and 1 are removed to get a better class balance). CHESS\nis the king-rook vs. king data set (KRK) with 4 classes\n(“draw”, “eight”, “eleven”, “ﬁfteen”) from the original data\nset with 6 attributes. Table 3 shows the comparison be-\ntween the layer-by-layer supervised, unsupervised, GN and\nGCN algorithms.\nSummary: The performance of our algorithms is compara-\nble with standard layer-by-layer deep network algorithms,\nsigniﬁcant speedup and more interpretable features. The\nresults on additional data sets are are consistent with the\nUSPS handwriting data set we used throughout this paper.\n4. Conclusion and Future Work\nOur goal was to develop a greedy node-by-node deep net-\nwork algorithm that learns feature representations in each\nlayer of a deep network sequentially (similar to PCA in\nthe linear setting, but different because we partition data\namong features). Our two novel deep learning algorithms\noriginate from the idea of simulating a human’s learning\nprocess, namely building features in a streaming fashion\nincrementally, using part of the data to learn each feature.\nThis is in contrast to classical deep learning algorithms\nwhich obtain all the hierarchical features in a layer for all\nthe objects at the same time (train all nodes of a layer si-\nmultaneously on all data) – the human learner learns from\none or few objects at a time and is able to learn new fea-\ntures while leveraging the features it has learned on other\ndata. Our two new methods, corresponding to supervised\nlearning (GCN) and unsupervised learning (GN), do indeed\nlearn one feature of one group of training data at a time.\nSuch a design helps to construct more human-recognizable\nfeatures. We also developed amnesia, an ability for the\ngreedy algorithm to control the coordination among the\nfeatures. The results on several datasets reveal that our al-\ngorithms have a prediction performance comparable with\nthe standard layer-by-layer methods plus the advertised\nbeneﬁts of speed-up and a more interpretable features.\nIn the future, we would like to investigate two subproblems.\nFirst, whether it is possible to further exploit the node-by-\nnode paradigm by optimizing the hyper-parameters (learn-\ning rate and amnesia) to obtain superior performance. Sev-\neral questions need to be answered here: Is there a better\nway to partition the data? How to choose the optimal amne-\nsia factor? Should the learning rate be adjusted differently\nfor each inner node or each layer?\nScalability: our algorithms are in some sense learning in an\nNode-By-Node Greedy Deep Learning for Interpretable Features\nonline fashion, and so cannot exploit the matrix-vector and\nmatrix-matrix multiplication approaches to training that\ncan be easily implemented for multicore or GPU architec-\ntures. One way to handle such difﬁculty could be to learn\nseveral features at the same time in different machines and\nexchange information (as memorization constraints) every\nfew epochs (to simulate a group of human learners). Dis-\ntributing our algorithm over different models of parallel\ncomputing appears to be a challenging problem. The adap-\ntion of the algorithm to parallel computing will surely re-\nquire creative upgrades in the algorithm design.\nReferences\nAbu-Mostafa, Yaser, Magdon-Ismail, Malik, and Lin,\nHsuan-Tien. Learning From Data: A Short Course. aml-\nbook.com, 2012.\nBengio, Yoshua. Deep learning of representations for un-\nsupervised and transfer learning. Unsup. and Transfer\nLearning Challenges in ML, 7:19, 2012.\nBengio,\nYoshua,\nLamblin,\nPascal,\nPopovici,\nDan,\nLarochelle, Hugo, et al. Greedy layer-wise training of\ndeep networks. NIPS, 19:153, 2007.\nBoureau, Y-lan, Cun, Yann L, et al. Sparse feature learning\nfor deep belief networks. In NIPS, pp. 1185–1192, 2008.\nChen, Gang. Deep learning with nonparametric clustering.\narXiv preprint arXiv:1501.03084, 2015.\nCoates, Adam and Ng, Andrew Y. Learning feature repre-\nsentations with k-means. In Neural Networks: Tricks of\nthe Trade, pp. 561–580. Springer, 2012.\nCottrell, Garrison and Munro, Paul. Principal components\nanalysis of images via back propagation. In Proc. SPIE\n1001, 1988.\nDeng, Li, Hinton, Geoffrey, and Kingsbury, Brian. New\ntypes of deep neural network learning for speech recog-\nnition and related applications: An overview. In Proc.\nICASSP, pp. 8599–8603. IEEE, 2013.\nErhan, Dumitru, Bengio, Yoshua, Courville, Aaron, Man-\nzagol, Pierre-Antoine, Vincent, Pascal, and Bengio,\nSamy. Why does unsupervised pre-training help deep\nlearning? JMLR, 11:625–660, 2010.\nFaraoun, KM and Boukelif, A. Neural networks learning\nimprovement using the k-means clustering algorithm to\ndetect network intrusions. INFOCOMP Journal of Com-\nputer Science, 5(3):28–36, 2006.\nFukumizu, Kenji and Amari, Shun-ichi. Local minima and\nplateaus in hierarchical structures of multilayer percep-\ntrons. Neural Networks, 13(3):317–327, 2000.\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Do-\nmain adaptation for large-scale sentiment classiﬁcation:\nA deep learning approach. In Proc. ICML, pp. 513–520,\n2011.\nGori, Marco and Tesi, Alberto.\nOn the problem of lo-\ncal minima in backpropagation. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 14(1):76–86,\n1992.\nHinton, Geoffrey, Osindero, Simon, and Teh, Yee-Whye.\nA fast learning algorithm for deep belief nets. Neural\ncomputation, 18(7):1527–1554, 2006.\nHoff Jr, ME. Learning phenomena in networks of adaptive\nswitching circuits. PhD thesis, Department of Electrical\nEngineering, Stanford University, 1962.\nHornik, Kurt, Stinchcombe, Maxwell, and White, Halbert.\nMultilayer feedforward networks are universal approxi-\nmators. Neural networks, 2(5):359–366, 1989.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In NIPS, pp. 1097–1105, 2012.\nLeCun, Yann and Bengio, Yoshua. Convolutional networks\nfor images, speech, and time series. The handbook of\nbrain theory and neural networks, 3361(10), 1995.\nLee, Honglak, Grosse, Roger, Ranganath, Rajesh, and Ng,\nAndrew Y. Convolutional deep belief networks for scal-\nable unsupervised learning of hierarchical representa-\ntions. In Proc. ICML, pp. 609–616. ACM, 2009a.\nLee, Honglak, Pham, Peter, Largman, Yan, and Ng, An-\ndrew Y. Unsupervised feature learning for audio clas-\nsiﬁcation using convolutional deep belief networks. In\nNIPS, pp. 1096–1104, 2009b.\nLichman, M. UCI machine learning repository, 2013. URL\nhttp://archive.ics.uci.edu/ml.\nMartens, James. Deep learning via hessian-free optimiza-\ntion. In Proc. ICML, pp. 735–742, 2010.\nNgiam, Jiquan, Coates, Adam, Lahiri, Ahbik, Prochnow,\nBobby, Le, Quoc V, and Ng, Andrew Y. On optimization\nmethods for deep learning. In Proc. ICML, pp. 265–272,\n2011.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\nScikit-learn: Machine learning in Python.\nJournal of\nMachine Learning Research, 12:2825–2830, 2011.\nNode-By-Node Greedy Deep Learning for Interpretable Features\nPoultney, Christopher, Chopra, Sumit, Cun, Yann L, et al.\nEfﬁcient learning of sparse representations with an\nenergy-based model. In NIPS, pp. 1137–1144, 2006.\nRosenblatt, Frank. The perceptron: a probabilistic model\nfor information storage and organization in the brain.\nPsychological review, 65(6):386, 1958.\nRumelhart, DE, Hinton, GE, and Williams, RJ.\nLearn-\ning internal representation by back propagation. Parallel\ndistributed processing: exploration in the microstructure\nof cognition, 1, 1986.\nVapnik, Vladimir. The nature of statistical learning theory.\nSpringer Science, 2000.\nVincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and\nManzagol, Pierre-Antoine.\nExtracting and composing\nrobust features with denoising autoencoders. In Proc.\nICML, pp. 1096–1103, 2008.\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Ben-\ngio, Yoshua, and Manzagol, Pierre-Antoine. Stacked de-\nnoising autoencoders: Learning useful representations in\na deep network with a local denoising criterion. JMLR,\n11:3371–3408, 2010.\nWeston, Jason, Ratle, Fr´ed´eric, Mobahi, Hossein, and Col-\nlobert, Ronan. Deep learning via semi-supervised em-\nbedding. In Neural Networks: Tricks of the Trade, pp.\n639–655. Springer, 2012.\nWidrow, B. Adaptive switching circuits. In IRE WESCON\nConvention Record, pp. 96–104, 1960.\nYu, Dong and Deng, Li. Deep learning and its applications\nto signal and information processing. Signal Processing\nMagazine, IEEE, 28(1):145–154, 2011.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-02-19",
  "updated": "2016-02-19"
}