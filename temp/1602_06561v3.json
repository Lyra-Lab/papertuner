{
  "id": "http://arxiv.org/abs/1602.06561v3",
  "title": "Deep Learning in Finance",
  "authors": [
    "J. B. Heaton",
    "N. G. Polson",
    "J. H. Witte"
  ],
  "abstract": "We explore the use of deep learning hierarchical models for problems in\nfinancial prediction and classification. Financial prediction problems -- such\nas those presented in designing and pricing securities, constructing\nportfolios, and risk management -- often involve large data sets with complex\ndata interactions that currently are difficult or impossible to specify in a\nfull economic model. Applying deep learning methods to these problems can\nproduce more useful results than standard methods in finance. In particular,\ndeep learning can detect and exploit interactions in the data that are, at\nleast currently, invisible to any existing financial economic theory.",
  "text": "Deep Learning in Finance\nJ. B. Heaton ∗\nN. G. Polson †\nJ. H. Witte ‡\nFebruary 2016\nAbstract\nWe explore the use of deep learning hierarchical models for problems in ﬁnancial prediction and\nclassiﬁcation. Financial prediction problems – such as those presented in designing and pricing\nsecurities, constructing portfolios, and risk management – often involve large data sets with\ncomplex data interactions that currently are diﬃcult or impossible to specify in a full economic\nmodel.\nApplying deep learning methods to these problems can produce more useful results\nthan standard methods in ﬁnance. In particular, deep learning can detect and exploit inter-\nactions in the data that are, at least currently, invisible to any existing ﬁnancial economic theory.\nKey Words: Deep Learning, Machine Learning, Big Data, Artiﬁcial Intelligence, LSTM Mod-\nels, Finance, Asset Pricing, Volatility\n∗Conjecture LLC, jb@conjecturellc.com\n†Booth School of Business, University of Chicago, ngp@chicagobooth.edu\n‡Department of Mathematics, University College London, and Conjecture LLC, jhw@conjecturellc.com\n1\narXiv:1602.06561v3  [cs.LG]  14 Jan 2018\n1\nIntroduction\nFinancial prediction problems are of great practical and theoretical interest. They are also quite\ndaunting. Theory suggests that much information relevant to ﬁnancial prediction problems may\nbe spread throughout available economic and other data, an idea that also gains support from\nthe many disparate data sources that diﬀerent market participants watch for clues on future price\nmovements.\nDealing with this variety of data sources is diﬃcult. The collection of possibly relevant data is\nvery large, while the importance of the data and the potentially complex non-linear interactions in\nthe data are not well speciﬁed by ﬁnancial economic theory. In practice, this results in a plethora\nof predictive models, many with little theoretical justiﬁcation and subject to over-ﬁtting and poor\npredictive out-of-sample performance.\nWhat is needed is a method able to learn those complex features of the data inputs which lead to\ngood predictions of the target output variables (such as an asset or portfolio return).\nIn this paper, we introduce deep learning hierarchical decision models for problems in ﬁnancial pre-\ndiction and classiﬁcation. The deep learning predictor has a number of advantages over traditional\npredictors, which include that\n• input data can be expanded to include all items of possible relevance to the prediction problem,\n• non-linearities and complex interactions among input data are accounted for, which can help\nincrease in-sample ﬁt versus traditional models,\n• over-ﬁtting is more easily avoided.\nOur paper continues as follows.\nSection 2 introduces the deep learning framework.\nSection 3\npresents three ﬁnance applications of the deep learning framework. Section 4 presents an example.\nSection 5 concludes.\nA guiding principle throughout our paper is the construction of predictive models whose inputs\nare high-dimensional. See Breiman (2001) for a discussion that contrasts predictive algorithmic\nmodelling with traditional statistical approaches.\n2\nDeep Learning\nWe begin by introducing the general theoretical deep learning framework as well as several speciﬁ-\ncations.\n2.1\nArchitecture\nDeep learning is a form of machine learning.\nMachine learning is using data to train a model\nand then using the trained model to make predictions from new data. The fundamental machine\n2\nlearning problem is to ﬁnd a predictor of an output Y given an input X. A learning machine is\ndeﬁned as an input-output mapping Y = F(X), where the input space is high-dimensional and we\nwrite\nY = F(X) where X = (X1, . . . , Xp),\nand a predictor is denoted by ˆY (X) := F(X). The output T can be continuous, discrete as in\nclassiﬁcation, or mixed.\nFor example, in a classiﬁcation problem, we need to learn a mapping\nF : X →Y , where Y ∈{1, . . . , K} indexes categories.\nAs a form of machine learning, deep learning trains a model on data to make predictions, but is\ndistinguished by passing learned features of data through diﬀerent layers of abstraction. Raw data\nis entered at the bottom level, and the desired output is produced at the top level, the result of\nlearning through many levels of transformed data. Deep learning is hierarchical in the sense that,\nin every layer, the algorithm extracts features into factors, and a deeper level’s factors become the\nnext level’s features.\nSpeciﬁcally, a deep learning architecture can be described as follows. Let f1, . . . , fL be given univari-\nate activation functions for each of the L layers. Activation functions are non-linear transformations\nof weighted data. A semi-aﬃne activation rule is then deﬁned by\nfW,b\nl\n:= fl\n\n\nNl\nX\nj=1\nWljXj + bl\n\n= fl(WlXl + bl) ,\n1 ≤l ≤L,\nwhich implicitly needs the speciﬁcation of the number of hidden units Nl. Our deep predictor,\ngiven the number of layers L, then becomes the composite map\nˆY (X) := F(X) =\n\u0010\nfW1,b1\n1\n◦. . . ◦fWL,bL\nL\n\u0011\n(X) .\nPut simply, we model a high dimensional mapping, F, via the superposition of univariate semi-aﬃne\nfunctions. (Similar to a classic basis decomposition, the deep approach uses univariate activation\nfunctions to decompose a high dimensional X.)\nWe let Z(l) denote the l-th layer, and so X = Z(0). The ﬁnal output is the response Y , which can\nbe numeric or categorical. The explicit structure of a deep prediction rule is then\nZ(1) = f(1) \u0010\nW (0)X + b(0)\u0011\n,\nZ(2) = f(2) \u0010\nW (1)Z(1) + b(1)\u0011\n,\n. . .\nZ(L) = f(L) \u0010\nW (L−1)Z(L−1) + b(L−1)\u0011\n,\nˆY (X) = W (L)Z(L) + b(L) .\nHere, W (l) are weight matrices, and b(l) are threshold or activation levels.\nDesigning a good\npredictor depends crucially on the choice of univariate activation functions f(l).\nThe Z(l) are hidden features (or factors) which the algorithm extracts. One particular feature is that\n3\nthe weight matrices Wl ∈RNl×Nl−1 are matrix valued. This gives the predictor great ﬂexibility to\nuncover non-linear features of the data – particularly so in ﬁnance data, since the estimated hidden\nfeatures Z(l) can represent portfolios of payouts. The choice of the dimension Nl is key, however,\nsince if a hidden unit (columns of Wl) is dropped at layer l, then it eliminates all terms above it in\nthe layered hierarchy.\nPut diﬀerently, the deep approach employs hierarchical predictors comprising of a series of L non-\nlinear transformations applied to X. Each of the L transformations is referred to as a layer, where\nthe original input is X, the output of the ﬁrst transformation is the ﬁrst layer, and so on, with the\noutput ˆY as the (L + 1)-th layer. We use l ∈{1, . . . , L} to index the layers from 1 to L, which are\ncalled hidden layers. The number of layers L represents the depth of our architecture.\nCommonly used activation functions are sigmoidal (e.g., 1/(1 + exp(−x)), cosh(x), or tanh(x)),\nheaviside gate functions (e.g., I(x > 0)), or rectiﬁed linear units (ReLU) max{x, 0}.\nReLU’s\nespecially have been found to lend themselves well to rapid dimension reduction. A deep learning\npredictor is a data reduction scheme that avoids the curse of dimensionality through the use of\nunivariate activation functions. See Kolmorogov (1957), Lorenz (1976), Gallant and White (1988),\nHornik et al. (1989), and Poggio and Girosi (1990) for further discussion.\n2.2\nTraining a Deep Architecture\nConstructing a deep learner requires a number of steps. It is common to split the data-set into three\nsubsets, namely training, validation, and testing. The training set is used to adjust the weights of\nthe network. The validation set is used to minimize the over-ﬁtting and relates to the architecture\ndesign (a.k.a. model selection). Finally, testing is used to conﬁrm the actual predictive power of a\nlearner.\nOnce the activation functions, size, and depth of the learning routine have been chosen, we need\nto solve the training problem of ﬁnding ( ˆW,ˆb), where\nˆW = ( ˆW0, . . . , ˆWL) and ˆb = (ˆb0, . . . ,ˆbL)\ndenote the learning parameters which we compute during training. To do this, we need a training\ndataset D = {Y (i), X(i)}T\ni=1 of input-output pairs and a loss function L(Y, ˆY ) at the level of the\noutput signal. In its simplest form, we solve\narg minW,b\n1\nT\nT\nX\ni=1\nL(Yi, ˆY W,b(Xi)) .\n(1)\nOften, the L2-norm for a traditional least squares problem is chosen as error measure, and if we\nthen minimize the loss function\nL(Yi, ˆY (Xi)) = ∥Yi −ˆY (Xi)∥2\n2 ,\nour target function (1) becomes the mean-squared error (MSE) over the training dataset D =\n{Y (i), X(i)}T\ni=1.\n4\nIt is common to add a regularization penalty, denoted by φ(W, b), to avoid over-ﬁtting and to\nstabilize our predictive rule. We combine this with the loss function via a parameter λ > 0, which\ngauges the overall level of regularization. We then need to solve\narg minW,b\n1\nT\nT\nX\ni=1\nL(Yi, ˆY W,b(Xi)) + λφ(W, b) .\n(2)\nThe choice of the amount of regularization, λ, is a key parameter.\nThis gauges the trade-oﬀ\npresent in any statistical modelling that too little regularization will lead to over-ﬁtting and poor\nout-of-sample performance.\nIn many cases, we will take a separable penalty, φ(W, b) = φ(W) + φ(b). The most useful penalty\nis the ridge or L2-norm, which can be viewed as a default choice, namely\nφ(W) = ∥W∥2\n2 =\nT\nX\nl=1\nW ⊤\ni Wi.\nOther norms include the lasso, which corresponds to an L1-norm, and which can be used to induce\nsparsity in the weights and/or oﬀ-sets. The ridge norm is particularly useful when the amount of\nregularization, λ, has itself to be learned. This is due to the fact that there are many good predictive\ngeneralization results for ridge-type predictors. When sparsity in the weights is paramount, it is\ncommon to use a lasso L1-norm penalty.\n2.2.1\nProbabilistic Interpretation\nIn a traditional probabilistic setting, we could view the output Y as a random variable generated\nby a probability model p(Y |Y W,b(X)), where the conditioning is on the predictor ˆY (X).\nThe\ncorresponding loss function is then\nL(Y, ˆY ) = −log p(Y |Y\nˆ\nW,ˆb(X)),\nnamely the negative log-likelihood. For example, when predicting the probability of default, we\nhave a multinomial logistic regression model which leads to a cross-entropy loss function.\nFor\nmultivariate normal models in particular (which includes many ﬁnancial time series), the L2-norm\nbecomes a suitable error measure.\nProbabilistically, the regularization term, λφ(W, b), can be viewed as a negative log-prior distribu-\ntion over parameters, namely\n−log p(φ(W, b)) = λφ(W, b),\np(φ(W, b)) = C exp(−λφ(W, b)),\nwhere C is a suitable normalization constant. This framework then provides a correspondence with\nBayes learning. Our deep predictor is simply a regularized maximum a posteriori (MAP) estimator.\n5\nWe can show this using Bayes rule as\np(W, b|D) ∝p(Y |Y W,b(X))p(W, b)\n∝exp\n\u0010\n−log p(Y |Y W,b(X)) −log p(W, b)\n\u0011\n,\nand the deep learning predictor satisﬁes\nˆY := Y\nˆ\nW,ˆb(X) where ( ˆW,ˆb) := arg minW,b log p(W, b|D),\nand\n−log p(W, b|D) =\nT\nX\ni=1\nL(Y (i), Y W,b(X(i))) + λφ(W, b)\nis the log-posterior distribution over parameters given the training data, D = {Y (i), X(i)}T\ni=1.\n2.2.2\nCross Validation\nCross validation is a technique by which we split our training data into complementary subset to\nthen conduct analysis and validation on diﬀerent sets, aiming to reduce over-ﬁtting and increase\nout-of-sample performance.\nIn particular, when training on time series, we may split our training data into disjoint time\nperiods of identical length, which is particularly desirable in ﬁnancial applications where reliable\ntime consistent predictors are hard to come by and have to be trained and tested extensively.\nCross validation also provides a tool to decide what levels of regularization lead to good gener-\nalization (i.e., prediction), which is the classic variance-bias trade-oﬀ. A key advantage of cross\nvalidation (over traditional statistical metrics such as t-ratios and p-values) is that it also allows\nus to assess the size and depth of the hidden layers, that is, solve the model selection problem of\nchoosing L and Nl for 1 ≤l ≤L . This ability to pragmatically and seamlessly solve the model\nselection and estimation problems is one of the reasons for the current widespread use of machine\nlearning methods.\n2.2.3\nBack-propagation\nThe common numerical approach for the solution of (2) is a form of stochastic gradient descent,\nwhich adapted to a deep learning setting is usually called back-propagation. One caveat of back-\npropagation in this context is the multi-modality of the system to be solved (and the resulting slow\nconvergence properties), which is the main reason why deep learning methods heavily rely on the\navailability of large computational power.\nOne of the advantages of using a deep network is that ﬁrst-order derivative information is directly\navailable. There are tensor libraries available that directly calculate\n∇W,bL(Yi, ˆY W,b(Xi))\n6\nusing the chain rule across the training data-set. For ultra-large data-sets, we use mini-batches and\nstochastic gradient descent (SGD) to perform this optimization, see LeCun et al. (2012). An active\narea of research is the use of this information within a Langevin MCMC algorithm that allows\nsampling from the full posterior distribution of the architecture. The deep learning model by its\nvery design is highly multi-modal, and the parameters are high dimensional and in many cases\nunidentiﬁed in the traditional sense. Traversing the objective function is the desired problem, and\nhandling the multi-modal and slow convergence of traditional decent methods can be alleviated\nwith proximal algorithms such as the alternating method of multipliers (ADMM), as has been\ndiscussed in Polson et al. (2015 a, b).\n2.3\nPredictive Performance\nThere are two key training problems that can be addressed using the predictive performance of an\narchitecture.\n(i) How much regularization to add to the loss function. As indicated before, one approach is\nto use cross validation and to teach the algorithm to calibrate itself to a training data. An\nindependent hold-out data set is kept separately to perform an out-of-sample measurement\nof the training success in a second step. As we vary the amount of regularization, we obtain a\nregularization path and choose the level of regularization to optimize out-of-sample predictive\nloss. Another approach is to use Stein’s unbiased estimator of risk (SURE).\n(ii) A more challenging problem is to train the size and depth of each layer of the architecture,\ni.e., to determine L and N = (N1, . . . , NL). This is known as the model selection problem.\nIn the next subsection, we will describe a technique known as dropout, which solves this\nproblem.\nStein’s unbiased estimator of risk (SURE) proceeds as follows. For a stable predictor, ˆY , we can\ndeﬁne the degrees of freedom of a predictor by df = E\n\u0010PT\ni=1 ∂ˆYi/∂Yi\n\u0011\n. Then, given the scalability\nof our algorithm, the derivative ∂ˆY /∂Y is available using the chain rule for the composition of the\nL layers.\nNow let the in-sample MSE be given by err = ||Y −ˆY ||2\n2 and, for a future observation Y ⋆, the\nout-of-sample predictive MSE is\nErr = EY ⋆\n\u0010\n||Y ⋆−ˆY ||2\n2\n\u0011\n.\nIn expectation, we then have\nE (Err) = E\n\u0010\nerr + 2Var( ˆY , Y )\n\u0011\n.\nThe latter term can be written in terms of df as a covariance. Stein’s unbiased risk estimate then\nbecomes\nd\nErr = ||Y −ˆY ||2 + 2σ2\nn\nX\ni=1\n∂ˆYi\n∂Yi\n.\n7\nFigure 1: The hierarchical structure of several adaptive linear layers in a deep learning routine allows the extraction\nof non-linear features from the input data which can then be combined to a description of the desired target variable.\nThis way, for dynamic in- and outputs, we obtain a deep feature policy (DFP) which for every combination of inputs\ntells us which corresponding action gives us the best approximation of the target variable. In the above picture, we see\nthe setup for a DFP which through two hidden layers approximates the S&P500 based on the ten largest companies\nin the index.\nAs before, models with the best predictive MSE are favoured.\n2.4\nDropout for Model Selection\nDropout is a model selection technique. It is designed to avoid over-ﬁtting in the training process,\nand does so by removing input dimensions in X randomly with a given probability p. In a simple\nmodel with one hidden layer, we replace the network\nY (l)\ni\n= f(Z(l)\ni ),\nZ(l)\ni\n= W (l)\ni X(l) + b(l)\ni ,\nwith the dropout architecture\nD(l)\ni\n∼Ber(p),\n˜Y (l)\ni\n= D(l) ⋆X(l),\nY (l)\ni\n= f(Z(l)\ni ),\nZ(l)\ni\n= W (l)\ni X(l) + b(l)\ni .\n8\nIn eﬀect, this replaces the input X by D ⋆X, where ⋆denotes the element-wise product and D is\na matrix of independent Bernoulli Ber(p) distributed random variables.\nIt is instructive to see how this aﬀects the underlying loss function and optimization problem. For\nexample, suppose that we wish to minimise MSE, L(Y, ˆY ) = ∥Y −ˆY ∥2\n2, then, when marginalizing\nover the randomness, we have a new objective\narg minW ED∼Ber(p)∥Y −W(D ⋆X)∥2\n2 ,\nwhich is equivalent to\narg minW ∥Y −pWX∥2\n2 + p(1 −p)∥ΓW∥2\n2 ,\nwhere Γ = (diag(X⊤X))\n1\n2 . We can also interpret this last expression as a Bayesian ridge regression\nwith a g-prior. Put simply, dropout reduces the likelihood of over-reliance on small sets of input\ndata in training. See Hinton and Salakhutdinov (2006) and Srivastava et al. (2014). Lopes and\nWest (2004) provide a fully Bayesian approach to factor selection. Dropout can be viewed as the\noptimization version of the traditional spike-and-slab prior that has proven so popular in Bayesian\nmodel averaging.\nAnother application of dropout regularization is the choice of the number of hidden units in a layer.\n(This can be achieved if we drop units of the hidden rather than the input layer and then establish\nwhich probability p gives the best results). It is worth recalling though, as we have stated before,\nthat one of the dimension reduction properties of a network structure is that once a variable from\na layer is dropped, all terms above it in the network also disappear. This is just the nature of a\ncomposite structure for the deep predictor in (2.1).\nWe now turn to describing three widely used architecture designs that have become commonplace\nin applications of machine learning, namely auto-encoders, rectiﬁed neural networks (RNNs), and\nlong short term memory (LTSM) models. In Section 4, we provide an application of auto-encoders\nto smart asset indexing problems.\n2.5\nAuto-encoder\nAn auto-encoder is a deep learning routine which trains the architecture to approximate X by itself\n(i.e., X = Y ) via a bottleneck structure. This means we select a model F W,b(X) which aims to\nconcentrate the information required to recreate X. Put diﬀerently, an auto-encoder creates a more\ncost eﬀective representation of X.\nFor example, under an L2-loss function, we wish to ﬁnd\narg minW,B ∥F W,b(X) −X∥2\n2\nsubject to a regulariziation penalty on the weights and oﬀsets.\nIn an auto-encoder, for a training data set {X1, X2, . . .}, we set the target values as Yi = Xi. A\nstatic auto-encoder with two linear layers, akin to a traditional factor model, can be written as a\n9\ndeep learner as\nz(2) = W (1)X + b(1),\na(2) = f2(z(2)),\nz(3) = W (2)a(2) + b(2),\nY = F W,b(X) = a(3) = f3(z(3)),\nwhere a(2), a(3) are activation levels. It is common to set a(1) = X. The goal is to learn the weight\nmatrices W (1), W (2). If X ∈RN, then W (1) ∈RM,N and W (1) ∈RN,M, where M ≪N provides\nthe auto-encoding at a lower dimensional level.\nIf W2 is estimated from the structure of the training data matrix, then we have a traditional factor\nmodel, and the W1 matrix provides the factor loadings. (We note that PCA in particular falls into\nthis category, as we have seen in (3).) If W2 is estimated based on the pair ˆX = {Y, X} = X (which\nmeans estimation of W2 based on the structure of the training data matrix with the speciﬁc auto-\nencoder objective), then we have a sliced inverse regression model. If W1 and W2 are simultaneously\nestimated based on the training data X, then we have a two layer deep learning model.\nA dynamic one layer auto-encoder for a ﬁnancial time series (Yt) can, for example, be written as a\ncoupled system of the form\nYt = WxXt + WyYt−1 and\n\u0012\nXt\nYt−1\n\u0013\n= WYt .\nWe then need to learn the weight matrices Wx and Wy. Here, the state equation encodes and the\nmatrix W decodes the Yt vector into its history Yt−1 and the current state Xt.\nThe auto-encoder demonstrates nicely that in deep learning we do not have to model the variance-\ncovariance matrix explicitly, as our model is already directly in predictive form. (Given an estimated\nnon-linear combination of deep learners, there is an implicit variance-covariance matrix, but that\nis not the driver of the method.)\n2.6\nLong Short Term Memory Models (LSTMs)\nTraditional rectiﬁed neural nets (RNNs) can learn complex temporal dynamics via the set of deep\nrecurrence equations\nZt = f(WxzXt + Wzz + bx),\nYt = f(WhzZt + bz),\nwhere Xt is the input, Zt is the hidden layer with N hidden units, and Yt is the output at time t.\nFor a length T input sequence, the updates are computed sequentially.\nThough RNNs have proven successful on tasks such as speech recognition and text generation (see\nDean et al. 2012 and Lake et al. 2016), they have diﬃculty in learning long-term dynamics, due in\npart to the vanishing and exploding gradients that can result from propagating the gradients down\n10\nFigure 2: A deep auto-encoder depicted for the ten largest companies of the S&P500. The hidden four unit layer\nof the deep auto-encoder compresses the aggregate information contained in all considered stocks and then produces a\nreplication of every single input stock. If, instead of ten stocks, we compress all 500 stocks of the S&P500 by sending\nthem through the hidden bottleneck structure, we obtain a much more cost eﬀective representation of the original\nindex.\nthrough the many layers (corresponding to time) of the recurrent network.\nLong-short-term-memories (LSTMs) are a particular form of recurrent network which provide a\nsolution by incorporating memory units. This allows the network to learn when to forget previous\nhidden states and when to update hidden states given new information. Models with hidden units\nwith varying connections within the memory unit have been proposed in the literature with great\nempirical success. Speciﬁcally, in addition to a hidden unit Zt, LSTMs include an input gate, a\nforget gate, an input modulation gate, and a memory cell. The memory cell unit combines the\nprevious memory cell unit which is modulated by the forget and input modulation gate together\nwith the previous hidden state, modulated by the input gate. These additional cells enable an\nLSTM architecture to learn extremely complex long-term temporal dynamics that a vanilla RNN\nis not capable of. Additional depth can be added to LSTMs by stacking them on top of each other,\nusing the hidden state of the LSTM as the input to the next layer.\n11\nAn architecture for an LSTM model might be\nFt = σ(W T\nf [Zt−1, Xt] + bf),\nIt = σ(W T\ni [Zt−1, Xt] + bi),\n¯Ct = tanh(W T\nc [Zt−1, Xt] + bc),\nCt = Ft ⊗Ct−1 + It ⊗¯Ct,\nZt = Ot ⊗tanh(Ct).\nThe key addition, compared to an RNN, is the hidden state Ct, the information is added or removed\nfrom the memory state via layers deﬁned via a sigmoid function σ(x) = (1 + e−x)−1 and point-wise\nmultiplication ⊗. The ﬁrst gate Ft ⊗Ct−1, called the forget gate, allows to throw away some data\nfrom the previous cell state. The next gate, It ⊗¯Ct, called the input gate, decides which values\nwill be updated. Then the new cell state is a sum of the previous cell state, passed through the\nforgot gate selected components of the [Zt−1, Xt] vector. This provides a mechanism for dropping\nirrelevant information from the past and adding relevant information from the current time step.\nFinally, the output layer, Ot ⊗tanh(Ct), returns tanh applied to the hidden state with some of the\nentries removed.\nAn LSTM model might potentially improve predictors by utilizing data from the past by memorizing\nvolatility patterns from previous periods. The LSTM model allows to automate the identiﬁcation\nof the temporal relations in the data, at the cost of larger sets of parameters to be trained.\nThere are numerous ﬁnance applications of LTSM models. They provide a new class of volatility\nmodels that are capable of capturing long-memory eﬀects in the underlying structure of asset return\nmovements.\n3\nFinance Applications\nWe now come to discuss deep learning speciﬁcally in the context of ﬁnance. For areas of ﬁnance\napplications, see Fama and French (1992, 2008), Engle (1982), Campbell, Lo, and MacKinley\n(1997), Singleton (2006), and Daniel and Titman (2006). Hutchison, Lo, and Poggio (1994) provide\na shallow learner for option pricing.\n3.1\nDeep Factor Models versus Shallow Factor Models\nAlmost all shallow data reduction techniques can be viewed as consisting of a low dimensional\nauxiliary variable Z and a prediction rule speciﬁed by a composition of functions\nˆY = fW1,b1\n1\n(f2(W2X + b2)\n\u0001\n= fW1,b1\n1\n(Z), where Z := f2(W2X + b2).\nIn this formulation, we also recognize the previously introduced deep learning structure (2.1). The\nproblem of high dimensional data reduction in general is to ﬁnd the Z-variable and to estimate the\n12\nFigure 3: For the time period 2014/15, we see AAPL, MSFT, and XOM stock prices before (left) and after auto-\nencoding (right).\nlayer functions (f1, f2) correctly. In the layers, we want to uncover the low-dimensional Z-structure\nin a way that does not disregard information about predicting the output Y .\nPrincipal component analysis (PCA), reduced rank regression (RRR), linear discriminant analysis\n(LDA), project pursuit regression (PPR), and logistic regression are all shallow learners. See Wold\n(1956), Diaconis and Shahshahani (1984), Ripley (1996), Cook (2007), and Hastie et al. (2009) for\nfurther discussion.\nFor example, PCA reduces X to f2(X) using a singular value decomposition of the form\nZ = f2(X) = W ⊤X + b ,\n(3)\nwhere the columns of the weight matrix W form an orthogonal basis for directions of greatest\nvariance (which is in eﬀect an eigenvector problem). Similarly, for the case of X = (x1, . . . , xp),\nPPR reduces X to f2(X) by setting\nZ = f2(X) =\nN1\nX\ni=1\nfi(Wi1X1 + . . . + WipXp) .\nAs stated before, these types of dimension reduction are independent of y and can easily discard\ninformation that is valuable for predicting the desired output.\nSliced inverse regression (SIR)\novercomes this drawback somewhat by estimating the layer function f2 using data on both, Y and\n13\nX, but still operates independently of f1.\nDeep learning overcomes many classic drawbacks by jointly estimating f1 and f2 based on the full\ntraining data ˆX = {Yi, Xi}T\ni=1, using information on Y and X as well as their relationships, and\nby using L > 2 layers. If we choose to use non-linear layers, we can view a deep learning routine as\na hierarchical non-linear factor model, or, more speciﬁcally, as a generalized linear model (GLM)\nwith recursively deﬁned non-linear link functions.\nStock and Watson have used a similar approach when forecasting a single time series on inﬂation\nbased on a large numbers of predictors.\nAnother obvious application is cost eﬀective indexing\nreplication, where we are trying to create a small sub-portfolio which dynamics similar to the main\nindex (see also Section 4).\n3.2\nDefault Probabilities\nAnother area of great application of deep learning is credit risk analysis.\nThe goal of a deep\nlearning model is a feature representation of a high dimensional input space. For example, in image\nprocessing, one can think of the layers as ﬁrst representing objects, then object parts (faces), then\nedges, and ﬁnally pixels. A similar feature map can be found for the credit-worthiness of companies.\nWe can combine ﬁnancial asset return data with text data (earnings calls) and accounting data\n(book values, etc.) to obtain an image of the health of a ﬁrm.\nSpeciﬁcally, suppose that our observations Yi represents a multi-class 1-of-K indicator vector. We\nequate classes via Y = k for 1 ≤k ≤K. For example, the K classes might correspond to bond\nratings. At the extreme, we might have an indicator yi ∈{0, 1} which indicates bankrupt or not.\nWe need to model the probability of default. Suppose that Yi ∈{−1, 1} is categorical. Given the\noutput X, it is common to model the probability of default via a soft-max (or logic) activation\nfunction\np(Yi | W, b, X) =\n1\n1 + e ˆY W,b(X) ,\nwhere ˆY W,b(X) = W1f1(. . . + b1). We deﬁne ˆp(Xi) = arg max W,b p(Yi | W, b, Xi) as the maximum\nprobability estimator.\nGiven a multinomial likelihood p(Y, ˆY ), this then leads to a cross-entropy loss function\nL(Y, ˆY ) = −log p(Y, ˆY ) = −Y log ˆp(X).\nAlternatively, in its natural parameter space, we have the log-odds as a deep predictor ˆYi =\nlog(pi/(1 −pi). We have a multi-class predictor\np(Yi = k| ˆY\nˆ\nW,ˆb\ni\n(X)) = σk(W1Z1),\n14\nwhere σ(x) = 1/1 + ex. The negative log-likelihood is given by\nL(Yi, ˆYi) = −log p(Yi| ˆYi) = −log\nK\nY\nk=1\n( ˆYi,k)Yi,k = −\nK\nX\nk=1\nYi,k log ˆYi,k.\nTherefore, minimizing the cross-entropy cost functional is equivalent to a multi-class logistic likeli-\nhood function The gain from a deep learning approach is our ability to include the kitchen-sink into\nthe input space X. Feature extraction is the main output from a deep learner and these non-linear\ncontrast of input variables provide summaries of the tendency for ﬁrms to default.\n3.3\nEvent Studies\nLet y ∈S denote an observed output variable, where S = RN for regression and S = {1, . . . , K}\nfor classiﬁcation. We have a high dimensional input/covariate variable given by X = {Xt}T\nt=1 and\nXt ∈RN×M.\nWe can build a deep learner for event study analysis as follows. Given a series of input event\nembeddings X = (X1, . . . , Xn), we can use a weight matrix W1 ∈Rl to extract the l-possible\nevents. For example, l = 4 for earnings announcements during the year, and n = 252 for number\nof trading days. We now construct a hidden factor\nZj = W ⊤\n1 Xj−l+1,\nso we can measure the eﬀect of the l previous events on today’s return.\nWe might use a max-pooling activation function if we think that the eﬀect is based solely on the\nlargest value (i.e., max Zj), in which case the model ignores all other and focuses on the largest.\n4\nExample: Smart Indexing\nWhen aiming to replicate (or approximate) a stock index through a subset of stocks, there are two\nconceptual approaches we can choose from.\n(i) Identify a small group of stocks which historically have given a performance very similar to\nthat of the observed index.\n(ii) Identify a small group of stocks which historically have represented an over-proportionally\nlarge part of the total aggregate information of all the stocks the index comprises of.\nWhile, on the face of things, (i) and (ii) may appear very similar, they characterize, in fact, very\ndiﬀerent methodologies.\nMany classic approaches to index replication are essentially rooted in linear-regression, which is\npart of group (i). Frequently, by trial and error, we are trying to ﬁnd a small subset of stocks which\nin-sample gives a reasonable linear approximation of the considered index.\n15\nFigure 4: A deep auto-encoder compresses the the stocks of the S&P500. Above, we rank all stocks by their proximity\nto the auto-encoded information and create an equally weighted portfolio from the auto-encoder basis of the 10 leading\nstocks. Below, we use the leading ten (the most communal stocks) and the bottom ten (most individualistic) stocks\nto create an auto-encoder basis on which we train a deep feature policy (DFP) for the approximation of the S&P500.\n16\nThe deep learning version of (i) allows translating the input data through a hierarchical sequence\nof adaptive linear layers into a desired output, which means that, in training, even non-linear\nrelationships can be readily identiﬁed. Since every hidden layer provides a new interpretation of\nthe input features, we refer to the resulting strategy for approximation (or prediction) as a deep\nfeature policy (DFP), an example of which is given in Figure 1.\nThe availability of tailored non-linear relationships in deep learning makes the conventional objec-\ntive of (i), namely good in-sample approximation, an easily achieved triviality, and takes the focus\nstraight to training for out-of-sample performance (which brings us back to cross-validation and\ndropout, see Section 2).\nAnother weakness of (i) is that it ﬁts the ﬁnished (and through aggregation diluted) product.\nA deep auto-encoder avoids this problem by directly (rather than indirectly) approximating the\naggregate information contained in the considered family of index stocks. In Figure 2, a deep auto-\nencoder for a small set of ten stocks is depicted. In Figure 3, we see the stock paths before and\nafter compression.∗\nThe bottleneck structure of an auto-encoder creates a compressed set of information from which all\nstocks are re-created (through linear and non-linear relationships). Thus, for indexing, the stocks\nwhich are closest to the compressed core of the index can be interpreted as a non-linear basis of\nthe aggregate information of the considered family of stocks.\nIn Figure 4 at the top, we auto-encoded all stocks of the S&P500 over the period 2014/15. We\nthen ranked the stocks by how close they were to their own auto-encoded version; the closer, the\nhigher the communal information content of a stock. On the top right, we see the approximation of\nthe S&P500 obtained by simply investing in the ten stocks with the highest communal information\ncontent.\nWe notice that, while the ten stock auto-encoder basis is reasonable, the approximation is a little\noﬀ, particularly in the last seven months of the training period. It is instructive to observe how\nin-sample this deviation can easily be avoided by using a DFP index approximation.\nIn Figure 4 at the bottom, we combined the two sets of ten stocks with the highest and lowest\ncommunal information, respectively, and then trained a deep learning routine over the same period\n2014/15 to approximate the S&P500 index based on this expanded basis of twenty stocks.† For\nevery combination of inputs from the selected twenty stocks, the DFP gives us, based on the\nhierarchical composition of non-linear features extracted from the input data, an optimized action\nfor the approximation of the desired index.\nGiven suﬃcient diversity of the input data, a DFP can often be trained to approximate the target\ndata to almost arbitrarily accuracy, an improvement we now notice for the last six months of the\ntraining period in the bottom right chart in Figure 4.\nIn short, many classic models have had to focus on the wrong thing, namely in-sample approx-\nimation quality, due to their shortcomings in that area, while deep learning naturally addresses\n∗We use an auto-encoder with one hidden layer of 4 units and a sparsity constraint of ρ = 0.01 (to avoid training\nthe identity function).\n†We use a deep neural net with (4,2) hidden layers.\n17\nFigure 5: Having trained a 20 stock DFP auto-encoder basis as well as a simpler 10 stock deep auto-encoder basis\nfor the S&P500 on the two years 2014/15, we now consider the out-of-sample performance of the two approximations\nfor the periods 2010/11 and 2012/13. We observe that the superior ﬁtting qualities of the full DFP basis in-sample\nare out-weighed easily by the superior out-of-sample consistency of the generic deep auto-encoder basis.\nout-of-sample performance as optimization target.\nIn Figure 5, we apply our two example index trackers to the period 2010/14 as an out-of-sample\ntest. We notice how the previously superior DFP approximation is unreliable, while the simple\nauto-encoder basis (made up of ten stocks rich in communal information) provides a consistent\nindex replication. We conclude that, for index replication, auto-encoding as suggested by (ii) seems\nto be the more robust approach, and that the superior learning abilities of a DFP require careful\nhandling in training to achieve the desired result.\n5\nConclusion\nDeep learning presents a general framework for using large data sets to optimize predictive per-\nformance. As such, deep learning frameworks are well-suited to many problems – both practical\nand theoretical – in ﬁnance. This paper presents deep learning hierarchical decision models for\nproblems in ﬁnancial prediction and classiﬁcation. As we have demonstrated, deep learning has\nthe potential to improve – sometimes dramatically – on predictive performance in conventional\napplications. Our example on smart indexing in Section 4 presents just one way to implement deep\nlearning models in ﬁnance. Many other applications remain for development.\n18\nAt the same time, deep learning is likely to present signiﬁcant challenges to current thinking in\nﬁnance, including, most notably, the concept of market eﬃciency. Because it can model complex\nnon-linearities in the data, deep learning may be able to price assets to within arbitrarily small\npricing errors. Will this imply that markets are informationally eﬃcient, or will new tests of market\neﬃciency be necessary?\nOverall, it is unlikely that any theoretical models built from existing\naxiomatic foundations will be able to compete with the predictive performance of deep learning\nmodels. What this means for the future of ﬁnancial economics remains to be seen.\nIn the meantime, deep learning models are likely to exert greater and greater inﬂuence in the\npractice of ﬁnance, particularly where prediction is paramount.\nReferences\n[1] L. Breiman: Statistical modeling: the two cultures (with comments and a rejoinder by the\nauthor). Statistical Science, Vol. 16(3), pp. 199-231, 2001.\n[2] J. Y. Campbell, A. W. Lo and A. C. MacKinley: The econonmetrics of ﬁnancial markets.\nPrinceton University Press, 1997.\n[3] R. D. Cook: Fisher lecture: dimension reduction in regression, Statistical Science, pp. 1-26,\n2007.\n[4] K. Daniel and S. Titman: Market reactions to tangible and intangible information, Journal of\nFinance, Vol. 61, 1605-1643, 2006.\n[5] J. Dean, G. Corrado, R. Monga, et al.: Large scale distributed deep networks, Advances in\nNeural Information Processing Systems, pp. 1223-1231, 2012.\n[6] P. Diaconis and M. Shahshahani: On non-linear functions of linear combinations, SIAM Jour-\nnal on Scientiﬁc and Statistical Computing, Vol. 5(1), pp. 175-191, 1984.\n[7] R. Engle: Autoregressive conditional heteroscedasticity with estimates of the variance of United\nKingdom inﬂation, Econometrika, 50(4), 987-1007, 1982.\n[8] E. F. Fama and K. R. French: The cross-section of expected stock returns, Journal of Finance,\nVol. 47, pp. 427-465, 1992.\n[9] E. F. Fama and K. R. French: Dissecting anomalies, Journal of Finance, Vol. 53(4), pp.\n1653-1678, 2008.\n[10] A. R. Gallant and H. White: There exists a neural network that does not make avoidable\nmistakes, IEEE International Conference on Neural Networks, Vol. 1, pp. 657-664, 1988.\n[11] T. Hastie, R. Tibshirani, and J. Friedman: The elements of statistical learning, Vol 2, 2009.\n[12] G. E. Hinton and R. R. Salakhutdinov: Reducing the dimensionality of data with neural net-\nworks, Science, Vol. 313(5786), pp. 504-507, 2006.\n[13] K. Hornik, M. Stinchcombe, and H. White: Multilayer feedforward networks are universal\n19\napproximators, Neural networks, Vol. 2(5), pp. 359-366, 1989.\n[14] J. M. Hutchinson, A. W. Lo, and T. Poggio: A Nonparametric approach to pricing and hedging\nderivative securities via learning networks, Journal of Finance, Vol. 48(3), pp. 851-889, 1994.\n[15] A. Kolmogorov: The representation of continuous functions of many variables by superposition\nof continuous functions of one variable and addition, Dokl. Akad. Nauk SSSR, Vol. 114, pp.\n953-956, 1957.\n[16] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum: Human-level concept learning through\nprobabilistic program induction. Science, Vol. 3560, pp. 1332-1338, 2015\n[17] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Muller: Eﬃcient backprop, Neural networks:\nTricks of the trade, pp. 948, 2012.\n[18] H. F. Lopes and M. West: Bayesian Model Assessment in Factor Analysis. Statistica Sinica,\n14, pp. 41-67, 2004.\n[19] G. G. Lorentz: The 13th problem of Hilbert, Proceedings of Symposia in Pure Mathe-\nmatics, American Mathematical Society, Vol. 28, pp. 419-430, 1976.\n[20] T. Poggio and F. Girosi: Networks for approximation and learning, Proceedings of the\nIEEE, Vol. 78(9), pp. 1481-1497, 1990.\n[21] N. G. Polson, J. G. Scott, and B. T. Willard: Proximal algorithms in statistics and machine\nlearning, Statistical Science, 30, 559-581, 2015.\n[22] N. G. Polson, B. T. Willard, and M. Heidari: A statistical theory for Deep Learning, 2015.\n[23] B. D. Ripley: Pattern recognition and neural networks. Cambridge University Press, 1996.\n[24] K. J. Singelton: Empirical Dynamic Asset Pricing. Princeton Univertsity Press, 2006.\n[25] Srivastava et al.: Dropout: a simple way to prevent neural networks from overﬁtting, Journal\nof Machine Learning Research, 15, 1929-1958, 2014.\n[26] H. Wold: Causal inference from observational data: a review of end and means, Journal of\nthe Royal Statistical Society, Series A (General), pages 28-61, 1956.\n20\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-02-21",
  "updated": "2018-01-14"
}