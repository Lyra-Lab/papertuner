{
  "id": "http://arxiv.org/abs/1602.07031v1",
  "title": "Mobile Big Data Analytics Using Deep Learning and Apache Spark",
  "authors": [
    "Mohammad Abu Alsheikh",
    "Dusit Niyato",
    "Shaowei Lin",
    "Hwee-Pink Tan",
    "Zhu Han"
  ],
  "abstract": "The proliferation of mobile devices, such as smartphones and Internet of\nThings (IoT) gadgets, results in the recent mobile big data (MBD) era.\nCollecting MBD is unprofitable unless suitable analytics and learning methods\nare utilized for extracting meaningful information and hidden patterns from\ndata. This article presents an overview and brief tutorial of deep learning in\nMBD analytics and discusses a scalable learning framework over Apache Spark.\nSpecifically, a distributed deep learning is executed as an iterative MapReduce\ncomputing on many Spark workers. Each Spark worker learns a partial deep model\non a partition of the overall MBD, and a master deep model is then built by\naveraging the parameters of all partial models. This Spark-based framework\nspeeds up the learning of deep models consisting of many hidden layers and\nmillions of parameters. We use a context-aware activity recognition application\nwith a real-world dataset containing millions of samples to validate our\nframework and assess its speedup effectiveness.",
  "text": "1\nMobile Big Data Analytics\nUsing Deep Learning and Apache Spark\nMohammad Abu Alsheikh, Dusit Niyato, Shaowei Lin, Hwee-Pink Tan, and Zhu Han\nAbstract—The proliferation of mobile devices, such as smart-\nphones and Internet of Things (IoT) gadgets, results in the recent\nmobile big data (MBD) era. Collecting MBD is unproﬁtable\nunless suitable analytics and learning methods are utilized for\nextracting meaningful information and hidden patterns from\ndata. This article presents an overview and brief tutorial of\ndeep learning in MBD analytics and discusses a scalable learning\nframework over Apache Spark. Speciﬁcally, a distributed deep\nlearning is executed as an iterative MapReduce computing on\nmany Spark workers. Each Spark worker learns a partial deep\nmodel on a partition of the overall MBD, and a master deep\nmodel is then built by averaging the parameters of all partial\nmodels. This Spark-based framework speeds up the learning of\ndeep models consisting of many hidden layers and millions of\nparameters. We use a context-aware activity recognition appli-\ncation with a real-world dataset containing millions of samples\nto validate our framework and assess its speedup effectiveness.\nIndex Terms—Distributed deep learning, big data, Internet of\nthings, cluster computing, context-awareness.\nI. INTRODUCTION\nMobile devices have matured as a reliable and cheap plat-\nform for collecting data in pervasive and ubiquitous sensing\nsystems. Speciﬁcally, mobile devices are (a) sold in mass-\nmarket chains, (b) connected to daily human activities, and\n(c) supported with embedded communication and sensing\nmodules. According to the latest trafﬁc forecast report by\nCisco Systems [1], half a billion mobile devices were globally\nsold in 2015, and the mobile data trafﬁc grew by 74% gen-\nerating 3.7 exabytes (1 exabyte = 1018 bytes) of mobile data\nper month. Mobile big data (MBD) is a concept that describes\na massive amount of mobile data which cannot be processed\nusing a single machine. MBD contains useful information for\nsolving many problems such as fraud detection, marketing and\ntargeted advertising, context-aware computing, and healthcare.\nTherefore, MBD analytics is currently a high-focus topic\naiming at extracting meaningful information and patterns from\nraw mobile data.\nDeep learning is a solid tool in MBD analytics. Speciﬁcally,\ndeep learning (a) provides high-accuracy results in MBD\nM. Abu Alsheikh is with the School of Computer Engineering, Nanyang\nTechnological University, Singapore 639798, and also with the Sense and\nSense-abilities Programme, Institute for Infocomm Research, Singapore\n138632 (e-mail: stumyhaa@i2r.a-star.edu.sg). D. Niyato is with the School of\nComputer Engineering, Nanyang Technological University, Singapore 639798\n(e-mail: dniyato@ntu.edu.sg). S. Lin is with the School of Engineering\nSystems and Design Pillar, Singapore University of Technology and Design,\nSingapore 487372 (e-mail: shaowei lin@sutd.edu.sg). H.-P. Tan is with the\nSchool of Information Systems, Singapore Management University, Singapore\n188065 (e-mail: hptan@smu.edu.sg). Z. Han is with the School of Electrical\nand Computer Engineering, University of Houston, USA 77004 (e-mail:\nzhan2@uh.edu).\nanalytics, (b) avoids the expensive design of handcrafted\nfeatures, and (c) utilizes the massive unlabeled mobile data\nfor unsupervised feature extraction. Due to the curse of\ndimensionality and size of MBD, learning deep models in\nMBD analytics is slow and takes anywhere from a few hours\nto several days when performed on conventional computing\nsystems. Arguably, most mobile systems are not delay tolerant\nand decisions should be made as fast as possible to attain high\nuser satisfaction.\nTo cope with the increased demand on scalable and adaptive\nmobile systems, this article presents a tutorial on developing\na framework that enables time-efﬁcient MBD analytics using\ndeep models with millions of modeling parameters. Our frame-\nwork is built over Apache Spark [2] which provides an open\nsource cluster computing platform. This enables distributed\nlearning using many computing cores on a cluster where the\ncontinuously accessed data is cached to running memory, thus\nspeeding up the learning of deep models by several folds. To\nprove the viability of the proposed framework, we implement a\ncontext-aware activity recognition system [3] on a computing\ncluster and train deep learning models using millions of\ndata samples collected by mobile crowdsensing. In this test\ncase, a client request includes accelerometer signals and the\nserver is programmed to extract the underlying human activity\nusing deep activity recognition models. We show signiﬁcant\naccuracy improvement of deep learning over conventional\nmachine learning methods, improving 9% over random forests\nand 17.8% over multilayer perceptions from [4]. Moreover,\nthe learning time of deep models is decreased as a result\nof the paralleled Spark-based implementation compared to a\nsingle machine computation. For example, utilizing 6 Spark\nworkers can speedup the learning of a 5-layer deep model\nof 20 million parameters by 4 folds as compared to a single\nmachine computing.\nThe rest of this article is organized as follows. Section II\npresents an overview of MBD and discusses the challenges of\nMBD analytics. Section III discusses the advantages and chal-\nlenges of deep learning in MBD analytics. Then, Section IV\nproposes a Spark-based framework for learning deep models\nfor time-efﬁcient MBD analytics within large-scale mobile\nsystems. Section V presents experimental analysis using a real-\nworld dataset. Interesting research directions are discussed in\nSection VI. Finally, Section VII concludes the article.\nII. MOBILE BIG DATA (MBD): CONCEPTS AND FEATURES\nThis section ﬁrst introduces an overview of MBD and then\ndiscusses the key characteristics which make MBD analytics\nchallenging.\narXiv:1602.07031v1  [cs.DC]  23 Feb 2016\n2\nA. The Era of MBD\nFigure 1 (a) shows a typical architecture of large-scale\nmobile systems used to connect various types of portable\ndevices such as smartphones, wearable computers, and IoT\ngadgets. The widespread installation of various types of sen-\nsors, such as accelerometer, gyroscope, compass, and GPS\nsensors, in modern mobile devices allows many new applica-\ntions. Essentially, each mobile device encapsulates its service\nrequest and own sensory data in stateless data-interchange\nstructure, e.g., Javascript object notation (JSON) format. The\nstateless format is important as mobile devices operate on\ndifferent mobile operating systems, e.g., Android, IOS, and\nTizen. Based on the collected MBD, a service server utilizes\nMBD analytics to discover hidden patterns and information.\nThe importance of MBD analytics stems from its roles in\nbuilding complex mobile systems that could not be assembled\nand conﬁgured on small datasets. For example, an activity\nrecognition application [3], [5] uses embedded accelerometers\nof mobile devices to collect proper acceleration data about\ndaily human activities. After receiving a request, the service\nserver maps the accelerometer data to the most probable\nhuman activities which are used to support many interactive\nservices, e.g., healthcare, smart building, and pervasive games.\nMBD analytics is more versatile than conventional big\ndata problems as data sources are portable and data trafﬁc\nis crowdsourced. MBD analytics deals with massive amount\nof data which is collected by millions of mobile devices.\nNext, we discuss the main characteristics of MBD which\ncomplicate data analytics and learning on MBD compared to\nsmall datasets.\nB. Challenges of MBD Analytics\nFigure 1 (b) shows the main recent technologies that have\nproduced the challenging MBD era: large-scale and high-\nspeed mobile networks, portability, and crowdsourcing. Each\ntechnology contributes in forming the MBD characteristics in\nthe following way.\n• Large-scale and high-speed mobile networks: The growth\nof mobile devices and high-speed mobile networks, e.g.,\nWiFi and cellular networks, introduces massive and\ncontentiously-increasing mobile data trafﬁc. This has\nbeen reﬂected in the following MBD aspects:\n– MBD is massive (volume). In 2015, 3.7 exabytes\nof mobile data was generated per month which is\nexpected to increase through the coming years [1].\n– MBD is generated at increasing rates (velocity).\nMBD ﬂows at a high rate which impacts the latency\nin serving mobile users. Long queuing time of re-\nquests results in less satisﬁed users and increased\ncost of late decisions.\n• Portability: Each mobile device is free to move inde-\npendently among many locations. Therefore, MBD is\nnon-stationary (volatility). Due to portability, the time\nduration for which the collected data is valid for decision\nmaking can be relatively short. MBD analytics should be\nfrequently executed to cope with the newly collected data\nsamples.\nMobile data input (unlabeled data)\nFeature represenetation\nHigh-level features\nEncoding\nDecoding\nEncoding\nDecoding\nFixed\nEncoding\nDecoding\nFixed\nFixed\nIncreasingly complex features\nCopy\nCopy\nCopy\nA neuron\n(1) First layer training\n(2) Second layer training\n(3) Third layer training\nFig. 2: Generative layer-wise training of a deep model. Each\nlayer applies nonlinear transformation to its input vector and\nproduces intrinsic features at its output.\n• Crowdsourcing: A remarkable trend of mobile applica-\ntions is crowdsourcing for pervasive sensing which in-\ncludes a massive data collection from many participating\nusers. Crowdsensing differs from conventional mobile\nsensing systems as the sensing devices are not owned\nby one institution but instead by many individuals from\ndifferent places. This has introduced the following MBD\nchallenges:\n– MBD quality is not guaranteed (veracity). This as-\npect is critical for assessing the quality uncertainty\nof MBD as mobile systems do not directly manage\nthe sensing process of mobile devices. Since most\nmobile data is crowdsourced, MBD can contain\nlow quality and missing data samples due to noise,\nmalfunctioning or uncalibrated sensors of mobile de-\nvices, and even intruders, e.g., badly-labeled crowd-\nsourced data. These low quality data points affect the\nanalytical accuracy of MBD.\n– MBD is heterogeneous (variety). The variety of MBD\narises because the data trafﬁc comes from many spa-\ntially distributed data sources, i.e., mobile devices.\nBesides, MBD comes in different data types due to\nthe many sensors that mobile devices support. For\nexample, a triaxial accelerometer generates proper\nacceleration measurements while a light sensor gen-\nerates illumination values.\nMBD analytics (value) is mainly about extracting knowledge\nand patterns from MBD. In this way, MBD can be utilized\nfor providing better service to mobile users and creating\nrevenues for mobile businesses. The next section discusses\ndeep learning as a solid tool in MBD analytics.\nIII. DEEP LEARNING IN MBD ANALYTICS\nDeep learning is a new branch of machine learning which\ncan solve a broad set of complex problem in MBD analytics,\ne.g., classiﬁcation and regression. A deep learning model\nconsists of simulated neurons and synapses which can be\ntrained to learn hierarchical features from existing MBD\nsamples. The resulting deep model can generalize and process\nunseen streaming MBD samples.\nFor simplicity, we present a general discussion of deep\nlearning methods without focusing on the derivations of par-\nticular techniques. Nonetheless, we refer interested readers to\n3\nService Server\nBase station\nCellular network\nWiFi network \n(802.11 wireless network)\n(a)\n(b)\nInternet backbone\nRouter\nDSL Modem\nSGSN\nGGSN\nGGSN: Gateway general packet radio service (GPRS) support node\nSGSN: Serving GPRS support node\nGateway\nMobile sensor network\nPortability\nTime-varying data patterns\nLarge-scale & high-\nspeed networks\n2.5 exabytes from 7.4 \nbillion mobile devices [1]\nCrowdsensing\nDistributed data sensing\nMobile big \ndata (MBD)\nVolatility\nVeracity\nVariety\nMBD analytics \n(Value)\nFig. 1: Illustration of the MBD era. (a) Typical architecture of a modern mobile network connecting smartphones, wearable\ncomputers, and IoT gadgets. (b) Main technological advances behind the MBD era.\nmore technical papers of deep belief networks [6] and stacked\ndenoising autoencoders [7]. A deep model can be scaled to\ncontain many hidden layers and millions of parameters which\nare difﬁcult to be trained at once. Instead, greedy layer-\nby-layer learning algorithms [6], [7] were proposed which\nbasically work as follows:\n1) Generative layer-wise pre-training: This stage requires\nonly unlabeled data which is often abundant and cheap\nto collect in mobile systems using crowdsourcing. Fig-\nure 2 shows the layer-wise tuning of a deep model.\nFirstly, one layer of neurons is trained using the un-\nlabeled data samples. To learn the input data structure,\neach layer includes encoding and decoding functions:\nThe encoding function uses the input data and the layer\nparameters to generate a set of new features. Then,\nthe decoding function uses the features and the layer\nparameters to produce a reconstruction of the input data.\nAs a result, a ﬁrst set of features is generated at the\noutput of the ﬁrst layer. Then, a second layer of neurons\nis added at the top of the ﬁrst layer, where the output\nof the ﬁrst layer is fed as input of the second layer.\nThis process is repeated by adding more layers until\na suitable deep model is formed. Accordingly, more\ncomplex features are learned at each layer based on the\nfeatures that were generated at its lower layer.\n2) Discriminative ﬁne-tuning: The model’s parameters\nwhich were initialized in the ﬁrst step are then slightly\nﬁne-tuned using the available set of labeled data to solve\nthe problem at hand.\nA. Deep Learning Advantages in MBD Analytics\nDeep learning provides solid learning models for MBD\nanalytics. This argument can be supported with the following\nadvantages of using deep learning in MBD analytics:\n• Deep learning scores high-accuracy results which are a\ntop priority for growing mobile systems. High-accuracy\nresults of MBD analytics are required for sustainable\nbusiness and effective decisions. For example, a poor\nfraud detection results in expensive loss of income for\nmobile systems. Deep learning models have been reported\nas state-of-the-art methods in solving many MBD tasks.\nFor example, the authors in [8] propose a method for\nindoor localization using deep learning and channel state\ninformation. In [9], deep learning is successfully applied\nto inference tasks in mobile sensing, e.g., activity and\nemotion recognition, and speaker identiﬁcation.\n• Deep learning generates intrinsic features which are\nrequired in MBD analytics. A feature is a measurement\nattribute extracted from sensory data to capture the un-\nderlying phenomenon being observed and enable more\neffective MBD analytics. Deep learning can automatically\nlearn high-level features from MBD, eliminating the need\nfor handcrafted features in conventional machine learning\nmethods.\n• Deep Learning can learn from unlabeled mobile data\nwhich minimizes the data labeling effort. In most mo-\nbile systems, labeled data is limited, as manual data\nannotation requires expensive human intervention which\nis both costly and time consuming. On the other hand,\nunlabeled data samples are abundant and cheap to collect.\nDeep learning models utilize unlabeled data samples for\ngenerative data exploration during a pre-training stage.\nThis minimizes the need for labeled data during MBD\nanalytics.\n• Multimodal deep learning. The “variety” aspect of MBD\nleads to multiple data modalities of multiple sensors (e.g.,\naccelerometer samples, audio, and images). Multimodal\ndeep learning [10] can learn from multiple modalities and\nheterogeneous input signals.\n4\nB. Deep Learning Challenges in MBD Analytics\nDiscussing MBD in terms of volume only and beyond the\nanalytical and proﬁt perspectives is incomplete and restricted.\nCollecting MBD is unproﬁtable unless suitable learning meth-\nods and analytics are utilized in extracting meaningful in-\nformation and patterns. Deep learning in MBD analytics is\nslow and can take a few days of processing time, which does\nnot meet the operation requirements of most modern mobile\nsystems. This is due to the following challenges:\n• Curse of dimensionality: MBD comes with “volume” and\n“velocity” related challenges. Historically, data analyt-\nics on small amounts of collected data (a.k.a. random\nsampling) was utilized. Despite the low computational\nburdens of random sampling, it suffers from poor perfor-\nmance on unseen streaming samples. This performance\nproblem is typically avoided by using the full set of\navailable big data samples which signiﬁcantly increases\nthe computational burdens.\n• Large-scale deep models: To fully capture the information\non MBD and avoid underﬁtting, deep learning models\nshould contain millions of free parameters, e.g., a 5-layer\ndeep model with 2000 neurons per layer contains around\n20 million parameters. The model free parameters are\noptimized using gradient-based learning [6], [7] which is\ncomputationally expensive for large-scale deep models.\n• Time-varying deep models: In mobile systems, the con-\ntinuous adaptation of deep models over time is required\ndue to the “volatility” characteristic of MBD.\nTo tackle these challenges, we next describe a scalable frame-\nwork for MBD analytics using deep learning models and\nApache Spark.\nIV. A SPARK-BASED DEEP LEARNING FRAMEWORK FOR\nMBD ANALYTICS\nLearning deep models in MBD analytics is slow and com-\nputationally demanding. Typically, this is due to the large\nnumber of parameters of deep models and the large number\nof MBD samples. Figure 3 shows the proposed architec-\nture for learning deep models on MBD with Apache Spark.\nApache Spark [2] is an open source platform for scalable\nMapReduce computing on clusters. The main goal of the\nproposed framework is speeding up MBD decision-making\nby parallelizing the learning of deep models to a high perfor-\nmance computing cluster. In short, the parallelization of a deep\nmodel is performed by slicing the MBD into many partitions.\nEach partition is contained in a resilient distributed dataset\n(RDD) which provides an abstraction for data distribution by\nthe Spark engine. Besides data caching, RDDs of a Spark-\nprogram natively support fault-tolerant executions and recover\nthe program operations at worker nodes.\nIn short, our Spark-based framework consists of two main\ncomponents: (1) a Spark master and (2) one or more Spark\nworkers. The master machine initializes an instance of the\nSpark driver that manages the execution of many partial\nmodels at a group of Spark workers. At each iteration of the\ndeep learning algorithm (Figure 2), each worker node learns a\npartial deep model on a small partition of the MBD and sends\nthe computed parameters back to the master node. Then, the\nmaster node reconstructs a master deep model by averaging\nthe computed partial models of all executor nodes.\nA. Parallelized Learning Collections\nLearning deep models can be performed in two main steps:\n(1) gradient computation, and (2) parameter update (see [6],\n[7] for the mathematical derivation). In the ﬁrst step, the\nlearning algorithm iterates through all data batches indepen-\ndently to compute gradient updates, i.e., the rate of change,\nof the model’s parameters. In the second step, the model’s\nparameters are updated by averaging the computed gradient\nupdates on all data batches. These two steps ﬁt the learning\nof deep models in the MapReduce programming model [11],\n[12]. In particular, the parallel gradient computation is realized\nas a Map procedure, while the parameter update step reﬂects\nthe Reduce procedure. The iterative MapReduce computing of\ndeep learning on Apache Spark is performed as follows:\n1) MBD partitioning: The overall MBD is ﬁrst split into\nmany partitions using the parallelize() API of Spark.\nThe resulting MBD partitions are stored into RDDs and\ndistributed to the worker nodes. These RDDs are crucial\nto speedup the learning of deep models as the memory\ndata access latency is signiﬁcantly shorter than the disk\ndata operations.\n2) Deep learning parallelism: The solution of a deep\nlearning problem depends on the solutions of smaller\ninstances of the same learning problem with smaller\ndatasets. In particular, the deep learning job is divided\ninto learning stages. Each learning stage contains a set\nof independent MapReduce iterations where the solution\nof one iteration is the input for the next iteration. During\neach MapReduce iteration, a partial model is trained on\na separate partition of the available MBD as follows:\na) Learning partial models: Each worker node com-\nputes the gradient updates of its partitions of the\nMBD (a.k.a. the Map procedure). During this step,\nall Spark workers execute the same Map task in\nparallel but on different partitions of the MBD. In\nthis way, the expensive gradient computation task\nof the deep model learning is divided into many\nparallel sub-tasks.\nb) Parameter averaging: Parameters of the partial\nmodels are sent to the master machine to build\na master deep model by averaging the parameter\ncalculation of all Spark workers (a.k.a. the Reduce\nprocedure).\nc) Parameter dissemination: The resulting master\nmodel after the Reduce procedure is disseminated\nto all worker nodes. A new MapReduce iteration is\nthen started based on the updated parameters. This\nprocess is continued until the learning convergence\ncriterion is satisﬁed.\nAs a result, a well-tuned deep learning model is generated\nwhich can be used to infer information and patterns from\nstreaming requests. In the following, we discuss how the\n5\nService server\n...\nSecure connection\nCluster manager\n(resource scheduler)\nMany Spark workers\nTask\nTask \nTask\nPartial\nmodel\nTask\nPartial\nmodel\nDeep learning \nmodel\nDeep learning job\n(# of workers, data, and \ncode)\nOne Spark master\nTasks\nPartial\nmodels\nAssign computing\nresources\nServe requests based \non the learnt model\nRequest\nRequest\nRequest\nSpark\ndriver\n Cluster in the cloud\nMBD \npartition\nPartial\nmodel\nRDD\nRDD\nRDD\nRDD\nTask\nTask\nRDD\nTask\nLearning partial \nmodels\nParameter \naveraging\nMBD \npartitioning\nParameter \ndissemination\nInitialize \nparameters\nRead worker’s \nRDDs\nCompute \ngradients\nUpdate \nparameters\nLearning a partial  \nmodel\nPartial model\nConvergence\nDeep model\nConvergence\nFig. 3: A Spark-based framework for distributed deep learning in MBD analytics.\nproposed framework helps in tackling the key characteristics\nof MBD.\nB. Discussion\nThe proposed framework is grounded over deep learning\nand Apache Spark technologies to perform effective MBD an-\nalytics. This integration tackles the challenging characteristics\nof MBD as follows.\n• Deep learning: Deep learning addresses the “value” and\n“variety” aspects of MBD. Firstly, deep learning in MBD\nanalytics helps in understanding raw MBD. Therefore,\ndeep learning effectively addresses the “value” aspect\nof MBD. MBD analytics, as discussed in this article,\nis integral in providing user-customized mobile services.\nSecondly, deep learning enables the learning from mul-\ntimodal data distributions [10], e.g., concatenated input\nfrom accelerometer and light sensors, which is important\nfor the “variety” issue of MBD.\n• Apache Spark: The main role of the Spark platform in the\nproposed framework is tackling the “volume”, “velocity”,\nand “volatility” aspects of MBD. Essentially, the Spark\nengine tackles the “volume” aspect by parallelizing the\nlearning task into many sub-tasks each performed on a\nsmall partition of the overall MBD. Therefore, no single\nmachine is required to process the massive MBD volume\nas one chunk. Similarly, the Spark engine tackles the\n“velocity” point through its streaming extensions which\nenables a fast and high-throughput processing of stream-\ning data. Finally, the “volatility” aspect is addressed by\nsigniﬁcantly speeding up the training of deep models.\nThis ensures that the learned model reﬂects the latest\ndynamics of the mobile system.\nThe proposed framework does not directly tackle the “ve-\nracity” aspect of MBD. This quality aspect requires domain\nexperts to design conditional routines to check the validity\nof crowdsourced data before being added to a central MBD\nstorage.\nV. PROTOTYPING CONTEXT-AWARE ACTIVITY\nRECOGNITION SYSTEM\nContext-awareness [3], [5] has high impact on understand-\ning MBD by describing the circumstances during which the\ndata was collected, so as to provide personalized mobile\nexperience to end users, e.g., targeted advertising, healthcare,\nand social services. A context contains attributes of informa-\ntion to describe the sensed environment such as performed\nhuman activities, surrounding objects, and locations. A context\nlearning model is a program that deﬁnes the rules of mapping\nbetween raw sensory data and the corresponding context\nlabels, e.g., mapping accelerometer signals to activity labels.\nThis section describes a proof-of-concept case study in which\nwe consider a context-aware activity recognition system, e.g.,\ndetect walking, jogging, climbing stairs, sitting, standing, and\nlying down activities. We use real-world dataset during the\ntraining of deep activity recognition models.\nA. Problem Statement\nAccelerometers are sensors which measure proper accelera-\ntion of an object due to motion and gravitational force. Modern\nmobile devices are widely equipped with tiny accelerometer\ncircuits which are produced from electromechanically sensitive\nelements and generate electrical signal in response to any\nmechanical motion. The proper acceleration is distinctive\nfrom coordinate acceleration in classical mechanics. The latter\nmeasures the rate of change of velocity while the former\nmeasures acceleration relative to a free fall, i.e., the proper\nacceleration of an object in a free fall is zero.\nConsider a mobile device with an embedded accelerometer\nsensor that generates proper acceleration samples. Activity\nrecognition is applied to time series data frames which are\nformulated using a sliding and overlapping window. The\nnumber of time-series samples depends on the accelerome-\nter’s sampling frequency (in Hertz) and windowing length\n(in seconds). At time t, the activity recognition classiﬁer\n6\nf : xt →S matches the framed acceleration data xt with the\nmost probable activity label from the set of supported activity\nlabels S = {1, 2, . . . , N}, where N is the number of supported\nactivities in the activity detection component.\nConventional approaches of recognizing activities require\nhandcrafted features, e.g., statistical features [3], which are\nexpensive to design, require domain expert knowledge, and\ngeneralize poorly to support more activities. To avoid this, a\ndeep activity recognition model learns not only the mapping\nbetween raw acceleration data and the corresponding activity\nlabel, but also a set of meaningful features which are superior\nto handcrafted features.\nB. Experimental Setup\nIn this section, we use the Actitracker dataset [13] which\nincludes accelerometer samples of 6 conventional activities\n(walking, jogging, climbing stairs, sitting, standing, and lying\ndown) from 563 crowdsourcing users. Figure 4 (a) plots\naccelerometer signals of the 6 different activities. Clearly,\nhigh frequency signals are sampled for activities with active\nbody motion, e.g., walking, jogging, and climbing stairs. On\nthe other hand, low frequency signals are collected during\nsemi-static body motions, e.g., standing, sitting, and lying\ndown. The data is collected using mobile phones with 20Hz\nof sampling rate, and it contains both labeled and unlabeled\ndata of 2, 980, 765 and 38, 209, 772 samples, respectively. This\nis a real-world example of the limited number of labeled\ndata compared with unlabeled data as data labeling requires\nmanual human intervention. The data is framed using a 10-\nsec windowing function which generates 200 samples of\ntime-series samples. We ﬁrst pre-train deep models on the\nunlabeled data samples only, and we then ﬁne-tune the models\non the labeled dataset. To enhance the activity recognition\nperformance, we use the spectrogram of the acceleration signal\nas input of the deep models. Basically, different activities\ncontain different frequency contents which reﬂect the body\ndynamics and movements.\nWe implemented the proposed framework on a shared clus-\nter system (https://www.acrc.a-star.edu.sg) running the load\nsharing facility (LSF) management platform and RedHat\nLinux. Each node has 8 cores (Intel Xeon 5570 CPU with\nclock speed of 2.93Ghz) and a total of 24GB RAM. In our\nexperiments, we set the cores in multiples of 8 to allocate\nthe entire node’s resources. One partial model learning task is\ninitialized per each computing core. Each task learns using\na data batch consisting of 100 samples for 100 iterations.\nClearly, increasing the number of cores results in quicker\ntraining of deep models. Finally, it important to note that\ndistributed deep learning is a strong type of regularization.\nThus, regularization terms, such as the sparsity and dropout\nconstraints, are not recommended to avoid the problem of\nunderﬁtting.\nC. Experimental Results\n1) The impact of deep models: Figure 4 (b) shows the\nactivity recognition error under different setups of deep models\n(number of hidden layers and number of neurons at each\nTABLE I: Activity recognition error of deep learning and other\nconventional methods used in [4]. The conventional methods\nuse handcrafted statistical features.\nMETHOD\nRECOGNITION ERROR (%)\nMultilayer perceptrons\n32.2\nInstance-based learning\n31.6\nRandom forests\n24.1\nDeep models (5 layers of 2000\nneurons each)\n14.4\nlayer). Speciﬁcally, the capacity of a deep model to capture\nMBD structures is increased when using deeper models with\nmore layers and neurons. Nonetheless, using deeper models\nevolves a signiﬁcant increase in the learning algorithm’s\ncomputational burdens and time. An accuracy comparison\nof deep activity recognition models and other conventional\nmethods is shown in Table I. In short, these results clarify that\n(1) deep models are superior to existing shallow context learn-\ning models, and (2) the learned hierarchical features of deep\nmodels eliminate the need for handcrafted statistical features\nin conventional methods. In our implementation, we use early\nstopping to track the model capacity during training, select\nthe best parameters of deep models, and avoid overﬁtting.\nThe underﬁtting is typically avoided by using deeper models\nand more neurons per layer, e.g., 5 layers with 2000 neurons\nper layer. Next, a speedup analysis is presented to show the\nimportance of the Spark-based framework for learning deep\nmodels on MBD.\n2) The impact of computing cores: The main performance\nmetric of cluster-based computing is the task speedup metric.\nIn particular, we compute the speedup efﬁciency as T8\nTM , where\nT8 is the computing time of one machine with 8 cores, and\nTM is the computing time under different computing power.\nFigure 4 (c) shows the speedup in learning deep models when\nthe number of computing cores is varied. As the number of\ncores increases, the learning time decreases. For example,\nlearning a deep model of 5 layers with 2000 neurons per\nlayer can be trained in 3.63 hours with 6 Spark workers. This\nresults in the speedup efﬁciency of 4.1 as compared to a single\nmachine computing which takes 14.91 hours.\n3) MBD veracity: A normalized confusion matrix of a deep\nmodel is shown in Figure 5. This confusion matrix shows\nthe high performance of deep models on a per-activity basis\n(high scores at the diagonal entries). The incorrect detection\nof the “sitting” activity instead of the “lying down” activity is\ntypically due to the different procedures in performing the\nactivities by crowdsourcing users. This gives a real-world\nexample of the “veracity” characteristic of MBD, i.e., uncer-\ntainties in MBD collection.\nIn the next section, we identify some notable future research\ndirections in MBD collection, labeling, and economics.\nVI. FUTURE WORK\nBased on the proposed framework, the following future\nwork can be further pursued.\n7\n0\n10\n20\n30\n40\n50\n60\nTime slots (t)\n−20\n−10\n0\n10\n20\nAccelerometer's 3-axial signals \nwalking t 2[0;10)\njogging t 2[10;20)\nclimbing stairs t 2[20;30)\nsitting t 2[30;40)\nstanding t 2[40;50)\n lying down t 2[50;60)\nx-acceleration\ny-acceleration\nz-acceleration\n(a)\n1\n2\n3\n4\n5\nNumber of model's layers\n13\n14\n15\n16\n17\n18\n19\nRecognition error (%)\n500 neurons per layer\n1000 neurons per layer\n1500 neurons per layer\n2000 neurons per layer\n(b)\n8\n16\n24\n32\n48\n64\nNumber of computing cores\n0\n1\n2\n3\n4\n5\n6\n7\n8\nSpeedup efficiency\nUpper speedup limit\n5 layers of 1000 neurons each (T8=5.56 hours)\n5 layers of 2000 neurons each (T8=14.91 hours)\n(c)\nFig. 4: Experimental analysis. (a) Accelerometer signal of different human activities. (b) Recognition accuracy of deep learning\nmodels under different deep model setups. (c) Speedup of learning deep models using the Spark-based framework under different\ncomputing cores.\nWalk\nJog\nStairs\nSit\nStand\nLie down\nPredicted activity\nWalk\nJog\nStairs\nSit\nStand\nLie down\nActual activity\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFig. 5: Normalized confusion matrix of a deep model (5 layers\nof 2000 neurons each).\nA. Crowd Labeling of MBD\nA major challenge facing MBD analysts is the limited\namounts of labeled data samples as data labeling is typically a\nmanual process. An important research direction is proposing\ncrowd labeling methods for MBD. The crowd labeling can be\ndesigned under two main schemes: (1) paid crowd labeling,\nand (2) embedded crowd labeling. In the paid crowd labeling,\nthe crowdsourcing mobile users annotate mobile data and are\naccordingly paid based on their labeling performance and\nspeed. Under this paid scheme, optimal budget allocation\nmethods are required. In the embedded crowd labeling, data\nlabeling can be also achieved by adding labeling tasks within\nmobile application functional routines, e.g., CAPTCHA-based\nimage labeling [14]. Here, the mobile users can access more\nfunctions of a mobile application by indirectly helping in the\ndata labeling process. More work is required for designing\ninnovative methods for embedded crowd labeling without\ndisturbing the user experience or harming the mobile appli-\ncation’s main functionality.\nB. Economics of MBD\nMBD, as discussed in this article, is about extracting\nmeaningful information and patterns from raw mobile data.\nThis information is used during decision making and to\nenhance existing mobile services. An important research di-\nrection is proposing business models, e.g., pricing and auction\ndesign [15], for selling and buying MBD among mobile\norganizations and parties.\nC. Privacy and MBD Collection\nAs MBD is people-centric, mobile users would be con-\ncerned about the risks of sharing their personal mobile data\n8\nwith a service server. Thus, a low percentage of users will opt\nout of sharing their personal data unless trustworthy privacy\nmechanisms are applied. Meanwhile, anonymized data collec-\ntion, i.e., data that could not be used to identify individuals,\nis adopted by many services. An alternative research direction\nis proposing fair data exchange models which encourage the\nsharing of mobile data in return of rewarding points, e.g.,\npremium membership points.\nVII. CONCLUSIONS\nIn this article, we have presented and discussed a scalable\nSpark-based framework for deep learning in mobile big data\nanalytics. The framework enables the tuning of deep models\nwith many hidden layers and millions of parameters on a cloud\ncluster. Typically, deep learning provides a promising learning\ntool for adding value by learning intrinsic features from raw\nmobile big data. The framework has been validated using a\nlarge-scale activity recognition system as a case study. Finally,\nimportant research directions on mobile big data have been\noutlined.\nACKNOWLEDGMENT\nThis work was supported by the A*STAR Computational\nResource Centre through the use of its high performance\ncomputing facilities. We thank Ahmed Selim, Trinity College\nDublin, for valuable discussions in the early stages of the\nstudy.\nREFERENCES\n[1] Cisco, “Cisco visual networking index: Global mobile data trafﬁc\nforecast update 2015-2020,” White Paper, 2016.\n[2] Apache Spark, “Apache Spark–lightning-fast cluster computing,” 2016,\naccessed 19-February-2016. [Online]. Available: http://spark.apache.org\n[3] O. D. Lara and M. A. Labrador, “A survey on human activity recognition\nusing wearable sensors,” IEEE Communications Surveys & Tutorials,\nvol. 15, no. 3, pp. 1192–1209, 2013.\n[4] G. M. Weiss and J. W. Lockhart, “The impact of personalization on\nsmartphone-based activity recognition,” in AAAI Workshop on Activity\nContext Representation: Techniques and Languages, 2012.\n[5] C. Perera, A. Zaslavsky, P. Christen, and D. Georgakopoulos, “Context\naware computing for the Internet of things: A survey,” Communications\nSurveys & Tutorials, IEEE, vol. 16, no. 1, pp. 414–454, 2014.\n[6] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for\ndeep belief nets,” Neural computation, vol. 18, no. 7, pp. 1527–1554,\n2006.\n[7] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,\n“Stacked denoising autoencoders: Learning useful representations in a\ndeep network with a local denoising criterion,” The Journal of Machine\nLearning Research, vol. 11, pp. 3371–3408, 2010.\n[8] X. Wang, L. Gao, S. Mao, and S. Pandey, “Deepﬁ: Deep learning for\nindoor ﬁngerprinting using channel state information,” in IEEE Wireless\nCommunications and Networking Conference, March 2015, pp. 1666–\n1671.\n[9] N. D. Lane and P. Georgiev, “Can deep learning revolutionize mobile\nsensing?” in Proceedings of the 16th International Workshop on Mobile\nComputing Systems and Applications.\nACM, 2015, pp. 117–122.\n[10] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\n“Multimodal deep learning,” in Proceedings of the 28th International\nConference on Machine Learning, 2011, pp. 689–696.\n[11] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior,\nP. Tucker, K. Yang, Q. V. Le et al., “Large scale distributed deep\nnetworks,” in Advances in Neural Information Processing Systems, 2012,\npp. 1223–1231.\n[12] K. Zhang and X.-w. Chen, “Large-scale deep belief nets with MapRe-\nduce,” IEEE Access, vol. 2, pp. 395–403, 2014.\n[13] J. W. Lockhart, G. M. Weiss, J. C. Xue, S. T. Gallagher, A. B.\nGrosner, and T. T. Pulickal, “Design considerations for the WISDM\nsmart phone-based sensor mining architecture,” in Proceedings of the\n5th International Workshop on Knowledge Discovery from Sensor Data.\nACM, 2011, pp. 25–33.\n[14] L. Von Ahn, B. Maurer, C. McMillen, D. Abraham, and M. Blum,\n“reCAPTCHA: Human-based character recognition via web security\nmeasures,” Science, vol. 321, no. 5895, pp. 1465–1468, 2008.\n[15] P. Klemperer, Auctions: Theory and practice, ser. Princeton paperbacks.\nPrinceton University Press, 2004.\n",
  "categories": [
    "cs.DC",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2016-02-23",
  "updated": "2016-02-23"
}