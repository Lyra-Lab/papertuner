{
  "id": "http://arxiv.org/abs/1603.05691v4",
  "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?",
  "authors": [
    "Gregor Urban",
    "Krzysztof J. Geras",
    "Samira Ebrahimi Kahou",
    "Ozlem Aslan",
    "Shengjie Wang",
    "Rich Caruana",
    "Abdelrahman Mohamed",
    "Matthai Philipose",
    "Matt Richardson"
  ],
  "abstract": "Yes, they do. This paper provides the first empirical demonstration that deep\nconvolutional models really need to be both deep and convolutional, even when\ntrained with methods such as distillation that allow small or shallow models of\nhigh accuracy to be trained. Although previous research showed that shallow\nfeed-forward nets sometimes can learn the complex functions previously learned\nby deep nets while using the same number of parameters as the deep models they\nmimic, in this paper we demonstrate that the same methods cannot be used to\ntrain accurate models on CIFAR-10 unless the student models contain multiple\nlayers of convolution. Although the student models do not have to be as deep as\nthe teacher model they mimic, the students need multiple convolutional layers\nto learn functions of comparable accuracy as the deep convolutional teacher.",
  "text": "Published as a conference paper at ICLR 2017\nDO DEEP CONVOLUTIONAL NETS REALLY NEED TO\nBE DEEP AND CONVOLUTIONAL?\nGregor Urban1, Krzysztof J. Geras2, Samira Ebrahimi Kahou3, Ozlem Aslan4, Shengjie Wang5,\nAbdelrahman Mohamed6, Matthai Philipose6, Matt Richardson6, Rich Caruana6\n1UC Irvine, USA\n2University of Edinburgh, UK\n3Ecole Polytechnique de Montreal, CA\n4University of Alberta, CA\n5University of Washington, USA\n6Microsoft Research, USA\nABSTRACT\nYes, they do. This paper provides the ﬁrst empirical demonstration that deep\nconvolutional models really need to be both deep and convolutional, even when\ntrained with methods such as distillation that allow small or shallow models of\nhigh accuracy to be trained. Although previous research showed that shallow\nfeed-forward nets sometimes can learn the complex functions previously learned\nby deep nets while using the same number of parameters as the deep models they\nmimic, in this paper we demonstrate that the same methods cannot be used to train\naccurate models on CIFAR-10 unless the student models contain multiple layers\nof convolution. Although the student models do not have to be as deep as the\nteacher model they mimic, the students need multiple convolutional layers to learn\nfunctions of comparable accuracy as the deep convolutional teacher.\n1\nINTRODUCTION\nCybenko (1989) proved that a network with a large enough single hidden layer of sigmoid units can\napproximate any decision boundary. Empirical work, however, suggests that it can be difﬁcult to\ntrain shallow nets to be as accurate as deep nets. Dauphin and Bengio (2013) trained shallow nets\non SIFT features to classify a large-scale ImageNet dataset and found that it was difﬁcult to train\nlarge, high-accuracy, shallow nets. A study of deep convolutional nets suggests that for vision tasks\ndeeper models are preferred under a parameter budget (e.g. Eigen et al. (2014); He et al. (2015);\nSimonyan and Zisserman (2014); Srivastava et al. (2015)). Similarly, Seide et al. (2011) and Geras\net al. (2015) show that deeper models are more accurate than shallow models in speech acoustic\nmodeling. More recently, Romero et al. (2015) showed that it is possible to gain increases in accuracy\nin models with few parameters by training deeper, thinner nets (FitNets) to mimic much wider nets.\nCohen and Shashua (2016); Liang and Srikant (2016) suggest that the representational efﬁciency of\ndeep networks scales exponentially with depth, but it is unclear if this applies only to pathological\nproblems, or is encountered in practice on data sets such as TIMIT and CIFAR.\nBa and Caruana (2014), however, demonstrated that shallow nets sometimes can learn the functions\nlearned by deep nets, even when restricted to the same number of parameters as the deep nets. They\ndid this by ﬁrst training state-of-the-art deep models, and then training shallow models to mimic\nthe deep models. Surprisingly, and for reasons that are not well understood, the shallow models\nlearned more accurate functions when trained to mimic the deep models than when trained on the\noriginal data used to train the deep models. In some cases shallow models trained this way were as\naccurate as state-of-the-art deep models. But this demonstration was made on the TIMIT speech\nrecognition benchmark. Although their deep teacher models used a convolutional layer, convolution\nis less important for TIMIT than it is for other domains such as image classiﬁcation.\nBa and Caruana (2014) also presented results on CIFAR-10 which showed that a shallow model\ncould learn functions almost as accurate as deep convolutional nets. Unfortunately, the results on\nCIFAR-10 are less convincing than those for TIMIT. To train accurate shallow models on CIFAR-10\n1\narXiv:1603.05691v4  [stat.ML]  4 Mar 2017\nPublished as a conference paper at ICLR 2017\nthey had to include at least one convolutional layer in the shallow model, and increased the number\nof parameters in the shallow model until it was 30 times larger than the deep teacher model. Despite\nthis, the shallow convolutional student model was several points less accurate than a teacher model\nthat was itself several points less accurate than state-of-the-art models on CIFAR-10.\nIn this paper we show that the methods Ba and Caruana used to train shallow students to mimic deep\nteacher models on TIMIT do not work as well on problems such as CIFAR-10 where multiple layers\nof convolution are required to train accurate teacher models. If the student models have a similar\nnumber of parameters as the deep teacher models, high accuracy can not be achieved without multiple\nlayers of convolution even when the student models are trained via distillation.\nTo ensure that the shallow student models are trained as accurately as possible, we use Bayesian\noptimization to thoroughly explore the space of architectures and learning hyperparameters. Although\nthis combination of distillation and hyperparameter optimization allows us to train the most accurate\nshallow models ever trained on CIFAR-10, the shallow models still are not as accurate as deep\nmodels. Our results clearly suggest that deep convolutional nets do, in fact, need to be both deep and\nconvolutional, even when trained to mimic very accurate models via distillation (Hinton et al., 2015).\n2\nTRAINING SHALLOW NETS TO MIMIC DEEPER CONVOLUTIONAL NETS\nIn this paper, we revisit the CIFAR-10 experiments in Ba and Caruana (2014). Unlike in that work,\nhere we compare shallow models to state-of-the-art deep convolutional models, and restrict the\nnumber of parameters in the shallow student models to be comparable to the number of parameters in\nthe deep convolutional teacher models. Because we anticipated that our results might be different,\nwe follow their approach closely to eliminate the possibility that the results differ merely because of\nchanges in methodology. Note that the goal of this paper is not to train models that are small or fast\nas in Bucila et al. (2006), Hinton et al. (2015), and Romero et al. (2015), but to examine if shallow\nmodels can be as accurate as deep convolutional models given the same parameter budget.\nThere are many steps required to train shallow student models to be as accurate as possible: train\nstate-of-the-art deep convolutional teacher models, form an ensemble of the best deep models, collect\nand combine their predictions on a large transfer set, and then train carefully optimized shallow\nstudent models to mimic the teacher ensemble. For negative results to be informative, it is important\nthat each of these steps be performed as well as possible. In this section we describe the experimental\nmethodology in detail. Readers familiar with distillation (model compression), training deep models\non CIFAR-10, data augmentation, and Bayesian hyperparameter optimization may wish to skip to the\nempirical results in Section 3.\n2.1\nMODEL COMPRESSION AND DISTILLATION\nThe key idea behind model compression is to train a compact model to approximate the function\nlearned by another larger, more complex model. Bucila et al. (2006) showed how a single neural net\nof modest size could be trained to mimic a much larger ensemble. Although the small neural nets\ncontained 1000× fewer parameters, often they were as accurate as the large ensembles they were\ntrained to mimic.\nModel compression works by passing unlabeled data through the large, accurate teacher model to\ncollect the real-valued scores it predicts, and then training a student model to mimic these scores.\nHinton et al. (2015) generalized the methods of Bucila et al. (2006) and Ba and Caruana (2014)\nby incorporating a parameter to control the relative importance of the soft targets provided by the\nteacher model to the hard targets in the original training data, as well as a temperature parameter that\nregularizes learning by pushing targets towards the uniform distribution. Hinton et al. (2015) also\ndemonstrated that much of the knowledge passed from the teacher to the student is conveyed as dark\nknowledge contained in the relative scores (probabilities) of outputs corresponding to other classes,\nas opposed to the scores given to just the output for the one correct class.\nSurprisingly, distillation often allows smaller and/or shallower models to be trained that are nearly\nas accurate as the larger, deeper models they are trained to mimic, yet these same small models are\nnot as accurate when trained on the 1-hot hard targets in the original training set. The reason for\nthis is not yet well understood. Similar compression and distillation methods have also successfully\n2\nPublished as a conference paper at ICLR 2017\nbeen used in speech recognition (e.g. Chan et al. (2015); Geras et al. (2015); Li et al. (2014)) and\nreinforcement learning Parisotto et al. (2016); Rusu et al. (2016). Romero et al. (2015) showed that\ndistillation methods can be used to train small students that are more accurate than the teacher models\nby making the student models deeper, but thinner, than the teacher model.\n2.2\nMIMIC LEARNING VIA L2 REGRESSION ON LOGITS\nWe train shallow mimic nets using data labeled by an ensemble of deep teacher nets trained on the\noriginal 1-hot CIFAR-10 training data. The deep teacher models are trained in the usual way using\nsoftmax outputs and cross-entropy cost function. Following Ba and Caruana (2014), the student\nmimic models are not trained with cross-entropy on the ten p values where pk = ezk/ P\nj ezj output\nby the softmax layer from the deep teacher model, but instead are trained on the un-normalized log\nprobability values z (the logits) before the softmax activation. Training on the logarithms of predicted\nprobabilities (logits) helps provide the dark knowledge that regularizes students by placing emphasis\non the relationships learned by the teacher model across all of the outputs.\nAs in Ba and Caruana (2014), the student is trained as a regression problem given training data\n{(x(1), z(1)),...,(x(T ), z(T ))}:\nL(W) = 1\nT\nX\nt\n||g(x(t); W) −z(t)||2\n2,\n(1)\nwhere W represents all of the weights in the network, and g(x(t); W) is the model prediction on the\ntth training data sample.\n2.3\nUSING A LINEAR BOTTLENECK TO SPEED UP TRAINING\nA shallow net has to have more hidden units in each layer to match the number of parameters in\na deep net. Ba and Caruana (2014) found that training these wide, shallow mimic models with\nbackpropagation was slow, and introduced a linear bottleneck layer between the input and non-linear\nlayers to speed learning. The bottleneck layer speeds learning by reducing the number of parameters\nthat must be learned, but does not make the model deeper because the linear terms can be absorbed\nback into the non-linear weight matrix after learning. See Ba and Caruana (2014) for details. To match\ntheir experiments we use linear bottlenecks when training student models with 0 or 1 convolutional\nlayers, but did not ﬁnd the linear bottlenecks necessary when training student models with more than\n1 convolutional layer.\n2.4\nBAYESIAN HYPERPARAMETER OPTIMIZATION\nThe goal of this work is to determine empirically if shallow nets can be trained to be as accurate as\ndeep convolutional models using a similar number of parameters in the deep and shallow models. If\nwe succeed in training a shallow model to be as accurate as a deep convolutional model, this provides\nan existence proof that shallow models can represent and learn the complex functions learned by\ndeep convolutional models. If, however, we are unable to train shallow models to be as accurate as\ndeep convolutional nets, we might fail only because we did not train the shallow nets well enough.\nIn all our experiments we employ Bayesian hyperparameter optimization using Gaussian process\nregression to ensure that we thoroughly and objectively explore the hyperparameters that govern\nlearning. The implementation we use is Spearmint (Snoek et al., 2012). The hyperparameters we\noptimize with Bayesian optimization include the initial learning rate, momentum, scaling of the initial\nrandom weights, scaling of the inputs, and terms that determine the width of each of the network’s\nlayers (i.e. number of convolutional ﬁlters and neurons). More details of the hyperparameter\noptimization can be found in Sections 2.5, 2.7, 2.8 and in the Appendix.\n2.5\nTRAINING DATA AND DATA AUGMENTATION\nThe CIFAR-10 (Krizhevsky, 2009) data set consists of a set of natural images from 10 different object\nclasses: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The dataset is a labeled\nsubset of the 80 million tiny images dataset (Torralba et al., 2008) and is divided into 50,000 train and\n3\nPublished as a conference paper at ICLR 2017\n10,000 test images. Each image is 32×32 pixels in 3 color channels, yielding input vectors with 3072\ndimensions. We prepared the data by subtracting the mean and dividing by the standard deviation\nof each image vector. We train all models on a subset of 40,000 images and use the remaining\n10,000 images as the validation set for the Bayesian optimization. The ﬁnal trained models only\nused 80% of the theoretically available training data (as opposed to retraining on all of the data after\nhyperparameter optimization).\nWe employ the HSV-data augmentation technique as described by Snoek et al. (2015). Thus\nwe shift hue, saturation and value by uniform random values: ∆h ∼U(−Dh, Dh), ∆s ∼\nU(−Ds, Ds), ∆v ∼U(−Dv, Dv).\nSaturation and value values are scaled globally: as ∼\nU(\n1\n1+As , 1 + As), av ∼U(\n1\n1+Av , 1 + Av). The ﬁve constants Dh, Ds, Dv, As, Av are treated\nas additional hyperparameters in the Bayesian hyperparameter optimization.\nAll training images are mirrored left-right randomly with a probability of 0.5. The input images are\nfurther scaled and jittered randomly by cropping windows of size 24×24 up to 32×32 at random\nlocations and then scaling them back to 32×32. The procedure is as follows: we sample an integer\nvalue S ∼U(24, 32) and then a pair of integers x, y ∼U(0, 32 −S). The transformed resulting\nimage is R = fspline,3(I[x : x + S, y : y + S]) with I denoting the original image and fspline,3\ndenoting the 3rd order spline interpolation function that maps the 2D array back to 32×32 (applied to\nthe three color channels separately).\nAll data augmentations for the teacher models are computed on the ﬂy using different random seeds.\nFor student models trained to mimic the ensemble (see Section 2.7 for details of the ensemble teacher\nmodel), we pre-generated 160 epochs worth of randomly augmented training data, evaluated the\nensemble’s predictions (logits) on these samples, and saved all data and predictions to disk. All student\nmodels thus see the same training data in the same order. The parameters for HSV-augmentation in\nthis case had to be selected beforehand; we chose to use the settings found with the best single model\n(Dh = 0.06, Ds = 0.26, Dv = 0.20, As = 0.21, Av = 0.13). Pre-saving the logits and augmented\ndata is important to reduce the computational cost at training time, and to ensure that all student\nmodels see the same training data\nBecause augmentation allows us to generate large training sets from the original 50,000 images, we\nuse augmented data as the transfer set for model compression. No extra unlabeled data is required.\n2.6\nLEARNING-RATE SCHEDULE\nWe train all models using SGD with Nesterov momentum. The initial learning rate and momentum\nare chosen by Bayesian optimization. The learning rate is reduced according to the evolution of the\nmodel’s validation error: it is halved if the validation error does not drop for ten epochs in a row. It is\nnot reduced within the next eight epochs following a reduction step. Training ends if the error did not\ndrop for 30 epochs in a row or if the learning rate was reduced by a factor of more than 2000 in total.\nThis schedule provides a way to train the highly varying models in a fair manner (it is not feasible to\noptimize all of the parameters that deﬁne the learning schedule). It also decreases the time spent to\ntrain each model compared to using a hand-selected overestimate of the number of epochs to train,\nthus allowing us to train more models in the hyperparameter search.\n2.7\nSUPER TEACHER: AN ENSEMBLE OF 16 DEEP CONVOLUTIONAL CIFAR-10 MODELS\nOne limitation of the CIFAR-10 experiments performed in Ba and Caruana (2014) is that the teacher\nmodels were not state-of-the-art. The best deep models they trained on CIFAR-10 had only 88%\naccuracy, and the ensemble of deep models they used as a teacher had only 89% accuracy. The\naccuracies were not state-of-the-art because they did not use augmentation and because their deepest\nmodels had only three convolutional layers. Because our goal is to determine if shallow models can\nbe as accurate as deep convolutional models, it is important that the deep models we compare to (and\nuse as teachers) are as accurate as possible.\nWe train deep neural networks with eight convolutional layers, three intermittent max-pooling layers\nand two fully-connected hidden layers. We include the size of these layers in the hyperparameter\noptimization, by allowing the ﬁrst two convolutional layers to contain from 32 to 96 ﬁlters each, the\nnext two layers to contain from 64 to 192 ﬁlters, and the last four convolutional layers to contain\n4\nPublished as a conference paper at ICLR 2017\nfrom 128 to 384 ﬁlters. The two fully-connected hidden layers can contain from 512 to 1536 neurons.\nWe parametrize these model-sizes by four scalars (the layers are grouped as 2-2-4) and include the\nscalars in the hyperparameter optimization. All models are trained using Theano (Bastien et al., 2012;\nBergstra et al., 2010).\nWe optimize eighteen hyperparameters overall: initial learning rate on [0.01, 0.05], momentum on\n[0.80, 0.91], l2 weight decay on [5 · 10−5,4 · 10−4], initialization coefﬁcient on [0.8, 1.35] which\nscales the initial weights of the CNN, four separate dropout rates, ﬁve constants controlling the\nHSV data augmentation, and the four scaling constants controlling the networks’ layer widths. The\nlearning rate and momentum are optimized on a log-scale (as opposed to linear scale) by optimizing\nthe exponent with appropriate bounds, e.g. LR = e−x optimized over x on [3.0, 4.6]. See the\nAppendix for more details about hyperparameter optimization.\nWe trained 129 deep CNN models with Spearmint. The best model obtained an accuracy of 92.78%;\nthe ﬁfth best achieved 92.67%. See Table 1 for the sizes and architectures of the three best models.\nWe are able to construct a more accurate model on CIFAR-10 by forming an ensemble of multiple\ndeep convolutional neural nets, each trained with different hyperparameters, and each seeing slightly\ndifferent training data (as the augmentation parameters vary). We experimented with a number of\nensembles of the many deep convnets we trained, using accuracy on the validation set to select the\nbest combination. The ﬁnal ensemble contained 16 deep convnets and had an accuracy of 94.0% on\nthe validation set, and 93.8% on the ﬁnal test set. We believe this is among the top published results\nfor deep learning on CIFAR-10. The ensemble averages the logits predicted by each model before\nthe softmax layers.\nWe used this very accurate ensemble model as the teacher model to label the data used to train the\nshallower student nets. As described in Section 2.2, the logits (the scores just prior to the ﬁnal softmax\nlayer) from each of the CNN teachers in the ensemble model are averaged for each class, and the\naverage logits are used as ﬁnal regression targets to train the shallower student neural nets.\n2.8\nTRAINING SHALLOW STUDENT MODELS TO MIMIC AN ENSEMBLE OF DEEP\nCONVOLUTIONAL MODELS\nWe trained student mimic nets with 1, 3.161, 10 and 31.6 million trainable parameters on the\npre-computed augmented training data (Section 2.5) that was re-labeled by the teacher ensemble\n(Section 2.7). For each of the four student sizes we trained shallow fully-connected student MLPs\ncontaining 1, 2, 3, 4, or 5 layers of non-linear units (ReLU), and student CNNs with 1, 2, 3 or 4\nconvolutional layers. The convolutional student models also contain one fully-connected ReLU layer.\nModels with zero or only one convolutional layer contain an additional linear bottleneck layer to\nspeed up learning (cf. Section 2.3). We did not need to use a bottleneck to speed up learning for the\ndeeper models as the number of learnable parameters is naturally reduced by the max-pooling layers.\nThe student CNNs use max-pooling and Bayesian optimization controls the number of convolutional\nﬁlters and hidden units in each layer. The hyperparameters we optimized in the student models are:\ninitial learning rate, momentum, scaling of the initially randomly distributed learnable parameters,\nscaling of all pixel values of the input, and the scale factors that control the width of all hidden\nand convolutional layers in the model. Weights are initialized as in Glorot and Bengio (2010). We\nintentionally do not optimize and do not make use of weight decay and dropout when training student\nmodels because preliminary experiments showed that these consistently reduced the accuracy of\nstudent models by several percent. Please refer to the Appendix for more details on the individual\narchitectures and hyperparameter ranges.\n3\nEMPIRICAL RESULTS\nTable 1 summarizes results after Bayesian hyperparameter optimization for models trained on the\noriginal 0/1 hard CIFAR-10 labels. All of these models use weight decay and are trained with the\ndropout hyperparameters included in the Bayesian optimization. The table shows the accuracy of\nthe best three deep convolutional models we could train on CIFAR-10, as well as the accuracy of\n13.16 ≈Sqrt(10) falls halfway between 1 and 10 on log scale.\n5\nPublished as a conference paper at ICLR 2017\nTable 1: Accuracy on CIFAR-10 of shallow and deep models trained on the original 0/1 hard class\nlabels using Bayesian optimization with dropout and weight decay. Key: c = convolution layer; mp\n= max-pooling layer; fc = fully-connected layer; lfc = linear bottleneck layer; exponents indicate\nrepetitions of a layer. The last two models (*) are numbers reported by Ba and Caruana (2014). The\nmodels with 1-4 convolutional layers at the top of the table are included for comparison with student\nmodels of similar architecture in Table 2 . All of the student models in Table 2 with 1, 2, 3, and 4\nconvolutional layers are more accurate than their counterparts in this table that are trained on the\noriginal 0/1 hard targets — as expected distillation yields shallow models of higher accuracy than\nshallow models trained on the original training data.\nModel\nArchitecture\n# parameters\nAccuracy\n1 conv. layer\nc-mp-lfc-fc\n10M\n84.6%\n2 conv. layer\nc-mp-c-mp-fc\n10M\n88.9%\n3 conv. layer\nc-mp-c-mp-c-mp-fc\n10M\n91.2%\n4 conv. layer\nc-mp-c-c-mp-c-mp-fc\n10M\n91.75%\nTeacher CNN 1st\n76c2-mp-126c2-mp-148c4-mp-1200fc2\n5.3M\n92.78%\nTeacher CNN 2nd\n96c2-mp-171c2-mp-128c4-mp-512fc2\n2.5M\n92.77%\nTeacher CNN 3rd\n54c2-mp-158c2-mp-189c4-mp-1044fc2\n5.8M\n92.67%\nEnsemble of 16 CNNs\nc2-mp-c2-mp-c4-mp-fc2\n83.4M\n93.8%\nTeacher CNN (*)\n128c-mp-128c-mp-128c-mp-1k fc\n2.1M\n88.0%\nEnsemble, 4 CNNs (*)\n128c-mp-128c-mp-128c-mp-1k fc\n8.6M\n89.0%\nTable 2: Comparison of student models with varying number of convolutional layers trained to mimic\nthe ensemble of 16 deep convolutional CIFAR-10 models in Table 1 . The best performing student\nmodels have 3 – 4 convolutional layers and 10M – 31.6M parameters. The student models in this\ntable are more accurate than the models of the same architecture in Table 1 that were trained on the\noriginal 0/1 hard targets — shallow models trained with distillation are more accurate than shallow\nmodels trained on 0/1 hard targets. The student model trained by Ba and Caruana (2014) is shown in\nthe last line for comparison; it is less accurate and much larger than the student models trained here\nthat also have 1 convolutional layer.\n1 M\n3.16 M\n10 M\n31.6 M\n70 M\nBottleneck, 1 hidden layer\n65.8%\n68.2%\n69.5%\n70.2%\n–\n2 hidden layers\n66.2%\n70.9%\n73.4%\n74.3%\n–\n3 hidden layers\n66.8%\n71.0%\n73.0%\n73.9%\n–\n4 hidden layers\n66.7%\n69.5%\n71.6%\n72.0%\n–\n5 hidden layers\n66.4%\n70.0%\n71.4%\n71.5%\n–\n1 conv. layer, 1 max-pool, Bottleneck\n84.5%\n86.3%\n87.3%\n87.7%\n–\n2 conv. layers, 2 max-pool\n87.9%\n89.3%\n90.0%\n90.3%\n–\n3 conv. layers, 3 max-pool\n90.7%\n91.6%\n91.9%\n92.3%\n–\n4 conv. layers, 3 max-pool\n91.3%\n91.8%\n92.6%\n92.6%\n–\nSNN-ECNN-MIMIC-30k 128c-p-1200L-30k\n–\n–\n–\n–\n85.8%\ntrained on ensemble (Ba and Caruana, 2014)\n6\nPublished as a conference paper at ICLR 2017\nthe ensemble of 16 deep CNNs. For comparison, the accuracy of the ensemble trained by Ba and\nCaruana (2014)) is included at the bottom of the table.\n1\n3\n10\n31\nNumber of Parameters [millions]\n65\n70\n75\n80\n85\n90\nAccuracy\nteacher ensemble\ncompression gap\n2\n3\n4\n5\n1\n         convolutional gap               \ncompression gap\nCNN: 1 convolutional layer\nCNN: 2 convolutional layers\nCNN: 3 convolutional layers\nCNN: 4 convolutional layers\nMLP: 1 hidden layer\nMLP: 2 hidden layer\nMLP: 3 hidden layer\nMLP: 4 hidden layer\nMLP: 5 hidden layer\nFigure 1: Accuracy of student models with differ-\nent architectures trained to mimic the CIFAR10\nensemble. The average performance of the ﬁve\nbest models of each hyperparameter-optimization\nexperiment is shown, together with dashed lines\nindicating the accuracy of the best and the ﬁfth\nbest model from each setting. The short horizontal\nlines at 10M parameters are the accuracy of mod-\nels trained without compression on the original 0/1\nhard targets.\nTable 2 summarizes the results after Bayesian\nhyperparameter optimization for student mod-\nels of different depths and number of parameters\ntrained on soft targets (average logits) to mimic\nthe teacher ensemble of 16 deep CNNs. For\ncomparison, the student model trained by Ba\nand Caruana (2014) also is shown.\nThe ﬁrst four rows in Table 1 show the accuracy\nof convolutional models with 10 million param-\neters and 1, 2, 3, and 4 convolutional layers.\nThe accuracies of these same architectures with\n1M, 3.16M, 10M, and 31.6M parameters when\ntrained as students on the soft targets predicted\nby the teacher ensemble are shown in Table 2.\nComparing the accuracies of the models with 10\nmillion parameters in both tables, we see that\ntraining student models to mimic the ensemble\nleads to signiﬁcantly better accuracy in every\ncase. The gains are more pronounced for shal-\nlower models, most likely because their learn-\nable internal representations do not naturally\nlead to good generalization in this task when\ntrained on the 0/1 hard targets: the difference\nin accuracy for models with one convolutional\nlayer is 2.7% (87.3% vs. 84.6%) and only 0.8%\n(92.6% vs. 91.8%) for models with four convo-\nlutional layers.\nFigure 1 summarizes the results in Table 2 for\nstudent models of different depth, number of\nconvolutional layers, and number of parame-\nters when trained to mimic the ensemble teacher\nmodel. Student models trained on the ensemble\nlogits are able to achieve accuracies previously\nunseen on CIFAR-10 for models with so few\nlayers. Also, it is clear that there is a huge gap\nbetween the convolutional student models at the\ntop of the ﬁgure, and the non-convolutional stu-\ndent models at the bottom of the ﬁgure: the most\naccurate student MLP has accuracy less than\n75%, while the least accurate convolutional stu-\ndent model with the same number of parameters\nbut only one convolutional layer has accuracy\nabove 87%. And the accuracy of the convolu-\ntional student models increases further as more\nlayers of convolution are added. Interestingly,\nthe most accurate student MLPs with no convo-\nlutional layers have only 2 or 3 hidden layers;\nthe student MLPs with 4 or 5 hidden layers are\nnot as accurate.\nComparing the student MLP with only one hidden layer (bottom of the graph) to the student CNN\nwith 1 convolutional layer clearly suggests that convolution is critical for this problem even when\nmodels are trained via distillation, and that it is very unlikely that a shallow non-convolutional model\nwith 100 million parameters or less could ever achieve accuracy comparable to a convolutional model.\nIt appears that if convolution is critical for teacher models trained on the original 0/1 hard targets, it\n7\nPublished as a conference paper at ICLR 2017\nis likely to be critical for student models trained to mimic these teacher models. Adding depth to the\nstudent MLPs without adding convolution does not signiﬁcantly close this “convolutional gap”.\nFurthermore, comparing student CNNs with 1, 2, 3, and 4 convolutional layers, it is clear that CNN\nstudents beneﬁt from multiple convolutional layers. Although the students do not need as many\nlayers as teacher models trained on the original 0/1 hard targets, accuracy increases signiﬁcantly as\nmultiple convolutional layers are added to the model. For example, the best student with only one\nconvolutional layer has 87.7% accuracy, while the student with the same number of parameters (31M)\nand 4 convolutional layers has 92.6% accuracy.\nFigure 1 includes short horizontal lines at 10M parameters indicating the accuracy of non-student\nmodels trained on the original 0/1 hard targets instead of on the soft targets. This “compression\ngap” is largest for shallower models, and as expected disappears as the student models become\narchitecturally more similar to the teacher models with multiple layers of convolution. The beneﬁts of\ndistillation are most signiﬁcant for shallow models, yielding an increase in accuracy of 3% or more.\nOne pattern that is clear in the graph is that all student models beneﬁt when the number of parameters\nincreases from 1 million to 31 million parameters. It is interesting to note, however, that the largest\nstudent (31M) with a one convolutional layer is less accurate than the smallest student (1M) with two\nconvolutional layers, further demonstrating the value of depth in convolutional models.\nIn summary, depth-constrained student models trained to mimic a high-accuracy ensemble of deep\nconvolutional models perform better than similar models trained on the original hard targets (the\n“compression” gaps in Figure 1), student models need at least 3-4 convolutional layers to have high\naccuracy on CIFAR-10, shallow students with no convolutional layers perform poorly on CIFAR-10,\nand student models need at least 3-10M parameters to perform well. We are not able to compress\ndeep convolutional models to shallow student models without signiﬁcant loss of accuracy.\nWe are currently running a reduced set of experiments on ImageNet, though the chances of shallow\nmodels performing well on a more challenging problem such as ImageNet appear to be slim.\n4\nDISCUSSION\nAlthough we are not able to train shallow models to be as accurate as deep models, the models trained\nvia distillation are the most accurate models of their architecture ever trained on CIFAR-10. For\nexample, the best single-layer fully-connected MLP (no convolution) we trained achieved an accuracy\nof 70.2%. We believe this to be the most accurate shallow MLP ever reported for CIFAR-10 (in\ncomparison to 63.1% achieved by Le et al. (2013), 63.9% by Memisevic et al. (2015) and 64.3% by\nGeras and Sutton (2015)). Although this model cannot compete with convolutional models, clearly\ndistillation helps when training models that are limited by architecture and/or number of parameters.\nSimilarly, the student models we trained with 1, 2, 3, and 4 convolutional layers are, we believe,\nthe most accurate convnets of those depths reported in the literature. For example, the ensemble\nteacher model in Ba and Caruana (2014) was an ensemble of four CNNs, each of which had 3\nconvolutional layers, but only achieved 89% accuracy, whereas the single student CNNs we train via\ndistillation achieve accuracies above 90% with only 2 convolutional layers, and above 92% with 3\nconvolutional layers. The only other work we are aware of that achieves comparable high accuracy\nwith non-convolutional MLPs is recent work by Lin et al. (2016). They train multi-layer Z-Lin\nnetworks, and use a powerful form of data augmentation based on deformations that we did not use.\nInterestingly, we noticed that mimic networks perform consistently worse when trained using dropout.\nThis surprised us, and suggests that training student models on the soft-targets from a teacher provides\nsigniﬁcant regularization for the student models obviating the need for extra regularization methods\nsuch as dropout. This is consistent with the observation made by Ba and Caruana (2014) that student\nmimic models did not seem to overﬁt. Hinton et al. (2015) claim that soft targets convey more\ninformation per sample than Boolean hard targets. The also suggest that the “dark knowledge” in the\nsoft targets for other classes further helped regularization, and that early stopping was unnecessary.\nRomero et al. (2015) extend distillation by using the intermediate representations learned by the\nteacher as hints to guide training deep students, and teacher conﬁdences further help regularization\nby providing a measure of sample “simplicity” to the student, akin to curriculum learning. In other\nwork, Pereyra et al. (2017) suggest that the soft targets provided by a teacher provide a form of\nconﬁdence penalty that penalizes low entropy distributions and label smoothing, both of which\nimprove regularization by maintaining a reasonable ratio between the logits of incorrect classes.\n8\nPublished as a conference paper at ICLR 2017\nZhang et al. (2016) question the traditional view of regularization in deep models. Although they do\nnot discuss distillation, they suggest that in deep learning traditional function approximation appears\nto be deeply intertwined with massive memorization. The multiple soft targets used to train student\nmodels have a high information density (Hinton et al., 2015) and thus provide regularization by\nreducing the impact of brute-force memorization.\n5\nCONCLUSIONS\nWe train shallow nets with and without convolution to mimic state-of-the-art deep convolutional\nnets. If one controls for the number of learnable parameters, nets containing a single fully-connected\nnon-linear layer and no convolutional layers are not able to learn functions as accurate as deeper\nconvolutional models. This result is consistent with those reported in Ba and Caruana (2014).\nHowever, we also ﬁnd that shallow nets that contain only 1-2 convolutional layers also are unable\nto achieve accuracy comparable to deeper models if the same number of parameters are used in\nthe shallow and deep models. Deep convolutional nets are signiﬁcantly more accurate than shallow\nconvolutional models, given the same parameter budget. We do, however, see evidence that model\ncompression allows accurate models to be trained that are shallower and have fewer convolutional\nlayers than the deep convolutional architectures needed to learn high-accuracy models from the\noriginal 1-hot hard-target training data. The question remains why extra layers are required to train\naccurate models from the original training data.\nREFERENCES\nJimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NIPS, 2014.\nFrédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron,\nNicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and\nUnsupervised Feature Learning NIPS 2012 Workshop, 2012.\nJames Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins,\nJoseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler.\nIn SciPy, 2010.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006.\nWilliam Chan, Nan Rosemary Ke, and Ian Laner.\nTransferring knowledge from a RNN to a DNN.\narXiv:1504.01483, 2015.\nNadav Cohen and Amnon Shashua. Convolutional rectiﬁer networks as generalized tensor decompositions.\narXiv preprint arXiv:1603.00162, 2016.\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals\nand Systems, 2(4):303–314, 1989.\nYann N. Dauphin and Yoshua Bengio. Big neural networks waste capacity. arXiv:1301.3583, 2013.\nDavid Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architectures using a recursive\nconvolutional network. In ICLR (workshop track), 2014.\nKrzysztof J. Geras and Charles Sutton. Scheduled denoising autoencoders. In ICLR, 2015.\nKrzysztof J. Geras, Abdel-rahman Mohamed, Rich Caruana, Gregor Urban, Shengjie Wang, Ozlem Aslan,\nMatthai Philipose, Matthew Richardson, and Charles Sutton. Blending LSTMs into CNNs. arXiv:1511.06433,\n2015.\nXavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural networks.\nIn AISTATS, 2010.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\narXiv:1512.03385, 2015.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv:1503.02531,\n2015.\nAlex Krizhevsky. Learning multiple layers of features from tiny images, 2009.\n9\nPublished as a conference paper at ICLR 2017\nQuoc Le, Tamás Sarlós, and Alexander Smola. Fastfood-computing hilbert space expansions in loglinear time.\nIn ICML, 2013.\nJinyu Li, Rui Zhao, Jui-Ting Huang, and Yifan Gong. Learning small-size dnn with output-distribution-based\ncriteria. In INTERSPEECH, 2014.\nShiyu Liang and R Srikant. Why deep neural networks? arXiv preprint arXiv:1610.04161, 2016.\nZhouhan Lin, Roland Memisevic, Shaoqing Ren, and Kishore Konda. How far can we go without convolution:\nImproving fully-connected networks. arXiv:1511.02580v1, 2016.\nRoland Memisevic, Kishore Konda, and David Krueger. Zero-bias autoencoders and the beneﬁts of co-adapting\nfeatures. In ICLR, 2015.\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforce-\nment learning. In ICLR, 2016.\nGabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural\nnetworks by penalizing output distributions. ICLR, 2017.\nAdriana Romero, Ballas Nicolas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.\nFitNets: Hints for thin deep nets. ICLR, 2015.\nAndrei A. Rusu, Sergio Gomez Colmenarejo, Çaglar Gülçehre, Guillaume Desjardins, James Kirkpatrick,\nRazvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. In ICLR, 2016.\nFrank Seide, Gang Li, and Dong Yu. Conversational speech transcription using context-dependent deep neural\nnetworks. In INTERSPEECH, 2011.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\nIn ICLR, 2014.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning\nalgorithms. NIPS, 2012.\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md Patwary,\nMostofa Ali, Ryan P Adams, et al. Scalable bayesian optimization using deep neural networks. In ICML,\n2015.\nRupesh K Srivastava, Klaus Greff, and Juergen Schmidhuber. Training very deep networks. In NIPS, 2015.\nAntonio Torralba, Robert Fergus, and William T. Freeman. 80 million tiny images: A large data set for\nnonparametric object and scene recognition. TPAMI, 30(11), 2008.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning\nrequires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.\n10\nPublished as a conference paper at ICLR 2017\n6\nAPPENDIX\n6.1\nDETAILS OF TRAINING THE TEACHER MODELS\nWeights of trained nets are initialized as in Glorot and Bengio (2010). The models trained in Section 2.7\ncontain eight convolutional layers organized into three groups (2-2-4) and two fully-connected hidden layers.\nThe Bayesian hyperparameter optimization controls four constants C1, C2, C3, H1 all in the range [0, 1] that\nare then linearly transformed to the number of ﬁlters/neurons in each layer. The hyperparameters for which\nranges were not shown in Section 2.7 are: the four separate dropout rates (DOc1, DOc2, DOc3, DOf) and\nthe ﬁve constants Dh, Ds, Dv, As, Av controlling the HSV data augmentation. The ranges we selected are\nDOc1 ∈[0.1, 0.3], DOc2 ∈[0.25, 0.35], DOc3 ∈[0.3, 0.44], DOf1 ∈[0.2, 0.65], DOf2 ∈[0.2, 0.65], Dh ∈\n[0.03, 0.11], Ds ∈[0.2, 0.3], Dv ∈[0.0, 0.2], As ∈[0.2, 0.3], Av ∈[0.03, 0.2], partly guided by Snoek et al.\n(2015) and visual inspection of the resulting augmentations.\nThe number of ﬁlters and hidden units for the models have the following bounds:\n1 conv. layer: 50 - 500 ﬁlters, 200 - 2000 hidden units, number of units in bottleneck is the dependent variable.\n2 conv. layers: 50 - 500 ﬁlters, 100 - 400 ﬁlters, number of hidden units is the dependent variable.\n3 conv. layers: 50 - 500 ﬁlters (layer 1), 100 - 300 ﬁlters (layers 2-3), # of hidden units is dependent the variable.\n4 conv. layers: 50 - 300 ﬁlters (layers 1-2), 100 - 300 ﬁlters (layers 3-4), # of hidden units is the dependent\nvariable.\nAll convolutional ﬁlters in the model are sized 3×3, max-pooling is applied over windows of 2×2 and we use\nReLU units throughout all our models. We apply dropout after each max-pooling layer with the three rates\nDOc1, DOc2, DOc3 and after each of the two fully-connected layers with the same rate DOf.\nTable 3: Optimization bounds for student models. (Models trained on 0/1 hard targets were described\nin Sections 6.1 and 6.2.) Abbreviations: fc (fully-connected layer, ReLu), c (convolutional, ReLu),\nlinear (fully-connected bottleneck layer, linear activation function), dependent (dependent variable,\nchosen s.t. parameter budget is met).\n1st layer\n2nd layer\n3rd layer\n4th layer\n5th layer\nNo conv. layer (1M)\n500 - 5000 (fc)\ndependent (linear)\nNo conv. layer (3.1M)\n1000 - 20000 (fc)\ndependent (linear)\nNo conv. layer (10M)\n5000 - 30000 (fc)\ndependent (linear)\nNo conv. layer (31M)\n5000 - 45000 (fc)\ndependent (linear)\n1 conv. layer (1M)\n40 - 150 (c)\ndependent (linear)\n200 - 1600 (fc)\n1 conv. layer (3.1M)\n50 - 300 (c)\ndependent (linear)\n100 - 4000 (fc)\n1 conv. layer (10M)\n50 - 450 (c)\ndependent (linear)\n500 - 20000 (fc)\n1 conv. layer (31M)\n200 - 600 (c)\ndependent (linear)\n1000 - 4100 (fc)\n2 conv. layers (1M)\n20 - 120 (c)\n20 - 120 (c)\ndependent (fc)\n2 conv. layers (3.1M)\n50 - 250 (c)\n20 - 120 (c)\ndependent (fc)\n2 conv. layers (10M)\n50 - 350 (c)\n20 - 120 (c)\ndependent (fc)\n2 conv. layers (31M)\n50 - 800 (c)\n20 - 120 (c)\ndependent (fc)\n3 conv. layers (1M)\n20 - 110 (c)\n20 - 110 (c)\n20 - 110 (c)\ndependent (fc)\n3 conv. layers (3.1M)\n40 - 200 (c)\n40 - 200 (c)\n40 - 200 (c)\ndependent (fc)\n3 conv. layers (10M)\n50 - 350 (c)\n50 - 350 (c)\n50 - 350 (c)\ndependent (fc)\n3 conv. layers (31M)\n50 - 650 (c)\n50 - 650 (c)\n50 - 650 (c)\ndependent (fc)\n4 conv. layers (1M)\n25 - 100 (c)\n25 - 100 (c)\n25 - 100 (c)\n25 - 100 (c)\ndependent (fc)\n4 conv. layers (3.1M)\n50 - 150 (c)\n50 - 150 (c)\n50 - 200 (c)\n50 - 200 (c)\ndependent (fc)\n4 conv. layers (10M)\n50 - 300 (c)\n50 - 300 (c)\n50 - 350 (c)\n50 - 350 (c)\ndependent (fc)\n4 conv. layers (31M)\n50 - 500 (c)\n50 - 500 (c)\n50 - 650 (c)\n50 - 650 (c)\ndependent (fc)\n6.2\nDETAILS OF TRAINING MODELS OF VARIOUS DEPTHS ON CIFAR-10 HARD 0/1 LABELS\nModels in the ﬁrst four rows in Table 1 are trained similarly to those in Section 6.1, and are architecturally\nequivalent to the four convolutional student models shown in Table 2 with 10 million parameters. The following\nhyperparameters are optimized: initial learning rate [0.0015, 0.025] (optimized on a log scale), momentum\n[0.68, 0.97] (optimized on a log scale), constants C1, C2 ∈[0, 1] that control the number of ﬁlters or neurons\nin different layers, and up to four different dropout rates DOc1 ∈[0.05, 0.4], DOc2 ∈[0.1, 0.6], DOc3 ∈\n[0.1, 0.7], DOf1 ∈[0.1, 0.7] for the different layers. Weight decay was set to 2 · 10−4 and we used the same\ndata augmentation settings as for the student models. We use 5×5 convolutional ﬁlters, one nonlinear hidden\nlayer in each model and each max-pooling operation is followed by dropout with a separately optimized rate.\nWe use 2×2 max-pooling except in the model with only one convolutional layer where we apply 3×3 pooling as\nthis seemed to boost performance and reduces the number of parameters.\n11\nPublished as a conference paper at ICLR 2017\n6.3\nDETAILS OF TRAINING STUDENT MODELS OF VARIOUS DEPTHS ON ENSEMBLE LABELS\nOur student models have the same architecture as models in Section 6.2. The model without convolutional layers\nconsists of one linear layer that acts as a bottleneck followed by a hidden layer of ReLU units. The following\nhyperparameters are optimized: initial learning rate [0.0013, 0.016] (optimized on a log scale), momentum\n[0.68, 0.97] (optimized on a log scale), input-scale ∈[0.8, 1.25], global initialization scale (after initialization)\n∈[0.4, 2.0], layer-width constants C1, C2 ∈[0, 1] that control the number of ﬁlters or neurons. The exact ranges\nfor the number of ﬁlters and implicitly resulting number of hidden units was chosen for all twenty optimization\nexperiments independently, as architectures, number of units and number of parameters strongly interact.\nFor the non-convolutional models we chose a slightly different hyper-parameterization. Given that all layers (in\nmodels with “two layers” or more) are nonlinear and fully connected we treat all of them similarly from the\nhyperparameter-optimizer’s point of view. In order to smoothly enforce the parameter budgets without rejecting\nany samples from the Bayesian optimizer we instead optimize the ratios of hidden units in each layer (numbers\nbetween 0 and 1), and then re-normalize and scale them to the ﬁnal number of neurons in each layer to match\nthe target parameter budget.\n12\nPublished as a conference paper at ICLR 2017\n1\n3\n10\n31\n100\nNumber of Parameters [millions]\n65\n70\n75\n80\n85\n90\nAccuracy\nteacher ensemble\ncompression gap\n         convolutional gap               \ncompression gap\nCNN: 1 convolutional layer\nCNN: 2 convolutional layers\nCNN: 3 convolutional layers\nCNN: 4 convolutional layers\nMLP: 1 hidden layer\nMLP: 2 hidden layers\nMLP: 3 hidden layers\nMLP: 4 hidden layers\nMLP: 5 hidden layers\nFigure 2: See ﬁgure 1.\nFigure 2 is similar to 1 but includes preliminary re-\nsults from experiments for models with 100M param-\neters. We are also running experiments with 300M\nparameters. Unfortunately, Bayesian optimization\non models with 100M and 300M parameters is even\nmore expensive than for the other points in the graph.\nAs expected, adding capacity to the convolutional\nstudents (top of the ﬁgure) modestly increases their\naccuracy. Preliminary results for the MLPs however\n(too preliminary to include in the graph) may not\nshow the same increase in accuracy with increasing\nmodel size. Models with two or three hidden layers\nmay beneﬁt from adding capacity to each layer, but\nwe have yet to see any beneﬁt from adding capacity\nto the MLPs with four or ﬁve hidden layers.\n13\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2016-03-17",
  "updated": "2017-03-04"
}