{
  "id": "http://arxiv.org/abs/1603.07846v1",
  "title": "Deep Learning At Scale and At Ease",
  "authors": [
    "Wei Wang",
    "Gang Chen",
    "Haibo Chen",
    "Tien Tuan Anh Dinh",
    "Jinyang Gao",
    "Beng Chin Ooi",
    "Kian-Lee Tan",
    "Sheng Wang"
  ],
  "abstract": "Recently, deep learning techniques have enjoyed success in various multimedia\napplications, such as image classification and multi-modal data analysis. Large\ndeep learning models are developed for learning rich representations of complex\ndata. There are two challenges to overcome before deep learning can be widely\nadopted in multimedia and other applications. One is usability, namely the\nimplementation of different models and training algorithms must be done by\nnon-experts without much effort especially when the model is large and complex.\nThe other is scalability, that is the deep learning system must be able to\nprovision for a huge demand of computing resources for training large models\nwith massive datasets. To address these two challenges, in this paper, we\ndesign a distributed deep learning platform called SINGA which has an intuitive\nprogramming model based on the common layer abstraction of deep learning\nmodels. Good scalability is achieved through flexible distributed training\narchitecture and specific optimization techniques. SINGA runs on GPUs as well\nas on CPUs, and we show that it outperforms many other state-of-the-art deep\nlearning systems. Our experience with developing and training deep learning\nmodels for real-life multimedia applications in SINGA shows that the platform\nis both usable and scalable.",
  "text": "A\nDeep Learning At Scale and At Ease\nWei Wang, School of Computing, National University of Singapore, Singapore\nGang Chen, College of Computer Science, Zhejiang University, China\nHaibo Chen, NetEase, Inc., China\nTien Tuan Anh Dinh and Jinyang Gao and Beng Chin Ooi and\nKian-Lee Tan and Sheng Wang, School of Computing, National University of Singapore, Singapore\nRecently, deep learning techniques have enjoyed success in various multimedia applications, such as image classiﬁcation\nand multi-modal data analysis. Large deep learning models are developed for learning rich representations of complex data.\nThere are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One\nis usability, namely the implementation of different models and training algorithms must be done by non-experts without\nmuch effort especially when the model is large and complex. The other is scalability, that is the deep learning system must\nbe able to provision for a huge demand of computing resources for training large models with massive datasets. To address\nthese two challenges, in this paper, we design a distributed deep learning platform called SINGA which has an intuitive\nprogramming model based on the common layer abstraction of deep learning models. Good scalability is achieved through\nﬂexible distributed training architecture and speciﬁc optimization techniques. SINGA runs on GPUs as well as on CPUs, and\nwe show that it outperforms many other state-of-the-art deep learning systems. Our experience with developing and training\ndeep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable.\n1. INTRODUCTION\nIn recent years, we have witnessed successful adoptions of deep learning in various multimedia ap-\nplications, such as image and video classiﬁcation [Krizhevsky et al. 2012; Wu et al. 2014], content-\nbased image retrieval [Wan et al. 2014], music recommendation [Wang and Wang 2014] and multi-\nmodal data analysis [Wang et al. 2014; Feng et al. 2014; Zhang et al. 2014]. Deep learning refers\nto a set of feature learning models which consist of multiple layers. Different layers learn different\nlevels of abstractions (or features) of the raw input data [Le et al. 2012]. It has been regarded as a re-\nbranding of neural networks developed twenty years ago, since it inherits many key neural networks\ntechniques and algorithms. However, deep learning exploits the fact that high-level abstractions are\nbetter at representing the data than raw, hand-crafted features, thus achieving better performance\nin learning. Its recent resurgence is mainly fuelled by higher than ever accuracy obtained in image\nrecognition [Krizhevsky et al. 2012]. Three key factors behind deep learning’s remarkable achieve-\nment are the advances of neural net structures, immense computing power and the availability of\nmassive training datasets, which together enable us to train large models to capture the regularities\nof complex data more efﬁciently than twenty years ago.\nThere are two challenges in bringing deep learning to wide adoption in multimedia applications\n(and other applications for that matter). The ﬁrst challenge is usability, namely the implementa-\ntion of different models and training algorithms must be done by non-experts with little effort.\nThe user must be able to choose among many existing deep learning models, as different multi-\nmedia applications may beneﬁt from different models. For instance, the deep convolution neural\nnetwork (DCNN) is suitable for image classiﬁcation [Krizhevsky et al. 2012], recurrent neural net-\nwork (RNN) for language modelling [Mikolov et al. 2011], and deep auto-encoders for multi-modal\ndata analysis [Wang et al. 2014; Feng et al. 2014; Zhang et al. 2014]. Furthermore, the user must\nnot be required to implement most of these models and training algorithms from scratch, for they\nare too complex and costly. An example of complex models is the GoogleLeNet [Szegedy et al.\n2014] which comprises 22 layers of 10 different types. Training algorithms are intricate in details.\nFor instance the Back-Propagation [LeCun et al. 1996] algorithm is notoriously difﬁcult to debug.\nThe second challenge is scalability, that is the deep learning system must be able to provision for\na huge demand of computing resources for training large models with massive datasets. As larger\ntraining datasets and bigger models are being used to improve accuracy [Ciresan et al. 2010; Le\net al. 2012; Szegedy et al. 2014], memory requirement for training the model may easily exceed the\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\narXiv:1603.07846v1  [cs.LG]  25 Mar 2016\nA:2\nW. Wang et al.\ncapacity of a single CPU or GPU. In addition, the computational cost of training may be too high for\na single commodity server, which results in unreasonably long training time. For instance, it takes\n10 days [Yadan et al. 2013; Paine et al. 2013] to train the DCNN [Krizhevsky et al. 2012] with 1.2\nmillion training images and 60 million parameters using one GPU 1.\nAddressing both usability and scalability challenges requires a distributed training platform that\nsupports various deep learning models, that comes with an intuitive programming model (similar to\nMapReduce [Dean and Ghemawat 2004], Spark [Zaharia et al. 2012] and epiC [Jiang et al. 2014]\nin spirit), and that is scalable. Popular deep learning systems, including Caffe [Jia et al. 2014],\nTorch [Collobert et al. 2011] and Theano [Bastien et al. 2012], address the ﬁrst challenge but fall\nshort at the second challenge (they are not designed for distributed training). Similarly, Google’s\ndeep learning platform, called TensorFlow [Abadi et al. 2015], is designed to be ﬂexible and easy to\nuse, but its scalability remains unknown (TensorFlow only provides single node version for the time\nbeing). There are several systems supporting distributed training [Paine et al. 2013; Yadan et al.\n2013; Krizhevsky 2014], but they are model speciﬁc and do not generalize well to other models.\nGeneral distributed platforms such as MapReduce and Spark achieve good scalability, but they are\ndesigned for general data processing. As a result, they lack both the programming model and system\noptimization speciﬁc to deep learning, hindering the overall usability and scalability. Recently, there\nare several specialized distributed platforms [Dean et al. 2012; Coates et al. 2013; Chilimbi et al.\n2014] that exploit deep learning speciﬁc optimization and hence are able to achieve high training\nthroughput. However, they forgo usability issues: the platforms are closed-source and no details of\ntheir programming models are given, rendering them unusable by multimedia users.\nIn this paper, we present our effort in bringing deep learning to the masses. In particular, we\nextend our previous work [Ooi et al. 2015; Wang et al. 2015] on distributed training of deep learn-\ning models. In [Wang et al. 2015], we designed and implemented an open source distributed deep\nlearning platform, called SINGA2, which tackles both usability and scalability challenges at the\nsame time. In this paper, we will introduce optimization techniques and GPU support for SINGA.\nSINGA provides a simple, intuitive programming model which makes it accessible even to non-\nexperts. SINGA’s simplicity is driven by the observation that both the structures and training algo-\nrithms of deep learning models can be expressed using a simple abstraction: the neuron layer (or\nlayer). In SINGA, the user deﬁnes and connects layers to form the neural network model, and the\nruntime transparently manages other issues pertaining to the distributed training such as partitioning,\nsynchronization and communication. Particularly, the neural network is represented as a dataﬂow\ncomputation graph with each layer being a node. During distributed training, the graph is partitioned\nand each sub-graph can be trained on CPUs or on GPUs. SINGA’s scalability comes from its ﬂexible\nsystem architecture and speciﬁc optimization. Both synchronous and asynchronous training frame-\nworks are supported with a range of built-in partitioning strategies, which enables users to readily\nexplore and ﬁnd an optimal training conﬁguration. Optimization techniques, including minimizing\ndata transferring and overlapping computation and communication, are implemented to reduce the\ncommunication overhead from distributed training.\nIn summary, this paper makes the following contributions:\n(1) We present a distributed platform called SINGA which is designed to train deep learning models\nfor multimedia and other applications. SINGA offers a simple and intuitive programming model\nbased on the layer abstraction.\n(2) We describe SINGA’s distributed architecture and optimization for reducing the communication\noverhead in distributed training.\n(3) We demonstrate SINGA’s usability by describing the implementation of three multimedia ap-\nplications: multi-modal retrieval, dimensionality reduction and sequence modelling.\n1According to the authors, with 2 GPUs, the training still took about 6 days.\n2http://www.comp.nus.edu.sg/∼dbsystem/singa/\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:3\n(4) We evaluate SINGA’s performance by comparing it with other open-source systems. The results\nshow that SINGA is scalable and outperforms other systems in terms of training time.\nThis paper is an extension of our conference paper [Wang et al. 2015]. In [Wang et al. 2015], we\nhave presented the basic SINGA framework for a homogeneous architecture (where we consider\nonly CPU nodes). In this paper, we extend the framework to a heterogeneous setting that consists\nof both GPU and CPU processors. Optimization techniques in terms of reducing communication\noverhead from distributed training are introduced in this paper. Correspondingly, we conducted\nexperiments on GPUs in comparison with existing systems. The rest of this paper is organized as\nfollows. Section 2 provides the background on training deep learning models and related work. An\noverview of SINGA as a platform follows in Section 3. The programming model is discussed in\nSection 4. We discuss SINGA architecture and training optimization in Section 5. The experimental\nstudy is presented in Section 6 before we conclude in Section 7.\n2. BACKGROUND\nDeep learning is considered as a feature learning technique. A deep learning model typically consists\nof multiple layers, each associated with a feature transformation function. After going through all\nlayers, raw input features (e.g., pixels of images) are converted into high-level features that are used\nfor the task of interest, e.g., image classiﬁcation.\n2.1. Models and Training Algorithms\nA: Feed-forward \nB: Undirected \nC: Recurrent \nCNN \nMLP \nAuto-Encoders \nFig. 1: Deep learning model categorization.\nWe group popular deep learning models into three categories based on the connection types be-\ntween layers, as shown in Figure 1. Category A consists of feed-forward models wherein the layers\nare directly connected. The extracted features at higher layers are fed into prediction or classiﬁ-\ncation tasks, e.g., image classiﬁcation [Krizhevsky et al. 2012]. Example models in this category\ninclude Multi-Layer Perceptron (MLP), Convolution Neural Network (CNN) and Auto-Encoders.\nCategory B contains models whose layer connections are undirected. These models are often used\nto pre-train other models [Hinton and Salakhutdinov 2006], e.g., feed-forward models. Deep Belief\nNetwork (DBN), Deep Boltzmann Machine (DBM) and Restricted Boltzmann Machine (RBM) are\nexamples of such models. Category C comprises models that have recurrent connections. These\nmodels are called Recurrent Neutral Networks (RNN). They are widely used for modelling se-\nquential data in which prediction of the next position is affected by previous positions. Language\nmodelling [Mikolov et al. 2011] is a popular application of RNN.\nA deep learning model has to be trained to ﬁnd the optimal parameters for the transformation\nfunctions. The training quality is measured by a loss function (e.g., cross-entropy loss) for each\nspeciﬁc task. Since the loss functions are usually non-linear and non-convex, it is difﬁcult to get\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:4\nW. Wang et al.\ninitialize  \nparameters \ncompute  \ngradients \nupdate  \nparameters \nread mini-\nbatch data \nFig. 2: Flow of stochastic gradient descent algorithm.\nclosed-form solutions. A common approach is to use the Stochastic Gradient Descent (SGD) algo-\nrithm shown in Figure 2. SGD initializes the parameters with random values, and then iteratively re-\nﬁnes them to reduce the loss based on the computed gradients. There are three typical algorithms for\ngradient computation corresponding to the three model categories above: Back-Propagation (BP),\nContrastive Divergence (CD) and Back-Propagation Through Time (BPTT).\nTable I: Feature comparison with other open source systems.\nSINGA\nTensor-\nCaffe\nTorch\nMxNet\nTheano\nCuda-\nFlow\nconvnet2\nFeed-forward net\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nEnergy model\n✓\n✓\n×\n✓\n-\n✓\n×\nRNN\n✓\n✓\n✓\n✓\n✓\n✓\n×\nData parallelism\n✓\n✓\n✓\n✓\n✓\n-\n✓\nModel parallelism\n✓\n-\n-\n-\n-\n-\n✓\nHybrid parallelism\n✓\n-\n-\n-\n-\n-\n✓\nGPU\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nCPU\n✓\n✓\n✓\n✓\n✓\n✓\n×\nPython\n✓\n✓\n✓\n×\n✓\n✓\n✓\nR\nworking\n-\n✓\n-\n✓\n×\n×\nHDFS\n✓\n-\n-\n-\n✓\n-\n×\nMesos/YARN\n✓\n-\n-\n-\n-\n-\n×\n-: Could support but not implemented yet.\n2.2. Related Work\nDue to its outstanding capabilities in capturing complex regularities of multimedia data (e.g., image\nand video), deep learning techniques are being adopted by more and more multimedia applications,\ne.g., image retrieval [Wan et al. 2014], multi-modal retrieval [Wang et al. 2015; Wang et al. 2014],\nsentiment analysis [You et al. 2015], etc. In recent years, we have witnessed fast increase of deep\nlearning models’ depth, from tens of layers (e.g., AlexNet [Krizhevsky et al. 2012], VGG [Simonyan\nand Zisserman 2014]) to hundreds of layers [He et al. 2015]. It has been shown that deeper models\nwork better for the ImageNet challenge task [Szegedy et al. 2014; Simonyan and Zisserman 2014].\nMeanwhile, training datasets are also becoming larger, from 60,000 images in the MNIST and Cifar\ndatasets to millions of images in the ImageNet dataset. Complex deep models and massive training\ndatasets require a huge amount of computing resources for training.\nDifferent applications use different deep learning models. It is essential to provide a general deep\nlearning system for non-experts to implement their models without much effort. Recently, some dis-\ntributed training approaches have been proposed, for examples [Paine et al. 2013; Yadan et al. 2013;\nKrizhevsky 2014]. They are speciﬁcally optimized for training the AlexNet model [Krizhevsky\net al. 2012], thus cannot generalize well to other models. Other general distributed deep learning\nplatforms [Dean et al. 2012; Coates et al. 2013; Chilimbi et al. 2014] exploit deep learning speciﬁc\noptimization and hence are able to achieve high training throughput. However, they are closed-\nsource and there are no details of the programming model, rendering them unusable to developers.\nThere are also some popular open source systems for training deep learning models on a single\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:5\nnode, including TensorFlow [Abadi et al. 2015], Caffe [Jia et al. 2014], Torch [Collobert et al.\n2011], MxNet [Chen et al. 2015], Theano [Bastien et al. 2012] and Cuda-Convnet23. Table I shows\nthe comparison of SINGA and these systems in terms of supported features.\n3. OVERVIEW\nTrainOneBatch \nNeuralNet \nLayer \nstop \nUpdater: \nUpdate(step, param) \nDataShard \nWorker \nWorker \nServer \nServer \nCluster Topology \nFig. 3: SINGA overview.\nSINGA trains deep learning models using SGD over the worker-server architecture, as shown\nin Figure 3. Workers compute parameter gradients and servers perform parameter updates. To start\na training job, the user (or programmer) submits a job conﬁguration specifying the following four\ncomponents:\n— A NeuralNet describing the neural network (or neural net) structure with the detailed layers and\ntheir connections. SINGA comes with many built-in layers (Section 4.1.2), and users can also\nimplement their own layers.\n— A TrainOneBatch algorithm for training the model. SINGA implements different algorithms\n(Section 4.1.3) for all three model categories.\n— An Updater deﬁning the protocol for updating parameters at the servers (Section 4.1.4).\n— A Cluster Topology specifying the distributed architecture of workers and servers. SINGA’s ar-\nchitecture is ﬂexible and can support both synchronous and asynchronous training (Section 5).\nGiven a job conﬁguration, SINGA distributes the training tasks over the cluster and coordinates\nthe training. In each iteration, every worker calls TrainOneBatch function to compute parameter\ngradients. TrainOneBatch takes a NeuralNet object representing the neural net, and it visits (part of)\nthe model layers in an order speciﬁc to the model category. The computed gradients are sent to the\ncorresponding servers for updating. Workers then fetch the updated parameters at the next iteration.\n4. PROGRAMMING MODEL\nThis section describes SINGA’s programming model, particularly the main components of a SINGA\njob. We use the MLP model for image classiﬁcation (Figure 4(a)) as a running example. The model\nconsists of an input layer, a hidden feature transformation layer and a Softmax output layer.\n4.1. Programming Abstractions\n4.1.1. NeuralNet. NeuralNet represents a neural net instance in SINGA. It comprises a set of\nunidirectionally connected layers. Properties and connections of layers are speciﬁed by users. The\nNeuralNet object is passed as an argument to the TrainOneBatch function.\n3https://code.google.com/p/cuda-convnet/\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:6\nW. Wang et al.\n(a) Sample MLP.\nlayer:  name: “softmax loss” \n            type: SoftmaxLossLayer \n            srclayer: “hidden”,       \n                            “data” \nlayer:  name: “hidden” \n            type: HiddenLayer \n            srclayer: “data” \n            shape: 3 \nlayer:  name: “data” \n       type: kInputLayer \n       path: “./train.shard” \n \ndata \nhidden \nsoftmax loss \n(b) NeuralNet conﬁguration.\nBlob feature = data[0], gradient=data[1]; \nParam W, b; \n \nFunc  ComputeFeature(flag, srclayers) { \n      feature= logistic(dot(srclayers[0].feature, W.data) + b.data);  \n} \nFunc  ComputeGradient(flag, srclayers) { \n     Blob tmp = feature * (1-feature); \n     srclayers[0].gradient = tmp*dot(gradient, W.data.transpose()); \n     W.gradient = tmp*dot(src[0].data.transpose(), gradient); \n     b.gradient = tmp * gradient; \n} \n(c) Hidden layer implementation.\nFig. 4: Running example using an MLP.\nLayer connections in NeuralNet are not designed explicitly; instead each layer records its own\nsource layers as speciﬁed by users (Figure 4(b)). Although different model categories have different\ntypes of layer connections, they can be uniﬁed using directed edges as follows. For feed-forward\nmodels, nothing needs to be done as their connections are already directed. For undirected models,\nusers need to replace each edge with two directed edges, as shown in Figure 5(a). For recurrent mod-\nels, users can unroll a recurrent layer into directed-connecting sub-layers, as shown in Figure 5(b).\n(a) Convert connections in RBM.\n… \n(b) Unroll RNN.\nFig. 5: Unify neural net connections.\n4.1.2. Layer. Layer is a core abstraction in SINGA. Different layer implementations perform\ndifferent feature transformations to extract high-level features. In every SGD iteration, all layers in\nthe NeuralNet are visited by the TrainOneBatch function during the process of computing parameter\ngradients. From the dataﬂow perspective, we can regard the neural net as a graph where each layer\nis a node. The training procedure passes data along the connections of layers and invokes functions\nof layers. Distributed training can be easily conducted by assigning sub-graphs to workers.\nLayer: \n  vector<Blob> data \n  vector<Param> param \n  Func ComputeFeature(flag, srclayers); \n  Func ComputeGradient(flag, srclayers); \n \nParam:  \n  Blob data, gradient; \nFig. 6: Layer abstraction.\nFigure 6 shows the deﬁnition of a base layer. The data ﬁeld records data (blob) associated with\na layer. Some layers may require parameters (e.g., a weight matrix) for their feature transforma-\ntion functions. In this case, these parameters are represented by Param objects, each with a data\nﬁeld for the parameter values and a gradient ﬁeld for the gradients. The ComputeFeature function\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:7\nevaluates the feature blob by transforming features from the source layers. The ComputeGradient\nfunction computes the gradients associated with this layer. These two functions are invoked by the\nTrainOneBatch function during training (Section 4.1.3).\nSINGA provides a variety of built-in layers to help users build their models. Table II lists the\nlayer categories in SINGA. For example, the data layer loads a mini-batch of records via the Com-\nputeFeature function in each iteration. Users can also deﬁne their own layers for their speciﬁc re-\nquirements. Figure 4(c) shows an example of implementing the hidden layer h in the MLP. In this\nexample, beside feature blobs there are gradient blobs storing the gradients of the loss with respect\nto the feature blobs. There are two Param objects: the weight matrix W and the bias vector b. The\nComputeFeature function rotates (multiply W), shifts (plus b) the input features and then applies\nnon-linear (logistic) transformations. The ComputeGradient function computes the layer’s param-\neter gradients, as well as the source layer’s gradients that will be used for evaluating the source\nlayer’s parameter gradients.\nTable II: Layer categories.\nCategory\nDescription\nInput layers\nLoad records from ﬁle, database or HDFS.\nOutput layers\nDump records to ﬁle, database or HDFS.\nNeuron layers\nFeature transformation, e.g., convolution.\nLoss layers\nCompute objective loss, e.g., cross-entropy loss.\nConnection layers\nConnect layers when neural net is partitioned.\n4.1.3. TrainOneBatch. The TrainOneBatch function determines the sequence of invoking Com-\nputeFeature and ComputeGradient functions in all layers during each SGD iteration. SINGA imple-\nments two TrainOneBatch algorithms for the three model categories. For feed-forward and recurrent\nmodels, the BP algorithm is provided. For undirected modes (e.g., RBM), the CD algorithm is pro-\nvided. Users simply select the corresponding algorithm in the job conﬁguration. Should there be\nspeciﬁc requirements for the training workﬂow, users can deﬁne their own TrainOneBatch function\nfollowing a template shown in Algorithm 1. Algorithm 1 implements the BP algorithm which takes\na NeuralNet object as input. The ﬁrst loop visits each layer and computes their features, and the\nsecond loop visits each layer in the reverse order and computes parameter gradients. More details\non applying BP for RNN models (i.e., BPTT), and the CD algorithm.\nALGORITHM 1: BPTrainOneBatch\nInput: net\nforeach layer in net.layers do\nCollect(layer.params()) // receive parameters\nlayer.ComputeFeature() // forward prop\nend\nforeach layer in reverse(net.layers) do\nlayer.ComputeGradient()// backward prop\nUpdate(layer.params())// send gradients\nend\n4.1.4. Updater. Once the parameter gradients are computed, workers send these values to servers\nto update the parameters. SINGA implements several parameter updating protocols, such as Ada-\nGrad[Duchi et al. 2011]. Users can also deﬁne their own updating protocols by overriding the Up-\ndate function.\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:8\nW. Wang et al.\n4.2. Multimedia Applications\nThis section demonstrates the use of SINGA for multimedia applications. We discuss the training\nof three deep learning models for three different applications: a multi-modal deep neural network\n(MDNN) for multi-modal retrieval, a RBM for dimensionality reduction, and a RNN for sequence\nmodelling.\nImageParserLayer \nPoolingLayer \nRectifierLayer \nNormalizationLayer \nInner-ProductLayer \nSoftmaxLossLayer \nConvolutionLayer \nData Layer \nTextParserLayer \nbird \ndog \nship \nLabel Layer \nplan \nball \nsky \nSoftmaxLossLayer \nEuclidean loss \nInner-Product \nLogisticLayer \nInner-ProductLayer \nFig. 7: Structure of MDNN.\n4.2.1. MDNN for Multi-modal Retrieval. Feed-forward models such as CNN and MLP are widely\nused to learn high-level features in multimedia applications, especially for image classiﬁca-\ntion [Krizhevsky et al. 2012]. Here, we demonstrate the training of the MDNN [Wang et al. 2015],\nwhich combines a CNN and a MLP. MDNN is used for extracting features for the multi-modal\nretrieval task [Wang et al. 2014; Feng et al. 2014; Shen et al. 2000] that searches objects from dif-\nferent modalities. In MDNN, the CNN [Krizhevsky et al. 2012] is used to extract image features,\nand the MLP is used to extract text features. The training objective is to minimize a weighted sum\nof: (1) the error of predicting the labels of image and text documents using extracted features; and\n(2) the distance between features of relevant image and text objects. As a result, the learned features\nof semantically relevant objects from different modalities are similar. After training, multi-modal\nretrieval is conducted using the learned features.\nFigure 7 depicts neural net of MDNN model in SINGA. We can see that there are two parallel\npaths: one for text modality and the other for image modality. The data layer reads in records of\nsemantically relevant image-text pairs. The image layer, text layer and label layer then parse the\nvisual feature, text feature (e.g., tags of the image) and labels respectively from the records. The\nimage path consists of layers from DCNN [Krizhevsky et al. 2012], e.g., the convolution layer and\npooling layer. The text path includes an inner-product (or fully connected) layer, a logistic layer and\na loss layer. The Euclidean loss layer measures the distance of the feature vectors extracted from\nthese two paths. All except the parser layers, which are application speciﬁc, are SINGA’s built-in\nlayers. Since this model is a feed-forward model, the BP algorithm is selected for the TrainOneBatch\nfunction.\n4.2.2. RBM for Dimensionality Reduction. RBM is often employed to pre-train parameters for\nother models. In this example application, we use RBM to pre-train a deep auto-encoder [Hinton\nand Salakhutdinov 2006] for dimensionality reduction. Multimedia applications typically operate\nwith high-dimensional feature vectors, which demands large computing resources. Dimensionality\nreduction techniques, such as Principal Component Analysis (PCA), are commonly applied in the\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:9\npre-processing step. Deep auto-encoder is reported [Hinton and Salakhutdinov 2006] to have better\nperformance than PCA.\n \nW1, b2 \nW1, b1 \nInput feature \nInput feature \nInput feature \n \nW1, b2 \nW2, b3 \nW2, b4 \nW1, b2 \nW2, b4 \nW2\nT, b3 \nW1\nT, b1 \nReconstructionLossLayer \nRBM 1 \nRBM 2 \nDeep Auto-encoder \nStep 1 \nStep 2 \nFig. 8: Structure of RBM and deep auto-encoder.\nGenerally, the deep auto-encoder is trained to reconstruct the input feature using the feature of\nthe top layer. Hinton et al. [Hinton and Salakhutdinov 2006] used RBM to pre-train the parameters\nfor each layer, and ﬁne-tuned them to minimize the reconstruction error. Figure 8 shows the model\nstructure (with parser layer and data layer omitted) in SINGA. The parameters trained from the ﬁrst\nRBM (RBM 1) in step 1 are ported (through checkpoint) into step 2 wherein the extracted features\nare used to train the next model (RBM 2). Once pre-training is ﬁnished, the deep auto-encoder\nis unfolded for ﬁne-tuning. SINGA applies the contrastive divergence (CD) algorithm for training\nRBM and back-propagation (BP) algorithm for ﬁne-tuning the deep auto-encoder.\n4.2.3. RNN for Sequence Modelling. Recurrent neural networks (RNN) are widely used for mod-\nelling sequential data, e.g., natural language sentences. We use SINGA to train a Char-RNN model\n4 over Linux kernel source code, with each character as an input unit. The model predicts the next\ncharacter given the current character.\nOneHotLayer \nGRULayer \nGRULayer \nInnerProductLayer \nSoftmaxLossLayer \nLabelLayer \nFig. 9: Structure of 2-stacked Char-RNN (left before unrolling; right after unrolling).\nFigure 9 illustrates the net structure of the Char-RNN model. The input layer buffers all training\ndata (the Linux kernel code is about 6MB). In each iteration, it reads unroll len + 1 (unroll len is\nspeciﬁed by users) successive characters, e.g., “int a;” and passes the ﬁrst unroll len characters to\nOneHotLayers (one per layer). Each OneHotLayer converts its character into a one-hot vector repre-\nsentation. The input layer passes the last unroll len characters as labels to the RNNLabelLayer (the\nlabel of the ith character is the (i+1)th character, i.e., the objective is to predict the next character).\n4https://github.com/karpathy/char-rnn\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:10\nW. Wang et al.\nServer Group \nParameters \nServer \nServer \nServer \nServer Group \nWorker Group \nServer Group \nWorker Group \nWorker Group \n(Model Parallelism) \nParameters \nNeural Net \nData \nWorker \nWorker \nWorker \nWorker Group \n(Data Parallelism) \nParameters \nNeural Net \nData \nWorker \nWorker \nWorker \nFig. 10: Logical architecture of SINGA.\nEach GRULayer receives a one-hot vector and the hidden feature vector from its precedent layer.\nAfter some feature transformations, its own feature vector is passed to another stack of GRULayer\nand its successive GRULayer. The InnerProductLayers transform the output from the GRULayers\nin the second stack and feed them into the SoftmaxLossLayer. The ith SoftmaxLossLayer measures\nthe cross-entropy loss for predicting the ith character. The model is conﬁgured similarly as for feed-\nforward models except the training algorithm is BPTT, and unrolling length and connection types\nare speciﬁed for recurrent layers. Different colors are used for illustrating the neural net partitioning\nwhich will be discussed in Section 5.3.\n5. DISTRIBUTED TRAINING\nIn this section, we introduce SINGA’s architecture, and discuss how it supports a variety of dis-\ntributed training frameworks.\n5.1. System Architecture\nFigure 10 shows the logical architecture, which consists of multiple server groups and worker\ngroups, and each worker group communicates with only one server group. Each server group main-\ntains a complete replica of the model parameters, and is responsible for handling requests (e.g., get\nor update parameters) from worker groups. Neighboring server groups synchronize their parameters\nperiodically. Typically, a server group contains a number of servers, and each server manages a parti-\ntion of the model parameters. Each worker group trains a complete model replica against a partition\nof the training dataset (i.e. data parallelism), and is responsible for computing parameter gradients.\nAll worker groups run and communicate with the corresponding server groups asynchronously.\nHowever, inside each worker group, the workers compute parameter updates synchronously for the\nmodel replica. There are two strategies to distribute the training workload among workers within\na group: by model or by data. More speciﬁcally, each worker can compute a subset of parameters\nagainst all data partitioned to the group (i.e., model parallelism), or all parameters against a subset\nof data (i.e., data parallelism). SINGA also supports hybrid parallelism (Section 5.3).\nIn SINGA, servers and workers are execution units running in separate threads. If GPU devices\nare available, SINGA automatically assigns g GPU devices (g is user speciﬁed) to the ﬁrst g workers\non each node. A GPU worker executes the layer functions on GPU if they are implemented using\nGPU API (e.g., CUDA). Otherwise, the layer functions execute on CPU. SINGA provides several\nlinear algebra functions for users to implement their own layer functions. These linear algebra func-\ntions have both GPU and CPU implementation and they determine the running device of the calling\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:11\nthread automatically. In this way, we keep the implementation transparent to users. Workers and\nservers communicate through message passing. Every process runs the main thread as a stub that\naggregates local messages and forwards them to corresponding (remote) receivers.\n5.2. Training Frameworks\nIn SINGA, worker groups run asynchronously and workers within one group run synchronously.\nUsers can leverage this general design to run both synchronous and asynchronous training frame-\nworks. Speciﬁcally, users control the training framework by conﬁguring the cluster topology, i.e.,\nthe number of worker (resp. server) groups and worker (resp. server) group size. In the following,\nwe will discuss how to realize popular distributed training frameworks in SINGA, including Sand-\nblaster and Downpour from Google’s DistBelief system [Dean et al. 2012], AllReduce from Baidu’s\nDeepImage system [Wu et al. 2015] and distributed Hogwild from Caffe [Jia et al. 2014].\nServer \nWorker \nNode \nGroup \nInter-node  \nCommunication \n(a) Sandblaster. \n(b) AllReduce. \n(c) Downpour. \n(d) Distributed Hogwild. \nFig. 11: Training frameworks in SINGA.\n5.2.1. Synchronous Training. A synchronous framework is realized by conﬁguring the cluster\ntopology with only one worker group and one server group. The training convergence rate is the\nsame as that on a single node.\nFigure 11a shows the Sandblaster framework implemented in SINGA. A single server group is\nconﬁgured to handle requests from workers. A worker operates on its partition of the model, and\nonly communicates with servers handling the related parameters. Figure 11b shows the AllReduce\nframework in SINGA, in which we bind each worker with a server on the same node, so that each\nnode is responsible for maintaining a partition of parameters and collecting updates from all other\nnodes.\nSynchronous training is typically limited to a small or medium size cluster , e.g. fewer than\n100 nodes. When the cluster size is large, the synchronization delay is likely to be larger than the\ncomputation time. Consequently, the training cannot scale well.\n5.2.2. Asynchronous Training. An asynchronous framework is implemented by conﬁguring the\ncluster topology with more than one worker groups. The training convergence is likely to be different\nfrom single-node training, because multiple worker groups are working on different versions of the\nparameters [Zhang and Re 2014].\nFigure 11c shows the Downpour [Dean et al. 2012] framework implemented in SINGA. Simi-\nlar to the synchronous Sandblaster, all workers send requests to a global server group. We divide\nworkers into several groups, each running independently and working on parameters from the last\nupdate response. Figure 11d shows the distributed Hogwild framework, in which each node contains\na complete server group and a complete worker group. Parameter updates are done locally, so that\ncommunication cost during each training step is minimized. However, the server group must peri-\nodically synchronize with neighboring groups to improve the training convergence. The topology\n(connections) of server groups can be customized (the default topology is all-to-all connection).\nAsynchronous training can improve the convergence rate to some degree. But the improvement\ntypically diminishes when there are more model replicas. A more scalable training framework\nshould combine both the synchronous and asynchronous training. In SINGA, users can run a hybrid\ntraining framework by launching multiple worker groups that run asynchronously to improve the\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:12\nW. Wang et al.\nconvergence rate. Within each worker group, multiple workers run synchronously to accelerate one\ntraining iteration. Given a ﬁxed budget (e.g., number of nodes in a cluster), there are opportunities\nto ﬁnd one optimal hybrid training framework that trades off between the convergence rate and\nefﬁciency in order to achieve the minimal training time.\n5.3. Neural Network Partitioning\nIn this section, we describe how SINGA partitions the neural net to support data parallelism, model\nparallelism, and hybrid parallelism within one worker group.\nsub-layer 0 \nsub-layer 1 \nsub-layer 2 \nW[:,0:3] \nb[0:3] \nW[:,0] b[0] \nW[:,1] b[1] \nW[:,2] b[2] \nFig. 12: Partition the hidden layer in Figure 4(a).\nSINGA partitions a neural net at the granularity of layer. Every layer’s feature blob is considered\na matrix whose rows are feature vectors. Thus, the layer can be split on two dimensions. Partitioning\non dimension 0 (also called batch dimension) slices the feature matrix by row. For instance, if the\nmini-batch size is 256 and the layer is partitioned into 2 sub-layers, each sub-layer would have 128\nfeature vectors in its feature blob. Partitioning on this dimension has no effect on the parameters, as\nevery Param object is replicated in the sub-layers. Partitioning on dimension 1 (also called feature\ndimension) slices the feature matrix by column. For example, suppose the original feature vector has\n50 units, after partitioning into 2 sub-layers, each sub-layer would have 25 units. This partitioning\nsplits Param objects, as shown in Figure 12. Both the bias vector and weight matrix are partitioned\ninto two sub-layers (workers).\nNetwork partitioning is conducted while creating the NeuralNet instance. SINGA extends a layer\ninto multiple sub-layers. Each sub-layer is assigned a location ID, based on which it is dispatched to\nthe corresponding worker. Advanced users can also directly specify the location ID for each layer to\ncontrol the placement of layers onto workers. For the MDNN model in Figure 7, users can conﬁgure\nthe layers in the image path with location ID 0 and the layers in the text path with location ID 1,\nmaking the two paths run in parallel. Similarly, for the Char-RNN model shown in Figure 9, we\ncan place the layers of different colors onto different workers. Connection layers will be automati-\ncally added to connect the sub-layers. For instance, if two connected sub-layers are located at two\ndifferent workers, then a pair of bridge layers is inserted to transfer the feature (and gradient) blob\nbetween them. When two layers are partitioned on different dimensions, a concatenation layer which\nconcatenates feature rows (or columns) and a slice layer which slices feature rows (or columns) are\ninserted. Connection layers help make the network communication and synchronization transparent\nto the users.\nWhen every worker computes the gradients of the entire model parameters, we refer to this pro-\ncess as data parallelism. When different workers compute the gradients of different parameters, we\ncall this process model parallelism. In particular, partitioning on dimension 0 of each layer results in\ndata parallelism, while partitioning on dimension 1 results in model parallelism. Moreover, SINGA\nsupports hybrid parallelism wherein some workers compute the gradients of the same subset of\nmodel parameters while other workers compute on different model parameters. For example, to\nimplement the hybrid parallelism in [Krizhevsky 2014] for the CNN model, we set partition dim\n= 0 for lower layers and partition dim = 1 for higher layers. The following list summarizes the\npartitioning strategies, their trade-off is analyzed in Section 5.4.\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:13\n(1) Partitioning all layers into different subsets →model parallelism.\n(2) Partitioning each singe layer into sub-layers on batch dimension →data parallelism.\n(3) Partitioning each singe layer into sub-layers on feature dimension →model parallelism.\n(4) Hybrid partitioning of strategy 1, 2 and 3 →hybrid parallelism.\n5.4. Optimization\nDistributed training (i.e, partitioning the neural net and running workers over different layer parti-\ntions) increases the computation power, i.e., FLOPS. However, it introduces overhead in terms of\ncommunication and synchronization. Suppose we have a homogeneous computation environment,\nthat is, all workers run at the same speed and get the same workload (e.g., same number of training\nsamples and same size of feature vectors). In this case, we can ignore the synchronization overhead\nand analyze only the communication cost. The communication cost is mainly attributed to the data\ntransferred through PCIe over multiple GPUs in a single node, or through the network in a cluster.\nTo cut down the overall overhead, ﬁrst we try to reduce the amount of data to be transferred. Further\nmore, we try to parallelize the computation and communication, in order to mask the communica-\ntion time. Here we discuss synchronous training only (i.e., a single worker group), which has the\nidentical theoretical convergence as training in a single worker. Optimization techniques that may\naffect convergence rate of SGD are not considered, e.g., asynchronous SGD (i.e., multiple worker\ngroups) and parameter compression [Seide et al. 2014]. The following analysis works for training\neither over multiple CPU nodes or over multiple GPU cards on a single node.\n5.4.1. Reducing Data Transferring. Corresponding to the three basic partitioning strategies, there\nare three sources of communication overhead. The ﬁrst partitioning strategy results in data being\ntransferred along the boundary layers between workers. To reduce the overhead, we can select the\nboundary layers at “low trafﬁc” positions. In other words, layers with smaller feature dimensions\nare preferred, as they pass less data to the destination layer. The second partitioning strategy, i.e.,\ndata parallelism, replicates the parameters for each layer, hence their gradients are transferred to\na central parameter server for aggregation, and the new parameter values are broadcast back for\nthe next iteration. To reduce the overhead, we can apply data parallelism on layers with fewer\nparameters. The third partitioning strategy, i.e., model parallelism, slices the feature vector into sub-\nvectors, hence some layers (e.g., the fully connected layer whose neuron depends on all neurons of\nits source layer) need to fetch sub-vectors of source layers from other workers to compute its own\nfeature vector. To address this, we can apply model parallelism only for those layers whose neuron\ndependency is element-wise or with small feature dimension. For the last partitioning strategy, i.e.,\nhybrid partitioning, we can compare the overall overhead of different combinations of the basic\npartitioning strategies and select the combination that incurs minimal overhead. To illustrate, we\nuse the popular benchmark model, i.e., AlexNet, as an example. AlexNet is a feed-forward model\nwith single path, the ith layer depends on (i −1)th layer directly. It is not feasible to parallelize\nsubsets of layers as in MDNN, therefore we do not consider the ﬁrst partitioning strategy. Next, we\ndiscuss every type of layer involved in AlexNet one by one.\nConvolution layers contain 5% of the total parameters but 90-95% of the computation, according\nto AlexNet [Krizhevsky 2014]. It is essential to distribute the computation from these layers. Con-\nsidering that convolution layers have large feature dimensions, it is natural to apply data parallelism.\nFully connected layers occupy 95% of the total parameters and 5-10% of computa-\ntion [Krizhevsky 2014], therefore we avoid data parallelism for them. Particularly, with data paral-\nlelism, the communication overhead per worker is p, where p is the size of the (replicated) parame-\nters. Let b be the effective mini-batch size (summed over all workers), K be the number of workers,\nand dv (resp. dh) be the length of the visible (resp. hidden) feature vector. Using model parallelism,\nthe communication overhead per worker is b ∗dv for Figure 13(b), including sending its own data\nto other workers, i.e., b ∗dv/K, and receiving data from other workers, i.e., b ∗(K −1) ∗dv/K.\nFor the case in Figure 13(c), the overhead is b ∗dh, where each worker computes the partial feature\nof the hidden layer using sub-vectors of the visible layer and concatenates them to get the complete\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:14\nW. Wang et al.\n(a) Two fully connected layers.\nworker 1 \nworker 2 \n(b) Partition on hidden layer.\nworker 1 \nworker 2 \n(c) Partition on visible layer.\nFig. 13: Distributed computing for fully connected layers.\nfeature. To compare the two strategies, data parallelism is costlier than model parallelism when\np > b ∗dv or p > b ∗dh. For the ﬁrst fully connected layer in AlexNet, p is about 177 million\nwhile dv = dh = 4096. In practice, there are fewer than K = 8 GPU cards on a single node\n(i.e., K <= 8), and each worker runs with fewer than 128 samples per mini-batch, thus data paral-\nlelism is much costlier than model parallelism. Another approach is no-partitioning for these fully\nconnected layers. The overhead, in this case, comes from transferring features with other workers,\nwhich is b ∗(K −1) ∗dv/K. However, the computation power is reduced to 1/K as the workload\nof K workers is conducted by a single worker.\nFor pooling layers and local responsive normalization layers, each neuron depends on many neu-\nrons from their source layers. Moreover, they are inter-leaved with convolution layers, thus it is\ncheaper to apply data parallelism than model parallelism for them. For the remaining layers, they\ndo not have parameters and their neurons depend on source neurons element-wise, hence their par-\ntitioning strategies just need to be consistent with their source layers. Consequently, a simple hybrid\npartitioning strategy for AlexNet [Krizhevsky 2014] can be to apply data parallelism for layers be-\nfore (or under) the ﬁrst fully connected layer, and then apply model parallelism or no parallelism\nfor all other layers. The above hybrid partitioning can be easily conﬁgured in SINGA by setting the\npartition dim of each layer correspondingly.\n5.4.2. Overlapping Computation and Communication. Overlapping the computation and commu-\nnication is another common technique for system optimization. In SINGA, the communication com-\nprises transferring parameter gradients and values, and transferring layer data and gradients. First,\nfor parameter gradients/values, we can send them asynchronously while computing other layers.\nTake Figure 4 as an example, after the hidden layer ﬁnishes ComputeFeature, we can send the gra-\ndients asynchronously to the server for updates while the worker continues to load data for the next\niteration. Second, the transferring of layer data/gradients typically comes from model partitioning as\ndiscussed in Section 5.4.1. In this case, each worker owns a small subset of data and fetches all rest\nfrom other workers. To overlap the computation and communication, each worker can just initiate\nthe communication and then compute over its own data asynchronously. Take the Figure 13(b) as\nan example, to parallelize the computation and communication, SINGA runs over the layers shown\nin Figure 14 in order. The BridgeSrcLayer::ComptueFeature initiates the sending operations and\nreturns immediately. The BridgeDestLyer::ComputeFeature waits until data arrives (by checking\na signal for the ending of data transferring). All layers are sorted in topology order followed by\ncommunication priority.\nFor training with multiple GPUs on a single node we need another mechanism to overlap data\ntransfer and computation. The data transfer from CPU to GPU can only be processed by the worker\nitself. Consequently, as in Figure 14, the BridgeDestLayer has to copy the data from CPU to GPU it-\nself instead of relying on other threads to do it. In order to execute the copy operation synchronously,\nwe add a data copy queue for each GPU worker. This queue is checked frequently (e.g., before vis-\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:15\nv[0:b/2] \nh[0:b/2]  \nInnerProductLayer \nh[b/2:b] \nh[0:b] \nv[b/2:b] \nConcateLayer \nBridgeDestLayer \nBridgeSrcLayer \nInnerProductLayer \n1 \n2 \n3 \n4 \n5 \n1.5 \nCopy Queue \nFig. 14: Parallelize computation and communication for a GPU worker.\niting each layer) to initiate the copy operation (from CPU to GPU) asynchronously. A copy event is\npushed into the queue by other threads (e.g., stub or other worker). A callback function is associ-\nated with each copy event to signal the end of the copy operation, i.e., data transferring. If the copy\nevent for the BridgeDestLayer is initiated after step 1 (say at step 1.5), it could be done in parallel\nwith step 2. Later, when the worker visits BrdigeDestLayer (i.e., step 3), the copy event could have\nalready ﬁnished. The transferring of parameter values/gradients is processed in the same way. Each\nworker initiates asynchronous sending operations to servers immediately after it gets the gradients.\nAfter updating, the servers enqueue the event for copying the fresh parameter values back to the\nworkers. Then the workers can parallelize the copy operations and computation for other layers.\nDepending on the TrainOneBatch algorithm, we may assign a different priority for each copy event.\nFor example, for the BP algorithm, the fresh parameters of the bottom layers may have higher prior-\nity because the bottom layers will be visited earlier than other layers in the next iteration. Otherwise,\nthe computation of the bottom layers would be blocking while it waits for the fresh parameter.\n6. EXPERIMENTAL STUDY\nWe evaluated SINGA with real-life multimedia applications. Speciﬁcally, we used SINGA to train\nthe models discussed in Section 4.2, which required little development effort since SINGA comes\nwith many built-in layers and algorithms. We then measured SINGA’s training performance in terms\nof efﬁciency and scalability when running on CPUs and GPUs. We found that SINGA is more\nefﬁcient than other open-source systems, and it is scalable for both synchronous and asynchronous\ntraining.\n6.1. Applications of SINGA\nWe trained models for the example applications in Section 4.2 using SINGA. Users can train these\nmodels following the instructions on-line5.The neural nets are conﬁgured using the built-in layers\nas shown in Figure 7, 8, 9.\nMulti-modal Retrieval. We trained the MDNN model for multi-modal retrieval application. We\nused NUS-WIDE dataset [Chua et al. 2009], which has roughly 180,000 images after removing\nimages without tags or from non-popular categories. Each image is associated with several tags. We\nused Word2Vec [Mikolov et al. 2013] to learn a word embedding for each tag and aggregated the\nembedding of all the tags from the same image as a text feature. Figure 15 shows sample search\nresults. We ﬁrst used images as queries to retrieve similar images and text documents. It can be\nseen that image results are more relevant to the queries. For instance, the ﬁrst image result of the\nﬁrst query is relevant because both images are about architecture, but the text results are not very\nrelevant. This can be attributed to the large semantic gap between different modalities, making it\ndifﬁcult to locate semantically relevant objects in the latent (representation) space.\n5http://singa.apache.org/docs/examples.html\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:16\nW. Wang et al.\nnature canada digital quebec \nnature canada digital wild quebec \nnature canada scenery waterfalls \nnature explore food culture  \nwalking castle belgium raw \n \nlowers garden home lawn \ncow nederland cows \nflowers plants \nflowers park love quality photographer \nflower flowers orchid \nwater colors love heart joy \nsepia golden contrast awesome exposure  \ndigital great dream photographers \nblue color needles \nblue sea portrait mountain cold moon \nwater beach sun  \npink lake boat \ncloud evening \nshore \nsky blue bravo reflection ocean rainbow \nsky water ocean boat rocks reflections  \nblue water bravo explore white italy  \nrain boats canal rainy \nclouds green sea bay cliff needles \n \nsky architecture \nquebec montreal \n \n \ncity river lights new lines barge \nsky night art eos moon stars wales \nsky night lights bazaar \nsky sunset building office skyscraper  \ncity river old boat house ship thailand \nwhite house home  \ninterior modern  \nfrench furniture \ntraditional \nlight portrait window warehouse \nold man walking mosque \nred germany decay berlin dress court \nlight dark shoes clothes shirt clothing \nred car window field head hawk \n \nQuery \nText Results \nImage Results \nFig. 15: Multi-Modal Retrieval. Top 5 similar text documents (one line per document) and images\nare displayed.\nDimensionality Reduction. We trained RBM models to initialize the deep auto-encoder for di-\nmensionality reduction. We used the MNIST6 dataset consisting of 70,000 images of hand-written\ndigits. Following the conﬁguration used in [Hinton and Salakhutdinov 2006], we set the size of each\nlayer as 784→1000→500→250→2. Figure 16(a) visualizes sample columns of the weight matrix\nof the bottom (ﬁrst) RBM. We can see that Gabor-like ﬁlters are learned. Figure 16(b) depicts the\nfeatures extracted from the top-layer of the auto-encoder, wherein one point represents one image.\nDifferent colors represent different digits. We can see that most images are well clustered according\nto the ground truth, except for images of digit ’4’ and ’9’ (central part) which have some overlap (in\npractice, handwritten ’4’ and ’9’ digits are fairly similar in shape).\nChar-RNN We used the Linux kernel source code extracted using an online script7 for this ap-\nplication. The dataset is about 6 MB. The RNN model is conﬁgured similar to Figure 9. Since this\ndataset is small, we used one stack of recurrent layers (Figure 9 has two stacks). The training loss\nand accuracy is shown in Figure 17(b). We can see that the Char-RNN model can be trained to\npredict the next character given previous characters in the source code more and more accurately.\n6.2. Training Performance Evaluation on CPU\nWe evaluated SINGA’s training efﬁciency and scalability for both synchronous and asynchronous\nframeworks on a single multi-core node, and on a cluster of commodity servers.\n6.2.1. Methodologies. The deep convolution neural network8 for image classiﬁcation was used as\nthe training model for benchmarking. The training was conducted over the CIFAR10 dataset9 which\nhas 50,000 training images and 10,000 test images.\n6http://yann.lecun.com/exdb/mnist/\n7http://cs.stanford.edu/people/karpathy/char-rnn\n8https://code.google.com/p/cuda-convnet/\n9http://www.cs.toronto.edu/ kriz/cifar.html\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:17\n(a) Bottom RBM weight matrix.\n(b) Top layer features.\nFig. 16: Visualization of the weight matrix in the bottom RBM and top layer features in the deep\nauto-encoder.\n0\n20000\n40000\n60000\n80000\n100000\nIterations\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAccuracy\n(a) Training accuracy.\n0\n20000\n40000\n60000\n80000\n100000\nIterations\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nLoss\n(b) Training loss.\nFig. 17: Training accuracy and loss of Char-RNN.\nFor the single-node setting, we used a 24-core server with 500GB memory. The 24 cores are\ndistributed into 4 NUMA nodes (Intel Xeon 7540). Hyper-threading is turned on. For the multi-\nnode setting, we used a 32-node cluster. Each cluster node is equipped with a quad-core Intel Xeon\n3.1 GHz CPU and 8GB memory. The cluster nodes are connected by a 1Gbps switch.\nSINGA uses Mshadow10 and OpenBlas11 to accelerate linear algebra operations (e.g., matrix\nmultiplication). Caffe’s im2col and pooling code [Jia et al. 2014] is adopted to accelerate the con-\nvolution and pooling operations. We compiled SINGA using GCC with optimization level O2.\n6.2.2. Synchronous training. We compared SINGA with CXXNET12 and Caffe [Jia et al. 2014].\nAll three systems use OpenBlas to accelerate matrix multiplications. Both CXXNET and Caffe\nwere compiled with their default optimization levels: O3 for the former and O2 for the latter. We\nobserved that because synchronous training has the same convergence rate as that of sequential\nSGD, all systems would converge after same number of iterations (i.e., mini-batches). This means\nthe difference in total training time among these systems is attributed to the efﬁciency of a single\niteration. Therefore, we only compared the training time for one iteration. We ran 100 iterations for\neach system and averaged the result time over 50 iterations: 30th to 80th iteration, in order to to avoid\nthe effect of starting and ending phases.\n10https://github.com/dmlc/mshadow\n11http://www.openblas.net/\n12https://github.com/dmlc/cxxnet\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:18\nW. Wang et al.\n20\n21\n22\n23\n24\n25\nnumber of cores/workers\n0\n1\n2\n3\n4\n5\n6\n7\n8\ntime per iteration (seconds)\nCXXNET\nCaffe\nSINGA\nSINGA-dist\n(a) On the single node.\n4\n8\n16\n32\n64\n128\nnumber of workers\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ntime per iteration (seconds)\nPetuum\nSINGA\n(b) On the 32-node cluster.\nFig. 18: Synchronous training.\n0\n5000\n10000\n15000\n20000\ntraining time (seconds)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ntest accuracy\n1 worker\n2 workers\n4 workers\n8 workers\n16 workers\n32 workers\n(a) Caffe on the single node.\n0\n5000\n10000\n15000\ntraining time (seconds)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ntest accuracy\n1 worker\n2 workers\n4 workers\n8 workers\n16 workers\n32 workers\n(b) SINGA on the single node.\n0\n1000\n2000\n3000\n4000\n5000\ntraining time (seconds)\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\ntest accuracy\n1 worker per group\n2 workers per group\n4 workers per group\n(c) SINGA on the 32-node cluster.\nFig. 19: Asynchronous training.\nOn the 24-core single node, we used 256 images per mini-batch and varied the number of Open-\nBlas’s threads. The result is shown in Figure 18(a). SINGA-dist represents the SINGA conﬁguration\nin which there are multiple workers, each worker has 1 OpenBlas thread13. In contrast, SINGA\nrepresents the conﬁguration which has only 1 worker. We conﬁgured SINGA-dist with the cluster\ntopology consisting of one server group with four servers and one worker group with varying num-\nber of worker threads (Figure 18(a)). In other words, SINGA-dist ran as the in-memory Sandblaster\nframework. We can see that SINGA-dist has the best overall performance: it is the fastest for each\nnumber of threads, and it is also the most scalable. Other systems using multi-threaded OpenBlas\nscale poorly. This is because OpenBlas has little awareness of the application, and hence it cannot\nbe fully optimized. For example, it may only parallelize speciﬁc operations such as large matrix\nmultiplications. In contrast, in SINGA-dist partitions the mini-batch equally between workers and\nachieves parallelism at the worker level. Another limitation of OpenBlas, as shown in Figure 18(a),\nis that when there were more than 8 threads, the overheads caused by cross-CPU memory access\nstarted to have negative effect on the overall performance.\nOn the 32-node cluster, we compared SINGA against another distributed machine learning frame-\nwork called Petuum [Dai et al. 2013]. Petuum runs Caffe as an application to train deep learning\nmodels. It implements a parameter server to perform updates from workers (clients), while the\nworkers run synchronously. We used a larger mini-batch size (512 images) and disabled OpenBlas\nmulti-threading. We conﬁgured SINGA’s cluster topology to realize the AllReduce framework: there\nis 1 worker group and 1 server group, and in each node there are 4 workers and 1 server. We varied\n13OPENBLAS NUM THREADS=1\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:19\nthe size of worker group from 4 to 128, and the server group size from 1 to 32. We note that one\ndrawback of synchronous distributed training is that it cannot scale to too many nodes because there\nis typically an upper limit on the mini-batch size (1024 images, for instance). Consequently, there\nis an upper bound on the number of workers we can launch (1024 workers, for instance), otherwise\nsome workers will not be assigned any image to train. Figure 18(b) shows that SINGA achieves\nalmost linear scalability. In contrast, Petuum scales up to 64 workers, but becomes slower when 128\nworkers are launched. It might be attributed to the communication overheads at the parameter server\nand the synchronization delays among workers.\n6.2.3. Asynchronous training. We compared SINGA against Caffe which has support for in-\nmemory asynchronous training. On the single node, we conﬁgured Caffe to use the in-memory\nHogwild [Recht et al. 2011] framework, and SINGA to use the in-memory Downpour framework.\nTheir main difference is that parameter updates are done by workers in Caffe and by a single server\n(thread) in SINGA. Figure 19(a) and Figure 19(b) show the model accuracy versus training time\nwith varying numbers of worker groups (i.e. model replicas). Every worker processed 16 images\nper iteration, for a total of 60,000 iterations. We can see that SINGA trains faster than Caffe. Both\nsystems scale well as the number of workers increases, both in terms of the time to reach the same\naccuracy and of the ﬁnal converged accuracy. We can also observe that the training takes longer\nwith more workers. This is due to the increased overhead in context-switching when there are more\nthreads (workers). Finally, we note from the results that the performance difference becomes smaller\nwhen the cluster size (i.e., the number of model replicas) reaches 16. This implies that there would\nbe little beneﬁt in having too many model replicas. Thus, we ﬁxed the number of model replicas\n(i.e., worker groups) to 32 in the following experiments for the distributed asynchronous training.\nOn the 32-node cluster, we used mini-batch of 16 images per worker group and 60,000 train-\ning iterations. We varied the number of workers within one group, and conﬁgured the distributed\nDownpour framework to have 32 worker groups and 32 servers per server group (one server thread\nper node). We can see from Figure 19(c) that with more workers, the training is faster because\neach worker processes fewer images. However, the training is not as stable as in the single-node\nsetting. This may be caused by the delay of parameter synchronization between workers, which is\nnot present in single-node training because parameter updates are immediately visible on the shared\nmemory. The ﬁnal stage of training (i.e., last few points of each line) is stable because there is only\none worker group running during that time, namely the testing group. We note that using a warm-up\nstage, which trains the model using a single worker group at the beginning, may help to stabilize the\ntraining as reported in Google’s DistBelief system [Dean et al. 2012].\n6.3. Training Performance Evaluation on GPU\nWe evaluated the training performance of SINGA running on GPUs. We ﬁrst analyzed the two\noptimization techniques discussed in Section 5.4, then we compared SINGA with other open source,\nstate-of-the-art systems.\n6.3.1. Methodologies. We used the online benchmark model14 as the training workload. The\nmodel is adapted from the AlexNet [Krizhevsky 2014] model with some layers omitted. We per-\nformed experiments on single node which has one Intel i7 k5820 CPU, 3 GTX 970 GPUs and 16\nGB memory. CUDA v7.0 and cuDNN v3 are used. We measured the performance of synchronous\ntraining with different optimization techniques over 1, 2 and 3 GPU cards, and compared that with\nother state-of-the-art systems with the same conﬁguration.\n6.3.2. Overlapping Communication and Computation. In Section 5.4.2, we analyzed the optimiza-\ntion technique for hiding the communication overhead by overlapping it with the computation. Here\nwe evaluate the effect of this technique. Particularly, we compare the efﬁciency in terms of time\nper iteration for three versions of SINGA. No Copy version indicates that there is no communica-\n14https://github.com/soumith/convnet-benchmarks\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:20\nW. Wang et al.\n64\n128\n256\nMini-batch size\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nTime per iteration (millisecond)\nNo Copy\nAsync Copy\nSync Copy\n(a) Overlapping communication and computation.\n64\n128\n256\nMini-batch size\n0\n50\n100\n150\n200\nTime per iteration (millisecond)\nData parallelism\nHybrid parallelism\n(b) Reducing transferred data.\nFig. 20: Effect of optimization techniques.\ntion between GPU and CPU, which is widely used for training with a single GPU, where all SGD\noperations including parameter update are conducted on the single GPU. The other two versions\nconduct BP algorithm on GPU and parameter updating on CPU, differing only by whether the GPU\nand CPU communicate synchronously or asynchronously.\nFigure 20(a) shows the time per iteration with different mini-batch size. First, we can see that No\nCopy is the fastest one because it has no communication cost at all. Second, Async Copy is faster\nthan Sync Copy, which suggests that the asynchronous data transferring beneﬁts from the overlap-\nping communication and computation. Moreover, we can see that when the mini-batch increases, the\ndifference between Async Copy and Sync Copy decreases. This is because for large mini-batches,\nthe BP algorithm spends more time doing computation, which increases the overlap area of com-\nputation and communication, effectively reducing the overhead. For mini-batch size = 256, Async\nCopy is even faster than No Copy, this is because Async Copy does not do parameter update, which\nis done by the server in parallel with BP. However, No Copy has to do BP and parameter updating\nin sequential.\n6.3.3. Reducing Data Transferring. In Section 5.4.1, we discussed how hybrid partitioning is bet-\nter than other strategies in terms of the overheads in transferring feature vectors between layers in\ndifferent workers. To demonstrate its effectiveness, we ran SINGA using two partitioning strate-\ngies, i.e., data partitioning and hybrid partitioning for the ﬁrst fully connected layer in AlexNet.\nFigure 20(b) shows the time per iteration with different mini-batch sizes. We can see that hybrid\npartitioning has better performance over data partitioning and single GPU training. For data parti-\ntioning, only parameter gradients and values are transferred, which is independent of the mini-batch\nsize, thus the time per iteration does not change much when mini-batchs size increases. For hybrid\npartitioning, when the mini-batch size increases, more feature vectors would be transferred. Hence,\nthe time increases.\n6.3.4. Comparison with Other Systems. We compared SINGA with four other state-of-the-art\ndeep learning systems, namely Torch [Collobert et al. 2011], Caffe [Jia et al. 2014], Tensor-\nFlow [Abadi et al. 2015] and MxNet [Chen et al. 2015]. For fair comparison, we turned off the\nmanual tuning option provided by cuDNN and we use AllreduceCPU for MxNet which aggregates\nthe gradients on CPU like SINGA.\nWe ﬁrst ﬁxed the mini-batch size for each worker to be 96, and compared the ﬁve systems in\nterms of throughput. Next, we ﬁxed the overall mini-batch size to be 288, i.e. the training workload\nper iteration is ﬁxed with each worker having a mini-batch size of 288\nn (where n is the number of\nworkers), and compared the ﬁve systems in terms of efﬁciency. For both sets of experiments, we\nvaried the number of workers from 1 to 3. The results are shown in Figure 21(a) and Figure 21(b).\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:21\n1\n2\n3\nNumber of workers\n0\n200\n400\n600\n800\n1000\nNumber of images per second\nTensorFlow\nCaffe\nTorch\nMxNet\nSINGA\n(a) Mini-batch size per worker = 96.\n1\n2\n3\nNumber of workers\n0\n200\n400\n600\n800\n1000\n1200\nTime per iteration (millisecond)\nTensorFlow\nCaffe\nTorch\nMxNet\nSINGA\n(b) Total mini-batch size =288.\nFig. 21: Performance comparison of open source systems.\nWe can see that SINGA outperforms other systems in both sets of experiments. On a single GPU,\nthe difference with Torch and MxNet is not signiﬁcant because they also use cuDNN for the under-\nlying computation of convolution, pooling etc. layers. The performance of TensorFlow, Caffe and\nTorch is consistent with that from the on-line benchmark site. On multiple GPUs, SINGA has better\nperformance thanks to the optimization techniques introduced in Section 5.4. The performance of\nCaffe decreases when the number of workers is increased from 2 to 3. This could be caused by the\ntree reduction strategy15 which works well for GPU cards with direct transferring capability. How-\never, for our experiment, the GTX 970 cards do not provide such functionality, hence the data has\nto go through the CPU memory which incurs extra overhead when there are more than 2 workers.\n7. CONCLUSION\nIn this paper, we proposed a distributed deep learning platform, called SINGA, for supporting mul-\ntimedia applications. SINGA offers a simple and intuitive programming model, making it accessible\nto even non-experts. SINGA is extensible and able to support a wide range of multimedia applica-\ntions requiring different deep learning models. The ﬂexible training architecture gives the user the\nchance to balance the trade-off between the training efﬁciency and convergence rate. Optimization\ntechniques are applied to improve the training performance. We demonstrated the use of SINGA for\nrepresentative multimedia applications using a CPU cluster and a single node with multiple GPU\ncards, and showed that the platform is both usable and scalable.\nACKNOWLEDGMENTS\nThis work was in part supported by the National Research Foundation, Prime Minister’s Ofﬁce, Singapore under its Com-\npetitive Research Programme (CRP Award No. NRF-CRP8-2011-08) and A*STAR project 1321202073. Gang Chen’s work\nwas supported by National Natural Science Foundation of China (NSFC) Grant No. 61472348. We would like to thank the\nSINGA team members and NetEase for their contributions to the implementation of the Apache SINGA system, and the\nanonymous reviewers for their insightful and constructive comments.\nREFERENCES\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jef-\nfrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing\nJia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man´e, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\nVincent Vanhoucke, Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. (2015).\nhttp://tensorﬂow.org/ Software available from tensorﬂow.org.\n15https://github.com/BVLC/caffe/blob/master/docs/multigpu.md\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nA:22\nW. Wang et al.\nFr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard,\nand Yoshua Bengio. 2012. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature\nLearning NIPS 2012 Workshop. (2012).\nTianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng\nZhang. 2015. MXNet: A Flexible and Efﬁcient Machine Learning Library for Heterogeneous Distributed Systems.\narXiv preprint arXiv:1512.01274 (2015).\nTrishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. 2014. Project Adam: Building an Efﬁ-\ncient and Scalable Deep Learning Training System. In OSDI. USENIX Association, 571–582. https://www.usenix.org/\nconference/osdi14/technical-sessions/presentation/chilimbi\nTat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao. Zheng. July 8-10, 2009. NUS-WIDE: A\nReal-World Web Image Database from National University of Singapore. In CIVR’09.\nDan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, and J¨urgen Schmidhuber. 2010. Deep Big Simple Neural Nets\nExcel on Handwritten Digit Recognition. CoRR abs/1003.0358 (2010).\nAdam Coates, Brody Huval, Tao Wang, David J. Wu, Bryan C. Catanzaro, and Andrew Y. Ng. 2013. Deep learning with\nCOTS HPC systems. In ICML (3). 1337–1345.\nR. Collobert, K. Kavukcuoglu, and C. Farabet. 2011. Torch7: A Matlab-like Environment for Machine Learning. In BigLearn,\nNIPS Workshop.\nWei Dai, Jinliang Wei, Xun Zheng, Jin Kyu Kim, Seunghak Lee, Junming Yin, Qirong Ho, and Eric P. Xing. 2013. Petuum:\nA Framework for Iterative-Convergent Distributed ML. CoRR abs/1312.7651 (2013). http://arxiv.org/abs/1312.7651\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc’Aurelio Ranzato,\nAndrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. 2012. Large Scale Distributed Deep Networks. In\nNIPS. 1232–1240.\nJeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simpliﬁed Data Processing on Large Clusters. In (OSDI 2004), San\nFrancisco, California, USA, December 6-8, 2004. 137–150. http://www.usenix.org/events/osdi04/tech/dean.html\nJohn C. Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Op-\ntimization. Journal of Machine Learning Research 12 (2011), 2121–2159. http://dl.acm.org/citation.cfm?id=2021068\nFangxiang Feng, Xiaojie Wang, and Ruifan Li. 2014. Cross-modal Retrieval with Correspondence Autoencoder. In ACM\nMultimedia. 7–16. DOI:http://dx.doi.org/10.1145/2647868.2654902\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. arXiv\npreprint arXiv:1512.03385 (2015).\nGeoffrey Hinton and Ruslan Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science 313,\n5786 (2006), 504 – 507.\nYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and\nTrevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093\n(2014).\nDawei Jiang, Gang Chen, Beng Chin Ooi, Kian-Lee Tan, and Sai Wu. 2014. epiC: an Extensible and Scalable System for\nProcessing Big Data. PVLDB 7, 7 (2014), 541–552. http://www.vldb.org/pvldb/vol7/p541-jiang.pdf\nAlex Krizhevsky. 2014. One weird trick for parallelizing convolutional neural networks. CoRR abs/1404.5997 (2014).\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classiﬁcation with Deep Convolutional Neural\nNetworks. In NIPS. 1106–1114.\nQuoc V. Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Greg Corrado, Kai Chen, Jeffrey Dean, and Andrew Y.\nNg. 2012. Building high-level features using large scale unsupervised learning. In ICML.\nYann LeCun, L´eon Bottou, Genevieve B. Orr, and Klaus-Robert M¨uller. 1996. Efﬁicient BackProp. In Neural Networks:\nTricks of the Trade. 9–50. DOI:http://dx.doi.org/10.1007/3-540-49430-8 2\nTomas Mikolov, Stefan Kombrink, Luk´as Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2011. Extensions of recurrent\nneural network language model. In ICASSP. IEEE, 5528–5531. DOI:http://dx.doi.org/10.1109/ICASSP.2011.5947611\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations of\nWords and Phrases and their Compositionality. In NIPS. 3111–3119.\nBeng Chin Ooi, Kian-Lee Tan, Sheng Wang, Wei Wang, Qingchao Cai, Gang Chen, Jinyang Gao, Zhaojing Luo, Anthony\nK. H. Tung, Yuan Wang, Zhongle Xie, Meihui Zhang, and Kaiping Zheng. 2015. SINGA: A Distributed Deep Learning\nPlatform. In ACM Multimedia.\nThomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, and Thomas S. Huang. 2013. GPU Asynchronous Stochastic Gradient\nDescent to Speed Up Neural Network Training. CoRR abs/1312.6186 (2013).\nBenjamin Recht, Christopher Re, Stephen J. Wright, and Feng Niu. 2011. Hogwild: A Lock-Free Approach to Parallelizing\nStochastic Gradient Descent. In NIPS. 693–701.\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\nDeep Learning At Scale and At Ease\nA:23\nFrank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-bit stochastic gradient descent and its application to\ndata-parallel distributed training of speech DNNs. In INTERSPEECH 2014, 15th Annual Conference of the International\nSpeech Communication Association, Singapore, September 14-18, 2014. 1058–1062.\nHeng Tao Shen, Beng Chin Ooi, and Kian-Lee Tan. 2000. Giving meanings to WWW images. In ACM Multimedia. 39–47.\nKaren Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition.\nCoRR abs/1409.1556 (2014). http://arxiv.org/abs/1409.1556\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Van-\nhoucke, and Andrew Rabinovich. 2014. Going Deeper with Convolutions. CoRR abs/1409.4842 (2014).\nJi Wan, Dayong Wang, Steven Chu Hong Hoi, Pengcheng Wu, Jianke Zhu, Yongdong Zhang, and Jintao Li. 2014. Deep\nLearning for Content-Based Image Retrieval: A Comprehensive Study. In ACM Multimedia. 157–166.\nWei Wang, Gang Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng Chin Ooi, Kian-Lee Tan, and Sheng Wang. 2015. SINGA:\nPutting Deep Learning in the Hands of Multimedia Users. In ACM Multimedia.\nWei Wang, Beng Chin Ooi, Xiaoyan Yang, Dongxiang Zhang, and Yueting Zhuang. 2014. Effective Multi-Modal Retrieval\nbased on Stacked Auto-Encoders. PVLDB 7, 8 (2014), 649–660.\nWei Wang, Xiaoyan Yang, Beng Chin Ooi, Dongxiang Zhang, and Yueting Zhuang. 2015. Effective deep learning-based\nmulti-modal retrieval. The VLDB Journal (2015), 1–23. DOI:http://dx.doi.org/10.1007/s00778-015-0391-4\nXinxi Wang and Ye Wang. 2014. Improving Content-based and Hybrid Music Recommendation using Deep Learning. In\nACM Multimedia. 627–636. DOI:http://dx.doi.org/10.1145/2647868.2654940\nRen Wu, Shengen Yan, Yi Shan, Qingqing Dang, and Gang Sun. 2015. Deep Image: Scaling up Image Recognition. CoRR\nabs/1501.02876 (2015). http://arxiv.org/abs/1501.02876\nZuxuan Wu, Yu-Gang Jiang, Jun Wang, Jian Pu, and Xiangyang Xue. 2014. Exploring Inter-feature and Inter-\nclass Relationships with Deep Neural Networks for Video Classiﬁcation. In ACM Multimedia. 167–176.\nDOI:http://dx.doi.org/10.1145/2647868.2654931\nOmry Yadan, Keith Adams, Yaniv Taigman, and Marc’Aurelio Ranzato. 2013. Multi-GPU Training of ConvNets. CoRR\nabs/1312.5853 (2013).\nQuanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. 2015. Joint Visual-Textual Sentiment Analysis with Deep Neu-\nral Networks. In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, MM ’15, Brisbane,\nAustralia, October 26 - 30, 2015. 1071–1074. DOI:http://dx.doi.org/10.1145/2733373.2806284\nMatei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauly, Michael J. Franklin, Scott\nShenker, and Ion Stoica. 2012. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster\nComputing. In NSDI. 15–28.\nCe Zhang and Christopher Re. 2014. DimmWitted: A Study of Main-Memory Statistical Analytics. PVLDB 7, 12 (2014),\n1283–1294. http://www.vldb.org/pvldb/vol7/p1283-zhang.pdf\nHanwang Zhang, Yang Yang, Huan-Bo Luan, Shuicheng Yang, and Tat-Seng Chua. 2014. Start from Scratch: Towards\nAutomatically Identifying, Modeling, and Naming Visual Attributes. In ACM Multimedia. 187–196.\nACM Journal Name, Vol. V, No. N, Article A, Publication date: January YYYY.\n",
  "categories": [
    "cs.LG",
    "cs.DC"
  ],
  "published": "2016-03-25",
  "updated": "2016-03-25"
}