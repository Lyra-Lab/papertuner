{
  "id": "http://arxiv.org/abs/1604.01252v1",
  "title": "Comparative Deep Learning of Hybrid Representations for Image Recommendations",
  "authors": [
    "Chenyi Lei",
    "Dong Liu",
    "Weiping Li",
    "Zheng-Jun Zha",
    "Houqiang Li"
  ],
  "abstract": "In many image-related tasks, learning expressive and discriminative\nrepresentations of images is essential, and deep learning has been studied for\nautomating the learning of such representations. Some user-centric tasks, such\nas image recommendations, call for effective representations of not only images\nbut also preferences and intents of users over images. Such representations are\ntermed \\emph{hybrid} and addressed via a deep learning approach in this paper.\nWe design a dual-net deep network, in which the two sub-networks map input\nimages and preferences of users into a same latent semantic space, and then the\ndistances between images and users in the latent space are calculated to make\ndecisions. We further propose a comparative deep learning (CDL) method to train\nthe deep network, using a pair of images compared against one user to learn the\npattern of their relative distances. The CDL embraces much more training data\nthan naive deep learning, and thus achieves superior performance than the\nlatter, with no cost of increasing network complexity. Experimental results\nwith real-world data sets for image recommendations have shown the proposed\ndual-net network and CDL greatly outperform other state-of-the-art image\nrecommendation solutions.",
  "text": "Comparative Deep Learning of Hybrid Representations\nfor Image Recommendations\nChenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li\nCAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System,\nUniversity of Science and Technology of China, Hefei 230027, China\nleichy@mail.ustc.edu.cn, {dongeliu,wpli,zhazj,lihq}@ustc.edu.cn\nAbstract\nIn many image-related tasks, learning expressive and\ndiscriminative representations of images is essential, and\ndeep learning has been studied for automating the learn-\ning of such representations. Some user-centric tasks, such\nas image recommendations, call for effective representa-\ntions of not only images but also preferences and intents of\nusers over images. Such representations are termed hybrid\nand addressed via a deep learning approach in this paper.\nWe design a dual-net deep network, in which the two sub-\nnetworks map input images and preferences of users into a\nsame latent semantic space, and then the distances between\nimages and users in the latent space are calculated to make\ndecisions. We further propose a comparative deep learning\n(CDL) method to train the deep network, using a pair of im-\nages compared against one user to learn the pattern of their\nrelative distances. The CDL embraces much more training\ndata than naive deep learning, and thus achieves superior\nperformance than the latter, with no cost of increasing net-\nwork complexity. Experimental results with real-world data\nsets for image recommendations have shown the proposed\ndual-net network and CDL greatly outperform other state-\nof-the-art image recommendation solutions.\n1. Introduction\nWith the increasing abundance of images, ﬁnding out im-\nages that satisfy user needs from a huge collection is more\nand more required, which emphasizes the importance of im-\nage search and image recommendations working as ﬁlters\nfor users. Such tasks are not trivial, however, due to the\ngap in understanding the semantics of images as well as\nthe gap in understanding the intents or preferences of users\nThis work was supported by the National Program on Key Basic Re-\nsearch Projects (973 Program) under Grant 2015CB351800, by the Natural\nScience Foundation of China (NSFC) under Grants 61303149, 61331017,\n61390512, and 61472392, and by the Fundamental Research Funds for the\nCentral Universities under Grants WK2100060011 and WK3490000001.\nover images. Compared to their counterparts for structured\ndata, such as search of text and recommendations of book\nor movie, image search and recommendations raise more\nchallenges since images lack an immediately effective rep-\nresentation.\nHow to represent images both expressively and discrim-\ninatively is of essential importance in many image-related\ntasks including detection, registration, recognition, classi-\nﬁcation, and retrieval. This problem had been extensively\nstudied, and many kinds of hand-crafted features had been\ndesigned and adopted in different tasks [4, 20, 28]. Most of\nprevious work focuses on low-level visual features of im-\nages, but for image search and recommendations, it is often\nnot clear how to represent the intents or preferences of users\nwithin the framework of low-level features.\nOne feasible solution that has been studied is to utilize\nthe users’ information as constraints to reﬁne the image rep-\nresentations, making them consistent with both semantic la-\nbels and user provided hints [19, 23, 33]. For example, Liu\net al. [19] proposes to learn an image distance metric by\ncombining the images’ visual similarity and their “social\nsimilarity,” deﬁned from users’ interests in images that are\nmined from user data in online social networks. Nonethe-\nless, visual content of images and users’ intents/preferences\non images are of two different modalities, simply combin-\ning them may not turn out efﬁcient enough.\nRecently, deep network models have attracted much at-\ntention of researchers in the image processing ﬁeld. One\nsigniﬁcant advantage of deep networks is the automated\nlearning of image representations, which are demonstrated\nto be more effective than hand-crafted features, especially\nin semantic level image understanding [13].\nMoreover,\ndeep networks have achieved great success in processing\nother forms of data such as speech and text [22]. Promis-\ningly, multimodal data, such as images and users’ in-\ntents/preferences, may be efﬁciently handled by a single in-\ntegrated deep network.\nIn this paper, we study a dual-net deep network model\nfor the purpose of making recommendations of images to\narXiv:1604.01252v1  [cs.CV]  5 Apr 2016\nusers. The network consists of two sub-networks, which\nmap an image and the preferences of a user into a same la-\ntent semantic space, respectively. Therefore, the network\nachieves representations of both images and users, termed\nhybrid representations hereafter, and these hybrid represen-\ntations are directly comparable to make decisions of recom-\nmendations.\nMoreover, we propose a comparative deep learning\n(CDL) method to train the designed deep network.\nIn-\nstead of a naive learning, e.g. learning a distance between\na user and an image, the CDL uses two images com-\npared against one user, and learns the relative distances\namong them. Our key idea is depicted in Fig. 1, where\nfor a query user, her historical data used for learning con-\nsist of “positive” images, e.g. her favorites, and “nega-\ntive” images, e.g. her dislikes; the objective of CDL is\nthat the distance between the user and a positive image\nshall be less than the distance between the user and a neg-\native image. Thus, training data for CDL are triplets of\n(user, positive image, negative image) and these data\nare fed into a triple-net deep network consisting of three\nsub-networks, one of which is for user, and the other two\nare for positive and negative images and are actually iden-\ntical, as shown in Fig. 2. Note that after training, we need\nonly two sub-networks for user and image, respectively.\nThe designed dual-net network and CDL method have\nbeen veriﬁed on an image recommendation task with real-\nworld data sets. Experimental results display that the pro-\nposed CDL achieves superior performance than naive learn-\ning, and our proposed solution outperforms other state-of-\nthe-art image recommendation methods signiﬁcantly.\nThe remainder of this paper is organized as follows. Re-\nlated work is discussed in Section 2. Then our proposed\nCDL-based image recommendation solution is described,\nthe objective of CDL is formulated in Section 3, followed\nby detailed description of the deep network model in Sec-\ntion 4, and details of making image recommendations in\nSection 5. Experimental results are reported in Section 6,\nand concluding remarks in Section 7.\n2. Related Work\nWe give brief overview of related work at two aspects:\nlearning of image representations and personalized image\nrecommendations.\n2.1. Learning of Image Representations\nIn view of the limitation of hand-crafted image features\nsuch as those designed in [4, 15, 20, 28], more and more re-\nsearch focuses on designing effective deep learning models\nto extract image representations automatically [17, 21, 31].\nKarpathy et al. [31] proposes a supervised hashing method\nwith deep learning architecture, followed by a stage of si-\nmultaneous learning of hash function and image represen-\nTags: Flowers, Animals, Cat\nFriend List: Alice, Bob\nGroup List: Animal Lover\nPositive Images\nNegative Images\nLatent Space\nUser Preferences\nFigure 1. This ﬁgure depicts the key idea of our proposed compara-\ntive deep learning (CDL). One user’s preferences can be described\nby her frequently used tags as well as her friends’ preferences and\nher joined groups’ preferences. These preferences, together with\nimages, are mapped into a same latent semantic space. In that\nspace, the distance between the user and a “positive” image (e.g.\nfavorite image) shall be less than the distance between the user\nand a “negative” image (e.g. disliked image), which is taken as the\nobjective of CDL.\ntations. Furthermore, it is noticed that middle-layer outputs\nin deep learning models can be seamlessly utilized as image\nrepresentations, though the deep network is not trained for\nthat [13, 32, 34]. For example, Krizhevsky et al. [13] pro-\nposes a deep learning architecture to perform image classi-\nﬁcation, and the outputs of the 7th full-connection layer are\nalso veriﬁed to be kind of robust image representations.\nThe abovementioned work mainly focuses on low-level\nvisual features of images. But recently, along with the de-\nvelopment of user-centric applications such as image rec-\nommendations, it is worthwhile to learn not only visual in-\nformation but also intents or preferences of users for im-\nage representations. A paucity of work has made attempts\nat this aspect [7, 19, 23]. Pan et al. [23] proposes an em-\nbedding method to study the cross-view (i.e. text to im-\nage views) search problem with analyses of user click log.\nLiu et al. [19] consider jointly the users’ social relationship\nand images’ visual similarity to learn a new image distance\nmetric. But such work relies heavily on carefully designed\nhand-crafted features. Liu et al. [18] employ deep learning\narchitecture to capture user intent and image visual infor-\nmation, where user intent is described by only similarity\nbetween a pair of users. But in practice, there is multi-\nmodal information for drawing upon user intents, such as\ntags, browsing history and social groups. Moreover, the\ndeep architecture in [18] considers only one image at each\ntraining round. To the contrary, recent studies [14, 27, 30]\nindicate that deep ranking models perform much better by\nforming training data as triplets. To summary, how to de-\nsign an effective deep learning architecture to capture both\nvisual information and the intents or preferences of users\nover images is still a challenging open problem.\n2.2. Personalized Image Recommendations\nPersonalized recommendations for structured data such\nas book, movie, and music have been studied for a long\nwhile [1]. Typical technologies include content-based ﬁlter-\ning, collaborative ﬁltering, and hybrid of both [24]. How-\never, it is difﬁcult to directly adopt these technologies for\nimage recommendations, possibly due to several difﬁcul-\nties: images are highly unstructured and lack an immedi-\nate representations, user-image interaction data are often\ntoo sparse, users rarely provide ratings on images but rather\ngive implicit feedback. Nevertheless, mature technologies\nin recommender systems are still inspiring, for example,\nmatrix factorization [12] can be perceived as to learn latent\nrepresentations of users and items in a same semantic space.\nWith the development of social networks, recent research\nstarts to leverage social data to improve the performance of\nrecommendations [8, 10]. Most of existing work on im-\nage recommendations also follows this line [3, 5, 11, 16].\nFor example, Jing et al. [11] propose a novel probabilis-\ntic matrix factorization framework that combines the ratings\nof local community users for recommending Flickr photos.\nCui et al. [3] propose a regularized dual-factor regression\nmethod based on matrix factorization to capture the social\nattributes for recommendations. These methods ignore the\nvisual information of images, instead, they focus solely on\nmodeling users by discovering user proﬁles and behavior\npatterns. The representations of images and users are still\nisolated due to semantic gap and the sparsity of user-image\ninteractions.\nOnly a few recent work is concentrated on joint model-\ning of users and images for making recommendations [18,\n19, 25]. Sang et al. [25] propose a topic sensitive model\nthat concerns user preferences and user uploaded images to\nstudy users’ inﬂuences in social networks. Liu et al. [19]\npropose to recommend images by voting strategy accord-\ning to learnt social embedded image representations. Till\nnow, the existing methods often perform separate process-\ning of user information and image and then simply combin-\ning them. A fully integrated solution is to be investigated.\n3.\nProblem\nFormulation\nof\nComparative\nLearning\nWe address the hybrid representations, i.e. simultaneous\nrepresentations of both users and images in a same latent\nspace, via a deep learning approach. For this learning, how\nto prepare training data is not obvious. Given the fact that\nusers rarely provide ratings on images due to the abundance\nof online images, we shall be able to utilize users’ implicit\nfeedback on images. Such feedback, however, is still sparse\nand severely unbalanced, usually negative feedback is al-\nmost none [6]. A naive learning, e.g. learning a distance\nbetween a user and an image, will probably fail due to the\ntraining data.\nMotivated by previous efforts on deep ranking mod-\nels [14, 27, 30], we propose a comparative learning method\nto tackle the imperfect training data. Several symbols are\ndeﬁned as follows. Let an image be I and a user be U, we\nhave deﬁned functions π(I) and φ(U) that map I and U\nto a same latent space, respectively. Another function D is\nused to measure the distance between any two vectors in the\nlearnt latent space. Note that instead of learning a distance\nbetween user and image, we propose to learn comparatively\nthe relative distances between a user and two images. That\nis, the learning algorithm is given a sequence of triplets,\n{Tt = (Ut, I+\nt , I−\nt ), t = 1, 2, ..., T},\n(1)\nwhere T is the total amount of triplets, Ut, I+\nt , I−\nt indicate\nthe triple input elements, i.e. query user Ut prefers image I+\nt\nthan image I−\nt . Then, the learning is to ﬁnd such mapping\nfunctions π(·) and φ(·) and such a distance function D(·, ·),\nto satisfy\nD(π(Ut), φ(I+\nt )) < D(π(Ut), φ(I−\nt )), ∀t.\n(2)\nTo fulﬁll this learning, we may perceive Eq. (2) as a\nbinary classiﬁcation problem (the former distance is less\nor more than the latter), and thus can reuse the 0-1 loss\nfunction, or its better alternatives such as hinge loss func-\ntion.\nHowever, in order to make the distance measure\nmore discriminative (in Eq.\n(2) the difference between\nthe two distances should be as large as possible), we may\nalso adopt cross entropy as loss function. Speciﬁcally, let\noUt\nij = D(π(Ut), φ(i)) −D(π(Ut), φ(j)), and\nP Ut\nij =\neoUt\nij\n1 + eoUt\nij\n,\n(3)\nwe further deﬁne\n¯P Ut\nij =\n(\n0,\n(i = I+\nt , j = I−\nt )\n1,\n(i = I−\nt , j = I+\nt )\n,\n(4)\nthen our learning objective is deﬁned by cross entropy as,\nmin\nπ,φ,DL({Tt}) =\nX\nt\n−¯P Ut\nij log(P Ut\nij ) −(1 −¯P Ut\nij ) log(1 −P Ut\nij ).\n(5)\nIn this paper, we are interested in learning representa-\ntions of users and images and thus we may assume the dis-\ntance function D to be quite simple, for example the Eu-\nclidean. Then, the comparative learning leads to solutions\nConvolution\nConvolution\nConvolution\nConvolution\nConvolution\n11ൈ11:4ൈ4\n5ൈ5:2ൈ2\n3ൈ3:1ൈ1\n3ൈ3:1ൈ1\n3ൈ3:1ൈ1\n5ൈ5:1ൈ1\n3ൈ3:2ൈ2\n3ൈ3:2ൈ2\n57 57 96\n57 57 96\n27\n7 96\n14\n96\n14\n96\n14\n96\n6\n96\n4096\n4096\n1024\n4096\n2048\n1024\n1024\n1024\n1024\n1024\n1024\nCross\nEntropy\nLoss\nFunction\nUser \ninformation\nImage visual \ninformation\nImage visual \ninformation\nMax Pooling\nMax Pooling\nMax Pooling\n14\n96\nConvolution\nConvolution\nConvolution\nConvolution\nConvolution\n11ൈ11:4ൈ4\n5ൈ5:2ൈ2\n3ൈ3:1ൈ1\n3ൈ3:1ൈ1\n3ൈ3:1ൈ1\n5ൈ5:1ൈ1\n3ൈ3:2ൈ2\n3ൈ3:2ൈ2\n57 57 96\n57 57 96\n27\n7 96\n14\n96\n14\n96\n14\n96\n6\n96\n4096\n4096\n1024\nMax Pooling\nMax Pooling\nMax Pooling\n14\n96\nFC1_1\nFC2_1\nFC3_1\nFC_bottom\nFC_top\nDIFF2\nDIFF3\nSquare1\nSquare2\nFC1_2\nFC1_3\nDIFF1\nFC2_2\nFC2_3\nFC2_4\nFC3_2\nFC3_3\nFigure 2. This ﬁgure depicts the deep network used for comparative deep learning (CDL). There are three sub-networks that all output\n1024-dim vectors as representations of images and users, respectively. The top and bottom sub-networks processing images are identical.\nThe middle sub-network is processing users. Following these sub-networks are two distance calculating nets. The difference between\ndistances is fed into the ﬁnal cross-entropy loss function for comparison with label. The numbers shown above each arrow give the size of\nthe corresponding output. The numbers shown above each box indicate the size of kernel and size of stride for the corresponding layer.\nto the mapping functions π(·) and φ(·) that generate rep-\nresentations seamlessly. Traditionally, such learning prob-\nlems were solved by hand-crafted shallow models, but our\ncase raises more difﬁculties, since it is required to learn two\nmapping functions at the same time and the two functions\nare dealing with quite different modalities but shall embed\ninto a same space. We turn to deep learning to solve this\nproblem.\n4. Comparative Deep Learning (CDL)\nAs illustrated in Fig. 2, we design a deep network to\nperform the proposed comparative deep learning (CDL).\nThis network architecture takes triplets as inputs, i.e.\n(Ut, I+\nt , I−\nt ) with a query user Ut having relatively shorter\ndistance from image I+ than from image I−. There are\nthree sub-networks in the CDL architecture. The top and\nbottom sub-networks are two convolutional neural networks\n(CNNs) with identical conﬁguration and shared parameters,\nthey are designed to capture image visual information. The\nmiddle sub-network is a full-connection neural network that\nis designed for user’s information.\nThe two kinds of sub-networks in our architecture cor-\nrespond to mapping functions for image I : π(I) ∈Rd\nand for user U : φ(U) ∈Rd, respectively, where Rd is the\ntarget latent space. The outputs of these sub-networks are\nindeed hybrid representations of images and users (FC1 3,\nFC2 4 and FC3 3 in Fig. 2). To guarantee that the learnt\nfunctions π(·) and φ(·) can embed multimodal information\ninto the same latent space, we add two distance calculating\nnets that outputs two distances (FC top and FC bottom in\nFig. 2), and the difference between distances, i.e. oUt\nij in\nEq. (3), is fed into the ﬁnal cross entropy loss function to\nbe veriﬁed by the label. In the rest of this section, we will\ndescribe each part of the architecture in more details.\nIn the top/bottom sub-network, there are 5 convolutional\nlayers, 3 max-pooling layers and 3 full-connection layers.\nThese conﬁgurations including the sizes of convolution ker-\nnels in the convolution layers and the numbers of neurons\nin the full-connection layers are remarked in Fig. 2. The\narchitecture and settings of this sub-network are inspired\nby AlexNet [13], which achieves great success in modeling\nimage visual information. Input to this sub-network are the\npixel data of RGB channels of an image, and output of this\nsub-network is a 1024-dim vector (FC1 3 and FC3 3).\nThe middle sub-network is designed for capturing user’s\ninformation. Users’ preferences/intents can be described in\nvarious forms and different kinds of data. However, nor-\nmally neural networks accept only numerical vector inputs.\nWe adopt a traditional full-connection network to map an\ninput user vector to the representation, and leave the process\nof converting practical data into user vectors to be deﬁned in\nSection 5. This sub-network also outputs 1024-dim vectors\n(FC2 4) to be comparable with the image representations.\nAfterwards, the deep network performs distance calcula-\ntion. As the focus of this paper is on effective hybrid rep-\nresentations, we assume the distance function shall be quite\nsimple, yet we still design a sub-network for calculating dis-\ntance. It is completed by ﬁrst calculating the element-wise\ndifference vector (DIFF1 and DIFF2 shown in Fig. 2), then\ncalculating the element-wise square (Square1 and Square2\nin Fig. 2), and ﬁnally using a full-connection layer to de-\nrive the distance. A special note is that we adopt the idea of\ndropout (at rate 0.5) to bring in some randomization fac-\ntors to select partial dimensions of the learnt representa-\ntions. The full-connection layer acts as weighting factors\non the different dimensions of squared difference vector,\nand thus the distance calculating sub-network is equivalent\nto weighted l2-norm distance function. Many complicated\ndistance calculating networks can be adopted herein, but we\nleave them for future exploration.\n5. CDL for Image Recommendations\nSince our proposed CDL learns hybrid representations,\nit is well suitable for user-centric image processing tasks.\nIn this paper, we take personalized image recommendation\ntask as an example to discuss on the utility of CDL. We will\nrestrict our discussions to recommending new images to a\nuser based on her browsing history and will not dive into\ndetails of practice. There are two key issues to be solved\nbefore applying the CDL. First, how to preprocess user data\nto generate user vectors as inputs to the deep network. Sec-\nond, how to prepare triplets as training data.\nThere are several intuitive methods to generate user vec-\ntors. A straightforward method is bag-of-words, for exam-\nple, using a vector whose dimension is equal to the amount\nof possible tags, and entries of this vector correspond to the\ninterest levels of this user in these tags. Such interest lev-\nels can be estimated from the user’s browsing history and\ntagging history, and so on. This method faces two chal-\nlenges. First, tags may be too many and accordingly the\nvector may be too sparse. Second, the method cannot deal\nwith synonyms of tags. In this paper, we use the well-known\nword2vector [22] as a remedy for these problems. Tags are\nconverted to vectors1 and then vectors are clustered by k-\nmeans into 1024 semantic clusters. Then, tags are replaced\nby clusters and the bag-of-words method works on these\nclusters. Fig. 3 shows the distribution of clusters, where we\nobserve the clusters have variant frequencies and bear topi-\ncal polymerism to some degree.\n1Actually\nwe\nuse\nGoogle\ntrained\nvectors\ndownloaded\nfrom\nhttps://code.google.com/p/word2vec/.\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\n1\n25\n49\n73\n97\n121\n145\n169\n193\n217\n241\n265\n289\n313\n337\n361\n385\n409\n433\n457\n481\n505\n529\n553\n577\n601\n625\n649\n673\n697\n721\n745\n769\n793\n817\n841\n865\n889\n913\n937\n961\n985\n1009\nThe amount of users\nclusters\nFigure 3. This ﬁgure shows the distribution of clusters. The x-axis\ndisplays 1024 clusters and the y-axis is the number of users having\ninterests in this cluster. A user can be described by bag of words\nwhere words are indeed clusters.\nSince the input is a set of triplets in our proposed CDL, it\nis desirable to generate a set of pairwise images (a positive\nimage and a negative image) for each user. Positive images\nfor users are often handy since users’ behavior data such\nas “add to favorites” and “like” give such information ex-\nplicitly. However, negative images are not obvious [6]. An\nimage is not “liked” by a user dose not necessarily indicate\nthe user is not interested in the image, but rather the user\nnever saw it. We utilize social data to help solve this prob-\nlem. In general, a user has friendship with another usually\nindicates that both users have similar interests, and users\nof the same social group have similar interests also. For a\nspeciﬁc user, we deﬁne the set of potentially liked images\nas her friends’ favorite images and the images “liked” by\nusers in her joined groups. We then assign the images to be\nnegative, which have no tag of the user’s interests and are\nnot belonging to the set of potentially liked images. Due\nto abundance of negative images assigned in this manner,\nrandom sampling can be performed to generate a subset of\ntriplets for training purpose.\nLast but not the least question is how to make recom-\nmendations for users. This is performed in the following\nsteps. First, a set of candidate images are selected, where\neach candidate shall have at least one tag of the user’s inter-\nests. Second, the representations of these candidate images\nas well as of the user are calculated; these representations\ncan be calculated and stored in advance, or can be calcu-\nlated in parallel to accelerate. Third, distances are calcu-\nlated among the images and the user. Finally, K nearest\nneighboring images having minimum distances are chosen\nas recommendations.\n6. Experiments\nIn this section, we report the conducted experiments to\nevaluate the efﬁcacy of the proposed CDL of hybrid rep-\nresentations. We ﬁrst introduce our experimental settings.\nThen, we evaluate the performance of our learnt hybrid rep-\n0\n1\n2\n3\n4\n5\n6\n7\n8\n0\n500\n1000\n1500\n2000\n2500\n3000\nThe amount of users (in logs)\nThe amount of user favorite images\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nThe amount of users (in logs)\nThe amount of user tags\nFigure 4. Distributions of the amounts of user favorite images and\nuser tags. Note the logarithm scale of y-axis.\nresentations in personalized image recommendation task.\nFinally, we give some insights of our proposed approach.\n6.1. Experimental Settings\n6.1.1\nDatasets\nIn this paper, we use the same dataset as reported in [19].\nThe images and users’ information in this dataset are\ncrawled from Flickr through its API. There are 101, 496 im-\nages, 54, 173 users, 6, 439 groups and 35, 844 tags in this\ndataset. The details of crawling can be found in [19]. On\naverage, there are 23.5 tags and 5.8 favorite images for each\nuser. Due to the sparsity of user-image interactions, this\ndataset is not quite suitable for traditional recommendation\nalgorithms, especially collaborative ﬁltering. Therefore, we\ndo not compare our method with them.\nThe distributions of the amounts of user favorite images\nand user tags are shown in Fig. 4. Both of them are typical\nlong-tail distributions. Users having modest favorite images\nand tags usually have most valuable and robust informa-\ntion [26]. Too few favorites indicate inactivity of user, and\ntoo many favorites indicate quite diverse interests of user.\nThus, we ﬁlter out users that have less than 40 or more than\n200 favorite images from test [19]. Furthermore, according\nto statistics shown in Fig. 3, we further ﬁlter out users that\nhave interests in less than 80 or more than 280 clusters from\ntraining data, so as to improve the accuracy of training, but\nkeep them for test. Finally, we have 8, 616 users for training\nand 15, 023 users for test.\nFor each user, 20 images are randomly selected from her\nfavorite images and “concealed” for test. Training data are\nthen generated by randomly sampling the rest favorite im-\nages as well as assigned negative images (c.f. Section 5). 20\ntriplets are sampled for each user for training. Finally, there\nare 72, 161 distinct images in training data. After training,\nthe concealed favorite images are retrieved and mixed with\nother 80 images (for each user) for test.\n6.1.2\nCompared Approaches\nAs discussed in Section 3, our proposed CDL allows the\nchoice of different loss functions and we have used cross en-\ntropy for better performance. We also tested the use of hinge\nloss in replacement of cross entropy. Moreover, we compare\nour method with several state-of-the-art approaches.\nBorda Count with SIDL [2, 19]. Social embedding Im-\nage Distance Learning (SIDL) is a novel image distance\nlearning method that embeds the similarity of collective\nsocial and behavioral information into visual space. Af-\nter learning the social embedding image distance metric, it\ncan be adopted together with Borda Count method [2] to\nperform personalized image recommendations, as detailed\nin [19].\nBorda Count with BoW, ImageNet [13], LMNN [29],\nSocial+LMNN [19]. Bag of Words (BoW) feature is a tra-\nditional hand-crafted visual representation, and ImageNet\nfeature stands for deep learning based representation, both\ncan be used to measure image similarity with e.g. Euclidean\ndistance.\nLarge Margin Nearest Neighbor (LMNN) is a\nmetric learning method to reduce the margins of the near-\nest neighbors. Liu et al. proposes to embed social similar-\nity into LMNN, termed Social+LMNN. We then use BoW,\nImageNet, LMNN, and Social+LMNN with Borda Count\nmethod to evaluate the performance of personalized image\nrecommendations.\nTwoNets. In this paper, we propose the CDL instead\nof naive deep learning to learn hybrid representations. To\ndemonstrate the effectiveness of CDL, we also perform\nthe naive learning experiment called TwoNets.\nSpeciﬁ-\ncally, TwoNets is similar to the CDL but it has only two\nsub-networks, which process user and image, respectively;\nthe output representations of the two sub-networks are di-\nrectly compared to calculate a distance, and the distance\nis re-scaled by a logistic sigmoid function and then com-\npared with the ground-truth by cross entropy loss function;\nnote that in TwoNets, training data consist of doublets of\n(user, image) and the ground-truth is 0 or 1 indicating neg-\native or positive.\n6.1.3\nImplementation\nWe implement the CDL and TwoNets methods based on the\nopen source deep learning software Caffe [9]. In our exper-\niments, all images are resized to 256 × 256. The structure\nand parameters of sub-networks are illustrated in Fig. 2, and\nall probabilities of dropout are set to 0.5 [13]. The learn-\ning rate starts from 0.001 for all layers and the momentum\nis 0.9. The mini-batch size of images is 128. The weight\ndecay parameter is 0.0005. Training was done on a single\nGeForce Tesla K20c GPU with 5GB graphical memory, and\nit took about 4 days to ﬁnish training.\n6.2. Overall Performance\nIn our personalized image recommendation task, the tar-\nget is to recommend 20 images out of 100 candidates for\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nPrecision@K\nK\nBoW\nImageNet Features\nLMNN\nSocial+LMNN\nSIDL\nTwoNets\nCDL+Hinge-Loss\nCDL+Cross-Entropy\nFigure 5. Precision@K for different K values of compared image\nrecommendation methods.\neach user. To make a fair comparison, we implement ev-\nery comparative method to return top K recommended im-\nages where K is adjustable. Precision@K and Recall@K\nare used to evaluate the performance of each method, which\nare shown in Figs. 5 and 6, respectively. It can be seen that\nour approach performs the best in both precision and re-\ncall for all K values, which demonstrates the effectiveness\nof our proposed CDL of hybrid representations. Note that\nusing cross entropy as loss function has obvious advantage\ncompared to using hinge loss function in our image recom-\nmendation task (c.f. Section 3).\nThe approaches based solely on hand-crafted visual rep-\nresentations, i.e. BoW and LMNN, perform poorly in mak-\ning recommendations. The Precision result of BoW is near\nto random guess (random guess for recommending 20 out\nof 100 achieves precision 0.2). ImageNet Features lead to\nmuch better results, almost the third best after our CDL\nmethods and SIDL, which shows the advantage of deep\nlearning based representations.\nIf we add social factors to constrain LMNN (i.e. So-\ncial+LMNN), the performance will be improved a lot, due\nto the utilization of extra information besides visual fea-\ntures. The SIDL performs better than Social+LMNN, in-\ndicating the importance of carefully designed features to\ncapture visual information and embedding functions to in-\ntegrate multimodal information. Compared to SIDL, our\napproach leads signiﬁcant gains of average 42.58% and\n46.50% for precision and recall, respectively. It owes to the\nsuperiority of deep network models over traditional hand-\ncrafted models especially in capturing visual information.\nIt should be noted that TwoNets, also adopting deep\nnetwork model, has very poor performance. It is slightly\nbetter than BoW but the latter is near to random guess.\nThus, deep network models do not guarantee great suc-\ncess especially when the task is complicated (learning hy-\nbrid representations) and the training data are imperfect\n(unreliable negative samples). The proposed CDL outper-\nforms TwoNets signiﬁcantly and consistently, which further\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nRecall@K\nK\nBoW\nImageNet Features\nLMNN\nSocial+LMNN\nSIDL\nTwoNets\nCDL+Hinge-Loss\nCDL+Cross-Entropy\nFigure 6. Recall@K for different K values of compared image rec-\nommendation methods.\n‐0.025\n‐0.015\n‐0.005\n0.005\n0.015\n0.025\n1\n42\n83\n124\n165\n206\n247\n288\n329\n370\n411\n452\n493\n534\n575\n616\n657\n698\n739\n780\n821\n862\n903\n944\n985\nFeatures\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n39\n77\n115\n153\n191\n229\n267\n305\n343\n381\n419\n457\n495\n533\n571\n609\n647\n685\n723\n761\n799\n837\n875\n913\n951\n989\nFeatures\nFigure 8. Exemplar input and output of the user sub-network in our\ndesigned dual-net deep network. Left: pre-processed user vector\n(input). Right: learnt user representation (output).\ndemonstrates the effectiveness of the proposed comparative\nlearning method.\n6.3. Case Study and Insights\nIn this section, we present a case for comprehensive\nstudy to give some insights of our proposed approach. For\nthe selected user whose word cloud of frequent tags can be\nfound in Fig. 7 (Middle), we prepare a set of images for\ntraining, illustrated in Fig. 7 (Left). It can be observed that\npositive images match the user’s preferences as described\nby the word cloud, e.g. portrait, woman and mood. Obvi-\nously, there are large differences between positive images\nand negative images, which veriﬁes the effectiveness of our\ndesigned process for assigning negative images for training\n(c.f. Section 5).\nOur approach’s recommendation results for this user are\nshown in Fig. 7 (Right). Precision@20 is as high as 70%\nin this case. Given a closeup view, most of correct rec-\nommendations made by our approach are portraits with\ndarker tone and gloomy atmosphere, in the similar topics\nand styles of the user’s word cloud and the positive images\nin training data. Interestingly, the 1st image in the 2nd row\nin Fig. 7 (Right) is not belonging to the styles mentioned\nabove. Such images are not easy to be recommended if us-\ning purely tags. But we may compare this image with the\n2nd image in the 2nd row in Fig. 7 (Left), and observe their\nsimilarity in the sense of color, bokeh, and theme. This is\nFigure 7. (Best view in color.) Case study of making recommendations to a selected user. Left: some samples of training images for this\nuser, 10 positive and 10 negative, separated by the red line; unlike positive images that are indeed favorite images of this user, negative\nimages are “assigned” by the process discussed in Section 5. Middle: the word cloud of this user’s frequent tags retrieved from her\ntagging history and browsing history. Right: recommendation results sorted in relevance (ascending order of distance calculated by hybrid\nrepresentations), where correct results are highlighted by red borders.\n‐1.5\n‐1\n‐0.5\n0\n0.5\n1\n1.5\n1\n41\n81\n121\n161\n201\n241\n281\n321\n361\n401\n441\n481\n521\n561\n601\n641\n681\n721\n761\n801\n841\n881\n921\n961\n1001\nFeatures\n‐1.5\n‐1\n‐0.5\n0\n0.5\n1\n1.5\n1\n41\n81\n121\n161\n201\n241\n281\n321\n361\n401\n441\n481\n521\n561\n601\n641\n681\n721\n761\n801\n841\n881\n921\n961\n1001\nFeatures\n‐1.5\n‐1\n‐0.5\n0\n0.5\n1\n1.5\n1\n41\n81\n121\n161\n201\n241\n281\n321\n361\n401\n441\n481\n521\n561\n601\n641\n681\n721\n761\n801\n841\n881\n921\n961\n1001\nFeatures\n‐0.8\n‐0.6\n‐0.4\n‐0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n41\n81\n121\n161\n201\n241\n281\n321\n361\n401\n441\n481\n521\n561\n601\n641\n681\n721\n761\n801\n841\n881\n921\n961\n1001\nFeatures\nFigure 9. Exemplar learnt representations of positive images (top\nrow) and negative images (bottom row). Note the similarity be-\ntween positive images and dissimilarity between positive and neg-\native images, especially in the circled areas.\nwhere image representations help.\nSeveral images in the recommendation results are not\n“correct” according to ground-truth, but we cannot say\nﬁrmly that the user dislikes these images since we do not\nknow whether the user has ever seen them. Especially, the\n2nd image in the 2nd row and the last image in Fig. 7 (Right)\nare probably what user may like. Both images match the\nword cloud and the user’s positive images in the training\ndata. However, the 1st image in the 4th row is probably a\nmistake of recommendation. This photo has darker back-\nground and a human-like object (which is actually a skele-\nton), which interprets its being selected, but per view of the\nuser’s positive images, skeleton may not be his/her favorite.\nIn such cases, making ﬁner discrimination of similar objects\nmay help improve the recommendation accuracy.\nFig. 8 illustrates the input and output of the user sub-\nnetwork learnt by CDL. Fig. 8 (Left) is the input vec-\ntor, indeed a bag-of-words vector spanned over clusters of\nword2vector results, such vector is very sparse, dominated\nby several interests. Fig. 8 (Right) is the learnt user repre-\nsentation, not sparse any more. It shall be noted that such\ndense vectors are due to the following distance calculation\n(weighted l2-norm distance).\nFig. 9 illustrates several examples of learnt image repre-\nsentations, where the top row shows positive images and\nthe bottom row shows negative images.\nIt is interesting\nto ﬁnd that the representations of two positive images are\nquite similar, while they are very different from the rep-\nresentations of two negative images. Some obvious simi-\nlarity and dissimilarity are highlighted by circles in the ﬁg-\nure. However, such information is not easily perceived from\nthe images themselves. Therefore, simultaneous learning of\nhybrid representations is indeed quite different from only\nlearning the representations of images.\n7. Conclusions\nIn this paper, we explore learning of hybrid representa-\ntions to capture both visual information and intents or pref-\nerences of users over images, and utilizing such representa-\ntions for user-centric tasks such as personalized image rec-\nommendations. A dual-net deep network model is proposed\nto learn representations in a latent semantic space. We also\npropose a comparative deep learning method to train the de-\nsigned deep network, in which triplets of users and posi-\ntive/negative images are taken as inputs and the relative dis-\ntances are the objective of learning. The empirical evalua-\ntions on personalized image recommendation task show that\nour proposed approach achieves much better performance\nthan naive deep learning as well as several state-of-the-art\nimage recommendation solutions. The proposed compara-\ntive deep learning can be applied to many other user-centric\napplications, such as image search and image editing. We\nwill further explore along these directions.\nReferences\n[1] G. Adomavicius and A. Tuzhilin. Toward the next generation\nof recommender systems: A survey of the state-of-the-art\nand possible extensions. TKDE, 17(6):734–749, 2005. 3\n[2] J. A. Aslam and M. Montague. Models for metasearch. In\nSIGIR, pages 276–284, 2001. 6\n[3] P. Cui, Z. Wang, and Z. Su.\nWhat videos are similar\nwith you?: Learning a common attributed representation for\nvideo recommendation.\nIn ACM Multimedia, pages 597–\n606, 2014. 3\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In CVPR, pages 886–893, 2005. 1, 2\n[5] J. Fan, D. A. Keim, Y. Gao, and H. Luo. Justclick: Person-\nalized image recommendation via exploratory search from\nlarge-scale ﬂickr images. TCSVT, 19(2):1051–8215, 2009. 3\n[6] Y. Hu, Y. Koren, and C. Volinsky. Collaborative ﬁltering for\nimplicit feedback datasets. In ICDM, pages 263–272, 2008.\n3, 5\n[7] X.-S. Hua, L. Yang, J. Wang, J. Wang, M. Ye, K. Wang,\nY. Rui, and J. Li. Clickage: towards bridging semantic and\nintent gaps via mining click logs of search engines. In ACM\nMultimedia, pages 243–252, 2013. 2\n[8] M. Jamali and M. Ester. Trustwalker: a random walk model\nfor combining trust-based and item-based recommendation.\nIn KDD, pages 397–406, 2009. 3\n[9] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. arXiv:1408.5093,\n2014. 6\n[10] M. Jiang, P. Cui, F. Wang, W. Zhu, and S. Yang. Scalable\nrecommendation with social contextual information. TKDE,\n26(11):2789–2802, 2014. 3\n[11] Y. Jing, X. Zhang, L. Wu, and J. Wang. Recommendation on\nﬂickr by combining community user ratings and item impor-\ntance. In ICME, pages 1–6, 2014. 3\n[12] Y. Koren, R. Bell, and C. Volinsky.\nMatrix factorization\ntechniques for recommender systems. Computer, 48:30–37,\n2009. 3\n[13] A. Krizhevsky, I. Sutskever, and G.Hinton. Imagenet classi-\nﬁcation with deep convolutional neural networks. In NIPS,\npages 1106–1114, 2012. 1, 2, 4, 6\n[14] H. Lai, Y. Pan, Y. Liu, and S. Yan. Simultaneous feature\nlearning and hash coding with deep neural networks.\nIn\nCVPR, pages 3270–3278, 2015. 2, 3\n[15] K. Lenc and A. Vedaldi. Understanding image representa-\ntions by measuring their equivariance and equivalence. In\nCVPR, pages 991–999, 2015. 2\n[16] Y. Li, J. Luo, and T. Mei. Personalized image recommenda-\ntion for web search engine users. In ICME, pages 1–6, 2014.\n3\n[17] T. Lin, Y. Cui, S. Belongie, and J. Hays. Learning deep rep-\nresentations for ground-to-aerial geolocalization. In CVPR,\npages 5007–5015, 2015. 2\n[18] S. Liu, P. Cui, W. Zhu, and S. Yang. Learning socially embe-\nded visual representation from scratch. In ACM Multimedia,\npages 109–118, 2015. 2, 3\n[19] S. Liu, P. Cui, W. Zhu, S. Yang, and Q. Tian. Social embed-\nding image distance learning. In ACM Multimedia, pages\n617–626, 2014. 1, 2, 3, 6\n[20] D. G. Lowe. Object recognition from local scale-invariant\nfeatures. In ICCV, pages 1150–1157, 1999. 1, 2\n[21] A. Mahendran and A. Vedaldi. Understanding deep image\nrepresentations by inverting them. In CVPR, pages 5188–\n5196, 2015. 2\n[22] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nEfﬁcient\nestimation of word representations in vector space. In ICLR\nWorkshop, 2013. 1, 5\n[23] Y. Pan, T. Yao, T. Mei, H. Li, C. W. Ngo, and Y. Rui. Click-\nthrough-based cross-view learning for image search. In SI-\nGIR, pages 717–726, 2014. 1, 2\n[24] M. J. Pazzani. A framework for collaborative, content-based\nand demographic ﬁltering.\nArtiﬁcial Intelligence Review,\n13:393–408, 1999. 3\n[25] J. Sang and C. Xu. Right buddy makes the difference: An\nearly exploration of social relation analysis in multimedia ap-\nplications. In ACM Multimedia, pages 19–28, 2013. 3\n[26] B. Sigurbj¨ornsson and R. Van Zwol. Flickr tag recommen-\ndation based on collective knowledge. In WWW, pages 327–\n336, 2008. 6\n[27] J. Wang, Y. Song, T. Leung, C. Rosenberg, and J. Wang.\nLearning ﬁne-grained image similarity with deep ranking. In\nCVPR, pages 1386–1393, 2014. 2, 3\n[28] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong.\nLocality-constrained linear coding for image classiﬁcation.\nIn CVPR, pages 3360–3367, 2010. 1, 2\n[29] K. Q. Weinberger and L. K. Saul. Distance metric learn-\ning for large margin nearest neighbor classiﬁcation. JMLR,\n10:207–244, 2009. 6\n[30] P. Wu, S. C. Hoi, H. Xia, P. Zhao, D. Wang, and C. Miao. On-\nline multimodal deep similarity learning with application to\nimage retrieval. In ACM Multimedia, pages 153–162, 2013.\n2, 3\n[31] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan. Supervised hash-\ning for image retrieval via image representation learning. In\nAAAI, pages 2156–2162, 2014. 2\n[32] Z. Xu, Y. Yang, and A. G. Hauptmann. A discriminative cnn\nvideo representation for event detection.\nIn CVPR, pages\n1798–1807, 2015. 2\n[33] Z. Yuan, J. Sang, Y. Liu, and C. Xu. Latent feature learning\nin social media network. In ACM Multimedia, pages 253–\n262, 2013. 1\n[34] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In ECCV, pages 818–833, 2014. 2\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2016-04-05",
  "updated": "2016-04-05"
}