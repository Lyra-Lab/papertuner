{
  "id": "http://arxiv.org/abs/1604.03249v1",
  "title": "Attributes as Semantic Units between Natural Language and Visual Recognition",
  "authors": [
    "Marcus Rohrbach"
  ],
  "abstract": "Impressive progress has been made in the fields of computer vision and\nnatural language processing. However, it remains a challenge to find the best\npoint of interaction for these very different modalities. In this chapter we\ndiscuss how attributes allow us to exchange information between the two\nmodalities and in this way lead to an interaction on a semantic level.\nSpecifically we discuss how attributes allow using knowledge mined from\nlanguage resources for recognizing novel visual categories, how we can generate\nsentence description about images and video, how we can ground natural language\nin visual content, and finally, how we can answer natural language questions\nabout images.",
  "text": "Attributes as Semantic Units between\nNatural Language and Visual Recognition\nMarcus Rohrbach\nAbstract Impressive progress has been made in the ﬁelds of computer vision and\nnatural language processing. However, it remains a challenge to ﬁnd the best point\nof interaction for these very different modalities. In this chapter we discuss how\nattributes allow us to exchange information between the two modalities and in this\nway lead to an interaction on a semantic level. Speciﬁcally we discuss how attributes\nallow using knowledge mined from language resources for recognizing novel visual\ncategories, how we can generate sentence description about images and video, how\nwe can ground natural language in visual content, and ﬁnally, how we can answer\nnatural language questions about images.\n1 Introduction\nComputer vision has made impressive progress in recognizing large number of ob-\njects categories [83], diverse activities [92], and most recently also in describing\nimages and videos with natural language sentences [91, 89] and answering natu-\nral language questions about images [48]. Given sufﬁcient training data these ap-\nproaches can achieve impressive performance, sometimes even on par with humans\n[28]. However, humans have two key abilities most computer vision system lack. On\nthe one hand humans can easily generalize to novel categories with no or very little\ntraining data. On the other hand, humans can rely on other modalities, most notably\nlanguage, to incorporate knowledge in the recognition process. To do so humans\nseem to be able to rely on compositionality and transferability, which means they\ncan break up complex problems into components, and use previously learned com-\nponents in other (recognition) tasks. In this chapter we discuss how attributes can\nform such components which allow to transfer and share knowledge, incorporate\nexternal linguistic knowledge, and decompose the challenging problems of visual\nMarcus Rohrbach\nUC Berkley EECS and ICSI, Berkeley, USA\n1\narXiv:1604.03249v1  [cs.CV]  12 Apr 2016\n2\nMarcus Rohrbach\nGeneration\nA mother zebra feeding her \nbaby in front of a third zebra.\nSemantic attributes\nproperties: \nstriped, black, white\nsimilarities: \nsimilar to a horse\nhierarchical info:\na subclass of mammal\nzebra\nRecognition\n(a) Semantic attributes allow recognition\nof novel classes.\n(b) Sentence description for an image.\nImage and caption from MS COCO [8].\nFig. 1 Examples for textual descriptions and visual content.\ndescription and question answering into smaller semantic units, which are easier to\nrecognize and associate with textual representation.\nLet us ﬁrst illustrate this with two examples. Attribute descriptions given in the\nform of hierarchical information (a mammal), properties (striped, black, and white),\nand similarities, (similar to a horse), allow humans to recognize a visual category,\neven if they never observed this category before. Given this description in form of\nattributes most humans would be able to recognize the animal shown in Fig. 1(a)\nas a zebra. Furthermore, once humans know that Fig. 1(a) is a zebra, they can de-\nscribe what it is doing within a natural sentence, even if they never saw example\nimages with captions of zebras before (Fig. 1b). A promising way to handle these\nchallenges is to have compositional models which allow interaction between multi-\nmodal information at a semantic level.\nOne prominent way to model such a semantic level are semantic attributes. As the\nterm “attribute” has a large variety of deﬁnitions in the computer vision literature\nwe deﬁne for the course of this chapter as follows.\nDeﬁnition 1. An attribute is a semantic unit, which has a visual and a textual repre-\nsentation.\nThe ﬁrst part of this deﬁnition, the restriction to a semantic unit is important to\ndiscriminate attributes from other representations, which do not have human inter-\npretable meaning, such as image gradients, bag of (visual) words, or hidden repre-\nsentations in deep neural networks. We will refer to these as features. Of course for\na speciﬁc feature, one can try to ﬁnd or associate it with a semantic meaning or unit,\nbut typically it is unknown and once one is able to identify such a association, one\nhas found a representation for this semantic attribute. The restriction to a semantic\nunit allows to connect to other sources of information on a semantic level, i.e. a level\nof meaning. In the second part of the deﬁnition we restrict it to semantic units which\nAttributes as Semantic Units between Natural Language and Visual Recognition\n3\ncan be both represented textually and visually.1 This this speciﬁc for this chapter as\nwe want to exploit the connection between language and visual recognition. From\nthis deﬁnition it should also be clear that attributes are not distinct from objects,\nbut rather that objects are also attributes, as they obviously are semantic and have a\ntextual and visual representation.\nIn this chapter we discuss some of the most prominent directions where language\nunderstanding and visual recognition interact. Namely how knowledge mined from\nlanguage resources can help visual recognition, how we can ground language in\nvisual content, how we can generate language about visual content, and ﬁnally how\nwe can answer natural language questions about images, which can be seen as a\ncombination of grounding the question, recognition, and generating an answer. It\nis clear that these directions cannot cover all potential interactions between visual\nrecognition and language. Other directions include generating visual content from\nlanguage descriptions [e.g. 102, 45] or localizing images in text i.e. to ﬁnd where\nin a text an image is discussed. In the following we ﬁrst analyze challenges for\ncombining visual and linguistic modalities; afterwards we provide an overview of\nthis chapter which includes a discussion how the different sections relate to each\nother and to the idea of attributes.\n1.1 Challenges for combining visual and linguistic modalities\nOne of the fundamental differences between the visual and the linguistic modality\nis the level of abstraction. The basic data unit of the visual modality is a (photo-\ngraphic) image or video which always shows a speciﬁc instance of a category, or\neven more precisely a certain instance for a speciﬁc viewpoint, lighting, pose, time\netc. For example Fig. 1(a) shows one speciﬁc instance of the category zebra from a\nside view, eating grass. In contrast to this, the basic semantic unit of the linguistic\nmodality are words (which are strings of characters or phonemes for spoken lan-\nguage, but we will restrict ourselves to written linguistic expressions in this chap-\nter). Although a word might refer to a speciﬁc instance, the word, i.e. the string,\nalways represents a category of objects, activities, or attributes, abstracting from a\nspeciﬁc instance. Interestingly this difference, instance versus category level rep-\nresentation, is also what deﬁnes one of the core challenges in visual recognition\nand is also an important topic in computational linguistics. In visual recognition we\nare interested in deﬁning or learning models which abstract over a speciﬁc image\nor video to understand the visual characteristic of a category. In computational lin-\n1 There are attributes / semantic units, which are not visual but textually, e.g. smells, tastes, tactile\nsensory inputs, and ones which are visual but not textual, which are naturally difﬁcult to describe\nin language, but think of many visual patterns beyond striped and dotted, for which we do not\nhave name, or the different visual attributes between two people or faces which humans can clearly\nrecognize but which might be difﬁcult to put into words. We also like to note that some datasets\nsuch as Animals with Attributes [44] include non-visual attributes, e.g. smelly, which might still\nimprove classiﬁcation performance as they are correlated to visual features.\n4\nMarcus Rohrbach\nguistics, when automatically parsing a text, we frequently face the inverse challenge\nof trying to identify intra and extra linguistic references (co-reference resolution /\ngrounding2) of a word or phrase. These problems arise because words typically rep-\nresent concepts rather than instances and because anaphors, synonyms, hypernyms,\nor metaphorical expressions are used to refer to the identical object in the real world.\nUnderstanding that the visual and linguistic modalities have different levels of\nabstraction is important when trying to combine both modalities. In Section 2 we\nuse linguistic knowledge at category rather than instance level for visual knowledge\ntransfer, i.e. we use linguistic knowledge at the level where it is most expressive\nthat is at level of its basic representation. In Section 3, when describing visual input\nwith natural language, we put the point of interaction at a semantic attribute level\nand leave concrete realization of sentences to a language model rather than inferring\nit from the visual representation, i.e. we recognize the most important components\nor attributes of a sentence, which are activities, objects, tools, locations, or scenes\nand then generate a sentence based on these. In Section 4 we look at a model which\ngrounds phrases which refer to a speciﬁc instance by jointly learning visual and\ntextual representations. In Section 5 we answer questions about images by learning\nsmall modules which recognize visual elements which are selected according to the\nquestion and linked to the most important components in the questions, e.g. ques-\ntions words/phrases (How many), nouns, (dog) and qualiﬁers (black. By this com-\nposition in modules or attributes, we create an architecture, which allows learning\nthese attributes, which link visual and textual modality, jointly across all questions\nand images.\n1.2 Overview and outline\nIn this chapter we explain how linguistic knowledge can help to recognize novel\nobject categories and composite activities (Section 2), how attributes help to de-\nscribe videos and images with natural language sentences (Section 3), how to ground\nphrases in images (Section 4), and how compositional computation allows for effec-\ntive question answering about images (Section 5). We conclude with directions for\nfuture work in Section 6.\nAll these directions have in common that attributes form a layer or composi-\ntion which is beneﬁcial for connecting between textual and visual representations.\nIn Section 2, for recognizing novel object categories and composite activities, at-\ntributes form the layer where the transfer happens. Attributes are shared across\nknown and novel categories, while information mined from different language re-\nsources is able to provide the associations between the know categories and at-\ntributes at training time to learn attribute classiﬁers and between the attributes and\nnovel categories at test time to recognize the novel categories.\n2 co-reference is when two or more words refer to the same thing or person within text, while\ngrounding looks at how words refer to things outside text, e.g. images.\nAttributes as Semantic Units between Natural Language and Visual Recognition\n5\nWhen describing images and videos (Section 3), we ﬁrst learn an intermediate\nlayer of attribute classiﬁers, which are then used to generate natural language de-\nscriptions. This intermediate layer allows us to reason across sentences at a semantic\nlevel and in this way to build a model which generates consistent multi-sentence de-\nscription. Furthermore, we discuss how such an attribute classiﬁer layer allows us\nto describe novel categories where no paired image-caption data is available.\nWhen grounding sentences in images, we argue that it makes sense to do this\non a level of phrases are rather full sentences, as phrases form semantic units, or\nattributes, which can be well localized in images. Thus, in Section 4 we discuss how\nwe localize short phrases or referential expressions in images.\nIn Section 5 we discuss the task of visual question answering which connects\nthese previous sections, as one has to ground the question in the image and then\npredict or generate an answer. Here we show how we can decompose the question\ninto attributes which are in this case small neural network components, which are\ncomposed in a computation graph to predict the answer. This allows us to share and\ntrain the attributes across questions and images, but build a neural network which is\nspeciﬁc for a given question.\nThe order of the following sections weakly follows the historic development,\nwhere we start with work which appeared at the time when attributes started to\nbecome popular in computer vision [43, 18]. And the last section on visual question\nanswering, a problem which requires more complex interactions between language\nand visual recognition, has only recently become a topic in the computer vision\ncommunity [48, 4].\n2 Linguistic knowledge for recognition of novel categories\nWhile supervised training is an integral part of building visual, textual, or multi-\nmodal category models, more recently, knowledge transfer between categories has\nbeen recognized as an important ingredient to scale to a large number of categories\nas well as to enable ﬁne-grained categorization. This development reﬂects the psy-\nchological point of view that humans are able to generalize to novel3 categories with\nonly a few training samples [56, 6]. This has recently gained increased interest in\nthe computer vision and machine learning literature, which look at zero-shot recog-\nnition (with no training instances for a class) [44, 17, 58, 59, 22, 53, 21], and one- or\nfew-shot recognition [85, 6, 61]. Knowledge transfer is particularly beneﬁcial when\nscaling to large numbers of classes where training data is limited [53, 21, 70], dis-\ntinguishing ﬁne-grained categories [19, 13], or analyzing compositional activities in\nvideos [22, 72].\nRecognizing categories with no or only few labeled training instances is chal-\nlenging. In this section we ﬁrst discuss how we can build attribute classiﬁers using\n3 We use “novel” throughout this chapter to denote categories with no or few labeled training\ninstances.\n6\nMarcus Rohrbach\nFig. 2 Zero-shot recognition\nwith the Direct Attribute Pre-\ndiction model [43] allows\nrecognizing unseen classes z\nusing an intermediate layer\nof attributes a. Instead of\nmanually deﬁned associations\nbetween classes and attributes\n(cyan lines), Rohrbach et al.\n[69] reduce supervision by\nmining object-attribute as-\nsociation from language re-\nsources, such as Wikipedia,\nWordNet, and image or web\nsearch.\n\t\r  𝑦#\nKnown  \nclasses\nAttribute\nclassifiers\n𝑝𝑎& 𝑥(\n\t\r  𝑎#\t\r  \nspots\n\t\r  𝑎)\t\r  \nwhite\n\t\r  𝑎*\t\r  \nocean\n\t\r  𝑎+\n\t\r  𝑦)\n\t\r  𝑦*\n\t\r  𝑦,\n...\n\t\r  𝑧#\nNovel  classes\n\t\r  𝑧)\n\t\r  𝑧*\n\t\r  𝑧.\n...\n...\nsemantic  relatedness\nfrom  language\nWord\nNet\nonly category-labeled image data and different language resources which allow rec-\nognize novel categories (Section 2.1). And then, to further improve this transfer\nlearning approach, we discuss how to additionally integrate instance similarity and\nlabeled instances of the novel classes if available (Section 2.2). Furthermore we\ndiscuss what changes have to be made to apply similar ideas to composite activity\nrecognition (Section 2.3).\n2.1 Semantic relatedness mined from language resources for\nzero-shot recognition\nLampert et al. [43, 44] propose to use attribute based recognition to allow recog-\nnizing unseen categories based on their object-attribute associations. Their Direct\nAttribute Prediction (DAP) model is visualized in Fig. 2. Given images which are\nlabeled with known category labels y and object-attribute associations ay\nm between\ncategories and attributes, we can learn attribute classiﬁer p(am|xi) for an image xi.\nThis allows to recognize novel categories z if we have associations az\nm.\nTo scale the approach to a larger number of classes and attributes, Rohrbach\net al. [69, 73, 70] show how these previously manual deﬁned attribute associations\nay\nm and az\nm can be replaced with associations mined automatically from different\nlanguage resources. Table 1(a) compares several language resources and measures\nto estimate semantic relatedness to determine if a class should be associated with a\nspeciﬁc attribute. Yahoo Snippets [7, 73], which computes co-occurrence statistics\non summary snippets returned by search engines, shows the best performance of\nall single measures. Rohrbach et al. [73] also discuss several fusion strategies to\nget more robust measures by expanding the attribute inventory with clustering and\ncombining several measures, which can achieve performance on par with manually\ndeﬁned associations (second last versus last line in Table 1a).\nAttributes as Semantic Units between Natural Language and Visual Recognition\n7\nLanguage Resource\nMeasure\nin\nAUC\nWordNet [20], path\nLin measure [46]\n[69]\n60.5\nYahoo Web, hit count [54]\nDice coef. [11, 82]\n[69]\n60.4\nFlickr Img, hit count [69]\nDice coef. [11, 82]\n[69]\n70.1\nYahoo Img, hit count [69]\nDice coef. [11, 82]\n[69]\n71.0\nWikipedia [69]\nESA [23, 98]\n[69]\n69.7\nYahoo Snippets [7]\nDice/Snippets [73]\n[73]\n76.0\nYahoo Img\nExpanded attr.\n[73]\n77.2\nCombination\nClassiﬁer fusion\n[73]\n75.9\nCombination\nExpanded attr.\n[73]\n79.5\nmanual [43]\n[73]\n79.2\nAUC\nimages:\ntest\n+ train cls∗\nObject - Attribute Associations\nYahoo Img\n71.0\n73.2 ( +2.2 )\nClassiﬁer fusion\n79.5\n78.9 ( -0.6 )\nmanual\n79.2\n79.4 ( +0.2 )\nDirect Similarity\nYahoo Img\n79.9\n76.4 ( -2.5 )\nClassiﬁer fusion\n75.9\n72.3 ( -3.6 )\n∗Effect of adding images from\nknown classes in the test set as dis-\ntractors/negatives.\n(a) Attribute-based zero-shot recognition.\n(b) Attributes versus direct-similarity,\nreported in [73].\nTable 1 Zero-shot recognition on AwA dataset [43]. Results for different language resources to\nmine association. Trained on 92 images per class, mean area under the ROC curve (AUC) in %.\nAs an alternative to attributes, Rohrbach et al. [69] also propose to directly trans-\nfer information from most similar classes which does not require and intermediate\nlevel of attributes. While this achieves higher performance when the test set only\ncontains novel objects, in the more adversarial settings, when the test set also con-\ntains images from the known categories, the direct similarity based approach signif-\nicantly drops in performance as can be seen in Table 1(b).\nRohrbach et al. [70] extend zero-shot recognition from the 10 unseen categories\nin the AwA dataset to a setting of 200 unseen ImageNet [9] categories. One of\nthe main challenges in this setting is, that there are no pre-deﬁned attributes on this\ndataset available. Rohrbach et al. propose to mine part-attributes from WordNet [20]\nas ImageNet categories correspond to WordNet synsets. Additionally, as the known\nand unknown classes are leaf nodes of the ImageNet hierarchy, inner nodes can be\nused to group leaf nodes, similar to attributes. Also, the closest known leaf node\ncategories can transfer to the corresponding unseen leaf category.\nAn alternative approach is DeViSE [21] which learns an embedding into a se-\nmantic skip-gram word-space [55], trained on Wikipedia documents. Classiﬁcation\nis achieved by projecting an image in the word-space and taking the closest word as\nlabel. Consequently this also allows for zero-shot recognition.\nTable 2 compares the different approaches. The hierarchical variants [70] per-\nforms best, also compared to DeViSE [21] which relies on more powerful CNN\n[42] features. Further improvements can be achieved by metric learning [53]. As\na different application, Mrowca et al. [57] show how such hierarchical semantic\nknowledge allows to improve large scale object detection not just classiﬁcation.\nWhile the WordNet hierarchy is very reliable as it was manually created, the at-\ntributes are restricted to part attributes and the mining is not as reliably. To improve\nin this challenging setting, we discuss next how one can exploit instance similarity\nand few labeled examples if available.\n8\nMarcus Rohrbach\nApproach/Language resource\nin\nTop-5 Error\nHierarchy\nleaf WordNet nodes\n[69]\n72.8\ninner WordNet nodes\n[69]\n66.7\nall WordNet nodes\n[69]\n65.2\n+ metric learning\n[53]\n64.3∗\nPart Attributes\nWikipedia\n[69]\n80.9\nYahoo Holonyms\n[69]\n77.3\nYahoo Image\n[69]\n81.4\nYahoo Snippets\n[69]\n76.2\nall attributes\n[69]\n70.3\nDirect Similarity\nWikipedia\n[69]\n75.6\nYahoo Web\n[69]\n69.3\nYahoo Image\n[69]\n72.0\nYahoo Snippets\n[69]\n75.5\nall measures\n[69]\n66.6\nLabel embedding\nDeViSe\n[21]\n68.2∗\nTable 2 Large scale zero-shot recognition results. Flat error in % and hierarchical error in brackets.\n∗Note that [53, 21] report on a different set of unseen classes than [69].\nTransferring knowledge from known categories to novel classes is challenging as\nit is difﬁcult to estimate visual properties of the novel classes. Approaches discussed\nin the previous section can not exploit instance similarity or few labeled instances,\nif available. The approach Propagated Semantic Transfer (PST) [74] combines four\nideas to jointly handle the challenging scenario of recognizing novel categories.\nFirst, PST transfers information from known to novel categories by incorporating\nexternal knowledge, such as linguistic or expert-speciﬁed information, e.g., by a\nmid-level layer of semantic attributes as discussed in Section 2.1. Second, PST ex-\nploits the manifold structure of novel classes similar to unsupervised learning ap-\nproaches [94, 80]. More speciﬁcally it adapts the graph-based Label Propagation\nalgorithm [101, 100] – previously used only for semi-supervised learning [14] –\nto zero-shot and few-shot learning. In this transductive setting information is prop-\nagated between instances of the novel classes to get more reliable recognition as\nvisualized with the red graph in Fig. 3. Third, PST improves the local neighborhood\nin such graph structures by replacing the raw feature-based representation with a\nsemantic object- or attribute-based representation. And forth, PST generalizes from\nzero- to few-shot learning by integrating labeled training examples as certain nodes\nin its graph based propagation. Another positive aspect of PST is that attribute or\ncategory models do not have to be retrained if novel classes are added which can be\nan important aspect e.g. in a robotic scenario.\nAttributes as Semantic Units between Natural Language and Visual Recognition\n9\nFig. 3 Recognition of novel\ncategories. The approach\nPropagated Semantic Trans-\nfer [74] combines knowledge\ntransferred via attributes from\nknown classes (left) with few\nlabeled examples in graph\n(red lines) which is build ac-\ncording to instance similarity.\n\t\r  𝑦#\nKnown\t\r  classes\nAttribute\nclassifiers\n𝑝𝑎& 𝑥(\n\t\r  𝑎#\t\r  \nspots\n\t\r  𝑎)\t\r  \nwhite\n\t\r  𝑎*\t\r  \nocean\n\t\r  𝑎+\n\t\r  𝑦)\n\t\r  𝑦*\n\t\r  𝑦,\n...\n\t\r  𝑧#\nNovel\t\r  \nclasses\n\t\r  𝑧)\n\t\r  𝑧*\n\t\r  𝑧.\n...\n...\nWord\nNet\nsemantic\t\r  relatedness\nfrom\t\r  language\n2.2 Propagated semantic transfer\nFig. 4 shows results on the AwA [43] dataset. We note that in contrast to the pre-\nvious section the classiﬁers are trained on all training examples, not only 92 per\nclass. Fig. 4(a) shows zero-shot results, where no training examples are available for\nthe novel or in this case unseen classes. The table compares PST with propagating\non a graph based on attribute-classiﬁer similarity versus image descriptor similar-\nity and shows a clear beneﬁt of the former. This variant also outperform DAP and\nIAP [44] as well as Zero-Shot Learning [22]. Next we compare PST in the few-\nshot setting, i.e. we add labeled examples per class. In Fig. 4(b) we compare PST\nto two label propagation (LP) baselines [14]. We ﬁrst note that PST (red curves)\nseamlessly moves from zero-shot to few-shot, while traditional LP (blue and black\ncurves) needs at least one training example. We ﬁrst examine the three solid lines.\nThe black curve is the best LP variant from Ebert et al. [14] and uses similarity based\nimage features. LP in combination with the similarity metric based on the attribute\nclassiﬁer scores (blue curves) allows to transfer knowledge residing in the classiﬁer\ntrained on the known classes and gives a signiﬁcant improvement in performance.\nPST (red curve) additionally transfers labels from the known classes and improves\nfurther. The dashed lines in Fig. 4(b) provide results for automatically mined asso-\nciations between attributes and classes from language resources. It is interesting to\nnote that these automatically mined associations achieve performance very close to\nthe manual deﬁned associations (dashed vs. solid).\nFig. 5 shows results on the classiﬁcation task with 200 unseen ImageNet cate-\ngories. In Fig. 5(a) we compare PST to zero-shot without propagation presented as\ndiscussed in Section 2.1. For zero-shot recognition PST (red bars) improves perfor-\nmance over zero-shot without propagation (black bars) for all language resources\nand transfer variants. Similar to the AwA dataset, PST also improves over the LP-\nbaseline for few-shot recognition (Fig. 5b). The missing LP-baseline on raw features\nis due to the fact that for the large number of images and high dimensional features\n10\nMarcus Rohrbach\nPerformance\nApproach\nAUC\nAcc.\nDAP [44]\n81.4\n41.4\nIAP [44]\n80.0\n42.2\nZero-Shot Learning [22]\nn/a\n41.3\nPST [74]\non image descriptors\n81.2\n40.5\non attributes\n83.7\n42.7\n0\n10\n20\n30\n40\n50\n30\n35\n40\n45\n50\n# training samples per class\nmean Acc in %\n \n \nPST (ours) − manual def. ass.\nLP + attr. classifiers − manual ass.\nPST (ours) − Yahoo Image attr.\nLP + attr. classifiers − Yahoo Img attr.\nLP (Ebert et al, 2010)\n(a) Zero-Shot, in %.\n(b) Few-Shot\nFig. 4 Zero-shot results on AwA dataset. Predictions with attributes and manual deﬁned associa-\ntions. Adapted from [74].\n0\n10\n20\n30\nHierachy − leaf nodes\nHierachy − inner nodes\nAttributes − Wikipedia\nAttributes − Yahoo Holonyms\nAttributes − Yahoo Image\nAttributes − Yahoo Snippets\nDirect similarity − Wikipedia\nDirect similarity − Yahoo Web\nDirect similarity − Yahoo Image\nDirect similarity − Yahoo Snippets\ntop−5 accuracy (in %)\n \n \nzero−shot w/o propagation\nPST (ours)\n0\n5\n10\n15\n20\n30\n35\n40\n45\n50\n55\n60\n# training samples per class\ntop−5 accurracy in %\n \n \nPST (ours) − Hierachy (inner nodes)\nPST (ours) − Yahoo Img direct\nLP + object classifiers\n(a) Zero-Shot recognition\n(b) Few-Shot recognition\nFig. 5 Results on 200 unseen classes of ImageNet. Adapted from [74].\nthe graph construction is very time and memory consuming if not infeasible. In con-\ntrast, the attribute representation is very compact and thus computational tractable\neven with a large number of images.\n2.3 Composite activity recognition with attributes and script data\nUnderstanding activities in visual and textual data is generally regarded as more\nchallenging than understanding object categories due to the limited training data,\nchallenges in deﬁning the extend of an activity, and the similarities between activ-\nities [62]. However, long-term composite activities can be decomposed in shorter\nﬁne-grained activities [72]. Consider for example the composite cooking activities\nprepare scrambled egg which can be decomposed in attributes of ﬁne-grained ac-\ntivities (e.g. open, fry), ingredients (e.g. egg), and tools (e.g. pan, spatula). These\nattributes can than be shared and transferred across composite activities as visual-\nized in Fig. 6 using the same approaches as for objects and attributes discussed in\nthe previous section. However, the representations, both on the visual and on the\nlanguage side have to change. Fine-grained activities and associated attributes are\nAttributes as Semantic Units between Natural Language and Visual Recognition\n11\nFig. 6 Recognizing compos-\nite activities using attributes\nand script data.\n\t\r  𝑦#\nAttribute\nclassifier\ns\negg\n\t\r  \npan\nopen\n\t\r  𝑎%\n\t\r  𝑦&\n\t\r  𝑦'\n...\n\t\r  𝑧#\n\t\r  𝑧)\n...\n...\nScript  \ndata\nfry\nKnown  training  \ncomposites\nTest  composites\nprepare  onion\nseparate  egg\nprepare  scrambled  egg\n.  .  .\nvisually characterized by ﬁne-grained body motions and low inter-class variability.\nIn addition to holistic features [92], one consequently should exploit human pose-\nbased [71] and hand-centric [77] features. As the previously discussed language\nresources do not provide good associations between composite activities and their\nattributes, Rohrbach et al. [72] collected textual description (Script data) of these\nactivities with AMT. From this script data associations can be computed based on\neither the frequency statistics or, more discriminate, by term frequency times inverse\ndocument frequency (tf∗idf).\nTable 3 shows results on the MPII Cooking 2 dataset [76]. Comparing the ﬁrst\ncolumn (holistic Dense Trajectory features [92]) with the second, shows the bene-\nﬁt of adding the more semantic hand-[77] and pose-[71] features. Comparing line\n(1) with line (2) or (3) shows the beneﬁt of representing composite activities with\nattributes as this allows sharing across composite activities. Best performance is\nachieved with 57.4% mean AP in line (6) when combining compositional attributes\nwith the Propagated Semantic Transfer (PST) approach (see Section 2.2) and Script\ndata to determine associations between composites and attributes.\n3 Image and video description using compositional attributes\nIn this section we discuss how we can generate natural language sentences describ-\ning visual content, rather than just giving labels to images and videos as discussed\nin the previous section. This intriguing task has recently received increased atten-\ntion in computer vision and computational linguistics communities [89, 90, 91] and\nhas a large number of potential applications including human robot interaction, im-\nage and video retrieval, and describing visual content for visually impaired people.\nIn this section we focus on approaches which decouple the visual recognition and\nthe sentence generation and introduce an intermediate semantic layer, which can be\nseen a layer of attributes (Section 3.1). Introducing such a semantic layer has sev-\neral advantages. First, this allows to reason across sentences on a semantic level,\nwhich is, as we will see, beneﬁcial for multi-sentence description of videos (Sec-\n12\nMarcus Rohrbach\nAttribute training on:\nAll\nDisjoint\nComposites\nComposites\nActivity representation:\n[92] [92, 77, 71]\n[92] [92, 77, 71]\nWith training data for composites\nWithout attributes\n(1) SVM\n39.8\n41.1\n-\n-\nAttributes on gt intervals\n(2) SVM\n43.6\n52.3\n32.3\n34.9\nAttributes on automatic segmentation\n(3) SVM\n49.0\n56.9\n35.7\n34.8\n(4) NN\n42.1\n43.3\n24.7\n32.7\n(5) NN+Script data\n35.0\n40.4\n18.0\n21.9\n(6) PST+Script data\n54.5\n57.4\n32.2\n32.5\nNo training data for composites\nAttributes on automatic segmentation\n(7) Script data\n36.7\n29.9\n19.6\n21.9\n(8) PST + Script data\n36.6\n43.8\n21.1\n19.3\nTable 3 Composite cooking activity classiﬁcation on MPII Cooking 2 [76], mean AP in %. Top\nleft quarter: fully supervised, right column: reduced attribute training data, bottom section: no\ncomposite cooking activity training data, right bottom quarter: true zero shot. Adapted from [76].\ntion 3.2). Second, we can show that when learning reliable attributes, this leads to\nstate-of-the-art sentences generation with high diversity in the challenging scenario\nof movie description (Section 3.3). Third, this leads to a compositional structure\nwhich allows describing novel concepts in images and videos (Section 3.4).\n3.1 Translating image and video content to natural language\ndescriptions\nTo address the problem of image and video description, Rohrbach et al. [75] pro-\npose a two-step translation approach which ﬁrst predicts an intermediate semantic\nattribute layer and then learns how to translate from this semantic representation to\nnatural sentences. Figure 7 gives an overview of this two-step approach for videos.\nFirst, a rich semantic representation of the visual content including e.g. object and\nactivity attributes is predicted. To predict the semantic representation a CRF models\nthe relationships between different attributes of the visual input. And second, the\ngeneration of natural language is formulated as a machine translation problem us-\ning the semantic representation as source language and the generated sentences as\ntarget language. For this a parallel corpus of videos, annotated semantic attributes,\nand textual descriptions allows to adapt statistical machine translation (SMT) [39]\nto translate between the two languages. Rohrbach et al. train and evaluate their ap-\nAttributes as Semantic Units between Natural Language and Visual Recognition\n13\nDense \ntrajectories \n(Wang et al., 2013) \nInput video \n(cropped for \ndisplay) \nactivity \ntool \nsource \ntarget \nobject \nAttribute \nclassifier  \nscore vectors \nCRF \nBoW \nHist \nSematic \nRepresentation \n(SR) \nACTIVITY\nTOOL\nOBJECT\nSOURCE\nTARGET\n \nSVM \nmodel dependencies  \nwith CRF \ndecode \nWord and \nphrase-\nalginment \nLanguage \nmodel \nReordering \nmodel \nOptimized with \nMoses \n(Koehn et al., 2007) \ntake  out  hand   knife  drawer \nthe person  gets  out  a knife  from the drawer \ngets  out  the person  a knife  the drawer \nthe person  gets  out  a knife  the drawer \nOutput Description \nconcatenate \nFig. 7 Video description. Overview of the two-step translation approach [75] with an intermediate\nsemantic layer of attributes (SR) for describing videos with natural language. From [68].\nproach on the videos of the MPII Cooking dataset [71, 72] and the aligned descrip-\ntions from the TACoS corpus [62]. According to automatic evaluation and human\njudgments, the two-step translation approach signiﬁcantly outperforms retrieval and\nn-gram-based baseline approaches, motivated by prior work. This similarly can be\napplied to image description task, however, in both cases it requires an annotated\nsemantic attribute representation. In Sections 3.3 and 3.4 we discuss how we can\nextract such attribute annotations automatically from sentences. An alternative ap-\nproach is presented by Fang et al. [16] who mine visual concepts for image descrip-\ntion by integrating multiple instance learning [52]. Similar to the work presented\nin the following, Wu et al. [95] learn an intermediate attribute representation from\nthe image descriptions. Captions are then generated solely from the intermediate\nattribute representation.\n3.2 Coherent multi-sentence video description with variable level\nof detail\nMost approaches for automatic video description, including the one presented\nabove, focus on generating single sentence descriptions and are not able to vary the\ndescriptions’ level of detail. One advantage of the two-step approach with an explicit\nintermediate layer of semantic attributes is that it allows to reason on this semantic\nlevel. To generate coherent multi-sentence descriptions, Rohrbach et al. [64] extend\nthe two-step translation approach to model across-sentence consistency at the se-\nmantic level by enforcing a consistent topic, which is the prepared dish in the cook-\ning scenario. To produce shorter or one-sentence summaries, Rohrbach et al. select\nthe most relevant sentences on the semantic level by using tf∗idf (term frequency\ntimes inverse document frequency). For an example output on the TACoS Multi-\nLevel corpus [64] see Figure 8. In order to fully automatically do multi-sentence\ndescription, Rohrbach et al. propose a simple but effective method based on ag-\nglomerative clustering to perform automatic video segmentation. The most impor-\ntant component of good clustering is the similarity measure and it turns out that\n14\nMarcus Rohrbach\nDetailed:\nA man took a cutting board and knife from the drawer. He took out an orange\nfrom the refrigerator. Then, he took a knife from the drawer. He juiced one half\nof the orange. Next, he opened the refrigerator. He cut the orange with the knife.\nThe man threw away the skin. He got a glass from the cabinet. Then, he poured\nthe juice into the glass. Finally, he placed the orange in the sink.\nShort:\nA man juiced the orange. Next, he cut the orange in half. Finally, he poured the\njuice into a glass.\nOne sentence:\nA man juiced the orange.\nFig. 8 Coherent multi-sentence descriptions at three levels of detail, using automatic temporal\nsegmentation. See Section 3.2 for details. From [64].\nthe semantic attribute classiﬁers (see Fig. 7) are very well suited for that in con-\ntrast to Bag-of-Words dense trajectories [93]. This conﬁrm the observation made in\nSection 2.2 that attribute classiﬁers seem to form a good space for distance compu-\ntations.\nTo improve performance, Donahue et al. [12] show that the second step, the SMT-\nbased sentence generation, can be replaced with a deep recurrent network to better\nmodel visual uncertainty, but still relying on the multi-sentence reasoning on the\nsemantic level. On the TACoS Multi-Level corpus this achieves 28.8% BLEU@4,\ncompared to 26.9% [64] with SMT and 24.9% with SMT without multi-sentence\nreasoning [75].\n3.3 Describing movies with an intermediate layer of attributes\nTwo challenges arise, when extending the idea presented above to movie descrip-\ntion [67], which looks at the problem how to describe movies for blind people. First,\nand maybe more importantly, there are no semantic attributes annotated as on the\nkitchen data, and second, the data is more visually diverse and challenging. For the\nﬁrst challenge, Rohrbach et al. [67] propose to extract attribute labels from the de-\nscription to train visual classiﬁers to build a semantic intermediate layer by relying\non a semantic parsing approach of the description. To additionally accommodate\nthe second challenge of increased visual difﬁculty, Rohrbach et al. [66] show how\nto improve the robustness of these attributes or “Visual Labels” by three steps. First,\nby distinguishing three semantic groups of labels (verbs, objects and scenes) and us-\ning corresponding feature representations for each: activity recognition with dense\ntrajectories [92], object detection with LSDA [31], and scene classiﬁcation with\nAttributes as Semantic Units between Natural Language and Visual Recognition\n15\nSMT [67]\nSomeone is a man, someone is a man.\nS2VT [88]\nSomeone looks at him, someone turns to someone.\nVisual labels [66] Someone is standing in the crowd, a little man with a little smile.\nReference\nSomeone, back in elf guise, is trying to calm the kids.\nSMT [67]\nThe car is a water of the water.\nS2VT [88]\nOn the door, opens the door opens.\nVisual labels [66] The fellowship are in the courtyard.\nReference\nThey cross the quadrangle below and run along the cloister.\nSMT [67]\nSomeone is down the door, someone is a back of the door,\nand someone is a door.\nS2VT [88]\nSomeone shakes his head and looks at someone.\nVisual labels [66] Someone takes a drink and pours it into the water.\nReference\nSomeone grabs a vodka bottle standing open on the counter\nand liberally pours some on the hand.\nFig. 9 Qualitative results on the MPII Movie Description (MPII-MD) dataset [67]. The “Visual\nlabels” approach [66] which uses an intermediate layer of robust attributes, identiﬁes activities,\nobjects, and places better than related work. From [66].\nPlaces-CNN [99]. Second, training each semantic group separately, which removes\nnoisy negatives. And third, selecting only the most reliable classiﬁers. While Rohr-\nbach et al. use SMT for sentence generation in [67], they rely on a recurrent network\n(LSTM) in [66].\nThe Visual Labels approach outperforms prior work [88, 67, 96] on the MPII-\nMD [67] and M-VAD [86] dataset with respect to automatic and human evaluation.\nQualitative results are shown in Fig. 9. An interesting characteristic of the com-\npared methods is the size of the output vocabulary, which is 94 for [67], 86 for\n[88] (which uses an end-to-end LSTM approach without an intermediate semantic\nrepresentation) and 605 for [66]. Although it is far lower than 6,422 for the human\nreference sentences, it clearly shows a higher diversity of the output for [66].\n3.4 Describing novel object categories\nIn this section we discuss how to describe novel object categories which combines\nchallenges discussed for recognizing novel categories (Section 2) and generating\ndescriptions (Section 3.1). State-of-the-art deep image and video captioning ap-\nproaches (e.g. [91, 50, 12, 16, 89]) are limited to describe objects which appear\nin caption corpora such as MS COCO [8] which consist of pairs of images and\nsentences. In contrast, labeled image datasets without sentence descriptions (e.g.\nImageNet [10]) or text only corpora (e.g. Wikipedia) cover many more object cate-\ngories.\nHendricks et al. [30] propose the Deep Compositional Captioner (DCC) to ex-\nploit these vision-only and language-only unpaired data sources to describe novel\ncategories as visualized in Fig. 10. Similar to the attribute layer discussed in Sec-\ntion 3.1, Hendricks et al. extract words as labels from the descriptions to learn a\n“Lexical Layer”. The Lexical Layer is expanded by objects from ImageNet [10].\n16\nMarcus Rohrbach\nFig. 10 Describing novel ob-\nject categories which are not\ncontained in caption corpora\n(like otter). The Deep Compo-\nsitional Captioner (DCC) [30]\nuses an intermediate semantic\nattribute or “lexical” layer\nto connect classiﬁers learned\non unpaired image datasets\n(ImageNet) with text corpora\n(e.g. Wikipedia). This allows\nit to compose descriptions\nabout novel objects without\nany paired image-sentences\ntraining data. Adapted from\n[29].\nDeep \nCompositional \nCaptioner\nPaired Image-\nSentence Data\nA bus driving \ndown the \nstreet.\nUnpaired Text Data\nA otter that is sitting in \nthe water.\nA dog sitting on a boat \nin the water.\nUnpaired Image Data\nExisting \nMethods \nOtter\nAlpaca\nPizza\nBus\nYummy pizza \nsitting on the \ntable.\n[...] Otters live in a variety of \naquatic environments. They [...]\n[...] Pepperoni is a popular \npizza topping. [...]\nNo Transfer: A close up of a person holding a cell phone.\nDCC: A toad is sitting on a table.\nNo Transfer: A piece of cake with a fork and a fork.\nDCC: A tiramisu is sitting on a plate.\nNo Transfer: A couple of cows sitting next to each other.\nDCC: A couple of alpaca standing next to each other in a field.\nNo Transfer: A close up of a plate of food with a bowl of fruit.\nDCC: A close up of a plate of food with a bowl of persimmon.\nNo Transfer: A white and black and white photo of a white \nand blue fire hydrant.\nDCC: A candelabra is sitting on a table.\nNo Transfer: A close up of a plate of food on a table.\nDCC: A close up of a fig.\nNo Transfer: A small bird sitting on a green plant.\nDCC: A dragonfly with a green plant on a green plant.\nNo Transfer: A brown and white cow standing in a field.\nDCC: A impala is standing in the dirt.\nbird → toad\nchocolate → tiramisu\nsheep→ alpaca\nfruit→ persimmon\nvase→ candelabra\ntree→ fig\nbird→ dragonfly\ngiraffe→ impala\nbird→ cockatoo\nNo Transfer: A white bird standing on a white surface..\nDCC: A white cockatoo sitting on a grass covered field.\nbear→ coyote\nNo Transfer: A couple of giraffe standing next to each other.\nDCC: A coyote is standing in the middle of a forest.\nFig. 11 Qualitative results for describing novel ImageNet object categories. DCC [30] compared\nto an ablation without transfer. X →Y: known word X is transferred to novel word Y. From [29].\nTo be able to not only recognize but also generate the description about the novel\nobjects, DCC transfers the word prediction model from semantically closest known\nword in the Lexical Layer, where similarity is computed with Word2Vec [55]. Inter-\nesting to note is, that image captioning approaches such as [91, 12] do use ImageNet\ndata to (pre-) train the models (indicated with a dashed arrow in Fig. 10), but they\ndo not make use of the semantic information but only the learned representation.\nFig. 11 shows several categories where there exist no captions for training. With\nrespect to quantitative measures, compared to a baseline without transfer, DCC im-\nproves METEOR from 18.2% to 19.1% and F1 score, which measures the appear-\nAttributes as Semantic Units between Natural Language and Visual Recognition\n17\n- a little brown and white dog\n- a yellow collapsable toy tunnel\n- the lawn\n- a little brown and white dog\n- a yellow collapsable toy tunnel\n- the lawn\nInput: phrases + images \n(no bounding box annotation at training or test time)\nOutput: grounded phrases\n- a man\n- a small boy\n- their small, white dog\n- a toy\nGroundeR\n- a man\n- a small boy\n- their small, white dog [failure]\n- a toy [failure]\nAttend\na man\na man\nLSTM\nGrounding\nReconstruction\nInput\nLSTM\nAttend\na man\nLSTM\n(a) Without bounding box annotations at train-\ning or test time GroundeR [65] learns to ground\nfree-form natural language phrases in images.\n(b) GroundeR [65] reconstructs\nphrases by learning to attend to\nthe right box at training time.\n(c) GroundeR [65] local-\nizes boxes test time.\nFig. 12 Unsupervised grounding by learning to associate visual and textual semantic units. From\n[65].\nance of the novel object, from 0 to 34.3%. Hendricks et al. also show similar results\nfor video description.\n4 Grounding text in images\nIn this section we discuss the problem of grounding natural language in images.\nGrounding in this case means that given an image and a natural language sentence\nor phrase, we aim to localize the subset of the image which corresponds to the input\nphrase. For example, for the sentence “A little brown and white dog emerges from\na yellow collapsable toy tunnel onto the lawn.” and the corresponding image in\nFig. 12(a), we want to segment the sentence into phrases and locate the correspond-\ning bounding boxes (or segments) in the image. While grounding has been addressed\ne.g. in [40, 34, 5, 81], it is restricted to few categories. An exception are Karpathy\net al. [36, 37] who aim to discover a latent alignment between phrases in text and\nbounding box proposals in the image. Karpathy et al. [37] ground dependency-tree\nrelations to image regions using multiple instance learning (MIL) and a ranking\nobjective. Karpathy and Fei-Fei [36] simplify the MIL objective to just the maxi-\nmal scoring box and replace the dependency tree with a learned recurrent network.\nThese approaches have unfortunately not been evaluated with respect to the ground-\ning performance due to a lack of annotated datasets. Only recently two datasets were\n18\nMarcus Rohrbach\nA man walking by a sitting man\non the street.\nA white dog is following a black\ndog along the beach.\nThree people on a walk down\na cement path beside a ﬁeld of\nwildﬂowers with skyscrapers in\nthe background.\nFig. 13 Qualitative results for GroundeR unsupervised [65] on Flickr 30k Entities [60]. Compact\ntextual semantic units (phrases, e.g. “a sitting man”) are associated with visual semantic units\n(bounding boxes). Best viewed in color.\nreleased: Flickr30k Entities [60] augments Flickr30k [97] with bounding boxes for\nall noun phrases present in textual descriptions and ReferItGame [38] has localized\nreferential expressions in images. Even more recent, at the time of writing, efforts\nare being made to also collect grounded referential expressions for the MS COCO\n[47] dataset, namely the authors of ReferItGame are in progress of extending their\nannotations as well as longer referential expressions have been collected by Mao\net al. [51]. Similar efforts are also made in the Visual Genome project [41] which\nprovides densely annotated images with phrases.\nIn the following we focus on how to approach this problem and the ﬁrst ques-\ntion is, where is the best point of interaction between linguistic elements and visual\nelements? Following the approaches in [36, 37, 60] a good way to this is to decom-\npose both, sentence and image into concise semantic units or attributes which we\ncan match to each other. For the data as shown in Figures 12(a) and 13, sentences\ncan be split into phrases of typically a few words and images are composed into a\nlarger number of bounding box proposals [87]. An alternative is to integrate phrase\ngrounding in a fully-convolutional network, for bounding box prediction [35] or\nsegmentation prediction [33]. In the following, we discuss approaches which focus\non how to ﬁnd the association between visual and linguistic components, rather than\nthe actual segmentation into components. We ﬁrst look at an unsupervised setting\nwith respect to the grounding task, i.e. we assume that no bounding box annota-\ntions are available for training (Section 4.1), and then we show how to integrate\nsupervision (Section 4.2). Section 4.3 discusses the results.\n4.1 Unsupervised grounding\nAlthough many data sources contain images which are described with sentences or\nphrases, they typically do not provide the spatial localization of the phrases. This\nAttributes as Semantic Units between Natural Language and Visual Recognition\n19\nSCRC [32]\nGroundeR semi-supervised [65]\nGroundeR supervised [65]\nwith 3.12% annot.\nanywhere but the people – ﬁrst person in line – group people center – very top left of whole image\nthe street – tree to the far left – top middle sky – white car far right bottom corner\nFig. 14 Qualitative grounding results on ReferItGame Dataset [38]. Different colors show different\nreferential expressions for the same image. Best viewed in color.\nis true for both curated datasets such as MSCOCO [47] or large user generated\ncontent as e.g. in the YFCC 100M dataset [84]. Consequently, being able to learn\nfrom this data without grounding supervision would allow large amount and variety\nof training data. This setting is visualized in Fig. 12(a).\nFor this setting Rohrbach et al. [65] propose the approach GroundeR, which is\nable to learn the grounding by aiming to reconstruct a given phrase using an atten-\ntion mechanism as shown in Fig. 12(b). In more detail, given images paired with\nnatural language phrases (or sentence descriptions), but without any bounding box\ninformation, we want to localize these phrases with a bounding box in the image\n(Fig. 12c). To do this, GroundeR learns to attend to a bounding box proposal and,\nbased on the selected bounding box, reconstructs the phrase (Fig. 12b). Attention\nmeans that the model predicts a weighting over the bounding boxes and then takes\nthe weighted average of the features from all boxes. A softmax over the weights en-\ncourages that only one or a few boxes have high weights. As the second part of the\nmodel (Fig. 12b, bottom) is able to predict the correct phrase only if the ﬁrst part of\nthe model attended correctly (Fig. 12b, top), this can be learned without additional\nbounding box supervision. At test time we evaluate the grounding performance, i.e.\nwhether the model assigned the highest weight to / attended to the correct bounding\nbox. The model is able to learn these associations as the parameters of the model\nare learned across all phrases and images. Thus, for a proper reconstruction, the vi-\nsual semantic units and linguistic phrases have to match, i.e. the models learns what\ncertain visual phrases mean in the image.\n20\nMarcus Rohrbach\nApproach\nAccuracy\nUnsupervised training\nGroundeR (VGG-CLS) [65]\n24.66\nGroundeR (VGG-DET) [65]\n32.42\nSemi-supervised training\nGroundeR (VGG-CLS) [65]\n3.12% annotation\n33.02\n6.25% annotation\n37.10\n12.5% annotation\n38.67\nSupervised training\nCCA embedding [60]\n25.30\nSCRC (VGG+SPAT) [32]\n27.80\nGroundeR (VGG-CLS) [65]\n41.56\nGroundeR (VGG-DET) [65]\n47.70\nApproach\nAccuracy\nUnsupervised training\nLRCN [12] (reported in [32])\n8.59\nCAFFE-7K [27] (reported in [32])\n10.38\nGroundeR (VGG+SPAT) [65]\n10.44\nSemi-supervised training\nGroundeR (VGG+SPAT) [65]\n3.12% annotation\n15.03\n6.25% annotation\n19.53\n12.5% annotation\n21.65\nSupervised training\nSCRC (VGG+SPAT) [32]\n17.93\nGroundeR (VGG+SPAT) [65]\n26.93\n(a) Flickr 30k Entities dataset [60]\n(b) ReferItGame dataset [38]\nTable 4 Phrase grounding, accuracy in %. VGG-CLS: Pre-training the VGG network [79] for the\nvisual representation on ImageNet classiﬁcation data only. VGG-DET: VGG further ﬁne-tuned\nfor the object detection task on the PASCAL dataset [15] using Fast R-CNN [25]. VGG+SPAT:\nVGG-CLS + spatial bounding box features (box location and size).\n4.2 Semi-supervised and fully supervised grounding\nIf grounding supervision (phrase bounding box associations) is available, GroundeR\n[65] can integrate it by adding a loss over the attention mechanism (Fig. 12b, “At-\ntend”). Interestingly, this allows to provide supervision only for a subset of the\nphrases (semi-supervised) or all phrases (fully supervised).\nFor supervised grounding, Plummer et al. [60] proposed to learn a CCA em-\nbedding [26] between phrases and the visual representation. The Spatial Context\nRecurrent ConvNet (SCRC) [32] and the approach of Mao et al. [51] use a caption\ngeneration framework to score phrases on a set of bounding box proposals. This al-\nlows to rank bounding box proposals for a given phrase or referential expression. Hu\net al. [32] show the beneﬁt of transferring models trained on full-image description\ndatasets as well as spatial (bounding box location and size) and full-image context\nfeatures. Mao et al. [51] show how to discriminatively train the caption generation\nframework to better distinguish different referential expression.\n4.3 Grounding results\nIn the following we discuss results on the Flickr 30k Entities dataset [60] and the\nReferItGame dataset [38], which both provide ground truth alignment between noun\nphrases (within sentences) and bounding boxes. For the unsupervised models, the\ngrounding annotations are only used at test time for evaluation, not for training. All\nAttributes as Semantic Units between Natural Language and Visual Recognition\n21\nwhere\ncount\ncolor\n...\namber\nsitting\n...\nLSTM\nfloor\ncat\nCNN\nWhere is \nthe amber cat?\nLayout\nParser\nand\ndog\nFig. 15 To approach visual question answering, Andreas et al. [2] propose to dynamically create\na deep network which is composed of different “modules” (colored boxes). These “modules” rep-\nresent semantic units, i.e. attributes, which link linguistic units in the question with computational\nunits to do the corresponding visual recognition. Adapted from [1].\napproaches use the activations of the second last layer of the VGG network [79] to\nencode the image inside the bounding boxes.\nTable 4(a) compares the approaches quantitatively. The unsupervised variant of\nGroundeR reaches nearly the supervised performance of CCA [60] or SCRC[32]\non Flickr 30k Entities, successful examples are shown in Fig. 13. For the referen-\ntial expressions of the ReferItGame dataset the unsupervised variant of GroundeR\nreaches performance on par with prior work (Table 4b) and quickly gains perfor-\nmance when adding few labeled training annotation (semi-supervised training). In\nthe fully supervised setting GroundeR improves signiﬁcantly over state-of-the-art\non both datasets, which is also reﬂected in the qualitative results shown in Fig. 14.\n5 Visual question answering\nVisual question answering is the problem of answering natural language questions\nabout images, e.g. for the question “Where is the amber cat?” about the image\nshown in Fig. 15 we want to predict the corresponding answer on the ﬂoor, or just\nﬂoor. This is a very interesting problem with respect to several aspects. On the one\nhand it has many applications, such visual search, human-robot interaction, and as-\nsisting blind people. On the other hand, it is also an interesting research direction\nas it requires to relate textual and visual semantics. More speciﬁcally it requires to\nground the question in the image, e.g. by localizing the relevant part in the image\n(amber cat in Fig. 15), and then recognizing and predicting an answer based on\nthe question and the image content. Consequently, this problem requires more com-\nplex semantic interaction between language and visual recognition than in previous\nsections, speciﬁcally, the problem requires ideas from grounding (Section 4) and\nrecognition (Section 2) or description (Section 3).\n22\nMarcus Rohrbach\ntest-dev\ntest\nY/N Num Other All\nAll\nLSTM\n78.7 36.6\n28.1 49.8\n–\nATT+LSTM\n80.6 36.4\n42.0 57.2\n–\nNMN\n70.7 36.8\n39.2 54.8\n–\nNMN+LSTM\n81.2 35.2\n43.3 58.0\n–\nNMN+LSTM+FT 81.2 38.0\n44.0 58.6 58.7\nLSTM: a question-only baseline\nATT: single find+describe for all questions\nNMN+LSTM: full model shown in Fig. 15\n+FT: image features ﬁne-tuned on captions [12]\nNMN: ablation w/o LSTM\nhow many different\nlights in various\ndifferent shapes and\nsizes?\nfour (four)\nwhat color is the\nvase?\ngreen (green)\nis the bus full of\npassengers?\nno (no)\n(a) Results from evaluation server of [4] in %.\n(b) Answers from [1] (ground truth answers in parentheses).\nFig. 16 Results on the VQA dataset [4]. Adapted from [1].\nMost recent approaches to visual question answering learn a joint hidden embed-\nding of the question and the image to predict the answer [49, 63, 24, 4] where all\ncomputation is shared and identical for all questions. An exception to this is pro-\nposed by Wu et al. [95], who learn an intermediate attribute representation from\nthe image descriptions, similar to the work discussed in Sections 3.3 and 3.4. In-\nterestingly, this intermediate layer of attributes allows to query an external knowl-\nedge base to provide additional (textual) information not visible in the image. The\nembedded textual knowledge base information is combined with the attribute rep-\nresentation and the hidden representation of a caption-generation recurrent network\n(LSTM) and forms the input to an LSTM-based question-answer encoder-decoder\n[49].\nAndreas et al. [2] go one step further with respect to compositionality and\npropose to predict a compositional neural network structure from the questions.\nAs visualized in Fig. 15 the question “Where is the amber cat?” is decomposed\ninto network “modules” amber, cat, and, and where. These modules are seman-\ntic units, i.e. attributes, which connect most relevant semantic components of the\nquestions (i.e. word or short phrases) with corresponding computation to recog-\nnize it in the image. These Neural Module Networks (NMN) have different types\nof modules for different types of attributes. Different types have different col-\nors in Fig. 15. The find[cat] and find[amber] (green) modules take in CNN\nactivations (VGG [79], last convolutional layer) and produce a spatial attention\nheatmap, while combine[and] (orange) combines two heatmaps to a single one,\nand describe[where] (blue) takes in a heatmap and CNN features to predict an an-\nswer. Note that the distinction between different types, e.g. find versus describe,\nwhich have different kind of computation and different instances, e.g. find[cat]\nversus find[amber], which learn different parameters. All parameters are initial-\nized randomly and only trained from question answer pairs. Interestingly, in this\nwork attributes are not only distinguished with respect of their type, but also are\ncomposed with other attributes in a deep network, whose parameters’ are learned\nAttributes as Semantic Units between Natural Language and Visual Recognition\n23\nend-to-end from examples, here question-answer pairs. In a follow up work, An-\ndreas et al. [3] learn not only the modules, but also what the best network structure\nis from a set of parser proposals, using reinforcement learning.\nIn addition to NMN, Andreas et al. [2, 3] also incorporate a recurrent net-\nwork (LSTM) to model common sense knowledge and dataset bias which has been\nshown to be important for visual question answering [49]. Quantitative results in\nTable 16(a) indicate that NMNs are indeed a powerful tool to question answering, a\nfew qualitative results can be seen Fig. 16(b).\n6 Conclusions\nIn this chapter we presented several tasks and approaches where attributes enable\na connection of visual recognition with natural language on a semantic level. For\nrecognizing novel object categories or activities, attribute can build an intermedi-\nate representation which allows incorporating knowledge mined from language re-\nsources or script data (Section 2). For this scenario we saw that semantic attribute\nclassiﬁers additionally build a good metric distance space useful for constructing\ninstance graphs and learning composite activity recognition models. In Section 3\nwe explained how an intermediate level of attributes can be used to describe videos\nwith multiple sentences and at a variable level and allow describing novel object\ncategories. In Section 4 we presented approaches for unsupervised and supervised\ngrounding of phrases in images. Different phrases are semantically overlapping and\nthe examined approaches try to relate these semantic units by jointly learning repre-\nsentations for the visual and language modalities. Section 5 discusses an approach\nto visual question answering which composes the most important attributes of a\nquestion in a compositional computation graph, whose parameters are learned end-\nto-end only by back-propagating from the answers.\nWhile the discussed approaches take a step towards the challenges discussed in\nSection 1.1, there are many future steps ahead. While the approaches in Section 2\nuse many advanced semantic relatedness measures minded from diverse language\nresources they are not jointly trained on textual and visual modalities. Regneri et al.\n[62] and Silberer et al. [78] take a step in this direction by looking at joint semantic\nrepresentation from the textual and visual modalities. Section 3 presents composi-\ntional models for describing videos, but it is only a ﬁrst step towards automatically\ndescribing a movie to a blind person as humans can do it [67], which will require\nan even higher degree of semantic understanding, and transfer within and between\nmodalities. Section 4 describes interesting ideas to grounding in images and it will\nbe interesting to see how this scales to the size of the Internet. Visual question an-\nswering (Section 5) is an interesting emerging direction with many challenges as it\nrequires to solve all of the above, at least to some extend.\nAcknowledgements I would like to thank all my co-authors, especially those whose publications\nare presented in this chapter. Namely, Sikandar Amin, Jacob Andreas, Mykhaylo Andriluka, Trevor\n24\nMarcus Rohrbach\nDarrell, Sandra Ebert, Jiashi Feng, Annemarie Friedrich, Iryna Gurevych, Lisa Anne Hendricks,\nRonghang Hu, Dan Klein, Raymond Mooney, Manfred Pinkal, Wei Qiu, Michaela Regneri, Anna\nRohrbach, Kate Saenko, Michael Stark, Bernt Schiele, Gy¨orgy Szarvas, Stefan Thater, Ivan Titov,\nSubhashini Venugopalan, and Huazhe Xu.\nMarcus Rohrbach was supported by a fellowship within the FITweltweit-Program of the Ger-\nman Academic Exchange Service (DAAD).\nReferences\n[1] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.\nDeep composi-\ntional question answering with neural module networks.\narXiv preprint\narXiv:1511.02799, 2015.\n[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks.\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n[3] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning to compose neu-\nral networks for question answering. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Computational Linguistics\n(NAACL), 2016.\n[4] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and\nD. Parikh. Vqa: Visual question answering. In International Conference\non Computer Vision (ICCV), 2015.\n[5] K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D. M. Blei, and M. I.\nJordan. Matching words and pictures. Journal of Machine Learning Research\n(JMLR), 3:1107–1135, 2003.\n[6] E. Bart and S. Ullman. Single-example learning of novel classes using rep-\nresentation by similarity. In Proceedings of the British Machine Vision Con-\nference (BMVC), 2005.\n[7] H.-H. Chen, M.-S. Lin, and Y.-C. Wei. Novel association measures using\nweb search with double checking. In Proceedings of the Annual Meeting of\nthe Association for Computational Linguistics (ACL), 2006.\n[8] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L.\nZitnick. Microsoft COCO captions: Data collection and evaluation server.\narXiv preprint arXiv:1504.00325, 2015.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2009.\n[10] J. Deng, A. Berg, K. Li, and L. Fei-Fei. What does classifying more than\n10,000 image categories tell us? In European Conference on Computer Vision\n(ECCV), 2010.\n[11] L. R. Dice. Measures of the amount of ecologic association between species.\nEcology, 26(3):297–302, 1945.\n[12] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,\nK. Saenko, and T. Darrell. Long-term recurrent convolutional networks for\nAttributes as Semantic Units between Natural Language and Visual Recognition\n25\nvisual recognition and description. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2015.\n[13] K. Duan, D. Parikh, D. Crandall, and K. Grauman. Discovering Localized\nAttributes for Fine-grained Recognition. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2012.\n[14] S. Ebert, D. Larlus, and B. Schiele. Extracting Structures in Image Collec-\ntions for Object Recognition. In European Conference on Computer Vision\n(ECCV), 2010.\n[15] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman.\nThe pascal visual object classes (voc) challenge. International Journal of\nComputer Vision (IJCV), 88(2):303–338, 2010.\n[16] H. Fang, S. Gupta, F. N. Iandola, R. Srivastava, L. Deng, P. Doll´ar, J. Gao,\nX. He, M. Mitchell, J. C. Platt, C. L. Zitnick, and G. Zweig. From captions\nto visual concepts and back. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2015.\n[17] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth.\nDescribing objects by\ntheir attributes. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2009.\n[18] A. Farhadi, I. Endres, and D. Hoiem. Attribute-centric recognition for cross-\ncategory generalization.\nIn Conference on Computer Vision and Pattern\nRecognition (CVPR), 2010.\n[19] R. Farrell, O. Oza, V. Morariu, T. Darrell, and L. Davis. Birdlets: Subordinate\ncategorization using volumetric primitives and pose-normalized appearance.\nIn International Conference on Computer Vision (ICCV), 2011.\n[20] C. Fellbaum. WordNet: An Electronical Lexical Database. The MIT Press,\n1998.\n[21] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and\nT. Mikolov. Devise: A deep visual-semantic embedding model. In Con-\nference on Neural Information Processing Systems (NIPS), 2013.\n[22] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong. Learning multimodal latent\nattributes. IEEE Transactions on Pattern Analysis and Machine Intelligence\n(PAMI), 36(2):303–316, 2014.\n[23] E. Gabrilovich and S. Markovitch. Computing Semantic Relatedness using\nWikipedia-based Explicit Semantic Analysis. In Proceedings of the Interna-\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI), 2007.\n[24] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to\na machine? dataset and methods for multilingual image question answering.\nIn Conference on Neural Information Processing Systems (NIPS), 2015.\n[25] R. Girshick. Fast R-CNN. In International Conference on Computer Vision\n(ICCV), 2015.\n[26] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik. Improving\nimage-sentence embeddings using large weakly annotated photo collections.\nIn European Conference on Computer Vision (ECCV), 2014.\n26\nMarcus Rohrbach\n[27] S. Guadarrama, E. Rodner, K. Saenko, N. Zhang, R. Farrell, J. Donahue,\nand T. Darrell. Open-vocabulary object retrieval. In Robotics: science and\nsystems, 2014.\n[28] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation. In International Con-\nference on Computer Vision (ICCV), 2015.\n[29] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and\nT. Darrell.\nDeep compositional captioning: Describing novel object cat-\negories without paired training data.\narXiv preprint arXiv:1511.05284v1,\n2015.\n[30] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and\nT. Darrell. Deep compositional captioning: Describing novel object cate-\ngories without paired training data. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2016.\n[31] J. Hoffman, S. Guadarrama, E. Tzeng, J. Donahue, R. Girshick, T. Darrell,\nand K. Saenko. LSDA: Large scale detection through adaptation. In Confer-\nence on Neural Information Processing Systems (NIPS), 2014.\n[32] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell. Natural\nlanguage object retrieval. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2015.\n[33] R. Hu, M. Rohrbach, and T. Darrell. Segmentation from natural language\nexpressions. arXiv preprint arXiv:1603.06180, 2016.\n[34] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma, M. Bernstein, and\nL. Fei-Fei. Image retrieval using scene graphs. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2015.\n[35] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully convolutional lo-\ncalization networks for dense captioning. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016.\n[36] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating\nimage descriptions. In Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2015.\n[37] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment embeddings for bidi-\nrectional image sentence mapping.\nIn Conference on Neural Information\nProcessing Systems (NIPS), 2014.\n[38] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg. Referitgame: Refer-\nring to objects in photographs of natural scenes. In Proceedings of the Con-\nference on Empirical Methods in Natural Language Processing (EMNLP),\n2014.\n[39] P. Koehn.\nStatistical Machine Translation.\nCambridge University Press,\n2010.\n[40] C. Kong, D. Lin, M. Bansal, R. Urtasun, and S. Fidler. What are you talking\nabout? text-to-image coreference. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2014.\n[41] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY. Kalanditis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei. Visual\nAttributes as Semantic Units between Natural Language and Visual Recognition\n27\ngenome: Connecting language and vision using crowdsourced dense image\nannotations. arXiv preprint arXiv:1602.07332, 2016.\n[42] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. In Conference on Neural Information\nProcessing Systems (NIPS), 2012.\n[43] C. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen ob-\nject classes by between-class attribute transfer. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2009.\n[44] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-based classiﬁca-\ntion for zero-shot visual object categorization. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (PAMI), 36(3):453–465, 2014.\n[45] C. Liang, C. Xu, J. Cheng, W. Min, and H. Lu. Script-to-movie: A computa-\ntional framework for story movie composition. Multimedia, IEEE Transac-\ntions on, 15(2):401–414, 2013.\n[46] D. Lin. An information-theoretic deﬁnition of similarity. In International\nConference on Machine Learning (ICML), 1998.\n[47] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar,\nand C. L. Zitnick. Microsoft coco: Common objects in context. In European\nConference on Computer Vision (ECCV), 2014.\n[48] M. Malinowski and M. Fritz. A multi-world approach to question answering\nabout real-world scenes based on uncertain input. In Conference on Neural\nInformation Processing Systems (NIPS), 2014.\n[49] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-\nbased approach to answering questions about images. In International Con-\nference on Computer Vision (ICCV), 2015.\n[50] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille. Deep captioning\nwith multimodal recurrent neural networks (m-rnn). In International Confer-\nence on Learning Representations (ICLR), 2015.\n[51] J. Mao, J. Huang, A. Toshev, O. Camburu, A. Yuille, and K. Murphy. Gener-\nation and comprehension of unambiguous object descriptions. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2016.\n[52] O. Maron and T. Lozano-P´erez. A framework for multiple-instance learning.\nConference on Neural Information Processing Systems (NIPS), 1998.\n[53] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for\nLarge Scale Image Classiﬁcation: Generalizing to New Classes at Near-Zero\nCost. In European Conference on Computer Vision (ECCV), 2012.\n[54] R. Mihalcea and D. I. Moldovan. A method for word sense disambiguation\nof unrestricted text. In Proceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL), 1999.\n[55] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed\nrepresentations of words and phrases and their compositionality. In Confer-\nence on Neural Information Processing Systems (NIPS), 2013.\n[56] Y. Moses, S. Ullman, and S. Edelman. Generalization to novel images in\nupright and inverted faces. Perception, 25:443–461, 1996.\n28\nMarcus Rohrbach\n[57] D. Mrowca, M. Rohrbach, J. Hoffman, R. Hu, K. Saenko, and T. Darrell. Spa-\ntial semantic regularisation for large scale object detection. In International\nConference on Computer Vision (ICCV), 2015.\n[58] M. Palatucci, D. Pomerleau, G. Hinton, and T. Mitchell. Zero-shot learning\nwith semantic output codes. In Conference on Neural Information Processing\nSystems (NIPS), 2009.\n[59] D. Parikh and K. Grauman. Relative attributes. In International Conference\non Computer Vision (ICCV), 2011.\n[60] B. Plummer, L. Wang, C. Cervantes, J. Caicedo, J. Hockenmaier, and\nS. Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences\nfor richer image-to-sentence models. In International Conference on Com-\nputer Vision (ICCV), 2015.\n[61] R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng.\nSelf-taught learning:\nTransfer learning from unlabeled data. In International Conference on Ma-\nchine Learning (ICML), 2007.\n[62] M. Regneri, M. Rohrbach, D. Wetzel, S. Thater, B. Schiele, and M. Pinkal.\nGrounding Action Descriptions in Videos. Transactions of the Association\nfor Computational Linguistics (TACL), 2013.\n[63] M. Ren, R. Kiros, and R. Zemel. Image question answering: A visual seman-\ntic embedding model and a new dataset. In Conference on Neural Information\nProcessing Systems (NIPS), 2015.\n[64] A. Rohrbach, M. Rohrbach, W. Qiu, A. Friedrich, M. Pinkal, and B. Schiele.\nCoherent multi-sentence video description with variable level of detail. In\nProceedings of the German Confeence on Pattern Recognition (GCPR),\n2014.\n[65] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele.\nGround-\ning of textual phrases in images by reconstruction.\narXiv preprint\narXiv:1511.03745, 2015.\n[66] A. Rohrbach, M. Rohrbach, and B. Schiele. The long-short story of movie\ndescription. Proceedings of the German Confeence on Pattern Recognition\n(GCPR), 2015.\n[67] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A dataset for movie\ndescription.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), 2015.\n[68] M. Rohrbach. Combining visual recognition and computational linguistics:\nlinguistic knowledge for visual recognition and natural language descrip-\ntions of visual content. PhD thesis, Saarland University, 2014.\n[69] M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and B. Schiele. What helps\nWhere - and Why? Semantic Relatedness for Knowledge Transfer. In Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2010.\n[70] M. Rohrbach, M. Stark, and B. Schiele. Evaluating Knowledge Transfer and\nZero-Shot Learning in a Large-Scale Setting. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2011.\nAttributes as Semantic Units between Natural Language and Visual Recognition\n29\n[71] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A database for ﬁne\ngrained activity detection of cooking activities. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2012.\n[72] M. Rohrbach, M. Regneri, M. Andriluka, S. Amin, M. Pinkal, and B. Schiele.\nScript data for attribute-based recognition of composite activities. In Euro-\npean Conference on Computer Vision (ECCV), 2012.\n[73] M. Rohrbach, M. Stark, G. Szarvas, and B. Schiele. Combining language\nsources and robust semantic relatedness for attribute-based knowledge trans-\nfer. In Proceedings of the European Conference on Computer Vision Work-\nshops (ECCV Workshops), volume 6553 of LNCS, 2012.\n[74] M. Rohrbach, S. Ebert, and B. Schiele. Transfer Learning in a Transductive\nSetting. In Conference on Neural Information Processing Systems (NIPS),\n2013.\n[75] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele. Trans-\nlating video content to natural language descriptions. In International Con-\nference on Computer Vision (ICCV), 2013.\n[76] M. Rohrbach, A. Rohrbach, M. Regneri, S. Amin, M. Andriluka, M. Pinkal,\nand B. Schiele.\nRecognizing ﬁne-grained and composite activities using\nhand-centric features and script data. International Journal of Computer Vi-\nsion (IJCV), 2015.\n[77] A. Senina, M. Rohrbach, W. Qiu, A. Friedrich, S. Amin, M. Andriluka,\nM. Pinkal, and B. Schiele. Coherent multi-sentence video description with\nvariable level of detail. arXiv preprint arXiv:1403.6173, 2014.\n[78] C. Silberer, V. Ferrari, and M. Lapata. Models of semantic representation with\nvisual attributes. In Proceedings of the Annual Meeting of the Association for\nComputational Linguistics (ACL), 2013.\n[79] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-\nscale image recognition. In International Conference on Learning Represen-\ntations (ICLR), 2015.\n[80] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman. Dis-\ncovering Object Categories in Image Collections. In International Confer-\nence on Computer Vision (ICCV), 2005.\n[81] R. Socher and L. Fei-Fei. Connecting modalities: Semi-supervised segmen-\ntation and annotation of images using unaligned text corpora. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2010.\n[82] T. Sørensen. A method of establishing groups of equal amplitude in plant\nsociology based on similarity of species and its application to analyses of the\nvegetation on danish commons. Biol. Skr., 5:1–34, 1948.\n[83] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2015.\n[84] B. Thomee, B. Elizalde, D. A. Shamma, K. Ni, G. Friedland, D. Poland,\nD. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research.\nCommunications of the ACM, 59(2):64–73, 2016.\n30\nMarcus Rohrbach\n[85] S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst. In\nConference on Neural Information Processing Systems (NIPS), 1996.\n[86] A. Torabi, C. Pal, H. Larochelle, and A. Courville. Using descriptive video\nservices to create a large data source for video annotation research. arXiv\npreprint arXiv:1503.01070v1, 2015.\n[87] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selec-\ntive search for object recognition. International Journal of Computer Vision\n(IJCV), 104(2):154–171, 2013.\n[88] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and\nK. Saenko.\nSequence to sequence – video to text.\narXiv preprint\narXiv:1505.00487v2, 2015.\n[89] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and\nK. Saenko. Sequence to sequence – video to text. In International Con-\nference on Computer Vision (ICCV), 2015.\n[90] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and\nK. Saenko. Translating videos to natural language using deep recurrent neural\nnetworks. In Proceedings of the Conference of the North American Chapter\nof the Association for Computational Linguistics (NAACL), 2015.\n[91] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural\nimage caption generator.\nIn Conference on Computer Vision and Pattern\nRecognition (CVPR), 2015.\n[92] H. Wang and C. Schmid. Action recognition with improved trajectories. In\nInternational Conference on Computer Vision (ICCV), 2013.\n[93] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Action Recognition by Dense\nTrajectories. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2011.\n[94] M. Weber, M. Welling, and P. Perona. Towards automatic discovery of ob-\nject categories. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2000.\n[95] Q. Wu, C. Shen, A. v. d. Hengel, P. Wang, and A. Dick. Image captioning\nand visual question answering based on attributes and their related external\nknowledge. arXiv preprint arXiv:1603.02814, 2016.\n[96] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and\nA. Courville.\nDescribing videos by exploiting temporal structure.\narXiv\npreprint arXiv:1502.08029v4, 2015.\n[97] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descrip-\ntions to visual denotations: New similarity metrics for semantic inference\nover event descriptions. Transactions of the Association for Computational\nLinguistics (TACL), 2:67–78, 2014.\n[98] T. Zesch and I. Gurevych. Wisdom of crowds versus wisdom of linguists -\nmeasuring the semantic relatedness of words. Natural Language Engineer-\ning, 16(1):25–59, 2010.\n[99] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning Deep\nFeatures for Scene Recognition using Places Database. In Conference on\nNeural Information Processing Systems (NIPS), 2014.\nAttributes as Semantic Units between Natural Language and Visual Recognition\n31\n[100] D. Zhou, O. Bousquet, T. N. Lal, Jason Weston, and B. Sch¨olkopf. Learning\nwith Local and Global Consistency. In Conference on Neural Information\nProcessing Systems (NIPS), 2004.\n[101] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaus-\nsian ﬁelds and harmonic functions. In International Conference on Machine\nLearning (ICML), 2003.\n[102] C. L. Zitnick, D. Parikh, and L. Vanderwende. Learning the visual interpre-\ntation of sentences. In International Conference on Computer Vision (ICCV),\n2013.\n",
  "categories": [
    "cs.CV",
    "cs.CL"
  ],
  "published": "2016-04-12",
  "updated": "2016-04-12"
}