{
  "id": "http://arxiv.org/abs/1604.03986v1",
  "title": "Theoretically-Grounded Policy Advice from Multiple Teachers in Reinforcement Learning Settings with Applications to Negative Transfer",
  "authors": [
    "Yusen Zhan",
    "Haitham Bou Ammar",
    "Matthew E. taylor"
  ],
  "abstract": "Policy advice is a transfer learning method where a student agent is able to\nlearn faster via advice from a teacher. However, both this and other\nreinforcement learning transfer methods have little theoretical analysis. This\npaper formally defines a setting where multiple teacher agents can provide\nadvice to a student and introduces an algorithm to leverage both autonomous\nexploration and teacher's advice. Our regret bounds justify the intuition that\ngood teachers help while bad teachers hurt. Using our formalization, we are\nalso able to quantify, for the first time, when negative transfer can occur\nwithin such a reinforcement learning setting.",
  "text": "Theoretically-Grounded Policy Advice from Multiple Teachers in Reinforcement\nLearning Settings with Applications to Negative Transfer\nYusen Zhan1, Haitham Bou Ammar2, and Matthew E. Taylor1\n1Washington State University, Pullman, Washington\n2Princeton University, Princeton, New Jersey\nyusen.zhan@wsu.edu, hammar@princeton.edu, taylorm@eecs.wsu.edu\nAbstract\nPolicy advice is a transfer learning method where a\nstudent agent is able to learn faster via advice from\na teacher. However, both this and other reinforce-\nment learning transfer methods have little theoreti-\ncal analysis. This paper formally deﬁnes a setting\nwhere multiple teacher agents can provide advice\nto a student and introduces an algorithm to lever-\nage both autonomous exploration and teacher’s ad-\nvice. Our regret bounds justify the intuition that\ngood teachers help while bad teachers hurt. Us-\ning our formalization, we are also able to quantify,\nfor the ﬁrst time, when negative transfer can occur\nwithin such a reinforcement learning setting.\n1\nIntroduction\nReinforcement Learning (RL) has become a popular frame-\nwork for autonomous behavior generation from limited feed-\nback [Sutton and Barto, 1998]. Typical RL methods learn in\nisolation increasing their learning times and sample complex-\nities. Transfer learning aims to signiﬁcantly improve learn-\ning by providing informative knowledge from an external\nsource. The source of such knowledge varies from source\nagents to humans providing advice [Erez and Smart, 2008;\nTaylor and Stone, 2009]. In this paper, we focus on a frame-\nwork referred to as action advice or the advice model [Taylor\net al., 2014]. Here, the agent (i.e., student), learning in a\ntask, has access to a teacher (another agent or human) which\ncan provide action suggestions to facilitate learning. Given\n“good-enough” teachers, such advice models have shown\nmultiple beneﬁts over standard RL techniques. For example,\nothers [Taylor et al., 2014; Zimmer et al., 2014] show re-\nduced learning times and sample complexities for successful\nbehavior.\nThese methods, however, suffer from two main draw-\nbacks. First, validation results are empirical in nature and not\nformally-grounded. We do not have fundamental understand-\ning of these methods. Consequently, it is difﬁcult to formally\ncomprehend why these methods work. Second, most of these\ntechniques require the availability of a “good-enough” (op-\ntimal) teacher to beneﬁt the student. Unfortunately, access\nto such teachers is difﬁcult in a variety of complex domains,\nreducing the applicability of policy advice in real-world set-\ntings.\nIn this paper, we remedy the aforementioned drawbacks by\nproposing a new framework for policy advice. Our method\nformally generalizes current single-teacher advice models to\nthe multi-teacher setting. Our algorithm also remedies the\nneed for optimal teachers by exploiting both the student’s\nand the teacher’s knowledge. Even if the teacher is not op-\ntimal, a student, using our algorithm, is still capable of ac-\nquiring optimal behavior in a task; a property not supported\nby some state-of-the-art methods, e.g., learning from demon-\nstration. We theoretically and empirically analyze the perfor-\nmance of the proposed method and derive, for the ﬁrst time,\nregret bounds quantifying the successfulness of action advice.\nWe also provide theoretical justiﬁcation for current methods\n(i.e., single-teacher models) as special case of our formula-\ntion in the appendix. Our contributions can be summarized\nas:\n• deﬁning (formally) multi-teacher advice models,\n• introducing novel algorithms leveraging teacher and stu-\ndent knowledge,\n• deriving the regret analysis showing reduced sample\ncomplexities,\n• deriving theoretical guarantees for single teacher advice\nmodels, and\n• quantifying negative transfer under such advice model.\nInterestingly, these theoretical results justify a well-known\nintuition inherent to advice models: “good teachers help\nwhile bad teachers hurt.” The results show that students can\nstill achieve optimal behavior when being advised by bad\nteachers. They, however, pay an extra cost in terms of their\nlearning times or sample complexities, relative to an optimal\nteacher. This should inspire researchers to adopt high quality\nteacher policies or avoid “bad teachers” if possible.\nGiven our formalization, we also derive a relation to nega-\ntive transfer. We quantify, for the ﬁrst time, the occurrence\nof negative transfer in action advice models, shedding the\nlight on failure modes of these methods. Consequently, these\nresults yield two claims about transfer learning. First, high\nquality transfer knowledge may still cause negative transfer\nwhen the target algorithm is able to outperform the source\nknowledge. Second, expert knowledge is important for the\narXiv:1604.03986v1  [cs.LG]  13 Apr 2016\nresearchers to determine whether or not to transfer because\nevaluation of the transfer knowledge is usually expensive (it is\nequivalent to evaluating the teacher policy in the target MDP).\n2\nPreliminaries\n2.1\nOnline Reinforcement Learning & Regret\nModel\nIn RL, an agent must sequentially select actions to maxi-\nmize its total expected return. Such problems are formal-\nized as a Markov decision Process (MDP), deﬁned as M =\n⟨S, A, P, R⟩, where S and A denote the ﬁnite state and ac-\ntions spaces with a total size of |S| and |A| respectively,\nP : S × A × S →[0, 1] represents the probability transition\nkernel describing the task dynamics, and R : S × A →R is\nthe reward function quantifying the performance of the agent.\nThe total expected return of an agent following an algorithm,\nG, to compute the optimal action-selection rule from a start-\ning state s ∈S after T time steps is deﬁned as:\nRG(s, T) = E\n\" T\nX\nt=0\nR(st, at)\n#\n,\n(1)\nwith st ∈S and at ∈A. The goal is to determine an optimal\npolicy, π⋆: S →A that maximizes the total expected return.\nRegret Model: Similar to standard online learning, we\nquantify the performance of the algorithm, G, by measuring\nits regret with respect to optimality. We deﬁne the regret of a\nstate s after T time steps in terms of the expected reward as:\n∆G(s, T) = λ⋆T −RG(s, T),\n(2)\nwhere λ⋆is the optimal reward acquired by following an opti-\nmal algorithm G⋆at each time step. In the general case when\nno reachability assumptions are imposed, it is easy to con-\nstruct MDPs in which algorithms suffer high regret. Follow-\ning Puterman [2005], we remedy this problem by considering\nweakly-communicating MDPs1 deﬁned as follows.\nDeﬁnition 1. An MDP is called weakly communicating in\nsuch a case where the state set S can be decomposed into two\nsubsets, S1 and S2. In S1 any state is reachable from every\nother state under a deterministic policy, π, while states in S2\nare transient under all policies.\nThe optimal gain, λ⋆in Equation 2, is state independent.\nThat is, any s ∈S, shares the same optimal expected re-\nward [Puterman, 2005], which can be solved for using:\nh⋆+ λ⋆e = max\na∈A{R(s, a) + PT\ns,ah⋆},\nwhere h⋆is an |S| dimensional vector typically referred to\nas the bias vector, Ps,a denotes the probability to transition\nfrom s applying action a, and e ∈R|S| is a unit dimensional\nvector. When needed, we explicitly write the dependency of\nλ⋆and h⋆as h⋆(s; M) and λ⋆(M). We also deﬁne the span\nof h as: sp(h) = maxs∈S h(s) −mins∈S h(s).\n1Please note that weakly-communicating MDPs are considered\nthe most general among subclasses of MDPs, see Puterman [2005].\nAlgorithm 1 REGAL.C: Constrained Optimization\nInput: parameter H, dataset Di and current time T\nOutput: ˆπi+1\n1: ti = current time T\n2: Use Di to update the state transition probabilities by\nˆP t\ns,a(s′) =\nN(s, a, s′; t)\nmax{N(s, a; t), 1}\n(3)\n3: With t = ti, Mi is the set of MDPs s.t.\n\r\r\rPs,a −ˆP t\ns,a\n\r\r\r\n1 ≤\ns\n12|S| log(2|A|t/δ)\nmax {N(s, a, s; t), 1}\n4: Select M i ∈Mi by following optimization equation\nover ∀M ∈Mi,\nmax λ⋆(M) s.t. sp(h∗(M)) ≤H\n5: ˆπi+1=average reward optimal policy for M i (value itera-\ntion)\n6: return ˆπi+1\nFinally, we follow Bartlett and Tewari [2009] to deﬁne\nreachability in weakly communicating MDPs using the one-\nway diameter:\ndiamone-way(M) = maxs∈S minπ T π\ns1→¯s,\nwith T π\ns1→¯s being the expected number of steps needed for\nreaching ¯s = arg maxs∈S h⋆(s; M) from s1 ∈S.\n2.2\nAlgorithms for Weakly-Communicating MDPs\nREGAL.C is an on-line algorithm for weakly communicat-\ning MDPs developed by Bartlett and Tewari [2009].\nThe\nbasic idea is that the REGAL.C can estimate the true MDP\nwith high probability in order to learn an ϵ-optimal policy\nwith high probability. Let N(s, a, s′; t) be the number of\nstate-action-state triples (s, a, s′) that have been visited at\ntime t. Further, let ti to denote the initial time of the itera-\ntion i. For brevity, we use Ni(s, a, s′) and Ni(s, a) to de-\nnote N(s, a, s′; ti) and N(s, a; ti) at iteration i. We also\nuse vi(s, a) = Ni+1(s, a) −Ni(s, a) to denote the num-\nber of times a state-action pair (s, a) is visited during it-\neration i. For each iteration i, REGAL acquires a dataset\nDi as input and updates the transition probability (see Equa-\ntion 3). It then constructs a set of MDPs Mi to select from us-\ning max λ∗(M) s.t. sp(h∗(M)) ≤H, where H is the upper\nbound on the span sp(h∗(M)). Given the MDP, REGAL.C\nuses value iteration for acquiring the optimal policy. These\nsteps are summarized in Algorithm 1.\n2.3\nSingle Teacher Advice Model\nThe single teacher advice model is a framework in which a\nstudent learning in an environment beneﬁts from a teacher’s\nadvice to speed-up learning. We deﬁne such a framework as\nthe tuple of ⟨πT, b, S, fd⟩. Here, πT denotes the teacher’s\npolicy, b represents the budget constraining the teacher’s ad-\nvice, S is the student, and fd is a function controlling the ad-\nvice from the teacher to the student. Apart from considering\nsingle teacher models, previous work assumed optimal teach-\ners where students always execute recommended actions. It is\neasy to construct complex settings in which access to optimal\nteachers is difﬁcult. Consequently, we extend these works\nto the more realistic settings of sub-optimal teachers, as we\ndetail later.\n3\nMultiple Teacher Advice Model\nIn this section we start by extending the single teacher model\nof Taylor et al. [2014] to the multiple non-optimal teacher set-\nting. Our advice model for m teachers is deﬁned as the tuple\n⟨Π, B, S, fd⟩, where Π = {πT1\n1 , πT2\n2 , . . . , πTm\nm } is the set of\nm ∈N teacher policies, and B = {b1, b2, . . . , bm} denotes\nthe set of budgets. It is easy to see that in case Π = {πT} and\nB = {b}, we can easily recover the special case single teacher\nmodel. We also generalize the work of Taylor et al. [2014] by\nmaking no restrictive assumptions on the optimality of any of\nthe teachers. We measure the performance of the teacher with\nrespect to a base policy πB in terms of regret:\nDeﬁnition 2. Given a teacher’s policy, πT ∈Π, and a base\npolicy πB, then the regret of following πT is related to that\nacquired by following πB using:\n∆T(s, T) = ρ∆B(s, T),\nwhere ρ ≥0 denotes the regret ratio, ∆T(s, T) = λ⋆T −\nRT(s, T) and ∆B(s, T) = λ⋆T −RB(s, T).\nThe above deﬁnition captures the three interesting cases\nquantifying the performance of an advice-based algorithm. If\nthe teacher is optimal, i.e., when ∆T(s, T) = 0, ρ is also 0.\nIn case 0 < ρ ≤1, then ∆T(s, T) ≤∆B(s, T) indicating\nthe the teacher’s policy is at least as good as the base policy\nπB. Finally, when ρ > 1, ∆T(s, T) > ∆B(s, T) imply-\ning the underperformance of the teacher. Consequently, with\nthe correct choice of the teacher by ρ one can still achieve\nsuccessful advice even in such a generalized setting.\n4\nEfﬁcient Multi-teacher Advice\nIn this section, we propose a new algorithm which combines\nthe advice policy and the MDPs information collected so far.\nThis allows for an accurate framework outperforming state-\nof-the-art techniques for policy advice. On a high level, our\nalgorithm consists of three main steps. First, a combined pol-\nicy is constructed based on multiple teachers. Second, data\ndepending on both teacher’s advice as well as MDP informa-\ntion is collected. Third, a new policy is computed online.\nNext, we outline each of the three steps and describe our\nnovel algorithm. Having achieved an accurate advice model,\nwe then rigorously analyze the theoretical aspects of our\nmethod and show a decrease in sample complexities com-\npared to current techniques.\n4.1\nThe Grand-Teacher\nOur method of policy advice constructs a grand teacher com-\nbining all teacher policies in a meta-policy.\nTo construct\nthe grand-teacher, we use an ensemble method and design\ntwo meta-policy variations: online and ofﬂine-constructions.\nNext, we detail each of the two variations.\nAlgorithm 2 Ofﬂine Construction of the Meta-Teacher\nInput: The set of states in the MDP, S.\n1: while ∃s ∈S is not visited do\n2:\nFollow a policy in the MDP\n3:\nif Current state s is not visited then\n4:\nQuery all teachers for advice and select action a us-\ning Majority Vote.\n5: return πgrand-teacher\nOnline Grand-Teacher: In the online construction, when-\never the student observes an unvisited state, s ∈S, each\nteacher provides its policy advice of the form πTi\ni , for all\ni ∈{1, . . . , m} with m being the total number of teachers.\nThe student then selects and stores the majority action from\nall teachers for that state s. As far as budget is concerned, it\nis easy to see that we only require to know advice for each\nstate in S, thus b1 = · · · = bm = |S|. Though easy to\nimplement and test, the online construction suffers from the\npotentially unrealistic need for the continuous availability of\nonline teachers.\nOfﬂine Grand-Teacher: To eliminates the need for an on-\nline teacher at each visit of a new state, the ofﬂine procedure\ntraverses the states in the MDP for constructing the meta-\nadvice policy. The main steps of this construction is sum-\nmarized in Algorithm 2.\nNote that Algorithm 2 is capable of constructing an ofﬂine\nmeta-teacher but requires extra exploration in the MDP. We\nnext show that O\n\u0010\n|S| log |S|\nδ\n\u0011\nsteps are enough to explore\neach state in the MDP with high probability:\nTheorem 1 (Sample Complexity). If Algorithm 2 indepen-\ndently and uniformly explores each state s ∈S, then with\nprobability of at least 1 −δ, O\n\u0010\n|S| log |S|\nδ\n\u0011\nsteps are sufﬁ-\ncient to visit each state at least one time.\n4.2\nMulti-Teacher Advice Algorithm\nTo improve current methods and arrive at a more realistic\nadvice framework, we now introduce our algorithm combin-\ning the grand-teacher’s policy and information attained by the\nstudent from the MDP.\nOur algorithm is based on the following intuition. At the\nbeginning of the learning process, a student requires guidance\nas it typically has little to no information of the task to be\nsolved. As time progress and the student explores, the MDP\ncan be effectively exploited for successful learning. Unfortu-\nnately, such a process is not well modeled using current meth-\nods. Here, we remedy this problem by introducing an algo-\nrithm which follows the teacher’s advice at the very beginning\nand then switches to a policy computed by an algorithm oper-\nating within the MDP. That is, the teacher guides the student\nat the beginning of the learning process and as the student\ngathers more experience, the teacher’s inﬂuence diminishes\nover time by switching into a policy computed by REGAL.C.\nThe overall procedure is summarized in Algorithm 3. Note\nthat our algorithm is inspired by DAGGER [Ross et al., 2010]\nin the sense that policies are updated by collecting data using\nAlgorithm 3\nInput: πT = the grand-teacher policy, ˆπ1=any policy\nOutput: π, the ϵ-optimal policy\n1: T = 0\n2: for i = 1 to m do\n3:\nLet πi+1 = βiπT + (1 −βi)ˆπi\n4:\nFollow πi until Ti-steps\n5:\nGet dataset Di\n6:\nT = T + Ti\n7:\nˆπi+1 = REGAL.C(Di, T) {See Algorithm 1}\n8: return πm+1\na mixture of action selection rules (i.e., student and teacher\npolicies). Contrary to DAGGER, however, our method col-\nlects all trajectories opposed to only collecting inconsistent\nactions, allowing for more accurate and efﬁcient updates.\nTo leverage both the teacher’s and learned policies, we set\na mixed policy of the form πi+1 = βiπT + (1 −βi)ˆπi, for\n0 ≤βi ≤1 to guide the student’s dataset collection while al-\nlowing the teacher to fractionally control exploration needed\nto collect data at the next iteration. β should typically be set\nso as to decay exponentially over time. This decreases the\nstudent’s reliance on the teacher and allows it to exploit the\nknowledge gathered from the MDP to learn better behaving\npolicies than that of the teacher. It is for this reason that our\nalgorithm, contrary to other methods, does not impose any\noptimality restrictions on the teacher. Having collected the\ndataset, Algorithm 3 uses REGAL.C (Algorithm 1) to update\nˆπi.\n4.3\nTheoretical Guarantees\nIn this section we formally derive the regret exhibited by our\nalgorithm. At a high level, we provide two theoretical re-\nsults. In the ﬁrst, we consider the general teacher case, while\nin the second we derive a corollary of the regret for optimal\nteachers. We show, for the ﬁrst time, better than constant\nimprovements compared to standard learners.\nTheorem 2. Assume Algorithm 3 is running for total T\nsteps in a weakly communicating MPD M starting from\nan initial state s\n∈\nS.\nLet H be a parameter such\nthat H\n≥sp(h⋆(M)).\nThen, with a probability of at\nleast 1 −δ, the total regret is given by:\n∆(s, T)\n=\nO\n\u0012\n(1 −β + ρβ)H|S|\nq\n|A|T log |A|T\nδ\n\u0013\n, where β ∈[0, 1]\nsuch that 1−β = max1≤i≤m{1−βi}, and ρ ≥0 is the ratio\nbetween the teacher’s regret ∆T and the regret exhibited by\nREGAL.C ∆REGAL.C such that ∆T ≤ρ∆REGAL.C.\nProof. Due to the space limits, we provide a proof sketch.\nThe proof is based on the regret bound of REGAL.C. We in-\ntroduce the regret ratio to reduce the grand-teacher’s regret\nto the REGAL.C’s regret. Then, we apply the Hoeffding’s\ninequality to arrive at the statement of the theorem.\nTheorem 2 implies that the teacher improves learning as\nlong as it is “good.” Namely, if 0 < ρ ≤1, 1 −β + ρβ ≤1,\nβ ∈[0, 1] which implies the student can enjoy a fraction of\nREGAL.C’s regret. However, if ρ > 1, 1 −β + ρβ > 1,\nthe student suffers more regret than the original REGAL.C\nalgorithm. This justiﬁes our intuition that good teachers assist\nlearning while poor ones hamper learning. Moreover, if there\nexists prior knowledge that a teacher has poor performance,\nit would be better off for the student to neglect its advice as it\nwill suffer extra regret.\nIf the teacher’s ρ = 0, we have the following Corollary:\nCorollary 1. If the teacher is optimal, then with at least a\nprobability of 1 −δ the total regret is given by: ∆(s, T) =\nO\n\u0012\n(1 −β)H|S|\nq\n|A|T log |A|T\nδ\n\u0013\n.\nRemark 1. Please note that the above theoretical results are\nmore than a constant improvement to the regret. Notice that\nβ depends on the number of iterations which can be bounded\nby |S| and |A| of the input MDP M [Auer et al., 2009]. Fur-\nther, ρ depends on the input teacher’s policy which is also\nan input to Algorithm 3. Consequently, it can be shown that\nthese regret improvements exceed simple constant bounds.\n5\nNegative Transfer\nTo formalize the relation to negative transfer, we recognize\nthat the regret ratio can be written as:\nρ = ∆T(s, T)\n∆B(s, T) = λ⋆T −RT(s, T)\nλ⋆T −RB(s, T)\n(4)\nThis suggests that we can estimate the ratio by calculating λ⋆\nand Rπ(s, T), given a policy π. So, we use\nρ(π1, π2, T) = λ⋆T −Rπ1(s, T)\nλ⋆T −Rπ2(s, T)\nto denote the regret ratio between policy π1 and π2 until step\nT. At this stage, we deﬁne:\n• Negative transfer from policy π1 to π2 until T steps:\nρ(π1, π2, T) > 1.\n• Positive transfer from policy π1 to π2 until T steps:\nρ(π1, π2, T) ≤1\nTo formalize negative transfer, our goal at this stage is to\nrelate ρ(·) to a metric between source and target tasks. For\nthat sake, we deﬁne: ds\nt(πs) = ˆRπs\ns (s, T) −ˆRπs\nt (s, T), with\nˆRπs\ns (s, T) and ˆRπs\nt (s, T) being the agent’s estimates of the\nrewards in the source and the target after T steps. Conse-\nquently, an estimate ˆρ to ρ can be derived as:\nˆρ(πs, πt, T)\n= λ⋆T −ˆRπs\nt (s, T)\nλ⋆T −ˆRπt\nt (s, T)\n=\nλ⋆T +\n\u0010\nˆRπs\ns (s, T) −ˆRπs\nt (s, T)\n\u0011\n−ˆRπs\ns (s, T)\nλ⋆T −ˆRπt\nt (s, T)\n= λ⋆T + ds\nt(πs) −ˆRπs\ns (s, T)\nλ⋆T −ˆRπt\nt (s, T)\n.\nˆRπs\ns (s, T) and ˆRπt\nt (s, T) can be bounded by the Empirical\nBernstein bound [Audibert et al., 2007]. With a probability\n1 −δ, we have\n\f\f\f\f\f\nˆRπs\ns (s, T) −Eπs\n\" T\nX\nt=0\nRs(st, at)\n#\f\f\f\f\f ≤ϵ1,\nwith\nϵ1\n=\n¯σ\nq\n2 log(3/δ)\nns\n+\n6Rmax log(3/δ)\nns\n,\n¯σ\n=\nq\n1/ns\nPns\ni=1(Ri −¯R)2 is the standard deviation of the sam-\nple , we derive\nλ⋆T + ds\nt(πs) −C2\nλ⋆T −C4\n≤ˆρ(πs, πt, T) ≤λ⋆T + ds\nt(πs) −C1\nλ⋆T −C3\n(5)\nwith C1, C2, C3, and C4 are constants. Consequently, for\nnegative transfer:\nˆρ(πs, πt, T) ≥λ⋆T + ds\nt(πs) −C2\nλ⋆T −C4\n> 1.\nThen, assuming enough samples, negative transfer occurs if:\nds\nt(πs)\n>\n(\nEπs\n\" T\nX\nt=0\nRs(st, at)\n#\n−Eπt\n\" T\nX\nt=0\nRt(st, at)\n#)\n(6)\nThe condition sheds light on the negative transfer in the\nsense of metric notation and provides a formal way to deter-\nmine negative transfer. First, if the condition in Eq. 6 holds\nafter evaluation, researchers should avoid the source policy πs\nto the target tasks since it may cause negative transfer. Sec-\nond, if the researchers have enough expert knowledge about\ntheir working domain and transfer information, usually they\ncan avoid this evaluation phase in practice. In short, Eq. 6\nprovides a formal way to understand negative transfer and\njustify the intuition (adopt high quality source knowledge and\navoid bad teachers ) in the transfer practice.\n6\nExperimental Results\nGiven the above theoretical successes, this section provides\nempirical validation on three domains:\nCombination Lock: We use the domain described in Fig-\nure 2 which is a variation from [Whitehead, 1991]. The ex-\nperimental setting follows the caption description.\nGrid World is an RL benchmark in which an agent has\nto navigate an m × m grid world with the goal of reaching\na goal state. We employ an 11 × 11 grid world with a four\nroom layout as introduced in Sutton and Barto [1998]. The\nagent begins in the lower left corner of the map and navigates\nto the goal state being the upper right corner. To navigate, the\nagent has access (in each cell) to four actions transitioning\nit to the: north, south, west and east. Applying an action,\nit then transitions in that direction with a probability of 0.8\nand in the other three with a probability of 0.2. In case the\ndirection is blocked, the agent stays in the same state. Finally,\nthe agent receives a reward of 0 once reaching the goal state\nand a reward of −1 in all others.\nFigure 2: There are n + 1 states in the MDP. The last state\nhas only one action and the rest have two. The agent receives\nreward −1 for all actions, except when taking action a in state\nn, R(n, a) = 1. The agent stays in state n with probability\n0.9 and goes to state 0 with probability 0.1. The optimal pol-\nicy is to take action a in each state. Since there are n + 1\nstates, the budget B is at least n + 1 to achieve zero regret.\nBlock Dude is a game where an agent again navigates a\nmaze to reach a goal state. Reaching the goal directly is im-\npossible due to the presence of blocks restricting its move-\nment. The agent, however, can move to the left, right, and\nupwards. To reach the goal state, it needs to pick-up blocks\nand relocate them in correct positions. We use the default\nlevel 1 BURLAP [MacGlashan, 2014] in which there are two\nblocks and 3 × 25 maze. The agent receives a reward of +1\nin the goal state and a reward of −1 in all other states.\n6.1\nExperimental Setup & Results\nTo construct the grand teacher, we set the total number of\nteachers k = 10. For each teacher, the budget, bi, is set\nto the total number of states. In Algorithm 3, the maximum\nnumber of iterations and the size of each dataset, Di, were\nset to 10 and 200, respectively.\nValues of pi = 0.5i for\ni = 1, . . . , 10 were used to determine βi = pi. For Al-\ngorithm 1, the conﬁdence δ was set to 0.8 and H to 1000.\nThe optimal gain λ⋆and the optimal bias vector h can be\napproximated using the value function V [Puterman, 2005].\nLet Vl be the value function at iteration l, l = 0, 1, . . . .\nThe optimal gain λ⋆≈sp(Vl+1 −Vl), where sp(Vl) =\nmaxs∈S Vl(s) −mins∈S Vl(s) and the optimal bias vector\nh⋆≈Vl −lλl, when l is large enough.\nTo smooth the\nnatural variance in the student’s performance, each learning\ncurve is averaged over 10 independent trials of student learn-\ning. To better evaluate our method, we adopt six experimental\nsettings by considering different teachers and learning algo-\nrithms. For teachers, we consider three forms. The ﬁrst, re-\nferred to as “optimal teacher” provides optimal actions and is\nused by the grand teachers. The second, referred to as “worst\nteacher” advices the student to take actions with the lowest\nQ-values, while the last randomly selects action suggestions\nfrom the set of allowed moves. We also compare our method\nto REGAL.C (no advice), optimal policy (without learning),\nand Azar’s method [Azar et al., 2013]. Please note that Azar’s\nmethod can not converge to the optimal policy and suffers\nloss as its performance is restricted by the teacher.\nPerformance, measured by the average reward, is reported\nin Figure 1. First, it is clear that given optimal teachers, our\nmethod exactly traces the optimal policy achieving a regret of\n0. It is also important to note that in all three domains, even\nif the teacher was not optimal, and contrary to current tech-\nniques, our method is capable of acquiring optimal behavior.\n-200\n-195\n-190\n-185\n-180\n 0\n 2\n 4\n 6\n 8\n 10\nAverage Reward\nIterations\nWorst Teacher\nOptimal Teacher\nRand Teacher\nOptimal Policy\nREGAL.C\nAzar PA\n(a) Grid World\n-200\n-199.5\n-199\n-198.5\n-198\n-197.5\n-197\n-196.5\n-196\n 0\n 2\n 4\n 6\n 8\n 10\nAverage Reward\nIterations\nWorst Teacher\nOptimal Teacher\nRand Teacher\nOptimal Policy\nREGAL.C\nAzar PA\n(b) Combination Lock\n-200\n-195\n-190\n-185\n-180\n-175\n 0\n 2\n 4\n 6\n 8\n 10\nAverage Reward\nIterations\nWorst Teacher\nOptimal Teacher\nRand Teacher\nOptimal Policy\nREGAL.C\nAzar PA\n(c) Block Dude\nFigure 1: Our method with optimal teacher has similar performance as the optimal policy. And the REGAL algorithm (no\nadvice) outperform random teacher and worst teacher group which justiﬁes that the poorer teachers do harm the learning.\nAzar’s method depends on the quality of the teachers — when the teachers are very poor, the algorithm shows no learning.\nThis is achievable as our method allows for learning within\nthe multiple teacher framework.\n7\nRelated Work on Transfer Learning\nFew theoretical results on transfer and policy advice have\nbeen achieved.\nClosest to this work is that in Taylor et\nal. [2014], where the authors only provide empirical valida-\ntions to their approach without drawing on any theoretical\nanalysis. Given the theoretical derivations in this paper, we\nin fact note that the method [Taylor et al., 2014] is a special\ncase of ours considering only one-teacher advice models.\nAnother method considering advice under multiple teach-\ners is that in Azar et al. [2013]. Azar et al. propose a method\ncapable of selecting the best policy from a set of teacher\npolicies and derive sub-linear regret of the form O(\n√\nT)\nwith T being the total number of rounds. One drawback of\ntheir method, however, is the assumption of a “good-enough”\nteacher which can guide the student to optimality. Such a\nmethod may suffer huge regret if the overall quality of teacher\npolicies is poor. It also can not obtain better policies than\nthose of the teacher. Our algorithm remedies these problems\nby allowing agents to further improve, which gives them the\nopportunity to surpass the teacher’s performance.\nHuman advice is also a good source of policy advice. Usu-\nally, this method adopts the human advice as the teacher’s\npolicy to improve the learning performance. However, these\nworks focus on empirical validations [Cakmak and Lopes,\n2012; Grifﬁth et al., 2013].\nProbabilistic policy reuse is similar to our method in which\nthe algorithm follows its own knowledge with probability\n1 −ϵ and teacher’s policy with probability ϵ [Fern´andez and\nVeloso, 2006]. However, ϵ is not decaying over time, making\nthe algorithm divergent if teacher policies are not optimal.\nCederborg et al. introduce a policy shaping algorithm using\nhuman teachers, but focus on providing rewards rather than\naction advice [Cederborg et al., 2015]. Both of these works\nrely solely on empirical results.\nWork on transfer for RL is also related to this paper, where\nwe can consider policy advice as an instance of transferring\nfrom teachers to students [Lazaric, 2012].\nHere, Ferrante\net al., for instance, propose a method to transfer high qual-\nity samples from source to target tasks using bi-simulation\nmeasures [Ferrante et al., 2008]. Their method only transfers\nsamples once, while our approach gradually provides advice\nto the student. Due to space constraints, we refer the reader\nto Taylor and Stone [2009] for a comprehensive survey.\nLifelong reinforcement learning has drawn signiﬁcant at-\ntention to the transfer community recently. Brunskill and Li\nstudied online discovery problems in a lifelong learning set-\nting [Brunskill and Li, 2015]. Bou-Ammar et al. also studied\nsuch a problem and introduced constraints on the policy to\ncompute “safe” policies [Bou-Ammar et al., 2015]. Contrary\nto these works, in this paper, we focus on the single agent\nsetting operating within one task.\nFinally, Learning from Demonstration [Argall et al., 2009]\n(LfD) is also related to our work, but LfD usually assumes\nthat the expert is optimal and the student only tries to mimic\nthe expert.\n8\nConclusion and Future Work\nIn this paper, we formally deﬁned the multi-teacher ad-\nvice model and introduced a new algorithm which leverages\nteacher and student’s own knowledge in the weakly commu-\nnicating MDPs. We theoretically analyzed our algorithm and\nshowed, for the ﬁrst time, that the agent can achieve optimal-\nity even when starting from non-optimal teachers. Our results\nprovide a theoretical justiﬁcation for the intuition that “bad”\nteachers can hurt the learning process of the student. Also, we\nformally established the condition of negative transfer, shed-\nding light on future transfer learning research, where for ex-\nample, researchers can choose “good teachers” based on the\nEq 6 and avoid negative transfer with prior expert knowledge.\nIn future, we plan on adopting other online reinforcement\nlearning algorithms (e.g., REGAL.D [Bartlett and Tewari,\n2009], R-max [Brafman and Tennenholtz, 2003], or E3\n[Kearns and Singh, 2002]) to replace REGAL.C. We will\nprovide better methods to construct the “grand-teacher” with-\nout exploring the whole MDP. Also, extensions to large-scale\nMDPs may be an interesting direction for future research as\nwell.\n9\nAcknowledgements\nThis research has taken place in part at the Intelligent Robot\nLearning (IRL) Lab, Washington State University. IRL re-\nsearch is supported in part by grants AFRL FA8750-14-1-\n0069, AFRL FA8750-14-1-0070, NSF IIS-1149917, NSF\nIIS- 1319412, USDA 2014-67021-22174, and a Google Re-\nsearch Award.\nReferences\n[Argall et al., 2009] Brenna D Argall,\nSonia Chernova,\nManuela Veloso, and Brett Browning. A survey of robot\nlearning from demonstration. Robotics and autonomous\nsystems, 57(5):469–483, 2009.\n[Audibert et al., 2007] Jean-Yves Audibert, R´emi Munos,\nand Csaba Szepesv´ari.\nTuning bandit algorithms in\nstochastic environments. In Algorithmic Learning Theory,\npages 150–165. Springer, 2007.\n[Auer et al., 2009] Peter Auer, Thomas Jaksch, and Ronald\nOrtner.\nNear-optimal regret bounds for reinforcement\nlearning. In Advances in neural information processing\nsystems, pages 89–96, 2009.\n[Azar et al., 2013] Mohammad Gheshlaghi Azar, Alessan-\ndro Lazaric, Brunskill Emma, et al.\nRegret bounds\nfor reinforcement learning with policy advice.\nIn\nECML/PKDD-European Conference on Machine Learn-\ning and Principles and practice of knowledge Discovery\nin Database, 2013.\n[Bartlett and Tewari, 2009] Peter L Bartlett and Ambuj\nTewari. Regal: A regularization based algorithm for rein-\nforcement learning in weakly communicating MDPs. In\nProceedings of the Twenty-Fifth Conference on Uncer-\ntainty in Artiﬁcial Intelligence, pages 35–42. AUAI Press,\n2009.\n[Bou-Ammar et al., 2015] Haitham Bou-Ammar, Rasul Tu-\ntunov, and Eric Eaton.\nSafe policy search for life-\nlong reinforcement learning with sublinear regret. CoRR,\nabs/1505.05798, 2015.\n[Brafman and Tennenholtz, 2003] Ronen\nI\nBrafman\nand\nMoshe Tennenholtz.\nR-max-a general polynomial time\nalgorithm for near-optimal reinforcement learning.\nThe\nJournal of Machine Learning Research, 3:213–231, 2003.\n[Brunskill and Li, 2015] Emma Brunskill and Lihong Li.\nThe online discovery problem and its application to life-\nlong reinforcement learning.\nCoRR, abs/1506.03379,\n2015.\n[Cakmak and Lopes, 2012] Maya\nCakmak\nand\nManuel\nLopes. Algorithmic and human teaching of sequential de-\ncision tasks. In AAAI Conference on Artiﬁcial Intelligence\n(AAAI-12), 2012.\n[Cederborg et al., 2015] Thomas Cederborg, Ishaan Grover,\nCharles L. Isbell, and Andrea L Thomaz. Policy Shap-\ning With Human Teachers. In Proceedings of the Twenty-\nFourth International Joint Conference on Artiﬁcial Intelli-\ngence (IJCAI), 2015.\n[Erez and Smart, 2008] Tom Erez and William D Smart.\nWhat does shaping mean for computational reinforcement\nlearning?\nIn Development and Learning, 2008. ICDL\n2008. 7th IEEE International Conference on, pages 215–\n219. IEEE, 2008.\n[Fern´andez and Veloso, 2006] Fernando\nFern´andez\nand\nManuela Veloso.\nProbabilistic policy reuse in a rein-\nforcement learning agent.\nIn Proceedings of the ﬁfth\ninternational joint conference on Autonomous agents and\nmultiagent systems, pages 720–727. ACM, 2006.\n[Ferrante et al., 2008] Eliseo Ferrante, Alessandro Lazaric,\nand Marcello Restelli.\nTransfer of task representation\nin reinforcement learning using policy-based proto-value\nfunctions.\nIn Proceedings of the 7th international joint\nconference on Autonomous agents and multiagent systems-\nVolume 3, pages 1329–1332. International Foundation for\nAutonomous Agents and Multiagent Systems, 2008.\n[Grifﬁth et al., 2013] Shane Grifﬁth, Kaushik Subramanian,\nJonathan Scholz, Charles Isbell, and Andrea L Thomaz.\nPolicy shaping: Integrating human feedback with rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems, pages 2625–2633, 2013.\n[Kearns and Singh, 2002] Michael\nKearns\nand\nSatinder\nSingh.\nNear-optimal reinforcement learning in poly-\nnomial time.\nMachine Learning,\n49(2-3):209–232,\n2002.\n[Lazaric, 2012] Alessandro Lazaric. Transfer in reinforce-\nment learning: a framework and a survey. In Reinforce-\nment Learning, pages 143–173. Springer, 2012.\n[MacGlashan, 2014] James MacGlashan.\nThe Brown-\nUMBC Reinforcement Learning and Planning (BURLAP)\nhttp://burlap.cs.brown.edu/index.html, 2014.\n[Mohri et al., 2012] Mehryar Mohri, Afshin Rostamizadeh,\nand Ameet Talwalkar. Foundations of machine learning.\nMIT press, 2012.\n[Puterman, 2005] Martin L Puterman. Markov decision pro-\ncesses: Discrete stochastic dynamic programming (wiley\nseries in probability and statistics). 2005.\n[Ross et al., 2010] St´ephane Ross, Geoffrey J Gordon, and\nJ Andrew Bagnell. A reduction of imitation learning and\nstructured prediction to no-regret online learning. arXiv\npreprint arXiv:1011.0686, 2010.\n[Sutton and Barto, 1998] Richard S Sutton and Andrew G\nBarto. Introduction to reinforcement learning. MIT Press,\n1998.\n[Taylor and Stone, 2009] Matthew E Taylor and Peter Stone.\nTransfer learning for reinforcement learning domains: A\nsurvey.\nThe Journal of Machine Learning Research,\n10:1633–1685, 2009.\n[Taylor et al., 2014] Matthew E. Taylor, Nicholas Carboni,\nAnestis Fachantidis, Ioannis Vlahavas, and Lisa Torrey.\nReinforcement learning agents providing advice in com-\nplex video games.\nConnection Science, 26(1):45–63,\n2014.\n[Whitehead, 1991] Steven D Whitehead.\nComplexity and\ncooperation in q-learning. In Proceedings of the Eighth\nInternational Workshop on Machine Learning, pages 363–\n367, 1991.\n[Zimmer et al., 2014] Matthieu Zimmer, Paolo Viappiani,\nand Paul Weng.\nTeacher-Student Framework: a Rein-\nforcement Learning Approach. In AAMAS Workshop Au-\ntonomous Robots and Multirobot Systems, 2014.\nA\nProof of Theorem 1\nProof of Theorem 1. Let Zr\ni denotes the event that the i-th\nstate, si, is not visited in the ﬁrst r explorations.\nP[Zr\ni ] = (1 −1\n|S|)r ≤e−r/|S|.\nIf we choose r = c|S| log |S|, where c is a constant,\nP[Zr\ni ] ≤e−r/|S| = e−c log |S| = |S|−c.\nLet T be the number of steps at least one of all state is not\nvisited.\nP[T > c|S| log |S|] = P [∃i : Zr\ni s. t. r = c|S| log |S|]\n= P\n\n\n|S|\n[\ni=1\nZc|S| log |S|\ni\n\n\nApply the union bound\n≤\n|S|\n[\ni=1\nP\nh\nZc|S| log |S|\ni\ni\n≤|S| · |S|−c = |S|1−c\nSet |S|1−c = δ, we have\nT ≤|S| log |S|\nδ\n= O\n\u0012\n|S| log |S|\nδ\n\u0013\nwith probability at least 1 −δ.\nB\nProofs of Theorem 2 and Corollary 1\nNext, we will prove Theorem 2. To review some notation, we\nuse Ni(s, a) to denote the number of times a state-action pair\n(s, a) at iteration i. And vi(s, a) = Ni+1(s, a) −Ni(s, a)\ndenotes the number of times a state-action pair (s, a) is vis-\nited during iteration i. Let ∆i be the regret incurred in itera-\ntion i,\n∆i =\nX\ns,a\nvi(s, a) (λ∗−R(s, a)) .\n(7)\nThe total regret equals\nm\nX\ni=1\n∆i,\nwhere m is the number of iterations in Algorithm 3. Auer\net al. show that m ≤|S||A| log(8T/|S||A|) if T ≥|S||A|\n[Auer et al., 2009]. Let\nXi =\n\u001a1\nwith probabiltiy 1 −βi\n0\nwith probabiltiy βi.\nThat is, Xi is the indicator random variable for the iteration i.\nLemma 1. Consider an iteration i. Then, we can decompose\nthe regret as two components,\n∆i = Xi ˜∆i + (1 −Xi)∆T\ni ,\nwhere ˜∆i and ∆T\ni are the regret incurred in iteration i fol-\nlowing the policy πi and the grand-teacher policy πT, respec-\ntively.\nProof. According to Equation (7), we have\n∆i =\nX\ns,a\nvi(s, a) (λ∗−R(s, a))\nAt each decision step, the student agent either follows πi or\nthe teacher πT\n=\n X\ns\nXivi(s, πi(s)) (λ∗−R(s, πi(s)))\n!\n+\n X\ns\n(1 −Xi)vi(s, πT(s))\n\u0000λ∗−R(s, πT(s))\n\u0001\n!\nXi is not related to state s\n= Xi\n X\ns\nvi(s, πi(s)) (λ∗−R(s, πi(s)))\n!\n+ (1 −Xi)\n X\ns\nvi(s, πT(s))\n\u0000λ∗−R(s, πT(s))\n\u0001\n!\n= Xi ˜∆i + (1 −Xi)∆T\ni\nwhere ˜∆i = P\ns vi(s, πi(s)) (λ∗−R(s, πi(s))) is the re-\ngret that the student agent only follows πi and ∆T\ni\n=\nP\ns vi(s, πT(s))\n\u0000λ∗−R(s, πT(s))\n\u0001\nis the regret that the\nstudent agent only follows the grand teacher’s advice πT.\nLemma 2. The total regret Pm\ni=1 ∆i has following upper\nbound,\nm\nX\ni=1\n∆i ≤max\n1≤i≤m{ρi}\n m\nX\ni=1\n˜∆i −\nm\nX\ni=1\nXi ˜∆i\n!\n+\nm\nX\ni=1\nXi ˜∆i,\n(8)\nwhere ρi ≥0 is the ratio such that ∆T\ni = ρi ˜∆i, i = 1, . . . , m.\nProof. Consider an iteration i. Lemma 1 implies\n∆i = Xi ˜∆i + (1 −Xi)∆T\ni .\nTherefore,\nm\nX\ni=1\n∆i =\nm\nX\ni=1\n\u0010\nXi ˜∆i + (1 −Xi)∆T\ni\n\u0011\n=\nm\nX\ni=1\n\u0010\nXi ˜∆i + (1 −Xi)ρi ˜∆i\n\u0011\nAssume that ∆T\ni = ρi ˜∆i. Note that we introduce the regret\nratio ρi in Deﬁnition 2.\n=\nm\nX\ni=1\n\u0010\nρi ˜∆i + Xi ˜∆i −Xiρi ˜∆i\n\u0011\n=\nm\nX\ni=1\nρi ˜∆i −\nm\nX\ni=1\nXiρi ˜∆i +\nm\nX\ni=1\nXi ˜∆i\n≤max\n1≤i≤m{ρi}\n m\nX\ni=1\n˜∆i −\nm\nX\ni=1\nXi ˜∆i\n!\n+\nm\nX\ni=1\nXi ˜∆i\nHence, we will bound Pm\ni=1 ˜∆i and Pm\ni=1 Xi ˜∆i, separately.\nLemma 3 (Theorem 2 in [Bartlett and Tewari, 2009] ). With\nprobability at least 1 −δ, the total regret Pm\ni=1 ˜∆i of RE-\nGAL.C algorithm satisﬁes\nm\nX\ni=1\n˜∆i = O\n\u0010\nH|S|\np\n|A|T log(|A|T/δ)\n\u0011\n,\nwhere 0 < βi < 1 is the decaying variable in Algorithm 3.\nThe above Lemmas gives the upper bound of Pm\ni=1 ˜∆i,\nwhich is given by Theorem 2 in [Bartlett and Tewari, 2009].\nFor Pm\ni=1 Xi ˜∆i, we have following bound:\nLemma 4. With probability at least 1 −δ,\nm\nX\ni=1\nXi ˜∆i = O\n\u0012\nmax\n1≤i≤m{1 −βi}H|S|\np\n|A|T log(|A|T/δ)\n\u0013\nProof. Barlett and Tewari gives following bound [Bartlett\nand Tewari, 2009], with probability 1 −δ/2\nm\nX\ni=1\n˜∆i ≤H\n m\nX\ni=1\nX\ns,a\n2vi(s, a)\np\nNi(s, a)\np\n12|S| log(4|A|T/δ)\n+\np\n2T log(2/δ) + m +\n√\nT\n\u0011\nWith this result and Xi ≤1,\nm\nX\ni=1\nXi ˜∆i ≤H\n m\nX\ni=1\nXi\nX\ns,a\n2vi(s, a)\np\nNi(s, a)\np\n12|S| log(2|A|T/δ)\n+\np\n2T log(1/δ) + m +\n√\nT\n\u0011\n(9)\nSince the ﬁrst term\nm\nX\ni=1\nXi\nX\ns,a\n2vi(s, a)\np\nNi(s, a)\np\n12|S| log(2|A|T/δ)\ndominates the right-hand side, we need to bound it carefully.\nLet\nci =\nX\ns,a\n2vi(s, a)\np\nNi(s, a)\np\n12|S| log(2|A|T/δ)\nand\nZi = Xici,\nZ1, Z2, . . . Zm are independent random variables with Zi\nsuch that 0 ≤Zi ≤ci. Then apply Hoeffding’s inequal-\nity [Mohri et al., 2012], we obtain, with probability at least\n1 −δ/2,\nm\nX\ni=1\nZi ≤E\n\" m\nX\ni=1\nZi\n#\n−\nrPm\ni=1 ci log 2/δ\n2\n.\n(10)\nDue to the linearity of expectation,\nE\n\" m\nX\ni=1\nZi\n#\n=\nm\nX\ni=1\nE [Zi] =\nm\nX\ni=1\nE [Xi] ci =\nm\nX\ni=1\n(1 −βi)ci,\nCombining this with Eq. (10),\nm\nX\ni=1\nXici =\nm\nX\ni=1\nZi\n≤\nm\nX\ni=1\n(1 −βi)ci −\nrPm\ni=1 ci log 2/δ\n2\n≤\nm\nX\ni=1\n(1 −βi)ci\n=\nm\nX\ni=1\n(1 −βi)\nX\ns,a\n2vi(s, a)\np\nNi(s, a)\np\n12|S| log(4|A|T/δ)\n≤max\n1≤i≤m{1 −βi}\nm\nX\ni=1\nX\ns,a\n2vi(s, a)\np\nNi(s, a)\np\n12|S| log(4|A|T/δ).\nPlugging it into Eq. (9), we get\nm\nX\ni=1\nXi ˜∆i\n≤H\n \nmax\n1≤i≤m{1 −βi}\nm\nX\ni=1\nX\ns,a\n2vi(s, a)\np\nNi(s, a)\np\n12|S| log(4|A|T/δ)\n+\np\n2T log(2/δ) + m +\n√\nT\n\u0011\n(11)\nEq (20) in [Auer et al., 2009] gives\nm\nX\ni=1\nX\ns,a\nvi(s, a)\np\nNi(s, a)\n≤(\n√\n2 + 1)\np\n|S||A|T,\nSubstitute this for Eq. (11) and Eq. (20) in [Auer et al., 2009]\n(m ≤|S||A| log2(8T/|S||A|) if T ≥|S||A|), yielding,\nm\nX\ni=1\nXi ˜∆i = O\n\u0012\nmax\n1≤i≤m{1 −βi}H|S|\np\n|A|T log(|A|T/δ)\n\u0013\n,\nwith probability 1 −δ.\nProof of Theorem 2. Using Lemma 3 and 4, with probability\nat least 1 −δ,\nm\nX\ni=1\n˜∆i = O\n\u0010\nH|S|\np\n|A|T log(|A|T/δ)\n\u0011\n,\nand\nm\nX\ni=1\nXi ˜∆i = O\n\u0012\nmax\n1≤i≤m{1 −βi}H|S|\np\n|A|T log(|A|T/δ)\n\u0013\n.\nCombining with Eq. 8, we have,\nm\nX\ni=1\n∆i ≤max\n1≤i≤m{ρi}O\n\u0010\nH|S|\np\n|A|T log(|A|T/δ)\n\u0011\n−max\n1≤i≤m{ρi}O\n\u0012\nmax\n1≤i≤m{1 −βi}H|S|\np\n|A|T log(|A|T/δ)\n\u0013\n+ O\n\u0012\nmax\n1≤i≤m{1 −βi}H|S|\np\n|A|T log(|A|T/δ)\n\u0013\nLet ρ = max1≤i≤m{ρi} and 1 −β = max1≤i≤m{1 −βi}\n= O\n\u0010\n(1 −β + ρβ)H|S|\np\n|A|T log(|A|T/δ)\n\u0011\nProof of Corollary 1. If the teacher is optimal, then ∆T\ni\n=\n0 = ρi ˜∆i, that is ρi = 0 for i = 1, . . . , m. Therefore, ρ = 0.\nThe result follows.\nC\nDomains GUI Examples\nHere we provide the GUI examples of Grid World and Block\nDude which are used as the experimental domains in the main\npaper.\nFigure 3: An GUI example of the Grid World. The map is\nseparated into four rooms by the wall. An agent in the lower\nleft corner tries to reach the goal state in the upper right cor-\nner.\nFigure 4: An GUI example of the Block Dude. The agent\nneeds to move the blocks to assist it to reach the goal state.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-04-13",
  "updated": "2016-04-13"
}