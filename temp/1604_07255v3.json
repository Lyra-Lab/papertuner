{
  "id": "http://arxiv.org/abs/1604.07255v3",
  "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft",
  "authors": [
    "Chen Tessler",
    "Shahar Givony",
    "Tom Zahavy",
    "Daniel J. Mankowitz",
    "Shie Mannor"
  ],
  "abstract": "We propose a lifelong learning system that has the ability to reuse and\ntransfer knowledge from one task to another while efficiently retaining the\npreviously learned knowledge-base. Knowledge is transferred by learning\nreusable skills to solve tasks in Minecraft, a popular video game which is an\nunsolved and high-dimensional lifelong learning problem. These reusable skills,\nwhich we refer to as Deep Skill Networks, are then incorporated into our novel\nHierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using\ntwo techniques: (1) a deep skill array and (2) skill distillation, our novel\nvariation of policy distillation (Rusu et. al. 2015) for learning skills. Skill\ndistillation enables the HDRLN to efficiently retain knowledge and therefore\nscale in lifelong learning, by accumulating knowledge and encapsulating\nmultiple reusable skills into a single distilled network. The H-DRLN exhibits\nsuperior performance and lower learning sample complexity compared to the\nregular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.",
  "text": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft\nChen Tessler∗, Shahar Givony∗, Tom Zahavy∗, Daniel J. Mankowitz∗, Shie Mannor\n∗equally contributed\nTechnion Israel Institute of Technology, Haifa, Israel\nchen.tessler, shahargiv, tomzahavy {@campus.technion.ac.il }, danielm@tx.technion.ac.il, shie@ee.technion.ac.il\nAbstract\nWe propose a lifelong learning system that has the ability\nto reuse and transfer knowledge from one task to another\nwhile efﬁciently retaining the previously learned knowledge-\nbase. Knowledge is transferred by learning reusable skills\nto solve tasks in Minecraft, a popular video game which is\nan unsolved and high-dimensional lifelong learning problem.\nThese reusable skills, which we refer to as Deep Skill Net-\nworks, are then incorporated into our novel Hierarchical Deep\nReinforcement Learning Network (H-DRLN) architecture us-\ning two techniques: (1) a deep skill array and (2) skill dis-\ntillation, our novel variation of policy distillation (Rusu et\nal. 2015) for learning skills. Skill distillation enables the H-\nDRLN to efﬁciently retain knowledge and therefore scale in\nlifelong learning, by accumulating knowledge and encapsu-\nlating multiple reusable skills into a single distilled network.\nThe H-DRLN exhibits superior performance and lower learn-\ning sample complexity compared to the regular Deep Q Net-\nwork (Mnih et al. 2015) in sub-domains of Minecraft.\nIntroduction\nLifelong learning considers systems that continually learn\nnew tasks, from one or more domains, over the course of a\nlifetime. Lifelong learning is a large, open problem and is\nof great importance to the development of general purpose\nArtiﬁcially Intelligent (AI) agents. A formal deﬁnition of\nlifelong learning follows.\nDeﬁnition 1. Lifelong Learning is the continued learning\nof tasks, from one or more domains, over the course of a\nlifetime, by a lifelong learning system. A lifelong learning\nsystem efﬁciently and effectively (1) retains the knowledge\nit has learned; (2) selectively transfers knowledge to\nlearn new tasks; and (3) ensures the effective and efﬁcient\ninteraction between (1) and (2)(Silver, Yang, and Li 2013).\nA truly general lifelong learning system, shown in Figure\n2, therefore has the following attributes: (1) Efﬁcient reten-\ntion of learned task knowledge A lifelong learning system\nshould minimize the retention of erroneous knowledge. In\naddition, it should also be computationally efﬁcient when\nstoring knowledge in long-term memory. (2) Selective\ntransfer: A lifelong learning system needs the ability to\nchoose relevant prior knowledge for solving new tasks,\nFigure 1: A screenshot from Minecraft, a popular video\ngame which poses a challenging lifelong learning problem.\nwhile casting aside irrelevant or obsolete information. (3)\nSystem approach:\nEnsures the effective and efﬁcient\ninteraction of the retention and transfer elements.\nLifelong learning systems in real-world domains suffer\nfrom the curse of dimensionality. That is, as the state and\naction spaces increase, it becomes more and more difﬁcult\nto model and solve new tasks as they are encountered. In\naddition, planning over potentially inﬁnite time-horizons\nas well as efﬁciently retaining and reusing knowledge pose\nnon-trivial challenges. A challenging, high-dimensional\ndomain that incorporates many of the elements found\nin lifelong learning is Minecraft. Minecraft is a popular\nvideo game whose goal is to build structures, travel on\nadventures, hunt for food and avoid zombies. An example\nscreenshot from the game is seen in Figure 1. Minecraft is\nan open research problem as it is impossible to solve the\nentire game using a single AI technique (Smith and Aha ;\nOh et al. 2016). Instead, the solution to Minecraft may\nlie in solving sub-problems, using a divide-and-conquer\napproach, and then providing a synergy between the various\nsolutions. Once an agent learns to solve a sub-problem, it\nhas acquired a skill that can then be reused when a similar\nsub-problem is subsequently encountered.\narXiv:1604.07255v3  [cs.AI]  30 Nov 2016\nFigure 2: Lifelong Learning: A lifelong learning system\n(1) efﬁciently retains knowledge and (2) selectively trans-\nfers knowledge to solve new tasks. Upon solving a task, the\nknowledge base is reﬁned and new knowledge is added to\nthe system. A systems approach ensures efﬁcient and effec-\ntive interaction between (1) and (2).\nMany of the tasks that are encountered by an agent\nin\na\nlifelong\nlearning\nsetting\ncan\nbe\nnaturally\nde-\ncomposed into skill hierarchies (Stone et al. 2000;\nStone, Sutton, and Kuhlmann 2005; Bai, Wu, and Chen\n2015). In Minecraft for example, consider building a\nwooden house as seen in Figure 1. This task can be de-\ncomposed into sub-tasks (a.k.a skills) such as chopping\ntrees, sanding the wood, cutting the wood into boards and\nﬁnally nailing the boards together. Here, the knowledge\ngained from chopping trees can also be partially reused\nwhen cutting the wood into boards. In addition, if the agent\nreceives a new task to build a small city, then the agent can\nreuse the skills it acquired during the ‘building a house’ task.\nIn a high-dimensional, lifelong learning setting such as\nMinecraft, learning skills and when to reuse the skills is\nnon-trivial. This is key to efﬁcient knowledge retention and\ntransfer, increasing exploration, efﬁciently solving tasks and\nultimately advancing the capabilities of the Minecraft agent.\nReinforcement Learning (RL) provides a generalized\napproach to skill learning through the options framework\n(Sutton, Precup, and Singh 1999). Options are Temporally\nExtended Actions (TEAs) and are also referred to as skills\n(da Silva, Konidaris, and Barto 2012) and macro-actions\n(Hauskrecht et al. 1998). Options have been shown both\ntheoretically (Precup and Sutton 1997; Sutton, Precup, and\nSingh 1999) and experimentally (Mann and Mannor 2013;\nMankowitz, Mann, and Mannor 2014) to speed up the\nconvergence rate of RL algorithms. From here on in, we\nwill refer to options as skills.\nIn order to learn reusable skills in a lifelong learning\nsetting, the framework needs to be able to (1) learn skills,\n(2) learn a controller which determines when a skill should\nbe used and reused and (3) be able to efﬁciently accumu-\nlate reusable skills. There are recent works that perform\nskill learning (Mankowitz, Mann, and Mannor 2016a;\nMankowitz, Mann, and Mannor 2016b; Mnih et al. 2016a;\nBacon and Precup 2015), but these works have focused\non learning good skills and have not explicitly shown the\nability to reuse skills nor scale with respect to the number\nof skills in lifelong learning domains.\nWith the emergence of Deep RL, speciﬁcally Deep\nQ-Networks\n(DQNs),\nRL\nagents\nare\nnow\nequipped\nwith a powerful non-linear function approximator that\ncan learn rich and complex policies (or skills). Us-\ning these networks the agent learns policies (or skills)\nfrom\nraw\nimage\npixels,\nrequiring\nless\ndomain\nspe-\nciﬁc knowledge to solve complicated tasks (E.g Atari\nvideo games). While different variations of the DQN\nalgorithm exist (Van Hasselt, Guez, and Silver 2015;\nSchaul et al. 2015; Wang, de Freitas, and Lanctot 2015;\nBellemare et al. 2015), we will refer to the vanilla version\nunless otherwise stated. There are deep learning ap-\nproaches that perform sub-goal learning (Rusu et al. 2016;\nKulkarni et al. 2016), yet these approaches rely on providing\nthe task or sub-goal to the agent, prior to making a decision.\nKulkarni et al. (2016) also rely on manually constructing\nsub-goals a-priori for tasks and utilize intrinsic motivation\nwhich may be problematic for complicated problems\nwhere designing good intrinsic motivations is not clear and\nnon-trivial.\nIn our paper, we present our novel lifelong learning sys-\ntem called the Hierarchical Deep Reinforcement Learning\n(RL) Network (H-DRLN) architecture shown in Figure 3 (It\nis deﬁned formally in the Hierarchical Deep RL Network\nSection). While we do not claim to provide an end-to-end\nsolution, the H-DRLN contains all the basic building\nblocks of a truly general lifelong learning framework (see\nthe Related Work Section for an in-depth overview). The\nH-DRLN controller learns to solve complicated tasks in\nMinecraft by learning reusable RL skills in the form of\npre-trained Deep Skill Networks (DSNs). Knowledge is\nretained by incorporating reusable skills into the H-DRLN\nvia a Deep Skill module. There are two types of Deep Skill\nModules: (1) a DSN array (Figure 3, Module A) and (2)\na multi-skill distillation network (Figure 3, Module B),\nour novel variation of policy distillation (Rusu et al. 2015)\napplied to learning skills. Multi-skill distillation enables the\nH-DRLN to efﬁciently retain knowledge and therefore scale\nin lifelong learning, by encapsulating multiple reusable\nskills into a single distilled network. When solving a new\ntask, the H-DRLN selectively transfers knowledge in the\nform of temporal abstractions (skills) to solve the given\ntask. By taking advantage of temporally extended actions\n(skills), the H-DRLN learns to solve tasks with lower\nsample complexity and superior performance compared to\nvanilla DQNs.\nMain Contributions: (1) A novel Hierarchical Deep\nReinforcement Learning Network (H-DRLN) architecture\nwhich includes an H-DRLN controller and a Deep Skill\nModule. The H-DRLN contains all the basic building blocks\nfor a truly general lifelong learning framework. (2) We show\nthe potential to learn reusable Deep Skill Networks (DSNs)\nand perform knowledge transfer of the learned DSNs to new\ntasks to obtain an optimal solution. We also show the po-\ntential to transfer knowledge between related tasks without\nany additional learning. (3) We efﬁciently retain knowledge\nin the H-DRLN by performing skill distillation, our varia-\ntion of policy distillation, for learning skills and incorporate\nit into the Deep Skill Model to solve complicated tasks in\nMinecraft. (4) Empirical results for learning an H-DRLN\nin sub-domains of Minecraft with a DSN array and a dis-\ntilled skill network. We also verify the improved conver-\ngence guarantees for utilizing reusable DSNs (a.k.a options)\nwithin the H-DRLN, compared to the vanilla DQN.\nPrevious Research on Lifelong Learning in RL\nDesigning a truly general lifelong learning agent is a\nchallenging task. Previous works on lifelong learning in RL\nhave focused on solving speciﬁc elements of the general\nlifelong learning system as shown in Table 1.\nAccording to Deﬁnition 1, a lifelong learning agent\nshould be able to efﬁciently retain knowledge. This is\ntypically done by sharing a representation among tasks,\nusing distillation (Rusu et al. 2015) or a latent basis (Ammar\net al. 2014). The agent should also learn to selectively use\nits past knowledge to solve new tasks efﬁciently. Most\nworks have focused on a spatial transfer mechanism, i.e.,\nthey suggested learning differentiable weights from a shared\nrepresentation to the new tasks (Jaderberg et al. 2016;\nRusu et al. 2016). In contrast, Brunskill and Li (2014)\nsuggested a temporal transfer mechanism, which identiﬁes\nan optimal set of skills in past tasks and then learns to use\nthese skills in new tasks. Finally, the agent should have\na systems approach that allows it to efﬁciently retain\nthe knowledge of multiple tasks as well as an efﬁcient\nmechanism to transfer knowledge for solving new tasks.\nOur work incorporates all of the basic building blocks\nnecessary to performing lifelong learning. As per the life-\nlong learning deﬁnition, we efﬁciently transfer knowledge\nfrom previous tasks to solve a new target task by utilizing RL\nskills (Sutton, Precup, and Singh 1999). We show that skills\nreduce the sample complexity in a complex Minecraft en-\nvironment and suggest an efﬁcient mechanism to retain the\nknowledge of multiple skills that is scalable with the number\nof skills.\nBackground\nReinforcement Learning: The goal of an RL agent is to\nmaximize its expected return by learning a policy π : S →\n∆A which is a mapping from states s ∈S to a probabil-\nity distribution over the actions A. At time t the agent ob-\nWorks\nH-DRLN\nAmmar\nBrunskill\nRusu\nRusu\nJaderberg\n(this work)\net. al\nand Li\net. al.\net. al.\nAttribute\n(2014)\n(2014)\n(2015)\n(2016)\n(2016)\nMemory\nefﬁcient\n✓\n✓\n\u0017\n✓\n\u0017\n\u0017\nKnowledge\narchitecture\nRetention\nScalable to\nhigh\n✓\n\u0017\n\u0017\n✓\n✓\n✓\ndimensions\nTemporal\nabstraction\n✓\n\u0017\n✓\n\u0017\n\u0017\n\u0017\nSelective\ntransfer\nTransfer\nSpatial\nabstraction\n\u0017\n✓\n\u0017\n✓\n✓\n✓\ntransfer\nMulti-task\n✓\n✓\n✓\n✓\n\u0017\n✓\nSystems\nApproach\nTransfer\n✓\n✓\n✓\n\u0017\n✓\n✓\nTable 1: Previous works on lifelong learning in RL.\nserves a state st ∈S, selects an action at ∈A, and re-\nceives a bounded reward rt ∈[0, Rmax] where Rmax is\nthe maximum attainable reward and γ ∈[0, 1] is the dis-\ncount factor. Following the agents action choice, it tran-\nsitions to the next state st+1 ∈S . We consider inﬁnite\nhorizon problems where the cumulative return at time t is\ngiven by Rt = P∞\nt′=t γt′−trt. The action-value function\nQπ(s, a) = E[Rt|st = s, at = a, π] represents the expected\nreturn after observing state s and taking an action a under a\npolicy π. The optimal action-value function obeys a funda-\nmental recursion known as the Bellman equation:\nQ∗(st, at) = E\nh\nrt + γmax\na′ Q∗(st+1, a′)\ni\n.\nDeep Q Networks: The DQN algorithm (Mnih et al.\n2015) approximates the optimal Q function with a Convolu-\ntional Neural Network (CNN) (Krizhevsky, Sutskever, and\nHinton 2012), by optimizing the network weights such that\nthe expected Temporal Difference (TD) error of the optimal\nbellman equation (Equation 1) is minimized:\nEst,at,rt,st+1 ∥Qθ (st, at) −yt∥2\n2 ,\n(1)\nwhere\nyt =\n\n\n\nrt\nif st+1 is terminal\nrt + γmax\na’\nQθtarget\n\u0010\nst+1, a\n′\u0011\notherwise\nNotice that this is an ofﬂine learning algorithm, mean-\ning that the tuples {st,at, rt, st+1, γ} are collected from\nthe agents experience and are stored in the Experience\nReplay (ER) (Lin 1993). The ER is a buffer that stores\nthe agents experiences at each time-step t, for the purpose\nof ultimately training the DQN parameters to minimize\nthe loss function. When we apply minibatch training\nupdates, we sample tuples of experience at random from\nthe pool of stored samples in the ER. The DQN maintains\ntwo separate Q-networks. The current Q-network with\nparameters θ, and the target Q-network with parameters\nθtarget. The parameters θtarget are set to θ every ﬁxed num-\nber of iterations. In order to capture the game dynamics,\nthe DQN represents the state by a sequence of image frames.\nDouble DQN (Van Hasselt, Guez, and Silver 2015):\nDouble DQN (DDQN) prevents overly optimistic estimates\nof the value function. This is achieved by performing\naction selection with the current network θ and evaluating\nthe action with the target network θtarget yielding the\nDDQN target update yt = rt if st+1 is terminal, otherwise\nyt = rt + γQθtarget(st+1, maxa Qθ(st+1, a)). DDQN is\nutilized in this paper to improve learning performance.\nSkills, Options, Macro-actions (Sutton, Precup, and\nSingh 1999): A skill σ is a temporally extended control\nstructure deﬁned by a triple σ =< I, π, β > where I is\nthe set of states where the skill can be initiated, π is the\nintra-skill policy, which determines how the skill behaves\nin encountered states, and β is the set of termination\nprobabilities determining when a skill will stop executing.\nThe parameter β is typically a function of state s or time t.\nSemi-Markov Decision Process (SMDP): Planning with\nskills can be performed using SMDP theory. More formally,\nan SMDP can be deﬁned by a ﬁve-tuple < S, Σ, P, R, γ >\nwhere S is a set of states, Σ is a set of skills, and P\nis the transition probability kernel. We assume rewards\nreceived at each timestep are bounded by [0, Rmax].\nR : S × σ →[0, Rmax\n1−γ ] represents the expected discounted\nsum of rewards received during the execution of a skill σ\ninitialized from a state s. The solution to an SMDP is a skill\npolicy µ.\nSkill Policy: A skill policy µ : S →∆Σ is a map-\nping from states to a probability distribution over skills Σ.\nThe action-value function Q : S × Σ →R represents\nthe long-term value of taking a skill σ ∈Σ from a state\ns ∈S and thereafter always selecting skills according to pol-\nicy µ and is deﬁned by Q(s, σ) = E[P∞\nt=0 γtRt|(s, σ), µ].\nWe denote the skill reward as Rσ\ns = E[rt+1 + γrt+2 +\n· · · + γk−1rt+k|st = s, σ] and transition probability as\nP σ\ns,s′ = P∞\nj=0 γjPr[k = j, st+j = s′|st = s, σ]. Under\nthese deﬁnitions the optimal skill value function is given by\nthe following equation (Stolle and Precup 2002):\nQ∗\nΣ(s, σ) = E[Rσ\ns + γkmax\nσ′∈ΣQ∗\nΣ(s′, σ′)] .\n(2)\nPolicy Distillation (Rusu et al. 2015): Distillation (Hin-\nton, Vinyals, and Dean 2015) is a method to transfer knowl-\nedge from a teacher model T to a student model S. This\nprocess is typically done by supervised learning. For ex-\nample, when both the teacher and the student are sepa-\nrate deep neural networks, the student network is trained\nto predict the teacher’s output layer (which acts as labels\nfor the student). Different objective functions have been\npreviously proposed. In this paper we input the teacher\noutput into a softmax function and train the distilled net-\nwork using the Mean-Squared-Error (MSE) loss: cost(s) =\n∥Softmaxτ(QT (s))−QS(s)∥2 where QT (s) and QS(s) are\nthe action values of the teacher and student networks respec-\ntively and τ is the softmax temperature. During training, this\ncost function is differentiated according to the student net-\nwork weights.\nPolicy distillation can be used to transfer knowledge from\nN teachers Ti, i = 1, · · · N into a single student (multi-task\npolicy distillation). This is typically done by switching be-\ntween the N teachers every ﬁxed number of iterations dur-\ning the training process. When the student is learning from\nmultiple teachers (i.e., multiple policies), a separate student\noutput layer is assigned to each teacher Ti, and is trained for\neach task, while the other layers are shared.\nHierarchical Deep RL Network\nIn this Section, we present an in-depth description of the\nH-DRLN (Figure 3); a new architecture that extends the\nDQN and facilitates skill reuse in lifelong learning. Skills\nare incorporated into the H-DRLN via a Deep Skill Module\nthat can incorporate either a DSN array or a distilled\nmulti-skill network.\nDeep Skill Module:\nThe pre-learned skills are repre-\nsented as deep networks and are referred to as Deep Skill\nNetworks (DSNs). They are trained a-priori on various\nsub-tasks using our version of the DQN algorithm and\nthe regular Experience Replay (ER) as detailed in the\nBackground Section. Note that the DQN is one choice of\narchitecture and, in principal, other suitable networks may\nbe used in its place. The Deep Skill Module represents a set\nof N DSNs. Given an input state s ∈S and a skill index i,\nit outputs an action a according to the corresponding DSN\npolicy πDSNi. We propose two different Deep Skill Module\narchitectures: (1) The DSN Array (Figure 3, module A): an\narray of pre-trained DSNs where each DSN is represented\nby a separate DQN. (2) The Distilled Multi-Skill Network\n(Figure 3, module B), a single deep network that represents\nmultiple DSNs. Here, the different DSNs share all of the\nhidden layers while a separate output layer is trained for\neach DSN via policy distillation (Rusu et al. 2015). The\nDistilled skill network allows us to incorporate multiple\nskills into a single network, making our architecture scalable\nto lifelong learning with respect to the number of skills.\nH-DRLN architecture:\nA diagram of the H-DRLN\narchitecture is presented in Figure 3 (top). Here, the outputs\nof the H-DRLN consist of primitive actions as well as\nskills. The H-DRLN learns a policy that determines when\nto execute primitive actions and when to reuse pre-learned\nskills. If the H-DRLN chooses to execute a primitive action\nat at time t, then the action is executed for a single timestep.\nHowever, if the H-DRLN chooses to execute a skill σi\n(and therefore DSN i as shown in Figure 3), then DSN i\nexecutes its policy, πDSNi(s) until it terminates and then\ngives control back to the H-DRLN. This gives rise to two\nnecessary modiﬁcations that we needed to make in order to\nincorporate skills into the learning procedure and generate a\ntruly hierarchical deep network: (1) Optimize an objective\nfunction that incorporates skills; (2) Construct an ER that\nstores skill experiences.\nModule A\nModule B\nIf a1,a2...am\nelse if DSNi\nπD SNi (s)\nπ(s)=a1,a2 or am\nπ(s)=\nH-DRLN\nFigure 3: The H-DRLN architecture: It has outputs that\ncorrespond to primitive actions (a1, a2, ..., am) and DSNs\n(DSN1, DSN2, ..., DSNn). The Deep Skill Module (bot-\ntom) represents a set of skills. It receives an input and a skill\nindex and outputs an action according to the corresponding\nskill policy. The architecture of the deep skill module can be\neither a DSN array or a Distilled Multi-Skill Network.\nSkill Objective Function: As mentioned previously, a H-\nDRLN extends the vanilla DQN architecture to learn con-\ntrol between primitive actions and skills. The H-DRLN loss\nfunction has the same structure as Equation 1, however in-\nstead of minimizing the standard Bellman equation, we min-\nimize the Skill Bellman equation (Equation 2). More specif-\nically, for a skill σt initiated in state st at time t that has\nexecuted for a duration k, the H-DRLN target function is\ngiven by:\nyt =\n\n\n\nPk−1\nj=0\n\u0002\nγjrj+t\n\u0003\nif st+k terminal\nPk−1\nj=0\n\u0002\nγjrj+t\n\u0003\n+ γkmax\nσ’\nQθtarget\n\u0010\nst+k, σ\n′\u0011\nelse\nThis is the ﬁrst work to incorporate an SMDP cost\nfunction into a deep RL setting.\nSkill - Experience Replay: We extend the regular ER\n(Lin 1993) to incorporate skills and term this the Skill Ex-\nperience Replay (S-ER). There are two differences between\nthe standard ER and our S-ER. Firstly, for each sampled skill\ntuple, we calculate the sum of discounted cumulative re-\nwards, ˜r, generated whilst executing the skill. Second, since\nthe skill is executed for k timesteps, we store the transition\nto state st+k rather than st+1. This yields the skill tuple\n(st, σt, ˜rt, st+k) where σt is the skill executed at time t.\nExperiments\nTo solve new tasks as they are encountered in a lifelong\nlearning scenario, the agent needs to be able to adapt to new\ngame dynamics and learn when to reuse skills that it has\nlearned from solving previous tasks. In our experiments, we\nshow (1) the ability of the Minecraft agent to learn DSNs\non sub-domains of Minecraft (shown in Figure 4a −d). (2)\nThe ability of the agent to reuse a DSN from navigation\ndomain 1 (Figure 4a) to solve a new and more complex task,\ntermed the two-room domain (Figure 5a). (3) The potential\nto transfer knowledge between related tasks without any\nadditional learning. (4) We demonstrate the ability of the\nagent to reuse multiple DSNs to solve the complex-domain\n(Figure 5b). (5) We use two different Deep Skill Modules\nand demonstrate that our architecture scales for lifelong\nlearning.\nState space - As in Mnih et al. (2015), the state space is\nrepresented as raw image pixels from the last four image\nframes which are combined and down-sampled into an\n84 × 84 pixel image. Actions - The primitive action space\nfor the DSN consists of six actions: (1) Move forward, (2)\nRotate left by 30◦, (3) Rotate right by 30◦, (4) Break a\nblock, (5) Pick up an item and (6) Place it. Rewards - In all\ndomains, the agent gets a small negative reward signal after\neach step and a non-negative reward upon reaching the ﬁnal\ngoal (See Figure 4 and Figure 5 for the different domain\ngoals).\nTraining - Episode lengths are 30, 60 and 100 steps\nfor single DSNs, the two room domain and the complex\ndomain respectively. The agent is initialized in a random\nlocation in each DSN and in the ﬁrst room for the two room\nand complex domains. Evaluation - the agent is evaluated\nduring training using the current learned architecture every\n20k (5k) optimization steps (a single epoch) for the DSNs\n(two room and complex room domains). During evaluation,\nwe averaged the agent’s performance over 500 (1000) steps\nrespectively. Success percentage: The % of successful task\ncompletions during evaluation.\nFigure 4: The domains: (a)-(d) are screenshots for each of\nthe domains we used to train the DSNs.\nTraining a DSN\nOur ﬁrst experiment involved training DSNs in sub-domains\nof Minecraft (Figure 4a −d), including two navigation\ndomains, a pickup domain and a placement domain re-\nspectively. The break domain is the same as the placement\ndomain, except it ends with the break action. Each of these\ndomains come with different learning challenges. The\nNavigation 1 domain is built with identical walls, which\nprovides a signiﬁcant learning challenge since there are\nvisual ambiguities with respect to the agent’s location (see\nFigure 4a). The Navigation 2 domain provides a different\nlearning challenge since there are obstacles that occlude the\nagent’s view of the exit from different regions in the room\n(Figure 4b). The pick up (Figure 4c), break and placement\n(Figure 4d) domains require navigating to a speciﬁc location\nand ending with the execution of a primitive action (Pickup,\nBreak or Place respectively).\nIn order to train the different DSNs, we use the Vanilla\nDQN architecture (Mnih et al. 2015) and performed a grid\nsearch to ﬁnd the optimal hyper-parameters for learning\nDSNs in Minecraft. The best parameter settings that we\nfound include: (1) a higher learning ratio (iterations between\nemulator states, n-replay = 16), (2) higher learning rate\n(learning rate = 0.0025) and (3) less exploration (eps endt -\n400K). We implemented these modiﬁcations, since the stan-\ndard Minecraft emulator has a slow frame rate (approxi-\nmately 400 ms per emulator timestep), and these modiﬁ-\ncations enabled the agent to increase its learning between\ngame states. We also found that a smaller experience replay\n(replay memory - 100K) provided improved performance,\nprobably due to our task having a relatively short time hori-\nzon (approximately 30 timesteps). The rest of the parame-\nters from the Vanilla DQN remained unchanged. After we\ntuned the hyper-parameters, all the DSNs managed to solve\nthe corresponding sub-domains with almost 100% success\nas shown in Table 2. (see supplementary material for learn-\ning curves).\nTraining an H-DRLN with a DSN\nIn this experiment, we train the H-DRLN agent to solve a\ncomplex task, the two-room domain, by reusing a single\nDSN (pre-trained on the navigation 1 domain).\nTwo room Domain: This domain consists of two-rooms\n(Figure 5a(iii)). The ﬁrst room is shown in Figure 5a(i)\nwith its corresponding exit (Figure 5a(ii)). Note that the\nexit of the ﬁrst room is not identical to the exit of the\nnavigation 1 domain (Figure 4a). The second room contains\na goal (Figure 5a(iii)) that is the same as the goal of the\nnavigation 1 domain (Figure 4a). The agent’s available\naction set consists of the primitive movement actions and\nthe Navigate 1 DSN.\nSkill Reusability/Knowledge Transfer: We trained the\nH-DRLN architecture as well as the vanilla DQN on the\ntwo-room domain. We noticed two important observations.\n(1) The H-DRLN architecture solves the task after a single\nFigure 5: Composite domains: (a) The two-room domain\nand (b) the complex domain with three different tasks, (i)\nnavigation, (ii) pickup and (iii) placement\nepoch and generates signiﬁcantly higher reward compared\nto the vanilla DQN. This is because the H-DRLN makes\nuse of knowledge transfer by reusing the DSN trained on\nthe one-room domain to solve the two-room domain. This\nDSN is able to identify the exit of the ﬁrst room (which\nis different from the exit on which the DSN was trained)\nand navigates the agent to this exit. The DSN is also able\nto navigate the agent to the exit of the second room and\ncompletes the task. The DSN is a temporally extended\naction as it lasts for multiple time steps and therefore\nincreases the exploration of the RL agent enabling it to\nlearn to solve the task faster than the vanilla DQN. (2)\nAfter 39 epochs, the vanilla DQN completes the task with\n50% success percentage. This sub-optimal performance\nis due to wall ambiguities, causing the agent to get stuck\nin sub-optimal local minima. After the same number of\nepochs, the agent completes the task using the H-DRLN\nwith 76% success.\nKnowledge Transfer without Learning: We then\ndecided to evaluate the DSN (which we trained on the\nnavigation 1 domain) in the two-room domain without\nFigure 6: Two room domain success percentages for the\nvanilla DQN, the single DSN, the H-DRLN after a single\nepoch (START) and in the last epoch (END).\nperforming any additional learning on this network. We\nfound it surprising that the DSN, without any training on\nthe two-room domain, generated a higher reward compared\nto the vanilla DQN which was speciﬁcally trained on the\ntwo-room domain for 39 epochs. Figure 6 summarizes\nthe success percentage comparison between the different\narchitectures in the two-room domain. The vanilla DQN,\nDSN, H-DRLN START and H-DRLN END had average\nsuccess percentages of 50%, 67.65%, 73.08% and 76% re-\nspectively. The DSN performance is sub-optimal compared\nto the H-DRLN architecture but still manages to solve the\ntwo-room domain. This is an exciting result as it shows\nthe potential for DSNs to identify and solve related tasks\nwithout performing any additional learning.\nTraining an H-DRLN with a Deep Skill Module\nIn this section, we discuss our results for training and\nutilizing the H-DRLN with a Deep Skill Module to solve\nthe complex Minecraft domain. In each of the experiments\nin this section, we utilized DDQN to train the H-DRLN and\nthe DDQN baseline unless otherwise stated.\nComplex Minecraft Domain: This domain (Figure 5b)\nconsists of three rooms. Within each room, the agent is\nrequired to perform a speciﬁc task. Room 1 (Figure 5b(i))\nis a navigation task, where the agent needs to navigate\naround the obstacles to reach the exit. Room 2 (Figure\n5b(ii)) contains two tasks. (1) A pickup task whereby the\nagent is required to navigate to and collect a block in the\ncenter of the room; (2) A break task, where the agent needs\nto navigate to the exit and break a door. Finally, Room 3\n(Figure 5b(iii)) is a placement task whereby the agent needs\nto place the block that it collected in the goal location. The\nagent receives a non-negative reward if it successfully navi-\ngates through room 1, collects the block and breaks the door\nin room 2 and places the block in the goal location in room\n3 (Arrow path in Figure 5b). Otherwise, the agent receives a\nsmall negative reward at each timestep. Note that the agent\nneeds to complete three separate tasks before receiving a\nsparse, non-negative reward. The agent’s available action\nset are the original primitive actions as well as the DSN’s:\n(1) Navigate 2, (2) Pickup, (3) Break and (4) Placement.\nTraining and Distilling Multiple DSNs: As mentioned\nin the H-DRLN Section, there are two ways to incorporate\nskills into the Deep Skill Module: (1) DSN Array and (2)\nMulti-Skill Distillation. For both the DSN array and multi-\nskill distillation, we utilize four pre-trained DSNs (Navigate\n2, Pickup, Break and Placement). These DSNs collectively\nform the DSN array. For the multi-skill distillation, we uti-\nlized the pre-trained DSNs as teachers and distil these skills\ndirectly into a single network (the student) using the dis-\ntillation setup shown in Figure 7, and as described in the\nBackground Section. Once trained, we tested the distilled\nnetwork separately in each of the three individual rooms\n(Figure 4b −d). The performance for each room is shown\nin Table 2 for temperatures τ = 0.1 and τ = 1. The high\nsuccess percentages indicate that the agent is able to success-\nfully complete each task using a single distilled network. In\ncontrast to policy distillation, our novelty lies in the ability\nto, not only distil skills into a single network, but also learn a\ncontrol rule (using the H-DRLN) that switches between the\nskills to solve a given task.\nDomain\nτ = 0.1\nτ = 1\nOriginal DSN\nNavigation\n81.5\n78.0\n94.6\nPick Up\n99.6\n83.3\n100\nBreak\n78.5\n73.0\n100\nPlacement\n78.5\n73.0\n100\nTable 2: The success % performance of the distilled multi-\nskill network on each of the four tasks (Figures 4b −d).\nFigure 7: Multi-skill distillation.\nTraining the H-DRLN: We now show results for\ntraining the (1) H-DRLN with a DSN array, (2) H-DRLN\nwith DDQN and a DSN array and (3) H-DRLN with DDQN\nand a distilled multi-skill network (with τ = 0.1). This is\ncompared to (4) a DDQN baseline. The learning curves can\nbe seen in Figure 8. We performed these trials 5 times for\neach architecture and measured success rates of 85 ± 10%,\n91 ± 4% and 94 ± 4% (mean% ± std) for the H-DRLN,\nH-DRLN with DDQN and H-DRLN with DDQN and a\ndistilled multi-skill network respectively. To calculate these\nvalues we averaged the success percentages for the ﬁnal\n10 epochs. Note that the distilled H-DRLN has a higher\naverage success rate and both H-DRLN’s with DDQN have\nlower variance. The DDQN was unable to solve the task.\nThis is due to a combination of wall ambiguities (as in the\ntwo room domain) and requiring more time to learn. The\nH-DRLN is able to overcome ambiguities and also learns\nto reuse skills. We also trained the DDQN with intrinsic\nrewards which enabled it to solve the task. However, this\nrequired a signiﬁcantly larger amount of training time\ncompared to the H-DRLN and the result was therefore\nomitted.\nFigure 8: The success % learning curves for the (1) H-\nDRLN with a DSN array (blue), (2) H-DRLN with DDQN\nand a DSN array (orange), and (3) H-DRLN with DDQN\nand multi-skill distillation (black). This is compared with\n(4) the DDQN baseline (yellow).\nSkill usage: Figure 9 presents the usage % of skills by\nthe H-DRLN agent during training. We can see that around\ntraining epoch 50, the agent starts to use skills more fre-\nquently (black curve). As a result, the H-DRLN agent’s per-\nformance is signiﬁcantly improved, as can be seen by the in-\ncrease in reward (yellow curve). After epoch 93, the agent’s\nskill usage reduces with time as it needs to utilize more prim-\nitive actions. This observation makes sense, since planning\nonly with skills will yield a sub-optimal policy if the skills\nthemselves are sub-optimal. However, planning with both\nprimitive actions and skills always guarantees convergence\nto an optimal policy (utilizing only primitive actions in the\nworst-case) (Mann and Mannor 2013). In our case, the skills\nthat were trained on the one-room domains helped the agent\nto learn in the complex domain but were sub-optimal due to\nsmall changes between the one-room domains and the com-\nplex domain. Thus, the agent learned to reﬁne his policy by\nusing primitive actions. To conclude, Figures 8 and 9 tell us\nthat, while skills are used approximately 20% of the time by\nthe ﬁnal H-DRLN policy, they have a signiﬁcant impact on\naccelerating the agent’s learning capabilities.\nFigure 9: Skill usage % in the complex domain during train-\ning (black). The primitive actions usage % (blue) and the\ntotal reward (yellow) are displayed for reference.\nDiscussion\nWe presented our novel Hierarchical Deep RL Network\n(H-DRLN) architecture. This architecture contains all\nof the basic building blocks for a truly general lifelong\nlearning framework: (1) Efﬁcient knowledge retention via\nmulti-skill distillation; (2) Selective transfer using temporal\nabstractions (skills); (3) Ensuring interaction between (1)\nand (2) with the H-DRLN controller. We see this work as a\nbuilding block towards truly general lifelong learning using\nhierarchical RL and Deep Networks.\nWe have also provided the ﬁrst results for learning Deep\nSkill Networks (DSNs) in Minecraft, a lifelong learning\ndomain. The DSNs are learned using a Minecraft-speciﬁc\nvariation of the DQN (Mnih et al. 2015) algorithm. Our\nMinecraft agent also learns how to reuse these DSNs on\nnew tasks by the H-DRLN. We incorporate multiple skills\ninto the H-DRLN using (1) the DSN array and (2) the\nscalable distilled multi-skill network, our novel variation of\npolicy distillation.\nIn addition, we show that the H-DRLN provides superior\nlearning performance and faster convergence compared to\nthe DDQN, by making use of skills. Our work can also\nbe interpreted as a form of curriculum learning (Bengio et\nal. 2009) for RL. Here, we ﬁrst train the network to solve\nrelatively simple sub-tasks and then use the knowledge it\nobtained to solve the composite overall task. We also show\nthe potential to perform knowledge transfer between related\ntasks without any additional learning. This architecture also\nhas the potential to be utilized in other 3D domains such\nas Doom (Kempka et al. 2016) and Labyrinth (Mnih et al.\n2016b).\nRecently, it has been shown that Deep Networks tend to\nimplicitly capture the hierarchical composition of a given\ntask (Zahavy, Zrihem, and Mannor 2016). In future work,\nwe plan to utilize this implicit hierarchical composition to\nlearn DSNs. In addition, we aim to (1) learn the skills online\nwhilst the agent is learning to solve the task. This could be\nachieved by training the teacher networks (DSNs), whilst\nsimultaneously guiding learning in the student network (our\nH-DRLN); (2) Perform online reﬁnement of the previously\nlearned skills; (3) Train the agent in real-world Minecraft\ndomains.\nAcknowledgement\nThis research was supported in part by the European Com-\nmunitys Seventh Framework Programme (FP7/2007-2013)\nunder grant agreement 306638 (SUPREL) and the Intel Col-\nlaborative Research Institute for Computational Intelligence\n(ICRI-CI).\nReferences\n[Ammar et al. 2014] Ammar, H. B.; Eaton, E.; Ruvolo, P.;\nand Taylor, M. 2014. Online multi-task learning for policy\ngradient methods. In Proceedings of the 31st International\nConference on Machine Learning.\n[Bacon and Precup 2015] Bacon, P.-L., and Precup, D. 2015.\nThe option-critic architecture. In NIPS Deep Reinforcement\nLearning Workshop.\n[Bai, Wu, and Chen 2015] Bai, A.; Wu, F.; and Chen, X.\n2015. Online planning for large markov decision processes\nwith hierarchical decomposition. ACM Transactions on In-\ntelligent Systems and Technology (TIST) 6(4):45.\n[Bellemare et al. 2015] Bellemare, M. G.; Ostrovski, G.;\nGuez, A.; Thomas, P. S.; and Munos, R. 2015. Increasing\nthe action gap: New operators for reinforcement learning.\narXiv preprint arXiv:1512.04860.\n[Bengio et al. 2009] Bengio, Y.; Louradour, J.; Collobert, R.;\nand Weston, J. 2009. Curriculum learning. In Proceed-\nings of the 26th annual international conference on machine\nlearning, 41–48. ACM.\n[Brunskill and Li 2014] Brunskill, E., and Li, L. 2014. Pac-\ninspired option discovery in lifelong reinforcement learning.\nIn Proceedings of the 31st International Conference on Ma-\nchine Learning (ICML-14).\n[da Silva, Konidaris, and Barto 2012] da\nSilva,\nB.;\nKonidaris, G.; and Barto, A.\n2012.\nLearning parame-\nterized skills. In Proceedings of the Twenty Ninth ICML.\n[Hauskrecht et al. 1998] Hauskrecht, M.; Meuleau, N.; Kael-\nbling, L. P.; Dean, T.; and Boutilier, C. 1998. Hierarchical\nsolution of markov decision processes using macro-actions.\nIn Proceedings of the Fourteenth Conference on Uncertainty\nin Artiﬁcial Intelligence, 220–229.\n[Hinton, Vinyals, and Dean 2015] Hinton, G.; Vinyals, O.;\nand Dean, J. 2015. Distilling the knowledge in a neural\nnetwork. arXiv preprint arXiv:1503.02531.\n[Jaderberg et al. 2016] Jaderberg, M.; Mnih, V.; Czarnecki,\nW. M.; Schaul, T.; Leibo, J. Z.; Silver, D.; and Kavukcuoglu,\nK. 2016. Reinforcement Learning with Unsupervised Aux-\niliary Tasks. ArXiv e-prints.\n[Kempka et al. 2016] Kempka, M.; Wydmuch, M.; Runc, G.;\nToczek, J.; and Jaskowski, W. 2016. Vizdoom: A doom-\nbased AI research platform for visual reinforcement learn-\ning. CoRR abs/1605.02097.\n[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,\nA.;\nSutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁca-\ntion with deep convolutional neural networks. In Advances\nin neural information processing systems, 1097–1105.\n[Kulkarni et al. 2016] Kulkarni, T. D.; Narasimhan, K. R.;\nSaeedi, A.; and Tenenbaum, J. B. 2016. Hierarchical deep\nreinforcement learning: Integrating temporal abstraction and\nintrinsic motivation. arXiv preprint arXiv:1604.06057.\n[Lin 1993] Lin, L.-J.\n1993.\nReinforcement learning for\nrobots using neural networks.\n[Mankowitz, Mann, and Mannor 2014] Mankowitz,\nD.\nJ.;\nMann, T. A.; and Mannor, S. 2014. Time regularized inter-\nrupting options. Internation Conference on Machine Learn-\ning.\n[Mankowitz, Mann, and Mannor 2016a] Mankowitz, D. J.;\nMann, T. A.; and Mannor, S. 2016a. Adaptive Skills, Adap-\ntive Partitions (ASAP). Neural Information Processing Sys-\ntems (NIPS).\n[Mankowitz, Mann, and Mannor 2016b] Mankowitz, D. J.;\nMann, T. A.; and Mannor, S. 2016b. Iterative Hierarchical\nOptimization for Misspeciﬁed Problems (IHOMP). arXiv\npreprint arXiv:1602.03348.\n[Mann and Mannor 2013] Mann, T. A., and Mannor, S.\n2013. Scaling up approximate value iteration with options:\nBetter policies with fewer iterations. Internation Conference\non Machine Learning.\n[Mnih et al. 2015] Mnih, V.; Kavukcuoglu, K.; Silver, D.;\nRusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Ried-\nmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al.\n2015.\nHuman-level control through deep reinforcement learning.\nNature 518(7540):529–533.\n[Mnih et al. 2016a] Mnih, V.; Agapiou, J.; Osindero, S.;\nGraves, A.; Vinyals, O.; Kavukcuoglu, K.; et al.\n2016a.\nStrategic attentive writer for learning macro-actions. arXiv\npreprint arXiv:1606.04695.\n[Mnih et al. 2016b] Mnih, V.; Badia, A. P.; Mirza, M.;\nGraves, A.; Lillicrap, T. P.; Harley, T.; Silver, D.; and\nKavukcuoglu, K. 2016b. Asynchronous methods for deep\nreinforcement learning. arXiv preprint arXiv:1602.01783.\n[Oh et al. 2016] Oh, J.; Chockalingam, V.; Singh, S.; and\nLee, H. 2016. Control of memory, active perception, and\naction in minecraft. arXiv preprint arXiv:1605.09128.\n[Precup and Sutton 1997] Precup, D., and Sutton, R. S.\n1997. Multi-time models for temporally abstract planning.\nIn Advances in Neural Information Processing Systems 10\n(Proceedings of NIPS’97).\n[Rusu et al. 2015] Rusu, A. A.; Gomez Colmenarejo, S.;\nGulcehre, C.; Desjardins, G.; Kirkpatrick, J.; Pascanu, R.;\nMnih, V.; Kavukcuoglu, K.; and Hadsell, R. 2015. Policy\nDistillation. arXiv 1–12.\n[Rusu et al. 2016] Rusu, A. A.; Rabinowitz, N. C.; Des-\njardins, G.; Soyer, H.; Kirkpatrick, J.; Kavukcuoglu, K.; Pas-\ncanu, R.; and Hadsell, R. 2016. Progressive neural networks.\narXiv preprint arXiv:1606.04671.\n[Schaul et al. 2015] Schaul, T.; Quan, J.; Antonoglou, I.; and\nSilver, D.\n2015.\nPrioritized experience replay.\narXiv\npreprint arXiv:1511.05952.\n[Silver, Yang, and Li 2013] Silver, D. L.; Yang, Q.; and Li, L.\n2013. Lifelong machine learning systems: Beyond learning\nalgorithms. In AAAI Spring Symposium: Lifelong Machine\nLearning, 49–55. Citeseer.\n[Smith and Aha ] Smith, D. B. M. R. L., and Aha, D. W. Se-\nlecting subgoals using deep learning in minecraft: A prelim-\ninary report.\n[Stolle and Precup 2002] Stolle, M., and Precup, D. 2002.\nLearning options in reinforcement learning. In International\nSymposium on Abstraction, Reformulation, and Approxima-\ntion, 212–223. Springer.\n[Stone et al. 2000] Stone, P.; Veloso, M.; Ave, P.; and Park,\nF.\n2000.\nLayered Learning.\nEMCL 2000 Proceedings\nof the 17th International Conference on Machine Learning\n1810(June):369–381.\n[Stone, Sutton, and Kuhlmann 2005] Stone, P.; Sutton, R. S.;\nand Kuhlmann, G.\n2005.\nReinforcement learning for\nrobocup soccer keepaway. Adaptive Behavior 13(3):165–\n188.\n[Sutton, Precup, and Singh 1999] Sutton, R. S.; Precup, D.;\nand Singh, S. 1999. Between MDPs and semi-MDPs: A\nframework for temporal abstraction in reinforcement learn-\ning. Artiﬁcial Intelligence 112(1).\n[Van Hasselt, Guez, and Silver 2015] Van Hasselt, H.; Guez,\nA.; and Silver, D. 2015. Deep reinforcement learning with\ndouble q-learning. arXiv preprint arXiv:1509.06461.\n[Wang, de Freitas, and Lanctot 2015] Wang, Z.; de Freitas,\nN.; and Lanctot, M.\n2015.\nDueling network archi-\ntectures for deep reinforcement learning.\narXiv preprint\narXiv:1511.06581.\n[Zahavy, Zrihem, and Mannor 2016] Zahavy,\nT.;\nZrihem,\nN. B.; and Mannor, S.\n2016.\nGraying the black box:\nUnderstanding dqns. Proceedings of the 33th international\nconference on machine learning (ICML).\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2016-04-25",
  "updated": "2016-11-30"
}