{
  "id": "http://arxiv.org/abs/1604.07704v1",
  "title": "Tournament selection in zeroth-level classifier systems based on average reward reinforcement learning",
  "authors": [
    "Zhaoxiang Zang",
    "Zhao Li",
    "Junying Wang",
    "Zhiping Dan"
  ],
  "abstract": "As a genetics-based machine learning technique, zeroth-level classifier\nsystem (ZCS) is based on a discounted reward reinforcement learning algorithm,\nbucket-brigade algorithm, which optimizes the discounted total reward received\nby an agent but is not suitable for all multi-step problems, especially\nlarge-size ones. There are some undiscounted reinforcement learning methods\navailable, such as R-learning, which optimize the average reward per time step.\nIn this paper, R-learning is used as the reinforcement learning employed by\nZCS, to replace its discounted reward reinforcement learning approach, and\ntournament selection is used to replace roulette wheel selection in ZCS. The\nmodification results in classifier systems that can support long action chains,\nand thus is able to solve large multi-step problems.",
  "text": "Tournament selection in zeroth-level classifier systems based on \naverage reward reinforcement learning \n \nZang Zhaoxiang, Li Zhao, Wang Junying, Dan Zhiping \nzxzang@gmail.com; zangzx@hust.edu.cn \n(Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, \nChina Three Gorges University, Yichang Hubei, 443002, China; \nCollege of Computer and Information Technology, China Three Gorges University, Yichang Hubei, \n443002, China) \n \nAbstract: As a genetics-based machine learning technique, zeroth-level classifier \nsystem (ZCS) is based on a discounted reward reinforcement learning algorithm, \nbucket-brigade algorithm, which optimizes the discounted total reward received by an \nagent but is not suitable for all multi-step problems, especially large-size ones. There \nare some undiscounted reinforcement learning methods available, such as R-learning, \nwhich optimize the average reward per time step. In this paper, R-learning is used as \nthe reinforcement learning employed by ZCS, to replace its discounted reward \nreinforcement learning approach, and tournament selection is used to replace roulette \nwheel selection in ZCS. The modification results in classifier systems that can support \nlong action chains, and thus is able to solve large multi-step problems.  \n \nKey words: average reward; reinforcement learning; R-learning; learning classifier \nsystems (LCS); zeroth-level classifier system (ZCS); multi-step problems \n1 Introduction \nLearning Classifier Systems (LCSs) are rule-based adaptive systems which use \nGenetic Algorithm (GA) and some machine learning methods to facilitate rule \ndiscovery and rule learning[1]. LCSs are competitive with other techniques on \nclassification tasks, data mining[2, 3] or robot control applications[4, 5]. In general, \nan LCS is a model of an intelligent agent interacting with an environment. Its ability \nto choose the best policy acting in the environment, namely adaptability, improves \nwith experience. The source of the improvement is the learning from reinforcement, \ni.e. payoff, provided by the environment. The aim of an LCS is to maximize the \nachieved environmental payoffs. To do this, LCSs try to evolve and develop a \npopulation of compact and maximally general \"condition-action-payoff\" rules, called \nclassifiers, which tell the system in each state (identified by the condition) the amount \nof payoffs for any available action. So, LCSs can be seen as a special method of \nreinforcement learning that provides a different approach to get generalization.  \nThe original Learning Classifier System framework proposed by Holland, is \nreferred to as the traditional framework now. And then, Willson proposed \nstrength-based Zeroth-level Classifier System (ZCS)[6], and accuracy-based X \nClassifier System (XCS)[7]. The XCS classifier system has solved the former main \nshortcoming of LCSs, which is the problem of strong over-generals, by its accuracy \nbased fitness approach. Bull and Hurst[8] have recently shown that, despite its relative \nsimplicity, ZCS is able to perform optimally through its use of fitness sharing. That is, \nZCS was shown to perform as well, with appropriate parameters, as the more complex \nXCS on a number of tasks. \nDespite current research has focused on the use of accuracy in rule predictions as \nthe fitness measure, the present work departs from this popular approach and takes a \nstep backward, aiming to uncover the potential of strength based LCS (and \nparticularly ZCS) in sequential decision problems. In this direction, we will discuss \nthe use of average reward in ZCS, and will introduce an undiscounted reinforcement \nlearning technique called R-learning[9, 10] for ZCS to optimize average reward, \nwhich is a different metric from the discounted reward optimized by original ZCS. In \nparticular, we apply R-learning based ZCS to large multi-step problems and compare \nit with ZCS. Experimental results are encouraging, in that ZCS with R-learning can \nperform optimally or near optimally in these problems. Later, we will refer to our \nproposal as \"ZCSAR\", and the \"AR\" stands for \"average reward\". \nThe rest of the paper is structured as follows: Section 2 provides some necessary \nbackground knowledge on reinforcement learning, including Sarsa and R-learning. \nSection 3 provides a brief description of ZCS and maze environments. How ZCS can \nbe modified to include the average reward reinforcement learning is described in \nSection 4, while Section 5 analyzes the trouble resulting from our modification to \nZCS, and presents some solution to it. Experiments with our proposal and some \nrelated discussion are given in Section 6. Finally, Section 9 ends the paper, presents \nour main conclusions and some directions for future research. \n2 Reinforcement learning \nReinforcement learning is a formal framework in which an agent manipulates its \nenvironment through a series of actions, and receives some rewards as feedback to its \nactions, but is not told what the correct actions would have been. The agent stores its \nknowledge about how to make decisions that maximize rewards or minimize costs \nover a period of time. Reinforcement learning must learn to perform a task by trial \nand error from a reinforcement signal (the reward values) that is not as informative as \nmight be desired. In reinforcement learning for multi-step problems, the \nreinforcement signal usually gives delayed reward, which typically comes at the end \nof a series of actions. Delayed reward makes learning much more difficult. \nGenerally, the reinforcement learning framework consists of \n A discrete set of environment states, S ; \n A discrete set of available actions, A ; \n An immediate reinforcement function R , mapping S\nA\n\n into the real value \nr , where r  is the expected environmental payoff after performing the \naction a , from A, in a particular state s , from S . \nOn each step of interaction the agent perceives the environment to be in state s ; \nthe agent then chooses an action a  in the set A , and the action a  is performed in \nthe environment. As a result of taking action a , the agent receives a reward r  and a \nnew state s. \nThe agent’s job is to find a policy , mapping states to actions, that maximizes \nsome long-run measure of reinforcement. There are mainly two measures to value a \npolicy: discounted reward optimality and average reward optimality.  \nIn discounted reinforcement learning, the performance measure being optimized \nusually is the infinite-horizon discounted model[11], which takes the long-run reward \nof the agent into account, but rewards receiving in the future are geometrically \ndiscounted according to a discount factor \n\n\n0\n1\n\n\n\n\n: \n \n\n\n1\n0\nlim\n( )\nN\nt\nt\nt\nN\nE\nr s\n\n\n\n\n\n \n(1) \nwhere E  denotes expected value, and \n( )\ntr s  is the reward received at time t  \nstarting from state s  under a policy. An optimal discounted policy maximizes the \nabove infinite-horizon discounted reward. \nOn the other hand, undiscounted reinforcement learning usually optimizes the \naverage reward model[9], in which the agent is supposed to take actions that \nmaximize its long-run average reward per step: \n \n\n\n1\n0\n( )\n( )\nlim\nN\nt\nt\nN\nE\nr s\ns\nN\n\n\n\n\n\n\n \n(2) \nIf a policy maximizes the average reward over all states, it is referred to as a gain \noptimal policy. Usually, average reward \n( )s\n\n can be denoted as , which is state \nindependent[12] and greatly simplifies the design of average reward algorithms. \nHow does the agent find a policy to maximize the long-run measure of \nreinforcement? Most of the reinforcement learning algorithms are based on estimating \nstate-action pair value function (called action value function) that indicates how good \nit is for the agent to perform a given action in a given state. Here, \"how good\" is \ndefined in terms of future expected reward value, usually as (1) or (2), corresponding \nto the discounted reward and average reward optimality. We will give a brief \ndescription of two typical reinforcement learning algorithms based on discounted \nreward and on average reward optimality, respectively. \n2.1 Sarsa Algorithm \nSarsa is a well-known reinforcement learning algorithm that can be seen as a \nvariant of Q-learning algorithm[11]. It is based on iteratively approximating the table \nof all action values \n( , )\nQ s a , named the Q-table. Initially, all the \n( , )\nQ s a  values are \nset to 0. At time step \n1\nt , the agent perceives the environment state \n1\nts , chooses an \naction \n1\nta  by the -greedy policy. The action \n1\nta  is performed in the \nenvironment, and the agent receives an immediate reward \n1\n1\n(\n,\n)\nimm\nt\nt\nr\ns\na\n\n for doing \naction \n1\nta , and a new environment state \nts . Then, the entry \n1\n1\n(\n,\n)\nt\nt\nQ s\na\n\n is updated \nusing the following rule: \n \n\n\n1\n1\n1\n1\n1\n1\n1\n1\nˆ\n(\n,\n)\n(\n,\n)\n(\n,\n)\n(\n,\n)\nt\nt\nt\nt\nt\nt\nt\nt\nQ s\na\nQ s\na\nQ s\na\nQ s\na\n\n\n\n\n\n\n\n\n\n\n\n\n \n(3) \nHere, 0\n1\n\n\n is the learning rate controlling how quickly errors in the \nestimated action values are corrected; \n1\n1\nˆ(\n,\n)\nt\nt\nQ s\na\n\n is the new estimate of \n1\n1\n(\n,\n)\nt\nt\nQ s\na\n\n, \nand is computed as \n \n1\n1\n1\n1\nˆ(\n,\n)\n(\n,\n)\n( , )\nt\nt\nimm\nt\nt\nt\nQ s\na\nr\ns\na\nQ s a\n\n\n\n\n\n\n\n, \n(4) \nwhere \n1\n1\n(\n,\n)\nimm\nt\nt\nr\ns\na\n\n is the immediate reward received for performing \n1\nta  in \nstate \n1\nts . \n2.2 R-learning \nSince Q-learning discounts future rewards, it prefers actions that result in \nshort-term ordinary rewards to those that result in long-term sustained or considerable \nrewards. On the contrary, the R-learning algorithm[9] proposed by Schwartz \nmaximizes the average reward per time step. \nR-learning is similar to Q-learning in form. It is based on iteratively \napproximating the action values \n( , )\nR s a , which represent the average adjusted reward \nof doing an action a  in state s  once, and then following corresponding policy \nsubsequently. \nR-learning algorithm consists of the following steps: \n1) Initialize all the \n( , )\nR s a  values to zero, and the average reward variable  \nalso initialized to zero. \n2) Let the current time step be \n1\nt . From the current state \n1\nts , choose an \naction \n1\nta  by some exploration/action-selection mechanism, such as the -greedy \npolicy. \n3) Perform the action \n1\nta , observe the immediate reward \n1\n1\n(\n,\n)\nimm\nt\nt\nr\ns\na\n\n received \nand the subsequent state \nts . \n4) Update R values using the following rule: \n \n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n1\n,\n,\n,\nmax\n,\n,\nt\nt\nt\nt\nR\nimm\nt\nt\nt\nt\nt\na A\nR s\na\nR s\na\nr\ns\na\nR s a\nR s\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n(5) \n5) If \n\n\n\n\n1\n1\n1\n,\nmax\n,\nt\nt\nt\na A\nR s\na\nR s\na\n\n\n\n\n\n (i.e. if a greedy/non-random action \n1\nta  was \nchosen), then update the average reward  according to the rule: \n \n\n\n\n\n\n\n\n\n1\n1\n1\n,\nmax\n,\nmax\n,\nimm\nt\nt\nt\nt\na A\na A\nr\ns\na\nR s a\nR s\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n(6) \n6) \n1\nt\nt\n, and go to step 2. \nHere, 0\n1\nR\n\n\n is the learning rate for updating action values \n( , )\nR , and \n0\n1\n\n\n\n is the learning rate for updating average reward . \nThe update rule for action value \n( , )\nR  differs from the rule for Q-learning in \nsubtracting the average reward  from the immediate reward, and not discounting \nthe next maximum action value. The estimation of the average reward  is a critical \ntask in R-learning. As mentioned above, the average reward , under some \nconditions, does not depend on any state, and is constant over the whole state \nspace[12]. This facilitates the use of average reward algorithms. \nFollowing the basic R-learning algorithm, [10] proposed some variations. The \nvariations mainly focus on different ways to update the average reward, \ncorresponding to the step 5 given above. \n3 ZCS Classifier System and Its Testing Environments \n3.1 A Brief Description of ZCS \nThe following is a brief description of ZCS, further information can be found in \n[6] and [8]. \nThe ZCS architecture was introduced by Stewart Wilson in 1994. It is a \nMichigan style LCS without internal memory, which periodically receives a binary \nencoded input from its environment. The system determines an appropriate response \nbased on this input and performs the indicated action, usually altering the state of the \nenvironment. The action is rewarded by a scalar reinforcement. Internally the system \ncycles through a sequence of performance, reinforcement and discovery.  \nThe ZCS rule base consists of a population of classifiers, symbolized by [ ]\nP . \nThis population has a fixed maximum size \nN . Each classifier is a \ncondition-action-strength rule \n, ,\nc a str\n\n. The rule condition c  is a string of \ncharacters from the ternary alphabet {0,1,#}, where # acts as a wildcard allowing a \nclassifier to generalize over different input messages. The action \n1\n{ ,\n,\n}\nn\na\na\na\n\n\n is \nrepresented by a binary string and both conditions and actions are initialized \nrandomly. Strength scalar str  acts as an indication of the perceived utility of that \nrule within the system. The strength of each rule is initialized to a predetermined \nvalue termed \n0\nS . \nOn receipt of an environmental input message \nts , the rule-base is scanned and \nany classifiers whose condition matches input message \nts  is placed in a match set \n[M]. Match set [M] is a subset of the whole population [ ]\nP  of classifiers. If on some \ntime-step, [M] is empty or has a total strength \n[\n]\nM\nStr\n that is less than a fixed fraction \n(0\n1)\n\n\n\n\n of the mean strength of the population [ ]\nP , then a covering operator is \ninvoked. A new rule is created with a condition that matches the environmental input \nand a randomly selected action. The rule’s condition is then made less specific by the \nrandom inclusion of #’s at a probability of \n#P  per bit. The new rule is given a \nstrength equal to the population average and inserted into the population, overwriting \na rule selected for deletion. The deleted rules are chosen using roulette-wheel \nselection based on the reciprocal of strength.  \nThus a particular action a  is selected from the match set by roulette wheel \nselection policy based on the total strength \n( , )\nt\nStr s a  of the classifiers in [M] which \nadvocate that action. For all actions \n\n\n1,\n,\nn\na\na\na\n\n\n in [M], \n( , )\nt\nStr s a  is named as \nsystem strength, which is computed as: \n \n.\n[\n]\n( , )\n.\nt\ncl a a cl\nM\nStr s a\ncl str\n\n\n \n(7) \ncl  stands for a classifier, \n.\ncl str  for strength of cl , and \n.\ncl a  for its action.  \nWhen an action has been selected, all rules in the [M] that advocate this action \nare placed in action set [A] and the system executes the action. Depending on \nenvironmental circumstances, a scalar reward reinforcement value r (maybe null) is \nsupplied to ZCS as a consequence of executing a , together with a new input \nconfiguration \n1\nts . \nReinforcement in ZCS consists of redistributing payoff between subsequent \naction sets. In each cycle, a \"bucket-brigade\" credit-assignment policy similar to Sarsa \nis employed:  \n1) A fixed fraction \n(0\n1)\n\n\n\n\n of the strength of each member of [A] at current \ntime step is deducted and placed in a common bucket B : \n[ ]\n[\n]\n( )\n(1\n)\n( )\nA\nA\nstr\ni\nstr\ni\n\n\n\n\n; \n[\n]( )\nA\ni\nB\nstr\ni\n\n\n\n, where \n[ ]( )\nA\nstr\ni  stands for the strength of the i-th classifier of [A]. \nB  is initially set to zero. \n2) If a reward \n1\nr is received from the environment as a consequence of \nexecuting \n1\na at the previous time step t-1, then a fixed fraction\n(0\n1)\n\n\n\n\n of \n1\nr \nis distributed evenly amongst the members of \n1\n[ ]\nA : \n1\n1\n[ ]\n[\n]\n1\n1\n( )\n( )\nA\nA\nstr\ni\nstr\ni\nr\nA\n\n\n\n\n\n\n\n, \nwhere \n1\nA is the number of classifiers in \n1\n[ ]\nA . \n3) Classifiers in \n1\n[ ]\nA  (if it is non-empty) have their strengths incremented by \n1\nB A\n\n, \n1\n1\n[ ]\n[\n]\n1\n( )\n( )\nA\nA\nstr\ni\nstr\ni\nB A\n\n\n\n\n\n\n, where is a pre-determined discount factor \n(0\n1\n\n\n), B  is the total amount put in the current bucket in step 1. \n4) Finally, the bucket B  is emptied, and all classifiers in the set difference [M] - \n[A] have their strengths reduced by a small fraction \n(0\n1)\n\n\n\n\n, which acts as a \"tax\" \nto \nencourage \nexploitation \nof \nstrong \nclassifier \nsets: \n[\n]\n[ ]\ncl\nM\ncl\nA\n\n\n\n\n: \n.\n(1\n) .\ncl str\ncl str\n\n\n\n.  \nThen the above process can be written as a re-assignment: \n \n1\n1\n1\n[\n]\n[ ]\n1\n[ ]\n[\n]\n(\n)\nA\nA\nA\nA\nStr\nStr\nr\nStr\nStr\n\n\n\n\n\n\n\n\n\n\n\n  \n(8) \n1\n[\n]\nA\nStr\nis the total strength of members of \n1\n[ ]\nA , also known as \n1\n1\n(\n,\n)\nt\nStr s\na\n\n; \n[\n]\nA\nStr\nis the total strength of members of [A], also known as \n( , )\nt\nStr s a . So, Equation \ncan be rewritten as  \n \n1\n1\n1\n1\n1\n1\n1\n(\n,\n)\n(\n,\n)\n(\n( , )\n(\n,\n))\nt\nt\nt\nt\nStr s\na\nStr s\na\nr\nStr s a\nStr s\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n(9) \nZCS employs GA as discovery mechanism over the whole rule-set [ ]\nP  at each \ninstance (panmictic). On each cycle there is a probability \nGA\n\n of GA invocation. \nWhen called, the GA uses roulette wheel selection to determine the parent rules based \non strength. Two offspring are produced via crossover (single point, using probability \n) and mutation (using probability ). The parents then donate half their strength to \ntheir offspring who replace existing members of the population. The deleted rules are \nchosen based on the reciprocal of strength.  \n3.2 Maze Environments \nMaze problems, usually represented as grid-like two-dimensional areas that may \ncontain different objects of any quantity and with different properties (for example, \nobstacle, goal, or can be empty), serve as a simplified virtual model of the real \nenvironment, and can be used for developing core algorithms of many real-world \napplications related to the problem of navigation. The agent should learn the shortest \npath to goal states, without knowing the environmental model in advance. \n \n \nFigure 1. (a) Maze6 environment; (b) Woods14 environment. Food object is marked \nwith F, and obstacle is marked with T. \n \nLCS has been the most widely used class of algorithms for reinforcement \nlearning in mazes for the last twenty years, and has presented the most promising \nperformance results[6, 13]. Figure 3(a) presents Woods1[6] maze environment. The \nmaze may contain different obstacles in any quantity, such as T standing for tree in \nWoods1, and some objects for learning purposes, like virtual food F, which is the \nagents’ goal to reach. It must be noted that, if a maze has not enough obstacles to \nmark its boundary, the left and right edges of the maze are connected, as are the top \nand bottom. In this paper, the agent is randomly placed in the maze on an empty cell, \nand the agent has two boolean sensors for each of the eight adjacent squares. The \nagent can move into any adjacent square that is free. \n4 Adding R-learning to ZCS \nIn this section, we show how ZCS can be modified to include R-learning[9, 10] \nto optimize average reward, which is different from the discounted reward optimized \nby Sarsa-learning. The implementation of our system, ZCSAR, is also discussed here. \nAs mentioned above, ZCS uses a \"bucket-brigade\" credit-assignment policy \nsimilar to Sarsa to update the classifiers population. From Equation (9), \nbucket-brigade algorithm in ZCS is indeed similar to the Sarsa update rule (3). \nBesides, the comparison shows that (i) ZCS represents each entry in the Q-table by a \nset of classifiers, i.e. \n1\n1\n(\n,\n)\nt\nt\nQ s\na\n\n\n is represented by the classifiers in \n1\n[ ]\nA , and \n( ,\n)\nt\nt\nQ s a  is represented by the classifiers in [ ]\nA ; (ii) The system strength \n( , )\nt\nStr s a , \nT T T T T T T T\nT\nT\nT\nT\nT\nT\nT\nT\nF\nT\nT T\nT\nT T\nT\nT\nT\nT\nT\nT T T T T T T T\nT\nT\nT\nT\nT\nT\nT\nT\nT    \nT T T T T T T T\nT\nT\nT\nT\nT\nT\nT T T\nT T T\nT T\nT T T\nT\nF\nT T T\nT T\nT T T T T\nT T T T T T T T\nT\nT\nT\nT\nT\nT T T T\nT T\nT\nT\nT T\nT\nT T T T\nT T T T\nT T T T\nT\nT\nT\nT\nT\nT\nT  \n(a)  \n \n \n \n \n \n \n(b) \nspecified in Equation (7), also known as \n[\n]\nA\nStr\n, corresponds to the value \n( , )\nt\nQ s a  in \nEquation (3), and \n1\n( , )\nt\nr\nStr s a\n\n\n\n in Equation (9) corresponds to the estimate \n1\n1\nˆ(\n,\n)\nt\nt\nQ s\na\n\n of value \n1\n1\n(\n,\n)\nt\nt\nQ s\na\n\n in Equation (4); (iii) Only one entry \n1\n1\n(\n,\n)\nt\nt\nQ s\na\n\n is \nupdated in tabular Sarsa algorithm at time step t , while in ZCS a set of classifiers is \nusually updated in one time step. \nR-learning has been introduced in Section 2, and it is a new type of \nreinforcement learning. R-learning and Sarsa algorithm are similar in form but not in \nmeaning, since Sarsa algorithm is based on the discounted reward optimality, while \nR-learning, based on the average reward optimality, maximizes the average reward \nper step. In R-learning, we can define the estimate of \n1\n1\n(\n,\n)\nt\nt\nR s\na\n\n as \n \n\n\n\n\n1\n1\n1\n1\nˆ(\n,\n)\n,\nmax\n,\nt\nt\nimm\nt\nt\nt\na A\nR s\na\nr\ns\na\nR s a\n\n\n\n\n\n\n\n\n\n \n(10) \nThus, Equation (5) can be rewritten as \n \n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n1\nˆ\n,\n,\n,\n,\nt\nt\nt\nt\nR\nt\nt\nt\nt\nR s\na\nR s\na\nR s\na\nR s\na\n\n\n\n\n\n\n\n\n\n\n\n\n \n(11) \nThe major difference between Equation (11) and (3) is that they use different \nmethods to compute the estimate \n1\n1\nˆ(\n,\n)\nt\nt\nR s\na\n\n\n and \n1\n1\nˆ(\n,\n)\nt\nt\nQ s\na\n\n\n. Additionally, \nR-learning needs to estimate the average reward , which is extra work than in Sarsa \nalgorithm. \nFrom what has been discussed above, the analogies between Sarsa and ZCS, the \ndifference and similarity between Sarsa and R-learning have been presented. We can \nget that, the system strength \n( , )\nt\nStr s a  in ZCS corresponds to the action value \n( , )\nt\nR s a , and \n1\n( , )\nt\nr\nStr s a\n\n\n\n in ZCS corresponds to the new estimate \n1\n1\nˆ(\n,\n)\nt\nt\nR s\na\n\n of \n1\n1\n(\n,\n)\nt\nt\nR s\na\n\n. In order to add R-learning to ZCS, we only need to focus on the methods \nto compute \n1\n1\nˆ(\n,\n)\nt\nt\nR s\na\n\n in Equation (10) and \n1\n( , )\nt\nr\nStr s a\n\n\n\n in Equation (9). Given \nthe correspondence between the system strength \n( , )\nt\nStr s a  and the action value \n( , )\nt\nR s a , the average reward approach to compute \n1\n( , )\nt\nr\nStr s a\n\n\n\n in Equation (9) \ncan be modified as \n1\n( , )\nt\nr\nStr s a\n\n\n\n. Thus, Equation (9) is changed as: \n \n1\n1\n1\n1\n1\n1\n1\n(\n,\n)\n(\n,\n)\n(\n( , )\n(\n,\n))\nt\nt\nt\nt\nStr s\na\nStr s\na\nr\nStr s a\nStr s\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n(12) \nEquation (12) will replace Equation (9) in ZCS to change the whole \nreinforcement learning mechanism employed by the original ZCS.  \nAbout the specific update rule of classifiers in \n1\n[ ]\nA , the step 2 and 3 in Section \n3.1 can be modified as: \n2) If a reward \n1\nr is received from the environment as a consequence of \nexecuting \n1\na at the previous time step t-1, and the estimate of average reward is , \nthen a fixed fraction\n(0\n1)\n\n\n\n\n of \n1\nr\n\n\n is distributed evenly amongst the \nmembers of \n1\n[ ]\nA : \n1\n1\n[\n]\n[\n]\n1\n1\n( )\n( )\n(\n)\nA\nA\nstr\ni\nstr\ni\nr\nA\n\n\n\n\n\n\n\n\n\n, where \n1\nA is the number of \nclassifiers in \n1\n[ ]\nA . \n3) Classifiers in \n1\n[ ]\nA  (if it is non-empty) have their strengths incremented by \n1\nB A, \n1\n1\n[ ]\n[\n]\n1\n( )\n( )\nA\nA\nstr\ni\nstr\ni\nB A\n\n\n\n\n\n, where B  is the total amount put in the current \nbucket. \nNext, a procedure to estimate the average reward  needs to be added to ZCS. \nStep 5 in the description of R-learning algorithm in Section 2.2 can be moved to ZCS \nthrough some modifications. To do so, Step 5 in Section 2.2 can be rewritten as: \nIf \n1\n[\n]\n1\nmax\n(\n, )\nA\na A\nt\nStr\nStr s\na\n\n\n\n\n (i.e. if a greedy/non-random action \n1\nta  was \nchosen), then update the average reward  according to the rule: \n \n\n\n1\n1\nmax\n( , )\nmax\n(\n, )\nt\nt\na A\na A\nr\nStr s a\nStr s\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n(13) \nThe new type of Step 5 can be inserted into the procedure of ZCS, and located \njust before the update of classifiers in \n1\n[ ]\nA . It must be noted that, at the first time \nstep of each trial in an experiment, there is no need to update the average reward , \nsince no previous environmental reward available at that time. And at the beginning \nof an experiment,  is initialized to zero. In addition, the update value of average \nreward  is not used in Equation (12) directly. Instead, its more stable moving \naverage value is adopted to avoid the heavy oscillations with its update values, since \naverage reward  is updated by the immediate reward \n1\ntr  which is stochastic and \nwith great fluctuation. The window size for moving average is 100, i.e. moving \naverage is computed as the average of the last 100 updated values. If the window size \nis too small, the moving average will have no effect; if the window size is too big, the \nchanging trend of average reward will be hidden, which will limit the immediate \nfeedback function of average reward. \n \nThrough the two steps above, we have replaced Sarsa algorithm in ZCS with \nR-learning, getting the new system ZCSAR. However, in order to speed up the \nprocess of convergence in ZCSAR, the fluctuation of the estimate  needs to be \nreduced over time. So we make the learning rate \n\n in Equation (13) decayed over \ntime using a simple rule: \n \nmax\nmin\nNumOfTrials\n\n\n\n\n\n\n\n\n\n\n\n, \n(14) \nwhere \nmax\n\n\n is the initial value of \n\n, \nmin\n\n\n is the minimum learning rate \nrequired, and NumOfTrials  is the number of exploration trials (problems) in an \nexperiment. \n\n is updated at the beginning of each exploration trial using Equation \n(14), but not at each time step. \n5 Subtraction Trouble and Tournament Selection \nWhen ZCSAR uses Equation (12) as reinforcement learning mechanism, some \nissues arise. The update rule for system strength \n( , )\nStr  differs from the rule for \nSarsa-learning in subtracting the average reward  from the immediate reward, and \nnot discounting the next system strength (action value). The subtraction may cause \nsystem strength \n( , )\nStr  negative, which does not appear in original ZCS and \ndiscounted reward reinforcement learning Sarsa. The negative \n( , )\nStr , less than zero, \noccurs when the value of \n1\n( , )\nt\nr\nStr s a\n\n\n\n is continuously negative for some time \nsteps. In most time steps, reward is delayed, so \n1\nr is zero. The estimation of the \naverage reward  is not an easy task in sparse reward domains. It may differ largely \nfrom the true value of average reward in early stage of learning. Thus, whether the \nvalue of \n1\n( , )\nt\nr\nStr s a\n\n\n\n is negative or not depends mainly on the difference of  \nand \n( , )\nt\nStr s a .  \nIf \n( , )\nStr  is negative, the sum of strength of classifiers in action set is also less \nthan zero, which means some classifiers’ strength is negative in action set. However, \nall components of ZCS were designed on the supposition that classifier’s strength is \ngreater than zero. Specially, roulette wheel selection (proportionate selection) based \non classifier’s strength (or its reciprocal) is adopted as action selection method in \nmatch set [M], parents selection method in GA, classifier selection method in GA \ndeletion and covering operator deletion. It is known that classifier’s strength must be \npositive in roulette wheel selection. ZCS is in line with this requirement, but not \nZCSAR. \nThis is a problem caused by subtraction. To address this problem, an easy way is \nto make negative values be zero, i.e. let classifier’s strength not less than zero. We \nindicate this method as \"truncation\". In other words, if ZCSAR still uses roulette \nwheel selection, truncation is an easy method to adapt it. \nHowever, is truncation method proper and effective for ZCSAR? Is there any \nalternative to tackle this problem? A promising proposal is to replace roulette wheel \nselection with tournament selection in ZCSAR. Tournament selection with \ntournament sizes proportionate to the actual set size is shown to outperform roulette \nwheel selection in the widely-used classifier system XCS[14]. So it is expected that \ntournament selection can also improve the performance of ZCSAR. And importantly, \nin contrast to roulette wheel selection, tournament selection is independent of fitness \nscaling and does not require positive classifier strength, so classifiers’ strength can be \nless than zero in ZCSAR with tournament selection.  \nIn tournament selection, classifiers are not selected proportional to their strength, \nbut tournaments are held in which the classifier with the highest strength wins. \nStochastic tournaments are not considered herein. Participants for the tournament are \nchosen at random from the corresponding classifier set in which selection is applied. \nThe size of the tournament is dependent on the corresponding classifier set size, and \nthe size of each tournament has the size of the fraction \n(0,1]\n\n of the corresponding \nclassifier set size. Parameter  controls the selection pressure. Instead of roulette \nwheel selection in action selection in match set [M], parents selection in GA, \nclassifier deletion selection in GA and covering operator, three independent \ntournaments are held in which the classifier with the highest (or lowest) strength is \nselected, and  values are 0.1, 0.4, 0.6 respectively.  \nLater, \nwe \nwill \nrefer \nto \nour proposals as \"ZCSAR+Roulette\" and \n\"ZCSAR+Tournament\" in the remainder of this work, to indicate ZCSAR with \nroulette wheel selection and truncation method, and ZCSAR with tournament \nselection respectively. \n6 Experiments in Maze Environments \nTwo maze problems are tested and studied here, to illustrate the generality and \neffectiveness of our approaches, and ZCS for comparison. \n6.1 Experimental Setup \nTo conduct experiments, every experiment typically consists of 12000 problems \n(trials) that the agent must solve. And for each problem, the agent is placed into a \nrandomly chosen empty square in the mazes. Then the agent moves under the control \nof the classifier system avoiding obstacles until either it reaches the food or had taken \n500 steps, at which point the problem ended unconditionally. The agent will not \nchange its position if it chooses an action to move to a square with an obstacle inside, \nthough one time-step still elapses. When the agent reaches the food, it receives a \nconstant reward of 1000; otherwise, it receives a reward equal to 0. And in order to \nevaluate the final policy evolved, in each experiment, exploration is turned off during \nthe last 2000 problems and the system works only in exploitation. In exploitation \nproblems, the action which predicts the highest payoff is always selected in match set \n[M], and the genetic algorithm is turned off. System performance is computed as the \naverage number of steps to food in the last 50 problems. Every statistic results \npresented in this paper is averaged on 10 experiments. \nThe following classifier structure was used for LCS in the experiments: Each \nclassifier has 16 binary bits in the condition field: two bits for each of the 8 \nneighbouring squares, with 00 representing the situation that the square is empty, 11 \nthat it contains food (F), and 01 that it is an obstacle (T). \nthe general LCS’s parameters used for ZCS, ZCSAR+Roulette, and \nZCSAR+Tournament are set as follows: β=0.6, =0.1, \nGA\n\n=0.25, =0.5, χ=0.5, \nμ=0.002, \n0\nS =20.0, \n#P =0.33, N =800. Some specific parameters are set as follows: \nfor ZCSAR+Roulette, and ZCSAR+Tournament, \nmax\n\n\n=0.005, \nmin\n\n\n=0.00001; and in \nZCS, =0.71. The detailed description of these parameters is available in [6] and [8]. \n6.2 Experimental Results and Discussions \nIn \nthe \nfirst \nexperiment, \nwe \napplied \nZCS, \nZCSAR+Roulette \nand \nZCSAR+Tournament to Maze6 environment (Figure 1(a)). Maze6 is a typical and \nsomewhat difficult environment for testing the learning systems since the goal \nposition for agents to reach is hidden by some obstacles, and there is not any \nregularity in it. Each sensory-action pair in this maze almost needs a special classifier \nto cover (i.e. it only allows few generalizations), so ZCS is likely to produce \nover-general classifiers in it. Besides, the optimal solution in Maze6 requires the agent \nto perform long sequences of actions to reach the goal state. The optimal average path \nto the food in Maze6 is 5.19 steps. This experiment is used to show that ZCS with \naverage reward reinforcement learning can solve the general maze problem. \nFigure \n2 \nreports \nthe \nperformance \nof \nZCS, \nZCSAR+Roulette \nand \nZCSAR+Tournament in Maze6 environment. In the three cases, the results all \nconverge to near optimum during the last 2000 exploitation problems, and there is \nalmost no difference between them, about 5.85, 6.21, and 6.02 respectively. \nZCSAR+Roulette and ZCSAR+Tournament can almost perform as well as ZCS in \nthis environment. During the learning period (first 10000 problems), the three \nsystems’ performance deviates from optimum, since the GA continues to function and \nprobabilistic action selection (roulette wheel selection or tournament selection) is \nused. In addition, ZCSAR+Tournament changes continuously and oscillates heavily \nwithin the first 10000 learning problems, which is possibly caused by tournament \nselection used as action selection mechanism in match set [M].  \n0\n2000\n4000\n6000\n8000\n10000\n12000\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nMaze6\nNumber of Steps to Goal\nNumber of Problems (Trials)\n ZCS\n ZCSAR+Roulette\n ZCSAR+Tournament\n Optimum 5.19\n \nFigure 2. Performance of applying ZCSAR+Roulette and ZCSAR+Tournament to \nMaze6, compared with ZCS. Error bars represent the standard error. Curves are \naverages over 10 experiments. \n \n0\n2000\n4000\n6000\n8000\n10000\n12000\n0\n50\n100\n150\n200\n250\n300\nWoods14\nNumber of Steps to Goal\nNumber of Problems (Trials)\n ZCS\n ZCSAR+Roulette\n ZCSAR+Tournament\n Optimum 9.5\n \nFigure 3. Performance of applying ZCSAR+Roulette and ZCSAR+Tournament to \nWoods14, compared with ZCS. Error bars represent the standard error. Curves are \naverages over 10 experiments. \n \nIn the second experiment, the testing environment is Woods14 (Figure 1(b)), \nwhich is a corridor of 18 blank cells and a food cell at the end. The optimal average \npath to the food in Woods14 is 9.5 steps. The agent needs longer sequences of actions \nto reach the goal position, resulting in a sparser reception of delayed reward. So, it is \ncomplex to most LCSs[15].  \nIt can be seen from Figure 2 that, in Woods14, performances of the three systems \noscillate above the optimum during training period, while evolve promising solutions \nduring the last 2000 exploitation problems. ZCSAR+Tournament gets about 9.50 \nsteps to find food, and ZCS gets about 10.70 steps. ZCSAR+Roulette performs less \nwell (near optimum) and converges to about 21.36 steps. ZCSAR+Tournament can \nget the optimal solution in Woods14. This seems because of the average reward \nreinforcement learning and tournament selection employed by ZCSAR+Tournament, \nwhich guarantees the system can disambiguate those early states in the long action \nchains effectively. \n7 Conclusions \nIn this paper, due to the similarity between Sarsa and bucket-brigade algorithm \nin ZCS, and the similarity in form between Sarsa algorithm and R-learning, \nbucket-brigade algorithm in ZCS is replaced with R-learning through some \nmodifications. R-learning is an undiscounted reinforcement learning technique to \noptimize average reward, which is a different metric from the discounted reward \noptimized by bucket-brigade algorithm. Thus, ZCS with R-learning, ZCSAR, is able \nto maximize the average reward per time step, not the cumulative discounted rewards. \nThis is helpful to support long action chains in large multi-step learning problems.  \nHowever, R-learning will cause some classifiers’ strength is negative in ZCSAR. \nThis does not meet the supposition that classifier’s strength is greater than zero in \nZCS. Specially, roulette wheel selection based on classifier’s strength (or its \nreciprocal) used in ZCS requires that classifier’s strength is positive. To address this \nproblem, \ntwo \nextended \nsystems \nare \npresented: \n\"ZCSAR+Roulette\" \nand \n\"ZCSAR+Tournament\". ZCSAR+Roulette indicates ZCSAR with roulette wheel \nselection and truncation method, while ZCSAR+Tournament indicates ZCSAR with \ntournament selection. Truncation means to cut off those negative strength values, set \nthem to zero. \nWe test ZCSAR+Roulette and ZCSAR+Tournament on two well-known \nmulti-step problems, compared with ZCS. Overall, experiments show that \nZCSAR+Tournament can evolve optimal or near-optimal solutions in these typically \ndifficult multi-step environments, while ZCSAR+Roulette can just reach the \nsuboptimum in Woods14 environment. Especially in Woods14 environment, the \nperformance of ZCSAR+Tournament is very good, but ZCS just reaches a \nnear-optimal performance. \nBecause of the basic change of the reinforcement learning employed by ZCS, \nand \ntournament \nselection \nis \nused \nto \nreplace \nroulette \nwheel \nselection, \nZCSAR+Tournament still needs some extra testing to study their performance in \nother problems. Additionally, we plan to consider the impact of average reward \nreinforcement learning in ZCS when the environment is stochastic. \n \nReferences: \n [1]. Bull, L., A brief history of learning classifier systems: from CS-1 to XCS and its variants. \nEvolutionary Intelligence, 2015: p. 1-16. \n [2]. Ebadi, T., et al., Human-interpretable Feature Pattern Classification System using Learning \nClassifier Systems. Evolutionary Computation, 2014. 22(4): p. 629-650. \n [3]. Tzima, F.A. and P.A. Mitkas, ZCS Revisited: Zeroth-Level Classifier Systems for Data Mining, \nin Proceedings of the 2008 IEEE International Conference on Data Mining Workshops. 2008, IEEE \nComputer Society: Washington, DC, USA. p. 700--709. \n [4]. Cadrik, T. and M. Mach, Control of agents in a multi-agent system using ZCS evolutionary \nclassifier systems, in 2014 IEEE 12th International Symposium on Applied Machine Intelligence and \nInformatics (SAMI). 2014, IEEE: Herl'any, Slovakia. p. 163-166. \n [5]. Cádrik, T. and M. Mach, Usage of ZCS Evolutionary Classifier System as a Rule Maker for \nCleaning Robot Task, in Emergent Trends in Robotics and Intelligent Systems, P. Sinčák, et al., P. \nSinčák, et al.^Editors. 2015, Springer International Publishing. p. 113-119. \n [6]. Wilson, S.W., ZCS: A zeroth level classifier system. Evolutionary Computation, 1994. 2(1): p. \n1-18. \n [7]. Wilson, S.W., Classifier Fitness Based on Accuracy. Evolutionary Computation, 1995. 3(2): p. \n149-175. \n [8]. Bull, L. and J. Hurst, ZCS Redux. Evolutionary Computation, 2002. 10(2): p. 185-205. \n [9]. Schwartz, A., A reinforcement learning method for maximizing undiscounted rewards, in \nProceedings of the Tenth International Conference on Machine Learning, P. Utgoff, P. Utgoff^Editors. \n1993, Morgan Kaufmann. p. 298-305. \n[10]. Singh, S.P., Reinforcement learning algorithms for average-payoff Markovian decision processes, \nin Proceedings of the twelfth national conference on Artificial intelligence (vol. 1). 1994, American \nAssociation for Artificial Intelligence: Menlo Park, CA, USA. p. 700--705. \n[11]. Sutton, R.S. and A.G. Barto, Reinforcement learning: an introduction. Adaptive computation and \nmachine learning. 1998, Cambridge, MA: MIT Press. \n[12]. Mahadevan, S., Average reward reinforcement learning: Foundations, algorithms, and empirical \nresults. Machine Learning, 1996. 22: p. 159-195. \n[13]. Zatuchna, Z. and A. Bagnall, A learning classifier system for mazes with aliasing clones. Natural \nComputing, 2009. 8(1): p. 57-99. \n[14]. Butz, M.V., K. Sastry and D.E. Goldberg, Strong, Stable, and Reliable Fitness Pressure in XCS \ndue to Tournament Selection. Genetic Programming and Evolvable Machines, 2005. 6(1): p. 53-77. \n[15]. Zang, Z., et al., Learning classifier system with average reward reinforcement learning. \nKnowledge-Based Systems, 2013. 40(0): p. 58 - 71. \n \n",
  "categories": [
    "cs.AI",
    "cs.NE",
    "I.2"
  ],
  "published": "2016-04-26",
  "updated": "2016-04-26"
}