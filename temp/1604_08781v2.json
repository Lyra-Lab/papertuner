{
  "id": "http://arxiv.org/abs/1604.08781v2",
  "title": "Teaching natural language to computers",
  "authors": [
    "Joseph Corneli",
    "Miriam Corneli"
  ],
  "abstract": "\"Natural Language,\" whether spoken and attended to by humans, or processed\nand generated by computers, requires networked structures that reflect creative\nprocesses in semantic, syntactic, phonetic, linguistic, social, emotional, and\ncultural modules. Being able to produce novel and useful behavior following\nrepeated practice gets to the root of both artificial intelligence and human\nlanguage. This paper investigates the modalities involved in language-like\napplications that computers -- and programmers -- engage with, and aims to fine\ntune the questions we ask to better account for context, self-awareness, and\nembodiment.",
  "text": "Teaching natural language to computers\nJoseph Corneli\nMiriam Corneli\nDepartment of Computing\nFormer Senior English Language Fellow\nGoldsmiths College, University of London\n(Nepal / Sri Lanka), US Department of State\nj.corneli@gold.ac.uk\nlanguagecoach.miriam@gmail.com\nAbstract\n“Natural Language,” whether spoken and attended\nto by humans, or processed and generated by com-\nputers, requires networked structures that reﬂect\ncreative processes in semantic, syntactic, phonetic,\nlinguistic, social, emotional, and cultural modules.\nBeing able to produce novel and useful behav-\nior following repeated practice gets to the root of\nboth artiﬁcial intelligence and human language.\nThis paper investigates the modalities involved in\nlanguage-like applications that computers – and\nprogrammers – engage with, and aims to ﬁne tune\nthe questions we ask to better account for context,\nself-awareness, and embodiment.\n1\nIntroduction\nIn the genesis of intelligent computational systems, one\noften observes programs that write before they can read,\ncompose before they can listen, and paint before they can\nsee. However, the most successful systems in poetry, mu-\nsic, and visual art are indeed perceptually aware – and de-\nrive signiﬁcant beneﬁts from that ability [Kurzweil, 1990;\nScreene and Wiggins, 2015; Colton et al., 2015b].\nWe will argue that the bar by which we judge computa-\ntional creativity in text generation – and appreciation – can\nand must be raised, in order to build systems that we can\nmeaningfully communicate with. Our paper takes the form\nof a necessarily high-level sketch, supported by hand-crafted\nexamples that draw on both standard and custom software.\nMost of these examples concern computer poetry, but our aim\nis not to present a technical or aesthetic achievement. Rather,\nwe use the examples to survey the limitations of current sys-\ntems and to indicate some potentially novel approaches.\nSpeciﬁcally, we highlight three interrelated thematic areas\nthat we think will repay effort.\n(Limited) Contextual Understanding\nHere, we are con-\ncerned with what makes a response to some circumstance\nmeaningful.\nThis certainly requires context [Ogden and\nRichards, 1923]. For example, a response in a conversational\ndialog generally considers the previous elements of the ex-\nchange, and perhaps also previous exchanges, and elements\nof a shared culture. This understanding does not need to be,\nnor in general can it be, “complete.” Furthermore, in many\ncases, the reader or listener will hear meanings that were not\nthere originally [Veale, 2015] – however, the interpolation of\nmeaning by the reader cannot always be relied upon. Later\nin this paper we include an example of a computer-generated\npoem that is essentially just verbose babbling ﬁtted to a prede-\nﬁned template. This poem was previously reviewed by a pub-\nlished poet, and it does not stand up well to critical scrutiny\n[Corneli et al., 2015]. The poem misses any sense of “why” –\nand the program that generated this poem would not be able\nto offer what the Provençal poets called a razo [Agamben,\n1995, p. 51], that is, an exposition of the poem.\n(Limited) Self-awareness\nHere, we focus exclusively on\nthe ability to reason about creativity as a process. Computer\nprograms often have limited metadata about their software\nprocesses, for example the “signatures” of functions, specify-\ning the types of input data that the functions will accept, or\n“contracts” specifying preconditions, postconditions, and in-\nvariants. Along with these representations often comes some\nlimited ability to reason formally about code. But few if any\ncontemporary software systems would be able to convinc-\ningly make sense of a simple writing prompt, or adapt a dia-\nlogical process of response in order to reach an agreement, or\nrespond to feedback from a critic in order to produce a better\npoem. In the future, such computational abilities with lan-\nguage may be commonplace. These abilities may depend on\nfairly profound epistemic features, for example, the computer\nmight need to recognize when its “knowledge” is uncertain,\nand proceed accordingly – perhaps asking for help, or making\nmultiple generative attempts in parallel and assessing which\none works better relative to its contextual understanding.\n(Limited)\nEmbodiment\nWe\nwill\nconsider\nprogram\nﬂowcharts [Charnley et al., 2014] as the primary framework\nwith which to describe the computer’s “process” layer.\nWhether embodied as a ﬂowchart or a Von Neumann ma-\nchine or something else, computational processes are also\nphysical processes.\nThe manipulations of the nodes and\narrows of a ﬂowchart or of some other collection of physical\nobjects, like the ﬂowchart’s corresponding script, or the\nwords of a poem, can (potentially) be thought about with\narXiv:1604.08781v2  [cs.CL]  28 Jun 2016\nrespect to its gestural content.\nThis deﬁnition of gesture\ndue to the 12th Century theologist Hugues de Saint-Victor,\nquoted in [Mazzola, 2016], shows the connection with our\nother themes:\nGestus est motus et ﬁguratio membrorum\ncorporis, ad omnem agendi et habendi modum.\nMazolla\nglosses this as follows:\nGesture is the movement and ﬁguration of the\nbody’s limbs with an aim, but also according to the\nmeasure and modality proper to the achievement of\nall action and attitude.\nIntroducing this quote requires us to make one signiﬁcant\ncaveat. Whereas humans tend to perceive ourselves as rela-\ntively free beings, able to act according to a purpose and even\nto choose which purpose to serve, we regard computers in a\nvery different light. At best, a computer can be programmed\nto optimize its behavior relative to some constraint. This per-\nspective does not sit well with the typical understanding of\nthe English word “aim.”1 The point to make here is that a\ncomputational system is understood relative to an operating\nenvironment, and its behavior is worked out relative to that\nenvironment. Under some circumstances we would call this\nprocess “programming,” and under somewhat different cir-\ncumstances we would describe it as “self-programming” or\n“automatic programming.” In short, it is not necessary to\nattribute intention to the computer, but – once again – it is\nnecessary to think about its behavior in context.\nThe remainder of the paper is structured as follows: First,\nwe will explore these themes from a computing standpoint,\ndeveloping a technical sketch rather than a formal system de-\nscription. Then we turn to a discussion that evaluates this\ntechnical sketching from the point of view of the second au-\nthor, an English as a Foreign Language teacher with a prior\nbackground in consciousness studies. Finally, we draw on\nthis discussion to outline a plan for future computational ex-\nperiments centered on making sense of language.\n2\nMotivation\nOscar Schwartz offers the following framing:\nCan a computer write poetry? This is a provoca-\ntive question. You think about it for a minute, and\nyou suddenly have a bunch of other questions like:\nWhat is a computer? What is poetry? What is cre-\nativity? [Schwartz, 2015]\nComputer poetry may sound like a bit of a lark – after all,\nit’s not clear that anyone really needs it. Nevertheless, ask-\ning these questions about poetry begins to suggest a way of\nworking with language that has wider implications.\nConsider Turing’s idea that machines “would be able to\nconverse with each other to sharpen their wits” [Turing,\n1951]. This could be realized as a Q&A site speciﬁcally for\ncomputers. The discussions could address all manner of prac-\ntical concerns, for example, those arising for bots that are en-\ngaged in code or editorial review tasks. A reputation system\nand web of trust could be used to maintain quality control. If\n1It would appear that Mazolla freely introduced this concept, in-\nstead of sticking with the more literal “agenda.”\nthe participating computational systems had sufﬁcient abili-\nties with natural language, this system could be bridged into\na Q&A site that is in everyday use by human beings. How-\never, before a computational system would be useful in any\nQ&A context, it would presumably need to be able to be able\nto model the meanings of the questions that are being asked\nreasonably well, and also to be able to compose meaningful\nresponses. For now, we will side-step the Chinese Room-\nstyle question of whether the system “really” understands\nwhat it is processing [Bishop, 2004] and focus on the more\napplied question: how would meaningful responses occur?\nA high-level outline could be something like this [Corneli et\nal., 2015]: (1) Read and understand the “prompt” to a sufﬁ-\ncient degree; (2) Compose a response that “makes sense”; (3)\nCriticize the response along various dimensions, for instance,\ndoes it read well, does it tell a story or develop a character?;\n(4) Consider how it might be improved. This outline is based\non an established process that groups of creative writers use\nto critique and revise poetry or literary works for publication.\nWhen we turn to computer generated text – say, poetry, to\nbe concrete – in addition to examining the generated poem, a\nsophisticated audience can also examine the process whereby\nthe poem was generated, and read the product against this\nprocess (or vice versa). Indeed, the computer can create both\npoem and process, the later via automatic programming – and\noffer its own assessments of both as well, as long as it can\nmake sense of the success criteria.\n2.1\nRelated work\nNatural language processing often begins with a grammar. If\nnone is available, it may be induced, for instance by using\ncompression techniques [Wolff, 1988]. Both in older [Red-\nington and Chater, 1997] and quite recent work [Hermann\net al., 2015], statistical and neural network approaches to\ncorpus-based language understanding have shown strong po-\ntential for developing “reasoned” ways of thinking about lin-\nguistic structure without the usual grammatical assumptions.\nCorpus methods help to understand the patterns in the way\npeople use language, and the creative potential of unexpected\nword combinations [Hoey, 2005].\nOne example cited by Hoey is Tennyson’s famous line\nTheirs is not to reason why. Here, the word reason is used\nwith its verb sense, rather than with the noun sense that most\nreaders would expect based on their prior experience with the\ntwo-word phrase reason why. This unique feature makes the\nline memorable and interesting. The psycholinguistic prop-\nerties of the broader phenomenon of “lexical priming” have\nbeen extensively studied [Pace-Sigge, 2013]. One empirical\nresult is that priming works differently for native speakers and\nfor non-native ESL speakers, insofar as native speakers are\nmore affected by binding of words within formulas, whereas\nsomeone learning a new language tends to only recognize the\nstrings that they have encountered before.\nThe ascendant status of data-driven methods in natural lan-\nguage processing does not obviate symbolic AI, which con-\ntinues to be useful for work with specialist languages. For\ncomputer programming languages in particular, techniques\nfor reﬂection and, in the case of LISP, homoiconicity (i.e.,\ntreating code as data) make it possible to write computer pro-\ngrams that reason about, write, or adapt computer programs.\nArtiﬁcial languages have been used in video games in cre-\native ways, but not, as yet, for functional communication\nwith or between non-player characters. Multi-agent systems\nhave, however, been used in poetry generation. One example\nsystem creates poems based on repeated words and sounds\ndriven by a model of agents’ emotional states [Kirke and Mi-\nranda, 2013], inspired by earlier work in music [Kirke, 2011].\nAs for computer programs that read poetry, this is typically\nlimited to reading (and mimicking) surface style, without ex-\ntending to meaning [Carslisle, 2000] – even if some readers\nwere fooled. Without knowing what’s in a poem, it seems\ndifﬁcult to be other than superﬁcial.\nJust how far the “surface” goes is a question much dis-\ncussed by poets and translators of poetry. Red Pine, transla-\ntor of the 13th-14th Century Chinese poet and Buddhist monk\nStonehouse (石屋) wrote as follows [Pine, 2014, p. xxiv]:\nI don’t know how others do it, but when I’ve tried\nto think of a metaphor for what I go through, I keep\ncoming up with the image of a dance. [ ...] I try to\nget close enough to feel the poet’s rhythm, not only\nthe rhythm of the words but also the rhythm of the\npoet’s heart.\nA typical approach to poetry generation might take Stone-\nhouse’s corpus and notice that he often writes about clouds\nand mountains and plants, and attempt to generate a poem\n“in the style of Stonehouse,” referencing some of these typ-\nical concepts and aiming to get the number of syllables and\nthe grammatical structure right. However, there is little doubt\nthat a reader with an ear for Chinese poetry or some familiar-\nity with the ideas of Chan Buddhism would recognize these\nersatz attempts for what they are: “dead words” [Pine, 2014,\np. xxiii]. More interesting computer poems in a somewhat re-\nlated genre (Japanese haiku) have been created by program-\nmers working from the premise [Rzepka and Araki, 2015,\np. 2497] that a reader’s interest in a haiku stems from:\nfeeling that the poet understands a situation and that\nwe can mentally agree with what she/he (or maybe\nit) shares with us.\nThe challenges posed by computer poetry serve as a point\nof departure. “Poetry exercises are used to allow learners to\nexplore the complexities of English” [Parker, 2010] – or an-\nother language – and the contexts in which it is meaningful.\n3\nExploration\nHere are 10 short examples of writing prompts excerpted\nfrom the book “642 Things to Write About” [San Francisco\nWriters’ Grotto, 2012]:\n1. What can happen in a second.\n2. The worst Thanksgiving dish you ever had.\n3. A houseplant is dying. Tell it why it needs to live.\n4. Tell a story that begins with a ransom note.\n5. Write a recipe for disaster.\n6. If you had one week to live....\n7. What your desk thinks about at night.\n8. The one thing you are most ashamed of...\n9. Describe your best friend.\n10. Describe Heaven.\nIn order to create a computer-generated response to any of\nthese prompts, in addition to understanding what the prompt\nis saying, some further understanding of the topic is required.\nThe response itself will have various standard features. Many\nof the following features of stories are found (with minor\nadaptations) in poetry, and other kinds of writing:\nA story is not a modular presentation of ideas but\na multi-layered work consisting of interdependent\ncharacters, plot elements, and settings. [Kim et al.,\n2014]\nLet us consider, then, a simple theory of stories and story-\ntelling, using the prompts above as our domain. One suitable\ntheory would involve a micro-world containing: A Scenario,\nA Narrator, An Audience, A Beginning, A Middle, and An\nEnd. Consider that – with respect to the “Thanksgiving dish”\nprompt – the computer has presumably never tasted food of\nany kind. It could, however, “imagine” a scenario in which\nthere is a character who eats a Thanksgiving dish.\n3.1\nAn example Scenario\nThe scenario could be represented with various relations:\nSquanto memberOf Patuxet tribe, Patuxet tribe\nhasCardinality 1, Thanksgiving isa event,\nThanksgiving hasHost Pilgrims, Thanksgiving\nhasGuest Squanto, Thanksgiving hasFood eel,\neel hasCondition burnt\nNaturally, this might be extended with further information\nas that comes to light; and in practice we might use a more\nrobust formalism.\n3.2\nThe other components\nThe Narrator would walk through the scenario and say what’s\nthere. On a metaphorical level, the narrator’s role is some-\nwhat similar to the way a virtual camera moves through a 3D\nsimulation in order to create a ﬁlm. However, the Narrator\nneeds to consider the Audience in order to be effective. As\nindicated in the quote from Kim et al., above, the data in the\nScenario needs to be structured when telling the story.\nHere is one possible presentation of the scenario, embel-\nlished with some facts, ﬁctions, and local color, and combined\ninto several sentences that ﬂow reasonably well.\nSquanto was the last surviving member of the\nPatuxet tribe. He attended the ﬁrst Thanksgiving\nwith the Pilgrims wearing a new buckskin jacket.\nOne of the foods that was served was eel, but it had\nbeen rather badly burnt and Squanto didn’t ﬁnd it\nto his liking.\nThis text manages to include a range of emotionally evoca-\ntive, thought provoking, character establishing, and sen-\nsory language (“last surviving member,” “ﬁrst Thanksgiving,”\n“new buckskin jacket,” “rather badly burnt,” “didn’t ﬁnd it to\nhis liking”) – and at least one unexpected word combination\n(“Thanksgiving... eel”). It also has a discernible Beginning,\nMiddle, and End. It seems appropriate to call it a story.\n3.3\nHow to come up with stories?\nLet’s start with a parse:\n(TOP\n(NP\n(NP (DT The) (JJS worst)\n(NNP Thanksgiving) (NN dish))\n(SBAR (S (NP (PRP you)) (ADVP (RB ever))\n(VP (VBD had)))) (. .)))\nHere are some associated word meanings from WordNet:2\nWord\nGloss from WordNet\n(DT The)\ndeterminer\n(JJS worst)\n(superlative\nof\n‘bad’)\nmost\nwanting in quality or value or\ncondition\n(NNP\nThanksgiving)\ncommemorates a feast held in\n1621 by the Pilgrims and the\nWampanoag\n(NN dish)\na particular item of prepared\nfood\n(PRP you)\npronoun\n(RB ever)\nat any time\n(VBD had)\nserve oneself to, or consume\nregularly\nThere are some other interesting word senses available, and\nknowing which one to pick, or how (and how much) to com-\nbine various senses seems like a bit of an art form. Should we\nconsider a short prayer of thanks before a meal when thinking\nof “Thanksgiving”? For now, we will say a short prayer and\ntentatively assume that a (NN dish) is what you eat, rather\nthan what you eat off of. “Lexical priming” [Hoey, 2005]\ntechniques would help make the relevant distinction here.\nBut supposing we get this far, now what? We’ve moved\nfrom one sentence to several quasi-sentences, without get-\nting that much closer to a “Scenario” like the one envis-\naged above. One possibility is that the WordNet expansion\ncould be sufﬁcient give us relevant keywords and phrases,\nfrom which a small corpus could be built (e.g., by doing a\nweb search for the glosses) and then mined to learn relations\nbetween the items in that corpus. Alternatively or addition-\nally, these meanings might be connected to a pre-computed\nmodel of linguistic meaning drawing from a much larger\nbackground corpus [McGregor et al., 2016].\nMining signiﬁcant associations from large scale text cor-\npora is something people have explored in various ways.\nFinding subject-verb-object triples, in particular, is a popu-\nlar method: one well-known algorithm is presented by Rusu\net al. [2007]. This is sometimes called “building a seman-\ntic model.” Other more sophisticated approaches might draw\non associations with a pre-existing ontology [Kiryakov et al.,\n2004] – the particular beneﬁt of the Rusu et al. approach\nis that it can be implemented using a simple parsing-based\nmethod. The basic theme of building semantic models of text\ngoes back to Quillian [1969] – about whom there will be more\nto say later. For now, we just remark that in addition to ex-\npanding the writing prompt, we may also want to draw on\nsome “stock” associations stored in a background knowledge\nbase. We illustrate the method from Rusu et al. by applying\nit to the beginning of the novel Frankenstein (Table 1).\nYou will rejoice to hear that no disaster has ac-\ncompanied the commencement of an enterprise\nwhich you have regarded with such evil forebod-\nings. ... [Shelley, 1831] (emphasis added)\n1.\ndisaster\nregarded\nforebodings\n2.\nyesterday\nincreasing\nconﬁdence\n3.\nLondon\nﬁlls\ndelight\n4.\nfeeling\nunderstand\nfeeling\n5.\nbreeze\ngives\nforetaste\nTable 1: 5 triples extracted from Frankenstein\nAs far as text understanding goes, the result in Table 1 is\nnot particularly encouraging. However, even a low-ﬁdelity\ndatabase of background knowledge would allow us to extend\nthe story. Perhaps Squanto would decide that the burnt eel\ngives a foretaste of things to come. To make this associa-\ntion, we might use methods similar to the ones used to rea-\nson about ConceptNet triples – which have been employed to\ngood effect in text and concept generation within the FloWr\nframework [Llano et al., 2016], which is described below.\n3.4\nCan FloWr ﬂowcharts be used to solve the\ncomposition problem?\nFloWr [Charnley et al., 2014] is a ﬂowcharting system with\nbasic text processing abilities. Metadata describing the “Pro-\ncessNodes” from which FloWr’s ﬂowcharts are formed can\nbe used to pose and solve simple automatic programming\nproblems. A writing prompt like “worst Thanksgiving dish”\ncan be interpreted as a constraint – or, more broadly, a ﬁt-\nness function – that steers the generative process, and trickles\nthrough to guide the choice of functional components and,\neventually, words. By “ﬁtness function,” we understand that\ntext may be composed iteratively, and improved along the\nway, relative to some context of evaluation. The simple ex-\namples in Figure 1 illustrate problems that can be solved quite\nstraightforwardly:\n“Give me a list of mixed adjectives and nouns.”\n(the italicized terms are “independent” variables that tell the\nWordSenseCategoriser node how to behave); or\n“Give me a list of 5 rhyming couplets built of text\nfrom The Guardian and Twitter mentioning ‘eels’.”\nIt may make sense to include considerably more abstrac-\ntion in the description of larger and more complex ﬂowcharts.\nFor example, a ﬂowchart discussed by Corneli et al. [2015]\nincludes 28 nodes and generates the following poem (and oth-\ners that are similar):\nOh dog the mysterious demon\nWhy do you feel startle of attention?\nOh demon the lonely encounter\nghostly elusive ruler\nOh encounter the horrible glimpse\nhelpless introspective consciousness\n2NB., WordNet contains no entries for determiners or pronouns.\nFigure 1: Two simple FloWr ﬂowcharts\nnumber N\nDictionary\nlist of words of length N\nlist of words, word sense\nWordSenseCategoriser\nlist extracting words with the word sense\ninput string(s)\nTextRankKeyphraseExtractor\nlist of key phrases extracted from the strings\nphrases, number M of phonemes\nRhymeMatcher\ntuple of couplets with M rhyming phonemes\nnumber N, word\nTwitter\nsome N tweets containing the word\ntuples\nTuplesAppender\na list combining the tuples\nlists\nListAppender\na list combining the lists\nTable 2: Triples describe the functional mapping from input to output for selected FloWr nodes\nWould the most succinct description of the ﬂowchart be\nan approximately 28 clause sentence that is equivalent to the\nﬂowchart? Perhaps, in the current case, everything can be\ncompressed down to the following template (ﬁxed for this\nﬂowchart), and potentially further with optimizations:\nOh THEME the COLLOCATE SIMILAR\nWhy do you feel INVERSION of DESIRED?\nOh SIMILAR the COLLOCATE-OF-SIMILAR SIMILAR-\nTO-SIMILAR\nSIMILAR-TO-COLLOCATE SIMILAR-TO-COLLOCATE′\nCOLLOCATE-OF-SIMILAR\nOh\nSIMILAR-TO-SIMILAR′\nthe COLLOCATE-OF-\nSIMILAR-TO-SIMILAR SIMILAR-TO-SIMILAR-TO-\nSIMILAR\nSIMILAR-TO-COLLOCATE SIMILAR-TO-COLLOCATE\nCOLLOCATE-OF-COLLOCATE\nThe connection between this template, its instantiation, and\nthe putative prompt, “Write a poem about an old dog who is\nafraid of attention,” is tenuous at best. Nevertheless, reason-\nable hope exists for future work that would generate models\n– and a Narrator – tailor-made to a given prompt.\nFloWr’s ProcessNodes deﬁne a micro-language denoting\nthe available ways in which the system can transform input\ndata to output data (Table 2). In a meaningful expansion of\na given prompt, many choices would have to be made. New\nﬂowcharts built in response to writing prompts or other con-\ntextual data would constitute the system’s core “learnings.” In\nshort, poetry and process need to be thought about together.\n4\nDiscussion\nIn this section, we will brieﬂy review human language learn-\ning from a second language teaching perspective, and then\ndraw comparisons with the foregoing description of a hypo-\nthetical computer language learner.\nFirst, why are primary language learning and second/\nforeign language learning often considered separately? One\nof the biggest differences between the two cases is that af-\nter learning a ﬁrst language, “neural pathways” have been set\ndown, so that second language information has to be encoded\n“along with” or “beside” the ﬁrst language pathways. New\nneural connections have to be formed to maintain the mem-\nory of the second language, whereas the ﬁrst language has\nbeen implanted quite thoroughly. So when learning verbs,\nnouns, etc., the person (child, adult) learns or has learned, for\ninstance, “stand up!” ﬁrst and then that “levantate!” means a\nsimilar thing, in Spanish, or that “chair” = “silla.” As they\nprogress in the second language, what we ﬁnd is a process\nthat linguists call “interlanguage” [Selinker, 1972], or a lan-\nguage that is not quite English, and not totally Spanish yet,\nbecause of the variations in the two languages in structure,\nphonemes, morphemes, allophones, semantics, and so on.\nLanguage is also very multi-modal, and if we think again\nin terms of neurons and brain pathways, the word “dog” not\nonly brings up images of a furry canine animal but all kinds\nof other associations. Pet dogs. Large and small dogs. Dogs\nwith different color fur (visual cortex). Dog as a verb (mo-\ntor cortex associations). The smell of wet dogs (olfactory\nbulb). Fear of dogs (limbic system). Favorite pet dogs you\nhave loved in your life (emotions). The spelling or sound\nof the word D-O-G as opposed to D-A-W-G (auditory and\nvisual cortex).\nIf you are from certain countries perhaps\neven dog meat. Imagining a “demon dog” will invoke net-\nworks running all over the brain [Schlegel et al., 2013]. Now\nwhat about computers, do they have a similar symbolic or\nmulti-modal operating ability? People, as they become lit-\nerate, learn to write and sound out letters at the same time\n– and the foundations of ﬁrst language learning draw heav-\nily on a sensory-motor channel [Iverson and Thelen, 1999;\nHernandez, 2013].\nComputers may be said to have a ﬁrst language with sev-\neral dialects, highly constrained by grammar – namely, byte-\ncode and programming languages. The ﬂowcharts described\nabove begin to recover a degree of multi-modality, and a pro-\ncess orientation that is similar in certain partial respects to\nsensory-motor experience. We might also think of ﬂowcharts\nas akin to neurons and cortexes, as above. After all, there is\na kind of embodiment even in the brain – it is an active and\nevolving organ [Doidge, 2007]. Flowcharts are rather differ-\nent from classical neural network models, but one common\nfeature is that they would need to be “trained” if they are to\nunderstand and express language.\nAgain, poetry could be part of the way forward. Exercises\nfrom a book like Writing Simple Poems: Pattern Poetry for\nLanguage Acquisition [Holmes and Moulton, 2001] might be\nused to teach computers as well as humans. Note that gram-\nmar and poetry are very different, and perhaps complemen-\ntary. Thus, for example, a “Learn English!” notebook found\nin Japan with Subject – Verb – Direct Object – Indirect Object\n– Prepositional Phrase – Adjectival Phrase written at the top\nof each page offers a useful rubric for Japanese students, since\nthe verb comes at the end of the sentence in Japanese (SOV).\nBut grammar will only get you so far. Consider Chomsky’s\nfamous nonsense statement, “Colorless green ideas sleep fu-\nriously.” A poetic gesture – like preﬁxing that phrase with a\ndescription of “planted tulip bulbs” – is able to make some\nsense out of nonsense by adding context!3\nSocial context is also likely to be relevant: peer learning\nis very useful for human language learners [Rancière, 1991;\nRaw, 2014]. One idea would be to adapt the Q&A model\nmentioned in Section 2 as a “social” site about poetry.\n5\nFuture work\nQuillian’s “The Teachable Language Comprehender: A Sim-\nulation Program and Theory of Language” [1969] took the\nnovel – and fundamental – approach of understanding things\nin such a way that new understandings could be added di-\nrectly to its knowledge base.4 When reading a piece of text,\nthe TLC program would search its memory for related infor-\nmation that it could use to make sense of the input. More\nspeciﬁcally, a given text would be expanded using “form\ntests” which extracted meaningful pieces of the text, and con-\nnected these to items stored in memory. Quillian writes that\n“ultimately, a human-like memory should relate descriptive\nknowledge of the world to perceptual-motor activity, in a\nmanner like that indicated by Piaget” – but deems this to be\n“far beyond our present scope” [Quillian, 1969, p. 474].\nFuture research might use FloWr to develop a TLC-like li-\nbrary of “form tests” and generative tools that would add a\nmulti-modal aspect to knowledge representation. To be sure,\na ﬂowchart-based representation of poetic process would be\nquite different from the embodied sensory-motor experience\nof humans. Nevertheless, computational processes that allow\nus to model text generation contextually, procedurally, and\ngesturally can help to understand the way linguistic mean-\ning comes to be. This is not something we can readily learn\nfrom parsing, corpus-based modeling, or grammar-based text\n3http://www.linguistlist.org/issues/2/2-\n457.html#2\n4In practice, “While the monitor can add TLC’s encoded output\nto the program’s memory, the program itself makes no attempt to do\nso, nor to solve the problems inherent in doing so” [Quillian, 1969,\np. 473].\ngeneration alone. There is exciting potential for future ex-\nperiments with natural language that strives to capture and\nexpress shades of meaning and the “feel” of the language.\nExperimentation is necessary: if we have learned anything\nabout language, it is that “learners should be motivated to\nspeak bravely” [Wang, 2014].\n6\nAcknowledgments\nJoseph Corneli’s work on this paper was supported by the\nFuture and Emerging Technologies (FET) programme within\nthe Seventh Framework Programme for Research of the Eu-\nropean Commission, under FET-Open Grant number 611553\n(COINVENT). Thanks to Teresa Llano for conversations\nabout her work on the “demon dog” poem, quoted above.\nReferences\n[Agamben, 1995] Giorgio Agamben. Idea of prose. SUNY\nPress, 1995. (First published in 1985, as Idea della prosa.)\n[Bishop, 2004] J.M. Bishop.\nA view inside the Chinese\nroom. Philosopher, 28(4):47–51, 2004.\n[Carslisle, 2000] Judith\nP\nCarslisle.\nComments\non\nKurzweil’s Cybernetic Poet. AMCIS 2000 Proceedings,\npage 123, 2000.\n[Charnley et al., 2014] John Charnley, Simon Colton, and\nMaria Teresa Llano. The FloWr framework: Automated\nﬂowchart construction, optimisation and alteration for cre-\native systems.\nIn Simon Colton, Dan Ventura, Nada\nLavraˇc, and Michael Cook, editors, Proc. 5th International\nConference on Computational Creativity, 2014.\n[Colton et al., 2015a] S. Colton, H. Toivonen, M. Cook, and\nD. Ventura, editors. Proc. Sixth International Conference\non Computational Creativity, ICCC 2015. Association for\nComputational Creativity, 2015.\n[Colton et al., 2015b] Simon Colton, Jakob Halskov, Dan\nVentura, Ian Gouldstone, Michael Cook, P Blanca, et al.\nThe Painting Fool Sees! New Projects with the Automated\nPainter. In Colton et al. [2015a].\n[Corneli et al., 2015] Joseph\nCorneli,\nAnna\nJordanous,\nRosie Shepperd, Maria Teresa Llano, Joanna Misztal, Si-\nmon Colton, and Christian Guckelsberger. Computational\nPoetry Workshop: Making Sense of Work in Progress. In\nColton et al. [2015a].\n[Doidge, 2007] Norman Doidge. The brain that changes it-\nself: Stories of personal triumph from the frontiers of brain\nscience. Penguin, 2007.\n[Hermann et al., 2015] Karl Moritz Hermann, Tomas Ko-\ncisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. Teaching machines\nto read and comprehend. In Advances in Neural Informa-\ntion Processing Systems, pages 1684–1692, 2015.\n[Hernandez, 2013] Arturo Hernandez. The Bilingual Brain.\nOUP USA, 2013.\n[Hoey, 2005] Michael Hoey. Lexical priming: A new theory\nof words and language. Psychology Press, 2005.\n[Holmes and Moulton, 2001] Vicki L Holmes and Mar-\ngaret R Moulton. Writing simple poems: Pattern poetry\nfor language acquisition. Ernst Klett Sprachen, 2001.\n[Iverson and Thelen, 1999] Jana M Iverson and Esther The-\nlen. Hand, mouth and brain. The dynamic emergence of\nspeech and gesture.\nJournal of Consciousness Studies,\n6(11-12):11–12, 1999.\n[Kim et al., 2014] Joy Kim, Justin Cheng, and Michael S\nBernstein. Ensemble: exploring complementary strengths\nof leaders and crowds in creative collaboration.\nIn\nProc. 17th ACM conference on Computer supported coop-\nerative work & social computing, pages 745–755, 2014.\n[Kirke and Miranda, 2013] Alexis Kirke and Eduardo Mi-\nranda. Emotional and Multi-agent Systems in Computer-\naided Writing and Poetry. In Procs. of the Artiﬁcial In-\ntelligence and Poetry Symposium (AISB’13), pages 17–22,\nExeter University, Exeter, UK, 2013.\n[Kirke, 2011] Alexis Kirke.\nApplication of intermediate\nmulti-agent systems to integrated algorithmic composition\nand expressive performance of music. PhD thesis, Univer-\nsity of Plymouth, 2011.\n[Kiryakov et al., 2004] Atanas Kiryakov, Borislav Popov,\nIvan Terziev, Dimitar Manov, and Damyan Ognyanoff. Se-\nmantic annotation, indexing, and retrieval. Web Seman-\ntics: Science, Services and Agents on the World Wide Web,\n2(1):49–79, 2004.\n[Kurzweil, 1990] Ray Kurzweil. The age of intelligent ma-\nchines. MIT Press, 1990.\n[Llano et al., 2016] Maria Teresa Llano, Simon Colton, Rose\nHepworth, and Jeremy Gow. Automated Fictional Ideation\nvia Knowledge Base Manipulation. Cognitive Computa-\ntion, pages 1–22, 2016.\n[Mazzola, 2016] Guerino Mazzola. Melting Glass Beads—\nThe Multiverse Game of Gestures and Strings.\nGlass\nBead, Site 0: Castalia, the Game of Ends and Means, 2016.\n[McGregor et al., 2016] Stephen\nMcGregor,\nMatthew\nPurver, and Geraint Wiggins. Words, concepts, and the\ngeometry of analogy.\nIn Dimitrios Kartsaklis, Martha\nLewis, and Laura Rimell, editors, Proceedings of Work-\nshop on Semantic Spaces at the Intersection of NLP,\nPhysics and Cognitive Science. Electronic Proceedings in\nTheoretical Computer Science, 2016.\n[Ogden and Richards, 1923] Charles\nKay\nOgden\nand\nIvor Armstrong Richards, editors. The Meaning of Mean-\ning. A Study of the Inﬂuence of Language upon Thought\nand of the Science of Symbolism.\nHarcourt, Brace and\nCompany, Inc., 1923.\n[Pace-Sigge, 2013] Michael Pace-Sigge. The concept of lex-\nical priming in the context of language use. ICAME Jour-\nnal, 37:149–173, 2013.\n[Parker, 2010] Philip M. Parker. An introduction to “Graph\nTheoretic Poetry”, 2010. Published on totopoetry.com.\n[Pine, 2014] Red Pine. The Mountain Poems of Stonehouse.\nCopper Canyon Press, 2014.\n[Quillian, 1969] M. Ross Quillian. The Teachable Language\nComprehender:\nA Simulation Program and Theory of\nLanguage. Commun. ACM, 12(8):459–476, August 1969.\n[Rancière, 1991] Jacques Rancière.\nThe ignorant school-\nmaster: Five lessons in intellectual emancipation. Stan-\nford University Press, 1991.\n[Raw, 2014] Laurence Raw. The Paragogy of Adaptation in\nan EFL context. In Deborah Cartmell and Imelda Whele-\nhan, editors, Teaching Adaptations, pages 26–40. Palgrave\nMacmillan, 2014.\n[Redington and Chater, 1997] M. Redington and N. Chater.\nProbabilistic and distributional approaches to language ac-\nquisition.\nTrends in Cognitive Sciences, 1(7):273–281,\n1997.\n[Rusu et al., 2007] Delia Rusu, Lorand Dali, Blaž Fortuna,\nMarko Grobelnik, and Dunja Mladeni´c. Triplet extraction\nfrom sentences. In Proc. 10th International Multiconfer-\nence Information Society-IS, pages 8–12, 2007.\n[Rzepka and Araki, 2015] Rafal Rzepka and Kenji Araki.\nHaiku Generator That Reads Blogs and Illustrates Them\nwith Sounds and Images.\nIn Proceedings of the 24th\nInternational Conference on Artiﬁcial Intelligence, pages\n2496–2502. AAAI Press, 2015.\n[San Francisco Writers’ Grotto, 2012] San Francisco Writ-\ners’ Grotto. 642 Things to Write About. Chronicle Books,\n2012.\n[Schlegel et al., 2013] Alexander Schlegel, Peter J. Kohler,\nSergey V. Fogelson,\nPrescott Alexander,\nDedeepya\nKonuthula, and Peter Ulric Tse. Network structure and dy-\nnamics of the mental workspace. Proc. National Academy\nof Sciences, 110(40):16277–16282, 2013.\n[Schwartz, 2015] Oscar Schwartz. Can a computer write po-\netry?, May 2015. Lecture at TEDxYouth@Sydney.\n[Screene and Wiggins, 2015] Francis Screene and Geraint A\nWiggins. Learning large scale musical form to enable cre-\nativity. In Colton et al. [2015a].\n[Selinker, 1972] Larry Selinker.\nInterlanguage.\nInterna-\ntional Review of Applied Linguistics in Language Teaching\n(IRAL), 10(1-4):209–232, 1972.\n[Shelley, 1831] Mary Wollstonecraft Shelley. Frankenstein,\nor, The modern Prometheus. H. Colburn and R. Bentley,\n1831.\n[Turing, 1951] Alan Turing. Intelligent machinery, a hereti-\ncal theory, 1951. Lecture at ‘51 Society’, Manchester.\n[Veale, 2015] Tony Veale. Game of tropes: Exploring the\nplacebo effect in computational creativity. In Proceedings\nof the Sixth International Conference on Computational\nCreativity June, page 78, 2015.\n[Wang, 2014] Zhiqin Wang. Developing accuracy and ﬂu-\nency in spoken English of Chinese EFL learners. English\nlanguage teaching, 7(2):110, 2014.\n[Wolff, 1988] J.G. Wolff.\nLearning syntax and meanings\nthrough optimization and distributional analysis.\nCate-\ngories and processes in language acquisition, 1(1), 1988.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "H.5.2; D.1.2; J.5"
  ],
  "published": "2016-04-29",
  "updated": "2016-06-28"
}