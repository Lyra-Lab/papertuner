{
  "id": "http://arxiv.org/abs/1605.01369v2",
  "title": "Accelerating Deep Learning with Shrinkage and Recall",
  "authors": [
    "Shuai Zheng",
    "Abhinav Vishnu",
    "Chris Ding"
  ],
  "abstract": "Deep Learning is a very powerful machine learning model. Deep Learning trains\na large number of parameters for multiple layers and is very slow when data is\nin large scale and the architecture size is large. Inspired from the shrinking\ntechnique used in accelerating computation of Support Vector Machines (SVM)\nalgorithm and screening technique used in LASSO, we propose a shrinking Deep\nLearning with recall (sDLr) approach to speed up deep learning computation. We\nexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network\n(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data\nsets. Results show that the speedup using shrinking Deep Learning with recall\n(sDLr) can reach more than 2.0 while still giving competitive classification\nperformance.",
  "text": "arXiv:1605.01369v2  [cs.LG]  19 Sep 2016\nAccelerating Deep Learning with Shrinkage and Recall\nShuai Zheng∗, Abhinav Vishnu† and Chris Ding∗\n†Paciﬁc Northwest National Laboratory, Richland, WA, USA\nabhinav.vishnu@pnnl.gov\n∗Department of Computer Science and Engineering,\nUniversity of Texas at Arlington, TX, USA\nzhengs123@gmail.com, chqding@uta.edu\nAbstract—Deep Learning is a very powerful machine learn-\ning model. Deep Learning trains a large number of parameters\nfor multiple layers and is very slow when data is in large scale\nand the architecture size is large. Inspired from the shrinking\ntechnique used in accelerating computation of Support Vector\nMachines (SVM) algorithm and screening technique used in\nLASSO, we propose a shrinking Deep Learning with recall\n(sDLr) approach to speed up deep learning computation. We\nexperiment shrinking Deep Learning with recall (sDLr) using\nDeep Neural Network (DNN), Deep Belief Network (DBN) and\nConvolution Neural Network (CNN) on 4 data sets. Results\nshow that the speedup using shrinking Deep Learning with\nrecall (sDLr) can reach more than 2.0 while still giving\ncompetitive classiﬁcation performance.\nKeywords-Deep Learning; Deep Neural Network (DNN);\nDeep Belief Network (DBN); Convolution Neural Network\n(CNN)\nI. INTRODUCTION\nDeep Learning [1] has become a powerful machine\nlearning model. It differs from traditional machine learning\napproaches in the following aspects: Firstly, Deep Learning\ncontains multiple non-linear hidden layers and can learn\nvery complicated relationships between inputs and outputs.\nDeep architectures using multiple layers outperform shadow\nmodels [2]. Secondly, there is no need to extract human\ndesign features [3], which can reduce the dependence of the\nquality of human extracted features. We mainly study three\nDeep Learning models in this work: Deep Neural Networks\n(DNN), Deep Belief Network (DBN) and Convolution Neu-\nral Network (CNN).\nDeep Neural Network (DNN) is the very basic deep\nlearning model. It contains multiple layers with many hidden\nneurons with non-linear activation function in each layer.\nFigure 1 shows one simple example of Deep Neural Network\n(DNN) model. This Deep neural network has one input layer,\ntwo hidden layers and one output layer. Training process of\nDeep Neural Network (DNN) includes forward propagation\nand back propagation. Forward propagation uses the current\nconnection weight to give a prediction based on current state\nThis work was conducted while the ﬁrst author was doing internship at\nPaciﬁc Northwest National Laboratory, Richland, WA, USA.\nof model. Back propagation computes the amount of weight\nshould be changed based on the difference of ground truth\nlabel and forward propagation prediction. Back propagation\nin Deep Neural Network (DNN) is a non-convex problem.\nDifferent initialization affects classiﬁcation accuracy and\nconvergence speed of models.\nSeveral unsupervised pretraining methods for neural net-\nwork have been proposed to improve the performance of\nrandom initialized DNN, such as using stacks of RBMs\n(Restricted Boltzmann Machines) [1], autoencoders [4],\nor DBM (Deep Boltzmann Machines) [5]. Compared to\nrandom initialization, pretraining followed with ﬁnetuning\nbackpropagation will improve the performance signiﬁcantly.\nDeep Belief Network (DBN) is a generative unsupervised\npretraining network which uses stacked RBMs [6] during\npretraining. A DNN with a corresponding conﬁgured DBN\noften produces much better results. DBN has undirected con-\nnections between its ﬁrst two layers and directed connections\nbetween all its lower layers[7] [5].\nConvolution Neural Network (CNN) [8] [3] [9] has been\nproposed to deal with images, speech and time-series. This is\nbecause standard DNN has some limitations. Firstly, images,\nspeeches are usually large. A simple Neural Network to\nprocess an image size of 100 × 100 with 1 layer of 100\nhidden neurons will require 1,000,000 (100 × 100 × 100)\nweight parameters. With so many variables, it will lead\nto overﬁtting easily. Computation of standard DNN model\nrequires expensive memory too. Secondly, standard DNN\ndoes not consider the local structure and topology of the\ninput. For example, images have strong 2D local structure.\nMany areas in the image are similar. Speeches have a strong\n1D structure, where variables temporally nearby are highly\ncorrelated. CNN forces the extraction of local features by\nrestricting the receptive ﬁelds of hidden neurons to be local\n[9].\nHowever, the training process for deep learning algo-\nrithms, including DNN, DBN, CNN, is computationally ex-\npensive. This is due to the large number of training data and\na large number of parameters for multiple layers. Inspired\nfrom the shrinking technique [10] [11] used in accelerating\ncomputation of Support Vector Machines (SVM) algorithm\nInput layer\nHidden layer\nOutput layer\nHidden layer\nFigure 1: Deep Neural Network (DNN).\nand screening [12] [13] technique used in LASSO, we\npropose an accelerating algorithm shrinking Deep Learning\nwith Recall (sDLr). The main contribution of sDLr is that\nit can reduce the running time signiﬁcantly. Though there is\na trade-off between classiﬁcation improvement and speedup\non training time, for some data sets, sDLr approach can\neven improve classiﬁcation accuracy. It should be noted\nthat the approach sDLr is a general model and a new\nway of thinking, which can be applied to both large data,\nlarge network and small data small network, both sequential\nand parallel implementations. We will study the impact of\nproposed accelerating approaches on DNN, DBN and CNN\nusing 4 data sets from computer vision and high energy\nphysics, biology science.\nII. MOTIVATION\nThe amount of data in our world has been exploding.\nAnalyzing large data sets, so-called big data, will become\na key basis of competition, underpinning new waves of\nproductivity growth, innovation, and consumer interest [14].\nA lot of big data technologies, including cloud computing,\ndimensionality reduction have been proposed [15], [16],\n[17], [18], [19]. Analyzing big data with machine learning\nalgorithms requires special hardware implementations and\nlarge amount of running time.\nSVM [20] solves the following optimization problem:\nmin\nw,ξ,b\n1\n2wT w + C\nn\nX\ni=1\nξi,\n(1)\nsubject to yi(wT Φ(xi) −b) > 1 −ξi,\nξi > 0, i = 1, ..., n,\nwhere xi is a training sample, yi is the corresponding label,\nξi is positive slack variable, Φ(xi) is mapping function,\nw gives the solution and is known as weight vector, C\ncontrols the relative importance of maximizing the margin\nand minimizing the amount of the slack. Since SVM learn-\ning problem has much less support vectors than training\nexamples, shrinking [10] [11] was proposed to eliminate\ntraining samples for large learning tasks where the fraction\nof support vectors is small compared to the training sample\nsize or when many support vectors are at the upper bound\nof Lagrange multipliers.\nLASSO [21] is an optimization problem to ﬁnd sparse\nrepresentation of some signals with respect to a predeﬁned\ndictionary. It solves the following problem:\nmin\nx\n1\n2∥Dx −y∥2\n2 + λ∥x∥1,\n(2)\nwhere y is a testing point, D ∈ℜp×n is a dictionary with\ndimension p and size n, λ is a parameter controls the sparsity\nof representation x. When both p and n are large, which is\nusually the case in practical applications, such as denoising\nor classiﬁcation, it is difﬁcult and time-intensive to compute.\nScreening [12] [13] is a technique used to reduce the size\nof dictionary using some rules in order to accelerate the\ncomputation of LASSO.\nEither in shrinking of SVM or in screening of LASSO,\nthese approaches are trying to reduce the size of computation\ndata. Inspired from these two techniques, we propose a faster\nand reliable approach for deep learning, shrinking Deep\nLearning.\nIII. SHRINKING DEEP LEARNING\nGiven testing point xi ∈ℜp×1, i = 1, 2, ..., n, let class\nindicator vector be y(0)\ni\n∈ℜ1×c, where n is number of\ntesting samples, c is number of classes, yi has all 0s except\none 1 to indicate the class of this test point. Let the output\nof a neural network for testing point xi be yi ∈ℜ1×c. yi\ncontains continuous values and is the ith row of Y.\nA. Standard Deep Learning\nAlgorithm 1 gives the framework of standard deep learn-\ning. During each epoch (iteration), standard deep learning\nﬁrst runs a forward-propagation on all training data, then\ncomputes the output Y(w), where output Y is a function of\nweight parameters w. Deep learning tries to ﬁnd an optimal\nw to minimize error loss e = [e1, ..., en], which can be\nsum squared error loss (DNN, DBN in our experiment) or\nsoftmax loss (CNN in our experiment). In backpropagation\nprocess, deep learning updates weight parameter vector\nusing gradient descent. For an training data xi, gradient\ndescent can be denoted as:\nw(epoch+1) = w(epoch) −η∇ei(w(epoch)),\n(3)\nwhere η is step size.\nBefore we present shrinking Deep Leaning algorithm, we\nﬁrst give Lemma 1.\nLemma 1: Magnitude\nof\ngradient\n∇ei(w(epoch))\nin\nEq.(3) is positive correlated with the error ei, i = 1, ..., n.\nAlgorithm 1 Deep Learning (DL)\nInput: Data matrix X ∈ℜp×n, class matrix Y(0) ∈ℜn×c\nOutput: Classiﬁcation error\n1: Preprocessing training data\n2: Active training data index A = {1, 2, ..., n}\n3: for epoch = 1, 2, ... do\n4:\nRun forward-propagation on A\n5:\nCompute forward-propagation output Y ∈ℜn×c\n6:\nRun back-propagation\n7:\nUpdate weight w using Eq.(3)\n8: end for\n9: Compute classiﬁcation error using Y and Y(0)\nProof: In the case of sum squared error, error loss of\nsample xi is given as:\nei = 1\n2\nc\nX\nk=1\n(yik(w) −y(0)\nik )2, i = 1, ..., n.\n(4)\nUsing Eq.(4), gradient ∇ei(w) is:\n∇ei(w) =\nc\nX\nk=1\n(yik(w) −y(0)\nik )∇yik(w)\n∇ei(w) = ∇yi(w)(yi −y(0)\ni\n)T .\n(5)\nAs we can see from Eq.(5), ∇ei(w) is linear related to\n(yi −y(0)\ni\n). Data points with larger error will have larger\ngradient, thus will have a stronger and larger correction\nsignal when updating w. Data points with smaller error will\nhave smaller gradient, thus will have a weaker and smaller\ncorrection signal when updating w.\nIn the case of softmax loss function, ei is denoted as:\nei = −\nc\nX\nk=1\ny(0)\nik logpik, i = 1, ..., n\n(6)\npik =\nexp(yik(w))\nPc\nj=1 exp(yij(w)).\n(7)\nUsing Eq.(6), gradient ∇ei(w) is:\n∇ei(w) =\nc\nX\nk=1\n∂ei(yik)\n∂yik\n∇yik(w),\n∇ei(w) =\nc\nX\nk=1\n(pik −y(0)\nik )∇yik(w).\n(8)\nNow let’s see the relation between softmax loss function\n(Eq.(6)) and its gradient with respect to weight parameter\nw (Eq.(8)). For example, given point i is in class 1, so\ny(0)\ni1 = 1 and y(0)\nij\n= 0, j ̸= 1. When pi1 is large, pi1 →1,\nsoftmax loss function (Eq.(6)) is very small. For gradient\nof softmax loss function (Eq.(8)), when k = 1, (pi1 −1)\nis close to 0; when k ̸= 1, (pi1 −0) is also close to 0. In\nsummary, when softmax loss function (Eq.(6)) is very small,\nits gradient (Eq.(8)) is also very small.\nAlgorithm 2 Shrinking Deep Learning (sDL)\nInput: Data matrix X ∈ℜp×n, class matrix Y(0) ∈ℜn×c,\nelimination rate s (s is a percentage), stop threshold t\nOutput: Classiﬁcation error\n1: Preprocessing training data\n2: Active training data index A = {1, 2, ..., n}\n3: for epoch = 1, 2, ... do\n4:\nRun forward-propagation on A\n5:\nCompute forward-propagation output Y ∈ℜn×c\n6:\nRun back-propagation\n7:\nUpdate weight w using Eq.(3)\n8:\nif nepoch >= t then\n9:\nCompute error using Eq.(4)\n10:\nCompute set S, which contains indexes of nepochs\nsmallest ei values (nepoch is size of A in current epoch)\n11:\nEliminate all samples in S and update A, A = A −S\n12:\nend if\n13: end for\n14: Compute classiﬁcation error using Y and Y(0)\nAlgorithm 3 Shrinking Deep Learning with Recall (sDLr)\nInput: Data matrix X ∈ℜp×n, class matrix Y(0) ∈ℜn×c,\nelimination rate s (s is a percentage), stop threshold t\nOutput: Classiﬁcation error\n1: Preprocessing training data\n2: Active training data index A = {1, 2, ..., n}, A0 = A\n3: for epoch = 1, 2, ... do\n4:\nRun forward-propagation on A\n5:\nCompute forward-propagation output Y ∈ℜn×c\n6:\nRun back-propagation\n7:\nUpdate weight w using Eq.(3)\n8:\nif nepoch >= t then\n9:\nCompute error using Eq.(4)\n10:\nCompute set S, which contains indexes of nepochs\nsmallest ei values (nepoch is size of A in current epoch)\n11:\nEliminate all samples in S and update A, A = A −S\n12:\nelse\n13:\nUse all data for training, A = A0\n14:\nend if\n15: end for\n16: Compute classiﬁcation error using Y and Y(0)\nB. Shrinking Deep Learning\nIn order to accelerate computation and inspired from\ntechniques of shrinking in SVM and screening of LASSO,\nwe propose shrinking Deep Learning in Algorithm 2 by\neliminating samples with small error (Eq.(4)) from training\ndata and use less data for training.\nAlgorithm 2 gives the outline of shrinking Deep Learning\n(sDL). Compared to standard deep learning in Algorithm\n1, sDL requires two more inputs, elimination rate s and\nstop threshold t. s is a percentage indicating the amount\nof training data to be eliminated during one epoch, t is a\nnumber indication to stop eliminating training data when\nnepoch < t, where nepoch is current number of training\ndata. We maintain an index vector A. In Algorithm 1, both\nforward and backward propagation apply on all training\ndata. In Algorithm 2, the training process is applied on\na subset of all training data. In the ﬁrst epoch, we set\nA = {1, 2, ..., n} to include all training indexes. After\nforward and backward propagation in each epoch, we select\nthe nepochs indexes of training data with smallest error ei,\nwhere nepoch is size of current number of training data A.\nThen we eliminate indexes in S from A, and update A,\nA = A −S. When nepoch < t, we stop eliminating training\ndata anymore. Lemma 1 gives theoretical foundation that\nsamples with small error will smaller impact on the gradient.\nThus eliminating those samples will not impact the gradient\nsigniﬁcantly. Figure 2 shows that the errors using sDL is\nsmaller than errors using DL, which proves that sDL gives\na stronger correction signal and reduce the errors faster.\nWhen eliminating samples, elimination rate s denotes the\npercentage of samples to be removed. We select the nepochs\nindexes of training data with smallest error ei. For the same\nepoch, in different batches, the threshold used to eliminate\nsamples is different. Assume there are nbatch batches one\nepoch, in every batch, we need to drop nepochs/nbatch\nsamples on average. In batch i, let the threshold to drop\nnepochs/nbatch smallest error be ti; in batch i + 1, let the\nthreshold be ti+1. ti and ti+1 will differ a lot. We use\nexponential smoothing [22] to adjust the threshold used in\nbatch i+1: instead of using ti+1 as the threshold to eliminate\nsamples, we use the following t′\ni+1:\nt′\ni+1 = αt′\ni + (1 −α)ti+1,\n(9)\nwhere α ∈[0, 1) is a weight parameter which controls the\nimportance of past threshold values, t′\n1 = t1. The intuition\nusing exponential smoothing is that we want the threshold\nused in each epoch to be consistent. Samples with errors\nless than t′\ni+1 in batch i+1 will be eliminated. If α is close\nto 0, the smoothing effect on threshold is not obvious; if\nα is close to 1, the threshold t′\ni+1 will deviate a lot from\nti+1. In practical, we ﬁnd α between 0.5 and 0.6 is a good\nsetting in terms of smoothing threshold. We will show this\nin experiment part.\nIV. SHRINKING WITH RECALL\nAs the training data in sDL becomes less and less, the\nweight parameter w trained is based on the subset of training\ndata. It is not optimized for the entire training dataset.\nWe now introduce shrinking Deep Learning with recall\n(Algorithm 3) to deal with this situation. In order to utilize\nall the training data, when the number of active training\nsamples nepoch < t, we start to use all training samples, as\nshown in Algorithm 3, A = A0. Algorithm 3 ensures that\nthe model trained is optimized for the entire training data.\nShrinking with recall of Algorithm 3 will produce competi-\ntive classiﬁcation performance with standard Deep Learning\nof Algorithm 1. In experiment, we will also investigate the\nimpact the threshold t on the classiﬁcation results (see Figure\n7).\n0\n500\n1000\n10\n−20\n10\n−10\n10\n0\nSample ID\nSum squared error\n \n \nDL\nsDL\nFigure 2: Sum squared errors (Eq.(4)) using sDNN (shrink-\ning DNN) is smaller than errors using standard DNN in the\nsame epoch on 1000 samples from MNIST data.\n(a) MNIST (10 classes, size 28 × 28, randomly select 50\nimages).\n(b) CIFAR-10 (10 classes in total, size 32× 32, each row is\na class, randomly select 10 images from each class).\nFigure 3: Sample images.\nV. EXPERIMENTS\nIn experiment, we test our algorithms on data sets of\ndifferent domains using 5 different random initialization.\nThe data sets we used are listed in Table I. MNIST is\na standard toy data set of handwritten digits; CIFAR-10\nTable II: MNIST classiﬁcation error improvement (IMP) and training time Speedup.\nMethod\nDNN\nsDNNr\nIMP/Speedup\nDBN\nsDBNr\nIMP/Speedup\nCNN\nsCNNr\nIMP/Speedup\nTesting error\n0.0387\n0.0324\n16.3%\n0.0192\n0.0182\n5.21%\n0.0072\n0.0073\n−1.39%\nTraining time (s)\n1653\n805\n2.05\n1627\n700\n2.32\n3042\n1431\n2.13\n0\n50\n100\n0.04\n0.045\n0.05\n0.055\n0.06\nIteration\nClassification testing error\n \n \n784−100−10\n784−100−100−10\n784−200−10\n784−1000−10\n(a) Testing error.\n0\n50\n100\n0\n0.005\n0.01\n0.015\n0.02\nIteration\nClassification training error\n \n \n784−100−10\n784−100−100−10\n784−200−10\n784−1000−10\n(b) Training error.\nFigure 4: MNIST DNN testing and training error on different\nnetwork (100 iterations/epochs).\nTable I: Overview of data sets used in this paper.\nDataset\nDimensionality\nTraining Set\nTesting Set\nMNIST\n784 (28 × 28 grayscale)\n60K\n10K\nCIFAR-10\n3072 (32 × 32 color)\n50K\n10K\nHiggs Boson\n7\n50K\n20K\nAlternative Splicing\n3446\n2500\n945\ncontains tiny natural images; Higgs Boson is a dataset\nfrom high energy physics. Alternative Splicing is RNA\nfeatures used for predicting alternative gene splicing. We\nuse DNN and DBN implementation from [23] and CNN\nimplementation from [24]. All experiments were conducted\non a laptop with Intel Core i5-3210M CPU 2.50GHz, 4GB\nRAM, Windows 7 64-bit OS.\nA. Results on MNIST\nMNIST is a standard toy data set of handwritten digits\ncontaining 10 classes. It contains 60K training samples and\n10K testing samples. The image size is 784 (grayscale 28 ×\n28). Figure 3a shows some examples of MNIST dataset.\n1) Deep Neural Network: In experiment, we ﬁrst test on\nsome network architecture and ﬁnd a better one for our\nfurther investigations. Figure 4a and Figure 4b show the\n0\n50\n100\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nIteration\nClassification testing error\n \n \nDNN\nsDNN\nsDNNr\n(a) Testing error.\n0\n50\n100\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nIteration\nClassification training error\n \n \nDNN\nsDNN\nsDNNr\n(b) Training error.\nFigure 5: MNIST testing and training error (100 itera-\ntions/epochs).\ntesting and training classiﬁcation error for different network\nsettings. Results show that “784-1000-10” is a better setting\nwith lower testing error and converges faster in training.\nWe will use network “784-1000-10” for DNN and DBN on\nMNIST. Learning rate is set to be 1; activation function is\ntangent function and output unit is sigmoid function.\nFigure 5 shows the testing error and training error of\nusing standard DNN, sDNN (Shrinking DNN) and sDNNr\n(shrinking DNN with recall). Results show that sDNNr\nimproves the accuracy of standard DNN. While for training\nerror, both DNN and sDNNr give almost 0 training error.\nFigure 6 shows training time and number of active sam-\nples in each iteration (epoch). In our experiments, for sDNN\nand sDNNr, we set eliminate rate s = 20%. sDNNr has\na recall process to use the the entire training samples, as\nshown in Figure 6. When the number of active samples is\nless than t = 20% × 60K of total training samples, we stop\neliminating samples. The speedup using sDNNr compared\nto DNN is\nSpeedup = tDNN\ntsDNNr\n= 2.05.\n(10)\n0\n20\n40\n60\n80\n100\n0\n5\n10\n15\n20\n25\nIteration\nRunning time (seconds)\n \n \nDNN\nsDNN\nsDNNr\n(a) Training time.\n0\n20\n40\n60\n80\n100\n0\n2\n4\n6\n8\nx 10\n4\nIteration\nNumber of active samples\n \n \nDNN\nsDNN\nsDNNr\n(b) Number of active samples.\nFigure 6: MNIST training time and number of active samples\n(100 iterations/epochs).\n0\n20\n40\n60\n80\n100\n0.035\n0.04\n0.045\n0.05\n0.055\nIteration\nClassification testing error\n \n \nDNN\nsDNNr t=0.1\nsDNNr t=0.2\nsDNNr t=0.3\nsDNNr t=0.4\nsDNNr t=0.5\nFigure 7: MNIST classiﬁcation testing error using different\nrecall threshold t (100 iterations/epochs).\nRecall is a technique when the number of training samples\nis decreased to a threshold t, we start to use all training\nsamples. There is a trade-off between speedup and classi-\nﬁcation error: setting a lower t could reduce computation\ntime more, but could increase classiﬁcation error. Figure 7\nshows the effect of using different recall threshold t sDNNr\non MNIST data. When we bring all training samples back\nat t = 20% × 60K, we get the best testing error. It is worth\nnoting that the classiﬁcation error of sDNNr is improved\ncompared to standard DNN, which could imply that there is\nless overﬁtting for this data set.\nFigure 8 shows an example of exponential smoothing on\nthe elimination threshold (Eq.(9)) during one epoch. The\nthreshold using α = 0.5 smooths the curve a lot.\n10\n20\n30\n40\n50\n60\n1\n2\n3\n4\n5\n6 x 10\n−7\nBatch\nThreshold\n \n \nα = 0\nα = 0.5\nFigure 8: Exponential smoothing (see Eq.(9)) effect on one\nepoch of MNIST (60 batches with 1000 samples/batch ).\n0\n50\n100\n0.02\n0.04\n0.06\n0.08\nIteration\nClassification testing error\n \n \nDNN\nDBN\nsDBNr\n(a) Testing error\n0\n20\n40\n60\n80\n100\n0\n5\n10\n15\n20\n25\nIteration\nRunning time (seconds)\n \n \nDBN\nsDBNr\n(b) Training Time\nFigure 9: MNIST DBN result(100 iterations/epochs): (a)\ncompares classiﬁcation testing error; (b) compares training\ntime.\n2) Deep Belief Network: Figure 9 shows the classiﬁcation\ntesting error and training time of using Deep Belief Network\n(DBN) and shrinking DBN with recall (sDBNr) on MNIST.\nNetwork setting is same as it is in DNN experiment. sDBNr\nfurther reduces the classiﬁcation error of DBN to 0.0182 by\nusing sDBNr.\n3) Convolution Neural Networks (CNN): The network\narchitecture used in MNIST is 4 convolutional layers with\neach of the ﬁrst 2 convolutional layers followed by a max-\npooling layer, then 1 layer followed by a ReLU layer, 1\nlayer followed by a Softmax layer. The ﬁrst 2 convolutional\nlayers have 5 × 5 receptive ﬁeld applied with a stride of 1\npixel. The 3rd convolutional layer has 4 × 4 receptive ﬁeld\n0\n10\n20\n30\n40\n50\n0.005\n0.01\n0.015\n0.02\n0.025\nIteration\nClassification testing error\n \n \nCNN\nsCNN\nsCNNr\n(a) Testing error\n0\n10\n20\n30\n40\n50\n0\n20\n40\n60\n80\n100\nIteration\nRunning time (seconds)\n \n \nCNN\nsCNN\nsCNNr\n(b) Training Time\nFigure 10: MNIST CNN result(50 iterations/epochs): (a)\ncompares classiﬁcation testing error; (b) compares training\ntime.\nTable III: CIFAR-10 classiﬁcation error improvement (IMP)\nand training time Speedup.\nMethod\nCNN\nsCNNr\nIMP/Speedup\nTesting error (top 1)\n0.2070\n0.2066\n0.19%\nTraining time (s)\n5571\n3565\n1.56\nand the 4th layer has 1 × 1 receptive ﬁeld with a stride of 1\npixel. The max pooling layers pool 2 × 2 regions at strides\nof 2 pixels. Figure 10 shows the classiﬁcation testing error\nand training time of CNN on MNIST data.\nTable II summarizes the classiﬁcation error improvement\n(IMP) and training time speedup of DNN, DBN and CNN\non MNIST data, where improvement is IMP = (errDL −\nerrsDLr)/errDL.\nB. Results on CIFAR-10\nCIFAR-10 [25] data contains 60,000 32 × 32 color image\nin 10 classes, with 6,000 images per class. There are\n50,000 training and 10,000 testing images. CIFAR-10 is an\nobject dataset, which includes airplane, car, bird, cat and\nso on and classes are completely mutually exclusive. In our\nexperiment, we use CNN network to evaluate the perfor-\nmance in terms of classiﬁcation error. Network architecture\nuses 5 convolutional layers: for the ﬁrst three layers, each\nconvolutional layer is followed by a max pooling layer;\n4th convolutional layer is followed by a ReLU layer; the\nTable IV: Higgs Boson classiﬁcation error improvement\n(IMP) and training time Speedup.\nDNN Network\nMethod\nDNN\nsDNNr\nIMP/Speedup\n7-20-20-2\nTesting error\n0.4759\n0.4512\n5.19%\nTraining time (s)\n21\n13\n1.62\n7-50-2\nTesting error\n0.3485\n0.3386\n2.84%\nTraining time (s)\n52\n18\n2.89\nTable V: Alternative Splicing error improvement (IMP) and\ntraining time Speedup.\nDNN Network\nMethod\nDNN\nsDNNr\nIMP/Speedup\n1389 −100 −3\nTesting error\n0.2681\n0.2960\n10.4%\nTraining time (s)\n32\n20\n1.60\n5th layer is followed by a softmax loss output layer. Table\nIII shows the classiﬁcation error and training time. Top-1\nclassiﬁcation testing error in Table III means that the predict\nlabel is determined by considering the class with maximum\nprobability only.\nC. Results on Higgs Boson\nHiggs Boson is a subset of data from [26] with 50, 000\ntraining and 20, 000 testing. Each sample is a signal process\nwhich either produces Higgs bosons particle or not. We use 7\nhigh-level features derived by physicists to help discriminate\nparticles between the two classes. Both activation function\nand output function were sigmoid function. The DNN batch-\nsize is 100 and recall threshold t = 20% × 50, 000. We test\non different network settings and choose the best. Table IV\nshows the experiment results using different network.\nD. Results on Alternative Splicing\nAlternative Splicing [27] is a set of RNA sequences used\nin bioinfomatics. It contains 3446 cassette-type mouse exons\nwith 1389 features per exon. We randomly select 2500 exons\nfor training and use the rest for testing. For each exon, the\ndataset contains three real-valued positive prediction targets\nyi = [qinc\nqexc\nqnc], corresponding to probabilities that\nthe exon is more likely to be included in the given tissue,\nmore likely to be excluded, or more likely to exhibit no\nchange relative to other tissues. To demonstrate the effective\nof proposed shrinking Deep Learning with recall approach,\nwe use a simple DNN network of different number of layers\nand neurons with optimal tangent activation function and\nsigmoid output function. We use the following average sum\nsquared error criteria to evaluate the model performance\nerror = Pn\ni=1 ∥yi −y(0)\ni\n∥2/n, where yi is the predict\nvector label and y(0)\ni\nis the ground-truth label vector, n is\nnumber of samples. The DNN batchsize is 100 and recall\nthreshold t = 20% × 2500. We test on different network\nsettings and choose the best. Table V shows the experiment\nresult.\nVI. CONCLUSION\nIn conclusion, we proposed a shrinking Deep Learning\nwith recall (sDLr) approach and the main contribution of\nsDLr is that it can reduce the running time signiﬁcantly. Ex-\ntensive experiments on 4 datasets show that shrinking Deep\nLearning with recall can reduce training time signiﬁcantly\nwhile still gives competitive classiﬁcation performance.\nREFERENCES\n[1] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimen-\nsionality of data with neural networks,” Science, vol. 313, no.\n5786, pp. 504–507, 2006.\n[2] Y. Bengio, “Learning deep architectures for ai,” Foundations\nand trends R⃝in Machine Learning, vol. 2, no. 1, pp. 1–127,\n2009.\n[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet\nclassiﬁcation with deep convolutional neural networks,” in\nAdvances in neural information processing systems, 2012, pp.\n1097–1105.\n[4] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.\nManzagol, “Stacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local denoising cri-\nterion,” The Journal of Machine Learning Research, vol. 11,\npp. 3371–3408, 2010.\n[5] R. Salakhutdinov and G. E. Hinton, “Deep boltzmann ma-\nchines,” in International Conference on Artiﬁcial Intelligence\nand Statistics, 2009, pp. 448–455.\n[6] G. E. Hinton, “Training products of experts by minimizing\ncontrastive divergence,” Neural computation, vol. 14, no. 8,\npp. 1771–1800, 2002.\n[7] L. Deng, “Three classes of deep learning architectures and\ntheir applications: a tutorial survey,” APSIPA transactions on\nsignal and information processing, 2012.\n[8] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-\nbased learning applied to document recognition,” Proceedings\nof the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n[9] Y. LeCun and Y. Bengio, “Convolutional networks for images,\nspeech, and time series,” The handbook of brain theory and\nneural networks, vol. 3361, no. 10, 1995.\n[10] T. Joachims, “Making large scale svm learning practical,”\nUniversit¨at Dortmund, Tech. Rep., 1999.\n[11] J. Narasimhan, A. Vishnu, L. Holder, and A. Hoisie, “Fast\nsupport vector machines using parallel adaptive shrinking on\ndistributed systems,” arXiv preprint arXiv:1406.5161, 2014.\n[12] J. Wang, J. Zhou, P. Wonka, and J. Ye, “Lasso screening\nrules via dual polytope projection,” in Advances in Neural\nInformation Processing Systems, 2013, pp. 1070–1078.\n[13] A. Bonnefoy, V. Emiya, L. Ralaivola, and R. Gribonval,\n“A dynamic screening principle for the lasso,” in Signal\nProcessing Conference (EUSIPCO), 2014 Proceedings of the\n22nd European.\nIEEE, 2014, pp. 6–10.\n[14] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs,\nC. Roxburgh, and A. H. Byers, “Big data: The next frontier\nfor innovation, competition, and productivity,” 2011.\n[15] S. Zheng, X. Cai, C. H. Ding, F. Nie, and H. Huang, “A\nclosed form solution to multi-view low-rank regression.” in\nAAAI, 2015, pp. 1973–1979.\n[16] S. Zheng and C. Ding, “Kernel alignment inspired linear\ndiscriminant analysis,” in Joint European Conference on\nMachine Learning and Knowledge Discovery in Databases.\nSpringer Berlin Heidelberg, 2014, pp. 401–416.\n[17] D. Williams, S. Zheng, X. Zhang, and H. Jamjoom, “Tide-\nwatch: Fingerprinting the cyclicality of big data workloads,”\nin IEEE INFOCOM 2014-IEEE Conference on Computer\nCommunications.\nIEEE, 2014, pp. 2031–2039.\n[18] X. Zhang, Z.-Y. Shae, S. Zheng, and H. Jamjoom, “Virtual\nmachine migration in an over-committed cloud,” in 2012\nIEEE Network Operations and Management Symposium.\nIEEE, 2012, pp. 196–203.\n[19] S. Zheng, Z.-Y. Shae, X. Zhang, H. Jamjoom, and L. Fong,\n“Analysis and modeling of social inﬂuence in high perfor-\nmance computing workloads,” in European Conference on\nParallel Processing.\nSpringer Berlin Heidelberg, 2011, pp.\n193–204.\n[20] J. A. Suykens and J. Vandewalle, “Least squares support\nvector machine classiﬁers,” Neural processing letters, vol. 9,\nno. 3, pp. 293–300, 1999.\n[21] R. Tibshirani, “Regression shrinkage and selection via the\nlasso,” Journal of the Royal Statistical Society. Series B\n(Methodological), pp. 267–288, 1996.\n[22] E. S. Gardner, “Exponential smoothing: The state of the art,”\nJournal of forecasting, vol. 4, no. 1, pp. 1–28, 1985.\n[23] R. B. Palm, “Prediction as a candidate for learning deep hi-\nerarchical models of data,” Technical University of Denmark,\n2012.\n[24] A. Vedaldi and K. Lenc, “Matconvnet-convolutional neural\nnetworks for matlab,” arXiv preprint arXiv:1412.4564, 2014.\n[25] A. Krizhevsky and G. Hinton, “Learning multiple layers of\nfeatures from tiny images,” 2009.\n[26] P. Baldi, P. Sadowski, and D. Whiteson, “Searching for exotic\nparticles in high-energy physics with deep learning,” Nature\ncommunications, vol. 5, 2014.\n[27] H. Y. Xiong, Y. Barash, and B. J. Frey, “Bayesian prediction\nof tissue-regulated splicing using rna sequence and cellular\ncontext,” Bioinformatics, vol. 27, no. 18, pp. 2554–2562,\n2011.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "cs.NE"
  ],
  "published": "2016-05-04",
  "updated": "2016-09-19"
}