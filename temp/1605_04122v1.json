{
  "id": "http://arxiv.org/abs/1605.04122v1",
  "title": "Natural Language Semantics and Computability",
  "authors": [
    "Richard Moot",
    "Christian Retoré"
  ],
  "abstract": "This paper is a reflexion on the computability of natural language semantics.\nIt does not contain a new model or new results in the formal semantics of\nnatural language: it is rather a computational analysis of the logical models\nand algorithms currently used in natural language semantics, defined as the\nmapping of a statement to logical formulas - formulas, because a statement can\nbe ambiguous. We argue that as long as possible world semantics is left out,\none can compute the semantic representation(s) of a given statement, including\naspects of lexical meaning. We also discuss the algorithmic complexity of this\nprocess.",
  "text": "Natural Language Semantics and Computability\nRichard Moot1 and Christian Retor´e2\n1 CNRS LaBRI\n2 Universit´e de Montpellier & LIRMM\nAbstract. This paper is a reﬂexion on the computability of natural language se-\nmantics. It does not contain a new model or new results in the formal semantics\nof natural language: it is rather a computational analysis of the logical models\nand algorithms currently used in natural language semantics, deﬁned as the map-\nping of a statement to logical formulas — formulas, because a statement can be\nambiguous. We argue that as long as possible world semantics is left out, one can\ncompute the semantic representation(s) of a given statement, including aspects of\nlexical meaning. We also discuss the algorithmic complexity of this process.\nIntroduction\nIn the well-known Turing test for artiﬁcial intelligence, a human interrogator needs\nto decide, via a question answering session with two terminals, which of his two inter-\nlocutors is a man and which is a machine (Turing 1950). Although early systems like\nEliza based on matching word patterns may seem clever at ﬁrst sight, they clearly do\nnot pass the test. One often forgets that, in addition to reasoning and access to knowl-\nedge representation, passing the Turing test presupposes automated natural language\nanalysis and generation which, despite signiﬁcant progress in the ﬁeld, has not yet been\nfully achieved. These natural language processing components of the Turing test are of\nindependent interest and used in computer programs for question answering and trans-\nlation (however, since both of these tasks are generally assumed to be AI-complete it is\nunlikely that a full solution for these problems would be simpler than a solution for the\nTuring test itself).\nIf we deﬁne the semantics of a (sequence of) sentence(s) σ as the mapping to a\nrepresentation φ(σ) that can be used by a machine for natural language processing\ntasks, two very different ideas of semantics come to mind.\n1. One notion of semantics describes what the sentence(s) speaks about. The domi-\nnant model for this type of semantics represents meaning using word vectors (only\ninvolving referential/full words nouns, adjectives, verbs, adverbs, ...and not gram-\nmatical words) which represent what σ speaks about. This is clearly computable.\nOne must ﬁx a thesaurus of n words that acts as a vector basis. Usually words not in\nthe thesaurus or basis are expanded into their deﬁnition with words in the thesaurus.\nBy counting occurrences of words from the thesaurus in the text (substituting words\nnot in the thesaurus with their deﬁnition) and turning this into a n-dimensional vec-\ntor reduced to be of euclidian norm 1, we obtain word meanings in the form of\narXiv:1605.04122v1  [cs.CL]  13 May 2016\nn-dimensional vectors. This notion of semantics provides a useful measure of se-\nmantic similarity between words and texts; typical applications include exploring\nBig Data and ﬁnding relevant pages on the internet. This kind of semantics models\nwhat a word (or a text) speaks about.\n2. The other notion of semantics, the one this paper is about, is of a logical nature. It\nmodels what is asserted, refuted, ...assumed by the sentences. According to this\nview, computational semantics is the mapping of sentence(s) to logical formula(s).\nThis is usually done compositionally, according to Frege’s principle “the meaning\nof a compound expression is a function of the meaning of its components” to which\nMontague added “and of its syntactic structure”. This paper focuses on this logical\nand compositional notion of semantics and its extension (by us and others) to lexical\nsemantics; these extensions allow us to conclude from a sentence like “I started a\nbook” that the speaker started reading (or, depending on the context, writing) a\nbook.\nWe should comment that, in our view, semantics is a (computable) function from\nsentence(s) to logical formulae, since this viewpoint is not so common in linguistics.\n– Cognitive sciences also consider the language faculty as a computational device\nand insist on the computations involved in language analysis and production. Ac-\ntually there are two different views of this cognitive and computational view: one\nview, promoted by authors such as Pinker (1994), claims that there is a speciﬁc\ncognitive function for language, a “language module” in the mind, while others,\nlike Langacker (2008), think that our language faculty is just our general cognitive\nabilities applied to language.\n– In linguistics and above all in philosophy of language many people think that sen-\ntences cannot have any meaning without a context, such a context involving both\nlinguistic and extra-linguistic information. Thus, according to this view, the input\nof our algorithm should include context. Our answer is ﬁrstly that linguistic context\nis partly taken into account since we are able to produce, in addition to formulae,\ndiscourse structures. Regarding the part of context that we cannot take into account,\nbe it linguistic or not, our answer is that it is not part of semantics, but rather an\naspect of pragmatics. And, as argued by Corblin (2013), if someone is given a few\nsentences on a sheet of paper without any further information, he starts imagining\nsituations, may infer other statements from what he reads, ..., and such thoughts\nare the semantics of the sentence.\n– The linguistic tradition initiated by Montague (1974) lacks some coherence regard-\ning computability. On the one hand, Montague gives an algorithm for parsing sen-\ntences and for computing their meaning as a logical formula. On the other hand, he\nasserts that the meaning of a sentence is the interpretation of the formula in possible\nworlds, but these models are clearly uncomputable! Furthermore, according to him,\neach intermediate step, including the intensional/modal formulae should be forgot-\nten, and the semantics is deﬁned as the set of possible worlds in which the semantic\nformula is true: this cannot even be ﬁnitely described, except by these intermediate\nformulas; a fortiori it cannot be computed. Our view is different, for at least three\nreasons, from the weakest to the strongest:\n• Models for higher order logic, as in Montague, are not as simple as is some-\ntimes assumed, and they do not quite match the formulas: completeness fails.\nThis means that a model and even all models at once contains less information\nthan the formula itself.\n• We do not want to be committed to any particular interpretation. Indeed, there\nare alternative relevant interpretations of formulas, as the following non ex-\nhaustive list shows: dialogical interpretations (that are the sets of proofs and/or\nrefutations), game theoretic semantics and ludics (related to the former style\nof interpretation), set of consequences of the formula, structures inhabited by\ntheir normal proofs as in intuitionistic logic,...\n• Interpreting the formula(s) is no longer related to linguistics, although some\ninterpretations might useful for some applications. Indeed, once you have a a\nformula, interpreting it in your favourite way is a purely logical question. De-\nciding whether it is true or not in a model, computing all its proofs or all its\nrefutations, deﬁning game strategies, computing its consequences or the cor-\nresponding structure has nothing to do with the particular natural language\nstatement you started with.\n1\nComputational semantics `a la Montague\nWe shall ﬁrst present the general algorithm that maps sentences to logical formulae,\nreturning to lexical semantics in Section 2. The ﬁrst step is to compute a syntactic\nanalysis that is rich and detailed enough to enable the computation of the semantics\n(in the form of logical formulae). The second step is to incorporate the lexical lambda\nterms and to reduce the obtained lambda term — this step possibly includes the choice\nof some lambda terms from the lexicon that ﬁx the type mismatches.\n1.1\nCategorial syntax\nIn order to express the process that maps a sentence to its semantic interpretation(s)\nin the form of logical formulae, we shall start with a categorial grammar. This is not\nstrictly necessary: Montague (1974) used a context free grammar (augmented with a\nmechanism for quantiﬁer scope), but if one reads between the lines, at some points he\nconverts the phrase structure into a categorial derivation, so we shall, following Moot &\nRetor´e (2012), directly use a categorial analysis. Although richer variants of categorial\ngrammars are possible, and used in practice, we give here an example with Lambek\ngrammars, and brieﬂy comment on variants later.\nCategories are freely generated from a set of base categories, for example np (noun\nphrase), n (common noun), S (sentence), by two binary operators: \\ and /: A\\B and\nB/A are categories whenever A and B are categories. A category A\\B intuitively looks\nfor a category A to its left in order to form a B. Similarly, a category B/A combines\nwith an A to its right to form a B. The full natural deduction rules are shown in Figure 1.\nA lexicon provides, for each word w of the language, a ﬁnite set of categories lex(w).\nWe say a sequence of words w1,...,wn is of typeC whenever ∀i∃ci ∈lex(wi) c1,...,cn ⊢\nC. Figure 2 shows an example lexicon (top) and a derivation of a sentence (bottom).\nΓ ⊢A\n∆⊢A\\B\n\\e\nΓ ,∆⊢B\nA,Γ ⊢B\n\\i\nΓ ⊢A\\B\n∆⊢B/A\nΓ ⊢A\n\\e\nΓ ,∆⊢B\nΓ ,A ⊢B\n\\i\nΓ ⊢B/A\nFig. 1. Natural deduction proof rules for the Lambek calculus\nWord Syntactic Type\nkid n\ncartoon n\nwatched (np\\S)/np\nevery (S/(np\\S))/n\na ((S/np)\\S)/n\nevery\n(S/(np\\S))/n\nkid\nn\n/e\n(S/(np\\S))\nwatched\n(np\\S)/np\n[np]1\n/e\n(np\\S)\n\\e\nS\n/i(1)\nS/np\na\n((S/np)\\S)/n\ncartoon\nn\n/e\n(S/np)\\S\n\\e\nS\nFig. 2. Lexicon and example derivation\n1.2\nFrom syntactic derivation to typed linear lambda terms\nCategorial derivations, being a proper subset of derivations in multiplicative intu-\nitionistic linear logic, correspond to (simply typed) linear lambda terms. This makes the\nconnection to Montague grammar particularly transparent.\nDenoting by e the set of entities (or individuals) and by t the type for propositions\n(these can be either true or false, hence the name t) one has the following mapping from\nsyntactic categories to semantic/logical types.\n(Syntactic type)∗= Semantic type\nS∗= t\na sentence is a proposition\nnp∗= e\na noun phrase is an entity\nn∗= e →t\na noun is a subset of the set of entities (maps entities\nto propositions)\n(A\\B)∗= (B/A)∗= A∗→B∗extends easily to all syntactic categories\nUsing this translation of categories into types which forgets the non commutativity,\nthe Lambek calculus proof of Figure 2 is translated to the linear intuitionistic proof\nshown in Figure 3; we have kept the order of the premisses unchanged to highlight the\nsimilarity with the previous proof. Such a proof can be viewed as a simply typed lambda\nterm with the two base types e and t.\n(a(e→t)→((e→t)→t) cartoone→t)(λye(every(e→t)→((e→t)→t) kide→t)(watchede→e→t y))\nevery\n(e →t) →(e →t) →t\nkid\n(e →t)\n→e\n(e →t) →t\nwatched\ne →e →t\ny\n[e]1\n→e\ne →t\n→e\nt\n→i(1)\ne →t\na\n(e →t) →(e →t) →t\ncartoon\n(e →t)\n→e\n(e →t) →t\n→e\nt\nFig. 3. The multiplicative linear logic proof corresponding to Figure 2\nAs observed by Church (1940), the simply typed lambda calculus with two types e\nand t is enough to express higher order logic, provided one introduces constants for the\nlogical connectives and quantiﬁers, that is a constants “∃” and “∀” of type (e →t) →t,\nand constants “∧”, “∨” et “⇒” of type t →(t →t).\nIn addition to the syntactic lexicon, there is a semantic lexicon that maps any word\nto a simply typed lambda term with atomic types e and t and whose type is the transla-\ntion of its syntactic formula. Figure 4 presents such a lexicon for our current example.\nFor example, the word “every” is assigned formula (S/(np\\S))/n. According to the\ntranslation function above, we know the corresponding semantic term must be of type\n(e →t) →((e →t) →t), as it is in Figure 3. The term we assign in in the seman-\ntic lexicon is the following (both the type and the term are standard in a Montagovian\nsetting).\nλPe→t λQe→t (∀(e→t)→t (λxe(⇒t→(t→t) (P x)(Q x))))\nUnlike the lambda terms computed for proof, the lexical entries in the semantic lexicon\nneed not be linear: the lexical entry above is not a linear lambda term since the single\nabstraction binds two occurrences of x.\nSimilarly, the syntactic type of “a”, the formula ((S/np)\\S)/n has corresponding\nsemantic type (e →t) →((e →t) →t) (though syntactically different, a subject and\nan object generalized quantiﬁer have the same semantic type), and the following lexical\nmeaning recipe.\nλPe→t λQe→t (∃(e→t)→t (λxe(∧t→(t→t)(P x)(Q x))))\nFinally, “kid”, “cartoon” and “watched” are assigned the constants kide→t,\ncartoone→t and watchede→(e→t) respectively.\nword\nsyntactic type u\nsemantic type u∗\nsemantics: λ-term of type u∗\nevery\n(S/(np\\S))/n (subject)\n(e →t) →((e →t) →t)\nλPe→t λQe→t (∀(e→t)→t (λxe(⇒t→(t→t) (P x)(Q x))))\na\n((S/np)\\S)/n (object)\n(e →t) →((e →t) →t)\nλPe→t λQe→t (∃(e→t)→t (λxe(∧t→(t→t)(P x)(Q x))))\nkid\nn\ne →t\nλxe(kide→t x)\ncartoon n\ne →t\nλxe(cartoone→t x)\nwatched (np\\S)/np\ne →(e →t)\nλye λxe ((watchede→(e→t) x) y)\nFig. 4. Semantic lexicon for our example grammar\nBecause the types of these lambda terms are the same as those of the words in\nthe initial lambda term, we can take the linear lambda term associated with the sen-\ntence and substitute, for each word its corresponding lexical meaning, transforming the\nderivational semantics, in our case the following3\n(a(e→t)→((e→t)→t) cartoone→t)(λye(every(e→t)→((e→t)→t) kide→t)(watchede→e→t y))\ninto an (unreduced) representation of the meaning of the sentence.\n((λPe→t λQe→t(∃(e→t)→t (λxe(∧t→(t→t)(P x)(Q x)))))cartoone→t)\n((λye(((λPe→t λQe→t (∀(e→t)→t (λxe(⇒t→(t→t) (P x)(Q x)))))kide→t x)))(watchede→e→ty))\n3 There are exactly two (non-equivalent) proofs of this sentence. The second proof using the\nsame premisses corresponds to the second, more prominent reading of the sentence whose\nlambda term is: (every kid)(λxe.(a cartoon)(λye((watched y) x))\nThe above term reduces to\n(∃(e→t)→t λxe(∧t→(t→t)(cartoon x)(∀(e→t)→t (λze(⇒t→(t→t) (kid z)((watched x)z))))))\nthat is4:\n∃x.cartoon(x)∧∀z.kid(z) ⇒watched(z,x)\nThe full algorithm to compute the semantics of a sentence as a logical formula is\nshown in Figure 5.\nLambek calculus proof\n↓∗\n(multiplicative) intuitionistic linear logic proof\n≡Curry-Howard\n(linear) lambda term\n↓lex\nSubstitute the lexical (simply typed,\nbut not necessarily linear!) lambda terms.\n↓β\nTarget language:\nHigher-Order Logic (HOL, as Montague)\nFig. 5. The standard categorial grammar method for computing meaning\n2\nAdding sorts, coercions, and uniform operations\nMontague (as Frege) only used a single type for entities: e. But it is much better to\nhave many sorts in order to block the interpretation of some sentences:\n(1)\n* The table barked.\n(2)\nThe dog barked.\n(3)\n?The sergeant barked.\nAs dictionaries say “barked” can be said from animals, usually dogs. The ﬁrst one\nis correctly rejected: one gets barkdog→t(the table)artifact and dog ̸= artifact.\nHowever we need to enable the last example barkdog→t(the sergeant)human and\nin this case we use coercions (Bassac et al. 2010, Retor´e 2014): the lexical entry for the\nverb “barked” which only applies to the sort of “dogs” provides a coercion c : human →\ndog from “human” to “dog”. The revised lexicon provides each word with the lambda\nterm that we saw earlier (typed using some of the several sorts / base type) and some\noptional lambda terms that can be used if needed to solve type mismatches.\nSuch coercions are needed to understand sentences like:\n4 We use the standard convention to translate a term ((py)x) into a predicate p(x,y).\n(4)\nThis book is heavy.\n(5)\nThis book is interesting.\n(6)\nThis book is heavy and interesting.\n(7)\nWashington borders the Potomac.\n(8)\nWashington attacked Iraq.\n(9)\n* Washington borders the Potomac and attacked Iraq.\nLambek calculus proof\n↓∗\n(multiplicative) intuitionistic linear logic proof\n≡Curry-Howard\n(linear) lambda term\n↓lex\nSubstitute the lexical ΛTyn terms.\n↓coercions\nSolve type mismatches by the coercions provided by the lexicon.\n↓β\nTarget language:\nHigher-Order Logic (HOL, as Montague)\nFig. 6. Computing meaning in a framework with coercion\nThe ﬁrst two sentences will respectively use a coercions from book to physical ob-\nject and a coercion from books to information. Any time an object has several related\nmeanings, one can consider the conjunction of properties referring to those particular\naspects. For these operations (and others acting uniformly on types) we exploit poly-\nmorphically typed lambda terms (system F). When the related meanings of a word are\nincompatible (this is usually the case) the corresponding coercions are declared to be in-\ncompatible in the lexicon (one is declared as rigid). This extended process is described\nin Figure 6. Some remarks on our use of system F:\n– We use it for the syntax of semantics (a.k.a. metalogic, glue logic)\n– The formulae of semantics are the usual ones (many sorted as in Tyn)\n– We have a single constant for operations that act uniformly on types, like quantiﬁers\nor conjunction over predicates that apply to different facets of a given word.\n3\nComplexity of the syntax\nAs we remarked before, when computing the formal semantics of a sentence in\nthe Montague tradition, we (at least implicitly) construct a categorial grammar proof.\nTherefore, we need to study the complexity of parsing/theorem proving in categorial\ngrammar ﬁrst. The complexity generally studied in this context is the complexity of de-\nciding about the existence of a proof (a parse) for a logical statement (a natural language\nsentence) as a function of the number of words in this sentence5.\nPerhaps surprisingly, the simple product-free version of the Lambek calculus we\nhave used for our examples is already NP-complete (Savateev 2009). However, there is\na notion of order, which measures the level of “nesting” of the implications as deﬁned\nbelow.\norder(p) = 0\norder(A/B) = order(B\\A) = max(order(A),(order(B)+1))\nAs an example, the order of formula (np\\S)/np is 1, whereas the order of for-\nmula S/(np\\S) is 2. For the Lambek calculus, the maximum order of the formulas in a\ngrammar is a good indication of its complexity. Grammars used for linguistic purposes\ngenerally have formulas of order 3 or, at most, 4. We know that once we bound the order\nof formulas in the lexicon of our grammars to be less than a ﬁxed n, parsing becomes\npolynomial for any choice of n (Pentus 2010)6.\nThe NP-completeness proof of Savateev (2009) uses a reduction from SAT, where\na SAT problem with c clauses and v variables produces a Lambek grammar of order\n3+4c, with (2c+1)(3v+1) atomic formulas.\nThe notion of order therefore provides a neat indicator of the complexity: the NP-\ncompleteness proof requires formulas of order 7 and greater, whereas the formulas used\nfor linguistic modelling are of order 4 or less.\nEven though the Lambek calculus is a nice and simple system, we know that the\nLambek calculus generates only context-free languages (Pentus 1995), and there is\ngood evidence that at least some constructions in natural language require a slightly\nlarger class of languages (Shieber 1985). One inﬂuential proposal for such a larger class\nof languages are the mildly context-sensitive languages (Joshi 1985), characterised as\nfollows.\n– contains the context-free languages,\n– limited cross-serial dependencies (i.e includes anbncn but maybe not anbncndnen)\n– semilinearity (a language is semilinear iff there exists a regular language to which\nit is equivalent up to permutation)\n– polynomial ﬁxed recognition7\n5 For many algorithms, the complexity is a function of the number of atomic subformulas of the\nformulas in the sentence. Empirically estimation shows the number of atomic formulas is a bit\nover twice the number of words in a sentence.\n6 For the algorithm of Pentus (2010), the order appears as an exponent in the worst-case com-\nplexity: for a grammar of order n there is a multiplicative factor of 25(n+1). So though polyno-\nmial, this algorithm is not necessarily efﬁcient.\n7 The last two items are sometimes stated as the weaker condition “constant growth” instead\nof semilinearity and the stronger condition of polynomial parsing instead of polynomial ﬁxed\nrecognition. Since all other properties are properties of formal languages, we prefer the formal\nlanguage theoretic notion of polynomial ﬁxed recognition.\nThere are various extensions of the Lambek calculus which generate mildly context-\nsensitive languages while keeping the syntax-semantics interface essentially the same\nas for the Lambek calculus. Currently, little is known about upper bounds of the classes\nof formal languages generated by these extensions of the Lambek calculus. Though\nMoot (2002) shows that multimodal categorial grammars generate exactly the context-\nsensitive languages, Buszkowski (1997) underlines the difﬁculty of adapting the result\nof Pentus (1995) to extensions of the Lambek calculus8.\nBesides problems from the point of view of formal language theory, it should be\nnoted that the goal we set out at the start of this paper was not just to generate the\nright string language but rather to generate the right string-meaning pairs. This poses\nadditional problems. For example, a sentence with n quantiﬁed noun phrases has up to\nn! readings. Although the standard notion of complexity for categorial grammars is the\ncomplexity deciding whether or not a proof exists, formal semanticists, at least since\nMontague (1974), want their formalisms to generate all and only the correct readings\nfor a sentence: we are not only interested in whether or not a proof exists but, since\ndifferent natural deduction proofs correspond to different readings, also in what the\ndifferent proofs of a sentence are9.\nWhen we look at the example below\n(10)\nEvery representative of a company saw most samples.\nit has ﬁve possible readings (instead of 3! = 6), since the reading\n∀x.representative of(x,y) ⇒most(z,sample(z)) ⇒∃y.company(y) ∧see(x,z)\nhas an unbound occurrence of y (the leftmost occurrence). The Lambek calculus anal-\nysis has trouble with all readings where “a company” has wide scope over at least one\nof the two other quantiﬁers. We can, of course, remedy this by adding new, more com-\nplex types to the quantiﬁer “a”, but this would increase the order of the formulas and\nthere is, in principle, no bound on the number of constructions where a medial quanti-\nﬁer has wide scope over a sentence. A simple counting argument shows that Lambek\ncalculus grammars cannot generate the n! readings required for quantiﬁer scope of an n-\nquantiﬁer sentence: the number of readings for a Lambek calculus proof is proportional\nto the Catalan numbers and this number is in o(n!)10; in other words, given a Lambek\n8 We can side-step the need for a Pentus-like proof by looking only at fragments of order 1, but\nthese fragments are insufﬁcient even for handling quantiﬁer scope.\n9 Of course, when our goal is to generate (subsets of) n! different proofs rather than a single\nproof (if one exists), then we are no longer in NP, though it is unknown whether an algorithm\nexists which produces a sort of shared representation for all such subsets such that 1) the\nalgorithm outputs “no” when the sentence is ungrammatical 2) the algorithm has a fairly trivial\nalgorithm (say of a low-degree polynomial at worst) for recovering all readings from the shared\nrepresentation 3) the shared structure is polynomial in the size of the input.\n10 We need to be careful here: the number of readings for a sentence with n quantiﬁers is Θ(n!),\nwhereas the maximum number of Lambek calculus proofs is O(cc2n\n0 Cc1c2n), for constants c0,\nc1, c2 which depend on the grammar (c0 is the maximum number of formulas for a single\nword, c1 is the maximum number of (negative) atomic subformulas for a single formula and c2\nrepresent the minimum number of words needed to add a generalized quantiﬁer to a sentence,\ncalculus grammar, the number of readings of a sentence with n quantiﬁers grows much\nfaster than the number of Lambek calculus proofs for this sentence, hence the grammar\nfails to generate many of the required readings.\nSince the eighties, many variants and extensions of the Lambek calculus have been\nproposed, each with the goal of overcoming the limitations of the Lambek calculus.\nExtensions/variations of the Lambek calculus — which include multimodal categorial\ngrammars (Moortgat 1997), the Displacement calculus (Morrill et al. 2011) and ﬁrst-\norder linear logic (Moot & Piazza 2001) — solve both the problems of formal lan-\nguage theory and the problems of the syntax-semantics interface. For example, there\nare several ways of implementing quantiﬁers yielding exactly the ﬁve desired readings\nfor sentence 10 without appealing to extra-grammatical mechanisms. Carpenter (1994)\ngives many examples of the advantages of this logical approach to scope, notably its\ninteraction with other semantic phenomena like negation and coordination.\nThough these modern calculi solve the problems with the Lambek calculus, they\ndo so without excessively increasing the computational complexity of the formalism:\nmultimodal categorial grammars are PSPACE complete (Moot 2002), whereas most\nother extensions are NP-complete, like the Lambek calculus.\nEven the most basic categorial grammar account of quantiﬁer scope requires for-\nmulas of order 2, while, in contrast to the Lambek calculus, the only known polynomial\nfragments of these logics are of order 1. Hence the known polynomial fragments have\nvery limited appeal for semantics.\nIs the NP-completeness of our logics in conﬂict with the condition of polyno-\nmial ﬁxed recognition required of mildly context-sensitive formalisms? Not necessarily,\nsince our goals are different: we are not only interested in the string language gener-\nated by our formalism but also in the string-meaning mappings. Though authors have\nworked on using mildly context-sensitive formalisms for semantics, they generally use\none of the two following strategies for quantiﬁer scope: 1) an external mechanism for\ncomputing quantiﬁer scope (e.g. Cooper storage, (Cooper 1975)), or 2) an underspeci-\nﬁcation mechanism for representing quantiﬁer scope (Fox & Lappin 2010).\nFor case 1 (Cooper 1975), a single syntactic structure is converted into up to n! se-\nmantic readings, whereas for case 2, though we represent all possible readings in a sin-\ngle structure, even deciding whether the given sentence has a semantic reading at all be-\ncomes NP-complete (Fox & Lappin 2010), hence we simply shift the NP-completeness\nfrom the syntax to the syntax-semantics interface11. Our current understanding there-\nfore indicates that NP-complete is the best we can do when we want to generate the\nsemantics for a sentence. We do not believe this to be a bad thing, since pragmatic\nand processing constraints rule out many of the complex readings and enumerating\nall readings of sentences like sentence 10 above (and more complicated examples) is\na difﬁcult task. There is a trade-off between the work done in the syntax and in the\nsyntax-semantics interface, where the categorial grammar account incorporates more\ni.e. c2n is the number of words required to produce an n-quantiﬁer sentence) and O(cc2n\n0 Cc1c2n)\nis in o(n!).\n11 In addition, Ebert (2005) argues that underspeciﬁcation languages are not expressive enough\nto capture all possible readings of a sentence in a single structure. So underspeciﬁcation does\nnot solve the combinatorial problem but, at best, reduces it.\nthan the traditional mildly context-sensitive formalisms. It is rather easy to set up a cat-\negorial grammar parser in such a way that it produces underspeciﬁed representations in\ntime proportional to n2 (Moot 2007). However, given that such an underspeciﬁed rep-\nresentation need not have any associated semantics, such a system would not actually\nqualify as a parser. We believe, following Carpenter (1994) and Jacobson (2002), that\ngiving an integrated account of the various aspects of the syntax-semantics interface is\nthe most promising path.\nOur grammatical formalisms are not merely theoretical tools, but also form the ba-\nsis of several implementations (Morrill & Valent´ın 2015, Moot 2015), with a rather\nextensive coverage of various semantic phenomena and their interactions, including\nquantiﬁcation, gapping, ellipsis, coordination, comparative subdeletion, etc.\n4\nComplexity of the semantics\nThe complexity of the syntax discussed in the previous section only considered the\ncomplexity of computing unreduced lambda terms as the meaning of a sentence. Even\nin the standard, simply typed Montagovian framework, normalizing lambda terms is\nknown to be of non-elementary complexity (Schwichtenberg 1982), essentially due to\nthe possibility of recursive copying. In spite of this forbidding worst-time complex-\nity, normalization does not seem to be a bottleneck in the computation of meaning for\npractical applications (Bos et al. 2004, Moot 2010).\nIs there a deeper reason for this? We believe that natural language semantics uses a\nrestricted fragment of the lambda calculus, soft lambda calculus. This calculus restricts\nrecursive copying and has been shown to characterize the complexity class P exactly\n(Lafont 2004, Baillot & Mogbil 2004). Hence, this would explain why even naive im-\nplementations of normalization perform well in practice.\nThe question of whether soft linear logic sufﬁces for our semantic parser may ap-\npear hard to answer, however, it an obvious (although tedious) result. To show that all\nthe semantic lambda terms can be typed in soft linear logic, we only need to verify\nthat every lambda in the lexicon is soft. There is a ﬁnite number of words, with only a\nﬁnite number of lambda terms per word. Furthermore, words from open classes (nouns,\nverbs, adjectifs, manner adverbs,... in which speakers may introduce new words... about\n200.000 inﬂected word forms) are the most numerous and all have soft and often even\nlinear lambda terms. Thus only closed class words (grammatical words such as pro-\nnouns, conjunctions, auxiliary verbs,... and some complex adverbs, such as “too”) may\npotentially need a non-soft semantic lambda term: there are less than 500 such words,\nso it is just a matter of patience to prove they all have soft lambda terms. Of course,\nﬁnding deep reasons (cognitive, linguistic) for semantic lambda terms to be soft in any\nlanguage would be much more difﬁcult (and much more interesting!).\nWhen adding coercions, as in Section 2, the process becomes a bit more compli-\ncated. However, the system of Lafont (2004) includes second-order quantiﬁers hence re-\nduction stays polynomial once coercions have been chosen. Their choice (as the choice\nof the syntactic category) increases complexity: when there is a type mismatch gA→XuB\none needs to chose one of the coercions of type B →A provided by the entries of the\nwords in the analysed phrase, with the requirement that when a rigid coercion is used,\nall other coercions provided by the same word are blocked (hence rigid coercions, as op-\nposed to ﬂexible coercions decrease the number of choices for other type mismatches).\nFinally, having computed a set of formulas in higher-order logic corresponding to\nthe meaning of a sentence, though of independent interest for formal semanticists, is\nonly a step towards using these meaning representations for concrete applications. Typ-\nical applications such as question answering, automatic summarization, etc. require\nworld knowledge and common sense reasoning but also a method for deciding about\nentailment: that is, given a set of sentences, can we conclude that another sentence is\ntrue. This question is of course undecidable, already in the ﬁrst-order case. However,\nsome recent research shows that even higher-order logic formulas of the type produced\nby our analysis can form the basis of effective reasoning mechanisms (Chatzikyriakidis\n& Luo 2014, Mineshima et al. 2015) and we leave it as an interesting open question to\nwhat extent such reasoning can be applied to natural language processing tasks.\n5\nConclusion\nIt is somewhat surprising that, in constrast to the well-developed theory of the al-\ngorithmic complexity of parsing, little is known about semantic analysis, even though\ncomputational semantics is an active ﬁeld, as the recurring conferences with the same\ntitle as well as the number of natural language processing applications show. In this\npaper we simply presented remarks on the computability and on the complexity of this\nprocess. The good news is that semantics (at least deﬁned as a set of logical formula)\nis computable. This was known, but only implicitly: Montague gave a set of instruction\nto compute the formula (and to interpret it in a model), but he never showed that, when\ncomputing such logical formula(s):\n– the process he deﬁned stops with a normal lambda terms of type proposition (t),\n– eta-long normal lambda terms with constants being either logical connectives or\nconstants of a ﬁrst (or higher order) logical language are in bijective correspondence\nwith formulas of this logical language (this is more or less clear in the work of\nChurch (1940) on simple type theory).\n– the complexity of the whole process has a known complexity class, in particular the\nbeta-reduction steps which was only discovered years after his death (Schwichten-\nberg 1982).\nA point that we did not discuss is that we considered worst case complexity viewed\nas a function from the number of words in a sentence a logical formula. Both aspects\nof our point of view can be challenged: in practice, grammar size is at least as impor-\ntance as sentence length and average case complexity may be more appropriate than\nworst case complexity. Though the high worst case complexity shows that computing\nthe semantics of a sentence is not always efﬁcient, we nevertheless believe, conﬁrmed\nby actual practice, that statistical models of a syntactic or semantic domain improve\nefﬁciency considerably, by providing extra information (as a useful though faillible “or-\nacle”) for many of the difﬁcult choices. Indeed, human communication and understand-\ning are very effective in general, but, from time to time, we misunderstand eachother or\nneed to ask for clariﬁcations. For computers, the situation is almost identical: most sen-\ntences are analysed quickly, while some require more time or even defeat the software.\nEven though it is quite difﬁcult to obtain the actual probability distribution on sentence-\nmeaning pairs, we can simply estimate such statistics empirically by randomly selecting\nmanually annotated examples from a corpus. The other aspect, the sentence length, is,\nas opposed to what is commonly assumed in complexity theory, not a very saﬁsfactory\nempirical measure of performance: indeed the average number of words per sentence is\naround 10 in spoken language and around 25 in written language. Sentences with more\nthan 100 words are very rare12. Furthermore, lengthy sentences tend to have a simple\nstructure, because otherwise they would quickly become incomprehensible (and hard\nto produce as well). Experience with parsing shows that in many cases, the grammar\nsize is at least as important as sentence length for the empirical complexity of parsing\nalgorithms (Joshi 1997, Sarkar 2000, G´omez-Rodr´ıguez et al. 2006). Grammar size,\nthough only a constant factor in the complexity, tends to be a big constant for realistic\ngrammars: grammars with between 10.000 and 20.000 rules are common.\nWe believe that the complexity of computing the semantics and of reasoning with\nthe semantic representations are some of the most important reasons that the Turing test\nis presently out of reach.\n12 To given an indication, the TLGbank contains more than 14.000 French sentences and has a\nmedian of 26 words per sentence, 99% of sentences having less than 80 words, with outliers\nat 190 and at 266 (the maximum sentence length in the corpus).\nBibliography\nBaillot, P. & Mogbil, V. (2004), Soft lambda-calculus: a language for polynomial\ntime computation, in ‘Foundations of software science and computation structures’,\nSpringer, pp. 27–41.\nBassac, C., Mery, B. & Retor´e, C. (2010), ‘Towards a type-theoretical account of lexical\nsemantics’, Journal of Logic, Language and Information 19(2), 229–245.\nBos, J., Clark, S., Steedman, M., Curran, J. R. & Hockenmaier, J. (2004), Wide-\ncoverage semantic representation from a CCG parser, in ‘Proceedings of COLING-\n2004’, pp. 1240–1246.\nBuszkowski, W. (1997), Mathematical linguistics and proof theory, in J. van Benthem\n& A. ter Meulen, eds, ‘Handbook of Logic and Language’, Elsevier, chapter 12,\npp. 683–736.\nCarpenter, B. (1994), Quantiﬁcation and scoping: A deductive account, in ‘The Pro-\nceedings of the 13th West Coast Conference on Formal Linguistics’.\nChatzikyriakidis, S. & Luo, Z. (2014), ‘Natural language inference in Coq’, Journal of\nLogic, Language and Information 23(4), 441–480.\nChurch, A. (1940), ‘A formulation of the simple theory of types’, Journal of Symbolic\nLogic 5(2), 56–68.\nCooper, R. (1975), Montague’s Semantic Theory and Transformational Grammar, PhD\nthesis, University of Massachusetts.\nCorblin, F. (2013), Cours de s´emantique: Introduction, Armand Colin.\nEbert, C. (2005), Formal Investigations of Underspeciﬁed Representations, PhD thesis,\nKing’s College, University of London.\nFox, C. & Lappin, S. (2010), ‘Expressiveness and complexity in underspeciﬁed seman-\ntics’, Linguistic Analysis 36(1–4), 385–417.\nG´omez-Rodr´ıguez, C., Alonso, M. A. & Vilares, M. (2006), On the theoretical and prac-\ntical complexity of TAG parsers, in ‘Proceedings of Formal Grammar (FG 2006)’,\npp. 87–101.\nJacobson, P. (2002), ‘The (dis)organization of the grammar: 25 years’, Linguistics and\nPhilosophy 25(5-6), 601–626.\nJoshi, A. (1985), Tree adjoining grammars: How much context-sensitivity is required\nto provide reasonable structural descriptions?, in ‘Natural Language Parsing’, Cam-\nbridge University Press, pp. 206–250.\nJoshi, A. (1997), Parsing techniques, in R. A. Cole, J. Mariani, H. Uszkoreit, A. Zaenen\n& V. Zue, eds, ‘Survey of the State of the Art in Human Language Technology’,\nCambridge University Press and Giardini, chapter 11.4, pp. 351–356.\nLafont, Y. (2004), ‘Soft linear logic and polynomial time’, Theoretical Computer Sci-\nence 318(1), 163–180.\nLangacker, R. (2008), Cognitive Grammar — A Basic Introduction., Oxford University\nPress.\nMineshima, K., Martınez-G´omez, P., Miyao, Y. & Bekki, D. (2015), Higher-order logi-\ncal inference with compositional semantics, in ‘Proceedings of EMNLP’, pp. 2055–\n2061.\nMontague, R. (1974), The proper treatment of quantiﬁcation in ordinary English, in\nR. Thomason, ed., ‘Formal Philosophy. Selected Papers of Richard Montague’, Yale\nUniversity Press.\nMoortgat, M. (1997), Categorial type logics, in J. van Benthem & A. ter Meulen, eds,\n‘Handbook of Logic and Language’, Elsevier/MIT Press, chapter 2, pp. 93–177.\nMoot, R. (2002), Proof Nets for Linguistic Analysis, PhD thesis, Utrecht Institute of\nLinguistics OTS, Utrecht University.\nMoot, R. (2007), Filtering axiom links for proof nets, in L. Kallmeyer, P. Monachesi,\nG. Penn & G. Satta, eds, ‘Proccedings of Formal Grammar 2007’.\nMoot, R. (2010), Wide-coverage French syntax and semantics using Grail, in ‘Proceed-\nings of Traitement Automatique des Langues Naturelles (TALN)’, Montreal. System\nDemo.\nMoot, R. (2015), ‘Linear one: A theorem prover for ﬁrst-order linear logic’,\nhttps://github.com/RichardMoot/LinearOne.\nMoot, R. & Piazza, M. (2001), ‘Linguistic applications of ﬁrst order multiplicative lin-\near logic’, Journal of Logic, Language and Information 10(2), 211–232.\nMoot, R. & Retor´e, C. (2012), The Logic of Categorial Grammars: A Deductive Ac-\ncount of Natural Language Syntax and Semantics, Springer.\nMorrill, G. & Valent´ın, O. (2015), Computational coverage of TLG: The Montague test,\nin ‘Proceedings CSSP 2015 Le onzi`eme Colloque de Syntaxe et S´emantique `a Paris’,\npp. 63–68.\nMorrill, G., Valent´ın, O. & Fadda, M. (2011), ‘The displacement calculus’, Journal of\nLogic, Language and Information 20(1), 1–48.\nPentus, M. (1995), Lambek grammars are context free, in ‘Proceedings of Logic in\nComputer Science’, pp. 429–433.\nPentus, M. (2010), ‘A polynomial-time algorithm for Lambek grammars of bounded\norder’, Linguistic Analysis 36(1–4), 441–471.\nPinker, S. (1994), The Language Instinct, Penguin Science.\nRetor´e, C. (2014), The Montagovian Generative Lexicon ΛTyn: a Type Theoretical\nFramework for Natural Language Semantics, in ‘Proceedings of TYPES’, pp. 202–\n229.\nSarkar, A. (2000), Practical experiments in parsing using tree adjoining grammars, in\n‘Proceeding of TAG+ 5’.\nSavateev, Y. (2009), Product-free Lambek calculus is NP-complete, in ‘Symposium on\nLogical Foundations of Computer Science (LFCS) 2009’, pp. 380–394.\nSchwichtenberg, H. (1982), Complexity of normalization in the pure typed lambda-\ncalculus, in ‘The L. E. J. Brouwer Centenary Symposium’, North-Holland, pp. 453–\n457.\nShieber, S. (1985), ‘Evidence against the context-freeness of natural language’, Lin-\nguistics & Philosophy 8, 333–343.\nTuring, A. (1950), ‘Computing machinery and intelligence’, Mind 49, 433–460.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.CC"
  ],
  "published": "2016-05-13",
  "updated": "2016-05-13"
}