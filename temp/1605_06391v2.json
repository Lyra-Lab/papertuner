{
  "id": "http://arxiv.org/abs/1605.06391v2",
  "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach",
  "authors": [
    "Yongxin Yang",
    "Timothy Hospedales"
  ],
  "abstract": "Most contemporary multi-task learning methods assume linear models. This\nsetting is considered shallow in the era of deep learning. In this paper, we\npresent a new deep multi-task representation learning framework that learns\ncross-task sharing structure at every layer in a deep network. Our approach is\nbased on generalising the matrix factorisation techniques explicitly or\nimplicitly used by many conventional MTL algorithms to tensor factorisation, to\nrealise automatic learning of end-to-end knowledge sharing in deep networks.\nThis is in contrast to existing deep learning approaches that need a\nuser-defined multi-task sharing strategy. Our approach applies to both\nhomogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our\ndeep multi-task representation learning in terms of both higher accuracy and\nfewer design choices.",
  "text": "Published as a conference paper at ICLR 2017\nDEEP MULTI-TASK REPRESENTATION LEARNING:\nA TENSOR FACTORISATION APPROACH\nYongxin Yang, Timothy M. Hospedales\nQueen Mary, University of London\n{yongxin.yang, t.hospedales}@qmul.ac.uk\nABSTRACT\nMost contemporary multi-task learning methods assume linear models. This set-\nting is considered shallow in the era of deep learning. In this paper, we present\na new deep multi-task representation learning framework that learns cross-task\nsharing structure at every layer in a deep network. Our approach is based on\ngeneralising the matrix factorisation techniques explicitly or implicitly used by\nmany conventional MTL algorithms to tensor factorisation, to realise automatic\nlearning of end-to-end knowledge sharing in deep networks. This is in contrast\nto existing deep learning approaches that need a user-deﬁned multi-task sharing\nstrategy. Our approach applies to both homogeneous and heterogeneous MTL.\nExperiments demonstrate the efﬁcacy of our deep multi-task representation learn-\ning in terms of both higher accuracy and fewer design choices.\n1\nINTRODUCTION\nThe paradigm of multi-task learning is to learn multiple related tasks simultaneously so that knowl-\nedge obtained from each task can be re-used by the others. Early work in this area focused on neural\nnetwork models (Caruana, 1997), while more recent methods have shifted focus to kernel methods,\nsparsity and low-dimensional task representations of linear models (Evgeniou & Pontil, 2004; Ar-\ngyriou et al., 2008; Kumar & Daum´e III, 2012). Nevertheless given the impressive practical efﬁcacy\nof contemporary deep neural networks (DNN)s in many important applications, we are motivated to\nrevisit MTL from a deep learning perspective.\nWhile the machine learning community has focused on MTL for shallow linear models recently, ap-\nplications have continued to exploit neural network MTL (Zhang et al., 2014; Liu et al., 2015). The\ntypical design pattern dates back at least 20 years (Caruana, 1997): deﬁne a DNN with shared lower\nrepresentation layers, which then forks into separate layers and losses for each task. The sharing\nstructure is deﬁned manually: full-sharing up to the fork, and full separation after the fork. However\nthis complicates DNN architecture design because the user must specify the sharing structure: How\nmany task speciﬁc layers? How many task independent layers? How to structure sharing if there are\nmany tasks of varying relatedness?\nIn this paper we present a method for end-to-end multi-task learning in DNNs. This contribution\ncan be seen as generalising shallow MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008;\nKumar & Daum´e III, 2012) to learning how to share at every layer of a deep network; or as learning\nthe sharing structure for deep MTL (Caruana, 1997; Zhang et al., 2014; Spieckermann et al., 2014;\nLiu et al., 2015) which currently must be deﬁned manually on a problem-by-problem basis.\nBefore proceeding it is worth explicitly distinguishing some different problem settings, which have\nall been loosely referred to as MTL in the literature. Homogeneous MTL: Each task corresponds\nto a single output. For example, MNIST digit recognition is commonly used to evaluate MTL algo-\nrithms by casting it as 10 binary classiﬁcation tasks (Kumar & Daum´e III, 2012). Heterogeneous\nMTL: Each task corresponds to a unique set of output(s) (Zhang et al., 2014). For example, one\nmay want simultaneously predict a person’s age (task one: multi-class classiﬁcation or regression)\nas well as identify their gender (task two: binary classiﬁcation) from a face image.\nIn this paper, we propose a multi-task learning method that works on all these settings. The key idea\nis to use tensor factorisation to divide each set of model parameters (i.e., both FC weight matrices,\n1\narXiv:1605.06391v2  [cs.LG]  16 Feb 2017\nPublished as a conference paper at ICLR 2017\nand convolutional kernel tensors) into shared and task-speciﬁc parts. It is a natural generalisation\nof shallow MTL methods that explicitly or implicitly are based on matrix factorisation (Evgeniou &\nPontil, 2004; Argyriou et al., 2008; Kumar & Daum´e III, 2012; Daum´e III, 2007). As linear methods,\nthese typically require pre-engineered features. In contrast, as a deep network, our generalisation\ncan learn directly from raw image data, determining sharing structure in a layer-wise fashion. For\nthe simplest NN architecture – no hidden layer, single output – our method reduces to matrix-based\nones, therefore matrix-based methods including (Evgeniou & Pontil, 2004; Argyriou et al., 2008;\nKumar & Daum´e III, 2012; Daum´e III, 2007) are special cases of ours.\n2\nRELATED WORK\nMulti-Task Learning\nMost contemporary MTL algorithms assume that the input and model are\nboth D-dimensional vectors. The models of T tasks can then be stacked into a D × T sized matrix\nW. Despite different motivations and implementations, many matrix-based MTL methods work\nby placing constrains on W. For example, posing an ℓ2,1 norm on W to encourage low-rank W\n(Argyriou et al., 2008). Similarly, (Kumar & Daum´e III, 2012) factorises W as W = LS, i.e., it\nassigns a lower rank as a hyper-parameter. An earlier work (Evgeniou & Pontil, 2004) proposes\nthat the linear model for each task t can be written as wt = ˆwt + ˆw0. This is the factorisation\nL = [ ˆw0, ˆw1, . . . , ˆwT ] and S = [11×T ; IT ]. In fact, such matrix factorisation encompasses many\nMTL methods. E.g., (Xue et al., 2007) assumes S·,i (the ith column of S) is a unit vector generated\nby a Dirichlet Process and (Passos et al., 2012) models W using linear factor analysis with Indian\nBuffet Process (Grifﬁths & Ghahramani, 2011) prior on S.\nTensor Factorisation\nIn deep learning, tensor factorisation has been used to exploit factorised\ntensors’ fewer parameters than the original (e.g., 4-way convolutional kernel) tensor, and thus com-\npress and/or speed up the model, e.g., (Lebedev et al., 2015; Novikov et al., 2015). For shallow linear\nMTL, tensor factorisation has been used to address problems where tasks are described by multiple\nindependent factors rather than merely indexed by a single factor (Yang & Hospedales, 2015). Here\nthe D-dimensional linear models for all unique tasks stack into a tensor W, of e.g. D × T1 × T2\nin the case of two task factors. Knowledge sharing is then achieved by imposing tensor norms on\nW (Romera-paredes et al., 2013; Wimalawarne et al., 2014). Our framework factors tensors for the\ndifferent reason that for DNN models, parameters include convolutional kernels (N-way tensors) or\nD1 × D2 FC layer weight matrices (2-way tensors). Stacking up these parameters for many tasks\nresults in D1 × · · · × DN × T tensors within which we share knowledge through factorisation.\nHeterogeneous MTL and DNNs\nSome studies consider heterogeneous MTL, where tasks may\nhave different numbers of outputs (Caruana, 1997). This differs from the previously discussed stud-\nies (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Bonilla et al., 2007; Jacob et al., 2009; Kumar\n& Daum´e III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) which implicitly as-\nsume that each task has a single output. Heterogeneous MTL typically uses neural networks with\nmultiple sets of outputs and losses. E.g., Huang et al. (2013) proposes a shared-hidden-layer DNN\nmodel for multilingual speech processing, where each task corresponds to an individual language.\nZhang et al. (2014) uses a DNN to ﬁnd facial landmarks (regression) as well as recognise facial\nattributes (classiﬁcation); while Liu et al. (2015) proposes a DNN for query classiﬁcation and in-\nformation retrieval (ranking for web search). A key commonality of these studies is that they all\nrequire a user-deﬁned parameter sharing strategy. A typical design pattern is to use shared layers\n(same parameters) for lower layers of the DNN and then split (independent parameters) for the top\nlayers. However, there is no systematic way to make such design choices, so researchers usually rely\non trial-and-error, further complicating the already somewhat dark art of DNN design. In contrast,\nour method learns where and how much to share representation parameters across the tasks, hence\nsigniﬁcantly reducing the space of DNN design choices.\nParametrised DNNs\nOur MTL approach is a parameterised DNN (Sigaud et al., 2015), in that\nDNN weights are dynamically generated given some side information – in the case of MTL, given\nthe task identity. In a related example of speaker-adaptive speech recognition (Tan et al., 2016) there\nmay be several clusters in the data (e.g., gender, acoustic conditions), and each speaker’s model\ncould be a linear combination of these latent task/clusters’ models. They model each speaker i’s\nweight matrix W (i) as a sum of K base models ˜W, i.e., W (i) = PK\nk=1 λ(i)\np\n˜W (p). The difference\nbetween speakers/tasks comes from λ and the base models are shared. An advantage of this is that,\n2\nPublished as a conference paper at ICLR 2017\nwhen new data come, one can choose to re-train λ parameters only, and keep ˜W ﬁxed. This will\nsigniﬁcantly reduce the number of parameters to learn, and consequently the required training data.\nBeyond this, Yang & Hospedales (2015) show that it is possible to train another neural network to\npredict those λ values from some abstract metadata. Thus a model for an unseen task can be gener-\nated on-the-ﬂy with no training instances given an abstract description of the task. The techniques\ndeveloped here are compatible with both these ideas of generating models with minimal or no effort.\n3\nMETHODOLOGY\n3.1\nPRELIMINARIES\nWe ﬁrst recap some tensor factorisation basics before explaining how to factorise DNN weight\ntensors for multi-task representation learning. An N-way tensor W with shape D1 × D2 × · · · DN\nis an N-dimensional array containing QN\nn=1 Dn elements. Scalars, vectors, and matrices can be\nseen as 0, 1, and 2-way tensors respectively, although the term tensor is usually used for 3-way or\nhigher. A mode-n ﬁbre of W is a Dn-dimensional vector obtained by ﬁxing all but the nth index.\nThe mode-n ﬂattening W(n) of W is the matrix of size Dn ×Q\ni¬n Di constructed by concatenating\nall of the Q\ni¬n Di mode-n ﬁbres along columns.\nThe dot product of two tensors is a natural extension of matrix dot product, e.g., if we have a tensor\nA of size M1 ×M2 ×· · · P and a tensor B of size P ×N1 ×N2 . . . , the tensor dot product A•B will\nbe a tensor of size M1 × M2 × · · · N1 × N2 · · · by matrix dot product AT\n(−1)B(1) and reshaping1.\nMore generally, tensor dot product can be performed along speciﬁed axes, A •\nB\n(i,j)\n= AT\n(i)B(j)\nand reshaping. Here the subscripts indicate the axes of A and B at which dot product is performed.\nE.g., when A is of size M1 × P × M3 × · · · MI and B is of size N1 × N2 × P × · · · NJ, then\nA •\nB\n(2,3)\nis a tensor of size M1 × M3 × · · · MI × N1 × N2 × · · · NJ.\nMatrix-based Knowledge Sharing\nAssume we have T linear models (tasks) parametrised by D-\ndimensional weight vectors, so the collection of all models forms a size D × T matrix W. One\ncommonly used MTL approach (Kumar & Daum´e III, 2012) is to place a structure constraint on W,\ne.g., W = LS, where L is a D × K matrix and S is a K × T matrix. This factorisation recovers a\nshared factor L and a task-speciﬁc factor S. One can see the columns of L as latent basis tasks, and\nthe model w(i) for the ith task is the linear combination of those latent basis tasks with task-speciﬁc\ninformation S·,i.\nw(i) := W·,i = LS·,i =\nK\nX\nk=1\nL·,kSk,i\n(1)\nFrom Single to Multiple Outputs\nConsider extending this matrix factorisation approach to the\ncase of multiple outputs. The model for each task is then a D1 × D2 matrix, for D1 input and\nD2 output dimensions. The collection of all those matrices constructs a D1 × D2 × T tensor. A\nstraightforward extension of Eq. 1 to this case is\nW (i) := W·,·,i =\nK\nX\nk=1\nL·,·,kSk,i\n(2)\nThis is equivalent to imposing the same structural constraint on W T\n(3) (transposed mode-3 ﬂattening\nof W). It is important to note that this allows knowledge sharing across the tasks only. I.e., knowl-\nedge sharing is only across-tasks not across dimensions within a task. However it may be that the\nknowledge learned in the mapping to one output dimension may be useful to the others within one\ntask. E.g., consider recognising photos of handwritten and print digits – it may be useful to share\nacross handwritten-print; as well as across different digits within each. In order to support general\nknowledge sharing across both tasks and outputs within tasks, we propose to use more general tensor\nfactorisation techniques. Unlike for matrices, there are multiple deﬁnitions of tensor factorisation,\nand we use Tucker (Tucker, 1966) and Tensor Train (TT) (Oseledets, 2011) decompositions.\n1We slightly abuse ‘-1’ referring to the last axis of the tensor.\n3\nPublished as a conference paper at ICLR 2017\n3.2\nTENSOR FACTORISATION FOR KNOWLEDGE SHARING\nTucker Decomposition\nGiven an N-way tensor of size D1 ×D2 · · ·×DN, Tucker decomposition\noutputs a core tensor S of size K1 × K2 · · · × KN, and N matrices U (n) of size Dn × Kn, such\nthat,\nWd1,d2,...,dN\n=\nK1\nX\nk1=1\nK2\nX\nk2=1\n· · ·\nKN\nX\nkN=1\nSk1,k2,...,kN U (1)\nd1,k1U (2)\nd2,k2 · · · U (N)\ndN,kN\n(3)\nW\n=\nS •\nU (1)\n(1,2)\n•\nU (2)\n(1,2)\n· · · •\nU (N)\n(1,2)\n(4)\nTucker decomposition is usually implemented by an alternating least squares (ALS) method (Kolda\n& Bader, 2009). However (Lathauwer et al., 2000) treat it as a higher-order singular value decom-\nposition (HOSVD), which is more efﬁcient to solve: U (n) is exactly the U matrix from the SVD of\nmode-n ﬂattening W(n) of W, and the core tensor S is obtained by,\nS = W •\nU (1)\n(1,1)\n•\nU (2)\n(1,1)\n· · · •\nU (N)\n(1,1)\n(5)\nTensor Train Decomposition\nTensor Train (TT) Decomposition outputs 2 matrices U (1) and\nU (N) of size D1 × K1 and KN−1 × DN respectively, and (N −2) 3-way tensors U(n) of size\nKn−1 × Dn × Kn. The elements of W can be computed by,\nWd1,d2,...,dN\n=\nK1\nX\nk1=1\nK2\nX\nk2=1\n· · ·\nKN−1\nX\nkN−1=1\nU (1)\nd1,k1U(2)\nk1,d2,k2U(3)\nk2,d3,k3 · · · U (N)\nkN−1,dN\n(6)\n=\nU (1)\nd1,·U(2)\n·,d2,·U(3)\n·,d3,· · · · U (d)\n·,dN\n(7)\nW\n=\nU (1) • U(2) · · · • U (N)\n(8)\nwhere U(n)\n·,dn,· is a matrix of size Kn−1 × Kn sliced from U(n) with the second axis ﬁxed at dn. The\nTT decomposition is typically realised with a recursive SVD-based solution (Oseledets, 2011).\nKnowledge Sharing\nIf the ﬁnal axis of the input tensor above indexes tasks, i.e. if DN = T then\nthe last factor U (N) in both decompositions encodes a matrix of task speciﬁc knowledge, and the\nother factors encode shared knowledge.\n3.3\nDEEP MULTI-TASK REPRESENTATION LEARNING\nTo realise deep multi-task representation learning (DMTRL), we learn one DNN per-task each with\nthe same architecture2. However each corresponding layer’s weights are generated with one of the\nknowledge sharing structures in Eq. 2, Eq. 4 or Eq. 8. It is important to note that we apply these\n‘right-to-left’ in order to generate weight tensors with the speciﬁed sharing structure, rather than\nactually applying Tucker or TT to decompose an input tensor. In the forward pass, we synthesise\nweight tensors W and perform inference as usual, so the method can be thought of as tensor com-\nposition rather than decomposition.\nOur weight generation (construct tensors from smaller pieces) does not introduce non-differentiable\nterms, so our deep multi-task representation learner is trainable via standard backpropagation.\nSpeciﬁcally, in the backward pass over FC layers, rather than directly learning the 3-way tensor\nW, our methods learn either {S, U1, U2, U3} (DMTRL-Tucker, Eq. 4), {U1, U2, U3} (DMTRL-TT,\nEq. 8), or in the simplest case {L, S} (DMTRL-LAF3, Eq. 2). Besides FC layers, contemporary\n2Except heterogeneous MTL, where the output layer is necessarily unshared due to different dimensionality.\n3LAF refers to Last Axis Flattening.\n4\nPublished as a conference paper at ICLR 2017\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nHomogeneous MTL\n(Shallow)\nHeterogeneous MTL\n...\n...\n...\n...\nSTL\nMTL\nUD­MTL\nDMTRL\nSTL\n...\n...\n...\n...\n...\n...\n...\nHomogeneous MTL\n(Deep)\n...\n...\n...\n...\nUD­MTL\nDMTRL\nSTL\nFigure 1: Illustrative example with two tasks corresponding to two neural networks in homogeneous\n(single output) and heterogeneous (different output dimension) cases. Weight layers grouped by\nsolid rectangles are tied across networks. Weight layers grouped by dashed rectangles are softly\nshared across networks with our method. Ungrouped weights are independent.\nHomogeneous MTL Shallow: Left is STL (two independent networks); right is MTL. In the case\nof vector input and no hidden layer, our method is equivalent to conventional matrix-based MTL\nmethods. Homogeneous MTL Deep: STL (Left) is independent networks. User-deﬁned-MTL (UD-\nMTL) selects layers to share/separate. Our DMTRL learns sharing at every layer. Heterogeneous\nMTL: UD-MTL selects layers to share/separate. DMTRL learns sharing at every shareable layer.\nDNN designs often exploit convolutional layers. Those layers usually contain kernel ﬁlter parame-\nters that are 3-way tensors of size H ×W ×C, (where H is height, W is width, and C is the number\nof input channels) or 4-way tensors of size H ×W ×C ×M, where M is the number of ﬁlters in this\nlayer (i.e., the number of output channels). The proposed methods naturally extend to convolution\nlayers as convolution just adds more axes on the left-hand side. E.g., the collection of parameters\nfrom a given convolutional layer of T neural networks forms a tensor of shape H ×W ×C ×M ×T.\nThese knowledge sharing strategies provide a way to softly share parameters across the correspond-\ning layers of each task’s DNN: where, what, and how much to share are learned from data. This is\nin contrast to the conventional Deep-MTL approach of manually selecting a set of layers to undergo\nhard parameter sharing: by tying weights so each task uses exactly the same weight matrix/tensor\nfor the corresponding layer (Zhang et al., 2014; Liu et al., 2015); and a set of layers to be completely\nseparate: by using independent weight matrices/tensors. In contrast our approach beneﬁts from:\n(i) automatically learning this sharing structure from data rather than requiring user trial and error,\nand (ii) smoothly interpolating between fully shared and fully segregated layers, rather than a hard\nswitching between these states. An illustration of the proposed framework for different problem\nsettings can be found in Fig. 1.\n4\nEXPERIMENTS\nImplementation Details\nOur method is implemented with TensorFlow (Abadi et al., 2015). The\ncode is released on GitHub4. For DMTRL-Tucker, DMTRL-TT, and DMTRL-LAF, we need to\nassign the rank of each weight tensor. The DNN architecture itself may be complicated and so\ncan beneﬁt from different ranks at different layers, but grid-search is impractical. However, since\n4https://github.com/wOOL/DMTRL\n5\nPublished as a conference paper at ICLR 2017\nboth Tucker and TT decomposition methods have SVD-based solutions, and vanilla SVD is directly\napplicable to DMTRL-LAF, we can initialise the model and set the ranks as follows: First train the\nDNNs independently in single task learning mode. Then pack the layer-wise parameters as the input\nfor tensor decomposition. When SVD is applied, set a threshold for relative error so SVD will pick\nthe appropriate rank. Thus our method needs only a single hyper parameter of max reconstruction\nerror (we set to ϵ = 10% throughout) that indirectly speciﬁes the ranks of every layer. Note that\ntraining from random initialisation also works, but the STL-based initialisation makes rank selection\neasy and transparent. Nevertheless, like (Kumar & Daum´e III, 2012) the framework is not sensitive\nto rank choice so long as they are big enough. If random initialisation is desired to eliminate the\npre-training requirement, good practice is to initialise parameter tensors by a suitable random weight\ndistribution ﬁrst, then do decomposition, and use the decomposed values for initialising the factors\n(the real learnable parameters in our framework). In this way, the resulting re-composed tensors will\nhave approximately the intended distribution. Our sharing is applied to weight parameters only, bias\nterms are not shared. Apart from initialisation, decomposition is not used anywhere.\n4.1\nHOMOGENEOUS MTL\nDataset, Settings and Baselines We use MNIST handwritten digits. The task is to recognise digit\nimages zero to nine. When this dataset is used for the evaluation of MTL methods, ten 1-vs-all\nbinary classiﬁcation problems usually deﬁne ten tasks (Kumar & Daum´e III, 2012). The dataset has\na given train (60,000 images) and test (10,000 images) split. Each instance is a monochrome image\nof size 28 × 28 × 1.\nWe use a modiﬁed LeNet (LeCun et al., 1998) as the CNN architecture. The ﬁrst convolutional layer\nhas 32 ﬁlters of size 5 × 5, followed by 2 × 2 max pooling. The second convolutional layer has 64\nﬁlters of size 4 × 4, and again a 2 × 2 max pooling. After these two convolutional layers, two fully\nconnected layers with 512 and 1 output(s) are placed sequentially. The convolutional and ﬁrst FC\nlayer use RELU f(x) = max(x, 0) activation function. We use hinge loss, ℓ(y) = max(0, 1−ˆy·y),\nwhere y ∈±1 is the true label and ˆy is the output of each task’s neural network.\nConventional matrix-based MTL methods (Evgeniou & Pontil, 2004; Argyriou et al., 2008; Kumar\n& Daum´e III, 2012; Romera-paredes et al., 2013; Wimalawarne et al., 2014) are linear models taking\nvector input only, so they need a preprocessing that ﬂattens the image into a vector, and typically\nreduce dimension by PCA. As per our motivation for studying Deep MTL, our methods decisively\noutperform such shallow linear baselines. Thus to ﬁnd a stronger MTL competitor, we instead search\nuser deﬁned architectures for Deep-MTL parameter sharing (cf (Zhang et al., 2014; Liu et al., 2015;\nCaruana, 1997)). In all of the four parametrised layers (pooling has no parameters), we set the ﬁrst\nN (1 ≤N ≤3) to be hard shared5. We then use cross-validation to select among the three user-\ndeﬁned MTL architectures and the best option is N = 3, i.e., the ﬁrst three layers are fully shared\n(we denote this model UD-MTL). For our methods, all four parametrised layers are softly shared\nwith the different factorisation approaches. To evaluate different MTL methods and a baseline of\nsingle task learning (STL), we take ten different fractions of the given 60K training split, train the\nmodel, and test on the 10K testing split. For each fraction, we repeat the experiment 5 times with\nrandomly sampled training data. We report two performance metrics: (1) the mean error rate of the\nten binary classiﬁcation problems and (2) the error rate of recognising a digit by ranking each task’s\n1-vs-all output (multi-class classiﬁcation error).\nResults\nAs we can see in Fig. 2, all MTL approaches outperform STL, and the advantage is more\nsigniﬁcant when the training data is small. The proposed methods, DMTRL-TT and DMTRL-\nTucker outperform the best user-deﬁned MTL when the training data is very small, and their perfor-\nmance is comparable when the training data is large.\nFurther Discussion\nFor a slightly unfair comparison, in the case of binary classiﬁcation with 1000\ntraining data, shallow matrix-based MTL methods with PCA feature (Kang et al., 2011; Kumar &\nDaum´e III, 2012) reported 14.0% / 13.4% error rate. With the same amount of data, our methods\n5This is not strictly all possible user-deﬁned sharing options. For example, another possibility is the ﬁrst\nconvolutional layer and the ﬁrst FC layer could be fully shared, with the second convolutional layer being in-\ndependent (task speciﬁc). However, this is against the intuition that lower/earlier layers are more task agnostic,\nand later layers more task speciﬁc. Note that sharing the last layer is technically possible but not intuitive, and\nin any case not meaningful unless at least one early layer is unshared, as the tasks are different.\n6\nPublished as a conference paper at ICLR 2017\n10-2\n10-1\n100\nFraction of Training Data\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\nError Rate\nBinary Classification\nSTL\nDMTRL-LAF\nDMTRL-Tucker\nDMTRL-TT\nUD-MTL\n10-2\n10-1\n100\nFraction of Training Data\n0\n0.05\n0.1\n0.15\n0.2\nError Rate\nMulti-class Classification\nSTL\nDMTRL-LAF\nDMTRL-Tucker\nDMTRL-TT\nUD-MTL\nFigure 2: Homogeneous MTL: digit recognition on MNIST dataset. Each digit provides a task.\nhave error rate below 6%. This shows the importance of our deep end-to-end multi-task represen-\ntation learning contribution versus conventional shallow MTL. Since the error rates in (Kang et al.,\n2011; Kumar & Daum´e III, 2012) were produced on a private subset of MNIST dataset with PCA\nrepresentations only, to ensure a direct comparison, we implement several classic MTL methods and\ncompare them in Appendix A.\nFor readers interested in the connection to model capacity (number of parameters), we present fur-\nther analysis in Appendix B.\n4.2\nHETEROGENEOUS MTL: FACE ANALYSIS\nDataset, Settings and Baselines\nThe AdienceFaces (Eidinger et al., 2014) is a large-scale face\nimages dataset with the labels of each person’s gender and age group. We use this dataset for\nthe evaluation of heterogeneous MTL with two tasks: (i) gender classiﬁcation (two classes) and\n(ii) age group classiﬁcation (eight classes). Two independent CNN models for this benchmark are\nintroduced in (Levi & Hassncer, 2015). The two CNNs have the same architecture except for the\nlast fully-connected layer, since the heterogeneous tasks have different number of outputs (two /\neight). We take these CNNs from (Levi & Hassncer, 2015) as the STL baseline. We again search\nfor the best possible user-deﬁned MTL architecture as a strong competitor: the proposed CNN has\nsix layers – three convolutional and three fully-connected layers. The last fully-connected layer has\nnon-shareable parameters because they are of different size. To search the MTL design-space, we\ntry setting the ﬁrst N (1 ≤N ≤5) layers to be hard shared between the tasks. Running 5-fold\ncross-validation on the train set to evaluate the architectures, we ﬁnd the best choice is N = 5 (i.e.,\nall layers fully shared before the ﬁnal heterogeneous outputs). For our proposed methods, all the\nlayers before the last heterogeneous dimensionality FC layers are softly shared.\nWe select increasing fractions of the AdienceFaces train split randomly, train the model, and evaluate\non the same test set. For reference, there are 12245 images with gender labelled for training, 4007\nones for testing, and 11823 images with age group labelled for training, and 4316 ones for testing.\nResults\nFig. 3 shows the error rate for each task. For the gender recognition task, we ﬁnd that:\n(i) User-deﬁned MTL is not consistently better than STL, but (ii) our methods, esp., DMTRL-\nTucker, consistently outperform both STL and the best user-deﬁned MTL. For the harder age group\nclassiﬁcation task, our methods generally improve on STL. However UD-MTL does not consistently\nimprove on STL, and even reduces performance when the training set is bigger. This is the negative\ntransfer phenomenon (Rosenstein et al., 2005), where using a transfer learning algorithm is worse\nthan not using it. This difference in outcomes is attributed to sufﬁcient data eventually providing\nsome effective task-speciﬁc representation. Our methods can discover and exploit this, but UD-\nMTL’s hard switch between sharing and not sharing can not represent or exploit such increasing\ntask-speciﬁcity of representation.\n7\nPublished as a conference paper at ICLR 2017\n10-2\n10-1\n100\nFraction of Training Data\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\nError Rate\nGender Classification\n10-2\n10-1\n100\nFraction of Training Data\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nError Rate\nAge Group Classification\nSTL\nDMTRL-LAF\nDMTRL-Tucker\nDMTRL-TT\nUD-MTL\nFigure 3: Heterogeneous MTL: Age and Gender recognition in AdienceFace dataset.\n4.3\nHETEROGENEOUS MTL: MULTI-ALPHABET RECOGNITION\nDataset, Settings and Baselines\nWe next consider the task of learning to recognise handwritten\nletters in multiple languages using the Omniglot (Lake et al., 2015) dataset. Omniglot contains\nhandwritten characters in 50 different alphabets (e.g., Cyrillic, Korean, Tengwar), each with its own\nnumber of unique characters (14 ∼55). In total, there are 1623 unique characters, and each has\nexactly 20 instances. Here each task corresponds to an alphabet, and the goal is to recognise its\ncharacters. MTL has a clear motivation here, as cross-alphabet knowledge sharing is likely to be\nuseful as one is unlikely to have extensive training data for a wide variety of less common alphabets.\nThe images are monochrome of size 105 × 105. We design a CNN with 3 convolutional and 2 FC\nlayers. The ﬁrst conv layer has 8 ﬁlters of size 5 × 5; the second conv layer has 12 ﬁlters of size\n3 × 3, and the third convolutional layer has 16 ﬁlters of size 3 × 3. Each convolutional layer is\nfollowed by a 2 × 2 max-pooling. The ﬁrst FC layer has 64 neurons, and the second FC layer has\nsize corresponding to the number of unique classes in the alphabet. The activation function is tanh.\nWe use a similar strategy to ﬁnd the best user-deﬁned MTL model: the CNN has 5 parametrised\nlayers, of which 4 layers are potentially shareable. So we tried hard-sharing the ﬁrst N (1 ≤N ≤4)\nlayers. Evaluating these options by 5-fold cross-validation, the best option turned out to be N = 3,\ni.e., the ﬁrst three layers are hard shared. For our methods, all four shareable layers are softly shared.\nSince there is no standard train/test split for this dataset, we use the following setting: We repeat-\nedly pick at random 5, . . . 90% of images per class for training. Note that 5% is the minimum,\ncorresponding to one-shot learning. The remaining data are used for evaluation.\nResults\nFig. 4 reports the average error rate across all 50 tasks (alphabets). Our proposed MTL\nmethods surpass the STL baseline in all cases. User-deﬁned MTL does not work well when the\ntraining data is very small, but does help when training fraction is larger than 50%.\nMeasuring the Learned Sharing\nCompared to the conventional user-deﬁned sharing architec-\ntures, our method learns how to share from data. We next try to quantify the amount of sharing\nestimated by our model on the Omniglot data. Returning to the key factorisation W = LS, we\ncan ﬁnd that S-like matrix appears in all variants of proposed method. It is S in DMTRL-LAF, the\ntransposed U (N) in DMTRL-Tucker, and U (N) in DMTRL-TT (N is the last axis of W). S is a\nK × T size matrix, where T is the number of tasks, and K is the number of latent tasks (Kumar\n& Daum´e III, 2012) or the dimension of task coding (Yang & Hospedales, 2015). Each column\nof S is a set of coefﬁcients that produce the ﬁnal weight matrix/tensor by linear combination. If\nwe put STL and user-deﬁned MTL (for a certain shared layer) in this framework, we see that STL\nis to assign (rather than learn) S to be an identity matrix IT . Similarly, user-deﬁned MTL (for a\ncertain shared layer) is to assign S to be a matrix with all zeros but one particular row is all ones,\ne.g., S = [11×T ; 0]. Between these two extremes, our method learns the sharing structure in S. We\npropose the following equation to measure the learned sharing strength:\nρ =\n1\n\u0000T\n2\n\u0001\nX\ni<j\nΩ(S·,i, S·,j) =\n2\nT(T −1)\nX\ni<j\nΩ(S·,i, S·,j)\n(9)\n8\nPublished as a conference paper at ICLR 2017\nConv1 Conv2 Conv3 FC1\nFC2\nLayers\n0\n0.2\n0.4\n0.6\n0.8\n1\nSharing Strength\nSharing Strength at Each Layer\nDMTRL-LAF\nDMTRL-Tucker\nDMTRL-TT\nUD-MTL\n0.05 0.10 0.20 0.50 0.60 0.70 0.80 0.90\nFraction of Training Data\n0.3\n0.4\n0.5\n0.6\n0.7\nError Rate\nAlphabet Classification\nSTL\nDMTRL-LAF\nDMTRL-Tucker\nDMTRL-TT\nUD-MTL\nFigure 4: Results of multi-task learning of multilingual character recognition (Omniglot dataset).\nBelow: Illustration of the language pairs estimated to be the most related (left - Georgian Mkhedruli\nand Inuktitut) and most unrelated (right - Balinese and ULOG) character recognition tasks.\nHere Ω(a, b) is a similarity measure for two vectors a and b and we use cosine similarity. ρ is the\naverage on all combinations of column-wise similarity. So ρ measures how much sharing is encoded\nby S between ρ = 0 for STL (nothing to share) and ρ = 1 for user-deﬁned MTL (completely shared).\nSince S is a real-valued matrix in our scenario, we normalise it before applying Eq. 9: First we take\nabsolute values, because large either positive or negative value suggests a signiﬁcant coefﬁcient.\nSecond we normalise each column of S by applying a softmax function, so the sum of every column\nis 1. The motivation behind the second step is to make a matched range of our S with S = IT or\nS = [11×T ; 0], as for those two cases, the sum of each column is 1 and the range is [0, 1].\nFor the Omniglot experiment, we plot the measured sharing amount for training fraction 10%. Fig. 4\nreveals that three proposed methods tend to share more for bottom layers (‘Conv1’, ‘Conv2’, and\n‘Conv3’) and share less for top layer (‘FC1’). This is qualitatively similar to the best user-deﬁned\nMTL, where the ﬁrst three layers are fully shared (ρ = 1) and the 4th layer is completely not shared\n(ρ = 0). However, our methods: (i) learn this structure in a purely data-driven way and (ii) beneﬁts\nfrom the ability to smoothly interpolate between high and low degrees of sharing as depth increases.\nAs an illustration, Fig. 4 also shows example text from the most and least similar language pairs as\nestimated at our multilingual character recogniser’s FC1 layer (the result can vary across layers).\n5\nCONCLUSION\nIn this paper, we propose a novel framework for end-to-end multi-task representation learning in\ncontemporary deep neural networks. The key idea is to generalise matrix factorisation-based multi-\ntask ideas to tensor factorisation, in order to ﬂexibly share knowledge in fully connected and convo-\nlutional DNN layers. Our method provides consistently better performance than single task learning\nand comparable or better performance than the best results from exhaustive search of user-deﬁned\nMTL architectures. It reduces the design choices and architectural search space that must be ex-\nplored in the workﬂow of Deep MTL architecture design (Caruana, 1997; Zhang et al., 2014; Liu\net al., 2015), relieving researchers of the need to decide how to structure layer sharing/segregation.\nInstead sharing structure is determined in a data-driven way on a layer-by-layer basis that moreover\nallows a smooth interpolation between sharing and not sharing in progressively deeper layers.\nAcknowledgements\nThis work was supported by EPSRC (EP/L023385/1), and the European\nUnion’s Horizon 2020 research and innovation program under grant agreement No 640891.\n9\nPublished as a conference paper at ICLR 2017\nREFERENCES\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Man´e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\nMike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-\ncent Vanhoucke, Vijay Vasudevan, Fernanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from\ntensorﬂow.org.\nAndreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learn-\ning. Machine Learning, 2008.\nEdwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task gaussian process prediction.\nIn Neural Information Processing Systems (NIPS), 2007.\nRich Caruana. Multitask learning. Machine Learning, 1997.\nHal Daum´e III. Frustratingly easy domain adaptation. In ACL, 2007.\nEran Eidinger, Roee Enbar, and Tal Hassner. Age and gender estimation of unﬁltered faces. IEEE\nTransactions on Information Forensics and Security, 2014.\nTheodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In Knowledge\nDiscovery and Data Mining (KDD), 2004.\nThomas L. Grifﬁths and Zoubin Ghahramani. The indian buffet process: An introduction and review.\nJournal of Machine Learning Research (JMLR), 2011.\nJui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yifan Gong. Cross-language knowledge transfer\nusing multilingual deep neural network with shared hidden layers. In International Conference\non Acoustics, Speech, and Signal Processing (ICASSP), 2013.\nLaurent Jacob, Jean-philippe Vert, and Francis R Bach. Clustered multi-task learning: A convex\nformulation. In Neural Information Processing Systems (NIPS), 2009.\nZhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature\nlearning. In International Conference on Machine Learning (ICML), 2011.\nTamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review, 2009.\nAbhishek Kumar and Hal Daum´e III. Learning task grouping and overlap in multi-task learning. In\nInternational Conference on Machine Learning (ICML), 2012.\nBrenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 2015.\nLieven De Lathauwer, Bart De Moor, and Joos Vandewalle. A multilinear singular value decompo-\nsition. SIAM Journal on Matrix Analysis and Applications, 2000.\nVadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S. Lempitsky.\nSpeeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. In International\nConference on Learning Representations (ICLR), 2015.\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\nGradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 1998.\nG. Levi and T. Hassncer. Age and gender classiﬁcation using convolutional neural networks. In\nComputer Vision and Pattern Recognition Workshops (CVPRW), 2015.\nXiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representa-\ntion learning using multi-task deep neural networks for semantic classiﬁcation and information\nretrieval. NAACL, 2015.\n10\nPublished as a conference paper at ICLR 2017\nAlexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing neural\nnetworks. In Neural Information Processing Systems (NIPS), 2015.\nI. V. Oseledets. Tensor-train decomposition. SIAM Journal on Scientiﬁc Computing, 2011.\nAlexandre Passos, Piyush Rai, Jacques Wainer, and Hal Daum´e III. Flexible modeling of latent task\nstructures in multitask learning. In International Conference on Machine Learning (ICML), 2012.\nBernardino Romera-paredes, Hane Aung, Nadia Bianchi-berthouze, and Massimiliano Pontil. Mul-\ntilinear multitask learning. In International Conference on Machine Learning (ICML), 2013.\nMichael T. Rosenstein, Zvika Marx, Leslie Pack Kaelbling, and Thomas G. Dietterich. To transfer\nor not to transfer. In In NIPS Workshop, Inductive Transfer: 10 Years Later, 2005.\nOlivier Sigaud, Clement Masson, David Filliat, and Freek Stulp. Gated networks: an inventory.\narXiv, 2015.\nSigurd Spieckermann, Steffen Udluft, and Thomas Runkler. Data-efﬁicient temporal regression with\nmultitask recurrent neural networks. In NIPS Workshop on Transfer and Multi-Task Learning,\n2014.\nTian Tan, Yanmin Qian, and Kai Yu. Cluster adaptive training for deep neural network based acoustic\nmodel. IEEE/ACM Trans. Audio, Speech & Language Processing, 24(3):459–468, 2016.\nL. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 1966.\nKishan Wimalawarne, Masashi Sugiyama, and Ryota Tomioka. Multitask learning meets tensor\nfactorization: task imputation via convex optimization. In Neural Information Processing Systems\n(NIPS), 2014.\nYa Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishnapuram. Multi-task learning for classiﬁca-\ntion with dirichlet process priors. Journal of Machine Learning Research (JMLR), 2007.\nYongxin Yang and Timothy M. Hospedales. A uniﬁed perspective on multi-domain and multi-task\nlearning. In International Conference on Learning Representations (ICLR), 2015.\nZhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by\ndeep multi-task learning. In European Conference on Computer Vision (ECCV), 2014.\n11\nPublished as a conference paper at ICLR 2017\nA\nCOMPARISON WITH CLASSIC (SHALLOW) MTL METHODS\nWe provide a comparison with classic (shallow, matrix-based) MTL methods for the ﬁrst experiment\n(MNIST, binary one-vs-rest classiﬁcation, 1% training data, mean of error rates for 10-fold CV). A\nsubtlety in making this comparison is what feature should the classic methods use? Conventionally\nthey use a PCA feature (obtained by ﬂattening the image, then dimension reduction by PCA). How-\never for visual recognition tasks, performance is better with deep features – a key motivation for our\nfocus on deep approaches to MTL. We therefore also compare the classic methods when using a\nfeature extracted from the penultimate layer of the CNN network used in our experiment.\nModel\nPCA Feature\nCNN Feature\nSingle Task Learning\n16.89\n11.52\nEvgeniou & Pontil (2004)\n15.27\n10.32\nArgyriou et al. (2008)\n15.64\n9.56\nKumar & Daum´e III (2012)\n14.08\n9.41\nDMTRL-LAF\n-\n8.25\nDMTRL-Tucker\n-\n9.24\nDMTRL-TT\n-\n7.31\nUD-MTL\n-\n9.34\nTable 1: Comparison with classic MTL methods. MNIST binary classiﬁcation error rate (%).\nAs expected, the classic methods improve on STL, and they perform signiﬁcantly better with CNN\nthan PCA features. However, our DMTRL methods still outperform the best classic methods, even\nwhen they are enhanced by CNN features. This is due to soft (cf hard) sharing of the feature extrac-\ntion layers and the ability of end-to-end training of both the classiﬁer and feature extractor. Finally,\nwe note that more fundamentally, the classic methods are restricted to binary problems (due to their\nmatrix-based nature) and so, unlike our tensor-based approach, they are unsuitable for multi-class\nproblems like omniglot and age-group classiﬁcation.\nB\nMODEL CAPACITY AND PERFORMANCE\nWe list the number of parameters for each model in the ﬁrst experiment (MNIST, binary one-vs-rest\nclassiﬁcation) and the performance (1% training data, mean of error rate for 10-fold CV).\nModel\nError Rate (%)\nNumber of parameters\nRatio\nSTL\n11.52\n4351K\n1.00\nDMTRL-LAF\n8.25\n1632K\n0.38\nDMTRL-Tucker\n9.24\n1740K\n0.40\nDMTRL-TT\n7.31\n2187K\n0.50\nUD-MTL\n9.34\n436K\n0.10\nUD-MTL-Large\n9.39\n1644K\n0.38\nTable 2: Comparison of deep models: Error rate and number of parameters.\nThe conventional hard-sharing method (UD-MTL) design is to share all layers except the top layer.\nIts number of parameter is roughly 10% of the single task learning method (STL), as most parameters\nare shared across the 10 tasks corresponding to 10 digits. Our soft-sharing methods also signiﬁcantly\nreduce the number of parameters compared to STL, but are larger than UD-MTL’s hard sharing.\nTo compare our method to UD-MTL, while controlling for network capacity, we expanded UD-\nMDL by adding more hidden neurons so its number of parameter is close to our methods (denoted\nUD-MTL-Large). However UD-MDL performance does not increase. This is evidence that our\nmodel’s good performance is not simply due to greater capacity than UD-MTL.\n12\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-05-20",
  "updated": "2017-02-16"
}