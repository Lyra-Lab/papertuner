{
  "id": "http://arxiv.org/abs/1605.09131v1",
  "title": "Classification under Streaming Emerging New Classes: A Solution using Completely Random Trees",
  "authors": [
    "Xin Mu",
    "Kai Ming Ting",
    "Zhi-Hua Zhou"
  ],
  "abstract": "This paper investigates an important problem in stream mining, i.e.,\nclassification under streaming emerging new classes or SENC. The common\napproach is to treat it as a classification problem and solve it using either a\nsupervised learner or a semi-supervised learner. We propose an alternative\napproach by using unsupervised learning as the basis to solve this problem. The\nSENC problem can be decomposed into three sub problems: detecting emerging new\nclasses, classifying for known classes, and updating models to enable\nclassification of instances of the new class and detection of more emerging new\nclasses. The proposed method employs completely random trees which have been\nshown to work well in unsupervised learning and supervised learning\nindependently in the literature. This is the first time, as far as we know,\nthat completely random trees are used as a single common core to solve all\nthree sub problems: unsupervised learning, supervised learning and model update\nin data streams. We show that the proposed unsupervised-learning-focused method\noften achieves significantly better outcomes than existing\nclassification-focused methods.",
  "text": "arXiv:1605.09131v1  [cs.LG]  30 May 2016\nClassiﬁcation under\nStreaming Emerging New Classes:\nA Solution using Completely Random Trees\nXin Mu⋆, Kai Ming Ting♯, Zhi-Hua Zhou⋆∗\n⋆National Key Laboratory for Novel Software Technology\nNanjing University, Nanjing 210093, China\n♯School of Engineering and Information Technology,\nFederation University, Victoria, Australia.\nAbstract\nThis paper investigates an important problem in stream mining, i.e., classiﬁcation under stream-\ning emerging new classes or SENC. The common approach is to treat it as a classiﬁcation problem\nand solve it using either a supervised learner or a semi-supervised learner. We propose an alter-\nnative approach by using unsupervised learning as the basis to solve this problem. The SENC\nproblem can be decomposed into three sub problems: detecting emerging new classes, classifying\nfor known classes, and updating models to enable classiﬁcation of instances of the new class and\ndetection of more emerging new classes. The proposed method employs completely random trees\nwhich have been shown to work well in unsupervised learning and supervised learning indepen-\ndently in the literature. This is the ﬁrst time, as far as we know, that completely random trees are\nused as a single common core to solve all three sub problems: unsupervised learning, supervised\nlearning and model update in data streams. We show that the proposed unsupervised-learning-\nfocused method often achieves signiﬁcantly better outcomes than existing classiﬁcation-focused\nmethods.\nKey words:\nData stream, Emerging new class, Ensemble method, Completely Random Trees\nThis paper investigates an important problem in data streams, i.e., classiﬁcation under streaming\nemerging new class or SENC. In many real-world data mining problems, the environment is open\n∗Corresponding author. Email: zhouzh@nju.edu.cn\nPreprint submitted for review\nMay 31, 2016\nmoney\nplane\n2014.01\n2014.06\n2014.10\nplane\nfootball\nphone\nmoney\nplane\nfootball\nmoney\nFigure 1: Image classiﬁcation in a data stream\nand changes gradually. In the streaming classiﬁcation problem, some new classes are likely to\nemerge as the environment changes. The predictive accuracy of a previously trained classiﬁer\nwill be severely degraded if it is used to classify instances of a previously unseen class in the data\nstream. Ideally, we would like instances of a new class to be detected as soon as they emerge in\nthe data stream; and only instances which are likely to belong to known classes are passed to the\nclassiﬁer to predict their classes.\nIt is assumed that true class labels are not available throughout the entire process, except a\ntraining set of known classes which is used to train a classiﬁer (and a detector for new classes) at\nthe beginning of the data stream. After the deployment of the classiﬁer (and the detector), any\nfuture updates of the models must rely on the unlabelled instances as they appear in the data\nstream. Note that this assumption does not prevent the proposed method from using true class\nlabels when they are available. It sets the hardest condition in the SENC problem.\nAn illustrative example is provided in Figure 1 which shows a news image classiﬁer system making\npredictions in a data stream. Assume that a classiﬁer about news content is built in early 2014,\nwhich starts with two classes (money and airplane); then some new classes (football and phone)\nemerge in two later periods in the data stream. The system must have the ability to detect those\nnew classes and update itself timely in order to maintain the predictive accuracy.\nConceptually, the SENC problem can be decomposed into three sub problems: detecting emerging\nnew classes, classifying known classes, and updating models to enable classiﬁcation of instances\nof the new classes and detection of more emerging new classes. For every test instance in a data\nstream, the detector acts as a ﬁlter to determine whether it is likely to belong to a known class.\n2\nIf it is, the instance is passed on to the classiﬁer to produce a class prediction. Otherwise, the\ninstance is declared a new class and placed in a buﬀer which stores candidates of previously\nunseen class. When the candidates have reached the buﬀer size, they are used to update both the\nclassiﬁer and the detector. The process repeats in the data stream after the models are updated.\nThe overall aim of the task is to maintain high classiﬁcation accuracy continuously in a data\nstream.\nThus, the challenges in the SENC problem are to detect emerging new classes and\nclassify instances of known classes with high accuracy, and to perform model update eﬃciently in\ndata streams. In order to maintain the model complexity to a reasonable size, model components\nrelated to currently inactive classes must be eliminated from the current model.\nWe show that these challenges can be met by using completely random trees, and the proposed\nmethod often achieves signiﬁcantly better outcomes than existing more complicated methods.\nThe proposed method has the following distinguishing features:\n• The proposed method employs an unsupervised learning method as the basis to solve the\nSENC problem, and has a single common core which acts as distinct unsupervised learner\nand supervised learner. In contrast, most existing methods treat this problem as a classiﬁca-\ntion problem and employ a supervised or semi-supervised learning approach [MP03, DYZ14]\nto solve it.\n• The method explicitly diﬀerentiates anomalies of known classes from instances of emerging\nnew classes using an unsupervised learning anomaly detection approach.\n• The model is updated without the initial training set because the proposed method does not\nneed to train new models for every future model updated. In contrast, most existing meth-\nods must keep this training set in order to train new models (e.g., LACU-SVM [DYZ14].)\nNote that most of the existing methods mentioned above are designed to solve part of the SENC\nproblem only. Details are provided in Section 2.\nOur main contribution is the proposal to shift the focus of treating SENC as a classiﬁcation\nproblem to one based on unsupervised anomaly detection problem. In other words, the focus is\nshifted from the second sub problem to the ﬁrst sub problem which is more critical in solving the\n3\nentire problem. This shift brings about an integrated approach to solve all three sub problems\nin SENC. No such solution exists in the current classiﬁcation-focused approaches, as far as we\nknow.\nThe rest of this paper is organized as follows: Section 1 describes the intuition of the proposed\nalgorithm. Section 2 reviews the related work. Section 4 and 5 describe related deﬁnitions and\nthe details of the proposed algorithm. We report the experimental results in Section 6. The\nconclusion is provided in the last section.\n1. The intuition\n1.1. Detecting emerging new classes\nThe intuition is that anomalies of known classes are at the fringes of the data cloud of known\nclasses, and instances of any emerging new classes are far from the known classes. To detect\nemerging new classes, we propose to treat instances of any new class as “outlying” anomalies\nwhich are signiﬁcantly diﬀerent from both instances and anomalies of the known classes.\nThe anomaly detector for the SENC problem must be able to diﬀerentiate between these two\ntypes of anomalies. The assumption is that anomalies of the known classes are more “normal”\nthan the “outlying” anomalies. This is a reasonable assumption in this context because only\ninstances of the known classes are available to train the anomaly detector.\nAn anomaly detector often categorises the feature space into two types of regions: anomaly and\nnormal. Following the above idea, we propose to further subdivide each anomaly region into\ntwo sub regions: “outlying” anomaly sub region and anomaly sub region: (1) The instances in\nanomaly sub region is closer to the region of normal instances than instances from emerging new\nclasses as the anomalies and normal instances are generated from the same distribution. (2)\n“Outlying” anomaly sub region is further away from the normal region and anomaly sub region.\nA test instance is regarded as belonging to an emerging new class if it falls in the “outlying”\nanomaly sub region.\n4\nNormal region \nAnomaly region\n“Outlying” anomaly sub region\nAnomaly sub region\nAnomaly of known class\nKnown classes \nEmerging new class\nFigure 2: An illustration to build an “outlying” anomaly sub region\nFigure 2 illustrates the normal and anomaly regions constructed by an anomaly detector. The\nanomaly region is further partitioned into two sub regions. The sub region outside the anomaly\nsub region is the “outlying” anomaly sub region.\nThe construction of “outlying” anomaly sub regions assumes that anomaly regions can be iden-\ntiﬁed. We show in Section 4.2 that this can be easily achieved using a threshold of the anomaly\nscores provided by an anomaly detector to categorise all regions into two types: anomaly and\nnormal.\n1.2. Classiﬁcation and eﬃcient model update\nIf we treat the second sub problem, i.e., classiﬁcation, as having no relation to the ﬁrst sub\nproblem for detecting emerging new classes, then any classiﬁer can be applied.\nHowever, in\norder to facilitate eﬃcient model update that enables classiﬁcation of newly detected class and\ndetection of more emerging new classes in data streams, we suggest an integrated approach which\nhas a single common core for both the detection and classiﬁcation tasks.\nAn unsupervised learner iForest[LTZ08], which induces completely random trees, has enabled\nus to implement the integrated approach with ease. This is because previous works [FWYM03,\nLTF05] have shown that, ensemble of completely random trees [Zho12, Chap.3.5], as an extreme\ncase of variable-random trees [LTYZ08], can be successfully applied as a powerful classiﬁer. We\nuse exactly the same completely random trees, generated for the purpose of anomaly detection,\nfor classiﬁcation. This can be easily achieved by simply recording the class labels (provided in the\ntraining set) in each leaf. This is the only additional step that needs to be done in the training\nprocess to produce an ensemble of completely random trees that will act as both an unsupervised\n5\nlearner (to detect emerging new classes) and an supervised learner (to classify known classes) in\ndata streams.\nAs the single core for both tasks is completely random trees only, they can be updated easily\nwhen a suﬃcient number of instances of emerging new classes have been detected. The single core\nalso facilitates to maintain the model complexity in a reasonable size by using eﬀective model\nretiring mechanism and growing mechanism in the model update process.\nIn a nutshell, we introduce a simple and unique method to solve the SENC problem and show\nthat the proposed method can detect emerging new classes and classify known classes with high\naccuracy, and perform model update eﬃciently in data streams. Our empirical evaluation shows\nthat it often performs signiﬁcantly better than existing more complex methods.\n2. Related work\nThe SENC problem has the following challenges:\n1. In the extreme case, no true labels except in the initial training set, i.e, true labels are not\navailable after the model deployment.\n2. A prediction must be made immediately for each incoming instance in the stream.\n3. Store no data permanently from the data stream.\n4. Fast model update.\nNote that, as far as we know, there is no an algorithm that using one single core to conquer the\nwhole SENC challenges. We review the related work with respect to these challenges as following.\nClass-incremental learning (C-IL) [ZC02] is a branch of incremental learning which modiﬁes a\npreviously trained classiﬁer to deal with emerging new classes. It has been found to be useful\nin various applications, e.g., detecting bots [CRT11], face recognition [HAY+07] and video con-\ncept detection[YYH07]. C-IL problems includes open set recognition[SdRRSB13], Learning with\nAugmented Class (LAC)[DYZ14]. All of these works are in the batch mode setting. The SENC\nproblem is a C-IL problem in the data stream context.\n6\nIn addition, many existing methods treat the SENC problem as a classiﬁcation problem. This is\nthe reason why they have employed supervised learning or semi-supervised learning approaches.\nMoreover, most of these studies assume that instances of an emerging new class are identiﬁed\nby some other mechanism and focuses on methods to train and incorporate classiﬁers which can\nclassify new classes incrementally with previously trained classiﬁers[DYZ14, KOC13]. As a result,\nno existing methods in C-IL meet the four challenges mentioned above.\nLearning with Augmented Class (LAC) [DYZ14] is a new eﬀort for C-IL and addresses a research\ngap, i.e., to produce a detector for emerging new classes. Utilising unlabelled instances through\nsemi-supervised learning, LACU-SVM [DYZ14] modiﬁes a previously trained classiﬁer to identify\nemerging new classes. Assuming the set of unlabelled instances containing suﬃcient instances of\nan emerging new class, a trained LACU-SVM can then assign a test instance to either one of\nthe known classes or emerging new class. While it solves the ﬁrst and second sub problems, it\nis a batch-mode method that requires to store all training data. Thus, it is not suitable in data\nstreams and does not meet the four challenges.\nThe aim of novel class detection is to identify new data which are not previously seen by a machine\nlearning system during training. This is the ﬁrst sub problem of SENC. An example of this work\nin Bioinformatics [SdC04] employs an one-class SVM approach to detect novel classes.\nIt is\ninteresting to note that this approach does not make a distinction between novel class detection\nand anomaly detection (or outlier detection) [CBK09], which is the identiﬁcation of items, events\nor observations which do not conform to an expected pattern in a data set in batch mode. It\nthus also does not meet the four SENC challenges in data streams.\nThe goal of change point detection is to detect changes in the generating distributions of the time-\nseries. Many works have been conducted to tackle this problem [BN96] which include parametric\nmethods [DDD05] and non-parametric methods [BD93]. This problem is equivalent to the ﬁrst\nsub problem in SENC, without addressing the classiﬁcation and model update issues. Yet, others\nhave focused on classiﬁcation in data streams [BHP+09, JA03, KM07], without addressing the\nemerging new classes problem.\nAnother related work, ECSMiner, [MGK+11] tackles the novel class detection and classiﬁcation\nproblems by introducing time constraints for delayed classiﬁcation. ECSMiner assumes that true\nlabels of new emerging class can be obtained after some time delay; otherwise, models cannot be\n7\nupdated. In contrast, our proposed method assumes that no labels are available for the entire\nduration of a data stream.\nThe SENC problem can be solved by treating the ﬁrst two sub problems independently by using\nexisting methods, i.e., a new class detector and a known classes classiﬁer. To detect emerging new\nclass, existing anomaly detectors (such as LOF [BKNS00], iForest [LTZ08] and one-class SVM\n[MP03]) can be employed; and multi-class SVM [CL11]) can be used as an the classiﬁer for known\nclasses. In addition, existing supervised or semi-supervised batch classiﬁcation methods can be\nadapted to solve the SENC problem, e.g., One-vs-rest SVM [RK04] and LACU-SVM [DYZ14].\nHowever, all these algorithms do not solve the SENC problem satisfactorily. Table 1 summarizes\nthe ability of these algorithms and the proposed SENCForest to meet the four challenges.\nTable 1: Ability of algorithms to meet the challenges of the SENC problem.\nAlgorithm\nChallenge\n1\n2\n3\n4\nLOF+SVM\n×\n✓\n×\n×\n1SVM+SVM\n×\n✓\n×\n×\nOne-vs-rest SVM\n×\n✓\n×\n×\nLACU-SVM\n×\n✓\n×\n×\niForest+SVM\n×\n✓\n×\n✓\nECSMiner\n×\n×\n✓\n✓\nSENCForest\n✓\n✓\n✓\n✓\nDetails about those algorithms implemented and the proposed SENCForest are provided in fol-\nlowing sections.\nSENCForest is the only one which can meet all four challenges. Only ECSMiner, among existing\nalgorithms, can meet Challenge #3. Note that all existing algorithms assume that true labels are\nmade available after the model deployment at some points in time—unable to meet Challenge\n#1.\n8\n3. Terminology Deﬁnition\nBefore introducing the detail of our proposed algorithm, we will give the formal deﬁnitions of\nmany important concepts used in this paper.\nDeﬁnition 3.1 Classiﬁcation under Streaming Emerging new Class (SENC) problem: Given\na training data set D = {(xi, yi)}L\ni=1, where xi ∈Rd is a training instance and yi ∈Y =\n{1, 2, . . . , K} is the associated class label. A streaming data S = {(x′\nt, y′\nt)}∞\nt=1, where x′ ∈Rd,\ny′ ∈Y ′ = {1, 2, . . . , K, K + 1, . . . , M} with M > K. The goal of learning with the SENC problem\nis to learn a model f with D initially; then f is used as a detector for emerging new class and\na classiﬁer for known class. f is updated timely such that it maintains accurate predictions for\nknown and emerging new classes on streaming data S.\nThe SENC problem can have diﬀerent variations. The hardest condition is when true class labels\nare not available throughout the entire process, except that the initial training set of known classes\nis used to train a classiﬁer (and a detector for new classes) at the beginning of the data stream.\nA relaxation of this condition produces easier SENC problems. For example, true class labels\nare available at some intervals in streaming data S. In this paper, we show that the proposed\nmethod can deal with the hardest condition (in Section 5.2) as well as some easier conditions (in\nSection 5.3).\nDeﬁnition 3.2 Scores for test instances: Model f yields a score for a test instance x, which\ndetermines x as belonging to either a known class or an emerging new class (i.e., an “outlying”\nanomaly.)\nDeﬁnition 3.3 Known Class Region and Anomaly Region: Based on the score from f, the\nfeature space is divided into two types of regions : (a) known class regions K which have score\n≥ˆτ, (b) anomaly regions A which have score < ˆτ, where ˆτ is a threshold.\nDeﬁnition 3.4 Anomalies of Known Classes: Let O = {x1, . . . , xn} be the training instances in\nan anomaly region A. The center of O is deﬁned as c = 1\nn\nP\nx∈O x. Let e ∈O be the farthest\ninstance from c. A ball B centered at c with radius r = dist(c, e) is an anomaly sub region.\nInstances which fall into anomaly sub regions are Anomalies of Known Classes.\nDeﬁnition 3.5 Instances of an emerging new class are “outlying” anomalies: Q = A\\B.\n9\n4. The Proposed Algorithm\nIn this section, we propose an eﬃcient algorithm to deal with the SENC problem named SENC-\nForest which is composed of SENCTrees and assigns each instance, as it appears in a data\nstream, a class label: Emerging New Class or one of the known classes. Instead of treating it as\na classiﬁcation problem, we formulate it as a new class detection problem and solve it using an\nunsupervised anomaly detector as the basis to build SENCForest which will ﬁnally act as both\nunsupervised learner and supervised learner.\nWe provide an overview of the procedure in section 4.1. The pertinent details in the procedure\nare then provided in the following three sections.\n4.1. SENCForest: An Overview\nSENCForest has four major steps:\n1. Train a detector for emerging new classes. Given the initial training set of known classes\nD, an unsupervised anomaly detector SENCForest is trained, ignoring the class information, as\nfollows:\n1. Build an iForest [LTZ08].\n2. Determine the path length[LTZ08] threshold ˆτ.\n3. Within each region A, construct ball B which covers all training instances which fall into\nthis region. The area of the ball B is anomaly sub region A. Any test instances which fall\ninto B are regarded as anomalies of known classes; those that fall outside B are regarded\nas instances from an emerging new class.\nThe path length is introduced in iForest[LTZ08], which can be regard as an anomaly score for\ndetermining known class region and anomaly region(like Deﬁnition 3.3). After training the detec-\ntor of SENCForest, model SENCForest can yields a new class score for a test instance x through\naggregating results of each tree in SENCForest. Detail of iForest will be described in the following\nsection.\n10\nAnomaly of known class\nKnown classes \n \nEmerging new class\n \nc1\nr1\nc3\nr2\nc2\nr3\nNormal region \n“Outlying\" anomaly sub region\nAnomaly sub region\nAnomaly sub region\nFigure 3: An illustration to build an “outlying” anomaly sub region\nFigure 3 illustrates the regions constructed by an iTree which has axis-parallel boundaries, and\nthe additional subdivision employs a ball to partition each anomaly region into two sub regions.\nThe anomaly sub region outside the ball is the “outlying” anomaly sub region.\n2. Using known class information to build a classiﬁer from a detector. Once the above\nnew class detector is constructed, class distributions based on known class labels are recorded\nin each K or B region. Each region with class distribution acts as a classiﬁer that outputs the\nmajority class as the classiﬁcation result for a test instance which fall into the region.\nThe training set is discarded once the training process is completed.\n3. Deployment in data stream. SENCForest is now ready to be deployed in a data stream,\nand it is assumed that no true class labels are available for model updated throughout the entire\ndata stream. An instance in the data stream is given a class prediction by SENCForest if it falls\ninto K or B region; otherwise, it is identiﬁed as an instance from an emerging new class and\nplaced in a buﬀer of size s.\n4. Model update. The model update process in SENCForest is simple. It begins when the\nbuﬀer is full. Using instances from the buﬀer, the same tree growing process is then applied to\neach leaf of every existing tree until the stopping criterion is satisﬁed. The rest of the model\nupdate process follows the same steps from 1.2 onwards, as described above.\nNote that the\nupdate largely involves newly grown subtrees, i.e., replacing leaf nodes which have the number\nof instances more than a set limit after taking new instances from the buﬀer into consideration.\nThus, the whole process can be completed quickly. To maintain model size, mechanisms to retire\nSENCForest are also employed in the model update process.\n11\nSection 4.2 describes the pertinent details of training SENCForest as both unsupervised detector\nand supervised learner. Deploying SENCForest and model update in data streams are provided\nin Section 4.3 and Section 4.4, respectively.\n4.2. SENCForest: Training process\nThe training procedure to build an SENCForest with both detection and classiﬁcation functions\nis detailed in Algorithms 1 and 2. These are the combined step to build iForest[LTZ08] and\nto produce a classiﬁer from a detector. The trees are then used to determine the path length\nthreshold and to construct “outlying” anomaly regions described following respectively. Note\nthat the procedure is the same as in building iForest, except in line 2 of Algorithm 2. As the\ntrees constructed are not exactly iTrees, we name the trees with the new classiﬁcation capability,\nSENCTrees.\nBuild an iForest. The unsupervised anomaly detector iForest [LTZ08] is an ensemble of Iso-\nlation Tree (iTrees). “Isolation” is a unique concept in anomaly detection, as each iTree is built\nto isolate every instance from the rest of the instances in the training set. The idea is based on\nthe fact that since anomalies are ‘few’ and ‘diﬀerent’, they are more susceptible to isolation than\nnormal instances. Hence, an anomaly can be isolated using fewer partitions in an iTree than a\nnormal instance.\nLiu et.\nal.\n[LTZ08] show that iTrees can be created using a completely random process to\nachieve the required isolation. Given a random subsample of size ψ, a partition is produced by\nrandomly selecting an attribute and its cut-point between the minimum and maximum values in\nthe subsample. To produce an iTree, the partitioning process is repeated recursively until every\ninstance in the subsample is isolated. An iForest is an ensemble of z iTrees, each generated using\na subsample randomly selected from the given training set.\nIn the testing process, an instance having a short path length, which is the number of edges it\ntraversed from the root node to a leaf node of an iTree, is more like to be an anomaly. The\naverage path length from all iTrees is used as the anomaly score for each test instance.\nFor both instances of emerging new class and anomalies of known classes, iForest will produce\nshort path lengths because they all are individually ‘few’ and ‘diﬀerent’ from the known classes.\n12\nIn order words, they are all in the regions with short path length in iTrees. We called this type\nof region, anomaly region A to diﬀerentiate them from normal region K which have long path\nlength.\nIn order to detect emerging new class, we ﬁrst need to determine a path length threshold to\ndiﬀerentiate A from K. Then, build a sub region B in each A region which covers all training\ninstances in the region. As these instances are from known classes, they are anomalies of known\nclasses. These two processes are described in the following paragraph.\nDetermine the path length threshold. As each region in iTree has its own path length, and\nanomaly regions A are expected to have shorter path length than that from normal regions K,\nwe employ the following method to determine the path length threshold to separate these two\ntypes of regions.\nWe produce a list L which orders all path lengths representing all regions in an iTree in ascending\norder. A threshold τ in this list yields two sub-lists Ll and Lr. To ﬁnd the best threshold, we\nuse the following criterion which minimises the diﬀerence in standard deviations σ(.):\nˆτ = arg min\nτ\n|σ(Lr) −σ(Ll)|\nThe threshold ˆτ is used to diﬀerentiate anomaly regions A from normal regions K, where the\nformer has low path length and the latter has long path length.\nUsing a tree, Figure 4 shows an example of cumulative distribution for list L and its SDdiﬀ\n(= |σ(Lr) −σ(Ll)|) curve. Note that the minimum SDdiﬀpoint separates into two clear regions:\nanomaly and normal regions.\nNote that (i) because threshold ˆτ is determined automatically, no additional parameter is intro-\nduced; and (ii) this process does not require training data.\nConstruct “outlying” anomaly sub regions. After ˆτ is determined, a ball B is constructed\nusing all training instances in every region A of a tree, according to Deﬁnitions 3.4 and 3.5.\nWhen balls B have been built for all A regions in every SENCTree, the SENCForest has the ﬁrst\nfunction as an unsupervised detector and is ready to detect instances of emerging new classes.\n13\n4\n8\n12\n16\n0\n1\n2\n3\n0\n10\n20\n30\n40\nSDdiff\nCumulative frequency \nSDdiff\nNumber of leaf nodes\nOrdered path length\nFigure 4: Determining path length threshold: Cumulative frequency and the corresponding SDdiﬀcurve, where\nthe x-axis is the ordered path lengths from all regions in an iTree. The point which yields the minimum SDdiﬀis\nchosen as the threshold to diﬀerentiate anomaly regions A from normal regions K.\nA test instance which falls into A but outside B is an “outlying” anomaly, i.e., an instance of an\nemerging new class.\nProduce a classiﬁer from a detector To incorporate the second function of being a classiﬁer\ninto SENCForest, all we have to do is to record class distribution F[j] in each region from K and\nB using the training subsample, where F[j] denotes the number of class j instances in a region.\nNote that this is the only step class labels are required.\nOnce the above training steps are completed, SENCForest is ready to be deployed to a data\nstream.\nAlgorithm 1 Build SENCForest\nInput:\nD - input data, z - number of trees, ψ - subsample size.\nOutput:\nSENCForest\n1: initialize: SENCForest ←{}\n2: for i = 1, . . . , z do\n3:\nXi ←sample(D, ψ)\n4:\nSENCForest ←SENCForest ∪SENCTree(Xi)\n5: end for\n4.3. Deployment in data stream\nGiven a test instance x, SENCForest(x) produces a class label y ∈{b1, . . . , bm, NewClass}, where\nm is the number of known classes thus far and NewClass is the label given for an emerging new\n14\nAlgorithm 2 SENCTree\nInput:\nX - input data, MinSize - minimum internal node size\nOutput:\nSENCTree\n1: if |X| < MinSize then\n2:\nreturn LeafNode{|X|, F[·], c, r}, as deﬁned in Section 4.2.\n3: else\n4:\nlet Q be a list of attributes in X\n5:\nrandomly select an attribute q ∈Q\n6:\nrandomly select a split point p from max and min values of attribute q in X\n7:\nXL ←filter(X, q ≤p)\n8:\nXR ←filter(X, q > p)\n9:\nreturn inNode{Left ←SENCTree(XL),\n10:\nRight ←SENCTree(XR),\n11:\nSplittAtt ←q,\n12:\nSplittValue ←p },\n13: end if\nclass. Note that though SENCForest can detect instances of any number of emerging new classes,\nthey are grouped into one new class for the purpose of model update. We will focus on model\nupdate on one new class in one period (but multiple new classes could emerge in diﬀerent periods\nof a data stream) for the rest of the paper. We discuss the issue of model update for multiple\nnew classes in Section 5.4.\nAlgorithm 3 describes the testing process during the deployment of SENCForest in a data stream.\nIn line 3 of Algorithm 3, SENCForest(x) outputs the majority class among all classes produced\nfrom z trees. A tree outputs NewClass if test instance x falls into an A region but outside the\nB region; otherwise, it outputs the majority of class from\narg max\nj∈{b1,...,bm}\nF[j]\nwhere F[j] is the class frequency for class j recorded in the region (K or B) into which x falls.\nIf SENCForest(x) outputs NewClass, x is placed in buﬀer B which stores the candidates of the\npreviously unseen class (line 5). When the number of candidates has reached the buﬀer size, the\n15\ncandidates are used to update both the classiﬁer and the detector (line 7). Once these updates\nare completed, the buﬀer is reset and the new model is ready for the next test instance in the\ndata stream.\nAlgorithm 3 Deploying SENCForest in data stream\nInput:\nSENCForest, B - buﬀer of size s\nOutput:\ny - class label for each x in a data stream\n1: while not end of data stream do\n2:\nfor each x do\n3:\ny ←SENCForest(x)\n4:\nif y = NewClass then\n5:\nB ←B ∪{x}\n6:\nif |B| ≥s then\n7:\nUpdate (SENCForest, B)\n8:\nB ←NULL\n9:\nm ←m + 1\n10:\nend if\n11:\nend if\n12:\nOutput y ∈{b1, . . . , bm, NewClass}.\n13:\nend for\n14: end while\n4.4. Model Update\n4.4.1. Growing Mechanism\nThere are two growing mechanisms: one for growing a subtree in an SENCTree, and the other\nfor the growing multiple SENCForests.\nGrowing a subtree in an SENCTree. Updating SENCForest with buﬀer B is a simple process\nof updating each leaf node in every tree using ψ instances, randomly selected from B. This is\ndepicted in Algorithm 4. The update at each node (line 10) involves either a replacement with a\nnewly grown subtree or a simple update of the class frequency to include the new class bm+1.\n16\nAlgorithm 4 Update SENCForest\nInput: SENCForest - existing model, B - input data\nOutput: a new model of SENCForest\n1: initialize: All instances in B are assigned a new class bm+1\n2: for i = 1, ..., z do\n3:\nB′ ←sample(B, ψ)\n4:\nTree ←SENCForest.Tree[i]\n5:\nfor j = 1, ...,Tree.LeafNodeNumber do\n6:\nX′ ←instances of B′ which fall into Tree.LeafNodej\n7:\nif |X′| > 0 then\n8:\nX ←Pseudo instances from Tree.LeafNodej\n9:\nX′ ←X′ ∪X\n10:\nTree.LeafNodej ←SENCTree(X′)\n11:\nend if\n12:\nend for\n13:\nrecalculate ˆτ for Tree\n14:\nSENCForest.Tree[i] ←Tree\n15: end for\n17\nNew SENCTree\n \nSENCTree\n \nEmerging new class instance\n \nPseudo instance\n \nSubTree\n \n Old SENCTree\n \n1 \n1 \nTraining a SubTree with emerging new \nclass instances and pseudo instances \nFigure 5: Replacing a leaf node with a trained subtree\nIf there are some instances which fall into a leaf node, a subtree needs to be grown as follows.\nAs the previous training set is not stored, pseudo instances are generated for the leaf node which\nhave the same attribute-values as centre c. The number of pseudo instances for each class j is as\nrecorded in F[j]. The combined set of pseudo instances X and X′ (i.e., the subset of B′ which\nfalls into the same leaf node) is used as input to SENCTree (line 10). An example procedure is\ndepicted in Figure 5. In the top left ﬁgure, we assume that some emerging new class instances\n(green triangle) fell into node 1 (there are three instances fell into in training process) in an\nSENCTree. Then the combined set consists of pseudo instances and instances of the emerging\nnew class. A new subTree is built by using the combined set. Finally, in the bottom left ﬁgure,\nnode 1 is replaced with this new subTree. Every leaf node goes through the same process.\nNote that the update process retains the original tree structure, and all pseudo instances in a leaf\nnode will still be placed into a single leaf node of the newly grown subtree. Thus, the predictions\nfor the known classes are not altered in the model update process.\nOnce each tree has completed the model update, ˆτ is recalculated as described in Section 4.2.\nGrowing multiple SENCForests. When the number of classes in a SENCForest reaches ρ, its\nSENCTrees will stop growing for any emerging new class. A new SENCForest is grown instead\nfor the next ρ emerging new classes. This user-deﬁned parameter is set based on the memory\nspace available.\n18\n4.4.2. Prediction using Multiple SENCForests\nIn a model with multiple SENCForests, the ﬁnal prediction is resolved as follows. For a given x,\nSENCForest i yields prediction yi and probability\npi = Number of SENCTrees predicting yi\nTotal number of SENCTrees\nThe ﬁnal prediction is NewClass only if all SENCForests predict x as belonging to NewClass.\nOtherwise, the ﬁnal prediction is the known class which has the highest pi. This procedure is\ngiven in Algorithm 5.\nAlgorithm 5 Final Prediction from E SENCForests\nInput:\nx - an instance in the data stream\nOutput:\nyı - class label for x\n1: for i = 1, ..., E do\n2:\n⟨yi, pi⟩←SENCForesti(x)\n3: end for\n4: if ∀i yi = NewClass then\n5:\nı ←1\n6: else\n7:\nL ←{i ∈{1, . . . , E} | yi ̸= NewClass}\n8:\nı ←arg maxi∈L pi\n9: end if\n10: Output yı\n4.4.3. Retiring Mechanism\nA mechanism to retire SENCForest is required as the data stream progresses. A SENCForest is\nretired under the following scenarios:\n1. When a SENCForest is not used for predicting known classes for a certain period of time, it\nis eliminated for any future predictions. In other words, a SENCForest outputs “NewClass”\nfor a long time, this SENCForest will be retired\n19\n9+4),UXKYZ\n(a) Classiﬁcation result in a data stream\n0\n(b) Emerging new class detection result of iForest+SVM\n9+4),UXKYZ\n0\n(c) Emerging new class detection result of ENCiFer\nFigure 6: An example data stream on the KDDCUP 99 data set. The x-axis is the time steps in the data stream.\nThe known classes at each duration (ti - ti+1) are denoted as b1,b2,b3, and b4. The details of the two methods,\niForest+SVM and None+SVM, are described in Table 2.\n2. In the event that the number of SENCForests has reached the preset limit ρ and no SENC-\nForest can be retired based on (1), then the least used SENCForest in the last period is\nchosen to retire.\nThe number of known class predictions is recorded for each SENCForest in data stream. The\none which has made the minimum number of predictions for known classes is identiﬁed to be the\nleast used SENCForest.\n5. Experiment\nThis section reports the empirical evaluation we have conducted to assess the performance of\nSENCForest in comparison with several state-of-the-art methods.\n5.1. Experimental Setup\nData Stream: To simulate emerging new classes in a data stream, we assume that an initial\ntraining set with two known classes are available to train the initial models. When the trained\n20\n9+4),UXKYZ\n(a) Classiﬁcation result in a data stream\n0\n(b) Emerging new class detection result of iForest+SVM\n9+4),UXKYZ\n0\n(c) Emerging new class detection result of ENCiFer\nFigure 7: An example data stream on the MNIST data set.\nmodels are deployed at the beginning of a data stream, instances of the two known classes and\nan emerging new class appear in the ﬁrst period of the data stream with uniform distribution. It\nis assumed that the method employed will update its models sometime within the ﬁrst period.\nIn the second period, instances of the three classes seen in the ﬁrst period and another emerging\nnew class appear with uniform distribution. Instances appear one at a time, and the deployed\nmethod is expected to make a prediction for each instance before processing the next, i.e., each\ninstance is predicted as belonging to either an emerging new class or one of the known classes\nthus far.\nNo true class labels for all instances are available throughout the entire data stream.1 Model\nupdate is based on the instances of the emerging new class identiﬁed at the time the model update\nis triggered.\nFigures 6 and 7 show example data streams using the KDDCUP 99 data set and the MNIST\ndata set. The class composition in the two distinct periods in the data stream are described as\nfollows:\n1This is a more stringent condition than previous studies (e.g., [MGK+11]) which assume that true labels are\navailable for model update, after some time delay.\n21\nKnown classes\nNew class\nFirst period: t0 - t2\nt0 - t1\nb1,b2\n✓\nt1 - t2\nb1,b2,b3\n×\nSecond period: t2 - t4\nt2 - t3\nb1,b2,b3\n✓\nt3 - t4\nb1,b2,b3,b4\n×\n12\n15\n21\n20\n62\n15\n88\n8\nt\n1\n Average number of leaves\n Average number of leaves in anomaly regions\n Average path length threshold\nt\n0\nt\n3\n46\n(a)\n10\n13\n14\n21\n101\n18\n121\n12\nt\n1\nt\n0\n Average number of leaves\n Average number of leaves in anomaly regions\n Average path length threshold \nt\n3\n86\n(b)\nFigure 8: Information of the evolving SENCForest at three diﬀerent times in the data stream on (a) KDDCUP 99\ndata set; (b) MNIST data set.\nIn the ﬁrst period, all instances of the emerging new class identiﬁed by a method is placed in a\nbuﬀer B of size s. When the buﬀer is full (marked as t1), the method updates its model before\nprocessing the next instance. Note that t1 diﬀers for diﬀerent methods as their detection rates for\nthe new class are diﬀerent, as shown in Figures 6(b) and 6(c) for iForest+SVM and SENCForest\n(so as in Figures 7(b) and 7(c).) The buﬀer is reset to be empty when the model of a method has\nbeen updated. Note that after the model is updated, the new class in t0 - t1 becomes a known\nclass b3 of the updated model in t1 - t2, as shown in the table above.\nSimilarly, in the second period between t2 and t4, t3 is the time when the buﬀer is full and the\nmodel of a method is updated for the second time. The new class in t2 - t3 becomes a known\nclass b4 of the updated model in t3 - t4.\nFigure 8 shows the information of the evolving SENCForest at three diﬀerent times in the data\nstream on two data sets.\nEvaluation measures: To evaluate the predictive accuracy of algorithms in the SENC problem,\nwe introduce EN Accuracy in a ﬁxed window size. Let N be the total number of instances in a\nwindow; An be the total number of emerging class instances identiﬁed correctly; and Ao be the\n22\nTable 2: Methods used in the empirical evaluation. D is the training set for the current models; B is the set of\nnew class instances in the buﬀer and model update is triggered when the buﬀer is full. After each model update,\nD ←D ∪B; and D needs to be stored for the next model update for all methods, except SENCForest and\nNone+SVM. U is an additional set of unlabelled instances used by LACU-SVM only. In the experiments, the data\nsize of U is the total data size of D and B.\nMethod\nDetection\nClassiﬁcation\nModel Update\nLOF+SVM\nLOF\nmulti-class SVM\ntrain new LOF and SVM with D ∪B\n1SVM+SVM\none-class SVM\nmulti-class SVM\ntrain new 1SVM and SVM with D ∪B\n1R-SVM\nOne-vs-rest SVM\ntrain new 1R-SVM with D ∪B\nLACU-SVM\nLACU-SVM\ntrain new LACU-SVM with D ∪B and U\nECSMiner\nECSMiner\ntrain a new classiﬁer in each ﬁxed interval, assuming true labels are given\niForest+SVM\niForest\nmulti-class SVM\ntrain new iForest and SVM with D ∪B\nSENCForest+SVM\nSENCForest\nmulti-class SVM\nUpdate SENCForest with B and train new SVM with D ∪B\nNone+SVM\nNo detector\nmulti-class SVM\nno model update\nSENCForest\nSENCForest\nUpdate SENCForest with B\ntotal number of known class instances classiﬁed correctly,\nEN Accuracy = An + Ao\nN\nFigures 6(a) and 7(a) show examples of EN Accuracy results of three methods in a data stream.\nTo evaluate the accuracy of new class detection, we compute F-measure in t0 - t1 and t2 - t3 to\nmeasure the detection performance in these two durations. This measure produces a combined\neﬀect of precision (P) and recall (R) of the detection performance. F-measure = 1 if a detector\nidentiﬁes all instances of emerging new class with no false positives.\nF-measure = 2 ∗P ∗R\nP + R\nThe cumulative numbers of instances of the true and predicted new class are also plotted in four\nconsecutive durations. In t1-t4, it shows that both methods make some false positives resulting\nin more instances predicted as belonging to the new class than it actually has. The F-measures\nachieved by each detection method in t0-t1 and t2-t3 are shown in Figures 6(b) & 6(c) and Figures\n7(b) & 7(c). In this example, SENCForest performs better than iForest+SVM because it has\nbetter F-measure, fewer false positives and higher EN Accuracy.\nIn the experiments reported in Section 5.2, the diﬀerence in performance between two methods\nis considered to be signiﬁcance on paired t-tests at 95% signiﬁcance level in our paper\n23\nContenders: The complete list of the methods used for new class detection, classiﬁcation and\nmodel update methods is shown in Table 2. As some of these methods can act as a new class\ndetector only, a state-of-the-art classiﬁer, i.e., multi-class SVM [CL11], is employed to classify\ninstances of known classes. Note that three types of information, additional to that was provided\nto SENCForest, are required for other methods. First, true labels must be provided at each model\nupdate. Otherwise, no models could be updated. ECSMiner assumes that true labels are given\nat the end of a ﬁxed interval (Tl) in order to update model. Other existing methods requires\nall instances in B must be given the true labels. Second, LACU-SVM needs to have additional\nunlabelled data before training at each model update. Third, the initial training set must be\nstored and incorporated at each model update. SENCForest is the only method which does not\nrequire (i) true labels during the entire data stream after training, (ii) to store the initial training\nset, and (iii) unlabelled training set.\nA brief description of each of the methods used in the experiment is given as follows:\n1. LOF or Local Outlier Factor [BKNS00] is a density-based anomaly detector which employs\nk-nearest neighbour procedure to estimate density.\n2. One-class SVM [SPST+01] is a state-of-the-art outlier detector [MP03] which learns from\nnormal instances only. It computes a binary function to capture regions in input space where\nthe probability density lives.\n3. One-vs-rest SVM is a scheme for multi-class classiﬁcation [RK04] where a two-class SVM\nfk(·) is built for each class. In the original One-vs-rest SVM, a test instance x is predicted\nas belonging to class k if fk(·) produces the highest conﬁdence. To adapt One-vs-rest SVM\nto predict the emerging new class, the classiﬁer produces a classiﬁcation prediction only if\nmaxk fk(x) > 0; otherwise x is predicted as belonging to the emerging new class.\n4. LACU-SVM [DYZ14] is a semi-supervised learner which modiﬁes a previously trained\nmodel by considering the structure presented in the unlabelled data so that the misclassiﬁ-\ncation risks among the known classes as well as between the new and the known classes are\nminimized simultaneously. It produces a classiﬁer which predicts one of the known classes\nor the new class. This method also trains k binary classiﬁers fk(·) for each known class. Like\n24\nOne-vs-rest SVM, LACU-SVM makes a prediction for the known class if maxk fk(x) > 0;\notherwise x is predicted as belonging to the emerging new class.\n5. ECSMiner [MGK+11] is an algorithm for novel class detection and classiﬁcation. It em-\nploys the clusters identiﬁed by k-means to detect novel classes: instances which are not\nwithin the boundaries of any clusters are treated as novel class candidates and placed in a\nbuﬀer, then a new measure is deﬁned to decide whether they are emerging new classes. K\nnearest neighbor is used as the classiﬁer to make predictions for instances of known classes.\nModel update can only occurs if true labels are available within some ﬁxed duration.\n6. iForest [LTZ08] is an unsupervised anomaly detector which builds a model to isolate each\ntraining instance from the rest of the training set.\nIn the experiments, all methods were executed in the MATLAB environment. The following\nimplementations are used: SVM in the LIBSVM package [CL11]; LACU-SVM and iForest were\nthe codes as released by the corresponding authors; and LOF is in the outlier detection toolbox.2\nThe ECSMiner code is completed based on the authors’ paper [MGK+11]. We set the max size\nof each tree to 300, which avoids to the worst case that growing inﬁnitely by random partition.\nThe parameter settings used for these algorithms are provided in Table 5 in Appendix A.\nData sets: Five data sets are used to assess the performance of all methods , including Synthetic,\nKDDCup 993, Forest Cover4, MHAR and MNIST5. For KDDCup 99 data set, we use the four\nlargest classes, i.e., normal, neptune, smurf and back.\nFor Forest Cover data set, we use 10\nattributes, and all binary attributes are removed. A description for Synthetic and MHAR data\nsets are provided in Appendix B. A summary of the data characteristics is provided in Table 3.\nSimulation: In the following experiment, each data set is used to simulate a data stream over\nten trials. In each trial, the initial training set has two classes, and the emerging new class in\neach period is a class diﬀerent from the known classes. These classes are randomly selected from\nthe available classes. The instances in the initial training set and the data sequence in the data\n2https://goker.wordpress.com/2011/12/30/outlier-detection-toolbox-in-matlab/\n3http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n4https://kdd.ics.uci.edu/databases/covertype/covertype.data.html\n5http://cis.jhu.edu/ sachin/digit/digit.html\n25\nTable 3: A summary of data sets used in the experiments.\nData set\n#classes\n#attributes\nSynthetic\n4\n2\nKDDCup 99\n4\n41\nForest Cover\n7\n10\nMHAR\n6\n561\nMNIST\n10\n784\nstream are randomly selected from the given data set, but following uniform class distribution.\nFor all real-world data sets, the data size of the initial training set D is 500 per class; the buﬀer\nsize |B| = 250; and the total number of instances which have appeared in the data stream at the\nend of the ﬁrst period at t2 is 1000; and the second period (t2 - t4) has a total of 1500 instances.\nAs we can aﬀord to generate more data in the synthetic data set, D, B, and the data size at each\nperiod are double to examine the eﬀect of larger data sizes. The average result of ten trials is\nreported.\nThe following sections will give related evaluation results. Section 5.2 describes the empirical\nevaluation under the condition that no true labels are available after the data stream has started.\nSection 5.3 reports results under the long streams situation. Section 5.4 describes using SENC-\nForest under the condition that emerging multiple new classes in a period.\n5.2. Empirical results\nThe results for the ﬁve data sets are shown in Figure 9.\nIn terms of new class detection, SENCForest produced the highest F-measure in all data sets.\nRecall that SENCForest+SVM uses SENCForest only for new class detection; thus both SENC-\nForest and SENCForest+SVM have the same F-measure performance.\nThe closest contenders are LACU-SVM and 1R-SVM, each had the second or third highest F-\nmeasure in three data sets. SENCForest was signiﬁcantly better than all contenders, except in\nMNIST (wrt LACU-SVM) and Forest Cover (wrt ECSMiner).\n26\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nF-measure\n \n LOF+SVM\n 1SVM+SVM\n 1R-SVM\n LACU-SVM\n iForest+SVM\n ECSMiner\n SENCForest+SVM\n None+SVM\n SENCForest\nEN_Accuracy\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n \n(a) Synthetic\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nF-measure\n \n LOF+SVM\n 1SVM+SVM\n 1R-SVM\n LACU-SVM\n iForest+SVM\n ECSMiner\n SENCForest+SVM\n None+SVM\n SENCForest\nEN_Accuracy\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n \n(b) KDD Cup 99\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nF-measure\n \n LOF+SVM\n 1SVM+SVM\n 1R-SVM\n LACU-SVM\n iForest+SVM\n ECSMiner\n SENCForest+SVM\n None+SVM\n SENCForest\nEN_Accuracy\n0.4\n0.5\n0.6\n0.7\n0.8\n \n(c) Forest Cover\n0.4\n0.5\n0.6\n0.7\n0.8\nF-measure\n \n LOF+SVM\n 1SVM+SVM\n 1R-SVM\n LACU-SVM\n iForest+SVM\n ECSMiner\n SENCForest+SVM\n None+SVM\n SENCForest\nEN_Accuracy\n0.4\n0.5\n0.6\n0.7\n0.8\n \n(d) MHAR\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nF-measure\n \n LOF+SVM\n 1SVM+SVM\n 1R-SVM\n LACU-SVM\n iForest+SVM\n ECSMiner\n SENCForest+SVM\n None+SVM\n SENCForest\nEN_Accuracy\n0.4\n0.5\n0.6\n0.7\n0.8\n \n(e) MNIST\nFigure 9: Average result over ten trials. In each trial, the following is computed: Average F-measure in t0 - t1 and\nt2 - t3; and average accuracy over the entire duration from t0 to t4. Two standard errors over ten trials are shown\nas the error bar. Note that SENCForest and SENCForest+SVM are using the same detector to detect emerging\nnew class; thus they have the equivalent F-measure result.\nIn terms of EN Accuracy, SENCForest and SENCForest+SVM produced the highest performance\nin all data sets. This result shows that (i) the accurate detection of emerging new class leads\ndirectly to high classiﬁcation accuracy; and (ii) SENCForest as a classiﬁer is competitive to SVM.\nLACU-SVM was the closest contender which had the second highest accuracy in three data sets.\nBeside SENCForest+SVM, SENCForest performed signiﬁcantly better than the other contenders\nin three data sets. The two exceptions are wrt to LACU-SVM (in MNIST and Synthetic) and\n1R-SVM (in Synthetic).\nAn analysis is provided below:\n• LOF and one-class SVM: the poor detection performance of these two methods wrt to\n27\niForest is likely to be due to the parameter search, i.e., a search for a wider range of values\nmay improve their performance.\nHowever, such search is a computationally expensive\nprocess, and this makes them unsuitable for data stream applications.\n• iForest performed worse than SENCForest in all data sets, and the diﬀerences were sig-\nniﬁcance in four data sets.\nThis shows that an unsupervised anomaly detector can be\nsuccessfully used in the SENC problem if anomaly regions are reshaped (as described in\nSections 4.2) to detect emerging new classes.\n• While One-vs-rest SVM performed reasonably well in classiﬁcation, it is not a good choice\nfor detection of emerging new classes, in comparison with SENCForest.\n• LACU-SVM is the only method which requires additional unlabelled instances in training\nthe initial model and in every model update. While obtaining unlabelled instances may not\nbe a problem in real applications, it is important to note that its detection performance is\nhighly depended on the existence of a new class in the set of unlabelled instances. Insuﬃcient\ninstances of the new class will severely limit LACU-SVM’s ability to detect the new class.\nIn the experiment, LACU-SVM was provided a set of unlabelled instances in t0, t1 and\nt3, in addition to those instances in the initial training set and the buﬀer, in order to\nupdate its model. This additional data set was not available to all other methods. Despite\nthis additional training information, LACU-SVM still performed signiﬁcantly worse than\nSENCForest in four data sets in terms of F-measure.\n• ECSMiner is the only algorithm which was provided with true labels in order to train a\nnew classiﬁer in each ﬁxed interval, which occurs more often than at each model update,\nover the entire data stream. Despite this advantage, it still performed signiﬁcantly worse\nthan SENCForest in four out of ﬁve data sets in both measures.6\n• The result of None+SVM clearly shows that not using a detector is not an option in the\nSENC problem.\n• SENCForest is the best choice detector and a competitive classiﬁer in the SENC problem.\n6ECSMiner [MGK+11] had employed the KDD CUP 99 and Forest Cover datasets in their evaluation. Our\nECSMiner results are compatible with theirs in these two datasets. However, ECSMiner performed poorly in the\nother three datasets.\n28\nWhile it is possible that a more sophisticated classiﬁer may yield a higher accuracy in\nclassifying known classes, it often comes at a high computational cost in an extensive\nparameter search.\n• While using SVM, in addition to SENCForest, could potentially produce a better accuracy\nthan that from SENCForest alone, this comes with a computational cost which is usu-\nally too expensive in the data streams context. Note that to achieve the performance of\nSENCForest+SVM presented in Figure 9, it needs to store all instances thus far, which is\nimpossible in data streams. In contrast, SENCForest achieves comparable result as SENC-\nForest+SVM without the need to store any data.\n5.3. SENCForest in long data streams\nThe aims of this section are to examine the ability of SENCForest to (i) maintain good perfor-\nmance using limited memory in long data streams; and (ii) make use of true class labels when\nthey are available.\nWe simulate a long data stream using the MNIST data set. This stream has twelve emerging\nnew classes7. The initial training data set has 2 classes, and every subsequent period has 1000\ninstances from one emerging new class and two known classes. The maximum number of classes\nwhich can be handled by each SENCForest is set to 3. Other settings are the same as used in the\nlast section. In addition, true class labels are assumed to be available in Q percentage of instances\nin the buﬀer before a model update. SENCForest with Q = 0%, 50% and 100% are compared\nwith LACU-SVM in the experiment. Recall that, as in the previous experiment, LACU-SVM is\ngiven 100% true labels at each model update and an additional set of unlabelled instances; and\nECSMiner is also provided with 100% true labels at each model update.\nFigure 11 shows the average number of leaves of each SENCForest at the start of each time\nperiod. Note that a new SENCForest was produced at periods 2, 5, 8, 11, and the ﬁrst two\nSENCForests, A and B, were retired at periods 8 and 11, respectively. Table 4 shows the further\n7Classes are reused in the simulation when they are no more in use in the current period. Because this simulation\nneeds a number of classes, that is why only the MNIST dataset, out of the ﬁve datasets, can be used in the long\nstream simulation.\n29\n0\n2k\n4k\n6k\n8k\n10k\n12k\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEN_Accuracy\nStream\n SENCForests with Q=0%\n SENCForests with Q=100%\n SENCForests with Q=50% \n None+SVM\n LACU-SVM\n ECSMiner\n(a)\n0\n2\n4\n6\n8\n10\n0\n2k\n4k\n6k\n8k\n10k\n12k\nTime(s)\n SENCForests\n ECSMiner\n LACU-SVM\nStream\n(b)\nFigure 10: (a) Result of long data stream in the MNIST data set; the bar chart on the bottom shows SENCForests\nwith Q = 0%, the number of SENCForests and the number of retired SENCForests at each period. (b) The time\nspent (in seconds) to do model update in each period.\ninformation about SENCForests(Q = 0%) at the start of each time period. The ﬁrst three rows\nprovide the overall information; and the last three lines show the detailed information of the only\nevolving SENCForest at each time period, e.g., periods 2, 3, 4 for SENCForestB, periods 5, 6, 7\nfor SENCForestC and so on. Note that the number of leaves in anomaly regions may decrease as\nSENCForest grows. This happens when instances of new classes fall into few leaves only.\nThe number of SENCForests is maintained at a preset memory limit through retiring not-in-use\nSENCForests. Note that the model size is constrained within the set limit of three SENCForests\nwhich allows the proposed method to deal with inﬁnite data streams. In contrast, LACU-SVM\ncontinues to demand larger and larger memory size to accommodate larger training set size as\nthe stream progresses.\nThe result in Figure 10 (a) shows that SENCForest with Q = 0% maintains good predictive\naccuracy over the long stream. SENCForest is able to make use of true class labels to improve\nits performance along the stream. The extent of the improvement increases as Q increases. In\ncontrast, the predictive accuracy of ECSMiner and LACU-SVM continued to decrease as the\nstream progressed.\nAs a result, as shown in Figure 10(b), its training time continued to grow as the stream progressed.\nECSMiner has the least model update time, because k-nearest neighbor is as base learner, which\nonly spends time in building the clusters in buﬀer. But, that means it needs to save the cluster\nsummary of each cluster into memory.\n30\n0\n2\n4\n6\n8\n10\n12\n40\n80\n120\nSENCForest\nA\n Retired\nAverage number of leaves\nPeriod\n SENCForest\nA\n \n SENCForest\nB\n \n SENCForest\nC\n SENCForest\nD\n \n SENCForest\nE\n \nSENCForest\nB\n Retired\nFigure 11: Average number of leaves of each evolving SENCForest at the start of each time period.\nTable 4: Information of each evolving SENCForest (marked with ∗) at the start of each time period on the simulated\ndata stream using MNIST. Note that only the latest SENCForest at any time period is evolving or growing; and\nall earlier built SENCForests (if any) have stopped growing. The subscript indicates the latest SENCForest shown\nin Figure 11.\nPeriod\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nNumber of known classes\n3\n4\n5\n6\n7\n8\n9\n7\n8\n9\n7\n8\nNumber of SENCForests\n1\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n3\nNumber of retired SENCForests\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\nAverage number of leaves∗\n114A\n32B\n68B\n104B\n32C\n69C\n106C\n44D\n70D\n105D\n38E\n63E\nAverage number of leaves in anomaly regions∗\n14A\n11B\n15B\n12B\n9C\n12C\n13C\n14D\n13D\n14D\n16E\n15E\nAverage path length threshold∗\n16A\n8B\n16B\n17B\n10C\n17C\n18C\n11D\n17D\n20D\n11E\n15E\n5.4. Multiple new classes in a period\nThe emergence of multiple new classes in a period is a challenge in the SENC problem. Although\nSENCForest is designed to deal with one emerging new class in each period, it can still perform\nwell by treating these emerging classes in a period as a single new class. Figure 12 shows that\nSENCForest performs as well when every period has two emerging new classes. In this stream,\nthere are three periods; each period has 2000 instances and 4 classes (i.e., two emerging new\nclasses and two known classes).\nIn the event that it is important to identify each class in each period, a clustering algorithm[Agg13]\ncan be used to achieve this aim before proceeding to do the model update.\n6. Conclusions and future work\nThis paper contributes to decompose the SENC problem into three sub problems and posits that\nthe ability to tackle the ﬁrst sub problem of detecting emerging new classes eﬀectively is crucial\n31\n0\n2k\n4k\n6k\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n SENCForest\n LACU-SVM\n None+SVM\nEN_Accuracy\nStream\nFigure 12: Result of two emerging new classes in each period.\nfor the whole problem. The diﬃculty of the SENC problem is highlighted by the inability of\nexisting methods to solve it satisfactorily.\nWe show that the unsupervised-anomaly-detection-focused approach, coupled with an integrated\nmethod using completely random trees, provides a complete solution for the entire SENC prob-\nlem. The current classiﬁcation-focused approach has failed to provide one thus far.\nThe strength of SENCForest is its ability to detect new class with high accuracy. The use of\nan unsupervised anomaly detector, incorporated with the new ability to diﬀerentiate between\nanomalies of known classes and instances of new classes, underlines the source of the strength.\nExisting supervised and semi-supervised methods are unable to achieve the same level of detection\naccuracy because the focus was on the second sub problem: classiﬁcation, rather than the ﬁrst\nsub problem: emerging new class detection.\nThe fact that the unsupervised learner consists of completely random trees facilitate the use of a\ncommon core which can be converted to an eﬀective classiﬁer with ease. The common core also\nmakes model updates in data streams to be a simple model adjustment, rather than training a\ncompletely new model as in most existing methods. Like in previous work, we show that the\ncompletely random trees are a classiﬁer competitive to state-of-the-art classiﬁers, especially in\nthe data stream context which demands fast model update and classiﬁcation time.\nOur empirical evaluation shows that SENCForest outperforms eight existing methods, despite\nthe fact that it was not given the true class labels in the entire data stream; and other methods\nwere given the true class labels at each model update. In addition, it works eﬀectively in long\nstream with emerging new classes under the limited memory environment. No existing methods\nhave the capability to work under the same condition, as far as we know.\n32\nIn the future, we plan to improve the proposed method to deal with concept drift and to diﬀer-\nentiate two or more emerging new classes before model updates. From a broader perspective,\nthe proposed method is the ﬁrst implementation of the unsupervised-anomaly-detection-focused\napproach to the SENC problem. We intend to explore other implementations of the same ap-\nproach.\n7. appendices\n7.1. Parameter settings\nThe parameter settings of all algorithms used in the experiments are provided in Table 5. A\n10-fold cross-validation on the training set is used in the parameter search to determine the ﬁnal\nsettings for all SVM algorithms.\nThe parameter search for LOF is as described in [DYZ14].\nECSMiner employs K-means and K is set to 5 in the experiment.\nTable 5: The settings used in the experiments.\nMethod\nParameter setting & search range\nLOF\nk = [3, 9]\n1SVM\nc = 0.1 ∼100, default settings in others\n1R-SVM\nγ = 2α/num features, α = [−5, 5]\nc = 0.1 ∼100, and default in others\nLACU-SVM\nramps = −0.3, η = 1.3,\nλ = 0.1, max iter = 10\nECSMiner\nS = 250, M = 6, K = 5\nq = 5, Tl = 200\niForest\nψ = 200, t = 100, MinSize = 10\nSENCForest\nψ = 200, z = 100, ρ = 3, MinSize = 10\n7.2. Descriptions of data sets\nSynthetic: We simulate a data stream using a two dimensional synthetic data set as shown\nbelow. It contains 20,000 instances and has four overlapping Gaussian distribution. The ﬁrst two\n33\n0\n5\n10\n15\n0\n5\n10\n15\n20\nX\nY\ninitial known classes are marked with purple. In the ﬁrst period, instances of class blue emerge\nas the ﬁrst new class. In the second period, instances of class red emerge as the second new class.\nMHAR: This data set [AGO+12] is collected from 30 volunteers wearing a smart phone on the\nwaist and performing 6 activities (walking, upstairs, downstairs, standing, sitting, laying). The\nembedded 3D-accelerometer and 3D-gyroscope of a Samsung Galaxy S2 smart phone were used\nto collect data at a constant rate of 50 Hz. This data set has 6 classes, 10299 instances and 561\nattributes.\nReferences\n[Agg13] Charu C. Aggarwal. A survey of stream clustering algorithms. In Data Clustering:\nAlgorithms and Applications, pages 231–258. 2013.\n[AGO+12] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis Reyes-\nOrtiz. Human activity recognition on smartphones using a multiclass hardware-\nfriendly support vector machine. In Proceedings of the 4th International Workshop\non Ambient Assisted Living and Home Care, pages 216–223, Vitoria-Gasteiz, Spain,\n2012.\n[BD93] E Brodsky and Boris S Darkhovsky. Nonparametric methods in change point prob-\nlems. Springer Netherlands, 1993.\n[BHP+09] Albert Bifet, Geoﬀrey Holmes, Bernhard Pfahringer, Richard Kirkby, and Ricard\nGavald`a. New ensemble methods for evolving data streams. In Proceedings of the\n15th ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pages 139–148, Paris, France, 2009.\n34\n[BKNS00] Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and J¨org Sander. LOF:\nidentifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD\nInternational Conference on Management of Data, pages 93–104, Dallas, Texas,\nUSA, 2000.\n[BN96] Mich`ele Basseville and Igor V. Nikiforov. Detection of abrupt changes: Theory and\napplication. Automatica, 32(8):1235–1236, 1996.\n[CBK09] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A\nsurvey. ACM Computing Surveys (CSUR), 41(3):15, 2009.\n[CL11] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector ma-\nchines. ACM Trans. Intelligent Systems and Technology, 2:27:1–27:27, 2011. Soft-\nware available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.\n[CRT11] Feilong Chen, Supranamaya Ranjan, and Pang-Ning Tan. Detecting bots via incre-\nmental LS-SVM learning with dynamic feature adaptation. In Proceedings of the\n17th ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pages 386–394, San Diego, CA, USA, 2011.\n[DDD05] Fr´ed´eric Desobry, Manuel Davy, and Christian Doncarli. An online kernel change\ndetection algorithm. IEEE Trans. Signal Processing, 53(8):2961–2974, 2005.\n[DYZ14] Qing Da, Yang Yu, and Zhi-Hua Zhou.\nLearning with augmented class by ex-\nploiting unlabeled data. In Proceedings of the 28th AAAI Conference on Artiﬁcial\nIntelligence, pages 1760–1766, Qu´ebec City, Qu´ebec, Canada, 2014.\n[FWYM03] Wei Fan, Haixun Wang, Philip S. Yu, and Sheng Ma. Is random model better? on\nits accuracy and eﬃciency. In Proceedings of the 3rd IEEE International Conference\non Data Mining, pages 51–58, Melbourne, Florida, USA, 2003.\n[HAY+07] Chang Huang, Haizhou Ai, Takayoshi Yamashita, Shihong Lao, and Masato\nKawade. Incremental learning of boosted face detector. In Proceedings of the 11th\nIEEE Conference on Computer Vision, pages 1–8, Rio de Janeiro, Brazil, 2007.\n[JA03] Ruoming Jin and Gagan Agrawal. Eﬃcient decision tree construction on stream-\n35\ning data. In Proceedings of the 9th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 571–576, New York, NY, USA, 2003.\n[KM07] J Zico Kolter and Marcus A Maloof. Dynamic weighted majority: An ensemble\nmethod for drifting concepts. Journal of Machine Learning Research, 8:2755–2790,\n2007.\n[KOC13] Ilja Kuzborskij, Francesco Orabona, and Barbara Caputo. From n to n+ 1: Multi-\nclass transfer incremental learning. In Proceedings of the 26th IEEE Conference on\nComputer Vision and Pattern Recognition, pages 3358–3365, Portland, OR, USA,\n2013.\n[LTF05] Fei Tony Liu, Kai Ming Ting, and Wei Fan. Maximizing tree diversity by building\ncomplete-random decision trees. In Proceedings of the 9th Paciﬁc-Asia Conference\non Knowledge Discovery and Data Mining, pages 605–610, Hanoi, Vietnam, 2005.\n[LTYZ08] Fei Tony Liu, Kai Ming Ting, Yang Yu, and Zhi-Hua Zhou. Spectrum of variable-\nrandom trees. Journal of Artiﬁcial Intelligence Research, pages 355–384, 2008.\n[LTZ08] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In Proceedings of\nthe 8th IEEE International Conference on Data Mining, pages 413–422, Pisa, Italy,\n2008.\n[MGK+11] M.M. Masud, Jing Gao, L. Khan, Jiawei Han, and Bhavani Thuraisingham. Clas-\nsiﬁcation and novel class detection in concept-drifting data streams under time\nconstraints. IEEE Trans. Knowledge and Data Engineering, 23(6):859–874, 2011.\n[MP03] J. Ma and S. Perkins. Time-series novelty detection using one-class support vector\nmachines. In Proceedings of the International Joint Conference on Neural Networks,\npages 1741–1745, Portland, OR, USA, 2003.\n[RK04] Ryan M. Rifkin and Aldebaro Klautau. In defense of one-vs-all classiﬁcation. Jour-\nnal of Machine Learning Research, 5:101–141, 2004.\n[SdC04] Eduardo J. Spinosa and Andr´e Carlos Ponce Leon Ferreira de Carvalho. Svms for\nnovel class detection in bioinformatics. In III Brazilian Workshop on Bioinformat-\nics, pages 81–88, Bras´ılia, Distrito Federal, Brazil, 2004.\n36\n[SdRRSB13] Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E\nBoult. Toward open set recognition. IEEE Trans. Pattern Analysis and Machine\nIntelligence, 35(7):1757–1772, 2013.\n[SPST+01] Bernhard Sch¨olkopf, John C. Platt, John C. Shawe-Taylor, Alex J. Smola, and\nRobert C. Williamson. Estimating the support of a high-dimensional distribution.\nNeural Computation, 13(7):1443–1471, 2001.\n[YYH07] Jun Yang, Rong Yan, and Alexander G Hauptmann. Cross-domain video concept\ndetection using adaptive svms. In Proceedings of the 15th international conference\non Multimedia, pages 188–197, Augsburg, Germany, 2007.\n[ZC02] Zhi-Hua Zhou and Zhao-Qian Chen. Hybrid decision tree. Knowledge-based systems,\n15(8):515–528, 2002.\n[Zho12] Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. 2012.\n37\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-05-30",
  "updated": "2016-05-30"
}