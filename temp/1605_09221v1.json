{
  "id": "http://arxiv.org/abs/1605.09221v1",
  "title": "Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym, a Gym RL Agent",
  "authors": [
    "Timothy J. O'Shea",
    "T. Charles Clancy"
  ],
  "abstract": "This paper presents research in progress investigating the viability and\nadaptation of reinforcement learning using deep neural network based function\napproximation for the task of radio control and signal detection in the\nwireless domain. We demonstrate a successful initial method for radio control\nwhich allows naive learning of search without the need for expert features,\nheuristics, or search strategies. We also introduce Kerlym, an open Keras based\nreinforcement learning agent collection for OpenAI's Gym.",
  "text": "Deep Reinforcement Learning Radio Control and\nSignal Detection with KeRLym, a Gym RL Agent\nTimothy J O’Shea and T. Charles Clancy\nVirginia Polytechnic Institute and State University\nAbstract. This paper presents research in progress investigating the vi-\nability and adaptation of reinforcement learning using deep neural net-\nwork based function approximation for the task of radio control and\nsignal detection in the wireless domain. We demonstrate a successful\ninitial method for radio control which allows naive learning of search\nwithout the need for expert features, heuristics, or search strategies. We\nalso introduce Kerlym, an open Keras based reinforcement learning agent\ncollection for OpenAI’s Gym.\nIntroduction\nRadios are ubiquitous in modern society. Between cellular devices, wearable\ndevices, computing devices, medical devices, and other devices we carry and\noperate regularly through each day, radio frequency communications have be-\ncome the most pervasive and convenient way we communicate information in our\ndaily lives. Unfortunately, our spectrum resources are limited and our data needs\nare growing in a seemingly unbounded manner. Despite this wireless spectrum\ncrunch, our methods for allocation, adaptation and optimization of spectrum use\nremains very much in the dark ages today. Spectrum is still allocated in a static\nfashion, and devices are oblivious and unaware of the use of or the availability\nof resources in their direct vicinity. The ﬁeld of cognitive radio and dynamic\nspectrum access have attempted to address this through the introduction of ex-\npert systems which attempt to perform spectrum sensing and some degree of\ncharacterization of their environment, but their impact has been heavily limited\nby their inability to generalize to new regions, protocols, emitters, and radio\npropagation environments.\nGeneralized policy learning has and continues to be an open challenge in\nCS and AI for many years, however in recent years advances in reinforcement\nlearning have made massive strides towards the advancement of this ﬁeld. Recent\nWork by Minh [3], Silver [11], Sutton [1], and others has begun to demonstrate\nthe ability to learn exceedingly complex and varied tasks using deep neural\nnetwork based policy function approximation to implement Q-Learning.\nTo address this problem of learning to rapidly understand the surrounding\nradio environment, we introduce a radio signal search environment for the re-\ncently released Gym RL framework from OpenAI in which to begin evaluating\nand scoring diﬀerent approaches.\narXiv:1605.09221v1  [cs.LG]  30 May 2016\n2\nT. O’Shea, T. C. Clancy\nWe also implement a general purpose open source Deep Neural network based\nQ-Learning function approximation learner for Gym using Keras primitives to\nlearn a policy for rapidly exploring this environment through its set of discrete\nactions and observations [5].\n1\nReinforcement Learning Policy\nWe introduce KeRLym [9], an open source deep reinforcement learning agent\ncollection written in python using Keras to implement GPU optimized deep\nneural networks on top of Theano [2] and TensorFlow [12]. OpenAI recently\nreleased Gym [8], a collection of reinforcement learning benchmark environments,\nan API to easily use them, and a web based high-score board for algorithm\ncomparison. We leverage this API in our reinforcement learner to provide a\nstandard agent interface and to rapidly provide a wide range of tasks we can\ntest its performance and tuning against.\n1.1\nPolicy Learning\nSince Google Deepmind’s Nature paper on Deep-Q Networks [3], there has been\na surge of interest in the capabilities of reinforcement learning use deep neural\nnetwork policy network approximation. This is an exciting and growing area with\nmuch potential improvement for learning algorithms yet to come. For the scope of\nthis work we implement a parametric version of the Deep Q-Learning algorithm\nalong with a Double Q-Learning [7] implementation in the KeRLym toolbox. We\nimplement a variety of function approximation networks which can be used inside\nthem including dense fully connected networks, convolutional networks similar\nto those used in the Atari paper, and recurrent networks leveraging LSTM which\nmay improve sequence learning in POMDPs as discussed in [6].\nThe approaches in are similar for both algorithms, a value function Q(s, a; θ)\nis updated using a form of stochastic gradient descnt, SGD, in the form of:\nθt+1 = θt + α (Yt, −Q (St, At; θt)) ∇θQ(St, At; θt)\n(1)\nHowever, in single Q-learning we directly compute Yt in a greedy manner\nusing our latest θ as:\nYt = Rt+1 + γmax\na Q (St+1, a; θt)\n(2)\nWhereas in double Q-learning we maintain two sets of weights θ and θ′ which\nwe alternate between using for decision making and greedy policy update pur-\nposes:\nYt = Rt+1 + γQ\n\u0012\nSt+1, argmax\na\nQ (St+1, a; θt) ; θ′\n\u0013\n(3)\nThis helps to reduce overestimation value bias and imrpoves policy learning\nrate and stability for many tasks.\nRL Radio Control & KeRLym\n3\nWe implement ϵ-greedy learning with a default constant value of 0.1, to\nchoose the greedy policy 90% of the time, simply to avoid the tuning required\nwith epsilon decay schedules for stability of comparison of this work.\nWe also implement experience replay, keeping around a memory of 1,000,000\nprevious actions to draw training samples from in addition to the new experience\ngained each time-step. We use a learning rate of 0.001 in a Keras Adam [4] solver,\nand a discount rate of γ = 0.99 in our experiments.\nWithin the KeRLym toolbox we hope to extend the number of solvers avail-\nable\n1.2\nDeep-Q Network Implementation\nOur Q function Q(s, a, θ) is a Deep Neural Network with random initial pa-\nrameters θ implemented in the Keras framework on top of Theano, running on\nan Nvidia Titan X. We zero the output regression layer weights to reduce ini-\ntial error in value function output. We start with a similar architecture to the\nconvolutional network used by Mnih et al in [3], but make changes which show\nimprovement in our domain and account for the input information form.\nSince we are passing both scalar stored variables containing sensor infor-\nmation, and contiguous frequency domain values into the value function as the\ncurrent state, we treat each input conﬁguration value as an independent discrete\ninput with fully connected logic, while we reduce the parameter space and al-\nlow frequency domain ﬁlters to form and be used shift-invariantly on the power\nspectrum by using a set of convolutional layers, similar to our approach on raw\ntime-domain samples in [10].\nUltimately we concatenate the activations from both of these paths into\ndense fully connected layers to perform the output regression task for output\naction-value estimates.\nFig. 1. Action-Value Network Architecture\n4\nT. O’Shea, T. C. Clancy\n2\nRadio Search Environment\n2.1\nEnvironment Overview\nTypical electronic devices such as cellular phones contain at this point highly\nﬂexible Radio Frequency Integrated Circuits (RFIC) which allow the frequency\ntuning and digitization of relatively large arbitrary bands of interest. Typically\nthey are programmed in an exceedingly simple way by a carrier to brute force\nthrough a small list of carrier-assigned channels and bandwidth, however they\nare in fact capable of tuning to relatively arbitrary center frequencies between\n100 MHz and 6GHz and providing often powers of two decimations of a 10-20\nMHz wide bandwidth.\nInstead of brute force search for signals on several carrier centric bands,\nwe propose instead to allow machine learning to derive a general search policy\nto identify signals providing useful connectivity while optimizing for minimal\nsearch time, battery consumption and power usage possible. To do this we boil\nthe search task down into a relatively small set of possible discrete actions which\nmay be taken towards the end-goal.\nFig. 2. Initial Radio Discrete Action Set\n2.2\nEnvironment Implementation\nWe begin by building an environment for the Gym Reinforcement Learning en-\nvironment to attempt to mirror our problem statement, and a reasonable set of\nassumptions for what a real system could do and sense, but at a relatively small\nscale of complexity for initial work.\nWe simulate a single radio receiver sampling at a bandwidth of 20 MHz,\nwhich can be decimated and re-tuned using the set of discrete actions in 2.\nThe discrete actions we allow are as follows, where we refer to the variables:\ncenter frequency (fc), bandwidth (bw), maximum bandwidth (bwmax), minimum\nbandwidth (bwmin), maximum center frequency (fcmax), and minimum center\nfrequency (fcmin).\n– Freq Down: fc−= max(bw/2, fcmin)\n– Freq Up: fc+ = min(bw/2, fcmax)\n– BW Down Left: bw = max(bw/2, bwmin); fc−= bw/2\n– BW Down Left: bw = max(bw/2, bwmin); fc+ = bw/2\n– BW Max: bw = bwmax\n– Detect: Assert that a signal is in the current window.\n– Finished: Assert that all signals in band have been detected.\nRL Radio Control & KeRLym\n5\nThe environment chooses a random frequency within the band of interest\n(100MHz to 200MHz in this work) to place a single sinusoidal tone. For each\nagent observation, it returns a small band-limited window into the environment\ntuned to the chosen center frequency and bandwidth. The Detect action asserts\nthat there is a signal within the current band either correctly or falsely, Finish\nassets that we have correctly found the signal and our search path is complete,\nand bandwidth and frequency actions change our receiver conﬁguration accord-\ning to the list above. A single optimal path to a solution through the environment\nmight look something like shown in ﬁgure 3. In this case each look window for a\ntime-step is represented by a red bar above the wideband power spectrum plot.\nFig. 3. Environment Search Scenario\n3\nTraining Considerations\nThere are numerous ways to deﬁne penalties and rewards for this search process\nwithin the environment which pose a number of diﬀerent considerations for the\ntraining process, we propose 3 potential rewards schemes below.\nScheme\nA B\nC\nDetect(True) 1 1\n0\nDetect(False) 0 -1\n0\nBW-(True)\n1 1\n0\nBW-(False)\n0 0\n0\nFinish(True) 1 1 nfound*depth\nFinish(False) 0 -1\n0\nTable 1. Environment Reward and Penalty Schemes\n6\nT. O’Shea, T. C. Clancy\nOour agent’s goal at run-time is to detect the signal present somewhere in\nthe band and localize the signal using BW-L and BW-R actions to zoom in on\nit, these rewards and penalties are designed to reﬂect that. Scheme A results\nin perhaps the fastest training rate and simples approach towards directly re-\nwarding good actions, Scheme B provides a strong disincentive for false positive\nactions, but slows down learning, and Scheme C provides a simple ﬁnal score\nwhich requires a more delayed-reward style of learning.\n4\nConclusions and Future Work\nWe can plot a number of statistics during the training process which give us\ninsight into how the training is going. Shown in ﬁgure 4 we have the training\nstatistics under Scheme B with early exiting (on Finish(False)). From this graph\nit is clear that we are learning a relatively clear separation between good and\nbad action values, as can be seen from the separation in the 3rd plot, and that\nour reward is growing and our ﬁnishing time is growing long enough to succeed\nsome of the time.\nFig. 4. Plots During Network Training\n’ In future work we hope to provide a more comprehensive trade between\nthe trade oﬀs described above, learn a policy which performs at a more satis-\nfying reward level, and and compare the impact of reward/penalty schemes on\ntraditional receiver operating characteristics, ROC, curves for performance. We\nare excited about the potential in this area and positive this approach will be\nfruitful.\nREFERENCES\n7\nReferences\n[1]\nR. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 1998.\n[2]\nJ. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-\njardins, J. Turian, D. Warde-Farley, and Y. Bengio, “Theano: a CPU and\nGPU math expression compiler”, in Proceedings of the Python for Scien-\ntiﬁc Computing Conference (SciPy), Oral Presentation, Austin, TX, Jun.\n2010.\n[3]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learning”,\narXiv preprint arXiv:1312.5602, 2013.\n[4]\nD. Kingma and J. Ba, “Adam: a method for stochastic optimization”, arXiv\npreprint arXiv:1412.6980, 2014.\n[5]\nF. Chollet, Keras, https://github.com/fchollet/keras, 2015.\n[6]\nM. Hausknecht and P. Stone, “Deep recurrent q-learning for partially ob-\nservable mdps”, arXiv preprint arXiv:1507.06527, 2015.\n[7]\nH. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with\ndouble q-learning”, arXiv preprint arXiv:1509.06461, 2015.\n[8]\nY. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Bench-\nmarking deep reinforcement learning for continuous control”, arXiv preprint\narXiv:1604.06778, 2016.\n[9]\nT. O’Shea, Kerlym: keras reinforcement learning gym agents, https://\ngithub.com/osh/kerlym, 2016.\n[10]\nT. J. O’Shea, J. Corgan, and T. C. Clancy, “Convolutional radio modula-\ntion recognition networks”, arXiv preprint arXiv:1602.04105, 2016.\n[11]\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driess-\nche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al.,\n“Mastering the game of go with deep neural networks and tree search”, Na-\nture, vol. 529, no. 7587, pp. 484–489, 2016.\n[12]\nM. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin, et al., “Tensorﬂow: large-scale ma-\nchine learning on heterogeneous systems, 2015”, Software available from\ntensorﬂow. org,\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-05-30",
  "updated": "2016-05-30"
}