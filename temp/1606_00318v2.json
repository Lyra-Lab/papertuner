{
  "id": "http://arxiv.org/abs/1606.00318v2",
  "title": "Discovering Phase Transitions with Unsupervised Learning",
  "authors": [
    "Lei Wang"
  ],
  "abstract": "Unsupervised learning is a discipline of machine learning which aims at\ndiscovering patterns in big data sets or classifying the data into several\ncategories without being trained explicitly. We show that unsupervised learning\ntechniques can be readily used to identify phases and phases transitions of\nmany body systems. Starting with raw spin configurations of a prototypical\nIsing model, we use principal component analysis to extract relevant low\ndimensional representations the original data and use clustering analysis to\nidentify distinct phases in the feature space. This approach successfully finds\nout physical concepts such as order parameter and structure factor to be\nindicators of the phase transition. We discuss future prospects of discovering\nmore complex phases and phase transitions using unsupervised learning\ntechniques.",
  "text": "Discovering Phase Transitions with Unsupervised Learning\nLei Wang\nBeijing National Lab for Condensed Matter Physics and Institute of Physics,\nChinese Academy of Sciences, Beijing 100190, China\nUnsupervised learning is a discipline of machine learning which aims at discovering patterns in big\ndata sets or classifying the data into several categories without being trained explicitly. We show\nthat unsupervised learning techniques can be readily used to identify phases and phases transitions\nof many body systems. Starting with raw spin conﬁgurations of a prototypical Ising model, we use\nprincipal component analysis to extract relevant low dimensional representations the original data\nand use clustering analysis to identify distinct phases in the feature space. This approach successfully\nﬁnds out physical concepts such as order parameter and structure factor to be indicators of the phase\ntransition. We discuss future prospects of discovering more complex phases and phase transitions\nusing unsupervised learning techniques.\nClassifying phases of matter and identifying phase\ntransitions between them is one of the central topics of\ncondensed matter physics research. Despite an astronom-\nical number of constituting particles, it often suﬃces to\nrepresent states of a many-body system with only a few\nvariables. For example, a conventional approach in con-\ndensed matter physics is to identify order parameters via\nsymmetry consideration or analyzing low energy collec-\ntive degree of freedoms and use them to label phases of\nmatter [1].\nHowever, it is harder to identify phases and phase tran-\nsitions in this way in an increasing number of new states\nof matter, where the order parameter may only be deﬁned\nin an elusive nonlocal way [2]. These new developments\ncall for new ways of identifying appropriate indicators of\nphase transitions.\nTo meet this challenge, we use machine learning tech-\nniques to extract information of phases and phase tran-\nsitions directly from many-body conﬁgurations. In fact,\napplication of machine learning techniques to condensed\nmatter physics is a burgeoning ﬁeld [3–13][33]. For ex-\nample, regression approaches are used to predict crystal\nstructures [3], to approximate density functionals [6], and\nto solve quantum impurity problems [10]; artiﬁcial neural\nnetworks are trained to classify phases of classical statis-\ntical models [13]. However, most of those applications\nuse supervised learning techniques (regression and clas-\nsiﬁcation), where a learner needs to be trained with the\npreviously solved data set (input/output pairs) before it\ncan be used to make predictions.\nOn the other hand, in the unsupervised learning, there\nis no such explicit training phase. The learner should by\nitself ﬁnd out interesting patterns in the input data. Typ-\nical unsupervised learning tasks include cluster analysis\nand feature extraction. Cluster analysis divides the input\ndata into several groups based on certain measures of sim-\nilarities. Feature extraction ﬁnds a low-dimensional rep-\nresentation of the dataset while still preserving essential\ncharacteristics of the original data. Unsupervised learn-\ning methods have broad applications in data compres-\nsion, visualization, online advertising and recommender\nsystem, etc. They are often being used as a preprocessor\nof supervised learning to simplify the training procedure.\nIn many cases, unsupervised learning also lead to better\nhuman interpretations of complex datasets.\nIn this paper, we explore the application of unsuper-\nvised learning in many-body physics with a focus on\nphase transitions. The advantage of unsupervised learn-\ning is that one assumes neither the presence of the phase\ntransition nor the precise location of the critical point.\nDimension reduction techniques can extract salient fea-\ntures such as order parameter and structure factor from\nthe raw conﬁguration data. Clustering analysis can then\ndivide the data into several groups in the low-dimensional\nfeature space, representing diﬀerent phases. Our studies\nshow that unsupervised learning techniques have great\npotentials of addressing the big data challenge in the\nmany-body physics and making scientiﬁc discoveries.\nAs an example, we consider the prototypical classical\nIsing model\nH = −J\nX\n⟨i,j⟩\nσiσj,\n(1)\nwhere the spins take two values σi = {−1, +1}.\nWe\nconsider the model (1) on a square lattice with periodic\nboundary conditions and set J = 1 as the energy unit.\nThe system undergoes a phase transition at temperature\nT/J = 2/ln(1 +\n√\n2) ≈2.269 [14]. A discrete Z2 spin\ninversion symmetry is broken in the ferromagnetic phase\nbelow Tc and is restored in the disordered phase at tem-\nperatures above Tc.\nWe generate 100 uncorrelated spin conﬁguration sam-\nples using Monte Carlo simulation [15] at temperatures\nT/J = 1.6, 1.7, . . . , 2.9 each and collect them into an\nM × N matrix\nX =\n\n\n\n↑↓↑. . . ↑↑↑\n...\n↓↑↓. . . ↑↓↑\n\n\n\nM×N\n,\n(2)\nwhere M = 1400 is the total number of samples, and N\nis the number of lattice sites. The up and down arrows\narXiv:1606.00318v2  [cond-mat.stat-mech]  6 Jun 2016\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nℓ\n10-3\n10-2\n10-1\n100\n˜λℓ\nN =202\nN =402\nN =802\n0\n400\n800\n1200 1600\ni\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nw1\nFigure 1: The ﬁrst few explained variance ratios obtained\nfrom the raw Ising conﬁgurations.\nThe inset shows the\nweights of the ﬁrst principal component on an N = 402 square\nlattice.\nin the matrix denote σi = ±1. Such a matrix is the only\ndata we feed to the unsupervised learning algorithm.\nOur goal is to discover possible phase transition of the\nmodel (1) without assuming its existence. This is diﬀer-\nent from the supervised learning task, where exact knowl-\nedge of Tc was used to train a learner [13]. Moreover,\nthe following analysis does not assume any prior knowl-\nedge about the lattice geometry and the Hamiltonian.\nWe are going to use the unsupervised learning approach\nto extract salient features in the data and then use this\ninformation to cluster the samples into distinct phases.\nKnowledge about the temperature of each sample and\nthe critical temperature Tc of the Ising model is used to\nverify the clustering.\nInterpreting each row of X as a coordinate of an N-\ndimensional space, the M data points form a cloud cen-\ntered around the origin of a hypercube [34]. Discovering\na phase transition amounts to ﬁnd a hypersurface which\ndivides the data points into several groups, each repre-\nsenting a phase. The task is akin to the standard unsu-\npervised learning technique: cluster analysis [16], where\nnumerous algorithms are available, and they group the\ndata based on diﬀerent criteria.\nHowever, direct applying clustering algorithms to the\nIsing conﬁgurations may not be very enlightening. The\nreasons are twofold. First, even if one manages to sep-\narate the data into several groups, clusters in high di-\nmensional space may not directly oﬀer useful physical\ninsights. Second, many clustering algorithms rely on a\ngood measure of similarity between the data points. Its\ndeﬁnition is, however, ambiguous without supplying of\ndomain knowledge such as the distance between two spin\nconﬁgurations.\nOn the other hand, the raw spin conﬁguration is a\n50\n25\n0\n25\n50\ny2\n(a)\n50\n25\n0\n25\n50\ny2\n(b)\n100\n50\n0\n50\n100\ny1\n50\n25\n0\n25\n50\ny2\n(c)\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nFigure 2: Projection of the samples onto the plane of the\nleading two principal components. The color bar on the right\nindicates the temperature T/J of the samples. The panels\n(a-c) are for N = 202, 402 and 802 sites respectively.\nhighly redundant description of the system’s state be-\ncause there are correlations among the spins. Moreover,\nas the temperature varies, there is an overall tendency\nin the raw spin conﬁgurations, such as lowering the total\nmagnetization. In the following, we will try to ﬁrst iden-\ntify some crucial features in the raw data. They provide\nan eﬀective low dimensional representation of the original\ndata. And in terms of these features, the meaning of the\ndistance between conﬁgurations becomes more transpar-\nent. The separation of phases is also often clearly visible\nand comprehensible by the human in the reduced space\nspanned by these features. Therefore, feature extraction\ndoes not only simpliﬁes the subsequent clustering anal-\nysis but also provides eﬀective means of visualizing and\noﬀering physical insights. We denote the crucial features\nextracted by the unsupervised learning as indicators of\nthe phase transition. In general, they do not necessarily\nneed to be the same as the conventional order parame-\nters deﬁned in condensed matter physics. This unsuper-\nvised learning approach nevertheless provides an alterna-\ntive view of phases and phase transitions.\nPrincipal component analysis (PCA) [17] is a widely\nused feature extraction technique. The principal compo-\nnents are mutually orthogonal directions along which the\nvariances of the data decrease monotonically. PCA ﬁnds\nthe principal components through a linearly transforma-\ntion of the original coordinates Y = XW. When applied\nto the Ising conﬁgurations in Eq. (2), PCA ﬁnds the most\nsigniﬁcant variations of the data changing with the tem-\nperature. We interpret them as relevant features in the\ndata and use them as indicators of the phase transition\nif there is any.\nWe write the orthogonal transformation into column\nvectors W = (w1, w2, . . . , wN) and denote wℓas weights\n3\nFigure 3: Typical conﬁgurations of the COP Ising model at\nbelow (a,b) and above (c) the critical temperature. Red and\nblue pixels indicate up and down spins. There are exactly half\nof the pixels are red/blue due to the constraint P\ni σi ≡0.\nof the principal components in the conﬁguration space.\nThey are determined by an eigenproblem [18] [35]\nXT Xwℓ= λℓwℓ.\n(3)\nThe eigenvalues are nonnegative real numbers sorted in\na descending order λ1 ≥λ2 . . . ≥λN ≥0. Using the\nterminology of PCA, we denote the normalized eigenval-\nues ˜λℓ= λℓ/ PN\nℓ=1 λℓas explained variance ratio. When\nkeeping only the ﬁrst few principal components, PCA is\nan eﬃcient dimension reduction approach which captures\nmost variations of the original data. Moreover, PCA also\nyields an optimal approximation of the data in the sense\nof minimizing the squared reconstruction error [18].\nFigure 1 shows the ﬁrst few explained variance ratios\nfor various system sizes. Notably, there is only one dom-\ninant principal component. As the temperature changes\nthe Ising conﬁgurations vary most signiﬁcantly along the\nﬁrst principal component, whose weight is shown in the\ninset of Fig. 1. The ﬂat distribution all over the lattice\nsites means the transformation actually gives the uniform\nmagnetization\n1\nN\nP\ni σi. In this sense, PCA has identi-\nﬁed the order parameter of the Ising model (1) upon a\nphase transition.\nNext, we project the samples in the space spanned\nby the ﬁrst two principal components, shown in Fig-\nure 2. The color of each sample indicates its temperature.\nThe projected coordinates are given by the matrix-vector\nproduct\nyℓ= Xwℓ.\n(4)\nThe variation of the data along the ﬁrst principal axis y1\nis indeed much stronger than that along the second prin-\ncipal axis y2. Most importantly, one clearly observes that\nas the system size enlarges the samples tend to split into\nthree clusters. The high-temperature samples lie around\nthe origin while the low-temperature samples lie sym-\nmetrically at ﬁnite y1. The samples at the critical tem-\nperature (light yellow dots) have broad spread because of\nlarge critical ﬂuctuations. We note that Ref. [13] presents\na diﬀerent low dimension visualization of the Ising conﬁg-\nurations using stochastic neighbor embedding technique.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nℓ\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n˜λℓ\nw1\nw2\nw3\nw4\n0.04\n0.02\n0.00\n0.02\n0.04\nFigure 4: Explained variance ratios of the COP Ising model.\nInsets show the weights corresponding to the four leading\nprincipal components.\nWhen folding the horizontal axis of Fig. 2 into P\ni |σi|\nor (P\ni σi)2 the two clusters associated with the low-\ntemperature phase merge together. With such a linear\nseparable low dimensional representation of the original\ndata, a cluster analysis [36] can easily divide the samples\ninto two phases, thus identifying the phase transition.\nNotice that our unsupervised learning analysis does not\nonly ﬁnds the phase transition and an estimate of the\ncritical temperature but also provides insight into the\norder parameter.\nHaving established the baseline of applying the un-\nsupervised learning techniques in the prototypical Ising\nmodel, we now turn to a more challenging case where\nthe learner can make nontrivial ﬁndings.\nFor this, we\nconsider the same Ising model Eq. (1) with a conserved\norder parameter (COP) P\ni σi ≡0. This model describes\nclassical lattice gasses [19], where the occupation of each\nlattice site can be either one or zero and the particles\ninteract via a short-range attraction. The conserved to-\ntal magnetization corresponds to the constraint of a half\nﬁlled lattice.\nOn a square lattice with periodic boundary conditions,\nthe spins tend to form two domains at low-temperatures\nshown in Fig. 3(a,b). The two domain walls wrap around\nthe lattice either horizontally or vertically to minimize\nthe domain wall energy [19]. Besides, the domains can\nalso shift in space due to translational invariance.\nAs\nthe temperature increases, these domain walls melt and\nthe system restores both the translational and rota-\ntional symmetries in the high-temperature phase shown\nin Fig. 3(c). At zero total magnetization, the critical tem-\nperature of such solid-gas phase transition is the same as\nthe Ising transition Tc/J ≈2.269 [20]. However, since\nthe total magnetization is conserved, simply summing up\nthe Ising spins can not be used as an indicator to distin-\nguish the two phases. In fact, it is unclear to the author\nwhich quantity signiﬁes the phase transition before this\n4\nFigure 5: Projections of the COP Ising samples to the four\nleading principal components.\nstudy. It is, therefore, a good example to demonstrate\nthe ability of the unsupervised learning approach.\nWe perform the same PCA on the COP Ising conﬁg-\nurations sampled with Monte Carlo simulation [19] and\nshow the ﬁrst few explained variance ratios in Fig. 4.\nNotably, there are four instead of one leading princi-\npal components. Their weights plotted in the insets of\nFig. 4 show notable nonuniformity over the lattice sites.\nThis indicates that in the COP Ising model the spatial\ndistribution of the spins varies drastically as the tem-\nperature changes. Denote Euclidean coordinate of site\ni as (µi, νi), where µi, νi = 1, 2, . . . ,\n√\nN. The weights\nof the four leading principal components can be writ-\nten as cos(θi), cos(φi), sin(θi), sin(φi), where (θi, φi) =\n(µi, νi)×2π/\n√\nN [37]. Note these four mutually orthogo-\nnal weights correspond to the two orientations of the do-\nmain walls shown in Fig. 3(a,b). Therefore, the PCA cor-\nrectly ﬁnds out the rotational symmetry breaking caused\nby the domain wall formation.\nTo visualize the samples in the four-dimensional fea-\nture space spanned by the ﬁrst few principal compo-\nnents, we plot two-dimensional projections in Fig. 5. In\nall cases, the high-temperature samples are around the\norigin while the low-temperature samples form a sur-\nrounding cloud. Motivated by the circular shapes of all\nthese projections, we further reduce to a two-dimensional\nspace via a nonlinear transformation (y1, y2, y3, y4) 7→\n(y2\n1 + y2\n2, y2\n3 + y2\n4).\nAs shown in Fig. 6(a), the line\nP4\nℓ=1 y2\nℓ= const (a four dimensional sphere of a constant\nradius) separates the low and high temperature samples.\nThis motivates a further dimension reduction to a single\nvariable P4\nℓ=1 y2\nℓas an indicator of the phase transition\nin the COP Ising model.\nSubstituting weights of the four principal components\ncos(θi), cos(φi), sin(θi), sin(φi), the sum P4\nℓ=1 y2\nℓis pro-\nFigure 6: (a) Further projection of the COP Ising samples to\na two-dimensional space. (b) The structure factor Eq. (5) of\nthe COP Ising model versus temperature for various system\nsizes.\nportional to\nS =\n1\nN 2\nX\ni,j\nσiσj [cos (θi −θj) + cos (φi −φj)] .\n(5)\nEven though such structure factor was unknown to the\nauthor before it was discovered by the learner, one can\nconvince himself it indeed captures the domain wall for-\nmation at low temperatures shown in Fig. 3(a,b). Fig-\nure 6(b) shows the structure factor versus temperature\nfor various system sizes. It decreases as the temperature\nincreases and clearly serves as a good indicator of the\nphase transition. We emphasis that the input spin con-\nﬁgurations contain no information about the lattice ge-\nometry nor the Hamiltonian. However, the unsupervised\nlearner has by itself extracted meaningful information re-\nlated to the breaking of the orientational order. There-\nfore, even without the knowledge of the lattice and the\nanalytical understanding of the structure factor Eq. (5),\nP4\nℓ=1 y2\nℓplays the same role of separating the phases in\nthe projected space.\nIt is interesting to compare our analysis of phase tran-\nsitions to standard imagine recognition applications. In\nthe Ising model example, the learner essentially ﬁnds out\nthe brightness of the imagine P\ni σi as an indicator of the\nphase transition. While in the COP Ising model exam-\nple, instead of detecting sharpness of the edges (melting\nof domain walls) following the ordinary imagine recog-\nnition routine, the PCA learner ﬁnds out the structure\nfactor Eq. (5) related to symmetry breaking, which is a\nfundamental concept in phase transition and condensed\nmatter physics.\nConsidering PCA is arguably one of the simplest un-\nsupervised learning techniques, the obtained results are\nrather encouraging. In essence, our analysis ﬁnds out the\ndominant collective modes of the system related to the\nphase transition. The approach can be readily general-\nized to more complex cases such as models with emergent\nsymmetry and order by disorder [21]. The unsupervised\nlearning approach is particularly proﬁtable in the case of\n5\nhidden or multiple intertwined orders, where it can help\nto single out various phases.\nAlthough nonlinear transformation of the raw conﬁg-\nuration Eq. (5) was discovered via visualization in Fig. 5,\nsimple PCA is however limited to linear transformations.\nTherefore, it remains challenging to identify more subtle\nphase transitions related to the topological order, where\nthe indicators of the phase transition are nontrivial non-\nlinear functions of the original conﬁgurations. For this\npurpose, it would be interesting to see if a machine learn-\ning approach can comprehend concepts such as duality\ntransformation [22], Wilson loop [23] and string order pa-\nrameter [24]. A judicial apply of kernel techniques [25] or\nneural network based deep autoencoders [26] may achieve\nsome of these goals.\nFurthermore, although our discussions focus on ther-\nmal phase transitions of the classical Ising model, the\nunsupervised learning approaches can also be used to an-\nalyze quantum many-body systems and quantum phase\ntransitions [27]. In these applications, diagnosing quan-\ntum states of matter without knowledge of Hamiltonian\nis a useful paradigm for cases with only access to wave-\nfunctions or experimental data.\nAcknowledgment\nThe author thanks Xi Dai, Ye-Hua\nLiu, Yuan Wan, QuanSheng Wu and Ilia Zintchenko\nfor discussions and encouragement.\nThe author also\nthanks Zi Cai for discussions and careful readings of the\nmanuscript. L.W. is supported by the start-up funding\nof IOP-CAS.\n[1] P. W. Anderson, Basic notions of condensed matter\nphysics (The Benjamin-Cummings Publishing Company,\n1984).\n[2] X.-G. Wen, Quantum ﬁeld theory of many-body systems\n(Oxford University Press, 2004).\n[3] S. Curtarolo, D. Morgan, K. Persson, J. Rodgers,\nand\nG. Ceder, Physical Review Letters 91, 135503 (2003).\n[4] O. S. Ovchinnikov, S. Jesse, P. Bintacchit, S. Trolier-\nMcKinstry, and S. V. Kalinin, Physical Review Letters\n103, 157203 (2009).\n[5] G. Hautier, C. C. Fischer, A. Jain, T. Mueller,\nand\nG. Ceder, Chemistry of Materials 22, 3762 (2010).\n[6] J. C. Snyder, M. Rupp, K. Hansen, K.-R. Müller,\nand\nK. Burke, Physical Review Letters 108, 253002 (2012).\n[7] Y. Saad, D. Gao, T. Ngo, S. Bobbitt, J. R. Chelikowsky,\nand W. Andreoni, Physical Review B 85, 104104 (2012).\n[8] E. LeDell, Prabhat, D. Y. Zubarev, B. Austin,\nand\nW. A. Lester, Journal of Mathematical Chemistry 50,\n2043 (2012).\n[9] M. Rupp, A. Tkatchenko, K.-R. Müller, and O. A. von\nLilienfeld, Physical Review Letters 108, 058301 (2012).\n[10] L.-F. Arsenault, A. Lopez-Bezanilla, O. A. von Lilienfeld,\nand A. J. Millis, Physical Review B 90, 155136 (2014).\n[11] G. Pilania, J. E. Gubernatis, and T. Lookman, Physical\nReview B 91, 214302 (2015).\n[12] Z. Li, J. R. Kermode, and A. De Vita, Physical Review\nLetters 114, 096405 (2015).\n[13] J. Carrasquilla and R. G. Melko, (2016), 1605.01735 .\n[14] L. Onsager, Physical Review 65, 117 (1944).\n[15] U. Wolﬀ, Physical Review Letters 62, 361 (1989).\n[16] B. S. Everitt, S. Landau, M. Leese, and D. Stahl, Cluster\nAnalysis (Wiley, 2010).\n[17] K. Pearson, Philosophical Magazine 2, 559 (1901).\n[18] I. Jolliﬀe, Principal Component Analysis (John Wiley &\nSons, Ltd, Chichester, UK, 2002).\n[19] M. Newman and G. T. Barkema, Monte Carlo methods\nin statistical physics (Oxford, 1999).\n[20] C. N. Yang, Physical Review 85, 808 (1952).\n[21] R. Moessner and S. L. Sondhi, Physical Review B 63,\n224401 (2001).\n[22] F. J. Wegner, Journal of Mathematical Physics 12, 2259\n(1971).\n[23] K. G. Wilson, Physical Review D 10, 2445 (1974).\n[24] M. den Nijs and K. Rommelse, Physical Review B 40,\n4709 (1989).\n[25] B. Schölkopf, A. Smola, and K. R. Müller, Neural com-\nputation (1998).\n[26] G. E. Hinton and R. R. Salakhutdinov, Science 313, 504\n(2006).\n[27] S. Sachdev, Quantum phase transitions (Cambridge Uni-\nversity Press, 2011).\n[28] L. Saitta, A. Giordana, and A. Cornuéjols, Phase Transi-\ntions in Machine Learning (Cambridge University Press,\n2011).\n[29] P. Mehta and D. J. Schwab, (2014), 1410.3831 .\n[30] E.\nM.\nStoudenmire\nand\nD.\nJ.\nSchwab,\n(2016),\n1605.05775v1 .\n[31] S. Lloyd, M. Mohseni,\nand P. Rebentrost,\n(2013),\n1307.0411 .\n[32] S. R. White, Physical Review Letters 69, 2863 (1992).\n[33] We also note application of physics ideas such as phase\ntransition [28], renormalization group [29], tensor net-\nworks [30] and quantum computation [31] to machine\nlearning.\n[34] Each column of X sums up to zero since on average each\nsite has zero magnetization.\n[35] In practice this eigenproblem is often solved by singu-\nlar value decomposition of X. In fact, replacing the in-\nput data X (raw spin conﬁgurations collected at various\ntemperature) by the wave function of a one-dimensional\nquantum system, the math here is identical to the trunca-\ntion of Schmidt coeﬃcients in the density-matrix renor-\nmalization group calculations [32].\n[36] See,\nfor\nexample,\nthe\nmethods\nprovided\nin\nthe\nscikit-learn cluster module http://scikit-learn.org/\nstable/modules/clustering.html\n[37] The weights shown in the inset of Fig. 4 are linear mix-\ntures of them.\n",
  "categories": [
    "cond-mat.stat-mech",
    "stat.ML"
  ],
  "published": "2016-06-01",
  "updated": "2016-06-06"
}