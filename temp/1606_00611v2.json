{
  "id": "http://arxiv.org/abs/1606.00611v2",
  "title": "Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks",
  "authors": [
    "Boris Knyazev",
    "Erhardt Barth",
    "Thomas Martinetz"
  ],
  "abstract": "In visual recognition tasks, such as image classification, unsupervised\nlearning exploits cheap unlabeled data and can help to solve these tasks more\nefficiently. We show that the recursive autoconvolution operator, adopted from\nphysics, boosts existing unsupervised methods by learning more discriminative\nfilters. We take well established convolutional neural networks and train their\nfilters layer-wise. In addition, based on previous works we design a network\nwhich extracts more than 600k features per sample, but with the total number of\ntrainable parameters greatly reduced by introducing shared filters in higher\nlayers. We evaluate our networks on the MNIST, CIFAR-10, CIFAR-100 and STL-10\nimage classification benchmarks and report several state of the art results\namong other unsupervised methods.",
  "text": "Recursive Autoconvolution for Unsupervised\nLearning of Convolutional Neural Networks\nBoris Knyazev∗†, Erhardt Barth† and Thomas Martinetz†\n∗Bauman Moscow State Technical University\nEmail: bknyazev@bmstu.ru, borknyaz@gmail.com\n†Institut für Neuro- und Bioinformatik, University of Lübeck\nEmail: {barth, martinetz}@inb.uni-luebeck.de\nAbstract—In visual recognition tasks, such as image classiﬁ-\ncation, unsupervised learning exploits cheap unlabeled data and\ncan help to solve these tasks more efﬁciently. We show that the\nrecursive autoconvolution operator, adopted from physics, boosts\nexisting unsupervised methods by learning more discriminative\nﬁlters. We take well established convolutional neural networks\nand train their ﬁlters layer-wise. In addition, based on previous\nworks we design a network which extracts more than 600k\nfeatures per sample, but with the total number of trainable\nparameters greatly reduced by introducing shared ﬁlters in\nhigher layers. We evaluate our networks on the MNIST, CIFAR-\n10, CIFAR-100 and STL-10 image classiﬁcation benchmarks and\nreport several state of the art results among other unsupervised\nmethods.\nI. INTRODUCTION\nLarge-scale visual tasks can now be solved with big deep\nneural networks, if thousands of labeled samples are available\nand if training time is not an issue. Efﬁcient GPU implemen-\ntations of standard computational blocks make training and\ntesting feasible.\nA major drawback of supervised neural networks is that they\nheavily rely on labeled data. It is true that in real applications\nit does not really matter which methods are used to achieve\nthe desired outcome. But in some cases, labeling can be an\nexpensive process. However, visual data are full of abstract\nfeatures unrelated to object classes. Unsupervised learning\nexploits abundant amounts of these cheap unlabeled data and\ncan help to solve the same tasks more efﬁciently. In general,\nunsupervised learning is important for moving towards artiﬁcial\nintelligence [1].\nIn this work, we learn a visual representation model for\nimage classiﬁcation, which is particularly effective (in terms of\naccuracy) when the number of labels is relatively small. As a\nresult, our model can potentially be successfully applied to other\ntasks (e.g., biomedical data analysis or anomaly detection), in\nwhich label information is often scarce or expensive.\nWe are inspired by the previous works in which network\nﬁlters (or weights) are learned layer-wise without label infor-\nmation [2, 3, 4, 5, 6, 7, 8]. In accordance with these works,\nwe thoroughly validate our models and conﬁrm its advantage\nin the tasks with only few labeled training samples, such as\nSTL-10 [3] and reduced variants of MNIST [9] and CIFAR-\n10 [10]. In addition, on full variants of these datasets and\non CIFAR-100 we demonstrate that unsupervised learning is\nn = 0\nn = 1\nn = 2\n(a)\nn = 0\nn = 1\nn = 2\n(b)\nn = 0\nn = 1\nn = 2\n(c)\nn = 0\nn = 1\nn = 2\n(d)\nFig. 1: Recursive autoconvolution (Eq. 2) of orders n = 0, 1, 2 applied to\nsamples: MNIST (a); CIFAR-10 (b); STL-10 (c,d). Note how the patterns\nobtained for n = 1, 2 (highlighted in green frames) are similar to the low and\nmid-level features of CNNs (e.g., visualized in Fig. 2 in [15]). The novelty of\nthis work is that we learn ﬁlters from these patterns.\nsteadily approaching performance levels of supervised models,\nincluding convolutional neural networks (CNNs) trained by\nbackpropagation on thousands of labeled samples [11, 12, 13].\nThis way, we provide further evidence that unsupervised learn-\ning is promising for building efﬁcient visual representations.\nThe main contribution of this work is adaptation of the\nrecursive autoconvolution operator [14] for convolutional\narchitectures. Concretely, we demonstrate that this operator\ncan be used together with existing clustering methods (e.g., k-\nmeans) or other learning methods (e.g., independent component\nanalysis (ICA)) to train ﬁlters that resemble the ones learned by\nCNNs and, consequently, are more discriminative (Fig. 1, 2 and\nSections III, IV-A). Secondly, we substantially reduce the total\nnumber of learned ﬁlters in higher layers of some networks\nwithout loss of classiﬁcation accuracy (Section IV-A3), which\nallows us to train larger models (such as our AutoCNN-L32\nwith over 600k features in the output). Finally, we report\nseveral state of the art results among unsupervised methods\nwhile keeping computational cost relatively low (Section V-D).\nII. RELATED WORK\nUnsupervised learning is used quite often as an additional\nregularizer in the form of weights initialization [16] or\nreconstruction cost [13, 17], or as an independent visual model\ntrained on still images [18, 10] or image sequences [19].\nHowever, to a larger extent, our method is related to another\nseries of works [2, 3, 4, 5, 20, 8] and, in particular, [6, 7],\nin which ﬁlters (or some basis) are learned layer-wise and,\narXiv:1606.00611v2  [cs.CV]  26 Mar 2017\ncontrary to the methods above, neither backpropagation nor\nﬁne tuning is used.\nIn these works, learning ﬁlters with clustering methods, such\nas k-means, is a standard approach [3, 20, 8, 7, 6]. For this\nreason, and to make comparison of our results easier, k-means\nis also adopted in this work as a default method. Moreover,\nclustering methods can learn overcomplete dictionaries without\nadditional modiﬁcations, such as done for ICA in [5]. Nev-\nertheless, since ICA [21] is also a common practice to learn\nﬁlters, we conduct a couple of simple experiments with this\nmethod, as well as with principal component analysis (PCA),\nto probe our novel idea more thoroughly. In contrast to various\npopular coding schemes [2, 3, 4, 8], our forward pass is built\nupon a well established supervised method - a convolutional\nneural network [9].\nRecently, convolutional networks were successfully trained\nlayer-wise in an unsupervised way [6, 7]. In their works, as well\nas in our work, the forward pass is mostly kept standard, while\nmethods to learn stronger (in terms of classiﬁcation) ﬁlters\nare developed. For instance, in [6], k-means is enhanced by\nintroducing convolutional clustering. Convolutional extension\nof clustering and coding methods is one of the ways to\nreduce redundancy in ﬁlters and improve classiﬁcation, e.g.,\nconvolutional sparse coding [22]. In this work, we suggest\nanother concept of making ﬁlters more powerful, namely, by\nrecursive autoconvolution applied to image patches before\nunsupervised learning.\nAutoconvolution (or self-convolution) and its properties seem\nto have been ﬁrst analyzed in physics (in spectroscopy [23])\nand later in function optimization [24] as the problem of\ndeautoconvolution arose. This operator also appeared in visual\ntasks to extract invariant patterns [25]. But, to the best of our\nknowledge, its recursive version, used as a pillar in this work,\nwas ﬁrst suggested in [14] for parametric description of images\nand temporal sequences. By contrast, we use this operator to\nlearn convolution kernels in a multilayer CNN, which we refer\nto as an AutoCNN.\nIII. AUTOCONVOLUTION\nOur ﬁlter learning procedure, discussed further in Section\nIV-A, is largely based on autoconvolution and its recursive\nextension. In this and the next sections, we formulate the basics\nof these operators in general terms.\nWe ﬁrst describe the routine for processing arbitrary discrete\ndata based on autoconvolution. It is convenient to consider\nautoconvolution in the frequency domain. According to the\nconvolution theorem, for N-dimensional discrete signals X\nand Y, such as images (N = 2): F(X∗Y) = kF(X)◦F(Y),\nwhere F - the N-dimensional forward discrete Fourier trans-\nform (DFT), ◦- point-wise matrix product, k - a normalizing\ncoefﬁcient (which will be ignored further, since we apply\nnormalization afterwards). Hence, autoconvolution is deﬁned\nas\nX ∗X = F−1(F(X)2),\n(1)\nwhere F−1 - the N-dimensional inverse DFT, ∗- convolution.\nBecause of squared frequencies, some phase information is\nn = 0\n(a)\nn = 1\nn = 2\nn = 3\n(b)\n(c)\n(d)\nFig. 2: Examples of ﬁlters D(1) of size 13 × 13 pixels learned using k-means\non the MNIST (a,b) and CIFAR-10 (c,d) patches with (b,d) and without (a,c)\nwhitening for recursive autoconvolution orders n = 0, 1, 2, 3. In each case,\n16 ﬁlters are learned. Note that ﬁlters are more sparse for higher n. Similar\nﬁlters are learned in the ﬁrst layer of our AutoCNN networks.\nlost and the inverse operation becomes ill-posed [24]. In this\nwork, we do not address this problem.\nTo extract patterns from X, it is necessary to make sure that\nmean(X) = 0 and std(X) > 0 before computing (1). Also,\nto compute linear autoconvolution, X is ﬁrst zero-padded, i.e.\nfor one dimensional case X have to be padded with zeros to\nlength (2s −1), where s - length of X before padding.\nA. Recursive autoconvolution\nWe adopt the recursive autoconvolution (RA) operator,\nproposed earlier in [14], which is an extension of (1):\nXn = Xn−1 ∗Xn−1 = F−1(F(Xn−1)2),\n(2)\nwhere n = [0, nmax] - an index of the recursive iteration (or\nautoconvolution order). If n = 0, X0 equals the input, i.e. a\nraw image patch. For n = 1 Eq. 2 becomes equal Eq. 1. In\nour work, we limit nmax = 3 as higher orders do not lead to\nbetter classiﬁcation results.\nIn [14], image patterns extracted using this operator were\nused for parametric description of images. The novelty of this\nwork is that we use extracted patterns as convolution kernels,\ni.e. ﬁlters, because we noticed that applying Eq. 2 with n ≥1\nto image patches provides sparse wavelet-like patterns (Fig. 1,\n2), which are usually learned by a CNN in the ﬁrst layer (see\n[12], Fig. 3 or [19], Fig. 6) or by other unsupervised learning\nmethods, e.g., ICA (see [5], Fig. 1), or sparse coding (see [2],\nFig. 2). In addition, these patterns are similar to more complex\nmid-level features of CNNs (e.g., visualized in Fig. 2 in [15]).\nIV. AUTOCNN ARCHITECTURE\nIn this section, we describe the architecture of a multilayer\nconvolutional neural network (AutoCNN), which we train\nin an unsupervised way and then use as a feature extractor\n(Fig. 3). It is based on classical CNNs [9], however, there\nis no backward pass and neither ﬁlters nor connections\nare trained using labels. In addition, based on [6, 7] we\ndesign two networks (AutoCNN-S32 and AutoCNN-L32) with\nfeatures randomly split into several (32) groups (see Section\nIV-A2), which is not typical for CNNs, but important in our\nwork. Since we have only forward pass, we are not limited\nby differentiable functions and can add nonlinearities such\nas Rootsift normalization (sign(x)\np\n|x|/∥x∥1) [26], which\nimproves classiﬁcation signiﬁcantly for some datasets. Batch\nINPUT from X(0): \n32x32x3\nFeature maps X(1):\n16x16x32x32\nFilters D(2): \n5x5x32x256\nReLU\nMaxPool\nLCN\nGrouping\nFeat. maps X(2):\n8x8x256x32\nFilters D(3): \n5x5x256x1024\nFeat. maps X(3):\n4x4x1024x32\nBatch norm \nConv\nAbs\nMaxPool\nLCN\nAbs\nMaxPool\nRootsift\nBatch norm \nConv\nRootsift\nBatch norm \nConv\nFilters D(1): \n5x5x3x1024\nRA\nRA\nRA\nOUTPUT:\n672k features\nmultidict\nFig. 3: Overview of our best architecture for CIFAR-10 (AutoCNN-L32). Filters\nD(1), D(2) and D(3) are learned with unsupervised learning and recursive\nautoconvolution (RA). Features X(1), X(2) are pooled and concatenated with\nX(3) to form a large feature vector (672k values). Steps that are usually\nmissing in CNNs are in bold.\nstandardization 1 (or ’batch norm’) is employed for all networks\nbefore convolutions - an entire batch is treated as a vector. In\nsome cases (see details in Table VI) we follow [7] and apply\nLocal Contrast Normalization (LCN) between layers, but we\ndo it after pooling for speedup.\nThe architectures of all networks used in this work are\nsummarized in Table VII.\nA. Training the network\nTraining our network is analogous to previous works on layer-\nwise learning [6, 7]. We start by learning ﬁlters D(1) for layer\nl = 1 using training (unlabeled) images X(0), then process\nthese images with the learned ﬁlters D(1) followed by rectiﬁ-\ncation, pooling, normalization and further steps (depending on\nthe architecture) that yield the ﬁrst layer feature maps X(1).\nUsing these features, we train ﬁlters D(2) for layer 2, and then\nprocess them X(1) with the learned ﬁlters D(2), and so forth.\nFeatures concatenated from all layers (multidictionary features)\nare used for classiﬁcation with support vector machines (SVM).\nTraining ﬁlters D(1) ∈Rs1×s1×d1×K1 for layer 1, that is\nK1 ﬁlters of size s1 × s1 with d1 color channels, is performed\nin three steps: (1) random patches X(1) ∈Rs1×s1×d1 are\nextracted from all training (unlabeled) samples; (2) recursive\nautoconvolution (Eq. 2) is applied to them; (3) one of the unsu-\npervised learning methods is applied to the autoconvolutional\npatches. Filters for the following layers are trained according\nto this procedure as well. Unless otherwise stated, k-means\nwith Euclidean distance is employed for ﬁlter learning.\n1) Learning with recursive autoconvolution (RA): Learning\nﬁlters with RA is a novel idea, so we explain some steps of its\nefﬁcient application to our tasks. One of the issues with RA is\nthat the spatial size of image patches X is doubled after each\niteration n due to zero-padding (see Eq. 1 and 2). To learn\nﬁlters from patches, we need all the patches to have some ﬁxed\nsize, so we simply take the central part of the result or resize\n(subsample) it to its original size after each iteration (Fig. 1,\nwhere the second option is picked). We randomly choose one\nof these options to make the set of patches richer.\n1Along this work, vector x is considered standardized if its mean(x) = 0\nand std(x) = 1.\nNext, according to our statistics of extracted patches, pre-\nsented in Fig. 4(a), autoconvolution order n is inversely propor-\ntional to the joint spatial σxy and frequency σuv resolution, i.e.\nn ∼1/(σxyσuv), where σxy = σxσy = √D1D2 and D1, D2\n- are eigenvalues of the weighted covariance matrix of X in\nthe spatial domain; analogously for σuv. Therefore, to cover a\nwider range of spatio-frequency properties and to learn a more\ndiverse set of ﬁlters, patches extracted with several orders are\ncombined into one global set. That is, we take results of several\norders (e.g., in case n = [0, 3] we have 4 patches instead of 1)\nand put them into one global set of autoconvolutional patches.\nNote that in case n = 0, we extract more patches to make the\ntotal number of input data points for k-means about the same\nas for combinations of orders. In this global set, all patches\nare ﬁrst scaled to have values in the range [0,1], then they\nare ZCA-whitened as in [3, 6, 7, 10]. Even though, with RA\nwe can extract Gabor-like patterns without whitening (see Fig.\n2(a),(c)), such preprocessing is essential for k-means to learn\nless correlated ﬁlters. For this whitened set k-means produces\na set of Kl data points (a dictionary) D(l) ∈Rsl×sl×dl×Kl.\nThese data points are ﬁrst l2-normalized and then used as\nconvolution kernels (ﬁlters) for layer l.\n2) Grouping of feature maps: Filters of the second and\nfollowing layers can be trained either in the same way as of the\nﬁrst layer or using a slightly modiﬁed procedure borrowed from\n[6, 7]. Concretely, features X(l), l > 1 can be split (randomly\nin this work) into Gl groups. It is useful in practice, because\nﬁlters of layer l + 1 will have smaller depth dl+1 = Kl/Gl\n(instead of dl+1 = Kl as in typical CNNs) and smaller overall\ndimensionality, so that it is easier to learn such ﬁlters with k-\nmeans. At the same time, the number of features becomes larger\nby factor Gl, and more features usually improve classiﬁcation\nin case of unsupervised learning.\n3) Shared ﬁlters for Gl > 1: The disadvantage of such\nsplitting is that ﬁlters have to be learned for each group indepen-\ndently, i.e. it is necessary to run k-means Gl times. For instance,\nfor the second layer we have D(2) ∈Rs2×s2×d2×K2×G1, i.e.\nthe total number of ﬁlters equals K2 × G1. Our contribution\nis that instead of treating groups independently, we learn\nﬁlters for all groups taken together without negative effect\non classiﬁcation accuracy (see results in Table Ib). In this\ncase, patches from all G1 feature map groups are concatenated\nbefore clustering and k-means is run only once. Thus, the total\nnumber of ﬁlters in layer 2 becomes equal K2, i.e. the same\nﬁlters are shared between all G1 groups. This trick enables us\nto learn our large AutoCNN-L32 with G1 = 32 in a feasible\ntime.\nV. EXPERIMENTS\nA. Experimental setup\nWe evaluate our method on four image classiﬁcation bench-\nmarks: MNIST [9], CIFAR-10, CIFAR-100 [10] and STL-10\n[3]. To demonstrate that unsupervised learning is particularly\neffective for datasets with few training samples, such as STL-\n10, which has only 100 training images per class, we test our\nmethod on reduced versions of MNIST and CIFAR-10, namely\n0.006 0.1\n0.2\n0.3\n0.4\n0.5\n0\n100\n200\n300\nσxyσuv\nfrequency\n(a) Distribution on CIFAR-10\n5\n7\n9\n11\n13\n15\n98.2\n98.4\n98.6\n98.8\nﬁlter size (s1), px\ncross-validation accuracy, %\n(b) MNIST (10k/10k, 28x28)\n5\n7\n9\n11\n13\n15\n17\n62\n64\n66\n68\nﬁlter size (s1), px\ncross-validation accuracy, %\n(c) CIFAR-10 (10k/10k, 32x32x3)\n9\n13\n17\n21\n25\n29\n54\n56\n58\nﬁlter size (s1), px\ntest accuracy, %\n(d) STL-10 (1k/8k, 96x96x3)\nno RA (n = 0)\nRA (n = 1)\nRA (n = 2)\nRA (n = [0, 3])\nno RA (K1 = 96; n = 0)\nRA (K1 = 96; n = [1, 3])\nno RA (K1 = 192; n = 0)\nRA (K1 = 192; n = [1, 3])\nno RA (K1 = 96; n = 0)\nRA (K1 = 96; n = [0, 3])\nno RA (K1 = 256; n = 0)\nRA (K1 = 256; n = [0, 3])\nno RA (K1 = 512; n = 0)\nRA (K1 = 512; n = [0, 3])\nFig. 4: (a) Recursive autoconvolution (RA) makes ﬁlters more localized and sparse as shown by distributions of the joint spatial and frequency resolution\nσxyσuv for the ﬁrst layer ﬁlters learned on CIFAR-10 with k-means with different orders n. (b-d) For our simple single layer network (AutoCNN-S1) RA\nimproves results for a wide range of ﬁlter sizes s1 and number of ﬁlters (K1) on three datasets: MNIST (b), CIFAR-10 (c), STL-10 (d). Not that in some\ncases results with RA are better than without RA even if the latter has two times more ﬁlters. The number of training/test samples and input image sizes in the\ndatasets are indicated in parentheses for reference.\nMNIST (100), MNIST (300) and CIFAR-10 (400) with just\n100, 300 and 400 training images per class respectively. We\nfollow the same experimental protocol as in previous works,\ne.g., [6, 18]: 10 random subsets (folds) from the training set\nare drawn, while the test set remains ﬁxed. For STL-10 these\nfolds are predeﬁned. Unless otherwise stated, we report average\nclassiﬁcation accuracies (or errors on MNIST) in percent; on\nreduced datasets, these results are averaged over 10 runs. In\nTables Ia-V better results are indicated in bold. Images of all\ndatasets, except for MNIST, are ZCA-whitened as in most\nprevious works (see details in Table VI).\nWhile in previous works all labeled training samples are\ntypically used as unlabeled data during unsupervised learning,\nwe found that it is enough to use at most 10k-20k samples\nto learn ﬁlters in all experiments, including STL-10, which\ncontains 100k unlabeled samples.\nWe train models with 1-3 layers according to the proposed\narchitecture (Section IV). The details of all models are\npresented in Table VII. Network parameters (number of layers\nand ﬁlters, pooling size and stride, etc.) are chosen so that to\nmake them more consistent with previous works [18, 27, 7, 6]\nand within this work.\nIn case of a multilayer network, we use multidictionary\nfeatures for classiﬁcation, which is a standard approach in\nunsupervised learning [6, 8, 18]. For example, in case of three\nlayers, features of bottom layers (l = 1, 2) are pooled so that\ntheir spatial size equals the size of the third layer features,\nafterwards they are concatenated.\nIn all cases, except for AutoCNN-L32, a linear SVM is used\nas a classiﬁer.\nB. Effect of ﬁlter size and number of ﬁlters\nFirst, we experiment with a simple single layer architecture\n(AutoCNN-S1) to see the effect of recursive autoconvolution\n(Eq. 2) on classiﬁcation performance depending on the ﬁlter\nsize (s1) and the number of ﬁlters (K1) (Fig. 4 (b)-(d)).\nIn this experiment, in case of MNIST and CIFAR-10, 10-\nfold cross-validation is performed with 10k training and 10k\ntest samples drawn from the original training sets. For STL-\n10 we use the original predeﬁned folds. We observe that\nrecursive autoconvolution consistently improves classiﬁcation.\nIt is evident especially for larger ﬁlters, because RA makes\nﬁlters more localized and sparse (Fig. 4(a)). This experiment\nalso shows that the results generalize well across different\ndatasets and preprocessing strategies.\nWe also experimented with each of the orders in Eq. 2\nindependently (results not shown) to determine ﬁlters of which\norders contribute the most during classiﬁcation, but we found\nthat, generally, combinations of orders work best.\nTABLE I: (a) Comparison of learning algorithms on CIFAR-10(400) using\nAutoCNN-S1-256. (b) Shared vs independent ﬁlters on CIFAR-10(400) for a\ntwo layer model AutoCNN-S32 + RA (N - number of trainable parameters, †:\ntime for learning ﬁlters with k-means for 32 groups measured on Intel Xeon\nCPU E5-2620v3.).\n(a)\nLearning method Raw\nRA\nk-means\n64.0 65.6\nPCA (s1 ≥11)\n60.7 59.8\nICA (s1 ≥11)\n62.8 64.5\n(b)\nFilters\nAvg acc N Train t †\nIndepend. 72.0 432.5k 470s\nShared\n71.9 43.6k\n20s\n1) Recursive autoconvolution and ICA or PCA: To demon-\nstrate generalizability of our method, we investigated if\nrecursive autoconvolution is able to improve other learning\nmethods. For this purpose, we trained K1 = 256 ﬁlters with\nprincipal (PCA) and independent (ICA) component analysis\n[21] on patches with n = 0 (Raw) and n = [0, 3] (RA) using\nthe same procedure as with k-means (Section IV-A). Filter\nsizes have to be chosen based on results from the previous\nexperiment (Fig. 4 (b)-(d)). However, we had to use larger ﬁlters\nto satisfy s1×s1×3 ≥K1, since we did not learn overcomplete\ndictionaries. Even though RA does not improve PCA ﬁlters\nfor some reasons, in case of ICA the gap between results\nwith and without RA is notable (Table Ia), which conﬁrms the\neffectiveness of our method.\nC. Multilayer performance\nWe performed a series of experiments with convolutional\narchitectures that match or similar to the ones in the previous\nworks on unsupervised learning [18, 27]. It allows us to fairly\ncompare features and to see the beneﬁt of RA for a multilayer\nAutoCNN.\n1) Comparison to Exemplar-CNN: Compared to Exemplar-\nCNNs [18], our results appear to be slightly inferior at ﬁrst\n(Table II), but our networks have only 3 layers and we do\nnot exploit data augmentation so extensively. For instance, we\ndo not use any color augmentation (see details in Table VI).\nTo improve our results, we trained a larger 3 layer network\n(AutoCNN-L), and were able to outperform Exemplar-CNNs\non CIFAR-10(400).\nTABLE II: Comparison of our features with and without RA to Exemplar-CNN\n[18] (A: data augmentation, A∗: large data augmentation, average accuracy\n(%) for 10 folds is reported).\nModel\nCIFAR-10(400)\nSTL-10\n# ﬁlters\nExemplar-CNN + A∗\n75.7\n74.9\n64-128-256-512\nExemplar-CNN + A∗\n77.4\n75.4\n92-256-512-1024\nRaw\nRA\nRaw\nRA\nAutoCNN-S\n66.5\n68.4\n60.6\n62.3\n64-128-256\nAutoCNN-S - Rootsift 60.9\n61.0\n51.9\n52.5\n64-128-256\nAutoCNN-S + A\n69.9\n72.1\n67.6\n69.2\n64-128-256\nAutoCNN-M + A\n72.2\n74.5\n69.7 70.6\n92-256-512\nAutoCNN-L + A\n75.9\n77.6\n72.4\n73.1\n256-1024-2048\nNote, however, that for all models in Table II the results with\nRA are better than without (columns ’Raw’), while all model\nsettings are kept identical. In fact, for CIFAR-10(400) the\nnetwork without recursive autoconvolution has to be virtually\ndoubled in size in order to catch up wih the score of the network\nwith RA (compare results of AutoCNN-S and AutoCNN-M).\nUsing a small AutoCNN-S network we estimated the\ncontribution of Rootsift [26] to our results by removing it\nfrom our pipeline. Not only it boosts classiﬁcation, but also\nallows recursive autoconvolution to work more efﬁciently for\nmultilayer models. However, note that our plots in Fig. 4 and\nall results on MNIST are obtained without it, so Rootsift is\nnot always required.\n2) Comparison to CONV-WTA: Compared to the Winner\nTakes All Autoencoder (CONV-WTA) [27], our networks are\nmore efﬁcient even with a smaller number of ﬁlters (Table III).\nWith RA our results are further improved. In a few cases we\nhad to project features with PCA to 4096 (PCA4k) dimensions\nbefore classiﬁcation to ﬁt all CIFAR-10 features into memory. It\nis an undesirable operation, because PCA degrades our features.\nWe\nalso\ncompare\nour\nmodels\nto\nCONV-WTA\non\nMNIST(100) and full MNIST (Table IV). Two layer AutoCNN-\nS2 with recursive autoconvolution yields better results than\nCONV-WTA, even though our model has two times fewer ﬁlters\nin the second layer (other parameters are the same, Rootsift is\nnot used on MNIST). Our error of 0.39% on full MNIST is\ncompetitive even when compared to supervised learning. For\ninstance, the same error was reported in [12] and 0.47% in\n[11] with 2 and 3 layer convolutional networks respectively.\nWithout RA our model performs signiﬁcantly worse.\nTABLE III: Comparison of our features with and without RA to CONV-WTA\n[27] on full CIFAR-10 (A: data augmentation).\nModel\nCIFAR-10\n# ﬁlters\nCONV-WTA\n72.3\n256\nCONV-WTA\n77.9\n256-1024\nCONV-WTA\n80.1\n256-1024-4096\nRaw\nRA\nAutoCNN-S1-256\n71.3 72.4\n256\nAutoCNN-M2\n80.0 80.4\n256-1024\nAutoCNN-M2 + PCA4k\n77.4 78.9\n256-1024\nAutoCNN-L + PCA4k\n80.1 81.4 256-1024-2048\nAutoCNN-L + PCA4k + A 82.7 84.4 256-1024-2048\nTABLE IV: Classiﬁcation errors on MNIST using unsupervised methods (for\nMNIST(100) the format is error ± std).\nModel\nMNIST (100)\nMNIST\n# ﬁlters\nSparse coding [2]\n−\n0.59\n169\nC-SVDDNet [28]\n−\n0.35\n400\nCONV-WTA [27]\n−\n0.64\n128\nCONV-WTA [27]\n1.92\n0.48\n128-2048\nAutoCNN-S1-128\n2.89 ± 0.16\n0.82\n128\nAutoCNN-S1-128 + RA 2.45 ± 0.10\n0.69\n128\nAutoCNN-S2\n1.91 ± 0.10\n0.57\n128-1024\nAutoCNN-S2 + RA\n1.75 ± 0.10\n0.39\n128-1024\nSemi-supervised best\n0.84 ± 0.08 [17]\n0.5 [6]\nD. Performance of large AutoCNN-L32\nUnsupervised layer-wise learning does not suffer from\noverﬁtting, but making layers wider than in our AutoCNN-L is\ncomputationally demanding. Meanwhile, splitting features into\nGl groups (see Section IV-A2) provides more features, and in\ncertain settings it also beneﬁts classiﬁcation. So we designed\na large AutoCNN-L32 network with Gl = 32 groups. Due\nto feature splitting and the idea of shared ﬁlters introduced\nearlier in this work (Section IV-A3 and Table Ib), training\ntime remains comparable to AutoCNN-L. We also tried other\narchitectures, but present results only with the best one.\n1) Dimension reduction and classiﬁcation: Output features\nof AutoCNN-L32 are prohibitively large, so we ﬁrst apply\nrandomized principal component analysis [31] together with\nwhitening and then train an SVM on the projected data. In this\ncase, we use an RBF-SVM in an efﬁcient GPU implementation\n[32]. A nonlinear SVM is chosen, because it performs better\nwith the projected features. The SVM regularization constant\nis ﬁxed to C = 16 and the width of the RBF kernel is chosen\nto be γ = 1/NP CA, where NP CA is the dimensionality of the\nprojected feature vector, which is the input of the SVM.\n2) Evaluation: AutoCNN-L32 achieves very competitive\nresults on both reduced and full datasets (Table V). We found\nthat for layers 2-3 of this particular model it is better to train\nﬁlters with PCA rather than with k-means. It is surprising,\nbecause according to our results with a single layer model\n(Table Ia), PCA should be worse than k-means in this task. But\nTABLE V: Classiﬁcation accuracies on the test sets using unsupervised methods (A: data augmentation, †: obtained with a larger network and supervised\nlearning of connections between layers).\nModel\nCIFAR-10(400)\nCIFAR-10\nSTL-10\nCIFAR-100\n# ﬁlters\nNOMP-20 [8]\n72.2 ± 0.4\n82.9\n67.9 ± 0.6\n60.8\n3200-6400-6400\nConv. clustering, unsup. [6]\n−\n−\n65.4 (74.1†)\n−\n96-1536\nCONV-WTA [27]\n−\n80.1\n−\n−\n256-1024-4096\nCommittee of nets + A [7]\n−\n−\n68.0 ± 0.6\n−\n300-5625\nk-means + A [20]\n72.6 ± 0.7\n81.9\n63.7\n−\n3 layers, 6400\nExemplar-CNN + A∗[18]\n77.4 ± 0.2\n84.3\n75.4 ± 0.3\n−\n92-256-512-1024\nAutoCNN-L32 + RA + PCA1k\n76.4 ± 0.4\n85.4\n68.7 ± 0.5\n63.9\n1024-256-1024\nAutoCNN-L32 + A + PCA1.5k\n78.2 ± 0.3\n87.1\n74.0 ± 0.6\n67.1\nAutoCNN-L32 + RA + A + PCA1.5k\n79.4 ± 0.3\n87.9\n74.5 ± 0.6\n67.8\nPretrained on ImageNet\n73.8 ± 0.4 [29]\n89.1 [30]\n−\n67.7 [30]\nSemi-supervised state of the art\n79.6 ± 0.5 [17]\n92.2 [13]\n74.3 [13]\n69.1 [13]\nwe obtain just 77.4±0.3% on CIFAR-10(400) with AutoCNN-\nL32 if k-means is used for all layers. To obtain results on\nCIFAR-100 we use the same settings as on CIFAR-10.\nE. Analysis of results\nNotably, in most of the previous works (see Tables IV and\nV), the classiﬁcation results are very good either for simple\ngrayscale images (MNIST) or more complex colored datasets\n(CIFAR-10, CIFAR-100, STL-10) or for smaller or larger\ndatasets only. The only exception seems to be Ladder Networks\n[17], which are much larger and deeper than our models and\nare semi-supervised. Also, in [28] better results are achieved\non MNIST, but it can be quite easy to ﬁne tune to such a\nsimple task 2. Our models are especially effective for CIFAR-\n10, showing performance close to Ladder Networks. Our results\non this dataset (both reduced and full) are several percent higher\n(absolute difference) compared to other unsupervised models.\nOn full CIFAR-10 and CIFAR-100 with 5000 and 500 training\nlabels per class respectively, our models also outperform\nsome advanced fully supervised CNNs [11, 12] and better\nor comparable to CNNs pretrained in an unsupervised way\n[16] or on the large-scale dataset with over 1 million of training\nsamples [29, 30]. On STL-10 our network with RA compares\nfavorably to the 10 layer convolutional autoencoder (SWWAE)\n[13]. Other previous works, showing higher accuracies, are\neither fully or semi-supervised.\nOur deeper networks outperform our shallow ones, which\nsuggests the importance of depth in our case in the same way\nas in supervised CNNs.\nOne of the drawbacks in our work is that our best results\non CIFAR and STL-10 are obtained with a nonlinear SVM,\nbut we believe that given the overall simplicity of our models\nit can be reasonable to use a more complex kernel. Moreover,\nin our experience the RBF kernel does not always improve\nclassiﬁcation results if used on top of CNN features, but does\nin our case. In addition, we show competitive results with\na linear SVM in Tables Ia-IV. We also do not thoroughly\nvalidate all model components (which would be out of scope),\nbecause the main intention of this work is to show that recursive\nautoconvolution boosts classiﬁcation in a number of different\nsettings and is important to achieve our best results.\n2See https://github.com/bknyaz/gabors\nIn this work, we show good results both for simple and more\ncomplex datasets, as well as both for smaller and larger ones\nusing the same model with few tuning parameters. Among\nunsupervised learning methods without data augmentation we\nreport state of the art results for all datasets. One of the main\nreasons for such results is that recursive autoconvolution allows\nus to learn a set of ﬁlters with rich spatio-frequency properties.\nIn AutoCNNs, most of the learned ﬁlters have joint spatial\nand frequency resolution close to the theoretical minimum\n(Fig. 4(a), RA with n = [0, 3]), i.e. they resemble simple edge\ndetectors, such as Gabor ﬁlters. Other ﬁlters are uniformly\ndistributed across a wide range of resolutions and typically\nhave complex meaningful shapes (Fig. 1, 2). By learning large\nsets of such ﬁlters our models are able to detect a lot of\nhighly diverse features, so that an SVM can easily choose the\ndiscriminative ones.\na) Computational complexity.: Note that the numbers of\nnetwork ﬁlters presented in Tables II-V do not always reﬂect\nthe total number of trainable parameters (N) nor the total\ncomputational cost. While our 3 layer AutoCNN-L32 model\nhas more ﬁlters than some CNNs, for convolutional layers it\nhas N ≈6.8·106 parameters, while both the largest Exemplar-\nCNN [18] and CONV-WTA [27] models have ≈40 · 106\nparameters (in case of CIFAR-10). Training our large 3 layer\nnetwork on full CIFAR-10 with data augmentation takes about\n85 minutes on NVIDIA GTX 980 Ti, Intel Xeon CPU E5-\n2620v3 and 64GB RAM in a Matlab implementation with\nVLFeat [33], MatConvNet [34] and GTSVM [32]. It’s also\nimportant that computational cost of recursive autoconvolution\nfor ﬁlter learning is less than 10% of overall training time.\nSource codes to reproduce our results are available at\nhttps://github.com/bknyaz/autocnn_unsup.\nVI. CONCLUSION\nThe importance of unsupervised learning in visual tasks\nis increasing and development is driven by the necessity to\nbetter exploit massive amounts of unlabeled data. We propose\na novel idea for unsupervised feature learning and report\ncompetitive results in several image classiﬁcation tasks among\nthe works not relying on supervised learning. Moreover, we\nreport state of the art results among unsupervised methods\nwithout data augmentation. We adopt recursive autoconvolution\nand demonstrate its great utility for unsupervised learning\nmethods, such as k-means and ICA. We argue that it can also\nbe integrated into more recent learning methods, such as convo-\nlutional clustering, to boost their performance. Furthermore, we\nsigniﬁcantly reduce the total number of trainable parameters by\nusing shared ﬁlters. As a result, the proposed autoconvolutional\nnetwork performs better than most of the unsupervised, and\nseveral supervised, models in various classiﬁcation tasks with\nonly few, but also with thousands of, labeled samples.\nACKNOWLEDGMENT\nThis work is jointly supported by the German Academic\nExchange Service (DAAD) and the Ministry of Education and\nScience of the Russian Federation (project number 3708).\nREFERENCES\n[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep\nlearning. Nature, 521(7553):436–444, 2015.\n[2] Kai Labusch, Erhardt Barth, and Thomas Martinetz. Simple\nmethod for high-performance digit recognition based on sparse\ncoding. Neural Networks, IEEE Transactions on, 19(11):1985–\n1989, 2008.\n[3] Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis\nof single-layer networks in unsupervised feature learning. In\nInternational conference on artiﬁcial intelligence and statistics,\npages 215–223, 2011.\n[4] Adam Coates and Andrew Y Ng. The importance of encoding\nversus training with sparse coding and vector quantization. In\nICML, pages 921–928, 2011.\n[5] Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Y\nNg. ICA with reconstruction cost for efﬁcient overcomplete\nfeature learning. In NIPS, pages 1017–1025, 2011.\n[6] Aysegul Dundar, Jonghoon Jin, and Eugenio Culurciello. Con-\nvolutional clustering for unsupervised learning. arXiv preprint\narXiv:1511.06241v2, 2016.\n[7] Bogdan Miclut, Thomas Kaester, Thomas Martinetz, and Erhardt\nBarth. Committees of deep feedforward networks trained with\nfew data. arXiv preprint arXiv:1406.5947, 2014.\n[8] Tsung-Han Lin and HT Kung. Stable and efﬁcient representation\nlearning with nonnegativity constraints. Journal of Machine\nLearning Research, 2014.\n[9] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.\nGradient-based learning applied to document recognition. Pro-\nceedings of the IEEE, 86(11):2278–2324, 1998.\n[10] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers\nof features from tiny images, 2009.\n[11] Matthew D Zeiler and Rob Fergus.\nStochastic pooling for\nregularization of deep convolutional neural networks. arXiv\npreprint arXiv:1301.3557, 2013.\n[12] Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia\nSchmid. Convolutional kernel networks. In NIPS, pages 2627–\n2635, 2014.\n[13] Junbo Zhao, Michael Mathieu, Ross Goroshin, and Yann\nLecun.\nStacked what-where auto-encoders.\narXiv preprint\narXiv:1506.02351, 2015.\n[14] BA Knyazev and VM Chernenkiy. Convolutional sparse coding\nfor static and dynamic images analysis. Science & Education of\nBauman MSTU/Nauka i Obrazovanie of Bauman MSTU, 1(11),\n2014.\n[15] Matthew D Zeiler and Rob Fergus. Visualizing and understanding\nconvolutional networks. In European Conference on Computer\nVision, pages 818–833. Springer, 2014.\n[16] Tom Le Paine, Pooya Khorrami, Wei Han, and Thomas S\nHuang. An analysis of unsupervised pre-training in light of\nrecent advances. arXiv preprint arXiv:1412.6597, 2014.\n[17] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola,\nand Tapani Raiko.\nSemi-supervised learning with ladder\nnetworks. In NIPS, pages 3532–3540, 2015.\n[18] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg,\nMartin Riedmiller, and Thomas Brox.\nDiscriminative unsu-\npervised feature learning with exemplar convolutional neural\nnetworks. arXiv preprint arXiv:1406.6909v2, 2015.\n[19] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of\nvisual representations using videos. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 2794–2802,\n2015.\n[20] Ka Y Hui. Direct modeling of complex invariances for visual\nobject features. In ICML, 2013.\n[21] Aapo Hyvärinen. Fast and robust ﬁxed-point algorithms for\nindependent component analysis.\nNeural Networks, IEEE\nTransactions on, 10(3):626–634, 1999.\n[22] Hilton Bristow, Anders Eriksson, and Simon Lucey.\nFast\nconvolutional sparse coding.\nIn Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages\n391–398, 2013.\n[23] V Dose, Th Fauster, and H-J Gossmann.\nThe inversion of\nautoconvolution integrals. Journal of Computational Physics,\n41(1):34–50, 1981.\n[24] Rudolf Gorenﬂo and Bernd Hofmann. On autoconvolution and\nregularization. Inverse Problems, 1994.\n[25] Janne Heikkila. Multi-scale auto-convolution for afﬁne invariant\npattern recognition. In Pattern Recognition, 2002. Proceedings.\n16th International Conference on, volume 1, pages 119–122.\nIEEE, 2002.\n[26] Relja Arandjelovi´c and Andrew Zisserman.\nThree things\neveryone should know to improve object retrieval. In Computer\nVision and Pattern Recognition (CVPR), 2012 IEEE Conference\non, pages 2911–2918. IEEE, 2012.\n[27] Alireza Makhzani and Brendan J Frey. Winner-take-all autoen-\ncoders. In NIPS, pages 2773–2781, 2015.\n[28] Dong Wang and Xiaoyang Tan. Unsupervised feature learning\nwith C-SVDDNet. arXiv preprint arXiv:1412.7259, 2014.\n[29] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised\nrepresentation learning with deep convolutional generative\nadversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n[30] Lars Hertel, Erhardt Barth, Thomas Käster, and Thomas Mar-\ntinetz. Deep convolutional neural networks as generic feature\nextractors. In 2015 International Joint Conference on Neural\nNetworks (IJCNN), pages 1–4. IEEE, 2015.\n[31] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.\nFinding structure with randomness: Probabilistic algorithms for\nconstructing approximate matrix decompositions. SIAM review,\n53(2):217–288, 2011.\n[32] Andrew Cotter, Nathan Srebro, and Joseph Keshet. A GPU-\ntailored approach for training kernelized SVMs. In SIGKDD,\npages 805–813. ACM, 2011.\n[33] Andrea Vedaldi and Brian Fulkerson. VLFeat: An open and\nportable library of computer vision algorithms. In Proceedings\nof the 18th ACM international conference on Multimedia, pages\n1469–1472. ACM, 2010.\n[34] Andrea Vedaldi and Karel Lenc. MatConvNet: Convolutional\nneural networks for matlab. In Proceedings of the 23rd Annual\nACM Conference on Multimedia Conference, pages 689–692.\nACM, 2015.\nTABLE VI: Dataset and some model parameters used in the experiments (LCN - Local Contrast Normalization)\nDataset\nPreprocessing\nLCN (>1 layers)\nMultidictionary (>1 layers)\nData augmentation (A)\nMNIST\n−\n−\n+\n−\nCIFAR-10, CIFAR-100\nZCA-whitening\n+\n+\nmirroring\nSTL-10\nZCA-whitening\n+\n+\nmirroring, crops(72px), scaling(1-1.3), rotation(±10 degrees)\nTABLE VII: Network architectures used in the experiments\nModel\nL K1\nn1\ns1, d1\nR1\nm1\nG1\nK2\nn2\ns2, d2\nR2 m2 G2\nK3\nn3\ns3, d3\nR3 m3 root SVM\nMNIST\nAutoCNN-S1\n1\nK1\n1-3 s1xs1x1 |x|\n4(4)\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nLinear\nAutoCNN-S1-128\n1\n128\n0\n7x7x3\n|x|\n4(4)\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nLinear\nAutoCNN-S1-128 + RA\n1\n128 1-3 11x11x3 |x|\n4(4)\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nLinear\nAutoCNN-S2\n2\n128 1-3\n7x7x3\n|x|\n5(3)\n1\n1024 0-2 5x5x128 |x| 3(2) −\n−\n−\n−\n−\n−\n−\nLinear\nCIFAR-10 and CIFAR-100\nAutoCNN-S1\n1\nK1\n0-3 s1xs1x3 relu\n8(8)\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nLinear\nAutoCNN-S1-256\n1\n256\n0\n9x9x3\nrelu\n8(8)\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n+\nLinear\nAutoCNN-S1-256 + RA\n1\n256 0-3 13x13x3 relu\n8(8)\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n+\nLinear\nAutoCNN-S32\n2\n128 0-3\n9x9x3\nrelu\n3(2)\n32\n64\n0-2\n7x7x4\n|x| 5(4) −\n−\n−\n−\n−\n−\n+\nLinear\nAutoCNN-M2\n2\n256 0-3\n5x5x3\nrelu\n3(2)\n1\n1024 0-2 5x5x256 |x| 5(4) −\n−\n−\n−\n−\n−\n+\nLinear\nAutoCNN-S\n3\n64\n0-3\n5x5x3\nrelu\n3(2)\n1\n128 0-3 5x5x64\n|x| 3(2)\n1\n256 0-2 5x5x128\n|x| 3(2)\n+\nLinear\nAutoCNN-M\n3\n92\n0-3\n5x5x3\nrelu\n3(2)\n1\n256 0-3 5x5x92\n|x| 3(2)\n1\n512 0-2 5x5x256\n|x| 3(2)\n+\nLinear\nAutoCNN-L\n3\n256 0-3\n5x5x3\nrelu\n3(2)\n1\n1024 0-3 5x5x256 |x| 3(2)\n1\n2048 0-2 5x5x1024 |x| 3(2)\n+\nLinear\nAutoCNN-L32\n3 1024 0-3\n5x5x3\nrelu\n3(2)\n32\n256 0-2 5x5x32\n|x| 3(2) 32 1024 0-2 5x5x256\n|x| 3(2)\n+\nRBF\nSTL-10\nAutoCNN-S1\n1\nK1\n0-3 s1xs1x3 relu 20(20) −\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\n−\nLinear\nAutoCNN-S\n3\n64\n0-3\n7x7x3\nrelu\n5(4)\n1\n128 0-3 5x5x64\n|x| 4(3)\n1\n256 0-2 5x5x128\n|x| 3(2)\n+\nLinear\nAutoCNN-M\n3\n92\n0-3\n7x7x3\nrelu\n5(4)\n1\n256 0-3 5x5x92\n|x| 4(3)\n1\n512 0-2 5x5x256\n|x| 3(2)\n+\nLinear\nAutoCNN-L\n3\n256 0-3\n7x7x3\nrelu\n5(4)\n1\n1024 0-3 5x5x256 |x| 4(3)\n1\n2048 0-2 5x5x1024 |x| 3(2)\n+\nLinear\nAutoCNN-L32\n3 1024 0-3\n7x7x3\nrelu\n5(4)\n32\n256 0-2 5x5x32\n|x| 4(3) 32 1024 0-2 5x5x256\n|x| 3(2)\n+\nRBF\nL - number of layers;\nKl - number of ﬁlters in layer l;\nsl, dl - ﬁlter size and depth for layer l;\nnl - recursive autoconvolution (RA) order to learn ﬁlters for layer l (in case RA is not applied nl = 0);\nml - max pooling size and stride (in parentheses); for STL-10 in case of data augmentation ml = 4(3) instead of 5(4) in the ﬁrst layer;\nGl - number of feature map groups;\nRl - rectiﬁer after layer l (relu: max(0, x), |x| - absolute values);\nroot - Rootsift normalization after each layer (sign(x)\np\n|x|/∥x∥1).\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2016-06-02",
  "updated": "2017-03-26"
}