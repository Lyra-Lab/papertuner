{
  "id": "http://arxiv.org/abs/1606.06582v1",
  "title": "Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification",
  "authors": [
    "Yuting Zhang",
    "Kibok Lee",
    "Honglak Lee"
  ],
  "abstract": "Unsupervised learning and supervised learning are key research topics in deep\nlearning. However, as high-capacity supervised neural networks trained with a\nlarge amount of labels have achieved remarkable success in many computer vision\ntasks, the availability of large-scale labeled images reduced the significance\nof unsupervised learning. Inspired by the recent trend toward revisiting the\nimportance of unsupervised learning, we investigate joint supervised and\nunsupervised learning in a large-scale setting by augmenting existing neural\nnetworks with decoding pathways for reconstruction. First, we demonstrate that\nthe intermediate activations of pretrained large-scale classification networks\npreserve almost all the information of input images except a portion of local\nspatial details. Then, by end-to-end training of the entire augmented\narchitecture with the reconstructive objective, we show improvement of the\nnetwork performance for supervised tasks. We evaluate several variants of\nautoencoders, including the recently proposed \"what-where\" autoencoder that\nuses the encoder pooling switches, to study the importance of the architecture\ndesign. Taking the 16-layer VGGNet trained under the ImageNet ILSVRC 2012\nprotocol as a strong baseline for image classification, our methods improve the\nvalidation-set accuracy by a noticeable margin.",
  "text": "Augmenting Supervised Neural Networks with Unsupervised Objectives\nfor Large-scale Image Classiﬁcation\nYuting Zhang\nYUTINGZH@UMICH.EDU\nKibok Lee\nKIBOK@UMICH.EDU\nHonglak Lee\nHONGLAK@EECS.UMICH.EDU\nDepartment of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA\nAbstract\nUnsupervised learning and supervised learning\nare key research topics in deep learning. How-\never, as high-capacity supervised neural net-\nworks trained with a large amount of labels have\nachieved remarkable success in many computer\nvision tasks, the availability of large-scale la-\nbeled images reduced the signiﬁcance of un-\nsupervised learning.\nInspired by the recent\ntrend toward revisiting the importance of un-\nsupervised learning, we investigate joint super-\nvised and unsupervised learning in a large-scale\nsetting by augmenting existing neural networks\nwith decoding pathways for reconstruction. First,\nwe demonstrate that the intermediate activations\nof pretrained large-scale classiﬁcation networks\npreserve almost all the information of input im-\nages except a portion of local spatial details.\nThen, by end-to-end training of the entire aug-\nmented architecture with the reconstructive ob-\njective, we show improvement of the network\nperformance for supervised tasks.\nWe evalu-\nate several variants of autoencoders, including\nthe recently proposed “what-where\" autoencoder\nthat uses the encoder pooling switches, to study\nthe importance of the architecture design. Tak-\ning the 16-layer VGGNet trained under the Ima-\ngeNet ILSVRC 2012 protocol as a strong base-\nline for image classiﬁcation, our methods im-\nprove the validation-set accuracy by a noticeable\nmargin.\n1. Introduction\nUnsupervised and supervised learning have been two asso-\nciated key topics in deep learning. One important appli-\ncation of deep unsupervised learning over the past decade\nwas to pretrain a deep neural network, which was then\nﬁnetuned with supervised tasks (such as classiﬁcation).\nMany deep unsupervised models were proposed, such as\nstacked (denoising) autoencoders (Bengio et al., 2007; Vin-\ncent et al., 2010), deep belief networks (Hinton et al., 2006;\nLee et al., 2009), sparse encoder-decoders (Ranzato et al.,\n2007; Kavukcuoglu et al., 2010), and deep Boltzmann ma-\nchines (Salakhutdinov & Hinton, 2009). These approaches\nsigniﬁcantly improved the performance of neural networks\non supervised tasks when the amount of available labels\nwere not large.\nHowever, over the past few years, supervised learning with-\nout any unsupervised pretraining has achieved even better\nperformance, and it has become the dominating approach\nto train deep neural networks for real-world tasks, such\nas image classiﬁcation (Krizhevsky et al., 2012) and ob-\nject detection (Girshick et al., 2016). Purely supervised\nlearning allowed more ﬂexibility of network architectures,\ne.g., the inception unit (Szegedy et al., 2015) and the resid-\nual structure (He et al., 2016), which were not limited by\nthe modeling assumptions of unsupervised methods. Fur-\nthermore, the recently developed batch normalization (BN)\nmethod (Ioffe & Szegedy, 2015) has made the neural net-\nwork learning further easier. As a result, the once popu-\nlar framework of unsupervised pretraining has become less\nsigniﬁcant and even overshadowed (LeCun et al., 2015) in\nthe ﬁeld.\nSeveral attempts (e.g.,\nRanzato & Szummer (2008);\nLarochelle & Bengio (2008); Sohn et al. (2013); Goodfel-\nlow et al. (2013)) had been made to couple the unsuper-\nvised and supervised learning in the same phase, making\nunsupervised objectives able to impact the network train-\ning after supervised learning took place. These methods\nunleashed new potential of unsupervised learning, but they\nhave not yet been shown to scale to large amounts of la-\nbeled and unlabeled data. Rasmus et al. (2015) recently\nproposed an architecture that is easy to couple with a clas-\nsiﬁcation network by extending the stacked denoising au-\ntoencoder with lateral connections, i.e., from encoder to\nthe same stages of the decoder, and their methods showed\npromising semi-supervised learning results. Nonetheless,\nthe existing validations (Rasmus et al., 2015; Pezeshki\net al., 2016) were mostly on small-scale datasets like\nMNIST. Recently, Zhao et al. (2015) proposed the “what-\narXiv:1606.06582v1  [cs.LG]  21 Jun 2016\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nwhere” autoencoder (SWWAE) by extending the stacked\nconvolutional autoencoder using Zeiler et al. (2011)’s “un-\npooling” operator, which recovers the locational details\n(which was lost due to max-pooling) using the pooling\nswitches from the encoder. While achieving promising re-\nsults on the CIFAR dataset with extended unlabeled data\n(Torralba et al., 2008), SWWAE has not been demonstrated\neffective for larger-scale supervised tasks.\nIn this paper, inspired by the recent trend toward simulta-\nneous supervised and unsupervised neural network learn-\ning, we augment challenge-winning neural networks with\ndecoding pathways for reconstruction, demonstrating the\nfeasibility of improving high-capacity networks for large-\nscale image classiﬁcation. Speciﬁcally, we take a segment\nof the classiﬁcation network as the encoder and use the mir-\nrored architecture as the decoding pathway to build several\nautoencoder variants. The autoencoder framework is easy\nto construct by augmenting an existing network without\ninvolving complicated components.\nDecoding pathways\ncan be trained either separately from or together with the\nencoding/classiﬁcation pathway by the standard stochas-\ntic gradient descent methods without special tricks, such\nas noise injection and activation normalization.\nThis paper ﬁrst investigates reconstruction properties of the\nlarge-scale deep neural networks. Inspired by Dosovitskiy\n& Brox (2016), we use the auxiliary decoding pathway of\nthe stacked autoencoder to reconstruct images from inter-\nmediate activations of the pretrained classiﬁcation network.\nUsing SWWAE, we demonstrate better image reconstruc-\ntion qualities compared to the autoencoder using the un-\npooling operators with ﬁxed switches, which upsamples an\nactivation to a ﬁxed location within the kernel. This re-\nsult suggests that the intermediate (even high-level) feature\nrepresentations preserve nearly all the information of the\ninput images except for the locational details “neutralized”\nby max-pooling layers.\nBased on the above observations, we further improve the\nquality of reconstruction, an indication of the mutual infor-\nmation between the input and the feature representations\n(Vincent et al., 2010), by ﬁnetuning the entire augmented\narchitecture with supervised and unsupervised objectives.\nIn this setting, the image reconstruction loss can also im-\npact the classiﬁcation pathway. To the contrary of conven-\ntional beliefs in the ﬁeld, we demonstrate that the unsuper-\nvised learning objective posed by the auxiliary autoencoder\nis an effective way to help the classiﬁcation network obtain\nbetter local optimal solutions for supervised tasks. To the\nbest of our knowledge, this work is the ﬁrst to show that un-\nsupervised objective can improve the image classiﬁcation\naccuracy of deep convolutional neural networks on large-\nscale datasets, such as ImageNet (Deng et al., 2009). We\nsummarize our main contributions as follows:\n• We show that the feature representations learned by\nhigh-capacity neural networks preserve the input in-\nformation extremely well, despite the spatial invari-\nance induced by pooling. Our models can perform\nhigh-quality image reconstruction (i.e., “inversion”)\nfrom intermediate activations with the unpooling op-\nerator using the known switches from the encoder.\n• We successfully improve the large-scale image classi-\nﬁcation performance of a state-of-the-art classiﬁcation\nnetwork by ﬁnetuning the augmented network with a\nreconstructive decoding pathway to make its interme-\ndiate activations preserve the input information better.\n• We study several variants of the resultant autoen-\ncoder architecture, including instances of SWWAE\nand more basic versions of autoencoders, and provide\ninsight on the importance of the pooling switches and\nthe layer-wise reconstruction loss.\n2. Related work\nIn terms of using image reconstruction to improve clas-\nsiﬁcation, our work is related to supervised sparse cod-\ning and dictionary learning work, which is known to ex-\ntract sparse local features from image patches by sparsity-\nconstrained reconstruction loss functions. The extracted\nsparse features are then used for classiﬁcation purposes.\nMairal et al. (2009) proposed to combine the reconstruction\nloss of sparse coding and the classiﬁcation loss of sparse\nfeatures in a uniﬁed objective function. Yang et al. (2010)\nextended this supervised sparse coding with max-pooling\nto obtain translation-invariant local features.\nZeiler et al. (2010) proposed deconvolutional networks for\nunsupervised feature learning that consist of multiple lay-\ners of convolutional sparse coding with max-pooling. Each\nlayer is trained to reconstruct the output of the previous\nlayer. Zeiler et al. (2011) further introduced the “unpooling\nwith switches” layer to deconvolutional networks to enable\nend-to-end training.\nAs an alternative to sparse coding and discriminative con-\nvolutional networks, autoencoders (Bengio, 2009) are an-\nother class of models for representation learning, in partic-\nular for the non-linear principal component analysis (Dong\n& McAvoy, 1996; Scholz & Vigário, 2002) by minimiz-\ning the reconstruction errors of a bottlenecked neural net-\nwork. The stacked autoencoder (SAE) (Bengio et al., 2007)\nis amenable for hierarchical representation learning. With\npooling-induced sparsity bottlenecks (Makhzani & Frey,\n2015), the convolutional SAE (Masci et al., 2011) can learn\nfeatures from middle-size images. In these unsupervised\nfeature learning studies, sparsity is the key regularizer to\ninduce meaningful features in a hierarchy.\n2\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nBy injecting noises or corruptions to the input, denoising\nautoencoders (Vincent et al., 2008; 2010) can learn robust\nﬁlters to recover the uncorrupted input. Valpola (2015) fur-\nther added noises to intermediate layers of denoising auto-\nencoders with lateral connections, which was called “lad-\nder network”. Rasmus et al. (2015) combined a classiﬁca-\ntion task with the ladder network for semi-supervised learn-\ning, and they showed improved classiﬁcation accuracy on\nMNIST and CIFAR-10. Here, supervision from the labeled\ndata is the critical objective that prevents the autoencoder\nfrom learning trivial features.\nZhao et al. (2015) proposed the SWWAE, a convolutional\nautoencoder with unpooling layer, and combined it with\nclassiﬁcation objective for semi-supervised learning. This\nmodel integrates a discriminative convolutional network\n(for classiﬁcation) and a deconvolutional network (for re-\nconstruction) and can be regarded as a uniﬁcation of decon-\nvolutional networks, autoencoders and discriminative con-\nvolutional networks. They demonstrated promising results\non small scale datasets such as MNIST, SVHN and STL10.\nImproving representation learning with auxiliary tasks is\nnot new (Suddarth & Kergosien, 1990). The idea behind\nis that the harder the tasks are, the better representations\na network can learn. As an alternative to the autoencoder,\nLee et al. (2015)’s “deeply supervised network” incorpo-\nrated classiﬁcation objectives for intermediate layers, was\nable to improve the top-layer classiﬁcation accuracy for\nreasonably large-scale networks (Wang et al., 2015). In\nearlier work, Ranzato & Szummer (2008) conducted layer-\nwise training by both classiﬁcation and reconstruction ob-\njectives.\nRecently, more task-speciﬁc unsupervised ob-\njectives for image and video representation learning were\ndeveloped by using spatial context (Doersch et al., 2015)\nand video continuity (Wang & Gupta, 2015). In contrast,\nautoencoder-based methods are applicable in more general\nscenarios.\n3. Methods\nIn this section, we describe the training objectives and ar-\nchitectures of the proposed augmented network. In Sec-\ntion 3.1, we brieﬂy review the architectures of recent net-\nworks for vision tasks, and present the general form of\nour method. In Section 3.2, we augment the classiﬁcation\nnetwork with auxiliary pathways composed of deconvolu-\ntional architectures to build fully mirrored autoencoders, on\nwhich we specify the auxiliary objective functions.\n3.1. Unsupervised loss for intermediate representations\nDeep neural networks trained with full supervision\nachieved the state-of-the-art image classiﬁcation per-\nformance.\nCommonly\nused\nnetwork\narchitectures\nMax \n    pooling\n Unpooling \nw/ switch\n(SWWAE only)\nPooling \nswitches\nconv3_1\nconv3_2\nconv3_3\npool3\ndec:conv3_1\ndec: conv3_2\ndec: conv3_3\ndec: pool3\npool2\ndec: pool2\nFigure 1. Example micro-architectures in macro-layers (the 3rd\nmacro-layer of VGGNet and its mirrored decoder). Encoder: a\nnumber of convolutional layers followed by a max-pooling layer.\nDecoder: the same number of deconvolutional layers preceded by\nan unpooling layer, where the known pooling switches given by\nthe associated pooling layer are used for SWWAE.\n(Krizhevsky et al., 2012) contain a single pathway of con-\nvolutional layers succeeded by nonlinear activation func-\ntions and interleaved with max-pooling layers to gradu-\nally transform features into high-level representations and\ngain spatial invariance at different scales. Recent networks\n(Simonyan & Zisserman, 2015; Szegedy et al., 2015; He\net al., 2016; Szegedy et al., 2016) often nest a group of\nconvolutional layers before applying a max-pooling layer.\nAs these layers work together as the feature extractor for\na particular scale, we refer to the group as a macro-layer\n(see the left half of Figure 1).\nFully-connected inner-\nproduct layer and/or global average-pooling layer follow\nthe convolution-pooling macro-layers to feed the top-layer\nclassiﬁer.\nA network of L convolution-pooling macro-\nlayers is deﬁned as\nal = fl(al−1; φl), for l = 1, 2, . . . , L + 1,\n(1)\nwhere a0 = x is the input, fl(l = 1, 2, . . . , L) with the pa-\nrameter φl is the lth macro-layer, and fL+1 denotes the rest\nof the network, including the inner-product and classiﬁca-\ntion layers. The classiﬁcation loss is C(x, y) = ℓ(aL+1, y),\nwhere y is the ground truth label, and ℓis the cross-entropy\nloss when using a softmax classiﬁer.\nLet x1, x2, . . . , xN denote a set of training images asso-\nciated with categorical labels y1, y2, . . . , yN.\nThe neu-\nral network is trained by minimizing\n1\nN\nPN\ni=1 C(xi, yi),\nwhere we omit the L2-regularization term on the param-\neters. Though this objective can effectively learn a large-\nscale network by gradient descent with a huge amount of\nlabeled data, it has two limitations. On the one hand, the\ntraining of lower intermediate layers might be problem-\natic, because the gradient signals from the top layer can\nbecome vanished (Hochreiter et al., 2001) on its way to\nthe bottom layer. Regularization by normalization (Ioffe\n& Szegedy, 2015) can alleviate this problem, but will also\nlead to large yet noisy gradients when networks are deep\n(He et al., 2016). On the other hand, the data space is infor-\n3\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\na\ndeconv\ndeconv\ndeconv\ndeconv\nconv\nconv\ndeconv\nconv\nconv\nconv\ninner \nproduct\ninner \nproduct\ninner \nproduct\nL2 \nloss\nsoftmax \nloss\na\na\nimage\npool1\npool2\npool3\npool4\npool5\ndec: \npool1\ndec: \npool3\ndec: \npool4\ndec: \nimage\nfc6\nfc7\nprobability\ndec: \npool2\none-hot \nlabel\nimage\npool1\npool2\npool3\npool4\npool5\ndec: \npool1\ndec: \npool3\ndec: \npool4\ndec: \nimage\nfc6\nfc7\nprobability\ndec: \npool2\none-hot \nlabel\nimage\npool1\npool2\npool3\npool4\npool5\ndec: \npool1\ndec: \npool3\ndec: \npool4\ndec: \nimage\nfc6\nfc7\nprobability\ndec: \npool2\none-hot \nlabel\n(a) SAE-first (stacked architecture; reconstruction loss at the first layer) \n(b) SAE-all (stacked architecture; reconstruction loss at all layers) \n(c) SAE-layerwise (layer-wise architecture)\nFigure 2. Model architectures of networks augmented with au-\ntoencoders.\n: nodes;\n: encoder macro-layer;\n: de-\ncoder macro-layer;\n: inner-product layer;\n: reconstruc-\ntion loss;\n: classiﬁcation loss.\nmative by itself, but the fully supervised objective guides\nthe representation learning purely by the labels.\nA solution to both problems is to incorporate auxiliary un-\nsupervised training objectives to the intermediate layers.\nMore speciﬁcally, the objective function becomes\n1\nN\nN\nX\ni=1\n(C(xi, yi) + λU(xi)) ,\n(2)\nwhere U(·) is the unsupervised objective function associat-\ning with one or more auxiliary pathways that are attached to\nthe convolution-pooling macro-layers in the original clas-\nsiﬁcation network.\n3.2. Network augmentation with autoencoders\nGiven the network architecture for classiﬁcation deﬁned\nin Eq. (1), we take the sub-network composed of all the\nconvolution-pooling macro-layers as the encoding path-\nway, and generate a fully mirrored decoder network as\nan auxiliary pathway of the original network. The inner-\nproduct layers close to the top-level classiﬁer may be ex-\ncluded from the autoencoder, since they are supposed to be\nmore task-relevant.\nTaking a network of ﬁve macro-layers as an example (e.g.,\nVGGNet), Figure 2a shows the network augmented with a\nstacked autoencoder. The decoding starts from the pooled\na\nimage\npool1\npool2\npool3\npool4\ndec: \npool1\ndec: \npool3\ndec: \npool4\nfc6\nfc7\nprobability\ndec: \npool2\none-hot \nlabel\nnoisy: \npool1\nnoisy: \npool3\nnoisy: \npool4\nnoisy: \nimage\nnoisy: \npool2\npool5\nnoisy: \npool5\nparameter tying\nFigure 3. Ladder network architectures Rasmus et al. (2015).\n:\nnodes;\n: noisy nodes;\n: encoder macro-layer;\n: de-\ncoder macro-layer;\n: inner-product layer;\n: reconstruc-\ntion loss;\n: classiﬁcation loss;\n: parameter tying.\nfeature map from the 5th macro-layer (pool5) all the way\ndown to the image input. Reconstruction errors are mea-\nsured at the network input (i.e., the ﬁrst layer) so that we\nterm the model as “SAE-ﬁrst”. More speciﬁcally, the de-\ncoding pathway is\nˆaL = aL, ˆal−1 = f dec\nl\n(ˆal; ψl), ˆx = ˆa0.\n(3)\nwith the loss USAE-ﬁrst(x) = ∥ˆx −x∥2\n2. Here, ψl’s are de-\ncoder parameters.\nThe auxiliary training signals of SAE-ﬁrst emerge from\nthe bottom of the decoding pathway, and they get merged\nwith the top-down signals for classiﬁcation at the last\nconvolution-pooling macro-layer into the encoder pathway.\nTo allow more gradient to ﬂow directly into the preceding\nmacro-layers, we propose the “SAE-all” model by replac-\ning the unsupervised loss by USAE-all(x) = PL−1\nl=0 γl∥ˆal −\nal∥2\n2 , which makes the autoencoder have an even better\nmirrored architecture by matching activations for all the\nmacro-layer (illustrated in Figure 2b).\nIn Figure 2c, we propose one more autoencoder vari-\nant with layer-wise decoding architecture, termed “SAE-\nlayerwise”.\nIt reconstructs the output activations of ev-\nery macro-layer to its input. The auxiliary loss of SAE-\nlayerwise is the same as SAE-all, i.e., USAE-layerwise(x) =\nUSAE-all(x), but the decoding pathway is replaced by\nˆal−1 = f dec\nl\n(al; ψl).\nSAE-ﬁrst/all encourages top-level convolution features to\npreserve as much information as possible. In contrast, the\nauxiliary pathways in SAE-layerwise focus on inverting the\nclean intermediate activations (from the encoder) to the in-\nput of the associated macro-layer, admitting parallel layer-\nwise training. We investigated both in Section 4.3 and take\nSAE-layerwise decoders as architectures for efﬁcient pre-\ntraining.\nIn Figure 1, we illustrate the detailed architecture of f3(·)\nand f dec\n3\n(·) for Simonyan & Zisserman (2015)’s 16-layer\nVGGNet. Inspired by Zeiler et al. (2011), we use Zhao\n4\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\net al. (2015)’s SWWAE as the default for the micro-\narchitecture.\nMore speciﬁcally, we record the pooling\nswitches (i.e., the locations of the local maxima) in the en-\ncoder, and unpool activations by putting the elements at the\nrecorded locations and ﬁlling the blanks with zeros. Un-\npooling with known switches can recover the local spatial\nvariance eliminated by the max-pooling layer, avoiding the\nauxiliary objectives from deteriorating the spatial invari-\nance of the encoder ﬁlters, which is arguably important for\nclassiﬁcation. We studied the autoencoders with ﬁxed and\nknown unpooling switch, respectively. In Section 4.2 we\nefﬁciently trained the autoencoders augmented from a pre-\ntrained deep non-BN network, where the decoder is hard to\nlearn from scratch.\nRasmus et al. (2015)’s ladder network (Figure 3) is a more\nsophisticated way to augment existing sequential architec-\ntures with autoencoders. It is featured by the lateral con-\nnections (vertical in Figure 3) and the combinator functions\nthat merge the lateral and top-down activations. Due to the\nlateral connections, noise must be added to the encoder;\notherwise, the combinator function can trivially copy the\nclean activations from the encoder. In contrast, no autoen-\ncoder variant used in our work has “lateral\" connections,\nwhich makes the overall architectures of our models sim-\npler and more standard. In SWWAE, the pooling switch\nconnections do not bring the encoder input directly to the\ndecoder, so they cannot be taken as the lateral connections\nlike in the “ladder network”. Moreover, noise injection is\nalso unnecessary for our models. We leave it as an open\nquestion whether denoising objectives can help with the\naugmented (what-where) autoencoder for large-scale data.\n4. Experiments\nIn this section, we evaluated different variants of the aug-\nmented network for image reconstruction and classiﬁca-\ntion on ImageNet ILSVRC 2012 dataset, using the train-\ning set for training, and validation set for evaluation.\nOur experiments were mainly based on the 16-layer VG-\nGNet (Simonyan & Zisserman, 2015).1 To compare with\nexisting methods on inverting neural networks (Dosovit-\nskiy & Brox, 2016), we also partially used Krizhevsky\net al. (2012)’s network, termed AlexNet, trained on\nILSVRC2012 training set.\nOur code and trained mod-\nels can be obtained at http://www.ytzhang.net/\nsoftware/recon-dec/\n4.1. Training procedure\nTraining a deep neural network is non-trivial. Therefore,\nwe propose the following strategy to make the networks\n1The pretrained network was obtained from http://www.\nrobots.ox.ac.uk/~vgg/research/very_deep/.\naugmented from the classiﬁcation network efﬁciently train-\nable.\n1. We initialized the encoding pathway with the pre-\ntrained classiﬁcation network, and the decoding path-\nways with Gaussian random initialization.\n2. For any variant of the augmented network, we ﬁxed\nthe parameters for the classiﬁcation pathway and\ntrained the layer-wise decoding pathways of the SAE-\nlayerwise network.\n3. For SAE-ﬁrst/all, we initialized the decoding path-\nway with the pretrained SAE-layerwise parameters\nand ﬁnetuned the decoder. (Skip this step for SAE-\nlayerwise.)\n4. We ﬁnetuned all the decoding and the encod-\ning/classiﬁcation pathways together with a reduced\nlearning rate.\nUp to Step 3, we trained the decoding pathways with the\nclassiﬁcation pathway ﬁxed.\nFor all the four steps, we\ntrained the networks by mini-batch stochastic gradient de-\nscent (SGD) with the momentum 0.9.\nIn Step 2, the SAE-layerwise model has separate sub-\npathways for decoding, so the training can be done in par-\nallel for every macro-layer. The decoding sub-network for\neach macro-layer was relatively “shallow” so that it is easy\nto learn. We found the learning rate annealing not criti-\ncal for SAE-layerwise pretraining. Proper base learning\nrates could make it sufﬁciently converged within 1 epoch.\nThe chosen layer-wise learning rates VGGNet were sum-\nmarized in Appendix A1 (Table A-1). We used a small\nmini-batch size of 16 for SGD.\nFor very deep networks, training the decoding pathways\nof SAE-ﬁrst/all from random initialization is difﬁcult when\nbatch normalization is absent (e.g., in the VGGNet). Ini-\ntializing with SAE-layerwise as in Step 3 is critical to ef-\nﬁciently train the stacked decoding pathways of SAE-ﬁrst\nand SAE-all.\nFor SAE-all (Step 3, 4) and SAE-layerwise (Step 4), we\nbalanced the reconstruction loss among different macro-\nlayer, where the criterion was to make the weighted loss for\nevery layer comparable to each other. We summarized the\nbalancing weights for VGGNet in Appendix A1 (Table A-\n1). The SGD mini-batch size was set to a larger value (here,\n64) in Step 4 for better stability.\nWe adopted commonly used data augmentation schemes.\nAs to VGGNet, we randomly resized the image to\n[256, 512] pixels with respect to the shorter edge, and then\nrandomly cropped a 224 × 224 patch (or its horizontally\nmirrored image) to feed into the network. As to AlexNet,\n5\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nLayer\nimage\npool1\npool2\nconv3\nconv4\npool5\nfc6\nfc7\nfc8\nDosovitskiy &\nBrox (2016)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nFigure 4. AlexNet reconstruction on ImageNet ILSVRC2012 validation set. See Appendix A2.5 (Figure A-4) for more results.\nLayer\nimage\npool1\npool2\npool3\npool4\npool5\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nFigure 5. VGGNet reconstruction on ImageNet ILSVRC2012 validation set. See Appendix A2.5 (Figure A-4) for more results.\nwe followed Krizhevsky et al. (2012)’s data augmentation\nscheme, cropping an image at the center to make it square\nwith the shorter edge unchanged, resizing the square to\n256×256, and randomly sampling a 227×227 patch or its\nhorizontally mirrored counterpart to feed the network. We\nignored the RGB color jittering so as to always take ground\ntruth natural images as the reconstruction targets.\nOur implementation was based on the Caffe framework (Jia\net al., 2014).\n4.2. Image reconstruction via decoding pathways\nUsing reconstructive decoding pathways, we can visualize\nthe learned hierarchical features by inverting a given clas-\nsiﬁcation network, which is a useful way to understand the\nlearned representations. The idea of reconstructing the en-\ncoder input from its intermediate activations was ﬁrst ex-\nplored by Dosovitskiy & Brox (2016), in contrast to vi-\nsualizing a single hidden node (Zeiler & Fergus, 2014)\nand dreaming out images (Mahendran & Vedaldi, 2015).\nAs the best existing method for inverting neural networks\nwith no skip link, it used unpooling with ﬁxed switches to\nupsample the intermediate activation maps. This method\ndemonstrated how much information the features produced\nby each layer could preserve for the input. As shown in\nFigure 4 (the top row), not surprisingly, the details of the in-\nput image gradually diminished as the representations went\nthrough higher layers.\nThe commonly used classiﬁcation network mainly con-\nsists of convolution/inner-product and max-pooling oper-\nators. Based only on Dosovitskiy & Brox (2016)’s visual-\nization, it is hard to tell how much the two types of opera-\ntors contribute to the diminishing of image details, respec-\ntively. Note that our SAE-ﬁrst architecture is comparable\nto Dosovitskiy & Brox (2016)’s model except for the better\nmirrored architectures between the encoder and decoder,\nwhich allow extending to SWWAE. Using the SWWAE-\nﬁrst network (“what-where” version of SAE-ﬁrst), we were\nable to revert the max-pooling more faithfully, and to study\nthe amount of information that the convolutional ﬁlters and\ninner-product coefﬁcients preserved.\nTo compare with Dosovitskiy & Brox (2016), we aug-\nmented AlexNet to the corresponding SWWAE-ﬁrst ar-\nchitecture.2 Unlike in Section 3, we built SWWAE-ﬁrst\nnetwork starting from every layer, i.e., decoding path-\nway could start from conv1 to fc8. Each macro-layer\nin AlexNet included exactly one convolutional or inner-\nproduct layer. We trained the decoding pathway with the\nencoding/classiﬁcation pathway ﬁxed.\nAs shown in Figure 4, the images reconstructed from any\n2The decoding pathway almost fully mirrored the classiﬁca-\ntion network except the ﬁrst layer (conv1). This convolutional\nlayer used the stride 4 rather than 1, which approximates two ad-\nditional 2 × 2 pooling layers. Therefore, we used three deconvo-\nlutional layers to inverse the conv1 layer.\n6\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nlayer, even including the top 1000-way classiﬁcation layer,\nwere almost visually perfect.3 Only the local contrast and\ncolor saturation became slightly different from the original\nimages as the layer went higher. The surprisingly good re-\nconstruction quality suggests that the features produced by\nAlexNet preserved nearly all the information of the input\nexcept for the spatial invariance gained by the max-pooling\nlayers.\nAs commonly believed, learning task-relevant features for\nclassiﬁcation and preserving information were conﬂicting\nto some extent, since the “nuisance” should be removed for\nsupervised tasks. According to our experiments, the loca-\ntional details in different scales were almost the only in-\nformation signiﬁcantly neutralized by the deep neural net-\nwork. For the convolutional and inner-product layers, it\nseems important to encode the input into a better (e.g., task-\nrelevant) form without information loss.\nWe conducted similar experiments based on the 16-layer\nVGGNet.\nAs no results using the unpooling with ﬁxed\nswitches had been reported yet, we trained the decod-\ning pathways for both SAE-ﬁrst (with ﬁxed unpool-\ning switches) and SWWAE-ﬁrst (with known unpooling\nswitches). We described the detailed training strategy in\nSection 4.3. In Figure 5, we showed the reconstruction ex-\namples up to the 5th macro-layer (the 13th layer). Images\nreconstructed by SAE-ﬁrst were blurry for higher layers. In\ncontrast, SWWAE-ﬁrst could well recover the shape details\nfrom the pool5 features. In addition, the SWWAE-ﬁrst\nmodel could also reasonably reconstruct non-ImageNet and\neven non-natural images like text screenshots, depth maps,\nand cartoon pictures, as shown in Appendix A2.5 (Fig-\nure A-3). These results suggest that the high-level feature\nrepresentations were also adaptable to other domains.\nSince the architecture was much deeper than AlexNet, VG-\nGNet resulted in noisier reconstruction. Assuming the abil-\nity of preserving information as a helpful property for deep\nneural network, we took the reconstruction loss as an aux-\niliary objective function for training the classiﬁcation net-\nwork, as will be described in Section 4.3.\n4.3. Image classiﬁcation with augmented architectures\nWe took as the baseline the 16-layer VGGNet (Simonyan &\nZisserman (2015)’s Model D), one of the best open source\nconvolutional neural networks for large-scale image classi-\nﬁcation.\nWe needed only to use the classiﬁcation pathway for test-\ning.\nWe report results with the following two schemes\nfor sampling patches to show both more ablative and more\n3For the fc6 and fc7 layers, we applied inner-product fol-\nlowed by relu nonlinearity; for the fc8 layer, we applied only\ninner-product, but not softmax nonlinearity.\npractical performance on single networks.\nSingle-crop We resized the test image, making its shorter\nedge 256 pixels, and used only the single 224 × 224\npatch (without mirroring) at the center to compute\nthe classiﬁcation score. It allowed us to examine the\ntradeoff between training and validation performance\nwithout complicated post-processing.\nConvolution We took the VGGNet as a fully convolu-\ntional network and used a global average-pooling to\nfuse the classiﬁcation scores obtained at different lo-\ncations in the grid. The test image was resized to 256\npixels for the shorter edge and mirrored to go through\nthe convolution twice. It was a replication of Sec-\ntion 3.2 of (Simonyan & Zisserman, 2015).\nWe report the experimental results in Table 1. Several VG-\nGNet (classiﬁcation pathway only) results are presented\nto justify the validity of our baseline implementation. As\na replication of Simonyan & Zisserman (2015)’s “single-\nscale” method, our second post-processing scheme could\nachieve similar comparable accuracy. Moreover, ﬁnetun-\ning the pretrained VGGNet model further without the aug-\nmented decoding network using the same training proce-\ndure did not lead to signiﬁcant performance change.\nAs a general trend, all of the networks augmented with au-\ntoencoders outperformed the baseline VGGNet by a no-\nticeable margin. In particular, compared to the VGGNet\nbaseline, the SWWAE-all model reduced the top-1 errors\nby 1.66% and 1.18% for the single-crop and convolution\nschemes, respectively. It also reduced the top-5 errors by\n1.01% and 0.81%, which are 10% and 9% relative to the\nbaseline errors.\nTo the best of our knowledge, this work provides the ﬁrst\nexperimental results to demonstrate the effectiveness of un-\nsupervised learning objectives for improving the state-of-\nthe-art image classiﬁcation performance on large-scale re-\nalistic datasets. For SWWAE-all, the validation accuracy\nin Table 1 was achieved in ∼16 epochs, which took 4~5\ndays on a workstation with 4 Nvidia Titan X GPUs. Taking\npretrained VGGNet as the reference, 75% of the relative\naccuracy improvement (∼1.25% absolute top-1 accuracy\nimprovement) could be achieved in ∼4 epochs (∼1 day).\nApart from the general performance gain due to reconstruc-\ntive decoding pathways, the architecture changes could re-\nsult in relatively small differences. Compared to SWWAE-\nlayerwise, SWWAE-all led to slightly higher accuracy,\nsuggesting the usefulness of posing a higher requirement\non the top convolutional features for preserving the input\ninformation.\nThe slight performance gain of SWWAE-\nall over SAE-all with ﬁxed unpooling switches indicates\nthat the switch connections could alleviate the difﬁculty\n7\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nSampling\nSingle-crop (center patch, no mirroring)\nConvolution\nErrors\nTop-1\nTop-5\nTop-1\nTop-5\nModel\nTrain\nVal.\nTrain\nVal.\nValidation\nVGGNet †\n–\n–\n–\n–\n27.0 ∗\n8.8 ∗\nVGGNet †\n–\n–\n–\n–\n26.8 ∗∗\n8.7 ∗∗\nVGGNet\n17.43\n29.05\n4.02\n10.07\n26.97\n8.94\nSAE-ﬁrst\n15.36\n27.70\n3.13\n09.28\n26.09\n8.30\nSAE-all\n15.64\n27.54\n3.23\n09.17\n26.10\n8.21\nSAE-layerwise\n16.20\n27.60\n3.42\n09.19\n26.06\n8.17\nSWWAE-ﬁrst\n15.10\n27.60\n3.08\n09.23\n25.87\n8.14\nSWWAE-all\n15.67\n27.39\n3.24\n09.06\n25.79\n8.13\nSWWAE-layerwise\n15.42\n27.53\n3.32\n09.10\n25.97\n8.20\n† The numbers in the last rows are from Table 3 (Model D) in Simonyan & Zisserman (2015) (the most comparable to our settings).4\n∗from a slightly different model trained with single-scale (256px) data augmentation. ∗∗Test scale is 384px.\nTable 1. Classiﬁcation errors on ImageNet ILSVRC-2012 validation dataset based on 16-layer VGGNet. SAE models use the unpooling\nwith ﬁxed switches, and SWWAE models uses the unpooling with known switches.\nof learning a stacked convolutional autoencoder.\nIn the\nmeanwhile, it also suggests that, without pooling switches,\nthe decoding pathway can beneﬁt the classiﬁcation net-\nwork learning similarly. Using the unpooling with ﬁxed\nswitches, the decoding pathway may not be limited for re-\nconstruction, but can also be designed for the structured\noutputs that are not locationally aligned with the input im-\nages (e.g, adjacent frames in videos, another viewpoint of\nthe input object).\nTo ﬁgure out whether the performance gain was due to the\npotential regularization effects of the decoding pathway or\nnot, we evaluated the networks on 50,000 images randomly\nchosen from the training set. Interestingly, the networks\naugmented with autoencoders achieved lower training er-\nrors than the baseline VGGNet. Hence, rather than regular-\nizing, it is more likely that the auxiliary unsupervised loss\nhelped the CNN to ﬁnd better local optima in supervised\nlearning. Compared to SAE/SWWAE-all, SAE/SWWAE-\nﬁrst led to lower training errors but higher validation errors,\na typical symptom of slight overﬁtting. Thus, incorporat-\ning layer-wise reconstruction loss was an effective way to\nregularize the network training.\nWe provide more discussion for the decoding pathways in\nAppendix A2, including image reconstruction results af-\nter ﬁnetuning the augmented networks (Appendix A2.5),\ntraining curves (Appendix A2.2), and comparison be-\ntween the pretrained and ﬁnetuned convolution ﬁlters (Ap-\npendix A2.1).\n4In our experiments, the 16-layer VGGNet (Simonyan & Zis-\nserman (2015)’s Model D) achieved 10.07% for the single-crop\nscheme and 8.94% for the convolution scheme (in a single scale),\nwhich is comparable to 8.8% in Table 3 of (Simonyan & Zisser-\nman, 2015). In that table, the best reported number for the Model\nD was 8.1%, but it is trained and tested using a different resizing\nand cropping method, thus not comparable to our results.\n5. Conclusion\nWe proposed a simple and effective way to incorporate\nunsupervised objectives into large-scale classiﬁcation net-\nwork learning by augmenting the existing network with re-\nconstructive decoding pathways. Using the resultant au-\ntoencoder for image reconstruction, we demonstrated the\nability of preserving input information by intermediate rep-\nresentation as an important property of modern deep neural\nnetworks trained for large-scale image classiﬁcation. We\nleveraged this property further by training the augmented\nnetwork composed of both the classiﬁcation and decoding\npathways. This method improved the performance of the\n16-layer VGGNet, one of the best existing networks for im-\nage classiﬁcation by a noticeable margin. We investigated\ndifferent variants of the autoencoder, and showed that 1) the\npooling switch connections between the encoding and de-\ncoding pathways were helpful, but not critical for improv-\ning the performance of the classiﬁcation network in large-\nscale settings; 2) the decoding pathways mainly helped the\nsupervised objective reach a better optimum; and 3) the\nlayer-wise reconstruction loss could effectively regularize\nthe solution to the joint objective. We hope this paper will\ninspire further investigations on the use of unsupervised\nlearning in a large-scale setting.\nAcknowledgements\nThis work was funded by Software R&D Center, Samsung\nElectronics Co., Ltd; ONR N00014-13-1-0762; and NSF\nCAREER IIS-1453651. We also thank NVIDIA for do-\nnating K40c and TITAN X GPUs. We thank Jimei Yang,\nSeunghoon Hong, Ruben Villegas, Wenling Shang, Kihyuk\nSohn, and other collaborators for helpful discussions.\n8\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nReferences\nBengio, Y. Learning deep architectures for ai. Foundation\nand Trends in Machine Learning, 2(1):1–127, January\n2009.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.\nGreedy layer-wise training of deep networks. In NIPS,\n2007.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\nFei-Fei, L. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\nDoersch, C., Gupta, A., and Efros, A. A. Unsupervised\nvisual representation learning by context prediction. In\nICCV, 2015.\nDong, D. and McAvoy, T. J. Nonlinear principal compo-\nnent analysis based on principal curves and neural net-\nworks. Computers & Chemical Engineering, 20(1):65–\n78, 1996.\nDosovitskiy, A. and Brox, T. Inverting visual representa-\ntions with convolutional networks. In CVPR, 2016.\nGirshick, R., Donahue, J., Darrell, T., and Malik, J.\nRegion-based convolutional networks for accurate object\ndetection and segmentation. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 38(1):142–158,\nJan 2016.\nGoodfellow, I., Mirza, M., Courville, A., and Bengio, Y.\nMulti-prediction deep boltzmann machines.\nIn NIPS,\n2013.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In CVPR, 2016.\nHinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning\nalgorithm for deep belief nets. Neural computation, 18\n(7):1527–1554, 2006.\nHochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber,\nJ. Gradient ﬂow in recurrent nets: the difﬁculty of learn-\ning long-term dependencies. In A Field Guide to Dynam-\nical Recurrent Networks. 2001.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerat-\ning deep network training by reducing internal covariate\nshift. In ICML, 2015.\nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,\nGirshick, R., Guadarrama, S., and Darrell, T.\nCaffe:\nConvolutional architecture for fast feature embedding.\narXiv:1408.5093, 2014.\nKavukcuoglu, K., Ranzato, M. A., and LeCun, Y. Fast in-\nference in sparse coding algorithms with applications to\nobject recognition. arXiv:1010.3467, 2010.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn NIPS, 2012.\nLarochelle, H. and Bengio, Y. Classiﬁcation using discrim-\ninative restricted boltzmann machines. In ICML, 2008.\nLeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-\nture, 521(7553):436–444, 2015.\nLee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z.\nDeeply-supervised nets. In AISTATS, 2015.\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convo-\nlutional deep belief networks for scalable unsupervised\nlearning of hierarchical representations. In ICML, 2009.\nMahendran, A. and Vedaldi, A. Understanding deep image\nrepresentations by inverting them. In CVPR, 2015.\nMairal, J., Ponce, J., Sapiro, G., Zisserman, A., and Bach,\nF. R. Supervised dictionary learning. In NIPS, 2009.\nMakhzani, A. and Frey, B. J. Winner-take-all autoencoders.\nIn NIPS, 2015.\nMasci, J., Meier, U., Cire¸san, D., and Schmidhuber,\nJ. Stacked convolutional auto-encoders for hierarchical\nfeature extraction. In International Conference on Arti-\nﬁcial Neural Networks, 2011.\nPezeshki, M., Fan, L., Brakel, P., Courville, A., and Ben-\ngio, Y. Deconstructing the ladder network architecture.\narXiv:1506.02351, 2016.\nRanzato, M. A. and Szummer, M. Semi-supervised learn-\ning of compact document representations with deep net-\nworks. In ICML, 2008.\nRanzato, M. A., Huang, F. J., Boureau, Y.-L., and LeCun,\nY. Unsupervised learning of invariant feature hierarchies\nwith applications to object recognition. In CVPR, 2007.\nRasmus, A., Valpola, H., Honkala, M., Berglund, M., and\nRaiko, T. Semi-supervised learning with ladder network.\nIn NIPS, 2015.\nSalakhutdinov, R. and Hinton, G. E. Deep boltzmann ma-\nchines. In AISTATS, 2009.\nScholz, M. and Vigário, R. Nonlinear pca: a new hierarchi-\ncal approach. In ESANN, 2002.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition.\nIn ICLR,\n2015.\nSohn, K., Zhou, G., Lee, C., and Lee, H. Learning and se-\nlecting features jointly with point-wise gated Boltzmann\nmachines. In ICML, 2013.\n9\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nSuddarth, S. and Kergosien, Y. Rule-injection hints as a\nmeans of improving network performance and learning\ntime. Neural Networks, 412:120–129, 1990.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V., and Rabi-\nnovich, A. Going deeper with convolutions. In CVPR,\n2015.\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wo-\njna, Z. Rethinking the inception architecture for com-\nputer vision. 2016.\nTorralba, A., Fergus, R., and Freeman, W. 80 million tiny\nimages: A large data set for nonparametric object and\nscene recognition. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 30(11):1958–1970, Nov\n2008.\nValpola, H. From neural PCA to deep unsupervised learn-\ning. In Advances in Independent Component Analysis\nand Learning Machines (Chapter 8), pp. 143 – 171.\n2015.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-\nA. Extracting and composing robust features with de-\nnoising autoencoders. In ICML, 2008.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Man-\nzagol, P.-A. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local\ndenoising criterion. Journal of Machine Learning Re-\nsearch, 11:3371–3408, December 2010.\nWang, L., Lee, C.-Y., Tu, Z., and Lazebnik, S. Training\ndeeper convolutional networks with deep supervision.\narXiv:1505.02496, 2015.\nWang, X. and Gupta, A. Unsupervised learning of visual\nrepresentations using videos. In ICCV, 2015.\nYang, J., Yu, K., and Huang, T. Supervised translation-\ninvariant sparse coding. In CVPR, 2010.\nZeiler, M. D. and Fergus, R. Visualizing and understanding\nconvolutional networks. In ECCV, 2014.\nZeiler, M. D., Krishnan, D., Taylor, G. W., and Fergus, R.\nDeconvolutional networks. CVPR, 2010.\nZeiler, M., Taylor, G., and Fergus, R. Adaptive deconvolu-\ntional networks for mid and high level feature learning.\nIn ICCV, 2011.\nZhao, J., Mathieu, M., Goroshin, R., and Lecun, Y. Stacked\nwhat-where auto-encoders. arXiv:1506.02351, 2015.\n10\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nAppendices\nA1. Parameters for VGGNet-based models\nMacro-\nLearning rate\nLoss weighting 1\nlayer\nSAE-layerwise\nSAE-layerwise/all\n1\n3 × 10−9\n1 × 10−4\n2\n1 × 10−8\n1 × 10−12\n3\n3 × 10−12\n1 × 10−12\n4\n1 × 10−12\n1 × 10−12\n5\n1 × 10−11\n1 × 10−10\nLR: learning rate; 1 the top-level softmax is weighted by 1.\nTable A-1. Layer-wise training parameters for networks augmented from VGGNet\nWe report the learning parameters for 16-layer VGGNet-based model in Table A-1. We chose the learning rates that lead to\nthe largest decrease in the reconstruction loss in the ﬁrst 2000 iterations for each layer. The “loss weighting” are balancing\nfactors for reconstruction losses in different layers varied to make them comparable in magnitude. In particular, we com-\nputed image reconstruction loss against RGB values normalized to [0,1], which are different in scale from intermediate\nfeatures. We also did not normalize the reconstruction loss with feature dimensions for any layer.\nA2. More experimental results and discussions\nA2.1. Learned ﬁlters\nCompared to the baseline VGGNet, the ﬁnetuned SWWAE-all model demonstrated ∼35% element-wise relative change\nof the ﬁlter weights on average for all the layers. A small portion of the ﬁlters showed stronger contrast after ﬁnetuning.\nQualitatively, the ﬁnetuned ﬁlters kept the pretrained visual shapes. In Figure A-1, we visualize the ﬁrst-layer 3 × 3\nconvolution ﬁlters.\n(a) Pretrained VGGNet\n(b) Finetuned SWWAE-all\nFigure A-1. Visualization of the normalizaed ﬁrst-layer convolution ﬁlters in 16-layer VGGNet-based network.\nThe ﬁlters of the\nSWWAE-all model had nearly the same patterns to those of the pretrained VGGNet, but showed stronger contrast. It is more clear\nsee the difference if displaying the two images alternatively in the same place. (online example: http://www.ytzhang.net/\nfiles/publications/2016-icml-recon-dec/filters/)\nA2.2. Training curve\nIn Figure A-2, we report the training curves of validation accuracy for SWWAE-all, where the pretrained VGGNet classi-\nﬁcation network and decoder network were taken as the starting point.\n11\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n# Epoch\n71\n71.5\n72\n72.5\n73\nAccuracy / %\nTop-1 Validation Accuracy\npretrained VGGNet baseline\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n# Epoch\n89.8\n90\n90.2\n90.4\n90.6\n90.8\n91\nAccuracy / %\nTop-5 Validation Accuracy\npretrained VGGNet baseline\nFigure A-2. Training curves for the single-crop validation accuracy of VGGNet-based SWWAE-all models.\nA2.3. Selection of different model variants\nThe performance for different variants of the augmented network are comparable, but we can still choose the best available\none. In particular, we provide following discussions.\n• Since the computational costs were similar for training and the same for testing, we can use the best available ar-\nchitecture depending on tasks. For example, when using decoding pathways for spatially corresponded tasks like\nreconstruction (as in our paper) and segmentation, we can use the SWWAE. For more general objectives like pre-\ndicting next frames, where pooling switches are non-transferrable, we can still use ordinary SAEs to get competitive\nperformance.\n• S(WW)AE-ﬁrst has less hyper-parameters than S(WW)AE-all, and can be trained ﬁrst for quick parameter search. It\ncan be switched to *-all for better performance.\nA2.4. Ladder networks\nWe tried training a ladder network following the same procedures of pretraining auxiliary pathways and ﬁnetuning the\nwhole network as for our models, which is also similar to Rasmus et al. (2015)’s strategy. We used the augmented multi-\nlayer perceptron (AMLP) combinator, which Pezeshki et al. (2016) proposed as the best combinator function. Different\nfrom the previous work conducted on the variants of MNIST dataset, the pretrained VGGNet does not have batch normal-\nization (BN) layers, which pushed us to remove the BN layers from the ladder network. However, BN turned out to be\ncritical for proper noise injection, and the non-BN ladder network did not perform well. It might suggest that our models\nare easier to pair with a standard convolutional network and train on large-scale datasets.\nA2.5. Image reconstruction\nIn Figure A-3, we visualize the images reconstructed by the pretrained decoder of SWWAE-ﬁrst and the ﬁnal models for\nSWWAE-ﬁrst/all, and reported the L2 reconstruction loss on the validation set. Finetuning the entire networks also resulted\nin better reconstruction quality, which is consistent with our assumption that enhancing the ability of preserving input\ninformation can lead to better features for image classiﬁcation. Since the shape details had already been well recovered\nby the pretrained decoder, the ﬁnetuned SWWAE-ﬁrst/all mainly improved the accuracy of colors. Note that the decoder\nlearning is more difﬁcult for SWWAE-all than SWWAE-ﬁrst, which explains its slightly higher reconstruction loss and\nbetter regularization ability.\nIn Figure A-4 and A-5, we showed more examples for reconstructing input images from pretrained neural network features\nfor AlexNet and VGGNet.\n12\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nModel\nL2 Loss\nImageNet\nNon-ImageNet 1\nGround truth\n-\nSWWAE-ﬁrst\n(Pretrained,\nﬁxing encoder)\n513.4\nSWWAE-ﬁrst\n(Finetuned with\nencoder)\n462.2\nSWWAE-all\n(Finetuned with\nencoder)\n493.0\n1 The ﬁrst three images are from morgueﬁle.com; the fourth is a screenshot of Wikipedia; the ﬁfth is a depth image from NYU\ndataset; the last is used with permission from Debbie Ridpath Ohi at Inkygirl.com\nFigure A-3. Image reconstruction from pool5 features to images. The reconstruction loss is computed on the ILSVRC2012 validation set\nand measured with L2-distance with the ground truth (RGB values are in [0, 1]). The ﬁrst 2 example images are from the ILSVRC2012\nvalidation set (excluding the 100 categories). The rest are not in ImageNet.\n13\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nLayer\nimage\npool1\npool2\nconv3\nconv4\npool5\nfc6\nfc7\nfc8\nDosovitskiy &\nBrox (2016)\n(ﬁxed unpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known\nunpooling\nswitches)\nFigure A-4. AlexNet reconstruction on ImageNet ILSVRC2012 validation set. (Best viewed when zoomed in on a screen.)\n14\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nLayer\nimage\npool1\npool2\npool3\npool4\npool5\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\n15\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nLayer\nimage\npool1\npool2\npool3\npool4\npool5\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\n16\nAugmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classiﬁcation\nLayer\nimage\npool1\npool2\npool3\npool4\npool5\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nSAE-ﬁrst (ﬁxed\nunpooling\nswitches)\nSWWAE-ﬁrst\n(known unpooling\nswitches)\nFigure A-5. VGGNet reconstruction on ImageNet ILSVRC2012 validation set. (Best viewed when zoomed in on a screen.)\n17\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2016-06-21",
  "updated": "2016-06-21"
}