{
  "id": "http://arxiv.org/abs/1607.00215v3",
  "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?",
  "authors": [
    "Ian Osband",
    "Benjamin Van Roy"
  ],
  "abstract": "Computational results demonstrate that posterior sampling for reinforcement\nlearning (PSRL) dramatically outperforms algorithms driven by optimism, such as\nUCRL2. We provide insight into the extent of this performance boost and the\nphenomenon that drives it. We leverage this insight to establish an\n$\\tilde{O}(H\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in\nfinite-horizon episodic Markov decision processes, where $H$ is the horizon,\n$S$ is the number of states, $A$ is the number of actions and $T$ is the time\nelapsed. This improves upon the best previous bound of $\\tilde{O}(H S\n\\sqrt{AT})$ for any reinforcement learning algorithm.",
  "text": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?\nIan Osband 1 2 Benjamin Van Roy 1\nAbstract\nComputational results demonstrate that posterior\nsampling for reinforcement learning (PSRL)\ndramatically outperforms existing algorithms\ndriven by optimism, such as UCRL2. We pro-\nvide insight into the extent of this performance\nboost and the phenomenon that drives it.\nWe\nleverage this insight to establish an ˜O(H\n√\nSAT)\nBayesian regret bound for PSRL in ﬁnite-horizon\nepisodic Markov decision processes. This im-\nproves upon the best previous Bayesian regret\nbound of ˜O(HS\n√\nAT) for any reinforcement\nlearning algorithm.\nOur theoretical results are\nsupported by extensive empirical evaluation.\n1. Introduction\nWe consider the reinforcement learning problem in which\nan agent interacts with a Markov decision process with the\naim of maximizing expected cumulative reward (Burnetas\n& Katehakis, 1997; Sutton & Barto, 1998). Key to per-\nformance is how the agent balances between exploration\nto acquire information of long-term beneﬁt and exploita-\ntion to maximize expected near-term rewards. In princi-\nple, dynamic programming can be applied to compute the\nBayes-optimal solution to this problem (Bellman & Kal-\naba, 1959). However, this is computationally intractable\nfor anything beyond the simplest of toy problems and di-\nrect approximations can fail spectacularly poorly (Munos,\n2014). As such, researchers have proposed and analyzed a\nnumber of heuristic reinforcement learning algorithms.\nThe literature on efﬁcient reinforcement learning offers sta-\ntistical efﬁciency guarantees for computationally tractable\nalgorithms. These provably efﬁcient algorithms (Kearns\n& Singh, 2002; Brafman & Tennenholtz, 2002) predom-\ninantly address the exploration-exploitation trade-off via\noptimism in the face of uncertainty (OFU): at any state, the\nagent assigns to each action an optimistically biased esti-\n1Stanford University, California, USA 2Deepmind, London,\nUK. Correspondence to: Ian Osband <ian.osband@gmail.com>.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\nmate of future value and selects the action with the greatest\nestimate. If a selected action is not near-optimal, the es-\ntimate must be overly optimistic, in which case the agent\nlearns from the experience. Efﬁciency relative to less so-\nphisticated exploration arises as the agent avoids actions\nthat can neither yield high value nor informative data.\nAn alternative approach, based on Thompson sampling\n(Thompson, 1933), involves sampling a statistically plau-\nsibly set of action values and selecting the maximizing\naction. These values can be generated, for example, by\nsampling from the posterior distribution over MDPs and\ncomputing the state-action value function of the sampled\nMDP. This approach, originally proposed in Strens (2000),\nis called posterior sampling for reinforcement learning\n(PSRL). Computational results from Osband et al. (2013)\ndemonstrate that PSRL dramatically outperforms existing\nalgorithms based on OFU. The primary aim of this paper is\nto provide insight into the extent of this performance boost\nand the phenomenon that drives it.\nWe show that, in Bayesian expectation and up to constant\nfactors, PSRL matches the statistical efﬁciency of any stan-\ndard algorithm for OFU-RL. We highlight two key short-\ncomings of existing state of the art algorithms for OFU\n(Jaksch et al., 2010) and demonstrate that PSRL does not\nsuffer from these inefﬁciencies. We leverage this insight\nto produce an ˜O(H\n√\nSAT) bound for the Bayesian regret\nof PSRL in ﬁnite-horizon episodic Markov decision pro-\ncesses where H is the horizon, S is the number of states,\nA is the number of actions and T is the time elapsed. This\nimproves upon the best previous bound of ˜O(HS\n√\nAT) for\nany RL algorithm. We discuss why we believe PSRL sat-\nisﬁes a tighter ˜O(\n√\nHSAT), though we have not proved\nthat. We complement our theory with computational exper-\niments that highlight the issues we raise; empirical results\nmatch our theoretical predictions.\nMore importantly, we highlights a tension in OFU RL be-\ntween statistical efﬁciency and computational tractability.\nWe argue that any OFU algorithm that matches PSRL in\nstatistical efﬁciency would likely be computationally in-\ntractable. We provide proof of this claim in a restricted\nsetting. Our key insight, and the potential beneﬁts of ex-\nploration guided by posterior sampling, are not restricted\nto the simple tabular MDPs we analyze.\narXiv:1607.00215v3  [stat.ML]  13 Jun 2017\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\n2. Problem formulation\nWe consider the problem of learning to optimize a ran-\ndom ﬁnite-horizon MDP M ∗=(S,A,R∗,P ∗,H,ρ) over re-\npeated episodes of interaction, where S = {1, .., S} is\nthe state space, A = {1, .., A} is the action space, H is\nthe horizon, and ρ is the initial state distribution. In each\ntime period h = 1, .., H within an episode, the agent ob-\nserves state sh ∈S, selects action ah ∈A, receives a\nreward rh ∼R∗(sh, ah), and transitions to a new state\nsh+1 ∼P ∗(sh, ah). We note that this formulation, where\nthe unknown MDP M ∗is treated as itself a random vari-\nable, is often called Bayesian reinforcement learning.\nA policy µ is a mapping from state s ∈S and period h =\n1, .., H to action a ∈A. For each MDP M and policy µ\nwe deﬁne the state-action value function for each period h:\nQM\nµ,h(s,a):=EM,µ\n\n\nH\nX\nj=h\nrM(sj,aj)\n\f\f\fsh =s,ah =a\n\n, (1)\nwhere rM(s,a)=E[r|r∼RM(s,a)]. The subscript µ in-\ndicates that actions over periods h+1,...,H are selected\naccording to the policy µ. Let V M\nµ,h(s):=QM\nµ,h(s,µ(s,h)).\nWe say a policy µM is optimal for the MDP M if\nµM ∈argmaxµV M\nµ,h(s) for all s∈S and h=1,...,H.\nLet Ht denote the history of observations made prior\nto time t.\nTo highlight this time evolution within\nepisodes, with some abuse of notation, we let skh =st for\nt=(k−1)H +h, so that skh is the state in period h of\nepisode k. We deﬁne Hkh analogously. An RL algorithm\nis a deterministic sequence {πk|k=1,2,...} of functions,\neach mapping Hk1 to a probability distribution πk(Hk1)\nover policies, from which the agent samples a policy µk\nfor the kth episode. We deﬁne the regret incurred by an RL\nalgorithm π up to time T to be\nRegret(T,π,M ∗):=\n⌈T/H⌉\nX\nk=1\n∆k,\n(2)\nwhere ∆k denotes regret over the kth episode, deﬁned with\nrespect to true MDP M ∗by\n∆k :=\nX\nS\nρ(s)(V M ∗\nµ∗,1(s)−V M ∗\nµk,1(s))\n(3)\nwith µ∗=µM ∗. We note that the regret in (2) is random,\nsince it depends on the unknown MDP M ∗, the learning\nalgorithm π and through the history Ht on the sampled\ntransitions and rewards. We deﬁne\nBayesRegret(T,π,φ):=E[Regret(T,π,M ∗)|M ∗∼φ],\n(4)\nas the Bayesian expected regret for M ∗distributed accord-\ning to the prior φ. We will assess and compare algorithm\nperformance in terms of the regret and BayesRegret.\n2.1. Relating performance guarantees\nFor the most part, the literature on efﬁcient RL is sharply\ndivided between the frequentist and Bayesian perspective\n(Vlassis et al., 2012). By volume, most papers focus on\nminimax regret bounds that hold with high probability for\nany M ∗∈M some class of MDPs (Jaksch et al., 2010).\nBounds on the BayesRegret are generally weaker analyt-\nical statements than minimax bounds on regret. A regret\nbound for any M ∗∈M implies an identical bound on the\nBayesReget for any φ with support on M. A partial con-\nverse is available for M ∗drawn with non-zero probability\nunder φ, but does not hold in general (Osband et al., 2013).\nAnother common notion of performance guarantee is given\nby so-called “sample-complexity” or PAC analyses that\nbound the number of ϵ-sub-optimal decisions taken by an\nalgorithm (Kakade, 2003; Dann & Brunskill, 2015).\nIn\ngeneral, optimal bounds on regret ˜O(\n√\nT) imply optimal\nbounds on sample complexity ˜O(ϵ−2), whereas optimal\nbounds on the sample complexity give only an ˜O(T 2/3)\nbound on regret (Osband, 2016).\nOur formulation focuses on the simple setting on ﬁnite\nhorizon MDPs, but there are several other problems of in-\nterest in the literature. Common formulations include the\ndiscounted setting1 and problems with inﬁnite horizon un-\nder some connectedness assumption (Bartlett & Tewari,\n2009). This paper may contain insights that carry over to\nthese settings, but we leave that analysis to future work.\nOur analysis focuses upon Bayesian expected regret in ﬁ-\nnite horizon MDPs. We ﬁnd this criterion amenable to (rel-\natively) simple analysis and use it obtain actionable insight\nto the design of practical algorithms. We absolutely do not\n“close the book” on the exploration/exploitation problem -\nthere remain many important open questions. Nonetheless,\nour work may help to develop understanding within some\nof the outstanding issues of statistical and computational\nefﬁciency in RL. In particular, we shed some light on how\nand why posterior sampling performs so much better than\nexisting algorithms for OFU-RL. Crucially, we believe that\nmany of these insights extend beyond the stylized problem\nof ﬁnite tabular MDPs and can help to guide the design of\npractical algorithms for generalization and exploration via\nrandomized value functions (Osband, 2016).\n3. Posterior sampling as stochastic optimism\nThere is a well-known connection between posterior sam-\npling and optimistic algorithms (Russo & Van Roy, 2014).\nIn this section we highlight the similarity of these ap-\nproaches. We argue that posterior sampling can be thought\nof as a stochastically optimistic algorithm.\n1Discount γ =1−1/H gives an effective horizon O(H).\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nBefore each episode, a typical OFU algorithm constructs\na conﬁdence set to represent the range of MDPs that are\nstatistically plausible given prior knowledge and observa-\ntions. Then, a policy is selected by maximizing value si-\nmultaneously over policies and MDPs in this set. The agent\nthen follows this policy over the episode. It is interesting to\ncontrast this approach against PSRL where instead of max-\nimizing over a conﬁdence set, PSRL samples a single sta-\ntistically plausible MDP and selects a policy to maximize\nvalue for that MDP.\nAlgorithm 1 OFU RL\nInput: conﬁdence set constructor Φ\n1: for episode k=1,2,.. do\n2:\nConstruct conﬁdence set Mk =Φ(Hk1)\n3:\nCompute µk ∈argmaxµ,M∈Mk V M\nµ,1\n4:\nfor timestep h=1,..,H do\n5:\ntake action akh =µk(skh,h)\n6:\nupdate Hkh+1 =Hkh ∪(skh,akh,rkh,skh+1)\n7:\nend for\n8: end for\nAlgorithm 2 PSRL\nInput: prior distribution φ\n1: for episode k=1,2,.. do\n2:\nSample MDP Mk ∼φ(·|Hk1)\n3:\nCompute µk ∈argmaxµV Mk\nµ,1\n4:\nfor timestep h=1,..,H do\n5:\ntake action akh =µk(skh,h)\n6:\nupdate Hkh+1 =Hkh ∪(skh,akh,rkh,skh+1)\n7:\nend for\n8: end for\n3.1. The blueprint for OFU regret bounds\nThe general strategy for the analysis of optimistic algo-\nrithms follows a simple recipe (Strehl & Littman, 2005;\nSzita & Szepesvári, 2010; Munos, 2014):\n1. Design conﬁdence sets (via concentration inequality)\nsuch that M ∗∈Mk for all k with probability ≥1−δ.\n2. Decompose the regret in each episode\n∆k =V M ∗\nµ∗,1 −V M ∗\nµk,1 =V M ∗\nµ∗,1 −V Mk\nµk,1\n|\n{z\n}\n∆opt\nk\n+V Mk\nµk,1 −V M ∗\nµk,1\n|\n{z\n}\n∆conc\nk\nwhere Mk is the imagined optimistic MDP.\n3. By step (1.) ∆opt\nk\n≤0 for all k with probability ≥1−δ.\n4. Use concentration results with a pigeonhole argument\nover all possible trajectories {H11,H21,..} to bound,\nwith probability at least 1−δ,\nRegret(T,π,M ∗)≤\n⌈T/H⌉\nX\nk=1\n∆conc\nk\n|M ∗∈Mk≤f(S,A,H,T,δ).\n3.2. Anything OFU can do, PSRL can expect to do too\nIn this section, we highlight the connection between poste-\nrior sampling and any optimistic algorithm in the spirit of\nSection 3.1. Central to our analysis will be the following\nnotion of stochastic optimism (Osband et al., 2014).\nDeﬁnition 1 (Stochastic optimism).\nLet X and Y be real-valued random variables with ﬁnite\nexpectation. We will say that X is stochastically optimistic\nfor Y if for any convex and increasing u:R→R:\nE[u(X)]≥E[u(Y )].\n(5)\nWe will write X ≽so Y for this relation.\nThis notion of optimism is dual to second order stochastic\ndominance (Hadar & Russell, 1969), X ≽so Y if and only\nif −Y ≽ssd −X. We say that PSRL is a stochastically opti-\nmistic algorithm since the random imagined value function\nV Mk\nµk,1 is stochastically optimistic for the true optimal value\nfunction V M ∗\nµ∗,1 conditioned upon any possible history Hk1\n(Russo & Van Roy, 2014). This observation leads us to a\ngeneral relationship between PSRL and the BayesRegret of\nany optimistic algorithm.\nTheorem 1 (PSRL matches OFU-RL in BayesRegret).\nLet πopt be any optimistic algorithm for reinforcement\nlearning in the style of Algorithm 1. If πopt satisﬁes re-\ngret bounds such that, for any M ∗any T >0 and any δ>0\nthe regret is bounded with probability at least 1−δ\nRegret(T,πopt,M ∗)≤f(S,A,H,T,δ).\n(6)\nThen, if φ is the distribution of the true MDP M ∗and the\nproof of (6) follows Section 3.1, then for all T >0\nBayesRegret(T,πPSRL,φ)≤2f(S,A,H,T,δ=T −1)+2.\n(7)\nSketch proof. This result is established in Osband et al.\n(2013) for the special case of πopt =πUCRL2. We include\nthis small sketch as a refresher and a guide for high level\nintuition. First, note that conditioned upon any data Hk1,\nthe true MDP M ∗and the sampled Mk are identically\ndistributed.\nThis means that E[∆opt|Hk1]≤0 for all k.\nTherefore, to establish a bound upon the Bayesian regret\nof PSRL, we just need to bound P⌈T/H⌉\nk=1\nE[∆conc\nk\n|Hk].\nWe can use that M ∗|Hk1 =D Mk |Hk1 again in step (1.)\nfrom Section 3.1 to say that both M ∗,Mk lie within Mk\nfor all k with probability at least 1−2δ via a union bound.\nThis means we can bound the concentration error in PSRL,\nBayesRegret(T,πPSRL,φ)≤\n⌈T/H⌉\nX\nk=1\nE[∆conc\nk\n|M ∗,Mk∈Mk]+2δT\nThe ﬁnal step follows from decomposing ∆conc\nk\nby\nadding and subtracting the imagined optimistic value ˜Vk\ngenerated by πopt. Through an application of the triangle\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\ninequality,\n∆conc\nk\n≤|V Mk\nµk,1 −˜Vk|+| ˜Vk −V ∗\nµk,1|\nwe\ncan\nmirror step (4.) to bound the regret from concentration,\nP⌈T/H⌉\nk=1\nE[∆conc\nk\n|M ∗,Mk ∈Mk]≤2f(S,A,H,T,δ).\nThis result (and proof strategy) was established in multi-\narmed bandits by Russo & Van Roy (2014). We complete\nthe proof of Theorem 1 with the choice δ=T −1 and that\nthe regret is uniformly bounded by T.\nTheorem 1 suggest that, according to Bayesian expected re-\ngret, PSRL performs within a factor of 2 of any optimistic\nalgorithm whose analysis follows Section 3.1.\nThis in-\ncludes the algorithms UCRL2 (Jaksch et al., 2010), UCFH\n(Dann & Brunskill, 2015), MORMAX (Szita & Szepesvári,\n2010) and many more.\nImportantly, and unlike existing OFU approaches, the al-\ngorithm performance is separated from the analysis of the\nconﬁdence sets Mk. This means that PSRL even attains\nthe big O scaling of as-yet-undiscovered approaches to\nOFU, all at a computational cost no greater than solving\na single known MDP - even if the matched OFU algorithm\nπopt is computationally intractable.\n4. Some shortcomings of existing OFU-RL\nIn this section, we discuss how and why existing OFU al-\ngorithms forgo the level of statistical efﬁciency enjoyed by\nPSRL. At a high level, this lack of statistical efﬁciency\nemerges from sub-optimal construction of the conﬁdence\nsets Mk. We present several insights that may prove cru-\ncial to the design of improved algorithms for OFU. More\nworryingly, we raise the question that perhaps the optimal\nstatistical conﬁdence sets Mk would likely be computa-\ntionally intractable. We argue that PSRL offers a compu-\ntationally tractable approximation to this unknown “ideal”\noptimistic algorithm.\nBefore we launch into a more mathematical argument it\nis useful to take intuition from a simple estimation prob-\nlem, without any decision making. Consider an MDP with\nA=1,H =2,S =2N +1 as described in Figure 1. Every\nepisode the agent transitions from s=0 uniformly to s∈\n{1,..,2N} and receives a deterministic reward from {0,1}\ndepending upon this state. The simplicity of these exam-\nples means even a naive monte-carlo estimate of the value\nshould concentrate 1/2± ˜O(1/√n) after n episodes of in-\nteraction. Nonetheless, the conﬁdence sets suggested by\nstate of the art OFU-RL algorithm UCRL (Jaksch et al.,\n2010) become incredibly mis-calibrated as S grows.\nTo see how this problem occurs, consider any algorithm for\nfor model-based OFU-RL that builds up conﬁdence sets for\neach state and action independently, such as UCRL. Even\nif the estimates are tight in each state and action, the result-\ning optimistic MDP, simultaneously optimistic across each\nstate and action, may be far too optimistic. Geometrically\nFigure 1. MDPs to illustrate the scaling with S.\nFigure 2. MDPs to illustrate the scaling with H.\nFigure 3. Union bounds give loose rectangular conﬁdence sets.\nthese independent bounds form a rectangular conﬁdence\nset. The corners of this rectangle will be\n√\nS misspeciﬁed\nto the underlying distribution, an ellipse, when combined\nacross S independent estimates (Figure 3).\nSeveral algorithms for OFU-RL do exist which address\nthis loose dependence upon S (Strehl et al., 2006; Szita\n& Szepesvári, 2010). However, these algorithms depend\nupon a partitioning of data for future value, which leads\nto a poor dependence upon the horizon H or equivalently\nthe effective horizon\n1\n1−γ in discounted problems. We can\nuse a similar toy example from Figure 2 to understand\nwhy combining independently optimistic estimates through\ntime will contribute to a loose bound in H.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nThe natural question to ask is, “Why don’t we simply apply\nthese observations to design an optimistic algorithm which\nis simultaneously efﬁcient in S and H?”. The ﬁrst imped-\niment is that designing such an algorithm requires some\nnew intricate concentration inequalities and analysis. Do-\ning this rigorously may be challenging, but we believe it\nwill be possible through a more careful application of ex-\nisting tools to the insights we raise above. The bigger chal-\nlenge is that, even if one were able to formally specify such\nan algorithm, the resulting algorithm may in general not be\ncomputationally tractable.\nA similar observation to this problem of optimistic opti-\nmization has been shown in the setting of linear bandits\n(Dani et al., 2008; Russo & Van Roy, 2014). In these works\nthey show that the problem of efﬁcient optimization over\nellipsoidal conﬁdence sets can be NP-hard. This means that\ncomputationally tractable implementations of OFU have to\nrely upon inefﬁcient rectangular conﬁdence sets that give\nup a factor of\n√\nD where D is the dimension of the underly-\ning problem. By contrast, Thompson sampling approaches\nremain computationally tractable (since they require solv-\ning only a single problem instance) and so do not suffer\nfrom the loose conﬁdence set construction. It remains an\nopen question whether such an algorithm can be designed\nfor ﬁnite MDPs. However, these previous results in the\nsimpler bandit setting H =1 show that these problems with\nOFU-RL cannot be overcome in general.\n4.1. Computational illustration\nIn this section we present a simple series of computational\nresults to demonstrate this looseness in both S and H.\nWe sample K =1000 episodes of data from the MDP and\nthen examine the optimistic/sampled Q-values for UCRL2\nand PSRL. We implement a version of UCRL2 optimized\nfor ﬁnite horizon MDPs and implement PSRL with a uni-\nform Dirichlet prior over the initial dynamics P(0,1)=\n(p1,..,p2N) and a N(0,1) prior over rewards updating as if\nrewards had N(0,1) noise. For both algorithms, if we say\nthat R or P are known then we mean that we use the true R\nor P inside UCRL2 or PSRL. In each experiment, the es-\ntimates guided by OFU become extremely mis-calibrated,\nwhile PSRL remains stable.\nThe results of Figure 5 are particularly revealing. They\ndemonstrates the potential pitfalls of OFU-RL even when\nthe underlying transition dynamics entirely known. Sev-\neral OFU algorithms have been proposed to remedy the\nloose UCRL-style L1 concentration from transitions (Fil-\nippi et al., 2010; Araya et al., 2012; Dann & Brunskill,\n2015) but none of these address the inefﬁciency from\nhyper-rectangular conﬁdence sets. As expected, these loose\nconﬁdence sets lead to extremely poor performance in\nterms of the regret. We push full results to Appendix C\nalong with comparison to several other OFU approaches.\nFigure 4. R known, P unknown, vary N in the MDP Figure 1.\nFigure 5. P known, R unknown, vary N in the MDP Figure 1.\nFigure 6. R,P unknown, vary H in the MDP Figure 2\n5. Better optimism by sampling\nUntil now, all analyses of PSRL have come via comparison\nto some existing algorithm for OFU-RL. Previous work,\nin the spirit of Theorem 1, leveraged the existing analy-\nsis for UCRL2 to establish an ˜O(HS\n√\nAT) bound upon\nthe Bayesian regret (Osband et al., 2013). In this section,\nwe present a new result that bounds the expected regret\nof PSRL ˜O(H\n√\nSAT). We also include a conjecture that\nimproved analysis could result in a Bayesian regret bound\n˜O(\n√\nHSAT) for PSRL, and that this result would be unim-\nprovable (Osband & Van Roy, 2016).\n5.1. From S to\n√\nS\nIn this section we present a new analysis that improves the\nbound on the Bayesian regret from S to\n√\nS. The proof of\nthis result is somewhat technical, but the essential argument\ncomes from the simple observation of the loose rectangular\nconﬁdence sets from Section 4. The key to this analysis\nis a technical lemma on Gaussian-Dirichlet concentration\n(Osband & Van Roy, 2017).\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nTheorem 2. Let M ∗be the true MDP distributed accord-\ning to prior φ with any independent Dirichlet prior over\ntransitions. Then the regret for PSRL is bounded\nBayesRegret(T,πPSRL,φ)= ˜O\n\u0010\nH\n√\nSAT\n\u0011\n.\n(8)\nOur proof of Theorem 2 mirrors the standard OFU-\nRL analysis from Section 3.1.\nTo condense our no-\ntation we write xkh:=(skh,akh) and V k\nk,h:=V Mk\nµk,h.\nLet\nthe posterior mean of rewards ˆrk(x):=E[r∗(x)|Hk1],\ntransitions ˆPk(x):=E[P ∗(x)|Hk1] with respective devi-\nations from sampling noise wR(x):=rk(x)−ˆrk(x) and\nwP\nh (x):=(Pk(x)−ˆPk(x))T V k\nkh+1.\nWe note that, conditional upon the data Hk1 the true\nreward and transitions are independent of the rewards\nand transitions sampled by PSRL, so that E[r∗(x)|Hk1]=\nˆrk(x),E[P ∗(x)|Hk1]= ˆPk(x) for any x.\nHowever,\nE[wR(x)|Hk1] and E[wP\nh (x)|Hk1] are generally non-zero,\nsince the agent chooses its policy to optimize its reward un-\nder Mk. We can rewrite the regret from concentration via\nthe Bellman operator (section 5.2 of Osband et al. (2013)),\nE\nh\nV k\nk1−V ∗\nk1|Hk1\ni\n=\nE\nh\n(rk−r∗)(xk1)+Pk(xk1)T V k\nk2−P ∗(xk1)T V ∗\nk2 | Hk1\ni\n=\nE\n\u0014\n(rk−r∗)(xk1)+\n\u0010\nPk(xk1)−ˆPk(xk1)\n\u0011T\nV k\nk2\n+ E\nh\u0010\nV k\nk2−V ∗\nk2\n\u0011\n(s′)|s′∼P ∗(xk1)\ni\n|Hk1\n\u0015\n=\n...\n=\nE\n\u0014PH\nh=1{rk(xk1)−ˆr∗(xk1)}\n+PH\nh=1\n\u001a\u0010\nPk(xkh)−ˆPk(xkh)\n\u0011T\nV k\nkh\n\u001b\n| Hk1\n\u0015\n≤\nE\nhPH\nh=1|wR(xkh)|+PH\nh=1|wP\nh (xkh)| | Hk1\ni\n.\n(9)\nWe can bound the contribution from unknown rewards\nwR\nk (xkh) with a standard argument from earlier work\n(Buldygin & Kozachenko, 1980; Jaksch et al., 2010).\nLemma 1 (Sub-Gaussian tail bounds).\nLet x1,..,xn be independent samples from sub-Gaussian\nrandom variables. Then, for any δ>0\nP\n \n1\nn\n\f\f\nn\nX\ni=1\nxi\n\f\f≥\nr\n2log(2/δ)\nn\n!\n≤δ.\n(10)\nThe key piece of our new analysis will be to show that the\ncontribution from the transition estimate PH\nh=1|wP (xkh)|\nconcentrates at a rate independent of S. At the root of\nour argument is the notion of stochastic optimism (Osband,\n2016), which introduces a partial ordering over random\nvariables. We make particular use of Lemma 2, that re-\nlates the concentration of a Dirichlet posterior with that of a\nmatched Gaussian distribution (Osband & Van Roy, 2017).\nLemma 2 (Gaussian-Dirichlet dominance).\nFor all ﬁxed V ∈[0,1]N,\nα∈[0,∞)N\nwith αT 1≥2,\nif\nX ∼N(α⊤V/α⊤1,1/α⊤1)\nand\nY =P T V\nfor\nP ∼Dirichlet(α) then X ≽so Y .\nWe can use Lemma 2 to establish a similar concentration\nbound on the error from sampling wP\nh (x).\nLemma 3 (Transition concentration). For any independent\nprior over rewards with r∈[0,1], additive sub-Gaussian\nnoise and an independent Dirichlet prior over transitions\nat state-action pair xkh, then\nwP\nh (xkh)≤2H\ns\n2log(2/δ)\nmax(nk(xkh)−2,1)\n(11)\nwith probability at least 1−δ.\nSketch proof. Our proof relies heavily upon some techni-\ncal results from the note from Osband & Van Roy (2017).\nWe cannot apply Lemma 2 directly to wP , since the fu-\nture value V k\nkh+1 is itself be a random variable whose value\ndepends on the sampled transition Pk(xkh). However, al-\nthough V k\nkh+1 can vary with Pk, the structure of the MDP\nmeans that resultant wP (xkh) is still no more optimistic\nthan the most optimistic possible ﬁxed V ∈[0,H]S.\nWe begin this proof only for the simply family of MDPs\nwith S =2, which we call M2. We write p:=Pk(xkh)(1)\nfor the ﬁrst component of the unknown transition at xkh\nand similarly ˆp:= ˆPk(xkh)(1). We can then bound the tran-\nsition concentration,\n|wP\nh (xkh)|\n=\n|(Pk(xkh)−ˆPk(xkh))T V k\nkh+1|\n≤\n|(p−ˆp)||(V k\nkh+1(1)−V k\nkh+1(2))|\n≤\n|p−ˆp| sup\nRk,Pk\n|(V k\nkh+1(1)−V k\nkh+1(2))|\n≤\n|(p−ˆp)|H\n(12)\nLemma 2 now implies that for any α∈R+ with αT 1≥2,\nthe random variables p∼Dirichlet(α) and X ∼N(0,σ2 =\n1/αT 1) are ordered,\nX ≽so p−ˆp =⇒|X|H ≽so |p−ˆp|H ≽so |wP\nh (xkh)|.\n(13)\nWe conclude the proof for M ∈M2 through an application\nof Lemma 1. To extend this argument to multiple states\nS >2 we consider the marginal distribution of Pk over any\nsubset of states, which is Beta distributed similar to (12).\nWe push the details to Appendix A.\nTo complete the proof of Theorem 2 we combine Lemma 1\nwith Lemma 3. We rescale δ←δ/2SAT so that these con-\nﬁdence sets hold at each R(s,a),P(s,a) via union bound\nwith probability at least 1−1\nT ,\nE\nhPH\nh=1\n\b\n|wR(xkh)|+|wP\nh (xkh)|\n\t\n|Hk1\ni\n≤\nPH\nh=12(H +1)\nq\n2log(4SAT )\nmax(nk(xkh)−2,1).\n(14)\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nWe can now use (14) together with a pigeonhole principle\nover the number of visits to each state and action:\nBayesRegret(T,πPSRL,φ)\n≤\nP⌈T/H⌉\nk=1\nPH\nh=12(H +1)\nq\n2log(4SAT )\nnk(xkh)\n+2SA+1\n≤\n10H\np\nSAT log(4SAT).\nThis completes the proof of Theorem 2.\nPrior work has designed similar OFU approaches that im-\nprove the learning scaling with S. MORMAX (Szita &\nSzepesvári, 2010) and delayed Q-learning (Strehl et al.,\n2006), in particular, come with sample complexity bounds\nthat are linear in S, and match lower bounds. But even in\nterms of sample complexity, these algorithms are not neces-\nsarily an improvement over UCRL2 or its variants (Dann &\nBrunskill, 2015). For clarity, we compare these algorithms\nin terms of T π(ϵ):=min\n\b\nT | 1\nT BayesRegret(T,π,φ)≤ϵ\n\t\n.\nDelayQ\nMORMAX\nUCRL2\nPSRL\nTheorem 2\n˜O\n\u0010\nH9SA\nϵ4\n\u0011\n˜O\n\u0010\nH7SA\nϵ2\n\u0011\n˜O\n\u0010\nH2S2A\nϵ2\n\u0011\n˜O\n\u0010\nH2SA\nϵ2\n\u0011\nTable 1. Learning times compared in terms of T π(ϵ).\nTheorem 1 implies T PSRL(ϵ)= ˜O( H2SA\nϵ2\n). MORMAX and\ndelayed Q-learning reduces the S-dependence of UCRL2,\nbut this comes at the expense of worse dependence on H,\nand the resulting algorithms are not practical.\n5.2. From H to\n√\nH\nRecent analyses (Lattimore & Hutter, 2012; Dann & Brun-\nskill, 2015) suggest that simultaneously reducing the de-\npendence of H to\n√\nH may be possible. They note that\n“local value variance” satisﬁes a Bellman equation. Intu-\nitively this captures that if we transition to a bad state V ≃\n0, then we cannot transition anywhere much worse dur-\ning this episode. This relation means that PH\nh=1wP\nh (xkh)\nshould behave more as if they were independent and grow\nO(\n√\nH), unlike our analysis which crudely upper bounds\nthem each in turn O(H). We present a sketch towards an\nanalysis of Conjecture 1 in Appendix B.\nConjecture 1. For any prior over rewards with r∈[0,1],\nadditive sub-Gaussian noise and any independent Dirichlet\nprior over transitions, we conjecture that\nE\n\u0002\nRegret(T,πPSRL,M ∗)\n\u0003\n= ˜O\n\u0010√\nHSAT\n\u0011\n,\n(15)\nand that this matches the lower bounds for any algorithm\nup to logarithmic factors.\nThe results of (Bartlett & Tewari, 2009) adapted to ﬁnite\nhorizon MDPs would suggest a lower bound Ω(H\n√\nSAT)\non the minimax regret for any algorithm. However, the as-\nsociated proof is incorrect (Osband & Van Roy, 2016). The\nstrongest lower bound with a correct proof is Ω(\n√\nHSAT)\n(Jaksch et al., 2010). It remains an open question whether\nsuch a lower bound applies to Bayesian regret over the class\nof priors we analyze in Theorem 2.\nOne particularly interesting aspect of Conjecture 1 is that\nwe can construct another algorithm that satisﬁes the proof\nof Theorem 2 but would not satisfy the argument for Con-\njecture 1 of Appendix B. We call this algorithm Gaussian\nPSRL, since it operates in a manner similar to PSRL but ac-\ntually uses the Gaussian sampling we use for the analysis\nof PSRL in its algorithm.\nAlgorithm 3 Gaussian PSRL\nInput: Posterior MAP estimates rk, ˆPk, visit counts nk\nOutput: Random Qk,h(s,a)≽soQ∗\nh(s,a) for all (s,a,h)\n1: Initialize Qk,H+1(s,a)←0 for all (s,a)\n2: for timestep h=H,H−1,..,1 do\n3:\nVk,h+1(s)←maxαQk,h+1(s,α)\n4:\nSample wk(s,a,h)∼N\n\u0010\n0,\n(H+1)2\nmax(nk(s,a)−2,1)\n\u0011\n5:\nQk,h(s,a)←rk(s,a)+ ˆPk(s,a)T V +wk(s,a,h) ∀(s,a)\n6: end for\nAlgorithm 3 presents the method for sampling random Q-\nvalues according to Gaussian PSRL, the algorithm then\nfollows these samples greedily for the duration of the\nepisode, similar to PSRL. Interestingly, we ﬁnd that our\nexperimental evaluation is consistent with ˜O(HS\n√\nAT),\n˜O(H\n√\nSAT) and ˜O(\n√\nHSAT) for UCRL2, Gaussian\nPSRL and PSRL respectively.\n5.3. An empirical investigation\nWe now discuss a computational study designed to illus-\ntrate how learning times scale with S and H, and to em-\npirically investigate Conjecture 1. The class of MDPs we\nconsider involves a long chain of states with S =H =N\nand with two actions: left and right. Each episode the agent\nbegins in state 1. The optimal policy is to head right at ev-\nery timestep, all other policies have zero expected reward.\nInefﬁcient exploration strategies will take Ω(2N) episodes\nto learn the optimal policy (Osband et al., 2014).\nFigure 7. MDPs that highlight the need for efﬁcient exploration.\nWe evaluate several learning algorithms from ten random\nseeds and N =2,..,100 for up to ten million episodes each.\nOur goal is to investigate their empirical performance and\nscaling. We believe this is the ﬁrst ever large scale empir-\nical investigation into the scaling properties of algorithms\nfor efﬁcient exploration.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nWe highlight results for three algorithms with ˜O(\n√\nT)\nBayesian regret bounds:\nUCRL2, Gaussian PSRL and\nPSRL. We implement UCRL2 with conﬁdence sets opti-\nmized for ﬁnite horizon MDPs. For the Bayesian algo-\nrithms we use a uniform Dirichlet prior for transitions and\nN(0,1) prior for rewards. We view these priors as simple\nways to encode very little prior knowledge. Full details and\na link to source code are available in Appendix D.\nFigure 8 display the regret curves for these algorithms for\nN ∈{5,10,30,50}. As suggested by our analysis, PSRL\noutperforms Gaussian PSRL which outperforms UCRL2.\nThese differences seems to scale with the length of the\nchain N and that even for relatively small MDPs, PSRL\nis many orders of magnitude more efﬁcient than UCRL2.\nFigure 8. PSRL outperforms other methods by large margins.\nWe investigate the empirical scaling of these algorithms\nwith respect to N. The results of Theorem 2 and Conjec-\nture 1 only bound the Bayesian regret according to the prior\nφ. The family of environments we consider in this exam-\nple are decidedly not from this uniform distribution; in fact\nthey are chosen to be as difﬁcult as possible. Nevertheless,\nthe results of Theorem 2 and Conjecture 1 provide remark-\nably good description for the behavior we observe.\nDeﬁne learning time(π,N):=min\nn\nK | 1\nK\nPK\nk=1∆k ≤0.1\no\nfor the algorithm π on the MDP from Figure 7 with size N.\nFor any Bπ >0, the regret bound ˜O(√BπT) would imply\nlog(learning time)(π,N)=BπH ×log(N)+o(log(N)).\nIn the cases of Figure 7 with H =S =N then the bounds\n˜O(HS\n√\nAT),\n˜O(H\n√\nSAT) and\n˜O(\n√\nHSAT) would\nsuggest a slope Bπ of 5,4 and 3 respectively.\nRemarkably, these high level predictions match our empiri-\ncal results almost exactly, as we show in Figure 9. These re-\nsults provide some support to Conjecture 1 and even, since\nthe spirit of these environments is similar example used\nin existing proofs, the ongoing questions of fundamental\nlower bounds (Osband & Van Roy, 2016). Further, we note\nthat every single seed of PSRL and Gaussian PSRL learned\nthe optimal policy for every single N. We believe that this\nsuggests it may be possible to extend our Bayesian analy-\nsis to provide minimax regret bounds of the style in UCRL2\nfor suitable choice of diffuse uninformative prior.\nFigure 9. Empirical scaling matches our conjectured analysis.\n6. Conclusion\nPSRL is orders of magnitude more statistically efﬁcient\nthan UCRL and the same computational cost as solving a\nknown MDP. We believe that analysts will be able to for-\nmally specify an OFU approach to RL whose statistical ef-\nﬁciency matches PSRL. However, we argue that the result-\ning conﬁdence sets which address both the coupling over H\nand S may result in a computationally intractable optimiza-\ntion problem. Posterior sampling offers a computationally\ntractable approach to statistically efﬁcient exploration.\nWe should stress that the ﬁnite tabular setting we analyze is\nnot a reasonable model for most problems of interest. Due\nto the curse of dimensionality, RL in practical settings will\nrequire generalization between states and actions. The goal\nof this paper is not just to improve a mathematical bound\nin a toy example (although we do also do that). Instead,\nwe hope this simple setting can highlight some shortcom-\nings of existing approaches to “efﬁcient RL” and provide\ninsight into why algorithms based on sampling may offer\nimportant advantages. We believe that these insights may\nprove valuable as we move towards algorithms that solve\nthe problem we really care about: synthesizing efﬁcient ex-\nploration with powerful generalization.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nAcknowledgements\nThis work was generously supported by DeepMind, a re-\nsearch grant from Boeing, a Marketing Research Award\nfrom Adobe, and a Stanford Graduate Fellowship, cour-\ntesy of PACCAR. The authors would like to thank Daniel\nRusso for many hours of discussion and insight leading to\nthis research, Shipra Agrawal and Tor Lattimore for point-\ning out several ﬂaws in some early proof steps, anonymous\nreviewers for their helpful comments and many more col-\nleagues at DeepMind including Remi Munos, Mohammad\nAzar and more for inspirational conversations.\nReferences\nAraya, Mauricio, Buffet, Olivier, and Thomas, Vincent.\nNear-optimal brl using optimistic local transitions. arXiv\npreprint arXiv:1206.4613, 2012.\nAsmuth, John, Li, Lihong, Littman, Michael L, Nouri, Ali,\nand Wingate, David. A Bayesian sampling approach to\nexploration in reinforcement learning. In Proceedings of\nthe Twenty-Fifth Conference on Uncertainty in Artiﬁcial\nIntelligence, pp. 19–26. AUAI Press, 2009.\nBartlett, Peter L. and Tewari, Ambuj. REGAL: A regu-\nlarization based algorithm for reinforcement learning in\nweakly communicating MDPs.\nIn Proceedings of the\n25th Conference on Uncertainty in Artiﬁcial Intelligence\n(UAI2009), pp. 35–42, June 2009.\nBellman, Richard and Kalaba, Robert. On adaptive control\nprocesses. IRE Transactions on Automatic Control, 4(2):\n1–9, 1959.\nBrafman, Ronen I. and Tennenholtz, Moshe. R-max - a\ngeneral polynomial time algorithm for near-optimal re-\ninforcement learning. Journal of Machine Learning Re-\nsearch, 3:213–231, 2002.\nBuldygin, Valerii V and Kozachenko, Yu V. Sub-gaussian\nrandom variables. Ukrainian Mathematical Journal, 32\n(6):483–489, 1980.\nBurnetas, Apostolos N and Katehakis, Michael N. Optimal\nadaptive policies for Markov decision processes. Math-\nematics of Operations Research, 22(1):222–255, 1997.\nDani, Varsha, Hayes, Thomas P., and Kakade, Sham M.\nStochastic linear optimization under bandit feedback. In\nCOLT, pp. 355–366, 2008.\nDann, Christoph and Brunskill, Emma. Sample complex-\nity of episodic ﬁxed-horizon reinforcement learning. In\nAdvances in Neural Information Processing Systems, pp.\nTBA, 2015.\nFilippi, Sarah, Cappé, Olivier, and Garivier, Aurélien. Op-\ntimism in reinforcement learning and kullback-leibler di-\nvergence. In Communication, Control, and Computing\n(Allerton), 2010 48th Annual Allerton Conference on,\npp. 115–122. IEEE, 2010.\nFonteneau, Raphaël, Korda, Nathan, and Munos, Rémi.\nAn optimistic posterior sampling strategy for Bayesian\nreinforcement learning.\nIn NIPS 2013 Workshop on\nBayesian Optimization (BayesOpt2013), 2013.\nGopalan, Aditya and Mannor, Shie. Thompson sampling\nfor learning parameterized Markov decision processes.\narXiv preprint arXiv:1406.7498, 2014.\nHadar, Josef and Russell, William R. Rules for ordering\nuncertain prospects. The American Economic Review,\npp. 25–34, 1969.\nJaksch, Thomas, Ortner, Ronald, and Auer, Peter. Near-\noptimal regret bounds for reinforcement learning. Jour-\nnal of Machine Learning Research, 11:1563–1600,\n2010.\nKakade, Sham. On the Sample Complexity of Reinforce-\nment Learning. PhD thesis, University College London,\n2003.\nKearns, Michael J. and Singh, Satinder P. Near-optimal\nreinforcement learning in polynomial time.\nMachine\nLearning, 49(2-3):209–232, 2002.\nKolter, J Zico and Ng, Andrew Y. Near-Bayesian explo-\nration in polynomial time. In Proceedings of the 26th\nAnnual International Conference on Machine Learning,\npp. 513–520. ACM, 2009.\nLattimore, Tor and Hutter, Marcus. PAC bounds for dis-\ncounted MDPs. In Algorithmic learning theory, pp. 320–\n334. Springer, 2012.\nMunos, Rémi. From bandits to monte-carlo tree search:\nThe optimistic principle applied to optimization and\nplanning. 2014.\nOsband, Ian.\nDeep Exploration via Randomized Value\nFunctions. PhD thesis, Stanford, 2016.\nOsband, Ian and Van Roy, Benjamin. Model-based rein-\nforcement learning and the eluder dimension.\nIn Ad-\nvances in Neural Information Processing Systems, pp.\n1466–1474, 2014a.\nOsband, Ian and Van Roy, Benjamin. Near-optimal rein-\nforcement learning in factored MDPs. In Advances in\nNeural Information Processing Systems, pp. 604–612,\n2014b.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nOsband, Ian and Van Roy, Benjamin. On lower bounds\nfor regret in reinforcement learning.\narXiv preprint\narXiv:1608.02732, 2016.\nOsband, Ian and Van Roy, Benjamin. Gaussian-dirichlet\nposterior dominance in sequential learning.\narXiv\npreprint arXiv:1702.04126, 2017.\nOsband, Ian, Russo, Daniel, and Van Roy, Benjamin.\n(More) efﬁcient reinforcement learning via posterior\nsampling. In NIPS, pp. 3003–3011. Curran Associates,\nInc., 2013.\nOsband, Ian, Van Roy, Benjamin, and Wen, Zheng. Gen-\neralization and exploration via randomized value func-\ntions. arXiv preprint arXiv:1402.0635, 2014.\nRusso, Daniel and Van Roy, Benjamin. Learning to opti-\nmize via posterior sampling. Mathematics of Operations\nResearch, 39(4):1221–1243, 2014.\nStrehl, Alexander L and Littman, Michael L. A theoretical\nanalysis of model-based interval estimation. In Proceed-\nings of the 22nd international conference on Machine\nlearning, pp. 856–863. ACM, 2005.\nStrehl, Alexander L., Li, Lihong, Wiewiora, Eric, Lang-\nford, John, and Littman, Michael L.\nPAC model-free\nreinforcement learning. In ICML, pp. 881–888, 2006.\nStrens, Malcolm J. A. A Bayesian framework for reinforce-\nment learning. In ICML, pp. 943–950, 2000.\nSutton, Richard and Barto, Andrew. Reinforcement Learn-\ning: An Introduction. MIT Press, March 1998.\nSzita, István and Szepesvári, Csaba.\nModel-based rein-\nforcement learning with nearly tight exploration com-\nplexity bounds. In Proceedings of the 27th International\nConference on Machine Learning (ICML-10), pp. 1031–\n1038, 2010.\nThompson, W.R. On the likelihood that one unknown prob-\nability exceeds another in view of the evidence of two\nsamples. Biometrika, 25(3/4):285–294, 1933.\nVlassis, Nikos, Ghavamzadeh, Mohammad, Mannor, Shie,\nand Poupart, Pascal. Bayesian reinforcement learning. In\nReinforcement Learning, pp. 359–386. Springer, 2012.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nAPPENDICES\nA. Proof of Lemma 3\nThis section centers around the proof of Lemma 3, which we reproduce below for completeness. In the main paper we\npresent a simple sketch for the special case of S =2. We now extend this argument to general MDPs with S >2. The main\nstrategy for this proof is to proceed via an inductive argument and consider the contribution of each component of Pk in\nturn. We will see that, for any choice of component, the resultant random variable is dominated by a matched Gaussian\nrandom variable just as in (12).\nLemma 3 (Transition concentration). For any independent prior over rewards with r∈[0,1], additive sub-Gaussian noise\nand an independent Dirichlet prior over transitions at state-action pair xkh, then\nwP\nh (xkh)≤2H\ns\n2log(2/δ)\nmax(nk(xkh)−2,1)\n(11)\nwith probability at least 1−δ.\nOur analysis of Lemma 3 will rely heavily upon the technical analysis of Osband & Van Roy (2017). We ﬁrst reproduce\nLemma 2 from Osband & Van Roy (2017) in terms of stochastic optimism, rather than second order stochastic dominance.\nLemma 4 (Beta vs Dirichlet dominance).\nLet X =P ⊤v for the random variable P ∼Dirichlet(α) and constants v∈RS and α∈RS\n+. Without loss of generality,\nassume v1 ≤v2 ≤···≤vS. Let ˜α=Ps\ni=1αi(vi −v1)/(vd −v1) and ˜β =Pd\ni=1αi(vd −vi)/(vd −v1). Then, there exists a\nrandom variable ˜P ∼Beta(˜α, ˜β) such that, for ˜X = ˜Pvd +(1−˜P)v1, E[ ˜X|X]=X and ˜X ≽so X.\nProof. Let γi =Gamma(α,1) be independent and identically distributed and let γ =Pd\ni=1γi, so that P ≡D γ/γ. Let\nα0\ni =αi(vi −v1)/(vd −v1) and α1\ni =αi(vd −vi)/(vd −v1) so that α=α0 +α1. Deﬁne independent random variables\nγ0 ∼Gamma(α0\ni ,1) and γ1 ∼Gamma(α1\ni ,1) so that γ ≡D γ0 +γ1.\nTake γ0 and γ1 to be independent, and couple these variables with γ so that γ =γ0 +γ1. Note that ˜β =Pd\ni=1α0\ni and\n˜α=Pd\ni=1α1\ni . Let γ0 =Pd\ni=1γ0\ni and γ1 =Pd\ni=1γ1\ni , so that 1−˜P ≡D γ0/γ and ˜P ≡D γ1/γ. Couple these variables so that\n1−˜P =γ0/γ and ˜P =γ1/γ. We can now say,\nE[ ˜X|X]\n=\nE[(1−˜P)v1 + ˜Pvd|X]=E\n\u0014v1γ0\nγ\n+ vdγ1\nγ\n\f\f\fX\n\u0015\n=\nE\n\u0014\nE\n\u0014v1γ0 +vdγ1\nγ\n\f\f\fγ,X\n\u0015\f\f\fX\n\u0015\n=E\n\u0014v1E[γ0|γ]+vdE[γ1|γ]\nγ\n\f\f\fX\n\u0015\n=\nE\n\"\nv1\nPd\ni=1E[γ0\ni |γi]+vd\nPd\ni=1xp[γ1\ni |γi]\nγ\n\f\f\fX\n#\n(a)=\nE\n\"\nv1\nPd\ni=1γiα0\ni /αi +vd\nPd\ni=1γiα1\ni /αi\nγ\n\f\f\fX\n#\n=\nE\n\"\nv1\nPd\ni=1γi(vi −v1)+vd\nPd\ni=1γi(vd −vi)\nγ(vd −v1)\n\f\f\fX\n#\n=\nE\n\"Pd\ni=1γivi\nγ\n\f\f\fX\n#\n=E\n\" d\nX\ni=1\npivi\n\f\f\fX\n#\n=X,\nwhere (a) follows from elementary properties of Gamma distribution (Osband & Van Roy, 2017). Therefore, ˜X is a\nmean-preserving spread of X and so by deﬁnition of stochastic optimism ˜X ≽so X.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nNext, consider any ﬁxed Pk(xkh) and let Rk and Pk(x̸=xkh) vary in any arbitrary way to maximize the variation from\ntransition wP\nk (xkh)=(Pk(xkh)−ˆP(xkh))T V k\nkh+1 through their effects on the future value V k\nkh+1 ∈[0,H]S. We can then\nupper bound the deviation from transitions by the deviation under the worst possible v∈[0,H]S.\nwP\nh (xkh)≤\nmax\nRk,Pk(x̸=xkh)(Pk(xkh)−ˆPk(xkh))T V k\nkh+1 ≤\nmax\nv∈[0,H]S(Pk(xkh)−ˆPk(xkh))T v.\n(16)\nWe can then apply Lemma 4 to (16): for any possible value of v∈[0,H]S there is a matched Beta random variable that\nis stochastically optimistic for wP\nh (xkh). This means that we can then apply Lemma 2 to show that there is a matched\nX ∼\n\u0010\n0, H2\nαT 1\n\u0011\n≽so wP\nh (xkh). To complete the proof of Lemma 3 we apply the Gaussian tail concentration Lemma 1.\nB. Conjecture of ˜O(\n√\nHSAT) bounds\nThe key remaining loose piece of our analysis concerns the summation PH\nh=1wP\nh (xkh). Our current proof of Theorem\n2 bounds each wP\nh (xkh) independently. Each term is ˜O(\nq\nH\nnk(xkh)) and we bound the resulting sum ˜O(H\nq\nH\nnk(xkh)).\nHowever, this approach is very loose and pre-supposes that each timestep could be maximally bad during a single episode.\nTo repeat our geometric intuition, we have assumed a worst-case hyper-rectangle over all timesteps H when the actual\ngeometry should be an ellipse. We therefore suffer an additional term of ˜O(\n√\nH) in exactly the style of Figure 3.\nIn fact, it is not even possible to sequentially get the “worst-case” transitions O(H) at each and every timestep during an\nepisode, since once your sample gets one such transition then there will be no more future value to deplete. Rather than just\nbeing independent per timestep, which would be enough for us to end up with an ˜O(\n√\nH) saving, they actually have some\nkind of anti-correlation property through the law of total variance. A very similar observation is used by recent analyses in\nthe sample complexity setting (Lattimore & Hutter, 2012) and also ﬁnite horizon MDPs (Dann & Brunskill, 2015). This\nseems to suggest that it should be possible to combine the insights of Lemma 3 with, for example, Lemma 4 of (Dann &\nBrunskill, 2015) to remove both the\n√\nS and the\n√\nH from our bounds to prove Conjecture 1.\nWe note that this informal argument would not apply Gaussian PSRL, since it generates wP from some Gaussian posterior\nwhich does not satisfy the Bellman operators. Therefore, we should be able to ﬁnd some evidence for this conjecture if we\nﬁnd domains where UCRL, Gaussian PSRL and PSRL all demonstrate their (unique) predicted scalings. We present some\nevidence of this effect in Section 5.3 and ﬁnd that that our empirical results are consistent with this conjecture.\nC. Estimation experiments\nIn this section we expand upon the simple examples given by Section 4.1 to a full decision problem with two actions. We\ndeﬁne an MDP similar to Figures 1 and 2 but now with two actions. The ﬁrst action is identical to Figure 1, but the second\naction modiﬁes the transition probabilities to favor the rewarding states with probability 0.6/N and assigning only 0.4/N\nto the non-rewarding states.\nWe now investigate the regret of several learning algorithms which we adapt to this setting. These algorithms are based\nupon BEB (Kolter & Ng, 2009), BOLT (Araya et al., 2012), ϵ-greedy with ϵ=0.1, Gaussian PSRL (see Algorithm 3),\nOptimistic PSRL (which takes K =10 samples and takes the maximum over sampled Q-values similar to BOSS (Asmuth\net al., 2009)), PSRL (Strens, 2000), UCFH (Dann & Brunskill, 2015) and UCRL2 (Jaksch et al., 2010). We link to the full\ncode for implementation in Appendix D.\nWe see that the loose estimates in OFU algorithms from Figures 4 and 5 lead to bad performance in a decision problem.\nThis poor scaling with the number of successor states N occurs when either the rewards or the transition function is\nunknown. We note that in stochastic environments the PAC-Bayes algorithm BOLT, which relies upon optimistic fake\nprior data, can sometimes concentrate too quickly and so incur the maximum linear regret. In general, although BOLT is\nPAC-Bayes, it concentrates too fast to be PAC-MDP just like BEB (Kolter & Ng, 2009).\nIn Figure 12 we see a similar effect as we increase the episode length H. We note the second order UCFH modiﬁcation\nimproves upon UCRL2’s miscalibration with H, as is reﬂected in their bounds (Dann & Brunskill, 2015). We note that\nboth BEB and BOLT scale poorly with the horizon H.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nFigure 10. Known rewards R and unknown transitions P, similar to Figure 4.\nFigure 11. Unknown rewards R and known transitions P, similar to Figure 5.\nFigure 12. Unknown rewards R and transitions P, similar to Figure 6.\nD. Chain experiments\nAll of the code and experiments used in this paper are available in full on github. As per the review request we have removed\nthe link to this code, but instead include an anonymized excerpt of the some of the code in our submission ﬁle. We hope\nthat researchers will ﬁnd this simple codebase useful for quickly prototyping and experimenting in tabular reinforcement\nlearning simulations.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nIn addition to the results already presented we also investigate the scaling of similar Bayesian learning algorithms BEB\n(Kolter & Ng, 2009) and BOLT (Araya et al., 2012). We see that neither algorithms scale as gracefully as PSRL, although\nBOLT comes close. However, as observed in Appendix C, BOLT can perform poorly in highly stochastic environments.\nBOLT also requires S-times more computational cost than PSRL or BEB. We include these algorithms in Figure 13.\nFigure 13. Scaling of more learning algorithms.\nD.1. Rescaling conﬁdence sets\nIt is well known that provably-efﬁcient OFU algorithms can perform poorly in practice. In response to this observation,\nmany practitioners suggest rescaling conﬁdence sets to obtain better empirical performance (Szita & Szepesvári, 2010;\nAraya et al., 2012; Kolter & Ng, 2009). In Figure 14 we present the performance of several algorithms with conﬁdence sets\nrescaled ∈{0.01,0.03,0.1,0.3,1}. We can see that rescaling for tighter conﬁdence sets can sometimes give better empirical\nperformance. However, it does not change the fundamental scaling of the algorithm. Also, for aggressive scalings some\nseeds may not converge at all.\nFigure 14. Rescaled proposed algorithms for more aggressive learning.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nD.2. Prior sensitivities\nWe ran all of our Bayesian algorithms with uninformative independent priors for rewards and transitions. For rewards, we\nuse r(s,a)∼N(0,1) and updated as if the observed noise were Gaussian with precision τ = 1\nσ2 =1. For transitions, we use\na uniform Dirichlet prior P(s,a)∼Dirchlet(α). In Figures 15 and 16 we examine the performance of Gaussian PSRL and\nPSRL on a chain of length N =10 as we vary τ and α=α01.\nFigure 15. Prior sensitivity in Gaussian PSRL.\nFigure 16. Prior sensitivity in PSRL.\nWe ﬁnd that both of the algorithms are extremely robust over several orders of magnitude. Only large values of τ (which\nmeans that the agent updates it reward prior too quickly) caused problems for some seeds in this environment. Developing\na more clear frequentist analysis of these Bayesian algorithms is a direction for important future research.\nD.3. Optimistic posterior sampling\nWe compare our implementation of PSRL with a similar optimistic variant which samples K ≥1 samples from the posterior\nand forms the optimistic Q-value over the envelope of sampled Q-values. This algorithm is sometimes called “optimistic\nposterior sampling” (Fonteneau et al., 2013). We experiment with this algorithm over several values of K but ﬁnd that the\nresultant algorithm performs very similarly to PSRL, but at an increased computational cost. We display this effect over\nseveral magnitudes of K in Figures 17 and 18.\nWhy is Posterior Sampling Better than Optimism for Reinforcement Learning?\nFigure 17. PSRL with multiple samples is almost indistinguishable.\nFigure 18. PSRL with multiple samples is almost indistinguishable.\nThis algorithm “Optimistic PSRL” is spiritually very similar to BOSS (Asmuth et al., 2009) and previous work had sug-\ngested that K >1 could lead to improved performance. We believe that an important difference is that PSRL, unlike\nThompson sampling, should not resample every timestep but previous implementations had compared to this faulty bench-\nmark (Fonteneau et al., 2013).\n",
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2016-07-01",
  "updated": "2017-06-13"
}