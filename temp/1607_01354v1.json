{
  "id": "http://arxiv.org/abs/1607.01354v1",
  "title": "Learning Discriminative Features using Encoder-Decoder type Deep Neural Nets",
  "authors": [
    "Vishwajeet Singh",
    "Killamsetti Ravi Kumar",
    "K Eswaran"
  ],
  "abstract": "As machine learning is applied to an increasing variety of complex problems,\nwhich are defined by high dimensional and complex data sets, the necessity for\ntask oriented feature learning grows in importance. With the advancement of\nDeep Learning algorithms, various successful feature learning techniques have\nevolved. In this paper, we present a novel way of learning discriminative\nfeatures by training Deep Neural Nets which have Encoder or Decoder type\narchitecture similar to an Autoencoder. We demonstrate that our approach can\nlearn discriminative features which can perform better at pattern\nclassification tasks when the number of training samples is relatively small in\nsize.",
  "text": "Learning Discriminative Features using Encoder/Decoder type Deep\nNeural Nets\nVishwajeet Singh1, Killamsetti Ravi Kumar2, K Eswaran3\n1ALPES, Bolarum, Hyderabad 500010, vsthakur@gmail.com\n2ALPES, Bolarum, Hyderabad 500010, ravi.killamsetti@gmail.com\n3SNIST, Ghatkesar, Hyderabad 501301, kumar.e@gmail.com\nAbstract: As machine learning is applied to an increasing variety of complex problems, which are\ndeﬁned by high dimensional and complex data sets, the necessity for “task oriented feature learn-\ning” grows in importance. With the advancement of Deep Learning algorithms, various successful\nfeature learning techniques have evolved.\nIn this paper, we present a novel way of learning discriminative features by training Deep Neural\nNets which have Encoder/Decoder type architecture similar to an Autoencoder. We demonstrate\nthat our approach can learn discriminative features which can perform better at pattern classiﬁca-\ntion tasks when the number of training samples is relatively small in size.\n1.\nIntroduction\nIn the ﬁeld of machine learning and statistics, many linear ([12]), nonlinear ([24] & [15]) and\nstochastic ([19]) methods have been developed to reduce the dimensionality of data so that relevant\ninformation can be used for classiﬁcation of patterns ([21] & [18]). Researchers have solved pattern\nrecognition problems (to varying degrees of success) like face detection [5], gender classiﬁcation\n[13], human expression recognition [14], object learning [1], unsupervised learning of new tasks\n[8] and also have studied complex neuronal properties of higher cortical areas [9]. However, most\nof the above techniques did not require automatic feature extraction as a pre-processing step to\npattern classiﬁcation.\nIn contrast to the above, there exist many practical applications characterized by high dimen-\nsionality of data (such as speech recognition, remote sensing, e.t.c), where ﬁnding sufﬁcient la-\nbeled examples might not be affordable or feasible. At the same time there may be lot of unlabeled\ndata available easily. Unsupervised feature learning techniques, like the Autoencoder ([7], [16] ,\n[3] and [20]), try to capture the essential structure underlying the high-dimensional input data by\nconverting them into lower dimensional data without losing information. Autoencoder follows an\nEncoder/Decoder type neural network architecture (see ﬁgure 2), where the dimensionality of the\ninput and the output layers are the same. The output of the network is forced (via learning) to\nbe the same as it’s input. Typically all the other layers in the network are smaller in size when\ndimensionality reduction is the goal of learning. This way they learn features that are much lower\nin dimension as compared to the input data and are rich in information to later perform pattern\nclassiﬁcation on the labeled data sets.\nThe primary aim of dimensionality reduction for pattern classiﬁcation problems is to remove\nthe unnecessary information from data and extract information which is meaningful for achieving\n1\narXiv:1607.01354v1  [cs.LG]  22 Mar 2016\nefﬁcient pattern recognition/classiﬁcation. With the advent of Autoencoder and various forms of\nUnsupervised Feature Learning, a signiﬁcant amount of success is achieved in this aspect. But\nthese methods demand large amount of data to be available for learning.\nFig. 1. Input-to-Output Mapping of an Autoencoder\nAnother very important aspect is that by mapping the input back to itself as output, the Autoen-\ncoder network retains lot of additional information present in the input which is not relevant to the\npattern classiﬁcation problem. To elaborate further, ﬁgure 1 depicts the mapping of an Autoen-\ncoder where it is trying to learn handwritten digits. The ﬁrst two inputs, although they represent\nthe same character zero, the network is forced to learn the thickness and the exact shape of the\nhandwritten digit. Features learnt by this approach still contain lot of information which is not\nuseful for pattern classiﬁcation and hence can be treated as noisy. When the amount of data avail-\nable to train these networks is reduced, their ability to learn discriminative features also reduces\nsigniﬁcantly, as will be shown in section 4.\nFig. 2. Architecture of Encoder/Decoder Type Neural Networks\nIn this paper we focus on the scenario where there is very little labeled data per class and zero\nunlabeled data available. In this context we describe a novel way of learning discriminative features\nusing Deep Neural Nets which have an Encoder/Decoder architecture (see ﬁgure 2). We refer to\nthis network as “Discriminative Encoder”. Section 2 introduces the concept of “Discriminative\nEncoder” and explains how it is different from the Autoencoder. Sections 3 and 4 provide the\nresults of benchmarking “Discriminative Encoder” on standard machine learning data sets. The\nunique feature of this study is that we have benchmarked the performance on data sets of varying\nsizes in terms of number of training samples and number of classes. Lastly, Section 5 concludes\n2\nwith the ﬁndings and future direction.\n2.\nDiscriminative Encoder\nThe motivation behind this approach is to extract meaningful information from a relatively small\nset of labeled samples such that:\n1. features learnt are less sensitive to intra-class difference in the inputs of samples belonging to\nthe same class\n2. features learnt are highly sensitive to inter-class differences in the inputs of samples belonging\nto different class\nTo achieve this we use the Encoder/Decoder neural network architecture similar to the Autoen-\ncoder. The difference is that instead of mapping the input back to itself as output (i.e, output =\ninput), we make the input map to an “Ideal Input” of the class. Figure 3 depicts this concept, here\nwe are making all the handwritten zeros to map to an “Ideal Zero” (or a standard template of zero).\nSimilarly, all the handwritten ones are mapped to an “’Ideal One”.\nFig. 3. Input-to-Output Mapping of a Discriminative Encoder\nThis mapping forces the network to ignore the differences between samples belonging to the\nsame class and focus on differences between samples belonging to different classes. In simple\nwords, the features learnt this way are “discriminative” in nature.\n3.\nExperiments\nThe purpose of the present exercise is to benchmark the Discriminative Encoder network on\ndatasets of varying sizes in terms of the number of training samples and the number of classes.\nTable 1 summarizes the datasets used in this study.\nThe uniqueness of this work is that we have used much compact or simpler models, in terms\nof number of parameters, when compared to similar work in the ﬁeld of Deep Learning ([4]). The\nresults obtained are comparable with the state-of-the research in deep learning (some of which are\ncited).\n3\nName\nNumber of\nclasses\nTotal number\nof sample\n#Input\nFeatures\nExtended Yale\nFace Dataset\n(Frontal Pose)\n38\n2432\n900\nExtended Yale\nFace Dataset\n(All Poses)\n28\n11482\n900\nNCKU\nTaiwan Face\nDataset\n90\n3330\n768\nMNIST\nDataset\n10\n70000\n784\nTable 1 List of datasets used for benchmarking\n3.1.\nExtended Yale B Data Set (Frontal Pose)\nThe Extended Yale B (Frontal Pose) Face data set ([6] and [11]) consists of frontal images of\n38 subjects taken across 64 different illumination conditions (64 images per subject and 2432\nimages in total). The size of the original images was 168x192, which was reduced to 30x30 in our\nexperimentation (i.e. we have 900 features per input image). Some of the sample images from this\ndata set are shown in ﬁgure 4.\nFig. 4. Sample faces from Extended Yale B (Frontal Pose) dataset\nTo train the Discriminative Encoder of dimension 400−200−64−900 (this convention represent\nthe number of processing elements in each layer), 48 images per subject (1824 in total) were taken\nfrom the data set and the remaining 16 per subject (608 in total) were kept for testing the classiﬁer.\nOnce the network was trained with high accuracy, the data set was reduced from 900 dimension\nfeature vector to 64 dimensional feature vector. The results of using supervised classiﬁers on the\n64 dimensional data set are described in table 2\nAt this point, we would like to highlight that this particular data set has been used extensively in\nthe Machine Learning community to benchmark Face Recognition algorithms, although a lot of the\npublished work makes use of domain speciﬁc knowledge to build their face recognition systems\nand can possibly ignore the dark images in the dataset (see table 3 for comparison). The work in\n[22] has utilized large databases for 3D morphological modeling and albedo modeling. We have\nneither made use of any domain speciﬁc knowledge, nor have we removed any dark images from\nthe data set. It may be noticed that classiﬁcation in the reduced dimensional space, in general,\n4\nFig. 5. Input-to-Output Mapping of a Discriminative Encoder for the Extended Yale B (Frontal\nPose) dataset\nwould give a better classiﬁcation as the noise in the original data set would have been removed\nduring training.\nClassiﬁer\nSetting\nAccuracy\nin R900\nAccuracy\nin R64\nNeural\nNetwork\n75-50-38\n98.3%\nk-Nearest\nNeighbor\nk=3\n60.6%\n97.3%\nk-Nearest\nNeighbor\nk=5\n60.3%\n97.5%\nk-Nearest\nNeighbor\nk=7\n58.5%\n97.5%\nk-Nearest\nNeighbor\nk=9\n56.7%\n97.5%\nTable 2 Results on Extended Yale B (Frontal) Face data set\n3.2.\nExtended Yale B Data Set (All Poses)\nThe Extended Yale B data set ([6]) consists of images of 28 subjects taken across 9 poses and 64\ndifferent illumination conditions (576 images per subject and 16128 images in total). Some of the\nsample images from this data set are shown in ﬁgure 6.\nThe original images contained lot of background information and hence we had to extract only\nthe faces from the images ﬁrst, which was done using OpenCV library. Of the total 16128 images,\nfaces could be detected only in 11482 images and the rest were quiet dark for the faces to be\ndetected. The reduced data set contains approximately 410 images per subject. The size of the\nimages was reduced to 30x30 in our experimentation (i.e. we have 900 features per input image).\nTo train the Discriminative Encoder of dimension 400 −200 −64 −900, 8600 images were\ntaken from the reduced data set and the remaining 2882 images were kept for testing the classiﬁer.\n5\nStudy\n#Subjects\n#Train\nImages per\nSubject\n#Model\nParams\n(million)\nAccuracy\nCurrent\nPaper\n38\n48\n0.5\n98.3%\nHinton et.\nal. [22]\n10\n7\n1.3\n97%\nTable 3 Comparison of results on Extended Yale B (Frontal) data set\nFig. 6. Sample of faces from Extended Yale B (All Pose) Face data set\nOnce the network was trained with high accuracy, the data set was reduced from 900 dimension\nfeature vector to a 64 dimensional feature vector. The results of using supervised classiﬁers on the\n64 dimensional data set are described in table 4\n3.3.\nTaiwan Face Data Set\nThis data set [23] is provided by the Robotics Lab of the Dept of Computer Science of National\nCheng Kung University in Taiwan. The whole database contains 6660 images of 90 subjects. Each\nsubject has 74 images, where 37 images were taken every 5 degree from right proﬁle (deﬁned as\n+90o) to left proﬁle (deﬁned as −90o) in the pan rotation. The remaining 37 images are generated\n(synthesized) by the existing 37 images using commercial image processing software in the way\nof ﬂipping them horizontally. Some sample images from the dataset are shown in Figure 7\nFig. 7. Sample of faces from Taiwan Face data set\nIn our experiments, we have considered only half of this data set, i.e., 3330 images of 90\nsubjects and each subject has 37 images which were taken every 5 degree from right proﬁle (deﬁned\nas +90o) to left proﬁle (deﬁned as −90o) in the pan rotation. In all the images, only the face part of\nthe image was retained and the region containing the clothes on subjects body were trimmed from\nthe original image. Later the images were reduced to 24x32 pixels size (i.e. 768 features).\nTo train network of dimension 196 −64 −25 −768, 26 images per subject (2340 in total) were\ntaken from the data set and the remaining 11 per subject (990 in total) were kept for testing the\nclassiﬁer. Once the network was trained, the data set was reduced from 768 dimension feature\nvector to a 25 dimensional feature vector. The results of using supervised classiﬁers on the 25\ndimensional data set are described in table 5\n6\nClassiﬁer\nSetting\nAccuracy\nin R900\nAccuracy\nin R64\nNeural\nNetwork\n75-50-38\n95.7%\nk-Nearest\nNeighbor\nk=3\n81.6%\n95.4%\nk-Nearest\nNeighbor\nk=5\n81.3%\n95.4%\nk-Nearest\nNeighbor\nk=7\n81.0%\n95.4%\nk-Nearest\nNeighbor\nk=9\n80.5%\n95.3%\nTable 4 Results on Extended Yale B (All Pose) face data set\nClassiﬁer\nSetting\nAccuracy\nin R768\nAccuracy\nin R25\nNeural\nNetwork\n25-50-90\n99.5%\nk-Nearest\nNeighbor\nk=3\n97.171%\n99.6%\nk-Nearest\nNeighbor\nk=5\n94.44%\n99.6%\nk-Nearest\nNeighbor\nk=7\n91.81%\n99.6%\nk-Nearest\nNeighbor\nk=9\n89.09%\n99.6%\nTable 5 Results on Taiwan Face data set\n3.4.\nMNIST Data Set\nThe MNIST database ([10]) of images of handwritten digits (0-9) is a standard benchmark data set\nused in the machine learning community. It has a training set of 60,000 examples (approximately\n6000 examples per digit), and a test set of 10,000 examples. The dimensionality of images is 28x28\n(i.e. 784 features per input to the network).\nFig. 8. MNIST Input and Output of the trained network\nThe network architecture contains 225−100−36−784 processing elements in the layers of the\nnetwork. With sufﬁcient amount of training, this network was able to learn all the mapping with\nhigh accuracy (See Figure 8). The trained network was then used to reduce the dimensionality\nof the entire data set from R784 to R36. Table 6 shows the results of using supervised classiﬁers\n7\n(k-Nearest Neighbor classiﬁer and Neural Network) to classify the reduced 36 dimensional data.\nClassiﬁer\nSetting\nAccuracy\nin R784\nAccuracy\nin R36\nNeural\nNetwork\n36-5-10\n98.08%\nk-Nearest\nNeighbor\nk=3\n97.05%\n97.5%\nk-Nearest\nNeighbor\nk=5\n96.88%\n97.5%\nk-Nearest\nNeighbor\nk=7\n96.94%\n97.6%\nk-Nearest\nNeighbor\nk=9\n96.59%\n97.6%\nTable 6 Results on MNIST data set\nIn the case of MNIST data set, k-Nearest Neighbor works in the high dimensional space due to\nthe availability of lot of training data, which appears to be reasonably clustered.\nSome of the state-of-the-art algorithms, like [16] and [2], use atleast 7 times more the number\nof parameters (weights) as compared to the ones used in this paper (see table 7).\nStudy\nMethod\n#Model\nParams\n(million)\nAccuracy\nThis Paper\nDiscriminative\nEncoder\n0.23\n98.08%\nHinton et.\nal. [16]\nAutoencoder\n1.7\n99%\nSchmidhuber\net. al. [2]\nSimple Deep\nNeural Nets +\nElastic\nDistortions\n11.9 mil\n99.65%\nTable 7 Comparison of results on MNIST data set\n4.\nComparative Analysis\nThis section discusses the results of comparative analysis of a k-Nearest Neighbor (kNN) classi-\nﬁer, here k=3, performance on various dimensionality reduction approaches. Table 8 shows the\nresults of performing kNN classiﬁcation on the data sets in the original input space (IS), after\ndimensionality reduction by principal component analysis (PCA), after dimensionality reduction\nby Autoencoder (AE) and ﬁnally after dimensionality reduction by Discriminative Encoder (DE).\nThe table also shows the network architectures of Autoencoder and Discriminative Encoder. It is\nalso important to note that we have not used Boltzman pre-training for either Autoencoder or for\nDiscriminative Encoder. Backpropagation algorithm with mini-batch gradient descent was used to\n8\ntrain the networks after random initialization of weights.\nDataset\nInput\nSpace\nSize\nReduced\nSpace\nSize\nNetwork\n(AE)\nNetwork\n(DE)\nIS\nPCA\nAE\nDE\nYale\n(Frontal\nPose)\n900\n64\n400-200-\n64-200-\n400-900\n400-200-\n64-900\n60.6%\n51.4%\n82.4%\n97.3%\nYale (All\nPoses)\n900\n64\n400-200-\n64-200-\n400-900\n400-200-\n64-900\n81.6%\n74.6%\n89.1%\n95.4%\nTaiwan\nFace Db\n768\n25\n196-64-25-\n64-196-768\n196-64-25-\n768\n97.1%\n96.9%\n96.8%\n99.6%\nMNIST\n784\n36\n225-100-\n36-100-\n225-784\n225-100-\n36-784\n97.0%\n97.3%\n97.0%\n97.5%\nTable 8 Results of 3-NN classiﬁer on all datasets using various dimensionality reduction approaches: IS (original\ninput space), PCA (principal component analysis), AE (autoencoder), DE (discriminative encoder)\n• From tables 1 and 8, we can see that the “Discriminative Encoder” very clearly outperforms\nPCA and Autoencoder on Extended Yale (Frontal Pose) Face dataset where the the number\nof samples is the least. It also performs much better on Extended Yale (All Pose) Face dataset\nand on Taiwan Face dataset as compared to PCA and Autoencoder. When the number of\nsamples increase in the MNIST case, we can see that the performance of all the dimensionality\nreduction approaches (PCA, Autoencoder and Discriminative Encoder) is almost alike. These\nresults support our claim that the Discriminative Encoder is good at extracting discriminative\nfeatures even when the number of samples is less.\n• An observation regarding the performance of Autoencoder and Discriminative Encoder on\nthe Yale dataset. It can be seen that the performance of Autoencoder increases in “All Pose”\ndataset when compared to “Frontal Pose” dataset, while the performance of Discriminative\nEncoder decreases. Autoencoders improved performance can directly be attributed to the\nincrease in the availability of training data. In case of Discriminative Encoder, the slight\ndecrease in performance is due to the fact that the mapping that it is trying to learn is getting\ncomplicated, wherein the network tries to map different poses and illumination conditions to\nthe frontal pose and illumination condition. Overall, the Discriminative Encoder performs\nmuch better than the Autoencoder on both of these datasets.\n• How does Discriminative Encoder perform better when there are few training samples ? Dis-\ncriminative Encoders forces all the samples belonging to the same class map to the “Ideal\nInput” of that class. This is a kind of supervisory feedback in the learning process, which\nthe Autoencoder does not have. Due to this supervisory feedback the Discriminative Encoder\nreceives during the training, it is able to extract lot of discriminative information available in\nthe training set.\n9\n5.\nConclusion\nIn this paper, we have presented a novel way of learning discriminative features by training En-\ncoder/Decoder type Deep Neural Nets. We have demonstrated that our approach can learn discrim-\ninative features which can perform better at pattern classiﬁcation tasks when the number of training\nsamples is relatively small in size. Also, we have found that when the number of samples to train\nare less in number, then relatively smaller sized networks (fewer processing elements per layer)\ncan learn complex features, without any domain speciﬁc knowledge, and give high performance\non pattern recognition tasks.\nWe would like to further our research by introducing the stacking and denoising approaches\nto train deep neural networks ([17]). Also we would like to explore feature learning in an semi-\nsupervised setting.\n6.\nReferences\n6.1.\nJournal articles\n[1] Pierre, Baldi., Kurt, Hornik.: ’Neural networks and principal component analysis: Learning\nfrom examples without local minima’, Neural Networks., 2, (1), pp. 53–58, 1989.\n[2] Dan, Claudiu, Ciresan., Ueli, Meier., Luca, Maria, Gambardella., et al.:’Deep big simple neural\nnets excel on handwritten digit recognition’, Neural Computation , 22, Number 12\n[3] Dasika, Ratna, Deepthi., K, Eswaran.: ’A mirroring theorem and its application to a new\nmethod of unsupervised hierarchical pattern classiﬁcation’, International Journal of Computer\nScience and Information Security, 6, pp. 016–025, 2009.\n[4] K, Eswaran., Vishwajeet, Singh.:’Some Theorems for Feed Forward Neural Networks’, Inter-\nnational Journal of Computer Applications, 130, pp. 1–17, 2015.\n[5] Christophe, Garcia., Manolis, Delakis.:’Convolutional face ﬁnder: A neural architecture for\nfast and robust face detection’, IEEE Trans. Pattern Analysis and Machine Intelligence, 26,\n(11), pp. 1408–1423, November 2004.\n[6] A, S, Georghiades., P,N, Belhumeur., D, J, Kriegman.: ’From few to many: Illumination cone\nmodels for face recognition under variable lighting and pose’, IEEE Trans. Pattern Analysis\nand Machine Intelligence, 23, (6), pp. 643–660, 2001\n[7] G, E, Hinton., R, R, Salakhutdinov.: ’Reducing the dimensionality of data with neural net-\nworks’, Science, 313, (5786), pp. 504–507, July 2006.\n[8] J, J, Hopﬁeld., Carlos, D, Brody.: ’Learning rules and network repair in spike-timing-based\ncomputation networks’, Proceedings of the National Academy of Sciences, 101, (1), pp. 337–\n342, 2004.\n[9] Brian, Lau., Garrett, B, Stanley., Yang, Dan.: ’Computational subunits of visual cortical neu-\nrons revealed by artiﬁcial neural networks’, Proceedings of the National Academy of Sciences,\n99, (13), pp. 8974–8979, 2002.\n[10] Y, Lecun., L, Bottou., Y, Bengio., et al.: ’Gradient-based learning applied to document recog-\nnition’, Proceedings of the IEEE, 86, (11), pp. 2278–2324, Nov 1998.\n10\n[11] K,C, Lee., J, Ho., D, Kriegman.: ’Acquiring linear subspaces for face recognition under\nvariable lighting’, IEEE Transactions on Pattern Analysis and Machine Intelligence, 27, (5),\npp. 684–698, 2005.\n[12] K, Pearson.: ’On lines and planes of closest ﬁt to systems of points in space’, Philosophical\nMagazine, 6, (2), pp. 559–572, 1901.\n[13] S,L, Phung., A, Bouzerdoum.: ’A pyramidal neural network for visual pattern recognition’,\nIEEE Transactions on Neural Networks, 18, (2), pp. 329–343, March 2007.\n[14] M, Rosenblum., Y, Yacoob., L,S, Davis.: ’Human expression recognition from motion using\na radial basis function network architecture’, IEEE Transactions on Neural Networks, 7, (5),\npp. 1121–1138, Sep 1996.\n[15] Sam, T, Roweis., Lawrence, K, Saul.: ’Nonlinear dimensionality reduction by locally linear\nembedding’, SCIENCE, 290, (5000), pp. 2323–2326, 2000.\n[16] Ruslan, Salakhutdinov., Geoffrey, E, Hinton.: ’Learning a nonlinear embedding by preserving\nclass neighbourhood structure’, JMLR, 2, pp. 412–419, 2007.\n[17] Pascal, Vincent., Hugo, Larochelle., Isabelle, Lajoie., et al.: ’Stacked Denoising Autoen-\ncoders: Learning Useful Representations in a Deep Network with a Local Denoising Crite-\nrion’, Journal of Machine Learning Research, 11, pp. 3371–3408, 2010.\n[18] Quan, Wang.: ’Kernel principal component analysis and its applications in face recognition\nand active shape models’, Computer Vision and Pattern Recognition, 2012.\n6.2.\nConference Paper\n[19] Ella, Bingham., Heikki, Mannila.: ’Random Projection in Dimensionality Reduction: Ap-\nplications to Image and Text Data’, Proceedings of the Seventh ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, pp. 245–250, 2001.\n[20] Dasika, Ratna, Deepthi., K, Eswaran.: ’Pattern Recognition and Memory Mapping using Mir-\nroring Neural Networks’, IEEE International Conference on Emerging Trends in Computing\n(IEEE, ICETiC 2009)\n[21] Thorsten, Joachims.: ’Text categorization with support vector machines: Learning with many\nrelevant features’, European Conference on Machine Learning, pp. 137–142, 1998.\n[22] Yichuan, Tang., Ruslan, Salakhutdinov., Geoffrey, E, Hinton.: ’Deep lambertian networks’,\nProceedings of the 29th International Conference on Machine Learning ICML 2012.\n6.3.\nWebsites\n[23] ’Face Database from Robotics Lab of National Cheng Kung University. Taiwan’,\nhttp://robotics.csie.ncku.edu.tw/Databases/FaceDetect\nPoseEstimate.htm , accessed 15th\nApril 2015\n6.4.\nBook, book chapter and manual\n11\n[24] B, Scholkopf., A, Smola., K,R, Muller.: ’Kernel principal component analysis’, In Advances\nin Kernel Methods - Support Vector Learning, pp. 327–352, 1999.\n12\n",
  "categories": [
    "cs.LG",
    "stat.ML",
    "I.5; I.5.3"
  ],
  "published": "2016-03-22",
  "updated": "2016-03-22"
}