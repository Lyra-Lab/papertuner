{
  "id": "http://arxiv.org/abs/1607.05822v2",
  "title": "Incremental Learning for Fully Unsupervised Word Segmentation Using Penalized Likelihood and Model Selection",
  "authors": [
    "Ruey-Cheng Chen"
  ],
  "abstract": "We present a novel incremental learning approach for unsupervised word\nsegmentation that combines features from probabilistic modeling and model\nselection. This includes super-additive penalties for addressing the cognitive\nburden imposed by long word formation, and new model selection criteria based\non higher-order generative assumptions. Our approach is fully unsupervised; it\nrelies on a small number of parameters that permits flexible modeling and a\nmechanism that automatically learns parameters from the data. Through\nexperimentation, we show that this intricate design has led to top-tier\nperformance in both phonemic and orthographic word segmentation.",
  "text": "Incremental Learning for Fully Unsupervised Word\nSegmentation Using Penalized Likelihood and Model\nSelection\nRuey-Cheng Chen∗\nSep 1, 2014\nAbstract\nWe present a novel incremental learning approach for unsupervised word segmentation that combines\nfeatures from probabilistic modeling and model selection. This includes super-additive penalties for ad-\ndressing the cognitive burden imposed by long word formation, and new model selection criteria based on\nhigher-order generative assumptions. Our approach is fully unsupervised; it relies on a small number of\nparameters that permits ﬂexible modeling and a mechanism that automatically learns parameters from the\ndata. Through experimentation, we show that this intricate design has led to top-tier performance in both\nphonemic and orthographic word segmentation.\n1\nIntroduction\nInformation criteria are meta-heuristics that measure the quality of statistical ﬁts. Well-known examples\nsuch as Akaike information criterion [1] and minimum description length [24] were developed in the context\nof model selection. These powerful tools deal directly with statistical ﬁts regardless of how these models\nare inferred, thereby allowing one to efﬁciently test a large amount of hypotheses of different distributional\nassumptions.\nDespite being around for many decades, this level of optimization was scarce in computational linguis-\ntics. One reason is that most solutions developed in the past for linguistic problems rest on fairly simple\ndistributional assumptions, and few needs to go beyond conventional techniques for the inference. But as\nmany recent methods are beginning to embrace sophisticated structural assumptions such as context or syn-\ntax, the complexity of inference can be overwhelmingly large. One such troubling case is the nonparametric\nBayesian approach for unsupervised word segmentation. Nonparametric Bayesian modeling [14,19] is com-\nmonly deemed as state of the art for unsupervised word segmentation, but the computation overhead has\nlargely impeded its application on orthographic data, such as texts written in Chinese, Japanese, ancient\nlanguages, or even those with little linguistic resources.\nThis need for efﬁciently processing large text body has inspired some focused deployment of minimum\ndescription length (MDL) into the area [7,17,32]. Troubled by the lack of efﬁcient search procedure for MDL,\nthese methods use specialized algorithms to explore the hypothesis space. The search algorithm is usually\nlight on execution and takes one or more input parameters that serve as knobs for biasing search direction.\nIt is therefore easy to induce many “search models” at the same time by just changing the parameters, and\nthen combine these outputs altogether using information criteria such as MDL. This approach has proven\nto be efﬁcient and scales well to large benchmarks in orthographic word segmentation. But as these search\nalgorithms lean more towards using simple heuristics and hardly take structural assumptions into account,\nMDL-based methods are sometimes criticized for having inferior performance in the modeling of language.\nIn this paper, we seek to bridge this gap between more sophisticated probabilistic models and more\nefﬁcient MDL-based methods. We develop a fully unsupervised, simplistic approach called incremental\n∗National Taiwan University. email: rueycheng@ntu.edu.tw\n1\narXiv:1607.05822v2  [cs.CL]  23 Sep 2016\nlearning that combines the best of both worlds: We use a model selection framework for efﬁciently testing\nhypotheses, but for better language modeling we use a probabilistic search procedure that optimizes over\nthe penalized likelihood. The incremental learning approach is so named because the search procedure is\na greedy algorithm that checks only nearby conﬁgurations. Each search instance maintains an assumption\nabout the language to be modeled; in each iteration, the instance would incrementally update its hypothesis\nbased on the boundary placement decision it has made so far. Also it is known that using over-simplistic\ngenerative assumptions can easily lead to undersegmentation. To ﬁx this problem, we propose using a new\nlength-based penalty term as in Bayesian modeling [14]. This penalty term is developed to approximate\nthe cognitive burden (the number of meanings, in a sense) imposed by long word formation. Thus it is\nintroduced into our method as a super-additive function, which penalizes longer words harder than shorter\nones. We compared this incremental learning algorithm with many reference methods on both phonemic\nand orthographic word segmentation tasks. The result shows that the performance of incremental learning\nsurpassed existing MDL-based algorithms and compared well to the best records on both benchmarks.\nThe rest of the paper is organized as follows. Section 2 brieﬂy reviews the development of unsupervised\nword segmentation and some of the recent advances. In Section 3, we describe the proposed method, model\nselection criteria, and some other design features. Section 4 covers our test result on two standard benchmarks\nfor phonemic and orthographic word segmentation. The analysis is expanded later in Section 5 to look at the\ncorrelation between F-score and information criteria. We give out concluding remarks in Section 6.\n2\nUnsupervised Word Segmentation\nUnsupervised word segmentation has been a focus in natural language processing for certain East-Asian\nlanguages such as Japanese or Chinese. Some early traces of unsupervised word segmentation dates back\nto 1950s in linguistic studies. Harris [16] studied morpheme segmentation and suggested that low transition\nprobability between two phonemes may indicate a potential morpheme boundary. This idea has been a major\ninﬂuence on several approaches in orthographic Chinese word segmentation, such as accessor variety [12] and\nbranching entropy [18,28]. Besides the focus on word boundaries, some effort has been put into measurement\nof the quality of word-like strings. Sproat and Shih [26] used pointwise mutual information to measure the\nassociation strength of a string in Chinese texts. Kit and Wilks [20] proposed using the amount of information\ngained by modeling subword units as a single word to measure word quality. See Zhao and Kit [31] for a\ndetailed review.\nNonparametric Bayesian methods have been introduced to unsupervised word segmentation by Goldwa-\nter et al. [14] to address the problem of context modeling. They relied on a hierarchical Dirichlet process\n(HDP) to better capture between-word dependencies and avoid misidentifying common word collocations\nas whole words. This approach was later extended by Mochihashi et al. [23] that installs two Pitman-Yor\nprocesses at both word and character levels, aiming at a joint resolution of the dependency issues. Johnson\nand Goldwater [19] proposed an extension of HDP on probabilistic context-free grammar, called adaptor\ngrammars, whose performance is currently state of the art for unsupervised phonemic word segmentation.\nMinimum description length (MDL) is an inductive principle in information theory that equates salience\nto parsimony in statistical modeling [24,25]. It is perhaps best known for the connection between the famous\ntwo-part code and the Bayesian formalism. Early applications of MDL in computational linguistic target\nmainly on very general unsupervised learning problems, such as grammar induction [15] and language ac-\nquisition [4,5,9,10]. There has been some constant interest from problem subareas in applying MDL to their\nwork, but in the early days MDL served mainly as a motive for Bayesian modeling. Not much work has\nbeen done to explore the true merit of MDL as an information criterion until recently in lexical acquisition\nsubareas such as morphology [8,13] and unsupervised word segmentation [2,30].\nThis idea has later been extended in Zhikov et al. [32]. They varied the decision threshold on branch-\ning entropy to generate a number of candidate segmentations, each would later go through an adjustment\nprocedure for locally reducing the description length. All these candidates are then judged using the MDL\ncriterion to produce the ﬁnal output. This method works well on both phonemic and orthographic corpora,\nand has gather some attention from the research community. A number of follow-up work has devoted to\nexperimenting with different boundary placing strategies. Hewlett and Cohen [17] used Bootstrapped Voting\n2\nLet T be the input character sequence and θ be a language model.\n1. Compute ∆(s) for all n-gram s in sequence T, for all 2 ≤n ≤nmax:\n∆(s) = [L(T(s), θ(s)) −L(T, θ)] + [pen(T(s)) −pen(T)].\n(1)\n(T(s) and θ(s) are new sequence and model created by “compressing” s into a single token.)\n2. Let s∗be the minimizer of ∆(·). If ∆(s∗) < 0, stop and output T.\n3. Update T by substituting all occurrences of s∗in T with a new token s′. Update θ accordingly\nto reﬂect this change in token counts and the inclusion of s′. Add an deﬁnition s′ = s∗\n1s∗\n2 . . . s∗\nn\n(token-wise representation of the n-gram) into the lexicon.\n4. Go back to Step 1.\nFigure 1: The incremental learning framework.\nExperts, a sophisticated algorithm also based on branching entropy, to take the job of segmentation gener-\nation. Chen et al. [7] proposed using regularized compression, a compression-based algorithm that builds\nup word representation from characters in a bottom-up fashion; a later version of this algorithm in Chen [6]\nhad been shown to work well on phonemic data. Recently, some negative result with MDL on orthographic\nChinese segmentation was reported in Magistry and Sagot [22], suggesting that MDL has sometimes failed\nto improve their baseline measure.\n3\nIncremental Learning Using Maximum Penalized Likelihood\nThere has been some efforts aiming at formalizing the MDL-based approach for unsupervised word seg-\nmentation [7, 32]. In this section, we propose a general framework that incorporates many design elements\nfrom these previous attempts. We chose between two types of search strategies: in a space of boundary\nplacements [32] or a space spanned by lexemes in [7]. As our goal is to have something simple to work on,\neventually we settle on Chen et al.’s method for ease of implementation.\nFramework\nOur algorithm is given in Figure 1. Let us ﬁrst assume the input to algorithm is a sequence of\nN characters, denoted as XN. The basic idea of incremental learning is to iteratively process this sequence\nof “words” and make it more compact. Initially, this sequence contains only characters, t1t2 . . . tN = XN.\nThen, in each iteration our algorithm would pick a contiguous subsequence of words from the input, say\ns = titi+1 . . . ti+n−1 (for some i and n ≤nmax), and concatenate all its components into one single string.\nWe do this for all occurrences of s, thereby reducing the total number of words in the original word sequence.\nFor simplicity let us write the input sequence as T and the new sequence produced by compressing some\ncandidate s as T(s). In each iteration, we look for some candidate s∗that minimizes\nL(T(s), θ(s)) + pen(T(s)),\n(2)\nwhere L(T, θ) is the sum of the log likelihood estimate and the complexity term for θ, as in:\n−log p(T|θ) −1\n2(# unigram) log N.\n(3)\nNote that p(T|θ) is a categorical distribution over unigram words; we use maximum likelihood to estimate\nthis probability.\nSince our goal is to ﬁnd the candidate that minimizes the change plus the penalty, it sufﬁces in each\niteration to calculate only the change in the penalized maximum likelihood estimate. We have implemented\n3\nefﬁcient update steps similar to Chen [6] by deriving a bound for the likelihood component using the mean-\nvalue theorem. These steps are omitted here for space limitation.\nPenalized Likelihood\nOur objective function (2) in the incremental learning algorithm can be seen as a\njoint log-likelihood estimate p(XN, T, θ). The ﬁrst component p(T, θ) has been accounted for in L(T, θ);\nwhat was left is p(XN|T, θ), the probability of generating the input based on a sequence of words T and a\nlanguage model θ. This is not about testing whether T is a feasible segmentation for XN, but more about\nassessing how likely one may think the segmentation is reasonable. Inspired by the string-length-based\npriors used in Bayesian modeling [14], we assign this component as a penalty prior for demoting long word\nformation. This penalty term is by design a summation of super-additive function values of individual token\nlength (in characters). A super-additive function f satisﬁes that f(x + y) > f(x) + f(y) for all x and y.\nThis deﬁnition aims to approximate the cognitive overhead created by composition: putting more sub-word\nunits together would create new meanings. This is best illustrated by considering the number of possible\ncollocations and singletons in n word units, which is a quadratic function\n\u0000n\n2\n\u0001\n+\n\u0000n\n1\n\u0001\n, or more naively the\nnumber of possible combinations 2n. Both examples here are in super-additive forms.\nIn this paper, we consider the following two types of penalties, x log x and x2. We also ﬁnd it useful to\nhave an intercept term. The penalties are deﬁned as follows:\npen1(T) = −α|t| + β\nX\nt∈T\n|t| log |t|,\npen2(T) = −α|t| + β\nX\nt∈T\n|t|2.\n(4)\nModel Selection\nIn this paper, we use Akaike information criterion (AIC) and minimum description length\n(MDL) to measure the quality of a speciﬁc segmentation output. These criteria are applied to data-model pair\n(T, θ). For AIC, we use the ﬁnite correction proposed by Sugiura [27]:\nAICc = −log pˆθ(XN) +\nNk\nN −k −1,\nwhere pˆθ(XN) is the maximum likelihood estimate for observations XN and k is the complexity term (num-\nber of parameters needed for ﬁtting the model ˆθ.) For the deﬁnition of MDL, we follow Rissanen [25] and\nadd a complexity term for coding the word-level codebook, as in Zhikov et al. [32]. Our ﬁnal formula is\ndeﬁned as:\nMDL = −log pˆθ(XN) + k\n2 log N + cbl(θ),\nwhere cbl(θ) is the code length of the lexicon. Both criteria depends on a log-likelihood component, whose\nestimation is based on the designated language model θ. In this study, we consider unigram, bigram, and\ntrigram models.\n4\nExperiment\nOur incremental learning algorithm relies on two free parameters α and β to assign correct penalty weights\nto each segmentation candidate. The best combination may depend on language, representation, and even\ncorpus statistics. It is therefore infeasible to assume one magic setting that works well universally. In this\nexperiment, we show how the adaption to data can be achieved using model selection. Furthermore, we\njustify how super-additive penalty may improve incremental learning, and also verify the inﬂuence of higher-\norder dependencies to model selection. Standard performance measures such as precision (P), recall (R) and\nF-measures (F) are used for evaluating segmentation accuracy. We report these ﬁgures at three levels: token,\nboundary, and lexicon. The experimental setup and our methodology are detailed as follows.\n4\nα\nβ\nP\nR\nF\nBP\nBR\nBF\nLP\nLR\nLF\nBase\n0.0\n0.0\n37.9\n14.5\n21.0\n95.6\n12.2\n21.7\n9.8\n40.1\n15.8\nBase (stop in 500 runs)\n0.0\n0.0\n51.0\n54.3\n52.6\n68.7\n75.1\n71.8\n48.8\n18.8\n27.1\nAIC1\n229474.0\n1.6\n0.4\n38.5\n17.2\n23.8\n99.0\n21.5\n35.4\n11.8\n40.8\n18.3\nAIC2\n178471.4\n4.9\n1.3\n46.1\n27.0\n34.0\n98.9\n40.9\n57.9\n25.9\n63.8\n36.8\nAIC3\n150167.8\n1.9\n1.4\n82.6\n78.6\n80.5\n92.9\n86.5\n89.6\n62.4\n61.6\n62.0\nMDL1\n323797.8\n0.5\n0.7\n71.8\n57.9\n64.1\n94.7\n68.8\n79.7\n49.8\n64.3\n56.2\nMDL2\n290370.0\n3.7\n2.3\n83.2\n84.2\n83.7\n90.5\n92.1\n91.3\n59.2\n56.3\n57.7\nMDL3\n320373.9\n1.5\n1.5\n73.0\n79.8\n76.2\n82.7\n93.5\n87.8\n59.5\n46.7\n52.3\nAIC1\n237542.5\n3.7\n0.1\n27.8\n12.4\n17.2\n99.2\n21.5\n35.4\n8.9\n35.8\n14.3\nAIC2\n177499.9\n3.7\n0.2\n45.2\n27.0\n33.8\n98.5\n42.5\n59.3\n26.0\n65.6\n37.2\nAIC3\n150730.3\n0.6\n0.3\n81.6\n78.5\n80.0\n91.5\n86.6\n89.0\n61.2\n61.2\n61.2\nMDL1\n326346.4\n0.0\n0.2\n81.3\n74.4\n77.7\n94.3\n82.9\n88.3\n61.6\n63.1\n62.4\nMDL2\n290033.9\n1.7\n0.5\n79.7\n81.1\n80.4\n88.0\n90.1\n89.0\n58.3\n55.1\n56.6\nMDL3\n319573.5\n0.0\n0.3\n72.5\n80.1\n76.1\n82.1\n94.3\n87.8\n60.4\n46.6\n52.6\nTable 1: Test results on the Bernstein-Ratner corpus for different incremental learning settings: zero penalty\n(top), x log x penalty (middle) and x2 penalty (bottom).\n4.1\nPhonemic Word Segmentation\nWe used the Brent’s version of Bernstein-Ratner corpus to evaluate our method. [3, 5]. This corpus is the\nphonemic transcription of English child-directed speech from the CHILDES database [21], and has been\nwidely used as a standard testbed for unsupervised word segmentation. It has 9,790 utterances that comprise\ntotally 95,809 words in phonemic representation.\nOur result will be compared with the following methods: (1) incremental learning without adding the\npenalty, i.e., setting α = 0.0 and β = 0.0, (2) MDL-based methods such as regularized compression [6] and\nEntropyMDL [32], and (3) adaptor grammars [19].\nWe used two classes of information criteria in this experiment, derived from AIC and MDL. Both classes\ndepend on a likelihood term −log P(Xn), for which in this study we obtained three estimates under the\nunigram, bigram, and trigram assumptions. We denote these criteria as AICn and MDLn respectively for\nn = 1, 2, 3. The complexity term K (degrees of freedom) for AICn is given by:\nAIC1 =\nX\nw\n(1 + |w|) + # unigrams,\nAIC2 =\nX\nw\n(1 + |w|) + 1 + 2 × # bigrams,\nAIC3 =\nX\nw\n(1 + |w|) + 1 + 2 × # trigrams.\n(5)\nFor MDLn, we have the following formula:\nMDLn = # n-grams\n(6)\nWe ran a grid search for both α and β from 0 to 5 with step size 0.1, which amounts to 2,601 combinations\nin total. For each combination, we ﬁrst let the algorithm run to ﬁnish, collect the output, and then compute\nthe AIC and MDL values. This procedure is repeated on both penalty settings. Each AIC or MDL criteria\nwill have one combination that achieves the minimum. We collected these combinations and then chose the\none that minimize over the entire family as the designated output for AICn or MDLn.\nResult\nThe test result is summarized in Table 1. From top to bottom we have the results using different\npenalties: zero penalty, x log x, and x2. Zero-penalty and its upper bound (the best possible result among\nintermediate output) are included here merely as lab controls for the super-additive penalties. Zero penalty\n5\nP\nR\nF\nBP\nBR\nBF\nLP\nLR\nLF\nAIC3\n82.9\n79.4\n81.1\n92.7\n87.2\n89.9\n62.7\n63.1\n62.9\nMDL2\n83.7\n84.2\n84.0\n91.0\n91.8\n91.4\n59.9\n58.0\n58.9\nAIC3\n82.5\n81.0\n81.7\n91.2\n88.9\n90.0\n59.9\n61.2\n60.5\nMDL2\n80.7\n80.6\n80.6\n89.5\n89.3\n89.4\n59.3\n57.8\n58.6\nTable 2: Performance results for top-10 ensemble using x log x (top) and x2 (bottom) penalties.\nP\nR\nF\nMDL2, x log x + ensemble\n83.7\n84.2\n84.0\nAIC3, x2 + ensemble\n82.5\n81.0\n81.7\nChen [6]\n79.1\n81.7\n80.4\nHewlett and Cohen [17]\n79.3\n73.4\n76.2\nZhikov et al. [32]\n76.3\n74.5\n75.4\nF\nAdaptor grammars, colloc3-syllable\n87.0\nMDL2, x log x + ensemble\n84.0\nAIC3, x2 + ensemble\n81.7\nAdaptor grammars, colloc\n76.0\nAdaptor grammars, unigram\n56.0\nTable 3: Performance comparison with MDL-based methods (left) and adaptor grammars (right). Figures for\nthe latter were reproduced from the reference implementation [19] using batch initialization and maximum\nmarginal decoding. Note that colloc3-syllable adaptor grammars is not fully unsupervised due to a small\namount of phoneme productions built into its core.\nitself did not do very well, achieving only 21.6 for token F-score (hereafter abbreviated as F-score or F-\nmeasure); its best intermediate output was found in 500 iterations, giving a mediocre 52.6 in F-measure.\nWith x log x penalty, AICn has achieved 23.8, 34.0, and 80.5 in F-score, respectively for n = 1, 2, 3.\nMDLn does slightly better in general, giving 64.1, 83.7, and 76.2 in F-score. The best performance for AIC\nand MDL is on AIC3 and MDL2. These two runs are still clear winners on x2 penalty. In F-measure, AIC3\nand MDL2 does equally well, delivered 80.0 and 80.4 respectively in terms of F-measure. The performance\nfor both criteria on x2 is very close to AIC3 on x log x.\nIn general, the trigram estimate AIC3 works the best for AIC, constantly achieving the least decision\nvalue among the three and the best performance. The same goes for the bigram estimate MDL2 in the MDL\ncamp, whose performance surpassed the other three consistently across two penalty settings. Furthermore,\nthe unigram and bigram estimates do not seem to play well with AIC. On either penalty setting, both AIC1\nand AIC2 have struggled to catch up with the zero-penalty upper-bound performance. The result on MDL\nruns seem more coherent in performance. MDL1 and MDL3 lagged behind MDL2 only by a small margin\nabout 3 to 4 points in F-score. From the test result, it seems fair to conclude that the proposed incremental\nlearning framework is effective in unsupervised word segmentation.\nWe also experimented with a simple ensemble method that combines top-k segmentation outputs. Sup-\npose there are totally n plausible positions for placing boundaries on the input corpus. For each position, we\ncheck how many in the k outputs have actually placed a boundary there and seek to obtain a majority decision.\nThese n decisions are later put together as a combined output; ties are interpreted as “no-boundary-here”. We\ntested top-10 ensemble for AIC3 and MDL2 on both penalty settings. The result is given in Table 2. The\ntoken-level performance for all four experimental runs was improved by 0.2 to 1.7 points in F-score. Top-10\nensemble also slightly improves lexicon F-measure on x log x penalties, although in general it has a mixed\neffect on the other measures. The overall best performance is now on MDL2 using x log x penalty, achieving\n84.0 in token F-score.\nIn Table 3, we make an overall comparison between our result and reference methods, including MDL-\nbased methods and adaptor grammars. The summary on the left shows that our incremental learning frame-\nwork has outperformed all the existing MDL-based approaches, winning out regularized compression by\nabout 4 points in F-score. On the right, we ﬁnd the performance of incremental learning has surpassed both\nunigram and colloc adaptor grammars by a large margin. We also compared our approach with colloc3-\nsyllable adaptor grammars, which is commonly thought as weakly supervised. The result shows that incre-\nmental learning approach lagged behind colloc3-syllable by 3 points in F-score. Our interpretation is that\nthis more advanced version of adaptor grammars has built in some linguistic/structural assumptions that have\n6\nTime (s)\nMDL2, x log x + ensemble, 51 × 51\n9,350\nAdaptor grammars, colloc\n73,356\nAdaptor grammars, colloc3-syllable\n376,732\nTable 4: Timing results on the Bernstein-Ratner corpus, all methods tested on an Intel Xeon 2.5GHz 8-core\nmachine with 8GB RAM. All the tests were done on a single core; no parallelization was intended in this\ntiming benchmark.\nAS\nMSR\nCityU\nPKU\nMDL2, x log x\n80.9\n80.0\n80.5\n80.6\nWang et al. [29], Setting 3\n76.9\n79.7\n80.8\n79.3\nZhikov et al. [32]\n–\n79.5\n79.8\n–\nChen et al. [7]\n–\n77.4\n77.0\n–\nZhao and Kit [31]\n–\n66.5\n68.4\n–\nTable 5: Test results in token F-measure on the SIGHAN Bakeoff-2005 training sets. The baseline results all\ncome from the literature.\nno equivalent in our simplistic framework. In the case of colloc3-syllable, it was a small set of phonemic\nproductions that helps improving syllable-level modeling accuracy.\nDespite the inability of modeling sophisticated generative nature in language, incremental learning has\ngood processing speed that permits more CPU cycles to go into parameter estimation. On our test machine, it\ntook merely 3 to 4 seconds for testing one combination of α and β. Running 7-fold colloc3-syllable adaptor\ngrammars would take roughly 4 days; our approach would ﬁnish in 3 hours even on a 51 × 51 grid search.\nDetailed results are given in Table 4.\n4.2\nOrthographic Word Segmentation\nWe also tested our approach under the setting for orthographic word segmentation on a public corpus SIGHAN\nBakeoff-2005 [11]. This corpus is a standard benchmark for Chinese word segmentation; it has 4 subsets,\nand each comes with a ofﬁcial training/test split. For simplicity, our experiment was conducted exclusively\non these training sets. The details are given below.\n# passages\n# words\nAS\n708,953\n5.45M\nMSR\n86,924\n2.37M\nCityU\n53,019\n1.46M\nPKU\n19,056\n1.10M\nIt is easy to see that each of these sets is much more sizable than our previous experiment. As of this\nwriting, it remains difﬁcult if not impossible to apply hierarchical Bayesian methods to a corpus of this\nsize. Hence, in this experiment, we chose to compare with less sophisticated approaches such as MDL-\nbased methods, DLG [31], and ESA [29]. The latter is known to have the best segmentation accuracy on\nthe Bakeoff-2005 corpus, although the best ﬁgures reported was achieved by explicitly hand-picking the\nparameter. To make this a fair comparison, we instead took the median from all the reported runs for 10-\niteration ESA. Note that ESA still has the best performance after this ﬁx. Our experiment setup is compatible\nwith the setting 3 for ESA, meaning that hard boundaries are set up around punctuation marks. Besides that,\nlittle preprocessing was done to the corpus body.\nDue to the scale of experiment, we chose to test only the best setting found in the previous round: MDL2\nwith x log x penalty. Even so, the experiment remains very time-consuming; on the largest set AS, it took\nseveral hours for our incremental learning algorithm to complete. Therefore, for estimating the parameters\nwe ﬁrst ran a search on α and then a second on β in a way analogous to the previous experiment. We ran this\n7\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\na\nb\n20\n40\n60\n80\nf\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\na\nb\n12.0 12.1 12.2 12.3 12.4 12.5\nlog(aic3)\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\na\nb\n12.6\n12.7\n12.8\nlog(mdl2)\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\na\nb\n20\n40\n60\n80\nf\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\na\nb\n12.0 12.1 12.2 12.3 12.4 12.5\nlog(aic3)\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\na\nb\n12.6\n12.7\n12.8\nlog(mdl2)\nFigure 2: Heat maps printed in greyscale for token F-score (left), AIC3 (middle), and MDL2 (right) in a\ntruncated hypothesis space represented by α and β, based on x log x (top) and x2 (bottom) penalties. Note\nthat AIC3 and MDL2 are log-transformed for better contrast.\nPenalty\nAIC1\nAIC2\nAIC3\nMDL1\nMDL2\nMDL3\nOutput\nx log x\n-0.80\n-0.79\n-0.76\n-0.93\n-0.78\n-0.46\nx2\n-0.94\n-0.89\n-0.87\n-0.90\n-0.87\n-0.54\nFull trace\nx log x\n0.44\n-0.47\n-0.79\n-0.79\n-0.96\n-0.92\nx2\n0.03\n-0.53\n-0.78\n-0.73\n-0.93\n-0.86\nTable 6: Rank correlations (Spearman’s ρ) between token F-measure and information criteria\ngrid search only once on the AS set; the found combination (α, β) = (2.0, 3.0) was used throughout the rest\nof the experiment on other subsets.\nResult\nWe had our method run to stop on each of the subsets. The improvement on F-score over iterations is\nsteady and consistent; this trend is made obvious in Figure 3. The found parameters seemed to generalize well\nacross subsets, in spite of the difference on orthographic representation and usage1. The overall performance\nis given in Table 5. Our incremental learning method achieved in F-score above 80.0 on all four subsets,\nand it compares favorably with weaker baselines such as MDL-based methods and DLG. On two sets AS\nand PKU, incremental learning outperformed the strong baseline ESA, winning out by 4.0 and 1.3 in F-\nscore respectively. On the other two sets MSR and CityU, incremental learning seems on a par with ESA: it\nimproved over the baseline by 0.3 on MSR, but it also lagged behind by 0.3 on CityU.\nTo conclude, our result suggests that incremental learning is effective and efﬁcient for unsupervised word\nsegmentation. The main reason is that penalized likelihood has better coverage in the hypothesis space;\nthrough varying the parameters α and β, we are able to explore many more search paths than an ordinary\ngreedy method can. Moreover, the information criteria such as AIC and MDL allow us to make a guess fairly\nefﬁciently on the solution quality. It is the combination of the two that makes incremental learning a plausible\nalternative for this speciﬁc application.\n1AS and CityU are in traditional Chinese; MSR and PKU in simpliﬁed Chinese.\n8\n55\n60\n65\n70\n75\n80\n0\n10000\n20000\n30000\n40000\n50000\nITER\nF\nSubset\nAS\nMSR\nCityU\nPKU\nFigure 3: The improvement on F-score across iterations on the Bakeoff-2005 corpus.\n5\nPost-Hoc Analysis\nWe have initial evidence to support that information criteria can suggest good ways to segment words. It nev-\nertheless raises a question on the reliability of these meta-heuristics. By reliability, we mean how consistent\nthe prediction made matches the truth. This has little to do with the exact decision values; in our application,\nwe care only about whether an information criterion assigns sensible ranking to the given hypotheses. In this\nsection, we discuss two ways to examine this relation.\nSpectral Correlation\nThe ﬁrst one is to look for spectral patterns for the response values in the hypothesis\nspace; a simple visualization as in Figure 2 would do the trick. We plot the F-score, AIC3, and MDL2 values\nas individual heat maps on the parameter search plane. It is interesting to note that, on both penalties, the\nbest segmentation performance concentrates on a cone-like region with its peak facing the y-axis. For any\nα, performance declines as β moves away from this region either towards zero or inﬁnity. As α increases,\nthe spread gets larger and results in more gentle decline, so it is not hard to imagine the true performance\ncontour as a mountain that has a fat end at large α. We found that in general AIC3 and MDL2 assign similar\nrankings to hypothesis, although the AIC3 ranking seems more ﬁne-grained. Both criteria have missed on the\ntrue optimal solutions, but MDL2 does a bit better in modeling by using two “valleys” to cap the true region.\nRank Correlation\nThe second angle is to check on the rank correlation. We computed Spearman’s ρ be-\ntween token F-measure and all the information criteria used in the experiment. For comparison, we also used\nthe full trace, which is the intermediate outputs collected every 100 iteration for all the combinations. The\nresult is given in Table 6. We notice that on the output set both AIC and MDL give strong, negative corre-\nlation, i.e., ρ < −0.7, with the true performance, with one exception MDL3 that shows medium correlation.\nThis however does not match our empirical result. Note that the output set alone is not sufﬁcient to draw\nconclusion on the predictability over the entire hypothesis space; the criteria may still assign good ranking to\na bad hypothesis not covered in the output set.\nThe rank correlation on the full trace has shown a better ﬁt to the true result. Only 4 criteria, including\nAIC3, MDL1, MDL2, and MDL3, are strongly correlated with the F-score. It was suggested that MDL\ncorrelates well with data under generative assumptions that vary in complexity. The best correlation is found\non MDL2, which sets record by achieving -0.96 on x log x penalty. This has once again been evidence in\nfavor of our empirical ﬁndings.\nThe analysis can be further enhanced with some visual cues as given in Figure 4, which plots F-score\nagainst ranking for all six decision criteria using the full trace data. It suggests that AIC3, MDL2, and\n9\nFigure 4: F-score plotted against various information criteria on the full trace.\nMDL3 are all able to ﬁnd good segmentation hypotheses reliably, although AIC3 has some serious problem\nin pushing solutions towards mid-ranked regions.\n6\nConclusions\nIn this paper, we introduce an incremental learning algorithm for unsupervised word segmentation that com-\nbines probabilistic modeling and model selection in one framework. We show with extensive experiments\nthat simple ideas, such as the super-additive penalties and higher-order generative assumptions in model se-\nlection, can achieve very competitive performance in both phonemic and orthographic word segmentation.\nOur algorithm is very efﬁcient and scales well on large corpora, making it more useful than other algorithms\nin real-world applications. Besides all that, this framework is general enough so that it is easy to replace\nthe objective function or model selection method with more sophisticated ones. For our future work, we\nwill focus on enhancing accuracy by incorporating more complex structural assumptions such as syntax and\ncontext.\nReferences\n[1] Hirotugu Akaike. A new look at the statistical model identiﬁcation. Automatic Control, IEEE Transac-\ntions on, 19(6):716–723, December 1974.\n[2] Shlomo Argamon, Navot Akiva, Amihood Amir, and Oren Kapah. Efﬁcient unsupervised recursive\nword segmentation using minimum description length. In Proceedings of the 20th international con-\nference on Computational Linguistics, COLING ’04, Stroudsburg, PA, USA, 2004. Association for\nComputational Linguistics.\n[3] Nan Bernstein-Ratner. The phonology of parent child speech. Children’s language, 6:159–174, 1987.\n10\n[4] Michael R. Brent. An efﬁcient, probabilistically sound algorithm for segmentation and word discovery.\nMaching Learning, 34(1-3):71–105, February 1999.\n[5] Michael R. Brent and Timothy A. Cartwright. Distributional regularity and phonotactic constraints are\nuseful for segmentation. In Cognition, pages 93–125, 1996.\n[6] Ruey-Cheng Chen. An improved MDL-based compression algorithm for unsupervised word segmen-\ntation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 166–170, Soﬁa, Bulgaria, August 2013. Association for Computa-\ntional Linguistics.\n[7] Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang. A regularized compression method to unsuper-\nvised word segmentation. In Proceedings of the Twelfth Meeting of the Special Interest Group on Com-\nputational Morphology and Phonology, SIGMORPHON ’12, pages 26–34, Montreal, Canada, 2012.\nAssociation for Computational Linguistics.\n[8] Mathias Creutz and Krista Lagus. Unsupervised discovery of morphemes. In Proceedings of the ACL-02\nworkshop on Morphological and phonological learning - Volume 6, MPL ’02, pages 21–30, Strouds-\nburg, PA, USA, 2002. Association for Computational Linguistics.\n[9] Carl de Marcken. Linguistic structure as composition and perturbation. In Proceedings of the 34th\nAnnual Meeting on Association for Computational Linguistics, ACL ’96, pages 335–341, Stroudsburg,\nPA, USA, 1996. Association for Computational Linguistics.\n[10] Carl de Marcken. Unsupervised language acquisition. PhD thesis, Massachusetts Institute of Technol-\nogy, 1996.\n[11] Thomas Emerson. The second international chinese word segmentation bakeoff. In Proceedings of the\nFourth SIGHAN Workshop on Chinese Language Processing, volume 133. Jeju Island, Korea, 2005.\n[12] Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin Zheng. Accessor variety criteria for chinese word\nextraction. Comput. Linguist., 30:75–93, March 2004.\n[13] John Goldsmith.\nUnsupervised learning of the morphology of a natural language.\nComputational\nLinguistics, 27(2):153–198, June 2001.\n[14] Sharon Goldwater, Thomas L. Grifﬁths, and Mark Johnson. A bayesian framework for word segmenta-\ntion: Exploring the effects of context. Cognition, 112(1):21–54, July 2009.\n[15] Peter Gr¨unwald. A minimum description length approach to grammar inference. In Connectionist,\nStatistical, and Symbolic Approaches to Learning for Natural Language Processing, pages 203–216,\nLondon, UK, UK, 1996. Springer-Verlag.\n[16] Zellig S. Harris. From phoneme to morpheme. Language, 31(2):190–222, 1955.\n[17] Daniel Hewlett and Paul Cohen.\nFully unsupervised word segmentation with BVE and MDL.\nIn\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human\nLanguage Technologies: short papers - Volume 2, HLT ’11, pages 540–545, Portland, Oregon, 2011.\nAssociation for Computational Linguistics.\n[18] Zhihui Jin and Kumiko Tanaka-Ishii. Unsupervised segmentation of chinese text by use of branching\nentropy. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06,\npages 428–435, Sydney, Australia, 2006. Association for Computational Linguistics.\n[19] Mark Johnson and Sharon Goldwater. Improving nonparameteric bayesian inference: experiments on\nunsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Tech-\nnologies: The 2009 Annual Conference of the North American Chapter of the Association for Compu-\ntational Linguistics, NAACL ’09, pages 317–325, Boulder, Colorado, 2009. Association for Computa-\ntional Linguistics.\n11\n[20] Chunyu Kit and Yorick Wilks. Unsupervised learning of word boundary with description length gain.\nIn CoNLL-99, pages 1–6, Bergen, Norway, 1999.\n[21] Brian MacWhinney and Catherine Snow. The child language data exchange system: an update. Journal\nof child language, 17(2):457–472, June 1990.\n[22] Pierre Magistry and Benoˆıt Sagot. Can MDL improve unsupervised chinese word segmentation? In\nProceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages 2–10, Nagoya,\nJapan, October 2013. Asian Federation of Natural Language Processing.\n[23] Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. Bayesian unsupervised word segmentation\nwith nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual\nMeeting of the ACL and the 4th International Joint Conference on Natural Language Processing of\nthe AFNLP: Volume 1 - Volume 1, ACL ’09, pages 100–108, Suntec, Singapore, 2009. Association for\nComputational Linguistics.\n[24] Jorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, September 1978.\n[25] Jorma Rissanen. A universal prior for integers and estimation by minimum description length. The\nAnnals of Statistics, 11(2):416–431, 1983.\n[26] Richard W. Sproat and Chilin Shih. A statistical method for ﬁnding word boundaries in Chinese text.\nComputer Processing of Chinese and Oriental Languages, 4(4):336–351, 1990.\n[27] Nariaki Sugiura. Further analysis of the data by akaike’s information criterion and the ﬁnite corrections.\nCommunications in Statistics - Theory and Methods, 7(1):13–26, January 1978.\n[28] Kumiko Tanaka-Ishii. Entropy as an indicator of context boundaries: An experiment using a web search\nengine. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Kwong, editors, Natural Language Processing\nIJCNLP 2005, volume 3651 of Lecture Notes in Computer Science, chapter 9, pages 93–105. Springer\nBerlin / Heidelberg, Berlin, Heidelberg, 2005.\n[29] Hanshi Wang, Jian Zhu, Shiping Tang, and Xiaozhong Fan. A new unsupervised approach to word\nsegmentation. Computational Linguistics, 37(3):421–454, March 2011.\n[30] Hua Yu. Unsupervised word induction using MDL criterion. In Proceedings of the International Sym-\nposium of Chinese Spoken Language Processing, Beijin, China, 2000.\n[31] Hai Zhao and Chunyu Kit. An empirical comparison of goodness measures for unsupervised chinese\nword segmentation with a uniﬁed framework. In The Third International Joint Conference on Natural\nLanguage Processing (IJCNLP 2008), pages 9–16, 2008.\n[32] Valentin Zhikov, Hiroya Takamura, and Manabu Okumura. An efﬁcient algorithm for unsupervised\nword segmentation with branching entropy and MDL.\nIn Proceedings of the 2010 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP ’10, pages 832–842, Cambridge, Mas-\nsachusetts, 2010. Association for Computational Linguistics.\n12\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2016-07-20",
  "updated": "2016-09-23"
}