{
  "id": "http://arxiv.org/abs/1607.05966v1",
  "title": "Onsager-corrected deep learning for sparse linear inverse problems",
  "authors": [
    "Mark Borgerding",
    "Philip Schniter"
  ],
  "abstract": "Deep learning has gained great popularity due to its widespread success on\nmany inference problems. We consider the application of deep learning to the\nsparse linear inverse problem encountered in compressive sensing, where one\nseeks to recover a sparse signal from a small number of noisy linear\nmeasurements. In this paper, we propose a novel neural-network architecture\nthat decouples prediction errors across layers in the same way that the\napproximate message passing (AMP) algorithm decouples them across iterations:\nthrough Onsager correction. Numerical experiments suggest that our \"learned\nAMP\" network significantly improves upon Gregor and LeCun's \"learned ISTA\"\nnetwork in both accuracy and complexity.",
  "text": "arXiv:1607.05966v1  [cs.IT]  20 Jul 2016\nONSAGER-CORRECTED DEEP LEARNING FOR SPARSE LINEAR INVERSE PROBLEMS\nMark Borgerding and Philip Schniter\nDept. of ECE, The Ohio State University, Columbus, OH 43202\nEmail: borgerding.7@osu.edu, schniter.1@osu.edu\nABSTRACT\nDeep learning has gained great popularity due to its widespread suc-\ncess on many inference problems. We consider the application of\ndeep learning to the sparse linear inverse problem encountered in\ncompressive sensing, where one seeks to recover a sparse signal\nfrom a small number of noisy linear measurements. In this paper,\nwe propose a novel neural-network architecture that decouples pre-\ndiction errors across layers in the same way that the approximate\nmessage passing (AMP) algorithm decouples them across iterations:\nthrough Onsager correction. Numerical experiments suggest that our\n“learned AMP” network signiﬁcantly improves upon Gregor and Le-\nCun’s “learned ISTA” network in both accuracy and complexity.1\nIndex Terms— Deep learning, compressive sensing, sparse\ncoding, approximate message passing.\n1. INTRODUCTION\nWe consider the problem of recovering a signal s ∈CN from a noisy\nlinear measurement\ny = Φs + n ∈CM,\n(1)\nwhere Φ ∈CM×N represents a linear measurement operator and\nn ∈CM a noise vector. In many cases of interest, M ≪N. We\nwill assume that the signal vector s has a sparse representation in a\nknown orthonormal basis Ψ ∈CN×N, i.e., that s = Ψx for some\nsparse vector x ∈CN. Thus we deﬁne A ≜ΦΨ ∈CM×N, write\n(1) as\ny = Ax + n,\n(2)\nand seek to recover an sparse x from y. In the sequel, we will refer\nto this problem as the “sparse linear inverse” problem. Note that the\nresulting estimate bx of x can be easily converted into an estimate bs\nof s via bs = Ψbx.\nThe sparse linear inverse problem has received enormous atten-\ntion over the last few years, in part because it is central to compres-\nsive sensing [1] and sparse coding [2]. Many methods have been de-\nveloped to solve this problem. Most of the existing methods involve\na reconstruction algorithm that inputs a pair (y, A) and produces an\nsparse estimate bx. A myriad of such algorithms have been proposed,\nincluding both sequential (e.g., greedy) and iterative varieties. Some\nrelevant algorithms will be reviewed in Section Section 2.1.\nRecently, a different approach to solving this problem has\nemerged from the ﬁeld of “deep learning,” whereby a neural\nnetwork with many layers is trained using a set of D examples2\n1This work was supported by the NSF I/UCRC grant IIP-1539960.\n2Since orthonormal Ψ implies x = ΨHs, training examples of the\nform {(y(d), s(d))} can be converted to {(y(d), x(d))}D\nd=1 via x(d) =\nΨHs(d).\n{(y(d), x(d))}D\nd=1. Once trained, the network can be used to pre-\ndict the sparse x that corresponds to a given input y. Note that\nknowledge of the operator A is not necessary here. Previous work\n(e.g., [3–6]) has shown that the deep-learning approach to solving\nsparse linear inverse problems has the potential to offer signiﬁcant\nimprovements, in both accuracy and complexity, over the traditional\nalgorithmic approach.\nRelation to prior work: In this paper, we show how recent ad-\nvances in iterative reconstruction algorithms suggest modiﬁcations\nto traditional neural-network architectures that yield improved accu-\nracy and complexity when solving sparse linear inverse problems.\nIn particular, we show how “Onsager correction,” which lies at the\nheart of the approximate message passing (AMP) algorithm [7], can\nbe employed to construct deep networks with increased accuracy\nand computational efﬁciency (i.e., fewer layers needed to produce\nan accurate estimate). To our knowledge, the application of Onsager\ncorrection to deep neural networks is novel.\n2. ITERATIVE ALGORITHMS AND DEEP LEARNING\n2.1. Iterative Algorithms\nOne of the best known algorithmic approaches to solving the sparse\nlinear inverse problem is through solving the convex optimization\nproblem [8,9]\nbx = arg min\nx\n1\n2∥y −Ax∥2\n2 + λ∥x∥1,\n(3)\nwhere λ > 0 is a tunable parameter that controls the tradeoff be-\ntween sparsity and measurement ﬁdelity in bx. The convexity of (3)\nleads to provably convergent algorithms and bounds on the perfor-\nmance of the estimate bx (see, e.g., [10]).\n2.1.1. ISTA\nOne of the simplest approaches to solving (3) is the iterative soft-\nthresholding algorithm (ISTA) [11], which consists of iterating the\nsteps (for t = 0, 1, 2, . . . and bx0 = 0)\nvt = y −Abxt\n(4a)\nbxt+1 = η\n\u0000bxt + βAHvt; λ\n\u0001\n,\n(4b)\nwhere β ∈(0, 1/∥A∥2\n2] is a stepsize, vt is the iteration-t residual\nmeasurement error, and η(·; λ) : CN →CN is the “soft threshold-\ning” denoiser that operates componentwise as:\n[η(r; λ)]j = sgn(rj) max{|rj| −λ, 0}.\n(5)\n2.1.2. FISTA\nAlthough ISTA is guaranteed to converge under β ∈(0, 1/∥A∥2\n2)\n[12], it converges somewhat slowly and so many modiﬁcations have\nbeen proposed to speed it up. Among the most famous is “fast ISTA”\n(FISTA) [13],\nvt = y −Abxt\n(6a)\nbxt+1 = η\n\u0000bxt + βAHvt + t−2\nt+1 (bxt −bxt−1) ; λ\n\u0001\n,\n(6b)\nwhich converges in roughly an order-of-magnitude fewer iterations\nthan ISTA (see Fig. 1).\n2.1.3. AMP\nRecently, the approximate message passing (AMP) algorithm [7,14]\nwas applied to (3), giving\nvt = y −Abxt + btvt−1\n(7a)\nbxt+1 = η\n\u0000bxt + AHvt; λt\n\u0001\n,\n(7b)\nfor bx0 = 0, v−1 = 0, t = 0, 1, 2, . . . , and\nbt =\n1\nM ∥bxt∥0\n(8)\nλt =\nα\n√\nM ∥vt∥2.\n(9)\nHere, α is a tuning parameter that has a one-to-one correspondence\nwith λ in (3) [14]. Comparing AMP to ISTA, we see two major\ndifferences: i) AMP’s residual vt in (7a) includes the “Onsager cor-\nrection” term btvt−1, and ii) AMP’s denoising threshold λt in (7b)\ntakes the prescribed, t-dependent value (9). We now describe the\nrationale behind these differences.\nWhen A is a typical realization of a large i.i.d. (sub)Gaussian\nrandom matrix with entries of variance M −1, the Onsager correc-\ntion decouples the AMP iterations in the sense that the input to the\ndenoiser, rt ≜bxt + AHvt, can be modeled as3\nrt = x + N(0, σ2\nt IN) with σ2\nt =\n1\nM ∥vt∥2\n2.\n(10)\nIn other words, the Onsager correction ensures that the denoiser in-\nput is an additive white Gaussian noise (AWGN) corrupted version\nof the true signal x with known AWGN variance σ2\nt . (See Fig. 5.)\nThe resulting problem, known as “denoising,” is well understood.\nFor example, for an independent known prior p(x) = QN\nj=1 pj(xj),\nthe mean-squared error (MSE)-optimal denoiser4 is simply the pos-\nterior mean estimator (i.e., bxt+1,j = E{xj|rt,j; σt}), which can\nbe computed in closed form for many pj(·). In the more realis-\ntic case that pj(·) are unknown, we may be more interested in the\nminimax denoiser, i.e., minimizer of the maximum MSE over an as-\nsumed family of priors. Remarkably, for sparse priors, i.e., pj(xj) =\n(1−γ)δ(xj)+γepj(xj) with γ ∈(0, 1) and arbitrary unknown epj(·),\nsoft-thresholding (5) with a threshold proportional to the AWGN\nstandard deviation (i.e., λt = ασt as in (9)) is nearly minimax opti-\nmal [14]. Thus, we can interpret the AMP algorithm (7) as a nearly\nminimax approach to the sparse linear inverse problem.\n3The AMP model (10) is provably accurate in the large-system limit (i.e.,\nM, N →∞with M/N converging to a ﬁxed positive constant) [15].\n4AMP with MSE-optimal denoising was ﬁrst described in [16].\n100\n101\n102\n103\n104\n-40\n-35\n-30\n-25\n-20\n-15\n-10\n-5\n0\nISTA\nFISTA\nAMP\naverage NMSE [dB]\niterations\nFig. 1. Average NMSE versus iteration number for AMP, FISTA,\nISTA (from left to right).\n2.1.4. Comparison of ISTA, FISTA, and AMP\nWe now compare the average per-iteration behavior of ISTA, FISTA,\nand AMP for an A drawn i.i.d. N(0, M −1). In our experiment, all\nquantities were real-valued, the problem dimensions were N = 500\nand M = 250, the elements of x were drawn i.i.d. N(0, 1) with\nprobability γ = 0.1 and were otherwise set to zero, and the noise\nn was drawn i.i.d. N(0, v) with v set to yield a signal-to-noise ratio\n(SNR) of 40 dB. Recall that ISTA, FISTA, and AMP all estimate\nx by iteratively minimizing (3) for a chosen value of λ (selected\nvia α in the case of AMP). We chose the minimax optimal value\nof α for AMP (which is 1.1402 since γ = 0.1 [14]) and used the\ncorresponding λ for ISTA and FISTA. Figure 1 shows the average\nnormalized MSE (NMSE) versus iteration t, where NMSE ≜∥bxt −\nx∥2\n2/∥x∥2\n2 and 1000 realizations of (x, n) were averaged. We see\nthat AMP requires roughly an order-of-magnitude fewer iterations\nthan FISTA, which requires roughly an order-of-magnitude fewer\niterations than ISTA.\n2.2. Deep Learning\nIn deep learning [17], training data {(y(d), x(d))}D\nd=1 composed of\n(feature,label) pairs are used to train the parameters of a deep neural\nnetwork with the goal of accurately predicting the label x of a test\nfeature y. The deep network accepts y and subjects it to many layers\nof processing, where each layer consists of a linear transformation\nfollowed by a non-linearity.\nTypically, the label space is discrete (e.g., y is an image and x\nis its class in {cat, dog,. . . , tree}). In our sparse linear inverse prob-\nlem, however, the “labels” x are continuous and high-dimensional\n(e.g., CN or RN). Remarkably, Gregor and LeCun demonstrated\nin [3] that a well-constructed deep network can accurately predict\neven labels such as ours.\nThe neural network architecture proposed in [3] is closely re-\nlated to the ISTA algorithm discussed in Section 2.1.1. To under-\nstand the relation, we rewrite the ISTA iteration (4) as\nbxt+1 = η\n\u0000Sbxt + By; λ\n\u0001\nwith\n(\nB ≜βAH\nS ≜IN −BA\n(11)\nand “unfold” iterations t = 1...T , resulting in the T -layer feed-\nforward neural network shown in Fig. 2. Whereas ISTA uses the\n+\n+\n+\ny\nB\nS\nS\nS\nbx1\nbx2\nbx3\nbx4\nη\nη\nη\nη\nFig. 2. The feed-forward neural network constructed by unfolding\nT =4 iterations of ISTA.\nvalues of S and B prescribed in (11) and a common value of λ at\nall layers, Gregor and LeCun [3] proposed to use layer-dependent\nthresholds λ ≜[λ1, λ2, . . . , λT ] and “learn” both the thresholds λ\nand the matrices B, S from the training data {(y(d), x(d))}D\nd=1 by\nminimizing the quadratic loss\nLT (Θ) = 1\nD\nD\nX\nd=1\n\r\rbxT (y(d); Θ) −x(d)\r\r2\n2.\n(12)\nHere, Θ = [B, S, λ] denotes the set of learnable parameters and\nbxT (y(d); Θ) the output of the T -layer network with input y(d) and\nparameters Θ. The resulting approach was coined “learned ISTA”\n(LISTA).\nRelative to existing algorithms for the sparse linear inverse prob-\nlem with optimally tuned regularization parameters (e.g., λ or α),\nLISTA generates estimates of comparable MSE with signiﬁcantly\nfewer matrix-vector multiplications. As an example, for the problem\ndescribed in Section 2.1.4, LISTA took only 16 layers to reach an\nNMSE of −35 dB, whereas AMP took 25 iterations. (More details\nwill be given in Section 4.)\nOther authors have also applied ideas from deep learning to the\nsparse linear inverse problem. For example, [4] extended the ap-\nproach from [3] to handle structured sparsity and dictionary learning\n(when the training data are {y(d)}D\nd=1 and A is unknown). More\nrecently, [6] extended [3] from the ℓ2 + ℓ1 objective of (3) to the\nℓ2+ℓ0 objective, and [5] proposed to learn the MSE-optimal scalar\ndenoising function η by learning the parameters of a B-spline. The\nidea of “unfolding” an iterative algorithm and learning its parameters\nvia training data has also been used to recover non-sparse signals.\nFor example, it has been applied to speech enhancement [18], im-\nage deblurring [19], image super resolution [20], compressive imag-\ning [21,22], and video compressive sensing [23].\n3. LEARNED AMP\nRecall that LISTA involves learning the matrix S ≜IN −BA,\nwhere B ∈CN×M and A ∈CM×N. As noted in [3], when M <\nN/2, it is advantageous to leverage the IN −BA structure of S,\nleading to network layers of the form shown in Fig. 3, with ﬁrst-\nlayer inputs bx0 = 0 and v0 = y. Although not considered in\n[3], the network in Fig. 3 allows both A and B to vary with the\nlayer t, allowing for modest improvement (as will be demonstrated\nin Section 4).\n3.1. The LAMP Network\nWe propose to construct a neural network from unfolded AMP (7)\nwith tunable parameters {At, Bt, αt}T −1\nt=0\nlearned from training\ndata. The hope is that a “learned AMP” (LAMP) will require fewer\nlayers than LISTA, just as AMP typically requires many fewer\niterations than ISTA to converge.\nFigure 4 shows one layer of the LAMP network. Comparing\nFig. 4 to Fig. 3, we see two main differences:\n+\n+\n−\nbxt\nbxt+1\nvt\nvt+1\ny\ny\nrt\nBt\nAt\nη(•;λt)\nFig. 3. The tth layer of the LISTA network, with tunable parameters\n{At, Bt, λt}T −1\nt=0 .\n+\n+\n+\n−\n×\nbxt\nbxt+1\nvt\nvt+1\ny\ny\nrt\nαt∥•∥2\n√\nM\nλt\nBt\nAt\nη(•; •)\nbt+1\nFig. 4. The tth layer of the LAMP network, with tunable parameters\n{At, Bt, αt}T −1\nt=0\n1. LAMP includes a feed-forward path from vt to vt−1 that is\nnot present in LISTA. This path implements an “Onsager cor-\nrection” whose goal is to decouple the layers of the network,\njust as it decoupled the iterations of the AMP algorithm (re-\ncall Section 2.1.3).\n2. LAMP’s denoiser threshold λt = αt∥vt∥2/\n√\nM varies with\nthe realization vt, whereas LISTA’s is constant.\nNote that LAMP is built on a generalization of the AMP al-\ngorithm (7) wherein the matrices (A, AH) manifest as (At, Bt) at\niteration t. An important question is whether this generalization pre-\nserves the independent-Gaussian nature (10) of the denoiser input\nerror—the key feature of AMP. It can be shown that the desired\nbehavior does occur when i) At = βtA with scalar βt and ii)\nBt = AHCt with appropriately scaled Ct. Thus, for LAMP, we\nimpose5 At = βtA and learn only βt, and we initialize Bt appro-\npriately before learning.\nFrom Fig. 4, we see that the βt scaling within At can be moved\nto the denoiser η(·; ·) under a suitable re-deﬁnition of bxt, and we\ntake this approach in LAMP. Thus, the tth layer of LAMP can be\nsummarized as\nbxt+1 = βtη\n\u0000bxt + Btvt;\nαt\n√\nM ∥vt∥2\n\u0001\n(13a)\nvt+1 = y −Abxt+1 + βt\nM ∥bxt+1∥0vt,\n(13b)\nwith ﬁrst-layer inputs bx0 = 0 and v0 = y. Figure 5(c) shows the\nQQplot of LAMP’s denoiser input error\n\u0000bxt + Btvt\n\u0001\n−x. The\nshape of the plot conﬁrms that the error is Gaussian.\n3.2. Learning The LAMP Parameters\nOur proposed learning procedure is described below, where it is as-\nsumed that A is known. But if A was unknown, it could be esti-\nmated using a least-squares ﬁt to the training data. Empirically, we\nﬁnd that there is essentially no difference in ﬁnal test MSE between\nLAMP networks where i) A is known, ii) A is estimated via least-\nsquares, or iii) A is learned using back-propagation to minimize the\nloss LT from (12).\n3.2.1. Tied {Bt}\nIn the “tied” case, Bt = B0 ∀t, and so the adjustable parameters\nare Θ =\n\u0002\nB0, {αt}T −1\nt=0 , {βt}T −1\nt=0\n\u0003\n. We proposed to learn Θ by\n5Here “A” refers to the true measurement matrix from (2). If A is un-\nknown, it can be learned from the training data as described in Section 3.2.\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n-0.2\n-0.15\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n(a) ISTA\nStandard Normal Quantiles\nQuantiles of Input Sample\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\n(b) AMP\nStandard Normal Quantiles\nQuantiles of Input Sample\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n(c) LAMP\nStandard Normal Quantiles\nQuantiles of Input Sample\nFig. 5. QQplots of the denoiser input error evaluated at the ﬁrst\niteration t for which NMSE(bxt) < −15 dB. Note ISTA’s error is\nheavy tailed while AMP’s and LAMP’s errors are Gaussian due to\nOnsager correction.\nstarting with a 1-layer network and growing it one layer at a time.\nFor the ﬁrst layer (i.e., t = 0), we initialize B0 as the regularized\npseudo-inverse\nB0 = γ−1AH\u0000AAH + IM\n\u0001−1,\n(14)\nwith γ chosen so that tr(AB0) = N, and we use backpropagation\nto learn the values of α0, β0 that minimize the loss L0 from (12).\nThen, for each new layer t = 1, . . . , T −1, we\n1. initialize B0, αt, βt at the values from layer t−1,\n2. optimize αt, βt alone using backpropagation, and\n3. re-optimize\nall parameters\nB0, {αi}t\ni=0, {βi}t\ni=0\nusing\nbackpropagation to minimize the loss Lt from (12).\n3.2.2. Untied {Bt}\nIn the “untied” case, Bt is allowed to vary across layers t. We pro-\npose the same learning procedure as above, but with one exception:\nwhen initializing the parameters for each new layer, we initialize Bt\nat the regularized pseudo-inverse (14) rather than the learned value\nBt−1.\n3.2.3. Structured Bt\nMotivated by the M < N case, we recommend constraining Bt =\nAHCt and learning only Ct ∈CM×M, which reduces the num-\nber of free parameters in Bt from MN to M 2. After these Bt are\nlearned, there is no need to represent them in factored form, although\ndoing so may be advantageous if A has a fast implementation (e.g.,\nFFT). Empirically, we ﬁnd that there is essentially no difference in ﬁ-\nnal test MSE between LAMP networks where i) Bt is unconstrained\nand ii) Bt = AHCt. However, the learning procedure is more efﬁ-\ncient in the latter case.\n4. NUMERICAL RESULTS\nWe now evaluate the performance of LISTA and LAMP on the sparse\nlinear inverse problem described in Section 2.1.4. The data were\ngenerated as described in Section 2.1.4, with training mini-batches\nof size D=1000 and a single testing mini-batch of size 1000 (drawn\nindependently of the training data). The training and testing meth-\nods were implemented in Python using TensorFlow [24] with the\nADAM optimizer [25]. For LAMP, we performed the learning as\ndescribed in Section 3.2. For LISTA, we used the same approach\nto learn “untied”\n\u0002\nB0, S0, {λt}T −1\nt=0\n\u0003\nand “tied” {Bt, St, λt}T −1\nt=0 ,\nwith no constraints on St or Bt (because we found that adding con-\nstraints degraded performance).\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n-40\n-35\n-30\n-25\n-20\n-15\n-10\nLISTA tied\nLISTA untied\nLAMP tied\nLAMP untied\naverage NMSE [dB]\nlayers\nFig. 6. Test NMSE versus layer for i.i.d. Gaussian A.\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n-35\n-30\n-25\n-20\n-15\n-10\nLISTA tied\nLISTA untied\nLAMP tied\nLAMP untied\naverage NMSE [dB]\nlayers\nFig. 7. Test NMSE versus layer for A with condition number 15.\nFigure 6 shows average test NMSE ∥bxt −x∥2\n2/∥x∥2 versus\nlayer t for the same i.i.d. Gaussian A and test data used to create\nFig. 1, allowing a direct comparison. The ﬁgure shows LAMP sig-\nniﬁcantly outperforming LISTA and AMP in convergence time and\nﬁnal NMSE. For example, to reach −34 dB NMSE, tied-LAMP took\nonly 7 layers, tied-LISTA took 15, and AMP took 23.\nFigure 7 shows the results of a similar experiment, but where\nthe singular values of A were replaced by a geometric sequence that\nyielded ∥A∥2\nF = N and a condition-number of 15. For this A, AMP\ndiverged but LAMP did not, due in part to the “preconditioning”\neffect of (14).\nThe ﬁgures also show that the untied versions of LAMP and\nLISTA yielded small improvements over the tied versions, but at\nthe cost of a T -fold increase in parameter storage plus signiﬁcantly\nincreased training time.\n5. CONCLUSION\nWe considered the application of deep learning to the sparse linear\ninverse problem. Motivated by the AMP algorithm, we proposed the\nuse of Onsager correction in deep neural networks, for the purpose\nof decoupling and Gaussianizing errors across layers. Empirical re-\nsults demonstrated improved accuracy and efﬁciency over Gregor\nand LeCun’s LISTA [3].\n6. REFERENCES\n[1] Y. C. Eldar and G. Kutyniok, Compressed Sensing: Theory and\nApplications. New York: Cambridge Univ. Press, 2012.\n[2] B. A. Olshausen and D. J. Field, “Sparse coding with an over-\ncomplete basis set: A strategy employed by v1,” Vision Re-\nsearch, vol. 37, pp. 3311–3325, 1997.\n[3] K. Gregor and Y. LeCun, “Learning fast approximations of\nsparse coding,” in Proc. Int. Conf. Mach. Learning, pp. 399–\n406, 2010.\n[4] P. Sprechmann, P. Bronstein, and G. Sapiro, “Learning efﬁcient\nstructured-sparse models,” in Proc. Int. Conf. Mach. Learning,\npp. 615–622, 2012.\n[5] U. Kamilov and H. Mansour, “Learning optimal nonlineari-\nties for iterative thresholding algorithms,” IEEE Signal Pro-\ncess. Lett., vol. 23, pp. 747–751, May 2016.\n[6] Z. Wang, Q. Ling, and T. S. Huang, “Learning deep ℓ0 en-\ncoders,” in Proc. AAAI Conf. Artiﬁcial Intell., pp. 2194–2200,\n2016.\n[7] D. L. Donoho, A. Maleki, and A. Montanari, “Message pass-\ning algorithms for compressed sensing,” Proc. Nat. Acad. Sci.,\nvol. 106, pp. 18914–18919, Nov. 2009.\n[8] R. Tibshirani, “Regression shrinkage and selection via the\nlasso,” J. Roy. Statist. Soc. B, vol. 58, no. 1, pp. 267–288, 1996.\n[9] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic\ndecomposition by basis pursuit,” SIAM J. Scientiﬁc Comput.,\nvol. 20, no. 1, pp. 33–61, 1998.\n[10] E. Cand`es, J. Romberg, and T. Tao, “Stable signal recov-\nery from incomplete and inaccurate measurements,” Commu-\nnications on Pure and Applied Mathematics, vol. 59, no. 8,\npp. 1207–1223, 2006.\n[11] A. Chambolle, R. A. DeVore, N. Lee, and B. J. Lucier, “Non-\nlinear wavelet image processing: Variational problems, com-\npression, and noise removal through wavelet shrinkage,” IEEE\nTrans. Image Process., vol. 7, pp. 319–335, Mar. 1998.\n[12] I. Daubechies, M. Defrise, and C. D. Mol, “An iterative thresh-\nolding algorithm for linear inverse problems with a sparsity\nconstraint,,” Commun. Pure & Appl. Math., vol. 57, pp. 1413–\n1457, Nov. 2004.\n[13] A. Beck and M. Teboulle,\n“A fast iterative shrinkage-\nthresholding algorithm for linear inverse problems,” SIAM J.\nImag. Sci., vol. 2, no. 1, pp. 183–202, 2009.\n[14] A. Montanari, “Graphical models concepts in compressed\nsensing,” in Compressed Sensing: Theory and Applications\n(Y. C. Eldar and G. Kutyniok, eds.), Cambridge Univ. Press,\n2012.\n[15] M. Bayati and A. Montanari, “The dynamics of message pass-\ning on dense graphs, with applications to compressed sensing,”\nIEEE Trans. Inform. Theory, vol. 57, pp. 764–785, Feb. 2011.\n[16] D. L. Donoho, A. Maleki, and A. Montanari, “Message pass-\ning algorithms for compressed sensing: I. Motivation and con-\nstruction,” in Proc. Inform. Theory Workshop, (Cairo, Egypt),\npp. 1–5, Jan. 2010.\n[17] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning.\nMIT Press, 2016.\n[18] J. R. Hershey, J. Le Roux, and F. Weninger, “Deep unfolding:\nModel-based inspiration of novel deep architectures,” Tech.\nRep. TR2014-117, Mitsubishi Electric Research Labs, 2014.\n(See also arXiv:1409.2574).\n[19] U. Schmidt and S. Roth, “Shrinkage ﬁelds for effective image\nrestoration,” in Proc. IEEE Conf. Comp. Vision Pattern Recog.,\npp. 2774–2781, 2014.\n[20] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-\nresolution using deep convolutional networks,” IEEE Trans.\nPattern Anal. Mach. Intell., vol. 38, pp. 295–307, Feb. 2016.\n[21] A. Mousavi, A. Patel, and R. Baraniuk, “A deep learning ap-\nproach to structured signal recovery,” in Proc. Allerton Conf.\nCommun. Control Comput., pp. 1336–1343, 2015.\n[22] K. Kulkarni, S. Lohi, P. Turaga, R. Kerviche, and A. Ashok,\n“ReconNet: Non-iterative reconstruction of images from com-\npressively sensed random measurements,” in Proc. IEEE\nConf. Comp. Vision Pattern Recog.,\n2016.\n(see also\narXiv:1601.06892).\n[23] M. Iliadis, L. Spinoulas, and A. K. Katsaggelos, “Deep\nfully-connected networks for video compressive sensing,” in\narXiv:1603:04930, 2016.\n[24] M. Abadi, A. Agarwal, P. Barham, et al., “TensorFlow: Large-\nscale machine learning on heterogeneous systems,” 2015. Soft-\nware available from tensorﬂow.org.\n[25] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” in Proc. Internat. Conf. on Learning Repres., 2015.\n(see also arXiv:1412.6980).\n",
  "categories": [
    "cs.IT",
    "cs.LG",
    "math.IT",
    "stat.ML"
  ],
  "published": "2016-07-20",
  "updated": "2016-07-20"
}