{
  "id": "http://arxiv.org/abs/1607.06556v1",
  "title": "Syntax-based Attention Model for Natural Language Inference",
  "authors": [
    "PengFei Liu",
    "Xipeng Qiu",
    "Xuanjing Huang"
  ],
  "abstract": "Introducing attentional mechanism in neural network is a powerful concept,\nand has achieved impressive results in many natural language processing tasks.\nHowever, most of the existing models impose attentional distribution on a flat\ntopology, namely the entire input representation sequence. Clearly, any\nwell-formed sentence has its accompanying syntactic tree structure, which is a\nmuch rich topology. Applying attention to such topology not only exploits the\nunderlying syntax, but also makes attention more interpretable. In this paper,\nwe explore this direction in the context of natural language inference. The\nresults demonstrate its efficacy. We also perform extensive qualitative\nanalysis, deriving insights and intuitions of why and how our model works.",
  "text": "Syntax-based Attention Model for Natural Language Inference\nPengfei Liu\nXipeng Qiu∗\nXuanjing Huang\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\nSchool of Computer Science, Fudan University\n825 Zhangheng Road, Shanghai, China\n{pﬂiu14,xpqiu,xjhuang}@fudan.edu.cn\nAbstract\nIntroducing attentional mechanism in neu-\nral network is a powerful concept, and has\nachieved impressive results in many natural\nlanguage processing tasks. However, most of\nthe existing models impose attentional distri-\nbution on a ﬂat topology, namely the entire\ninput representation sequence.\nClearly, any\nwell-formed sentence has its accompanying\nsyntactic tree structure, which is a much rich\ntopology. Applying attention to such topol-\nogy not only exploits the underlying syntax,\nbut also makes attention more interpretable.\nIn this paper, we explore this direction in the\ncontext of natural language inference. The re-\nsults demonstrate its efﬁcacy.\nWe also per-\nform extensive qualitative analysis, deriving\ninsights and intuitions of why and how our\nmodel works.\n1\nIntroduction\nRecently, adopting neural attentional mechanism\nhas proven to be an extremely successful technique\nin a wide range of natural language processing tasks,\nranging from machine translation (Bahdanau et al.,\n2014), sentence summarization (Rush et al., 2015),\nquestion answering (Hermann et al., 2015) and text\nentailment (Rockt¨aschel et al., 2015; Wang and\nJiang, 2015; Cheng et al., 2016). The basic idea is\nto learn and attend to most relevant parts of (poten-\ntially preprocessed) a sequence X while analysing\nor generating another sequence Y .\nTaking the following two sentences as examples,\nwhere we highlight the helpful partial information\n∗Corresponding author.\nFigure 1:\nA motivated example to illustrate\nsequence-based and syntax-based attention model\nfor target word “autumn”. The square boxes repre-\nsent hidden states of the words (or phrases); darker\nindicates higher alignment.\nalignment from X according to Y with attention.\nX: A toddler sits on a rock chair with fallen leaves.\nY : A little child sits quietly on a stone bench in au-\ntumn.\nThe sequence-based attention is illustrated in Fig-\nure 1(a). The representation is a ﬂat sequence, and\nattention distribution is applied to this simple topol-\nogy. Although the idea is to soft-align words and\nphrases in the two sentences, one can observe that:\n1) The hidden state of each position incorporates its\ncontext information, which is implicit and sequen-\ntial, alignment at phrase-level is thus challenging\n(e.g. “autumn” to “fallen leaves”). 2) As we will\ndiscuss shortly, the attention is implemented with a\nweighted sum of sequence, thus lacks linguistic in-\nterpretation for its semantic composition.\nAny well-formed sentences have its underlying\nsyntactic structure. It is a tree topology that encodes\na sentence’s important composing subcomponents.\nEvidently, this is in stark contrast with the ﬂat and\nsequential topology the existing models assume.\narXiv:1607.06556v1  [cs.CL]  22 Jul 2016\nIn this paper we extend the attentional mecha-\nnism from a sequence to a tree, allowing syntactic\ninformation to be integrated. As shown in Figure\n1(b), syntax-based attention allows neural models to\nmore explicitly capture the phrase-level alignment.\nIn addition, it clearly reaches a higher level of in-\nterpretability. While this observation is general, in\nthis paper we demonstrate its effectiveness in natu-\nral language inference. We believe other tasks such\nas neural translation model (Bahdanau et al., 2014;\nLuong et al., 2015) can similarly beneﬁt from this\nidea.\nThe contributions of this paper can be summa-\nrized as follows.\n1. We extend sequence-based attention to syntax-\nbased, therefore incorporating richer linguistic\nproperties.\n2. We design and validate our algorithm that\nmakes such topological attentional mechanism\npossible.\n3. Beyond quantitative measurement, we care-\nfully perform qualitative analysis, and demon-\nstrate why and how the idea works.\n4. Our work can be regarded as an attempt to\nboost the generalization ability of attention\nmatching mechanism by encoding prior knowl-\nedge (syntax). As an example, our results show\nsyntactic structure of sentence or phrase is cru-\ncial for text semantic matching.\n2\nNeural Attention Model for Natural\nLanguage Inference\nNatural language inference, also called text entail-\nment, is a task to determine the semantic relation-\nship (entailment, contradiction, or neutral) between\ntwo sentences (a premise and a hypothesis). This\ntask is important involved in many natural language\nprocessing (NLP) problems, such as information ex-\ntraction, relation extraction, text summarization or\nmachine translation.\nTo better understand this task, we give an example\nin the dataset as follows:\nPremise: These girls are having a great time look-\ning for seashells.\nHypothesis: The girls are happy.\nLabel: entailment\nMore precisely, NLI can be framed as a sim-\nple three-way classiﬁcation task, which requires\nthe model to be able to represent and reason with\nthe core phenomena of natural language semantics\n(Bowman et al., 2016).\n2.1\nLong Short-Term Memory Network\nLong\nshort-term\nmemory\nneural\nnetwork\n(LSTM)\n(Hochreiter\nand\nSchmidhuber,\n1997)\nis a type of recurrent neural network (RNN) (Elman,\n1990),\nand speciﬁcally addresses the issue of\nlearning long-term dependencies. LSTM maintains\na memory cell that updates and exposes its content\nonly when deemed necessary.\nWhile there are numerous LSTM variants, here\nwe use the LSTM architecture used by (Jozefowicz\net al., 2015), which is similar to the one in (Graves,\n2013) but without peep-hole connections.\nWe deﬁne the LSTM units at each time step t to\nbe a collection of vectors in Rd: an input gate it, a\nforget gate ft, an output gate ot, a memory cell ct\nand a hidden state ht. d is the number of the LSTM\nunits. The elements of the gating vectors it, ft and\not are in [0, 1].\nThe LSTM is precisely speciﬁed as follows.\n\n\n˜ct\not\nit\nft\n\n=\n\n\ntanh\nσ\nσ\nσ\n\nTA,b\n\u0014 xt\nht−1\n\u0015\n,\n(1)\nct = ˜ct ⊙it + ct−1 ⊙ft,\n(2)\nht = ot ⊙tanh (ct) ,\n(3)\nwhere xt is the input at the current time step; TA,b\nis an afﬁne transformation which depends on param-\neters of the network A and b. σ denotes the logistic\nsigmoid function and ⊙denotes elementwise multi-\nplication.\nThe update of each LSTM unit can be written pre-\ncisely as\n(ht, ct) = LSTM(ht−1, ct−1, xt).\n(4)\nHere, the function LSTM(·, ·, ·) is a shorthand\nfor Eq. (1-3).\nLSTM can map the input sequence of arbitrary\nlength to a ﬁxed-sized vector, and has been success-\nfully applied to a wide range of NLP tasks, such\nas machine translation (Sutskever et al., 2014), lan-\nguage modelling (Sutskever et al., 2011) and natural\nlanguage inference (Rockt¨aschel et al., 2015).\n2.2\nNeural Attention Model\nGiven two sequences X = x1, x2, · · · , xn and Y =\ny1, y2, · · · , ym, we let xi ∈Rd denote the embedded\nrepresentation of the word xi. The standard LSTM\nhas one temporal dimension: at position i of sen-\ntence x1:n, the output hx\ni reﬂects the meaning of the\nsubsequence x1:i = x1, · · · , xi.\nThe main idea of attention model (Hermann et al.,\n2015) is that the representation of sentence X is ob-\ntained dynamically based on the degree of alignment\nbetween the words in sentence X and Y . More for-\nmally, for sentence X and Y , we ﬁrst compute the\nhidden state of each sentence by two LSTMs: 1:\nhx\ni = LSTM(hx\ni−1, cx\ni−1, xi)\n(5)\nhy\nj = LSTM(hy\nj−1, cy\nj−1, yj)\n(6)\nWhile processing sentence Y at time j, the model\nemits an attention vector αj ∈Rn to weight hx\ni , the\nhidden states of X, thereby obtaining a ﬁne-grained\nrepresentation r of sentence X as follows:\nrx\nj =\nn\nX\ni=1\nαjihx\ni + tanh(Wrrx\nj−1)\n(7)\nwhere αji can be compute as:\nαji = softmax(eji) =\nexp(eji)\nP\ni′ exp(eji′)\n(8)\nWhere eji is a alignment score and can obtained by:\neji = we · tanh(Wyhy\nj + Wxhx\ni + Wrrx\nj−1)\n(9)\nwhere Wy, Wx, Wr are learned parameters.\nFinally, the representation of the sentence pair h∗\nis constructed by the last attention-weighted repre-\nsentation rm and the last output vector hy\nm as:\nh∗= tanh(Wxrx\nm + Wyhy\nm).\n(10)\n1The model used by (Rockt¨aschel et al., 2015) is a little dif-\nferent from this for a better performance, in which encoding of\none sentence is conditioned on the other.\nFigure 2: Two matching frameworks: Sequence-\nbased attention model and syntax-based attention\nmodel. The box represents hidden state h of a node\nand the bold yellow box represents the node yj of\nsentence Y at the position j. The darker blue box\nrepresents a higher alignment score between the cor-\nresponding node and the node yj.\nFor the entailment task, the ﬁnal representation h∗\nof sentence-pair, is fed into the output layer, gen-\nerating the probabilities over all pre-deﬁned classes\n(entailment, contradiction, or neutral) .\nˆl = softmax(Woh∗+ bo)\n(11)\nwhere Wo and bo are parameters of the model.\n3\nSyntax-Based Attention Matching Model\nThe building block of this work syntax-based\ninstead of sequence-based compositional model.\nThere are several such candidates, such as recur-\nsive neural network (Socher et al., 2013) and tree-\nstructured LSTM (Tai et al., 2015). In this paper, we\nuse latter model since for its superior performance\nin representing sentence meaning.\n3.1\nTree-structured LSTM\nDifferent\nwith\nstandard\nLSTM,\ntree-structured\nLSTM composes its state from an input vector and\nthe hidden states of children units. More formally,\nthe model takes as input a syntactic tree (con-\nstituency tree or dependency tree), then a compo-\nsition function is applied to combine the children\nnodes according to the syntactic structure to obtain\nan new compositional vector for their parent node.\nHere we investigate two types of composition\nfunctions for constituency and dependency tree re-\nspectively.\nComposition Function for Constituency Tree\nGiven constituency tree T induced by a sentence,\nthere are at most N children nodes for each parent\nnode. We refer to hjk and cjk as the hidden state\nand memory cell of the k-th child of node j. The\ntransition equations of each node j are as follows:\n\n\n˜cj\noj\nij\n\n=\n\n\ntanh\nσ\nσ\n\n\n\u0012\nWp\n\u0014xj\nHj\n\u0015\n+ bp\n\u0013\n,\n(12)\nfjk = σ(Wfxj + Uf\nkHj),\n(13)\ncj = ˜cj ⊙ij +\nN\nX\nk\ncjk ⊙fjk,\n(14)\nhj = oj ⊙tanh (cj) ,\n(15)\nHj = hj1 ⊕hj2 ⊕· · · ⊕hjN,\n(16)\nwhere xj denotes the input vector and is non-zero if\nand only if it is a leaf node. σ represents the logistic\nsigmoid function and ⊙denotes element-wise mul-\ntiplication. Wp, Wf, and Uk is the weight matrix\nwhich depends on parameters of the network.\nComposition Function for Dependency Tree\nFor the dependency tree, we refer to C(j) as the set\nof children of node j. Then the transition equations\nof each node j are formulated as:\n\n\n˜cj\noj\nij\n\n=\n\n\ntanh\nσ\nσ\n\n\n\u0012\nWp\n\u0014xj\n˜Hj\n\u0015\n+ bp\n\u0013\n,\n(17)\nfjk = σ(Wfxj + Ufhjk)\n(18)\ncj = ˜cj ⊙ij +\nC(j)\nX\nk\ncjk ⊙fjk,\n(19)\nhj = oj ⊙tanh (cj) ,\n(20)\n˜Hj =\nC(j)\nX\nk\nhjk\n(21)\nwhere Wp, Wf, and Uf are the weight matrices\nwhich depend on parameters of the network.\nThe update of each unit can be written precisely\nas\nhj = TreeLSTM(Hj, ˜Hj, xj).\n(22)\nHere, the function TreeLSTM(·, ·, ·) is a short-\nhand for Eq. (12-16) for constituency tree or (17-20)\nfor dependency tree.\n3.2\nSyntax-Based Attention Matching Model\nThe second stage of the design is to apply attention\nto the tree topology. For two trees T x and T y in-\nduced by sentence X and Y , the representation of\ntheir subtrees hx\ni and hy\nj can be obtained as follows:\nhx\ni = TreeLSTM(Hx\ni , ˜Hj, xi)\n(23)\nhy\nj = TreeLSTM(Hy\nj , ˜Hj, yj)\n(24)\nAt node j of tree T y, we reread over tree T x and\ncompute a weighted tree representation rx\nj of tree\nT x, which also recursively accumulate information\nfrom its children Rj = {rj1, rj2, · · · , rjN}.\nrx\nj =\nTn\nX\ni=1\nαjihx\ni + tanh(g(Rj))\n(25)\nwhere Tn denotes the number of nodes of tree Tx;\nαji measures the alignment degree between two sub-\ntrees; g(Rj) is recursively accumulate information\nfrom its children.\nFor constituency tree,\ng(Rj) = Wr(rj1 ⊕rj2 ⊕· · · ⊕rjN).\n(26)\nFor dependency tree,\ng(Rj) = ˜\nWr\nC(j)\nX\nk\nrjk.\n(27)\nThe attention αji between two subtrees hy\nj and hx\ni\ncan be computed as\neji = we · tanh(Wyhy\nj + Wxhx\ni + g(Rj)),\n(28)\nαji = softmax(eji).\n(29)\nThe ﬁnal representation h∗of two trees T x and\nT y can be obtained by\nh∗= tanh(Wxrx\nTm + Wyhy\nTm),\n(30)\nwhere Tm denotes the number of nodes of tree Ty.\nTo facilitate the description later, we refer to\nSAT-LSTMs as our proposed syntax-based attention\nmodel. dLSTM and cLSTM represent LSTMs are\nbuilt over a dependency and constituency respec-\ntively.\n4\nTraining\nGiven a sentence pair (X, Y ) and its label l. The\noutput ˆl of neural network is the probabilities of the\ndifferent classes.\nThe parameters of the network\nare trained to minimise the cross-entropy of the pre-\ndicted and true label distributions.\nL(X, Y ; l,ˆl) = −\nC\nX\nj=1\nlj log(ˆlj),\n(31)\nwhere l is one-hot representation of the ground-truth\nlabel l;ˆl is predicted probabilities of labels; C is the\nclass number.\nTo minimize the objective, we use stochastic gra-\ndient descent with the diagonal variant of AdaGrad\n(Duchi et al., 2011). To prevent exploding gradients,\nwe perform gradient clipping by scaling the gradient\nwhen the norm exceeds a threshold (Graves, 2013).\n4.1\nInitialization and Hyperparameters\nOrthogonal Initialization\nWe use orthogonal ini-\ntialization of our LSTMs, which allows neurons to\nreact to the diverse patterns and is helpful to train a\nmulti-layer network (Saxe et al., 2013).\nUnsupervised Initialization\nThe word embed-\ndings for all of the models are initialized with the\n100d GloVe vectors (840B token version, (Penning-\nton et al., 2014)). The other parameters are initial-\nized by randomly sampling from uniform distribu-\ntion in [−0.1, 0.1].\nHyperparameters\nFor each task, we take the\nhyperparameters which achieve the best perfor-\nmance on the development set via an small grid\nsearch over combinations of the initial learn-\ning rate [0.05, 0.0005, 0.0001], l2 regularization\n[0.0, 5E−5, 1E−5, 1E−6] and the threshold value\nof gradient norm ρ [5, 10, 50].\nThe ﬁnal hyper-\nparameters are reported in Table 1.\nHyper-parameters\nSNLI\nEmbedding size\n100\nHidden layer size\n100\nInitial learning rate 0.005\nRegularization\n0.0\nρ\n50\nTable 1: Hyper-parameters for our model on SNLI.\n5\nExperiment\nWe use the Stanford Natural Language Inference\nCorpus (SNLI) (Bowman et al., 2015). This cor-\npus contains 570K sentence pairs, and all of the\nsentences and labels stem from human annotators.\nSNLI is two orders of magnitude larger than all other\nexisting RTE corpora. Therefore, the massive scale\nof SNLI allows us to train powerful neural networks\nsuch as our proposed architecture in this paper.\n5.1\nData Preparation\nWe parse the sentences in the dataset for our tree-\nstructured LSTMs. More speciﬁcally, for the De-\npendency Tree-LSTMs, we produce dependency\nparses(Chen and Manning, 2014) of each sentence;\nFor constituency Tree-LSTMs, the trees are parsed\nby binarized constituency parser(Klein and Man-\nning, 2003).\n5.2\nCompetitor Methods\n• Neural bag-of-words (NBOW): Each sequence\nis represented as the sum of the embeddings of\nthe words it contains, and then they are con-\ncatenated and fed to a multi-layer perceptron\n(MLP).\n• LSTM encoders: The sentence pair are en-\ncoded by LSTMs respectively.\n• Attention LSTM encoders (AT-LSTMs): The\nsentence pair are encoded with the consider-\nation of the alignment of words between two\nsentences (Rockt¨aschel et al., 2015).\n• Tree-based CNN encoders: The sentence pair\nare encoded by tree-based CNNs respectively\n(Mou et al., 2015).\n• Tree-based LSTM encoders: The sentence pair\nare encoded by tree-based LSTM respectively.\nFigure 3: Visualization of syntax-based alignments\nover two subtrees. The numbers along the dotted\nlines represent the alignment scores.\n• SPINN-PI encoder: The sentence pair are en-\ncoded by stack-augmented parser-interpreter\nneural network with parsed input respectively,\nwhich is proposed by (Bowman et al., 2016).\n5.3\nResults\nTable 2 provides a comparison of results on SNLI\ndataset. From the table, we can observe that:\n• For two kinds of syntax-based LSTM encoders,\ncLSTM achieve better performances than dL-\nSTM, which is consistent with Gildea (2004)\nexperiment results on tree-based alignment.\nWe think the reason is that constituency-based\nmodel can better learn the semantic composi-\ntionality and it has taken the orders of child\nnodes into consideration.\n• Irrespective of attention mechanism, both two\nsyntax-based LSTM encoders are superior to\nsequence-based LSTM encoder, which indi-\ncates the effectiveness of syntax-based compo-\nsition.\n• SAT-cLSTMs\nsurpass\nall\nthe\ncompetitor\nmethods and achieve the best performance.\nMore\nprecisely,\nSAT-cLSTMs\noutperform\nAT-LSTMs by 2.1%, and are superior to\nTree-LSTM encoders by 3.8%, which suggests\nthe\nimportance\nof\nincorporating\nsyntactic\ninformation into attention models.\n5.4\nExperiment Analysis\n5.4.1\nAnalysis of Compositionality and\nAttention Mechanism\nCan our model select useful composition infor-\nmation using attention mechanism ? To answer this\nquestion, we sample several sub-tree pairs from test\ndataset which achieve the best alignment of a sen-\ntence pair.\nAs shown in Figure 3, we can observe that,\n• The alignments in these cases are consistent\nwith people’s understanding. For example, the\nalignment degree α(autumn, fallen leaves)\nis much higher than α(autumn, fallen) and\nα(autumn, leaves), which is crucial for the ﬁ-\nnal prediction of the two sentence’ relation and\nindicates the effectiveness of this syntax-based\ncomposition.\n• Our model has learned the alignment between\nsubtrees, meaning that matching patterns at\nword-phrase or phrase-phrase level can be cap-\ntured effectively not merely at word-word level.\n5.4.2\nAnalysis of Phrases Representations\nWe compute the representations of each subtree\nand show some examples sampled from test dataset\nwith their most related neighbors in Table 3.\nThe phrasal paraphrases, such as “having a\ngreat time/enjoy time together”, have\nobtained close representations, which is more help-\nful for the identiﬁcation of the entailment rela-\ntion of two sentences.\nBesides, we can see the\nability of the model to learn a variety of gen-\neral paraphrastic transformations, such as possessive\nrule “persons’s/of a person” and verb par-\nticle shift “holding his cup up/holding\nup a white plastic cup”.\nSome other examples such as “wearing a\npink dress/in a pink dress/dressed\nin pink” indicate our SAT-LSTMs model is more\nrobust to syntactic variations, which is more crucial\nto boost the generalization ability while encoding a\nsentence or sentence pair.\n5.4.3\nAnalysis of Learned Sentence\nRepresentations\nWe explore the sentence representations learned\nby the three different models on the SNLI. Table 4\nModel\nHidden.\nTrain acc. (%)\nDev. acc. (%)\nTest acc. (%)\nPrevious non-NN results\nLexicalized classiﬁer (Bowman et al., 2015)\n—\n99.7\n—\n78.2\nPrevious sentence encoder-based NN results\nLSTM encoders (Bowman et al., 2015)\n100\n84.8\n—\n77.6\nTree-based CNN encoders (Mou et al., 2015)\n300\n83.4\n82.4\n82.1\nSPINN-PI encoders (Bowman et al., 2016)\n300\n89.2\n—\n83.2\nAT-LSTMs encoders (Rockt¨aschel et al., 2015)\n100\n85.3\n83.7\n83.5\nOur results\nTree-dLSTM encoders\n100\n83.5\n77.1\n78.7\nTree-cLSTM encoders\n100\n82.2\n79.8\n80.3\nAT-LSTMs encoders\n100\n84.2\n82.7\n82.0\nSAT-dLSTMs\n100\n86.6\n83.8\n83.4\nSAT-cLSTMs\n100\n87.9\n85.0\n84.1\nTable 2:\nResults of our proposed models against other neural models on SNLI corpus. Hidden. is the\nnumber of neurons in hidden state h. Train, Dev. and Test denote the classiﬁcation accuracy. SAT-LSTMs\ndenote our proposed syntax-based attention model. dLSTM and cLSTM represent LSTMs are built over a\ndependency and constituency respectively.\nperson ’s\nholding his cup up\nwearing a pink dress\nhaving a great time\npeople ’s\nholding up a white plastic cup\nin a pink dress\nhaving a good time\nbelong to the lady\nwith a cup in his hand\ndressed in pink\nenjoy time together\nof a person\nwith a beer in his hand\nwearing a pink dress\nis very happy\nof humans\nholds up a playing card\nin pink\nenjoying a night\nTable 3: Nearest neighbor phrases drawn from the SNLI test set, which based on cosine similarity of different\nrepresentations produced by SAT-LSTMs.\nthe boys are bare chested\na golden retriever nurses puppies\nNBOW\nthe men are naked\na cat nurses puppies\nthe boys are stretching\na puppy barks at a girl\nthe boys are sleeping\nthe dog is a labrador retriever\nthe boys are sitting down\na golden retriever nurses some other dogs puppies\nthe man has nothing on his face\na girl is sitting on a park bench holding a puppy\nAT-LSTMs\na man is outside with no bag on his back\na big dog watching over a smaller dog\nhis bald head is exposed\nthe big dog is checking out the smaller dog\na man in summer clothing skiing on thin snow\na gal is holding a stuffed dog\nthe man is not wearing a shirt\na golden retriever nurses some other dogs puppies\nSAT-LSTMs\ntwo men are shirtless\nthree puppies are snuggling with their mother by the ﬁre\nthe man is completely nude\npuppies next to their mother\na man without a shirt is on the water\na mother dog checking up on her baby puppy\nTable 4: Nearest neighbor sentences drawn from the SNLI test set, which based on cosine similarity of\ndifferent representations emitted by NBOW, AT-LSTMs and SAT-LSTMs.\nillustrates the nearest neighbors of sentence repre-\nsentations learned from NBOW, AT-LSTMs, SAT-\nLSTMs.\nAs shown in Table 4, NBOW ﬁnds a sen-\ntence’s neighbors with full consideration of lexical\nparaphrase.\nWhile the neighbors returned by\nSAT-LSTMs are mostly syntactic variations with\nmeaning preserving.\nFor example, for the ﬁrst\nsentence\n“the boy are bare chested”,\nNBOW gives the “the men are naked” most\nlikely based on the word pair “bare/naked”,\nthereby ignoring the information of “chested”.\nHowever,\nthe sentences given by SAT-LSTMs\ncontain\nthe\nsame\nmeaning\nwith\nample\nways\nof\nexpressions,\nsuch\nas\n“the man is not\nwearing a shirt” and “the man without\na shirt”, which accurately reﬂect the meaning\nof “bare chested”.\nCompared with AT-LSTMs, SAT-LSTMs can\nprovide more ﬂexible syntactic expressions. For ex-\nample, for the sentence ‘a golden retriever\nnurses puppies”,\nSAT-LSTMs capture this\nsyntactic\nparaphrase\n‘A nurses B/B is\nsnuggling with A”, which is difﬁcult for\nNBOW and AT-LSTMs models.\n6\nRelated Work\nThere has been recent work proposing to incorpo-\nrate syntax priori into neural network.\nSocher et\nal. (2012) use a recursive neural network model\nthat learns compositional vector representations for\nphrases and sentences of arbitrary syntactic type\nand length. Tai et al. (2015) introduce a general-\nization of the standard LSTM architecture to tree-\nstructured network. Bowman et al. (2016) propose\nan stack-augmented Parser-Interpreter Neural Net-\nwork for sentence encoding, which combines pars-\ning and interpretation within a single tree-sequence\nhybrid model. These models are designed for rep-\nresenting a sentence in more plausible way, while\nwe want to model the strong interaction of two sen-\ntences over tree structure.\nMore recently, several works have tried to in-\ncorporate priori into attention based model. Cohn\net al. (2016) extend the attentional neural transla-\ntion model to include structural biases from word\nbased alignment models. Gu et al. (2016) incorpo-\nrate copying mechanism into attention based model\nto address the OOV problem in a more systemic way\nfor machine translation. Different with these mod-\nels, we augment attention model with syntax priori\nfor semantic matching.\nAnother thread of work is sequential attention\nmodels for natural language inference. Rockt¨aschel\net al. (2015) propose to use attention model for sen-\ntence pair encoding. Wang and Jiang (2015) extend\nthis model by paying more attention to important\nword-level matching results. Compared with these\nmodels, we integrate syntax structure into attention\nmatching model, which can match two trees in a\nplausible way.\n7\nOutlook\nNatural language has its underlying syntactic struc-\nture, which gives a feasibility to assign attention to\ntree-structured topologies instead of a ﬂat sequence.\nAlthough we just use it in context of natural lan-\nguage inference, the idea of syntax-based attention\nmodel can be easily transferred to other tasks for\nphrase-level alignment, such as neural translation\nmodel.\nWhen we submit our paper, we ﬁnd this\npaper (Eriguchi et al., 2016), which proposed tree-\nto-sequence attention based model for neural ma-\nchine translation, thereby showing the effectiveness\nof syntax-based attention mechnism. The major dif-\nference is their model is based on word-to-word and\nword-to-phrase attention (sequence conditioned on\ntree) whereas our proposed model focus on phrase-\nto-phrase attention (tree over tree).\n8\nConclusion\nIn this paper, we integrate syntax structure into at-\ntention model. Compared with sequence-based at-\ntention model, our model can easily capture phrase-\nlevel alignment.\nExperiments on Stanford Natu-\nral Language Inference Corpus demonstrate the efﬁ-\ncacy of our proposed model and its superiority to\ncompetitor models.\nFurthermore, we have made\nan elaborate experiment design and case analysis to\nevaluate the effectiveness of our syntax-base match-\ning model and explain why attention over trees is a\ngood idea.\nIn future, we wish to use our SAT-LSTMs\nmatching model to learn the representation of\nphrasal(Wieting et al., 2015) or syntactic para-\nphrases from massive paraphrase dataset, such as\nPPDB (Ganitkevitch et al., 2013). We expect that\nthe learned representation of subtree with rich prior\nknowledge should be useful for downstream tasks in\na pre-trained manner.\nReferences\n[Bahdanau et al.2014] D. Bahdanau, K. Cho, and Y. Ben-\ngio.\n2014.\nNeural machine translation by jointly\nlearning to align and translate.\nArXiv e-prints,\nSeptember.\n[Bowman et al.2015] Samuel R. Bowman, Gabor Angeli,\nChristopher Potts, and Christopher D. Manning. 2015.\nA large annotated corpus for learning natural language\ninference. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing.\n[Bowman et al.2016] Samuel R Bowman, Jon Gauthier,\nAbhinav Rastogi, Raghav Gupta, Christopher D Man-\nning, and Christopher Potts.\n2016.\nA fast uniﬁed\nmodel for parsing and sentence understanding. arXiv\npreprint arXiv:1603.06021.\n[Chen and Manning2014] Danqi Chen and Christopher D\nManning.\n2014.\nA fast and accurate dependency\nparser using neural networks. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 740–750.\n[Cheng et al.2016] Jianpeng Cheng, Li Dong, and Mirella\nLapata. 2016. Long short-term memory-networks for\nmachine reading. arXiv preprint arXiv:1601.06733.\n[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,\nEkaterina Vymolova, Kaisheng Yao, Chris Dyer, and\nGholamreza Haffari. 2016. Incorporating structural\nalignment biases into an attentional neural translation\nmodel. arXiv preprint arXiv:1601.01085.\n[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram\nSinger. 2011. Adaptive subgradient methods for on-\nline learning and stochastic optimization. The Journal\nof Machine Learning Research, 12:2121–2159.\n[Elman1990] Jeffrey L Elman. 1990. Finding structure in\ntime. Cognitive science, 14(2):179–211.\n[Eriguchi et al.2016] Akiko\nEriguchi,\nKazuma\nHashimoto, and Yoshimasa Tsuruoka.\n2016.\nTree-\nto-sequence attentional neural machine translation.\nCoRR, abs/1603.06075.\n[Ganitkevitch et al.2013] Juri\nGanitkevitch,\nBenjamin\nVan Durme, and Chris Callison-Burch. 2013. Ppdb:\nThe paraphrase database.\nIn HLT-NAACL, pages\n758–764.\n[Gildea2004] Daniel Gildea.\n2004.\nDependencies vs.\nconstituents for tree-based alignment.\nIn EMNLP,\npages 214–221. Citeseer.\n[Graves2013] Alex Graves. 2013. Generating sequences\nwith recurrent neural networks.\narXiv preprint\narXiv:1308.0850.\n[Gu et al.2016] Jiatao Gu, Zhengdong Lu, Hang Li, and\nVictor OK Li. 2016. Incorporating copying mecha-\nnism in sequence-to-sequence learning. arXiv preprint\narXiv:1603.06393.\n[Hermann et al.2015] Karl Moritz Hermann, Tomas Ko-\ncisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. 2015. Teach-\ning machines to read and comprehend. In Advances in\nNeural Information Processing Systems, pages 1684–\n1692.\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and\nJ¨urgen Schmidhuber. 1997. Long short-term memory.\nNeural computation, 9(8):1735–1780.\n[Jozefowicz et al.2015] Rafal\nJozefowicz,\nWojciech\nZaremba, and Ilya Sutskever.\n2015.\nAn empirical\nexploration of recurrent network architectures.\nIn\nProceedings of The 32nd International Conference on\nMachine Learning.\n[Klein and Manning2003] Dan Klein and Christopher D\nManning. 2003. Accurate unlexicalized parsing. In\nProceedings of the 41st Annual Meeting on Associ-\nation for Computational Linguistics-Volume 1, pages\n423–430.\n[Luong et al.2015] Thang\nLuong,\nHieu\nPham,\nand\nChristopher D. Manning. 2015. Effective approaches\nto attention-based neural machine translation.\nIn\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages\n1412–1421, Lisbon, Portugal, September. Association\nfor Computational Linguistics.\n[Mou et al.2015] Lili Mou, Men Rui, Ge Li, Yan Xu,\nLu Zhang, Rui Yan, and Zhi Jin. 2015. Recogniz-\ning entailment and contradiction by tree-based convo-\nlution. arXiv preprint arXiv:1512.08422.\n[Pennington et al.2014] Jeffrey\nPennington,\nRichard\nSocher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. Proceedings\nof the Empiricial Methods in Natural Language\nProcessing (EMNLP 2014), 12:1532–1543.\n[Rockt¨aschel et al.2015] Tim\nRockt¨aschel,\nEdward\nGrefenstette, Karl Moritz Hermann, Tom´aˇs Koˇcisk`y,\nand Phil Blunsom.\n2015.\nReasoning about en-\ntailment with neural attention.\narXiv preprint\narXiv:1509.06664.\n[Rush et al.2015] Alexander M. Rush, Sumit Chopra, and\nJason Weston. 2015. A neural attention model for ab-\nstractive sentence summarization. In Proceedings of\nthe 2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 379–389, Lisbon, Portu-\ngal, September.\n[Saxe et al.2013] Andrew M Saxe, James L McClelland,\nand Surya Ganguli. 2013. Exact solutions to the non-\nlinear dynamics of learning in deep linear neural net-\nworks. arXiv preprint arXiv:1312.6120.\n[Socher et al.2012] Richard\nSocher,\nBrody\nHuval,\nChristopher D Manning, and Andrew Y Ng.\n2012.\nSemantic compositionality through recursive matrix-\nvector spaces.\nIn Proceedings of EMNLP, pages\n1201–1211.\n[Socher et al.2013] Richard\nSocher,\nAlex\nPerelygin,\nJean Y Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. 2013. Recur-\nsive deep models for semantic compositionality over a\nsentiment treebank. In Proceedings of EMNLP.\n[Sutskever et al.2011] Ilya Sutskever, James Martens, and\nGeoffrey E Hinton. 2011. Generating text with recur-\nrent neural networks. In Proceedings of the 28th Inter-\nnational Conference on Machine Learning (ICML-11),\npages 1017–1024.\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc VV Le. 2014. Sequence to sequence learning\nwith neural networks. In Advances in Neural Informa-\ntion Processing Systems, pages 3104–3112.\n[Tai et al.2015] Kai Sheng Tai, Richard Socher, and\nChristopher D Manning. 2015. Improved semantic\nrepresentations from tree-structured long short-term\nmemory networks. arXiv preprint arXiv:1503.00075.\n[Wang and Jiang2015] Shuohang Wang and Jing Jiang.\n2015. Learning natural language inference with lstm.\narXiv preprint arXiv:1512.08849.\n[Wieting et al.2015] John Wieting, Mohit Bansal, Kevin\nGimpel, and Karen Livescu. 2015. Towards univer-\nsal paraphrastic sentence embeddings. arXiv preprint\narXiv:1511.08198.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2016-07-22",
  "updated": "2016-07-22"
}