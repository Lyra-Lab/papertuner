{
  "id": "http://arxiv.org/abs/1608.02971v1",
  "title": "Neuroevolution-Based Inverse Reinforcement Learning",
  "authors": [
    "Karan K. Budhraja",
    "Tim Oates"
  ],
  "abstract": "The problem of Learning from Demonstration is targeted at learning to perform\ntasks based on observed examples. One approach to Learning from Demonstration\nis Inverse Reinforcement Learning, in which actions are observed to infer\nrewards. This work combines a feature based state evaluation approach to\nInverse Reinforcement Learning with neuroevolution, a paradigm for modifying\nneural networks based on their performance on a given task. Neural networks are\nused to learn from a demonstrated expert policy and are evolved to generate a\npolicy similar to the demonstration. The algorithm is discussed and evaluated\nagainst competitive feature-based Inverse Reinforcement Learning approaches. At\nthe cost of execution time, neural networks allow for non-linear combinations\nof features in state evaluations. These valuations may correspond to state\nvalue or state reward. This results in better correspondence to observed\nexamples as opposed to using linear combinations. This work also extends\nexisting work on Bayesian Non-Parametric Feature Construction for Inverse\nReinforcement Learning by using non-linear combinations of intermediate data to\nimprove performance. The algorithm is observed to be specifically suitable for\na linearly solvable non-deterministic Markov Decision Processes in which\nmultiple rewards are sparsely scattered in state space. A conclusive\nperformance hierarchy between evaluated algorithms is presented.",
  "text": "1\nNeuroevolution-Based Inverse Reinforcement\nLearning\nKaran K. Budhraja and Tim Oates\nComputer Science and Electrical Engineering\nUniversity of Maryland, Baltimore County\nBaltimore, Maryland 21250, USA\nEmail: karanb1@umbc.edu, oates@cs.umbc.edu\nAbstract—The problem of Learning from Demonstration is\ntargeted at learning to perform tasks based on observed exam-\nples. One approach to Learning from Demonstration is Inverse\nReinforcement Learning, in which actions are observed to infer\nrewards. This work combines a feature based state evaluation\napproach to Inverse Reinforcement Learning with neuroevolu-\ntion, a paradigm for modifying neural networks based on their\nperformance on a given task. Neural networks are used to learn\nfrom a demonstrated expert policy and are evolved to generate a\npolicy similar to the demonstration. The algorithm is discussed\nand evaluated against competitive feature-based Inverse Rein-\nforcement Learning approaches. At the cost of execution time,\nneural networks allow for non-linear combinations of features\nin state evaluations. These valuations may correspond to state\nvalue or state reward. This results in better correspondence to\nobserved examples as opposed to using linear combinations. This\nwork also extends existing work on Bayesian Non-Parametric\nFeature Construction for Inverse Reinforcement Learning by\nusing non-linear combinations of intermediate data to improve\nperformance. The algorithm is observed to be speciﬁcally suitable\nfor a linearly solvable non-deterministic Markov Decision Pro-\ncesses in which multiple rewards are sparsely scattered in state\nspace. A conclusive performance hierarchy between evaluated\nalgorithms is presented.\nI. INTRODUCTION\nThe concept of Reinforcement Learning (RL) is motivated\nby modeling learning from experience. The environment is\nsegmented into states, each of which contain information to\ndescribe the environment in that segment. The learner, also\ncalled the agent, may beneﬁt differently depending on which\nstate it is in. This creates a notion of rewards corresponding to\neach state. RL then seeks to ﬁnd the optimal actions over all\nthe states based on rewards observed when taking actions in\nthose states over time. An example would be a child learning\nhow to build a pyramid from a set of blocks.\nInverse Reinforcement Learning (IRL) is motivated by\nlearning from examples, such as someone showing a child how\nto build the block pyramid and the child then trying to replicate\nthe process. As opposed to RL, an agent in IRL does not\nobserve rewards; it attempts to recover them based on observed\nexamples. The observations are in the form of traces of state-\naction pairs. IRL allows an agent to understand its environment\nin terms of evaluation of a state. It is therefore intuitive that\nIRL is a means to implement Learning from Demonstration\n(LfD) [1], [2]. LfD describes a problem in which an agent\nlearns to perform a task by observing how it is to be done.\nThe learner exploits the demonstrator’s experience and does\nnot have to learn from experience itself.\nOne approach to IRL is to exploit state information by\nreconstructing rewards from state features and their combina-\ntions. To obtain such a non-linear function, this work employs\na neural network as its ﬁrst contribution. Speciﬁcation of\nnetwork structure can limit the efﬁciency of the generated\nnetwork. For this reason, the neural network is generated\nby cumulative modiﬁcation (using a Genetic Algorithm) to\na simple feed-forward network. The approach is summarized\nin Figure 1 with an overview in Section V-A.\nAs opposed to mapping states to rewards as in [3], this\nwork proposes generation of state values, from which state\nrewards can be derived. This improves robustness to noise,\nsince state values typically do not exhibit steep transitions.\nUse of state values therefore favours real-world applications\nsuch as robotics.\nThe use of neural networks also allows for inherent ad-\nvantages over regression trees [4], [5]. Unlike regression\ntrees (used in [3]), neural networks are capable of learning\nnon-linear data boundaries. They are able to generate more\nabstract features at hidden neurons. Finally, the fact that neural\nnetworks can approximate any function with sufﬁcient data\n(universal approximators) makes them intuitively preferable.\nFurther, when ﬁtting a highly non-uniform function, neural\nnetworks are better than kernel functions (used in Gaussian\nProcess (GP) regression [6]) at generalizing non-locally and\nscaling to larger datasets. This is because kernel functions\ntypically generalize locally.\nWork in [7] involves the use of composite state features with\npriors to estimate rewards over several algorithm iterations.\nThe second contribution of this work is utilizing the non-linear\nnature of neural networks to improve the performance of [7]\nin the case of complex reward structure. This is done by using\na neural network for reward estimation.\nSection II describes relevant work on neuroevolution and\nfeature based IRL. Section III discusses IRL and a feature\nbased approach to the problem. This is followed by details\nof the proposed neuroevolution based algorithm in Section\nIV. Section V on experimental evaluation of the algorithm\nis followed by the concluding remarks in Section VI.\narXiv:1608.02971v1  [cs.NE]  9 Aug 2016\n2\nFig. 1. NEAT-IRL summary. A population of neural networks is used to generate mappings from state features to state rewards. The networks are evolved\nin structure and connections using Genetic Algorithms (GA). Fitness of a network is determined by closeness of the optimal policy generated from rewards\nbased on that network, to the demonstrated policy.\nII. RELATED WORK\nFeature construction for Inverse Reinforcement Learning\n(FIRL) uses regression trees and quadratic programming [3].\nOptimization then involves selection of a sub-tree without sig-\nniﬁcant loss in regression ﬁtness. In [6], FIRL is acknowledged\nto have limited capabilites in representing a reward function\nthat uses a non-linear combination of features. Another such\ntechnique, called GPIRL, is based on Gaussian Process (GP)\nregression [6] and ﬁts the reward function into a GP kernel as\na non-linear combination of state features.\nRecent techniques have also incorporated a non-parametric\nBayesian framework to improve learning. This means that the\nnumber of parameters used by the models increases based on\nthe training data. Composite features in [7] are deﬁned as\nlogical conjunctions of state features. The IRL model extends\n[8] by deﬁning a prior on these composite features. In [9]\nand [10], the reward function is additionally assumed to be\ngenerated by a composition of sub-tasks to be performed in\nthe MDP space. This algorithm targets detection of sub-goals\nbut does not estimate the ﬁnal policy over all states in state\nspace (it only targets states observed in the demonstration).\nBayesian Non-Parametric FIRL (BNP-FIRL) [7] uses the\nIndian Buffet Process (IBP) [11] to deﬁne priors over compos-\nite state features. The IBP deﬁnes a distribution over inﬁnitely\nlarge (in number of columns) binary matrices. It is used to\ndetermine the number of composite features and the composite\nfeatures themselves. Features and corresponding weights are\nrecorded over several iterations of the algorithm. These values\nare then either aggregated (as a mean-based result) or used\nfor estimating Maximum A-Posteriori (MAP) [12] (as a MAP-\nbased result) state reward values. While the two results provide\ncompetitive performance when calculating reward functions,\nusing the mean-based result performs signiﬁcantly better than\nthe MAP-based result when focusing on action matching rather\nthan reward matching. For this reason, MAP-based results are\nexcluded from comparison. More importantly, this emphasizes\nthe importance of how the state reward values computed\nper iteration are ﬁnally used. A non-linear combination of\nthis data intuitively provides better performance than a linear\ncombination (as in the case of mean). Experimental results\nin [7] indicate superiority of BNP-FIRL over GPIRL in a\nnon-deterministic (a certain percentage of actions are random)\nMDP, such as the ones used to evaluate our work.\nIn an expectation-maximization based approach, the reward\nfunction is modeled as a weighted sum of functions in [13].\nParameters of the optimal policy in the model are deﬁned\nas the probability distribution of actions for each state in\nthe state space, based on the optimal policy. The algorithm\nthen attempts to simultaneously estimate the weights and\nparameters. The algorithm is not compared with [6], but the\ntwo algorithms have been individually compared with standard\nMaximum Entropy IRL. Visual observation of performance\nof these two algorithms indicates that a GP kernel based\napproach is competitive to, if not better than, an expectation-\nmaximization based approach.\nVery recently, the use of Deep Learning [14] for IRL\nproblems has been explored in [15] and [16]. The inputs to\nthe ﬁrst layer of the deep neural network are state features.\nThe performance of this algorithm has been shown to surpass\nthat of existing algorithms [6], [7]. The algorithm focuses on\nachieving correct expected state value, whereas our algorithm\nfocuses on learning the optimal policy. Intuitively, it is ex-\npected that our work will perform competitively with the use\nof deep neural networks. The reason for this is that the premise\nof both algorithms is similar: they use state features as input\nto a neural network and evaluate state reward or state value as\nthe output of the neural network. In addition, the use of neu-\nroevolution (evolving the structure of a neural network based\non task requirements) allows for a more compact network due\nto dynamic construction of the network. Comparison with this\ntechnique is therefore regarded as outside of the scope of this\nwork.\nFinally, work in [17] on Maximum Likelihood IRL\n(MLIRL) covers three problem spaces: linear IRL, non-linear\nIRL and multiple intentions IRL. Linearity and non-linearity\nis in the context of the reward function modeled as a function\n3\nof state features. Multiple intentions refers to an IRL setting\nwhere an MDP comprises of multiple reward functions. The\nalgorithm emphasizes that other IRL methods are not suitable\nfor a uniﬁed approach over all the mentioned problem spaces.\nHowever, it is noted that specialized IRL algorithms are more\nsuitable if the number of experts and the reward function shape\n(linear or non-linear) is known. Its performance against other\nIRL algorithms is competitive. Performance of MLIRL for our\nproblem setting is therefore evaluated as at most competitive\nwith [6] (evaluated in [17]). MLIRL is therefore excluded from\ncomparison for our work.\nOn the lines of FIRL, a neural network can provide a map-\nping from state features to state value. Since a neural network\ncan compactly represent complex combinations of inputs,\ninternal neurons may represent more informative features,\nas those obtained in FIRL. In an alternative implementation\nextending BNP-FIRL, a neural network is used to provide\nstate value based on feature and weight data gathered over\nseveral iterations of state reward estimation in the BNP-FIRL\nalgorithm.\nSince the function to be generated is unknown, so is its\ncomplexity. This implies uncertainty about the optimal number\nof layers and nodes in each layer to be used. Nodes in later\nlayers of a neural network may be able to deﬁne a function\nfor which it may take several nodes in the earlier layers of the\nneural network to deﬁne. There is therefore a trade-off between\nthe number of hidden layers and the number of nodes in each\nlayer. Because of this, using a ﬁxed structure for the neural\nnetwork in this scenario may be sub-optimal. Neuroevolution\nsolves this problem by generating the optimal neural network\nusing techniques such as Genetic Programming (GP) [18],\nEvolutionary Programming (EP) [19], Simulated Annealing\n(SA) [20], Genetic Algorithms (GA) [21], [22], Evolution\nStrategies (ES) [23], [24], Evolutionary Algorithms (EA) [25]\nand Memetic Algorithms (MA) [26].\nIn a direct encoding scheme, all neurons and connections\nin the neural network are explicitly speciﬁed by the genotype.\nIn case of an indirect encoding scheme, these values are\nexpressed implicitly by smaller parts of the genotype (encod-\ning of the neural network). Indirect encoding is suitable for\nsolving tasks which have a high degree of regularity (such as\ncontrolling the legs of a millipede like robot). Approximating\na function is a problem that lacks regularity. A well estab-\nlished direct encoding-based neuroevolution technique such\nas NeuroEvolution of Augmenting Topolgies (NEAT) [22],\n[27], which evolves both the structure and parameters of the\nneural network, is therefore preferred for our work.\nNEAT evolves a population of neural networks governed\nby a GA [28], [29], and therefore by a ﬁtness function. There\ncurrently exist many extensions of NEAT, including rtNEAT\n[21] (a real-time version of NEAT, which enforces perterbation\nto avoid stagnation of ﬁtness level) and FS-NEAT [30] (NEAT\ntailored to feature selection).\nIn [31], demonstration bias is introduced to NEAT-based\nagent learning. This is done by providing advice in the form\nof a rule-based grammar. This does not, however, provide\nan embedded undertanding of preference for a state in state\nspace. This is further explored by mapping states to actions\nin [32] where different methods of demonstration are studied\nin a video game environment [33]. NEAT-based IRL is also\nimplemented in a multiple agent setting as in [34] where\ngroups of genomes share ﬁtness information.\nState values are used to generate a corresponding policy. To\nincorporate learning by demonstration, this policy is matched\nwith the demonstration and the neural network is evolved\nthereof. Such directed neuroevolution can be considered as\nan extension of [31], [32] with better insight to evaluation of\na state. For the purpose of this document, the proposed work\nis referred to as NEAT-IRL.\nIII. PROBLEM DEFINITION\nA formal deﬁnition of an MDP [35] is repeated here for\nreference. An MDP is a set of states (S), actions (A) and\ntransition probabilities (θ) between states when an action is\ntaken in a state. Additionally, each state-action pair corre-\nsponds to a reward (R). A discount factor (γ) is used while\naggregating rewards corresponding to a trajectory of state-\naction pairs. A policy (π) describes a set of actions to be\ntaken over the state space. The optimal policy (π∗), then,\nmaximizes the expected discounted sum of rewards between\ntwo given states (start and goal) in an episodic task (which\nrepetitively solves the same problem). Alternatively, in the\ncase of a continual task (which does not have terminal\nstates), the optimal policy maximizes this sum of rewards\nover the lifetime of the learning agent. State value (v) is\nthe expected return (sum of discounted R values) when an\narbitrary π is followed, starting at that state. The concept\nof an MDP is extended to deﬁne a Linear MDP (LMDP): a\nlinearly solvable MDP (using KL-divergence [36] or maximum\nentropy control) [37]. An LMDP is deﬁned by state costs (q)\nin correspondence to S as an alternative to R [17]. Passive\ndynamics (p) describes transition probabilities in the absence\nof control. Following a policy (π) as opposed to p occurs at a\ncost of the KL divergence between π and p. Such a cost makes\nthe optimization problem convex, removing questions of local\noptima [38]. Additionally, an exponential transformation of\nthe optimal v function transforms the associated Bellaman\nEquation [39] (MDP solution) to a linear function. An optimal\nv function corresponds to a v function evaluated over π∗.\nThe goal of IRL is to learn a reward function for each\nstate based on parts of a given policy (a demonstration). In\na broader sense, the goal is to be able to generate a policy\nover a state space (S), which is correlated to what has been\ndemonstrated. For the purpose of this work, the demonstrations\nreceived by the algorithm are assumed to be performed by\nan expert, meaning that they are assumed to be optimal. A\ndemonstration (D) consists of numerous examples, each of\nwhich is a trace of the optimal policy through state space.\nThese are represented in the form of sequences of state-action\npairs (s, a).\nOne method to generate a policy is to generate state\nvalues for each state based on state features. It is assumed\nthat a weighted combination of state features can provide a\nquantitative evaluation of a state (with a motivation similar to\nFIRL). The ﬁrst problem, then, is to learn a mapping from\n4\nstate features to state values that produces a policy for which\nstate-action pairs are consistent with the given examples.\nAdditionally, values of weights and features over several\niterations of BNP-FIRL may be used in different ways to\nderive a state reward value. The second problem, then, is to\nlearn a non-linear mapping from these values to state reward,\nwhich produces a policy consistent with the given examples\n(as described for the ﬁrst problem).\nThe domain used for this work is a grid world Markov\nDecision Process (MDP), but the concept can be extended to\nother state spaces. The use of a grid world MDP is aligned with\nexperiments in [3], [9], [10], [17], [13]. In case of a grid world\nMDP, an agent has 5 possible actions: move up, move down,\nmove left, move right, do nothing. In case of a deterministic\nMDP, the action taken is always the action selected. However,\nin a non-deterministic MDP, the action taken is sometimes\nrandom, irrespective of the action selected.\nIV. PROPOSED METHOD\nExamples are provided as traces of subsequent states in the\nstate space as per the demonstration policy. Multiple traces\nmay overlap, which means that a single state may be covered\nmore than once in a set of examples. If no examples overlap,\nthe set of states involved in the examples has an upper bound\nof the sum the length of each example (in states). An example\nfrom an MDP policy perspective is given in Figure 2. These\nexamples serve as demonstration (D) in IRL (used for learning\nstate rewards).\nThe following sections provide an overview of fundamental\nalgorithms in the context of this work and then continue to\ndeﬁne an approach to NEAT-based IRL.\nA. BNP-FIRL\nBNP-FIRL is summarized in Figure 3. BNP-FIRL decom-\nposes reward (r) as a product of composite features (Φ) and\nweights (w). There are a total of K composite features and so\nthe size of w is K. Each column in Φ is a binary vector of\nsize |S|, indicating the presence or absence of the feature per\nstate. The size of Φ is therefore K × |S|. The product of w\nand Φ is thus a vector of size |S|.\nΨ denotes a matrix of M atomic features (the original state\nfeatures as deﬁned by the MDP). As for χ, each column in χ\ncontains |S| binary data items, making its size M × |S|.\nThe matrix Z indicates the atomic features that comprise\neach composite feature. It is therefore M × K in size. A\nBernoulli distributed binary matrix (with p = 0.5), U (also\nM × K), is used to negate atomic features when generating\ncomposite features. X is an M-dimensional binary vector in-\ndicating use of the atomic features to form composite features.\nα is a constant parameter over which a Poisson distribution\nis calculated (required by the IBP when computing the number\nand values of composite features). A Bernoulli distribution\n(p = κ) is used to generate priors on X. Finally, κ is Beta\ndistributed using the closed interval deﬁned by β.\nτ is a vector of N demonstrations that is assumed to be\ngenerated using the optimal policy for the MDP. The posterior\nprobability of τ given r and constant η is then computed. η\nFig. 2.\nPolicy sampling in a grid world MDP: 4 demonstrations of length\n2. The top part of the ﬁgure shows the optimal policy for the MDP. For\neach demonstration, a state is selected at random. The optimal policy is then\nfollowed for steps equal to demonstration length (2 in this case).\nFig. 3.\nBNP-FIRL summary [7]. The reward function r is computed as a\nproduct of composite features Φ and associated weights w. Values of Φ and\nw are selected to optimize the posterior probability of τ given r.\nis the parameter representing the conﬁdence of actions being\noptimal [7]. The algorithm iteratively converges to a value of\nr which maximizes this probability. The values of Φ and w\ncomputed in each iteration are stored in a vector. To compute\nmean-based results, the sum of rewards per iteration is used\nas the ﬁnal reward. The use of mean at the ﬁnal stage of the\nBNP-FIRL algorithm is denoted by BNP-FIRL(mean).\nB. NEAT\nGAs [28], [29] are biologically inspired population based\nstochastic search techniques. A population of genomes is\n5\nevolved in favour of optimizing a fitness function. The\nperformance of GA is primarily governed by two parameters:\npopulation size (NP ) and maximum number of generations\n(NG).\nAn Artiﬁcial Neural Network (ANN) or a Neural Network\n(NN) [40] is a function approximation model inspired by\nbiological neural networks. The model is composed of in-\nterconnected nodes or neurons which exchange information\nto produce the desired output. In our work, we use neural\nnetworks containing a single output node.\nNEAT evolves neural networks using GA, guided by a\nfitness function. Each member of the population cor-\nresponds to a genotype or genome and a phenotype (the\nactual neural network). A genome consists of node genes\n(listing input, output and hidden nodes) and connection genes\n(listing connections between nodes and associated weights).\nGenotype to phenotype mapping is summarized in Figure 4.\nNEAT begins with relatively less complex neural networks and\nthen increases complexity based on a fitness requirement.\nSpeciﬁcally, the initial network is perceptron-like and only\ncomprises of input and output neurons. The gene is evolved\nby either addition of a neuron into a connection path or by\ncreating a connection between existing neurons.\nC. NEAT-IRL\nThe result of NEAT-IRL is a neural network which can\nproduce state values based on state features.\nNeural networks represented by a genome population in\nNEAT are considered to use state features as input and\nproduce state value as output. A corresponding policy is\nevaluated for that state space. It is therefore suitable that the\nﬁtness function be the coherence between the generated and\ndemonstrated policies. In implementation, this is done by using\nthe coherence (c) of generated action directions (dG) with the\ndemonstration (dD) as ﬁtness value.\nc =\n\n\n\n1\nif dG = dD (in correct direction)\n−1\nif dG = −dD (opposite to the correct direction)\n0\notherwise (other wrong direction or no action)\n(1)\nThese values are intuitive to the cosine of the angle between\nthe generated and example actions. Fitness is computed as c\naccumulated over all states included in the demonstration. If\nall of the demonstrated actions are replicated correctly, the\nalgorithm is terminated. This is summarized in Figure 1.\nD. BNP-FIRL(NEAT)\nThe use of NEAT is similar to NEAT-IRL where the ﬁtness\nvalue is based on matching examples in the demonstration.\nHowever, the inputs are no longer state features.\nThe dimensions of Φ and thus w vary across iterations\nbecause of variation in composite features considered. A non-\nlinear combination of Φ or w over time is therefore not\npossible. However, the size of r is constant for each iteration.\nThe set of values of r over iterations of the algorithm then\nserves as input to the neural network. The output of the neural\nnetwork is used as the resultant state reward vector. This\nreward vector is used to compute the optimal policy, which\ncan then be used to compute ﬁtness values.\nE. FIRL and GPIRL vs NEAT-IRL\nFIRL, GPIRL and NEAT-IRL all consider individual states\nas opposed to traces of state sequences, making them memory\nefﬁcient in terms of the storage of examples. However, FIRL\nand GPIRL generate a function which produces state rewards,\nas opposed to NEAT-IRL which produces state values. State\nrewards evaluate immediate desirability whereas state values\nevaluate long term desirability of a state. An agent therefore\nseeks actions that lead to states of higher value and not highest\nreward to maximize long term reward [41]. State values also\nobserve smoother transition amongst states in close proxim-\nity, though this is not necessary for state rewards. Further,\nconverting a set of state values to a policy requires a single\ncomputational step (value based greedy action selection). In\ncase of state rewards, however, the policy is accumulated over\nseveral epochs of computation, thereby resulting in greater\ntime complexity. Additionally, a policy generated using state\nvalues is more robust to noise (in context of real-world\napplications) than one generated based on state rewards. The\nreason is that in case of state values, policy evaluation only\nrequires a comparison of adjacent state values. The ﬁnal\npolicy is therefore only affected if the state values are close\nenough for noise to change action selection. In case of policy\ngeneration using state rewards, noise would be accumulated\nover each epoch of computation and would therefore have a\nmore signiﬁcant effect on the generated policy. A disadvantage\nof generating state values is that they transfer poorly to other\nMDPs with similar feature sets as opposed to state rewards\n[17]. However, our problem space does not concern transfer\nlearning.\nAdditionally, FIRL involves convex optimization (minimiza-\ntion) and the use of regression trees, whereas NEAT-IRL is\nbuilt on neural networks.\n[42] shows that GP behaviour can be replicated by a multi\nlayer perceptron neural network with a sufﬁciently (tending to\ninﬁnity) large number of hidden neurons. This stands with a\nrequirement to use weight decay [43], [44]. In practice, this\nimplies limitation of neural networks for approximation of\nGP behaviour instead of exactness. Further, GP models can\nbe optimized to ﬁt data exactly with speciﬁc hyper-parameter\nvalues [45]. This implies a trade-off between exactness and\nover-ﬁtting data.\nNEAT-IRL does, however, introduce two new parameters\n(NP and NG) to the IRL problem, which increases degrees\nof freedom. The performance of a ﬁxed set of parameters will\nvary in different environments. Algorithm performance may\ntherefore need to be evaluated across these parameters for\noptimal value assignments.\nF. BNP-FIRL(mean) vs BNP-FIRL(NEAT)\nThe set of functions deﬁned by a linear combination of\nvariables is a subset of the set of functions deﬁned by a non-\nlinear combination of those variables. Non-linear combinations\nare therefore more powerful in expressing relationships among\n6\nFig. 4.\nGenotype to phenotype mapping example for NEAT [22]. A genotype corresponds to the encoding of the structure (nodes) and connections of a\nneural network. These are represented as two genes. The phenotype is the actual neural network generated based on its phenotype.\nvariables and direct towards better function approximations at\nthe cost of function complexity. In the case of the mean-based\nresult of BNP-FIRL in particular, the linear combination is\nsimply a sum of variables. This allows for signiﬁcant scope\nfor improvement in approximation of the ﬁnal value of state\nrewards.\nSimilar to NEAT-IRL, the use of NEAT with BNP-FIRL\nincreases the number of algorithm parameters and may re-\nquire an additional level of optimization for better algorithm\nperformance.\nV. EXPERIMENTS\nOriginally conceptualized in a Python implementation based\non MultiNEAT [46], NEAT-IRL is currently implemented in\nMATLAB using [47] and is evaluated using existing tools\nin the IRL toolkit containing FIRL and GPIRL [48]. The\nimplementation of BNP-FIRL exists in an extended version\nof the toolkit [49]. It is used with [48] for the experiments in\nthis work. Inherent to the toolkit, there exist 2×(n−1) binary\nstate features for a grid of size n. These bit patterns contain\nsub patterns which are consistent for a row or column in the\ngrid, thereby forming a coordinate system. This is exempliﬁed\nin Figure 5. Additionally, state rewards are assigned randomly\nfor each macro block. The algorithms are evaluated over\nmany such randomly generated grid worlds and are compared\non how well they estimate the randomly generated reward\nstructure.\nFIRL and GPIRL consistently produce better results than\nthe other IRL algorithms they are compared to in [3], [6].\nThe case is similar with BNP-FIRL(mean) [7]. Additionally,\nGPIRL performs consistently better than FIRL. It is therefore\nsufﬁcient to evaluate GPIRL, NEAT-IRL, BNP-FIRL(mean)\nand BNP-FIRL(NEAT) when examining performance im-\nprovements. NEAT-IRL is not compared with the work done\nin [31], [32] in sight of a dependency on rule based learning.\nThe algorithms are evaluated in a number of ways. NEAT-\nIRL is evaluated individually and also in comparison to\nGPIRL, BNP-FIRL(mean) and BNP-FIRL(NEAT). The IRL\ntoolkit [48] deﬁnes misprediction score as the probability that\nthe agent will take a non-optimal action (different from what\nwould have been in an example) in a state. This is measured\nbased on matching the expert policy (used for demonstration)\nand the policy generated by the IRL algorithm. These scores\nare evaluated for both linear and standard MDPs. With 5 pos-\nsible actions at any state in a grid world, default misprediction\nscore is 0.8. Macroblock size, b, which speciﬁes the number of\nadjacent cells in a grid to be assigned the same reward value, is\nset to 1 for all experiments. This is done so that state features\ncorrespond to unique rewards. Average values are computed\nover 25 executions. Furthermore, NEAT-IRL may end execu-\ntion early when a generated policy completely matched with\ndemonstrated examples. This may, however, lead to under-\nﬁtting, which is also a possible contributor to hindering the\nperformance of NEAT-IRL in terms of misprediction score.\nNote that the default values used for NEAT in [47] are\nNP = 150 and NG = 200. Additionally, GPIRL is evaluated\nin [48] using a default grid size (n) of 32, i.e. a 32 × 32 grid\nwith 16 training samples (NS) of length (LS) 8 each. Reduced\nvalues (scaled in proportion) are used for evaluation of NEAT-\nIRL and BNP-FIRL(NEAT) for computational tractability.\nPrimary results discussed in this work use the conﬁguration\nn = 16, NS = 8 and LS = 4. Certain supplementary\nevaluations use the conﬁguration n = 4, NS = 4 and LS = 1.\nThe number of samples in this case is more than the scaled\nvalue (NS = 2) to avoid inference based on little data. In doing\nso, the demonstrations include 25% of the total states, which\nis justiﬁed by considering the proportions used in the original\nsetting (n = 32). The primary grid allows for standardized\nevaluation of the algorithms, whereas a smaller grid allows\nmore tractable analysis of the MDPs over which the algorithms\nare evaluated. The remaining NEAT conﬁguration including\nGA parameters are as used in [47]. GPIRL parameters are\nused as default in [48]. Note that for smaller data samples,\ninterpolation used by GPIRL exceeded the possible number\nof points, thereby causing mathematical errors. This was ﬁxed\nby limiting interpolation to the maximum possible, in case the\nnumber of interpolations was more than what was possible.\nIn the misprediction score graphs plotted, a solid line is used\n7\nFig. 5. State features for a gird world MDP (n = 3). There are 2×(3−1) = 4\nfeatures per state. Features have binary values and demonstrate patterns across\nrows and columns.\nto denote performance data on a standard MDP and a dotted\nline is used to denote performance data on a linear MDP. Addi-\ntionally, the term computational complexity is interchangeably\nused with computational time complexity and execution time\nis calculated in seconds. Additionally, misprediction score is\nobserved to be lower when testing on a linear MDP than on a\nstandard MDP irrespective of which of these two MDP types\nwas used for training the algorithm.\nA. NEAT-IRL\nEvaluation begins with testing NEAT-IRL parameters NP\nand NG. This is done to evaluate the effect of NEAT param-\neters on algorithm performance and complexity.\nAs expected in the use of GAs, a larger population of\ngenomes provides better performance. Note that beyond a\nthreshold value, an increase in population does not contribute\nto signiﬁcant optimization. This is because the capacity of\nsearch that can be performed by the extra (beyond threshold)\npopulation of genomes is limited by the size of the current state\nspace. Execution time for a linear implementation of NEAT-\nIRL is linear with population size.\nCoherent to GA behaviour, an increase in the number of\nmaximum generations results in lower misprediction score.\nThis is subject to stagnation in a manner similar to that\ndiscussed for population size. NEAT-IRL execution time is\nlinear with the maximum number of generations as expected,\nsince computation is constant per generation of the algorithm.\nB. GPIRL and BNP-FIRL(mean) vs NEAT-IRL and BNP-\nFIRL(NEAT)\nGPIRL,\nBNP-FIRL(mean),\nNEAT-IRL\nand\nBNP-\nFIRL(NEAT) are compared across three aspects of the\nMDP: the MDP type (standard or linear), the amount of\ndeterminism in the MDP (d, where a value of 1.0 represents\ncomplete determinism) and different values of NS. This\ntests the ability of the algorithms to reconstruct the reward\nfunction across different amounts of data. For each of these\nevaluations, NEAT parameters are arbitrarily set as NP = 50\nand NG = 50.\nDependency on NS is evaluated by varying NS from 1 to 8.\nTo evaluate dependency on d, performance of the algorithms in\ntwo settings (d = 0.7 as in [7] and d = 1.0) are tested. Both of\nthese settings are tested on standard and linear MDPs. These\nFig. 6.\nNumber of samples evaluation (standard MDP, d = 0.7). The\nmisprediction performance of BNP-FIRL(NEAT) is competitive with that of\nGPIRL and BNP-FIRL(mean). Time complexity is least for GPIRL, with that\nof BNP-FIRL(NEAT) being competitive with that of BNP-FIRL(mean).\nexperiments are depicted in Figure 6, Figure 7, Figure 8 and\nFigure 9.\nIn context of misprediction score, the performance of al-\ngorithms using neural networks is more competitive with the\ncompared algorithms in a non-deterministic setting than in a\ndeterministic setting. This favours use of the algorithm in a real\nworld setting where non-determinism exists because of various\nsources of noise. It is also the case that better performance\nof neural networks is observed for a linear MDP than for a\nstandard MDP. This is attributed to the neural network being\nable to perform better on a more easily solvable MDP given\na set of parameters. Additionally, the composite features and\nthus reward function over iterations provide better performance\nof neural networks in BNP-FIRL(NEAT) as compared to\nthe use of state features in NEAT-IRL. As in [7], BNP-\nFIRL(mean) is observed to be consistently better than GPIRL.\nPerformance of NEAT-IRL improves at a slower pace than\nthe other algorithms. This is attributed to the values of NP =\n50 and NG = 50 being unoptimized for the MDPs being used.\nIt is demonstrated in section V-A that these values are tunable\nto improve the performance of the algorithm.\nExecution time is least for GPIRL in all four experimental\nsettings. In a trade-off with better performance, an increased\nexecution time is observed for BNP-FIRL(mean). This in-\ncrease is more signiﬁcant in the case of d = 0.7. This implies\nthat a linear MDP is a harder problem for NPB-FIRL(mean)\nto solve. The additional layer of NEAT at the ﬁnal stage of\n8\nFig. 7.\nNumber of samples evaluation (standard MDP, d = 1.0). GPIRL\noutperforms BNP-FIRL(NEAT), BNP-FIRL(mean) and NEAT-IRL on mis-\nprediction score. The time complexity for NEAT-IRL is signiﬁcantly greater\nthan that of the other IRL methods.\nBNP-FIRL does not introduce noticeable increase in execution\ntime. Note that this is also because of the scale of values used,\nwhich results in lower resolution between the two graphs.\nExecution time for NEAT-IRL remains largest across all of\nthe experimental settings. In the presence of less demonstra-\ntion data, the algorithm may often match all examples and\ndiscontinue evolving the network further. As the number of\nexamples increases, ﬁtting becomes more difﬁcult and causes\nan increase in the number of generations. This explains the\ngradual increase in time complexity. Since there is a limit on\nthe maximum number of generations that may be executed, the\ntime complexity later stagnates. BNP-FIRL internally uses a\nmatrix multiplication based technique to solve the MDP given\nstate calculated rewards. However, NEAT-IRL processes each\nstate based on state values of neighbouring state values. It\nis therefore possible that the additional conditional sequences\nresult in signiﬁcantly increased time complexity for NEAT-\nIRL as opposed to BNP-FIRL(NEAT). Perhaps, then, if we\nuse neural networks to generate state reward instead of state\nvalue, we could integrate with the MDP solution method\nused by BNP-FIRL and reduce time complexity. However,\nmapping from state features to rewards is argued to decrease\nperformance as compared to mapping from state features to\nstate values (as discussed in Section IV-E).\nFrom a performance improvement perspective, Figure 8\nexamines a setting where the use of neural networks provides\ncompetitive performance to the other algorithms in our exam-\nination set. However, a smoother graph is required for better\nFig. 8. Number of samples evaluation (linear MDP, d = 0.7). The collective\nmisprediction performances of BNP-FIRL(NEAT) and BNP-FIRL(mean) are\nbetter than better than those of GPIRL and NEAT-IRL. Time complexity is\nleast for GPIRL, with that of BNP-FIRL(NEAT) being competitive with that\nof BNP-FIRL(mean).\ncomparison. For this reason, an average is considered over 100\nexecutions. The results are shown in Figure 10. Execution time\ndata is similar on average to that in Figure 8 and is therefore\nomitted. The ﬁgure establishes that the performance of BNP-\nFIRL(NEAT) is competitive to that of BNP-FIRL(mean), both\nof which are better than GPIRL and NEAT-IRL. The ﬁgure\nalso establishes that in this setting, the use of neural networks\nalone (NEAT-IRL) outperforms GPIRL. The experiment in\nFigure 8 is also extended to observe performance in the\npresence of 9 −16 samples (calculated over 25 executions).\nThis is shown in Figure 11. While the trends between GPIRL,\nBNP-FIRL(mean) and BNP-FIRL(NEAT) continue, limitation\nin performance of NEAT-IRL attributed to NEAT parameter\nsettings is observed.\nTo compare BNP-FIRL(mean) and BNP-FIRL(NEAT) al-\ngorithms, the use of neural networks (BNP-FIRL(NEAT)) is\ntested for its better adaptability to non linear boundaries as\ncompared to BNP-FIRL(mean). The algorithms are evaluated\nover various non-deterministic (d = 0.7) linear MDPs (varied\nby random seed initialization) to observe algorithm perfor-\nmance over different optimal policies for different MDPs.\nOverall, the performance of the two algorithms is indistin-\nguishable. However, a closer look at speciﬁc seed values\nshows that BNP-FIRL(NEAT) performs noticeably better than\nNPBFIRL(mean) for certain seed values. The policies for the\nMDPs for these situations are shown in Figure 12 and Figure\n13.\n9\nFig. 9.\nNumber of samples evaluation (linear MDP, d = 1.0). BNP-\nFIRL(mean) outperforms BNP-FIRL(NEAT), GPIRL and NEAT-IRL on mis-\nprediction score. The time complexity for NEAT-IRL is signiﬁcantly greater\nthan that of the other IRL methods.\nFig. 10.\nNumber of samples evaluation (linear MDP, d\n=\n0.7, 100\nexecutions). BNP-FIRL(mean) and BNP-FIRL(NEAT) collectively perform\nbetter than GPIRL and NEAT-IRL. An average over 100 iterations asserts the\ncompetitive performances of FIRL(mean) and BNP-FIRL(NEAT).\nFrom MDPs corresponding to these seed values, it is unclear\non whether the number of goal states discriminates perfor-\nmance between the two algorithms. It is then hypothesized that\nBNP-FIRL(NEAT) performs better than BNP-FIRL(mean) in\nthe presence of multiple goal states. To evaluate this, two\nMDPs (n = 4) are manually constructed with goals randomly\nplaced in the MDP. This is done by associating an arbitrary\nreward value of 100 to those states. The differences in perfor-\nmances of the two algorithms are also compared for signiﬁ-\ncance using two tailed t-test. The results are summarized in\nFig. 11.\nNumber of samples evaluation (linear MDP, d = 0.7, 9 −16\nsamples). The performance of BNP-FIRL(NEAT) remains competitive with\nBNP-FIRL(mean). These two algorithms continue to perform better than\nGPIRL and NEAT-IRL even with increased number of training samples.\nTABLE I\nPERFORMANCE OF BNP-FIRL(MEAN) AND BNP-FIRL(NEAT) ON\nCONSTRUCTED MDPS. BNP-FIRL(MEAN) PERFORMS BETTER IN THE\nPRESENCE OF LESS GOAL STATES. AS THE NUMBER OF FOAL STATES\nINCREASES AND THE REWARD FUNCTION TO BE LEARNED BECOMES\nMORE COMPLEX, BNP-FIRL(NEAT) BEGINS TO SIGNIFICANTLY\nOUTPERFORM BNP-FIRL(MEAN).\n# Goals\nBNPmean\nBNPNEAT\np-value\n1\n0.2308\n0.2545\n3.8837 e-4\n2\n0.3292\n0.3392\n0.1870\n3\n0.4119\n0.3913\n0.0063\n4\n0.4954\n0.4674\n3.9776 e-5\nTable I. MBNP −F IRL(mean) and MBNP −F IRL(NEAT ) repre-\nsent misprediction scores corresponding to BNP-FIRL(mean)\nand BNP-FIRL(NEAT) respectively. To overcome the ﬂuctu-\nation of numbers for averages over smaller number of runs\n(such as 25), the results are averaged over 1000 runs.\nIn the presence of a single goal state, BNP-FIRL(mean)\nsigniﬁcantly outperforms BNP-FIRL(NEAT). However, as the\nnumber of goals increases, BNP-FIRL(NEAT) eventually com-\npetes and later signiﬁcantly outperforms BNP-FIRL(mean).\nFrom an MDP perspective, increasing the number of goals\nresults in a more complex policy. This is because the state\nreward and hence state value surface has more than one\noptima. This is visually exempliﬁed in Figure 14 and Figure\n15.\nThe hypothesis is then tested on a n = 16 scale (with\nNS = 16, LS = 4 for correspondingly scaled input samples).\n4 goals with reward value 100 are placed at each corner of\nthe grid. In an average over 100 executions, BNP-FIRL(NEAT)\nresults in a signiﬁcantly (p−value = 4.1819e−06) lower mis-\nprediction score (0.2672) as compared to BNP-FIRL(mean)\n(0.3072). The hypothesis that the use of a neural network\nallows to learn more complicated reward structures is therefore\nconﬁrmed.\nA conclusive performance hierarchy between these algo-\nrithms in a non-deterministic (d = 0.7) linear MDP exper-\nimental setting is then established as BNP-FIRL(NEAT) >\nBNP-FIRL(mean) > NEAT-IRL > GPIRL.\n10\nFig. 12.\nMDP solutions (seeds 7, 15). The level of complexity of optimal\npolicies is different for the two seed values. It is unclear whether the number\nof goal states (and therefore complexity of optimal policy) depicts a situation\nwhere BNP-FIRL(NEAT) would outperform BNP-FIRL(mean).\nVI. CONCLUSION\nThese experiments conclude that algorithms using NEAT\nperform better on a non-deterministic linear MDP than BNP-\nFIRL(mean) and GPIRL (as in Figure 10). This is useful\nconsidering that real world MDPs contain uncertainty in action\ncaused by various sources of noise.\nGiven the competitive performance of BNP-FIRL(NEAT)\nand\nBNP-FIRL(mean),\nhospitable\nMDPs\nfor\nBNP-\nFIRL(NEAT) are then evaluated and experiments highlight\nthe possibility of MDPs with multiple goals being favourable\n(as\nin\nFigure\n12\nand\nFigure\n13).\nThis\nhypothesis\nis\nexamined and superior performance of BNP-FIRL(NEAT)\nis conﬁrmed in cases of MDPs containing multiple goal\nstates.\nBNP-FIRL(NEAT)\ncan\nto\nbetter\nestimate\nmore\ncomplex reward structure (as in Table I). A corresponding\nhierarchy of evaluated algorithms is then established with\nBNP-FIRL(NEAT) ranked the highest.\nAdditionally, NEAT parameters can be tuned to improve\nperformance and time complexity for a given set of examples.\nThe current implementation of NEAT-IRL is also capable of\ngreater time efﬁciency. Computations speciﬁc to each genome\nin a population can be parallelized. Further, NEAT-IRL policy\nprediction is currently done for all states. This can be limited to\nFig. 13.\nMDP solutions (seeds 24, 25). The level complexity of optimal\npolicies is low for both of the seed values. It is therefore unclear what part\nof the MDP impacts the performance of BNP-FIRL(mean) as opposed to\nBNP-FIRL(NEAT).\nonly demonstrated states, since that is what determines ﬁtness.\nIn future work, BNP-FIRL(NEAT) may be integrated to\nmultiple agent settings and may also be extended to incor-\nporate a cost of sharing information [34].\nREFERENCES\n[1] P. Abbeel, “Inverse reinforcement learning,” http://www.cs.berkeley.edu/\n∼pabbeel/cs287-fa12/slides/inverseRL.pdf, 2012.\n[2] A. Y. Ng, S. J. Russell et al., “Algorithms for inverse reinforcement\nlearning.” in Icml, 2000, pp. 663–670.\n[3] S. Levine, Z. Popovic, and V. Koltun, “Feature construction for inverse\nreinforcement learning,” in Advances in Neural Information Processing\nSystems, 2010, pp. 1342–1350.\n[4] R. Caruana and A. Niculescu-Mizil, “An empirical comparison of\nsupervised learning algorithms,” in Proceedings of the 23rd international\nconference on Machine learning.\nACM, 2006, pp. 161–168.\n[5] Y. Bengio, “Learning deep architectures for ai,” Foundations and\ntrends R⃝in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.\n[6] S. Levine, Z. Popovic, and V. Koltun, “Nonlinear inverse reinforcement\nlearning with gaussian processes,” in Advances in Neural Information\nProcessing Systems, 2011, pp. 19–27.\n[7] J. Choi and K.-E. Kim, “Bayesian nonparametric feature construction\nfor inverse reinforcement learning,” in Proceedings of the Twenty-Third\ninternational joint conference on Artiﬁcial Intelligence.\nAAAI Press,\n2013, pp. 1287–1293.\n[8] D. Ramachandran and E. Amir, “Bayesian inverse reinforcement learn-\ning,” Urbana, vol. 51, p. 61801, 2007.\n11\nFig. 14. Example MDP solutions (1 goal, 2 goals). In the presence of less\ngoals, the optimal policy shows smooth direction towards the goal(s). This\ncan be encoded using a simple (more linear) reward function as opposed to a\nmore non-linear function required in the presence of a more complex policy\n(as in Figure 15).\n[9] B. Michini and J. P. How, “Bayesian nonparametric inverse reinforce-\nment learning,” in Machine Learning and Knowledge Discovery in\nDatabases.\nSpringer, 2012, pp. 148–163.\n[10] B.\nMichini,\nT.\nJ.\nWalsh,\nA.-A.\nAgha-Mohammadi,\nand\nJ.\nP.\nHow, “Bayesian nonparametric reward learning from demonstration,”\nRobotics, IEEE Transactions on, vol. 31, no. 2, pp. 369–386, 2015.\n[11] Z. Ghahramani and T. L. Grifﬁths, “Inﬁnite latent feature models and\nthe indian buffet process,” in Advances in neural information processing\nsystems, 2005, pp. 475–482.\n[12] J. Choi and K.-E. Kim, “Map inference for bayesian inverse reinforce-\nment learning,” in Advances in Neural Information Processing Systems,\n2011, pp. 1989–1997.\n[13] J. Hahn and A. M. Zoubir, “Inverse reinforcement learning using\nexpectation maximization in mixture models,” in Acoustics, Speech and\nSignal Processing (ICASSP), 2015 IEEE International Conference on.\nIEEE, 2015, pp. 3721–3725.\n[14] L. Deng and D. Yu, “Deep learning: methods and applications,” Foun-\ndations and Trends in Signal Processing, vol. 7, no. 3–4, pp. 197–387,\n2014.\n[15] M. Wulfmeier, P. Ondruska, and I. Posner, “Deep inverse reinforcement\nlearning,” arXiv preprint arXiv:1507.04888, 2015.\n[16] M. hew Alger, “Deep inverse reinforcement learning.”\n[17] M. C. Vroman, “Maximum likelihood inverse reinforcement learning,”\nPh.D. dissertation, Rutgers University-Graduate School-New Brunswick,\n2014.\n[18] F. Gruau et al., “Neural network synthesis using cellular encoding and\nthe genetic algorithm.” 1994.\n[19] P. J. Angeline, G. M. Saunders, and J. B. Pollack, “An evolutionary\nalgorithm that constructs recurrent neural networks,” Neural Networks,\nIEEE Transactions on, vol. 5, no. 1, pp. 54–65, 1994.\nFig. 15. Example MDP solutions (3 goal, 4 goals). With an increase in the\nnumber of goals, the optimal policy becomes more complex than it is for\nless goals (as in Figure 14. The estimated reward function is therefore also\nexpected to be more complex.\n[20] X. Yao and Y. Liu, “A new evolutionary system for evolving artiﬁcial\nneural networks,” Neural Networks, IEEE Transactions on, vol. 8, no. 3,\npp. 694–713, 1997.\n[21] K. O. Stanley, B. D. Bryant, and R. Miikkulainen, “Real-time neu-\nroevolution in the nero video game,” Evolutionary Computation, IEEE\nTransactions on, vol. 9, no. 6, pp. 653–668, 2005.\n[22] K. O. Stanley and R. Miikkulainen, “Evolving neural networks through\naugmenting topologies,” Evolutionary computation, vol. 10, no. 2, pp.\n99–127, 2002.\n[23] Y. Kassahun and G. Sommer, “Efﬁcient reinforcement learning through\nevolutionary acquisition of neural topologies.” in ESANN, 2005, pp. 259–\n266.\n[24] N. T. Siebel and G. Sommer, “Evolutionary reinforcement learning of\nartiﬁcial neural networks,” International Journal of Hybrid Intelligent\nSystems, vol. 4, no. 3, pp. 171–183, 2007.\n[25] C. W. Rempis, “Evolving complex neuro-controllers with interactively\nconstrained neuro-evolution,” 2012.\n[26] G. I. Sher, Handbook of neuroevolution through Erlang.\nSpringer\nScience & Business Media, 2012.\n[27] K. Stanley, “The neuroevolution of augmenting topologies (neat) users\npage,” http://www.cs.ucf.edu/∼kstanley/neat.html, 2014.\n[28] J. R. Koza, Genetic programming: on the programming of computers by\nmeans of natural selection.\nMIT press, 1992, vol. 1.\n[29] W. Banzhaf, P. Nordin, R. E. Keller, and F. D. Francone, Genetic\nprogramming: an introduction. Morgan Kaufmann San Francisco, 1998,\nvol. 1.\n[30] S. Whiteson, P. Stone, K. O. Stanley, R. Miikkulainen, and N. Kohl,\n“Automatic feature selection in neuroevolution,” in Proceedings of the\n7th annual conference on Genetic and evolutionary computation. ACM,\n2005, pp. 1225–1232.\n12\n[31] C. H. Yong, K. O. Stanley, R. Miikkulainen, and I. Karpov, “Incorpo-\nrating advice into neuroevolution of adaptive agents.” in AIIDE, 2006,\npp. 98–104.\n[32] I. V. Karpov, V. K. Valsalam, and R. Miikkulainen, “Human-assisted\nneuroevolution through shaping, advice and examples,” in Proceedings\nof the 13th annual conference on Genetic and evolutionary computation.\nACM, 2011, pp. 371–378.\n[33] K. O. Stanley, B. D. Bryant, and R. Miikkulainen, “Evolving neural\nnetwork agents in the nero video game,” Proceedings of the IEEE, pp.\n182–189, 2005.\n[34] R. Miikkulainen, E. Feasley, L. Johnson, I. Karpov, P. Rajagopalan,\nA. Rawal, and W. Tansey, “Multiagent learning through neuroevolution,”\nin Advances in Computational Intelligence.\nSpringer, 2012, pp. 24–46.\n[35] M. L. Puterman, Markov decision processes: discrete stochastic dynamic\nprogramming.\nJohn Wiley & Sons, 2014.\n[36] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” The\nannals of mathematical statistics, pp. 79–86, 1951.\n[37] B. D. Ziebart, “Modeling purposeful adaptive behavior with the principle\nof maximum causal entropy,” 2010.\n[38] E. Todorov, “Linearly-solvable markov decision problems,” in Advances\nin neural information processing systems, 2006, pp. 1369–1376.\n[39] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press Cambridge, 1998, vol. 1, no. 1.\n[40] S. Haykin and N. Network, “A comprehensive foundation,” Neural\nNetworks, vol. 2, no. 2004, 2004.\n[41] A. G. Barto, Reinforcement learning: An introduction. MIT press, 1998.\n[42] M. Lilley and M. Frean, “Neural networks: a replacement for gaussian\nprocesses?” in Intelligent Data Engineering and Automated Learning-\nIDEAL 2005.\nSpringer, 2005, pp. 195–202.\n[43] R. M. Neal, “Priors for inﬁnite networks,” in Bayesian Learning for\nNeural Networks.\nSpringer, 1996, pp. 29–53.\n[44] R. M. Neal, “Bayesian learning for neural networks,” Ph.D. dissertation,\nUniversity of Toronto, 1995.\n[45] C. E. Rasmussen, “Gaussian processes for machine learning,” 2006.\n[46] P. Chervenski, “Multineat,” http://multineat.com/, 2012.\n[47] C. Mayr, “Matlab neat,” http://nn.cs.utexas.edu/?neatmatlab, 2003.\n[48] S. Levine, Z. Popovic, and V. Koltun, “Irl toolkit,” http://graphics.\nstanford.edu/projects/gpirl/irl toolkit.zip, 2011.\n[49] J. Choi and K.-E. Kim, “Irl toolkit,” http://ailab.kaist.ac.kr/codes/\nbayesian-nonparametric-feature-construction-for-irl, 2013.\n",
  "categories": [
    "cs.NE",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2016-08-09",
  "updated": "2016-08-09"
}