{
  "id": "http://arxiv.org/abs/1608.03714v2",
  "title": "Unsupervised feature learning from finite data by message passing: discontinuous versus continuous phase transition",
  "authors": [
    "Haiping Huang",
    "Taro Toyoizumi"
  ],
  "abstract": "Unsupervised neural network learning extracts hidden features from unlabeled\ntraining data. This is used as a pretraining step for further supervised\nlearning in deep networks. Hence, understanding unsupervised learning is of\nfundamental importance. Here, we study the unsupervised learning from a finite\nnumber of data, based on the restricted Boltzmann machine learning. Our study\ninspires an efficient message passing algorithm to infer the hidden feature,\nand estimate the entropy of candidate features consistent with the data. Our\nanalysis reveals that the learning requires only a few data if the feature is\nsalient and extensively many if the feature is weak. Moreover, the entropy of\ncandidate features monotonically decreases with data size and becomes negative\n(i.e., entropy crisis) before the message passing becomes unstable, suggesting\na discontinuous phase transition. In terms of convergence time of the message\npassing algorithm, the unsupervised learning exhibits an easy-hard-easy\nphenomenon as the training data size increases. All these properties are\nreproduced in an approximate Hopfield model, with an exception that the entropy\ncrisis is absent, and only continuous phase transition is observed. This key\ndifference is also confirmed in a handwritten digits dataset. This study\ndeepens our understanding of unsupervised learning from a finite number of\ndata, and may provide insights into its role in training deep networks.",
  "text": "arXiv:1608.03714v2  [cond-mat.dis-nn]  11 Nov 2016\nUnsupervised feature learning from ﬁnite data by message passing: discontinuous\nversus continuous phase transition\nHaiping Huang and Taro Toyoizumi\nRIKEN Brain Science Institute, Wako-shi, Saitama 351-0198, Japan\n(Dated: November 14, 2016)\nUnsupervised neural network learning extracts hidden features from unlabeled training data. This\nis used as a pretraining step for further supervised learning in deep networks. Hence, understanding\nunsupervised learning is of fundamental importance. Here, we study the unsupervised learning from\na ﬁnite number of data, based on the restricted Boltzmann machine learning. Our study inspires\nan eﬃcient message passing algorithm to infer the hidden feature, and estimate the entropy of\ncandidate features consistent with the data. Our analysis reveals that the learning requires only a\nfew data if the feature is salient and extensively many if the feature is weak. Moreover, the entropy\nof candidate features monotonically decreases with data size and becomes negative (i.e., entropy\ncrisis) before the message passing becomes unstable, suggesting a discontinuous phase transition. In\nterms of convergence time of the message passing algorithm, the unsupervised learning exhibits an\neasy-hard-easy phenomenon as the training data size increases. All these properties are reproduced\nin an approximate Hopﬁeld model, with an exception that the entropy crisis is absent, and only\ncontinuous phase transition is observed. This key diﬀerence is also conﬁrmed in a handwritten digits\ndataset. This study deepens our understanding of unsupervised learning from a ﬁnite number of\ndata, and may provide insights into its role in training deep networks.\nPACS numbers: 02.50.Tt, 87.19.L-, 75.10.Nr\nHumans and other animals can learn new concepts\nfrom only a handful of training data but standard ma-\nchine learning algorithms require huge data to uncover\nhidden features [1].\nLearning hidden features in unla-\nbeled data is called unsupervised learning. How many\ndata are required to learn a feature? What key factors\ndetermine the success of unsupervised learning? These\nfundamental questions are largely unsolved so far, and\nrarely explained by a physics model. Understanding how\ndata size conﬁnes learning process is a topic of interest\nnot only in machine learning [2] but also in cognitive\nneuroscience [3]. The underlying neural mechanism or\ninspired algorithms are still elusive, but recent progress\nin mean ﬁeld theory of restricted Boltzmann machine [4]\nallows us to develop a statistical mechanics model to un-\nderstand how learning improves with data size.\nRestricted Boltzmann machine (RBM) is a basic block\nwidely used in building a deep belief network [5, 6]. It\nconsists of two layers of neurons. The visible layer re-\nceives data, and based on this, the hidden layer builds\nan internal representation. No lateral connections exist\nwithin each layer for computational eﬃciency. The sym-\nmetric connections between visible and hidden neurons\nrepresent hidden features in the data that the network\nlearns.\nA common strategy to train a RBM is to ap-\nply a gradient-descent update algorithm of the connec-\ntions based on stochastically sampled neural activities [6].\nHowever, no theory was proposed to address how learn-\ning improves and how the number of candidate features\ndecreases with data size.\nHere we tackle this problem by taking a diﬀerent per-\nspective. We propose a Bayesian inference framework to\nuncover the connections, i.e., a feature vector, from data\nby explicitly modeling its posterior distribution. Based\non this framework, we develop a message passing algo-\nrithm for inferring an optimal feature vector, while mon-\nitoring its entropy to quantify how many candidate fea-\nture vectors are consistent with given data.\nIn this study, we consider a RBM [4, 7] with a sin-\ngle hidden neuron, whose activity is generated according\nto P(σ, h) ∝e−βE(σ,h)/\n√\nN, where σ = {σi|σi = ±1, i =\n1, . . . , N} represents binary activity of N visible neurons,\nh = ±1 is the activity of the hidden neuron, β/\n√\nN is\nthe system-size and inverse-temperature dependent scal-\ning factor, and E(σ, h) = −hξTσ is the energy function\ncharacterized by a feature vector ξ (T indicates vector\ntranspose). We assume that each element of the feature\nvector takes a binary value ξi = ±1. While generaliza-\ntion to a case with multiple hidden neurons is possible,\nwe do not explore it here as the analysis becomes more\ninvolved.\nTo perform unsupervised learning, we generate M in-\ndependent samples as training data {σa}M\na=1 from a RBM\nwith randomly generated true feature vector ξtrue, where\neach element is drawn from ±1 with equal probability.\nAnother RBM learns this feature vector from the data.\nWe formulate the learning process as Bayesian inference.\nGiven the training data, the posterior distribution of the\nfeature vector is\nP(ξ|{σa}M\na=1) ∝\nM\nY\na=1\nP(σa|ξ) = 1\nZ\nM\nY\na=1\ncosh\n\u0012 β\n√\nN\nξTσa\n\u0013\n,\n(1)\nwhere Z is the partition function of the model and we\nassume a uniform prior about ξ. Here, the data {σa}M\na=1\nserves as the quenched disorder (data constraints), and\n2\nthe inverse-temperature parameter β characterizes the\nlearning diﬃculty on a network of dimension N. If M >\n1, the model becomes non-trivial as the partition function\ncan not be computed exactly for a large N. This M can\nbe proportional to the system size, and in this case we\ndeﬁne a data density as α = M/N. Hereafter, we omit\nthe conditional dependence of P(ξ|{σa}M\na=1) on {σa}M\na=1.\nIn the following,\nwe compute the maximizer of\nthe\nposterior\nmarginals\n(MPM)\nestimator\nˆξi\n=\narg maxξi Pi(ξi) [8], which maximizes the overlap q =\n1\nN\nPN\ni=1 ξtrue\ni\nˆξi between the true and estimated feature\nvectors. If q = 0, the data do not give any information\nabout the feature vector. If q = 1, the feature vector is\nperfectly estimated. Hence, the task now is to compute\nmarginal probabilities, e.g., Pi(ξi), which is still a hard\nproblem due to the interaction among data constraints.\nHowever, by mapping the model (Eq. (1)) onto a factor\ngraph [4, 9], the marginal probability can be estimated\nby message passing (Fig. 1) as we explain below.\nWe\nﬁrst assume that elements of the feature vector on the\nfactor graph are weakly correlated (also named Bethe ap-\nproximation [10]), then by using the cavity method [9],\nwe deﬁne a cavity probability Pi→a(ξi) of ξi on a modi-\nﬁed factor graph with data node a removed. Under the\nweak correlation assumption, Pi→a(ξi) satisﬁes a recur-\nsive equation (namely belief propagation (BP) in com-\nputer science [11]):\nPi→a(ξi) ∝\nY\nb∈∂i\\a\nµb→i(ξi),\n(2a)\nµb→i(ξi) =\nX\n{ξj|j∈∂b\\i}\ncosh\n\u0012 β\n√\nN\nξTσb\n\u0013 Y\nj∈∂b\\i\nPj→b(ξj),\n(2b)\nwhere the proportionality symbol ∝omits a normaliza-\ntion constant, ∂i\\a deﬁnes the neighbors of feature node\ni except data node a, ∂b\\i deﬁnes the neighbors of data\nnode b except feature node i, and the auxiliary quantity\nµb→i(ξi) represents the contribution from data node b to\nfeature node i given the value of ξi [9]. An equation sim-\nilar to Eq. (2) was recently derived to compute activity\nstatistics of a RBM [4].\nIn the thermodynamic limit, the sum inside the hy-\nperbolic cosine function excluding the i-dependent term\nin Eq. (2b) is a random variable following a normal dis-\ntribution with mean Gb→i and variance Ξ2\nb→i [4], where\nGb→i =\n1\n√\nN\nP\nj∈∂b\\i σb\njmj→b and Ξ2\nb→i ≃1\nN\nP\nj∈∂b\\i(1−\nm2\nj→b). The cavity magnetization is deﬁned as mj→b =\nP\nξj ξjPj→b(ξj).\nThus the intractable sum over all ξj\n(j ̸= i) can be replaced by an integral over the nor-\nmal distribution.\nFurthermore, because ξi is a binary\nvariable, Pi→a(ξi) and µb→i(ξi) are completely charac-\nterized [4] by the cavity magnetization mi→a and cavity\nbias ub→i =\n1\n2 ln\nµb→i(ξi=1)\nµb→i(ξi=−1), respectively. Using these\nexpressions, the BP equation (Eq. (2)) could be reduced\nFIG. 1:\n(Color online) Schematic illustration of factor graph\nrepresentation and message passing. Left panel: circle nodes\nindicate features to be inferred. Square nodes indicate data\nconstraints. A connection σa\ni indicates how the feature ξi is\nrelated to a-th data. Right panel: the top panel shows data\nnode a collects messages from its neighboring features other\nthan node i and produces an output message to node i. The\nbottom panel shows feature node i collects messages from its\nneighboring data nodes other than b and produces an output\nmessage to data node b. Iteration of these messages gives a\ncoherent understanding of the feature learning process.\nto the following practical recursive equations:\nmi→a = tanh\n\nX\nb∈∂i\\a\nub→i\n\n,\n(3a)\nub→i = tanh−1 \u0010\ntanh(βGb→i) tanh(βσb\ni /\n√\nN)\n\u0011\n,\n(3b)\nwhere mi→a can be interpreted as the message passing\nfrom feature i to the data constraint a, while ub→i can be\ninterpreted as the message passing from data constraint\nb to feature i (Fig. 1). If the weak correlation assump-\ntion is self-consistent, the BP would converge to a ﬁxed\npoint corresponding to a stationary point of the Bethe\nfree energy function with respect to the cavity messages\n{mi→a, ua→i} [9].\nBy initializing the message on each link of the factor\ngraph (Fig. 1), we run the BP equation (Eq. (3)) un-\ntil it converges within a preﬁxed precision.\nFrom this\nﬁxed point, one can extract useful information about the\ntrue feature vector, by calculating the marginal proba-\nbility as Pi(ξi) = 1+miξi\n2\nwhere mi = tanh\n\u0000P\nb∈∂i ub→i\n\u0001\n.\nAn alternative strategy to use passing messages to in-\nfer the true feature vector is called reinforced BP (rBP).\nThis strategy is a kind of soft-decimation [12], which\nprogressively enhances/weakens the current local ﬁeld\n(ht\ni = P\nb∈∂i ub→i) each feature component feels by the\nreinforcement rule ht\ni ←ht\ni + ht−1\ni\nwith a probability\n1 −γt, until a solution ({ˆξi = sgn(mi)}) is stable over\niterations. γ usually takes a value close to 1.\nAnother important quantity is the number of candidate\nfeature vectors consistent with the data, characterized by\nthe entropy per neuron s = −1\nN\nP\nξ P(ξ) ln P(ξ). Under\nthe Bethe approximation, s is evaluated as summing up\n3\ncontributions from single feature nodes and data nodes\n[13].\nWe use the above mean ﬁeld theory to study unsu-\npervised learning of the feature vector in single random\nrealizations of the true feature vector and analyze how\nits performance depends on model parameters. We ﬁrst\nexplore eﬀects of β (N = 100). Note that β scales the\nenergy and hence tunes the diﬃculty of the learning. As\nshown in Fig. 2 (a), it requires an extensive number of\ndata to learn a weak feature vector (β = 0.5).\nHow-\never, as β increases, the inference becomes much better.\nThe overlap grows more rapidly at β = 1.0. When α is\nabove 10, one can get a nearly perfect inference of the\nfeature vector. We also use the reinforcement strategy to\ninfer the true feature vector, and it has nearly the same\nperformance with reduced computer time, because the\nestimation of the true feature need not be carried out at\nthe ﬁxed point.\nRemarkably, the overlap improves at some α, reveal-\ning that the RBM could extract the hidden feature vec-\ntor only after suﬃcient data are shown. This critical α\ndecreases with the saliency β of the hidden feature. A\nstatistical analysis of Eq. (3) reveals that αc =\n1\nβ4 [13].\nNext, we show the entropy per neuron in the inset of\nFig. 2 (a). This quantity that describes how many fea-\nture vectors are consistent with the data becomes nega-\ntive (i.e., entropy crisis [14, 15]) at a zero-entropy αZE.\nHowever, the BP equation is still stable (convergent),\nand thus the instability occurs after the entropy crisis.\nThis suggests the existence of a discontinuous glass tran-\nsition at a value of α less than or equal to αZE, as com-\nmonly observed in some spin glass models of combinato-\nrial satisfaction problems [15–18]. As shown in the inset\nof Fig. 2 (a), αc can be either larger or smaller than αZE,\ndepending on β. If αc < αZE, a continuous transition is\nfollowed by a discontinuous transition, where intra-state\nand inter-state overlaps [19] bifurcate discontinuously. If\nαc > αZE, the predicted continuous transition is inac-\ncurate at least under the replica symmetric assumption.\nDetailed discussions about this novel property will be\nprovided in a forthcoming extended work [20].\nInter-\nestingly, despite the likely glass transition and entropy\ncrisis, the ﬁxed point of BP still yields good inference of\nthe feature vector, which may be related to the Nishi-\nmori condition (Bayes-optimal inference) [20, 21], since\nthe temperature parameter used in inference is the same\nas that used to generate the data.\nNext, we study the median of learning time. The learn-\ning time is measured as the number of iterations when\nthe message passing converges.\nFig. 2 (b) shows that\nlearning is fast at small α, slow around the critical α,\nand becomes fast again at large α. This easy-hard-easy\nphenomenon can be understood as follows. When a few\ndata are presented, the inference is less constrained, and\nthus there exist many candidate feature vectors consis-\ntent with the data, the BP converges fast to estimate the\n0\n2\n4\n6\n8\n10\n12\n14\n16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n \n \nq\n \n=0.5\n  \n=0.8\n  \n=1.0\n  rBP\n(a)\n0\n1\n2\n3\n4\n5\n6\n-2.0\n-1.5\n-1.0\n-0.5\n0.0\n0.5\nentropy per neuron\n0\n1\n2\n3\n4\n5\n6\n0\n200\n400\n600\n800\n1000\n1200\n \n \n Learning time\n \n=1.0, N=100\n \n=0.8, N=100\n \n=1.0, N=200\n(b)\n0\n100\n200\n300\n400\n0\n1000\n2000\n3000\nPeak learning time\nN\nslope~6.055\nFIG. 2: (Color online) Bayesian learning in the RBM model\n(Eq. (1)). 30 random realizations of the model are consid-\nered.\nThe error bars characterize the standard deviation.\nN = 100. (a) Inference overlap q versus data density α with\ndiﬀerent values of feature saliency β. Results obtained by rBP\n(γ = 0.95) are also shown for comparison (β = 1). The inset\nshows the entropy per neuron. Arrows in the inset indicate\nthresholds for continuous transitions. (b) Median of conver-\ngence (learning) time deﬁned by the number of iterations at\nwhich BP converges to a ﬁxed point. Two diﬀerent values of\nβ are considered. For β = 1, results for a larger N are also\nshown. The inset shows that the peak learning time scales\nlinearly with N (β = 1).\nmarginals. Once relatively many data are presented, the\nnumber of candidate feature vectors reduces (Fig. 2 (a)),\nand the BP requires more iterations to ﬁnd a candidate\nfeature vector. We also observe that the peak learning\ntime at αc scales linearly with N within the measured\nrange (the inset of Fig. 2 (b)).\nThis intermediate re-\ngion is thus termed hard phase. Large β moves the hard\nphase to the small α region. Once the number of data is\nsuﬃciently large, the inference becomes easy once again\nas the number of necessary iterations drops drastically.\nThis may be because the feature space around the true\nfeature vector dominates the posterior probability in the\npresence of suﬃcient data.\nIt is interesting to show that one can also perform the\nsame unsupervised learning by using an associative mem-\n4\nory (Hopﬁeld) model deﬁned by\n˜P(ξ) ∝\nY\na\ne\n˜\nβ\n2N\n\u0010\nξTσa\n\u00112\n,\n(4)\nwhere ˜β = β2. This posterior distribution about a feature\nvector ξ given data {σa}M\na=1 can be obtained by a small-\nβ expansion of Eq. (1) [22]. This relationship indicates\nthat one can infer the feature vector of a RBM using this\nHopﬁeld model if β is small enough. In this case, the\ntrue feature vector is also interpreted as a unique memory\npattern in the Hopﬁeld model. By a small-β expansion,\nwe interpret the unsupervised learning in a RBM model\nas recovering stored pattern in a Hopﬁeld model from\nnoisy data [23]. In the following, we generate data by\na RBM with true feature ξtrue and compute the MPM\nestimator ˆξ of the memory pattern in Eq. (4) based on\nthe given data.\nAnalogous to the derivation of Eq. (3), we can derive\nthe practical BP corresponding to the posterior proba-\nbility (Eq. (4)):\nmi→a = tanh\n\n\n˜β\n√\nN\nX\nb∈∂i\\a\nσb\ni ˜Gb→iFb→i\n\n,\n(5)\nwhere ˜Gb→i =\n1\n√\nN\nP\nk∈∂b\\i σb\nkmk→b, Fb→i = 1+\n˜βCb→i\n1−˜βCb→i\nin which Cb→i =\n1\nN\nP\nk∈∂b\\i(1 −m2\nk→b). The entropy of\ncandidate feature vectors can also be evaluated from the\nﬁxed point of this iterative equation [13].\nBayesian learning performance of the Hopﬁeld model is\nshown in Fig. 3. This model does not show an entropy cri-\nsis in the explored range of α. As α increases, the entropy\ndecreases much more slowly for weak features than for\nstrong ones. For β = 0.5, the overlap stays slightly above\nzero for a wide range of α. At a suﬃciently large α (∼8),\nthe overlap starts to increase continuously. It is impos-\nsible to predict underlying structures if the number of\ndata is insuﬃcient. This phenomenon is named retarded\nlearning ﬁrst observed in unsupervised learning based on\nGaussian or mixture-of-Gaussian data [24, 25]. At large\nα where the entropy value approaches zero, the overlap\napproaches one. All the properties except the entropy\ncrisis are qualitatively similar in both the approximate\nHopﬁeld and RBM model. The absence of entropy crisis\nmay be related to the absence of p-spin (p > 2) interanc-\ntions in the approximate model, which has thus only the\ncontinuous glass transition at αc =\n\u0010\n1\n˜β −1\n\u00112\n, predicted\nby a statistical analysis of Eq. (5) [13]. Note that the\nspin glass transition in a standard Hopﬁeld model where\nmany random patterns are stored is of second order [26].\nThe current analysis sheds light on understanding the\nrelationship between RBM and associative memory net-\nworks [22, 27] within an unsupervised learning frame-\nwork.\n0\n2\n4\n6\n8\n10\n12\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n \n \nentropy per neuron\n \n=0.5\n \n=0.8\n \n=1.0\n(a)\n0\n2\n4\n6\n8\n10\n12\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n \n \nq\n \n=0.5\n \n=0.8\n \n=1.0\n(b)\nFIG. 3:\n(Color online) Bayesian learning in the approximate\nHopﬁeld model (Eq. (4)). 30 random realizations of the model\nare considered. The error bars characterize the standard de-\nviation. N = 100. (a) Entropy per neuron versus the data\ndensity α. (b) Overlap versus the data density. Results for\nRBM (solid symbols) are shown for comparison.\n0.00\n0.05\n0.10\n0.15\n0.20\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n \n \nentropy per neuron\n RBM (0,1)\n Hopfield (0,1)\n RBM (3,7)\n Hopfield (3,7)\nFIG. 4:\n(Color online) Entropy per neuron versus α for\nMNIST dataset (handwritten digits (0,1) and (3, 7)) at β = 1.\n5\nFinally, we test our theory on the MNIST handwritten\ndigit dataset [28]. For simplicity, we consider some com-\nbinations of two diﬀerent digits (e.g., 0 and 1, 3 and 7).\nEach digit is represented by a 28 × 28 gray-scale image.\nResults show that the real dataset shares common prop-\nerties with our model (Fig. 4), which does not change\nqualitatively when diﬀerent combinations even the whole\ndataset are used. The inferred feature vector (receptive\nﬁeld of hidden neuron) improves as the number of data\ngrows, serving as a local structure detector [13].\nThis\nis indicated by the precision-recall curve moving to the\nrightmost upper corner of the plot as the data size in-\ncreases [13]. Importantly, the presence and absence of\nthe entropy crisis in a RBM and a Hopﬁeld model, re-\nspectively, are also conﬁrmed in this real dataset.\nIn conclusion, we build a physics model of unsuper-\nvised learning based on the RBM and propose a Bayesian\ninference framework to extract a hidden feature vector\nfrom a ﬁnite number of data. The mean ﬁeld theory in-\nspires an eﬃcient message passing procedure to infer the\nhidden feature. Unlike previous approaches, each data in\nthis work is treated as a constraint on the factor graph,\nand the message passing carries out a probabilistic infer-\nence of the hidden feature without sampling activities of\nneurons. We show that, salient features can be recovered\nby even a few data. Conversely, it is impossible to recover\nthe weak features by a ﬁnite amount of data. Interest-\ningly, the entropy of candidate feature vectors becomes\nnegative before the message passing algorithm becomes\nunstable, suggesting a discontinuous glass transition to\nresolve the entropy crisis, a typical statistical mechanics\nphenomenon revealed in studies of spin glass models [15–\n18]. In terms of the convergence time of the message pass-\ning algorithm, we reveal the easy-hard-easy phenomenon\nfor the current unsupervised learning, which is explained\nby our theory. All these properties except the entropy cri-\nsis are also observed in an approximate Hopﬁeld model,\nwhere we infer from data a hidden feature of a RBM.\nRemarkably, these phenomena are also conﬁrmed in a\nreal dataset (Fig. 4). This work provides a theoretical\nbasis to understand eﬃcient neuromorphic implementa-\ntion of RBM with simple binary feature elements [29].\nWe also hope our study will provide important insights\ninto a physics understanding of unsupervised learning,\nespecially its important role in pretraining deep neural\nnetworks for superior performances [6].\nH.H. thanks Pan Zhang for useful discussions. This\nwork was supported by the program for Brain Map-\nping by Integrated Neurotechnologies for Disease Stud-\nies (Brain/MINDS) from Japan Agency for Medical Re-\nsearch and development, AMED, and by RIKEN Brain\nScience Institute.\n[1] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, Sci-\nence 350, 1332 (2015).\n[2] G. E. Hinton, Trends in Cognitive Sciences 11, 428 (2007).\n[3] D. Kersten, P. Mamassian, and A. Yuille, Annu. Rev. Psy-\nchol. 55, 271 (2004).\n[4] H. Huang and T. Toyoizumi, Phys. Rev. E 91, 050101\n(2015).\n[5] G. E. Hinton and R. R. Salakhutdinov, Science 313, 504\n(2006).\n[6] Y. Bengio, A. Courville, and P. Vincent, Pattern Analysis\nand Machine Intelligence, IEEE Transactions on 35, 1798\n(2013).\n[7] G. Hinton, S. Osindero, and Y. Teh, Neural Computation\n18, 1527 (2006).\n[8] H. Nishimori, Statistical Physics of Spin Glasses and In-\nformation Processing: An Introduction (Oxford University\nPress, Oxford, 2001).\n[9] M. M´ezard and A. Montanari, Information, Physics, and\nComputation (Oxford University Press, Oxford, 2009).\n[10] M. M´ezard and G. Parisi, Eur. Phys. J. B 20, 217 (2001).\n[11] J. S. Yedidia, W. T. Freeman, and Y. Weiss, IEEE Trans\nInf Theory 51, 2282 (2005).\n[12] A. Braunstein and R. Zecchina, Phys. Rev. Lett 96,\n030201 (2006).\n[13] See supplemental material at http://... for details about\nthe entropy formula, statistical analysis of BP equations,\nand applications on MNIST dataset.\n[14] B. Derrida, Phys. Rev. Lett. 45, 79 (1980).\n[15] O. C. Martin, M. M´ezard, and O. Rivoire, Phys. Rev.\nLett. 93, 217205 (2004).\n[16] A. Montanari, Eur. Phys. J. B 23, 121 (2001).\n[17] H. Hai-Ping, Commun. Theor. Phys 63, 115 (2015).\n[18] H. Huang and Y. Kabashima, Phys. Rev. E 90, 052813\n(2014).\n[19] E. Barkai, D. Hansel, and I. Kanter, Phys. Rev. Lett. 65,\n2312 (1990).\n[20] H. Huang and T. Toyoizumi, in preparation.\n[21] Y. Iba, Journal of Physics A: Mathematical and General\n32, 3875 (1999).\n[22] E. Agliari,\nA. Barra,\nA. Galluzzi,\nF. Guerra, and\nF. Moauro, Phys. Rev. Lett. 109, 268101 (2012).\n[23] S. Cocco, R. Monasson, and V. Sessak, Phys. Rev. E 83,\n051123 (2011).\n[24] M. Biehl and A. Mietzner, Journal of Physics A: Mathe-\nmatical and General 27, 1885 (1994).\n[25] T. L. H. Watkin and J. P. Nadal, Journal of Physics A:\nMathematical and General 27, 1899 (1994).\n[26] D. J. Amit, H. Gutfreund, and H. Sompolinsky, Phys.\nRev. Lett. 55, 1530 (1985).\n[27] M. M´ezard, ArXiv e-prints (2016), 1608.01558.\n[28] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner, Proceed-\nings of the IEEE 86, 2278 (1998).\n[29] M.\nCourbariaux,\nY.\nBengio,\nand\nJ.-P.\nDavid,\nArXiv:1511.00363 (2015).\n6\nThe number of candidate feature vectors in the RBM model\nFor the unsupervised learning in the RBM model, one important quantity is the number of candidate feature vectors\nconsistent with the input noisy data, characterized by the entropy per neuron s = −1\nN\nP\nξ P(ξ) ln P(ξ). Under the\nBethe approximation [10], s is evaluated as summing up contributions from single feature nodes and data nodes:\nNs =\nX\ni\n∆Si −(N −1)\nX\na\n∆Sa,\n(S1)\nwhere single feature node contribution is expressed as ∆Si = P\na∈∂i\nh\nβ2Ξ2\na→i/2 + ln cosh(βGa→i + βσa\ni /\n√\nN)\ni\n+\nln\n\u00001 + Q\na∈∂i Ga→i\n\u0001\n−\n\u0002P\na∈∂i Ha→i(+1) + Q\na∈∂i Ga→i\nP\na∈∂i Ha→i(−1)\n\u0003\n/\n\u00001 + Q\na∈∂i Ga→i\n\u0001\n, and single data node\ncontribution ∆Sa = ln cosh(βGa) −β2Ξ2\na/2 −βGa tanh(βGa). We deﬁne Ga→i = e−2ua→i, Ha→i(ξi) = β2Ξ2\na→i +\n(βGa→i + βσa\ni ξi/\n√\nN) tanh(βGa→i + βσa\ni ξi/\n√\nN), Ga =\n1\n√\nN\nP\ni∈∂a σa\ni mi→a, and Ξ2\na = 1\nN\nP\ni∈∂a(1 −m2\ni→a).\nThe number of candidate feature vectors in the approximate Hopﬁeld model\nFor the approximate Hopﬁeld model, the entropy can be evaluated as Ns = P\ni ∆Si −(N −1) P\na ∆Sa, where\nsingle feature node contribution reads ∆Si = −P\na∈∂i\nh\n1\n2 ln(1 −˜βCa→i) +\n˜βCa→i\n2(1−˜βCa→i) +\n˜β\n2 (1/N + ˜G2\na→i)F ′\na→i\ni\n+\nln\n\u0010\n2 cosh(˜βHi)\n\u0011\n−(˜βHi + ˜βH′\ni) tanh(˜βHi), and single data node contribution ∆Sa = −1\n2 ln(1 −˜βCa) −\n˜βCa\n2(1−˜βCa) −\n˜β\n2 ˜G2\naF ′\na, where ˜Ga =\n1\n√\nN\nP\nk∈∂a σa\nkmk→a, F ′\na→i =\n˜βCa→i\n(1−˜βCa→i)2 , F ′\na =\n˜βCa\n(1−˜βCa)2 , Ca =\n1\nN\nP\nk∈∂a(1 −m2\nk→a),\nHi =\n1\n√\nN\nP\nb∈∂i σb\ni ˜Gb→iFb→i, and H′\ni =\n1\n√\nN\nP\nb∈∂i σb\ni ˜Gb→iF ′\nb→i.\nA statistical analysis of practical BP equations\nWe ﬁrst statistically analyze the practical BP equations (Eq. (3) in the main text) for the RBM. In a large N limit,\nthe cavity bias can be approximated as ub→i ≃βσb\ni\n√\nN tanh βGb→i. We then deﬁne a cavity ﬁeld as hi→a = P\nb∈∂i\\a ub→i.\nThe sum in the cavity ﬁeld involves an order of O(N) terms, which are assumed to be nearly independent under the\nreplica symmetric assumption. Therefore, the cavity ﬁeld follows a normal distribution with mean zero and variance\nαβ2 ˆQ, where ˆQ ≡\n\ntanh2 βGb→i\n\u000b\n. Note that Gb→i is also a random variable subject to a normal distribution with zero\nmean and variance Q. Q is deﬁned by Q = 1\nN\nP\ni m2\ni . To derive the variance of Gb→i, we used\n1\nN\nP\nk∈∂b\\i m2\nk→b ≃Q,\nwhich is reasonable in the large N limit (thermodynamics limit). Finally, we arrive at the following thermodynamic\nrecursive equation:\nQ =\nZ\nDz tanh2 β\nq\nα ˆQz,\n(S2a)\nˆQ =\nZ\nDz tanh2 β\np\nQz,\n(S2b)\nwhere Dz = dze−z2/2\n√\n2π\n. Note that Q = 0 is a stable solution of Eq. (S2) only when α ≤αc =\n1\nβ4 . The threshold can be\nobtained by a Taylor expansion of Eq. (S2) around Q = 0.\nNext, we perform a statistical analysis of the practical BP equation (Eq. (5) in the main text) for the approximate\nHopﬁeld model.\nSimilarly, a cavity ﬁeld deﬁned by hi→a =\n1\n√\nN\nP\nb∈∂i\\a σb\ni ˜Gb→iFb→i can be approximated by a\nrandom variable following a normal distribution with mean zero and variance\nαQ\n(1−˜β(1−Q))2 , where Q ≡\n1\nN\nP\ni m2\ni .\nConsequently, we obtain the ﬁnal thermodynamic equation as:\nQ =\nZ\nDz tanh2\n \n˜β\n1 −˜β(1 −Q)\np\nαQz\n!\n.\n(S3)\nObviously, Q = 0 is a solution of Eq. (S3), which is stable up to αc =\n\u0010\n1\n˜β −1\n\u00112\n. The threshold can be analogously\nderived by a linear stability analysis.\n7\n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \nY Axis Title\nFIG. S1:\nTwo examples of digits (0 and 1) from the MNIST dataset are shown.\nApplications of our theory on MNIST dataset\nWe are interested in whether our model results hold in real dataset or not. For simplicity, we consider MNIST\ndataset [28] with only handwritten digits 0 and 1 (see Fig. S1). Each digit is represented by a 28 × 28 gray-scale\nimage. Given the dataset (a set of digit 0 and 1 images), our theory could estimate the corresponding hidden feature\nvector consistent with these images. The feature vector is also called the receptive ﬁeld of the hidden neuron, because\nit determines the ﬁring response of the hidden neuron. We organize the inferred feature vector into a 28 × 28 matrix\n(the same size as the input image), and plot the matrix as the gray-scale image (black indicates ξi = −1, and white\nindicates ξi = 1). As shown in Fig. S2, the inferred feature vector gets much better as the number of input images\ngrows, serving as a local structure detector. This is indicated by a few active synaptic weights in the center of the\nimage, where local characteristics of handwritten digits are approximately captured when suﬀcient data samples are\nshown to the machine. In a deep network composed of many layers of stacked RBM, the low-level features (e.g.,\nedges or contours) detected by the hidden neuron could be passed to deeper layers, where high-level information (e.g.,\nobject identity) could be extracted [3].\nThe eﬀect of the data size on the feature learning performance can be quantitatively measured by the precision-\nrecall curve. First, we computed the weighted sum (local ﬁeld H) of the hidden neuron given the input image. These\nﬁelds are then ranked. Digits 1 and 0 are discriminated by introducing a threshold Hth for corresponding local ﬁelds.\nThe true positive (TP) event is identiﬁed when digit 1 is predicted by a local ﬁeld above the threshold. The false\npositive (FP) event is identiﬁed when digit 0 is wrongly predicted by a local ﬁeld above the threshold. The false\nnegative event (FN) is identiﬁed when digit 1 is wrongly predicted by a local ﬁeld below the threshold. The recall\n(RC) is deﬁned as RC =\nPTP\nPTP+PFN , and the precision (PR) is deﬁned as PR =\nPTP\nPTP+PFP , where PTP is deﬁned as the\nnumber of TP events in all presented samples. Thus the precision-recall curve is the parametric curve of PR(Hth) and\nRC(Hth). The closer we can get to (1, 1) in the plot, the better the unsupervised feature learning understands the\nembedded feature structure of the data. As shown in Fig. S3, as the data size increases, the performance improves,\nand it behaves much better than a random guess of synaptic weights.\n[1] M. M´ezard and G. Parisi. The bethe lattice spin glass revisited. Eur. Phys. J. B, 20:217, 2001.\n[2] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document recognition. Proceedings of\nthe IEEE, 86:2278–2324, 1998.\n[3] James J. DiCarlo, Davide Zoccolan, and Nicole C. Rust. How Does the Brain Solve Visual Object Recognition? Neuron,\n73:415–434, 2012.\n8\n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \nj\ni\n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \n5\n10\n15\n20\n25\n5\n10\n15\n20\n25\n \n \nM=5\nM=100\nM=140\nM=5\nM=50\nM=150\nRBM\nHopfield\nFIG. S2:\nInferred features evolve with the number of input images (M) on the MNIST dataset. The top panel displays the\nresult for the RBM model, and the bottom panel displays the result for the approximate Hopﬁeld model.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n M=5\n M=100\n M=140\n RG M=100\nPrecision\nRecall\n(a) RBM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n M=5\n M=50\n M=150\n RG M=50\nPrecision\nRecall\n(b) Hopfield\nFIG. S3:\n(Color Online) The precision-recall curve corresponding to Fig. S2. The unsupervised learning performance is also\ncompared with the random guess (RG) case.\n",
  "categories": [
    "cond-mat.dis-nn",
    "cond-mat.stat-mech",
    "cs.LG",
    "q-bio.NC"
  ],
  "published": "2016-08-12",
  "updated": "2016-11-11"
}