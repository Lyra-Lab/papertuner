{
  "id": "http://arxiv.org/abs/1608.04434v1",
  "title": "Natural Language Processing using Hadoop and KOSHIK",
  "authors": [
    "Emre Erturk",
    "Hong Shi"
  ],
  "abstract": "Natural language processing, as a data analytics related technology, is used\nwidely in many research areas such as artificial intelligence, human language\nprocessing, and translation. At present, due to explosive growth of data, there\nare many challenges for natural language processing. Hadoop is one of the\nplatforms that can process the large amount of data required for natural\nlanguage processing. KOSHIK is one of the natural language processing\narchitectures, and utilizes Hadoop and contains language processing components\nsuch as Stanford CoreNLP and OpenNLP. This study describes how to build a\nKOSHIK platform with the relevant tools, and provides the steps to analyze wiki\ndata. Finally, it evaluates and discusses the advantages and disadvantages of\nthe KOSHIK architecture, and gives recommendations on improving the processing\nperformance.",
  "text": "Natural Language Processing using Hadoop and KOSHIK \nHong Shi, Emre Erturk  \nEastern Institute of Technology, New Zealand \n \nAbstract \nNatural language processing, as a data analytics related technology, is used widely in many \nresearch areas such as artificial intelligence, human language processing, and translation. At \npresent, due to explosive growth of data, there are many challenges for natural language \nprocessing. Hadoop is one of the platforms that can process the large amount of data required for \nnatural language processing. KOSHIK is one of the natural language processing architectures, and \nutilizes Hadoop and contains language processing components such as Stanford CoreNLP and \nOpenNLP. This study describes how to build a KOSHIK platform with the relevant tools, and provides \nthe steps to analyze wiki data. Finally, it evaluates and discusses the advantages and disadvantages \nof the KOSHIK architecture, and gives recommendations on improving the processing performance.  \n1. Introduction \n1.1. Natural Language Processing \nNatural language processing (NLP) is a technique to analyze readable text that is generated by \nhumans for artificial intelligence, language processing, and translation (Behzadi, 2015). In order to \naccurately analyze the text, there are some methods for NLP to use in order to deal with the \nchallenges such as the collection and storage of the text corpus, and analysis. NLP techniques also \ngain experience and benefits through research in linguistics, computational statics, artificial \nintelligence, machine learning and other sciences (Behzadi, 2015). However, at present, because \nof the information explosion, the use of traditional NLP faces many challenges such as the volume \nof structured and unstructured data, velocity of processing data, accuracy of the results. In addition, \nthere are many slangs and ambiguous expressions used on social media networks, which give NLP \npressure to analyze the meanings, which may also be hard for some people. Moreover, people \nnowadays heavily depend on search engines like Google and Bing (which use NLP as their core \ntechnique) in their daily study, work, and entertainment. All of these factors encourage computer \nscientists and researchers to find more robust, efficient and standardized solutions for NLP.  \n1.2. Big Data \nBig Data is designed as generic platform to resolve the issues of volume, velocity, variety, \nveracity and value in data analytics (IBM, 2012). The data is collected from different sources, for \nexample, daily logs, social media, and business transactions. Big Data demands the ability to hoard \nlarge amounts of data. With advanced storage technologies, the data size could be as high as \nterabytes (1012 bytes), petabytes (1015 bytes) and exabytes (1018 bytes). Furthermore, research on \nNLP often overlaps with research on or the use of Big Data platforms. Big Data handles all types of \nformats which are structured and unstructured. The structured data is readable and well-designed \nthat is commonly stored in traditional relationship databases. Unstructured data does not have a \npredefined format. This type of data can be found in, for example, emails, images, and videos. Big \nData is widely used in business forecasts, scientific research, analysis of social issues, healthcare, \nand meteorology. While promoting advances in NLP, it is also important to be aware of ethical \nissues around the potential misuse and dual-use of big data and NLP tools (Hovy & Spruit, 2016). \n1 \n \nSome of these issues can be very interesting for information technology students to debate and \nlearn more about (Erturk, 2013).  \n1.3. Hadoop \nHadoop is a application created using Java, and provides a set of tools to do data processing \nwhich includes data storage, access, analysis (White, 2012). According to White (2012), the main \ncomponents in Hadoop are HDFS (Hadoop distributed file system) and MapReduce. HDFS is solid \nand fault-tolerant, and provides a Java-based API that integrates with MapReduce to process the \nlarge data in parallel using a cluster of servers (Taylor, 2010). As Murthy, Padmakar and Reddy (2015) \nobserved, traditional relational databases, which store structured data, could also be used together \nwith Hadoop HDFS with full database management systems (DBMS) features. MapReduce is \ncreated with inspiration from the theory published by Google, and it provides functions that split \nlarge set of data computing into small computing tasks (White, 2012).  \n1.4. Natural Language Processing on Hadoop \nIdealy, Hadoop, with the features of distributed storage system, multi-tasks processing system, \ngeneric platform and open source, can be used for NLP research. There are many papers discussing \nthe solutions which utilize Hadoop for NLP. For example, HDFS is used in the research of Markham, \nKowolenko and Michaelis (2015) to manage large amount of unstructured data, which was \ncollected from the Internet with Hadoop tools. Idris, Hussain, Siddiqi, Hassan, Bilal and Lee (2015) \ndesigned a system named MRPack which gives an end-to-end MapReduce processing model for \ntext processing, which includes a job task for NLP. MRPack shows it has better performance in data \naccessing, data managing and data writing, as well as less programming work and demanding for \nI/O management (Idris et al., 2015). \nThis study reviews KOSHIK, a Hadoop based framework which has been developed from the \npaper written by Exner and Nugues (2014). Then it gives the findings for what Hadoop components \nthis framework used, and recommendations to improve the NLP performance. Finally, it shows the \nsteps to test KOSHIK.  \n2. Literature Review \n2.1. Introduction \nIt is described that “KOSHIK is a framework for batch-oriented large scale-processing and \nquerying of unstructured natural language documents. In particular, it builds on the Hadoop \necosystem and takes full advantage of the data formats and tools present in this environment to \nachieve its task” (Exner & Nugues, 2014, p. 463). KOSHIK tries to resolve the common challenges \nin NLP such as volume, velocity, and variety by adopting Hadoop for the system infrastructure, \nusing a batch-oriented annotation model to continually add annotations and enabling a generic \nalgorithm platform to analyze the variety of text (Exner & Nugues, 2014). It is also argued that \nbefore developing KOSHIK, there were many other NLP frameworks such as MULTEXT, GATE, and \nUIMA, which were important for document retrieval, indexing, and querying of processed \ninformation applications (Exner & Nugues, 2014). \n2 \n \n2.2. KOSHIK Architecture \n \nFigure 1. An overview of the KOSHIK architecture. (Exner & Nugues, 2014). \nExner and Nugues (2014) stated that “KOSHIK supports a variety of NLP tools implemented \natop of the Hadoop distributed computing framework. The requirements on KOSHIK were driven \nby the desire to support scalability, reliability, and a large number of input formats and processing \ntasks”. KOSHIK supports different kinds of documents such as CoNLL-X, CoNLL 2008 and 2009 and \nWikipedia dump files (Exner & Nugues, 2014; Buchholz and Marsi, 2006; Surdeanu et al., 2008). In \norder to analyze different kinds of language, KOSHIK involves a large set of filters, tokenizers, \ntaggers, parsers, and coreference solvers through several language specific NLP tools such as \nOpenNLP, Mate Tools, Stanford CoreNLP, etc. (Exner & Nugues, 2014). With the developed \nannotation model in KOSHIK, this framework could simplify the process of content processors, \nwhich only require computing resource focus on the input and output of annotated documents \n(Exner & Nugues, 2014). \nKOSHIK utilizes Hadoop as its distributed computing framework, which not only used for a \nbetter performance on large corpus analysis, but also for the division and distribution of a set of \ndocuments, management of processing jobs, and to get the process result (Exner & Nugues, 2014). \nPig and Hive which are core members of Hadoop ecosystem are adopted into this framework to \nmanage the workflows for processing large data-sets and easily query information by SQL-like \nlanguage respectively (Exner & Nugues, 2014). \n3. Key Terms \nAfter evaluating this paper, Hive and Pig which are data querying components of Hadoop, and \nOpenNLP and Stanford CoreNLP which are NLP process tools are identified for future research. \nAccording to Thusoo et al. (2009), Hive is a data warehouse infrastructure built based on \nHadoop for providing data summarization, query, and analysis. It could be used to deal with large \ndistributed data such as HDFS and traditional file systems like FAT, NTFS and exFat (Thusoo et al., \n2009). In addition, HiveQL, a SQL-like language, is created to query data on Hadoop which will \ntranslate the query to MapReduce jobs for high performance (Thusoo et al., 2009). \n \n3 \n \nThe Pig is a high-level declarative querying language inspired by SQL which will translate the \nquery to MapReduce jobs (Pis Olston, Reed, Srivastava, Kumar & Tomkins, 2008). According to \nGates et al. (2009), there are many data analysis projects which adopted Pig because it is easy to \nlearn and use, and could quickly implement different versions of algorithms. \nLingad, Karimi and Yin (2013) found that “OpenNLP is a Java based library for various natural \nlanguage processing tasks, such as tokenization, part-of-speech (POS) tagging, and named entity \nrecognition. For named entity recognition, it trains a Maximum Entropy model using the \ninformation from the whole document to recognize entities in documents”. Although OpenNLP \nprovides many functions for NLP, the model to process the document should be considered that \nVerspoor (2012) argued that OpenNLP has low quality to divide sentence into parts, but it could \nimprove the performance by using annotator. \nStanford CoreNLP is a NLP tool and utilizes annotation to analyse text that it provides most of \nthe common core NLP steps, from tokenization through to coreference resolution (Manning, 2014). \nIt is also described that Stanford CoreNLP provides a complete toolkit and tools for grammatical \nanalysis, accurate analysis, and supports different kinds of languages (Manning, 2014).  \n4. Testing KOSHIK \n4.1. Preparing the Hardware Environment \nAccording to Exner and Nugues (2014), KOSHIK runs on Hadoop with Hive, and requires other \nNLP process libraries (OpenNLP, Mate and Stanford CoreNLP). The experimental environment \n(including hardware, software, and test data) used in this paper is described in the following tables.  \n \nHardware Environment \nHardware \nDescription \nCPU \nINTEL i5 quad cores 2.7Ghz  \nHard disk \nSATA 500G \nMemory \nDDR3 16G \nGraphic Card \nNVidia 940 \nNetwork \n1G LAN \n \nSoftware Environment \nOperation \nSystem/application/library \nVersion \nOperation System \nCentos 6.7 \nKOSHIK \n1.01 \nJAVA \n1.80 \nHadoop \n2.60 \nHive \n1.1.0 \nCloudera \n5.70 \nHue \n3.90 \nVirtualBox \n5.0.20 \nStanford CoreNLP \n3.6 \nOpenNLP \n1.5.0 \nMate \n \n \nTest Data \nItem \nDescription \nWiki Data \n12GB English wiki data. \n \n4 \n \nIn this test environment, Cloudera Quickstart VM is utilized to quickly construct the software \nenvironment. According to Cloudera (2016), Cloudera Quickstart VM is a virtual machine which \ninstalled most of the applications related to Hadoop such as Hive, Spark, HBase. One benefit to use \nthis virtual machine is that it has already configured Hadoop and integrated with Hive, HBase and \nHue, which is a management tool for applications in Hadoop ecosystem (Cloudera, 2016). \nTherefore, it is an ideal platform for researching and making proof of concept. \n4.2. Preparing the Software \n4.2.1. Download software and wiki data \nThe following table list the download links for the required software. \nSoftware \nLink \nKOSHIK \nhttps://github.com/peterexner/KOSHIK \nCloudera Quickstart VM \nhttp://www.cloudera.com/downloads/quickstart_vms/5-\n7.html \nVirtualBox \nhttps://www.virtualbox.org/wiki/Downloads \nStanford CoreNLP \nhttp://nlp.stanford.edu/software/corenlp.shtml \nOpenNLP \nhttp://opennlp.sourceforge.net/models-1.5/ \nMate \nhttps://code.google.com/p/mate-tools/downloads/list \nWikiData \nhttps://dumps.wikimedia.org/enwiki/20160501/enwiki-\n20160501-pages-articles.xml.bz2 \n \n4.2.2. Configure software \nThe following table describes steps to configure the required software. \nSoftware \nStep Description \nVirtualBox \n1. Unzip Cloudera Quickstart VM. \n2. Import the unzipped Cloudera Quickstart \nVM into VirtualBox. \nKOSHIK required libraries \n1. Create a folder anywhere named model. \n2. Create a folder named is2 in model folder \nand put downloaded files CoNLL2009-ST-\nEnglish-ALL.anna-3.3.lemmatizer.model, \nCoNLL2009-ST-English-ALL.anna-\n3.3.parser.model, CoNLL2009-ST-English-\nALL.anna-3.3.postagger.model in it. \n3. Create a folder named lth in model folder \nand put downloaded file CoNLL2009-ST-\nEnglish-ALL.anna-3.3.srl-4.1.srl.model in \nit. \n4. Create a folder named opennlp in model \nfolder and put downloaded file en-\nsent.bin in it. \n5. Compress the model folder to model.zip. \nVirtual Machine. \n1. Start virtual machine. \n2. Create a folder named koshik_test. \n3. Copy \nenwiki-20160501-pages-\narticles.xml.bz2 to koshik_test folder. \n4. Copy downloaded KOSHIK to koshik_test \nfolder. \n5. Copy model.zip to koshik_test folder. \nHadoop \n1. Under koshik_test folder, copy enwiki-\n20160501-pages-articles.xml.bz2 \nto \nHadoop file system. \n5 \n \n \n4.3. Testing \nThe following show the steps to test KOSHIK to analyze the WIKI data. Steps 4, 5, and 6 are similar \nto those mentioned by Nugues (2014).  \n \nStep \nDescription \nCommand \n1 \nImport WIKI data \ninto KOSHIK. \nhadoop jar Koshik-1.0.1.jar se.lth.cs.koshik.util.Import -input \n/enwiki-20160501-pages-articles.xml -inputformat wikipedia -\nlanguage eng -charset utf-8 -output /enwiki_avro \n2 \nStart KOSHIK map-\nreduce \njobs \nto \nanalyze the data. \nhadoop jar Koshik-1.0.1.jar se.lth.cs.koshik.util.EnglishPipeline -D \nmapred.reduce.tasks=12 -D mapred.child.java.opts=-Xmx8G -\narchives model.zip -input /enwiki_avro -output \n/enwiki_semantic \n3 \nImport \nthe \nanalyzed \nresult \ninto \nHive \nfor \nquerying. \nCREATE EXTERNAL TABLE koshikdocs ROW FORMAT SERDE \n'org.apache.hadoop.hive.serde2.avro.AvroSerDe' \nSTORED \nAS \nINPUTFORMAT \n'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' \nOUTPUTFORMAT \n'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' \nLOCATION \n'/hivetablekoshik' \nTBLPROPERTIES('avro.schema.url'='hdfs:///AvroDocument.avsc'); \nLOAD DATA INPATH '/enwiki_semantic/*.avro' INTO TABLE \nkoshikdocs; \n4 \nQuery number of \nanalyzed articles. \nSELECT count(identifier) from koshikdocs; \nSELECT count(key) FROM (SELECT explode(ann) AS (key,value) \nFROM \n(SELECT \nann \nFROM \nkoshikdocs \nLATERAL \nVIEW \nexplode(annotations.features) annTable as ann) annmap) \ndecmap WHERE key='POSTAG' AND value LIKE 'NN%'; \n5 \nQuery number of \nsentences \nSELECT \ncount(ann) \nFROM \nkoshikdocs \nLATERAL \nVIEW \nexplode(annotations.layer) annTable as ann WHERE ann LIKE \n'%Sentence'; \n6 \nQuery number of \nnouns. \nSELECT count(key) FROM (SELECT explode(ann) AS (key,value) \nFROM (SELECT ann FROM koshikdocs LATERAL VIEW \nexplode(annotations.features) annTable as ann) annmap) \ndecmap WHERE key='POSTAG' AND value LIKE 'NN%'; \n \n5. Findings: Advantages and Disadvantages of KOSHIK \nKOSHIK provides an architecture which utilizes Hadoop and related tools as well as individual \nNLP tools and language models, and simplifies the construction of a whole NLP system. By adopting \nthis architecture, people who work with an NLP system could focus on their own professional areas. \nFor example, linguists could focus on creating effective language models to improve the accuracy \nof NLP, while algorithm designers could provide high performance NLP analysis components. When \nKOSHIK uses the sentence detection component from Mate and Stanford, it has the potential to \nprocess other human languages in the future, if the NLP tools continue to add new language \nmodels. Another advantage is that KOSHIK supports different kinds of document types, and it is \ngiven related APIs to expand the compatible document types. Moreover, with HDFS, KOSHIK has \nthe ability to analyze a large sets of data with high input and output performance. The data \nprocessing can be passed on to MapReduce, which will provide more computing power as required.  \nCurrently, KOSHIK has not yet become a mature product that is ready to be used by businesses. \n6 \n \nFirst, it lacks support and documentation, and the learning curve for this tool is high. For example, \nthis tool utilizes the language models of Mate, OpenNLP and Stanford CoreNLP. However, there are \ntoo many components for the user to discern which one KOSHIK is using for a particular task. \nPerhaps only its developers can see it from the source code. Secondly, since the last recorded \nchange of code, this tool has not been maintained regularly, and there has not been any update \nfor almost two years. It is uncertain whether this tool will have more functions and a stable version. \n6. Conclusions \nNatural language processing plays an important role in search engines, speech to text \nconversion tools, intelligent assistants, and artificial intelligence. It will continue to influence the \nuser experience on the internet. With more and more data generated, there will be different kinds \nof data processed on Big Data platforms. Hadoop provides useful tools and has a mature ecosystem \nwhich is ideal for natural language processing. There are already some research reports, and \nsoftware tools for natural language processing utilizing Hadoop. KOSHIK is one that provides an \nNLP architecture which utilizes Hadoop and Hive to process large amount of data. It is friendly for \ndevelopers and linguists because it can separate these two types of work and allow each of them \nto focus on their own areas, thereby increasing the performance of NLP system. The architecture \nof KOSHIK is expandable; so it provides APIs to add new functions for the system. By utilizing \nOpenNLP, Mate and Stanford CoreNLP, KOSHIK supports different types of natural language \nprocessing. Documentation for KOSHIK is required to learn this tool, and to enable other \ndevelopers to contribute toward it. However, because the source code has not been updated for a \nwhile, it is not ready for business use, but rather for personal testing and development. \n7. Recommendations \nBased on the architecture of KOSHIK and the process speed of large data, this architecture \ncould increase processing speed by adopting Spark and GPU processing. \nSpark is a cluster computing system maintained by Apache and it supports in-memory \ncomputing. This can improve the data analysis speed as high as possible, compared to the original \nMapReduce method in Hadoop (Zaharia, Chowdhury, Franklin, Shenker & Stoica, 2010). Based on \nthe test results of Zaharia et al. (2010) where they used Spark to analyze a 39 GB wiki data, the \nquery time using Spark was 0.5 to 1 second. The Hadoop query took 35 seconds; therefore, Spark \nis much faster than Hadoop.  \nAt present, GPUs are successfully integrated into Hadoop and MapReduce frameworks which \ncould increase the data processing speed (Yadav, Bhadoria & Suri, 2015). In addition, Yadav, \nBhadoria and Suri (2015) found that there are some libraries such as JCUDA and Java Aparapi that \nprovide APIs to interact with GPUs and extract better performance from them to support high \nperformance computing within Hadoop. \n \n \n \n7 \n \nReferences \nBehzadi, F. (2015). Natural language processing and machine learning: A review. \nInternational Journal of Computer Science and Information Security, 13(9), 101-\n106.  \nCloudera. (2016). Overview of Cloudera and the Cloudera Documentation Set. \nRetrieved from \nhttp://www.cloudera.com/documentation/enterprise/latest/topics/introduction\n.html. \nErturk, E. (2013). The impact of intellectual property policies on ethical attitudes \ntoward internet piracy. Knowledge Management: An International Journal, \n12(1), 101-109.  \nExner, P., & Nugues, P. (2014). KOSHIK: A large-scale distributed computing \nframework for NLP. In 3rd International Conference on Pattern Recognition \nApplications and Methods, 463-470. \nGates, A. F., Natkovich, O., Chopra, S., Kamath, P., Narayanamurthy, S. M., Olston, \nC., ... & Srivastava, U. (2009). Building a high-level dataflow system on top of \nMap-Reduce: The Pig experience. Proceedings of the VLDB Endowment, 2(2), \n1414-1425. \nHashem, I. A. T., Yaqoob, I., Anuar, N. B., Mokhtar, S., Gani, A., & Khan, S. U. (2015). \nThe rise of “big data” on cloud computing: Review and open research issues. \nInformation Systems, 47, 98-115. \nHovy, D., & Spruit, S. (2016). The Social Impact of Natural Language Processing. \nAnnual Meeting of the Association for Computational Linguistics, Berlin, \nGermany.  \nIBM (2012). What is big data. Retrieved from http://www-\n01.ibm.com/software/data/bigdata/what-is-big-data.html. \nIdris, M., Hussain, S., Siddiqi, M. H., Hassan, W., Bilal, H. S., & Lee, S. (2015). MRPack: \nMulti-algorithm execution using compute-intensive approach in MapReduce. \nPLoS One, 10(8). http://dx.doi.org/10.1371/journal.pone.0136259. \nLingad, J., Karimi, S., & Yin, J. (2013). Location extraction from disaster-related \nmicroblogs. In Proceedings of the 22nd international conference on World Wide \nWeb companion, 1017-1020. \nManning, C. D., Surdeanu, M., Bauer, J., Finkel, J. R., Bethard, S., & McClosky, D. \n(2014, June). The Stanford CoreNLP Natural Language Processing Toolkit. ACL \n(System Demonstrations), 55-60. \nMarkham, S. K., Kowolenko, M., & Michaelis, T. L. (2015). Unstructured text analytics \nto support new product development decisions. Research Technology \nManagement, 58(2), 30-38.  \nMell, P. & Grance, T. (2011). The NIST Definition of Cloud Computing: \nRecommendations of the National Institute of Standards and Technology. \nRetrieved from \nhttp://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf. \n8 \n \nMurthy, B. V. R., Padmakar, V., & Reddy, M. A. (2015). Hadoop architecture and its \nfunctionality. International Journal of Computer Science and Information \nSecurity, 13(4), 97-103.  \nNugues, P. (2014). Question answering and the development of the Hajen system. \nRetrieved from \nhttp://cst.ku.dk/projekter/semantikprojekt/arrangementer/information/Copenh\nague20141103.pdf  \nOlston, C., Reed, B., Srivastava, U., Kumar, R., & Tomkins, A. (2008, June). Pig latin: a \nnot-so-foreign language for data processing. In Proceedings of the 2008 ACM \nSIGMOD international conference on Management of data, 1099-1110. \nTaylor, R. C. (2010). An overview of the Hadoop/MapReduce/HBase framework and \nits current applications in bioinformatics. BMC Bioinformatics, 11. \nhttp://dx.doi.org/10.1186/1471-2105-11-S12-S1. \nThusoo, A., Sarma, J. S., Jain, N., Shao, Z., Chakka, P., Anthony, S., ... & Murthy, R. \n(2009). Hive: a warehousing solution over a map-reduce framework. \nProceedings of the VLDB Endowment, 2(2), 1626-1629. \nVerspoor, K., Cohen, K. B., Lanfranchi, A., Warner, C., Johnson, H. L., Roeder, C., . . . \nHunter, L. E. (2012). A corpus of full-text journal articles is a robust evaluation \ntool for revealing differences in performance of biomedical natural language \nprocessing tools. BMC Bioinformatics, 13, 207. http://dx.doi.org/10.1186/1471-\n2105-13-207. \nWhite, T. (2012). Hadoop: The definitive guide. O'Reilly Media: Sepastopol, USA. \nZaharia, M., Chowdhury, M., Franklin, M. J., Shenker, S., & Stoica, I. (2010). Spark: \nCluster Computing with Working Sets. HotCloud, 10, 10-10.  \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2016-08-15",
  "updated": "2016-08-15"
}