{
  "id": "http://arxiv.org/abs/1608.06651v2",
  "title": "Unsupervised, Efficient and Semantic Expertise Retrieval",
  "authors": [
    "Christophe Van Gysel",
    "Maarten de Rijke",
    "Marcel Worring"
  ],
  "abstract": "We introduce an unsupervised discriminative model for the task of retrieving\nexperts in online document collections. We exclusively employ textual evidence\nand avoid explicit feature engineering by learning distributed word\nrepresentations in an unsupervised way. We compare our model to\nstate-of-the-art unsupervised statistical vector space and probabilistic\ngenerative approaches. Our proposed log-linear model achieves the retrieval\nperformance levels of state-of-the-art document-centric methods with the low\ninference cost of so-called profile-centric approaches. It yields a\nstatistically significant improved ranking over vector space and generative\nmodels in most cases, matching the performance of supervised methods on various\nbenchmarks. That is, by using solely text we can do as well as methods that\nwork with external evidence and/or relevance feedback. A contrastive analysis\nof rankings produced by discriminative and generative approaches shows that\nthey have complementary strengths due to the ability of the unsupervised\ndiscriminative model to perform semantic matching.",
  "text": "Unsupervised, Efﬁcient and Semantic Expertise Retrieval\nChristophe Van Gysel\ncvangysel@uva.nl\nMaarten de Rijke\nderijke@uva.nl\nMarcel Worring\nm.worring@uva.nl\nUniversity of Amsterdam, Amsterdam, The Netherlands\nABSTRACT\nWe introduce an unsupervised discriminative model for the task\nof retrieving experts in online document collections. We exclu-\nsively employ textual evidence and avoid explicit feature engineer-\ning by learning distributed word representations in an unsupervised\nway. We compare our model to state-of-the-art unsupervised sta-\ntistical vector space and probabilistic generative approaches. Our\nproposed log-linear model achieves the retrieval performance lev-\nels of state-of-the-art document-centric methods with the low infer-\nence cost of so-called proﬁle-centric approaches. It yields a statis-\ntically signiﬁcant improved ranking over vector space and genera-\ntive models in most cases, matching the performance of supervised\nmethods on various benchmarks. That is, by using solely text we\ncan do as well as methods that work with external evidence and/or\nrelevance feedback. A contrastive analysis of rankings produced\nby discriminative and generative approaches shows that they have\ncomplementary strengths due to the ability of the unsupervised dis-\ncriminative model to perform semantic matching.\n1.\nINTRODUCTION\nThe transition to the knowledge and information economy [1]\nintroduces a great reliance on cognitive capabilities [50]. It is cru-\ncial for employers to facilitate information exchange and to stim-\nulate collaboration [17]. In the past, organizations would set-up\nspecial-purpose database systems for their members to maintain\na proﬁle [7]. However, these systems required employees to be\nproactive. In addition, self-assessments are known to diverge from\nreality [10, 32] and document collections can quickly become prac-\ntically infeasible to manage manually. Therefore, there has been an\nactive interest in automated approaches for constructing expertise\nproﬁles [7, 61] and retrieving experts from an organization’s het-\nerogeneous document repository [15]. Expert ﬁnding (also known\nas expertise retrieval or expert search) addresses the task of ﬁnding\nthe right person with the appropriate skills and knowledge [6]. It\nattempts to provide an answer to the question:\nGiven a topic X, who are the candidates with the most\nexpertise w.r.t. X?\nThis is an electronic version of an Article published in the 25th International\nWorld Wide Web Conference.\nc⃝2016 International World Wide Web Conference Committee.\nWWW 2016, April 11–15, 2016, Montréal, Québec, Canada.\nACM 978-1-4503-4143-1/16/04.\nhttp://dx.doi.org/10.1145/2872427.2882974.\nThe expertise retrieval task gained popularity in the research com-\nmunity during the TREC Enterprise Track [61] and has remained\nrelevant ever since, while broadening to social media and to track-\ning the dynamics of expertise [5, 6, 10, 19, 21–23, 47, 49, 62].\nExisting methods fail to address key challenges: (1) Queries and\nexpert documents use different representations to describe the same\nconcepts [26, 34]. Term mismatches between queries and experts\n[34] occur due to the inability of widely used maximum-likelihood\nlanguage models to make use of semantic similarities between words\n[53]. (2) As the amount of available data increases, the need for\nmore powerful approaches with greater learning capabilities than\nsmoothed maximum-likelihood language models is obvious [63].\n(3) Supervised methods for expertise retrieval [23, 47] were intro-\nduced at the turn of the last decade. However, the acceleration\nof data availability has the major disadvantage that, in the case of\nsupervised methods, manual annotation efforts need to sustain a\nsimilar order of growth. This calls for the further development of\nunsupervised methods. (4) In some expertise retrieval methods, a\nlanguage model is constructed for every document in the collection.\nThese methods lack efﬁcient query capabilities for large document\ncollections, as each query term needs to be matched against ev-\nery document [6]. Our proposed solution has a strong emphasis on\nunsupervised model construction, efﬁcient query capabilities and\nsemantic matching between query terms and candidate experts.\nSpeciﬁcally, we propose an unsupervised log-linear model with\nefﬁcient inference capabilities for the expertise retrieval task. We\nshow that our approach improves retrieval performance compared\nto vector space-based and generative language models, mainly due\nto its ability to perform semantic matching [34]. Our method does\nnot require supervised relevance judgments and is able to learn\nfrom raw textual evidence and document-candidate associations\nalone. The purpose of this work is to provide insight in how dis-\ncriminative language models can improve performance of core re-\ntrieval tasks compared to maximum-likelihood language models.\nTherefore, we avoid explicit feature engineering and the incorpo-\nration of external evidence in this paper. In terms of performance,\nthe current best-performing formal language model [4] exhibits a\nworst-case time complexity linear in the size of the document col-\nlection. In contrast, the inference time complexity of our approach\nis asymptotically bounded by the number of candidate experts.\nOur research questions are as follows: (1) How does our dis-\ncriminative log-linear model compare to vector space-based meth-\nods and generative language models for the expert retrieval task in\nterms of retrieval performance? (2) What can we learn regarding\nthe different types of errors made by generative and discriminative\nlanguage models? (3) How does the complexity of inference in\nour log-linear model compare to vector-space based and generative\narXiv:1608.06651v2  [cs.IR]  17 Sep 2017\nmodels? (4) How does the log-linear model handle incremental\nindexing and what are its limitations?\nWe contribute: (i) An unsupervised log-linear model with efﬁ-\ncient inference capabilities for the expertise retrieval task, together\nwith an open-source implementation.1 (ii) Comparison of the re-\ntrieval performance of the log-linear model with traditional vector\nspace-based models and language model methods on well-known\nbenchmarks. (iii) Insights in how certainty in predictions by the\nlog-linear model correlates with performance. (iv) In-depth analy-\nsis of the inference complexity of the log-linear model. (v) Com-\nparative error analysis between the semantic log-linear model and\ntraditional generative language models that perform exact match-\ning. (vi) Insight in the relative strengths of semantic matching and\nexact matching for the expert retrieval task.\nThe remainder of this paper is organized as follows. In Section 2,\nwe brieﬂy discuss related work. Section 3 introduces the log-linear\nmodel for the expert retrieval problem. In Section 4 we state our\nresearch questions and detail our experimental set-up and imple-\nmentations. We provide an overview of our experimental results,\nfollowed by a discussion and analysis in Section 5. Section 6 con-\ncludes the paper and discusses ideas for future work.\n2.\nRELATED WORK\nWe ﬁrst discuss prior work on expert retrieval and its relation\nto document retrieval. Then, we review semantic search methods.\nThe ideas we present in this paper are inspired by neural language\nmodels used in automated speech recognition (ASR) and natural\nlanguage processing (NLP). Consequently, we also review work\nfrom those ﬁelds.\n2.1\nExpert retrieval\nEarly expert retrieval systems were often referred to as expert\nlocator and expertise management systems [38]. These database\nsystems often relied on people to self-assess their expertise against\na predeﬁned set of topics [39], which is known to generate unreli-\nable results [7].\nWith the introduction of the P@NOPTIC system [15], and later\nthe TREC Enterprise track [61], there has been an active research\ninterest in automated expertise proﬁling methods. It is useful to dis-\ntinguish between proﬁle-based methods, which create a textual rep-\nresentation of a candidate’s knowledge, and document-based meth-\nods, which represent candidates as a weighted combination of doc-\numents. The latter generally performs better at ranking, while the\nformer is more efﬁcient as it avoids retrieving all documents rele-\nvant to a query [6, p. 221].\nThere has been much research on generative probabilistic mod-\nels for expert retrieval [21, 49]. Such models have been categorized\nin candidate generation models [13], topic generation models [4, 5]\nand proximity-based variants [5, 54]. Of special relevance to us are\nthe unsupervised proﬁle-centric (Model 1) and document-centric\n(Model 2) models of Balog et al. [4], which focus on raw textual ev-\nidence without incorporating collection-speciﬁc information (e.g.,\nquery modeling, document importance or document structure). Su-\npervised discriminative models [23, 47, 60] are preferred when\nquery-candidate relevance pairs are available for training. Unlike\ntheir generative counterparts these models have no issue combining\ncomplex and heterogeneous features (e.g., link-based features, doc-\nument importance features, etc.); they resemble Learning to Rank\n(L2R) methods for document retrieval [6, 35]. However, a lack\nof training data may greatly hinder their applicability [6, p. 179].\nBeyond unsupervised generative and supervised discriminative ap-\n1https://github.com/cvangysel/SERT\nproaches, there are graph-based approaches based on random walks\n[55] and voting-based approaches based on data fusion [36]. De-\nmartini et al. [19] propose a vector space-based method for the en-\ntity ranking task; their framework extends vector spaces operating\non documents to entities. See [6] for a survey on the topic.\n2.2\nLatent semantic models for document re-\ntrieval\nLi and Xu [34] note that the query document mismatch poses the\nmost critical challenge in search. Semantic matching is an impor-\ntant attempt to remedy this problem. There has been much work\non bridging the semantic gap for various different tasks [19, 26, 28,\n30, 31, 41, 48, 53]. Expertise and document retrieval [6, p. 224] are\nclosely related as performance of the latter can greatly impact that\nof the former [37].\nLatent Semantic Models (LSM) ﬁrst became popular through\nthe introduction of Latent Semantic Indexing (LSI) [18], followed\nby probabilistic LSI (pLSI) [27]. LSMs based on neural networks\n[28, 53, 58] emerged in the last decade. Salakhutdinov and Hinton\n[53] employ unsupervised deep auto-encoders to map documents to\nbit patterns using semantic addressing. Huang et al. [28] perform\nsemantic matching of documents and queries by leveraging click-\nthrough data optimizing for Web document ranking. Further, deep\nmodels [12, 20] have been proposed to learn to rank [35].\n2.3\nNeural language modeling\nLarge-vocabulary neural probabilistic language models for mod-\neling word sequence distributions have become very popular re-\ncently [8, 43, 44]. These models learn continuous-valued distributed\nrepresentations for words, also known as embeddings [42, 45, 48],\nin order to ﬁght the curse of dimensionality and increase general-\nization by introducing the expectation that similar word vectors sig-\nnify semantically or syntactically similar words. Recurrent neural\nlanguage models have shown to perform well in ASR [40]. Col-\nlobert et al. [14] propose a uniﬁed neural network architecture for\nvarious NLP tasks. Even more recently, there has been a surge in\nmultimodal neural language models [31], which lend themselves to\nthe task of automated image captioning.\nWhat we add on top of the related work described above is the\nfollowing. In this work we model the conditional probability of\nthe expertise of a candidate given a single query term (contrary\nto binary relevance given a character-based n-gram [28]). In the\nprocess we learn a distributed vector representation (similar to LSI,\npLSI and semantic hashing) for both words and candidates such\nthat nearby representations indicate semantically similar concepts.\nWe propose a log-linear model that is similar to neural language\nmodels. The important difference is that we predict a candidate\nexpert instead of the next word. To the best of our knowledge,\nwe are the ﬁrst to propose such a solution. We employ an embed-\nding layer in our shallow model for the same reasons as mentioned\nabove: we learn continuous word representations that incorporate\nsemantic and syntactic similarity tailored to an expert’s domain.\n3.\nA LOG-LINEAR MODEL FOR\nEXPERT SEARCH\nIn the setting of this paper we have a document collection D and\na predeﬁned set of candidate experts C (entities to be retrieved).\nDocuments d ∈D are represented as a sequence of words w1, . . . ,\nw|d| originating from a vocabulary V , where wi ∈V and the oper-\nator |·| denotes the document length in tokens. For every document\nd ∈D we write Cd to denote the set of candidates c ∈C asso-\nciated with it (i.e., C = S\nd∈D Cd). These document-candidate\nassociations can be obtained explicitly from document meta-data\n(e.g., the author of an e-mail) or implicitly by mining references\nto candidates from the document text. Notice that some documents\nmight not be associated with any candidate. When presented with a\nquery q of constituent terms t1, ..., t|q|, the expert retrieval task is\nto return a list of candidates ρ(C) ordered according to topical ex-\npertise. We generate this ranking using a relatively shallow neural\nnetwork which directly models P(c | q).\nWe employ vector-based distributed representations [26], for both\nwords (i.e., word embeddings) and candidate experts, in a way that\nmotivates the unsupervised construction of features that express\nregularities of the expertise ﬁnding domain. These representations\ncan capture the similarity between concepts (e.g., words and can-\ndidate experts) by the closeness of their representations in vector\nspace. That is, concepts with similar feature activations are inter-\npreted by the model as being similar, or even interchangeable.\n3.1\nThe model\nTo address the expert search task, we model P(cj | q) and rank\ncandidates cj accordingly for a given q. We propose an unsuper-\nvised, discriminative approach to obtain these probabilities. We\nconstruct our model solely from textual evidence: we do not re-\nquire query-candidate relevance assessments for training and do not\nconsider external evidence about the corpus (e.g., different weight-\nings for different sub-collections), the document (e.g., considering\ncertain parts of the document more useful) nor link-based features.\nLet e denote the size of the vector-based distributed representa-\ntions of both words in V and candidate experts in C. These repre-\nsentations will be learned by the model using gradient descent [42]\n(Section 3.2). For notational convenience, we write P(c | ·) for the\n(conditional) probability distribution over candidates, which is the\nresult of vector arithmetic. We deﬁne the probability of a candidate\ncj given a single word wi ∈V as the log-linear model\nP(c | wi) = 1\nZ1 exp (Wc · (Wp · vi) + bc) ,\n(1)\nwhere Wp is the e × |V | projection matrix that maps the one-hot\nrepresentation (i.e., 1-of-|V |) of word wi, vi, to its e-dimensional\ndistributed representation, bc is a |C|-dimensional bias vector and\nWc is the |C| × e matrix that maps the word embedding to an un-\nnormalized distribution over candidates C, which is then normal-\nized by Z1 = P|C|\nj=1 [exp (Wc · (Wp · vi) + bc)]j. If we consider\nBayes’ theorem, the transformation matrix Wc and bias vector bc\ncan be interpreted as the term log-likelihood log P(wi | c) and\ncandidate log-prior log P(c), respectively. The projection matrix\nWp attempts to soften the curse of dimensionality introduced by\nlarge vocabularies V and maps words to word feature vectors [8].\nSupport for large vocabularies is crucial for retrieval tasks [28, 53].\nWe then assume conditional independence of a candidate’s ex-\npertise given an observation of data (i.e., a word). Given a sequence\nof words w1, . . . , wk we have:\nP(c | w1, . . . , wk) = 1\nZ2\n˜P(c | w1, . . . , wk) = 1\nZ2\nk\nY\ni=1\nP(c | wi)\n= 1\nZ2 exp\n k\nX\ni=1\nlog (P(c | wi))\n!\n(2)\nwhere ˜P(c | w1, . . . , wk) denotes the unnormalized score and\nZ2 = P|C|\nj=1 exp\n\u0010Pk\ni=1 log (P(cj | wi))\n\u0011\nis a normalizing term.\nThe transformation to log-space in (2) is a well-known trick to pre-\nvent ﬂoating point underﬂow [46, p. 445]. Given (2), inference is\nstraight-forward. That is, given query q = t1, . . . , tk, we compute\nP(c | t1, . . . , tk) and rank the candidate experts in descending\norder of probability.\nEq. 1 deﬁnes a neural network with a single hidden layer. We\ncan add additional layers. Preliminary experiments, however, show\nthat the shallow log-linear model (1) performs well-enough in most\ncases. Only for larger data sets did we notice a marginal gain from\nadding an additional layer between projection matrix Wp and the\nsoftmax layer over C (Wc and bias bc), at the expense of longer\ntraining times and loss of transparency.\n3.2\nParameter estimation\nThe matrices Wp, Wc and the vector bc in (1) constitute the\nparameters of our model. We estimate them using error back prop-\nagation [51] as follows. For every document dj ∈D we construct\nan ideal distribution over candidates p = P(c | dj) based on the\ndocument-candidate associations Cdj such that\nP(c | dj) =\n(\n1\n|Cdj |,\nc ∈Cdj\n0,\nc ̸∈Cdj\nWe continue by extracting n-grams where n remains ﬁxed during\ntraining. For every n-gram w1, . . . , wn generated from document\nd we compute ˜p = P(c | w1, . . . , wn) using (2). During model\nconstructing we then optimize the cross-entropy H(p, ˜p) (i.e., the\njoint probability of the training data if |Cdj| = 1 for all j) using\nbatched gradient descent. The loss function for a single batch of m\ninstances with associated targets (p(i), ˜p(i)) is as follows:\nL(Wp, Wc, bc)\n= 1\nm\nm\nX\ni=1\n|dmax|\n|d(i)| H(p(i), ˜p(i))\n+ λ\n2m\n X\ni,j\nW 2\npi,j +\nX\ni,j\nW 2\nci,j\n!\n(3)\n= −1\nm\nm\nX\ni=1\n|dmax|\n|d(i)|\n|C|\nX\nj=1\nP(cj | d(i)) log\n\u0010\nP(cj | w(i)\n1 , . . . , w(i)\nn )\n\u0011\n+ λ\n2m\n X\ni,j\nW 2\npi,j +\nX\ni,j\nW 2\nci,j\n!\n,\nwhere d(i) refers to the document from which n-gram w(i)\n1 , . . . , w(i)\nn\nwas extracted, dmax = arg maxd∈D |d| indicates the longest doc-\nument in the collection, and λ is a weight regularization parameter.\nThe update rule for a particular parameter θ (Wp, Wc or bc) given\na single batch of size m is:\nθ(t+1) = θ(t) −α(t) ⊙∂L(Wp\n(t), Wc\n(t), bc\n(t))\n∂θ\n,\n(4)\nwhere α(t) and θ(t) denote the per-parameter learning rate and pa-\nrameter θ at time t, respectively. The learning rate α consists of\nthe same number of elements as there are parameters; in the case\nof a global learning rate, all elements of α are equal to each other.\nThe derivatives of the loss function (3) are given in the Appendix.\nIn the next section we will discuss our experimental setup, fol-\nlowed by an overview of our experimental results and further anal-\nysis in Section 5.\n4.\nEXPERIMENTAL SETUP\n4.1\nResearch questions\nAs indicated in the introduction, we seek to answer the following\nresearch questions:\nRQ1 How does our discriminative log-linear model compare to\nvector space-based methods and generative language models\nfor the expert retrieval task in terms of retrieval performance?\nIn particular, how does the model perform when compared against\nvector space-based (LSI and TF-IDF) and generative approaches\n(proﬁle-centric Model 1 and document-centric Model 2)?\nRQ2 What can we learn regarding the different types of errors\nmade by generative and discriminative language models?\nDoes the best-performing generative model simply perform slightly\nbetter on the topics for which the other models perform decent as\nwell, or do they make very different errors? If the latter holds,\nan ensemble of the rankings produced by both model types might\nexceed performance of the individual rankings.\nRQ3 How does the complexity of inference in our log-linear model\ncompare to vector-space based and generative models?\nThe worst-case inference cost of document-centric models makes\nthem unattractive in online settings where the set of topics is not\ndeﬁned beforehand and the document collection is large. Proﬁle-\ncentric methods are preferred in such settings as they infer from\none language model per candidate expert for every topic (i.e., a\npseudo-document consisting of a concatenation of all documents\nassociated with an expert) [6]. Vector space-based methods [19]\nhave similar problems due to the curse of dimensionality [29] and\nconsequently their inferential time complexity is likewise asymp-\ntotically bounded by the number of experts.\nRQ4 How does the log-linear model handle incremental indexing\nand what are its limitations?\n4.2\nBenchmarks\nThe proposed method is applicable in the setting of the Expert\nSearch task of the TREC Enterprise track from 2005 to 2008 [61].\nWe therefore evaluate on the W3C and CERC benchmarks released\nby the track. The W3C dataset [16] is a crawl of the W3C’s sites\nin June 2004 (mailing lists, web pages, etc.). The CSIRO Enter-\nprise Research Collection (CERC) [2] is a dump of the intranet\nof Australia’s national science agency. Additionally, we evaluate\nour method on a smaller, more recent benchmark based on the em-\nployee database of Tilburg University (TU) [10], which consists of\nbi-lingual, heterogeneous documents. See Table 1.\nMining document-candidate associations and how they inﬂuence\nperformance has been extensively covered in previous work [4, 6]\nand is beyond the scope of this work. For TU, the associations\nare part of the benchmark. For W3C, a list of possible candidates\nis given and we extract the associations ourselves by performing\na case-insensitive match of full name or e-mail address [4]. For\nCERC, we make use of publicly released associations [3].\nAs evaluation measures we use Mean Average Precision (MAP),\nMean Reciprocal Rank (MRR), Normalized Discounted Cumula-\ntive Gain at rank 100 (NDCG@100) and Precision at rank 5 (P@5)\nand rank 10 (P@10).\n4.3\nBaselines\nWe compare our approach to existing unsupervised methods for\nexpert retrieval that solely rely on textual evidence and static doc-\nument-candidate associations. (1) Demartini et al. [19] propose a\ngeneric framework to adapt vector spaces operating on documents\nto entities. We compare our method to TF-IDF (raw frequency and\ninverse document frequency) and LSI (300 latent topics) variants\nof their vector space model for entity ranking (using cosine sim-\nilarity). (2) In terms of language modeling, Balog et al. [4] pro-\npose two models for expert ﬁnding based on generative language\nmodels. The ﬁrst takes a proﬁle-centric approach comparing the\nlanguage model of every expert to the query, while the second is\ndocument-centric. We consider both models with different smooth-\ning conﬁgurations: Jelinek-Mercer (jm) smoothing with λ = 0.5\n[4] and Dirichlet (d) smoothing with β equal to the average doc-\nument length [5] (see Table 1). Signiﬁcance of results produced\nby the baselines (compared to our method) is determined using a\ntwo-tailed paired randomization test [59].\n4.4\nImplementation details\nThe vocabulary V is constructed from each corpus by ignoring\npunctuation, stop words and case; numbers are replaced by a nu-\nmerical placeholder token. During our experiments we prune V\nby only retaining the 216 most-frequent words so that each word\ncan be encoded by a 16-bit unsigned integer. Incomplete n-gram\ninstances are padded by a special-purpose token.\nIn terms of parameter initialization, we sample the initial matri-\nces Wc and Wp (1) uniformly in the range\n\"\n−\nr\n6.0\nm + n,\nr\n6.0\nm + n\n#\nfor an m × n matrix, as this initialization scheme improves model\ntraining convergence [25], and take the bias vector bc to be null.\nThe projection layer Wp is initialized with pre-trained word repre-\nsentations trained on Google News data [41]; the number of word\nfeatures is set to e = 300, similar to pre-trained representations.\nWe used adadelta (ρ = 0.95, ϵ = 10−6) [64] with batched gra-\ndient descent (m = 1024) and weight decay λ = 0.01 during\ntraining on NVidia GTX480 and NVidia Tesla K20 GPUs. We only\niterate once over the entire training set for each experiment.\n5.\nRESULTS AND DISCUSSION\nWe start by giving a high-level overview of our experimental re-\nsults and then address issues of scalability, provide an error analysis\nand discuss the issue of incremental indexing.\n5.1\nOverview of experimental results\nWe evaluate the log-linear model on the W3C, CERC and TU\nbenchmarks (Section 4.2). During training we extract non-overlap-\nping n-grams for the W3C and CERC benchmarks and overlapping\nn-grams for the TU benchmark. As the TU benchmark is consider-\nably smaller, we opted to use overlapping n-grams to counter data\nsparsity. The architecture of each benchmark model (e.g., num-\nber of candidate experts) is inherently speciﬁed by the benchmarks\nthemselves (see Table 1). However, the choice of n-gram size dur-\ning training remains open. Errors for input w1, . . . , wn are propa-\ngated back through Wc until the projection matrix Wp is reached;\nif a single word wi causes a large prediction error, then this will in-\nﬂuence its neighboring words w1, . . . , wi−1, wi+1, . . . , wn as well.\nTable 1: An overview of the three datasets (W3C, CERC and TU) used for evaluation and analysis.\nW3C\nCERC\nTU\nNumber of documents\n331,037\n370,715\n31,209\nAverage document lengtha\n1,237.23\n460.48\n2,454.93\nNumber of candidatesb\n715\n3,479\n977\nNumber of document-candidate associations\n200,939\n236,958\n36,566\nNumber of documents (with Cd > 0)\n93,826\n123,934\n27,834\nNumber of associations per documentc\n2.14 ± 3.29\n1.91 ± 3.70\n1.13 ± 0.39\nNumber of associations per candidate\n281.03 ± 666.63\n68.11 ± 1,120.74\n37.43 ± 61.00\nQueries\n49\n(2005)\n50\n(2007)\n1,662\n(GT1)\n50\n(2006)\n77\n(2008)\n1,266\n(GT5)\na Measured in number of tokens.\nb Only candidates with at least a single document association are considered.\nc Only documents with at least one association are considered.\nThis allows the model to learn continuous word representations tai-\nlored to the expert retrieval task and the benchmark domain.\nA larger window size has a negative impact on batch throughput\nduring training. We are thus presented with the classic trade-off\nbetween model performance and construction efﬁciency. Notice,\nhowever, that the number of n-grams decreases as the window size\nincreases if we extract non-overlapping instances. Therefore, larger\nvalues of n lead to faster wall-clock time model construction for the\nW3C and CERC benchmarks in our experiments.\nWe sweep over the window width n = 2i (0 ≤i < 6) for all\nthree benchmarks and their corresponding relevance assessments.\nWe report MAP and MRR for every conﬁguration (see Figure 1).\nWe observe a signiﬁcant performance increase between n = 1\nand n = 2 on all benchmarks, which underlines the importance\nof the window size parameter. The increase in MAP implies that\nthe performance achieved is not solely due to initialization with\npre-trained representations (Section 4.4), but that the model efﬁ-\nciently learns word representations tailored to the problem domain.\nThe highest MAP scores are attained for relatively low n. As n\nincreases beyond n = 8 a gradual decrease in MAP is observed on\nall benchmarks. In our remaining experiments we choose n = 8\nregardless of the benchmark.\nWords wi that mainly occur in documents associated with a par-\nticular expert are learned to produce distributions P(c | wi) with\nless uncertainty than words associated with many experts in (1).\nThe product of P(c | wi) in (2) aggregates this expertise evidence\ngenerated by query terms. Hence, queries with strong evidence for\na particular expert should be more predictable than very generic\nqueries. To quantify uncertainty we measure the normalized en-\ntropy [56] of P(c | q):\nη(c | q) = −\n1\nlog(|C|)\n|C|\nX\nj=1\np(cj | q) log(p(cj | q)).\n(5)\nEquation 5 can be interpreted as a similarity measure between the\ngiven distribution and the uniform distribution. Importantly, Fig-\nure 2 shows that there is a statistically signiﬁcant negative correla-\ntion between query-wise normalized entropy and average precision\nfor all benchmarks.\nTable 2 presents a comparison between the log-linear model and\nthe various baselines (Section 4.3). Our unsupervised method sig-\nniﬁcantly (p < 0.01) outperforms the LSI-based method consis-\ntently. In the case of the TF-IDF method and the proﬁle-centric\ngenerative language models (Model 1), we always perform bet-\nter and statistical signiﬁcance is achieved in the majority of cases.\nThe document-centric language models (Model 2) perform slightly\nbetter than our method on two (out of six) benchmarks in terms\nof MAP and NDCG@100:(1) For the CERC 2007 assessment we\nmatch performance of the document-centric generative model with\nJelinek-Mercer smoothing. (2) For TU GT1 the generative coun-\nterpart seems to outperform our method.\nNotice that over all assessments, the log-linear model consis-\ntently outperforms all proﬁle-centric approaches and is only chal-\nlenged by the smoothed document-centric approach. In addition,\nfor the precision-based measures (P@k and MRR), the log-linear\nmodel consistently outperforms all other methods we compare to.\nNext, we turn to a topic-wise comparative analysis of discrimi-\nnative and generative models. After that, we analyze the scalability\nand efﬁciency of the log-linear model and compare it to that of the\ngenerative counterparts, and address incremental indexing.\n5.2\nError analysis\nHow does our log-linear model achieve its superior performance\nover established generative models? Figure 3 depicts the per-topic\ndifferences in average precision between the log-linear model and\nModel 2 (with Jelinek-Mercer smoothing) on all benchmarks. For\neach plot, the vertical bars with a positive AP difference correspond\nto test topics for which the log-linear model outperforms Model 2\nand vice versa for bars with a negative AP difference.\nThe beneﬁt gained from the projection matrix Wp is two-fold.\nFirst, it avoids the curse of dimensionality introduced by large vo-\ncabularies. Second, term similarity with respect to the expertise\ndomain is encoded in latent word features. When examining words\nnearby query terms in the embedding space, we found words to be\nrelated to the query term. For example, word vector representations\nof xml and nonterminal are very similar for the W3C benchmark (l2\nnorm). This can be further observed in Figure 1: log-linear models\ntrained on single words perform signiﬁcantly worse compared to\nthose that are able to learn from neighboring words.\nWe now take a closer look at the topics for which the log-linear\nmodel outperforms Model 2 and vice versa. More speciﬁcally, we\ninvestigate textual evidence related to a topic and whether it is con-\nsidered relevant by the benchmark. For the log-linear model, we ex-\namine terms nearby topic terms in Wp (l2-norm), as these terms are\nconsidered semantically similar by the model and provide a means\nfor semantic matching. For every benchmark, we ﬁrst consider top-\nTable 2: Evaluation results for models trained on the W3C, CERC and TU benchmarks. Sufﬁxes (d) and (jm) denote Dirichlet and\nJelinek-Mercer smoothing, respectively (Section 4.3). Signiﬁcance of results is determined using a two-tailed paired randomization\ntest [59] (∗∗∗p < 0.01; ∗∗p < 0.05; ∗p < 0.1) with respect to the log-linear model (adjusted using the Benjamini-Hochberg\nprocedure for multiple testing [9]).\nW3C\n2005\n2006\nMAP NDCG@100 MRR\nP@5\nP@10\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nLSI\n0.135\n0.266\n0.306\n0.192 0.196\n0.245\n0.371\n0.482\n0.287\n0.338\nTF-IDF\n0.243\n0.426\n0.541\n0.384 0.350\n0.343\n0.531\n0.650\n0.492\n0.498\nModel 1 (d)\n0.192\n0.358\n0.433\n0.276 0.266\n0.321\n0.491\n0.635\n0.478\n0.449\nModel 1 (jm)\n0.190\n0.352\n0.390\n0.272 0.276\n0.311\n0.483\n0.596\n0.502\n0.437\nModel 2 (d)\n0.198\n0.369\n0.429\n0.288 0.272\n0.261\n0.419\n0.551\n0.441\n0.404\nModel 2 (jm)\n0.211\n0.380\n0.451\n0.332 0.296\n0.260\n0.423\n0.599\n0.449\n0.429\nLog-linear (ours) 0.248\n0.444\n0.618∗0.412\n0.361\n0.484∗∗∗\n0.667∗∗∗\n0.833∗∗∗\n0.713∗∗∗\n0.644∗∗∗\nCERC\n2007\n2008\nMAP NDCG@100 MRR\nP@5\nP@10\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nLSI\n0.031\n0.107\n0.060\n0.016 0.014\n0.038\n0.099\n0.106\n0.042\n0.055\nTF-IDF\n0.332\n0.486\n0.463\n0.196 0.141\n0.269\n0.465\n0.525\n0.332\n0.277\nModel 1 (d)\n0.287\n0.427\n0.384\n0.156 0.096\n0.181\n0.355\n0.388\n0.200\n0.172\nModel 1 (jm)\n0.278\n0.420\n0.384\n0.156 0.084\n0.170\n0.347\n0.339\n0.181\n0.159\nModel 2 (d)\n0.352\n0.495\n0.454\n0.180 0.138\n0.264\n0.461\n0.510\n0.281\n0.244\nModel 2 (jm)\n0.361\n0.500\n0.467\n0.192 0.138\n0.274\n0.463\n0.517\n0.278\n0.239\nLog-linear (ours) 0.344\n0.493\n0.513\n0.215\n0.150\n0.342∗∗∗\n0.519∗∗\n0.656∗∗\n0.381∗\n0.299\nTU\nGT1\nGT5\nMAP NDCG@100 MRR\nP@5\nP@10\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nLSI\n0.095\n0.205\n0.153\n0.060 0.051\n0.097\n0.208\n0.129\n0.043\n0.036\nTF-IDF\n0.216\n0.356\n0.324\n0.131 0.097\n0.233\n0.378\n0.288\n0.108\n0.079\nModel 1 (d)\n0.171\n0.308\n0.258\n0.103 0.082\n0.241\n0.385\n0.292\n0.109\n0.081\nModel 1 (jm)\n0.189\n0.325\n0.277\n0.112 0.085\n0.231\n0.373\n0.271\n0.100\n0.075\nModel 2 (d)\n0.154\n0.284\n0.228\n0.087 0.070\n0.191\n0.334\n0.233\n0.084\n0.065\nModel 2 (jm)\n0.234\n0.370\n0.342\n0.136 0.101\n0.253\n0.394\n0.302\n0.108\n0.081\nLog-linear (ours) 0.219\n0.356\n0.351\n0.145∗0.105\n0.287∗∗∗\n0.425∗∗∗\n0.363∗∗∗\n0.134∗∗∗\n0.092∗∗∗\nics where exact matches (Model 2) perform best, followed by ex-\namples which beneﬁt from semantic matching (log-linear model).\nTopic identiﬁers are between parentheses.\nW3C Topics P3P speciﬁcation and CSS3 (EX8 and EX69, respec-\ntively) should return candidates associated with the deﬁnition\nof these standards. The log-linear model, however, considers\nthese close to related technologies such as CSS2 for CSS3\nand UTF-8 for P3P. Semantic matching works for topics Se-\nmantic Web Coordination and Annotea server protocol (EX1\nand EX103), where the former is associated with RDF li-\nbraries, RDF-related jargon and the names of researchers in\nthe ﬁeld, while the latter is associated with implementations\nof the protocol and the maintainer of the project.\nCERC For CSIRO, topic nanohouse (CE-035) is mentioned in\nmany irrelevant contexts (i.e., spam) and therefore seman-\ntic matching fails. The term ﬁsh oil (CE-126) is quickly as-\nsociated with different kinds of ﬁsh, oils and organizations\nrelated to marines and ﬁsheries. On the other hand, we ob-\nserve sensor networks (CE-018) to be associated with sen-\nsor/networking jargon and sensor platforms. Topic forensic\nscience workshop (CE-103) expands to syntactically-similar\nterms (e.g., plural), the names of science laboratories and ref-\nerences\nto\nsupport/law-protection\norganizations.\nTU The TU benchmark contains both English and Dutch textual\nevidence. Topics sustainable tourism and interpolation (1411\nand 4882) do not beneﬁt from semantic matching due to a\nsemantic gap: interpolation is associated with the polyno-\nmial kind while the relevance assessments focus on stochas-\ntic methods. Interestingly, for the topic law and informati-\nzation/computerization (1719) we see that the Dutch trans-\nlation of law is very closely related. Similar terms to in-\nformatization are, according to the log-linear model, Dutch\nwords related to cryptography. Similar dynamics are at work\nfor legal-political space (12603), where translated terms and\nsemantic-syntactic relations aid performance.\nIn order to further quantify the effect of the embedding matrix Wp,\nwe artiﬁcially expand benchmark topic terms by k nearby terms.\nWe then examine how the performance of a proﬁle-centric gener-\native language model [4, Model 1] evolves for different values of\nk (Figure 4). The purpose of this analysis is to provide further\ninsight in the differences between maximum-likelihood language\nmodels and the log-linear model. Figure 4 shows that, for most\nbenchmarks, MAP increases as k goes up. Interestingly enough,\nthe two benchmarks that exhibit a decrease in MAP for larger k\n(CERC 2007 and TU GT1) are likewise those for which generative\nlanguage models outperform the log-linear model in Table 2. This\nsuggests that the CERC 2007 and TU GT1 benchmarks require ex-\n1 2\n4\n8\n16\n32\nWindow size\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMAP\n2005\n2006\n(a) W3C\n1 2\n4\n8\n16\n32\nWindow size\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nMAP\n2007\n2008\n(b) CERC\n1 2\n4\n8\n16\n32\nWindow size\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nMAP\nGT1\nGT5\n(c) TU\n1 2\n4\n8\n16\n32\nWindow size\n0.0\n0.2\n0.4\n0.6\n0.8\nMRR\n2005\n2006\n(d) W3C\n1 2\n4\n8\n16\n32\nWindow size\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nMRR\n2007\n2008\n(e) CERC\n1 2\n4\n8\n16\n32\nWindow size\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nMRR\nGT1\nGT5\n(f) TU\nFigure 1: Sensitivity analysis for window size (n-gram) during parameter estimation (3) for W3C, CERC and TU benchmarks.\nact term matching, while the remaining four benchmarks beneﬁt\ngreatly from the semantic matching provided by our model.\nThe per-topic differences suggest that Model 2 and the log-linear\nmodel make very different errors: Model 2 excels at retrieving ex-\nact query matches, while the log-linear model performs seman-\ntic matching. Based on these observations we hypothesize that a\ncombination of the two approaches will raise retrieval performance\neven further. To test this hypothesis, we propose a simple ensem-\nble of rankings generated by Model 2 and the log-linear model by\nre-ranking candidates according to the multiplicatively-combined\nreciprocal rank:\nrankensemble(cj, qi) ∝\n1\nrankmodel 2(cj, qi) ·\n1\nranklog-linear(cj, qi), (6)\nwhere rankM(cj, qi) denotes the position of candidate cj in a rank-\ning generated by model M for answering query qi. Equation (6) is\nequivalent to performing data fusion using CombSUM [57] where\nthe scores are given by the logarithm of the reciprocal ranks of the\nexperts. Table 3 compares the result of this ensemble to that of its\nconstituents. Compared to the supervised methods of Fang et al.\n[23], we conclude that our fully unsupervised ensemble matches\nthe performance of their method on the CERC 2007 benchmark\nand outperforms their method on the W3C 2005 benchmark. The\nsuperior performance of the ensemble suggests the viability of hy-\nbrid methods that combine semantic and exact matching.\n5.3\nScalability and efﬁciency\nInference in the log-linear model is expressed in linear algebra\noperations (Section 3). These operations can be efﬁciently per-\nformed by highly optimized software libraries and special-purpose\nhardware (i.e., GPUs). But the baseline methods against which we\ncompare do not beneﬁt from these speed-ups. Furthermore, many\nimplementation-speciﬁc details and choice of parameter values can\ninﬂuence runtime considerably (e.g. size of the latent representa-\ntions). Therefore, we opt for a theoretical comparison of the infer-\nence complexity of the log-linear model and compare these to the\nbaselines (Section 4.3).\nThe log-linear model generates a ranking of candidate experts by\nstraight-forward matrix operations. The look-up operation in the\nprojection matrix Wp occurs in constant time complexity, as the\nmultiplication with the one-hot vector vi comes down to selecting\nthe i-th column from Wp. Multiplication of the |C| × e matrix\nWc with the e-dimensional word feature vector exhibits O(|C| · e)\nruntime complexity. If we consider addition of the bias term and\ndivision by the normalizing function Z1, the time complexity of (1)\nbecomes\nO(|C| · (e + (e −1))\n|\n{z\n}\nmatrix-vector multiplication\n+ |C|\n|{z}\nbias term\n+ 2 · |C| −1\n|\n{z\n}\nZ1\n).\nNotice, however, that the above analysis considers sequential exe-\ncution. Modern computing hardware has the ability to parallelize\ncommon matrix operations [24, 33]. The number of candidate ex-\nperts |C| is the term that impacts performance most in the log-linear\nmodel (under the assumption that |C| ≫e).\nIf we consider n terms, where n is the query length during infer-\nence or the window size during training, then the complexity of (2)\nbecomes\nO(n · |C| · (2 · e −1) + n · (3 · |C| −1)\n|\n{z\n}\nn forward-passes\n+ (n −1) · |C|\n|\n{z\n}\nfactor product\n+ 2 · |C| −1\n|\n{z\n}\nZ2\n)\nNotice that Z2 does not need to be computed during inference as it\ndoes not affect the candidate expert ranking.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nη(c|q)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAP\n(a) W3C (R = −0.39∗∗∗)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nη(c|q)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAP\n(b) CERC (R = −0.44∗∗∗)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nη(c|q)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAP\n(c) TU (R = −0.30∗∗∗)\nFigure 2: Scatter plot of the normalized entropy of distribution P(c | q) (2) returned by the log-linear model and per-query av-\nerage precision for W3C, CERC and TU benchmarks. Pearson’s R and associated p-value (two-tailed paired permutation test:\n∗∗∗p < 0.01; ∗∗p < 0.05; ∗p < 0.1) are between parentheses. The depicted linear ﬁt was obtained using an ordinary least squares\nregression.\nTable 3: Comparison of Model 2, the log-linear model and an ensemble of the former on W3C, CERC and TU benchmarks. Signiﬁ-\ncance of results is determined using a two-tailed paired randomization test [59] (∗∗∗p < 0.01; ∗∗p < 0.05; ∗p < 0.1) with respect to\nthe ensemble ranking (adjusted using the Benjamini-Hochberg procedure for multiple testing [9]).\nW3C\n2005\n2006\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nModel 2 (jm)\n0.211\n0.380\n0.451\n0.332\n0.296\n0.260\n0.423\n0.599\n0.449\n0.429\nLog-linear (ours)\n0.248\n0.444\n0.618\n0.412\n0.361\n0.484∗∗∗\n0.667∗∗\n0.833\n0.713∗∗\n0.644∗∗\nEnsemble\n0.291∗∗∗\n0.479∗∗\n0.668\n0.440\n0.378\n0.433\n0.634\n0.825\n0.657\n0.586\nCERC\n2007\n2008\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nModel 2 (jm)\n0.361\n0.500\n0.467\n0.192\n0.138\n0.274\n0.463\n0.517\n0.278\n0.239\nLog-linear (ours)\n0.344\n0.493\n0.513\n0.215\n0.150\n0.342\n0.519\n0.656\n0.381\n0.299\nEnsemble\n0.452∗∗\n0.589∗∗∗\n0.627∗∗∗\n0.248∗\n0.160\n0.395∗∗∗\n0.593∗∗∗\n0.716\n0.459∗∗\n0.357∗∗∗\nTU\nGT1\nGT5\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nMAP\nNDCG@100\nMRR\nP@5\nP@10\nModel 2 (jm)\n0.234\n0.370\n0.342\n0.136\n0.101\n0.253\n0.394\n0.302\n0.108\n0.081\nLog-linear (ours)\n0.219\n0.356\n0.351\n0.145\n0.105\n0.287\n0.425\n0.363\n0.134\n0.092\nEnsemble\n0.271∗∗∗\n0.417∗∗∗\n0.403∗∗∗\n0.165∗∗∗\n0.121∗∗∗\n0.331∗∗∗\n0.477∗∗∗\n0.402∗∗∗\n0.156∗∗∗\n0.105∗∗∗\nIn terms of space complexity, parameters Wp, Wc and bc, in ad-\ndition to the intermediate results, all require memory space propor-\ntional to their size. Considering (2) for a sequence of k words and\nbatches of m instances, we require O(m·k·|C|) ﬂoating point num-\nbers for every forward-pass to ﬁt in-memory. While such an upper\nbound seems reasonable by modern computing standards, it is a\nseverely limiting factor when considering large-scale communities\nand while utilizing limited-memory GPUs for fast computation.\nThe inferential complexity of the vector space-based models for\nentity retrieval [19] depends mainly on the dimensionality of the\nvectors and the number of candidate experts. The dimensionality\nof the latent entity representations is too high for efﬁcient nearest\nneighbor retrieval [29] due to the curse of dimensionality. There-\nfore, the time complexity for the LSI- and TF-IDF-based vector\nspace models are respectively O(γ · |C|) and O(|V | · |C|), where\nγ denotes the number of latent topics in the LSI-based method. As\nhyperparameters e and γ both indicate the dimensionality of la-\ntent entity representations, the time complexity of the LSI-based\nmethod is comparable to that of the log-linear model. We note that\n|V | ≫|C| for all benchmarks (|V | is between 18 to 91 times larger\nthan |C|) we consider in this paper and therefore conclude that the\nTF-IDF method loses to the log-linear model in terms of efﬁciency.\nCompared to the unsupervised generative models of Balog et al.,\nwe have the proﬁle-centric Model 1 and the document-centric Mo-\ndel 2 with inference time complexity O(n · |C|) and O(n · |D|),\nrespectively, with |D| ≫|C|. In the previous section we showed\nthat the log-linear model always performs better than Model 1 and\nnearly always outperforms Model 2. Hence, our log-linear model\ngenerally achieves the expertise retrieval performance of Model 2\n(or higher) at the complexity cost of Model 1 during inference.\n5.4\nIncremental indexing\nExisting unsupervised methods use well-understood maximum-\nlikelihood language models that support incremental indexing. We\nnow brieﬂy discuss the incremental indexing capabilities of our\nproposed method. Extending the set of candidate experts C re-\nquires the log-linear model to be re-trained from scratch as it changes\nthe topology of the network. Moreover, every document associated\nwith a candidate expert is considered as a negative example for all\nother candidates. While it is possible to reiterate over all past doc-\numents and only learn an additional row in matrix Wc, the ﬁnal\noutcome is unpredictable.\nIf we consider a stream of documents instead of a predeﬁned set\nD, the log-linear model can be learned in an online fashion. How-\never, stochastic gradient descent requires that training examples are\n−1.0\n−0.5\n0.0\n0.5\n1.0\n△MAP\n(a) W3C 2005\n−1.0\n−0.5\n0.0\n0.5\n1.0\n△MAP\n(b) CERC 2007\n−1.0\n−0.5\n0.0\n0.5\n1.0\n△MAP\n(c) TU GT1\n−1.0\n−0.5\n0.0\n0.5\n1.0\n△MAP\n(d) W3C 2006\n−1.0\n−0.5\n0.0\n0.5\n1.0\n△MAP\n(e) CERC 2008\n−1.0\n−0.5\n0.0\n0.5\n1.0\n△MAP\n(f) TU GT5\nFigure 3: Difference of average precision between log-linear model and Model 2 [4] with Jelinek-Mercer smoothing per topic for\nW3C, CERC and TU benchmarks.\npicked at random such that the batched update rule (4) behaves like\nthe empirical expectation over the full training set [11]. While we\nmight be able to justify the assumption that documents arrive ran-\ndomly, the n-grams extracted from those documents clearly violate\nthis requirement.\nConsidering a stream of documents leads to the model forgetting\nexpertise evidence as an (artiﬁcial) shift in the underlying distribu-\ntion of the training data occurs. While such behavior is undesirable\nfor the task considered in this paper, it might be well-suited for\ntemporal expert ﬁnding [22, 52], where expertise drift over time\nis considered. However, temporal expertise ﬁnding is beyond the\nscope for this paper and left for future work.\n6.\nCONCLUSIONS\nWe have introduced an unsupervised discriminative, log-linear\nmodel for the expert retrieval task. Our approach exclusively em-\nploys raw textual evidence. Future work can focus on improving\nperformance by feature engineering and incorporation of external\nevidence. Furthermore, no relevance feedback is required during\ntraining. This renders the model suitable for a broad range of ap-\nplications and domains.\nWe evaluated our model on the W3C, CERC and TU bench-\nmarks and compared it to state-of-the-art vector space-based en-\ntity ranking (based on LSI and TF-IDF) and language modeling\n(proﬁle-centric and document-centric) approaches. The log-linear\nmodel combines the ranking performance of the best maximum-\nlikelihood language modeling approach (document-centric) with\ninference time complexity linear in the number of candidate ex-\nperts. We observed a notable increase in precision over existing\nmethods. Analysis of our model’s output reveals a negative corre-\nlation between the per-query performance and ranking uncertainty:\nhigher conﬁdence (i.e., lower entropy) in the rankings produced by\nour approach often occurs together with higher rank quality.\nAn error analysis of the log-linear model and traditional lan-\nguage models shows that the two make very different errors. These\nerrors are mainly due to the semantic gap between query intent and\nthe raw textual evidence. Some benchmarks expect exact query\nmatches, others are helped by our semantic matching. An ensemble\nof methods employing exact and semantic matching generally out-\nperforms the individual methods. This observation calls for further\nresearch in the area of combining exact and semantic matching.\nOne current limitation of our work is its scalability with respect\nto the number of candidate experts. We have started investigating\ntrade-offs between model performance and time/space complexity.\nIn the future we hope to apply scalable variants of this method on\nlarge-scale social media communities, for the purpose of determin-\ning topic ownership. While in this work we focus on expertise re-\ntrieval, the ideas we proposed can easily be transferred to the more\ngeneral entity retrieval task. Moreover, our approach is likely to\nbe applicable to authorship attribution and various other entity re-\ntrieval and prediction tasks.\nAcknowledgments. We thank Isaac Sijaranamual, Manos Tsagkias, Tom\nKenter, Zhaochun Ren, Ke Tran and Katja Hoffman for their useful com-\nments and insights.\nThis research was supported by Amsterdam Data Science, the Dutch\nnational program COMMIT, Elsevier, the European Community’s Seventh\nFramework Programme (FP7/2007-2013) under grant agreement nr 312827\n(VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch\nAcademy of Sciences (KNAW) under the Elite Network Shifts project, the\nMicrosoft Research Ph.D. program, the Netherlands eScience Center un-\nder project number 027.012.105, the Netherlands Institute for Sound and\nVision, the Netherlands Organisation for Scientiﬁc Research (NWO) under\nproject nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.-\n930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, the Yahoo Faculty\nResearch and Engagement Program, and Yandex. All content represents the\nopinion of the authors, which is not necessarily shared or endorsed by their\nrespective employers and/or sponsors.\nComputing resources were provided by the Netherlands Organisation for\nScientiﬁc Research (NWO) through allocation SH-322-15 of the Cartesius\nsystem and by the Advanced School for Computing and Imaging (ASCII)\nby allocation of the Distributed ASCII Supercomputer 4 (DAS-4) system.\n0\n1\n3\n5\n10\nk\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nMAP\n2005\n2006\n(a) W3C\n0\n1\n3\n5\n10\nk\n0.15\n0.20\n0.25\n0.30\nMAP\n2007\n2008\n(b) CERC\n0\n1\n3\n5\n10\nk\n0.15\n0.20\n0.25\n0.30\nMAP\nGT1\nGT5\n(c) TU\nFigure 4: Effect of query expansion by adding nearby terms in Wp (1) in traditional language models (Model 1 [4] with Jelinek-\nMercer smoothing) for W3C, CERC and TU benchmarks.\nAPPENDIX\nThe derivative of (3) w.r.t. bias term bc equals\n∂L(Wp, Wc, bc)\n∂bc\n=\n−1\nm\n \nm\nX\ni=1\n|dmax|\n|d(i)|\n|C|\nX\nj=1\nP(cj|d(i))\n∂log\n\u0010\nP(cj | w(i)\n1 , . . . , w(i)\nn )\n\u0011\n∂bc\n!\nand w.r.t. an arbitrary matrix parameter θ (Wp or Wc):\n∂L(Wp, Wc, bc)\n∂θ\n=\n−1\nm\n \nm\nX\ni=1\n|dmax|\n|d(i)|\n|C|\nX\nj=1\nP(cj|d(i))\n∂log\n\u0010\nP(cj | w(i)\n1 , . . . , w(i)\nn )\n\u0011\n∂θ\n!\n+ λ\nm\nX\ni,j\nθi,j.\nFurther differentiation for parameter θ (Wp, Wc or bc):\n∂log (P(cj | w1, . . . , wn))\n∂θ\n=\n1\nP(cj | w1, . . . , wn)\n∂P(cj | w1, . . . , wn)\n∂θ\n∂P(cj | w1, . . . , wn)\n∂θ\n=\n∂˜\nP (cj|w1,...,wn)\n∂θ\nZ2 −˜P(cj | w1, . . . , wn) ∂Z2\n∂θ\nZ2\n2\n∂Z2\n∂θ\n=\nX\nk\n∂˜P(ck | w1, . . . , wn)\n∂θ\n∂˜P(cj | w1, . . . , wn)\n∂θ\n=\nX\nk\n∂P(cj | wk)\n∂θ\nY\ni̸=k\nP(cj | wi)\nFor a given candidate cj and word wi, following (1) we have\nP(cj | wi)\n=\n˜P(cj | wi)\nZ1\n=\nexp\n\u0000\u0000Pe\nk=1 Wcj,kWpk,i\n\u0001\n+ bcj\n\u0001\nP|C|\nl=1 exp\n\u0000\u0000Pe\nk=1 Wcl,kWpk,i\n\u0001\n+ bcl\n\u0001\nand consequently, with W ⊤\npi denoting the i-th column of matrix\nWp,\n∂P(cj | wi)\n∂Wcj\n=\n\u0010\nZ1 −˜P(cj | wi)\n\u0011\n˜P(cj | wi)W ⊤\npi\nZ2\n1\n∂P(cj | wi)\n∂bcj\n=\n\u0010\nZ1 −˜P(cj | wi)\n\u0011\n˜P(cj | wi)\nZ2\n1\n∂P(cj | wi)\n∂W ⊤\npi\n=\n\u0010\nWcj −P|C|\nl=1 Wcl\n\u0011\n˜P(cj | wi)\nZ1\n(7)\nAs can be seen in (7), the distributed representations of candidates\ncj at time t + 1 are updated using the representation of words wi at\ntime t and vice versa.\nREFERENCES\n[1] The knowledge-based economy. Techn. report, Organisation for\nEconomic Co-operation and Development, 1996.\n[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview\nof the TREC 2007 enterprise track. In TREC, 2007.\n[3] K. Balog. People Search in the Enterprise. PhD thesis,\nUniversity of Amsterdam, 2008.\n[4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for\nexpert ﬁnding in enterprise corpora. In SIGIR, pages 43–50,\n2006.\n[5] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling\nframework for expert ﬁnding. IPM, 45:1–19, 2009.\n[6] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si.\nExpertise retrieval. Found. & Tr. in Information Retrieval, 6\n(2-3):127–256, 2012.\n[7] I. Becerra-Fernandez. Role of artiﬁcial intelligence technologies\nin the implementation of People-Finder knowledge management\nsystems. Knowledge-Based Systems, 13(5):315–320, 2000.\n[8] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural\nprobabilistic language model. JMLR, 3:1137–1155, 2003.\n[9] Y. Benjamini and Y. Hochberg. Controlling the false discovery\nrate: a practical and powerful approach to multiple testing.\nJSTOR, pages 289–300, 1995.\n[10] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and A. van den\nBosch. On the assessment of expertise proﬁles. JASIST, 64(10):\n2024–2044, 2013.\n[11] L. Bottou. Large-scale machine learning with stochastic gradient\ndescent. In COMPSTAT, pages 177–186. Springer, 2010.\n[12] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,\nN. Hamilton, and G. Hullender. Learning to rank using gradient\ndescent. In ICML, pages 89–96, 2005.\n[13] Y. Cao, J. Liu, S. Bao, and H. Li. Research on Expert Search at\nEnterprise Track of TREC 2005. In TREC, pages 2–5, 2005.\n[14] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa. Natural language processing (almost) from\nscratch. JMLR, 12(Aug):2493–2537, 2011.\n[15] N. Craswell, D. Hawking, A.-M. Vercoustre, and P. Wilkins.\nP@noptic expert: Searching for experts not just for documents.\nIn Ausweb Poster Proceedings, pages 21–25, 2001.\n[16] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the\nTREC 2005 enterprise track. In TREC, 2005.\n[17] T. H. Davenport and L. Prusak. Working Knowledge. Harvard\nBusiness Review Press, 1998.\n[18] S. C. Deerwester, S. T. Dumais, and R. A. Harshman. Indexing\nby latent semantic analysis. JASIS, 1990.\n[19] G. Demartini, J. Gaugaz, and W. Nejdl. A vector space model\nfor ranking entities and its application to expert search. In ECIR,\npages 189–201. Springer, 2009.\n[20] L. Deng, X. He, and J. Gao. Deep stacking networks for\ninformation retrieval. In ICASSP, pages 3153–3157, 2013.\n[21] H. Fang and C. Zhai. Probabilistic models for expert ﬁnding. In\nECIR, pages 418–430, Berlin, Heidelberg, 2007.\nSpringer-Verlag.\n[22] Y. Fang and A. Godavarthy. Modeling the dynamics of personal\nexpertise. In SIGIR, pages 1107–1110, 2014.\n[23] Y. Fang, L. Si, and A. P. Mathur. Discriminative models of\nintegrating document evidence and document-candidate\nassociations for expert search. In SIGIR, pages 683–690, 2010.\n[24] K. Fatahalian, J. Sugerman, and P. Hanrahan. Understanding the\nefﬁciency of gpu algorithms for matrix-matrix multiplication. In\nSIGGRAPH HWWS, pages 133–137. ACM, 2004.\n[25] X. Glorot and Y. Bengio. Understanding the difﬁculty of training\ndeep feedforward neural networks. In AISTATS, pages 249–256,\n2010.\n[26] G. E. Hinton. Learning distributed representations of concepts.\nIn 8th Annual Conference of the Cognitive Science Society,\nvolume 1, page 12, Amherst, MA, 1986.\n[27] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR,\npages 50–57. ACM, 1999.\n[28] P.-s. Huang, N. M. A. Urbana, X. He, J. Gao, L. Deng, A. Acero,\nand L. Heck. Learning deep structured semantic models for web\nsearch using clickthrough data. In CIKM, pages 2333–2338,\n2013.\n[29] P. Indyk and R. Motwani. Approximate nearest neighbors:\ntowards removing the curse of dimensionality. In STOC, pages\n604–613. ACM, 1998.\n[30] M. Karimzadehgan and C. Zhai. Estimation of statistical\ntranslation models based on mutual information for ad hoc\ninformation retrieval. In SIGIR, pages 323–330. ACM, 2010.\n[31] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural\nlanguage models. In ICML, pages 595–603, 2014.\n[32] J. Kruger and D. Dunning. Unskilled and unaware of it: how\ndifﬁculties in recognizing one’s own incompetence lead to\ninﬂated self-assessments. J. Personality and Social Psych., 77\n(6):1121, 1999.\n[33] J. Krüger and R. Westermann. Linear algebra operators for gpu\nimplementation of numerical algorithms. ACM Transactions on\nGraphics, 22(3):908–916, 2003.\n[34] H. Li and J. Xu. Semantic matching in search. Found. & Tr. in\nInformation Retrieval, 7(5):343–469, June 2014.\n[35] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer,\n2011.\n[36] C. MacDonald and I. Ounis. Voting for candidates: adapting\ndata fusion techniques for an expert search task. In CIKM, pages\n387–396, 2006.\n[37] C. Macdonald and I. Ounis. Expert search evaluation by\nsupporting documents. In ECIR, pages 555–563. Springer, 2008.\n[38] M. T. Maybury. Expert ﬁnding systems. Techn. Report\nMTR-06B000040, MITRE, 2006.\n[39] D. W. McDonald and M. S. Ackerman. Expertise recommender.\nIn CSCW, pages 231–240, 2000.\n[40] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and\nS. Khudanpur. Recurrent neural network based language model.\nIn Interspeech, pages 1045–1048, 2010.\n[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed\nrepresentations of words and phrases and their compositionality.\nIn NIPS, pages 3111–3119, 2013.\n[42] T. Mikolov, G. Corrado, K. Chen, and J. Dean. Efﬁcient\nestimation of word representations in vector space. arXiv\n1301.3781, 2013.\n[43] A. Mnih and G. Hinton. Three new graphical models for\nstatistical language modelling. In ICML, pages 641–648, 2007.\n[44] A. Mnih and G. Hinton. A scalable hierarchical distributed\nlanguage model. In NIPS, pages 1081–1088, 2008.\n[45] A. Mnih and K. Kavukcuoglu. Learning word embeddings\nefﬁciently with noise-contrastive estimation. In NIPS, pages\n2265–2273, 2013.\n[46] G. Montavon, G. B. Orr, and K.-R. Müller. Neural Networks:\nTricks of the Trade. Springer, 2012.\n[47] C. Moreira, B. Martins, and P. Calado. Using rank aggregation\nfor expert search in academic digital libraries. In Simpósio de\nInformática, INForum, pages 1–10, 2011.\n[48] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global\nVectors for Word Representation. In EMNLP, pages 1532–1543,\n2014.\n[49] D. Petkova and W. B. Croft. Hierarchical language models for\nexpert ﬁnding in enterprise corpora. In ICTAI ’06, pages\n599–606, 2006.\n[50] W. W. Powell and K. Snellman. The knowledge economy.\nAnnual review of sociology, pages 199–220, 2004.\n[51] D. Rumelhart, G. Hinton, and R. Williams. Learning internal\nrepresentations by back propagation. In Parallel Distributed\nProcessing, pages 318–362. MIT Press, 1986.\n[52] J. Rybak, K. Balog, and K. Nørvåg. Temporal expertise\nproﬁling. In ECIR, pages 540–546. Springer, 2014.\n[53] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J.\nApproximate Reasoning, 50(7):969–978, 2009.\n[54] P. Serdyukov and D. Hiemstra. Modeling documents as mixtures\nof persons for expert ﬁnding. In ECIR, pages 309–320. Springer,\n2008.\n[55] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling multi-step\nrelevance propagation for expert ﬁnding. In CIKM, pages\n1133–1142, 2008.\n[56] C. Shannon. A mathematical theory of communication. Bell\nSystem Technical J., 27:379–423, 623–656, 1948.\n[57] J. A. Shaw, E. A. Fox, J. A. Shaw, and E. A. Fox. Combination\nof multiple searches. In TREC, pages 243–252, 1994.\n[58] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent\nsemantic model with convolutional-pooling structure for\ninformation retrieval. In CIKM, pages 101–110, 2014.\n[59] M. D. Smucker, J. Allan, and B. Carterette. A comparison of\nstatistical signiﬁcance tests for information retrieval evaluation.\nIn CIKM, pages 623–632. ACM, 2007.\n[60] P. Sorg and P. Cimiano. Finding the right expert: Discriminative\nmodels for expert retrieval. In KDIR, pages 190–199, 2011.\n[61] TREC. Enterprise Track, 2005–2008.\n[62] D. van Dijk, M. Tsagkias, and M. de Rijke. Early detection of\ntopical expertise in community question and answering. In\nSIGIR, 2015.\n[63] V. Vapnik. Statistical learning theory, volume 1. Wiley New\nYork, 1998.\n[64] M. D. Zeiler. Adadelta: An adaptive learning rate method.\nCoRR, abs/1212.5701, 2012.\n",
  "categories": [
    "cs.IR",
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "published": "2016-08-23",
  "updated": "2017-09-17"
}