{
  "id": "http://arxiv.org/abs/1609.03348v4",
  "title": "A Threshold-based Scheme for Reinforcement Learning in Neural Networks",
  "authors": [
    "Thomas H. Ward"
  ],
  "abstract": "A generic and scalable Reinforcement Learning scheme for Artificial Neural\nNetworks is presented, providing a general purpose learning machine. By\nreference to a node threshold three features are described 1) A mechanism for\nPrimary Reinforcement, capable of solving linearly inseparable problems 2) The\nlearning scheme is extended to include a mechanism for Conditioned\nReinforcement, capable of forming long term strategy 3) The learning scheme is\nmodified to use a threshold-based deep learning algorithm, providing a robust\nand biologically inspired alternative to backpropagation. The model may be used\nfor supervised as well as unsupervised training regimes.",
  "text": "A Threshold-based Scheme for Reinforcement \nLearning in Neural Networks \n \n \nThomas H. Ward \n \n \n \n \n \nthomas.holland.ward@gmail.com \n \n \nAbstract \n \nA generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,\n \n \n \n \n \n \n \n \n \n \n \n \n \nproviding a general purpose learning machine. By reference to a node threshold three features are\n  \n \n \n \n \n \n \n  \n \n \n \n \n \ndescribed 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)\n \n  \n \n \n \n \n \n \n \n \n \n \n \nThe learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of\n \n \n \n \n \n \n  \n \n \n \n \n \n \nforming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nalgorithm, providing a robust and biologically inspired alternative to backpropagation. The scheme may be\n \n  \n \n \n \n \n  \n \n \n \n \n \nused for supervised as well as unsupervised training regimes. \n \n1 Introduction \n \nThis paper proposes that a general purpose learning machine can be achieved by implementing Reinforcement\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nLearning in an Artificial Neural Network (ANN), three interdependent methods which attempt to emulate the core\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmechanisms of that process are presented. Ultimately the biological plausibility of this scheme may be validated by\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nreference to natural organisms. However that does not preclude the possibility that there is more than one underlying\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nmechanism providing Reinforcement Learning in nature.  \n \nAI research has characteristically followed a bottom-up approach; focusing on subsystems that address distinct,\n \n \n \n \n  \n \n \n \n \n \n \n \n \nspecialized and unrelated problem domains. In contrast the work presented follows a distinctly top-down approach\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nattempting to model intelligence as a whole system; a causal agent interacting with the environment [6]. The agent is\n \n \n \n \n  \n \n  \n \n \n \n \n \n \n \n \n  \nnot designed to solve a particular problem, but is instead assigned a reward condition. The reward condition serves\n \n \n \n  \n \n \n  \n \n  \n \n \n \n \n \n \nas a goal, and in the path a variety of unknown challenges may be present. To solve these problems efficiently the\n  \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nagent requires intelligence. \n \nThis top-down approach assumes that the core self organizing mechanisms of learning that exist in natural\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \norganisms can be replicated in artificial autonomous agents. These can then be scaled up by endowing the agent with\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmore resources (sensors, neurons & motors). Given sufficient resources and learning opportunities an agent may\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nprovide an efficient solution to a problem provided one exists. Also given the generalization properties of ANN’s\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nthe agent can provide appropriate responses to novel stimuli.  \n \nA distinction is made between supervised, unsupervised and reinforcement training regimes. Supervised learning\n \n \n \n \n \n \n \n \n \n \n \n \n \nregimes use a (human) trainer to assign desired input-output pattern pairings. Unsupervised training regimes are\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \ntypically used to cluster a data set into related groups. Reinforcement Learning (RL) may be considered a subtype of\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n  \n \n \nunsupervised training; it is sometimes called learning with a critic rather than learning with a teacher as the feedback\n \n   \n \n \n \n  \n \n \n \n \n  \n \n \n \n \nis evaluative (right or wrong) rather than instructive (where a desired output action is prescribed). Significant RL\n \n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \nsuccesses have been achieved with the use of Temporal Difference (TD) methods [5][7], notably Q-learning[2].  \n \n1 \nFirst a definition of intelligence is required: \n \nThe demonstration of beneficial behaviors acquired through learning. \n \nA beneficial action/behavior being one that would result in a positive survival outcome (eg successful feeding,\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nmating, self preservation) for the agent. For the most part our inherent internal reward systems encourage us to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nperform beneficial behaviors, but this is not always the case (eg substance abuse may be rewarding but not\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbeneficial). The term ‘desirable behavior’ is avoided due to existing usage of the term ‘desired output’ in supervised\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n  \n \nlearning schemes. \n \nLet’s revise our definition, and expectation, of intelligence: \n \nThe demonstration of rewarding behaviors acquired through learning. \n \nRewarding behaviors/actions will be selected for reinforcement (ie learnt) over non rewarding ones. Rewarding\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbehaviors are those that allow the agent to achieve the reward condition, thereby achieving its goal(s) in an\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nacceptably efficient manner (eg elapsed time, steps taken, energy expended). Rewarding behaviors may lead to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npleasure, or at least a reduction in pain. Goals are attained by achieving the pre-established reward condition, and\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nthereby satiating active desire(s). Behaviors need not be active they may be passive; inaction may lead to reward and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntherefore be reinforced. \n \nFrom initial state ​st if action ​at results in an immediate reward in the subsequent state ​s\n​\nt+1\n​\n, action ​a\n​\nt will be\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nreinforced. If the same (or similar from generalization) input pattern is encountered the learnt action will be\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nperformed. This process of learning is termed Primary Reinforcement. Primary Reinforcement reward conditions (eg\n \n \n \n \n  \n \n \n \n \n \n \n \n \nhunger or thirst) typically drive some form of homeostatic, adaptive control function for the agent [5][8]. \n \n \nThis is in contrast to Secondary/Conditioned Reinforcement where from initial state ​s\n​\nt action ​a\n​\nt does not result in\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nimmediate reward in subsequent state ​st+1. If later action ​at+1 does result in a reward in state ​s\n​\nt+2\n​\n, this will lead to\n \n \n \n \n \n  \n \n \n \n \n \n  \n  \n \n \n \n \n  \nreinforcement of actions ​at+1 and ​at. The number of actions learnt from start to goal is arbitrary and depends on the\n \n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \n \n \n \n \nrelative size of the reward relative to the cost (or pain) in attaining it.  \n \nA learning scheme is presented that enables an embodied neural network agent to autonomously determine and learn\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \ndesirable behaviors. The agent may be embodied in a real or artificial environment. Artificial environments may be\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nmodelled on physical environments or even abstract problem domains. The environment may be any set of input\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npatterns, however there must be a causal relationship between the output (behavior) of the agent and the subsequent\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \ninput pattern.  \n \nThe common theme of this work is the application of thresholds to neural network activation functions, which\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthereby inform the learning process. Whilst the use of thresholds is by no means novel, mainstream learning\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmethods are not heavily reliant upon them. By contrast the three presented features are all strictly dependent on the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npresence of a threshold.  \n \nThresholds for neuron activation are widely found in animal cells, where sufficient ‘excitation’ is required to result\n \n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \nin an electrical action potential or ​‘spike’ that can be signalled to other neurons [11]. Activations exceeding the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthreshold represent the occurrence of one or more spikes, with stronger activations representing sustained ​‘spike\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntrains’ (fig 1).  \n2 \n \nfig1 Logistic Activation Function with threshold \n \nThis paper presents three features: \n \n1.\nA Primary Reinforcement learning scheme is achieved by wrapping a (supervised) backpropagation\n \n \n \n \n \n \n \n \n  \n \n \nnetwork within an unsupervised framework.  \n \nPrimary Reinforcement enables ‘desirable’ actions (behaviors) to be autonomously generated and learnt;\n \n \n \n \n \n \n \n \n \n \n \n \nthis is of particular benefit when a human supervisor is not available / does not know what the desired\n \n \n \n \n \n  \n \n \n \n \n  \n \n \n \n \n \n \noutput should be, or when the environment, or agent itself,  is changeable. \n \n2.\nThe framework is then extended to provide Conditioned (Secondary) Reinforcement. \n \nConditioned (Secondary) Reinforcement enables an arbitrarily long sequence of chained behaviors to be\n \n \n \n \n \n \n \n \n \n \n \n \n \nautonomously learnt, in expectation of a primary reward; this provides long term strategy. \n \n3.\nAn algorithm is described, termed Threshold Assignment of Connections (TAC), that replaces\n \n \n \n \n \n \n \n \n \n \n \n \nbackpropagation within the framework, conversely this algorithm can also be used in supervised training\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nschemes. \n \nThe Threshold Assignment of Connections algorithm provides a biologically inspired alternative to\n \n \n \n \n \n \n \n \n \n \n \n \nbackpropagation. \n \n \nThe examples that follow are focused mainly on target tracking, however this approach may be applied to a wide\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nrange of real and abstract problem spaces. The tasks are small in scale and primarily serve as proof of concept.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nWhile learning rate performance has been documented, the scheme has not been performance tuned. The intent of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthis work is to establish a biologically plausible working model of intelligence that is simple, scalable and generic.  \n \n \n2 Primary Reinforcement \n \nIn this section a learning scheme is described that provides Primary Reinforcement learning in an Artificial Neural\n \n \n  \n \n  \n \n \n \n \n \n \n \n \n \n \nNetwork. Weight adjustments using backpropagation[3] are traditionally used in supervised learning schemes; that\n \n \n \n \n \n \n \n  \n \n \n \n \nis desired activations (or training sets) are established prior to the learning phase. In this example backpropagation\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nwill be used in an unsupervised learning scheme; desired actions will be generated on-the-fly. This approach,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntermed ​Threshold Assignment of Patterns (TAP)​, relies on a threshold in the output layer to determine the\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \ndesired output pattern.  \n3 \nUnlike mainstream methods, the presented scheme will create it’s own actions ​de novo rather than relying on a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \npredefined set. This provides a neural explanation of action selection and enables neural adaption should the causal\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nrelationship between the agent and environment alter (eg damaged motors, icy surfaces). \nAlthough reward results in reinforcement the scheme is also stochastic; punishment results in new candidate\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbehaviors being randomly generated. Sensing utilizes a sensory input pattern and behavior arises from a motor\n \n \n \n \n \n  \n \n \n \n \n \n \n  \n \noutput pattern. Thinking (or processing) is implemented via layers of artificial neurons. \nThe learning scheme consists of the following components: \n \n●\nArtificial Neural Network \n●\nLearning Algorithm \n●\nFramework  \n●\nEnvironment \n \nThe Artificial Neural Network architecture is that of a familiar multilayer perceptron (MLP)[3]. An input (sensor)\n \n \n \n \n  \n \n  \n \n \n \n \n \n \n \npattern representing the environment, produces activations that feed forward and result in an output (motor) pattern\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nrepresenting a behavior.  \n \nThe Learning Algorithm used to derive weight updates is the standard (supervised) back propagation learning\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nalgorithm[3][10].  \n \nThe Framework sits between the network and environment. The framework acts as an interface between network\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nand environment, and is responsible for establishing a reward condition as Primary Reinforcer and determining\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n(desired output) behavior based on that\nreward condition. The framework, network and learning algorithm\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nconstitute the agent.  \n \nThe Environment may be real or simulated. Simulated environments consist of states, and physics rules which\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndefine the relationship between states. The physics rules take the current (t) network output and determine which\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninput pattern is next (t+1) presented to the network.  \n  \n \n \n  \nfig2 State ‘t’ \n \n \nfig3  State ‘t+1’ \nBackpropagation requires a set of desirable output patterns to be established. Through learning the desired output\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \npatterns will inform the network what the required output node activations are for each input pattern. In this scheme\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n4 \ndesired output patterns will be dynamically generated on-the-fly. But how can the agent determine potentially\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ncomplex desired output patterns from a simple yes/no reward condition? \n \nDesired activation values are set depending on whether a reward or punishment occurred after the output response. If\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n  \na reward condition occurred all actual activations above threshold (eg fig 4 node 1) will be reinforced by setting the\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \ncorresponding desired activation to 1.0 and all activations below threshold (eg fig 4 node 2) will be reinforced by\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nsetting the corresponding desired activation to 0.0. If punishment (no reward) occurred all actual activations, above\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nor below threshold, will be weakened by setting desired activations to near threshold values. \n \nfig4   Desired activation : Punishment vs Reward \n \nThe desired activation\nof an output layer node can be determined by reference to the actual activation of the\n \n \n du(t)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nreward node \n in conjunction with the actual activation of the output layer node \n(equation 1).\nar(t+1)\nau(t)\n  \n \nIn reward conditions (reward threshold\nis achieved), the output node activation\nwill be strengthened; a\n \n \n \n \n θr\n \n \n \n \n \n au(t)\n \n \n  \nmaximal desired activation will be set if the output node threshold\nwas achieved and a minimal value if it was\n \n \n \n \n \n  \n \n \n \n θu\n \n \n  \n \n   \n \nnot.  \n \nIn punishment conditions (reward threshold\nis not achieved), the output node activation will be weakened by\n \n \n \n \n θr\n \n \n \n \n \n \n \n \n \n \n \nassigning it a random moderate desired activation, regardless of whether the output node threshold was achieved. \n \n \n \nThe agent will then learn by trial and error according to the following process (Box 1): \n \n5 \nProcessing steps \n1.\nInput pattern for state ‘t’ is presented and forward propagated through the network (Box 2).\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nNode output activation may be in the range 0-1 (Box 3). A threshold value of 0.5 is assigned\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \nto each node in output layer. \n2.\nAgent’s behavior, based on whether output layer activations have exceeded threshold values\n \n \n \n \n \n \n \n \n \n \n \n \n(fig 1), determines subsequent input pattern ‘t+1’. \n3.\nFramework determines ‘reward’ value based on ‘t+1’ input pattern. \n4.\nWeight changes are made: \na.\nDesired ‘t’ activations at the output layer are calculated: \ni.\nIf reward condition then the desired ‘t’ activation for that node is set to\n \n \n \n \n \n \n \n \n \n \n  \n \n \nmaximal value (1.0) for those that were above threshold, and set to\n \n \n \n \n \n \n \n \n \n \n \n \nminimal value (0.0) for those that were below threshold. \nii.\nIf no reward then the desired ‘t’ activation for all nodes are random\n \n \n \n \n \n \n \n \n \n \n \n \n \nmoderate values [0.45..0.55]. \nb.\nWeight changes for state ‘t’ are made according to error values which are back\n \n \n \n \n \n \n \n \n \n \n \n \n \n \npropagated through the network (Box 4). \n \nBox 1 Threshold Assignment of Patterns processing steps \n \nInput activation for unit u. \n= \netinput\nn\nu\n∑\n \ni\neight\nw\nui ai  \n \n \nBox 2  Input activation \n \nOutput activation for unit u. \n= \nau\n1\n1 + e−netinputu  \n \n \nBox 3  Logistic Activation Function \n \n1) Derive desired activation for output unit u. \n \n6 \n \n \n \n2) Derive delta error value for an output layer node u, by finding difference between desired activation (\n) and\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n du  \n \nactual activation (\n).\nau\n \n=(\n-\n)\n)\nelta\nd\nu\ndu au au 1\n( −au  \n \n3) Derive weight change for connection between a hidden unit h and an output unit u, using learning rate. \n \n= \nweight\nΔ\nuh\nrate delta\nl\nu ah  \n \n4) Derive delta error value for a hidden unit h, using weighted sum of all units in output layer . \n \n= \nelta\nd\nh\nah 1 \na )\n(\n− h ∑\n \nu\nelta\nd\nu\neight\nw\nuh  \n \n5) Derive weight change for connection between an input unit i and a hidden unit h, using learning rate. \n \n= \nweight\nΔ\nhi\nrate delta\nl\nh ai  \n \n \nBox 4  Backpropagation weight update \nIn this way randomized desired out patterns will generate a new candidate behavior on the next presentation of that\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nsame (or similar) stimulus. In effect the response has been established before it is first manifested, and this\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrewarding response will be reinforced on future presentations. Conversely non-rewarding behaviors will be\n \n \n \n \n \n \n \n \n \n \n \n \n \ndestroyed in favour of a new candidate behavior. Akin to natural selection, only rewarding behaviors will survive. \n \nA behaviour (ie selected action) is represented by a generated output pattern of activity. Since output should be\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nconsidered at motor rather than functional level, representations may be distributed rather than winner-take-all. The\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nactual function of an action is determined by the causal relationship between the agent and environment. The output\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \npattern will be determined by simultaneous excitation and inhibition arising from an input stimulus. In this sense the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nroles of ​Sense-Think-Act are tightly coupled. \n \nA mapping is formed from an input pattern stimuli to an output pattern motor response, dependent on the reward\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthat follows. No distinction is made between learning and testing phase; that is the agent in a continual process of\n \n \n \n  \n \n \n \n \n \n \n  \n \n \n  \n \n \n \nlearning and evaluation. A behavior is deemed to have been learnt when all output node activations are mature (eg\n \n \n  \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nabove 0.9 or less than 0.1) for a given input pattern and results in a reward. \n2.1 Example: Target tracking (part I) \n \n2.1.1 Problem description \nIn this example the agent must autonomously learn how to track a target (fig 5). \n7 \n \nfig5 Target tracking example start state \n \n●\nThe target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \n \n \nby moving it to the centre cell. \n●\nThe network is rewarded only if it moves the target to it’s centremost cell. Once the target is moved to the\n \n  \n \n   \n \n \n \n \n \n \n \n \n \n  \n  \n \ncentremost cell it is moved to a new starting position on the grid. \n●\nThe agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target\n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \n \n \n \nuntil it does so. \n●\nThe agent may only move the target 1 step (ie to an adjacent cell) in each iteration. \n●\nTo increase the task difficulty the agent may not move the target diagonally in one movement, two steps are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrequired.  \n \n2.1.2 Network configuration \nA network was created (table 1)(fig 6): \n \nInput layer \n9 input nodes; the centre node is designated as a special ‘reward’\n \n \n \n \n \n  \n \n  \n \n \nnode \nHidden layer \n12 hidden nodes \nOutput layer \n4 motor nodes \nNotes \nEach layer fully connected to the next via weights. \nWeights initially randomised. \nEach hidden and output nodes assigned an exclusive bias unit. \nLearning rate = 1.0 \n \nTable 1 Target tracking network configuration \n8 \n \nfig6 Target tracking network topology \n \nThe nine input nodes are mapped to each cell in the grid (fig7): \n \nfig7 Target tracking input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe agent is able to move the target within the grid. In order for the agent to move the target in any given direction\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthere must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTherefore there is only a 1-in-16 chance (2​4​) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nframework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\n \n \n \n \n \n \n \n \n \n \n   \n  \n \n  \n \n \n \n \nzero if it is to be punished. A reward occurs if the target is moved to the centre cell (fig 8): \n \n   \n \n9 \nfig8 Target tracking reward condition \nThe behavior, or output response of the network, will causally influence the subsequent input pattern. Reward\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfeedback will be given to the network, thereby informing whether the output was ‘correct’. The output patterns are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nnot predetermined. The network is presented a pattern and produces a response behavior. If the behavior was\n \n \n \n \n \n  \n \n \n  \n \n \n \n \n \n \nrewarding then a stronger version of the actual output is assigned as the desired output pattern. If the behavior was\n \n  \n \n \n \n \n \n  \n \n \n \n \n \n  \n \n \n \nnot rewarding a random pattern is set as the desired output pattern. A threshold is required at the output layer to\n \n  \n \n \n \n \n \n \n \n \n  \n  \n \n \n \n \n \n \nmake this decision. \n \n2.1.3 Results \n \nInitially the agent moves the target randomly. Activations tend to be weak (eg 0.4 - 0.6) across all output nodes.\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nWhen rewarding behaviors are discovered these are further reinforced and the output matures (eg < 0.1, > 0.9). \n \nWith sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one step\n \n \n \n \n \n \n \n \n  \n  \n \n \n \n \n \nleft/right/up/down towards the food reward from starting locations (cells 1,3,5,7) (table 2). If the target is placed\n \n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \ndirectly on the centre cell it will remain stationary. \n \n \nScheme \n# pattern presentations \nThreshold Assignment of Patterns \n(Unsupervised backpropagation) \n116816 \n \nTable 2 Target tracking results \nHowever, the agent is unable to learn how to move the target when placed in corners (cells 0,2,6,8). The physics\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrules prevent the agent from moving the target diagonally in one step, instead two steps are required. Whilst the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nagent was able to solve this ‘1-step’ solution in 116816 presentations, it spent much of it’s time temporarily ‘stuck’\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nin corner cells. The agent can only reinforce behaviors where there is an immediate reward in the subsequent input\n \n \n \n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \npattern. The network is unable to solve the ‘temporal credit assignment’ problem. In order to learn two or more\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nconsecutive behaviors secondary (conditioned) reinforcement is required (fig 9). \n \n \n \n \nfig9 Primary Reinforcement partial solution for target tracking task \nNote; In this example the reward condition was facilitated by assigning an existing input layer node as the ’Reward\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nNode’. However, the reward condition could be evaluated against a combination of existing input layer nodes, a\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n  \nseparate input layer node(s), or even no node at all (see XOR example below).  \n \n2.2 Example: XOR \n2.2.1 Problem description \nThe agent will be required to solve the XOR problem (table 3) in order to test the network's ability to map non linear\n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \n \n \n  \n \n \n \n10 \ntransformations that require multiple layers of neurons. This test will will also provide a performance comparison\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nbetween unsupervised reinforcement learning and the supervised regime.  \n \n \n(input) \nA \n(input) \nB \n(output) \nXOR \n0 \n0 \n0 \n0 \n1 \n1 \n1 \n0 \n1 \n1 \n1 \n0 \n \nTable 3 XOR task \nAny input pattern can be considered an environment, and any output pattern a behavior. Thus any set of mappings\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \ncan be learnt as they would under a conventional supervised learning regime. In contrast to the previous experiment,\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nthe network will now receive controlled exposure to all input patterns in turn. The behavior, or output response of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe network will not influence the subsequent input pattern. However, reward feedback will be given to the network\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nthereby informing whether the output was correct according to the desired pattern in the supervised training set. This\n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \n \n \ntightly controlled presentation of input patterns is termed ‘guided’. \n \nFor clarity, and to allow for a close comparison with the supervised backpropagation training regime, no explicit\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nreward node will be established in the network topology. The framework is still responsible for setting the reward\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \ncondition. To allow exposure to all the input patterns they will be cycled through in sequence. The framework will\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nevaluate the output and decide whether it should be reinforced or not. \n \n2.2.2 Network configuration \n \nA network was created (table 4)(fig 10): \nInput layer \n2 input nodes \nHidden layer \n3 hidden nodes \nOutput layer \n1 output node \nNotes \nEach layer fully connected to the next via weights. \nWeights initially randomised. \nEach hidden and output nodes assigned an exclusive bias unit. \nLearning rate = 1.0 \n \nTable 4 XOR network configuration \n \n11 \n \n \nfig10\nXOR network topology \n \n2.2.3 Results \n \n \nScheme \n# pattern presentations \nSupervised backpropagation \n2182 \nThreshold Assignment of Patterns  \n(unsupervised backpropagation) \n7550 \n \nTable 5 XOR results \nThe Threshold Assignment of Patterns (unsupervised backpropagation) regime required significantly more pattern\n \n \n \n \n \n \n \n \n \n \n \n \npresentations to learn the XOR solution than the supervised regime (table 5). Both were using the same set of initial\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nweights and hyperparameters (learning rate etc). Both are using the same backpropagation algorithm to perform\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nweight updates. The difference in performance can be attributed to the manner in which desired output patterns are\n \n \n \n \n \n \n \n \n  \n \n  \n \n \n \n \n \nprovided. In the supervised scheme the desired patterns are known ​a priori, in the unsupervised (guided) learning\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nregime these must be explored by the network through trial and error. This performance gap would be expected to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nwiden as the number of nodes in the output layer grows and with it the number of candidate combinations. \n \n2.3 Summary  \nUnsupervised Primary Reinforcement can be achieved, with reference to a threshold, by dynamically generating\n \n \n \n \n \n \n \n \n  \n \n \n \n \ndesired output activations and feeding these into a supervised learning algorithm. These experiments demonstrate\n \n \n \n \n \n \n  \n \n \n \n \n \n \nthe network's ability to learn and map arbitrary sets of patterns by reward, thereby producing behaviors that allow it\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \nto reach it’s goal in an efficient manner. Primary Reinforcement may solve problems that are that are linearly\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninseparable. It is capable of training weights deep within the network, thereby capable of forming complex abstract\n   \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrepresentations. \nThe input patterns can be presented in any order. However, in order for the network to identify which output\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npatterns (or behaviors) were correct, the network must be presented with subsequent reward feedback. The\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nunsupervised Reinforcement Learning regime requires more iterations to learn compared to the supervised regime.\n \n \n \n \n \n \n  \n \n  \n \n \n \nThe principal reason being that the unsupervised agent must explore the ‘correct’ solution through trial and error.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAnd even when agent does not receive reinforcement, the new random candidate behavior may be a repeat of a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n  \nprior incorrect one. \n12 \nPrimary Reinforcement may only learn a single behavior sequence, therefore it is not suitable for acquiring long\n \n \n \n \n  \n \n \n \n   \n \n \n \n \n \nterm strategy that lead to distal rewards. Problems requiring long term strategy must use Secondary/Conditioned\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nReinforcement. \n \n3 Secondary/Conditioned Reinforcement \nPrimary reinforcement provides a generic method of autonomously establishing beneficial output responses. But it\n \n \n  \n \n \n \n \n \n \n \n \n  \nhas a significant limitation; it can only provide a one step mapping from start state(s) to goal. A more useful feature\n  \n \n  \n \n \n  \n \n \n \n \n \n \n \n  \n \n \n \nis the ability to establish an efficient series of behavior steps leading from start state(s) to a goal. This is the benefit\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n   \n \n  \n \n \nprovided by Conditioned Reinforcement. The term ‘Conditioned’ Reinforcement is preferred over that of\n \n \n \n \n \n \n \n \n \n \n \n \n \n‘Secondary’ Reinforcement, since the latter may imply chained behaviors only 2 steps deep. In fact the number of\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nchained behaviors can be arbitrarily deep, depending on the strength of the reward and subsequent reduction\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(discount factor) applied to it. This approach, termed ​Threshold Assignment of Rewards (TAR)​, relies on\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbuilding an association between a rewarding stimulus and an internal proxy reward. \nIn Primary Reinforcement a mapping is established between an input pattern stimuli to an output pattern motor\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nresponse. In Secondary Reinforcement a mapping is established between an input pattern stimuli to an output\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \npattern motor response AND a reward node. With sufficient reinforcement the response activation on the reward\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nnode matures; the reward node is now available as a proxy reward condition (secondary/conditioned reinforcer) for\n \n \n \n \n  \n \n \n  \n \n \n \n \n \n \na given input pattern. In turn this conditioned reinforcer can help to create further conditioned reinforcers, that are\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nactivated only in response to a recognized input pattern stimuli. \nThe first conditioned response to be learnt will be closest to the primary reinforcer. Thereafter a chain of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nconditioned reinforcers can be established; via backward induction a series of ‘breadcrumbs’ are laid out in reverse\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \nfrom the goal. Using this mechanism planned or goal oriented tasks can be solved ‘model-free’. The mapped\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nreward node provides a ​state-value function for the agent. This mechanic resembles action-value mappings derived\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nfrom ​sample-backups in SARSA and Q-learning methods[5][2]. However, in the present scheme state-action\n \n \n \n \n \n \n \n \n \n \n \n \n \nmappings are dealt with separately (TAP), and can be readily decoupled should an ​actor-critic architecture be\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nemployed [8]. Also while TD methods achieve exploration via (𝜺-greedy) probability, in the presented scheme\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nexploration occurs when the agent fails to obtain a reward that satisfies a threshold value. If the agent is pursuing a\n \n \n \n \n \n \n \n  \n \n \n  \n \n  \n \n  \n  \nsuboptimal policy (path) to the goal, the threshold can be raised until the optimal path is found. Risky exploration\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \ncan therefore be avoided unless required. \nThis approach is intended to overcome the temporal ‘hill climbing’ limitation outlined in the previous example. It\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n  \nis based on the a similar architecture and learning rule as before but with one important addition: a special reward\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nnode is added to the output layer. Unlike other nodes in the output layer the special reward node is not a motor\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n  \n \nneuron. Once this becomes mature it behaves like an input reward node; deciding which behaviors should be learnt.\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nBehaviors leading to a distant reward can be chained together. \n \nThe prior desired activation\nof an output layer reward node can be determined by reference to the subsequent\n \n \n \n du(t)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nactual activation of the output layer reward node \n(equation 2).\nau(t+1)\n \n \nIn reward conditions (reward threshold\nis achieved), the output reward node activation\nwill be assigned the\n \n \n \n \n θu\n \n \n \n \n \n \n au(t)\n \n \n \n \nproduct of the subsequent actual activation of the output layer reward node \nand discount factor .\nau(t+1)\nγ   \n \nIn punishment conditions (reward threshold\nis not achieved), the output reward node activation will be assigned a\n \n \n \n \n θu\n \n \n \n \n \n \n \n \n \n \n  \nminimal value. \n \n  \n13 \n \nEssentially mapping are now formed between input patterns and rewards, rather than just input patterns and motor\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nnodes (Box 5). \n \nProcessing steps \n \nDifferences to the Primary Reinforcement process described previously are highlighted in bold. \n \n1.\nInput pattern for state ‘t’ is presented and forward propagated through the network (Box 2).\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nNode output activation may be in the range 0-1 (Box 3). A threshold value of 0.5 is assigned\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \nto each node in output layer. \n2.\nAgent’s behavior, based on whether output layer activations have exceeded threshold values\n \n \n \n \n \n \n \n \n \n \n \n \n(fig 1), determines subsequent input pattern ‘t+1’. \n3.\nFramework determines ‘reward’ value based on ‘t+1’ input pattern ​and on activation of\n \n \n \n \n \n \n \n \n \n \n \n \n \nspecial output reward node exceeding reward threshold  (eg > 0.8 )​. \n4.\nWeight changes are made: \na.\nDesired ‘t’ activations at the output layer are calculated: \ni.\nIf reward condition then the desired ‘t’ activation for that node is set to\n \n \n \n \n \n \n \n \n \n \n  \n \n \nmaximal value (1.0) for those that were above threshold, and set to\n \n \n \n \n \n \n \n \n \n \n \n \nminimal value (0.0) for those that were below threshold. ​Set the desired\n \n \n \n \n \n \n \n \n \n \n \n \nactivation for the special output reward node to discount (eg 95%) of\n \n \n \n \n \n \n \n \n \n \n \n \nreward value. \nii.\nIf no reward then the desired ‘t’ activation for all nodes are random\n \n \n \n \n \n \n \n \n \n \n \n \n \nmoderate values [0.45..0.55]. ​Set the desired activation for the special\n \n \n \n \n \n \n \n \n \n \noutput reward node to minimal value (0.0). \nb.\nWeight changes for state ‘t’ are made according to error values which are back\n \n \n \n \n \n \n \n \n \n \n \n \n \n \npropagated through the network (Box 6). \n \nNote: To achieve this both the current and previous activations must be stored. Weight changes are \nderived using back propagation on previous activations. \n \nBox 5 Threshold Assignment of Rewards processing steps \n \n \n1) Derive desired activation for reward output unit u. \n14 \n \n \n2) Derive delta error value for an output layer node u, by finding difference between desired activation (\n) and\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n du  \n \nactual activation (\n).\nau\n \n=(\n-\n)\n)\nelta\nd\nu\ndu au au 1\n( −au  \n \n3) Derive weight change for connection between a hidden unit h and an output unit u, using learning rate. \n \n= \nweight\nΔ\nuh\nrate delta\nl\nu ah  \n \n4) Derive delta error value for a hidden unit h, using weighted sum of all units in output layer . \n \n= \nelta\nd\nh\nah 1 \na )\n(\n− h ∑\n \nu\nelta\nd\nu\neight\nw\nuh  \n \n5) Derive weight change for connection between an input unit i and a hidden unit h, using learning rate. \n \n= \nweight\nΔ\nhi\nrate delta\nl\nh ai  \n \nBox 6  Backpropagation weight update \n3.1 Example: Target tracking (part II) \nThe same problem as described in section ​3.1.1 is revisited. In this example the agent is equipped with the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nresources required to facilitate Secondary (Conditioned) Reinforcement. \n \n3.1.1 Problem description \nIn this example the agent must autonomously learn how to track a target (fig 11). \n \nfig11\nTarget tracking example start state \n \n●\nThe target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \n \n \nby moving it to the centre cell. \n●\nThe network is rewarded only if it moves the target to it’s centremost cell. Once the target is moved to the\n \n  \n \n   \n \n \n \n \n \n \n \n \n \n  \n  \n \ncentremost cell it is moved to a new starting position on the grid. \n●\nThe agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target\n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \n \n \n \nuntil it does so. \n●\nThe agent may only move the target 1 step (ie to an adjacent cell) in each iteration. \n●\nTo increase the task difficulty the agent may not move the target diagonally in one movement, two steps are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n15 \nrequired.  \n \n3.1.2 Network configuration \nA network was created (table 6)(fig 12): \nInput layer \n9 input nodes; the centre node is designated as a special ‘reward’\n \n \n \n \n \n  \n \n  \n \n \nnode \nHidden layer \n12 hidden nodes \nOutput layer \n4 motor nodes + 1 reward node \nNotes \nEach layer fully connected to the next via weights. \nWeights initially randomised. \nEach hidden and output nodes assigned an exclusive bias unit. \nLearning rate = 1.0 \nTable 6 Target tracking network configuration \n \n \n \nfig12\n \n \nThe nine input nodes are mapped to each cell in the grid (fig 13): \n16 \n \nfig13\nTarget tracking input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe agent is able to move the target within the grid. In order for the agent to move the target in any given direction\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthere must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTherefore there is only a 1-in-16 chance (2​4​) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nframework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\n \n \n \n \n \n \n \n \n \n \n   \n  \n \n  \n \n \n \n \nzero if it is to be punished. A reward occurs if the target is moved to the centre cell (fig 14): \n \n \nfig14\nTarget tracking reward condition \nThe network is also assigned an additional special reward node in the output layer. A reward also occurs if the \nactivation of this output reward node is above the reward threshold (eg 0.8), which indicates it has been previously \nreinforced. If the subsequent input reward node or subsequent output reward node is activated, connections to the \noutput reward node and motor output nodes will be reinforced. \n \nNote; In this example a discrete special reward node was added to the output layer. It is possible to to have no\n \n \n \n  \n \n \n \n \n \n \n \n \n \n   \n \n \n \n \n \ndedicated reward node at output layer, instead it possible to test if activations of all motor neurons are high (and\n \n \n \n \n \n \n  \n \n \n  \n \n \n \n \n \n \n \n \nhave therefore matured). Consequently activations, connections and therefore the strength of memories will be\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nproportional to the reward. \n \n3.1.3 Results \n \nInitially the agent moves the target randomly. Activations tend to be weak (eg 0.4 - 0.6) across all output nodes.\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nWhen rewarding behaviors are discovered these are further reinforced and the output matures (eg < 0.1, > 0.9). \n \n \nScheme \n# pattern presentations \nThreshold Assignment of Reward \n(Unsupervised backpropagation) \n110324 \n \nTable 7 Target tracking results \n17 \n \n \n \n \nfig15\nConditioned Reinforcement full solution for target tracking task \nThe agent is able to learn how to move the target when placed in corners (cells 0,2,6,8) (table 7). The agent has\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \neffectively established proxy rewards in intermediate locations (cells 1,3,5,7) allowing chained sequences of\n \n \n \n \n \n \n \n \n \n \n \n \n \nbehaviors to be learned (fig 15). The chaining of sequences of behaviors enables long term strategy to be acquired,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthis is demonstrated more substantially in the following maze navigation problem. \n3.2 Example: Maze navigation \n3.2.1 Problem description \nIn this example the agent must autonomously learn how to navigate a target through a maze (fig 16). \n \n \nfig16\nMaze task start state \n●\nThe target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \n \n \nby moving it the top centre cell. \n●\nOnce the target is moved to the target cell (cell 1) it is moved back to it’s original starting position on the\n \n \n  \n \n \n \n \n \n \n   \n \n \n \n \n \n \n \n \n \ngrid (cell 3). \n●\nThe agent has no prior knowledge. It does not know that food will lead to a reward until it happens. \n●\nThe agent may only move the target 1 step (ie to an adjacent cell) in each iteration. \n●\nTo increase the task difficulty the agent may not move the target diagonally in one movement, two steps are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrequired.  \n●\nTo further increase task difficulty there is an invisible barrier between the start state and the goal. The agent\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nmust learn to navigate around the barrier. \n \n3.2.2 Network configuration \n \nA network was created (table 8) (fig 17): \nInput layer \n9 input nodes; the centre node is designated as a special ‘reward’\n \n \n \n \n \n  \n \n  \n \n \nnode \n18 \nHidden layer \n12 hidden nodes \nOutput layer \n4 motor nodes + 1 reward node \nNotes \nEach layer fully connected to the next via weights. \nWeights initially randomised. \nEach hidden and output nodes assigned an exclusive bias unit. \nLearning rate = 1.0 \n \nTable 8 Maze task network configuration \n \nfig17\nMaze task network topology \n \n \nThe nine input nodes are mapped to each cell in the grid (fig 18): \n \nfig18\nMaze task input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe agent is able to move the target within the grid. In order for the agent to move the target in any given direction\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n19 \nthere must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTherefore there is only a 1-in-16 chance (2​4​) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nframework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\n \n \n \n \n \n \n \n \n \n \n   \n  \n \n  \n \n \n \n \nzero if it is to be punished. A reward occurs if the target is moved to the top centre cell (cell 1). \n \nThe network is also assigned an additional special reward node in the output layer. A reward also occurs if the \nactivation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously \nreinforced. Connections to the output reward node will be reinforced in the same manner as motor output  nodes if \nthe subsequent input reward node or subsequent output reward node is activated. \n \n \n3.2.3 Results \n \nInitially the network moves randomly through the environment. In time the agent is able to learn how to move the \ntarget around corners. The physics rules prevent the agent from moving the target diagonally in one step, instead two \nsteps are required. The agent can only reinforce behaviors where there is an immediate reward in the next input \npattern. The agent has effectively established proxy rewards, or sub-goals, in intermediate locations (cells 2,5,4) \nallowing chained sequences of behaviors to be learned (fig 19-22). \n \n \n \n \nfig19\nAgent encounters the Primary \nReinforcer and learns first \nbehavior. \n \nfig20\nAgent establishes first \nConditioned Reinforcer. \n \n \nfig21\nAgent establishes a subsequent \nConditioned Reinforcer by \nreference to the the initial \nConditioned Reinforcer. \n \nfig22\nAgent establishes a series of \nConditioned Reinforcers from \ngoal to starting position. \n \n \n \nWith sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one step\n \n \n \n \n \n \n \n \n  \n  \n \n \n \n \n \nleft/right/up/down towards the goal from the starting location (cell 3).  \n \nThe chaining of sequences of behaviors enables long term strategy to be acquired (table 9). \n \n \n \nScheme \n# pattern presentations \nThreshold Assignment of Reward \n(Unsupervised backpropagation) \n80184 \n \nTable 9 Target tracking results \n \nLearning rates can be improved by shaping [9], that is initially placing the agent closer to the reward and once learnt \nthe distance may be increased. \n \n20 \n3.3 Summary \nA considerable challenge facing Reinforcement Learning schemes is that rewards can be very temporally delayed. \nSecondary/Conditioned Reinforcement provides an effective solution to this ‘Temporal’ Credit Assignment \nProblem. The reward condition was facilitated by adding single reward node to the output layer in addition to \nevaluating the reward at the input layer. The output reward desired activation experiences ‘reduction’ as it becomes \ntemporally distant (number of iterations) from the target input reward. It is necessary to apply a discount factor to \nthe output reward node in order to prevent the agent from becoming infinitely rewarded on a proxy reward that has \nbeen established. If the agent becomes fixated on a proxy reward it will become ‘stuck’, repeating the previous \nbehavior. \nThe network learns appropriate mappings based on the temporal ordering of events; these mappings are contingent\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \non subsequent reward conditions. Learning is based on cause and effect. It is nondeterministic; alternate policies\n \n \n \n \n  \n \n \n \n \n   \n \n \n \nmay be used to attain a reward. The scheme does not rely on ​eligibility traces [1], or require long-term retention of\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nprior network states (other than t-1 activations).  \nIf the reward condition is removed the agent displays extinction; behaviours weaken and eventually become \nrandom. \nIt is also possible to achieve this effect without an explicit motivator node in output layer. Since only strong\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(output) behaviors are those which have been reinforced, a test can be made against the strength of the entire output\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \npattern rather than a specific node.  \nWhen using a single node representation for the (mapped) output layer reward node and applying a discount factor,\n \n  \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \na diminishing desired activation is set for that node. The node is interpreted in an analogue (continuous) fashion.\n \n \n \n  \n \n \n \n \n \n  \n \n \n \n \n \n \nAn alternative representation would be to introduce a discrete output reward node for each time step away from the\n \n \n \n \n  \n  \n \n \n \n \n \n \n \n \n \n \n \ninput node reward.  \n \n4 Threshold Assignment of Connections \nSo far backpropagation, an algorithm intended for supervised learning, has been housed within an unsupervised\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nframework; desired output patterns have been presented by a framework acting as a surrogate supervisor. Using the\n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \n \nsame framework as described in earlier sections, the backpropagation learning algorithm was replaced with an\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nalternative algorithm, termed ​Threshold Assignment of Connections (TAC)​.  \nA biological plausibility concern faces supervised learning schemes in general: \n●\nHow are desired output patterns selected and presented to the network ? \nIn previous examples, once backpropagation is placed in the Reinforcement Learning framework it is no longer a\n \n \n \n \n  \n \n \n \n \n \n   \n \n  \ntruly supervised learning scheme; desired output patterns are not known ​a priori. However backpropagation faces\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nthe additional biological plausibility concern: \n●\nHow is error back propagated ? \nThus far research has provided little neurobiological support for backpropagation [4][15]. TAC does not require\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbackward connections or tenuous biochemical explanations. TAC could be explained by neuromodulators rather\n \n \n \n \n \n \n \n \n \n \n \n \n \nthan at the neural computation level. This would provide a more robust solution with a simpler biological\n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \nexplanation. \n \nThe desired activation\nof any node can be determined by reference to the actual activation of the reward node\n \n \n du(t)  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nin conjunction with the actual activation of the postsynaptic node\nand presynaptic node\n(equation\nar(t+1)  \n \n \n \n \n \n \n \n \n \n au(t)\n \n \n ah(t)\n \n3).  \n \nIf the presynaptic node did not fire connections from that node will not be strengthened or weakened; if the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \npresynaptic node threshold\nwas not achieved the desired activation will remain as the actual activation\n. If\n \n \n θh\n \n \n \n \n \n \n \n \n \n \n \n au(t)   \n21 \nthe presynaptic node did fire connections from that node may be strengthened or weakened. \n \nIn reward conditions (reward threshold\nis achieved), the postsynaptic node activation\nwill be strengthened; a\n \n \n \n \n θr\n \n \n \n \n \n au(t)\n \n \n  \nmaximal desired activation will be set if the postsynaptic node threshold\nwas achieved and a minimal value if it\n \n \n \n \n \n  \n \n \n \n θu\n \n \n  \n \n   \nwas not.  \n \nIn punishment conditions (reward threshold\nis not achieved), the postsynaptic node activation will be weakened\n \n \n \n \n θr\n \n \n \n \n \n \n \n \n \n \nby assigning it a random moderate desired activation, regardless of whether the postsynaptic node threshold was\n \n   \n \n \n \n \n \n \n \n \n \n \n \n \nachieved.  \n \n \nDirect ‘on-node’ delta values that can be used to calculate weight updates, rather than backpropagating them (Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n7). \n \nProcessing steps \n \nDifferences to the Conditioned Reinforcement process described previously are highlighted in bold. \n \n1.\nInput pattern for state ‘t’ is presented and forward propagated through the network (Box 2).\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nNode output activation may be in the range 0-1 (Box 3). A threshold value of 0.5 is assigned\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \nto all nodes in every layer. \n2.\nAgent’s behavior, based on whether output layer activations have exceeded threshold values\n \n \n \n \n \n \n \n \n \n \n \n \n(fig 1), determines subsequent input pattern ‘t+1’. \n3.\nFramework determines ‘reward’ value based on ‘t+1’ input pattern ​and on activation of\n \n \n \n \n \n \n \n \n \n \n \n \n \nspecial output reward node exceeding reward threshold (eg > 0.8 )​. \n4.\nWeight changes are made: \na.\nDesired ‘t’ activations ​on every node​ are calculated: \ni.\nIf reward condition then the desired ‘t’ activation for that node is set to\n \n \n \n \n \n \n \n \n \n \n  \n \n \nmaximal value (1.0) for those that were above threshold, and set to\n \n \n \n \n \n \n \n \n \n \n \n \nminimal value (0.0) for those that were below threshold. ​Set the desired\n \n \n \n \n \n \n \n \n \n \n \n \nactivation for the special output reward node to 95% of reward value. \nii.\nIf no reward then the desired ‘t’ activation for all nodes are random\n \n \n \n \n \n \n \n \n \n \n \n \n \nmoderate values [0.45..0.55]. ​Set the desired activation for the special\n \n \n \n \n \n \n \n \n \n \noutput reward node to minimal value (0.0). \nb.\nWeight changes for state ‘t’ are made for all connections to each node in situ\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n22 \naccording to error values (Box 8); these are not back propagated. \n \nNote: To achieve this both the current and previous activations must be stored.  \n \nBox 7 Threshold Assignment of Connections processing steps \n \n1) Calculate desired activation for postsynaptic unit u. \n \n \n \n2) Derive delta error value for postsynaptic node u, by finding difference between desired activation (\n) and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n du  \n \nactual activation (\n).\nau\n \n=(\n-\n)\n)\nelta\nd\nu\ndu au au 1\n( −au  \n \n3) Derive weight change for connection between presynaptic unit h and postsynaptic unit u, using learning rate. \n \n= \nweight\nΔ\nuh\nrate delta\nl\nu ah  \n \n \nBox 8 Threshold Assignment of Connections weight change \n \n4.1 Example: Maze navigation \nThe same problem as described in section ​3.2.2 is revisited. In this example the agent is equipped with the TAC\n \n \n \n \n \n \n \n  \n \n \n \n \n \n  \n \n \n \n \nalgorithm rather than backpropagation. \n \n4.1.1 Problem description \nIn this example the agent must autonomously learn how to navigate a target through a maze (fig 23). \n \n23 \n \nfig23\nMaze task start state \n●\nThe target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,\n \n \n \n  \n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \n \n \nby moving it the top centre cell. \n●\nOnce the target is moved to the target cell (cell 1) it is moved back to it’s original starting position on the\n \n \n  \n \n \n \n \n \n \n   \n \n  \n \n \n \n \n \n \ngrid (cell 3). \n●\nThe agent has no prior knowledge. It does not know that food will lead to a reward until it happens. \n●\nThe agent may only move the target 1 step (ie to an adjacent cell) in each iteration. \n●\nTo increase the task difficulty the agent may not move the target diagonally in one movement, two steps\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nare required.  \n●\nTo further increase task difficulty there is an invisible barrier between the start state and goal. The agent\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nmust learn to navigate around the barrier. \n \n4.1.2 Network configuration \nA network was created (table 10)(fig 23): \nInput layer \n9 input nodes; the centre node is designated as a special ‘reward’\n \n \n \n \n \n  \n \n  \n \n \nnode \nHidden layer \n12 hidden nodes \nOutput layer \n4 motor nodes + 1 reward node \nNotes \nEach layer fully connected to the next via weights. \nWeights initially randomised. \nEach hidden and output nodes assigned an exclusive bias unit. \nLearning rate = 1.0 \n \nTable 10 Maze task network configuration \n24 \n \nfig24\nMaze task network topology \n \nThe nine input nodes are mapped to each cell in the grid (fig 25): \n \nfig25\nMaze task input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe agent is able to move the target within the grid. In order for the agent to move the target in any given direction\n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nthere must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTherefore there is only a 1-in-16 chance (2​4​) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nframework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and\n \n \n \n \n \n \n \n \n \n \n   \n  \n \n  \n \n \n \n \nzero if it is to be punished. A reward occurs if the target is moved to the top centre cell (cell 1). \n \nThe network is also assigned an additional special reward node in the output layer. A reward also occurs if the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nactivation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously\n \n \n \n \n \n \n \n  \n \n \n \n \n \n  \n \n \n \nreinforced. If the subsequent input reward node or subsequent output reward node is activated, connections to the\n  \n \n \n \n \n \n \n \n \n \n  \n \n  \n \noutput reward node and motor output nodes will be reinforced. \n25 \n \n4.1.3 Results \n \nInitially the network moves randomly through the environment. In time the agent is able to learn how to move\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \naround corners to the goal. The physics rules prevent the agent from moving the target diagonally in one step,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninstead two steps are required. The agent can only reinforce behaviors where there is an immediate reward in the\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nnext input pattern. The agent has effectively established proxy rewards, or sub-goals, in intermediate locations\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n(cells 2,5,4) allowing chained sequences of behaviors to be learned (fig 26-29). \n \n \n \nfig26\nAgent encounters the Primary \nReinforcer and learns first \nbehavior. \n \nfig27\nAgent establishes first \nConditioned Reinforcer. \n \n \nfig28\nAgent establishes a subsequent \nConditioned Reinforcer by \nreference to the the initial \nConditioned Reinforcer. \n \nfig29\nAgent establishes a series of \nConditioned Reinforcers from \ngoal to starting position. \n \n \n \nWith sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nstep left/right/up/down towards the food reward from the starting location (cell 3). \n \nThe chaining of sequences of behaviors enables long term strategy to be acquired (table 11). \n \nScheme \n# pattern presentations \nThreshold Assignment of Reward \n(Unsupervised backpropagation) \n80184 \nThreshold Assignment of Connections \n(Unsupervised) \n18212 \n \nTable 11 Maze task tracking results \n \nLearning rates can be improved by shaping [9], that is initially placing the agent closer to the reward and once\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlearnt increasing the distance. \n \n4.2 Example: XOR \nThe same problem as described in section ​3.1.2 is revisited. In this example the agent is equipped with the TAC\n \n \n \n \n \n \n \n  \n \n \n \n \n \n  \n \n \n \n \nalgorithm rather than backpropagation. \n4.2.1 Problem description \nThe agent will be required to solve the XOR problem (table 12) in order to test the network's ability to map non\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nlinear transformations that require multiple layers of neurons. This test will will also provide a performance\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \ncomparison between unsupervised reinforcement learning and the supervised regime.  \n \n \n26 \n(input) \nA \n(input) \nB \n(output) \nXOR \n0 \n0 \n0 \n0 \n1 \n1 \n1 \n0 \n1 \n1 \n1 \n0 \n \nTable 12 XOR task \n \nThe behavior, or output response of the network will not influence the subsequent input pattern. However, reward\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nfeedback will be given to the network thereby informing whether the output was correct. Once again the output\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npatterns are not predetermined. This tightly controlled presentation of input patterns is termed ‘guided’. The physics\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nrules are modified in order to guide the agent. \n \n4.2.2 Network configuration \nA network was created (table 13) (fig 30): \nInput layer \n2 input nodes \nHidden layer \n3 hidden nodes \nOutput layer \n1 output node \nNotes \nEach layer fully connected to the next via weights. \nWeights initially randomised. \nEach hidden and output nodes assigned an exclusive bias unit. \nLearning rate = 1.0 \n \nTable 13 XOR Network configuration \n \n \nfig30\nXOR topology \n27 \n4.2.3 Results \n \nScheme \n# pattern presentations \nSupervised backpropagation \n2182 \nThreshold Assignment of Connections \n(unsupervised) \n311 \n \nTable 14 XOR results \n \nThe Threshold Assignment of Connections (unsupervised) scheme required significantly less pattern presentations \nto learn the XOR solution than the supervised regime (table 14). Both were using the same set of initial weights and \nlearning parameters (learning rate etc). Despite the fact that in the supervised scheme the desired patterns are known \na priori, while in the unsupervised (guided) learning regime these must be discovered by the network through trial \nand error. However this performance gap would be expected to lessen as the number of nodes in the output layer \ngrows, increasing dimensionality and with it the number of candidate combinations. \n \n4.3 Summary \nThese experiments demonstrate TAC may solve linearly inseparable problems that are normally reserved for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsupervised training regimes. It is capable of training weights deep within the network, thereby capable of forming\n \n \n   \n \n \n \n \n \n \n \n \n \n \n \n \ncomplex abstract representations. TAC can also solve problems that require long term strategy, when applied in\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nconjunction with Secondary (Conditioned) Reinforcement. Once TAC has replaced backpropagation, the\n \n \n \n \n \n \n \n \n \n \n \nframework is no longer required to act as a surrogate supervisor. \nIt should be noted that while the scheme suggests assigning random desired activations around the threshold value\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n[0.45..0.55] in punishment conditions, it was found to be at least as effective to set this in the range [0-1]. It should\n \n \n \n  \n \n  \n  \n \n \n  \n \n  \n \n \n  \n \nbe appreciated that (with moderate learning rates) this has the same effect of establishing an ‘immature’ response\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \naround the threshold level (fig 1). These alternatives have implications as to what similar mechanisms we might\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nexpect to find in natural organisms.  \n \n5 Discussion \nThe merits of the present scheme can be evaluated in terms of Machine Learning Applications as well as the broader\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nCognitive Science implications (eg biological, psychological, philosophical).  \n5.1 Applications \n5.1.1 Primary Reinforcement (TAP) \nOne of the key advantages of Primary Reinforcement over supervised learning schemes is that the agent is able to\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n  \n  \nautonomously explore solutions (behaviors) to problems, of use when a human may be unable to provide training\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \ndata. Another benefit of Primary Reinforcement is that learning is dynamic and continuous, there is no distinction\n \n \n \n \n \n  \n \n  \n \n \n \n  \n \n \nbetween training and classification phases. This is useful when the agent encounters novel stimuli, or when a once\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n  \n \nuseful response is no longer so. \n28 \n \nfig31\n‘Rags the robot’  \n \n‘Rags the robot’ learns how to track objects with a \nvision sensor. The robot has an onboard Arduino \nmicrocontroller and is wirelessly controlled by desktop \ncomputer. \n \nThe learning scheme can be readily applied to robotics. As proof of concept an agent was created to remotely control\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n‘Rags the robot’; a mobile Arduino based device (fig 31). The task was to target an object, in this case a red frisbee.\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \n \nFor this purpose Rags was equipped with a vision sensor (PixyCam), and was capable of rotating via a pair of motor\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n  \n \n \n \ncontrolled wheels. The agent begins each learning session with randomised network weights. Similar to the previous\n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \ntargeting tasks, the robot initially moves randomly, rotating passed the target. Eventually the agent is able to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nco-ordinate its motors and align itself with the target. A key benefit of TAP is that action is dynamically generated\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \n \nby the network itself, rather than being reliant on a predefined set of actions. If the causal relationship between agent\n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n \n \n \nand environment changes (eg rotation direction of the wheel motors is inverted), Rags will compensate via neural\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \nadaption. \nThe Primary Reinforcement solution presented is readily scalable; additional sensors, motors and hidden units can\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \nbe easily added without reworking the underlying learning process. These additional resources will be utilized in\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nproblem solving. Alternate behaviors will be adopted in the face of unforeseen environmental hazards, or if the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nagent suffers sensor/motor damage. \nAlthough Primary Reinforcement enables autonomous agents to discover solutions autonomously, a potential issue\n \n \n \n \n \n \n \n \n \n  \n \n \nis that the learned behavior may not be ideal but is sufficient to achieve a reward condition; this can be termed a\n \n \n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \n \n \n  \nsuboptimal action (behavior) ​limitation. For example a ball may have been successfully kicked into a goal, but the\n \n \n \n \n \n  \n \n \n \n \n \n \n  \n \n \n \nkicking technique itself was poor. To avoid this a higher standard can be achieved by setting a more stringent reward\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n  \n \n \n \ncondition (eg lowering the reward value or raising the threshold), the tradeoff being more optimal behaviors are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npotentially slower to be explored and learnt. \nAnother limitation of the present solution is that we are unable to set arbitrary desired output activations on output\n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nnodes, a maximal or minimal value is set (ie 1.0 or 0.0). For example in a network with three output nodes we may\n  \n \n \n \n  \n \n \n \n \n \n \n   \n \n \n \n \n \n \n \nchoose a desired output pattern vector of [0.6, 0.3, 0.7] for a supervised network, but in the present scheme the\n  \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \ndesired output pattern vector would be limited to [1.0, 0.0, 1.0]. This shortcoming can be termed a ​binary limitation.\n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n  \n \n \nTo workaround this limitation alternate output representations may be required to achieve the same level of\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ngranularity. \nOne of the challenges of scaling relates to the complexity of output responses; the more output nodes that are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrequired for a rewarding behavior to occur, the longer it will take for the agent to explore candidate behaviors and\n \n  \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \ndiscover the requisite combination. Also it should also be noted that not all node activations may be relevant to\n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \nproducing a rewarding response. These may be included in the dynamically generated desired output pattern, and the\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nagent is unable to differentiate which nodes were responsible for achieving the reward. These redundant activations\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nmay be eliminated by conforming to what can be termed a ​laziness principle, where rewards are reduced in\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nproportion to output node activations.  \nNote; while Reinforcement Learning is essentially a trial and error approach to learning, it is possible to first part\n \n \n \n  \n  \n \n \n \n \n \n   \n \n \n \n \n29 \ntrain  in supervised mode, save the weights, and then switch to unsupervised mode. \n \n5.1.2 Secondary reinforcement (TAR) \nA potential issue facing the presented scheme may be termed a ​suboptimal policy (path) limitation. At first glance\n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \nthis is similar to the ​suboptimal behavior ​limitation described in Primary Reinforcement, but on a temporal level.\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \nThere may be a tendency to ​exploit known rewards rather than ​explore new ones. The problem being that the while\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe acquired sequence of behaviors reliably leads to the goal, and the individual behaviors may have been optimal,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe route was not the most efficient one possible. Once learnt the agent has no pressure to alter its preferred policy, a\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \ncondition which afflicts natural organisms. An optimal policy can be found by decreasing the reward value or\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nincreasing the threshold relative to reward, but since exploration involves potential risk it can be avoided unless\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nrequired. In addition behaviors may be ​‘shaped’ to elicit the optimal policy [9].  \n5.1.3 Threshold Assignment of Connections (TAC) \nTAC is significantly easier to implement than backpropagation. It also provides a fully neural explanation of\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \nReinforcement Learning, with processing essentially only at the node level. This would greatly simplify any\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nhardware implementations of the method. It has fewer error derivation dependencies than backpropagation, and\n \n \n \n \n  \n \n \n \n \n \n \n \n \nhence should be more resilient to damage/information loss occurring during weight updates. It is also is not\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nexpected to suffer from the vanishing gradient problem. However, further scalability testing is required for TAC\n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n(eg increasing network depth & breadth). \nIn terms of performance, TAC was found to learn in fewer iterations than supervised backpropagation. However this\n \n \n \n \n \n \n  \n  \n \n \n \n \n \n \n \nadvantage is expected to diminish as the number of output nodes, and therefore potential candidate behaviors,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nincrease. \n \n5.2 Limitations \n5.2.1 Timing issues and Actor-Critic architecture \nIn the examples provided the action function (TAP) is coupled with that of the value function (TAR), since they\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nboth share the same network architecture. Consequently the output for the two functions are derived\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsimultaneously. This presents a timing issue, since activations are fed forward through the action component before\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nit has been given the opportunity to learn from output of the value component. \nThis timing issue can be avoided by use of an actor-critic architecture [8][14], providing separate pathways for the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ntwo functions. The value function (TAR) can thereby be derived and used to dictate learning for itself and also for\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nthe action function (TAR) prior to new actions being issued.  \nThis would have implications regarding how we would expect connection weights in these two pathways to\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nrespond to output from the value function. In the value pathway we would expect weights to be based on prior (t-1)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n \n \nactivations (since new activations have passed through this pathway), but in the action pathway we would expect\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nweight updates to be based on existing (t) activations (since new activations have not yet passed through this\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \npathway). \n \n5.3 Final words \nWith reference to a threshold an entirely neural based explanation of Reinforcement Learning has been presented.\n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \nA threshold has been used to provide the roles of action generation and selection in Primary Reinforcement, the\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nroles of exploitation and exploration in Conditioned Reinforcement, and the role of credit assignment in\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n30 \nmulti-layered networks. These roles are interdependent and rely on the causal interaction between agent and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nenvironment. \nThe scheme presented provides a self organizing model of cognition, being fundamentally hedonistic with learning\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \ndriven by reward. The scheme is based on a single continuum of pleasure and pain, with a single learning\n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n  \n \n \nmechanism for the two; pleasure effectively being the alleviation of pain. Whilst the examples provided are\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nconfigured with only a single drive, this should be extended to multiple drives working in concert/competition. As\n \n \n  \n \n \n \n \n \n  \n \n \n  \n \n \na rule of thumb the agent should conform to a ‘laziness principle’, that is they should be punished for exerting\n \n \n \n \n \n \n \n \n  \n \n \n  \n \n \n \n \n \n \nunnecessary energy, thereby achieving their goal with greater efficiency. The examples provided entail the drive\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nbeing always active, and therefore continually reinforcing or weakening behaviors. However, it should be taken\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nthat when not active (eg drives are satiated), behaviors will not be reinforced or weakened and the agent will not\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nengage in active learning. This does not preclude the scheme being augmented with other forms of learning (eg\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nHebbian). Also while the examples presented are based on feedforward architectures the present scheme is not\n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \nmutually exclusive with recurrency [13]. Indeed it is envisaged LSTM [12] and other methods of recurrency could\n \n \n \n \n \n   \n \n \n \n \n \n \n \n \n \nbe used to enhance the presented scheme, providing information of prior state.  \nThe scheme’s true potential lies in embodiment, whether in real or artificial environments. Agents may also be\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nplaced in abstract environments that do not physically exist (eg a stock market). The scheme is generic and readily\n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n \n \n \nscalable, and given sufficient time and resources may tackle a wide variety of tasks. Tasks can be assigned either by\n \n \n \n \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \nsetting of the reward condition goal, or by placing in the path to that goal. If the presented scheme of Reinforcement\n \n \n \n \n \n \n \n \n \n \n \n  \n \n  \n \n \n \n \n \nLearning should be deemed biologically or psychologically implausible, the practical benefits remain of utility.  \n5.4 Appendix \n5.4.1 Platform \nDesktop PC (i3 3.7GHz, 16GB ram) \nOS Linux Mint 17.3  \nNeural Network software written in C (GCC) : ​https://github.com/thward/neural_agent \n \n6 References \n[1] Charles W. Anderson. 1986. Learning and Problem Solving with Multilayer Connectionist Systems.\n \n \n \n \n \n \n \n \n \n \n \n \nTechnical Report. University of Massachusetts, Amherst, MA, USA. \n[2] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nM.(2013). Playing atari with deep reinforcement learning.arXiv preprint arXiv:1312.5602 \n[3] Rumelhart, David E., Hinton, Geoffrey E., Williams, Ronald J., “Learning representations by\n \n \n \n \n \n \n \n \n \n \n \n \nback-propagating errors”, Nature, 1986 \n[4] D. G. Stork, \"Is backpropagation biologically plausible?,\" ​Neural Networks, 1989. IJCNN., International\n \n \n \n \n \n \n \n \n \n \n \n \nJoint Conference on, Washington, DC, USA, 1989, pp. 241-246 vol.2. \n[5] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998. \n[6] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines\n \n \n \n \n \n \n  \n \n \n  \n \n \n \nthat learn and think like people. arXiv preprint arXiv:1604.00289, 2016. \n[7] Gerald Tesauro, ”Temporal difference learning and TD-Gammon”, Communications of the ACM CACM\n \n \n \n \n \n \n \n \n \n \n \n \nHomepage archive, Volume 38 Issue 3, March 1995, Pages 58-68 \n[8] Andrew G. Barto, Richard S. Sutton, Charles W. Anderson, “Neuronlike adaptive elements that can solve\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ndifficult\nlearning\ncontrol\nproblems”,\nIEEE\nTransactions\non\nSystems,\nMan,\nand\nCybernetics\n \n \n \n \n \n \n \n \n \n \n \n(Volume:SMC-13 ,  Issue: 5) pages 834 - 846, 1983 \n31 \n[9] Skinner, B. F. (1953). Science and human behavior. New York: Macmillan, Pages. 92–3 \n[10]William Bechtel, Adele Abrahamsen, “Connectionism and the mind”, Wiley,1991, pages 70-97 Skinner,\n \n \n \n \n \n \n \n \n \n \n \n \nB. F. (1953). Science and human behavior. New York: Macmillan, Pages. 92–3 \n[11]Hodgkin, A. L., A. F. Huxley, and B. Katz. “Measurement of Current-Voltage Relations in the Membrane\n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \nof the Giant Axon of Loligo.” The Journal of Physiology 116.4 (1952): 424–448. \n[12]Sepp\nHochreiter\nand\nJürgen\nSchmidhuber\n(1997). \"Long short-term memory\" (PDF). Neural\n \n \n \n \n \n \n \n \n \n \n \nComputation. 9 (8): 1735–1780. ​http://dx.doi.org/10.1162/neco.1997.9.8.1735 \n[13]Elman,\nJ.\nL.\n(1990),\nFinding\nStructure\nin\nTime.\nCognitive\nScience,\n14:\n179–211.\n \n \n \n \n \n \n \n \n \n \n \n \ndoi:10.1207/s15516709cog1402_1 \n[14]Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., Silver, D., and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nKavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. arXiv preprint\n \n \n \n \n \n \n \n \n \n \n \narXiv:1602.01783. \n[15]Bengio, Y., Lee, D.-H., Bornschein, J., and Lin, Z. (2015). Towards biologically plausible deep learning.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \narXiv preprint arXiv:1502.04156. \n \n32 \n",
  "categories": [
    "cs.LG",
    "cs.NE"
  ],
  "published": "2016-09-12",
  "updated": "2017-01-14"
}