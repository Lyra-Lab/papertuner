{
  "id": "http://arxiv.org/abs/1609.06086v1",
  "title": "Modelling Stock-market Investors as Reinforcement Learning Agents [Correction]",
  "authors": [
    "Alvin Pastore",
    "Umberto Esposito",
    "Eleni Vasilaki"
  ],
  "abstract": "Decision making in uncertain and risky environments is a prominent area of\nresearch. Standard economic theories fail to fully explain human behaviour,\nwhile a potentially promising alternative may lie in the direction of\nReinforcement Learning (RL) theory. We analyse data for 46 players extracted\nfrom a financial market online game and test whether Reinforcement Learning\n(Q-Learning) could capture these players behaviour using a risk measure based\non financial modeling. Moreover we test an earlier hypothesis that players are\n\"na\\\"ive\" (short-sighted). Our results indicate that a simple Reinforcement\nLearning model which considers only the selling component of the task captures\nthe decision-making process for a subset of players but this is not sufficient\nto draw any conclusion on the population. We also find that there is not a\nsignificant improvement of fitting of the players when using a full RL model\nagainst a myopic version, where only immediate reward is valued by the players.\nThis indicates that players, if using a Reinforcement Learning approach, do so\nna\\\"ively",
  "text": "Modelling Stock-market Investors as\nReinforcement Learning Agents [Correction]\nAlvin Pastore\nDepartment of Computer Science\nUniversity of Shefﬁeld\nShefﬁeld, United Kingdom\nEmail: apastore1@shefﬁeld.ac.uk\nUmberto Esposito\nDepartment of Computer Science\nUniversity of Shefﬁeld\nShefﬁeld, United Kingdom\nEmail: acp12ue@shefﬁeld.ac.uk\nEleni Vasilaki\nDepartment of Computer Science\nUniversity of Shefﬁeld\nShefﬁeld, United Kingdom\nEmail: e.vasilaki@shefﬁeld.ac.uk\nAbstract—Decision making in uncertain and risky environ-\nments is a prominent area of research. Standard economic\ntheories fail to fully explain human behaviour, while a potentially\npromising alternative may lie in the direction of Reinforcement\nLearning (RL) theory. We analyse data for 46 players extracted\nfrom a ﬁnancial market online game and test whether Rein-\nforcement Learning (Q-Learning) could capture these players\nbehaviour using a risk measure based on ﬁnancial modeling.\nMoreover we test an earlier hypothesis that players are “na¨ıve”\n(short-sighted). Our results indicate that a simple Reinforcement\nLearning model which considers only the selling component of the\ntask captures the decision-making process for a subset of players\nbut this is not sufﬁcient to draw any conclusion on the population.\nWe also ﬁnd that there is not a signiﬁcant improvement of ﬁtting\nof the players when using a full RL model against a myopic\nversion, where only immediate reward is valued by the players.\nThis indicates that players, if using a Reinforcement Learning\napproach, do so na¨ıvely.\nI.\nINTRODUCTION\nOne of the most challenging ﬁelds of research is human\ndecision-making. Understanding the processes involved and\ntrying to predict or replicate behaviours has been, historically,\nthe ultimate goal of many disciplines. Economics for example,\nhas a long tradition of trying to formalise human behaviour\ninto descriptive or normative models. These models have\nbeen employed for several years (e.g. Expected Utility model\n[1]) but have been proven to be inadequate [2]–[5], giving\nrise to new research areas like behavioural and experimental\neconomics. Psychology as well, is natively concerned with\ndecision-making. Sequential decision problems have been used\nto evaluate people’s risk attitude, in order to predict actual risk\nproneness in real life scenarios [6]–[8]. While economics and\npsychology are focused on the high-level manifestations and\nimplications of decision-making, neuroscience aims at under-\nstanding the biological machinery and the neural processes\nbehind human (or animal) behaviour [9]–[15].\nRecently these ﬁelds of research have started to collaborate,\ncontributing to the rise of an emerging multi-disciplinary ﬁeld\ncalled neuroeconomics [16]–[20]. This discipline approaches\nthe problem from several perspectives and on different levels\nof abstraction. RL is a theoretical framework [21], exten-\nsively used in neuroeconomics literature for addressing a wide\narray of problems involving learning in partially observable\nenvironments [22]–[27]. RL is based on the concept of re-\nward/punishment for the actions taken. The agents act in an\nenvironment of which they possess only partial knowledge.\nTo be able to achieve the best behaviour, i.e. maximise their\nreward, the agents have to learn through experience and update\ntheir beliefs. Learning happens as a result of the agent’s\ninterpretation of the interactions with its surroundings and the\nconsequences of a “reward” feedback signal. The ability of this\nframework to model and therefore understand behavioural data\nand its underlying neural implications, is of pivotal importance\nin decision making [28].\nRL can accurately capture human and animal learning\npatterns and has been proven effective at describing the\nfunctioning of some areas of the human brain, like the\nbasal ganglia, and the functions of neurotransmitters such\nas dopamine [22], [29]–[31]. One of the most remarkable\nsimilarities between biological functioning and RL models\nis the one about Temporal Difference (TD) error [21], [32]–\n[34] and the activation of mid-brain dopamine neurons [35]–\n[40]. These ﬁndings supported the notion that TD Learning is\nimplemented in the brain with dopaminergic neurons in the\nstriatum [29], [34], [41]–[50], [78], making it a reasonable\nﬁrst choice for a modelling attempt. Humans and animals are\nvery advanced signal detectors whose behaviour is susceptible\nto changes in the rewards resulting from their choices [51],\n[52]. Both neuroscience and psychology have extensively\nemployed tasks in which the exploration-exploitation trade-\noff was of crucial importance [20], [53]–[56]. It is crucial for\nthe individuals to maximise their reward using the information\nat their disposal but to do so advantageously they need to\nlearn which actions lead to better rewards. Decision making\nin uncertain environments is a challenging problem because of\nthe competing importance of the two strategies: exploitation\nis, of course, the best course of action, but only when enough\nknowledge about the quality of the actions is available, while\nexploration increases the knowledge about the environment.\nA complicated task that encompasses all these features is\nstocks selection in ﬁnancial markets, where investors have\nto choose among hundreds of possible securities to create\ntheir portfolio. Stock trends are non-monotonic because they\nare not guaranteed to achieve a global maximum and the\nfuture distribution of reward is intrinsically stochastic. After\npurchasing a stock, investors are faced with the decisions on\nwhen to sell it (Market timing problem [56]). To be able to\nachieve the best return from their investments, people need\nto be careful in considering how to maximise their proﬁt in\nthe long term and not only in a single episode. We speculate\nthat RL is part of the decision making process for investors.\narXiv:1609.06086v1  [cs.CE]  20 Sep 2016\nFig. 1.\nModels comparisons. Figure (a) shows the goodness of ﬁt (Maximum Likelihood Estimate) of the myopic model (2 free parameters, with γ = 0)\nagainst the random model. Each bar represents the corresponding player’s MLE while the hollow diamond represents the MLE of the random model. The height\nof the bars differs greatly as the players have different amount of transactions. The asterisks indicate the players better ﬁtted by the myopic version of our RL\nmodel as opposed to a random model. For 15% of the players the myopic model ﬁts better than random (players 11, 18, 28, 31, 33, 37, 42 with p-value < 0.05\ncalculated with the Likelihood ratio χ2 test at 95% conﬁdence level). Figure (b) shows the same comparison, this time with the result of the Likelihood Ratio\nTest portrayed on the y-axis. The horizontal line represent the threshold (critical value) for statistical signiﬁcance which depends on the degrees of freedom. For\nmyopic model versus random dof = 2 and critical value is 5.991. Figures (c) and (d) show the comparison for the ﬁtting of the full RL (3 free parameters)\nmodel against the simpler version (γ = 0) model. In (c) each bar represent a player’s full RL model MLE while the ﬁlled diamond represents the MLE of the\nmyopic model. Players who are better ﬁtted by a full model are marked with an asterisk (statistically signiﬁcant at 95% conﬁdence with the same test as the\nprevious comparison). Even if there is a statistical improvement in the ﬁtting for 6 players (12, 14, 30, 39, 40, 44) this does not translate in a better overall\nperformance of the full RL model when it is compared to the random model. Figures (e) and (f) show this comparison. The full RL ﬁts better than random\nfor 7 players (11, 14, 18, 28, 31, 39, 42). Some of these are already captured by the simpler model, only two (14 and 39) were not already captured by the\nmyopic model. At the same time the full model loses two players (33 and 37). This is due to the fact that this comparison has a higher critical value (7.815 with\ndof = 3). These results have been tested for the entire population using Binomial Proportion Conﬁdence Intervals calculated with Clopper-Pearson method, the\nresults are negative as only 7 out of 46 players are better than random (Fig. 3 in the Appendix)\nThis speculation is supported by Choi et al. [57], who studied\nindividual investors decisions on 401(k) savings plans. Over\nthe years, investors could decide to increase or decrease the\npercentage of their salary to commit to this retirement plan.\nTheir results suggest that investors’ decisions are inﬂuenced\nby personal experiences: they show that those investors who\nhave experienced a positive reward from previous investment\nin their 401(k) fund, tend to increase the investment in that\nparticular product, compared to those who experienced a lower\nreward outcome. This kind of behaviour follows a “na¨ıve\nreinforcement learning” and is in contrast with the disposi-\ntion effect [59], [65](the unwillingness of investors to sell\n“losing” investments). Huang et al. investigated how personal\nexperience in investments affects future decisions about the\nselection of stocks [58]. They used data that spans from 1991\nto 1996, from a large discount broker. Again, the pattern of\nrepurchasing ﬁnancial product which yielded positive return\nwas found. As Huang suggests, by understanding the way past\nexperience affects investors’ decisions, it might be possible\nto make predictions about the ﬁnancial markets involved. RL\nhas also been used, with promising results, to develop Stock\nMarket Trading Systems [60]–[63] and to build Agent Based\nStock Market Simulations [64]. While these works use RL\nto predict future prices, they do not try to describe human\nbehaviour. With these notions as background we decided to\ninvestigate and try to model human choices in a stochastic,\nnon-stationary environment. We hypothesise that RL is a\ncomponent of decision making and to test this we compare\ntwo RL models against a purely random one. Our modelling\nattempts are based on two assumptions. First, we assume that\nrisk is a proxy of the internal representation of the actions\nfor some players. To test this we use a measure of systematic\nrisk widely used in ﬁnance and economics to categorise the\ndifferent choices into three discrete classes. We also assume\nthat the reward signal is based on the cash income arising\nfrom the sales an investor makes. This assumption follows a\nwidely researched behaviour referred to as “disposition effect”\nin literature [59], [65], the tendency of individual investors to\nsell stocks which increased in value since when they were\npurchased, while holding onto the stocks which lost value.\nThis phenomenon is stronger for individual investors but it\nalso exhibited by institutional investors, such as mutual funds\nFig. 2.\nComparison of the ranked vs scrambled discretisation. Test for\nthe assumption that investors use risk to internally classify the different\nstocks in discrete degrees of risk. The errorbars represent the probability and\n99% conﬁdence intervals of the comparison of the stock discretisation based\non the βF risk measure as opposed to 500 randomly generated scrambled\ndiscretisations. The 7 players for which the RL model outperformed the\nrandom model all have a statistically signiﬁcative probability of using a\nrepresentation of actions discretisation based on risk. The comparison of the\nentire dataset resulted in 31 players out of 46 (∼67%) to have a probability\nand CI above chance.\nand corporations [66]–[72]. Following these indications we\nmapped the sell transactions to a reward signal to ﬁt our\nmodels. Finally, we hypothesise that not all players are short-\nsighted, to test this we compare a full RL model (3 free\nparameters) against a myopic model (2 free parameters, no\ngamma). The difference is that the latter can be considered a\nna¨ıve RL as it does not take into account future rewards, it\nonly seeks to maximise immediate rewards.\nII.\nMETHOD\nA. Dataset\nThe dataset has been extracted from the publicly accessible\nonline trading simulation game VirtualTrader1, which is man-\naged by IEX Media Group BV in the Netherlands. Players can\nsubscribe for free and start playing the game with an assigned\nvirtual cash budget of 100k GBP. The players will then pick\nthe stocks they prefer from the FTSE100 stock index pool\n(107 stocks at the time of data collection) and create their\nown portfolio. These competitors are ranked according to the\nreturn of their investment. This is composed of “holdings”\nand “cash”. The former represent the shares possessed by a\nplayer while the latter is the amount of money not invested (i.e.\nderiving from sold stocks or never invested). The simulation\nfollows real world data evolution, for example price ﬂuctua-\ntions and price splits. The delay is usually in the order of 10-15\nminutes and the player can access a visual representation of\nthe stocks time series. All the transactions are stored for each\nplayer. For this study we considered transactions that span from\nthe 1st of January 2014 to the 31st of May 2014. This time\nperiod has been chosen because at that time the player ranking\nwas determining the winner of the monthly prize giveaway.\n1http://www.virtualtrader.co.uk - Copyright IEX Media Group BV\nTwo possible rewards can be identiﬁed: a psychological one,\nconsisting of the ranking position and a tangible one being the\nprize for the highest achiever.\nThe transactions have been stored in a database in order\nto be manipulated and used to ﬁt models with different\ncombination of free parameters. The rows are structured in 6\nﬁelds: Date, Type, Stock, Volume, Price and Total. The dataset\ninitially contained about 100k transactions that were reduced\nto about 1.4k. This was due to preprocessing, which removed\nthe many instances of inactive players who played only at\nthe beginning and/or at the end of the time frame considered.\nIn the ﬁnal version of the dataset there are 46 players. The\naverage amount of transactions per player is 30. The player\nwho played the most during the six months performed 107\ntransactions. We considered the full amount of transactions\neach player operated in the game.\nB. Reinforcement Learning Setup\nWe adopted a widely used off-policy RL framework called\nQ-learning [21]. The learning rule of this model is:\n∆Q(st, at) = α\n\u0014\nrt+1 + γ max\na\nQ(st+1, a) −Q(st, at)\n\u0015\n(1)\nwhere Q(st, at) represents the value of action a while in\nstate s, at time t. α ∈[0, 2] is the step-size parameter and\ncontrols the rate of learning. γ ∈[0, 1] is the discount factor\nand represents how far-sighted the model is, It encodes how\nmuch a future reward is worth at time t. When γ = 0 only\nimmediate rewards are taken into account by the player.\nTo test this framework the task has been mapped as follows.\nThere are two states (win, loss) calculated according to the\nproﬁt of the player (details in equations 6 and 7). These two\nstates reﬂect the dichotomy rooted in the Prospect Theory’s\nvalue function gain/loss spectrum [73].\nSince all players begin with the same initial budget our\ncalculation of the proﬁt uses the returns accumulated by selling\nstocks. This choice reduces the scope of the model, focusing on\nthe cash component of the players assets. This will be referred\nto as the “Sell” model. The actions are mapped to the stocks\navailable for trading.\nIn order to avoid dimensionality issues, 107 stocks for 2\nstates give rise to 214 potential actions, we decided to classify\nthe stocks in 3 classes of risk using a widely used ﬁnancial\nmodelling measure, CAPM Beta. The acronym stands for\nCapital Asset Pricing Model, a model developed by Sharpe[74]\nused to explain the relationship between the expected return of\na security and its risk . In this report we will refer to ﬁnancial\nvolatility measure CAPM Beta as βF :\nβF = Cov(ra, rb)\nV ar(rb)\n(2)\nThis ﬁnancial modelling measure quantiﬁes the volatility\nof a security in comparison to the market or a reference bench-\nmark [75]. Relatively safe investments like utilities stocks (e.g.\ngas and power) have a low βF , while high-tech stocks (e.g.\nNasdaq or MSH Morgan Stanley High-Tech) have a high βF .\nAs an example, the βF of the index of reference (that\nrepresents the portion of the market considered) is exactly 1.\nA βF ∈(0, 1) indicates that the asset has a lower volatility\ncompared to the market or low correlation of the asset price\nmovements compared to the market. While if βF > 1 it\nsigniﬁes an investment with higher volatility compared to\nthe benchmark. Following the previous example, high-tech\nsecurities with a βF > 1 could yield better returns compared\nto their benchmark index, when the market is going up. This\nalso poses more risk because in case the market loses value,\nthe security would lose value at a higher rate than the index.\nβF is considered a measure of the systematic risk and can\nbe estimated by regression. Considering the returns of an asset\na and the returns of the corresponding benchmark b:\nra = α + βrb\n(3)\nβF has been calculated for each stock in the FTSE100\nat the time of the game by considering daily returns in the\nyear between 1st June 2013 and 31st May 2014. The measure\nassociated with each stock is used to rank them and subdivide\nthem in three classes, containing respectively 36, 36 and 35\nstocks each.\nReward r at time step t+1 is deﬁned by the gain (or loss)\nmade in a sell transaction. Buying transactions are kept into\naccount to track players portfolios and to calculate the price\ndifference. They were not used as actions, but we might extend\nour modelling scenario by integrating a “Buy” model in the\nfuture and consider purchase actions by changing the reward\nscheme. The reward is calculated as:\nrt+1 = vt+1\n \npt+1 −1\nt\nt\nX\ni=1\nvipi\n!\n(4)\nwhere vi and pi are the volume and the price of the stock\ntraded at the i-th time step. The second term of the difference\nis a weighted average of the stock prices at previous times.\nTo avoid numerical instabilities, reward has been ﬂattened\nwith a sigmoid function into the range [−1, +1]. Speciﬁcally\na hyperbolic tangent has been used, with ρ = 500 to capture\nmost of the variability of the rewards, only ﬂattening the\nextreme values. This choice is in line with prospect theory\nvalue function which is concave for gains and convex for losses\n[73].\ntanh(r) = 1 −e−r/ρ\n1 + e−r/ρ\n(5)\nAs in this study we focused on the sell subset of the players\ninteractions, the states are based solely on proﬁt, which in turn\nis based on the reward of the sell transactions. The proﬁt and\nstates are deﬁned as:\nProfit =\nX\nt\ntanh(rt)\n(6)\nState =\n\u001a0,\nif Profit < 0.\n1,\notherwise.\n(7)\nThe RL framework is composed of a learning model (eq.\n1) and an action-selection model that is responsible for picking\nthe best action. In our setup the former is Q-learning and\nthe latter is Soft-Max. An action a is picked at time t with\nprobability:\nP(at) =\neQt(a)β\nPn\nb=1 eQt(b)β\n(8)\nwhere n is the number of available actions (i.e. 3 in\nthis study) and β is the inverse temperature parameter and\nrepresents the greediness of the action-selection model. In the\nlimit β →0 the actions become equiprobable and the model\nreverts to random. Higher values of β approximate a greedy\nmodel which picks the best known action (fully exploitative).\nThe full model has 3 free parameters: α (step-size parameter\nor learning rate), β (exploration-exploitation trade-off) and γ\n(discount factor). For this study we used a bounded gradient\ndescent search with 27 combinations of initial guess points.\nThese are the combination of values of the free parameters\nfrom where the search starts. By having different entry points\nwe hope to reduce the chance of the search getting stuck in a\nlocal minimum solution. The search has been performed with\nthe following boundaries:\n•\nα ∈(0.0001, 2)\n•\nβ ∈(0, 50)\n•\nγ ∈(0, 0.9999) (for the myopic model γ = 0)\nThe entry points are the combinations rising from the\nfollowing values:\n•\nα ∈{0, 0.5, 2}\n•\nβ ∈{0, 25, 50}\n•\nγ ∈{0, 0.5, 0.9999}\nThe search results have been obtained on python 2.7.9\nand scipy.optimize.minimize with scipy 0.17.1.\nC. Model Testing Routine\nMaximum Likelihood Estimate has been used as a measure\nof the model ﬁtness, following Daw’s comprehensive analysis\nof methodology [76]. MLE is the appropriate method to\nassess model performance because it evaluates which set of\nmodel parameters are more likely to generate the data using a\nprobabilistic approach. Data likelihood is a powerful method\nbecause it keeps into account the presence of noise in the\nchoices. It does so by using probability estimates for the\npotential actions.\nGiven a model M and its corresponding set of parameters\nθM the likelihood function is deﬁned as P(D|M, θM), where\nD is the dataset (the list of choices and the associated rewards).\nApplying Bayes’ rule:\nP(θM|D, M) ∝P(D|M, θM) · P(θM|M)\n(9)\nThe left hand side of the proportionality is the posterior\nprobability distribution over the free parameters, given the data.\nThis quantity is proportional to the product of the likelihood\nof the data, given the parameters and the prior probability of\nthe parameters. Treating the latter as ﬂat we obtain that the\nmost probable value for θM (the best set of free parameters)\nis the Maximum Likelihood Estimate (MLE), that is the\nset of parameter which maximises the likelihood function,\nP(D|M, θM) and it is commonly denoted ˆθM. The likelihood\nfunction is maximised through the following process. At each\ntimestep, for every action, the observation model (Soft-Max)\nestimates a probability. These probabilities are then multiplied\ntogether. To avoid numerical problems that could arise when\nmultiplying probabilities, the sum of their logarithm is cal-\nculated instead. The negative of this value, also known as\nNegative Log-Likelihood, is then used. The aim is then to\nminimise this quantity, which is the equivalent of maximising\nthe likelihood function, P(D|M, θM). In the future we will\nrefer to the Negative Log-Likelihood as MLE for simplicity,\nkeeping in mind that lower values represent better ﬁt.\nThe values of MLE generated represent the goodness of ﬁt\nof the model with its associated set of parameters. To compare\nthe selected model with a random model and for statistical\nsigniﬁcance we adopted the Likelihood Ratio Test [77]. This\nstatistical test uses the likelihood ratio to compare two nested\nmodels and takes into account the different number of free\nparameters of the two. It encapsulates this information, when\ntesting for signiﬁcance, using the difference of the two amounts\nas degrees of freedom for the chi-square (χ2) test. Since the\ntest statistic is distributed χ2 it is straightforward to estimate\nthe p-value associated with the χ2 value.\nThe baseline for comparison is a random model which\nhas 0 parameters as there is no learning involved and the\naction-selection policy is random ( 1\n3 chance of picking any\nof the three stock bins). The ﬁrst comparison is between this\nrandom model and the simpler of the proposed models, which\nhas only two free parameters (γ = 0). This setup represent\nthe na¨ıve learning procedure that could explain investors’\nbehaviour showed in literature[57]. The full model, with all the\nthree free parameters, has also been tested against the myopic\nmodel to assess whether some players are better ﬁtted by a\nmore complex version of the framework. Finally the model\ngoodness of ﬁt has been evaluated with the adopted action\nclassiﬁcation (based on risk) against 500 randomly generated\nstock classiﬁcations. This has been done to test the assumption\nthat players internally classify the stock range into discrete\ndegrees of risk.\nIII.\nRESULTS\nResults for the test of the hypothesis that RL is a com-\nponent of decision making are shown in Fig. 1. The best set\nof parameters was found according to MLE through gradient\ndescent search. The best model MLE has been compared to\nthe random model MLE using the Likelihood Ratio Test [77].\nThe random model MLE is easily estimated as:\nP(D|Mrand) = log\nNt\nY 1\n3 =\nNt\nX\nlog 1\n3\n(10)\nwhere Nt is the number of transactions for each player\nin the dataset. As shown in Fig. 1 (a) and (b) 15% of the\nplayers in our dataset is better ﬁtted by a myopic RL model\nas opposed to a random model. In Fig. 1 (c) and (d) we report\nan improvement in the ﬁtting for some players using a full\nRL model against the myopic (nested) version of the model.\nThis improvement is not reﬂected in the comparison of the full\nRL model against the random model, as shown in Fig. 1 (e)\nand (f). Most of the players that can be ﬁtted with our models\nare well represented by a myopic model. These results follow\nwhat found by Choi et al.[57] and Huang et al. [58]. We made\nthe assumption that players, when faced with the choice to\ntrade many stocks (107 for this task), internally model these\nin discrete groups of risk using readily available information\nsuch as stock historical prices and returns, which in turn are\nused to estimate their volatility (βF ). To test whether this\nassumption holds true for the players in our dataset, we ran the\nsimpler version of our model on the risk-ranked discretisation\nand on 500 independent and randomly scrambled discreti-\nsations. The results are shown in Fig. 2 and are generated\nusing Bayesian Information Criterion (BIC) as a measure for\ncomparison of ﬁtness and Binomial Proportion Conﬁdence In-\nterval calculated with Clopper-Pearson method using Matlab\n8.4.0.150421 (R2014b) function binofit. The BIC\nhas been used as the Likelihood Ratio Test can only be used to\ncompare nested models, while in this case the comparison is\nbetween models with the same number of parameters that are\ntested on different data arrangements. This procedure estimates\nthe probability that the ranked discretisation is better than the\n500 scrambled discretisations (BICranked < BICs, where\nBICs is the BIC for the s-th scrambled). The results shown\nin Fig. 2 are for 99% conﬁdence interval. The results shown\nare only for those 7 players who are ﬁtted signiﬁcantly by the\nmyopic model. As shown in Fig. 2, all the players are well\nabove the chance threshold. This indicates that risk based on\nhistorical data could be considered a proxy for action selection\nfor the players who are well ﬁtted by our RL myopic model.\nIV.\nCONCLUSION\nWe investigated a publicly available dataset consisting of\ntrading transactions operated by players of an investment game.\nWe based the discretisation of the actions on the assumption\nthat risk can capture the internal modelling that players operate\nwhen facing this task. This assumption was shown to hold\ntrue and be statistically signiﬁcant for a subset of the players,\n31 out of 46 and speciﬁcally for the 7 players who are best\nﬁtted by a RL model. This could signify that the remaining\nplayers might use other types of discretisation techniques based\non different measures (or a combination of them) or they do\nnot use technical analysis but fundamental analysis (e.g. using\nﬁnancial statements and reports). In this work, we investigated\na model which combines two versions of a Reinforcement\nLearning framework using Q-learning as an update rule and\nSoft-Max as action-selection policy on a discretised action\nspace according to the risk measure βF . It is possible that\ndifferent model combinations, which use different learning\nrules or different measures of risk, ﬁt the players population in\nour dataset better. It is also likely that, by restricting our focus\non the sell model, we missed some features of what constitutes\nthe reward signal that players receive. In the full version of the\ngame, in fact, players might try to maximise both holdings and\ncash simultaneously, in order to compete in the ranking.\nThe myopic model is a nested version with only two free\nparameters, representing the learning rate (α) and the degree\nof greediness (β). The full version extends the simpler model\nwith a discount factor (γ) which regulates how much of the\nfuture rewards is taken into account when updating the values\nof present state-action pairs. 15% of the players are well\nﬁtted by a RL model with γ = 0 and there is no signiﬁcant\nimprovement of ﬁtting by extending this model including\ngamma as a free parameter. Previous literature pointed in the\ndirection of investors being na¨ıve (short-sighted) [57], [58]\nand these results, albeit for a subset of the dataset, conﬁrm\nthis indication. The hypothesis that RL is a component of the\ndecision making process for some investors is not conﬁrmed\nas either version of the tested model (short or far-sighted)\nis statistically better than chance only for a subset of the\nplayers. This subset, within this population, is not large enough\nto draw a statistically meaningful positive result. By means\nof a Binomial Proportion Conﬁdence Interval calculated with\nClopper-Pearson method we get a negative result for the entire\npopulation within a 99% conﬁdence interval (Fig. 3 in the Ap-\npendix). While this exploratory study gives some perspectives\non how Reinforcement Learning can be used to model learning\nand action-selection for investing problems, future work will\nfocus on different models and risk classiﬁcation techniques as\nwell as on a deeper investigation of the typical parameters of\nthe best performing players and the correlation of different\nstrategies and performance of stock trading together with a\nstudy of different RL models.\nACKNOWLEDGMENT\nThe authors would like to thank their colleagues of the\nShefﬁeld Neuroeconomics interdisciplinary research group for\ninsightful discussion.\nREFERENCES\n[1]\nJ. Von Neumann and O. Morgenstern, Theory of games and economic\nbehavior. 1947.\n[2]\nC. Stramer, “Developments in Non-Expected Utility Theory: The Hunt\nfor a Descriptive Theory of Choice under Risk.” pp. 332–382, 2000.\n[3]\nS. Frederick, G. Loewenstein, and T. O’Donoghue, “Time Discounting\nand time preference: a critical review.” Journal of Economic Literature,\nvol. XL, pp. 351–401, 2002.\n[4]\nA. Tversky and D. Kahneman, “Judgment under uncertainty : heuristics\nand biases.” Science, vol. 185, no. 4157, pp. 1124–1131, 1974.\n[5]\nD. Kahneman, J. L. Knetsch, and R. H. Thaler, “Anomalies: The\nEndowment Effect, Loss Aversion, and Status Quo Bias.” Journal of\nEconomic Perspectives, vol. 5, no. 1, pp. 193–206, 1991.\n[6]\nU. Hoffrage, A. Weber, R. Hertwig, and V. M. Chase, “How to\nKeep Children Safe in Trafﬁc: Find the Daredevils Early.” Journal of\nExperimental Psychology: Applied, vol. 9, no. 4, pp. 249–260, 2003.\n[7]\nT. J. Pleskac, “Decision making and learning while taking sequential\nrisks.” Journal of Experimental Psychology: Learning, vol. 34, no. 1,\npp. 167–185, 2008.\n[8]\nT. S. Wallsten, T. J. Pleskac, and C. W. Lejuez, “Modeling Behavior\nin a Clinically Diagnostic Sequential Risk-Taking Task.” Psychological\nReview,, vol. 112, no. 4, pp. 862–880, 2005.\n[9]\nK. H. Britten, W. T. Newsome, M. N. Shadlen, S. Celebrini, and J. a.\nMovshon, “A relationship between behavioral choice and the visual\nresponses of neurons in macaque MT.” Visual neuroscience, vol. 13,\nno. 1, pp. 87–100, 1996.\n[10]\nK. H. Britten, M. N. Shadlen, W. T. Newsome, and J. a. Movshon, “The\nanalysis of visual motion: a comparison of neuronal and psychophysical\nperformance.” The Journal of neuroscience: the ofﬁcial journal of the\nSociety for Neuroscience, vol. 12, no. 12, pp. 4745–4765, 1992.\n[11]\nJ. I. Gold and M. N. Shadlen, “Neural computations that underlie\ndecisions about sensory stimuli.” Trends in Cognitive Sciences, vol. 5,\nno. 1, pp. 10–16, 2001.\n[12]\n——, “Banburismus and the brain: Decoding the relationship between\nsensory stimuli, decisions, and reward.” Neuron, vol. 36, no. 2, pp.\n299–308, 2002.\n[13]\n——, “The neural basis of decision making.” Annual review of neuro-\nscience, vol. 30, pp. 535–574, 2007.\n[14]\nM. N. Shadlen, K. H. Britten, W. T. Newsome, and J. a. Movshon,\n“A computational analysis of the relationship between neuronal and\nbehavioral responses to visual motion.” J Neurosci, vol. 16, no. 4, pp.\n1486–1510, 1996.\n[15]\nM. N. Shadlen and W. T. Newsome, “Motion perception: seeing and\ndeciding.” Proceedings of the National Academy of Sciences of the\nUnited States of America, vol. 93, no. 2, pp. 628–633, 1996.\n[16]\nA. G. Sanfey, G. Loewenstein, S. M. McClure, and J. D. Cohen,\n“Neuroeconomics: cross-currents in research on decision-making.”\nTrends in cognitive sciences, vol. 10, no. 3, pp. 108–16, mar 2006.\n[17]\nP. W. Glimcher and A. Rustichini, “Neuroeconomics: the consilience\nof brain and decision.” Science (New York, N.Y.), vol. 306, no. 5695,\npp. 447–52, oct 2004.\n[18]\nG. Loewenstein, S. Rick, and J. D. Cohen, “Neuroeconomics.” Annual\nreview of psychology, vol. 59, pp. 647–72, jan 2008. [Online].\n[19]\nD. J. Barraclough, M. L. Conroy, and D. Lee, “Prefrontal cortex and\ndecision making in a mixed-strategy game.” Nature neuroscience, vol. 7,\nno. 4, pp. 404–410, 2004.\n[20]\nA. Soltani, D. Lee, and X. J. Wang, “Neural mechanism for stochastic\nbehaviour during a competitive game.” Neural Networks, vol. 19, no. 8,\npp. 1075–1090, 2006.\n[21]\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An introduction,\nA Bradford Book, Ed.\nMIT Press, Cambridge, MA, 1998.\n[22]\nK. Doya, “Reinforcement learning: Computational theory and biological\nmechanisms.” HFSP Journal, vol. 1, no. 1, p. 30, 2007.\n[23]\nG. Tesauro, “TD-Gammon, a Self-Teaching Backgammon Program,\nAchieves Master-Level Play.” Neural Computation, vol. 6, no. 2, pp.\n215–219, 1994.\n[24]\nA. Abbeel, P, Coates, A, Morgan, Q, Ng, “An Application of Re-\ninforcement Learning to Aerobatic Helicopter Flight.” Advances in\nNeural Information Processing Systems 19: Proceedings of the 2006\nConference, 1-9. 2006.\n[25]\nR. Hafner and M. Riedmiller, “Neural reinforcement learning controllers\nfor a real robot application.” Proceedings - IEEE International Confer-\nence on Robotics and Automation, no. April, pp. 2098–2103, 2007.\n[26]\nM. a. Walker, “An Application of Reinforcement Learning to Dialogue\nStrategy Selection in a Spoken Dialogue System for Email.” Journal of\nArtiﬁcial Intelligence Research, vol. 12, p. 387, 2000.\n[27]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. a. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis, “Human-level control through\ndeep reinforcement learning.” Nature, vol. 518, no. 7540, pp. 529–533,\n2015.\n[28]\nP. Dayan and N. D. Daw, “Decision theory, reinforcement learning,\nand the brain.” Cognitive, affective & behavioral neuroscience, vol. 8,\nno. 4, pp. 429–53, dec 2008.\n[29]\nW. Schultz, P. Dayan, and P. R. Montague, “A neural substrate of\nprediction and reward.” Science (New York, N.Y.), vol. 275, no. 5306,\npp. 1593–9, mar 1997.\n[30]\nK. Doya, “Complementary roles of basal ganglia and cerebellum in\nlearning and motor control.” Current Opinion in Neurobiology, vol. 10,\npp. 732–739, 2000.\n[31]\nN. D. Daw and K. Doya, “The computational neurobiology of learning\nand reward.” Current Opinion in Neurobiology, vol. 16, no. 2, pp. 199–\n204, 2006.\n[32]\nC. J. C. H. Watkins,, “Learning from delayed rewards.” University of\nCambridge , 1989.\n[33]\nP. Werbos, “A menu of designs for reinforcement learning over time.\nIn Neural Networks for Control .” Neural Networks for Control, MIT\nPress, Cambridge, Massachusetts, pp. 67–95, 1990.\n[34]\nO. Hikosaka, K. Nakamura, and H. Nakahara, “Basal ganglia orient\neyes to reward.” Journal of neurophysiology, vol. 95, no. 2, pp. 567–\n584, 2006.\n[35]\nA. Schultz, W, Romo, R, Ljungberg, T, Mirenowicz, J, Hollerman, JR,\nand Dickson, “Reward-related signals carried by dopamine neurons.” in\nModels of Information Processing in the Basal Ganglia, M. Cambridge,\nEd., 1995, pp. 233–248.\n[36]\nR. E. Suri and W. Schultz, “Learning of sequential movements by neural\nnetwork model with dopamine-like reinforcement signal.” Experimental\nBrain Research, vol. 121, no. 3, pp. 350–354, 1998.\n[37]\nP. Waelti, a. Dickinson, and W. Schultz, “Dopamine responses comply\nwith basic assumptions of formal learning theory.” Nature, vol. 412,\nno. 6842, pp. 43–48, 2001.\n[38]\nT. Satoh, S. Nakai, T. Sato, and M. Kimura, “Correlated coding of\nmotivation and outcome of decision by dopamine neurons.” The Journal\nof neuroscience : the ofﬁcial journal of the Society for Neuroscience,\nvol. 23, no. 30, pp. 9913–9923, 2003.\n[39]\nH. Nakahara, H. Itoh, R. Kawagoe, Y. Takikawa, and O. Hikosaka,\n“Dopamine Neurons Can Represent Context-Dependent Prediction Er-\nror.” Neuron, vol. 41, no. 2, pp. 269–280, 2004.\n[40]\nG. Morris, A. Nevet, D. Arkadir, E. Vaadia, and H. Bergman, “Mid-\nbrain dopamine neurons encode decisions for future action.” Nature\nneuroscience, vol. 9, no. 8, pp. 1057–1063, 2006.\n[41]\nA. Barto, “Adaptive critics and the basal ganglia.” in Models of\nInformation Processing in the Basal Ganglia, M. Cambridge, Ed., 1995,\npp. 215–232.\n[42]\nP. R. Montague, P. Dayan, and T. J. Sejnowski, “A framework for mes-\nencephalic dopamine systems based on predictive Hebbian learning.”\nThe Journal of neuroscience : the ofﬁcial journal of the Society for\nNeuroscience, vol. 16, no. 5, pp. 1936–1947, 1996.\n[43]\nK. Samejima, Y. Ueda, K. Doya, and M. Kimura, “Representation of\naction-speciﬁc reward values in the striatum.” Science (New York, N.Y.),\nvol. 310, no. 5752, pp. 1337–1340, 2005.\n[44]\nR. Kawagoe, Y. Takikawa, and O. Hikosaka, “Expectation of reward\nmodulates cognitive signals in the basal ganglia.” Nature neuroscience,\nvol. 1, no. 5, pp. 411–416, 1998.\n[45]\n——, “Reward-predicting activity of dopamine and caudate neurons–a\npossible mechanism of motivational control of saccadic eye movement.”\nJournal of neurophysiology, vol. 91, no. 2, pp. 1013–1024, 2004.\n[46]\nW. Schultz, “Getting formal with dopamine and reward.” Neuron,\nvol. 36, no. 2, pp. 241–263, 2002.\n[47]\nJ. J. Day, M. F. Roitman, R. M. Wightman, and R. M. Carelli,\n“Associative learning mediates dynamic shifts in dopamine signaling\nin the nucleus accumbens.” Nature neuroscience, vol. 10, no. 8, pp.\n1020–1028, 2007.\n[48]\nS. E. Hyman, R. C. Malenka, and E. J. Nestler, “Neural mechanisms\nof addiction: the role of reward-related learning and memory.” Annual\nreview of neuroscience, vol. 29, pp. 565–598, 2006.\n[49]\nD. Joel, Y. Niv, and E. Ruppin, “Actor-critic models of the basal ganglia:\nnew anatomical and computational perspectives.” Neural Networks,\nvol. 15, no. 4-6, pp. 535–547, 2002.\n[50]\nJ. R. Wickens, J. C. Horvitz, R. M. Costa, and S. Killcross, “Dopamin-\nergic mechanisms in actions and habits.” The Journal of neuroscience\n: the ofﬁcial journal of the Society for Neuroscience, vol. 27, no. 31,\npp. 8181–8183, 2007.\n[51]\nA. a. Stocker and E. P. Simoncelli, “Noise characteristics and prior\nexpectations in human visual speed perception.” Nature neuroscience,\nvol. 9, no. 4, pp. 578–585, 2006.\n[52]\nK. K¨ording, “Decision theory: what ”should” the nervous system do?”\nScience (New York, N.Y.), vol. 318, no. 5850, pp. 606–610, 2007.\n[53]\nN. D. Daw, J. P. O’Doherty, P. Dayan, B. Seymour, and R. J. Dolan,\n“Cortical substrates for exploratory decisions in humans.” Nature, vol.\n441, no. 7095, pp. 876–879, 2006.\n[54]\nG. Luksys, W. Gerstner, and C. Sandi, “Stress, genotype and\nnorepinephrine in the prediction of mouse behavior using reinforcement\nlearning.” Nature neuroscience, vol. 12, no. 9, pp. 1180–6, sep 2009.\n[55]\nR. Frey and R. Hertwig, “Sell in May and Go Away ? Learning and Risk\nTaking in Nonmonotonic Decision Problems.” Journal of Experimental\nPsychology, vol. 41, no. 1, pp. 193–208, 2015.\n[56]\nS. Benartzi and R. H. Thaler, “Heuristics and Biases in Retirement\nSavings Behavior.” Journal of Economic Perspectives, vol. 21, no. 3,\npp. 81–104, 2007.\n[57]\nJ.\nChoi and\nD.\nLaibson, “Reinforcement\nlearning\nand\nsavings\nbehavior.” The Journal of Finance, vol. 64, no. 6, 2009.\n[58]\nX. Huang, “Industry Investment Experience and Stock Selection.”\nAvailable at SSRN 1786271, no. November, 2012.\n[59]\nT. Odean, “Are Investors Reluctant to Realize Their Losses ?” vol. LIII,\nno. 5, pp. 1775–1798, 1998.\n[60]\nY. Chen, S. Mabu, K. Hirasawa, and J. Hu, “Trading rules on stock\nmarkets using genetic network programming with sarsa learning.”\nProceedings of the 9th annual conference on Genetic and evolutionary\ncomputation GECCO 07, vol. 12, p. 1503, 2007.\n[61]\nJ. Lee, “Stock price prediction using reinforcement learning.” Industrial\nElectronics. Proceedings. ISIE 2001. IEEE International Symposium\non, vol. 1, pp. 690–695, 2001.\n[62]\nJ. O, J. Lee, J. Lee, and B. Zhang, “Adaptive stock trading with\ndynamic asset allocation using reinforcement learning.” Information\nSciences, vol. 176, no. 15, pp. 2121–2147, 2006.\n[63]\nJ. Moody and M. Saffell, “Learning to trade via direct reinforcement.”\nIEEE Transactions on Neural Networks, vol. 12, no. 4, pp. 875–889,\n2001.\n[64]\nA. V. Rutkauskas and T. Ramanauskas, “Building an artiﬁcial stock\nmarket populated by reinforcementlearning agents.” Journal of Business\nEconomics and Management, vol. 10, no. 4, pp. 329–341, 2009.\n[65]\nB. M. Barber and T. Odean, “The Behavior of Individual Investors.”\nHandbook of the Economics of Finance, vol. 2, PB, pp. 1533–1570,\n2013\n[66]\nB. M. Barber, Y. T. Lee, Y. J. Liu, and T. Odean, “Is the aggregate\ninvestor reluctant to realise losses? Evidence from Taiwan.” European\nFinancial Management, vol. 13, no. 3, pp. 423–447, 2007.\n[67]\nP. Brown, N. Chappel, R. Da Silva Rosa, and T. Walter, “The Reach\nof the Disposition Effect: Large Sample Evidence Across Investor\nClasses.” International Review of Finance, vol. 6, no. 1-2, p. 43, 2006.\n[68]\nA. Frazzini, “The disposition effect and underreaction to news.” Journal\nof Finance, vol. 61, no. 4, pp. 2017–2046, 2006.\n[69]\nM. Grinblatt and M. Keloharju, “What Makes Investors Trade?” The\nJournal Of Finance, vol. 56 (2), no. 2, pp. 549–578, 2001.\n[70]\nC. Heath and M. Lang, “Psychological Factors and Stock Option\nExercise.” Quarterly Journal of Economics vol. 114, no. 2, pp. 601–627,\n1999.\n[71]\nT. Odean, “Do Investors Trade Too Much?” American Economic\nReview, vol. 89 (5), pp. 1279-1298, 1998.\n[72]\nZ. Shapira and I. Venezia, “Patterns of behavior of professionally\nmanaged and independent investors”, Journal of Banking and Finance,\nvol. 25, no. 8, pp. 1573–1587, 2001.\n[73]\nKahneman, D., and Tversky, A. “Prospect Theory: An Analysis of\nDecision under Risk.”, Econometrica: Journal of the Econometric\nSociety, vol. 47(3), 263291, 1979.\n[74]\nW. Sharpe, “Capital Asset Prices: A Theory of Market Equilibrium\nUnder Conditions of Risk,” The Journal of Finance, vol. XIX, no. 3,\npp. 425–442, 1964.\n[75]\nS. Beninga, Financial Modeling, 2000.\n[76]\nN. D. Daw, “Trial-by-trial data analysis using computational models,”\nin Decision Making, Affect, and Learning: Attention and Performance\nXXIII, 2011\n[77]\nJ. P. Huelsenbeck and K. a. Crandall, “Phylogeny Estimation and\nHypothesis Testing Using Maximum Likelihood,” Annual Review of\nEcology and Systematics, vol. 28, no. 1, pp. 437–466, 1997.\n[78]\nR. M. Costa, “Plastic Corticostriatal Circuits for Action Learning Whats\nDopamine Got to Do with It?,” Annals of the New York Academy of\nSciences, pp. 172-191, 2007\nAPPENDIX\nThis manuscript is a correction to the article “Modelling\nStock-market Investors as Reinforcement Learning Agents” by\nthe same authors, issued in the proceedings of the 2015 IEEE\nConference on Evolving and Adaptive Intelligent Systems. The\ncorrections include ﬁxing a bug in the script which estimated\nthe probabilities used in the calculation of model ﬁtness. In\nthe previous work we applied some constraints and used a\ndifferent measure: the number of transactions considered for\neach player was capped at 25 and the measure of risk used to\nrank the stocks and classify them into discrete categories was\ndeﬁned by the authors as:\nR(sj) =\n\f\f\f\fβF (sj) ·\nσ(sj)\nmax σ(s)\n\f\f\f\f\n(11)\nwhere βF is the ﬁnancial modelling measure of volatility\nof a security used in the present work, σ(sj) is the standard\ndeviation of the j-th stock and the max σ(s) is the largest\nstandard deviation in the stock pool. This measure of riskiness\nwas used as it was believed to take into account the graphical\ninterpretation of the ﬂuctuations in time series (σ) and the\noverall trend of the security compared to the market (βF ).\nFig. 3.\nPopulation model ﬁtting statistic using Binomial Proportion Con-\nﬁdence Interval calculated with Clopper-Pearson method. The results are\nnegative as only 7 out of 46 players are better than random. The errorbar\nrepresent the 99% conﬁdence interval for the myopic model to be correctly\nﬁtting the players in the dataset. As the errorbar lies entirely below chance\nthreshold we conclude that the models investigated do not correctly represent\nthe behaviour of the players in the data.\nFig. 4.\nUpdated model comparison of Fig 1 (a) and (b) in the original paper.\nThis ﬁgure shows the comparison of the full RL model (grey bars) against the\nrandom model (+ signs) according to their MLE. This is the same comparison\nas in 1 (c) but for the original paper conﬁguration: players transactions capped\nat 25 and the combination of βF and σ, as in Eq. 11, used to determine stocks\nrisk degree and their classiﬁcation. In this case the portion of players captured\nby the full RL model is only 8% (4 players, 14, 18, 24, 26). Only 2 players\nare captured by the myopic model (14 and 18).\nFig. 5.\nUpdated model comparison of Fig 1 (c) and (d) in the original paper.\nThis ﬁgure shows the comparison of the full RL model (grey bars) against\nthe myopic model (ﬁlled diamonds signs) according to their MLE. This is the\nsame comparison as in 1 (b) but for the original paper conﬁguration: players\ntransactions capped at 25 and the combination of βF and σ, as in Eq. 11,\nused to determine stocks risk degree and their classiﬁcation. The 4 players\nwho are signiﬁcantly better ﬁtted by a full model are 14, 21, 24, 26.\n",
  "categories": [
    "cs.CE",
    "cs.LG"
  ],
  "published": "2016-09-20",
  "updated": "2016-09-20"
}