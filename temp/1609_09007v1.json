{
  "id": "http://arxiv.org/abs/1609.09007v1",
  "title": "Unsupervised Neural Hidden Markov Models",
  "authors": [
    "Ke Tran",
    "Yonatan Bisk",
    "Ashish Vaswani",
    "Daniel Marcu",
    "Kevin Knight"
  ],
  "abstract": "In this work, we present the first results for neuralizing an Unsupervised\nHidden Markov Model. We evaluate our approach on tag in- duction. Our approach\noutperforms existing generative models and is competitive with the\nstate-of-the-art though with a simpler model easily extended to include\nadditional context.",
  "text": "Unsupervised Neural Hidden Markov Models\nKe Tran2∗Yonatan Bisk1 Ashish Vaswani3∗Daniel Marcu1 Kevin Knight1\n1Information Sciences Institute, University of Southern California\n2Informatics Institute, University of Amsterdam\n3Google Brain, Mountain View\nm.k.tran@uva.nl, ybisk@isi.edu,\navaswani@google.com, marcu@isi.edu, knight@isi.edu\nAbstract\nIn this work, we present the ﬁrst results for\nneuralizing an Unsupervised Hidden Markov\nModel. We evaluate our approach on tag in-\nduction. Our approach outperforms existing\ngenerative models and is competitive with the\nstate-of-the-art though with a simpler model\neasily extended to include additional context.\n1\nIntroduction\nProbabilistic graphical models are among the most\nimportant tools available to the NLP community. In\nparticular, the ability to train generative models us-\ning Expectation-Maximization (EM), Variational In-\nference (VI), and sampling methods like MCMC has\nenabled the development of unsupervised systems for\ntag and grammar induction, alignment, topic models\nand more. These latent variable models discover hid-\nden structure in text which aligns to known linguis-\ntic phenomena and whose clusters are easily identiﬁ-\nable.\nRecently, much of supervised NLP has found great\nsuccess by augmenting or replacing context, features,\nand word representations with embeddings derived\nfrom Deep Neural Networks. These models allow for\nlearning highly expressive non-convex functions by\nsimply backpropagating prediction errors. Inspired\nby Berg-Kirkpatrick et al. (2010), who bridged the\ngap between supervised and unsupervised training\nwith features, we bring neural networks to unsuper-\nvised learning by providing evidence that even in\n∗This research was carried out while all authors were at the\nInformation Sciences Institute.\nunsupervised settings, simple neural network mod-\nels trained to maximize the marginal likelihood can\noutperform more complicated models that use expen-\nsive inference.\nIn this work, we show how a single latent variable\nsequence model, Hidden Markov Models (HMMs),\ncan be implemented with neural networks by sim-\nply optimizing the incomplete data likelihood. The\nkey insight is to perform standard forward-backward\ninference to compute posteriors of latent variables\nand then backpropagate the posteriors through the\nnetworks to maximize the likelihood of the data.\nUsing features in unsupervised learning has been\na fruitful enterprise (Das and Petrov, 2011; Berg-\nKirkpatrick and Klein, 2010; Cohen et al., 2011) and\nattempts to combine HMMs and Neural Networks\ndate back to 1991 (Bengio et al., 1991). Addition-\nally, similarity metrics derived from word embed-\ndings have also been shown to improve unsupervised\nword alignment (Songyot and Chiang, 2014).\nInterest in the interface of graphical models and\nneural networks has grown recently as new infer-\nence procedures have been proposed (Kingma and\nWelling, 2014; Johnson et al., 2016). Common to\nthis work and ours is the use of neural networks to\nproduce potentials. The approach presented here is\neasily applied to other latent variable models where\ninference is tractable and are typically trained with\nEM. We believe there are three important strengths:\n1. Using a neural network to produce model prob-\nabilities allows for seamless integration of addi-\ntional context not easily represented by condi-\ntioning variables in a traditional model.\narXiv:1609.09007v1  [cs.CL]  28 Sep 2016\n2. Gradient based training trivially allows for mul-\ntiple objectives in the same loss function.\n3. Rich model representations do not saturate as\nquickly and can therefore utilize large quantities\nof unlabeled text.\nOur focus in this preliminary work is to present\na generative neural approach to HMMs and demon-\nstrate how this framework lends itself to modularity\n(e.g. the easy inclusion of morphological informa-\ntion via Convolutional Neural Networks §5), and the\naddition of extra conditioning context (e.g. using an\nRNN to model the sentence §6). Our approach will\nbe demonstrated and evaluated on the simple task of\npart-of-speech tag induction. Future work, should\ninvestigate the second and third proposed strengths.\n2\nFramework\nGraphical models have been widely used in NLP.\nTypically potential functions ψ(z, x) over a set of\nlatent variables, z, and observed variables, x, are de-\nﬁned based on hand-crafted features. Moreover, in-\ndependence assumptions between variables are often\nmade for the sake of tractability. Here, we propose\nusing neural networks (NNs) to produce the poten-\ntials since neural networks are universal function ap-\nproximators. Neural networks can extract useful task-\nspeciﬁc abstract representations of data. Addition-\nally, Long Short-Term Memory (LSTM) (Hochre-\niter and Schmidhuber, 1997) based Recurrent Neural\nNetworks (RNNs), allow for modeling unbounded\ncontext with far fewer parameters than naive one-hot\nfeature encodings. The reparameterization of poten-\ntials with neural networks (NNs) is seamless:\nψ(z, x) = fNN(z, x | θ)\n(1)\nThe sequence of observed variables are denoted\nas x = {x1, . . . , xn}. In unsupervised learning, we\naim to ﬁnd model parameters θ that maximize the\nevidence p(x | θ). We focus on cases when the pos-\nterior is tractable and we can use Generalized EM\n(Dempster et al., 1977) to estimate θ.\np(x) =\nX\nz\np(x, z)\n(2)\n= Eq(z)[ln p(x, z | θ)] + H[q(z)]\n(3)\n+ KL (q(z) ∥p(z | x, θ))\n(4)\nText\nPierre\nVinken\nwill\njoin\nthe\nboard\nPTB\nNNP\nNNP\nMD\nVB\nDT\nNN\nTable 1: Example Part-of-Speech tagged text.\nwhere q(z) is an arbitrary distribution, and H is the\nentropy function. The E-step of EM estimates the\nposterior p(z | x) based on the current parameters θ.\nIn the M-step, we choose q(z) to be the posterior\np(z | x), setting the KL-divergence to zero. Addition-\nally, the entropy term H[q(z)] is a constant and can\ntherefore be dropped. This means updating θ only\nrequires maximizing Ep(z | x)[ln p(x, z | θ)]. The gra-\ndient is therefore deﬁned in terms of the gradient of\nthe joint probability scaled by the posteriors:\nJ(θ) =\nX\nz\np(z | x)∂ln p(x, z | θ)\n∂θ\n(5)\nIn order to perform the gradient update in Eq 5,\nwe need to compute the posterior p(z | x).\nThis\ncan be done efﬁciently with the Message Passing al-\ngorithm. Note that, in cases where the derivative\n∂\n∂θ ln p(x, z | θ) is easy to evaluate, we can perform\ndirect marginal likelihood optimization (Salakhutdi-\nnov et al., 2003). We do not address here the question\nof semi-supervised training, but believe the frame-\nwork we present lends itself naturally to the incorpo-\nration of constraints or labeled data. Next, we demon-\nstrate the application of this framework to HMMs in\nthe service of part-of-speech tag induction.\n3\nPart-of-Speech Induction\nPart-of-speech tags encode morphosyntactic informa-\ntion about a language and are a fundamental tool in\ndownstream NLP applications. In English, the Penn\nTreebank (Marcus et al., 1994) distinguishes 36 cate-\ngories and punctuation. Tag induction is the task of\ntaking raw text and both discovering these latent clus-\nters and assigning them to words in situ. Classes can\nbe very speciﬁc (e.g. six types of verbs in English)\nto their syntactic role. Example tags are shown in Ta-\nble 1. In this example, board is labeled as a singular\nnoun while Pierre Vinken is a singular proper noun.\nTwo natural applications of induced tags are as\nthe basis for grammar induction (Spitkovsky et al.,\n2011; Bisk et al., 2015) or to provide a syntactically\ninformed, though unsupervised, source of word em-\nbeddings.\nz1\nzt−1\nzt+1\nzT\nzt\nxt\nxt+1\nxt−1\nx1\nxT\nFigure 1: Pictorial representation of a Hidden Markov Model.\nLatent variable (zt) transitions depend on the previous value\n(zt−1), and emit an observed word (xt) at each time step.\n3.1\nThe Hidden Markov Model\nA common model for this task, and our primary\nworkhorse, is the Hidden Markov Model trained with\nthe unsupervised message passing algorithm, Baum-\nWelch (Welch, 2003).\nModel\nHMMs model a sentence by assuming that\n(a) every word token is generated by a latent class,\nand (b) the current class at time t is conditioned on\nthe local history t−1. Formally, this gives us an emis-\nsion p(xt | zt) and transition p(zt | zt−1) probability.\nThe graphical model is drawn pictorially in Figure 1,\nwhere shaded circles denote observations and empty\nones are latent. The probability of a given sequence\nof observations x and latent variables z is given by\nmultiplying transitions and emissions across all time\nsteps (Eq. 6). Finding the optimal sequence of latent\nclasses corresponds to computing an argmax over the\nvalues of z.\np(x, z) =\nn+1\nY\nt=1\np(zt | zt−1)\nn\nY\nt=1\np(xt | zt)\n(6)\nBecause our task is unsupervised we do not have\na priori access to these distributions, but they can be\nestimated via Baum-Welch. The algorithm’s outline\nis provided in Algorithm 1.\nTraining an HMM with EM is highly non-convex\nand likely to get stuck in local optima (Johnson,\n2007). Despite this, sophisticated Bayesian smooth-\ning leads to state-of-the-art performance (Blunsom\nand Cohn, 2011). Blunsom and Cohn (2011) fur-\nther extend the HMM by augmenting its emission\ndistributions with character models to capture mor-\nphological information and a tri-gram transition ma-\ntrix which conditions on the previous two states. Re-\ncently, Lin et al. (2015) extended several models\nAlgorithm 1 Baum-Welch Algorithm\nRandomly Initialize distributions (θ)\nrepeat\nCompute forward messages:\n∀i,t αi(t)\nCompute backward messages:\n∀i,t βi(t)\nCompute posteriors:\np(zt = i | x, θ) ∝αi(t)βi(t)\np(zt = i, zt+1 = j | x, θ)\n∝αi(t)p(zt+1 =j|zt =i)\n×βj(t + 1)p(xt+1|zt+1 =j)\nUpdate θ\nuntil Converged\nincluding the HMM to include pre-trained word em-\nbeddings learned by different skip-gram models. Our\nwork will fully neuralize the HMM and learn embed-\ndings during the training of our generative model.\nThere has also been recent work on by Rastogi et al.\n(2016) on neuralizing Finite-State Transducers.\n3.2\nAdditional Comparisons\nWhile the main focus of our paper is the seamless\nextension of an unsupervised generative latent vari-\nable model with neural networks, for completeness\nwe will also include comparisons to other techniques\nwhich do not adhere to the generative assumption.\nWe include Brown clusters (Brown et al., 1992) as\na baseline and two clustering techniques as state-\nof-the-art comparisons: Christodoulopoulos et al.\n(2011) and Yatbaz et al. (2012).\nOf particular interest to us is the work of Brown\net al.\n(1992).\nBrown clusters group word types\nthrough a greedy agglomerative clustering according\nto their mutual information across the corpus based\non bigram probabilities. Brown clusters do not ac-\ncount for a word’s membership in multiple syntactic\nclasses, but are a very strong baseline for tag induc-\ntion. It is possible our approach could be improved\nby augmenting our objective function to include mu-\ntual information computations or a bias towards a\nharder clustering.\n4\nNeural HMM\nThe aforementioned training of an HMM assumes ac-\ncess to two distributions: (1) Emissions with K × V\nparameters, and (2) Transitions with K × K parame-\nters. Here we assume there are K clusters and V\nword types in our vocabulary.\nOur neural HMM\n(NHMM) will replace these matrices with the out-\nput of simple feed-forward neural networks. All con-\nditioning variables will be presented as input to the\nnetwork and its ﬁnal softmax layer will provide prob-\nabilities. This should replicate the behavior of the\nstandard HMM, but without an explicit representa-\ntion of the necessary distributions.\n4.1\nProducing Probabilities\nProducing emission and transition probabilities al-\nlows for standard inference to take place in the\nmodel.\nEmission Architecture\nLet vk ∈RD be vector\nembedding of tag zk, wi ∈RD and bi vector embed-\nding and bias of word i respectively. The emission\nprobability p(wi | zk) is given by\np(wi | zk) =\nexp(v⊤\nk wi + bi)\nPV\nj=1 exp(v⊤\nk wj + bj)\n(7)\nThe emission probability can be implemented by a\nneural network where wi is the weight of unit i at\nthe output layer of the network. The tag embeddings\nvk are obtained by a simple feed-forward neural net-\nwork consisting of a lookup table following by a non-\nlinear activation (ReLU). When using morphology\ninformation (§5), we will ﬁrst use another network\nto produce the word embedddings wi.\nTransition Architecture\nWe produce the transi-\ntion probability directly by using a linear layer of\nD × K2. More speciﬁcally, let q ∈RD be a query\nembedding. The unnormalized transition matrix T is\ncomputed as\nT = U⊤q + b\n(8)\nwhere U ∈RD×K2 and b ∈RK2. We then reshape\nT to a K × K matrix and apply a softmax layer per\nrow to produce valid transition probabilities.\n4.2\nTraining the Neural Network\nThe probabilities can now be used to perform the\naforementioned forward and backward passes over\nthe data to compute posteriors. In this way, we per-\nform the E-step as though we were training a vanilla\nHMM.\nTraditionally, these values would simply\nbe re-normalized during the M-step to re-estimate\nmodel parameters. Instead, we use them to re-scale\nour gradients (following the discussion from §2).\nCombining the HMM factorization of the joint proba-\nbility p(x, z) from Eq. 6 with the gradient from Eq. 5,\nyields the following update rule:\nJ(θ) =\nX\nz\np(z | x)∂ln p(x, z | θ)\n∂θ\n=\nX\nt\nX\nzt\np(zt | x)∂ln p(xt | zt, θ)\n∂θ\n+ p(zt, zt−1 | x)∂ln p(zt | zt−1, θ)\n∂θ\n(9)\nThe posteriors p(zt | x) and p(zt, zt−1 | x) are ob-\ntained by running Baum-Welch as shown in Algo-\nrithm 1. Where traditional supervised training can\nfollow a clear gradient signal towards a speciﬁc\nassignment, here we are propagating the model’s\n(un)certainty instead. An additional complication in-\ntroduced by this paradigm is the question of how\nmany gradient steps to take on a given minibatch. In\nincremental EM the posteriors are simply accumu-\nlated and normalized. Here, we repeatedly recom-\npute gradients on a minibatch until reaching the max-\nimum number of epochs or a convergence threshold\nis met.\nFinally, notice that the factorization of the HMM\nallows us to evaluate the joint distribution p(x, z | θ)\neasily. We therefore employ Direct Marginal Likeli-\nhood (DML) (Salakhutdinov et al., 2003) to optimize\nthe model’s parameters. After trying both EM and\nDML we found EM to be slower to converge and per-\nform slightly weaker. For this reason, the presented\nresults will all be trained with DML.\n4.3\nHMM and Neural HMM Equivalence\nAn important result we see in Table 2 is that the Neu-\nral HMM (NHMM) performs almost identically to\nthe HMM. At this point, we have replaced the un-\nderlying machinery, but the model still has the same\ninformation bottlenecks as a standard HMM, which\nlimit the amount and type of information carried be-\ntween words in the sentence. Additionally, both ap-\nproaches are optimizing the same objective function,\ndata likelihood, via the computation of posteriors.\nThe equivalency is an important sanity check. The\nTag embeddings\nReLU\nLinear\nSoftmax\nChar-CNN\nFigure 2: Computational graph of Char-CNN emission network.\nA character convolutional neural network is used to compute the\nweight of the linear layer for every minibatch.\nfollowing two sections will demonstrate the extensi-\nbility of this approach.\n5\nConvolutions for Morphology\nThe ﬁrst beneﬁt of moving to neural networks is the\nease with which new information can be provided\nto the model. The ﬁrst experiment we will perform\nis replacing words with embedding vectors derived\nfrom a Convolutional Neural Network (CNN) (Kim\net al., 2016; Jozefowicz et al., 2016). We use a convo-\nlutional kernel with widths from 1 to 7, which covers\nup to 7 character n-grams (Figure 2). This allows the\nmodel to automatically learn lexical representations\nbased on preﬁx, sufﬁx, and stem information about a\nword. No additional changes to learning are required\nfor extension.\nAdding the convolution does not dramatically\nslow down our model because the emission distribu-\ntions can be computed for the whole batch in one\noperation. We simply pass the whole vocabulary\nthrough the convolution in a single operation.\n6\nInﬁnite Context with LSTMs\nOne of the most powerful strengths of neural net-\nworks is their ability to create compact representa-\ntion of data. We will explore this here in the creation\nof transition matrices. In particular, we chose to aug-\nment the transition matrix with all preceding words\nin the sentence: p(zt | zt−1, w0, . . . , wt−1). Incorpo-\nrating this amount of context in a traditional HMM is\nintractable and impossible to estimate, as the number\nof parameters grows exponentially.\nFor this reason, we use a stacked LSTM to\nform a low dimensional representation of the\nsentence (C0...t−1) which can be easily fed to\nour network when producing a transition matrix:\nxt\nxt−1\nx1\nxT\nTt−1,t\nFigure 3: A graphical representation of our LSTM transition\nnetwork. Transition matrix Tt−1,t from time step t −1 to t is\ncomputed based on the hidden state of the LSTM at time t −1.\np(zt | zt−1, C0...t−1) in Figure 3.\nBy having the\nLSTM only consume up to the previous word, we do\nnot break any sequential generative model assump-\ntions.1 In terms of model architecture, the query em-\nbedding q will be replaced by a hidden state ht−1 of\nthe LSTM at time step t −1.\n7\nEvaluation\nOnce a model is trained, the one best latent sequence\nis extracted for every sentence and evaluated on three\nmetrics.\nMany-to-One (M-1)\nMany-to-one computes the\nmost common true part-of-speech tag for each clus-\nter. It then computes tagging accuracy as if the clus-\nter were replaced with that tag. This metric is easily\ngamed by introducing a large number of clusters.\nOne-to-One (1-1)\nOne-to-One performs the same\ncomputation as Many-to-One but only one cluster is\nallowed to be assigned to a given tag. This prevents\nthe gaming of M-1.\nV-Measure (VM)\nV-Measure is an F-measure\nwhich trades off conditional entropy between the\nclusters and gold tags.\nChristodoulopoulos et al.\n(2010) found VM is to be the most informative and\nconsistent metric, in part because it is agnostic to the\nnumber of induced tags.\n8\nData and Parameters\nTo evaluate our approaches, we follow the existing\nliterature and train and test on the full WSJ corpus.\n1This interpretation does not complicate the computation\nof forward-backward messages when running Baum-Welch,\nthough it does, by design, break Markovian assumption about\nknowledge of the past.\nThere are three components of our models which can\nbe tuned. Something we have to be careful of when\ntrain and test are the same data. To avoid cheating,\nno values were tuned in this work.\nArchitecture\nThe ﬁrst parameter is the number of\nhidden units. We chose 512 because it was the largest\npower of two we could ﬁt in memory. When we ex-\ntended our model to include the convolutional emis-\nsion network, we only used 128 units, due to the\nintensive computation of Char-CNN over the whole\nvocabulary per minibatch.\nThe second design choice was the number of\nLSTM layers. We used a three layer LSTM as it\nworked well for (Tran et al., 2016), and we applied\ndropout (Srivastava et al., 2014) over the vertical con-\nnections of the LSTMs (Pham et al., 2014) with a rate\nof 0.5.\nFinally, the maximum number of inner loop up-\ndates applied per batch is set to six. We train all the\nmodels for ﬁve epochs and perform gradient clipping\nwhenever the gradient norm is greater than ﬁve. To\ndetermine when to stop applying the gradient during\ntraining we simply check when the log probability\nhas converged ( new−old\nold\n< 10−4) or if the maximum\nnumber of inner loops has been reached. All opti-\nmization was done using Adam (Kingma and Ba,\n2015) with default hyper-parameters.\nInitialization\nIn addition to architectural choices\nwe have to initialize all of our parameters. Word em-\nbeddings (and character embeddings in the CNN) are\ndrawn from a Gaussian N(0, 1). The weights of all\nlinear layers in the model are drawn from a uniform\ndistribution with mean zero and a standard deviation\nof\np\n1/nin, where nin is the input dimension of the\nlinear layer.2 Additionally, weights for the LSTMs\nare initialized using N(0, 1/2n), where n is the num-\nber of hidden units, and the bias of the forget gate\nis set to 1, as suggested by Józefowicz et al. (2015).\nWe present some parameter and modeling ablation\nanalysis in §10.\nIt is worth emphasizing that parameters are shared\nat the lower level of our network architectures (see\nFigure 2 and Figure 3).\nSharing parameters not\nonly allows the networks to share statistical strength,\nbut also reduces the computational cost of comput-\n2This is the default parameter initialization in Torch.\nSystem\nM-1 1-1 VM\nBase\nHMM\n62.5 41.4 53.3\nBrown\n68.2 49.9 63.0\nSOTA\nClark (2003)\n71.2\n65.6\nChristodoulopoulos (2011)\n72.8\n66.1\nBlunsom (2011)\n77.5\n69.8\nYatbaz (2012)\n80.2\n72.1\nOur Work\nNHMM\n59.8 45.7 54.2\n+ Conv\n74.1 48.3 66.1\n+ LSTM\n65.1 52.4 60.4\n+ Conv & LSTM\n79.1 60.7 71.7\nTable 2: English Penn Treebank results with 45 induced clusters.\nWe see signiﬁcant gains from both morphology (+Conv) and ex-\ntended context (+LSTM). The combination of these approaches\nresults in a very simple system which is competitive with the\nbest generative model in the literature.\ning sufﬁcient statistics during training due to the\nmarginalization over latent variables.\nIn all of our experiments, we use minibatch size of\n256 and sentences of 40 words or less due to mem-\nory constraints. Evaluation was performed on all\nsentence lengths. Additionally, we map all the digits\nto 0, but do not lower-case the data or perform any\nother preprocessing. All model code is available on-\nline for extension and replication at\nhttps://github.com/ketranm/neuralHMM.\n9\nResults\nOur results are presented in Table 2 along with two\nbaseline systems, and the four top performing and\nstate-of-the-art approaches. As noted earlier, we are\nhappy to see that our NHMM performs almost iden-\ntically with the standard HMM. Second, we ﬁnd that\nour approach, while simple and fast, is competitive\nwith Blunsom (2011). Their Hierarchical Pitman-Yor\nProcess for trigram HMMs with character modeling\nis a very sophisticated Bayesian approach and the\nmost appropriate comparison to our work.\nWe see that both extended context (+LSTM) and\nthe addition of morphological information (+Conv)\nprovide substantial boosts to performance. Interest-\ningly, the gains are not completely complementary,\nas we note that the six and twelve point gains of these\nadditions only combine to a total of sixteen points in\nConﬁguration\nM-1\n1-1\nVM\nUniform initialization\n65.5\n50.1\n61.7\n1 LSTM layer, no dropout\n69.3\n52.7\n63.6\n1 LSTM layer, dropout\n71.0\n55.7\n66.2\n3 LSTM layers, no dropout\n72.7\n52.2\n65.1\nBest Model\n79.1\n60.7\n71.7\nTable 3: Exploring different conﬁgurations of NHMM\nVM improvement. This might imply that at least\nsome of the syntactic context being captured by the\nLSTM is mirrored in the morphology of the language.\nThis hypothesis is something future work should in-\nvestigate with morphologically rich languages.\nFinally, the newer work of Yatbaz et al. (2012)\noutperforms our approach. It is possible our perfor-\nmance could be improved by following their lead and\nincluding knowledge of the future.\n10\nParameter Ablation\nOur model design decisions and weight initializa-\ntions were chosen based on best practices set forth\nin the supervised training literature. We are lucky\nthat these also behaved well in the unsupervised set-\nting. Within unsupervised structure prediction, to our\nbest knowledge, there has been no empirical study\non neural network architecture design and weight ini-\ntialization. We therefore provide an initial overview\non the topic for several of our decisions.\nWeight Initialization\nIf we run our best model\n(NHMM+Conv+LSTM) with all the weights initial-\nized from a uniform distribution U(−10−4, 10−4)3\nwe ﬁnd a dramatic drop in V-Measure performance\n(61.7 vs 71.7 in Table 3). This is consistent with the\ncommon wisdom that unlike supervised learning (Lu-\nong et al., 2015), weight initialization is important to\nachieve good performance on unsupervised tasks. It\nis possible that performance could be further enhance\nvia the popular technique of ensembling, would al-\nlow for combining models which converged to differ-\nent local optima.\nLSTM Layers And Dropout\nWe ﬁnd that dropout\nis important in training an unsupervised NHMM.\n3We choose small standard derivation here for numerical sta-\nbility when computing forward-backward messages.\nRemoving dropout causes performance to drop six\npoints. To avoid tuning the dropout rate, future work\nmight investigate the effect of variational dropout\n(Kingma et al., 2015) in unsupervised learning. We\nalso observed that the number of LSTM layers has\nan impact on V-Measure. Had we simply used a sin-\ngle layer we would have lost nearly ﬁve points. It\nis possible that more layers, perhaps coupled with\nmore data, would yield even greater gains.\n11\nFuture Work\nIn addition to parameter tuning and multilingual eval-\nuation, the biggest open questions for our approach\nare the effects of additional data and augmenting the\nloss function. Neural networks are notoriously data\nhungry, indicating that while we achieve competitive\nresults, it is possible our model will scale well when\nrun with large corpora. This would likely require the\nuse of techniques like NCE (Gutmann and Hyväri-\nnen, 2010) which have been shown to be highly ef-\nfective in related tasks like neural language mod-\neling (Mnih and Teh, 2012; Vaswani et al., 2013).\nSecondly, despite focusing on ways to augment an\nHMM, Brown clustering and systems inspired by it\nperform very well. They aim to maximize mutual\ninformation rather than likelihood. It is possible that\naugmenting or constraining our loss will yield addi-\ntional performance gains.\nOutside of simply maximizing performance on tag\ninduction, a more subtle, but powerful contribution\nof this work may be its demonstration of the easy\nand effective nature of using neural networks with\nBayesian models traditionally trained by EM. We\nhope this approach scales well to many other do-\nmains and tasks.\nAcknowledgments\nThis work was supported by Contracts W911NF-15-\n1-0543 and HR0011-15-C-0115 with the US Defense\nAdvanced Research Projects Agency (DARPA) and\nthe Army Research Ofﬁce (ARO). Additional thanks\nto Christos Christodoulopoulos.\nReferences\nYoshua Bengio, Renato De Mori, Flammia Giovanni, and\nRalf Kompe. 1991. Global optimization of a neural\nnetwork - hidden markov model hybrid. In Proceed-\nings of the International Joint Conference on Neural\nNetworks, Seattle, WA.\nTaylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-\nnetic grammar induction. In Proceedings of the 48th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1288–1297, Uppsala, Sweden, July.\nTaylor Berg-Kirkpatrick, Alexandre Bouchard-Côté, John\nDeNero, and Dan Klein. 2010. Painless unsupervised\nlearning with features. In Human Language Technolo-\ngies: The 2010 Annual Conference of the North Ameri-\ncan Chapter of the Association for Computational Lin-\nguistics, pages 582–590.\nYonatan Bisk, Christos Christodoulopoulos, and Julia\nHockenmaier.\n2015.\nLabeled grammar induction\nwith minimal supervision. In Proceedings of the 53rd\nAnnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 2: Short Pa-\npers), pages 870–876, Beijing, China, July.\nPhil Blunsom and Trevor Cohn. 2011. A Hierarchical\nPitman-Yor Process HMM for Unsupervised Part of\nSpeech Induction. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Linguis-\ntics: Human Language Technologies, pages 865–874,\nPortland, Oregon, USA, June.\nPeter F Brown, Peter V deSouza, Robert L Mercer, Vin-\ncent J Della Pietra, and Jenifer C Lai. 1992. Class-\nBased n-gram Models of Natural Language. Computa-\ntional Linguistics, 18.\nChristos Christodoulopoulos, Sharon Goldwater, and\nMark Steedman. 2010. Two Decades of Unsupervised\nPOS induction: How far have we come? In Proceed-\nings of the 2010 Conference on Empirical Methods in\nNatural Language Processing, Cambridge, MA, Octo-\nber.\nChristos Christodoulopoulos, Sharon Goldwater, and\nMark Steedman. 2011. A Bayesian Mixture Model\nfor Part-of-Speech Induction Using Multiple Features.\nIn Proceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing, Edinburgh,\nScotland, UK., July.\nShay B.\nCohen, Dipanjan Das, and Noah A.\nSmith.\n2011.\nUnsupervised structure prediction with non-\nparallel multilingual guidance.\nIn Proceedings of\nthe 2011 Conference on Empirical Methods in Natural\nLanguage Processing, pages 50–61, Edinburgh, Scot-\nland, UK., July.\nDipanjan Das and Slav Petrov.\n2011.\nUnsupervised\npart-of-speech tagging with bilingual graph-based pro-\njections. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 600–609, Portland,\nOregon, USA, June.\nA Dempster, N Laird, and D Rubin.\n1977.\nMaxi-\nmum likelihood from incomplete data via the EM algo-\nrithm. Journal of the Royal Statistical Society. Series\nB (Methodological), January.\nMichael Gutmann and Aapo Hyvärinen. 2010. Noise-\ncontrastive estimation: A new estimation principle for\nunnormalized statistical models. In International Con-\nference on Artiﬁcial Intelligence and Statistics.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735–\n1780, November.\nMatthew J Johnson, David Duvenaud, Alexander B\nWiltschko, Sandeep R Datta, and Ryan P Adams. 2016.\nComposing graphical models with neural networks for\nstructured representations and fast inference.\nArXiv\ne-prints, March.\nMark Johnson. 2007. Why doesn’t EM ﬁnd good HMM\nPOS-taggers. In Proceedings of the 2007 Joint Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and Computational Natural Language Learn-\ning (EMNLP-CoNLL), January.\nRafal Józefowicz, Wojciech Zaremba, and Ilya Sutskever.\n2015. An empirical exploration of recurrent network\narchitectures. In Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, pages 2342–2350.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the Limits\nof Language Modeling. ArXiv e-prints, February.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. AAAI.\nDiederik Kingma and Jimmy Ba.\n2015.\nAdam: A\nmethod for stochastic optimization. The International\nConference on Learning Representations (ICLR).\nDiederik P Kingma and Max Welling.\n2014.\nAuto-\nencoding variational bayes. The International Confer-\nence on Learning Representations (ICLR).\nDiederik P Kingma, Tim Salimans, and Max Welling.\n2015. Variational dropout and the local reparameteriza-\ntion trick. In Advances in Neural Information Process-\ning Systems 28, pages 2575–2583. Curran Associates,\nInc.\nChu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori\nLevin. 2015. Unsupervised pos induction with word\nembeddings. In Proceedings of the 2015 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 1311–1316, Denver, Colorado, May–\nJune.\nThang Luong, Hieu Pham, and Christopher D.\nMan-\nning. 2015. Effective approaches to attention-based\nneural machine translation.\nIn Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412–1421, Lisbon, Por-\ntugal, September.\nMitchell P Marcus, Grace Kim, Mary Ann Marcinkiewicz,\nRobert MacIntyre, Ann Bies, Mark Ferguson, Karen\nKatz, and Britta Schasberger. 1994. The Penn Tree-\nbank: Annotating Predicate Argument Structure.\nIn\nARPA Human Language Technology Workshop.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and sim-\nple algorithm for training neural probabilistic language\nmodels. In Proceedings of the 29th International Con-\nference on Machine Learning (ICML-12), pages 1751–\n1758, New York, NY, USA, July.\nVu Pham, Christopher Bluche, Théodore Kermorvant, and\nJérôme Louradour. 2014. Dropout improves recurrent\nneural networks for handwriting recognition.\nIn In-\nternational Conference on Frontiers in Handwriting\nRecognition (ICFHR), pages 285–290, Sept.\nPushpendre Rastogi, Ryan Cotterell, and Jason Eisner.\n2016. Weighting ﬁnite-state transductions with neural\ncontext. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies,\npages 623–633, San Diego, California, June.\nRuslan Salakhutdinov, Sam Roweis, and Zoubin Ghahra-\nmani. 2003. Optimization with em and expectation-\nconjugate-gradient.\nIn Proceedings, Intl. Conf. on\nMachine Learning (ICML, pages 672–679.\nTheerawat Songyot and David Chiang. 2014. Improving\nword alignment using word similarity. In Proceedings\nof the 2014 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 1840–1845,\nDoha, Qatar, October.\nValentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang,\nand Daniel Jurafsky. 2011. Unsupervised dependency\nparsing without gold part-of-speech tags. In Proceed-\nings of the 2011 Conference on Empirical Methods in\nNatural Language Processing, pages 1281–1290, Ed-\ninburgh, Scotland, UK., July.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. 2014. Dropout:\nA simple way to prevent neural networks from overﬁt-\nting. JMLR, (1):1929–1958, January.\nKe Tran, Arianna Bisazza, and Christof Monz. 2016. Re-\ncurrent memory networks for language modeling. In\nProceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 321–\n331, San Diego, California, June.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with large-scale neu-\nral language models improves translation. In Proceed-\nings of the 2013 Conference on Empirical Methods in\nNatural Language Processing, pages 1387–1392, Seat-\ntle, Washington, USA, October.\nLloyd R Welch. 2003. Hidden Markov Models and the\nBaum-Welch Algorithm.\nIEEE Information Theory\nSociety Newsletter, 53(4):1–24, December.\nMehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.\nLearning Syntactic Categories Using Paradigmatic\nRepresentations of Word Context. In Proceedings of\nthe 2012 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Nat-\nural Language Learning, pages 940–951, Jeju Island,\nKorea, July.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2016-09-28",
  "updated": "2016-09-28"
}