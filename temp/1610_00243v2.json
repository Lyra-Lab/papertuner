{
  "id": "http://arxiv.org/abs/1610.00243v2",
  "title": "Deep unsupervised learning through spatial contrasting",
  "authors": [
    "Elad Hoffer",
    "Itay Hubara",
    "Nir Ailon"
  ],
  "abstract": "Convolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and\ntraining methods. In this work we present a novel approach for unsupervised\ntraining of Convolutional networks that is based on contrasting between spatial\nregions within images. This criterion can be employed within conventional\nneural networks and trained using standard techniques such as SGD and\nback-propagation, thus complementing supervised methods.",
  "text": "Under review as a conference paper at ICLR 2017\nDEEP UNSUPERVISED LEARNING THROUGH SPATIAL\nCONTRASTING\nElad Hoffer\nTechnion - Israel Institute of Technology\nHaifa, Israel\nehoffer@tx.technion.ac.il\nItay Hubara\nTechnion - Israel Institute of Technology\nHaifa, Israel\nitayh@tx.technion.ac.il\nNir Ailon ∗\nTechnion - Israel Institute of Technology\nHaifa, Israel\nnailon@cs.technion.ac.il\nABSTRACT\nConvolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and train-\ning methods. In this work we present a novel approach for unsupervised training\nof Convolutional networks that is based on contrasting between spatial regions\nwithin images. This criterion can be employed within conventional neural net-\nworks and trained using standard techniques such as SGD and back-propagation,\nthus complementing supervised methods.\n1\nINTRODUCTION\nFor the past few years convolutional networks (ConvNets, CNNs) LeCun et al. (1998) have proven\nthemselves as a successful model for vision related tasks Krizhevsky et al. (2012) Mnih et al. (2015)\nPinheiro et al. (2015) Razavian et al. (2014). A convolutional network is composed of multiple\nconvolutional and pooling layers, followed by a fully-connected afﬁne transformations. As with\nother neural network models, each layer is typically followed by a non-linearity transformation such\nas a rectiﬁed-linear unit (ReLU).\nA convolutional layer is applied by cross correlating an image with a trainable weight ﬁlter. This\nstems from the assumption of stationarity in natural images, which means that features learned for\none local region in an image can be shared for other regions and images.\nDeep learning models, including convolutional networks, are usually trained in a supervised man-\nner, requiring large amounts of labeled data (ranging between thousands to millions of examples\nper-class for classiﬁcation tasks) in almost all modern applications. These models are optimized a\nvariant of stochastic-gradient-descent (SGD) over batches of images sampled from the whole train-\ning dataset and their ground truth-labels. Gradient estimation for each one of the optimized param-\neters is done by back propagating the objective error from the ﬁnal layer towards the input. This is\ncommonly known as ”backpropagation” Rumelhart et al..\nOne early well known usage of unsupervised training of deep architectures was as part of a pre-\ntraining procedure used for obtaining an effective initial state of the model. The network was later\nﬁne-tuned in a supervised manner as displayed by Hinton (2007). Such unsupervised pre-training\nprocedures were later abandoned, since they provided no apparent beneﬁt over other initialization\nheuristics in more careful fully supervised training regimes. This led to the de-facto almost exclusive\nusage of neural networks in supervised environments.\n∗The author acknowledges the generous support of ISF grant number 1271/13\n1\narXiv:1610.00243v2  [cs.LG]  4 Dec 2018\nUnder review as a conference paper at ICLR 2017\nIn this work we will present a novel unsupervised learning criterion for convolutional network based\non comparison of features extracted from regions within images. Our experiments indicate that by\nusing this criterion to pre-train networks we can improve their performance and achieve state-of-\nthe-art results.\n2\nPREVIOUS WORKS\nUsing unsupervised methods to improve performance have been the holy grail of deep learning for\nthe last couple of years and vast research efforts have been focused on that. We hereby give a short\noverview of the most popular and recent methods that tried to tackle this problem.\nAutoEncoders and reconstruction loss\nThese are probably the most popular models for unsu-\npervised learning using neural networks, and ConvNets in particular. Autoencoders are NNs which\naim to transform inputs into outputs with the least possible amount of distortion. An Autoencoder\nis constructed using an encoder G(x; w1) that maps an input to a hidden compressed representation,\nfollowed by a decoder F(y; w2), that maps the representation back into the input space. Mathemat-\nically, this can be written in the following general form:\nˆx = F(G(x; w1); w2)\nThe underlying encoder and decoder contain a set of trainable parameters that can be tied together\nand optimized for a predeﬁned criterion. The encoder and decoder can have different architectures,\nincluding fully-connected neural networks, ConvNets and others. The criterion used for training is\nthe reconstruction loss, usually the mean squared error (MSE) between the original input and its\nreconstruction Zeiler et al. (2010)\nmin∥x −ˆx∥2\nThis allows an efﬁcient training procedure using the aforementioned backpropagation and SGD tech-\nniques. Over the years autoencoders gained fundamental role in unsupervised learning and many\nmodiﬁcation to the classic architecture were made. Ng (2011) regularized the latent representation\nto be sparse, Vincent et al. (2008) substituted the input with a noisy version thereof, requiring the\nmodel to denoise while reconstructing. Kingma et al. (2014) obtained very promising results with\nvariational autoencoders (VAE). A variational autoencoder model inherits typical autoencoder ar-\nchitecture, but makes strong assumptions concerning the distribution of latent variables. They use\nvariational approach for latent representation learning, which results in an additional loss component\nand speciﬁc training algorithm called Stochastic Gradient Variational Bayes (SGVB). VAE assumes\nthat the data is generated by a directed graphical model p(x|z) and require the encoder to learn an\napproximation qw1(z|x) to the posterior distribution pw2(z|x) where w1 and w2 denote the param-\neters of the encoder and decoder. The objective of the variational autoencoder in this case has the\nfollowing form:\nL(w1, w2, x) = −DKL(qw1(z|x)||pw2(z)) + Eqw1(z|x)\n\u0000log pw2(x|z)\n\u0001\nRecently, a stacked set of denoising autoencoders architectures showed promising results in both\nsemi-supervised and unsupervised tasks. A stacked what-where autoencoder by Zhao et al. (2015)\ncomputes a set of complementary variables that enable reconstruction whenever a layer implements\na many-to-one mapping. Ladder networks by Rasmus et al. (2015) - use lateral connections to allow\nhigher levels of an autoencoder to focus on invariant abstract features by applying a layer-wise cost\nfunction.\nExemplar Networks:\nThe unsupervised method introduced byDosovitskiy et al. (2014) takes a\ndifferent approach to this task and trains the network to discriminate between a set of pseudo-classes.\nEach pseudo-class is formed by applying multiple transformations to a randomly sampled image\npatch. The number of pseudo-classes can be as big as the size of the input samples. This criterion\nensures that different input samples would be distinguished while providing robustness to the applied\ntransformations.\n2\nUnder review as a conference paper at ICLR 2017\nContext prediction\nAnother method for unsupervised learning by context was introduced by Do-\nersch et al. (2015). This method uses an auxiliary criterion of predicting the location of an image\npatch given another from the same image. This is done by classiﬁcation to 1 of 9 possible locations.\nAdversarial Generative Models:\nThis a recently introduced model that can be used in an unsu-\npervised fashion Goodfellow et al. (2014). Adversarial Generative Models uses a set of networks,\none trained to discriminate between data sampled from the true underlying distribution (e.g., a set\nof images), and a separate generative network trained to be an adversary trying to confuse the ﬁrst\nnetwork. By propagating the gradient through the paired networks, the model learns to generate\nsamples that are distributed similarly to the source data. As shown by Radford et al. (2015),this\nmodel can create useful latent representations for subsequent classiﬁcation tasks as demonstrated\nSampling Methods:\nMethods for training models to discriminate between a very large number of\nclasses often use a noise contrasting criterion. In these methods, roughly speaking, the posterior\nprobability P(t|yt) of the ground-truth target t given the model output on an input sampled from the\ntrue distribution yt = F(x) is maximized, while the probability P(t|yn) given a noise measurement\ny = F(n) is minimized. This was successfully used in a language domain to learn unsupervised\nrepresentation of words. The most noteworthy case is the word2vec model introduced by Mikolov\net al. (2013). When using this setting in language applications, a natural contrasting noise is a smooth\napproximation of the Unigram distribution. A suitable contrasting distribution is less obvious when\ndata points are sampled from a high dimensional continuous space, such as in the case of patches of\nimages.\n2.1\nPROBLEMS WITH CURRENT APPROACHES\nOnly recently the potential of ConvNets in an unsupervised environment began to bear fruit, still we\nbelieve it is not fully uncovered.\nThe majority of unsupervised optimization criteria currently used are based on variations of recon-\nstruction losses. One limitation of this fact is that a pixel level reconstruction is non-compliant with\nthe idea of a discriminative objective, which is expected to be agnostic to low level information in the\ninput. In addition, it is evident that MSE is not best suited as a measurement to compare images, for\nexample, viewing the possibly large square-error between an image and a single pixel shifted copy\nof it. Another problem with recent approaches such as Rasmus et al. (2015); Zeiler et al. (2010)\nis their need to extensively modify the original convolutional network model. This leads to a gap\nbetween unsupervised method and the state-of-the-art, supervised, models for classiﬁcation - which\ncan hurt future attempt to reconcile them in a uniﬁed framework, and also to efﬁciently leverage\nunlabeled data with otherwise supervised regimes.\n3\nLEARNING BY COMPARISONS\nThe most common way to train NN is by deﬁning a loss function between the target values and\nthe network output. Learning by comparison approaches the supervised task from a different angle.\nThe main idea is to use distance comparisons between samples to learn useful representations. For\nexample, we consider relative and qualitative examples of the form X1 is closer to X2 than X1 is to\nX3. Using a comparative measure with neural network to learn embedding space was introduced in\nthe “Siamese network” framework by Bromley et al. (1993) and later used in the works of Chopra\net al. (2005). One use for this methods is when the number of classes is too large or expected to vary\nover time, as in the case of face veriﬁcation, where a face contained in an image has to compared\nagainst another image of a face. This problem was recently tackled by Schroff et al. (2015) for\ntraining a convolutional network model on triplets of examples. There, one image served as an\nanchor x, and an additional pair of images served as a positive example x+ (containing an instance\nof the face of the same person) together with a negative example x−, containing a face of a different\nperson. The training objective was on the embedded distance of the input faces, where the distance\nbetween the anchor and positive example is adjusted to be smaller by at least some constant α from\nthe negative distance. More precisely, the loss function used in this case was deﬁned as\nL(x, x+, x−) = max {∥F(x) −F(x+)∥2 −∥F(x) −F(x−)∥2 + α, 0}\n(1)\n3\nUnder review as a conference paper at ICLR 2017\nwhere F(x) is the embedding (the output of a convolutional neural network), and α is a predeﬁned\nmargin constant. Another similar model used by Hoffer & Ailon (2015) with triplets comparisons for\nclassiﬁcation, where examples from the same class were trained to have a lower embedded distance\nthan that of two images from distinct classes. This work introduced a concept of a distance ratio\nloss, where the deﬁned measure amounted to:\nL(x, x+, x−) = −log\ne−∥F (x)−F (x+)∥2\ne−∥F (x)−F (x+)∥2 + e−∥F (x)−F (x−)∥2\n(2)\nThis loss has a ﬂavor of a probability of a biased coin ﬂip. By ‘pushing’ this probability to zero,\nwe express the objective that pairs of samples coming from distinct classes should be less similar to\neach other, compared to pairs of samples coming from the same class. It was shown empirical by\nBalntas et al. (2016) to provide better feature embeddings than the margin based distance loss 1\n4\nOUR CONTRIBUTION: SPATIAL CONTRASTING\nOne implicit assumption in convolutional networks, is that features are gradually learned hierar-\nchically, each level in the hierarchy corresponding to a layer in the network. Each spatial location\nwithin a layer corresponds to a region in the original image. It is empirically observed that deeper\nlayers tend to contain more ‘abstract’ information from the image. Intuitively, features describing\ndifferent regions within the same image are likely to be semantically similar (e.g. different parts\nof an animal), and indeed the corresponding deep representations tend to be similar. Conversely,\nregions from two probably unrelated images (say, two images chosen at random) tend to be far from\neach other in the deep representation. This logic is commonly used in modern deep networks such\nas Szegedy et al. (2015) Lin et al. (2013) He et al. (2015), where a global average pooling is used to\naggregate spatial features in the ﬁnal layer used for classiﬁcation.\nOur suggestion is that this property, often observed as a side effect of supervised applications, can\nbe used as a desired objective when learning deep representations in an unsupervised task. Later, the\nresulting representation can be used, as typically done, as a starting point or a supervised learning\ntask. We call this idea which we formalize below Spatial contrasting. The spatial contrasting crite-\nrion is similar to noise contrasting estimation Gutmann & Hyv¨arinen (2010) Mnih & Kavukcuoglu\n(2013), in trying to train a model by maximizing the expected probability on desired inputs, while\nminimizing it on contrasting sampled measurements.\n4.1\nFORMULATION\nWe will concern ourselves with samples of images patches ˜x(m) taken from an image x. Our con-\nvolutional network model, denoted by F(x), extracts spatial features f so that f (m) = F(˜x(m))\nfor an image patch ˜x(m). We wish to optimize our model such that for two features representing\npatches taken from the same image ˜x(1)\ni , ˜x(2)\ni\n∈xi for which f (1)\ni\n= F(˜x(1)\ni ) and f (2)\ni\n= F(˜x(2)\ni ),\nthe conditional probability P(f (1)\ni\n|f (2)\ni\n) will be maximized.\nThis means that features from a patch taken from a speciﬁc image can effectively predict, under our\nmodel, features extracted from other patches in the same image. Conversely, we want our model\nto minimize P(fi|fj) for i, j being two patches taken from distinct images. Following the logic\npresented before, we will need to sample contrasting patch ˜x(1)\nj\nfrom a different image xj such that\nP(f (1)\ni\n|f (2)\ni\n) > P(f (1)\nj\n|f (2)\ni\n), where f (1)\nj\n= F(˜x(1)\nj ). In order to obtain contrasting samples, we use\nregions from two random images in the training set. We will use a distance ratio, described earlier\n2 for the supervised case, to represent the probability two feature vectors were taken from the same\nimage. The resulting training loss for a pair of images will be deﬁned as\nLSC(x1, x2) = −log\ne−∥f (1)\n1\n−f (2)\n1\n∥2\ne−∥f (1)\n1\n−f (2)\n1\n∥2 + e−∥f (1)\n1\n−f (1)\n2\n∥2\n(3)\nEffectively minimizing a log-probability under the SoftMax measure. This formulation is portrayed\nin ﬁgure 4.1. Since we sample our contrasting sample from the same underlying distribution, we\n4\nUnder review as a conference paper at ICLR 2017\ncan evaluate this loss considering the image patch as both patch compared (anchor) and contrast\nsymmetrically. The ﬁnal loss will be the average between these estimations:\nˆ\nLSC(x1, x2) = 1\n2 [LSC(x1, x2) + LSC(x2, x1)]\nFigure 1: Spatial contrasting depiction.\n4.2\nMETHOD\nSince training convolutional network is done in batches of images, we can use the multiple samples\nin each batch to train our model. Each image serves as a source for both an anchor and positive\npatches, for which the corresponding features should be closer, and also a source for contrasting\nsamples for all the other images in that batch. For a batch of N images, two samples from each\nimage are taken, and N 2 different distance comparisons are made. The ﬁnal loss is the average\ndistance ratio for images in the batch:\n¯\nLSC({x}N\ni=1) = 1\nN\nN\nX\ni=1\nLSC(xi, {x}j̸=i) = −1\nN\nN\nX\ni=1\nlog\ne−∥f (1)\ni\n−f (2)\ni\n∥2\nPN\nj=1 e−∥f (1)\ni\n−f (2)\nj\n∥2\n(4)\nSince the criterion is differentiable with respect to its inputs, it is fully compliant with standard\nmethods for training convolutional network and speciﬁcally using backpropagation and gradient\ndescent. Furthermore, SC can be applied to any layer in the network hierarchy. In fact, SC can be\nused at multiple layers within the same convolutional network. The spatial properties of the features\nmeans that we can also sample from feature space ˜f (m) ∈f instead of from the original image,\nwhich we use to simplify implementation. The complete algorithm for batch training is described in\n1. This algorithm is also related to the batch normalization layer Ioffe & Szegedy (2015), a recent\nusage for batch statistics in neural networks. Spatial contrasting also uses the batch statistics, but to\nsample contrasting patches.\n5\nEXPERIMENTS\nIn this section we report empirical results showing that using SC loss as an unsupervised pretraining\nprocedure can improve state-of-the-art performance on subsequent classiﬁcation. We experimented\n5\nUnder review as a conference paper at ICLR 2017\nAlgorithm 1 Calculation the spatial contrasting loss\nRequire: X = {x}N\ni=1 # Training on batches of images\n# Get the spatial features for the whole batch of images\n# Size: N × Wf × Hf × C\n{f}N\ni=1 ←ConvNet(X)\n# Sample spatial features and calculate embedded distance between all pairs of images\nfor i = 1 to N do\n˜f (1)\ni\n←sample(fi)\nfor j = 1 to N do\n˜f (2)\nj\n←sample(fj)\nDist(i, j) ←∥˜f (1)\ni\n−˜f (2)\nj\n∥2\nend for\nend for\n# Calculate log SoftMax normalized distances\ndi ←−log\ne−Dist(i,i)\nPN\nk=1 e−Dist(i,k)\n# Spatial contrasting loss is the mean of distance ratios\nreturn\n1\nN\nPN\ni=1 di\nwith MNIST, CIFAR-10 and STL10 datasets. We used modiﬁed versions of well studied networks\nsuch as those of Lin et al. (2013) Rasmus et al. (2015). A detailed description of our architecture\ncan be found in Appendix A.\nIn each one of the experiments, we used the spatial contrasting criterion to train the network on the\nunlabeled images. Training was done by using SGD with an initial learning rate of 0.1 that was\ndecreased by a factor of 10 whenever the measured loss stopped decreasing. After convergence, we\nused the trained model as an initialization for a supervised training on the complete labeled dataset.\nThe supervised training was done following the same regime, only starting with a lower initial\nlearning rate of 0.01. We used mild data augmentations, such as small translations and horizontal\nmirroring.\nThe datasets we used are:\n• STL10 (Coates et al. (2011)). This dataset consists of 100, 000 96 × 96 colored, unlabeled\nimages, together with another set of 5, 000 labeled training images and 8, 000 test images .\nThe label space consists of 10 object classes.\n• Cifar10 (Krizhevsky & Hinton (2009)). The well known CIFAR-10 is an image classiﬁ-\ncation benchmark dataset containing 50, 000 training images and 10, 000 test images. The\nimage sizes 32 × 32 pixels, with color. The classes are airplanes, automobiles, birds, cats,\ndeer, dogs, frogs, horses, ships and trucks.\n• MNIST (LeCun et al. (1998)). The MNIST database of handwritten digits is one of the most\nstudied dataset benchmark for image classiﬁcation. The dataset contains 60,000 examples\nof handwritten digits from 0 to 9 for training and 10,000 additional examples for testing.\nEach sample is a 28 x 28 pixel gray level image.\n5.1\nRESULTS ON STL10\nSince STL10 is comprised of mostly unlabeled data, it is the most suitable to highlight the beneﬁts\nof the spatial contrasting criterion. The initial training was unsupervised, as described earlier, using\nthe entire set of 105, 000 samples (union of the original unlabeled set and labeled training set).\nThe representation outputted by the training, was used to initialize supervised training on the 5, 000\nlabeled images. Evaluation was done on a separate test set of 8, 000 samples. Comparing with state\nof the art results 1 we see an improvement of 7% in test accuracy over the best model by Zhao et al.\n6\nUnder review as a conference paper at ICLR 2017\nTable 1: State of the art results on STL-10 dataset\nModel\nSTL-10 test accuracy\nZero-bias Convnets - Paine et al. (2014)\n70.2%\nTriplet network - Hoffer & Ailon (2015)\n70.7%\nExemplar Convnets - Dosovitskiy et al. (2014)\n72.8%\nTarget Coding - Yang et al. (2015)\n73.15%\nStacked what-where AE - Zhao et al. (2015)\n74.33%\nSpatial contrasting initialization (this work)\n81.34% ± 0.1\nThe same model without initialization\n72.6% ± 0.1\nTable 2: State of the art results on Cifar10 dataset with only 4000 labeled samples\nModel\nCifar10 (400 per class) test accuracy\nConvolutional K-means Network - Coates & Ng (2012)\n70.7%\nView-Invariant K-means - Hui (2013)\n72.6%\nDCGAN - Radford et al. (2015)\n73.8%\nExemplar Convnets - Dosovitskiy et al. (2014)\n76.6%\nLadder networks - Rasmus et al. (2015)\n79.6%\nSpatial contrasting initialization (this work)\n79.2% ± 0.3\nThe same model without initialization\n72.4% ± 0.1\n(2015), setting the SC as best model at 81.3% test classiﬁcation accuracy. We also compare with\nthe same network, but without SC initialization, which achieves a lower classiﬁcation of 72.6%.\nThis is an indication that indeed SC managed to leverage unlabeled examples to provide a better\ninitialization point for the supervised model.\n5.2\nRESULTS ON CIFAR10\nFor Cifar10, we used a previously used setting Coates & Ng (2012) Hui (2013) Dosovitskiy et al.\n(2014) to test a model’s ability to learn from unlabeled images. In this setting, only 4, 000 samples\nfrom the available 50, 000 are used with their label annotation, but the entire dataset is used for\nunsupervised learning. The ﬁnal test accuracy is measured on the entire 10, 000 test set.\nIn our experiments, we trained our model using SC criterion on the entire dataset, and then used\nonly 400 labeled samples per class (for a total of 4000) in a supervised regime over the initialized\nnetwork. The results are compared with previous efforts in table 2. Using the SC criterion allowed\nan improvement of 6.8% over a non-initialized model, and achieved a ﬁnal test accuracy of 79.2%.\nThis is a competitive result with current state-of-the-art model of Rasmus et al. (2015).\n5.3\nRESULTS ON MNIST\nThe MNIST dataset is very different in nature from the Cifar10 and STL10, we experimented earlier.\nThe biggest difference, relevant to this work, is that spatial regions sampled from MNIST images\nusually provide very little, or no information. Because of this fact, SC is much less suited for\nuse with MNIST, and was conjured to have little beneﬁt. We still, however, experimented with\ninitializing a model with SC criterion and continuing with a fully-supervised regime over all labeled\nexamples. We found again that this provided beneﬁt over training the same network without pre-\ninitialization, improving results from 0.63% to 0.34% error on test set. The results, compared with\nprevious attempts are included in 3.\n6\nCONCLUSIONS AND FUTURE WORK\nIn this work we presented spatial contrasting - a novel unsupervised criterion for training convo-\nlutional networks on unlabeled data. Its is based on comparison between spatial features sampled\nfrom a number of images. We’ve shown empirically that using spatial contrasting as a pretraining\n7\nUnder review as a conference paper at ICLR 2017\nTable 3: results on MNIST dataset\nModel\nMNIST test error\nStacked what-where AE - Zhao et al. (2015)\n0.71%\nTriplet network - Hoffer & Ailon (2015)\n0.56%\nJarrett et al. (2009)\n0.53%\nLadder networks - Rasmus et al. (2015)\n0.36%\nDropConnect - Wan et al. (2013)\n0.21%\nSpatial contrasting initialization (this work)\n0.34% ± 0.02\nThe same model without initialization\n0.63% ± 0.02\ntechnique to initialize a ConvNet, can improve its performance on a subsequent supervised train-\ning. In cases where a lot of unlabeled data is available, such as the STL10 dataset, this translates to\nstate-of-the-art classiﬁcation accuracy in the ﬁnal model.\nSince the spatial contrasting loss is a differentiable estimation that can be computed within a network\nin parallel to supervised losses, future work will attempt to embed it as a semi-supervised model.\nThis usage will allow to create models that can leverage both labeled an unlabeled data, and can be\ncompared to similar semi-supervised models such as the ladder network Rasmus et al. (2015). It is\nis also apparent that contrasting can occur in dimensions other than the spatial, the most straightfor-\nward is the temporal one. This suggests that similar training procedure can be applied on segments\nof sequences to learn useful representation without explicit supervision.\nREFERENCES\nVassileios Balntas, Edward Johns, Lilian Tang, and Krystian Mikolajczyk. Pn-net: Conjoined triple\ndeep network for learning local image descriptors. arXiv preprint arXiv:1601.05030, 2016.\nJane Bromley, James W Bentz, L´eon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard\nS¨ackinger, and Roopak Shah. Signature veriﬁcation using a siamese time delay neural network.\nInternational Journal of Pattern Recognition and Artiﬁcial Intelligence, 7(04):669–688, 1993.\nSumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with\napplication to face veriﬁcation. In Computer Vision and Pattern Recognition, 2005. CVPR 2005.\nIEEE Computer Society Conference on, volume 1, pp. 539–546. IEEE, 2005.\nAdam Coates and Andrew Y Ng. Learning feature representations with k-means. In Neural Net-\nworks: Tricks of the Trade, pp. 561–580. Springer, 2012.\nAdam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in unsuper-\nvised feature learning. In International Conference on Artiﬁcial Intelligence and Statistics, pp.\n215–223, 2011.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n1422–1430, 2015.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimina-\ntive unsupervised feature learning with convolutional neural networks. In Advances in Neural\nInformation Processing Systems, pp. 766–774, 2014.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-\nmation Processing Systems, pp. 2672–2680, 2014.\nMichael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In International Conference on Artiﬁcial Intelligence and\nStatistics, pp. 297–304, 2010.\n8\nUnder review as a conference paper at ICLR 2017\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. arXiv preprint arXiv:1512.03385, 2015.\nGeoffrey E Hinton. To recognize shapes, ﬁrst learn to generate images. Progress in brain research,\n165:535–547, 2007.\nElad Hoffer and Nir Ailon. Deep metric learning using triplet network. In Similarity-Based Pattern\nRecognition, pp. 84–92. Springer, 2015.\nKa Y Hui. Direct modeling of complex invariances for visual object features. In Proceedings of the\n30th International Conference on Machine Learning (ICML-13), pp. 352–360, 2013.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by re-\nducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine\nLearning, pp. 448–456, 2015.\nKevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the best multi-\nstage architecture for object recognition?\nIn Computer Vision, 2009 IEEE 12th International\nConference on, pp. 2146–2153. IEEE, 2009.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Com-\nputer Science Department, University of Toronto, Tech. Rep, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classiﬁcation with Deep Con-\nvolutional Neural Networks. Advances In Neural Information Processing Systems, pp. 1–9, 2012.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,\n2013.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013.\nAndriy Mnih and Koray Kavukcuoglu. Learning word embeddings efﬁciently with noise-contrastive\nestimation. In Advances in Neural Information Processing Systems, pp. 2265–2273, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level\ncontrol through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\nAndrew Ng. Sparse autoencoder. 2011.\nTom Le Paine, Pooya Khorrami, Wei Han, and Thomas S Huang. An analysis of unsupervised\npre-training in light of recent advances. arXiv preprint arXiv:1412.6597, 2014.\nPedro O Pinheiro, Ronan Collobert, and Piotr Dollar. Learning to segment object candidates. In\nAdvances in Neural Information Processing Systems, pp. 1981–1989, 2015.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nAntti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko.\nSemi-\nsupervised learning with ladder networks. In Advances in Neural Information Processing Systems,\npp. 3532–3540, 2015.\nAli Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-\nshelf: an astounding baseline for recognition. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition Workshops, pp. 806–813, 2014.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-\npropagating errors. Cognitive modeling, 5(3):1.\n9\nUnder review as a conference paper at ICLR 2017\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face\nrecognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 815–823, 2015.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\ncomposing robust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning, pp. 1096–1103. ACM, 2008.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural\nnetworks using dropconnect. In Proceedings of the 30th International Conference on Machine\nLearning (ICML-13), pp. 1058–1066, 2013.\nShuo Yang, Ping Luo, Chen Change Loy, Kenneth W Shum, and Xiaoou Tang. Deep representation\nlearning with target coding. 2015.\nMatthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks.\nIn Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 2528–2535.\nIEEE, 2010.\nJunbo Zhao, Michael Mathieu, Ross Goroshin, and Yann Lecun. Stacked what-where auto-encoders.\narXiv preprint arXiv:1506.02351, 2015.\n7\nAPPENDIX\nTable 4: Convolutional models used, based on Lin et al. (2013), Rasmus et al. (2015)\nModel\nSTL10\nCIFAR-10\nMNIST\nInput: 96 × 96 RGB\nInput: 32 × 32 RGB\nInput: 28 × 28 monochrome\n5 × 5 conv. 64 BN ReLU\n3 × 3 conv. 96 BN LeakyReLU\n5 × 5 conv. 32 ReLU\n1 × 1 conv. 160 BN ReLU\n3 × 3 conv. 96 BN LeakyReLU\n1 × 1 conv. 96 BN ReLU\n3 × 3 conv. 96 BN LeakyReLU\n3 × 3 max-pooling, stride 2\n2 × 2 max-pooling, stride 2 BN\n2 × 2 max-pooling, stride 2 BN\n5 × 5 conv. 192 BN ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 conv. 64 BN ReLU\n1 × 1 conv. 192 BN ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 conv. 64 BN ReLU\n1 × 1 conv. 192 BN ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 max-pooling, stride 2\n2 × 2 max-pooling, stride 2 BN\n2 × 2 max-pooling, stride 2 BN\n3 × 3 conv. 192 BN ReLU\n1 × 1 conv. 192 BN ReLU\n1 × 1 conv. 192 BN ReLU\nSpatial contrasting criterion\n3 × 3 conv. 256 ReLU\n3 × 3 conv. 192 BN LeakyReLU\n3 × 3 conv. 128 BN ReLU\n3 × 3 max-pooling, stride 2\n1 × 1 conv. 192 BN LeakyReLU\n1 × 1 conv. 10 BN ReLU\ndropout, p = 0.5\n1 × 1 conv. 10 BN LeakyReLU\nglobal average pooling\n3 × 3 conv. 128 ReLU\nglobal average pooling\ndropout, p = 0.5\nfully-connected 10\n10-way softmax\n10\nUnder review as a conference paper at ICLR 2017\nFigure 2: First layer convolutional ﬁlters after spatial-contrasting training\n11\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2016-10-02",
  "updated": "2018-12-04"
}