{
  "id": "http://arxiv.org/abs/1610.00633v2",
  "title": "Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates",
  "authors": [
    "Shixiang Gu",
    "Ethan Holly",
    "Timothy Lillicrap",
    "Sergey Levine"
  ],
  "abstract": "Reinforcement learning holds the promise of enabling autonomous robots to\nlearn large repertoires of behavioral skills with minimal human intervention.\nHowever, robotic applications of reinforcement learning often compromise the\nautonomy of the learning process in favor of achieving training times that are\npractical for real physical systems. This typically involves introducing\nhand-engineered policy representations and human-supplied demonstrations. Deep\nreinforcement learning alleviates this limitation by training general-purpose\nneural network policies, but applications of direct deep reinforcement learning\nalgorithms have so far been restricted to simulated settings and relatively\nsimple tasks, due to their apparent high sample complexity. In this paper, we\ndemonstrate that a recent deep reinforcement learning algorithm based on\noff-policy training of deep Q-functions can scale to complex 3D manipulation\ntasks and can learn deep neural network policies efficiently enough to train on\nreal physical robots. We demonstrate that the training times can be further\nreduced by parallelizing the algorithm across multiple robots which pool their\npolicy updates asynchronously. Our experimental evaluation shows that our\nmethod can learn a variety of 3D manipulation skills in simulation and a\ncomplex door opening skill on real robots without any prior demonstrations or\nmanually designed representations.",
  "text": "Deep Reinforcement Learning for Robotic Manipulation with\nAsynchronous Off-Policy Updates\nShixiang Gu∗,1,2,3 and Ethan Holly∗,1 and Timothy Lillicrap4 and Sergey Levine1,5\nAbstract— Reinforcement learning holds the promise of en-\nabling autonomous robots to learn large repertoires of behav-\nioral skills with minimal human intervention. However, robotic\napplications of reinforcement learning often compromise the\nautonomy of the learning process in favor of achieving training\ntimes that are practical for real physical systems. This typically\ninvolves introducing hand-engineered policy representations\nand human-supplied demonstrations. Deep reinforcement learn-\ning alleviates this limitation by training general-purpose neural\nnetwork policies, but applications of direct deep reinforcement\nlearning algorithms have so far been restricted to simulated\nsettings and relatively simple tasks, due to their apparent\nhigh sample complexity. In this paper, we demonstrate that\na recent deep reinforcement learning algorithm based on off-\npolicy training of deep Q-functions can scale to complex\n3D manipulation tasks and can learn deep neural network\npolicies efﬁciently enough to train on real physical robots. We\ndemonstrate that the training times can be further reduced\nby parallelizing the algorithm across multiple robots which\npool their policy updates asynchronously. Our experimental\nevaluation shows that our method can learn a variety of\n3D manipulation skills in simulation and a complex door\nopening skill on real robots without any prior demonstrations\nor manually designed representations.\nI. INTRODUCTION\nReinforcement learning methods have been applied to\nrange of robotic control tasks, from locomotion [1], [2] to\nmanipulation [3], [4], [5], [6] and autonomous vehicle control\n[7]. However, practical real-world applications of reinforce-\nment learning have typically required signiﬁcant additional\nengineering beyond the learning algorithm itself: an appro-\npriate representation for the policy or value function must\nbe chosen so as to achieve training times that are practical\nfor physical hardware [8], and example demonstrations must\noften be provided to initialize the policy and mitigate safety\nconcerns during training [9]. In this work, we show that\na recently proposed deep reinforcement learning algorithms\nbased on off-policy training of deep Q-functions [10], [11]\ncan be extended to learn complex manipulation policies from\nscratch, without user-provided demonstrations, and using\nonly general-purpose neural network representations that do\nnot require task-speciﬁc domain knowledge.\nOne of the central challenges with applying direct deep\nreinforcement learning algorithms to real-world robotic plat-\nforms has been their apparent high sample-complexity. We\ndemonstrate that, contrary to commonly held assumptions,\nrecently developed off-policy deep Q-function based al-\ngorithms such as the Deep Deterministic Policy Gradient\n∗equal contribution, 1Google Brain, 2University of Cambridge, 3MPI\nT¨ubingen, 4Google DeepMind, 5UC Berkeley\nFig. 1: Two robots learning a door opening task. We present\na method that allows multiple robots to cooperatively learn\na single policy with deep reinforcement learning.\nalgorithm (DDPG) [10] and Normalized Advantage Function\nalgorithm (NAF) [11] can achieve training times that are\nsuitable for real robotic systems. We also demonstrate that\nwe can further reduce training times by parallelizing the\nalgorithm across multiple robotic platforms. To that end,\nwe present a novel asynchronous variant of NAF, evaluate\nthe speedup obtained with varying numbers of learners in\nsimulation, and demonstrate real-world results with paral-\nlelism across multiple robots. An illustration of these robots\nlearning a door opening task is shown in Figure 1.\nThe main contribution of this paper is a demonstration of\nasynchronous deep reinforcement learning using our parallel\nNAF algorithm across a cluster of robots. Our technical\ncontribution consists of the asynchronous variant of the NAF\nalgorithm, as well as practical extensions of the method to\nenable sample-efﬁcient training on real robotic platforms.\nWe also introduce a simple and effective safety mechanism\nfor constraining exploration at training time, and present\nsimulated experiments that evaluate the speedup obtained\nfrom parallelizing across a variable number of learners.\nOur experiments also evaluate the beneﬁts of deep neural\nnetwork representations for several complex manipulation\ntasks, including door opening and pick-and-place, by com-\nparing to more standard linear representations. Our real\nworld experiments show that our approach can be used to\nlearn a door opening skill from scratch using only general-\npurpose neural network representations and without any\nhuman demonstrations. To the best of our knowledge, this\nis the ﬁrst demonstration of autonomous door opening that\ndoes not use human-provided examples for initialization.\nII. RELATED WORK\nApplications of reinforcement learning (RL) in robotics\nhave included locomotion [1], [2], manipulation [3], [4],\narXiv:1610.00633v2  [cs.RO]  23 Nov 2016\n[5], [6], and autonomous vehicle control [7]. Many of\nthe RL methods demonstrated on physical robotic systems\nhave used relatively low-dimensional policy representations,\ntypically with under one hundred parameters, due to the\ndifﬁculty of efﬁciently optimizing high-dimensional policy\nparameter vectors [12]. Although there has been considerable\nresearch on reinforcement learning with general-purpose\nneural networks for some time [13], [14], [15], [16], [17],\nsuch methods have only recently been developed to the point\nwhere they could be applied to continuous control of high-\ndimensional systems, such as 7 degree-of-freedom (DoF)\narms, and with large and deep neural networks [18], [19],\n[10], [11]. This has made it possible to learn complex skills\nwith minimal manual engineering, though it has remained\nunclear whether such approaches could be adapted to real\nsystems given their sample complexity.\nIn robotic learning scenarios, prior work has explored both\nmodel-based and model-free learning algorithms. Model-\nbased algorithms have explored a variety of dynamics esti-\nmation schemes, including Gaussian processes [20], mixture\nmodels [21], and local linear system estimation [22], with\na more detailed overview in a recent survey [23]. Deep\nneural network policies have been combined with model-\nbased learning in the context of guided policy search al-\ngorithms [19], which use a model-based teacher to train a\ndeep network policies. Such methods have been successful\non a range of real-world tasks, but rely on the ability of\nthe model-based teacher to discover good trajectories for the\ngoal task. As shown in recent work, this can be difﬁcult\nin domains with severe discontinuities in the dynamics and\nreward function [24].\nIn this work, we focus on model-free reinforcement learn-\ning, which includes policy search methods [25], [3], [23]\nand function approximation methods [26], [27], [14]. Both\napproaches have recently been combined with deep neural\nnetworks for learning complex tasks [28], [29], [18], [10],\n[11], [30]. Direct policy gradient methods offer the beneﬁt\nof unbiased gradient estimates, but tend to require more\nexperience, since on-policy estimators preclude reuse of past\ndata [18]. On the other hand, methods based on Q-function\nestimation often allow us to leverage past off-policy data. We\ntherefore build on Q-function based methods, extending Nor-\nmalized Advantage Estimation (NAF) [11], which extends Q-\nlearning to continuous action spaces. This method is closely\nrelated to Deep Deterministic Policy Gradient (DDPG) [10]\nas well as NFQCA [31], with principle differences being that\nNFQCA employs a batch episodic update and typically resets\nnetwork parameters between episodes.\nAccelerating robotic learning by pooling experience from\nmultiple robots has long been recognized as a promising\ndirection in the domain of cloud robotics, where it is typically\nreferred to as collective robotic learning [32], [33], [34], [35].\nIn deep reinforcement learning, parallelized learning has also\nbeen proposed to speed up simulated experiments [30]. The\ngoals of this prior work are fundamentally different from\nours: while prior asynchronous deep reinforcement learning\nwork seeks to reduce overall training time, under the as-\nsumption that simulation time is inexpensive and the training\nis dominated by neural network computations, our work\ninstead seeks to minimize the training time when training\non real physical robots, where experience is expensive and\ncomputing neural network backward passes is comparatively\ncheap. In this case, we retain the use of a replay buffer,\nand focus on asynchronous execution and neural network\ntraining. Our results demonstrate that we achieve signiﬁcant\nspeedup in overall training time from simultaneously collect-\ning experience across multiple robotic platforms. This result\nagrees with a concurrent result on parallelized guided policy\nsearch [36], and in fact we tackle a similar door opening\ntask. In contrast to this concurrent work, we perform the\ntask from scratch, without initializing the algorithm from\nuser demonstrations. While the concurrent work by Yahya et\nal. provides a more extensive exploration of generalization\nto physical variation, we focus on analyzing learning speed\nof model-free deep reinforcement learning methods.\nIII. BACKGROUND\nIn this section, we will formulate the robotic reinforcement\nlearning problem, introduce essential notation, and describe\nthe existing algorithmic foundations on which we build the\nmethods for this work. The goal in reinforcement learning\nis to control an agent attempting to maximize a reward\nfunction which, in the context of a robotic skill, denotes\na user-provided deﬁnition of what the robot should try to\naccomplish. At state xxxt in time t, the agent chooses and\nexecutes action uuut according to its policy π(uuut|xxxt), transitions\nto a new state xxxt according to the dynamics p(xxxt|xxxt,uuut)\nand receives a reward r(xxxt,uuut). Here, we consider inﬁnite-\nhorizon discounted return problems, where the objective is\nthe γ−discounted future return from time t to ∞, given by\nRt = ∑∞\ni=t γ(i−t)r(xxxi,uuui). The goal is to ﬁnd the optimal policy\nπ∗which maximizes the expected sum of returns from the\ninitial state distribution, given by R = Eπ[R1].\nAmong reinforcement learning methods, off-policy meth-\nods such as Q-learning offer signiﬁcant data efﬁciency com-\npared to on-policy variants, which is crucial for robotics\napplications. Q-learning trains a greedy deterministic pol-\nicy π(uuut|xxxt) = δ(uuut = µµµ(xxxt)) by iterating between learning\nthe Q-function, Qπn(xxxt,uuut) = Eri≥t,xxxi>t∼E,uuui>t∼πn[Rt|xxxt,uuut], of\na policy and updating the policy by greedily maximiz-\ning the Q-function, µµµn+1(xxxt) = argmaxuuu Qπn(xxxt,uuut). Let θ Q\nparametrize the action-value function, β be an arbitrary\nexploration policy, and ρβ be the state visitation distribution\ninduced by β, the learning objective is to minimize the\nBellman error, where we ﬁx the target yt:\nL(θ Q) = Exxxt∼ρβ ,uuut∼β,xxxt+1,rt∼E[(Q(xxxt,uuut|θ Q)−yt)2]\nyt = r(xxxt,uuut)+γQ(xxxt+1,µµµ(xxxt+1))\nFor continuous action problems, the policy update step is\nintractable for a Q-function parametrized by a deep neural\nnetwork. Thus, we will investigate Deep Deterministic Policy\nGradient (DDPG) [10] and Normalized Advantage Functions\n(NAF) [11]. DDPG circumvents the problem by adopting\nan actor-critic method, while NAF restricts the class of\nQ-function to the expression below to enable closed-form\nupdates, as in the discrete action case. During exploration, a\ntemporally-correlated noise is added to the policy network\noutput. For more details and comparisons on DDPG and\nNAF, please refer to [10], [11] as well as experimental results\nin Section V-B.\nQ(xxx,uuu|θ Q) = A(xxx,uuu|θ A)+V(xxx|θV)\nA(xxx,uuu|θ A) = −1\n2(uuu−µµµ(xxx|θ µ))TPPP(xxx|θ P)(uuu−µµµ(xxx|θ µ))\nWe evaluate both DDPG and NAF in our simulated ex-\nperiments, where they yield comparable performance, with\nNAF producing slightly better results overall for the tasks\nexamined here. On real physical systems, we focus on\nvariants of the NAF method, which is simpler, requires\nonly a single optimization objective, and has fewer hyper-\nparameters.\nThis RL formulation can be applied on robotic systems\nto learn a variety of skills deﬁned by reward functions.\nHowever, the learning process is typically time consuming,\nand requires a number of practical considerations. In the\nnext section, we will present our main technical contribution,\nwhich consists of a parallelized variant of NAF, and also\ndiscuss a variety of technical contributions necessary to apply\nNAF to real-world robotic skill learning.\nIV. ASYNCHRONOUS TRAINING OF NORMALIZED\nADVANTAGE FUNCTIONS\nIn this section, we present our primary contribution: an\nextension of NAF that makes it practical for use with real-\nworld robotic platforms. To that end, we describe how\nonline training of the Q-function estimator can be performed\nasynchronously, with a learner thread that trains the network\nand one or more worker threads that collect data by executing\nthe current policy on one or more robots. Besides making\nNAF suitable for real time applications, this approach also\nmakes it straightforward to collect experience from multiple\nrobots in parallel. This is crucial in real-world robot learning,\nsince the learning time is often constrained by the data\ncollection rate in real time, rather than network training\nspeed. When data collection is the limiting factor, then 2-\n3 times quicker data collection may translate directly to 2-3\ntimes faster skill acquisition on a real robot. We also describe\npractical considerations, such as safety constraints, which are\nnecessary in order to allow the exploration required to train\ncomplex policies from scratch on real systems. To the best\nof our knowledge, this is the ﬁrst direct deep RL method that\nhas been demonstrated on a real robotics platform with many\nDoFs and contact dynamics, and without demonstrations or\nsimulated pretraining [18], [10], [11]. As we will show in\nour experimental evaluation, this approach can be used to\nlearn complex tasks such as door opening from scratch,\nwhich previously required additional details such as human\ndemonstrations to succeed [6].\nA. Asynchronous Learning\nIn asynchronous NAF, the learner thread is separated from\nthe experience collecting worker threads. The asynchronous\nAlgorithm 1 Asynchronous NAF - N collector threads and\n1 trainer thread\n// trainer thread\nRandomly initialize normalized Q network Q(xxx,uuu|θQ), where\nθQ = {θ µ,θP,θV } as in Eq. 1\nInitialize target network Q′ with weight θQ′ ←θQ\nInitialize shared replay buffer R ←/0\nfor iteration=1,I do\nSample a random minibatch of m transitions from R\nSet yi =\n(\nri +γV ′(xxx′\ni|θQ′)\nif\nti < T\nri\nif\nti = T\nUpdate\nthe\nweight\nθQ\nby\nminimizing\nthe\nloss:\nL = 1\nm ∑i(yi −Q(xxxi,uuui|θQ))2\nUpdate the target network: θQ′ ←τθQ +(1−τ)θQ′\nend for\n// collector thread n, n = 1...N\nRandomly initialize policy network µµµ(xxx|θ µ\nn )\nfor episode=1,M do\nSync policy network weight θ µ\nn ←θ µ\nInitialize a random process N for action exploration\nReceive initial observation state xxx1 ∼p(xxx1)\nfor t=1,T do\nSelect action uuut = µµµ(xxxt|θ µ\nn )+Nt\nExecute uuut and observe rt and xxxt+1\nSend transition (xxxt,uuut,rt,xxxt+1,t) to R\nend for\nend for\nlearning algorithm is summarized in Algorithm 1. The\nlearner thread uses the replay buffer to perform asynchronous\nupdates to the deep neural network Q-function approximator.\nThis thread runs on a central server, and dispatches updated\npolicy parameters to each of the worker threads. The experi-\nence collecting worker threads run on the individual robots,\nand send the observation, action, and reward for each time\nstep to the central server to append to the replay buffer. This\ndecoupling between the training and the collecting threads\nallows the controllers on each of the robots to run in real\ntime, without experiencing delays due to the computational\ncost of backpropagation through the network. Furthermore, it\nmakes it straightforward to parallelize experience collection\nacross multiple robots simply by adding additional worker\nthreads. We only use one thread for training the network;\nhowever, the gradient computation can also be distributed in\nsame way as [30] within our framework. While the trainer\nthread keeps training from the centralized replay buffer, the\ncollector threads sync their policy parameters with the trainer\nthread at the beginning of each episode, execute commands\non the robots, and push experience into the buffer.\nB. Safety Constraints\nEnsuring safe exploration poses a signiﬁcant challenge for\nreal-world training with reinforcement learning. Q-learning\nrequires a signiﬁcant amount of noisy exploration for gath-\nering the experience necessary for action-value function\napproximation. For all experiments, we set a maximum com-\nmanded velocity allowed per joint, as well as strict position\nlimits for each joint. In addition to joint position limits, we\nused a bounding sphere for the end-effector position. If the\ncommanded joint velocities would send the end-effector out-\nside of the sphere, we used the forward kinematics to project\nthe commanded velocity onto the surface of the sphere, plus\nsome correction velocity to force toward the center. For\nexperiments with no contacts, these safety constraints were\nsufﬁcient to prevent unsafe exploration; for experiments with\ncontacts, additional heuristics were required for safety.\nC. Network Architectures\nTo minimize manual engineering, we use a simple and\nreadily available state representation consisting of joint\nangles and end-effector positions, as well as their time\nderivatives. In addition, we append a target position to the\nstate, which depends on the task: for the reaching task,\nthis is the goal position for the end-effector; for the door\nopening, this is the handle position when the door is closed\nand the quaternion measurement of the sensor attached to\nthe door frame. Since the state representation is compact,\nwe use standard feed-forward networks to parametrize the\naction-value functions and policies. We use two-hidden-layer\nnetwork with size 100 units each to parametrize each of µµµ(x),\nLLL(x) (Cholesky decomposition of PPP(xxx)), and V(xxx) in NAF\nand µµµ(x) and Q(xxx,uuu) in DDPG. For Q(xxx,uuu) in DDPG, the\naction vector uuu added as another input to second hidden layer\nfollowed by a linear projection. ReLU is used as hidden\nactivations and hyperbolic tangent (Tanh) is used for the\nﬁnal layer activation function in the policy networks µµµ(xxx)\nto bound the action scale.\nTo illustrate the importance of deep neural networks\nfor representing policies or action-value functions, we\nstudy these neural network models against another sim-\npler parametrization. Speciﬁcally we study a variant of\nNAF (Linear-NAF) as below, where µµµ(xxx) = f(kkk + KKKxxx),\nPPP,kkk,KKK,BBB,bbb,c are learnable matricies, vectors, or scalars of\nappropriate dimension, and f is Tanh to enforce bounded\nactions.\nQ(xxx,uuu) = 1\n2(uuu−µµµ(xxx))TPPP(uuu−µµµ(xxx))+xxxTBBBxxx+xxxTbbb+c\nIf f is identity, then the expression corresponds to a globally\nquadratic Q-function and a linear feedback policy, though\ndue to the Tanh non-linearity, the Q-function is not linear\nwith respect to state-action features.\nV. SIMULATED EXPERIMENTS\nWe ﬁrst performed a detailed investigation of the learning\nalgorithms using simulated tasks modeled using the MuJoCo\nphysics simulator [37]. Simulated environments enable fast\ncomparisons of design choices, including update frequen-\ncies, parallelism, network architectures, and other hyper-\nparameters. We modeled a 7-DoF lightweight arm that was\nalso used in our physical robot experiments, as well as a 6-\nDoF Kinova JACO arm with 3 additional degrees of freedom\nin the ﬁngers, for a total of 9 degrees of freedom. Both arms\nwere controlled at the level of joint velocities, except the\nthree JACO ﬁnger joints which are controlled with torque\nactuators. The 7-DoF arm is controlled at 20Hz to match the\nreal-world robot experiments, and the JACO arm is controlled\nat 100Hz. Gravity is turned off for the 7-DoF arm, which is\na valid assumption given that the actual robot uses built-\nin gravity compensation. Gravity is enabled for the JACO\narm. Different arm geometries, control frequencies, and\ngravity settings illustrate the learning algorithm’s robustness\nto different learning environments.\nA. Simulation Tasks\nFig. 2: The 7-DoF arm and JACO arm in simulation.\nTasks include random-target reaching, door pushing, door\npulling, and pick & place in a 3D environment, as detailed\nbelow. The 7-DoF arm is set up for the random target\nreaching and door tasks, while the JACO arm is used for\nthe pick & place task (see Figure 2). Details of each task\nare below, where d is Huber loss and ci’s are non-negative\nconstants. Discount factor of γ = 0.98 is chosen and the\nAdam optimizer [38] with base learning rate of either 0.0001\nor 0.001 is used for all the experiments. Importantly, almost\nno hyperparameter search was required to ensure that the\nemployed algorithms were successful across robot and task.\n1) Reaching (7-DoF arm): The 7-DoF arm tries to reach\na random target in space from a ﬁxed initial conﬁguration.\nA random target is generated per episode by sampling points\nuniformly from a cube of size 0.2m centered around a\npoint. State features include the 7 joint angles and their time\nderivatives, the end-effector position and the target position,\ntotalling 20 dimensions. Each episode duration is 150 time\nsteps (7.5 seconds). Success rate is computed from 5 random\ntest episodes where an episode is successful if the arm can\nreach within 5 cm of the target. Given the end-effector\nposition eee and the target position yyy, the reward function is\nbelow,\nr(xxx,uuu) = −c1d(yyy,eee(xxx))−c2uuuTuuu\n2) Door Pushing and Pulling (7-DoF arm): The 7-DoF\narm tries to open the door by pushing or pulling the handle\n(see Figure 2). For each episode, the door position is sampled\nrandomly within a rectangle of 0.2m by 0.1m. The handle\ncan be turned downward for up to 90 degrees, while the\ndoor can be opened up to 90 degrees in both directions.\nThe door has a spring such that it closes gradually when no\nforce is applied. The door has a latch such that it could\nonly open the door only when the handle is turned past\napproximately 60 degrees. To make the setting similar to the\nreal robot experiment where the quaternion readings from the\nVectorNav IMU are used for door angle measurements, the\nquaternion of the door handle is used to compute the loss.\nThe reward function is composed of two parts: the closeness\nof the end-effector to the handle, and the measure of how\nmuch the door is opened in the right direction. The ﬁrst\npart depends on the distance between end-effector position eee\nand the handle position hhh in its neutral state. The second part\ndepends on the distance between the quaternion of the handle\nqqq and its value when the handle is turned and door is opened\nqqqo. We also added the distance when the door is at neutral\nposition as offset di = d(qqqo,qqqi) such that, when the door\nis opened the correct way, it receives positive reward. State\nfeatures include the 7 joint angles and their time derivatives,\nthe end-effector position, the resting handle position, the door\nframe position, the door angle, and the handle angle, totally\n25 dimensions. Each episode duration is 300 time steps (15\nseconds). Success rate is computed from 20 random test\nepisodes where an episode is successful if the arm can open\nthe door in the correct direction by a minimum of 10 degrees.\nr(xxx,uuu) = −c1d(hhh,eee(xxx))+c2(−d(qqqo,qqq(xxx))+di)−c3uuuTuuu\n3) pick & place (JACO): The JACO arm tries to pick\nup a stick suspending in the air by a string and place it\nnear the target upward in the space (see Figure 2). The hand\nbegins near to, but not in contact with the stick, so the grasp\nmust be learned. The task is similar to a task previously\nexplored with on-policy methods [30], except that here the\ntask requires moving the stick to multiple targets. For each\nepisode a new target is sampled from a square of size 0.24\nm a t a ﬁxed height, while the initial stick position and\nthe arm conﬁguration are ﬁxed. Given the grip site position\nggg (where the three ﬁngers meet when closed), the three\nﬁnger tip positions fff 1, fff 2, fff 3, the stick position sss and the\ntarget position yyy, the reward function is below. State features\ninclude the position and rotation matrices of all geometries\nin the environment, the target position and the vector from\nthe stick to the target, totally 180 dimensions. The large\nobservation dimensionality creates an interesting comparison\nwith the above two tasks. Each episode duration is 300 time\nsteps (3 seconds). Success rate is computed from 20 random\ntest episodes where an episode is judged successful if the\narm can bring the stick within 5 cm of the target.\nr(xxx,uuu) =−c1d(sss(xxx),ggg(xxx))−c2\n3\n∑\ni=1\nd(sss(xxx), fff i(xxx))\n−c3d(yyy,sss(xxx))−c4uuuTuuu\nB. Neural Network Policy Representations\nNeural networks are powerful function approximators, but\nthey have signiﬁcantly more parameters than the simpler\nlinear models that are often used in robotic learning [23],\n[8]. In this section, we compare empirical performance of\nDDPG, NAF, and Linear-NAF as described in Section IV-C.\nIn particular, we want to verify if deep representations for\npolicy and value functions are necessary for solving complex\ntasks from scratch, and evaluate how they compare with\nlinear models in terms of convergence rate. For the 7-DoF\n(a) Door Pulling\n(b) JACO pick & place\nFig. 3: The ﬁgure shows the learning curves for two tasks,\ncomparing DDPG, Linear-NAF, and NAF. Note that the\nlinear model struggles to learn the tasks, indicating the\nimportance of expressive nonlinear policy representations.\narm tasks, DDPG and NAF models have signiﬁcantly more\nparameters than Linear-NAF, while the pick & place task\nhas a high-dimensional observation, and thus the parameter\nsizes are more comparable. Of course, many other linear\nrepresentations are possible, including DMPs [39], splines\n[3], and task-speciﬁc representations [40]. This comparison\nonly serves to illustrate that our tasks are complex enough\nthat simple, fully generic linear representations are not by\nthemselves sufﬁcient for success. For the experiments in this\nsection, batch normalization [41] is applied. These exper-\niments were conducted synchronously, where 1 parameter\nupdate is applied per 1 time step in simulation.\nFigure 3 shows the experimental results on the 7-DoF door\npulling and JACO pick & place tasks and Table 4 summarizes\nthe overall results. For reaching and pick & place, Linear-\nNAF learns good policies competitive with those of NAF\nand DDPG, but converges signiﬁcantly slower than both\nNAF and DDPG. This is contrary to common belief that\nneural networks take signiﬁcantly more data and update steps\nto converge to good solutions. One possible explanation is\nthat in RL the data collection and the model learning are\ncoupled, and if the model is more expressive, it can explore a\ngreater variety of complex policies efﬁciently and thus collect\ndiverse and good data quickly. This is not a problem for well-\npre-trained policy learning but could be an important issue\nwhen learning from scratch. In the case of door tasks, the\nlinear model completely fails to learn perfect policies. More\nthorough investigations into how expressivity of the policy\nMax. success rate (%)\nEpisodes to 100% success (1000s)\nDDPG\nLin-NAF\nNAF\nDDPG\nLin-NAF\nNAF\nReach\n100±0\n100±0\n100±0\n3.2±0.7\n8±3\n3.6±1.0\nDoor Pull\n100± 0\n5 ± 6\n100± 0\n10±8\nN/A\n6±3\nDoor Push\n100±0\n40± 10\n100± 0\n3.1± 1.0\nN/A\n4.2± 1.0\nPick & Place\n100±0\n100±0\n100±0\n4.4± 0.6\n12± 3\n2.9±0.9\nFig. 4: The table summarizes the performances of DDPG, Linear-NAF, and NAF across four tasks. Note that the linear\nmodel learns the perfect reaching and pick & place policies given enough time, but fails to learn either of the door tasks.\ninteract with reinforcement learning is a promising direction\nfor future work.\nAdditionally, the experimental results on the door tasks\nshow that Linear-NAF does not succeed in learning such\ntasks. The difference from above tasks likely comes from\nthe complexity of policies. For reaching and pick & place,\nthe tasks mainly requires learning single-motion policies,\ne.g. close ﬁngers to grasp the stick and move it to\nthe target. For the door tasks, the robot is required to\nlearn how to hook onto the door handle in different lo-\ncations, turn it, and push or pull. See the supplemen-\ntary video at https://sites.google.com/site/\ndeeproboticmanipulation/ for learned resulting be-\nhaviors for each tasks.\nC. Asynchronous Training\n(a) Reaching\n(b) Door Pushing\nFig. 5: Asynchronous training of NAF in simulation. Note\nthat both learning speed and ﬁnal policy success rates de-\npending signiﬁcantly on the number of workers.\nIn asynchronous training, the training thread continu-\nously trains the network at a ﬁxed frequency determined\nby network size and computational hardware, while each\ncollector thread runs at a speciﬁed control frequency. The\nmain question to answer is: given these constraints, how\nmuch speedup can we gain from increasing the number of\nworkers, i.e. the data collection speed? To analyze this in a\nrealistic but controlled setting, we ﬁrst set up the following\nexperiment in simulation. We locked each collector thread\nto run at S times the speed of the training thread. Then, we\nvaried the number of collector threads N. Thus, the overall\ndata collection speed is approximately S×N times that of the\ntrainer thread. For our experiments, we varied N and ﬁxed\nS = 1/5 since our training thread runs at approximately 100\nupdates per second on CPU, while the collector thread in\nreal robot will be locked to 20Hz. Layer normalization is\napplied [42].\nFigure 5 shows the results on reaching and door pushing.\nThe x-axis shows the number of parameter updates, which\nis proportional to the amount of wall-clock time required for\ntraining, since the amount of data per step increases with the\nnumber of workers. The results demonstrate three points: (1)\nunder some circumstances, increasing data collection makes\nthe learning converge signiﬁcantly faster with respect to the\nnumber of gradient steps, (2) ﬁnal policy performances de-\npend a lot on the ratio between collecting and training speeds,\nand (3) there is a limit where collecting more data does not\nhelp speed up learning. However, we hypothesize that accel-\nerating the speed of neural network training, which in these\ncases was pegged to one update per time step, could allow\nthe model to ingest more data and beneﬁt more from greater\nparallelism. This is particularly relevant as parallel computa-\ntional hardware, such as GPUs, are improved and deployed\nmore widely. Videos of the learned policies are available\nin supplementary materials and online: https://sites.\ngoogle.com/site/deeproboticmanipulation/\nVI. REAL-WORLD EXPERIMENTS\nThe real-world experiments are conducted with the 7-DoF\narm shown in Figure 6. The tasks are the same as the sim-\nulation tasks in Section V-A with some minor changes. For\nreaching, the same state representation and reward functions\nare used. The randomized target position is sampled from a\ncube of 0.4 m, providing more diverse and extreme targets\nfor reaching. We noticed that these more aggressive targets,\ncombined with stricter safety measures (slower movements\nand tight joint limits), reduced the performance compared\nto the simulation, and thus we relax the deﬁnition of a\nFig. 6: Two robots learning to open doors using asynchronous NAF. The ﬁnal policy learned with two workers could achieve\na 100% success rate on the task across 20 consecutive trials.\nFig. 7: The 7-DoF arm random target reaching with asyn-\nchronous NAF on real robots. Note that 1 worker suffers in\nboth learning speed and ﬁnal policy performance.\nsuccessful episode for reporting, marking episodes within 10\ncm as successful. For the door task, the robot was required to\nreach for and pull the door open by hooking the handle with\nthe end-effector. Due to the geometry of the workspace, we\ncould not test the door pushing task on the real hardware. The\norientation of the door was measured by a VectorNav IMU\nattached to the back of the door. Unlike in the simulation, we\ncannot automatically reposition the door for every episode,\nso the pose of the door was kept ﬁxed. State features for the\ndoor task include the joint angles and their time derivatives,\nthe end effector position and the quaternion reading from the\nIMU, totalling 21 dimensions.\nA. Random Target Reaching\nThe simulation results in Section 5 provide approximate\nperformance gains that can be expected from parallelism.\nHowever, the simulation setting does not consider several is-\nsues that could arise in real-world experiments: delays due to\nslow resetting procedures, non-constant execution speeds of\nthe training thread, and subtle physics discrepancies among\nrobots. Thus, it is important to demonstrate the beneﬁts from\nparallel training with real robots.\nWe set up the same reaching experiment in the real world\nacross up to four robots. Robots execute policies at 20\nHz, while the training thread simply updates the network\ncontinuously at approximately 100 Hz. The same network\narchitecture and hyper-parameters from the simulation ex-\nperiment are used.\nFigure 7 conﬁrms that 2 or 4 workers signiﬁcantly im-\nproves learning speed over 1 worker, though the gains on this\nsimple task are not substantial past 2 workers. Importantly,\nwhen the training thread is not synchronized with the data\ncollection thread and the data collection is too slow, it may\nFig. 8: Learning curves for real-world door opening. Learn-\ning with two workers signiﬁcantly outperforms the single\nworker, and achieves a 100% success rate in under 500,000\nupdate steps, corresponding to about 2.5 hours of real time.\nnot just slow down learning but also hurt the ﬁnal policy\nperformance, as observed in the 1-worker case. Further\ndiscrepancies from the simulation may also be explained by\nphysical discrepancies among different robots. The learned\npolicies are presented in the supplementary video.\nB. Door Opening\nThe previous section describes a real-world evaluation\nof asynchronous NAF and demonstrates that learning can\nbe accelerated by using multiple workers. In this section,\nwe describe a more complex door opening task. Door\nopening presents a practical application of robotic learning\nthat involves complex and discontinuous contact dynamics.\nPrevious work has demonstrated learning of door opening\npolicies using example demonstration provided by a human\nexpert [6]. In this work, we demonstrate that we can learn\npolicies for pulling open a door from scratch using asyn-\nchronous NAF. The entire task required approximately 2.5\nhours to learn with two workers learning simultaneously, and\nthe ﬁnal policy achieves 100% success rate evaluated across\n20 consecutive trials. An illustration of this task is shown in\nFigure 6, and the supplementary video shows different stages\nin the learning process, as well as the ﬁnal learned policy.\nFigure 8 illustrates the difference in the learning process\nbetween one and two workers, where the horizontal axis\nshows the number of parameter updates. 100,000 updates\ncorrespond to approximately half an hour, with some delays\nincurred due to periodic policy evaluation, which is only used\nfor measuring the reward for the plot. One worker required\nsigniﬁcantly more than 4 hours to achieve 100% success\nrate, while two workers achieved the same success rate in\n2.5 hours. Qualitatively, the learning process goes through\na set of stages as the robots learn the task, as illustrated by\nlearning curves in Figure 8, where the plateau near reward=0\ncorresponds to placing the hook near the handle, but not\npulling the door open. In the ﬁrst stage, the robots are unable\nto reach the handle, and explore the free space to determine\nan effective policy for reaching. Once the robots begin to\ncontact the handle sporadically, they will occasionally pull\non the handle by accident, but require additional training to\nbe able to reach the handle consistently; this corresponds to\nthe plateau in the learning curves. At this point, it becomes\nmuch easier for the robots to pull open the door, and a\nsuccessful policy emerges. The ﬁnal policy learned by the\ntwo workers was able to open the door every time, including\nin the presence of exploration noise.\nVII. DISCUSSION AND FUTURE WORK\nWe presented an asynchronous deep reinforcement learn-\ning approach that can be used to learn complex robotic\nmanipulation skills from scratch on real physical robotic\nmanipulators. We demonstrate that our approach can learn a\ncomplex door opening task with only a few hours of training,\nand our simulated results demonstrate that training times de-\ncrease with more learners. Our technical contribution consists\nof a novel asynchronous version of the normalized advantage\nfunctions (NAF) deep reinforcement learning algorithm, as\nwell as a number of practical extensions to enable safe and\nefﬁcient deep reinforcement learning on physical systems,\nand our experiments conﬁrm the beneﬁts of nonlinear deep\nneural network policies over simpler shallow representations\nfor complex robotic manipulation tasks.\nWhile we’ve shown that deep off-policy reinforcement\nlearning algorithms are capable of learning complex ma-\nnipulation skills from scratch and without purpose built\nrepresentations, our method has a number of limitations.\nAlthough each of the tasks is learned from scratch, the\nreward function provides some amount of guidance to the\nlearning algorithm. In the reacher task, the reward provides\nthe distance to the target, while in the door task, it provides\nthe distance from the gripper to the handle as well as the\ndifference between the current and desired door pose. If the\nreward consists only of a binary success signal, both tasks\nbecome substantially more difﬁcult and require considerably\nmore exploration. However, such simple binary rewards may\nbe substantially easier to engineer in many practical robotic\nlearning applications. Improving exploration and learning\nspeed in future work to enable the use of such sparse rewards\nwould further improve the practical applicability of the class\nof methods explored here.\nAnother promising direction of future work is to inves-\ntigate how diverse experience of multiple robotic platforms\ncan be appropriately integrated into a single policy. While we\ntake the simplest approach of pooling all collected experi-\nence, multi-robot learning differs fundamentally from single-\nrobot learning in the diversity of experience that multiple\nrobots can collect. For example, in a real-world instantiation\nof the door opening example, each robot might attempt to\nopen a different door, eventually allowing for generalization\nacross door types. Properly handling such diversity might\nbeneﬁt from explicit exploration or even separate policies\ntrained on each robot, with subsequent pooling based on\npolicy distillation [43]. Exploring these extensions of our\nmethod could enable the training of highly generalizable\ndeep neural network policies in future work.\nACKNOWLEDGEMENTS\nWe sincerely thank Peter Pastor, Ryan Walker, Mrinal\nKalakrishnan, Ali Yahya, Vincent Vanhoucke for their as-\nsistance and advice on robot set-ups, Gabriel Dulac-Arnold\nand Jon Scholz for help on parallelization, and the Google\nBrain, X, and DeepMind teams for their support.\nREFERENCES\n[1] N. Kohl and P. Stone, “Policy gradient reinforcement learning for fast\nquadrupedal locomotion,” in International Conference on Robotics and\nAutomation (IROS), 2004.\n[2] G. Endo, J. Morimoto, T. Matsubara, J. Nakanishi, and G. Cheng,\n“Learning CPG-based biped locomotion with a policy gradient\nmethod: Application to a humanoid robot,” International Journal of\nRobotic Research, vol. 27, no. 2, pp. 213–228, 2008.\n[3] J. Peters and S. Schaal, “Reinforcement learning of motor skills with\npolicy gradients,” Neural Networks, vol. 21, no. 4, pp. 682–697, 2008.\n[4] E. Theodorou, J. Buchli, and S. Schaal, “Reinforcement learning\nof motor skills in high dimensions,” in International Conference on\nRobotics and Automation (ICRA), 2010.\n[5] J. Peters, K. M¨ulling, and Y. Alt¨un, “Relative entropy policy search,”\nin AAAI Conference on Artiﬁcial Intelligence, 2010.\n[6] M. Kalakrishnan, L. Righetti, P. Pastor, and S. Schaal, “Learning\nforce control policies for compliant manipulation,” in International\nConference on Intelligent Robots and Systems (IROS), 2011.\n[7] P. Abbeel, A. Coates, M. Quigley, and A. Ng, “An application of\nreinforcement learning to aerobatic helicopter ﬂight,” in Advances in\nNeural Information Processing Systems (NIPS), 2006.\n[8] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in\nrobotics: A survey,” International Journal of Robotic Research, vol. 32,\nno. 11, pp. 1238–1274, 2013.\n[9] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal, “Learning and\ngeneralization of motor skills by learning from demonstration,” in\nInternational Conference on Robotics and Automation (ICRA), 2009.\n[10] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforce-\nment learning,” International Conference on Learning Representations\n(ICLR), 2016.\n[11] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-\nlearning with model-based acceleration,” in International Conference\non Machine Learning (ICML), 2016.\n[12] M. Deisenroth, G. Neumann, and J. Peters, “A survey on policy search\nfor robotics,” Foundations and Trends in Robotics, vol. 2, no. 1-2, pp.\n1–142, 2013.\n[13] K. J. Hunt, D. Sbarbaro, R. ˙Zbikowski, and P. J. Gawthrop, “Neural\nnetworks for control systems: A survey,” Automatica, vol. 28, no. 6,\npp. 1083–1112, Nov. 1992.\n[14] M. Riedmiller, “Neural ﬁtted q iteration–ﬁrst experiences with a\ndata efﬁcient neural reinforcement learning method,” in European\nConference on Machine Learning.\nSpringer, 2005, pp. 317–328.\n[15] R. Hafner and M. Riedmiller, “Neural reinforcement learning con-\ntrollers for a real robot application,” in International Conference on\nRobotics and Automation (ICRA), 2007.\n[16] M. Riedmiller, S. Lange, and A. Voigtlaender, “Autonomous reinforce-\nment learning on raw visual input data in a real world application,” in\nInternational Joint Conference on Neural Networks, 2012.\n[17] J. Koutn´ık, G. Cuccu, J. Schmidhuber, and F. Gomez, “Evolving large-\nscale neural networks for vision-based reinforcement learning,” in\nConference on Genetic and Evolutionary Computation, ser. GECCO\n’13, 2013.\n[18] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel, “Trust\nregion policy optimization,” in International Conference on Machine\nLearning (ICML), 2015.\n[19] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training\nof deep visuomotor policies,” Journal of Machine Learning Research\n(JMLR), vol. 17, 2016.\n[20] M. Deisenroth and C. Rasmussen, “PILCO: a model-based and data-\nefﬁcient approach to policy search,” in International Conference on\nMachine Learning (ICML), 2011.\n[21] T. Moldovan, S. Levine, M. Jordan, and S. Abbeel, “Optimism-driven\nexploration for nonlinear systems,” in International Conference on\nRobotics and Automation (ICRA), 2015.\n[22] R. Lioutikov, A. Paraschos, G. Neumann, and J. Peters, “Sample-\nbased information-theoretic stochastic optimal control,” in Interna-\ntional Conference on Robotics and Automation, 2014.\n[23] M. P. Deisenroth, G. Neumann, J. Peters et al., “A survey on policy\nsearch for robotics.” Foundations and Trends in Robotics, vol. 2, no.\n1-2, pp. 1–142, 2013.\n[24] Y. Chebotar, M. Kalakrishnan, A. Yahya, A. Li, S. Schaal, and\nS. Levine, “Path integral guided policy search,” arXiv preprint\narXiv:1610.00529, 2016.\n[25] R. Williams, “Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning,” Machine Learning, vol. 8, no.\n3-4, pp. 229–256, May 1992.\n[26] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,\nno. 3-4, pp. 279–292, 1992.\n[27] R. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient\nmethods for reinforcement learning with function approximation,” in\nAdvances in Neural Information Processing Systems (NIPS), 1999.\n[28] J. Koutn´ık, G. Cuccu, J. Schmidhuber, and F. Gomez, “Evolving large-\nscale neural networks for vision-based reinforcement learning,” in Pro-\nceedings of the 15th annual conference on Genetic and evolutionary\ncomputation.\nACM, 2013, pp. 1061–1068.\n[29] V. Mnih et al., “Human-level control through deep reinforcement\nlearning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015.\n[30] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in International Conference on Machine\nLearning (ICML), 2016, pp. 1928–1937.\n[31] R. Hafner and M. Riedmiller, “Reinforcement learning in feedback\ncontrol,” Machine learning, vol. 84, no. 1-2, pp. 137–169, 2011.\n[32] M. Inaba, S. Kagami, F. Kanehiro, and Y. Hoshino, “A platform\nfor robotics research based on the remote-brained robot approach,”\nInternational Journal of Robotics Research, vol. 19, no. 10, 2000.\n[33] J. Kuffner, “Cloud-enabled humanoid robots,” in IEEE-RAS Interna-\ntional Conference on Humanoid Robotics, 2010.\n[34] B. Kehoe, A. Matsukawa, S. Candido, J. Kuffner, and K. Goldberg,\n“Cloud-based robot grasping with the google object recognition en-\ngine,” in IEEE International Conference on Robotics and Automation,\n2013.\n[35] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A survey of research\non cloud robotics and automation,” IEEE Transactions on Automation\nScience and Engineering, vol. 12, no. 2, April 2015.\n[36] A. Yahya, A. Li, M. Kalakrishnan, Y. Chebotar, and S. Levine, “Col-\nlective robot reinforcement learning with distributed asynchronous\nguided policy search,” arXiv preprint arXiv:1610.00673, 2016.\n[37] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for\nmodel-based control,” in 2012 IEEE/RSJ International Conference on\nIntelligent Robots and Systems.\nIEEE, 2012, pp. 5026–5033.\n[38] J. Ba and D. Kingma, “Adam: A method for stochastic optimization,”\n2015.\n[39] J. Kober and J. Peters, “Learning motor primitives for robotics,” in\nInternational Conference on Robotics and Automation (ICRA), 2009.\n[40] R. Tedrake, T. W. Zhang, and H. S. Seung, “Learning to walk in 20\nminutes.”\n[41] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” arXiv preprint\narXiv:1502.03167, 2015.\n[42] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[43] A. Rusu, S. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,\nR. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, “Policy\ndistillation,” in International Conference on Learning Representations\n(ICLR), 2016.\n",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2016-10-03",
  "updated": "2016-11-23"
}