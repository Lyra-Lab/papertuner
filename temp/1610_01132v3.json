{
  "id": "http://arxiv.org/abs/1610.01132v3",
  "title": "A Non-generative Framework and Convex Relaxations for Unsupervised Learning",
  "authors": [
    "Elad Hazan",
    "Tengyu Ma"
  ],
  "abstract": "We give a novel formal theoretical framework for unsupervised learning with\ntwo distinctive characteristics. First, it does not assume any generative model\nand based on a worst-case performance metric. Second, it is comparative, namely\nperformance is measured with respect to a given hypothesis class. This allows\nto avoid known computational hardness results and improper algorithms based on\nconvex relaxations. We show how several families of unsupervised learning\nmodels, which were previously only analyzed under probabilistic assumptions and\nare otherwise provably intractable, can be efficiently learned in our framework\nby convex optimization.",
  "text": "arXiv:1610.01132v3  [cs.LG]  27 Dec 2016\nA Non-generative Framework and Convex Relaxations for\nUnsupervised Learning\nElad Hazan\nTengyu Ma\nDecember 28, 2016\nAbstract\nWe give a novel formal theoretical framework for unsupervised learning with two distinctive\ncharacteristics. First, it does not assume any generative model and based on a worst-case perfor-\nmance metric. Second, it is comparative, namely performance is measured with respect to a given\nhypothesis class. This allows to avoid known computational hardness results and improper algo-\nrithms based on convex relaxations. We show how several families of unsupervised learning models,\nwhich were previously only analyzed under probabilistic assumptions and are otherwise provably in-\ntractable, can be efﬁciently learned in our framework by convex optimization.\n1\nIntroduction\nUnsupervised learning is the task of learning structure from unlabelled examples. Informally, the main\ngoal of unsupervised learning is to extract structure from the data in a way that will enable efﬁcient\nlearning from future labelled examples for potentially numerous independent tasks.\nIt is useful to recall the Probably Approximately Correct (PAC) learning theory for supervised learn-\ning [Val84], based on Vapnik’s statistical learning theory [Vap98]. In PAC learning, the learning can\naccess labelled examples from an unknown distribution. On the basis of these examples, the learner\nconstructs a hypothesis that generalizes to unseen data. A concept is said to be learnable with respect to\na hypothesis class if there exists an (efﬁcient) algorithm that outputs a generalizing hypothesis with high\nprobability after observing polynomially many examples in terms of the input representation.\nThe great achievements of PAC learning that made it successful are its generality and algorithmic\napplicability: PAC learning does not restrict the input domain in any way, and thus allows very general\nlearning, without generative or distributional assumptions on the world. Another important feature is\nthe restriction to speciﬁc hypothesis classes, without which there are simple impossibility results such\nas the “no free lunch” theorem. This allows comparative and improper learning of computationally-hard\nconcepts.\nThe latter is a very important point which is often understated. Consider the example of sparse\nregression, which is a canonical problem in high dimensional statistics. Fitting the best sparse vector\nto linear prediction is an NP-hard problem [Nat95]. However, this does not prohibit improper learning,\nsince we can use a ℓ1 convex relaxation for the sparse vectors (famously known as LASSO [Tib96]).\nUnsupervised learning, on the other hand, while extremely applicative and well-studied, has not\nseen such an inclusive theory. The most common approaches, such as restricted Boltzmann machines,\ntopic models, dictionary learning, principal component analysis and metric clustering, are based almost\nentirely on generative assumptions about the world. This is a strong restriction which makes it very hard\nto analyze such approaches in scenarios for which the assumptions do not hold. A more discriminative\napproach is based on compression, such as the Minimum Description Length criterion. This approach\ngives rise to provably intractable problems and doesn’t allow improper learning.\n1\nMain results.\nWe start by proposing a rigorous framework for unsupervised learning which allows\ndata-dependent, comparative learning without generative assumptions. It is general enough to encom-\npass previous methods such as PCA, dictionary learning and topic models. Our main contribution are\noptimization-based relaxations and efﬁcient algorithms that are shown to improperly probably learn\nprevious models, speciﬁcally:\n1. We consider the classes of hypothesis known as dictionary learning. We give a more general\nhypothesis class which encompasses and generalizes it according to our deﬁnitions. We proceed\nto give novel polynomial-time algorithms for learning the broader class. These algorithms are\nbased on new techniques in unsupervised learning, namely sum-of-squares convex relaxations.\nAs far as we know, this is the ﬁrst result for efﬁcient improper learning of dictionaries without\ngenerative assumptions. Moreover, our result handles polynomially over-complete dictionaries,\nwhile previous works [AGMM15, BKS15a] apply to at most constant factor over-completeness.\n2. We give efﬁcient algorithms for learning a new hypothesis class which we call spectral autoen-\ncoders. We show that this class generalizes, according to our deﬁnitions, the class of PCA (prin-\ncipal component analysis) and its kernel extensions.\nStructure of this paper.\nIn the following chapter we a non-generative, distribution-dependent deﬁ-\nnition for unsupervised learning which mirrors that of PAC learning for supervised learning. We then\nproceed to an illustrative example and show how Principal Component Analysis can be formally learned\nin this setting. The same section also gives a much more general class of hypothesis for unsupervised\nlearning which we call polynomial spectral decoding, and show how they can be efﬁcient learned in our\nframework using convex optimization. Finally, we get to our main contribution: a convex optimization\nbased methodology for improper learning a wide class of hypothesis, including dictionary learning.\n1.1\nPrevious work\nThe vast majority of work on unsupervised learning, both theoretical as well as applicative, focuses on\ngenerative models. These include topic models [BNJ03], dictionary learning [DH06], Deep Boltzmann\nMachines and deep belief networks [Sal09] and many more. Many times these models entail non-convex\noptimization problems that are provably NP-hard to solve in the worst-case.\nA recent line of work in theoretical machine learning attempts to give efﬁcient algorithms for these\nmodels with provable guarantees. Such algorithms were given for topic models [AGM12], dictionary\nlearning [AGM13, AGMM15], mixtures of gaussians and hidden Markov models [HK13, AGH+14] and\nmore. However, these works retain, and at times even enhance, the probabilistic generative assumptions\nof the underlying model. Perhaps the most widely used unsupervised learning methods are clustering\nalgorithms such as k-means, k-medians and principal component analysis (PCA), though these lack\ngeneralization guarantees. An axiomatic approach to clustering was initiated by Kleinberg [Kle03]\nand pursued further in [BDA09]. A discriminative generalization-based approach for clustering was\nundertaken in [BBV08] within the model of similarity-based clustering.\nAnother approach from the information theory literature studies with online lossless compression.\nThe relationship between compression and machine learning goes back to the Minimum Description\nLength criterion [Ris78]. More recent work in information theory gives online algorithms that attain\noptimal compression, mostly for ﬁnite alphabets [ADJ+13, OSZ04]. For inﬁnite alphabets, which are the\nmain object of study for unsupervised learning of signals such as images, there are known impossibility\nresults [JOS05]. This connection to compression was recently further advanced, mostly in the context\nof textual data [PWMH13].\nIn terms of lossy compression, Rate Distortion Theory (RDT) [Ber71, CT06] is intimately related\nto our deﬁnitions, as a framework for ﬁnding lossy compression with minimal distortion (which would\n2\ncorrespond to reconstruction error in our terminology). Our learnability deﬁnition can be seen of an\nextension of RDT to allow improper learning and generalization error bounds. Another learning frame-\nwork derived from lossy compression is the information bottleneck criterion [TPB00], and its learning\ntheoretic extensions [SST08]. The latter framework assumes an additional feedback signal, and thus is\nnot purely unsupervised.\nThe downside of the information-theoretic approaches is that worst-case competitive compression\nis provably computationally hard under cryptographic assumptions. In contrast, our compression-based\napproach is based on learning a restriction to a speciﬁc hypothesis class, much like PAC-learning. This\ncircumvents the impossibility results and allows for improper learning.\n2\nA formal framework for unsupervised learning\nThe basis constructs in an unsupervised learning setting are:\n1. Instance domain X, such as images, text documents, etc. Target space, or range, Y. We usually\nthink of X = Rd, Y = Rk with d ≫k. (Alternatively, Y can be all sparse vectors in a larger\nspace. )\n2. An unknown, arbitrary distribution D on domain X.\n3. A hypothesis class of decoding and encoding pairs,\nH ⊆{(h, g) ∈{X 7→Y} × {Y 7→X}},\nwhere h is the encoding hypothesis and g is the decoding hypothesis.\n4. A loss function ℓ: H × X 7→R⩾0 that measures the reconstruction error,\nℓ((g, h), x) .\nFor example, a natural choice is the ℓ2-loss ℓ((g, h), x) = ∥g(h(x))−x∥2\n2. The rationale here is to\nlearn structure without signiﬁcantly compromising supervised learning for arbitrary future tasks.\nNear-perfect reconstruction is sufﬁcient as formally proved in Appendix A.1. Without generative\nassumptions, it can be seen that near-perfect reconstruction is also necessary.\nFor convenience of notation, we use f as a shorthand for (h, g) ∈H, a member of the hypothesis\nclass H. Denote the generalization ability of an unsupervised learning algorithm with respect to a\ndistribution D as\nloss\nD (f) = E\nx∼D[ℓ(f, x)].\nWe can now deﬁne the main object of study: unsupervised learning with respect to a given hypothesis\nclass. The deﬁnition is parameterized by real numbers: the ﬁrst is the encoding length (measured in bits)\nof the hypothesis class. The second is the bias, or additional error compared to the best hypothesis. Both\nparameters are necessary to allow improper learning.\nDeﬁnition 2.1. We say that instance D, X is (k, γ)-C -learnable with respect to hypothesis class H if\nexists an algorithm that for every δ, ε > 0, after seeing m(ε, δ) = poly(1/ε, log(1/δ), d) examples,\nreturns an encoding and decoding pair (h, g) (not necessarily from H) such that:\n1. with probability at least 1 −δ, lossD((h, g)) ⩽min(h,g)∈H lossD((h, g)) + ε + γ.\n2. h(x) has an explicit representation with length at most k bits.\n3\nFor convenience we typically encode into real numbers instead of bits. Real encoding can often\n(though not in the worst case) be trivially transformed to be binary with a loss of logarithmic factor.\nFollowing PAC learning theory, we can use uniform convergence to bound the generalization error\nof the empirical risk minimizer (ERM). Deﬁne the empirical loss for a given sample S ∼Dm as\nloss\nS (f) = 1\nm ·\nX\nx∈S\nℓ(f, x)\nDeﬁne the ERM hypothesis for a given sample S ∼Dm as\nˆfERM = arg min\nˆf∈H\nloss\nS ( ˆf) .\nFor a hypothesis class H, a loss function ℓand a set of m samples S ∼Dm, deﬁne the empirical\nRademacher complexity of H with respect to ℓand S as, 1\nRS,ℓ(H) =\nE\nσ∼{±1}m\n\"\nsup\nf∈H\n1\nm\nX\nx∈S\nσiℓ(f, x)\n#\nLet the Rademacher complexity of H with respect to distribution D and loss ℓas Rm(H) =\nES∼Dm[RS,ℓ(H)]. When it’s clear from the context, we will omit the subscript ℓ.\nWe can now state and apply standard generalization error results. The proof of following theorem is\nalmost identical to [MRT12, Theorem 3.1]. For completeness we provide a proof in Appendix A.\nTheorem 2.1. For any δ > 0, with probability 1 −δ, the generalization error of the ERM hypothesis is\nbounded by:\nloss\nD ( ˆfERM) ⩽min\nf∈H loss\nD (f) + 6Rm(H) +\ns\n4 log 1\nδ\n2m\nAn immediate corollary of the theorem is that as long as the Rademacher complexity of a hypothesis\nclass approaches zero as the number of examples goes to inﬁnity, it can be C learned by an inefﬁcient\nalgorithm that optimizes over the hypothesis class by enumeration and outputs an best hypothesis with\nencoding length k and bias γ = 0. Not surprisingly such optimization is often intractable and hences\nthe main challenge is to design efﬁcient algorithms. As we will see in later sections, we often need to\ntrade the encoding length and bias slightly for computational efﬁciency.\nNotation.\nFor every vector z ∈Rd1 ⊗Rd2, we can view it as a matrix of dimension d1 × d2, which is\ndenoted as M(z). Therefore in this notation, M(u ⊗v) = uv⊤.\nLet vmax(·) : (Rd)⊗2 →Rd be the function that compute the top right-singular vector of some\nvector in (Rd)⊗2 viewed as a matrix. That is, for z ∈(Rd)⊗2, then vmax(z) denotes the top right-\nsingular vector of M(z). We also overload the notation vmax for generalized eigenvectors of higher\norder tensors. For T ∈(Rd)⊗ℓ, let vmax(T) = argmax∥x∥⩽1 T(x, x, . . . , x) where T(·) denotes the\nmulti-linear form deﬁned by tensor T.\nWe use ∥A∥ℓp→ℓq to denote the induced operator norm of A from ℓp space to ℓq space. For simplicity,\nwe also deﬁne |A|1 = ∥A∥ℓ∞→ℓ1 = P\nij |Aij|, |A|∞= ∥A∥ℓ1→ℓ∞= maxij |Aij|. We note that\n∥A∥ℓ1→ℓ1 is the max column ℓ1 norm of A, and ∥A∥ℓ1→ℓ2 is the largest column ℓ2 norm of A.\n1Technically, this is the Rademacher complexity of the class of functions ℓ◦H. However, since ℓis usually ﬁxed for certain\nproblem, we emphasize in the deﬁnition more the dependency on H.\n4\n3\nSpectral autoencoders: unsupervised learning of algebraic manifolds\n3.1\nAlgebraic manifolds\nThe goal of the spectral autoencoder hypothesis class we deﬁne henceforth is to learn the representation\nof data that lies on a low-dimensional algebraic variety/manifolds. The linear variety, or linear manifold,\ndeﬁned by the roots of linear equations, is simply a linear subspace. If the data resides in a linear\nsubspace, or close enough to it, then PCA is effective at learning its succinct representation.\nOne extension of the linear manifolds is the set of roots of low-degree polynomial equations. For-\nmally, let k, s be integers and let c1, . . . , cds−k ∈Rds be a set of vectors in ds dimension, and consider\nthe algebraic variety\nM =\nn\nx ∈Rd : ∀i ∈[ds −k], ⟨ci, x⊗s⟩= 0\no\n.\nObserve that here each constraint ⟨ci, x⊗s⟩is a degree-s polynomial over variables x, and when s = 1\nthe variety M becomes a liner subspace. Let a1, . . . , ak ∈Rds be a basis of the subspaces orthogonal\nto all of c1, . . . , cds−k, and let A ∈Rk×ds contains ai as rows. Then we have that given x ∈M, the\nencoding\ny = Ax⊗s\npins down all the unknown information regarding x. In fact, for any x ∈M, we have A⊤Ax⊗s = x⊗s\nand therefore x is decodable from y. The argument can also be extended to the situation when the data\npoint is close to M (according to a metric, as we discuss later). The goal of the rest of the subsections\nis to learn the encoding matrix A given data points residing close to M.\n3.2\nWarm up: PCA and kernel PCA\nIn this section we illustrate our framework for agnostic unsupervised learning by showing how PCA and\nkernel PCA can be efﬁciently learned within our model. The results of this sub-section are not new, and\ngiven only for illustrative purposes.\nThe class of hypothesis corresponding to PCA operates on domain X = Rd and range Y = Rk\nfor some k < d via linear operators. In kernel PCA, the encoding linear operator applies to the s-th\ntensor power x⊗s of the data. That is, the encoding and decoding are parameterized by a linear operator\nA ∈Rk×ds,\nHpca\nk,s =\nn\n(hA, gA) : hA(x) = Ax⊗s, , gA(y) = A†y\no\n,\nwhere A† denotes the pseudo-inverse of A. The natural loss function here is the Euclidean norm,\nℓ((g, h), x) = ∥x⊗s −g(h(x))∥2 = ∥(I −A†A)x⊗s∥2 .\nTheorem 3.1. For a ﬁxed constant s ⩾1, the class Hpca\nk,s is efﬁciently C -learnable with encoding length\nk and bias γ = 0.\nThe proof of the Theorem follows from two simple components: a) ﬁnding the ERM among Hpca\nk,s can\nbe efﬁciently solved by taking SVD of covariance matrix of the (lifted) data points. b) The Rademacher\ncomplexity of the hypothesis class is bounded by O(ds/m) for m examples. Thus by Theorem 2.1 the\nminimizer of ERM generalizes. The full proof is deferred to Appendix B.\n5\n3.3\nSpectral Autoencoders\nIn this section we give a much broader set of hypothesis, encompassing PCA and kernel-PCA, and\nshow how to learn them efﬁciently. Throughout this section we assume that the data is normalized to\nEuclidean norm 1, and consider the following class of hypothesis which naturally generalizes PCA:\nDeﬁnition 3.1 (Spectral autoencoder). We deﬁne the class Hsa\nk,s as the following set of all hypothesis\n(g, h),\nHsa\nk =\n\u001a\n(h, g) : h(x)\n= Ax⊗s, A ∈Rk×ds\ng(y)\n= vmax(By), B ∈Rds×k\n\u001b\n.\n(3.1)\nWe note that this notion is more general than kernel PCA: suppose some (g, h) ∈Hpca\nk,s has recon-\nstruction error ε, namely, A†Ax⊗s is ε-close to x⊗s in Euclidean norm. Then by eigenvector perturbation\ntheorem, we have that vmax(A†Ax⊗s) also reconstructs x with O(ε) error, and therefore there exists a\nPSCA hypothesis with O(ε) error as well . Vice versa, it’s quite possible that for every A, the recon-\nstruction A†Ax⊗s is far away from x⊗s so that kernel PCA doesn’t apply, but with spectral decoding we\ncan still reconstruct x from vmax(A†Ax⊗s) since the top eigenvector of A†Ax⊗s is close x.\nHere the key matter that distinguishes us from kernel PCA is in what metric x needs to be close\nto the manifold so that it can be reconstructed. Using PCA, the requirement is that x is in Euclidean\ndistance close to M (which is a subspace), and using kernel PCA x⊗2 needs to be in Euclidean distance\nclose to the null space of ci’s. However, Euclidean distances in the original space and lifted space\ntypically are meaningless for high-dimensional data since any two data points are far away with each\nother in Euclidean distance. The advantage of using spectral autoencoders is that in the lifted space the\ngeometry is measured by spectral norm distance that is much smaller than Euclidean distance (with a\npotential gap of d1/2). The key here is that though the dimension of lifted space is d2, the objects of our\ninterests is the set of rank-1 tensors of the form x⊗2. Therefore, spectral norm distance is a much more\neffective measure of closeness since it exploits the underlying structure of the lifted data points.\nWe note that spectral autoencoders relate to vanishing component analysis [LLS+13]. When the\ndata is close to an algebraic manifold, spectral autoencoders aim to ﬁnd the (small number of) essential\nnon-vanishing components in a noise robust manner.\n3.4\nLearnability of polynomial spectral decoding\nFor simplicity we focus on the case when s = 2. Ideally we would like to learn the best encoding-\ndecoding scheme for any data distribution D. Though there are technical difﬁculties to achieve such a\ngeneral result. A natural attempt would be to optimize the loss function f(A, B) = ∥g(h(x)) −x∥2 =\n∥x −vmax(BAx⊗2)∥2. Not surprisingly, function f is not a convex function with respect to A, B, and\nin fact it could be even non-continuous (if not ill-deﬁned)!\nHere we make a further realizability assumption that the data distribution D admits a reasonable\nencoding and decoding pair with reasonable reconstruction error.\nDeﬁnition 3.2. We say a data distribution D is (k, ε)-regularly spectral decodable if there exist A ∈\nRk×d2 and B ∈Rd2×k with ∥BA∥op ⩽τ such that for x ∼D, with probability 1, the encoding\ny = Ax⊗2 satisﬁes that\nM(By) = M(BAx⊗2) = xx⊤+ E ,\n(3.2)\nwhere ∥E∥op ⩽ε. Here τ ⩾1 is treated as a ﬁxed constant globally.\nTo interpret the deﬁnition, we observe that if data distribution D is (k, ε)-regularly spectrally decod-\nable, then by equation (3.2) and Wedin’s theorem (see e.g. [Vu11] ) on the robustness of eigenvector to\nperturbation, M(By) has top eigenvector2 that is O(ε)-close to x itself. Therefore, deﬁnition 3.2 is a\n2Or right singular vector when M(By) is not symmetric\n6\nsufﬁcient condition for the spectral decoding algorithm vmax(By) to return x approximately, though it\nmight be not necessary. Moreover, this condition partially addresses the non-continuity issue of using\nobjective f(A, B) = ∥x −vmax(BAx⊗2)∥2, while f(A, B) remains (highly) non-convex. We resolve\nthis issue by using a convex surrogate.\nOur main result concerning the learnability of the aforementioned hypothesis class is:\nTheorem 3.2. The hypothesis class Hsa\nk,2 is C - learnable with encoding length O(τ 4k4/δ4) and bias δ\nwith respect to (k, ε)-regular distributions in polynomial time.\nOur approach towards ﬁnding an encoding and decoding matrice A, B is to optimize the objective,\nminimize f(R) = E\nh\r\rRx⊗2 −x⊗2\r\r\nop\ni\n(3.3)\ns.t. ∥R∥S1 ⩽τk\nwhere ∥· ∥S1 denotes the Schatten 1-norm3.\nSuppose D is (k, ε)-regularly decodable, and suppose hA and gB are the corresponding encoding\nand decoding function. Then we see that R = AB will satisﬁes that R has rank at most k and f(R) ⩽ε.\nOn the other hand, suppose one obtains some R of rank k′ such that f(R) ⩽δ, then we can produce hA\nand gB with O(δ) reconstruction simply by choosing A ∈Rk′×d2B and B ∈Rd2×k′ such that R = AB.\nWe use (non-smooth) Frank-Wolfe to solve objective (3.3), which in particular returns a low-rank\nsolution. We defer the proof of Theorem 3.2 to the Appendix B.1.\nWith a slightly stronger assumptions on the data distribution D, we can reduce the length of the code\nto O(k2/ε2) from O(k4/ε4). See details in Appendix C.\n4\nA family of optimization encodings and efﬁcient dictionary learning\nIn this section we give efﬁcient algorithms for learning a family of unsupervised learning algorithms\ncommonly known as ”dictionary learning”. In contrast to previous approaches, we do not construct an\nactual ”dictionary”, but rather improperly learn a comparable encoding via convex relaxations.\nWe consider a different family of codes which is motivated by matrix-based unsupervised learning\nmodels such as topic-models, dictionary learning and PCA. This family is described by a matrix A ∈\nRd×r which has low complexity according to a certain norm ∥· ∥α, that is, ∥A∥α ⩽cα. We can\nparametrize a family of hypothesis H according to these matrices, and deﬁne an encoding-decoding pair\naccording to\nhA(x) = arg min\n∥y∥β⩽k\n1\nd |x −Ay|1 , gA(y) = Ay\nWe choose ℓ1 norm to measure the error mostly for convenience, though it can be quite ﬂexible. The\ndifferent norms ∥·∥α, ∥·∥β over A and y give rise to different learning models that have been considered\nbefore. For example, if these are Euclidean norms, then we get PCA. If ∥· ∥α is the max column ℓ2 or\nℓ∞norm and ∥· ∥b is the ℓ0 norm, then this corresponds to dictionary learning (more details in the next\nsection).\nThe optimal hypothesis in terms of reconstruction error is given by\nA⋆= arg min\n∥A∥α⩽cα\nE\nx∼D\n\u00141\nd |x −gA(hA(x))|1\n\u0015\n= arg min\n∥A∥α⩽cα\nE\nx∼D\n\u0014\nmin\ny∈Rr:∥y∥β⩽k\n1\nd |x −Ay|1\n\u0015\n.\nThe loss function can be generalized to other norms, e.g., squared ℓ2 loss, without any essential\nchange in the analysis. Notice that this optimization objective derived from reconstruction error is\n3Also known as nuclear norm or trace norm\n7\nidentically the one used in the literature of dictionary learning. This can be seen as another justiﬁcation\nfor the deﬁnition of unsupervised learning as minimizing reconstruction error subject to compression\nconstraints.\nThe optimization problem above is notoriously hard computationally, and signiﬁcant algorithmic\nand heuristic literature attempted to give efﬁcient algorithms under various distributional assump-\ntions(see [AGM13, AGMM15, AEB05] and the references therein). Our approach below circumvents\nthis computational hardness by convex relaxations that result in learning a different creature, albeit with\ncomparable compression and reconstruction objective.\n4.1\nImproper dictionary learning: overview\nWe assume the max column ℓ∞norm of A is at most 1 and the ℓ1 norm of y is assumed to be at\nmost k. This is a more general setting than the random dictionaries (up to a re-scaling) that previous\nworks [AGM13, AGMM15] studied. 4In this case, the magnitude of each entry of x is on the order of\n√\nk if y has k random ±1 entries. We think of our target error per entry as much smaller than 15. We\nconsider Hk\ndict that are parametrized by the dictionary matrix A = Rd×r,\nHdict\nk\n=\nn\n(hA, gA) : A ∈Rd×r, ∥A∥ℓ1→ℓ∞⩽1\no\n,\nwhere hA(x) = arg min\n∥y∥1⩽k\n|x −Ay|1 , gA(y) = Ay\nHere we allow r to be larger than d, the case that is often called over-complete dictionary. The choice\nof the loss can be replaced by ℓ2 loss (or other Lipschitz loss) without any additional efforts, though for\nsimplicity we stick to ℓ1 loss. Deﬁne A⋆to be the the best dictionary under the model and ε⋆to be the\noptimal error,\nA⋆= arg min∥A∥ℓ1→ℓ∞⩽1 Ex∼D\n\u0002\nminy∈Rr:∥y∥1⩽k |x −Ay|1\n\u0003\n(4.1)\nε⋆= Ex∼D\n\u0002 1\nd · |x −gA⋆(hA⋆(x))|1\n\u0003\n.\nAlgorithm 1 group encoding/decoding for improper dictionary learning\nInputs: N data points X ∈Rd×N ∼DN. Convex set Q. Sampling probability ρ.\n1. Group encoding: Compute\nZ = arg min\nC∈Q\n|X −C|1 ,\n(4.2)\nand let\nY = h(X) = PΩ(Z) ,\nwhere PΩ(B) is a random sampling of B where each entry is picked with probability ρ.\n2. Group decoding: Compute\ng(Y ) = arg min\nC∈Q\n|PΩ(C) −Y |1 .\n(4.3)\nTheorem 4.1. For any δ > 0, p ⩾1, the hypothesis class Hdict\nk\nis C -learnable (by Algorithm 2) with\nencoding length ˜O(k2r1/p/δ2), bias δ + O(ε⋆) and sample complexity dO(p) in time nO(p2)\n4The assumption can be relaxed to that A has ℓ∞norm at most k and ℓ2-norm at most\n√\nd straightforwardly.\n5We are conservative in the scaling of the error here. Error much smaller than\n√\nk is already meaningful.\n8\nWe note that here r can be potentially much larger than d since by choosing a large constant p the\noverhead caused by r can be negligible. Since the average size of the entries is\n√\nk, therefore we can get\nthe bias δ smaller than average size of the entries with code length roughly ≈k.\nThe proof of Theorem 4.1 is deferred to Section 5.6. To demonstrate the key intuition and tech-\nnique behind it, in the rest of the section we consider a simpler algorithm that achieves a weaker goal:\nAlgorithm 1 encodes multiple examples into some codes with the matching average encoding length\n˜O(k2r1/p/δ2), and these examples can be decoded from the codes together with reconstruction error\nε⋆+ δ. Next, we outline the analysis of Algorithm 1, and we will show later that one can reduce the\nproblem of encoding a single examples to the problem of encoding multiple examples together.\nHere we overload the notation gA⋆(hA⋆(·)) so that gA⋆(hA⋆(X)) denotes the collection of all the\ngA⋆(hA⋆(xj)) where xj is the j-th column of X. Algorithm 1 assumes that there exists a convex set\nQ ⊂Rd×N such that\nn\ngA⋆(hA⋆(X)) : X ∈Rd×No\n⊂{AY : ∥A∥ℓ1→ℓ∞⩽1, ∥Y ∥ℓ1→ℓ1 ⩽k} ⊂Q .\n(4.4)\nThat is, Q is a convex relaxation of the group of reconstructions allowed in the class Hdict. Algo-\nrithm 1 ﬁrst uses convex programming to denoise the data X into a clean version Z, which belongs to\nthe set Q. If the set Q has low complexity, then simple random sampling of Z ∈Q serves as a good\nencoding.\nThe following Lemma shows that if Q has low complexity in terms of sampling Rademacher width,\nthen Algorithm 1 will give a good group encoding and decoding scheme.\nLemma 4.2. Suppose convex Q ⊂Rd×N satisﬁes condition (4.4). Then, Algorithm 1 gives a group\nencoding and decoding pair such that with probability 1−δ, the average reconstruction error is bounded\nby ε⋆+ O(\np\nSRWm(Q) + O(\np\nlog(1/δ)/m) where m = ρNd and SRWm(·) is the sampling\nRademacher width (deﬁned in subsection 5.2), and the average encoding length is ˜O(ρd).\nThe proofs here are technically standard:\nLemma 4.2 simply follows from Lemma 5.1 and\nLemma 5.2 in Section 5. Lemma 5.1 shows that the difference between Z and X is comparable to\nε⋆, which is a direct consequence of the optimization over a large set Q that contains optimal recon-\nstruction. Lemma 5.2 shows that the sampling procedure doesn’t lose too much information given a\ndenoised version of the data is already observed, and therefore, one can reconstruct Z from Y .\nThe novelty here is to use these two steps together to denoise and achieve a short encoding. The\ntypical bottleneck of applying convex relaxation on matrix factorization based problem (or any other\nproblem) is the difﬁculty of rounding. Here instead of pursuing a rounding algorithm that output the\nfactor A and Y , we look for a convex relaxation that preserves the intrinsic complexity of the set which\nenables the trivial sampling encoding. It turns out that controlling the width/complexity of the convex\nrelaxation boils down to proving concentration inequalities with sum-of-squares (SoS) proofs, which is\nconceptually easier than rounding.\nTherefore, the remaining challenge is to design convex set Q that simultaneously has the following\nproperties\n(a) is a convex relaxation in the sense of satisfying condition (4.4)\n(b) admits an efﬁcient optimization algorithm\n(c) has low complexity (that is, sampling rademacher width ˜O(N poly(k)))\nMost of the proofs need to be deferred to Section 5. We give a brief overview: In subsection 5.3 we will\ndesign a convex set Q which satisﬁes condition (4.4) but not efﬁciently solvable, and in subsection 5.4\nwe verify that the sampling Rademacher width is O(Nk log d). In subsection 5.5, we prove that a sum-\nof-squares relaxation would give a set Qsos\np\nwhich satisﬁes (a), (b) and (c) approximately. Concretely,\nwe have the following theorem.\n9\nTheorem 4.3. For every p ⩾4, let N = dc0p with a sufﬁciently large absolute constant c0. Then, there\nexists a convex set Qsos\np\n⊂Rd×N (which is deﬁned in subsection 5.5.2) such that (a) it satisﬁes condi-\ntion 4.4; (b) The optimization (4.2) and (4.3) are solvable by semideﬁnite programming with run-time\nnO(p2); (c) the sampling Rademacher width of Qsos\np\nis bounded by\np\nSRWm(Q) ⩽˜O(k2r2/pN/m).\nWe note that these three properties (with Lemma 4.2) imply that Algorithm 1 with Q = Qsos\np\nand ρ = O(k2r2/pd−1/δ2 · log d) gives a group encoding-decoding pair with average encoding length\nO(k2r2/p/δ2 · log d) and bias δ.\nProof Overview of Theorem 4.3: At a very high level, the proof exploits the duality between sum-\nof-squares relaxation and sum-of-squares proof system. Suppose w1, . . . , wd are variables, then in SoS\nrelaxation an auxiliary variable WS is introduced for every subset S ⊂[d] of size at most s, and valid\nlinear constraints and psd constraint for WS’s are enforced. By convex duality, intuitively we have that if\na polynomial q(x) = P\n|S|⩽s αSxS can be written as a sum of squares of polynomial q(x) = P\nj rj(x)2,\nthen the corresponding linear form over XS, P\n|S|⩽s αSXS is also nonnegative. Therefore, to certify\ncertain property of a linear form P\n|S|⩽s αSXS over XS, it is sufﬁcient (and also necessary by duality)\nthat the corresponding polynomial admit a sum-of-squares proof.\nHere using the idea above, we ﬁrst prove the Rademacher width of the convex hull of reconstruc-\ntions Q0 = conv {Z = AY : ∥A∥ℓ1→ℓ∞⩽1, ∥Y ∥ℓ1→ℓ1 ⩽k} using a SoS proof. Then the same proof\nautomatically applies to for the Rademacher width of the convex relaxation (which is essentially a set of\nstatements about linear combinations of the auxiliary variables). We lose a factor of r2/p because SoS\nproof is not strong enough for us to establish the optimal Rademacher width of Q0.\n5\nAnalysis of Improper Dictionary Learning\nIn this section we give the full proof of the Theorems and Lemmas in Section 4. We start by stating\ngeneral results on denoising, Rademacher width and factorizable norms, and proceed to give specialized\nbounds for our setting in section 5.4.\n5.1\nGuarantees of denoising\nIn this subsection, we give the guarantees of the error caused by the denoising step. Recall that ε⋆is the\noptimal reconstruction error achievable by the optimal (proper) dictionary (equation (4.1)).\nLemma 5.1. Let Z be deﬁned in equation (4.2). Then we have that\n1\nNd\nE\nX∼DN [|Z −X|1] ⩽ε⋆\n(5.1)\nProof. Let Y ⋆= A⋆hA⋆(X) where hA⋆(X) denote the collection of encoding of X using hA⋆. Since\nY ⋆∈{AY : ∥A∥|ℓ1→ℓ∞⩽1, ∥Y ∥ℓ1→ℓ1 ⩽k} ⊂Q, we have that Y ⋆is a feasible solution of optimiza-\ntion (4.2). Therefore, we have that\n1\nNd E [|Z −X|1] ⩽\n1\nNd E [|X −Y ⋆|1] = ε⋆, where the equality is by\nthe deﬁnition of ε⋆.\n5.2\nSampling Rademacher Width of a Set\nAs long as the intrinsic complexity of the set Q is small then we can compress by random sampling.\nThe idea of viewing reconstruction error the test error of a supervised learning problem started with the\nwork of Srebro and Shraibman [SS05], and has been used for other completion problems, e.g., [BM15].\nWe use the terminology “Rademacher width” instead of “Rademacher complexity” to emphasize that\nthe notation deﬁned below is a property of a set of vectors (instead of that of a hypothesis class).\n10\nFor any set W ⊂RD, and an integer m, we deﬁne its sampling Rademacher width (SRW) as,\nSRWm(W) = E\nσ,Ω\n\u0014 1\nm sup\nx∈W\n⟨x, σ⟩Ω\n\u0015\n,\n(5.2)\nwhere Ωis random subset of [D] of size m, ⟨a, b⟩Ωis deﬁned as P\ni∈Ωaibi and σ ∼{±1}D.\nLemma 5.2. ([BM15, Theorem 2.4]) With probability at least 1−δ over the choice of Ω, for any ˜z ∈RD\n1\nD|˜z −z|1 ⩽1\nm|PΩ(˜z) −PΩ(z)|1 + 2SRWm(W) + M\nr\nlog(1/δ)\nm\n.\nwhere M = sup˜z∈W,i∈[D] |zi −˜zi|.\n5.3\nFactorable norms\nIn this subsection, we deﬁne in general the factorable norms, from which we obtain a convex set Q\nwhich satisﬁes condition (4.4) (see Lemma 5.3).\nFor any two norms ∥· ∥α, ∥· ∥β that are deﬁned on matrices of any dimension, we can deﬁne the\nfollowing quantity\nΓα,β(Z) =\ninf\nZ=AB ∥A∥α∥B∥β\n(5.3)\nFor any p, q, s, t ⩾1, we use Γp,q,s,t(·) to denote the function Γℓp→ℓq,ℓs→ℓt(·). When p = t,\nΓp,q,s,p(Z) is the factorable norm [TJ89, Chapter 13] of matrix Z. In the case when p = t = 2,\nq = ∞, s = 1, we have that Γ2,∞,1,2(·) is the γ2-norm [LMSS07] or max norm [SS05], which has been\nused for matrix completion.\nThe following Lemma is the main purpose of this section which shows a construction of a convex\nset Q that satisﬁes condition (4.4).\nLemma 5.3. For any q, t ⩾1 we have that Γ1,q,1,t(·) is a norm. As a consequence, letting Q1,∞,1,1 =\n{C ∈RN×d : Γ1,∞,1,1(C) ⩽\n√\ndk}, we have that Q1,∞,1,1 is a convex set and it satisﬁes condition (4.4).\nTowards proving Lemma 5.3, we prove a stronger result that if p = s = 1, then Γp,q,s,t is also a\nnorm. This result is parallel to [TJ89, Chapter 13] where the case of p = t is considered.\nTheorem 5.4. Suppose that ∥· ∥α and ∥· ∥β are norms deﬁned on matrices of any dimension such that\n1. ∥[A, B]∥α ⩽max{∥A∥α, ∥B∥α}\n2. ∥· ∥β is invariant with respect to appending a row with zero entries.\nThen, Γα,β(·) is a norm.\nProof. Non-negativity: We have that Γα,β(Z) ⩾0 by deﬁnition as a product of two norms. Further,\nΓα,β(Z) = 0 if and only if ∥A∥α = 0 or ∥B∥β = 0, which is true if and only if A or B are zero, which\nmeans that Z is zero.\nAbsolute scalability: For any positive t, we have that if Z = AB and Γα,β(Z) ⩽∥A∥α∥B∥β, then\ntZ = (tA) · B and Γα,β(tZ) ⩽t∥A∥α∥B∥β. Therefore by deﬁnition of Γα,β(Z), we get Γα,β(tZ) ⩽\ntΓα,β(Z).\nIf we replace Z by tZ and t by 1/t we obtain the other direction, namely Γα,β(Z) ⩽1/t · Γα,β(tZ).\nTherefore, Γα,β(tZ) = tΓα,β(Z) for any t ⩾0.\nTriangle inequality: We next show that Γα,β(Z) satisﬁes triangle inequality, from which the result\nfollows. Let W and Z be two matrices of the same dimension. Suppose A, C satisfy that Z = AC and\n11\nΓα,β(Z) = ∥A∥α∥C∥β. Similarly, suppose W = BD, and Γα,β(W) = ∥B∥α∥D∥β. Therefore, we\nhave that W + Z = [tA, B]\n\u0014t−1C\nD\n\u0015\n, and that for any t > 0,\nΓα,β(W + Z) ⩽∥[tA, B]∥α\n\r\r\r\r\n\u0014t−1C\nD\n\u0015\r\r\r\r\nβ\n(by deﬁntion of Γα,β)\n⩽∥[tA, B]∥α\n \r\r\r\r\n\u0014 0\nD\n\u0015\r\r\r\r\nβ\n+ t−1\n\r\r\r\r\n\u0014C\n0\n\u0015\r\r\r\r\nβ\n!\n(by triangle inquality)\n⩽max {t ∥A∥α , ∥B∥α}\n\u0010\nt−1 ∥C∥β + ∥D∥β\n\u0011\n(by assumptions on ∥· ∥α and ∥· ∥β)\nPick t = ∥B∥α\n∥A∥α , we obtain that,\nΓα,β(W + Z) ⩽∥A∥α ∥C∥β + ∥B∥α ∥D∥β\n= Γα,β(Z) + Γα,β(W) .\nNote that if ∥· ∥α is a ℓ1 →ℓq norm, then it’s also the max column-wise ℓq norm, and therefore it\nsatisﬁes the condition a) in Theorem 5.4. Moreover, for similar reason, ∥· ∥β = ∥· ∥ℓ1→ℓt satisﬁes the\ncondition b) in Theorem 5.4. Hence, Lemma 5.3 is a direct consequence of Theorem 5.4. Lemma 5.3\ngives a convex set Q that can be potentially used in Algorithm 1.\n5.4\nSampling Rademacher width of level set of Γ1,∞,1,1\nHere we give a Rademacher width bound for the speciﬁc set we’re interested in, namely the level sets of\nΓ1,∞,1,1, formally,\nQ1,∞,1,1 = {C ∈RN×d : Γ1,∞,1,1(C) ⩽k}.\nBy the deﬁnition Q1,∞,1,1 satisﬁes condition (4.4). See section 5.2 for deﬁnition of Ramemacher\nwidth.\nLemma 5.5. It holds that\nSRWm(Q1,∞,1,1) ⩽eO\n r\nk2N\nm\n!\n.\nProof of Lemma 5.5. Recall the deﬁnition of the sample set Ωof coordinates from C, and their multi-\nplication by i.i.d Rademacher variables in section 5.2. Reusing notation, let ξ = σ ⊙Ωand we use Q as\na shorthand for Q1,∞,1,1. Here ⊙means the entry-wise Hadamard product (namely, each coordinate in\nΩis multiplied by an independent Rademacher random variable). We have that\nSRWm(Q) = E\nξ\n\"\n1\nm sup\nC∈Q\n⟨C, ξ⟩\n#\n= E\nξ\n\"\n1\nm\nsup\n∥A∥ℓ1→ℓ∞⩽1,∥B∥ℓ1→ℓ1⩽k\n⟨AB, ξ⟩\n#\n(by deﬁntiion of Q)\n= E\nξ\n\"\n1\nm\nsup\n∥A∥ℓ1→ℓ∞⩽1,∥B∥ℓ1→ℓ1⩽k\n⟨B, A⊤ξ⟩\n#\n⩽E\nξ\n\"\n1\nm\nsup\n∥A∥ℓ1→ℓ∞⩽1\nk\nN\nX\ni=1\n∥A⊤ξi∥∞\n#\n.\n(By ⟨U, V ⟩⩽\n\u0010PN\ni=1 ∥Ui∥∞\n\u0011\n∥V ∥ℓ1→ℓ1)\n12\nLet ρ =\nm\ndN be the probability of any entry belongs to Ω. Let ξi denote the i-th column of ξ, and Aj\ndenotes the j-th column of A. Therefore, each entry of ξi has probability ρ/2 to be +1 and -1, and has\nprobability 1 −ρ of being 0. By concentration inequality we have that for ρ ⩾log r\nd , and any ﬁxed A\nwith ∥A∥ℓ1→ℓ∞= max ∥Aj∥∞⩽1,\nE\nξi\n[∥A⊤ξi∥∞] ⩽O(\np\nρd log r log d) .\n(5.4)\nMoreover, we have that\nVarξi[∥A⊤ξi∥∞] ⩽O(\np\nρd log r log d) .\n(5.5)\nMoreover, ∥A⊤ξi∥∞has an sub-exponential tail.\n(Technically, its ψ1-Orlicz norm is bounded by\nO(√ρd log r log d)). Note that the variance of PN\ni=1 ∥A⊤ξi∥∞will decrease as N increases, and there-\nfore for large enough N = (drρ)Ω(1), we will have that with probability 1 −exp(−Ω(dr)Ω(1)),\nN\nX\ni=1\n∥A⊤ξi∥∞⩽O(N\np\nρd log r log d)\nTherefore, using the standard ε-net covering argument, we obtain that with high probability,\nsup\n∥A∥ℓ1→ℓ2⩽\n√\nd\nN\nX\ni=1\n∥A⊤ξi∥∞⩽O(N\np\nρd log r log d) .\nHence, altogether we have\nSRWm(Q) ⩽E\nξ\n\n1\nm\nsup\n∥A∥ℓ1→ℓ2⩽\n√\nd\nk∥A⊤ξ∥ℓ1→ℓ∞\n\n⩽eO\n r\nk2N\nm\n!\n.\n5.5\nConvex Relaxation for Γ1,∞,1,1 norm\n5.5.1\nSum-of-squares relaxation\nHere we will only brieﬂy introduce the basic ideas of Sum-of-Squares (Lasserre) relaxation [Par00,\nLas01] that will be used for this paper. We refer readers to the extensive study [Las15, Lau09, BS14] for\ndetailed discussions of sum of squares proofs and their applications to algorithm design. Recently, there\nhas been a popular line of research on applications of sum-of-squares algorithms to machine learning\nproblems [BKS15b, BKS14, BM16, MW15, GM15, HSS15, HSSS16, MSS16]. Here our technique in\nthe next subsection is most related to that of [BM16], with the main difference that we deal with ℓ1 norm\nconstraints that are not typically within the SoS framework.\nLet R[x]d denote the set of all real polynomials of degree at most d with n variables x1, . . . , xn. We\nstart by deﬁning the notion of pseudo-expectation. The intuition is that the pseudo-expectation behave\nlike the actual expectation of a real probability distribution on squares of polynomials.\nDeﬁnition 5.1 (pseudo-expectation). A degree-d pseudo-expectation ˜E is a linear operator that maps\nR[x]d to R and satisﬁes ˜E(1) = 1 and ˜E(p2(x)) ⩾0 for all real polynomials p(x) of degree at most d/2.\nDeﬁnition 5.2. Given a set of polynomial equations A = {q1(x) = 0, . . . , qn(x) = 0}, we say degree-\nd pseudo-expectation ˜E satisﬁes constraints A if ˜E [qi(x)r(x)] = 0 for every i and r(x) such that\ndeg(qir) ⩽d.\n13\nOne can optimize over the set of pseudo-expectations that satisfy A in nO(d) time by the following\nsemideﬁnite program:\nVariables\n˜E[xS]\n∀S : |S| ⩽d\nSubject to\n˜E\n\u0002\nqi(x)xK\u0003\n= 0\n∀i, K : |K| + deg(qi) ⩽d\n˜E\nh\nx⊗d/2(x⊗d/2)⊤i\n⪰0\nDeﬁnition 5.3 (SoS proof of degree d). For a set of constraints A = {q1(x) = 0, . . . , qn(x) = 0}, and\nan integer d, we write\nA ⊢d p(x) ⩾q(x)\nif there exists polynomials hi(x) for i = 0, 1, . . . , ℓand gj(x) for j = 1, . . . , t such that deg(hi) ⩽d/2\nand deg(gjrj) ⩽d that satisfy\np(x) −q(x) =\nℓ\nX\ni=1\nhi(x)2 +\nt\nX\nj=1\nrj(x)gj(x) .\nWe will drop the subscript d when it is clear form the context.\nThe following fact follows from the deﬁnitions above but will be useful throughout the proof.\nProposition 5.6. Suppose A ⊢d p(x) ⩾q(x). Then for any degree-d pseudo-expectation ˜E that satisﬁes\nA, we have ˜E[p(x)] ⩾˜E[q(x)].\n5.5.2\nRelaxation for Γ1,∞,1,1 norm\nIn this section, we introduce convex relaxations for the Γ1,∞,1,1 norm. For convenience, let p be a power\nof 2. Let A and B be formal variables of dimension d × r and r × N in this section. We introduce\nmore formal variables for the relaxation. Let b be formal variables of dimension r × N. We consider the\nfollowing set of polynomial constraints over formal variables A, B, b:\nA =\n(\n∀i, j, Bij = bp−1\nij\n,\nr\nX\nℓ=1\nbp\nℓj ⩽kp/(p−1), ∀i, k, A2\nik ⩽1\n)\n.\nFor any real matrix C ∈Rd×N, we deﬁne\nA(C) = {C = AB} ∪A .\nWe deﬁne our convex relaxation for Q1,∞,1,1 as follows,\nQsos\np\n= {C ∈Rd×N : ∃degree-O(p2) pseudo-expectation ˜E that satisﬁes A(C)}\n(5.6)\nLemma 5.7. For any p ⩾4, we have\nQ1,∞,1,1 ⊂Qsos\np\nand therefore Qsos\np\nsatisﬁes condition (4.4).\nProof. Suppose C ∈Q1,∞,1,1. Then by deﬁnition of the Γ1,∞,1,1-norm (equation (5.3)), we have\nthat there exists matrices U, V such that C = UV and U 2\nij ⩽1 and ∥V ∥ℓ1→ℓ1 ⩽k. Now construct\nvij = V 1/(p−1)\nij\n. We have Pr\nℓ=1 bp\nij ⩽\n\u0010Pr\nℓ=1 vp−1\nij\n\u0011p/(p−1)\n⩽kp/(p−1). Therefore, Then we have\nthat A = U, B = V, b = v satisﬁes the constraint (5.6). Then the trivial pseudo-expectation operator\n˜E[p(A, B, b)] = p(U, V, v) satisﬁes A(C) and therefore C ∈Qsos\np\nby deﬁnition.\n14\nTheorem 5.8. Suppose N = dc0p for large enough absolute constant c0. Then the sampling Rademacher\ncomplexity of Qsos\np\nis bounded by,\nSRWm(Qsos\np ) ⩽O\n r\np2Nk2r2/p log d\nm\n!\n.\nThe proof of Theorem 5.8 ﬁnally boils down to prove certain empirical process statement with SoS\nproofs. We start with the following two lemmas.\nLemma 5.9. Suppose N = dc0p for larger enough constant c0, and let ξ = σ ⊙Ωwhere σ and Ωare\ndeﬁned in Section 5.2. Then, we have\nA ⊢⟨AB, ξ⟩p ⩽N p−1kp\nN\nX\ni=1\n∥A⊤ξi∥p\np\nProof. Let ξi be the i-th column of ξ. We have that\nA ⊢⟨AB, ξ⟩p = ⟨A⊤ξ, B⟩p =\n N\nX\ni=1\n⟨A⊤ξ, Bi⟩\n!p\n⩽N p−1\n N\nX\ni=1\n⟨A⊤ξi, Bi⟩p\n!\n(since ⊢\n\u0000 1\nN\nP\ni∈N αi\n\u0001p ⩽1\nN\nP\ni∈[N] αp\ni )\n= N p−1\nN\nX\ni=1\n⟨A⊤ξi, b⊙p−1\ni\n⟩p\n(by Bij = bp−1\nij\n)\n⩽N p−1\nN\nX\ni=1\n∥A⊤ξi∥p\np∥bi∥(p−1)p\np\n(by ⊢\n\u0010P\nj aibp−1\nj\n\u0011p\n⩽(P\ni ap\ni ) (P bp\ni )p−1; see Lemma D.1)\n⩽N p−1kp\nN\nX\ni=1\n∥A⊤ξi∥p\np\n(by constraint Pr\nℓ=1 bp\nℓj ⩽kp/(p−1))\nLemma 5.10. In the setting of Lemma 5.9, let ρ = m/(Nd). Let x = (x1, . . . , xd) be indeterminates\nand let B = {x2\n1 ⩽1, . . . , x2\nd ⩽1}. Suppose ρ ⩾1/d. With probability at least 1 −exp(−Ω(d)) over\nthe choice of ξ, there exists a SoS proof,\nB ⊢∥ξ⊤x∥p\np ⩽N · O(ρdp2)p/2.\n(5.7)\nAs an immediate consequence, let ξi be the i-th column of ξ, then, with probability at least 1 −\nexp(−Ω(d)) over the choice of ξ there exists SoS proof,\nA ⊢\nN\nX\ni=1\n∥A⊤ξi∥p\np ⩽Nr · O(ρdp2)p/2 .\n(5.8)\nProof. Recall that p is an even number. We have that\nB ⊢∥ξ⊤x∥p\np =\n\u0010\nx⊗p/2\u0011⊤\n N\nX\ni=1\nξ⊗p/2\ni\n\u0010\nξ⊗p/2\ni\n\u0011⊤\n!\nx⊗p/2\n15\nLet\nT\n=\nPN\ni=1 ξ⊗p/2\ni\n\u0010\nξ⊗p/2\ni\n\u0011⊤\n∈\nRdp/2×dp/2.\nThen,\nby\ndeﬁnition\nwe\nhave\nthat\nE\nh\u0000x⊗p/2\u0001⊤Tx⊗p/2i\n= E\nhPN\ni=1⟨ξi, x⟩pi\n. It follows that\nB ⊢∥ξ⊤x∥p\np =\n\u0010\nx⊗p/2\u0011⊤\n(T −E[T])x⊗p/2 + E\n\" N\nX\ni=1\n⟨ξi, x⟩p\n#\n⩽∥x∥p ∥T −E[T]∥+ E\n\" N\nX\ni=1\n⟨ξi, x⟩p\n#\n(by ⊢y⊤By ⩽∥y∥2∥B∥)\n⩽dp/2 ∥T −E[T]∥+ E\n\" N\nX\ni=1\n⟨ξi, x⟩p\n#\n.\nLet ζ have the same distribution as ξi. Then we have that\nE [⟨ζ, x⟩p] =\nX\nα\ntαxα .\nwhere tα = 0 if xα contains an odd degree. Moreover, let |α| be the number xi with non-zero exponents\nthat xα has. Then we have tα ⩽ρ|α|pp. Therefore we have that for ρ ⩾1/d,\nB ⊢E [⟨ζ, x⟩p] ⩽dp/2ρp/2pp .\nIt follows that\nB ⊢E\n\" N\nX\ni=1\n⟨ξi, x⟩p\n#\n⩽N · O(ρdp2)p/2 .\nNext, we use Bernstein inequality to bound ∥T −E[T]∥. We control the variance by\nσ2 ≜\n\r\r\r\r\rE\n\" N\nX\ni=1\n∥ξ∥pξ⊗p/2\ni\n\u0010\nξ⊗p/2\ni\n\u0011⊤\n#\r\r\r\r\r ⩽Nd2p.\nMoreover, each summand in the deﬁnition of T can be bounded by\n\r\r\r\rξ⊗p/2\ni\n\u0010\nξ⊗p/2\ni\n\u0011⊤\r\r\r\r ⩽dp. Note that\nthese bounds are not necessarily tight since they already sufﬁce for our purpose. Therefore, we obtain\nthat with high probability over the chance of ξ, it holds that ∥T −E[T]∥⩽\np\nNdO(p) log N. Therefore\nfor N ⩾dΩ(p), we have that with high probability over the choice of ξ,\nB ⊢∥ξ⊤x∥p\np\n(5.9)\n⩽dp/2 ∥T −E[T]∥+ E\n\" N\nX\ni=1\n⟨ξi, x⟩p\n#\n⩽N · O(ρdp2)p/2 .\nFinally we show equation (5.7) implies equation (5.8). Let Ai be the i-th column of A. Note that we\nhave PN\ni=1 ∥A⊤ξi∥p\np = Pr\ni=1 ∥ξ⊤Ai∥p\np, since both left-hand side and right-hand side are equal to the\np-th power of all the entries of the matrix A⊤ξ. Since A ⊢{∀j, A2\nij ⩽1}, we can invoke the ﬁrst part\nof the Lemma and use equation (5.7) to complete the proof.\nCombining Lemma 5.9 and Lemma 5.10 we obtain the following corollary,\nCorollary 5.11. In the setting of Lemma 5.9, with probability 1 −exp(−Ω(d)), we have\nA ⊢⟨AB, ξ⟩p ⩽N pkpr · O(ρd)p/2 .\n(5.10)\n16\nNow we are ready to prove Theorem 5.8. Essentially the only thing that we need to do is take\npseudo-expectation on the both sides of equation (5.10).\nProof of Theorem 5.8. Recall that ξ = σ ⊙Ω. We have that\nSRWm(Qsos\np )p = E\nξ\n\"\n1\nmp\nsup\nC∈Qsos\nd\n⟨C, ξ⟩\n#p\n⩽E\nξ\n\"\n1\nmp\nsup\nC∈Qsos\nd\n⟨C, ξ⟩p\n#\n(by Jensen’s inequality)\n= E\nξ\n\"\n1\nmp\nsup\n˜E that satisﬁes A(C)\n˜E [⟨AB, ξ⟩]\n#\n(by deﬁnition of Qsos\np )\n⩽E\n\"\n1\nmp\nsup\n˜E that satisﬁes A(C)\n˜E [⟨AB, ξ⟩] | equation (5.8) holds\n#\n+ P [equation (5.8) doesn’t hold] · dO(1)\n⩽1\nmpN pkpr · O(ρdp2)p/2 + exp(−Ω(d))dO(1)\n(by Corollary 5.11 and Proposition 5.6)\n⩽1\nmpN pkpr · O(ρdp2)p/2 .\nTherefore using the fact that ρ = m/(Nd), taking p-th root on the equation above, we obtain,\nSRWm(Qsos\np ) ⩽O\n r\np2Nr2/pk2 log d\nm\n!\n.\n5.6\nProof of Theorem 4.1\nIn this subsection we prove Theorem 4.1 using the machinery developed in previous subsections. Es-\nsentially the idea here is to reduce the problem of encoding one example to the problem of encoding a\ngroup of examples. To encode an example x, we draw N −1 fresh examples x1, . . . , xN−1, and then\ncall the group encoding Algorithm 1.\nWe also note that the encoding procedure can be simpliﬁed by removing the denoising step, and this\nstill allows efﬁcient decoding steps. Since in this case the encoding contains more noise from the data,\nwe prefer the current scheme.\nProof of Theorem 4.1. We will prove that Algorithm 2 with Q = Qp\nsos and ρ = O(k2r2/pd−1/δ2 ·\nlog d) gives the desired encoding/decoding.\nIndeed with high probability, the encoding length is\n˜O(ρd) = ˜O(k2r2/p/δ2). Next we will show that we obtain good reconstruction in the sense that\n1\nd E [|x −g(h(x))|1] =\n1\nd E [|x −˜z|1] ⩽O(ε⋆+ δ). Here and throughout the proof, the expecta-\ntion is over the randomness of x and the randomness of the algorithm (that is, the randomness of\nx, x1, . . . , xN−1 and the random sampling). First of all, by symmetry, we have that\nE [|x −˜z1|1] =\n1\nNd E [|[x1, . . . , xN−1, x] −[˜z1, . . . , ˜zN−1, ˜z]|1] .\n(5.12)\nLet X = [x1, . . . , xN−1, x], and ˜Z = [˜z1, . . . , ˜zN−1, ˜z] for simplicity of notations. Let [Z1,i, zi] =\narg min[C,c]∈Q |[X1,i, xi] −[C, c]|1 where X1,i be the samples drawn in the encoding process for xi.\n17\nAlgorithm 2 Encoding/decoding for improper dictionary learning\nGiven: Integer N. Convex set Q ⊂Rd×N. Sampling probability ρ.\n1. Encoding: input: data point x ∈Rd ∼D, output: codeword y = h(x) ∈Rk×N Draw N −1\ndata points X1 ∈Rd×N−1 ∼DN−1.\nDenoising step:\n[Z1, z] = arg min\n[C,c]∈Q\n|[X1, x] −[C, c]|1 ,\n(5.11)\nRandom sampling step:\nh(x) = PΩ(z) ,\nwhere PΩ(z) is a random sampling of z where each entry is picked with probability ρ.\n2. Decoding: input: codeword y ∈Rk×N output: reconstruction g(y) ∈Rd×N\nTake N more samples x1, . . . , xN. Encode them to y1 = h(x1), . . . , yN = h(xN). Then, compute\n[˜z1, . . . , ˜zN−1, ˜z] = arg min\nC∈Q\n|PΩ([C, c]) −[y1, . . . , yN, y]|1 .\nReturn g(y) = ˜z.\nLet Z = [z1, . . . , zN−1, z]. By Lemma 5.1 and the symmetry, we have that for any i, 1\nd E [|zi −xi|1] ⩽\n1\ndN E\n\u0002\n|[Z1,i, zi] −[X1,i, xi]|1\n\u0003\n⩽ε⋆. Thus, we have\n1\ndN E [|X −Z|1] ⩽ε⋆.\nLet ˜Z ∈Rd×N be a helper matrix in the analysis deﬁned as\nˆZ = arg min\nC∈Q\n|X −C|1 .\n(5.13)\nThen by Lemma 5.1 we have that\n1\nNd E\nh\f\f\f ˆZ −X\n\f\f\f\n1\ni\n⩽ε⋆. Then by triangle inequality we obtain that\n1\nNd E\nh\f\f\f ˆZ −Z\n\f\f\f\n1\ni\n⩽2ε⋆.\n(5.14)\nNote that by deﬁnition ˆZ ∈Q. Therefore, by Lemma 5.2, we have that\n1\nNd E\nh\f\f\f ˜Z −Z\n\f\f\f\n1\ni\n⩽1\nm E\nh\f\f\fPΩ( ˜Z) −PΩ(Z)\n\f\f\f\n1\ni\n+ 2SRWm(W)\n(by Lemma 5.2)\n⩽1\nm E\nh\f\f\fPΩ( ˆZ) −PΩ(Z)\n\f\f\f\n1\ni\n+ 2SRWm(W)\n(since ˜Z is the minimizer, and ˆZ ∈Q.)\n=\n1\nNd E\nh\f\f\f ˆZ −Z\n\f\f\f\n1\ni\n+ 2SRWm(W)\n⩽ε⋆+ δ .\n(using equation (5.14) and Theorem 4.3 (c))\nFinally by triangle inequality again we have\n1\nNd E\nh\f\f\f ˜Z −X\n\f\f\f\n1\ni\n⩽\n1\nNd E\nh\f\f\f ˜Z −Z\n\f\f\f\n1\ni\n+ 1\nNd E [|X −Z|1] ⩽ε⋆.\n(5.15)\nCombining equation (5.12) and (5.15) we complete the proof.\n18\n6\nConclusions\nWe have deﬁned a new framework for unsupervised learning which replaces generative assumptions by\nnotions of reconstruction error and encoding length. This framework is comparative, and allows learning\nof particular hypothesis classes with respect to an unknown distribution by other hypothesis classes.\nWe demonstrate its usefulness by giving new polynomial time algorithms for two unsupervised\nhypothesis classes. First, we give new polynomial time algorithms for dictionary models in signiﬁcantly\nbroader range of parameters and assumptions. Another domain is the class of spectral encodings, for\nwhich we consider a new class of models that is shown to strictly encompass PCA and kernel-PCA.\nThis new class is capable, in contrast to previous spectral models, learn algebraic manifolds. We give\nefﬁcient learning algorithms for this class based on convex relaxations.\nAcknowledgements\nWe thank Sanjeev Arora for many illuminating discussions and crucial observations in earlier phases\nof this work, amongst them that a representation which preserves information for all classiﬁers requires\nlossless compression.\nReferences\n[ADJ+13]\nJayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha\nSuresh. Tight bounds for universal compression of large alphabets. In Proceedings of the\n2013 IEEE International Symposium on Information Theory, Istanbul, Turkey, July 7-12,\n2013, pages 2875–2879, 2013.\n[AEB05]\nMichal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: Design of dictionaries for\nsparse representation. In IN: PROCEEDINGS OF SPARS05, pages 9–12, 2005.\n[AGH+14]\nAnimashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgar-\nsky. Tensor decompositions for learning latent variable models. J. Mach. Learn. Res.,\n15(1):2773–2832, January 2014.\n[AGM12]\nSanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models–going beyond svd. In\nFoundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages\n1–10. IEEE, 2012.\n[AGM13]\nSanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and\novercomplete dictionaries. arXiv preprint arXiv:1308.6273, 2013.\n[AGMM15] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efﬁcient, and neural\nalgorithms for sparse coding. In Proceedings of The 28th Conference on Learning Theory,\nCOLT 2015, Paris, France, July 3-6, 2015, pages 113–149, 2015.\n[BBV08]\nMaria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative framework for\nclustering via similarity functions. In Proceedings of the Fortieth Annual ACM Symposium\non Theory of Computing, STOC ’08, pages 671–680, 2008.\n[BDA09]\nShai Ben-David and Margareta Ackerman. Measures of clustering quality: A working\nset of axioms for clustering.\nIn D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou,\neditors, Advances in Neural Information Processing Systems 21, pages 121–128. Curran\nAssociates, Inc., 2009.\n19\n[Ber71]\nToby Berger. Rate distortion theory: A mathematical basis for data compression. 1971.\n[BKS14]\nBoaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations.\nIn STOC, pages 31–40, 2014.\n[BKS15a]\nBoaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor de-\ncomposition via the sum-of-squares method. In Proceedings of the Forty-Seventh Annual\nACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17,\n2015, pages 143–151, 2015.\n[BKS15b]\nBoaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor de-\ncomposition via the sum-of-squares method. In Proceedings of the Forty-seventh Annual\nACM Symposium on Theory of Computing, STOC ’15, 2015.\n[BM15]\nBoaz Barak and Ankur Moitra. Tensor prediction, rademacher complexity and random\n3-xor. CoRR, abs/1501.06521, 2015.\n[BM16]\nBoaz Barak and Ankur Moitra. Noisy tensor completion via the sum-of-squares hierarchy.\nIn Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA,\nJune 23-26, 2016, pages 417–445, 2016.\n[BNJ03]\nDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach.\nLearn. Res., 3:993–1022, March 2003.\n[BS14]\nBoaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal al-\ngorithms. In Proceedings of International Congress of Mathematicians (ICM), 2014. To\nappear.\n[CT06]\nThomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in\nTelecommunications and Signal Processing). Wiley-Interscience, 2006.\n[DH06]\nD. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE\nTrans. Inf. Theor., 47(7):2845–2862, September 2006.\n[GM15]\nRong Ge and Tengyu Ma. Decomposing overcomplete 3rd order tensors using sum-of-\nsquares algorithms. In Approximation, Randomization, and Combinatorial Optimization.\nAlgorithms and Techniques, APPROX/RANDOM 2015, August 24-26, 2015, Princeton, NJ,\nUSA, pages 829–849, 2015.\n[HK12]\nElad Hazan and Satyen Kale. Projection-free online learning. In Proceedings of the 29th\nInternational Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK,\nJune 26 - July 1, 2012, 2012.\n[HK13]\nDaniel Hsu and Sham M Kakade.\nLearning mixtures of spherical gaussians: moment\nmethods and spectral decompositions. In Proceedings of the 4th conference on Innovations\nin Theoretical Computer Science, pages 11–20. ACM, 2013.\n[HSS15]\nSamuel B. Hopkins, Jonathan Shi, and David Steurer. Tensor principal component analysis\nvia sum-of-square proofs. In Proceedings of The 28th Conference on Learning Theory,\nCOLT 2015, Paris, France, July 3-6, 2015, pages 956–1006, 2015.\n[HSSS16]\nSamuel B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer. Fast spectral algo-\nrithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors. In\nProceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC\n2016, Cambridge, MA, USA, June 18-21, 2016, pages 178–191, 2016.\n20\n[JOS05]\nNikola Jevtic, Alon Orlitsky, and Narayana P. Santhanam. A lower bound on compression\nof unknown alphabets. Theor. Comput. Sci., 332(1-3):293–311, 2005.\n[Kle03]\nJon M. Kleinberg. An impossibility theorem for clustering. In S. Becker, S. Thrun, and\nK. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 463–\n470. MIT Press, 2003.\n[Las01]\nJean B. Lasserre. Global optimization with polynomials and the problem of moments.\nSIAM Journal on Optimization, 11(3):796–817, 2001.\n[Las15]\nJean Bernard Lasserre. An introduction to polynomial and semi-algebraic optimization.\nCambridge Texts in Applied Mathematics. Cambridge: Cambridge University Press. ,\n2015.\n[Lau09]\nMonique Laurent. Sums of squares, moment matrices and optimization over polynomials.\nIn Mihai Putinar and Seth Sullivant, editors, Emerging Applications of Algebraic Geometry,\nvolume 149 of The IMA Volumes in Mathematics and its Applications, pages 157–270.\nSpringer New York, 2009.\n[LLS+13]\nRoi Livni, David Lehavi, Sagi Schein, Hila Nachlieli, Shai Shalev-Shwartz, and Amir\nGloberson. Vanishing component analysis. In Proceedings of the 30th International Con-\nference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages\n597–605, 2013.\n[LMSS07]\nNati Linial, Shahar Mendelson, Gideon Schechtman, and Adi Shraibman.\nComplexity\nmeasures of sign matrices. Combinatorica, 27(4):439–463, 2007.\n[MRT12]\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine\nlearning. MIT press, 2012.\n[MSS16]\nTengyu Ma, Jonathan Shi, and David Steurer. Polynomial-time tensor decompositions with\nsum-of-squares. In FOCS 2016, to appear, 2016.\n[MW15]\nTengyu Ma and Avi Wigderson. Sum-of-squares lower bounds for sparse PCA. In Ad-\nvances in Neural Information Processing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada,\npages 1612–1620, 2015.\n[Nat95]\nB. K. Natarajan.\nSparse approximate solutions to linear systems.\nSIAM J. Comput.,\n24(2):227–234, 1995.\n[OSZ04]\nAlon Orlitsky, Narayana P. Santhanam, and Junan Zhang. Universal compression of mem-\noryless sources over unknown alphabets. IEEE Trans. Information Theory, 50(7):1469–\n1481, 2004.\n[Par00]\nPablo A. Parrilo. Structured Semideﬁnite Programs and Semialgebraic Geometry Methods\nin Robustness and Optimization. PhD thesis, California Institute of Technology, 2000.\n[PWMH13] Hristo S Paskov, Robert West, John C Mitchell, and Trevor Hastie. Compressive feature\nlearning. In Advances in Neural Information Processing Systems, pages 2931–2939, 2013.\n[Ris78]\nJorma Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.\n[Sal09]\nRuslan Salakhutdinov.\nLearning Deep Generative Models.\nPhD thesis, University of\nToronto, Toronto, Ont., Canada, Canada, 2009. AAINR61080.\n21\n[SS05]\nNathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Learning Theory,\n18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30,\n2005, Proceedings, pages 545–560, 2005.\n[SST08]\nOhad Shamir, Sivan Sabato, and Naftali Tishby. Learning and Generalization with the\nInformation Bottleneck, pages 92–107. Springer Berlin Heidelberg, Berlin, Heidelberg,\n2008.\n[Tib96]\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society. Series B (Methodological), 58(1):267–288, 1996.\n[TJ89]\nN. Tomczak-Jaegermann. Banach-Mazur distances and ﬁnite-dimensional operator ideals.\nPitman monographs and surveys in pure and applied mathematics. Longman Scientiﬁc &\nTechnical, 1989.\n[TPB00]\nNaftali Tishby, Fernando C. N. Pereira, and William Bialek. The information bottleneck\nmethod. CoRR, physics/0004057, 2000.\n[Val84]\nLeslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–\n1142, 1984.\n[Vap98]\nVladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\n[Vu11]\nVan Vu. Singular vectors under random perturbation. Random Structures and Algorithms,\n39(4):526–538, 2011.\n22\nA\nProof of Theorem 2.1\nProof of Theorem 2.1. [MRT12, Theorem 3.1] asserts that with probability at least 1 −δ, we have that\nfor every hypothesis f ∈H,\nloss\nD (f) ⩽loss\nS (f) + 2Rm(H) +\ns\nlog 1\nδ\n2m\nby negating the loss function this gives\n| loss\nD (f) −loss\nS (f)| ⩽2Rm(H) +\ns\nlog 2\nδ\n2m\nand therefore, letting f ∗= arg minf∈H lossD(f), we have\nloss\nD ( ˆfERM) ⩽loss\nS ( ˆfERM) + 2Rm(H) +\ns\nlog 1\nδ\n2m\n(by [MRT12, Theorem 3.1])\n⩽loss\nS (f ∗) + 2Rm(H) +\ns\nlog 1\nδ\n2m\n( by deﬁnition of ERM)\n⩽loss\nD (f ∗) + 6Rm(H) +\ns\n4 log 1\nδ\n2m\n( using [MRT12, Theorem 3.1] again)\nA.1\nLow reconstruction error is sufﬁcient for supervised learning\nThis section observes that low reconstruction error is a sufﬁcient condition for unsupervised learning to\nallow supervised learning over any future task.\nLemma A.1. Consider any supervised learning problem with respect to distribution D over X × L that\nis agnostically PAC-learnable with respect to L-Lipschitz loss function ℓand with bias γ1.\nSuppose that unsupervised hypothesis class H is C -learnable with bias γ2 over distribution D and\ndomain X, by hypothesis f : X 7→Y. Then the distribution ˜Df over Y × L, which gives the pair\n(f(x), y) the same measure as D gives to (x, y), is agnostically PAC-learnable with bias γ1 + Lγ2.\nProof. Let h : X 7→Y be a hypothesis that PAC-learns distribution D. Consider the hypothesis\n˜h : Y 7→L , ˜h(y) = (h ◦g)(y)\nThen by deﬁnition of reconstruction error and the Lipschitz property of ℓwe have\nloss\n˜Df\n(˜h) =\nE\n(y,l)∼˜Df\n[ℓ(˜h(y), l)]\n=\nE\n(y,l)∼˜Df\n[ℓ((h ◦g)(y), l)]\n=\nE\n(x,l)∼D\n[ℓ(h(˜x), l)]\n(D(x) = ˜Df(y))\n=\nE\n(x,l)∼D\n[ℓ(h(x), l)] +\nE\n(x,l)∼D\n[ℓ(h(˜x), l) −ℓ(h(x), l)]\n= γ1 +\nE\n(x,l)∼D\n[ℓ(h(˜x), l) −ℓ(h(x), l)]\n( PAC learnability)\n23\n⩽γ1 + L E\nx∼D ∥x −˜x∥\n( Lipschitzness of ℓ◦h)\n= γ1 + L E\nx∼D ∥x −g ◦f(x)∥\n⩽γ1 + Lγ2\n( C -learnability)\nB\nProof of Theorem 3.1\nProof of Theorem 3.1. We assume without loss of generality s = 1. For s > 1 the proof will be identical\nsince one can assume x⊗s is the data points (and the dimension is raised to ds).\nLet x1, . . . , xm be a set of examples ∼Dm. It can be shown that any minimizer of ERM\nA∗= arg min\nA∈Rd×k ∥xi −A†Axi∥2\n(B.1)\nsatisﬁes that (A∗)†A∗is the the projection operator to the subspace of top k eigenvector of Pm\ni=1 xix⊤\ni .\nTherefore ERM (B.1) is efﬁciently solvable.\nAccording to Theorem 2.1, the ERM hypothesis generalizes with rates governed by the Rademacher\ncomplexity of the hypothesis class. Thus, it remains to compute the Rademacher complexity of the\nhypothesis class for PCA. We assume for simplicity that all vectors in the domain have Euclidean norm\nbounded by one.\nRS(Hpca\nk ) =\nE\nσ∼{±1}m\n\"\nsup\n(h,g)∈Hpca\nk\n1\nm\nX\ni∈S\nσiℓ((h, g), xi)\n#\n=\nE\nσ∼{±1}m\n\"\nsup\nA∈Rd×k\n1\nm\nX\ni∈S\nσi∥xi −A†Axi∥2\n#\n=\nE\nσ∼{±1}m\n\"\nsup\nA∈Rd×k\n1\nm\nX\ni∈S\nσiTr((I −A†A)\n m\nX\ni=1\nxix⊤\ni\n!\n(I −A†A)⊤)\n#\n=\nE\nσ∼{±1}m\n\"\nsup\nA∈Rd×k Tr\n \n(I −A†A)\n \n1\nm\nm\nX\ni=1\nσixix⊤\ni\n!!#\n.\nThen we apply Holder inequality, and effectively disentangle the part about σ and A:\nE\nσ∼{±1}m\n\"\nsup\nA∈Rd×k Tr\n \n(I −A†A)\n \n1\nm\nm\nX\ni=1\nσixix⊤\ni\n!!#\n⩽\nE\nσ∼{±1}m\n\"\nsup\nA∈Rd×k ∥I −A†A∥F\n\r\r\r\r\r\n1\nm\nm\nX\ni=1\nσixix⊤\ni\n\r\r\r\r\r\nF\n#\n(Holder inequality)\n⩽\n√\nd\nE\nσ∼{±1}m\n\"\r\r\r\r\r\n1\nm\nm\nX\ni=1\nσixix⊤\ni\n\r\r\r\r\r\nF\n#\n( since A†A is a projection, ∥I −A†A∥⩽1.)\n⩽\n√\nd\nE\nσ∼{±1}m\n\n\n\r\r\r\r\r\n1\nm\nm\nX\ni=1\nσixix⊤\ni\n\r\r\r\r\r\n2\nF\n\n\n1/2\n(Cauchy-Schwarz inequality)\n⩽\n√\nd\nv\nu\nu\nt 1\nm2\nm\nX\ni=1\n⟨σixix⊤\ni , σixix⊤\ni ⟩\n(since E[σiσj] = 0 for i ̸= j)\n24\n⩽\np\nd/m .\n(by ∥x∥⩽1)\nThus, from Theorem 2.1 we can conclude that the class Hpca\nk\nis learnable with sample complexity\n˜O( d\nε2)6.\nB.1\nProof of Theorem 3.2\nTheorem 3.2 follows from the following lemmas and the generalization theorem 2.1 straightforwardly.\nLemma B.1. Suppose distribution D is (k, ε)-regularly spectral decodable. Then for any δ > 0, solving\nconvex optimization (3.3) with non-smooth Frank-Wolfe algorithm [HK12, Theorem 4.4] with k′ =\nO(τ 4k4/δ4) steps gives a solution ˆR of rank k′ such that f( ˆR) ⩽δ + ε.\nLemma\nB.2.\nThe\nRademacher\ncomplexity\nof\nthe\nclass\nof\nfunction\nΦ\n=\nn\r\rRx⊗2 −x⊗2\r\r\nop : R s.t. ∥R∥S1 ⩽τk\no\nwith m examples is bounded from above by at most\nRm(Φ) ⩽2τk ·\np\n1/m\nHere Lemma B.2 follows from the fact that\n\r\rRx⊗2 −x⊗2\r\r\nop is bounded above by 2τk when ∥x∥⩽\n1 and ∥R∥S1 ⩽τk. The rest of the section focuses on the proof of Lemma B.1.\nLemma B.1 basically follows from the fact that f is Lipschitz and guarantees of the Frank-Wolfe\nalgorithm.\nProposition B.3. The objective function f(R) is convex and 1-Lipschitz. Concretely, Let ℓx(R) =\n∥Rx⊗2 −x⊗2∥op. Then\n∂ℓx ∋(u ⊗v)(x⊗2)⊤\nwhere ∂ℓx is the set of sub-gradients of ℓx with respect to R, and u, v ∈Rd are (one pair of) top left and\nright singular vectors of M(Rx⊗2 −x⊗2).\nProof. This simply follows from calculating gradient with chain rule. Here we use the fact that A ∈\n(Rd)⊗2, the sub-gradient of ∥A∥op contains the vector a ⊗b where a, b are the top left and right singular\nvectors of M(A). We can also verify by deﬁnition that (u ⊗v)(x⊗2)⊤is a sub-gradient.\nf(R′) −f(R) ⩾(u ⊗v)⊤(R′x⊗2 −Rx⊗2)\n(by convexity of ∥· ∥op)\n= ⟨(u ⊗v)(x⊗2)⊤, R′ −R⟩.\nNow we are ready to prove Lemma B.1.\nProof of Lemma B.1. Since D is (k, ε)-regularly decodable, we know that there exists a rank-k solution\nR∗with f(R∗) ⩽ε. Since ∥R∗∥op ⩽τ, we have that ∥R∥S1 ⩽rank(R∗) · ∥R∥op ⩽τk. Therefore R∗\nis feasible solution for the objective (3.3) with f(R∗) ⩽ε.\nBy Proposition B.3, we have that f(R) is 1-Lipschitz. Moreover, for any R, S with ∥R∥S1 ⩽\nτk, ∥S∥S1 ⩽τk we have that ∥R −S∥F ⩽∥R∥F + ∥S∥F ⩽∥R∥S1 + ∥S∥S1 ⩽2τk. Therefore the\ndiameter of the constraint set is at most τk.\nBy [HK12, Theorem 4.4], we have that Frank-Wolfe algorithm returns solution R with f(R) −\nf(R∗) ⩽ε + δ in\n\u0000 τk\nδ\n\u00014 iteration.\n6For ℓ> 1 the sample complexity is ˜O(dℓ/ε2).\n25\nC\nShorter codes with relaxed objective for Polynomial Spectral Compo-\nnents Analysis\nNotations.\nFor a matrix A, let σ1(A) ⩾σ2(A) ⩾.. be its singular values. Then the Schatten p-norm,\ndenoted by ∥· ∥Sp, for p ⩾1 is deﬁned as ∥A∥Sp = (P\ni σi(A)p)1/p. For even integer p, an equivalent\nand simple deﬁnition is that ∥A∥p\nSp ≜Tr((A⊤A)p/2).\nIn this section we consider the following further relaxation of objective (3.3).\nminimize f4(R) := E\nh\r\rRx⊗2 −x⊗2\r\r2\nSp\ni\n(C.1)\ns.t. ∥R∥S1 ⩽τk\nSince ∥A∥F ⩾∥A∥S4 ⩾∥A∥S∞= ∥A∥, this is a relaxation of the objective (3.3), and it interpolates\nbetween kernal PCA and spectral decoding. Our assumption is weaker than kernal PCA but stronger\nthan spectral decodable.\nDeﬁnition C.1 (Extension of deﬁnition 3.2). We say a data distribution D is (k, ε)-regularly spectral\ndecodable with ∥· ∥Sp norm if the error E in equation (5.10) is bounded by ∥E∥Sp ⩽ε.\nWe can reduce the length of the code from O(k4) to O(k2) for any constant p.\nTheorem C.1. Suppose data distribution is (k, ε)-spectral decodable with norm ∥· ∥Sp for p = O(1),\nthen solving (C.1) using (usual) Frank-Wolfe algorithm gives a solution ˆR of k′ = O(k2τ 2/ε2) with\nf(R) ⩽ε + δ. As a direct consequence, we obtain encoding and decoding pair (gA, hB) ∈Hsa\nk′ with\nk′ = O(k2τ 2/ε2) and reconstruction error ε + δ.\nThe main advantage of using relaxed objective is its smoothness. This allows us to optimize over\nthe Schatten 1-norm constraint set much faster using usual Frank-Wolfe algorithm. Therefore the key\nhere is to establish the smoothness of the objective function. Theorem C.1 follows from the following\nproposition straightforwardly.\nProposition C.2. Objective function fp (equation (C.1)) is convex and O(p)-smooth.\nProof. Since ∥·∥Sp is convex and composition of convex function with linear function gives convex func-\ntion. Therefore,\n\r\rRx⊗2 −x⊗2\r\r\nSp is a convex function. The square of an non-negative convex function\nis also convex, and therefore we proved that fp is convex. We prove the smoothness by ﬁrst showing\nthat ∥A∥2\nSp is a smooth function with respect to A. We use the deﬁnition ∥A∥p\nSp = Tr((A⊤A)p/2). Let\nE be a small matrix that goes to 0, we have\n∥A + E∥p\nSp = Tr((A⊤A)p/2) + T1 + T2 + o(∥E∥2\nF )\n(C.2)\nwhere T1, T2 denote the ﬁrst order term and second order term respectively.\nLet U = A⊤E +\nE⊤A and V\n= A⊤A.\nWe note that T2 is a sum of the traces of matrices like UV UV U p/2−2.\nBy Lieb-Thirring inequality, we have that all these term can be bounded by Tr(U p/2−2V 2) =\n2Tr((A⊤A)p/2−2A⊤EE⊤A) + 2Tr((A⊤A)p/2−2A⊤EA⊤E⊤). For the ﬁrst term, we have that\nTr((A⊤A)p/2−2A⊤EE⊤A) ⩽∥(AA⊤)(p−2)/4E∥2 ⩽∥(AA⊤)(p−2)/4∥2\nS∞∥E∥2\nF = ∥A∥p−2\nS∞∥E∥2\nF\nwhere in the ﬁrst inequality we use Cauchy-Schwarz. Then for the second term we have\nTr((A⊤A)p/2−2A⊤EA⊤E⊤) ⩽∥(A⊤A)(p−2)/4E∥F ∥AE(A⊤A)(p−4)/4∥F\n⩽∥(A⊤A)(p−2)/4E∥2\nF\n(by Lieb-Thirring inequality)\n⩽∥(AA⊤)(p−2)/4∥2∥E∥2\nF = ∥A∥p−2\nS∞∥E∥2\nF\n(C.3)\n26\nTherefore, ﬁnally we got\nT2 ⩽O(p2) · ∥A∥p−2\nSp−2∥E∥2\nF\n(C.4)\nTherefore, we complete the proof by having,\n∥A + E∥2\nSp ⩽(∥A∥p\nSp + T1 + T2 + o(∥E∥2))2/p ⩽∥A∥Sp(1 + T ′\n1 +\n2\np∥A∥p/2\nSp\nT2) + o(∥E∥2)\n(by (1 + x)p/2 ⩽1 + 2x/p + o(∥x∥2))\n⩽∥A∥2\nSp + T ′′\n1 + O(p)∥E∥2 + o(∥E∥2)\n(by equation (C.3))\nD\nToolbox\nLemma D.1. Let p ⩾2 be a power of 2 and u = [u1, . . . , un] and v = [v1, . . . , vn] be indeterminants.\nThen there exists SoS proof,\n⊢\n\nX\nj\nuivp−1\nj\n\n\np\n⩽\n X\ni\nup\ni\n! \u0010X\nvp\ni\n\u0011p−1\n(D.1)\nProof Sketch. The inequality follows from repeated application of Cauchy-Schwarz. For example, for\np = 4 we have\n⊢\n X\ni\nu4\ni\n! \u0010X\nv4\ni\n\u00113\n⩾\n X\ni\nu2\ni v2\ni\n!2 \u0010X\nv4\ni\n\u00112\n(by Cauchy-Schwarz)\n⩾\n X\ni\nuiv3\ni\n!4\n(by Cauchy-Schwarz again)\nFor p = 2s with s > 2, the statement can be proved inductively.\n27\n",
  "categories": [
    "cs.LG",
    "cs.DS",
    "stat.ML"
  ],
  "published": "2016-10-04",
  "updated": "2016-12-27"
}