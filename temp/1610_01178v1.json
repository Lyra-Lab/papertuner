{
  "id": "http://arxiv.org/abs/1610.01178v1",
  "title": "A Tour of TensorFlow",
  "authors": [
    "Peter Goldsborough"
  ],
  "abstract": "Deep learning is a branch of artificial intelligence employing deep neural\nnetwork architectures that has significantly advanced the state-of-the-art in\ncomputer vision, speech recognition, natural language processing and other\ndomains. In November 2015, Google released $\\textit{TensorFlow}$, an open\nsource deep learning software library for defining, training and deploying\nmachine learning models. In this paper, we review TensorFlow and put it in\ncontext of modern deep learning concepts and software. We discuss its basic\ncomputational paradigms and distributed execution model, its programming\ninterface as well as accompanying visualization toolkits. We then compare\nTensorFlow to alternative libraries such as Theano, Torch or Caffe on a\nqualitative as well as quantitative basis and finally comment on observed\nuse-cases of TensorFlow in academia and industry.",
  "text": "A Tour of TensorFlow\nProseminar Data Mining\nPeter Goldsborough\nFakultät für Informatik\nTechnische Universität München\nEmail: peter.goldsborough@in.tum.de\nAbstract— Deep learning is a branch of artiﬁcial intelligence\nemploying deep neural network architectures that has signiﬁ-\ncantly advanced the state-of-the-art in computer vision, speech\nrecognition, natural language processing and other domains. In\nNovember 2015, Google released TensorFlow, an open source deep\nlearning software library for deﬁning, training and deploying\nmachine learning models. In this paper, we review TensorFlow\nand put it in context of modern deep learning concepts and\nsoftware. We discuss its basic computational paradigms and\ndistributed execution model, its programming interface as well\nas accompanying visualization toolkits. We then compare Ten-\nsorFlow to alternative libraries such as Theano, Torch or Caffe\non a qualitative as well as quantitative basis and ﬁnally comment\non observed use-cases of TensorFlow in academia and industry.\nIndex Terms— Artiﬁcial Intelligence, Machine Learning, Neu-\nral Networks, Distributed Computing, Open source software,\nSoftware packages\nI. INTRODUCTION\nModern artiﬁcial intelligence systems and machine learn-\ning algorithms have revolutionized approaches to scientiﬁc\nand technological challenges in a variety of ﬁelds. We can\nobserve remarkable improvements in the quality of state-of-\nthe-art computer vision, natural language processing, speech\nrecognition and other techniques. Moreover, the beneﬁts of\nrecent breakthroughs have trickled down to the individual,\nimproving everyday life in numerous ways. Personalized dig-\nital assistants, recommendations on e-commerce platforms,\nﬁnancial fraud detection, customized web search results and\nsocial network feeds as well as novel discoveries in genomics\nhave all been improved, if not enabled, by current machine\nlearning methods.\nA particular branch of machine learning, deep learning,\nhas proven especially effective in recent years. Deep learning\nis a family of representation learning algorithms employing\ncomplex neural network architectures with a high number\nof hidden layers, each composed of simple but non-linear\ntransformations to the input data. Given enough such trans-\nformation modules, very complex functions may be modeled\nto solve classiﬁcation, regression, transcription and numerous\nother learning tasks [1].\nIt is noteworthy that the rise in popularity of deep learning\ncan be traced back to only the last few years, enabled primarily\nby the greater availability of large data sets, containing more\ntraining examples; the efﬁcient use of graphical processing\nunits (GPUs) and massively parallel commodity hardware to\ntrain deep learning models on these equally massive data\nsets as well as the discovery of new methods such as the\nrectiﬁed linear unit (ReLU) activation function or dropout as\na regularization technique [1]–[4].\nWhile deep learning algorithms and individual architectural\ncomponents such as representation transformations, activa-\ntion functions or regularization methods may initially be\nexpressed in mathematical notation, they must eventually be\ntranscribed into a computer program for real world usage.\nFor this purpose, there exist a number of open source as\nwell as commercial machine learning software libraries and\nframeworks. Among these are Theano [5], Torch [6], scikit-\nlearn [7] and many more, which we review in further detail\nin Section II of this paper. In November 2015, this list was\nextended by TensorFlow, a novel machine learning software\nlibrary released by Google [8]. As per the initial publication,\nTensorFlow aims to be “an interface for expressing machine\nlearning algorithms” in “large-scale [...] on heterogeneous\ndistributed systems” [8].\nThe remainder of this paper aims to give a thorough review\nof TensorFlow and put it in context of the current state of\nmachine learning. In detail, the paper is further structured\nas follows. Section II will provide a brief overview and\nhistory of machine learning software libraries, listing but\nnot comparing projects similar to TensorFlow. Subsequently,\nSection III discusses in depth the computational paradigms\nunderlying TensorFlow. In Section IV we explain the current\nprogramming interface in the various supported languages. To\ninspect and debug models expressed in TensorFlow, there exist\npowerful visualization tools, which we examine in Section\nV. Section VI then gives a comparison of TensorFlow and\nalternative deep learning libraries on a qualitative as well as\nquantitative basis. Before concluding our review in Section\nVIII, Section VII studies current real world use cases of\nTensorFlow in literature and industry.\nII. HISTORY OF MACHINE LEARNING LIBRARIES\nIn this section, we aim to give a brief overview and key\nmilestones in the history of machine learning software li-\nbraries. We begin with a review of libraries suitable for a wide\nrange of machine learning and data analysis purposes, reaching\nback more than 20 years. We then perform a more focused\nstudy of recent programming frameworks suited especially to\nthe task of deep learning. Figure 1 visualizes this section in\na timeline. We wish to emphasize that this section does in no\narXiv:1610.01178v1  [cs.LG]  1 Oct 2016\nway compare TensorFlow, as we have dedicated Section VI to\nthis speciﬁc purpose.\nA. General Machine Learning\nIn the following paragraphs we list and brieﬂy review a\nsmall set of general machine learning libraries in chronolog-\nical order. With general, we mean to describe any particular\nlibrary whose common use cases in the machine learning and\ndata science community include but are not limited to deep\nlearning. As such, these libraries may be used for statisti-\ncal analysis, clustering, dimensionality reduction, structured\nprediction, anomaly detection, shallow (as opposed to deep)\nneural networks and other tasks.\nWe begin our review with a library published 21 years\nbefore TensorFlow: MLC++ [9]. MLC++ is a software library\ndeveloped in the C++ programming language providing algo-\nrithms alongside a comparison framework for a number of\ndata mining, statistical analysis as well as pattern recognition\ntechniques. It was originally developed at Stanford University\nin 1994 and is now owned and maintained by Silicon Graphics,\nInc (SGI1). To the best of our knowledge, MLC++ is the oldest\nmachine learning library still available today.\nFollowing MLC++ in the chronological order, OpenCV2\n(Open Computer Vision) was released in the year 2000 by\nBradski et al. [10]. It is aimed primarily at solving learning\ntasks in the ﬁeld of computer vision and image recognition,\nincluding a collection of algorithms for face recognition,\nobject identiﬁcation, 3D-model extraction and other purposes.\nIt is released under a BSD license and provides interfaces in\nmultiple programming languages such as C++, Python and\nMATLAB.\nAnother machine learning library we wish to mention is\nscikit-learn3 [7]. The scikit-learn project was originally devel-\noped by David Cournapeu as part of the Google Summer of\nCode program4 in 2008. It is an open source machine learning\nlibrary written in Python, on top of the NumPy, SciPy and\nmatplotlib frameworks. It is useful for a large class of both\nsupervised and unsupervised learning problems.\nThe Accord.NET5 library stands apart from the aforemen-\ntioned examples in that it is written in the C# (“C Sharp”)\nprogramming language. Released in 2008, it is composed not\nonly of a variety of machine learning algorithms, but also\nsignal processing modules for speech and image recognition\n[11].\nMassive Online Analysis6 (MOA) is an open source frame-\nwork for online and ofﬂine analysis of massive, potentially\ninﬁnite, data streams. MOA includes a variety of tools for\nclassiﬁcation, regression, recommender systems and other\ndisciplines. It is written in the Java programming language\n1https://www.sgi.com/tech/mlc/\n2http://opencv.org\n3http://scikit-learn.org/stable/\n4https://summerofcode.withgoogle.com\n5http://accord-framework.net/index.html\n6http://moa.cms.waikato.ac.nz\nand maintained by staff of the University of Waikato, New\nZealand. It was conceived in 2010 [12].\nThe Mahout7 project, part of Apache Software Foundation8,\nis a Java programming environment for scalable machine\nlearning applications, built on top of the Apache Hadoop9 plat-\nform. It allows for analysis of large datasets distributed in the\nHadoop Distributed File System (HDFS) using the MapReduce\nprogramming paradigm. Mahout provides machine learning\nalgorithms for classiﬁcation, clustering and ﬁltering.\nPattern10 is a Python machine learning module we include\nin our list due to its rich set of web mining facilities. It com-\nprises not only general machine learning algorithms (e.g. clus-\ntering, classiﬁcation or nearest neighbor search) and natural\nlanguage processing methods (e.g. n-gram search or sentiment\nanalysis), but also a web crawler that can, for example, fetch\nTweets or Wikipedia entries, facilitating quick data analysis on\nthese sources. It was published by the University of Antwerp\nin 2012 and is open source.\nLastly, Spark MLlib11 is an open source machine learning\nand data analysis platform released in 2015 and built on top\nof the Apache Spark12 project [13], a fast cluster computing\nsystem. Similar to Apache Mahout, it supports processing\nof large scale distributed datasets and training of machine\nlearning models across a cluster of commodity hardware. For\nthis, it includes classiﬁcation, regression, clustering and other\nmachine learning algorithms [14].\nB. Deep Learning\nWhile the software libraries mentioned in the previous\nsection are useful for a great variety of different machine\nlearning and statistical analysis tasks, the following paragraphs\nlist software frameworks especially effective in training deep\nlearning models.\nThe ﬁrst and oldest framework in our list suited to the\ndevelopment and training of deep neural networks is Torch13,\nreleased already in 2002 [6]. Torch consisted originally of\na pure C++ implementation and interface. Today, its core\nis implemented in C/CUDA while it exposes an interface\nin the Lua14 scripting language. For this, Torch makes use\nof a LuaJIT (just-in-time) compiler to connect Lua routines\nto the underlying C implementations. It includes, inter alia,\nnumerical optimization routines, neural network models as\nwell as general purpose n-dimensional array (tensor) objects.\nTheano15, released in 2008 [5], is another noteworthy deep\nlearning library. We note that while Theano enjoys greatest\npopularity among the machine learning community, it is, in\nessence, not a machine learning library at all. Rather, it is a\n7http://mahout.apache.org\n8http://www.apache.org\n9http://hadoop.apache.org\n10http://www.clips.ua.ac.be/pages/pattern\n11http://spark.apache.org/mllib\n12http://spark.apache.org/\n13http://torch.ch\n14https://www.lua.org\n15http://deeplearning.net/software/theano/\n1992\n1994\n1996\n1998\n2000\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016\nMLC++\nOpenCV\nTorch\nscikit\nAccord\nTheano\nMOA\nMahout\nPattern\nDL4J\nCaffe\ncuDNN\nTensorFlow\nMLlib\nFig. 1: A timeline showing the release of machine-learning libraries discussed in section I in the last 25 years.\nprogramming framework that allows users to declare math-\nematical expressions symbolically, as computational graphs.\nThese are then optimized, eventually compiled and ﬁnally\nexecuted on either CPU or GPU devices. As such, [5] labels\nTheano a “mathematical compiler”.\nCaffe16 is an open source deep learning library maintained\nby the Berkeley Vision and Learning Center (BVLC). It\nwas released in 2014 under a BSD-License [15]. Caffe is\nimplemented in C++ and uses neural network layers as its\nbasic computational building blocks (as opposed to Theano\nand others, where the user must deﬁne individual mathematical\noperations making up layers). A deep learning model, con-\nsisting of many such layers, is stored in the Google Protocol\nBuffer format. While models can be deﬁned manually in this\nProtocol Buffer “language”, there exist bindings to Python\nand MATLAB to generate them programmatically. Caffe is\nespecially well suited to the development and training of\nconvolutional neural networks (CNNs or ConvNets), used\nextensively in the domain of image recognition.\nWhile the aforementioned machine learning frameworks\nallowed for the deﬁnition of deep learning models in Python,\nMATLAB and Lua, the Deeplearning4J17 (DL4J) library\nenables also the Java programmer to create deep neural\nnetworks. DL4J includes functionality to create Restricted\nBoltzmann machines, convolutional and recurrent neural net-\nworks, deep belief networks and other types of deep learning\nmodels. Moreover, DL4J enables horizontal scalability using\ndistributed computing platforms such as Apache Hadoop or\nSpark. It was released in 2014 by Adam Gibson under an\nApache 2.0 open source license.\nLastly, we add the NVIDIA Deep Learning SDK18 to\nto this list. Its main goal is to maximize the performance\nof deep learning algorithms on (NVIDIA) GPUs. The SDK\nconsists of three core modules. The ﬁrst, cuDNN, provides\nhigh performance GPU implementations for deep learning\nalgorithms such as convolutions, activation functions and\ntensor transformations. The second is a linear algebra library,\ncuBLAS, enabling GPU-accelerated mathematical operations\non n-dimensional arrays. Lastly, cuSPARSE includes a set of\nroutines for sparse matrices tuned for high efﬁciency on GPUs.\nWhile it is possible to program in these libraries directly, there\n16http://caffe.berkeleyvision.org\n17http://deeplearning4j.org\n18https://developer.nvidia.com/deep-learning-software\nexist also bindings to other deep learning libraries, such as\nTorch19.\nIII. THE TENSORFLOW PROGRAMMING MODEL\nIn this section we provide an in-depth discussion of the\nabstract computational principles underlying the TensorFlow\nsoftware library. We begin with a thorough examination of the\nbasic structural and architectural decisions made by the Ten-\nsorFlow development team and explain how machine learning\nalgorithms may be expressed in its dataﬂow graph language.\nSubsequently, we study TensorFlow’s execution model and\nprovide insight into the way TensorFlow graphs are assigned\nto available hardware units in a local as well as distributed\nenvironment. Then, we investigate the various optimizations\nincorporated into TensorFlow, targeted at improving both\nsoftware and hardware efﬁciency. Lastly, we list extensions to\nthe basic programming model that aid the user in both com-\nputational as well as logistical aspects of training a machine\nlearning model with TensorFlow.\nA. Computational Graph Architecture\nIn TensorFlow, machine learning algorithms are represented\nas computational graphs. A computational or dataﬂow graph\nis a form of directed graph where vertices or nodes describe\noperations, while edges represent data ﬂowing between these\noperations. If an output variable z is the result of applying a\nbinary operation to two inputs x and y, then we draw directed\nedges from x and y to an output node representing z and\nannotate the vertex with a label describing the performed\ncomputation. Examples for computational graphs are given\nin Figure 2. The following paragraphs discuss the principle\nelements of such a dataﬂow graph, namely operations, tensors,\nvariables and sessions.\n1) Operations: The major beneﬁt of representing an algo-\nrithm in form of a graph is not only the intuitive (visual)\nexpression of dependencies between units of a computational\nmodel, but also the fact that the deﬁnition of a node within\nthe graph can be kept very general. In TensorFlow, nodes\nrepresent operations, which in turn express the combination or\ntransformation of data ﬂowing through the graph [8]. An oper-\nation can have zero or more inputs and produce zero or more\noutputs. As such, an operation may represent a mathematical\n19https://github.com/soumith/cudnn.torch\nz\nx\ny\n+\nz = x + y\nx⊤w\nx\nw\ndot\nb\nz\n+\nˆy\nσ\nˆy = σ(x⊤w + b)\nFig. 2: Examples of computational graphs. The left graph\ndisplays a very simple computation, consisting of just an\naddition of the two input variables x and y. In this case, z\nis the result of the operation +, as the annotation suggests.\nThe right graph gives a more complex example of computing\na logistic regression variable ˆy in for some example vector x,\nweight vector w as well as a scalar bias b. As shown in the\ngraph, ˆy is the result of the sigmoid or logistic function σ.\nequation, a variable or constant, a control ﬂow directive, a\nﬁle I/O operation or even a network communication port.\nIt may seem unintuitive that an operation, which the reader\nmay associate with a function in the mathematical sense, can\nrepresent a constant or variable. However, a constant may be\nthought of as an operation that takes no inputs and always\nproduces the same output corresponding to the constant it\nrepresents. Analogously, a variable is really just an operation\ntaking no input and producing the current state or value of\nthat variable. Table ?? gives an overview of different kinds of\noperations that may be declared in a TensorFlow graph.\nAny operation must be backed by an associated implemen-\ntation. In [8] such an implementation is referred to as the\noperation’s kernel. A particular kernel is always speciﬁcally\nbuilt for execution on a certain kind of device, such as a CPU,\nGPU or other hardware unit.\n2) Tensors: In TensorFlow, edges represent data ﬂowing\nfrom one operation to another and are referred to as tensors.\nA tensor is a multi-dimensional collection of homogeneous\nvalues with a ﬁxed, static type. The number of dimensions\nof a tensor is termed its rank. A tensor’s shape is the\ntuple describing its size, i.e. the number of components, in\neach dimension. In the mathematical sense, a tensor is the\ngeneralization of two-dimensional matrices, one-dimensional\nCategory\nExamples\nElement-wise operations\nAdd, Mul, Exp\nMatrix operations\nMatMul, MatrixInverse\nValue-producing operations\nConstant, Variable\nNeural network units\nSoftMax, ReLU, Conv2D\nCheckpoint operations\nSave, Restore\nTABLE I: Examples for TensorFlow operations [8].\nvectors and also scalars, which are simply tensors of rank zero.\nIn terms of the computational graph, a tensor can be seen\nas a symbolic handle to one of the outputs of an operation.\nA tensor itself does not hold or store values in memory, but\nprovides only an interface for retrieving the value referenced\nby the tensor. When creating an operation in the TensorFlow\nprogramming environment, such as for the expression x + y,\na tensor object is returned. This tensor may then be supplied\nas input to other computations, thereby connecting the source\nand destination operations with an edge. By these means, data\nﬂows through a TensorFlow graph.\nNext to regular tensors, TensorFlow also provides a\nSparseTensor data structure, allowing for a more space-\nefﬁcient dictionary-like representation of sparse tensors with\nonly few non-zeros entries.\n3) Variables: In a typical situation, such as when per-\nforming stochastic gradient descent (SGD), the graph of a\nmachine learning model is executed from start to end multiple\ntimes for a single experiment. Between two such invocations,\nthe majority of tensors in the graph are destroyed and do\nnot persist. However, it is often necessary to maintain state\nacross evaluations of the graph, such as for the weights and\nparameters of a neural network. For this purpose, there exist\nvariables in TensorFlow, which are simply special operations\nthat can be added to the computational graph.\nIn detail, variables can be described as persistent, mutable\nhandles to in-memory buffers storing tensors. As such, vari-\nables are characterized by a certain shape and a ﬁxed type.\nTo manipulate and update variables, TensorFlow provides the\nassign family of graph operations.\nWhen creating a variable node for a TensorFlow graph, it\nis necessary to supply a tensor with which the variable is\ninitialized upon graph execution. The shape and data type of\nthe variable is then deduced from this initializer. Interestingly,\nthe variable itself does not store this initial tensor. Rather,\nconstructing a variable results in the addition of three distinct\nnodes to the graph:\n1) The actual variable node, holding the mutable state.\n2) An operation producing the initial value, often a con-\nstant.\n3) An initializer operation, that assigns the initial value\nto the variable tensor upon evaluation of the graph.\nAn example for this is given in Figure 3.\n4) Sessions: In TensorFlow, the execution of operations\nand evaluation of tensors may only be performed in a special\nenvironment referred to as session. One of the responsibilities\nof a session is to encapsulate the allocation and management\nof resources such as variable buffers. Moreover, the Session\ninterface of the TensorFlow library provides a run routine,\nwhich is the primary entry point for executing parts or the\nentirety of a computational graph. This method takes as input\nthe nodes in the graph whose tensors should be computed and\nreturned. Moreover, an optional mapping from arbitrary nodes\nin the graph to respective replacement values — referred to as\nfeed nodes — may be supplied to run as well [8].\nv′\nv\ni\nassign\nv = i\nFig. 3: The three nodes that are added to the computational\ngraph for every variable deﬁnition. The ﬁrst, v, is the variable\noperation that holds a mutable in-memory buffer containing\nthe value tensor of the variable. The second, i, is the node\nproducing the initial value for the variable, which can be\nany tensor. Lastly, the assign node will set the variable\nto the initializer’s value when executed. The assign node\nalso produces a tensor referencing the initialized value v′ of\nthe variable, such that it may be connected to other nodes\nas necessary (e.g. when using a variable as the initializer for\nanother variable).\nUpon invocation of run, TensorFlow will start at the\nrequested output nodes and work backwards, examining the\ngraph dependencies and computing the full transitive closure\nof all nodes that must be executed. These nodes may then\nbe assigned to one or many physical execution units (CPUs,\nGPUs etc.) on one or many machines. The rules by which\nthis assignment takes place are determined by TensorFlow’s\nplacement algorithm, discussed in detail in Subsection ??.\nFurthermore, as there exists the possibility to specify explicit\norderings of node evaluations, called control dependencies, the\nexecution algorithm will ensure that these dependencies are\nmaintained.\nB. Execution Model\nTo execute computational graphs composed of the various\nelements just discussed, TensorFlow divides the tasks for its\nimplementation among four distinct groups: the client, the\nmaster, a set of workers and lastly a number of devices. When\nthe client requests evaluation of a TensorFlow graph via a\nSession’s run routine, this query is sent to the master\nprocess, which in turn delegates the task to one or more worker\nprocesses and coordinates their execution. Each worker is\nsubsequently responsible for overseeing one or more devices,\nwhich are the physical processing units for which the kernels\nof an operation are implemented.\nWithin this model, there are two degrees of scalability. The\nﬁrst degree pertains to scaling the number of machines on\nwhich a graph is executed. The second degree refers to the\nfact that on each machine, there may then be more than\none device, such as, for example, ﬁve independent GPUs\nand/or three CPUs. For this reason, there exist two “versions”\nof TensorFlow, one for local execution on a single machine\n(but possibly many devices), and one supporting a distributed\nimplementation across many machines and many devices.\nclient\nmaster\nrun\nworker A\nGPU0\nCPU0\n...\nworker B\nCPU0\nCPU1\n...\nFig. 4: A visualization of the different execution agents in a\nmulti-machine, multi-device hardware conﬁguration.\nFigure 4 visualizes a possible distributed setup. While the\ninitial release of TensorFlow supported only single-machine\nexecution, the distributed version was open-sourced on April\n13, 2016 [16].\n1) Devices: Devices are the smallest, most basic entities\nin the TensorFlow execution model. All nodes in the graph,\nthat is, the kernel of each operation, must eventually be\nmapped to an available device to be executed. In practice,\na device will most often be either a CPU or a GPU. How-\never, TensorFlow supports registration of further kinds of\nphysical execution units by the user. For example, in May\n2016, Google announced its Tensor Processing Unit (TPU),\na custom built ASIC (application-speciﬁc-integrated-circuit)\noptimized speciﬁcally for fast tensor computations [17]. It is\nthus understandably easy to integrate new device classes as\nnovel hardware emerges.\nTo oversee the evaluation of nodes on a device, a worker\nprocess is spawned by the master. As a worker process may\nmanage one or many devices on a single machine, a device is\nidentiﬁed not only by a name, but also an index for its worker\ngroup. For example, the ﬁrst CPU in a particular group may\nbe identiﬁed by the string “/cpu:0”.\n2) Placement Algorithm: To determine what nodes to as-\nsign to which device, TensorFlow makes use of a placement\nalgorithm. The placement algorithm simulates the execution\nof the computational graph and traverses its nodes from\ninput tensors to output tensors. To decide on which of the\navailable devices D = {d1, . . . , dn} to place a given node\nν encountered during this traversal, the algorithm consults\na cost model Cν(d). This cost model takes into account\nfour pieces of information to determine the optimal device\nˆd = arg mind∈D Cν(d) on which to place the node during\nexecution:\n1) Whether or not there exists an implementation (kernel)\nfor a node on the given device at all. For example, if\nthere is no GPU kernel for a particular operation, any\nGPU device would automatically incur an inﬁnite cost.\n2) Estimates of the size (in bytes) for a node’s input and\noutput tensors.\n3) The expected execution time for the kernel on the device.\n4) A heuristic for the cost of cross-device (and possibly\ncross-machine) transmission of the input tensors to the\noperation, in the case that the input tensors have been\nplaced on nodes different from the one currently under\nconsideration.\n3) Cross-Device Execution: If the hardware conﬁguration\nof the user’s system provides more than one device, the\nplacement algorithm will often distribute a graph’s nodes\namong these devices. This can be seen as partitioning the set\nof nodes into classes, one per device. As a consequence, there\nmay be cross-device dependencies between nodes that must be\nhandled via a number of additional steps. Let us consider for\nthis two devices A and B with particular focus on a node ν on\ndevice A. If ν’s output tensor forms the input to some other\noperations α, β on device B, there initially exist cross-device\nedges ν →α and ν →β from device A to device B. This is\nvisualized in Figure 5a.\nIn practice, there must be some means of transmitting ν’s\noutput tensor from A, say a GPU device, to B — maybe\na CPU device. For this reason, TensorFlow initially replaces\nthe two edges ν →α and ν →β by three new nodes. On\ndevice A, a send node is placed and connected to ν. In\ntandem, on device B, two recv nodes are instantiated and\nattached to α and β, respectively. The send and recv nodes\nare then connected by two additional edges. This step is shown\nin Figure 5b. During execution of the graph, cross-device\ncommunication of data occurs exclusively via these special\nnodes. When the devices are located on separate machines,\ntransmission between the worker processes on these machines\nmay involve remote communication protocols such as TCP or\nRDMA.\nFinally, an important optimization made by TensorFlow at\nthis step is “canonicalization” of (send, receive) pairs. In\nthe setup displayed in Figure 5b, the existence of each recv\nnode on device B would imply allocation and management of\na separate buffer to store ν’s output tensor, so that it may then\nbe fed to nodes α and β, respectively. However, an equivalent\nand more efﬁcient transformation places only one recv node\non device B, streams all output from ν to this single node,\nand then to the two dependent nodes α and β. This last and\nﬁnal evolution is given in Figure 5c.\nC. Optimizations\nTo ensure a maximum of efﬁciency and performance of the\nTensorFlow execution model, a number of optimizations are\nbuilt into the library. In this subsection, we examine three\nsuch improvements: common subgraph elimination, execution\nscheduling and ﬁnally lossy compression.\n1) Common Subgraph Elimination: An optimization per-\nformed by many modern compilers is common subexpression\nelimination, whereby a compiler may possibly replace the\ncomputation of an identical value two or more times by a\nsingle instance of that computation. The result is then stored\nin a temporary variable and reused where it was previously re-\ncalculated. Similarly, in a TensorFlow graph, it may occur that\nthe same operation is performed on identical inputs more than\nonce. This can be inefﬁcient if the computation happens to\nbe an expensive one. Moreover, it may incur a large memory\noverhead given that the result of that operation must be held in\nDevice A\nν\nDevice B\nβ\nα\n(a)\nDevice A\nν\nDevice B\nβ\nα\nsend\nrecv\nrecv\n(b)\nDevice A\nν\nDevice B\nβ\nα\nsend\nrecv\n(c)\nFig. 5: The three stages of cross-device communication be-\ntween graph nodes in TensorFlow. Figure 5a shows the initial,\nconceptual connections between nodes on different devices.\nFigure 5b gives a more practical overview of how data is\nactually transmitted across devices using send and recv\nnodes. Lastly, Figure 5c shows the ﬁnal, canonicalized setup,\nwhere there is at most one recv node per destination device.\nmemory multiple times. Therefore, TensorFlow also employs a\ncommon subexpression, or, more aptly put, common subgraph\nelimination pass prior to execution. For this, the computational\ngraph is traversed and every time two or more operations of the\nsame type (e.g. MatMul) receiving the same input tensors are\nencountered, they are canonicalized to only one such subgraph.\nThe output tensor of that single operation is then redirected to\nall dependent nodes. Figure 6 gives an example of common\nsubgraph elimination.\n2) Scheduling: A simple yet powerful optimization is to\nschedule node execution as late as possible. Ensuring that the\nresults of operations remain in memory only for the minimum\nrequired amount of time reduces peak memory consumption\nand can thus greatly improve the overall performance of the\nsystem. The authors of [8] note that this is especially vital on\ndevices such as GPUs, where memory resources are scarce.\nFurthermore, careful scheduling also pertains to the activation\nof send and recv nodes, where not only memory but also\nnetwork resources are contested.\n3) Lossy Compression: One of the primary goals of many\nmachine learning algorithms used for classiﬁcation, recogni-\ntion or other tasks is to build robust models. With robust\nwe mean that an optimally trained model should ideally not\nchange its response if it is ﬁrst fed a signal and then a\nz\nx\ny\n+\nz′\nx\ny\n+\nz2\n×\nz\nx\ny\n+\nz2\n×\nFig. 6: An example of how common subgraph elimination is\nused to transform the equations z = x + y, z′ = x + y,\nz2 = z · z′ to just two equations z = x + y and z2 = z · z.\nThis computation could theoretically be optimized further to a\nsquare operation requiring only one input (thus reducing the\ncost of data movement), though it is not known if TensorFlow\nemploys such secondary canonicalization.\nnoisy variation of that signal. As such, these machine learning\nalgorithms typically do not require high precision arithmetic as\nprovided by standard IEEE 754 32-bit ﬂoating point values.\nRather, 16 bits of precision in the mantissa would do just\nas well. For this reason, another optimization performed by\nTensorFlow is the internal addition of conversion nodes to the\ncomputational graph, which convert such high-precision 32-bit\nﬂoating-point values to truncated 16-bit representations when\ncommunicating across devices and across machines. On the\nreceiving end, the truncated representation is converted back\nto 32 bits simply by ﬁlling in zeros, rather than rounding [8].\nD. Additions to the Basic Programming Model\nHaving discussed the basic computational paradigms and\nexecution model of TensorFlow, we will now review three\nmore advanced topics that we deem highly relevant for any-\none wishing to use TensorFlow to create machine learning\nalgorithms. First, we discuss how TensorFlow handles gradient\nback-propagation, an essential concept for many deep learning\napplications. Then, we study how TensorFlow graphs support\ncontrol ﬂow. Lastly, we brieﬂy touch upon the topic of\ncheckpoints, as they are very useful for maintenance of large\nmodels.\n1) Back-Propagation Nodes: In a large number of deep\nlearning and other machine learning algorithms, it is necessary\nto compute the gradients of particular nodes of the computa-\ntional graph with respect to one or many other nodes. For\nexample, in a neural network, we may compute the cost c\nof the model for a given example x by passing that example\nthrough a series of non-linear transformations. If the neural\nnetwork consists of two hidden layers represented by functions\nf(x; w) = fx(w) and g(x; w) = gx(w) with internal weights\nw, we can express the cost for that example as c = (fx ◦\ngx)(w) = fx(gx(w)). We would then typically calculate the\ngradient dc/dw of that cost with respect to the weights and\nuse it to update w. Often, this is done by means of the back-\npropagation algorithm, which traverses the graph in reverse to\ncompute the chain rule [fx(gx(w))]′ = f ′\nx(gx(w)) · g′\nx(w).\nIn [18], two approaches for back-propagating gradients\nthrough a computational graph are described. The ﬁrst, which\nthe authors refer to as symbol-to-number differentiation, re-\nceives a set of input values and then computes the numerical\nvalues of the gradients at those input values. It does so\nby explicitly traversing the graph ﬁrst in the forward order\n(forward-propagation) to compute the cost, then in reverse\norder (back-propagation) to compute the gradients via the\nchain rule. Another approach, more relevant to TensorFlow,\nis what [18] calls symbol-to-symbol derivatives and [8] terms\nautomatic gradient computation. In this case, gradients are\nnot computed by an explicit implementation of the back-\npropagation algorithm. Rather, special nodes are added to\nthe computational graph that calculate the gradient of each\noperation and thus ultimately the chain rule. To perform back-\npropagation, these nodes must then simply be executed like\nany other nodes by the graph evaluation engine. As such, this\napproach does not produce the desired derivatives as a numeric\nvalue, but only as a symbolic handle to compute those values.\nWhen TensorFlow needs to compute the gradient of a\nparticular node ν with respect to some other tensor α, it\ntraverses the graph in reverse order from ν to α. Each\noperation o encountered during this traversal represents a\nfunction depending on α and is one of the “links” in the chain\n(ν ◦. . . ◦o ◦. . . )(α) producing the output tensor of the graph.\nTherefore, TensorFlow adds a gradient node for each such\noperation o that takes the gradient of the previous link (the\nouter function) and multiplies it with its own gradient. At the\nend of the traversal, there will be a node providing a symbolic\nhandle to the overall target derivative\ndν\ndα, which implicitly\nimplements the back-propagation algorithm. It should now be\nclear that back-propagation in this symbol-to-symbol approach\nis just another operation, requiring no exceptional handling.\nFigure 7 shows how a computational graph may look before\nand after gradient nodes are added.\nIn [8] it is noted that symbol-to-symbol derivatives may\nincur a considerable performance cost and especially result\nin increased memory overhead. To see why, it is important\nto understand that there exist two equivalent formulations\nof the chain rule. The ﬁrst reuses previous computations\nand therefore requires them to be stored longer than strictly\nnecessary for forward-propagation. For arbitrary functions f,\nw\nx\ny\nz\nf\ng\nh\n(a)\nw\nx\ny\nz\nf\ng\nh\ndx\ndw\ndy\ndx\ndz\ndy\nf ′\ng′\nh′\ndz\ndw\ndz\ndx\n×\n×\n(b)\nFig. 7: A computational graph before (7a) and after (7b)\ngradient nodes are added. In this symbol-to-symbol approach,\nthe gradient dz\ndw is just simply an operation like any other and\ntherefore requires no special handling by the graph evaluation\nengine.\ng and h it is given in Equation 1:\ndf\ndw = f ′(y) · g′(x) · h′(w) with y = g(x), x = h(w)\n(1)\nThe second possibility for computing the chain rule was\nalready shown, where each function recomputes all of its\narguments and invokes every function it depends on. It is given\nin Equation 2 for reference:\ndf\ndw = f ′(g(h(w))) · g′(h(w)) · h′(w)\n(2)\nAccording to [8], TensorFlow currently employs the ﬁrst\napproach. Given that the inner-most functions must be recom-\nputed for almost every link of the chain if this approach is\nnot employed, and taking into consideration that this chain\nmay consist of many hundreds or thousands of operations,\nthis choice seems sensible. However, on the ﬂip side, keeping\ntensors in memory for long periods of time is also not optimal,\nespecially on devices like GPUs where memory resources are\nscarce. For Equation 2, memory held by tensors could in\ntheory be freed as soon as it has been processed by its graph\ndependencies. For this reason, in [8] the development team\nof TensorFlow states that recomputing certain tensors rather\nthan keeping them in memory may be a possible performance\nimprovement for the future.\n2) Control Flow: Some machine learning algorithms may\nbeneﬁt from being able to control the ﬂow of their execution,\nperforming certain steps only under a particular condition\nor repeating some computation a ﬁxed or variable number\nof times. For this, TensorFlow provides a set of control\nﬂow primitives including if-conditionals and while-loops.\nThe possibility of loops is the reason why a TensorFlow\ncomputational graph is not necessarily acyclic. If the number\nof iterations for of a loop would be ﬁxed and known at\ngraph compile-time, its body could be unrolled into an acyclic\nsequence of computations, one per loop iteration [5]. However,\nto support a variable amount of iterations, TensorFlow is\nforced to jump through an additional set of hoops, as described\nin [8].\nOne aspect that must be especially cared for when intro-\nducing control ﬂow is back-propagation. In the case of a\nconditional, where an if-operation returns either one or the\nother tensor, it must be known which branch was taken by\nthe node during forward-propagation so that gradient nodes\nare added only to this branch. Moreover, when a loop body\n(which may be a small graph) was executed a certain number\nof times, the gradient computation does not only need to know\nthe number of iterations performed, but also requires access to\neach intermediary value produced. This technique of stepping\nthrough a loop in reverse to compute the gradients is referred\nto as back-propagation through time in [5].\n3) Checkpoints: Another extension to TensorFlow’s basic\nprogramming model is the notion of checkpoints, which allow\nfor persistent storage and recovery of variables. It is possible to\nadd Save nodes to the computational graph and connect them\nto variables whose tensors you wish to serialize. Furthermore,\na variable may be connected to a Restore operation, which\ndeserializes a stored tensor at a later point. This is especially\nuseful when training a model over a long period of time\nto keep track of the model’s performance while reducing\nthe risk of losing any progress made. Also, checkpoints are\na vital element to ensuring fault tolerance in a distributed\nenvironment [8].\nIV. THE TENSORFLOW PROGRAMMING INTERFACE\nHaving conveyed the abstract concepts of TensorFlow’s\ncomputational model in Section III, we will now concretize\nthose ideas and speak to TensorFlow’s programming interface.\nWe begin with a brief discussion of the available language\ninterfaces. Then, we provide a more hands-on look at Ten-\nsorFlow’s Python API by walking through a simple practical\nexample. Lastly, we give insight into what higher-level ab-\nstractions exist for TensorFlow’s API, which are especially\nbeneﬁcial for rapid prototyping of machine learning models.\nA. Interfaces\nThere currently exist two programming interfaces, in C++\nand Python, that permit interaction with the TensorFlow back-\nend. The Python API boasts a very rich feature set for creation\nand execution of computational graphs. As of this writing,\nthe C++ interface (which is really just the core backend\nimplementation) provides a comparatively much more limited\nAPI, allowing only to execute graphs built with Python and\nserialized to Google’s Protocol Buffer20 format. While there is\nexperimental support for also building computational graphs\nin C++, this functionality is currently not as extensive as in\nPython.\nIt is noteworthy that the Python API integrates very well\nwith NumPy21, a popular open source Python numeric and\n20https://developers.google.com/protocol-buffers/\n21http://www.numpy.org\nscientiﬁc programming library. As such, TensorFlow tensors\nmay be interchanged with NumPy ndarrays in many places.\nB. Walkthrough\nIn the following paragraphs we give a step-by-step walk-\nthrough of a practical, real-world example of TensorFlow’s\nPython API. We will train a simple multi-layer perceptron\n(MLP) with one input and one output layer to classify hand-\nwritten digits in the MNIST22 dataset. In this dataset, the\nexamples are small images of 28 × 28 pixels depicting hand-\nwritten digits in ∈{0, . . . , 9}. We receive each such example\nas a ﬂattened vector of 784 gray-scale pixel intensities. The\nlabel for each example is the digit it is supposed to represent.\nWe begin our walkthrough by importing the TensorFlow\nlibrary and reading the MNIST dataset into memory. For this\nwe assume a utility module mnist_data with a method\nread which expects a path to extract and store the dataset.\nMoreover, we pass the parameter one_hot=True to specify\nthat each label be given to us as a one-hot-encoded vector\n(d1, . . . , d10)⊤where all but the i-th component are set to\nzero if an example represents the digit i:\nimport tensorflow as tf\n# Download and extract the MNIST data set.\n# Retrieve the labels as one-hot-encoded vectors.\nmnist = mnist_data.read(\"/tmp/mnist\", one_hot=True)\nNext, we create a new computational graph via the\ntf.Graph constructor. To add operations to this graph, we\nmust register it as the default graph. The way the TensorFlow\nAPI is designed, library routines that create new operation\nnodes always attach these to the current default graph. We\nregister our graph as the default by using it as a Python context\nmanager in a with-as statement:\n# Create a new graph\ngraph = tf.Graph()\n# Register the graph as the default one to add nodes\nwith graph.as_default():\n# Add operations ...\nWe are now ready to populate our computational graph\nwith operations. We begin by adding two placeholder nodes\nexamples and labels. Placeholders are special variables\nthat must be replaced with concrete tensors upon graph execu-\ntion. That is, they must be supplied in the feed_dict argu-\nment to Session.run(), mapping tensors to replacement\nvalues. For each such placeholder, we specify a shape and\ndata type. An interesting feature of TensorFlow at this point\nis that we may specify the Python keyword None for the ﬁrst\ndimension of each placeholder shape. This allows us to later on\nfeed a tensor of variable size in that dimension. For the column\nsize of the example placeholder, we specify the number of\nfeatures for each image, meaning the 28×28 = 784 pixels. The\nlabel placeholder should expect 10 columns, corresponding\n22http://yann.lecun.com/exdb/mnist/\nto the 10-dimensional one-hot-encoded vector for each label\ndigit:\n# Using a 32-bit floating-point data type tf.float32\nexamples = tf.placeholder(tf.float32, [None, 784])\nlabels = tf.placeholder(tf.float32, [None, 10])\nGiven an example matrix X ∈Rn×784 containing n images,\nthe learning task then applies an afﬁne transformation X·W+\nb, where W is a weight matrix ∈R784×10 and b a bias vector\n∈R10. This yields a new matrix Y ∈Rn×10, containing\nthe scores or logits of our model for each example and each\npossible digit. These scores are more or less arbitrary values\nand not a probability distribution, i.e. they need neither be\n∈[0, 1] nor sum to one. To transform the logits into a valid\nprobability distribution, giving the likelihood Pr[x = i] that\nthe x-th example represents the digit i, we make use of the\nsoftmax function, given in Equation 3. Our ﬁnal estimates are\nthus calculated by softmax(X · W + b), as shown below:\n# Draw random weights for symmetry breaking\nweights = tf.Variable(tf.random_uniform([784, 10]))\n# Slightly positive initial bias\nbias = tf.Variable(tf.constant(0.1, shape=[10]))\n# tf.matmul performs the matrix multiplication XW\n# Note how the + operator is overloaded for tensors\nlogits = tf.matmul(examples, weights) + bias\n# Applies the operation element-wise on tensors\nestimates = tf.nn.softmax(logits)\nsoftmax(x)i =\nexp(xi)\nP\nj exp(xj)\n(3)\nNext, we compute our objective function, producing the\nerror or loss of the model given its current trainable parameters\nW and b. We do this by calculating the cross entropy\nH(L, Y)i = −P\nj Li,j · log(Yi,j) between the probability\ndistributions of our estimates Y and the one-hot-encoded\nlabels L. More precisely, we consider the mean cross entropy\nover all examples as the loss:\n# Computes the cross-entropy and sums the rows\ncross_entropy = -tf.reduce_sum(\nlabels * tf.log(estimates), [1])\nloss = tf.reduce_mean(cross_entropy)\nNow\nthat\nwe\nhave\nan\nobjective\nfunction,\nwe\ncan\nrun\n(stochastic)\ngradient\ndescent\nto\nupdate\nthe\nweights of our model. For this, TensorFlow provides a\nGradientDescentOptimizer class. It is initialized with\nthe learning rate of the algorithm and provides an operation\nminimize, to which we pass our loss tensor. This is the\noperation we will run repeatedly in a Session environment\nto train our model:\n# We choose a learning rate of 0.5\ngdo = tf.train.GradientDescentOptimizer(0.5)\noptimizer = gdo.minimize(loss)\nFinally, we can actually train our algorithm. For this,\nwe enter a session environment using a tf.Session as\na context manager. We pass our graph object to its con-\nstructor, so that it knows which graph to manage. To then\nexecute nodes, we have several options. The most gen-\neral way is to call Session.run() and pass a list of\ntensors we wish to compute. Alternatively, we may call\neval() on tensors and run() on operations directly.\nBefore evaluating any other node, we must ﬁrst ensure\nthat the variables in our graph are initialized. Theoreti-\ncally, we could run the Variable.initializer oper-\nation for each variable. However, one most often just uses\nthe tf.initialize_all_variables() utility opera-\ntion provided by TensorFlow, which in turn executes the\ninitializer operation for each Variable in the graph.\nThen, we can perform a certain number of iterations of\nstochastic gradient descent, fetching an example and label\nmini-batch from the MNIST dataset each time and feeding\nit to the run routine. At the end, our loss will (hopefully) be\nsmall:\nwith tf.Session(graph=graph) as session:\n# Execute the operation directly\ntf.initialize_all_variables().run()\nfor step in range(1000):\n# Fetch next 100 examples and labels\nx, y = mnist.train.next_batch(100)\n# Ignore the result of the optimizer (None)\n_, loss_value = session.run(\n[optimizer, loss],\nfeed_dict={examples: x, labels: y})\nprint(’Loss at step {0}: {1}’\n.format(step, loss_value))\nThe full code listing for this example, along with some\nadditional implementation to compute an accuracy metric at\neach time step is given in Appendix I.\nC. Abstractions\nYou may have observed how a relatively large amount of\neffort was required to create just a very simple two-layer\nneural network. Given that deep learning, by implication of its\nname, makes use of deep neural networks with many hidden\nlayers, it may seem infeasible to each time create weight and\nbias variables, perform a matrix multiplication and addition\nand ﬁnally apply some non-linear activation function. When\ntesting ideas for new deep learning models, scientists often\nwish to rapidly prototype networks and quickly exchange\nlayers. In that case, these many steps may seem very low-level,\nrepetitive and generally cumbersome. For this reason, there\nexist a number of open source libraries that abstract these\nconcepts and provide higher-level building blocks, such as\nentire layers. We ﬁnd PrettyTensor23, TFLearn24 and Keras25\nespecially noteworthy. The following paragraphs give a brief\noverview of the ﬁrst two abstraction libraries.\n1) PrettyTensor: PrettyTensor is developed by Google and\nprovides a high-level interface to the TensorFlow API via\nthe Builder pattern. It allows the user to wrap TensorFlow\noperations and tensors into “pretty” versions and then quickly\nchain any number of layers operating on these tensors. For\n23https://github.com/google/prettytensor\n24https://github.com/tﬂearn/tﬂearn\n25http://keras.io\nexample, it is possible to feed an input tensor into a fully\nconnected (“dense”) neural network layer as we did in Sub-\nsection IV-B with just a single line of code. Shown below is\nan example use of PrettyTensor, where a standard TensorFlow\nplaceholder is wrapped into a library-compatible object and\nthen fed through three fully connected layers to ﬁnally output\na softmax distribution.\nexamples = tf.placeholder([None, 784], tf.float32)\nsoftmax = (prettytensor.wrap(examples)\n.fully_connected(256, tf.nn.relu)\n.fully_connected(128, tf.sigmoid)\n.fully_connected(64, tf.tanh)\n.softmax(10))\n2) TFLearn: TFLearn is another abstraction library built\non top of TensorFlow that provides high-level building blocks\nto quickly construct TensorFlow graphs. It has a highly\nmodular interface and allows for rapid chaining of neural\nnetwork layers, regularization functions, optimizers and other\nelements. Moreover, while PrettyTensor still relied on the\nstandard tf.Session setup to train and evaluate a model,\nTFLearn adds functionality to easily train a model given an\nexample batch and corresponding labels. As many TFLearn\nfunctions, such as those creating entire layers, return vanilla\nTensorFlow objects, the library is well suited to be mixed\nwith existing TensorFlow code. For example, we could replace\nthe entire setup for the output layer discussed in Subsection\nIV-B with just a single TFLearn method invocation, leaving\nthe rest of our code base untouched. Furthermore, TFLearn\nhandles everything related to visualization with TensorBoard,\ndiscussed in Section V, automatically. Shown below is how we\ncan reproduce the full 65 lines of standard TensorFlow code\ngiven in Appendix I with less than 10 lines using TFLearn.\nimport tflearn\nimport tflearn.datasets.mnist as mnist\nX, Y, validX, validY = mnist.load_data(one_hot=True)\n# Building our neural network\ninput_layer = tflearn.input_data(shape=[None, 784])\noutput_layer = tflearn.fully_connected(input_layer,\n10, activation=’softmax’)\n# Optimization\nsgd = tflearn.SGD(learning_rate=0.5)\nnet = tflearn.regression(output_layer,\noptimizer=sgd)\n# Training\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y, validation_set=(validX, validY))\nV. VISUALIZATION OF TENSORFLOW GRAPHS\nDeep learning models often employ neural networks with\na highly complex and intricate structure. For example, [19]\nreports of deep convolutional network based on the Google\nInception model with more than 36,000 individual units,\nwhile [8] states that certain long short-term memory (LSTM)\narchitectures can span over 15,000 nodes. To maintain a\nclear overview of such complex networks, facilitate model\ndebugging and allow inspection of values on various levels of\ndetail, powerful visualization tools are required. TensorBoard,\na web interface for graph visualization and manipulation built\ndirectly into TensorFlow, is an example for such a tool. In this\nsection, we ﬁrst list a number of noteworthy features of Ten-\nsorBoard and then discuss how it is used from TensorFlow’s\nprogramming interface.\nA. TensorBoard Features\nThe core feature of TensorBoard is the lucid visualization of\ncomputational graphs, exempliﬁed in Figure 8a. Graphs with\ncomplex topologies and many layers can be displayed in a\nclear and organized manner, allowing the user to understand\nexactly how data ﬂows through it. Especially useful is Ten-\nsorBoard’s notion of name scopes, whereby nodes or entire\nsubgraphs may be grouped into one visual block, such as\na single neural network layer. Such name scopes can then\nbe expanded interactively to show the grouped units in more\ndetail. Figure 8b shows the expansion of one the name scopes\nof Figure 8a.\nFurthermore, TensorBoard allows the user to track the\ndevelopment of individual tensor values over time. For this,\nyou can attach two kinds of summary operations to nodes\nof the computational graph: scalar summaries and histogram\nsummaries. Scalar summaries show the progression of a scalar\ntensor value, which can be sampled at certain iteration counts.\nIn this way, you could, for example, observe the accuracy\nor loss of your model with time. Histogram summary nodes\nallow the user to track value distributions, such as those of\nneural network weights or the ﬁnal softmax estimates. Figures\n8c and 8d give examples of scalar and histogram summaries,\nrespectively. Lastly, TensorBoard also allows visualization of\nimages. This can be useful to show the images sampled for\neach mini-batch of an image classiﬁcation task, or to visualize\nthe kernel ﬁlters of a convolutional neural network [8].\nWe note especially how interactive the TensorBoard web in-\nterface is. Once your computational graph is uploaded, you can\npan and zoom the model as well as expand or contract individ-\nual name scopes. A demonstration of TensorBoard is available\nat https://www.tensorﬂow.org/tensorboard/index.html.\nB. TensorBoard in Practice\nTo integrate TensorBoard into your TensorFlow code, three\nsteps are required. Firstly, it is wise to group nodes into\nname scopes. Then, you may add scalar and histogram sum-\nmaries to you operations. Finally, you must instantiate a\nSummaryWriter object and hand it the tensors produced\nby the summary nodes in a session context whenever you\nwish to store new summaries. Rather than fetching individual\nsummaries, it is also possible to combine all summary nodes\ninto one via the tf.merge_all_summaries() operation.\nwith tf.name_scope(’Variables’):\nx = tf.constant(1.0)\ny = tf.constant(2.0)\ntf.scalar_summary(’z’, x + y)\nmerged = tf.merge_all_summaries()\n(a)\n(b)\n(c)\n(d)\nFig. 8: A demonstration of Tensorboard’s graph visualization\nfeatures. Figure ?? shows the complete graph, while Figure\n8b displays the expansion of the ﬁrst layer. Figures 8c and 8d\ngive examples for scalar and history summaries, respectively.\nwriter = tf.train.SummaryWriter(’/tmp/log’, graph)\nwith tf.Session(graph=graph):\nfor step in range(1000):\nwriter.add_summary(\nmerged.eval(), global_step=step)\nVI. COMPARISON WITH OTHER DEEP LEARNING\nFRAMEWORKS\nNext to TensorFlow, there exist a number of other open\nsource deep learning software libraries, the most popular being\nTheano, Torch and Caffe. In this section, we explore the\nqualitative as well as quantitative differences between Ten-\nsorFlow and each of these alternatives. We begin with a “high\nlevel” qualitative comparison and examine where TensorFlow\ndiverges or overlaps conceptually or architecturally. Then, we\nreview a few sources of quantitative comparisons and state as\nwell as discuss their results.\nA. Qualitative Comparison\nThe following three paragraphs compare Theano, Torch\nand Caffe to TensorFlow, respectively. Table II provides an\noverview of the most important talking points.\n1) Theano: Of the three candidate alternatives we discuss,\nTheano, which has a Python frontend, is most similar to Ten-\nsorFlow. Like TensorFlow, Theano’s programming model is\ndeclarative rather than imperative and based on computational\ngraphs. Also, Theano employs symbolic differentiation, as\ndoes TensorFlow. However, Theano is known to have very\nlong graph compile times as it translates Python code to\nC++/CUDA [5]. In part, this is due to the fact that Theano\napplies a number of more advanced graph optimization algo-\nrithms [5], while TensorFlow currently only performs common\nsubgraph elimination. Moreover, Theano’s visualization tools\nare very poor in comparison to TensorBoard. Next to built-in\nfunctionality to output plain text representations of the graph\nor static images, a plugin can be used to generate slightly in-\nteractive HTML visualizations. However, it is nowhere near as\npowerful as TensorBoard. Lastly, there is also no (out-of-the-\nbox) support for distributing the execution of a computational\ngraph, while this is a key feature of TensorFlow.\n2) Torch: One of the principle differences between Torch\nand TensorFlow is the fact that Torch, while it has a C/CUDA\nbackend, uses Lua as its main frontend. While Lua(JIT) is\none of the fastest scripting languages and allows for rapid\nprototyping and quick execution, it is not yet very mainstream.\nThis implies that while it may be easy to train and develop\nmodels with Torch, Lua’s limited API and library ecosystem\ncan make industrial deployment harder compared to a Python-\nbased library such as TensorFlow (or Theano). Besides the\nlanguage aspect, Torch’s programming model is fundamen-\ntally quite different from TensorFlow. Models are expressed\nin an imperative programming style and not as declarative\ncomputational graphs. This means that the programmer must,\nin fact, be concerned with the order of execution of operations.\nIt also implies that Torch does not use symbol-to-symbol,\nbut rather symbol-to-number differentiation requiring explicit\nforward and backward passes to compute gradients.\n3) Caffe: Caffe is most dissimilar to TensorFlow — in var-\nious ways. While there exist high-level MATLAB and Python\nfrontends to Caffe for model creation, its main interface is\nreally the Google Protobuf “language” (it is more a fancy,\ntyped version of JSON), which gives a very different expe-\nrience compared to Python. Also, the basic building blocks\nin Caffe are not operations, but entire neural network layers.\nIn that sense, TensorFlow can be considered fairly low-level\nin comparison. Like Torch, Caffe has no notion of a compu-\ntational graphs or symbols and thus computes derivatives via\nthe symbol-to-number approach. Caffe is especially well suited\nfor development of convolutional neural networks and image\nrecognition tasks, however it falls short in many other state-of-\nthe-art categories supported well by TensorFlow. For example,\nCaffe, by design, does not support cyclic architectures, which\nform the basis of RNN, LSTM and other models. Caffe has\nno support for distributed execution26.\nLibrary\nFrontends\nStyle\nGradients\nDistributed\nExecution\nTensorFlow\nPython, C++†\nDeclarative\nSymbolic\n✓‡\nTheano\nPython\nDeclarative\nSymbolic\n×\nTorch\nLuaJIT\nImperative\nExplicit\n×\nCaffe\nProtobuf\nImperative\nExplicit\n×\n† Very limited API.\n‡ Starting with TensorFlow 0.8, released in April 2016 [16].\nTABLE II: A table comparing TensorFlow to Theano, Torch\nand Caffe in several categories.\nB. Quantitative Comparison\nWe will now review three sources of quantitative compar-\nisons between TensorFlow and other deep learning libraries,\nproviding a summary of the most important results of each\nwork. Furthermore, we will brieﬂy discuss the overall trend\nof these benchmarks.\nThe ﬁrst work, [20], authored by the Bosch Research\nand Technology Center in late March 2016, compares the\nperformance of TensorFlow, Torch, Theano and Caffe (among\nothers) with respect to various neural network architectures.\nTheir setup involves Ubuntu 14.04 running on an Intel Xeon\nE5-1650 v2 CPU @ 3.50 GHz and an NVIDIA GeForce GTX\nTitan X/PCIe/SSE2 GPU. One benchmark we ﬁnd noteworthy\ntests the relative performance of each library on a slightly\nmodiﬁed reproduction of the LeNet CNN model [21]. More\nspeciﬁcally, the authors measure the forward-propagation time,\nwhich they deem relevant for model deployment, and the\nback-propagation time, important for model training. We have\nreproduced an excerpt of their results in Table III, where we\nshow their outcomes on (a) a CPU running 12 threads and (b)\na GPU. Interestingly, for (a), TensorFlow ranks second behind\nTorch in both the forward and backward measure while in (b)\nTensorFlow’s performance drops signiﬁcantly, placing it last\nin both categories. The authors of [20] note that one reason\nfor this may be that they used the NVIDIA cuDNN v2 library\nfor their GPU implementation with TensorFlow while using\ncuDNN v3 for the others. They state that as of their writing,\n26https://github.com/BVLC/caffe/issues/876\nLibrary\nForward (ms)\nBackward (ms)\nTensorFlow\n16.4\n50.1\nTorch\n4.6\n16.5\nCaffe\n33.7\n66.4\nTheano\n78.7\n204.3\n(a) CPU (12 threads)\nLibrary\nForward (ms)\nBackward (ms)\nTensorFlow\n4.5\n14.6\nTorch\n0.5\n1.7\nCaffe\n0.8\n1.9\nTheano\n0.5\n1.4\n(b) GPU\nTABLE III: This table shows the benchmarks performed\nby [20], where TensorFlow, Torch, Caffe and Theano are\ncompared on a LeNet model reproduction [21]. IIIa shows\nthe results performed with 12 threads each on a CPU, while\nIIIb gives the outcomes on a graphics chips.\nLibrary\nForward (ms)\nBackward (ms)\nTensorFlow\n26\n55\nTorch\n25\n46\nCaffe\n121\n203\nTheano\n–\n–\nTABLE IV: The result of Soumith Chintala’s benchmarks for\nTensorFlow, Torch and Caffe (not Theano) on an AlexNet\nConvNet model [22], [23].\nthis was the recommended conﬁguration for TensorFlow27.\nThe second source in our collection is the convnet-\nbenchmarks repository on GitHub by Soumith Chintala [22],\nan artiﬁcial intelligence research engineer at Facebook. The\ncommit we reference28 is dated April 25, 2016. Chintala\nprovides an extensive set of benchmarks for a variety of\nconvolutional network models and includes many libraries,\nincluding TensorFlow, Torch and Caffe in his measurements.\nTheano is not present in all tests, so we will not review its\nperformance in this benchmark suite. The author’s hardware\nconﬁguration is a 6-core Intel Core i7-5930K CPU @ 3.50GHz\nand an NVIDIA Titan X graphics chip running on Ubuntu\n14.04. Inter alia, Chintala gives the forward and backward-\npropagation time of TensorFlow, Torch and Caffe for the\nAlexNet CNN model [23]. In these benchmarks, TensorFlow\nperforms second-best in both measures behind Torch, with\nCaffe lagging relatively far behind. We reproduce the relevant\nresults in Table IV.\nLastly, we review the results of [5], published by the\nTheano development team on May 9, 2016. Next to a set\nof benchmarks for four popular CNN models, including the\naforementioned AlexNet architecture, the work also includes\nresults for an LSTM network operating on the Penn Treebank\n27As of TensorFlow 0.8, released in April 2016 and thus after the publi-\ncation of [20], TensorFlow now supports cuDNN v4, which promises better\nperformance on GPUs than cuDNN v3 and especially cuDNN v2.\n28Commit sha1 hash: 84b5bb1785106e89691bc0625674b566a6c02147\n2\n4\n6\n8\n10\n12\n14\n16\n18\n103 words/sec\nTheano\nTorch\nTensorFlow\nSmall\nLarge\nFig. 9: The results of [5], comparing TensorFlow, Theano and\nTorch on an LSTM model for the Penn Treebank dataset [24].\nOn the left the authors tested a small model with a single\nhidden layer and 200 units; on the right they use two layers\nwith 650 units each.\ndataset [24]. Their benchmarks measure words processed per\nsecond for a small model consisting of a single 200-unit hidden\nlayer with sequence length 20, and a large model with two 650-\nunit hidden layers and a sequence length of 50. In [5] also a\nmedium-sized model is tested, which we ignore for our review.\nThe authors state a hardware conﬁguration consisting of an\nNVIDIA Digits DevBox with 4 Titan X GPUs and an Intel\nCore i7-5930K CPU. Moreover, they used cuDNN v4 for all\nlibraries included in their benchmarks, which are TensorFlow,\nTorch and Theano. Results for Caffe are not given. In their\nbenchmarks, TensorFlow performs best among all three for\nthe small model, followed by Theano and then Torch. For\nthe large model, TensorFlow is placed second behind Theano,\nwhile Torch remains in last place. Table 9 shows these results,\ntaken from [5].\nWhen TensorFlow was ﬁrst released, it performed poor on\nbenchmarks, causing disappointment within the deep learning\ncommunity. Since then, new releases of TensorFlow have\nemerged, bringing with them improving results. This is re-\nﬂected in our selection of works. The earliest of the three\nsources, [20], published in late March 2016, ranks TensorFlow\nconsistently uncompetitive compared to Theano, Torch and\nCaffe. Released almost two months later, [22] ranks Tensor-\nFlow comparatively better. The latest work reviewed, [5], then\nplaces TensorFlow in ﬁrst or second place for LSTMs and also\nother architectures discussed by the authors. We state that one\nreason for this upward trend is that [5] uses TensorFlow with\ncuDNN v4 for its GPU experiments, whereas [20] still used\ncuDNN v2. While we predict that TensorFlow will improve its\nperformance on measurements similar to the ones discussed in\nthe future, we believe that these benchmarks — also today —\ndo not make full use of TensorFlow’s potential. The reason\nfor this is that all tests were performed on a single machine.\nAs we reviewed in depth in section III-B, TensorFlow was\nbuilt with massively parallel distributed computing in mind.\nThis ability is currently unique to TensorFlow among the\npopular deep learning libraries and we estimate that it would\nbe advantageous to its performance, particularly for large-scale\nmodels. We thus hope to see more benchmarks in literature in\nthe future, making better use of TensorFlow’s many-machine,\nmany-device capabilities.\nVII. USE CASES OF TENSORFLOW TODAY\nIn this section, we investigate where TensorFlow is already\nin use today. Given that TensorFlow was released only little\nover 6 months ago as of this writing, its adoption in academia\nand industry is not yet widespread. Migration from an existing\nsystem based on some other library within small and large\norganizations necessarily takes time and consideration, so this\nis not unexpected. The one exception is, of course, Google,\nwhich has already deployed TensorFlow for a variety of learn-\ning tasks [19], [25]–[28]. We begin with a review of selected\nmentions of TensorFlow in literature. Then, we discuss where\nand how TensorFlow is used in industry.\nA. In Literature\nThe ﬁrst noteworthy mention of TensorFlow is [29], pub-\nlished by Szegedy, Ioffe and Vanhoucke of the Google Brain\nTeam in February 2016. In their work, the authors use\nTensorFlow to improve on the Inception model [19], which\nachieved best performance at the 2014 ImageNet classiﬁcation\nchallenge. The authors report a 3.08% top-5 error on the\nImageNet test set.\nIn [25], Ramsundar et al. discuss massively “multitask\nnetworks for drug discovery” in a joint collaboration work\nbetween Stanford University and Google, published in early\n2016. In this paper, the authors employ deep neural networks\ndeveloped with TensorFlow to perform virtual screening of\npotential drug candidates. This is intended to aid pharmaceu-\ntical companies and the scientiﬁc community in ﬁnding novel\nmedication and treatments for human diseases.\nAugust and Ni apply TensorFlow to create recurrent neural\nnetworks for optimizing dynamic decoupling, a technique for\nsuppressing errors in quantum memory [30]. With this, the\nauthors aim to preserve the coherence of quantum states,\nwhich is one of the primary requirements for building universal\nquantum computers.\nLastly, [31] investigates the use of sequence to sequence\nneural translation models for natural language processing of\nmultilingual media sources. For this, Barzdins et al. use\nTensorFlow with a sliding-window approach to character-level\nEnglish to Latvian translation of audio and video content. The\nauthors use this to segment TV and radio programs and cluster\nindividual stories.\nB. In Industry\nAdoption of TensorFlow in industry is currently limited only\nto Google, at least to the extent that is publicly known. We\nhave found no evidence of any other small or large corporation\nstating its use of TensorFlow. As mentioned, we link this to\nTensorFlow’s late release. Moreover, it is obvious that many\ncompanies would not make their machine learning methods\npublic even if they do use TensorFlow. For this reason, we\nwill review uses of TensorFlow only within Google, Inc.\nRecently, Google has begun augmenting its core search ser-\nvice and accompanying PageRank algorithm [32] with a sys-\ntem called RankBrain [33], which makes use of TensorFlow.\nRankBrain uses large-scale distributed deep neural networks\nfor search result ranking. According to [33], more than 15\npercent of all search queries received on www.google.com\nare new to Google’s system. RankBrain can suggest words\nor phrases with similar meaning for unknown parts of such\nqueries.\nAnother area where Google applies deep learning with\nTensorFlow is smart email replies [27]. Google has inves-\ntigated and already deployed a feature whereby its email\nservice Inbox suggests possible replies to received email.\nThe system uses recurrent neural networks and in particular\nLSTM modules for sequence-to-sequence learning and natural\nlanguage understanding. An encoder maps a corpus of text to\na “thought vector” while a decoder synthesizes syntactically\nand semantically correct replies from it, of which a selection\nis presented to the user.\nIn [26] it is reported how Google employs convolutional\nneural networks for image recognition and automatic text\ntranslation. As a feature integrated into its Google Translate\nmobile app, text in a language foreign to the user is ﬁrst\nrecognized, then translated and ﬁnally rendered on top of the\noriginal image. In this way, for example, street signs can be\ntranslated. [26] notes especially the challenge of deploying\nsuch a system onto low-end phones with slow network con-\nnections. For this, small neural networks were used and trained\nto discover only the most essential information in order to\noptimize available computational resources.\nLastly, we make note of the decision of Google DeepMind,\nan AI division within Google, to move from Torch7 to\nTensorFlow [28]. A related source, [17], states that DeepMind\nmade use of TensorFlow for its AlphaGo29 model, alongside\nGoogle’s newly developed Tensor Processing Unit (TPU),\nwhich was built to integrate especially well with TensorFlow.\nIn a correspondence of the authors of this paper with a member\nof the Google DeepMind team, the following four reasons\nwere revealed to us as to why TensorFlow is advantageous to\nDeepMind:\n1) TensorFlow is included in the Google Cloud Platform30,\nwhich enables easy replication of DeepMind’s research.\n2) TensorFlow’s support for TPUs.\n3) TensorFlow’s main interface, Python, is one of the core\nlanguages at Google, which implies a much greater\ninternal tool set than for Lua.\n4) The ability to run TensorFlow on many GPUs.\nVIII. CONCLUSION\nWe have discussed TensorFlow, a novel open source deep\nlearning library based on computational graphs. Its ability\n29https://deepmind.com/alpha-go\n30https://cloud.google.com/compute/\nto perform fast automatic gradient computation, its inherent\nsupport for distributed computation and specialized hardware\nas well as its powerful visualization tools make it a very\nwelcome addition to the ﬁeld of machine learning. Its low-level\nprogramming interface gives ﬁne-grained control for neural net\nconstruction, while abstraction libraries such as TFLearn allow\nfor rapid prototyping with TensorFlow. In the context of other\ndeep learning toolkits such as Theano or Torch, TensorFlow\nadds new features and improves on others. Its performance\nwas inferior in comparison at ﬁrst, but is improving with new\nreleases of the library.\nWe note that very little investigation has been done in\nliterature to evaluate TensorFlow’s qualities with respect to dis-\ntributed execution. We esteem this one of its principle strong\npoints and thus encourage in-depth study by the academic\ncommunity in the future.\nTensorFlow has gained great popularity and strong support\nin the open-source community with many third-party contri-\nbutions, making Google’s move a sensible decision already.\nWe believe, however, that it will not only beneﬁt its parent\ncompany, but the greater scientiﬁc community as a whole;\nopening new doors to faster, larger-scale artiﬁcial intelligence.\nAPPENDIXI\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\" A one-hidden-layer-MLP MNIST-classifier. \"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# Import the training data (MNIST)\nfrom tensorflow.examples.tutorials.mnist import\ninput_data\nimport tensorflow as tf\n# Possibly download and extract the MNIST data set.\n# Retrieve the labels as one-hot-encoded vectors.\nmnist = input_data.read_data_sets(\"/tmp/mnist\",\none_hot=True)\n# Create a new graph\ngraph = tf.Graph()\n# Set our graph as the one to add nodes to\nwith graph.as_default():\n# Placeholder for input examples (None =\nvariable dimension)\nexamples = tf.placeholder(shape=[None, 784],\ndtype=tf.float32)\n# Placeholder for labels\nlabels = tf.placeholder(shape=[None, 10],\ndtype=tf.float32)\nweights =\ntf.Variable(tf.truncated_normal(shape=[784,\n10], stddev=0.1))\nbias = tf.Variable(tf.constant(0.1, shape=[10]))\n# Apply an affine transformation to the input\nfeatures\nlogits = tf.matmul(examples, weights) + bias\nestimates = tf.nn.softmax(logits)\n# Compute the cross-entropy\ncross_entropy = -tf.reduce_sum(labels *\ntf.log(estimates),\nreduction_indices=[1])\n# And finally the loss\nloss = tf.reduce_mean(cross_entropy)\n# Create a gradient-descent optimizer that\nminimizes the loss.\n# We choose a learning rate of 0.01\noptimizer =\ntf.train.GradientDescentOptimizer(0.5).minimize(loss)\n# Find the indices where the predictions were\ncorrect\ncorrect_predictions = tf.equal(\ntf.argmax(estimates, dimension=1),\ntf.argmax(labels, dimension=1))\naccuracy =\ntf.reduce_mean(tf.cast(correct_predictions,\ntf.float32))\nwith tf.Session(graph=graph) as session:\ntf.initialize_all_variables().run()\nfor step in range(1001):\nexample_batch, label_batch =\nmnist.train.next_batch(100)\nfeed_dict = {examples: example_batch, labels:\nlabel_batch}\nif step % 100 == 0:\n_, loss_value, accuracy_value =\nsession.run(\n[optimizer, loss, accuracy],\nfeed_dict=feed_dict\n)\nprint(\"Loss at time {0}: {1}\".format(step,\nloss_value))\nprint(\"Accuracy at time {0}:\n{1}\".format(step, accuracy_value))\nelse:\noptimizer.run(feed_dict)\nREFERENCES\n[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature,\nvol. 521, no. 7553, pp. 436–444, May 2015. [Online]. Available:\nhttp://dx.doi.org/10.1038/nature14539\n[2] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\nboltzmann\nmachines,”\nin\nProceedings\nof\nthe\n27th\nInternational\nConference on Machine Learning (ICML-10), J. FÃijrnkranz and\nT. Joachims, Eds.\nOmnipress, 2010, pp. 807–814. [Online]. Available:\nhttp://www.icml2010.org/papers/432.pdf\n[3] N.\nSrivastava,\nG.\nHinton,\nA.\nKrizhevsky,\nI.\nSutskever,\nand\nR.\nSalakhutdinov,\n“Dropout:\nA\nsimple\nway\nto\nprevent\nneural\nnetworks\nfrom\noverﬁtting,”\nJournal\nof\nMachine\nLearning\nResearch,\nvol.\n15,\npp.\n1929–1958,\n2014.\n[Online].\nAvailable:\nhttp://jmlr.org/papers/v15/srivastava14a.html\n[4] L. Rampasek and A. Goldenberg, “Tensorﬂow: Biology’s gateway to\ndeep learning?” Cell Systems, vol. 2, no. 1, pp. 12–14, 2016. [Online].\nAvailable: http://dx.doi.org/10.1016/j.cels.2016.01.009\n[5] The Theano Development Team, R. Al-Rfou, G. Alain, A. Almahairi,\nC. Angermueller, D. Bahdanau, N. Ballas, F. Bastien, J. Bayer, A. Be-\nlikov, A. Belopolsky, Y. Bengio, A. Bergeron, J. Bergstra, V. Bis-\nson, J. Bleecher Snyder, N. Bouchard, N. Boulanger-Lewandowski,\nX. Bouthillier, A. de Brébisson, O. Breuleux, P.-L. Carrier, K. Cho,\nJ. Chorowski, P. Christiano, T. Cooijmans, M.-A. Côté, M. Côté,\nA. Courville, Y. N. Dauphin, O. Delalleau, J. Demouth, G. Desjardins,\nS. Dieleman, L. Dinh, M. Ducoffe, V. Dumoulin, S. Ebrahimi Kahou,\nD. Erhan, Z. Fan, O. Firat, M. Germain, X. Glorot, I. Goodfellow,\nM. Graham, C. Gulcehre, P. Hamel, I. Harlouchet, J.-P. Heng, B. Hidasi,\nS. Honari, A. Jain, S. Jean, K. Jia, M. Korobov, V. Kulkarni, A. Lamb,\nP. Lamblin, E. Larsen, C. Laurent, S. Lee, S. Lefrancois, S. Lemieux,\nN. Léonard, Z. Lin, J. A. Livezey, C. Lorenz, J. Lowin, Q. Ma, P.-A.\nManzagol, O. Mastropietro, R. T. McGibbon, R. Memisevic, B. van\nMerriënboer, V. Michalski, M. Mirza, A. Orlandi, C. Pal, R. Pas-\ncanu, M. Pezeshki, C. Raffel, D. Renshaw, M. Rocklin, A. Romero,\nM. Roth, P. Sadowski, J. Salvatier, F. Savard, J. Schlüter, J. Schulman,\nG. Schwartz, I. Vlad Serban, D. Serdyuk, S. Shabanian, É. Simon,\nS. Spieckermann, S. Ramana Subramanyam, J. Sygnowski, J. Tanguay,\nG. van Tulder, J. Turian, S. Urban, P. Vincent, F. Visin, H. de Vries,\nD. Warde-Farley, D. J. Webb, M. Willson, K. Xu, L. Xue, L. Yao,\nS. Zhang, and Y. Zhang, “Theano: A Python framework for fast\ncomputation of mathematical expressions,” ArXiv e-prints, May 2016.\n[6] R. Collobert, S. Bengio, and J. Marithoz, “Torch: A modular machine\nlearning software library,” 2002.\n[7] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,\nJ. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay, “Scikit-learn: Machine learning in python,” J. Mach.\nLearn. Res., vol. 12, pp. 2825–2830, Nov. 2011. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=1953048.2078195\n[8] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,\nA. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,\nM. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray,\nC. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals,\nP. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng,\n“TensorFlow: Large-scale machine learning on heterogeneous systems,”\n2015, software available from tensorﬂow.org. [Online]. Available:\nhttp://tensorﬂow.org/\n[9] R. Kohavi, D. Sommerﬁeld, and J. Dougherty, “Data mining using mscr;\nlscr; cscr;++ a machine learning library in c++,” in Tools with Artiﬁcial\nIntelligence, 1996., Proceedings Eighth IEEE International Conference\non, Nov 1996, pp. 234–245.\n[10] G. Bradski, “The opencv library,” Doctor Dobbs Journal, vol. 25, no. 11,\npp. 120–126, 2000.\n[11] C. R. de Souza, “A tutorial on principal component analysis with\nthe accord.net framework,” CoRR, vol. abs/1210.7463, 2012. [Online].\nAvailable: http://arxiv.org/abs/1210.7463\n[12] A. Bifet, G. Holmes, B. Pfahringer, P. Kranen, H. Kremer, T. Jansen,\nand T. Seidl, “Moa: Massive online analysis, a framework for stream\nclassiﬁcation and clustering,” in Journal of Machine Learning Research\n(JMLR) Workshop and Conference Proceedings, Volume 11: Workshop\non Applications of Pattern Analysis.\nJournal of Machine Learning\nResearch, 2010, pp. 44–50.\n[13] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica,\n“Spark: Cluster computing with working sets,” in Proceedings of the\n2Nd USENIX Conference on Hot Topics in Cloud Computing, ser.\nHotCloud’10.\nBerkeley, CA, USA: USENIX Association, 2010, pp.\n10–10. [Online]. Available: http://dl.acm.org/citation.cfm?id=1863103.\n1863113\n[14] X. Meng, J. K. Bradley, B. Yavuz, E. R. Sparks, S. Venkataraman,\nD. Liu, J. Freeman, D. B. Tsai, M. Amde, S. Owen, D. Xin, R. Xin,\nM. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar, “Mllib:\nMachine learning in apache spark,” CoRR, vol. abs/1505.06807, 2015.\n[Online]. Available: http://arxiv.org/abs/1505.06807\n[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B. Girshick,\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\nfast feature embedding,” CoRR, vol. abs/1408.5093, 2014. [Online].\nAvailable: http://arxiv.org/abs/1408.5093\n[16] D. Murray, “Announcing tensorﬂow 0.8 â ˘A¸S now with distributed\ncomputing support!” Google Research Blog, April 2016 (accessed\nMay 22, 2016), http://googleresearch.blogspot.de/2016/04/announcing-\ntensorﬂow-08-now-with.html.\n[17] N. Jouppi, “Google supercharges machine learning tasks with tpu\ncustom chip,” Google Cloud Platform Blog, May 2016 (accessed\nMay 22, 2016), https://cloudplatform.googleblog.com/2016/05/Google-\nsupercharges-machine-learning-tasks-with-custom-chip.html.\n[18] I. G. Y. Bengio and A. Courville, “Deep learning,” 2016, book\nin\npreparation\nfor\nMIT\nPress.\n[Online].\nAvailable:\nhttp://www.\ndeeplearningbook.org\n[19] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, “Going Deeper with Convolutions,”\nArXiv e-prints, Sep. 2014.\n[20] S. Bahrampour, N. Ramakrishnan, L. Schott, and M. Shah, “Comparative\nStudy of Deep Learning Software Frameworks,” ArXiv e-prints, Nov.\n2015.\n[21] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning\napplied to document recognition,” Proceedings of the IEEE, vol. 86,\nno. 11, pp. 2278–2324, Nov 1998.\n[22] S. Chintala, “convnet-benchmarks,” GitHub, April 2016 (accessed May\n24, 2016), https://github.com/soumith/convnet-benchmarks.\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in Neural Infor-\nmation Processing Systems, 2012, p. 2012.\n[24] M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini, “Building\na large annotated corpus of english: The penn treebank,” Comput.\nLinguist., vol. 19, no. 2, pp. 313–330, Jun. 1993. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=972470.972475\n[25] B. Ramsundar, S. Kearnes, P. Riley, D. Webster, D. Konerding, and\nV. Pande, “Massively Multitask Networks for Drug Discovery,” ArXiv\ne-prints, Feb. 2015.\n[26] O. Good, “How google translate squeezes deep learning onto a\nphone,”\nGoogle\nResearch\nBlog,\nJul.\n2015\n(accessed:\nMay\n25,\n2016), http://googleresearch.blogspot.de/2015/07/how-google-translate-\nsqueezes-deep.html.\n[27] G.\nCorrado,\n“Computer,\nrespond\nto\nthis\nemail.”\nGoogle\nResearch\nBlog,\nNov.\n2015\n(accessed:\nMay\n25,\n2016),\nhttp://googleresearch.blogspot.de/2015/11/computer-respond-to-this-\nemail.html.\n[28] K.\nKavukcuoglu,\n“Deepmind\nmoves\nto\ntensorﬂow,”\nGoogle\nResearch\nBlog,\nApr.\n2016\n(accessed\nMay\n24,\n2016),\nhttp://googleresearch.blogspot.de/2016/04/deepmind-moves-to-\ntensorﬂow.html.\n[29] C. Szegedy, S. Ioffe, and V. Vanhoucke, “Inception-v4, Inception-ResNet\nand the Impact of Residual Connections on Learning,” ArXiv e-prints,\nFeb. 2016.\n[30] M. August and X. Ni, “Using recurrent neural networks to optimize\ndynamical decoupling for quantum memory,” in arXiv.org, vol. quant-\nph, no. arXiv:1604.00279.\nTechnical University of Munich, Max\nPlanck Institute for Quantum Optics, Apr. 2016. [Online]. Available:\nhttp://arxiv.org/pdf/1604.00279v1.pdf\n[31] G. Barzdins, S. Renals, and D. Gosko, “Character-Level Neural Transla-\ntion for Multilingual Media Monitoring in the SUMMA Project,” ArXiv\ne-prints, Apr. 2016.\n[32] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation\nranking: Bringing order to the web.” Stanford InfoLab, Technical Report\n1999-66, November 1999, previous number = SIDL-WP-1999-0120.\n[Online]. Available: http://ilpubs.stanford.edu:8090/422/\n[33] J.\nClark,\n“Google\nturning\nits\nlucrative\nweb\nsearch\nover\nto\nai machines,” Bloomberg Technology, Oct. 2015 (accessed: May\n25, 2016), http://www.bloomberg.com/news/articles/2015-10-26/google-\nturning-its-lucrative-web-search-over-to-ai-machines.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-10-01",
  "updated": "2016-10-01"
}