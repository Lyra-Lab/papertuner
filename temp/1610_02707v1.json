{
  "id": "http://arxiv.org/abs/1610.02707v1",
  "title": "Multi-Objective Deep Reinforcement Learning",
  "authors": [
    "Hossam Mossalam",
    "Yannis M. Assael",
    "Diederik M. Roijers",
    "Shimon Whiteson"
  ],
  "abstract": "We propose Deep Optimistic Linear Support Learning (DOL) to solve\nhigh-dimensional multi-objective decision problems where the relative\nimportances of the objectives are not known a priori. Using features from the\nhigh-dimensional inputs, DOL computes the convex coverage set containing all\npotential optimal solutions of the convex combinations of the objectives. To\nour knowledge, this is the first time that deep reinforcement learning has\nsucceeded in learning multi-objective policies. In addition, we provide a\ntestbed with two experiments to be used as a benchmark for deep multi-objective\nreinforcement learning.",
  "text": "Multi-Objective Deep Reinforcement Learning\nHossam Mossalam, Yannis M. Assael, Diederik M. Roijers, Shimon Whiteson\nDepartment of Computer Science\nUniversity of Oxford\nOxford, United Kingdom\n{ms15ham, yannis.assael, diederik.roijers, shimon.whiteson}@cs.ox.ac.uk\nAbstract\nWe propose Deep Optimistic Linear Support Learning (DOL) to solve high-\ndimensional multi-objective decision problems where the relative importances\nof the objectives are not known a priori. Using features from the high-dimensional\ninputs, DOL computes the convex coverage set containing all potential optimal\nsolutions of the convex combinations of the objectives. To our knowledge, this\nis the ﬁrst time that deep reinforcement learning has succeeded in learning multi-\nobjective policies. In addition, we provide a testbed with two experiments to be\nused as a benchmark for deep multi-objective reinforcement learning.\n1\nIntroduction\nIn recent years, advances in deep learning have been instrumental in solving a number of challeng-\ning reinforcement learning (RL) problems, including high-dimensional robot control [1–3], visual\nattention [4], solving riddles [5], the Atari learning environment (ALE) [6–14] and Go [15, 16].\nWhile the aforementioned approaches have focused on single-objective settings, many real-world\nproblems have multiple possibly conﬂicting objectives. For example, an agent that may want to\nmaximise the performance of a web application server, while minimising its power consumption [17].\nSuch problems can be modelled as multi-objective Markov decision processes (MOMDPs), and\nsolved with multi-objective reinforcement learning (MORL) [18]. Because it is typically not clear\nhow to evaluate available trade-offs between different objectives a priori, there is no single optimal\npolicy. Hence, it is desirable to produce a coverage set (CS) which contains at least one optimal\npolicy (and associated value vector) for each possible utility function that a user might have.\nSo far, deep learning methods for Markov decision processes (MDPs) have not been extended\nto MOMDPs. One reason is that it is not clear how neural networks can account for unknown\npreferences and the resulting sets of value vectors. In this paper, we circumvent this issue by taking\nan outer loop approach [19] to multi-objective reinforcement learning, i.e., we aim to learn an\napproximate coverage set of policies, each represented by a neural network, by evaluating a sequence\nof scalarised single-objective problems. In order to enable the use of deep Q-Networks [7] for learning\nin MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20].\nOLS is a generic outer loop method for solving multi-objective decision problems, i.e., it repeatedly\ncalls a single-objective solver as a subroutine. OLS terminates after a ﬁnite number of calls to that\nsubroutine and produces an approximate CS. In principle any single-objective solver can be used, as\nlong as it is OLS-compliant, i.e., produces policy value vectors rather than scalar values. Making a\nsingle-objective solver OLS-compliant typically requires little effort.\nWe present three new deep multi-objective RL algorithms. First, we investigate how the learning\nsetting effects OLS, and how deep RL can be made OLS-compliant. Using an OLS-compliant\nneural network combined with the OLS framework results in Deep OLS Learning (DOL). Our\nempirical evaluation shows that DOL can tackle multi-objective problems with much larger inputs\nthan classical multi-objective RL algorithms. We improve upon DOL by leveraging the fact that\narXiv:1610.02707v1  [cs.AI]  9 Oct 2016\nthe OLS framework solves a series of single-objective problems that become increasingly similar\nas the series progresses [21], which results in increasingly similar optimal value vectors. Deep\nQ-networks produce latent embeddings of the features of a problem w.r.t. the function value. Hence,\nwe hypothesise that we can reuse parts of the network used to solve the previous single-objective\nproblem, in order to speed up learning on the next one. This results in two new algorithms that we\ncall Deep OLS Learning with Full Reuse (DOL-FR), which reuses all parameter values of neural\nnetworks, and Deep OLS Learning with Partial Reuse (DOL-PR) which reuses all parameter values\nof neural networks, except those for the last layer of the network. We show empirically that reusing\nonly part of the network (DOL-PR) is more effective than reusing the entire network (DOL-FR) and\ndrastically improves the performance compared to DOL without reuse.\n2\nBackground\nIn a single-objective RL setting [22], an agent observes the current state st ∈S at each discrete\ntime step t, chooses an action at ∈A according to a potentially stochastic policy π, observes a\nreward signal R(st, at) = rt ∈R, and transitions to a new state st+1. Its objective is to maximise\nan expectation over the discounted return, Rt = rt + γrt+1 + γ2rt+2 + · · · , where rt is the reward\nreceived at time t and γ ∈[0, 1] is a discount factor.\nMarkov Decision Process (MDP). Such sequential decision problems are commonly modelled\nas a ﬁnite single-objective Markov decision process (MDP), a tuple of ⟨S, A, R, T, γ⟩. The Q-\nfunction of a policy π is Qπ(s, a) = E [Rt|st = s, at = a]. The optimal action-value function\nQ∗(s, a) = maxπ Qπ(s, a) obeys the Bellman optimality equation:\nQ∗(s, a) = Es′\nh\nR(s, a) + γ max\na′ Q∗(s′, a′) | s, a\ni\n.\n(1)\nDeep Q-Networks (DQN). Deep Q-learning [7] uses neural networks parameterised by θ to represent\nQ(s, a; θ). DQNs are optimised by minimising:\nLi(θi) = Es,a,r,s′\nh\n(yDQN\ni\n−Q(s, a; θi))2i\n,\n(2)\nat each iteration i, with target yDQN\ni\n= r + γ maxa′ Q(s′, a′; θ−\ni ). Here, θ−\ni are the parameters of a\ntarget network that is frozen for a number of iterations while updating the online network Q(s, a; θi)\nby gradient descent. The action a is chosen from Q(s, a; θi) by an action selector, which typically\nimplements an ϵ-greedy policy that selects the action that maximises the Q-value with a probability of\n1−ϵ and chooses randomly with a probability of ϵ. DQN uses experience replay [23]: during learning,\nthe agent builds a dataset of episodic experiences and is then trained by sampling mini-batches of\nexperiences. Experience replay is used in [7] to reduce variance by breaking correlation among the\nsamples, whilst, it enables re-use of past experiences for learning.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nw1\n4\n6\n8\n10\nVw\nV1\nV2\nV3\nV ∗\nS (w)\nCorner w\nFigure 1:\nThe two corner weights of a\nV ∗\nS (w) with S containing three value vec-\ntors for a 2-objective MOMDP.\nMulti-Objective MDPs (MOMDP). An MOMDP, is an\nMDP in which the reward function R(st, at) = rt ∈Rn\ndescribes a vector of n rewards, one for each objective [18].\nWe use bold variables to denote vectors. The solution to\nan MOMDP is a set of policies called a coverage set, that\ncontains at least one optimal policy for each possible pref-\nerence, i.e., utility or scalarisation function, f, that a user\nmight have. This scalarisation function maps each possible\npolicy value vector, Vπ onto a scalar value. In this paper,\nwe focus on the highly prevalent case where the scalarisa-\ntion function, is linear, i.e., f(Vπ, w) = w·Vπ, where w\nis a vector that determines the relative importance of the\nobjectives, such that f(Vπ, w) is a convex combination\nof the objectives. The corresponding coverage set is called\nthe convex coverage set (CCS) [18].\nOptimistic Linear Support (OLS). OLS takes an outer loop approach in which the CCS is incre-\nmentally constructed by solving a series of scalarised, i.e., single-objective, MDPs for different linear\nscalarisation vectors w. This enables the use of DQNs as a single-objective MDP solver. In each\n2\niteration, OLS ﬁnds one policy by solving a scalarised MDP, and its value vector Vπ is added to an\nintermediate approximate coverage set, S.\nUnlike other outer loop methods, OLS uses the concept of corner weights to pick the weights to\nuse for creating scalarised instances and the concept of estimated improvement to prioritise those\ncorner weights. To deﬁne corner weights, we ﬁrst deﬁne the scalarised value function V ∗\nS (w) =\nmaxV π∈S w · Vπ, as a function of the linear scalarisation vector w, for a set of value vectors S.\nV ∗\nS (w) for an S containing three value vectors is depicted in Figure 1. V ∗\nS (w) forms a piecewise linear\nand convex function that comprise the upper surface of the scalarised values of each value vector. The\ncorner weights are the weights at the corners of the convex upper surface [24], marked with crosses\nin the ﬁgure. OLS always selects the corner weight w that maximises an optimistic upper bound on\nthe difference between V ∗\nS (w) and the optimal scalarised value function, i.e., V ∗\nCCS(w) −V ∗\nS (w),\nand solves the single-objective MDP scalarized by the selected w.\nIn the planning setting for which OLS was devised, such an upper bound can typically be computed\nusing upper bounds on the error with respect to the optimal value of the scalarised policy values at\neach previous w in the series, in combination with linear programs. The error bounds at the previous\nw stem from the approximation quality of the single-objective planning methods that OLS uses.\nHowever, in reinforcement learning, the true CCS is fundamentally unknown and no upper bounds\ncan be given on the approximation quality of deep Q-learning. Therefore, we use V ∗\nCCS(w)−V ∗\nS (w)\nas a heuristic to determine the priority, where V ∗\nCCS(w) is deﬁned as maximal attainable scalarised\nvalue if we assume that the values found for previous w in the series were optimal for those w.\n3\nMethodology\nIn this section, we propose our algorithms for MORL that employ deep Q-learning. Firstly, we\npropose our basic deep OLS learning (DOL) algorithm; we build off the OLS framework for multi-\nobjective learning and integrate DQN. Then, we improve on this algorithm by introducing Deep OLS\nLearning with Partial (DOL-PR) and Full Reuse (DOL-FR). DOL, DOL-PR, and DOL-FR make use\nof a single-objective subroutine, which is deﬁned together with DOL in Section 3.1.\n3.1\nDeep OLS Learning (DOL)\nThere are two requirements to make use of the OLS framework. We ﬁrst need a scalarized, i.e.,\nsingle-objective learning algorithm that is OLS compliant. OLS compliance entails that rather\nthan learning a single value per Q(s, a), we need a vector-valued Q-value Q(s, a). The estimates\nof Q(s, a) need to be accurate enough to determine the next corner weight in the series of linear\nscalarisation weights, w, that OLS is going to generate. To satisfy those requirements we adjust our\nneural network architectures to output a matrix of |A| × n (where n is the number of objectives)\ninstead of just |A|, and we train for an extended number of episodes.\nWe deﬁne scalarised deep Q-learning, which uses this network architecture, and optimises the\nparameters to maximise the inner product of w and the Q-values for a given w instead of the scalar\nQ-values as in standard deep Q-learning. Using scalarised deep Q-learning as a subroutine in OLS\nresults in our ﬁrst algorithm: deep OLS learning (DOL).\n3.2\nDeep OLS Learning with Full (DOL-FR) and Partial Reuse (DOL-PR)\nWhile DOL can already tackle very large MOMDPs, re-learning the parameters for the entire network\nwhen we move to the next w in the sequence is rather inefﬁcient. Fortunately, we can exploit the\nfollowing observation: the optimal value vectors (and thus optimal policies) for a scalarised MOMDP\nwith a w and a w′ that are close together, are typically close as well [21]. Because deep Q-networks\nlearn to extract the features of a problem that are relevant to the rewards of an MOMDP, we can speed\nup computation by reusing the neural network parameters that were trained earlier in the sequence.\nIn Algorithm 1, we present an umbrella version of three novel algorithms, which we denote DOL.\nThe different algorithms are obtained by setting the reuse parameter (i.e., the type of reuse) to one\nof three values: DOL (without reuse) is obtained by setting reuse to ‘none’, DOL with full reuse\n(DOL-FR) is obtained by setting reuse to ‘full’, and DOL with partial reuse (DOL-PR) is obtained\nby setting reuse to ‘partial’.\n3\nAlgorithm 1 Deep OLS Learning (with different types of reuse)\n1: function DOL(m, τ, template, reuse)\n2:\n▷Where, m – the (MOMDP) environment, τ – improvement threshold,\n3:\n▷template – speciﬁcation of DQN architecture, reuse – the type of reuse\n4:\nS = empty partial CSS\n5:\nW = empty list of explored corner weights\n6:\nQ = priority queue initialised with the extrema weights simplex with inﬁnite priority\n7:\nDQN_Models = empty table of DQNs, indexed by the weight, w, for which it was learnt\n8:\nwhile Q is not empty ∧it ≤max_it do\n9:\nw = Q.pop()\n10:\nif reuse = ‘none’ ∨DQN_Models is empty then\n11:\nmodel = a randomly initialised DQN, from a pre-speciﬁed architecture template\n12:\nelse\n13:\nmodel = copyNearestModel(w, DQN_Models)\n14:\nif reuse = ‘partial’ then reinitialise the last layer of model with random weights\n15:\nV, new_model = scalarisedDeepQLearning(m, w, model)\n16:\nW = W ∪w\n17:\nif (∃w′) w′·V > max\nU∈S w′·U then\n18:\nWdel = Wdel∪corner weights made obsolete by V from Q\n19:\nWdel = Wdel ∪{w}\n20:\nRemove Wdel from Q\n21:\nRemove vectors from S that are no longer optimal for any w after adding V\n22:\nWV = newCornerWeights(S, V)\n23:\nS = S ∪{V}\n24:\nDQN_Models[w] = new_model\n25:\nfor each w′ ∈WV do\n26:\nif estimateImprovement(w′, W, S) > τ then\n27:\nQ.add(w′)\n28:\nit ++\n29:\nreturn S, DQN_Models\nDOL-FR applies full deep Q-network reuse; we start learning for a new scalarisation weight w′,\nusing the complete network we optimised for the previous w that is closest to w′ in the sequence of\nscalarisation weights that OLS generated so far. DOL-PR applies partial deep Q-network reuse; we\ntake the same network as for full reuse, but we reinitialise the last layer of the network randomly,\nin order to escape local optima. DOL (without reuse) does no reuse whatsoever, i.e., all network\nparameters are initialised randomly at the start of each iteration.\nDOL keeps track of the partial CCS, S, to which at most one value vector will be added at each\niteration (line 4). To ﬁnd these vectors, scalarised deep Q-learning (Section 3.1) is run for different\ncorner weights. The corner weights that are not yet explored are kept in a priority queue, Q, and\nafter they have been explored, are stored in a list W (line 5 and 6). Q is initialised with the extrema\nweights and keeps track of the scalarisation weights ordered by estimated improvement. In order to\nreuse the learnt parameters in DOL-PR/FR, DOL keeps track of them along with the corner weight w\nfor which they were found in DQN_Models.\nFollowing the OLS framework, at each iteration of DOL, the weight with the highest improvement\nis popped (line 9). After selecting w, DOL now reuses the DQNs it learnt in previous iterations\n(depending on the parameter reuse). The function copyNearestModel ﬁnds the network learnt for\nthe closest weight to the current corner weight on line 13. In the case of full reuse (reuse = ‘full′),\nall parameter values are copied. In the case of partial reuse (reuse = ‘partial′), the last layer is\nreinitialised with random parameter values (line 14), and in the case of no reuse (reuse = ‘none′)\nall the network parameters are reset (line 11).\nFollowing the different types of reuse, scalarised deep Q-learning, as described in Section 3.1 is\ninvoked for the w popped off of Q on line 9. Scalarised deep Q-learning returns a value vector,\nV, corresponding to the learnt policy represented by a DQN, which is also returned (line 15). The\ncurrent corner weight is added to the list of explored weights (line 16), which is used to determine\n4\nthe priorities for subsequently discovered corner weights in the current and future iterations. If there\nis a weight vector w in the weight simplex for which the scalarised value is higher than for any of\nthe vectors in S, the value vector is added to S, and new corner weights are determined and stored\n(lines 18-27). The DQN that corresponds to V is stored in DQN_models[w]. If V does not improve\nupon S for any w, it is discarded.\nExtending S with V leads to new corner weights. These new corner weights and their estimated\nimprovement are calculated using the newCornerWeights and estimateImprovement methods\nof OLS [20]. The new corner weights are added to Q if their improvement value is greater than the\nthreshold τ (lines 25-27). Also, corner weights in Q which are made obsolete (i.e. are no longer on\nthe convex upper surface) by the new value vector are removed (line 18-19). This is repeated until\nthere are no more corner weights in Q, at which point DOL terminates.\n4\nExperimental Evaluation\nIn this section, we evaluate the performance of DOL and DOL-PR/FR. We make use of two multi-\nobjective reinforcement learning problems called mountain car (MC) and deep sea treasure (DST) .\nWe ﬁrst show, how DOL and DOL-PR/FR are able to learn the correct CSS, using direct access to the\nstate st of the problems. Then, we explore the scalability of our proposed methods, and evaluate the\nperformance of weight reuse, we create an image version of the DST problem, in which we use a\nbitmap as input for scalarised deep Q-learning.\n4.1\nSetup\nFor both the raw and image problems we follow the DQN setup of [7], employing experience replay\nand a target network to stabilise learning. We use an ϵ-greedy exploration policy with ϵ annealing\nfrom ϵ = 1 to 0.05, for the ﬁrst 2000 and 3000 episodes, respectively, and learning continues for an\nequal number of episodes. The discount factor is γ = 0.97, and the target network is reset every 100\nepisodes. To stabilise learning, we execute parallel episodes in batches of 32. The parameters are\noptimised using Adam and a learning rate of 10−3. In each experiment we average over 5 runs.\nQ(a  | s  )\nInput s t\nt\nt\nConv \n16×3×3\nConv \n32×3×3\nLinear \n800\nFigure 2: DST architecture.\nFor the raw state model we used an MLP architecture with 1\nhidden layer of 100 neurons, and rectiﬁed linear unit activations.\nTo process the 3×11×10 image inputs of Deep Sea we employed\ntwo convolutional layers of 16 × 3 × 3 and 32 × 3 × 3 and a fully\nconnected layer on top. Finally, to facilitate future research we\npublish the source-code to replicate our experiments 1.\n4.2\nMulti-Objective Mountain Car\nIn order to show that DOL, DOL-FR, and DOL-PR can learn a CCS, we ﬁrst test on the\nmulti-objective mountain car problem (MC). MC is a variant of the famous mountain car prob-\nlem introduced in [22]. In single-objective mountain car problem, the agent controls a car lo-\ncated in a valley between two hills and it tries to get the car to reach the top of the hill on\nthe right side. The car has a limited engine power, thus, the agent needs to oscillate the car\nbetween both hills until the car has gathered enough inertia that would let it reach the goal.\n1\n2\n3\n4\n5\n6\nIteration\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n Max CCS Diff\nDOL\nDOL-FR\nDOL-PR\nFigure 3: MC raw version mean\nCSS error.\nThe reward in the single-objective variant is −1 for all time steps\nand 0 when the goal is reached. Our multi-objective variant adds\nanother reward which is the fuel consumption for each time step,\nwhich is proportional to the force exerted by the car. In MC, there\nare only 2 value vectors in the CCS, and is thus a small problem.\nRaw version. We evaluate our proposed methods within the MC\nenvironment with the agent having direct access to the st. We em-\nploy the same neural network architecture as for DST. However,\nfor MC, we used the CCS obtained by q-table algorithm as the\ntrue CCS which was then used for Max CCS error calculations as\nthe true CCS. As it can be seen in Figure 3, the three algorithms achieve very similar results on the\n1https://github.com/hossam-mossalam/multi-objective-deep-rl\n5\nMC problem with DOL-PR achieving the least error. The algorithms learn a good approximation to\nthe CCS in 2 iterations. After that, they continue making tiny improvements to these vectors that\nare not visible on the graph. The different algorithms behave equally well, which is due to the fact\nthat for the extrema of the weight space, i.e., the ﬁrst two iterations, the optimal policies are very\ndifferent, and reuse does not contribute signiﬁcantly.\n4.3\nDeep Sea Treasure\n0.5\n28\n52\n73\n82\n90\n115\n120\n134\n143\nFigure 4: Image DST map.\nTo test the performance of our algorithms on a problem with a larger\nCCS, we adapt the well-known deep sea treasure (DST) [25] bench-\nmark for MORL. In DST, the agent controls a submarine searching\nfor treasures in a 10 × 11 grid. The state st consists of the current\nagent’s coordinates (x, y). The grid contains 10 treasures that their\nrewards increase in proportion to the distance from the starting point\ns0 = (0, 0). The agent’s action spaces is formed by navigation in four\ndirections, and the map is depicted in Figure 4.\nAt each time-step the agent gets rewarded for the two different objec-\ntives. The ﬁrst is zero unless a treasure value was received, and the\nsecond is a time penalty of −1 for each time-step. To be able to learn\na CCS instead of a Pareto front, as it was in the original work [25], we\nhave adapted the values of the treasures such that the value of the most efﬁcient policy for reaching\neach treasure is in the CCS. The rewards for both objectives were normalised between [0 −1] to\nfacilitate the learning.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTreasure Value\n0.13\n0.11\n0.09\n0.07\n0.05\n0.03\n0.01\n0.01\nTime Penalty\nDOL\nDOL-PR\ntrue CCS\nFigure\n5:\nCCS\nafter\n4000\nepisodes in DST raw version.\nRaw version. We ﬁrst evaluate our proposed methods, in a simple\nscenario, where the agent has direct access to the st. Hence, we\nemploy a simple neural network architecture, to measure the\nmaximum error in scalarised value with respect to the true CCS.\nThe true CSS is obtained by planning with an exact algorithm\non the underlying MOMDP. We refer to this error as Max CCS\nError. An analytical visualisation of measuring the true CSS and\nthe discovered CSS difference, is illustrated in Figure 5. As it can\nbe seen in Figure 6a, DOL exhibits the highest error. Contrary\nto the preliminary expectations, having access to the raw state\ninformation st does not make the feature extraction and reuse\nredundant. Furthermore, we discovered that when DOL-FR was used and the initialisation model\nalready corresponded to an optimal policy, the miss-approximation error increased signiﬁcantly, and\nless so for DOL-PR. We therefore conclude that our algorithms can efﬁciently approximate a CCS,\nand that reuse enables more accurate learning.\nImage version. Similar to the raw version, our deep convolutional architectures for the image version,\nare still able to approximate the CCS with a high accuracy. As seen in Figure 6b, the reuse methods\nshow higher performance than DOL, and DOL-PR exhibits the highest stability as well. This is\nattributed to the fact that the network has learned to encode the state-space, which paves the way\n0\n5\n10\n15\n20\nIteration\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n Max CCS Diff\nDOL\nDOL-FR\nDOL-PR\n(a) DST raw version.\n0\n5\n10\n15\n20\n25\n30\nIteration\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n Max CCS Diff\nDOL\nDOL-FR\nDOL-PR\n(b) DST image version.\n0\n5\n10\n15\n20\n25\n30\n35\nIteration\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n Max CCS Diff\n500\n2000\n4000\n10000\n(c) DST episodes vs accuracy.\nFigure 6: The Figures (a) and (b) illustrate the maximum CSS error in DST raw and image versions,\nrespectivly. The results are averaged over 5 experiments. Figure (c) shows the accuracy achieved for\ndifferent number of episodes for DOL-PR.\n6\ntowards efﬁcient learning of the Q-values. DOL-PR exhibits the highest performance, as by resetting\nthe last layer, we keep this encoded state-space, but we still allow DOL to train a new set of Q-values\nfrom scratch. We therefore conclude that DOL-PR is the preferred algorithm.\nAccuracy vs Episodes. We further investigated the effects of the number of training episodes on the\nmax CCS error. As can be seen in Figure 6c, the error is highly affected by the number of training\nepisodes. Speciﬁcally, for a small number of episodes DOL-PR is unable to providine sufﬁcient\naccuracy to build the CCS. It is interesting to note that though the error decreases up to 4000 episodes,\nat 10000 episodes the network is overﬁtting which results in lower performance.\n5\nRelated Work\nMulti-objective reinforcement learning [18, 25] has recently seen a renewed interest. Most algorithms\nin the literature [26–28] are however based on an inner loop approach, i.e., replacing the inner\nworkings of single-objective solvers to work with sets of value vectors in the innermost workings of\nthe algorithm. This is a fundamentally different approach, of which it is not clear how it could be\napplied to DQN, i.e., back-propagation cannot be transformed into a multi-objective algorithm in\nsuch a way. Other work does apply an outer loop approach but does not employ Deep RL [29–31].\nWe argue that enabling deep RL is essential for scaling up to larger problems.\nAnother popular class of MORL algorithms are heuristic policy search methods that ﬁnd a set\nof alternative policies. These are for example based on multi-objective evolutionary algorithms\n(MOEAs) [32, 33] or Pareto local search (PLS) [34]. Especially MOEAs are compatible with neural\nnetworks, but evolutionary optimisation of NNs is typically rather slow compared to back-propagation\n(which is what the deep Q-learning algorithm that we employ in this paper as a single-objective\nsubroutine uses).\nOutside of MORL, there are algorithms that are based on OLS but apply to different problem settings.\nNotably, the OLSAR algorithm [21] does planning in multi-objective partially observable MDPs\n(POMDPs), and applies reuse to the alpha matrices that it makes use of to represent the multi-objective\nvalue function. Unlike in our work, however, these alpha matrices form a guaranteed lower bound on\nthe value function and can be reused fully without affecting the necessary exploration for learning\nin later iterations. Furthermore, the variational OLS (VOLS) algorithm [35], applies OLS to multi-\nobjective coordination graphs and reuses reparameterisations of these graphs that are returned by the\nsingle-objective variational inference methods that VOLS uses as a subroutine. These variational\nsubroutines are not made OLS compliant, like the DQNs in this paper, but the value vectors are\nretrieved by a separate policy evaluation step (which would be suboptimal in the context of deep RL).\n6\nDiscussion\nIn this work, we proposed three new algorithms that enable the usage of deep Q-learning for multi-\nobjective reinforcement learning. Our algorithms build off the recent optimistic linear support\nframework, and as such tackle the problem by learning one policy and corresponding value vector\nper iteration. Further, we extend the main deep OLS learning (DOL), to take advantage of the nature\nof neural networks, and introduce full (DOL-FR) and partial (DOL-PR) parameter reuse, in between\nthe iterations, to pave the way towards faster learning.\nWe showed empirically that in problems with large inputs, our algorithms can learn CCS with high\naccuracy. For these problems DOL-PR outperforms DOL and DOL-FR, indicating that a) reuse is\nuseful, and b) doing partial reuse rather than full reuse effectively prevents the model from getting\nstuck in a policy that was optimal for a previous w. In future work, we are planning to incorporate\nearly stopping technique, and optimise our model for the accuracy requirements of OLS, while\nlowering the number of episodes required.\nAcknowledgements\nThis work is in part supported by the TERESA project (EC-FP7 grant #611153).\n7\nReferences\n[1] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. arXiv\npreprint arXiv:1504.00702, 2015.\n[2] Y. M. Assael, N. Wahlström, T. B. Schön, and M. P. Deisenroth. Data-efﬁcient learning of feedback\npolicies from image pixels using deep dynamical models. NIPS Deep Reinforcement Learning Workshop,\n2015.\n[3] M. Watter, J. T. Springenberg, J. Boedecker, and M. A. Riedmiller. Embed to control: A locally linear\nlatent dynamics model for control from raw images. In NIPS, 2015.\n[4] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In ICLR, 2015.\n[5] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson. Learning to communicate with deep multi-agent\nreinforcement learning. arXiv preprint arXiv:1605.06676, 2016.\n[6] X. Guo, S. Singh, H. Lee, R. L. Lewis, and X. Wang. Deep learning for real-time Atari game play using\nofﬂine Monte-Carlo tree search planning. In NIPS, pages 3338–3346. 2014.\n[7] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature,\n518(7540):529–533, 2015.\n[8] B. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep\npredictive models. arXiv preprint arXiv:1507.00814, 2015.\n[9] Z. Wang, N. de Freitas, and M. Lanctot. Dueling network architectures for deep reinforcement learning.\narXiv preprint 1511.06581, 2015.\n[10] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In ICLR, 2016.\n[11] H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double Q-learning. In AAAI,\n2016.\n[12] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks\nin Atari games. In NIPS, pages 2845–2853, 2015.\n[13] M. G. Bellemare, G. Ostrovski, A. Guez, P. S. Thomas, and R. Munos. Increasing the action gap: New\noperators for reinforcement learning. In AAAI, 2016.\n[14] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D. Maria, V. Panneershelvam, M. Suleyman,\nC. Beattie, S. Petersen, S. Legg, V. Mnih, K. Kavukcuoglu, and D. Silver. Massively parallel methods for\ndeep reinforcement learning. In Deep Learning Workshop, ICML, 2015.\n[15] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver. Move Evaluation in Go Using Deep Convolutional\nNeural Networks. In ICLR, 2015.\n[16] D. Silver, A. Huang, C. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lilli-\ncrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural\nnetworks and tree search. Nature, 529(7587):484–489, 2016.\n[17] G. Tesauro, R. Das, H. Chan, J. O. Kephart, C. Lefurgy, D. W. Levine, and F. Rawson. Managing\npower consumption and performance of computing systems using reinforcement learning. In NIPS 2007:\nAdvances in Neural Information Processing Systems 20, pages 1497–1504, 2007.\n[18] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley. A survey of multi-objective sequential decision-\nmaking. Journal of Artiﬁcial Intelligence Research, 47:67–113, 2013.\n[19] D. M. Roijers, S. Whiteson, and F. A. Oliehoek. Computing convex coverage sets for faster multi-objective\ncoordination. Journal of Artiﬁcial Intelligence Research, 52:399–443, 2015.\n[20] D. M. Roijers. Multi-Objective Decision-Theoretic Planning. PhD thesis, University of Amsterdam, 2016.\n[21] D. M. Roijers, S. Whiteson, and F. A. Oliehoek. Point-based planning for multi-objective POMDPs. In\nIJCAI 2015: Proceedings of the Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence,\npages 1666–1672, July 2015.\n[22] R. S. Sutton and A. G. Barto. Introduction to reinforcement learning. MIT Press, 1998.\n[23] L. Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie Mellon\nUniversity, Pittsburgh, January 1993.\n8\n[24] H.-T. Cheng. Algorithms for partially observable Markov decision processes. PhD thesis, University of\nBritish Columbia, Vancouver, 1988.\n[25] P. Vamplew, R. Dazeley, A. Berry, E. Dekker, and R. Issabekov. Empirical evaluation methods for\nmultiobjective reinforcement learning algorithms. Machine Learning, 84(1-2):51–80, 2011.\n[26] L. Barrett and S. Narayanan. Learning all optimal policies with multiple criteria. In ICML, pages 41–47,\n2008.\n[27] K. V. Moffaert and A. Nowé. Multi-objective reinforcement learning using sets of Pareto dominating\npolicies. Journal of Machine Learning Research, 15:3483–3512, 2014.\n[28] M. A. Wiering, M. Withagen, and M. M. Drugan. Model-based multi-objective reinforcement learning. In\nADPRL 2014: Proceedings of the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement\nLearning, pages 1–6, 2014.\n[29] S. Q. Yahyaa, M. M. Drugan, and B. Manderick. The scalarized multi-objective multi-armed bandit\nproblem: an empirical study of its exploration vs. exploitation tradeoff. In IJCNN 2014: Proceedings of\nthe 2014 International Joint Conference on Neural Networks, pages 2290–2297, 2014.\n[30] K. Van Moffaert, T. Brys, A. Chandra, L. Esterle, P. R. Lewis, and A. Nowé. A novel adaptive weight\nselection algorithm for multi-objective multi-agent reinforcement learning. In IJCNN 2014: Proceedings\nof the 2013 International Joint Conference on Neural Networks, pages 2306–2314, 2014.\n[31] S. Natarajan and P. Tadepalli. Dynamic preferences in multi-criteria reinforcement learning. In ICML,\n2005.\n[32] C. C. Coello, G. B. Lamont, and D. A. Van Veldhuizen. Evolutionary algorithms for solving multi-objective\nproblems. Springer Science & Business Media, 2007.\n[33] H. Handa. Solving multi-objective reinforcement learning problems by EDA-RL - acquisition of various\nstrategies. In ISDA 2009: Proceedings of the Ninth Internatonal Conference on Intelligent Sysems Design\nand Applications, pages 426–431, 2009.\n[34] C. Kooijman, M. de Waard, M. Inja, D. M. Roijers, and S. Whiteson. Pareto local policy search for\nMOMDP planning. In ESANN 2015: Special Session on Emerging Techniques and Applications in\nMulti-Objective Reinforcement Learning at the European Symposium on Artiﬁcial Neural Networks,\nComputational Intelligence and Machine Learning 2015, pages 53–58, April 2015.\n[35] D. M. Roijers, S. Whiteson, A. T. Ihler, and F. A. Oliehoek. Variational multi-objective coordination. In\nMALIC 2015: NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems, 2015.\n9\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2016-10-09",
  "updated": "2016-10-09"
}