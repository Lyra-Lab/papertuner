{
  "id": "http://arxiv.org/abs/1611.00201v1",
  "title": "Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics",
  "authors": [
    "Jay M. Wong"
  ],
  "abstract": "Despite outstanding success in vision amongst other domains, many of the\nrecent deep learning approaches have evident drawbacks for robots. This\nmanuscript surveys recent work in the literature that pertain to applying deep\nlearning systems to the robotics domain, either as means of estimation or as a\ntool to resolve motor commands directly from raw percepts. These recent\nadvances are only a piece to the puzzle. We suggest that deep learning as a\ntool alone is insufficient in building a unified framework to acquire general\nintelligence. For this reason, we complement our survey with insights from\ncognitive development and refer to ideas from classical control theory,\nproducing an integrated direction for a lifelong learning architecture.",
  "text": "Towards Lifelong Self-Supervision:\nA Deep Learning Direction for Robotics\nJay M. Wong\nAbstract\nDespite outstanding success in vision amongst other domains, many of the recent deep learning approaches have\nevident drawbacks for robots. This manuscript surveys recent work in the literature that pertain to applying deep\nlearning systems to the robotics domain, either as means of estimation or as a tool to resolve motor commands directly\nfrom raw percepts. These recent advances are only a piece to the puzzle. We suggest that deep learning as a tool\nalone is insufﬁcient in building a uniﬁed framework to acquire general intelligence. For this reason, we complement our\nsurvey with insights from cognitive development and refer to ideas from classical control theory, producing an integrated\ndirection for a lifelong learning architecture.\nKeywords\nRobotics, Deep Learning, Cognition, Autonomy, Lifelong Learning\n1\nIntroduction\nAs roboticists and scientists, is our goal to engineer a solution\nto work for a particular task in a speciﬁed domain or is it\nto build a system that has the capacity to acquire general\nintelligence (a notion characterized by Legg and Hutter\n(2007))? An historic example is that success in aviation, with\nautonomous aerial navigation does not immediately imply\nsuccess in a task like autonomous driving, which shares some\nsimilarities. Or assume that we solve autonomous driving\ntomorrow—likely an engineering effort like changing our\nhighways, roads, and infrastructure with increased sensory\nNg and Lin (2016) will not generalize especially well\nto robotic tasks in mobile manipulation. Should we then\nadd additional sensors to all possible environments (e.g.\nresidential homes) where autonomous systems are likely to\noperating in? How is it then to operate in novel, unknown,\nor disastrous environments? Or in space? In fact, it is under\ninspirations from these disastrous and unstructured domains,\nthat have given rise to recent technological advances with\nthe DARPA Robotics Challenge (e.g. Johnson et al. (2015);\nFeng et al. (2015a,b); Kohlbrecher et al. (2015); Yi et al.\n(2015); Kuindersma et al. (2016); Dellin et al. (2016)). This\nmanuscript presents rather a different direction in thinking,\nwhere instead of engineering and redesigning systems to\nperform competently in novel tasks and domains, perhaps\na system that can bootstrap its lifetime of experiences can\nquickly learn useful solutions in these new areas—we refer\nto the process by which this long-term knowledge repertoire\nis acquired as lifelong learning.\nLifelong learning should not address only novel domains,\nbut also should consider optimizing behavior at existing\ntasks. Let’s consider the following hypothetical scenario.\nAn autonomous system (e.g. robot, mobile manipulator,\nunmanned aerial vehicle (UAV), etc.) returns from a mission\nor accomplishes some task. We are now out of things to\nprovide it. Likely in many cases, the robot is left somewhere\nin corner of a laboratory until there are subsequent tasks to\naccomplish. But what if instead, it uses this downtime as an\nemergent possibility for continuous progress?—and continue\nto operate, either reﬁning its inherent representations of\nthe world (which have generally, to this date, been hand-\ndeﬁned by human operators) or optimizing its inherent\nmotion primitives (e.g. tuning internal control parameters\nthat perhaps due to wear and tear are now highly suboptimal).\nWhat if it can learn to build complex motor behaviors, that\nmay prove useful in future missions from exploiting existing\nstructure in its primitives?\nA misconception is that what we are referring to as\nlifelong learning does not necessarily imply that the system\nlearns from scratch. It is not an end-to-end approach\nfor motor development or task solving. In other words,\nit does not necessarily imply learning motor torques\ndirectly from raw sensory input. Instead, its underlying\npurpose is generalization and structural bootstrapping, a\nterm coined by Worgotter et al. (2015), where existing\nknowledge is exploited for generalization to novel activities.\nWe draw insight from cognitive development to learn\ncomplex motor behavior and structural representations\nby an account of intrinsic and extrinsic properties (e.g.\nenvironmental uncertainties) inﬂuencing the system in ways\nbeyond engineering analyses. Lifelong learning implies\nthat systems learn over a lifetime of complex tasks and\ndomains, achieving generalized solutions. This form of\ngeneralizability for both domain and task is extremely\nimportant for designing robust, high-performance systems.\nInterestingly, a study by Pinto and Gupta (2016) found\nThe Charles Stark Draper Laboratory, Inc, Cambridge, MA, USA\nCorresponding author:\nJay M. Wong, Planning, Autonomy, and Automation Group, The Charles\nStark Draper Laboratory, Inc, 555 Technology Square, Cambridge, MA,\nUSA\nEmail: jmwong@draper.com\nThis document contains 211 references.\narXiv:1611.00201v1  [cs.RO]  1 Nov 2016\n2\nthat convolutional networks achieved higher performance\ngrasping when trained on both grasping and pushing tasks\nwhen compared to grasping alone, suggesting that inter-task\nrepresentation sharing helps build a better understanding of\nthe environment overall.\nThis manuscript provides an initial survey on recent\nadvances in deep learning pertaining to the robotics\ndomain and complements this review with inspirations in\ncognitive development and control theory outlining that\nthe coalescence of these ideas may pave way for lifelong\nlearning robots. We indicate that deep learning alone is likely\nincapable of solving all problems in a uniﬁed framework.\nInstead, we discuss a connectionist approach in lifelong self-\nsupervision, drawing ideas from these other areas to tackle\nthe acquisition of general motor intelligence. We formulate\na direction by discussing how systems can intrinsically\nmotivate themselves to attack the problem of building\naccurate representations of structures in the world and the\ndevelopment of complex motor behavior simultaneously.\nThis particular direction incorporates the use of hierarchies\nof neural networks under the popularized notion of deep\nlearning. We suggest that the use of deep learning as\nan approximation tool allows robots to encode complex\nfunctions that describe physical phenomena concerning\ninteractions with the world and sensory-driven control.\n1.1\nFrom Computer Vision to Robotics\nDeep learning has exhibited major success stories in the\ncomputer vision domain. This particular tool, popularized\nby Krizhevsky et al. (2012) in the ImageNet competition,\nshowed signiﬁcant promise when learned latent feature rep-\nresentations by a neural network that incorporated a series\nof convolution, pooling, and densely connected neurons out-\nperformed existing hand crafted feature representations that\nhave otherwise been the standard. The support from compu-\ntational machinery (GPUs) allowed for efﬁcient paralleliza-\ntion necessary for training neural network structures with\nmassive datasets. Since then, the computer vision community\nhas produced a plethora of deep learning research and have\nplateaued close to human level capabilities in recognition by\nbuilding deeper and deeper networks Szegedy et al. (2015),\nﬁne tuning, and introducing extra features (e.g. surface nor-\nmals) Madai-Tahy et al. (2016). However, that is not to say\nthat these powerful, state of the art, demonstrations and their\nsolutions are immediately applicable to mobile perceptual\nsystems. There exists a number of fundamental differences\nin these two domains that hinder trivial compatibilities. First,\nthe deep learning solutions in computer vision are generally\nsupervised. Supervised learning is very constrained. And in\nmany computer vision tasks, a particular input is associated\nwith a single, correct output.\nYet quite evidently, this is not the case in robotics—robots\ndo more than classiﬁcation. They must perform actions in\nthe world. They must build representations of things they\nsense and act on these sensory signals whereas computer\nvision systems do not necessarily act. Classiﬁcation helps in\nidentifying the entities in the world, but to accomplish tasks,\nrobots must perform actions and manipulate such entities.\nThe connection between perception and action is essential in\nbuilding perceptual systems in the real world.\n1.2\nOn Overgeneralization\nAn immediate drawback for these computer vision architec-\ntures is that studies have found that image classiﬁcation has\nan unfortunate overgeneralization (“fooling”) phenomenon.\nThese classiﬁcation tasks take as input generally a single\nsensor modality, in many cases, RGB. Where deep learning\ntools fail is when adversarial RGB examples are construed\nin attempts to fool these networks into very incorrect predic-\ntions presented in studies by Szegedy et al. (2013); Nguyen\net al. (2015); Carlini and Wagner (2016). In these works, hill\nclimbing and gradient ascent methods were used to evolve\nimages to match very incorrect classes with high probability\npredicted by the network. Although there is work to make\nthese networks robust to such malicious attacks (e.g. Bendale\nand Boult (2015); Wang et al. (2016a))1, we theorize that the\naddition of multiple modalities (with more than vision alone)\nmay alleviate such intriguing and devastating phenomena, as\nthe real world obeys certain structures that are locally con-\nstraining Kurakin et al. (2016). For instance, it is increasingly\ndifﬁcult to fool a predictor that reasons with depth and tactile\ninformation with physical adversarial entities. Furthermore,\nby the universal approximator theorem Hornik et al. (1989),\nmultilayer feedforward networks are capable of representing\narbitrarily complex functions, even those that are robust to\nsuch adversarial anomalies. It becomes then a formulation\nproblem where models must be trained with an adversarial\nobjective Goodfellow et al. (2014b). Other work elaborates\nthat networks should be also realized with proper regulariza-\ntion Tanay and Grifﬁn (2016). In fact, a particular variant\nof neural networks that implicitly enforces regularization\nmay be beneﬁcial. For instance, DivNet is an approach that\nattempts to model neuronal diversity allowing for efﬁcient\nauto-pruning, in turn reducing network size and providing\ninherent regularization Mariet and Sra (2015). Another rem-\nedy realized a particular network layer called competitive\novercomplete output layer to mitigate this overgeneralization\nproblem of neural networks Kardan and Stanley (2016). Such\na layer forces outputs to explicitly compete with each other,\nresulting in tight-ﬁtting regions around the training data.\n1.3\nOn Deep Reinforcement Learning\nImpressive success stories has been shown in game domains\nthat integrate perception and action. Namely work by Google\nDeepMind has pushed this particular frontier and have\nrevolutionized the intersection between deep learning and\nits connection to reinforcement learning. Mnih et al. (2013,\n2015) built a single learning framework that could learn to\nplay a large number of Atari games beyond human level\ncompetence through a trial and error approach, where the\nonly information given to the system were several game\nframes, the game score, and a discrete control set. A\nneural network was used to approximate the Q values of a\ndiscrete set of actions from several game play frames and\nexecuted the highest valued action resulting in a competent\ngameplay policy. The idea of using a neural network as\n1This particular phenomenon is categorized under open set recognition.\nBendale and Boult (2015) proposed a new model layer, OpenMax, that\nestimates the probability of input from unknown classes and rejects fooling\nadversarial examples.\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n3\nan approximation to a value function is not something\nprofound and novel. Dating back two and a half decades\nago, Tesauro attempted to tackle the game Backgammon—\na board game that had approximately 1020 states, making\ntraditional table-based reinforcement learning infeasible.\nInstead, a backpropagation layered neural network was\nused to approximate the value function describing board\npositions and probabilities of victory. It was shown in various\nincarnations of his algorithm where both raw encodings and\nhand-crafted features derived from human task knowledge\nwere used to learn competent gameplay policies Tesauro\n(1992, 1995a,b). Furthermore, other researchers in the past\nhave leveraged recurrent neural networks to learn Q values\nusing a history of features to form policies Schmidhuber\n(1991); Lin and Mitchell (1992); Meeden et al. (1993).\nRecently, the defeat of 18-time Go world champion, Lee\nSedol, by DeepMind’s AlphaGo system established a major\nmilestone for deep learning frameworks. Their success was\nnot simply attributed to deep reinforcement learning but also\nclever integration with Monte Carlo tree search Silver et al.\n(2016)—it is to show that deep learning alone may not be\nthe solution to all problems, but as a tool, deep learning may\nused in complement with other algorithms to produce very\npowerful results.\n1.4\nOn Domain Transfer\nUnfortunately, a reason why many of these game domains\nare successful is that the domain is fully observable. Despite\nthere being some studies that inject partial observability\nand attempt to tackle this problem with augmented memory\nstructure Heess et al. (2015a), the algorithms developed\nthrough gameplay should not be considered as game-\nchanging success stories in physical dynamical systems. The\nreal world obeys physics and uncertainties that can not be\nperfectly modeled in simulation, and as a result policies\nlearned in simulation have difﬁculty generalizing to real\nrobot systems. For instance, despite millions of training\nsteps in over hundreds of hours of simulations, visuomotor\npolicies learned on a Baxter simulator fails when given real\nworld observations on the physical platform Zhang et al.\n(2015). Interestingly, James and Johns (2016) were able\nto transfer visuomotor policies learned in simulations to a\nreal system, however, their approach required massaging the\nscene by occluding complex areas with a physical black\nbox, hiding wires, and mimicking the simulation setting. The\nintroduction of progressive neural networks show promise by\nexploiting deep composition of features amongst columns of\ndomain-speciﬁc networks. However, immediate drawbacks\nare that it assumes known task boundaries and exhibits\nquadratic parameter growth when a new column is necessary\nfor each novel domain. Despite alleviation to the cause,\nthese networks still need to be trained in the new domain\nto achieve competence Rusu et al. (2016a). In later works,\nRusu et al. (2016b), demonstrated domain transfer using\nfeatures learning in MuJoCo simulation with a Kinova arm\nto the physical system—the reaching task they showed,\nhowever, was highly constrained in a small region of static\nspace and still require several hours of training in the real\nworld. A generalized method for domain alignment has\nrecently been presented to mitigate performance loss when\nadapting robot visuomotor representations from synthetic\nto real environments Tzeng et al. (2015). The technique,\nhowever, requires paired synthetic and real views of the same\nscene to adapt the deep visual representations.\n1.5\nOn Self-Supervision\nAs a result, there is no escaping the fact that robots\nmust collect their own training data in the real world—\nto tackle this, various approaches by Pinto and Gupta\n(2015); Levine et al. (2016); Wong et al. (2016) have been\nproposed for self-supervision. These studies hint at methods\nin which robots can label their own experiences and collect\npotentially massive datasets pertaining to a single task.\nHowever, they provide no notion of learning beyond the\ntask at hand—conceptually they are incapable of exhibiting\nlifelong learning. To address this, we suggest a direction in\nwhich robots acquire completely unsupervised visuomotor\nskills derived from a basis of inherent primitives that are\nreinforced by the world to generate behaviors that adhere\nto physical properties of the environment. This hierarchical\nset of motor behaviors should then be coupled with an\nintrinsically motivated structure learning module to allow\ncontinuous affordance and interaction outcome prediction\nregarding entities in the world—as a result, this produces\npermanent artifacts that can be reused for future tasks.\nFurthermore, the intrinsic motivator must both promise the\nacquisition of continuously reﬁned forward models and the\ncapacity for the development of arbitrarily complex motor\nbehaviors.\nThis paper is in agreement with Silver et al. (2013) in\nwhich they address the machine learning community that we\nneed to seriously consider the nature of systems that have\ncapacity to learn over a lifetime of experiences rather than for\nsome speciﬁed task or domain. Conceptually, this direction\nof thinking is relatable to the notion of deep developmental\nlearning primarily proposed by Sigaud and Droniou (2016)\nin which ﬁrst sensory motor control must transform raw\nsensations to a predictive process—we refer to this as a\nforward affordance model. They also outlined the challenges\nof integrating behavioral optimization and a curiosity\nmechanism for deep developmental systems. We suggest\nto attack these simultaneously through the continuously\nreﬁnement of motion primitives and the exploitation of their\ncombinatoric sequences and compositions to achieve motor\ndevelopment. To address the latter, we suggest instrinsic\nmotivators driven by information theoretic measures in\nregards to the forward affordance model’s predictions of\nworld state. Lastly, we quickly address a certain debate\ndating back two decades between planning and reﬂexive\narchitectures for the design of robot behavior Brooks\n(1987a,b). While we agree that hierarchical behavioral\nresponses eliminates the need for planning, we take a stand\nthat is much similar to our ideologies with deep learning—\nthat is, to reiterate, a grandiloquent singular architecture is\nlikely infeasible in many senses. Behavioral responses need\nnot be learned when set rules and instructions are provided\n(e.g. autonomous missions and manuals where there are\nprecise trajectories through the task)—perhaps this is where\nwe should consider planning for task solving. As such, we\nﬁrmly believe that there is no single individual or rather, what\nRodney Brooks calls theists, that is truly correct. Instead,\nwe ﬁnd that excitement is in building a system that marries\n4\nthe many theisms and relates technologies that are both\nclassical and of recent “hype.” As such, this leads us to\na unifying, perhaps even holistic, direction in thinking. To\nour knowledge, this paper is the ﬁrst attempt at outlining\na lifelong learning direction by integrating deep learning,\ncognitive development, and classical control theory.\n2\nImplications for Robotics\nIn the most simplifying sense, deep learning is an algorithmic\ntool that leverages neural networks as nonlinear function\napproximators in which weighted connections between\ninput and output neurons are trained via error back\npropagation. Doing so, encodes a function that minimizes\nthe disparity between prediction and truth by building latent\nrepresentations in the hidden layers. The deep learning\ndomain has been quickly exploding since its large success\nin vision popularized by the outstanding results in the\nImageNet competition Krizhevsky et al. (2012), producing\na plethora of general reviews on deep neural networks.\nAs a result, we omit the basics of neural networks,\nconvolutions, autoencoding, regularization, recurrency, and\nrelated concepts in this manuscript. The reader is referred\nto the following general reviews Bengio (2009); LeCun\net al. (2015); Goodfellow et al. (2016) for a thorough\noverview. Instead, we will critique a number of deep learning\nframeworks most applicable to the robotics domains and\noutline the drawbacks and skepticism that arise with these\nrecent works.\n2.1\nDetection, Estimation, and Tracking\nTools that have been popularized by the computer vision\ncommunity has generally been leveraged to tackle individual\ncomponents in the robotics domain. A number of these\nindividual triumphs used neural networks as a means of\napproximating otherwise very complex and highly nonlinear\nfunctions pertaining to estimation and scene understanding.\nFor instance, tasks relating to rule-based navigation like\nautonomous driving in particular may require understanding\nthe identities of objects in the world. Labeling and scene\nunderstanding can be regarded as a segmentation problem\ngiven visual input. As such, SegNet, a semantic pixel-wise\nsegmentation encoder-decoder network, was presented to\nachieve competitive predictive capability Badrinarayanan\net al. (2015). Other methods also attempted to solve a\nsimilar task but were either generating object proposals Noh\net al. (2015); Hariharan et al. (2015) or required multi-stage\ntraining Socher et al. (2011); Zheng et al. (2015). Still,\nthese segementation results were shown to be especially\nrobust for detection problems. In particular, Pinheiro et al.\n(2015) built a system to predict segmentation masks given\ninput patches by using DeepMask, a neural network that\ngenerated such proposals. These proposals were then passed\nto an object classiﬁer, producing state of the art segmentation\nresults. Extensions to this work gave way to a bottom-up/top-\ndown segmentation reﬁnement network that was capable of\ngenerating high ﬁdelity object masks with a 50% speedup.\nThe network, SharpMask leveraged features from all layers\nof the network by ﬁrst generating a course mask prediction\nand reﬁning this mask in a top-down fashion Pinheiro et al.\n(2016).\nStudies have shown that pre-trained convolutional neural\nnetwork features were useful for RGB-D object recognition\nand pose estimation Schwarz et al. (2015). A consequence\nof this became a ﬂurry of research regarding using\ndeep architectures for detection and pose estimation. An\napproach for the detection of pedestrians was shown using\nan unsupervised multi-stage feature learning approach by\nSermanet et al. (2013). Meanwhile, results indicated high\naccuracy in human pose estimation with DeepPose—likely\nthis is due to deep neural networks capturing context and\nreasoning in a holistic manner Toshev and Szegedy (2014).\nPoseNet, a convolutional neural network camera pose\nregressor, was presented with impressive robustness to\ndifﬁcult lighting, motion blur, and different camera intrinsics\nKendall et al. (2015). This was later extended to a Bayesian\nmodel able to provide localization uncertainty Kendall and\nCipolla (2015), establishing a critical step forward for\nmobile robots, especially connecting close ties to algorithms\nthat operate under uncertainty. In work by Wilkinson and\nTakahashi (2015), a pretrained convolutional network was\nused to predict object descriptions and aspect deﬁnitions\npertaining to sensory geometries in relation to objects.\nUnfortunately, class and object descriptors were selected\nas arbitrary pretrained AlexNet layers and the overall\nframework relied on a number of thresholds that are difﬁcult\nto deﬁne.\nOthers continue to investigate the use of these tools to\nlearn useful features for contexts like laser based odometry\nestimation Nicolai et al. (2016). In research by Byravan and\nFox (2016), deep networks were used to segment rigid bodies\nin the scene and predict motions of these entities in SE3.\nAs hyperparametric approximators, these networks have\nbeen found success in the tracking regime in which recurrent\nneural networks were used to ﬁlter raw laser measurements\nand shown to infer object location and identify in both\nvisible and occluded scenes Ondruska and Posner (2016).\nThis technique is described as a neural network analogous\nto Bayesian ﬁltering. In addition, by learning to track\nwith a large set of unsupervised data, a new task like\nsemantic classiﬁcation could be learned by exploiting rich\ninternal structure through inductive transfer Ondruska et al.\n(2016). An approach was presented by Song and Xiao\n(2015) where 3D bounding boxes of objects were generated\nthrough a methodology they call Deep Sliding Shapes. Given\nKinect images, they learned a multiscale 3D region proposal\nnetwork that is fully convolutional and identiﬁes interesting\nregions in the scene. Then, an object recognition network\nwas learned to perform 3D box regressions.\nA sensory-fusion architecture that incorporated the use of\nLSTMs to capture temporal dependencies has been presented\nto anticipate and fuse information from multiple sensory\nmodalities. This Fusion RNN was demonstrated as part of\na maneuver anticipation pipeline that outperformed state of\nthe art on a benchmark consisting of a dataset of 1180 miles\nof natural driving Jain et al. (2016). Similarly, Krishnan et al.\n(2015) developed a deep network capable of approximating\na broad class of Kalman ﬁlters, enhancing them to arbitrarily\ncomplex transition dynamics and emission distributions.\nYet, despite outstanding results in classiﬁcation, identi-\nﬁcation and pose estimation, and semantic segmentation,\nsystems that perform actions in the world still require a\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n5\nconnection from detected entities in space to motor com-\nmands. In part, these research solutions only attempt to\ndevelop robust perceptual interfaces to autonomous systems,\nbut however, a key, perhaps, paramount module is one that\nreasons over sensations and executes useful motor control.\nAs such, perception alone may not be the answer, but\nsomewhere in the intersection of perception, cognition, and\naction.\n2.2\nFrom Perception to Motor Control\nA number of studies looked into introducing the predictive\npower of neural networks in place of traditional feature\nextracting perceptual pipelines to solve detection and control\nproblems with physical robot experiments. In particular,\nin place of hand-designed features like those of Kragic\nand Christensen (2003); Maitin-Shepard et al. (2010) for\ngrasping, Lenz et al. (2015) presented a deep architecture\nto learn useful feature representations for grasp detection.\nA two-step cascaded network system was shown where\ntop detections were re-evaluated by the second network,\nallowing for quick pruning of unlikely candidate grasps.\nThe network operated on RGB-D input and successfully\ngeneralized to execute grasps on both a Baxter and PR2\nrobot. Likewise, a grasp detection system was demonstrated\nby Wang et al. (2016b), that mapped RGB-D images to\ngripper grasping pose by ﬁrst segmenting the graspable\nobjects from the scene using geometric features (for both\nobjects and gripper). They then applied a convolutional\nnetwork to the graspable objects which used a structure\npenalty term to optimize the connections between modalities.\nSimilarly, in work presented by van Hoof et al. (2016),\ndeep autoencoders were used to learn compact latent\nrepresentations for reinforcement learning to form policies\ndescribing tactile skills. These feedback policies were\nlearned directly from high-dimensional space under iterative\non-policy exploration and vastly outperformed a baseline\npolicy learned directly from the raw sensor data.\nContrary to these works, instead of predicting the single\nbest grasp pose from a given image, Johns et al. (2016)\ndemonstrated a convolutional network that predicted a\nscore for every possible grasp pose, such a value function\ndescribed what they denote as a grasp function. They\ndiscussed that such a method can attribute to robust grasping\nby smoothing this grasp function with a function describing\npose uncertainty. Although in their demonstrations, it\nappears this particular approach achieved some-80% grasp\nsuccess rate, fundamental assumptions are that the object is\nisolated in the scene and the grasping device is a parallel jaw\ngripper.\nIn a different approach, Varley et al. (2016) showed that\nconvolutional networks can be used for shape completion\ngiven an observed point cloud. In their method, the network\nlearned to predict a complete mesh model of objects (ﬁlling\nin the occluded regions of the scene), which was then\nsmoothed and used to support grasp planning.\nThe use of convolutional neural networks as a means\nfor automatic feature extraction has been employed in\nimitation learning paradigms where actions are learned for\nan autonomous navigation task directly from raw visual data.\nThe network is encoded with no initial knowledge of the task,\ntargets, or environment in which it is acting in. In a simulated\nstudy Hussein et al. (2016) showed that using deep active\nlearning can signiﬁcantly improved the imitated policy\nthrough a small number of samples—this is accomplished\nby the network querying a teacher for the correct action\nto take in situations of low conﬁdence. Unfortunately, this\nframework relies on the fact that there is a teacher present\nwith competent knowledge of the domain and appropriate\nactions. A framework using time-delay deep neural network\nwas shown by Noda et al. (2013) that both fused multimodal\nsensory information and learned sensorimotor behaviors\nsimultaneously. They demonstrated that a single network was\nable to encode six object manipulation behaviors dependent\non temporal sequence changes with the environment and\ndisplayed object.\nSince robots operate in dynamic and partially observable\nenvironments, selecting the best action is nontrivial since\nit is dependent on the time history of interactions (or\nsequences of actions in the past). As such, ways to learn these\noptimal policies generally rely on a trial and error paradigm\nvia reinforcement learning. For example, this is especially\npresent in the recent successful demonstrations of Atari\ngameplay Mnih et al. (2013, 2015). Despite its demonstration\nthrough an artiﬁcial agent, the Deep Q Networks presented\nhas immediate implications in robot control, however, may\nnot be effective on physical domains especially when\nrewards are sparse making efﬁcient exploration essential.\nA method proposed by Lipton et al. (2016) demonstrated\nexploration by Thompson sampling where using Monte\nCarlo samples from a Bayes-by-Backprop neural network\nprovided improvement over the standard DQN approach that\nrelied either on ϵ-greedy or Boltzmann exploration.\nIn a particular study, Finn et al. (2016b) proposed to\nuse neural networks as a tool to learn arbitrarily complex\nand nonlinear cost functions for inverse optimal control\nproblems allowing systems to learn from demonstration\nusing efﬁcient sample-based approximations. Their methods\nwere demonstrated on simulated tasks as well as on a mobile\nmanipulator. Another study presented a belief-driven active\nobject recognition system that used a pretrained AlexNet\nﬁrst to derive belief state. A Deep Q Network was then\nincorporated to actively examine objects by selecting actions\n(in hand manipulations) that minimized overall classiﬁcation\nerrors, resulting in an efﬁcient policy for recognizing objects\nwith high levels of accuracy Malmir et al. (2016). Instead\nof training the action selection network over the pretrained\nconvolutional network, this system was later extended to be\ntrainable end-to-end Malmir et al. (2015).\nIn contrast to the large population of work that uses\nconvolution to extract useful feature representations at the\noutput layer of a neural network and use these features\nto associate control, Ku et al. (2016) demonstrated a\ndifferent approach. They showed that using intermediate\nfeatures of a convolutional network was sufﬁcient for gross\nand ﬁner grain manipulation supporting palm and ﬁnger\ngrasps. The technique localized features corresponding to\nhigh activations given point clouds of simple household\nobjects through targeted backpropagation. Using this, they\npresented a hierarchical controller composing of ﬁnger and\npalm pre-posture positions on the R2 robot, however, alike\nwork by Wilkinson and Takahashi (2015), the speciﬁc layer\nto obtain information from is still human deﬁned.\n6\nFrom Pixel to Motion – Recently, an end-to-end strategy\nfor visuomotor control popularized by Levine et al.\n(2015) has shown promise for deep learning in robotics.\nUsing optimal control policies as supervised signal for\nneural networks, they demonstrated task learning relevant\nto local spatial features obtained through convolution.\nMore importantly, this showed promise for an end-to-end\ntraining approach to obtain visuomotor policies producing\na network that commanded motor torques directly from\nthe raw visual input. Finn et al. (2015) proposed the\nuse of deep spatial autoencoders to acquire informative\nfeature points that correspond to task-relevant positions.\nThe method learns to associate motions with these points\nusing an efﬁcient locally-linear reinforcement learning\nmethod—because the resulting policies are based off these\nlearned feature points, the robot is capable of dynamically\nmanipulation in a closed-loop manner. Similar approaches\nto visuomotor control has been demonstrated by Tai and\nLiu (2016) the learned end-to-end exploration policies on\na mobile robot and folks from nVidia for self-driving\ncars, where an autonomous vehicle was driven by vision\nalone through an end-to-end system Bojarski et al. (2016).\nIt appears that learning motor control directly from raw\nsensory signals induces robustness and produces a control\nsolution that is otherwise too complex to hand design.\nLikely, action outcomes are not deterministic and pose\nestimation future establishes uncertainties, whereas, these\nconvolutional learning strategies aims to resolve motor\ncommands straight from raw percepts—learning both useful\nfeature representations and control policies simultaneously.\nAn issue with end-to-end methods and deep reinforcement\nlearning in general is that it demands extremely large sets of\ndata. For such reasons, Guided Policy Search (GPS) Levine\net al. (2015) attempts to bias training for the reduction\nof the number of instances needed and looked to acquire\nvisuomotor policies by the means of a supervised learning\nproblem. A reset-free GPS algorithm was introduced by\nMontgomery et al. (2016) to address the issue with its\nrequirement for a consistent set of initial states. Meanwhile,\nChebotar et al. (2016) later extended GPS to account for\nhighly discontinuous contact dynamics through an path\nintegral optimizer and on-policy sampling to increase the\ndiversity of instances which they argued was crucial for high\ngeneralizability.\nIn the original formulation of GPS, the learning problem\nis decomposed into a number of stages. First full-state\ninformation is used to create locally-linear approximations\nto the dynamics around nominal trajectories, then optimal\ncontrol is used to ﬁnd locally-linear policies along those\ntrajectories. Lastly, it uses supervised learning with an\nEuclidean loss objective to create complex nonlinear\nversions of these policies that reproduce similar optimized\ntrajectories. In other words, GPS iteratively optimizes local\npolicies (concerning speciﬁc task instances) which are then\nused to train a global policy that is general across instances.\nHowever, to do this, it requires data on the physical system—\nrobots must collect data for training to reﬁne the network\noriginally trained by guided policies in the form of optimal\ntrajectories.\nTo collect massive sets of data, one may consider having\nthe robot obtain its own experiences without the need\nof meticulous human labeling or supervision. Addressing\nthe problem of self-supervision, work by Pinto and Gupta\n(2015) showed that robots can self-supervise themselves\nto learn visuomotor skills without manual labels. In\ntheir experiments, they demonstrated remarkable robustness\nwhere a Baxter robot self-labeled 50, 000 grasp examples in\nover 700 hours of manipulation. Under similar inspirations,\nLevine et al. (2016) used a distributed system consisting\nof 14 robot manipulators to collect a massive dataset\n(800, 000 grasps) over the course of two months for grasping\nand eye-hand coordination. However, both of these self-\nsupervised methods considered specifying a heuristic to\nclassify grasp examples. Unfortunately, these heuristics are\nhuman-speciﬁc and somewhat arbitrary. Wong et al. (2016)\ndeveloped a comparable self-supervision approach with a\nkey distinction being the use of feedback from closed-loop\nmotion primitives as a supervisory signal rather than these\nhuman-speciﬁc parameters. Still, a fundamental problem for\nall of these studies is that they are demonstrated and tailored\nfor a single speciﬁc, predeﬁned task. A study by Pinto and\nGupta (2016) found that learning over a number of tasks\nhelps discover richer representations of the environment,\nthus outperforming models that have otherwise been trained\non a single task alone. But an open research problem is to\nconsider methods by which robots can motivate themselves\nto select useful tasks to learn from.\nIn a study with ideas analogous to adversarial training,\nPinto et al. (2016) demonstrated that having a protagonist-\nantagonist paradigm resulted in more effective learning\nof visuomotor policies. They discovered that having an\nantagonist robot that aimed to prevent the protagonist from\ngrasping, resulted in learning higher performance grasping,\ndue to the necessity to learn a robust policy to overcome this\nadversary. In summary, they emphasize that not all data is\nthe same. Contrary to the massive 800, 000 grasping dataset\npresented by Levine et al. (2016), they found systems that\nattack harder examples tend to achieve faster convergence\nand higher performance.\nOn Abstract Parametrized Skills – End-to-end methods\nproduce amazing results by learning visuomotor torque-\nlevel control policies straight from raw pixel information.\nHowever,\ntwo\nimmediate\ndrawbacks\nare\ncritical\nfor\nautonomous systems. Firstly, the robot can only learn\nvery task speciﬁc motions rather than abstract notions of\nskills and representations reuseable throughout its lifetime.\nIn particular, GPS allows the robot to quickly encode\nvisuomotor policies by guided trajectories acquired through\noptimization, but since they operate over joint torques it\nis difﬁcult to decipher abstract skill boundaries2. Secondly,\nby operating over joint torques, the network loses control\n2In fact, a coalition of researchers during a Robotics: Science and Systems\n(RSS) workshop entitled Are the Sceptics Right? Limits and Potentials of\nDeep Learning in Robotics (June 2016) in Michigan, USA have argued\nthat it may not be ideal to learn end-to-end. They indicate that in the same\nway that we would never want to learn sort when we have quicksort, it\nmakes little sense to learn low-level torque activations when we understand\nkinematics.\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n7\nguarantees and parametrization insight that abstract skills\nderived from control-theoretic approaches may provide3.\nBy learning at the lowest level of motor units (e.g. in\nconﬁguration space over joint torques), systems need a\nmassive set of examples and training steps to cover this\nspace. For instance, even for a simple 2D spatial reaching\ntasks in a conﬁned 40cm × 30cm area using a 6 DOF Kinova\narm with three ﬁngers was said to require over 50 million\nsteps via an end-to-end (raw pixel to joint space mapping)\nparadigm Rusu et al. (2016b). For such reasons, learning over\na parameterized space abstracts away these basis motor units\nand we theorize will accelerate learning.\nThe notion of control guarantees is of chief importance\nthrough an industrial and product-delivering perspective.\nEspecially in scenarios where human factors are involved\nor other high ﬁdelity situations, systems that acquired\nexpertise through learning over a history of experience\nmust exhibit certiﬁed guarantees. Indeed, the maximum\nactivation visualization approach, “deep dream” can be used\nto identify convolution features to attempt to make sense of\nthe network’s latent representations Mahendran and Vedaldi\n(2016), but, we are concerned with a stronger sense of\nguarantee, especially, in the sense of control derived from\nthe output of these networks. In autonomous driving, for\nexample, the system must be analyzed that one can establish\ncertiﬁed guarantees, up to sensor noise, that regardless of\ninput, the vehicles will never attempt to give commands that\nresult in collision with entities in the world. Such analytics is\nextremely difﬁcult to reason over if the commands are over\nlow-level speciﬁcs like wheel torques. Rather, analysts can\nreason easily in Cartesian space—perhaps then, the robot\nshould learn sets of abstract skills that operate with goals\nthat are easily interpretable for validation and certiﬁcations.\nAs such, the direction in which we should look into may live\ncloser to the realm of acquiring such parametrized skills.\nWhile there exists a plethora of work for learning these\nskills, (e.g. Konidaris and Barto (2009); Da Silva et al.\n(2012); Masson and Konidaris (2015)), we believe that to\nbetter investigate a principled formulation for lifelong learn-\ning cognitive systems, we should investigate the perspective\nof learning through the lens of cognitive psychologists—\nsuch allows us to better understand the development of\ncognition and action in living organisms. Insight from this\nbecomes fundamental in drawing computational analogs\noriginating from developmental processes to better design\nartiﬁcial, learning systems.\n3\nComplex Sensorimotor Hierarchies\nThe artiﬁcial neural networks architecture as an explanatory\nmeans to a connectionist model of cognition and action\nis not a concept that resides solely in computation. In\nfact, a number of cognitive psychologists showed intrigue\nwhen these networks were at its infancy, dating back two\ndecades Rumelhart (1998). Most prominently, Thelen and\nSmith (1996) describes that such models are exciting in\nthe sense that there exists only process. They indicate that\nthe essence of behavior is distributed among numerous\nindividual units, that together, in the strengths of their\nconnections, describe behavior. They are plastic and modify\nthemselves through dynamical processes with the world.\nIn particular, Thelen and Smith (1996) argue against the\nnotion that these “neural networks contain some privileged\nicon of behavior, abstracted from complex motivation and\nenvironmental contexts in which it is performs.” In other\nwords, the theory that networks encode a particular context-\nindependent behavior—something like a Central Pattern\nGenerator (CPG) is entirely incorrect. They emphasize that\nbehavior is context-speciﬁc, even in the case of CPGs—\nmany studies that elicit such behavior and draw such\nconclusions are based off an impoverished form of induced\nbehavior. To this regard, it may be true that behavior exists\nin some innate form that when given appropriate stimuli\nwill generate seemingly high level actions—resulting in the\nmisclassiﬁcation of there being such generators. Thelen and\nSmith conclude that the development of these into complex\nbehavior is entangled in motivation and context-speciﬁcity.\nFollowing this notion, the work that has been discussed\nto this point fail to tackle this intertwined cobweb of action,\nenvironment, and motivation. Although reinforcement learn-\ning paradigms do describe the acquisition of action through\nthe system’s interaction with entities in the world in context-\nspeciﬁc situations, many of these studies fail to indicate\nprincipled motivators. Rewards are generally task-speciﬁc\nand user deﬁned—not an inherent property derived by the\nsystem itself in its interpretation of situational contexts.\nAdmittedly, even promising developmental frameworks like\nthe ones outlined by Mugan and Kuipers (2012); Grupen and\nHuber (2005), are culprit to ad hoc reward structures. Most\nimportantly, however, through Grupen and Huber (2005)’s\noutline of ﬁgurative schematas that organize into the devel-\nopment of robot behavior, they emphasize the need for con-\ntrol knowledge to be represented in a manner that supports\ngeneralization. Similar aspirations are found in the action\nschema framework Platt et al. (2006). The ability to construct\nand reused learned behaviors in a general manner is of fun-\ndamental importance—thus, we share a similar view on the\nacquisition of motor behavior. These views were originally\nderived from the proposal under Piaget and Cook (1952)’s\naccount, where human infants exhibit a sensorimotor stage\nthat lasts approximately 24 months while producing control\nknowledge that support generalization and reuse. Such reuse\nand organization implies underlying hierarchies of motor\nbehavior.\nIn comparable work, Heess et al. (2016) emphasized\nthe importance of hierarchical controllers that operate\nat different time scales in support of modularity and\ngeneralizability. Their work showed a promising step\nforward in locomotive skill transfer between a number of\nsimulated bodies with many degrees of freedom, wherein\nhigh-level controllers modulated low-level motor skills\nwhich emerged from pretraining. Other work has looked\ninto building implicit plans or macro-actions by interaction\nalone using a recurrent neural network structure Mnih\net al. (2016). However, these works do not necessarily\n3It is entirely untrue to state that Guided Policy Search algorithms\nhave no guarantees. Actually, its original formulation has asymptotic\nlocal convergence guarantees. Later Montgomery and Levine (2016)\nreformulated GPS under approximate mirror descent to ﬁnd convergence\nguarantees in simpliﬁed convex and linear settings and bounded guarantees\nin the nonlinear setting.\n8\ndiscuss how these temporal hierarchies of options, skills or\nmacro-actions play with intrinsic motivators, where systems\nderive their own reward paradigms and build continuously\nextended motor hierarchies. Kulkarni et al. (2016) presented\na hierarchical-DQN framework which integrated hierarchical\nvalue functions and intrinsic motivation by having a top-level\nfunction learn policies over intrinsic goals and a lower-level\nlearning policies to achieve these goals. They suggested that\nintrinsic motivation be derived from the space of entities\nand relations which is sufﬁciently bounded and ﬁnite in\nAtari games, however, may exhibit explosive growth in\nthe physical world. Future work indicated a connection to\ndeep generative models—wherein, in this paper, we derive\nintrinsic motivation from a deep generative dynamics model.\nIn the following subsections, we summarize work\nthat provide systems with the ability to learn complex\nsensorimotor hierarchies resulting from experience and\ninteraction with the world. Complex action-related behavior\nare expressed as motor hierarchies emerging through the\ncombinatoric sequencing and composition of actions, that\nat the lowest level, are learned by associating sensory\ninput to resolve motor primitives. As such, we begin at\nthe lowest level of motor development, on how a robot\ncan associate sensor input to activate closed-loop motor\nprimitives. Next, we investigate learning to bootstrap these\nprimitives to develop more complex behaviors. And lastly,\nwe incorporate techniques present in the literature to suggest\nthe learning of control goals that evolve primitive reﬂexes\nto intentional goal-oriented behaviors. In Section 4, we\ndescribe an intrinsically motivating paradigms that leverages\nthe system’s ability in its understanding of the world and of\nits own inherent actions and representations through control\ncontexts—such motivators are to drive the processes that\ngovern the development of these sensorimotor behaviors and\ncognitive representations. A plausible unifying framework is\nillustrated in Figure 1 by piecing together various selected\nstudies currently in the literature.\nWe now quickly elaborate on each of these pieces and on\ntheir respective subsections in this manuscript.\nSection 3.1 describes how to activate motor primitives\ngiven sensory input, deriving a control context—it discusses\nan approximation to the function fγT\ni : s 7→γT\nφi|σ\nτ .\nSection 3.2 investigates how to build hierarchies of\ncomplex behaviors by combining existing controllers to\nconstruct new ones under the current state description\nΓT (s) = γT\n1 ∪γT\n2 ∪· · · ∪γT\nn . This is described by learning\nthe policy π∗\nn+1 : ΓT (s) 7→{φ1|σ\nτ , φ2|σ\nτ , · · · , φn|σ\nτ }. The\nnetworks proposed here approximates the value function\nfπ∗\nn+1 : ΓT (s) 7→V π∗\nn+1(ΓT (s)), where ΓT\nis the deep\ncontrol context at time T encompassing all control state\ndescriptions γT .\nSection 3.3 describes methods to learn continuous\ncontrol parameters for controllers φ1|σ\nτ , φ2|σ\nτ , · · · , φn|σ\nτ , thus\napproximating the function fρi : s 7→ρφi|σ\nτ .\nSince both the learning of fπ∗\nn+1 and fρ,φi|σ\nτ is by trial and\nerror, it requires that environmental reward is deﬁned. We\nrefer to the system’s level of understanding regarding entities\nin the world and of its own actions to establish reward. As\nsuch, Section 4.1 discusses learning to predict the transition\ndynamics of the world by approximating fW : s, ρφi|σ\nτ 7→s′.\nReLu\ndense\ndense\nsoftmax\ndense\nsoftmax\ndense\nsoftmax\nmax\nmax\nmax\nReLu\ndense\nReLu\ndense\nSec 3.1: Control Contexts and Motor Primitives:  \nHuber et. al (1996), Coelho (2001), Wong et. al (2016)\nSec 4.1: Aﬀordance Prediction:  \nMathieu et. al (2015) \nFinn et al (2016)\nSec 3.2 Complex Behaviors:  \nHuber et. al (1996) \nPlatt et. al (2004-2010) \nMnih et. al (2013, 2015)\nSec 3.3: Continuous \nParametrization for Actions: \n \nActor-Critic Approaches \nNormalized Advantage Functions  \n \nLillicrap et. al (2015) \nHausknecht and Stone (2015) \nGu et. al (2016)\n* An oversimpliﬁcation of many plausible \ncontinuous parameter network structures \nInput: State (Multimodal State) \nOutput: Continuous action parameters\nFigure 1. A candidate framework marrying various learning\nmodules that have been individually presented in the literature\nSection 4.2 outlines a plausible reward structure derived\nfrom affordance predictions.\nAnd ﬁnally, Section 5 describes how a system can exploit\nthese learned behaviors and representations to solve useful\ntasks and mission in the world.\n3.1\nActivating Motion Primitives\nIn their revolutionary book, Thelen and Smith (1996)\noutlines the misinformations of contemporary theories in\ncognitive development, suggesting that it is not the case\nthat cognitive and motor skills emerge linearly through\ndevelopment but these primitive forms of behaviors are\ninherently ingrained. They are reinforced by interaction with\nthe environment to seek dynamically stable solutions. As a\nresult, skills emerge from context-speciﬁc situations that are\nafforded by the world. It appears that local circuitry within\nthe spinal cord mediates a number of closed-loop sensory\nmotor reﬂexes—for instance, the spinal stretch reﬂex Purves\net al. (2001). In fact, it has been observed that all humans\ndeveloping infants exhibit similar chronicles of reﬂexive\nmotor behaviors.\nThe Central Nervous System is organized according to\nmovement patterns Aronson (1981)—with its most basic\nform being the reﬂex4. Under the notion of epigenetic\n4Grupen and Huber (2005) expresses that packaged movement patterns\n“reside in the central and peripheral nervous system and range from\ninvoluntary responses to cortically mediated visual reﬂexes [and] contribute\nto the organization of behavior at the most basic level by constituting a\nsensorimotor instruction set for the developing organism.”\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n9\ndevelopmental theory primitive reﬂexes, expressed as neuro-\nanatomical structures, are the basic building blocks of\nbehavior. In this regard, complex behavior emerges from\ncombinatoric sequences of primitive control in response\nto reinforcement by the world. These primitive forms of\nbehavior collectively describe an epigenetic computational\nbasis by which complex control actions are derived Grupen\nand Huber (2005). The ability to convert these developmental\nreﬂexes into intentional actions remains an fundamental\ncognitive process Zelazo (1983) and has been brieﬂy\nexplored in work by Wong et al. (2016) in which a\ndeep network controlled closed-loop motion primitives.\nThis was accomplished through the activation of output\nneurons describing control state derived from time varying\ndynamics. In short, this reduces sensorimotor control to a\nsupervised learning problem in which the state of the world\ndescribed through sensor modalities (vision was shown in\ntheir study) is used to predict the probability of controller\nstate transitions. Consequently, additional complexity of\noutput neurons simpliﬁed learning to a reduced network size\nand training data.\nThe primitive controllers used in their study are all\ninstances of negative feedback stabilized systems. In other\nwords, these motor primitives are regulators that minimize\nerror between sensation and desired. Consider a simple\nproportional derivative heading (θ) controller that yields a\nsingle degree of freedom dynamical system represented by\nthe second order differential equation in the canonical form,\n¨θ + 2ζωn ˙θ + ω2\nnθ = 0\nwhere ζ = B/(2\n√\nKI) is the damping ratio and ωn =\n√\nKI\nis the natural frequency. It is assumed here that K, B, and I\nare all constants representing the proportional and derivative\ngains, and the scalar moment of inertia around the rotation\naxis respectively under a canonical spring-mass-damper\nmodel Hebert et al. (2015). Closed-loop feedback controllers\nsuch as these have well understood proofs of stability. For\ninstance, the proportional derivative (PD) controller such\nas the one we described here is provably stable Lyapunov\n(1992). This property is established for systems formulated\nas harmonic oscillators similar to the above second order\ndifferential equation. Convergence results have been proven\nfor closed-loop controllers by Coelho and Grupen (1997)\nfor regular convex prismatic objects when two closed-loop\ncontrollers executed in a particular sequence. Experiments\nwere later shown on a robot manipulator agreeing with such\nconvergence guarantees Coelho (2001)5. Likewise, optimal\ncontrollers described by regulators like linear quadratic\nregulators (LQRs) and its variants fall into this categorization\nas well. In fact, dynamically balancing robots like the uBot\nplatforms Kuindersma et al. (2009); Ruiken et al. (2013),\nimplement a variant of LQR. To reiterate the notion of\nlearning abstractions, we suggest strongly that rather than\nlearning a balancing policy from scratch, perhaps a better\ndirection is to consider this closed-loop controller as a skill\nand employing learning architectures at the skill level of\nabstraction.\nNamely, in spirit of the ﬂurry of work on autonomous\nvehicles, we emphasize that motion planners and path\ntracking procedures generally fall under this umbrella notion\nas well. In particular, path tracking in navigation at the\nmost primitive sense, leverage a heading and longitudinal\ncontroller like those described by Hebert et al. (2015).\nPotential ﬁeld methods for path planning Ge and Cui (2002);\nWang and Chirikjian (2000); Khatib (1985) like Harmonic\nfunction path planning Connolly and Grupen (1993) have\nshown success dating back over two decades of research.\nEspecially with recent advances in GPU parallelization for\nefﬁcient relaxation and the logarithmic transformations to\nprevent diminishing gradients Wray et al. (2016), these\nclassical methods still remain powerful path generators. An\nelegant control theoretic relaxation-based method to velocity\nplanning presented by Hebert et al. (2015) is shown to be\ndone in linear time of the path, resulting in minimal path\ndeviations and maximal performance envelope. Many of\nthese navigation solutions are fast and robust—so instead\nof replacing them completely with a learned approximation,\nwe suggest treating resulting plans like motion primitives\nor inherent behaviors in the context of forming complex\nvisuomotor hierarchies. Likewise, solutions to other motion\nplanners like RRTs or A* variants can interpreted as motion\nprimitives given that their trajectory can be used to describe\nsome transient, converged, or goal completion evaluation.\nUnder this broad encompass, even dynamic motion\nprimitives (DMPs) Ijspeert et al. (2003) and other powerful\noptimization based methods for locomotion, like those\ndeveloped during the DARPA Robotics Challenge (e.g. Feng\net al. (2015b); Kuindersma et al. (2016)), fall into this\ncategorization of plausible motor primitives.\nMotion primitives like these can be formalized under the\nControl Basis framework in which the interaction between\nthe embodied system and the environment is modeled as a\ndynamical system, allowing the robot to evaluate the status\nof its actions as a state describing a time varying control\nsystem. These controllers φ|σ\nτ , consist of a combination of\npotential functions (φ ∈Φ), sensory inputs (σ ⊆Σ), and\nmotor resources (τ ⊆T ) Huber et al. (1996b). Controllers\nachieve their objective by descending along gradients in the\npotential function ∇φ(σ) with respect to changes in the value\nof the motor variables ∂uτ, described by the error Jacobian\nJ = ∂φ(σ)/∂uτ. References to low-level motor units are\ncomputed as ∆uτ = κJ#∆φ(σ), where κ is a control gain,\nJ# is the pseudoinverse of J Nakamura (1990), ∆φ(σ)\ndescribes the difference between the reference and actual\npotential Sen and Grupen (2014).\nThe time history or trajectory of dynamics (φ, ˙φ) as a result\nof interactions with the environment by executing controllers\nhave been shown to have predictive capability regarding the\nstate of the environment. It was originally shown by Coelho\nand Grupen (1998) that dynamics elicited by abstract actions\nin the form of controllers serve as important identiﬁers\nfor the current control context—one of many ﬁnite sets of\ndynamic models that capture system behavior. The state\ndescription γt for a particular control action φ|σ\nτ at time t\nis derived directly from the dynamics (φ, ˙φ) of the controller\n5In the next subsection, we will discuss composite controllers which have\nsimilar convergence guarantees as outlined and proven by Platt et al. (2010).\n10\nReLu\ndense\ndense\nsoftmax\ndense\nsoftmax\ndense\nsoftmax\nmax\nmax\nmax\nReLu\ndense\nReLu\ndense\nFigure 2. A deep control context deﬁned over the collection of\ncontrol state predictions from each γT -network\ngiven a speciﬁed control goal g such that,\nγt(φ|σ\nτ ) =\n\n\n\n\n\n\n\n\n\nUNDEFINED :\nφ has undeﬁned reference\nTRANSIENT :\n| ˙φ| > ϵ\nCONVERGED :\n| ˙φ| ≤ϵ and φ reaches g\nQUIESCENT :\n| ˙φ| ≤ϵ and φ fails to reach g\nCollectively, the state descriptions over all abstract actions\ndescribe the control context. This collection can be thought\nof as a compressed variant of sensorimotor contexts Hemion\n(2016a) that is speciﬁcally grounded to particular control\ninteractions through the function approximate fγT : s 7→\nγT\nφ|σ\nτ , however, the hierarchy in this case, is achieved by\nlearning new control programs to discover more abstract\ncontexts.\nMotor units that operate at the torque or joint level are\nabstracted away into high level parameterized (continuous\nand goal-oriented) motion controllers that achieve particular\nobjectives.\nAs\nsuch,\na\nsurrogate\nof\ncontrol\ncontext\nis produced by inferring the time-varying dynamics—\nspeciﬁcally, Figure 2 illustrates the concatenation of state\ndescriptions over several γT -networks originally presented\nby Wong et al. (2016) to formulate a deep control context,\nwhich provides implications into intriguing reinforcement\nlearning methods that have been otherwise inubiquitous.\n3.2\nComposition of Motor Behaviors\nLearning these γT -networks results in a set of sensorimotor\npolicies for activating motor primitives—such a policy is\na sensory-driven predictive process that activates primitives\nwhen appropriate state descriptions are predicted by the\nnetwork. A fundamental drawback is that this reduces the\nset of available behaviors of a system to some static set\npreordained by initial primitive “reﬂexes”—in fact, it is\nindeed the size of the primitive set. This is not the case\nin development however, since the set of skills and their\ncompetence grows Thelen and Smith (1996). As such, both\nrobots and humans must learn how to make use of their\ninnate primitives to build new, complex motor behaviors.\nPreviously, we discussed how infants derive complex\nactions through developmental chronicles of emergence and\ninhibition of primitives. In the same sense that complex\nmotor behaviors emerge from combinatoric sequences of\nprimitive reﬂexes in the human infant, a computation\ncomposition and sequencing of primitive closed-loop motor\nunits form policies for high-level control policies.\nThis idea is not new. The idea of acquiring hierarchies\nof motor policies through reinforcement learning over a\nbasis of existing motor controllers dates back two decades\nto work by Huber et. al (See, Huber et al. (1996a);\nHuber and Grupen (1997); Grupen and Huber (2005)).\nThis framework was ﬁrst shown on a quadruped walking\nrobot, where it quickly learned walking gait behaviors\ngiven a set of primitive kinematic conditioning controllers\nin a state space constrained by valid tripod stances. They\noutlined the learning of a locomotive gait schemata over a\nbasis of quasistatic kinematic conditioning controllers with\ntime-varying constraints represented as artiﬁcial maturation\nprocesses and domain requirements. Unfortunately, these\nconstraints are deﬁned a priori and are inherent in\ntheir developmental scheduler or assembler. Rewards are\nclosely tied to phases of development and known task\nspeciﬁcations. For instance, in particular developmental\nstage the reward attributed to the maximization of angular\nvelocity. Regardless, the system was able to learn a\ncompilation of control knowledge through environmental,\ntask relevant rewards under traditional Q-learning over\nthe control context or concatenated state descriptions over\nthe basis of innate control programs. Emerging schemata\nconsisted of rotate, translate, and maze traversal behaviors.\nExpressiveness was increased by using a method to allow for\nconcurrent control composition Grupen and Huber (2005).\nLater, this was extended to a number of other applications,\nfor instance in robot manipulation and grasping domains\nproducing a series of profound and sophisticated control\nparadigms namely in work by Platt et al. (2002, 2004, 2005,\n2006, 2010). To clarify, the forming of new control policies\ncan be formulated under the options framework Sutton\net al. (1999); Stolle and Precup (2002), where primitive\nsensorimotor skills are seen as possible high-level abstracted\noptions.\nNew control policies do not emerge by sequencing innate\ncontrollers alone, but also can emerge by combining many\ncontrollers, or simply operating inferior control programs\ninside the nullspace of primal ones. An important aspect\nto the creation of new control policies is the notion of\nnull space composition, a mechanism for systematically\nspecifying composite controllers derived from combinations\nof other controllers. To our knowledge, this was ﬁrst\nintroduced in work by Huber et al. (1996a) and popularized\nin manipulation by the work of Platt. This composition uses\nthe subject-to, ◁, operator to project the control gradient of\nsubsequent, subordinate controllers into the null space of\na superior controller, allowing all controllers to attempt to\nachieve their objectives if there exists resources allowed by\nthose superior in a sequential priority fashion. Thus, new\ncontrol policies can be expressed in terms of existing motor\nbehaviors. Platt (2006) derives a general formulation for the\ncomposition of k controllers in numerical priority under the\neffector space Y as the following,\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n11\n∇(φk ◁, . . . , ◁φ1) ≡∇yφ1 + [N(∇yφT\n1 )]∇yφ2\n+\nh\nN\n\u0010∇yφT\n1\n∇yφT\n2\n\u0011i\n∇yφ3\n...\n+\n\"\nN\n \n∇yφT\n1\n...\n∇yφT\nk−1\n!#\n∇yφk\nThe null space of the (k −1) controllers are computed by\nconcatenating all control gradients as,\nN\n ∇φT\n1 (y)\n...\n∇φT\nk−1(y)\n!\n= I −\n ∇φT\n1 (y)\n...\n∇φT\nk−1(y)\n!# ∇φT\n1 (y)\n...\n∇φT\nk−1(y)\n!\nwhere I is identity. The above produces a null-space N\nthat is tangent to all k −1 gradients. The idea of operating\nmultiple control programs through compositions in null-\nspace paves way for coarticulate control paradigms. This\nnotion relies on the fact that in many cases controllers\nare redundant, having more resources than necessary to\nachieve a particular objective (e.g. redundant in the sense\nof degrees of freedom). As such, these excess resources\ncan be used by inferior controllers to achieve secondary\nobjectives simultaneously by operating under the constraint\nof not interfering with the primary objectives Rohanimanesh\net al. (2004).\nThe optimal sequencing of controllers described by some\npolicy π to solve a particular task is generally achieved\nthrough table-based reinforcement learning using the control\ncontext as a state representation. Yet, the use of neural\nnetworks for classical control problems like the peg-in\nhole insertion task dates back to the early nineties. For\ninstance, work by Gullapalli et al. (1992) used multilayered\nfeedforward networks to generate real valued outputs as\ncontrol parameters. Simple networks such as these can easily\napproximate the value functions that describe the optimal\npolicy π that expresses a trajectory through control contexts\nand closed-loop (primitive and composite) controllers. In\nfact, this is a simpliﬁed version of DeepMind’s Atari\napproach Mnih et al. (2013, 2015) where current state is\nused to approximate a value function, however the key\ndistinction is that by describing current state via control\ncontexts, this reduces the input dimensionality drastically.\nThe options framework has been demonstrated with DQNs\nwhere option heads were integrated into the output end of the\nnetworks. Osband et al. (2016) used these “bootstrap heads”\nwith different motivations allowing the network to output\na distribution over Q-functions. Similarly, this was later\nextended by Arulkumaran et al. (2016) with an additional\nsupervisory network, allowing the policy to be decomposed\ninto combination of simpler sub-policies. Instead, since\nwe learn over control contexts the input dimensionality is\nextremely reduced and leads to faster learning.\nIf one were to consider the use of these γT -networks as a\nperceptual interface to activate motor primitives, null-space\ncomposition is achieved simply by taking the controller\nφi|σ\nτ corresponding to the control state γT\ni predicted by the\nnetwork and using the null-space operator between multiple\ninstances of these (e.g. the composition of networks fγ,φj|σ\nτ\nsubject to fγ,φi|σ\nτ is trivially φj|σ\nτ ◁φi|σ\nτ ).\nEach controller φ|σ\nτ requires a control reference that\nimplies an error to minimize. In many cases, these goals\nare generally human-deﬁned. For instance, Hart (2009a)\nuses hue saturation to select interesting areas in the scene.\nHowever, recently deep learning architectures were shown\nto be powerful tools of extracting candidate features in the\nworld. Leveraging the predictive γT -networks provides these\nindicates stimuli in the world that derives useful control\ngoals, or in other words, control references, that feed into\nthese controllers φ|σ\nτ . In fact, the null-space composition\nin this case, is mostly unchanged and operate almost\nidentically. A forward propagation over each γT -network\npredicts control context or state description that indicate\nactivations for each primitive.\nIn the following section, we investigate the literature in\nreinforcement learning, both deep and classical, for methods\nthat learn these control references or goal parameters that\ntake on continuous values.\n3.3\nContinuous Control Parameterization\nThe γT -network shown in Wong et al. (2016) makes an\nunfortunate assumption that the motion primitive described\nby closed-loop controller φ|σ\nτ inherently deﬁnes a static\ncontrol goal g. This particular form of goal is derived\nfrom cognitive development insight, allowing networks to\nlearn when to execute particular abstract skills represented\nas reﬂexive actions. However, the question that remains\nis the encoding of how should these skills be performed.\nInfants readily bootstrap their innate reﬂexive repertoire\nto quickly learn the functionality of their end effectors as\nentities conform to their hands via palmar grasp reﬂex.\nStatic parametrized goals allows for quick association-based\nlearning, but do not generalized well to future task that\nrequire goals outside of innate reﬂex descriptions. As a\nresult, both infants and robots must learn useful parameters\nto their inherent motor behaviors in response to stimuli in the\nworld. The γT -networks can be extended to predict varying\nparameterizations of control goals by a concatenation of\nstate and action parameters after the convolutional layers, a\ntechnique that is used in many predictive networks like those\npresented in Levine et al. (2015); Finn et al. (2016a). In fact,\nwork by Takahashi et al. (2017) implemented this extension\nproducing a variant of γT -networks that account for varying\ncontrol goal parameters.\nLearning control parameters is generally accomplished via\na trial and error approach, for instance, through popularized\ndeep reinforcement learning methods Mnih et al. (2013,\n2015). Unfortunately, a major drawback of the approach used\nto tackle learning control policies for the Atari games is\nthat the network’s output is ﬁxed over the set of discrete\ncontrol actions. Quite evidently, robots must be able to\nperform continuous parameterized actions in the real world\ninstead of ﬁxed discretized control actions. To address this\nLillicrap et al. (2015) presented an actor-critic approach to\nlearn over continuous domains with neural networks using a\ndeep variant to the deterministic policy gradient originally\nproposed by Silver et al. (2014). Similarly, Balduzzi and\nGhifary (2015) also extended the original deterministic\npolicy gradient algorithms with a network that explicitly\nlearned ∂Q/∂a—unfortunately though, their methods were\nonly shown on low-dimensional domains.\n12\nHeess et al. (2015b) introduced the stochastic value\ngradients to learn stochastic policies through a Q-critic. They\nfound that stochastic control can be supported by treating\nthe stochasticity in the Bellman equation as a deterministic\nfunction consisting of external, Gaussian noise. From this,\nthey revealed a “reparametrization” trick, similar to that of\nKingma and Welling (2013). Meanwhile, Wawrzy´nski and\nTanwani (2013) trained stochastic policies using a replay\nexperience buffer with the actor-critic framework. The use\nof the replay buffer has been essential to ensure that data\nsamples are independent and identically distributed. This\nparticular technique was popularized by Mnih et al. (2015)\nwith original insight dating back twenty years in work by Lin\n(1993). A trusted policy optimization approach proposed by\nSchulman et al. (2015), directly builds a stochastic neural\nnetwork policy without this decomposition and does not\nrequire the learning of an action-value network. It appears\nto produce near monotonically improvements but require\ncareful selection of updates to the policy parameters to\nprevent large divergences to the existing policy. Furthermore,\nit has been theorized that this technique appears to be\nless data efﬁcient. Work by Hausknecht and Stone (2015)\nhas demonstrated a solution to reinforcement learning\nin continuous parameterized action spaces, where they\nsuccessfully trained a RoboCup soccer agent that scored\nmore reliably than the 2012 champion.\nUsing value function estimation like these approaches for\ncontinuous domains generally uses two networks to represent\nthe policy and value function individually Schulman et al.\n(2015); Lillicrap et al. (2015). There has been work however,\nto reformulate the original Q-learning scheme that results\nin an elegant effort that can be ported to the continuous\nsetting—namely work by Gu et al. (2016b) has attempted\nto show this by learning a single network that outputs\nboth policy and value function. Their work was based off\ndueling networks shown by Wang et al. (2015) where they\ndecomposed learning into two streams corresponding to the\nlearning of state-value and advantages for each action—\nthough their work was presented in the discrete setting.\nThese dueling networks were improved from the formulation\nof double Q networks. The motivation behind double Q\nnetworks is that Q-learning, even in the tabular setting,\nexhibits overoptimism due to estimation errors, however,\nVan Hasselt et al. (2015) found that by decomposing\nthe max operation in the target into two value functions\ncorresponding to action selection and action evaluation, not\nonly reduced this particular overoptimism, but also lead to\nperformance increase.\nHowever, approaches that rely on a model-free reinforce-\nment learning method like traditional Q-learning has yet\nanother major drawback. They are tragically inefﬁcient with\nexperience. For example, the learned Atari policies required\nmillions of gameplay examples to converge to competence.\nConsequently, in the past a number of model-based methods\nlike Dyna Sutton (1990, 1991), Prioritized Sweeping Moore\nand Atkeson (1993), and Queue-Dyna Peng and Williams\n(1993) have been suggested to make more efﬁcient use of\ntraining examples while increasing computation. Many of\nthese select k samples to use for update as opposed to a single\nupdate with traditional Q-learning. To do this, these methods\nuse experience to not only learn an optimal policy, but also\nconstruct a transition ˆT and reward ˆR model, meanwhile\nupdating the values of k additional state-action pairs. And\nas summarized by Kaelbling et al. (1996), Dyna does this by\nselecting k randomly while the latter two methods prioritize\nthe selection of the pairs to “regions of interest.” Dyna\nis shown to converge ten times faster than traditional Q-\nlearning and the prioritized methods being two-fold faster\nthen Dyna—thus, making much better use of experiences\nfor learning. Quite recently, these ideas stemming back two\ndecades of research has reincarnated into a deep learning\nframework that incorporates model-based acceleration for\ndeep reinforcement learning with continuous control param-\neters. First, Gu et al. (2016b) reformulated Q-learning in\nthe continuous setting into normalized advantage functions,\nan alternative to policy gradient and actor-critic methods,\nwhich decomposes the quality term Q into a state value\nterm V and an advantage term A. This particular insight\nhave been explored by others in the past Baird III (1993);\nHarmon and Baird III (1996); Wang et al. (2015). Next,\nthey showed that policy learning can be achieved by taking\na learned model of the dynamics and simulating synthetic\nplausible outcomes via imagination rollout and appending\nthe experiences to the replay buffer. Doing so, increases the\nefﬁciency of data usage and is a likely candidate to learn the\ncontrol parameters needed for γT networks. Interestingly,\nthese imagination rollouts according to learned dynamics\nmodels corresponds to a form of λ-return, where given this\nmodel, we can simulate a number of n step trajectories by\ntraversing this aspect transition network. We then weigh\nthese n step backups yielding a compound backup—such\nan update has been shown to make more efﬁcient use of\nexperiences Sutton and Barto (1998).\nIn a distributed approach, Gu et al. (2016a) introduced a\nparallelizable learning algorithm leveraging the normalized\nadvantage functions, to be used across multiple robots which\ncan pool their policy updates asynchronously resulting in\naccelerated learning.\nWith almost all frameworks to date, experience updates\nfrom the replay buffer were uniformly selected from this\nreplay buffer. In fact, it this may not be ideal since\nindividual samples may have varying degrees of signiﬁcance.\nAs a result, Schaul et al. (2015) proposed the prioritized\nexperience replay which samples at the same frequency\nthey were originally experienced—this result was shown to\nimprove state of the art, and perhaps could be a potential\ncandidate for model-based acceleration sampling. Similar\nworks by Zhai et al. (2016) used prioritized sampling to bias\nexperience selections.\nModel referred updates accelerates learning by simulating\nsynthetic on-policy futures. In essence, this describes a\nforward dynamics model of the interactions, perhaps even\nover a lifetime of interaction experiences. Evidently, the\nuse of Guided Policy Search tries to ﬁt the dynamics in a\nlocally-linear fashion and using the model as a reference\nto apply imagination rollouts by placing these cumulatively\nconstructed synthetic experiences into the replay buffer\nfor updates. There is in fact, a connection between these\ndynamics models that attribute to the increased efﬁciency\nof deep reinforcement learning methods and algorithms\nto provide artiﬁcial curiosity. Quite fortunately, these\nrepresentations that concern the transition dynamics and\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n13\nreward have immediate ties and are analogous to an\ninteraction-based knowledge repertoire that adheres to a\nseries of task planning and intrinsic motivation frameworks.\n4\nUnsupervised Affordances\nThe\ndevelopment\nof\nan\ninteraction-based\nknowledge\nrepertoire is an important structure that can be incorporated\ninto a number of algorithms that necessitates forward models\nor predictions of future state. Fortunately, the learning of this\naction centric knowledge is a close analog of the transition\ndynamics that is already a component of these model-\nbased acceleration techniques presented to make efﬁcient\nuse of experiences in reinforcement learning paradigms.\nAs such, this structural acquisition has immediate ties to\nintrinsic motivators that seek to provide artiﬁcial curiosity\nto autonomous systems. Intrinsic motivators allows systems\nto build their own representations that reﬂect the inherent\nuncertainties of the system.\nCuriosity is important for learning systems—as without\na sense of curiosity, learning becomes very task speciﬁc\nand one dimensional—the robot can not choose to learn\nnovel skills, only a task-speciﬁc motion for a human-deﬁned\ntask. For such reasons, one might consider introducing some\nform of curiousity Frank et al. (2015) into the approaches\npreviously discussed. We believe that the development of\nmotor behavior should be driven by intrinsic motivators in\norder to learn task-generalizable skills that can be reused in\nthe future. While a thorough survey of intrinsic motivators\nis outside the scope of this paper (for further readings, see\nBarto (2013)), we acknowledge that a number of researchers\nwho have applied schemes to autonomously learn new\nskills Utgoff and Stracuzzi (2002); Barto et al. (2004) and\nrepresentations Oudeyer et al. (2007); Hart (2009b). Instead,\nwe envision a system that accomplishes this simultaneously.\nAs for this, we look into insight from cognitive\ndevelopment. In fact, a number of studies have shown\nthat motor development in biological systems greatly\ninﬂuences the development of perception and cognition.\nFor instance, Piaget (1953, 1954) described motor skills\nas a mechanism that drives development in other domains\nby generating new sensorimotor experiences and further\nstudies have described cognition and perception as embodied\nphenomena—grounded to the body and its actions Gibson\n(1988). Other studies have shown that infants are highly\nsensitive to action-outcome relations and are capable of\nlearning contingencies between their own behavior and\noutcomes in the world. In particular, Libertus and Needham\n(2010) showed that sensory-motor experiences motivates\ninfants to reproduce manipulation outcomes and foster\nreaching and grasping skills. Evidently, infants learn models\nthrough manipulation and interaction forming a sense of\nbehavioral organization6. Such organization is an interplay\nbetween representation and motor development, driven\nby curiosity—which we regard, should adhere to model-\nreferenced objectives that aim to explain the complex\ndynamic phenomena in the world. A common, perhaps,\nubiquitous representation describes action-related contexts\nregarding entities in the world. Such a representation\nis of chief importance to cognitive systems. In fact,\nHemion (2016b) argues that instead of having contemporary\ncomputational reinforcement learning agents learn each\nindividual skill entirely from scratch, the development of a\nworld model can be used to support adaptive behavior and\nlearning for cognitive systems.\n4.1\nPredicting the Dynamics of the World\nOriginating from ecological approaches to visual perception,\nthe central concept to the Gibsonian perceptual framework\nis the notion of an affordance, an observable environmental\ncontext that invokes a variety of latent interactions.\nAffordances emphasize an agent-world relationship and\nconstitutes an interactionist account of perception as it\nreﬂects environmental signals in relation to an agent’s ability\nto act on those signals Chemero (2003). In the strongest\nsense, Gibson’s theory of direct perception holds that the\ntransformation from signal to behavior is expressed directly\nby neural projections that evolve to recognize opportunities\nfor context-speciﬁc actions Reed (1996); Turvey (1992).\nSuch theories emphasize that percepts themselves provide\na direct index into all the “action possibilities latent in\nthe environment” Gibson (1977), thus, applicable actions\nand related outcomes are immediately recognized without\nnecessarily identifying the object itself.\nThe Gibsonian notion of affordances describes action\npossibilities and can be seen as a surrogate of world state,\ninduced by sensory input and interaction. Gibson’s theory of\naffordance advocates for modeling the environment directly\nin terms of the actions it affords. These representations\nare idiosyncratic and reﬂect only those actions that can\nbe generated by the agent. Research has been done to\ninvestigate the autonomous acquisition of such affordance\nrepresentations with intrinsic motivators. For instance, an\nexample of multiple intrinsic reward functions have been\nproposed to learn the transition dynamics of a particular task\nHester and Stone (2015). Others have looked into domain-\nindependent intrinsic rewards, like novelty or certainty,\nfor learning adaptive, non-stationary policies based on\ndata gathered from experience Hart (2009b); Sequeira\net al. (2014). In particular, model exploration programs\nhave been presented by Hart (2009b), but the methods\nreported lacked multimodal sensor integration and do not\nproduce knowledge structures that are easily transferrable\nto other tasks. A multimodal structure learning paradigm\nwas proposed by Wong and Grupen (2016) extending the\nideas initially presented by Hart—in their studies, they\nleveraged a promising representation describing affordances\n6The developmental process from neonate to approximately a year in\nage consists of a precisely-timed chronicle of emergence and inhabitation\nof primitive, postural, or bridge reﬂexes that contribute to the organized\ndevelopment of complex behavior and skill acquisition Law et al.\n(2011). With age, myelination occurs in the infant, resulting in increased\ncontrollable degrees of freedom and resolution in motor activity Oudeyer\net al. (2013). This form of maturation is especially prominent when ﬁne\nmotor control start to emerge in cases such as pincer grasp reﬂexes. When\nthe infant develops appropriate skills, it begins to play and interact with\nthe environment and objects in it by exploratory activity Oudeyer et al.\n(2007). Increased motor acuity and reﬁned motor skills are important\nfor development in general and affect what kinds of information can be\nextracted from the environment. As motor skills develop, complicated\nrepresentations of the world can too be constructed through addition\ninformation provided through these actions Libertus and Needham (2010).\n14\nin terms of aspect nodes. The graphical structure called an\naspect transition graph encodes Markovian state as nodes\nand actions as edges in a multi-graph Ku et al. (2014).\nSuch a model is generally used in object identiﬁcation tasks\nby planning in belief space (rolling out a population of\nthese forward models) to select informative actions Sen and\nGrupen (2014); Ruiken et al. (2016b). Methods for robots\nto autonomously acquire these models has been described in\nwork by Wong and Grupen (2016); Ruiken et al. (2016b),\nwhere systems are intrinsically motivated by a variant of the\ndifferential variance function originally proposed by Hart\n(2009b) to acquire complete graphical representations of\nobjects—associating actions and futures derived from all\npossible interactions under controlled settings.\nEvidently though, the aspect transition graph model\nhas two major disadvantages. One being that in studies\nregarding learning these graphs by Hart (2009b); Wong\nand Grupen (2016); Ruiken et al. (2016b), it is assumed\nthat a sample mean and variance approximates the true\nunderlying transition distribution. Unfortunately, it is not the\ncase that this distribute is necessarily Gaussian N(µ, Σ),\nfor instance, it may be arbitrarily complex and multimodal.\nThe next large criticism is that these models assume some\ndiscretization granularity of sensory input space into aspect\nmodels. The deﬁnition of what constitutes an aspect is task-\ndependent and difﬁcult to manage in a task-generic way.\nBoth of these issues can be addressed by approximating this\narbitrary complex representation in high dimensionality—\nthis approximation over the Markovian state describing\nwhat constitutes an aspect and potential outcomes derived\nfrom interactions attributes to a new model, a deep aspect\ntransition network. This network is otherwise an extension\nof the original aspect transition model graphical structure\nwith the key distinction being that it captures interactions\nover many possible granularities, deriving a continuous form\nof aspect state.7 A fundamental extension to the graphical\nstructure is that the deep variants makes no assumptions of\naspect boundaries, rather, aspect nodes take on continuous\nstate description.\nThe acquisition of a model that explains the dynamics of\nentities in the world and their evolution through interaction\n(the representation we referred to as an aspect transition\nnetwork) is closely related to work that attempts to predict\nphysics, structure, and futures given current state. Despite\napplicable research in deep ﬁltering and sensor fusion\napproaches Krishnan et al. (2015); Jain et al. (2016), likely\nmaking predictions in the original sensory-space holds\npromise in robustness for planning algorithms, since actions\nare directly derived from future scenes. Many encoder-\ndecoder networks hope to achieve this by upsampling to\ngenerate predictions in visual (more speciﬁcally, sensory)\nspace. Unfortunately, the prediction in visual space may be\nnonsense. For instance, recall many of these hill-climbing\nalgorithms to fool the network into predicting visually\ninplausible images. As such, a particular line of work have\nlooked into guarantees that the prediction has physical\nproperties that are relevant and meaningful in real life.\nGoodfellow et al. (2014a) proposed the use of generative\nadversarial networks (GANs) that have been widely\naccepted as a tool to generate visually plausible predictions\nthat fall into the realm of reality, rather than blurred or\nmeaningless output. This is accomplished by simulating\ntwo models: a discriminative model D and a generative\nmodel G who is trained to maximize the probability of\nD making a mistake—this framework is analogous to a\nminimax two-player game. Building off this work, Mathieu\net al. (2015) incorporated the adversarial training techniques\ninto their convolutional network architectures to deal with\nblur resulting from standard mean squared error loss. They\nshowed that their network was capable of predicting vivid\nfuture scenes under an image gradient difference loss\nfunction given a set of input sequences.\nLearning the transition dynamics of entities in the world\nhas immediate correlation with an understanding of intuitive\nphysics. Work by Lerer et al. (2016) incorporated a variant\nof the DeepMask network Pinheiro et al. (2016) that was\naltered to support multi-class predictions and replicated\na number of times to predict the segmentation trajectory\nof multiple time steps in the future for a falling block\nprediction task. A ResNet-34 was trained as the trunk of\nthe convolutional network and their approach, PhysNet,\nwas shown to outperform all other methods for predicting\nthe future locations of the falling blocks. To insert spatial\ninvariance to neural networks, work by Jaderberg et al.\n(2015), introduced a differentiable Deep Spatial Transformer\nmodule that can be applied to convolutional networks\nallowing it to be able to explicitly actively transform inherent\nfeature maps.\nJain et al. (2015) presented a generic framework to model\ntime-space interactions using statio-temporal graphs with a\nrecurrent neural network architecture. The use of spatio-\ntemporal structures impose high-level intuitions allow for\nimprovements in modeling human motion and predicting\nobject interactions. Prediction work by Oh et al. (2015) has\nmade several interesting discoveries in predicting futures\nin the gameplay domain (namely in the game Space\nInvaders). They showed that a feedforward network was\nbetter at predicting precise movements of objects when\nrecurrent structures consistently made a few pixels of\ntranslation error. Their hypothesis is due to the failure of\nprecise spatio-temporal encodings in the recurrent setting,\nhowever, they found that recurrent structures were better at\npredicting events that have long-term dependencies. Long-\nterm sequential movements of objects as a result of an\napplied force vector at a particular location in the image\nwere learned by a deep neural network while taking into\naccount the geometry and appearance of the scene by using\nconvolutions and recurrent layers in the network Mottaghi\net al. (2016). Others looked into building models for action-\nconditioned video prediction that explicitly models the\nmotion of pixels rather than predicting the future as a\nwhole. This is achieved by predicting distributions over\npixel motion from previous frames and as a result, the\nmodel is partially invariant to occlusions. Their model was\ntrained on a dataset of 50, 000 robot interaction videos and\nresulted in the learning of a “visual imagination”—a concept\nof predicting different futures based on the the robot’s\n7The choice to derive these transition networks from affordances deﬁned\nthrough aspect graphs is due to convenience. We are aware and acknowledge\nthe numerous affordance representations in the literature ike Object Action\nComplexes (OACs) Kruger et al. (2011), etc, to name a few.\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n15\ncourses of actions Finn et al. (2016a). Similarily, Santana and\nHotz (2016) trained a realistic, action-conditioned vehicle\nsimulator using generative adversarial networks. These video\nprediction mechanisms share a similar action-conditioned\nform of function approximation as aspect transition networks\ngiven by fW : s, ρφi|σ\nτ 7→s′.\nFurther work presented by Agrawal et al. (2016) allowed\na robot to gather over 400 hours of experience by poking\ndifferent objects over 50, 000 times. They learned both an\ninverse and forward model of the dynamics—the inverse\nmodel provided supervision to build informative visual\nfeatures, which then was used by the forward model to\npredict the interaction outcomes. They refer to these accurate\nmodels for multi-step decision making.\n4.2\nDeriving Environmental Reward\nA likely candidate for environmental reward is one that is\ncomputed through an information theoretic interpretation\nof the affordance prediction networks corresponding to\nthe system’s understanding of interaction and dynamics\nof the world. Assume the robot interacts with the world\nby executing some set of control programs and obtains\ninteraction tuples ⟨s, Φ, P, S′⟩describing the initial state\ns, the control programs that were executed φi|σ\nτ ∈Φ with\nparameters ρi ∈P resulting in future states s′\ni ∈S′—in turn\nthis outlines some experience dataset described by Dt =\n{e1, e2, · · · , en} where each experience tuple consisting of\nei = ⟨s, φk|σ\nτ , ρk, s′⟩.\nNow, consider the class of reward structures that adhere to\nusing uncertainty and degree of understanding to penalize\nor promote the selection of new actions and behaviors to\nemerge. A prominent example of structures of this nature\nis the differential variance intrinsically motivated function\noriginally proposed by Hart (2009b). Such a function\nmotivates systems to perform actions that it is most uncertain\nabout, allow it to exploit this reward to build new behaviors.\nWong and Grupen (2016) proposed to use this function\nto learn complete affordance models by showing that the\nsystem consumes rewards as representations become more\naccurate. Unfortunately, these metrics imply a Gaussian\ndistribution assumption that likely fails when adapting\nto high dimensional sensory-spaces, as such candidate\nsurrogates are information theoretic functions that prescribe\ndistance metrics on predictions and truths. As such consider,\nIfW = H(fW (s, ρφi|σ\nτ )) + H(s′) −H(fW (s, ρφi|σ\nτ ), s′)\nwhere, IfW = I(fW (s, ρφi|σ\nτ ); s′)) expresses the mutual\ninformation between the prediction of what the outcome\nshould be according to the network fW given state s and\ncontrol parameters ρφi|σ\nτ and the actual outcome s′—in\nessence, a measure of the similarity between prediction\nfW (s, ρφi|σ\nτ ) and result s′.\nWe consider this a candidate reward scheme and express\nits mechanics under two plausible scenarios,\nScenario 1, Low Mutual Information: In the case that\nthere is low mutual information between the output of\nthe affordance network fW and the true outcome s′, this\nis attributed to two likely culprits, either the network\napproximating fW is not converged, in which case, the\nsystem does not have a good understanding of the underlying\ndynamics of the world, or the action networks corresponding\nto fγT , fρ, fπ∗\nN do not approximate the appropriate control\nparameters or falsely predicts activations of primitives in the\nworld. Either way, these networks are continuously trained\nwith current dataset Dt.\nScenario 2, High Mutual Information: In the case that\nthere is high mutual information between these quantities,\nthe system has acquired good approximations to interaction\noutcomes via fW\ngiven its current set of controllers\nexpressed\nas\nboth\nprimitives\nφi|σ\nτ ∈Φ\nand\ncomplex\nencodings π∗\nj ∈Π∗. And those behaviors have likely found\nuseful control goals describe by the approximator fρi. As\nsuch, this becomes a state of either habituation or emergent\nbehavior—when high mutual information is observed, a\nlikely course of action to continue the development of\ncomplex behaviors and interaction-based data collection is\nto spawn a new network fπ∗\nn+1 with the sole purpose of\nattempting to learn new behavioral control sequences and\ncompositions that result in unexpected transition dynamics\nin the world. Simply, the control policy is rewarded when it\nlearns sequences of actions that fool the dynamics prediction\nnetwork fW into new states, while the fW networks uses\nthese novel interactions to reﬁne its inherent representation.\nThis direction of thinking is adapted from a form of\nadversarial training where the affordance network tries\nto best predict the outcome of futures under interactions\nwhile the behavioral networks attempts to learn new control\nparameters that the affordance network fails to predict—\nhence, new behaviors develop that broaden the system’s\nunderstanding of the world. From this fact, the reward\ngiven to the predictor fW and the reward given to the\ndeveloping behavioral policy network fπ∗\nn+1 can not be and\nshould not be the same. In fact, they exhibit an inverse\nvariation phenomena, therefore, the reward for new behav-\nioral policy networks must be derived from the predic-\ntion network’s ill-performance. An example reward struc-\nture that obeys this particular property is the Kullback-\nLeibler (KL) divergence given by, D(fW (s, ρφk|σ\nτ )||s′) =\nP\ni,j fW (s, ρφk|σ\nτ )i,j log(fW (s, ρφk|σ\nτ )i,j/si,j —this partic-\nular quantity is ubiquitously used as a measure of the simi-\nlarity between two distributions, describing relative entropy.\nSuch metrics generally concern controlling the exploration\nand exploitation tradeoffs in learning architectures Levine\net al. (2015). For instance, a number of approaches have\nused the KL-divergence to control policy update step sizes\nPeters et al. (2010); Levine and Abbeel (2014); Schulman\net al. (2015); Akrour et al. (2016).\nA key concern with this approach is the stability of the\nlearned control policy under an ever-changing dynamics\nmodel may be compromised, especially when computing\nrewards in response to its predictive accuracy. To address\nthis, it is wise to consider a target network paradigm like\nthat of Mnih et al. (2015), except instead of freezing the\ntarget Q network for stability, one should consider freezing\nthe affordance network fW when computing rewards for the\ncorresponding behavioral policy network. Since otherwise,\nthese rewards will be drastically non-stationary and thus may\nhave large implications on convergence issues.\n16\n5\nImplications for Task Planning\nPerhaps, one of the most recurrent themes throughout this\nmanuscript is that there likely is not a single method\ncapable of solving all problems, especially those that\nconcern developing artiﬁcial intelligence for physical robotic\nsystems. Similarly, we ﬁnd that a candidate approach is the\nmarriage between several theisms of thought. For instance,\nconsider a lifelong learning framework that generates useful\nartifacts that task planners can exploit—in actuality, let\nus brieﬂy entertain this idea. Quite obviously, we would\nlike robots to perform useful tasks or missions in the\nreal world given its massive repertoire of motor skills and\nprecise, learned representation of interaction dynamics in\nthe world. Because these representations and control policies\nare all derived by the robot through intrinsic motivation, it\nencodes inherent uncertainties that allow for robust plans\nand execution of actions. So evidently, it is up to task\nplanners to ﬁnd plans over these control skills and transition\ndynamics such that the robot will solve useful problems8.\nPlanning generally assumes some form of forward dynamics\nmodel, of how actions affect the state of the world—in\nthis particular scenario, we suggest that a learned aspect\ntransition network fW will serve purposefully as it is a\nrepresentation for forward dynamics. In other works, basic\npush motion planning was achieved using dynamics in the\nform of video prediction and visual foresight Finn and\nLevine (2016). In another study by Tamar et al. (2016),\nvalue iteration networks were presented as an architecture\nthat allows systems with the capability of learning to plan\nby embedding the fully differentiable neural network with a\n“planning module.”\nInterestingly, research has shown that an aspect geometry\nalone is sufﬁcient in describing a number of complex\nrobotic tasks Ruiken et al. (2016a). In particular, object\nidentiﬁcation and assembly tasks can be reconﬁgured into a\nmodel-referenced belief-space planner. The aspect deﬁnition\nprescribes sensory geometries to deﬁne a Markovian state\ndescribed by an aspect node in a geometric structure\noutlining the geometric constellations under some ﬁeld of\nview—thus, encoding latent affordances of entities in the\nworld. The speciﬁc geometries of features embedded in the\nenvironment can be used to drive belief-space architectures\ninto task-speciﬁc solutions. Simply in this setting, the\nartifacts produced by the approximations describe the\nnetworks fγT , fπ∗, fρ and fW , which are respectively the\nnetworks for control state prediction, complex behavioral\npolicies, continuous control parameters, and world transition\ndynamics, can be used during planning rollouts (i.e. a\nMonte Carlo simulation of trajectories according to action-\nconditioned transition dynamics fW via parameters fρ).\nWe decompose this task solution into a mathematical\nrepresentation outlined by a (partially observable) Markov\nDecision Process. Firstly, Markovian state is given by\nthe robot’s perception or raw sensory input. The set of\nactions in this case collectively describe the set of all\ncontrol state predictors, control parameter learning networks,\nand complex policies given by the three set of networks:\nfγT , fπ∗, fρ. And lastly, the transition dynamics are encoded\nthrough the aspect transition network fW , with actions\nbeing constrained through the γT -networks’ prediction that\ndecides the likely control states at any given time instance.\nIn planning, one may simply perform rollouts over the\nMarkovian state and predict likely candidate actions that can\nbe executed. Many planning approaches at this point resort\nto sample techniques in conjunction with RRTs, preimage\nbackchaining Kaelbling and Lozano-P´erez (2011), or large\npopulation of dynamics models Ruiken et al. (2016b).\nInstead, we have control state networks that speciﬁcally\ndescribe candidate control programs—for each of these\nactions, we rollout under the aspect transition network\nthe candidate future states. A similar affordance model\nreferenced Active Belief Planner Ruiken et al. (2016b) has\nbeen presented recently to solve object identiﬁcation tasks.\nIn fact, their planner uses these affordance representations\nas forward models during problem solving behavior in\nnon-ideal contexts that include sensor noise, suboptimal\nlighting, missing information, and extraneous information\narising from scenes that can contain multiple objects\nin initially unknown arrangements. Unfortunately, their\nmethods requires that large populations of transition models\nare explicitly described—this population is what would be\na useful structure to approximate using fW , the aspect\ntransition network.\nEvidently this roll out can be performed by a number\nof generic planners that expand states accordingly to their\nfuture dynamics. As such, a number of planners like A*,\nAll-Domain Execution and Planning Technology (ADEPT)\nRicard and Kolitz (2003), or Hierarchical Planning in the\nNow (HPN) Kaelbling and Lozano-P´erez (2011) can solve\nfor task relevant plans while leveraging learned artifacts for\nadditional robustness.\nA fundamental problem with planners in general consist of\nthe planning horizon and the branching factor prescribed by\nthe possible number of actions at any given state. Hierarchy\nattempts to reduce the planning horizon by only planning\nto the ﬁrst executable primitive. Still, hierarchical planning\nscales exponentially (number of operators to the shallowest\nabstraction level). Fortunately, the control state description\nnetworks fγT and complex behavioral policy networks fπ∗\ndrastically helps reduce this planning complexity by in turn\ndecreasing the depth at which the planner must roll out due\nto the consideration of more complex motions. Secondly, the\npossible actions at any given state is constrained by those\nphysically plausible which is immediately evaluated by the\napproximator fγT\ni for each control program φi|σ\nτ —quiescent\nactions should then not be considered.\nInterestingly, another problem that arises is when a\ncognitive system increases its skill set, this in turn has\nmonotonically increasing effects on the branching factor\nof planners. One may consider only feasible transitions to\nattack this phenomena—the feasibility of actions requires\neither geometric evaluations or explicitly deﬁned dynamics\nmodels. For instance, HPN uses generators that reason over\nthe geometry of entities in the world Kaelbling and Lozano-\nP´erez (2011). The concept of an aspect transition graph\nunder planning frameworks do a good job of ensuring\n8While a complete review of all task planning frameworks is outside the\nscope of this paper, we discuss how lifelong learning artifacts can integrate\nwith a number of selected planners to accomplish task-relevant solutions.\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n17\nthat only feasible actions are planned for by exploiting\nthe likely transitions under learned object models Ruiken\net al. (2016b). Similarily, the network variants are good\ncandidates for quickly evaluating potential control contexts\nand feasibility of particular actions parameters.\nImportantly, by no means do the control policies\nand transition dynamics learned through lifelong intrinsic\nmotivation limit the use of more sophisticated planners.\nDealing with unstructured and dynamic environments\nbecomes a fundamental problem. Therein, a number of\nframeworks have been adjusted to operate over uncertainties\nlike for example, the Active Belief Planner Ruiken et al.\n(2016b) and the Belief-space HPN Kaelbling and Lozano-\nP´erez (2013).\nA key insight is that many of these video prediction\nparadigms (as described in Section 4) can be inferred as\naffordance models that predict all possible futures given\ninteraction with the world—a promising advantage of using\ngenerative adversarial networks. As such, this inherently\nencodes the belief over many possible outcomes that may\nresult in interaction and can be leveraged in planning. For\ninstance, one can operate in the belief space of futures by\nobserving the manifold on which the many futures lie. It\nappears that deep generative adversarial networks, like the\nfuture prediction networks trained adversarially, obey certain\narithmetics Radford et al. (2015) and as a result can be used\nto discover such a futures manifold.\n6\nConclusion\nThis paper has provided an initial survey of recent advances\nin deep learning applicable to mobile perceptual systems,\nnamely pertaining to the robotics domain. We discuss a\nseries of challenges that arise when applied to physical\nembodied systems that are otherwise unseen in strictly\nvision and simulation domains. And we have outlined these\nrecent advances in detection, control, and future prediction\nproblems that are most relevant to robotics and candidate\nplanners in a new learning direction.\nThese advances were structured in this manuscript in such\na way that implies a future direction in self-supervision and\nlifelong learning. Piecing together these individual research\nideas, we indicate that the technologies currently may be ripe\nto design a lifelong self-supervised system to learn complex\nbehaviors in the real world. As such, the acquisition over an\nextended period of learning can be leveraged in numerous\nrobotics tasks both in research and industry by coupling\nthese learned artifacts with existing task planners. As Silver\net al. (2013) mentions, it is time to move on from task-\nspeciﬁc machine learning—instead, learn over an extensive\nrepertoire, over numerous tasks in order to acquire general\nintelligence.\nHowever, with using physical hardware a concern\nrevolving around exploration of control actions comes\ninto play. One of the most fundamental concerns with\nlearning systems considers the question of what constitutes\na safe exploration paradigm. Rather, how does one ensure\nthat the system does not perform catastrophic actions\nduring exploration? Safe reinforcement paradigms have\nbeen outlined by Thomas (2015), discussing algorithms to\nsearch for new and reﬁned policies while ensuring that\nthe probability of bad policies are minimized. In these\nworks, Thomas et al. (2015) presented a method using\nthe trajectories of other policies that were executed in the\npast to efﬁciently, with high conﬁdence, perform off-policy\nevaluations to gauge exploration candidates. With such an\napproach, it becomes possible to evaluate the performance of\nnew policies without explicit execution. Perhaps, the future\nfor self-supervised systems lies in the connection to metrics\nthat safeguards the hardware while effectively evaluating its\npossible actions. Measures like these should be considered\nin order to built a system that learns over a lifetime of\nexperiences.\nRecently, the emergence of deep symbolic reinforcement\nlearning may be a promising architecture by combining\nrecent breakthroughs in deep reinforcement learning with\nclassical symbolic artiﬁcial intelligence Garnelo et al.\n(2016). Simply, a neural network backend is used to extract\nuseful symbolic representations which are then used by a\nsymbolic frontend for action selection. Although the work\nis still at its infancy, a fundamental drawback is that alike\nthe formulation of DQN, it requires that the system has a\nspeciﬁed task that it is trying to solve in which reward can\nbe evaluated from. In fact, the resulting artifact of this is\na meta-policy composed of sub-policies under a speciﬁed\ntask—these sub-policies are however locally optimal under\nany combination of interactions between entities. There\nare two evident issues consisting of scaling, due to the\nnature of considering all possible interactions, and troubles\nwith global optima. However, insight from the two network\napproach, learning a value and a policy network, may help\nsupport some of these immediate issues. Perhaps as this idea\ndevelops, it may be considered as a module in lifelong self-\nsupervision, due to its promising connections with symbolic\nhierarchical planning Kaelbling and Lozano-P´erez (2013).\nAn important aspect of reinforcement learning is the\ncapability of transfer, both between systems and between\ntask domains. Work by Devin et al. (2016) provided insight\non the decomposition of network policies into robot-speciﬁc\nand task-speciﬁc modules that supported transfer between\ntasks and different robot morphologies (e.g. varying in\nnumber of links and joints). Interestingly, under the control\nbasis formulation, parametrized controllers already supports\ngeneralization from robot to robot, with an assumption\nthat the new system has sufﬁciently motor resources the\nsame control objectives. As such, the high level behavioral\nnetworks, those that are composed of primitive parametrized\ncontrollers, too inherit this form of generalizability.\nA good review by Lake et al. (2016) discusses the\nfundamental cognitive problems with building systems that\nexpertly accomplish tasks by pattern recognition alone.\nWherein to build cognitive systems that learn and think\nlike people, they suggest that these systems must have the\ncapability to support both explanation and understanding.\nSystems must be able to understand intuitive physics,\nhave the capacity to learn to learn, and build grounded\ngeneralizations that span new tasks and situations—such\na view is similar to the direction presented in this paper.\nOur survey is particularly tailored towards the connection\nof these ideas with physical robot systems. We suggest that\nperhaps the goal is not the build a system that exactly mimics\nhuman cognition and learning, but instead draw insight and\n18\ncomputational analogs from ideas in cognitive development.\nRobots are not humans and do not necessarily have to learn\nat their granularity nor produce the same artifacts through\nlearning. But, we regard that studying the development of\ncognition in biological systems may be crucial in building\nalgorithms for artiﬁcial systems.\nIn this manuscript, we outlined powerful nonlinear\napproximation\ntools\nwith\ninspirations\nfrom\ncognitive\ndevelopment and control theory to produce a direction\nin which lifelong learning frameworks can be applied\nto\nautonomous\nsystems\nthat\ncontinuously\nacquire\na\nhierarchy of complex motor behaviors in addition to\na dynamics representation of interactions in the world.\nA number the ideas presented in this manuscript were\ninﬂuenced by the computational development of action\nand representation going back to Grupen and Huber\n(2005). Their work, however, assumes there exists some\npreordained developmental guideline under the notion of a\nDevelopmental Assembler that provides design constraints\nand developmental schedules. Such entities assigns task\nspeciﬁc rewards that are the fundamental motivators to the\nacquisition of complex behaviors. In the case of our review,\nwe outlined an example reward paradigm that computes\nreward that adhere to the system’s internal predictions\nof the world and of it evolution through interaction—\ntying together the dynamic modeling of action related\ncomplexes in the world. Further, we surveyed numerous\ndeep learning advances pertaining to robotics and found a\nclose connection between many deep reinforcement learning\nparadigms with classical concurrent control schemes under\nthe control basis formulation. With this survey, we would\nlike to acknowledge that deep learning should be considered\nas an hyperparametric approximation tool that alone is likely\nnot capable of attacking all problems. And as such, we\nforesee a direction in which these powerful approximators\nare integrated with closed-loop control and optimization\nparadigms, driven by principled motivators, and inspired\nby insight from cognitive development to realize a robust\nlifelong self-supervised system.\nAcknowledgements\nAlthough the author’s afﬁliations are with the Charles Stark\nDraper Laboratory, Inc, any opinions, ﬁndings, conclusions,\nor recommendations expressed in this material are solely\nthose of the author(s) and do not necessarily reﬂect the views\nof organization.\nI would like to thank Takeshi Takahashi and Roderic\nA. Grupen for their expertise, enthusiasm, and insight on\npreliminary work with control state prediction networks that\nlead up to this direction of thinking.\nLastly, and most importantly, the stand, the structure, and\nthe direction of this manuscript would not be in its current\nform, without the critiques and “skepticisms” of my good\nfriend Mitchell Hebert.\nReferences\nAgrawal P, Nair A, Abbeel P, Malik J and Levine S (2016) Learning\nto poke by poking: Experiential learning of intuitive physics.\narXiv preprint:1606.07419 .\nAkrour R, Abdolmaleki A, Abdulsamad H and Neumann G (2016)\nModel-free trajectory optimization for reinforcement learning.\narXiv preprint:1606.09197 .\nAronson A (1981) Clinical Examinations in Neurology. Philadel-\nphia, PA: W.B. Saunders Co.\nArulkumaran K, Dilokthanakul N, Shanahan M and Bharath AA\n(2016) Classifying options for deep reinforcement learning.\narXiv preprint:1604.08153 .\nBadrinarayanan V, Kendall A and Cipolla R (2015) Segnet: A\ndeep convolutional encoder-decoder architecture for image\nsegmentation. arXiv preprint:1511.00561 .\nBaird III LC (1993) Advantage updating. Technical report, DTIC\nDocument.\nBalduzzi D and Ghifary M (2015) Compatible value gradients for\nreinforcement learning of continuous deep policies.\narXiv\npreprint:1509.03005 .\nBarto AG (2013) Intrinsic motivation and reinforcement learning.\nIn: Intrinsically motivated learning in natural and artiﬁcial\nsystems. Springer, pp. 17–47.\nBarto AG, Singh S and Chentanez N (2004) Intrinsically\nMotivated Learning of Hierarchical Collections of Skills. In:\nInternational Conference on Developmental Learning.\nBendale A and Boult T (2015) Towards open set deep networks.\narXiv preprint:1511.06233 .\nBengio Y (2009) Learning deep architectures for ai. Foundations\nand trends in Machine Learning 2(1): 1–127.\nBojarski M, Del Testa D, Dworakowski D, Firner B, Flepp B,\nGoyal P, Jackel LD, Monfort M, Muller U, Zhang J et al.\n(2016) End to end learning for self-driving cars.\narXiv\npreprint:1604.07316 .\nBrooks R (1987a) Intelligence without representation.\nIn:\nProceedings of the Workshop on the Foundations of Artiﬁcial\nIntelligence. MIT Press.\nBrooks RA (1987b) Planning is just a way of avoiding ﬁguring out\nwhat to do next .\nByravan A and Fox D (2016) Se3-nets: Learning rigid body motion\nusing deep neural networks. arXiv preprint:1606.02378 .\nCarlini N and Wagner D (2016) Towards evaluating the robustness\nof neural networks. arXiv preprint:1608.04644 .\nChebotar Y, Kalakrishnan M, Yahya A, Li A, Schaal S and\nLevine S (2016) Path integral guided policy search.\narXiv\npreprint:1610.00529 .\nChemero A (2003) An outline of a theory of affordances.\nEcological Psychology 15(3): 181–195.\nCoelho J (2001) Multiﬁngered Grasping: Haptic Reﬂexes and\nControl Context.\nPhD Thesis, University of Massachusetts,\nAmherst, MA.\nCoelho J and Grupen R (1997) A control basis for learning\nmultiﬁngered grasps. Journal of Robotic Systems 14(7): 545–\n557.\nCoelho JA and Grupen R (1998) Dynamic control models as\nstate abstractions. In: Neural Information Processing Systems\nWorkshop:\nAbstraction and Hierarchy in Reinforcement\nLearning.\nConnolly CI and Grupen RA (1993) The applications of harmonic\nfunctions to robotics. Journal of Robotic Systems 10(7): 931–\n946.\nDa\nSilva\nB,\nKonidaris\nG\nand\nBarto\nA\n(2012)\nLearning\nparameterized skills. arXiv preprint:1206.6398 .\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n19\nDellin CM, Strabala K, Haynes GC, Stager D and Srinivasa SS\n(2016) Guided manipulation planning at the darpa robotics\nchallenge trials. In: Experimental Robotics. Springer, pp. 149–\n163.\nDevin C, Gupta A, Darrell T, Abbeel P and Levine S (2016)\nLearning modular neural network policies for multi-task and\nmulti-robot transfer. arXiv preprint:1609.07088 .\nFeng S, Whitman E, Xinjilefu X and Atkeson CG (2015a)\nOptimization-based full body control for the darpa robotics\nchallenge. Journal of Field Robotics 32(2): 293–312.\nFeng S, Xinjilefu X, Atkeson CG and Kim J (2015b) Optimization\nbased controller design and implementation for the atlas robot\nin the darpa robotics challenge ﬁnals.\nIn: IEEE-RAS 15th\nInternational Conference on Humanoid Robots (Humanoids).\npp. 1028–1035.\nFinn C, Goodfellow I and Levine S (2016a) Unsupervised learning\nfor physical interaction through video prediction.\narXiv\npreprint:1605.07157 .\nFinn C and Levine S (2016) Deep visual foresight for planning\nrobot motion. arXiv preprint:1610.00696 .\nFinn C, Levine S and Abbeel P (2016b) Guided cost learning:\nDeep inverse optimal control via policy optimization. arXiv\npreprint:1603.00448 .\nFinn C, Tan XY, Duan Y, Darrell T, Levine S and Abbeel P (2015)\nDeep spatial autoencoders for visuomotor learning.\narXiv\npreprint:1509.06113 .\nFrank M, Leitner J, Stollenga M, F¨orster A and Schmidhuber\nJ (2015) Curiosity driven reinforcement learning for motion\nplanning on humanoids. Intrinsic motivations and open-ended\ndevelopment in animals, humans, and robots .\nGarnelo M, Arulkumaran K and Shanahan M (2016) Towards Deep\nSymbolic Reinforcement Learning. arXiv preprint:1609.05518\n.\nGe SS and Cui YJ (2002) Dynamic motion planning for mobile\nrobots using potential ﬁeld method. Autonomous Robots 13(3):\n207–222.\nGibson EJ (1988) Exploratory behavior in the development of\nperceiving, acting, and the acquiring of knowledge.\nAnnual\nreview of psychology 39(1): 1–42.\nGibson JJ (1977) The theory of affordances.\nIn: Shaw R and\nBransford J (eds.) Perceiving, Acting, and Knowing: Toward\nan Ecological Psychology, chapter 3. Hillsdale, New Jersey:\nLawrence Erlbaum, pp. 67–82.\nGoodfellow I, Bengio Y and Courville A (2016) Deep learning.\nBook in preparation for MIT Press .\nGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley\nD, Ozair S, Courville A and Bengio Y (2014a) Generative\nadversarial nets.\nIn: Advances in Neural Information\nProcessing Systems. pp. 2672–2680.\nGoodfellow IJ, Shlens J and Szegedy C (2014b) Explaining and\nharnessing adversarial examples. arXiv preprint:1412.6572 .\nGrupen RA and Huber M (2005) A framework for the development\nof robot behavior. AAAI Spring Symposium on Developmental\nRobotics .\nGu S, Holly E, Lillicrap T and Levine S (2016a) Deep\nreinforcement learning for robotic manipulation.\narXiv\npreprint:1610.00633 .\nGu S, Lillicrap T, Sutskever I and Levine S (2016b) Continuous\ndeep q-learning with model-based acceleration.\narXiv\npreprint:1603.00748 .\nGullapalli V, Grupen RA and Barto AG (1992) Learning reactive\nadmittance control.\nIn: IEEE International Conference on\nRobotics and Automation. pp. 1475–1480.\nHariharan B, Arbel´aez P, Girshick R and Malik J (2015)\nHypercolumns for object segmentation and ﬁne-grained\nlocalization.\nIn: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. pp. 447–456.\nHarmon ME and Baird III LC (1996) Multi-player residual\nadvantage learning with general function approximation.\nWright Laboratory, WL/AACF, Wright-Patterson Air Force\nBase, OH : 45433–7308.\nHart S (2009a) The Development of Hierarchical Knowledge in\nRobot Systems.\nPhD Thesis, University of Massachusetts\nAmherst.\nHart S (2009b) An intrinsic reward for affordance exploration. In:\nIEEE International Conference on Development and Learning.\nHausknecht MJ and Stone P (2015) Deep reinforcement learning in\nparameterized action space. CoRR arXiv preprint:1511.04143.\nHebert M, Wong JM and Grupen RA (2015) Phase lag bounded\nvelocity planning for high performance path tracking.\nIn:\nIEEE-RAS 15th International Conference on Humanoid\nRobots. pp. 947–952.\nHeess N, Hunt JJ, Lillicrap TP and Silver D (2015a) Memory-\nbased control with recurrent neural networks.\narXiv\npreprint:1512.04455 .\nHeess N, Wayne G, Silver D, Lillicrap T, Erez T and Tassa Y\n(2015b) Learning continuous control policies by stochastic\nvalue gradients.\nIn: Advances in Neural Information\nProcessing Systems. pp. 2944–2952.\nHeess N, Wayne G, Tassa Y, Lillicrap T, Riedmiller M and\nSilver D (2016) Learning and transfer of modulated locomotor\ncontrollers. arXiv preprint:1610.05182 .\nHemion NJ (2016a) Context discovery for model learning in\npartially observable environments. arXiv preprint:1608.00737\n.\nHemion NJ (2016b) Discovering latent states for model learning:\nApplying sensorimotor contingencies theory and predictive\nprocessing to model context. arXiv preprint:1608.00359 .\nHester T and Stone P (2015) Intrinsically motivated model learning\nfor developing curious robots. Artiﬁcial Intelligence .\nHornik K, Stinchcombe M and White H (1989) Multilayer\nfeedforward networks are universal approximators.\nNeural\nnetworks 2(5): 359–366.\nHuber M and Grupen R (1997) Learning to coordinate controllers\n- reinforcement learning on a control basis. In: Proceedings\nof the Fifteenth International Joint Conference on Artiﬁcial\nIntelligence (IJCAI). Nagoya, JP: IJCAII.\nHuber M, MacDonald W and Grupen R (1996a) A control basis\nfor multilegged walking. In: Proceedings of the Conference on\nRobotics and Automation. Minneapolis, MN: IEEE.\nHuber M, MacDonald WS and Grupen RA (1996b) A control basis\nfor multilegged walking. In: IEEE International Conference on\nRobotics and Automation.\nHussein A, Gaber MM and Elyan E (2016) Deep active learning\nfor autonomous navigation. In: International Conference on\nEngineering Applications of Neural Networks. Springer, pp. 3–\n17.\n20\nIjspeert A, Nakanishi J and Schaal S (2003) Learning attractor\nlandscapes for learning motor primitives. In: Becker S, Thrun\nS and Obermayer K (eds.) Advances in Neural Information\nProcessing Systems 15. MIT Press, pp. 1547–1554.\nJaderberg M, Simonyan K, Zisserman A et al. (2015) Spatial\ntransformer networks.\nIn: Advances in Neural Information\nProcessing Systems. pp. 2017–2025.\nJain A, Singh A, Koppula HS, Soh S and Saxena A (2016)\nRecurrent neural networks for driver activity anticipation via\nsensory-fusion architecture. In: IEEE International Conference\non Robotics and Automation (ICRA). pp. 3118–3125.\nJain A, Zamir AR, Savarese S and Saxena A (2015) Structural-\nrnn: Deep learning on spatio-temporal graphs.\narXiv\npreprint:1511.05298 .\nJames S and Johns E (2016) 3d simulation for robot arm control\nwith deep q-learning. arXiv preprint:1609.03759 .\nJohns E, Leutenegger S and Davison AJ (2016) Deep learning a\ngrasp function for grasping under gripper pose uncertainty.\narXiv preprint:1608.02239 .\nJohnson M, Shrewsbury B, Bertrand S, Wu T, Duran D, Floyd M,\nAbeles P, Stephen D, Mertins N, Lesman A et al. (2015) Team\nihmc’s lessons learned from the darpa robotics challenge trials.\nJournal of Field Robotics 32(2): 192–208.\nKaelbling LP, Littman ML and Moore AW (1996) Reinforcement\nlearning: A survey. Journal of artiﬁcial intelligence research\n4: 237–285.\nKaelbling LP and Lozano-P´erez T (2011) Hierarchical task and\nmotion planning in the now. In: IEEE International Conference\non Robotics and Automation (ICRA). pp. 1470–1477.\nKaelbling LP and Lozano-P´erez T (2013) Integrated task and\nmotion planning in belief space.\nInternational Journal of\nRobotics Research 32(9-10).\nKardan N and Stanley KO (2016) Fitted learning: Models with\nawareness of their limits. arXiv preprint:1609.02226 .\nKendall A and Cipolla R (2015) Modelling uncertainty in deep\nlearning for camera relocalization. arXiv preprint:1509.05909\n.\nKendall A, Grimes M and Cipolla R (2015) Posenet: A convolu-\ntional network for real-time 6-dof camera relocalization. In:\nProceedings of the IEEE International Conference on Com-\nputer Vision. pp. 2938–2946.\nKhatib O (1985) Real-time obstacle avoidance for manipulators and\nmobile robots. In: ICRA. IEEE, pp. 500–505.\nKingma DP and Welling M (2013) Auto-encoding variational\nbayes. arXiv preprint:1312.6114 .\nKohlbrecher S, Romay A, Stumpf A, Gupta A, Von Stryk O, Bacim\nF, Bowman DA, Goins A, Balasubramanian R and Conner DC\n(2015) Human-robot teaming for rescue missions: Team vigir’s\napproach to the 2013 darpa robotics challenge trials. Journal\nof Field Robotics 32(3): 352–377.\nKonidaris G and Barto AG (2009) Efﬁcient skill learning using\nabstraction selection.\nIn: International Joint Conference on\nArtiﬁcial Intelligence, volume 9. pp. 1107–1112.\nKragic D and Christensen HI (2003) Robust visual servoing. The\ninternational journal of robotics research 22(10-11): 923–939.\nKrishnan RG, Shalit U and Sontag D (2015) Deep kalman ﬁlters.\narXiv preprint:1511.05121 .\nKrizhevsky A, Sutskever I and Hinton GE (2012) Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn:\nAdvances in Neural Information Processing Systems.\nKruger N, Geib C, Piater J, Petrick R, Steedman M, Worgotter F,\nUde A, Asfour T, Kraft D, Omrcen D, Agostini A and Dillmann\nR (2011) Objectaction complexes: Grounded abstractions of\nsensorymotor processes.\nRobotics and Autonomous Systems\n59(10): 740 – 757.\nKu L, Sen S, Learned-Miller E and Grupen R (2014) Aspect\ntransition graph: An affordance-based model.\nIn: European\nConference on Computer Vision, Workshop on Affordances:\nVisual Perception of Affordances and Functional Visual\nPrimitives for Scene Analysis.\nKu LY, Learned-Miller E and Grupen R (2016) Associating\ngrasping with convolutional neural network features.\narXiv\npreprint:1609.03947 .\nKuindersma S, Deits R, Fallon M, Valenzuela A, Dai H, Permenter\nF, Koolen T, Marion P and Tedrake R (2016) Optimization-\nbased locomotion planning, estimation, and control design for\nthe atlas humanoid robot. Autonomous Robots 40(3): 429–455.\nKuindersma S, Hannigan E, Ruiken D and Grupen R (2009)\nDexterous Mobility with the uBot-5 Mobile Manipulator. In:\n14th International Conference on Advanced Robotics (ICAR).\nMunich, Germany.\nKulkarni TD, Narasimhan KR, Saeedi A and Tenenbaum JB\n(2016) Hierarchical deep reinforcement learning: Integrating\ntemporal\nabstraction\nand\nintrinsic\nmotivation.\narXiv\npreprint:1604.06057 .\nKurakin A, Goodfellow I and Bengio S (2016) Adversarial\nexamples in the physical world. arXiv preprint:1607.02533 .\nLake BM, Ullman TD, Tenenbaum JB and Gershman SJ (2016)\nBuilding machines that learn and think like people.\narXiv\npreprint:1604.00289 .\nLaw J, Lee MH, Hulse M and Tomassetti A (2011) The infant\ndevelopment timeline and its application to robot shaping.\nAdaptive Behaviour 19(5): 335–358.\nLeCun Y, Bengio Y and Hinton G (2015) Deep learning. Nature\n521(7553): 436–444.\nLegg S and Hutter M (2007) Universal intelligence: A deﬁnition of\nmachine intelligence. Minds and Machines 17(4): 391–444.\nLenz I, Lee H and Saxena A (2015) Deep learning for detecting\nrobotic grasps. The International Journal of Robotics Research\n34(4-5): 705–724.\nLerer A, Gross S and Fergus R (2016) Learning physical intuition\nof block towers by example. arXiv preprint:1603.01312 .\nLevine S and Abbeel P (2014) Learning neural network policies\nwith guided policy search under unknown dynamics.\nIn:\nAdvances in Neural Information Processing Systems. pp. 1071–\n1079.\nLevine S, Finn C, Darrell T and Abbeel P (2015) End-\nto-end\ntraining\nof\ndeep\nvisuomotor\npolicies.\narXiv\npreprint:1504.00702 .\nLevine S, Pastor P, Krizhevsky A and Quillen D (2016) Learning\nhand-eye coordination for robotic grasping with deep learning\nand large-scale data collection. arXiv preprint:1603.02199 .\nLibertus K and Needham A (2010) Teach to reach: The effects\nof active vs. passive reaching experiences on action and\nperception. Vision research 50(24): 2750–2757.\nLillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y,\nSilver D and Wierstra D (2015) Continuous control with deep\nreinforcement learning. arXiv preprint:1509.02971 .\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n21\nLin LJ (1993) Reinforcement learning for robots using neural\nnetworks. Technical report, DTIC Document.\nLin\nLJ\nand\nMitchell\nTM\n(1992)\nMemory\napproaches\nto\nreinforcement learning in non-markovian domains. Technical\nreport, Department of Computer Science. Carnegie Mellon\nUniversity.\nLipton ZC, Gao J, Li L, Li X, Ahmed F and Deng L (2016) Efﬁcient\nexploration for dialog policy learning with deep bbq networks\n& replay buffer spiking. arXiv preprint:1608.05081 .\nLyapunov AM (1992) The general problem of the stability of\nmotion. International Journal of Control 55(3): 531–534.\nMadai-Tahy L, Otte S, Hanten R and Zell A (2016) Revisiting\ndeep convolutional neural networks for rgb-d based object\nrecognition. In: International Conference on Artiﬁcial Neural\nNetworks. pp. 29–37.\nMahendran A and Vedaldi A (2016) Visualizing deep convolutional\nneural networks using natural pre-images.\nInternational\nJournal of Computer Vision : 1–23.\nMaitin-Shepard J, Cusumano-Towner M, Lei J and Abbeel P (2010)\nCloth grasp point detection based on multiple-view geometric\ncues with application to robotic towel folding.\nIn: IEEE\nInternational Conference on Robotics and Automation.\nMalmir M, Sikka K, Forster D, Fasel I, Movellan JR and Cottrell\nGW (2015) Deep active object recognition by joint label and\naction prediction. arXiv preprint:1512.05484 .\nMalmir M, Sikka K, Forster D, Movellan J and Cottrell GW (2016)\nDeep q-learning for active recognition of germs: Baseline\nperformance on a standardized dataset for active learning. In:\nProceedings of the British Machine Vision Conference.\nMariet\nZ\nand\nSra\nS\n(2015)\nDiversity\nnetworks.\narXiv\npreprint:1511.05077 .\nMasson W and Konidaris G (2015) Reinforcement learning with\nparameterized actions. arXiv preprint:1509.01644 .\nMathieu M, Couprie C and LeCun Y (2015) Deep multi-\nscale video prediction beyond mean square error.\narXiv\npreprint:1511.05440 .\nMeeden L, McGraw G and Blank D (1993) Emergent control and\nplanning in an autonomous vehicle.\nIn: Proceedings of the\nﬁfteenth annual meeting of the cognitive science society. pp.\n735–740.\nMnih V, Agapiou J, Osindero S, Graves A, Vinyals O, Kavukcuoglu\nK et al. (2016) Strategic attentive writer for learning macro-\nactions. arXiv preprint:1606.04695 .\nMnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I,\nWierstra D and Riedmiller M (2013) Playing atari with deep\nreinforcement learning. In: NIPS Deep Learning Workshop.\nMnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare\nMG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G\net al. (2015) Human-level control through deep reinforcement\nlearning. Nature 518(7540): 529–533.\nMontgomery\nW,\nAjay\nA,\nFinn\nC,\nAbbeel\nP\nand\nLevine\nS (2016) Reset-free guided policy search: Efﬁcient deep\nreinforcement learning with stochastic initial states.\narXiv\npreprint:1610.01112 .\nMontgomery W and Levine S (2016) Guided policy search as\napproximate mirror descent. arXiv preprint:1607.04614 .\nMoore AW and Atkeson CG (1993) Prioritized sweeping:\nReinforcement learning with less data and less time. Machine\nLearning 13(1): 103–130.\nMottaghi R, Rastegari M, Gupta A and Farhadi A (2016) ” what\nhappens if...” learning to predict the effect of forces in images.\narXiv preprint:1603.05600 .\nMugan J and Kuipers B (2012) Autonomous learning of high-level\nstates and actions in continuous environments. Transactions on\nAutonomous Mental Development 4(1): 70–86.\nNakamura Y (1990) Advanced robotics: redundancy and optimiza-\ntion. Addison-Wesley Longman Publishing Co., Inc.\nNg A and Lin Y (2016) Self-driving cars wont work until we change\nour roads—and attitudes.\nNguyen A, Yosinski J and Clune J (2015) Deep neural networks are\neasily fooled: High conﬁdence predictions for unrecognizable\nimages. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR).\nNicolai A, Skeele R, Eriksen C and Hollinger GA (2016) Deep\nlearning for laser based odometry estimation .\nNoda K, Arie H, Suga Y and Ogata T (2013) Multimodal\nintegration learning of object manipulation behaviors using\ndeep neural networks. In: IEEE/RSJ International Conference\non Intelligent Robots and Systems. pp. 1728–1733.\nNoh H, Hong S and Han B (2015) Learning deconvolution network\nfor semantic segmentation.\nIn: Proceedings of the IEEE\nInternational Conference on Computer Vision. pp. 1520–1528.\nOh J, Guo X, Lee H, Lewis RL and Singh S (2015) Action-\nconditional video prediction using deep networks in atari\ngames.\nIn: Advances in Neural Information Processing\nSystems. pp. 2863–2871.\nOndruska P, Dequaire J, Wang DZ and Posner I (2016) End-to-\nend tracking and semantic segmentation using recurrent neural\nnetworks .\nOndruska\nP\nand\nPosner\nI\n(2016)\nDeep\ntracking:\nSeeing\nbeyond seeing using recurrent neural networks.\narXiv\npreprint:1602.00991 .\nOsband I, Blundell C, Pritzel A and Van Roy B (2016) Deep\nexploration via bootstrapped dqn. arXiv preprint:1602.04621 .\nOudeyer PY, Baranes A and Kaplan F (2013) Intrinsically motivated\nlearning of real-world sensorimotor skills with developmental\nconstraints. In: Baldassarre G and Mirolli M (eds.) Intrinsically\nMotivated Learning in Natural and Artiﬁcial Systems. Springer.\nOudeyer PY, Kaplan F and Hafner V (2007) Intrinsic motivation\nsystems\nfor\nautonomous\nmental\ndevelopment.\nIEEE\nTransactions on Evolutionary Computation 11(2): 265–286.\nPeng J and Williams RJ (1993) Efﬁcient learning and planning\nwithin the dyna framework. Adaptive Behavior 1(4): 437–454.\nPeters J, M¨ulling K and Altun Y (2010) Relative entropy policy\nsearch.\nIn: Association for the Advancement in Artiﬁcial\nIntelligence.\nPiaget J (1953) The origin of intelligence in the child.\nPiaget J (1954) The construction of reality in the child. Routledge.\nPiaget J and Cook M (1952) The origins of intelligence in children,\nvolume 8. International Universities Press New York.\nPinheiro PO, Collobert R and Dollar P (2015) Learning to segment\nobject candidates.\nIn: Advances in Neural Information\nProcessing Systems. pp. 1990–1998.\nPinheiro PO, Lin TY, Collobert R and Doll´ar P (2016) Learning to\nreﬁne object segments. arXiv preprint:1603.08695 .\nPinto L, Davidson J and Gupta A (2016) Supervision via\ncompetition: Robot adversaries for learning tasks.\narXiv\npreprint:1610.01685 .\n22\nPinto L and Gupta A (2015) Supersizing self-supervision: Learning\nto grasp from 50k tries and 700 robot hours.\narXiv\npreprint:1509.06825 .\nPinto L and Gupta A (2016) Learning to push by grasp-\ning: Using multiple tasks for effective learning.\narXiv\npreprint:1609.09025 .\nPlatt R (2006) Learning and Generalizing Control-Based Grasping\nand Manipulation Skills.\nPhD Thesis, University of\nMassachusetts Amherst.\nPlatt R, Fagg A and Grupen R (2005) Reusing schematic grasping\npolicies.\nIn: Proceedings of the IEEE-RAS International\nConference on Humanoid Robots. Tsukuba, Japan.\nPlatt R, Fagg A and Grupen R (2010) Null-space grasp control:\nTheory and experiments.\nRobotics, IEEE Transactions on\n26(2): 282 –295. DOI:10.1109/TRO.2010.2042754.\nPlatt R, Fagg AH and Grupen R (2002) Nullspace composition of\ncontrol laws for grasping.\nIn: International Conference on\nIntelligent Robots and Systems (IROS). Laussane, Switzerland.\nPlatt R, Fagg AH and Grupen R (2004) Manipulation gaits:\nSequences of grasp control tasks. In: Proceedings of the 2004\nIEEE Conference on Robotics and Automation. New Orleans,\nLA.\nPlatt\nR,\nGrupen\nR\nand\nFagg\nA\n(2006)\nImproving\ngrasp\nskills using schema structured learning.\nIn: Proceedings\nof International Conference on Developmental Learning.\nBloomington, Indiana.\nPurves D, Augustine GJ, Fitzpatrick D, Katz LC, LaMantia\nAS, McNamara JO and Williams SM (2001) The Spinal\nCord Circuitry Underlying Muscle Stretch Reﬂexes. Sinauer\nAssociates.\nRadford A, Metz L and Chintala S (2015) Unsupervised\nrepresentation learning with deep convolutional generative\nadversarial networks. arXiv preprint:1511.06434 .\nReed E (1996) Encountering the World. New York, NY: Oxford\nUniversity Press.\nRicard M and Kolitz S (2003) The adept framework for intelligent\nautonomy. Technical report, DTIC Document.\nRohanimanesh K, Platt R, Mahadevan S and Grupen R (2004)\nCoarticulation in markov decision processes. In: Advances in\nNeural Information Processing Systems. pp. 1137–1144.\nRuiken D, Lanighan MW and Grupen RA (2013) Postural modes\nand control for dexterous mobile manipulation: the umass ubot\nconcept. In: IEEE-RAS International Conference on Humanoid\nRobots.\nRuiken D, Liu TQ, Takahashi T and Grupen RA (2016a)\nReconﬁgurable tasks in belief-space planning. In: Integrating\nMultiple Knowledge Representation and Reasoning Techniques\nin Robotics Workshop at IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS).\nRuiken D, Wong JM, Liu TQ, Hebert M, Takahashi T, Lanighan\nMW and Grupen RA (2016b) Affordance-based active belief\nrecognition using visual and manual actions. In: IEEE/RSJ the\nInternational Conference on Intelligent Robots and Systems.\nRumelhart DE (1998) The architecture of mind: A connectionist\napproach. Mind readings : 207–238.\nRusu AA, Rabinowitz NC, Desjardins G, Soyer H, Kirkpatrick J,\nKavukcuoglu K, Pascanu R and Hadsell R (2016a) Progressive\nneural networks. arXiv preprint:1606.04671 .\nRusu AA, Vecerik M, Roth¨orl T, Heess N, Pascanu R and\nHadsell R (2016b) Sim-to-real robot learning from pixels with\nprogressive nets. arXiv preprint:1610.04286 .\nSantana E and Hotz G (2016) Learning a driving simulator. arXiv\npreprint:1608.01230 .\nSchaul T, Quan J, Antonoglou I and Silver D (2015) Prioritized\nexperience replay. arXiv preprint:1511.05952 .\nSchmidhuber JH (1991) Reinforcement learning in markovian\nand non-markovian environments.\nIn: Advances in Neural\nInformation Processing Systems. pp. 500–506.\nSchulman J, Levine S, Moritz P, Jordan MI and Abbeel P (2015)\nTrust region policy optimization. arXiv preprint:1502.05477 .\nSchwarz M, Schulz H and Behnke S (2015) Rgb-d object recog-\nnition and pose estimation based on pre-trained convolutional\nneural network features. In: IEEE International Conference on\nRobotics and Automation.\nSen S and Grupen R (2014) Integrating task level planning\nwith stochastic control. Technical Report UM-CS-2014-005,\nUniversity of Massachusetts Amherst.\nSequeira P, Melo FS and Paiva A (2014) Learning by appraising: an\nemotion-based approach to intrinsic reward design. Adaptive\nBehavior .\nSermanet P, Kavukcuoglu K, Chintala S and LeCun Y (2013)\nPedestrian detection with unsupervised multi-stage feature\nlearning. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. pp. 3626–3633.\nSigaud O and Droniou A (2016) Towards deep developmental\nlearning. IEEE Transactions on Cognitive and Developmental\nSystems 8(2): 99–114.\nSilver D, Huang A, Maddison CJ, Guez A, Sifre L, van den\nDriessche G, Schrittwieser J, Antonoglou I, Panneershelvam\nV, Lanctot M et al. (2016) Mastering the game of go with deep\nneural networks and tree search. Nature 529(7587): 484–489.\nSilver D, Lever G, Heess N, Degris T, Wierstra D and Riedmiller\nMA (2014) Deterministic policy gradient algorithms.\nIn:\nProceedings of the 31th International Conference on Machine\nLearning. pp. 387–395.\nSilver DL, Yang Q and Li L (2013) Lifelong machine learning\nsystems: Beyond learning algorithms.\nIn: AAAI Spring\nSymposium: Lifelong Machine Learning. pp. 49–55.\nSocher R, Lin CC, Manning C and Ng AY (2011) Parsing natural\nscenes and natural language with recursive neural networks. In:\nProceedings of the 28th international conference on machine\nlearning (ICML-11). pp. 129–136.\nSong S and Xiao J (2015) Deep sliding shapes for amodal 3d object\ndetection in rgb-d images. arXiv preprint:1511.02300 .\nStolle M and Precup D (2002) Learning options in reinforcement\nlearning.\nIn: International Symposium on Abstraction,\nReformulation, and Approximation. Springer, pp. 212–223.\nSutton R and Barto A (1998) Reinforcement Learning: An\nIntroduction. Cambridge, MA: MIT Press.\nSutton R, Precup D and Singh S (1999) Between mdps and semi-\nmdps: A framework for temporal abstraction in reinforcement\nlearning. Artiﬁcial Intelligence 112: 181–211.\nSutton RS (1990) Integrated architectures for learning, planning,\nand reacting based on approximating dynamic programming.\nIn: Proceedings of the seventh international conference on\nmachine learning. pp. 216–224.\nJay M. Wong — Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics\n23\nSutton RS (1991) Planning by incremental dynamic programming.\nIn: Proceedings of the Eighth International Workshop on\nMachine Learning. pp. 353–357.\nSzegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan\nD, Vanhoucke V and Rabinovich A (2015) Going deeper with\nconvolutions.\nIn: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. pp. 1–9.\nSzegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow\nI and Fergus R (2013) Intriguing properties of neural networks.\narXiv preprint:1312.6199 .\nTai L and Liu M (2016) Towards cognitive exploration through\ndeep reinforcement learning for mobile robots.\narXiv\npreprint:1610.01733 .\nTakahashi T, Wong JM and Grupen RA (2017) Self-supervised deep\nsensorimotor learning using closed loop motion primitives. In:\nSubmitted to IEEE International Conference on Robotics and\nAutomation.\nTamar A, Levine S and Abbeel P (2016) Value iteration networks.\narXiv preprint:1602.02867 .\nTanay T and Grifﬁn L (2016) A boundary tilting persepective\non\nthe\nphenomenon\nof\nadversarial\nexamples.\narXiv\npreprint:1608.07690 .\nTesauro G (1992) Practical issues in temporal difference learning.\nIn: Reinforcement Learning. pp. 33–53.\nTesauro G (1995a) Td-gammon: A self-teaching backgammon\nprogram. In: Applications of Neural Networks. Springer, pp.\n267–285.\nTesauro G (1995b) Temporal difference learning and td-gammon.\nCommunications of the ACM 38(3): 58–68.\nThelen E and Smith L (1996) A Dynamic Systems Approach to the\nDevelopment of Cognition and Action. A Bradford book. MIT\nPress. ISBN 9780262700597.\nThomas PS (2015) Safe reinforcement learning. Technical report,\nUniveristy of Massachusetts Amherst.\nThomas PS, Theocharous G and Ghavamzadeh M (2015) High-\nconﬁdence off-policy evaluation.\nIn: Association for the\nAdvancement of Artiﬁcial Intelligence. pp. 3000–3006.\nToshev A and Szegedy C (2014) Deeppose: Human pose estimation\nvia deep neural networks.\nIn: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. pp.\n1653–1660.\nTurvey M (1992) Affordances and prospective control: An outline\nof the ontology. Ecological Psychology 4: 173–187.\nTzeng E, Devin C, Hoffman J, Finn C, Peng X, Levine S, Saenko\nK and Darrell T (2015) Towards adapting deep visuomotor\nrepresentations from simulated to real environments.\narXiv\npreprint:1511.07111 .\nUtgoff PE and Stracuzzi DJ (2002) Many-layered learning. Neural\nComputation 14(10): 2497–2529.\nVan Hasselt H, Guez A and Silver D (2015) Deep reinforcement\nlearning with double q-learning. arXiv preprint:1509.06461 .\nvan Hoof H, Chen N, Karl M, van der Smagt P and Peters J (2016)\nStable reinforcement learning with autoencoders for tactile and\nvisual data. International Conference on Intelligent Robots and\nSystems.\nVarley J, DeChant C, Richardson A, Nair A, Ruales J and Allen\nP (2016) Shape completion enabled robotic grasping. arXiv\npreprint:1609.08546 .\nWang Q, Guo W, Ororbia I, Alexander G, Xing X, Lin L, Giles\nCL, Liu X, Liu P and Xiong G (2016a) Using non-invertible\ndata transformations to build adversary-resistant deep neural\nnetworks. arXiv preprint:1610.01934 .\nWang Y and Chirikjian GS (2000) A new potential ﬁeld method for\nrobot path planning 2: 977–982.\nWang Z, de Freitas N and Lanctot M (2015) Dueling network\narchitectures\nfor\ndeep\nreinforcement\nlearning.\narXiv\npreprint:1511.06581 .\nWang Z, Li Z, Wang B and Liu H (2016b) Robot grasp detec-\ntion using multimodal deep convolutional neural networks.\nAdvances in Mechanical Engineering 8(9).\nWawrzy´nski P and Tanwani AK (2013) Autonomous reinforcement\nlearning with experience replay. Neural Networks 41: 156–167.\nWilkinson E and Takahashi T (2015) Efﬁcient aspect object models\nusing pre-trained convolutional neural networks. In: IEEE-RAS\n15th International Conference on Humanoid Robots. pp. 284–\n289.\nWong\nJM\nand\nGrupen\nRA\n(2016)\nIntrinsically\nmotivated\nmultimodal structure learning.\nIn: IEEE International\nConference on Development and Learning and on Epigenetic\nRobotics.\nWong JM, Takahashi T and Grupen RA (2016) Self-supervised\ndeep visuomotor learning from motor unit feedback. In: Are\nthe Skeptics Right? Limits and Potentials of Deep Learning\nin Robotics Workshop at Robotics: Science and Systems\nConference (RSS-WS).\nWorgotter F, Geib C, Tamosiunaite M, Aksoy EE, Piater J,\nXiong H, Ude A, Nemec B, Kraft D, Kruger N, Wchter\nM and Asfour T (2015) Structural bootstrapping - a novel,\ngenerative mechanism for faster and more efﬁcient acquisition\nof action-knowledge.\nIEEE Transactions on Autonomous\nMental Development 7(2): 140–154.\nWray KH, Ruiken D, Grupen RA and Zilberstein S (2016) Log-\nspace harmonic function path planning.\nIn: Twenty-Ninth\nInternational Conference on Intelligent Robots and Systems.\nYi SJ, McGill SG, Vadakedathu L, He Q, Ha I, Han J, Song H,\nRouleau M, Zhang BT, Hong D et al. (2015) Team thor’s entry\nin the darpa robotics challenge trials 2013. Journal of Field\nRobotics 32(3): 315–335.\nZelazo PR (1983) The development of walking: new ﬁndings and\nold assumptions. Journal of Motor Behavior 15(2): 99–137.\nZhai J, Liu Q, Zhang Z, Zhong S, Zhu H, Zhang P and Sun C (2016)\nDeep q-learning with prioritized sampling. In: International\nConference on Neural Information Processing. Springer, pp.\n13–22.\nZhang F, Leitner J, Milford M, Upcroft B and Corke P (2015)\nTowards vision-based deep reinforcement learning for robotic\nmotion control. arXiv preprint:1511.03791 .\nZheng S, Jayasumana S, Romera-Paredes B, Vineet V, Su Z, Du\nD, Huang C and Torr PH (2015) Conditional random ﬁelds\nas recurrent neural networks.\nIn: Proceedings of the IEEE\nInternational Conference on Computer Vision. pp. 1529–1537.\n",
  "categories": [
    "cs.RO",
    "cs.AI"
  ],
  "published": "2016-11-01",
  "updated": "2016-11-01"
}