{
  "id": "http://arxiv.org/abs/1611.00336v2",
  "title": "Stochastic Variational Deep Kernel Learning",
  "authors": [
    "Andrew Gordon Wilson",
    "Zhiting Hu",
    "Ruslan Salakhutdinov",
    "Eric P. Xing"
  ],
  "abstract": "Deep kernel learning combines the non-parametric flexibility of kernel\nmethods with the inductive biases of deep learning architectures. We propose a\nnovel deep kernel learning model and stochastic variational inference procedure\nwhich generalizes deep kernel learning approaches to enable classification,\nmulti-task learning, additive covariance structures, and stochastic gradient\ntraining. Specifically, we apply additive base kernels to subsets of output\nfeatures from deep neural architectures, and jointly learn the parameters of\nthe base kernels and deep network through a Gaussian process marginal\nlikelihood objective. Within this framework, we derive an efficient form of\nstochastic variational inference which leverages local kernel interpolation,\ninducing points, and structure exploiting algebra. We show improved performance\nover stand alone deep networks, SVMs, and state of the art scalable Gaussian\nprocesses on several classification benchmarks, including an airline delay\ndataset containing 6 million training points, CIFAR, and ImageNet.",
  "text": "Stochastic Variational Deep Kernel Learning\nAndrew Gordon Wilson*\nCornell University\nZhiting Hu*\nCMU\nRuslan Salakhutdinov\nCMU\nEric P. Xing\nCMU\nAbstract\nDeep kernel learning combines the non-parametric ﬂexibility of kernel methods\nwith the inductive biases of deep learning architectures. We propose a novel deep\nkernel learning model and stochastic variational inference procedure which gener-\nalizes deep kernel learning approaches to enable classiﬁcation, multi-task learning,\nadditive covariance structures, and stochastic gradient training. Speciﬁcally, we\napply additive base kernels to subsets of output features from deep neural archi-\ntectures, and jointly learn the parameters of the base kernels and deep network\nthrough a Gaussian process marginal likelihood objective. Within this framework,\nwe derive an efﬁcient form of stochastic variational inference which leverages local\nkernel interpolation, inducing points, and structure exploiting algebra. We show\nimproved performance over stand alone deep networks, SVMs, and state of the\nart scalable Gaussian processes on several classiﬁcation benchmarks, including an\nairline delay dataset containing 6 million training points, CIFAR, and ImageNet.\n1\nIntroduction\nLarge datasets provide great opportunities to learn rich statistical representations, for accurate\npredictions and new scientiﬁc insights into our modeling problems. Gaussian processes are promising\nfor large data problems, because they can grow their information capacity with the amount of available\ndata, in combination with automatically calibrated model complexity [22, 26].\nFrom a Gaussian process perspective, all of the statistical structure in data is learned through a kernel\nfunction. Popular kernel functions, such as the RBF kernel, provide smoothing and interpolation,\nbut cannot learn representations necessary for long range extrapolation [23, 26]. With smoothing\nkernels, we can only use the information in a large dataset to learn about noise and length-scale\nhyperparameters, which tell us only how quickly correlations in our data vary with distance in the\ninput space. If we learn a short length-scale hyperparameter, then by deﬁnition we will only make\nuse of a small amount of training data near each testing point. If we learn a long length-scale, then\nwe could subsample the data and make similar predictions.\nTherefore to fully use the information in large datasets, we must build kernels with great repre-\nsentational power and useful learning biases, and scale these approaches without sacriﬁcing this\nrepresentational ability. Indeed many recent approaches have advocated building expressive kernel\nfunctions [e.g., 23, 10, 27, 26, 18, 32], and emerging research in this direction takes inspiration\nfrom deep learning models [e.g., 29, 6, 3]. However, the scalability, general applicability, and in-\nterpretability of such approaches remain a challenge. Recently, Wilson et al. [31] proposed simple\nand scalable deep kernels for single-output regression problems, with promising performance on\nmany experiments. But their approach does not allow for stochastic training, multiple outputs, deep\narchitectures with many output features, or classiﬁcation. And it is on classiﬁcation problems, in\nparticular, where we typically have high dimensional input vectors, with little intuition about how\nthese vectors should correlate, and therefore most want to learn a ﬂexible non-Euclidean similarity\nmetric [1].\n*Equal contribution. 29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona,\nSpain.\narXiv:1611.00336v2  [stat.ML]  2 Nov 2016\nIn this paper, we introduce inference procedures and propose a new deep kernel learning model\nwhich enables (1) classiﬁcation and non-Gaussian likelihoods; (2) multi-task learning1; (3) stochastic\ngradient mini-batch training; (4) deep architectures with many output features; (5) additive covariance\nstructures; and (5) greatly enhanced scalability.\nWe propose to use additive base kernels corresponding to Gaussian processes (GPs) applied to subsets\nof output features of a deep neural architecture. We then linearly mix these Gaussian processes,\ninducing correlations across multiple output variables. The result is a deep probabilistic neural\nnetwork, with a hidden layer composed of additive sets of inﬁnite basis functions, linearly mixed\nto produce correlated output variables. All parameters of the deep architecture and base kernels\nare jointly learned through a marginal likelihood objective, having integrated away all GPs. For\nscalability and non-Gaussian likelihoods, we derive stochastic variational inference (SVI) which\nleverages local kernel interpolation, inducing points, and structure exploiting algebra, and a hybrid\nsampling scheme, building on Wilson and Nickisch [28], Wilson et al. [30], Titsias [25], Hensman\net al. [11], and Nickson et al. [19]. The resulting approach, SV-DKL, has a complexity of O(m1+1/D)\nfor m inducing points and D input dimensions, versus the standard O(m3) for efﬁcient stochastic\nvariational methods.\nWe achieve good predictive accuracy and scalability over a wide range of classiﬁcation tasks,\nwhile retaining a straightforward, general purpose, and highly practical probabilistic non-parametric\nrepresentation, with code available at https://people.orie.cornell.edu/andrew/code.\n2\nBackground\nThroughout this paper, we assume we have access to vectorial input-output pairs D = {xi, yi},\nwhere each yi is related to xi through a Gaussian process and observation model. For example,\nin regression, one could model y(x)|f(x) ∼N(y(x); f(x), σ2I), where f(x) is a latent vector of\nindependent Gaussian processes f j ∼GP(0, kj), and σ2I is a noise covariance matrix.\nThe computational bottleneck in working with Gaussian processes typically involves computing\n(KX,X + σ2I)−1y and log |KX,X| over an n × n covariance matrix KX,X evaluated at n training\ninputs X. Standard procedure is to compute the Cholesky decomposition of KX,X, which incurs\nO(n3) computations and O(n2) storage, after which predictions cost O(n2) per test point. Gaussian\nprocesses are thus typically limited to at most a few thousand training points. Many promising\napproaches to scalability have been explored, for example, involving randomized methods [21, 17, 32]\n, and low rank approximations [24, 20]. Wilson and Nickisch [28] recently introduced the KISS-GP\napproximate kernel matrix eKX,X′ = MXKZ,ZM ⊤\nX′, which admits fast computations, given the\nexact kernel matrix KZ,Z evaluated on a latent multidimensional lattice of m inducing inputs Z, and\nMX, a sparse interpolation matrix. Without requiring any grid structure in X, KZ,Z decomposes\ninto a Kronecker product of Toeplitz matrices, which can be approximated by circulant matrices [30].\nExploiting such structure in combination with local kernel interpolation enables one to use many\ninducing points, resulting in near-exact accuracy in the kernel approximation, and O(n) inference.\nUnfortunately, this approach does not typically apply to D > 5 dimensional inputs [30].\nMoreover, the Gaussian process marginal likelihood does not factorize, and thus stochastic gradient\ndescent does not ordinarily apply. To address this issue, Hensman et al. [11] extended the variational\napproach from Titsias [25] and derived a stochastic variational GP posterior over inducing points\nfor a regression model which does have the required factorization for stochastic gradient descent.\nHensman et al. [13], Hensman et al. [12], and Dezfouli and Bonilla [7] further combine this with\na sampling procedure for estimating non-conjugate expectations. These methods have O(m3)\nsampling complexity which becomes prohibitive where many inducing points are desired for accurate\napproximation. Nickson et al. [19] consider Kronecker structure in the stochastic approximation of\nHensman et al. [11] for regression, but do not leverage local kernel interpolation or sampling.\n1We follow the GP convention where multi-task learning involves a function mapping a single input to\nmultiple correlated output responses (class probabilities, regression responses, etc.). Unlike NNs which naturally\nhave correlated outputs by sharing hidden basis functions (and multi-task can have a more specialized meaning),\nmost GP models perform multiple binary classiﬁcation, ignoring correlations between output classes. Even\napplying a GP to NN features for deep kernel learning does not naturally produce multiple correlated outputs.\n2\nx1\nxD\nInput layer\nh(1)\n1\nh(1)\nA\n. . .\n. . .\nh(2)\n1\nh(2)\nB\nh(L)\n1\nh(L)\nQ\nW (1)\nW (2)\nW (L)\nHidden layers\nAdditive GP layer\ny1\nyC\nOutput layer\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nf1\nfJ\nA\nFigure 1: Deep Kernel Learning for Multidimensional Outputs. Multidimensional inputs x ∈RD are mapped\nthrough a deep architecture, and then a series of additive Gaussian processes f1, . . . , fJ, with base kernels\nk1, . . . , kJ, are each applied to subsets of the network features h(L)\n1\n, . . . , h(L)\nQ . The thick lines indicate a\nprobabilistic mapping. The additive Gaussian processes are then linearly mixed by the matrix A and mapped to\noutput variables y1, . . . , yC (which are then correlated through A). All of the parameters of the deep network,\nbase kernel, and mixing layer, γ = {w, θ, A} are learned jointly through the (variational) marginal likelihood of\nour model, having integrated away all of the Gaussian processes. We can view the resulting model as a Gaussian\nprocess which uses an additive series of deep kernels with weight sharing.\nTo address these limitations, we introduce a new deep kernel learning model for multi-task classiﬁca-\ntion, mini-batch training, and scalable kernel interpolation which does not require low dimensional\ninput spaces. In this paper, we view scalability and ﬂexibility as two sides of one coin: we most want\nthe ﬂexible models on the largest datasets, which contain the necessary information to discover rich\nstatistical structure. We show that the resulting approach can learn very expressive and interpretable\nkernel functions on large classiﬁcation datasets, containing millions of training points.\n3\nDeep Kernel Learning for Multi-task Classiﬁcation\nWe propose a new deep kernel learning approach to account for classiﬁcation and non-Gaussian\nlikelihoods, multiple correlated outputs, additive covariances, and stochastic gradient training.\nWe propose to build a probabilistic deep network as follows: 1) a deep non-linear transformation\nh(x, w), parametrized by weights w, is applied to the observed input variable x, to produce Q\nfeatures at the ﬁnal layer L, h(L)\n1\n, . . . , h(L)\nQ ; 2) J Gaussian processes, with base kernels k1, . . . , kJ,\nare applied to subsets of these features, corresponding to an additive GP model [e.g., 8]. The base\nkernels can thus act on relatively low dimensional inputs, where local kernel interpolation and learning\nbiases such as similarities based on Euclidean distance are most natural; 3) these GPs are linearly\nmixed by a matrix A ∈RC×J, and transformed by an observation model, to produce the output\nvariables y1, . . . , yC. The mixing of these variables through A produces correlated multiple outputs,\na multi-task property which is uncommon in Gaussian processes or SVMs. The structure of this\nnetwork is illustrated in Figure 1. Critically, all of the parameters in the model (including base kernel\nhyperparameters) are trained through optimizing a marginal likelihood, having integrated away the\nGaussian processes, through the variational inference procedures described in section 4.\nFor classiﬁcation, we consider a special case of this architecture. Let C be the number of classes, and\nwe have data {xi, yi}n\ni=1, where yi ∈{0, 1}C is a one-shot encoding of the class label. We use the\nsoftmax observation model:\np(yi|f i, A) =\nexp(A(f i)⊤yi)\nP\nc exp(A(f i)⊤ec),\n(1)\nwhere f i ∈RJ is a vector of independent Gaussian processes followed by a linear mixing layer\nA(f i) = Af i; and ec is the indicator vector with the cth element being 1 and the rest 0.\nFor the jth Gaussian process in the additive GP layer, let f j = {fij}n\ni=1 be the latent functions on\nthe input data features. By introducing a set of latent inducing variables uj indexed by m inducing\n3\ninputs Z, we can write [e.g., 20]\np(f j|uj) = N(f j|K(j)\nX,ZK(j),−1\nZ,Z\nuj, eK(j)) ,\neK = KX,X −KX,ZK−1\nZ,ZKZ,X .\n(2)\nSubstituting the local interpolation approximation KX,X′ = MKZ,ZM ⊤of Wilson and Nickisch\n[28] into Eq. (5), we ﬁnd eK(j) = 0; it therefore follows that f j = KX,ZK−1\nZ,Zu = Mu. In section 4\nwe exploit this deterministic relationship between f and u, governed by the sparse matrix M, to\nderive a particularly efﬁcient stochastic variational inference procedure.\nEq. (1) and Eq. (5) together form the additive GP layer and the linear mixing layer of the proposed\ndeep probabilistic network in Figure 1, with all parameters (including network weights) trained jointly\nthrough the Gaussian process marginal likelihood.\n4\nStructure Exploiting Stochastic Variational Inference\nExact inference and learning in Gaussian processes with a non-Gaussian likelihood is not analytically\ntractable. Variational inference is an appealing approximate technique due to its automatic regulariza-\ntion to avoid overﬁtting, and its ability to be used with stochastic gradient training, by providing a\nfactorized approximation to the Gaussian process marginal likelihood. We develop our stochastic\nvariational method equipped with a fast sampling scheme for tackling any intractable marginalization.\nLet u = {uj}J\nj=1 be the collection of the inducing variables of the J additive GPs. We assume a\nvariational posterior over the inducing variables q(u). By Jensen’s inequality we have\nlog p(y) ≥Eq(u)p(f|u)[log p(y|f)] −KL[q(u)∥p(u)] ≜L(q),\n(3)\nwhere we have omitted the mixing weights A for clarity. The KL divergence term can be interpreted\nas a regularizer encouraging the approximate posterior q(u) to be close to the prior p(u). We aim\nat tightening the marginal likelihood lower bound L(q) which is equivalent to minimizing the KL\ndivergence from q to the true posterior.\nSince the likelihood function typically factorizes over data instances: p(y|f) = Qn\ni=1 p(yi|f i),\nwe can optimize the lower bound with stochastic gradients. In particular, we specify q(u) =\nQ\nj N(uj|µj, Sj) for the independent GPs, and iteratively update the variational parameters\n{µj, Sj}J\nj=1 and the kernel and deep network parameters using a noisy approximation of the gradient\nof the lower bound on minibatches of the full data. Henceforth we omit the index j for clarity.\nUnfortunately, for general non-Gaussian likelihoods the expectation in Eq (7) is usually intractable.\nWe develop a sampling method for tackling this intractability which is highly efﬁcient with structured\nreparameterization, local kernel interpolation, and structure exploiting algebra.\nUsing local kernel interpolation, the latent function f is expressed as a deterministic local interpolation\nof the inducing variables u (section 3). This result allows us to work around any difﬁcult approximate\nposteriors on f which typically occur in variational approaches for GPs. Instead, our sampler only\nneeds to account for the uncertainty on u. The direct parameterization of q(u) yields a straightforward\nand efﬁcient sampling procedure. The latent function samples (indexed by t) are then computed\ndirectly through interpolation f (t) = Mu(t).\nAs opposed to conventional mean-ﬁeld methods, which assume a diagonal variational covariance\nmatrix, we use the Cholesky decomposition for reparameterizing u in order to preserve structures\nwithin the covariance. Speciﬁcally, we let S = LT L, resulting in the following sampling procedure:\nu(t) = µ + Lϵ(t);\nϵ(t) ∼N(0, I).\nEach step of the above standard sampler has complexity of O(m2), where m is the number of\ninducing points. Due to the matrix vector product, this sampling procedure becomes prohibitive\nin the presence of many inducing points, which are required for accuracy on large datasets with\nmultidimensional inputs – particularly if we have an expressive kernel function [28].\nWe scale up the sampler by leveraging the fact that the inducing points are placed on a grid (taking\nadvantage of both Toeplitz and circulant structure), and additionally imposing a Kronecker decompo-\nsition on L = ND\nd=1 Ld, where D is the input dimension of the base kernel. With the fast Kronecker\n4\nmatrix-vector products, we reduce the above sampling cost of O(m2) to O(m1+1/D). Our approach\nthus greatly improves over previous stochastic variational methods which typically scale with O(m3)\ncomplexity, as discussed shortly.\nNote that the KL divergence term between the two Gaussians in Eq (7) has a closed form without the\nneed for Monte Carlo estimation. Computing the KL term and its derivatives, with the Kronecker\nmethod, is O(Dm\n3\nD ). With T samples of u and a minibatch of data points of size B, we can estimate\nthe marginal likelihood lower bound as\nL ≃N\nTB\nT\nX\nt=1\nB\nX\ni=1\nlog p(yi|f (t)\ni ) −KL[q(u)∥p(u)],\n(4)\nand the derivatives ∇L w.r.t the model hyperparameters γ and the variational parameters\n{µ, {Ld}D\nd=1} can be taken similarly. We provide the detailed derivation in the supplement.\nAlthough a small body of pioneering work has developed stochastic variational methods for Gaussian\nprocesses, our approach distinctly provides the above representation-preserving variational approx-\nimation, and exploits algebraic structure for signiﬁcant advantages in scalability and accuracy. In\nparticular, a similar variational lower bound as in Eq (7) was proposed in [25, 11] for a sparse GP,\nwhich were extended to non-conjugate likelihoods, with the intractable integrals estimated using\nGaussian quadrature as in the KLSP-GP [12] or univariate Gaussian samples as in the SAVI-GP [7].\nHensman et al. [13] estimates nonconjugate expectations with a hybrid Monte Carlo sampler (denoted\nas MC-GP). The computations in these approaches can be costly, with O(m3) complexity, due to\na complicated variational posterior over f as well as the expensive operations on the full inducing\npoint matrix. In addition to its increased efﬁciency, our sampling scheme is much simpler, without\nintroducing any additional tuning parameters. We empirically compare with these methods and show\nthe practical signiﬁcance of our algorithm in section 5.\nVariational methods have also been used in GP regression for stochastic inference (e.g., [19, 11]),\nand some of the most recent work in this area applied variational auto-encoders [15] for coupled\nvariational updates (aka back constraints) [5, 2]. We note that these techniques are orthogonal and\ncomplementary to our inference approach, and can be leveraged for further enhancements.\n5\nExperiments\nWe evaluate our proposed approach, stochastic variational deep kernel learning (SV-DKL), on a\nwide range of classiﬁcation problems, including an airline delay task with over 5.9 million data\npoints (section 5.1), a large and diverse collection of classiﬁcation problems from the UCI repository\n(section 5.2), and image classiﬁcation benchmarks (section 5.3). Empirical results demonstrate the\npractical signiﬁcance of our approach, which provides consistent improvements over stand-alone\nDNNs, while preserving a GP representation, and dramatic improvements in speed and accuracy over\nmodern state of the art GP models. We use classiﬁcation accuracy when comparing to DNNs, because\nit is a standard for evaluating classiﬁcation benchmarks with DNNs. However, we also compute the\nnegative log probability (NLP) values (supplement), which show similar trends.\nAll experiments were performed on a Linux machine with eight 4.0GHz CPU cores, one Tesla K40c\nGPU, and 32GB RAM. We implemented deep neural networks with Caffe [14].\nModel Training\nFor our deep kernel learning model, we used deep neural networks which produce\nC-dimensional top-level features. Here C is the number of classes. We place a Gaussian process on\neach dimension of these features. We used RBF base kernels. The additive GP layer is then followed\nby a linear mixing layer A ∈RC×C. We initialized A to be an identity matrix, and optimized in the\njoint learning procedure to recover cross-dimension correlations from data.\nWe ﬁrst train a deep neural network using SGD with the softmax loss objective, and rectiﬁed linear\nactivation functions. After the neural network has been pre-trained, we ﬁt an additive KISS-GP\nlayer, followed by a linear mixing layer, using the top-level features of the deep network as inputs.\nUsing this pre-training initialization, our joint SV-DKL model of section 3 is then trained through the\nstochastic variational method of section 4 which jointly optimizes all the hyperparameters γ of the\ndeep kernel (including all network weights), as well as the variational parameters, by backpropagating\nderivatives through the proposed marginal likelihood lower bound of the additive Gaussian process in\n5\nsection 4. In all experiments, we use a relatively large mini-batch size (speciﬁed according to the\nfull data size), enabled by the proposed structure exploiting variational inference procedures. We\nachieve good performance setting the number of samples T = 1 in Eq. 4 for expectation estimation\nin variational inference, which provides additional conﬁrmation for a similar observation in [15].\n5.1\nAirline Delays\nWe ﬁrst consider a large airline dataset consisting of ﬂight arrival and departure details for all\ncommercial ﬂights within the US in 2008. The approximately 5.9 million records contain extensive\ninformation about the ﬂights, including the delay in reaching the destination. Following [12], we\nconsider the task of predicting whether a ﬂight was subject to delay based on 8 features (e.g., distance\nto be covered, day of the week, etc).\nClassiﬁcation accuracy\nTable 1 reports the classiﬁcation accuracy of 1) KLSP-GP [12], a recent\nscalable variational GP classiﬁer as discussed in section 4; 2) stand-alone deep neural network (DNN);\n3) DNN+, a stand-alone DNN with an extra Q × c fully-connected hidden layer with Q, c deﬁned as\nin Figure 1; 4) DNN+GP which is a GP applied to a pre-trained DNN (with same architecture as in\n2); and 5) our stochastic variational DKL method (SV-DKL) (same DNN architecture as in 2). For\nDNN, we used a fully-connected architecture with layers d-1000-1000-500-50-c.2\nThe DNN component of the SV-DKL model has the exact same architecture. The SV-DKL joint\ntraining was conducted using a large minibatch size of 50,000 to reduce the variance of the stochastic\ngradient. We can use such a large minibatch in each iteration (which is daunting for regular GP even\nas a whole dataset) due to the efﬁciency of our inference strategy within each mini-batch, leveraging\nstructure exploiting algebra.\nFrom the table we see that SV-DKL outperforms both the alternative variational GP model (KLSP-\nGP) and the stand-alone deep network. DNN+GP outperforms stand-alone DNNs, showing the\nnon-parametric ﬂexibility of kernel methods. By combining KISS-GP with DNNs as part of a joint\nSV-DKL procedure, we obtain better results than DNN and DNN+GP. Besides, both the plain DNN\nand SV-DKL notably improve on stand-alone GPs, indicating a superior capacity of deep architectures\nto learn representations from large but ﬁnite training sets, despite the asymptotic approximation\nproperties of Gaussian processes. By contrast, adding an extra hidden layer, as in DNN+, does not\nimprove performance.\nFigure 2(a) further studies how performance changes as data size increases. We observe that the\nproposed SV-DKL classiﬁer trained on 1/50 of the data already can reach a competitive accuracy as\ncompared to the KLSP-GP model trained on the full dataset. As the number of the training points\nincreases, the SV-DKL and DNN models continue to improve. This experiment demonstrates the\nvalue of expressive kernel functions on large data problems, which can effectively capture the extra\ninformation available as seeing more training instances. Furthermore, SV-DKL consistently provides\nbetter performance over the plain DNN, through its non-parametric ﬂexibility.\nScalability\nWe next measure the scalability of our variational DKL in terms of the number of\ninducing points m in each GP. Figure 2(c) shows the runtimes in seconds, as a function of m, for\nevaluating the objective and any relevant derivatives. We compare our structure exploiting variational\nmethod with the scalable variational inference in KLSP-GP, and the MCMC-based variational method\nin MC-GP [13]. We see that our inference approach is far more efﬁcient than previous scalable\nalgorithms. Moreover, when the number of inducing points is not too large (e.g., m = 70), the added\ntime for SV-DKL over DNN is reasonable (e.g., 0.39s vs. 0.27s), especially considering the gains in\nperformance and expressive power. Figure 2(d) shows the runtime scaling of different variational\nmethods as m grows. We can see that the runtime of our approach increases only slowly in a wide\nrange of m (< 2, 000), greatly enhancing the scalability over the other methods. This empirically\nvalidates the improved time complexity of our new inference method as presented in section 4.\nWe next investigate the total training time of the models. Table 1, right panel, lists the time cost of\ntraining KLSP-GP, DNN, and SV-DKL; and Figure 2(b) shows how the training time of SV-DKL and\nDNN changes as more training data is presented. We see that on the full dataset DKL, as a GP model,\nsaves over 60% time as compared to the modern state of the art KLSP-GP, while at the same time\n2We obtained similar results with other DNN architectures (e.g., d-1000-1000-500-50-20-c).\n6\nTable 1: Classiﬁcation accuracy and training time on the airline delay dataset, with n data points, d input\ndimensions, and c classes. KLSP-GP is a stochastic variational GP classiﬁer proposed in [12]. DNN+ is the\nDNN with an extra hidden layer. DNN+GP is a GP applied to ﬁxed pre-trained output layer of the DNN (without\nthe extra hidden layer). Following Hensman et al. [12], we selected a hold-out sets of 100,000 points uniformly\nat random, and the results of DNN and SV-DKL are averaged over 5 runs ± one standard deviation. Since the\ncode of KLSP-GP is not publicly available we directly show the results from [12].\nDatasets\nn\nd\nc\nAccuracy\nTotal Training Time (h)\nKLSP-GP [12]\nDNN\nDNN+\nDNN+GP\nSV-DKL\nKLSP-GP\nDNN\nSV-DKL\nAirline\n5,934,530\n8\n2\n∼0.675\n0.773±0.001\n0.722±0.002\n0.7746±0.001\n0.781±0.001\n∼11\n0.53\n3.98\n1\n2\n3\n4\n5\n6\nx 10\n6\n0.66\n0.68\n0.7\n0.72\n0.74\n0.76\n0.78\n#Training Instances\nAccuracy\n \n \nDNN\nSV−DKL\nKLSP−GP\n1\n2\n3\n4\n5\n6\nx 10\n6\n0\n2\n4\n6\n8\n10\n12\n#Training Instances\nTraining time (h)\n \n \nDNN\nSV−DKL\nKLSP−GP\n70 200\n400\n800\n1200\n1600\n2000\n0\n100\n200\n300\n#Inducing points\nRuntime (s)\n \n \nSV−DKL\nKLSP−GP\nMC−GP\nDNN\n70 200\n400\n800\n1200\n1600\n2000\n1\n50\n100\n150\n200\n#Inducing points\nRuntime scaling\n \n \nSV−DKL\nKLSP−GP\nMC−GP\nslope=1\nFigure 2: (a) Classiﬁcation accuracy vs. the number of training points (n). We tested the deep models, DNN\nand SV-DKL, by training on 1/50, 1/10, 1/3, and the full dataset, respectively. For comparison, the cyan diamond\nand black dashed line show the accuracy level of KLSP-GP trained on the full data. (b) Training time vs. n. The\ncyan diamond and black dashed line show the training time of KLSP-GP on the full data. (c) Runtime vs. the\nnumber of inducing points (m) on airline task, by applying different variational methods for deep kernel learning.\nThe minibatch size is ﬁxed to 50,000. The runtime of the stand-alone DNN does not change as m varies. (d)\nThe scaling of runtime relative to the runtime of m = 70. The black dashed line indicates a slope of 1.\nachieving over an 18% improvement in predictive accuracy. Generally, the training time of SV-DKL\nincreases slowly with growing data sizes, and has only modest additional overhead compared to\nstand-alone architectures, justiﬁed by improvements in performance, and the general beneﬁts of a\nnon-parametric probabilistic representation. Moreover, the DNN was fully trained on a GPU, while\nin SV-DKL the base kernel hyperparameters and variational parameters were optimized on a CPU.\nSince most updates of the SV-DKL parameters are computed in matrix forms, we believe the already\nmodest time gap between SV-DKL and DNNs can be almost entirely closed by deploying the whole\nSV-DKL model on GPUs.\n5.2\nUCI Classiﬁcation Tasks\nThe second evaluation of our proposed algorithm (SV-DKL) is conducted on a number of commonly\nused UCI classiﬁcation tasks of varying sizes and properties. Table 2 lists the classiﬁcation accuracy\nof SVM, DNN, DNN+ (a stand-alone DNN with an extra Q × c fully-connected hidden layer with Q,\nc deﬁned as in Figure 1), DNN+GP (a GP trained on the top level features of a trained DNN without\nthe extra hidden layer), and SV-DKL (same architecture as DNN).\nThe plain DNN, which learns salient features effectively from raw data, gives notably higher accuracy\ncompared to an SVM, the mostly widely used kernel method for classiﬁcation problems. We see that\nthe extra layer in DNN+GP can sometimes harm performance. By contrast, non-parametric ﬂexibility\nof DNN+GP consistently improves upon DNN. And SV-DKL, by training a DNN through a GP\nmarginal likelihood objective, consistently provides further enhancements (with particularly notable\nperformance on the Connect4 and Covtype datasets).\n5.3\nImage Classiﬁcation\nWe next evaluate the proposed scalable SV-DKL procedure for efﬁciently handling high-dimensional\nhighly-structured image data. We used a minibatch size of 5,000 for stochastic gradient training of\nSV-DKL. Table 3 compares SV-DKL with the most recent scalable GP classiﬁers. Besides KLSP-GP,\nwe also collected the results of the MC-GP [13] which uses a hybrid Monte Carlo sampler to tackle\nnon-conjugate likelihoods, SAVI-GP [7] which approximates with a univariate Gaussian sampler,\n7\nTable 2: Classiﬁcation accuracy on the UCI datasets. We report the average accuracy ± one standard\ndeviation using 5-fold cross validation. To compare with an SVM, we used the popular libsvm [4]\ntoolbox. RBF kernel was used in SVM, and optimal hyper-parameters are selected automatically\nusing the built-in functionality. On each dataset we used a fully-connected DNN which has the same\narchitecture as in the airline delay task, except for DNN+ which has an additional hidden layer.\nDatasets\nn\nd\nc\nAccuracy\nSVM\nDNN\nDNN+\nDNN+GP\nSV-DKL\nAdult\n48,842\n14\n2\n0.849±0.001\n0.852±0.001\n0.845±0.001\n0.853±0.001\n0.857±0.001\nConnect4\n67,557\n42\n3\n0.773±0.002\n0.805±0.005\n0.804±0.009\n0.811±0.005\n0.841±0.006\nDiabetes\n101,766\n49\n25\n0.869±0.000\n0.889±0.001\n0.890±0.001\n0.891±0.001\n0.896±0.001\nCovtype\n581,012\n54\n7\n0.796±0.006\n0.824±0.004\n0.811±0.002\n0.827±0.006\n0.860±0.006\nTable 3: Classiﬁcation accuracy on the image classiﬁcation benchmarks. MNIST-Binary is the task to\ndifferentiate between odd and even digits on the MNIST dataset. We followed the standard training-test set\npartitioning of all these datasets. We have collected recently published results of a variety of scalable GPs.\nFor CNNs, we used the respective benchmark architectures (or with slight adaptations) from Caffe. CNN+ is\na stand-alone CNN with Q × c fully connected extra hidden layer. See the text for more details, including a\ncomparison with ResNets on CIFAR10.\nDatasets\nn\nd\nc\nAccuracy\nMC-GP [13]\nSAVI-GP [7]\nD-GPLVM [9]\nKLSP-GP [12]\nCNN\nCNN+\nCNN+GP\nSV-DKL\nMNIST-Binary\n60K\n28×28\n2\n—\n—\n—\n0.978\n0.9934\n0.8838\n0.9938\n0.9940\nMNIST\n60K\n28×28\n10\n0.9804\n0.9749\n0.9405\n—\n0.9908\n0.9909\n0.9915\n0.9920\nCIFAR10\n50K\n3×32×32\n10\n—\n—\n—\n—\n0.7592\n0.7618\n0.7633\n0.7704\nSVHN\n73K\n3×32×32\n10\n—\n—\n—\n—\n0.9214\n0.9193\n0.9221\n0.9228\nas well as the distributed GP latent variable model (denoted as D-GPLVM) [9]. We see that on the\nrespective benchmark tasks, SV-DKL improves over all of the above scalable GP methods by a large\nmargin. We note that these datasets are very challenging for conventional GP methods.\nWe further compare SV-DKL to stand-alone convolutional neural networks, and GPs applied to\nﬁxed pre-trained CNNs (CNN+GP). On the ﬁrst three datasets in Table 3, we used the reference\nCNN models implemented in Caffe; and for the SVHN dataset, as no benchmark architecture is\navailable, we used the CIFAR10 architecture which turned out to perform quite well. As we can see,\nthe SV-DKL model outperforms CNNs and CNN+GP on all datasets. By contrast, the extra hidden\nQ × c hidden layer CNN+ does not consistently improve performance over CNN.\nWe also provide brief evaluations of SV-DKL for ResNets and ImageNet, as we believe such\nexploration will be a promising direction for future research.\nResNet Comparison: Based on one of the best public implementations on Caffe, the ResNet-20 has\n0.901 accuracy on CIFAR10, and SV-DKL (with this ResNet base architecture) improves to 0.910.\nImageNet: We randomly selected 20 categories of images with an AlexNet variant as the base NN\n[16], which has an accuracy of 0.6877, while SV-DKL achieves 0.7067 accuracy.\n5.3.1\nInterpretation\nIn Figure 3(a) we investigate the deep kernels learned on the MNIST dataset by randomly selecting 4\nclasses and visualizing the covariance matrices of respective dimensions. The covariance matrices are\nevaluated on the set of test inputs, sorted in terms of the labels of the input images. We see that the\ndeep kernel on each dimension effectively discovers the correlations between the images within the\ncorresponding class. For instance, in c = 2 the data points between 2k-3k (i.e., images of digit 2) are\nstrongly correlated with each other, and carry little correlation with the rest of the images. Besides,\nwe can also clearly observe that the rest of the data points also form multiple “blocks”, rather than\nbeing crammed together without any structure. This validates that the DKL procedure and additive\nGPs do capture the correlations across different dimensions.\nTo further explore the learnt dependencies between the output classes and the additive GPs serving\nas the bases, we visualized the weights of the mixing layer (A) in Fig. 3(b), enabling the correlated\n8\nc=2\n2k\n4k\n6k\n8k\n10k\n2k\n4k\n6k\n8k\n10k\nc=3\n2k\n4k\n6k\n8k\n10k\n2k\n4k\n6k\n8k\n10k\nc=6\n2k\n4k\n6k\n8k\n10k\n2k\n4k\n6k\n8k\n10k\nc=8\n2k\n4k\n6k\n8k\n10k\n2k\n4k\n6k\n8k\n10k\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nInput dimensions\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nOutput classes\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-0.1\n0\n0.1\n0.2\nFigure 3: (a) The induced covariance matrices on classes 2, 3, 6, and 8, on test cases of the MNIST dataset\nordered according to the labels. (b) The ﬁnal mixing layer (i.e., matrix A) on MNIST digit recognition.\nmulti-output (multi-task) nature of the model. Besides the expected high weights along the diagonal,\nwe ﬁnd that class 9 is also highly correlated with dimension 0 and 6, which is consistent with the\nvisual similarity between digit “9” and “0”/“6”. Overall, the ability to interpret the learned deep\ncovariance matrix as discovering an expressive similarity metric across data instances is a distinctive\nfeature of our approach.\n6\nDiscussion\nWe introduced a scalable Gaussian process model which leverages deep learning, stochastic variational\ninference, structure exploiting algebra, and additive covariance structures. The resulting deep kernel\nlearning approach, SV-DKL, allows for classiﬁcation and non-Gaussian likelihoods, multi-task\nlearning, and mini-batch training. SV-DKL achieves superior performance over alternative scalable\nGP models and stand-alone deep networks on many signiﬁcant benchmarks.\nSeveral fundamental themes emerge from the exposition: (1) kernel methods and deep learning\napproaches are complementary, and we can combine the advantages of each approach; (2) expressive\nkernel functions are particularly valuable on large datasets; (3) by viewing neural networks through\nthe lens of metric learning, deep learning approaches become more interpretable.\nDeep learning is able to obtain good predictive accuracy by automatically learning structure which\nwould be difﬁcult to a priori feature engineer into a model. In the future, we hope deep kernel learning\napproaches will be particularly helpful both for characterizing uncertainty and for interpreting these\nlearned features, leading to new scientiﬁc insights into our modelling problems.\nAcknowledgements:\nWe thank NSF IIS-1563887, ONR N000141410684, N000141310721,\nN000141512791, and ADeLAIDE FA8750-16C-0130-001 grants. We also thank anonymous review-\ners for helpful comments.\nReferences\n[1] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On the surprising behavior of distance metrics\nin high dimensional space. Springer, 2001.\n[2] T. D. Bui and R. E. Turner. Stochastic variational inference for Gaussian process latent variable\nmodels using back constraints. Black Box Learning and Inference NIPS workshop, 2015.\n[3] R. Calandra, J. Peters, C. E. Rasmussen, and M. P. Deisenroth. Manifold Gaussian processes\nfor regression. arXiv preprint arXiv:1402.5876, 2014.\n[4] C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. ACM Transactions\non Intelligent Systems and Technology (TIST), 2(3):27, 2011.\n[5] Z. Dai, A. Damianou, J. González, and N. Lawrence. Variational auto-encoded deep Gaussian\nprocesses. arXiv preprint arXiv:1511.06455, 2015.\n9\n[6] A. Damianou and N. Lawrence. Deep Gaussian processes. Artiﬁcial Intelligence and Statistics,\n2013.\n[7] A. Dezfouli and E. V. Bonilla. Scalable inference for Gaussian process models with black-box\nlikelihoods. In Advances in Neural Information Processing Systems, pages 1414–1422, 2015.\n[8] N. Durrande, D. Ginsbourger, and O. Roustant. Additive kernels for Gaussian process modeling.\narXiv preprint arXiv:1103.4023, 2011.\n[9] Y. Gal, M. van der Wilk, and C. Rasmussen. Distributed variational inference in sparse Gaussian\nprocess regression and latent variable models. In Advances in Neural Information Processing\nSystems, pages 3257–3265, 2014.\n[10] M. Gönen and E. Alpaydın. Multiple kernel learning algorithms. Journal of Machine Learning\nResearch, 12:2211–2268, 2011.\n[11] J. Hensman, N. Fusi, and N. Lawrence. Gaussian processes for big data. In Uncertainty in\nArtiﬁcial Intelligence (UAI). AUAI Press, 2013.\n[12] J. Hensman, A. Matthews, and Z. Ghahramani. Scalable variational Gaussian process classiﬁca-\ntion. In Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and\nStatistics, pages 351–360, 2015.\n[13] J. Hensman, A. G. Matthews, M. Filippone, and Z. Ghahramani. MCMC for variationally sparse\nGaussian processes. In Advances in Neural Information Processing Systems, pages 1648–1656,\n2015.\n[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and\nT. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093, 2014.\n[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[16] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, 2012.\n[17] Q. Le, T. Sarlos, and A. Smola. Fastfood-computing Hilbert space expansions in loglinear time.\nIn Proceedings of the 30th International Conference on Machine Learning, pages 244–252,\n2013.\n[18] J. R. Lloyd, D. Duvenaud, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani. Automatic con-\nstruction and Natural-Language description of nonparametric regression models. In Association\nfor the Advancement of Artiﬁcial Intelligence (AAAI), 2014.\n[19] T. Nickson, T. Gunter, C. Lloyd, M. A. Osborne, and S. Roberts. Blitzkriging: Kronecker-\nstructured stochastic Gaussian processes. arXiv preprint arXiv:1510.07965, 2015.\n[20] J. Quiñonero-Candela and C. Rasmussen. A unifying view of sparse approximate Gaussian\nprocess regression. The Journal of Machine Learning Research, 6:1939–1959, 2005.\n[21] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Neural Informa-\ntion Processing Systems, 2007.\n[22] C. E. Rasmussen and Z. Ghahramani. Occam’s razor. In Neural Information Processing Systems\n(NIPS), 2001.\n[23] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for Machine Learning. The MIT\nPress, 2006.\n[24] B. W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression\ncurve ﬁtting. Journal of the Royal Statistical SocietyB, 47(1):1–52, 1985.\n[25] M. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In\nInternational Conference on Artiﬁcial Intelligence and Statistics, pages 567–574, 2009.\n10\n[26] A. G. Wilson. Covariance kernels for fast automatic pattern discovery and extrapolation with\nGaussian processes. PhD thesis, University of Cambridge, 2014.\n[27] A. G. Wilson and R. P. Adams. Gaussian process kernels for pattern discovery and extrapolation.\nInternational Conference on Machine Learning (ICML), 2013.\n[28] A. G. Wilson and H. Nickisch. Kernel interpolation for scalable structured Gaussian processes\n(KISS-GP). International Conference on Machine Learning (ICML), 2015.\n[29] A. G. Wilson, D. A. Knowles, and Z. Ghahramani. Gaussian process regression networks. In\nInternational Conference on Machine Learning (ICML), Edinburgh, 2012.\n[30] A. G. Wilson, C. Dann, and H. Nickisch. Thoughts on massively scalable Gaussian processes.\narXiv preprint arXiv:1511.01870, 2015. https://arxiv.org/abs/1511.01870.\n[31] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing. Deep kernel learning. Artiﬁcial\nIntelligence and Statistics, 2016.\n[32] Z. Yang, A. J. Smola, L. Song, and A. G. Wilson. A la carte - learning fast kernels. Artiﬁcial\nIntelligence and Statistics, 2015.\nA\nNegative Log Probability (NLP) Results\nTables 4, 5, and 6 show the negative log probability values on different tasks. Generally we observed\nsimilar trends as from the classiﬁcation accuracy results.\nTable 4: Negative log probability results on the airline delay dataset, with the same experimental\nsetting as in section 5.1.\nDatasets\nn\nd\nc\nNLP\nKLSP-GP\nDNN\nDNN+GP\nSV-DKL\nAirline\n5,934,530\n8\n2\n∼0.61\n0.474±0.001\n0.473±0.001\n0.461±0.001\nTable 5: Negative log probability results on the UCI datasets, with the same experimental setting as\nin section 5.2.\nDatasets\nn\nd\nc\nNLP\nDNN\nDNN+GP\nSV-DKL\nAdult\n48,842\n14\n2\n0.316±0.003\n0.314±0.003\n0.312±0.003\nConnect4\n67,557\n42\n3\n0.495±0.003\n0.478±0.003\n0.449±0.002\nDiabetes\n101,766\n49\n25\n0.404±0.001\n0.396±0.002\n0.385±0.002\nCovtype\n581,012\n54\n7\n0.435±0.004\n0.429±0.005\n0.365±0.004\nTable 6: Negative log probability results on the image classiﬁcation benchmarks, with the same\nexperimental setting as in section 5.3.\nDatasets\nn\nd\nc\nNLP\nMC-GP\nKLSP-GP\nCNN\nCNN+GP\nSV-DKL\nMNIST-Binary\n60K\n28×28\n2\n—\n0.069\n0.020\n0.019\n0.018\nMNIST\n60K\n28×28\n10\n0.064\n—\n0.028\n0.028\n0.028\nCIFAR10\n50K\n3×32×32\n10\n—\n—\n0.738\n0.738\n0.734\nSVHN\n73K\n3×32×32\n10\n—\n—\n0.309\n0.310\n0.308\n11\nB\nStochastic Variational Inference for Deep Kernel Learning Classiﬁcation\nRecall the SV-DKL classiﬁcation model\np(yi|f i, A) =\nexp(a(f i)T yi)\nP\nc exp(a(f i)T ec)\np(f j|uj) = M (j)uj\np(uj) = N(uj|0, K(j)\nZ,Z),\n(5)\nLet u = {uj}J\nj=1. We assume a variational posterior over the inducing variables\nq(u) =\nY\nj\nN(uj|µj, Sj)\n(6)\nBy Jensen’s inequality we have\nlog p(y) ≥Eq(u)p(f|u)[log p(y|f)] −KL[q(u)∥p(u)]\n≜L(q),\n(7)\nIn the following we omit the GP index j when there is no ambiguity. Due to the deterministic\nmapping, we can obtain latent function samples from the samples of u:\nf (t) = Mu(t).\n(8)\nTo sample from q(u), we use the Cholesky decomposition for reparameterizing u in order to preserve\nstructures within the covariance. Speciﬁcally, we let S = LT L. This results in the following sampling\nprocedure for u:\nu(t) = µ + Lϵ(t);\nϵ(t) ∼N(0, I).\nWe further scale up the sampler by leveraging the fact that the inducing points are placed on a grid,\nand imposing Kronecker decomposition on L = ND\nd=1 Ld, where D is the input dimension of the\nbase kernel. With the fast Kronecker matrix-vector products, the sampling cost is O(m1+1/D). Note\nthat\nS =\n\u0010O\nLd\n\u0011T \u0010O\nLd\n\u0011\n=\nD\nO\nd=1\nLT\nd Ld :=\nD\nO\nd=1\nSd\nWith the samples, then for any h(u), we have\nEq(u)[h(u)] ≃1\nT\nT\nX\nt=1\nh(u(t))\n= 1\nT\nT\nX\nl=1\nh(µ + Lϵ(t))\n≃Ep(ϵ)[h(µ + Lϵ)]\n(9)\nNext we give the derivation of the objective lower bound and its derivatives in detail. In the following\nwe denote K := KZ,Z for clarity.\nComputation of the marginal likelihood lower bound\nThe expectation term of objective lower\nbound Eq (7) can be computed straightforwardly following Eq (9). The KL term has a closed form\n(we omit the GP index j):\nKL(q(u)∥p(u)) = 1\n2\n\b\nlog |K| −log |S| −D + tr(K−1S) + µT K−1µ\n\t\n.\n(10)\n12\nWith the Kronecker product representation of the covariance matrices, all the above matrix operations\ncan be conducted efﬁciently:\nlog det S = log\nD\nY\nd=1\ndet(LdLT\nd )rankd\n= 2\nD\nX\nd=1\nrankd\nmd\nX\np=1\nlog Ld,pp\ntr(K−1S) =\nD\nY\nd=1\ntr(K−1\nd Sd),\n(11)\nwhere md is the number of inducing points in dimension d (we have m = QD\nd=1 md); rankd =\nQ\nd′̸=d rank(Sd′); and K = ND\nd=1 Kd.\nDerivatives w.r.t the base kernel hyperparameters\nNote that the base kernel hyperparameters θ\nare only involved in the KL term of Eq (7). The derivative is\n∂L\n∂θ = ∂KL(q∥p)\n∂θ\n= 1\n2\n\u001a\ntr(K−1 ∂K\n∂θ ) + tr(∂K−1\n∂θ\nS) −µT K−1 ∂K\n∂θ K−1µ\n\u001b\n= 1\n2\n\u001a\ntr(K−1 ∂K\n∂θ ) −tr(K−1 ∂K\n∂θ K−1S) −µT K−1 ∂K\n∂θ K−1µ\n\u001b\n(12)\nNote that the matrix inversions and traces can be computed efﬁciently by leveraging the Kronecker\nproduct as in Eq (11).\nDerivatives w.r.t other model parameters\nOther model parameters, including the deep network\nweights and the top-layer mixing weights, are only involved in the likelihood expectation term in\nEq (7), and can be computed conveniently by following Eq (9) with h(·) replaced by the respective\nderivatives of the softmax likelihood in Eq (5).\nDerivatives w.r.t the variational parameters\nWe only show the derivatives w.r.t the variational\ncovariance parameters L. The derivatives w.r.t the variational means µ can be derived similarly.\n(1) The derivative of the softmax expectation term of input i w.r.t the (p, q)-th element of L(j)\nd ,\ndenoted as λ for clarity, is given by\n∇λ log p(yi|f i) = Ep(ϵ)\n\" X\nc\n1(yic = 1)Acj −\nexp(a(f i)T yi)\nP\nc exp(a(f i)T ec)Acj\n!\nM (j)\ni· ∇λL(j)ϵ\n#\n,\nwhere M (j)\ni·\nis the ith row of the interpolation matrix (i.e., the interpolation vector of input i); and\n∇λL(j) = L(j)\n1\n⊗· · · ⊗∇λL(j)\nd\n⊗· · · ⊗L(j)\nD . Note that for D = 1, we can directly write down the\nderivatives w.r.t the whole matrix L(j) which is efﬁcient for computing:\n∇L(j) log p(yi|f i) = Ep(ϵ)\n\" X\nc\n1(yic = 1)Acj −\nexp(a(f i)T yi)\nP\nc exp(a(f i)T ec)Acj\n!\n(ϵM (j)\ni· )T\n#\n.\n(2) The derivative of the KL term is (index j omitted):\n∇λKL[q(u)∥p(u)] = 1\n2\n∂−log |S| + tr(K−1S)\n∂λ\n= −(L−1\nd )pq + tr(K−1\n1 S1) · · · tr(K−1\nd Ld∇λLT\nd ) · · · tr(K−1\nD SD),\nwhere tr(K−1\nd Ld∇λLT\nd ) = (K−1\nd· Ld)pq. Note that the Kronecker factor matrices are small (with\nsize md × md) and thus the above computations are fast.\n13\n",
  "categories": [
    "stat.ML",
    "cs.LG",
    "stat.ME"
  ],
  "published": "2016-11-01",
  "updated": "2016-11-02"
}