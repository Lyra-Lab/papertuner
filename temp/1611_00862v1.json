{
  "id": "http://arxiv.org/abs/1611.00862v1",
  "title": "Quantile Reinforcement Learning",
  "authors": [
    "Hugo Gilbert",
    "Paul Weng"
  ],
  "abstract": "In reinforcement learning, the standard criterion to evaluate policies in a\nstate is the expectation of (discounted) sum of rewards. However, this\ncriterion may not always be suitable, we consider an alternative criterion\nbased on the notion of quantiles. In the case of episodic reinforcement\nlearning problems, we propose an algorithm based on stochastic approximation\nwith two timescales. We evaluate our proposition on a simple model of the TV\nshow, Who wants to be a millionaire.",
  "text": "JMLR: Workshop and Conference Proceedings 60 (2016) 1–16\nACML 2016\nQuantile Reinforcement Learning\nHugo Gilbert\nhugo.gilbert@lip6.fr\nSorbonnes Universit´es\nUPMC Univ Paris 06\nCNRS, LIP6 UMR 7606\nParis, France\nPaul Weng\npaweng@cmu.edu\nSchool of Electronics and Information Technology\nSYSU-CMU Joint Institute of Engineering\nSYSU-CMU Shunde Joint Research Institute\nGuangzhou, 510006, PR China\nAbstract\nIn reinforcement learning, the standard criterion to evaluate policies in a state is the expec-\ntation of (discounted) sum of rewards. However, this criterion may not always be suitable,\nwe consider an alternative criterion based on the notion of quantiles. In the case of episodic\nreinforcement learning problems, we propose an algorithm based on stochastic approxima-\ntion with two timescales. We evaluate our proposition on a simple model of the TV show,\nWho wants to be a millionaire.\nKeywords: Reinforcement learning, Quantile, Ordinal Decision Model, Two-Timescale\nStochastic Approximation\n1. Introduction\nMarkov decision process and reinforcement learning are powerful frameworks for building\nautonomous agents (physical or virtual), which are systems that make decisions without\nhuman supervision in order to perform a given task. Examples of such systems abound:\nexpert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al., 2001), acrobatic\nhelicopter ﬂight (Abbeel et al., 2010) or human-level video game player (Mnih et al., 2015).\nHowever, the standard framework assumes that rewards are numeric, scalar and additive\nand that policies are evaluated with the expectation criterion. In practice, it may happen\nthat such numerical rewards are not available, for instance, when the agent interacts with\na human who generally gives ordinal feedback (e.g., “excellent”, “good”, “bad” and so on).\nBesides, even when this numerical information is available, one may want to optimize a\ncriterion diﬀerent than the expectation, for instance in one-shot decision-making.\nSeveral works considered the case where preferences are qualitative. Markov decision\nprocesses with ordinal reward have been investigated (Weng, 2012, 2011) and diﬀerent\nordinal decision criteria have been proposed in that context. More generally, preference-\nbased reinforcement learning (Akrour et al., 2012; F¨urnkranz et al., 2012; Busa-Fekete et al.,\n2013, 2014) has been proposed to tackle situations where the only available preferential\ninformation concerns pairwise comparisons of histories.\nc⃝2016 H. Gilbert & P. Weng.\narXiv:1611.00862v1  [cs.LG]  3 Nov 2016\nGilbert Weng\nIn this paper, we propose to search for a policy that optimizes a quantile instead of\nthe expectation. Intuitively, the τ-quantile of a distribution is the value q such that the\nprobability of getting a value lower than q is τ (and therefore the probability of getting\na value greater than q is 1 −τ). The median is an example of quantile where τ = 0.5.\nInterestingly, in order to use this criterion only an order over valuations is needed.\nThe quantile criterion is extensively used as a decision criterion in many domains. In\nﬁnance, it is a risk measure and is known as Value-at-Risk (Jorion, 2006). For its cloud\ncomputing services, Amazon reports (DeCandia et al., 2007) that they optimize the 99.9%-\nquantile1. In fact, decisions in the web industry are often made based on quantiles (Wolski\nand Brevik, 2014; DeCandia et al., 2007). More generally, in the service industry, because\nof skewed distributions (Benoit and Van den Poel, 2009), one generally does not want that\ncustomers are satisﬁed on average, but rather that most customers (e.g., 99% of them) to\nbe as satisﬁed as possible.\nThe use of the quantile criterion can be explained by the nice properties it enjoys:\n• preferences and uncertainty can be valued on scales that are not commensurable,\n• preferences over actions or trajectories can be expressed on a purely ordinal scale,\n• preferences over policies are more robust than with the standard criterion of maxi-\nmizing the expectation of cumulated rewards.\nThe contributions of this paper are as follows. To the best of our knowledge, we are\nthe ﬁrst to propose an RL algorithm to learn a policy optimal for the quantile criterion.\nThis algorithm is based on stochastic approximation with two timescales. We present an\nempirical evaluation of our proposition on a version of Who wants to be a millionaire.\nThe paper is organized as follows. Section 2 presents the related work. Section 3 recalls\nthe necessary background for presenting our approach. Section 4 states the problem we solve\nand introduce our algorithm, Quantile Q-learning. Section 5 presents some experimental\nresults. Finally, we conclude in Section 6.\n2. Related Work\nA great deal of research on MDPs (Boussard et al., 2010) considered decision criteria diﬀer-\nent to the standard ones (i.e., expected discounted sum of rewards, expected total rewards\nor expected average rewards). For instance, in the operations research community, White\n(1987) notably considered diﬀerent cases where preferences over policies only depend on\nsums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance for-\nmulations. In this context, he showed the suﬃciency of working in a state space augmented\nwith the sum of rewards obtained so far. Filar et al. (1989) investigated decision criteria\nthat are variance-penalized versions of the standard ones. They formulated the obtained\noptimization problem as a non-linear program. Yu et al. (1998) optimized the probability\nthat the total reward becomes higher than a certain threshold.\nAdditionally, in the artiﬁcial intelligence community, Liu and Koenig (2005, 2006) also\ninvestigated the use of EU as a decision criterion in MDPs. To optimize it, they proposed a\n1. Or 0.01%-quantile, depending on whether the problem is expressed in terms of costs or rewards.\n2\nQuantile RL\nfunctional variation of Value Iteration. In the continuation of this work, Gilbert et al. (2015)\ninvestigated the use of Skew-Symmetric Bilinear (SSB) utility (Fishburn, 1981) functions —\na generalization of EU that enables intransitive behaviors and violation of the independence\naxiom — as decision criteria in ﬁnite-horizon MDPs. Interestingly, SSB also encompasses\nprobabilistic dominance, a decision criterion that is employed in preference-based sequential\ndecision-making (Akrour et al., 2012; F¨urnkranz et al., 2012; Busa-Fekete et al., 2013, 2014).\nIn theoretical computer science, sophisticated decision criteria have also been studied\nin MDPs. For instance, Gimbert (2007) proved that many decision criteria based on ex-\npectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy.\nBruy`ere et al. (2014) considered sophisticated preferences over policies, which amounts to\nsearching for policies that maximize the standard criterion while ensuring an expected sum\nof rewards higher than a threshold with probability higher than a ﬁxed value. This work\nhas also been extended to the multiobjective setting (Randour et al., 2014).\nRecent work in Markov decision process and reinforcement learning considered condi-\ntional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B¨auerle and\nOtt (2011) proved the existence of deterministic wealth-Markovian policies optimal with\nrespect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for\nCVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead\nof as objective function.\nCloser to our work, several quantile-based decision models have been investigated in\ndiﬀerent contexts. In uncertain MDPs where the parameters of the transition and reward\nfunctions are imprecisely known, Delage and Mannor (2007) presented and investigated a\nquantile-like criterion to capture the trade-oﬀbetween optimistic and pessimistic viewpoints\non an uncertain MDP. The quantile criterion they use is diﬀerent to ours as it takes into\naccount the uncertainty present in the parameters of the MDP.\nIn MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision\nmodels were proposed to compute policies that maximize a quantile using linear program-\nming. While quantiles in those works are deﬁned on distributions over ordinal rewards,\nquantiles in this paper are deﬁned on distributions over histories.\nMore recently, in the machine learning community, quantile-based criteria have been\nproposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning.\nYu and Nikolova (2013) proposed an algorithm in the pure exploration setting for diﬀerent\nrisk measures, including Value-at-Risk. Carpentier and Valko (2014) studied the problem\nof identifying arms with extreme payoﬀs, a particular case of quantiles. Finally, Sz¨or´enyi\net al. (2015) investigated MAB problems where a quantile is optimized instead of the mean.\nThe algorithm we propose is based on stochastic approximation with two timescales, a\ntechnique proposed by Borkar (1997, 2008). This method has recently been exploited in\nachievability problems (Blackwell, 1956) in the context of multiobjective MDPs (Kalathil\net al., 2014) and for learning SSB-optimal policies (Gilbert et al., 2016).\n3. Background\nWe provide in this section the background information necessary to present our algorithm\nto learn a policy optimal for the quantile criterion.\n3\nGilbert Weng\n3.1. Markov Decision Process\nMarkov Decision Processes (MDPs) oﬀer a powerful formalism to model and solve sequential\ndecision-making problems (Puterman, 1994). A ﬁnite horizon MDP is formally deﬁned as\na tuple MT = (S, A, P, R, s0) where:\n• T is a ﬁnite horizon,\n• S is a ﬁnite set of states,\n• A is a ﬁnite set of actions,\n• P : S × A × S →R is a transition function with P(s, a, s′) being the probability of\nreaching state s′ when action a is performed in state s,\n• R : S × A →R is a bounded reward function and\n• s0 ∈S is an initial state.\nIn this model, starting from initial state s0, an agent chooses at every time step t an\naction at to perform in her current state st, which she can observe. This action results in\na new state st+1 ∈S according to probability distribution P(st, at, .), and a reward signal\nR(st, at), which penalizes or reinforces the choice of this action.\nWe will call t-history ht a succession of t state-action pairs starting from state s0 (e.g.,\nht = (s0, a1, s1, . . . , st−1, at−1, st)). The action choices of the agent is guided by a policy.\nMore formally, a policy π at an horizon T is a sequence of T decision rules (δT , . . . , δ1).\nDecision rules prescribe which action the agent should perform at a given time step. They\ncan be Markovian if they only depend on the current state. Besides, a decision rule is\neither deterministic if it always selects the same action in a given situation or randomized if\nit prescribes a probability distribution over possible actions. Consequently, a policy can be\nMarkovian, deterministic or randomized according to the type of its decision rules. Lastly, a\npolicy is stationary if it applies the same decision rule at every time step, i.e., π = (δ, δ, . . .).\nPolicies can be compared with respect to diﬀerent decision criteria. The usual criterion\nis the expected (discounted) sum of rewards, for which an optimal deterministic Markovian\npolicy is known to exist for any horizon T. This criterion is deﬁned as follows. First, the\nvalue of a history ht = (s0, a1, s1, . . . , st−1, at, st) is described as the (possibly discounted)\nsum of rewards obtained along it, i.e.,\nr(ht) =\nt\nX\ni=1\nγi−1R(si−1, ai)\nwhere γ ∈[0, 1] is a discount factor. Then, the value of a policy π = (δT , . . . , δ1) in a state\ns is set to be the expected value of the histories that can be generated by π from s. This\nvalue, given by the value function vπ\nT : S →R can be computed iteratively as follows:\nvπ\n0 (s) = 0\nvπ\nt (s) = R(s, δt(s)) + γ\nX\ns′∈S\nP(s, δt(s), s′)vπ\nt−1(s′)\n(1)\n4\nQuantile RL\nThe value vπ\nt (s) is the expectation of cumulated rewards obtained by the agent if she\nperforms action δt(s) in state s at time-step t and continues to follow policy π thereafter.\nThe higher the values of vπ\nt (s) are, the better. Therefore, value functions induce a preference\nrelation ≿π over policies in the following way:\nπ ≿π π′ ⇔∀s ∈S, vπ\nT (s) ≥vπ′\nT (s)\nA solution to an MDP is a policy, called optimal policy, that ranks the highest with\nrespect to ≿π. Such a policy can be found by solving the following equations, which yields\nthe value function of an optimal policy:\nv∗\n0(s) = 0\nv∗\nt (s) = max\na∈A R(s, a) + γ\nX\ns′∈S\nP(s, a, s′)v∗\nt−1(s′)\n3.2. Reinforcement Learning\nIn the reinforcement learning (RL) setting, the assumption of the knowledge of the envi-\nronment is relaxed: both dynamics through the transition function and preferences via the\nreward function are not known anymore. While interacting with its environment, an RL\nagent tries to learn a good policy by trial and error.\nTo make ﬁnite horizon MDPs learnable, we assume the decision process is repeated\ninﬁnitely many times.\nThat is, when horizon T is reached, we assume that the agent\nautomatically returns to the initial state and the problem starts over.\nA simple algorithm to solve such an RL problem is the Q-learning algorithm (see Algo-\nrithm 1), which estimates the Q-function:\nQt(s, a) = R(s, a) + γ\nX\ns′∈S\nP(s, a, s′)Vt−1(s′)\nAnd obviously, we have: Vt(s) = max\na∈A Qt(s, a).\nIn Algorithm 1, Line 1 generally depends on the Qt−1(s, ·) and possibly on iteration n.\nA simple strategy to perform this choice is called ε-greedy where the best action dictated\nby Qt−1(s, ·) is chosen with probability 1 −ε (with ε a small positive value) or a random\naction is chosen otherwise. A schedule can be deﬁned so that parameter ε tends to zero as\nn tends to inﬁnity. Besides, αn(s, a) ∈(0, 1) on Line 2 is a learning rate. In the general\ncase, it depends on iteration n, state s and action a, although in practice it is often chosen\nas a constant.\n3.3. Limits of standard criteria\nThe standard decision criteria used in MDPs, which are based on expectation, may not be\nreasonable in some situations. Firstly, unfortunately, in many cases, the reward function R\nis not known. In those cases, one can try to recover the reward function from a human expert\n(Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013). However,\neven for an expert user, the elicitation of the reward function can reveal burdensome. In\ninverse reinforcement learning (Ng and Russell, 2000), the expert is assumed to know an\n5\nGilbert Weng\nData: MT = (S, A, G, P, R, s0) MDP\nResult: Q\nbegin\nQ0(s, a) ←−0, ∀(s, a) ∈S × A\ns ←−s0\nt ←−1\nfor n = 1 to N do\n1\na ←−choose action\nr, s′ ←−perform action a in state s\n2\nQt(s, a) ←−Qt(s, a) + αn(s, a)\n\u0000r + γ maxa′∈A Qt−1(s′, a′) −Qt(s, a)\n\u0001\n3\nif t = T then\ns ←−s0\nt ←−1\nelse\ns ←−s′\nt ←−t + 1\nend\nend\nend\nAlgorithm 1: Q-learning\noptimal policy, which is rarely true in practice. In interactive settings (Regan and Boutilier,\n2009; Weng and Zanuttini, 2013), this elicitation process can be cognitively very complex\nas it requires to balance several criteria in a complex manner and as it can imply a large\nnumber of parameters. In this paper, we address this problem by only assuming that a\nstrict weak ordering over histories is known.\nSecondly, for numerous applications, the expectation of cumulated reward, as used in\nEquation 1, may not be the most appropriate criterion (even when a numeric reward function\nis deﬁned). For instance, in case of high variance or when a policy is known to be only\napplied a few times, the solution given by this criterion may not be satisfying for risk-averse\nagent. Moreover, in some domains (e.g., web industry or more generally service industry),\ndecisions about performance are often based on the minimal quality of 99% of the possible\noutcomes. Therefore, in this article we aim at using a quantile (deﬁned in Section 3.5) as a\ndecision criterion to solve an MDP.\n3.4. MDP with End States\nIn this paper, we work with episodic MDPs with end states. Such an MDP is formally\ndeﬁned as a tuple MT = (S, A, G, P, s0) where S, A, P, s0 are deﬁned as previously, G ⊆S\nis a ﬁnite set of end states and T is a ﬁnite maximal horizon (i.e., an end state is attained\nafter at most T time steps.). We call episode a history starting from s0 and ending in a\nﬁnal state of G.\nWe assume that a preference relation is deﬁned over end states: We write g′ ≺g if\nend state g is preferred to end state g′. Without loss of generality, we assume that G =\n{g1, . . . , gn} and end states are ordered with increasing preference, i.e., g1 ≺g2 ≺. . . ≺gn.\nThe weak relation of ≺is denoted ⪯.\n6\nQuantile RL\nNote that a ﬁnite horizon MDP can be reformulated as an MDP with end states by\nstate augmentation. Although the resulting MDP may have a large-sized state space, the\ntwo models are formally equivalent. We focus on episodic MDPs with end states to simplify\nthe presentation of our approach.\n3.5. Quantile Criterion\nWe deﬁne quantiles of distributions over end states of G, which are ordered by ⪯. Let\nτ ∈[0, 1] be a ﬁxed parameter. Intuitively, the τ-quantile of a distribution of end states, is\nthe value q ∈G such that the probability of getting an end state equal or lower than q is\nτ and that of getting an end state equal or greater than q is 1 −τ. The 0.5-quantile, also\nknown as median, can be seen as the ordinal counterpart of the mean. The 0-quantile (resp.\n1-quantile) is the minimum (resp. maximum) of a distribution. More generally, quantiles,\nwhich have been axiomatically characterized by Rostek (2010), deﬁne decision criteria that\nhave the nice property of not requiring numeric valuations, but only an order.\nThe formal deﬁnition of quantiles can be stated as follows. Let pπ denote the probability\ndistribution over end states induced by a policy π from initial state s0, the cumulative distri-\nbution induced by pπ is then deﬁned as F π where F π(g) = P\ng′⪯g pπ(g′) is the probability of\ngetting an end state not preferred to g when applying policy π. Similarly, the decumulative\ndistribution induced by pπ is deﬁned as Gπ(g) = P\ng⪯g′ pπ(g′) is the probability of getting\nan end state not lower than g.\nThese two notions of cumulative and decumulative enable us to deﬁne two kinds of\ncriteria. First, given a policy π, we deﬁne the lower τ-quantile for τ ∈(0, 1] as:\nqπ\nτ = min{g ∈G | F π(g) ≥τ}\n(2)\nwhere the min operator is with respect to ⪯.\nThen, given a policy π, we deﬁne the upper τ-quantile for τ ∈[0, 1) as:\nqπ\nτ = max{g ∈G | Gπ(g) ≥1 −τ}\n(3)\nwhere the max operator is with respect to ⪯.\nIf τ = 0 or τ = 1 only one of qπ\nτ or qπ\nτ is deﬁned and we deﬁne the τ-quantile qπ\nτ as that\nvalue. When both are deﬁned, by construction, we have qπ\nτ ⪯qπ\nτ . If those two values are\nequal, qπ\nτ is deﬁned as equal to them. For instance, this is always the case in continuous\nsettings for continuous distributions. However, in our discrete setting, it could happen that\nthose values diﬀer, as shown by Example 1.\nExample 1 Consider an MDP where G = {g1 ≺g2 ≺g3}. Let π be a policy that attains\neach end state with probabilities 0.5, 0.2 and 0.3 respectively. It is easy to check that qπ\n0.5 = g1\nwhereas qπ\n0.5 = g2.\nWhen the lower and the upper quantiles diﬀer, one may deﬁne the quantile as a function\nof the lower and upper quantiles (Weng, 2012). For simplicity, in this paper, we focus on\noptimizing directly the lower and the upper quantiles.\nThe quantile criterion is diﬃcult to optimize, even when a numerical reward function is\ngiven and the quality of an episode is deﬁned as the cumulative of rewards received along\nthe episode. This diﬃculty comes notably from two related sources:\n7\nGilbert Weng\n• The quantile criterion is non-linear: for instance, the τ-quantile q˜π\nτ of the mixed policy\n˜π that generates an episode using policy π with probability p and π′ with probability\n1 −p is not equal to pqπ\nτ + (1 −p)qπ′\nτ .\n• The quantile criterion is non-dynamically consistent: A sub-policy at time step t of\nan optimal policy for horizon T may not be optimal for horizon T −t.\nIn decision theory (McClennen, 1990), three approaches have been considered for such kinds\nof decision criteria:\n1. Consequentialist approach: at each time step t, follow an optimal policy for the prob-\nlem with horizon T −t and initial state st even if the resulting policy is not optimal\nat horizon T;\n2. Resolute choice approach: at time step t = 0, apply an optimal policy for the problem\nwith horizon T and initial state s0 and do not deviate from it;\n3. Sophisticated resolute choice approach (Jaﬀray, 1998; Fargier et al., 2011): apply a\npolicy π (chosen at the beginning) that trades oﬀbetween how much π is optimal for\nall horizons T, T −1, . . . , 1.\nWith non-dynamically consistent preferences, it is debatable to adopt a consequentialist\napproach, as the sequence of decisions may lead to dominated results. In this paper, we\nadopt a resolute choice point of view. We leave the third approach for future work.\n4. Quantile-based Reinforcement Learning\nIn this section, we ﬁrst state the problem solved in this paper and some useful properties.\nThen, we present our algorithm called Quantile Q-learning (or QQ-learning for short),\nwhich is an extension of Q-learning and exploits a two-timescale stochastic approximation\ntechnique.\n4.1. Problem Statement\nIn this paper, we aim at learning a policy that is optimal for the quantile criterion from\na ﬁxed initial state. We assume that the underlying MDP is an episodic MDP with end\nstates. Let τ ∈(0, 1) be a ﬁxed parameter. Formally, the problem of determining a policy\noptimal for the lower/upper τ-quantile can be stated as follows:\nπ∗= arg max\nπ\nqπ\nτ\nor\nπ∗= arg max\nπ\nqπ\nτ\n(4)\nWe focus on learning a policy that is deterministic and Markovian.\nThe optimal lower/upper quantiles satisfy the following lemmas:\nLemma 1 The optimal lower τ-quantile q∗\nτ satisﬁes:\nq∗\nτ = min{g : F ∗(g) ≥τ}\n(5)\nF ∗(g) = min\nπ F π(g)\n∀g ∈G\n(6)\n8\nQuantile RL\nand the optimal upper τ-quantile q∗\nτ satisﬁes:\nq∗\nτ = max{g : G∗(g) ≥1 −τ}\n(7)\nG∗(g) = max\nπ\nGπ(g)\n∀g ∈G\n(8)\nThen, if the optimal lower quantile (q∗\nτ) or upper quantile (q∗\nτ) were known, the problem\nwould be relatively easy to solve. By Lemma 1, an optimal policy for the lower quantile\ncould be obtained as follows:\nπ∗= arg min\nπ\nF π(q−\nτ )\n(9)\nwhere q−\nτ = g1 if q∗\nτ = g1 and q−\nτ = gi if q∗\nτ = gi+1.\nThe reason one needs to use q−\nτ\ninstead of the optimal lower quantile is that otherwise it may happen that the cumulative\ndistribution of a non-optimal policy π is smaller than or equal to that of an optimal policy\nat the lower quantile q∗\nτ (but greater below q∗\nτ as π is not optimal). In that case, the lower\nquantile of π might not be q∗\nτ. Such a thing cannot happen for the upper quantile. In that\nsense, determining an optimal policy for the upper quantile is easier. By Lemma 1, it is\nsimply given by:\nπ∗= arg max\nπ\nGπ(q∗\nτ)\n(10)\nIn practice, if the lower/upper quantiles were known, those policies could be computed by\nsolving a standard MDP with the following reward functions:\nRθ(s) =\n\n\n\n0\n∀s ̸∈G\n0\n∀s = gi ∈G, θ ≥i\n1\n∀s = gi ∈G, θ ≤i −1\nfor the lower quantile with θ = k if q−\nτ = gk and\nRθ(s) =\n\n\n\n0\n∀s ̸∈G\n1\n∀s = gi ∈G, θ ≤i\n0\n∀s = gi ∈G, θ ≥i + 1\nfor the upper quantile with θ = k if q∗\nτ = gk. Note that Rθ can be rewritten to depend on\nthe optimal lower quantile:\nRθ(s) =\n\n\n\n0\n∀s ̸∈G\n0\n∀s = gi ∈G, θ ≥i + 1\n1\n∀s = gi ∈G, θ ≤i\nwith θ = k if q∗\nτ = gk.\nSolving an MDP with Rθ amounts to minimizing the probability of ending in a ﬁnal\nstate strictly less preferred than q∗\nτ, which solves Equation 9. Similarly, solving an MDP\nwith Rθ amounts to maximizing the probability of ending in a ﬁnal state at least as preferred\nas q∗\nτ, which solves Equation 10.\nNow, the issue here is that the lower and upper quantiles are not known. We show\nin the next subsection that this problem can be overcome with a two-timescale stochastic\napproximation technique.\n9\nGilbert Weng\nData: MT = (S, A, G, P, s0)\nbegin\nInitialize θ to a random value\nfor n = 1, 2, . . . do\n4\nSolve MDP MT with reward function Rθ\nif V ∗\nθ (s0) < 1 −τ then\nθ ←−θ −1/n\nelse\nθ ←−θ + 1/n\nend\nend\nend\nAlgorithm 2: Simple strategy for ﬁnding the optimal upper quantile\n4.2. QQ-learning\nAs the lower and upper quantiles are not known, we let parameter θ vary in R+ during the\nlearning steps and we reﬁne the deﬁnition of the previous reward functions to make sure\nthey are both well-deﬁned for all θ ∈R+ and smooth in θ:\nRθ(s) =\n\n\n\n\n\n\n\n0\n∀s ̸∈G\n−1\n∀s = gi ∈G, θ ≥i + 1\n0\n∀s = gi ∈G, θ ≤i\ni −θ\nelse\nRθ(s) =\n\n\n\n\n\n\n\n0\n∀s ̸∈G\n1\n∀s = gi ∈G, θ ≤i\n0\n∀s = gi ∈G, θ ≥i + 1\ni + 1 −θ\nelse\nIn the remaining of the paper, we present how to solve for the upper quantile. A similar\napproach can be developed for the lower quantile.\nIn order to ﬁnd the optimal upper\nquantile, one could use the strategy described in Algorithm 2. Value V ∗\nθ (s0) approximates\nthe probability of reaching an end state whose index is at least as high as θ. If that value\nis smaller than 1 −τ, it means θ is too high and should be decreased. Otherwise θ is too\nsmall and should be increased. Parameter θ will then converge to the index of the optimal\nupper quantile, which is the maximal value for θ such that V ∗\nθ (s0) ≥1 −τ. The optimal\npolicy for V ∗\nθ is an optimal policy for the upper τ-quantile.\nIn a reinforcement learning setting, the solve MDP part (line 4 in Algorithm 2) could\nbe replaced by an RL algorithm such as Q-learning. The problem is that such algorithm is\nonly guaranteed to converge to the solution when n →∞. It would be therefore diﬃcult to\nintegrate Q-learning in Algorithm 2. Instead, a good policy can be learned while searching\nfor the correct value of θ. To that aim, we use a two-timescale technique (Borkar, 1997, 2008)\nin which Q-learning and the update of parameter θ are run concurrently but at diﬀerent\nspeeds (i.e., at two diﬀerent timescales). For this to work, parameter θ needs to be seen as\n10\nQuantile RL\nData: MT = (S, A, G, P, s0) with P unknown\nResult: Q\nbegin\nQ(s, a) ←−0, ∀(s, a) ∈S × A\ns ←−s0\nt ←−1\nfor n = 1 to N do\na ←−choose action\nr, s′ ←−perform action a in state s\nQt(s, a) ←−Qt(s, a) + αn(s, a)\n\u0000r + γ maxa′∈A Qt−1(s′, a′) −Qt(s, a)\n\u0001\nif V ∗\nθ (s0) < 1 −τ then\nθ ←−θ −1/n\nelse\nθ ←−θ + 1/n\nend\nif s′ ∈G then\ns ←−s0\nt ←−1\nelse\ns ←−s′\nt ←−t + 1\nend\nend\nend\nAlgorithm 3: QQ-learning for the upper-quantile\nquasi-static for the Q-learning algorithm. This is possible if the ratio of the learning rate\nof Q-learning and that of the update of θ satisﬁes:\nlim\nn→∞\nβn\nαn\n= 0\n(11)\nwhere βn = 1/n is the learning rate for parameter θ and αn is the learning rate in the Q-\nlearning algorithm. Equation 11 implies that parameter θ is changing at a slower timescale\nthan the Q function.\n5. Experimental Results\nTo demonstrate the soundness of our approach, we evaluate our algorithm on the domain,\nWho wants to be a millionaire. We present the experimental results below.\n5.1. Domain\nIn this popular television game show, a contestant needs to answer a maximum of 15\nmultiple-choice questions (with four possible answers) of increasing diﬃculty, for increas-\ningly large sums, roughly doubling the pot at each question. At each time step, the con-\ntestant may decide to walk away with the money currently won. If she answers incorrectly,\nthen all winnings are lost except what has been earned at a “guarantee point” (questions\n11\nGilbert Weng\n5 and 10). The player is allowed 3 lifelines (50:50, which removes two choices, ask the\naudience and call a friend for suggestions); each can only be used once. We used the ﬁrst\nmodel of the Spanish 2003 version of the game presented by Perea and Puerto (2007). The\nprobability of answering correctly is a function of the question’s number and increased by\nthe lifelines used (if any).\n5.2. Results\nWe plot the results (see Figure 1) obtained for two diﬀerent learning runs, one with 1 million\nlearning steps and the other with 10 million learning steps. The θ-updates were interleaved\nwith a Q-learning phase using an ε-greedy exploration strategy with ε = 0.01 and with\nlearning rate α(n) = 1/(n + 1)11/20. One can check that Equation 11 is satisﬁed.\nWe\noptimize the upper quantile with τ = 0.3. During the learning process we maintain a vector\nf of frequencies with which each ﬁnal state has been attained. We deﬁne the quantity score\nas the cross product between f and the vector of rewards obtained when attaining each ﬁnal\nstate given the current value of θ. Put another way, score is the value of the non-stationary\npolicy that has been played since the beginning of the learning process. Moreover, at each\niteration we compute V ∗\nθ (s0), the optimal value in s0 given the current value of θ.\nOn Figures 1(a) and 1(b) we observe the evolution of V ∗\nθ (s0) as the number of iterations\nincreases. We observe that V ∗\nθ (s0) converges towards 1−τ = 0.7 as the number of iterations\nincreases with oscillations of decreasing amplitude that are due to the ever changing θ value.\nFigures 1(c) and 1(d) show the evolution of the score as the number of iterations increases.\nScore converges towards a value of 0.7 but inferior.\nThis is due to the exploration of\nthe Q-learning algorithm. Lastly, 1(e) and 1(f ) plot the evolution of θ as the number of\niteration increases. The value of θ converges towards a value of 4 which the one for which\nV ∗\nθ (s0) = 1 −τ = 0.7.\n6. Conclusion\nWe have presented an algorithm for learning a policy optimal for the quantile criterion in the\nreinforcement learning setting, when the MDP has a special structure, which corresponds to\nrepeated episodic decision-making problems. It is based on stochastic approximation with\ntwo timescales (Borkar, 2008). Our proposition is experimentally validated on the domain,\nWho wants to be millionaire.\nAs future work, it would be interesting to investigate how to choose the learning rate\nαn in order to ensure a fast convergence. Moreover, our approach could be extended to\nother settings than episodic MDPs. Besides, it would also be interesting to explore whether\ngradient-based algorithms could be developed for the optimization of quantiles, based on\nthe fact that a quantile is solution of an optimization problem where the objective function\nis piecewise linear (Koenker, 2005).\n12\nQuantile RL\n(a)\n(b)\n(c)\n(d)\n(e)\n(f )\nFigure 1: Evolution of V ∗\nθ (s0), score and θ for 1 million learning steps (left) and 10 million\nlearning steps (right)\n13\nGilbert Weng\nReferences\nPieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobatics through ap-\nprenticeship learning. International Journal of Robotics Research, 29(13):1608–1639, 2010.\nR. Akrour, M. Schoenauer, and M. S´ebag. April: Active preference-learning based reinforcement\nlearning. In ECML PKDD, Lecture Notes in Computer Science, volume 7524, pages 116–131,\n2012.\nNicole B¨auerle and Jonathan Ott. Markov decision processes with average value-at-risk criteria.\nMathematical Methods of Operations Research, 74(3):361–379, 2011.\nD.F. Benoit and D. Van den Poel.\nBeneﬁts of quantile regression for the analysis of customer\nlifetime value in a contractual setting: An application in ﬁnancial services. Expert Systems with\nApplications, 36:10475–10484, 2009.\nD. Blackwell. An analog of the minimax theorem for vector payoﬀs. Paciﬁc Journal of Mathematics,\n6(1):1–8, 1956.\nV. Borkar and Rahul Jain. Risk-constrained Markov decision processes. IEEE Transactions on\nAutomatic Control, 59(9):2574–2579, 2014.\nVivek S. Borkar. Stochastic approximation : a dynamical systems viewpoint. Cambridge university\npress New Delhi, Cambridge, 2008.\nISBN 978-0-521-51592-4.\nURL http://opac.inria.fr/\nrecord=b1132816.\nV.S. Borkar. Stochastic approximation with time scales. Systems & Control Letters, 29(5):291–294,\n1997.\nM. Boussard, M. Bouzid, A.I. Mouaddib, R. Sabbadin, and P. Weng. Markov Decision Processes in\nArtiﬁcial Intelligence, chapter Non-Standard Criteria, pages 319–359. Wiley, 2010.\nV´eronique Bruy`ere, Emmanuel Filiot, Mickael Randour, and Jean-Fran¸cois Raskin.\nMeet your\nexpectations with guarantees: beyond worst-case synthesis in quantitative games. In STACS,\n2014.\nR. Busa-Fekete, B. Sz¨orenyi, P. Weng, W. Cheng, and E. H¨ullermeier. Preference-based reinforce-\nment learning. In European Workshop on Reinforcement Learning, Dagstuhl Seminar, 2013.\nRobert Busa-Fekete, Balazs Szorenyi, Paul Weng, Weiwei Cheng, and Eyke H¨ullermeier. Preference-\nbased Reinforcement Learning: Evolutionary Direct Policy Search using a Preference-based Rac-\ning Algorithm. Machine Learning, 97(3):327–351, 2014.\nAlexandra Carpentier and Michal Valko. Extreme bandits. In NIPS, 2014.\nYinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in MDPs. In NIPS,\n2014.\nG. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubra-\nmanian, P. Vosshall, and W. Vogels. Dynamo: amazon’s highly available key-value store. ACM\nSIGOPS Operating Systems Review, 41(6):205–220, 2007.\nE. Delage and S. Mannor.\nPercentile optimization in uncertain Markov decision processes with\napplication to eﬃcient exploration. In ICML, pages 225–232, 2007.\n14\nQuantile RL\nH´el`ene Fargier, Gildas Jeantet, and Olivier Spanjaard. Resolute choice in sequential decision prob-\nlems with multiple priors. In IJCAI, 2011.\nJerzy A. Filar. Percentiles and Markovian decision processes. Operations Research Letters, 2(1):\n13 – 15, 1983.\nISSN 0167-6377.\ndoi: http://dx.doi.org/10.1016/0167-6377(83)90057-3.\nURL\nhttp://www.sciencedirect.com/science/article/pii/0167637783900573.\nJerzy A. Filar, L. C. M. Kallenberg, and Huey-Miin Lee. Variance-penalized Markov decision pro-\ncesses. Mathematics of Operations Research, 14:147–161, 1989.\nP.C. Fishburn. An axiomatic characterization of skew-symmetric bilinear functionals, with applica-\ntions to utility theory. Economics Letters, 8(4):311–313, 1981.\nJ. F¨urnkranz, E. H¨ullermeier, W. Cheng, and S.H. Park. Preference-based reinforcement learning:\nA formal framework and a policy iteration algorithm. Machine Learning, 89(1):123–156, 2012.\nHugo Gilbert, Olivier Spanjaard, Paolo Viappiani, and Paul Weng. Solving MDPs with skew sym-\nmetric bilinear utility functions. In IJCAI, pages 1989–1995, 2015.\nHugo Gilbert, Bruno Zanuttini, Paolo Viappiani, Paul Weng, and Esther Nicart. Model-free rein-\nforcement learning with skew-symmetric bilinear utilities. In International Conference on Uncer-\ntainty in Artiﬁcial Intelligence (UAI), 2016.\nHugo Gimbert. Pure stationary optimal strategies in Markov decision processes. In STACS, 2007.\nJean-Yves Jaﬀray. Implementing resolute choice under uncertainty. In UAI, 1998.\nPhilippe Jorion. Value-at-Risk: The New Benchmark for Managing Financial Risk. McGraw-Hill,\n2006.\nD. Kalathil, V.S. Borkar, and R. Jain. A learning scheme for blackwell’s approachability in mdps\nand stackelberg stochastic games. Arxiv, 2014.\nR. Koenker. Quantile Regression. Cambridge university press, 2005.\nY. Liu and S. Koenig. Risk-sensitive planning with one-switch utility functions: Value iteration. In\nAAAI, pages 993–999. AAAI, 2005.\nY. Liu and S. Koenig. Functional value iteration for decision-theoretic planning with general utility\nfunctions. In AAAI, pages 1186–1193. AAAI, 2006.\nE. McClennen. Rationality and dynamic choice: Foundational explorations. Cambridge university\npress, 1990.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518:529–533, 2015.\nA.Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In ICML. Morgan Kaufmann,\n2000.\nF. Perea and J. Puerto. Dynamic programming analysis of the TV game who wants to be a million-\naire? European Journal of Operational Research, 183(2):805 – 811, 2007. ISSN 0377-2217. doi:\nhttp://dx.doi.org/10.1016/j.ejor.2006.10.041. URL http://www.sciencedirect.com/science/\narticle/pii/S0377221706010320.\n15\nGilbert Weng\nM.L. Puterman. Markov decision processes: discrete stochastic dynamic programming. Wiley, 1994.\nMickael Randour, Jean-Fran¸cois Raskin, and Ocan Sankur. Percentile queries in multi-dimensional\nMarkov decision processes. CoRR, abs/1410.4801, 2014. URL http://arxiv.org/abs/1410.\n4801.\nK. Regan and C. Boutilier. Regret based reward elicitation for Markov decision processes. In UAI,\npages 444–451. Morgan Kaufmann, 2009.\nM.J. Rostek. Quantile maximization in decision theory. Review of Economic Studies, 77(1):339–371,\n2010.\nBal´azs Sz¨or´enyi, R´obert Busa-Fekete, Paul Weng, and Eyke H¨ullermeier. Qualitative multi-armed\nbandits: A quantile-based approach. In ICML, pages 1660–1668, 2015.\nGerald Tesauro. Temporal diﬀerence learning and td-gammon. Communications of the ACM, 38(3):\n58–68, 1995.\nP. Weng. Markov decision processes with ordinal rewards: Reference point-based preferences. In\nICAPS, volume 21, pages 282–289. AAAI, 2011.\nP. Weng. Ordinal decision models for Markov decision processes. In ECAI, volume 20, pages 828–833.\nIOS Press, 2012.\nP. Weng and B. Zanuttini. Interactive value iteration for Markov decision processes with unknown\nrewards. In IJCAI, 2013.\nD. J. White. Utility, probabilistic constraints, mean and variance of discounted rewards in Markov\ndecision processes. OR Spektrum, 9:13–22, 1987.\nR. Wolski and J. Brevik. QPRED: Using quantile predictions to improve power usage for private\nclouds. Technical report, UCSB, 2014.\nJia Yuan Yu and Evdokia Nikolova.\nSample complexity of risk-averse bandit-arm selection.\nIn\nIJCAI, 2013.\nStella X. Yu, Yuanlie Lin, and Pingfan Yan. Optimization models for the ﬁrst arrival target distri-\nbution function in discrete time. Journal of mathematical analysis and applications, 225:193–223,\n1998.\nB. Zhang, Q. Cai, J. Mao, E Chang, and B. Guo. Spoken dialogue management as planning and\nacting under uncertainty. In 7th European Conference on Speech Communication and Technology,\n2001.\n16\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2016-11-03",
  "updated": "2016-11-03"
}