{
  "id": "http://arxiv.org/abs/1611.01886v4",
  "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax",
  "authors": [
    "Wentao Huang",
    "Kechen Zhang"
  ],
  "abstract": "A framework is presented for unsupervised learning of representations based\non infomax principle for large-scale neural populations. We use an asymptotic\napproximation to the Shannon's mutual information for a large neural population\nto demonstrate that a good initial approximation to the global\ninformation-theoretic optimum can be obtained by a hierarchical infomax method.\nStarting from the initial solution, an efficient algorithm based on gradient\ndescent of the final objective function is proposed to learn representations\nfrom the input datasets, and the method works for complete, overcomplete, and\nundercomplete bases. As confirmed by numerical experiments, our method is\nrobust and highly efficient for extracting salient features from input\ndatasets. Compared with the main existing methods, our algorithm has a distinct\nadvantage in both the training speed and the robustness of unsupervised\nrepresentation learning. Furthermore, the proposed method is easily extended to\nthe supervised or unsupervised model for training deep structure networks.",
  "text": "Published as a conference paper at ICLR 2017\nAN\nINFORMATION-THEORETIC\nFRAMEWORK\nFOR\nFAST AND ROBUST UNSUPERVISED LEARNING VIA\nNEURAL POPULATION INFOMAX\nWentao Huang & Kechen Zhang\nDepartment of Biomedical Engineering\nJohns Hopkins University School of Medicine\nBaltimore, MD 21205, USA\n{whuang21,kzhang4}@jhmi.edu\nABSTRACT\nA framework is presented for unsupervised learning of representations based on\ninfomax principle for large-scale neural populations. We use an asymptotic ap-\nproximation to the Shannon’s mutual information for a large neural population to\ndemonstrate that a good initial approximation to the global information-theoretic\noptimum can be obtained by a hierarchical infomax method. Starting from the\ninitial solution, an efﬁcient algorithm based on gradient descent of the ﬁnal ob-\njective function is proposed to learn representations from the input datasets, and\nthe method works for complete, overcomplete, and undercomplete bases. As con-\nﬁrmed by numerical experiments, our method is robust and highly efﬁcient for\nextracting salient features from input datasets. Compared with the main existing\nmethods, our algorithm has a distinct advantage in both the training speed and the\nrobustness of unsupervised representation learning. Furthermore, the proposed\nmethod is easily extended to the supervised or unsupervised model for training\ndeep structure networks.\n1\nINTRODUCTION\nHow to discover the unknown structures in data is a key task for machine learning. Learning good\nrepresentations from observed data is important because a clearer description may help reveal the\nunderlying structures. Representation learning has drawn considerable attention in recent years\n(Bengio et al., 2013). One category of algorithms for unsupervised learning of representations is\nbased on probabilistic models (Lewicki & Sejnowski, 2000; Hinton & Salakhutdinov, 2006; Lee\net al., 2008), such as maximum likelihood (ML) estimation, maximum a posteriori (MAP) probabil-\nity estimation, and related methods. Another category of algorithms is based on reconstruction error\nor generative criterion (Olshausen & Field, 1996; Aharon et al., 2006; Vincent et al., 2010; Mairal\net al., 2010; Goodfellow et al., 2014), and the objective functions usually involve squared errors with\nadditional constraints. Sometimes the reconstruction error or generative criterion may also have a\nprobabilistic interpretation (Olshausen & Field, 1997; Vincent et al., 2010).\nShannon’s information theory is a powerful tool for description of stochastic systems and could\nbe utilized to provide a characterization for good representations (Vincent et al., 2010). However,\ncomputational difﬁculties associated with Shannon’s mutual information (MI) (Shannon, 1948) have\nhindered its wider applications. The Monte Carlo (MC) sampling (Yarrow et al., 2012) is a conver-\ngent method for estimating MI with arbitrary accuracy, but its computational inefﬁciency makes it\nunsuitable for difﬁcult optimization problems especially in the cases of high-dimensional input stim-\nuli and large population networks. Bell and Sejnowski (Bell & Sejnowski, 1995; 1997) have directly\napplied the infomax approach (Linsker, 1988) to independent component analysis (ICA) of data with\nindependent non-Gaussian components assuming additive noise, but their method requires that the\nnumber of outputs be equal to the number of inputs. The extensions of ICA to overcomplete or\nundercomplete bases incur increased algorithm complexity and difﬁculty in learning of parameters\n(Lewicki & Sejnowski, 2000; Kreutz-Delgado et al., 2003; Karklin & Simoncelli, 2011).\n1\narXiv:1611.01886v4  [cs.LG]  10 Mar 2017\nPublished as a conference paper at ICLR 2017\nSince Shannon MI is closely related to ML and MAP (Huang & Zhang, 2016), the algorithms of\nrepresentation learning based on probabilistic models should be amenable to information-theoretic\ntreatment. Representation learning based on reconstruction error could be accommodated also by\ninformation theory, because the inverse of Fisher information (FI) is the Cram´er-Rao lower bound\non the mean square decoding error of any unbiased decoder (Rao, 1945). Hence minimizing the\nreconstruction error potentially maximizes a lower bound on the MI (Vincent et al., 2010).\nRelated problems arise also in neuroscience. It has long been suggested that the real nervous sys-\ntems might approach an information-theoretic optimum for neural coding and computation (Barlow,\n1961; Atick, 1992; Borst & Theunissen, 1999). However, in the cerebral cortex, the number of neu-\nrons is huge, with about 105 neurons under a square millimeter of cortical surface (Carlo & Stevens,\n2013). It has often been computationally intractable to precisely characterize information coding\nand processing in large neural populations.\nTo address all these issues, we present a framework for unsupervised learning of representations\nin a large-scale nonlinear feedforward model based on infomax principle with realistic biological\nconstraints such as neuron models with Poisson spikes. First we adopt an objective function based\non an asymptotic formula in the large population limit for the MI between the stimuli and the neural\npopulation responses (Huang & Zhang, 2016). Since the objective function is usually nonconvex,\nchoosing a good initial value is very important for its optimization. Starting from an initial value, we\nuse a hierarchical infomax approach to quickly ﬁnd a tentative global optimal solution for each layer\nby analytic methods. Finally, a fast convergence learning rule is used for optimizing the ﬁnal objec-\ntive function based on the tentative optimal solution. Our algorithm is robust and can learn complete,\novercomplete or undercomplete basis vectors quickly from different datasets. Experimental results\nshowed that the convergence rate of our method was signiﬁcantly faster than other existing methods,\noften by an order of magnitude. More importantly, the number of output units processed by our\nmethod can be very large, much larger than the number of inputs. As far as we know, no existing\nmodel can easily deal with this situation.\n2\nMETHODS\n2.1\nAPPROXIMATION OF MUTUAL INFORMATION FOR NEURAL POPULATIONS\nSuppose the input x is a K-dimensional vector, x = (x1, · · · , xK)T , the outputs of N neurons are\ndenoted by a vector, r = (r1, · · · , rN)T , where we assume N is large, generally N ≫K. We\ndenote random variables by upper case letters, e.g., random variables X and R, in contrast to their\nvector values x and r. The MI between X and R is deﬁned by I(X; R) =\nD\nln p(x|r)\np(x)\nE\nr,x, where\n⟨·⟩r,x denotes the expectation with respect to the probability density function (PDF) p(r, x).\nOur goal is to maxmize MI I(X; R) by ﬁnding the optimal PDF p(r|x) under some constraint\nconditions, assuming that p(r|x) is characterized by a noise model and activation functions f(x; θn)\nwith parameters θn for the n-th neuron (n = 1, · · · , N). In other words, we optimize p(r|x) by\nsolving for the optimal parameters θn. Unfortunately, it is intractable in most cases to solve for the\noptimal parameters that maximizes I(X; R). However, if p(x) and p(r|x) are twice continuously\ndifferentiable for almost every x ∈RK, then for large N we can use an asymptotic formula to\napproximate the true value of I(X; R) with high accuracy (Huang & Zhang, 2016):\nI(X; R) ≃IG = 1\n2\n\u001c\nln\n\u0012\ndet\n\u0012G(x)\n2πe\n\u0013\u0013\u001d\nx\n+ H(X),\n(1)\nwhere det (·) denotes the matrix determinant and H(X) = −⟨ln p(x)⟩x is the stimulus entropy,\nG(x) = J(x) + P (x) ,\n(2)\nJ(x) = −\n\u001c∂2 ln p (r|x)\n∂x∂xT\n\u001d\nr|x\n,\n(3)\nP(x) = −∂2 ln p (x)\n∂x∂xT\n.\n(4)\nAssuming independent noises in neuronal responses, we have p(r|x) = QN\nn=1 p(rn|x; θn),\nand the Fisher information matrix becomes J(x) ≈N PK1\nk=1 αkS(x; θk), where S(x; θk) =\n2\nPublished as a conference paper at ICLR 2017\nD\n∂ln p(r|x;θk)\n∂x\n∂ln p(r|x;θk)\n∂xT\nE\nr|x and αk > 0 (k = 1, · · · , K1) is the population density of param-\neter θk, with PK1\nk=1 αk = 1, and 1 ≤K1 ≤N (see Appendix A.1 for details). Since the cerebral\ncortex usually forms functional column structures and each column is composed of neurons with the\nsame properties (Hubel & Wiesel, 1962), the positive integer K1 can be regarded as the number of\ndistinct classes in the neural population.\nTherefore, given the activation function f(x; θk), our goal becomes to ﬁnd the optimal popula-\ntion distribution density αk of parameter vector θk so that the MI between the stimulus x and the\nresponse r is maximized. By Eq. (1), our optimization problem can be stated as follows:\nminimize QG[{αk}] = −1\n2 ⟨ln (det (G(x)))⟩x ,\n(5)\nsubject to\nK1\nX\nk=1\nαk = 1, αk > 0, ∀k = 1, · · · , K1.\n(6)\nSince QG[{αk}] is a convex function of {αk} (Huang & Zhang, 2016), we can readily ﬁnd the\noptimal solution for small K by efﬁcient numerical methods. For large K, however, ﬁnding an\noptimal solution by numerical methods becomes intractable. In the following we will propose an\nalternative approach to this problem. Instead of directly solving for the density distribution {αk}, we\noptimize the parameters {αk} and {θk} simultaneously under a hierarchical infomax framework.\n2.2\nHIERARCHICAL INFOMAX\nFor clarity, we consider neuron model with Poisson spikes although our method is easily applicable\nto other noise models. The activation function f(x; θn) is generally a nonlinear function, such as\nsigmoid and rectiﬁed linear unit (ReLU) (Nair & Hinton, 2010). We assume that the nonlinear\nfunction for the n-th neuron has the following form: f(x; θn) = ˜f(yn; ˜θn), where\nyn = wT\nn x.\n(7)\nwith wn being a K-dimensional weights vector, ˜f(yn; ˜θn) is a nonlinear function, θn = (wT\nn , ˜θ\nT\nn)T\nand ˜θn are the parameter vectors (n = 1, · · · , N).\nIn general, it is very difﬁcult to ﬁnd the optimal parameters, θn, n = 1, · · · , N, for the following\nreasons. First, the number of output neurons N is very large, usually N ≫K. Second, the activation\nfunction f(x; θn) is a nonlinear function, which usually leads to a nonconvex optimization problem.\nFor nonconvex optimization problems, the selection of initial values often has a great inﬂuence on\nthe ﬁnal optimization results. Our approach meets these challenges by making better use of the large\nnumber of neurons and by ﬁnding good initial values by a hierarchical infomax method.\nWe divide the nonlinear transformation into two stages, mapping ﬁrst from x to yn (n = 1, · · · , N),\nand then from yn to ˜f(yn; ˜θn), where yn can be regarded as the membrane potential of the n-th\nneuron, and ˜f(yn; ˜θn) as its ﬁring rate. As with the real neurons, we assume that the membrane\npotential is corrupted by noise:\n˘Yn = Yn + Zn,\n(8)\nwhere Zn ∼N\n\u00000, σ2\u0001\nis a normal distribution with mean 0 and variance σ2. Then the mean\nmembrane potential of the k-th class subpopulation with Nk = Nαk neurons is given by\n¯Yk = 1\nNk\nNk\nX\nn=1\n˘Ykn = Yk + ¯Zk, k = 1, · · · , K1,\n(9)\n¯Zk ∼N(0, N −1\nk σ2).\n(10)\nDeﬁne vectors ˘y = (˘y1, · · · , ˘yN)T , ¯y = (¯y1, · · · , ¯yK1)T and y = (y1, · · · , yK1)T , where yk =\nwT\nk x (k = 1, · · · , K1). Notice that ˘yn (n = 1, · · · , N) is also divided into K1 classes, the same\nas for rn. If we assume f(x; θk) = ˜f(¯yk; ˜θk), i.e. assuming an additive Gaussian noise for yn\n(see Eq. 9), then the random variables X, Y , ˘Y , ¯Y and R form a Markov chain, denoted by\nX →Y →˘Y →¯Y →R (see Figure 1), and we have the following proposition (see Appendix\nA.2).\n3\nPublished as a conference paper at ICLR 2017\nX\nY\nR\nY\nY\nW X\nY + Z\n(\nT\n1/Nk\nf(    )\nY\nx\nx\nx\ny-\ny-\ny-\nym1\n(\nymNk\n(\nymNk\nK1\nrmNk\nymi\n(\nrN1\nyN1\n(\nyN\n(\nrN\nyN\nyni\n(\nyn1\n(\nyn1\nyni\nymi\nym1\nyN1\nri\nyi\ny1\nyi\n(\ny1\n(\nrni\nrn1\nrmi\nrm1\nr1\n1\nk\nK\nk\n1\nFigure 1: A neural network interpretaton for random variables X, Y , ˘Y , ¯Y , R.\nProposition 1. With the random variables X, Y , ˘Y , ¯Y , R and Markov chain X →Y →˘Y →\n¯Y →R, the following equations hold,\nI(X; R) = I(Y ; R) ≤I( ˘Y ; R) ≤I( ¯Y ; R),\n(11)\nI(X; R) ≤I(X; ¯Y ) = I(X; ˘Y ) ≤I(X; Y ),\n(12)\nand for large Nk (k = 1, · · · , K1),\nI( ˘Y ; R) ≃I( ¯Y ; R) ≃I(Y ; R) = I(X; R),\n(13)\nI(X; Y ) ≃I(X; ¯Y ) = I(X; ˘Y ).\n(14)\nA major advantage of incorporating membrane noise is that it facilitates ﬁnding the optimal solution\nby using the infomax principle. Moreover, the optimal solution obtained this way is more robust;\nthat is, it discourages overﬁtting and has a strong ability to resist distortion. With vanishing noise\nσ2 →0, we have ¯Yk →Yk, ˜f(¯yk; ˜θk) ≃˜f(yk; ˜θk) = f(x; θk), so that Eqs. (13) and (14) hold as\nin the case of large Nk.\nTo optimize MI I(Y ; R), the probability distribution of random variable Y , p(y), needs to be de-\ntermined, i.e. maximizing I(Y ; R) about p(y) under some constraints should yield an optimal\ndistribution: p∗(y) = arg maxp(y) I(Y ; R). Let C = maxp(y) I (Y ; R) be the channel capacity of\nneural population coding, and we always have I(X; R) ≤C (Huang & Zhang, 2016). To ﬁnd a\nsuitable linear transformation from X to Y that is compatible with this distribution p∗(y), a reason-\nable choice is to maximize I(X; ˘Y ) (≤I(X; Y )), where ˘Y is a noise-corrupted version of Y . This\nimplies minimum information loss in the ﬁrst transformation step. However, there may exist many\ntransformations from X to ˘Y that maximize I(X; ˘Y ) (see Appendix A.3.1). Ideally, if we can ﬁnd\na transformation that maximizes both I(X; ˘Y ) and I(Y ; R) simultaneously, then I(X; R) reaches\nits maximum value: I(X; R) = maxp(y) I (Y ; R) = C.\nFrom the discussion above we see that maximizing I(X; R) can be divided into two steps,\nnamely, maximizing I(X; ˘Y ) and maximizing I(Y ; R). The optimal solutions of max I(X; ˘Y )\nand max I(Y ; R) will provide a good initial approximation that tend to be very close to the optimal\nsolution of max I(X; R).\nSimilarly, we can extend this method to multilayer neural population networks. For example, a two-\nlayer network with outputs R(1) and R(2) form a Markov chain, X →˜R(1) →R(1) →¯R(1) →\n4\nPublished as a conference paper at ICLR 2017\nR(2), where random variable ˜R(1) is similar to Y , random variable R(1) is similar to ˘Y , and ¯R(1)\nis similar to ¯Y in the above. Then we can show that the optimal solution of max I(X; R(2)) can\nbe approximated by the solutions of max I(X; R(1)) and max I( ˜R(1); R(2)), with I( ˜R(1); R(2)) ≃\nI( ¯R(1); R(2)).\nMore generally, consider a highly nonlinear feedforward neural network that maps the input x to\noutput z, with z = F(x;θ) = hL ◦· · · ◦h1 (x), where hl (l = 1, · · · , L) is a linear or nonlinear\nfunction (Montufar et al., 2014). We aim to ﬁnd the optimal parameter θ by maximizing I (X; Z). It\nis usually difﬁcult to solve the optimization problem when there are many local extrema for F(x;θ).\nHowever, if each function hl is easy to optimize, then we can use the hierarchical infomax method\ndescribed above to get a good initial approximation to its global optimization solution, and go from\nthere to ﬁnd the ﬁnal optimal solution. This information-theoretic consideration from the neural\npopulation coding point of view may help explain why deep structure networks with unsupervised\npre-training have a powerful ability for learning representations.\n2.3\nTHE OBJECTIVE FUNCTION\nThe optimization processes for maximizing I(X; ˘Y ) and maximizing I(Y ; R) are discussed in detail\nin Appendix A.3. First, by maximizing I(X; ˘Y ) (see Appendix A.3.1 for details), we can get the\noptimal weight parameter wk (k = 1, · · · , K1, see Eq. 7) and its population density αk (see Eq. 6)\nwhich satisfy\nW = [w1, · · · , wK1] = aU0Σ−1/2\n0\nC,\n(15)\nα1 = · · · = αK1 = K−1\n1 ,\n(16)\nwhere a =\nq\nK1K−1\n0 , C = [c1, · · · , cK1] ∈RK0×K1, CCT = IK0, IK0 is a K0 × K0 identity\nmatrix with integer K0 ∈[1, K], the diagonal matrix Σ0 ∈RK0×K0 and matrix U0 ∈RK×K0 are\ngiven in (A.44) and (A.45), with K0 given by Eq. (A.52). Matrices Σ0 and U0 can be obtained\nby Σ and U with UT\n0 U0 = IK0 and U0Σ0UT\n0 ≈UΣUT ≈\n\nxxT \u000b\nx (see Eq. A.23). The\noptimal weight parameter wk (15) means that the input variable x must ﬁrst undergo a whitening-\nlike transformation ˆx = Σ−1/2\n0\nUT\n0 x, and then goes through the transformation y = aCT ˆx, with\nmatrix C to be optimized below. Note that weight matrix W satisﬁes rank(W) = min(K0, K1),\nwhich is a low rank matrix, and its low dimensionality helps reduce overﬁtting during training (see\nAppendix A.3.1).\nBy maximizing I (Y ; R) (see Appendix A.3.2), we further solve the the optimal parameters ˜θk for\nthe nonlinear functions ˜f(yk; ˜θk), k = 1, · · · , K1. Finally, the objective function for our optimiza-\ntion problem (Eqs. 5 and 6) turns into (see Appendix A.3.3 for details):\nminimize Q [C] = −1\n2\nD\nln\n\u0010\ndet\n\u0010\nCˆΦC\nT \u0011\u0011E\nˆx ,\n(17)\nsubject to CCT = IK0,\n(18)\nwhere ˆΦ = diag\n\u0000φ(ˆy1)2, · · · , φ(ˆyK1)2\u0001\n, φ(ˆyk) = a−1 |∂gk(ˆyk)/∂ˆyk| (k = 1, · · · , K1), gk(ˆyk) =\n2\nq\n˜f(ˆyk; ˜θk), ˆyk = a−1yk = cT\nk ˆx, and ˆx = Σ−1/2\n0\nUT\n0 x. We apply the gradient descent method to\noptimize the objective function, with the gradient of Q[C] given by:\ndQ[C]\ndC\n= −\n\u001c\u0010\nCˆΦC\nT \u0011−1\nCˆΦ + ˆxωT\n\u001d\nˆx\n,\n(19)\nwhere ω = (ω1, · · · , ωK1)T , ωk = φ(ˆyk)φ′(ˆyk)cT\nk\n\u0010\nCˆΦC\nT \u0011−1\nck, k = 1, · · · , K1.\nWhen K0 = K1 (or K0 > K1), the objective function Q[C] can be reduced to a simpler form,\nand its gradient is also easy to compute (see Appendix A.4.1). However, when K0 < K1, it is\ncomputationally expensive to update C by applying the gradient of Q[C] directly, since it requires\nmatrix inversion for every ˆx. We use another objective function ˆQ[C] (see Eq. A.118) which is an\napproximation to Q[C], but its gradient is easier to compute (see Appendix A.4.2). The function\n5\nPublished as a conference paper at ICLR 2017\nˆQ[C] is the approximation of Q[C], ideally they have the same optimal solution for the parameter\nC.\nUsually, for optimizing the objective in Eq. 17, the orthogonality constraint (Eq. 18) is unnecessary.\nHowever, this orthogonality constraint can accelerate the convergence rate if we employ it for the\ninitial iteration to update C (see Appendix A.5).\n3\nEXPERIMENTAL RESULTS\nWe have applied our methods to the natural images from Olshausen’s image dataset (Olshausen &\nField, 1996) and the images of handwritten digits from MNIST dataset (LeCun et al., 1998) using\nMatlab 2016a on a computer with 12 Intel CPU cores (2.4 GHz). The gray level of each raw image\nwas normalized to the range of 0 to 1. M image patches with size w × w = K for training were\nrandomly sampled from the images. We used the Poisson neuron model with a modiﬁed sigmoidal\ntuning function ˜f(y; ˜θ) =\n1\n4(1+exp(−βy−b))2 , with g(y) = 2\nq\n˜f(y; ˜θ) =\n1\n1+exp(−βy−b), where\n˜θ = (β, b)T . We obtained the initial values (see Appendix A.3.2): b0 = 0 and β0 ≈1.81\nq\nK1K−1\n0 .\nFor our experiments, we set β = 0.5β0 for iteration epoch t = 1, · · · , t0 and β = β0 for t =\nt0 + 1, · · · , tmax, where t0 = 50.\nFirstly, we tested the case of K = K0 = K1 = 144 and randomly sampled M = 105 image patches\nwith size 12×12 from the Olshausen’s natural images, assuming that N = 106 neurons were divided\ninto K1 = 144 classes and ϵ = 1 (see Eq. A.52 in Appendix). The input patches were preprocessed\nby the ZCA whitening ﬁlters (see Eq. A.68). To test our algorithms, we chose the batch size to be\nequal to the number of training samples M, although we could also choose a smaller batch size. We\nupdated the matrix C from a random start, and set parameters tmax = 300, v1 = 0.4, and τ = 0.8\nfor all experiments.\nIn this case, the optimal solution C looked similar to the optimal solution of IICA (Bell & Sejnowski,\n1997). We also compared with the fast ICA algorithm (FICA) (Hyv¨arinen, 1999), which is faster\nthan IICA. We also tested the restricted Boltzmann machine (RBM) (Hinton et al., 2006) for a\nunsupervised learning of representations, and found that it could not easily learn Gabor-like ﬁlters\nfrom Olshausen’s image dataset as trained by contrastive divergence. However, an improved method\nby adding a sparsity constraint on the output units, e.g., sparse RBM (SRBM) (Lee et al., 2008) or\nsparse autoencoder (Hinton, 2010), could attain Gabor-like ﬁlters from this dataset. Similar results\nwith Gabor-like ﬁlters were also reproduced by the denoising autoencoders (Vincent et al., 2010),\nwhich method requires a careful choice of parameters, such as noise level, learning rate, and batch\nsize.\nIn order to compare our methods, i.e. Algorithm 1 (Alg.1, see Appendix A.4.1) and Algorithm\n2 (Alg.2, see Appendix A.4.2), with other methods, i.e. IICA, FICA and SRBM, we implemented\nthese algorithms using the same initial weights and the same training data set (i.e. 105 image patches\npreprocessed by the ZCA whitening ﬁlters). To get a good result by IICA, we must carefully select\nthe parameters; we set the batch size as 50, the initial learning rate as 0.01, and ﬁnal learning rate\nas 0.0001, with an exponential decay with the epoch of iterations. IICA tends to have a faster\nconvergence rate for a bigger batch size but it may become harder to escape local minima. For\nFICA, we chose the nonlinearity function f(u) = log cosh(u) as contrast function, and for SRBM,\nwe set the sparseness control constant p as 0.01 and 0.03. The number of epoches for iterations was\nset to 300 for all algorithms. Figure 2 shows the ﬁlters learned by our methods and other methods.\nEach ﬁlter in Figure 2(a) corresponds to a column vector of matrix ˇC (see Eq. A.69), where each\nvector for display is normalized by ˇck ←ˇck/ max(|ˇc1,k|, · · · , |ˇcK,k|), k = 1, · · · , K1. The results\nin Figures 2(a), 2(b) and 2(c) look very similar to one another, and slightly different from the results\nin Figure 2(d) and 2(e). There are no Gabor-like ﬁlters in Figure 2(f), which corresponds to SRBM\nwith p = 0.03.\nFigure 3 shows how the coefﬁcient entropy (CFE) (see Eq. A.122) and the conditional entropy\n(CDE) (see Eq. A.125) varied with training time. We calculated CFE and CDE by sampling once\nevery 10 epoches from a total of 300 epoches. These results show that our algorithms had a fast\nconvergence rate towards stable solutions while having CFE and CDE values similar to the algorithm\nof IICA, which converged much more slowly. Here the values of CFE and CDE should be as small\n6\nPublished as a conference paper at ICLR 2017\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 2: Comparison of ﬁlters obtained from 105 natural image patches of size 12×12 by our\nmethods (Alg.1 and Alg.2) and other methods. The number of output ﬁlters was K1 = 144. (a):\nAlg.1. (b): Alg.2. (c): IICA. (d): FICA. (e): SRBM (p = 0.01). (f): SRBM (p = 0.03).\n100\n101\n102\ntime (seconds)\n1.8\n1.85\n1.9\n1.95\n2\ncoefficient entropy (bits)\nAlg.1\nAlg.2\nIICA\nFICA\nSRBM (p = 0.01)\nSRBM (p = 0.03)\n(a)\n100\n101\n102\ntime (seconds)\n-400\n-350\n-300\n-250\n-200\n-150\nconditional entropy (bits)\nAlg.1\nAlg.2\nIICA\n(b)\n100\n101\n102\ntime (seconds)\n-200\n-100\n0\n100\n200\n300\nconditional entropy (bits)\nSRBM (p = 0.01)\nSRBM (p = 0.03)\nSRBM (p = 0.05)\nSRBM (p = 0.10)\n(c)\nFigure 3: Comparison of quantization effects and convergence rate by coefﬁcient entropy (see\nA.122) and conditional entropy (see A.125) corresponding to training results (ﬁlters) shown in Fig-\nure 2. The coefﬁcient entropy (panel a) and conditional entropy (panel b and c) are shown as a\nfunction of training time on a logarithmic scale. All experiments run on the same machine using\nMatlab. Here we sampled once every 10 epoches out of a total of 300 epoches. We set epoch number\nt0 = 50 for Alg.1 and Alg.2 and the start time to 1 second.\nas possible for a good representation learned from the same data set. Here we set epoch number\nt0 = 50 in our algorithms (see Alg.1 and Alg.2), and the start time was set to 1 second. This\nexplains the step seen in Figure 3 (b) for Alg.1 and Alg.2 since the parameter β was updated when\nepoch number t = t0. FICA had a convergence rate close to our algorithms but had a big CFE,\nwhich is reﬂected by the quality of the ﬁlter results in Figure 2. The convergence rate and CFE for\nSRBM were close to IICA, but SRBM had a much bigger CDE than IICA, which implies that the\ninformation had a greater loss when passing through the system optimized by SRBM than by IICA\nor our methods.\n7\nPublished as a conference paper at ICLR 2017\nFrom Figure 3(c) we see that the CDE (or MI I(X; R), see Eq. A.124 and A.125) decreases (or\nincreases) with the increase of the value of the sparseness control constant p. Note that a smaller\np means sparser outputs. Hence, in this sense, increasing sparsity may result in sacriﬁcing some\ninformation. On the other hand, a weak sparsity constraint may lead to failure of learning Gabor-\nlike ﬁlters (see Figure 2(f)), and increasing sparsity has an advantage in reducing the impact of\nnoise in many practical cases. Similar situation also occurs in sparse coding (Olshausen & Field,\n1997), which provides a class of algorithms for learning overcomplete dictionary representations of\nthe input signals. However, its training is time consuming due to its expensive computational cost,\nalthough many new training algorithms have emerged (e.g. Aharon et al., 2006; Elad & Aharon,\n2006; Lee et al., 2006; Mairal et al., 2010). See Appendix A.5 for additional experimental results.\n4\nCONCLUSIONS\nIn this paper, we have presented a framework for unsupervised learning of representations via in-\nformation maximization for neural populations. Information theory is a powerful tool for machine\nlearning and it also provides a benchmark of optimization principle for neural information pro-\ncessing in nervous systems. Our framework is based on an asymptotic approximation to MI for a\nlarge-scale neural population. To optimize the infomax objective, we ﬁrst use hierarchical infomax\nto obtain a good approximation to the global optimal solution. Analytical solutions of the hierarchi-\ncal infomax are further improved by a fast convergence algorithm based on gradient descent. This\nmethod allows us to optimize highly nonlinear neural networks via hierarchical optimization using\ninfomax principle.\nFrom the viewpoint of information theory, the unsupervised pre-training for deep learning (Hinton &\nSalakhutdinov, 2006; Bengio et al., 2007) may be reinterpreted as a process of hierarchical infomax,\nwhich might help explain why unsupervised pre-training helps deep learning (Erhan et al., 2010). In\nour framework, a pre-whitening step can emerge naturally by the hierarchical infomax, which might\nalso explain why a pre-whitening step is useful for training in many learning algorithms (Coates\net al., 2011; Bengio, 2012).\nOur model naturally incorporates a considerable degree of biological realism. It allows the opti-\nmization of a large-scale neural population with noisy spiking neurons while taking into account of\nmultiple biological constraints, such as membrane noise, limited energy, and bounded connection\nweights. We employ a technique to attain a low-rank weight matrix for optimization, so as to reduce\nthe inﬂuence of noise and discourage overﬁtting during training. In our model, many parameters\nare optimized, including the population density of parameters, ﬁlter weight vectors, and parameters\nfor nonlinear tuning functions. Optimizing all these model parameters could not be easily done by\nmany other methods.\nOur experimental results suggest that our method for unsupervised learning of representations has\nobvious advantages in its training speed and robustness over the main existing methods. Our model\nhas a nonlinear feedforward structure and is convenient for fast learning and inference. This simple\nand ﬂexible framework for unsupervised learning of presentations should be readily extended to\ntraining deep structure networks. In future work, it would interesting to use our method to train deep\nstructure networks with either unsupervised or supervised learning.\nACKNOWLEDGMENTS\nWe thank Prof. Honglak Lee for sharing Matlab code for algorithm comparison, Prof. Shan Tan for\ndiscussions and comments and Kai Liu for helping draw Figure 1. Supported by grant NIH-NIDCD\nR01 DC013698.\nREFERENCES\nAharon, M., Elad, M., & Bruckstein, A. (2006). K-SVD: An algorithm for designing overcomplete\ndictionaries for sparse representation. Signal Processing, IEEE Transactions on, 54(11), 4311–\n4322.\n8\nPublished as a conference paper at ICLR 2017\nAmari, S. (1999). Natural gradient learning for over- and under-complete bases in ica. Neural\nComput., 11(8), 1875–1883.\nAtick, J. J. (1992). Could information theory provide an ecological theory of sensory processing?\nNetwork: Comp. Neural., 3(2), 213–251.\nBarlow, H. B. (1961). Possible principles underlying the transformation of sensory messages. Sen-\nsory Communication, (pp. 217–234).\nBell, A. J. & Sejnowski, T. J. (1995). An information-maximization approach to blind separation\nand blind deconvolution. Neural Comput., 7(6), 1129–1159.\nBell, A. J. & Sejnowski, T. J. (1997). The ”independent components” of natural scenes are edge\nﬁlters. Vision Res., 37(23), 3327–3338.\nBengio, Y. (2012). Deep learning of representations for unsupervised and transfer learning. Unsu-\npervised and Transfer Learning Challenges in Machine Learning, 7, 19.\nBengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new per-\nspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8), 1798–1828.\nBengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al. (2007). Greedy layer-wise training of\ndeep networks. Advances in neural information processing systems, 19, 153.\nBorst, A. & Theunissen, F. E. (1999). Information theory and neural coding. Nature neuroscience,\n2(11), 947–957.\nCarlo, C. N. & Stevens, C. F. (2013). Structural uniformity of neocortex, revisited. Proceedings of\nthe National Academy of Sciences, 110(4), 1488–1493.\nCoates, A., Ng, A. Y., & Lee, H. (2011). An analysis of single-layer networks in unsupervised\nfeature learning. In International conference on artiﬁcial intelligence and statistics (pp. 215–\n223).\nCortes, C. & Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3), 273–297.\nCover, T. M. & Thomas, J. A. (2006). Elements of Information, 2nd Edition. New York: Wiley-\nInterscience.\nEdelman, A., Arias, T. A., & Smith, S. T. (1998). The geometry of algorithms with orthogonality\nconstraints. SIAM J. Matrix Anal. Appl., 20(2), 303–353.\nElad, M. & Aharon, M. (2006). Image denoising via sparse and redundant representations over\nlearned dictionaries. Image Processing, IEEE Transactions on, 15(12), 3736–3745.\nErhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., & Bengio, S. (2010). Why does\nunsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11,\n625–660.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &\nBengio, Y. (2014). Generative adversarial nets. In Advances in Neural Information Processing\nSystems (pp. 2672–2680).\nHinton, G. (2010). A practical guide to training restricted boltzmann machines. Momentum, 9(1),\n926.\nHinton, G., Osindero, S., & Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural\ncomputation, 18(7), 1527–1554.\nHinton, G. E. & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786), 504–507.\nHuang, W. & Zhang, K. (2016). Information-theoretic bounds and approximations in neural popu-\nlation coding. Neural Comput, submitted, URL https://arxiv.org/abs/1611.01414.\n9\nPublished as a conference paper at ICLR 2017\nHubel, D. H. & Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction and functional archi-\ntecture in the cat’s visual cortex. The Journal of physiology, 160(1), 106–154.\nHyv¨arinen, A. (1999). Fast and robust ﬁxed-point algorithms for independent component analysis.\nNeural Networks, IEEE Transactions on, 10(3), 626–634.\nKarklin, Y. & Simoncelli, E. P. (2011). Efﬁcient coding of natural images with a population of noisy\nlinear-nonlinear neurons. In Advances in neural information processing systems, volume 24 (pp.\n999–1007).\nKonstantinides, K. & Yao, K. (1988). Statistical analysis of effective singular values in matrix rank\ndetermination. Acoustics, Speech and Signal Processing, IEEE Transactions on, 36(5), 757–763.\nKreutz-Delgado, K., Murray, J. F., Rao, B. D., Engan, K., Lee, T. S., & Sejnowski, T. J. (2003).\nDictionary learning algorithms for sparse representation. Neural computation, 15(2), 349–396.\nLeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to docu-\nment recognition. Proceedings of the IEEE, 86(11), 2278–2324.\nLee, H., Battle, A., Raina, R., & Ng, A. Y. (2006). Efﬁcient sparse coding algorithms. In Advances\nin neural information processing systems (pp. 801–808).\nLee, H., Ekanadham, C., & Ng, A. Y. (2008). Sparse deep belief net model for visual area v2. In\nAdvances in neural information processing systems (pp. 873–880).\nLewicki, M. S. & Olshausen, B. A. (1999). Probabilistic framework for the adaptation and compar-\nison of image codes. JOSA A, 16(7), 1587–1601.\nLewicki, M. S. & Sejnowski, T. J. (2000). Learning overcomplete representations. Neural compu-\ntation, 12(2), 337–365.\nLinsker, R. (1988). Self-Organization in a perceptual network. Computer, 21(3), 105–117.\nMairal, J., Bach, F., Ponce, J., & Sapiro, G. (2009). Online dictionary learning for sparse coding.\nIn Proceedings of the 26th annual international conference on machine learning (pp. 689–696).:\nACM.\nMairal, J., Bach, F., Ponce, J., & Sapiro, G. (2010). Online learning for matrix factorization and\nsparse coding. The Journal of Machine Learning Research, 11, 19–60.\nMontufar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the number of linear regions of deep\nneural networks. In Advances in Neural Information Processing Systems (pp. 2924–2932).\nNair, V. & Hinton, G. E. (2010). Rectiﬁed linear units improve restricted boltzmann machines. In\nProceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807–\n814).\nOlshausen, B. A. & Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties by\nlearning a sparse code for natural images. Nature, 381(6583), 607–609.\nOlshausen, B. A. & Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy\nemployed by v1? Vision Res., 37(23), 3311–3325.\nRao, C. R. (1945). Information and accuracy attainable in the estimation of statistical parameters.\nBulletin of the Calcutta Mathematical Society, 37(3), 81–91.\nShannon, C. (1948). A mathematical theory of communications. Bell System Technical Journal, 27,\n379–423 and 623–656.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout:\nA simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning\nResearch, 15(1), 1929–1958.\n10\nPublished as a conference paper at ICLR 2017\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P.-A. (2010). Stacked denoising\nautoencoders: Learning useful representations in a deep network with a local denoising criterion.\nThe Journal of Machine Learning Research, 11, 3371–3408.\nYarrow, S., Challis, E., & Series, P. (2012). Fisher and shannon information in ﬁnite neural popula-\ntions. Neural computation, 24(7), 1740–1780.\nAPPENDIX\nA.1\nFORMULAS FOR APPROXIMATION OF MUTUAL INFORMATION\nIt follows from I(X; R) =\nD\nln p(x|r)\np(x)\nE\nr,x and Eq. (1) that the conditional entropy should read:\nH(X|R) = −⟨ln p(x|r)⟩r,x ≃−1\n2\n\u001c\nln\n\u0012\ndet\n\u0012G(x)\n2πe\n\u0013\u0013\u001d\nx\n.\n(A.1)\nThe Fisher information matrix J(x) (see Eq. 3), which is symmetric and positive semideﬁnite, can\nbe written also as\nJ(x) =\n\u001c∂ln p(r|x)\n∂x\n∂ln p(r|x)\n∂xT\n\u001d\nr|x\n.\n(A.2)\nIf we suppose p(r|x) is conditional independent, namely, p(r|x) = QN\nn=1 p(rn|x; θn), then we\nhave (see Huang & Zhang, 2016)\nJ(x) = N\nZ\nΘ\np(θ)S(x; θ)dθ,\n(A.3)\nS(x; θ) =\n\u001c∂ln p(r|x; θ)\n∂x\n∂ln p(r|x; θ)\n∂xT\n\u001d\nr|x\n,\n(A.4)\nwhere p(θ) is the population density function of parameter θ,\np(θ) = 1\nN\nN\nX\nn=1\nδ(θ −θn),\n(A.5)\nand δ(·) denotes the Dirac delta function. It can be proved that the approximation function of MI\nIG[p(θ)] (Eq. 1) is concave about p(θ) (Huang & Zhang, 2016). In Eq. (A.3), we can approximate\nthe continuous integral by a discrete summation for numerical computation,\nJ(x) ≈N\nK1\nX\nk=1\nαkS(x; θk),\n(A.6)\nwhere PK1\nk=1 αk = 1, αk > 0, k = 1, · · · , K1, 1 ≤K1 ≤N.\nFor Poisson neuron model, by Eq. (A.4) we have (see Huang & Zhang, 2016)\np(r|x; θ) = f(x; θ)r\nr!\nexp (−f(x; θ)),\n(A.7)\nS(x; θ) =\n1\nf(x; θ)\n∂f(x; θ)\n∂x\n∂f(x; θ)\n∂xT\n= ∂g(x; θ)\n∂x\n∂g(x; θ)\n∂xT\n,\n(A.8)\nwhere f(x; θ) ≥0 is the activation function (mean response) of neuron and\ng(x; θ) = 2\np\nf(x; θ).\n(A.9)\n11\nPublished as a conference paper at ICLR 2017\nSimilarly, for Gaussian noise model, we have\np(r|x; θ) =\n1\nσ\n√\n2π exp\n \n−(r −f(x; θ))2\n2σ2\n!\n,\n(A.10)\nS(x; θ) = 1\nσ2\n∂f(x; θ)\n∂x\n∂f(x; θ)\n∂xT\n,\n(A.11)\nwhere σ > 0 denotes the standard deviation of noise.\nSometimes we do not know the speciﬁc form of p(x) and only know M samples, x1, · · · , xM,\nwhich are independent and identically distributed (i.i.d.) samples drawn from the distribution p(x).\nThen we can use the empirical average to approximate the integral in Eq. (1):\nIG ≈1\n2\nM\nX\nm=1\nln (det (G(xm))) + H (X) .\n(A.12)\nA.2\nPROOF OF PROPOSITION 1\nProof. It follows from the data-processing inequality (Cover & Thomas, 2006) that\nI(X; R) ≤I(Y ; R) ≤I( ˘Y ; R) ≤I( ¯Y ; R),\n(A.13)\nI(X; R) ≤I(X; ¯Y ) ≤I(X; ˘Y ) ≤I(X; Y ).\n(A.14)\nSince\np(¯yk|x) = p(˘yk1, · · · , ˘ykNk |x) = N(wT\nk x, N −1\nk σ2), k = 1, · · · , K1,\n(A.15)\nwe have\np(¯y|x) = p(˘y|x),\n(A.16)\np(¯y) = p(˘y),\n(A.17)\nI(X; ¯Y ) = I(X; ˘Y ).\n(A.18)\nHence, by (A.14) and (A.18), expression (12) holds.\nOn the other hand, when Nk is large, from Eq. (10) we know that the distribution of ¯Zk, namely,\nN\n\u00000, N −1\nk σ2\u0001\n, approaches a Dirac delta function δ(¯zk). Then by (7) and (9) we have p (r|¯y) ≃\np(r|y) = p (r|x) and\nI(X; R) = I (Y ; R) −\n\u001c\nln p (r|y)\np (r|x)\n\u001d\nr,x\n= I (Y ; R) ,\n(A.19)\nI (Y ; R) = I( ¯Y ; R) −\n\u001c\nln p (r|¯y)\np (r|y)\n\u001d\nr,y,¯y\n≃I( ¯Y ; R),\n(A.20)\nI (Y ; R) = I( ˘Y ; R) −\n\u001c\nln p (r|˘y)\np (r|y)\n\u001d\nr,y,˘y\n≃I( ˘Y ; R),\n(A.21)\nI(X; Y ) = I(X; ¯Y ) −\n\u001c\nln p (x|¯y)\np (x|y)\n\u001d\nx,y,¯y\n≃I(X; ¯Y ).\n(A.22)\nIt follows from (A.13) and (A.19) that (11) holds. Combining (11), (12) and (A.20)–(A.22), we\nimmediately get (13) and (14). This completes the proof of Proposition 1.\n□\nA.3\nHIERARCHICAL OPTIMIZATION FOR MAXIMIZING I(X; R)\nIn the following, we will discuss the optimization procedure for maximizing I(X; R) in two stages:\nmaximizing I(X; ˘Y ) and maximizing I(Y ; R).\n12\nPublished as a conference paper at ICLR 2017\nA.3.1\nTHE 1ST STAGE\nIn the ﬁrst stage, our goal is to maximize the MI I(X; ˘Y ) and get the optimal parameters wk\n(k = 1, · · · , K1). Assume that the stimulus x has zero mean (if not, let x ←x −⟨x⟩x) and\ncovariance matrix Σx. It follows from eigendecomposition that\nΣx =\n\nxxT \u000b\nx ≈\n1\nM −1XXT = UΣUT ,\n(A.23)\nwhere X = [x1, · · · , xM], U = [u1, · · · , uK] ∈RK×K is an unitary orthogonal matrix and Σ =\ndiag\n\u0000σ2\n1, · · · , σ2\nK\n\u0001\nis a positive diagonal matrix with σ1 ≥· · · ≥σK > 0. Deﬁne\n˜x = Σ−1/2UT x,\n(A.24)\n˜wk = Σ1/2UT wk,\n(A.25)\nyk = ˜wT\nk ˜x,\n(A.26)\nwhere k = 1, · · · , K1. The covariance matrix of ˜x is given by\nΣ˜x =\nD\n˜x˜xT E\n˜x ≈IK,\n(A.27)\nand IK is a K × K identity matrix. From (1) and (A.11) we have I(X; ˘Y ) = I( ˜X; ˘Y ) and\nI( ˜X; ˘Y ) ≃I′\nG = 1\n2 ln\n \ndet\n ˜G\n2πe\n!!\n+ H( ˜X),\n(A.28)\n˜G ≈Nσ−2\nK1\nX\nk=1\nαk ˜wk ˜wT\nk + IK.\n(A.29)\nThe following approximations are useful (see Huang & Zhang, 2016):\np(˜x) ≈N (0, IK),\n(A.30)\nP(˜x) = −∂2 ln p (˜x)\n∂˜x∂˜xT\n≈IK.\n(A.31)\nBy the central limit theorem, the distribution of random variable ˜X is closer to a normal distribu-\ntion than the distribution of the original random variable X. On the other hand, the PCA models\nassume multivariate gaussian data whereas the ICA models assume multivariate non-gaussian data.\nHence by a PCA-like whitening transformation (A.24) we can use the approximation (A.31) with\nthe Laplace’s method of asymptotic expansion, which only requires that the peak be close to its\nmean while random variable ˜X needs not be exactly Gaussian.\nWithout any constraints on the Gaussian channel of neural populations, especially the peak ﬁring\nrates, the capacity of this channel may grow indeﬁnitely: I( ˜X; ˘Y ) →∞. The most common\nconstraint on the neural populations is an energy or power constraint which can also be regarded as\na signal-to-noise ratio (SNR) constraint. The SNR for the output ˘yn of the n-th neuron is given by\nSNRn = 1\nσ2\nD\u0000wT\nn x\n\u00012E\nx ≈1\nσ2 ˜wT\nn ˜wn, n = 1, · · · , N.\n(A.32)\nWe require that\n1\nN\nN\nX\nn=1\nSNRn ≈1\nσ2\nK1\nX\nk=1\nαk ˜wT\nk ˜wk ≤ρ,\n(A.33)\nwhere ρ is a positive constant. Then by Eq. (A.28), (A.29) and (A.33), we have the following\noptimization problem:\nminimize Q′\nG[ ˆ\nW] = −1\n2ln\n\u0010\ndet\n\u0010\nNσ−2 ˆ\nW ˆ\nW\nT + IK\n\u0011\u0011\n,\n(A.34)\nsubject to h = Tr\n\u0010\nˆ\nW ˆ\nW\nT \u0011\n−E ≤0,\n(A.35)\n13\nPublished as a conference paper at ICLR 2017\nwhere Tr (·) denotes matrix trace and\nˆ\nW = ˜\nWA\n1/2 = Σ1/2UT WA1/2 = [ˆw1, · · · , ˆwK1],\n(A.36)\nA = diag (α1, · · · , αK1),\n(A.37)\nW = [w1, · · · , wK1],\n(A.38)\n˜\nW = [˜w1, · · · , ˜wK1],\n(A.39)\nE = ρσ2.\n(A.40)\nHere E is a constant that does not affect the ﬁnal optimal solution so we set E = 1. Then we obtain\nan optimal solution as follows:\nW = aU0Σ−1/2\n0\nVT\n0 ,\n(A.41)\nA = K−1\n1 IK1,\n(A.42)\na =\nq\nEK1K−1\n0\n=\nq\nK1K−1\n0 ,\n(A.43)\nΣ0 = diag\n\u0000σ2\n1, · · · , σ2\nK0\n\u0001\n,\n(A.44)\nU0 = U (:, 1:K0) ∈RK×K0,\n(A.45)\nV0 = V (:, 1:K0) ∈RK1×K0,\n(A.46)\nwhere V = [v1, · · · , vK1] is an K1 × K1 unitary orthogonal matrix, parameter K0 represents the\nsize of the reduced dimension (1 ≤K0 ≤K), and its value will be determined below. Now the\noptimal parameters wn (n = 1, · · · , N) are clustered into K1 classes (see Eq. A.6) and obey an\nuniform discrete distribution (see also Eq. A.60 in Appendix A.3.2).\nWhen K = K0 = K1, the optimal solution of W in Eq. (A.41) is a whitening-like ﬁlter. When\nV = IK, the optimal matrix W is the principal component analysis (PCA) whitening ﬁlters. In the\nsymmetrical case with V = U, the optimal matrix W becomes a zero component analysis (ZCA)\nwhitening ﬁlter. If K < K1, this case leads to an overcomplete solution, whereas when K0 ≤K1 <\nK, the undercomplete solution arises. Since K0 ≤K1 and K0 ≤K, Q′\nG achieves its minimum\nwhen K0 = K. However, in practice other factors may prevent it from reaching this minimum. For\nexample, consider the average of squared weights,\nς =\nK1\nX\nk=1\nαk ∥wk∥2 = Tr\n\u0010\nWAWT \u0011\n= E\nK0\nK0\nX\nk=1\nσ−2\nk ,\n(A.47)\nwhere ∥·∥denotes the Frobenius norm. The value of ς is extremely large when any σk becomes\nvanishingly small. For real neurons these weights of connection are not allowed to be too large.\nHence we impose a limitation on the weights: ς ≤E1, where E1 is a positive constant. This yields\nanother constraint on the objective function,\n˜h = E\nK0\nK0\nX\nk=1\nσ−2\nk\n−E1 ≤0.\n(A.48)\nFrom (A.35) and (A.48) we get the optimal K0 = arg max ˜\nK0\n\u0010\nE ˜K−1\n0\nP ˜\nK0\nk=1 σ−2\nk\n\u0011\n. By this con-\nstraint, small values of σ2\nk will often result in K0 < K and a low-rank matrix W (Eq. A.41).\nOn the other hand, the low-rank matrix W can ﬁlter out the noise of stimulus x. Consider the\ntransformation Y = WT X with X = [x1, · · · , xM] and Y = [y1, · · · , yM] for M samples. It\nfollows from the singular value decomposition (SVD) of X that\nX = US ˜V\nT ,\n(A.49)\nwhere U is given in (A.23), ˜V is a M ×M unitary orthogonal matrix, S is a K ×M diagonal matrix\nwith non-negative real numbers on the diagonal, Sk,k =\n√\nM −1σk (k = 1, · · · , K, K ≤M), and\nSST = (M −1)Σ. Let\n˘X =\n√\nM −1U0Σ1/2\n0\n˜VT\n0 ≈X,\n(A.50)\n14\nPublished as a conference paper at ICLR 2017\nwhere ˜V0 = ˜V (:, 1:K0) ∈RM×K0, Σ0 and U0 are given in (A.44) and (A.45), respectively. Then\nY = WT X = aV0Σ−1/2\n0\nUT\n0 US ˜V\nT = WT ˘X = a\n√\nM −1V0 ˜VT\n0 ,\n(A.51)\nwhere ˘X can be regarded as a denoised version of X. The determination of the effective rank\nK0 ≤K of the matrix ˘X by using singular values is based on various criteria (Konstantinides &\nYao, 1988). Here we choose K0 as follows:\nK0 = arg min\nK′\n0\n\n\nv\nu\nu\nt\nPK′\n0\nk=1 σ2\nk\nPK\nk=1 σ2\nk\n≥ϵ\n\n,\n(A.52)\nwhere ϵ is a positive constant (0 < ϵ ≤1).\nAnother advantage of a low-rank matrix W is that it can signiﬁcantly reduce overﬁtting for learning\nneural population parameters. In practice, the constraint (A.47) is equivalent to a weight-decay reg-\nularization term used in many other optimization problems (Cortes & Vapnik, 1995; Hinton, 2010),\nwhich can reduce overﬁtting to the training data. To prevent the neural networks from overﬁtting,\nSrivastava et al. (2014) presented a technique to randomly drop units from the neural network dur-\ning training, which may in fact be regarded as an attempt to reduce the rank of the weight matrix\nbecause the dropout can result in a sparser weights (lower rank matrix). This means that the update\nis only concerned with keeping the more important components, which is similar to ﬁrst performing\na denoising process by the SVD low rank approximation.\nIn this stage, we have obtained the optimal parameter W (see A.41). The optimal value of matrix\nV0 can also be determined, as shown in Appendix A.3.3.\nA.3.2\nTHE 2ND STAGE\nFor this stage, our goal is to maximize the MI I(Y ; R) and get the optimal parameters ˜θk,\nk = 1, · · · , K1. Here the input is y = (y1, · · · , yK1)T and the output r = (r1, · · · , rN)T is\nalso clustered into K1 classes. The responses of Nk neurons in the k-th subpopulation obey a Pois-\nson distribution with mean ˜f(eT\nk y; ˜θk), where ek is a unit vector with 1 in the k-th element and\nyk = eT\nk y. By (A.24) and (A.26), we have\n⟨yk⟩yk = 0,\n(A.53)\nσ2\nyk =\n\ny2\nk\n\u000b\nyk = ∥˜wk∥2 .\n(A.54)\nThen for large N, by (1)–(4) and (A.30) we can use the following approximation,\nI(Y ; R) ≃˘IF = 1\n2\n*\nln\n \ndet\n ˘J(y)\n2πe\n!!+\ny\n+ H(Y ),\n(A.55)\nwhere\n˘J(y) = diag\n\u0010\nNα1 |g′\n1(y1)|2 , · · · , NαK1\n\f\fg′\nK1(yK1)\n\f\f2\u0011\n,\n(A.56)\ng′\nk(yk) = ∂gk(yk)\n∂yk\n, k = 1, · · · , K1,\n(A.57)\ngk(yk) = 2\nq\n˜f(yk; ˜θk), k = 1, · · · , K1.\n(A.58)\nIt is easy to get that\n˘IF = 1\n2\nK1\nX\nk=1\n*\nln\n \nNαk |g′\nk(yk)|2\n2πe\n!+\ny\n+ H(Y )\n≤1\n2\nK1\nX\nk=1\n*\nln\n \n|g′\nk(yk)|2\n2πe\n!+\ny\n−K1\n2 ln\n\u0012K1\nN\n\u0013\n+ H(Y ),\n(A.59)\n15\nPublished as a conference paper at ICLR 2017\nwhere the equality holds if and only if\nαk = 1\nK1\n, k = 1, · · · , K1,\n(A.60)\nwhich is consistent with Eq. (A.42).\nOn the other hand, it follows from the Jensen’s inequality that\n˘IF =\n*\nln\n\np (y)−1 det\n ˘J(y)\n2πe\n!1/2\n\n+\ny\n≤ln\nZ\ndet\n ˘J(y)\n2πe\n!1/2\ndy,\n(A.61)\nwhere the equality holds if and only if p (y)−1 det\n\u0010\n˘J(y)\n\u00111/2\nis a constant, which implies that\np (y) =\ndet\n\u0010\n˘J(y)\n\u00111/2\nR\ndet\n\u0010\n˘J(y)\n\u00111/2\ndy\n=\nQK1\nk=1 |g′\nk(yk)|\nR QK1\nk=1 |g′\nk(yk)| dy\n.\n(A.62)\nFrom (A.61) and (A.62), maximizing ˜IF yields\np (yk) =\n|g′\nk(yk)|\nR\n|g′\nk(yk)| dyk\n, k = 1, · · · , K1.\n(A.63)\nWe assume that (A.63) holds, at least approximately. Hence we can let the peak of g′\nk(yk) be at\nyk = ⟨yk⟩yk = 0 and\n\ny2\nk\n\u000b\nyk = σ2\nyk = ∥˜wk∥2. Then combining (A.57), (A.61) and (A.63) we ﬁnd\nthe optimal parameters ˜θk for the nonlinear functions ˜f(yk; ˜θk), k = 1, · · · , K1.\nA.3.3\nTHE FINAL OBJECTIVE FUNCTION\nIn the preceding sections we have obtained the initial optimal solutions by maximizing I\n\u0010\nX; ˘Y\n\u0011\nand I(Y ; R). In this section, we will discuss how to ﬁnd the ﬁnal optimal V0 and other parameters\nby maximizing I(X; R) from the initial optimal solutions.\nFirst, we have\ny = ˜\nWT ˜x = aˆy,\n(A.64)\nwhere a is given in (A.43) and\nˆy = (ˆy1, · · · , ˆyK1)T = CT ˆx = ˇCT ˇx,\n(A.65)\nˆx = Σ−1/2\n0\nUT\n0 x,\n(A.66)\nC = VT\n0 ∈RK0×K1,\n(A.67)\nˇx = U0Σ−1/2\n0\nUT\n0 x = U0ˆx,\n(A.68)\nˇC = U0C = [ˇc1, · · · ,ˇcK1].\n(A.69)\nIt follows that\nI(X; R) = I\n\u0010\n˜X; R\n\u0011\n≃˜IG = 1\n2\n\u001c\nln\n\u0012\ndet\n\u0012G(ˆx)\n2πe\n\u0013\u0013\u001d\nˆx\n+ H( ˜X),\n(A.70)\nG(ˆx) = N ˆ\nWˆΦ ˆ\nW\nT + IK,\n(A.71)\nˆ\nW = Σ1/2UT WA1/2 = a\nq\nK−1\n1 IK\nK0C =\nq\nK−1\n0 IK\nK0C,\n(A.72)\n16\nPublished as a conference paper at ICLR 2017\nwhere IK\nK0 is a K × K0 diagonal matrix with value 1 on the diagonal and\nˆΦ = Φ2,\n(A.73)\nΦ = diag (φ(ˆy1), · · · , φ(ˆyK1)) ,\n(A.74)\nφ(ˆyk) = a−1\n\f\f\f\f\n∂gk(ˆyk)\n∂ˆyk\n\f\f\f\f ,\n(A.75)\ngk(ˆyk) = 2\nq\n˜f(ˆyk; ˜θk),\n(A.76)\nˆyk = a−1yk = cT\nk ˆx, k = 1, · · · , K1.\n(A.77)\nThen we have\ndet (G(ˆx)) = det\n\u0010\nNK−1\n0 CˆΦC\nT + IK0\n\u0011\n.\n(A.78)\nFor large N and K0/N →0, we have\ndet (G(ˆx)) ≈det (J(ˆx)) = det\n\u0010\nNK−1\n0 CˆΦC\nT \u0011\n,\n(A.79)\n˜IG ≈˜IF = −Q −K\n2 ln (2πe) −K0\n2 ln (ε) + H( ˜X),\n(A.80)\nQ = −1\n2\nD\nln\n\u0010\ndet\n\u0010\nCˆΦC\nT \u0011\u0011E\nˆx ,\n(A.81)\nε = K0\nN .\n(A.82)\nHence we can state the optimization problem as:\nminimize Q [C] = −1\n2\nD\nln\n\u0010\ndet\n\u0010\nCˆΦC\nT \u0011\u0011E\nˆx ,\n(A.83)\nsubject to CCT = IK0.\n(A.84)\nThe gradient from (A.83) is given by:\ndQ[C]\ndC\n= −\n\u001c\u0010\nCˆΦC\nT \u0011−1\nCˆΦ + ˆxωT\n\u001d\nˆx\n,\n(A.85)\nwhere C = [c1, · · · , cK1], ω = (ω1, · · · , ωK1)T , and\nωk = φ(ˆyk)φ′(ˆyk)cT\nk\n\u0010\nCˆΦC\nT \u0011−1\nck, k = 1, · · · , K1.\n(A.86)\nIn the following we will discuss how to get the optimal solution of C for two speciﬁc cases.\nA.4\nALGORITHMS FOR OPTIMIZATION OBJECTIVE FUNCTION\nA.4.1\nALGORITHM 1: K0 = K1\nNow CCT = CT C = IK1, then by Eq. (A.83) we have\nQ1[C] = −\n* K1\nX\nk=1\nln (φ(ˆyk))\n+\nˆx\n,\n(A.87)\ndQ1[C]\ndC\n= −\n\nˆxωT \u000b\nˆx ,\n(A.88)\nωk = φ′(ˆyk)\nφ(ˆyk) , k = 1, · · · , K1.\n(A.89)\nUnder the orthogonality constraints (Eq. A.84), we can use the following update rule for learning C\n(Edelman et al., 1998; Amari, 1999):\nCt+1 = Ct + µt\ndCt\ndt ,\n(A.90)\ndCt\ndt\n= −dQ1[Ct]\ndCt\n+ Ct\n\u0012dQ1[Ct]\ndCt\n\u0013T\nCt,\n(A.91)\n17\nPublished as a conference paper at ICLR 2017\nwhere the learning rate parameter µt changes with the iteration count t, t = 1, · · · , tmax. Here we\ncan use the empirical average to approximate the integral in (A.88) (see Eq. A.12). We can also\napply stochastic gradient descent (SGD) method for online updating of Ct+1 in (A.90).\nThe orthogonality constraint (Eq. A.84) can accelerate the convergence rate. In practice, the orthog-\nonal constraint (A.84) for objective function (A.83) is not strictly necessary in this case. We can\ncompletely discard this constraint condition and consider\nminimize Q2 [C] = −\n* K1\nX\nk=1\nln (φ (ˆyk))\n+\nˆx\n−1\n2 ln\n\u0000det\n\u0000CT C\n\u0001\u0001\n,\n(A.92)\nwhere we assume rank (C) = K1 = K0. If we let\ndC\ndt = −CCT dQ2 [C]\ndC\n,\n(A.93)\nthen\nTr\n\u0012dQ2 [C]\ndC\ndCT\ndt\n\u0013\n= −Tr\n\u0012\nCT dQ2 [C]\ndC\ndQ2 [C]\ndCT\nC\n\u0013\n≤0.\n(A.94)\nTherefore we can use an update rule similar to Eq. A.90 for learning C. In fact, the method can also\nbe extended to the case K0 > K1 by using the same objective function (A.92).\nThe learning rate parameter µt (see A.90) is updated adaptively, as follows. First, calculate\nµt = vt\nκt\n, t = 1, · · · , tmax,\n(A.95)\nκt = 1\nK1\nK1\nX\nk=1\n∥∇Ct(:, k)∥\n∥Ct(:, k)∥,\n(A.96)\nand Ct+1 by (A.90) and (A.91), then calculate the value Q1\n\u0002\nCt+1\u0003\n. If Q1\n\u0002\nCt+1\u0003\n< Q1[Ct], then\nlet vt+1 ←vt, continue for the next iteration; otherwise, let vt ←τvt, µt ←vt/κt and recalculate\nCt+1 and Q1\n\u0002\nCt+1\u0003\n. Here 0 < v1 < 1 and 0 < τ < 1 are set as constants. After getting Ct+1\nfor each update, we employ a Gram–Schmidt orthonormalization process for matrix Ct+1, where\nthe orthonormalization process can accelerate the convergence. However, we can discard the Gram–\nSchmidt orthonormalization process after iterative t0 (> 1) epochs for more accurate optimization\nsolution C. In this case, the objective function is given by the Eq. (A.92). We can also further\noptimize parameter b by gradient descent.\nWhen K0 = K1, the objective function Q2 [C] in Eq. (A.92) without constraint is the same as the\nobjective function of infomax ICA (IICA) (Bell & Sejnowski, 1995; 1997), and as a consequence\nwe should get the same optimal solution C. Hence, in this sense, the IICA may be regarded as a\nspecial case of our method. Our method has a wider range of applications and can handle more\ngeneric situations. Our model is derived by neural populations with a huge number of neurons and it\nis not restricted to additive noise model. Moreover, our method has a faster convergence rate during\ntraining than IICA (see Section 3).\nA.4.2\nALGORITHM 2: K0 ≤K1\nIn this case, it is computationally expensive to update C by using the gradient of Q (see Eq. A.85),\nsince it needs to compute the inverse matrix for every ˆx. Here we provide an alternative method for\nlearning the optimal C. First, we consider the following inequalities.\n18\nPublished as a conference paper at ICLR 2017\nProposition 2. The following inequations hold,\n1\n2\nD\nln\n\u0010\ndet\n\u0010\nCˆΦCT \u0011\u0011E\nˆx ≤1\n2 ln\n\u0010\ndet\n\u0010\nC\nD\nˆΦ\nE\nˆx CT \u0011\u0011\n,\n(A.97)\n\nln\n\u0000det\n\u0000CΦCT \u0001\u0001\u000b\nˆx ≤ln\n\u0000det\n\u0000C ⟨Φ⟩ˆx CT \u0001\u0001\n(A.98)\n≤1\n2 ln\n\u0010\ndet\n\u0010\nC ⟨Φ⟩2\nˆx CT \u0011\u0011\n(A.99)\n≤1\n2 ln\n\u0010\ndet\n\u0010\nC\nD\nˆΦ\nE\nˆx CT \u0011\u0011\n,\n(A.100)\nln\n\u0000det\n\u0000CΦCT \u0001\u0001\n≤1\n2 ln\n\u0010\ndet\n\u0010\nCˆΦCT \u0011\u0011\n,\n(A.101)\nwhere C ∈RK0×K1, K0 ≤K1, and CCT = IK0.\nProof. Functions ln\n\u0010\ndet\n\u0010\nC\nD\nˆΦ\nE\nˆx CT \u0011\u0011\nand ln\n\u0000det\n\u0000C ⟨Φ⟩ˆx CT \u0001\u0001\nare concave functions about\np (ˆx) (see the proof of Proposition 5.2. in Huang & Zhang, 2016), which fact establishes inequalities\n(A.97) and (A.98).\nNext we will prove the inequality (A.101). By SVD, we have\nCΦ = ¨U ¨D ¨V\nT ,\n(A.102)\nwhere ¨U is a K0 × K0 unitary orthogonal matrix, ¨V = [¨v1, ¨v2, · · · , ¨vK1] is an K1 × K1 unitary\northogonal matrix, and ¨D is an K0 × K1 rectangular diagonal matrix with K0 positive real numbers\non the diagonal. By the matrix Hadamard’s inequality and Cauchy–Schwarz inequality we have\ndet\n\u0000CΦCT CΦCT \u0001\ndet\n\u0010\nCˆΦC\nT \u0011−1\n= det\n\u0012\n¨D ¨V\nT CT C ¨V ¨D\nT \u0010\n¨D ¨D\nT \u0011−1\u0013\n= det\n\u0010\n¨VT\n1 CT C ¨V1\n\u0011\n= det\n\u0010\nC ¨V1\n\u00112\n≤\nK0\nY\nk=1\n\u0010\nC ¨V1\n\u00112\nk,k\n≤\nK0\nY\nk=1\n\u0010\nCCT \u00112\nk,k\n\u0010\n¨VT\n1 ¨V1\n\u00112\nk,k\n= 1,\n(A.103)\nwhere ¨V1 = [¨v1, ¨v2, · · · , ¨vK0] ∈RK1×K0. The last equality holds because of CCT = IK0 and\n¨VT\n1 ¨V1 = IK0. This establishes inequality (A.101) and the equality holds if and only if K0 = K1\nor C ¨V1 = IK0.\nSimilarly, we get inequality (A.99):\nln\n\u0000det\n\u0000C ⟨Φ⟩ˆx CT \u0001\u0001\n≤1\n2 ln\n\u0010\ndet\n\u0010\nC ⟨Φ⟩2\nˆx CT \u0011\u0011\n.\n(A.104)\nBy Jensen’s inequality, we have\n⟨φ (ˆyk)⟩2\nˆx ≤\nD\nφ (ˆyk)2E\nˆx , ∀k = 1, · · · , K1.\n(A.105)\nThen it follows from (A.105) that inequality (A.100) holds:\n1\n2 ln\n\u0010\ndet\n\u0010\nC ⟨Φ⟩2\nˆx CT \u0011\u0011\n≤1\n2 ln\n\u0010\ndet\n\u0010\nC\nD\nˆΦ\nE\nˆx CT \u0011\u0011\n.\n(A.106)\n19\nPublished as a conference paper at ICLR 2017\nThis completes the proof of Proposition 2.\n□\nBy Proposition 2, if K0 = K1 then we get\n1\n2\nD\nln\n\u0010\ndet\n\u0010\nˆΦ\n\u0011\u0011E\nˆx ≤1\n2 ln\n\u0010\ndet\n\u0010D\nˆΦ\nE\nˆx\n\u0011\u0011\n,\n(A.107)\n⟨ln (det (Φ))⟩ˆx ≤ln (det (⟨Φ⟩ˆx))\n(A.108)\n= 1\n2 ln\n\u0010\ndet\n\u0010\n⟨Φ⟩2\nˆx\n\u0011\u0011\n(A.109)\n≤1\n2 ln\n\u0010\ndet\n\u0010D\nˆΦ\nE\nˆx\n\u0011\u0011\n,\n(A.110)\nln (det (Φ)) = 1\n2 ln\n\u0010\ndet\n\u0010\nˆΦ\n\u0011\u0011\n.\n(A.111)\nOn the other hand, it follows from (A.81) and Proposition 2 that\n\nln\n\u0000det\n\u0000CΦCT \u0001\u0001\u000b\nˆx ≤−Q ≤1\n2 ln\n\u0010\ndet\n\u0010\nC\nD\nˆΦ\nE\nˆx CT \u0011\u0011\n,\n(A.112)\n\nln\n\u0000det\n\u0000CΦCT \u0001\u0001\u000b\nˆx ≤−ˆQ ≤1\n2 ln\n\u0010\ndet\n\u0010\nC\nD\nˆΦ\nE\nˆx CT \u0011\u0011\n.\n(A.113)\nHence we can see that ˆQ is close to Q (see A.81). Moreover, it follows from the Cauchy–Schwarz\ninequality that\nD\n(Φ)k,k\nE\nˆx = ⟨φ (ˆyk)⟩ˆyk ≤\n\u0012Z\nφ (ˆyk)2 dˆyk\nZ\np (ˆyk)2 dˆyk\n\u00131/2\n,\n(A.114)\nwhere k = 1, · · · , K1, the equality holds if and only if the following holds:\np (ˆyk) =\nφ (ˆyk)\nR\nφ (ˆyk) dˆyk\n, k = 1, · · · , K1,\n(A.115)\nwhich is the similar to Eq. (A.63).\nSince I(X; R) = I(Y ; R) (see Proposition 1), by maximizing I(X; R) we hope the equality in\ninequality (A.61) and equality (A.63) hold, at least approximatively. On the other hand, let\nCopt = arg min\nC Q[C] = arg max\nC\n\u0010D\nln\n\u0010\ndet(CˆΦCT )\n\u0011E\nˆx\n\u0011\n,\n(A.116)\nˆCopt = arg min\nC\nˆQ[C] = arg max\nC\n\u0010\nln\n\u0010\ndet\n\u0010\nC ⟨Φ⟩2\nˆx CT \u0011\u0011\u0011\n,\n(A.117)\nCopt and ˆCopt make (A.63) and (A.115) to hold true, which implies that they are the same optimal\nsolution: Copt = ˆCopt.\nTherefore, we can use the following objective function ˆQ[C] as a substitute for Q[C] and write the\noptimization problem as:\nminimize ˆQ[C] = −1\n2 ln\n\u0010\ndet\n\u0010\nC ⟨Φ⟩2\nˆx CT \u0011\u0011\n,\n(A.118)\nsubject to CCT = IK0.\n(A.119)\nThe update rule (A.90) may also apply here and a modiﬁed algorithm similar to Algorithm 1 may\nbe used for parameter learning.\nA.5\nSUPPLEMENTARY EXPERIMENTS\nA.5.1\nQUANTITATIVE METHODS FOR COMPARISON\nTo quantify the efﬁciency of learning representations by the above algorithms, we calculate the co-\nefﬁcient entropy (CFE) for estimating coding cost as follows (Lewicki & Olshausen, 1999; Lewicki\n& Sejnowski, 2000):\nˇyk = ζ ˇwT\nk ˇx, k = 1, · · · , K1,\n(A.120)\nζ =\nK1\nPK1\nk=1 ∥ˇwk∥\n,\n(A.121)\n20\nPublished as a conference paper at ICLR 2017\nwhere ˇx is deﬁned by Eq. (A.68), and ˇwk is the corresponding optimal ﬁlter. To estimate the\nprobability density of coefﬁcients qk(ˇyk) (k = 1, · · · , K1) from the M training samples, we apply\nthe kernel density estimation for qk(ˇyk) and use a normal kernel with an adaptive optimal window\nwidth. Then we deﬁne the CFE h as\nh = 1\nK1\nK1\nX\nk=1\nHk( ˇYk),\n(A.122)\nHk( ˇYk) = −∆P\nnqk(n∆) log2 qk(n∆),\n(A.123)\nwhere qk(ˇyk) is quantized as discrete qk(n∆) and ∆is the step size.\nMethods such as IICA and SRBM as well as our methods have feedforward structures in which\ninformation is transferred directly through a nonlinear function, e.g., the sigmoid function. We\ncan use the amount of transmitted information to measure the results learned by these methods.\nConsider a neural population with N neurons, which is a stochastic system with nonlinear transfer\nfunctions. We chose a sigmoidal transfer function and Gaussian noise with standard deviation set to\n1 as the system noise. In this case, from (1), (A.8) and (A.11), we see that the approximate MI IG is\nequivalent to the case of the Poisson neuron model. It follows from (A.70)–(A.82) that\nI(X; R) = I\n\u0010\n˜X; R\n\u0011\n= H( ˜X) −H\n\u0010\n˜X|R\n\u0011\n≃˜IG = H( ˜X) −h1,\n(A.124)\nH\n\u0010\n˜X|R\n\u0011\n≃h1 = −1\n2\n\u001c\nln\n\u0012\ndet\n\u0012 1\n2πe\n\u0010\nNK−1\n0 CˆΦC\nT + IK0\n\u0011\u0013\u0013\u001d\nˆx\n,\n(A.125)\nwhere we set N = 106. A good representation should make the MI I(X; R) as big as possible.\nEquivalently, for the same inputs, a good representation should make the conditional entropy (CDE)\nH\n\u0010\n˜X|R\n\u0011\n(or h1) as small as possible.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 4: Comparison of basis vectors obtained by our method and other methods. Panel (a)–(e)\ncorrespond to panel (a)–(e) in Figure 2, where the basis vectors are given by (A.130). The basis\nvectors in panel (f) are learned by MBDL and given by (A.127).\n21\nPublished as a conference paper at ICLR 2017\nA.5.2\nCOMPARISON OF BASIS VECTORS\nWe compared our algorithm with an up-to-date sparse coding algorithm, the mini-batch dictionary\nlearning (MBDL) as given in (Mairal et al., 2009; 2010) and integrated in Python library, i.e. scikit-\nlearn. The input data was the same as the above, i.e. 105 nature image patches preprocessed by the\nZCA whitening ﬁlters.\nWe denotes the optimal dictionary learned by MBDL as ˇB ∈RK×K1 for which each column\nrepresents a basis vector. Now we have\nx ≈UΣ1/2UT ˇBy = ˜By,\n(A.126)\n˜B = UΣ1/2UT ˇB,\n(A.127)\nwhere y = (y1, · · · , yK1)T is the coefﬁcient vector.\nSimilarly, we can obtain a dictionary from the ﬁlter matrix C. Suppose rank (C) = K0 ≤K1, then\nit follows from (A.64) that\nˆx =\n\u0010\naCCT \u0011−1\nCy.\n(A.128)\nBy (A.66) and (A.128), we get\nx ≈By = aBCT Σ−1/2\n0\nUT\n0 x,\n(A.129)\nB = a−1U0Σ1/2\n0\n\u0010\nCCT \u0011−1\nC = [b1, · · · , bK1] ,\n(A.130)\nwhere y = WT x = aCT Σ−1/2\n0\nUT\n0 x, the vectors b1, · · · , bK1 can be regarded as the basis vectors\nand the strict equality holds when K0 = K1 = K. Recall that X = [x1, · · · , xM] = US ˜V\nT\n(see Eq. A.49) and Y = [y1, · · · , yM] = WT X = a\n√\nM −1CT ˜VT\n0 , then we get ˘X = BY =\n√\nM −1U0Σ1/2\n0\n˜VT\n0 ≈X. Hence, Eq. (A.129) holds.\nThe basis vectors shown in Figure 4(a)–4(e) correspond to ﬁlters in Figure 2(a)–2(e). And Fig-\nure 4(f) illustrates the optimal dictionary ˜B learned by MBDL, where we set the regularization pa-\nrameter as λ = 1.2/\n√\nK, the batch size as 50 and the total number of iterations to perform as 20000,\nwhich took about 3 hours for training. From Figure 4 we see that these basis vectors obtained by the\nabove algorithms have local Gabor-like shapes except for those by SRBM. If rank(ˇB) = K = K1,\nthen the matrix ˇB−T can be regarded as a ﬁlter matrix like matrix ˇC (see Eq. A.69). However,\nfrom the column vector of matrix ˇB−T we cannot ﬁnd any local Gabor-like ﬁlter that resembles the\nﬁlters shown in Figure 2. Our algorithm has less computational cost and a much faster convergence\nrate than the sparse coding algorithm. Moreover, the sparse coding method involves a dynamic\ngenerative model that requires relaxation and is therefore unsuitable for fast inference, whereas the\nfeedforward framework of our model is easy for inference because it only requires evaluating the\nnonlinear tuning functions.\nA.5.3\nLEARNING OVERCOMPLETE BASES\nWe have trained our model on the Olshausen’s nature image patches with a highly overcomplete\nsetup by optimizing the objective (A.118) by Alg.2 and got Gabor-like ﬁlters. The results of 400\ntypical ﬁlters chosen from 1024 output ﬁlters are displayed in Figure 5(a) and corresponding base\n(see Eq. A.130) are shown in Figure 5(b). Here the parameters are K1 = 1024, tmax = 100,\nv1 = 0.4, τ = 0.8, and ϵ = 0.98 (see A.52), from which we got rank (B) = K0 = 82. Compared\nto the ICA-like results in Figure 2(a)–2(c), the average size of Gabor-like ﬁlters in Figure 5(a) is\nbigger, indicating that the small noise-like local structures in the images have been ﬁltered out.\nWe have also trained our model on 60,000 images of handwritten digits from MNIST dataset (LeCun\net al., 1998) and the resultant 400 typical optimal ﬁlters and bases are shown in Figure 5(c) and\nFigure 5(d), respectively. All parameters were the same as Figure 5(a) and Figure 5(b): K1 = 1024,\ntmax = 100, v1 = 0.4, τ = 0.8 and ϵ = 0.98, from which we got rank (B) = K0 = 183. From\nthese ﬁgures we can see that the salient features of the input images are reﬂected in these ﬁlters and\nbases. We could also get the similar overcomplete ﬁlters and bases by SRBM and MBDL. However,\nthe results depended sensitively on the choice of parameters and the training took a long time.\n22\nPublished as a conference paper at ICLR 2017\n(a)\n(b)\n(c)\n(d)\nFigure 5: Filters and bases obtained from Olshausen’s image dataset and MNIST dataset by Al-\ngorithm 2. (a) and (b): 400 typical ﬁlters and the corresponding bases obtained from Olshausen’s\nimage dataset, where K0 = 82 and K1 = 1024. (c) and (d): 400 typical ﬁlters and the corresponding\nbases obtained from the MNIST dataset, where K0 = 183 and K1 = 1024.\nFigure 6 shows that CFE as a function of training time for Alg.2, where Figure 6(a) corresponds to\nFigure 5(a)-5(b) for learning nature image patches and Figure 6(b) corresponds to Figure 5(c)-5(d)\nfor learning MNIST dataset. We set parameters tmax = 100 and τ = 0.8 for all experiments and\nvaried parameter v1 for each experiment, with v1 = 0.2, 0.4, 0.6 or 0.8. These results indicate a fast\nconvergence rate for training on different datasets. Generally, the convergence is insensitive to the\nchange of parameter v1.\nWe have also performed additional tests on other image datasets and got similar results, conﬁrming\nthe speed and robustness of our learning method. Compared with other methods, e.g., IICA, FICA,\nMBDL, SRBM or sparse autoencoders etc., our method appeared to be more efﬁcient and robust for\nunsupervised learning of representations. We also found that complete and overovercomplete ﬁlters\nand bases learned by our methods had local Gabor-like shapes while the results by SRBM or MBDL\ndid not have this property.\n23\nPublished as a conference paper at ICLR 2017\n100\n101\n102\ntime (seconds)\n1.75\n1.8\n1.85\n1.9\n1.95\ncoefficient entropy (bits)\nv1 = 0.2\nv1 = 0.4\nv1 = 0.6\nv1 = 0.8\n(a)\n100\n101\n102\ntime (seconds)\n1.6\n1.7\n1.8\n1.9\n2\n2.1\ncoefficient entropy (bits)\nv1 = 0.2\nv1 = 0.4\nv1 = 0.6\nv1 = 0.8\n(b)\nFigure 6: CFE as a function of training time for Alg.2, with v1 = 0.2, 0.4, 0.6 or 0.8. In all\nexperiments parameters were set to tmax = 100, t0 = 50 and τ = 0.8. (a): corresponding to\nFigure 5(a) or Figure 5(b). (b): corresponding to Figure 5(c) or Figure 5(d).\nA.5.4\nIMAGE DENOISING\nSimilar to the sparse coding method applied to image denoising (Elad & Aharon, 2006), our method\n(see Eq. A.130) can also be applied to image denoising, as shown by an example in Figure 7. The\nﬁlters or bases were learned by using 7×7 image patches sampled from the left half of the image, and\nsubsequently used to reconstruct the right half of the image which was distorted by Gaussian noise.\nA common practice for evaluating the results of image denoising is by looking at the difference\nbetween the reconstruction and the original image. If the reconstruction is perfect the difference\nshould look like Gaussian noise. In Figure 7(c) and 7(d) a dictionary (100 bases) was learned by\nMBDL and orthogonal matching pursuit was used to estimate the sparse solution. 1 For our method\n(shown in Figure 7(b)), we ﬁrst get the optimal ﬁlters parameter W, a low rank matrix (K0 < K),\nthen from the distorted image patches xm (m = 1, · · · , M) we get ﬁlter outputs ym = WT xm\nand the reconstruction ˘xm = Bym (parameters: ϵ = 0.975 and K0 = K1 = 14). As can be seen\nfrom Figure 7, our method worked better than dictionary learning, although we only used 14 bases\ncompared with 100 bases used by dictionary learning. Our method is also more efﬁcient. We can\nget better optimal bases B by a generative model using our infomax approach (details not shown).\n1Python source code is available at http://scikit-learn.org/stable/ downloads/plot image denoising.py\n24\nPublished as a conference paper at ICLR 2017\n(a)\n(b)\n(c)\n(d)\nFigure 7: Image denoising. (a): the right half of the original image is distorted by Gaussian noise\nand the norm of the difference between the distorted image and the original image is 23.48. (b):\nimage denoising by our method (Algorithm 1), with 14 bases used. (c) and (d): image denoising\nusing dictionary learning, with 100 bases used.\n25\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.IT",
    "math.IT",
    "q-bio.NC",
    "stat.ML"
  ],
  "published": "2016-11-07",
  "updated": "2017-03-10"
}