{
  "id": "http://arxiv.org/abs/1611.04687v2",
  "title": "Intrinsic Geometric Information Transfer Learning on Multiple Graph-Structured Datasets",
  "authors": [
    "Jaekoo Lee",
    "Hyunjae Kim",
    "Jongsun Lee",
    "Sungroh Yoon"
  ],
  "abstract": "Graphs provide a powerful means for representing complex interactions between\nentities. Recently, deep learning approaches are emerging for representing and\nmodeling graph-structured data, although the conventional deep learning methods\n(such as convolutional neural networks and recurrent neural networks) have\nmainly focused on grid-structured inputs (image and audio). Leveraged by the\ncapability of representation learning, deep learning based techniques are\nreporting promising results for graph applications by detecting structural\ncharacteristics of graphs in an automated fashion. In this paper, we attempt to\nadvance deep learning for graph-structured data by incorporating another\ncomponent, transfer learning. By transferring the intrinsic geometric\ninformation learned in the source domain, our approach can help us to construct\na model for a new but related task in the target domain without collecting new\ndata and without training a new model from scratch. We thoroughly test our\napproach with large-scale real corpora and confirm the effectiveness of the\nproposed transfer learning framework for deep learning on graphs. According to\nour experiments, transfer learning is most effective when the source and target\ndomains bear a high level of structural similarity in their graph\nrepresentations.",
  "text": "Transfer Learning for Deep Learning on Graph-Structured Data\nJaekoo Lee, Hyunjae Kim, Jongsun Lee, Sungroh Yoon\nElectrical and Computer Engineering\nSeoul National University\nSeoul 08826, Republic of Korea\nsryoon@snu.ac.kr\nAbstract\nGraphs provide a powerful means for representing complex\ninteractions between entities. Recently, new deep learning ap-\nproaches have emerged for representing and modeling graph-\nstructured data while the conventional deep learning methods,\nsuch as convolutional neural networks and recurrent neural\nnetworks, have mainly focused on the grid-structured inputs\nof image and audio. Leveraged by representation learning ca-\npabilities, deep learning-based techniques can detect struc-\ntural characteristics of graphs, giving promising results for\ngraph applications. In this paper, we attempt to advance deep\nlearning for graph-structured data by incorporating another\ncomponent: transfer learning. By transferring the intrinsic ge-\nometric information learned in the source domain, our ap-\nproach can construct a model for a new but related task in\nthe target domain without collecting new data and without\ntraining a new model from scratch. We thoroughly tested our\napproach with large-scale real-world text data and conﬁrmed\nthe effectiveness of the proposed transfer learning framework\nfor deep learning on graphs. According to our experiments,\ntransfer learning is most effective when the source and tar-\nget domains bear a high level of structural similarity in their\ngraph representations.\nIntroduction\nRecently, many deep neural network models have been\nadopted successfully in various ﬁelds (LeCun, Bengio, and\nHinton 2015; Schmidhuber 2015). In particular, convolu-\ntional neural networks (CNN) (Krizhevsky, Sutskever, and\nHinton 2012) for image and video recognition and recurrent\nneural networks (RNN) (Sutskever, Vinyals, and Le 2014)\nfor speech and natural language processing (NLP) often de-\nliver unprecedented levels of performance. Deep learning\nhas also triggered advances in implementing human-level\nintelligence (e.g., in the game of Go (Silver et al. 2016)).\nCNN and RNN extract data-driven features from in-\nput data (e.g., image, video, and audio data) structured in\ntypically low-dimensional regular grids (see Fig. 1, top).\nSuch grid structures are often assumed to have statistical\ncharacteristics (e.g., stationarity and locality) to facilitate\nthe modeling process. Learning algorithms then take ad-\nvantage of this assumption and boost performance by re-\nCopyright c⃝2017, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nducing the complexity of parameters (Schmidhuber 2015;\nBruna et al. 2013; Henaff, Bruna, and LeCun 2015).\nIn reality, there exist a wide variety of data types in which\nwe need more general non-grid structures to represent and\nmodel complex interactions among entities. Examples in-\nclude social media mining and protein interaction studies.\nFor such applications, a graph can provide a natural way\nof representing entities and their interactions (Deo 2016).\nFor graph-structured input, it is more challenging to ﬁnd\nthe statistical characteristics that can be assumed for grid-\nstructured input (Bruna et al. 2013; Henaff, Bruna, and Le-\nCun 2015).\nTheoretical challenges including the above and practical\nlimitations, such as data quantity/quality and training efﬁ-\nciency, make it difﬁcult to apply conventional deep learn-\ning approaches directly, igniting research on adapting deep\nlearning to graph-structured data (Bruna et al. 2013; Henaff,\nBruna, and LeCun 2015; Jain et al. 2015; Li et al. 2015).\nIn many graph analysis methods, the structural properties\nderived from input graphs play a crucial role in uncover-\ning hidden patterns (Koutra, Vogelstein, and Faloutsos 2013;\nLee, Kim, and Yoon 2015). The representation learning ca-\npability of deep networks is useful for automatically detect-\ning data-driven structural features, and deep learning ap-\nproaches have reported promising results.\nIn this paper, we attempt to advance deep learning for\ngraph-structured data by incorporating another key compo-\nnent: transfer learning (Pan and Yang 2010). By overcoming\nthe common assumption that training and test data should be\ndrawn from the same feature space and distribution, transfer\nlearning between task domains can alleviate the burden of\ncollecting data and training models for a new task. Given the\nimportance of structural characteristics in graph analysis, the\ncore of our proposal is to transfer the data-driven structural\nfeatures learned by deep networks from a source domain to\na target domain, as informally shown in Fig. 1 (bottom). In\nthe context of graphs, we call the transferred information the\nintrinsic geometric information.\nStarting from this intuitive baseline, we need to ﬁll in\nmany details to implement transfer learning for deep learn-\ning on graph data. In particular, we need to answer two im-\nportant questions: (Q1) under what condition can we expect\na successful knowledge transfer between task domains and\n(Q2) how do we actually perform the transfer most effec-\narXiv:1611.04687v2  [cs.NE]  5 Dec 2016\nFully connected\nOutput\nLow-\n \ndir\ng la\nn\nois\nn\ne\nm\nid\ne\nc\na\np\ns \ne\nr\nu\ntc\nu\nrts\nHigh-\nh\np\na\nr\ng la\nn\nois\nn\ne\nm\nid\ne\nc\na\np\ns \ne\nr\nu\ntc\nu\nrts\n…\nInput\nHidden layers\n…\nConvolution\nPooling\nf\nF\n…\n…\nGraph spectrum transform \nby graph Laplacian\nConvolution on graphs\nPooling\nFully connected\nBy transfer learning of intrinsic geometric information\nDomain\n≈\nFigure 1: Conventional CNN works on a regular grid domain (top); proposed transfer learning framework for CNN, which can\ntransfer intrinsic geometric information obtained from a source graph domain to a target graph domain (bottom).\ntively? This paper tries to address these questions.\nTo demonstrate the effectiveness of our approach, we\ntested it with large-scale public NLP datasets for text classi-\nﬁcation (Zhang, Zhao, and LeCun 2015). Each dataset con-\ntained a corpus of news articles, Internet reviews, or ontol-\nogy entries. We represented a dataset (e.g., Amazon reviews)\nwith a graph to capture the interactions among the words in\nthe dataset. We then used the spectral CNN (SCNN) (Bruna\net al. 2013; Henaff, Bruna, and LeCun 2015) to model the\ngraph using neural networks. The learned model can be\nused for classifying unseen texts from the same data source\n(Amazon). Furthermore, our experimental results conﬁrmed\nthat our transfer learning methodology allows us to implic-\nitly derive a model for classifying texts from another source\n(e.g., Yelp reviews) without collecting new data and without\nrepeating all the learning procedures from scratch.\nOur speciﬁc contributions can be summarized as follows:\n• We proposed a new transfer learning framework for deep\nlearning on input data in non-grid structure such as\ngraphs. To the best of the authors’ knowledge, this work is\nthe ﬁrst attempt of its kind. Adopting our approach will re-\nlieve the burden of re-collecting data and re-training mod-\nels for related tasks.\n• To address Q1, we investigated the conditions for success-\nful knowledge transfers between graph domains. We con-\njectured that two graphs with similar structural character-\nistics would give better results and conﬁrmed it by com-\nparing graph similarity and transfer learning accuracy.\n• To answer Q2, we tested diverse alternatives to the com-\nponents of the proposed framework: graph generation, in-\nput representation, and deep network construction. In par-\nticular, to improve the SCNN model for extracting data-\ndriven structural features from graphs, we analyzed and\noptimized the key factors that affect the performance of\nSCNN (e.g., the method to quantify spectral features of a\ngraph).\n• We performed an extensive set of experiments, using both\nsynthetic and real-world data, to show the effectiveness of\nour approach.\nRelated Work\nGraphs can provide a general way of representing the di-\nverse interactions of entities and have been studied exten-\nsively (Sonawane and Kulkarni 2014). In addition to studies\non representation and quantiﬁcation of relations and similar-\nities (Koutra, Vogelstein, and Faloutsos 2013), various stud-\nies focused on large-scale graph data and use of structural\ninformation. Recently, deep learning methods to automati-\ncally extract structural characteristics from graphs have been\nproposed (Duvenaud et al. 2015; Li et al. 2015).\nExamples of deep learning applied to non-grid, non-\nEuclidean space include graph wavelets from applying deep\nauto-encoders to graphs and using the properties of auto-\nmatically extracted features (Rustamov and Guibas 2013),\nanalysis of molecular ﬁngerprints of proteins saved as\ngraphs (Duvenaud et al. 2015), and a CNN-based model for\nhandling tree structures in the context of programming lan-\nguage processing (Mou et al. 2016).\nParticularly relevant to our approach is the localized\nSCNN model (Boscaini et al. 2015), which is a deep learn-\ning approach that can extract the properties of deformable\nshapes. The generalized SCNN model (Bruna et al. 2013;\nHenaff, Bruna, and LeCun 2015), a key component of our\nframework, borrowed the Fourier transform concept from\nthe signal processing ﬁeld in order to apply CNNs in a grid\ndomain to a graph-structured domain. In this model, the con-\nvolutional operation was re-deﬁned for graphs.\nMore recently, the restricted Boltzmann machine (LeCun,\nBengio, and Hinton 2015) was used to learn structural fea-\ntures from graphs in an unsupervised manner for classiﬁca-\ntion (Niepert, Ahmed, and Kutzkov 2016). For efﬁcient and\nscalable semi-supervised classiﬁcation of graph-structured\ndata, the ﬁrst-order approximation of spectral graph convo-\nlutions was utilized (Kipf and Welling 2016). Our method\ncould adopt these approaches as its base learning model to\nimprove the effectiveness of transfer learning.\nProposed Method\nFig. 2 presents a diagram illustrating the overall ﬂow of the\nproposed method, which consists of ﬁve steps, A–E. The\nﬁrst three steps are to produce a graph from input and to\nidentify unique structural features from the graph. The last\ntwo steps are to apply transfer learning based on the learned\nfeatures and graph similarity to carry out inference.\nStep A: Graph Production\nWe represent data elements of input data and their interac-\ntions and relations as nodes and edges, respectively, in a\ngraph. From an input dataset, we construct an undirected,\nconnected, and weighted graph G = (V, E, A), where V\nand E represent the sets of vertices and edges, respectively,\nand A denotes the weighted adjacency matrix. Assume that\n|V | = N and |E| = M.\nWe utilize two recent techniques to derive a graph (more\nspeciﬁcally, the edge set E) from input data: co-occurrence\ngraph estimation (CoGE) (Sonawane and Kulkarni 2014)\nand supervised graph estimation (SGE) (Henaff, Bruna, and\nLeCun 2015). CoGE directly quantiﬁes the closeness of data\nelements based on the frequency of co-occurrence, while\nSGE automatically learns a similarity features among ele-\nments through a fully connected network model.\nB: Representation of Graphs in Spectral Domain\nWe extract the intrinsic geometric characteristics of the en-\ntire graph by deriving (non-)normalized Laplacian matrix L\nof the graph constructed in step A. For a graph domain, L\nprovides the values for graph spectral bases in the convolu-\ntion operation of SCNN (Mohar 1997; Koutra, Vogelstein,\nand Faloutsos 2013).\nWe consider three types of L: the non-normalized Lapla-\ncian (Lbasic), the random walk-based normalized Laplacian\n(Lrw), and the random walk with restart based normalized\nLaplacian (Lrwr) given by (Tong, Faloutsos, and Pan 2006):\nLbasic = D −A\n(1)\nLrw = D−1(D −A) = I −D−1A\n(2)\nLrwr = [I + ϵ2D −ϵA]−1 ≈[I −ϵA]−1\n(3)\n≈I + ϵA + ϵ2A2 + · · ·\n(4)\nwhere D represents the degree matrix1 of the graph, and ϵ\nrepresents the probability of restart. Note that the approxi-\nmation in Eq. (3) is attained by attenuating neighboring in-\nﬂuence, while the approximation in Eq. (4) is attained by\nbelief propagation and its fast approximation.\nL is a symmetric matrix that can be decomposed through\nthe diagonalization by combining eigenvalues λl and the\n1A diagonal matrix that shows the degree (i.e., the number of\nedges attached) of each node.\ncorresponding orthogonal eigenvectors ul(n), where l is the\norder of an eigenvalue, and n ∈[1, N] is the index of a\nnode (Mohar 1997).\nRecall that a function f : V 7→R deﬁned on the nodes\nof graph G can be represented by a vector f ∈RN with\nthe n-th dimension of f indicating the value at the n-th ver-\ntex in V (Shuman et al. 2013; Shuman, Ricaud, and Van-\ndergheynst 2016). As in the Fourier transform, the eigen-\nfunctions of L represent the function f deﬁned by the nodes\nin the graph: fG(n) = PN−1\nl=0\nˆfG(λl)ul(n) ↔ˆfG(λl) =\nPN\nn=1 fG(n)ul(n), where ˆf, the transformed function of\nf, is represented by a set of basis eigenvectors. The Par-\nseval’s theorem also holds (i.e., the energy of the trans-\nformed function is the same as that of the original func-\ntion), and ⟨f, g⟩= ⟨ˆf, ˆg⟩for two functions f and g, verify-\ning the consistency between the two domains (Chung 1997;\nShuman, Ricaud, and Vandergheynst 2016).\nThis indicates that an input function deﬁned on the ver-\ntex domain of a graph can be converted into the correspond-\ning graph spectral domain by using the concept of Fourier\nanalysis on graphs. The generalized convolutional operation\n(denoted by ∗G) of functions f and g can be deﬁned by the\ndiagonalized linear multiplication in the spectral domain as\nfollows (Bruna et al. 2013; Henaff, Bruna, and LeCun 2015;\nShuman, Ricaud, and Vandergheynst 2016):\n(f ∗G g)(n) =\nN−1\nX\nl=0\nˆf(λl)ˆg(λl)ul(n)\nwhich can also be expressed as\nf ∗G g = ˆg(L)f = U\n\n\nˆg(λ0)\n. . .\n0\n...\n...\n...\n0\n. . .\nˆg(λN−1)\n\nU T f\n(5)\nwhere U is a matrix having the eigenvectors of the graph\nLaplacian in its columns that quantify the intrinsic structural\ngeometry of the entire graph domain and serve as the spec-\ntral bases of a graph. This matrix functions as the Fourier\ntransform into the graph spectral domain. In this regard, a\nreceptive ﬁlter learned through the convolution operation in\na convolution layer of a CNN in a regular grid domain can\nbe regarded as a matrix on g, which is diagonalized by ˆg(λi)\n(0 ≤i ≤N −1) elements on input f deﬁned in the graph\ndomain provided by Eq. (5).\nFor conventional CNNs, the j-th output xk+1,j from the\nk-th layer is deﬁned as\nxk+1,j = h\n\n\nφk−1\nX\ni=1\nFk,i,j ∗xk,i\n\n, j = 1, . . . , φk\n(6)\nwhere φk is the number of feature maps, xk is the input of\nk-th layer, h is a nonlinear function, and Fk,i,j is the ﬁlter\nmatrix from the i-th feature map to the j-th feature map.\nFor the SCNN, the transform of input xk of size n×φk−1\nLayers on model\nInput in source \ndomain\n(Data set A)\nInput in target \ndomain\n(Data set B)\nCoGE\n/SGE\nCoGE\n/SGE\nGraph \nLaplacian \nConvolution \non Graph\n(A) Producing graph\n(C) Applying convolution networks to graph\nPooling\nFully \nconnected\nOutput\n…\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\n(B) Representation of graphs in spectral domain\nn\nn\nn\nn\nn\nn\n…\nSolid line: training process\nD\ns\ns\ne\nc\no\nr\np\n \ng\nnin\nr\na\nel r\ne\nfs\nn\na\nrt :\ne\nnil \nd\ne\ntt\no\n(E) Transfer learning in spectral domain\n(D) Learning transferable features\nFigure 2: Overview of the proposed method.\ninto output xk+1 of size n × φk is given by\nxk+1,j = h\n\nU\nφk−1\nX\ni=1\nFk,i,jU T xk,i\n\n, j = 1, . . . , φk\n(7)\nwhere h is a nonlinear function, and Fk,i,j is a diagonal ma-\ntrix. This implies that training the weights of learnable ﬁl-\nters are the same as training the multipliers on the eigenval-\nues of the Laplacian (Bruna et al. 2013; Henaff, Bruna, and\nLeCun 2015). This characterizes the SCNN, a generalized\nCNN model that has several ﬁlter banks through generalized\nconvolutional operations on a graph.\nWe augment the SCNN model so that it can support\nspatial locality, which is made independent of input size\nby using windowed smoothing ﬁlters. They are deﬁned as\nˆPk(l) = PK\nk=0 akλk\nl for K < N, based on the polyno-\nmial kernel ak with degree K (Shuman, Ricaud, and Van-\ndergheynst 2016). This is based on the fact (originally ob-\nserved in signal processing) that the smoothness in the spec-\ntral domain can have spatial decay or local features in the\noriginal domain. We implement this idea using the eigen-\nvectors of the subsampled Laplacian as the low-frequency\neigenvectors of the Laplacian (Boscaini et al. 2015).\nC: Applying Convolutional Networks to Graphs\nWe train the SCNN model by using the information obtained\nthrough the previous steps to represent the geometric infor-\nmation of local behaviors from the surface of a structural\ngraph domain. The model has a hierarchical structure con-\nsisting of layers for convolutional and pooling, and a fully\nconnected layer as shown in Fig. 2. The training determines\nthe weights of each layer by minimizing the task-speciﬁc\ncost (loss) function. The model can learn various data-driven\nfeatures by re-deﬁning the convolution operation with the\nspectral information of the structural graph domain (Bruna\net al. 2013; Henaff, Bruna, and LeCun 2015).\nD: Learning Transferable Features\nOnce the model training is completed, it contains data-\ndriven features for the graph-structured data derived from\nthe input in steps A and B. As stated in Introduction, the\ncore of our proposal is to transfer the information on struc-\ntural characteristics of a graph learned by deep learning. The\nfeatures learned in step C provide this information.\nE: Transfer Learning in Spectral Domain\nAccording to (Pan and Yang 2010), a domain in the context\nof transfer learning consists of a feature space X and a prob-\nability distribution P(X), where X ∈X. Given a domain\nD = {X, P(X)}, we can denote a task by T = {Y, f(·)}\nwith a label space Y and a predictive function f(·) that is\nlearned from training data {x, y} where x ∈X and y ∈Y.\nThe objective of general transfer learning is then to improve\nlearning fT (·) in the target domain DT by exploiting the\nknowledge in the source domain DS and task TS.\nIn the present context, we transfer the intrinsic geometric\ninformation learned from the graph GS encoding the knowl-\nedge in DS and TS in steps A–D. We skip the steps to gener-\nate GT for TT in DT as well as the steps to extract the struc-\ntural characteristics therefrom. Under the condition that GS\nand GT bear structural similarities, we can directly build a\nmodel for TT by (1) copying the convolutional and pooling\nlayers that contain the features trained for TS in DS, and by\n(2) training the fully connected layer of the model for ﬁne\ntuning weights for TT in DT .\nThis way of transfer learning provides efﬁciency in learn-\ning and also helps to minimize the problems resulting from\nlack of data and imperfect structural information for the new\ntask. Note that the proposed method guarantees the spectral\nhomogeneity of graphs by using the union of node sets on\nthe heterogeneous source and target datasets. In our method,\nit is possible to utilize the spectral features of graphs from\nheterogeneous datasets.\nTable 1: Details of the real-world datasets used\nAG\nDBP\nYELP\nAMAZ\nTrain\n120, 000\n560, 000\n580, 000\n3, 600, 000\nTest\n7, 600\n70, 000\n38, 000\n400, 000\n#class\n4\n14\n2\n2\nName\nsim(G1, G2)\n∗[corr(wv1, wv2)\n†]\nAG\n0.37[0.45]\n0.28[0.36]\n0.35[0.42]\nDBP\n0.37[0.45]\n0.23[0.29]\n0.33[0.40]\nYELP\n0.28[0.36]\n0.23[0.29]\n0.50[0.58]\nAMAZ\n0.35[0.42]\n0.33[0.40]\n0.50[0.58]\nAG: a corpus of news articles on the web; DBP: ontology data from DBpedia;\nYELP: reviews from Yelp; AMAZ: reviews from Amazon.\n∗sim(G1, G2) = 0 indicates that two graphs G1 and G2 are structurally\ncomplementary, whereas the value of 1 means that they are identical.\n† corr(wv1, wv2) represents the correlation between the log-normalized bag\nof words extracted from each of the text corpora.\nResults and Discussion\nWe tested the proposed method by performing topic classi-\nﬁcation of text documents. Text data carry information on\nnot only individual words but also on their relationships,\nand graph-based methods are widely used for text mining.\nWe utilized large-scale public NLP data (Zhang, Zhao, and\nLeCun 2015), which contained multiple corpora of news ar-\nticles, Internet reviews, or ontology entries (Table 1). For\ncontrolled experiments, we also generated two pairs of syn-\nthetic datasets by random sampling of the real corpora. One\npair consisted of two corpora with high similarity, and the\nother pair consisted of two corpora with low similarity.\nFor measuring the structural similarity between graphs, as\nshown in Table 1 and Fig. 3, we used the methods reported\nby existing studies (Koutra, Vogelstein, and Faloutsos 2013;\nLee, Kim, and Yoon 2015); refer to the note below Table 1\nfor more details. Note that YELP and AMAZ bear the high-\nest similarity in terms of the metrics used.\nWe implemented the deep networks with Torch and Cuda\nusing AdaGrad as the optimizer and ReLU as the activation.\nWe carried out 10-fold cross validation. Note that the pro-\nposed method can offer an efﬁcient training scheme with\nrelatively low computation cost of O(n2.376) by leaving out\nthe eigenvalue decomposition on the SCNN and re-using the\nmodel trained by the data in the source domain. In our ex-\nperiments, the proposed method provided more than 10%\nreduction in the average training time.\nUsing the above setting, we ﬁrst carried out comprehen-\nsive experiments to determine what factors affected the per-\nformance of the SCNN model for graph modeling. Table 2\nlists part of the results we obtained by varying the net ar-\nchitecture, the method to generate graphs, and the type of\nLaplacian matrix along with the resulting classiﬁcation ac-\ncuracy for each combination. We can observe from Table 2\nthat the Laplacian methods do not signiﬁcantly affect the\nperformance, but Lrwr had the beneﬁt in terms of computa-\ntional complexity. SGE tended to give more accurate results\nthan CoGE, which implies that the initial graph generation\naffected the model training more critically than structural\nTable 2: Performance of SCNN model with various hyper-\nparameters for text topic classiﬁcation task\nModel\nGraph\nL\nClassiﬁcation accuracy\narchitecture\n∗\ngener.\ntype\nAG\nDBP\nYELP\nAMAZ\nGC8-FC500\nCoGE\nLbasic\n0.89\n0.95\n0.91\n0.88\nGC8-FC500\nCoGE\nLrw\n0.89\n0.95\n0.91\n0.88\nGC8-FC500\nCoGE\nLrwr\n0.89\n0.93\n0.90\n0.88\nGC8-FC500\nSGE\nLbasic\n0.91\n0.97\n0.91\n0.89\nGC8-FC500\nSGE\nLrw\n0.91\n0.97\n0.91\n0.88\nGC8-FC500\nSGE\nLrwr\n0.89\n0.95\n0.91\n0.89\nGC8-FC1K\nCoGE\nLbasic\n0.90\n0.95\n0.93\n0.88\nGC8-FC1K\nCoGE\nLrw\n0.90\n0.97\n0.92\n0.89\nGC8-FC1K\nCoGE\nLrwr\n0.89\n0.96\n0.92\n0.89\nGC8-FC1K\nSGE\nLbasic\n0.91\n0.97\n0.92\n0.88\nGC8-FC1K\nSGE\nLrw\n0.91\n0.97\n0.92\n0.88\nGC8-FC1K\nSGE\nLrwr\n0.89\n0.96\n0.91\n0.88\nGC8-GC8-FC1K\nCoGE\nLbasic\n0.89\n0.96\n0.92\n0.89\nGC8-GC8-FC1K\nCoGE\nLrw\n0.89\n0.97\n0.92\n0.89\nGC8-GC8-FC1K\nCoGE\nLrwr\n0.89\n0.97\n0.92\n0.88\nGC8-GC8-FC1K\nSGE\nLbasic\n0.91\n0.97\n0.92\n0.88\nGC8-GC8-FC1K\nSGE\nLrw\n0.91\n0.97\n0.92\n0.88\nGC8-GC8-FC1K\nSGE\nLrwr\n0.89\n0.97\n0.92\n0.89\n∗For training, we set the kernel degree K = 60, learning rate to 0.01 and used\ncross-entropy cost function with AdaGrad optimizer. GC8 means the use of\ngraph convolutional layers with 8 feature maps, and FC500/FC1K means the\nuse of fully connected layer with 500/1000 hidden units.\nfeature extraction. For the experiments shown in Table 2,\nthe GC8-GC8-FC1K model (refer to the note below Table 2\nfor notation) gave the best results, and we used this model\nas our main learning model in the following experiments.\nWe then performed experiments to determine the effec-\ntiveness of transfer learning using the synthetic datasets.\nThe results are shown in Fig. 3; the plots in the top row\nare from the pair of synthetic corpora with high similar-\nity [sim(G1, G2) = 0.75 and corr(wv1, wv2) = 0.95]\nfor varying quantities of ﬁne-tuning data for training the\ntransferred model in the target domain (1%, 3%, 5%, and\n10% of the entire target data). The plots in the bottom row\nof Fig. 3 correspond to the results from the pair of syn-\nthetic corpora with low similarity [sim(G1, G2) = 0.30 and\ncorr(wv1, wv2) = 0.50]. We can observe that transfer learn-\ning is more effective for the higher similarity case, in which\nthe test accuracy of the transferred model increased signif-\nicantly faster than that of the source domain model. Using\nonly 1% of the target domain data was sufﬁcient for train-\ning, and using more data did not provide a noticeable differ-\nence. For the lower similarity case, the training in the target\ndomain was limited and could not deliver the same level of\naccuracy in the source domain due to discrepancies in the\nunderlying structure between the source and target domains.\nFinally, we tested our approach with four corpora (AG,\nDBP, YELP, and AMAZ) as shown in Fig. 4. The plots in\nthe top row represent the test accuracy of the model trained\nwith the original data (solid line) and those of the transferred\nmodel trained with each of the other data (dotted line). The\nbottom plots represent the test loss. For the two corpora with\nthe highest level of similarity (YELP and AMAZ), the effect\nPercentage of the target dataset used for training the transferred model\nT\ny\nc\na\nr\nu\nc\nc\na\n ts\ne\n1%\n5%\n10%\n3%\nG\n(\nm\ni\nS\n1,G2\n \n5\n7\n.\n0\n=\n)\n[Corr(WS1, WS2\n]\n5\n9\n.\n0\n=\n)\nG\n(\nm\ni\nS\n1,G2\n \n0\n3\n.\n0\n=\n)\n[Corr(WS1, WS2\n]\n0\n5\n.\n0\n=\n)\nEpochs\nsource\ntarget\nsource\ntarget\nFigure 3: Results of intrinsic geometric information transfer learning for synthetic datasets (best viewed in color). Top: source\nand target datasets have high similarity in graph representations. Bottom: source and target datasets have low similarity. Each\ncolumn: the percentage (1%, 3%, 5% and 10%) of the target dataset used for training the transferred model (ﬁne tuning the\nfully connected layer). We repeated every experiment 10 times, and each data point shows a boxplot; red (source domain) and\nblue (target domain) lines connect the median locations of the boxplots.\nof transfer learning was most salient. The test accuracy of\nthe transferred model was comparable to that of the source\nmodel (for YELP) or was only 5–8% lower (for AMAZ).\nFor the other cases with lower similarity than YELP and\nAMAZ, transfer learning was less effective. These results\nagain conﬁrmed our observation that the knowledge transfer\nis most successful when the source and target domains have\nhigh level of structural similarity between underlying graph\nrepresentations.\nConclusion\nWe have proposed a new transfer learning framework for\ndeep learning on graph-structured data. Our approach can\ntransfer the intrinsic geometric information learned from\nthe graph representation of the source domain to the target\ndomain. We observed that the knowledge transfer between\ntasks domains is most effective when the source and target\ndomains possess high similarity in their graph representa-\ntions. We anticipate that adoption of our methodology will\nhelp extend the territory of deep learning to data in non-grid\nstructure as well as to cases with limited quantity and quality\nof data. To prove this, we are planning to apply our approach\nto diverse datasets in different domains.\nAcknowledgments\nThis work was supported by Ministry of Science, ICT & Fu-\nture Planning [No.2016M3A7B4911115, No.PA-C000001],\nthe Institute for Information Communications Technology\nPromotion [No.B0101-16-0307], the Brain Korea 21 Plus\nProject in 2016 grant funded by the Korea government, and\nSamsung Research Funding Center of Samsung Electronics\n[No.SRFC-IT1601-05].\nReferences\n[Boscaini et al. 2015] Boscaini, D.; Masci, J.; Melzi, S.;\nBronstein, M. M.; Castellani, U.; and Vandergheynst, P.\n2015.\nLearning class-speciﬁc descriptors for deformable\nshapes using localized spectral convolutional networks. In\nComputer Graphics Forum, volume 34, 13–23. Wiley On-\nline Library.\n[Bruna et al. 2013] Bruna, J.; Zaremba, W.; Szlam, A.; and\nLeCun, Y. 2013. Spectral networks and locally connected\nnetworks on graphs. arXiv preprint arXiv:1312.6203.\n[Chung 1997] Chung, F. R. 1997. Spectral graph theory,\nvolume 92. American Mathematical Soc.\n[Deo 2016] Deo, N. 2016. Graph theory with applications\nto engineering and computer science. Courier Dover Publi-\ncations.\n[Duvenaud et al. 2015] Duvenaud, D. K.; Maclaurin, D.;\nIparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik,\nA.; and Adams, R. P.\n2015.\nConvolutional networks on\ngraphs for learning molecular ﬁngerprints. In Advances in\nNeural Information Processing Systems, 2215–2223.\n[Henaff, Bruna, and LeCun 2015] Henaff, M.; Bruna, J.; and\nLeCun, Y. 2015. Deep convolutional networks on graph-\nstructured data. arXiv preprint arXiv:1506.05163.\n[Jain et al. 2015] Jain, A.; Zamir, A. R.; Savarese, S.; and\nSaxena, A. 2015. Structural-RNN: Deep learning on spatio-\ntemporal graphs. arXiv preprint arXiv:1511.05298.\nAG\nDBP\nYELP\nAMAZ\nTarget\ndomain\nT\ny\nc\na\nr\nu\nc\nc\na\n ts\ne\nT\ns\ns\nol ts\ne\nEpochs\nFigure 4: Results of the proposed method on real-world datasets (best viewed in color). Each plot in the top row: test accuracy of\nthe model trained with the original data (solid lines) and those of the models trained with the other data sources and transferred\n(dotted lines). Each plot in the bottom row: test loss. For YELP and AMAZ, transfer learning was most effective, given that\nthey have the highest level of structural similarity of all the cases (see Table 1).\n[Kipf and Welling 2016] Kipf, T. N., and Welling, M. 2016.\nSemi-supervised classiﬁcation with graph convolutional net-\nworks. arXiv preprint arXiv:1609.02907.\n[Koutra, Vogelstein, and Faloutsos 2013] Koutra, D.; Vogel-\nstein, J. T.; and Faloutsos, C. 2013. DeltaCon: A principled\nmassive-graph similarity function. SIAM.\n[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,\nA.;\nSutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁca-\ntion with deep convolutional neural networks. In Advances\nin Neural Information Processing Systems, 1097–1105.\n[LeCun, Bengio, and Hinton 2015] LeCun, Y.; Bengio, Y.;\nand Hinton, G.\n2015.\nDeep learning.\nNature\n521(7553):436–444.\n[Lee, Kim, and Yoon 2015] Lee, J.; Kim, G.; and Yoon, S.\n2015. Measuring large-scale dynamic graph similarity by\nRICom: RWR with intergraph compression. In IEEE Inter-\nnational Conference on Data Mining, 829–834.\n[Li et al. 2015] Li, Y.; Tarlow, D.; Brockschmidt, M.; and\nZemel, R. 2015. Gated graph sequence neural networks.\narXiv preprint arXiv:1511.05493.\n[Mohar 1997] Mohar, B.\n1997.\nSome applications of\nLaplace eigenvalues of graphs. Springer.\n[Mou et al. 2016] Mou, L.; Li, G.; Zhang, L.; Wang, T.; and\nJin, Z. 2016. Convolutional neural networks over tree struc-\ntures for programming language processing. In AAAI Con-\nference on Artiﬁcial Intelligence.\n[Niepert, Ahmed, and Kutzkov 2016] Niepert, M.; Ahmed,\nM.; and Kutzkov, K. 2016. Learning convolutional neural\nnetworks for graphs. arXiv preprint arXiv:1605.05273.\n[Pan and Yang 2010] Pan, S. J., and Yang, Q. 2010. A survey\non transfer learning. IEEE Transactions on Knowledge and\nData Engineering 22(10):1345–1359.\n[Rustamov and Guibas 2013] Rustamov, R., and Guibas,\nL. J.\n2013.\nWavelets on graphs via deep learning.\nIn\nAdvances in Neural Information Processing Systems, 998–\n1006.\n[Schmidhuber 2015] Schmidhuber, J. 2015. Deep learning in\nneural networks: An overview. Neural Networks 61:85–117.\n[Shuman et al. 2013] Shuman, D. I.; Narang, S. K.; Frossard,\nP.; Ortega, A.; and Vandergheynst, P. 2013. The emerg-\ning ﬁeld of signal processing on graphs: Extending high-\ndimensional data analysis to networks and other irregular\ndomains. IEEE Signal Processing Magazine 30(3):83–98.\n[Shuman, Ricaud, and Vandergheynst 2016] Shuman, D. I.;\nRicaud, B.; and Vandergheynst, P. 2016. Vertex-frequency\nanalysis on graphs. Applied and Computational Harmonic\nAnalysis 40(2):260–291.\n[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.;\nGuez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser,\nJ.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al.\n2016. Mastering the game of Go with deep neural networks\nand tree search. Nature 529(7587):484–489.\n[Sonawane and Kulkarni 2014] Sonawane, S., and Kulkarni,\nP. 2014. Graph based representation and analysis of text\ndocument: A survey of techniques. International Journal of\nComputer Applications 96(19).\n[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.;\nand Le, Q. V. 2014. Sequence to sequence learning with\nneural networks. In Advances in Neural Information Pro-\ncessing Systems, 3104–3112.\n[Tong, Faloutsos, and Pan 2006] Tong, H.; Faloutsos, C.;\nand Pan, J.-Y.\n2006.\nFast random walk with restart and\nits applications. In IEEE International Conference on Data\nMining, 613–622.\n[Zhang, Zhao, and LeCun 2015] Zhang, X.; Zhao, J.; and\nLeCun, Y. 2015. Character-level convolutional networks\nfor text classiﬁcation. In Advances in Neural Information\nProcessing Systems, 649–657.\n",
  "categories": [
    "cs.NE"
  ],
  "published": "2016-11-15",
  "updated": "2016-12-05"
}