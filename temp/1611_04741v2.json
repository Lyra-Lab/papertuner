{
  "id": "http://arxiv.org/abs/1611.04741v2",
  "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference",
  "authors": [
    "Biswajit Paria",
    "K. M. Annervaz",
    "Ambedkar Dukkipati",
    "Ankush Chatterjee",
    "Sanjay Podder"
  ],
  "abstract": "In this work we use the recent advances in representation learning to propose\na neural architecture for the problem of natural language inference. Our\napproach is aligned to mimic how a human does the natural language inference\nprocess given two statements. The model uses variants of Long Short Term Memory\n(LSTM), attention mechanism and composable neural networks, to carry out the\ntask. Each part of our model can be mapped to a clear functionality humans do\nfor carrying out the overall task of natural language inference. The model is\nend-to-end differentiable enabling training by stochastic gradient descent. On\nStanford Natural Language Inference(SNLI) dataset, the proposed model achieves\nbetter accuracy numbers than all published models in literature.",
  "text": "A Neural Architecture Mimicking Humans\nEnd-to-End\nfor Natural Language Inference\nBiswajit Paria\nIIT Kharagpur\nbiswajitsc@iitkgp.ac.in\nAnnervaz K M, Ambedkar Dukkipati\nIndian Institute of Science, Bangalore\n{annervaz.km,ad}@csa.iisc.ernet.in\nAnkush Chatterjee\nIIT Kharagpur\nankushchatterjee@iitkgp.ac.in\nSanjay Podder\nAccenture Technology Labs\nsanjay.podder@accenture.com\nAbstract—In this work we use the recent advances in represen-\ntation learning to propose a neural architecture for the problem\nof natural language inference. Our approach is aligned to mimic\nhow a human does the natural language inference process given\ntwo statements. The model uses variants of Long Short Term\nMemory (LSTM), attention mechanism and composable neural\nnetworks, to carry out the task. Each part of our model can be\nmapped to a clear functionality humans do for carrying out\nthe overall task of natural language inference. The model is\nend-to-end differentiable enabling training by stochastic gradient\ndescent. On Stanford Natural Language Inference(SNLI) dataset,\nthe proposed model achieves better accuracy numbers than all\npublished models in literature.\nI. INTRODUCTION AND MOTIVATION\nThe problem of Natural Language Inference (NLI) is to\nidentify whether a statement (hypothesis: H) in natural lan-\nguage can be inferred or contradicted in the context of another\nstatement (premise: P) in natural language. If it can neither\nbe inferred nor contradicted, we say hypothesis is ‘neutral’\nto premise. NLI is one of the most important component\nfor natural language understanding systems [Benthem, 2008;\nMacCartney and Manning, 2009]. NLI has multitude of appli-\ncations in natural language question answering [Harabagiu and\nHickl, 2006], semantic search, text summarization [Lacatusu\net al., 2006] etc.\nConsider\nthe\nthree\nstatements\nA:\nThe couple\nis\nwalking\non\nthe\nsea shore.\nB:\nThe man and\nwoman\nare\nwide awake.\nC:\nThe man and woman\nare\nshopping on the busy street.\nHere\nthe\nstatement A is the premise and, B and C both are hypotheses.\nB can be inferred from A, where as it is reasonably clear C\ncannot be true if A were. A and C can be true together, in a\nstrict sense, by arguing that there was a busy shopping option\nby the sea shore, which is not true generally. The problem of\nNLI thus falls in more “common sense reasoning” segment\ncompared to strict logical inferencing and is subtly different\nfrom deduction in formal logical setting [MacCartney, 2009].\nUnsupervised feature learning and deep learning [Bengio,\n2009; LeCun et al., 2015] based on neural networks have\ngained prominence in the last few years. State of the art neural\nnetworks models and appropriate algorithms to train these\nmodels have been proposed for multitude of tasks in computer\nvision, natural language processing, speech recognition etc\nand these models hold benchmark results for most problems.\nIn the area of natural language processing, the recent deep\nlearning models have been proven superior to conventional\nrule-based or machine learning approaches in many tasks like\npart of speech tagging, question answering, sentiment analysis,\ndocument classiﬁcation [Kumar et al., 2015] etc. Not only\ndeep learning models hold the state of the art results for\nthese problems, many model constructs used like attention\nmechanism have close alignment with human thought process.\nMotivated by the same, we dissect the problem of NLI into\nvarious sub tasks, similar to how human carries out NLI. We\nthen realize each sub tasks using a deep learning construct,\nweave them together to create a complete end-to-end model\nfor NLI. Let us ﬁrst see how we can dissect the problem of NLI\nas humans do it. When seeing the two statements A and B as\nin the example above, humans ﬁrst aligns information snippets\nbetween the sentences like (the couple, the man and\nwoman) and (walking, wide awake). We notice that\nﬁrst pair is equivalent. From the second pair we conclude\nthat walking is possible only in the state of being awake.\nFrom the results of these two different kinds of processing we\nconclude sentence B can be inferred from sentence A. Suppose\nin A if it were dog instead of couple, it would not have\nbeen equivalent, we could not have inferred B even though\nthe second pair results are the same. Each pair results are\nimportant, some cases they are independent, but in most cases\nthey are dependent as humans make use of a lot of contextual\ninformation. We analyze shopping on a street is not possible\nat sea shore and conclude C is contradicted by A. Note that\nfor inferring B, we never paid attention to where the couple\nwere walking, but to contradict C, we paid attention to the\nplace. Humans ﬁrst align the needed information according to\nthe context, compare each pair differently by making use of\nthe contextual information and then deduce ﬁnally by making\nuse of each of the comparison results.\nThe main contributions of this paper are as follows.\n1) A neural architecture using variants of long short term\nmemory, composable neural networks and attention\nmechanism is proposed for the problem of natural lan-\nguage inference.\n2) The model is inspired from how humans carry out\nthe task of natural language inference and hence very\narXiv:1611.04741v2  [cs.CL]  27 Jan 2017\nintuitive. Each step of the humans in performing NLI is\nmimicked by an appropriate deep learning construct in\nthe model.\n3) We present detailed experimental results on Stanford\nNatural Language Inference(SNLI) Dataset [Bowman\net al., 2015], and shows that proposed model outper-\nforms all the other models\nII. PRELIMINARIES AND BACKGROUND\nIn a deep learning framework, the natural language sen-\ntences are converted into a numerical representation by word\nembeddings, in the ﬁrst place. This numerical representations\nare then encoded by using a bi-directional LSTM or a binary\ntree LSTM, to consider various information snippets along\nwith the context in which they appear. Attention mechanism\nis used to learn the parts of the information that needs to be\naligned and processed together according to the context. The\ngenerated pairs by attention mechanism are then processed\nseparately using a set of different operators selected by soft\ngating. The outputs of the different process pairs are then\naggregated or composed together for the ﬁnal prediction task.\nBelow we brieﬂy describe concepts of word embeddings and\nLSTMs. Attention mechanism and composition, and their\nmotivations are introduced along with the model.\nA. Word Embeddings\nThe ﬁrst challenge encountered in applying deep learning\nmodels for NLP is to ﬁnd a correct numerical representation\nfor words. “You shall know a word by the company it keeps”\n(Firth, J. R. 1957:11), is one of the most inﬂuential ideas in\nnatural language processing. Multiple models for representing\na word as a numerical vector, based on the context it appears,\nstem from this idea. Many vector representations for words\nhave been proposed, including the well known latent semantic\nindexing [Dumais, 2004]. Vector representations for words in\nthe context of neural networks was proposed in [Bengio et al.,\n2003]\nIn this paper, each word in the vocabulary is assigned a\ndistributed word feature vector, w ∈Rm. The probability\ndistribution of word sequences, P(wt|wt−(n−1), . . . , wt−1),\nis then expressed in terms of these word feature vectors.\nThe word feature vectors and parameters of the probability\nfunction (a neural network) are learned together by training\na suitable feed-forward neural network to maximize the log-\nlikelihood of the text corpora, considering each text snippet\nof ﬁxed window size as a training sample. [Mikolov et al.,\n2013a] adapted this model and proposed two new models:\nContinuous bag of words and skip-gram model, popularly\nknown as ‘Word2Vec’ models. Continuous bag of word models\ntry to predict the current word given the previous and next\nsurrounding words, discarding the word order, in a ﬁxed\ncontext window. Skip-gram model tries to predict the sur-\nrounding words given the current word. These models have\nbetter training complexity, and thus can be used for training on\nlarge corpus. The vectors generated by these models on large\ncorpus have shown to capture subtle semantic relationships\nbetween words, by simple vector operations on them [Mikolov\net al., 2013b]. The drawback of these models is that they\nmostly use local information (words in a contextual window).\nTo effectively utilize the aggregated global information from\nthe corpus without incurring high computational cost, ‘GloVe’\nword vectors were proposed by [Pennington et al., 2014]. This\nmodel tries to create word vectors such that dot product of\ntwo vectors will closely resemble the co-occurrence statistics\nof the corresponding words in the full corpus. The model have\nshown to be more effective compared to Word2Vec models for\ncapturing semantic regularities on smaller corpus.\nB. Recurrent Neural Network and Long Short Term Mem-\nory(LSTM)\nThe basic idea behind Recurrent Neural Networks (RNN)\nis to capture and encode the information present in a given\nsequence like text. Given a sequence of words, a numerical\nrepresentation (GloVe or Word2Vec vectors) for a word is\nfed to a neural network and the output is computed. While\ncomputing the output for the next word, the output from the\nprevious word (or time step) is also considered. RNNs are\ncalled recurrent because they perform the same computation\nfor every element of a sequence using the output from previ-\nous computations. At any step RNN performs the following\ncomputation,\nRNN(ti) = f(W ∗xti + U ∗RNN(ti−1)),\nwhere W and U are the trainable parameters of the model,\nand f is a nonlinear function. The bias terms are left out\nhere and have to be added appropriately. RNN(ti) is the\noutput at ith timestep, which can either be utilized as is,\nor can be fed again to a parameterized construct such as\nsoftmax [Bishop, 2006], depending on the task at hand. The\ntraining is done by formulating a loss objective function based\non the outputs at all timesteps, and trying to minimize the\nloss. The vanilla RNNs explained above have difﬁculty in\nlearning long term dependencies in the sequence via gradient\ndescent training [Bengio et al., 1994]. Also training vanilla\nRNNs is shown to be difﬁcult because of vanishing and\nexploding gradient problems [Pascanu et al., 2013]. Long short\nterm memory (LSTM) [Hochreiter and Schmidhuber, 1997], a\nvariant of RNN is shown to be effective in capturing long-term\ndependencies and easier to train compared to vanilla RNNs.\nMultiple variants of LSTMs have been proposed in literature.\nOne can refer to [Greff et al., 2015] for a comprehensive\nsurvey of LSTM variants.\nA LSTM module has three parameterized gates, input gate\n(i), forget gate (f) and output gate (o). A gate g operates by\ngti = σ(W g ∗xti + U g ∗hti−1),\nwhere W g and U g are the parameters of the gate g, ht−1 is\nthe hidden state at the previous time step and σ stands for the\nsigmoid function. All the three gates have the same equation\nform and inputs, but they have different set of parameters.\nAlong with hidden state, LSTM module also has a cell state.\nThe updation of the hidden state and cell state at anytime step\nare controlled by the various gates as follows,\nCti = fti ∗Cti−1 + iti ∗tanh(W C ∗xti + U C ∗hti−1)\nand\nhti = oti ∗tanh(Cti),\n(1)\nwhere W C and U C are again parameters of the model. The key\ncomponent of the LSTM is the cell state. The LSTM has the\ncapability to modify and retain the content on the cell state as\nrequired by the task, using the gates and hidden states. While\nforward LSTM takes the input sequence as it is, a backward\nLSTM takes the input in the reverse order. A backward LSTM\nis used to capture the dependencies of a word on future words\nin the original sequence. A concatenation of a forward LSTM\nand a backward LSTM is known as bi-directional LSTM (bi-\nLSTM) [Greff et al., 2015].\n1) Binary Tree Long Short Term Memory: The LSTM or bi-\nLSTM model process the information in a sequential manner,\nas a linear chain. But a natural language sentence have more\nsyntactic structure to it, and the information is represented\nmore as a tree structure than a linear chain. To incorporate this\nway of processing information, tree structured LSTMs were\nintroduced by Tai et al. 2015. In a tree structured LSTM (Tree-\nLSTM) each node will have multiple previous time steps,\none each corresponding to a child in the tree structure for\nthe node, compared to a single previous time step of a linear\nchain. Different set of parameters for each child is included\nfor the input and output gates to learn how different child\ninformation have to be processed. Using separate parameters\nchild information is summed up to form the input and output\ngate values of every node, as follows:\ngti = σ\n\nW g ∗xti +\nX\nl∈child(i)\nU g\ntl ∗htl\n\n.\nMultiple forget gates (one for each child) are included\nto learn the information from each child that needs to be\nremembered. Forget gate update for each k ∈child(i) is,\nfti,tk\n=\nσ\n\nW f ∗xti +\nX\nl∈child(i)\nU f\ntk,tl ∗htl\n\n.\nThen cell state is updated based on the forget gate values and\ncell state of the children is below.\nCti =\nX\nl∈child(i)\nfti,tl ∗Ctl+\niti ∗tanh\n\nW C ∗xti +\nX\nl∈child(i)\nU C\ntl ∗htl\n\n.\nHidden states are then computed similar to normal LSTM as\ngiven in (1). The bias terms are left out in all the equations\nand have to be added appropriately wherever needed. All Ws\nand Us in the above equations are model parameters, to be\nlearned.\nThe tree structure can be formed by considering the syn-\ntactic parse of the sentence, leading to different variations of\nTree LSTM [Tai et al., 2015]. If we consider the syntactic\nstructure, each sample in the training data creates different\ntree structures, leading to difﬁculty in training the model\nefﬁciently. To work around this we considered complete binary\ntree structures, formed by pairing adjacent words recursively.\nWe call this btree-LSTM in the subsequent discussion.\nIII. THE PROPOSED MODEL\nThe model ﬁrst encodes the sentences using a normal bi-\nLSTM or a btree-LSTM. This is to consider the different\nsegments of the sentence along with the context, which is\nan essential part of human processing as explained earlier. In\ncase of bi-LSTM, the encodings are augmented along with\nthe corresponding word vectors to create enhanced encodings.\nIn the case of btree-LSTM encodings this enhancement is not\ndone since, there is no one-to-one correspondence with the\nnumber of words in the sentence after the encoding. If the\nbi-LSTM encodings are done there will be n encodings for\na n-length sentence, where as if btree-LSTM encodings are\ndone, there will be 2n −1 encodings. btree-LSTMs considers\nmore possible phrasal structures(along with the context) of the\ninput sentence compared to a bi-LSTM, as shown below.\n(v1, · · · , vn) ←bi −LSTM(S),\n(v1, · · · , v2n−1) ←btree −LSTM(S),\nand\nSe ←(s1, v1, · · · , sn, vn).\n(2)\nThe phrase encodings (vi or vi, si, i = 1, . . . , n) in (2)\nrepresents the various information snippets in the sentence\nS along with the context in which they appear. We do this\nencodings for both the sentences, hypothesis H and premise\nP. Next phase is to align the information snippets between\nthe hypothesis and premise, as humans do, for which one can\nincorporate neural attention.\nA. Attention Mechanism\nAttention mechanism was introduced in the context of\nmachine translation recently [Bahdanau et al., 2014; Luong\net al., 2015], where in words or phrases from one language\nhas to be mapped or aligned to words or phrases in another\nlanguage for the purpose of translating. We use similar concept\nto learn this alignment for our purpose of NLI. Given two sets\nof vectors, a = {a1, .., an} and b = {b1, .., bn}, the attention\nvalue (a numerical quantity) vij is associated for each element\nof the ﬁrst set ai to each element of the second set bj. Forall\nai ∈a, attend((b1, · · · , bn), ai) = (vi1, · · · , vin), where,\nvij =\n(bj)T ai\nP\nr(br)T ai\nOne can see that for all i, P\nj vij\n=\n1. After learning,\nattention values will be high for elements that are mapped and\nlow for other elements. For example with bi-LSTM or btree-\nLSTM encoding corresponding to the man and woman\nwill have high attention value to the encoding corresponding\nto the couple, and low attention values for other snippets,\nin the context of the given sentences. Given an element we can\ngenerate the attention values and sum up the elements of the\nsecond set, using the attention values as weights, to create a\nrepresentation of the information that element is interested in\nor aligned with in the second set. As the attention values are\nhigh only for aligned encodings the summed up vector from\nthe second set will be dominated by the aligned information.\nThe phrase encodings of hypothesis H are aligned with the\nphrase encodings of the premise P using an attention mode\nas given in (3). The result of the alignment is computed using\na weighted sum of the phrase encodings of the premise P,\nusing attention values as weights.\nForall He\np ∈He, attend(Pe, He\np) = (a1, · · · , an),\nwhere\nai =\nPe\niT He\np\nP\nj Pe\njT He\np\nand\ntp =\nX\ni\nai · Pe\ni\n(3)\nNow that information snippets are aligned, pairs of\n(tp, He\np), they need to be processed. Different operators have\nto be applied based on the pairs and context. All the individual\nresults have then to be aggregated to make the ﬁnal decision.\nWe use neural network composition for this purpose.\nB. Task Composition\nOften a large task can be solved by composing the results of\nvarious different sub tasks, each computed separately. Such an\napproach for Question Answering was introduced by [Andreas\net al., 2016]. We adapt this approach for our purpose here.\nAfter learning the alignment of encodings, we need to perform\ndifferent functions or comparisons, depending on the kind\nof inputs and the sentence context, to see whether they\ncontribute positively or negatively towards ﬁnal prediction. In\nour example after aligning the encoding corresponding to the\nman and woman with the encoding for the couple the\nmodel has to process whether they are equivalent. Similarly\nafter aligning walking to wide awake the model has to\ndo a different kind of processing to verify that wide awake is\nfollowed from walking. Again if in an example, all birds\nare aligned with canary, model might have to check for\na type of or subset of relationship. Depending on the type of\ninput, the context of the sentence, different functions(operators\nor tasks) have to be applied. The operators also has to learn\nwhat it is supposed to do. Towards this purpose we introduced\nk number of operators, each is a two layer feed forward neural\nnetwork, with different set of parameters. If a and b are the\naligned encodings corresponding to two different text snippets,\nthey are passed through k different two layer feed forward\nFig. 1.\nAttention and Single Task Module used after bi-LSTM and btree-\nLSTM encodings\nneural network, the outputs of each weighed according to a\nsoft gating function as\n(g1, · · · , gk) = softmax(W T [a, b])\nO =\nX\ni\ngi · σ\n\u0000(W i\n2)T ∗σ((W i\n1)T [a, b]\n\u0001\n.\nwhere W s are model parameters. The soft gating function\nhelps to chose which operator has to be chosen to be applied,\nbased on their types and context in the sentence in which\nthey appear. Recall from the example different operators have\nto be applied to compare (the couple, the man and\nwoman) and (walking,wide awake). This is realized\nby soft gating function.\nExpression for (tp, He\np) pairs from (3) are given in (4). A\nschematic diagram of this module is given in Figure 1.\ntaski(tp, He\np) = σ\n\u0000(W i\n2)T ∗σ((W i\n1)T [tp, He\np]\n\u0001\n(g1, · · · , gk) = softmax(W T [tp, He\np])\nOp =\nX\ni\ngi · taski(tp, He\np)\n(4)\nEach O denotes a certain output for an input encoding pair.\nDifferent pairs yields different Os. All the Os has to be aggre-\ngated(composed) towards the output for the ﬁnal prediction.\nIn our example after understanding the man and woman\nand the couple are equivalent and wide awake\nfollows from walking, the model will have two O vectors\none for each pair. Both the Os have to be considered in\nmaking the ﬁnal judgement. How to aggregate the various O\ns have to be learned by the model. There are two parts to\nit. One is the order in which they have to be aggregated, if\nthere are more than two. Each ordering will give a different\ntree structured computation. The second being what exactly\nmeans aggregation. In the example the aggregation is an ’and’\noperator, both has to be satisﬁed. In another example it could\nbe ’or’ etc. Ideally for this, we should bring in a reinforcement\nlearning mechanism similar to the one that is used in [Andreas\net al., 2016], to learn the order of aggregation and a neural\nnetwork [Socher et al., 2011] for the learning the aggregation\noperator. In our current model(for which results are discussed),\nwe aggregate the operator outputs O by using a normal LSTM.\nThe aggregation order learning which maps to tree structured\ncomputation is envisioned as a part of future work.\nThe aggregated result A is then passed through a compari-\nson layer to do the ﬁnal prediction, which is shown below,\nA = lstm(O1, · · · , On),\nlabel = softmax(W T A)),\nand\nloss = H(labelgold, label),\n(5)\nwhere W is the model parameter and H(p, q) denotes the\ncross-entropy between p and q. We minimize this loss aver-\naged across the training samples, to learn the various model\nparameters using stochastic gradient descent [Bottou, 2012]. A\nschematic diagram of the complete model is given in Figure 2.\nC. Relevant Previous Work\nNLI is a well studied problem with a rich literature using\nclassical machine learning techniques. With the advent of deep\nlearning, many models including LSTMs were used for NLI.\nRecently Stanford Natural Language Inference(SNLI) dataset\nwas created [Bowman et al., 2015] using crowd sourcing.\nMany deep learning models have been benchmarked on this\ndataset for NLI. Detail list is available at http://nlp.stanford.\nedu/projects/snli/. This recent thesis Bowman [2016] covers\ndeep learning based works in detail.\nMany of deep learning based works relied on creating en-\ncodings of the sentences using LSTMs or convolutional neural\nnetworks or gated recurrent units or variants of recursive\nneural networks, and then using these encodings for the ﬁnal\nprediction task Bowman et al. [2016]; Mou et al. [2016];\nVendrov et al. [2015] are all these kinds of work. Bowman\net al. (2016) also introduced an efﬁcient mechanism to learn\nthe binary parse of the tree along with creating encodings for\nthe prediction task. Works in Rockt¨aschel et al. [2015]; Wang\nand Jiang [2015] used neural attention mechanism along with\nLSTMs for the problem of NLI.\nThere are 3 main works in the space, which claims state\nof the art results. 1. To address the problem of compressing a\nlot of information in a single LSTM cell, Cheng et al. [2016]\nintroduced Long Short Term Memory Networks(LSTMN) for\nNatural Language Inference. 2. Munkhdalai and Yu (2016) in-\ntroduced Neural Tree Indexers (NTI), by bringing in attention\nover tree structures of the sentences. 3. Parikh et al. [2016] is\nanother very recent work, which uses the attention mechanism\nover words, compare them and then aggregate the results. As\nexplained earlier we considers attention over possible sentence\nsegment encodings(considering context), subtask division, op-\nerator selection and learning, and aggregation learning. Our\nmodel is aligned with human thought process and hence very\nintuitive and achieves state of the art results.\nIV. EXPERIMENTS AND EVALUATION\nThe model was implemented in TensorFlow [Abadi et al.,\n2015] - an open-source library for numerical computation\nfor Deep Learning. All experiments were carried on a Dell\nPrecision Tower 7910 server with Nvidia Titan X GPU. The\nmodels were trained using the Adam’s Optimizer [Kingma\nand Ba, 2014] in a stochastic gradient descent [Bottou, 2012]\nfashion. We used batch normalization [Ioffe and Szegedy,\n2015] while training. The various model parameters used are\nmentioned in Table I.\nWe experimented with both GloVe vectors trained1 on\nCommon Crawl dataset as well as Word2Vec vector trained2\non Google news dataset. We used Google News trained\nword2vec word embeddings for the ﬁnal reported results.\nBefore matching a word in the dataset with a word in the\nword2vec collection, we converted all characters to lower\ncase. The word embeddings are not trained along with the\nmodel. However before using them in our model, we trans-\nformed the embeddings using a learnable single layer neural\nnetwork(σ(W T · w), where W is the model parameter and\nw is the word embeddings). For out of vocabulary words,\nwe assigned them random word vectors. Each element of\nthe word vector is randomly sampled from N(0, 0.06). This\ndecision has been taken after observing that the word2vec\nvector elements are approximately distributed according to the\nabove normal distribution.\nTABLE I\nMODEL & TRAINING PARAMETERS\nParameter Name\nValue\nWord Vector Dimension\n300\nSequence Length\n64\nbi-LSTM Hidden State Dimension\n300\nbtree-LSTM Hidden State Dimension\n300\nOperator Count\n11\nBatch Size\n40\nBatch Norm γ init. value\n0.001\nThere are two main datasets available in the public do-\nmain for NLI. Sentences Involving Compositional Knowledge\n(SICK) [Marelli et al., 2014b] dataset from the SemEval-\n2014 task [Marelli et al., 2014a] which involves, predicting\nthe degree of relatedness between two sentences, detecting\nthe entailment relation holding between them. SICK consists\n1http://nlp.stanford.edu/data/glove.840B.300d.zip\n2https://code.google.com/archive/p/word2vec/\nFig. 2. The Complete Model: The upper tree learning(top of the ﬁgure) is envisioned for future work, in the current model a simple LSTM is used there\ninstead\nof 10000 sentence pairs manually labelled for relatedness\nand entailment. We have experimented with this dataset and\nhave got very good results. The dataset being small and\nthe model having large number of parameters, overﬁtting\ncould have happened. As benchmark is not available for other\nstate of the art models for comparison on SICK we are not\nincluding our results on this dataset. The Stanford Natural\nLanguage Inference Corpus(SNLI) [Bowman et al., 2015]\ndataset contains 570k human-written English sentence pairs\nmanually labeled for balanced classiﬁcation with the labels\nentailment, contradiction, and neutral, supporting the task of\nnatural language inference. We are presenting our results on\nthis dataset in comparison with other state of the art models.\nThe comparison results of various models on SNLI dataset\nis given in Table II. One can see that our model with bi-lstm\nencodings have better accuracy numbers compared to all pub-\nlished results, but fall short very close to the results reported\nin not yet published works [Munkhdalai and Yu, 2016; Parikh\net al., 2016]. The model with btree-lstm encodings have better\naccuracy numbers than all the models. The class level accuracy\nresults of various models on SNLI dataset is given in Table III.\nV. CONCLUSION & FUTURE WORK\nWe presented a complete deep learning model for the\nproblem of natural language inference. The model used deep\nlearning constructs like LSTM variants, attention mechanism\nand composable neural networks to mimic humans for natural\nlanguage inference. The model is end-to-end differentiable,\nenabling training by simple stochastic gradient descent. From\nthe initial experiments, the model have better accuracy num-\nbers than all the published models. The model is interpretable\nin close alignment with human process while performing\nNLI, unlike other complicated deep learning models. We hope\nfurther experiments and hyper parameter tuning will improve\nthese results further.\nThere are different enhancements for the model possible and\npotential future work directions. The btree-LSTM currently\nuses a complete binary tree structure formed by considering\nneighbouring encodings recursively. A binary tree learning\nscheme, similar to [Bowman et al., 2016] can be considered to\nbe incorporated in the model. Tree construction based on the\nordering of attention values will lead to heap like structures.\nWe are currently working on this model which we have named\nHeap-LSTM. Currently the model uses soft gating for operator\nselection, hard selection with appropriate learning mechanism\nis something that has to be explored. The model aggregates\nthe operator outputs using a simple LSTM, the aggregation\ntree structure learning using appropriate learning mechanism\nsimilar to [Andreas et al., 2016] is another major stream of\nwork.\nThe alignment of the model with human thought process,\nalready better accuracy numbers than all published models just\nfrom initial experiments, all advocate the exploration of model\nenhancements in these directions.\nTABLE II\nCOMPARISON RESULTS ON SNLI DATASET\nModel\nTrain Accuracy\nTest Accuracy\n#Parameters\nClassiﬁer(hand crafted features) [Bowman et al., 2015]\n99.7\n78.2\nGRU encoders [Vendrov et al., 2015]\n98.8\n81.4\n15.0M\nTree-based CNN encoders [Mou et al., 2016]\n83.3\n82.1\n3.5M\nSPINN-NP encoders [Bowman et al., 2016]\n89.2\n83.2\n3.7M\nLSTM with attention [Rockt¨aschel et al., 2015]\n85.3\n83.5\n252K\nmLSTM [Wang and Jiang, 2015]\n92.0\n86.1\n1.9M\nLSTM Networks [Cheng et al., 2016]\n88.5\n86.3\n3.4M\nword-word attention and aggregation [Parikh et al., 2016]\n90.5\n86.8\n582K\nNTI with global attention [Munkhdalai and Yu, 2016]\n88.5\n87.3\n-\nOur model with bi-LSTM encoders\n89.8\n86.4\n6M\nOur model with btree-LSTM encoders\n88.6\n87.6\n2M\nTABLE III\nCLASS LEVEL ACCURACY. N: NEUTRAL CLASS, E: ENTAILMENT CLASS, C: CONTRADICTION\nMethod\nN\nE\nC\nSPINN-NP encoders [Bowman et al., 2016]\n80.6\n88.2\n85.5\nmLSTM [Wang and Jiang, 2015]\n81.6\n91.6\n87.4\nword-word attention and aggregation [Parikh et al., 2016]\n83.7\n92.1\n86.7\nOur model with bi-LSTM encoders\n84.3\n90.6\n86.9\nOur model with btree-LSTM encoders\n84.8\n93.2\n87.4\nREFERENCES\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\nZhifeng Chen, and Craig Citro et al. TensorFlow: Large-\nscale machine learning on heterogeneous systems, 2015.\nURL http://tensorﬂow.org/. Software available from tensor-\nﬂow.org.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan\nKlein. Learning to compose neural networks for question\nanswering. arXiv preprint arXiv:1601.01705, 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473, 2014.\nYoshua Bengio. Learning deep architectures for ai. Founda-\ntions and trends R⃝in Machine Learning, 2(1):1–127, 2009.\nYoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning\nlong-term dependencies with gradient descent is difﬁcult.\nIEEE transactions on neural networks, 5(2):157–166, 1994.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Chris-\ntian Janvin. A neural probabilistic language model. The\nJournal of Machine Learning Research, 3:1137–1155, 2003.\nJohan van Benthem. A brief history of natural logic. College\nPublications, 2008.\nChristopher M Bishop.\nPattern recognition and machine\nlearning. springer, 2006.\nL´eon\nBottou.\nStochastic\nGradient\nTricks,\nvolume\n7700, page 430445.\nSpringer, January 2012.\nURL\nhttps://www.microsoft.com/en-us/research/publication/\nstochastic-gradient-tricks/.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D. Manning.\nA large annotated corpus for\nlearning natural language inference. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP). Association for Computational\nLinguistics, 2015.\nSamuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav\nGupta, Christopher D Manning, and Christopher Potts. A\nfast uniﬁed model for parsing and sentence understanding.\narXiv preprint arXiv:1603.06021, 2016.\nSamuel Ryan Bowman.\nModeling Natural Language Se-\nmantics in Learned Representations. PhD thesis, Stanford\nUniversity, 2016.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-\nterm memory-networks for machine reading. arXiv preprint\narXiv:1601.06733, 2016.\nSusan T Dumais. Latent semantic analysis. Annual review of\ninformation science and technology, 38(1):188–230, 2004.\nKlaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık, Bas R\nSteunebrink, and J¨urgen Schmidhuber. Lstm: A search space\nodyssey. arXiv preprint arXiv:1503.04069, 2015.\nSanda Harabagiu and Andrew Hickl. Methods for using textual\nentailment in open-domain question answering. In Proceed-\nings of the 21st International Conference on Computational\nLinguistics and the 44th annual meeting of the Association\nfor Computational Linguistics, pages 905–912. Association\nfor Computational Linguistics, 2006.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997.\nSergey Ioffe and Christian Szegedy.\nBatch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift. arXiv preprint arXiv:1502.03167, 2015.\nDiederik Kingma and Jimmy Ba.\nAdam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014.\nAnkit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,\nRobert English, Brian Pierce, Peter Ondruska, Ishaan Gul-\nrajani, and Richard Socher.\nAsk me anything: Dynamic\nmemory networks for natural language processing. arXiv\npreprint arXiv:1506.07285, 2015.\nFinley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,\nJeremy Bensley, Bryan Rink, Patrick Wang, and Lara Taylor.\nLccs gistexter at duc 2006: Multi-strategy multi-document\nsummarization. In Proceedings of DUC06, 2006.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep\nlearning. Nature, 521(7553):436–444, 2015.\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning.\nEffective approaches to attention-based neural machine\ntranslation. arXiv preprint arXiv:1508.04025, 2015.\nBill MacCartney.\nNatural language inference.\nPhD thesis,\n2009.\nBill MacCartney and Christopher D Manning. An extended\nmodel of natural logic.\nIn Proceedings of the eighth\ninternational conference on computational semantics, pages\n140–156. Association for Computational Linguistics, 2009.\nMarco\nMarelli,\nLuisa\nBentivogli,\nMarco\nBaroni,\nRaf-\nfaella Bernardi, Stefano Menini, and Roberto Zamparelli.\nSemeval-2014 task 1: Evaluation of compositional distribu-\ntional semantic models on full sentences through semantic\nrelatedness and textual entailment. SemEval-2014, 2014a.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa Ben-\ntivogli, Raffaella Bernardi, and Roberto Zamparelli.\nA\nsick cure for the evaluation of compositional distributional\nsemantic models. In LREC, pages 216–223, 2014b.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\nEfﬁcient estimation of word representations in vector space.\narXiv preprint arXiv:1301.3781, 2013a.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic\nregularities in continuous space word representations.\nIn\nHLT-NAACL, volume 13, pages 746–751, 2013b.\nLili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan,\nand Zhi Jin.\nNatural language inference by tree-based\nconvolution and heuristic matching.\nIn The 54th Annual\nMeeting of the Association for Computational Linguistics,\npage 130, 2016.\nTsendsuren Munkhdalai and Hong Yu. Neural tree indexers\nfor text understanding. arXiv preprint arXiv:1607.04492,\n2016.\nAnkur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob\nUszkoreit.\nA decomposable attention model for natural\nlanguage inference. arXiv preprint arXiv:1606.01933, 2016.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the\ndifﬁculty of training recurrent neural networks. Proceedings\nof the 30th international conference on machine learning\n(ICML-13), 28:1310–1318, 2013.\nJeffrey Pennington, Richard Socher, and Christopher D Man-\nning.\nGlove: Global vectors for word representation.\nIn\nEMNLP, volume 14, pages 1532–1543, 2014.\nTim Rockt¨aschel, Edward Grefenstette, Karl Moritz Her-\nmann, Tom´aˇs Koˇcisk`y, and Phil Blunsom.\nReasoning\nabout entailment with neural attention.\narXiv preprint\narXiv:1509.06664, 2015.\nRichard Socher, Cliff C Lin, Chris Manning, and Andrew Y\nNg.\nParsing natural scenes and natural language with\nrecursive neural networks.\nIn Proceedings of the 28th\ninternational conference on machine learning (ICML-11),\npages 129–136, 2011.\nKai Sheng Tai, Richard Socher, and Christopher D Man-\nning.\nImproved semantic representations from tree-\nstructured long short-term memory networks. arXiv preprint\narXiv:1503.00075, 2015.\nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun.\nOrder-embeddings of images and language. arXiv preprint\narXiv:1511.06361, 2015.\nShuohang Wang and Jing Jiang. Learning natural language\ninference with long short-term memory-networks. In Pro-\nceedings of NAACL., 2015.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2016-11-15",
  "updated": "2017-01-27"
}