{
  "id": "http://arxiv.org/abs/1611.05244v2",
  "title": "Deep Transfer Learning for Person Re-identification",
  "authors": [
    "Mengyue Geng",
    "Yaowei Wang",
    "Tao Xiang",
    "Yonghong Tian"
  ],
  "abstract": "Person re-identification (Re-ID) poses a unique challenge to deep learning:\nhow to learn a deep model with millions of parameters on a small training set\nof few or no labels. In this paper, a number of deep transfer learning models\nare proposed to address the data sparsity problem. First, a deep network\narchitecture is designed which differs from existing deep Re-ID models in that\n(a) it is more suitable for transferring representations learned from large\nimage classification datasets, and (b) classification loss and verification\nloss are combined, each of which adopts a different dropout strategy. Second, a\ntwo-stepped fine-tuning strategy is developed to transfer knowledge from\nauxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel\nunsupervised deep transfer learning model is developed based on co-training.\nThe proposed models outperform the state-of-the-art deep Re-ID models by large\nmargins: we achieve Rank-1 accuracy of 85.4\\%, 83.7\\% and 56.3\\% on CUHK03,\nMarket1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model\n(45.1\\%) beats most supervised models.",
  "text": "Deep Transfer Learning for Person Re-identiﬁcation\nMengyue Geng\nYaowei Wang\nTao Xiang\nYonghong Tian\nAbstract\nPerson re-identiﬁcation (Re-ID) poses a unique chal-\nlenge to deep learning: how to learn a deep model with mil-\nlions of parameters on a small training set of few or no la-\nbels. In this paper, a number of deep transfer learning mod-\nels are proposed to address the data sparsity problem. First,\na deep network architecture is designed which differs from\nexisting deep Re-ID models in that (a) it is more suitable\nfor transferring representations learned from large image\nclassiﬁcation datasets, and (b) classiﬁcation loss and veri-\nﬁcation loss are combined, each of which adopts a different\ndropout strategy. Second, a two-stepped ﬁne-tuning strategy\nis developed to transfer knowledge from auxiliary datasets.\nThird, given an unlabelled Re-ID dataset, a novel unsuper-\nvised deep transfer learning model is developed based on\nco-training. The proposed models outperform the state-of-\nthe-art deep Re-ID models by large margins: we achieve\nRank-1 accuracy of 85.4%, 83.7% and 56.3% on CUHK03,\nMarket1501, and VIPeR respectively, whilst on VIPeR, our\nunsupervised model (45.1%) beats most supervised models.\n1. Introduction\nPerson re-identiﬁcation (Re-ID) is the problem of match-\ning people across non-overlapping camera views, which\ntypically arises in a surveillance application. Despite the\nbest efforts from the computer vision researchers, it re-\nmains an unsolved problem [67].\nEarlier works focus\non either designing view-insensitive feature representations\n[8, 57, 23, 33, 65, 36], or learning an effective distance met-\nric [56, 34, 28, 3, 61, 63, 35], or both [11, 26]. Recently,\ninspired by the success of deep neural networks, particu-\nlarly deep Convoluational Neural Networks (CNNs) in var-\nious vision problems [22, 44, 48, 12], deep Re-ID models\nstarted to attract attention [25, 58, 6, 1, 43, 49, 50, 29, 4, 55].\nHowever, unlike other visual recognition problems, es-\npecially closely related ones such as face veriﬁcation, only\nlimited success has been achieved so far by deep Re-ID\nmodels: they only marginally improve over the hand-crafted\nfeature + metric learning based alternatives on large datasets\nsuch as Market1501 [66], and are outperformed on small\ndatasets such as VIPeR [10]. Lack of large labelled train-\ning set is an obvious reason. Collecting matching pairs of\nperson images in a camera network is a notoriously difﬁ-\ncult task [64]. As a result, even the largest published Re-\nID datasets only have modest sizes: 1,360 unique identities\nin CUHK03 [25] and 1,501 in Market1501 [66]. In con-\ntrast, the widely used LFW dataset [17] for face veriﬁcation\nhas 5,749 identities – faces of celebrities are much easier\nto collect and label than passers-by captured by a surveil-\nlance camera network. Importantly, one could easily col-\nlect a much larger auxiliary dataset of faces to assist in the\nmodel learning: one of the state-of-the-art results on LFW\nwas obtained by pretraining the deep model on an auxiliary\nface dataset of 200M images of 8M identities [41].\nGiven insufﬁcient training samples, transferring feature\nrepresentations learned from a larger auxiliary dataset be-\ncomes necessary. Indeed, transfer learning has been con-\nsidered in most existing deep Re-ID works. In particular,\ngiven a small Re-ID dataset with only a few hundreds of\nlabelled identities, existing models typically pretrain with\nlarger Re-ID datasets followed by ﬁne-tuning on the target\nset, with a notable exception of [55] which learns a sin-\ngle model jointly across multiple Re-ID datasets before the\nﬁne-tuning in each. In other words, only Re-ID datasets are\nconsidered as auxiliary datasets – hardly ideal because all\nRe-ID datasets published so far are relatively small. Impor-\ntantly the domain gaps between different Re-ID datasets are\noften large due to the drastically different camera viewing\nconditions; designing the most suitable method to prevent\nnegative transfer is thus a challenging task [40].\nWe argue that to transfer knowledge that is generalis-\nable to any Re-ID dataset, we should go beyond existing\nRe-ID datasets and consider much larger sources. An obvi-\nous choice would be the ImageNet dataset [5] which con-\ntains millions of images of thousands of object categories\nand has been shown to be useful as an auxiliary dataset for\nmodel pretraining for a variety of visual recognition tasks\n[7]. However, transferring knowledge from ImageNet to\na Re-ID dataset has a number obstacles. First, the object\ncategorisation task of ImageNet is very different from the\nobject instance veriﬁcation task of person Re-ID. Second,\nthe inputs to a Re-ID model are person detection images\nin CCTV surveillance videos, which have very different as-\npect ratios (people are typically upright) and much lower\nresolutions. This is the reason why most recent deep Re-ID\nmodels [55, 4, 50, 29, 42] have very different network ar-\nchitectures compared with those of the models excelled on\nthe ImageNet object categorisation task, e.g. having smaller\nﬁlter size, and being shallower with most pooling layers re-\nmoved. Such models are designed for training from scratch\n1\narXiv:1611.05244v2  [cs.CV]  22 Nov 2016\non Re-ID datasets, and are unsuitable for knowledge trans-\nfer from ImageNet.\nThe proposed deep Re-ID network architecture in this\nwork is designed speciﬁcally for transferring generalisable\nfeature representations learned from ImageNet to Re-ID\ndatasets. To this end, we make two key design choices: (1)\nThe base network structure is a standard GoogleNet [48]\nwhich has been optimised for ImageNet. (2) Two losses\nare combined. These include an identity classiﬁcation loss\nwhich is chosen because the model needs to be pretrained\non the auxiliary ImageNet dataset for the object classiﬁ-\ncation task.\nThe other loss is a veriﬁcation loss, which\naims to learn a feature representation for matching person.\nWe argue that by combining the two losses, our model can\nbridge the large task discrepancy between object categori-\nsation and object instance veriﬁcation, as well as the large\ndomain gap between ImageNet and Re-ID datasets.\nApart from having different architecture and training ob-\njectives compared to existing deep Re-ID models, the pro-\nposed model also has the following distinctive features. (1)\nSince all target Re-ID datasets are relatively small given\nmillions of model parameters, avoiding overﬁtting is of\nparamount importance. Dropout is a widely adopted tech-\nnique for overcoming overﬁtting. In our model, two dif-\nferent dropout strategies are employed for the two different\nlosses. (2) We propose a two-stepped ﬁne-tuning strategy\nafter the model is pretrained due to the unique combina-\ntion of the two losses. Compared to the conventional one-\nstepped ﬁne-tuning strategy, it is much more effective as\nshown in our experiments (see Sec. 5.3).\nLearning a Re-ID model given a set of unlabelled data\nis a more challenging task, but also has more practical uses\nin real-world applications – a Re-ID system is typically in-\nstalled for a camera network monitoring a large public space\n(e.g. a train station or a shopping mall) which can easily\nconsist of hundreds of cameras. Even labelling a few hun-\ndreds people across all camera views is infeasible. How-\never, person detection images can be readily obtained in\neach view using a person detector, resulting in an unlabelled\nRe-ID dataset. Transfer learning from labelled source data\nto unlabelled target data is an unsupervised1domain adapta-\ntion problem which has not been studied by existing deep\nRe-ID models. In this work, a novel co-training based un-\nsupervised transfer learning model is proposed.\nSpeciﬁ-\ncally, the model alternates between a graph regularised dis-\ncriminative dictionary learning model and a soft-label self-\ntraining deep model, with the former providing the soft-\nlabels for the latter and the graph regularisation provided\nin the opposite direction. We show that such a deep/non-\ndeep hybrid co-training framework can effectively prevent\nmodel drift and yield Re-ID performance that is better than\n1By ‘unsupervised’, we mean target-unsupervised domain adaptation,\na deﬁnition adopted by [9, 62, 32].\nmost existing supervised learning based models.\n2. Related Work\nDeep Re-ID model\nExisting deep Re-ID models [25, 58,\n6, 1, 43, 49, 50, 29, 4, 55, 42] differ signiﬁcantly in their\nnetwork architectures, which are largely determined by the\ntraining objectives/losses. Speciﬁcally, most existing works\ncast the Re-ID problem as a deep metric learning problem\nand employ pairwise veriﬁcation loss [58, 1, 43, 49, 50, 42]\nor triplet ranking loss [6, 29, 4], or both [52]. Correspond-\ningly the overall network architecture is a Siamese CNN\nnetwork with either two or three branches for the pairwise\nor triplet loss respectively. None of them uses an identity\nclassiﬁcation loss with the only exception of [55] which has\nan one-branch architecture. In contrast, our model has a\nSiamese two-branch architecture with an identity classiﬁ-\ncation loss for each branch and pairwise veriﬁcation loss\nacross the two branches. This architecture is similar to the\none used for deep face veriﬁcation [47]. Combining the two\nloses in [47] aims to exploit the strengths of the two losses:\nthe classiﬁcation loss pulls different classes apart and the\nveriﬁcation loss makes the intra-class distance small. In\ncontrast, we choose the combination so that the model can\nbe pretrained on the ImageNet object classiﬁcation task –\namong the two losses the classiﬁcation loss makes sure that\nthe ImageNet-learned representation is relevant whilst the\nveriﬁcation loss guides the adaptation towards the person\nRe-ID dataset/veriﬁcation task.\nApart from the overall architecture (one, two, or three\nbranches), existing models also have very different base\nnetwork structure (the convolution/pooling layers in each\nbranch). It is noted that most recently proposed deep Re-ID\nmodels [52, 50, 29, 4, 55, 42] have base networks tailor-\nmade for the Re-ID problem, that is, they take into con-\nsideration the smaller input image size and different (non-\nsquare) aspect ratio of person detection images in a Re-ID\ndataset. In particular, the ﬁlter-size/stride step are typically\nmuch smaller with fewer pooling layers, compared to the\nImageNet-oriented GoogleNet [48] or VGG Net [44], so\nthat they can be learned from scratch using Re-ID datasets\nalone. However, this simpliﬁed base network architecture,\ntogether with the lack of the classiﬁcation-veriﬁcation loss\ncombination mean that the existing models are unable to\nexploit the rich transferable feature representation learned\nfrom ImageNet.\nDropout strategy\nDropout [46] is a widely adopted tech-\nnique in deep learning to counter overﬁtting, a problem that\nis particularly acute in Re-ID due to the small data size.\nGiven the two losses, we propose to use different dropout\nstrategies for each loss-associated layers. Speciﬁcally, the\nstandard random dropout [46] is deployed for the classiﬁ-\ncation loss layers, whilst for the pairwise veriﬁcation loss\nlayers, we introduce pairwise-consistent dropout, that is,\neach pair of compared training data points share the same\ndropout mask. We show experimentally that such a modiﬁ-\ncation can bring about 3% improvement in Re-ID accuracy.\nDeep transfer learning\nTransfer learning or domain\nadaptation is an extensively studied topic [39]. Transfer\nlearning is widely used for deep learning when a target task\nis short of labelled data. The most common deep trans-\nfer learning strategy is ﬁne-tuning [60]: ﬁrst train a base\nnetwork using a large source data and then copy the ﬁrst\nn layers to the corresponding layers of the target network,\nfollowed by randomly initialising the remaining layers and\nﬁnally ﬁne-tune only them or all layers. A systematic study\nis presented in [60] which examines how transferable fea-\ntures of different layers are between the source and target\ndomains. It concludes that the generalisation ability dimin-\nishes when the discrepancy between the source and target\ntasks increases. Note that the source and target tasks consid-\nered in [60] were classifying different subsets of ImageNet,\nso the task/domain discrepancy studied is nowhere near as\nbig as in our ImageNet →Re-ID transfer setting. As a re-\nsult, the conventional one-stepped ﬁne-tuning strategy be-\ncomes inadequate. To overcome the large task discrepancy\nbetween classiﬁcation and veriﬁcation, we propose a two-\nstepped ﬁne-tuning strategy whereby the network is ﬁrst\nﬁne-tuned with the classiﬁcation loss only, followed by ﬁne-\ntuning with both classiﬁcation and veriﬁcation losses.\nNote that beyond ﬁne-tuning, several recent works take a\nmulti-task joint training approach [30, 9, 62, 31, 32, 55], one\nof which is designed speciﬁcally for Re-ID [55]. Most of\nthem aim to minimise the discrepancy between the marginal\n[30, 9, 62] or joint [31] distributions of the source and tar-\nget data, e.g., by introducing a cross-domain loss that is de-\nsigned to blur the domain boundary [9]. However, these\nworks assume that the tasks are the same or similar in\nthe two domains, e.g., classifying the same object classes\nshared by two datasets. They are thus unsuitable when the\nsource and target domains have completely different tasks,\nin our case object categorisation in ImageNet and person\nmatching in Re-ID – aligning the data distributions of the\ntwo datasets would not make any sense. The joint learning\n+ multi-task learning + ﬁne-tuning based deep Re-ID model\nin [55] is clearly not suitable for transferring from ImageNet\nto Re-ID with the different source and target tasks.\nDeep unsupervised domain adaptation\nIn theory, any\nunsupervised deep learning methods can potentially be ap-\nplied for domain adaptation when the ﬁrst n layers are pre-\ntrained on the source data. These include auto-encoder [13]\nand dictionary learning [19] which can be implemented as\nneural network layers and integrated as the later/top lay-\ners of a CNN network [53].\nThe main limitation of an\nunsupervised model is that it cannot learn discriminative\nfeatures. Hence soft-label self-training based deep unsu-\npervised learning has become popular recently [16].\nIn\nthis work, a novel co-training [37] based unsupervised do-\nmain adaptation method is proposed to overcome the main\ndrawback of self-training based methods, i.e., model drift\n[45]. Combined with the proposed two-stepped ﬁne-tuning\nmethod, this gives us a powerful deep unsupervised Re-ID\nmodel that outperforms not only alternative unsupervised\nmodels, but also most supervised models which use train-\ning labels. Recently a number of deep unsupervised trans-\nfer learning models are proposed [9, 62, 32]. Nevertheless,\nthe domain gap between different Re-ID datasets is sig-\nniﬁcant and cannot be overcome by just aligning the data\ndistributions, making them less effective than the proposed\nco-training based unsupervised learning method, as demon-\nstrated in our experiments.\nOur contributions are as follows: (1) A new deep Re-\nID network architecture is designed for transferring fea-\nture representation learned from large image classiﬁcation\ndatasets. It is unique in its loss combination and dropout\nstrategy.\n(2) A two-stepped ﬁne-tuning strategy is fur-\nther developed for deep transfer learning. (3) A novel co-\ntraining based unsupervised domain adaptation method is\nproposed for unsupervised Re-ID. (4) We present compre-\nhensive experimental evaluations on 5 benchmarks. Our ex-\nperiments show that the proposed models outperform the\nstate-of-the-art deep Re-ID models by a signiﬁcant margin.\n3. Deep Re-ID Model\n3.1. Network Architecture\nOverview The overall network architecture of the pro-\nposed deep Re-ID model is illustrated in Fig. 1. It is es-\nsentially a two-branch Siamese network that takes a pair\nof input person detection images as input and aims to\nlearn a deep representation of person appearance that is\nidentity-discriminiative so that images of the same per-\nson can be matched correctly whilst visually similar peo-\nple can be distinguished.\nThe model has two training\ntasks/objectives/losses: an ID classiﬁcation loss and a pair-\nwise veriﬁcation loss. As a result, the network contains\nfour parts (see Fig. 1): a base network shared by the two\nbranches, a loss-speciﬁc dropout unit, an ID classiﬁcation\nsubnet, and a pairwise veriﬁcation subnet. The two main\nbranches of the network have the same base network archi-\ntecture and share their parameters, hence the name Siamese.\nAfter feature vectors are computed for the input images\nusing the base network, they are fed into a loss-speciﬁc\ndropout unit so that either a pairwise-consistent dropout or\nthe standard random dropout is applied to the features. Af-\nter that, the pairwise veriﬁcation subnet takes a pair of fea-\ntures and learn to distinguish whether they come from the\nsame person or not. In the meantime, the person ID classi-\nﬁcation subnet learns to classify each feature output of the\nbase network into a class corresponding to the input image\nperson ID.\nBase network\nThe base network is a CNN that learns a\ndeep representation from the input images. Various CNN\nInput images\n⋯\n⋯⋯\nBase Network\nClassification Subnet\nVerification Subnet\nInput images\n⋯\n⋯⋯\nLoss Specific\nDropout Unit\n෥𝒚𝒊\n෥𝒚𝒋\n෥𝒚𝒊−෥𝒚𝒋\n𝒚𝒊\n𝒚𝒋\nBase Network\nFigure 1. The proposed deep Re-ID network architecture.\narchitectures can be considered.\nIn this paper we use\nGoogLeNet [48]. This is different from the base networks\nused by most existing deep Re-ID models. We deliberately\nchoose an existing network that is competitive in the Ima-\ngeNet classiﬁcation benchmark, and has been widely used\nin many other vision problems, rather than designing a sim-\npliﬁed bespoke Re-ID network. This is because we aim to\nuse the network to transfer generalisable feature represen-\ntations from the much larger ImageNet dataset. Among the\nrecently proposed networks that achieved good classiﬁca-\ntion performance on ImageNet, GoogLeNet is chosen over\nVGG net [44] due to the fact that it has much smaller pa-\nrameter size2.\nLoss speciﬁc dropout unit\nGiven an input image x, the\nbase network produces a D-dimensional vector of output y.\nIt will then enter a loss speciﬁc dropout unit where the oper-\nation depends on whether the output of the unit ey is fed into\nthe classiﬁcation subnet subject to the classiﬁcation loss or\nthe veriﬁcation subnet with a pairwise veriﬁcation loss. For\nthe former, the standard dropout operation [46] takes place.\nConcretely it will randomly set part of the elements of y to\nzero. Formally, we have\ney = r ∗y\n(1)\nwhere the D-dimensional vector r is a dropout mask and\n∗denotes an element-wise product. Each element of r is\na random variable sampled following a Bernoulli process,\ni.e., the d-th element rd ∼Bernoulli(p) and has a proba-\nbility of p to be 1. Given a different image in a mini-batch,\na different random dropout mask will be applied and the\noutput is then fed into the classiﬁcation subnet.\nSuch a completely random dropout operation is never-\ntheless not appropriate when the output ey is to be fed into\nthe veriﬁcation subnet. In particular, as to be detailed later,\ngiven a mini-batch of training images, each pair of im-\nages will have their output vectors eyi and eyj subject to an\n2We found that the residual networks [12] have a similar performance\nas GoogLeNet when used as the base network in our model.\nelement-wise subtraction operation in the subnet. There-\nfore, if two randomly generated dropout masks ri and rj\nare applied, the difference of two vectors could be caused\nby the different random masks rather than the appearances\nof the two compared people – a clearly undesirable effect.\nTo address this problem, we introduce a pairwise-consistent\ndropout for the veriﬁcation subnet, that is, we make sure\nthat ri = rj when the i-th and j-th images are compared in\nthe veriﬁcation subnet. Interestingly, although it is intuitive,\nwe could not ﬁnd any published work that discuss this need\nto design different dropout strategy for layers used as input\nto pairwise or triplet losses. For example, all relevant codes\nwe could ﬁnd on GitHub use the standard random dropout.\nWe show in our experiments that by adopting the pairwise-\nconsistent dropout for the veriﬁcation loss, a 3% improve-\nment can be obtained compared to the random dropout.\nPerson ID classiﬁcation subnet The person ID classiﬁca-\ntion part learns a softmax classiﬁer with a cross-entropy loss\nthat distinguishes different people from each other. After\nthe features are extracted from the base network and the\nrandom dropout is applied, a softmax layer with N nodes\nare then connected, where N is the unique person number\nin the training set.\nPairwise veriﬁcation subnet The pairwise veriﬁcation sub-\nnet ﬁrst takes two feature vectors eyi and eyj as input. They\nare ﬁrst fused with element-wise subtraction. Subsequently,\nthe difference vector is passed to a rectiﬁed linear unit\n(ReLU). After a fully connected (FC) layer, the last layer\nof the veriﬁcation network is a softmax layer with two out-\nput nodes, corresponding to whether or not the input image\npair contains the same person. Note that for pairwise ver-\niﬁcation, the margin based contrastive loss is much widely\nused beyond Re-ID [47]. However, for Re-ID, with a few\nexceptions [50, 51], this subtraction + binary cross-entropy\nloss is more popular [52, 58, 49]. We ﬁnd empirically that\nusing the contrastive loss leads to worse performance in our\nmodel. It is also worth pointing out that more sophisticated\ndifferencing operations have been developed to deal with\nthe mis-alignment issue of compared person detection im-\nages [25, 1]; however they cannot be applied this late in our\nnetwork architecture after a forward-pass of the GoogLeNet\nbase network. A full-blown Mahalanobis metric learning\nloss [42] could also be deployed in our veriﬁcation subnet,\nbut that will sacriﬁce the testing efﬁciency as the model can-\nnot be used as SIR (single image representation) [52] model\nany more.\n3.2. Model Training and Testing\nOur network is too big to train effectively from scratch\nusing existing person Re-ID datasets. Transfer learning us-\ning other datasets as auxiliary data is thus necessary. The\ndeep transfer learning models developed in this paper will\nbe described in the next section. Here we focus on the test-\ning part, that is, after the model is learned, how to use it\nfor matching a probe image against a set of gallery images\nin a test set. Note that since the test people have different\nidentities as the training people, the ID classiﬁcation subnet\nis redundant during testing. The veriﬁcation subnet could\npotentially be used to generate a matching score – given the\nprobe image and each gallery image, they can be fed into\nthe network to compute the same-identity/different-identity\nscore. However, by doing so the model becomes a cross im-\nage representation (CIR) model [52], which means that the\ninput image pair have to go through the FC layer in the sub-\nnet and the softmax loss layer. Instead, we intend to use our\nmodel as a SIR model, that is, we pre-compute the output\nvector of the base network y for the gallery; and when any\nprobe comes in, we compute its feature output and compare\nwith the gallery output vectors using a simple Euclidean dis-\ntance, which is about 3 magnitude faster in our model than\nentering the veriﬁcation subnet and computing the softmax\nscore as the distance. This testing procedure is clearly more\nsuitable for real-time applications than those of the alterna-\ntive CIR models [25, 52, 43, 50, 29, 42].\n4. Deep Transfer Learning for Re-ID\nWe consider two transfer learning settings: supervised\nwhen the target Re-ID dataset is labelled with person iden-\ntities, and unsupervised when it is unlabelled.\n4.1. Supervised Transfer Learning\nStaged transfer learning\nAs in existing Re-ID works,\nthere are two scenarios under the supervised setting: the\ntarget Re-ID dataset is ‘large’, i.e. having more than 1,000\nidentities, for instance CUHK03 [25] and Market1501 [66],\nand it is ‘small’ with less than 1,000, e.g. VIPeR [10].\nExisting deep Re-ID models are trained from scratch for\nthe large datasets, i.e., without transfer learning. For the\nsmall datasets, the models are typically pretrained on large\ndatasets (e.g., CUHK03+Market1501), and then ﬁne-tuned\non the small target dataset. We call this an one-staged trans-\nfer learning method based on an one-stepped ﬁne-tuning\nstrategy.\nWith the unique combination of classiﬁcation and veri-\nﬁcation losses and the corresponding two subnets, transfer\nlearning from ImageNet is conducted for our model regard-\nless of the target dataset size. Speciﬁcally, for a large Re-ID\ndataset, the transfer learning is one-staged, i.e., ImageNet\n→Re-ID dataset, whilst two-staged transfer learning is re-\nquired when the target dataset size is small, i.e., ImageNet\n→large Re-ID datasets →small Re-ID dataset. Impor-\ntantly, in each stage, we develop a two-stepped ﬁne-tuning\nstrategy for more effective transfer learning compared with\nthe conventional one-stepped one.\nTwo-stepped ﬁne-tuning\nThis strategy is described\nbased on the second stage of the small dataset scenario,\ni.e. large Re-ID datasets →small Re-ID dataset. The same\nstrategy is adopted for the ImageNet →large Re-ID dataset\ntransfer learning.\nSuppose we have a large source Re-ID dataset3 S and a\nsmall target dataset T with Ns and Nt unique person identi-\nties respectively. Given an initial model trained using S, our\ngoal is to transfer the learned feature representation from S\nto T. Note that the softmax ID classiﬁcation layer in the ini-\ntial network cannot be re-used because the Ns and Nt iden-\ntities have no overlap. The original Ns-nodes softmax layer\nthus has to be replaced with a randomly initialised one with\nNt nodes. In the ﬁrst step of the ﬁne-tuning, we freeze all\nother layers and train only the newly added softmax layer,\ni.e., the classiﬁcation subnet. Freezing the other parts of net-\nwork (base network + veriﬁcation subnet) is critical for this\nstage of training: without locking them, the randomly ini-\ntialised parameters of the softmax layer will backpropagate\nharmful gradients to the base network, generating ‘garbage\ngradients’ that will derail the model adaptation. After the\nsoftmax layer is fully trained so that the learned features\nfrom S can do a decent job in classifying the Nt new iden-\ntities, in the second stage we ﬁne-tune the softmax layer\nas well as all other layers of the network using the target\ndataset T. We will show in our experiments (see Sec. 5.3)\nthat the proposed two-stepped ﬁne-tuning strategy is much\nbetter than an one-stepped one.\n4.2. Unsupervised Transfer Learning\nNow the Mt target training images of a unknown number\nof identities are unlabelled. For simplicity of symbols, we\nassume they are collected from two camera views denoted\nas A and B respectively. Let’s denote the training set as\nX = {Xa, Xb}, where Xa = {xa\n1, ..., xa\nMa} contains Ma\nimages in view A, while Xb = {xb\n1, ..., xb\nMb} for the Mb\nimages in view B, we thus have Mt = Ma + Mb. For\neach image x, an D-dimensional feature vector y = φ(x) is\ncomputed by the base network to represent its appearance,\nwhere φ denote the mapping function learned by the base\nnetwork using the source dataset S. We wish to learn a\nbetter network using T with Mt unlabelled images yielding\nan updated mapping function eφ.\n3If more than one are used, they are simply merged into one.\nSelf-training\nOne solution to the unsupervised transfer\nlearning problem is to use the same two-staged supervised\ntransfer learning model with two-stepped ﬁne-tuning. In-\nstead of using the identity labels to set the training objec-\ntives, we use soft (pseudo) labels. Speciﬁcally, for each\nof the Ma images from camera A xa\ni ∈Xa, we assign it\nwith a unique class label. After that, each of the Mb images\nfrom camera B is assigned with the same label as its nearest\nneighbour from A based on ||φ(xa\ni ) −φ(xb\nj)||2. Note that\nthese labels clearly do not correspond to the real identity la-\nbels: for a start, there could be multiple images per person\nin each camera, so there are less than Ma identities; second,\nthe nearest neighbour can only give a visually similar per-\nson which by no means is always the same person. These\nsoft labels are thus highly noisy. In a self-training strategy,\nthe ﬁne-tuned network will produce an updated mapping\nfunction eφ which will be used to generate another set of\nsoft labels for retraining. Model drift is thus a big prob-\nlem: the errors in the soft labels will be propagated with the\niterations and quickly magniﬁed.\nCo-training\nOne solution to the model drift problem is\nco-training [2, 54]. It was ﬁrst designed for using the same\nmodel with two sufﬁcient and yet conditionally independent\nviews (feature representations) as inputs to label some un-\nlabelled instances for each other [2]. Since in most problem\nsettings, such views do not exist, in practice one often has\na co-training style algorithm whereby two different mod-\nels with the same features or even same model with same\nfeature but different parameter settings are used [54]. The\nkey is that both models need to be somewhat effective and\nimportantly complementary to each other.\nIn our case, we have already got the self-training deep\nCNN as one of the two models. The other unsupervised\nmodel needs to be of similar effectiveness yet complemen-\ntary. To this end, we choose a graph regularised subspace\nlearning model [15, 59]. Such a model aims to learn a dis-\ncriminative subspace where the data distribution is smooth\nwith regard to a K-nearest neighbour (KNN) graph con-\nstructed in the input feature space. In such a learned sub-\nspace, data clusters can be formed to provide the soft-labels\nfor the self-training deep model. In the meantime, it uses\nthe deep model learned feature vector y = φ(x) as model\ninput as well as to construct the graph for regularisation.\nFormally, given our pretrained deep Re-ID model, we\nobtain a feature matrix from the base network output Y =\n[Ya, Yb] ∈RD×Mt, where Ya = [ya\n1, ... , ya\nMa] ∈\nRD×Ma and Yb = [yb\n1, ... , yb\nMb] ∈RD×Mb. We aim\nto learn a subspace deﬁned by a dictionary D and a new\nrepresentation Z in the subspace. D and Z can be estimated\njointly by solving the following optimisation problem:\n(D∗, Z∗) = min\nD,Z ∥Y −DZ∥2\nF + λΩ(Z) s.t. ∥di∥2\n2 ≤1,\n(2)\nwhere the ﬁrst term is the reconstruction error evaluating\nhow well a linear combination of the learned atoms can\napproximate the input data, and ||.||F denotes the matrix\nFrobenious norm. Ω(Y) is the graph regularisation term\nthat is weighted by λ:\nΩ(Z) =\nX\nij\nWij∥zi −zj∥2\n2.\n(3)\nwhere the graph is encoded by an afﬁnity matrix W ∈\nRMt×Mt for Mt data points where Wi,j ̸= 0 only when yi\nand yj are from two different camera views and are nearest\nneighbours. With the learned new representation Z, we can\ngenerate soft labels for the unlabelled target data, that is, the\ncross-view nearest neighbours are obtained by ||za\ni −zb\nj||2\ninstead of ||φ(xa\ni )−φ(xb\nj)||2. With these soft-labels, another\nround of self-training of the deep model is carried out and\nthe updated base network then produces input vectors and\nnew graph for the subspace learning model. This iterative\nprocess normally converges after 2-3 iterations.\n5. Experiments\n5.1. Datasets and Settings\nDatasets\nFive widely used datasets are used including\ntwo large datasets and three small ones.\nCUHK03 [25]\ncontains 13,164 images of 1,360 identities from 6 cameras.\nWe use the 20 standard training/test splits as provided in\n[25]: 100 identities are randomly selected for testing and\nanother 100 for validation, whilst the remaining 1160 for\ntraining. Both manually cropped and automated detected\nperson images are used for our evaluations. As in most pre-\nvious works, we adopt the single-shot setting. Market1501\n[66] contains 32,668 detected person bounding boxes of\n1,501 identities from 6 cameras. We use the training and\ntest splits provided in [66] under both the single-query (SQ)\nand multi-query (MQ) evaluation settings. VIPeR [10] con-\ntains 632 identities and each has two images in two views\nwith distinct view angles. The 632 identities are randomly\ndivided into two equal halves, one for training and the other\nfor testing. The training process is repeated for 10 times\nwith different training/testing splits and the averaged per-\nformance is reported. PRID [14] extracts pedestrian images\nfrom recorded trajectory video frames. It has two camera\nviews, each contains 385 and 749 identities, respectively.\nOnly 200 identities appear in both views. In each of 10\nsingle-shot data split, 100 out of that 200 people are chosen\nrandomly for training, while the remaining 100 of one view\nare used as the probe set, and the remaining 649 people’s\nimages of the other view are used as gallery, which thus\nincludes the 100 people in the probe set. CUHK01 [24]\ncontains 971 individuals captured from two camera views.\nThere are two settings; the ﬁrst is the single-shot setting,\nthat is, one image for each individual in each camera view\nis randomly selected for both training and testing, and 485\nidentities are used for training and the other 486 for test-\ning. Under the other setting only 100 identities are used for\ntesting with the rest 871 for training. We use both settings\nunder the supervised setting and only the ﬁrst setting is used\nunder the unsupervised setting for fair comparisons with the\npublished results.\nEvaluation metrics\nWe use the Cumulated Matching\nCharacteristics (CMC) curve to evaluate the performance\nof Re-ID methods. Due to space limitation and for easier\ncomparison with published results, we only report the cu-\nmulated matching accuracy at selected ranks in tables rather\nthan plotting the actual curves. Note that we also use mean\naverage precision (mAP) as suggested in [66] to evaluate\nthe performance on Market-1501.\n5.2. Implementation Details\nWe use the Caffe [18] framework to implement our mod-\nels. In this section we will give some implementation details\non input data organisation, detailed structure of the veriﬁca-\ntion and classiﬁcation subnets and training settings.\nInput data organisation\nAs described above, our net-\nwork has two different tasks/training objective: the ID clas-\nsiﬁcation task and pairwise veriﬁcation task.\nThere are\ndifferent ways to organise the training images into mini-\nbatches for model training. The simplest way is to organ-\nise the training images into pairs. Speciﬁcally, one could\nrandomly select positive and negative image pairs and pack\nthem into one minibatch. However, this is very inefﬁcient\n– GPU memory is often the hardware bottleneck limiting\nthe number of pairs one could include in each minibatch.\nTo overcome this problem, we follow the minibatch gener-\nation scheme introduced in [6] which organises the mini-\nbatch according to person identities and generates pairs dy-\nnamically.\nIn particular, we keep only one set of base\nnetwork parameters in the GPU memory and organise our\nminibatches as follows: In each iteration, we randomly se-\nlect K person; for each person we then randomly select M\nimages. These K∗M distinct images are loaded to form one\nminibatch. For pair generation, we ﬁrst exhaustively gener-\native all the positive and negative pairs according to person\nidentity and then randomly duplicate the positive pairs till\nthe numbers of the positive and negative pairs are equal, i.e.,\nbalanced. In this way, much more image pairs can be gen-\nerated in each minibatch for better training of the model.\nIn our experiments, we randomly select 32 people in each\nmini-batch, and two images for each person, resulting in\n3,968 positive and negative pairs being generated respec-\ntively.\nVeriﬁcation subnet\nAs shown in Fig.1, each pair of im-\nages, after going through the GoogeLeNet base network and\npairwise-consistent dropout, are represented by two 1,024D\nvectors. Inside the veriﬁcation subnet, they are ﬁrst subject\nto an element-wise subtraction to produce a single 1,024D\nvector. After passing through a ReLU layer, this vector is\nthen fed into a 1024-dimensional FC layer, followed by a\ntwo-node softmax layer.\nClassiﬁcation subnet\nThe classiﬁcation subnet consists\nof a single N nodes softmax layer where N is the unique\nperson identities in the training set.\nAuxiliary losses\nThe original GoogLeNet [48] has an-\nother two auxiliary losses/branches extended from the mid-\ndle layers of the network. We follow this design pattern by\nadding extra ID classiﬁcation and pairwise veriﬁcation sub-\nnets on the two extended branches. This results in a total 6\nlosses in our network.\nTraining setting\nThe initial learning rate is set to 0.001\nand is multiplied by 0.1 every 40K iterations. For super-\nvised two-stepped transfer learning from ImageNet to large\nRe-ID datasets(CUHK03 and Market-1501), the network is\ntrained for 20K and 150K iterations for each step, respec-\ntively. To perform two-stepped transfer learning from large\nto small Re-ID datasets (e.g. VIPeR), we train the network\nfor 20K iterations for each step.\nData augmentation\nTo reduce overﬁtting, we also per-\nform data augmentation on the Re-ID datasets as in most\ndeep Re-ID works. Similar to [1], for each training image,\nwe generate 5 augmented images around the image center\nby performing random 2D transformation.\nParameter Settings\nFor training our supervised models,\nthe weight between the veriﬁcation loss and classiﬁcation\nloss is 3:1. For our unsupervised co-training method, there\nis one free parameter λ (see Eq.2) which needs to be deter-\nmined. This is done by cross-validation using half of the\ntraining data as the validation set.\nFor all other details about the model architecture and\ntraining, please see the source code to be released soon.\n5.3. Supervised Transfer Learning\nResults on large datasets\nOn the two large Re-ID\ndatasets, namely CUHK03 and Market, one-staged ﬁne-\ntuning is employed in our model, that is, pretraining on Ima-\ngeNet (ILSVRC 2012) followed by two-stepped ﬁne-tuning\ndetailed in Sec. 4.1. The results of our model are compared\nwith the state-of-the-art deep and non-deep Re-ID models in\nTable 1 and Table 2 respectively (they are grouped together\nin the tables). Due to space limit, only the most competi-\ntive ones since 2015 are chosen. We can make the follow-\ning observations: (1) Our model signiﬁcantly outperforms\nthe state-of-the-art: on CUHK03, the gap is 10.1% using\nthe manually cropped images and 16.0% using the detected\nones. The gap is even bigger for Market, particular on the\nmAP metric: 26.0% over Gated S-CNN [50] under the sin-\ngle query setting. (2) The best competitors on these two\nlarge datasets are all deep learning based. However, their\nadvantages over the hand-crafted feature based models are\nmodest (especially on Market) and far less pronounced than\nwhat is widely observed in other visual recognition tasks.\nThis is because the large datasets are still relatively small to\nrelease the full potential of a deep model. However, with\nour model, the gap is clear now. The main reason, as we\nexplained earlier, is that our model is able to transfer fea-\nture representations learned from ImageNet thanks to the\nManual\nDetected\nXQDA [26]\n52.2\n46.2\nMLAPG [27]\n57.9\n51.1\nDNS [61]\n62.5\n54.7\nLSSCDL [63]\n57.0\n51.2\nSiamese LSTM [51]\n-\n57.3\nIDLA [1]\n54.7\n44.9\nDGD [55]\n75.3\n-\nGated S-CNN [50]\n-\n68.1\nEDM [42]\n61.3\n52.0\nJoint Learning [52]\n-\n52.1\nCAN [29]\n65.7\n63.1\nOurs\n85.4\n84.1\nTable 1. Supervised results (Rank 1 matching accuracy in %) on\nthe CUHK03 dataset. ‘-’ means no reported result is available.\nSingle query\nMulti-query\nR1\nmAP\nR1\nmAP\nXQDA [26]\n43.8\n22.2\n54.1\n28.4\nSCSP [3]\n51.9\n26.3\n-\n-\nDNS [61]\n61.0\n35.6\n71.5\n46.0\nSiamese LSTM [51]\n-\n-\n61.6\n35.3\nGated S-CNN [50]\n65.8\n39.5\n76.0\n48.4\nCAN [29]\n48.2\n24.4\n-\n-\nOurs\n83.7\n65.5\n89.6\n73.8\nTable 2. Supervised results on Market-1501\nVIPeR\nPRID\nCUHK01\n(Nt=871/485)\nSCSP [3]\n53.5\n-\n-\nLSSCDL [63]\n42.6\n-\n-\nTMA [35]\n43.8\n-\n-\nℓ1 GL [20]\n41.5\n30.1\n-/50.1\nSiamese LSTM [51]\n42.4\n-\n-\nMetric Ensemble [38]\n45.9\n-\n-\nDNS [61]\n51.1\n40.9\n-/69.0\nIDLA [1]\n34.8\n-\n65.0/47.5\nDGD [55]\n38.6\n64.0*\n-/66.6\nMCP-CNN [4]\n47.8\n22.0\n-/53.7\nGated S-CNN [50]\n37.8\n-\n-\nEDM [42]\n40.9\n-\n86.6/-\nJoint Learning [52]\n35.8\n-\n72.5/-\nCAN [29]\n-\n-\n81.0/-\nOurs\n56.3\n43.6\n93.2 / 77.0\nTable 3. Supervised results on VIPeR, PRID and CUHK01. *The\nDGD results on PRID were obtained by using 10 times more train-\ning images from the original PRID video dataset, giving it a huge\nunfair advantage.\nselected base network (GoogLeNet) and the training objec-\ntives (classiﬁcation + veriﬁcation loss). In contrast, none of\nthe compared models transfer knowledge from other aux-\niliary sources – we found that they cannot even if they are\npretrained on ImageNet.\nResults on small datasets\nOn the three smaller datasets,\ntwo-staged transfer learning are required, i.e., ImageNet →\nCUHK03+Market →VIPeR/PRID/CUHK01.\nThe com-\nSingle query\nMulti-query\nR1\nmAP\nR1\nmAP\nSID\n76.6\n51.7\n83.6\n62.2\nPV\n74.6\n55.0\n81.5\n63.1\nTL\n63.3\n41.5\n72.4\n49.7\nSID + PV\n83.7\n65.5\n89.6\n73.8\nSID + TL\n80.4\n59.3\n86.1\n67.8\nPV + TL\n71.0\n52.4\n79.0\n60.7\nSID + PV + TL\n83.1\n65.1\n88.7\n73.0\nTable 4. Comparing different loss selections on Market1501.\nparative results are presented in Table 3.\nNote that the\ncompared hand-crafted feature based models have two sub-\ngroups: those with one type of feature and those using\nmultiple based model fusion/ensemble. In addition, most\ncompared deep models use transfer learning, but one-staged\n(typically from CUHK03+Market) and one-stepped ﬁne-\ntuning. It can be seen that our deep Re-ID model achieves\nthe best results on all three datasets. The improvements on\nthe two smallest, VIPeR and PRID, are around 3%, but on\nthe larger CUHK01, the gap is remarkable. In contrast, the\nexisting deep Re-ID models struggle on the small datasets,\nand none of them can beat the best hand-crafted features\nbased models. This is again due to their inferior transfer\nlearning ability.\nLoss selection\nWe start our ablation study by ﬁrst exam-\nining the selection of losses. We argue that it is the combi-\nnation of ID classiﬁcation loss and pairwise veriﬁcation loss\nthat enables our model to effectively transfer useful rep-\nresentations from the classiﬁcation-oriented Imagenet and\nadapt it to the veriﬁcation task of Re-ID. To validate this\nclaim, we consider three losses: softmax ID classiﬁcation\n(SID), pairwise veriﬁcation (PV) and triplet loss (TL), and\ntheir combinations. All three have been used in existing Re-\nID models, but never before has SID been combined with\nPV. We use the same base network pretrained on ImageNet\nand test on Market. We can draw the following conclusions\nfrom the results in Table 4: (1) When used alone, SID and\nPV perform similarly with TL being the worst; (2) When\nSID is used together with PV or TL, the performance im-\nproves dramatically. But without SID, PV+TL gives worse\nresult than PV alone. This suggests clearly that having the\nclassiﬁcation loss is indeed the key for knowledge transfer\nfrom ImageNet. (3) When all three losses are combined the\nperformance is slightly worse which means that with SID\nand PV, the TL loss is redundant.\nPairwise-consistent\ndropout\nand\ntwo-stepped\nﬁne-\ntuning\nTwo other contributions made in our model is\nthe pairwise-consistent dropout and the two-stepped ﬁne-\ntuning. Table 5 shows that the pairwise-consistent dropout\nbrings about 3% improvement on both Market-1501 and\nVIPeR. Note that triplet loss (TL) also beneﬁts, albeit to\na smaller extent.\nWe expect whenever these two losses\nare used, this pairwise-consistent dropout should be chosen\nLoss Type\nDropout Strategy\nMarket-1501\nVIPeR\nSID + PV\nRandom\n80.8\n53.1\nPairwise-consistent\n83.7\n56.3\nSID + TL\nRandom\n79.3\n51.9\nPairwise-consistent\n80.4\n52.3\nTable 5. Rank-1 results of different dropout strategies\nR1\nR5\nR10\nR20\nOne-stepped\n47.6\n77.2\n86.8\n93.1\nTwo-stepped\n56.3\n83.3\n90.5\n96.0\nTable 6. Two-stepped vs. one-stepped ﬁne-tuning VIPeR\nVIPeR\nPRID\nCUHK01\nDLLR [21]\n29.6\n21.1\n-\nCDTL [40]\n31.5\n24.2\n27.1\nℓ1 GL [20]\n33.5\n25.0\n41.0\nOurs\n45.1\n36.2\n68.8\nTable 7. Unsupervised transfer learning results\nover the standard random dropout. Table 6 suggests that the\ntwo-stepped ﬁne-tuning is even more critical, bringing in\nabout 8.7% at Rank 1 on VIPeR.\n5.4. Unsupervised Transfer Learning\nOur co-training based unsupervised transfer learning\nmodel is compared against the best reported results on the\nthree small datasets in Table 7. Note that to the best of our\nknowledge, no published deep Re-ID model has attempted\nthis challenging setting. The results clearly show that we\ncan beat the existing hand-crafted features based models by\nbig margins. Compared with the supervised learning re-\nsults in Table 3, our unsupervised model is very compet-\nitive, beating most of them, particularly the deep learning\nbased ones. This indicates that with the developed unsuper-\nvised deep learning model, we can readily deploy a Re-ID\nsystem to a new camera network requiring only some per-\nson detections but no manual labelling. This is thus a sig-\nniﬁcant step towards real-world deployment of automated\nRe-ID.\nOur ablation study shows that the two models employed\nin the co-training framework: a soft-label self-training deep\nmodel and a discriminative subspace learning model are\nboth effective and co-training yield clear improvements. In\naddition we compare the proposed model with existing deep\nunsupervised transfer learning models such as [9] to demon-\nstrate that our model is far more effective.\n5.5. Evaluations on Base Network Selection\nIn this experiment we compare our base network,\nGoogLeNet and the one used in the DGD model [55] which\nwe call DGDNet4. It is chosen because it is representative\nof the trending smaller/shallower bespoke Re-ID network\nand has obtained the best results among existing deep Re-\n4Note that we only use their base network, and do not follow their\njoint-training + domain guided dropout + individual dataset ﬁne-tuning\npipeline. Instead, we use the same one/two-staged and two-stepped ﬁne-\ntuning transfer learning strategy exactly as our models for fair comparison.\nDataset\nNetwork\nLoss\nI.Net Pre.?\nR1\nmAP\nMarket\nDGDNet\nSID\nYES\n47.5\n23.1\nNO\n47.8\n23.8\nSID + PV\nYES\n71.3\n48.9\nNO\n82.7\n63.4\nGoogLeNet\nSID\nYES\n76.6\n51.7\nNO\n55.0\n31.4\nSID + PV\nYES\n83.7\n65.5\nNO\n68.7\n45.3\nVIPeR\nDGDNet\nSID + PV\nYES\n37.4\nN/A\nNO\n51.5\nN/A\nGoogLeNet\nSID + PV\nYES\n56.3\nN/A\nNO\n37.0\nN/A\nTable 8. Base network comparison\nID models. Apart from the base network, the other part of\nthe models are identical, i.e., one-staged transfer learning +\ntwo stepped ﬁne-tuning for large Re-ID datasets and two-\nstaged transfer learning + two stepped ﬁne-tuning for small\nRe-ID datasets. Table 8 shows that: (1) With only SID loss,\nthe smaller base network performs much worse on Market\nwith or without pretraining on ImageNet. However, with the\nSID+PV combination, the results with DGDNet are much\nimproved, but transfer learning from Imagenet now has a\nnegative effect. (2) With GoogLeNet as base network, trans-\nfer learning from ImageNet becomes crucial – it is too big\nto be trained from any Re-ID dataset from scratch. (3) On\nthe small VIPeR dataset, with our two-staged transfer learn-\ning, the two base networks have quite different behaviours:\nWith the large Market+CUHK03 as auxiliary dataset, our\nmodel with DGDNet as base network is quite effective pro-\nvided no pretraining on ImageNet is conducted, but not as\neffective as the GoogLeNet base network with ImageNet\npretraining (51.5% vs 56.3%) – this shows the advantage\nof using a deeper base network, that is, it can learn more\ngeneralisable feature representations that can beneﬁt small\nRe-ID datasets. In summary, this results shows that even\nfor smaller deep networks tailor-made for Re-ID, combin-\ning classiﬁcation loss with veriﬁcation loss is hugely bene-\nﬁcial; but a better network design would be adopting a base\nnetwork tailor-made for ImageNet and using ImageNet as\nauxiliary dataset for transfer learning.\n5.6. Alternative Unsupervised Transfer Learning\nModels\nWe ﬁrst examine the effectiveness of the co-training\nstrategy. Our co-training model alternates between a soft-\nlabel self-training deep model and a graph-regularised sub-\nspace learning model. Table 9 shows that both models are\neffective on its own and when combined in our co-training\nframework, boost the performance by 2-3%.\nIn addition, we compare our model with two alternative\nunsupervised transfer learning methods. The ﬁrst one com-\nbines a CNN with an autoencoder. Autoencoders (AE) [13]\nis widely used in unsupervised learning and can be stacked\non top of a CNN model to turn it into an unsupervised\nR1\nR5\nR10\nR20\nSelf-training\n42.8\n66.9\n77.3\n85.9\nSubspace\n42.3\n71.5\n79.8\n87.5\nAE\n36.4\n62.3\n74.0\n81.9\nAdversarial [9]\n22.8\n38.6\n50.3\n63.9\nOurs\n45.1\n73.1\n81.7\n89.4\nTable 9. Evaluations on alternative unsupervised model on VIPeR.\nmodel. In our CNN+autoencoder model, the input layer of\nthe autoencoder is the feature output of the base network;\nthe middle layer dimension is set to 512 and the output layer\nhas the same dimension as the input layer (1,024). Formally,\nfor each input image xi in the target dataset T, the autoen-\ncoder is learned to minimise the following objective:\nJ(xi) = 1\n2||φ(xi) −fd(fe(φ(xi)))||2\n2\n(4)\nWhere fe and fd denote the mapping functions of the en-\ncoder and decoder respectively. Note that since the size of\nT is too small to train the AE from scratch, we initialize the\nparameters of the AE layers by ﬁrst pretraining them using\nimages in the source dataset S. The second model com-\npared is the deep unsupervised domain alignment model\nusing gradient reversal [9]. Speciﬁcally, we add a domain\nclassiﬁer connected to the feature extractor (i.e. our base\nnetwork) via a gradient reversal layer that multiplies the\ngradient by a certain negative constant during the back-\npropagation based training. The results in Table 9 show that\nboth the compared models yield much weaker performance\nthan the proposed co-training based model. The autoen-\ncoder model is weaker because it is not discriminative. The\ngradient reversal based models fare even worse, which sug-\ngests that the domain adaptation problem for Re-ID poses\nunique challenges that cannot be addressed by simple do-\nmain alignment.\n5.7. Qualitative Results\nTo gain some insights into what the model has actu-\nally learned and the contribution of knowledge transfer\nfrom large auxiliary dataset such as ImageNet, we visu-\nalise in Fig. 2 some feature responses at the ﬁrst convolution\nlayer of our GoogLeNet base network which is trained on\nMarket-1501 using the proposed pipeline. For comparison,\nwe also visualise the feature responses of the same layer of\nanother GoogLeNet base network. The only difference be-\ntween these two networks is that the second one is trained\nfrom scratch rather than using ImageNet pretrained param-\neters. In particular, the ﬁrst row of Fig. 2 shows the original\ninput images of two people under different camera views,\nwhilst the second and the third row shows the correspond-\ning feature responses of the two models.\nIt can be seen clearly that the learned features by the\nImageNet pretrained model ﬁre accurately at speciﬁc body\nparts. In contrast, the features learned by the network with-\nout ImageNet pretraining are much more fuzzy. This sug-\ngests that one of the key beneﬁts of pretraining on Imagenet\nis that the model is more aware of the concept of visual ob-\njects and thus is able to delineate object (person) and object\nparts (e.g. head, torso, arms etc.) more accurately, which\nlays a solid foundation for discovering discriminative fea-\ntures for matching people.\n6. Conclusion\nWe have proposed a number of novel deep transfer learn-\ning models to tackle the challenging person Re-ID problem\nwith small datasets. Our experiments validated the claim\nthat using a deep base network together with a combina-\ntion of classiﬁcation and veriﬁcation loss is key for transfer-\nring representations learned from large image classiﬁcation\ndatasets. Importantly, we show for the ﬁrs time that, a co-\ntraining based deep unsupervised transfer learning model\ncan perform effective Re-ID without any labelled data.\nReferences\n[1] E. Ahmed, M. Jones, and T. K. Marks. An improved deep\nlearning architecture for person re-identiﬁcation. In CVPR,\npages 3908–3916, 2015. 1, 2, 5, 7, 8\n[2] A. Blum and T. Mitchell. Combining labeled and unlabeled\ndata with co-training. In COLT, pages 92–100, 1998. 6\n[3] D. Chen, Z. Yuan, B. Chen, and N. Zheng. Similarity learn-\ning with spatial constraints for person re-identiﬁcation. In\nCVPR, pages 1268–1277, 2016. 1, 8\n[4] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng. Per-\nson re-identiﬁcation by multi-channel parts-based cnn with\nimproved triplet loss function. In CVPR, pages 1335–1344,\n2016. 1, 2, 8\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database. In\nCVPR, pages 248–255. IEEE, 2009. 1\n[6] S. Ding, L. Lin, G. Wang, and H. Chao.\nDeep fea-\nture learning with relative distance comparison for person\nre-identiﬁcation.\nPattern Recognition, 48(10):2993–3003,\n2015. 1, 2, 7\n[7] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell.\nDecaf: A deep convolutional\nactivation feature for generic visual recognition.\nCoRR,\nabs/1310.1531, 2013. 1\n[8] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and\nM. Cristani. Person re-identiﬁcation by symmetry-driven ac-\ncumulation of local features. In CVPR, pages 2360–2367.\nIEEE, 2010. 1\n[9] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation\nby backpropagation. In ICML, 2015. 2, 3, 9, 10\n[10] D. Gray, S. Brennan, and H. Tao.\nEvaluating appearance\nmodels for recognition, reacquisition, and tracking. In Proc.\nIEEE International Workshop on PETS, volume 3. Citeseer,\n2007. 1, 5, 6\n[11] D. Gray and H. Tao. Viewpoint invariant pedestrian recogni-\ntion with an ensemble of localized features. In ECCV, pages\n262–275. Springer, 2008. 1\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016. 1, 4\nInput Image \nWith ImageNet Pretrain\nWithout ImageNet Pretrain\nFigure 2. Visualisation of feature responses of different networks. Higher responses are indicated by warmer colours.\n[13] G. E. Hinton and R. R. Salakhutdinov.\nReducing the\ndimensionality of data with neural networks.\nScience,\n313(5786):504–507, 2006. 3, 9\n[14] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof. Person\nre-identiﬁcation by descriptive and discriminative classiﬁca-\ntion. In Scandinavian conference on Image analysis, pages\n91–102. Springer, 2011. 6\n[15] H. Hu, Z. Lin, J. Feng, and J. Zhou. Smooth representation\nclustering. In CVPR, pages 3834–3841, 2014. 6\n[16] C. Huang, C. C. Loy, and X. Tang. Unsupervised learning\nof discriminative attributes and visual representations.\nIn\nCVPR, 2016. 3\n[17] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.\nLabeled faces in the wild: A database for studying face\nrecognition in unconstrained environments. Technical Re-\nport 07-49, University of Massachusetts, Amherst, 2007. 1\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell.\nCaffe: Convolu-\ntional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093, 2014. 7\n[19] K. Kenneth, M.Joseph, R. Bhaskar, E. Kjersti, L. Te-Won,\nand S. Terrence. Dictionary learning algorithms for sparse\nrepresentation. Neural Computing, 15(2), Feb. 2003. 3\n[20] E. Kodirov, T. Xiang, Z. Fu, and S. Gong.\nPerson re-\nidentiﬁcation by unsupervised ℓ1 graph learning. In ECCV,\npages 178–195. Springer, 2016. 8, 9\n[21] E. Kodirov, T. Xiang, and S. Gong. Dictionary learning with\niterative laplacian regularisation for unsupervised person re-\nidentiﬁcation. In BMVC, volume 3, page 8, 2015. 9\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, pages 1097–1105, 2012. 1\n[23] I. Kviatkovsky, A. Adam, and E. Rivlin. Color invariants\nfor person reidentiﬁcation. IEEE TPAMI, 35(7):1622–1634,\n2013. 1\n[24] W. Li and X. Wang.\nLocally aligned feature transforms\nacross views. In CVPR, pages 3594–3601, 2013. 6\n[25] W. Li, R. Zhao, T. Xiao, and X. Wang. Deepreid: Deep ﬁlter\npairing neural network for person re-identiﬁcation. In CVPR,\npages 152–159, 2014. 1, 2, 5, 6\n[26] S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identiﬁcation\nby local maximal occurrence representation and metric\nlearning. In CVPR, pages 2197–2206, 2015. 1, 8\n[27] S. Liao and S. Z. Li. Efﬁcient psd constrained asymmetric\nmetric learning for person re-identiﬁcation. In ICCV, pages\n3685–3693, 2015. 8\n[28] G. Lisanti, I. Masi, A. D. Bagdanov, and A. Del Bimbo. Per-\nson re-identiﬁcation by iterative re-weighted sparse ranking.\nIEEE TPAMI, 37(8):1629–1642, 2015. 1\n[29] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan. End-to-end\ncomparative attention networks for person re-identiﬁcation.\narXiv preprint arXiv:1606.04404, 2016. 1, 2, 5, 8\n[30] M. Long, Y. Cao, J.Wang, and M. I. Jordan. Learning trans-\nferable features with deep adaptation networks. In ICML,\n2015. 3\n[31] M. Long, J. Wang, and M. I. Jordan. Deep transfer learn-\ning with joint adaptation networks. CoRR, abs/1605.06636,\n2016. 3\n[32] M. Long, J. Wang, and M. I. Jordan.\nUnsupervised do-\nmain adaptation with residual transfer networks.\nCoRR,\nabs/1602.04433, 2016. 2, 3\n[33] B. Ma, Y. Su, and F. Jurie. Local descriptors encoded by\nﬁsher vectors for person re-identiﬁcation. In ECCV Work-\nshops and Demonstrations, pages 413–422. Springer, 2012.\n1\n[34] L. Ma, X. Yang, and D. Tao. Person re-identiﬁcation over\ncamera networks using multi-task distance metric learning.\nIEEE TIP, 23(8):3656–3670, 2014. 1\n[35] N. Martinel, A. Das, C. Micheloni, and A. K. Roy-\nChowdhury.\nTemporal model adaptation for person re-\nidentiﬁcation. In ECCV, pages 858–877. Springer, 2016. 1,\n8\n[36] T. Matsukawa, T. Okabe, E. Suzuki, and Y. Sato. Hierarchi-\ncal gaussian descriptor for person re-identiﬁcation. In CVPR,\npages 1363–1372, 2016. 1\n[37] K. Nigam and R. Ghani. Analyzing the effectiveness and\napplicability of co-training. In CIKM, pages 86–93, 2000. 3\n[38] S. Paisitkriangkrai, C. Shen, and A. van den Hengel. Learn-\ning to rank in person re-identiﬁcation with metric ensembles.\nIn CVPR, pages 1846–1855, 2015. 8\n[39] S. Pan and Q. Yang. A survey on transfer learning. IEEE\nTKDE, 22(10):1345–1359, 2010. 3\n[40] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang,\nand Y. Tian. Unsupervised cross-dataset transfer learning for\nperson re-identiﬁcation. In CVPR, 2016. 1, 9\n[41] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-\nﬁed embedding for face recognition and clustering. In CVPR,\npages 815–823, 2015. 1\n[42] H. Shi, Y. Yang, X. Zhu, S. Liao, Z. Lei, W. Zheng, and\nS. Z. Li. Embedding deep metric for person re-identiﬁcation:\nA study against large variations. In ECCV, pages 732–748,\n2016. 1, 2, 5, 8\n[43] H. Shi, X. Zhu, S. Liao, Z. Lei, Y. Yang, and S. Z. Li.\nConstrained learning for person re-identiﬁcation.\nCoRR,\nabs/1511.07545, 2015. 1, 2, 5\n[44] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 1, 2, 4\n[45] P. Siva and T. Xiang.\nWeakly supervised object detector\nlearning with model drift detection. In ICCV, pages 343–\n350. IEEE, 2011. 3\n[46] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: a simple way to prevent neu-\nral networks from overﬁtting. Journal of Machine Learning\nResearch, 15(1):1929–1958, 2014. 2, 4\n[47] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face\nrepresentation by joint identiﬁcation-veriﬁcation. In NIPS,\npages 1988–1996. 2014. 2, 4\n[48] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In CVPR, pages 1–9, 2015.\n1, 2, 4, 7\n[49] E. Ustinova, Y. Ganin, and V. S. Lempitsky.\nMultire-\ngion bilinear convolutional neural networks for person re-\nidentiﬁcation. CoRR, abs/1512.05300, 2015. 1, 2, 4\n[50] R. R. Varior, M. Haloi, and G. Wang.\nGated siamese\nconvolutional neural network architecture for human re-\nidentiﬁcation. In ECCV, pages 791–808. Springer, 2016. 1,\n2, 4, 5, 7, 8\n[51] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang.\nA\nsiamese long short-term memory architecture for human re-\nidentiﬁcation. In ECCV, pages 135–153. Springer, 2016. 4,\n8\n[52] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang. Joint\nlearning of single-image and cross-image representations for\nperson re-identiﬁcation. In CVPR, 2016. 2, 4, 5, 8\n[53] K. Wang, L. Lin, W. Zuo, S. Gu, and L. Zhang. Dictionary\npair classiﬁer driven convolutional neural networks for ob-\nject detection. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2016. 3\n[54] W. Wang and Z. Zhou.\nAnalyzing co-training style algo-\nrithms. In ECML, pages 454–465, 2007. 6\n[55] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep fea-\nture representations with domain guided dropout for person\nre-identiﬁcation. In CVPR, 2016. 1, 2, 3, 8, 9\n[56] F. Xiong, M. Gou, O. Camps, and M. Sznaier. Person re-\nidentiﬁcation using kernel-based metric learning methods. In\nECCV, pages 1–16. Springer, 2014. 1\n[57] Y. Yang, J. Yang, J. Yan, S. Liao, D. Yi, and S. Z. Li. Salient\ncolor names for person re-identiﬁcation.\nIn ECCV, pages\n536–551. Springer, 2014. 1\n[58] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Deep metric learning for\nperson re-identiﬁcation. In ICPR, pages 34–39. IEEE, 2014.\n1, 2, 4\n[59] M. Yin, J. Gao, and Z. Lin. Laplacian regularized low-rank\nrepresentation and its applications. IEEE TPAMI, 38(3):504–\n517, 2016. 6\n[60] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-\nferable are features in deep neural networks? In NIPS, pages\n3320–3328. 2014. 3\n[61] L. Zhang, T. Xiang, and S. Gong. Learning a discriminative\nnull space for person re-identiﬁcation. In CVPR, 2016. 1, 8\n[62] X. Zhang, F. X. Yu, S. Chang, and S. Wang. Deep trans-\nfer network:\nUnsupervised domain adaptation.\nCoRR,\nabs/1503.00591, 2015. 2, 3\n[63] Y. Zhang, B. Li, H. Lu, A. Irie, and X. Ruan. Sample-speciﬁc\nsvm learning for person re-identiﬁcation. In CVPR, 2016. 1,\n8\n[64] R. Zhao, W. Ouyang, and X. Wang. Unsupervised salience\nlearning for person re-identiﬁcation. In CVPR, pages 3586–\n3593, 2013. 1\n[65] R. Zhao, W. Ouyang, and X. Wang. Learning mid-level ﬁl-\nters for person re-identiﬁcation. In CVPR, pages 144–151,\n2014. 1\n[66] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.\nScalable person re-identiﬁcation: A benchmark. In ICCV,\npages 1116–1124, 2015. 1, 5, 6, 7\n[67] L. Zheng, Y. Yang, and A. G. Hauptmann.\nPerson re-\nidentiﬁcation:\nPast, present and future.\narXiv preprint\narXiv:1610.02984, 2016. 1\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2016-11-16",
  "updated": "2016-11-22"
}