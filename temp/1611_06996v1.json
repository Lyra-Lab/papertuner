{
  "id": "http://arxiv.org/abs/1611.06996v1",
  "title": "Spatial contrasting for deep unsupervised learning",
  "authors": [
    "Elad Hoffer",
    "Itay Hubara",
    "Nir Ailon"
  ],
  "abstract": "Convolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and\ntraining methods. In this work we present a novel approach for unsupervised\ntraining of Convolutional networks that is based on contrasting between spatial\nregions within images. This criterion can be employed within conventional\nneural networks and trained using standard techniques such as SGD and\nback-propagation, thus complementing supervised methods.",
  "text": "arXiv:1611.06996v1  [stat.ML]  21 Nov 2016\nSpatial contrasting for deep unsupervised learning\nElad Hoffer\nTechnion - Israel Institute of Technology\nHaifa, Israel\nehoffer@tx.technion.ac.il\nItay Hubara\nTechnion - Israel Institute of Technology\nHaifa, Israel\nitayh@tx.technion.ac.il\nNir Ailon\nTechnion - Israel Institute of Technology\nHaifa, Israel\nnailon@cs.technion.ac.il\nAbstract\nConvolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and train-\ning methods. In this work we present a novel approach for unsupervised training\nof Convolutional networks that is based on contrasting between spatial regions\nwithin images. This criterion can be employed within conventional neural net-\nworks and trained using standard techniques such as SGD and back-propagation,\nthus complementing supervised methods.\n1\nIntroduction\nFor the past few years convolutional networks (ConvNets, CNNs) LeCun et al. [1998] have proven\nthemselves as a successful model for vision related tasks Krizhevsky et al. [2012] Mnih et al. [2015]\nPinheiro et al. [2015] Razavian et al. [2014]. A convolutional network is composed of multiple con-\nvolutional and pooling layers, followed by a fully-connected afﬁne transformations. As with other\nneural network models, each layer is typically followed by a non-linearity transformation such as a\nrectiﬁed-linear unit (ReLU).\nA convolutional layer is applied by cross correlating an image with a trainable weight ﬁlter. This\nstems from the assumption of stationarity in natural images, which means that features learned for\none local region in an image can be shared for other regions and images.\nDeep learning models, including convolutional networks, are usually trained in a supervised man-\nner, requiring large amounts of labeled data (ranging between thousands to millions of examples\nper-class for classiﬁcation tasks) in almost all modern applications. These models are optimized a\nvariant of stochastic-gradient-descent (SGD) over batches of images sampled from the whole train-\ning dataset and their ground truth-labels. Gradient estimation for each one of the optimized param-\neters is done by back propagating the objective error from the ﬁnal layer towards the input. This is\ncommonly known as \"backpropagation\" Rumelhart et al..\nOne early well known usage of unsupervised training of deep architectures was as part of a pre-\ntraining procedure used for obtaining an effective initial state of the model. The network was later\nﬁne-tuned in a supervised manner as displayed by Hinton [2007]. Such unsupervised pre-training\nprocedures were later abandoned, since they provided no apparent beneﬁt over other initialization\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nheuristics in more careful fully supervised training regimes. This led to the de-facto almost exclusive\nusage of neural networks in supervised environments.\nIn this work we will present a novel unsupervised learning criterion for convolutional network based\non comparison of features extracted from regions within images. Our experiments indicate that by\nusing this criterion to pre-train networks we can improve their performance and achieve state-of-the-\nart results.\n2\nProblems with Current Approaches\nThe majority of unsupervised optimization criteria currently used are based on variations of recon-\nstruction losses. One limitation of this fact is that a pixel level reconstruction is non-compliant with\nthe idea of a discriminative objective, which is expected to be agnostic to low level information in the\ninput. In addition, it is evident that MSE is not best suited as a measurement to compare images, for\nexample, viewing the possibly large square-error between an image and a single pixel shifted copy\nof it. Another problem with recent approaches such as Rasmus et al. [2015], Zeiler et al. [2010]\nis their need to extensively modify the original convolutional network model. This leads to a gap\nbetween unsupervised method and the state-of-the-art, supervised, models for classiﬁcation - which\ncan hurt future attempt to reconcile them in a uniﬁed framework, and also to efﬁciently leverage\nunlabeled data with otherwise supervised regimes.\n3\nLearning by Comparisons\nThe most common way to train NN is by deﬁning a loss function between the target values and the\nnetwork output. Learning by comparison approaches the supervised task from a different angle. The\nmain idea is to use distance comparisons between samples to learn useful representations. For exam-\nple, we consider relative and qualitative examples of the form “X1 is closer to X2 than X1 is to X3.\nUsing a comparative measure with neural network to learn embedding space was introduced in the\n“Siamese network” framework by Bromley et al. [1993] and later used in the works of Chopra et al.\n[2005]. One use for this methods is when the number of classes is too large or expected to vary over\ntime, as in the case of face veriﬁcation, where a face contained in an image has to compared against\nanother image of a face.\n4\nOur Contribution: Spatial Contrasting\nOne implicit assumption in convolutional networks, is that features are gradually learned hierarchi-\ncally, each level in the hierarchy corresponding to a layer in the network. Each spatial location\nwithin a layer corresponds to a region in the original image. It is empirically observed that deeper\nlayers tend to contain more ‘abstract’ information from the image. Intuitively, features describing\ndifferent regions within the same image are likely to be semantically similar (e.g. different parts\nof an animal), and indeed the corresponding deep representations tend to be similar. Conversely,\nregions from two probably unrelated images (say, two images chosen at random) tend to be far from\neach other in the deep representation. This logic is commonly used in modern deep networks such\nas Szegedy et al. [2015] Lin et al. [2013] He et al. [2015], where a global average pooling is used to\naggregate spatial features in the ﬁnal layer used for classiﬁcation.\nOur suggestion is that this property, often observed as a side effect of supervised applications, can be\nused as a desired objective when learning deep representations in an unsupervised task. Later, the re-\nsulting representation can be used, as typically done, as a starting point or a supervised learning task.\nWe call this idea which we formalize below Spatial contrasting. The spatial contrasting criterion\nis similar to noise contrasting estimation Gutmann and Hyvärinen [2010] Mnih and Kavukcuoglu\n[2013], in trying to train a model by maximizing the expected probability on desired inputs, while\nminimizing it on contrasting sampled measurements.\n4.1\nFormulation\nWe will concern ourselves with samples of images patches ˜x(m) taken from an image x. Our con-\nvolutional network model, denoted by F(x), extracts spatial features f so that f (m) = F(˜x(m))\n2\nfor an image patch ˜x(m). We wish to optimize our model such that for two features representing\npatches taken from the same image ˜x(1)\ni\n, ˜x(2)\ni\n∈xi for which f (1)\ni\n= F(˜x(1)\ni\n) and f (2)\ni\n= F(˜x(2)\ni ),\nthe conditional probability P(f (1)\ni\n|f (2)\ni\n) will be maximized.\nThis means that features from a patch taken from a speciﬁc image can effectively predict, under our\nmodel, features extracted from other patches in the same image. Conversely, we want our model\nto minimize P(fi|fj) for i, j being two patches taken from distinct images. Following the logic\npresented before, we will need to sample contrasting patch ˜x(1)\nj\nfrom a different image xj such that\nP(f (1)\ni\n|f (2)\ni\n) > P(f (1)\nj\n|f (2)\ni\n), where f (1)\nj\n= F(˜x(1)\nj ). In order to obtain contrasting samples, we use\nregions from two random images in the training set. We will use a distance ratio, described earlier\n?? for the supervised case, to represent the probability two feature vectors were taken from the same\nimage. The resulting training loss for a pair of images will be deﬁned as\nLSC(x1, x2) = −log\ne−∥f (1)\n1\n−f (2)\n1\n∥2\ne−∥f (1)\n1\n−f (2)\n1\n∥2 + e−∥f (1)\n1\n−f (1)\n2\n∥2\n(1)\nEffectively minimizing a log-probability under the SoftMax measure.\n4.2\nMethod\nSince training convolutional network is done in batches of images, we can use the multiple samples\nin each batch to train our model. Each image serves as a source for both an anchor and positive\npatches, for which the corresponding features should be closer, and also a source for contrasting\nsamples for all the other images in that batch. For a batch of N images, two samples from each\nimage are taken, and N 2 different distance comparisons are made. The ﬁnal loss is the average\ndistance ratio for images in the batch:\n¯\nLSC({x}N\ni=1) = 1\nN\nN\nX\ni=1\nLSC(xi, {x}j̸=i) = −1\nN\nN\nX\ni=1\nlog\ne−∥f (1)\ni\n−f (2)\ni\n∥2\nPN\nj=1 e−∥f (1)\ni\n−f (2)\nj\n∥2\n(2)\nSince the criterion is differentiable with respect to its inputs, it is fully compliant with standard\nmethods for training convolutional network and speciﬁcally using backpropagation and gradient\ndescent. Furthermore, SC can be applied to any layer in the network hierarchy. In fact, SC can be\nused at multiple layers within the same convolutional network.\n5\nExperiments\nIn this section we report empirical results showing that using SC loss as an unsupervised pretraining\nprocedure can improve state-of-the-art performance on subsequent classiﬁcation. In each one of the\nexperiments, we used the spatial contrasting criterion to train the network on the unlabeled images.\nWe then used the trained model as an initialization for a supervised training on the complete labeled\ndataset.\n5.1\nResults on STL10\nThis dataset consists of 100, 000 96 × 96 colored, unlabeled images, together with another set of\n5, 000 labeled training images and 8, 000 test images . The label space consists of 10 object classes.\n5.2\nResults on Cifar10\nThe well known CIFAR-10 is an image classiﬁcation benchmark dataset containing 50, 000 training\nimages and 10, 000 test images. The image sizes 32×32 pixels, with color. The classes are airplanes,\nautomobiles, birds, cats, deer, dogs, frogs, horses, ships and trucks For Cifar10, we used a previously\nused setting Coates and Ng [2012] Hui [2013] Dosovitskiy et al. [2014] to test a model’s ability to\nlearn from unlabeled images. In this setting, only 4, 000 samples from the available 50, 000 are used\nwith their label annotation, but the entire dataset is used for unsupervised learning.\n3\nTable 1: State of the art results on STL-10 dataset\nModel\nSTL-10 test accuracy\nZero-bias Convnets - Paine et al. [2014]\n70.2%\nTriplet network - Hoffer and Ailon [2015]\n70.7%\nExemplar Convnets - Dosovitskiy et al. [2014]\n72.8%\nTarget Coding - Yang et al. [2015]\n73.15%\nStacked what-where AE - Zhao et al. [2015]\n74.33%\nSpatial contrasting initialization (this work)\n81.34% ± 0.1\nThe same model without initialization\n72.6% ± 0.1\nTable 2: State of the art results on Cifar10 dataset with only 4000 labeled samples\nModel\nCifar10 (400 per class) test accuracy\nConvolutional K-means Network - Coates and Ng [2012]\n70.7%\nView-Invariant K-means - Hui [2013]\n72.6%\nDCGAN - Radford et al. [2015]\n73.8%\nExemplar Convnets - Dosovitskiy et al. [2014]\n76.6%\nLadder networks - Rasmus et al. [2015]\n79.6%\nSpatial contrasting initialization (this work)\n79.2% ± 0.3\nThe same model without initialization\n72.4% ± 0.1\n6\nConclusions and future work\nIn this work we presented spatial contrasting - a novel unsupervised criterion for training convo-\nlutional networks on unlabeled data. Its is based on comparison between spatial features sampled\nfrom a number of images. We’ve shown empirically that using spatial contrasting as a pretraining\ntechnique to initialize a ConvNet, can improve its performance on a subsequent supervised training.\nIn cases where a lot of unlabeled data is available, such as the STL10 dataset, this translates to\nstate-of-the-art classiﬁcation accuracy in the ﬁnal model.\nSince the spatial contrasting loss is a differentiable estimation that can be computed within a net-\nwork in parallel to supervised losses, future work will attempt to embed it as a semi-supervised\nmodel. This usage will allow to create models that can leverage both labeled an unlabeled data,\nand can be compared to similar semi-supervised models such as the ladder network Rasmus et al.\n[2015]. It is is also apparent that contrasting can occur in dimensions other than the spatial, the most\nstraightforward is the temporal one. This suggests that similar training procedure can be applied on\nsegments of sequences to learn useful representation without explicit supervision.\nReferences\nJane Bromley, James W Bentz, Léon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard Säckinger, and\nRoopak Shah. Signature veriﬁcation using a “siamese” time delay neural network. International Journal of\nPattern Recognition and Artiﬁcial Intelligence, 7(04):669–688, 1993.\nSumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application\nto face veriﬁcation. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society\nConference on, volume 1, pages 539–546. IEEE, 2005.\nAdam Coates and Andrew Y Ng. Learning feature representations with k-means. In Neural Networks: Tricks\nof the Trade, pages 561–580. Springer, 2012.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsuper-\nvised feature learning with convolutional neural networks. In Advances in Neural Information Processing\nSystems, pages 766–774, 2014.\nMichael Gutmann and Aapo Hyvärinen.\nNoise-contrastive estimation: A new estimation principle for un-\nnormalized statistical models. In International Conference on Artiﬁcial Intelligence and Statistics, pages\n297–304, 2010.\n4\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv\npreprint arXiv:1512.03385, 2015.\nGeoffrey E Hinton. To recognize shapes, ﬁrst learn to generate images. Progress in brain research, 165:\n535–547, 2007.\nElad Hoffer and Nir Ailon. Deep metric learning using triplet network. In Similarity-Based Pattern Recognition,\npages 84–92. Springer, 2015.\nKa Y Hui. Direct modeling of complex invariances for visual object features. In Proceedings of the 30th\nInternational Conference on Machine Learning (ICML-13), pages 352–360, 2013.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning, pages\n448–456, 2015.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classiﬁcation with Deep Convolutional\nNeural Networks. Advances In Neural Information Processing Systems, pages 1–9, 2012.\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.\nAndriy Mnih and Koray Kavukcuoglu. Learning word embeddings efﬁciently with noise-contrastive estimation.\nIn Advances in Neural Information Processing Systems, pages 2265–2273, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529–533, 2015.\nTom Le Paine, Pooya Khorrami, Wei Han, and Thomas S Huang. An analysis of unsupervised pre-training in\nlight of recent advances. arXiv preprint arXiv:1412.6597, 2014.\nPedro O Pinheiro, Ronan Collobert, and Piotr Dollar. Learning to segment object candidates. In Advances in\nNeural Information Processing Systems, pages 1981–1989, 2015.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional\ngenerative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nAntti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning\nwith ladder networks. In Advances in Neural Information Processing Systems, pages 3532–3540, 2015.\nAli Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.\nCnn features off-the-shelf: an\nastounding baseline for recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops, pages 806–813, 2014.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating\nerrors. Cognitive modeling, 5(3):1.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,\nVincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 1–9, 2015.\nShuo Yang, Ping Luo, Chen Change Loy, Kenneth W Shum, and Xiaoou Tang. Deep representation learning\nwith target coding. 2015.\nMatthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks. In Computer\nVision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2528–2535. IEEE, 2010.\nJunbo Zhao, Michael Mathieu, Ross Goroshin, and Yann Lecun. Stacked what-where auto-encoders. arXiv\npreprint arXiv:1506.02351, 2015.\n5\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2016-11-21",
  "updated": "2016-11-21"
}