{
  "id": "http://arxiv.org/abs/1611.07174v2",
  "title": "Deep Recurrent Convolutional Neural Network: Improving Performance For Speech Recognition",
  "authors": [
    "Zewang Zhang",
    "Zheng Sun",
    "Jiaqi Liu",
    "Jingwen Chen",
    "Zhao Huo",
    "Xiao Zhang"
  ],
  "abstract": "A deep learning approach has been widely applied in sequence modeling\nproblems. In terms of automatic speech recognition (ASR), its performance has\nsignificantly been improved by increasing large speech corpus and deeper neural\nnetwork. Especially, recurrent neural network and deep convolutional neural\nnetwork have been applied in ASR successfully. Given the arising problem of\ntraining speed, we build a novel deep recurrent convolutional network for\nacoustic modeling and then apply deep residual learning to it. Our experiments\nshow that it has not only faster convergence speed but better recognition\naccuracy over traditional deep convolutional recurrent network. In the\nexperiments, we compare the convergence speed of our novel deep recurrent\nconvolutional networks and traditional deep convolutional recurrent networks.\nWith faster convergence speed, our novel deep recurrent convolutional networks\ncan reach the comparable performance. We further show that applying deep\nresidual learning can boost the convergence speed of our novel deep recurret\nconvolutional networks. Finally, we evaluate all our experimental networks by\nphoneme error rate (PER) with our proposed bidirectional statistical n-gram\nlanguage model. Our evaluation results show that our newly proposed deep\nrecurrent convolutional network applied with deep residual learning can reach\nthe best PER of 17.33\\% with the fastest convergence speed on TIMIT database.\nThe outstanding performance of our novel deep recurrent convolutional neural\nnetwork with deep residual learning indicates that it can be potentially\nadopted in other sequential problems.",
  "text": "1\nDeep Recurrent Convolutional Neural Network:\nImproving Performance For Speech Recognition\nZewang Zhang, Student Member, IEEE, Zheng Sun, Student Member, IEEE, Jiaqi Liu, Student Member, IEEE,\nJingwen Chen, Student Member, IEEE, Zhao Huo, Member, IEEE, and Xiao Zhang, Member, IEEE\nAbstract—A deep learning approach has been widely applied in\nsequence modeling problems. In terms of automatic speech recog-\nnition (ASR), its performance has signiﬁcantly been improved\nby increasing large speech corpus and deeper neural network.\nEspecially, recurrent neural network and deep convolutional\nneural network have been applied in ASR successfully. Given\nthe arising problem of training speed, we build a novel deep\nrecurrent convolutional network for acoustic modeling and then\napply deep residual learning to it. Our experiments show that\nit has not only faster convergence speed but better recognition\naccuracy over traditional deep convolutional recurrent network.\nIn the experiments, we compare the convergence speed of our\nnovel deep recurrent convolutional networks and traditional deep\nconvolutional recurrent networks. With faster convergence speed,\nour novel deep recurrent convolutional networks can reach the\ncomparable performance. We further show that applying deep\nresidual learning can boost the convergence speed of our novel\ndeep recurret convolutional networks. Finally, we evaluate all our\nexperimental networks by phoneme error rate (PER) with our\nproposed bidirectional statistical n-gram language model. Our\nevaluation results show that our newly proposed deep recurrent\nconvolutional network applied with deep residual learning can\nreach the best PER of 17.33% with the fastest convergence speed\non TIMIT database. The outstanding performance of our novel\ndeep recurrent convolutional neural network with deep residual\nlearning indicates that it can be potentially adopted in other\nsequential problems.\nIndex Terms—Convergence speed, deep recurrent convolu-\ntional neural network, deep residual learning, acoustic modeling\nI. INTRODUCTION\nS\nEQUENCE modeling is an important part of artiﬁcial\nintelligence, and an efﬁcient sequential model can help\nmachine learn how to think as human intelligence and how\nto interact with the world by sequential actions and logical\nthinking skills. A major problem faced by sequence modeling\nis that the deep learning model suffers too much training time,\nand new powerful sequential models should be invented to\nmeet the demand of both robustness and efﬁciency.\nAutomatic speech recognition(ASR) is designed to tran-\nscript human speech into spoken phonemes. ASR has been\nZ. Zhang, Z. Sun, J. Liu, J. Chen and X. Zhang are with the Department\nof Physics, Sun Yat–Sen University, Guangzhou, 510275 P.R. China\n(e-mail:\nzhangzw3@mail2.sysu.edu.cn;\nsunzh6@mail2.sysu.edu.cn;\nliujq33@mail2.sysu.edu.cn;\nchenjw93@mail2.sysu.edu.cn;\nzhangx-\niao@mail.sysu.edu.cn).\nZ. Huo is with China University of Political Science and Law (e-mail:\nhuozhao@cupl.edu.cn).\nC. H. Lee is with the Institute of High Performance Computing (e-mail:\ncalvin-lee@ihpc.a-star.edu.sg).\nManuscript received MM DD, YYYY; revised MM DD, YYYY.\ninvestigated for several decades. Traditionally, a statistical\nmodel of maximum likelihood decoding and maximum mutual\ninformation estimation are used for speech recognition [1],\n[2], the use of gaussian mixture model (GMM) combined with\nhidden markov model (HMM) for speech recognition had also\nbecome predominant several years ago [3], [4], [5]. With the\nspring-up of deep learning, deep neural network (DNN) with\nHMM states has been shown to outperform the traditional\nmethod of GMM-HMM [6], [7], [8], [9], [10], [11], thus\nmany new training tricks have been proposed to improve the\nperformance of DNNs for acoustic modeling, such as powerful\nnon-linear activation functions, layer-wise mini-batch training,\nbatch normalization, dropout and fast gradient descent method\n[21], [22], [23], [24], [25].\nDNN is very good at exploiting non-linear feature repre-\nsentation, but it lacks internal time dependency and sequential\nmodeling ability, since human speech is a sequential problem\nwith dynamic features to handle. Recurrent neural network\n(RNN) is a powerful tool for sequential modeling owing to its\nrecurrent hidden states between adjacent time-steps, and DNNs\nare gradually replaced by RNNs which have been successfully\napplied in ASR in the last several years. Meanwhile, Deep\nLong Short-term Memory RNNs and deep bidirectional RNNs\nare proposed to exploit long time memory in ASR [12], [13],\n[14], [15], [16], [17]. Besides, sequence training of RNNs with\nconnectionist temporal classiﬁcation (CTC) has shown great\nperformance in end-to-end ASR [18], [19], [20]. Traditional\nframe-wise cross entropy training needs pre-segmented data\nby hand, but CTC is an end-to-end training method for\nRNNs which decodes the output probability distribution into\nphoneme sequences without requiring pre-segmented training\ndata. RNN has been widely used in ASR, but RNN can’t depict\nvery long time dependency because of its vanishing gradient\nproblem, and deeper RNN seems to have little improvement\nwhen the number of layers reaches a limit. Although LSTM\nimproves the performance of RNN, an disadvantage of LSTM\nis that it requires too much computation and stores multiple\ngating neural responses at each time-step, which will become\na computational bottleneck.\nVery recently, some other novel neural networks structures\nhave been proposed, of which convolutional neural network\n(CNN) is one of the most attractive models. CNN is an older\ndeep neural network architecture [26], and has enjoyed the\ngreat popularity as a efﬁcient approach in character recognition\n[27]. Human speech, which is also a sequential signal, can be\ntransformed into a feature map that we can take similarly as\nan image. Human speech signals are highly variable because\narXiv:1611.07174v2  [cs.CL]  27 Dec 2016\n2\nof different speaking accents, different speaking styles and\nuncertain noises from the environment. For speech recognition,\nCNN has several advantages: (i) human speech signal has\nlocal correlations in both time and frequency, CNN is well\nsuited to exploit these correlations explicitly through a local\nconnectivity. (ii) CNN has the ability to capture the frequency\nshift in human speech signal.\nSome researchers proposed CNN can be used in speech\nrecognition, and it has been proved that deep CNN has better\nperformance over general feed-forward neural network or\nGMM-HMM in several speech recognition tasks [31], [32],\n[33], [34]. Most of previous application of CNNs in speech\nrecognition only used fewer convolutional layers. For example,\nAbdel-Hamid et al. [28] used one convolutional layer, one\npooling layer and a few full-connected layers. Amodei et al.\n[13] also used only three convolutional layers as the feature\npreprocessing layers. Some researcher has shown that CNN-\nbased speech recognition which uses raw speech as input can\nbe more robust [35], and very deep CNNs has also been proved\nto show great performance in noisy speech recognition and\nLarge Vocabulary Continuous Speech Recognition (LVCSR)\ntasks [29], [30], [33], [37]. Generally, very small ﬁlters with\n3*3 kernels have recently been successfully applied in acoustic\nmodeling in hybrid NN-HMM speech recognition system, and\npooling layer has been proved to be replaced by full-connected\nconvolutional layers and pooling has no highlights for LVCSR\ntasks [36].\nCNN has the ability to exploit the internal dependency\nof speech sequences while having the advantage of fewer\nparameters than RNN. This is to say, we can use less complex\ncomputational cost to achieve the same performance as RNN.\nWith the datasets of human speech becoming larger, we ﬁrmly\nbelieve that overﬁtting would be less important but the con-\nvergence speed is what we always care about. Meanwhile, we\nﬁnd there are few work that discusses the convergence speed\nof different conﬁgurations in deep CNNs for ASR. Typical\narchitecture of deep CNNs for ASR usually contains some\nfully-connected convolutional layers at the bottom, followed\nby several recurrent layers and fully-connected feedforward\nlayers, but we ﬁnd that, in practice, it’s too slow to train\nthis type of architecture for acoustic modeling. Recently, deep\nresidual learning has been shown to obtain good convergence\nperformance and compelling convergence in computer vision\n[38], [39], which attributes to its identity mapping as the\nskip connection in the residual block. To explore how can we\nattain an architecture with faster convergence, we (i) propose\na novel deep recurrent convolutional networks and compare\nits performance with traditional deep convolutional recurrent\nnetworks, and (ii) apply deep residual learning in all of our\nexperimental models. In detail, we compare the convergence\nspeed of above three different architectures, besides, we eval-\nuate three different architecutres combined with our proposed\nbidirectional statistical n-gram language model by PER. Our\nwork has meaningful reference for who are also applying deep\nCNNs to attain a faster convergence speed in ASR.\nThis paper is organized as follows. First, we review the\nbasics of commonly used models in ASR including recurrent\nneural network and convolutional neural network (Section II),\nthen we explain how the end-to-end approach of connectionist\ntemporal classiﬁcation can be used to decode the output\nphonemes probability distribution (Section III). Besides, we\npropose a bidirectional statistical n-gram language model to\nrectify the output sequences of acoustic model in Section\nIV. Section V explains the setup and some training details\nin our experiments. Section VI presents the comparison of\nthe convergence speed between traditional deep convolutional\nrecurrent networks, our novel deep recurrent convolutional net-\nworks and those applied with deep residual learning. Finally,\nwe show the evaluation results by minimum test PER of all\nexperimental architectures in Section VII.\nII. REVIEW OF NEURAL NETWORK\nA. ELU Nonlinearity\nThe most common functions applied to a neuron’s output is\nReLU [23] as a function of its input with f(x) = max(0, x).\nReLU behaves better than traditional non-linear functions such\nas sigmoid or tanh, since the mean value of ReLU’s activations\nis not zero, some neurons in practice always become dead\nduring backpropagation. Exponential linear unit (ELU) was\nintroduced in [24], in contrast to ReLU, ELU has negative\nvalues which pushes the mean of activations close to zero,\nthat is to say, ELU can decrease the gap between the normal\ngradient and unit natural gradient and, therefore, speed up\ntraining. The expression of ELU nonlinearity is shown below\nin Equation ??.\nf(x) =\n\u001a x\nif x > 0\nα(exp(x) −1)\nif x ⩽0\n(1)\nSince our work is based on deep CNNs, we can take ELU as\nthe non-linear function to faster our network’s convergence.\nFast learning has a great impact on performance of training\nlarge datasets.\nB. Recurrent Neural Network\nGeneral forward neural network can’t depict the time\ndependency for sequence modeling problems well, such as\nautomatic speech recognition. One type of special forward\nneural network is recurrent neural network(RNN). When RNN\nis folded out in time, it can be considered as a DNN with\nmany sequential layers. In contrast to forward neural network,\nRNN is used to build time dependency of input features with\ninternal memory units. RNN can receive input and produce\noutput at every layer, the general architecture of RNN is shown\nin Figure 1. As shown in Figure 1, a very simple RNN is\ncomposed of an input layer, a hidden layer and an output layer.\nThe recurrent hidden layer is designed to pass the forward\ninformation to backward time-steps. We can depict internal\nrelationship of a general RNN in Equation (??).\nOt = f(yt) = f(Whh ∗ht−1 + Wxh ∗xt + bt)\n(2)\nwhere Whh is the weight matrix between adjacent hidden\nunits, ht−1 is the hidden unit of previous time-step, Wxh is\nthe weight matrix between input layer and hidden layer, xt\nis the input at time t, and the bias bt is added and ﬁnally\nan activation function f(·), typically sigmoid, tanh, ReLU or\n3\n. . .\n. . .\n. . .\nFig. 1.\nGeneral architecture of a very simple RNN with a single hidden\nlayer unfolded in time. Three layers shown above are input layer, recurrent\nhidden layer and output layer, respectively. The state of each hidden unit at t\nis decided by the state of hidden unit at t-1 and input at t.\nELU, will be applied to generate the output of the recurrent\nlayer. If several recurrent layers are stacked, the output of\nprevious layer becomes the input of next layer.\nC. Convolutional Neural Network\nCompared to standard fully-connected neural networks and\nRNNs, CNNs which are proposed in [27] have much fewer\nfree parameters so that they are easier to train. CNNs are pretty\nsimilar to ordinary neural networks, they are made of trainable\nweights and bias and they can also be stacked to a deep depth,\nwhich has been successfully applied in ImageNet competition\n[44].\nA typical architecture of simple CNN is composed of a\nconvolutional layer and a pooling layer which is shown in\nFigure 2. In most cases, a typical convolutional layer contains\nseveral feature maps, each of which is a kind of ﬁlter with\nshared parameters. These ﬁlters are spatial and extend through\nthe full depth of input volume. Pooling layer is designed for\ndimensionality reduction and full-connected layer can output\nthe probability distribution of all different classes.\nFig. 2.\nGeneral architecture of 1-dimensional CNN. A typical CNN is\ncomposed of a convolutional layer and a pooling layer. We present a\nconvolutional layer with ﬁlter size 1*3 and a pooling layer with pool size\n1*3. We set the stride size to be 1 and no padding.\nIn our experimented model, we replace the pooling layer\nwith the full convolutional layer. Especially, we pad the input\nlayer with zeros in both dimensions, since that the output layer\ncan be the same size as the input layer if we set the stride to\nbe 1. The details of padding is shown in Figure 3.\nIII. REVIEW OF CONNECTIONIST TEMPORAL\nCLASSIFICATION\nWe also replace the traditional HMM decoder for output\nsequences. Labelling unsegmented sequential data is very\n1\n3\n5\n4\n2\n7\n9\n6\n8\n1\n3\n5\n4\n2\n7\n9\n6\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n-1 0\n0\n1\n1\n1\n-1 0\n*\nPadding=same\n=\n0 10 0\n-4 9\n0\n11 12 1\ninput\npadded with zeros\nfilter\noutput\nFig. 3. Example of convolution with padding in two dimensions. A 3*3 region\nis padded with zeros to a 5*5 region, then each 3*3 region is multiplied by\nthe convolutional kernel to attain the output.\ncommon in speech recognition, and CTC is good at achieving\nthis. The basic idea of CTC is to interpret the outputs of\nnetwork as a probability distribution over all possible phe-\nnomes. Given this distribution, we can derive the objective\nfunction of sequence labeling. Since the objective function is\ndifferentiable, we can train it by backpropagation through time\nalgorithm.\nUsing the probability distributions learned by deep CNNs,\nwe would then use a CTC loss layer to ﬁnally output the\nphenome sequence. For a given input sequence, the goal of\nCTC is to minimize the average edit distance from the output\nsequence to actual sequence. Suppose L is an alphabet of\nall output phonemes, in addition to L, the CTC network\nneeds one more blank label, which is inserted between the\nadjacent two non-blank labels. The ﬁrst |L| activations stand\nfor the probability of corresponding non-blank labels which we\nobserved, and the probability of extra blank label is interpreted\nin the last activation unit. Given the output probability of CTC\nnetwork, we can use beam search algorithm to compute the\ntotal probability of any sequence by considering its all paths\nas shown in Figure 4.\nIn detail, if we deﬁne yt\nk as the probability of outputting\nlabel k at time t, which deﬁnes a distribution over the set L\n′T\nof length T sequences over the alphabet L\n′ = L ∩{blank}:\np(π | x) =\nT\nY\nt=1\nyt\nπt, ∀π ∈L\n′T\n(3)\nIn (??), we take the elements of L\n′T as different paths, which\nis denoted as π. Given a labelling l and a mapping function B\nwhich removes all blanks and repeated labels, we can compute\nits probability by summing all its possible paths:\np(l | x) =\nX\nπ∈B−1(l)\np(π | x)\n(4)\nTherefore, the output sequence should be the most probable\nlabelling, it’s a decoding problem about how to ﬁnd the\noutput sequence. We require an efﬁcient way of calculating\nthe probabilities p(l | x) of each labelling. Since from (??)\nwe may feel that it’s very difﬁcult to handle because there\nare many paths corresponding to a giving labelling. However,\nthe problem can be converted to a dynamic programming\nproblem, like what we use in HMMs. The key point of\ndynamic algorithm is to break down the sum over all paths\ninto forward and backward variables.\nTo account for the blanks, CTC considers adding blanks\nbetween every pair of non-blank labels, including the begin-\nning and the end. Therefore, the length of modiﬁed label l′\n4\nis 2|l| + 1. Since the state transition space for dynamic pro-\ngramming shouldn’t be too large for computational efﬁciency,\nwe assume that state transition only occurs between blank and\nnon-blank label or two distinct non-blank labels. Besides, we\nallow all preﬁxes begins with a blank or the ﬁrst symbol in l\nand ends with a blank or the last symbol in l. Thus, we can\n.    .    .\nae\ndcl\ny\ner\nblank\nblank\nblank\nblank\nblank\nt=0\nt=1\nt=2\nt=T-2\nt=T-1\nt=T\nFig. 4. Illustration of the forward backward algorithm. White circle represents\nCTC blank, while yellow circle represents non-blank label. The direction of\narrows represents what the forward variables are updated to, and backward\nvariables are updated against them.\ndraw the expressions of dynamic program formulation from\nFigure 4. Deﬁne the forward variable\nαt(s) = αt−1(s) + αt(s −1)\n(5)\nWe can conclude that if the current label is blank or the current\nblank is the same as it was two steps ago, then the current\nforward variable is deﬁned\nαt(s) = αt(s)yt\nl′\ns\n(6)\nOtherwise, the current forward variable is deﬁned\nαt(s) = (αt(s) + αt−1(s −2))yt\nl′\ns\n(7)\nSince the last label must only be a blank or the last label in l,\nthe ﬁnal probability of a given label is calculated by forward\nvariables as\np(l|x) = αT (|l|) + αT (|l| −1)\n(8)\nThe forward variables are deﬁned previously, and the back-\nward variables can be deﬁned similarly. In practice, in order\nto avoid any underﬂows on any digital computer, we must\nnormalize both the forward variables and backward variables.\nFinally, we can calculate the maximum likelihood error as\nln(p(l|x)) =\nT\nX\nt=1\nln(\nX\ns\nαt(s))\n(9)\nFor forward and backward variables deﬁned above, we can\ncalculate the probability of any labels occurred at any time t\ngiven a labelling l as\nαt(s)βt(s) = yt\nls\nX\nπ∈β−1(l)\np(π|x)\n(10)\nSince p(l | x) can be obtained by summing over all s,\ndifferentiating this w.r.t yt\nk, we only consider paths that go\nthrough label k at time t.\np(l | x)\nyt\nk\n=\n1\nyt\nk\n2\nX\ns∈pos(k,l)\nαt(s)βt(s)\n(11)\npos(k, l) represents the set of positions where k occurs in l,\nso the objective function’s gradient is\n1\nyt\nk\n(−ln(p(l | x))) =\n1\np(l | x)\n1\nyt\nk\n2 P\ns∈pos(k,l) αt(s)βt(s)\n(12)\nOur prediction of acoustic modeling is tied to CTC loss, and\nthe loss takes the output of deep convolutional neural network\nas input. Using back propagation algorithm, we can update all\nparameters of acoustic model to reach a minimum of loss.\nIV. BIDIRECTIONAL N-GRAM LANGUAGE MODEL\nStatistical language modeling and neural network have both\nbeen successfully used in speech recognition [40], [41], [42],\n[43]. Traditional n-gram model always makes an assumption\nthat the probability of the current word depends only on the\nprobability of the previous N-1 words, we propose a new bidi-\nrectional n-gram model which considers context probability of\ntwo sides. Computation of our bidirectional n-gram model is\ncomposed of two parts. First, We deﬁne forward n-gram going\nleft to right in a sentence and we obtain the forward probability\nfor each phoneme given previous phoneme phrase.\nPf(ph) = P(phn|phn−1, ..., phn−(N−1))\n(13)\nSecond, we reverse the whole training sentence to obtain the\nbackward probability for each phoneme given future phoneme\nphrase.\nPb(ph) = P(phn|phn+1, ..., phn+(N−1))\n(14)\nOur model takes as input bidirectional n-gram phoneme\ncounts, thus it combines the bidirectional context information\ncapacity and simple computation complexity. Besides, to make\nour model more robust, we perform bigram, trigram and\nfour-gram including both forward probability and backward\nprobabilitybased on TIMIT corpus. We shows an example how\nour language model rectify mislabeled phonemes of acoustic\nmodel in Figure 5.\nV. EXPERIMENTAL SETUP\nA. Dataset\nWe perform our speech recognition experiments on a public\ncommonly used speech dataset: TIMIT. TIMIT is composed\nof 630 speakers with 6300 utterances of different sexes and\ndialects. Every audio clip is followed by phoneme transcrip-\ntions and sentence transcriptions, and we take the phoneme\ntranscriptions as ground labels. The output is probability\ndistribution of 62 labels including 61 non-blank phonemes and\none blank label for CTC. Since a typical machine learning\ndataset contains training set, validation set and test set, we\ndesign to split 6300 utterances of TIMIT into 5000 training\nutterances, 1000 validation utterances and 300 test utterances.\n5\nrectiﬁed sequences\nn-gram rectify\npredicted sequences\ndcl\nae\nd\ner\ndcl\ndcl\nae\ny\ner\ndcl\ndcl\nae\n?\ner\ndcl\ner\ndcl\n?\ndcl\nae\nFig. 5. Example of the bidirectional statistical n-gram language model rectifying the output phoneme sequence.\nFor the generalization of our model, we randomly choose six\ndifferent partitions of TIMIT and then select the best partition\nby cross validation. We build a simply baseline model of deep\nneural networks to evaluate the performance of six different\npartitions, and we choose the best partition whose test cost\ncurve is going down both stably and fast.\nB. Feature Selection\nAt the stage of pre-processing, each piece of speech is\nanalyzed using a 25-ms Hamming window with a ﬁxed overlap\nof 10-ms. Each feature vector of a frame are calculated by\nFourier-transform-based ﬁlter-bank analysis, which includes\n39 log energy coefﬁcients distributed on 13 mel frequency cep-\nstral coefﬁcients (MFCCs), along with their ﬁrst and second\ntemporal derivatives. Besides, all feature data are normalized\nso that each vector dimension has a zero mean and unit\nvariance. Since the feature matrix of each audio speech differs\nin time length, we pad each feature matrix with zeros to a max\nlength.\nC. Implementation\nWe build our deep neural network models based on open-\nsource library Lasagne, which is a lightweight library to build\nand train neural networks in Theano. We take our experiment\non GPU Tesla K80 to speed up our training. For CTC part, we\nchoose to use a C++ implementation of Baidu Research and\nwe write some Python code to wrap it in Lasagne. Besides,\nall cross validation, feature generation, data loading, result\nanalysis, visualization are implemented in Python code by\nourselves.\nD. Training Details\nAll our experimental models are trained by end-to-end\nstochastic gradient descent algorithm with a mini-batch of\n32. In detail, since the Adam optimization method [25] is\ncomputationally efﬁcient, requires little memory allocation and\nis well suitable for training deep learning models with large\ndata, we adopt the Adam method with a learning rate of\n0.00005 at the start of training. Instead of setting learning\nrate to be 0.001 as the paper said, we ﬁnd that a learning\nrate of 0.00005 can make the divergence more stable in\npractice. As our experimental models are very deep, we would\nlike to adopt some regulations to avoid overﬁtting. Recetnly,\nbatch normalization [22] has shown a better regularization\nperformance, however it would add extra parameters and\nneeds heavy data augmentaion, which we would like to avoid.\nInstead, we add a dropout layer after the recurrent layers and\nafter the ﬁrst full-connected feedforward layer to prevent it\nfrom overﬁtting. To keep the sequential information for better\nacoustic modeling, we reserve the whole context information\nfor end-to-end training instead of splitting each audio into\nframes of same length for frame-wise cross-entropy training.\nBesides, on the top layer of our experimental models, we set\nthe activation function to be linear, for that CTC decoding has\nwrapped the softmax layer inside.\nE. Evaluation\nSince our proposed model is end-to-end and phoneme-\nlevel, we use phoneme error rate (PER) to evaluate the result.\nThe PER is computed after the CTC network had decoded\nthe whole output sentence into a sequence of phonemes.\nWe then compute the Damerau-Levenshtein distance between\nour predicted sequence and the truth sequence to obtain the\nmistake we made. The average number of mistakes over the\nlength of the phoneme sequence is just our PER. We evaluate\nour model on the test set ﬁnally.\nVI. ARCHITECTURE\nTo explore the convergence properties of deep convolu-\ntional networks with recurrent networks, we have conducted\nexperiments with different conﬁgurations. In this section,\nwe’ll discuss three typical experiments we have tried, which\nare novel deep recurrent convolutional networks, traditional\ndeep convolutional recurrent networks and residual networks.\nWe’ll compare the convergence speed of them according to\ndifferent conﬁgurations including number of layers, number\nof parameters, number of feature maps and applying residual\nlearning.\nA. End-to-End Training\nTraditional training approach of acoustic modeling is based\non frame-wise cross-entropy of predicted output and true label,\nwhich needs handy alignment between input frames and output\nlabels. To avoid such high labor cost, our approach exploits the\ndynamic decoding method based on CTC which can perform\nsupervised learning on sequence data and avoid alignment\nbetween input data and output label. We choose a commonly\nused dataset TIMIT to work on our acoustic model. TIMIT\ncontains 6300 utterances of 630 speakers in 8 dialects, each\naudio contains phoneme transcription, word transcription and\nthe whole sentence. We choose the phoneme transcription as\nlabels for phoneme-level training.\n6\nPre-Emphasis\n Fourier Transform\nPower Spectrum\nFilterbank Energy\nLogarithmic Transform\nDiscrete Cosine Transform\nNormalization\nNeural Network\nCTC Decoding \nOutput Sequence\nTriangular Filter Integration\nN-gram LM\nFig. 6.\nOverall process of our ASR system. We ﬁrst calculate the mel\nfrequency cepstral coefﬁcients (MFCCs) of audio speech, and they are taken\nas input of experimental acoustic model. CTC is used for transcribe output\nprobability distribution into phoneme sequence, and bidirectional statistical\nn-gram model is to rectify it.\nB. Deep convolutional recurrent networks\n1) Details of experimental models: Since many recent ASR\nsystems use deep CNNs for acoustic modeling as part of\nASR, especially, deep CNNs are used for feature preprocessing\nfollowed by RNN CTC decoding network. Conventional deep\nCNNs contain convolution and pooling, but we ﬁnd that pool-\ning can be replaced by convolution with fewer feature maps\nin practice. Inspired by this, we also build four deep convolu-\ntional recurrent networks, which are composed of deep con-\nvolutional layers, four recurrent layers and two full-connected\nfeedforward layers. They are distinguished by different number\nof feature maps at convolutional layer. As shown in Figure 7,\nthe two deep fully-connected convolutional recurrent networks\n“CR1” and “CR2” differ in number of feature maps in bottom\nconvolutional layers. We set the number of feature maps to\ngo down gradually, which means that the number of feature\nmaps becomes narrower from bottom layers to up layers.\nBoth “CR3” and “CR4” have narrower deep convolutional\nlayers, but they have different bottom convolutional layers.\nThe parameters of each model are shown in Table I.\nInput:Fr*39\n(conv 3*3,16)*10\n(conv 3*3,2)*2\n(DNN,256)\n(DNN,63)\nCTC\n(RNN,128)*4\nInput:Fr*39\n(conv 3*3,24)*10\n(conv 3*3,8)*2\n(conv 3*3,4)*2\n(conv 3*3,2)*2\n(DNN,256)\n(DNN,63)\nCTC\n(RNN,128)*4\nInput:Fr*39\n(conv 3*3,32)*10\n(conv 3*3,8)*2\n(conv 3*3,4)*2\n(conv 3*3,2)*2\n(DNN,256)\n(DNN,63)\nCTC\n(RNN,128)*4\nInput:Fr*39\n(conv 3*3,16)*6\n(conv 3*3,8)*2\n(conv 3*3,4)*2\n(conv 3*3,2)*2\n(DNN,256)\n(DNN,63)\nCTC\n(RNN,128)*4\nCR1\nCR2\nCR3\nCR4\nFig. 7. Architectures of four different deep convolutional recurrent models.\nWe have conducted many experimental models, “CR1”, “CR2”, “CR3” and\n“CR4” are all traditional deep convolutional recurrent networks with better\nperformance.\nTABLE I\nPARAMETERS OF DEEP CONVOLUTIONAL RECURRENT MODELS\nModel\nCR1\nCR2\nCR3\nCR4\n#Params\n19k\n22k\n26k\n18k\n2) Comparison of cost curves: The cost curves of four\nconvolutional recurrent models are shown in Figure 8. Com-\nparing to other three models, “CR2” converges fastest. Since\n“CR1” and “CR2” both have similar deep fully-connected\nconvolutional layers, but differ in number of feature maps.\nWe can ﬁnd that “CR2” behaves much better than “CR1”.\n“CR3” and “CR4” have narrower structures, they behave very\nsimilarly but much poorer than “CR2”. So we can draw a\nconclusion that for convolutional recurrent networks, deep\nfully-connected convolutional layers can be of much help to\nconvergence. However, narrower structure doesn’t improve the\nperformance slightly here.\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n20\n40\n60\n80\n100\n120\n140\n160\n180\nCR1\nCR2\nCR3\nCR4\nTime (minute)\nCost\nFig. 8.\nCost curves of different convolutional recurrent models. We totally\nconsider 91 epochs for each model. Since each model takes different time for\nan iteration, so two lines above have different length.\nC. Deep recurrent convolutional networks\nSome previous ASR systems use pure stacked RNNs for\nacoustic modeling [12], [14], and some recent ASR systems\nstart to focus on taking some shallow CNNs as the stage\nof feature preprocessing in the bottom layers [32], [13]. We\npropose a new architecture for acoustic modeling which is\ncomposed of several recurrent layers followed by deep CNNs.\nOur new architecture has some highlights: First, we make use\nof RNNs to depict short time dependency of the input feature.\nSecond, CNN is able to depict the local correlations of small\nﬁeld, but our deep stacked CNNs can see the whole context\ninformation. Third, as opposite to the traditionally used 6*6\nor 3*4 ﬁlter and 1*3 pooling in speech recognition [33], we\nuse the small ﬁlters of 3*3 to build the full convolutional\nlayers with no pooling layer. Filters of 3*3 is successfully\nused in computer vision, and we also ﬁnd ﬁlters of 3*3 can\neffectively capture the high-level features along both time\ndimension and frequency dimension with little computational\ncomplexity. The stride of convolutional layer is set to be 1,\nand each convolutional layer is padded with zeros in both\ndimensions to keep the sizes of input and output feature maps\nunchanged.\n7\nInput:Fr*39\n(RNN,128)*4\n(conv 3*3,24)*2\n(conv 3*3,48)*2\n(conv 3*3,24)*2\n(conv 3*3,12)*2\n(conv 3*3,6)*2\n(conv 3*3,3)*2\n(DNN,256)\n(DNN,63)\nCTC\nInput:Fr*39\n(RNN,128)*4\n(conv 3*3,16)*6\n(conv 3*3,8)*2\n(conv 3*3,4)*2\n(conv 3*3,2)*2\n(DNN,256)\n(DNN,63)\nCTC\nInput:Fr*39\n(RNN,128)*2\n(DNN,256)\n(DNN,63)\nCTC\nInput:Fr*39\n(RNN,128)*2\n(conv 3*3,32)*2\n(conv 3*3,64)*2\n(conv 3*3,32)*2\n(conv 3*3,16)*2\n(conv 3*3,8)*2\n(conv 3*3,4)*2\n(DNN,256)\n(DNN,63)\nCTC\nInput:Fr*39\n(RNN,128)*2\n(DNN,256)\n(DNN,63)\nCTC\nInput:Fr*39\n(RNN,128)*2\n(conv 3*3,16)*6\n(conv 3*3,8)*2\n(conv 3*3,4)*2\n(conv 3*3,2)*2\n(DNN,256)\n(DNN,63)\nCTC\nRC1\nRC2\nRC5\nRC3\nRC6\nRC4\n(conv 3*3,16)*10\n(conv 3*3,2)*2\n(conv 3*3,24)*10\n(conv 3*3,2)*2\nFig. 9. Architectures of six different deep recurrent convolutional networks. We also conducted many experimental models of deep recurrent convolutional\nnetworks, and we show some six ones above.\n1) Details of experimental networks: We build six different\nnetworks which are combinations of RNNs and CNNs. The\nsimilarities of six networks are as follows. Continuous frames\nof 39 dimensional feature vectors are passed into the bottom\nrecurrent layer as input of the network, and each recurrent\nlayer has 128 neurons to store hidden information. Besides,\nat the top of each network, we add two full-connected feed-\nforward layers, which have 256 hidden neurons and 62 output\nneurons, respectively. Since the CTC network has the softmax\nfunction, so we don’t add any activation function on the top\nlayer. Instead, we take the linear activation function as the\noutput followed by CTC network.\nAs shown in Figure 9, “RC1” is a model with four recurrent\nlayers at the bottom, followed by totally 12 convolutional lay-\ners, of which they are two convolutional layers with 24 feature\nmaps, two convolutional layers with 48 feature maps, two\nconvolutional layers with 24 feature maps, two convolutional\nlayer with 12 feature maps, two convolutional layers with 6\nfeature maps and two convolutional layers with three feature\nmaps. Compared with “RC1”, we also build “RC2” with fewer\nparameters, and the difference between both is that “RC2” has\n6 convolutional layers with 16 feature maps, 2 convolutional\nlayers with 8 feature maps, 2 convolutional layers with 4\nfeature maps and 2 convolutional layers with 2 feature maps.\nBesides, we also build “RC3”, which has the similar structure\nas “RC1” but with only 2 recurrent layers. Comparing with\n“RC2”, we also build a similar network as “RC4”, which\nhas the same convolutional layers but with only 2 recurrent\nlayers. Besides, we also build another two networks “RC5” and\n“RC6”, which have more full-connected convolutional layers\nthan previous four networks. The parameters of six networks\nare shown in Table II.\nTABLE II\nPARAMETERS OF DIFFERENT RECURRENT CONVOLUTIONAL MODELS\nModel\nRC1\nRC2\nRC3\nRC4\nRC5\nRC6\n#Params\n29K\n21K\n23K\n15K\n15K\n15K\n2) Comparison of cost curves: To investigate the con-\nvergence of different networks in Figure 9, we present the\ncomparison of cost curves in Figure 10. The horizontal axis\nrepresents time, of which the unit is minute. The vertical\naxis represents training cost. We totally train each model for\n42 epochs, and each marker denotes an epoch in Figure 10.\nResults show that “RC1” converges the slowest at both the\nbeginning and the end, which is probably caused by too many\nparameters and too many convolutional layers of different\nfeature maps. Differently, “RC2” performs much better than\n“RC1”, since both ﬁnished 42 epochs at same time, but\nthere is a great gap between their cost curves. Comparing\n”RC1” with “RC2”, “RC2” has fewer parameters and more\ncontinuous full-connected convolutional layers. “RC3” has\nfewer recurrent layers than “RC1”, and “RC3” also behaves\nmuch better than “RC1”. Similarly, “RC4” has fewer recurrent\nlayers than “RC2”, and it behaves much better than ”RC2”.\n“RC4” ﬁnishes 42 epochs in only 900 minutes, but ”RC2”\n8\ntakes around 1100 minutes. Since “RC4”, “RC5” and “RC6”\nhave the similar number of parameters, we ﬁnd that “RC4”\nbehaves best among three, which probably attributes to the\ndifferent structure of “RC4”. Based on “RC5”, “RC6” has\nmore convolutional feature maps, but we ﬁnd this degrades\nthe performance of “RC5”.\nTotally, our experiments show that narrower fully-connected\nconvolutional layers can help improve model to converge,\nas “RC2” and “RC4”. Besides, deep fully-connected con-\nvolutional layers with same number of feature maps have\nslightly inferior performance to narrower ones, as “RC5”\nand “RC6”. For convergence, narrower recurrent convolutional\nlayers behave better.\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n40\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\n160\nRC1\nRC2\nRC3\nRC4\nRC5\nRC6\nTime (minute)\nCost\nFig. 10.\nCost curves of different recurrent convolutional architectures. We\ntotally consider 42 epochs for every model. Since each model takes different\ntime for an iteration, so two lines above have different length.\n3) Comparison of “RC2” and “CR2”: To investigate the\ndifference of convergence between deep recurrent convolu-\ntional network and deep convolutional recurrent network, we\npresent a comparison of convergence curves between both. Re-\ncurrent convolutional network is our novel proposed network\nfor acoustic modeling, and convolutional recurrent network\nis the conventional one, they have the similar number of\nparameters. Since we have discussed some different models\nof them previously, we especially select two better models\n“RC2” and “CR2” to be shown in Figure 11. As shown\n0\n5\n00\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n20\n40\n60\n80\n100\n120\n140\n160\n180\nRC2\nCR2\nTime (minute)\nCost\nFig. 11.\nCost curves of deep convolutional recurrent network “CR2” and\ndeep recurrent convolutional “RC2”. For both networks, we totally consider\n88 epochs, respectively. Each pentagram marker denotes an epoch. Since each\nmodel takes different time for an iteration, so two lines above have different\nlength.\nin Figure 11, we count 88 epochs for both “CR2” and\n“RC2”. Observing the training cost curves along time, we\ncan draw some conclusions. First, “RC2” ﬁnishes 88 epoches\nin less than 3000 minutes, while “CR2” ﬁnishes in nearly\n4000 minutes, even though “CR2” has the same number of\nparameters as “RC2”, but it converges one quarter faster than\n“RC2”. Besides, in the ﬁrst 2000 minutes, “RC2” behaves\nmuch better than “CR2”, but in the last half of training,\n“CR2” catches up with “RC2”. Therefore, our proposed deep\nrecurrent convolutional network has the faster convergence\nperformance at the beginning, and reach the same performance\nas conventional deep convolutional recurrent network later.\nD. Residual networks\nGenerally, deeper convolutional neural networks can have\nlarger capacity for feature representation, however, it has\nbeen shown that deeper convolutional neural networks are\nmore difﬁcult to train for their degradation problem, although\nthere are some modern optimization methods. Recently, a\nnew residual learning framework has been proposed to ease\nthe training of very deep convolutional neural networks, and\ndeep residual networks [38] have been proved to improve\nconvergence and higher accuracy in image classiﬁcation with\nno more extra parameters. General deep residual networks\n(ResNets) are composed of many stacked “Residual Blocks”,\nand each residual block can be expressed in following form:\nyl = h(xl) + ϝ(xl, Wl)\n(15)\nxl+1 = f(yl)\n(16)\nwhere xl and xl+1 are input and output of the l-th residual\nblock, and ϝ is a residual mapping function. In general,\nh(xl) = xl is an identity mapping and f is an activation\nfunction, which we set to be elu [24] here. With the pres-\nence of short-cut connection, residual networks can ease the\ndegradation problem of deeper convolutional neural networks\nbecause the additional layers can only simply perform an\nidentity mapping. Although ResNets with over 100 layers\nhave shown great accuracy for several challenging image\nclassiﬁcation tasks on ImageNet competitions [44] and MS\nCOCO competitions [45], we also want to explore how the\nresidual blocks behave in our experimental models. To explore\nhow ResNets behave in ASR, we propose a novel architecture\nwhich combines deep fully convolutional network with resid-\nual learning framework in ASR. We build two ResNets based\non both “CR2” and “RC2”, which are shown in Figure 12.\nTo avoid extra parameters, we build residual blocks only\nwith layers of same dimension. “CR2” and “RC2” are two\nmodels with best performance among all discussed models\nabove. For “Res-RC2”, we build four residual blocks based\non “RC2”, and each residual block contains several layers\nwith the same number of feature maps. In contrast, “Res-CR2”\ncontains two residual blocks based on “CR2”. Figure 13 gives\nthe comparison of cost curves between two plain networks\nand their residual versions. “Res-RC2” shows the best per-\nformance, it ﬁnishes 88 epochs in only 1500 minutes, and its\ncost also goes down very quickly. Comparing “Res-RC2” with\n“RC2”, we can draw a conclusion that “Res-RC2” converges\n9\nInput:Fr*39\n+\n(conv 3*3,16)*6\nResidual Block 1\n+\n(conv 3*3,8)*2\nResidual Block 2\n+\n(conv 3*3,4)*2\nResidual Block 3\n+\n(conv 3*3,2)*2\nResidual Block 4\n(DNN,256)\n(DNN,63)\nCTC\nInput:Fr*39\n+\n(conv 3*3,24)*10\nResidual Block 1\n+\n(conv 3*3,2)*2\nResidual Block 2\n（a): Res-RC2\n(DNN,256)\n(DNN,63)\nCTC\n(RNN,128)*4\n（b): Res-CR2\n(RNN,128)*4\nFig. 12. Architectures of both networks applied with deep residual learning\nframework. (a) We build four residual blocks based our novel network “RC2”,\neach residual block contains layers with same number of feature maps. (b) We\nbuild two residual blocks based on traditional network “CR2”, each residual\nblock also contains layers with same number of feature maps.\n20\n40\n60\n80\n100\n120\n140\n160\n180\nRC2\nRes-RC2\nCR2\nRes-CR2\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n0\nTime (minute)\nCost\nFig. 13. Cost curves of two plain networks and their residual versions. Each\nnetwork is counted for 88 epochs.\ntwice as fast as ”RC2”, which mainly attributes to the four\nshortcuts connection of identity mapping in “Res-RC2”, thus,\nthe difﬁculty of training “Res-CR2” can be eased. However,\ncompared to “CR2”, “Res-CR2” with only two residual blocks\nconverges much slower. Concretely, “CR2” ﬁnishes 88 epochs\nin less than 4000 minutes, while “Res-CR2” ﬁnished 88\nepochs in nearly 7000 minutes. Although “Res-CR2” has two\nresidual blocks based on “CR2”, but the degradation problem\nhas been exposed out of expectation. For this degradation\nproblem, we propose two possible reasons. (i) The bottom\nresidual block of “Res-CR2” has ten convolutional layers, we\nthink it’s so deep that the power of deep residual learning\nmay be restricted. (ii) “Res-CR2” has two recurrent layers on\ntop of convolutional layers, so the convergence may be mainly\ninﬂuenced by the top recurrent layers and residual blocks make\nno difference here.\nVII. EVALUATION\nTIMIT is a small 16 kHz speech corpus with 6300 ut-\nterances, from which the validation set of 300 utterances\nand the test set of 300 utterances are derived. We use 62\nphonemes as output labels, including 61 non-blank labels and\none blank label for CTC. After acoustic modeling, a proba-\nbility distribution of 63 labels will be decoded by the CTC\nnetwork into ﬁnal phoneme sequences. Then, our proposed\nbidirectional hybrid n-gram language model over phonemes,\nestimated from the whole training set, is used to rectify the\nﬁnal sequence. Since our acoustic models are phoneme-level,\nTABLE III\nPHONEME ERROR RATE OF DIFFERENT ARCHITECTURES\nType\nModel Structure\nvalidation PER\ntest PER\nDeep\nconvolutional\nrecurrent\nnetworks\nCR1\n21.56%\n20.08%\nCR2\n20.59%\n18.73%\nCR3\n23.89%\n22.32%\nCR4\n20.56%\n19.31%\nDeep recurrent\nconvolutional\nnetworks\nRC1\n32.17%\n30.91%\nRC2\n21.36%\n20.71%\nRC3\n25.66%\n24.34%\nRC4\n27.44%\n25.54%\nRC5\n25.76%\n24.11%\nRC6\n26.60%\n24.87%\nDeep Residual\nnetworks\nRes-RC2\n18.77%\n17.33%\nRes-CR2\n20.13%\n18.90%\nwe evaluate them by PER on the test set. Each experiment of\nan architecture has been conducted several times, we present\nthe minimum validation PER and minimum test PER for every\narchitecture in Table III. According to evaluation result, our\nnovel deep recurrent convolutional network “RC2” obtains a\nPER of 20.71%, with accuracy competitive to traditional deep\nbelief network acoustic model. Although, deep convolutional\nrecurrent network “CR2” achieves a test PER of 18.73%,\nwhich is a 2% relative improvement over the novel “RC2”,\nhowever, when we apply the residual learning framework to\nthem, we ﬁnd that “Res-RC2” obtains a test PER of 17.33%,\nwhich is an obvious improvement over “RC2”. Furthermore,\n“Res-CR2” attains a PER of 18.90% with slightly improve-\nment over “CR2”, which probably is caused by the heavy\nresidual block as we have discussed previously.\nVIII. CONCLUSIONS\nWe propose a new architecture for sequence modeling in\nthis work, and our novel deep recurrent convolutional network\ncan have appealing performance on speech recognition task.\nIn detail, we present an experimental comparison between two\ndifferent acoustic models in speech recognition including tra-\nditional deep convolutional recurrent networks and our novel\ndeep recurrent convolutional networks. Besides, we apply\ndeep residual learning in both acoustic models. Traditional\napplication of deep CNNs are used for the stage of feature\npreprocessing, followed by recurrent layers and CTC decoding\nlayer. In contrast, it takes too much time to converge in\npractice. Our proposed deep recurrent convolutional network\ntakes recurrent networks as feature preprocessing, and deep\n10\nconvolutional layers are designed to depict high-level feature\nrepresentation. Experiments show that, compared to traditional\ndeep convolutional recurrent networks, our novel deep recur-\nrent convolutional network can converge in less time in the\nﬁrst half period and also attains a comparable PER. Besides,\nwe try to apply deep residual learning in our acoustic models.\nWe build some residual blocks through a shortcut connection\nas identity mapping for each model, and experiments show that\nour proposed novel deep recurrent convolutional networks can\nbeneﬁt a lot from these residual blocks both in accuracy and\nconvergence. However, heavy residual blocks seem to have\nsome negative impacts on the traditional deep convolutional\nrecurrent networks in terms of convergence speed. Finally, we\npresent a detailed analysis about their performance according\nto different training cost curves, and our proposed “Res-RC2”\nattains the best PER of 17.33%. Our experiments verify that\nthe novel deep recurrent convolutional networks can take place\nof traditional deep convolutional recurrent networks in ASR\nwith less training time. In particular, deep residual learning can\nalso be applied in the novel deep recurrent convolutional neural\nnetworks to make great improvement in both convergence\nspeed and recognition accuracy.\nACKNOWLEDGMENT\nThe author would like to thank Chengyou Xie and Qiong-\nhaofeng Wu for helpful discussions on automatic speech\nrecognition.\nREFERENCES\n[1] Bahl, Lalit R., Frederick Jelinek, and Robert L. Mercer, “A maximum\nlikelihood approach to continuous speech recognition,” IEEE transac-\ntions on Pattern Analysis and Machine Intelligence, 1983, pp. 179–190.\n[2] Bahl, L. R., et al., “Maximum mutual information estimation of hidden\nMarkov model parameters for speech recognition,” proc. ICASSP, vol.\n86, 1986.\n[3] Levinson, S. E., L. R. Rabiner, and M. M. Sondhi, “An introduction\nto the application of the theory of probabilistic functions of a markov\nprocess to automatic speech recognition,” Bell Labs Technical Journal,\nvol. 62, no. 4, 1983, pp. 1035–1074.\n[4] Rabiner, Lawrence R., “A tutorial on hidden Markov models and selected\napplications in speech recognition,” Proceedings of the IEEE, vol. 77,\nno. 2, 1989, pp. 257–286.\n[5] Levinson, S. E., L. R. Rabiner, and M. M. Sondhi, “An Introduction to\nthe Application of the Theory of Probabilistic Functions of a Markov\nProcess to Automatic Speech Recognition,” Bell System Technical Jour-\nnal, vol. 62, no. 4, 1983, pp. 1035–1074.\n[6] Deng, Li, et al., “Recent advances in deep learning for speech research\nat Microsoft,” 2013 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2013, pp. 8604–8608.\n[7] Dahl, George E., et al., “Context-dependent pre-trained deep neural\nnetworks for large-vocabulary speech recognition,” IEEE Transactions\non Audio, Speech, and Language Processing, vol. 20, no. 1, 2012,\npp. 30–42.\n[8] Hinton, Geoffrey, et al., “Deep neural networks for acoustic modeling\nin speech recognition: The shared views of four research groups,” IEEE\nSignal Processing Magazine, vol. 29, no. 6, 2012, pp. 82–97.\n[9] Weng, Chao, et al., “Deep neural networks for single-channel multi-\ntalker speech recognition,” 2015 IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 23, no. 10, 2015, pp. 1670–\n1679.\n[10] Dahl, G. E., T. N. Sainath, and G. E. Hinton, “Improving deep neural net-\nworks for LVCSR using rectiﬁed linear units and dropout,” 2013 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2013, pp. 8609–8613.\n[11] Miao, Y., and F. Metze. “Improving low-resource CD-DNN-HMM using\ndropout and multilingual DNN training,” Proc Interspeech, 2013.\n[12] Graves Alex, and Navdeep Jaitly, “Towards end-to-end speech recog-\nnition with recurrent neural networks,” International Conference of\nMachine Learning, vol. 14, 2014.\n[13] Amodei, Dario, et al., “Deep speech 2: End-to-end speech recognition\nin english and mandarin,” arXiv preprint arXiv:1512.02595, 2015.\n[14] Hannun, Awni, et al., “Deep speech: Scaling up end-to-end speech\nrecognition,” arXiv preprint arXiv:1412.5567, 2014.\n[15] Graves Alex, Abdel-rahman Mohamed, and Geoffrey Hinton, “Speech\nrecognition with deep recurrent neural networks” 2013 IEEE In-\nternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2013.\n[16] Graves Alex, and Jrgen Schmidhuber, “Framewise phoneme classiﬁca-\ntion with bidirectional LSTM and other neural network architectures,”\nNeural Networks, vol. 18, no. 5, 2005, pp. 602–610.\n[17] Sak, Hasim, Andrew W. Senior, and Franoise Beaufays, “Long short-\nterm memory recurrent neural network architectures for large scale\nacoustic modeling,” Interspeech, 2014.\n[18] Graves Alex, et al., “Connectionist temporal classiﬁcation: labelling\nunsegmented sequence data with recurrent neural networks,” Proceed-\nings of the 23rd International Conference on Machine Learning, 2006,\npp. 369–376.\n[19] Hwang, Kyuyeon, and Wonyong Sung, “Online sequence training of\nrecurrent neural networks with connectionist temporal classiﬁcation,”\narXiv preprint arXiv:1511.06841, 2015.\n[20] Graves Alex, Sequence Transduction with Recurrent Neural Networks,”\nComputer Science, vol. 58, no. 3, 2012, pp. 235–242.\n[21] Srivastava, Nitish, et al., “Dropout: a simple way to prevent neural\nnetworks from overﬁtting,” Journal of Machine Learning Research, vol.\n15, no. 1, 2014, pp. 1929–1958.\n[22] Ioffe, Sergey, and Christian Szegedy, “Batch normalization: Accelerat-\ning deep network training by reducing internal covariate shift,” arXiv\npreprint arXiv:1502.03167, 2015.\n[23] Nair, Vinod, and Geoffrey E. Hinton, “Rectiﬁed linear units improve\nrestricted boltzmann machines,” Proceedings of the 27th International\nConference on Machine Learning (ICML-10), 2010.\n[24] Clevert, Djork-Arn, Thomas Unterthiner, and Sepp Hochreiter, “Fast and\naccurate deep network learning by exponential linear units (elus),” arXiv\npreprint arXiv:1511.07289, 2015.\n[25] Kingma, Diederik, and Jimmy Ba, “Adam: A method for stochastic\noptimization,” arXiv preprint arXiv:1412.6980, 2014.\n[26] Fukushima, Kunihiko, “Neocognitron: A self-organizing neural network\nmodel for a mechanism of pattern recognition unaffected by shift in\nposition,” Biological cybernetics, vol.36, no. 4, 1980, pp. 193–202.\n[27] Le Cun, B. Boser, et al., “Handwritten digit recognition with a back-\npropagation network,” Advances in neural information processing sys-\ntems, 1990.\n[28] Abdel-Hamid, Ossama, et al., “Convolutional neural networks for speech\nrecognition,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 22, no. 10, 2014, pp. 1533–1545.\n[29] Qian, Yanmin, and Philip C. Woodland, “Very deep convolutional\nneural networks for robust speech recognition,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 24, no. 12, 2016,\npp. 2263–2276.\n[30] Sercu, Tom, and Vaibhava Goel, “Advances in very deep convolutional\nneural networks for LVCSR,” arXiv preprint arXiv:1604.01792, 2016.\n[31] Sainath, T. N., et al., “Convolutional, long short-term memory, fully\nconnected deep neural networks,” 2015 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 4580–\n4584.\n[32] Abdel-Hamid, Ossama, et al., “Applying convolutional neural networks\nconcepts to hybrid NN-HMM model for speech recognition,” 2012 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2012.\n[33] Sainath, Tara N., et al., “Deep convolutional neural networks for\nLVCSR,” 2013 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2013.\n[34] Abdel-Hamid, Ossama, et al., “Convolutional neural networks for speech\nrecognition,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 22, no. 10, 2014, pp. 1533–1545.\n[35] Palaz, Dimitri, and Ronan Collobert, “Analysis of cnn-based speech\nrecognition system using raw speech as input,” Proc Interspeech, No.\nEPFL-CONF-210029, 2015.\n[36] Sainath, Tara N., et al., “Improvements to deep convolutional neural\nnetworks for LVCSR,” 2013 Automatic Speech Recognition and Under-\nstanding (ASRU) Workshop, 2013.\n[37] Tth, Lszl, “Convolutional Deep Maxout Networks for Phone Recogni-\ntion,” Interspeech, 2014.\n11\n[38] He, Kaiming, et al., Deep residual learning for image recognition,” arXiv\npreprint arXiv:1512.03385, 2015.\n[39] He, Kaiming, et al., “Identity mappings in deep residual networks,” arXiv\npreprint arXiv:1603.05027, 2016.\n[40] Niesler, Thomas R., and Philip C. Woodland, “A variable-length\ncategory-based n-gram language model,” 1996 IEEE International Con-\nference on Acoustics, Speech, and Signal Processing (ICASSP), Vol. 1,\n1996.\n[41] Bengio, Yoshua, et al., “A neural probabilistic language model,” Journal\nof Machine Learning Research, 2003, pp. 1137–1155.\n[42] Mikolov, Tomas, et al., “Recurrent neural network based language\nmodel,” Interspeech, vol. 2, 2010.\n[43] Damavandi, Babak, et al., “NN-grams: Unifying neural network and\nn-gram language models for speech recognition,” arXiv preprint\narXiv:1606.07470, 2016.\n[44] Russakovsky, Olga, et al., “ImageNet large scale visual recognition\nchallenge,” International Journal of Computer Vision, vol. 115, no. 3,\n2015, pp. 211–252.\n[45] Lin, Tsung-Yi, et al., “Microsoft coco: Common objects in context,”\nEuropean Conference on Computer Vision, Springer International Pub-\nlishing, 2014.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2016-11-22",
  "updated": "2016-12-27"
}