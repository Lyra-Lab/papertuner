{
  "id": "http://arxiv.org/abs/1611.07308v1",
  "title": "Variational Graph Auto-Encoders",
  "authors": [
    "Thomas N. Kipf",
    "Max Welling"
  ],
  "abstract": "We introduce the variational graph auto-encoder (VGAE), a framework for\nunsupervised learning on graph-structured data based on the variational\nauto-encoder (VAE). This model makes use of latent variables and is capable of\nlearning interpretable latent representations for undirected graphs. We\ndemonstrate this model using a graph convolutional network (GCN) encoder and a\nsimple inner product decoder. Our model achieves competitive results on a link\nprediction task in citation networks. In contrast to most existing models for\nunsupervised learning on graph-structured data and link prediction, our model\ncan naturally incorporate node features, which significantly improves\npredictive performance on a number of benchmark datasets.",
  "text": "Variational Graph Auto-Encoders\nThomas N. Kipf\nUniversity of Amsterdam\nT.N.Kipf@uva.nl\nMax Welling\nUniversity of Amsterdam\nCanadian Institute for Advanced Research (CIFAR)\nM.Welling@uva.nl\n1\nA latent variable model for graph-structured data\nFigure 1: Latent space of unsupervised VGAE\nmodel trained on Cora citation network dataset [1].\nGrey lines denote citation links. Colors denote doc-\nument class (not provided during training). Best\nviewed on screen.\nWe introduce the variational graph auto-\nencoder (VGAE), a framework for unsupervised\nlearning on graph-structured data based on the\nvariational auto-encoder (VAE) [2, 3]. This\nmodel makes use of latent variables and is ca-\npable of learning interpretable latent representa-\ntions for undirected graphs (see Figure 1).\nWe demonstrate this model using a graph con-\nvolutional network (GCN) [4] encoder and a\nsimple inner product decoder.\nOur model\nachieves competitive results on a link predic-\ntion task in citation networks. In contrast to\nmost existing models for unsupervised learn-\ning on graph-structured data and link prediction\n[5, 6, 7, 8], our model can naturally incorporate\nnode features, which signiﬁcantly improves pre-\ndictive performance on a number of benchmark\ndatasets.\nDeﬁnitions\nWe are given an undirected, unweighted graph G = (V, E) with N = |V| nodes. We\nintroduce an adjacency matrix A of G (we assume diagonal elements set to 1, i.e. every node is\nconnected to itself) and its degree matrix D. We further introduce stochastic latent variables zi,\nsummarized in an N × F matrix Z. Node features are summarized in an N × D matrix X.\nInference model\nWe take a simple inference model parameterized by a two-layer GCN:\nq(Z | X, A) = QN\ni=1 q(zi | X, A) , with\nq(zi | X, A) = N(zi | µi, diag(σ2\ni )) .\n(1)\nHere, µ = GCNµ(X, A) is the matrix of mean vectors µi; similarly log σ = GCNσ(X, A).\nThe two-layer GCN is deﬁned as GCN(X, A) = ˜A ReLU\n\u0000˜AXW0\n\u0001\nW1, with weight matrices\nWi. GCNµ(X, A) and GCNσ(X, A) share ﬁrst-layer parameters W0. ReLU(·) = max(0, ·) and\n˜A = D−1\n2 AD−1\n2 is the symmetrically normalized adjacency matrix.\nGenerative model\nOur generative model is given by an inner product between latent variables:\np (A | Z) = QN\ni=1\nQN\nj=1 p (Aij | zi, zj) , with\np (Aij = 1 | zi, zj) = σ(z⊤\ni zj) ,\n(2)\nwhere Aij are the elements of A and σ(·) is the logistic sigmoid function.\nLearning\nWe optimize the variational lower bound L w.r.t. the variational parameters Wi:\nL = Eq(Z|X,A)\n\u0002\nlog p (A | Z)\n\u0003\n−KL\n\u0002\nq(Z | X, A) || p(Z)\n\u0003\n,\n(3)\narXiv:1611.07308v1  [stat.ML]  21 Nov 2016\nwhere KL[q(·)||p(·)] is the Kullback-Leibler divergence between q(·) and p(·). We further take\na Gaussian prior p(Z) = Q\ni p(zi) = Q\ni N(zi | 0, I). For very sparse A, it can be beneﬁcial to\nre-weight terms with Aij = 1 in L or alternatively sub-sample terms with Aij = 0. We choose the\nformer for the following experiments. We perform full-batch gradient descent and make use of the\nreparameterization trick [2] for training. For a featureless approach, we simply drop the dependence\non X and replace X with the identity matrix in the GCN.\nNon-probabilistic graph auto-encoder (GAE) model\nFor a non-probabilistic variant of the VGAE\nmodel, we calculate embeddings Z and the reconstructed adjacency matrix ˆA as follows:\nˆA = σ\n\u0000ZZ⊤\u0001\n, with\nZ = GCN(X, A) .\n(4)\n2\nExperiments on link prediction\nWe demonstrate the ability of the VGAE and GAE models to learn meaningful latent embeddings on\na link prediction task on several popular citation network datastets [1]. The models are trained on\nan incomplete version of these datasets where parts of the citation links (edges) have been removed,\nwhile all node features are kept. We form validation and test sets from previously removed edges and\nthe same number of randomly sampled pairs of unconnected nodes (non-edges).\nWe compare models based on their ability to correctly classify edges and non-edges. The validation\nand test sets contain 5% and 10% of citation links, respectively. The validation set is used for\noptimization of hyperparameters. We compare against two popular baselines: spectral clustering\n(SC) [5] and DeepWalk (DW) [6]. Both SC and DW provide node embeddings Z. We use Eq. 4 (left\nside) to calculate scores for elements of the reconstructed adjacency matrix. We omit recent variants\nof DW [7, 8] due to comparable performance. Both SC and DW do not support input features.\nFor VGAE and GAE, we initialize weights as described in [9]. We train for 200 iterations using\nAdam [10] with a learning rate of 0.01. We use a 32-dim hidden layer and 16-dim latent variables in\nall experiments. For SC, we use the implementation from [11] with an embedding dimension of 128.\nFor DW, we use the implementation provided by the authors of [8] with standard settings used in\ntheir paper, i.e. embedding dimension of 128, 10 random walks of length 80 per node and a context\nsize of 10, trained for a single epoch.\nDiscussion\nResults for the link prediction task in citation networks are summarized in Table 1.\nGAE* and VGAE* denote experiments without using input features, GAE and VGAE use input\nfeatures. We report area under the ROC curve (AUC) and average precision (AP) scores for each\nmodel on the test set. Numbers show mean results and standard error for 10 runs with random\ninitializations on ﬁxed dataset splits.\nTable 1: Link prediction task in citation networks. See [1] for dataset details.\nMethod\nCora\nCiteseer\nPubmed\nAUC\nAP\nAUC\nAP\nAUC\nAP\nSC [5]\n84.6 ± 0.01\n88.5 ± 0.00\n80.5 ± 0.01\n85.0 ± 0.01\n84.2 ± 0.02\n87.8 ± 0.01\nDW [6]\n83.1 ± 0.01\n85.0 ± 0.00\n80.5 ± 0.02\n83.6 ± 0.01\n84.4 ± 0.00\n84.1 ± 0.00\nGAE*\n84.3 ± 0.02\n88.1 ± 0.01\n78.7 ± 0.02\n84.1 ± 0.02\n82.2 ± 0.01\n87.4 ± 0.00\nVGAE*\n84.0 ± 0.02\n87.7 ± 0.01\n78.9 ± 0.03\n84.1 ± 0.02\n82.7 ± 0.01\n87.5 ± 0.01\nGAE\n91.0 ± 0.02\n92.0 ± 0.03\n89.5 ± 0.04\n89.9 ± 0.05\n96.4 ± 0.00\n96.5 ± 0.00\nVGAE\n91.4 ± 0.01\n92.6 ± 0.01\n90.8 ± 0.02\n92.0 ± 0.02\n94.4 ± 0.02\n94.7 ± 0.02\nBoth VGAE and GAE achieve competitive results on the featureless task. Adding input features\nsigniﬁcantly improves predictive performance across datasets. A Gaussian prior is potentially a\npoor choice in combination with an inner product decoder, as the latter tries to push embeddings\naway from the zero-center (see Figure 1). Nevertheless, the VGAE model achieves higher predictive\nperformance on both the Cora and the Citeseer dataset.\nFuture work will investigate better-suited prior distributions, more ﬂexible generative models and the\napplication of a stochastic gradient descent algorithm for improved scalability.\n2\nAcknowledgments\nWe would like to thank Christos Louizos, Mart van Baalen, Taco Cohen, Dave Herman, Pramod Sinha\nand Abdul-Saboor Sheikh for insightful discussions. This project was funded by SAP Innovation\nCenter Network.\nReferences\n[1] P. Sen, G. M. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective\nclassiﬁcation in network data. AI Magazine, 29(3):93–106, 2008.\n[2] D. P. Kingma and M. Welling.\nAuto-encoding variational bayes.\nIn Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2014.\n[3] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate\ninference in deep generative models. In Proceedings of The 31st International Conference on\nMachine Learning (ICML), 2014.\n[4] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.\narXiv preprint arXiv:1609.02907, 2016.\n[5] L. Tang and H. Liu. Leveraging social media networks for classiﬁcation. Data Mining and\nKnowledge Discovery, 23(3):447–478, 2011.\n[6] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In\nProceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining (KDD), pages 701–710. ACM, 2014.\n[7] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network\nembedding. In Proceedings of the 24th International Conference on World Wide Web, pages\n1067–1077. ACM, 2015.\n[8] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings\nof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n(KDD), 2016.\n[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Aistats, volume 9, pages 249–256, 2010.\n[10] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2015.\n[11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine\nLearning Research, 12:2825–2830, 2011.\n3\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2016-11-21",
  "updated": "2016-11-21"
}