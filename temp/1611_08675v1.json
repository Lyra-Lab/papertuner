{
  "id": "http://arxiv.org/abs/1611.08675v1",
  "title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems",
  "authors": [
    "Heriberto Cuayáhuitl",
    "Seunghak Yu",
    "Ashley Williamson",
    "Jacob Carse"
  ],
  "abstract": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN)\nfor multiple tasks (domains) face scalability problems. We propose a method for\nmulti-domain dialogue policy learning---termed NDQN, and apply it to an\ninformation-seeking spoken dialogue system in the domains of restaurants and\nhotels. Experimental results comparing DQN (baseline) versus NDQN (proposed)\nusing simulations report that our proposed method exhibits better scalability\nand is promising for optimising the behaviour of multi-domain dialogue systems.",
  "text": "arXiv:1611.08675v1  [cs.AI]  26 Nov 2016\nDeep Reinforcement Learning for\nMulti-Domain Dialogue Systems∗\nHeriberto Cuayáhuitl1, Seunghak Yu2, Ashley Williamson1, Jacob Carse1\n1University of Lincoln, School of Computer Science, United Kingdom\n2Artiﬁcial Intelligence Team, Samsung Electronics Co. Ltd., Seoul, South Korea\nHCuayahuitl@lincoln.ac.uk\nAbstract\nStandard deep reinforcement learning methods such as Deep Q-Networks (DQN)\nfor multiple tasks (domains) face scalability problems. We propose a method\nfor multi-domain dialogue policy learning—termed NDQN, and apply it to an\ninformation-seeking spoken dialogue system in the domains of restaurants and\nhotels. Experimental results comparing DQN (baseline) versus NDQN (proposed)\nusing simulations report that our proposed method exhibits better scalability and\nis promising for optimising the behaviour of multi-domain dialogue systems.\n1\nIntroduction\nDialogue systems based on the Reinforcement Learning (RL) paradigm offer the possibility to treat\ndialogue design as an optimisation problem, and are attractive because they can improve their per-\nformance over time with experience. But the application of RL is not trivial due to the complexity\nof the problem such as large state-action spaces exhibited in human-machine conversations. This is\nespecially true in multi-domain systems, where the number of state variables (features) and dialogue\nactions increases rapidly as more domains are taken into account. On the one hand, unique situations\nin the interaction can be described by a large number of variables (e.g. words raised in the conversa-\ntion by the system and user) so that enumerating them would result in very large state spaces. On\nthe other hand, the action space can also be large due to the wide range of unique dialogue actions\n(e.g. requests, apologies, conﬁrmations in multiple contexts).\nWhile one can aim to optimise the interaction via compression of the search space, it is usually un-\nclear what features to incorporate in the state representation. This is a strong motivation for applying\nDeep Reinforcement Learning (DRL) to dialogue management so that the agent can simultaneously\nlearn its feature representation and policy [15]. This paper makes use of raw noisy text as features in\nan attempt to avoid engineered features to represent the dialogue state. By using this representation,\ndialogue agents bypass spoken language understanding in order to learn dialogue policies directly\nfrom raw (noisy) text to actions [1].\nWe address dialogue optimisation using the divide-and-conquer approach, in which dialogue states\ncan be described at different levels of granularity, and an action can execute behaviour using either\na single dialogue action (taking one dialogue turn) or a composite one (equivalent to a subdialogue\ntaking multiple dialogue turns). This approach offers at least two beneﬁts: (a) modularity helps to\noptimise subdialogues that may be easier to optimise than the whole dialogue; and (b) subdialogues\nmay include only relevant dialogue knowledge in the states and relevant actions, thus reducing\nsigniﬁcantly the size of possible solutions: consequently they can be found faster. These properties\n∗Funding from Samsung Electronics Ltd. and the University of Lincoln is gratefully acknowledged. We\nthank Raymond Kirk for helping with App development (to integrate the agents in this paper) and system\ntesting. We also thank Heesik Jeon and the AI team at Samsung for their executive efforts in this project.\nAppears in: NIPS Workshop on Deep Reinforcement Learning, Barcelona, Spain, 2016.\nare crucial for training the behaviour of multi-domain spoken dialogue systems in which there may\nbe a large set of state variables or a large number of actions.\nBelow we describe a data-driven method to the approach described, which we have applied to an\ninformation-seeking dialogue system in the domains of restaurants and hotels. Experimental re-\nsults show that the proposed method can train policies faster and more effectively than a standard\nalgorithm in the literature, showing promise for training multi-domain dialogue systems.\n2\nLiterature Review\nRecently, multi-domain spoken conversational agents have received an increasing amount of atten-\ntion. This may be due to the fact that speech technologies such as Automated Speech Recognition\n(ASR) and Text-To-Speech (TTS) have reached a high degree of maturity. But the question of How\nto design conversational systems for human-machine interaction in multiple domains (or tasks)? is\nstill an open and interesting problem in artiﬁcial intelligence. The dialogue system proposed by [11]\nused a distributed architecture of domain experts modulated by a domain selector. The latter used a\ndecision tree with classiﬁcation errors over 20% in 5 domains. This indicates that not only individual\ndomains have to exhibit robust interactions against errors, but also that errors increase by incorporat-\ning more domains.[9] used rule-based classiﬁers for predicting user intentions, which are executed\nusing a Hierarchical Task Network (HTN) incorporating expert knowledge. Trainable multi-domain\ndialogue systems using traditional reinforcement learning include [4, 13, 22, 5]. These systems use\na modest amount of features, and in contrast to neural-based systems, they require manual feature\nengineering.\nRecent work on deep learning applied to task-oriented conversational agents include the following.\n[6] uses a Recurrent Neural Network (RNN) for dialogue act prediction in a POMDP-based dialogue\nsystem, which focuses on mapping system and user sentences to dialogue acts. [2] applies Deep\nReinforcement Learning with a fully-connected neural network for trading negotiations in board\ngames, which focuses on mapping game situations to dialogue actions. [20] trains RNN-based\nclassiﬁers for predicting dialogue success in multi-domain dialogue systems, which can be applied\nto unseen domains. [17] also trains RNN-based classiﬁers but for belief tracking in order to improve\nthe robustness of recognised user responses across dialogue turns. Other neural-based conversational\nagents have been applied to text prediction using the sequence-to-sequence approach [19, 21], and\nto reasoning with inference for text-based question answering [23].\nFrom these works, we can observe that supervised learning is the dominating form of training in\nneural-based conversational agents. To our knowledge, we report the ﬁrst multi-domain dialogue\nsystem using deep reinforcement learning. This form of learning is interesting because it can per-\nform feature learning and policy learning simultaneously, and its effective application in real-world\ndialogue scenarios remains to be demonstrated.\n3\nMethod\nOur proposed method to scale up Deep Reinforcement Learning (DRL) for multi-domain dialogue\nsystems has two stages. First, multi-policy learning via a network of DRL agents; and second, more\ncompact state representations by compressing raw inputs. Although these two stages can be applied\nindependently, their combination aims for further scalability than any one of them individually.\n3.1\nNetwork of Deep Q-Networks\nWe propose to optimise multi-domain dialogue systems using a network of Deep Reinforcement\nLearners, for example a network of Deep Q-networks (DQN) — see [15, 16] for an introduction\nto the standard DQN method. In our method, instead of training a single DQN,we train a set of\nDQNs (also referred to as NDQNs), where every DQN represents a specialised skill to converse\nin a particular subdialogue — see Figure 1. In addition, the network of agents enable DQNs to\nbe executed without a ﬁxed structure in order to support ﬂexible and unstructured dialogues. In\ncontrast to hierarchical DQNs [12] that follow a strict sequence of agents, in our method an NDQN\nallows transitions between all DQN agents except for self-transitions and loops (the latter using\na stack-based approach as in [3]). Furthermore, while user responses can motivate transitions to\n2\nanother domain in the network, completing a subdialogue within a domain motivates a transition\nto the previous domain to resume the interaction. Algorithm 1 describes the procedure to train and\nexecute NDQNs.\nAn optimal policy in an NDQN performs action selection according to\nπ∗\nθ(d)(s) = arg max\na∈A(d) Q∗(d)(s, a; θ(d)),\n(1)\nwhere domain or skill d ∈D is selected according to\nd = arg max\nd′∈D F(d′|d, e),\n(2)\nand evidence e takes into account all features that describe the environment space of domain d.\nWhile this transition function (Eq. 2) is used for high-level transitions in the interaction, Equation 1\nis used for low-level transitions within a node (skill) in the network and subject to reinforcement\nlearning. NDQN assumes that the domain transition function F can be deterministic or probabilistic\n(the latter due to uncertainty in the interaction), and it is a prior requirement for NDQN-Learning.\nLanguage\nGenerator\nUser\nSimulation\nASR Error\nGeneration\nState \nTransition\nFunction\nReward\nFunction\nEnvironment\nstate\nreward\naction\nDeep Reinforcement Learning Agents\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nDOMAN2\nMETA-DOMAIN\nDOMAIN1\nFigure 1: Multi-domain DRL agents with ﬂexible interaction. The dashed arrows connecting do-\nmains denote ﬂexible transitions between domains in order to avoid a rigid structure in the interac-\ntion. Although all policies are considered for decision-making, only one domain can be executed at\na time implying that a previous domain can continue its execution in order to resume the interaction.\n3.2\nNDQNs with Compressed Raw Inputs\nPrevious work on dialogue policy learning using DRL map raw (noisy) text to actions [1, 24]. This\nis not only computationally intensive, but it becomes infeasible for dialogue systems with large\nvocabularies. To tackle this problem we propose to use delexicalised sentences (as proposed in\n[8]) and synonymised sentences. This has the advantage that dialogue policies can be trained from\nmore compact state representations than those using only raw inputs, and have coverage for a larger\nvocabulary than trained for.\n3\nAlgorithm 1 Network of Deep Q-Learners (NDQN)\n1: Initialise set of Deep Q-Networks with replay memories D(d), action-value functions Q(d) with\nrandom weights θ(d), and target action-value functions ˆQ(d) with weights ˆθ(d) = θ(d)\n2: repeat\n3:\nd ←initial domain, predeﬁned or deﬁned by arg maxd∈D Fo(d)\n4:\ns ←initial environment state in S(d)\n5:\nrepeat\n6:\nrepeat\n7:\nChoose action a ∈A(d) in s derived from Q(d) (e.g. ǫ-greedy, Thompson)\n8:\nExecute action a and observe reward r and next state s′\n9:\nAppend transition (s, a, r, s′) to D(d)\n10:\nB(d) ←sample random minibatch of experiences from D(d)\n11:\nd′ ←select next domain according to arg maxd′∈D F(d′|s′, e)\n12:\nyj =\n(\nrj\nif ﬁnal step of episode\nrj + γ maxa∈A(d) ˆQ(d)(s′, a′; ˆθ(d)),\notherwise\n13:\nGradient descent step on\n\u0000yj −Q(d)(s′, a′; θ(d))\n\u00012 using B(d)\n14:\nReset ˆQ(d) = Q(d) every C steps\n15:\ns ←s′\n16:\nuntil s is a terminal state or d ̸= d′\n17:\nd ←d′\n18:\nuntil s is a goal state\n19: until convergence\n3.2.1\nDelexicalisation\nConsider a dialogue system for restaurant search receiving the following user request—with corre-\nsponding delexicalised sentence underneath.\nI am looking for italian food in the city centre\nI am looking for $foodtype food in the $area\nThe latter representation combining words and slot IDs (denoted with the symbol ‘$’) has several\npractical advantages. For example, policies can be learnt faster, they contribute to further scalability\nof systems with large vocabularies, and policies do not have to be retrained if the slot values change\nover time. In this work we use heuristics to replace slot values by slot IDs, and a trainable component\nfor automatic slot labelling is considered beyond the scope of this paper.\n3.2.2\nSynonymization\nConsider the same system above receiving the following user request given the unknown words\n‘fancy’ and ‘cuisine’—with corresponding synonyms underneath.\nWe fancy italian cuisine in the centre of town\nWe want italian food in the centre of town\nWe argue that word synonyms can be useful in such situations because the unknown word ‘fancy’\ncan trigger the known word feature ‘want’. Similarly, the unknown word ‘cuisine’ can trigger the\nknown word feature ‘food’. In this way, the vocabulary of our NDQNs incorporate a mapping\nfrom ﬁller words and slot values to synonyms in order to cope with unseen wordings. Due to the\ncomplexity of automatic generation of meaningful synonyms, our synonyms have been manually\nspeciﬁed but they can be generated automatically for example from word embeddings [14].\n4\n4\nMulti-Domain Dialogue System\nThe proposed computational framework for training multi-domain dialogue agents is a substantial\nextension from the publicly available software tools SimpleDS [1] and ConvnetJS [10]. It can be\nexecuted in training or test mode using simulations or speech-based interactions (via an App). Our di-\nalogue system runs under a client-server architecture, where the learning agents—one per domain—\nact as the clients and the dialogue system as the server. They communicate by exchanging messages,\nwhere the clients tell the server the action to execute, and the server tells the clients the state and\nreward observed. The elements for training multi-domain dialogue systems are as follows.\nState Spaces\nThey include word-based features depending on the vocabulary of each learning\nagent. They include 177 unique words2 without synonyms, and 150 unique words with synonyms.\nFor example, an agent in the domain of restaurants has relevant features for its domain and it is\nagnostic of features in other domains. While words derived from system responses are treated as\nbinary variables (i.e. word present or absent), the words derived from noisy user responses can be\nseen as continuous variables by taking ASR conﬁdence scores into account. Since a single variable\nper word is used, user features override system ones in case of overlaps.\nAction Spaces\nThey include dialogue acts for the targeted domains—currently 69 unique actions\nin total. Example dialogue act types, dialogue acts without parameters, are as follows: Salutation(),\nRequest(), AskFor(), Apology(), ExpConﬁrm(), ImpConﬁrm(), Retrieve(), Provide(), among others.\nRather than learning with whole action sets, our framework supports learning from constrained\nactions by applying learning updates only on the set of valid actions. These actions are derived\nfrom the most likely actions, Pr(a|s) > 0.0001, from Naive Bayes classiﬁers (due to scalability\npurposes) trained from example dialogues. See example demonstration dialogue in Appendix A.\nIn addition to the most probable data-like actions, the constrained actions are extended with the\nlegitimate requests, apologies and conﬁrmations. The fact that constrained actions are data-driven\nand driven by domain-independent heuristics, facilitates its usage across domains.\nState Transition Functions\nThey are based on numerical vectors representing the last system and\nuser responses. Taking a wider dialogue context is also possible but not explored in this paper. The\nsystem responses are straightforward, 0 if absent and 1 if present (hit-or-miss). The user responses\ncorrespond to the conﬁdence level [0..1] of noisy user responses. While system responses are gen-\nerated from stochastic templates, user responses are generated from semi-random user behaviour.\nThese elements enable the creation of a vast amount of different conversations for agent training.\nDomain Transition Function\nThis function speciﬁes the next domain or task in focus. It is cur-\nrently deﬁned deterministically, and it is also implemented as a SVM classiﬁer trained from ex-\nample interactions—see Appendix A. The design of this classiﬁer follows that of a two-deep fully\nconnected neural network with 80 nodes in each hidden layer, with tanh activation, and an SVM out-\nput layer, using Hinge Loss. While the input layer accepts domain-independent words-as-features\nvectors representing the unique global vocabulary shared amongst all domains in a hit-or-miss ap-\nproach, the output layer has 3 classes representing system domains (meta3, restaurants and hotels).\n15K dialogues of data were generated, partitioned as a 60-40 training-testing split, and trained for\n180 epochs. Initial results of this classiﬁer shows a 87.5% classiﬁcation accuracy on user-simulated\ndata.\nReward Function\nIt is deﬁned as R(s, a, s′) = GR+DR−DL, where GR is goal-based reward\ntreated as task success [0..1] (the proportion of positively conﬁrmed slots and information retrieved\nand presented); DR is a data-like probability of having observed action a in state s; and DL = t∗w\nis a dialogue length measure used to encourage efﬁcient interactions with t time steps and weight\nw (-0.1 in our case). The DR scores are derived from Naive Bayes classiﬁers to allow statistical\ninference over actions given states (Pr(a|s)).\n2The unique words in our system’s vocabulary excludes words from information presentation due to the\nvast amount of information about hotels and restaurants. Nonetheless and during testing, our system retrieves\nlive information from http://www.bookatable.co.uk) and www.reservetravel.com.\n3We refer to meta domain as subdialogues containing domain-general system and user responses.\n5\nFigure 2: Learning curves of the baseline system: (left) without input compression, (right) with\ninput compression. The higher the better in blue straight lines, and the lower the better in other\nmetrics.\nModel Architectures\nWe use fully-connected multilayer neural nets, trained with stochastic gra-\ndient descent, where nodes in the input layers depend on the vocabulary of each agent. The use\nof convolutional neural nets is work in progress. They include 2 hidden layers with 80 nodes with\nRectiﬁed Linear Units to normalise their weights [18]. Dropout[7] and adaptive learning rates are\nalso part of our work in progress. Other hyperparameters include experience replay size=10000,\nburning steps=1000, discount factor=0.7, minimum epsilon=0.001, batch size=32, and learning\nsteps=30000.\n5\nExperimental Results\nHere we compare a multi-domain dialogue system using a standard DRL method versus our pro-\nposed method described in Sections 3 and 4. While the former (DQN) uses a single policy for\nlearning (baseline), the latter (NDQN) uses multiple policies (proposed). Both multi-domain dia-\nlogue systems use the same data, resources and hyperparameters for training, the only difference\nbetween both systems is the learning method (DQN or NDQN) or state representation (with or with-\nout compression).\nWe use four different metrics to measure system performance: avg. reward, learning time, avg. task\nsuccess, and avg. dialogue length. The higher the better in the ﬁrst and third, and the lower the\nbetter in the second and fourth. Figure 2 shows learning curves for the baseline DQN-based system,\nand Figure 3 shows learning curves for the proposed NDQN-based system. Both the baseline and\nproposed system report results over 150K learning steps (about 8700 dialogues). Our results report\nthat training multi-domain systems using a single policy is twofold harder than using a multi-policy\napproach. First, this is evidenced by the fact that the baseline policies do not improve over time, and\nthe policies with the proposed method do. This is presumably due to the abstraction exhibited in the\nmulti-policy approach—more focused system actions rather than interleaving them across domains.\nSecond, our proposed system also learned 4.6 times faster than the baseline, which was accelerated\nfurther to 4.7 times faster by using compressed inputs4. By applying synonymization we are able to\nuse a smaller vocabulary when training and then a much wider vocabulary at runtime, which adds\nrobustness in the presence of unseen dialogues. These results show indication of better scalability\nfor NDQN to multiple domains.\nAlthough the currently generated dialogues using the proposed method seem reasonable, a natural\nquestion to ask is How good (qualitatively speaking) are the trained policies? This question will be\nanswered in an evaluation reported in future work.\n4Ran on Intel Core i5-3210M CPU @ 2.50GHz x 4; 8GiB DDR4 RAM @ 2400MHz.\n6\n(a) Meta Domain: (left) without input compression, (right) with input compression\n(b) Restaurants Domain: (left) without input compression, (right) with input compression\n(c) Hotels Domain: (left) without input compression, (right) with input compression\nFigure 3: Learning curves of the proposed system using simultaneous policy learning with the pro-\nposed method. The higher the better in avg. reward and avg. task success, and the lower the better in\nother metrics. The plots on the left correspond to our proposed system with word-based features, and\nthe plots on the right correspond to our proposed system with delexicalised inputs as features. The\nlatter plots show no performance degradation despite of using more compact state representations.\n7\nMethod / System\nBaseline (DQN)\nProposed (NDQN)\nWithout input compression\n28.57 hrs\n6.21 hrs\nWith input compression\n16.63 hrs\n6.05 hrs\nTable 1: Learning times of the baseline and proposed systems\n6\nConclusion and Future Work\nWe have described a method for training multi-domain dialogue systems in a more scalable way\nthan traditional deep reinforcement learning, e.g. using the DQN method. The proposed method\nuses a Network of DQN (NDQN) agents in order to train specialised agents, and compression of\ninput features. Experimental results using simulations report that the proposed method (NDQN) can\ntrain policies faster and more effectively than DQN—almost 5 times faster than DQNs. In addition,\nour results report that input compression contribute faster learning without performance degradation\nacross metrics. These dialogue agents are the ﬁrst applications of deep reinforcement learning to\noptimising the behaviour of multi-domain human-machine dialogues.\nIn future work we plan to carry out a data collection of speech-based interactions with recruited\nparticipants in order to evaluate and/or retrain our multi-domain dialogue policies with a larger\ndataset. Other future work includes applying and reﬁning the proposed framework to larger sets of\ndomains, and applying it to multimodal interaction in different types of devices and machines.\nA\nExample Demonstration Dialogue\nWe aim for a data-efﬁcient system, trainable from a small set of demonstrations. This dialogue is\none of the 12 manually-speciﬁed seed demonstrations for inducing the system’s features (words)\nand actions (dialogue acts), and for training classiﬁers to predict the most likely actions given the\nprevious system and user responses. The latter were used by the DRL agents described in Section 3\nfor inducing their dialogue behaviour according to the learning environment described in Section 4.\nSquare brackets denote system actions, double quotes denote system or user responses, and words\nbeginning with an underscore encapsulate live web-based information according to the user requests.\nDomain\nAgent\n[Action]\"Verbalisation\"\nMeta\nSYS\n[Salutation(greeting)]\"Welcome to MultiDS!\"\n[Provide(intro)]\"You may ask for information about hotels and restaurants.\"\n[Request(hmihy)]\"How can I help you?\"\nUSR\n\"I am looking for a hotel in edinburgh on the 2nd of january for 2 nights\"\nHotels\nSYS\n[Apology(h_nights)]\"Sorry, how long?\"\nUSR\n\"2 nights\"\nSYS\n[ExpConﬁrm(h_day=$h_day,h_month=$h_month,h_nights=$h_nights)]\n\"Did you say on the 2nd of january for 2 nights?\"\nUSR\n\"yes i did\"\nSYS\n[ExpConﬁrm(h_city=$h_city)]\"Did you say in edinburgh?\"\nUSR\n\"i did\"\nSYS\n[Retrieve(h_info)]\"Let me see.\"\n[Provide(h_info)]\"I found the following hotels. _hotels.\"\nMeta\n[AskFor(h_more)]\"Anything else?\"\nUSR\n\"I am looking for cheap japanese food in the centre of town\"\nRestaurants\nSYS\n[Apology(r_area)]\"Sorry, what area?\"\nUSR\n\"In the centre\"\nSYS\n[ExpConﬁrm(r_price=$r_price,r_area=$r_area)]\n\"Did you say cheap food in the centre?\"\nUSR\n\"yes i did\"\nSYS\n[ExpConﬁrm(r_food=$r_food)]\"Did you say japanese food?\"\nUSR\n\"i did\"\nSYS\n[Retrieve(h_info)]\"Let me see.\"\n[Provide(h_info)]\"I found the following restaurants. _res.\"\nMeta\nAnything else?\nUSR\n\"no\"\nSYS\n[Salutation(closing)]\"Okay, talk to you soon. Bye!\"\n8\nReferences\n[1] H. Cuayáhuitl. SimpleDS: A simple deep reinforcement learning dialogue system. CoRR, abs/1601.04574,\n2016.\n[2] H. Cuayáhuitl, S. Keizer, and O. Lemon. Strategic dialogue management via deep reinforcement learning.\nCoRR, abs/1511.08099, 2015.\n[3] H. Cuayáhuitl, I. Kruijff-Korbayová, and N. Dethlefs. Nonstrict hierarchical reinforcement learning for\ninteractive systems and robots. ACM Transactions on Interactive Intelligent Systems (TiiS), 4(3), 2014.\n[4] H. Cuayáhuitl, S. Renals, O. Lemon, and H. Shimodaira. Evaluation of a hierarchical reinforcement\nlearning spoken dialogue system. Computer Speech & Language, 24(2), 2010.\n[5] M. Gasic, N. Mrksic, P. Su, D. Vandyke, T. Wen, and S. J. Young. Policy committee for adaptation in\nmulti-domain spoken dialogue systems. In ASRU, 2015.\n[6] W. Ge and B. Xu. Dialogue management based on multi-domain corpus. In Annual Meeting of the Special\nInterest Group on Discourse and Dialogue (SIGDIAL), 2015.\n[7] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C. Courville, and Y. Bengio. Maxout networks. In\nInternational Conference on Machine Learning (ICML), 2013.\n[8] M. Henderson, B. Thomson, and S. J. Young. Robust dialog state tracking using delexicalised recurrent\nneural networks and unsupervised adaptation. In IEEE Spoken Language Technology Workshop, 2014.\n[9] H. Jeon, H. R. Oh, I. Hwang, and J. Kim. An intelligent dialogue agent for the IoT home. In AAAI\nWorkshop on AI Applied to Assistive Technologies and Smart Environments, 2016.\n[10] A.\nKarpathy.\nConvNetJS:\nJavascript\nlibrary\nfor\ndeep\nlearning.\nhttp://cs.stanford.edu/people/karpathy/convnetjs/, 2015.\n[11] K. Komatani, N. Kanda, M. Nakano, K. Nakadai, H. Tsujino, T. Ogata, and H. G. Okuno. Multi-domain\nspoken dialogue system with extensibility and robustness against speech recognition errors. In SIGdial\nWorkshop on Discourse and Dialogue, 2006.\n[12] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. B. Tenenbaum. Hierarchical deep reinforcement learn-\ning: Integrating temporal abstraction and intrinsic motivation. CoRR, abs/1604.06057, 2016.\n[13] P. Lison. Multi-policy dialogue management. In SIGDIAL, 2011.\n[14] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and\nphrases and their compositionality. In NIPS, 2013.\n[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing\natari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013.\n[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature,\n518(7540), 02 2015.\n[17] N. Mrksic, D. Ó. Séaghdha, B. Thomson, M. Gasic, P. Su, D. Vandyke, T. Wen, and S. J. Young. Multi-\ndomain dialog state tracking using recurrent neural networks. CoRR, abs/1506.07190, 2015.\n[18] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In International\nConference on Machine Learning (ICML), 2010.\n[19] I. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville, and J. Pineau. Hierarchical neural network generative\nmodels for movie dialogues. CoRR, abs/1507.04808, 2015.\n[20] D. Vandyke, P. Su, M. Gasic, N. Mrksic, T. Wen, and S. J. Young.\nMulti-domain dialogue success\nclassiﬁers for policy training. In ASRU, 2015.\n[21] O. Vinyals and Q. V. Le. A neural conversational model. CoRR, abs/1506.05869, 2015.\n[22] Z. Wang, H. Chen, G. Wang, H. Tian, H. Wu, and H. Wang. Policy learning for domain selection in an\nextensible multi-domain spoken dialogue system. In Empirical Methods in Natural Language Processing\nEMNLP, 2014.\n[23] J. Weston, S. Chopra, and A. Bordes. Memory networks. CoRR, abs/1410.3916, 2014.\n[24] T. Zhao and M. Eskénazi. Towards end-to-end learning for dialog state tracking and management using\ndeep reinforcement learning. CoRR, abs/1606.02560, 2016.\n9\n",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "published": "2016-11-26",
  "updated": "2016-11-26"
}