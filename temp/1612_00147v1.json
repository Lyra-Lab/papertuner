{
  "id": "http://arxiv.org/abs/1612.00147v1",
  "title": "Combining Deep Reinforcement Learning and Safety Based Control for Autonomous Driving",
  "authors": [
    "Xi Xiong",
    "Jianqiang Wang",
    "Fang Zhang",
    "Keqiang Li"
  ],
  "abstract": "With the development of state-of-art deep reinforcement learning, we can\nefficiently tackle continuous control problems. But the deep reinforcement\nlearning method for continuous control is based on historical data, which would\nmake unpredicted decisions in unfamiliar scenarios. Combining deep\nreinforcement learning and safety based control can get good performance for\nself-driving and collision avoidance. In this passage, we use the Deep\nDeterministic Policy Gradient algorithm to implement autonomous driving without\nvehicles around. The vehicle can learn the driving policy in a stable and\nfamiliar environment, which is efficient and reliable. Then we use the\nartificial potential field to design collision avoidance algorithm with\nvehicles around. The path tracking method is also taken into consideration. The\ncombination of deep reinforcement learning and safety based control performs\nwell in most scenarios.",
  "text": "1 \n \nCombining Deep Reinforcement Learning  \nand Safety Based Control for Autonomous Driving \n \nXi Xiong    Jianqiang Wang    Fang Zhang    Keqiang Li   \nState Key Laboratory of Automotive Safety and Energy, Tsinghua University \n \n \nAbstract \nWith the development of state-of-art deep reinforcement learning, we can efficiently tackle \ncontinuous control problems. But the deep reinforcement learning method for continuous control \nis based on historical data, which would make unpredicted decisions in unfamiliar scenarios. \nCombining deep reinforcement learning and safety based control can get good performance for \nself-driving and collision avoidance. In this passage, we use the Deep Deterministic Policy Gradient \nalgorithm to implement autonomous driving without vehicles around. The vehicle can learn the \ndriving policy in a stable and familiar environment, which is efficient and reliable. Then we use the \nartificial potential field to design collision avoidance algorithm with vehicles around. The path \ntracking method is also taken into consideration. The combination of deep reinforcement learning \nand safety based control performs well in most scenarios. \n \n1. Introduction  \nThere are two major paradigms for autonomous driving: the learning method and the control \nmethod. With the success of deep learning and reinforcement learning, more and more people have \nfocused on using the learning method for autonomous driving. The combination of deep learning \nand reinforcement learning can tackle problems of high dimensional inputs. The DQN network \n(Minh et al., 2015) can play Atari games at the human level. But the DQN is not efficient to solve \nproblems with high dimensional action state spaces. The combination of Q-network with actor-\ncritic structure can perform well in the continuous control field. The DDPG algorithm (Lillicrap et \nal., 2015) presents an actor-critic, model-free algorithm based on the deterministic policy gradient \nthat can operate over continuous action spaces. This algorithm can learn policies end-to-end: from \nlow-dimension inputs or raw pixel inputs to final actions. \nIn the DDPG algorithm, the positive reward is the velocity projected along the track direction. We \nwant the vehicle to run along the track as fast as possible. The negative reward is the penalty for \ncollision. However, this method can’t perform well without sufficient training. The policy would \nmake unpredicted decisions in unfamiliar scenarios, which is also the shortcoming of data based \nmethod. In addition, avoiding collision is the basic function when designing the control strategy \nfor autonomous driving. We also want the vehicle to run on the road with higher safety level, \nincluding driving along the central road track and keeping safety distances from vehicles around.  \n2 \n \nThe artificial potential field method is widely used for collision avoidance in the field of robot path \nplanning. We combine the ideas in the artificial potential field (Khatib, 1986) with deep \nreinforcement learning for autonomous driving to put both merits into full use. \nPath tracking is also important for the autonomous driving strategy because we assume that driving \nfar away from the center of the road is with high risk. We can use the path tracking method to get \na relative safe state. \n2. Background \n2.1. Deep Reinforcement Learning \nIn the structure of reinforcement learning, the agent interacts with the environment. After every \ndiscrete time t , the agent implements the action a . Then, the environment changes its previous \nstate \nts  to \n1\nts , and the agent gets its reward tr . The goal of reinforcement learning is to maximize \nthe discounted accumulative reward \n( ,\n)\nT\ni t\nt\ni\ni\ni t\nR\nr s a\n\n\n\n. The action-value function is used to \nexpress the expectation of \ntR . \n( ,\n)\n(\n,\n)\nt\nt\nt\nt\nt\nQ s a\nE R s a\n\n. We define the optimal action value \nfunction as the maximum achievable expected return under the strategy , \n( , )\nmax\n,\n,\nt\nt\nt\nQ s a\nE R s\ns a\na\n\n\n\n\n\n\n\n\n\n                                        (1) \nThe optimal value functions can be expressed with Bellman equations, \n1\n1\n1\n( , )\nmax\n(\n,\n) ,\nt\nt\nt\na\nQ s a\nE r\nQ s\na\ns a\n\n\n\n\n\n\n\n\n\n\n\n\n\n.                                     (2) \nIn DQN algorithm, it’s common to use neural networks to approximate the \n( , )\nQ s a\n\n. We assume \nthe approximator as \n( , ; )\n( , )\nQ s a\nQ s a\n\n\n\n, with the parameters . The Q-network can be trained \nby minimizing the loss function, \n1\n2\n1\n1\n1\n( )\n(\nmax\n(\n,\n;\n)\n( ,\n;\n))\ni\ni\ni\ni\ni\ni\ni\ni\ni\na\nL\nE\nr\nQ s\na\nQ s a\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           (3) \nThe DQN algorithm can work well in high dimensional state spaces but is not effective in \ncontinuous action spaces because the optimization of \nta  at every time step is too slow to be \npractical with nontrivial action spaces. We use the Deep Deterministic Policy Gradient algorithm \nto solve continuous control problems. \nWe divide the control policies into the stochastic policy and the deterministic policy (Sutton et al., \n2012). We assume the stochastic policy as \n(\n)\n(\n)\na s\nP a s\n\n\n, which represents the action \nprobability distribution. We also denote the state distribution as \n( )s\n\n\n. The objective performance \ncan be expressed as an expectation, \n(\n)\n( )\n(\n)\n( , )d d\nS\nJ\ns\na s\nQ\ns a\na s\n\n\n\n\n\n\n\n\n\n\n\n\n                                     (4) \n3 \n \nThe essence of policy gradient algorithm is to adjust the parameters of the policy in the direction \nof the performance gradient \n(\n)\nJ\n\n\n\n\n. The policy gradient theorem (Sutton et al, 1999) can be \nexpressed as, \n                                  \n(\n)\n( )\n(\n)\n( , )d d\nS\nJ\ns\na s\nQ\ns a\na s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n                                                  \n( , )\nln\n(\n)\nE Q\ns a\na s\n\n\n\n\n\n\n\n\n\n                                                   (5) \nAs for the continuous control problems, we assume the policy to be deterministic. We use \n\nto \nrepresent the reflection from the state spaces to the action spaces, namely \n( )\na\ns\n\n\n\n. As with the \ndefinition of stochastic policy, we define the objective performance as, \n(\n)\n( )\n( , )d\n( )\n( ,\n( ))d\nS\nS\nJ\ns\nQ\ns a\ns\ns\nQ\ns\ns\ns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                             (6) \nWe also use the policy gradient for the deterministic policy. If \n( )s\n\n\n\n\n and \n( , )\naQ\ns a\n\n\n both \nexist, then the gradient can be expressed as, \n(\n)\n( )\n( )\n( , )d\n( , )\n( )\na\na\nS\nJ\ns\ns\nQ\ns a\ns\nE\nQ\ns a\ns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            (7) \n2.2. Safety Based Control \nWhen considering the safety of the vehicle, avoiding collision and driving along the track are the \nmost important issues, especially the former function. \nThe artificial potential field method is widely used for robot path planning. The goal of potential \nfield method is to make the robot move from the initial position to the target position in a desired \nmanner while avoiding collision.  \nThere are two types of potential field in the domain of robot path planning, the attractive potential \nfield and repulsive potential field. The attractive potential part represents the energy to get to the \ntarget position. The repulsive potential part represents the potential risk of collision.  \n( )\n( )\n( )\nart\natt\nrep\nU\nx\nU\nx\nU\nx\n\n\n                                                   (8) \nwhere \n( )\nart\nU\nx  is the artificial potential field, \n( )\natt\nU\nx  is the attractive potential field, and the \n( )\nrep\nU\nx is the repulsive potential field. \nThe potential forces are the gradients of the respective potential field, \natt\natt\nF\nU\n\n                                                               (9) \nrep\nrep\nF\nU\n\n                                                            (10) \nWhen we consider multiple targets and obstacles (Fig 1), the final potential forces are the sum of \nattractive forces and repulsive forces. The attractive and repulsive potential forces are vectors, then \nthe total force can be expressed as the sum of vectors. \n4 \n \nRobot\nTarget 1\nObstacle 1\nObstacle 2\nTarget 2\natt\nF\n1\natt\nF\n2\natt\nF\nrep\nF\n1\nrep\nF\n2\nrep\nF\n \nFigure 1: Multiple targets and obstacles for artificial potential field. The attractive force \natt1\nF\nand \natt2\nF\n produced by Target 1 and Target 2 are vectors. \natt\nF\n is the vector sum of \natt1\nF\n and \natt2\nF\n. \nAccordingly, \nrep\nF\n is the vector sum of \nrep1\nF\n and \nrep2\nF\n produced by Obstacle 1 and Obstacle 2. \n3. Combining Deep Reinforcement Learning and Safety Based Control \n3.1. Methodology \nIn the field of cognitive science, there are two major learning paradigms, the empiricism and the \nspeculation. Empiricism is a way of learning from historical experiences. Speculation is the way of \nlogical thinking, which means taking measures by reasoning. The thinking process of humans \ncontains both empiricism and speculation, which are interactive during the process.  \nThe deep reinforcement learning method is just like learning from our past experiences. The safety \nbased control, which contains artificial potential method and path tracking, is like the speculation \nand logical thinking. Using the deep reinforcement learning is efficient and can work well in a \nrelative stable and familiar environment, but this method would be difficult to cover all scenarios. \nWe combine the deep reinforcement learning and safety control to solve the problem. \n3.2. Algorithm \nWe tackle the problem using perception sensor data, including vehicle speed, vehicle position on \nthe road track and opponent vehicle distances (Loiacono et al., 2013). The input data can be divided \ninto two parts. The features of opponent distances can be used for collision avoidance. Other \nparameters can be used for deep reinforcement learning and path tracking. Each of the three \nmethods has its own acceleration and steering commands. We then balance the weight of these \nthree action outputs, \nl\nf\np\n\n\n\n\n\n\n\n\n\n\n                                                  (11) \nl\nf\np\n\n\n\n\n\n\n\n\n\n\n                                                   (12) \n                                                              s.t.   \n1\n\n\n\n\n\n                                                         (13) \n, \nl, \nf\n and \np\n respectively represents the final steering action, the learning policy steering \naction, the potential field steering action and the path tracking steering action. , \nl, \nf and \np\n \nrespectively represents the final acceleration action, the learning policy acceleration action, the \n5 \n \npotential acceleration action and the path tracking acceleration action. ,  and  are \nrespectively the weight parameters of the three methods. \n3.2.1. Deep Reinforcement Learning \nFirstly, we train the vehicle without opponents. The positive reward at each step is the velocity of \nthe car projected along the track direction. We don’t need to set negative rewards. The structure of \nDDPG is shown in Figure 2. The policy network is used to generate actions and the value function \nnetwork is used to approximate the optimal Q-values. The input states are data from vehicle speed \nsensors, current engine speed, track sensors, wheel speed, track position and vehicle angle. After \nseveral hours training, we can use the policy to implement actions (\nl and \nl) without opponents, \nwhich can also be applicable to other tracks because of the stable familiar environment. \n(\n)\nQ\nQ\nL\n\n\n\n( )s\n\n\n\n \nFigure 2: Deep Deterministic Policy Gradient Policy Architecture. The policy network is used \nto implement acceleration and steering demands. The input state parameters are partial sensor data. \nThen the action and state pairs are used in the critic network, which is the process of Q-learning. \nThe actor-critic architecture can update the policy in the direction performance gradient \n(\n)\nJ\n\n\n\n\n. \nBy using the actor-critic structure, we can update critic by minimizing the loss, \n2\n1\n1\n1\n(\n)\n( ( ,\n)\n(\n, (\n);\n)\n( ,\n;\n))\nQ\nQ\nQ\ni\ni\ni\ni\ni\ni\ni\nL\nE\nr s a\nQ s\ns\nQ s a\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                  (14) \nAnd we can update the actor policy by using the deterministic policy gradient, \n( , )\n( )\na\nJ\nE\nQ\ns a\ns\n\n\n\n\n\n\n\n\n\n\n\n\n                                       (15) \n3.2.2. Artificial Potential Field Method \nAs for the artificial potential field, we only consider the repulsive potential field for collision \navoidance. As is shown in the Figure 3, in the coordinate system of the ego vehicle, the \nrep\nF\n \nprojected along the x-axis corresponds to the steering command and the force projected along the \n6 \n \ny-axis corresponds to the acceleration command. We assume the forces are continuous and only \nrelated to the distances, \n_\n1 cos\nrep\nx\ni\ni\ni\nF\nd \n\n\n                                                  (16) \n_\n1 sin\nrep\ny\ni\ni\ni\nF\nd \n\n\n                                                   (17) \nwhere \ni is the obstacle angle in the coordinate system of the ego vehicle, \nid  is the obstacle \ndistance from the ego vehicle, and  represents the power to be determined. \nVehicle\nObstacle 1\nObstacle 2\n1\n2\n\n \nFigure 3: Repulsive potential field forces in ego vehicle coordinate system. The forces projected \nalong the x-axis of ego vehicle correspond to the steering command. The forces projected along the \ny-axis correspond to the acceleration command. \nThen the output actions are proportional to the potential field, \n_\nf\nfx\nrep\nx\nk F\n\n                                                            (18) \n_\nf\nfy\nrep\ny\nk F\n\n\n                                                            (19) \nwhere \nfx\nk  and \nfy\nk  are respectively the proportional coefficients of \n_\nrep\nx\nF\n and \n_\nrep\ny\nF\n. \n3.2.3. Path Tracking \nAs for the path tracking function, we want the vehicle to drive along the central track of the road. \nThe goal of path tracking is to minimize the angle between the car direction and the direction of \ntrack axis and shorten the distance between the vehicle centroid and the central road track (Kapania \net al., 2015). The equation (20) represents the steering command with tracking error and heading \nerror. We also tackle the acceleration command \np\naccording to the steering command. The basic \nrule is to decrease the vehicle speed when the steering command is high enough. \n1\n2\np\ne\n\n\n\n\n\n                                                     (20) \n7 \n \nwhere  is the angle between the car direction and the direction of track axis, e  is the distance \nbetween the vehicle centroid and the central road track, and \n1\n and \n2\n are respectively their \ncoefficients. \n\ne\n \nFigure 4: Diagram showing tracking error e  and heading error . The goal of path tracking \nis to minimize the tracking error and heading error to keep the vehicle drive along the track. \n4. Experiments \n4.1. Experiments Setup \nWe use TORCS platform (Loiacono et al., 2013) to implement our autonomous driving algorithm. \nFirst we train the policy network without opponents on GPU. The actor neural network consists of \ntwo hidden layers with 400 and 300 units respectively. The final output layer is a tanh layer to \nimplement steering and acceleration commands. The policy network learning rate is \n4\n10. The \ncritic neural network also consists of two hidden layers with 400 and 300 units with the learning \nrate \n3\n10. The discounted factor  is 0.99 and the training minibatch size is 64. \nThe input states for the actor-critic architecture are focus sensors, track sensors, vehicle speed, \nengine speed, wheel speed, track position and vehicle angle. The output actions are the steering \ncommands and acceleration commands. \nAfter the training, we combine the learning policy actions and safety based control actions. The \nparameters for the safety based control are shown in Table 1. The input states for the repulsive \npotential field are the 36 opponent distances. The path tracking method uses the angle error and \ntrack position to calculate the actions. \nTable 1: Parameters for the artificial potential field and path tracking \nSymbol \nValue \nSymbol \nValue \nfx\nk  \n20 \n2\n \n2 \nfy\nk  \n10 \n \n0.4 \n \n1.5 \n \n0.3 \n1\n \n3.18 \n \n0.3 \n \n4.2. Results \nWe first train the driving policy using DDPG algorithm without opponents on GPU. The average \nQ-value of the actor-critic structure has increased gradually (Fig 5). During the training process, \nwe divide the reward by 150 to limit the one-step reward to [0,2]. After approximate 13 hours \n8 \n \ntraining, the average Q-value reaches approximate 110. We then use the policy for autonomous \ndriving. The vehicle could perform well using the trained policy network.  \n200K\n400K\n600K\n800K\n1.00M\n1.20M\n1.40M\n1.60M\n0.00\n20.0\n40.0\n60.0\n80.0\n100\n120\n140\nAverage Q-value\nTraining steps\n \nFigure 5: Average Q-value during the training process. After 13 hours, the average value \nreaches 110, we then used the policy network for autonomous driving without vehicles around. \nWe then combine the DDPG policy network and safety based control. The ratio coefficients for \nDDPG, Path Tracing and Artificial Potential Field (APF) method are 0.4, 0.3 and 0.3. Figure \n6(a)~6(d) show 4 typical scenarios using the combined algorithm and Figure 6(d)~6(e) show the \nrespective steering commands and acceleration commands by DDPG, Path Tracking and APF.  \nFigure 6(a) shows the vehicle runs along the curve. The DDPG algorithm outputs the major steering \ncommand and the APF commands for steering and acceleration are 0 because no vehicle around is \ndetected. Figure 6(b) and Figure 6(c) show that there is one opponent around and APF commands \noutput corresponding actions. The opponent distance in Figure 6(c) is shorter than Figure 6(b), so \nthe APF commands play the major parts in Figure 6(c). Figure 6(d) shows the scenario in which \nthe vehicle runs along the curve with two vehicles around. The ego vehicle is far from the track so \nthe Path Tracking steering command is higher than two other methods.  \n5. Conclusion \nIn this paper, we combine the deep reinforcement learning and safety based control, including \nartificial potential field and path tracking for autonomous driving. We first use the DDPG algorithm \nto get the driving policy using partial state inputs and then combine the policy network and safety \nbased control to avoid collision and drive along the track. Experiments show that the three \nalgorithms coordinate well in the TORCS environment. \n \n \n \n \n \n \n \n9 \n \n1\n2\n3\n4\n \n(a)                                  (b)                                  (c)                                   (d)    \nDDPG\nPath Tracking\nAPF\nSteering Command\nAcceleration Command\n-1.0\n-0.5\n0.0\n0.5\n1.0\n-1.0\n-0.5\n0.0\n0.5\n1.0\n \n                                     (e)                                                               (f) \nFigure 6: Typical driving scenarios and corresponding commands. The positive steering \ncommand represents turning left and vice cersa. The negative acceleration command represents \nbraking control. The blue race car with red box is the ego vehicle. (a), Driving along the curve with \nno opponent around. (b), One opponent vehicle in front. (c), One opponent in bottom left. (d), Two \nopponents around while driving along the curve. (e), The steering commands in the four typical \nscenarios using DDPG, Path Tracking and APF. (f), The accelearation commands in the four typical \nscenarios using DDPG, Path Taracking and APF.  \n6. References \nSutton R S, Barto A G. Reinforcement learning: An introduction. Cambridge: MIT press, 2012. \nLillicrap T P, Hunt J J, Pritzel A, et al. Continuous control with deep reinforcement learning. arXiv preprint \narXiv:1509.02971, 2015. \nMnih V, Badia A P, Mirza M, et al. Asynchronous methods for deep reinforcement learning. arXiv preprint \narXiv:1602.01783, 2016. \nMnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning. \nNature, 2015, 518(7540): 529-533. \nSilver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree \nsearch. Nature, 2016, 529(7587): 484-489. \nMnih V, Kavukcuoglu K, Silver D, et al. Playing atari with deep reinforcement learning. arXiv preprint \narXiv:1312.5602, 2013. \nSilver D, Lever G, Heess N, et al. Deterministic policy gradient algorithms. In ICML, 2014. \nSutton R S, McAllester D A, Singh S P, et al. Policy Gradient Methods for Reinforcement Learning with \nFunction Approximation[C]//NIPS. 1999, 99: 1057-1063. \n10 \n \nNair A, Srinivasan P, Blackwell S, et al. Massively parallel methods for deep reinforcement learning[J]. \narXiv preprint arXiv:1507.04296, 2015. \nKhatib O. Real-time obstacle avoidance for manipulators and mobile robots. The international journal of \nrobotics research, 1986, 5(1): 90-98. \nHausknecht M, Stone P. Deep Reinforcement Learning in Parameterized Action Space. arXiv preprint \narXiv:1511.04143, 2015. \nChen C, Seff A, Kornhauser A, et al. Deepdriving: Learning affordance for direct perception in \nautonomous driving[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: \n2722-2730. \nLoiacono D, Cardamone L, Lanzi P L. Simulated car racing championship: Competition software \nmanual[J]. arXiv preprint arXiv:1304.1672, 2013. \nKapania N R, Gerdes J C. Path tracking of highly dynamic autonomous vehicle trajectories via iterative \nlearning control[C]//2015 American Control Conference (ACC). IEEE, 2015: 2753-2758. \nGe S S, Cui Y J. Dynamic motion planning for mobile robots using potential field method. Autonomous \nRobots, 2002, 13(3): 207-222. \nVadakkepat P, Tan K C, Ming-Liang W. Evolutionary artificial potential fields and their application in real \ntime robot path planning[C]//Evolutionary Computation, 2000. Proceedings of the 2000 Congress on. \nIEEE, 2000, 1: 256-263. \n",
  "categories": [
    "cs.RO"
  ],
  "published": "2016-12-01",
  "updated": "2016-12-01"
}