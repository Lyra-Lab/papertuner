{
  "id": "http://arxiv.org/abs/1612.01072v1",
  "title": "Word Recognition with Deep Conditional Random Fields",
  "authors": [
    "Gang Chen",
    "Yawei Li",
    "Sargur N. Srihari"
  ],
  "abstract": "Recognition of handwritten words continues to be an important problem in\ndocument analysis and recognition. Existing approaches extract hand-engineered\nfeatures from word images--which can perform poorly with new data sets.\nRecently, deep learning has attracted great attention because of the ability to\nlearn features from raw data. Moreover they have yielded state-of-the-art\nresults in classification tasks including character recognition and scene\nrecognition. On the other hand, word recognition is a sequential problem where\nwe need to model the correlation between characters. In this paper, we propose\nusing deep Conditional Random Fields (deep CRFs) for word recognition.\nBasically, we combine CRFs with deep learning, in which deep features are\nlearned and sequences are labeled in a unified framework. We pre-train the deep\nstructure with stacked restricted Boltzmann machines (RBMs) for feature\nlearning and optimize the entire network with an online learning algorithm. The\nproposed model was evaluated on two datasets, and seen to perform significantly\nbetter than competitive baseline models. The source code is available at\nhttps://github.com/ganggit/deepCRFs.",
  "text": "WORD RECOGNITION WITH DEEP CONDITIONAL RANDOM FIELDS\nGang Chen†\nYawei Li⋆\nSargur N. Srihari†\n† Department of Computer Science, SUNY at Buffalo, Buffalo NY 14260\n⋆School of Communication and Information Engineering\nUniversity of Electronic Science and Technology of China\nChengdu, Sichuan 611731 China\nABSTRACT\nRecognition of handwritten words continues to be an impor-\ntant problem in document analysis and recognition. Exist-\ning approaches extract hand-engineered features from word\nimages–which can perform poorly with new data sets. Re-\ncently, deep learning has attracted great attention because of\nthe ability to learn features from raw data. Moreover they\nhave yielded state-of-the-art results in classiﬁcation tasks in-\ncluding character recognition and scene recognition. On the\nother hand, word recognition is a sequential problem where\nwe need to model the correlation between characters. In this\npaper, we propose using deep Conditional Random Fields\n(deep CRFs) for word recognition. Basically, we combine\nCRFs with deep learning, in which deep features are learned\nand sequences are labeled in a uniﬁed framework. We pre-\ntrain the deep structure with stacked restricted Boltzmann ma-\nchines (RBMs) for feature learning and optimize the entire\nnetwork with an online learning algorithm.\nThe proposed\nmodel was evaluated on two datasets, and seen to perform\nsigniﬁcantly better than competitive baseline models.\n1. INTRODUCTION\nWord recognition [1, 2, 3] can be formulated as a sequence\nlabeling problem. Each word image is a set of candidate char-\nacter segments obtained ﬁrst by segmentation methods, then\nit is labeled with classiﬁers, such as support vector machines\n(SVM) and Conditional Random Fields (CRFs). Although it\nhas been researched many years, it is still an challenge prob-\nlem, considering the complex writing styles. For example,\nthe poor quality in handwritten documents makes character\nboundaries hard to determine [4]. What’s more, even given\nthe segmented characters in the word images, it is still hard\nto get satisﬁed results, because different people have different\nhandwriting styles. In this paper, we attempt to address the\nsecond issue. In other words, we assume character segmen-\ntation is given, and then directly recognize the entire word\nwithout character segmentation, as [3] did.\nRecently, deep learning has attracted great attention be-\ncause it can learn features automatically from raw data, which\nhas been thought as a vital step forward to artiﬁcial intelli-\ngence (AI). Linear CRFs, as a graphic model with correla-\ntion among labels, has been a powerful tool for sequential\nlabeling, with applications on a wide range of tasks, such\nas character recognition, speech recognition and natural lan-\nguage processing. Moreover, as a discriminative model, it has\nshown advantages over generative models, i.e. HMMs on la-\nbeling sequence data [7, 8]. Thus, we can leverage CRFs for\nword recognition, because it has shown promising results on\na variety of handwriting datasets [7, 3].\nInspired by deep learning for feature learning, we unify\ndeep learning and CRFs into one framework, so that it can dis-\ncover discriminative features to improve word classiﬁcation\ntask. Thus, our model is more powerful than linear Condi-\ntional Random Fields (CRFs) because the objective function\nlearns latent non-linear features so that target labeling can be\nbetter predicted. Different from traditional approaches, we\ntrain our model with an independent learning stage and also\nuse online learning to estimate model parameters. We test our\nmethod on two handwriting datasets, and show the advantages\nof our method over shallow CRFs models and deep learning\nmethods signiﬁcantly.\n2. RELATED WORK\nHandwriting recognition [2] is a classical recognition prob-\nlem, which has been researched for a long time. However,\nconsidering the complex cases, such as cursive styles and spu-\nrious strokes, it is far from being solved.\nOver the past decades, many methods have beed proposed\n[1, 9] for handwriting recognition, and good success has been\nachieved for small-vocabulary and highly constrained do-\nmains such as digital recognition [9], mail sorting [1, 2] and\ncheck processing [10]. Marti et al proposed to use Hidden\nMarkov model (HMM) to incorporate the context information\nfor handwritten material recognition [11]. In their method,\neach character has 14 states, and words are modeled as a\nconcatenation of these states under HMM. Later, Vincia-\nrelli et al proposed a sliding window approach [12], an high\norder n-gram HMM model (up to trigrams) and demon-\narXiv:1612.01072v1  [cs.CV]  4 Dec 2016\nstrated promising results on large vocabulary handwritten\ndata.\nBoosted decision trees [13] had been used on word\nrecognition and retrieval, and yielded good retrieval perfor-\nmance with low word error rate. Inspired by the advantages\nof CRFs [14] for sequential labeling problem, several meth-\nods were put forward recently. For example, [7] leveraged\nCRFs for handwriting recognition, and demonstrated that\non the whole word recognition task, linear CRFs performs\nbetter than HMM. Similarly, another CRFs with dynamical\nprogramming [3] was also proposed to word recognition.\nMore recently, deep learning methods, such as CNN,\nDBN and recurrent neural network (RNN) [15] had gained\nsigniﬁcant attention on classiﬁcation tasks.\nFor example,\nGraves et al [16] used the bi-directional and multi-dimensional\nlong short term memory (LSTM) [21] and yielded promising\nresults on Arabic handwriting recognition. Furthermore, Hid-\nden conditional random ﬁelds [8] was proposed and yielded\nthe state of the art results on the upenn OCR dataset [17].\nCRFs also has been combined with neural network for se-\nquential labeling (DNN+CRFs [20]).\n3. WORD RECOGNITION WITH DEEP CRFS\nLet D = {⟨xi, yi⟩}N\ni=1 be a set of N training examples.\nEach example is a pair of a time series ⟨xi, yi⟩, with xi =\n{xi,1, xi,2, ..., xi,Ti} and yi = {yi,1, yi,2, ..., yi,Ti}, where\nxi,t ∈Rd is the i-th observation at time t and yi,t is the cor-\nresponding label (we indicate its encoded vector as yi,t that\nuses a so-called 1-of-K encoding).\n3.1. Linear CRFs\nLinear ﬁrst-order CRFs [14] is a conditional discriminative\nmodel over the label sequence given the data\np(yi|xi) = exp{E(xi, yi)}\nZ(xi)\n(1)\nwhere Z(xi) is the partition function and E(xi, yi) is the en-\nergy function given by\nE(xi, yi) = yT\ni,1π + yT\ni,Tiτ\n+\nTi\nX\nt=1\n(xT\ni,tWyi,t + bT yi,t) +\nTi\nX\nt=2\nyT\ni,t−1Ayi,t\n(2)\nwhere yT\ni,1π and yT\ni,Tiτ are the initial-state and ﬁnal-state\nfactors respectively, bT yi,t is the bias term for labels,\nA ∈RK×K represents the state transition parameters and\nW ∈Rd×K represents the classiﬁcation parameter of the\ndata-dependent term. One of the main disadvantages of linear\nCRFs is the linear dependence on the raw input data term.\nThus, we introduce our sequential labeling model with deep\nfeature learning, which leverages both context information,\nas well as the nonlinear representations in the deep learning\narchitecture [9].\n3.2. Deep CRFs\nAlthough it is possible to leverage the deep neural networks\nfor structured prediction, its output space is explosively grow-\ning because of non-determined length of sequential data.\nThus, we consider a compromised model, which combines\nCRFs and deep learning in an uniﬁed framework. Thus, we\npropose an objective function with L layers neural network\nstructure,\nL(D; θ, ω) = −\nN\nX\ni=1\nlogp(yi,1, ..., yi,Ti|hi,1, ..., hi,Ti)\n+ λ2||θ||2 + λ3||ω||\n(3)\nwhere θ and ω are the top layer parameters and lower layer\n(l = {1, ..., L −1}) parameters respectively, which will be\nexplained later. The ﬁrst row on the right side of the equation\nis from the linear CRFs in Eq. (1), but with latent features,\nwhich depends respectively on θ and the latent non-linear fea-\ntures hi = {hi,1, .., hi,Ti} in the coding space, with\nlogp(yi,1, ..., yi,Ti|hi,1, ..., hi,Ti)\n=\nTi\nX\nt=2\nyT\ni,t−1Ayi,t +\nTi\nX\nt=1\n\u0000hT\ni,tWyi,t + bT yi,t\n\u0001\n+yT\ni,1π + yT\ni,Tiτ −log(Z(hi))\n(4)\nand non-linear mappings hi is the output with L −1 layers\nneural network, s.t.\nhi = fL−1 ◦fL−2 ◦· · · ◦f1\n|\n{z\n}\nL−1 times\n(xi)\n(5)\nwhere ◦indicates the function composition, and fi is logistic\nfunction with the weight parameter Wl respectively for l =\n{1, .., L −1}, refer more details in Sec. 3.3. With a bit abuse\nof notation, we denote hi,t = f1→(L−1)(xi,t).\nThe last two terms in Eq. 3 are for regularization on the all\nparameters with θ = {A, W, π, τ, b, c}, and ω = {Wl|l ∈\n[1, .., L−1]}. We add the ℓ2 regularization to θ as most linear\nCRFs does, while we have the ℓ1-regularized term on weight\nparameters ω in the deep neural network to avoid overﬁtting\nin the learning process.\nThe aim of our objective function in Eq. (3) is for sequen-\ntial labeling, which explores both the advantages of Markov\nproperties in CRFs and latent representations in deep learn-\ning. Firstly, our model can learn non-linear representation and\nlabel sequences with non-determined length. Secondly, our\nmodel can predict structured outputs or label sequences, while\nthe DBN model [9] is just one label for each instance, which\nis independent without context information. Note that we use\nthe ﬁrst-order CRFs for clarity in Eq. 4 and the rest of the\npaper, which can be easily extended for the second or high-\norder cases. Lastly, compared to traditional DNN+CRFs [20],\nwe use an online algorithm in our deep learning model for pa-\nrameter updating, which has the potential to handle large scale\ndataset.\n3.3. Learning\nWe take an online learning strategy in our method, which is\ndifferent from traditional approaches. We use RBMs to ini-\ntialize the weights for l = {1, .., L −1} layer by layer greed-\nily, with contrast divergence [6] (we used CD-1 in our exper-\niments). Then we compute the sub-gradient w.r.t. θ and ω in\nthe objective function, and optimize it with online learning.\nInitialization: In our deep model, the weights from the\nlayers 1 to L −1 are Wl respectively, for l = {1, .., L −1},\nand the top layer L has weight W. We ﬁrst pre-train the L-\nlayer deep structure with RBMs layer by layer greedily.\nLearning: In training the CRFs with deep feature learn-\ning, our aim is to minimize objective function L(D; θ, ω) in\nEq. (3). Because we introduce the deep neural network here\nfor feature learning, the objective is not convex function any-\nmore. However, we can ﬁnd a local minimum in Eq. (3). In\nour learning framework, we optimize the objective function\nwith an online learn algorithm, by mixing perceptron training\nand stochastic gradient descent.\nFirstly, we can calculate the (sub)gradients for all param-\neters. Considering different regularization methods for θ and\nω respectively, we can calculate gradients w.r.t. them sepa-\nrately. As for the parameters in the negative log likelihood in\nEq. 3, we can compute the gradients w.r.t. θ as follows\n∂L\n∂A =\nN\nX\ni=1\nTi\nX\nt=2\nyi,t−1(yi,t)T −γi,t−1(γi,t)T ;\n(6a)\n∂L\n∂W =\nN\nX\ni=1\nTi\nX\nt=1\n\u0000hi,t(yi,t −γi,t)T\n(6b)\nwhere yi,t ∈RK is the 1-of-K encoding for labeling, γi,t ∈\nRK is the vector of length K, which is the posterior probabil-\nity for labels in the sequence and will be introduced in Sec.\n3.4; and hi = {hi,1, ..., hi,Ti} are the latent features learned\nvia Eq. (5). In the above gradients, we have ignored the gra-\ndients w.r.t. biases for convenience. Note that it is easy to\nderive the gradients of the ℓ2 regularization term w.r.t. θ in\nthe objective in Eq. (3), which can be added to the gradients\nin Eq. (6).\nAs for the gradients of weights ω = {Wl|l ∈[1, .., L −\n1]}, we ﬁrst use backpropagation to get the partial gradient\nin the neural network, refer to [9] for more details. Then the\ngradient of the ℓ1 term in Eq. (3) can be attached to get the\nﬁnal gradients w.r.t. Wl for l = {1, .., L −1}.\nFinally, we use a mixture of perceptron learning and\nstochastic gradient descent to optimize the objective function.\nIn our experiments, we tried L-BFGS, but it can be easily\ntrapped into the bad local minimum, and performs worse than\nother optimization methods in almost all experiments. Thus,\nin this work, we use perceptron-based learning for the CRF\nrelated parameters and stochastic gradient descent for the\nparameters in the deep structure in all our experiments.\nThus, for the CRF related parameters θ in Eq. (3), we\nﬁrst project xi into the code hi according to Eq. (5). Then,\nthe updating rule takes the form below\nθ ←θ + ηθ\n∂\n∂θ\n\u0000E(hi, yi) −E(hi, y∗\ni )\n\u0001\n(7)\nwhere y∗\ni is the most violated constraint in the misclassiﬁ-\ncated case, and ηθ is a parameter step size. Note that the\nposterior probability γi,t ∈RK in Eq. (6) should be changed\ninto the hard label assignment y∗\ni,t in the inference stage.\nWhile for the weights ω in the deep neural network, we\nﬁrst use backpropagation to compute the gradients, and then\nupdate it as follows\nω ←ω −ηω\n∂L\n∂ω\n(8)\nwhere ηω is the step size for the parameters.\n3.4. Inference\nIn the testing stage, the main inferential problem is to com-\npute the most likely label sequence y∗\n1,...,T given the data\nx1,...,T by argmaxy′1,...,T p(y′1,...,T |x1,...,T ).\nGiven any new sequence xi\n=\n{xi,1, ..., xi,Ti}, we\nﬁrst use Eq.\n(5) to compute the non-linear code hi =\n{hi,1, ..., hi,Ti}.\nThen, we can do the inference as linear\nCRFs does by thinking hi as the new obersevation.\nThe\ninference problem can be formulated as\ny∗\n1,...,T = argmax\ny′1,...,Ti\np(y′1,...,Ti|hi)\n(9)\nThis can be solved efﬁciently with Viterbi algorithm [18, 19].\nNote that γi,t = p(y∗\ni,t|hi) is the posterior probability from\nViterbi algorithm, and can be used in Eq. 6 to calculate the\ngradient w.r.t. θ.\n4. EXPERIMENTS\nTo test our method, we compared our method to the state of\nthe art approaches and performed experiments on word recog-\nnition task on two widely used datasets: OCR dataset and IC-\nDAR2003 word recognition dataset.\n4.1. Data sets\n1. The OCR dataset [17] contains data for 6, 877 handwritten\nwords with 55 unique words, in which each word xi is rep-\nresented as a series of handwritten characters {xi1, ..., xi,Ti}.\nThe data consists of a total of 52, 152 characters (i.e., frames),\nwith 26 unique classes. Each character is a binary image of\nsize 16×8 pixels, leading to a 128-dimensional binary feature\nvector. In our experiments, we used the four data sets in [8],\nwhich are available on the author’s website1.\n1http://cseweb.ucsd.edu/˜lvdmaaten/hucrf/\nHidden-Unit_Conditional_Random_Fields.html\nWord recognition error rate (%)\nLinear-chain CRF [14]\n53.2\nLSTM [16]\n2.31\nDNN [9]\n18.5\nHidden-unit CRF [8]\n4.62\nTraditional DNN+CRFs [20]\n5.19\nOur method\n1.60\nTable 1. The experimental comparisons on the OCR dataset.\nThe results reveal the merits of our method, and show that our\ndeep CRFs outperforms other methods signiﬁcantly.\n2. ICDAR 2003 word recognition dataset2.After deleting\nnon recognizable numbers and characters, we have 1147 ef-\nfective words available for training. All of these words belong\nto 874 classes (words), which consisted of 70 unique charac-\nters.\n4.2. Experimental Setup\nIn our experiments, we randomly initialized the weight W\nby sampling from the normal Gaussian distribution, and all\nother parameters in θ to be zero (i.e. biases b and c, and the\ntransition matrix A all to be zero). As for ω = {Wl|l ∈\n[1, .., L −1]}, we initialized them with DBN, which had been\nmentioned before. As for the number of layers and the num-\nber of hidden units in each layer, we set differently according\nto the dimensionality for different datasets. In all the exper-\niments, we used the 3-layer deep CRFs model with hidden\nunits [400 200 100] respectively in each layer. As for context\nmodel, we used the second order potentials over characters to\nrecognize words.\nEvaluation: we use word error rate to measure all meth-\nods in 10-fold cross-validation. In all experiments with per-\nceptron learning, we did not use regularization terms. In other\nwords, we set λ2 = 0. And λ3 = 2 × 10−4 for weights in the\ndeep network. For each dataset, we followed the protocol in\n[8] and divided it into 10 folds (9 folds as the training set, and\nthe rest as the testing/validation set), and performed 100 full\nsweeps through the training data, to update the model parame-\nters. We tuned the base step size based on the error on a small\nheld-out validation set. From the base step size, we computed\nparameter-speciﬁc step sizes ηθ and ηω as suggested by [22].\nIn Table 1, we compared the performance of our method\nwith the performance of competing models on the OCR\ndataset. Our method yields a generalization word error of\n1.6%, while the best performance of other methods is 2.31%.\nIt also demonstrates that our model is signiﬁcantly better than\nother methods, and the deep structure is deﬁnitely helpful than\nthe shadow models, such as hidden CRFs. In addition, our\nmethod is signiﬁcantly better than traditional DNN+CRFs.\n2http://algoval.essex.ac.uk/icdar/datasets/\nTrialTrain/word.zip\nFig. 1. The samples from the ICDAR 2003 word recognition\ndataset. The subset in our experiment has total 70 unique\ncharacters and 874 words.\nword recognition (error rate) (%)\nLinear-chain CRF [14]\n97.6\nDNN [9]\n41.8\nHidden-unit CRF [8]\n62.7\nTraditional DNN+CRFs [20]\n82.6\nOur method\n40.2\nTable 2. The experimental comparisons on the ICDAR2003\nword recognition dataset. The results reveal that our deep\nCRFs is effective for word recognition.\nThus, as for our model, it shows that the learning stage is\nhelpful and can improve the accuracy signiﬁcantly.\nWe also tested our method on the ICDAR2003 dataset. In\nour experiment, we ﬁrst split the words into characters, and\nthen we tested our deep CRFs over the characters. As for\nword segmentation, we use an naive approach (more sophis-\nticated word segmentation methods will be deﬁnitely help-\nful in this case). Basically, given the word image, we know\nthe number of characters in this image according to its label.\nThus, we split the word image equally by dividing the number\nof characters. Since the sizes of all images are different, we\nneed to resize all characters into the same dimension. Hence,\nwe computed the width-height ratio for each character (which\nis the split one from its corresponding word image) on all the\ndataset, and then get the average size for all characters. The\nresult character size is 65 × 40, and we reshape it to a binary\nvector with 2600 dimension (Note that we binarize all the im-\nages or characters). Table 2 shows the word recognition error\nrate on ICDAR2003 dataset with ﬁve folder cross validation.\nAgain, it demonstrates that our method outperforms CRFs re-\nlated models and deep learning methods. It also indicates that\nour model is better than traditional DNN+CRFs.\n5. CONCLUSION\nIn this paper, we propose a deep conditional random ﬁelds\n(deep CRFs) for word classiﬁcation problems. Compared to\ntraditional DNN+CRFs, we propose a mixture online learn-\ning algorithm: perceptron training for CRFs parameters and\nstochastic gradient descent for low level weights in the deep\nstructure. We update parameters in an online fashion, which\nmakes it possible to apply our model to large scale datasets.\nWe tested our methods on widely used word recognition\ndatasets, and show that our deep CRFs is effective compared\nto other shallow CRFs and deep learning methods.\n6. REFERENCES\n[1] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel, “Backpropaga-\ntion applied to handwritten zip code recognition,” Neu-\nral Comput., vol. 1, no. 4, pp. 541–551, Dec. 1989.\n[2] R. Plamondon and S. N. Srihari, “On-line and off-\nline handwriting recognition: A comprehensive survey,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 1,\npp. 63–84, Jan. 2000.\n[3] S. Shetty, H. Srinivasan, and S. N. Srihari, “Handwrit-\nten word recognition using conditional random ﬁelds.”\nin ICDAR.\nIEEE Computer Society, 2007, pp. 1098–\n1102.\n[4] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner,\n“Gradient-based learning applied to document recogni-\ntion,” in Proceedings of the IEEE, 1998, pp. 2278–2324.\n[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Ima-\ngenet classiﬁcation with deep convolutional neural net-\nworks,” in Advances in Neural Information Processing\nSystems, 2012.\n[6] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learn-\ning algorithm for deep belief nets,” Neural Comput.,\nvol. 18, no. 7, pp. 1527–1554, jul 2006.\n[7] S. Feng, R. Manmatha, and A. McCallum, “Explor-\ning the use of conditional random ﬁeld models and\nhmms for historical handwritten document recognition,”\nin Proceedings of the Second International Conference\non Document Image Analysis for Libraries, ser. DIAL\n’06.\nWashington, DC, USA: IEEE Computer Society,\n2006, pp. 30–37.\n[8] L. van der Maaten, M. Welling, and L. K. Saul, “Hidden-\nunit conditional random ﬁelds,” in AISTATS, 2011, pp.\n479–488.\n[9] G. E. Hinton and R. R. Salakhutdinov, “Reducing the\ndimensionality of data with neural networks,” Science,\nvol. 313, no. 5786, pp. 504–507, Jul. 2006.\n[10] S. Madhvanath and V. Govindaraju, “The role of holis-\ntic paradigms in handwritten word recognition,” IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 23, no. 2, pp.\n149–164, Feb. 2001.\n[11] U.-V. Marti and H. Bunke, “Using a statistical language\nmodel to improve the performance of an hmm-based\ncursive handwriting recognition systems,” Jnl. of Pat-\ntern Recognition and Artiﬁcal Intelligence, pp. 65–90,\n2001.\n[12] A. Vinciarelli, S. Bengio, and H. Bunke, “Ofﬂine recog-\nnition of large vocabulary cursive handwritten text,” in\nProceedings of International Conference on Document\nAnalysis and Recognition (ICDAR), 2003.\n[13] N. R. Howe, T. M. Rath, and R. Manmatha, “Boosted\ndecision trees for word recognition in handwritten doc-\nument retrieval,” in Proceedings of the 28th Annual In-\nternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, ser. SIGIR ’05.\nNew York, NY, USA: ACM, 2005, pp. 377–383.\n[14] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, “Con-\nditional random ﬁelds: Probabilistic models for seg-\nmenting and labeling sequence data,” in ICML, 2001,\npp. 282–289.\n[15] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780,\nNov. 1997.\n[16] A. Graves and J. Schmidhuber, “Ofﬂine handwriting\nrecognition with multidimensional recurrent neural net-\nworks.” in NIPS, D. Koller, D. Schuurmans, Y. Bengio,\nand L. Bottou, Eds.\nCurran Associates, Inc., 2008, pp.\n545–552.\n[17] B. Taskar, C. Guestrin, and D. Koller, “Max-margin\nmarkov networks,” in NIPS.\nMIT Press, 2003.\n[18] L. R. Rabiner, “A tutorial on hidden markov models and\nselected applications in speech recognition,” in PRO-\nCEEDINGS OF THE IEEE, 1989, pp. 257–286.\n[19] C. M. Bishop, Pattern Recognition and Machine Learn-\ning.\nSecaucus, NJ, USA: Springer-Verlag New York,\nInc., 2006.\n[20] T. M. T. Do and T. Artires, “Neural conditional random\nﬁelds.” in AISTATS, ser. JMLR Proceedings, Y. W. Teh\nand D. M. Titterington, Eds., vol. 9.\nJMLR.org, 2010,\npp. 177–184.\n[21] A. Karpathy and F.-F. Li, “Deep visual-semantic align-\nments for generating image descriptions.” in CVPR.\nIEEE, 2015, pp. 3128–3137.\n[22] A. Gelfand,\nY. Chen,\nL. van der Maaten,\nand\nM. Welling, “On herding and the perceptron cycling the-\norem,” in Advances in Neural Information Processing\nSystems 23, J. Lafferty, C. K. I. Williams, J. Shawe-\nTaylor, R. Zemel, and A. Culotta, Eds., 2010, pp. 694–\n702.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2016-12-04",
  "updated": "2016-12-04"
}