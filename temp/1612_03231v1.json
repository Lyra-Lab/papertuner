{
  "id": "http://arxiv.org/abs/1612.03231v1",
  "title": "A natural language interface to a graph-based bibliographic information retrieval system",
  "authors": [
    "Yongjun Zhu",
    "Erjia Yan",
    "Il-Yeol Song"
  ],
  "abstract": "With the ever-increasing scientific literature, there is a need on a natural\nlanguage interface to bibliographic information retrieval systems to retrieve\nrelated information effectively. In this paper, we propose a natural language\ninterface, NLI-GIBIR, to a graph-based bibliographic information retrieval\nsystem. In designing NLI-GIBIR, we developed a novel framework that can be\napplicable to graph-based bibliographic information retrieval systems. Our\nframework integrates algorithms/heuristics for interpreting and analyzing\nnatural language bibliographic queries. NLI-GIBIR allows users to search for a\nvariety of bibliographic data through natural language. A series of text- and\nlinguistic-based techniques are used to analyze and answer natural language\nqueries, including tokenization, named entity recognition, and syntactic\nanalysis. We find that our framework can effectively represents and addresses\ncomplex bibliographic information needs. Thus, the contributions of this paper\nare as follows: First, to our knowledge, it is the first attempt to propose a\nnatural language interface to graph-based bibliographic information retrieval.\nSecond, we propose a novel customized natural language processing framework\nthat integrates a few original algorithms/heuristics for interpreting and\nanalyzing natural language bibliographic queries. Third, we show that the\nproposed framework and natural language interface provide a practical solution\nin building real-world natural language interface-based bibliographic\ninformation retrieval systems. Our experimental results show that the presented\nsystem can correctly answer 39 out of 40 example natural language queries with\nvarying lengths and complexities.",
  "text": "A natural language interface to a graph-based bibliographic information \nretrieval system \nYongjun Zhu1, Erjia Yan, Il-Yeol Song \nCollege of Computing and Informatics, Drexel University, 3141 Chestnut Street, Philadelphia, PA 19104. \n{zhu, ey86, song}@drexel.edu \n \nAbstract \nWith the ever-increasing scientific literature, there is a need on a natural language interface to \nbibliographic information retrieval systems to retrieve related information effectively. In this paper, we \npropose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval \nsystem. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based \nbibliographic information retrieval systems.  Our framework integrates algorithms/heuristics for \ninterpreting and analyzing natural language bibliographic queries. NLI-GIBIR allows users to search for a \nvariety of bibliographic data through natural language. A series of text- and linguistic-based techniques \nare used to analyze and answer natural language queries, including tokenization, named entity \nrecognition, and syntactic analysis. We find that our framework can effectively represents and addresses \ncomplex bibliographic information needs. Thus, the contributions of this paper are as follows: First, to \nour knowledge, it is the first attempt to propose a natural language interface to graph-based bibliographic \ninformation retrieval. Second, we propose a novel customized natural language processing framework \nthat integrates a few original algorithms/heuristics for interpreting and analyzing natural language \nbibliographic queries. Third, we show that the proposed framework and natural language interface \nprovide a practical solution in building real-world natural language interface-based bibliographic \n                                                             \n1 Corresponding author \ninformation retrieval systems. Our experimental results show that the presented system can correctly \nanswer 39 out of 40 example natural language queries with varying lengths and complexities. \nKeywords: information retrieval; natural language interface; graph database; information visualization  \n \n1. Introduction \n \nBibliographic information retrieval systems such as Web of Science, Scopus, and Google Scholar \nhave become an unalienable component in searching bibliographic data (Chadegani et al., 2013). These \nsystems continuously index ever-increasing scientific literature, thus providing a source for scholars to \nlearn, create, and represent new knowledge (Jacso, 2005). These systems, however, have several \nmethodological limitations. Web of Science and Scopus provide form-based interfaces, in which users \nfirst select fields (e.g., topic and author) and then type appropriate values for each field. Advanced \nsearching is an option, in which users can formulate queries using tags and Boolean operators (Score, \n2009). While form-based interfaces are effective for simple queries (e.g., papers written by an author), it \nis known that the interface is not adequate in dealing with ad-hoc information and imposes a burden on \nusers to understand and select fields or tags (Noessner et al., 2010). Apart from form-based search \ninterfaces, Google Scholar leverages an intuitive keyword-based interface (Falagas et al., 2008). While a \nkeyword-based search interface is perhaps the most widely used for everyday information retrieval, it has \nchallenges in understanding queries, particularly in recognizing how keywords are related to one another \n(Tumer et al., 2009).  \nAn alternative to these form- and keyword-based interfaces is a natural language interface. A \nnatural language interface allows users to formulate queries expressed in natural language. It is more \nflexible than a form-based interface and has a higher level of expressiveness than a keyword-based \ninterface (Androutsopoulos et al., 1995). In current form- and keyword-based bibliographic information \nretrieval systems, users are provided with limited options to represent complex bibliographic queries (e.g., \npapers on information retrieval, which were cited by John’s papers in SIGIR). Through a natural \nlanguage interface, users can represent complex bibliographic queries using natural language and get \nrelevant results in one step without the need to fill out forms or try with different keywords. While form- \nand keyword-based interfaces are predominately adopted in current bibliographic information retrieval \nsystems, we design a framework of developing a natural language interface for bibliographic information \nretrieval. In particular, we aim to address the following two core questions concerning natural language \nbibliographic information retrieval:  \n1) How to design a framework to build a system that can interpret, query, and answer natural \nlanguage bibliographic queries; and  \n2) How to implement and evaluate a bibliographic information retrieval system with a natural \nlanguage interface?  \nThe framework interprets bibliographic queries expressed in controlled natural language and \nreturns relevant bibliographic data and relations. Natural language queries supported in the framework are \nrestricted to complex nominal phrases that describe bibliographic entities. We implement a natural \nlanguage-based interface on a graph-based bibliographic information retrieval system designed in our \nprevious work (Zhu, Yan, & Song, 2016). While our previous work introduced a general framework for a \ngraph-based bibliographic information retrieval system called GIBIR, the current work focuses on the \ninterface on top of the GIBIR. Theoretically, this study is novel because it introduces natural language \ninterfaces for graph-based bibliographic information retrieval. This study examines a series of approaches \nincluding query interpretation, processing, and visualization specifically for bibliographic searching \nenvironment―they consider a wide range of bibliographic information needs and the characteristics of \nbibliographic data. Thus, a natural language interface tailored for bibliographic environment provides a \nnew and effective way of searching bibliographic data. In addition, from practical aspects, by enabling \nusers to formulate bibliographic information needs in natural language, it liberates users from learning \ncumbersome ways of representing those needs. With ever-increasing bibliographic data, a natural \nlanguage interface allows an effective retrieval of data by enabling the representation of complex \nbibliographic information needs and simplifying the search process into a single step without multiple \nrefining procedures.  \nThis paper makes the following three contributions: First, it is the first attempt to propose a \nnatural language interface to graph-based bibliographic information retrieval. While there was an attempt \nto design a natural language interface (Doszkocs and Rapp, 1979) a few decades ago, it was proposed for \nretrieving bibliographic data specific to MEDLINE. In this paper, we propose a novel approach utilizing \npractical natural language techniques for modern graph-based bibliographic information retrieval systems. \nSecond, we propose a novel customized natural language processing framework that integrates a few \noriginal algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. The \nproposed framework has been developed by carefully examining characteristics of bibliographic data and \nbibliographic queries and thus, provides higher accuracy to satisfy more bibliographic information needs.   \nThird, we show that the proposed framework and natural language interface provide a practical solution to \nbuild real-world natural language interface-based bibliographic information retrieval systems.  Our \nexperimental results show that the presented system can correctly answer 39 out of 40 example natural \nlanguage queries with varying lengths and complexities  \nThe remainder of this paper is organized as follows: Section 2 surveys background knowledge on \nnatural language interface, named entity recognition, and syntactic analysis, which form the foundation of \nthe framework. Section 3 presents the natural language processing framework in detail.  Section 4 shows \nthe implemented system with example queries and reports experimental results of the framework by \ntesting 40 natural language queries. Section 5 concludes our paper. \n \n2. Literature Review \n  \nWe review three areas of research that directly connect to natural language-enabled bibliographic \ninformation retrieval systems, including natural language interface, named entity recognition, and \nsyntactic analysis. \n2.1. Natural language interface \n \nNatural language interfaces (NLI) are used to query structured information stored in databases. \nTwo types of NLI can be distinguished: one is natural language interfaces to databases (NLIDB), in \nwhich a relational database is used to store structured information; the other is natural language interfaces \nto knowledge bases (NLIKB) that use an ontology to manage information (e.g., Habernal & Konopík, \n2013; Abacha & Zweigenbaum, 2015). While the two types of NLI use different database systems, they \nhave common components, including the interpretation of natural language queries and concept mappings \nbetween entities in queries and databases (e.g., Cafarella & Etzioni, 2005; Tablan et al., 2008).  \nThe relational data model (Codd, 1970) proposed in the early 1970s had a major impact on \nNLIDB research. NLIDB are highly portable and can be attached to existing databases because relational \ndatabases are the norm of most traditional information retrieval systems (Vicknair et al., 2010). Compared \nto NLIDB, NLIKB have a relatively short history with the inception of semantic web (Berners-Lee et al., \n2001). Databases in this category deploy rich expressive power of ontologies represented in the resource \ndescription framework (Miller, 1998), thus generally achieving higher performances (e.g., Kaufmann & \nBernstein, 2010). Readers can refer to Androutsopoulos and colleagues’ work (1995) for a comprehensive \nreview of NLIDB systems. Recent NLIKB systems include PowerAqua (Fazzinga & Lukasiewicz, 2010), \nORAKEL (Cimiano et al., 2008), FREyA (Damljanovic et al., 2010), PANTO (Wang et al., 2007), and \nNLP-Reduce (Kaufmann et al., 2007). \nThe NLI designed in this paper is an NLI to graph databases (e.g., Roy & Zeng, 2013). Graph \ndatabases have comparable expressive power with ontologies (i.e., triple stores), but a much higher \nscalability, which are more suitable to real-world systems (Angles & Gutierrez, 2008). Graph databases \nhave been increasingly used in information retrieval systems (e.g., Park & Lim, 2015). Graph databases \nexcel relational databases in answerable questions due to its advantage on representing complex relations \namong data given that natural language queries are represented using complex relations among concepts. \nGiven the graph-like characteristics of bibliographic data as discussed in our previous work (Zhu, Yan, & \nSong, 2016), a natural language interface to graph database-based bibliographic information retrieval \nsystems provides a novel way of accessing and retrieving bibliographic data. \n2.2. Named entity recognition  \n \nNamed entity recognition (NER) is a task of identifying names of things in texts. These things \ninclude but not limited to persons, organizations, locations, and biomedical entities (Nadeau & Sekine, \n2007). Early NER systems used rule-based methods to recognize named entities. In a rule-based NER \nsystem, patterns in a text are identified and appropriate rules are handcrafted based on those patterns. \nThus, a rule-based method is mainly used in self-contained domains and has a limited applicability (e.g., \nRau, 1991). A dictionary-based NER system utilizes predefined dictionaries and performs a look-up in \ntexts (e.g., Ryu, Jang, & Kim, 2014; Mu, Lu, & Ryu, 2014). The method is widely used in domains such \nas biomedicine, in which named entities are well recorded and managed, for instance, in protein \nrecognition (Tsuruoka & Tsujii, 2003) and drug recognition (Rindflesch et al., 2000). Another popular \ncategory of NER is statistical NER (e.g., Derczynski et al., 2015). Widely used statistical NER includes \nmaximum entropy (ME)- (Chieu & Ng, 2002), hidden Markov models (HMM)- (Bikel et al., 1997), and \nconditional random fields (CRF)-based (McCallum & Li, 2003) NER systems. Some NER systems use \nmore than one type of NER: for example, Stanford NER (Finkel et al., 2005) provides both dictionary- \nand statistical-based NER through a gazette feature. \n \nBibliographic data are relatively easy to obtain through well-known bibliographic databases such \nas Web of Science and DBLP. Thus, in this paper, we used a dictionary-based approach to recognize \nbibliographic named entities (i.e., authors, papers, organizations, terms, and sources) from a natural \nlanguage query. By recognizing bibliographic named entities in a query, we are able to extract these \nentities as well as their relations to learn and answer queries.  \n2.3. Syntactic analysis (Parsing) \n \nA classic way of parsing is to derive parses from a string of words based on a structure grammar \nof prewritten phrases (i.e., context-free grammar) (e.g., Earley, 1980). With the introduction of annotated \ndata such as The Peen Treebank (Marcus et al., 1993), a number of statistical parsers were proposed and \nbecame popular. Readers can refer to Collins’ work (1997) for a more extensive review on statistical \nparsing models.  \nTwo popular ways of representing syntactic structures are constituency and dependency. For \nconstituency, words in a sentence are organized into nested constituents; while for dependency, dependent \nrelations between words are shown (Klein & Manning, 2004). Dependency parses can be obtained from \ndependency parsers (e.g., Fersini et al., 2014) or phrase structure parsers (i.e., constituency) by a \nconversion system (e.g., De Marneffe et al., 2006). The proposed framework uses a dependency structure \nto identify grammatical relations among words. Because we are interested in grammatical relations among \nbibliographic named entities recognized in natural language queries, dependency structures are more \nstraightforward than constituency structures that also show relations between phrases.  \n \n3. The Framework \nThe framework is designed to take a natural language query as the input and return correct \nanswers as the output. This is achieved by translating the input into a database query language. A natural \nlanguage query is translated into a graph query language because we use a graph database to manage \nbibliographic data. Multiple steps are involved in the translation, including finding answers to questions \nsuch as: 1) what is being asked? 2) what entities should be used to constrain the answer? and 3) how does \nthe asked entity relate to other entities? Figure 1 uses a flow chart to describe how the core components of \nthe framework interact with each other.  \n \nFigure 1. The flow chart of the designed framework \n \nThe steps are as follows: 1) a user formulates a query expressed in natural language; 2) \nbibliographic named entity recognition is performed by referencing predefined dictionaries and \nrecognized bibliographic named entities are then extracted; 3) a natural language query is tokenized based \non the result of bibliographic named entity recognition; 4) the tokenized natural language query is parsed \nto identify grammatical relations among bibliographic named entities; 5) the grammatical relations are \nfiltered and graph relations are generated ; 6) a graph query is formulated by combining bibliographic \nnamed entities and graph relations; 7) the graph query is translated into a graph query language; and 8) a \ngraph database is queried.  In the following subsections, we introduce each step in detail with example \nqueries. \n3.1. The formulation of natural language queries  \n \nAlthough the framework is designed to process a natural language query, it is not a question \nanswering system. Thus, a complete sentence with an interrogative pronoun is not supported in the \nframework.  Instead, noun phrases such as “papers that were written by John” and “authors of papers that \nwere published in SIGIR” are expected queries. Because the interpretation of natural language queries \ndepends on syntactic analysis, queries are expected to have no grammatical error. In addition, relative \npronouns, such as “that”, are expected to be included in a query to guarantee that a syntactic parser parses \nthe query correctly. For example, a query “papers that were written by John” is the preferred form of \n“papers written by John”.   \n3.2. The recognition and extraction of bibliographic named entities \n \nWe adopt a dictionary-based named entity recognition approach. We use a simple map structure \nto construct a dictionary, in which keys are names of bibliographic entities (e.g., “John”, “SIGIR”, and \n“information retrieval”) and values are their bibliographic types (i.e., Paper, Author, Term, Source, and \nOrganization). These five bibliographic types are regarded as the most useful as shown in previous \nstudies (e.g., Sun, Yu, and Han 2009). A dictionary is constructed by preprocessing the bibliographic \ndataset on which we perform searches. Five types of bibliographic instances and their type information \nare extracted from a self-explanatory dataset. Disambiguation is not performed due to the lack of \nappropriate identification data. We also add five bibliographic types as keys with annotations to show that \nthey are bibliographic types. For example, the entry <“paper”, “class_Paper”> is added to the \ndictionary so that the system recognizes words such as “paper” and “author” in natural language queries. \nAn additional annotation “class_” is added because we want to differentiate five entity types with \nbibliographic entities. \n \nAn approximate string matching algorithm introduced in Gusfield’s work (1997) is used to \nimplement the NER algorithm. In the algorithm, a distance of 1 was assigned to insertion, deletion, and \nsubstitution of a character. A maximum distance of 1 was allowed, so that we can recognize plurals or \nsingulars when we have only one form of the two of bibliographic named entities. For example, \n“Information System” in a query could be identified as a named entity when we only have the term \n“Information Systems” in our dictionary  \n3.3. The tokenization of natural language queries \n \nWe tokenize queries based on the results of named entity recognition to prepare parsing in the \nnext step. After recognizing named entities, we mark named entities of multiples words as single tokens, \nand then feed queries into a standard tokenizer. This supervised tokenization complements tokenizers’ \nshortage of domain knowledge on technical terms. For example, without using the results of named entity \nrecognition, terms composed of multiple words such as “information retrieval” will be processed into two \ndifferent tokens. Tokenization based on the results of named entity recognition can avoid this limitation \nbecause terms recognized as a single named entity are treated as one token. Table 1 shows the difference \nbetween tokenization without NER and with NER using an example query “papers about information \nretrieval and data mining”, in which tokens are separated by pairs of parentheses. \nTable 1. Tokenization without NER and with NER \nQuery \npapers about information retrieval and data mining \nTokenization without NER \n(papers), (about), (information), (retrieval), (and), (data), (mining) \nTokenization with NER \n(papers), (about), (information retrieval), (and), (data mining) \n \n 3.4. The parsing of tokenized natural language queries and the extraction of grammatical relations \n \nWe use Stanford parser (Klein & Manning, 2003) to parse queries. The output we generate is the \nStanford dependencies (De Marneffe et al., 2006) that use 56 grammatical relations to represent binary \nrelations among tokens. Grammatical relations are used to find out which tokens depend on or modify \nother tokens. For a bibliographic natural language query, parsing is used to find out grammatical relations \namong bibliographic named entities represented by tokens. Table 2 shows the dependency relations of a \nsample query “papers about information retrieval and data mining”. Readers can refer to De Marneffe \nand colleague’s work (2006) for a detailed explanation of each dependency relation. \nTable 2. Dependency relations of the query “papers about information retrieval and data mining” \nOrder \nSubject \nObject \nRelation Code \nRelation Name \n1 \n \npapers \nroot \nroot \n2 \ninformation retrieval \nabout \ncase \ncase marker \n3 \npapers \ninformation retrieval \nnmod \nnmod_preposition \n4 \ninformation retrieval \nand \ncc \ncoordination \n5 \npapers \ndata mining \nnmod \nnmod_preposition \n6 \ninformation retrieval \ndata mining \nconj \nconj_collapsed \n \nFor queries that involve citations such as “papers about information retrieval that were cited by \npapers that were written by John”, they are divided into two parts: a cited part and a citing part. By doing \nso, we reduce the complexities and errors in interpreting queries, because a long list of dependency \nrelations may be error-prone. By dividing the example query into two parts, we no longer need to \nconsider grammatical relations between “papers” in the cited part and “John” in the citing part. This is a \npractical way to improve the performance of a parser, and thus, words such as “cited”, “cites”, “cite”, and \n“citing” are used to divide a query into two parts. Parsing is separately applied to each part, and the \nresults are integrated in a later step to generate graph relations. \n3.5. The generation of graph relations from dependency relations  \n \nA graph query is a graph representation of a natural language query, in which nodes are \nrecognized bibliographic named entities and links are relations of those entities. Graph relations denote \nrelations that are necessary for building complete graph queries that represent natural language queries. \nThus, graph relations are subsets of dependency relations, and graph relations are selected from \ndependency relations. Irrelevant relations (i.e., relations among non-bibliographic named entities) that are \nincluded in dependency relations are omitted in this process. The selection is performed by considering \nboth the patterns of queries and the database schema that is used to store bibliographic data.  \nFigure 2 shows the algorithm we use to select graph relations from dependency relations. We \nbuild the heuristics by combing the test results of a list of expected queries and the database schema. \nThus, the heuristics introduced here are dependent on the database schema we use and subject to change if \na different schema is employed (see Figure 1 in Zhu, Yan, and Song, 2016) for the schema we used in the \ngraph-based system).   \n \nFigure 2. The flow chart of selecting graph relations from dependency relations \n \n \nAs shown in Figure 2, a relation is selected as a graph relation if both the subject and the object of \nthe relation are named entities. “conj” denotes “conjunct”, and it is used if two tokens are connected by a \ncoordinating conjunction, such as “and” and “or”. In our case, the relations do not play constructive role \nin building a graph query, and is thus discarded. Accordingly, the third and fifth relations in Table 2 are \nselected as graph relations while the sixth relation is not. Table 3 shows another dependency relations of \nan example query “papers that were written by John”. \nTable 3. Dependency relations of the query “papers that were written by John” \nOrder \nSubject \nObject \nRelation Code \nRelation Name \n1 \n \npapers \nroot \nroot \n2 \nwritten \npapers \nnsubjpass \nnominal passive subject \n3 \npapers \nthat \nref \nreferent \n4 \nwritten \nwere \nauxpass \npassive auxiliary \n5 \npapers \nwritten \nacl:relcl \nrelative clause modifier \n6 \nJohn \nby \ncase \ncase marker \n7 \nwritten \nJohn \nnmod \nnmod_preposition \n \n \nTable 3 shows the case in which two bibliographic entities are not directly connected by a \ndependency relation. It is a normal use case and the algorithm can deal with such use cases. First, the fifth \nand seventh dependency relations are selected. Then, the subject of fifth relation “papers” and the object \nof seventh relation “John” are connected to form a new graph relation as shown in Figure 2. It is a \nrepeated pattern in bibliographic natural language queries that two relation types “acl”relcl” and “nmod” \nare used to connect two bibliographic named entities.  \n3.6. The formulation of graph queries \n \nIn this step, bibliographic named entities are converted into graph nodes, and graph relations are \nchecked for connectedness and direction. We also integrate a cited part and a citing part for queries that \ninvolve citations in this step. \n3.6.1. The conversion of bibliographic named entities to graph nodes \nThe conversion takes place in three steps. First, we identify the bibliographic named entity that a \nquery is asking. For example, in the query “papers that were written by John”, the answer node is \n“papers”. The identification of an answer node is to locate the object of a “root” relation in parsing results \n(e.g., “papers” in Table 3). Second, we assign each bibliographic named entity a unique instance name \nthat will be used when generating a graph query language. This allows us to differentiate bibliographic \nnamed entities with the same name and type the entity “papers” in the query “papers that were cited by \npapers that were written by John”. Lastly, we identify bibliographic named entities that constrain the \nanswer node. For example, “information retrieval” in the query “papers about information retrieval” \nconstrains the answer node “papers” by adding a condition. If the type of a bibliographic named entity \ndoes not contain the string “class_”, the named entity is a constraint node. This explains the reason that \nwe add the string “class_” to the values of five bibliographic types when constructing the dictionary. \nTable 4 shows instance names, answer nodes, and one or more constraint nodes in the query “papers that \nwere cited by papers that were written by John”. \nTable 4. Graph nodes in the query \"papers that were cited by papers that were written by John\" \nNamed Entity \nInstance \nAnswer Node \nConstraint Node \npapers \ncited_Class_Paper_1 \nYes \nNo \npapers \nciting_Class_Paper_2 \nNo \nNo \nJohn \nciting_Author_3 \nNo \nYes \n \nInformation shown in Table 4 is an important building block of a graph query language used to \nquery graph databases. It enables the construction of a graph query language by providing all necessary \ninformation of nodes in a bibliographic graph.    \n3.6.2. The check of connectedness and directions of graph relations \nConnectedness denotes whether two bibliographic named entities are directly connected in a \ndatabase schema. For example, two bibliographic named entities “papers” and “happy university” in the \nquery “papers by happy university” are not directly connected in the schema: “Paper” is connected to \n“Author” and “Author” is connected to “Organization”. Even though the parsing results suggest a \ndependency relation between the two bibliographic named entities, the dependency relation should not be \nselected as a graph relation because it does not conform to the database schema. Thus, we check every \ndependency relation and add required nodes and relations to form a complete set of graph relations \n(Figure 3). \n \nFigure 3: The check of connectedness and directions of the query \"papers by happy university\" \n \n \nAfter checking the connectedness of each graph relation and adding necessary new nodes and \nrelations, we check the direction of each graph relation to see whether the source and target of each graph \nrelation conforms to the database schema. In a graph query language, we need to provide a set of graph \nrelations with explicit definitions of sources and targets. For example, the relation between “Paper” and \n“Author” can be either modeled as “WRITES” or “IS_WRITEEN_ BY”, which have different directions. In \nthe above example, the graph relation (papers, Author) was converted into (Author, papers) based on the \nschema we used. \n3.6.3. The integration of cited and citing parts \n \nAs mentioned previously, we divide a query that involves citations into two parts to reduce the \ncomplexities in interpreting natural language queries. These two parts are parsed and converted into graph \nnodes and graph relations separately. To generate a single graph query, we need to integrate both nodes \nand relations from two parts. The integration of nodes is achieved by creating a new node set and moving \nall cited and citing graph nodes to the set. The integration of relations is achieved by connecting two \nbibliographic named entities with the type of “Paper” in cited and citing parts. If one or two parts do not \ninclude a bibliographic named entity with the type of “Paper”, we add a new graph node “Paper” to the \npart(s) and a graph relation that connects cited paper and citing paper. For example, the query “authors \ncited by John” denotes authors whose papers that were cited by papers written by John, but both the cited \nand citing part do not have a bibliographic named entity with the type of “Paper”. Figure 4 shows the way \nto handle such queries. \n \nFigure 4. The integration of cited and citing parts in the query \"authors cited by John\" \n \nAs shown in Figure 4, two “Paper” nodes are added to both cited and citing parts. The nodes are \nthen connected to the existing nodes “authors’ and “John”, respectively. Finally, two “Paper” nodes are \nconnected through a citation relation. \n3.7. The translation of a graph query into a graph query language \n \nIn this step, we translate a graph query into a graph query language. Widely used graph query \nlanguages such as Cypher, Germlin, and SPARQL have different syntaxes, but have the same building \nblocks, i.e., patterns, constraints, and return types. Because graph relations in a graph query are checked \nfor connectedness and directions, and thus conform to the database schema, they can be directly translated \ninto a graph query language. Constraints and return types are also available as we identify an answer node \nand constraint nodes in the previous step. Figure 5 shows how the graph query of a natural language \nquery “authors that were cited by John” is translated into a graph query language. Four graph nodes \nderived from four named entities (NE1, NE2, NE3, and NE4) and three relations (R1, R2, and R3) among \nthese graph nodes are identified. These nodes and relation are directly used to generate a graph query \nlanguage. \n \nFigure 5. The translation of the graph query \"authors that were cited by John\" into a graph query \nlanguage \n \n \nGraph relations are used to derive patterns (i.e., paths), and a constraint is derived from the \nconstraint node (i.e., citing_Author_4). The return type in a graph query language is the answer node (i.e., \ncited_Author_1) in the graph query. With these three building blocks, a query language can be generated.  \n3. 8. The query of a graph database \n \nThe generated query is submitted to a graph database to retrieve bibliographic data. Another \noption to query graph databases is to use embedded codes written in programming languages such as Java \nand C++, as graph databases provide application program interface (API) for data management. However, \nthis approach would reduce the compatibility of a system because graph databases have different APIs. \nThus, the framework is designed to translate a natural language query into a graph query language that is \nsupported by a number of graph databases (Holzschuher & Peinl, 2013). \n \n4. System Implementation and Experimental Results  \n4.1. System implementation \n \nA web-based system is implemented by adding a natural language querying layer to a graph-\nbased system (Zhu, Yan, & Song, 2016). It is based on the Spring Framework (as an application \nframework), Neo4j (as a graph database), and D3.js (as a visualization library). Figure 6 shows the \ngraphical interface for users to formulate natural language queries. The example query is “Papers about \nclassification, which were cited by Asoke K. Nandi 's papers that had been presented in Pattern \nRecognition”. \n \nFigure 6. A natural language interface with an example query \n \n \nAfter typing the natural language query and clicking the “Search” button, the system analyzes the \nnatural language query. Recognized bibliographic named entities, dependency relations of the query, \ngraph nodes, graph relations, and a graph query are shown in Figure 7. \n \nFigure 7. The analysis of a natural language query \n \n \nFigure 7 shows how the example query was analyzed. First, bibliographic named entities such as \nPapers, classification, Asoke K. Nandi, papers, and Pattern Recognition were recognized. As mentioned \npreviously, these bibliographic named entities were extracted from the dataset we used in the experiment \nand stored into a dictionary. Dependency relations among all tokens in the query are also shown as the \nresult of a syntactic analysis. Nodes were then obtained from bibliographic named entities while relations \nwere selected from dependency relations. By integrating graph nodes and graph relations, the system \ngenerated a graph query to visualize the results of the natural language query. As an interactive \ninformation retrieval system, users can modify or proceed with the current natural language query by \nreferencing the analysis of the graph query. The final search results are obtained by clicking the “Results” \nbutton (Figure 8). \n \nFigure 8. The search results of the example query \n \n \nFigure 8 shows the final search results, which are the correct answers for the example query. In \nthe system, we used Cypher as the graph query language, which is the default query language of Neo4j. \nSearch results showed that there are three entries that matched the natural language query. \n4.2. Experimental Results \nThe data used in the experiment were derived from a dataset provided by Tang and colleagues \n(2008). It contains 629,814 papers, 595,775 authors, 12,609 sources, 291,109 terms, and 1,000 \norganizations. Given the characteristics of a natural language interface, precision and recall are not \nsuitable metrics because they are 100%, if the natural language query is interpreted correctly, and 0%, \notherwise (Li & Jagadish, 2014). Thus, the effectiveness of a natural language interface to a relational \ndatabase or a knowledge base is evaluated as the ratio of correctly answered queries and query execution \ntime, as practiced in related research (e.g., Li & Jagadish, 2014; Tablan et al., 2008; Zhu, Yan, & Song, \n2016).  \nWe tested both the ratio of correctly answered queries and query execution time by forming four \ngroups of queries based on the number of bibliographic named entities in a query, which ranges from two-\nnamed entities to five-named entities. Ten queries for each group were tested. When formulating test \nqueries, we considered a variety of meta-paths and included as many meta-paths as possible. For example, \nfor two-node queries, we included meta-paths such as “Authorà Paper”, “Authorà Organization”, \n“Sourceà Paper”, “Paperà Term”, and “Paperà Paper”. As the number of named entities in a query \nincreases, the number of meta-paths also grows. Therefore, we selected 10 meta-paths that are \nrepresentative in bibliographic searching based on our domain knowledge. Forty tested queries are listed \nin the Appendix. The ratio of correctly answered queries for each group is shown in Table 5.  \nTable 5. The ratio of correctly answered queries \nThe number of named entities \n2 \n3 \n4 \n5 \nThe ratio of correctly answered queries \n10/10 \n10/10 \n9/10 \n10/10 \n \nAs shown in Table 5, we did not see a correlation between the number of named entities in a \nquery and the ratio of correctly answered queries. The example query that our framework processed \nincorrectly is “Authors who are affiliated with University007 and wrote Papers about clustering”. The \nreason of the misinterpretation is that the parser misidentified “wrote” as the root of the query, which \nshould be “authors”. Our framework performed 100% correctly for all other test queries.  \nQuery execution time includes the time of interpreting a natural language query (i.e., recognizing \nnamed entities and parsing) and the time of answering the query in a graph database. Time spent in \nformulating a query is not considered to leave out human factors and to focus on the performance of the \nsystem. The test environment is a laptop PC with a Windows 7 64-bit operating system, an Intel Core i5-\n3320M CPU, and 16GB RAM. The execution time for each query and average execution time in each \ngroup are shown in Figure 9. The query that was incorrectly interpreted was excluded from the \ncalculation. \n \nFigure 9. The query execution time of queries with the number of named entities from two to five \n \n \nQuery execution time is affected by the length of a query as well as the number of items in the \nsearch result that matched the query. A long natural language query need more time to be interpreted than \na short query as the time spent on recognizing named entities in the query and parsing the query increases. \nQuery execution time also increases if there are many items that matched the query. The average \nexecution time is 4.8 seconds for two-named entity query, 5.6 seconds for three-named entity query, 6.5 \nseconds for four-named entity query, and 7.8 seconds for five-named entity query.  The longest time \ntaken to process a query is about ten seconds. Nonetheless, an industry-scale systems use more powerful \nservers, we believe the execution time should be reduced in real-world use cases. \n \n5.  Conclusions \nIn this paper, we presented a natural language interface for searching bibliographic data.  We \ndesigned and implemented a framework of building a natural language interface to a graph-based \nbibliographic information retrieval system. The framework allows users to query bibliographic \ninformation by formulating and answering queries represented in natural language. An important step in \ninterpreting natural language queries is to recognize bibliographic named entities in natural language \nqueries. We identified relations among recognized bibliographic entities by parsing queries and finding \ndependency relations. We tested the framework using a large empirical dataset, and the experiment results \nshowed that the method correctly interpreted 39 out of 40 natural language queries with varied levels of \ncomplexities. \nThe contributions of this paper are as follows: first, to our knowledge, it is the first attempt to \npropose a natural language interface to graph-based bibliographic information retrieval; second, we have \nproposed a novel customized natural language processing framework that integrates a few original \nalgorithms/heuristics for interpreting and analyzing natural language bibliographic queries; and third, we \nhave shown that the proposed framework and natural language interface provide a practical solution to \nbuild real-world natural language interface-based bibliographic information retrieval systems.  \nOur natural language interface has several limitations. First, it is domain-dependent. Because \nbibliographic information retrieval is a specialized area, some customized rules and heuristics were \nintroduced in the framework to ensure higher performance, which might limit its applicability to other \ndomains. In addition, since there has been no prior study in this area, there lacked a baseline, which did \nnot allow us to compare the performance of our framework with previous studies. Instead, we created test \nqueries with varying lengths and complexities from scratch by considering a variety of bibliographic \ninformation needs. Thus, from the perspective of evaluations, our contribution lies in the building of a \nbaseline (i.e., a test dataset and benchmark scores) for future studies related to natural language-based \nbibliographic information retrieval. Another limitation of the study lies in the users’ vocabulary use and \nentity resolution. Because the proposed framework focuses on a broad scope of a complete system design, \nit does not strongly tackle the above issues. Methods used in query expansion could be one of useful \nsolutions for these issues, and we expect to address these issues in our future works. Lastly, we see the \nvalue of user centered evaluation and plan to evaluate the system with users in our future works. \nConcepts and procedures introduced in this paper can serve as a foundation and guideline for \nfuture studies that aim to improve bibliographic information retrieval by utilizing the power of natural \nlanguage. We expect natural language interfaces to retrieval systems would be significantly improved as \ntechnologies of natural language processing advances. With an ever-increasing volume of publications, a \nnatural language interface is a promising solution to cope with this and guide users toward a more \ninformed bibliographic information retrieval.  \n \nAcknowledgement \nThis project was made possible in part by the Institute of Museum and Library Services (Grant Award \nNumber: RE-07-15-0060-15), for the project titled “Building an entity-based research framework to \nenhance digital services on knowledge discovery and delivery”. \n \nReferences \nAbacha, A. B., & Zweigenbaum, P. (2015). MEANS: A medical question-answering system combining \nNLP techniques and semantic web technologies. Information Processing & Management, 51(5), 570-594. \ndoi:10.1016/j.ipm.2015.04.006 \nAghaei Chadegani, A., Salehi, H., Yunus, M. M., Farhadi, H., Fooladi, M., Farhadi, M., & Ale Ebrahim, \nN. (2013). A comparison between two main academic literature collections: Web of Science and Scopus \ndatabases. Asian Social Science, 9(5), 18-26. \nAndroutsopoulos, I., Ritchie, G. D., & Thanisch, P. (1995). Natural language interfaces to databases–an \nintroduction. Natural language engineering, 1(01), 29-81. \nAngles, R., & Gutierrez, C. (2008). Survey of graph database models. ACM Computing Surveys (CSUR), \n40(1), 1. \nBerners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic web. Scientific American, 284(5), 28-37. \nBikel, D. M., Miller, S., Schwartz, R., & Weischedel, R. (1997). Nymble: a high-performance learning \nname-finder. In Proceedings of the fifth conference on applied natural language processing (pp. 194-201). \nAssociation for Computational Linguistics. \nCafarella, M. J., & Etzioni, O. (2005, May). A search engine for natural language applications. In \nProceedings of the 14th international conference on World Wide Web (pp. 442-452). ACM. \nChieu, H. L., & Ng, H. T. (2002). Named entity recognition: a maximum entropy approach using global \ninformation. In Proceedings of the 19th international conference on Computational linguistics-Volume 1 \n(pp. 1-7). Association for Computational Linguistics. \nCimiano, P., Haase, P., Heizmann, J., Mantel, M., & Studer, R. (2008). Towards portable natural \nlanguage interfaces to knowledge bases – The case of the ORAKEL system. Data and Knowledge \nEngineering, 65(2), 325–354. \nCodd, E. F. (1970). A relational model of data for large shared data banks. Communications of the ACM, \n13(6), 377-387. \nCollins, M. (1997). Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th \nAnnual Meeting of the Association for Computational Linguistics and Eighth Conference of the European \nChapter of the Association for Computational Linguistics (pp. 16-23). Association for Computational \nLinguistics. \nDamljanovic, D., Agatonovic, M., & Cunningham, H. (2010). Natural language interfaces to ontologies: \nCombining syntactic analysis and ontology-based lookup through the user interaction. In The semantic \nweb: Research and applications (pp. 106-120). Springer Berlin Heidelberg. \nDerczynski, L., Maynard, D., Rizzo, G., Erp, M. v., Gorrell, G., Troncy, R., Bontcheva, K. (2015). \nAnalysis of named entity recognition and linking for tweets. Information Processing & Management, \n51(2), 32-49. doi:10.1016/j.ipm.2014.10.006 \nDe Marneffe, M. C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses \nfrom phrase structure parses. In Proceedings of LREC (Vol. 6, No. 2006, pp. 449-454). \nDoszkocs, T. E., & Rapp, B. A. (1979). Searching MEDLINE in english: A prototype user interface with \nnatural language query, ranked output, and relevance feedback. Information Choices and Policies: \nProceedings of the 1979 ASIS Annual Meeting, Volume 16, New York, Knowledge Industry Publications \nfor American Society for Information Science, 1979 131-139.s, \nEarley, J. (1970). An efficient context-free parsing algorithm. Communications of the ACM, 13(2), 94-\n102. \nFalagas, M. E., Pitsouni, E. I., Malietzis, G. A., & Pappas, G. (2008). Comparison of PubMed, Scopus, \nweb of science, and Google scholar: strengths and weaknesses. The FASEB journal, 22(2), 338-342. \nFazzinga, B., & Lukasiewicz, T. (2010). Semantic search on the web. Semantic Web, 1(1–2), 89–96. \nFersini, E., Messina, E., Felici, G., & Roth, D. (2014). Soft-constrained inference for named entity \nrecognition. Information Processing & Management, 50(5), 807-819. doi:10.1016/j.ipm.2014.04.005 \nFinkel, J. R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for \nComputational Linguistics (pp. 363-370). Association for Computational Linguistics. \nGusfield, D. (1997). Algorithms on strings, trees, and sequences: Computer science and computational \nbiology. New York; Cambridge [England];: Cambridge University Press. \nHabernal, I., & Konopík, M. (2013). SWSNL: Semantic web search using natural language. Expert \nSystems with Applications, 40(9), 3649-3664. \nHolzschuher, F., & Peinl, R. (2013). Performance of graph query languages: comparison of cypher, \ngremlin and native access in neo4j. In Proceedings of the Joint EDBT/ICDT 2013 Workshops (pp. 195-\n204). ACM. \nJacso, P. (2005). As we may search-Comparison of major features of the Web of Science, Scopus, and \nGoogle Scholar citation-based and citation-enhanced databases. CURRENT SCIENCE-BANGALORE-, \n89(9), 1537. \nKaufmann, E., & Bernstein, A. (2010). Evaluating the usability of natural language query languages and \ninterfaces to Semantic Web knowledge bases. Web Semantics: Science, Services and Agents on the \nWorld Wide Web, 8(4), 377-393. \nKaufmann, E., Bernstein, A., & Fischer, L. (2007). NLP-reduce: A ‘‘naïve’’ but domain independent \nnatural language interface for querying ontologies. In 4th European Semantic Web Conference (ESWC \n2007) (pp. 1–2). \nKlein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual \nMeeting on Association for Computational Linguistics-Volume 1 (pp. 423-430). Association for \nComputational Linguistics. \nKlein, D., & Manning, C. D. (2004). Corpus-based induction of syntactic structure: Models of \ndependency and constituency. In Proceedings of the 42nd Annual Meeting on Association for \nComputational Linguistics (p. 478). Association for Computational Linguistics. \nLi, F., & Jagadish, H. V. (2014). Constructing an interactive natural language interface for relational \ndatabases. Proceedings of the VLDB Endowment, 8(1), 73-84. \nMarcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building a large annotated corpus of \nEnglish: The Penn Treebank. Computational linguistics, 19(2), 313-330. \nMcCallum, A., & Li, W. (2003). Early results for named entity recognition with conditional random \nfields, feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural \nlanguage learning at HLT-NAACL 2003-Volume 4 (pp. 188-191). Association for Computational \nLinguistics. \nMiller, E. (1998). An introduction to the resource description framework. Bulletin of the American \nSociety for Information Science and Technology, 25(1), 15-19. \nMu, X., Lu, K., & Ryu, H. (2014). Explicitly integrating MeSH thesaurus help into health information \nretrieval systems: An empirical user study. Information Processing & Management, 50(1), 24-40. \ndoi:10.1016/j.ipm.2013.03.005 \nNadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae \nInvestigationes, 30(1), 3-3. doi:10.1075/li.30.1.03nad \nNoessner, J., Niepert, M., Meilicke, C., & Stuckenschmidt, H. (2010). Leveraging terminological \nstructure for object reconciliation. In The Semantic Web: Research and Applications (pp. 334-348). \nSpringer Berlin Heidelberg. \nPark, C., & Lim, S. (2015). Efficient processing of keyword queries over graph databases for finding \neffective answers. Information Processing & Management, 51(1), 42. \nRau, L. F. (1991). Extracting company names from text. In Artificial Intelligence Applications, 1991. \nProceedings. Seventh IEEE Conference on (Vol. 1, pp. 29-32). IEEE. \nRindflesch, T. C., Tanabe, L., Weinstein, J. N., & Hunter, L. (2000). EDGAR: extraction of drugs, genes \nand relations from the biomedical literature. In Pac Symp Biocomput (Vol. 5, pp. 514-25). \nRoy, S., & Zeng, W. (2013). Cognitive canonicalization of natural language queries using semantic strata. \nACM Transactions on Speech and Language Processing (TSLP), 10(4), 1-30. doi:10.1145/2539053 \nRyu, P., Jang, M., & Kim, H. (2014). Open domain question answering using wikipedia-based knowledge \nmodel. Information Processing & Management, 50(5), 683-692. doi:10.1016/j.ipm.2014.04.007 \nScore, S. C. (2009). Web of Science and Scopus: A comparative review of content and searching \ncapabilities. The Charleston Advisor. \nSun, Y., Yu, Y., & Han, J. (2009). Ranking-based clustering of heterogeneous information networks with \nstar network schema. Paper presented at the 797-806. doi:10.1145/1557019.1557107 \nTablan, V., Damljanovic, D., & Bontcheva, K. (2008). A natural language query interface to structured \ninformation. (pp. 361-375). Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/978-3-540-\n68234-9_28 \nTang, J., Zhang, J., Yao, L., Li, J., Zhang, L., & Su, Z. (2008). Arnetminer: extraction and mining of \nacademic social networks. In Proceedings of the 14th ACM SIGKDD international conference on \nKnowledge discovery and data mining (pp. 990-998). ACM \nTsuruoka, Y., & Tsujii, J. I. (2003). Boosting precision and recall of dictionary-based protein name \nrecognition. In Proceedings of the ACL 2003 workshop on Natural language processing in biomedicine-\nVolume 13 (pp. 41-48). Association for Computational Linguistics. \nTumer, D., Shah, M. A., & Bitirim, Y. (2009). An empirical evaluation on semantic search performance \nof keyword-based and semantic search engines: Google, Yahoo, MSN and Hakia. In Internet Monitoring \nand Protection, 2009. ICIMP'09. Fourth International Conference on (pp. 51-55). IEEE. \nVicknair, C., Macias, M., Zhao, Z., Nan, X., Chen, Y., & Wilkins, D. (2010). A comparison of a graph \ndatabase and a relational database: a data provenance perspective. In Proceedings of the 48th annual \nSoutheast regional conference (p. 42). ACM. \nWang, C., Xiong, M., Zhou, Q., & Yu, Y. (2007). Panto: A portable natural language interface to \nontologies. In The Semantic Web: Research and Applications (pp. 473-487). Springer Berlin Heidelberg. \nZhu, Y., Yan, E., & Song, I.-Y. (2016). The use of a graph-based system to improve bibliographic \ninformation retrieval: System design, implementation, and evaluation. Journal of the Association for \nInformation Science and Technology. doi: 10.1002/asi.23677 \n \nAppendix: Natural language queries tested in the experiment \n1. Papers by Gerard Salton  \n2. Michael Lawrence’s papers \n3. Papers that were written by Sangjun Lee \n4. Papers about ontology  \n5. Authors of Automatic text structuring experiments \n6. Papers that were cited by Energy-Aware and Time-Critical Geo-Routing in Wireless Sensor \nNetworks \n7. Terms of Opacity generalised to transition systems \n8. Organization of Johann Eder \n9. Sources that published The Effect of Faults on Network Expansions \n10. Papers that were published in Theoretical Computer Science \n11. Papers about classification and DNA \n12. Papers that were written by John R. Mick and published in ACM SIGMICRO Newsletter \n13. Papers cites papers that were written by Braham Barkat \n14. Papers about modulation which were published in Neural Networks \n15. Authors of University713 who wrote A control word model for detecting conflicts between \nmicrooperations \n16. Sources that published Zesheng Chen's papers \n17. Authors whose papers were published in AI Communications \n18. Authors who wrote papers that were about simulation \n19. Terms of Junghyun Nam's papers \n20. Organizations of authors of A New Quadtree Decomposition Reconstruction Methods \n21. Papers about survey, semantic, and retrieval \n22. Authors of papers that were cited by papers that were published in Decision Support Systems \n23. Papers that cite papers that were written by Rainer Engelke and published in Microsystem \nTechnologies \n24. Nina Yevtushenko’s papers that were cited by papers that were written by Sergey Buffalov \n25. Sources that published papers about genome and mining \n26. Terms of Rafae Bhatti’s papers that were published in Communications of the ACM \n27. Sources that published Tomasz Jurdzinski’s papers which are about automata \n28. Terms of papers that were written by authors at University123 \n29. Organizations of authors whose papers were published in Journal of Multivariate Analysis \n30. Authors who are affiliated with University007 and wrote papers about clustering \n31. Papers about classification, which were cited by Asoke K. Nandi 's papers that had been \npresented in Pattern Recognition \n32. Authors of papers that were cited by papers that were written by Changqiu Jin and published in \nJournal of Computational Physics \n33. Terms of papers that were cited by papers about kernel and regression \n34. Sources that published papers cited papers about middleware and embedded \n35. Organizations of authors whose papers were cited by papers that were published in Journal of \nRobotic Systems \n36. Organizations of authors who wrote paperson similarity and bayesian \n37. Papers about bayesian and electron which were written by authors at University170 \n38. Sources of papers, which were about eigenvalue and written by authors at University40 \n39. Authors at University899, who wrote papers that were about classifier, which were published in \nApplied Intelligence \n40. Terms of papers that were published in Cybernetics and Systems Analysis and written by authors \nat University362 \n \n",
  "categories": [
    "cs.IR",
    "cs.CL"
  ],
  "published": "2016-12-10",
  "updated": "2016-12-10"
}