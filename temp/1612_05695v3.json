{
  "id": "http://arxiv.org/abs/1612.05695v3",
  "title": "Reinforcement Learning Using Quantum Boltzmann Machines",
  "authors": [
    "Daniel Crawford",
    "Anna Levit",
    "Navid Ghadermarzy",
    "Jaspreet S. Oberoi",
    "Pooya Ronagh"
  ],
  "abstract": "We investigate whether quantum annealers with select chip layouts can\noutperform classical computers in reinforcement learning tasks. We associate a\ntransverse field Ising spin Hamiltonian with a layout of qubits similar to that\nof a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to\nnumerically simulate quantum sampling from this system. We design a\nreinforcement learning algorithm in which the set of visible nodes representing\nthe states and actions of an optimal policy are the first and last layers of\nthe deep network. In absence of a transverse field, our simulations show that\nDBMs are trained more effectively than restricted Boltzmann machines (RBM) with\nthe same number of nodes. We then develop a framework for training the network\nas a quantum Boltzmann machine (QBM) in the presence of a significant\ntransverse field for reinforcement learning. This method also outperforms the\nreinforcement learning method that uses RBMs.",
  "text": "REINFORCEMENT LEARNING\nUSING QUANTUM BOLTZMANN MACHINES\nDANIEL CRAWFORD, ANNA LEVIT, NAVID GHADERMARZY, JASPREET S. OBEROI,\nAND POOYA RONAGH\nAbstract. We investigate whether quantum annealers with select chip layouts can outperform\nclassical computers in reinforcement learning tasks.\nWe associate a transverse ﬁeld Ising spin\nHamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use\nsimulated quantum annealing (SQA) to numerically simulate quantum sampling from this system.\nWe design a reinforcement learning algorithm in which the set of visible nodes representing the\nstates and actions of an optimal policy are the ﬁrst and last layers of the deep network. In absence\nof a transverse ﬁeld, our simulations show that DBMs are trained more eﬀectively than restricted\nBoltzmann machines (RBM) with the same number of nodes. We then develop a framework for\ntraining the network as a quantum Boltzmann machine (QBM) in the presence of a signiﬁcant\ntransverse ﬁeld for reinforcement learning. This method also outperforms the reinforcement learning\nmethod that uses RBMs.\n1. Introduction\nRecent theoretical extensions of the quantum adiabatic theorem [1, 2, 3, 4, 5] suggest the pos-\nsibility of using quantum devices with manufactured spins [6, 7] as samplers of the instantaneous\nsteady states of quantum systems. With this motivation, we consider reinforcement learning as\nthe computational task of interest, and design a method of reinforcement learning consisting of\nsampling from a layout of quantum bits similar to that of a deep Boltzmann machine (DBM) (see\nFig. 1b for a graphical representation). We use simulated quantum annealing (SQA) to demonstrate\nthe advantage of reinforcement learning using deep Boltzmann machines and quantum Boltzmann\nmachines over their classical counterpart, for small problem instances.\nReinforcement learning ([8], known also as neuro-dynamic programming [9]) is an area of optimal\ncontrol theory at the intersection of approximate dynamic programming and machine learning. It\nhas been used successfully for many applications, in ﬁelds such as engineering [10, 11], sociology\n[12, 13], and economics [14, 15].\nIt is important to diﬀerentiate between reinforcement learning and common streams of research\nin machine learning. For instance, in supervised learning, the learning is facilitated by training\nsamples provided by a source external to the agent and the computer. In reinforcement learning,\nthe training samples are provided only by the interaction of the agent itself with the environment.\nFor example, in a motion planning problem in an uncharted territory, it is desired that the agent\nDate: January 7, 2019.\nKey words and phrases. Reinforcement learning, Machine learning, Neuro-dynamic programming, Markov decision\nprocess, Quantum Monte Carlo simulation, Simulated quantum annealing, Restricted Boltzmann machine, Deep\nBoltzmann machine, General Boltzmann machine, Quantum Boltzmann machine.\nPublished in: Quantum Information and Computation, Vol. 18, No. 1&2, pp. 0051–0074, Rinton Press (2018).\n1\narXiv:1612.05695v3  [quant-ph]  3 Jan 2019\n2\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\ns1\ns2\nsN\na1\na2\naM\n...\n...\n...\nhidden\nlayer\nvisible\nlayer\n(a)\ns1\ns2\nsN\n...\n...\n...\n...\n...\n...\n. . .\na1\na2\naM\nstate\nlayer\nhidden\nlayers\naction\nlayer\n(b)\nFigure 1. (a) The general RBM layout used in RBM-based reinforcement learning. The visible\nlayer on the left consists of state and action nodes, and is connected to the hidden layer, forming a\ncomplete bipartite graph. (b) The general DBM layout used in DBM-based reinforcement learning.\nThe visible nodes on the left represent states and the visible nodes on the right represent actions.\nThe training procedure captures the correlations between states and actions in the weights of the\nedges between the nodes.\nlearns in the fastest possible way to navigate correctly, with the fewest blind decisions required to be\nmade. This is known as the dilemma of exploration versus exploitation; that is, neither exploration\nnor exploitation can be pursued exclusively without facing a penalty or failing at the task. The\ngoal is hence not only to design an algorithm that eventually converges to an optimal policy, but\nfor it to be able to generate good policies early in the learning process. We refer the reader to [8,\nCh. 1.1] for a thorough introduction to use cases and problem scenarios addressed by reinforcement\nlearning.\nThe core idea in reinforcement learning is deﬁning an operator on the Banach space of real-\nvalued functions on the set of states of a system such that a ﬁxed point of the operator carries\ninformation about an optimal policy of actions for a ﬁnite or inﬁnite number of decision epochs.\nA numerical method for computing this ﬁxed point is to explore this function space by travelling\nin a direction that minimizes the distance between two consecutive applications of the contraction\nmapping operator [9].\nThis optimization task, called learning in the context of reinforcement learning, can be performed\nby locally parametrizing the above function space using a set of auxiliary variables, and applying\na gradient method to these variables. One approach for such a parametrization, due to [16], is to\nuse the weights of a restricted Boltzmann machine (RBM) (see Fig. 1a) as the parameters, and the\nfree energy of the RBM as an approximator for the elements in the function space. The descent\ndirection is then calculated in terms of the expected values of the nodes of the RBM.\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n3\nR\nW\nP\n(a)\nR\nW\nR\nR\nP\n(b)\n⟲\nW\n↑\n←\n←\n↑\n←\n←↑\n←↑\n←\n←\n↑\n↑\n←↑\n←↑\n(c)\n→\n0.05\n0.05\n0.8\n0.05\n0.05\n→\nW\n1/15\n1/15\n0.8\n1/15\n(d)\nFigure 2. (a) A 3 × 5 maze. W represents a wall, R is a positive real number representing a\nreward, and P is a real number representing a penalty. (b) The previous maze with two additional\nstochastic rewards. (c) The set of all optimal actions for each cell of the maze in Fig (a). An\noptimal traversal policy is a choice of any combination of these actions. (d) A sample conditional\nstate transition probability for a windy problem with no obstacles (left), and with a wall present\n(right).\nIt follows from the universal approximation theorem [17] that RBMs can approximate any joint\ndistribution over binary variables [18, 19].\nHowever, in the context of reinforcement learning,\nRBMs are not necessarily the best choice for approximating Q-functions relating to Markov decision\nprocesses because RBMs may require an exponential number of hidden variables with respect to\nthe number of visible variables in order to approximate the desired joint distribution [18, 19]. On\nthe other hand, DBMs have the potential to model higher-order dependencies than RBMs, and are\nmore robust than deep belief networks [20].\nOne may, therefore, consider replacing the RBM with other graphical models and investigating\nthe performance of the models in the learning process. Except in the case of RBMs, calculat-\ning statistical data from the nodes of a graphical model amounts to sampling from a Boltzmann\ndistribution, creating a bottleneck in the learning procedure. Therefore, any improvement in the\neﬃciency of Boltzmann distribution sampling is beneﬁcial for reinforcement learning and machine\nlearning in general.\nAs we explain in what follows, DBMs are good candidates for reinforcement learning tasks.\nMoreover, an important advantage of a DBM layout for a quantum annealing system is that the\nproximity and couplings of the qubits in the layout are similar to those of a sequence of bipartite\nblocks in D-Wave Systems’ devices [21], and it is therefore feasible that such layouts could be\nmanufactured in the near future. In addition, embedding Boltzmann machines in larger quantum\nannealer architectures is problematic when excessively large weights and biases are needed to em-\nulate logical nodes of the Boltzmann machine using chains and clusters of physical qubits. These\n4\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\nare the reasons why, instead of attempting to embed a Boltzmann machine structure on an existing\nquantum annealing system as in [22, 23, 24, 25], we work under the assumption that the network\nitself is the native connectivity graph of a near-future quantum annealer, and, using numerical\nsimulations, we attempt to understand its applicability to reinforcement learning.\nWe also refer the reader to current trends in machine learning using quantum circuits, speciﬁcally,\n[26] and [27] for reinforcement learning, and [28] and [29] for training quantum Boltzmann machines\nwith applications in deep learning and tomography. To the best of our knowledge, the present paper\ncomplements the literature on quantum machine learning as the ﬁrst proposal on reinforcement\nlearning using adiabatic quantum computation.\nQuantum Monte Carlo (QMC) numerical simulations have been found to be useful in simulat-\ning time-dependant quantum systems. Simulated quantum annealing (SQA) [30, 31], one of the\nmany ﬂavours of QMC methods, is based on the Suzuki–Trotter expansion of the path integral\nrepresentation of the Hamiltonian of Ising spin models in the presence of a transverse ﬁeld driver\nHamiltonian. Even though the eﬃciency of SQA for ﬁnding the ground state of an Ising model is\ntopologically obstructed [32], we consider the samples generated by SQA to be good approxima-\ntions of the Boltzmann distribution of the quantum Hamiltonian [33]. Experimental studies have\nshown similarities in the behaviour of SQA and that of quantum annealing [34, 35] and its physical\nrealization by D-Wave Systems [36, 37].\nWe expect that when SQA is set such that the ﬁnal strength of the transverse ﬁeld is negligible,\nthe distribution of the samples approaches the classical limit one expects to observe in absence\nof the transverse ﬁeld.\nAnother classical algorithm which can be used to obtain samples from\nthe Boltzmann distribution is conventional simulated annealing (SA), which is based on thermal\nannealing. Note that this algorithm can be used to create Boltzmann distributions from the Ising\nspin model only in the absence of a transverse ﬁeld. It should, therefore, be possible to use SA or\nSQA to approximate the Boltzmann distribution of a classical Boltzmann machine. However, unlike\nin the case of SA, it is possible to use SQA not only to approximate the Boltzmann distribution\nof a classical Boltzmann machine, but also that of a graphical model in which the energy operator\nis a quantum Hamiltonian in the presence of a transverse ﬁeld. These graphical models, called\nquantum Boltzmann machines (QBM), were ﬁrst introduced in [38].\nWe use SQA simulations to provide evidence that a quantum annealing device that approximates\nthe distribution of a DBM or a QBM may improve the learning process compared to a reinforcement\nlearning method that uses classical RBM techniques. Other studies have shown that SQA is more\neﬃcient than thermal SA [30, 31]. Therefore, our method, used in conjunction with SQA, can also\nbe viewed as a quantum-inspired approach for reinforcement learning.\nWhat distinguishes our work from current trends in quantum machine learning is that (i) we\nconsider the use of quantum annealing in reinforcement learning applications rather than frequently\nstudied classiﬁcation or recognition problems; (ii) using SQA-based numerical simulations, we as-\nsume that the connectivity graph of a DBM directly maps to the native layout of a feasible quantum\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n5\nannealer; and (iii) the results of our experiments using SQA to simulate the sampling of an entan-\ngled system of spins suggest that using quantum annealers in reinforcement learning tasks can oﬀer\nan advantage over thermal sampling.\n2. Preliminaries\n2.1. Adiabatic Evolution of Open Quantum Systems. The evolution of a quantum system\nunder a slowly changing time-dependent Hamiltonian is characterized by the quantum adiabatic\ntheorem (QAT). QAT has a long history going back to the work of Born and Fock [39]. Colloquially,\nQAT states that a system remains close to its instantaneous steady state provided there is a gap\nbetween the eigenenergy of the steady state and the rest of the Hamiltonian’s spectrum at every\npoint in time if the evolution is suﬃciently slow. This result motivated [40] and [41] to introduce the\nclosely related paradigms of quantum computing known as quantum annealing (QA) and adiabatic\nquantum computation (AQC).\nQA and AQC, in turn, inspired eﬀorts in the manufacturing of physical realizations of adia-\nbatic evolution via quantum hardware ([6]). In reality, the manufactured chips operate at nonzero\ntemperature and are not isolated from their environment. Therefore, the existing adiabatic theory\ndid not describe the behaviour of these machines. A contemporary investigation in quantum adi-\nabatic theory was thus initiated to study adiabaticity in open quantum systems ([1, 2, 3, 4, 5]).\nThese references prove adiabatic theorems to various degrees of generality and under a variety of\nassumptions about the system.\nIn fact, [2] develops an adiabatic theory for equations of the form\n(1)\nε ˙x(s) = L(s)x(s),\nwhere L is a family of linear operators on a Banach space and L(s) is a generator of a contrac-\ntion semigroup for every s. This provides a general framework that encompasses many adiabatic\ntheorems, including that of classical stochastic systems, all the way to quantum evolutions of open\nsystems generated by Lindbladians. The manifold of instantaneous stationary states is identical\nto ker(L(s)), and [2] shows that the dynamics of the system are parallel-transported along this\nmanifold as ε →0.\nAn example of (1) is the case in which the Banach space is the space of bounded operators on a\nHilbert space, and in this case we study the evolution of the density matrix ρ of a quantum system.\nThe Lindbladian is deﬁned via the adjoint action of a Hermitian H on ρ, and couplings to the heat\nbath are represented via a family of operators Γα with P\nα Γ∗\nαΓα being bounded:\nLρ = −i[H, ρ] + 1\n2\nX\nα\n([Γαρ, Γ∗\nα] + [Γα, ρΓ∗\nα]).\nIn the work of [2], it was then proven that ρ(s) is parallel-transported along ker(L(s)), and that if\nL:\n(i) is the generator of a contraction semigroup;\n(ii) has closed and complementary range and kernel;\n6\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\n(iii) is Ck with respect to s; and\n(iv) is constant near the endpoints s = 0 and s = 1;\nthen the solution to (1) with initial condition in ker(L(0)) deviates only in O(εk) from ker(L(1))\nat s = 1.\nThe authors of [5] focuses on estimating the adiabatic error in terms of the physical parameters\nof the theory. In particular, they study the case of a quantum system coupled to a thermal bath\nsatisfying the Kubo–Martin–Schwinger (KMS) condition. Given a distance δ, in order for the norm\nof the solution of (1) to stay δ-close to the instantaneous steady state of the system at s = 1, they\nshow that ε has to decrease at a rate of O(λ2), where λ denotes the smallest nonzero eigenvalue in\nL. Note that the KMS condition implies that the Gibbs state exp(−βH(s))/ tr[exp(−βH(s))] is,\nin fact, in ker(L(s)).\nThis stream of research suggests promising opportunities to use quantum annealers to sample\nfrom the Gibbs state of a quantum Hamiltonian using adiabatic evolution.\nIn this paper, the\ntransverse ﬁeld Ising model (TFIM) has been the centre of attention. In practice, due to additional\ncomplications in quantum annealing (e.g., level crossings and gap closure), the samples gathered\nfrom the quantum annealer are far from the Gibbs state of the ﬁnal Hamiltonian. In fact, [42]\nsuggests that the distribution of the samples would correspond more closely to an instantaneous\nHamiltonian at an intermediate point in time, called the freeze-out point. Therefore, our goal is\nto investigate the applicability of sampling from a TFIM with signiﬁcant Γ to free energy–based\nreinforcement learning.\n2.2. Simulated Quantum Annealing. Simulated quantum annealing (SQA) methods are a class\nof quantum-inspired algorithms that perform discrete optimization by classically simulating quan-\ntum tunnelling phenomena (see [43, p. 422] for an introduction). The algorithm used in this paper\nis a single spin-ﬂip version of quantum Monte Carlo numerical simulation based on the Suzuki–\nTrotter formula, and uses the Metropolis acceptance probabilities. The SQA algorithm simulates\nthe quantum annealing phenomena of an Ising spin model with a transverse ﬁeld, that is,\n(2)\nH(t) = −\nX\ni,j\nJijσz\ni σz\nj −\nX\ni\nhiσz\ni −Γ(t)\nX\ni\nσx\ni ,\nwhere σz and σx represent the Pauli z- and x-matrices, respectively, the indices i and j range over\nthe sites of the system, and the time t ranges from 0 to 1. In this quantum evolution, the strength\nof the transverse ﬁeld is slowly reduced to zero at ﬁnite temperature. In our implementations, we\nhave used a linear transverse ﬁeld schedule for the SQA algorithm as in [31] and [44]. Based on the\nSuzuki–Trotter formula, the key idea of this algorithm is to approximate the partition function of\nthe Ising model with a transverse ﬁeld as a partition function of a classical Hamiltonian denoted\nby Heﬀ, corresponding to a classical Ising model of one dimension higher. More precisely,\n(3)\nHeﬀ(σ) = −\nX\ni,j\nr\nX\nk=1\nJij\nr σikσjk −J+ X\ni\nr\nX\nk=1\nσikσi,k+1 −\nX\ni\nr\nX\nk=1\nhi\nr σik ,\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n7\nwhere r is the number of replicas, J+ =\n1\n2β log coth\n\u0010\nΓβ\nr\n\u0011\n, and σik represent spins of the classical\nsystem of one dimension higher.\nIn our experiments, the strength Γ of the transverse ﬁeld is scheduled to linearly decrease from\n20.00 to one of Γf = 0.01 or 2.00. The inverse temperature β is set to the constant 2.00. The initial\nvalue, 20.00, of the transverse ﬁeld is empirically chosen to be well above the coupling strengths\ncreated during the training. Each spin is replicated 25 times to represent the Trotter slices in the\nextra dimension. The simulation is set to iterate over all replications of all spins one time per\nsweep, and the number of sweeps is set to 300, which appears to be large enough for the sizes of\nIsing models constructed during our experiments. For each instance of input, the SQA algorithm\nis run 150 times. After termination, the conﬁguration of each replica, as well as the conﬁguration\nof the entire classical Ising model of one dimension higher, is returned.\nAlthough the SQA algorithm does not follow the dynamics of a physical quantum annealer explic-\nitly, it is used to simulate this process, as it captures major quantum phenomena such as tunnelling\nand entanglement [34]. In [34], for example, it is shown that quantum Monte Carlo simulations can\nbe used to understand the tunnelling behaviour in quantum annealers. As mentioned previously,\nit readily follows from the results of [33] that the limiting distribution of SQA is the Boltzmann\ndistribution of Heﬀ. This makes SQA a candidate classical algorithm for sampling from Boltzmann\ndistributions of classical and quantum Hamiltonians. The former is achieved by setting Γf ≃0,\nand the latter by constructing an eﬀective Hamiltonian of the system of one dimension higher, rep-\nresenting the quantum Hamiltonian with non-negligible Γf. Alternatively, a classical Monte Carlo\nsimulation used to sample from the Boltzmann distribution of the classical Ising Hamiltonian is the\nSA algorithm, based on thermal ﬂuctuations of classical spin systems.\n2.3. Markov Decision Process. The stochastic control problem of interest to us is a Markov\ndecision process (MDP), deﬁned as having:\n(i) ﬁnite sets of states S and actions A;\n(ii) a controlled Markov chain [45], deﬁned by a transition kernel P(s′ ∈S|s ∈S, a ∈A);\n(iii) a real-valued function r : S × A →R, known as the immediate reward structure; and\n(iv) a constant γ ∈[0, 1), known as the discount factor.\nA function π : S →A is called a stationary policy; that is, it is a choice of action π(s) for every\nstate s independent of the point in time that the controlled process reaches s. The application of a\nstationary policy π reduces the MDP into a time-homogeneous Markov chain Π, with a transition\nprobability P(s′|s, π(s)). The random process Π with initial condition Π0 = s we denote by Πs.\nOur Markov decision problem is to ﬁnd\nπ∗(s) = argmax\nπ\nV (π, s),\n(4)\nWhen both S and A are ﬁnite, the MDP is said to be ﬁnite.\nThe transition kernel does not need to be time-homogeneous; however, this deﬁnition suﬃces for the purposes of\nthis work.\nFor more-general statements, see [45].\n8\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\nwhere\nV (π, s) = E\n\" ∞\nX\ni=0\nγi r(Πs\ni, π(Πs\ni))\n#\n.\n(5)\n2.3.1. Maze Traversal as a Markov Decision Process. Maze traversal is a problem typically used\nto develop and benchmark reinforcement learning algorithms [46]. A maze is structured as a two-\ndimensional grid of r rows and c columns in which a decision-making agent is free to move up,\ndown, left, or right, or to stand still. During the maze traversal, the agent encounters obstacles\n(e.g., walls), rewards (e.g., goals), and penalties (negative rewards, e.g., a pit). Each cell of the\nmaze can contain either a deterministic or stochastic reward, a wall, a pit, or a neutral value.\nFig. 2a and Fig. 2b show examples of two mazes. Fig. 2c shows the corresponding solutions to the\nmaze in Fig. 2a.\nThe goal of the reinforcement learning algorithm in the maze traversal problem is for the agent\nto learn the optimal action to take in each cell of the maze by maximizing the total reward, that is,\nﬁnding a route across the maze that avoids walls and pits while favouring rewards. This problem\ncan be modelled as an MDP determined by the following components:\n• The state of the system is the agent’s position within the maze. The position state s takes values\nin the set of states\nS = {1, ..., r} × {1, ..., c}.\n• In any state, the agent can decide to take one of the ﬁve actions\na ∈{↑, ↓, ←, →, ⟲}.\nThese actions will guide the agent through the maze. An action that would lead the agent into\na wall (W) or outside of the maze boundary is treated as an inadmissible action. Each action\ncan be viewed as an endomorphism on the set of states\na : S →S.\nIf a = ⟲, then a(s) = s; otherwise, a(s) is the state adjacent to S in the direction shown by a.\nWe do not consider training samples where a is inadmissible.\n• The transition kernel determines the probability of the agent moving from one state to another\ngiven a particular choice of action. In the simplest case, the probability of transition from s to\na(s) is one:\nP(a(s)|s, a) = 1.\nWe call the maze clear if the associated transition kernel is as above, as opposed to the windy\nmaze, in which there is a nonzero probability that if the action a is taken at state s, the next\nstate will diﬀer from a(s).\n• The immediate reward r(s, a) that the agent gains from taking an action a in state s is the value\ncontained in the destination state. Moving into a cell containing a reward returns the favourable\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n9\nvalue R, moving into a cell containing a penalty returns the unfavourable value P, and moving\ninto a cell with no reward returns a neutral value in the interval (P, R).\n• A discount factor for future rewards is a non-negative constant γ < 1. In our experiments, this\ndiscount factor is set to γ = 0.8. The discount factor is a feature of the problem rather than\na free parameter of an implementation. For example, in a ﬁnancial application scenario, the\ndiscount factor might be a function of the risk-free interest rate.\nThe immediate reward for moving into a cell with a stochastic reward is given by a random\nvariable R. If an agent has prior knowledge of this distribution, then it should be able to treat the\ncell as one with a deterministic reward value of E[R]. This allows us to ﬁnd the set of all optimal\npolicies in each maze instance. This policy information is denoted by α∗: S →2A, associating with\neach state s ∈S a set of optimal actions α∗(s) ⊆A.\nIn our maze model, the neutral value is set to 100, the reward R = 200, and the penalty P = 0.\nIn our experiments, the stochastic reward R is simulated by drawing a sample from the Bernoulli\ndistribution 200 Ber(0.5); hence, it has the expected value E[R] = 100, which is identical to the\nneutral value. Therefore, the solutions depicted in Fig. 2c are solutions to the maze of Fig. 2b as\nwell.\n2.4. Value Iteration. Bellman [47] writes V (π, s) recursively in the following manner using the\nmonotone convergence theorem:\nV (π, s) = E\n\" ∞\nX\ni=0\nγi r(Πs\ni, π(Πs\ni))\n#\n= E[r(Πs\n0, π(Πs\n0))] + γ E\n\" ∞\nX\ni=0\nγi r(Πs\ni+1, π(Πs\ni+1))\n#\n= E[r(s, π(s))] + γ\nX\ns′∈S\nP(s′|s, π(s)) V (π, s′) .\nIn particular, it leads to the Bellman optimality equation:\n(6)\nV ∗(s) = V (π∗, s) = max\na\n \nE[r(s, a)] + γ\nX\ns′∈S\nP(s′|s, a) V ∗(s′)\n!\n.\nHence, V ∗is a ﬁxed point for the operator\nTV (f) : s 7→max\na\n\u0012\nE[r(s, a)] + γ\nZ\nf\n\u0013\non the space L∞(S) of bounded functions S →R endowed with the max norm. Here, the integral\nis taken with respect to the probability measure on S, induced by the conditional probability\ndistribution P(s′|s, a). It is easy to check that TV is a contraction mapping, and thus V ∗is the\nunique ﬁxed point of TV and the uniform limit of any sequence of functions {T n\nV f}n. Numerical\ncomputation of this limit using (6), called value iteration, is a common method of solving the\nMarkov decision problem (4). However, even the ε-optimal algorithms for this approach depend\n10\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\nheavily on the cardinality of both S and A, and suﬀer from the curse of dimensionality [47, 48].\nMoreover, the value iteration method requires having full knowledge of the transition probabilities,\nas well as the distribution of the immediate rewards.\n2.5. Q-functions. For a stationary policy π, the Q-function (also known as the action–value func-\ntion) is deﬁned as a mapping of a pair (s, a) to the expected value of the reward of the Markov\nchain that begins with taking action a at initial state s and continuing according to π [8]:\nQ(π, s, a) = E[r(s, a)] + E\n\" ∞\nX\ni=1\nγi r(Πs\ni, π(Πs\ni))\n#\n.\nIt is straightforward to check that\nV (π∗, s) = max\na\nQ(π∗, s, a),\nand for Q∗(s, a) = maxπ Q(π, s, a) = Q(π∗, s, a), the optimal policy for the MDP can be retrieved\nvia the following:\nπ∗(s) = argmax\na\nQ∗(s, a).\n(7)\nThis reduces the Markov decision problem to computing Q∗(s, a). The Bellman optimality equation\nfor Q∗(s, a) is\nQ∗(s, a) = E[r(s, a)] + γ\nX\ns′\nP(s′|s, a) max\na′\nQ∗(s′, a′),\nwhich makes Q∗the ﬁxed point of a diﬀerent operator\nTQ(f) : (s, a) 7→E[r(s, a)] + γ\nZ\nmax\na′\nf\ndeﬁned on L∞(S × A).\n2.6. Temporal-Diﬀerence Gradient Descent. In this section, we derive the Q-learning method\nfor MDPs. From the previous section, we know that starting from an initial Q0 : S × A →R, the\nsequence {Qn = T n\nQQ0} converges to Q∗. The diﬀerence\n(8)\nQn+1(s, a) −Qn(s, a) = E[r(s, a)] + γ\nX\ns′\nP(s′|s, a) max\na\nQn(s′, a)\n|\n{z\n}\n(∗)\n−Qn(s, a)\nis called the temporal diﬀerence of Q-functions, and is denoted by ETD.\nEmploying a gradient approach to ﬁnd the ﬁxed point of T on L∞(S × A) involves locally\nparametrizing the functions in this space by a vector of parameters θ, that is,\nQ(s, a) = Q(s, a; θ),\nand travelling in the direction that minimizes ∥ETD∥2:\n∆θ ∝−ETD∇θETD .\n(9)\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n11\nThe method TD(0) consists of treating the (∗) in (8) as constant with respect to the parameter-\nization θ, in which case we may write\n∆θ ∝∼ETD(s, a)∇θQ(s, a; θ).\nFor an agent agnostic with respect to the transition kernel or the distribution of the reward\nr(s, a), or both, this update rule for θ is not possible. The alternative is to substitute, at each\niteration, the expected value\nX\ns′\nP(s′|s, a) max\na\nQn(sn+1, a)\nby maxa Qn(sn+1, a), where sn+1 is drawn from the probability distribution P(s′|s, a), and substitute\nE[r(sn, an)] by a sample of r(sn, an). This leads to a successful Monte Carlo training method called\nQ-learning.\nIn what follows, we explain the case in which θ comprises the weights of a Boltzmann machine.\nLet us begin by introducing clamped Boltzmann machines, which are of particular importance in\nthe case of reinforcement learning.\n2.7. Clamped Boltzmann Machines. A classical Boltzmann machine is a type of stochastic\nneural network with two sets V and H of visible and hidden nodes, respectively. Both visible and\nhidden nodes represent binary random variables. We use the same notation for a node and the\nbinary random variable it represents. The interactions between the variables represented by their\nrespective nodes are speciﬁed by real-valued weighted edges of the underlying undirected graph. A\nGBM, as opposed to models such as RBMs and DBMs, allows weights between any two nodes.\nThe energy of the classical Boltzmann machine is\n(10)\nE (v, h) = −\nX\nv∈V, h∈H\nwvhvh −\nX\n{v,v′}⊆V\nwvv′vv′ −\nX\n{h,h′}⊆H\nwhh′hh′,\nwith wvh, wvv′, and whh′ denoting the weights between visible and hidden, visible and visible, and\nhidden and hidden nodes of the Boltzmann machine, respectively, deﬁned as a function of binary\nvectors v and h corresponding to the visible and hidden variables, respectively.\nA clamped GBM is a neural network whose underlying graph is the subgraph obtained by\nremoving the visible nodes for which the eﬀect of a ﬁxed assignment v of the visible binary variables\ncontributes as constant coeﬃcients to the associated energy\n(11)\nEv(h) = −\nX\nv∈V, h∈H\nwvhvh −\nX\n{v,v′}⊆V\nwvv′vv′ −\nX\n{h,h′}⊆H\nwhh′hh′ .\nA clamped quantum Boltzmann machine (QBM) has the same underlying graph as a clamped\nGBM, but instead of a binary random variable, a qubit is associated to each node of the network.\nThe energy function is substituted by the quantum Hamiltonian\n(12)\nHv = −\nX\nv∈V, h∈H\nwvhvσz\nh −\nX\n{v,v′}⊆V\nwvv′vv′ −\nX\n{h,h′}⊆H\nwhh′σz\nhσz\nh′ −Γ\nX\nh∈H\nσx\nh ,\n12\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\nwhere σz\nh represent the Pauli z-matrices and σx\nh represent the Pauli x-matrices. Thus, a clamped\nQBM with Γ = 0 is equivalent to a clamped classical Boltzmann machine. This is because Hv\nis a diagonal matrix in the σz-basis, the spectrum of which is identical to the range of Ev. The\nremainder of this section is formulated for the clamped QBMs, acknowledging that it can easily be\nspecialized to clamped classical Boltzmann machines.\nLet β =\n1\nkBT be a ﬁxed thermodynamic beta. For an assignment of visible variables v, F(v)\ndenotes the equilibrium free energy, and is deﬁned as\nF(v) := −1\nβ ln Zv = ⟨Hv⟩+ 1\nβ tr(ρv ln ρv) .\n(13)\nHere, Zv = tr(e−βHv) is the partition function of the clamped QBM and ρv is the density matrix\nρv =\n1\nZv e−βHv. The term −tr(ρv ln ρv) is the entropy of the system. The notation ⟨· · · ⟩is used\nfor the expected value of any observable with respect to the Gibbs measure, in particular,\n⟨Hv⟩= 1\nZv\ntr(Hve−βHv).\n2.8. Reinforcement Learning Using Clamped Boltzmann Machines. In this section, we\nexplain how a general Boltzmann machine (GBM) can be used to provide a Q-function approximator\nin a Q-learning method. To the best of our knowledge, this derivation has not been previously given,\nalthough it can be readily derived from the ideas presented in [16] and [38]. Following [16], the goal\nis to use the negative free energy of a Boltzmann machine to approximate the Q-function through\nthe relationship\nQ(s, a) ≈−F(s, a) = −F(s, a; θ)\nfor each admissible state–action pair (s, a) ∈S × A. Here, s and a are binary vectors encoding the\nstate s and action a on the state nodes and action nodes, respectively, of the Boltzmann machine.\nIn reinforcement learning, the visible nodes of the GBM are partitioned into two subsets of state\nnodes S and action nodes A.\nThe parameters θ, to be trained according to a TD(0) update rule (see Sec. 2.6), are the weights\nin a Boltzmann machine. For every weight w, the update rule is\n∆w = −ε(rn(sn, an) + γ max\na\nQ(sn+1, a) −Q(sn, an))∂F\n∂w .\nFrom (13), we obtain\n∂F(s, a)\n∂w\n= −\n1\nβZs,a\n∂\n∂w tr\n\u0010\ne−βHs,a\u0011\n=\n1\nβZs,a\ntr\n\u0012\nβe−βHs,a ∂\n∂wHs,a\n\u0013\n=\n\u001c ∂\n∂wHs,a\n\u001d\n.\nTherefore, the update rule for TD(0) for the clamped QBM can be rewritten as\n(14)\n∆wvh = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))v⟨σz\nh⟩\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n13\nand\n(15)\n∆whh′ = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))⟨σz\nhσz\nh′⟩,\nwhere the thermodynamic beta is absorbed into the learning rate ε, and\nan+1 = argmax\na\nQ(sn+1, a).\nHere, h and h′ denote two distinct hidden nodes and (by a slight abuse of notation) the letter v\nstands for a visible (state or action) node, and also the value of the variable associated to that\nnode.\nTo approximate the right-hand side of each of (14) and (15), we use SQA experiments. By [49,\nTheorem 6], we may ﬁnd the expected values of the observables ⟨σz\nh⟩and ⟨σz\nhσz\nh′⟩by averaging\nthe corresponding spins in the classical Ising model of one dimension higher used in SQA. To\napproximate the Q-function, we take advantage of [49, Theorem 4] and use (13) applied to this\nclassical Ising model. More precisely, let Heﬀ\nv represent the Hamiltonian of the classical Ising model\nof one dimension higher and the associated energy function E eﬀ\nv . The free energy of this model can\nbe written\nF(v) = ⟨Heﬀ\nv ⟩+ 1\nβ\nX\nc\nP(c|v) log P(c|v) ,\n(16)\nwhere c ranges over all spin conﬁgurations of the classical Ising model of one dimension higher.\nThe above argument holds in the absence of the transverse ﬁeld, that is, for the classical Boltz-\nmann machine. In this case, the TD(0) update rule is given by\n(17)\n∆wvh = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))v⟨h⟩\nand\n(18)\n∆whh′ = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))⟨hh′⟩,\nwhere ⟨h⟩(referred to as activations of the hidden nodes in machine learning terminology) and\n⟨hh′⟩are the expected values of the variables and the product of variables, respectively, in the\nbinary encoding of the hidden nodes with respect to the Boltzmann distribution given by P(h|v) =\nexp(−βEv(h))/P\nh′ exp(−βEv(h′)). Therefore, they may be approximated using SA or SQA when\nΓ →0.\nThe values of the Q-functions in (17) and (18) can also be approximated empirically, since, in a\nclassical Boltzmann machine,\nF(v) =\nX\nh\nP(h|v)Ev(h) + 1\nβ\nX\nh\nP(h|v) log P(h|v)\n(19)\n= −\nX\ns∈S\nh∈H\nwshs⟨h⟩−\nX\na∈A\nh∈H\nwaha⟨h⟩−\nX\n{h,h′}⊆H\nuhh′⟨hh′⟩+ 1\nβ\nX\nh\nP(h|s, a) log P(h|s, a).\n14\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\nAlgorithm 1\nRBM-RL\n1: initialize weights of RBM\n2: for all training samples (s1, a1) do\n3:\ns2 ←a1(s1), a2 ←argmaxa Q(s2, a)\n4:\ncalculate ⟨hi⟩for (i = 1, 2) using (21)\n5:\ncalculate F(si, ai) for (i = 1, 2) using (20)\n6:\nQ(si, ai) ←−F(si, ai) for (i = 1, 2)\n7:\nupdate RBM weights using (22) and (23)\n8:\nπ(s1) ←argmaxa Q(s1, a)\n9: return π\nRemark 2.1. In the case of an RBM, Sallans and Hinton [16] show that the free energy is given\nby\n(20)\n−F(s, a) =\nX\ns∈S\nh∈H\nwshs⟨h⟩+\nX\na∈A\nh∈H\nwaha⟨h⟩−1\nβ\nX\nh∈H\n[⟨h⟩log⟨h⟩+ (1 −⟨h⟩) log(1 −⟨h⟩)] .\nThe update rule for the weights of the RBM is (17) alone. Moreover, in the case of RBMs, the\nequilibrium free energy F(s, a) and its derivatives with respect to the weights can be calculated\nwithout the need for Boltzmann distribution sampling, according to the closed formula\n⟨h⟩= P(σh = 1|s, a) = σ\n X\ns∈S\nwshs +\nX\na∈A\nwaha\n!\n(21)\n=\n(\n1 + exp\n \n−\nX\ns∈S\nwshs −\nX\na∈A\nwaha\n!)−1\n.\nHere, σ denotes the sigmoid function. Note that, in the general case, since the hidden nodes of a\nclamped Boltzmann machine are not independent, the calculation of the free energy is intractable.\n3. Algorithms\nIn this section, we present the details of classical reinforcement learning using RBM, a semi-\nclassical approach based on a DBM (using SA and SQA), and a quantum reinforcement learning\napproach (using SQA or quantum annealing). All of the algorithms are based on the Q-learning\nTD(0) method presented in the previous section. Pseudo-code for these methods is provided in\nAlgorithms 1, 2, and 3 below.\n3.1. Reinforcement Learning Using RBMs. The RBM reinforcement learning algorithm is\ndue to Sallans and Hinton [16]. This algorithm uses the update rule (17), with v representing state\nor action encoding, to update the weights of an RBM, and (21) to calculate the expected values\nof random variables associated with the hidden nodes ⟨h⟩. As explained in Sec. 2.8, the main\nadvantage of RBM is that it has explicit formulas for the hidden-node activations, given the values\nof the visible nodes. Moreover, only for RBMs can the entropy portion of the free energy (19) be\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n15\nAlgorithm 2\nDBM-RL\n1: initialize weights of DBM\n2: for all training samples (s1, a1) do\n3:\ns2 ←a1(s1), a2 ←argmaxa Q(s2, a)\n4:\napproximate ⟨hi⟩, ⟨hih′i⟩, P(h|si, ai)\nusing SA or SQA for (i = 1, 2)\n5:\ncalculate F(si, ai) using (19) for (i = 1, 2)\n6:\nQ(si, ai) ←−F(si, ai) for (i = 1, 2)\n7:\nupdate DBM weights using (18), (22), and (23)\n8:\nπ(s1) ←argmaxa Q(s1, a)\n9: return π\nwritten in terms of the activations of the hidden nodes. More-complicated network architectures\ndo not possess this property, so there is a need for a Boltzmann distribution sampler.\nIn Algorithm 1, we recall the steps of the classical reinforcement learning algorithm using an RBM\nwith a graphical model similar to that shown in Fig. 1a. We set the initial Boltzmann machine\nweights using Gaussian zero-mean values with a standard deviation of 1.00, as is common practice\nfor implementing Boltzmann machines [50]. Consequently, this initializes an approximation of a\nQ-function and a policy π given by\nπ(s) = argmax\na\nQ(s, a) .\nIn each training iteration, we select a state–action pair (s1, a1) ∈S × A. We associate a classical\nspin variable σh to each hidden node h. Then, the activations of the hidden nodes are calculated\nvia (21). In our experiments, all Boltzmann machines have as many state nodes as |S| and as\nmany action nodes as |A|. We associate one node for every state s ∈S, and the corresponding\nbinary encoding is s = (0, 0, . . . , 1, . . . , 0), with zeroes everywhere except at the index of the node\ncorresponding to s. We use similar encoding for the actions, using the action nodes. A subsequent\nstate s2 is obtained from the state–action pair (s1, a1) using the transition kernel outlined in Sec.\n2, and a corresponding action a2 is chosen via policy π. The free energy of the RBM is calculated\nusing (20) for both (s1, a1) and (s2, a2).\nThis results in an approximation of the Q-function (see Sec. 2.5) deﬁned on the state–action\nspace S × A [16],\nQ(s, a) ≈−F(s, a) ,\nfor both state–action pairs. We then use the update rule (17), or, more precisely,\n(22)\n∆wsh = ε(r(s1, a1) + γQ(s2, a2) −Q(s1, a1))s1⟨h⟩\nand\n(23)\n∆wah = ε(r(s1, a1) + γQ(s2, a2) −Q(s1, a1))a1⟨h⟩,\nwith a learning rate ε to update the weights of the RBM. In view of (7), the best known policy can\nbe acquired via π(s) = argmaxa Q(s, a) for any state s.\n16\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\n3.2. Reinforcement Learning Using DBMs. Since we are interested in the dependencies be-\ntween states and actions, we consider a DBM architecture that has a layer of states connected to\nthe ﬁrst layer of hidden nodes, followed by multiple hidden layers, and a layer of actions connected\nto the ﬁnal layer of hidden nodes (see Fig. 1). We demonstrate the advantages of this deep architec-\nture trained using SQA and the derivation in Sec. 2.8 of the temporal-diﬀerence gradient method\nfor reinforcement learning using general Boltzmann machines (GBM).\nIn Algorithm 2, we summarize the DBM-RL method. Here, the graphical model of the Boltzmann\nmachine is similar to that shown in Fig. 1b.\nThe initialization of the weights of the DBM is\nperformed in a similar fashion to the previous algorithm.\nIn each training iteration, we select a state–action pair (s1, a1) ∈S×A. Every node corresponding\nto a state or an action is removed from the graph and the conﬁgurations of the spins corresponding\nto the hidden nodes are sampled using SA or SQA on an Ising spin model constructed as follows:\nthe state s1 contributes to a bias of ws1h to σh if h is adjacent to s1; and the action a1 contributes\nto a bias of wa1h to σh if h is adjacent to a1. The bias on any spin σh for which h is a hidden node\nnot adjacent to state s1 or action a1 is zero.\nA subsequent state s2 is obtained from the state–action pair (s1, a1) using the transition kernel\noutlined in Sec. 2, and a corresponding action a2 is chosen via policy π. Another SQA sampling is\nperformed in a similar fashion to the above for this pair.\nAccording to lines 4 and 5 of Algorithm 2, the samples from the SA or SQA algorithm are used\nto approximate the free energy of the classical DBM at points (s1, a1) and (s2, a2) using (19).\nIf SQA is used, averages are taken over each replica of each run; hence, there are 3750 samples\nof conﬁgurations of the hidden nodes for each state–action pair. The strength Γ of the transverse\nﬁeld is scheduled to linearly decrease from 20.00 to Γf = 0.01.\nThe SA algorithm is used with a linear inverse temperature schedule that increases from 0.01 to\n2.00 in 50,000 sweeps, and is run 150 times. So, if SA is used, there are only 150 sample points\nused in the above approximation. The results of DBM-RL using SA or SQA have no signiﬁcant\ndiﬀerences.\nThe ﬁnal diﬀerence between Algorithm 1 and Algorithm 2 is that the update rule now includes\nupdates of weights between two hidden nodes given by (18),\n(24)\n∆whh′ = ε(r(s1, a1) + γQ(s2, a2) −Q(s1, a1))⟨hh′⟩,\nin addition to the previous rules (22) and (23).\n3.3. Reinforcement Learning Using QBMs. The last algorithm is QBM-RL, presented in\nAlgorithm 3. The initialization is performed as in Algorithms 1 and 2. However, according to lines\n4 and 5, the samples from the SQA algorithm are used to approximate the free energy of a QBM\nat points (s1, a1) and (s2, a2) by computing the free energy corresponding to an eﬀective classical\nIsing spin model of one dimension higher representing the quantum Ising spin model of the QBM,\nvia (16).\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n17\nIn this case, ⟨Heﬀ\ns,a⟩from (16) is approximated by the average energy of the entire system of one\ndimension higher and P(c|s, a) is approximated by the normalized frequency of the conﬁguration\nc of the entire system of one dimension higher (hence, there are only 150 sample points for each\ninput instance in this case). The strength Γ of the transverse ﬁeld in SQA is scheduled to linearly\ndecrease from 20.00 to Γf = 2.00. In this algorithm, the weights are updated as in Algorithm 2.\nHowever, ⟨h⟩and ⟨hh′⟩in this algorithm represent expectations of measurements in the z-basis.\nIn each training iteration, we select a state–action pair (s1, a1) ∈S×A. Every node corresponding\nto a state or an action is removed from this graph and the conﬁgurations of the spins corresponding\nto the hidden nodes are sampled using SQA on an Ising spin model constructed as follows: the\nstate s1 contributes to a bias of ws1h to σh if h is adjacent to s1; and the action a1 contributes to\na bias of wa1h to σh if h is adjacent to a1. The bias on any spin σh for which h is a hidden node\nnot adjacent to state s1 or action a1 is zero.\nA subsequent state s2 is obtained from the state–action pair (s1, a1) using the transition kernel\noutlined in Sec. 2, and a corresponding action a2 is chosen via policy π. Another SQA sampling is\nperformed in a similar fashion to the above for this pair.\nIn Fig. 3a and Fig. 3b, the selection of (s1, a1) is performed by sweeping across the set of state–\naction pairs. In Fig. 3d, the selection of (s1, a1) and s2 is performed by sweeping over S × A × S.\nIn Fig. 3c, the selection of s1, a1, and s2 are all performed uniformly randomly.\nWe experiment with a variety of learning-rate schedules, including exponential, harmonic, and\nlinear; however, we found that for the training of both RBMs and DBMs, an adaptive learning-\nrate schedule performed best (for information on adaptive subgradient methods, see [51]). In our\nexperiments, the initial learning rate is set to 0.01.\nIn all of our studied algorithms, training terminates when a desired number of training samples\nhave been processed, after which the updated policy is returned.\n4. Numerical Results\nWe study the performance of temporal-diﬀerence reinforcement learning algorithms (explained\nin detail in Sec. 3) using Boltzmann machines. We generalize the method introduced in [16], and\nAlgorithm 3\nQBM-RL\n1: initialize weights of QBM\n2: for all training samples (s1, a1) do\n3:\ns2 ←a1(s1), a2 ←argmaxa Q(s2, a)\n4:\napproximate ⟨hi⟩, ⟨hih′i⟩, ⟨Heﬀ\nsi,ai⟩,\nand P(c|si, ai) using SQA for (i = 1, 2)\n5:\ncalculate F(si, ai) using (16) for (i = 1, 2)\n6:\nQ(si, ai) ←−F(si, ai) for (i = 1, 2)\n7:\nupdate QBM weights using (18), (22), and (23)\n8:\nπ(s1) ←argmaxa Q(s1, a)\n9: return π\n18\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\ncompare the policies obtained from these algorithms to the optimal policy using a ﬁdelity measure,\nwhich we deﬁne in (25).\nFor Tr independent trials of the same reinforcement learning algorithm, Ts training samples are\nused for reinforcement learning. The ﬁdelity measure at the i-th training sample is deﬁned by\n(25)\nﬁd(i) = (Tr × |S|)−1\nTr\nX\nl=1\nX\ns∈S\n1A(s,i,l)∈α∗(s),\nwhere A(s, i, l) denotes the action assigned at the l-th run and i-th training sample to the state s.\nIn our experiments, each algorithm is run 1440 times, and for each run of an algorithm, Ts = 500\ntraining samples are generated.\nFig. 3a and Fig. 3b show the ﬁdelity of the generated policies obtained from various reinforcement\nlearning experiments on two clear 3 × 5 mazes. In Fig. 3a, the maze includes one reward, one wall,\nand one pit, and in Fig. 3b, the maze additionally includes two stochastic rewards.\nIn these\nexperiments, the training samples are generated by sweeping over the maze. Each sweep iterates\nover the maze elements in the same order. This explains the periodic behaviour of the ﬁdelity\ncurves (cf. Fig. 3c).\nThe curves labelled ‘QBM-RL’ represent the ﬁdelity of reinforcement learning using QBMs.\nSampling from the QBM is performed using SQA. All other experiments use classical Boltzmann\nmachines as their graphical model. In the experiment labelled ‘RBM-RL’, the graphical model\nis an RBM, trained classically using formula (21). The remaining curve is labelled ‘DBM-RL’ for\nclassical reinforcement learning using a DBM. In these experiments, sampling from conﬁgurations of\nthe DBM is performed with SQA (with Γf = 0.01). The ﬁdelity results of DBM-RL coincide closely\nwith those of sampling conﬁgurations of the DBM using SA; therefore, we have not included them.\nFig. 3c regenerates the results of Fig. 3a using uniform random sampling (i.e., without sweeping\nthrough the maze).\nOur next result, shown in Fig. 3d, compares RBM-RL, DBM-RL, and QBM-RL for a windy\nmaze of size 3 × 5. The transition kernel for this experiment is chosen such that P(a(s)|s, a) = 0.8,\nand P(s′|s, a) has a nonzero value for all s′ ̸= a(s) that are reachable from s by taking some action,\nin which case all these values are equal. The transition probability is zero for all other states.\nFig. 2d shows examples of the transition probabilities in the windy problem.\nTo demonstrate the performance of RBM-RL, DBM-RL, and QBM-RL with respect to scaling,\nwe deﬁne another measure called average ﬁdelity, avℓ, where we take the average ﬁdelity over the\nlast ℓtraining samples of the ﬁdelity measure. Given Ts total training samples and ﬁd(i) as deﬁned\nabove, we write\navℓ= 1\nℓ\nTs\nX\ni=Ts−ℓ\nﬁd(i) .\nIn Fig. 4, we report the eﬀect of maze size on avℓfor RBM-RL, DBM-RL, and QBM-RL for varying\nmaze sizes. We plot avℓfor each algorithm with ℓ= 500, 250, and 10 as a function of maze size\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n19\n0\n100\n200\n300\n400\n500\nTraining Sample\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nﬁd\n1R1W1P\n(a)\n0\n100\n200\n300\n400\n500\nTraining Sample\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nﬁd\n3R1W1P\n(b)\n0\n100\n200\n300\n400\n500\nTraining Sample\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nﬁd\n1R1W1P\n(c)\n0\n100\n200\n300\n400\n500\nTraining Sample\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nﬁd\n1R1W1P-windy\n(d)\nRBM-RL\nDBM-RL\nQBM-RL\nFigure 3. Comparison of RBM-RL, DBM-RL, and QBM-RL training results. Every underlying\nRBM has 16 hidden nodes and every DBM has two layers of eight hidden nodes. The shaded areas\nindicate the standard deviation of each training algorithm. (a) The ﬁdelity curves for the three\nalgorithms run on the maze in Fig 2a. (b) The ﬁdelity curves for the maze in Fig 2b. (c) The\nﬁdelity curves of the mentioned three algorithms corresponding to the same experiment as that\nof (a), except that the training is performed by uniformly generated training samples rather than\nsweeping across the maze. (d) The ﬁdelity curves corresponding to a windy maze similar to Fig 2a.\nfor a family of problems with one deterministic reward, two stochastic rewards, one pit, and n −2\nwalls. We use nine n × 5 mazes in this experiment, indexed by various values of n. In addition to\nthe avℓplots, we include a dotted-line plot depicting the ﬁdelity for a completely random policy.\nThe ﬁdelity of the random policy is given by the average probability of choosing an optimal action\nat each state when generating admissible actions uniformly at random, which is given by\n18n+7\n48n+24.\nNote that the ﬁdelity of the random policy increases as the maze size increases. This is due to the\n20\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\nR\nR\nW\nW\nW\n...\nR\nP\n⟲\nW\nW\n...\n↑\n←\n←\n↑\n←↑\n↑\n←↑\n↑\n←↑\n←↑\n↑\n↑\n←↑\n←↑\n←\n←\n←\nn −2\n5\n(a)\n2 × 5\n3 × 5\n4 × 5\n5 × 5\n6 × 5\n7 × 5\n8 × 5\n9 × 5\n10 × 5\nMaze Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\navℓ\n3R(n−2)W1P - QBM/DBM [10,10], RBM [20]\nDBM-RL, ℓ= 500\nDBM-RL, ℓ= 250\nDBM-RL, ℓ= 10\nRBM-RL, ℓ= 500\nRBM-RL, ℓ= 250\nRBM-RL, ℓ= 10\nQBM-RL, ℓ= 500\nQBM-RL, ℓ= 250\nQBM-RL, ℓ= 10\n(b)\nFigure 4. A comparison between the performance of RBM-RL, DBM-RL, and QBM-RL as the\nsize of the maze grows. All Boltzmann machines have 20 hidden nodes. (a) The schematics of an\nn × 5 maze with one deterministic reward, 2 stochastic rewards, one pit, and n −2 walls. (b) The\nscaling of the average ﬁdelity of each algorithm run on each instance of the n × 5 maze. The dotted\nline is the average ﬁdelity of uniformly randomly generated actions.\nfact that maze rows containing a wall have more average admissible optimal actions than the top\nand bottom rows of the maze.\n5. Discussion\nThe ﬁdelity curves in Fig. 3 show that DBM-RL outperforms RBM-RL with respect to the\nnumber of training samples. Therefore, we expect that in conjunction with a high-performance\nsampler of Boltzmann distributions (e.g., a quantum or a quantum-inspired oracle taken as such),\nDBM-RL improves the performance of reinforcement learning. QBM-RL is not only on par with\nDBM-RL, but actually slightly improves upon it by taking advantage of sampling in the presence\nof a signiﬁcant transverse ﬁeld.\nThis is a positive result for the potential of sampling from a quantum device in machine learn-\ning, as we do not expect quantum annealing to obtain the Boltzmann distribution of a classical\nHamiltonian [42, 52, 53]. However, given the discussion in Sec. 2.1, a quantum annealer viewed\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n21\nas an open system coupled to a heat bath could be a better choice of sampler from its instanta-\nneous Hamiltonian in earlier stages of the annealing process, compared to a sampler of the problem\nHamiltonian at the end of the evolution. Therefore, these experiments address whether a quantum\nBoltzmann machine with a transverse ﬁeld Ising Hamiltonian can perform at least as well as a\nclassical Boltzmann machine.\nIn each experiment, the ﬁdelity curves from DBM-RL produced using SQA with Γf = 0.01 match\nthe ones produced using SA. This is consistent with our expectation that using SQA with Γ →0\nproduces samples from the same distribution as SA, namely, the Boltzmann distribution of the\nclassical Ising Hamiltonian with no transverse ﬁeld.\nThe best algorithm in our experiments is evidently QBM-RL using SQA. Here, the ﬁnal trans-\nverse ﬁeld is Γf = 2.00, corresponding to one-third of the anneal for a quantum annealing algorithm\nthat evolves along the convex linear combination of the initial and ﬁnal Hamiltonians with constant\nspeed. This is consistent with ideas found in [38] on sampling at freeze-out [42].\nFig. 3c shows that, whereas the maze can be solved with fewer training samples using ordered\nsweeps of the maze, the periodic behaviour of the ﬁdelity curves is due to this periodic choice of\ntraining samples. This eﬀect disappears once the training samples are chosen uniformly randomly.\nFig. 3d shows that the improvement in the learning of the DBM-RL and QBM-RL algorithms\npersists in the case of more-complicated transition kernels. The same ordering of ﬁdelity curves\ndiscussed earlier is observed: QBM-RL outperforms DBM-RL, and DBM-RL outperforms RBM-\nRL.\nIt is worth mentioning that, even though it may seem that more connectivity between the\nhidden nodes may allow a Boltzmann machine to capture more- complicated correlations between\nthe visible nodes, the training process of the Boltzmann machine becomes more computationally\ninvolved. In our reinforcement learning application, an RBM with m hidden nodes, and n = |S|+|A|\nvisible nodes, has mn weights to train. A DBM with two hidden layers of equal size has 1\n4m(2n+m)\nweights to train. Therefore, when m < 2n, the training of the DBM is in a domain of a lower\ndimension. Further, a GBM with all of its hidden nodes forming a complete graph requires mn+\n\u0000m\n2\n\u0001\nweights to train, which is always larger than that of an RBM or a DBM with the same number of\nhidden nodes.\nOne can observe from Fig. 4 that, as the maze size increases and the complexity of the rein-\nforcement learning task increases, avℓdecreases for each algorithm. The RBM algorithm, while\nalways outperformed by DBM-RL and QBM-RL, shows a much faster decay in average ﬁdelity as\na function of maze size compared to both DBM-RL and QBM-RL. For larger mazes, the RBM\nalgorithm fails to capture maze traversal knowledge, and approaches avℓof a random action al-\nlocation (the dotted line), whereas the DBM-RL and QBM-RL algorithms continue to be trained\nwell. DBM-RL and QBM-RL are capable of training the agent to traverse larger mazes, whereas\nthe RBM algorithm, utilizing the same number of hidden nodes and a larger number of weights,\nfails to converge to an output that is better than a random policy.\n22\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\nThe runtime and computational resources needed to compare DBM-RL and QBM-RL with RBM-\nRL have not been investigated here. We expect that in view of [19], the size of RBM needed to\nsolve larger maze problems will grow exponentially. Thus, it would be interesting to research the\nextrapolation of the asymptotic complexity and size of the DBM-RL and QBM-RL algorithms\nwith the aim of attaining a quantum advantage. Applying the algorithms described in this paper\nto tasks that have larger state and action spaces, as well as to more-complicated environments, will\nallow us to demonstrate the scalability and usefulness of the DBM-RL and QBM-RL approaches.\nThe experimental results shown in Fig. 4 represent only a rudimentary attempt to investigate\nthis matter, yet the results are promising. However, this experiment does not provide a practical\ncharacterization of the scaling of our approach, and further investigation is needed.\nAcknowledgements\nWe would like to thank Hamed Karimi, Helmut Katzgraber, Murray Thom, Matthias Troyer,\nand Ehsan Zahedinejad, as well as the referees and editorial board of Quantum Information and\nComputation, for reviewing this work and providing many helpful suggestions. The idea of using\nSQA to run experiments involving measurements with a nonzero transverse ﬁeld was communi-\ncated in person by Mohammad Amin. We would also like to thank Marko Bucyk for editing this\nmanuscript.\nReferences\n[1] M. S. Sarandy and D. A. Lidar, “Adiabatic approximation in open quantum systems,” Phys. Rev. A, vol. 71,\np. 012331, 2005.\n[2] J. E. Avron, M. Fraas, G. M. Graf, and P. Grech, “Adiabatic theorems for generators of contracting evolutions,”\nCommun. Math. Phys., vol. 314, no. 1, pp. 163–191, 2012.\n[3] T. Albash, S. Boixo, D. A. Lidar, and P. Zanardi, “Quantum adiabatic Markovian master equations,” New J.\nPhys., vol. 14, no. 12, p. 123016, 2012.\n[4] S. Bachmann, W. De Roeck, and M. Fraas, “The Adiabatic Theorem for Many-Body Quantum Systems,”\narXiv:1612.01505, 2016.\n[5] L. C. Venuti, T. Albash, D. A. Lidar, and P. Zanardi, “Adiabaticity in open quantum systems,” Phys. Rev. A,\nvol. 93, p. 032118, 2016.\n[6] M. W. Johnson, M. H. S. Amin, S. Gildert, T. Lanting, F. Hamze, N. Dickson, R. Harris, A. J. Berkley,\nJ. Johansson, P. Bunyk, E. M. Chapple, C. Enderud, J. P. Hilton, K. Karimi, E. Ladizinsky, N. Ladizinsky,\nT. Oh, I. Perminov, C. Rich, M. C. Thom, E. Tolkacheva, C. J. S. Truncik, S. Uchaikin, J. Wang, B. Wilson,\nand G. Rose, “Quantum annealing with manufactured spins,” Nature, vol. 473, pp. 194–198, 2011.\n[7] J. Kelly, R. Barends, A. G. Fowler, A. Megrant, E. Jeﬀrey, T. C. White, D. Sank, J. Y. Mutus, B. Campbell,\nY. Chen, Z. Chen, B. Chiaro, A. Dunsworth, I. C. Hoi, C. Neill, P. J. J. O’Malley, C. Quintana, P. Roushan,\nA. Vainsencher, J. Wenner, A. N. Cleland, and J. M. Martinis, “State preservation by repetitive error detection\nin a superconducting quantum circuit,” Nature, vol. 519, pp. 66–69, 2015.\n[8] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT Press, 1998.\n[9] D. Bertsekas and J. Tsitsiklis, Neuro-dynamic Programming. Anthropological Field Studies, Athena Scientiﬁc,\n1996.\n[10] V. Derhami, E. Khodadadian, M. Ghasemzadeh, and A. M. Z. Bidoki, “Applying reinforcement learning for web\npages ranking algorithms,” Appl. Soft Comput., vol. 13, no. 4, pp. 1686–1692, 2013.\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n23\n[11] S. Syaﬁie, F. Tadeo, and E. Martinez, “Model-free learning control of neutralization processes using reinforcement\nlearning,” Engineering Applications of Artiﬁcial Intelligence, vol. 20, no. 6, pp. 767–782, 2007.\n[12] I. Erev and A. E. Roth, “Predicting how people play games: Reinforcement learning in experimental games with\nunique, mixed strategy equilibria,” Am. Econ. Rev., pp. 848–881, 1998.\n[13] H. Shteingart and Y. Loewenstein, “Reinforcement learning and human behavior,” Current Opinion in Neuro-\nbiology, vol. 25, pp. 93–98, 2014.\n[14] T. Matsui, T. Goto, K. Izumi, and Y. Chen, “Compound reinforcement learning: theory and an application to\nﬁnance,” in European Workshop on Reinforcement Learning, pp. 321–332, Springer, 2011.\n[15] Z. Sui, A. Gosavi, and L. Lin, “A reinforcement learning approach for inventory replenishment in vendor-managed\ninventory systems with consignment inventory,” Engineering Management Journal, vol. 22, no. 4, pp. 44–53, 2010.\n[16] B. Sallans and G. E. Hinton, “Reinforcement learning with factored states and actions,” JMLR, vol. 5, pp. 1063–\n1088, 2004.\n[17] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward networks are universal approximators,”\nNeural Networks, vol. 2, no. 5, pp. 359–366, 1989.\n[18] J. Martens, A. Chattopadhya, T. Pitassi, and R. Zemel, “On the representational eﬃciency of restricted Boltz-\nmann machines,” in Advances in Neural Information Processing Systems, pp. 2877–2885, 2013.\n[19] N. Le Roux and Y. Bengio, “Representational power of restricted Boltzmann machines and deep belief networks,”\nNeural Computation, vol. 20, no. 6, pp. 1631–1649, 2008.\n[20] R. Salakhutdinov and G. E. Hinton, “Deep Boltzmann Machines,” in Proceedings of the Twelfth International\nConference on Artiﬁcial Intelligence and Statistics, AISTATS 2009, Clearwater Beach, Florida, USA, April\n16-18, 2009, pp. 448–455, 2009.\n[21] R. Harris, M. W. Johnson, T. Lanting, A. J. Berkley, J. Johansson, P. Bunyk, E. Tolkacheva, E. Ladizinsky,\nN. Ladizinsky, T. Oh, F. Cioata, I. Perminov, P. Spear, C. Enderud, C. Rich, S. Uchaikin, M. C. Thom, E. M.\nChapple, J. Wang, B. Wilson, M. H. S. Amin, N. Dickson, K. Karimi, B. Macready, C. J. S. Truncik, and\nG. Rose, “Experimental investigation of an eight-qubit unit cell in a superconducting optimization processor,”\nPhys. Rev. B, vol. 82, p. 024511, 2010.\n[22] M. Benedetti, J. Realpe-Gmez, R. Biswas, and A. Perdomo-Ortiz, “Quantum-assisted learning of graphical\nmodels with arbitrary pairwise connectivity,” arXiv:1609.02542, 2016.\n[23] S. H. Adachi and M. P. Henderson, “Application of Quantum Annealing to Training of Deep Neural Networks,”\narXiv:1510.06356, 2015.\n[24] M. Denil and N. de Freitas, “Toward the implementation of a quantum RBM,” in NIPS 2011 Deep Learning\nand Unsupervised Feature Learning Workshop, 2011.\n[25] M. Benedetti, J. Realpe-G´omez, R. Biswas, and A. Perdomo-Ortiz, “Estimation of eﬀective temperatures in\nquantum annealers for sampling applications: A case study with possible applications in deep learning,” Phys.\nRev. A, vol. 94, p. 022308, 2016.\n[26] V. Dunjko, J. M. Taylor, and H. J. Briegel, “Quantum-enhanced machine learning,” Phys. Rev. Lett., vol. 117,\np. 130501, 2016.\n[27] D. Dong, C. Chen, H. Li, and T. J. Tarn, “Quantum reinforcement learning,” IEEE Transactions on Systems,\nMan, and Cybernetics, Part B (Cybernetics), vol. 38, pp. 1207–1220, 2008.\n[28] N. Wiebe, A. Kapoor, and K. M. Svore, “Quantum deep learning,” Quantum Inf. Comput., vol. 16, no. 7-8,\npp. 541–587, 2016.\n[29] M. Kieferova and N. Wiebe, “Tomography and Generative Data Modeling via Quantum Boltzmann Training,”\narXiv:1612.05204, 2016.\n[30] E. Crosson and A. W. Harrow, “Simulated Quantum Annealing Can Be Exponentially Faster than Classical\nSimulated Annealing,” arXiv:1601.03030, 2016.\n24\nD. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH\n[31] B. Heim, T. F. Rønnow, S. V. Isakov, and M. Troyer, “Quantum versus classical annealing of Ising spin glasses,”\nScience, vol. 348, no. 6231, pp. 215–217, 2015.\n[32] M. B. Hastings and M. H. Freedman, “Obstructions to classically simulating the quantum adiabatic algorithm,”\nQuantum Information & Computation, vol. 13, pp. 1038–1076, 2013.\n[33] S. Morita and H. Nishimori, “Convergence theorems for quantum annealing,” J. Phys. A: Mathematical and\nGeneral, vol. 39, no. 45, p. 13903, 2006.\n[34] S. V. Isakov, G. Mazzola, V. N. Smelyanskiy, Z. Jiang, S. Boixo, H. Neven, and M. Troyer, “Understanding\nquantum tunneling through quantum Monte Carlo simulations,” arXiv:1510.08057, 2015.\n[35] T. Albash, T. F. Rønnow, M. Troyer, and D. A. Lidar, “Reexamining classical and quantum models for the\nD-Wave One processor,” arXiv:1409.3827, 2014.\n[36] L. T. Brady and W. van Dam, “Quantum Monte Carlo simulations of tunneling in quantum adiabatic optimiza-\ntion,” Phys. Rev. A, vol. 93, no. 3, p. 032304, 2016.\n[37] S. W. Shin, G. Smith, J. A. Smolin, and U. Vazirani, “How ‘Quantum’ is the D-Wave Machine?,”\narXiv:1401.7087, 2014.\n[38] M. H. Amin, E. Andriyash, J. Rolfe, B. Kulchytskyy, and R. Melko, “Quantum Boltzmann machine,”\narXiv:1601.02036, 2016.\n[39] M. Born and V. Fock, “Beweis des Adiabatensatzes,” Zeitschrift fur Physik, vol. 51, pp. 165–180, 1928.\n[40] T. Kadowaki and H. Nishimori, “Quantum annealing in the transverse Ising model,” Phys. Rev. E, vol. 58,\npp. 5355–5363, 1998.\n[41] E. Farhi, J. Goldstone, S. Gutmann, and M. Sipser, “Quantum Computation by Adiabatic Evolution,”\narXiv:quant-ph/0001106, 2000.\n[42] M. H. Amin, “Searching for quantum speedup in quasistatic quantum annealers,” Phys. Rev. A, vol. 92, no. 5,\np. 052323, 2015.\n[43] S. M. Anthony Brabazon, Michael O’Neill, Natural Computing Algorithms. Springer-Verlag Berlin Heidelberg,\n2015.\n[44] R. Martoˇn´ak, G. E. Santoro, and E. Tosatti, “Quantum annealing by the path-integral Monte Carlo method:\nThe two-dimensional random Ising model,” Phys. Rev. B, vol. 66, no. 9, p. 094203, 2002.\n[45] S. Yuksel, “Control of stochastic systems.” Course lecture notes, Queen’s University (Kingston, ON Canada),\nRetrieved in May, 2016.\n[46] R. S. Sutton, “Integrated architectures for learning, planning, and reacting based on approximating dynamic\nprogramming,” in In Proceedings of the Seventh International Conference on Machine Learning, pp. 216–224,\nMorgan Kaufmann, 1990.\n[47] R. Bellman, “Dynamic programming and Lagrange multipliers,” Proceedings of the National Academy of Sci-\nences, vol. 42, no. 10, pp. 767–769, 1956.\n[48] M. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.\n[49] M. Suzuki, “Relationship between d-dimensional quantal spin systems and (d+1)-dimensional Ising systems\nequivalence, critical exponents and systematic approximants of the partition function and spin correlations,”\nProgr. Theor. Exp. Phys., vol. 56, no. 5, pp. 1454–1469, 1976.\n[50] G. Hinton, “A practical guide to training restricted Boltzmann machines,” Momentum, vol. 9, no. 1, p. 926,\n2010.\n[51] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online learning and stochastic optimiza-\ntion,” JMLR, vol. 12, no. Jul, pp. 2121–2159, 2011.\n[52] Y. Matsuda, H. Nishimori, and H. G. Katzgraber, “Ground-state statistics from annealing algorithms: quantum\nversus classical approaches,” New. J. Phys., vol. 11, no. 7, p. 073021, 2009.\n[53] L. C. Venuti, T. Albash, M. Marvian, D. Lidar, and P. Zanardi, “Relaxation versus adiabatic quantum steady-\nstate preparation,” Phys. Rev. A, vol. 95, p. 042302, 2017.\nREINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES\n25\n[54] E. Farhi and A. W. Harrow, “Quantum Supremacy through the Quantum Approximate Optimization Algo-\nrithm,” arXiv:1602.07674, 2016.\n[55] F. Abtahi and I. Fasel, “Deep belief nets as function approximators for reinforcement learning,” Frontiers in\nComputational Neuroscience, 2011.\n[56] S. Elfwing, E. Uchibe, and K. Doya, “Scaled free-energy based reinforcement learning for robust and eﬃcient\nlearning in high-dimensional state spaces,” Value and Reward Based Learning in Neurobots, p. 30, 2015.\n[57] R. Martoˇn´ak, G. E. Santoro, and E. Tosatti, “Quantum annealing by the path-integral Monte Carlo method:\nThe two-dimensional random Ising model,” Phys. Rev. B, vol. 66, no. 9, p. 094203, 2002.\n[58] M. Otsuka, J. Yoshimoto, and K. Doya, “Free-energy-based reinforcement learning in a partially observable\nenvironment.,” ESANN 2010 proceedings, European Symposium on Artiﬁcial Neural Networks – Computational\nIntelligence and Machine Learning, 2010.\n[59] S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush, N. Ding, Z. Jiang, J. M. Martinis, and H. Neven,\n“Characterizing quantum supremacy in near-term devices,” arXiv:1608.00263v2, 2016.\n[60] J. Raymond, S. Yarkoni, and E. Andriyash, “Global warming:\nTemperature estimation in annealers,”\narXiv:1606.00919, 2016.\n[61] P. M. Long and R. Servedio, “Restricted Boltzmann machines are hard to approximately evaluate or simulate,”\nin Proceedings of the 27th International Conference on Machine Learning, pp. 703–710, 2010.\n[62] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, “A learning algorithm for Boltzmann machines,” Cogn. Sci.,\nvol. 9, no. 1, pp. 147–169, 1985.\n[63] S. Singh, T. Jaakkola, M. L. Littman, and C. Szepesv´ari, “Convergence results for single-step on-policy\nreinforcement-learning algorithms,” Machine learning, vol. 38, no. 3, pp. 287–308, 2000.\n[64] N. Fr´emaux, H. Sprekeler, and W. Gerstner, “Reinforcement learning using a continuous time actor-critic frame-\nwork with spiking neurons,” PLoS Comput. Biol., vol. 9, no. 4, p. e1003024, 2013.\n[65] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, et al., “Mastering the game of go with deep neural networks and tree search,”\nNature, vol. 529, no. 7587, pp. 484–489, 2016.\n[66] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518,\nno. 7540, pp. 529–533, 2015.\nE-mail address, Daniel Crawford: daniel.crawford@1qbit.com\nE-mail address, Anna Levit: anna.levit@1qbit.com\nE-mail address, Navid Ghadermarzy: navidgh@math.ubc.ca\nE-mail address, Jaspreet S. Oberoi: jaspreet.oberoi@1qbit.com\nE-mail address, Pooya Ronagh: pooya.ronagh@1qbit.com\n(Daniel Crawford, Anna Levit, Jaspreet S. Oberoi, Pooya Ronagh) 1QB Information Technologies (1QBit)\n(Navid Ghadermarzy) Department of Mathematics, University of British Columbia\n(Jaspreet S. Oberoi) School of Engineering Science, Simon Fraser University\n(Pooya Ronagh) Institute for Quantum Computing and Department of Physics and Astronomy, Uni-\nversity of Waterloo\n",
  "categories": [
    "quant-ph",
    "cs.AI",
    "cs.LG",
    "cs.NE",
    "math.OC"
  ],
  "published": "2016-12-17",
  "updated": "2019-01-03"
}