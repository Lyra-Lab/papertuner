{
  "id": "http://arxiv.org/abs/1612.06370v2",
  "title": "Learning Features by Watching Objects Move",
  "authors": [
    "Deepak Pathak",
    "Ross Girshick",
    "Piotr Dollár",
    "Trevor Darrell",
    "Bharath Hariharan"
  ],
  "abstract": "This paper presents a novel yet intuitive approach to unsupervised feature\nlearning. Inspired by the human visual system, we explore whether low-level\nmotion-based grouping cues can be used to learn an effective visual\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\nconvolutional network to segment objects from a single frame. Given the\nextensive evidence that motion plays a key role in the development of the human\nvisual system, we hope that this straightforward approach to unsupervised\nlearning will be more effective than cleverly designed 'pretext' tasks studied\nin the literature. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection, our representation\nsignificantly outperforms previous unsupervised approaches across multiple\nsettings, especially when training data for the target task is scarce.",
  "text": "Learning Features by Watching Objects Move\nDeepak Pathak1,2,*, Ross Girshick1, Piotr Doll´ar1, Trevor Darrell2, and Bharath Hariharan1\n1Facebook AI Research (FAIR)\n2University of California, Berkeley\nAbstract\nThis paper presents a novel yet intuitive approach to un-\nsupervised feature learning. Inspired by the human visual\nsystem, we explore whether low-level motion-based group-\ning cues can be used to learn an effective visual represen-\ntation. Speciﬁcally, we use unsupervised motion-based seg-\nmentation on videos to obtain segments, which we use as\n‘pseudo ground truth’ to train a convolutional network to\nsegment objects from a single frame. Given the extensive\nevidence that motion plays a key role in the development of\nthe human visual system, we hope that this straightforward\napproach to unsupervised learning will be more effective\nthan cleverly designed ‘pretext’ tasks studied in the litera-\nture. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection,\nour representation signiﬁcantly outperforms previous un-\nsupervised approaches across multiple settings, especially\nwhen training data for the target task is scarce.\n1. Introduction\nConvNet-based image representations are extremely ver-\nsatile, showing good performance in a variety of recogni-\ntion tasks [9, 15, 19, 50]. Typically these representations\nare trained using supervised learning on large-scale image\nclassiﬁcation datasets, such as ImageNet [41]. In contrast,\nanimal visual systems do not require careful manual anno-\ntation to learn, and instead take advantage of the nearly in-\nﬁnite amount of unlabeled data in their surrounding envi-\nronments. Developing models that can learn under these\nchallenging conditions is a fundamental scientiﬁc problem,\nwhich has led to a ﬂurry of recent work proposing methods\nthat learn visual representations without manual annotation.\nA recurring theme in these works is the idea of a ‘pre-\ntext task’: a task that is not of direct interest, but can be\nused to obtain a good visual representation as a byprod-\nuct of training. Example pretext tasks include reconstruct-\n∗Work done during an internship at FAIR.\nFigure 1. Low-level appearance cues lead to incorrect grouping\n(top right). Motion helps us to correctly group pixels that move\ntogether (bottom left) and identify this group as a single object\n(bottom right). We use unsupervised motion-based grouping to\ntrain a ConvNet to segment objects in static images and show that\nthe network learns strong features that transfer well to other tasks.\ning the input [4, 20, 44], predicting the pixels of the next\nframe in a video stream [17], metric learning on object track\nendpoints [46], temporally ordering shufﬂed frames from a\nvideo [29], and spatially ordering patches from a static im-\nage [8, 30]. The challenge in this line of research lies in\ncleverly designing a pretext task that causes the ConvNet\n(or other representation learner) to learn high-level features.\nIn this paper, we take a different approach that is moti-\nvated by human vision studies. Both infants [42] and newly\nsighted congenitally blind people [32] tend to oversegment\nstatic objects, but can group things properly when they move\n(Figure 1). To do so, they may rely on the Gestalt principle\nof common fate [34, 47]: pixels that move together tend\nto belong together. The ability to parse static scenes im-\nproves [32] over time, suggesting that while motion-based\ngrouping appears early, static grouping is acquired later,\npossibly bootstrapped by motion cues. Moreover, experi-\nments in [32] show that shortly after gaining sight, human\nsubjects are better able to name objects that tend to be seen\n1\narXiv:1612.06370v2  [cs.CV]  12 Apr 2017\nin motion compared to objects that tend to be seen at rest.\nInspired by these human vision studies, we propose to\ntrain ConvNets for the well-established task of object fore-\nground vs. background segmentation, using unsupervised\nmotion segmentation to provide ‘pseudo ground truth’.\nConcretely, to prepare training data we use optical ﬂow to\ngroup foreground pixels that move together into a single ob-\nject. We then use the resulting segmentation masks as au-\ntomatically generated targets, and task a ConvNet with pre-\ndicting these masks from single, static frames without any\nmotion information (Figure 2). Because pixels with differ-\nent colors or low-level image statistics can still move to-\ngether and form a single object, the ConvNet cannot solve\nthis task using a low-level representation. Instead, it may\nhave to recognize objects that tend to move and identify\ntheir shape and pose. Thus, we conjecture that this task\nforces the ConvNet to learn a high-level representation.\nWe evaluate our proposal in two settings. First, we test\nif a ConvNet can learn a good feature representation when\nlearning to segment from the high-quality, manually labeled\nsegmentations in COCO [27], without using the class labels.\nIndeed, we show that the resulting feature representation is\neffective when transferred to PASCAL VOC object detec-\ntion. It achieves state-of-the-art performance for representa-\ntions trained without any semantic category labels, perform-\ning within 5 points AP of an ImageNet pretrained model\nand 10 points higher than the best unsupervised methods.\nThis justiﬁes our proposed task by showing that given good\nground truth segmentations, a ConvNet trained to segment\nobjects will learn an effective feature representation.\nOur goal, however, is to learn features without man-\nual supervision. Thus in our second setting we train with\nautomatically generated ‘pseudo ground truth’ obtained\nthrough unsupervised motion segmentation on uncurated\nvideos from the Yahoo Flickr Creative Commons 100 mil-\nlion (YFCC100m) [43] dataset. When transferred to object\ndetection, our representation retains good performance even\nwhen most of the ConvNet parameters are frozen, signif-\nicantly outperforming previous unsupervised learning ap-\nproaches. It also allows much better transfer learning when\ntraining data for the target task is scarce. Our representation\nquality tends to increase logarithmically with the amount of\ndata, suggesting the possibility of outperforming ImageNet\npretraining given the countless videos on the web.\n2. Related Work\nUnsupervised learning is a broad area with a large vol-\nume of work; Bengio et al. [5] provide an excellent survey.\nHere, we brieﬂy revisit some of the recent work in this area.\nUnsupervised learning by generating images.\nClassical\nunsupervised representation learning approaches, such as\nautoencoders [4, 20] and denoising autoencoders [44], at-\nFigure 2. Overview of our approach. We use motion cues to seg-\nment objects in videos without any supervision. We then train a\nConvNet to predict these segmentations from static frames, i.e.\nwithout any motion cues. We then transfer the learned representa-\ntion to other recognition tasks.\ntempt to learn feature representations from which the orig-\ninal image can be decoded with a low error.\nAn alter-\nnative to reconstruction-based objectives is to train gener-\native models of images using generative adversarial net-\nworks [16]. These models can be extended to produce good\nfeature representations by training jointly with image en-\ncoders [10,11]. However, to generate realistic images, these\nmodels must pay signiﬁcant attention to low-level details\nwhile potentially ignoring higher-level semantics.\nSelf-supervision via pretext tasks.\nInstead of producing\nimages, several recent studies have focused on providing\nalternate forms of supervision (often called ‘pretext tasks’)\nthat do not require manual labeling and can be algorithmi-\ncally produced. For instance, Doersch et al. [8] task a Con-\nvNet with predicting the relative location of two cropped\nimage patches.\nNoroozi and Favaro [30] extend this by\nasking a network to arrange shufﬂed patches cropped from\na 3×3 grid.\nPathak et al. [35] train a network to per-\nform an image inpainting task. Other pretext tasks include\npredicting color channels from luminance [25, 51] or vice\nversa [52], and predicting sounds from video frames [7,33].\nThe assumption in these works is that to perform these\ntasks, the network will need to recognize high-level con-\ncepts, such as objects, in order to succeed. We compare our\napproach to all of these pretext tasks and show that the pro-\nposed natural task of object segmentation leads to a quanti-\ntatively better feature representation in many cases.\nLearning from motion and action.\nThe human visual\nsystem does not receive static images; it receives a continu-\nous video stream. The same idea of deﬁning auxiliary pre-\ntext tasks can be used in unsupervised learning from videos\ntoo. Wang and Gupta [46] train a ConvNet to distinguish be-\n2\ntween pairs of tracked patches in a single video, and pairs\nof patches from different videos. Misra et al. [29] ask a\nnetwork to arrange shufﬂed frames of a video into a tem-\nporally correct order. Another such pretext task is to make\npredictions about the next few frames: Goroshin et al. [17]\npredict pixels of future frames and Walker et al. [45] predict\ndense future trajectories. However, since nearby frames in a\nvideo tend to be visually similar (in color or texture), these\napproaches might learn low-level image statistics instead of\nmore semantic features. Alternatively, Li et al. [26] use mo-\ntion boundary detection to bootstrap a ConvNet-based con-\ntour detector, but ﬁnd that this does not lead to good feature\nrepresentations. Our intuitions are similar, but our approach\nproduces semantically strong representations.\nAnimals and robots can also sense their own motion\n(proprioception), and a possible task is to predict this sig-\nnal from the visual input alone [2,14,21]. While such cues\nundoubtedly can be useful, we show that strong representa-\ntions can be learned even when such cues are unavailable.\n3. Evaluating Feature Representations\nTo measure the quality of a learned feature representa-\ntion, we need an evaluation that reﬂects real-world con-\nstraints to yield useful conclusions. Prior work on unsuper-\nvised learning has evaluated representations by using them\nas initializations for ﬁne-tuning a ConvNet for a particu-\nlar isolated task, such as object detection [8]. The intuition\nis that a good representations should serve as a good start-\ning point for task-speciﬁc ﬁne-tuning. While ﬁne-tuning for\neach task can be a good solution, it can also be impractical.\nFor example, a mobile app might want to handle multiple\ntasks on device, such as image classiﬁcation, object detec-\ntion, and segmentation. But both the app download size and\nexecution time will grow linearly with the number of tasks\nunless computation is shared. In such cases it may be desir-\nable to have a general representation that is shared between\ntasks and task-speciﬁc, lightweight classiﬁer ‘heads’.\nAnother practical concern arises when the amount of la-\nbeled training data is too limited for ﬁne-tuning. Again, in\nthis scenario it may be desirable to use a ﬁxed general rep-\nresentation with a trained task-speciﬁc ‘head’ to avoid over-\nﬁtting. Rather than emphasizing any one of these cases, in\nthis paper we aim for a broader understanding by evaluating\nlearned representations under a variety of conditions:\n1. On multiple tasks: We consider object detection, im-\nage classiﬁcation and semantic segmentation.\n2. With shared layers: We ﬁne-tune the pretrained Con-\nvNet weights to different extents, ranging from only\nthe fully connected layers to ﬁne-tuning everything\n(see [30] for a similar evaluation on ImageNet).\n3. With limited target task training data: We reduce\nthe amount of training data available for the target task.\n4. Learning Features by Learning to Group\nThe core intuition behind this paper is that training a\nConvNet to group pixels in static images into objects with-\nout any class labels will cause it to learn a strong, high-\nlevel feature representation. This is because such grouping\nis difﬁcult from low-level cues alone: objects are typically\nmade of multiple colors and textures and, if occluded, might\neven consist of spatially disjoint regions. Therefore, to ef-\nfectively do this grouping is to implicitly recognize the ob-\nject and understand its location and shape, even if it cannot\nbe named. Thus, if we train a ConvNet for this task, we\nexpect it to learn a representation that aids recognition.\nTo test this hypothesis, we ran a series of experiments us-\ning high-quality manual annotations on static images from\nCOCO [27]. Although supervised, these experiments help\nto evaluate a) how well our method might work under ideal\nconditions, b) how performance is impacted if the segments\nare of lower quality, and c) how much data is needed. We\nnow describe these experiments in detail.\n4.1. Training a ConvNet to Segment Objects\nWe frame the task as follows: given an image patch con-\ntaining a single object, we want the ConvNet to segment\nthe object, i.e., assign each pixel a label of 1 if it lies on the\nobject and 0 otherwise. Since an image contains multiple\nobjects, the task is ambiguous if we feed the ConvNet the\nentire image. Instead, we sample an object from an image\nand crop a box around the ground truth segment. However,\ngiven a precise bounding box, it is easy for the ConvNet to\ncheat: a blob in the center of the box would yield low loss.\nTo prevent such degenerate solutions, we jitter the box in\nposition and scale. Note that a similar training setup was\nused for recent segmentation proposal methods [37,38].\nWe use a straightforward ConvNet architecture that takes\nas input a w × w image and outputs an s × s mask. Our\nnetwork ends in a fully connected layer with s2 outputs fol-\nlowed by an element-wise sigmoid. The resulting s2 dimen-\nsional vector is reshaped into an s×s mask. We also down-\nsample the ground truth mask to s × s and sum the cross\nentropy losses over the s2 locations to train the network.\n4.2. Experiments\nTo enable comparisons to prior work on unsupervised\nlearning, we use AlexNet [24] as our ConvNet architecture.\nWe use s = 56 and w = 227. We use images and anno-\ntations from the trainval set of the COCO dataset [27], dis-\ncarding the class labels and only using the segmentations.\nDoes training for segmentation yield good features?\nFollowing recent work on unsupervised learning, we per-\nform experiments on the task of object detection on PAS-\nCAL VOC 2007 using Fast R-CNN [15].1 We use multi-\n1https://github.com/rbgirshick/py-faster-rcnn\n3\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers Finetuned\n0\n10\n20\n30\n40\n50\n60\n% mean AP\nObject Detection (VOC2007)\nImageNet [21]\nSupervised Masks\nContext [6] (unsupervised)\nFigure 3. Our representation trained on manually-annotated seg-\nments from COCO (without class labels) compared to ImageNet\npretraining and context prediction (unsupervised) [8], evaluated\nfor object detection on PASCAL VOC 2007. ‘>cX’: all layers\nabove convX are ﬁne-tuned; ‘All’: the entire net is ﬁne-tuned.\nscale training and testing [15]. In keeping with the motiva-\ntion described in Section 3, we measure performance with\nConvNet layers frozen to different extents. We compare our\nrepresentation to a ConvNet trained on image classiﬁcation\non ImageNet, and the representation trained by Doersch et\nal. [8]. The latter is competitive with the state-of-the-art.\n(Comparisons to other recent work on unsupervised learn-\ning appear later.) The results are shown in Figure 3.\nWe ﬁnd that our supervised representation outperforms\nthe unsupervised context prediction model across all sce-\nnarios by a large margin, which is to be expected. Notably\nthough, our model maintains a fairly small gap with Ima-\ngeNet pretraining. This result is state-of-the-art for a model\ntrained without semantic category labels. Thus, given high-\nquality segments, our proposed method can learn a strong\nrepresentation, which validates our hypothesis.\nFigure 3 also shows that the model trained on context\nprediction degrades rapidly as more layers are frozen. This\ndrop indicates that the higher layers of the model have be-\ncome overly speciﬁc to the pretext task [49], and may not\ncapture the high-level concepts needed for object recogni-\ntion. This is in contrast to the stable performance of the\nImageNet trained model even when most of the network is\nfrozen, suggesting the utility of its higher layers for recog-\nnition tasks. We ﬁnd that this trend is also true for our rep-\nresentation: it retains good performance even when most of\nthe ConvNet is frozen, indicating that it has indeed learned\nhigh-level semantics in the higher layers.\nCan the ConvNet learn from noisy masks?\nWe next ask\nif the quality of the learned representation is impacted by\nthe quality of the ground truth, which is important since the\nsegmentations obtained from unsupervised motion-based\ngrouping will be imperfect. To simulate noisy segments, we\ntrain the representation with degraded masks from COCO.\nWe consider two ways of creating noisy segments: intro-\nducing noise in the boundary and truncating the mask.\nFigure 4. We degrade ground truth masks to measure the impact\nof segmentation quality on the learned representation. From left\nto right, the original mask, dilated and eroded masks (boundary\nerrors), and a truncated mask (truncation can be on any side).\n0\n4\n8 12 16 20\nMorph kernel size\n30\n35\n40\n45\n50\n55\n% mean AP\n0 10 20 30 40 50\n% Truncation\n3\n10\n30\n100\n% Data\nObject Detection (VOC 2007)\nFigure 5. VOC object detection accuracy using our supervised\nConvNet as noise is introduced in mask boundaries, the masks are\ntruncated, or the amount of data is reduced. Surprisingly, the rep-\nresentation maintains quality even with large degradation.\nNoise in the segment boundary simulates the foreground\nleaking into the background or vice-versa.\nTo introduce\nsuch noise during training, for each cropped ground truth\nmask, we randomly either erode or dilate the mask using\na kernel of ﬁxed size (Figure 4, second and third images).\nThe boundaries become noisier as the kernel size increases.\nTruncation simulates the case when we miss a part of the\nobject, such as when only part of the object moves. Specif-\nically, for each ground truth mask, we zero out a strip of\npixels corresponding to a ﬁxed percentage of the bounding\nbox area from one of the four sides (Figure 4, last image).\nWe evaluate the representation trained with these noisy\nground truth segments on object detection using Fast R-\nCNN with all layers up to and including conv5 frozen (Fig-\nure 5). We ﬁnd that the learned representation is surpris-\ningly resilient to both kinds of degradation. Even with large,\nsystematic truncation (up to 50%) or large errors in bound-\naries, the representation maintains its quality.\nHow much data do we need?\nWe vary the amount of\ndata available for training, and evaluate the resulting rep-\nresentation on object detection using Fast-RCNN with all\nconv layers frozen. The results are shown in the third plot\nin Figure 5. We ﬁnd that performance drops signiﬁcantly as\nthe amount of training data is reduced, suggesting that good\nrepresentations will need large amounts of data.\nIn summary, these results suggest that training for seg-\nmentation leads to strong features even with imprecise ob-\nject masks. However, building a good representation re-\nquires signiﬁcant amounts of training data. These observa-\ntions strengthen our case for learning features in an unsu-\npervised manner on large unlabeled datasets.\n4\nFigure 6. From left to right: a video frame, the output of uNLC\nthat we use to train our ConvNet, and the output of our ConvNet.\nuNLC is able to highlight the moving object even in potentially\ncluttered scenes, but is often noisy, and sometimes fails (last two\nrows). Nevertheless, our ConvNet can still learn from this noisy\ndata and produce signiﬁcantly better and smoother segmentations.\n5. Learning by Watching Objects Move\nWe ﬁrst describe the motion segmentation algorithm we\nuse to segment videos, and then discuss how we use the\nsegmented frames to train a ConvNet.\n5.1. Unsupervised Motion Segmentation\nThe key idea behind motion segmentation is that if there\nis a single object moving with respect to the background\nthrough the entire video, then pixels on the object will move\ndifferently from pixels on the background. Analyzing the\noptical ﬂow should therefore provide hints about which pix-\nels belong to the foreground. However, since only a part of\nthe object might move in each frame, this information needs\nto be aggregated across multiple frames.\nWe adopt the NLC approach from Faktor and Irani [12].\nWhile NLC is unsupervised with respect to video segmenta-\ntion, it utilizes an edge detector that was trained on labeled\nedge images [39]. In order to have a purely unsupervised\nmethod, we replace the trained edge detector in NLC with\nunsupervised superpixels. To avoid confusion, we call our\nimplementation of NLC as uNLC. First uNLC computes a\nper-frame saliency map based on motion by looking for ei-\nther pixels that move in a mostly static frame or, if the frame\ncontains signiﬁcant motion, pixels that move in a direction\ndifferent from the dominant one. Per-pixel saliency is then\naveraged over superpixels [1].\nNext, a nearest neighbor\ngraph is computed over the superpixels in the video using\nlocation and appearance (color histograms and HOG [6]) as\nfeatures. Finally, it uses a nearest neighbor voting scheme\nto propagate the saliency across frames.\nFigure 7. Examples of segmentations produced by our ConvNet on\nheld out images. The ConvNet is able to identify the motile object\n(or objects) and segment it out from a single frame. Masks are not\nperfect but they do capture the general object shape.\nWe ﬁnd that uNLC often fails on videos in the wild.\nSometimes this is because the assumption of there being a\nsingle moving object in the video is not satisﬁed, especially\nin long videos made up of multiple shots showing differ-\nent objects. We use a publicly available appearance-based\nshot detection method [40] (also unsupervised) to divide the\nvideo into shots and run uNLC separately on each shot.\nVideos in the wild are also often low resolution and\nhave compression artifacts, which can degrade the result-\ning segmentations. From our experiments using strong su-\npervision, we know our approach can be robust to such\nnoise. Nevertheless, since a large video dataset comprises\na massive collection of frames, we simply discard badly\nsegmented frames based on two heuristics.\nSpeciﬁcally,\nwe discard: (1) frames with too many (>80%) or too few\n(<10%) pixels marked as foreground; (2) frames with too\nmany pixels (>10%) within 5% of the frame border that are\nmarked as foreground. In preliminary tests, we found that\nresults were not sensitive to the precise thresholds used.\nWe ran uNLC on videos from YFCC100m [43], which\ncontains about 700,000 videos. After pruning, we ended\nup with 205,000 videos. We sampled 5-10 frames per shot\nfrom each video to create our dataset of 1.6M images, so we\nhave slightly more frames than images in ImageNet. How-\never, note that our frames come from fewer videos and are\ntherefore more correlated than images from ImageNet.\nWe stress that our approach in generating this dataset\nis completely unsupervised, and does not use any form of\nsupervised learning in any part of the pipeline. The code\nfor the segmentation and pruning, together with our auto-\nmatically generated dataset of frames and segments, will be\nmade publicly available soon.\n5\nOur motion segmentation approach is far from state-of-\nthe-art, as can be seen by the noisy segments shown in Fig-\nure 6. Nevertheless, we ﬁnd that our representation is quite\nresilient to this noise (as shown below). As such, we did not\naim to improve the particulars of our motion segmentation.\n5.2. Learning to Segment from Noisy Labels\nAs before, we feed the ConvNet cropped images, jit-\ntered in scale and translation, and ask it to predict the motile\nforeground object. Since the motion segmentation output is\nnoisy, we do not trust the absolute foreground probabilities\nit provides. Instead, we convert it into a trimap representa-\ntion in which pixels with a probability <0.4 are marked as\nnegative samples, those with a probability >0.7 are marked\nas positives, and the remaining pixels are marked as “don’t\ncares” (in preliminary experiments, our results were found\nto be robust to these thresholds). The ConvNet is trained\nwith a logistic loss only on the positive and negative pixels;\ndon’t care pixels are ignored. Similar techniques have been\nsuccessfully explored earlier in segmentation [3,22].\nDespite the steps we take to get good segments, the\nuNLC output is still noisy and often grossly incorrect, as\ncan be seen from the second column of Figure 6. However,\nif there are no systematic errors, then these motion-based\nsegments can be seen as perturbations about a true latent\nsegmentation. Because a ConvNet has ﬁnite capacity, it will\nnot be able to ﬁt the noise perfectly and might instead learn\nsomething closer to the underlying correct segmentation.\nSome positive evidence for this can be seen in the output\nof the trained ConvNet on its training images (Fig. 6, third\ncolumn). The ConvNet correctly identiﬁes the motile object\nand its rough shape, leading to a smoother, more correct\nsegmentation than the original motion segmentation.\nThe ConvNet is also able to generalize to unseen images.\nFigure 7 shows the output of the ConvNet on frames from\nthe DAVIS [36], FBMS [31] and VSB [13] datasets, which\nwere not used in training. Again, it is able to identify the\nmoving object and its rough shape from just a single frame.\nWhen evaluated against human annotated segments in these\ndatasets, we ﬁnd that the ConvNet’s output is signiﬁcantly\nbetter than the uNLC segmentation output as shown below:\nMetric\nuNLC\nConvNet (unsupervised)\nMean IoU (%)\n13.1\n24.8\nPrecision (%)\n15.4\n29.9\nRecall (%)\n45.8\n59.3\nThese results conﬁrm our earlier ﬁnding that the Con-\nvNet is able to learn well even from noisy and often incor-\nrect ground truth. However, the goal of this paper is not\nsegmentation, but representation learning. We evaluate the\nlearned representation in the next section.\n6. Evaluating the Learned Representation\n6.1. Transfer to Object Detection\nWe ﬁrst evaluate our representation on the task of object\ndetection using Fast R-CNN. We use VOC 2007 for cross-\nvalidation: we pick an appropriate learning rate for each\nmethod out of a set of 3 values {0.001, 0.002 and 0.003}.\nFinally, we train on VOC 2012 train and test on VOC 2012\nval exactly once. We use multi-scale training and testing\nand discard difﬁcult objects during training.\nWe present results with the ConvNet parameters frozen\nto different extents. As discussed in Section 3, a good repre-\nsentation should work well both as an initialization to ﬁne-\ntuning and also when most of the ConvNet is frozen.\nWe compare our approach to ConvNet representations\nproduced by recent prior work on unsupervised learn-\ning [2, 8, 10, 30, 33, 35, 46, 51]. We use publicly available\nmodels for all methods shown. Like our ConvNet represen-\ntation, all models have the AlexNet architecture, but differ\nin minor details such as the presence of batch normalization\nlayers [8] or the presence of grouped convolutions [51].\nWe also compare to two models trained with strong\nsupervision.\nThe ﬁrst is trained on ImageNet classiﬁca-\ntion. The second is trained on manually-annotated segments\n(without class labels) from COCO (see Section 4).\nResults are shown in Figure 8(a) (left) and Table 1 (left).\nWe ﬁnd that our representation learned from unsupervised\nmotion segmentation performs on par or better than prior\nwork on unsupervised learning across all scenarios.\nAs we saw in Section 4.2, in contrast to ImageNet super-\nvised representations, the representations learned by previ-\nous unsupervised approaches show a large decay in perfor-\nmance as more layers are frozen, owing to the representa-\ntion becoming highly speciﬁc to the pretext task. Similar\nto our supervised approach trained on segmentations from\nCOCO, we ﬁnd that our unsupervised approach trained on\nmotion segmentation also shows stable performance as the\nlayers are frozen. Thus, unlike prior work on unsupervised\nlearning, the upper layers in our representation learn high-\nlevel abstract concepts that are useful for recognition.\nIt is possible that some of the differences between our\nmethod and prior work are because the training data is from\ndifferent domains (YFCC100m videos vs. ImageNet im-\nages). To control for this, we retrained the model from [8]\non frames from our video dataset (see Context-videos in Ta-\nble 1). The two variants perform similarly: 33.4% mean AP\nwhen trained on YFCC with conv5 and below frozen com-\npared to 33.2% for the ImageNet version. This conﬁrms\nthat the different image sources do not explain our gains.\n6.2. Low-shot Transfer\nA good representation should also aid learning when\ntraining data is scarce, as we motivated in Section 3. Fig-\n6\nFull train set\n150 image set\nMethod\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\n#wins\nSupervised\nImagenet\n56.5\n57.0\n57.1\n57.1\n55.6\n52.5\n17.7\n19.1\n19.7\n20.3\n20.9\n19.6\nNA\nSup. Masks (Ours)\n51.7\n51.8\n52.7\n52.2\n52.0\n47.5\n13.6\n13.8\n15.5\n17.6\n18.1\n15.1\nNA\nUnsupervised\nJigsaw‡ [30]\n49.0\n50.0\n48.9\n47.7\n45.8\n37.1\n5.9\n8.7\n8.8\n10.1\n9.9\n7.9\nNA\nKmeans [23]\n42.8\n42.2\n40.3\n37.1\n32.4\n26.0\n4.1\n4.9\n5.0\n4.5\n4.2\n4.0\n0\nEgomotion [2]\n37.4\n36.9\n34.4\n28.9\n24.1\n17.1\n–\n–\n–\n–\n–\n–\n0\nInpainting [35]\n39.1\n36.4\n34.1\n29.4\n24.8\n13.4\n–\n–\n–\n–\n–\n–\n0\nTracking-gray [46]\n43.5\n44.6\n44.6\n44.2\n41.5\n35.7\n3.7\n5.7\n7.4\n9.0\n9.4\n9.0\n0\nSounds [33]\n42.9\n42.3\n40.6\n37.1\n32.0\n26.5\n5.4\n5.1\n5.0\n4.8\n4.0\n3.5\n0\nBiGAN [10]\n44.9\n44.6\n44.7\n42.4\n38.4\n29.4\n4.9\n6.1\n7.3\n7.6\n7.1\n4.6\n0\nColorization [51]\n44.5\n44.9\n44.7\n44.4\n42.6\n38.0\n6.1\n7.9\n8.6\n10.6\n10.7\n9.9\n0\nSplit-Brain Auto [52]\n43.8\n45.6\n45.6\n46.1\n44.1\n37.6\n3.5\n7.9\n9.6\n10.2\n11.0\n10.0\n0\nContext [8]\n49.9\n48.8\n44.4\n44.3\n42.1\n33.2\n6.7\n10.2\n9.2\n9.5\n9.4\n8.7\n3\nContext-videos† [8]\n47.8\n47.9\n46.6\n47.2\n44.3\n33.4\n6.6\n9.2\n10.7\n12.2\n11.2\n9.0\n1\nMotion Masks (Ours)\n48.6\n48.2\n48.3\n47.0\n45.8\n40.3\n10.2\n10.2\n11.7\n12.5\n13.3\n11.0\n9\nTable 1. Object detection AP (%) on PASCAL VOC 2012 using Fast R-CNN with various pretrained ConvNets. All models are trained on\ntrain and tested on val using consistent Fast R-CNN settings. ‘–’ means training didn’t converge due to insufﬁcient data. Our approach\nachieves the best performance in the majority of settings. †Doersch et al. [8] trained their original context model using ImageNet images.\nThe Context-videos model is obtained by retraining their approach on our video frames from YFCC. This experiment controls for the\neffect of the distribution of training images and shows that the image domain used for training does not signiﬁcantly impact performance.\n‡Noroozi et al. [30] use a more computationally intensive ConvNet architecture (>2× longer to ﬁnetune) with a ﬁner stride at conv1,\npreventing apples-to-apples comparisons. Nevertheless, their model works signiﬁcantly worse than our representation when either layers\nare frozen or in case of limited data and is comparable to ours when network is ﬁnetuned with full training data.\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n20\n25\n30\n35\n40\n45\n50\n55\n60\n% mean AP\nObject detection (VOC 2012): Full train set\nImageNet [21]\nTracking-gray [43]\nColorization [48]\nContext [6]\nBiGAN [8]\nSounds [30]\nSup. Masks (Ours)\nMotion Masks (Ours)\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n0\n5\n10\n15\n20\n% mean AP\nObject detection (VOC 2012): 150 image set\n(a) Performance vs. Finetuning\n105\n106\n107\nNumber of frames / images\n30\n35\n40\n45\n50\n55\n60\n65\n% mean AP\nContext-videos[6]\nImageNet\nTracking-gray[43]\nSup. Masks\n(Ours)\nObject detection (VOC 2007)\nMotion Masks (Ours)\n(b) Performance vs. Data\nFigure 8. Results on object detection using Fast R-CNN. (a) VOC 2012 object detection results when the ConvNet representation is frozen\nto different extents. We compare to other unsupervised and supervised approaches. Left: using the full training set. Right: using only\n150 training images (note the different y-axis scales). (b) Variation of representation quality (mean AP on VOC 2007 object detection with\nconv5 and below frozen) with number of training frames. A few other methods are also shown. Context-videos [8] is the representation of\nDoersch et al. [8] retrained on our video frames. Note that most other methods in Table 1 use ImageNet as their train set.\nure 8(a) (right) and Table 1 (right) show how we compare\nto other unsupervised and supervised approaches on the task\nof object detection when we have few (150) training images.\nWe observe that in this scenario it actually hurts to ﬁne-\ntune the entire network, and the best setup is to leave some\nlayers frozen. Our approach provides the best AP overall\n(achieved by freezing all layers up to and including conv4)\namong all other representations from recent unsupervised\nlearning methods by a large margin. The performance in\nother low-shot settings is presented in Figure 10.\nNote that in spite of its strong performance relative to\nprior unsupervised approaches, our representation learned\nwithout supervision on video trails both the strongly super-\nvised mask and ImageNet versions by a signiﬁcant margin.\nWe discuss this in the following subsection.\n6.3. Impact of Amount of Training Data\nThe quality of our representation (measured by Fast\nR-CNN performance on VOC 2007 with all conv layers\nfrozen) grows roughly logarithmically with the number of\n7\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n35\n45\n55\n65\n75\n% mean AP\nImage classification (VOC 2007)\nImageNet [21]\nTracking-gray [43]\nColorization [48]\nContext [6]\nBiGAN [8]\nSounds [30]\nSup. Masks (Ours)\nMotion Masks (Ours)\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n10\n20\n30\n40\n% mean Accuracy\nAction classification (Stanford 40)\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n15\n25\n35\n45\n% mean IoU\nSemantic Segmentation (VOC 2011)\nFigure 9. Results on image (object) classiﬁcation on VOC 2007, single-image action classiﬁcation on Stanford 40 Actions, and semantic\nsegmentation on VOC 2011. Results shown with ConvNet layers frozen to different extents (note that the metrics vary for each task).\nframes used. With 396K frames (50K videos), it is already\nbetter than prior state-of-the-art [8] trained on a million Im-\nageNet images, see Figure 8(b). With our full dataset (1.6M\nframes) accuracy increases substantially. If this logarithmic\ngrowth continues, our representation will be on par with one\ntrained on ImageNet if we use about 27M frames (or 3 to 5\nmillion videos, the same order of magnitude as the number\nof images in ImageNet). Note that frames from the same\nvideo are very correlated. We expect this number could be\nreduced with more algorithmic improvements.\n6.4. Transfer to Other Tasks\nAs discussed in Section 3, a good representation should\ngeneralize across tasks. We now show experiments for two\nother tasks: image classiﬁcation and semantic image seg-\nmentation. For image classiﬁcation, we test on both object\nand action classiﬁcation.\nImage Classiﬁcation.\nWe experimented with image clas-\nsiﬁcation on PASCAL VOC 2007 (object categories) and\nStanford 40 Actions [48] (action labels). To allow compar-\nisons to prior work [10, 51], we used random crops during\ntraining and averaged scores from 10 crops during testing\n(see [10] for details).\nWe minimally tuned some hyper-\nparameters (we increased the step size to allow longer train-\ning) on VOC 2007 validation, and used the same settings for\nboth VOC 2007 and Stanford 40 Actions. On both datasets,\nwe trained with different amounts of ﬁne-tuning as before.\nResults are in the ﬁrst two plots in Figure 9.\nSemantic Segmentation.\nWe use fully convolutional net-\nworks for semantic segmentation with the default hyper-\nparameters [28]. All the pretrained ConvNet models are\nﬁnetuned on union of images from VOC 2011 train set and\nadditional SBD train set released by Hariharan et al. [18],\nand we test on the VOC 2011 val set after removing over-\nlapping images from SBD train. The last plot in Figure 9\nshows the performance of different methods when the num-\nber of layers being ﬁnetuned is varied.\nAnalysis.\nLike object detection, all these tasks require se-\nmantic knowledge. However, while in object detection the\nConvNet is given a tight crop around the target object, the\ninput in these image classiﬁcation tasks is the entire image,\nand semantic segmentation involves running the ConvNet in\na sliding window over all locations. This difference appears\nto play a major role. Our representation was trained on ob-\nject crops, which is similar to the setup for object detection,\nbut quite different from the setups in Figure 9. This mis-\nmatch may negatively impact the performance of our repre-\nsentation, both for the version trained on motion segmenta-\ntion and the strongly supervised version. Such a mismatch\nmay also explain the low performance of the representation\ntrained by Wang et al. [46] on semantic segmentation.\nNevertheless, when the ConvNet is progressively frozen,\nour approach is a strong performer. When all layers un-\ntil conv5 are frozen, our representation is better than other\napproaches on action classiﬁcation and second only to col-\norization [51] on image classiﬁcation on VOC 2007 and\nsemantic segmentation on VOC 2011. Our higher perfor-\nmance on action classiﬁcation might be due to the fact that\nour video dataset has many people doing various actions.\n7. Discussion\nWe have presented a simple and intuitive approach to\nunsupervised learning by using segments from low-level\nmotion-based grouping to train ConvNets. Our experiments\nshow that our approach enables effective transfer especially\nwhen computational or data constraints limit the amount\nof task-speciﬁc tuning we can do. Scaling to larger video\ndatasets should allow for further improvements.\nWe noted in Figure 6 that our network learns to reﬁne\nthe noisy input segments. This is a good example of a sce-\nnario where ConvNets can learn to extract signal from large\namounts of noisy data. Combining the reﬁned, single-frame\noutput from the ConvNet with noisy motion cues extracted\nfrom the video should lead to better pseudo ground truth,\nand can be used by the ConvNet to bootstrap itself. We\nleave this direction for future work.\n8\n20\n25\n30\n35\n40\n45\n50\n55\n% mean AP\nObject detection (VOC 2012): 2800 image set\nImageNet [21]\nTracking-gray [43]\nColorization [48]\nContext [6]\nBiGAN [8]\nSounds [30]\nSup. Masks (Ours)\nMotion Masks (Ours)\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n5\n10\n15\n20\n25\n30\n35\n40\n% mean AP\nObject detection (VOC 2012): 500 image set\n15\n20\n25\n30\n35\n40\n45\n50\n% mean AP\nObject detection (VOC 2012): 1400 image set\nImageNet [21]\nTracking-gray [43]\nColorization [48]\nContext [6]\nBiGAN [8]\nSounds [30]\nSup. Masks (Ours)\nMotion Masks (Ours)\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n5\n10\n15\n20\n25\n30\n35\n% mean AP\nObject detection (VOC 2012): 300 image set\n10\n15\n20\n25\n30\n35\n40\n45\n% mean AP\nObject detection (VOC 2012): 800 image set\nImageNet [21]\nTracking-gray [43]\nColorization [48]\nContext [6]\nBiGAN [8]\nSounds [30]\nSup. Masks (Ours)\nMotion Masks (Ours)\nAll\n>c1\n>c2\n>c3\n>c4\n>c5\nLayers finetuned\n0\n5\n10\n15\n20\n25\n% mean AP\nObject detection (VOC 2012): 150 image set\nFigure 10. Results for object detection on Pascal VOC 2012 using Fast R-CNN and varying number of images available for ﬁnetuning.\nEach plot shows the comparison of different unsupervised learning methods as the number of layers being ﬁnetuned is varied. Different\nplots depict this variation for different amounts of data available for ﬁnetuning Fast R-CNN (please note the different y-axis scales for each\nplot). As the data for ﬁnetuning decreases, it is actually better to freeze more layers. Our method works well across all the settings and\nscales and as the amount of data decreases. When layers are frozen or data is limited, our method signiﬁcantly outperforms other methods.\nThis suggests that features learned in the higher layers of our model are good for recognition.\nReferences\n[1] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and\nS. S¨usstrunk. SLIC superpixels compared to state-of-the-art\nsuperpixel methods. TPAMI, 2012. 5\n[2] P. Agrawal, J. Carreira, and J. Malik. Learning to see by\nmoving. ICCV, 2015. 3, 6, 7\n[3] C. Arteta, V. Lempitsky, and A. Zisserman. Counting in the\nwild. In ECCV, 2016. 6\n[4] Y. Bengio. Learning deep architectures for AI. Foundations\nand trends in Machine Learning, 2009. 1, 2\n[5] Y. Bengio, A. Courville, and P. Vincent.\nRepresentation\nlearning: A review and new perspectives.\nTPAMI, 35(8),\n2013. 2\n[6] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. CVPR, 2005. 5\n[7] V. R. de Sa.\nLearning classiﬁcation with unlabeled data.\nNIPS, 1994. 2\n[8] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual\nrepresentation learning by context prediction. ICCV, 2015.\n1, 2, 3, 4, 6, 7, 8\n[9] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional ac-\ntivation feature for generic visual recognition. ICML, 2014.\n1\n[10] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial Fea-\nture Learning. ICLR, 2017. 2, 6, 7, 8\n[11] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky,\nO. Mastropietro, and A. Courville. Adversarially learned in-\nference. ICLR, 2017. 2\n[12] A. Faktor and M. Irani. Video Segmentation by Non-Local\nConsensus voting. BMVC, 2014. 5\n[13] F. Galasso, N. Nagaraja, T. Cardenas, T. Brox, and\nB. Schiele. A uniﬁed video segmentation benchmark: An-\nnotation, metrics and analysis. ICCV, 2013. 6\n[14] R. Garg, V. K. B.G., G. Carneiro, and I. Reid. Unsupervised\ncnn for single view depth estimation: Geometry to the res-\ncue. ECCV, 2016. 3\n[15] R. Girshick. Fast R-CNN. ICCV, 2015. 1, 3, 4\n[16] I. Goodfellow,\nJ. Pouget-Abadie,\nM. Mirza,\nB. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-\nerative adversarial nets. NIPS, 2014. 2\n[17] R. Goroshin, M. Mathieu, and Y. LeCun. Learning to lin-\nearize under uncertainty. NIPS, 2015. 1, 3\n[18] B. Hariharan, P. Arbel´aez, L. Bourdev, S. Maji, and J. Malik.\nSemantic contours from inverse detectors. ICCV, 2011. 8\n[19] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-\ncolumns for object segmentation and ﬁne-grained localiza-\ntion. CVPR, 2015. 1\n[20] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimen-\nsionality of data with neural networks. Science, 2006. 1,\n2\n[21] D. Jayaraman and K. Grauman. Learning image representa-\ntions tied to ego-motion. ICCV, 2015. 3\n9\n[22] P. Kohli, P. H. Torr, et al. Robust higher order potentials for\nenforcing label consistency. IJCV, 2009. 6\n[23] P. Kr¨ahenb¨uhl, C. Doersch, J. Donahue, and T. Darrell. Data-\ndependent initializations of convolutional neural networks.\nICLR, 2016. 7\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nIma-\ngeNet classiﬁcation with deep convolutional neural net-\nworks. NIPS, 2012. 3\n[25] G. Larsson, M. Maire, and G. Shakhnarovich. Learning rep-\nresentations for automatic colorization. ECCV, 2016. 2\n[26] Y. Li, M. Paluri, J. M. Rehg, and P. Doll´ar. Unsupervised\nlearning of edges. CVPR, 2016. 3\n[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-\nmon objects in context. ECCV, 2014. 2, 3\n[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. CVPR, 2015. 8\n[29] I. Misra, C. L. Zitnick, and M. Hebert. Shufﬂe and Learn:\nUnsupervised Learning using Temporal Order Veriﬁcation.\nECCV, 2016. 1, 3\n[30] M. Noroozi and P. Favaro. Unsupervised Learning of Visual\nRepresentations by Solving Jigsaw Puzzles. ECCV, 2016. 1,\n2, 3, 6, 7\n[31] P. Ochs, J. Malik, and T. Brox.\nSegmentation of moving\nobjects by long term video analysis. TPAMI, 36(6), 2014. 6\n[32] Y. Ostrovsky, E. Meyers, S. Ganesh, U. Mathur, and P. Sinha.\nVisual parsing after recovery from blindness. Psychological\nScience, 2009. 1\n[33] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and\nA. Torralba. Ambient sound provides supervision for visual\nlearning. ECCV, 2016. 2, 6, 7\n[34] S. E. Palmer. Vision science: Photons to phenomenology.\nMIT press, 1999. 1\n[35] D. Pathak, P. Kr¨ahenb¨uhl, J. Donahue, T. Darrell, and\nA. Efros. Context Encoders: Feature Learning by Inpaint-\ning. CVPR, 2016. 2, 6, 7\n[36] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. V. Gool,\nM. Gross, and A. Sorkine-Hornung. A Benchmark Dataset\nand Evaluation Methodology for Video Object Segmenta-\ntion. CVPR, 2016. 6\n[37] P. O. Pinheiro, R. Collobert, and P. Dollr. Learning to Seg-\nment Object Candidates. NIPS, 2015. 3\n[38] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-\ning to Reﬁne Object Segments. ECCV, 2016. 3\n[39] L. Z. Piotr Doll´ar. Structured forests for fast edge detection.\nICCV, 2013. 5\n[40] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid.\nCategory-speciﬁc video summarization. ECCV, 2014. 5\n[41] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge. IJCV, 2015. 1\n[42] E. S. Spelke. Principles of object perception. Cognitive sci-\nence, 14(1), 1990. 1\n[43] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,\nD. Poland, D. Borth, and L.-J. Li. YFCC100M: The new\ndata in multimedia research. Communications of the ACM,\n59(2), 2016. 2, 5\n[44] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.\nExtracting and composing robust features with denoising au-\ntoencoders. ICML, 2008. 1, 2\n[45] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An uncer-\ntain future: Forecasting from static images using variational\nautoencoders. ECCV, 2016. 3\n[46] X. Wang and A. Gupta. Unsupervised learning of visual rep-\nresentations using videos. ICCV, 2015. 1, 2, 6, 7, 8\n[47] M. Wertheimer. Laws of organization in perceptual forms.\n1938. 1\n[48] B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and L. Fei-\nFei. Human action recognition by learning bases of action\nattributes and parts. ICCV, 2011. 8\n[49] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-\nferable are features in deep neural networks?\nNIPS, 2014.\n4\n[50] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-\nbased R-CNNs for ﬁne-grained category detection. ECCV,\n2014. 1\n[51] R. Zhang, P. Isola, and A. A. Efros. Colorful Image Col-\norization. ECCV, 2016. 2, 6, 7, 8\n[52] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders:\nUnsupervised learning by cross-channel prediction. CVPR,\n2017. 2, 7\n10\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2016-12-19",
  "updated": "2017-04-12"
}