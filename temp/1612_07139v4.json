{
  "id": "http://arxiv.org/abs/1612.07139v4",
  "title": "A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation",
  "authors": [
    "Lei Tai",
    "Jingwei Zhang",
    "Ming Liu",
    "Joschka Boedecker",
    "Wolfram Burgard"
  ],
  "abstract": "Deep learning techniques have been widely applied, achieving state-of-the-art\nresults in various fields of study. This survey focuses on deep learning\nsolutions that target learning control policies for robotics applications. We\ncarry out our discussions on the two main paradigms for learning control with\ndeep networks: deep reinforcement learning and imitation learning. For deep\nreinforcement learning (DRL), we begin from traditional reinforcement learning\nalgorithms, showing how they are extended to the deep context and effective\nmechanisms that could be added on top of the DRL algorithms. We then introduce\nrepresentative works that utilize DRL to solve navigation and manipulation\ntasks in robotics. We continue our discussion on methods addressing the\nchallenge of the reality gap for transferring DRL policies trained in\nsimulation to real-world scenarios, and summarize robotics simulation platforms\nfor conducting DRL research. For imitation leaning, we go through its three\nmain categories, behavior cloning, inverse reinforcement learning and\ngenerative adversarial imitation learning, by introducing their formulations\nand their corresponding robotics applications. Finally, we discuss the open\nchallenges and research frontiers.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nA Survey of Deep Network Solutions\nfor Learning Control in Robotics:\nFrom Reinforcement to Imitation\nLei Tai∗1, Jingwei Zhang∗2, Ming Liu1, Joschka Boedecker2, Wolfram Burgard2,\nAbstract—Deep learning techniques have been widely applied,\nachieving state-of-the-art results in various ﬁelds of study. This\nsurvey focuses on deep learning solutions that target learning\ncontrol policies for robotics applications. We carry out our\ndiscussions on the two main paradigms for learning control\nwith deep networks: Deep Reinforcement Learning and Imitation\nLearning. For Deep Reinforcement Learning (DRL), we begin\nfrom traditional reinforcement learning algorithms, showing how\nthey are extended to the deep context and effective mechanisms\nthat could be added on top of the DRL algorithms. We then\nintroduce representative works that utilize DRL to solve nav-\nigation and manipulation tasks in robotics. We continue our\ndiscussion on methods addressing the challenge of the reality\ngap for transferring DRL policies trained in simulation to real-\nworld scenarios, and summarize robotics simulation platforms for\nconducting DRL research. For Imitation Leaning, we go through\nits three main categories, behavior cloning, inverse reinforce-\nment learning and generative adversarial imitation learning, by\nintroducing their formulations and their corresponding robotics\napplications. Finally, we discuss the open challenges and research\nfrontiers.\nIndex Terms—Deep Learning, Robotics, Deep Reinforcement\nLearning, Imitation Learning.\nI. INTRODUCTION\nA. Deep Learning\nDeep learning, as a solution for artiﬁcial intelligence that is\ncapable of building progressively more abstract representations\nof input data, plays an essential role in various ﬁelds of study\n(Goodfellow et al., 2016).\nFrom image classiﬁcation (Krizhevsky et al., 2012; He et al.,\n2016; Huang et al., 2017), to semantic segmentation (Long\net al., 2015; Chen et al., 2016), from playing Atari games at the\nhuman-level with only pixel inputs (Mnih et al., 2015, 2016),\nto learning policies capable of driving real robotic systems in\nnavigation (Zhu et al., 2017b; Zhang et al., 2017a; Tai et al.,\n2017) and manipulation (Levine et al., 2016; Yu et al., 2018)\ntasks, the learning power of deep networks drives the state-of-\nthe-art in various research directions (Schmidhuber, 2015).\nRecent years have witnessed a rapidly growing trend of\nutilizing deep learning techniques for robotics tasks. Replac-\ning hand-crafted features with learned hierarchical distributed\n*indicates equal contribution.\n1Lei Tai and Ming Liu are with The Hong Kong University of Science and\nTechnology. {ltai, eelium}ust.hk\n2Jingwei\nZhang,\nJoschka\nBoedecker\nand\nWolfram\nBurgard\nare\nwith\nUniversity\nof\nFreiburg.\n{zhang, jboedeck,\nburgard}@informatik.uni-freiburg.de\ndeep features, learning control policies directly from high-\ndimensional sensory inputs, the robotics community is making\nsolid progress towards building fully autonomous intelligent\nsystems.\nB. Deep Learning for Robotics: From Perception to Control\nAutonomous intelligent robotics systems require two essen-\ntial building blocks: perception and control.\nThe perception pipeline can be viewed as a passive proce-\ndure: intelligent agents receive observations from the environ-\nment, then infer desired properties or detect target quantities\nfrom those sensory inputs. We refer readers to Deng (2014)\nand Guo et al. (2016) for a comprehensive overview of\ndeep learning techniques for perception. Compared with pure\nperception, the problem of control for autonomous agents\ngoes one step further, seeking to actively interact with or\ninﬂuence the environment by conducting sequences of actions.\nThis active nature leads to the following major distinctions\nbetween perception and control, in terms of deep learning\nbased approaches:\nData distribution: When learning perception through su-\npervised learning techniques, the training datasets are collected\nand labeled before the learning phase begins. In this case,\nthe data points can be viewed as being independently and\nidentically distributed (i.i.d), such that a direct mapping from\nthe input to the labels can be learned via standard stochastic\ngradient descent methods and variants. In contrast, for control,\nthe datasets are collected in an online manner, which makes the\ndata points sequential in nature: the consecutive observations\nreceived by the agent are temporally correlated since the agent\nactively inﬂuences the data distribution by the actions it takes.\nIgnoring this underlying temporal correlation would lead to\ncompounding errors (Bagnell, 2015).\nSupervision signal: The supervision for learning perception\nis often direct and strong, in that each training sample is\nprovided along with its ground truth label. In control tasks, on\nthe other hand, either only sparse reward signals are available\nwhen learning behaviors through deep reinforcement learning,\nor the feedback is often delayed and not instantaneous, even\nwhen demonstrations from experts are provided in the scenario\nof imitation learning, since the credit for achieving a certain\ngoal needs to be correctly assigned to all the actions taken\nalong the trajectory.\nData collection: As discussed before, the dataset for percep-\ntion can be collected off-line, while the dataset for control has\narXiv:1612.07139v4  [cs.RO]  9 Apr 2018\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nto be collected in an on-line manner, since actions are actively\ninvolved in the learning process. This greatly limits the number\nof samples one can collect, since executing actions in the\nreal world with real robotics systems is a relatively expensive\nprocedure. In cases where the control policies are trained in\nsimulation, the problem of the reality gap arises when they\nare deployed in real-world scenarios, where the discrepancies\nbetween the modalities of the synthetic renderings and the real\nsensory readings impose major challenges.\nRecognizing those distinctions, various deep learning based\nalgorithms have been proposed to solve control for robotics. In\nthis survey, we review the deep learning approaches for control\ntasks based on their underlying learning paradigms, and we\ncarry out our discussion through the following sections:\n• Sec. II Deep Reinforcement Learning\n– Sec. II-A RL Overview\n– Sec. II-B RL Algorithms\n– Sec. II-C DRL Algorithms\n– Sec. II-D DRL Mechanisms\n– Sec. II-E DRL for Navigation\n– Sec. II-F DRL for Manipulation\n– Sec. II-G The Reality Gap: From Simulation to the\nReal World\n– Sec. II-H Simulation Platforms\n• Sec. III Imitation Learning\n– Sec. III-A Behavior Cloning\n– Sec. III-B Inverse Reinforcement Learning\n– Sec. III-C Generative Adversarial Imitation Learning\nII. DEEP REINFORCEMENT LEARNING\nBeing the ﬁrst to stabilize large-scale reinforcement learning\nwith deep convolutional neural networks as function approx-\nimators, deep Q-networks (DQN) (Mnih et al., 2015) have\nbrought increased research and applications of deep reinforce-\nment learning (DRL) methods. In the following we ﬁrst review\nthe basic concepts and algorithms in traditional reinforcement\nlearning (RL). Then we continue to the several most inﬂuential\nDRL algorithms and mechanisms, on the basis of which we\ndiscuss DRL solutions for robotics control, with an emphasis\non navigation and manipulation applications.\nA. RL Overview\nWe formalize a robotics task (e.g., navigation, manipulation)\nas a Markov Decision Process (MDP), in which the agent\ninteracts with the environment through a sequence of obser-\nvations, actions, and reward signals. An MDP is a 5−tuple\n⟨S, A, P, R, γ⟩:\n• S: set of all states.\n• A: set of all actions.\n• P: the transition dynamics, where P(s′|s, a) deﬁnes the\ndistribution of the next state s′ by taking action a in state\ns, where s, s′ ∈S, a ∈A. We also denote the initial state\ndistribution P(s0) as ρ0.\n• R: set of all possible rewards. In the following, we denote\nthe instantaneous scalar reward received by the agent by\ntaking action at from state st as Rt+1(st, at), and use\nRt+1 as short for Rt+1(st, at). There also exist other\ndeﬁnitions of the reward function that depend only on\nthe state itself, in which R(s) refers to the reward signal\nthat the agent receives by arriving at state s. In some of\nthe following discussions, the negative counterpart of the\nreward function, the cost function, is used, and is denoted\nas c(s).\n• γ: a discount factor in the range of [0, 1].\nreward\naction\nstate\nAgents\nEnvironments\nFig. 1. The reinforcement learning loop in the context of robotics. In state\nst, the autonomous agent takes and action at, receives a reward Rt+1, and\ntransits to the next state st+1.\nIn an MDP, the agent takes an action at in state st, receives\na reward Rt+1, and transits to the next state st+1 following the\ntransition dynamics P(st+1|st, at). This process in the context\nof robotics is depicted in Fig.1.\nIn robotics, we mainly consider episodic MDPs, where there\nexists a terminal state (e.g., a mobile ground vehicle reaches\na certain goal location, a manipulator successfully grabs a red\ncup) that, once reached, terminates the current episode. Also\nfor an episodic MDP with a time horizon of T, an episode will\nstill be terminated after a maximum of T time steps, even if\nby then the terminal state has not yet been reached.\nAnother point worth mentioning is the partial observability.\nIn a robotics task, an autonomous agent perceives the world\nwith its onboard sensor (e.g., RGB/depth camera, IMU, laser\nrange sensor, 3D Lidar), receiving one observation per time\nstep. However, simply representing st by xt often does not\nsatisfy the Markov property: one such sensor reading can\nhardly capture all the necessary information for the agent to\nmake decisions in the future, in which case the underlying\nprocedure is called a Partial Observeble MDP (POMDP). This\nis often dealt with by either stacking several (e.g, N) consec-\nutive observations {xt−N+1, xt−N+2, . . . , xt} to represent st,\nor by feeding xt into a recurrent neural network instead of a\nfeed forward one, such that the past information is naturally\naccounted with (e.g., by the cell state when using the long\nshort-term memories (LSTMs).\nReinforcement learning agents are designed to learn from\ninteractions how to behave to achieve a certain goal (Sutton\nand Barto, 1998). More precisely, here the objective of learn-\ning is to maximize the expected discounted return, where the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\ndiscounted return is deﬁned as follows:\nGt = Rt+1 + γRt+2 + γ2Rt+3 + · · · + γT −t−1RT\n(1)\n=\nT\nX\nk=t\nγk−tRk+1.\n(2)\nTo solve control, two important deﬁnitions are introduced:\n• Policies: π, µ\n– π(a|s): stochastic policy, where actions are drawn\nfrom a probability distribution deﬁned by π(a|s).\n– µ(s): determinstic policy, where actions are deter-\nministically selected for a given state s.\n• Value functions: V, Q\n– V π(s): state-value function, deﬁned as the expected\nreturn when starting from state s and following\npolicy π thereafter:\nV π(s) = Eπ [Gt|st = s]\n(3)\n= Eπ\n\" T\nX\nk=t\nγk−tRk+1|st = s\n#\n.\n(4)\n– Qπ(s, a): action-value function, deﬁned as the ex-\npected return by taking the action a from state s,\nthen following π thereafter:\nQπ(s, a) = Eπ [Gt|st = s, at = a]\n(5)\n= Eπ\n\" T\nX\nk=t\nγk−tRk+1|st = s, at = a\n#\n.\n(6)\n– Q∗(s, a): optimal value function (We omit the case\nfor state-value function V here since the action-value\nfunction Q is a much more effective representation\nfor control.):\nQ∗(s, a) = max\nπ\nQπ(s, a).\n(7)\n– π∗(a|s): optimal policy:\nπ∗(a|s) = argmax\na\nQ∗(s, a).\n(8)\nB. RL Algorithms\nWith the deﬁnitions of the core components, we now\ncontinue to discuss the different classes of RL algorithms. We\nemphasize those methods that have been extended with deep\nlearning variants.\n1) Value-based Methods: These methods are based on esti-\nmating the values of being in a given state, then extracting the\ncontrol policies from the estimated values. The recursive value\nestimation procedures are based on the Bellman Equations.\nBelow, we list the Bellman Expectation Equation (Eq. 9) and\nthe Bellman Optimality Equation (Eq. 10):\nQπ(s, a) = Eπ [Rt+1 + γQπ(st+1, at+1)|st = s, at = a] ,\n(9)\nQ∗(s, a) = E\nh\nRt+1 + γ max\na′ Q∗(st+1, a′)|st = s, at = a\ni\n.\n(10)\nFollowing the formulations of Eq. 9 and Eq. 10 respectively,\nwe have the two most well-known value-based RL methods:\nSARSA and Q-learning, which follow the same recursive\nbackup procedures, given as follows:\nQπ(st, at) ←Qπ(st, at) + αδt,\n(11)\nδt = yt −Qπ(st, at).\n(12)\nIn this estimation procedure, Q-values are recursively up-\ndated by a step size of α towards a target value yt. δt is termed\nthe td-error (temporal difference error), and yt the td-target.\nThe difference between SARSA and Q-learing comes in their\ntd-target’s. Below, we list the td-targets for SARSA and Q-\nlearing in Eq. 13 and Eq. 14 respectively:\nySARSA\nt\n= Rt+1 + γQπ(st+1, at+1),\n(13)\nyQ-learning\nt\n= Rt+1 + γ max\na′ Qπ(st+1, a′).\n(14)\nSARSA updates its Q-value estimates using the transitions\ngenerated by following the behavioural policy π, which makes\nSARSA an on-policy method; Q-learning, on the other hand, is\noff-policy, since its value estimations are updated not towards\nthe behavioural policy, but towards a target optimal policy.\nThere are also other value-based methods, such as Monte-\nCarlo control, which uses the true return of complete trajec-\ntories as its update target instead of bootstrapping from old\nestimates, and λ-variants, which mix the sample return and\n1-step lookahead estimations.\nA reformulation of the Q-value function, the successor\nrepresentation (Dayan, 1993), is also studied in the recent\nliterature (Kulkarni et al., 2016; Barreto et al., 2017; Zhang\net al., 2017a):\nRt+1(st, at) = φ(st, at)⊤· ω,\n(15)\nQπ(s, a) = ψπ(s, a)⊤· ω,\n(16)\nwhere\nψπ(s, a) = Eπ\n\" T\nX\nk=t\nγk−tφ(sk, ak)|st = s, at = a\n#\n,\n(17)\nis termed the successor feature. This line of formulation de-\ncouples the task speciﬁc reward estimation into the estimation\nof representative features φ(·) and a reward weight ω, and\nthe estimation of the expected occurrence of the features φ(·)\nunder speciﬁc world dynamics following a speciﬁc policy. It\ncombines the computational efﬁciency of model-free methods\nwith the ﬂexibility of some model-based methods. We refer\nreaders to Dayan (1993), Kulkarni et al. (2016), Barreto et al.\n(2017) and Zhang et al. (2017a) for more detailed discussions\nand extensions.\n2) Policy-based Methods: Unlike value-based methods,\npolicy-based methods do not maintain value estimations, but\nwork directly on policies. When it comes to high-dimensional\nor continuous action spaces, policy-based methods generally\ngive much more effective solutions than value-based ap-\nproaches. They can learn stochastic policies instead of just\ndeterministic policies, and have better convergence properties.\nPolicy-based approaches operate on parameterized policies,\nand search for parameters that maximize the policy objec-\ntive function. The policy search can be carried out in two\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nparadigms: gradient-free (Fu et al., 2005; Szita and L¨orincz,\n2006) and gradient-based. We focus on the gradient descent\nmethods from the gradient-based family as they remain the\nmethod of choice in recent studies. More formally, given\npolicy πθ(·) with parameters θ, policy optimization searches\nfor the best θ that maximizes an objective function J (πθ):\nJ (πθ) = Eπθ[fπθ(·)].\n(18)\nHere, fπθ(·) is a score function, which judges the goodness\nof a policy. There are multiple valid choices for the score\nfunction; we refer readers to Schulman et al. (2015b) for a\nfull discussion.\nThe policy gradient is deﬁned as\n∇θJ (πθ) = Eπθ [∇θ log πθ · fπθ(·)] .\n(19)\nIntuitively speaking, ﬁrstly, some actions, experiences or\ntrajectories are sampled following the current policy πθ and\nthe goodness of those samples is given by fπθ(·), the score\nfunction and ∇θ log πθ points out the direction in the param-\neter space that would lead to an increase of the probability\nof those actions being sampled. Thus, by ascending along the\npolicy gradient given in Eq. 19, we end up with policies that\nare capable of generating samples with higher scores.\nThe standard REINFORCE algorithm (Williams, 1992), a\nwell-known method in RL, plugs in the sample return as the\nscore function:\nfπθ(·) = Gt.\n(20)\nThis algorithm, however, suffers from the very high vari-\nance. A common way to reduce the variance of the estimation\nwhile keeping it unbiased is by subtracting a baseline b(s)\nfrom the return:\nfπθ(·) = Gt −bt(st).\n(21)\nA commonly used baseline is a learned estimate of the state-\nvalue function V (s). This leads us to the actor-critic class of\nalgorithms, since it involves estimating the value functions\nalong with policy search.\nBefore we go into actor-critic methods, several details are\nworthy of pointing out.\nFirstly, directly following the policy gradient might not be\ndesirable in the robotics setting, since hardware constraints and\nsafety requirements should be carefully dealt with. Popular\napproaches for cautious exploration include avoiding signiﬁ-\ncant changes in the policy, or explicitly discouraging entering\nundesired regions in the state space (Deisenroth et al., 2013).\nWe also note that, so far, we have only been discussing the\npolicy gradient for the stochasitic polices, which integrate over\nboth the state and action spaces, and might not be efﬁcient\nin high-dimentional action spaces. The deterministic policy\ngradient (Silver et al., 2014), on the other hand, only requires\nintegrating over the state space, which makes it a much more\nsample-efﬁcient algorithm. Below we list the stochastic policy\ngradient (for πθ(a|s), Eq. 22) and the deterministic policy\ngradient (for µθ(s), Eq. 23) when using the Q-value function\nas their score function:\n∇θJ (πθ) = Es,a [∇θ log πθ(a|s) · Qπ(s, a)] ,\n(22)\n∇θJ (µθ) = Es [∇θµθ(s) · Qµ(s, µθ(s))] .\n(23)\n3) Actor-critic Methods: Following on from the discus-\nsions of the policy-based methods, actor-critic algorithms\nmaintain an explicit representation of both the policy (the\nactor) and the value estimates (the critic). The most widely\nused actor-critic algorithms use the following score function:\nfπθ(·) = Qπθ(st, at) −V πθ(st).\n(24)\nCompared againest Eq. 21, Eq. 24 replaces the return Gt\nwith its unbiased estimate Qπθ(st, at), and uses V πθ(st) as\nits baseline function to reduce variance. In fact,\nA(s, a) = Q(s, a) −V (s)\n(25)\nis called the advantage function, which estimates the advan-\ntage of taking a particular action a in state s.\n4) Integraing Planning and Learning: So far, we have\nbeen discussing model-free methods where the agent is not\nprovided with the underlying transition model and simply\nlearns optimal behaviors from experiences. There also exists\nanother branch of model-based algorithms where a model is\nlearned from experiences. with which the agent can interact\nand collect imaginary rollouts (Sutton, 1991), It has also been\nextended with DRL methods (Weber et al., 2017; Kalweit and\nBoedecker, 2017). However, the need for learning a model\nbrings in another source of approximation error, and model-\nbased RL can only perform as well as the estimated model.\nThis problem might be partially dealt with by Model Predicted\nControl (MPC) methods, which are not a focus of this survey,\nso we will skip the details.\nC. DRL Algorithms\nRecent successes of DRL have extended the aforementioned\nalgorithms to the high-dimensional domain, by deploying deep\nneural networks as powerful non-linear function approximators\nfor the optimal value functions V ∗(s), Q∗(s, a), A∗(s, a), and\nthe optimal policies π∗(a|s), µ∗(s). They usually take the ob-\nservations as input (e.g, raw pixel images from Atari emulators\n(Mnih et al., 2015) or joint angles of robot arms ()), and output\neither the Q-values, from which greedy actions are selected,\nor policies that can be directly used to execute agents. In the\nfollowing, we cover the most inﬂuential DRL algorithms.\n1) DQN (Mnih et al., 2015): : As a value-based method,\nDQN approximates the optimal Q-value function with a deep\nconvolutional neural network, called the deep Q-network,\nwhose weights we denote as θQ: Q(s, a; θQ) ≈Q∗(s, a). In\nturn, the td-error (Eq. 12) and the td-target (Eq. 14) from the\nstandard Q-learning are adopted into:\nδDQN\nt\n= yDQN\nt\n−Q(st, at; θQ\nt ),\n(26)\nyDQN\nt\n= Rt+1 + γ max\na′ Q(st+1, a′; θ−\nt ).\n(27)\nThen an update step is performed based on the following\ngradient calculation with a learning rate of α:\nθt+1 ←θt −α ·\n\u0012\n∂\n\u0010\nδDQN\nt\n(θQ\nt )\n\u00112\n/∂θQ\nt\n\u0013\n.\n(28)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nTwo main techniques have been proposed in DQN to\nstabilize learning: target-network and experience replay.\nTarget-network: In Eq. 27, the td-target is computed using\nthe output from a target-network θ−, instead of the Q-network\nθQ. The target-network and the Q-network share the same\nnetwork architecture, but only the weights of the Q-network\nare learned and updated. The weights of the Q-network θQ\nare only periodically copied to the target-network θ−. This\nreduces the correlations of the estimated Q-values with the\ntarget estimations. There is also soft update (Lillicrap et al.,\n2015), where a small portion of θQ are mixed into θ−in\nevery iteration, instead of the hard update used in the original\nDQN, where θQ are directly and completely copied to θ−\nevery several (e.g, 10, 000) iterations.\nExperience replay: In this technique, instead of directly\nusing the incoming frames from the online interactions, the\ncollected experiences are ﬁrstly stored into a replay memory.\nDuring training, random samples are drawn from the replay\nmemory (4 consecutive observations are stacked together to\nform a state, so as to deal with the partial observability) to\nbe fed into the network as mini-batches. This way, gradient\ndescent methods from the supervised learning literature can\nbe safely used, to minimize the min-squared error between the\npredicted Q-values (output by the Q-network) and the target\nQ-values (output by the target-network). Experience replay\nthereby removes the temporal correlations in the consecutive\nobservations, and smoothes over changes in the online data\ndistribution.\nFurther techniques have been proposed on the basis of DQN\nto stabilize learning and improve efﬁciency: Double DQN\n(Van Hasselt et al., 2016) and Dueling DQN (Wang et al.,\n2016b).\nFor Double DQN (Van Hasselt et al., 2016), the greedy ac-\ntion is chosen based on the output from θQ (the original DQN\nuses θ−), then the target Q-value of the chosen greedy action\nis computed using θ−(Eq. 29). This prevents overoptimistic\nvalue estimates and avoids upward bias:\nyDouble\nt\n= Rt+1 + γQ(st+1, argmax\na′\nQ(st+1, a′; θQ\nt ); θ−\nt ).\n(29)\nFor Dueling DQN (Wang et al., 2016b), two output heads\nare used to estimate the state-value V and the advantage A\nrespectively for each action. This helps the agent to efﬁciently\nlearn which states are valuable, without having to learn the\neffect of each action for each state.\n2) DDPG (Lillicrap et al., 2015): : DQN can deal with\nhigh-dimensional state spaces, but is only capable of handling\ndiscrete and low-dimensional action spaces. The deep deter-\nministic policy gradient (DDPG) combines techniques from\nDQN with actor-critic methods, targeting solving continuous\ncontrol tasks from raw pixels inputs.\nIf we write out the expectation in Eq. 9 for the stochasitic\npolicy π(a|s) and deterministic policy µ(s), we get (E repre-\nsents the environment that the agent is interacting with)\nQπ(st, at) =\nERt+1,st+1∼E\n\u0002\nRt+1 + γEat+1∼π [Qπ(st+1, at+1)]\n\u0003\n,\n(30)\nQµ(st, at) =\nERt+1,st+1∼E [Rt+1 + γQµ(st+1, µ(st+1))] .\n(31)\nDDPG represents the Q-value estimates with θQ, and the\ndeterministic policy with θµ. θµ is learned via the DPG given\nin Eq. 23, and θQ is learned following Eq. 31. (Note that\ndifferent from DQN, where the dependence of the Q-value on\na is represented by outputting one value for each action, the\nQ-network in DDPG deals with this dependence by taking the\naction as input for θQ.)\n3) NAF (Gu et al., 2016): : Normalized advantage function\noffers another way to enable Q-learning in continuous action\nspaces with deep neural networks and is considerably sim-\npler than DDPG. For continuous action problems, standard\nQ-learning is not easily directly applicable, as it requires\nmaximizing a complex non-linear function for determining\nthe greedy action. The key idea in NAF is to represent the\nQ-value function Q(s, a) in such a way that its maximum\nargmaxa Q(s, a) can be easily analytically determined during\nthe Q-learning update.\nNAF uses the same techniques of target network and\nexperience replay as DQN, but differs in the network outputs.\nInstead of directly outputting the Q-value estimates, its last\nhidden layer is connected to three output heads: θV , θµ and\nθL. θV represents the state value V (s), while θµ and θL are\nused for estimating the advantage A(s, a); then Q(s, a) can\nbe computed according to Eq. 25. To give a speciﬁc example,\ne.g, if both θµ and θL are represented with linear layers, with\nthe number of outputs of θL being the square of that of θµ\n(equal to the action dimensions), then the output of θL is ﬁrst\nreshaped into a matrix, from which L(s; θL), being the lower-\ntriangular of that matrix, is extracted, with the diagonal terms\nexponentiated. Then the advantage can be estimated by\nA(s, a; θµ, θL)\n= −1\n2 (a −µ(s; θµ))T P(s; θL) (a −µ(s; θµ)) ,\n(32)\nwhere\nP(s; θL) = L(s; θL)L(s; θL)T .\n(33)\nAlthough this representation is more restritive than a general\nnetwork approximator, the greedy action for the Q-value is\nalways directly given by µ(s; θµ). An asynchronous version\nof NAF has also been proposed (Gu et al., 2017).\n4) A3C (Mnih et al., 2016): : Minh et. al. proposed several\nasynchronous DRL algorithms. They deploy multiple actor-\nlearners to collect experiences on multiple instances of the\nenvironment, while each actor-learner accumulates gradients\ncalculated from its own collected samples w.r.t. its own set of\nnetwork parameters θ; these gradients are used to update the\nweights of a shared model θ.\nThe most effective one, A3C (asynchronous advantage\nactor-critic), which has been very inﬂuential and become a\nstandard baseline in recent DRL research, maintains a policy\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nrepresentation π(a|s; θπ) and a value estimate V (s; θV ). It\nuses the advantage function as the score fucntion in its policy\ngradient, which is estimated using a mixture of n-step returns\nby each actor-learner. To be more speciﬁc, each actor-learner\nthread spawns its own copy of the environment and collects\nrollouts of experiences up to Tmax (e.g, 20) steps. After an\nactor-learner completes a segment of a rollout, it accumulates\ngradients from the experience of every time step contained\nin the rollout {0, 1, · · · , t, · · · , T} by ﬁrst estimating the\nadvantage function (e.g., for time step t) according to the\nfollowing formulation\nA(st, at; θπ, θV ) =\n\"T −1\nX\nk=t\n\u0002\nγk−tRk+1\n\u0003\n+ γT −tV (sT ; θV ) −V (st; θV ); θπ\n#\n,\n(34)\nthen calculating the corresponding gradients w.r.t. the its\ncurrent set of network parameters θπ, θV , which are then used\nto update the shared model θπ, θV :\ndθπ ←dθπ + ∇θπ log π(at|st; θπ)A(st, at; θπ, θV ),\n(35)\ndθV ←dθV + ∂A(st, at; θπ, θV )2/∂θV .\n(36)\nThe parallelization greatly stabilizes the update of the\nparameters as the samples collected by different actor-learners\nat the same time are much less correlated, which eliminates\nthe requirement for keeping a replay memory. Also by running\ndifferent exploration policies in different threads, the learners\nare very likely to explore different parts of the state space.\nDue to it being highly efﬁcient, lightweight and conceptually\nsimple, A3C is considered as a standard starting point in recent\nDRL research.\n5) A2C (Wang et al., 2016a; Wu et al., 2017): : Some\nrecent works found that the asynchrony in A3C does not\nnecessarily lead to improved performance compared to the\nsynchronous version: A2C. Different from A3C, A2C waits\nfor each actor to ﬁnish its segment of experience before\nperforming an update, which is averaged over all actors. This\ndetail allows for effective GPU implementation.\n6) GPS (Levine and Koltun, 2013): : As a model-based pol-\nicy search algorithm, Guided Policy Search (GPS) is relatively\nsample efﬁcient. GPS starts from guiding samples generated\nfrom some initial optimal control policies and augmented from\nsamples generated from the current policy, from which at every\niteration a set of training trajectories are sampled to optimize\nthe current policy with supervised learning. The updated policy\nis then added as an additional cost term to bound the change in\nthe policy, with which the trajectory optimization is performed\nagain (e.g., with an LQR solver).\n7) TRPO (Schulman et al., 2015a): : By making several\napproximations to the theoretically justiﬁed scheme, Schulman\net al. (2015a) proposed a practical algorithm for optimizing\nlarge nonlinear policies, with guaranteed monotonic improve-\nment.\nTo illustrate the algorithm, let us ﬁrst deﬁne the expected\ndiscounted cost for an inﬁnite horizon MDP, which replaces\nthe reward function R in the expected discounted return with\nthe cost function c:\nη(π) = Eπ\n\" ∞\nX\nt=0\nγtc(st)|s0 ∼ρ0\n#\n.\n(37)\nIn turn, we can rewrite the deﬁnitions for the state-value\nfunctions Eq. 4 and action-value functions Eq. 6 in terms of\nthe cost function c:\nV π(s) = Eπ\n\" ∞\nX\nk=t\nγk−tc(sk)|st = s\n#\n,\n(38)\nQπ(s, a) = Eπ\n\" ∞\nX\nk=t\nγk−tc(sk)|st = s, at = a\n#\n,\n(39)\nand in turn, we have the advantage function:\nAπ(s, a) = Qπ(s, a) −V π(s).\n(40)\nSince we are looking for a step size for the policy update\nthat can guarantee a monotonic improvement from an old\npolicy πold to an updated policy π, it is beneﬁcial to write\nthe expected cost of π in terms of that of πold, which leads to\nthe following identity:\nη(π) = η(πold) + Eπ\n\" ∞\nX\nt=0\nγtAπold(st, at)|s0 ∼ρ0\n#\n.\n(41)\nBefore continueing, we denote the (unnormalized) dis-\ncounted visitation frequencies for state s under policy π as\nρπ(s), more formally,\nρπ(s) =\n\u0000P(s0 = s) + γP(s1 = s) + γ2P(s2 = s) + · · ·\n\u0001\n=\n∞\nX\nt=0\nγtP(st = s),\n(42)\nwhere s0 ∼ρ0, and the actions are selected following π.\nNow, instead of summing over timesteps, if we sum over\nstates, Eq. 41 can be rewritten as\nη(π) = η(πold) +\nX\ns∼ρπ\nρπ(s)\nX\na∼π\nπ(a|s)Aπold(s, a).\n(43)\nThis equation indicates that η is guaranteed to decrease or\nstay constant if the expected advantage at every state has\na non-positive value. Since Eq. 43 is difﬁcult to directly\noptimize, due to the complex dependency of ρπ on π, a local\napproximation ignoring changes in the state visitation density\ninduced by the changes in the policy, that matches η to the ﬁrst\norder is introduced (the term η(π) is left out here as it does not\naffect the solution to the underlying optimization problem):\nLπold(π) =\nX\ns∼ρπold\nρπold(s)\nX\na∼π\nπ(a|s)Aπold(s, a).\n(44)\nStandard policy gradient methods ascend on the 1st order\ngradient, where an increase on Lθold(θ) does not guarantee an\nincrease in η(πθ) with large step sizes, due to the approxima-\ntions made above.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\nTRPO extends the policy improvement bound in the mixture\npolicies setting given by Kakade and Langford (2002) to\ngeneral stochastic policies, and shows that\nη(π) ≤Lπold(π) + CDmax\nKL (πold, π),\n(45)\n(46)\nwhere\nC =\n2ϵγ\n(1 −γ)2 ,\n(47)\nϵ = max\ns\n|Ea∼π [Aπold(s, a)]| .\n(48)\nThis means that by performing the following optimization\n(here we denote Lθold(θ) := Lπθold(πθ)) with parameterized\npolicies), we are guaranteed to improve the true objective η:\nminimize\nθ\n[Lθold(θ) + CDmax\nKL (πθold, πθ)] .\n(49)\nHowever, if the penalty coefﬁcient C, as calculated in Eq.\n47, is used in practice, the step sizes will be very small.\nTo deal with this, TRPO ﬁrst replaces the sum over actions\nin Eq. 44 by an importance sampling estimator (here, we only\ndiscuss the case for single path sampling, where πold is used to\ngenerate trajectories) (Aπθold is replaced by Qπθold which only\nchanges the objective by a constant, and the Q-values are to be\nreplaced by empirical estimates from sample averages, either\nsingle path or vine):\nLθold(θ) = Es∼ρπθold,a∼πθold\n\u0014 πθ(a|s)\nπθold(a|s)Aπθold(s, a)\n\u0015\n.\n(50)\nThen it turns the soft constraint in Eq. 49 into the following\nhard constraint problem:\nminimize\nθ\nEs∼ρπθold,a∼πθold\n\u0014 πθ(a|s)\nπθold(a|s)Qπθold(s, a)\n\u0015\n,\n(51)\nsubject to Es∼ρπθold [DKL(πθold(·|s)||πθ(·|s))] ≤δ.\n(52)\nwhere δ is a hyper parameter for the upper bound of the\nKL divergence between the old and the updated policy (e.g.,\nδ = 0.01). This constrained optimization problem is solved\nusing a conjugate gradient followed by a line search; we refer\nreaders to Schulman et al. (2015a) for a detailed description.\n8) PPO (Schulman et al., 2017): : Instead of reformulating\na hard constraint problem as in TRPO (Eq. 51 and 52), PPO\nsolves the original soft constraint optimization (Eq. 49) with\n1st-order SGD, adapting C according to the KL divergence.\nSince it is much simpler implementation-wise compared to\nTRPO and gives a googd performance, PPO has become the\ndefault DRL algorithm at OpenAI. A distributed version of\nPPO has also been proposed (Heess et al., 2017).\n9) ACKTR (Wu et al., 2017): : The Actor Critic Kronecker-\nFactored Trust Region (ACKTR) is a scalable trust region\nnatural gradient method for a actor-critic, with the Kronecker-\nfactored approximation to the curvature. It is more compu-\ntationally efﬁcient than TRPO, and is more sample efﬁcient\nthan those methods taking steps in the gradient direction (e.g,\nA2C) rather than the natural gradient direction.\nD. DRL Mechanisms\nMany useful mechanisms have also been proposed that can\nbe added on top of the aforementioned DRL algorithms. These\nmechanisms generally work orthogonally with the algorithms,\nand some can accelerate the DRL training by a large margin.\nBelow we list several conceptually simple yet very effective\nones.\n• Auxiliary Tasks (Mirowski et al., 2016; Jaderberg et al.,\n2016; Levine et al., 2016; Yu et al., 2018; Riedmiller\net al., 2018): Uses additional supervised or unsuper-\nvised tasks (e.g., regressing depth images from color\nimages, detecting loop closures, predicting end-effector\nposes) alongside the main reinforcement learning task,\nto compensate for the sparse supervision signals usually\nprovided to DRL agents.\n• Prioritized Experience Replay (Schaul et al., 2015b):\nPrioritizes memory replay according to td-error; can be\nadded to off-policy methods.\n• Hindsight Experience Replay (Andrychowicz et al.,\n2017): Relabels the reward for collected experiences to\nmake better use of failure trajectories, and effectively\nspeed up off-policy methods with binary or sparse reward\nstructures.\n• Curriculum Learning (Bengio et al., 2009; Florensa\net al., 2017; Zhang et al., 2017b): Presents the learning\nagent with progressively more complex task settings, such\nthat it can grasp gradually more sophasticated skills.\n• Curiosity-driven Exploration (Pathak et al., 2017):\nAugments the standard external reward with internal\nreward measured by intrinsic motivation.\n• Asymmetric Self-replay for Exploration (Sukhbaatar\net al., 2017): Drives exploration through an automatic\ncurricula generated via the interplay of two versions of\nthe same agent.\n• Noise in Parameter Space for Exploration (Fortunato\net al., 2018; Plappert et al., 2018): Pertubates network\nparameters to aid exploration.\nE. DRL for Navigation\nAutonomous navigation is one of the essential problems and\nchallenges in mobile robotics. It can roughly be described as\nthe ability of a robot to plan and follow a trajectory through the\nenvironment to reach a certain goal location without colliding\nwith any obstacles in between. The recent literature has seen\na growing number of proposed methods that tackle the task\nof autonomous navigation with DRL algorithms. Those works\nformulate the navigation problem as MDPs or POMDPs that\nﬁrst take in the sensor readings (color/depth images, laser\nscans, etc.) as observations and stack or augment them into\nstates, and then search for the optimal policy that is capable of\nguiding the agent to navigate to goal locations in a timely and\ncollision-free manner. Below we discuss several representative\nworks in this category, that target the ﬁeld of robotics.\nZhu et al. (2017b) input both the ﬁrst-person view and the\nimage of the target object to the A3C model, formulating a\ntarget-driven navigation problem based on the universal value\nfunction approximators (Schaul et al., 2015a). The training of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\ntheir model requires the features output from the pretrained\nResNet-50 (He et al., 2016), and is performed in an indoor\nsimulator (Kolve et al., 2017) where each new room is\nregarded as a new scene for which several scene-speciﬁc layers\nare added as another output head of the model. The success\nrate for generalizing the navigation policies to new targets one\nstep away from the trained targets is 70%, and around is 42%\nfor those that are two steps away. For navigation tasks with\noptimal solutions of 17.6 steps, Zhu et al. (2017b) achieved\n210.7 average trajectory lengths after being trained on 100\nmillion frames with an A3C agent. The trained policy was\nable to navigate a real robot inside an ofﬁce environment after\nbeing ﬁne-tuned on images collected from the real scene.\nZhang et al. (2017a) work on a deep successor representa-\ntion formulation (Kulkarni et al., 2016; Barreto et al., 2017)\nfor the Q-value function (Eqs. 15,16,17), targeting learning\nrepresentations that are transferrable between related naviga-\ntion tasks. Following the observation that most of the value-\nbased DRL methods, such as DQN, usually learn a black-box\nfunction approximator for the optimal value functions, which\nmakes how to transfer the knowledge gained from one task to\na related task unclear, they extend on the successor feature\nrepresentation that decouples the learning of the optimal\nvalue functions into two parts, learning task-speciﬁc reward\nfunctions, and learning task-speciﬁc features, and how those\nfeatures evolve under the current task dynamics. While this\nrepresentation has been shown to work well on transferring\nlearned policies to differently scaled reward functions and\nchanged goals in ﬁxed environments, Zhang et al. (2017a)\nextend the formulations to cope with transferring policies\nto new environments. Both experiments in a simulated 3D\nmaze with RGB inputs and real-world robotic experiments\nwith depth image inputs are presented. The trained agents,\neither pre-trained or transferred, all achieved near-optimal\nperformance, validating the ability of the proposed method\nto transfer DRL navigation policies into new environments.\nThe two methods mentioned above propose to learn naviga-\ntion policies without a requirement for performing localization\nor mapping as in the traditional planning pipelines in robotics.\nThey deal with navigating to different targets either by feeding\nthe target image as input (Zhu et al., 2017b) or by treating it\nas a transfer problem (Zhang et al., 2017a). Tai et al. (2017),\nin contrast, propose a learning-based mapless motion planner,\nunder the assumption that the relative position of the target\nw.r.t. the robot can be obtained via cheap solutions such\nas wiﬁor visible light localization, which are applicable to\nindoor robotic systems such as vacuum robots. The inputs for\nthe model is 10-dimensional laser ranges, and the network\noutputs continuous steering commands after being trained via\nan asynchronous version of the DDPG. Since the simulated\nlaser ranges and the real laser readings are quite similar,\nthe trained model is directly generalizable to indoor ofﬁce\nenvironments.\nMirowski et al. (2016) greatly improve the data efﬁciency\nand task performance of their variant of an A3C agent when\nlearning to navigate in simulated 3D mazes, by using addi-\ntional supervision signals from auxiliary tasks. In particular,\nthe learning agent is additionally supervised by losses from\ndepth prediction and loop closure classiﬁcation. Extensive ex-\nperiments are presented, validating the ability of the proposed\nagent to localize and to navigate between frequently changing\nstart and goal locations.\nThe aforementioned methods all deal with navigation in\nstatic environments. Chen et al. (2017a) propose a DRL\nbased systematic solution for socially aware navigation in\ndynamic environments with pedestrians. They extend a prior\nwork (Chen et al., 2017b), and build a robotic system for\ntheir real-world experiment, where a differential-drive mobile\nrobot is mounted with a Lidar for localization and three Intel\nRealsense’s for obstacle avoidance. From the sensor readings,\nthe speed, velocity and radius of pedestrians are estimated,\nfrom which the reward (designed based on social norms) is\ncalculated. Read robotic experiments show that the proposed\nmethod is capable of navigating agents at human walking\nspeed in a dynamic environment with many pedestrians.\nLong et al. (2017) deal with decentralized multi-agent\ncollision avoidance with PPO. They supervised the agents with\na well-shaped reward function, and test the algorithm under\nextensive simulated scenarios.\nThere is also a growing trend in the recent literature to in-\ncorporate traditional Simultaneous Localization and Mapping\n(SLAM) (Thrun et al., 2005) procedures, either partially or\nfully and embedded internally or externally, into DRL network\narchitectures, with the intention to cultivate more sophisticated\nnavigation capabilities in DRL agents (Stachenfeld et al.,\n2017; Kanitscheider and Fiete, 2017). Below we review the\nmost representative robotics works in this promising direction.\nGupta et al. (2017a) train a Cognitive Mapping and Planning\n(CMP) model with DAGGer, which is an imitation learning\nalgorithm that we will talk about in Sec. III-A. Although it\ndose not use DRL for training the navigation policies, we\nfeel it ﬁts best into this part of the discussion. CMP takes\nin the ﬁrst-person view RGB images and applies egomotion\nto an internal mapper module, to encourage an egocentric\nmulti-scale map representation to emerge out of the training\nprocess. Planning is done on this egocentric map utilizing\nValue Iteration Networks (VIN) (Tamar et al., 2016). Also\ntrained with DAGGer, Gupta et al. (2017b) unify map-based\nspatial reasoning and path planning. Given the images and\nposes of several reference points and the starting point, as\nwell as the pose for the goal, their proposed method is able\nto navigate toward the agent the desired goal location.\nZhang et al. (2017b) proposed Neural SLAM, based on the\nNeural Map proposed by Parisotto and Salakhutdinov (2017),\nwhere the Neural Turing Machine (NTM) is deployed for\nthe DRL agent to interact with. More speciﬁcally, Neural\nSLAM embeds the motion prediction step and the measurement\nupdate step of traditional SLAM into the network architecture,\nbiasing the write/read operations in the NTM towards SLAM\noperations, and treats the external memory as an internal\nrepresentation of the environment for the learning agent. The\nwhole architecture is then trained via A3C, and experiments\nshow that the embedded structures are able to encourage\nthe evolution of cognitive mapping capabilities of the agent,\nduring the course of its exploration through the environment.\nKhan et al. (2017) design a network architecture that\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\ncontains three components: a convolution network for feature\nextraction, a planner module to pre-plan in the environment,\nand a controller module that learns to selectively store past\ninformation that could be useful for planning.\nBruce et al. (2017) propose a method that enables zero-shot\ntransfer for a mobile robot to learn to navigate to a ﬁxed goal\nin an environment with variations unseen during the training.\nThey introduce interactive replay, in which a rough world\nmodel is built from a single traversal of the environment. The\nagent is then able to interact with this world model to generate\na large number of diverse trajectories, which can substantially\nreduce the number of real experiences needed.\nChaplot et al. (2018) introduce a network structure mim-\nicking the Bayesian ﬁltering process with a specially designed\nperception model. It takes as input the agent’s observation, and\noutputs a likelihood map, inside which the belief is propagated\nthrough time following the classic ﬁltering process used for\nlocalization in robotics (?).\nInspired by graph-based SLAM algorithms (Thrun et al.,\n2005; K¨ummerle et al., 2011), Parisotto et al. (2018) embed the\nglobal pose optimization into the network architecture design\nfor their Neural Graph Optimizer, which is composed of a\nlocal pose estimation model, a pose selection module and a\ngraph optimization process. Savinov et al. (2018) introduce a\nmemory architecture, Semi-parametric Topological Memory,\nfor navigation in unseen environments. It contains a non-\nparametric graph with nodes representing locations in the\nenvironment, and a parametric deep network retrieving nodes\nfrom the graph based on observations.\nF. DRL for Manipulation\nIn terms of manipulation, the tasks being considered for\nevaluating DRL algorithms are more standardized in the recent\nliterature (Lillicrap et al., 2015; Schulman et al., 2015a; Mnih\net al., 2016; Heess et al., 2017; Wu et al., 2017). Most of\nsuch works benchmark the proposed algorithms on standard\ntasks, including reaching, pushing, pick-and-place, etc., using\nthe MuJoCo simulator (Todorov et al., 2012). Below we focus\non the works that are presented with real robotic experiments.\nGu et al. (2017) propose an asynchronous version of NAF.\nTaking in the low dimensional states as inputs (joint angles,\nend effector poses, as well as their time derivatives, and the\npose of the target), in addition to well-shaped reward signals,\nit allows the robot to learn a real-world door opening task in\nabout 2.5 hours in a completely end-to-end manner, achieving\na 100% success rate.\nLevine et al. (2016) successfully train deep visuomotor\npolicies with GPS, a model-based approach. Their proposed\nvisuomotor policy network takes as input monocular RGB\nimages and passes them through several convolutional layers\nand a spatial soft argmax layer, which are then concatenated\nwith the robot conﬁgurations (joint angles, end effector poses).\nThese representations are then passed through several fully\nconnected layers and used to predict the corresponding motor\ntorques. Various experiments on a PR2 robot (with a 7-DOF\narm) such as hanging a coat hanger on a clothes rack, inserting\na block into a shape sorting cube, or screwing on a bottle\ncap have demonstrated to validate the effectiveness of the\napproach. This method, however, requires a known and fully\nobserved state space, which could limit its potential use cases.\nModel-based DRL methods are also utilized by Finn et al.\n(2016) and Tzeng et al. (2015), learning useful state rep-\nresentations for generating successful control policies. Fu\net al. (2016) proposed one-shot learning of manipulation skills\nthrough model-based reinforcement learning by leveraging\nthe neural network priors as a dynamic model. Learning\ndexterous manipulation skills with multi-ﬁngered hands, for\nwhich model-based (Kumar et al., 2016; Gupta et al., 2016)\nand model-free (Popov et al., 2017) DRL algorithms have\nbeen proposed and demonstrated in real robotic experiments,\nis quite challenging.\nWhile many works have carefully designed their reward\nstructure to guide reinforcement learning, Riedmiller et al.\n(2018) propose a method to speed up learning from only\nbinary or sparse rewards, under the observation that well-\nshaped rewards can often bias the learned control policy into\npotentially suboptimal directions. In contrast when only sparse\nreward signals are provided to the agent, the learner can\ndiscover novel and potentially preferable solutions. To achieve\nthis, alongside the policy learning for the main task, Riedmiller\net al. (2018) learn policies (which they refer to as intentions)\nfor a set of semantically grounded auxiliary tasks, whose\nsupervision signals can be easily obtained by the activation\nof certain sensors. Then a scheduling policy is learned to\nsequence the intention-policies. Their proposed algorithm is\nable to learn to solve challenging manipulation tasks from\nscratch, such as stacking two blocks into a tower or cleaning\nup a desk by putting objects desk into a box with a lid that\ncan be opened, with a 9-DOF robot arm. Moreover, in their\nreal-world experiments, a single robot arm learns a lifting task\nin about 10 hours.\nG. The Reality Gap: From Simulation to the Real World\nAlthough DRL offers a general framework for agents to\nlearn high-dimensional control policies, it typically requires\nseveral millions of training samples. This makes it infeasible\nto train DRL agents directly in real-world scenarios, since real\nrobotic control experiences are relatively expensive to obtain.\nAs a consequence, DRL algorithms are generally trained in\nsimulated environments, then transferred to the real world\nand deployed onto real robotic systems. This brings about the\nproblem of the reality gap, which refers to the discrepancies\nin lighting conditions, noise patterns, textures, etc., between\nsynthetic renderings and real-world sensory readings. The\nreality gap imposes major challenges for generalizing the DRL\npolicies trained in simulation to real scenarios.\nSince the problem of the reality gap is most severe in\nthe visual domain, in that the aforementioned discrepancies\nare most signiﬁcant between rendered color images and real\ncolor camera readings, some robotics works have proposed\nto circumvent this problem by using other input modalities\nwhose domain variations are less distinct, such as depth\nimages (Zhang et al., 2017a) or laser ranges (Tai et al., 2017).\nHowever, bridging the reality gap in the visual domain is of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\ngreat importance and remains one of the focuses of recent\nworks. Below, we review methods that deal with the reality\ngap for visual control.\n1) Domain Adaptation: In the visual domain, domain\nadaptation can also be referred to as image-to-image trans-\nlation, which focuses on translating images from a source\ndomain to a target domains, It can be considered as the method\nof choice in the recent literature to tackle the reality gap\nfor visual control. The domain confusion loss, as proposed\nby Tzeng et al. (2014), is another solution that learns a\nsemantically meaningful and domain invariant representation.\nHowever, minimizing the domain confusion loss requires that\nthe data from both the source and the target domain are\navailable from the beginning of the whole learning pipeline,\nwhich might not be as ﬂexible in the robotics context.\nIn the following, we ﬁrst formalize the domain adaptation\nproblem, then continue to introduce several of the most general\nmethods that require the least human intervention and are most\ndirectly applicable to robotics control tasks.\nConsider visual data sources from two domains: X (e.g.,\nsynthetic images rendered by a simulator; x ∼psim, where psim\nrepresents the simulated data distribution) and Y (e.g., real\nsensory readings from the onboard color camera of a mobile\nrobot; y ∼preal, where preal represents the distribution of the\nreal color image readings). As we have just discussed, DRL\nagents are typically trained in the synthetic domain X, then\ndeployed onto real robotic platforms to perform control tasks\nin the real-world domain Y. Domain adaptation methods aim\nto learn a mapping between these two domains.\nGANs: Most of the domain adaptation works are based on\nGenerative Adversarial Networks (GANs) (Goodfellow et al.,\n2014; Radford et al., 2015; Arjovsky et al., 2017). When\nlearning a GAN model, a generator G and a discriminator D\nare trained in an adversarial manner. In the context of domain\nadaptation for visual inputs, the generator G takes images\nfrom the source domain, and tries to generate output images\nmatching those from the target domain, while the discriminator\nD learns to tell the generated target images and the real target\nimages apart.\nCycleGAN (Zhu et al., 2017a): Zhu et al. propose one of\nthe most popular unsupervised domain adaptation methods in\nthe recent literature, Zhu et al. (2017a) proposed a simple yet\nvery effective formulation that does not require paired data\nfrom the two domains of interest. Observing that the mapping\nfrom the source domain to the target domain, GY : X →Y, is\nhighly under-constrained, CycleGAN proposes to add a cycle-\nconsistent loss to enforce that a reverse mapping from the\ntarget domain back to the source domain exists: GX : Y →X.\nMore formally, CycleGAN learns two generative models to\nmap between domains X and Y: GY with its discriminator\nDY, and GX with its discriminator DX, by training two GANs\nsimultaneously:\nLGANY(GY, DY; X, Y) =\n(53)\nEy [log DY(y)] + Ex [log(1 −DY(GY(x)))] ,\nLGANX(GX, DX; Y, X) =\n(54)\nEx [log DX(x)] + Ey [log(1 −DX(GX(y)))] ,\non top of which two sets of cycle consistency loss are added\nto constrain the two mappings:\nLcycY(GX, GY; Y) = Ey [||GY(GX(y)) −y||1] ,\n(55)\nLcycX(GY, GX; X) = Ex [||GX(GY(x)) −x||1] .\n(56)\nThe full objective of CycleGAN then adds up to (λ denotes\nthe weighting of the cycle consistency loss)\nL(GY, GX, DY, DX; X, Y) =\nLGANY(GY, DY; X, Y)\n+ LGANX(GX, DX; Y, X)\n+ λcycLcycY(GX, GY; Y)\n+ λcycLcycX(GY, GX; X),\n(57)\nwhich corresponds to the following optimization problem:\nG∗\nY, G∗\nX = arg min\nGY,GX max\nDY,DX L(GY, GX, DY, DX).\n(58)\nThis conceptually simple method works surprisingly well in\npractice, especially in domains with relatively few semantic\ntypes (e.g., when the source domain images contain only\nhorses and background, and the target domain images contain\nonly zebras and background), where it is less challenging\nfor the algorithm to ﬁnd the matching semantics between\nthe two domains (e.g., horse ↔zebra). However, the results\nof CycleGAN on translating between more complex data\ndistributions containing many more semantic types, such as\nbetween urban street scenario images and their corresponding\nsemantic labels, are not as satisfactory, in that the generators\noften permute the labels for some semantics.\nCyCADA (Hoffman et al., 2017): The semantic consistency\nloss proposed in CyCADA offers a good solution to learning\nthe mapping between more complex data distributions with\nrelatively more semantic types. To be more speciﬁc, in Cy-\nCADA, a semantic segmentation network f is ﬁrst trained in\nthe domain where semantic labels are available (e.g., fX for\nthe synthetic domain X). (This is applicable for the domain\nadaptation between the simulated domain X and the real-\nworld domain Y in the context of robotics, since many recent\nrobotics simulators provide the ground truth semantic maps of\nthe rendered images, while the labels for the real images are\nexpensive to obtain.) Then this semantic segmentation network\nis used to constrain the semantic consistency between the input\nand the translated output images of the generators:\nLsemY(GY; X, fX) = CrossEnt(fX(X), fX(GY(X))), (59)\nLsemX(GX; Y, fX) = CrossEnt(fX(Y), fX(GX(Y))), (60)\nwhere CrossEnt(SX, fX(X)) represents the cross-entropy loss\nbetween the semantics of X predicted by the pretrained\nsemantic segmentation network fX and the ground truth label\nSX. The semantic consistency losses are then added to the\nCycleGAN objective (Eq. 58).\n2) Domain Adaptation for Visual DRL Policies: While\nmany extensions and variants have been proposed for image-\nto-image translation in the computer vision literature, here we\nfocus on those domain adaptation methods that speciﬁcally\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n11\nitarget transferring DRL control policies from simulation to\nreal scenarios.\nFor manipulation tasks, Bousmalis et al. (2017) deal with\nthe reality gap by adapting synthetic images to the realistic\ndomain before feeding them into the DRL policy network\nduring the training phase. However, the additional adaptation\nstep required for every training iteration could signiﬁcantly\nslow down the whole learning pipeline. Tobin et al. (2017)\nproposed to randomise the lighting conditions, viewing angles\nand textures of objects during the training phase of the DRL\npolicies in simulation, in the hope that after being exposed to\nenough variations, the learned model can naturally generalize\nto real-world scenarios. However, this method can only be\napplied to simulators where such randomization can be easily\nachieved at a low cost, which is not the case for most of the\npopular robotic simulators. Moreover, there is no guarantee\nthat for a random real-world scenario, its visual modality\ncan be covered by the randomized simulations. Similarly,\nrandomizing the dynamics of the simulator during training has\nalso been proposed (Peng et al., 2017) to bridge the reality gap.\nRusu et al. (2017) propose to progressively adapt the learned\ndeep features and representations from the synthetic domain\nto the real-world domain. However, this method still requires\ngoing through an expensive DRL control policy training phase\n(although this procedure can be greatly sped up by initial\ntraining in the simulator) for each new visually different real-\nworld scene.\nThe aforementioned methods realize domain adaptation via\nthe sim-to-real direction, meaning that they either translate the\nsynthetic images to the real-world domain during the training\nof DRL policies, or adapt the deep features of the simulated\ndomain to those of the realistic domain. However, the DRL\npolicy learning and the adaptation of the policies are entangled\nin this line of methods.\nThe recently proposed model of VR Goggles (Zhang et al.,\n2018) decouples the two components of policy learning and\npolicy adaptation, by tackling the reality gap from the real-\nto-sim direction, which requires no extra transfer steps during\nthe expensive training of DRL policies. Speciﬁcally, the VR\nGoggles deal with the reality gap only during the actual\ndeployment phase, by translating real-world sensor reading\nstreams back to the simulated domain, so as to adapt the\nunseen or unfamiliar characteristics of the real scenes to the\nsynthetic features, which the agent has already learned well\nhow to deal with, to make the robot feel at home. To constrain\nthe consistency between the generated subsequent frames, a\nshift loss is added to the optimization objective, which is\ninspired by the artistic style transfer for videos literature\n(Ruder et al., 2017). This method is validated in transferring\nDRL navigation policies, which could be considered more\nchallenging than manipulation tasks, since the environments\nthe navigation agents operate in are typically at much larger\nscales than the conﬁned workspace of manipulators.\nBoth results of outdoor and indoor scene adaptation have\nbeen presented. For the outdoor experiment, the synthetic data\nis collected from the CARLA simulator (Dosovitskiy et al.,\n2017) which provides the ground truth semantic labels, and the\nreal world data is gathered from the RobotCar dataset (Mad-\ndern et al., 2017). The semantic consistency loss is added for\nthe outdoor scenario, with a semantic segmentation network\ntrained using the DeepLab model (Chen et al., 2016). The\nsemantic consistency is critical for outdoor scenes containing\nvarious semantic types, without such a constraint, permutation\nof semantics occurs. It is also critical for situations where the\nmodel fails to generate a virtual car at the position at which\nthere is a real car in the real image (This kind of performance\nis as reported by Yang et al. (2018) whithout constraining\nthe semantic consistency), which could potentially lead to\naccidents in self-driving scenarios.\nFor indoor scenes, the semantic loss is not added, as the\nsimulated domain Gazebo (Koenig et al., 2004) does not\nprovide ground truth labels, and also the real scene, which is\na real ofﬁce environment, contains relatively fewer semantic\ntypes. A real robot (Turtlebot3 Wafﬂe) is deployed in the\nofﬁce environment and feed its sensor readings (captured by\na RealSense R200 camera) to the VR Goggles model. The\ntranslated Gazebo images are then fed to the DRL policy\nnetwork to give control commands. The VR Goggles offer a\nlightweight and ﬂexible solution for transferring DRL visual\ncontrol policies from simulation to the real world, and should\nalso be applicable to manipulation tasks.\nH. Simulation Platforms\nAs mentioned before, DRL algorithms, at their current state,\nare in general not sample efﬁcient enough to be directly trained\non real robotic platforms. Thus robotics simulators are utilized\nfor the initial training of DRL policies. Here we review several\nof the most widely used simulation platforms that are suitable\nfor DRL training.\nWe summarize the most commonly used simulators in Table\nI, listing their available sensor observation types and their\ntarget use cases.\nIII. IMITATION LEARNING\nDRL offers a formulation for control skills acquisition.\nHowever, relying on learning from trial and error, DRL meth-\nods typically require a signiﬁcant amount of system interaction\ntime. Also, a carefully designed well-shaped reward structure\nis necessary to guide the search of optimal policies, which can\noften be non-trivial in complex scenarios.\nImitation learning, as an alternative to learning control\npolicies, guides the policy search, not by hand-designed reward\nsignals, but by providing the learning agent with experts’\ndemonstrations (Bagnell, 2015). It offers a paradigm for agents\nto learn successful policies in ﬁelds where people can easily\ndemonstrate the desired behavior but ﬁnd it difﬁcult to hand\nprogram or hardcode the correct cost or reward function. This\nis especially useful for humanoid robots or manipulators with\nhigh degrees of freedom.\nPerhaps the most simple method of imitation learning is\naddressing it as a standard supervised learning problem. But\nas we have discussed, as a method for learning policies to\nmake sequential control decisions, imitation learning cannot\nbe conducted effectively by directly applying the classical su-\npervised learning approaches. We emphasize the most critical\ndistinctions below:\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n12\nTABLE I\nROBOTIC SIMULATORS.\nSimulator\nModalities\nFramerate\nTarget Use Case\nGazebo (Koenig et al., 2004)\nSensor Plugins\n10s+FPS\nGeneral Purposes\nVrep (Rohmer et al., 2013)\nSensor Plugins\n10s+FPS\nGeneral Purposes\nAirsim (Shah et al., 2017)\nDepth/Color/Semantics\n20s+FPS\nAutonomous Driving\nCarla (Dosovitskiy et al., 2017)\nDepth/Color/Semantics\n30s+FPS\nAutonomous Driving\nTorcs (You et al., 2017)\nColor/Semantics\n100s+FPS\nAutonomous Driving\nAI2-Thor (Kolve et al., 2017)\nColor\n100s+FPS\nIndoor Navigation\nMinos (Savva et al., 2017)\nDepth/Color/Semantics\n100s+FPS\nIndoor Navigation\nHouse3D (Wu et al., 2018)\nDepth/Color/Semantics\n600s+FPS\nIndoor Navigation\nIndependent VS. Compounding Errors: Standard su-\npervised learning assumes that the predictions made by the\nlearning agents do not affect the environment in which they\noperate; hence the data distribution they are to encounter\nis assumed to be the same as what they have experienced.\nHowever, although the learning errors are independent for\neach sample in supervised learning, they are compounded in\nimitation learning. This is due to the fact that the standard\nsupervised learning algorithms are only expected to do well\nover samples that are drawn from the same distribution as\nthey have been trained on. This i.i.d. assumption, however,is\nbadly violated in imitaiton learning, in which an early error\ncould potentially cascade to a sequence of mistakes, carried\nout by the control decisions that are made sequentially by the\nlearning agent.\nSingle-Timestep VS. Multi-Timestep Decisions: Super-\nvised learning agents are only capable of learning reactive\npolicies, since they completely ignore the temporal depen-\ndence between subsequent decisions, which leads to myopic\nstrategies. In contrast, for making informative decisions, clas-\nsical planning approaches in robotics reason far into the future\n(but often require sophisticatedly designed cost functions).\nAlso, a naive imitation of the experts’ demonstrations often\nmisses the true learning objective: instead of copying the\ndemonstrated behaviors given by the experts, the actual goal\nof imitation learning is in some cases quite different and in-\nexplicitly optimized in the demonstrations, such as to increase\nthe success rate of accomplishing a speciﬁc task, to minimize\nthe probability of colliding with obstacles, or to minimize the\ntotal travel cost.\nIn the following, we proceed by going through the three\nmost common approaches of imitation learning, which address\nthe above issues from different perspectives, and introduce\nrepresentative works in robotics for each.\nA. Behavior Cloning\nBehavior cloning tackles the problem of imitation learning\nin a supervised manner, by directly learning the mapping be-\ntween the input observations and their corresponding actions,\nwhich are given by the expert policy. This simple formulation\ncan give a satisfactory performance when there is enough\ntraining data, but will lead to compounding errors, as we\nhave just discussed. One of the most well-known algorithms\nto compensate for this is DAGGer (in which DAGG stands\nfor Data AGGregation) (Ross et al., 2011), which interleaves\nexecution and learning. To be more speciﬁc, in the ith iteration\nof DAGGer, the current learned policy πi−1 will be executed\nto collect experiences. Then those newly recorded observations\nwill be relabeled by the expert policy πE. These corrected new\nexperiences Di will be added to the existing dataset D, on\nwhich a new policy πi is trained. This interaction between ex-\necution and learning halts the error compounding and bounds\nthe expected error to that in the standard supervised learning\nsetting.\nDue to its simple formulation, behavior cloning has been\nwidely studied and applied in robotics control problems.\nWe start with the literature in the ﬁeld of navigation and\nself-driving imitation. Bojarski et al. (2016) learn a direct\nmapping from raw ﬁrst-person view color images to steering\ncommands, on a training dataset collected by driving on a\nwide variety of roads and in diverse weather and lighting\nconditions, which in total adds up to 72 hours of driving data.\nTai et al. (2016) drive an indoor mobile robot autonomously\nthrough a dataset based on joystick commands from human\ndemonstrator. A depth visual image is taken as the only input\nin their implementation. Giusti et al. (2016) train a deep\nnetwork to determine actions that can keep a quadrotor on\na trail, by learning on single monocular images collected\nfrom the robot’s perspective. Eight hours of video is captured\nusing three GoPros mounted on the head of a hiker, with one\npointing to the left, one to the right, and one straight ahead.\nThe optimal actions for the collected images can then be easily\nlabeled; e.g., the quadrotor should turn right when facing an\nimage collected from the left-facing camera. Codevilla et al.\n(2017) observes that the pure behavior cloning assumption\ncould break under certain situations, such as when a driver\napproaches an intersection. The driver’s subsequent actions\ncannot be fully explained by the observations, since they are\nadditionally affected by the driver’s internal throughouts, such\nas the intended destination. To address this, a conditional\nimitation learning method is proposed to additionally con-\nstrain the imitation learning additionally on a representation\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n13\nof the expert’s intention, so as to resolve the ambiguity in\nthe perceptuomotor mapping. Both simulated and real-world\nexperiments are conducted, in which the synthetic dataset\nis collected in the simulated self-driving environment Carla\n(Dosovitskiy et al., 2017) and a real-world dataset from remote\ncontrolling a robotic truck in a residential area, each of which\ncontains two hours of driving time.\nIn terms of imitation learning for manipulation, a recently\nproposed line three works presents and improves on one-shot\nimitation learning: from taking low dimensional states and\nexpert action pairs as demonstrations (Duan et al., 2017), to\nlearning from demonstrations of raw visual images paired with\nactions (Finn et al., 2017b), and ﬁnally arriving at the current\nstate of learning from human demonstration videos without\nlabeled actions (Yu et al., 2018). Below we discuss these\nmethods in more detail.\nDuan et al. (2017) present the imitation agent with pairs\nof demonstrations for each iteration during training, in which\nthe network takes as input the ﬁrsth demonstration and a\nstate sampled from the second demonstration. The network\nis then trained using behavior cloning losses to predict the\ncorresponding action of that sampled state. The concrete\nexample used in their problem setting is a distribution of block\nstacking tasks, in which the goal is to control a robot arm\nto stack various numbers of cubic blocks into conﬁgurations\nspeciﬁed by the user. Each observation is a list of the relative\npositions of the blocks w.r.t. the gripper, and information\nindicating whether the gripper is open or closed. Several archi-\ntectural designs, such as temporal dropout and convolutions,\nneighborhood attention, are incorporated into their training\npipeline to cope with variable-dimensional and potentially long\nsequences of inputs. In their experiments, the performance of\npure behavior cloning achieves the same level of performance\nas training with DAGGer, suggesting that at least for this\nspeciﬁc block-stacking task, the interactive supervision in\nDAGGer might not necessarily lead to a performance gain.\nFinn et al. (2017b) and Yu et al. (2018) both extend the\nModel-Agnostic Meta-Learning (MAML) method (Finn et al.,\n2017a), which we will brieﬂy review here before proceeding.\nThe objective of MAML is to learn a model, such that, after\nbeing trained on a variety of learning tasks, it is able to learn to\nsolve new tasks with only a small number of training samples.\nFormally, this model of interest is denoted as fθ with weights\nθ, and the meta-learning is considered over a distribution of\ntasks p(T ). The model parameters will be updated from θ to\nθ′\ni, when adapting to a new task Ti. This update is performed\nusing gradient descent on task Ti:\nθ′\ni = θ −α∇θLTi(fθ),\n(61)\nwhere α denotes a step size, and L represents a behavior\ncloning loss function (e.g., mean squared error for continuous\nactions, cross-entropy loss for discrete actions). After the\nupdated θ′\ni is obtained, its performance is optimized w.r.t.\nθ across tasks sampled from p(T ), leading to the following\nmeta-learning objective:\nmin\nX\nTi∼p(T )\nLTi(fθ′\ni) =\nX\nTi∼p(T )\nLTi(fθ−α∇θLTi(fθ)),\n(62)\nwhich is performed via SGD such that θ is updated as follows:\nθ ←θ −β∇θ\nX\nTi∼p(T )\nLTi(fθ′\ni),\n(63)\nwhere β is the meta step size.\nHere, the meta-optimization is performed over θ, while the\nloss is computed using the updated parameters θ′. This objec-\ntive will help to ﬁnd model parameters that are sensitive to\nchanges in the task, such that small changes in the parameters\ncould lead to large improvements in the performance on any\ntask sampled from p(T ).\nBased on the formulation of MAML, Finn et al. (2017b)\nlearn policies that can be quickly adapted to new tasks using\na single demonstration. Here each observation input into the\nmodel contains a color image from the robot’s perspective,\nand the robot conﬁgurations (joint angles, end-effector poses).\nWhile both Duan et al. (2017) and Finn et al. (2017b) use\nonly robot demonstrations throughout training and testing, Yu\net al. (2018) is able to cope with domain shift by learning from\nboth robot and human demonstrations, in which the human\ndemonstrations are not labeled with expert actions. After\nmeta-learning, the proposed method is capable of learning\nfrom human videos. To cope with the unlabelled human\ndemonstrations, an adaptation loss function Lψ is learned\nalongside the meta-learning objective. During training, human\ndemonstrations are used to compute the updated policy pa-\nrameters θ′\ni with the gradients calculated using Lψ. Then the\nperformance of θ′\ni is evaluated using the behavior cloning loss\nto update both θ and ψ. Note that all the robot demonstrations\nare collected via teleoperation (Zhang et al., 2017c).\nA recent work from Eitel et al. (2017) introduces a model\nthat is able to propose push actions based on over-segmented\nRGB-D images, in order to separate unknown objects in\ncluttered environments.\nB. Inverse Reinforcement Learning\nInverse reinforcement learning (IRL) frames imitation\nlearning as solutions to MDPs, thus reducing the problem of\nlearning to the problem of recovering a utility function that\nmakes the demonstrated behavior (near-)optimal. After this\nutility function is obtained, reinforcement learning procedures\ncan be performed on top to search for optimal policies.\nA representative IRL method, the Maximum Entropy IRL\n(Ziebart et al., 2008), ﬁts a cost function from a family of\nfunctions C to optimize the following objective:\nargmax\nc∈C\n\u0012\nmin\nπ∈Q −H(π) + Eπ [c(s, a)]\n\u0013\n−EπE [c(s, a)] ,\n(64)\nwhere Q denotes the family of policies.\nIn robotics, the formulation of IRL offers an efﬁcient\nsolution for learning policies for socially compliant navigation\n(Okal and Arras, 2016; Pfeiffer et al., 2016; Kretzschmar et al.,\n2016), where the agent needs to not only avoid collisions with\nstatic obstacles but also to behave in a socially compliant\nmanner. Thus, the underlying cost function is non-trivial to\nhand-design, but the hebaviors are easy to demonstrate.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n14\nWulfmeier et al. (2015) extends Maximum Entropy IRL\nunder the deep learning context, utilizing fully convolutional\nneural networks as the approximator for learning the reward\nfunction. The proposed algorithm is successfully deployed for\nlearning the cost map in urban environments, from a dataset\nof driving behaviors demonstrated by human experts.\nC. Generative Adversarial Imitation Learning\nThe learning process of IRL can be indirect and slow.\nInspired by Generative Adversarial Networks (GANs) (Good-\nfellow et al., 2014), Ho and Ermon (2016) propose Generative\nAdversarial Imitation Learning (GAIL), which surpasses the\nintermediate step of learning a reward function and is capable\nof directly learning a policy from expert demonstrations. To\nbe more speciﬁc, in the GAIL model, a generator πθ with\nparameters θ is trained to generate state-action (S × A) pairs\nmatching those of the expert demonstrations, while the dis-\ncriminator Dω learns to tell apart the generated policy πθ from\nthe expert (demonstrated) policy πE. The GAIL optimization\nobjective is deﬁned as follows:\nEπθ [log(D(s, a))] + EπE [log(1 −D(s, a))] −λH(πθ),\n(65)\nwhere H(πθ) denotes the causal entropy. The training of GAIL\ninterleaves between updating parameters ω of the discriminator\nDω to maximize Eq. 65, and utilizing DRL techniques such\nas TRPO to minimize Eq. 65 w.r.t. the parameters θ of the\ngenerator πθ. The scores given out by the discriminator for\nthe generated experiences are regarded as costs for those state-\naction pairs for TRPO. Several extensions of GAIL have also\nbeen proposed (Baram et al., 2016; Wang et al., 2017).\nIn the ﬁeld of navigation, Li et al. (2017) successfully apply\nGAIL in simulated autonomous vehicle navigation scenarios\nwith raw visual input. Tai et al. (2018) learn a socially\ncompliant navigation policy through GAIL, based on raw\ndepth input, and demonstrate the learned behaviors in real\nrobotics experiments.\nFor manipulation, Stadie et al. (2017) extend the GAIL\nformulation with ideas from domain confusion loss (Tzeng\net al., 2014), and successfully utilize it to train agents to imitate\nthird-person demonstrations, by learning a domain-agnostic\nrepresentation of the agent’s observations.\nIV. CHALLENGES AND OPEN RESEARCH QUESTIONS\nUtilizing deep learning techniques for learning control for\nrobotics tasks has shown great potential. Yet, there still remain\nmany challenges for scaling up and stabilizing the afore-\nmentioned algorithms to meet the requirements of operating\nrobotics systems in real-world applications. We list the critical\nchallenges and the corresponding future research directions.\n• Sample Efﬁciency: Gathering experiences by interacting\nwith the environment for deep reinforcement learning, or\ncollecting expert demonstrations for imitation learning,\nare both expensive procedures, in terms of executing con-\ntrol commands on real robotics systems. Thus, designing\nsample efﬁcient algorithms is of critical importance.\n• Strong Real-time Requirements: A single forward pass\nof very deep networks with millions of parameters\ncould be relatively slow if not equipped with special\ncomputation hardware and might not meet the real-\ntime requirement for controlling real robotics systems.\nLearning compact representations for dexterous policies\nis preferable.\n• Safety Concerns: Real robotics systems, such as mobile\nrobots, quadrotors or self-driving cars, are expected to\noperate in environments that could be highly dynamic and\npotentially dangerous. Also, unlike a wrong prediction\nfrom a perception model, which would not cascade or\naffect the physical robotic systems or the environment, a\nsingle false output might lead to a serious accident. Thus,\nattention should be paid to include practical considera-\ntions to bound the uncertainty of the possible outcomes\nwhen deploying control policies on real autonomous\nsystems.\n• Stability, Robustness and Interpretability: DRL algo-\nrithms could be relatively unstable, and their performance\nmight deviate a lot between conﬁgurations that only\ndiffer slightly from each other (Henderson et al., 2017).\nTo overcome this problem, gaining more insight into\nthe learned representations and the policies, could be\nbeneﬁcial for detecting adversarial scenarios to prevent\nrobotic systems from safety threats.\n• Lifelong Learning: The visual appearance of the envi-\nronments that autonomous agents operate in cab vary\ndramatically during different seasons of the year, or\neven different times of day, which could hinder the\nperformance of the learned control policies. Hence the\nability of continuing to learn to adapt to environmental\nchanges as well as preserving the solutions to the already\nexperienced scenarios could be of critical value.\n• Generalization Between Tasks: Most of the aforemen-\ntioned algorithms are designed to excel in a particular\ntask, which is not ideal, as intelligent robotic systems are\nexpected to be capable of carrying out a set of tasks, with\na minimal total training time for all considered tasks.\nIn contrast, with the rapid development of deep learning,\nseveral research directions are gaining much attention for\nrobotics.\n• Unifying\nReinforcement\nLearning\nand\nImitation\nLearning: Several recent works (Veˇcer´ık et al., 2017;\nNair et al., 2017; Gao et al., 2018; Zhu et al., 2018) have\nintroduced algorithms that unify reinforcement learning\nand imitation learning such that the learning agent can\nbeneﬁt from both expert demonstrations and interactions\nwith the environment. This setup can be beneﬁcial for\nlearning control, as pure DRL algorithms are, in general,\nrelatively expensive to train, while learning purely by\nimitating demonstrated behaviors can restrict or bias the\ncontrol policy in potentially suboptimal directions. Thus,\nthe method of using demonstrations to kick-start the\npolicy learning, then applying DRL methods to adjust\nthe learned policy, can potentially lead to advanced per-\nformance.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n15\n• Meta-learning: Finn et al. (2017a) and Nichol and\nSchulman (2018) propose methods that can effectively\nlead the policy search to ﬁnd parameters that can be\nadapted to give good performance on a new task with\nonly a small number of training examples of the new task.\nSuch formulations could be very beneﬁcial, and have the\npotential to learn universal and robust policies.\nV. CONCLUSION\nIn this paper, we give a brief overview of deep learning\nsolutions for robotics control tasks, focusing mainly on deep\nreinforcement learning and imitation learning algorithms. We\nmainly introduce the formulations for each learning paradigm\nand the corresponding representative works in robotics. Fi-\nnally, We discuss the challenges and potential future research\ndirections.\nREFERENCES\nAndrychowicz M, Crow D, Ray A, Schneider J, Fong R,\nWelinder P, McGrew B, Tobin J, Abbeel OP and Zaremba W\n(2017) Hindsight experience replay. In: Advances in Neural\nInformation Processing Systems. pp. 5055–5065.\nArjovsky M, Chintala S and Bottou L (2017) Wasserstein gan.\narXiv preprint arXiv:1701.07875 .\nBagnell JA (2015) An invitation to imitation.\nTechnical\nreport, CARNEGIE-MELLON UNIV PITTSBURGH PA\nROBOTICS INST.\nBaram\nN,\nAnschel\nO\nand\nMannor\nS\n(2016)\nModel-\nbased adversarial imitation learning.\narXiv preprint\narXiv:1612.02179 .\nBarreto A, Dabney W, Munos R, Hunt JJ, Schaul T, Silver D\nand van Hasselt HP (2017) Successor features for transfer in\nreinforcement learning. In: Advances in Neural Information\nProcessing Systems. pp. 4058–4068.\nBengio Y, Louradour J, Collobert R and Weston J (2009)\nCurriculum learning. In: Proceedings of the 26th annual\ninternational conference on machine learning. ACM, pp.\n41–48.\nBojarski M, Del Testa D, Dworakowski D, Firner B, Flepp\nB, Goyal P, Jackel LD, Monfort M, Muller U, Zhang J\net al. (2016) End to end learning for self-driving cars. arXiv\npreprint arXiv:1604.07316 .\nBousmalis K, Irpan A, Wohlhart P, Bai Y, Kelcey M, Kalakr-\nishnan M, Downs L, Ibarz J, Pastor P, Konolige K et al.\n(2017) Using simulation and domain adaptation to im-\nprove efﬁciency of deep robotic grasping. arXiv preprint\narXiv:1709.07857 .\nBruce J, S¨underhauf N, Mirowski P, Hadsell R and Milford M\n(2017) One-shot reinforcement learning for robot navigation\nwith interactive replay. arXiv preprint arXiv:1711.10137 .\nChaplot DS, Parisotto E and Salakhutdinov R (2018) Active\nneural localization. arXiv preprint arXiv:1801.08214 .\nChen LC, Papandreou G, Kokkinos I, Murphy K and Yuille AL\n(2016) Deeplab: Semantic image segmentation with deep\nconvolutional nets, atrous convolution, and fully connected\ncrfs. arXiv preprint arXiv:1606.00915 .\nChen YF, Everett M, Liu M and How JP (2017a) Socially\naware motion planning with deep reinforcement learning.\narXiv preprint arXiv:1703.08862 .\nChen YF, Liu M, Everett M and How JP (2017b) Decentralized\nnon-communicating multiagent collision avoidance with\ndeep reinforcement learning. In: Robotics and Automation\n(ICRA), 2017 IEEE International Conference on. IEEE, pp.\n285–292.\nCodevilla F, M¨uller M, Dosovitskiy A, L´opez A and Koltun V\n(2017) End-to-end driving via conditional imitation learn-\ning. arXiv preprint arXiv:1710.02410 .\nDayan P (1993) Improving generalization for temporal dif-\nference learning: The successor representation.\nNeural\nComputation 5(4): 613–624.\nDeisenroth MP, Neumann G, Peters J et al. (2013) A survey\non policy search for robotics. Foundations and Trends R⃝in\nRobotics 2(1–2): 1–142.\nDeng L (2014) A tutorial survey of architectures, algorithms,\nand applications for deep learning. APSIPA Transactions on\nSignal and Information Processing 3.\nDosovitskiy A, Ros G, Codevilla F, L´opez A and Koltun\nV (2017) Carla: An open urban driving simulator. arXiv\npreprint arXiv:1711.03938 .\nDuan Y, Andrychowicz M, Stadie B, Ho OJ, Schneider J,\nSutskever I, Abbeel P and Zaremba W (2017) One-shot\nimitation learning.\nIn: Advances in neural information\nprocessing systems. pp. 1087–1098.\nEitel A, Hauff N and Burgard W (2017) Learning to singulate\nobjects using a push proposal network.\nIn: Proc. of\nthe International Symposium on Robotics Research (ISRR).\nPuerto Varas, Chile.\nFinn C, Abbeel P and Levine S (2017a) Model-agnostic meta-\nlearning for fast adaptation of deep networks. arXiv preprint\narXiv:1703.03400 .\nFinn C, Tan XY, Duan Y, Darrell T, Levine S and Abbeel P\n(2016) Deep spatial autoencoders for visuomotor learning.\nIn: Robotics and Automation (ICRA), 2016 IEEE Interna-\ntional Conference on. IEEE, pp. 512–519.\nFinn C, Yu T, Zhang T, Abbeel P and Levine S (2017b)\nOne-shot visual imitation learning via meta-learning. arXiv\npreprint arXiv:1709.04905 .\nFlorensa C, Held D, Wulfmeier M and Abbeel P (2017)\nReverse curriculum generation for reinforcement learning.\narXiv preprint arXiv:1707.05300 .\nFortunato M, Azar MG, Piot B, Menick J, Osband I, Graves\nA, Mnih V, Munos R, Hassabis D, Pietquin O et al.\n(2018) Noisy networks for exploration.\nIn: International\nConference on Learning Representations.\nURL https://\nopenreview.net/forum?id=rywHCPkAW.\nFu J, Levine S and Abbeel P (2016) One-shot learning of\nmanipulation skills with online dynamics adaptation and\nneural network priors. In: Intelligent Robots and Systems\n(IROS), 2016 IEEE/RSJ International Conference on. IEEE,\npp. 4019–4026.\nFu MC, Glover FW and April J (2005) Simulation optimiza-\ntion: a review, new developments, and applications.\nIn:\nProceedings of the 37th conference on Winter simulation.\nWinter Simulation Conference, pp. 83–95.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n16\nGao Y, Xu HH, Lin J, Yu F, Levine S and Darrell T (2018)\nReinforcement learning from imperfect demonstrations .\nGiusti A, Guzzi J, Cires¸an DC, He FL, Rodr´ıguez JP, Fontana\nF, Faessler M, Forster C, Schmidhuber J, Di Caro G et al.\n(2016) A machine learning approach to visual perception\nof forest trails for mobile robots.\nIEEE Robotics and\nAutomation Letters 1(2): 661–667.\nGoodfellow I, Bengio Y and Courville A (2016) Deep Learn-\ning. MIT Press. http://www.deeplearningbook.org.\nGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley\nD, Ozair S, Courville A and Bengio Y (2014) Generative\nadversarial nets. In: Advances in neural information pro-\ncessing systems. pp. 2672–2680.\nGu S, Holly E, Lillicrap T and Levine S (2017) Deep re-\ninforcement learning for robotic manipulation with asyn-\nchronous off-policy updates. In: Robotics and Automation\n(ICRA), 2017 IEEE International Conference on. IEEE, pp.\n3389–3396.\nGu S, Lillicrap T, Sutskever I and Levine S (2016) Contin-\nuous deep q-learning with model-based acceleration.\nIn:\nInternational Conference on Machine Learning (ICML-16).\nGuo Y, Liu Y, Oerlemans A, Lao S, Wu S and Lew MS\n(2016) Deep learning for visual understanding: A review.\nNeurocomputing 187: 27–48.\nGupta A, Eppner C, Levine S and Abbeel P (2016) Learning\ndexterous manipulation for a soft robotic hand from human\ndemonstrations. In: Intelligent Robots and Systems (IROS),\n2016 IEEE/RSJ International Conference on. IEEE, pp.\n3786–3793.\nGupta S, Davidson J, Levine S, Sukthankar R and Malik J\n(2017a) Cognitive mapping and planning for visual naviga-\ntion. arXiv preprint arXiv:1702.03920 .\nGupta S, Fouhey D, Levine S and Malik J (2017b) Unifying\nmap and landmark based representations for visual naviga-\ntion. arXiv preprint arXiv:1712.08125 .\nHe K, Zhang X, Ren S and Sun J (2016) Deep residual\nlearning for image recognition. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. pp.\n770–778.\nHeess N, Sriram S, Lemmon J, Merel J, Wayne G, Tassa\nY, Erez T, Wang Z, Eslami A, Riedmiller M et al. (2017)\nEmergence of locomotion behaviours in rich environments.\narXiv preprint arXiv:1707.02286 .\nHenderson P, Islam R, Bachman P, Pineau J, Precup D and\nMeger D (2017) Deep reinforcement learning that matters.\narXiv preprint arXiv:1709.06560 .\nHo J and Ermon S (2016) Generative adversarial imitation\nlearning. In: Advances in Neural Information Processing\nSystems. pp. 4565–4573.\nHoffman J, Tzeng E, Park T, Zhu JY, Isola P, Saenko K, Efros\nAA and Darrell T (2017) Cycada: Cycle-consistent adver-\nsarial domain adaptation. arXiv preprint arXiv:1711.03213\n.\nHuang G, Liu Z, Weinberger KQ and van der Maaten L (2017)\nDensely connected convolutional networks. In: Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, volume 1. p. 3.\nJaderberg M, Mnih V, Czarnecki WM, Schaul T, Leibo\nJZ, Silver D and Kavukcuoglu K (2016) Reinforcement\nlearning with unsupervised auxiliary tasks. arXiv preprint\narXiv:1611.05397 .\nKakade S and Langford J (2002) Approximately optimal\napproximate reinforcement learning. In: ICML, volume 2.\npp. 267–274.\nKalweit G and Boedecker J (2017) Uncertainty-driven imag-\nination for continuous deep reinforcement learning.\nIn:\nLevine S, Vanhoucke V and Goldberg K (eds.) Proceedings\nof the 1st Annual Conference on Robot Learning, Proceed-\nings of Machine Learning Research, volume 78. PMLR, pp.\n195–206. URL http://proceedings.mlr.press/v78/kalweit17a.\nhtml.\nKanitscheider I and Fiete I (2017) Training recurrent networks\nto generate hypotheses about how the brain solves hard\nnavigation problems. In: Advances in Neural Information\nProcessing Systems. pp. 4532–4541.\nKhan A, Zhang C, Atanasov N, Karydis K, Kumar V and Lee\nDD (2017) Memory augmented control networks.\narXiv\npreprint arXiv:1709.05706 .\nKoenig N, A B and Howard A (2004) Design and use\nparadigms for gazebo, an open-source multi-robot simula-\ntor. In: Intelligent Robots and Systems, 2004.(IROS 2004).\nProceedings. 2004 IEEE/RSJ International Conference on,\nvolume 3. IEEE, pp. 2149–2154.\nKolve E, Mottaghi R, Gordon D, Zhu Y, Gupta A and Farhadi\nA (2017) Ai2-thor: An interactive 3d environment for visual\nai. arXiv preprint arXiv:1712.05474 .\nKretzschmar H, Spies M, Sprunk C and Burgard W (2016)\nSocially compliant mobile robot navigation via inverse re-\ninforcement learning. The International Journal of Robotics\nResearch 35(11): 1289–1307.\nKrizhevsky A, Sutskever I and Hinton GE (2012) Imagenet\nclassiﬁcation with deep convolutional neural networks. In:\nAdvances in neural information processing systems. pp.\n1097–1105.\nKulkarni TD, Saeedi A, Gautam S and Gershman SJ (2016)\nDeep successor reinforcement learning.\narXiv preprint\narXiv:1606.02396 .\nKumar V, Todorov E and Levine S (2016) Optimal control\nwith learned local models: Application to dexterous manip-\nulation. In: Robotics and Automation (ICRA), 2016 IEEE\nInternational Conference on. IEEE, pp. 378–383.\nK¨ummerle R, Grisetti G, Strasdat H, Konolige K and Burgard\nW (2011) g 2 o: A general framework for graph optimiza-\ntion.\nIn: Robotics and Automation (ICRA), 2011 IEEE\nInternational Conference on. IEEE, pp. 3607–3613.\nLevine S, Finn C, Darrell T and Abbeel P (2016) End-to-\nend training of deep visuomotor policies. The Journal of\nMachine Learning Research 17(1): 1334–1373.\nLevine S and Koltun V (2013) Guided policy search. In: ICML\n(3). pp. 1–9.\nLi Y, Song J and Ermon S (2017) Inferring the latent structure\nof human decision-making from raw visual inputs. arXiv\npreprint arXiv:1703.08840 .\nLillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, Sil-\nver D and Wierstra D (2015) Continuous control with deep\nreinforcement learning. In: Proceedings of the International\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n17\nConference on Learning Representations (ICLR).\nLong J, Shelhamer E and Darrell T (2015) Fully convolutional\nnetworks for semantic segmentation.\nIn: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition. pp. 3431–3440.\nLong P, Fan T, Liao X, Liu W, Zhang H and Pan J (2017) To-\nwards optimally decentralized multi-robot collision avoid-\nance via deep reinforcement learning.\narXiv preprint\narXiv:1709.10082 .\nMaddern W, Pascoe G, Linegar C and Newman P (2017)\n1 Year, 1000km: The Oxford RobotCar Dataset.\nThe\nInternational Journal of Robotics Research (IJRR) 36(1):\n3–15. DOI:10.1177/0278364916679498. URL http://dx.doi.\norg/10.1177/0278364916679498.\nMirowski P, Pascanu R, Viola F, Soyer H, Ballard A, Banino\nA, Denil M, Goroshin R, Sifre L, Kavukcuoglu K et al.\n(2016) Learning to navigate in complex environments. arXiv\npreprint arXiv:1611.03673 .\nMnih V, Badia AP, Mirza M, Graves A, Lillicrap TP, Harley\nT, Silver D and Kavukcuoglu K (2016) Asynchronous\nmethods for deep reinforcement learning.\narXiv preprint\narXiv:1602.01783 .\nMnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J,\nBellemare MG, Graves A, Riedmiller M, Fidjeland AK,\nOstrovski G et al. (2015) Human-level control through deep\nreinforcement learning. Nature 518(7540): 529–533.\nNair A, McGrew B, Andrychowicz M, Zaremba W and Abbeel\nP (2017) Overcoming exploration in reinforcement learning\nwith demonstrations. arXiv preprint arXiv:1709.10089 .\nNichol A and Schulman J (2018) Reptile: a scalable metalearn-\ning algorithm. arXiv preprint arXiv:1803.02999 .\nOkal B and Arras KO (2016) Learning socially normative\nrobot navigation behaviors with bayesian inverse reinforce-\nment learning. In: ICRA. pp. 2889–2895.\nParisotto E, Chaplot DS, Zhang J and Salakhutdinov R (2018)\nGlobal pose estimation with an attention-based recurrent\nnetwork. arXiv preprint arXiv:1802.06857 .\nParisotto E and Salakhutdinov R (2017) Neural map: Struc-\ntured memory for deep reinforcement learning.\narXiv\npreprint arXiv:1702.08360 .\nPathak D, Agrawal P, Efros AA and Darrell T (2017)\nCuriosity-driven exploration by self-supervised prediction.\nIn: International Conference on Machine Learning (ICML),\nvolume 2017.\nPeng XB, Andrychowicz M, Zaremba W and Abbeel P (2017)\nSim-to-real transfer of robotic control with dynamics ran-\ndomization. arXiv preprint arXiv:1710.06537 .\nPfeiffer M, Schwesinger U, Sommer H, Galceran E and\nSiegwart R (2016) Predicting actions to act predictably:\nCooperative partial motion planning with maximum entropy\nmodels.\nIn: 2016 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS). pp. 2096–2101.\nPlappert M, Houthooft R, Dhariwal P, Sidor S, Chen RY,\nChen X, Asfour T, Abbeel P and Andrychowicz M (2018)\nParameter space noise for exploration.\nIn: International\nConference on Learning Representations.\nURL https://\nopenreview.net/forum?id=ByBAl2eAZ.\nPopov I, Heess N, Lillicrap T, Hafner R, Barth-Maron G,\nVecerik M, Lampe T, Tassa Y, Erez T and Riedmiller\nM (2017) Data-efﬁcient deep reinforcement learning for\ndexterous manipulation. arXiv preprint arXiv:1704.03073\n.\nRadford A, Metz L and Chintala S (2015) Unsupervised\nrepresentation learning with deep convolutional generative\nadversarial networks. arXiv preprint arXiv:1511.06434 .\nRiedmiller M, Hafner R, Lampe T, Neunert M, Degrave J,\nVan de Wiele V Tom adn Mnih, Heess N and Springenberg\nJT (2018) Learning by playing - solving sparse reward tasks\nfrom scratch. arXiv preprint arXiv:1802.10567 .\nRohmer E, Singh SP and Freese M (2013) V-rep: A versatile\nand scalable robot simulation framework.\nIn: Intelligent\nRobots and Systems (IROS), 2013 IEEE/RSJ International\nConference on. IEEE, pp. 1321–1326.\nRoss S, Gordon G and Bagnell D (2011) A reduction of imi-\ntation learning and structured prediction to no-regret online\nlearning.\nIn: Proceedings of the fourteenth international\nconference on artiﬁcial intelligence and statistics. pp. 627–\n635.\nRuder M, Dosovitskiy A and Brox T (2017) Artistic style\ntransfer for videos and spherical images.\narXiv preprint\narXiv:1708.04538 .\nRusu AA, Veˇcer´ık M, Roth¨orl T, Heess N, Pascanu R and\nHadsell R (2017) Sim-to-real robot learning from pixels\nwith progressive nets. In: Conference on Robot Learning.\npp. 262–270.\nSavinov N, Dosovitskiy A and Koltun V (2018) Semi-\nparametric topological memory for navigation.\nIn: Inter-\nnational Conference on Learning Representations.\nURL\nhttps://openreview.net/forum?id=SygwwGbRW.\nSavva M, Chang AX, Dosovitskiy A, Funkhouser T and\nKoltun V (2017) Minos: Multimodal indoor simulator\nfor navigation in complex environments.\narXiv preprint\narXiv:1712.03931 .\nSchaul T, Horgan D, Gregor K and Silver D (2015a) Universal\nvalue function approximators. In: International Conference\non Machine Learning. pp. 1312–1320.\nSchaul T, Quan J, Antonoglou I and Silver D (2015b) Prior-\nitized experience replay. arXiv preprint arXiv:1511.05952\n.\nSchmidhuber J (2015) Deep learning in neural networks: An\noverview. Neural networks 61: 85–117.\nSchulman J, Levine S, Abbeel P, Jordan M and Moritz P\n(2015a) Trust region policy optimization. In: International\nConference on Machine Learning. pp. 1889–1897.\nSchulman J, Moritz P, Levine S, Jordan M and Abbeel\nP\n(2015b)\nHigh-dimensional\ncontinuous\ncontrol\nus-\ning generalized advantage estimation.\narXiv preprint\narXiv:1506.02438 .\nSchulman J, Wolski F, Dhariwal P, Radford A and Klimov\nO (2017) Proximal policy optimization algorithms. arXiv\npreprint arXiv:1707.06347 .\nShah S, Dey D, Lovett C and Kapoor A (2017) Airsim:\nHigh-ﬁdelity visual and physical simulation for autonomous\nvehicles.\nIn: Field and Service Robotics.\nURL https:\n//arxiv.org/abs/1705.05065.\nSilver D, Lever G, Heess N, Degris T, Wierstra D and Ried-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n18\nmiller M (2014) Deterministic policy gradient algorithms.\nIn: Proceedings of the 31st International Conference on\nMachine Learning (ICML-14). pp. 387–395.\nStachenfeld KL, Botvinick MM and Gershman SJ (2017) The\nhippocampus as a predictive map.\nNature neuroscience\n20(11): 1643.\nStadie BC, Abbeel P and Sutskever I (2017) Third-person\nimitation learning. arXiv preprint arXiv:1703.01703 .\nSukhbaatar S, Kostrikov I, Szlam A and Fergus R (2017)\nIntrinsic motivation and automatic curricula via asymmetric\nself-play. arXiv preprint arXiv:1703.05407 .\nSutton RS (1991) Dyna, an integrated architecture for learning,\nplanning, and reacting. ACM SIGART Bulletin 2(4): 160–\n163.\nSutton RS and Barto AG (1998) Reinforcement learning: An\nintroduction, volume 1. MIT press Cambridge.\nSzita I and L¨orincz A (2006) Learning tetris using the noisy\ncross-entropy method. Neural computation 18(12): 2936–\n2941.\nTai L, Li S and Liu M (2016) A deep-network solution towards\nmodel-less obstacle avoidance. In: Intelligent Robots and\nSystems (IROS), 2016 IEEE/RSJ International Conference\non. IEEE, pp. 2759–2764.\nTai L, Paolo G and Liu M (2017) Virtual-to-real deep re-\ninforcement learning: Continuous control of mobile robots\nfor mapless navigation. In: 2017 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS). pp.\n31–36.\nTai L, Zhang J, Liu M and Burgard W (2018) Socially-\ncompliant navigation through raw depth inputs with gen-\nerative adversarial imitation learning.\nIn: Robotics and\nAutomation (ICRA), 2018 IEEE International Conference\non.\nTamar A, Wu Y, Thomas G, Levine S and Abbeel P (2016)\nValue iteration networks. In: Advances in Neural Informa-\ntion Processing Systems. pp. 2154–2162.\nThrun S, Burgard W and Fox D (2005) Probabilistic robotics.\nTobin J, Fong R, Ray A, Schneider J, Zaremba W and Abbeel\nP (2017) Domain randomization for transferring deep neural\nnetworks from simulation to the real world. In: Intelligent\nRobots and Systems (IROS), 2017 IEEE/RSJ International\nConference on. IEEE, pp. 23–30.\nTodorov E, Erez T and Tassa Y (2012) Mujoco: A physics\nengine for model-based control. In: Intelligent Robots and\nSystems (IROS), 2012 IEEE/RSJ International Conference\non. IEEE, pp. 5026–5033.\nTzeng E, Devin C, Hoffman J, Finn C, Abbeel P, Levine\nS, Saenko K and Darrell T (2015) Towards adapting deep\nvisuomotor representations from simulated to real environ-\nments. arXiv preprint arXiv:1511.07111 .\nTzeng E, Hoffman J, Zhang N, Saenko K and Darrell T (2014)\nDeep domain confusion: Maximizing for domain invariance.\narXiv preprint arXiv:1412.3474 .\nVan Hasselt H, Guez A and Silver D (2016) Deep reinforce-\nment learning with double q-learning. In: AAAI, volume 16.\npp. 2094–2100.\nVeˇcer´ık M, Hester T, Scholz J, Wang F, Pietquin O, Piot B,\nHeess N, Roth¨orl T, Lampe T and Riedmiller M (2017)\nLeveraging demonstrations for deep reinforcement learning\non robotics problems with sparse rewards. arXiv preprint\narXiv:1707.08817 .\nWang JX, Kurth-Nelson Z, Tirumala D, Soyer H, Leibo\nJZ, Munos R, Blundell C, Kumaran D and Botvinick M\n(2016a) Learning to reinforcement learn.\narXiv preprint\narXiv:1611.05763 .\nWang Z, de Freitas N and Lanctot M (2016b) Dueling network\narchitectures for deep reinforcement learning.\nIn: Pro-\nceedings of The 33rd International Conference on Machine\nLearning.\nWang Z, Merel JS, Reed SE, de Freitas N, Wayne G and\nHeess N (2017) Robust imitation of diverse behaviors. In:\nAdvances in Neural Information Processing Systems. pp.\n5326–5335.\nWeber T, Racani`ere S, Reichert DP, Buesing L, Guez A,\nRezende DJ, Badia AP, Vinyals O, Heess N, Li Y et al.\n(2017) Imagination-augmented agents for deep reinforce-\nment learning. arXiv preprint arXiv:1707.06203 .\nWilliams RJ (1992) Simple statistical gradient-following al-\ngorithms for connectionist reinforcement learning.\nIn:\nReinforcement Learning. Springer, pp. 5–32.\nWu Y, Mansimov E, Grosse RB, Liao S and Ba J (2017)\nScalable trust-region method for deep reinforcement learn-\ning using kronecker-factored approximation. In: Advances\nin neural information processing systems. pp. 5285–5294.\nWu Y, Wu Y, Gkioxari G and Tian Y (2018) Building gen-\neralizable agents with a realistic and rich 3d environment.\narXiv preprint arXiv:1801.02209 .\nWulfmeier M, Ondruska P and Posner I (2015) Maximum\nentropy deep inverse reinforcement learning. arXiv preprint\narXiv:1507.04888 .\nYang L, Liang X and Xing E (2018) Unsupervised real-to-\nvirtual domain uniﬁcation for end-to-end highway driving.\narXiv preprint arXiv:1801.03458 .\nYou Y, Pan X, Wang Z and Lu C (2017) Virtual to real\nreinforcement learning for autonomous driving.\narXiv\npreprint arXiv:1704.03952 .\nYu T, Finn C, Xie A, Dasari S, Zhang T, Abbeel P and\nLevine S (2018) One-shot imitation from observing hu-\nmans via domain-adaptive meta-learning.\narXiv preprint\narXiv:1802.01557 .\nZhang J, Springenberg JT, Boedecker J and Burgard W\n(2017a) Deep reinforcement learning with successor fea-\ntures for navigation across similar environments. In: 2017\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS). pp. 2371–2378.\nZhang J, Tai L, Boedecker J, Burgard W and Liu M (2017b)\nNeural slam. arXiv preprint arXiv:1706.09520 .\nZhang J, Tai L, Xiong Y, Liu M, Boedecker J and Burgard\nW (2018) Vr goggles for robots: Real-to-sim domain adap-\ntation for visual control. arXiv preprint arXiv:1802.00265\n.\nZhang T, McCarthy Z, Jow O, Lee D, Goldberg K and Abbeel\nP (2017c) Deep imitation learning for complex manipulation\ntasks from virtual reality teleoperation.\narXiv preprint\narXiv:1710.04615 .\nZhu JY, Park T, Isola P and Efros AA (2017a) Unpaired image-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n19\nto-image translation using cycle-consistent adversarial net-\nworks. arXiv preprint arXiv:1703.10593 .\nZhu Y, Mottaghi R, Kolve E, Lim JJ, Gupta A, Fei-Fei L and\nFarhadi A (2017b) Target-driven visual navigation in indoor\nscenes using deep reinforcement learning. In: Robotics and\nAutomation (ICRA), 2017 IEEE International Conference\non. IEEE, pp. 3357–3364.\nZhu Y, Wang Z, Merel J, Rusu A, Erez T, Cabi S, Tunya-\nsuvunakool S, Kram´ar J, Hadsell R, de Freitas N et al.\n(2018) Reinforcement and imitation learning for diverse\nvisuomotor skills. arXiv preprint arXiv:1802.09564 .\nZiebart BD, Maas AL, Bagnell JA and Dey AK (2008)\nMaximum entropy inverse reinforcement learning. In: AAAI,\nvolume 8. Chicago, IL, USA, pp. 1433–1438.\n",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.LG",
    "cs.SY"
  ],
  "published": "2016-12-21",
  "updated": "2018-04-09"
}