{
  "id": "http://arxiv.org/abs/1612.07454v1",
  "title": "How to Train Your Deep Neural Network with Dictionary Learning",
  "authors": [
    "Vanika Singhal",
    "Shikha Singh",
    "Angshul Majumdar"
  ],
  "abstract": "Currently there are two predominant ways to train deep neural networks. The\nfirst one uses restricted Boltzmann machine (RBM) and the second one\nautoencoders. RBMs are stacked in layers to form deep belief network (DBN); the\nfinal representation layer is attached to the target to complete the deep\nneural network. Autoencoders are nested one inside the other to form stacked\nautoencoders; once the stcaked autoencoder is learnt the decoder portion is\ndetached and the target attached to the deepest layer of the encoder to form\nthe deep neural network. This work proposes a new approach to train deep neural\nnetworks using dictionary learning as the basic building block; the idea is to\nuse the features from the shallower layer as inputs for training the next\ndeeper layer. One can use any type of dictionary learning (unsupervised,\nsupervised, discriminative etc.) as basic units till the pre-final layer. In\nthe final layer one needs to use the label consistent dictionary learning\nformulation for classification. We compare our proposed framework with existing\nstate-of-the-art deep learning techniques on benchmark problems; we are always\nwithin the top 10 results. In actual problems of age and gender classification,\nwe are better than the best known techniques.",
  "text": "How to Train Your Deep Neural Network with \nDictionary Learning \nVanika Singhal*, Shikha Singh+ and Angshul Majumdar# \n*IIIT Delhi  \nOkhla Phase 3 \nDelhi, 110020, India \nvanikas@iiitd.ac.in \n+IIIT Delhi  \nOkhla Phase 3 \nDelhi, 110020, India \nshikhas@iiitd.ac.in \n#IIIT Delhi  \nOkhla Phase 3 \nDelhi, 110020, India \nangshul@iiitd.ac.in \nAbstract: Currently there are two predominant ways to train deep neural \nnetworks. The first one uses restricted Boltzmann machine (RBM) and the \nsecond one autoencoders. RBMs are stacked in layers to form deep belief \nnetwork (DBN); the final representation layer is attached to the target to \ncomplete the deep neural network. Autoencoders are nested one inside the other \nto form stacked autoencoders; once the stcaked autoencoder is learnt the decoder \nportion is detached and the target attached to the deepest layer of the encoder to \nform the deep neural network. This work proposes a new approach to train deep \nneural networks using dictionary learning as the basic building block; the idea is \nto use the features from the shallower layer as inputs for training the next deeper \nlayer. One can use any type of dictionary learning (unsupervised, supervised, \ndiscriminative etc.) as basic units till the pre-final layer. In the final layer one \nneeds to use the label consistent dictionary learning formulation for \nclassification. We compare our proposed framework with existing state-of-the-\nart deep learning techniques on benchmark problems; we are always within the \ntop 10 results. In actual problems of age and gender classification, we are better \nthan the best known techniques.  \n1. Introduction \nInput\nTarget\nRepresentation\n \nInput\nTarget\nRepresentation\n \n \n \n \n(a) \n \n \n \n \n(b) \nFigure 1. a. Single Representation Layer Neural Network. b. Segregated Representation \nThe schematic diagram (Figure 1a) shows a shallow neural network with a single hidden \nlayer. Such a neural network is trained with training samples at the input and the \ncorresponding (usually binarized) class labels at the output targets. It learns the network \nweights by backpropagating the error.  \nTraining the shallow neural network can be perceived as a segregated problem – learning \nweights between the input and the hidden / representation layer and between \nrepresentation layer and the output / targets. If the network between the input and the \nrepresentation layer is already learnt, training the second network is trivial. It is a simple \nregression problem since both the input (representation of training samples) and the \noutputs are known. Training the first layer of network weights between the input and the \nrepresentation is the challenging task, since two variables (the network weights and the \nrepresentation) need to be learnt from the input training samples. This (training the first \nlayer) is the topic of ‘representation learning’.   \nInput (X)\nRepresentation (Z)\nNetwork D\n  \nInput\nRepresentation\nOutput=Input\nEncoder\nDecoder\n \n(a) \n \n \n \n \n(b) \nFigure 2. a. Restricted Boltzmann Machine. b. Autoencoder \nThe main concern while training the representation layer is that the information content \nof the input must be preserved. Currently there are two popular ways to train the \nrepresentation layer. One such approach is via Restricted Boltzmann Machine (RBM) [1]. \nRBM is an undirected graphical model that uses stochastic hidden units to model the \ndistribution over the stochastic visible units. The hidden layer is symmetrically connected \nwith the visible unit, and the architecture is “restricted” as there are no connections \nbetween units of the same layer. Traditionally, RBMs are used to model the distribution \nof the input data p(x).  \nThe schematic diagram of RBM is shown in Figure 2a. The objective is to learn the \nnetwork weights (W) and the representation (H). This is achieved by optimizing the \nBoltzmann cost function given by: \n(\n,\n)\n(\n,\n)\nE W H\np W H\ne\n\n \n \n \n \n \n \n \n \n \n(1) \nwhere, \n(\n,\n)\n-\nT\nE W H\nH WX\n\nincluding the bias terms. \nBroadly speaking, RBM learning is based upon maximizing the similarity between the \nprojection of the data and the representation, subject to the usual constraints of \nprobability. In RBM, the information content of the input is preserved in the sense of \nmaximizing similarity. Once the RBM is learnt, it is used as the first part of a single layer \nneural network. Once the targets are attached to the output of the RBM (and network \nweights learnt) it forms a complete neural network. \nThe second popular technique for representation learning is the autoencoder [2] (Figure \n2b). It consists of two parts – the encoder maps the input to a latent representation and the \ndecoder maps the latent representation back to the data. For a given input vector \n(including the bias term) x, the latent space is expressed as: \n(\n)\nh\nWx\n\n\n \n \n \n \n \n \n \n \n \n \n(2) \nHere is the non-linear activation function. The decoder reverse maps the representation \nto the data space – hence the name ‘autoencoder’ or ‘auto associative memory’.  \n' (\n)\nx\nW\nWx\n\n\n   \n \n \n \n \n \n \n \n \n(3) \nSince the data space is assumed to be the space of real numbers, there is no sigmoid \nfunction here. During training, the problem is to learn the encoding and decoding weights \n– W and W’. These are learnt by minimizing the Euclidean cost: \n2\n,\n'\narg min\n' (\n) F\nW W\nX\nW\nWX\n\n\n  \n \n \n \n \n \n \n \n(4) \nIn autoencoder, the information is preserved at the representation in the Euclidean sense, \nsuch that the inputs can be recovered with minimal l2-norm loss.  \nFor forming a neural network, the decoder portion of the autoencoder is removed. The \nencoder acts as the first layer (input to representation) of the neural network (Figure 1b). \nThe targets are attached to the representation and the corresponding weights are learnt to \ncomplete the neural network training.  \nIn recent times the Extreme Learning Machine (ELM) [3] is also gaining popularity. It is \na single layer neural network where the network weights between the input and the \nrepresentation layer are randomly assigned values. Therefore there is no representation \nlearning required. The second layer between the representation and the output is learnt in \nclosed form by minimizing the Euclidean loss. ELM is not the topic of discussion; but we \nmention it for the sake of completeness.   \nW2\nW1\nX\nH1\nH2\n \n \nInput Layer \nHidden Layer 1 \nOutput Layer \nHidden Layer L \n……… \n \n(a) \n \n \n \n \n \n(b) \nFigure 3. a. Deep Boltzmann Machine. b. Stacked Autoencoder \nUsually training a single layer neural network is easy; one does not employ RBM and \nautoencoder for training such shallow neural networks. Deep neural networks have \nmultiple representation layers. In such a case training them directly becomes difficult in \npractice. Representation learning techniques are used in such cases. One can either build \na deep neural network using RBM as the basic units or autoencoders.  \nRBMs can be stacked one after the other to form deep Botlzmann machine (DBM) \n(Figure 3a) [4]. DBM is undirected. There can be a directed model arising from stacking \nRBM leading to deep belief network (DBN) [5]; this is more attuned towards neural \nnetworks. The targets are attached to the final layer of the DBN and the weights between \nthe final representation layer and he target is learnt – thereby completing the training of \nthe deep neural network.  \nDeep networks can also be formed by stacking one autoencoder inside the other. This is \nshown in Figure 2b. These are called stacked autoencoders [5]; they have multiple levels \nof encoders and the same number of decoders. Once the stacked autoencoder is learnt, the \ndecoder portion is detached and the targets attached to the representation of the deepest \nlayer. This forms the deep neural network (once the weights between the deepest \nrepresentation layer and the target is learnt). \nThere are convolutional neural network (CNN) based deep learning models as well. They \nyield amazing results, but they are restricted mostly to imaging problems. Our interest \nlies in generic deep neural networks and hence CNN will not be discussed.  \nIn this work we will show how dictionary learning can be used as a representation \nlearning tool and deep neural networks be built with dictionary learning as basic units. \nThe proposed framework will be pitted against the best deep learning architectures on \nbenchmark problems; we will see how our simple framework features among the top 10 \nmethods. The framework has also been applied to the problem of face image based age \nand gender classification; we yield better results than the best known techniques.  \n2. Proposed Dictionary Based Deep Neural Network \n…\n=\nx                 D              z\n \n \nx                 D              z\n.\n.\n.\n \n(a) \n \n \n \n \n(b) \nFigure 4. a. Dictionary Learning. b. Our Neural Network Interpretation \nThe usual interpretation for dictionary learning is different is that it learns a basis (D) for \nrepresenting (Z) the data (X) (Figure 4a). The columns of D are called ‘atoms’. In this \nwork, we look at dictionary learning in a different manner. Instead of interpreting the \ncolumns as atoms, we can think of them as connections between the input and the \nrepresentation layer. To showcase the similarity, we have kept the color scheme intact in \nFigure 4b. \nUnlike a neural network which is directed from the input to the representation, the \ndictionary learning kind of network points in the other direction – from representation to \nthe input. Dictionary learning employs an Euclidean cost function (2), given by \n2\n,\nmin\nF\nD Z\nX\nDZ\n\n \n \n \n \n \n \n \n \n \n(5) \nThis is easily solved using alternating minimization of the dictionary D and the codes Z. \nToday most studies (following K-SVD [7]) impose an additional sparsity constraint on \nthe codes (Z), but it is not mandatory. \nNote that  dictionary learning indeed follow the basic premise of representation learning. \nThe information content of the inputs (X) are preserved in the features Z in the Euclidean \nsense.  \nBased on the neural network type interpretation of dictionary learning there are a handful \nof prior studies that proposed techniques to learn deeper features [8, 9]. The first layer of \ndictionary learns from the input training data. The subsequent layers learn from the \nfeatures from the previous layer as inputs. The prior studies only proposed a \nrepresentation learning tool. They did not learn a complete neural network. In this work, \nwe show how a deep neural network can be learnt with a plug-and-play approach. \n \nFigure 5. Deep Neural Network with Dictionary Learning. \nThe deep neural network is shown in Figure 5. Since dictionary learning is a synthesis \napproach the arrows are pointed (in the opposite direction) from the representation to the \ninput for the representation layers (Z1 and Z2). But for the final layer – between the final \nlevel of representation to the target the arrows point in the usual direction. For such a \nnetwork we write the cost function as: \n\n\n1\n2\n2\n1\n2\n,...\n, ,\nmin\n(... (\n))\nN\nN\nF\nF\nD\nD\nZ M X\nD\nD\nD Z\nT\nMZ\n\n\n\n\n\n\n  \n \n \n \n(6) \nHere D1 to DN are the N level dictionaries, Z the final level representation, T the targets \nand M the linear map from the representation to the targets.  \nSolving (6) exactly is a difficult problem. The difficulty arises in training all deep neural \nnetworks. To circumvent this, a greedy approach layer-by-layer training approach is \nfollowed [10]. We follow a greedy approach as well.  \nFor the first layer of dictionary learning, we can express \n\n\n1\n2 (... (\n))\nN\nZ\nD\nD Z\n\n\n\n\n. \nTherefore greedy learning of the first layer is represented by, \n1\n1\n2\n1\n1\n,\nmin\nF\nD Z\nX\nD Z\n\n \n \n \n \n \n \n \n \n \n(7) \nThis is a typical dictionary learning formulation. For the second layer we have \n\n\n1\n2 (... (\n))\nN\nZ\nD\nD Z\n\n\n\n\n; we substitute \n2\n3\n= (\n... (\n))\nN\nZ\nD\nD Z\n\n\n. This allows the expression  \n1\n1\n2\n2\n(\n)\nZ\nD Z\n\n\n \n \n \n \n \n \n \n \n \n(8) \nIt is easy to invert the activation function since it operates element-wise. This allows \nsolving (8) via dictionary learning. \n2\n2\n2\n1\n1\n2\n2\n,\nmin\n(\n)\nF\nD Z\nZ\nD Z\n\n\n  \n \n \n \n \n \n \n \n(9) \nOne may argue about values in Z that would make the output of \n1\nto be infinity. The \nproblem arises in any neural network. Recently an elegant solution has been proposed in \n[11] – that of adding slight amount of noise; we follow the same here.  \nWith such substitutions and dictionary learning we can learn until the penultimate layer. \nIn \nthe \nfinal \nlayer \nwe \nwill \nhave \nfor \nthe \nrepresentation \nlearning \nterm:\n1\n1\n1\n= (\n)\n(\n)=\nN\nN\nN\nN\nZ\nD Z\nZ\nD Z\n\n\n\n\n\n. This would lead to a cost function of the form \n2\n1\n1\n,\nmin\n(\n)-\nN\nN\nN\nF\nD\nZ\nZ\nD Z\n\n\n \n \n \n \n \n \n \n \n(10) \nBut there is also the term for mapping the representation to the targets – the second term \nis (6). Therefore, we need to add it to (10). The final level of joint representation and \nlinear map learning is therefore expressed as, \n2\n2\n1\n1\n, ,\nmin\n(\n)-\nN\nN\nN\nF\nF\nD\nZ M\nZ\nD Z\nT\nMZ\n\n\n\n\n \n \n \n \n \n \n(11) \nAlthough not a standard dictionary learning formulation (11) is a solved problem. It is \nknown as label-consistent KSVD [12] in computer vision literature.  \nWe have shown how the complex problem (6) can be segregated into smaller sub-\nproblems that have well-defined solutions in dictionary learning literature. For all the \nlayers till the final one a simple alternating minimization algorithms such as method of \noptimal directions [13] of multiplicative updates [14] can be used. For the last layer we \nuse LC-KSVD [12].  \nThere are several advantages of layer-wise learning: \n1. For each layer, well-tested dictionary learning algorithms are available. \n2. Learning the deep network is one go, requires solving a multitude of parameters. With \nlimited training data, learning so many networks leads to over-fitting. For greedy layer-\nwise learning the number of parameters to learn in each stage is relatively small. So the \nissue of over-fitting is less pronounced. \n3. There are certain mathematical guarantees for shallow dictionary learning [15]. These \nguarantees will be hard to generalize for multiple layers.  \n2.1. Plug-and-play Approach \nSo far we have discussed the use of standard dictionary learning for each of the layers. \nThis leads to the basic deep neural network architecture, but there is scope of further \nimprovement. In all the layers till the final layer the dictionary learning is unsupervised. \nSince each of the layers are learnt separately we can follow a plug-and-play approach for \nlearning these layers. We can pick up any supervised dictionary learning technique and \nuse it to generate features at each of the levels (before the final layer).  \nA few such examples of supervised dictionary learning will be given here. However there \nis a plethora of literature on this topic and given the limitations of space we cannot be \nencyclopedic in coverage.  \nOne of the first studies in supervised learning was proposed in [16]. Here on top of the \ndictionary learning cost function there is an extra term that accounts for classification \nerror: \n\n\n\n\n2\n, ,\nmin\n,\n,\nF\nD Z\nX\nDZ\nC y f Z\n\n\n\n\n\n \n \n \n \n \n \n \n(12) \nwhere \n\n\n1,1\ny\nand \n( )\nlog(1\n)\nx\nC x\ne\n\n\nis the classification error penalty that is very \nsimilar to the hinge loss used in SVM; \n( ,\n)\nf\nZ\nZ\nb\n\n\n\n\n.  \nWhen used in our plug-and-play deep learning framework, this technique is especially \nsuitable for solving binary classification problems. The features generated at level will be \noptimally separated for two classes. There are many other formulations for binary \nclassification using dictionary learning; for example [17] uses a Fisher linear discriminant \ncriterion. Owing to limitations in space we cannot discuss all such methods.  \nIn [18] a technique is proposed to address the multi-class feature learning problem. It \nlearns a separate dictionary for each class. The training samples are expressed as, \n1\n1\n...\nC\nC\nS\nS\nX\nD Z\nD Z\nD Z\n\n\n\n\n  \n \n \n \n \n \n \n(13) \nC classes are assumed here. D1 to DC are the class specific dictionaries, DS is the shared \ndictionary by all classes. Z1 to ZC are the features for each class and ZS the shared \nrepresentation. To make the representation discriminative [18] enforced that \n0\nciZ\n\n; It \nmeans that the non-zero coefficients of samples Xi will only concentrate on the sub-\ndictionaries Di and Ds, while the class-specific sub-dictionary Di will be having explicit \ncorrespondence to class labels i. The learning is formulated as, \n2\n2\n2\n,\n,\nmin\nT\ni\ni\ni\nS\ni s\ni\nj\nF\nF\nD Z\ni\ni\nj\nF\nX\nDZ\nX\nD Z\nD Z\nD D\n\n\n\n\n\n\n\n\n\n   \n \n \n(14) \nThe first two terms are the discriminative fidelity term. The last term is the mutual \nincoherence term between the dictionaries of every class.  An improvement of this \ntechniques was proposed in [19].  \nThere are several other formulations for multi-class supervised dictionary learning. It is \nnot possible to discuss all of them. However, we can pick up any suitable formulation and \ninstead of the unsupervised formulation in the pre-final layers, we can plug a multi-class \nsupervised dictionary learning techniques. \nFor the final layer we can use the simple LC-KSVD formulation as discussed before, or \nwe can use a slightly advanced version of it (dubbed LC-KSVD2 [12]) where class \nspecific atoms are learnt in the dictionary. This is given by, \n\n\n2\n2\n2\n1\n1\n, ,\n,\nmin\n(\n)-\nN\nN\nN\nF\nF\nF\nD\nZ M W\nZ\nD Z\nT\nMZ\nH\nWZ\n\n\n\n\n\n\n\n\n \n \n \n \n(15) \nH is a ‘discriminative’ sparse code corresponding to an input signal sample, if the \nnonzero values of Hi occur at those indices where the training sample Xi and the \ndictionary item D(N)k share the same label. \n3. Experimental Results \n3.1. Experiments on Benchmark Deep Learning Datasets \nIn this work we report results on object recognition benchmarking datasets – MNIST \n(error), CIFAR-10 (accuracy), CIFAR-100 (accuracy) and SVHN (error). We only \ncompare with prior published works (not including manuscripts in arxiv). Since all of \nthem are multi-class problems we try two variants of our proposed dictionary based deep \nneural network (DDNN). In the first one (DDNN1) unsupervised dictionary learning is \nused till the pre-final level; the final level uses LC-KSVD1. In the second variant \n(DDNN2) discriminative dictionary learning from [19] is used till the pre-final level; the \nfinal uses LC-KSVD2. Both variants use a three layer architecture; the number of \ndictionary atoms are halved in every layer. For the second variant, the number of atoms \nassigned to each dictionary is each layer is uniformly distributed across the classes. \nSince the said datasets have defined protocols, we just compare it with the results \nassembled by Rodrigo Beneson [20]; the results are shown in Tables 1-4. We find that \nour proposed techniques are always within the top 10. Most of the techniques in the \nfollowing tables are based on CNN – it requires significant hand tuning and heuristic \nparameter optimization. Our method is simple and straightforward and yet we perform at \npar or better than the most. We believe that using convolutional dictionary learning layers \nin the initial stages can boost the results even further.   \nTable 1. MNIST \nResult \nMethod \nVenue \n0.21% \nRegularization \nof \nNeural \nNetworks using DropConnect \nICML 2013 \n0.23% \nMulti-column \nDeep \nNeural \nNetworks for Image Classiﬁcation \nCVPR 2012 \n0.29% \nGeneralizing Pooling Functions in \nConvolutional Neural Networks: \nMixed, Gated, and Tree \nAISTATS \n2016 \n0.31% \nDDNN2 (Proposed) \n \n0.31% \nRecurrent Convolutional Neural \nNetwork for Object Recognition \nCVPR 2015 \n0.35% \nDeep Big Simple Neural Nets \nExcel \non \nHandwritten \nDigit \nRecognition \nNeural \nComputation \n2010 \n0.39% \nEfﬁcient \nLearning \nof \nSparse \nRepresentations with an Energy-\nBased Model \nNIPS 2006 \n0.40% \nDDNN1 (Proposed) \n \n0.40% \nBest Practices for Convolutional \nNeural \nNetworks \nApplied \nto \nVisual Document Analysis \nDAR 2003 \nTable 3. CIFAR-100 \nResult \nMethod \nVenue \n72.60% \nScalable Bayesian Optimization \nUsing Deep Neural Networks \nICML 2015 \n72.34% \nAll you need is a good init \nICLR 2015 \n69.17% \nLearning Activation Functions to \nImprove Deep Neural Networks \nICLR 2015 \n68.82% \nDDNN2 (Proposed) \n \n68.53% \nMulti-Loss \nRegularized \nDeep \nNeural Network \nCSVT 2015 \n68.40% \nSpectral \nRepresentations \nfor \nConvolutional Neural Networks \nNIPS 2015 \n68.25% \nRecurrent Convolutional Neural \nNetwork for Object Recognition \nCVPR 2015 \n68.00% \nDDNN1 (Proposed) \n \n67.76% \nTraining Very Deep Networks \nNIPS 2015 \n67.68% \nDeep \nConvolutional \nNeural \nNetworks as Generic Feature \nExtractors \nIJCNN 2015 \nTable 2. CIFAR-10 \nResult \nMethod \nVenue \n95.59% \nStriving for Simplicity: The All \nConvolutional Net \nICLR 2015 \n94.16% \nAll you need is a good init \nICLR 2015 \n93.95% \nGeneralizing Pooling Functions in \nConvolutional Neural Networks: \nMixed, Gated, and Tree \nAISTATS \n2016 \n93.63% \nScalable Bayesian Optimization \nUsing Deep Neural Networks \nICML 2015 \n93.08% \nDDNN2 (proposed) \n \n92.91% \nRecurrent Convolutional Neural \nNetwork for Object Recognition \nCVPR 2015 \n92.51% \nLearning Activation Functions to \nImprove Deep Neural Networks \nICLR 2015 \n92.40% \nTraining Very Deep Networks \nNIPS 2015 \n91.88% \nMulti-Loss \nRegularized \nDeep \nNeural Network \nCSVT 2015 \n91.77% \nDDNN1 (Proposed) \n \nTable 4. SVHN \nResult \nMethod \nVenue \n1.69% \nGeneralizing Pooling Functions in \nConvolutional Neural Networks: \nMixed, Gated, and Tree \nAISTATS \n2016 \n1.77% \nRecurrent Convolutional Neural \nNetwork for Object Recognition \nCVPR 2015 \n1.80% \nDDNN2 (proposed) \n \n1.92% \nRecurrent Convolutional Neural \nNetwork for Object Recognition \nCVPR 2015 \n1.94% \nRegularization \nof \nNeural \nNetworks using DropConnect \nICML 2013 \n2.15% \nBinaryConnect: Training Deep \nNeural Networks with binary \nweights during propagations \nNIPS 2015 \n2.26% \nDDNN1 (Proposed) \n \n2.35% \nNetwork in Network \nICLR 2014 \n2.47% \nMaxout Networks \nICML 2013 \n4.90% \nConvolutional neural networks \napplied to house numbers digit \nclassiﬁcation \nICPR 2012 \n3.2. Experiments on Age and Gender Classification from Face Image \nAdience is the benchmark dataset [21] for age and gender classification. The dataset \nconsists of images automatically uploaded to Flickr from smart-phone devices. Because \nthese images were uploaded without prior manual filtering, as is typically the case on \nmedia web-pages or social websites, viewing conditions in these images are highly \nunconstrained, reflecting many of the real-world challenges of faces appearing in Internet \nimages. Adience images therefore capture extreme variations in head pose, lightning \nconditions quality, and more. The entire Adience collection includes roughly 26K images \nof 2,284 subjects. Testing for age and gender classification is performed using a standard \nfive-fold, subject-exclusive cross-validation protocol, defined in [21]. We use the in-\nplane aligned version of the faces used there in. \nWe have compared our method with the very best available methods – DEX (Deep \nExpectation) [22] (winner of ChaLearn LAP Challenge at ICCV 2015 for age estimation) \nand Levi and Hassner [23] (best results on Adience). It is shown in the following table.  \nFor our proposed formulation, we have used the DDNN1 as the base model. Since age \nprediction is a multi-class problem we use DDNN2. For gender prediction (being a binary \nclassification problem) we use the FLD dictionary learning formulation [17] in each of \nthe pre-final stages; this is the DDNN3 formulation. The number of dictionary atoms are \nhalved in every layer.  \nTable 5. Age and Gender Classification Results \nMethod \nAge Prediction \nGender Prediction \nLevi and Hassner [23] (over-sampling) \n50.7 \n86.8 \nLevi and Hassner [23] (single crop) \n49.5 \n85.9 \nDEX [22] \n46.6 \nCannot predict gender \nDDNN1 \n49.8 \n86.2 \nDDNN2 & DDNN3 \n50.5 (DDNN2) \n87.0 (DDNN3) \nWe find that DDNN1 yields better results than the DEX [22] method for age prediction. \nIt also uses better results than [23] when the full image is used. But [23] proposed a \nsecond formulation where patches are taken from the image; DDNN1 cannot beat this \nmethod. However our proposed supervised formulations DDNN2 and DDNN3 yields \neven better results than the patch based formulation proposed in [23]. \n4. Conclusion \nThis work proposed a new method to train deep neural networks. Prior studies used RBM \nor autoencoder as the basic building blocks. This work shows how dictionary learning \ncan be used as building blocks for deep neural networks. The framework is flexible and \none can build the deep network in a plug-and-play fashion. One can pick and choose any \ndictionary learning variant of choice for each layer. There is a plethora of dictionary \nlearning techniques to choose from, and one has the liberty to mix and match these \ntechniques in our proposed plug-and-play framework. \nThis work applies the proposed framework for training deep neural network to some \ncomputer vision problems. We show that our technique always ranks among the top few \non benchmark deep learning datasets. When applied to the problem of face image based \ngender and age classification, we beat the state-of-the-art.  \nReferences \n[1] R. Salakhutdinov, A. Mnih and G. Hinton, “Restricted Boltzmann machines for collaborative \nfiltering”, ACM ICML, pp. 791-798, 2007.  \n[2] P. Baldi, “Autoencoders, unsupervised learning, and deep architectures”. ICML workshop on \nunsupervised and transfer learning, pp. 37-50, 2012. \n[3] G. B. Huang, Q. Y. Zhu, and C. K. Siew, “Extreme learning machine: theory and \napplications”, Neurocomputing, Vol. 70 (1), 489-501, 2006. \n[4] R. Salakhutdinov and G. E. Hinton, “Deep Boltzmann Machines”, AISTATS, 2009. \n[5] N. Le Roux and Y. Bengio, “Representational power of restricted Boltzmann machines and \ndeep belief networks”, Neural computation, Vol. 20 (6), pp. 1631-1649, 2008. \n[6] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio and P. A. Manzagol, “ Stacked denoising \nautoencoders: Learning useful representations in a deep network with a local denoising \ncriterion”. Journal of Machine Learning Research, Vol. 11, pp. 3371-3408, 2010. \n[7] R. Rubinstein, A. M. Bruckstein and M. Elad, \"Dictionaries for Sparse Representation \nModeling\", Proceedings of the IEEE, Vol. 98 (6), pp. 1045-1057, 2010. \n[8] V. Singhal, A. Gogna and A. Majumdar, “Deep Dictionary Learning vs Deep Belief Network \nvs Stacked Autoencoder: An Empirical Analysis”, ICONIP 2016. \n[9] S. Tariyal, A. Majumdar, R. Singh and M. Vatsa. “Greedy Deep Dictionary Learning”, \narXiv:1602.00203. \n[10] Y. Bengio, P. Lamblin, P. Popovici and H. Larochelle, “Greedy Layer-Wise Training of Deep \nNetworks”, NIPS, 2007. \n[11] Ç. Gülçehre, M. Moczulski, M. Denil and Y. Bengio, “Noisy Activation Functions”, ICML, \n2016. \n[12] Z. Jiang, Z. Lin and L. S. Davis, “Learning A Discriminative Dictionary for Sparse Coding \nvia Label Consistent K-SVD”, IEEE Transactions on Pattern Analysis and Machine \nIntelligence, Vol. 35, pp. 2651-2664, 2013. \n[13] K. Engan, S. Aase, and J. Hakon-Husoy, “Method of optimal directions for frame design,” \nIEEE ICASSP, 1999. \n[14] C. J. Lin, “On the convergence of multiplicative update algorithms for nonnegative matrix \nfactorization”, IEEE Transactions on Neural Networks, Vol. 18(6), 1589-1596, 2007. \n[15] S. Arora, A. Bhaskara, R. Ge and T. Ma, \"More Algorithms for Provable Dictionary \nLearning\", arXiv:1401.0579v1 \n[16] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, “Supervised dictionary learning”, \nNIPS, 2008. \n[17] M. Yang, L. Zhang, X. Feng, and D. Zhang, “Fisher discrimination dictionary learning for \nsparse representation”, ICCV, 2011. \n[18] Y. Sun, Q. Liu, J. Tang and D. Tao, “Learning Discriminative Dictionary for Group Sparse \nRepresentation,” IEEE Transactions on Image Processing, Vol. 23 (9), pp. 3816-3828, Sept. \n2014. \n[19] S. Yadav, M. Singh, M. Vatsa, R. Singh and A. Majumdar, “Low Rank Group Sparse \nRepresentation Based Classifier for Pose Variation”, IEEE ICIP 2016. \n[20] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html \n[21] E. Eidinger, R. Enbar and T. Hassner. “Age and gender estimation of unfiltered faces”, IEEE \nTransactions on Information Forensics and Security, Vol. 9 (12), pp. 2170-2179, 2014.  \n[22] R. Rothe, R. Timofte and L. V. Gool. “Deep expectation of real and apparent age from a \nsingle image without facial landmarks”, International Journal of Computer Vision, pp. 1-14, \n2016. \n[23] G. Levi and T. Hassner. “Age and gender classification using convolutional neural \nnetworks”. IEEE CVPR Workshop on Analysis and Modeling of Faces and Gestures, 34-42 \n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2016-12-22",
  "updated": "2016-12-22"
}