{
  "id": "http://arxiv.org/abs/1612.07548v1",
  "title": "Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning",
  "authors": [
    "Wendelin Böhmer",
    "Rong Guo",
    "Klaus Obermayer"
  ],
  "abstract": "This paper investigates a type of instability that is linked to the greedy\npolicy improvement in approximated reinforcement learning. We show empirically\nthat non-deterministic policy improvement can stabilize methods like LSPI by\ncontrolling the improvements' stochasticity. Additionally we show that a\nsuitable representation of the value function also stabilizes the solution to\nsome degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.",
  "text": "arXiv:1612.07548v1  [cs.AI]  22 Dec 2016\nThis paper has been presented at the 13th European Workshop on Reinforcement Learn-\ning (EWRL 2016) on the 3rd and 4th of December 2016 in Barcelona, Spain.\nNon-Deterministic Policy Improvement\nStabilizes Approximated Reinforcement Learning\nWendelin B¨ohmer∗and Rong Guo and Klaus Obermayer\nNeural Information Processing Group, Technische Universit¨at Berlin,\nMarchstraße 23, D-10587 Berlin, Germany.\n∗corresponding author (email: wendelin@ni.tu-berlin.de)\nEditor: Gergely Neu, Vince¸c G´omez and Csaba Szepesv´ari\nAbstract\nThis paper investigates a type of instability that is linked to the greedy policy improvement\nin approximated reinforcement learning. We show empirically that non-deterministic policy\nimprovement can stabilize methods like LSPI by controlling the improvements’ stochastic-\nity. Additionally we show that a suitable representation of the value function also stabilizes\nthe solution to some degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.\nKeywords:\nstability, approximate reinforcement learning, non-deterministic policy im-\nprovement, least-squares policy iteration, slow-feature-analysis representation\n1. Introduction\nThis paper investigates a type of instability that is linked to the greedy policy improvement\nin approximated reinforcement learning. We show empirically that non-deterministic policy\nimprovement can be used to achieve stability for large discount factors.\nThe presented\napproach is simple and should also be easily transferable to more sophisticated algorithms.\nRecently deep reinforcement learning (deep RL) has been very successful in solving\ncomplex tasks in large, often continuous state spaces (e.g. playing Atari games and Go,\nMnih et al., 2015; Silver et al., 2016).\nThese approaches use gradient based Q-learning\n(Watkins and Dayan, 1992) or policy gradient methods (Williams, 1992).\nGradients in\nneural networks must be based on i.i.d. distributed samples, though (see Riedmiller, 2005).\nDeep RL uses therefore mini-batches that are sampled i.i.d. from a ﬁxed set of experiences,\nwhich has been collected before training (called experience replay, Mnih et al., 2013).\nIn diﬀerence to online algorithms, which are often guaranteed to converge in the limit of\nan inﬁnite training sequence (e.g. Sutton et al., 2009), batch learning has long been known to\nbe vulnerable to the choice of training sets (Tsitsiklis and Van Roy, 1997; Bertsekas, 2007).\nDepending on the batch of training samples at hand, an RL algorithm can either converge\nto an almost optimal or to an arbitrarily bad policy. In practice, this depends strongly (but\nnot predictably) on the discount factor γ. For example, in Figure 1 we demonstrate that\n1\nB¨ohmer, Guo and Obermayer\nFigure 1: Navigation performance of policies, learned by LSPI in two environments (see\nsketched layouts), for varying discount factors γ. Error bars indicate mean and\nstandard deviation of the fraction of successful test-trajectories (starting at ran-\ndom positions) over 10 random-walk training sets with 50000 samples each. The\nagent can either move forward or rotate 45◦left or right (i.e. 3 actions). Reaching\nthe goal area is rewarded (+1) and crashing into a wall is punished (-1 or -10).\npolicies learned by least-squares policy iteration (LSPI, Lagoudakis and Parr, 2003) yield\nvery diﬀerent performances when the discount factor γ is varied (experimental details can\nbe found in Section 3). The left plot shows an unpredictable drop in performance for a\nsimple navigation experiment with continuous states and discrete actions, and the right\nplot the failure of LSPI to learn a suitable policy for a more complicated environment. The\nyoung discipline of deep RL has not yet reported eﬀects like these, but it is reasonable to\nassume that they happen in batch algorithms with more sophisticated architectures as well.\nMost authors attribute this instability to a lack of convergence guarantees in oﬀ-policy\nbatch value estimation (see Dann et al., 2014, for an overview). But the distribution of\ntraining samples in the batch may also have a profound impact on the policy improvement\nin approximate RL. For example, Perkins and Precup (2002) show for an algorithm simi-\nlar to LSPI, that the greedy policy improvement can cause the instability shown in Figure\n1. Although their analysis does not carry over to LSPI1, they show that a sequence of\nnon-deterministic policies converge reliably when they are changed slowly enough. Conser-\nvative policy iteration (CPI, Kakade and Langford, 2002) follows a similar line of thought\nand slows down the policy improvement considerably to guarantee convergence2. Safe pol-\nicy iteration (SPI, Pirotta et al., 2013) extends this concept by determining the speed of\n1.\nPerkins and Precup (2002) use open-ended on-policy online value estimation.\nTraining samples are\ndrawn every time the policy is improved and errors on observed samples can thus average out over time.\n2. In CPI, the next policy πi+1 is a combination of the previous policy πi and the greedy policy π∗\ni+1, i.e.,\nπi+1 = (1 −α)πi + α π∗\ni+1. CPI converges for small α ∈[0, 1]. In SPI the update rate α is determined\nby maximizing a lower bound on the policy improvement, which converges much faster than CPI.\n2\nNon-Deterministic Policy Improvement Stabilizes Approximated RL\nchange through a lower bound on the policy improvement. The algorithm improves con-\nvergence speed signiﬁcantly, but is computationally expensive even in ﬁnite state spaces.\nOther approaches suggest an actor-critic architecture to avoid oscillations (Wagner, 2011)\nor optimize a parameterizable softmax-policy directly (Azar et al., 2012).\nIn this paper we evaluate the idea of Perkins and Precup (2002) empirically with LSPI\nin continuous navigation tasks. Surprisingly, we ﬁnd that the stochasticity of the improved\npolicy stabilizes the solution, rather than the slowness of policy change.\nThis requires\nonly a small modiﬁcation to the policy improvement scheme. Although our approach is a\nheuristic and theoretically not as well-grounded as the above algorithms, it is fast, simple\nto implement, and can be applied to most algorithms used in deep RL.\n2. Non-Deterministic Policy Improvement\nIn this paper we consider tasks with continuous state space X and discrete3 action space\nA. A non-deterministic policy π(a|x) ∈[0, 1] , ∀a ∈A , P\na′∈A π(a′|x) = 1 , ∀x ∈X , can be\nevaluated by any algorithm to estimate the corresponding Q-value function q : X ×A →IR.\nTo converge to the optimal policy, the policy π must also be improved, either during Q-value\nestimation or in an additional step. The improvement in a state x ∈X usually chooses the\naction a ∈A that maximizes the current Q-value estimate q(x, a). Instead of this greedy\nimprovement, we propose to produce an improved non-deterministic policy. Examples are\nsoftmax πq\nβ or ǫ-greedy πq\nǫ policies4, that is, ∀a ∈A, ∀x ∈X:\nπq\nβ(a|x) =\nexp\n\u0000β q(x, a)\n\u0001\nP\na′∈A\nexp\n\u0000β q(x, a′)\n\u0001\nor\nπq\nǫ(a|x) = ǫ\n1\n|A| +\n( 1 −ǫ , if a = arg max\na′∈A\nq(x, a′)\n0\n, otherwise\n.\nExisting algorithms can be adapted by identifying the greedy policy improvement operator\nˆΓ∗and replacing it with the non-deterministic ˆΓβ, that is, for functions f, q : X × A →IR:\nˆΓ∗[f|q](x) = f\n\u0000x, arg max\na∈A\nq(x, a)\n\u0001\n⇒\nˆΓβ[f|q](x) =\nX\na∈A\nπq\nβ(a|x) f(x, a) ,\n∀x ∈X .\nHere β ∈[0, ∞) denotes the inverse stochasticity of the operator.\nFor example, a non-\ndeterministic version of the TD-error δt in Q-learning for the observation (xt, at, rt, xt+1) is\nδt = rt+γ ˆΓβ[q|q](xt+1)−q(xt, at), and the matrix A ∈IRm×m, which has to be inverted dur-\ning non-deterministic least-squares temporal diﬀerence learning (LSTD, Bradtke and Barto,\n1996, used by LSPI), would be computed from a training batch {xt, at, rt}n\nt=0 by\nAij\n=\n1\nn\nn−1\nP\nt=0\nφi(xt, at)\n\u0010\nφj(xt, at) −γˆΓβ[φj|q](xt+1)\n\u0011\n,\n∀i, j ∈{1, . . . , m} .\nSoftmax policies use more information than ǫ-greedy and are in most situations the better\nchoice. However, the stochasticity of the softmax depends strongly on the diﬀerences be-\ntween Q-values. Far away from the reward, Q-values can become very similar and softmax\n3.\nThe extension to continuous action spaces is straight forward, but requires to compute an integral for\neach application of the policy improvement operator ˆΓβ[f|q](x) =\nR\nπq\nβ(a|x) f(x, a) da.\n4.\nThe softmax is also called the Boltzmann or the Gibbs policy. Note the similarities to the policies of\nWagner (2011) and Azar et al. (2012), which both implement a softmax based on the optimized function.\n3\nB¨ohmer, Guo and Obermayer\npolicies become almost uniform distributions. The level of stochasticity turns out to be the\nmost reliable stabilizer for LSPI, and we used in our experiments (see Section 3) normalized\nQ-values ¯q for non-deterministic policy improvement ˆΓβ[f|¯q]. This normalizes the stochas-\nticity for all states by normalizing the diﬀerence between Q-values, that is, ∀x ∈X, ∀a ∈A:\n¯q(x, a) = q(x, a) −µ(x)\nσ(x)\n,\nµ(x) =\n1\n|A|\nP\na′∈A\nq(x, a′) ,\nσ(x) =\nr\n1\n|A|\nP\na′∈A\n\u0000q(x, a′)\n\u00012 −\n\u0000µ(x)\n\u00012 .\n3. Experiments\nWe evaluated the eﬀects of non-deterministic policy improvement at the example of a simple\nnavigation experiment in an U- and a S-shaped environment (see inlays of Figure 1). The\nthree dimensional state space X consisted of the agent’s two-dimensional position and its\norientation. The action space A contained 3 actions: a forward movement and two 45◦\nrotations. Crashing into a wall stopped movement and it would take the agent between 20\nand 25 unimpeded moves to traverse the environment in one spatial dimension. Reaching\nthe goal area (gray circle in the inlays) yielded a reward of +1 and crashes incurred a\npunishment of -1 in the U-shaped and -10 in the S-shaped environment. To represent the\nQ-value function, we chose a Fourier basis (Konidaris et al., 2011) and constructed 1500\nbasis functions over the space of states and actions. The bases contained all combinations\nof: 10 cosine functions (including a constant) for each spatial dimension; a constant, 2\ncosine and 2 sine functions for the orientation; and 3 discrete Kronecker-delta functions\nfor the actions. Irrespective their policy improvement, policies were evaluated greedily to\nremain comparable. Performance was measured in fraction of successful trajectories, which\nwe estimated by running the greedy policy from 200 random starting positions/orientations.\nSuccessful trajectories reach the goal within 100 actions without hitting a wall.\n3.1 Non-Deterministic Policy Improvement\nWe started out to test the idea of Perkins and Precup (2002) for LSPI by using non-\ndeterministic policy improvement (soft-LSPI) with slowly growing inverse stochasticity β\n(similar to simulated annealing, Haykin, 1998). However, we observed that the annealing\nprocess itself did not improve the learned policy. The performance was always comparable\nto soft-LSPI with the annealing’s ﬁnal stochasticity β (not shown here).\nFigure 2 plots the performance of greedy-LSPI and soft-LSPI (with constant stochas-\nticity β) for varying discount factors γ. In the face of sparse rewards, γ determines how far\nthat reward is propagated, before it is drowned in inevitable approximation errors. Low γ\nyields policies that are only correct close to the reward, and have therefore a bad perfor-\nmance. On the other hand, γ close to 1 can lead to nearly optimal policies everywhere, but\nperformance is strongly aﬀected by the instability investigated in this paper. Note that the\nlarge standard deviations in both plots stem from some training sets producing near opti-\nmal, while others producing nonsensical policies. For this reason we refer to these regimes\nas “instable”. First, one can observe that increasing stochasticity (lower β) drastically sta-\nbilizes the soft-LSPI policies. Secondly, note that there seems to be a trade-oﬀbetween\ninverse stochasticity β and discount factor γ. Low β reduces performance while increasing\nstability, but in the left plot the performance with low β becomes near optimal for larger γ,\n4\nNon-Deterministic Policy Improvement Stabilizes Approximated RL\nFigure 2: LSPI with greedy and softmax policy improvement, compared in the navigation\ntasks of Figure 1. Large standard deviations are usually caused by a mixture\nof excellent and horrible policies.\nWe therefore call these regimes “instable”.\nStochastic improvements (with small β, e.g. green triangles) decrease performance\nfor small γ, but stabilize convergence for large γ signiﬁcantly.\nFigure 3: LSPI policies based on diﬀerent representations in the navigation tasks of Figure\n1. Better representations (here RSK-SFA) generally improve performance, but\nnon-deterministic policy improvement is still needed to stabilize LSPI in complex\ntasks (e.g., right plot). Also note the pronounced trade-oﬀbetween β and γ.\n5\nB¨ohmer, Guo and Obermayer\ntoo. It appears therefore that instabilities can generally be counteracted by simultaneously\nlowering β and raising γ.\n3.2 Stabilization by Representation\nSo far the above instabilities have only been demonstrated for LSPI. One could argue that\nmore sophisticated approaches must not be aﬀected in the same way. In deep neural net-\nworks, for example, the lower layers may provide a representation of the state-action space\nthat stabilizes policy improvement. We want to investigate this by choosing basis functions,\nwhich are known to represent value functions well. B¨ohmer et al. (2013) show that features\nlearned by non-linear slow feature analysis (SFA, Wiskott and Sejnowski, 2002) approxi-\nmate an optimal encoding for value functions of all tasks in the same environment5. We\nused regularized sparse kernel SFA (RSK-SFA, B¨ohmer et al., 2012) with Gaussian kernels\nto learn such features from the training data. Figure 3 shows the results in comparison with\nthe trigonometric Fourier basis functions introduced above. Using the SFA representation\ncompletely avoided instability for the simpler task in the left plot (blue diamonds). The\nperformance improves in the S-shaped environment too, but the large standard deviations\nindicate that here greedy LSPI is not very stable for large discount factors γ. Soft-LSPI\nwith a low β (green triangles) stabilizes the solution, though. Using a deep architecture\nmay therefore reduce instability, but will probably not remove it all together. Nonetheless,\nour results suggest that non-deterministic policy improvement should be able to stabilize\ndeep architectures, too.\n4. Conclusion\nWe have shown that (at least) LSPI can become instable in some unpredictable regimes of\nthe discount factor γ. Here small diﬀerences in the training set can lead to large diﬀerences\nin policy performance. It is not exactly clear why solutions become instable, but we show\nthat learned policies can be stabilized by using a non-deterministic policy improvement\nscheme. All presented experiments became signiﬁcantly more stable by increasing stochas-\nticity 1\nβ and discount factor γ at the same time. Future works may extend our approach by\nadjusting both parameters during policy iteration (like in SPI, Pirotta et al., 2013). Better\nrepresentations of the state-action space have also improved stability to some extend. More\nsophisticated approaches (like deep RL) learn these representations implicitly in their lower\nlayers and may therefore be more stable than LSPI. Nonetheless, instabilities will probably\noccur, and non-deterministic policy improvement can most likely be employed to stabilize\nthe learned policy in deep RL, too.\nIn conclusion, when success or failure of learned policies depends crucially on the train-\ning set (e.g. during cross-validation), one should consider a non-deterministic policy im-\nprovement scheme. The scheme presented in this paper is computationally cheap, easy to\nimplement, and can be ﬁne-tuned with the inverse stochasticity β.\n5.\nStrictly speaking, this holds only for values of the sampling policy of the training data. However, SFA\nfeatures are reported to work well with LSPI for random-walk training sets (B¨ohmer et al., 2013).\n6\nNon-Deterministic Policy Improvement Stabilizes Approximated RL\nAcknowledgments\nWe thank the anonymous reviewers, who pointed us in the direction of CPI. This work was\nfunded by the German science foundation (DFG) within SPP 1527 autonomous learning.\nReferences\nMohammad Gheshlaghi Azar, Vicen¸c G´omez, and Hilbert J. Kappen.\nDynamic policy\nprogramming. Journal of Machine Learning Research, 13:3207–3245, 2012.\nDimitri P. Bertsekas.\nDynamic Programming and Optimal Control, volume 2.\nAthena\nScientiﬁc, 3rd edition, 2007.\nWendelin B¨ohmer, Steﬀen Gr¨unew¨alder, Hannes Nickisch, and Klaus Obermayer. Gen-\nerating feature spaces for linear algorithms with regularized sparse kernel slow feature\nanalysis. Machine Learning, 89(1-2):67–86, 2012.\nWendelin B¨ohmer, Steﬀen Gr¨unew¨alder, Yun Shen, Marek Musial, and Klaus Obermayer.\nConstruction of approximation spaces for reinforcement learning. Journal of Machine\nLearning Research, 14:2067–2118, July 2013.\nSteven J. Bradtke and Aandrew G. Barto. Linear least-squares algorithms for temporal\ndiﬀerence learning. Machine Learning, 22(1/2/3):33–57, 1996.\nChristoph Dann, Gerhard Neumann, and Jan Peters.\nPolicy evaluation with temporal\ndiﬀerences: a survey and comparison. Journal of Machine Learning Research, 15:809–\n883, 2014.\nSimon Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall, 2nd edition,\n1998. ISBN 978-0132733502.\nSham Kakade and John Langford. Approximately optimal approximate reinforcement learn-\ning. In Proceedings of the 19th International Conference on Machine Learning, pages\n267–274, 2002.\nG. D. Konidaris, S. Osentoski, and P.S. Thomas. Value function approximation in reinforce-\nment learning using the Fourier basis. In Proceedings of the Twenty-Fifth Conference on\nArtiﬁcial Intelligence, 2011.\nMichail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine\nLearning Research, 4:1107–1149, 2003.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,\nDaan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning.\nIn NIPS Deep Learning Workshop. 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig\n7\nB¨ohmer, Guo and Obermayer\nPetersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Ku-\nmaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through\ndeep reinforcement learning. Nature, 518(7540):529–533, February 2015.\nTheodore J. Perkins and Doina Precup. A convergent form of approximate policy iteration.\nIn Advances in Neural Information Processing Systems 15, pages 1595–1602. 2002.\nMatteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy\niteration. In Proceedings of the 30th International Conference on Machine Learning, pages\n307–315, 2013.\nMartin Riedmiller. Neural ﬁtted Q-iteration - ﬁrst experiences with a data eﬃcient neural\nreinforcement learning method. In 16th European Conference on Machine Learning, pages\n317–328. Springer, 2005.\nDavid Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George\nvan den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam,\nMarc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya\nSutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and\nDemis Hassabis. Mastering the game of Go with deep neural networks and tree search.\nNature, 529:484–503, January 2016.\nRichard S. Sutton, Csaba Szepesv´ari, and Hamid Reza Maei.\nA convergent o(n) algo-\nrithm for oﬀ-policy temporal-diﬀerence learning with linear function approximation. In\nAdvances in Neural Information Processing Systems, volume 21, pages 1609–1616. MIT\nPress, 2009.\nJohn N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-diﬀerence learning with\nfunction approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997.\nPaul Wagner. A reinterpretation of the policy oscillation phenomenon in approximate policy\niteration. In Advances in Neural Information Processing Systems 24, pages 2573–2581.\n2011.\nChristopher Watkins and Peter Dayan. Q-learning. Machine Learning, 8:279–292, 1992.\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist rein-\nforcement learning. Machine Learning, 8(3):229–256, 1992.\nLaurenz Wiskott and Terrence Sejnowski. Slow feature analysis: unsupervised learning of\ninvariances. Neural Computation, 14(4):715–770, 2002.\n8\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2016-12-22",
  "updated": "2016-12-22"
}