{
  "id": "http://arxiv.org/abs/1612.09057v4",
  "title": "Deep Learning and Hierarchal Generative Models",
  "authors": [
    "Elchanan Mossel"
  ],
  "abstract": "It is argued that deep learning is efficient for data that is generated from\nhierarchal generative models. Examples of such generative models include\nwavelet scattering networks, functions of compositional structure, and deep\nrendering models. Unfortunately so far, for all such models, it is either not\nrigorously known that they can be learned efficiently, or it is not known that\n\"deep algorithms\" are required in order to learn them.\n  We propose a simple family of \"generative hierarchal models\" which can be\nefficiently learned and where \"deep\" algorithm are necessary for learning. Our\ndefinition of \"deep\" algorithms is based on the empirical observation that deep\nnets necessarily use correlations between features. More formally, we show that\nin a semi-supervised setting, given access to low-order moments of the labeled\ndata and all of the unlabeled data, it is information theoretically impossible\nto perform classification while at the same time there is an efficient\nalgorithm, that given all labelled and unlabeled data, perfectly labels all\nunlabelled data with high probability.\n  For the proof, we use and strengthen the fact that Belief Propagation does\nnot admit a good approximation in terms of linear functions.",
  "text": "arXiv:1612.09057v4  [cs.LG]  4 Sep 2018\nDeep Learning and Hierarchical Generative Models\nElchanan Mossel∗\nMIT\nSeptember 6, 2018\nAbstract\nIt is argued that deep learning is eﬃcient for data that is gener-\nated from hierarchal generative models. Examples of such generative\nmodels include wavelet scattering networks, functions of compositional\nstructure, and deep rendering models. Unfortunately so far, for all such\nmodels, it is either not rigorously known that they can be learned eﬃ-\nciently, or it is not known that “deep algorithms” are required in order\nto learn them.\nWe propose a simple family of “generative hierarchal models” which\ncan be eﬃciently learned and where “deep” algorithm are necessary for\nlearning. Our deﬁnition of “deep” algorithms is based on the empir-\nical observation that deep nets necessarily use correlations between\nfeatures. More formally, we show that in a semi-supervised setting,\ngiven access to low-order moments of the labeled data and all of the\nunlabeled data, it is information theoretically impossible to perform\nclassiﬁcation while at the same time there is an eﬃcient algorithm,\nthat given all labelled and unlabeled data, perfectly labels all unla-\nbelled data with high probability.\nFor the proof, we use and strengthen the fact that Belief Propaga-\ntion does not admit a good approximation in terms of linear functions.\n∗Supported by ONR grant N00014-16-1-2227 and NSF CCF-1665252 and DMS-1737944\nelmos@mit.edu\n1\n1\nIntroduction\nWe assume that the reader is familiar with the basic concepts and develop-\nments in deep learning. We do not attempt to summarize the big body of\nwork studying neural networks and deep learning. We refer readers who are\nunfamiliar with the area to [GBC16] and the references within.\nWe hypothesize that deep learning is eﬃcient in learning data that is\ngenerated from generative hierarchical models.\nThis hypothesis is in the\nsame spirit of the work of Bruna, Mallat and others who suggested wavelet\nscattering networks [BM13] as the generative model, the work of Mhaskar,\nLiao and Poggio who suggested compositional functions as the generative\nmodel [MLP16] and work by Patel, Nguyen, and Baraniuk [PNB15] who\nsuggested hierarchical rending models. Unfortunately so far, for all previous\nmodels, it is either not rigorously known that they can be learned eﬃciently,\nor it is not known that “deep algorithms” are required in order to learn\nthem.\nThe approach presented in this paper is motivated by connections be-\ntween deep learning and evolution. In particular, we focus on simple gener-\native evolutionary models as our generative processes. While these models\nare not appropriate models for images or language inference, they provide\nseveral advantages:\n• There are well established biological processes of evolution. Thus the\ngenerative model studied is not a human-made abstraction but the\nactual process that generated the data.\n• The models are mathematically simple enough so we can provide very\naccurate answers as to the advantage of the depth in a semi-supervised\nsetting.\nWe further note that some of the most successful applications of deep\nlearning are in labeling objects that are generated in an evolutionary fashion\nsuch as the identiﬁcation of animals and breeds from images, see e.g. [LBH15]\nand the reference within. Let us consider the problem of identifying species\nfrom images.\nOne form of this problem was tackled by Darwin.\nIn his\nEvolution of Species, Darwin used phylogenetic trees to summarize the evo-\nlution of species [Dar59]. The evolutionary tree, in turn, helps in identifying\nobserved species. The problem of species identiﬁcation took a new twist\nin the DNA age, where morphological characters of species were replaced\nby DNA sequences as the data for inference of the relationship between\nspecies [Fel04, SS03].\n2\nOur hypothesis that deep learning is eﬀective in recovering generative hi-\nerarchal models leads us to explore other generative models and algorithms\nto recover these models. While the models we will develop in the current\nwork are restrictive, they represent an attempt to extend the phylogenetic\ntheory from the problem of reconstructing trees based on DNA sequences to\nreconstructing relationships based on diﬀerent types of representations with\nthe ultimate goal of understanding “real” representations such as represen-\ntations of natural languages and natural images.\nIn what follows we introduce a family of models.\nWe start with the\nphylogenetic model. The phylogenetic model we use, the symmetric Markov\nmodel, is a classical model. However, we study it from a new perspective:\n• First - in addition to a DNA sequence, each node of the tree is asso-\nciated with a label where diﬀerent nodes might have the same label.\nFor example, a node with a speciﬁc DNA sequence might have the\nlabel,”dog” or ”mammal”.\n• Second - we are interested in the semi-supervised learning problem,\nwhere the labels of a small subset of the data are known and the goal\nis to recover the labels of the remaining data.\nWe then deﬁne novel generative models which have additional features:\n• Change of representation. In phylogenetic models, the representation\nis given by the DNA sequences (or RNA, proteins etc.), while it seems\nlike in many deep learning situations, there isn’t necessarily a canonical\nrepresentation. We model this by introducing a permutation on the\nalphabet between every node and each of its descendants.\n• Interaction between features.\nIn classical phylogenetic models each\nletter evolves independently, while in most deep learning scenarios,\nthe interaction between features is key. We introduce a model that\ncaptures this property.\nIn order to establish the power of deep learning, we deﬁne two types of\nlimited algorithms (which are not ”deep”):\n• Local algorithms. Such algorithms have to determine the label of each\ndata point based on the labeled data only. The notion of local algo-\nrithms is closely related to the notion of supervised learning. Note,\nhowever, that local algorithms do not output a classiﬁer after observ-\ning the labeled data ; instead for each sample of unlabeled data we\nrun the local algorithm.\n3\n• Shallow Algorithms. These algorithms only use summary statistics for\nthe labeled data. In other words, such algorithms are not allowed to\nutilize high order correlations between diﬀerent features of the labeled\ndata (our results will apply to algorithms that can use bounded order\ncorrelations).\nIn our main results, we provide statistical lower bounds on the perfor-\nmance of local and shallow algorithms. We also provide eﬃcient algorithms\nthat are neither shallow nor local. Thus our results provide a formal in-\nterpretation of the power of deep learning. In the conclusion, we discuss a\nnumber of research directions and open problems.\n1.1\nRelated Work\nOur work builds on work in theoretical phylogenetics, with the aim of pro-\nviding a new theoretical perspective on deep learning. An important feature\nof our generative models is that they include both representation and labels.\nIn contrast, most of the work in the deep learning literature focuses on the\nencoding within the deep network. Much of the recent work in deep learning\ndeals with the encoding of data from one level to the next. In our model, we\navoid this (important) aspect by considering only representations that are es-\nsentially 1 to 1 (if we exclude the eﬀect of the noise or “nuisance variables”).\nThus our main focus is in obtaining rigorous results relating multi-level hi-\nerarchical models and classes of semi-supervised learning algorithms whose\ngoal is to label data generated from the models.\nA main theme of research in the theory of deep networks is studying\nthe expressive power of bounded width networks in terms of their depth,\nsee e.g. [CSS16, ES16, Tel16] and also [MLP16].\nOur results show that\nlearning of deep nets cannot be performed by simple methods that apply\nto shallow generative models. We also note that positive theoretical results\nof [ABGM14]. These, however, are not accompanied by lower bounds show-\ning that deep algorithms are needed in their setup.\n2\nHierarchical Generative Models\nIn this section, we will deﬁne the generative models that will be discussed\nin the paper.\n4\n2.1\nThe space of all objects - a tree\nAll the models will be deﬁned on a d-ary tree T = (V, E) of h levels, rooted\nat v0.\nThe assumption that the tree is regular is made for simplicity. Most of\nthe results can be extended to many other models of trees, including random\ntrees.\n2.2\nRepresentations\nThe representation is a function R : V →[q]k. The representation of node\nv ∈V is given by R(v).\nIn some examples, representations of nodes at diﬀerent levels are of the\nsame type.\nFor example, if we think of T as a phylogenetic tree, then\nR(v) may represent the DNA sequence of v. In other examples, R(v) has a\ndiﬀerent meaning for nodes at diﬀerent levels. For example, if T represents\na corpus of images of animals, then R(v) for a deep node v may deﬁne the\nspecies and the type of background, while R(v) at a lower level may represent\nthe pixels of an image of an animal (this is an illustration - none of the models\npresented at the paper are appropriate for image classiﬁcation).\nGiven a root to leaf path v0, . . . , vℓ, we will consider R(vℓ−1) as a higher\nlevel representation of R(vℓ),. Similarly R(vℓ−2) is a higher level representa-\ntion of R(vℓ−1) (and therefore of R(vℓ). Thus, each higher level representa-\ntion has many descendant lower level representations. In particular, all of\nthe representations considered are derived from R(v0).\n2.3\nLabels\nEach node v ∈V of the tree has a set of labels L(v). L(v) may be empty for\nsome nodes v. We require that if w is a descendant of v then L(v) ⊆L(w)\nand that every two nodes v1, v2 that have the same label ℓhave a common\nancestor v3 with label ℓ. In other words, the set of nodes labeled by a certain\nlabel is a node in the tree and all nodes below that node.\nFor example, a possible value for L(v) is {“dog”, “germanshepherd”}.\n2.4\nThe inference problem\nLet LT denote the set of leaves of T. Let S ⊂LT . The input to the inference\nproblem consists of the set {(R(v), L(v)) : v ∈S} which is the labeled data\nand the set {R(v) : v ∈LT \\ S} which is the unlabeled data.\n5\nThe desired output is L(v) for all v ∈LT , i.e., the labels of all the leaves\nof the tree.\n2.5\nGenerative Models\nWe consider a number of increasingly complex generative models. While\nall of the models are stylized, the more advanced ones capture more of\nthe features of “deep learning” compared to the simpler models.\nAll of\nthe models will be Markov models on the tree T (rooted at v0). In other\nwords, for each directed edge of the tree from a parent v to child w, we have\na transition matrix Mv,w of size [q]k × [q]k that determines the transition\nprobabilities from the representation R(v) to the representation R(w). We\nconsider the following models:\n2.5.1\nThe i.i.d. Model (IIDM)\nWe ﬁrst consider one of the simplest and most classical phylogenetic models\ngiven by i.i.d. symmetric Markov models. Special cases of this model, for\nq = 2 or q = 4 are some of the most basic phylogenetic evolutionary models.\nThese models called the CFN and Jukes-Cantor model respectively [JC69,\nNey71, Far73, Cav78]. The model is deﬁned as follows: If w is the parent of\nv then for each 1 ≤i ≤k independently, it holds that conditioned on R(w)\nfor all a ∈[q]:\nP[R(v)i = a] = 1 −λ\nq\n+ λδ(R(w)i = a).\nIn words, for each letter of R(w) independently, the letter given by the\nparent is copied with probability λ and is otherwise chosen uniformly at\nrandom.\n2.5.2\nThe Varying Representation Model (VRM)\nOne of the reasons the model above is simpler than deep learning models\nis that the representation of nodes is canonical. For example, the model\nabove is a classical model if R(w) is the DNA sequence of node w but is\na poor model if we consider R(w) to be the image of w, where we expect\ndiﬀerent levels of representation to have diﬀerent “meanings”. In order to\nmodel the non-canonical nature of neural networks we will modify the above\nrepresentation as follows. For each edge e = (w, v) directed from the parent\nw to the child v, we associate a permutation σe ∈Sq, which encodes the\nrelative representation between w and v. Now, we still let diﬀerent letters\n6\nevolve independently but with diﬀerent encodings for diﬀerent edges. So we\nlet:\nP[R(v)i = a] = 1 −λ\nq\n+ λδ(R(w)i = σ−1\ne (a)).\nIn words, each edge of the tree uses a diﬀerent representation of the set [q].\nWe say the the collection σ = (σe : v ∈E) is adversarial if σ is chosen\nby an adversary. We say that it is random if σe are chosen i.i.d. uniform.\nWe say that σ = (σe : v ∈E) are shared parameters if σe is just a function\nof the level of the edge e.\n2.5.3\nThe Feature Interaction Model (FIM)\nThe additional property we would like to introduce in our most complex\nmodel is that of an interaction between features. While the second model\nintroduces some indirect interaction between features emanating from the\nshared representation, deep nets include stronger interaction. To model the\ninteraction for each directed edge e = (w, v), we let σe ∈Sq2. We can view\nσe as a function from [q]2 →[q]2 and it will be useful for us to represent it as\na pair of functions σe = (fe, ge) where fe : [q]2 →[q] and ge : [q]2 →[q]. We\nalso introduce permutations Σ1, . . . , Σh ∈Sk which correspond to rewiring\nbetween the diﬀerent levels. We then let\nP[]\nR(v)i = a] = 1 −λ\nq\n+ λδ(R(w)i = a),\nand\nR(v)2i = fe(Σ|v|( ]\nR(w)(2i)), Σ|v|( ]\nR(w)(2i + 1))\nR(v)2i+1 = ge(Σ|v|( ]\nR(w)(2i)), Σ|v|( ]\nR(w)(2i + 1)).\nIn words, two features at the parent mutate to generate two features at\nthe child. The wiring between diﬀerent features at diﬀerent levels is given\nby some known permutation that is level dependent. This model resembles\nmany of the convolutional network models and our model and results easily\nextend to other variants of interactions between features.\nFor technical\nreasons we will require that for all i, j it holds that\n{Σj(2i), Σj(2i + 1)} ̸= {2i, 2i + 1}.\n(1)\nIn other words, the permutations actually permute the letters. It is easy\nto extend the model and our results to models that have more than two\nfeatures interact.\n7\n2.6\nThe parameter sharing setup\nWhile the traditional view of deep learning is in understanding one object -\nwhich is the deep net, our perspective is diﬀerent as we consider the space\nof all objects that can be encoded by the network and the relations between\nthem.\nThe two-point of view are consistent in some cases though.\nWe\nsay that the VRM model is ﬁxed parametrization if the permutation σe\nare the same for all the edges at the same level.\nSimilarly the FIM is\nparameter shared if the functions (fe, ge) depend on the level of the edge e\nonly. For an FIM model with a ﬁxed parametrization, the deep network that\nis associated with the model is just given by the permutation Σi ∈Sk and\nthe permutations (f 1, g1), . . . , (f h, gh) only. While our results and models\nare stated more generally, the shared parametrization setup deserves special\nattention:\n• Algorithmically: The shared parametrization problem is obviously eas-\nier - in particular, one expects, that as in practice, after the parameters\nσ and Σ are learned, classiﬁcation tasks per object should be performed\nvery eﬃciently.\n• Lower bounds: our lower bounds hold also for the shared parametriza-\ntion setup.\nHowever as stated in conjecture 6.1, we expect much\nstronger lower bounds for the FIM model. We expect that such lower\nbound hold even in the shared parametrization setup.\n3\nShallow, Local and Deep Learning\nWe deﬁne “deep learning” indirectly by giving two deﬁnitions of “shallow”\nlearning and of “local” learning. Deep learning will be deﬁned implicitly as\nlearning that is neither local nor shallow.\nA key feature that is observed in deep learning is the use of the correlation\nbetween features. We will call algorithms that do not use this correlation or\nuse the correlation in a limited fashion shallow algorithms.\nRecall that the input to the inference problem D is the union of the\nlabeled and unlabeled data\nD := {(R(v), L(v)) : v ∈S} ∪{R(v) : v ∈LT \\ S}.\nDeﬁnition 3.1. Let A = (A1, . . . , Aj) where Ai ⊂[k] for 1 ≤i ≤j. The\ncompression of the data according to A, denoted CA(D), is\nCA(D) := {R(v) : v ∈LT \\S}∪\n\u0010\nnD(Ai, x, ℓ) : 1 ≤i ≤j, x ∈[q]|Ai|, ℓis a label\n\u0011\n,\n8\nwhere for every possible label ℓ, 1 ≤i ≤j and x ∈[q]|Ai|, we deﬁne\nnD(Ai, x, ℓ) := #{v ∈S : L(v) = ℓ, R(v)Ai = x}.\nThe canonical compression of the data, C∗(D), is given by CA1,...,Ak(D),\nwhere Ai = {i} for all i.\nIn words, the canonical compression gives for every label ℓand every\n1 ≤i ≤k, the histogram of the i’th letter (or column) of the representation\namong all labeled data with label ℓ. Note that the unlabeled data is still\ngiven uncompressed.\nThe more general deﬁnition of compression allows for histograms of joint\ndistributions of multiple letters (columns). Note in particular that if A =\n(A1) and A1 = [k], then we may identify CA(D) with D as no compression\nis taking place.\nDeﬁnition 3.2. We say that an inference algorithm is s-shallow if the out-\nput of the algorithm as a function of the data depends only on CA(D) where\nA = (A1, . . . , Aj) and each Ai is of size at most s. We say that an inference\nalgorithm is shallow if the output of the algorithm as a function of the data\ndepends only on C∗(D).\nIn both cases, we allow the algorithm to be randomized, i.e., the output\nmay depend on a source of randomness that is independent of the data.\nWe next deﬁne what local learning means.\nDeﬁnition 3.3. Given the data D, we say that an algorithm is local if\nfor each R(w) for w ∈LT \\ S, the label of v is determined only by the\nrepresentation of w, R(w), and all labeled data {(R(v), L(v)) : v ∈S}.\nCompared to the deﬁnition of shallow learning, here we do not compress\nthe labeled data. However, the algorithm has to identify the label of each\nunlabeled data point without access to the rest of the unlabeled data.\n4\nMain Results\nOur main results include positive statements establishing that deep learning\nlabels correctly in some regimes along with negative statements that estab-\nlish that shallow or local learning do not. Combining both the positive and\nnegative results establishes a large domain of the parameter space where\ndeep learning is eﬀective while shallow or local learning isn’t. We conjecture\nthat the lower bounds in our paper can be further improved to yield a much\nstronger separation (see conjecture 6.1).\n9\n4.1\nMain parameters\nThe results are stated are in terms of\n• the branching rate of the tree d,\n• the noise level 1 −λ,\n• the alphabet size [q] and\n• The geometry of the set S of labeled data.\nAnother crucial parameter is k, the length of representation. We will con-\nsider k to be between logarithmic and polynomial in n = dh.\nWe will also require the following deﬁnition.\nDeﬁnition 4.1. Let ℓbe a label. We say that ℓis well represented in S if\nthe following holds. Let v ∈V be the vertex closest to the root v0 that is\nlabeled by ℓ(i.e., the set of labels of v0 contains ℓ). Then there are two edge\ndisjoint path from v to v1 ∈S and to v2 ∈S.\nThe following is immediate\nProposition 4.2. If the tree T is known and if ℓis well represented in S\nthen all leaves whose label is ℓcan be identiﬁed.\nProof. In general the set of leaves Lℓ, labeled by ℓcontains the leaves L′ of\nthe subtree rooted at the most common ancestor of all the elements of S\nlabeled by ℓ. If ℓis well represented, then Lℓ= L′.\n4.2\nThe power of deep learning\nTheorem 4.3. Assume that dλ2 > 1 or dλ > 1 + ε and q ≥q(ε) is suﬃ-\nciently large. Assume further that k ≥C log n. Then the following holds for\nall three models (IIDM, VRM, FIM) with high probability:\n• The tree T can be reconstructed. In other words, there exists an eﬃ-\ncient (deep learning) algorithm that for any two representation R(u)\nand R(v), where u and v are leaves of the tree, computes their graph\ndistance.\n• For all labels ℓthat are well represented in S, all leaves labeled by ℓ\ncan be identiﬁed.\n10\n4.3\nThe weakness of shallow and local learning\nWe consider two families of lower bounds - for local algorithms and for\nshallow algorithms.\nIn both cases, we prove information theory lower bounds by deﬁning\ndistributions on instances and showing that shallow/local algorithms do not\nperform well against these distributions.\n4.3.1\nThe Instances and lower bounds\nLet h0 < h1 < h. The instance is deﬁned as follows. Let dist denote the\ngraph distance\nDeﬁnition 4.4. The distribution over instances I(h0, h1) is deﬁned as fol-\nlows. Initialize S = ∅.\n• All nodes v1 with dist(v1, v0) < h0 are not labeled.\n• The nodes with dist(v1, v0) = h0 are labeled by a random permutation\nof ′′1”, . . . ,′′ dh0!”.\n• For each node v1 with dist(v1, v0) = h0, pick two random descendants\nv2, v′\n2 with dist(v′\n2, v0) = dist(v2, v0) = h1, such that the most common\nancestor of v2, v′\n2 is v1. Add to S all leaves in LT that are descendants\nof v2 and v′\n2.\nTheorem 4.5. Given an instance drawn from 4.4 and data generated from\nIIDM, VRM or FIM, the probability that a local algorithm labels a random\nleaf in LT \\ S correctly is bounded by\nd−h0(1 + O(kλh−h1q)).\nNote that given the distribution speciﬁed in the theorem, it is trivial to\nlabel a leaf correctly with probability d−h0 by assigning it any ﬁxed label.\nAs expected our bound is weaker for longer representations. A good choice\nfor h1 is h0 + 1 (or h0 + 2 if d = 2), while a good choice of h0 is 1, where we\nget the bound\nd−1(1 + O(kλh)q),\ncompared to d−1 which can be achieved trivially.\nTheorem 4.6. Consider a compression of the data CA(D), where A =\n(A1, . . . , Am) and let s = maxj≤m|Aj|. If dλ2 < 1 and given an instance\n11\ndrawn from 4.4 and data generated from IIDM, VRM or FIM, the probability\nthat a shallow algorithm labels a random leaf in LT \\ S correctly is at most\nd−h0 + Cmdh0 exp(−c(h −h1)),\nwhere c and C are positive constants which depend on λ and s.\nAgain, it is trivial to label nodes correctly with probability d−h0. For\nexample if h0 = 1, h1 = 2 (or = 3 to allow for d = 2) and we look at the\ncanonical compression C∗(D), we obtain the bound d−1 + O(k exp(−ch)) =\nd−1 + O(kn−α) for some α > 0. Thus when k is logarithmic of polyloga-\nrithmic in n, it is information theoretically impossible to label better than\nrandom.\n5\nProof Ideas\nA key idea in the proof is the fact that Belief Propagation cannot be ap-\nproximated well by linear functions. Consider the IIDM model with k = 1\nand a known tree. If we wish to estimate the root representation, given\nthe leaf representations, we can easily compute the posterior using Belief\nPropagation. Belief Propagation is a recursive, thus deep algorithm. Can it\nbe performed by a shallow net?\nWhile we do not answer this question directly, our results crucially rely\non the fact that Belief Propagation is not well approximated by one layer\nnets. Our proofs build on and strengthen results in the reconstruction on\ntrees community [MP03, JM04] by showing that there is a regime of param-\neters where Belief Propagation has a very good probability of estimating\nthe roof from the leaves.\nYet, 1 layer nets have an exponentially small\ncorrelation in their estimate.\nThe connection between reconstructing the roof value for a given tree\nand the structural question of reconstructing the root has been studied ex-\ntensively in the phylogenetic literature since the work of [Mos04] and the\nreminder of our proof builds on this connection to establish the main results.\n6\nDiscussion\nTheorem 4.5 establishes that local algorithms are inferior to deep algorithms\nif most of the data is unlabeled. Theorem 4.6 shows that in the regime where\nλ−1 ∈(\n√\nd, d) and for large enough q, shallow algorithms are inferior to deep\nalgorithms. We conjecture that stronger lower bound and therefore stronger\nseparation can be obtained for the VRM and FIM models. In particular:\n12\nConjecture 6.1. In the setup of Theorem 4.6 and the VRM model, the\nresults of the theorem extend to the regime dλ4 < 1.\nIn the case of the\nFIM model it extends to a regime where λ < 1 −φ(d, h), where φ decays\nexponentially in h.\n6.1\nRandom Trees\nThe assumption that the generative trees are regular was made for the ease\nof expositions and proofs. A natural follow up step is to extend our results\nto the much more realistic setup of randomly generated trees.\n6.2\nBetter models\nOther than random trees, better models should include the following:\n• The VRM and FIM both allow for the change of representation and for\nfeature interaction to vary arbitrarily between diﬀerent edges. More\nrealistic models should penalize variation in these parameters. This\nshould make the learning task easier.\n• The FIM model allows interaction only between ﬁxed nodes in one\nlevel to the next. This is similar to convolutional networks. However,\nfor many other applications, it makes sense to allow interaction with a\nsmall but varying number of nodes with a preference towards certain\nlocalities. It is interesting to extend the models and results in such\nfashion.\n• It is interesting to consider non-tree generating networks. In many\napplications involving vision and language, it makes sense to allow to\n“concatenate” two or more representations. We leave such models for\nfuture work.\n• As mentioned earlier, our work circumvents autoencoders and issues\nof overﬁtting by using compact, almost 1-1 dense representations. It\nis interesting to combine our “global’ framework with “local” autoen-\ncoders.\n6.3\nMore Robust Algorithms\nThe combinatorial algorithms presented in the paper assume that the data\nis generated accurately from the model. It is interesting to develop a robust\nalgorithm that is eﬀective for data that is approximately generated from\n13\nthe model.\nIn particular, it is very interesting to study if the standard\noptimization algorithms that are used in deep learning are as eﬃcient in\nrecovering the models presented here. We note that for the phylogenetic\nreconstruction problem, even showing that the Maximum Likelihood tree is\nthe correct one is a highly non-trivial task and we still do not have a proof\nthat standard algorithms for ﬁnding the tree, actually ﬁnd one, see [RS15].\n6.4\nDepth Lower bounds for Belief Propagation\nOur result suggest the following natural open problem:\nProblem 6.2. Consider the broadcasting process with k = 1 and large q in\nthe regime λ ∈(1/d, 1/\n√\nd). Is it true that the BP function is uncorrelated\nwith any network of size polynomial in dh and depth o(h)?\n7\nThe power of deep learning: proofs\nThe proof of all positive results is based on the following strategy:\n• Using the representations {R(v) : v ∈LT } reconstruct the tree T.\n• For each label ℓ, ﬁnd the most common w ancestor of {v : v ∈\nLT, R(v) ∈S, L(v) = ℓ} and label all nodes in the subtree root at\nw by ℓ.\nFor labels that are well represented, it follows that if the tree constructed\nat the ﬁrst step is the indeed the generative tree, then the identiﬁcation\nprocedure at the second step indeed identiﬁes all labels accurately.\nThe reconstruction of the tree T is based on the following simple iterative\n“deep” algorithm in which we iterate the following. Set h′ = h.\nLS This step computes the Local Structure of the tree: For each w1, w2\nwith dist(v0, w1) = dist(v0, w2) = h′, compute min(dist(w1, w2), 2r+2).\nThis identiﬁes the structure of the tree in levels min(h′ −r, . . . , h′).\nCond If h′ −r ≤0 then EXIT, otherwise, set h′ := h′ −r.\nAR Ancestral Reconstruction.\nFor each node w with dist(v0, w) = h′,\nestimate the representation R(w) from all its descendants at level h′+r.\nThis meta algorithm follows the main phylogenetic algorithm in [Mos04].\nWe give more details on the implementation of the algorithm in the 3 setups.\n14\n7.1\nIIDM\nWe begin with the easiest setup and explain the necessary modiﬁcation for\nthe more complicated ones later.\nThe analysis will use the following result from the theory of reconstruc-\ntion on trees.\nProposition 7.1. Assume that dλ2 > 1 or dλ > 1 + ε and q ≥q(ε) is\nsuﬃciently large. Then there exists λ1 > 0 and r such that the following\nholds. Consider a variant of the IIDM model with r levels and k = 1. For\neach leaf v, let R′(v) ∼λ2δR(v) + (1 −λ2)U, where λ2 > λ1 and U is a\nuniform label. Then there exists an algorithm that given the tree T, and\n(R′(v) : v ∈LT ) returns R′(v0) such that R′(v) ∼λ3δR(v)+(1−λ3)U where\nλ3 > λ1.\nProof. For the case of dλ2 > 1 this follows from[KS66, MP03]. In the other\ncase, this follows from [Mos01].\nLet λ(h′) denote the quality of the reconstructed representations at level\nh′. We will show by induction that λ(h′) > λ1 and that the distances be-\ntween nodes are estimated accurately. The base case is easy as we can accu-\nrately estimate the distance of each node to itself and further take λ(h) = 1.\nTo estimate the distance between w1 and w2 we note that the expected\nnormalized hamming distance dH(R(w1), R(w2)) between R(w1) and R(w2)\nis:\nq −1\nq\n(1 −λ(h′)2λdist(w1,w2))\nand moreover the Hamming distance is concentrated around the mean. Thus\nif k ≥C(λ′, q, r) log n then all distances up to 2r will be estimated accurately\nand moreover all other distances will be classiﬁed correctly as being larger\nthan 2r + 2. This establishes that the step LS is accurate with high proba-\nbility. We then apply Proposition 7.1 to recover ˆR(v) for nodes at level h′.\nWe conclude that indeed λ(h′) > λ1 for the new value of h′.\n7.2\nVRM\nThe basic algorithm for the VRM model is similar with the following two\nmodiﬁcations:\n• When estimating graph distance instead of the Hamming distance\ndH(R(w1), R(w2)), we compute\nd′\nH(R(w1), R(w2)) = min\nσ∈Sq dH (σ (R(w1)) , R(w2)) ,\n(2)\n15\ni.e., the minimal Hamming distance over all relative representations of\nw1 and w2. Again, using standard concentration results, we see that\nif k ≥C(λ′, q, r) log n then all distances up to 2r will be estimated\naccurately. Moreover, for any two nodes w1, w2 of distance at most 2r,\nthe minimizer σ in (2) is unique and equal to the relative permutation\nwith high probability. We write σ(w1, w2) for the permutation where\nthe minimum is attained.\n• To perform ancestral reconstruction, we apply the same algorithm as\nbefore with the following modiﬁcation: Given a node v at level h′\nand all of its descendants at level r + h′, w1, . . . , whd. We apply the\nreconstruction algorithm in Proposition 7.1 to the sequences\nR(w1), σ(w1, w2)(R(w2)), . . . , σ(w1, whd)(R(whd)),\nwhere recall that σ(w1, wj) is the permutation that minimizes the Ham-\nming distance between R(w1) and R(wj). This will insure that the\nsequence ˆR(v) has the right statistical properties. Note that addition-\nally to the noise in the reconstruction process, it is also permuted by\nσ(w1, v).\n7.3\nFIM\nThe analysis of FIM is similar to VRM. The main diﬀerence is that while in\nVRM model, we reconstructed each sequence up to a permutation σ ∈Sq, in\nthe FIM model there are permutations over Sq2 and diﬀerent permutations\ndo not compose as they apply to diﬀerent pairs of positions. In order to\novercome this problem, as we recover the tree structure, we also recover the\npermutation (fe, ge) up to a permutation σ ∈Sq that is applied to each\nletter individually.\nFor simplicity of the arguments, we assume that d ≥3. Let w1, w2, w3\nbe three vertices that are identiﬁed as siblings in the tree and let w be their\nparent. We know that R(w1), R(w2) and R(w3) are noisy versions of R(w),\ncomposed with permutations τ1, τ2, τ3 on Sq2.\nWe next apply concentration arguments to learn more about τ1, τ2, τ3.\nTo do so, recall that k ≥C(λ) log n. Fix x = (x1, x2) ∈[q]2, and consider all\noccurrences of τ1(x) in R(w1). Such occurrences are correlated with τ2(x) in\nR(w2) and τ3(x) in R(w3). We now consider occurrences of τ2(x) in R(w2)\nand τ3(x) in R(w3). Again the most common co-occurrence in w1 is τ1(x).\nThe following most likely occurrences values will be the 2q −1 values y\nobtained as τ1(x1, z2)), or τ1((z1, x2)) where z1 ̸= x1, z2 ̸= x2.\n16\nIn other words, for each value τ1(x) we recover the set\nA(x) = B(x) ∪C(x),\nwhere\nB(x) = {τ1(x1, z2) : z2 ̸= x2},\nC(x) = {τ1(z1, x2) : z1 ̸= x1}.\nNote that if y1, y2 ∈B(x) then y2 ∈A(y1) but this is not true if y1 ∈B(x)\nand y2 ∈C(x). We can thus recover for every value τ1(x) not only the set\nA(x) but also its partition into B(x) and C(x) (without knowing which one\nis which).\nOur next goal is to recover\n{(x, B(x)) : x ∈[q]2},\n{(x, C(x)) : x ∈[q]2}\nup to a possible global ﬂip of B and C. In order to do so, note that\n{x} ∪B(x) ∪y∈C(x) B(y) = [q]2,\nand if any of the B(y) is replaced by a C(y), this is no longer true. Thus\nonce we have identiﬁed B(y) for one y ∈C(x), we can identify B(y) for all\ny ∈C(x). Repeating this for diﬀerent values of x, recovers the desired B\nand C.\nWe next want to reﬁne this information even further. WLOG let x =\nτ1(0, 0) and let y = τ1(a, 0) ∈C(x). and z = (0, b) ∈B(x). And note that\nC(y) ∩B(z) contains a single element, i.e., τ1(a, b). We have thus recovered\nτ1 up to a permutation of S[q] as needed.\nAfter recovering τ1, τ2, τ3 etc. we may recover the ancestral state at their\nparent w up to the following degrees of freedom (and noise)\n• A permutation of Sq applied to each letter individually.\n• A global ﬂip of the sets B and C.\nIn general the second degree of freedom cannot be recovered. However\nif v2 is a sister of v1 (with the same degrees of freedom), then only the\ncorrect choice of the B/C ﬂips will minimize the distance deﬁned by taking\na minimum over permutations in Sq2. Thus by a standard concentration\nargument we may again recover the global B/C ﬂip and continue recursively.\nNote that this argument is using condition (1).\n17\n8\nThe limited power of limited algorithms\n8.1\nThe Limited Power of Local Algorithms\nTo prove lower bounds it suﬃces to prove them for the IIDM model as is a\nspecial case of the more general models. We ﬁrst prove Theorem 4.5.\nProof. Let R(w) be an unlabeled leaf representation. Let M = dh0. Let\nu1, . . . , uM denote the nodes level h0 and denote their labels by ℓ1, . . . , ℓM.\nLet vi, v′\ni denote the nodes below ui at level h1 with the property that the\nleaves of the tree rooted at vi are the elements of S with label ℓi.\nLet ui be the root of the tree w belongs to and let xi be the lowest\nintersection between the path from w to ui and the path between vi and v′\ni.\nWe write h′ for dist(w, xi). Note that h′ ≥h −h1. For j ̸= i let xj be the\nnode on the path between vj and v′\nj such that dist(vj, xj) = dist(vi, xi). We\nassume that in addition to the labeled data we are also given h′ and\nD′ = (ℓ1, R(x1)), . . . , (ℓM, R(xm)).\nNote that we are not given the index i.\nOf course having more information reduces the probability of error in\nlabeling R(w). However, note that R(w) is independent of {(R(v), L(v)) :\nv ∈S} conditioned on D′ and h′.\nIt thus suﬃces to upper bound the\nprobability of labeling R(w) correctly given D′. By Bayes:\nP[L(w) = ℓi|D′, h′] =\nP[R(w)|D′, L(w) = ℓi, h′]\nPM\nj=1 P[R(w)|D′, L(w) = ℓj, h′]\n=\nP[R(w)|R(xi), h′]\nPM\nj=1 P[R(w)|R(xj), h′]\nWe note that\n \n(1 −λh′)/q\n(λh′ + (1 −λh′)/q)\n!k\n≤P[R(w)|R(xi), h′]\nP[R(w)|R(xj), h′] ≤\n \n(λh′ + (1 −λh′)/q)\n(1 −λh′)/q\n!k\nSo the ratio is\n1 + O(kλh′q) = 1 + O(kλh−h1q)\nand therefore the probability of correct labeling is bounded by\n1\nM (1 + O(kλh−h1q))\nas needed.\n18\n8.2\nOn count reconstruction\nWe require the following preliminary result in order to bound the power of\nlocal algorithms.\nLemma 8.1. Consider the IIDM with dλ2 < 1 and assume that all the data\nis labeled and that is compressed as CA(D), where A = (A1) and A1 = [k],\ni.e, we are given the counts of the data. Let Px,h denote the distribution of\nCA(D) conditional on R(v0) = x. There there exists a distribution Q = Qh\nsuch that for all x, it holds and\nPx,h = (1 −η)Q + ηP ′\nx,h\nη ≤C exp(−ch),\n(3)\nwhere Q is independent of x and c, C are two positive constant which depend\non λ and k (but not on h).\nOur proof builds on the a special case of the result for k = 1, where [MP03]\nshow that the “count reconstruction problem is not solvable” which implies\nthe existence of η(h) which satisﬁes η(h) →0 as h →∞. The statement\nabove generalizes the result to all k. Moreover, we obtain an exponential\nbound on η in terms of h.\nProof. Assume ﬁrst that k = 1. The proof that the threshold for “count\nreconstruction is determined by the second eigenvalue” [MP03] implies the\nstatement of the lemma with a value η = η(h) which decays to 0 as h →∞.\nOur goal in the lemma above is to obtain a more explicit bound showing\nan exponential decay in h.\nSuch exponential decay follows from [JM04]\nfor a diﬀerent problem of robust reconstruction. Robust reconstruction is a\nvariation of the reconstruction problem, where the tree structure is known\nbut the value of each leaf is observed with probability δ > 0, independently\nfor each leaf. [JM04] proved that if dλ2 < 1 and if δ(d, λ) > 0 is small\nenough then the distributions Sx of the the partially observed leaves given\nR(v0) = x satisfy\nSx = (1 −η)S + ηS′\nX,\nη ≤C exp(−ch).\n(4)\nFrom the fact that the census reconstruction problem is not solvable [MP03],\nit follows that there exists a ﬁxed h′ = h′(δ) such that for the reconstruction\nproblem with h′ levels, the distribution of the counts at the leaves can be\ncoupled for all root values except with probability δ. We can now generate\nPx,h+h′ as follows: we ﬁrst generate Qx,h which is the representations at level\nh. Then each node at level h is marked as coupled with probability 1−δ and\n19\nuncoupled with probability δ independently. To generate the census Px,h+h′\nfrom Qx,h, as follows: for each coupled node at level h, we generate the cen-\nsus of the leaves below it at level h + h′ conditioned on the coupling being\nsuccessful (note that this census is independent of the representation of the\nnode at level h). For uncoupled nodes, we generate the census, conditioned\non the coupling being unsuccessful. From the description it is clear that\nPx,h+h′ can be generated from Qx,h. Since Qx,h has the representation (4),\nit now follows that Px,h+h′ has the desired representation (3) as needed.\nThe case of larger k is identical since the chain on k sequences has the\nsame value of λ. Therefore (3) follows from (4).\n8.3\nThe limited power of shallow algorithms\nWe now prove Theorem 4.6.\nProof. The idea of the proof is to utilize Lemma 8.1 to show that the com-\npressed labeled data is essentially independent of the unlabeled data. Write\nM = dh0. For each permutation σ of the M labels 1, . . . , M, we write Pσ for\nthe induced distribution on the compressed labeled data CA(D). Our goal\nis to show we can write\nPσ = (1 −η)P + ηP ′\nσ\n(5)\nwhere η is small. Note that (5) implies that the probability of labeling a\nunlabeled leaf accurately is at most M−1 + η. Indeed we may consider a\nproblem where in addition to the sample from Pσ we are also told if it is\ncoming from P or from P ′\nσ. If it is coming from P, we know it is generated\nindependently of the labels and therefore we cannot predict better than\nrandom (i.e. M−1).\nLet R(v1(1)), R(v2(1)), . . . , R(v1(M)), R(v2(M)) denote the representa-\ntions at the roots of the subtrees of the labeled data. Let I denote all of the\nrepresentations and let PI denote the distribution of CA(D) conditioned on\nthese representations. By convexity, to prove the desired coupling it suﬃces\nto prove\nPI = (1 −η)P + ηP ′\nI\nBy applying Lemma 8.1 to each of the trees rooted at v1(1), v2(1), . . . , v1(M), v2(M)\nand to each of the sets Ai, we obtain the desired results.\n20\nReferences\n[ABGM14] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma,\nProvable bounds for learning some deep representations, Inter-\nnational Conference on Machine Learning, 2014, pp. 584–592.\n[BM13]\nJoan Bruna and St´ephane Mallat, Invariant scattering convolu-\ntion networks, IEEE transactions on pattern analysis and ma-\nchine intelligence 35 (2013), no. 8, 1872–1886.\n[Cav78]\nJ. A. Cavender, Taxonomy with conﬁdence, Math. Biosci. 40\n(1978), no. 3-4.\n[CSS16]\nNadav Cohen, Or Sharir, and Amnon Shashua, On the expres-\nsive power of deep learning: A tensor analysis, 29th Annual Con-\nference on Learning Theory (Columbia University, New York,\nNew York, USA) (Vitaly Feldman, Alexander Rakhlin, and\nOhad Shamir, eds.), Proceedings of Machine Learning Research,\nvol. 49, PMLR, 23–26 Jun 2016, pp. 698–728.\n[Dar59]\nCharles Darwin, On the origin of species.\n[ES16]\nRonen Eldan and Ohad Shamir, The power of depth for feed-\nforward neural networks, Conference on Learning Theory, 2016,\npp. 907–940.\n[Far73]\nJ. S. Farris, A probability model for inferring evolutionary trees,\nSyst. Zool. 22 (1973), no. 4, 250–256.\n[Fel04]\nJ. Felsenstein, Inferring phylogenies, Sinauer, New York, New\nYork, 2004.\n[GBC16]\nIan Goodfellow, Yoshua Bengio, and Aaron Courville, Deep\nlearning, MIT Press, 2016.\n[JC69]\nT. H. Jukes and C. Cantor, Mammalian protein metabolism,\nEvolution of protein molecules (H. N. Munro, ed.), Academic\nPress, 1969, pp. 21–132.\n[JM04]\nS. Janson and E. Mossel, Robust reconstruction on trees is deter-\nmined by the second eigenvalue, Ann. Probab. 32 (2004), 2630–\n2649.\n21\n[KS66]\nH. Kesten and B. P. Stigum, Additional limit theorems for in-\ndecomposable multidimensional Galton-Watson processes, Ann.\nMath. Statist. 37 (1966), 1463–1481.\n[LBH15]\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton, Deep learn-\ning, Nature 521 (2015), no. 7553, 436–444.\n[MLP16]\nHrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio, Learn-\ning functions: when is deep better than shallow, arXiv preprint\narXiv:1603.00988 (2016).\n[Mos01]\nE. Mossel, Reconstruction on trees: beating the second eigen-\nvalue, Ann. Appl. Probab. 11 (2001), no. 1, 285–300.\n[Mos04]\nE. Mossel, Phase transitions in phylogeny, Trans. Amer. Math.\nSoc. 356 (2004), no. 6, 2379–2404 (electronic).\n[MP03]\nE. Mossel and Y. Peres, Information ﬂow on trees, Ann. Appl.\nProbab. 13 (2003), no. 3, 817–844.\n[Ney71]\nJ. Neyman, Molecular studies of evolution: a source of novel\nstatistical problems, Statistical desicion theory and related topics\n(S. S. Gupta and J. Yackel, eds.), 1971, pp. 1–27.\n[PNB15]\nAnkit B Patel, Tan Nguyen, and Richard G Baraniuk, A proba-\nbilistic theory of deep learning, arXiv preprint arXiv:1504.00641\n(2015).\n[RS15]\nS. Roch and A. Sly, Phase transition in the sample complexity\nof likelihood-based phylogeny inference, ArXiv e-prints (2015).\n[SS03]\nC. Semple and M. Steel, Phylogenetics, Mathematics and its\nApplications series, vol. 22, Oxford University Press, 2003.\n[Tel16]\nMatus Telgarsky, beneﬁts of depth in neural networks, Confer-\nence on Learning Theory, 2016, arXiv preprint arXiv:1602.04485,\npp. 1517–1539.\n22\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2016-12-29",
  "updated": "2018-09-04"
}