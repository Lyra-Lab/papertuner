{
  "id": "http://arxiv.org/abs/1701.03338v2",
  "title": "LanideNN: Multilingual Language Identification on Character Window",
  "authors": [
    "Tom Kocmi",
    "Ondřej Bojar"
  ],
  "abstract": "In language identification, a common first step in natural language\nprocessing, we want to automatically determine the language of some input text.\nMonolingual language identification assumes that the given document is written\nin one language. In multilingual language identification, the document is\nusually in two or three languages and we just want their names. We aim one step\nfurther and propose a method for textual language identification where\nlanguages can change arbitrarily and the goal is to identify the spans of each\nof the languages. Our method is based on Bidirectional Recurrent Neural\nNetworks and it performs well in monolingual and multilingual language\nidentification tasks on six datasets covering 131 languages. The method keeps\nthe accuracy also for short documents and across domains, so it is ideal for\noff-the-shelf use without preparation of training data.",
  "text": "LanideNN: Multilingual Language Identiﬁcation on Character Window\nTom Kocmi and Ondˇrej Bojar\nCharles University, Faculty of Mathematics and Physics\nInstitute of Formal and Applied Linguistics\n{kocmi,bojar}@ufal.mff.cuni.cz\nAbstract\nIn language identiﬁcation, a common ﬁrst\nstep in natural language processing, we\nwant to automatically determine the lan-\nguage of some input text.\nMonolingual\nlanguage identiﬁcation assumes that the\ngiven document is written in one language.\nIn multilingual language identiﬁcation, the\ndocument is usually in two or three lan-\nguages and we just want their names. We\naim one step further and propose a method\nfor textual language identiﬁcation where\nlanguages can change arbitrarily and the\ngoal is to identify the spans of each of the\nlanguages.\nOur method is based on Bidirectional Re-\ncurrent Neural Networks and it performs\nwell in monolingual and multilingual lan-\nguage identiﬁcation tasks on six datasets\ncovering 131 languages.\nThe method\nkeeps the accuracy also for short docu-\nments and across domains, so it is ideal\nfor off-the-shelf use without preparation of\ntraining data.\n1\nIntroduction\nThe World Wide Web is an ever growing source\nof textual data, especially data generated by web\nusers.\nAs more people get access to the web,\nmore languages and dialects start to appear and\nneed to be processed. In order to be able to use\nsuch data for further natural language processing\n(NLP) tasks, we need to know in which languages\nthey were written. Language identiﬁcation is thus\na key component for both building various NLP\nresources from the web and also for running many\nweb services.\nTechniques of language identiﬁcation can rely\non handcrafted rules, usually of high precision but\nlow coverage, or data-driven methods that learn to\nidentify languages based on sample texts of sufﬁ-\ncient quantity.\nIn this paper, we present a data-driven method\nfor language identiﬁcation based on bidirectional\nrecurrent neural networks called LanideNN (lan-\nguage identiﬁcation by neural networks, NN). The\nmodel is trained on character sliding window of\ninput texts with the goal of assigning a language\nto each character.\nWe show that the method is\napplicable for a large number of languages and\nacross text domains without any adaptation and\nthat it performs well in monolingual (one lan-\nguage per document) as well as multilingual (a few\nlanguages per document) language identiﬁcation\ntasks. Also, the performance does not drop with\nshorter texts.\nThe paper is structured as follows. In Section 2,\nwe brieﬂy review current approaches to language\nidentiﬁcation. Section 3 introduces our method,\nincluding the technical details of the neural net-\nwork architecture. For the training of our model,\nwe collect and manually clean a new dataset, as\ndescribed in Section 4. The model is evaluated on\nstandard test sets for monolingual (Section 5) as\nwell as multilingual (Section 6) language identi-\nﬁcation. Section 7 illustrates the behavior of our\nmethod in the motivating setting: identifying lan-\nguages in short texts. We conclude and summarize\nour plans in Section 8.\n2\nRelated Work\nOf the many possible approaches to language\nidentiﬁcation Hughes et al. (2006), character n-\ngram statistics are among the most popular ones.\nCavnar et al. (1994) were probably the ﬁrst; they\nused the 300 most frequent character n-grams\n(with n ranging from 1 to 5, as is also typically\nused in other works). All the n-gram-based ap-\narXiv:1701.03338v2  [cs.CL]  29 Jul 2017\nproaches differ primarily in the calculation of the\ndistance between the n-gram proﬁle of the train-\ning and test text (Selamat, 2011; Yang and Liang,\n2010), or by using additional features on top of the\nn-gram proﬁles (Padma et al., 2009; Carter et al.,\n2013). One of the fairly robust deﬁnitions of the\ndistance (or similarity) was proposed by Choong\net al. (2009) who simply check the proportion of\nn-gram types seen in the tested document of the\nmost frequent n-gram types extracted from train-\ning documents for each language. The highest-\nscoring language is then returned.\nHughes et al. (2006) mention a number of freely\navailable tools at that time. Since then, one aspect\nof the tools became also important: the number of\nlanguages covered.\nThe language identiﬁcation tool CLD21 by\nGoogle detects 80 languages and uses a Naive\nBayes classiﬁer, treating speciﬁcally unambiguous\nscripts such as Greek and using either character\nunigrams (Han and similar scripts) or fourgrams.\nAnother popular tool is Langid.py by Lui and\nBaldwin (2012), covering 97 languages out of the\nbox. Langid.py relies on Naive Bayes classiﬁer\nwith a multinominal event model and mixture of\nbyte n-grams for training. The tool includes to-\nkenization and fast feature extraction using Aho-\nCorasick string matching.\nTo our knowledge, and also according to the\nsurvey by Garg et al. (2014), neural networks have\nnot been used often for language identiﬁcation so\nfar. One exception is Al-Dubaee et al. (2010), who\ncombine a feed-forward network classiﬁer with\nwavelet transforms of feature vectors to identify\nEnglish and Arabic from the Unicode represen-\ntation of words, sentences or whole documents.\nThe beneﬁt of NN in this setting is not very clear\nto us because English and Arabic can be distin-\nguished by the script. During writing of this pa-\nper, we have found a new pre-print paper (Jaech\net al., 2016) which handles language identiﬁca-\ntion with NN. Speciﬁcally, they employ Convo-\nlutional Neural Networks followed by Recurrent\nNeural Networks. Their approach labels text on\nthe word level, which is problematic in languages\nwithout clear word delimiters. In comparison with\nour model, they need to pre-process the data and\nbreak long words into smaller chunks, whereas we\nsimply use text without any preprocessing.\nIn practice, several tools are often used at once,\n1https://github.com/CLD2Owners/cld2\nwith some form of majority voting. For example,\nTwitter internal language detector uses their in-\nhouse tool along with CLD2 and Langid.py, and\nthis triple agreement is reported to make less than\n1 % of errors.2\nMultilingual language identiﬁcation, i.e. iden-\ntiﬁcation of the set of languages used in a docu-\nment, is a less common task, explored e.g. by Lui\net al. (2014) who use a generative mixture model\non multilingual documents and establish the rel-\native proportion of languages used. Character n-\ngrams again serve as features, selected by infor-\nmation gain.\nSolorio et al. (2014) organized a shared task\nin language identiﬁcation at the word level. This\nmatches our aim, but the task included only four\nlanguage pairs and more importantly, the dataset\nwas collected from Twitter and for copyright rea-\nsons it is not available any more.\n3\nProposed Method\nThe method we propose is designed for short text\nwithout relying on document boundaries. Obvi-\nously, if documents are known and if they can be\nassumed to be monolingual, this additional knowl-\nedge should not be neglected. For the long term,\nwe however aim at a streamlined processing of\nnoisy data genuinely appearing in multilingual en-\nvironments. For instance, our method could sup-\nport the study of switching of languages (“code\nswitching”) in e-mails or other forms of conver-\nsation, or to analyse various online media such as\nTwitter, see e.g. Montes-Alcal´a (2007) or Solorio\net al. (2014).\nOur model takes source letters as input and pro-\nvides a language label for each of them. Whenever\nwe need to recognize the language of a document,\nwe take the language assigned by our model to the\nmajority of letters.\nThe goal of attributing a language tag to the\nsmallest text units is one of the reasons why we\ndecided to use neural networks and designed the\nmodel to provide a prediction at every time step\nwithout much overhead.\nIn the rest of this section, we explain the archi-\ntecture and training methods of the model.\nFigure 1: Illustration of our model LanideNN.\n3.1\nBidirectional Recurrent Neural Networks\nA recurrent neural network RNN (Elman, 1990) is\na variant of neural networks with recurrent con-\nnections in time. In principle, the history infor-\nmation available to an RNN is not limited (subject\nto a processing window, if used), so the network\ncan condition its output on features from a long\ndistance. The LSTM, one of the variants of RNN,\nmakes it particularly suitable for sequential pre-\ndiction tasks with arbitrary time dependencies, as\nshown by Hochreiter and Schmidhuber (1997).\nIn this work, we use the Elman-type network,\nwhere the hidden layer ht at a time step t is com-\nputed based on the current input layer xt and the\nprevious state of the hidden layer ht−1. The out-\nput yt is then derived from the ht by applying the\nsoftmax function f. More formally:\nht = tanh (Wxt + V ht−1 + b1)\n(1)\nyt = f (Uht + b2)\n(2)\nwhere U, V and W are connection weights to be\ncomputed in training time and bias vectors b1 and\nb2.\nWith the above deﬁnition, the RNN has access\nonly to information preceding the current position\n2https://blog.twitter.com/2015/\nevaluating-language-identification-\nperformance\nin the text. In our setting, the rest of the text (in a\nﬁxed-size window) is available, so we want to al-\nlow the model to use also future information, i.e.\nletters following the currently examined one. We\ntherefore deﬁne a second RNN which reads the in-\nput from the end to the beginning, changing the\ndeﬁnition to:\n−→h t = tanh\n\u0010−→\nWxt + −→\nV −→h t−1 + −→b 1\n\u0011\n(3)\n←−h t = tanh\n\u0010←−\nWxt + ←−\nV ←−h t+1 + ←−b 1\n\u0011\n(4)\nyt = f\n\u0010−→\nU −→h t + ←−\nU ←−h t + b2\n\u0011\n(5)\nwhere the left and right arrows indicate the direc-\ntion of network.\nThe simple unit with only tanh non-linearity is\ndifﬁcult to train and therefore we have selected\nthe Gated Recurrent Unit (GRU), recently pro-\nposed by Cho et al. (2014), as a replacement. We\nalso considered Long Short-Term Memory cells\n(LSTM) but they achieved slightly worse results\nin our setting. This changes equations (1), (3) and\n(4).\nThe proper equations for the GRU can be\nfound in Cho et al. (2014).\nThe model outputs a probability distribution\nover all language tags. In order to determine the\nlanguage of a character, we take the tag with the\nmaximum value.\nThe complete model is sketched in Figure 1.\n3.2\nTraining, Embeddings and Dropout\nWe train the model using the ﬁrst-order stochastic\ngradient descent method Adam (Kingma and Ba,\n2015). Our training criterion is the cross-entropy\nloss function3.\nWe represent each Unicode character using an\ne-dimensional real valued vector, analogously to\nword embeddings of Collobert et al. (2011). The\ncharacter embeddings are initialized randomly and\nare trained together with the rest of the network.\nTo prevent overﬁtting, we use dropout (Srivas-\ntava et al., 2014) during model training on the\ncharacter embedding layer4. The key idea is to\nrandomly drop (avoid updating of) connections.\nThis prevents neurons from co-adapting too much,\ni.e. starting to depend on outputs of other neurons\ntoo much, which is a typical symptom of overﬁt-\nting to training data.\n3.3\nModel Design\nOur model operates on a window of 200 charac-\nters of input text, i.e. individual letters, encoded in\nUnicode. Each character corresponds to one time\nstep of the BiRNN in the respective direction, see\nFigure 1. The model classiﬁes each character sep-\narately, but quickly learns to classify neighbouring\ncharacters with the same label.\nFor documents longer than the window size, we\nsimply move to the next window without any over-\nlap. The last window (or the only window if the\ndocument were too short) is ﬁlled with a padding\ncharacter, so the network always works on win-\ndows of the same size.\nWe set e, the size of the embedding layer, to\n200. The BiRNN uses a single hidden layer of 500\nGRU cells for each direction.\nThe main model was trained for over 530,000\nsteps (each step is the processing of one batch of\ninputs) on a single core of the GeForce GTX Ti-\ntan Z GPU. The training took around 5 days. The\nstopping criterion for the training was the error on\na development set.\n4\nTraining Data\nOur goal is to develop an off-the-shelf language\nrecognizer, with no need for retraining by the user\nand covering as many languages as possible. Find-\ning suitable training data is thus an important part\n3We set the learning rate to 0.0001 and train with the batch\nsize of 64 windows.\n4We set the dropout to the probability of 0.5 as customary.\nafr, amh, ara, arg, asm, ast, aze, bak, bcl, bel, ben, ber,\nbpy, bre, bul, cat, ceb, ces, che, chv, cos, cym, dan, deu,\ndiv, ekk, ell, eng, est, eus, fas, ﬁn, fra, fry, gla, gle, glg,\ngom, gsw, guj, hat, heb, hif, hin, hrv, hsb, hun, hye, ido,\nilo, ina, ind, isl, ita, jav, jpn, kal, kan, kas, kat, kaz, kir,\nkor, kur, lat, lav, lim, lit, ltz, lug, lus, mal, mar, min, mkd,\nmlg, mlt, mon, mri, msa, nds, nep, new, nld, nno, nor, nso,\noci, ori, oss, pam, pan, pms, pnb, pol, por, pus, roh, ron,\nrus, sah, scn, sin, slk, slv, sna, som, spa, sqi, srp, sun, swa,\nswe, tam, tat, tel, tgk, tgl, tha, tur, uig, ukr, urd, uzb, vec,\nvie, vol, wln, yid, zho, zul\nFigure 2: The 131 languages (and HTML) recog-\nnized by our system.\nof the endeavour.\nWe start from Wikipedia, as crawled and con-\nverted to a large multilingual corpus W2C by Ma-\njliˇs and ˇZabokrtsk´y (2012).\nW2C contains 106\nlanguages but we had to exclude a few of them5\nbecause they contained too little non-repeating\ntext.\nWe then focussed on ﬁnding corpora with at\nleast some languages not covered by the already\ncollected data. Those corpora were added whole,\nincluding languages that we already had, to im-\nprove the variety of our collection. We made use\nof the following ones:\nTatoeba6 is a collection of simple sentences for\nlanguage learners.\nTatoeba contains sen-\ntences in 307 languages, but for most lan-\nguages it has only a few hundred sentences.\nLeipzig corpora collection (Quasthoff\net\nal.,\n2006) covers 220 languages with newspaper\ntext and other various texts collected from\nthe web.\nEMILLE (Baker et al., 2002) contains texts in 14\nIndian languages and English.\nHaitian Creole training data (Callison-Burch et\nal., 2011) were prepared by the organizers\nof WMT11 shared task on machine transla-\ntion of SMS messages sent to an emergency\nhotline in the aftermath of the 2010 Haitian\nearthquake. We used only the ofﬁcial docu-\nments from the training data, not the actual\nSMS messages because they contained a lot\nof noise.\n5Speciﬁcally, HAT, IDO, MGL, MRI, VOL, as identiﬁed\nby ISO language codes.\n6http://tatoeba.org/\nTest Set\nDocuments\nLanguages\nEncoding\nDocument Length (bytes)\nAvg. # characters\nEuroGov\n1500\n10\n1\n17460.5 ± 39353.4\n17037.3\nTCL\n3174\n60\n12\n2623.2 ± 3751.9\n1686.1\nWikipedia\n4963\n67\n1\n1480.8 ± 4063.9\n1314.2\nTable 1: Summary of testsets for monolingual language identiﬁcation.\nAdditionally, we wanted our tool to distinguish\nHTML tags in the data, since they are the most\nfrequent markup that needs to be separated from\nthe processed data.\nTherefore, we have down-\nloaded several Github projects in HTML and col-\nlected all strings enclosed with angle brackets, as\na rather permissive approximation of HTML tags.\nWe have dropped tags which were too long and\nwe put each tag on a separate line. We have not\ndeduplicated them for the training set.\nThe cleanup of the collected data was mostly\nmanual. We deduplicated each of the sources by\ndropping identical lines, regardless of what lines\ncorrespond to in the individual sources (words,\nphrases, sentences or even paragraphs). We in-\nspected data ﬁles for individual languages and re-\nmoved lines containing English for languages not\nusing Latin script. We also removed Cyrilic char-\nacters from a few languages that should not con-\ntain them. This was done mostly in W2C corpora.\nFor the ﬁnal dataset, we mixed all sources for\na given language at the line level, keeping only\nlanguages with more than 500k characters in to-\ntal. Since the resources for some languages were\nhuge, we decided to set an upper bound on the\nnumber of characters per language. In order to\nroughly reﬂect the distribution of languages in\nthe world, we divided languages into three groups\nbased on the number speakers of the language ac-\ncording to Wikipedia. The ﬁrst group were lan-\nguages with more than 75M speakers, the second\nwith more than 10M speakers and the third group\ncontained the rest. For the ﬁrst group, we allowed\nat most 10M characters in the training set, the sec-\nond group was capped at 5M characters and the\nthird group was allowed only 1M characters per\nlanguage at most.7\nIn total, our ﬁnal training set includes 131 + 1\n(HTML) languages, see Figure 2.\nWe divide the corpus into non-overlapping\ntraining, development and test sections. We re-\n7Higher-quality sources such as Tatoeba are generally\nsmaller and since we mixed the sources by interleaving their\nlines, these smaller sources were likely included in full.\nleased the test set 8 but the training part cannot\nbe publicly released because of the restrictive per-\nmissions of some of the sources used. The test\nsection is limited to short text.\nIt contains 100\nlines for each of the 131 languages (HTML is not\nincluded), with the average line length of 142.3\ncharacters.\nEach line of the dataset starts with an ISO-3 la-\nbel of the language presented on that line. All lines\nwere shufﬂed.\nFor training and testing, the language labels as\nwell as all line breaks must be ignored, otherwise\nthe model could learn to set language boundaries\nat the new line character. After dropping all line\nbreaks, we obtain a multilingual text.\nThis way, we simulate a multilingual text and\nour algorithm has to learn to identify language\nboundaries without relying on any particular sym-\nbol. We are aware of the fact that the original seg-\nmentation of the corpora affects where these lan-\nguage switches are expected, and this will mostly\ncorrespond to sentence boundaries.\n5\nMonolingual Language Identiﬁcation\nMost of related research is focused on monolin-\ngual language identiﬁcation, i.e. recognizing the\nsingle language of an input document.\nWe compare our method in this setting with sev-\neral other algorithms on the dataset presented by\nBaldwin and Lui (2010). The dataset consists of 3\ndifferent test sets, each containing a different num-\nber of languages, styles and document lengths col-\nlected from different sources, see Table 1 for de-\ntails:\nEuroGov contains texts in Western European\nlanguages from European government re-\nsources.\nTCL was extracted by the Thai Computational\nLinguistics Laboratory in 2005 from online\nnews sources and the test set also contains\nmultiple ﬁle encodings. Since our method as-\nsumes Unicode input, we converted TCL to\nUnicode encoding.\n8https://ufal.mff.cuni.cz/tom-kocmi/lanidenn\nSystem\nTrained on\nSupported languages\nEuroGov\nTCL\nWikipedia\nLangDetect*\nWikipedia\n53\n.9929\n.818\n.867\nTextCat*\nTextCat Dataset\n75\n.941\n.605\n.706\nCLD*\nunknown\n64\n.983\n.732\n.831\nLangid.py*\nLui and Baldwin (2011)\n97\n.987\n.904\n.913\nLangid.py\nLui and Baldwin (2011)\n97\n.987\n.931\n.913\nCLD2\nunknown\n83\n.979\n.837\n.854\nOur model\nOur dataset\n136\n.977\n.954\n.893\nTable 2: Results of monolingual language identiﬁcation on the Baldwin and Lui (2010) test set. Entries\nmarked with “*” are accuracies reported by Lui and Baldwin (2012), the rest are our measurements.\nWikipedia are texts collected from a Wikipedia\ndump.\nTable 2 summarizes the accuracies of several\nalgorithms on the three test sets. For some algo-\nrithms, we report values as presented by Lui and\nBaldwin (2012) without re-running9. We re-ran\nthe Langid.py as the best algorithm out of them,\nand got the same results except for the TCL test-\nset, where we got better results than reported by\nLui and Baldwin (2012). After a discussion with\nthe authors, we believe the re-run beneﬁted from\nthe conversion of all texts to Unicode.\nWe compare our method with two top language\nrecognizers, Langid.py and CLD2. Our model is\ntrained on more languages and we do not restrict\nit to the languages included in the test set, so we\nmay be losing on detailed dialect labels.\nDespite the considerably higher number of\nlanguages covered, our model performs reason-\nably close to the competitors on EuroGov and\nWikipedia and best on TCL.\n5.1\nShort-Text Language Identiﬁcation\nIn order to demonstrate the ability of our method\nto identify language of very short texts such\nas tweets, search queries or user messages, we\nwanted to use an existing corpus, such as the one\nreleased by Twitter.10 Unfortunately, the corpus\ncontains only references to the actual tweets and\nmost of them are no longer available. We thus\nhave to rely on our own test set, as described in\nSection 4.\nResults on short texts are reported in Table 3.\nThe two other systems, Langid.py and CLD2\ncover fewer languages and they were trained on\ntexts unrelated to our collection of data. It is there-\n9We should mention that LangDetect used EuroGov as a\nvalidation set, so its score on this test set is not reliable.\n10https://blog.twitter.com/2015/\nevaluating-language-identification-\nperformance\nSystem\nAll languages\nCommon languages\nLangid.py\n.567\n.912\nCLD2\n.545\n.891\nOur model\n.950\n.955\nTable 3: Results on our test set for short texts.\nThe ﬁrst column shows accuracy over all 131 lan-\nguages and the second column shows accuracy\nover languages that all systems have in common.\nfore not surprising that they perform much worse\nwhen averaged over all languages.\nFor a fairer comparison, we report also accura-\ncies on a restricted version of the test set that in-\ncluded only languages supported by all the three\ntested tools. Both our competitors are meant to\nbe generally applicable, so they should (and do)\nperform quite well. Our system nevertheless out-\nperforms them, reaching the accuracy of 95.5. Ar-\nguably, we can be beneﬁtting from having trained\non (different) texts from the same sources as this\ntest set.\nTable 6 lists the most frequent misclassiﬁcations\nof our model on our test set (unordered language\npairs) of the 13100 items in the test set. The most\ncommon error is confusing Indonesian with Mod-\nern Standard Arabic, which indicates some noise\nin our training data rather than difﬁculty of sep-\narating these two languages. The following pairs\nare expected: Standard Estonian (ekk) vs. Esto-\nnian (est, a macro language which includes Stan-\ndard Estonian), Bashkir vs.\nTatar, Croatian vs.\nSerbian, Asturian vs. Spanish, ...\nFinally, our model is trained to distinguish also\nHTML as one additional language. We did not in-\nclude HTML in our test corpus but to satisfy the\nrequests of one of our reviewers, we checked the\nperformance on our development corpus: only one\nPortuguese and one Yakut segment was classiﬁed\nas HTML and none of the 100 HTML segments\nwere misclassiﬁed.\nSystem\nTraining set\nPM\nRM\nFM\nPµ\nRµ\nFµ\nVRL (2010) *\nALTW2010\n.497\n.467\n.464\n.833\n.826\n.829\nALTW2010 winner *\nALTW2010\n.718\n.703\n.699\n.932\n.931\n.932\nSEGLANG *\nALTW2010 - mono\n.801\n.810\n.784\n.866\n.946\n.905\nLINGUINI *\nALTW2010 - mono\n.616\n.535\n.513\n.713\n.688\n.700\nLui et al. (2014) *\nALTW2010 - mono\n.753\n.771\n.748\n.945\n.922\n.933\nLui et al. (2014) our retrain\nALTW2010 - mono\n.768\n.716\n.724\n.968\n.896\n.931\nOur model\nALTW2010 - mono\n.819\n.764\n.779\n.966\n.964\n.965\nOur model\nOur dataset\n.709\n.714\n.695\n.941\n.941\n.941\nTable 4: Results of multilingual language identiﬁcation on the ALTW2010 test set. * As reported by Lui\net al. (2014)\nSystem\nPM\nRM\nFM\nPµ\nRµ\nFµ\nSEGLANG *\n.809\n.975\n.875\n.771\n.975\n.861\nLINGUINI *\n.853\n.772\n.802\n.838\n.774\n.805\nLui et al. (2014) *\n.962\n.954\n.957\n.963\n.955\n.959\nLui et al. (2014) our retrain\n.962\n.963\n.961\n.963\n.964\n.963\nOur model trained on WikipediaMulti\n.962\n.974\n.966\n.954\n.974\n.964\nOur model trained on our dataset\n.774\n.778\n.774\n.949\n.972\n.961\nOur model trained on our dataset, restricted\n.966\n.973\n.966\n.956\n.973\n.964\nTable 5: Results of multilingual language identiﬁcation on the WikipediaMulti test set. * As reported by\nLui et al. (2014)\nind↔msa\n64\nekk↔est\n36\nbak↔tat\n28\nhrv↔srp\n17\nglg↔por\n17\nnno↔nor\n16\nast↔spa\n15\nfas↔pus\n13\nces↔slk\n13\nhrv↔slv\n10\ndan↔nor\n10\nnep↔new\n8\naze↔tur\n7\nmar↔new\n6\nceb↔tgl\n6\ncat↔spa\n6\narg↔spa\n6\nfra↔oci\n5\nTable 6: Most frequent confusions on our test set.\n6\nMultilingual Language Identiﬁcation\nIn multilingual language identiﬁcation, systems\nare expected to report the set of languages used in\neach input document. The evaluation criterion is\nthus macro- (M) or micro- (µ) averaged precision\n(P), recall (R) or F-measure (F).11\nWe evaluate our model on two existing test sets\nfor multilingual identiﬁcation, ALTW2010 shared\ntask and WikipediaMulti.\nWe are mainly inter-\nested in the performance of our general model,\ntrained on all our training data, on these test sets.\nBut since both test sets come with training data,\nwe also retrain our model to test its in-domain per-\nformance. We limit the training of these speciﬁc\nmodels to 140,000 training steps for ALTW2010\nand 75,000 steps for WikiMulti, keeping other set-\ntings identical to the main model. Each training\nstep amounts to the processing of 64 batches of\n200 letters of input. The number of steps for both\n11Note that for comparability with results reported in other\nworks, macro-averaged F-score is calculated as average over\nindividual F-scores instead of the harmonic mean of PM and\nRM. FM can thus fall out of the range between PM and RM.\ntasks was established by testing the error on the\ndevelopment parts of the datasets.\nTo interpret the character-level predictions by\nour model for multilingual identiﬁcation, we used\nthe ALTW2010 development data to empirically\nset the threshold: if a language is predicted for\nmore than 3 % of characters in the document, it\nis considered as one of the languages of the docu-\nment.\n6.1\nALTW 2010 Shared Task\nALTW 2010 shared task (VRL, 2010) provided\n10000 bilingual documents divided as follows:\n8000 training, 1000 development and 1000 test\ndocuments.\nThe results on the 1000 test documents are in\nTable 4.\nFor algorithms SEGLANG and LIN-\nGUINI, we only reproduce the results reported by\nLui et al. (2014). We use the system by Lui et al.\n(2014) as a proxy for the comparison: we retrain\ntheir system and obtain results similar to those re-\nported by the original authors. The differences are\nprobably due to the Gibbs sampling used in their\napproach.\nSome of the reported methods rely on the fact\nthat the documents in the dataset are bilingual.\nOther methods, including ours, simply break the\nbilingual documents into the individual languages\nand train on this simpliﬁed training set. We indi-\ncate this by stating “ALTW2010 - mono” in Ta-\nble 4.\nFigure 3: Illustration of text partitioning. The black triangles indicate true boundaries of languages. The\nblack part shows probability with which the language written in gray is detected and the gray part shows\ncomplement for the second language, since in this setup we restricted our model to use only the two\nlanguages in question. The misclassiﬁcation of Italian and German as English in the last two examples\nmay reﬂect increased noise in our English training data.\nThe main criterion of the ALTW2010 shared\ntask was to maximize the micro-averaged F-score\n(Fµ).\nWe see that our model trained on the\nALTW2010 data outperforms all other models in\nthis criterion (Fµ of .965) and so does our non-\nadapted version, reaching Fµ of .941.\n6.2\nWikipediaMulti\nWikipediaMulti (Lui et al., 2014) is a dataset of ar-\ntiﬁcially prepared multilingual documents, mixed\nfrom monolingual Wikipedia articles from 44 lan-\nguages. Each of the artiﬁcial documents contains\ntexts in 1 ≤k ≤5 randomly selected languages.\nThe average document length is 5500 bytes. The\ntraining set consists of 5000 monolingual docu-\nments, the development set consists of 5000 mul-\ntilingual documents and test set consists of 1000\ndocuments for each value of k.\nTable 5 shows that our model performs well,\nboth when trained on the provided data and when\ntrained on our training corpus. The model trained\non our dataset performs slightly worse in Fµ, but\nif we simply prevent it from predicting languages\nnot present in the test set, the score gets on par\nwith the adapted version, see the line labelled “re-\nstricted” in Table 5.\n7\nText Partitioning\nFigure 3 illustrates the behaviour of our model\non text with mixed languages. We have selected\nvery short (50–130 characters) and challenging\nsegments where the languages mostly share the\nsame script. Finding the boundary between lan-\nguages written in different scripts is quite easy, as\nillustrated by the ﬁrst example.\nOnly too late, we discovered that King and Ab-\nney (2013) provide a test set for word-level iden-\ntiﬁcation for 30 languages. We thus have to leave\nthe evaluation of our model on this dataset for fu-\nture.\n8\nConclusion\nWe have developed a language identiﬁcation algo-\nrithm based on bidirectional recurrent neural net-\nworks. The approach is designed for identifying\nlanguages on a short texts, allowing to detect code\nswitching including switches to formal markup\nlanguages like HTML.\nWe collected a dataset and trained our model\nto recognize considerably more languages than\nother state-of-the-art tools. Our algorithm and the\ntrained model is provided for academic and per-\nsonal use.12\nSince there is no established dataset for the\nnovel setting of text partitioning by language,\nwe evaluated our model in several common tasks\n(monolingual and multilingual language identiﬁ-\ncation for long and short texts) which were pre-\nviously handled by separate algorithms. Our ap-\nproach performs well, improving over the state of\nthe art in several cases.\nA number of things are planned: (1) improv-\ning the implementation, especially the speed of\napplication of a trained model, (2) further extend-\ning the set of covered languages and possibly in-\ncluding more artiﬁcial or programming languages\n(e.g. JavaScript, PHP) or common formal nota-\ntions (URLs, hashtags), (3) evaluating our method\non the dataset by King and Abney (2013), possibly\nextending this dataset to include more languages,\n(4) training and testing the model on noisy texts\nlike Tweets or forum posts, and (5) experimenting\nwith other network architectures and approaches,\npossibly also training the model on bytes instead\nof Unicode characters.\n9\nAcknowledgement\nThis work has received funding from the European\nUnion’s Horizon 2020 research and innovation\nprogramme under grant agreement no.\n645452\n(QT21), the grant GAUK 8502/2016, and SVV\nproject number 260 333.\nThis work has been using language resources\ndeveloped, stored and distributed by the LINDAT/\nCLARIN project of the Ministry of Education,\nYouth and Sports of the Czech Republic (project\nLM2015071)\nReferences\nShawki A Al-Dubaee, Nesar Ahmad, Jan Martinovic,\nand Vaclav Snasel. 2010. Language identiﬁcation\nusing wavelet transform and artiﬁcial neural net-\nwork. In Computational Aspects of Social Networks\n(CASoN), 2010 International Conference on, pages\n515–520. IEEE.\n12https://github.com/tomkocmi/LanideNN\nPaul Baker, Andrew Hardie, Tony McEnery, Hamish\nCunningham, and Robert J Gaizauskas.\n2002.\nEmille, a 67-million word corpus of indic languages:\nData collection, mark-up and harmonisation.\nIn\nLREC.\nTimothy Baldwin and Marco Lui.\n2010.\nLanguage\nidentiﬁcation: The long and the short of the mat-\nter. In Human Language Technologies: The 2010\nAnnual Conference of the North American Chap-\nter of the Association for Computational Linguistics,\npages 229–237, Los Angeles, California, June. As-\nsociation for Computational Linguistics.\nChris Callison-Burch, Philipp Koehn, Christof Monz,\nand Omar Zaidan.\n2011.\nFindings of the 2011\nworkshop on statistical machine translation. In Pro-\nceedings of the Sixth Workshop on Statistical Ma-\nchine Translation, pages 22–64, Edinburgh, Scot-\nland, July. Association for Computational Linguis-\ntics.\nSimon\nCarter,\nWouter\nWeerkamp,\nand\nManos\nTsagkias. 2013. Microblog language identiﬁcation:\novercoming the limitations of short, unedited and id-\niomatic text. Language Resources and Evaluation,\n47(1):195–215.\nWilliam B Cavnar, John M Trenkle, et al.\n1994.\nN-gram-based text categorization. Ann Arbor MI,\n48113(2):161–175.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio.\n2014.\nLearning\nphrase representations using rnn encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1724–\n1734, Doha, Qatar, October. Association for Com-\nputational Linguistics.\nChew Choong, Yoshiki Mikami, Chandrajith Ashu-\nboda Marasinghe, and ST Nandasara. 2009. Op-\ntimizing n-gram order of an n-gram based lan-\nguage identiﬁcation algorithm for 68 written lan-\nguages. International Journal on Advances in ICT\nfor Emerging Regions (ICTer), 2(2).\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011.\nNatural language processing (almost) from\nscratch.\nJ. Mach. Learn. Res., 12:2493–2537,\nNovember.\nJeffrey L Elman.\n1990.\nFinding structure in time.\nCognitive science, 14(2):179–211.\nArchana Garg, Vishal Gupta, and Manish Jindal. 2014.\nA survey of language identiﬁcation techniques and\napplications. Journal of Emerging Technologies in\nWeb Intelligence, 6(4):388–400.\nSepp Hochreiter and J¨urgen Schmidhuber.\n1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nBaden Hughes, Timothy Baldwin, Steven Bird, Jeremy\nNicholson, and Andrew Mackinlay. 2006. Recon-\nsidering language identiﬁcation for written language\nresources.\nIn Proceedings of LREC2006, pages\n485–488.\nAaron Jaech, George Mulcaire, Shobhit Hathi, Mari\nOstendorf, and Noah A. Smith. 2016. Hierarchi-\ncal character-word models for language identiﬁca-\ntion.\nIn Proceedings of The Fourth International\nWorkshop on Natural Language Processing for So-\ncial Media, pages 84–93, Austin, TX, USA, Novem-\nber. Association for Computational Linguistics.\nBen King and Steven Abney. 2013. Labeling the lan-\nguages of words in mixed-language documents us-\ning weakly supervised methods. In Proceedings of\nthe 2013 Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics: Human Language Technologies, pages 1110–\n1119, Atlanta, Georgia, June. Association for Com-\nputational Linguistics.\nDiederik Kingma and Jimmy Ba.\n2015.\nAdam: A\nmethod for stochastic optimization.\n3rd Interna-\ntional Conference for Learning Representations.\nMarco Lui and Timothy Baldwin. 2011. Cross-domain\nfeature selection for language identiﬁcation. In Pro-\nceedings of 5th International Joint Conference on\nNatural Language Processing, pages 553–561, Chi-\nang Mai, Thailand, November. Asian Federation of\nNatural Language Processing.\nMarco Lui and Timothy Baldwin. 2012. langid.py: An\noff-the-shelf language identiﬁcation tool.\nIn Pro-\nceedings of the ACL 2012 System Demonstrations,\npages 25–30, Jeju Island, Korea, July. Association\nfor Computational Linguistics.\nMarco Lui, Jey Han Lau, and Timothy Baldwin. 2014.\nAutomatic detection and language identiﬁcation of\nmultilingual documents. Transactions of the Asso-\nciation for Computational Linguistics, 2:27–40.\nMartin Majliˇs and Zdenˇek ˇZabokrtsk´y.\n2012.\nLan-\nguage richness of the web. In LREC, pp. 2927-2934.\nCecilia Montes-Alcal´a. 2007. Blogging in Two Lan-\nguages: Code-Switching in Bilingual Blogs.\nIn\nJonathan Holmquist, Augusto Lorenzino, and Lotﬁ\nSayahi, editors, Selected Proceedings of the Third\nWorkshop on Spanish Sociolinguistics, pages 162–\n170. Cascadilla Proceedings Project, Somerville,\nMA, USA.\nM. C. Padma, P. A. Vijaya, and P. Nagabhushan. 2009.\nLanguage identiﬁcation from an indian multilingual\ndocument using proﬁle features. In 2009 Interna-\ntional Conference on Computer and Automation En-\ngineering, pages 332–335, March.\nUwe Quasthoff, Matthias Richter, and Christian Bie-\nmann. 2006. Biemann c., corpus portal for search in\nmonolingual corpora. In Proceedings of the 5th In-\nternational Conference on Language Resources and\nEvaluation.\nAli Selamat, 2011.\nImproved N-grams Approach\nfor Web Page Language Identiﬁcation, pages 1–26.\nSpringer Berlin Heidelberg, Berlin, Heidelberg.\nThamar\nSolorio,\nElizabeth\nBlair,\nSuraj\nMahar-\njan,\nSteven\nBethard,\nMona\nDiab,\nMahmoud\nGhoneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-\nlia Hirschberg, Alison Chang, and Pascale Fung.\n2014. Overview for the ﬁrst shared task on language\nidentiﬁcation in code-switched data. In Proceedings\nof the First Workshop on Computational Approaches\nto Code Switching, pages 62–72, Doha, Qatar, Octo-\nber. Association for Computational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov.\n2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. J. Mach. Learn. Res., 15(1):1929–\n1958, January.\nNICTA VRL. 2010. Multilingual language identiﬁca-\ntion: Altw 2010 shared task dataset. In Australasian\nLanguage Technology Association Workshop 2010,\npage 4.\nX. Yang and W. Liang.\n2010.\nAn n-gram-and-\nwikipedia joint approach to natural language identi-\nﬁcation. In 2010 4th International Universal Com-\nmunication Symposium, pages 332–339, Oct.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-01-12",
  "updated": "2017-07-29"
}