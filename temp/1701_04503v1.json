{
  "id": "http://arxiv.org/abs/1701.04503v1",
  "title": "Deep Learning for Computational Chemistry",
  "authors": [
    "Garrett B. Goh",
    "Nathan O. Hodas",
    "Abhinav Vishnu"
  ],
  "abstract": "The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.",
  "text": "1 \n \n \n \n \nDeep Learning for Computational Chemistry \n \n \nGarrett B. Goh,*,† Nathan O. Hodas,‡ Abhinav Vishnu† \n \n†High Performance Computing Group, Pacific Northwest National Laboratory, 902 Battelle Blvd, \nRichland, WA 99354 \n‡Data Science and Analytics, Pacific Northwest National Laboratory, 902 Battelle Blvd, Richland, WA \n99354 \n \n \n \n \n \n* Corresponding Author: Garrett B. Goh  \nEmail: garrett.goh@pnnl.gov  \n  \n \n2 \n \nAbstract \nThe rise and fall of artificial neural networks is well documented in the scientific literature \nof both computer science and computational chemistry. Yet almost two decades later, we are now \nseeing a resurgence of interest in deep learning, a machine learning algorithm based on multilayer \nneural networks. Within the last few years, we have seen the transformative impact of deep \nlearning in many domains, particularly in speech recognition and computer vision, to the extent \nthat the majority of expert practitioners in those field are now regularly eschewing prior established \nmodels in favor of deep learning models. In this review, we provide an introductory overview into \nthe theory of deep neural networks and their unique properties that distinguish them from \ntraditional machine learning algorithms used in cheminformatics. By providing an overview of the \nvariety of emerging applications of deep neural networks, we highlight its ubiquity and broad \napplicability to a wide range of challenges in the field, including QSAR, virtual screening, protein \nstructure prediction, quantum chemistry, materials design and property prediction. In reviewing \nthe performance of deep neural networks, we observed a consistent outperformance against non-\nneural networks state-of-the-art models across disparate research topics, and deep neural network \nbased models often exceeded the “glass ceiling” expectations of their respective tasks. Coupled \nwith the maturity of GPU-accelerated computing for training deep neural networks and the \nexponential growth of chemical data on which to train these networks on, we anticipate that deep \nlearning algorithms will be a valuable tool for computational chemistry. \n \n \n3 \n \n1. \nIntroduction \nDeep Learning is the key algorithm used in the development of AlphaGo, a Go-playing \nprogram developed by Google that defeated the top human player in 2016.1 The development of \ncomputer programs to defeat human players in board games is not new; IBM’s chess-playing \ncomputer, Deep Blue, defeated the top chess player two decades ago in 1996.2 Nevertheless, it is \nworth noting that Go is arguably one of the world’s most complex board game. Played on a 19x19 \nboard, there are approximately 10170 legal positions that can be played. Compared to the \ncomplexity of Go, it has been estimated that the Lipinski virtual chemical space might contain \nonly 1060 compounds.3,4  \nDeep learning is a machine learning algorithm, not unlike those already in use in various \napplications in computational chemistry, from computer-aided drug design to materials property \nprediction.5-8 Amongst some of its more high profile achievements include the Merck activity \nprediction challenge in 2012, where a deep neural network not only won the competition and \noutperformed Merck’s internal baseline model, but did so without having a single chemist or \nbiologist in their team. In a repeated success by a different research team, deep learning models \nachieved top positions in the Tox21 toxicity prediction challenge issued by NIH in 2014.9 The \nunusually stellar performance of deep learning models in both predicting activity and toxicity in \nthese recent examples, originate from the unique characteristics that distinguishes deep learning \nfrom traditional machine learning algorithms.  \nFor those unfamiliar with the intricacies of machine learning algorithms, we will highlight \nsome of the key differences between traditional (shallow) machine learning and deep learning.  \nThe simplest example of a machine learning algorithm would be the ubiquitous least-squares linear \nregression.  In linear regression, the underlying nature of the model is known (linear in this \n4 \n \ncontext), and the input, otherwise known as the features of the model are linearly independent to \neach other. Additional complexity may be added to linear regression by transforming out the \noriginal data (i.e. squaring, taking the logarithm, etc.). As more of these non-linear terms are added, \nthe expressive power of the regression model increases. This description highlights three \ncharacteristics of traditional (shallow) machine learning. First, the features are provided by a \ndomain expert. In a process known as feature extraction and/or engineering, various \ntransformations and approximations are applied, which can be motivated from first principles, or \nmay be well-known approximations, or even educated guesses. Second, shallow learning is \ntemplate matching.  It does not learn a representation of the problem, it merely learns how to \nprecisely balance a set of input features to produce an output. Third, its expressive power grows \nwith the number of terms (i.e. parameters to be fitted), but it may require exponentially many terms \nif the nonlinear transformations are chosen poorly. For example, a simple power series expansion \nwill require an extremely large amount of terms (and parameters) to fit functions with large \namounts of oscillations. \nThe mapping of features into an output using the function provided is the task of a \nprocessing unit, and deep learning algorithms are constructed from a collection of these processing \nunits, arranged in a layered and hierarchical fashion. Therefore, unlike simpler machine learning \nalgorithms, it maps features through a series of non-linear functions that are stitched together in a \ncombinatorial fashion to optimally maximize the accuracy of the model. As a result of this complex \narchitecture, a deep learning algorithm learns multiple levels of representations that correspond to \ndifferent levels of abstraction, forming a hierarchy of concepts. By constructing these so-called \n“hierarchical representations,” deep learning has an internal state that may be transferred to new \nproblems, partially overcoming the template matching problem. Finally, information can take \n5 \n \nmany paths through the network, and, as a result, expressive power grows exponentially with \ndepth.10 Ultimately, these unique characteristics of deep learning enables it to utilize raw data \ndirectly as opposed to engineered features, and often the resulting models constructed produce a \ncomparable level of predictive accuracy. Deep learning achieves this ability because within the \nmultiple layers of the non-linear functions, the algorithm transforms the raw data and maps it to \nintermediate “output” that serve as input (features) for the latter layers in the algorithm, in the \nprocess gradually transforming raw data into learned features. In short, deep learning algorithms \nare potentially capable of automatically (i.e. without expert intervention) engineering the necessary \nfeatures that are relevant to optimally predict the output of interest. \nThe majority of deep learning algorithms currently developed are based off artificial neural \nnetworks, and for the purpose of this review we will focus on deep neural networks exclusively. \nIn the first half of this review, we will provide a brief non-technical introduction to deep learning, \nstarting with a basic background on artificial neural networks and highlighting the key technical \ndevelopments in the last decade that enabled deep neural networks. In addition, we will focus on \nhow deep learning differs from traditional machine learning algorithms that are used in \ncomputational chemistry, and how the ongoing resurgence of deep learning differs from artificial \nneural network models in the 1980s, which may be regarded as its “parent” algorithm. In the next \nhalf of the review, we will include a survey of recent developments of deep learning applications \nacross the field of computational chemistry, where we will examine its performance against \nexisting machine learning models, and future prospects for contributing to the field. This review \nwas written primarily to serve as an introductory entry point for computational chemists who wish \nto explore or integrate deep learning models in their research from an applications standpoint, and \n6 \n \nadditional references to existing literature reviews will be provided to cover the more technical \naspects of deep learning neural network architecture and optimization. \n2. \nDeep Learning 101 \nArtificial neural networks (ANNs), on which most deep learning algorithms are based on, \nare a class of machine learning algorithm inspired by biological neural networks, used to estimate \nor approximate functions by translating a large number of inputs into a target output (Figure 1a). \nANNs are constructed from a series of layers, and each layer comprises many “neurons”. Each \nneuron accepts an input value from the previous layer, and maps it onto a non-linear function. The \noutput of this function is used as the input for the next layer in the ANN, until it reaches the last \nlayer, where the output corresponds to the objective that is to be predicted. In addition, a tunable \nparameter, the “weight” (or coefficient) of each neuron’s function is adjusted in the construction \nof this model to minimize the error of the predicted value, a process known as “training” the neural \nnetwork. Figuratively, the collection of these neurons in ANNs mimics the way neurons work in \nbiological systems, hence its name, artificial neural network. \n \nFigure 1: (a) Schematic representation of a traditional feedforward artificial neural network (ANN) \nwith one hidden layer. Each neuron denoted as circles accepts a series of n input values and maps \n7 \n \nit to an output using a non-linear function, with a bias term (i.e. output of the neural network when \nit has zero input) applied to all neurons in the hidden layer. (b) Deep neural network (DNN) differ \nfrom ANN by having multiple (n>3) hidden layers as depicted in the schematic diagram, the bias \nterm is omitted here for simplicity. \n \nThe power of ANNs, as hinted earlier, lies in their ability to make multiple non-linear \ntransformations through many hidden layers of neurons, where the “hidden” term refers to layers \nthat are not directly adjacent to the input or output. In this process, increasingly complex and \nabstract features can be constructed, through the addition of more layers and/or increasing the \nwidth of layers (i.e. increasing the number of neurons per layer). Correspondingly, the model can \nlearn increasingly complex and abstract representations (i.e. “concepts” if the term is used loosely). \nHowever, for one to use more than a single hidden layer, it is necessary to determine how to assign \nerror attribution and make corrections to its weights by working backwards originating from the \npredicted output, and back through the neural network. This backwards propagation of errors is \nknown formally as “backpropagation”. Although the conceptual foundation of backpropagation \nwas discovered in 1963,11 it was not until 1986 that Hinton and co-workers discovered a way for \nthis algorithm to be applied to neural networks,12 which was a key historical development that \nenabled practically usable ANNs. \nDuring the process of backpropagation, an algorithm known as gradient descent is used to \nfind the minimum in the error surface caused by each respective neuron when generating a \ncorresponding output. Conceptually, gradient descent is no different from the steepest descent \nalgorithm used in classical molecular dynamics simulation. The major difference is instead of \niteratively minimizing an energy function and updating atomic coordinates for each step, an error \nfunction of the target output of the ANN is iteratively minimized and the weights of the neurons \n8 \n \nare updated each step, which are also known as “iteration” in the ANN literature. The data in the \ntraining set may be iterated over multiple times, with a complete pass over the data being called \nan “epoch.” \nA key issue with backpropagation is that the error signals become progressively more \ndiffused as the signal goes back through each hidden layer. This is because, as the signal goes \ndeeper into the model, an increasing number of neurons and weights are associated with a given \nerror.  Until recently, this made it difficult to train many layers efficiently; anything more than a \nfew layers that required a long time to converge with a high probability of overfitting, especially \nfor the layers closest to the output. In addition, the nonlinear transformation functions, such as \nsigmoids, had finite dynamic range, so error signals tends to decay as they passed through multiple \nlayers, which is more commonly known as the “vanishing gradient problem”.13 \nSince 1986, several key algorithms, including unsupervised pre-training,14 rectified linear \nfunctions15 and dropout,16 have been developed to improve the training process for ANN, to \naddress the vanishing gradient problem, and to reduce overfitting which ANN are particularly \nsusceptible to. Perhaps the largest impediment to training deep neural networks (DNN), was the \nvanishing gradient problem as it practically capped the depth of the neural network. Pre-training, \ndiscovered by Hinton et al. in 2006 is a fast, greedy, algorithm that uses an unsupervised layer-\nwise approach to train a DNN one layer at a time.14 After the pre-training phase is complete, a \nmore subtle fine-tuning process, such as stochastic gradient descent, is used to train the model. \nUsing the pre-training approach, the model would have already learnt the features before \nbackpropagation begins, mitigating the vanishing gradient problem. An alternative solution \nemerged in 2011, where Bengio and co-workers demonstrated the rectified linear activation \n(ReLU) function that sidesteps the vanishing gradient problem entirely. The ReLU’s first \n9 \n \nderivative is precisely unity or 0, generally ensuring that error signals can back-propagate without \nvanishing or exploding. (Figure 2).15  \n \nFigure 2: Plot of (a) sigmoidal and (b) rectified-linear (ReLU) function (in blue) and their \ncorresponding first derivative (in red). Unlike the sigmoidal function, where its derivative varies \nwith respect to the value of x, in the ReLU function, the first derivative is either 0 or 1. \nAs these methods enabled the training of deeper and more complex neural network \narchitecture, overfitting also became more of an issue, which led to the development of the dropout \nalgorithm. In dropout, for each epoch of the training process, a fixed proportion of neurons are \nrandomly selected to be temporarily excluded from the model. The net effect of dropout is that it \nsimulates many different architectures during training, which prevents co-dependency among \nneurons and reduces overfitting.16 Whilst the architecture of modern DNNs vary widely, a popular \nconfiguration is ReLU-based neural networks.  When coupled with dropout and early stopping, \nsuch ReLU networks have often been enough to regularize the model (i.e. prevent overfitting).17  \nHaving provided a summary of the key developments in ANNs and its associated \nalgorithms, we note that it is by no means comprehensive. In addition to the traditional feedforward \nDNN (Figure 1b) that has been discussed thus far, more recent developments include alternative \narchitectures, notably convolutional neural networks (Figure 3a),18,19 recurrent neural networks \n10 \n \n(Figure 3b),19,20 and autoencoders (Figure 3c) that have been highly successful in computer vision \nand natural language processing applications. A technical discussion of various DNN architectures \nwhile informative to understanding the deep learning literature, is beyond the scope of this review, \ntherefore, we refer our readers to the following prior publications summarizing this research \ntopic.21-24 By now, it should be evident that ANNs itself are not a new invention.  Indeed, the \nmathematical algorithm for ANNs was developed by McCulloch and Pitts in 1943,25 and \npractically trainable ANNs dates as far back to 1986, coinciding with invention of backpropagation \nfor neural networks by Rumelhart, Hinton, and Williams.12 Deeper neural networks beyond a few \nhidden layers (Figure 1b) was only achievable with more recent algorithmic developments in the \nlast few years.14-16 Therefore, how is DNNs not just the mere rebranding of ANNs of the last \ncentury, and how is it better than the traditional machine learning algorithms that are already \nsuccessfully used in various cheminformatics applications?  \n \n11 \n \nFigure 3: (a) Schematic diagram of a convolutional neural network (CNN). CNNs are designed \nwith the explicit assumption that the input is in the form of image data. Each convolutional layer \nextracts and preserves the spatial information and learns a representation which is then typically \npassed onto a traditional fully-connected feedforward neural network before the output layer. (b) \nSchematic diagram of a recurrent neural network (RNN). RNNs at its simplest implementation are \na modification of the standard feedforward neural network where each neuron in the hidden layer \nreceives an additional input from the output from the previous iteration of the model, denoted as \n“t-1” circular arrows. (c) Schematic diagram of an autoencoder, which are neural networks used \nin unsupervised learning. In autoencoders, the objective is to learn the identity function of the input \nlayer, and in the process, a compressed representation of the original data in the hidden layers is \nlearned. \nMany decades of chemistry research has led to the development of several thousand \nmolecular descriptors that describe a range of properties of conceivably any compound. Molecular \ndescriptors thus serve as features constructed using chemical knowledge and intuition (i.e. domain \nexpertise) that can be used in traditional machine learning models, which have achieved reasonable \nsuccess in computational chemistry applications.26-31 Traditional machine learning algorithms such \nas linear regression and decision trees are intuitive and create simple models that humans can \nunderstand. Nevertheless, as we progress to the prediction of more complex properties with non-\nlinear relationship, typically those associated with biological processes and materials engineering, \nit is often necessary to rely on more sophisticated and less transparent algorithms such as support \nvector machines (SVM) and random forests (RF) in order to achieve an acceptable level of \npredictive accuracy. At a first glance, deep learning algorithms falls under the latter category, but \nit has one major difference. Unlike SVMs and RFs, DNNs transform inputs and reconstruct them \n12 \n \ninto a distributed representation across the neurons of the hidden layers. With appropriate training \nmethods, different features will be learned by the neurons in the hidden layers of the system; this \nis referred to as automatic feature extraction. As each hidden layer becomes the input for the next \nlayer of the system and non-linear transformations can be applied along the way, it creates a model \nthat progressively “learns” increasingly abstract, hierarchical and deep features.  \nAutomatic feature extraction, a process that requires no domain knowledge, is therefore \none of the most significant benefits of a deep learning algorithm. This is unlike traditional machine \nlearning algorithms, where a model must be carefully constructed with the “correct” features based \noff chemical knowledge and intuition for it to perform and generalize well. It is for this reason, \nthat deep learning has become the dominant algorithm used in speech recognition32 and computer \nvision18,33-35 today. ImageNet is an annual assessment and competition of various algorithms for \nimage classification. Prior to deep learning, the state-of-the-art models employed hovered in the \n25-30% error rate, which falls short from the ideal goal of matching a trained human error rate of \n5.1%.36 In 2012, deep learning algorithms were first introduced to this community by Hinton and \nco-workers,18 and their DNN-based model achieved a 16.4% error rate. That was a significant \nimprovement from established models in computer vision at that time, and the second-best \nperforming model based off traditional machine learning algorithms only achieved a 26.2% error \nrate. Subsequent improvements in DNN-based models eventually achieved an error rate of under \n5.0%, exceeding human performance in 2015 (Figure 4), which was only 3 years after deep \nlearning made its introduction to the computer vision field.33,34 For practitioners in these field, the \nimpact of deep learning and its automatic feature extraction ability has been transformative, not \nonly in its ability to exceed “glass ceiling” expectations in the field, but the remarkably short time \nit has taken to achieve it. In recent years, deep learning has also demonstrated promise in other \n13 \n \ndisciplines outside the computer science domain, including high-energy particle physics37 and \nbioinformatics.38 \n \nFigure 4: Historical error rate of the best performing image classification algorithms in the annual \nImageNet competition.39 Established models of computer vision stagnated at 25-30%. The \nintroduction of deep learning in 2012 led to a significant improvement to ~15%, and human-level \naccuracy (~5%) for image classification was achieved by 2015.  \nAn equally important aspect of deep learning that has not been discussed is the role of non-\nalgorithmic developments over the years. Specifically, the availability of “big data” and the GPU \nhardware technological advances that were both absent in the last century have created a \nconfluence of events that makes the advent of DNNs different from the ANNs of the last century. \nThe seminal work in 2012 that is most widely regarded as the paper that propelled deep learning \nin the limelight was Hinton’s AlexNet paper.18 While algorithmic developments, notably dropout \n14 \n \ncontributed to its success, the availability of a much larger dataset comprising of 1.2 million \nimages, compared to datasets of 10,000 images used in the past, also played a critical role in its \nsuccess. With the development of deeper and larger neural networks, training time can often extend \nto days or weeks. However, much like how the field of computational chemistry has benefited \nfrom the rise of GPU-accelerated computing,40,41 this technology has also mitigated the training \nspeed issues of DNNs. \nOf the more practical considerations, the availability of open-source code and \ndocumentation for training neural networks on GPUs is also arguably another reason for the rapid \nproliferation of deep learning in recent years, including its impact on academic research as \nevidenced by the exponential growth of deep learning related publications since 2010 (Figure 5a). \nMuch like how the majority of the computational chemists in modern times no longer write their \nown code to perform molecular dynamics simulation or run quantum chemical calculations, but \ninstead rely on established software packages,42-48 the deep learning research community has too \nreach a similar level of maturity, with the current major software packages for training neural \nnetworks including Torch, Theano, Caffe, and Tensorflow. Perhaps the oldest of the four, Torch \nwas first released in 2002 as a machine learning scientific computing framework developed at \nNYU, but since then deep learning libraries has been added.49 Theano was the first purposed-\ndeveloped deep learning framework, released by Benjio and co-workers at Université de Montréal \nin 2008,50 and it has since developed into a community effort with over 250 contributors. This was \nclosely followed with the release of Caffe, developed by the Berkeley Vision and Learning Center \nin 2014.51 Most recently, Tensorflow,52 which is developed by Google was released in late 2015 \nhas arguably gained a surge of uptake in the deep learning community, as evidenced from its spike \nin google search rankings (Figure 5b), and the fact that its Github has been starred and forked over \n15 \n \n33,000 and 14,000 times respectively, despite it being only released for a little over a year. In \naddition, APIs, such as Keras released in 2015, has greatly simplified the construction and training \nof neural networks, which has significantly reduced the barrier of entry for new deep learning \npractitioners. \n \nFigure 5: Growth of (a) deep learning publications as indexed by ISI, and (b) annual google trends \nscore of major deep learning software packages since 2010. \nUnquestionably, the computer science domain has been the main benefactor of the surge \nof mineable data obtained from the internet (Figure 6a), and not surprisingly has also been the field \nwhere deep learning had the largest impact. In chemistry, we have also seen a corresponding \ngrowth of data in publically accessible databases, such as the Protein Data Bank (Figure 6b) and \nPubChem (Figure 6c), with more data being generated from recent developments in high-\nthroughput omics technologies.53 It is for these reasons that we are optimistic that the field of \ncomputational chemistry is starting to experience the same confluence of events, and this will \ngreatly facilitate deep learning applications in our field. We can leverage on the algorithmic \n16 \n \nbreakthroughs in the computer science domain, the increasing availability of chemical data, and \nthe now matured GPU-accelerated computing technologies (Figure 6d).  \n \nFigure 6: The growth of (a) global data generated, (b) number of structures deposited in the Protein \nData Bank, (c) number of compounds deposited in PubChem and (d) GPU computing power for \nscientific computing,54 all share similar parallels in their upwards trajectory. \n \n17 \n \n3. \nComputer-Aided Drug Design \nIn computer-aided drug design, traditional machine learning algorithms have a long history \nin the field of cheminformatics, notably in their contribution to quantitative structure activity \nrelationship (QSAR) applications. In QSAR, the output to be predicted is usually the biological \nactivity of a compound. Usually regression models are used, and the input data are molecular \ndescriptors, which are precomputed physicochemical properties of the molecule, designed from \nchemistry domain knowledge. Early work in QSAR applications used linear regression models, \nbut these were quickly supplanted by Bayesian neural networks,55-57 followed by RFs26 and \nSVMs.31 Practitioners in the field have historically favored models that allow for variable selection \nso that an informed chemist can determine if selected features made sense. In addition, models that \nallowed assessment of uncertainty of output predictions were also preferred. The field of QSAR is \nvast, and we refer readers to the following list of reviews for key historical technical \ndevelopments.58-61 For the purpose of this review, we will limit the scope of discussion to the \nperformance of DNN-based QSAR models and appropriate comparisons to traditional machine \nlearning models. \nThe first foray of deep learning into QSAR was the Merck challenge in 2012.62 In this \npublically available challenge, teams were provided precomputed molecular descriptors for \ncompounds and their corresponding experimentally measured activity for a total of 15 drug targets. \nSubmitted models were evaluated on their ability to predict activity against a test set not released \nto participants. The winning group used DNN models, led by Dahl who was part of Hinton’s \nresearch team.62 Notably, it should be emphasized that the team had no formally trained \ncomputational chemist in the group; they were from the computer science department. \n18 \n \nIn 2014, Dahl et, al., submitted an arxiv paper exploring the effectiveness of multi-task \nneural networks for QSAR applications, based on the algorithms used in the Merck challenge.63 In \nthis work, the authors used a multi-task DNN model. Here “multi-task” refers to a model that \npredicts not just a single output of interest, but multiple outputs simultaneously, which in their \ncase was the results from 19 assays. The dataset used was curated from PubChem and included \nover 100,000 data points. Molecular descriptors totaling 3764 descriptors per molecule were \ngenerated using Dragon,64 and they were used as the input features for the DNN. In an accuracy \nperformance benchmark against other traditional machine learning algorithms, such as gradient-\nboosted decision trees and logistic regression, the DNN-based model outperformed all others in \n14 of 19 assay predictions by a statistically significant margin and was comparable in terms of \nperformance in the remaining 5 assay prediction.63 In addition, the advantages of a multi-task \nneural network was noted by the authors, particularly in the fact that it develops a shared, learned \nfeature extraction pipeline for multiple tasks. This means that not only can learning more general \nfeatures produce better models, but weights in multi-task DNNs are also constrained by more data \ncases, sharing statistical strength.63 Lastly, an interesting observation from that study was how \nDNNs were able to handle thousands of correlated input features, which goes against traditional \nQSAR wisdom as highlighted by Winkler in 2002,65 although we note that the observations \npublished by Winkler at that time was prior to the development of DNNs. In Dahl’s work, the \nauthors observed that halving the input features did not led to any performance degradation. \nA subsequent study in 2015 published by Merck, comprehensively analyzed the training \nof DNNs and compared their performance to the current state of the art used in the field, RF-based \nmodels, on an expanded Merck challenge dataset.66 The authors concluded that DNNs could be \nadopted as a practical QSAR method, and easily outperformed RF models in most cases. In terms \n19 \n \nof practical adoption, the authors emphasized the dramatic advance in GPU hardware that DNNs \nleverage and also the economic cost advantages of deploying GPU resources as opposed to \nconventional CPU clusters that are used by traditional machine learning models.66 The key issue \nassociated with training deep neural networks, particularly in the number of tunable parameters \nwas also investigated. The authors discovered that most single task problems could be run on \narchitectures with two hidden layers, using only 500-1000 neurons per layer and 75 training \nepochs. More complex architecture and/or longer training time yielded incremental but \ndiminishing returns in model accuracy.66 Despite the overall promising performance of DNNs in \nthe Merck challenge and associated studies as summarized above, the results were received with \nskepticism by some practitioners in the research community.67 Common concerns include the \nsmall sample size, and that the incremental improvements in predictive accuracy was difficult to \njustify in the face of increase in model complexity. \nIn 2014, Hochreiter and co-workers published a peer-reviewed paper at the Neural \nInformation Processing Systems (NIPS) conference on the application of multi-task DNNs for \nQSAR application on a significantly larger dataset.68 In this study, the authors curated the entire \nChEMBL database, which was almost 2 orders of magnitude larger than the original Merck \nchallenge dataset. This dataset included 743,336 compounds, approximately 13 million chemical \nfeatures, and 5069 drug targets. It is also interesting that the authors did not use explicitly computed \nmolecular descriptors as input data, but rather used ECFP4 fingerprints69 instead. The authors \nbenchmarked the accuracy performance of the DNN model across 1230 targets, and compared \nthem against traditional machine learning models, including SVMs, logistic regression and others. \nIt should be noted that gradient-boosted decision trees which performed almost as well as DNNs \nin Dahl’s 2014 paper was not included in this study. Nevertheless, it was demonstrated that DNNs \n20 \n \noutperformed all models they tested on, which also included 2 commercial solutions and 3 \ncurrently implemented solutions by pharmaceutical companies (Figure 7).68 While most traditional \nmachine learning algorithms accuracy ranged from 0.7 to 0.8 AUC, DNNs achieved an AUC of \n0.83. Of the better performing models (AUC > 0.8), DNNs also had the least severe outliers, which \nthe authors hypothesized was due to the DNN’s shared hidden representation that enabled it to \npredict tasks which would be difficult to solve when examined in isolation. In agreement with \nDahl’s 2014 study, the use of a multi-task DNN conferred two advantages: (i) allowance for multi-\nlabel information and therefore utilizing relations between tasks and (ii) allowance to share hidden \nunit representations among prediction tasks.68 The authors in this study noted that the second \nadvantage was particularly important for some drug targets where very few measurements are \navailable, and thus suggested that a single target prediction may fail to construct an effective \nrepresentation.68 The use of multi-task DNNs partially mitigates this problem, as it can exploit \nrepresentations learned across different tasks and can boost the performance on tasks with fewer \ntraining examples. Moreover, DNNs provide hierarchical representations of a compound, where \nhigher levels represent more complex concepts that would be potentially more transferable beyond \nthe training set data.68  \n \n21 \n \n \nFigure 7: Performance accuracy (in terms of AUC metrics) of deep neural network against several \ntraditional machine learning algorithms, including: support vector machines (SVM), logistic \nregression (LR), k-nearest neighbor (k-NN) and commercially-implemented solutions (Pipeline \nPilot Bayesian Classifier, Parzen-Rosenblatt KDE-based approach and Similarity Ensemble \nApproach respectively) for activity prediction of a curated database obtained from ChEMBL.68 \nA similar large scale study was submitted to arxiv in 2015 by the Pande group and \nGoogle.70 In this study, about 200 drug targets were identified, but significantly more data points \n(40 million) were included. Unlike the earlier NIPS paper, Pande and co-workers focused their \ninvestigation on the effectiveness of multi-task learning in DNNs rather than the performance of \nthe DNN model itself. The authors curated a database that was combined from multiple sources of \npublicly available data, including PCBA from the PubChem Database,71 MUV from 17 \nchallenging datasets for virtual screening72, DUD-E group73 and the Tox21 dataset.74 As with \nHochreiter and co-workers, the molecules were featurized using ECFP fingerprints, and no explicit \nmolecular descriptors were computed. Amongst the key findings, was that multi-task performance \n22 \n \nimprovement was consistently observed, although it was not evident whether additional data or \nadditional tasks had a larger effect in improving performance.70 The authors also observed limited \ntransferability to tasks not contained in the training set, but the effect was not universal and \nrequired large amounts of data when it did work successfully, which partially reinforces the claims \nof multi-task learning advantages proposed by Hochreiter and Dahl.63,68 Curiously, the multi-task \nimprovement varied in degree from one dataset to another, and no satisfactory explanation was \nprovided. Nevertheless, the consistent outperformance of multi-task DNNs against traditional \nmachine learning models such as logistic regression and RF was evident (Figure 8), where the \nperformance lift in AUC ranges from 0.02 to 0.09.70  \n \nFigure 8: Consistent performance lift in accuracy (in terms of AUC metrics) was observed across \n3 different databases (PCBA, MUV, Tox21) when using multi-task deep neural networks (MT-\nDNN) as compared to logistic regression (LR), random forest (RF) and single-task neural network \n(ST-NN).70 \n23 \n \n \nTo date, there has been at least 4 reported applications of DNNs for QSAR, with consistent \nobservations that deep learning outperforms traditional machine learning counterparts. However, \nall of the studies have thus far mostly focused on biological activity prediction. Conceptually, \nDNNs should have similar performance in predicting other properties of interest, which may \ninclude ADMET properties, as well as applications in other parts of computer-aided drug design, \nsuch as in virtual screening. \nDrug-induced liver injury (DILI) is the most frequent cause of safety-related drug \nwithdrawals over the last 5 decades.75 Mechanisms underlying DILI are complicated and diverse, \ndrugs that cause DILI in humans are not easily probed by conventional means, making \ntoxicological studies of DILI difficult. A recent study from Xu et. al. used DNNs to predict DILI \ntoxicity.76 The authors used both explicit molecular descriptors, computed from Mold77 and \nPaDEL,78 as well as the URGNN method for molecular structuring encoding developed by Lusci \net. al.79 as input data for the DNNs. The model was trained on 475 drugs, with an external test set \nof 198 drugs, and the best model that utilized a DNN had accuracy of 86.9%, sensitivity of 82.5%, \nspecificity of 92.9%, and AUC of 0.955.76 In comparison, traditional models have lower absolute \nperformance metrics by 10-20%.76 Interestingly, using input from the URGNN molecular \nstructural encoding method, the authors created a model with the highest performance (AUC \n0.955), outperforming similarly trained DNNs that used calculated molecular descriptors from \nMold (AUC 0.931) and PaDEL (AUC 0.895).76 This suggests that a good molecular encoding \nmethod such as UGRNN may be more effective in providing the necessary features to DNNs, as \ndeep learning algorithms have the ability to automatically extract the necessary features, and this \nability may be on par or perhaps even better than domain-expert feature engineering through the \ndevelopment of explicit molecular descriptors.  \n24 \n \nAnother application for DNN modeling toxicity was published by Swamidass and co-\nworkers in 2015.80 One common mechanism of drug toxicity stems from electrophilic reactive \nmetabolites that covalently bind to proteins. Epixodes are a functional group of this nature, which \nare often formed by cytochrome P450 metabolism of drug molecules, which acts on aromatic or \ndouble bonds. Swamidass and co-workers results were particularly distinctive, because they \ndeveloped a DNN model to predict the specific location on a molecule that undergoes epoxidation, \ni.e. its site of epoxidation (SOE). This work was based off an earlier model, Xenosite, an ANN-\nbased model for P450 metabolism on small-molecules, which despite being a shallow network, \nwas already outperforming the accuracy of SVM-based models by as much as 5%.81 Further \nimprovements were subsequently achieved by investigating the effect of using different types of \nmolecular fingerprints for modeling P450 metabolism, where they discovered that further accuracy \ngains can be achieved by using a consensus model utilizing different fingerprint types,82 and a \nrelated sister model that predicted the site of glucoronidation metabolism.83 In their more recent \nwork on predicting epoxide-based toxicity, Swamidass and co-workers designed a 4-layer DNN \narchitecture, and trained the model on a database of 702 epoxidation reactions, and identified SOEs \nwith 94.9% AUC performance, and separated (i.e. classified) epoxidized and non-epoxidized \nmolecules with 79.3% AUC.80 Moreover, within epoxidized molecules, the model was able to \nprovide atomic-level precise information, by separating aromatic or double bond SOEs from all \nother aromatic or double bonds with AUCs of 92.5% and 95.1%, respectively.80 This makes the \nDNN model the first mechanistic model in the literature, which not only predicts the formation of \nreactive epoxides of drug candidates, but also accurately identifies the specific epoxidized bonds \nin the molecule. Using a similar DNN model, Swamidass and co-workers modelled the site of \nreactivity of small-molecules towards soft nucleophiles such as gluthaione (GSH).84 By training \n25 \n \nonly on qualitative reactivity data, they were able to construct a DNN-based model that identified \nsites of reactivity within reactive molecules with 90.8% accuracy, and separate reactive and \nunreactive molecules with 80.6% accuracy.84 In addition, the model’s predictions correlated well \nwith quantitative GSH reactivity measurements in external data sets that were more chemically \ndiverse, indicating the model’s generalizability across a larger area of chemical space.84 A \nsubsequent publication expanded the scope of the model to encompass reactivity towards GSH, \ncyanide, protein, and DNA. The resulting model yielded a cross-validated AUC performance of \n89.8% for DNA and 94.4% for protein, and separated electrophilically reactive molecules with \nDNA and protein from nonreactive molecules with a cross-validated AUC performance of 78.7% \nand 79.8%, respectively.85 Furthermore, the model’s performance also significantly outperformed \nreactivity indices calculated from QM methods.85 As drug toxicity is often caused by electrophilic \nreactive metabolites, models that assist in the study of identifying site reactivity, which has been \nup to now conspicuously absent in the literature, can potentially be utilized to construct a \nmechanism-based prediction of molecule toxicity.  \nA larger scale study on chemical toxicity was also recently published by the Hochreiter \ngroup in 2016.86 In this work, the authors reported on the application of DNN models on the Tox21 \nData Challenge released by NIH in 2014. The database consisted of 12,000 environmental \nchemicals and drugs, and their corresponding measurements on 12 different assays designed to \nmeasure a variety of toxicity effects. Not surprisingly, the DeepTox model developed by \nHochreiter and co-workers had the highest performance of all methods submitted to the Tox21 \nchallenge.9 Further analysis of their model indicated that using a multi-task DNN model led to \nconsistent outperformance against single-task models in 10 out of 12 assay predictions.86 \nAdditional benchmarks to traditional machine learning algorithms, including SVM, RF and Elastic \n26 \n \nNet, also demonstrated that DNN outperformed in 10 out of 15 cases.86 Lastly, while the original \nDeepTox model used molecular descriptors provided by NIH in the Tox21 challenge, the authors \nalso showed that a similarly trained DNN model developed using only ECFP4 fingerprint as input \ndata had similar performance to those trained on explicit molecular descriptors, which is similar \nto the observations made by Xu et. al. in their DILI toxicity model.76 Interestingly, on visualization \nof the first hidden layer of these DNNs, the author observed that 99% of neurons in that layer had \na significant association with at least one known toxicophore feature, suggesting that deep learning \ncan possibly support the discovery of new chemical knowledge in its hidden layers.86 \nIn line with the progress in QSAR and toxicity prediction, deep learning algorithms have \nalso started to make an impact in other aspects of computer-aided drug design. In 2013, Baldi and \nco-workers reported using a DNN model to predict molecule solubility.79 More recent research \ndevelopments in this direction was also submitted to arxiv by Pande and co-workers, where they \ndeveloped a multi-task DNN model for predicting not just solubility, but the entire spectrum of \nADMET properties.87 Deep learning may also have a future in virtual screening as a viable \nalternative or complement to existing docking methods. In 2016, an arxiv paper was submitted by \nAtomNet, a startup that developed a DNN model to classify the activity of small molecules docked \nin protein binding pockets.88 Remarkably, the AtomNet DNN model was able to achieve AUC \nmetrics ranging between 0.7 to 0.9 depending on the test set used, which significantly outperforms \nconventional docking methods, specifically Smina,89 a fork of AutoDock Vina90 by 0.1 to 0.2.88 \nFor additional recent developments of deep learning in applications that are more closely aligned \nto computational biology, we refer our readers to the following reviews that focuses on that \nresearch topic.91 \n \n \n27 \n \n4. \nComputational Structural Biology \n \nPredicting the spatial proximity of any two residues of a protein sequence when it is folded \nin its 3D structure is known as protein contact prediction. The prediction of contacts between \nsequentially distinct residues thus imposes strong constraints on its 3D structure, making it \nparticularly useful for ab initio protein structure prediction or engineering. While the use of \nphysics-based simulation methods, such as long-timescale molecular dynamics92,93 can be used for \nab initio protein structure prediction, the computational demands are formidable. Complementary \nmethods, such as knowledge based physical approaches developed by the groups of Wolynes, \nOnuchic and others are also an option,94,95 although their computational expense while lower is \nstill sufficiently demanding that it cannot be used for large-scale studies. Therefore, machine \nlearning approaches are viable alternatives, including those based off ANNs,96-98 SVMs,27 and \nhidden Markov model.99 Other approaches include template-based approaches that use homology \nor threading methods to identify structurally similar templates to base an inference of protein \ncontact prediction.100,101 The assessment of these various models for contact predictors is one of \nthe highlights of the Critical Assessment of protein Structure Prediction (CASP) challenge which \nstarted in 1996. Despite improvements over the years, the long-range contact prediction has \nhistorically hit a glass ceiling of just below 30% accuracy. The key historical developments of \ncomputational protein structure prediction is voluminous, and we refer interested readers to \nexisting reviews on this topic.102-105 For the purpose of this review, we will limit the scope of \ndiscussion to the performance of recent DNN-based models, and how they have been critical to \nbreaching the historical glass ceiling expectations in the field. \n \nIn 2012, Baldi and co-workers developed CMAPpro, a multi-stage machine learning \napproach, which improved contact prediction accuracy to 36%.106 Three specific improvements \n28 \n \nwere implemented in CMAPpro over earlier models. The first is the use of a 2D recursive neural \nnetwork to predict coarse contacts and orientations between secondary structure elements. In \naddition, a novel energy-based neural network approach was used to refine the prediction from the \nfirst network and used to predict residue-residue contact probabilities. Lastly, a DNN architecture \nwas used to tune the prediction of all the residue–residue contact probabilities by integrating spatial \nand temporal information.106 CMAPpro was trained on a 2356-member training set derived from \nthe ASTRAL database.107 For cross-validation purposes, the set was segmented into 10 disjoint \ngroups belonging to different SCOP fold, which meant that neither training nor validation set \nshared sequence or structural similarity. The resulting model performance was then tested against \na 364-member test set of new protein folds reported between version 1.73 and 1.75 release of the \nASTRAL database. CMAPpro performance was compared against several permutations of the \nmulti-stage machine learning model, including a single hidden layer neural network (NN), a single \nhidden layer neural network that utilized the coarse contact/orientation and alignment predictors, \nwhich is generated by the 2D recursive neural network and the energy-based neural network \n(NN+CA), and a deep neural network but without CA features (DNN). Based on the relative \nperformance, both the deep network architecture and CA features were required to achieve an \naccuracy of 36%; DNN and NN+CA each achieved 32%, while NN which represents the previous \nstate-of-the-art only achieved 26% accuracy.107 \nA different implementation of DNN for protein contact prediction was also reported by \nEickholt and Cheng in 2012.108 In their algorithm, DNCON, it combined deep learning with \nboosting techniques that was used to develop an ensemble predictor. A 1426-member dataset \nderived from the Protein Data Bank was used to train DNCON, with a random split between the \ntraining (1230-member) and validation (196-member) set. Explicitly engineered features were \n29 \n \nused as input for the DNN. Specifically, three classes of features were used: (i) those from two \nwindows centered on the residue pair in question (e.g. predicted secondary structure and solvent \naccessibility, information and likelihoods from the PSSM and Acthley factors, etc.), (ii) pairwise \nfeatures, (e.g. Levitt’s contact potential, Jernigan’s pairwise potential, etc.) and (iii) global features \n(e.g. protein length, percentage of predicted exposed alpha helix and beta sheet residues, etc.).108 \nUsing these engineered features, the DNN model was tasked to predict whether or not a particular \nresidue pair was in contact. In addition, boosted ensembles of classifiers was created by training \nseveral different DNNs using a sample of 90,000 long-range residue-residue pairs from a larger \npool obtained from the training set. In evaluating its performance, cross-validated accuracy of \nDNCON was 34.1%. The model’s performance transferability was demonstrated in its \nperformance benchmarks against the two best predictors of CASP9,109 ProC_S328 and SVMcon,27 \nwhich are based off RF and SVM algorithms respectively. In that assessment, the respective test \nset was used for each software. While the improvement was not as dramatic as that reported by \nBaldi and co-workers, DNCON performance was ~3% better than the state-of-the-art algorithms \nfor its time; ProC_S3 (32.6% vs 29.7%) and SVMcon (32.9% vs 28.5%).108 \nBoth DNN-based protein contact prediction models were noteworthy, as it enabled the \ncommunity to breakthrough the 30% accuracy barrier that was not possible in prior years. Apart \nfrom protein contact prediction, DNNs have also been successfully applied to the prediction of \nvarious protein angles, dihedrals, and secondary structure from only sequence data. Using DNNs, \nZhou, Yang and co-workers published a series of sequence-based predictions for Cα-based angles \nand torsions.110-112  Unlike protein contact prediction, backbone torsions are arguably better \nrestraints for use in ab initio protein structure prediction and other modeling purposes.113 In the \ndevelopment of these DNN-based models, Zhou, Yang and co-workers used a 4590-member \n30 \n \ntraining set and a 1199 independent test set obtained from the protein sequence culling server \nPISCES.114 Input data included specifically engineered features obtained from the Position \nSpecific Scoring Matrix generated by PSI-BLAST,115,116 as well as several other physicochemical \nproperties related to residue identity, including steric, hydrophobicity, volume, polarizability, \nisoelectric point, helix probability, amongst others.117 \nIn the development of the SPINE-X algorithm, a DNN was used to predict secondary \nstructure, residual solvent-accessible surface area (ASA), φ and ψ torsions directly.111 A six-step \nmachine learning architecture was developed where outputs such as ASA were used as subsequent \ninputs for other properties to be predicted, such as the torsions. Based on the evaluation of the \nmodel’s performance on the independent test set, it achieved a mean absolute error of 22⁰ and 33⁰ \nrespectively for the φ and ψ dihedrals. Secondary structure prediction accuracy on independent \ndatasets were ranging from 81.3% to 82.3%, and this achievement is noteworthy, considering that \nthe field of secondary structure prediction from sequence data has stagnated just under 80% \naccuracy in the recent decade, some of which utilize traditional machine learning algorithms.118 In \na similar fashion, for the SPIDER algorithm that was developed later, a DNN was used to predict \nCα angles (θ) and torsions (τ) directly.110 Based on the evaluation of the model’s performance, it \nachieved a mean absolute error of 9⁰ and 34⁰ for θ and τ respectively, and the authors observed \nthat the model’s error increased from helical residues to sheet residues to coil residues, following \nthe trend in unstructuredness. Using these predicted angles and torsions as restraints, the authors \nwere able to model the 3D structure of the proteins with an average RMSD of 1.9A between the \npredicted and native structure.110 The SPINE-X and SPIDER algorithm was subsequently re-\ntrained as a parallel multi-step algorithm that predicted simultaneously the following properties: \nsecondary structure, ASA, φ, ψ, θ and τ.112 This resulted in a modest improvement in overall \n31 \n \naccuracy of secondary structure by 2%, and reduction of MAE by 1-3⁰ for the angles/torsions, \nwhile maintaining the same level of ASA performance. \nApart from protein structure modeling, deep learning has also been utilized to predict other \nproperties of interest based on sequence data. For example, predicting sequence specificities for \nDNA and RNA-binding proteins was recently reported.119,120 In the seminal work by Frey and co-\nworkers,119 the DeepBind algorithm was developed to predict the sequence specificities of DNA \nand RNA-binding proteins. Using 12 terabases of sequence data, spanning thousands of public \nPBM, RNAcompete, ChIP-seq and HT-SELEX experiments, the raw data was used as an input \ninto a DNN algorithm to compute a predictive binding score. DeepBind’s ability to characterize \nDNA-binding protein specificity was demonstrated on the PBM data from the revised DREAM5 \nTF-DNA Motif Recognition Challenge by Weirauch et. al.121 Notably, DeepBind outperformed \nall existing 26 algorithms based on Pearson correlations and AUC metrics, and was ranked first \namongst 15 teams in the DREAM5 submission.119 Interestingly, their results also indicated that \nmodels trained on in vitro data worked well at scoring in vivo data, suggesting that the DNNs has \ncaptured a subset of the properties of nucleic binding itself. \nAs with the repeated occurrence of deep learning outperforming traditional machine \nlearning algorithms in other fields,18,32-35 as well as in computer aided drug design itself,63,68,70 the \nutilization of DNNs in pushing the “glass ceiling” boundaries of protein contact prediction and \nsecondary structure prediction should come as no surprise. Conspicuously absent from this review \nis the application of deep learning for RNA structure prediction and modeling, which to the best \nof our knowledge has yet to be reported. Compared to the protein database, available structural \ndata on RNA is smaller. Furthermore, most RNA structural data are not crystallographic but are \ninstead NMR-based, which itself is subjected to a higher uncertainty by virtue of the fact that \n32 \n \nNMR-structures themselves are approximations resolved using physics-based force field against \nexperimentally bounded restraints.122 Nevertheless, it will be interesting to see how deep learning \ncan benefit the RNA modeling community.  \nLastly, an interesting contrast in the use of deep learning in computational structural \nbiology applications compared to computer-aided drug design, is the exclusive use of engineered \nfeatures, and for some cases, the engineering of the architecture of the multi-stage machine \nlearning algorithm itself. While the findings from the computer-aided drug design field is \npreliminary, there are some indications that explicitly engineered features do not necessarily \nperform better against chemical fingerprints, which arguably require less chemical domain \nknowledge to construct. While we concede that proteins are considerably more complex than small \nmolecules, it would be interesting to determine if the performance of DNN models that uses input \ndata that includes only basic structural and connectivity information, without any specifically \nengineered features, can accurately predict properties such as protein secondary structure, and \nlong-range contacts. \n5. \nQuantum Chemistry \n \nUsing machine learning to supplement or replace traditional quantum mechanical (QM) \ncalculations has been emerging in the last few years. In this section, we will examine some machine \nlearning applications to quantum chemistry, and examine the relative performance of similar \nDNN-based models. In 2012, von Lilienfeld and co-workers developed a machine learning \nalgorithm based on non-linear statistical regression to predict the atomization energies of organic \nmolecules.29 This model used a 7000-member subset of the molecular generated database (GDB), \na library of 109 stable and synthetically-tractable organic compounds. The target data used for \ntraining was atomization energies of the 7000 compounds calculated using the PBE0 hybrid \n33 \n \nfunctional. No explicit molecular descriptors were used as input data, instead only the Cartesian \ncoordinates and nuclear charge were used in a “Coulomb” matrix representation. Arguably, \nwithout explicitly engineered features, this type of representation in the input data would be of the \nsame level as that provided by molecular fingerprints used in classical molecular modeling \napproaches. Using only 1000 compounds for the training set, von Lilienfeld and co-workers \nachieved a mean absolute error (MAE) accuracy of 14.9 kcal/mol. Further tests on an external \n6000 compound validation set yield similar accuracy of 15.3 kcal/mol, demonstrating the \ntransferability of the model within “in class” compounds. What was particularly groundbreaking \nabout this work was the ability to reasonably recapitulate QM-calculated energies, with a mean-\nabsolute error of ~15 kcal/mol, without having any implementation of the Schrodinger Equation \nin the machine learning algorithm at all. More importantly, considering that this work used a \ntraditional machine learning algorithm that lacks the advantages of DNN, and based on DNN’s \nhistorical performance, it suggests that a DNN-based model should perform even better. \n \nA subsequent publication by Hansen et. al. investigated a number of established machine \nlearning algorithms, and the influence of molecular representation on the performance of \natomization energy predictions on the same dataset as used in von Lilienfeld work.123 Amongst \nthe key findings was that using a randomized variant of the ‘Coulomb matrix’ greatly improved \nthe accuracy of atomization energies to achieve as low a MAE as 3.0 kcal/mol.123 Apart from being \nan inverse atom-distance matrix representation of the molecule, the randomized variant is unique \nand retains invariance with respect to molecular translation and rotation. An added “side effect” \nof this improved representation was that it was the richest one developed, as it is both high-\ndimensional and accounting for multiple indexing of atoms.123  The authors discovered that sorting \nvarious representation by information did yield a correspondingly lower accuracy across all \n34 \n \nmachine learning algorithms tested,123 which highlighted the importance of good data \nrepresentation in QM applications. In fairness, it should also be noted that the authors did \nbenchmark ANNs, and while they performed satisfactorily with a MAE of 3.5 kcal/mol, it was not \nconsiderably better than non-linear regression methods of MAE of 3.0 kcal/mol. Nevertheless, we \nhighlight the neural network used was “shallow” with a few layers, and together with the lack of \na larger dataset, does not represent a true DNN implementation. One particularly illuminating \nconjecture from this paper is by extrapolating the performance (MAE error) with respect to the \nsize of the dataset used, the authors concluded that 3 kcal/mol was probably the “baseline” error \nthat one could achieve regardless of the machine learning algorithm used.123 \n \nIn 2013, von Lilienfeld reported the application of the first multi-task DNN model that not \nonly predicted atomization energies, but several other electronic ground and excited state \nproperties.124 In this work, they attempted to capitalize on the advantages of multi-task learning, \nby predicting several electronic properties and potentially capturing correlations between \nseemingly unrelated properties and levels of theory. The data was represented using the \nrandomized variant of the ‘Coulomb matrix’.123 The target data was atomization energies, static \npolarizabilities, frontier orbital eigenvalues HOMO and LUMO, ionization potential and electron \naffinity calculated using several different level of theory such as PBE0, ZINDO, GW and SCS. \nThe atomization energy maintained a similar accuracy of MAE of 0.16 eV (~3.6 kcal/mol) and \nachieved comparable accuracy of MAE of 0.11 to 0.17eV (~2.5 to 3.9 kcal/mol) for the other \nenergy predictions, including HOMO, LUMO, ionization potential and electron affinity.124 \nFurthermore, this level of accuracy was similar to the error of the corresponding level of theory \nused in QM calculations for constructing the training set. \n35 \n \n \nWhile using machine learning algorithms to replace QM calculations is enticing, an \nalternative more “first principles grounded” approach is to use machine learning algorithms to \nsupplement existing QM algorithms. As first reported by von Lilienfeld and co-workers in 2015, \nthey demonstrated the Δ-learning approach, whereby a machine learning “correction term” was \ndeveloped.125 In that study, the authors used DFT calculated properties and were able to predict \nthe corresponding quantity at the G4MP2 level of theory using the Δ-learning correction term. \nThis composite QM/ML approach combines approximate but fast legacy QM approximations with \nmodern big-data based QM estimates trained on expensive and accurate results across chemical \nspace.125 However, we noted that this approach has thus far been only demonstrated using \ntraditional machine learning algorithms. If the performance boost using multi-task DNNs that we \nhave observed on numerous instances applies to this example, a DNN-based approach would \npotentially yield superior results, but that has yet to be reported in the literature. \n \nTo the best of our knowledge, the fewer examples of DNN in quantum chemistry \napplications seem to indicate that it is in an earlier stage of development compared to computer-\naided drug design and computational structural biology. From the literature, we know that \ntraditional machine learning models have been used in other QM applications, such as modeling \nelectronic quantum transport,126 learning parameters for accurate semi-empirical quantum \nchemical calculations,127 etc. In addition, new representation and fingerprints for QM applications \nare also being developed.128,129 Given the observed superior accuracy of DNN-based models \nagainst traditional machine learning models in other fields of computational chemistry, we suggest \nthat the development of DNN-based model for these classical examples of machine learning QM \napplications would be beneficial for the field.  \n \n \n36 \n \n6. \nComputational Material Design \n \nThe logical extension of DNN applications in the field of quantum chemistry is to predict \nand design material properties that are correlated to or based on QM-calculated properties. \nQuantitative structure property relationship (QSPR), which is the analogous version of QSAR in \nthe non-biological domain, is the science of predicting physical properties from more basic \nphysiochemical characteristics of the compound, and it has been extensively reviewed in prior \npublications.130,131 Similar to the early years of modern drug development, material discovery is \nprimarily driven by serendipity and institutional memory.132 This has relegated the field to \nexploratory trial-and-error experimental approaches, and the key bottleneck in molecular materials \ndesign is the experimental synthesis and characterization. In recent years, the paradigm of \ncomputational and rational materials design has been encapsulated under the materials genome \ninitiative.133,134 Due to the newness of this field, in this section, we will examine a few key \naccomplishments of using machine learning for computational material design, and highlighting \ndeep learning applications where available. \n \nA recent high profile example of using machine learning models to accelerate materials \nproperty research was published by Raccuglia et. al. in 2016.30 The synthesis of inorganic-organic \nhybrid materials, such as metal organic frameworks (MOFs), have been extensively studied for \ndecades, but the theoretical understanding of the formation of these compounds are only partially \nunderstood. In the work by Raccuglia et. al., the authors used a SVM-based model to predict the \nreaction outcomes for the crystallization of templated vanadium selenites. What was interesting \nabout their work, was the inclusion of “dark” reactions in training the model, which are failed or \nunsuccessful reactions collected from archived laboratory notebooks. The resulting model had an \n89% success rate, as defined by the synthesis of the target compound type. Notably, this exceeded \n37 \n \nthe human intuition success rate of 78%.132 While a DNN-based model was not used in the study \nper se, there is no technical reason why it could not be used in place of SVM as a tool used for \ncomputational materials synthesis prediction.  \nOne example of how DNN has been used to accelerate materials discovery was reported \nby Aspuru-Gizik and co-workers in 2015.135 Here, the authors used the dataset obtained from the \nHarvard Clean Energy Project – a high-throughput virtual screening effort for the discovery of \nhigh-performance organic photovoltaic materials. The metric to be predicted is power conversion \nefficiency (PCE) which is a function of the HOMO an LUMO energies and several other empirical \nparameters.135  As no high quality 3D data was available to generate Coulomb matrices, the authors \ndecided to use fingerprints based on molecular graphs as input representation. Four different \nrepresentations were tested and the results showed generally consistent accuracy (within the same \norder of magnitude) across HOMO, LUMO and PCE predictions. The dataset consisted of \n2,000,000 compounds randomly selected from the CEPDB database and another 50,000 was \nextracted as the test set. Testing errors of HOMO and LUMO was 0.15 and 0.12eV respectively \nwhich was almost a 5-fold improvement relative to prior non-neural network machine learning \nalgorithms.135 \n \nWhile DNN applications in material design is still at its infancy, it would be interesting to \nsee how its application will fare against traditional QSPR applications and upcoming rational \nmaterials design endeavors, such as in the prediction of spectral properties of fluorophores,136,137 \nproperties of ionic liquids,138 and nanostructure activity.139 \n7. \nReservations about Deep Learning and Of Being a Black Box \nMachine learning algorithms, while they may not be the first tool of choice for many \npractitioners in our field, undeniably possess a rich history in the cheminformatics field and in \n38 \n \napplications like QSAR and protein structure prediction. While it may be argued that deep learning \nin some sense is a resurgence of the previous artificial neural network, the algorithmic and \ntechnological breakthroughs in the last decade has enabled the development of staggeringly \ncomplex deep neural networks, allowing training of networks with hundreds of millions of \nweights. Coupled with the growth of data and GPU-accelerated scientific computing, deep learning \nhas overturned many applications in computer science domains, such as in speech recognition and \ncomputer vision. Given the similar parallels in the chemistry world, it suggest that deep learning \nmay be a valued tool to be added to the computational chemistry toolbox. As summarized in Table \n1, which presents key preliminary publications of DNN-based models, we have noted the broad \napplication of deep learning in many sub-fields of computational chemistry. In addition, the \nperformance of DNN-based model is almost always equivalent to existing state-of-the-art non \nneural-network models, and at times provided superior performance. Nevertheless, we have \nnoticed that the performance lift in many cases are not as significant, if one is to make a comparison \nto the improvements DNN has brought to its “parent” field of speech recognition and computer \nvision. One mitigating factor that explains the lack of a revolutionary advance in chemistry could \nbe the relative scarcity of data. Unlike the computer science domain where data is cheap, especially \nwhen obtained from the internet or social media, the quantity of usable data in chemistry is \nunderstandably smaller and more expensive since actual experiments or computations are needed  \nto generate useful data. In addition, the field of chemistry has been around for centuries and given \nthe fact that chemical principles are based on the laws of physics, it is not unconceivable that the \ndevelopment of features such as molecular descriptors to explain compound solubility for example, \nwould be an easier task than developing features to explain the difference between a dog and a cat, \na common task in computer vision. Therefore, with more accurate and better engineered features  \n39 \n \nTable 1: Meta-analysis of DNN-based model performance relative to state-of-the-art non-DNN models in various computational \nchemistry applications. Only appropriate comparisons are summarized; models trained on similar/identical datasets, using either \ninformation extracted from publications by the same group that reported multiple ML models or publically available competition. \nPrediction / \nCompetition \nDNN \nModels \nComments \nNon-DNN \nModels \nComments \nMerck \nKaggle \nChallenge \n(Activity) \n0.494 R2 \nDNN-based model was the top \nperforming model in the competition.62 \n0.488 R2 \nBest non-DNN model in the competition.140 \n0.465 R2 \nMedian DNN-based model recreated by \nMerck post-competition.66 \n0.423 R2 \nBest non-DNN model (RF-based) by Merck \npost-competition.66 \nActivity \n \n0.830 AUC \n \nMT-DNN based model trained on the \nChEMBL database.68 \n0.816 AUC Best non-DNN model (SVM) trained on the \nChEMBL database.68 \n0.873 AUC MT-DNN based model trained on the \nPCBA database.70 \n0.800 AUC Best non-DNN model (RF) based model \ntrained on the PCBA database.70 \n0.841 AUC \n \nMT-DNN based model trained on the \nMUV database.70 \n0.774 AUC Best non-DNN model (RF) based model \ntrained on the MUV database.70 \nNIH Tox21 \nChallenge \n(Toxicity) \n0.846 AUC DeepTox (MT-DNN based model) was \nthe top performing model.86 \n0.824 AUC Best non-DNN model (multi-tree ensemble \nmodel) was placed 3rd in the Tox21 \nchallenge.141 \n0.838 AUC Runner up in Tox21 challenge was based \noff associative neural networks \n(ASNN).142 \n0.818 AUC Post-competition MT-DNN model.70 \n0.790 AUC Post-competition RF model.70 \nAtom-level \nReactivity/ \nToxicity \n0.949 AUC DNN-based model that predicts site of \nepoxidation, a proxy for toxicity.80 \n- \nNo comparable model in the literature that can \nidentify site of reactivity or toxicity. \n0.898 AUC DNN-based model that predicts site of \nreactivity to DNA.84 \n0.944 AUC \n \nDNN-based model that predicts site of \nreactivity to protein.84 \nProtein \nContact \n36.0% acc. \nCMAPpro (DNN-based model).106 \n \n \n34.1% acc. \nDNCON (DNN-based model).108 \n29.7% acc. \n28.5% acc. \nBest non-DNN model reported in CASP9, \nProC_S3 (RF-based model)28 and SVMcon \n(SVM-based model)27 are listed respectively.  \n40 \n \nin chemistry, it is also plausible that we might not see such a large initial performance \nimprovement, especially for the relatively simpler chemical principles or concepts. \nFurthermore, as computational chemists, there is a greater emphasis placed on conceptual \nunderstanding compared to engineers or technologists which is arguably the more prevalent \nmindset in the computer science field. In this regard, deep learning algorithms currently fall short \non two accounts. First, it lacks the conceptual elegance of a first principles model that is based on \nthe actual laws of physics, and second, DNNs are essentially a black box; it is difficult to \nunderstand what the neural network has “learned” or exactly how it is predicting the property of \ninterest. \nTo address the first issue of conceptual elegance, from a certain perspective, this objection \nmay be more of a philosophical argument of scientific preferences. In most computational \nchemistry applications, unless one is solving the Schrodinger Equation exactly, which we know is \nimpossible for anything but a two body system, one must make approximations to the model. In \nthat sense, almost all of computational chemistry is an empirically-determined, and at times even \nintuitively-determined, approximation of the “true” first principles Schrodinger Equation. To \nillustrate this point, let us examine the historical development of classical molecular modeling \nforce fields, such as CHARMM42 and AMBER.43 For example, the parameterization of dihedral \nangle force constants have historically been targeted to QM-calculated values, the “true” values \ngrounded in validated physical principles. However, because the dynamics of real molecules do \nnot behave in an additive fashion (which itself is another approximation that classical molecular \nmodeling makes), more recent re-parameterization have started modifying dihedral parameters to \nempirically fit experimental NMR distribution, even though that may lead to deviations from the \nQM-calculated values.143,144 Similarly, the choice of columbic interactions to model electrostatic \n41 \n \nforces is only approximately correct, and recent parameter development of modeling charged ion \ninteractions have started fitting to various experimental observables such as osmotic pressure \nvalues, and the introduction of non-physical correction terms when modeling specific pairs of \nelectrostatic interactions.145-147 In these examples, approximation from first principles have to be \nmade, and this process is a human decision that is based on empirical data or at times “chemical \nintuition” – which as Raccuglia et. al. have shown, is not infallible and not always more \naccurate.132 At the risk of oversimplification of the work that computational chemist do, the \ndevelopment of existing computational chemistry models may be viewed as an elaborate curve \nfitting exercise. Instead of using human expert knowledge, a conceivable alternative may be to use \na deep learning algorithm to “suggest”, or perhaps even help us “decide” what approximations \nshould be made in order to achieve the desired results, in a move towards a future paradigm of a \nDNN-based artificial intelligence (AI) assisted chemistry research. This naturally leads to the \nsecond drawback of deep learning as the inevitable question surfaces - How do we know that the \ndeep learning model is learning the correct physics or chemistry? \nWe will concede that in its current implementation deep learning algorithms is still a black \nbox and interrogating what it “learns” is an extremely challenging task. Nevertheless, black box \nalgorithms such as SVM and RF are also used in several computational chemistry applications, \nnotably in examples where they are used primarily as a tool, and/or for prediction of properties \nthat are so complex that even a first principles understanding of the problem will not necessarily \naid in its prediction. We acknowledge that in order to advance deep learning to be more than just \nanother tool in the chemist’s toolkit, and for it to gain more widespread applicability and adoption \nfor scientific research, it is evident that improvement in interpretability of DNN is of paramount \ninterest. While interpretability of neural networks has historically not been a strong research focus \n42 \n \nfor practitioners in this field, it is noteworthy that several recent developments on improving \ninterpretability has been reported.148,149 Other viable options include the use of different neural \nnetwork based machine learning models, such as influence-relevance voters (IVR) that are \ndesigned for interpretability. As demonstrated on a few computational chemistry applications from \nthe work of Baldi and co-workers150,151, the IRV is a low-parameter neural network which refines \na k-nearest neighbor classifier by nonlinearly combining the influences of a chemical’s neighbors \nin the training set. IRV influences are decomposed, also nonlinearly, into a relevance component \nand a vote component. Therefore, the predictions of the IRV is by nature transparent, as the exact \ndata used to make a prediction can be extracted from the network by examining each prediction’s \ninfluences, making it  closer to a “white-box” neural network method.150,151 \n8. \nConclusion \nUnlike traditional machine learning algorithms currently used in computational chemistry, \ndeep learning distinguishes itself in its use of a hierarchical cascade of non-linear functions. This \nallows it to learn representations and extract out the necessary features from raw unprocessed data \nneeded to predict the desired physicochemical property of interest. It is this distinguishing feature \nthat has enabled deep learning to make significant and transformative impact in its “parent” field \nof speech recognition and computer vision. In computational chemistry, its impact is more recent \nand more preliminary. Nevertheless, based on the results from a number of recent studies, we have \nnoted the broad application of deep learning in many sub-fields of computational chemistry, \nincluding computer aided drug design, computational structural biology, quantum chemistry and \nmaterials design. In almost all applications we have examined, the performance of DNN-based \nmodel is frequently superior to traditional machine learning algorithms.  \n43 \n \nAs the complexity of the problem increases that enables the application of multi-task \nlearning (i.e. more predictions of different properties are required), and as the size of the dataset \nincreases, we have also seen deep learning progressing from frequently outperforming to always \noutperforming traditional machine learning models. In addition, some preliminary findings \nindicate that explicitly engineered features such as molecular descriptors may not be necessary to \nconstruct a high performing DNN model, and simpler representations in the form of molecular \nfingerprint or coulomb matrices may suffice. This is because of DNN’s ability to extract out its \nown features through its hidden layers. There is even indication that the features “learned” by \nDNNs correspond to actual chemical concepts such as toxicophores. Coupled with recent research \non improving interpretability of neural networks, it suggest that the future role of DNN in \ncomputational chemistry may not just be only a high-performance prediction tool, but perhaps as \na hypothesis generation device as well. \n \n44 \n \nAcknowledgement \nThe authors thank Dr. Nathan Baker for critically reviewing the manuscript and providing helpful \ncomments and insights on machine learning for computational chemistry applications. This \nresearch was funded by the Pacific Northwest Laboratory Directed Research and Development \n(LDRD) Program and the Linus Pauling Distinguished Postdoctoral Fellowship. \n \n \n45 \n \nReferences \n(1) \nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; van den Driessche, G.; \nSchrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; Dieleman, S.; Grewe, D.; \nNham, J.; Kalchbrenner, N.; Sutskever, I.; Lillicrap, T.; Leach, M.; Kavukcuoglu, K.; Graepel, \nT.; Hassabis, D. Nature 2016, 529, 484. \n(2) \nCampbell, M.; Hoane, A. J.; Hsu, F. H. Artificial Intelligence 2002, 134, 57. \n(3) \nReymond, J. L.; Awale, M. ACS Chem. Neurosci. 2012, 3, 649. \n(4) \nBohacek, R. S.; McMartin, C.; Guida, W. C. Med. Res. Rev. 1996, 16, 3. \n(5) \nLi, H. D.; Liang, Y. Z.; Xu, Q. S. Chemometr. Intell. Lab. 2009, 95, 188. \n(6) \nHautier, G.; Fischer, C. C.; Jain, A.; Mueller, T.; Ceder, G. Chem. Mater. 2010, 22, 3762. \n(7) \nMuller, K. R.; Ratsch, G.; Sonnenburg, S.; Mika, S.; Grimm, M.; Heinrich, N. J. Chem. \nInf. Model. 2005, 45, 249. \n(8) \nBartok, A. P.; Gillan, M. J.; Manby, F. R.; Csanyi, G. Phys. Rev. B 2013, 88. \n(9) \nNIH https://ncats.nih.gov/news/releases/2015/tox21-challenge-2014-winners. \n(10) \nRaghu, M.; Poole, B.; Kleinberg, J.; Ganguli, S.; Sohl-Dickstein, J. arXiv:1606.05336 \n2016. \n(11) \nBryson, A. E.; Denham, W. F.; Dreyfus., S. E. AIAA J. 1963, 1, 2544. \n(12) \nRumelhart, D. E.; Hinton, G. E.; Williams, R. J. Nature 1986, 323, 533. \n(13) \nHochreiter, S.; Bengio, Y.; Frasconi, P.; Schmidhuber, J. A Field Guide to Dynamical \nRecurrent Neural Networks. IEEE Press, 2001 2001. \n(14) \nHinton, G. E.; Osindero, S.; Teh, Y. W. Neural Comput. 2006, 18, 1527. \n(15) \nGlorot, X.; Bordes, A.; Bengio, Y. Proc. of the 14th Int. Conf. on Artificial Intelligence \nand Statistics (AISTATS) 2011. \n(16) \nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. J. Mach. \nLearn Res. 2014, 15, 1929. \n(17) \nLeCun, Y.; Bengio, Y.; Hinton, G. Nature 2015, 521, 436. \n(18) \nKrizhevsky, A.; Sutskever, I.; Hinton, G. E. Advances in Neural Information Processing \nSystems 2012. \n(19) \nW. Xiong; J. Droppo; X. Huang; F. Seide; M. Seltzer; A. Stolcke; D. Yu; Zweig, G. \narXiv:1610.05256 2016. \n(20) \nWu Y, et. al. arXiv:1609.08144 2016. \n(21) \nBengio, Y.; Courville, A.; Vincent, P. IEEE Trans. Pattern Anal. Mach. Intell. 2013, 35, \n1798. \n(22) \nSchmidhuber, J. Neural Netw. 2015, 61, 85. \n(23) \nArel, I.; Rose, D. C.; Karnowski, T. P. IEEE Comput. Intell. M. 2010, 5, 13. \n(24) \nGawehn, E.; Hiss, J. A.; Schneider, G. Mol. Inf. 2016, 35, 3. \n(25) \nMcculloch, W. S.; Pitts, W. B. Math. Biol. 1990, 52, 99. \n(26) \nSvetnik, V.; Liaw, A.; Tong, C.; Culberson, J. C.; Sheridan, R. P.; Feuston, B. P. J. \nChem. Inf. Comput. Sci. 2003, 43, 1947. \n(27) \nCheng, J.; Baldi, P. BMC Bioinform. 2007, 8, 113. \n(28) \nLi, Y. Q.; Fang, Y. P.; Fang, J. W. Bioinformatics 2011, 27, 3379. \n(29) \nRupp, M.; Tkatchenko, A.; Muller, K. R.; von Lilienfeld, O. A. Phys. Rev. Lett. 2012, \n108, 058301. \n(30) \nRaccuglia, P.; Elbert, K. C.; Adler, P. D.; Falk, C.; Wenny, M. B.; Mollo, A.; Zeller, M.; \nFriedler, S. A.; Schrier, J.; Norquist, A. J. Nature 2016, 533, 73. \n46 \n \n(31) \nDu, H. Y.; Wang, J.; Hu, Z. D.; Yao, X. J.; Zhang, X. Y. J. Agric. Food Chem. 2008, 56, \n10785. \n(32) \nGraves, A.; Schmidhuber, J. Neural Netw. 2005, 18, 602. \n(33) \nHe, K.; Zhang, X.; Ren, S.; Sun, J. arXiv:1502.01852 2015. \n(34) \nIoffe, S.; Szegedy, C. arXiv:1502.03167 2015. \n(35) \nSzegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; \nVanhoucke, V.; Rabinovich, A. arXiv:1409.4842 2014. \n(36) \nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, \nA.; Khosla, A.; Bernstein, M.; Berg, A. C.; Fei-Fei, L. IJCV 2015, 115, 211. \n(37) \nBaldi, P.; Sadowski, P.; Whiteson, D. Nat. Commun. 2014, 5, 4308. \n(38) \nChicco, D.; Sadowski, P.; Baldi, P. Proc.of the 5th ACM Conf. on Bioinf. Comput. Biol., \nand Health Inf. 2014, 533. \n(39) \nOlga Russakovsky, J. D., Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng \nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei \narXiv:1409.0575 2015. \n(40) \nStone, J. E.; Phillips, J. C.; Freddolino, P. L.; Hardy, D. J.; Trabuco, L. G.; Schulten, K. \nJ. Comput. Chem. 2007, 28, 2618. \n(41) \nYasuda, K. J. Chem. Theory Comput. 2008, 4, 1230. \n(42) \nBrooks, B. R.; Brooks, C. L.; Mackerell, A. D.; Nilsson, L.; Petrella, R. J.; Roux, B.; \nWon, Y.; Archontis, G.; Bartels, C.; Boresch, S.; Caflisch, A.; Caves, L.; Cui, Q.; Dinner, A. R.; \nFeig, M.; Fischer, S.; Gao, J.; Hodoscek, M.; Im, W.; Kuczera, K.; Lazaridis, T.; Ma, J.; \nOvchinnikov, V.; Paci, E.; Pastor, R. W.; Post, C. B.; Pu, J. Z.; Schaefer, M.; Tidor, B.; Venable, \nR. M.; Woodcock, H. L.; Wu, X.; Yang, W.; York, D. M.; Karplus, M. J. Comput. Chem. 2009, \n30, 1545. \n(43) \nCase, D. A.; Cheatham, T. E.; Darden, T.; Gohlke, H.; Luo, R.; Merz, K. M.; Onufriev, \nA.; Simmerling, C.; Wang, B.; Woods, R. J. J. Comput. Chem. 2005, 26, 1668. \n(44) \nPhillips, J. C.; Braun, R.; Wang, W.; Gumbart, J.; Tajkhorshid, E.; Villa, E.; Chipot, C.; \nSkeel, R. D.; Kale, L.; Schulten, K. J. Comput. Chem. 2005, 26, 1781. \n(45) \nHess, B.; Kutzner, C.; van der Spoel, D.; Lindahl, E. J. Chem. Theory Comput. 2008, 4, \n435. \n(46) \nValiev, M.; Bylaska, E. J.; Govind, N.; Kowalski, K.; Straatsma, T. P.; Van Dam, H. J. J.; \nWang, D.; Nieplocha, J.; Apra, E.; Windus, T. L.; de Jong, W. Comput. Phys. Comm. 2010, 181, \n1477. \n(47) \nShao, Y.; Molnar, L. F.; Jung, Y.; Kussmann, J.; Ochsenfeld, C.; Brown, S. T.; Gilbert, \nA. T. B.; Slipchenko, L. V.; Levchenko, S. V.; O'Neill, D. P.; DiStasio, R. A.; Lochan, R. C.; \nWang, T.; Beran, G. J. O.; Besley, N. A.; Herbert, J. M.; Lin, C. Y.; Van Voorhis, T.; Chien, S. \nH.; Sodt, A.; Steele, R. P.; Rassolov, V. A.; Maslen, P. E.; Korambath, P. P.; Adamson, R. D.; \nAustin, B.; Baker, J.; Byrd, E. F. C.; Dachsel, H.; Doerksen, R. J.; Dreuw, A.; Dunietz, B. D.; \nDutoi, A. D.; Furlani, T. R.; Gwaltney, S. R.; Heyden, A.; Hirata, S.; Hsu, C. P.; Kedziora, G.; \nKhalliulin, R. Z.; Klunzinger, P.; Lee, A. M.; Lee, M. S.; Liang, W.; Lotan, I.; Nair, N.; Peters, \nB.; Proynov, E. I.; Pieniazek, P. A.; Rhee, Y. M.; Ritchie, J.; Rosta, E.; Sherrill, C. D.; \nSimmonett, A. C.; Subotnik, J. E.; Woodcock, H. L.; Zhang, W.; Bell, A. T.; Chakraborty, A. K.; \nChipman, D. M.; Keil, F. J.; Warshel, A.; Hehre, W. J.; Schaefer, H. F.; Kong, J.; Krylov, A. I.; \nGill, P. M. W.; Head-Gordon, M. Phys. Chem. Chem. Phys. 2006, 8, 3172. \n47 \n \n(48) \nSchmidt, M. W.; Baldridge, K. K.; Boatz, J. A.; Elbert, S. T.; Gordon, M. S.; Jensen, J. \nH.; Koseki, S.; Matsunaga, N.; Nguyen, K. A.; Su, S. J.; Windus, T. L.; Dupuis, M.; \nMontgomery, J. A. J. Comput. Chem. 1993, 14, 1347. \n(49) \nCollobert, R.; Kavukcuoglu, K.; Farabet, C. Proc. of the NIPS BigLearn Workshop 2011. \n(50) \nRami Al-Rfou, et. al. arXiv:1605.02688 2016. \n(51) \nYangqing Jia, E. S., Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, \nSergio Guadarrama, Trevor Darrell arXiv:1408.5093 2014. \n(52) \nMartín Abadi, et. al. arXiv:1605.08695 2016. \n(53) \nSoon, W. W.; Hariharan, M.; Snyder, M. P. Mol. Syst. Biol. 2013, 9, 640. \n(54) \nGPU computing power data points were obtained from the reported double-precision \ncomputing power of NVIDIA Tesla Series GPUs for the flagship model released each year: \nC2070 (2010), M2090 (2011), K20 (2012), K40 (2013), K80 (2014), P100 (2015). \n(55) \nAjay; Walters, W. P.; Murcko, M. A. J. Med. Chem. 1998, 41, 3314. \n(56) \nBurden, F. R.; Winkler, D. A. J. Med. Chem. 1999, 42, 3183. \n(57) \nBurden, F. R.; Ford, M. G.; Whitley, D. C.; Winkler, D. A. J. Chem. Inf. Comput. Sci. \n2000, 40, 1423. \n(58) \nKarelson, M.; Lobanov, V. S.; Katritzky, A. R. Chem. Rev. 1996, 96, 1027. \n(59) \nGramatica, P. QSAR Comb. Sci. 2007, 26, 694. \n(60) \nVerma, J.; Khedkar, V. M.; Coutinho, E. C. Curr. Top. Med. Chem. 2010, 10, 95. \n(61) \nTropsha, A. Mol. Inf. 2010, 29, 476. \n(62) \nKaggle http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-\ninterview/. \n(63) \nDahl, G. E.; Jaitly, N.; Salakhutdinov, R. arXiv:1406.1231 2014. \n(64) \nMauri, A.; Consonni, V.; Pavan, M.; Todeschini, R. Match-Commun. Math. Co. 2006, \n56, 237. \n(65) \nWinkler, D. A. J. Phys. Chem. Lett. 2002, 3, 73. \n(66) \nMa, J.; Sheridan, R. P.; Liaw, A.; Dahl, G. E.; Svetnik, V. J. Chem. Inf. Model. 2015, 55, \n263. \n(67) \nLowe, D. \nhttp://blogs.sciencemag.org/pipeline/archives/2012/12/11/did_kaggle_predict_drug_candidate_\nactivities_or_not. \n(68) \nUnterthiner, T.; Mayr, A.; Klambauer, G.; Steijaert, M.; Ceulemans, H.; Wegner, J.; \nHochreiter, S. Conference Neural Information Processing Systems Foundation (NIPS 2014) \n2014. \n(69) \nRogers, D.; Hahn, M. J. Chem. Inf. Model. 2010, 50, 742. \n(70) \nRamsundar, B.; Kearnes, S.; Riley, P.; Webster, D.; Konerding, D.; Pande, V. \narXiv:1502.02072 2015. \n(71) \nWang, Y.; Xiao, J.; Suzek, T. O.; Zhang, J.; Wang, J.; Zhou, Z.; Han, L.; Karapetyan, K.; \nDracheva, S.; Shoemaker, B. A.; Bolton, E.; Gindulyte, A.; Bryant, S. H. Nucleic Acids Res \n2012, 40, D400. \n(72) \nRohrer, S. G.; Baumann, K. J. Chem. Inf. Model. 2009, 49, 169. \n(73) \nMysinger, M. M.; Carchia, M.; Irwin, J. J.; Shoichet, B. K. J. Med. Chem. 2012, 55, \n6582. \n(74) \nHuang, R.; Sakamuru, S.; Martin, M. T.; Reif, D. M.; Judson, R. S.; Houck, K. A.; Casey, \nW.; Hsieh, J. H.; Shockley, K. R.; Ceger, P.; Fostel, J.; Witt, K. L.; Tong, W.; Rotroff, D. M.; \n48 \n \nZhao, T.; Shinn, P.; Simeonov, A.; Dix, D. J.; Austin, C. P.; Kavlock, R. J.; Tice, R. R.; Xia, M. \nSci. Rep. 2014, 4, 5664. \n(75) \nAssis, D. N.; Navarro, V. J. Expert Opin. Drug. Metab. Toxicol. 2009, 5, 463. \n(76) \nXu, Y.; Dai, Z.; Chen, F.; Gao, S.; Pei, J.; Lai, L. J. Chem. Inf. Model. 2015, 55, 2085. \n(77) \nHong, H.; Xie, Q.; Ge, W.; Qian, F.; Fang, H.; Shi, L.; Su, Z.; Perkins, R.; Tong, W. J. \nChem. Inf. Model. 2008, 48, 1337. \n(78) \nYap, C. W. J. Comput. Chem. 2011, 32, 1466. \n(79) \nLusci, A.; Pollastri, G.; Baldi, P. J. Chem. Inf. Model. 2013, 53, 1563. \n(80) \nHughes, T. B.; Miller, G. P.; Swamidass, S. J. ACS Cent. Sci. 2015, 1, 168. \n(81) \nZaretzki, J.; Matlock, M.; Swamidass, S. J. J. Chem. Inf. Model. 2013, 53, 3373. \n(82) \nZaretzki, J.; Boehm, K. M.; Swamidass, S. J. J. Chem. Inf. Model. 2015, 55, 972. \n(83) \nDang, N. L.; Hughes, T. B.; Krishnamurthy, V.; Swamidass, S. J. Bioinformatics 2016, \n32, 3183. \n(84) \nHughes, T. B.; Miller, G. P.; Swamidass, S. J. Chem. Res. Toxicol. 2015, 28, 797. \n(85) \nHughes, T. B.; Dang, N. L.; Miller, G. P.; Swamidass, S. J. ACS Cent. Sci. 2016, 2, 529. \n(86) \nMayr, A.; Klambauer, G.; Unterthiner, T.; Hochreiter, S. Front. Env. Sci. 2016, 3, 1. \n(87) \nKearnes, S.; Goldman, B.; Pande, V. arXiv:1606.08793 2016. \n(88) \nWallach, I.; Dzamba, M.; Heifets, A. arXiv:1510.02855 2016. \n(89) \nKoes, D. R.; Baumgartner, M. P.; Camacho, C. J. J. Chem. Inf. Model. 2013, 53, 1893. \n(90) \nTrott, O.; Olson, A. J. J. Comput. Chem. 2010, 31, 455. \n(91) \nMamoshina, P.; Vieira, A.; Putin, E.; Zhavoronkov, A. Mol. Pharm. 2016, 13, 1445. \n(92) \nShaw, D. E.; Dror, R. O.; Salmon, J. K.; Grossman, J. P.; Mackenzie, K. M.; Bank, J. A.; \nYoung, C.; Deneroff, M. M.; Batson, B.; Bowers, K. J.; Chow, E.; Eastwood, M. P.; Ierardi, D. \nJ.; Klepeis, J. L.; Kuskin, J. S.; Larson, R. H.; Lindorff-Larsen, K.; Maragakis, P.; Moraes, M. \nA.; Piana, S.; Shan, Y. B.; Towles, B. Proc. of the Conf. on High Perf. Computing Networking, \nStorage and Analysis 2009. \n(93) \nLindorff-Larsen, K.; Piana, S.; Dror, R. O.; Shaw, D. E. Science 2011, 334, 517. \n(94) \nOnuchic, J. N.; Luthey-Schulten, Z.; Wolynes, P. G. Annu. Rev. Phys. Chem. 1997, 48, \n545. \n(95) \nWolynes, P. G. Philos. T. Roy. Soc. A 2005, 363, 453. \n(96) \nPunta, M.; Rost, B. Bioinformatics 2005, 21, 2960. \n(97) \nShackelford, G.; Karplus, K. Proteins 2007, 69, 159. \n(98) \nFariselli, P.; Olmea, O.; Valencia, A.; Casadio, R. Proteins 2001, 157. \n(99) \nBjorkholm, P.; Daniluk, P.; Kryshtafovych, A.; Fidelis, K.; Andersson, R.; Hvidsten, T. \nR. Bioinformatics 2009, 25, 1264. \n(100) Misura, K. M. S.; Chivian, D.; Rohl, C. A.; Kim, D. E.; Baker, D. Proc. Natl. Acad. Sci. \nUSA 2006, 103, 5361. \n(101) Skolnick, J.; Kihara, D.; Zhang, Y. Proteins 2004, 56, 502. \n(102) Shen, M. Y.; Sali, A. Protein Sci. 2006, 15, 2507. \n(103) Moult, J. Curr. Opin. Struct. Biol. 2005, 15, 285. \n(104) Zhang, Y. Curr. Opin. Struct. Biol. 2008, 18, 342. \n(105) Marks, D. S.; Hopf, T. A.; Sander, C. Nat. Biotechnol. 2012, 30, 1072. \n(106) Di Lena, P.; Nagata, K.; Baldi, P. Bioinformatics 2012, 28, 2449. \n(107) Chandonia, J. M.; Hon, G.; Walker, N. S.; Lo Conte, L.; Koehl, P.; Levitt, M.; Brenner, \nS. E. Nucleic Acids Res 2004, 32, D189. \n(108) Eickholt, J.; Cheng, J. Bioinformatics 2012, 28, 3066. \n49 \n \n(109) Monastyrskyy, B.; Fidelis, K.; Tramontano, A.; Kryshtafovych, A. Proteins 2011, 79, \n119. \n(110) Lyons, J.; Dehzangi, A.; Heffernan, R.; Sharma, A.; Paliwal, K.; Sattar, A.; Zhou, Y.; \nYang, Y. J. Comput. Chem. 2014, 35, 2040. \n(111) Faraggi, E.; Zhang, T.; Yang, Y.; Kurgan, L.; Zhou, Y. J. Comput. Chem. 2012, 33, 259. \n(112) Heffernan, R.; Paliwal, K.; Lyons, J.; Dehzangi, A.; Sharma, A.; Wang, J.; Sattar, A.; \nYang, Y.; Zhou, Y. Sci. Rep. 2015, 5, 11476. \n(113) Faraggi, E.; Yang, Y. D.; Zhang, S. S.; Zhou, Y. Q. Structure 2009, 17, 1515. \n(114) Wang, G. L.; Dunbrack, R. L. Nucleic Acids Res 2005, 33, W94. \n(115) Altschul, S. F.; Madden, T. L.; Schaffer, A. A.; Zhang, J. H.; Zhang, Z.; Miller, W.; \nLipman, D. J. Nucleic Acids Res 1997, 25, 3389. \n(116) Altschul, S.; Madden, T.; Schaffer, A.; Zhang, J. H.; Zhang, Z.; Miller, W.; Lipman, D. \nFASEB J. 1998, 12, A1326. \n(117) Meiler, J.; Muller, M.; Zeidler, A.; Schmaschke, F. J. Mol. Model. 2001, 7, 360. \n(118) Rost, B. J. Struct. Biol. 2001, 134, 204. \n(119) Alipanahi, B.; Delong, A.; Weirauch, M. T.; Frey, B. J. Nat. Biotechnol. 2015, 33, 831. \n(120) Zhang, S.; Zhou, J.; Hu, H.; Gong, H.; Chen, L.; Cheng, C.; Zeng, J. Nucleic Acids Res \n2016, 44, e32. \n(121) Weirauch, M. T.; Cote, A.; Norel, R.; Annala, M.; Zhao, Y.; Riley, T. R.; Saez-\nRodriguez, J.; Cokelaer, T.; Vedenko, A.; Talukder, S.; Bussemaker, H. J.; Morris, Q. D.; Bulyk, \nM. L.; Stolovitzky, G.; Hughes, T. R.; Consortium, D. Nat. Biotechnol. 2013, 31, 126. \n(122) Shen, Y.; Lange, O.; Delaglio, F.; Rossi, P.; Aramini, J. M.; Liu, G. H.; Eletsky, A.; Wu, \nY. B.; Singarapu, K. K.; Lemak, A.; Ignatchenko, A.; Arrowsmith, C. H.; Szyperski, T.; \nMontelione, G. T.; Baker, D.; Bax, A. Proc. Natl. Acad. Sci. USA 2008, 105, 4685. \n(123) Hansen, K.; Montavon, G.; Biegler, F.; Fazli, S.; Rupp, M.; Scheffler, M.; von Lilienfeld, \nO. A.; Tkatchenko, A.; Muller, K. R. J. Chem. Theory Comput. 2013, 9, 3404. \n(124) Montavon, G.; Rupp, M.; Gobre, V.; Vazquez-Mayagoitia, A.; Hansen, K.; Tkatchenko, \nA.; Müller, K.-R.; Anatole von Lilienfeld, O. New J. Phys. 2013, 15, 095003. \n(125) Ramakrishnan, R.; Dral, P. O.; Rupp, M.; von Lilienfeld, O. A. J. Chem. Theory Comput. \n2015, 11, 2087. \n(126) Lopez-Bezanilla, A.; von Lilienfeld, O. A. Phys. Rev. B 2014, 89, 235411  \n(127) Dral, P. O.; von Lilienfeld, O. A.; Thiel, W. J. Chem. Theory Comput. 2015, 11, 2120. \n(128) Faber, F.; Lindmaa, A.; von Lilienfeld, O. A.; Armiento, R. Int. J. Quantum Chem. 2015, \n115, 1094. \n(129) von Lilienfeld, O. A.; Ramakrishnan, R.; Rupp, M.; Knoll, A. Int. J. Quantum Chem. \n2015, 115, 1084. \n(130) Katritzky, A. R.; Lobanov, V. S.; Karelson, M. Chem. Soc. Rev. 1995, 24, 279. \n(131) Le, T.; Epa, V. C.; Burden, F. R.; Winkler, D. A. Chem. Rev. 2012, 112, 2889. \n(132) Kalinin, S. V.; Sumpter, B. G.; Archibald, R. K. Nat. Mater. 2015, 14, 973. \n(133) Breneman, C. M.; Brinson, L. C.; Schadler, L. S.; Natarajan, B.; Krein, M.; Wu, K.; \nMorkowchuk, L.; Li, Y.; Deng, H.; Xu, H. Adv. Funct. Mater. 2013, 23, 5746. \n(134) Jain, A.; Ong, S. P.; Hautier, G.; Chen, W.; Richards, W. D.; Dacek, S.; Cholia, S.; \nGunter, D.; Skinner, D.; Ceder, G.; Persson, K. A. APL Mater. 2013, 1. \n(135) Pyzer-Knapp, E. O.; Li, K.; Aspuru-Guzik, A. Adv. Funct. Mater. 2015, 25, 6495. \n(136) Venkatraman, V.; Abburu, S.; Alsberg, B. K. Phys. Chem. Chem. Phys. 2015, 17, 27672. \n(137) Schuller, A.; Goh, G. B.; Kim, H.; Lee, J. S.; Chang, Y. T. Mol. Inf. 2010, 29, 717. \n50 \n \n(138) Deetlefs, M.; Seddon, K. R.; Shara, M. Phys. Chem. Chem. Phys. 2006, 8, 642. \n(139) Fourches, D.; Pu, D. Q. Y.; Tassa, C.; Weissleder, R.; Shaw, S. Y.; Mumper, R. J.; \nTropsha, A. ACS Nano 2010, 4, 5703. \n(140) Kaggle http://www.kaggle.com/c/MerckActivity/leaderboard. \n(141) Barta, G. Front. Env. Sci. 2016, 4, 1. \n(142) Abdelaziz, A.; Spahn-Langguth, H.; Schramm, K.-W.; Tetko, I. V. Front. Env. Sci. 2016, \n4, 1. \n(143) Huang, J.; MacKerell, A. D. J. Comput. Chem. 2013, 34, 2135. \n(144) Beauchamp, K. A.; Lin, Y. S.; Das, R.; Pande, V. S. J. Chem. Theory Comput. 2012, 8, \n1409. \n(145) Goh, G. B.; Eike, D. M.; Murch, B. P.; Brooks, C. L. J. Phys. Chem. B. 2015, 119, 6217. \n(146) Gee, M. B.; Cox, N. R.; Jiao, Y. F.; Bentenitis, N.; Weerasinghe, S.; Smith, P. E. J. \nChem. Theory Comput. 2011, 7, 1369. \n(147) Luo, Y.; Roux, B. J. Phys. Chem. Lett. 2010, 1, 183. \n(148) Ribeiro, M. T.; Singh, S.; Guestrin, C. arXiv:1602.04938 2016. \n(149) Ribeiro, M. T.; Singh, S.; Guestrin, C. arxiv:1606.05386 2016. \n(150) Lusci, A.; Fooshee, D.; Browning, M.; Swamidass, J.; Baldi, P. J. Cheminform. 2015, 7. \n(151) Swamidass, S. J.; Azencott, C. A.; Lin, T. W.; Gramajo, H.; Tsai, S. C.; Baldi, P. J. \nChem. Inf. Model. 2009, 49, 756. \n \n",
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.CE",
    "cs.LG",
    "physics.chem-ph"
  ],
  "published": "2017-01-17",
  "updated": "2017-01-17"
}