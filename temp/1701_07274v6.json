{
  "id": "http://arxiv.org/abs/1701.07274v6",
  "title": "Deep Reinforcement Learning: An Overview",
  "authors": [
    "Yuxi Li"
  ],
  "abstract": "We give an overview of recent exciting achievements of deep reinforcement\nlearning (RL). We discuss six core elements, six important mechanisms, and\ntwelve applications. We start with background of machine learning, deep\nlearning and reinforcement learning. Next we discuss core RL elements,\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\nmodel, planning, and exploration. After that, we discuss important mechanisms\nfor RL, including attention and memory, unsupervised learning, transfer\nlearning, multi-agent RL, hierarchical RL, and learning to learn. Then we\ndiscuss various applications of RL, including games, in particular, AlphaGo,\nrobotics, natural language processing, including dialogue systems, machine\ntranslation, and text generation, computer vision, neural architecture design,\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\ntransportation systems, and computer systems. We mention topics not reviewed\nyet, and list a collection of RL resources. After presenting a brief summary,\nwe close with discussions.\n  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant\nupdate.",
  "text": "DEEP REINFORCEMENT LEARNING: AN OVERVIEW\nYuxi Li (yuxili@gmail.com)\nABSTRACT\nWe give an overview of recent exciting achievements of deep reinforcement learn-\ning (RL). We discuss six core elements, six important mechanisms, and twelve\napplications. We start with background of machine learning, deep learning and\nreinforcement learning. Next we discuss core RL elements, including value func-\ntion, in particular, Deep Q-Network (DQN), policy, reward, model and planning,\nexploration, and knowledge. After that, we discuss important mechanisms for RL,\nincluding attention and memory, unsupervised learning, transfer learning, multi-\nagent RL, hierarchical RL, and learning to learn. Then we discuss various appli-\ncations of RL, including games, in particular, AlphaGo, robotics, natural language\nprocessing, including dialogue systems, machine translation, and text generation,\ncomputer vision, business management, ﬁnance, healthcare, education, Industry\n4.0, smart grid, intelligent transportation systems, and computer systems. We\nmention topics not reviewed yet, and list a collection of RL resources. After pre-\nsenting a brief summary, we close with discussions.\nThis is the ﬁrst overview about deep reinforcement learning publicly available on-\nline. It is comprehensive. Comments and criticisms are welcome. (This particular\nversion is incomplete.)\nPlease see Deep Reinforcement Learning, https://arxiv.org/abs/\n1810.06339, for a signiﬁcant update to this manuscript.\n1\narXiv:1701.07274v6  [cs.LG]  26 Nov 2018\nCONTENTS\n1\nIntroduction\n5\n2\nBackground\n7\n2.1\nMachine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nDeep Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.3\nReinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3.1\nProblem Setup\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3.2\nExploration vs Exploitation\n. . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3.3\nValue Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3.4\nDynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3.5\nTemporal Difference Learning . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3.6\nMulti-step Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3.7\nFunction Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3.8\nPolicy Optimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.3.9\nDeep Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.3.10 RL Parlance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.3.11 Brief Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3\nCore Elements\n15\n3.1\nValue Function\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.1.1\nDeep Q-Network (DQN) And Extensions . . . . . . . . . . . . . . . . . .\n15\n3.2\nPolicy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.2.1\nActor-Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.2.2\nPolicy Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.2.3\nCombining Policy Gradient with Off-Policy RL . . . . . . . . . . . . . . .\n20\n3.3\nReward\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.4\nModel and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.5\nExploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.6\nKnowledge\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4\nImportant Mechanisms\n23\n4.1\nAttention and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.2\nUnsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.2.1\nHorde . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.2.2\nUnsupervised Auxiliary Learning\n. . . . . . . . . . . . . . . . . . . . . .\n24\n4.2.3\nGenerative Adversarial Networks\n. . . . . . . . . . . . . . . . . . . . . .\n25\n4.3\nTransfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.4\nMulti-Agent Reinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . . .\n26\n2\n4.5\nHierarchical Reinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.6\nLearning to Learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.6.1\nLearning to Learn/Optimize . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.6.2\nZero/One/Few-Shot Learning\n. . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.6.3\nNeural Architecture Design\n. . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5\nApplications\n28\n5.1\nGames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.1.1\nPerfect Information Board Games . . . . . . . . . . . . . . . . . . . . . .\n30\n5.1.2\nImperfect Information Board Games . . . . . . . . . . . . . . . . . . . . .\n33\n5.1.3\nVideo Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.2\nRobotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.2.1\nGuided Policy Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.2.2\nLearn to Navigate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.3\nNatural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.3.1\nDialogue Systems\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.3.2\nMachine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.3.3\nText Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.4\nComputer Vision\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.4.1\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.4.2\nRecognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n5.4.3\nMotion Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n5.4.4\nScene Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n5.4.5\nIntegration with NLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.4.6\nVisual Control\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.5\nBusiness Management\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.6\nFinance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.7\nHealthcare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.8\nEducation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.9\nIndustry 4.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.10 Smart Grid\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.11 Intelligent Transportation Systems . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.12 Computer Systems\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.12.1 Resource Allocation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5.12.2 Performance Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5.12.3 Security & Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n6\nMore Topics\n44\n3\n7\nResources\n45\n7.1\nBooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n7.2\nMore Books . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n7.3\nSurveys and Reports\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n7.4\nCourses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n7.5\nTutorials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n7.6\nConferences, Journals and Workshops . . . . . . . . . . . . . . . . . . . . . . . .\n47\n7.7\nBlogs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n7.8\nTestbeds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n7.9\nAlgorithm Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n8\nBrief Summary\n50\n9\nDiscussions\n52\n4\n1\nINTRODUCTION\nReinforcement learning (RL) is about an agent interacting with the environment, learning an optimal\npolicy, by trial and error, for sequential decision making problems in a wide range of ﬁelds in both\nnatural and social sciences, and engineering (Sutton and Barto, 1998; 2018; Bertsekas and Tsitsiklis,\n1996; Bertsekas, 2012; Szepesv´ari, 2010; Powell, 2011).\nThe integration of reinforcement learning and neural networks has a long history (Sutton and Barto,\n2018; Bertsekas and Tsitsiklis, 1996; Schmidhuber, 2015). With recent exciting achievements of\ndeep learning (LeCun et al., 2015; Goodfellow et al., 2016), beneﬁting from big data, powerful\ncomputation, new algorithmic techniques, mature software packages and architectures, and strong\nﬁnancial support, we have been witnessing the renaissance of reinforcement learning (Krakovsky,\n2016), especially, the combination of deep neural networks and reinforcement learning, i.e., deep\nreinforcement learning (deep RL).\nDeep learning, or deep neural networks, has been prevailing in reinforcement learning in the last\nseveral years, in games, robotics, natural language processing, etc. We have been witnessing break-\nthroughs, like deep Q-network (Mnih et al., 2015) and AlphaGo (Silver et al., 2016a); and novel ar-\nchitectures and applications, like differentiable neural computer (Graves et al., 2016), asynchronous\nmethods (Mnih et al., 2016), dueling network architectures (Wang et al., 2016b), value iteration\nnetworks (Tamar et al., 2016), unsupervised reinforcement and auxiliary learning (Jaderberg et al.,\n2017; Mirowski et al., 2017), neural architecture design (Zoph and Le, 2017), dual learning for\nmachine translation (He et al., 2016a), spoken dialogue systems (Su et al., 2016b), information\nextraction (Narasimhan et al., 2016), guided policy search (Levine et al., 2016a), and generative ad-\nversarial imitation learning (Ho and Ermon, 2016), etc. Creativity would push the frontiers of deep\nRL further with respect to core elements, mechanisms, and applications.\nWhy has deep learning been helping reinforcement learning make so many and so enormous achieve-\nments? Representation learning with deep learning enables automatic feature engineering and end-\nto-end learning through gradient descent, so that reliance on domain knowledge is signiﬁcantly\nreduced or even removed. Feature engineering used to be done manually and is usually time-\nconsuming, over-speciﬁed, and incomplete. Deep, distributed representations exploit the hierar-\nchical composition of factors in data to combat the exponential challenges of the curse of dimen-\nsionality. Generality, expressiveness and ﬂexibility of deep neural networks make some tasks easier\nor possible, e.g., in the breakthroughs and novel architectures and applications discussed above.\nDeep learning, as a speciﬁc class of machine learning, is not without limitations, e.g., as a black-box\nlacking interpretability, as an ”alchemy” without clear and sufﬁcient scientiﬁc principles to work\nwith, and without human intelligence not able to competing with a baby in some tasks. However,\nthere are lots of works to improve deep learning, machine learning, and AI in general.\nDeep learning and reinforcement learning, being selected as one of the MIT Technology Review 10\nBreakthrough Technologies in 2013 and 2017 respectively, will play their crucial role in achieving\nartiﬁcial general intelligence. David Silver, the major contributor of AlphaGo (Silver et al., 2016a;\n2017), even made a formula: artiﬁcial intelligence = reinforcement learning + deep learning (Silver,\n2016).\nThe outline of this overview follows. First we discuss background of machine learning, deep learn-\ning and reinforcement learning in Section 2. Next we discuss core RL elements, including value\nfunction in Section 3.1, policy in Section 3.2, reward in Section 3.3, model and planning in Sec-\ntion 3.4, exploration in Section 3.5, and knowledge in Section 3.6. Then we discuss important mech-\nanisms for RL, including attention and memory in Section 4.1, unsupervised learning in Section 4.2,\ntransfer learning in Section 4.3, multi-agent RL in Section 4.4, hierarchical RL in Section 4.5, and,\nlearning to learn in Section 4.6. After that, we discuss various RL applications, including games in\nSection 5.1, robotics in Section 5.2, natural language processing in Section 5.3, computer vision in\nSection 5.4, business management in Section 5.5, ﬁnance in Section 5.6, healthcare in Section 5.7,\neducation in Section 5.8, Industry 4.0 in Section 5.9, smart grid in Section 5.10, intelligent trans-\nportation systems in Section 5.11, and computer systems in Section 5.12. We present a list of topics\nnot reviewed yet in Section 6, give a brief summary in Section 8, and close with discussions in\nSection 9.\n5\nFigure 1: Conceptual Organization of the Overview\nIn Section 7, we list a collection of RL resources including books, surveys, reports, online courses,\ntutorials, conferences, journals and workshops, blogs, and open sources. If picking a single RL\nresource, it is Sutton and Barto’s RL book (Sutton and Barto, 2018), 2nd edition in preparation. It\ncovers RL fundamentals and reﬂects new progress, e.g., in deep Q-network, AlphaGo, policy gra-\ndient methods, as well as in psychology and neuroscience. Deng and Dong (2014) and Goodfellow\net al. (2016) are recent deep learning books. Bishop (2011), Hastie et al. (2009), and Murphy (2012)\nare popular machine learning textbooks; James et al. (2013) gives an introduction to machine learn-\ning; Provost and Fawcett (2013) and Kuhn and Johnson (2013) discuss practical issues in machine\nlearning applications; and Simeone (2017) is a brief introduction to machine learning for engineers.\nFigure 1 illustrates the conceptual organization of the overview. The agent-environment interac-\ntion sits in the center, around which are core elements: value function, policy, reward, model and\nplanning, exploration, and knowledge. Next come important mechanisms: attention and memory,\nunsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn.\nThen come various applications: games, robotics, NLP (natural language processing), computer vi-\nsion, business management, ﬁnance, healthcare, education, Industry 4.0, smart grid, ITS (intelligent\ntransportation systems), and computer systems.\nThe main readers of this overview would be those who want to get more familiar with deep re-\ninforcement learning. We endeavour to provide as much relevant information as possible. For\nreinforcement learning experts, as well as new comers, we hope this overview would be helpful as\na reference. In this overview, we mainly focus on contemporary work in recent couple of years, by\nno means complete, and make slight effort for discussions of historical context, for which the best\nmaterial to consult is Sutton and Barto (2018).\nIn this version, we endeavour to provide a wide coverage of fundamental and contemporary RL\nissues, about core elements, important mechanisms, and applications. In the future, besides further\nreﬁnements for the width, we will also improve the depth by conducting deeper analysis of the issues\ninvolved and the papers discussed. Comments and criticisms are welcome.\n6\n2\nBACKGROUND\nIn this section, we brieﬂy introduce concepts and fundamentals in machine learning, deep learn-\ning (Goodfellow et al., 2016) and reinforcement learning (Sutton and Barto, 2018). We do not give\ndetailed background introduction for machine learning and deep learning. Instead, we recommend\nthe following recent Nature/Science survey papers: Jordan and Mitchell (2015) for machine learn-\ning, and LeCun et al. (2015) for deep learning. We cover some RL basics. However, we recommend\nthe textbook, Sutton and Barto (2018), and the recent Nature survey paper, Littman (2015), for\nreinforcement learning. We also collect relevant resources in Section 7.\n2.1\nMACHINE LEARNING\nMachine learning is about learning from data and making predictions and/or decisions.\nUsually we categorize machine learning as supervised, unsupervised, and reinforcement learning.1\nIn supervised learning, there are labeled data; in unsupervised learning, there are no labeled data; and\nin reinforcement learning, there are evaluative feedbacks, but no supervised signals. Classiﬁcation\nand regression are two types of supervised learning problems, with categorical and numerical outputs\nrespectively.\nUnsupervised learning attempts to extract information from data without labels, e.g., clustering and\ndensity estimation. Representation learning is a classical type of unsupervised learning. However,\ntraining feedforward networks or convolutional neural networks with supervised learning is a kind of\nrepresentation learning. Representation learning ﬁnds a representation to preserve as much informa-\ntion about the original data as possible, at the same time, to keep the representation simpler or more\naccessible than the original data, with low-dimensional, sparse, and independent representations.\nDeep learning, or deep neural networks, is a particular machine learning scheme, usually for su-\npervised or unsupervised learning, and can be integrated with reinforcement learning, usually as a\nfunction approximator. Supervised and unsupervised learning are usually one-shot, myopic, consid-\nering instant reward; while reinforcement learning is sequential, far-sighted, considering long-term\naccumulative reward.\nMachine learning is based on probability theory and statistics (Hastie et al., 2009) and optimiza-\ntion (Boyd and Vandenberghe, 2004), is the basis for big data, data science (Blei and Smyth, 2017;\nProvost and Fawcett, 2013), predictive modeling (Kuhn and Johnson, 2013), data mining, informa-\ntion retrieval (Manning et al., 2008), etc, and becomes a critical ingredient for computer vision, nat-\nural language processing, robotics, etc. Reinforcement learning is kin to optimal control (Bertsekas,\n2012), and operations research and management (Powell, 2011), and is also related to psychology\nand neuroscience (Sutton and Barto, 2018). Machine learning is a subset of artiﬁcial intelligence\n(AI), and is evolving to be critical for all ﬁelds of AI.\nA machine learning algorithm is composed of a dataset, a cost/loss function, an optimization pro-\ncedure, and a model (Goodfellow et al., 2016). A dataset is divided into non-overlapping training,\nvalidation, and testing subsets. A cost/loss function measures the model performance, e.g., with\nrespect to accuracy, like mean square error in regression and classiﬁcation error rate. Training error\nmeasures the error on the training data, minimizing which is an optimization problem. Generaliza-\ntion error, or test error, measures the error on new input data, which differentiates machine learning\nfrom optimization. A machine learning algorithm tries to make the training error, and the gap be-\ntween training error and testing error small. A model is under-ﬁtting if it can not achieve a low\ntraining error; a model is over-ﬁtting if the gap between training error and test error is large.\nA model’s capacity measures the range of functions it can ﬁt. VC dimension measures the capacity\nof a binary classiﬁer. Occam’s Razor states that, with the same expressiveness, simple models are\npreferred. Training error and generalization error versus model capacity usually form a U-shape\nrelationship. We ﬁnd the optimal capacity to achieve low training error and small gap between train-\ning error and generalization error. Bias measures the expected deviation of the estimator from the\ntrue value; while variance measures the deviation of the estimator from the expected value, or vari-\nance of the estimator. As model capacity increases, bias tends to decrease, while variance tends to\n1Is reinforcement learning part of machine learning, or more than it, and somewhere close to artiﬁcial\nintelligence? We raise this question without elaboration.\n7\nincrease, yielding another U-shape relationship between generalization error versus model capacity.\nWe try to ﬁnd the optimal capacity point, of which under-ﬁtting occurs on the left and over-ﬁtting\noccurs on the right. Regularization add a penalty term to the cost function, to reduce the general-\nization error, but not training error. No free lunch theorem states that there is no universally best\nmodel, or best regularizor. An implication is that deep learning may not be the best model for some\nproblems. There are model parameters, and hyperparameters for model capacity and regularization.\nCross-validation is used to tune hyperparameters, to strike a balance between bias and variance, and\nto select the optimal model.\nMaximum likelihood estimation (MLE) is a common approach to derive good estimation of param-\neters. For issues like numerical underﬂow, the product in MLE is converted to summation to obtain\nnegative log-likelihood (NLL). MLE is equivalent to minimizing KL divergence, the dissimilarity\nbetween the empirical distribution deﬁned by the training data and the model distribution. Minimiz-\ning KL divergence between two distributions corresponds to minimizing the cross-entropy between\nthe distributions. In short, maximization of likelihood becomes minimization of the negative log-\nlikelihood (NLL), or equivalently, minimization of cross entropy.\nGradient descent is a common approach to solve optimization problems. Stochastic gradient descent\nextends gradient descent by working with a single sample each time, and usually with minibatches.\nImportance sampling is a technique to estimate properties of a particular distribution, by samples\nfrom a different distribution, to lower the variance of the estimation, or when sampling from the\ndistribution of interest is difﬁcult.\nFrequentist statistics estimates a single value, and characterizes variance by conﬁdence interval;\nBayesian statistics considers the distribution of an estimate when making predictions and decisions.\ngenerative vs discriminative\n2.2\nDEEP LEARNING\nDeep learning is in contrast to ”shallow” learning. For many machine learning algorithms, e.g.,\nlinear regression, logistic regression, support vector machines (SVMs), decision trees, and boosting,\nwe have input layer and output layer, and the inputs may be transformed with manual feature en-\ngineering before training. In deep learning, between input and output layers, we have one or more\nhidden layers. At each layer except input layer, we compute the input to each unit, as the weighted\nsum of units from the previous layer; then we usually use nonlinear transformation, or activation\nfunction, such as logistic, tanh, or more popular recently, rectiﬁed linear unit (ReLU), to apply to\nthe input of a unit, to obtain a new representation of the input from previous layer. We have weights\non links between units from layer to layer. After computations ﬂow forward from input to output, at\noutput layer and each hidden layer, we can compute error derivatives backward, and backpropagate\ngradients towards the input layer, so that weights can be updated to optimize some loss function.\nA feedforward deep neural network or multilayer perceptron (MLP) is to map a set of input values\nto output values with a mathematical function formed by composing many simpler functions at\neach layer. A convolutional neural network (CNN) is a feedforward deep neural network, with\nconvolutional layers, pooling layers and fully connected layers. CNNs are designed to process\ndata with multiple arrays, e.g., colour image, language, audio spectrogram, and video, beneﬁt from\nthe properties of such signals: local connections, shared weights, pooling and the use of many\nlayers, and are inspired by simple cells and complex cells in visual neuroscience (LeCun et al.,\n2015). ResNets (He et al., 2016d) are designed to ease the training of very deep neural networks\nby adding shortcut connections to learn residual functions with reference to the layer inputs. A\nrecurrent neural network (RNN) is often used to process sequential inputs like speech and language,\nelement by element, with hidden units to store history of past elements. A RNN can be seen as a\nmultilayer neural network with all layers sharing the same weights, when being unfolded in time of\nforward computation. It is hard for RNN to store information for very long time and the gradient\nmay vanish. Long short term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997) and\ngated recurrent unit (GRU) (Chung et al., 2014) were proposed to address such issues, with gating\nmechanisms to manipulate information through recurrent cells. Gradient backpropagation or its\nvariants can be used for training all deep neural networks mentioned above.\n8\nDropout (Srivastava et al., 2014) is a regularization strategy to train an ensemble of sub-networks\nby removing non-output units randomly from the original network. Batch normalization (Ioffe and\nSzegedy, 2015) performs the normalization for each training mini-batch, to accelerate training by\nreducing internal covariate shift, i.e., the change of parameters of previous layers will change each\nlayer’s inputs distribution.\nDeep neural networks learn representations automatically from raw inputs to recover the compo-\nsitional hierarchies in many natural signals, i.e., higher-level features are composed of lower-level\nones, e.g., in images, the hierarch of objects, parts, motifs, and local combinations of edges. Dis-\ntributed representation is a central idea in deep learning, which implies that many features may\nrepresent each input, and each feature may represent many inputs. The exponential advantages of\ndeep, distributed representations combat the exponential challenges of the curse of dimensionality.\nThe notion of end-to-end training refers to that a learning model uses raw inputs without manual\nfeature engineering to generate outputs, e.g., AlexNet (Krizhevsky et al., 2012) with raw pixels for\nimage classiﬁcation, Seq2Seq (Sutskever et al., 2014) with raw sentences for machine translation,\nand DQN (Mnih et al., 2015) with raw pixels and score to play games.\n2.3\nREINFORCEMENT LEARNING\nWe provide background of reinforcement learning brieﬂy in this section. After setting up the RL\nproblem, we discuss value function, temporal difference learning, function approximation, policy\noptimization, deep RL, RL parlance, and close this section with a brief summary. To have a good\nunderstanding of deep reinforcement learning, it is essential to have a good understanding of rein-\nforcement learning ﬁrst.\n2.3.1\nPROBLEM SETUP\nA RL agent interacts with an environment over time. At each time step t, the agent receives a state\nst in a state space S and selects an action at from an action space A, following a policy π(at|st),\nwhich is the agent’s behavior, i.e., a mapping from state st to actions at, receives a scalar reward\nrt, and transitions to the next state st+1, according to the environment dynamics, or model, for\nreward function R(s, a) and state transition probability P(st+1|st, at) respectively. In an episodic\nproblem, this process continues until the agent reaches a terminal state and then it restarts. The return\nRt = P∞\nk=0 γkrt+k is the discounted, accumulated reward with the discount factor γ ∈(0, 1]. The\nagent aims to maximize the expectation of such long term return from each state. The problem is set\nup in discrete state and action spaces. It is not hard to extend it to continuous spaces.\n2.3.2\nEXPLORATION VS EXPLOITATION\nmulti-arm bandit\nvarious exploration techniques\n2.3.3\nVALUE FUNCTION\nA value function is a prediction of the expected, accumulative, discounted, future reward, measur-\ning how good each state, or state-action pair, is. The state value vπ(s) = E[Rt|st = s] is the\nexpected return for following policy π from state s. vπ(s) decomposes into the Bellman equation:\nvπ(s) = P\na π(a|s) P\ns′,r p(s′, r|s, a)[r +γvπ(s′)]. An optimal state value v∗(s) = maxπ vπ(s) =\nmaxa qπ∗(s, a) is the maximum state value achievable by any policy for state s. v∗(s) decom-\nposes into the Bellman equation: v∗(s) = maxa\nP\ns′,r p(s′, r|s, a)[r + γv∗(s′)]. The action value\nqπ(s, a) = E[Rt|st = s, at = a] is the expected return for selecting action a in state s and then fol-\nlowing policy π. qπ(s, a) decomposes into the Bellman equation: qπ(s, a) = P\ns′,r p(s′, r|s, a)[r +\nγ P\na′ π(a′|s′)qπ(s′, a′)]. An optimal action value function q∗(s, a) = maxπ qπ(s, a) is the maxi-\nmum action value achievable by any policy for state s and action a. q∗(s, a) decomposes into the\nBellman equation: q∗(s, a) = P\ns′,r p(s′, r|s, a)[r + γ maxa′ q∗(s′, a′)]. We denote an optimal\npolicy by π∗.\n9\n2.3.4\nDYNAMIC PROGRAMMING\n2.3.5\nTEMPORAL DIFFERENCE LEARNING\nWhen a RL problem satisﬁes the Markov property, i.e., the future depends only on the current state\nand action, but not on the past, it is formulated as a Markov Decision Process (MDP), deﬁned by\nthe 5-tuple (S, A, P, R, γ). When the system model is available, we use dynamic programming\nmethods: policy evaluation to calculate value/action value function for a policy, value iteration and\npolicy iteration for ﬁnding an optimal policy. When there is no model, we resort to RL methods.\nRL methods also work when the model is available. Additionally, a RL environment can be a multi-\narmed bandit, an MDP, a POMDP, a game, etc.\nTemporal difference (TD) learning is central in RL. TD learning is usually refer to the learning\nmethods for value function evaluation in Sutton (1988). SARSA (Sutton and Barto, 2018) and Q-\nlearning (Watkins and Dayan, 1992) are also regarded as temporal difference learning.\nTD learning (Sutton, 1988) learns value function V (s) directly from experience with TD error,\nwith bootstrapping, in a model-free, online, and fully incremental way. TD learning is a prediction\nproblem. The update rule is V (s) ←V (s) + α[r + γV (s′) −V (s)], where α is a learning rate, and\nr+γV (s′)−V (s) is called TD error. Algorithm 1 presents the pseudo code for tabular TD learning.\nPrecisely, it is tabular TD(0) learning, where ”0” indicates it is based on one-step return.\nBootstrapping, like the TD update rule, estimates state or action value based on subsequent esti-\nmates, is common in RL, like TD learning, Q learning, and actor-critic. Bootstrapping methods are\nusually faster to learn, and enable learning to be online and continual. Bootstrapping methods are\nnot instances of true gradient decent, since the target depends on the weights to be estimated. The\nconcept of semi-gradient descent is then introduced (Sutton and Barto, 2018).\nInput: the policy π to be evaluated\nOutput: value function V\ninitialize V arbitrarily, e.g., to 0 for all states\nfor each episode do\ninitialize state s\nfor each step of episode, state s is not terminal do\na ←action given by π for s\ntake action a, observe r, s′\nV (s) ←V (s) + α[r + γV (s′) −V (s)]\ns ←s′\nend\nend\nAlgorithm 1: TD learning, adapted from Sutton and Barto (2018)\nOutput: action value function Q\ninitialize Q arbitrarily, e.g., to 0 for all states, set action value for terminal states as 0\nfor each episode do\ninitialize state s\nfor each step of episode, state s is not terminal do\na ←action for s derived by Q, e.g., ϵ-greedy\ntake action a, observe r, s′\na′ ←action for s′ derived by Q, e.g., ϵ-greedy\nQ(s, a) ←Q(s, a) + α[r + γQ(s′, a′) −Q(s, a)]\ns ←s′, a ←a′\nend\nend\nAlgorithm 2: SARSA, adapted from Sutton and Barto (2018)\n10\nOutput: action value function Q\ninitialize Q arbitrarily, e.g., to 0 for all states, set action value for terminal states as 0\nfor each episode do\ninitialize state s\nfor each step of episode, state s is not terminal do\na ←action for s derived by Q, e.g., ϵ-greedy\ntake action a, observe r, s′\nQ(s, a) ←Q(s, a) + α[r + γ maxa′ Q(s′, a′) −Q(s, a)]\ns ←s′\nend\nend\nAlgorithm 3: Q learning, adapted from Sutton and Barto (2018)\nSARSA, representing state, action, reward, (next) state, (next) action, is an on-policy control method\nto ﬁnd the optimal policy, with the update rule, Q(s, a) ←Q(s, a) + α[r + γQ(s′, a′) −Q(s, a)].\nAlgorithm 2 presents the pseudo code for tabular SARSA, precisely tabular SARSA(0).\nQ-learning is an off-policy control method to ﬁnd the optimal policy. Q-learning learns action value\nfunction, with the update rule, Q(s, a) ←Q(s, a) + α[r + γ maxa′ Q(s′, a′) −Q(s, a)]. Q learning\nreﬁnes the policy greedily with respect to action values by the max operator. Algorithm 3 presents\nthe pseudo code for Q learning, precisely, tabular Q(0) learning.\nTD-learning, Q-learning and SARSA converge under certain conditions. From an optimal action\nvalue function, we can derive an optimal policy.\n2.3.6\nMULTI-STEP BOOTSTRAPPING\nThe above algorithms are referred to as TD(0) and Q(0), learning with one-step return. We have TD\nlearning and Q learning variants and Monte-Carlo approach with multi-step return in the forward\nview. The eligibility trace from the backward view provides an online, incremental implementation,\nresulting in TD(λ) and Q(λ) algorithms, where λ ∈[0, 1]. TD(1) is the same as the Monte Carlo\napproach.\nEligibility trace is a short-term memory, usually lasting within an episode, assists the learning pro-\ncess, by affecting the weight vector. The weight vector is a long-term memory, lasting the whole\nduration of the system, determines the estimated value. Eligibility trace helps with the issues of\nlong-delayed rewards and non-Markov tasks (Sutton and Barto, 2018).\nTD(λ) uniﬁes one-step TD prediction, TD(0), with Monte Carlo methods, TD(1), using eligibility\ntraces and the decay parameter λ, for prediction algorithms. De Asis et al. (2018) made uniﬁcation\nfor multi-step TD control algorithms.\n2.3.7\nFUNCTION APPROXIMATION\nWe discuss the tabular cases above, where a value function or a policy is stored in a tabular form.\nFunction approximation is a way for generalization when the state and/or action spaces are large or\ncontinuous. Function approximation aims to generalize from examples of a function to construct\nan approximate of the entire function; it is usually a concept in supervised learning, studied in the\nﬁelds of machine learning, patten recognition, and statistical curve ﬁtting; function approximation in\nreinforcement learning usually treats each backup as a training example, and encounters new issues\nlike nonstationarity, bootstrapping, and delayed targets (Sutton and Barto, 2018). Linear function\napproximation is a popular choice, partially due to its desirable theoretical properties, esp. before the\nwork of Deep Q-Network (Mnih et al., 2015). However, the integration of reinforcement learning\nand neural networks dated back a long time ago (Sutton and Barto, 2018; Bertsekas and Tsitsiklis,\n1996; Schmidhuber, 2015).\nAlgorithm 4 presents the pseudo code for TD(0) with function approximation. ˆv(s, w) is the ap-\nproximate value function, w is the value function weight vector, ∇ˆv(s, w) is the gradient of the\napproximate value function with respect to the weight vector, and the weight vector is updated fol-\nlowing the update rule, w ←w + α[r + γˆv(s′, w) −ˆv(s, w)]∇ˆv(s, w).\n11\nInput: the policy π to be evaluated\nInput: a differentiable value function ˆv(s, w), ˆv(terminal, ·) = 0\nOutput: value function ˆv(s, w)\ninitialize value function weight w arbitrarily, e.g., w = 0\nfor each episode do\ninitialize state s\nfor each step of episode, state s is not terminal do\na ←π(·|s)\ntake action a, observe r, s′\nw ←w + α[r + γˆv(s′, w) −ˆv(s, w)]∇ˆv(s, w)\ns ←s′\nend\nend\nAlgorithm 4: TD(0) with function approximation, adapted from Sutton and Barto (2018)\nWhen combining off-policy, function approximation, and bootstrapping, instability and divergence\nmay occur (Tsitsiklis and Van Roy, 1997), which is called the deadly triad issue (Sutton and Barto,\n2018). All these three elements are necessary: function approximation for scalability and gener-\nalization, bootstrapping for computational and data efﬁciency, and off-policy learning for freeing\nbehaviour policy from target policy. What is the root cause for the instability? Learning or sampling\nare not, since dynamic programming suffers from divergence with function approximation; explo-\nration, greediﬁcation, or control are not, since prediction alone can diverge; local minima or complex\nnon-linear function approximation are not, since linear function approximation can produce instabil-\nity (Sutton, 2016). It is unclear what is the root cause for instability – each single factor mentioned\nabove is not – there are still many open problems in off-policy learning (Sutton and Barto, 2018).\nTable 1 presents various algorithms that tackle various issues (Sutton, 2016). Deep RL algorithms\nlike Deep Q-Network (Mnih et al., 2015) and A3C (Mnih et al., 2016) are not presented here, since\nthey do not have theoretical guarantee, although they achieve stunning performance empirically.\nBefore explaining Table 1, we introduce some background deﬁnitions. Recall that Bellman equa-\ntion for value function is vπ(s) = P\na π(a|s) P\ns′,r p(s′, r|s, a)[r + γvπ(s′)]. Bellman operator\nis deﬁned as (Bπv)(s) .= P\na π(a|s) P\ns′,r p(s′, r|s, a)[r + γvπ(s′)]. TD ﬁx point is then vπ =\nBπvπ. Bellman error for the function approximation case is then P\na π(a|s) P\ns′,r p(s′, r|s, a)[r +\nγˆvπ(s′, w)] −ˆvπ(s, w), the right side of Bellman equation with function approximation minus the\nleft side. It can be written as Bπvw −vw. Bellman error is the expectation of the TD error.\nADP algorithms refer to dynamic programming algorithms like policy evaluation, policy iteration,\nand value iteration, with function approximation. Least square temporal difference (LSTD) (Bradtke\nand Barto, 1996) computes TD ﬁx-point directly in batch mode. LSTD is data efﬁcient, yet with\nsquared time complexity.\nLSPE (Nedi´c and Bertsekas, 2003) extended LSTD. Fitted-Q algo-\nrithms (Ernst et al., 2005; Riedmiller, 2005) learn action values in batch mode. Residual gradi-\nent algorithms (Baird, 1995) minimize Bellman error. Gradient-TD (Sutton et al., 2009a;b; Mah-\nmood et al., 2014) methods are true gradient algorithms, perform SGD in the projected Bellman\nerror (PBE), converge robustly under off-policy training and non-linear function approximation.\nEmphatic-TD (Sutton et al., 2016) emphasizes some updates and de-emphasizes others by reweight-\ning, improving computational efﬁciency, yet being a semi-gradient method. See Sutton and Barto\n(2018) for more details. Du et al. (2017) proposed variance reduction techniques for policy eval-\nuation to achieve fast convergence. White and White (2016) performed empirical comparisons of\nlinear TD methods, and made suggestions about their practical use.\n2.3.8\nPOLICY OPTIMIZATION\nIn contrast to value-based methods like TD learning and Q-learning, policy-based methods opti-\nmize the policy π(a|s; θ) (with function approximation) directly, and update the parameters θ by\ngradient ascent on E[Rt]. REINFORCE (Williams, 1992) is a policy gradient method, updating θ\nin the direction of ∇θ log π(at|st; θ)Rt. Usually a baseline bt(st) is subtracted from the return to\nreduce the variance of gradient estimate, yet keeping its unbiasedness, to yield the gradient direction\n∇θ log π(at|st; θ)(Rt −bt(st)). Using V (st) as the baseline bt(st), we have the advantage func-\n12\nalgorithm\nTD(λ)\nSARSA(λ)\nADP\nLSTD(λ)\nLSPE(λ)\nFitted-Q\nResidual\nGradient\nGTD(λ)\nGQ(λ)\nlinear\ncomputation\n✓\n✓\n✓\n✓\nissue\nnonlinear\nconvergent\n✓\n✓\n✓\noff-policy\nconvergent\n✓\n✓\n✓\nmodel-free,\nonline\n✓\n✓\n✓\n✓\nconverges to\nPBE = 0\n✓\n✓\n✓\n✓\n✓\nTable 1: RL Issues vs. Algorithms\ntion A(at, st) = Q(at, st) −V (st), since Rt is an estimate of Q(at, st). Algorithm 5 presents the\npseudo code for REINFORCE algorithm in the episodic case.\nInput: policy π(a|s, θ), ˆv(s, w)\nParameters: step sizes, α > 0, β > 0\nOutput: policy π(a|s, θ)\ninitialize policy parameter θ and state-value weights w\nfor true do\ngenerate an episode s0, a0, r1, · · · , sT −1, aT −1, rT , following π(·|·, θ)\nfor each step t of episode 0, · · · , T −1 do\nGt ←return from step t\nδ ←Gt −ˆv(st, w)\nw ←w + βδ∇wˆv(st, w)\nθ ←θ + αγtδ∇θlogπ(at|st, θ)\nend\nend\nAlgorithm 5: REINFORCE with baseline (episodic), adapted from Sutton and Barto (2018)\nIn actor-critic algorithms, the critic updates action-value function parameters, and the actor updates\npolicy parameters, in the direction suggested by the critic. Algorithm 6 presents the pseudo code for\none-step actor-critic algorithm in the episodic case.\nInput: policy π(a|s, θ), ˆv(s, w)\nParameters: step sizes, α > 0, β > 0\nOutput: policy π(a|s, θ)\ninitialize policy parameter θ and state-value weights w\nfor true do\ninitialize s, the ﬁrst state of the episode\nI ←1\nfor s is not terminal do\na ∼π(·|s, θ)\ntake action a, observe s′, r\nδ ←r + γˆv(s′, w) −ˆv(s, w) (if s′ is terminal, ˆv(s′, w) .= 0)\nw ←w + βδ∇wˆv(st, w)\nθ ←θ + αIδ∇θlogπ(at|st, θ)\nI ←γI\ns ←s′\nend\nend\nAlgorithm 6: Actor-Critic (episodic), adapted from Sutton and Barto (2018)\n13\nPolicy iteration alternates between policy evaluation and policy improvement, to generate a sequence\nof improving policies. In policy evaluation, the value function of the current policy is estimated from\nthe outcomes of sampled trajectories. In policy improvement, the current value function is used to\ngenerate a better policy, e.g., by selecting actions greedily with respect to the value function.\n2.3.9\nDEEP REINFORCEMENT LEARNING\nWe obtain deep reinforcement learning (deep RL) methods when we use deep neural networks to\napproximate any of the following components of reinforcement learning: value function, ˆv(s; θ)\nor ˆq(s, a; θ), policy π(a|s; θ), and model (state transition function and reward function). Here, the\nparameters θ are the weights in deep neural networks. When we use ”shallow” models, like linear\nfunction, decision trees, tile coding and so on as the function approximator, we obtain ”shallow”\nRL, and the parameters θ are the weight parameters in these models. Note, a shallow model, e.g.,\ndecision trees, may be non-linear. The distinct difference between deep RL and ”shallow” RL is what\nfunction approximator is used. This is similar to the difference between deep learning and ”shallow”\nmachine learning. We usually utilize stochastic gradient descent to update weight parameters in deep\nRL. When off-policy, function approximation, in particular, non-linear function approximation, and\nbootstrapping are combined together, instability and divergence may occur (Tsitsiklis and Van Roy,\n1997). However, recent work like Deep Q-Network (Mnih et al., 2015) and AlphaGo (Silver et al.,\n2016a) stabilized the learning and achieved outstanding results.\n2.3.10\nRL PARLANCE\nWe explain some terms in RL parlance.\nThe prediction problem, or policy evaluation, is to compute the state or action value function for a\npolicy. The control problem is to ﬁnd the optimal policy. Planning constructs a value function or a\npolicy with a model.\nOn-policy methods evaluate or improve the behavioural policy, e.g., SARSA ﬁts the action-value\nfunction to the current policy, i.e., SARSA evaluates the policy based on samples from the same\npolicy, then reﬁnes the policy greedily with respect to action values. In off-policy methods, an agent\nlearns an optimal value function/policy, maybe following an unrelated behavioural policy, e.g., Q-\nlearning attempts to ﬁnd action values for the optimal policy directly, not necessarily ﬁtting to the\npolicy generating the data, i.e., the policy Q-learning obtains is usually different from the policy that\ngenerates the samples. The notion of on-policy and off-policy can be understood as same-policy and\ndifferent-policy.\nThe exploration-exploitation dilemma is about the agent needs to exploit the currently best action\nto maximize rewards greedily, yet it has to explore the environment to ﬁnd better actions, when the\npolicy is not optimal yet, or the system is non-stationary.\nIn model-free methods, the agent learns with trail-and-error from experience explicitly; the model\n(state transition function) is not known or learned from experience. RL methods that use models are\nmodel-based methods.\nIn online mode, training algorithms are executed on data acquired in sequence. In ofﬂine mode, or\nbatch mode, models are trained on the entire data set.\nWith bootstrapping, an estimate of state or action value is updated from subsequent estimates.\n2.3.11\nBRIEF SUMMARY\nA RL problem is formulated as an MDP when the observation about the environment satisﬁes the\nMarkov property. An MDP is deﬁned by the 5-tuple (S, A, P, R, γ). A central concept in RL\nis value function. Bellman equations are cornerstone for developing RL algorithms. Temporal\ndifference learning algorithms are fundamental for evaluating/predicting value functions. Control\nalgorithms ﬁnd optimal policies. Reinforcement learning algorithms may be based on value func-\ntion and/or policy, model-free or model-based, on-policy or off-policy, with function approximation\nor not, with sample backups (TD and Monte Carlo) or full backups (dynamic programming and\nexhaustive search), and about the depth of backups, either one-step return (TD(0) and dynamic pro-\ngramming) or multi-step return (TD(λ), Monte Carlo, and exhaustive search). When combining\n14\noff-policy, function approximation, and bootstrapping, we face instability and divergence (Tsitsiklis\nand Van Roy, 1997), the deadly triad issue (Sutton and Barto, 2018). Theoretical guarantee has been\nestablished for linear function approximation, e.g., Gradient-TD (Sutton et al., 2009a;b; Mahmood\net al., 2014), Emphatic-TD (Sutton et al., 2016) and Du et al. (2017). With non-linear function ap-\nproximation, in particular deep learning, algorithms like Deep Q-Network (Mnih et al., 2015) and\nAlphaGo (Silver et al., 2016a; 2017) stabilized the learning and achieved stunning results, which is\nthe focus of this overview.\n3\nCORE ELEMENTS\nA RL agent executes a sequence of actions and observe states and rewards, with major components\nof value function, policy and model. A RL problem may be formulated as a prediction, control or\nplanning problem, and solution methods may be model-free or model-based, with value function\nand/or policy. Exploration-exploitation is a fundamental tradeoff in RL. Knowledge would be crit-\nical for RL. In this section, we discuss core RL elements: value function in Section 3.1, policy in\nSection 3.2, reward in Section 3.3, model and planning in Section 3.4, exploration in Section 3.5,\nand knowledge in Section 3.6.\n3.1\nVALUE FUNCTION\nValue function is a fundamental concept in reinforcement learning, and temporal difference (TD)\nlearning (Sutton, 1988) and its extension, Q-learning (Watkins and Dayan, 1992), are classical algo-\nrithms for learning state and action value functions respectively. In the following, we focus on Deep\nQ-Network (Mnih et al., 2015), a recent breakthrough, and its extensions.\n3.1.1\nDEEP Q-NETWORK (DQN) AND EXTENSIONS\nMnih et al. (2015) introduced Deep Q-Network (DQN) and ignited the ﬁeld of deep RL. We present\nDQN pseudo code in Algorithm 7.\nInput: the pixels and the game score\nOutput: Q action value function (from which we obtain policy and select action)\nInitialize replay memory D\nInitialize action-value function Q with random weight θ\nInitialize target action-value function ˆQ with weights θ−= θ\nfor episode = 1 to M do\nInitialize sequence s1 = {x1} and preprocessed sequence φ1 = φ(s1)\nfor t = 1 to T do\nFollowing ϵ-greedy policy, select at =\n\u001aa random action\nwith probability ϵ\narg maxa Q(φ(st), a; θ)\notherwise\nExecute action ai in emulator and observe reward rt and image xt+1\nSet st+1 = st, at, xt+1 and preprocess φt+1 = φ(st+1)\nStore transition (φt, at, rt, φt+1) in D\n// experience replay\nSample random minibatch of transitions (φj, aj, rj, φj+1) from D\nSet yj =\n\u001arj\nif episode terminates at step j + 1\nrj + γ maxa′ ˆQ(φj+1, a′; θ−)\notherwise\nPerform a gradient descent step on (yj −Q(φj, aj; θ))2 w.r.t. the network parameter θ\n// periodic update of target network\nEvery C steps reset ˆQ = Q, i.e., set θ−= θ\nend\nend\nAlgorithm 7: Deep Q-Nework (DQN), adapted from Mnih et al. (2015)\nBefore DQN, it is well known that RL is unstable or even divergent when action value function is\napproximated with a nonlinear function like neural networks. DQN made several important contri-\nbutions: 1) stabilize the training of action value function approximation with deep neural networks\n15\n(CNN) using experience replay (Lin, 1992) and target network; 2) designing an end-to-end RL ap-\nproach, with only the pixels and the game score as inputs, so that only minimal domain knowledge\nis required; 3) training a ﬂexible network with the same algorithm, network architecture and hyper-\nparameters to perform well on many different tasks, i.e., 49 Atari games (Bellemare et al., 2013),\nand outperforming previous algorithms and performing comparably to a human professional tester.\nSee Chapter 16 in Sutton and Barto (2018) for a detailed and intuitive description of Deep Q-\nNetwork. See Deepmind’s description of DQN at https://deepmind.com/research/dqn/.\nDOUBLE DQN\nvan Hasselt et al. (2016a) proposed Double DQN (D-DQN) to tackle the over-estimate problem in\nQ-learning. In standard Q-learning, as well as in DQN, the parameters are updated as follows:\nθt+1 = θt + α(yQ\nt −Q(st, at; θt))∇θtQ(st, at; θt),\nwhere\nyQ\nt = rt+1 + γ max\na\nQ(st+1, a; θt),\nso that the max operator uses the same values to both select and evaluate an action. As a conse-\nquence, it is more likely to select over-estimated values, and results in over-optimistic value esti-\nmates. van Hasselt et al. (2016a) proposed to evaluate the greedy policy according to the online\nnetwork, but to use the target network to estimate its value. This can be achieved with a minor\nchange to the DQN algorithm, replacing yQ\nt with\nyD−DQN\nt\n= rt+1 + γQ(st+1, arg max\na\nQ(st+1, at; θt); θ−\nt ),\nwhere θt is the parameter for online network and θ−\nt is the parameter for target network. For\nreference, yQ\nt can be written as\nyQ\nt = rt+1 + γQ(st+1, arg max\na\nQ(st+1, at; θt); θt).\nD-DQN found better policies than DQN on Atari games.\nPRIORITIZED EXPERIENCE REPLAY\nIn DQN, experience transitions are uniformly sampled from the replay memory, regardless of the\nsigniﬁcance of experiences. Schaul et al. (2016) proposed to prioritize experience replay, so that\nimportant experience transitions can be replayed more frequently, to learn more efﬁciently. The\nimportance of experience transitions are measured by TD errors. The authors designed a stochastic\nprioritization based on the TD errors, using importance sampling to avoid the bias in the update\ndistribution. The authors used prioritized experience replay in DQN and D-DQN, and improved\ntheir performance on Atari games.\nDUELING ARCHITECTURE\nWang et al. (2016b) proposed the dueling network architecture to estimate state value function V (s)\nand associated advantage function A(s, a), and then combine them to estimate action value function\nQ(s, a), to converge faster than Q-learning. In DQN, a CNN layer is followed by a fully connected\n(FC) layer. In dueling architecture, a CNN layer is followed by two streams of FC layers, to estimate\nvalue function and advantage function separately; then the two streams are combined to estimate\naction value function. Usually we use the following to combine V (s) and A(s, a) to obtain Q(s, a),\nQ(s, a; θ, α, β) = V (s; θ, β) +\n\u0000A(s, a; θ, α) −max\na′ A(s, a′; θ, α)\n\u0001\nwhere α and β are parameters of the two streams of FC layers. Wang et al. (2016b) proposed to\nreplace max operator with average as the following for better stability,\nQ(s, a; θ, α, β) = V (s; θ, β) +\n\u0000A(s, a; θ, α) −a\n|A|A(s, a′; θ, α)\n\u0001\nDueling architecture implemented with D-DQN and prioritized experience replay improved previous\nwork, DQN and D-DQN with prioritized experience replay, on Atari games.\n16\nDISTRIBUTIONAL VALUE FUNCTION\nBellemare et al. (2017)\nRAINBOW\nHessel et al. (2018)\nMORE DQN EXTENSIONS\nDQN has been receiving much attention. We list several extensions/improvements here.\n• Anschel et al. (2017) proposed to reduce variability and instability by an average of previ-\nous Q-values estimates.\n• He et al. (2017) proposed to accelerate DQN by optimality tightening, a constrained opti-\nmization approach, to propagate reward faster, and to improve accuracy over DQN.\n• Liang et al. (2016) attempted to understand the success of DQN and reproduced results\nwith shallow RL.\n• O’Donoghue et al. (2017) proposed policy gradient and Q-learning (PGQ), as discussed in\nSection 3.2.3.\n• Oh et al. (2015) proposed spatio-temporal video prediction conditioned on actions and\nprevious video frames with deep neural networks in Atari games.\n• Osband et al. (2016) designed better exploration strategy to improve DQN.\n• Hester et al. (2018) proposed to learn from demonstration with new loss functions, as dis-\ncussed in Section 4.2.\n3.2\nPOLICY\nA policy maps state to action, and policy optimization is to ﬁnd an optimal mapping. As in Peters\nand Neumann (2015), the spectrum from direct policy search to value-based RL includes: evo-\nlutionary strategies, CMA-ES (covariance matrix adaptation evolution strategy), episodic REPS\n(relative entropy policy search), policy gradients, PILCO (probabilistic inference for learning con-\ntrol) (Deisenroth and Rasmussen, 2011), model-based REPS, policy search by trajectory optimiza-\ntion, actor critic, natural actor critic, eNAC (episodic natural actor critic), advantage weighted re-\ngression, conservative policy iteration, LSPI (least square policy iteration) (Lagoudakis and Parr,\n2003), Q-learning, and ﬁtted Q, as well as important extensions, contextual policy search, and hier-\narchical policy search.\nWe discuss actor-critic (Mnih et al., 2016). Then we discuss policy gradient, including deterministic\npolicy gradient (Silver et al., 2014; Lillicrap et al., 2016), trust region policy optimization (Schulman\net al., 2015), and, benchmark results (Duan et al., 2016). Next we discuss the combination of policy\ngradient and off-policy RL (O’Donoghue et al., 2017; Nachum et al., 2017; Gu et al., 2017).\nSee Retrace algorithm (Munos et al., 2016), a safe and efﬁcient return-based off-policy control\nalgorithm, and its actor-critic extension, Reactor (Gruslys et al., 2017), for Retrace-actor. See dis-\ntributed proximal policy optimization (Heess et al., 2017). McAllister and Rasmussen (2017) ex-\ntended PILCO to POMDPs.\n3.2.1\nACTOR-CRITIC\nAn actor-critic algorithm learns both a policy and a state-value function, and the value function\nis used for bootstrapping, i.e., updating a state from subsequent estimates, to reduce variance and\naccelerate learning (Sutton and Barto, 2018). In the following, we focus on asynchronous advantage\nactor-critic (A3C) (Mnih et al., 2016). Mnih et al. (2016) also discussed asynchronous one-step\nSARSA, one-step Q-learning and n-step Q-learning.\nIn A3C, parallel actors employ different exploration policies to stabilize training, so that experience\nreplay is not utilized. Different from most deep learning algorithms, asynchronous methods can\nrun on a single multi-core CPU. For Atari games, A3C ran much faster yet performed better than\n17\nor comparably with DQN, Gorila (Nair et al., 2015), D-DQN, Dueling D-DQN, and Prioritized\nD-DQN. A3C also succeeded on continuous motor control problems: TORCS car racing games\nand MujoCo physics manipulation and locomotion, and Labyrinth, a navigating task in random 3D\nmazes using visual inputs, in which an agent will face a new maze in each new episode, so that it\nneeds to learn a general strategy to explore random mazes.\nGlobal shared parameter vectors θ and θv, thread-speciﬁc parameter vectors θ′ and θ′\nv\nGlobal shared counter T = 0, Tmax\nInitialize step counter t ←1\nfor T ≤Tmax do\nReset gradients, dθ ←0 and dθv ←0\nSynchronize thread-speciﬁc parameters θ′ = θ and θ′\nv = θv\nSet tstart = t, get state st\nfor st not terminal and t −tstart ≤tmax do\nTake at according to policy π(at|st; θ′)\nReceive reward rt and new state st+1\nt ←t + 1, T ←T + 1\nend\nR =\n\u001a0\nfor terminal st\nV (st, θ′\nv)\notherwise\nfor i ∈{t −1, ..., tstart} do\nR ←ri + γR\naccumulate gradients wrt θ′: dθ ←dθ + ∇θ′ log π(ai|si; θ′)(R −V (si; θ′\nv))\naccumulate gradients wrt θ′\nv: dθv ←dθv + ∇θ′v(R −V (si; θ′\nv))2\nend\nUpdate asynchronously θ using dθ, and θv using dθv\nend\nAlgorithm 8: A3C, each actor-learner thread, based on Mnih et al. (2016)\nWe present pseudo code for asynchronous advantage actor-critic for each actor-learner thread\nin Algorithm 8.\nA3C maintains a policy π(at|st; θ) and an estimate of the value function\nV (st; θv), being updated with n-step returns in the forward view, after every tmax actions or\nreaching a terminal state, similar to using minibatches.\nThe gradient update can be seen as\n∇θ′ log π(at|st; θ′)A(st, at; θ, θv), where A(st, at; θ, θv) = Pk−1\ni=0 γirt+i + γkV (st+k; θv) −\nV (st; θv) is an estimate of the advantage function, with k upbounded by tmax.\nWang et al. (2017b) proposed a stable and sample efﬁcient actor-critic deep RL model using experi-\nence replay, with truncated importance sampling, stochastic dueling network (Wang et al., 2016b) as\ndiscussed in Section 3.1.1, and trust region policy optimization (Schulman et al., 2015) as discussed\nin Section 3.2.2. Babaeizadeh et al. (2017) proposed a hybrid CPU/GPU implementation of A3C.\n3.2.2\nPOLICY GRADIENT\nREINFORCE (Williams, 1992; Sutton et al., 2000) is a popular policy gradient method. Relatively\nspeaking, Q-learning as discussed in Section 3.1 is sample efﬁcient, while policy gradient is stable.\nDETERMINISTIC POLICY GRADIENT\nPolicies are usually stochastic. However, Silver et al. (2014) and Lillicrap et al. (2016) proposed\ndeterministic policy gradient (DPG) for efﬁcient estimation of policy gradients.\nSilver et al. (2014) introduced the deterministic policy gradient (DPG) algorithm for RL problems\nwith continuous action spaces. The deterministic policy gradient is the expected gradient of the\naction-value function, which integrates over the state space; whereas in the stochastic case, the pol-\nicy gradient integrates over both state and action spaces. Consequently, the deterministic policy\ngradient can be estimated more efﬁciently than the stochastic policy gradient. The authors intro-\nduced an off-policy actor-critic algorithm to learn a deterministic target policy from an exploratory\nbehaviour policy, and to ensure unbiased policy gradient with the compatible function approxima-\ntion for deterministic policy gradients. Empirical results showed its superior to stochastic policy\n18\ngradients, in particular in high dimensional tasks, on several problems: a high-dimensional bandit;\nstandard benchmark RL tasks of mountain car and pendulum and 2D puddle world with low dimen-\nsional action spaces; and controlling an octopus arm with a high-dimensional action space. The\nexperiments were conducted with tile-coding and linear function approximators.\nLillicrap et al. (2016) proposed an actor-critic, model-free, deep deterministic policy gradient\n(DDPG) algorithm in continuous action spaces, by extending DQN (Mnih et al., 2015) and DPG (Sil-\nver et al., 2014). With actor-critic as in DPG, DDPG avoids the optimization of action at every time\nstep to obtain a greedy policy as in Q-learning, which will make it infeasible in complex action\nspaces with large, unconstrained function approximators like deep neural networks. To make the\nlearning stable and robust, similar to DQN, DDPQ deploys experience replay and an idea similar to\ntarget network, ”soft” target, which, rather than copying the weights directly as in DQN, updates the\nsoft target network weights θ′ slowly to track the learned networks weights θ: θ′ ←τθ +(1−τ)θ′,\nwith τ ≪1. The authors adapted batch normalization to handle the issue that the different com-\nponents of the observation with different physical units. As an off-policy algorithm, DDPG learns\nan actor policy from experiences from an exploration policy by adding noise sampled from a noise\nprocess to the actor policy. More than 20 simulated physics tasks of varying difﬁculty in the Mu-\nJoCo environment were solved with the same learning algorithm, network architecture and hyper-\nparameters, and obtained policies with performance competitive with those found by a planning\nalgorithm with full access to the underlying physical model and its derivatives. DDPG can solve\nproblems with 20 times fewer steps of experience than DQN, although it still needs a large number\nof training episodes to ﬁnd solutions, as in most model-free RL methods. It is end-to-end, with raw\npixels as input. DDPQ paper also contains links to videos for illustration.\nHausknecht and Stone (2016) considers parameterization of action space.\nTRUST REGION POLICY OPTIMIZATION\nSchulman et al. (2015) introduced an iterative procedure to monotonically improve policies theoreti-\ncally, guaranteed by optimizing a surrogate objective function. The authors then proposed a practical\nalgorithm, Trust Region Policy Optimization (TRPO), by making several approximations, includ-\ning, introducing a trust region constraint, deﬁned by the KL divergence between the new policy and\nthe old policy, so that at every point in the state space, the KL divergence is bounded; approximat-\ning the trust region constraint by the average KL divergence constraint; replacing the expectations\nand Q value in the optimization problem by sample estimates, with two variants: in the single path\napproach, individual trajectories are sampled; in the vine approach, a rollout set is constructed and\nmultiple actions are performed from each state in the rollout set; and, solving the constrained opti-\nmization problem approximately to update the policy’s parameter vector. The authors also uniﬁed\npolicy iteration and policy gradient with analysis, and showed that policy iteration, policy gradient,\nand natural policy gradient (Kakade, 2002) are special cases of TRPO. In the experiments, TRPO\nmethods performed well on simulated robotic tasks of swimming, hopping, and walking, as well as\nplaying Atari games in an end-to-end manner directly from raw images.\nWu et al. (2017) proposed scalable TRPO with Kronecker-factored approximation to the curvature.\nhttps://blog.openai.com/openai-baselines-ppo/\nBENCHMARK RESULTS\nDuan et al. (2016) presented a benchmark for continuous control tasks, including classic tasks like\ncart-pole, tasks with very large state and action spaces such as 3D humanoid locomotion and tasks\nwith partial observations, and tasks with hierarchical structure, implemented various algorithms,\nincluding batch algorithms: REINFORCE, Truncated Natural Policy Gradient (TNPG), Reward-\nWeighted Regression (RWR), Relative Entropy Policy Search (REPS), Trust Region Policy Opti-\nmization (TRPO), Cross Entropy Method (CEM), Covariance Matrix Adaption Evolution Strategy\n(CMA-ES); online algorithms: Deep Deterministic Policy Gradient (DDPG); and recurrent variants\nof batch algorithms. The open source is available at: https://github.com/rllab/rllab.\nDuan et al. (2016) compared various algorithms, and showed that DDPG, TRPO, and Truncated Nat-\nural Policy Gradient (TNPG) (Schulman et al., 2015) are effective in training deep neural network\npolicies, yet better algorithms are called for hierarchical tasks.\n19\nIslam et al. (2017)\nTassa et al. (2018)\n3.2.3\nCOMBINING POLICY GRADIENT WITH OFF-POLICY RL\nO’Donoghue et al. (2017) proposed to combine policy gradient with off-policy Q-learning (PGQ),\nto beneﬁt from experience replay. Usually actor-critic methods are on-policy. The authors also\nshowed that action value ﬁtting techniques and actor-critic methods are equivalent, and interpreted\nregularized policy gradient techniques as advantage function learning algorithms. Empirically, the\nauthors showed that PGQ outperformed DQN and A3C on Atari games.\nNachum et al. (2017) introduced the notion of softmax temporal consistency, to generalize the hard-\nmax Bellman consistency as in off-policy Q-learning, and in contrast to the average consistency\nas in on-policy SARSA and actor-critic. The authors established the correspondence and a mutual\ncompatibility property between softmax consistent action values and the optimal policy maximizing\nentropy regularized expected discounted reward. The authors proposed Path Consistency Learning,\nattempting to bridge the gap between value and policy based RL, by exploiting multi-step path-wise\nconsistency on traces from both on and off policies.\nGu et al. (2017) proposed Q-Prop to take advantage of the stability of policy gradients and the\nsample efﬁciency of off-policy RL. Schulman et al. (2017) showed the equivalence between entropy-\nregularized Q-learning and policy gradient.\nGu et al. (2017)\n3.3\nREWARD\nRewards provide evaluative feedbacks for a RL agent to make decisions. Rewards may be sparse\nso that it is challenging for learning algorithms, e.g., in computer Go, a reward occurs at the end of\na game. There are unsupervised ways to harness environmental signals, see Section 4.2. Reward\nfunction is a mathematical formulation for rewards. Reward shaping is to modify reward function\nto facilitate learning while maintaining optimal policy. Reward functions may not be available for\nsome RL problems, which is the focus of this section.\nIn imitation learning, an agent learns to perform a task from expert demonstrations, with samples of\ntrajectories from the expert, without reinforcement signal, without additional data from the expert\nwhile training; two main approaches for imitation learning are behavioral cloning and inverse rein-\nforcement learning. Behavioral cloning, or apprenticeship learning, or learning from demonstration,\nis formulated as a supervised learning problem to map state-action pairs from expert trajectories\nto policy, without learning the reward function (Ho et al., 2016; Ho and Ermon, 2016). Inverse\nreinforcement learning (IRL) is the problem of determining a reward function given observations\nof optimal behaviour (Ng and Russell, 2000). Abbeel and Ng (2004) approached apprenticeship\nlearning via IRL.\nIn the following, we discuss learning from demonstration (Hester et al., 2018), and imitation learning\nwith generative adversarial networks (GANs) (Ho and Ermon, 2016; Stadie et al., 2017). We will\ndiscuss GANs, a recent unsupervised learning framework, in Section 4.2.3.\nSu et al. (2016b) proposed to train dialogue policy jointly with reward model. Christiano et al. (2017)\nproposed to learn reward function by human preferences from comparisons of trajectory segments.\nSee also Hadﬁeld-Menell et al. (2016); Merel et al. (2017); Wang et al. (2017); van Seijen et al.\n(2017).\nAmin et al. (2017)\nLEARNING FROM DEMONSTRATION\nHester et al. (2018) proposed Deep Q-learning from Demonstrations (DQfD) to attempt to accel-\nerate learning by leveraging demonstration data, using a combination of temporal difference (TD),\nsupervised, and regularized losses. In DQfQ, reward signal is not available for demonstration data;\nhowever, it is available in Q-learning. The supervised large margin classiﬁcation loss enables the\npolicy derived from the learned value function to imitate the demonstrator; the TD loss enables the\n20\nvalidity of value function according to the Bellman equation and its further use for learning with\nRL; the regularization loss function on network weights and biases prevents overﬁtting on small\ndemonstration dataset. In the pre-training phase, DQfD trains only on demonstration data, to obtain\na policy imitating the demonstrator and a value function for continual RL learning. After that, DQfD\nself-generates samples, and mixes them with demonstration data according to certain proportion to\nobtain training data. The authors showed that, on Atari games, DQfD in general has better initial\nperformance, more average rewards, and learns faster than DQN.\nIn AlphaGo (Silver et al., 2016a), to be discussed in Section 5.1.1, the supervised learning policy\nnetwork is learned from expert moves as learning from demonstration; the results initialize the RL\npolicy network. See also Kim et al. (2014); P´erez-D’Arpino and Shah (2017). See Argall et al.\n(2009) for a survey of robot learning from demonstration.\nVeˇcer´ık et al. (2017)\nGENERATIVE ADVERSARIAL IMITATION LEARNING\nWith IRL, an agent learns a reward function ﬁrst, then from which derives an optimal policy. Many\nIRL algorithms have high time complexity, with a RL problem in the inner loop.\nHo and Ermon (2016) proposed generative adversarial imitation learning algorithm to learn poli-\ncies directly from data, bypassing the intermediate IRL step. Generative adversarial training was\ndeployed to ﬁt the discriminator, the distribution of states and actions that deﬁnes expert behavior,\nand the generator, the policy.\nGenerative adversarial imitation learning ﬁnds a policy πθ so that a discriminator DR can not dis-\ntinguish states following the expert policy πE and states following the imitator policy πθ, hence\nforcing DR to take 0.5 in all cases and πθ not distinguishable from πE in the equillibrium. Such a\ngame is formulated as:\nmax\nπθ min\nDR −Eπθ[log DR(s)] −EπE[log(1 −DR(s))]\nThe authors represented both πθ and DR as deep neural networks, and found an optimal solution\nby repeatedly performing gradient updates on each of them. DR can be trained with supervised\nlearning with a data set formed from traces from a current πθ and expert traces. For a ﬁxed DR, an\noptimal πθ is sought. Hence it is a policy optimization problem, with −log DR(s) as the reward.\nThe authors trained πθ by trust region policy optimization (Schulman et al., 2015).\nLi et al. (2017)\nTHIRD PERSON IMITATION LEARNING\nStadie et al. (2017) argued that previous works in imitation learning, like Ho and Ermon (2016) and\nFinn et al. (2016b), have the limitation of ﬁrst person demonstrations, and proposed to learn from\nunsupervised third person demonstration, mimicking human learning by observing other humans\nachieving goals.\n3.4\nMODEL AND PLANNING\nA model is an agent’s representation of the environment, including the transition model and the\nreward model. Usually we assume the reward model is known. We discuss how to handle unknown\nreward models in Section 3.3. Model-free RL approaches handle unknown dynamical systems,\nhowever, they usually require large number of samples, which may be costly or prohibitive to obtain\nfor real physical systems. Model-based RL approaches learn value function and/or policy in a data-\nefﬁcient way, however, they may suffer from the issue of model identiﬁcation so that the estimated\nmodels may not be accurate, and the performance is limited by the estimated model. Planning\nconstructs a value function or a policy usually with a model, so that planning is usually related to\nmodel-based RL methods.\nChebotar et al. (2017) attempted to combine the advantages of both model-free and model-based RL\napproaches. The authors focused on time-varying linear-Gaussian policies, and integrated a model-\n21\nbased linear quadratic regulator (LQR) algorithm with a model-free path integral policy improve-\nment algorithm. To generalize the method for arbitrary parameterized policies such as deep neural\nnetworks, the authors combined the proposed approach with guided policy search (GPS) (Levine\net al., 2016a). The proposed approach does not generate synthetic samples with estimated models to\navoid degradation from modelling errors. See recent work on model-based learning, e.g., Gu et al.\n(2016b); Henaff et al. (2017); Hester and Stone (2017); Oh et al. (2017); Watter et al. (2015).\nTamar et al. (2016) introduced Value Iteration Networks (VIN), a fully differentiable CNN plan-\nning module to approximate the value iteration algorithm, to learn to plan, e.g, policies in RL. In\ncontrast to conventional planning, VIN is model-free, where reward and transition probability are\npart of the neural network to be learned, so that it may avoid issues with system identiﬁcation. VIN\ncan be trained end-to-end with backpropagation. VIN can generalize in a diverse set of tasks: sim-\nple gridworlds, Mars Rover Navigation, continuous control and WebNav Challenge for Wikipedia\nlinks navigation (Nogueira and Cho, 2016). One merit of Value Iteration Network, as well as Du-\neling Network(Wang et al., 2016b), is that they design novel deep neural networks architectures\nfor reinforcement learning problems. See a blog about VIN at https://github.com/karpathy/paper-\nnotes/blob/master/vin.md.\nSilver et al. (2016b) proposed the predictron to integrate learning and planning into one end-to-end\ntraining procedure with raw input in Markov reward process, which can be regarded as Markov\ndecision process without actions. See classical Dyna-Q (Sutton, 1990).\nWeber et al. (2017)\nAndrychowicz et al. (2017)\n3.5\nEXPLORATION\nA RL agent usually uses exploration to reduce its uncertainty about the reward function and tran-\nsition probabilities of the environment. In tabular cases, this uncertainty can be quantiﬁed as con-\nﬁdence intervals or posterior of environment parameters, which are related to the state-action visit\ncounts. With count-based exploration, a RL agent uses visit counts to guide its behaviour to re-\nduce uncertainty. However, count-based methods are not directly useful in large domains. Intrinsic\nmotivation suggests to explore what is surprising, typically in learning process based on change in\nprediction error. Intrinsic motivation methods do not require Markov property and tabular repre-\nsentation as count-based methods require. Bellemare et al. (2016) proposed pseudo-count, a density\nmodel over the state space, to unify count-based exploration and intrinsic motivation, by introducing\ninformation gain, to relate to conﬁdence intervals in count-based exploration, and to relate to learn-\ning progress in intrinsic motivation. The author established pseudo-count’s theoretical advantage\nover previous intrinsic motivation methods, and validated it with Atari games.\nNachum et al. (2017) proposed an under-appreciated reward exploration technique to avoid the pre-\nvious ineffective, undirected exploration strategies of the reward landscape, as in ϵ-greedy and en-\ntropy regularization, and to promote directed exploration of the regions, in which the log-probability\nof an action sequence under the current policy under-estimates the resulting reward. The under-\nappreciated reward exploration strategy resulted from importance sampling from the optimal policy,\nand combined a mode seeking and a mean seeking terms to tradeoff exploration and exploitation.\nThe authors implemented the proposed exploration strategy with minor modiﬁcations to REIN-\nFORCE, and validated it, for the ﬁrst time with a RL method, on several algorithmic tasks.\nOsband et al. (2016) proposed bootstrapped DQN to combine deep exploration with deep neural\nnetworks to achieve efﬁcient learning. Houthooft et al. (2016) proposed variational information\nmaximizing exploration for continuous state and action spaces. Fortunato et al. (2017) proposed\nNoisyNet for efﬁcient exploration by adding parametric noise added to weights of deep neural net-\nworks. See also Azar et al. (2017); Jiang et al. (2016); Ostrovski et al. (2017).\nTang et al. (2017)\nFu et al. (2017)\n22\n3.6\nKNOWLEDGE\n(This section would be an open-ended discussion.)\nKnowledge would be critical for further development of RL. Knowledge may be incorporated into\nRL in various ways, through value, reward, policy, model, exploration strategy, etc. During a per-\nsonal conversation with Rich Sutton, he mentioned that it is still wide open how to incorporate\nknowledge into RL.\nhuman intelligence, Lake et al. (2016), developmental start-up software — intuitive physics, intu-\nitive psychology; learning as rapid model building — compositionality, causality; learning to learn;\nthinking fast — approximate inference in structured models, model-based and model-free reinforce-\nment learning\nconsciousness prior, Bengio (2017)\nML with knowledge, Song and Roth (2017)\ncausality, Pearl (2018), Johansson et al. (2016)\ninterpretability, Zhang and Zhu (2018) surveyed visual interpretability for deep learning, Dong et al.\n(2017)\nGeorge et al. (2017)\nYang and Mitchell (2017)\n4\nIMPORTANT MECHANISMS\nIn this section, we discuss important mechanisms for the development of (deep) reinforcement learn-\ning, including attention and memory, unsupervised learning, transfer learning, multi-agent reinforce-\nment learning, hierarchical RL, and learning to learn. We note that we do not discuss in detail some\nimportant mechanisms, like Bayesian RL (Ghavamzadeh et al., 2015), POMDP (Hausknecht and\nStone, 2015), and semi-supervised RL (Audiffren et al., 2015; Finn et al., 2017; Zhu and Goldberg,\n2009).\n4.1\nATTENTION AND MEMORY\nAttention is a mechanism to focus on the salient parts. Memory provides data storage for long time,\nand attention is an approach for memory addressing.\nGraves et al. (2016) proposed differentiable neural computer (DNC), in which, a neural network\ncan read from and write to an external memory, so that DNC can solve complex, structured prob-\nlems, which a neural network without read-write memory can not solve. DNC minimizes memory\nallocation interference and enables long-term storage. Similar to a conventional computer, in a\nDNC, the neural network is the controller and the external memory is the random-access memory;\nand a DNC represents and manipulates complex data structures with the memory. Differently, a\nDNC learns such representation and manipulation end-to-end with gradient descent from data in a\ngoal-directed manner. When trained with supervised learning, a DNC can solve synthetic question\nanswering problems, for reasoning and inference in natural language; it can solve the shortest path\nﬁnding problem between two stops in transportation networks and the relationship inference prob-\nlem in a family tree. When trained with reinforcement learning, a DNC can solve a moving blocks\npuzzle with changing goals speciﬁed by symbol sequences. DNC outperformed normal neural net-\nwork like LSTM or DNC’s precursor Neural Turing Machine (Graves et al., 2014); with harder\nproblems, an LSTM may simply fail. Although these experiments are relatively small-scale, we\nexpect to see further improvements and applications of DNC. See Deepmind’s description of DNC\nat https://deepmind.com/blog/differentiable-neural-computers/.\nMnih et al. (2014) applied attention to image classiﬁcation and object detection. Xu et al. (2015)\nintegrated attention to image captioning. We brieﬂy discuss application of attention in computer\nvision in Section 5.4. The attention mechanism is also deployed in NLP, e.g., in Bahdanau et al.\n(2015; 2017), and with external memory, in differentiable neural computer (Graves et al., 2016) as\ndiscussed above. Most works follow a soft attention mechanism (Bahdanau et al., 2015), a weighted\n23\naddressing scheme to all memory locations. There are endeavours for hard attention (Gulcehre et al.,\n2016; Liang et al., 2017a; Luo et al., 2016; Xu et al., 2015; Zaremba and Sutskever, 2015), which is\nthe way conventional computers access memory.\nSee recent work on attention and/or memory, e.g., Ba et al. (2014; 2016); Chen et al. (2016b);\nDanihelka et al. (2016); Duan et al. (2017); Eslami et al. (2016); Gregor et al. (2015); Jader-\nberg et al. (2015); Kaiser and Bengio (2016); Kadlec et al. (2016); Luo et al. (2016); Oh\net al. (2016); Oquab et al. (2015); Vaswani et al. (2017); Weston et al. (2015); Sukhbaatar\net al. (2015); Yang et al. (2015); Zagoruyko and Komodakis (2017); Zaremba and Sutskever\n(2015). See http://distill.pub/2016/augmented-rnns/ and http://www.wildml.com/2016/01/attention-\nand-memory-in-deep-learning-and-nlp/ for blogs about attention and memory.\n4.2\nUNSUPERVISED LEARNING\nUnsupervised learning is a way to take advantage of the massive amount of data, and would be a crit-\nical mechanism to achieve general artiﬁcial intelligence. Unsupervised learning is categorized into\nnon-probabilistic models, like sparse coding, autoencoders, k-means etc, and probabilistic (gen-\nerative) models, where density functions are concerned, either explicitly or implicitly (Salakhut-\ndinov, 2016). Among probabilistic (generative) models with explicit density functions, some are\nwith tractable models, like fully observable belief nets, neural autoregressive distribution estima-\ntors, and PixelRNN, etc; some are with non-tractable models, like Botlzmann machines, variational\nautoencoders, Helmhotz machines, etc. For probabilistic (generative) models with implicit density\nfunctions, we have generative adversarial networks, moment matching networks, etc.\nIn the following, we discuss Horde (Sutton et al., 2011), and unsupervised auxiliary learning (Jader-\nberg et al., 2017), two ways to take advantages of possible non-reward training signals in environ-\nments. We also discuss generative adversarial networks (Goodfellow et al., 2014). See also Le et al.\n(2012), Chen et al. (2016), Liu et al. (2017).\nArtetxe et al. (2017)\n4.2.1\nHORDE\nSutton et al. (2011) proposed to represent knowledge with general value function, where policy,\ntermination function, reward function, and terminal reward function are parameters. The authors\nthen proposed Horde, a scalable real-time architecture for learning in parallel general value functions\nfor independent sub-agents from unsupervised sensorimotor interaction, i.e., nonreward signals and\nobservations. Horde can learn to predict the values of many sensors, and policies to maximize those\nsensor values, with general value functions, and answer predictive or goal-oriented questions. Horde\nis off-policy, i.e., it learns in real-time while following some other behaviour policy, and learns with\ngradient-based temporal difference learning methods, with constant time and memory complexity\nper time step.\n4.2.2\nUNSUPERVISED AUXILIARY LEARNING\nEnvironments may contain abundant possible training signals, which may help to expedite achieving\nthe main goal of maximizing the accumulative rewards, e.g., pixel changes may imply important\nevents, and auxiliary reward tasks may help to achieve a good representation of rewarding states.\nThis may be even helpful when the extrinsic rewards are rarely observed.\nJaderberg et al. (2017) proposed UNsupervised REinforcement and Auxiliary Learning (UNREAL)\nto improve learning efﬁciency by maximizing pseudo-reward functions, besides the usual cumulative\nreward, while sharing a common representation. UNREAL is composed of RNN-LSTM base agent,\npixel control, reward prediction, and value function replay. The base agent is trained on-policy with\nA3C (Mnih et al., 2016). Experiences of observations, rewards and actions are stored in a reply\nbuffer, for being used by auxiliary tasks. The auxiliary policies use the base CNN and LSTM,\ntogether with a deconvolutional network, to maximize changes in pixel intensity of different regions\nof the input images. The reward prediction module predicts short-term extrinsic reward in next\nframe by observing the last three frames, to tackle the issue of reward sparsity. Value function\nreplay further trains the value function. UNREAL improved A3C’s performance on Atari games,\nand performed well on 3D Labyrinth game. UNREAL has a shared representation among signals,\n24\nwhile Horde trains each value function separately with distinct weights. See Deepmind’s description\nof UNREAL at https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/.\nWe discuss robotics navigation with similar unsupervised auxiliary learning (Mirowski et al., 2017)\nin Section 5.2. See also Lample and Chaplot (2017).\n4.2.3\nGENERATIVE ADVERSARIAL NETWORKS\nGoodfellow et al. (2014) proposed generative adversarial nets (GANs) to estimate generative models\nvia an adversarial process by training two models simultaneously, a generative model G to capture\nthe data distribution, and a discriminative model D to estimate the probability that a sample comes\nfrom the training data but not the generative model G.\nGoodfellow et al. (2014) modelled G and D with multilayer perceptrons: G(z : θg) and D(x : θd),\nwhere θg and θd are parameters, x are data points, and z are input noise variables. Deﬁne a prior on\ninput noise variable pz(z). G is a differentiable function and D(x) outputs a scalar as the probability\nthat x comes from the training data rather than pg, the generative distribution we want to learn.\nD will be trained to maximize the probability of assigning labels correctly to samples from both\ntraining data and G. Simultaneously, G will be trained to minimize such classiﬁcation accuracy,\nlog(1 −D(G(z))). As a result, D and G form the two-player minimax game as follows:\nmin\nG max\nD Ex∼pdata(x)[log D(x)] + Ez∼pz(z)[log(1 −D(G(z)))]\nGoodfellow et al. (2014) showed that as G and D are given enough capacity, generative adversarial\nnets can recover the data generating distribution, and provided a training algorithm with backpropa-\ngation by minibatch stochastic gradient descent.\nSee Goodfellow (2017) for Ian Goodfellow’s summary of his NIPS 2016 Tutorial on GANs. GANs\nhave received much attention and many works have been appearing after the tutorial.\nGANs are notoriously hard to train. See Arjovsky et al. (2017) for Wasserstein GAN (WGAN) as a\nstable GANs model. Gulrajani et al. (2017) proposed to improve stability of WGAN by penalizing\nthe norm of the gradient of the discriminator with respect to its input, instead of clipping weights as\nin Arjovsky et al. (2017). Mao et al. (2016) proposed Least Squares GANs (LSGANs), another sta-\nble model. Berthelot et al. (2017) proposed BEGAN to improve WGAN by an equilibrium enforcing\nmodel, and set a new milestone in visual quality for image generation. Bellemare et al. (2017) pro-\nposed Cram´er GAN to satisfy three machine learning properties of probability divergences: sum\ninvariance, scale sensitivity, and unbiased sample gradients. Hu et al. (2017) uniﬁed GANs and\nVariational Autoencoders (VAEs).\nWe discuss imitation learning with GANs in Section 3.3, including generative adversarial imitation\nlearning, and third person imitation learning. Finn et al. (2016a) established a connection between\nGANs, inverse RL, and energy-based models. Pfau and Vinyals (2016) established the connection\nbetween GANs and actor-critic algorithms. See an answer on Quora, http://bit.ly/2sgtpx8, by Prof\nSridhar Mahadevan.\n4.3\nTRANSFER LEARNING\nTransfer learning is about transferring knowledge learned from different domains, possibly with\ndifferent feature spaces and/or different data distributions (Taylor and Stone, 2009; Pan and Yang,\n2010; Weiss et al., 2016). As reviewed in Pan and Yang (2010), transfer learning can be inductive,\ntransductive, or unsupervised; inductive transfer learning includes self-taught learning and multi-\ntask learning; and transductive transfer learning includes domain adaptation and sample selection\nbias/covariance shift.\nBousmalis et al. (2017)\nhttps://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html\nGupta et al. (2017a) formulated the multi-skill problem for two agents to learn multiple skills, de-\nﬁned the common representation using which to map states and to project the execution of skills, and\n25\ndesigned an algorithm for two agents to transfer the informative feature space maximally to trans-\nfer new skills, with similarity loss metric, autoencoder, and reinforcement learning. The authors\nvalidated their proposed approach with two simulated robotic manipulation tasks.\nSee also recent work in transfer learning e.g., Andreas et al. (2017); Dong et al. (2015); Ganin et al.\n(2016); Kaiser et al. (2017a); Kansky et al. (2017); Long et al. (2015; 2016); Maurer et al. (2016);\nMo et al. (2016); Parisotto et al. (2016); Papernot et al. (2017); P´erez-D’Arpino and Shah (2017);\nRajendran et al. (2017); Whye Teh et al. (2017); Yosinski et al. (2014). See Ruder (2017) for an\noverview about multi-task learning. See NIPS 2015 Transfer and Multi-Task Learning: Trends and\nNew Perspectives Workshop.\nLong et al. (2017)\nKillian et al. (2017)\nBarreto et al. (2017)\nMcCann et al. (2017)\n4.4\nMULTI-AGENT REINFORCEMENT LEARNING\nMulti-agent RL (MARL) is the integration of multi-agent systems (Shoham and Leyton-Brown,\n2009; Stone and Veloso, 2000) with RL, thus it is at the intersection of game theory (Leyton-Brown\nand Shoham, 2008) and RL/AI communities. Besides issues in RL like convergence and curse-of-\ndimensionality, there are new issues like multiple equilibria, and even fundamental issues like what\nis the question for multi-agent learning, whether convergence to an equilibrium is an appropriate\ngoal, etc. Consequently, multi-agent learning is challenging both technically and conceptually, and\ndemands clear understanding of the problem to be solved, the criteria for evaluation, and coherent\nresearch agendas (Shoham et al., 2007).\nMulti-agent systems have many applications, e.g., as we will discuss, games in Section 5.1, robotics\nin Section 5.2, Smart Grid in Section 5.10, Intelligent Transportation Systems in Section 5.11, and\ncompute systems in Section 5.12.\nBusoniu et al. (2008) surveyed works in multi-agent RL. There are several recent works, about new\ndeep MARL algorithms (Foerster et al., 2018; Foerster et al., 2017; Lowe et al., 2017; Omidshaﬁei\net al., 2017), new communication mechanisms in MARL (Foerster et al., 2016; Sukhbaatar et al.,\n2016), and sequential social dilemmas with MARL (Leibo et al., 2017).\nBansal et al. (2017)\nAl-Shedivat et al. (2017a)\nGhavamzadeh et al. (2006)\nFoerster et al. (2017)\nPerolat et al. (2017)\nLanctot et al. (2017)\nHadﬁeld-Menell et al. (2016)\nHadﬁeld-Menell et al. (2017)\nMhamdi et al. (2017)\nLowe et al. (2017)\nHoshen (2017)\n4.5\nHIERARCHICAL REINFORCEMENT LEARNING\nHierarchical RL is a way to learn, plan, and represent knowledge with spatio-temporal abstraction\nat multiple levels. Hierarchical RL is an approach for issues of sparse rewards and/or long hori-\nzons (Sutton et al., 1999; Dietterich, 2000; Barto and Mahadevan, 2003).\n26\nVezhnevets et al. (2016) proposed strategic attentive writer (STRAW), a deep recurrent neural net-\nwork architecture, for learning high-level temporally abstracted macro-actions in an end-to-end man-\nner based on observations from the environment. Macro-actions are sequences of actions commonly\noccurring. STRAW builds a multi-step action plan, updated periodically based on observing re-\nwards, and learns for how long to commit to the plan by following it without replanning. STRAW\nlearns to discover macro-actions automatically from data, in contrast to the manual approach in pre-\nvious work. Vezhnevets et al. (2016) validated STRAW on next character prediction in text, 2D\nmaze navigation, and Atari games.\nKulkarni et al. (2016) proposed hierarchical-DQN (h-DQN) by organizing goal-driven intrinsically\nmotivated deep RL modules hierarchically to work at different time-scales. h-DQN integrates a\ntop level action value function and a lower level action value function; the former learns a policy\nover intrinsic sub-goals, or options (Sutton et al., 1999); the latter learns a policy over raw actions\nto satisfy given sub-goals. In a hard Atari game, Montezuma’s Revenge, h-DQN outperformed\nprevious methods, including DQN and A3C.\nFlorensa et al. (2017) proposed to pre-train a large span of skills using Stochastic Neural Networks\nwith an information-theoretic regularizer, then on top of these skills, to train high-level policies\nfor downstream tasks. Pre-training is based on a proxy reward signal, which is a form of intrinsic\nmotivation to explore agent’s own capabilities; its design requires minimal domain knowledge about\nthe downstream tasks. Their method combined hierarchical methods with intrinsic motivation, and\nthe pre-training follows an unsupervised way.\nTessler et al. (2017) proposed a hierarchical deep RL network architecture for lifelong learning.\nReusable skills, or sub-goals, are learned to transfer knowledge to new tasks. The authors tested\ntheir approach on the game of Minecraft.\nSee also Bacon et al. (2017), Kompella et al. (2017), Machado et al. (2017), Peng et al. (2017a),\nSchaul et al. (2015), Sharma et al. (2017), Vezhnevets et al. (2017), Yao et al. (2014). See a survey\non hierarchical RL (Barto and Mahadevan, 2003).\nHarutyunyan et al. (2018)\n4.6\nLEARNING TO LEARN\nLearning to learn, also know as meta-learning, is about learning to adapt rapidly to new tasks. It\nis related to transfer learning, multi-task learning, representation learning, and one/few/zero-shot\nlearning. We can also see hyper-parameter learning and neural architecture design as learning to\nlearn. It is a core ingredient to achieve strong AI (Lake et al., 2016).\nhypermarameter tuning, e.g., Jaderberg et al. (2017)\nSutton (1992)\n4.6.1\nLEARNING TO LEARN/OPTIMIZE\nLi and Malik (2017) proposed to automate unconstrained continuous optimization algorithms with\nguided policy search (Levine et al., 2016a) by representing a particular optimization algorithm as a\npolicy, and convergence rate as reward. See also Andrychowicz et al. (2016).\nDuan et al. (2016) and Wang et al. (2016) proposed to learn a ﬂexible RNN model to handle a family\nof RL tasks, to improve sample efﬁciency, learn new tasks in a few samples, and beneﬁt from prior\nknowledge.\ncombinatorial optimization, e.g., Vinyals et al. (2015), Bello et al. (2016), Dai et al. (2017)\nXu et al. (2017)\nSmith et al. (2017)\nLi and Malik (2017)\n27\n4.6.2\nZERO/ONE/FEW-SHOT LEARNING\nLake et al. (2015) proposed an one-shot concept learning model, for handwritten characters in par-\nticular, with probabilistic program induction. Koch et al. (2015) proposed siamese neural networks\nwith metric learning for one-shot image recognition. Vinyals et al. (2016) designed matching net-\nworks for one-shot classiﬁcation. Duan et al. (2017) proposed a model for one-shot imitation learn-\ning with attention for robotics. Ravi and Larochelle (2017) proposed a meta-learning model for few\nshot learning. Johnson et al. (2016) presented zero-shot translation for Google’s multilingual neural\nmachine translation system. Kaiser et al. (2017b) designed a large scale memory module for life-\nlong one-shot learning to remember rare events. Kansky et al. (2017) proposed Schema Networks\nfor zero-shot transfer with a generative causal model of intuitive physics. Snell et al. (2017) pro-\nposed prototypical networks for few/zero-shot classiﬁcation by learning a metric space to compute\ndistances to prototype representations of each class.\n4.6.3\nNEURAL ARCHITECTURE DESIGN\nNeural networks architecture design is a notorious, nontrivial engineering issue. Neural architecture\nsearch provides a promising avenue to explore.\nZoph and Le (2017) proposed the neural architecture search to generate neural networks architec-\ntures with an RNN trained by RL, in particular, REINFORCE, searching from scratch in variable-\nlength architecture space, to maximize the expected accuracy of the generated architectures on a\nvalidation set. In the RL formulation, a controller generates hyperparameters as a sequence of to-\nkens, which are actions chosen from hyperparameters spaces; each gradient update to the policy\nparameters corresponds to training one generated network to convergence; an accuracy on a valida-\ntion set is the reward signal. The neural architecture search can generate convolutional layers, with\nskip connections or branching layers, and recurrent cell architecture. The authors designed a param-\neter server approach to speed up training. Comparing with state-of-the-art methods, the proposed\napproach achieved competitive results for an image classiﬁcation task with CIFAR-10 dataset; and\nbetter results for a language modeling task with Penn Treebank.\nZoph et al. (2017) proposed to transfer the architectural building block learned with the neural archi-\ntecture search (Zoph and Le, 2017) on small dataset to large dataset for scalable image recognition.\nBaker et al. (2017) proposed a meta-learning approach, using Q-learning with ϵ-greedy exploration\nand experience replay, to generate CNN architectures automatically for a given learning task. Zhong\net al. (2017) proposed to construct network blocks to reduce the search space of network design,\ntrained by Q-learning. See also Bello et al. (2017).\nThere are recent works exploring new neural architectures. Kaiser et al. (2017a) proposed to train a\nsingle model, MultiModel, which is composed of convolutional layers, an attention mechanism, and\nsparsely-gated layers, to learn multiple tasks from various domains, including image classiﬁcation,\nimage captioning and machine translation. Vaswani et al. (2017) proposed a new achichitecture for\ntranslation that replaces CNN and RNN with attention and positional encoding. Wang et al. (2016b)\nproposed the dueling network architecture to estimate state value function and associated advantage\nfunction, to combine them to estimate action value function for faster convergence. Tamar et al.\n(2016) introduced Value Iteration Networks, a fully differentiable CNN planning module to approx-\nimate the value iteration algorithm, to learn to plan. Silver et al. (2016b) proposed the predictron\nto integrate learning and planning into one end-to-end training procedure with raw input in Markov\nreward process.\nLiu et al. (2017)\nLiu et al. (2017)\n5\nAPPLICATIONS\nReinforcement learning has a wide range of applications. We discuss games in Section 5.1 and\nrobotics in Section 5.2, two classical RL application areas. Games will still be important testbeds\nfor RL/AI. Robotics will be critical in the era of AI. Next we discuss natural language processing\nin Section 5.3, which enjoys wide and deep applications of RL recently. Computer vision follows\nin Section 5.4, in which, there are efforts for integration of vision and language. Combinatorial\n28\nFigure 2: Deep RL Applications\noptimization including neural architecture design in Section ?? is an exciting application of RL. In\nSection 5.5, we discuss business management, like ads, recommendation, customer management,\nand marketing. We discuss ﬁnance in Section 5.6. Business and ﬁnance have natural problems for\nRL. We discuss healthcare in Section 5.7, which receives much attention recently, esp. after the\nsuccess of deep learning. We discuss Industry 4.0 in Section 5.9. Many countries have made plans\nto integrate AI with manufacturing. We discuss smart grid in Section 5.10, intelligent transportation\nsystems in Section 5.11, and computer systems in Section 5.12. There are optimization and control\nproblems in these areas, and many of them are concerned with networking and graphs. These appli-\ncation areas may overlap with each other, e.g., a robot may need skills for many of the application\nareas. We present deep RL applications brieﬂy in Figure 2.\nRL is usually for sequential decision making problems. However, some problems, seemingly non-\nsequential on surface, like machine translation and neural network architecture design, have been\napproached by RL. RL applications abound; and creativity would be the boundary.\nReinforcement learning is widely used in operations research (Powell, 2011), e.g., supply chain,\ninventory management, resource management, etc; we do not list it as an application area — it is\nimplicitly a component in application areas like intelligent transportation system and Industry 4.0.\nWe do not list smart city, an important application area of AI, as it includes several application\nareas here: healthcare, intelligent transportation system, smart grid, etc. We do not discuss some\ninteresting applications, like music generation (Briot et al., 2017; Jaques et al., 2017), and retrosyn-\nthesis (Segler et al., 2017). See previous work on lists of RL applications at: http://bit.ly/2pDEs1Q,\nand http://bit.ly/2rjsmaz. We may only touch the surface of some application areas. It is desirable to\ndo a deeper analysis of all application areas listed in the following, which we leave as a future work.\n29\n5.1\nGAMES\nGames provide excellent testbeds for RL/AI algorithms. We discuss Deep Q-Network (DQN) in\nSection 3.1.1 and its extensions, all of which experimented with Atari games. We discuss Mnih\net al. (2016) in Section 3.2.1, Jaderberg et al. (2017) in Section 4.2, and Mirowski et al. (2017)\nin Section 5.2, and they used Labyrinth as the testbed. See Yannakakis and Togelius (2018) for a\nbook on artiﬁcial intelligence and games. We discuss multi-agent RL in Section 4.4, which is at the\nintersection of game theory and RL/AI.\nBackgammon and computer Go are perfect information board games. In Section 5.1.1, we dis-\ncuss brieﬂy Backgammon, and focus on computer Go, in particular, AlphaGo. Variants of card\ngames, including majiang/mahjong, are imperfect information board games, which we discuss in\nSection 5.1.2, and focus on Texas Hold’em Poker. In video games, information may be perfect or\nimperfect, and game theory may be deployed or not. We discuss video games in Section 5.1.3. We\nwill see more achievements in imperfect information games and video games, and their applications.\n5.1.1\nPERFECT INFORMATION BOARD GAMES\nBoard games like Backgammon, Go, chess, checker and Othello, are classical testbeds for RL/AI al-\ngorithms. In such games, players reveal prefect information. Tesauro (1994) approached Backgam-\nmon by using neural networks to approximate value function learned with TD learning, and achieved\nhuman level performance. We focus on computer Go, in particular, AlphaGo (Silver et al., 2016a;\n2017), for its signiﬁcance.\nCOMPUTER GO\nThe challenge of solving Computer Go comes from not only the gigantic search space of size 250150,\nan astronomical number, but also the hardness of position evaluation (M¨uller, 2002), which was\nsuccessfully used in solving many other games, like Backgammon and chess.\nAlphaGo (Silver et al., 2016a), a computer Go program, won the human European Go champion, 5\ngames to 0, in October 2015, and became the ﬁrst computer Go program to won a human profes-\nsional Go player without handicaps on a full-sized 19 × 19 board. Soon after that in March 2016,\nAlphaGo defeated Lee Sedol, an 18-time world champion Go player, 4 games to 1, making headline\nnews worldwide. This set a landmark in AI. AlphaGo defeated Ke Jie 3:0 in May 2017. AlphaGo\nZero (Silver et al., 2017) further improved previous versions by learning a superhuman computer\nGo program without human knowledge.\nALPHAGO: TRAINING PIPELINE AND MCTS\nWe discuss brieﬂy how AlphaGo works based on Silver et al. (2016a) and Sutton and Barto (2018).\nSee Sutton and Barto (2018) for a detailed and intuitive description of AlphaGo. See Deepmind’s\ndescription of AlphaGo at goo.gl/lZoQ1d.\nAlphaGo was built with techniques of deep convolutional neural networks, supervised learning,\nreinforcement learning, and Monte Carlo tree search (MCTS) (Browne et al., 2012; Gelly and Silver,\n2007; Gelly et al., 2012). AlphaGo is composed of two phases: neural network training pipeline and\nMCTS. The training pipeline phase includes training a supervised learning (SL) policy network from\nexpert moves, a fast rollout policy, a RL policy network, and a RL value network.\nThe SL policy network has convolutional layers, ReLU nonlinearities, and an output softmax layer\nrepresenting probability distribution over legal moves. The inputs to the CNN are 19 × 19 × 48\nimage stacks, where 19 is the dimension of a Go board and 48 is the number of features. State-\naction pairs are sampled from expert moves to train the network with stochastic gradient ascent to\nmaximize the likelihood of the move selected in a given state. The fast rollout policy uses a linear\nsoftmax with small pattern features.\nThe RL policy network improves SL policy network, with the same network architecture, and the\nweights of SL policy network as initial weights, and policy gradient for training. The reward function\nis +1 for winning and -1 for losing in the terminal states, and 0 otherwise. Games are played between\nthe current policy network and a random, previous iteration of the policy network, to stabilize the\n30\nlearning and to avoid overﬁtting. Weights are updated by stochastic gradient ascent to maximize the\nexpected outcome.\nThe RL value network still has the same network architecture as SL policy network, except the out-\nput is a single scalar predicting the value of a position. The value network is learned in a Monte\nCarlo policy evaluation approach. To tackle the overﬁtting problem caused by strongly correlated\nsuccessive positions in games, data are generated by self-play between the RL policy network and\nitself until game termination. The weights are trained by regression on state-outcome pairs, us-\ning stochastic gradient descent to minimize the mean squared error between the prediction and the\ncorresponding outcome.\nIn MCTS phase, AlphaGo selects moves by lookahead search. It builds a partial game tree starting\nfrom the current state, in the following stages: 1) select a promising node to explore further, 2)\nexpand a leaf node guided by the SL policy network and collected statistics, 3) evaluate a leaf node\nwith a mixture of the RL value network and the rollout policy, 4) backup evaluations to update the\naction values. A move is then selected.\nALPHAGO ZERO\nAlphaGo Zero can be understood as an approximation policy iteration, incorporating MCTS inside\nthe training loop to perform both policy improvement and policy evaluation. MCTS may be regarded\nas a policy improvement operator. It outputs move probabilities stronger than raw probabilities of\nthe neural network. Self-play with search may be regarded as a policy evaluation operator. It uses\nMCTS to select moves, and game winners as samples of value function. Then the policy iteration\nprocedure updates the neural network’s weights to match the move probabilities and value more\nclosely with the improved search probabilities and self-play winner, and conduct self-play with\nupdated neural network weights in the next iteration to make the search stronger.\nThe features of AlphaGo Zero (Silver et al., 2017), comparing with AlphaGo (Silver et al., 2016a),\nare: 1) it learns from random play, with self-play reinforcement learning, without human data or\nsupervision; 2) it uses black and white stones from the board as input, without any manual feature\nengineering; 3) it uses a single neural network to represent both policy and value, rather than separate\npolicy network and value network; and 4) it utilizes the neural network for position evaluation and\nmove sampling for MCTS, and it does not perform Monte Carlo rollouts. AlphaGo Zero deploys\nseveral recent achievements in neural networks: residual convolutional neural networks (ResNets),\nbatch normalization, and rectiﬁer nonlinearities.\nAlphaGo Zero has three main components in its self-play training pipeline executed in parallel asyn-\nchronously: 1) optimize neural network weights from recent self-play data continually; 2) evaluate\nplayers continually; 3) use the strongest player to generate new self-play data.\nWhen AlphaGo Zero playing a game against an opponent, MCTS searches from the current state,\nwith the trained neural network weights, to generate move probabilities, and then selects a move.\nWe present a brief, conceptual pseudo code in Algorithm 9 for training in AlphaGo Zero, conducive\nfor easier understanding. Refer to the original paper (Silver et al., 2017) for details.\nSilver et al. (2017)\nDISCUSSIONS\nAlphaGo Zero is a reinforcement learning algorithm. It is neither supervised learning nor unsu-\npervised learning. The game score is a reward signal, not a supervision label. Optimizing the loss\nfunction l is supervised learning. However, it performs policy evaluation and policy improvement,\nas one iteration in policy iteration.\nAlphaGo Zero is not only a heuristic search algorithm. AlphaGo Zero is a policy iteration proce-\ndure, in which, heuristic search, in particular, MCTS, plays a critical role, but within the scheme of\nreinforcement learning policy iteration, as illustrated in the pseudo code in Algorithm 9. MCTS can\nbe viewed as a policy improvement operator.\n31\nInput: the raw board representation of the position, its history, and the colour to play as 19 × 19\nimages; game rules; a game scoring function; invariance of game rules under rotation and\nreﬂection, and invariance to colour transposition except for komi\nOutput: policy (move probabilities) p, value v\ninitialize neural network weights θ0 randomly\n//AlphaGo Zero follows a policy iteration procedure\nfor each iteration i do\n// termination conditions:\n// 1. both players pass\n// 2. the search value drops below a resignation threshold\n// 3. the game exceeds a maximum length\ninitialize s0\nfor each step t, until termination at step T do\n// MCTS can be viewed as a policy improvement operator\n// search algorithm: asynchronous policy and value MCTS algorithm (APV-MCTS)\n// execute an MCTS search πt = αθi−1(st) with previous neural network fθi−1\n// each edge (s, a) in the search tree stores a prior probability P(s, a), a visit count N(s, a),\nand an action value Q(s, a)\nwhile computational resource remains do\nselect: each simulation traverses the tree by selecting the edge with maximum upper\nconﬁdence bound Q(s, a) + U(s, a), where U(s, a) ∝P(s, a)/(1 + N(s, a))\nexpand and evaluate: the leaf node is expanded and the associated position s is\nevaluated by the neural network, (P(s, ·), V (s)) = fθi(s); the vector of P values are\nstored in the outgoing edges from s\nbackup: each edge (s, a) traversed in the simulation is updated to increment its visit\ncount N(s, a), and to update its action value to the mean evaluation over these\nsimulations, Q(s, a) = 1/N(s, a) P\ns′|s,a→s′ V (s′), where s′|s, a →s′ indicates that\na simulation eventually reached s′ after taking move a from position s\nend\n// self-play with search can be viewed as a policy evaluation operator: select each move\nwith the improved MCTS-based policy, uses the game winner as a sample of the value\nplay: once the search is complete, search probabilities π ∝N 1/τ are returned, where N is\nthe visit count of each move from root and τ is a parameter controlling temperature; play\na move by sampling the search probabilities πt, transition to next state st+1\nend\nscore the game to give a ﬁnal reward rT ∈{−1, +1}\nfor each step t in the last game do\nzt ←±rT , the game winner from the perspective of the current player\nstore data as (st, πt, zt)\nend\nsample data (s, π, z) uniformly among all time-steps of the last iteration(s) of self-play\n//train neural network weights θi\n//optimizing loss function l performs both policy evaluation, via (z −v)2, and policy\nimprovement, via −πT log p, in a single step\nadjust the neural network (p, v) = fθi(s):\nto minimize the error between the predicted value v and the self-play winner z, and\nto maximize similarity of neural network move probabilities p to search probabilities π\nspeciﬁcally, adjust the parameters θ by gradient descent on a loss function\n(p, v) = fθi(s) and l = (z −v)2 −πT log p + c∥θi∥2\nl sums over the mean-squared error and cross-entropy losses, respectively\nc is a parameter controlling the level of L2 weight regularization to prevent overﬁtting\nevaluate the checkpoint every 1000 training steps to decide if replacing the current best player\n(neural network weights) for generating next batch of self-play games\nend\nAlgorithm 9: AlphaGo Zero training pseudo code, based on Silver et al. (2017)\n32\nAlphaGo attains a superhuman level. It may conﬁrm that professionals have developed effective\nstrategies. However, it does not need to mimic professional plays. Thus it does not need to predict\ntheir moves correctly.\nThe inputs to AlphaGo Zero include the raw board representation of the position, its history, and the\ncolour to play as 19 × 19 images; game rules; a game scoring function; invariance of game rules\nunder rotation and reﬂection, and invariance to colour transposition except for komi. An additional\nand critical input is solid research and development experiences.\nAlphaGo Zero utilized 64 GPU workers (each maybe with multiple GPUs) and 19 CPU parameter\nservers (each with multiple CPUs) for training, around 2000 TPUs for data generation, and 4 TPUs\nfor game playing. The computation cost is too formidable for replicating AlphaGo Zero.\nAlphaGo requires huge amount of data for training, so it is still a big data issue. However, the data\ncan be generated by self play, with a perfect model or precise game rules.\nDue to the perfect model or precise game rules for computer Go, AlphaGo algorithms have their\nlimitations. For example, in healthcare, robotics and self driving problems, it is usually hard to\ncollect a large amount of data, and it is hard or impossible to have a close enough or even perfect\nmodel. As such, it is nontrivial to directly apply AlphaGo algorithms to such applications.\nOn the other hand, AlphaGo algorithms, especially, the underlying techniques, namely, deep learn-\ning, reinforcement learning, and Monte Carlo tree search, have many applications. Silver et al.\n(2016a) and Silver et al. (2017) recommended the following applications: general game-playing\n(in particular, video games), classical planning, partially observed planning, scheduling, constraint\nsatisfaction, robotics, industrial control, and online recommendation systems. AlphaGo Zero blog\nmentioned the following structured problems: protein folding, reducing energy consumption, and\nsearching for revolutionary new materials.2\nAlphaGo has made tremendous progress, and set a landmark in AI. However, we are still far away\nfrom attaining artiﬁcial general intelligence (AGI).\nIt is interesting to see how strong a raw deep neural network in AlphaGo can become, and how soon\na very strong computer Go program would be available on a mobile phone.\n5.1.2\nIMPERFECT INFORMATION BOARD GAMES\nImperfect information games, or game theory in general, have many applications, e.g., security and\nmedical decision support. It is interesting to see more progress of deep RL in such applications, and\nthe full version of Texas Hold’em.\nHeinrich and Silver (2016) proposed Neural Fictitious Self-Play (NFSP) to combine ﬁctitious self-\nplay with deep RL to learn approximate Nash equilibria for games of imperfect information in a\nscalable end-to-end approach without prior domain knowledge. NFSP was evaluated on two-player\nzero-sum games. In Leduc poker, NFSP approached a Nash equilibrium, while common RL methods\ndiverged. In Limit Texas Hold’em, a real-world scale imperfect-information game, NFSP performed\nsimilarly to state-of-the-art, superhuman algorithms which are based on signiﬁcant domain expertise.\nHeads-up Limit Hold’em Poker was essentially solved (Bowling et al., 2015) with counterfactual\nregret minimization (CFR), which is an iterative method to approximate a Nash equilibrium of an\nextensive-form game with repeated self-play between two regret-minimizing algorithms.\n2Andrej Karpathy posted a blog titled ”AlphaGo, in context”, after AlphaGo defeated Ke Jie in May 2017.\nHe characterized properties of Computer Go as: fully deterministic, fully observable, discrete action space,\naccessible perfect simulator, relatively short episode/game, clear and fast evaluation conducive for many trail-\nand-errors, and huge datasets of human play games, to illustrate the narrowness of AlphaGo. It is true that\ncomputer Go has limitations in the problem setting and thus potential applications, and is far from artiﬁcial\ngeneral intelligence. However, we see the success of AlphaGo as the triumph of AI, in particular, AlphaGo’s\nunderlying techniques, i.e., learning from demonstration (as supervised learning), deep learning, reinforcement\nlearning, and Monte Carlo tree search; these techniques are present in many recent achievements in AI. As\na whole technique, AlphaGo will probably shed lights on classical AI areas, like planning, scheduling, and\nconstraint satisfaction (Silver et al., 2016a), and new areas for AI, like retrosynthesis (Segler et al., 2017).\nReportedly, the success of AlphaGo’s conquering titanic search space inspired quantum physicists to solve the\nquantum many-body problem (Carleo and Troyer, 2017).\n33\nDEEPSTACK\nRecently, signiﬁcant progress has been made for Heads-up No-Limit Hold’em Poker (Moravˇc´ık\net al., 2017), the DeepStack computer program defeated professional poker players for the ﬁrst\ntime. DeepStack utilized the recursive reasoning of CFR to handle information asymmetry, focusing\ncomputation on speciﬁc situations arising when making decisions and use of value functions trained\nautomatically, with little domain knowledge or human expert games, without abstraction and ofﬂine\ncomputation of complete strategies as before.\n5.1.3\nVIDEO GAMES\nVideo games would be great testbeds for artiﬁcial general intelligence.\nWu and Tian (2017) deployed A3C with CNN to train an agent in a partially observable 3D envi-\nronment, Doom, from recent four raw frames and game variables, to predict next action and value\nfunction, following the curriculum learning (Bengio et al., 2009) approach of starting with simple\ntasks and gradually transition to harder ones. It is nontrivial to apply A3C to such 3D games directly,\npartly due to sparse and long term reward. The authors won the champion in Track 1 of ViZDoom\nCompetition by a large margin, and plan the following future work: a map from an unknown envi-\nronment, localization, a global plan to act, and visualization of the reasoning process.\nDosovitskiy and Koltun (2017) approached the problem of sensorimotor control in immersive en-\nvironments with supervised learning, and won the Full Deathmatch track of the Visual Doom AI\nCompetition. We list it here since it is usually a RL problem, yet it was solved with supervised\nlearning. Lample and Chaplot (2017) also discussed how to tackle Doom.\nPeng et al. (2017b) proposed a multiagent actor-critic framework, with a bidirectionally-coordinated\nnetwork to form coordination among multiple agents in a team, deploying the concept of dynamic\ngrouping and parameter sharing for better scalability. The authors used StarCraft as the testbed.\nWithout human demonstration or labelled data as supervision, the proposed approach learned strate-\ngies for coordination similar to the level of experienced human players, like move without collision,\nhit and run, cover attack, and focus ﬁre without overkill. Usunier et al. (2017); Justesen and Risi\n(2017) also studied StarCraft.\nOh et al. (2016) and Tessler et al. (2017) studied Minecraft, Chen and Yi (2017); Firoiu et al. (2017)\nstudied Super Smash Bros, and Kansky et al. (2017) proposed Schema Networks and empirically\nstudied variants of Breakout in Atari games.\nSee Justesen et al. (2017) for a survey about applying deep (reinforcement) learning to video games.\nSee Onta˜n´on et al. (2013) for a survey about Starcraft. Check AIIDE and CIG Starcraft AI Compe-\ntitions, and its history at https://www.cs.mun.ca/˜dchurchill/starcraftaicomp/history.shtml. See Lin\net al. (2017) for StarCraft Dataset.\n5.2\nROBOTICS\nRobotics is a classical area for reinforcement learning. See Kober et al. (2013) for a survey of RL in\nrobotics, Deisenroth et al. (2013) for a survey on policy search for robotics, and Argall et al. (2009)\nfor a survey of robot learning from demonstration. See the journal Science Robotics. It is interesting\nto note that from NIPS 2016 invited talk, Boston Dynamics robots did not use machine learning.\nIn the following, we discuss guided policy search (Levine et al., 2016a) and learn to navi-\ngate (Mirowski et al., 2017). See more recent robotics papers, e.g., Chebotar et al. (2016; 2017);\nDuan et al. (2017); Finn and Levine (2016); Gu et al. (2016a); Lee et al. (2017); Levine et al.\n(2016b); Mahler et al. (2017); P´erez-D’Arpino and Shah (2017); Popov et al. (2017); Yahya et al.\n(2016); Zhu et al. (2017b).\nWe recommend Pieter Abbeel’s NIPS 2017 Keynote Speech, Deep Learning for Robotics, slides at,\nhttps://www.dropbox.com/s/fdw7q8mx3x4wr0c/\n34\n5.2.1\nGUIDED POLICY SEARCH\nLevine et al. (2016a) proposed to train the perception and control systems jointly end-to-end, to map\nraw image observations directly to torques at the robot’s motors. The authors introduced guided\npolicy search (GPS) to train policies represented as CNN, by transforming policy search into su-\npervised learning to achieve data efﬁciency, with training data provided by a trajectory-centric RL\nmethod operating under unknown dynamics. GPS alternates between trajectory-centric RL and su-\npervised learning, to obtain the training data coming from the policy’s own state distribution, to\naddress the issue that supervised learning usually does not achieve good, long-horizon performance.\nGPS utilizes pre-training to reduce the amount of experience data to train visuomotor policies. Good\nperformance was achieved on a range of real-world manipulation tasks requiring localization, visual\ntracking, and handling complex contact dynamics, and simulated comparisons with previous policy\nsearch methods. As the authors mentioned, ”this is the ﬁrst method that can train deep visuomotor\npolicies for complex, high-dimensional manipulation skills with direct torque control”.\n5.2.2\nLEARN TO NAVIGATE\nMirowski et al. (2017) obtained the navigation ability by solving a RL problem maximizing cumu-\nlative reward and jointly considering un/self-supervised tasks to improve data efﬁciency and task\nperformance. The authors addressed the sparse reward issues by augmenting the loss with two\nauxiliary tasks, 1) unsupervised reconstruction of a low-dimensional depth map for representation\nlearning to aid obstacle avoidance and short-term trajectory planning; 2) self-supervised loop clo-\nsure classiﬁcation task within a local trajectory. The authors incorporated a stacked LSTM to use\nmemory at different time scales for dynamic elements in the environments. The proposed agent\nlearn to navigate in complex 3D mazes end-to-end from raw sensory input, and performed similarly\nto human level, even when start/goal locations change frequently.\nIn this approach, navigation is a by-product of the goal-directed RL optimization problem, in con-\ntrast to conventional approaches such as Simultaneous Localisation and Mapping (SLAM), where\nexplicit position inference and mapping are used for navigation. This may have the chance to replace\nthe popular SLAM, which usually requires manual processing.\n5.3\nNATURAL LANGUAGE PROCESSING\nIn the following we talk about natural language processing (NLP), dialogue systems in Section 5.3.1,\nmachine translation in Section 5.3.2, and text generation in Section 5.3.3. There are many interesting\nissues in NLP, and we list some in the following.\n• language tree-structure learning, e.g., Socher et al. (2011; 2013); Yogatama et al. (2017)\n• semantic parsing, e.g., Liang et al. (2017b)\n• question answering, e.g., Celikyilmaz et al. (2017), Shen et al. (2017), Trischler et al.\n(2016), Xiong et al. (2017a), and Wang et al. (2017a), Choi et al. (2017)\n• summarization, e.g., Paulus et al. (2017); Zhang and Lapata (2017)\n• sentiment analysis (Liu, 2012; Zhang et al., 2018), e.g., Radford et al. (2017)\n• information retrieval (Manning et al., 2008), e.g., Zhang et al. (2016), and Mitra and\nCraswell (2017)\n• information extraction, e.g., Narasimhan et al. (2016)\n• automatic query reformulation, e.g., Nogueira and Cho (2017)\n• language to executable program, e.g., Guu et al. (2017)\n• knowledge graph reasoning, e.g., Xiong et al. (2017c)\n• text games, e.g., Wang et al. (2016a), He et al. (2016b), and Narasimhan et al. (2015)\nDeep learning has been permeating into many subareas in NLP, and helping make signiﬁcant\nprogress. The above is a partial list. It appears that NLP is still a ﬁeld, more about synergy than\ncompetition, for deep learning vs. non-deep learning algorithms, and for approaches based on no\ndomain knowledge (end-to-end) vs linguistics knowledge. Some non-deep learning algorithms are\n35\neffective and perform well, e.g., word2vec (Mikolov et al., 2013; Mikolov et al., 2017) and fast-\nText (Joulin et al., 2017), and many works that study syntax and semantics of languages, see a\nrecent example in semantic role labeling (He et al., 2017). Some deep learning approaches to NLP\nproblems incorporate explicitly or implicitly linguistics knowledge, e.g., Socher et al. (2011; 2013);\nYogatama et al. (2017). See an article by Christopher D. Manning, titled ”Last Words: Computa-\ntional Linguistics and Deep Learning, A look at the importance of Natural Language Processing”,\nat http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning.\nMelis et al. (2017)\n5.3.1\nDIALOGUE SYSTEMS\nIn dialogue systems, conversational agents, or chatbots, human and computer interacts with natu-\nral language. We intentionally remove ”spoken” before ”dialogue systems” to accommodate both\nspoken and written language user interface (UI). Jurafsky and Martin (2017) categorize dialogue\nsystems as task-oriented dialog agents and chatbots; the former are set up to have short conversa-\ntions to help complete particular tasks; the latter are set up to mimic human-human interactions\nwith extended conversations, sometimes with entertainment value. As in Deng (2017), there are\nfour categories: social chatbots, infobots (interactive question answering), task completion bots\n(task-oriented or goal-oriented) and personal assistant bots. We have seen generation one dialogue\nsystems: symbolic rule/template based, and generation two: data driven with (shallow) learning. We\nare now experiencing generation three: data driven with deep learning, and reinforcement learning\nusually play an important role. A dialogue system usually include the following modules: (spoken)\nlanguage understanding, dialogue manager (dialogue state tracker and dialogue policy learning),\nand a natural language generation (Young et al., 2013). In task-oriented systems, there is usually a\nknowledge base to query. A deep learning approach, as usual, attempts to make the learning of the\nsystem parameters end-to-end. See Deng (2017) for more details. See a survey paper on applying\nmachine learning to speech recognition (Deng and Li, 2013).\nLi et al. (2017b) presented an end-to-end task-completion neural dialogue system with parameters\nlearned by supervised and reinforcement learning. The proposed framework includes a user sim-\nulator (Li et al., 2016d) and a neural dialogue system. The user simulator consists of user agenda\nmodelling and natural language generation. The neural dialogue system is composed of language\nunderstanding and dialogue management (dialogue state tracking and policy learning). The authors\ndeployed RL to train dialogue management end-to-end, representing the dialogue policy as a deep\nQ-network (Mnih et al., 2015), with the tricks of a target network and a customized experience re-\nplay, and using a rule-based agent to warm-start the system with supervised learning. The source\ncode is available at http://github.com/MiuLab/TC-Bot.\nDhingra et al. (2017) proposed KB-InfoBot, a goal-oriented dialogue system for multi-turn infor-\nmation access. KB-InfoBot is trained end-to-end using RL from user feedback with differentiable\noperations, including those for accessing external knowledge database (KB). In previous work, e.g.,\nLi et al. (2017b) and Wen et al. (2017), a dialogue system accesses real world knowledge from\nKB by symbolic, SQL-like operations, which is non-differentiable and disables the dialogue system\nfrom fully end-to-end trainable. KB-InfoBot achieved the differentiability by inducing a soft pos-\nterior distribution over the KB entries to indicate which ones the user is interested in. The authors\ndesigned a modiﬁed version of the episodic REINFORCE algorithm to explore and learn both the\npolicy to select dialogue acts and the posterior over the KB entries for correct retrievals.The authors\ndeployed imitation learning from rule-based belief trackers and policy to warm up the system.\nSu et al. (2016b) proposed an on-line learning framework to train the dialogue policy jointly with\nthe reward model via active learning with a Gaussian process model, to tackle the issue that it is\nunreliable and costly to use explicit user feedback as the reward signal. The authors showed em-\npirically that the proposed framework reduced manual data annotations signiﬁcantly and mitigated\nnoisy user feedback in dialogue policy learning.\nLi et al. (2016c) proposed to use deep RL to generate dialogues to model future reward for better\ninformativity, coherence, and ease of answering, to attempt to address the issues in the sequence\nto sequence models based on Sutskever et al. (2014): the myopia and misalignment of maximizing\nthe probability of generating a response given the previous dialogue turn, and the inﬁnite loop of\nrepetitive responses. The authors designed a reward function to reﬂect the above desirable properties,\n36\nand deployed policy gradient to optimize the long term reward. It would be interesting to investigate\nthe reward model with the approach in Su et al. (2016b) or with inverse RL and imitation learning\nas discussed in Section 3.3, although Su et al. (2016b) mentioned that such methods are costly, and\nhumans may not act optimally.\nSome recent papers follow: Asri et al. (2016), Bordes et al. (2017), Chen et al. (2016c), Eric and\nManning (2017), Fatemi et al. (2016), Kandasamy et al. (2017), Lewis et al. (2017), Li et al. (2016a),\nLi et al. (2017a), Li et al. (2017b), Lipton et al. (2016), Mesnil et al. (2015), Mo et al. (2016), Peng\net al. (2017a), Saon et al. (2016), Serban et al. (2017), Shah et al. (2016), She and Chai (2017),\nSu et al. (2016a), Weiss et al. (2017), Wen et al. (2015a), Wen et al. (2017), Williams and Zweig\n(2016), Williams et al. (2017), Xiong et al. (2017b), Xiong et al. (2017), Yang et al. (2016), Zhang\net al. (2017a), Zhang et al. (2017c), Zhao and Eskenazi (2016), Zhou et al. (2017). See Serban et al.\n(2015) for a survey of corpora for building dialogue systems.\nSee NIPS 2016 Workshop on End-to-end Learning for Speech and Audio Processing, and NIPS\n2015 Workshop on Machine Learning for Spoken Language Understanding and Interactions.\n5.3.2\nMACHINE TRANSLATION\nNeural machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al.,\n2014; Bahdanau et al., 2015) utilizes end-to-end deep learning for machine translation, and becomes\ndominant, against the traditional statistical machine translation techniques. The neural machine\ntranslation approach usually ﬁrst encodes a variable-length source sentence, and then decodes it to\na variable-length target sentence. Cho et al. (2014) and Sutskever et al. (2014) used two RNNs to\nencode a sentence to a ﬁx-length vector and then decode the vector into a target sentence. Bahdanau\net al. (2015) introduced the soft-attention technique to learn to jointly align and translate.\nHe et al. (2016a) proposed dual learning mechanism to tackle the data hunger issue in machine\ntranslation, inspired by the observation that the information feedback between the primal, translation\nfrom language A to language B, and the dual, translation from B to A, can help improve both\ntranslation models, with a policy gradient method, using the language model likelihood as the reward\nsignal. Experiments showed that, with only 10% bilingual data for warm start and monolingual\ndata, the dual learning approach performed comparably with previous neural machine translation\nmethods with full bilingual data in English to French tasks. The dual learning mechanism may\nhave extensions to many tasks, if the task has a dual form, e.g., speech recognition and text to\nspeech, image caption and image generation, question answering and question generation, search\nand keyword extraction, etc.\nSee Wu et al. (2016); Johnson et al. (2016) for Google’s Neural Machine Translation System;\nGehring et al. (2017) for convolutional sequence to sequence learning for fast neural machine trans-\nlation; Klein et al. (2017) for OpenNMT, an open source neural machine translation system; Cheng\net al. (2016) for semi-supervised learning for neural machine translation, and Wu et al. (2017c) for\nadversarial neural machine translation. See Vaswani et al. (2017) for a new approach for translation\nthat replaces CNN and RNN with attention and positional encoding. See Zhang et al. (2017b) for\nan open source toolkit for neural machine translation. See Monroe (2017) for a gentle introduction\nto translation.\nArtetxe et al. (2017)\n5.3.3\nTEXT GENERATION\nText generation is the basis for many NLP problems, like conversational response generation, ma-\nchine translation, abstractive summarization, etc.\nText generation models are usually based on n-gram, feed-forward neural networks, or recurrent\nneural networks, trained to predict next word given the previous ground truth words as inputs; then\nin testing, the trained models are used to generate a sequence word by word, using the generated\nwords as inputs. The errors will accumulate on the way, causing the exposure bias issue. Moreover,\nthese models are trained with word level losses, e.g., cross entropy, to maximize the probability of\nnext word; however, the models are evaluated on a different metrics like BLEU.\n37\nRanzato et al. (2016) proposed Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence\nprediction, with incremental learning and a loss function combining both REINFORCE and cross-\nentropy. MIXER is a sequence level training algorithm, aligning training and testing objective, such\nas BLEU, rather than predicting the next word as in previous works.\nBahdanau et al. (2017) proposed an actor-critic algorithm for sequence prediction, attempting to\nfurther improve Ranzato et al. (2016). The authors utilized a critic network to predict the value of a\ntoken, i.e., the expected score following the sequence prediction policy, deﬁned by an actor network,\ntrained by the predicted value of tokens. Some techniques are deployed to improve performance:\nSARSA rather than Monter-Carlo method to lessen the variance in estimating value functions; target\nnetwork for stability; sampling prediction from a delayed actor whose weights are updated more\nslowly than the actor to be trained, to avoid the feedback loop when actor and critic need to be\ntrained based on the output of each other; reward shaping to avoid the issue of sparse training signal.\nYu et al. (2017) proposed SeqGAN, sequence generative adversarial nets with policy gradient, inte-\ngrating the adversarial scheme in Goodfellow et al. (2014). Li et al. (2017a) proposed to improve\nsequence generation by considering the knowledge about the future.\n5.4\nCOMPUTER VISION\nComputer vision is about how computers gain understanding from digital images or videos. In the\nfollowing, after presenting background in computer vision, we discuss recognition, motion analysis,\nscene understanding, integration with NLP, and visual control.\nReinforcement learning would be an important ingredient for interactive perception (Bohg et al.,\n2017), where perception and interaction with the environment would be helpful to each other, in\ntasks like object segmentation, articulation model estimation, object dynamics learning and haptic\nproperty estimation, object recognition or categorization, multimodal object model learning, object\npose estimation, grasp planning, and manipulation skill learning.\nMore topics about applying deep RL to computer vision:\n• Liu et al. (2017) for semantic parsing of large-scale 3D point clouds;\n• Devrim Kaba et al. (2017) for view planning, which is a set cover problem;\n• Cao et al. (2017) for face hallucination, i.e., generating a high-resolution face image from\na low-resolution input image;\n• Brunner et al. (2018) for learning to read maps;\n• Bhatti et al. (2016) for SLAM-augmented DQN.\n5.4.1\nBACKGROUND\nTodo: AlexNet (Krizhevsky et al., 2012), ResNets (He et al., 2016d) DenseNets (Huang et al., 2017),\nFast R-CNN (Girshick, 2015), Faster R-CNN Ren et al. (2015), Mask R-CNN He et al. (2017),\nShrivastava et al. (2017), VAEs (variational autoencoder) (Diederik P Kingma, 2014)\nTodo: GANs (Goodfellow et al., 2014; Goodfellow, 2017); CycleGAN (Zhu et al., 2017a), Dual-\nGAN (Yi et al., 2017); See Arjovsky et al. (2017) for Wasserstein GAN (WGAN) as a stable GANs\nmodel. Gulrajani et al. (2017) proposed to improve stability of WGAN by penalizing the norm of\nthe gradient of the discriminator with respect to its input, instead of clipping weights as in Arjovsky\net al. (2017). Mao et al. (2016) proposed Least Squares GANs (LSGANs), another stable model.\nConnection with RL: Finn et al. (2016a) established a connection between GANs, inverse RL, and\nenergy-based models. Pfau and Vinyals (2016) established the connection between GANs and actor-\ncritic algorithms. Ho and Ermon (2016) and Li et al. (2017) studied the connection between GANs\nand imitation learning.\nautoencoder (Hinton and Salakhutdinov, 2006)\nFor disentangled factor learning, Kulkarni et al. (2015) proposed DC-IGN, the Deep Convolution\nInverse Graphics Network, which follows a semi-supervised way; and Chen et al. (2016a) proposed\nInfoGAN, an information-theoretic extension to the Generative Adversarial Network, which follows\n38\nan unsupervised way. Zhou et al. (2015) showed that object detectors emerge from learning to\nrecognize scenes, without supervised labels for objects.\nHiggins et al. (2017) proposed β-VAE to automatically discover interpretable, disentangled, fac-\ntorised, latent representations from raw images in an unsupervised way. The hyperparameter β\nbalances latent channel capacity and independence constraints with reconstruction accuracy. When\nβ = 1, β-VAE is the same as the original VAEs.\nEslami et al. (2016) proposed the framework of Attend-Infer-Repeat for efﬁcient inference in struc-\ntured image models to reason about objects explicitly. The authors deployed a recurrent neural\nnetwork to design an iterative process for inference, by attending to one object at a time, and for\neach image, learning an appropriate number of inference steps. The authors showed that, in an un-\nsupervised way, the proposed approach can learn generative models to identify multiple objects for\nboth 2D and 3D problems.\nZhang and Zhu (2018) surveyed visual interpretability for deep learning.\n5.4.2\nRECOGNITION\nRL can improve efﬁciency for image classiﬁcation by focusing only on salient parts. For visual\nobject localization and detection, RL can improve efﬁciency over approaches with exhaustive spatial\nhypothesis search and sliding windows, and strikes a balance between sampling more regions for\nbetter accuracy and stopping the search when sufﬁcient conﬁdence is obtained about the target’s\nlocation.\nMnih et al. (2014) introduced the recurrent attention model (RAM) to focus on selected sequence\nof regions or locations from an image or video for image classiﬁcation and object detection.\nThe authors used REINFORCE to train the model, to overcome the issue that the model is non-\ndifferentiable, and experimented on an image classiﬁcation task and a dynamic visual control prob-\nlem.\nCaicedo and Lazebnik (2015) proposed an active detection model for object localization with DQN,\nby deforming a bounding box with transformation actions to determine the most speciﬁc location\nfor target objects. Jie et al. (2016) proposed a tree-structure RL approach to search for objects se-\nquentially, considering both the current observation and previous search paths, by maximizing the\nlong-term reward associated with localization accuracy over all objects with DQN. Mathe et al.\n(2016) proposed to use policy search for visual object detection. Kong et al. (2017) deployed col-\nlaborative multi-agent RL with inter-agent communication for joint object search. Welleck et al.\n(2017) proposed a hierarchical visual architecture with an attention mechanism for multi-label im-\nage classiﬁcation. Rao et al. (2017) proposed an attention-aware deep RL method for video face\nrecognition.\nKrull et al. (2017) for 6D object pose estimation\n5.4.3\nMOTION ANALYSIS\nIn tracking, an agent needs to follow a moving object. Supanˇciˇc and Ramanan (2017) proposed\nonline decision-making process for tracking, formulated it as a partially observable decision-making\nprocess (POMDP), and learned policies with deep RL algorithms, to decide where to look for the\nobject, when to reinitialize, and when to update the appearance model for the object, where image\nframes may be ambiguous and computational budget may be constrained. Yun et al. (2017) also\nstudied visual tracking with deep RL.\nRhinehart and Kitani (2017) proposed Discovering Agent Rewards for K-futures Online (DARKO)\nto model and forecast ﬁrst-person camera wearer’s long-term goals, together with states, transitions,\nand rewards from streaming data, with online inverse reinforcement learning.\n5.4.4\nSCENE UNDERSTANDING\nWu et al. (2017b) studied the problem of scene understanding, and attempted to obtain a compact,\nexpressive, and interpretable representation to encode scene information like objects, their cate-\ngories, poses, positions, etc, in a semi-supervised way. In contrast to encoder-decoder based neural\n39\narchitectures as in previous works, Wu et al. (2017b) proposed to replace the decoder with a deter-\nministic rendering function, to map a structured and disentangled scene description, scene XML, to\nan image; consequently, the encoder transforms an image to the scene XML by inverting the ren-\ndering operation, a.k.a., de-rendering. The authors deployed a variant of REINFORCE algorithm to\novercome the non-differentiability issue of graphics rendering engines.\nWu et al. (2017a) proposed a paradigm with three major components, a convolutional perception\nmodule, a physics engine, and a graphics engine, to understand physical scenes without human an-\nnotations. The perception module recovers a physical world representation by inverting the graphics\nengine, inferring the physical object state for each segment proposal in input and combining them.\nThe generative physics and graphics engines then run forward with the world representation to re-\nconstruct the visual data. The authors showed results on both neural, differentiable and more mature\nbut non-differentiable physics engines.\nThere are recent works about physics learning, e.g., Agrawal et al. (2016); Battaglia et al. (2016);\nDenil et al. (2017); Watters et al. (2017); Wu et al. (2015).\n5.4.5\nINTEGRATION WITH NLP\nSome are integrating computer vision with natural language processing. Xu et al. (2015) integrated\nattention to image captioning, trained the hard version attention with REINFORCE, and showed\nthe effectiveness of attention on Flickr8k, Flickr30k, and MS COCO datasets. Rennie et al. (2017)\nintroduced self-critical sequence training, using the output of test-time inference algorithm as the\nbaseline in REINFORCE to normalize the rewards it experiences, for image captioning. See also\nLiu et al. (2016), Lu et al. (2016), and Ren et al. (2017) for image captioning. Strub et al. (2017)\nproposed end-to-end optimization with deep RL for goal-driven and visually grounded dialogue\nsystems for GuessWhat?! game. Das et al. (2017) proposed to learn cooperative Visual Dialog\nagents with deep RL. See also Kottur et al. (2017). See Pasunuru and Bansal (2017) for video\ncaptioning. See Liang et al. (2017d) for visual relationship and attribute detection.\n5.4.6\nVISUAL CONTROL\nVisual control is about deriving a policy from visual inputs, e.g., in games (Mnih et al., 2015; Silver\net al., 2016a; 2017; Oh et al., 2015; Wu and Tian, 2017; Dosovitskiy and Koltun, 2017; Lample\nand Chaplot, 2017; Jaderberg et al., 2017), robotics (Finn and Levine, 2016; Gupta et al., 2017b;\nLee et al., 2017; Levine et al., 2016a; Mirowski et al., 2017; Zhu et al., 2017b), and self-driving\nvehicles (Bojarski et al., 2016; Bojarski et al., 2017; Zhou and Tuzel, 2017). 3\n5.5\nBUSINESS MANAGEMENT\nReinforcement learning has many applications in business management, like ads, recommendation,\ncustomer management, and marketing.\nLi et al. (2010) formulated personalized news articles recommendation as a contextual bandit prob-\nlem, to learn an algorithm to select articles sequentially for users based on contextual information\nof the user and articles, such as historical activities of the user and descriptive information and cate-\ngories of content, and to take user-click feedback to adapt article selection policy to maximize total\nuser clicks in the long run.\nTheocharous et al. (2015) formulated a personalized ads recommendation systems as a RL problem\nto maximize life-time value (LTV) with theoretical guarantees. This is in contrast to a myopic\nsolution with supervised learning or contextual bandit formulation, usually with the performance\nmetric of click through rate (CTR). As the models are hard to learn, the authors deployed a model-\nfree approach to compute a lower-bound on the expected return of a policy to address the off-policy\nevaluation problem, i.e., how to evaluate a RL policy without deployment.\n3Although we include visual control here, it is not very clear if we should categorize the following type of\nproblems, e.g., DQN (Mnih et al., 2015) and AlphaGo (Silver et al., 2016a; 2017), into computer vision: pixels\n(DQN) or problem setting (Go board status) as the input, some deep neural networks as the architecture, and an\nend-to-end gradient descent/ascent algorithm as the optimization method to ﬁnd a policy, without any further\nknowledge of computer vision. Or we may see this as part of the synergy of computer vision and reinforcement\nlearning.\n40\nLi et al. (2015) also attempted to maximize lifetime value of customers. Silver et al. (2013) pro-\nposed concurrent reinforcement learning for the customer interaction problem. See Sutton and Barto\n(2018) for a detailed and intuitive description of some topics discussed here under the section title\nof Personalized Web Services.\n5.6\nFINANCE\nRL is a natural solution to some ﬁnance and economics problems (Hull, 2014; Luenberger, 1997),\nlike option pricing (Longstaff and Schwartz, 2001; Tsitsiklis and Van Roy, 2001; Li et al., 2009),\nand multi-period portfolio optimization (Brandt et al., 2005; Neuneier, 1997), where value function\nbased RL methods were used. Moody and Saffell (2001) proposed to utilize policy search to learn\nto trade; Deng et al. (2016) extended it with deep neural networks. Deep (reinforcement) learning\nwould provide better solutions in some issues in risk management (Hull, 2014; Yu et al., 2009). The\nmarket efﬁciency hypothesis is fundamental in ﬁnance. However, there are well-known behavioral\nbiases in human decision-making under uncertainty, in particular, prospect theory (Prashanth et al.,\n2016). A reconciliation is the adaptive markets hypothesis (Lo, 2004), which may be approached by\nreinforcement learning.\nIt is nontrivial for ﬁnance and economics academia to accept blackbox methods like neural networks;\nHeaton et al. (2016) may be regarded as an exception. However, there is a lecture in AFA 2017\nannual meeting: Machine Learning and Prediction in Economics and Finance. We may also be\naware that ﬁnancial ﬁrms would probably hold state-of-the-art research/application results.\nFinTech has been attracting attention, especially after the notion of big data. FinTech employs\nmachine learning techniques to deal with issues like fraud detection (Phua et al., 2010), consumer\ncredit risk (Khandani et al., 2010), etc.\n5.7\nHEALTHCARE\nThere are many opportunities and challenges in healthcare for machine learning (Miotto et al., 2017;\nSaria, 2014). Personalized medicine is getting popular in healthcare. It systematically optimizes the\npatient’s health care, in particular, for chronic conditions and cancers using individual patient infor-\nmation, potentially from electronic health/medical record (EHR/EMR). Dynamic treatment regimes\n(DTRs) or adaptive treatment strategies are sequential decision making problems. Some issues in\nDTRs are not in standard RL. Shortreed et al. (2011) tackled the missing data problem, and designed\nmethods to quantify the evidence of the learned optimal policy. Goldberg and Kosorok (2012) pro-\nposed methods for censored data (patients may drop out during the trial) and ﬂexible number of\nstages. See Chakraborty and Murphy (2014) for a recent survey, and Kosorok and Moodie (2015)\nfor an edited book about recent progress in DTRs. Currently Q-learning is the RL method in DTRs.\nLing et al. (2017) applied deep RL to the problem of inferring patient phenotypes.\nSome recent workshops at the intersection of machine learning and healthcare are:\nNIPS\n2016 Workshop on Machine Learning for Health (http://www.nipsml4hc.ws) and NIPS 2015\nWorkshop on Machine Learning in Healthcare (https://sites.google.com/site/nipsmlhc15/).\nSee\nICML 2017 Tutorial on Deep Learning for Health Care Applications: Challenges and Solutions\n(https://sites.google.com/view/icml2017-deep-health-tutorial/home).\n5.8\nEDUCATION\nMandel et al. (2014)\nLiu et al. (2014)\n5.9\nINDUSTRY 4.0\nThe ear of Industry 4.0 is approaching, e.g., see O’Donovan et al. (2015), and Preuveneers and\nIlie-Zudor (2017). Reinforcement learning in particular, artiﬁcial intelligence in general, will be\ncritical enabling techniques for many aspects of Industry 4.0, e.g., predictive maintenance, real-\ntime diagnostics, and management of manufacturing activities and processes. Robots will prevail in\nIndustry 4.0, and we discuss robotics in Section 5.2.\n41\nLiu and Tomizuka (2016; 2017) studied how to make robots and people to collaborate to achieve\nboth ﬂexibility and productivity in production lines. See a blog titled Towards Intelligent Industrial\nCo-robots, at http://bair.berkeley.edu/blog/2017/12/12/corobots/\nHein et al. (2017) designed a benchmark for the RL community to attempt to bridge the gap between\nacademic research and real industrial problems. Its open source based on OpenAI Gym is available\nat https://github.com/siemens/industrialbenchmark.\nSurana et al. (2016) proposed to apply guided policy search (Levine et al., 2016a) as discussed in\nSection 5.2.1 to optimize trajectory policy of cold spray nozzle dynamics, to handle complex trajec-\ntories traversing by robotic agents. The authors generated cold spray surface simulation proﬁles to\ntrain the model.\n5.10\nSMART GRID\nA smart grid is a power grid utilizing modern information technologies to create an intelligent elec-\ntricity delivery network for electricity generation, transmission, distribution, consumption, and con-\ntrol (Fang et al., 2012). An important aspect is adaptive control (Anderson et al., 2011). Glavic et al.\n(2017) reviewed application of RL for electric power system decision and control. Here we brieﬂy\ndiscuss demand response (Wen et al., 2015b; Ruelens et al., 2016).\nDemand response systems motivate users to dynamically adapt electrical demands in response to\nchanges in grid signals, like electricity price, temperature, and weather, etc. With suitable electricity\nprices, load of peak consumption may be rescheduled/lessened, to improve efﬁciency, reduce costs,\nand reduce risks. Wen et al. (2015b) proposed to design a fully automated energy management\nsystem with model-free reinforcement learning, so that it doesn’t need to specify a disutility function\nto model users’ dissatisfaction with job rescheduling. The authors decomposed the RL formulation\nover devices, so that the computational complexity grows linearly with the number of devices, and\nconducted simulations using Q-learning. Ruelens et al. (2016) tackled the demand response problem\nwith batch RL. Wen et al. (2015b) took the exogenous prices as states, and Ruelens et al. (2016)\nutilized the average as feature extractor to construct states.\n5.11\nINTELLIGENT TRANSPORTATION SYSTEMS\nIntelligent transportation systems (Bazzan and Kl¨ugl, 2014) apply advanced information technolo-\ngies for tackling issues in transport networks, like congestion, safety, efﬁciency, etc., to make trans-\nport networks, vehicles and users smart.\nAn important issue in intelligent transportation systems is adaptive trafﬁc signal control. El-Tantawy\net al. (2013) proposed to model the adaptive trafﬁc signal control problem as a multiple player\nstochastic game, and solve it with the approach of multi-agent RL (Shoham et al., 2007; Busoniu\net al., 2008). Multi-agent RL integrates single agent RL with game theory, facing challenges of\nstability, nonstationarity, and curse of dimensionality. El-Tantawy et al. (2013) approached the issue\nof coordination by considering agents at neighbouring intersections. The authors validated their\nproposed approach with simulations, and real trafﬁc data from the City of Toronto. El-Tantawy\net al. (2013) didn’t explore function approximation. See also van der Pol and Oliehoek (2017) for a\nrecent work, and Mannion et al. (2016) for an experimental review, about applying RL to adaptive\ntrafﬁc signal control.\nSelf-driving vehicle is also a topic of intelligent transportation systems. See Bojarski et al. (2016),\nBojarski et al. (2017), Zhou and Tuzel (2017).\nSee NIPS 2017, 2016 Workshop on Machine Learning for Intelligent Transportation Systems. Check\nfor a special issue of IEEE Transactions on Neural Networks and Learning Systems on Deep Re-\ninforcement Learning and Adaptive Dynamic Programming, tentative publication date December\n2017.\n5.12\nCOMPUTER SYSTEMS\nComputer systems are indispensable in our daily life and work, e.g., mobile phones, computers, and\ncloud computing. Control and optimization problems abound in computer systems, e,g., Mestres\n42\net al. (2016) proposed knowledge-deﬁned networks, Gavrilovska et al. (2013) reviewed learning and\nreasoning techniques in cognitive radio networks, and Haykin (2005) discussed issues in cognitive\nradio, like channel state prediction and resource allocation. We also note that Internet of Things\n(IoT)(Xu et al., 2014) and wireless sensor networks (Alsheikh et al., 2014) play an important role in\nIndustry 4.0 as discussed in Section 5.9, in Smart Grid as discussed in Section 5.10, and in Intelligent\nTransportation Systems as discussed in Section 5.11.\n5.12.1\nRESOURCE ALLOCATION\nMao et al. (2016) studied resource management in systems and networking with deep RL. The au-\nthors proposed to tackle multi-resource cluster scheduling with policy gradient, in an online manner\nwith dynamic job arrivals, optimizing various objectives like average job slowdown or completion\ntime. The authors validated their proposed approach with simulation.\nLiu et al. (2017) proposed a hierarchical framework to tackle resource allocation and power man-\nagement in cloud computing with deep RL. The authors decomposed the problem as a global tier\nfor virtual machines resource allocation and a local tier for servers power management. The au-\nthors validated their proposed approach with actual Google cluster traces. Such hierarchical frame-\nwork/decomposition approach was to reduce state/action space, and to enable distributed operation\nof power management.\nGoogle deployed machine learning for data centre power management, reducing energy consump-\ntion by 40%, https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/.\nOptimizing memory control is discussed in Sutton and Barto (2018).\n5.12.2\nPERFORMANCE OPTIMIZATION\nMirhoseini et al. (2017) proposed to optimize device placement for Tensorﬂow computational graphs\nwith RL. The authors deployed a seuqence-to-sequence model to predict how to place subsets of\noperations in a Tensorﬂow graph on available devices, using the execution time of the predicted\nplacement as reward signal for REINFORCE algorithm. The proposed method found placements\nof Tensorﬂow operations on devices for Inception-V3, recurrent neural language model and neural\nmachine translation, yielding shorter execution time than those placements designed by human ex-\nperts. Computation burden is one concern for a RL approach to search directly in the solution space\nof a combinatorial problem. We discuss combinatorial optimization in Section ??.\n5.12.3\nSECURITY & PRIVACY\nadversarial attacks, e.g., Huang et al. (2017),\nhttp://rll.berkeley.edu/adversarial/\nPapernot et al. (2016)\nAbadi et al. (2016)\nBalle et al. (2016)\nDelle Fave et al. (2014)\nCarlini and Wagner (2017b)\ndefenders Ian J. Goodfellow (2015); Carlini and Wagner (2017a); Madry et al. (2017); Tram`er et al.\n(2017)\nAnderson et al. (2017) https://github.com/endgameinc/gym-malware\nEvtimov et al. (2017) See a blog titled Physical Adversarial Examples Against Deep Neural\nNetworks at http://bair.berkeley.edu/blog/2017/12/30/yolo-attack/, which contains a brief survey\nof attack and defence algorithms. Check ACM Conference on Computer and Communications\nSecurity (CCS 2016) tutorial on Adversarial Data Mining:\nBig Data Meets Cyber Security,\nhttps://www.sigsac.org/ccs/CCS2016/tutorials/index.html\n43\n6\nMORE TOPICS\nWe list more interesting and/or important topics we have not discussed in this overview as below,\nhoping it would provide pointers for those who may be interested in studying them further. Some\ntopics/papers may not contain RL yet. However, we believe these are interesting and/or important\ndirections for RL in the sense of either theory or application. It would be deﬁnitely more desirable\nif we could ﬁnish reviewing these, however, we leave it as future work.\n• understanding deep learning, Daniely et al. (2016); Li et al. (2016b); Karpathy et al. (2016);\nKawaguchi et al. (2017); Koh and Liang (2017); Neyshabur et al. (2017); Shalev-Shwartz\net al. (2017); Shwartz-Ziv and Tishby (2017); Zhang et al. (2017)\n• interpretability, e.g., Al-Shedivat et al. (2017b); Doshi-Velez and Kim (2017); Harrison\net al. (2017); Lei et al. (2016); Lipton (2016); Miller (2017); Ribeiro et al. (2016); Huk\nPark et al. (2016)\n– NIPS 2017 Interpretable Machine Learning Symposium\n– ICML 2017 Tutorial on Interpretable Machine Learning\n– NIPS 2016 Workshop on Interpretable ML for Complex Systems\n– ICML Workshop on Human Interpretability in Machine Learning 2017, 2016\n• usable machine learning, Bailis et al. (2017)\n◦Cloud AutoML: Making AI accessible to every business,\nhttps://www.blog.google/topics/google-cloud/cloud-automl-making-ai-accessible-every-\nbusiness/\n• expressivity, Raghu et al. (2016)\n• testing, Pei et al. (2017)\n• deep learning efﬁciency, e.g., Han et al. (2016), Spring and Shrivastava (2017), Sze et al.\n(2017)\n• deep learning compression\n• optimization, e.g., Wilson et al. (2017), Czarnecki et al. (2017)\n• normalization, Klambauer et al. (2017), van Hasselt et al. (2016b)\n• curriculum learning, Graves et al. (2017), Held et al. (2017), Matiisen et al. (2017)\n• professor forcing, Lamb et al. (2016)\n• new Q-value operators, Kavosh and Littman (2017), Haarnoja et al. (2017)\n• large action space, e.g., Dulac-Arnold et al. (2015); He et al. (2016c)\n• Predictive State Representation, Downey et al. (2017), Venkatraman et al. (2017)\n• safe RL, e.g., Berkenkamp et al. (2017)\n• agent modelling, e.g., Albrechta and Stone (2018)\n• semi-supervised learning, e.g., Audiffren et al. (2015); Cheng et al. (2016); Dai et al.\n(2017); Finn et al. (2017); Kingma et al. (2014); Papernot et al. (2017); Yang et al. (2017);\nZhu and Goldberg (2009)\n• neural episodic control, Pritzel et al. (2017)\n• continual learning, Chen and Liu (2016); Kirkpatrick et al. (2017); Lopez-Paz and Ranzato\n(2017)\n– Satinder Singh, Steps towards continual learning, tutorial at Deep Learning and Rein-\nforcement Learning Summer School 2017\n• symbolic learning, Evans and Grefenstette (2017); Liang et al. (2017a); Parisotto et al.\n(2017)\n• pathNet, Fernando et al. (2017)\n• evolution strategies, Petroski Such et al. (2017), Salimans et al. (2017) , Lehman et al.\n(2017)\n44\n• capsules, Sabour et al. (2017)\n• DeepForest, Zhou and Feng (2017); Feng and Zhou (2017)\n• deep probabilistic programming, Tran et al. (2017)\n• active learning, e.g., Fang et al. (2017)\n• deep learning games, Schuurmans and Zinkevich (2016)\n• program learning, e.g., Balog et al. (2017); Cai et al. (2017); Denil et al. (2017); Parisotto\net al. (2017); Reed and de Freitas (2016)\n• relational reasoning, e.g., Santoro et al. (2017), Watters et al. (2017)\n• proving, e.g., Loos et al. (2017); Rockt¨aschel and Riedel (2017)\n• education, e.g.,\n• music generation, e.g., Jaques et al. (2017)\n• retrosynthesis, e.g., Segler et al. (2017)\n• quantum RL, e.g., Crawford et al. (2016)\n– NIPS 2015 Workshop on Quantum Machine Learning\n7\nRESOURCES\nWe list a collection of deep RL resources including books, surveys, reports, online courses, tutorials,\nconferences, journals and workshops, blogs, testbed, and open source algorithm implementations.\nThis by no means is complete.\nIt is essential to have a good understanding of reinforcement learning, before having a good under-\nstanding of deep reinforcement learning. We recommend to start with the textbook by Sutton and\nBarto (Sutton and Barto, 2018), the RL courses by Rich Sutton and by David Silver as the ﬁrst two\nitems in the Courses subsection below.\nIn the current information/social media age, we are overwhelmed by information, e.g., from Twitter,\narXiv, Google+, etc. The skill to efﬁciently select the best information becomes essential. The Wild\nWeek in AI (http://www.wildml.com) is an excellent series of weekly summary blogs. In an ear\nof AI, we expect to see an AI agent to do such tasks like intelligently searching and summarizing\nrelevant news, blogs, research papers, etc.\n7.1\nBOOKS\n• the deﬁnitive and intuitive reinforcement learning book by Richard S. Sutton and Andrew\nG. Barto (Sutton and Barto, 2018)\n• deep learning books (Deng and Dong, 2014; Goodfellow et al., 2016)\n7.2\nMORE BOOKS\n• theoretical RL books (Bertsekas, 2012; Bertsekas and Tsitsiklis, 1996; Szepesv´ari, 2010)\n• an operations research oriented RL book (Powell, 2011)\n• an edited RL book (Wiering and van Otterlo, 2012)\n• Markov decision processes (Puterman, 2005)\n• machine learning (Bishop, 2011; Hastie et al., 2009; Haykin, 2008; James et al., 2013;\nKuhn and Johnson, 2013; Murphy, 2012; Provost and Fawcett, 2013; Simeone, 2017; Zhou,\n2016)\n• artiﬁcial intelligence (Russell and Norvig, 2009)\n• natural language processing (NLP) (Deng and Liu, 2017; Goldberg, 2017; Jurafsky and\nMartin, 2017)\n• semi-supervised learning (Zhu and Goldberg, 2009)\n• game theory (Leyton-Brown and Shoham, 2008)\n45\n7.3\nSURVEYS AND REPORTS\n• reinforcement learning (Littman, 2015; Kaelbling et al., 1996; Geramifard et al., 2013;\nGrondman et al., 2012; Roijers et al., 2013); deep reinforcement learning (Arulkumaran\net al., 2017) 4\n• deep learning (LeCun et al., 2015; Schmidhuber, 2015; Bengio, 2009; Wang and Raj, 2017)\n• efﬁcient processing of deep neural networks (Sze et al., 2017)\n• machine learning (Jordan and Mitchell, 2015)\n• practical machine learning advices (Domingos, 2012; Smith, 2017; Zinkevich, 2017)\n• natural language processing (NLP) (Hirschberg and Manning, 2015; Cho, 2015; Young\net al., 2017)\n• spoken dialogue systems (Deng and Li, 2013; Hinton et al., 2012; He and Deng, 2013;\nYoung et al., 2013)\n• robotics (Kober et al., 2013)\n• transfer learning (Taylor and Stone, 2009; Pan and Yang, 2010; Weiss et al., 2016)\n• Bayesian RL (Ghavamzadeh et al., 2015)\n• AI safety (Amodei et al., 2016; Garc`ıa and Fern`andez, 2015)\n• Monte Carlo tree search (MCTS) (Browne et al., 2012; Gelly et al., 2012)\n7.4\nCOURSES\n• Richard Sutton, Reinforcement Learning, 2016, slides, assignments, reading materials, etc.\nhttp://www.incompleteideas.net/sutton/609%20dropbox/\n• David Silver, Reinforcement Learning, 2015, slides (goo.gl/UqaxlO), video-lectures\n(goo.gl/7BVRkT)\n• Sergey Levine, John Schulman and Chelsea Finn, CS 294: Deep Reinforcement Learning,\nSpring 2017, http://rll.berkeley.edu/deeprlcourse/\n• Katerina Fragkiadaki, Ruslan Satakhutdinov, Deep Reinforcement Learning and Control,\nSpring 2017, https://katefvision.github.io\n• Emma Brunskill, CS234: Reinforcement Learning, http://web.stanford.edu/class/cs234/\n• Charles Isbell, Michael Littman and Pushkar Kolhe, Udacity: Machine Learning: Rein-\nforcement Learning, goo.gl/eyvLfg\n• David Donoho, Hatef Monajemi, and Vardan Papyan, Theories of Deep Learning (Stanford\nSTATS 385), https://stats385.github.io\n• Nando de Freitas, Deep Learning Lectures, https://www.youtube.com/user/ProfNandoDF\n• Fei-Fei Li, Andrej Karpathy and Justin Johnson, CS231n: Convolutional Neural Networks\nfor Visual Recognition, http://cs231n.stanford.edu\n• Richard Socher, CS224d: Deep Learning for Natural Language Processing,\nhttp://cs224d.stanford.edu\n• Brendan Shillingford, Yannis Assael, Chris Dyer, Oxford Deep NLP 2017 course,\nhttps://github.com/oxford-cs-deepnlp-2017\n• Pieter Abbeel, Advanced Robotics, Fall 2015, https://people.eecs.berkeley.edu/ pabbeel/cs287-\nfa15/\n• Emo Todorov, Intelligent control through learning and optimization,\nhttp://homes.cs.washington.edu/∼todorov/courses/amath579/index.html\n• Abdeslam Boularias, Robot Learning Seminar, http://www.abdeslam.net/robotlearningseminar\n• MIT 6.S094: Deep Learning for Self-Driving Cars, http://selfdrivingcars.mit.edu\n• Jeremy Howard, Practical Deep Learning For Coders, http://course.fast.ai\n• Andrew Ng, Deep Learning Specialization\nhttps://www.coursera.org/specializations/deep-learning\n4Our overview is much more comprehensive, and was online much earlier, than this brief survey.\n46\n7.5\nTUTORIALS\n• Rich Sutton, Introduction to Reinforcement Learning with Function Approximation,\nhttps://www.microsoft.com/en-us/research/video/tutorial-introduction-to-reinforcement-\nlearning-with-function-approximation/\n• Deep Reinforcement Learning\n– David Silver, ICML 2016\n– David Silver, 2nd Multidisciplinary Conference on Reinforcement Learning and De-\ncision Making (RLDM), Edmonton, Alberta, Canada, 2015;\nhttp://videolectures.net/rldm2015 silver reinforcement learning/\n– John Schulman, Deep Learning School, 2016\n– Pieter Abbeel, Deep Learning Summer School, 2016;\nhttp://videolectures.net/deeplearning2016 abbeel deep reinforcement/\n– Pieter Abbeel and John Schulman, Deep Reinforcement Learning Through Policy Op-\ntimization, NIPS 2016\n– Sergey Levine and Chelsea Finn, Deep Reinforcement Learning, Decision Making,\nand Control, ICML 2017\n• John Schulman, The Nuts and Bolts of Deep Reinforcement Learning Research, Deep Re-\ninforcement Learning Workshop, NIPS 2016\n• Joelle Pineau, Introduction to Reinforcement Learning, Deep Learning Summer School,\n2016; http://videolectures.net/deeplearning2016 pineau reinforcement learning/\n• Andrew Ng, Nuts and Bolts of Building Applications using Deep Learning, NIPS 2016\n• Deep Learning Summer School, 2016, 2015\n• Deep Learning and Reinforcement Learning Summer Schools, 2017\n• Simons Institute Interactive Learning Workshop, 2017\n• Simons Institute Representation Learning Workshop, 2017\n• Simons Institute Computational Challenges in Machine Learning Workshop, 2017\n7.6\nCONFERENCES, JOURNALS AND WORKSHOPS\n• NIPS: Neural Information Processing Systems\n• ICML: International Conference on Machine Learning\n• ICLR: International Conference on Learning Representation\n• RLDM: Multidisciplinary Conference on Reinforcement Learning and Decision Making\n• EWRL: European Workshop on Reinforcement Learning\n• AAAI, IJCAI, ACL, EMNLP, SIGDIAL, ICRA, IROS, KDD, SIGIR, CVPR, etc.\n• Nature Machine Intelligence, Science Robotics, JMLR, MLJ, AIJ, JAIR, PAMI, etc\n• Nature May 2015, Science July 2015, survey papers on machine learning/AI\n• Science, July 7, 2017 issue, The Cyberscientist, a special issue about AI\n• Deep Reinforcement Learning Workshop, NIPS 2016, 2015; IJCAI 2016\n• Deep Learning Workshop, ICML 2016\n• http://distill.pub\n7.7\nBLOGS\n• Deepmind Blog, https://deepmind.com/blog/\n◦\n◦\n• Google Research Blog, https://research.googleblog.com, goo.gl/ok88b7\n◦The Google Brain Team — Looking Back on 2017, goo.gl/1G7jnb, goo.gl/uCWDLr\n47\n◦The Google Brain Team — Looking Back on 2016,\n• Berkeley AI Research Blog, http://bair.berkeley.edu/blog/\n• OpenAI Blog, https://blog.openai.com\n• Marc Bellemare, Classic and modern reinforcement learning,\nhttp://www.marcgbellemare.info/blog/\n• Denny Britz, The Wild Week in AI, a weekly AI & deep learning newsletter,\nwww.wildml.com, esp. goo.gl/MyrwDC\n• Andrej Karpathy, karpathy.github.io, esp. goo.gl/1hkKrb\n• Junling Hu, Reinforcement learning explained - learning to act based on long-term payoffs\nhttps://www.oreilly.com/ideas/reinforcement-learning-explained\n• Li Deng, How deep reinforcement learning can help chatbots\nhttps://venturebeat.com/2016/08/01/how-deep-reinforcement-learning-can-help-chatbots/\n• Reinforcement Learning, https://www.technologyreview.com/s/603501/10-breakthrough-\ntechnologies-2017-reinforcement-learning/\n• Deep Learning, https://www.technologyreview.com/s/513696/deep-learning/\n7.8\nTESTBEDS\n• The Arcade Learning Environment (ALE) (Bellemare et al., 2013; Machado et al., 2017) is\na framework composed of Atari 2600 games to develop and evaluate AI agents.\n• Ray RLlib: A Composable and Scalable Reinforcement Learning Library (Liang et al.,\n2017c), http://ray.readthedocs.io/en/latest/rllib.html\n• OpenAI Gym (https://gym.openai.com) is a toolkit for the development of RL algorithms,\nconsisting of environments, e.g., Atari games and simulated robots, and a site for the com-\nparison and reproduction of results.\n• OpenAI Universe (https://universe.openai.com) is used to turn any program into a Gym\nenvironment. Universe has already integrated many environments, including Atari games,\nﬂash games, browser tasks like Mini World of Bits and real-world browser tasks. Recently,\nGTA V was added to Universe for self-driving vehicle simulation.\n• DeepMind Control Suite, Tassa et al. (2018)\n• DeepMind released a ﬁrst-person 3D game platform DeepMind Lab (Beattie et al., 2016).\nDeepmind and Blizzard will collaborate to release the Starcraft II AI research environment\n(goo.gl/Ptiwfg).\n• Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents (Leibo\net al., 2018)\n• FAIR TorchCraft (Synnaeve et al., 2016) is a library for Real-Time Strategy (RTS) games\nsuch as StarCraft: Brood War.\n• Deepmind PySC2 - StarCraft II Learning Environment, https://github.com/deepmind/pysc2\n• David Churchill, CommandCenter: StarCraft 2 AI Bot,\nhttps://github.com/davechurchill/commandcenter\n• ParlAI is a framework for dialogue research, implemented in Python, open-sourced by\nFacebook. https://github.com/facebookresearch/ParlAI\n• ELF, an extensive, lightweight and ﬂexible platform for RL research (Tian et al., 2017)\n• Project Malmo (https://github.com/Microsoft/malmo), from Microsoft, is an AI research\nand experimentation platform built on top of Minecraft.\n• Twitter open-sourced torch-twrl, a framework for RL development.\n• ViZDoom is a Doom-based AI research platform for visual RL (Kempka et al., 2016).\n• Baidu Apollo Project, self-driving open-source, http://apollo.auto\n• TORCS is a car racing simulator (Bernhard Wymann et al., 2014).\n• MuJoCo, Multi-Joint dynamics with Contact, is a physics engine, http://www.mujoco.org.\n48\n• Nogueira and Cho (2016) presented WebNav Challenge for Wikipedia links navigation.\n• RLGlue (Tanner and White, 2009) is a language-independent software for RL experiments.\nIt may need extensions to accommodate progress in deep learning.\n• RLPy (Geramifard et al., 2015) is a value-function-based reinforcement learning frame-\nwork for education and research.\n7.9\nALGORITHM IMPLEMENTATIONS\nWe collect implementations of algorithms, either classical ones as in a textbook like Sutton and\nBarto (2018) or in recent papers.\n• Shangtong Zhang, Python code to accompany Sutton & Barto’s RL book and David Silver’s\nRL course, https://github.com/ShangtongZhang/reinforcement-learning-an-introduction\n• Learning Reinforcement Learning (with Code, Exercises and Solutions),\nhttp://www.wildml.com/2016/10/learning-reinforcement-learning/\n• OpenAI Baselines: high-quality implementations of reinforcement learning algorithms,\nhttps://github.com/openai/baselines\n• TensorFlow implementation of Deep Reinforcement Learning papers,\nhttps://github.com/carpedm20/deep-rl-tensorﬂow\n• Deep reinforcement learning for Keras, https://github.com/matthiasplappert/keras-rl\n• Code Implementations for NIPS 2016 papers, http://bit.ly/2hSaOyx\n• Benchmark results of various policy optimization algorithms (Duan et al., 2016),\nhttps://github.com/rllab/rllab\n• Tensor2Tensor (T2T) (Vaswani et al., 2017; Kaiser et al., 2017a;b)\n• DQN (Mnih et al., 2015), https://sites.google.com/a/deepmind.com/dqn/\n• Tensorﬂow implementation of DQN (Mnih et al., 2015),\nhttps://github.com/devsisters/DQN-tensorﬂow\n• Deep Q Learning with Keras and Gym, https://keon.io/deep-q-learning/\n• Deep Exploration via Bootstrapped DQN (Osband et al., 2016), a Torch implementation,\nhttps://github.com/iassael/torch-bootstrapped-dqn\n• DarkForest, the Facebook Go engine (Github), https://github.com/facebookresearch/darkforestGo\n• Using Keras and Deep Q-Network to Play FlappyBird,\nhttps://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\n• Deep Deterministic Policy Gradients (Lillicrap et al., 2016) in TensorFlow,\nhttp://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html\n• Deep Deterministic Policy Gradient (Lillicrap et al., 2016) to play TORCS,\nhttps://yanpanlau.github.io/2016/10/11/Torcs-Keras.html\n• Reinforcement learning with unsupervised auxiliary tasks (Jaderberg et al., 2017),\nhttps://github.com/miyosuda/unreal\n• Learning to communicate with deep multi-agent reinforcement learning,\nhttps://github.com/iassael/learning-to-communicate\n• Deep Reinforcement Learning: Playing a Racing Game - Byte Tank, http://bit.ly/2pVIP4i\n• Differentiable Neural Computer (DNC) (Graves et al., 2016),\nhttps://github.com/deepmind/dnc\n• Playing FPS Games with Deep Reinforcement Learning (Lample and Chaplot, 2017),\nhttps://github.com/glample/Arnold\n• Learning to Learn (Reed and de Freitas, 2016) in TensorFlow,\nhttps://github.com/deepmind/learning-to-learn\n• Value Iteration Networks (Tamar et al., 2016) in Tensorﬂow,\nhttps://github.com/TheAbhiKumar/tensorﬂow-value-iteration-networks\n49\n• Tensorﬂow implementation of the Predictron (Silver et al., 2016b),\nhttps://github.com/zhongwen/predictron\n• Meta Reinforcement Learning (Wang et al., 2016) in Tensorﬂow,\nhttps://github.com/awjuliani/Meta-RL\n• Generative adversarial imitation learning (Ho and Ermon, 2016), containing an im-\nplementation of Trust Region Policy Optimization (TRPO) (Schulman et al., 2015),\nhttps://github.com/openai/imitation\n• Starter code for evolution strategies (Salimans et al., 2017),\nhttps://github.com/openai/evolution-strategies-starter\n• Transfer learning (Long et al., 2015; 2016), https://github.com/thuml/transfer-caffe\n• DeepForest (Zhou and Feng, 2017), http://lamda.nju.edu.cn/ﬁles/gcforest.zip\n8\nBRIEF SUMMARY\nWe list some RL issues and corresponding proposed approaches covered in this overview, as well\nas some classical work. One direction of future work is to further reﬁne this section, especially for\nissues and solutions in applications.\n• issue: prediction, policy evaluation\nproposed approaches:\n– temporal difference (TD) learning (Sutton, 1988)\n• issue: control, ﬁnding optimal policy (classical work)\nproposed approaches:\n– Q-learning (Watkins and Dayan, 1992)\n– policy gradient (Williams, 1992)\n⋄reduce variance of gradient estimate: baseline, advantage function (Williams,\n1992; Sutton et al., 2000)\n– actor-critic (Barto et al., 1983)\n– SARSA (Sutton and Barto, 2018)\n• issue: the deadly triad: instability and divergence when combining off-policy, function\napproximation, and bootstrapping\nproposed approaches:\n– DQN with experience replay (Lin, 1992) and target network (Mnih et al., 2015)\n⋄overestimate problem in Q-learning: double DQN (van Hasselt et al., 2016a)\n⋄prioritized experience replay (Schaul et al., 2016)\n⋄better exploration strategy (Osband et al., 2016)\n⋄optimality tightening to accelerate DQN (He et al., 2017)\n⋄reduce variability and instability with averaged-DQN (Anschel et al., 2017)\n– dueling architecture (Wang et al., 2016b)\n– asynchronous methods (Mnih et al., 2016)\n– trust region policy optimization (Schulman et al., 2015)\n– distributed proximal policy optimization (Heess et al., 2017)\n– combine policy gradient and Q-learning (O’Donoghue et al., 2017; Nachum et al.,\n2017; Gu et al., 2017; Schulman et al., 2017)\n– GTD (Sutton et al., 2009a;b; Mahmood et al., 2014)\n– Emphatic-TD (Sutton et al., 2016)\n• issue: train perception and control jointly end-to-end\nproposed approaches:\n– guided policy search (Levine et al., 2016a)\n• issue: data/sample efﬁciency\n50\nproposed approaches:\n– Q-learning, actor-critic\n– model-based policy search, e.g., PILCO Deisenroth and Rasmussen (2011)\n– actor-critic with experience replay (Wang et al., 2017b)\n– PGQ, policy gradient and Q-learning (O’Donoghue et al., 2017)\n– Q-Prop, policy gradient with off-policy critic (Gu et al., 2017)\n– return-based off-policy control, Retrace (Munos et al., 2016), Reactor (Gruslys et al.,\n2017)\n– learning to learn, e.g., Duan et al. (2016); Wang et al. (2016); Lake et al. (2015)\n• issue: reward function not available\nproposed approaches:\n– imitation learning\n– inverse RL (Ng and Russell, 2000)\n– learn from demonstration (Hester et al., 2018)\n– imitation learning with GANs (Ho and Ermon, 2016; Stadie et al., 2017)\n– train dialogue policy jointly with reward model (Su et al., 2016b)\n• issue: exploration-exploitation tradeoff\nproposed approaches:\n– unify count-based exploration and intrinsic motivation (Bellemare et al., 2016)\n– under-appreciated reward exploration (Nachum et al., 2017)\n– deep exploration via bootstrapped DQN (Osband et al., 2016)\n– variational information maximizing exploration (Houthooft et al., 2016)\n• issue: model-based learning\nproposed approaches:\n– Dyna-Q (Sutton, 1990)\n– combine model-free and model-based RL (Chebotar et al., 2017)\n• issue: model-free planning\nproposed approaches:\n– value iteration networks (Tamar et al., 2016)\n– predictron (Silver et al., 2016b)\n• issue: focus on salient parts\nproposed approaches: attention\n– object detection (Mnih et al., 2014)\n– neural machine translation (Bahdanau et al., 2015)\n– image captioning (Xu et al., 2015)\n– replace CNN and RNN with attention in sequence modelling (Vaswani et al., 2017)\n• issue: data storage over long time, separating from computation\nproposed approaches: memory\n– differentiable neural computer (DNC) with external memory (Graves et al., 2016)\n• issue: beneﬁt from non-reward training signals in environments\nproposed approaches: unsupervised Learning\n– Horde (Sutton et al., 2011)\n– unsupervised reinforcement and auxiliary learning (Jaderberg et al., 2017)\n– learn to navigate with unsupervised auxiliary learning (Mirowski et al., 2017)\n– generative adversarial networks (GANs) (Goodfellow et al., 2014)\n• issue: learn knowledge from different domains\n51\nproposed approaches: transfer Learning (Taylor and Stone, 2009; Pan and Yang, 2010;\nWeiss et al., 2016)\n– learn invariant features to transfer skills (Gupta et al., 2017a)\n• issue: beneﬁt from both labelled and unlabelled data\nproposed approaches: semi-supervised learning (Zhu and Goldberg, 2009)\n– learn with MDPs both with and without reward functions (Finn et al., 2017)\n– learn with expert’s trajectories and those may not from experts (Audiffren et al., 2015)\n• issue: learn, plan, and represent knowledge with spatio-temporal abstraction at multiple\nlevels\nproposed approaches: hierarchical RL (Barto and Mahadevan, 2003)\n– options (Sutton et al., 1999), MAXQ (Dietterich, 2000)\n– strategic attentive writer to learn macro-actions (Vezhnevets et al., 2016)\n– integrate temporal abstraction with intrinsic motivation (Kulkarni et al., 2016)\n– stochastic neural networks for hierarchical RL (Florensa et al., 2017)\n– lifelong learning with hierarchical RL (Tessler et al., 2017)\n• issue: adapt rapidly to new tasks\nproposed approaches: learning to learn\n– learn to optimize (Li and Malik, 2017)\n– learn a ﬂexible RNN model to handle a family of RL tasks (Duan et al., 2016; Wang\net al., 2016)\n– one/few/zero-shot learning (Duan et al., 2017; Johnson et al., 2016; Kaiser et al.,\n2017b; Koch et al., 2015; Lake et al., 2015; Ravi and Larochelle, 2017; Vinyals et al.,\n2016)\n• issue: gigantic search space\nproposed approaches:\n– integrate supervised learning, reinforcement learning, and Monte-Carlo tree search as\nin AlphaGo (Silver et al., 2016a)\n• issue: neural networks architecture design\nproposed approaches:\n– neural architecture search (Bello et al., 2017; Baker et al., 2017; Zoph and Le, 2017)\n– new architectures, e.g., Kaiser et al. (2017a), Silver et al. (2016b), Tamar et al. (2016),\nVaswani et al. (2017), Wang et al. (2016b)\n9\nDISCUSSIONS\nIt is both the best and the worst of times for the ﬁeld of deep RL, for the same reason: it has been\ngrowing so fast and so enormously. We have been witnessing breakthroughs, exciting new methods\nand applications, and we expect to see much more and much faster. As a consequence, this overview\nis incomplete, in the sense of both depth and width. However, we attempt to summarize important\nachievements and discuss potential directions and applications in this amazing ﬁeld.\nIn this overview, we summarize six core elements – value function, policy, reward, model and plan-\nning, exploration, and knowledge; six important mechanisms – attention and memory, unsupervised\nlearning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn; and twelve ap-\nplications – games, robotics, natural language processing, computer vision, business management,\nﬁnance, healthcare, education, Industry 4.0, smart grid, intelligent transportation systems, and com-\nputer systems. We also discuss background of machine learning, deep learning, and reinforcement\nlearning, and list a collection of RL resources.\nWe have seen breakthroughs about deep RL, including deep Q-network (Mnih et al., 2015) and\nAlphaGo (Silver et al., 2016a). There have been many extensions to, improvements for and applica-\ntions of deep Q-network (Mnih et al., 2015).\n52\nNovel architectures and applications using deep RL were recognized in top tier conferences as best\npapers in 2016: dueling network architectures (Wang et al., 2016b) at ICML, spoken dialogue\nsystems (Su et al., 2016b) at ACL (student), information extraction (Narasimhan et al., 2016) at\nEMNLP, and, value iteration networks (Tamar et al., 2016) at NIPS. Gelly and Silver (2007) was\nthe recipient of Test of Time Award at ICML 2017. In 2017, the following were recognized as\nbest papers, Kottur et al. (2017) at EMNLP (short), and, Bacon et al. (2017) at AAAI (student).\nExciting achievements abound: differentiable neural computer (Graves et al., 2016), unsupervised\nreinforcement and auxiliary learning (Jaderberg et al., 2017), asynchronous methods (Mnih et al.,\n2016), dual learning for machine translation (He et al., 2016a), guided policy search (Levine et al.,\n2016a), generative adversarial imitation learning (Ho and Ermon, 2016), and neural architecture de-\nsign (Zoph and Le, 2017), etc. Creativity would push the frontiers of deep RL further with respect\nto core elements, mechanisms, and applications.\nState of the Art Control of Atari Games Using Shallow RL was accepted at AAMAS. It was also\nnominated for the Best Paper Award\nValue function is central to reinforcement learning, e.g., in deep Q-network and its many exten-\ntions. Policy optimization approaches have been gaining traction, in many, diverse applications,\ne.g., robotics, neural architecture design, spoken dialogue systems, machine translation, attention,\nand learning to learn, and this list is boundless. New learning mechanisms have emerged, e.g.,\nusing transfer/unsupervised/semi-supervised learning to improve the quality and speed of learn-\ning, and more new mechanisms will be emerging. This is the renaissance of reinforcement learn-\ning (Krakovsky, 2016). In fact, reinforcement learning and deep learning have been making steady\nprogress even during the last AI winter.\nA popular criticism about deep learning is that it is a blackbox, or even the ”alchemy” as a com-\nment during NIPS 2017 Test of Time Award (Rahimi and Recht, 2007) speech, so it is not clear\nhow it works. This should not be the reason not to accept deep learning; rather, having a better un-\nderstanding of how deep learning works is helpful for deep learning and general machine learning\ncommunity. There are works in this direction as well as for interpretability of deep learning as we\nlist in Section 6.\nIt is essential to consider issues of learning models, like stability, convergence, accuracy, data efﬁ-\nciency, scalability, speed, simplicity, interpretability, robustness, and safety, etc. It is important to\ninvestigate comments/criticisms, e.g., from conginitive science, like intuitive physics, intuitive psy-\nchology, causal model, compositionality, learning to learn, and act in real time (Lake et al., 2016),\nfor stronger AI. It is interesting to check Deepmind’s commentary (Botvinick et al., 2017) about\none additional ingredient, autonomy, so that agents can build and exploit their own internal models,\nwith minimal human manual engineering, and investigate the connection between neuroscience and\nRL/AI (Hassabis et al., 2017). See also Peter Norvig’s perspective at http://bit.ly/2qpehcd. See Sto-\nica et al. (2017) for systems challenges for AI.\nNature in May 2015 and Science in July 2015 featured survey papers on machine learning/AI. Sci-\nence Robotics launched in 2016. Science has a special issue on July 7, 2017 about AI on The\nCyberscientist. Nature Machine Intelligence will launch in January 2019. The coverage of AI by\npremier journals like Nature and Science and the launch of Science Robotics and Nature Machine\nIntelligence illustrate the apparent importance of AI. It is interesting to mention that NIPS 2017\nmain conference was sold out only two weeks after opening for registration.\nIt is worthwhile to envision deep RL considering perspectives of government, academia and industry\non AI, e.g., Artiﬁcial Intelligence, Automation, and the economy, Executive Ofﬁce of the President,\nUSA; Artiﬁcial Intelligence and Life in 2030 - One Hundred Year Study on Artiﬁcial Intelligence:\nReport of the 2015-2016 Study Panel, Stanford University (Stone et al., 2016); and AI, Machine\nLearning and Data Fuel the Future of Productivity by The Goldman Sachs Group, Inc., etc. See also\nthe recent AI Frontiers Conference, https://www.aifrontiers.com.\nDeep learning was among MIT Technology Review 10 Breakthrough Technologies in 2013. We\nhave been witnessing the dramatic development of deep learning in both academia and industry in\nthe last few years. Reinforcement learning was among MIT Technology Review 10 Breakthrough\nTechnologies in 2017. Deep learning has made many achievements, has ”conquered” speech recog-\nnition, computer vision, and now NLP, is more mature and well-accepted, and has been validated by\nproducts and market. In contrast, RL has lots of (potential, promising) applications, yet few products\n53\nso far, may still need better algorithms, may still need products and market validation. However, it\nis probably the right time to nurture, educate and lead the market. We will see both deep learning\nand reinforcement learning prospering in the coming years and beyond. Prediction is very difﬁcult,\nespecially about the future. However, we expect that 2018 for reinforcement learning would be 2010\nfor deep learning.\nDeep learning, in this third wave of AI, will have deeper inﬂuences, as we have already seen from\nits many achievements. Reinforcement learning, as a more general learning and decision making\nparadigm, will deeply inﬂuence deep learning, machine learning, and artiﬁcial intelligence in gen-\neral. Deepmind, conducting leading research in deep reinforcement learning, recently opened its\nﬁrst ever international AI research ofﬁce in Alberta, Canada, co-locating with the major research\ncenter for reinforcement learning led by Rich Sutton. It is interesting to mention that when Pro-\nfessor Rich Sutton started working in the University of Alberta in 2003, he named his lab RLAI:\nReinforcement Learning and Artiﬁcial Intelligence.\nACKOWLEDGEMENT\nWe appreciate comments from Baochun Bai, Kan Deng, Hai Fang, Hua He, Junling Hu, Ruitong\nHuang, Aravind Lakshminarayanan, Jinke Li, Lihong Li, Bhairav Mehta, Dale Schuurmans, David\nSilver, Rich Sutton, Csaba Szepesv´ari, Arash Tavakoli, Cameron Upright, Yi Wan, Qing Yu, Yao-\nliang Yu, attendants of various seminars and webinars, in particular, a seminar at MIT on AlphaGo:\nKey Techniques and Applications, and an AI seminar at the University of Alberta on Deep Rein-\nforcement Learning: An Overview. Any remaining issues and errors are our own.\nREFERENCES\nAbadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. (2016).\nDeep learning with differential privacy. In ACM Conference on Computer and Communications\nSecurity (ACM CCS).\nAbbeel, P. and Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In the\nInternational Conference on Machine Learning (ICML).\nAgrawal, P., Nair, A., Abbeel, P., Malik, J., and Levine, S. (2016). Learning to poke by poking:\nExperiential learning of intuitive physics. In the Annual Conference on Neural Information Pro-\ncessing Systems (NIPS).\nAl-Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mordatch, I., and Abbeel, P. (2017a). Con-\ntinuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments. ArXiv\ne-prints.\nAl-Shedivat, M., Dubey, A., and Xing, E. P. (2017b). Contextual Explanation Networks. ArXiv\ne-prints.\nAlbrechta, S. V. and Stone, P. (2018). Autonomous agents modelling other agents: A comprehensive\nsurvey and open problems. Artiﬁcial Intelligence.\nAlsheikh, M. A., Lin, S., Niyato, D., and Tan, H.-P. (2014). Machine learning in wireless sensor\nnetworks: Algorithms, strategies, and applications. IEEE Communications Surveys & Tutorials,\n16(4):1996–2018.\nAmin, K., Jiang, N., and Singh, S. (2017). Repeated inverse reinforcement learning. In the Annual\nConference on Neural Information Processing Systems (NIPS).\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man´e, D. (2016). Concrete\nProblems in AI Safety. ArXiv e-prints.\nAnderson, H. S., Kharkar, A., Filar, B., and Roth, P. (2017). Evading machine learning malware\ndetection. In Black Hat USA.\n54\nAnderson, R. N., Boulanger, A., Powell, W. B., and Scott, W. (2011). Adaptive stochastic control\nfor the smart grid. Proceedings of the IEEE, 99(6):1098–1115.\nAndreas, J., Klein, D., and Levine, S. (2017). Modular multitask reinforcement learning with policy\nsketches. In the International Conference on Machine Learning (ICML).\nAndrychowicz, M., Denil, M., Colmenarejo, S. G., Hoffman, M. W., Pfau, D., Schaul, T., Shilling-\nford, B., and de Freitas, N. (2016). Learning to learn by gradient descent by gradient descent. In\nthe Annual Conference on Neural Information Processing Systems (NIPS).\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin,\nJ., Abbeel, P., and Zaremba, W. (2017). Hindsight experience replay. In the Annual Conference\non Neural Information Processing Systems (NIPS).\nAnschel, O., Baram, N., and Shimkin, N. (2017). Averaged-DQN: Variance reduction and stabi-\nlization for deep reinforcement learning. In the International Conference on Machine Learning\n(ICML).\nArgall, B. D., Chernova, S., Veloso, M., and Browning, B. (2009). A survey of robot learning from\ndemonstration. Robotics and Autonomous Systems, 57(5):469–483.\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein GAN. ArXiv e-prints.\nArtetxe, M., Labaka, G., Agirre, E., and Cho, K. (2017). Unsupervised Neural Machine Translation.\nArXiv e-prints.\nArulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A. (2017). A Brief Survey of\nDeep Reinforcement Learning. ArXiv e-prints.\nAsri, L. E., He, J., and Suleman, K. (2016). A sequence-to-sequence model for user simulation\nin spoken dialogue systems.\nIn Annual Meeting of the International Speech Communication\nAssociation (INTERSPEECH).\nAudiffren, J., Valko, M., Lazaric, A., and Ghavamzadeh, M. (2015).\nMaximum entropy semi-\nsupervised inverse reinforcement learning. In the International Joint Conference on Artiﬁcial\nIntelligence (IJCAI).\nAzar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning.\nIn the International Conference on Machine Learning (ICML).\nBa, J., Hinton, G. E., Mnih, V., Leibo, J. Z., and Ionescu, C. (2016). Using fast weights to attend to\nthe recent past. In the Annual Conference on Neural Information Processing Systems (NIPS).\nBa, J., Mnih, V., and Kavukcuoglu, K. (2014). Multiple object recognition with visual attention. In\nthe International Conference on Learning Representations (ICLR).\nBabaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and Kautz, J. (2017). Reinforcement learn-\ning through asynchronous advantage actor-critic on a gpu. In the International Conference on\nLearning Representations (ICLR).\nBacon, P.-L., Harb, J., and Precup, D. (2017). The option-critic architecture. In the AAAI Conference\non Artiﬁcial Intelligence (AAAI).\nBahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y.\n(2017). An actor-critic algorithm for sequence prediction. In the International Conference on\nLearning Representations (ICLR).\nBahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to\nalign and translate. In the International Conference on Learning Representations (ICLR).\nBailis, P., Olukoton, K., Re, C., and Zaharia, M. (2017). Infrastructure for Usable Machine Learning:\nThe Stanford DAWN Project. ArXiv e-prints.\nBaird, L. (1995). Residual algorithms: Reinforcement learning with function approximation. In the\nInternational Conference on Machine Learning (ICML).\n55\nBaker, B., Gupta, O., Naik, N., and Raskar, R. (2017). Designing neural network architectures using\nreinforcement learning. In the International Conference on Learning Representations (ICLR).\nBalle, B., Gomrokchi, M., and Precup, D. (2016). Differentially private policy evaluation. In the\nInternational Conference on Machine Learning (ICML).\nBalog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. (2017).\nDeepcoder:\nLearning to write programs. In the International Conference on Learning Representations (ICLR).\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2017). Emergent Complexity via\nMulti-Agent Competition. ArXiv e-prints.\nBarreto, A., Munos, R., Schaul, T., and Silver, D. (2017). Successor features for transfer in rein-\nforcement learning. In the Annual Conference on Neural Information Processing Systems (NIPS).\nBarto, A. G. and Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning.\nDiscrete Event Dynamic Systems, 13(4):341–379.\nBarto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neuronlike elements that can solve difﬁcult\nlearning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13:835–846.\nBattaglia, P. W., Pascanu, R., Lai, M., Rezende, D., and Kavukcuoglu, K. (2016). Interaction net-\nworks for learning about objects, relations and physics. In the Annual Conference on Neural\nInformation Processing Systems (NIPS).\nBazzan, A. L. and Kl¨ugl, F. (2014). Introduction to Intelligent Systems in Trafﬁc and Transportation.\nMorgan & Claypool.\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., K¨uttler, H., Lefrancq, A., Green,\nS., Vald´es, V., Sadik, A., Schrittwieser, J., Anderson, K., York, S., Cant, M., Cain, A., Bolton,\nA., Gaffney, S., King, H., Hassabis, D., Legg, S., and Petersen, S. (2016). DeepMind Lab. ArXiv\ne-prints.\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A distributional perspective on reinforcement\nlearning. In the International Conference on Machine Learning (ICML).\nBellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., and\nMunos, R. (2017). The Cramer Distance as a Solution to Biased Wasserstein Gradients. ArXiv\ne-prints.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning environment:\nAn evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–\n279.\nBellemare, M. G., Schaul, T., Srinivasan, S., Saxton, D., Ostrovski, G., and Munos, R. (2016).\nUnifying count-based exploration and intrinsic motivation. In the Annual Conference on Neural\nInformation Processing Systems (NIPS).\nBello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. (2016). Neural Combinatorial Optimiza-\ntion with Reinforcement Learning. ArXiv e-prints.\nBello, I., Zoph, B., Vasudevan, V., and Le, Q. V. (2017). Neural optimizer search with reinforcement\nlearning. In the International Conference on Machine Learning (ICML).\nBengio, Y. (2009). Learning deep architectures for ai. Foundations and trends R⃝in Machine Learn-\ning, 2(1):1–127.\nBengio, Y. (2017). The Consciousness Prior. ArXiv e-prints.\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In the Inter-\nnational Conference on Machine Learning (ICML).\nBerkenkamp, F., Turchetta, M., Schoellig, A. P., and Krause, A. (2017). Safe model-based rein-\nforcement learning with stability guarantees. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\n56\nBernhard Wymann, E. E., Guionneau, C., Dimitrakakis, C., and R´emi Coulom, A. S. (2014).\nTORCS, The Open Racing Car Simulator. ”http://www.torcs.org”.\nBerthelot, D., Schumm, T., and Metz, L. (2017). BEGAN: Boundary Equilibrium Generative Ad-\nversarial Networks. ArXiv e-prints.\nBertsekas, D. P. (2012). Dynamic programming and optimal control (Vol. II, 4th Edition: Approxi-\nmate Dynamic Programming). Athena Scientiﬁc, Massachusetts, USA.\nBertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientiﬁc.\nBhatti, S., Desmaison, A., Miksik, O., Nardelli, N., Siddharth, N., and Torr, P. H. S. (2016). Playing\nDoom with SLAM-Augmented Deep Reinforcement Learning. ArXiv e-prints.\nBishop, C. (2011). Pattern Recognition and Machine Learning. Springer.\nBlei, D. M. and Smyth, P. (2017). Science and data science. PNAS, 114(33):8689–8692.\nBohg, J., Hausman, K., Sankaran, B., Brock, O., Kragic, D., Schaal, S., and Sukhatme, G. S. (2017).\nInteractive perception: Leveraging action in perception and perception in action. IEEE Transac-\ntions on Robotics, 33(6):1273–1291.\nBojarski, M., Testa, D. D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort,\nM., Muller, U., Zhang, J., Zhang, X., Zhao, J., and Zieba, K. (2016). End to End Learning for\nSelf-Driving Cars. ArXiv e-prints.\nBojarski, M., Yeres, P., Choromanska, A., Choromanski, K., Firner, B., Jackel, L., and Muller, U.\n(2017). Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car.\nArXiv e-prints.\nBordes, A., Boureau, Y.-L., and Weston, J. (2017). Learning end-to-end goal-oriented dialog. In the\nInternational Conference on Learning Representations (ICLR).\nBotvinick, M., Barrett, D. G. T., Battaglia, P., de Freitas, N., Kumaran, D., Leibo, J. Z., Lillicrap,\nT., Modayil, J., Mohamed, S., Rabinowitz, N. C., Rezende, D. J., Santoro, A., Schaul, T., Sum-\nmerﬁeld, C., Wayne, G., Weber, T., Wierstra, D., Legg, S., and Hassabis, D. (2017). Building\nmachines that learn and think for themselves. Behavioral and Brain Sciences, 40.\nBousmalis, K., Irpan, A., Wohlhart, P., Bai, Y., Kelcey, M., Kalakrishnan, M., Downs, L., Ibarz, J.,\nPastor, P., Konolige, K., Levine, S., and Vanhoucke, V. (2017). Using Simulation and Domain\nAdaptation to Improve Efﬁciency of Deep Robotic Grasping. ArXiv e-prints.\nBowling, M., Burch, N., Johanson, M., and Tammelin, O. (2015). Heads-up limit hold’em poker is\nsolved. Science, 347(6218):145–149.\nBoyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.\nBradtke, S. J. and Barto, A. G. (1996). Linear least-squares algorithms for temporal difference\nlearning. Machine Learning, 22(1-3):33–57.\nBrandt, M. W., Goyal, A., Santa-Clara, P., and Stroud, J. R. (2005). A simulation approach to\ndynamic portfolio choice with an application to learning about return predictability. The Review\nof Financial Studies, 18(3):831–873.\nBriot, J.-P., Hadjeres, G., and Pachet, F. (2017). Deep Learning Techniques for Music Generation -\nA Survey. ArXiv e-prints.\nBrowne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P. I., Rohlfshagen, P., Tavener, S.,\nPerez, D., Samothrakis, S., and Colton, S. (2012). A survey of Monte Carlo tree search methods.\nIEEE Transactions on Computational Intelligence and AI in Games, 4(1):1–43.\nBrunner, G., Richter, O., Wang, Y., and Wattenhofer, R. (2018). Teaching a machine to read maps\nwith deep reinforcement learning. In the AAAI Conference on Artiﬁcial Intelligence (AAAI).\n57\nBusoniu, L., Babuska, R., and Schutter, B. D. (2008). A comprehensive survey of multiagent rein-\nforcement learning. IEEE Transactions on Systems, Man, and Cybernetics - Part C: Applications\nand Reviews, 38(2).\nCai, J., Shin, R., and Song, D. (2017). Making neural programming architectures generalize via\nrecursion. In the International Conference on Learning Representations (ICLR).\nCaicedo, J. C. and Lazebnik, S. (2015). Active object localization with deep reinforcement learning.\nIn the IEEE International Conference on Computer Vision (ICCV).\nCao, Q., Lin, L., Shi, Y., Liang, X., and Li, G. (2017). Attention-aware face hallucination via deep\nreinforcement learning. In the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR).\nCarleo, G. and Troyer, M. (2017). Solving the quantum many-body problem with artiﬁcial neural\nnetworks. Science, 355(6325):602–606.\nCarlini, N. and Wagner, D. (2017a). Adversarial examples are not easily detected: Bypassing ten\ndetection methods. In ACM CCS 2017 Workshop on Artiﬁcial Intelligence and Security.\nCarlini, N. and Wagner, D. (2017b). Towards evaluating the robustness of neural networks. In IEEE\nSymposium on Security and Privacy.\nCelikyilmaz, A., Deng, L., Li, L., and Wang, C. (2017). Scaffolding Networks: Incremental Learn-\ning and Teaching Through Questioning. ArXiv e-prints.\nChakraborty, B. and Murphy, S. A. (2014). Dynamic treatment regimes. Annual Review of Statistics\nand Its Application, 1:447–464.\nChebotar, Y., Hausman, K., Zhang, M., Sukhatme, G., Schaal, S., and Levine, S. (2017). Com-\nbining model-based and model-free updates for trajectory-centric reinforcement learning. In the\nInternational Conference on Machine Learning (ICML).\nChebotar, Y., Kalakrishnan, M., Yahya, A., Li, A., Schaal, S., and Levine, S. (2016). Path integral\nguided policy search. ArXiv e-prints.\nChen, J., Huang, P.-S., He, X., Gao, J., and Deng, L. (2016). Unsupervised Learning of Predictors\nfrom Unpaired Input-Output Samples. ArXiv e-prints.\nChen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016a). InfoGAN:\nInterpretable representation learning by information maximizing generative adversarial nets. In\nthe Annual Conference on Neural Information Processing Systems (NIPS).\nChen, Y.-N., Hakkani-Tur, D., Tur, G., Celikyilmaz, A., Gao, J., and Deng, L. (2016b). Knowledge\nas a Teacher: Knowledge-Guided Structural Attention Networks. ArXiv e-prints.\nChen, Y.-N. V., Hakkani-T¨ur, D., Tur, G., Gao, J., and Deng, L. (2016c). End-to-end memory\nnetworks with knowledge carryover for multi-turn spoken language understanding. In Annual\nMeeting of the International Speech Communication Association (INTERSPEECH).\nChen, Z. and Liu, B. (2016). Lifelong Machine Learning. Morgan & Claypool Publishers.\nChen, Z. and Yi, D. (2017). The Game Imitation: Deep Supervised Convolutional Networks for\nQuick Video Game AI. ArXiv e-prints.\nCheng, Y., Xu, W., He, Z., He, W., Wu, H., Sun, M., and Liu, Y. (2016). Semi-supervised learning\nfor neural machine translation. In the Association for Computational Linguistics annual meeting\n(ACL).\nCho, K. (2015). Natural Language Understanding with Distributed Representation. ArXiv e-prints.\nCho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y. (2014).\nLearning phrase representations using RNN encoder-decoder for statistical machine translation.\nIn Conference on Empirical Methods in Natural Language Processing (EMNLP).\n58\nChoi, E., Hewlett, D., Polosukhin, I., Lacoste, A., Uszkoreit, J., and Berant, J. (2017). Coarse-to-ﬁne\nquestion answering for long documents. In the Association for Computational Linguistics annual\nmeeting (ACL).\nChristiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., and Amodei, D. (2017). Deep rein-\nforcement learning from human preferences. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of gated recurrent\nneural networks on sequence modeling. In NIPS 2014 Deep Learning and Representation Learn-\ning Workshop.\nCrawford, D., Levit, A., Ghadermarzy, N., Oberoi, J. S., and Ronagh, P. (2016). Reinforcement\nLearning Using Quantum Boltzmann Machines. ArXiv e-prints.\nCzarnecki, W. M., ´Swirszcz, G., Jaderberg, M., Osindero, S., Vinyals, O., and Kavukcuoglu, K.\n(2017). Understanding Synthetic Gradients and Decoupled Neural Interfaces. ArXiv e-prints.\nDai, H., Khalil, E. B., Zhang, Y., Dilkina, B., and Song, L. (2017). Learning combinatorial opti-\nmization algorithms over graphs. In the Annual Conference on Neural Information Processing\nSystems (NIPS).\nDai, Z., Yang, Z., Yang, F., Cohen, W. W., and Salakhutdinov, R. (2017). Good Semi-supervised\nLearning that Requires a Bad GAN. ArXiv e-prints.\nDaniely, A., Frostig, R., and Singer, Y. (2016). Toward deeper understanding of neural networks:\nThe power of initialization and a dual view on expressivity. In the Annual Conference on Neural\nInformation Processing Systems (NIPS).\nDanihelka, I., Wayne, G., Uria, B., Kalchbrenner, N., and Graves, A. (2016). Associative long\nshort-term memory. In the International Conference on Machine Learning (ICML).\nDas, A., Kottur, S., Moura, J. M. F., Lee, S., and Batra, D. (2017). Learning cooperative visual dialog\nagents with deep reinforcement learning. In the IEEE International Conference on Computer\nVision (ICCV).\nDe Asis, K., Hernandez-Garcia, J. F., Zacharias Holland, G., and Sutton, R. S. (2018). Multi-step\nreinforcement learning: A unifying algorithm. In the AAAI Conference on Artiﬁcial Intelligence\n(AAAI).\nDeisenroth, M. P., Neumann, G., and Peters, J. (2013). A survey on policy search for robotics.\nFoundations and Trend in Robotics, 2:1–142.\nDeisenroth, M. P. and Rasmussen, C. E. (2011). PILCO: A model-based and data-efﬁcient approach\nto policy search. In the International Conference on Machine Learning (ICML).\nDelle Fave, F. M., Jiang, A. X., Yin, Z., Zhang, C., Tambe, M., Kraus, S., and Sullivan, J. P. (2014).\nGame-theoretic security patrolling with dynamic execution uncertainty and a case study on a real\ntransit system. 50:321–367.\nDeng,\nL.\n(2017).\nThree\ngenerations\nof\nspoken\ndialogue\nsystems\n(bots),\ntalk\nat\nAI\nFrontiers\nConference.\nhttps://www.slideshare.net/AIFrontiers/\nli-deng-three-generations-of-spoken-dialogue-systems-bots.\nDeng, L. and Dong, Y. (2014). Deep Learning: Methods and Applications. Now Publishers Inc.\nDeng, L. and Li, X. (2013). Machine learning paradigms for speech recognition: An overview. IEEE\nTransactions on Audio, Speech, and Language Processing, 21(5):1060–1089.\nDeng, L. and Liu, Y. (2017). Deep Learning in Natural Language Processing (edited book, sched-\nuled August 2017). Springer.\nDeng, Y., Bao, F., Kong, Y., Ren, Z., and Dai, Q. (2016). Deep direct reinforcement learning for\nﬁnancial signal representation and trading. IEEE Transactions on Neural Networks and Learning\nSystems.\n59\nDenil, M., Agrawal, P., Kulkarni, T. D., Erez, T., Battaglia, P., and de Freitas, N. (2017). Learning\nto perform physics experiments via deep reinforcement learning. In the International Conference\non Learning Representations (ICLR).\nDenil, M., G´omez Colmenarejo, S., Cabi, S., Saxton, D., and de Freitas, N. (2017). Programmable\nAgents. ArXiv e-prints.\nDevrim Kaba, M., Gokhan Uzunbas, M., and Nam Lim, S. (2017). A reinforcement learning ap-\nproach to the view planning problem. In the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR).\nDhingra, B., Li, L., Li, X., Gao, J., Chen, Y.-N., Ahmed, F., and Deng, L. (2017). End-to-end\nreinforcement learning of dialogue agents for information access. In the Association for Compu-\ntational Linguistics annual meeting (ACL).\nDiederik P Kingma, M. W. (2014). Auto-encoding variational bayes. In the International Conference\non Learning Representations (ICLR).\nDietterich, T. G. (2000). Hierarchical reinforcement learning with the MAXQ value function de-\ncomposition. Journal of Artiﬁcial Intelligence Research, 13(1):227–303.\nDomingos, P. (2012). A few useful things to know about machine learning. Communications of the\nACM, 55(10):78–87.\nDong, D., Wu, H., He, W., Yu, D., and Wang, H. (2015). Multi-task learning for multiple language\ntranslation. In the Association for Computational Linguistics annual meeting (ACL).\nDong, Y., Su, H., Zhu, J., and Zhang, B. (2017). Improving interpretability of deep neural networks\nwith semantic information. In the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR).\nDoshi-Velez, F. and Kim, B. (2017). Towards A Rigorous Science of Interpretable Machine Learn-\ning. ArXiv e-prints.\nDosovitskiy, A. and Koltun, V. (2017). Learning to act by predicting the future. In the International\nConference on Learning Representations (ICLR).\nDowney, C., Hefny, A., Li, B., Boots, B., and Gordon, G. (2017). Predictive state recurrent neural\nnetworks. In the Annual Conference on Neural Information Processing Systems (NIPS).\nDu, S. S., Chen, J., Li, L., Xiao, L., and Zhou, D. (2017). Stochastic variance reduction methods for\npolicy evaluation. In the International Conference on Machine Learning (ICML).\nDuan, Y., Andrychowicz, M., Stadie, B. C., Ho, J., Schneider, J., Sutskever, I., Abbeel, P., and\nZaremba, W. (2017). One-shot imitation learning. In the Annual Conference on Neural Informa-\ntion Processing Systems (NIPS).\nDuan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking deep rein-\nforcement learning for continuous control. In the International Conference on Machine Learning\n(ICML).\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. (2016). RL2: Fast\nReinforcement Learning via Slow Reinforcement Learning. ArXiv e-prints.\nDulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt, J., Mann, T., Weber,\nT., Degris, T., and Coppin, B. (2015). Deep Reinforcement Learning in Large Discrete Action\nSpaces. ArXiv e-prints.\nEl-Tantawy, S., Abdulhai, B., and Abdelgawad, H. (2013). Multiagent reinforcement learning for\nintegrated network of adaptive trafﬁc signal controllers (marlin-atsc): methodology and large-\nscale application on downtown toronto. IEEE Transactions on Intelligent Transportation Systems,\n14(3):1140–1150.\nEric, M. and Manning, C. D. (2017). A Copy-Augmented Sequence-to-Sequence Architecture Gives\nGood Performance on Task-Oriented Dialogue. ArXiv e-prints.\n60\nErnst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode reinforcement learning. The\nJournal of Machine Learning Research, 6:503–556.\nEslami, S. M. A., Heess, N., Weber, T., Tassa, Y., Szepesv´ari, D., Kavukcuoglu, K., and Hinton,\nG. E. (2016). Attend, infer, repeat: Fast scene understanding with generative models. In the\nAnnual Conference on Neural Information Processing Systems (NIPS).\nEvans, R. and Grefenstette, E. (2017). Learning Explanatory Rules from Noisy Data. ArXiv e-prints.\nEvtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A., Rahmati, A., and Song, D.\n(2017). Robust Physical-World Attacks on Deep Learning Models. ArXiv e-prints.\nFang, M., Li, Y., and Cohn, T. (2017). Learning how to active learn: A deep reinforcement learning\napproach. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\nFang, X., Misra, S., Xue, G., and Yang, D. (2012). Smart grid - the new and improved power grid:\nA survey. IEEE Communications Surveys Tutorials, 14(4):944–980.\nFatemi, M., Asri, L. E., Schulz, H., He, J., and Suleman, K. (2016). Policy networks with two-\nstage training for dialogue systems. In the Annual SIGdial Meeting on Discourse and Dialogue\n(SIGDIAL).\nFeng, J. and Zhou, Z.-H. (2017). AutoEncoder by Forest. ArXiv e-prints.\nFernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra,\nD. (2017). PathNet: Evolution Channels Gradient Descent in Super Neural Networks. ArXiv\ne-prints.\nFinn, C., Christiano, P., Abbeel, P., and Levine, S. (2016a). A connection between GANs, inverse\nreinforcement learning, and energy-based models. In NIPS 2016 Workshop on Adversarial Train-\ning.\nFinn, C. and Levine, S. (2016). Deep visual foresight for planning robot motion. In IEEE Interna-\ntional Conference on Robotics and Automation (ICRA).\nFinn, C., Levine, S., and Abbeel, P. (2016b). Guided cost learning: Deep inverse optimal control via\npolicy optimization. In the International Conference on Machine Learning (ICML).\nFinn, C., Yu, T., Fu, J., Abbeel, P., and Levine, S. (2017). Generalizing skills with semi-supervised\nreinforcement learning. In the International Conference on Learning Representations (ICLR).\nFiroiu, V., Whitney, W. F., and Tenenbaum, J. B. (2017). Beating the World’s Best at Super Smash\nBros. with Deep Reinforcement Learning. ArXiv e-prints.\nFlorensa, C., Duan, Y., and Abbeel, P. (2017). Stochastic neural networks for hierarchical reinforce-\nment learning. In the International Conference on Learning Representations (ICLR).\nFoerster, J., Assael, Y. M., de Freitas, N., and Whiteson, S. (2016).\nLearning to communicate\nwith deep multi-agent reinforcement learning. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nFoerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. (2018). Counterfactual multi-\nagent policy gradients. In the AAAI Conference on Artiﬁcial Intelligence (AAAI).\nFoerster, J., Nardelli, N., Farquhar, G., Torr, P. H. S., Kohli, P., and Whiteson, S. (2017). Stabilising\nexperience replay for deep multi-agent reinforcement learning. In the International Conference\non Machine Learning (ICML).\nFoerster, J. N., Chen, R. Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and Mordatch, I. (2017).\nLearning with Opponent-Learning Awareness. ArXiv e-prints.\nFortunato, M., Gheshlaghi Azar, M., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos,\nR., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. (2017). Noisy Networks for Exploration.\nArXiv e-prints.\n61\nFu, J., Co-Reyes, J. D., and Levine, S. (2017). Ex2: Exploration with exemplar models for deep\nreinforcement learning. In the Annual Conference on Neural Information Processing Systems\n(NIPS).\nGanin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M.,\nand Lempitsky, V. (2016). Domain-adversarial training of neural networks. Journal of Machine\nLearning Research, 17(59):1–35.\nGarc`ıa, J. and Fern`andez, F. (2015). A comprehensive survey on safe reinforcement learning. The\nJournal of Machine Learning Research, 16:1437–1480.\nGavrilovska, L., Atanasovski, V., Macaluso, I., and DaSilva, L. A. (2013). Learning and reasoning\nin cognitive radio networks. IEEE Communications Surveys Tutorials, 15(4):1761–1777.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y. N. (2017). Convolutional Sequence\nto Sequence Learning. ArXiv e-prints.\nGelly, S., Schoenauer, M., Sebag, M., Teytaud, O., Kocsis, L., Silver, D., and Szepesv´ari, C. (2012).\nThe grand challenge of computer go: Monte carlo tree search and extensions. Communications\nof the ACM, 55(3):106–113.\nGelly, S. and Silver, D. (2007). Combining online and ofﬂine knowledge in uct. In the International\nConference on Machine Learning (ICML).\nGeorge, D., Lehrach, W., Kansky, K., L´azaro-Gredilla, M., Laan, C., Marthi, B., Lou, X., Meng, Z.,\nLiu, Y., Wang, H., Lavin, A., and Phoenix, D. S. (2017). A generative vision model that trains\nwith high data efﬁciency and breaks text-based CAPTCHAs. Science.\nGeramifard, A., Dann, C., Klein, R. H., Dabney, W., and How, J. P. (2015). Rlpy: A value-function-\nbased reinforcement learning framework for education and research. Journal of Machine Learning\nResearch, 16:1573–1578.\nGeramifard, A., Walsh, T. J., Tellex, S., Chowdhary, G., Roy, N., and How, J. P. (2013). A tutorial on\nlinear function approximators for dynamic programming and reinforcement learning. Foundations\nand Trends in Machine Learning, 6(4):375–451.\nGhavamzadeh, M., Mahadevan, S., and Makar, R. (2006). Hierarchical multi-agent reinforcement\nlearning. Autonomous Agents and Multi-Agent Systems, 13(2):197–229.\nGhavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A. (2015). Bayesian reinforcement learning:\na survey. Foundations and Trends in Machine Learning, 8(5-6):359–483.\nGirshick, R. (2015).\nFast R-CNN.\nIn the IEEE International Conference on Computer Vision\n(ICCV).\nGlavic, M., Fonteneau, R., and Ernst, D. (2017). Reinforcement learning for electric power system\ndecision and control: Past considerations and perspectives. In The 20th World Congress of the\nInternational Federation of Automatic Control.\nGoldberg, Y. (2017). Neural Network Methods for Natural Language Processing. Morgan & Clay-\npool Publishers.\nGoldberg, Y. and Kosorok, M. R. (2012). Q-learning with censored data. Annals of Statistics,\n40(1):529–560.\nGoodfellow, I. (2017). NIPS 2016 Tutorial: Generative Adversarial Networks. ArXiv e-prints.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., ,\nand Bengio, Y. (2014). Generative adversarial nets. In the Annual Conference on Neural Infor-\nmation Processing Systems (NIPS), page 2672?2680.\nGraves, A., Bellemare, M. G., Menick, J., Munos, R., and Kavukcuoglu, K. (2017). Automated\nCurriculum Learning for Neural Networks. ArXiv e-prints.\n62\nGraves, A., Wayne, G., and Danihelka, I. (2014). Neural Turing Machines. ArXiv e-prints.\nGraves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi´nska, A., Col-\nmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., nech Badia, A. P., Hermann, K. M.,\nZwols, Y., Ostrovski, G., Cain, A., King, H., Summerﬁeld, C., Blunsom, P., Kavukcuoglu, K., and\nHassabis, D. (2016). Hybrid computing using a neural network with dynamic external memory.\nNature, 538:471–476.\nGregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. (2015). Draw: A recurrent\nneural network for image generation.\nIn the International Conference on Machine Learning\n(ICML).\nGrondman, I., Busoniu, L., Lopes, G. A., and Babuˇska, R. (2012). A survey of actor-critic rein-\nforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man,\nand Cybernetics, Part C (Applications and Reviews), 42(6):1291–1307.\nGruslys, A., Gheshlaghi Azar, M., Bellemare, M. G., and Munos, R. (2017).\nThe Reactor: A\nSample-Efﬁcient Actor-Critic Architecture. ArXiv e-prints.\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2016a). Deep reinforcement learning for robotic\nmanipulation with asynchronous off-policy updates. ArXiv e-prints.\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., and Levine, S. (2017).\nQ-Prop: Sample-\nefﬁcient policy gradient with an off-policy critic. In the International Conference on Learning\nRepresentations (ICLR).\nGu, S., Lillicrap, T., Ghahramani, Z., Turner, R. E., Sch¨olkopf, B., and Levine, S. (2017). Interpo-\nlated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforce-\nment learning. In the Annual Conference on Neural Information Processing Systems (NIPS).\nGu, S., Lillicrap, T., Sutskever, I., and Levine, S. (2016b). Continuous deep Q-learning with model-\nbased acceleration. In the International Conference on Machine Learning (ICML).\nGulcehre, C., Chandar, S., Cho, K., and Bengio, Y. (2016). Dynamic Neural Turing Machine with\nSoft and Hard Addressing Schemes. ArXiv e-prints.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. (2017). Improved training\nof wasserstein gans. In the Annual Conference on Neural Information Processing Systems (NIPS).\nGupta, A., Devin, C., Liu, Y., Abbeel, P., and Levine, S. (2017a). Learning invariant feature spaces\nto transfer skills with reinforcement learning. In the International Conference on Learning Rep-\nresentations (ICLR).\nGupta, S., Davidson, J., Levine, S., Sukthankar, R., and Malik, J. (2017b). Cognitive mapping\nand planning for visual navigation. In the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR).\nGuu, K., Pasupat, P., Liu, E. Z., and Liang, P. (2017).\nFrom language to programs: Bridging\nreinforcement learning and maximum marginal likelihood. In the Association for Computational\nLinguistics annual meeting (ACL).\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-\nbased policies. In the International Conference on Machine Learning (ICML).\nHadﬁeld-Menell, D., Dragan, A., Abbeel, P., and Russell, S. (2016). Cooperative inverse reinforce-\nment learning. In the Annual Conference on Neural Information Processing Systems (NIPS).\nHadﬁeld-Menell, D., Milli, S., Abbeel, P., Russell, S., and Dragan, A. (2017). Inverse reward design.\nIn the Annual Conference on Neural Information Processing Systems (NIPS).\nHan, S., Mao, H., and Dally, W. J. (2016).\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and Huffman coding. In the International Conference\non Learning Representations (ICLR).\n63\nHarrison, B., Ehsan, U., and Riedl, M. O. (2017). Rationalization: A Neural Machine Translation\nApproach to Generating Natural Language Explanations. ArXiv e-prints.\nHarutyunyan, A., Vrancx, P., Bacon, P.-L., Precup, D., and Nowe, A. (2018). Learning with options\nthat terminate off-policy. In the AAAI Conference on Artiﬁcial Intelligence (AAAI).\nHassabis, D., Kumaran, D., Summerﬁeld, C., and Botvinick, M. (2017). Neuroscience-inspired\nartiﬁcial intelligence. Neuron, 95:245–258.\nHastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning: Data\nMining, Inference, and Prediction. Springer.\nHausknecht, M. and Stone, P. (2015). Deep recurrent Q-learning for partially observable MDPs. In\nthe AAAI Conference on Artiﬁcial Intelligence (AAAI).\nHausknecht, M. and Stone, P. (2016). Deep reinforcement learning in parameterized action space.\nIn the International Conference on Learning Representations (ICLR).\nHaykin, S. (2005). Cognitive radio: brain-empowered wireless communications. IEEE Journal on\nSelected Areas in Communications, 23(2):201–220.\nHaykin, S. (2008). Neural Networks and Learning Machines (third edition). Prentice Hall.\nHe, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T.-Y., and Ma, W.-Y. (2016a).\nDual learning\nfor machine translation. In the Annual Conference on Neural Information Processing Systems\n(NIPS).\nHe, F. S., Liu, Y., Schwing, A. G., and Peng, J. (2017). Learning to play in a day: Faster deep\nreinforcement learning by optimality tightening. In the International Conference on Learning\nRepresentations (ICLR).\nHe, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. (2016b). Deep reinforcement\nlearning with a natural language action space. In the Association for Computational Linguistics\nannual meeting (ACL).\nHe, J., Ostendorf, M., He, X., Chen, J., Gao, J., Li, L., and Deng, L. (2016c). Deep reinforcement\nlearning with a combinatorial action space for predicting popular reddit threads. In Conference\non Empirical Methods in Natural Language Processing (EMNLP).\nHe, K., Gkioxari, G., Doll´ar, P., and Girshick, R. (2017). Mask R-CNN. In the IEEE International\nConference on Computer Vision (ICCV).\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016d). Deep residual learning for image recognition. In\nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nHe, L., Lee, K., Lewis, M., and Zettlemoyer, L. (2017). Deep semantic role labeling: What works\nand what’s next. In the Association for Computational Linguistics annual meeting (ACL).\nHe, X. and Deng, L. (2013).\nSpeech-centric information processing: An optimization-oriented\napproach. Proceedings of the IEEE — Vol. 101, No. 5, May 2013, 101(5):1116–1135.\nHeaton, J. B., Polson, N. G., and Witte, J. H. (2016). Deep learning for ﬁnance: deep portfolios.\nApplied Stochastic Models in Business and Industry.\nHeess, N., TB, D., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y., Erez, T., Wang, Z.,\nEslami, A., Riedmiller, M., and Silver, D. (2017). Emergence of Locomotion Behaviours in Rich\nEnvironments. ArXiv e-prints.\nHein, D., Depeweg, S., Tokic, M., Udluft, S., Hentschel, A., Runkler, T. A., and Sterzing, V. (2017).\nA benchmark environment motivated by industrial control problems. In IEEE Symposium on\nAdaptive Dynamic Programming and Reinforcement Learning (IEEE ADPRL’17).\nHeinrich, J. and Silver, D. (2016).\nDeep reinforcement learning from self-play in imperfect-\ninformation games. In NIPS 2016 Deep Reinforcement Learning Workshop.\n64\nHeld, D., Geng, X., Florensa, C., and Abbeel, P. (2017). Automatic Goal Generation for Reinforce-\nment Learning Agents. ArXiv e-prints.\nHenaff, M., Whitney, W. F., and LeCun, Y. (2017).\nModel-Based Planning in Discrete Action\nSpaces. ArXiv e-prints.\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot,\nB., Azar, M., and Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement\nLearning. In the AAAI Conference on Artiﬁcial Intelligence (AAAI).\nHester, T. and Stone, P. (2017). Intrinsically motivated model learning for developing curious robots.\nArtiﬁcial Intelligence, 247:170–86.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan, J.,\nSendonaris, A., Dulac-Arnold, G., Osband, I., Agapiou, J., Leibo, J. Z., and Gruslys, A. (2018).\nDeep Q-learning from demonstrations. In the AAAI Conference on Artiﬁcial Intelligence (AAAI).\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner,\nA. (2017). β-VAE: Learning basic visual concepts with a constrained variational framework. In\nthe International Conference on Learning Representations (ICLR).\nHinton, G., Deng, L., Yu, D., Dahl, G. E., rahman Mohamed, A., Jaitly, N., Senior, A., Vanhoucke,\nV., Nguyen, P., Sainath, T. N., , and Kingsbury, B. (2012). Deep neural networks for acoustic\nmodeling in speech recognition. IEEE Signal Processing Magazine, 82.\nHinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786):504–507.\nHirschberg, J. and Manning, C. D. (2015). Advances in natural language processing. Science,\n349(6245):261–266.\nHo, J. and Ermon, S. (2016). Generative adversarial imitation learning. In the Annual Conference\non Neural Information Processing Systems (NIPS).\nHo, J., Gupta, J. K., and Ermon, S. (2016). Model-free imitation learning with policy optimization.\nIn the International Conference on Machine Learning (ICML).\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9:1735–\n1780.\nHoshen, Y. (2017). Vain: Attentional multi-agent predictive modeling. In the Annual Conference\non Neural Information Processing Systems (NIPS).\nHouthooft, R., Chen, X., Duan, Y., Schulman, J., Turck, F. D., and Abbeel, P. (2016).\nVime:\nVariational information maximizing exploration. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nHu, Z., Yang, Z., Salakhutdinov, R., and Xing, E. P. (2017). On Unifying Deep Generative Models.\nArXiv e-prints.\nHuang, G., Liu, Z., Weinberger, K. Q., and van der Maaten, L. (2017). Densely connected convolu-\ntional networks. In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nHuang, S., Papernot, N., Goodfellow, I., Duan, Y., and Abbeel, P. (2017). Adversarial Attacks on\nNeural Network Policies. ArXiv e-prints.\nHuk Park, D., Hendricks, L. A., Akata, Z., Schiele, B., Darrell, T., and Rohrbach, M. (2016). Atten-\ntive Explanations: Justifying Decisions and Pointing to the Evidence. ArXiv e-prints.\nHull, J. C. (2014). Options, Futures and Other Derivatives (9th edition). Prentice Hall.\nIan J. Goodfellow, Jonathon Shlens, C. S. (2015). Explaining and harnessing adversarial examples.\nIn the International Conference on Learning Representations (ICLR).\n65\nIoffe, S. and Szegedy, C. (2015).\nBatch normalization: Accelerating deep network training by\nreducing internal covariate shift. In the International Conference on Machine Learning (ICML).\nIslam, R., Henderson, P., Gomrokchi, M., and Precup, D. (2017). Reproducibility of benchmarked\ndeep reinforcement learning tasks for continuous control. In ICML 2017 Reproducibility in Ma-\nchine Learning Workshop.\nJaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., Vinyals, O.,\nGreen, T., Dunning, I., Simonyan, K., Fernando, C., and Kavukcuoglu, K. (2017). Population\nBased Training of Neural Networks. ArXiv e-prints.\nJaderberg, M., Mnih, V., Czarnecki, W., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K.\n(2017). Reinforcement learning with unsupervised auxiliary tasks. In the International Confer-\nence on Learning Representations (ICLR).\nJaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K. (2015). Spatial transformer\nnetworks. In the Annual Conference on Neural Information Processing Systems (NIPS).\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). An Introduction to Statistical Learning\nwith Applications in R. Springer.\nJaques, N., Gu, S., Turner, R. E., and Eck, D. (2017).\nTuning recurrent neural networks with\nreinforcement learning. Submitted to Int’l Conference on Learning Representations.\nJiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2016). Contextual\nDecision Processes with Low Bellman Rank are PAC-Learnable. ArXiv e-prints.\nJie, Z., Liang, X., Feng, J., Jin, X., Lu, W. F., and Yan, S. (2016). Tree-structured reinforcement\nlearning for sequential object localization.\nIn the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nJohansson, F. D., Shalit, U., and Sontag, D. (2016). Learning representations for counterfactual\ninference. In the International Conference on Machine Learning (ICML).\nJohnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N., Vi´egas, F., Watten-\nberg, M., Corrado, G., Hughes, M., and Dean, J. (2016). Google’s Multilingual Neural Machine\nTranslation System: Enabling Zero-Shot Translation. ArXiv e-prints.\nJordan, M. I. and Mitchell, T. (2015). Machine learning: Trends, perspectives, and prospects. Sci-\nence, 349(6245):255–260.\nJoulin, A., Grave, E., Bojanowski, P., and Mikolov, T. (2017). Bag of tricks for efﬁcient text clas-\nsiﬁcation. In Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics (EACL).\nJurafsky, D. and Martin, J. H. (2017). Speech and Language Processing (3rd ed. draft). Prentice\nHall.\nJustesen, N., Bontrager, P., Togelius, J., and Risi, S. (2017). Deep Learning for Video Game Playing.\nArXiv e-prints.\nJustesen, N. and Risi, S. (2017). Learning macromanagement in starcraft from replays using deep\nlearning. In IEEE Conference on Computational Intelligence and Games (CIG).\nKadlec, R., Schmid, M., Bajgar, O., and Kleindienst, J. (2016). Text Understanding with the Atten-\ntion Sum Reader Network. ArXiv e-prints.\nKaelbling, L. P., Littman, M. L., and Moore, A. (1996). Reinforcement learning: A survey. Journal\nof Artiﬁcial Intelligence Research, 4:237–285.\nKaiser, L. and Bengio, S. (2016). Can active memory replace attention? In the Annual Conference\non Neural Information Processing Systems (NIPS).\nKaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N., Jones, L., and Uszkoreit, J. (2017a).\nOne Model To Learn Them All. ArXiv e-prints.\n66\nKaiser, Ł., Nachum, O., Roy, A., and Bengio, S. (2017b). Learning to Remember Rare Events. In\nthe International Conference on Learning Representations (ICLR).\nKakade, S. (2002). A natural policy gradient. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Conference\non Empirical Methods in Natural Language Processing (EMNLP).\nKandasamy, K., Bachrach, Y., Tomioka, R., Tarlow, D., and Carter, D. (2017). Batch policy gradient\nmethods for improving neural conversation models. In the International Conference on Learning\nRepresentations (ICLR).\nKansky, K., Silver, T., M´ely, D. A., Eldawy, M., L´azaro-Gredilla, M., Lou, X., Dorfman, N., Sidor,\nS., Phoenix, S., and George, D. (2017). Schema networks: Zero-shot transfer with a generative\ncausal model of intuitive physics. In the International Conference on Machine Learning (ICML).\nKarpathy, A., Johnson, J., and Fei-Fei, L. (2016). Visualizing and understanding recurrent networks.\nIn ICLR 2016 Workshop.\nKavosh and Littman, M. L. (2017). A new softmax operator for reinforcement learning. In the\nInternational Conference on Machine Learning (ICML).\nKawaguchi, K., Pack Kaelbling, L., and Bengio, Y. (2017). Generalization in Deep Learning. ArXiv\ne-prints.\nKempka, M., Wydmuch, M., Runc, G., Toczek, J., and Jas´kowski, W. (2016). ViZDoom: A Doom-\nbased AI research platform for visual reinforcement learning. In IEEE Conference on Computa-\ntional Intelligence and Games.\nKhandani, A. E., Kim, A. J., and Lo, A. W. (2010). Consumer credit-risk models via machine-\nlearning algorithms. Journal of Banking & Finance, 34:2767–2787.\nKillian, T., Daulton, S., Konidaris, G., and Doshi-Velez, F. (2017). Robust and efﬁcient transfer\nlearning with hidden-parameter markov decision processes. In the Annual Conference on Neural\nInformation Processing Systems (NIPS).\nKim, B., massoud Farahmand, A., Pineau, J., and Precup, D. (2014). Learning from limited demon-\nstrations. In the Annual Conference on Neural Information Processing Systems (NIPS).\nKingma, D. P., Rezende, D. J., Mohamed, S., and Welling, M. (2014). Semi-supervised learning with\ndeep generative models. In the Annual Conference on Neural Information Processing Systems\n(NIPS).\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K.,\nQuan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and\nHadsell, R. (2017). Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–\n3526.\nKlambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. (2017). Self-Normalizing Neural\nNetworks. ArXiv e-prints.\nKlein, G., Kim, Y., Deng, Y., Senellart, J., and Rush, A. M. (2017). OpenNMT: Open-Source Toolkit\nfor Neural Machine Translation. ArXiv e-prints.\nKober, J., Bagnell, J. A., and Peters, J. (2013).\nReinforcement learning in robotics: A survey.\nInternational Journal of Robotics Research, 32(11):1238–1278.\nKoch, G., Zemel, R., and Salakhutdinov, R. (2015). Siamese neural networks for one-shot image\nrecognition. In the International Conference on Machine Learning (ICML).\nKoh, P. W. and Liang, P. (2017). Understanding black-box predictions via inﬂuence functions. In\nthe International Conference on Machine Learning (ICML).\n67\nKompella, V. R., Stollenga, M., Luciw, M., and Schmidhuber, J. (2017). Continual curiosity-driven\nskill acquisition from high-dimensional video inputs for humanoid robots. Artiﬁcial Intelligence,\n247:313–335.\nKong, X., Xin, B., Wang, Y., and Hua, G. (2017). Collaborative deep reinforcement learning for\njoint object search. In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nKosorok, M. R. and Moodie, E. E. M. (2015). Adaptive Treatment Strategies in Practice: Plan-\nning Trials and Analyzing Data for Personalized Medicine. ASA-SIAM Series on Statistics and\nApplied Probability.\nKottur, S., Moura, J. M., Lee, S., and Batra, D. (2017). Natural language does not emerge ’naturally’\nin multi-agent dialog. In Conference on Empirical Methods in Natural Language Processing\n(EMNLP).\nKrakovsky, M. (2016). Reinforcement renaissance. Communications of the ACM, 59(8):12–14.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation with deep convo-\nlutional neural networks. In the Annual Conference on Neural Information Processing Systems\n(NIPS).\nKrull, A., Brachmann, E., Nowozin, S., Michel, F., Shotton, J., and Rother, C. (2017). Poseagent:\nBudget-constrained 6d object pose estimation via reinforcement learning. In the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR).\nKuhn, M. and Johnson, K. (2013). Applied Predictive Modeling. Springer.\nKulkarni, T. D., Narasimhan, K. R., Saeedi, A., and Tenenbaum, J. B. (2016). Hierarchical deep\nreinforcement learning: Integrating temporal abstraction and intrinsic motivation. In the Annual\nConference on Neural Information Processing Systems (NIPS).\nKulkarni, T. D., Whitney, W., Kohli, P., and Tenenbaum, J. B. (2015). Deep convolutional inverse\ngraphics network. In the Annual Conference on Neural Information Processing Systems (NIPS).\nLagoudakis, M. G. and Parr, R. (2003). Least-squares policy iteration. The Journal of Machine\nLearning Research, 4:1107 – 1149.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through\nprobabilistic program induction. Science, 350(6266):1332–1338.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2016). Building machines that\nlearn and think like people. Behavioral and Brain Sciences, 24:1–101.\nLamb, A., Goyal, A., Zhang, Y., Zhang, S., Courville, A., and Bengio, Y. (2016). Professor forcing:\nA new algorithm for training recurrent networks. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nLample, G. and Chaplot, D. S. (2017). Playing FPS games with deep reinforcement learning. In the\nAAAI Conference on Artiﬁcial Intelligence (AAAI).\nLanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Perolat, J., Silver, D., and Graepel,\nT. (2017). A uniﬁed game-theoretic approach to multiagent reinforcement learning. In the Annual\nConference on Neural Information Processing Systems (NIPS).\nLe, Q. V., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G. S., Dean, J., and Ng, A. Y.\n(2012). Building high-level features using large scale unsupervised learning. In the International\nConference on Machine Learning (ICML).\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521:436–444.\nLee, A. X., Levine, S., and Abbeel, P. (2017). Learning visual servoing with deep features and trust\nregion ﬁtted Q-iteration. In the International Conference on Learning Representations (ICLR).\nLehman, J., Chen, J., Clune, J., and Stanley, K. O. (2017). Safe Mutations for Deep and Recurrent\nNeural Networks through Output Gradients. ArXiv e-prints.\n68\nLei, T., Barzilay, R., and Jaakkola, T. (2016). Rationalizing neural predictions. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nLeibo, J. Z., de Masson d’Autume, C., Zoran, D., Amos, D., Beattie, C., Anderson, K., Garc´ıa\nCasta˜neda, A., Sanchez, M., Green, S., Gruslys, A., Legg, S., Hassabis, D., and Botvinick, M. M.\n(2018). Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents. ArXiv\ne-prints.\nLeibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., and Graepel, T. (2017). Multi-agent reinforce-\nment learning in sequential social dilemmas. In the International Conference on Autonomous\nAgents & Multiagent Systems (AAMAS).\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. (2016a). End-to-end training of deep visuomotor\npolicies. The Journal of Machine Learning Research, 17:1–40.\nLevine, S., Pastor, P., Krizhevsky, A., and Quillen, D. (2016b). Learning Hand-Eye Coordination\nfor Robotic Grasping with Deep Learning and Large-Scale Data Collection. ArXiv e-prints.\nLewis, M., Yarats, D., Dauphin, Y. N., Parikh, D., and Batra, D. (2017). Deal or no deal? end-to-end\nlearning for negotiation dialogues. In FAIR.\nLeyton-Brown, K. and Shoham, Y. (2008). Essentials of Game Theory: A Concise, Multidisciplinary\nIntroduction. Morgan & Claypool Publishers.\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. (2017a). Dialogue learning with\nhuman-in-the-loop. In the International Conference on Learning Representations (ICLR).\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. (2017b). Learning through dialogue\ninteractions by asking questions. In the International Conference on Learning Representations\n(ICLR).\nLi, J., Monroe, W., and Jurafsky, D. (2016a). A Simple, Fast Diverse Decoding Algorithm for Neural\nGeneration. ArXiv e-prints.\nLi, J., Monroe, W., and Jurafsky, D. (2016b). Understanding Neural Networks through Representa-\ntion Erasure. ArXiv e-prints.\nLi, J., Monroe, W., and Jurafsky, D. (2017a). Learning to Decode for Future Success. ArXiv e-prints.\nLi, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D. (2016c). Deep reinforcement\nlearning for dialogue generation.\nIn Conference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nLi, K. and Malik, J. (2017). Learning to optimize. In the International Conference on Learning\nRepresentations (ICLR).\nLi, K. and Malik, J. (2017). Learning to Optimize Neural Nets. ArXiv e-prints.\nLi, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to person-\nalized news article recommendation. In the International World Wide Web Conference (WWW).\nLi, X., Chen, Y.-N., Li, L., and Gao, J. (2017b). End-to-End Task-Completion Neural Dialogue\nSystems. ArXiv e-prints.\nLi, X., Li, L., Gao, J., He, X., Chen, J., Deng, L., and He, J. (2015). Recurrent Reinforcement\nLearning: A Hybrid Approach. ArXiv e-prints.\nLi, X., Lipton, Z. C., Dhingra, B., Li, L., Gao, J., and Chen, Y.-N. (2016d). A User Simulator for\nTask-Completion Dialogues. ArXiv e-prints.\nLi, Y., Song, J., and Ermon, S. (2017). Infogail: Interpretable imitation learning from visual demon-\nstrations. In the Annual Conference on Neural Information Processing Systems (NIPS).\nLi, Y., Szepesv´ari, C., and Schuurmans, D. (2009). Learning exercise policies for American options.\nIn International Conference on Artiﬁcial Intelligence and Statistics (AISTATS09).\n69\nLiang, C., Berant, J., Le, Q., Forbus, K. D., and Lao, N. (2017a). Neural symbolic machines: Learn-\ning semantic parsers on freebase with weak supervision. In the Association for Computational\nLinguistics annual meeting (ACL).\nLiang, C., Berant, J., Le, Q., Forbus, K. D., and Lao, N. (2017b). Neural symbolic machines: Learn-\ning semantic parsers on freebase with weak supervision. In the Association for Computational\nLinguistics annual meeting (ACL).\nLiang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gonzalez, J., Goldberg, K., and Stoica, I.\n(2017c). Ray rllib: A composable and scalable reinforcement learning library. In NIPS 2017\nDeep Reinforcement Learning Symposium.\nLiang, X., Lee, L., and Xing, E. P. (2017d). Deep variation-structured reinforcement learning for\nvisual relationship and attribute detection.\nIn the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR).\nLiang, Y., Machado, M. C., Talvitie, E., and Bowling, M. (2016). State of the art control of atari\ngames using shallow reinforcement learning. In the International Conference on Autonomous\nAgents & Multiagent Systems (AAMAS).\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.\n(2016). Continuous control with deep reinforcement learning. In the International Conference on\nLearning Representations (ICLR).\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and\nteaching. Machine learning, 8(3):293–321.\nLin, Z., Gehring, J., Khalidov, V., and Synnaeve, G. (2017). Stardata: A starcraft ai research dataset.\nIn AAAI Conference on Artiﬁcial Intelligence and Interactive Digital Entertainment (AIIDE).\nLing, Y., Hasan, S. A., Datla, V., Qadir, A., Lee, K., Liu, J., and Farri, O. (2017). Diagnostic infer-\nencing via improving clinical concept extraction with deep reinforcement learning: A preliminary\nstudy. In Machine Learning for Healthcare.\nLipton, Z. C. (2016). The Mythos of Model Interpretability. ArXiv e-prints.\nLipton, Z. C., Gao, J., Li, L., Li, X., Ahmed, F., and Deng, L. (2016). Efﬁcient Exploration for\nDialogue Policy Learning with BBQ Networks & Replay Buffer Spiking. ArXiv e-prints.\nLittman, M. L. (2015). Reinforcement learning improves behaviour from evaluative feedback. Na-\nture, 521:445–451.\nLiu, B. (2012). Sentiment Analysis and Opinion Mining. Morgan & Claypool Publishers.\nLiu, C. and Tomizuka, M. (2016). Algorithmic safety measures for intelligent industrial co-robots.\nIn IEEE International Conference on Robotics and Automation (ICRA).\nLiu, C. and Tomizuka, M. (2017). Designing the robot behavior for safe human robot interactions,\nin Trends in Control and Decision-Making for Human-Robot Collaboration Systems (Y. Wang and\nF. Zhang (Eds.)). Springer.\nLiu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L., Yuille, A., Huang, J., and Murphy, K.\n(2017). Progressive Neural Architecture Search. ArXiv e-prints.\nLiu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., and Lu, J. (2017). 3DCNN-DQN-RNN: A\ndeep reinforcement learning framework for semantic parsing of large-scale 3d point clouds. In\nthe IEEE International Conference on Computer Vision (ICCV).\nLiu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K. (2017). Hierarchical Rep-\nresentations for Efﬁcient Architecture Search. ArXiv e-prints.\nLiu, N., Li, Z., Xu, Z., Xu, J., Lin, S., Qiu, Q., Tang, J., and Wang, Y. (2017). A hierarchical frame-\nwork of cloud resource allocation and power management using deep reinforcement learning. In\n37th IEEE International Conference on Distributed Computing (ICDCS 2017).\n70\nLiu, S., Zhu, Z., Ye, N., Guadarrama, S., and Murphy, K. (2016). Improved Image Captioning via\nPolicy Gradient optimization of SPIDEr. ArXiv e-prints.\nLiu, Y., Chen, J., and Deng, L. (2017). Unsupervised Sequence Classiﬁcation using Sequential\nOutput Statistics. ArXiv e-prints.\nLiu, Y.-E., Mandel, T., Brunskill, E., and Popovi´c, Z. (2014). Trading off scientiﬁc knowledge and\nuser learning with multi-armed bandits. In Educational Data Mining (EDM).\nLo, A. W. (2004).\nThe Adaptive Markets Hypothesis: Market efﬁciency from an evolutionary\nperspective. Journal of Portfolio Management, 30:15–29.\nLong, M., Cao, Y., Wang, J., and Jordan, M. I. (2015). Learning transferable features with deep\nadaptation networks. In the International Conference on Machine Learning (ICML).\nLong, M., Cao, Z., Wang, J., and Yu, P. S. (2017). Learning multiple tasks with multilinear relation-\nship networks. In the Annual Conference on Neural Information Processing Systems (NIPS).\nLong, M., Zhu, H., Wang, J., and Jordan, M. I. (2016). Unsupervised domain adaptation with\nresidual transfer networks. In the Annual Conference on Neural Information Processing Systems\n(NIPS).\nLongstaff, F. A. and Schwartz, E. S. (2001). Valuing American options by simulation: a simple\nleast-squares approach. The Review of Financial Studies, 14(1):113–147.\nLoos, S., Irving, G., Szegedy, C., and Kaliszyk, C. (2017). Deep Network Guided Proof Search.\nArXiv e-prints.\nLopez-Paz, D. and Ranzato, M. (2017). Gradient Episodic Memory for Continuum Learning. ArXiv\ne-prints.\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I. (2017). Multi-agent actor-critic\nfor mixed cooperative-competitive environments. In the Annual Conference on Neural Informa-\ntion Processing Systems (NIPS).\nLu, J., Xiong, C., Parikh, D., and Socher, R. (2016). Knowing When to Look: Adaptive Attention\nvia A Visual Sentinel for Image Captioning. ArXiv e-prints.\nLuenberger, D. G. (1997). Investment Science. Oxford University Press.\nLuo, Y., Chiu, C.-C., Jaitly, N., and Sutskever, I. (2016). Learning Online Alignments with Contin-\nuous Rewards Policy Gradient. ArXiv e-prints.\nMachado, M. C., Bellemare, M. G., and Bowling, M. (2017). A Laplacian framework for option dis-\ncovery in reinforcement learning. In the International Conference on Machine Learning (ICML).\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M.\n(2017). Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems\nfor General Agents. ArXiv e-prints.\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2017). Towards Deep Learning\nModels Resistant to Adversarial Attacks. ArXiv e-prints.\nMahler, J., Liang, J., Niyaz, S., Laskey, M., Doan, R., Liu, X., Aparicio Ojea, J., and Goldberg, K.\n(2017). Dex-Net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic\ngrasp metrics. In Robotics: Science and Systems (RSS).\nMahmood, A. R., van Hasselt, H., and Sutton, R. S. (2014). Weighted importance sampling for\noff-policy learning with linear function approximation.\nIn the Annual Conference on Neural\nInformation Processing Systems (NIPS).\nMandel, T., Liu, Y. E., Levine, S., Brunskill, E., and Popovi´c, Z. (2014). Ofﬂine policy evaluation\nacross representations with applications to educational games. In the International Conference\non Autonomous Agents & Multiagent Systems (AAMAS).\n71\nManning, C. D., Raghavan, P., and Sch¨utze, H. (2008). Introduction to Information Retrieval. Cam-\nbridge University Press.\nMannion, P., Duggan, J., and Howley, E. (2016). An experimental review of reinforcement learn-\ning algorithms for adaptive trafﬁc signal control. Autonomic Road Transport Support Systems,\nedited by McCluskey, T., Kotsialos, A., M¨uller, J., Kl¨ugl, F., Rana, O., and Schumann R., Springer\nInternational Publishing, Cham, pages 47–66.\nMao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016). Resource management with deep\nreinforcement learning. In ACM Workshop on Hot Topics in Networks (HotNets).\nMao, X., Li, Q., Xie, H., Lau, R. Y. K., and Wang, Z. (2016). Least Squares Generative Adversarial\nNetworks. ArXiv e-prints.\nMathe, S., Pirinen, A., and Sminchisescu, C. (2016).\nReinforcement learning for visual object\ndetection. In the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nMatiisen, T., Oliver, A., Cohen, T., and Schulman, J. (2017). Teacher-Student Curriculum Learning.\nArXiv e-prints.\nMaurer, A., Pontil, M., and Romera-Paredes, B. (2016). The beneﬁt of multitask representation\nlearning. The Journal of Machine Learning Research, 17(81):1–32.\nMcAllister, R. and Rasmussen, C. E. (2017). Data-efﬁcient reinforcement learning in continuous-\nstate POMDPs. In the Annual Conference on Neural Information Processing Systems (NIPS).\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R. (2017). Learned in Translation: Contextualized\nWord Vectors. ArXiv e-prints.\nMelis, G., Dyer, C., and Blunsom, P. (2017).\nOn the State of the Art of Evaluation in Neural\nLanguage Models. ArXiv e-prints.\nMerel, J., Tassa, Y., TB, D., Srinivasan, S., Lemmon, J., Wang, Z., Wayne, G., and Heess, N. (2017).\nLearning human behaviors from motion capture by adversarial imitation. ArXiv e-prints.\nMesnil, G., Dauphin, Y., Yao, K., Bengio, Y., Deng, L., He, X., Heck, L., Tur, G., Hakkani-T¨ur,\nD., Yu, D., and Zweig, G. (2015). Using recurrent neural networks for slot ﬁlling in spoken\nlanguage understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n23(3):530–539.\nMestres, A., Rodriguez-Natal, A., Carner, J., Barlet-Ros, P., Alarc´on, E., Sol´e, M., Munt´es, V.,\nMeyer, D., Barkai, S., Hibbett, M. J., Estrada, G., Ma`ruf, K., Coras, F., Ermagan, V., Latapie,\nH., Cassar, C., Evans, J., Maino, F., Walrand, J., and Cabellos, A. (2016). Knowledge-Deﬁned\nNetworking. ArXiv e-prints.\nMhamdi, E. M. E., Guerraoui, R., Hendrikx, H., and Maurer, A. (2017). Dynamic safe interrupt-\nibility for decentralized multi-agent reinforcement learning. In the Annual Conference on Neural\nInformation Processing Systems (NIPS).\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations\nin vector space. In the International Conference on Learning Representations (ICLR).\nMikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and Joulin, A. (2017). Advances in Pre-Training\nDistributed Word Representations. ArXiv e-prints.\nMiller, T. (2017). Explanation in Artiﬁcial Intelligence: Insights from the Social Sciences. ArXiv\ne-prints.\nMiotto, R., Wang, F., Wang, S., Jiang, X., and Dudley, J. T. (2017). Deep learning for healthcare:\nreview, opportunities and challenges. Brieﬁngs in Bioinformatics, pages 1–11.\nMirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., Kumar, N., and Moham-\nmad Norouzi, Samy Bengio, J. D. (2017). Device placement optimization with reinforcement\nlearning. In the International Conference on Machine Learning (ICML).\n72\nMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A., Banino, A., Denil, M., Goroshin, R.,\nSifre, L., Kavukcuoglu, K., Kumaran, D., and Hadsell, R. (2017). Learning to navigate in complex\nenvironments. In the International Conference on Learning Representations (ICLR).\nMitra, B. and Craswell, N. (2017). Neural Models for Information Retrieval. ArXiv e-prints.\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., and\nKavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In the In-\nternational Conference on Machine Learning (ICML).\nMnih, V., Heess, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent models of visual attention.\nIn the Annual Conference on Neural Information Processing Systems (NIPS).\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,\nRiedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou,\nI., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529–533.\nMo, K., Li, S., Zhang, Y., Li, J., and Yang, Q. (2016). Personalizing a Dialogue System with Transfer\nLearning. ArXiv e-prints.\nMonroe, D. (2017). Deep learning takes on translation. Communications of the ACM, 60(6):12–14.\nMoody, J. and Saffell, M. (2001). Learning to trade via direct reinforcement. IEEE Transactions on\nNeural Networks, 12(4):875–889.\nMoravˇc´ık, M., Schmid, M., Burch, N., Lis´y, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Jo-\nhanson, M., and Bowling, M. (2017). Deepstack: Expert-level artiﬁcial intelligence in heads-up\nno-limit poker. Science.\nM¨uller, M. (2002). Computer go. Artiﬁcial Intelligence, 134(1-2):145–179.\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M. G. (2016). Safe and efﬁcient off-\npolicy reinforcement learning. In the Annual Conference on Neural Information Processing Sys-\ntems (NIPS).\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. The MIT Press.\nNachum, O., Norouzi, M., and Schuurmans, D. (2017). Improving policy gradient by exploring\nunder-appreciated rewards. In the International Conference on Learning Representations (ICLR).\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the gap between value and\npolicy based reinforcement learning. In the Annual Conference on Neural Information Processing\nSystems (NIPS).\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam,\nV., Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver,\nD. (2015). Massively parallel methods for deep reinforcement learning. In ICML 2015 Deep\nLearning Workshop.\nNarasimhan, K., Kulkarni, T., and Barzilay, R. (2015).\nLanguage understanding for text-based\ngames using deep reinforcement learning. In Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nNarasimhan, K., Yala, A., and Barzilay, R. (2016). Improving information extraction by acquiring\nexternal evidence with reinforcement learning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nNedi´c, A. and Bertsekas, D. P. (2003). Least squares policy evaluation algorithms with linear func-\ntion approximation. Discrete Event Dynamic Systems: Theory and Applications, 13:79–110.\nNeuneier, R. (1997). Enhancing q-learning for optimal asset allocation. In the Annual Conference\non Neural Information Processing Systems (NIPS).\n73\nNeyshabur, B., Tomioka, R., Salakhutdinov, R., and Srebro, N. (2017). Geometry of Optimization\nand Implicit Regularization in Deep Learning. ArXiv e-prints.\nNg, A. and Russell, S. (2000). Algorithms for inverse reinforcement learning. In the International\nConference on Machine Learning (ICML).\nNogueira, R. and Cho, K. (2016). End-to-End Goal-Driven Web Navigation. ArXiv e-prints.\nNogueira, R. and Cho, K. (2017). Task-Oriented Query Reformulation with Reinforcement Learn-\ning. ArXiv e-prints.\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2017). PGQ: Combining policy\ngradient and Q-learning. In the International Conference on Learning Representations (ICLR).\nO’Donovan, P., Leahy, K., Bruton, K., and O’Sullivan, D. T. J. (2015). Big data in manufacturing:\na systematic mapping study. Journal of Big Data, 2(20).\nOh, J., Chockalingam, V., Singh, S., and Lee, H. (2016). Control of memory, active perception, and\naction in minecraft. In the International Conference on Machine Learning (ICML).\nOh, J., Guo, X., Lee, H., Lewis, R., and Singh, S. (2015). Action-conditional video prediction\nusing deep networks in atari games. In the Annual Conference on Neural Information Processing\nSystems (NIPS).\nOh, J., Singh, S., and Lee, H. (2017). Value prediction network. In the Annual Conference on Neural\nInformation Processing Systems (NIPS).\nOmidshaﬁei, S., Pazis, J., Amato, C., How, J. P., and Vian, J. (2017). Deep decentralized multi-task\nmulti-agent reinforcement learning under partial observability. In the International Conference\non Machine Learning (ICML).\nOnta˜n´on, S., Synnaeve, G., Uriarte, A., Richoux, F., Churchill, D., and Preuss, M. (2013).\nA\nsurvey of real-time strategy game ai research and competition in starcraft. IEEE Transactions on\nComputational Intelligence and AI in Games, 5(4):293–311.\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. (2015). Is object localization for free? – weakly-\nsupervised learning with convolutional neural networks. In the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nOsband, I., Blundell, C., Pritzel, A., and Roy, B. V. (2016). Deep exploration via bootstrapped DQN.\nIn the Annual Conference on Neural Information Processing Systems (NIPS).\nOstrovski, G., Bellemare, M. G., van den Oord, A., and Munos, R. (2017). Count-Based Exploration\nwith Neural Density Models. ArXiv e-prints.\nPan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge\nand Data Engineering, 22(10):1345 – 1359.\nPapernot, N., Abadi, M., Erlingsson, ´U., Goodfellow, I., and Talwar, K. (2017). Semi-supervised\nknowledge transfer for deep learning from private training data. In the International Conference\non Learning Representations (ICLR).\nPapernot, N., Goodfellow, I., Sheatsley, R., Feinman, R., and McDaniel, P. (2016). cleverhans\nv1.0.0: an adversarial machine learning library. ArXiv e-prints.\nParisotto, E., Ba, J. L., and Salakhutdinov, R. (2016). Actor-mimic: Deep multitask and transfer\nreinforcement learning. In the International Conference on Learning Representations (ICLR).\nParisotto, E., rahman Mohamed, A., Singh, R., Li, L., Zhou, D., and Kohli, P. (2017). Neuro-\nsymbolic program synthesis.\nIn the International Conference on Learning Representations\n(ICLR).\nPasunuru, R. and Bansal, M. (2017). Reinforced video captioning with entailment rewards. In\nConference on Empirical Methods in Natural Language Processing (EMNLP).\n74\nPaulus, R., Xiong, C., and Socher, R. (2017). A Deep Reinforced Model for Abstractive Summa-\nrization. ArXiv e-prints.\nPearl, J. (2018). Theoretical Impediments to Machine Learning With Seven Sparks from the Causal\nRevolution. ArXiv e-prints.\nPei, K., Cao, Y., Yang, J., and Jana, S. (2017). DeepXplore: Automated Whitebox Testing of Deep\nLearning Systems. ArXiv e-prints.\nPeng, B., Li, X., Li, L., Gao, J., Celikyilmaz, A., Lee, S., and Wong, K.-F. (2017a). Composite\ntask-completion dialogue system via hierarchical deep reinforcement learning. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nPeng, P., Yuan, Q., Wen, Y., Yang, Y., Tang, Z., Long, H., and Wang, J. (2017b).\nMultiagent\nBidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games. ArXiv e-prints.\nP´erez-D’Arpino, C. and Shah, J. A. (2017). C-learn: Learning geometric constraints from demon-\nstrations for multi-step manipulation in shared autonomy. In IEEE International Conference on\nRobotics and Automation (ICRA).\nPerolat, J., Leibo, J. Z., Zambaldi, V., Beattie, C., Tuyls, K., and Graepel, T. (2017). A multi-agent\nreinforcement learning model of common-pool resource appropriation. In the Annual Conference\non Neural Information Processing Systems (NIPS).\nPeters, J. and Neumann, G. (2015). Policy search: Methods and applications. ICML 2015 Tutorial.\nPetroski Such, F., Madhavan, V., Conti, E., Lehman, J., Stanley, K. O., and Clune, J. (2017). Deep\nNeuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural\nNetworks for Reinforcement Learning. ArXiv e-prints.\nPfau, D. and Vinyals, O. (2016). Connecting Generative Adversarial Networks and Actor-Critic\nMethods. ArXiv e-prints.\nPhua, C., Lee, V., Smith, K., and Gayler, R. (2010). A Comprehensive Survey of Data Mining-based\nFraud Detection Research. ArXiv e-prints.\nPopov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M., Lampe, T., Tassa, Y.,\nErez, T., and Riedmiller, M. (2017). Data-efﬁcient Deep Reinforcement Learning for Dexterous\nManipulation. ArXiv e-prints.\nPowell, W. B. (2011). Approximate Dynamic Programming: Solving the curses of dimensionality\n(2nd Edition). John Wiley and Sons.\nPrashanth, L., Jie, C., Fu, M., Marcus, S., and Szepes´ari, C. (2016). Cumulative prospect theory\nmeets reinforcement learning: Prediction and control. In the International Conference on Machine\nLearning (ICML).\nPreuveneers, D. and Ilie-Zudor, E. (2017). The intelligent industry of the future: A survey on emerg-\ning trends, research challenges and opportunities in industry 4.0. Journal of Ambient Intelligence\nand Smart Environments, 9(3):287–298.\nPritzel, A., Uria, B., Srinivasan, S., Puigdom`enech, A., Vinyals, O., Hassabis, D., Wierstra, D., and\nBlundell, C. (2017). Neural Episodic Control. ArXiv e-prints.\nProvost, F. and Fawcett, T. (2013). Data Science for Business. O’Reilly Media.\nPuterman, M. L. (2005). Markov decision processes : discrete stochastic dynamic programming.\nWiley-Interscience.\nRadford, A., Jozefowicz, R., and Sutskever, I. (2017). Learning to Generate Reviews and Discover-\ning Sentiment. ArXiv e-prints.\nRaghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-Dickstein, J. (2016). Survey of Expres-\nsivity in Deep Neural Networks. ArXiv e-prints.\n75\nRahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines. In the Annual\nConference on Neural Information Processing Systems (NIPS).\nRajendran, J., Lakshminarayanan, A., Khapra, M. M., P, P., and Ravindran, B. (2017). Attend, adapt\nand transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same\ndomain. the International Conference on Learning Representations (ICLR).\nRanzato, M., Chopra, S., Auli, M., and Zaremba, W. (2016). Sequence level training with recurrent\nneural networks. In the International Conference on Learning Representations (ICLR).\nRao, Y., Lu, J., and Zhou, J. (2017). Attention-aware deep reinforcement learning for video face\nrecognition. In the IEEE International Conference on Computer Vision (ICCV).\nRavi, S. and Larochelle, H. (2017). Optimization as a model for few-shot learning. In the Interna-\ntional Conference on Learning Representations (ICLR).\nReed, S. and de Freitas, N. (2016). Neural programmer-interpreters. In the International Conference\non Learning Representations (ICLR).\nRen, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN: Towards real-time object detection\nwith region proposal networks. In the Annual Conference on Neural Information Processing\nSystems (NIPS).\nRen, Z., Wang, X., Zhang, N., Lv, X., and Li, L.-J. (2017). Deep reinforcement learning-based\nimage captioning with embedding reward.\nIn the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR).\nRennie, S. J., Marcheret, E., Mroueh, Y., Ross, J., and Goel, V. (2017). Self-critical sequence training\nfor image captioning.\nIn the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR).\nRhinehart, N. and Kitani, K. M. (2017). First-person activity forecasting with online inverse rein-\nforcement learning. In the IEEE International Conference on Computer Vision (ICCV).\nRibeiro, M. T., Singh, S., and Guestrin, C. (2016). ”why should i trust you?” explaining the pre-\ndictions of any classiﬁer. In ACM International Conference on Knowledge Discovery and Data\nMining (SIGKDD).\nRiedmiller, M. (2005). Neural ﬁtted Q iteration - ﬁrst experiences with a data efﬁcient neural rein-\nforcement learning method. In European Conference on Machine Learning (ECML).\nRockt¨aschel, T. and Riedel, S. (2017). End-to-end Differentiable Proving. ArXiv e-prints.\nRoijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. (2013). A survey of multi-objective\nsequential decision-making. Journal of Artiﬁcial Intelligence Research, 48:67–113.\nRuder, S. (2017). An Overview of Multi-Task Learning in Deep Neural Networks. ArXiv e-prints.\nRuelens, F., Claessens, B. J., Vandael, S., Schutter, B. D., Babuˇska, R., and Belmans, R. (2016). Res-\nidential demand response of thermostatically controlled loads using batch reinforcement learning.\nIEEE Transactions on Smart Grid, PP(99):1–11.\nRussell, S. and Norvig, P. (2009). Artiﬁcial Intelligence: A Modern Approach (3rd edition). Pearson.\nSabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between capsules. In the Annual\nConference on Neural Information Processing Systems (NIPS).\nSalakhutdinov, R. (2016).\nFoundations of unsupervised deep learning, a talk at Deep Learn-\ning School, https://www.bayareadlschool.org.\nhttps://www.youtube.com/watch?v=\nrK6bchqeaN8.\nSalimans, T., Ho, J., Chen, X., and Sutskever, I. (2017). Evolution Strategies as a Scalable Alterna-\ntive to Reinforcement Learning. ArXiv e-prints.\n76\nSantoro, A., Raposo, D., Barrett, D. G. T., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap,\nT. (2017). A simple neural network module for relational reasoning. ArXiv e-prints.\nSaon, G., Sercu, T., Rennie, S., and Kuo, H.-K. J. (2016). The IBM 2016 English Conversational\nTelephone Speech Recognition System. In Annual Meeting of the International Speech Commu-\nnication Association (INTERSPEECH).\nSaria, S. (2014). A $3 trillion challenge to computational scientists: Transforming healthcare deliv-\nery. IEEE Intelligent Systems, 29(4):82–87.\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators.\nIn the International Conference on Machine Learning (ICML).\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In the\nInternational Conference on Learning Representations (ICLR).\nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61:85–\n117.\nSchulman, J., Abbeel, P., and Chen, X. (2017). Equivalence Between Policy Gradients and Soft\nQ-Learning. ArXiv e-prints.\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). Trust region policy\noptimization. In the International Conference on Machine Learning (ICML).\nSchuurmans, D. and Zinkevich, M. (2016). Deep learning games. In the Annual Conference on\nNeural Information Processing Systems (NIPS).\nSegler, M. H. S., Preuss, M., and Waller, M. P. (2017). Learning to Plan Chemical Syntheses. ArXiv\ne-prints.\nSerban, I. V., Lowe, R., Charlin, L., and Pineau, J. (2015). A survey of available corpora for building\ndata-driven dialogue systems. arXiv e-prints, abs/1512.05742.\nSerban, I. V., Sankar, C., Germain, M., Zhang, S., Lin, Z., Subramanian, S., Kim, T., Pieper, M.,\nChandar, S., Ke, N. R., Mudumba, S., de Brebisson, A., Sotelo, J. M. R., Suhubdy, D., Michalski,\nV., Nguyen, A., Pineau, J., and Bengio, Y. (2017). A Deep Reinforcement Learning Chatbot.\nArXiv e-prints.\nShah, P., Hakkani-T¨ur, D., and Heck, L. (2016). Interactive reinforcement learning for task-oriented\ndialogue management. In NIPS 2016 Deep Learning for Action and Interaction Workshop.\nShalev-Shwartz, S., Shamir, O., and Shammah, S. (2017). Failures of gradient-based deep learning.\nIn the International Conference on Machine Learning (ICML).\nSharma, S., Lakshminarayanan, A. S., and Ravindran, B. (2017). Learning to repeat: Fine grained\naction repetition for deep reinforcement learning. In the International Conference on Learning\nRepresentations (ICLR).\nShe, L. and Chai, J. (2017). Interactive learning for acquisition of grounded verb semantics towards\nhuman-robot communication. In the Association for Computational Linguistics annual meeting\n(ACL).\nShen, Y., Huang, P.-S., Gao, J., and Chen, W. (2017). Reasonet: Learning to stop reading in machine\ncomprehension. In ACM International Conference on Knowledge Discovery and Data Mining\n(SIGKDD).\nShoham, Y. and Leyton-Brown, K. (2009). Multiagent Systems: Algorithmic, Game-Theoretic, and\nLogical Foundations. Cambridge University Press.\nShoham, Y., Powers, R., and Grenager, T. (2007). If multi-agent learning is the answer, what is the\nquestion? Artiﬁcial Intelligence, 171:365–377.\n77\nShortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau, J., and Murphy, S. A. (2011). In-\nforming sequential clinical decision-making through reinforcement learning: an empirical study.\nMachine Learning, 84:109–136.\nShrivastava, A., Pﬁster, T., Tuzel, O., Susskind, J., Wang, W., and Webb, R. (2017). Learning\nfrom simulated and unsupervised images through adversarial training. In the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR).\nShwartz-Ziv, R. and Tishby, N. (2017). Opening the Black Box of Deep Neural Networks via\nInformation. ArXiv e-prints.\nSilver, D. (2016). Deep reinforcement learning, a tutorial at ICML 2016. http://icml.cc/\n2016/tutorials/deep_rl_tutorial.pdf.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,\nAntonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016a). Mastering the game of go with\ndeep neural networks and tree search. Nature, 529(7587):484–489.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L.,\nKumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., and Hassabis, D. (2017). Mastering Chess\nand Shogi by Self-Play with a General Reinforcement Learning Algorithm. ArXiv e-prints.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic\npolicy gradient algorithms. In the International Conference on Machine Learning (ICML).\nSilver, D., Newnham, L., Barker, D., Weller, S., and McFall, J. (2013).\nConcurrent reinforce-\nment learning from customer interactions. In the International Conference on Machine Learning\n(ICML).\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,\nL., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel,\nT., and Hassabis, D. (2017). Mastering the game of go without human knowledge. Nature,\n550:354–359.\nSilver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert,\nD., Rabinowitz, N., Barreto, A., and Degris, T. (2016b). The predictron: End-to-end learning and\nplanning. In NIPS 2016 Deep Reinforcement Learning Workshop.\nSimeone, O. (2017). A Brief Introduction to Machine Learning for Engineers. ArXiv e-prints.\nSmith, L. N. (2017). Best Practices for Applying Deep Learning to Novel Applications. ArXiv\ne-prints.\nSmith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. (2017). Federated multi-task learning. In\nthe Annual Conference on Neural Information Processing Systems (NIPS).\nSnell, J., Swersky, K., and Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning.\nArXiv e-prints.\nSocher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011). Semi-supervised re-\ncursive autoencoders for predicting sentiment distributions. In Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng, A., and Potts, C. (2013). Recur-\nsive deep models for semantic compositionality over a sentiment tree- bank. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\nSong, Y. and Roth, D. (2017). Machine Learning with World Knowledge: The Position and Survey.\nArXiv e-prints.\nSpring, R. and Shrivastava, A. (2017). Scalable and sustainable deep learning via randomized hash-\ning. In ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD).\n78\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout:\nA simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning\nResearch, 15:1929–1958.\nStadie, B. C., Abbeel, P., and Sutskever, I. (2017). Third person imitation learning. In the Interna-\ntional Conference on Learning Representations (ICLR).\nStoica, I., Song, D., Popa, R. A., Patterson, D. A., Mahoney, M. W., Katz, R. H., Joseph, A. D.,\nJordan, M., Hellerstein, J. M., Gonzalez, J., Goldberg, K., Ghodsi, A., Culler, D. E., and Abbeel,\nP. (2017). A berkeley view of systems challenges for AI. Technical Report No. UCB/EECS-2017-\n159.\nStone, P., Brooks, R., Brynjolfsson, E., Calo, R., Etzioni, O., Hager, G., Hirschberg, J., Kalyanakr-\nishnan, S., Kamar, E., Kraus, S., Leyton-Brown, K., Parkes, D., Press, W., Saxenian, A., Shah,\nJ., Tambe, M., and Teller, A. (2016). Artiﬁcial Intelligence and Life in 2030 - One Hundred\nYear Study on Artiﬁcial Intelligence: Report of the 2015-2016 Study Panel. Stanford University,\nStanford, CA.\nStone, P. and Veloso, M. (2000). Multiagent systems: A survey from a machine learning perspective.\nAutonomous Robots, 8(3):345–383.\nStrub, F., de Vries, H., Mary, J., Piot, B., Courville, A., and Pietquin, O. (2017).\nEnd-to-end\noptimization of goal-driven and visually grounded dialogue systems. ArXiv e-prints.\nSu, P.-H., Gasic, M., Mrksic, N., Rojas-Barahona, L., Ultes, S., Vandyke, D., Wen, T.-H., and Young,\nS. (2016a). Continuously Learning Neural Dialogue Management. ArXiv e-prints.\nSu, P.-H., Gasˇi´c, M., Mrksˇi´c, N., Rojas-Barahona, L., Ultes, S., Vandyke, D., Wen, T.-H., and Young,\nS. (2016b). On-line active reward learning for policy optimisation in spoken dialogue systems. In\nthe Association for Computational Linguistics annual meeting (ACL).\nSukhbaatar, S., Szlam, A., and Fergus, R. (2016). Learning multiagent communication with back-\npropagation. In the Annual Conference on Neural Information Processing Systems (NIPS).\nSukhbaatar, S., Weston, J., and Fergus, R. (2015). End-to-end memory networks. In the Annual\nConference on Neural Information Processing Systems (NIPS).\nSupanˇciˇc, III, J. and Ramanan, D. (2017). Tracking as online decision-making: Learning a policy\nfrom streaming videos with reinforcement learning. In the IEEE International Conference on\nComputer Vision (ICCV).\nSurana, A., Sarkar, S., and Reddy, K. K. (2016). Guided deep reinforcement learning for additive\nmanufacturing control application. In NIPS 2016 Deep Reinforcement Learning Workshop.\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks.\nIn the Annual Conference on Neural Information Processing Systems (NIPS).\nSutton, R. (2016). Reinforcement learning for artiﬁcial intelligence, course slides. http://www.\nincompleteideas.net/sutton/609%20dropbox/.\nSutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning,\n3(1):9–44.\nSutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approxi-\nmating dynamic programming. In the International Conference on Machine Learning (ICML).\nSutton, R. S. (1992). Adapting bias by gradient descent: An incremental version of delta-bar-delta.\nIn the AAAI Conference on Artiﬁcial Intelligence (AAAI).\nSutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd Edition, in\npreparation). MIT Press.\n79\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesv´ari, C., and Wiewiora,\nE. (2009a). Fast gradient-descent methods for temporal-difference learning with linear function\napproximation. In the International Conference on Machine Learning (ICML).\nSutton, R. S., Mahmood, A. R., and White, M. (2016). An emphatic approach to the problem of\noff-policy temporal-difference learning. The Journal of Machine Learning Research, 17:1–29.\nSutton, R. S., McAllester, D., Singh, S., and Mansour, Y. (2000). Policy gradient methods for rein-\nforcement learning with function approximation. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, D. (2011).\nHorde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor\ninteraction, , proc. of 10th. In International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS).\nSutton, R. S., Precup, D., and Singh, S. (1999). Between mdps and semi-mdps: A framework for\ntemporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181–211.\nSutton, R. S., Szepesv´ari, C., and Maei, H. R. (2009b). A convergent O(n) algorithm for off-policy\ntemporal-difference learning with linear function approximation. In the Annual Conference on\nNeural Information Processing Systems (NIPS).\nSynnaeve, G., Nardelli, N., Auvolat, A., Chintala, S., Lacroix, T., Lin, Z., Richoux, F., and Usunier,\nN. (2016). TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games.\nArXiv e-prints.\nSze, V., Chen, Y.-H., Yang, T.-J., and Emer, J. (2017). Efﬁcient Processing of Deep Neural Net-\nworks: A Tutorial and Survey. ArXiv e-prints.\nSzepesv´ari, C. (2010). Algorithms for Reinforcement Learning. Morgan & Claypool.\nTamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In the\nAnnual Conference on Neural Information Processing Systems (NIPS).\nTang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y., Schulman, J., Turck, F. D.,\nand Abbeel, P. (2017). Exploration: A study of count-based exploration for deep reinforcement\nlearning. In the Annual Conference on Neural Information Processing Systems (NIPS).\nTanner, B. and White, A. (2009). RL-Glue : Language-independent software for reinforcement-\nlearning experiments. Journal of Machine Learning Research, 10:2133–2136.\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden, D., Abdolmaleki, A.,\nMerel, J., Lefrancq, A., Lillicrap, T., and Riedmiller, M. (2018). DeepMind Control Suite. ArXiv\ne-prints.\nTaylor, M. E. and Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey.\nJournal of Machine Learning Research, 10:1633–1685.\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level\nplay. Neural Computation, 6(2):215–219.\nTessler, C., Givony, S., Zahavy, T., Mankowitz, D. J., and Mannor, S. (2017). A deep hierarchical\napproach to lifelong learning in minecraft. In the AAAI Conference on Artiﬁcial Intelligence\n(AAAI).\nTheocharous, G., Thomas, P. S., and Ghavamzadeh, M. (2015). Personalized ad recommendation\nsystems for life-time value optimization with guarantees. In the International Joint Conference\non Artiﬁcial Intelligence (IJCAI).\nTian, Y., Gong, Q., Shang, W., Wu, Y., and Zitnick, L. (2017). ELF: An Extensive, Lightweight and\nFlexible Research Platform for Real-time Strategy Games. ArXiv e-prints.\nTram`er, F., Kurakin, A., Papernot, N., Boneh, D., and McDaniel, P. (2017). Ensemble Adversarial\nTraining: Attacks and Defenses. ArXiv e-prints.\n80\nTran, D., Hoffman, M. D., Saurous, R. A., Brevdo, E., Murphy, K., and Blei, D. M. (2017). Deep\nprobabilistic programming. In the International Conference on Learning Representations (ICLR).\nTrischler, A., Ye, Z., Yuan, X., and Suleman, K. (2016). Natural language comprehension with the\nepireader. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\nTsitsiklis, J. N. and Van Roy, B. (1997). An analysis of temporal-difference learning with function\napproximation. IEEE Transactions on Automatic Control, 42(5):674–690.\nTsitsiklis, J. N. and Van Roy, B. (2001). Regression methods for pricing complex American-style\noptions. IEEE Transactions on Neural Networks, 12(4):694–703.\nUsunier, N., Synnaeve, G., Lin, Z., and Chintala, S. (2017). Episodic exploration for deep de-\nterministic policies: An application to StarCraft micromanagement tasks. In the International\nConference on Learning Representations (ICLR).\nvan der Pol, E. and Oliehoek, F. A. (2017). Coordinated deep reinforcement learners for trafﬁc light\ncontrol. In NIPS’16 Workshop on Learning, Inference and Control of Multi-Agent Systems.\nvan Hasselt, H., Guez, A., , and Silver, D. (2016a). Deep reinforcement learning with double Q-\nlearning. In the AAAI Conference on Artiﬁcial Intelligence (AAAI).\nvan Hasselt, H., Guez, A., Hessel, M., Mnih, V., and Silver, D. (2016b). Learning values across\nmany orders of magnitude. In the Annual Conference on Neural Information Processing Systems\n(NIPS).\nvan Seijen, H., Fatemi, M., Romoff, J., Laroche, R., Barnes, T., and Tsang, J. (2017). Hybrid\nreward architecture for reinforcement learning. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polo-\nsukhin, I. (2017). Attention is all you need. In the Annual Conference on Neural Information\nProcessing Systems (NIPS).\nVenkatraman, A., Rhinehart, N., Sun, W., Pinto, L., Hebert, M., Boots, B., Kitani, K. M., and\nBagnell, J. A. (2017). Predictive-state decoders: Encoding the future into recurrent networks. In\nthe Annual Conference on Neural Information Processing Systems (NIPS).\nVeˇcer´ık, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Roth¨orl, T., Lampe,\nT., and Riedmiller, M. (2017). Leveraging demonstrations for deep reinforcement learning on\nrobotics problems with sparse rewards. In the Annual Conference on Neural Information Pro-\ncessing Systems (NIPS).\nVezhnevets, A. S., Mnih, V., Agapiou, J., Osindero, S., Graves, A., Vinyals, O., and Kavukcuoglu,\nK. (2016). Strategic attentive writer for learning macro-actions. In the Annual Conference on\nNeural Information Processing Systems (NIPS).\nVezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu,\nK. (2017). Feudal networks for hierarchical reinforcement learning. In the International Confer-\nence on Machine Learning (ICML).\nVinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. (2016). Matching net-\nworks for one shot learning. In the Annual Conference on Neural Information Processing Systems\n(NIPS).\nVinyals, O., Fortunato, M., and Jaitly, N. (2015). Pointer networks. In the Annual Conference on\nNeural Information Processing Systems (NIPS).\nWang, H. and Raj, B. (2017). On the Origin of Deep Learning. ArXiv e-prints.\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C.,\nKumaran, D., and Botvinick, M. (2016). Learning to reinforcement learn. ArXiv e-prints.\nWang, S. I., Liang, P., and Manning, C. D. (2016a). Learning language games through interaction.\nIn the Association for Computational Linguistics annual meeting (ACL).\n81\nWang, W., Yang, N., Wei, F., Chang, B., and Zhou, M. (2017a). Gated self-matching networks for\nreading comprehension and question answering. In the Association for Computational Linguistics\nannual meeting (ACL).\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., and de Freitas, N. (2017b).\nSample efﬁcient actor-critic with experience replay. In the International Conference on Learning\nRepresentations (ICLR).\nWang, Z., Merel, J., Reed, S., Wayne, G., de Freitas, N., and Heess, N. (2017). Robust Imitation of\nDiverse Behaviors. ArXiv e-prints.\nWang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de Freitas, N. (2016b). Du-\neling network architectures for deep reinforcement learning. In the International Conference on\nMachine Learning (ICML).\nWatkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8:279–292.\nWatter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. (2015). Embed to control: A\nlocally linear latent dynamics model for control from raw images. In the Annual Conference on\nNeural Information Processing Systems (NIPS).\nWatters, N., Tacchetti, A., Weber, T., Pascanu, R., Battaglia, P., and Zoran, D. (2017).\nVisual\ninteraction networks: Learning a physics simulator from video. In the Annual Conference on\nNeural Information Processing Systems (NIPS).\nWeber, T., Racani`ere, S., Reichert, D. P., Buesing, L., Guez, A., Jimenez Rezende, D., Puig-\ndom`enech Badia, A., Vinyals, O., Heess, N., Li, Y., Pascanu, R., Battaglia, P., Silver, D., and\nWierstra, D. (2017). Imagination-augmented agents for deep reinforcement learning. In the An-\nnual Conference on Neural Information Processing Systems (NIPS).\nWeiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. Journal of Big\nData, 3(9).\nWeiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., and Chen, Z. (2017). Sequence-to-Sequence Models\nCan Directly Transcribe Foreign Speech. ArXiv e-prints.\nWelleck, S., Mao, J., Cho, K., and Zhang, Z. (2017). Saliency-based sequential image attention with\nmultiset prediction. In the Annual Conference on Neural Information Processing Systems (NIPS).\nWen, T.-H., Gasic, M., Mrksic, N., Su, P.-H., Vandyke, D., and Young, S. (2015a). Semantically con-\nditioned LSTM-based natural language generation for spoken dialogue systems. In Conference\non Empirical Methods in Natural Language Processing (EMNLP).\nWen, T.-H., Vandyke, D., Mrksic, N., Gasic, M., Rojas-Barahona, L. M., Su, P.-H., Ultes, S., and\nYoung, S. (2017). A network-based end-to-end trainable task-oriented dialogue system. In Pro-\nceedings of the 15th Conference of the European Chapter of the Association for Computational\nLinguistics (EACL).\nWen, Z., O’Neill, D., and Maei, H. (2015b). Optimal demand response using device-based rein-\nforcement learning. IEEE Transactions on Smart Grid, 6(5):2312–2324.\nWeston, J., Chopra, S., and Bordes, A. (2015). Memory networks. In the International Conference\non Learning Representations (ICLR).\nWhite, A. and White, M. (2016). Investigating practical linear temporal difference learning. In the\nInternational Conference on Autonomous Agents & Multiagent Systems (AAMAS).\nWhye Teh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and\nPascanu, R. (2017). Distral: Robust multitask reinforcement learning. In the Annual Conference\non Neural Information Processing Systems (NIPS).\nWiering, M. and van Otterlo, M. (2012). Reinforcement Learning: State-of-the-Art (edited book).\nSpringer.\n82\nWilliams, J. D., Asadi, K., and Zweig, G. (2017). Hybrid code networks: practical and efﬁcient\nend-to-end dialog control with supervised and reinforcement learning. In the Association for\nComputational Linguistics annual meeting (ACL).\nWilliams, J. D. and Zweig, G. (2016).\nEnd-to-end LSTM-based dialog control optimized with\nsupervised and reinforcement learning. ArXiv e-prints.\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Machine Learning, 8(3):229–256.\nWilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017). The Marginal Value of\nAdaptive Gradient Methods in Machine Learning. ArXiv e-prints.\nWu, J., Lu, E., Kohli, P., Freeman, B., and Tenenbaum, J. (2017a). Learning to see physics via visual\nde-animation. In the Annual Conference on Neural Information Processing Systems (NIPS).\nWu, J., Tenenbaum, J. B., and Kohli, P. (2017b). Neural scene de-rendering. In the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR).\nWu, J., Yildirim, I., Lim, J. J., Freeman, B., and Tenenbaum, J. (2015). Galileo: Perceiving physical\nobject properties by integrating a physics engine with deep learning. In the Annual Conference\non Neural Information Processing Systems (NIPS).\nWu, L., Xia, Y., Zhao, L., Tian, F., Qin, T., Lai, J., and Liu, T.-Y. (2017c). Adversarial Neural\nMachine Translation. ArXiv e-prints.\nWu, Y., Mansimov, E., Liao, S., Grosse, R., and Ba, J. (2017). Scalable trust-region method for\ndeep reinforcement learning using kronecker-factored approximation. In the Annual Conference\non Neural Information Processing Systems (NIPS).\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao,\nQ., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y.,\nKudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa,\nJ., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. (2016). Google’s neural\nmachine translation system: Bridging the gap between human and machine translation. ArXiv\ne-prints.\nWu, Y. and Tian, Y. (2017). Training agent for ﬁrst-person shooter game with actor-critic curriculum\nlearning. In the International Conference on Learning Representations (ICLR).\nXiong, C., Zhong, V., and Socher, R. (2017a). Dynamic coattention networks for question answer-\ning. In the International Conference on Learning Representations (ICLR).\nXiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M., Stolcke, A., Yu, D., and Zweig, G. (2017b).\nThe microsoft 2016 conversational speech recognition system. In The IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP).\nXiong, W., Hoang, T., and Wang, W. Y. (2017c). Deeppath: A reinforcement learning method for\nknowledge graph reasoning. In Conference on Empirical Methods in Natural Language Process-\ning (EMNLP).\nXiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., and Stolcke, A. (2017). The Microsoft 2017\nConversational Speech Recognition System. ArXiv e-prints.\nXu, D., Nair, S., Zhu, Y., Gao, J., Garg, A., Fei-Fei, L., and Savarese, S. (2017). Neural Task\nProgramming: Learning to Generalize Across Hierarchical Tasks. ArXiv e-prints.\nXu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S., and Bengio,\nY. (2015). Show, attend and tell: Neural image caption generation with visual attention. In the\nInternational Conference on Machine Learning (ICML).\nXu, L. D., He, W., and Li, S. (2014). Internet of things in industries: A survey. IEEE Transactions\non Industrial Informatics, 10(4):2233–2243.\n83\nYahya, A., Li, A., Kalakrishnan, M., Chebotar, Y., and Levine, S. (2016). Collective robot reinforce-\nment learning with distributed asynchronous guided policy search. ArXiv e-prints.\nYang, B. and Mitchell, T. (2017). Leveraging knowledge bases in lstms for improving machine\nreading. In the Association for Computational Linguistics annual meeting (ACL).\nYang, X., Chen, Y.-N., Hakkani-Tur, D., Crook, P., Li, X., Gao, J., and Deng, L. (2016). End-to-End\nJoint Learning of Natural Language Understanding and Dialogue Manager. ArXiv e-prints.\nYang, Z., He, X., Gao, J., Deng, L., and Smola, A. (2015). Stacked Attention Networks for Image\nQuestion Answering. ArXiv e-prints.\nYang, Z., Hu, J., Salakhutdinov, R., and Cohen, W. W. (2017). Semi-supervised qa with generative\ndomain-adaptive nets. In the Association for Computational Linguistics annual meeting (ACL).\nYannakakis, G. N. and Togelius, J. (2018). Artiﬁcial Intelligence and Games. Springer.\nYao, H., Szepesvari, C., Sutton, R. S., Modayil, J., and Bhatnagar, S. (2014). Universal option\nmodels. In the Annual Conference on Neural Information Processing Systems (NIPS).\nYi, Z., Zhang, H., Tan, P., and Gong, M. (2017). Dualgan: Unsupervised dual learning for image-\nto-image translation. In the IEEE International Conference on Computer Vision (ICCV).\nYogatama, D., Blunsom, P., Dyer, C., Grefenstette, E., and Ling, W. (2017). Learning to compose\nwords into sentences with reinforcement learning. In the International Conference on Learning\nRepresentations (ICLR).\nYosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep\nneural networks? In the Annual Conference on Neural Information Processing Systems (NIPS).\nYoung, S., Gaˇsi´c, M., Thomson, B., and Williams, J. D. (2013). POMDP-based statistical spoken\ndialogue systems: a review. PROC IEEE, 101(5):1160–1179.\nYoung, T., Hazarika, D., Poria, S., and Cambria, E. (2017). Recent Trends in Deep Learning Based\nNatural Language Processing. ArXiv e-prints.\nYu, L., Zhang, W., Wang, J., and Yu, Y. (2017). Seqgan: Sequence generative adversarial nets with\npolicy gradient. In the AAAI Conference on Artiﬁcial Intelligence (AAAI).\nYu, Y.-L., Li, Y., Szepesv´ari, C., and Schuurmans, D. (2009). A general projection property for dis-\ntribution families. In the Annual Conference on Neural Information Processing Systems (NIPS).\nYun, S., Choi, J., Yoo, Y., Yun, K., and Young Choi, J. (2017). Action-decision networks for visual\ntracking with deep reinforcement learning. In the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR).\nZagoruyko, S. and Komodakis, N. (2017). Paying more attention to attention: Improving the per-\nformance of convolutional neural networks via attention transfer. In the International Conference\non Learning Representations (ICLR).\nZaremba, W. and Sutskever, I. (2015). Reinforcement Learning Neural Turing Machines - Revised.\nArXiv e-prints.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning\nrequires rethinking generalization. In the International Conference on Learning Representations\n(ICLR).\nZhang, H., Yu, H., and Xu, W. (2017a). Listen, Interact and Talk: Learning to Speak via Interaction.\nArXiv e-prints.\nZhang, J., Ding, Y., Shen, S., Cheng, Y., Sun, M., Luan, H., and Liu, Y. (2017b). THUMT: An Open\nSource Toolkit for Neural Machine Translation. ArXiv e-prints.\nZhang, L., Wang, S., and Liu, B. (2018). Deep Learning for Sentiment Analysis : A Survey. ArXiv\ne-prints.\n84\nZhang, Q. and Zhu, S.-C. (2018). Visual interpretability for deep learning: a survey. Frontiers of\nInformation Technology & Electronic Engineering, 19(1):27–39.\nZhang, X. and Lapata, M. (2017). Sentence simpliﬁcation with deep reinforcement learning. In\nConference on Empirical Methods in Natural Language Processing (EMNLP).\nZhang, Y., Mustaﬁzur Rahman, M., Braylan, A., Dang, B., Chang, H.-L., Kim, H., McNamara, Q.,\nAngert, A., Banner, E., Khetan, V., McDonnell, T., Thanh Nguyen, A., Xu, D., Wallace, B. C.,\nand Lease, M. (2016). Neural Information Retrieval: A Literature Review. ArXiv e-prints.\nZhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Yoshua Bengio, C. L., and Courville, A. (2017c).\nTowards End-to-End Speech Recognition with Deep Convolutional Neural Networks. ArXiv e-\nprints.\nZhao, T. and Eskenazi, M. (2016). Towards end-to-end learning for dialog state tracking and man-\nagement using deep reinforcement learning. In the Annual SIGdial Meeting on Discourse and\nDialogue (SIGDIAL).\nZhong, Z., Yan, J., and Liu, C.-L. (2017). Practical Network Blocks Design with Q-Learning. ArXiv\ne-prints.\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2015). Object detectors emerge in\ndeep scene CNNs. In the International Conference on Learning Representations (ICLR).\nZhou, H., Huang, M., Zhang, T., Zhu, X., and Liu, B. (2017). Emotional Chatting Machine: Emo-\ntional Conversation Generation with Internal and External Memory. ArXiv e-prints.\nZhou, Y. and Tuzel, O. (2017). VoxelNet: End-to-End Learning for Point Cloud Based 3D Object\nDetection. ArXiv e-prints.\nZhou, Z.-H. (2016). Machine Learning (in Chinese). Tsinghua University Press, Beijing, China.\nZhou, Z.-H. and Feng, J. (2017). Deep forest: Towards an alternative to deep neural networks. In\nthe International Joint Conference on Artiﬁcial Intelligence (IJCAI).\nZhu, J.-Y., Park, T., Isola, P., and Efros, A. A. (2017a). Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In the IEEE International Conference on Computer Vision\n(ICCV).\nZhu, X. and Goldberg, A. B. (2009). Introduction to semi-supervised learning. Morgan & Claypool.\nZhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Li, F.-F., and Farhadi, A. (2017b). Target-\ndriven visual navigation in indoor scenes using deep reinforcement learning. In IEEE Interna-\ntional Conference on Robotics and Automation (ICRA).\nZinkevich, M. (2017).\nRules of Machine Learning:\nBest Practices for ML Engineering.\nhttp://martin.zinkevich.org/rules of ml/rules of ml.pdf.\nZoph, B. and Le, Q. V. (2017). Neural architecture search with reinforcement learning. In the\nInternational Conference on Learning Representations (ICLR).\nZoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. (2017). Learning Transferable Architectures for\nScalable Image Recognition. ArXiv e-prints.\n85\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-01-25",
  "updated": "2018-11-26"
}