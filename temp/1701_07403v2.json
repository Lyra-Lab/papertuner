{
  "id": "http://arxiv.org/abs/1701.07403v2",
  "title": "Learning Light Transport the Reinforced Way",
  "authors": [
    "Ken Dahm",
    "Alexander Keller"
  ],
  "abstract": "We show that the equations of reinforcement learning and light transport\nsimulation are related integral equations. Based on this correspondence, a\nscheme to learn importance while sampling path space is derived. The new\napproach is demonstrated in a consistent light transport simulation algorithm\nthat uses reinforcement learning to progressively learn where light comes from.\nAs using this information for importance sampling includes information about\nvisibility, too, the number of light transport paths with zero contribution is\ndramatically reduced, resulting in much less noisy images within a fixed time\nbudget.",
  "text": "Learning Light Transport the Reinforced Way\nKen Dahm and Alexander Keller\nAbstract We show that the equations of reinforcement learning and light transport\nsimulation are related integral equations. Based on this correspondence, a scheme to\nlearn importance while sampling path space is derived. The new approach is demon-\nstrated in a consistent light transport simulation algorithm that uses reinforcement\nlearning to progressively learn where light comes from. As using this information for\nimportance sampling includes information about visibility, too, the number of light\ntransport paths with zero contribution is dramatically reduced, resulting in much less\nnoisy images within a ﬁxed time budget.\n1 Introduction\nOne application of light transport simulation is the computational synthesis of images\nthat cannot be distinguished from real photographs. In such simulation algorithms\n[25], light transport is modeled by a Fredholm integral equation of the second kind\nand pixel colors are determined by estimating functionals of the solution of the\nFredholm integral equation. The estimators are averages of contributions of sampled\nlight transport paths that connect light sources and camera sensors.\nCompared to reality, where photons and their trajectories are abundant, a computer\nmay only consider a tiny fraction of path space, which is one of the dominant reasons\nfor noisy images. It is therefore crucial to efﬁciently ﬁnd light transport paths that\nhave an important contribution to the image. While a lot of research in computer\ngraphics has been focussing on importance sampling [19, 4, 3, 1, 24], for long there\nhas not been a simple and efﬁcient online method that can substantially reduce the\nnumber of light transport paths with zero contribution [33].\nKen Dahm · Alexander Keller\nNVIDIA, Fasanenstr. 81, 10623 Berlin, Germany e-mail: ken.dahm@gmail.com, e-mail: keller.\nalexander@gmail.com\n1\narXiv:1701.07403v2  [cs.LG]  15 Aug 2017\n2\nKen Dahm and Alexander Keller\np ∼Le fs cosθ\nFig. 1 In the illustration, radiance is integrated by sampling proportional to the product of emitted\nradiance Le and the bidirectional scattering distribution function fs representing the physical surface\nproperties taking into account the fraction of radiance that is incident perpendicular to the surface,\nwhich is the cosine of the angle θ between the surface normal and the direction of incidence. As\nsuch importance sampling does not consider blockers, light transport paths with zero contributions\ncannot be avoided unless visibility is considered.\nThe majority of zero contributions are caused by unsuitable local importance\nsampling using only a factor instead of the complete integrand (see Fig. 1) or by\ntrying to connect vertices of light transport path segments that are occluded, for\nexample shooting shadow rays to light sources or connecting path segments starting\nboth from the light sources and the camera sensors. An example for this inefﬁciency\nhas been investigated early on in computer graphics [30, 31]: The visible part of the\nsynthetic scene shown in Fig. 4 is lit through a door. By closing the door more and\nmore the problem can be made arbitrarily more difﬁcult to solve.\nWe therefore propose a method that is based on reinforcement learning [28] and\nallows one to sample light transport paths that are much more likely to connect\nlights and sensors. Complementary to ﬁrst approaches of applying machine learning\nto image synthesis [33], in Sec. 2 we show that light transport and reinforcement\nlearning can be modeled by the same integral equation. As a consequence, importance\nin light transport can be learned using any light transport algorithm.\nDeriving a relationship between reinforcement learning and light transport simula-\ntion, we establish an automatic importance sampling scheme as introduced in Sec. 3.\nOur approach allows for controlling the memory footprint, for suitable representa-\ntions of importance does not require preprocessing, and can be applied during image\nsynthesis and/or across frames, because it is able to track distributions over time. A\nsecond parallel between temporal difference learning and next event estimation is\npointed out in Sec. 4.\nAs demonstrated in Sec. 5 and shown in Fig. 8, already a simple implementation\ncan dramatically improve light transport simulation. The efﬁciency of the scheme\nis based on two facts: Instead of shooting towards the light sources, we are guiding\nlight transport paths to where the light comes from, which effectively shortens path\nlength, and we learn importance from a smoothed approximation instead from higher\nvariance path space samples [19, 10, 23].\nPage: 2\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\nLearning Light Transport the Reinforced Way\n3\nAgent St\nEnvironment\nAt\nSt+1\nRt+1(At | St)\nFig. 2 The setting for reinforcement learning: At time t, an agent is in state St and takes an action\nAt, which after interaction with the environment brings him to the next state St+1 with a scalar\nreward Rt+1.\n2 Identifying Q-Learning and Light Transport\nThe setting of reinforcement learning [28] is depicted in Fig. 2: An agent takes an\naction thereby transitioning to the resulting next state and receiving a reward. In\norder to maximize the reward, the agent has to learn which action to choose in what\nstate. This process very much resembles how humans learn.\nQ-learning [39] is a model free reinforcement learning technique. Given a set of\nstates S and a set of actions A, it determines a function Q(s,a) that for any s ∈S\nvalues taking the action a ∈A. Thus given a state s, the action a with the highest\nvalue may be selected next and\nQ(s,a) = (1−α)·Q(s,a)+α ·\n\u0012\nr(s,a)+γ ·max\na′∈A Q(s′,a′)\n\u0013\n(1)\nmay be updated by a fraction of α ∈[0,1], where r(s,a) is the reward for taking the\naction resulting in a transition to a state s′. In addition, the maximum Q-value of\npossible actions in s′ is considered and discounted by a factor of γ ∈[0,1).\nInstead of taking into account only the best valued action,\nQ(s,a) = (1−α)·Q(s,a)+α ·\n \nr(s,a)+γ · ∑\na′∈A\nπ(s′,a′)Q(s′,a′)\n!\naverages all possible actions in s′ and weighs their values Q(s′,a′) by a transition\nkernel π(s′,a′), which is a strategy called expected SARSA [28, Sec.6.6]. This is\nespecially interesting, as later it will turn out that always selecting the ”best” action\ndoes not perform as well as considering all options (see Fig. 4). For a continuous\nspace A of actions, we then have\nQ(s,a) = (1−α)·Q(s,a)+α ·\n\u0012\nr(s,a)+γ ·\nZ\nA π(s′,a′)Q(s′,a′)da′\n\u0013\n.\n(2)\nOn the other hand, the radiance\nPage: 3\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\n4\nKen Dahm and Alexander Keller\nL(x,ω) = Le(x,ω)+\nZ\nS +(x) L(h(x,ωi),−ωi)fs(ωi,x,ω)cosθidωi\n(3)\nin a point x on a surface into direction ω is modeled by a Fredholm integral equation\nof the second kind. Le is the source radiance and the integral accounts for all radiance\nthat is incident over the hemisphere S +(x) aligned by the surface normal in x and\ntransported into direction ω. The hitpoint function h(x,ωi) traces a ray from x into\ndirection ωi and returns the ﬁrst surface point intersected. The radiance from this\npoint is attenuated by the bidirectional scattering distribution function fs, where the\ncosine term of the angle θi between surface normal and ωi accounts for only the\nfraction that is perpendicular to the surface.\nA comparison of Eqn. 2 for α = 1 and Eqn. 3 reveals structural similarities of\nthe formulation of reinforcement learning and the light transport integral equation,\nrespectively, which lend themselves to matching terms: Interpreting the state s as\na location x ∈R3 and an action a as tracing a ray from location x into direction ω\nresulting in the point y := h(x,ω) corresponding to the state s′, the reward term r(s,a)\ncan be linked to the emitted radiance Le(y,−ω) = Le(h(x,ω),−ω) as observed from\nx. Similarly, the integral operator can be applied to the value Q, yielding\nQ(x,ω) = Le(y,−ω)+\nZ\nS +(y) Q(y,ωi)fs(ωi,y,−ω)cosθidωi,\n(4)\nwhere we identiﬁed the discount factor γ multiplied by the policy π and the bidi-\nrectional scattering distribution function fs. Taking a look at the geometry and the\nphysical meaning of the terms, it becomes obvious that Q in fact must be the radiance\nLi(x,ω) incident in x from direction ω and in fact is described by a Fredholm integral\nequation of the second kind - like the light transport equation 3.\nFig. 3 Comparison of simple path tracing without (left) and with (right) reinforcement learning\nimportance sampling. The top row is using 8 paths per pixel, while 32 are used for the bottom row.\nThe challenge of the scene is the area light source on the left indirectly illuminating the right part of\nthe scene. The enlarged insets illustrate the reduction of noise level.\nPage: 4\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\nLearning Light Transport the Reinforced Way\n5\na\nb\nc\nd\ne\nf\nFig. 4 Comparison at 1024 paths per pixel (the room behind the door is shown in Fig. 5): a) A\nsimple path tracer with cosine importance sampling, b) the Kelemen variant of the Metropolis light\ntransport algorithm, c) scattering proportional to Q, while updating Q with the maximum as in\nEqn. 1 and d) scattering proportional to Q weighted by the bidirectional scattering distribution\nfunction and updating accordingly by Eqn. 5. The predominant reinforcement approach of always\ntaking the best next action is inferior to selecting the next action proportional to Q, i.e. considering\nall alternatives. A comparison to the Metropolis algorithm reveals much more uniform lighting,\nespecially much more uniform noise and the lack of the typical splotches. e) The average path\nlength of path tracing (above image diagonal) is about 215, while with reinforcement learning it\namounts to an average of 134. The average path length thus is reduced by 40% in this scene. f)\nDiscretized hemispheres to approximate Q are stored in points on the scene surfaces determined\nby samples of the Hammersley low discrepancy point set. Retrieving Q for a query point results\nin searching for the nearest sample of Q that has a similar normal to the one in the query point\n(see especially the teapot handles). The red points indicate where in the scene hemispheres to hold\nthe Qk are stored. The colored areas indicate their corresponding Voronoi cells. Storing the Qk in\nthis example requires about 2 MBytes of memory. Scene courtesy (cc) 2013 Miika Aittala, Samuli\nLaine, and Jaakko Lehtinen (https://mediatech.aalto.ﬁ/publications/graphics/GMLT/).\nPage: 5\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\n6\nKen Dahm and Alexander Keller\n3 Q-Learning while Path Tracing\nIn order to synthesize images, we need to compute functionals of the radiance\nequation 3, i.e. project the radiance onto the image plane. For the purpose of this\narticle, we start with a simple forward path tracer [25, 14]: From a virtual camera,\nrays are traced through the pixels of the screen. Upon their ﬁrst intersection with\nthe scene geometry, the light transport path is continued into a scattering direction\ndetermined according to the optical surface properties. Scattering and ray tracing are\nrepeated until a light source is hit. The contribution of this complete light transport\npath is added to the pixel pierced by the initial ray of this light transport path when\nstarted at the camera.\nIn this simple form, the algorithm exposes quite some variance as can be seen in\nthe images on the left in Fig. 3. This noise may be reduced by importance sampling.\nWe therefore progressively approximate Eqn. 4 using reinforcement learning: Once a\ndirection has been selected and a ray has been traced by the path tracer,\nQ′(x,ω) = (1−α)·Q(x,ω)\n(5)\n+α ·\n\u0012\nLe(y,−ω)+\nZ\nS +(y) Q(y,ωi)fs(ωi,y,−ω)cosθidωi\n\u0013\nis updated using a learning rate α. The probability density function resulting from\nnormalizing Q in turn is used for importance sampling a direction to continue the path.\nAs a consequence more and more light transport paths are sampled that contribute\nto the image. Computing a global solution to Q in a preprocess would not allow for\nfocussing computations on light transport paths that contribute to the image.\n3.1 Implementation\nOften, approximations to Q are tabulated for each pair of state and action. In computer\ngraphics, there are multiple choices to represent radiance and for the purpose of this\narticle, we chose the data structure as used for irradiance volumes [6] to approximate\nQ. Fig. 5 shows an exemplary visualization of such a discretization during rendering:\nFor selected points y in space, the hemisphere is stratiﬁed and one value Qk(y) is\nstored per sector, i.e. stratum k. Fig. 4f illustrates the placement of probe centers y,\nwhich results from mapping a two-dimensional low discrepancy sequence onto the\nscene surface.\nNow the integral\nZ\nS +(y) Q(y,ωi)fs(ωi,y,−ω)cosθidωi ≈2π\nn\nn−1\n∑\nk=0\nQk(y)fs(ωk,y,−ω)cosθk\nin Eqn. 5 can be estimated by using each one uniform random direction ωk in each\nstratum k, where θk is the angle between the surface normal in y and ωk.\nPage: 6\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\nLearning Light Transport the Reinforced Way\n7\nFig. 5 The image shows parts of an example discretization of Q by a grid, where hemispheres are\nuniformly distributed across the ground plane. The false colors indicate magnitude, where small\nvalues are green and large values are red. The large values on each hemisphere point towards the\npart of the scene, where the light is coming from. For example, under the big area light source on\nthe left, most radiance is incident as reﬂected radiance from the wall opposite to the light source.\nThe method has been implemented in an importance driven forward path tracer\nas shown in Alg. 1: Only two routines for updating Q and selecting a scattering\ndirection proportional to Q need to be added. Normalizing the Q in a point y then\nresults in a probability density that is used for importance sampling during scattering\nby inverting the cumulative distribution function. In order to guarantee ergodicity,\nmeaning that every light transport path remains possible, all Q(y) are initialized to\nbe positive, for example by a uniform probability density or proportional to a factor\nof the integrand (see Fig. 1). When building the cumulative distribution functions\nin parallel every accumulated frame, values below a small positive threshold are\nreplaced by the threshold.\nThe parameters exposed by our implementation are the resolution of the discretiza-\ntion and the learning rate α.\n3.2 Consistency\nIt is desirable to craft consistent rendering algorithms [14], because then all renderer\nintroduced artifacts, like for example noise, are guaranteed to vanish over time. This\nrequires the Qk(y) to converge, which may be accomplished by a vanishing learning\nrate α.\nIn reinforcement learning [28], a typical approach is to count the number of visits\nto each pair of state s and action a and using\nPage: 7\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\n8\nKen Dahm and Alexander Keller\nAlgorithm 1: Augmenting a path tracer by reinforcement learning for impor-\ntance sampling requires only two additions: The importance Q needs to be\nupdated along the path and scattering directions are sampled proportional to Q\nas learned so far.\nFunction pathTrace(camera,scene)\nthroughput ←1\nray ←setupPrimaryRay(camera)\nfor i ←0 to ∞do\n(y,n) ←intersect(scene, ray)// y := h(x,ω)\n// addition 1: update Q\nif i > 0 then\nQ′(x,ω) =\n(1−α)Q(x,ω)+α\n\u0010\nLe(y,−ω)+\nR\nS 2+(y) fs(ωi,y,−ω)cosθiQ(y,ωi)dωi\n\u0011\nif isEnvironment(y) then\nreturn throughput· getRadianceFromEnvironment(ray,y)\nelse if isAreaLight(y)\nreturn throughput· getRadianceFromAreaLight(ray,y)\n// addition 2: scatter proportional to Q\n(ω, pω, fs) ←sampleScatteringDirectionProportionalToQ(y)\nthroughput ←throughput · fs ·cos(n,ω) / pω\nray ←(y,ω)\nα(s,a) =\n1\n1+visits(s,a).\nThe method resembles the one used to make progressive photon mapping consistent\n[7], where consistency has been achieved by decreasing the search radius around a\nquery point every time a photon hits sufﬁciently close. Similarly, the learning rate\nmay also depend on the total number of visits to a state s alone, or even may be\nchosen to vanish independently of state and action. Again, such approaches have\nbeen explored in consistent photon mapping [15].\nWhile the Qk(y) converge, they do not necessarily converge to the incident ra-\ndiance in Eqn. 4. First, as they are projections onto a basis, the Qk(y) at best only\nare an approximation of Q in realistic settings. Second, as the coefﬁcients Qk(y) are\nlearned during path tracing, i.e. image synthesis, and used for importance sampling,\nit may well happen that they are not updated everywhere at the same rate. Neverthe-\nless, since all operators are linear, the number of visits will be proportional to the\nnumber of light transport paths [15] and consequently as long as Qk(y) > 0 whenever\nLi(y,ωi) > 0 all Qk(y) will be updated eventually.\nPage: 8\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\nLearning Light Transport the Reinforced Way\n9\n3.3 Learning while Light Tracing\nFor guiding light transport paths from the light sources towards the camera, the\ntransported weight W of a measurement (see [29]), i.e. the characteristic function of\nthe image plane, has to be learned instead of the incident radiance Q. As W is the\nadjoint of Q, the same data structures may be used for its storage. Learning both Q\nand W allows one to implement bidirectional path tracing [29] with reinforcement\nlearning for importance sampling to guide both light and camera path segments\nincluding visibility information for the ﬁrst time. Note that guiding light transport\npaths this way may reach efﬁciency levels that even can make bidirectional path\ntracing and multiple importance sampling redundant [33] in many common cases.\nFig. 6 Two split-image comparisons of uniformly selecting area light sources and selection using\ntemporal difference learning, both at 16 paths per pixel. The scene on the left has 5000 area light\nsources, whereas the scene on the right has about 15000 (San Miguel scene courtesy Guillermo M.\nLeal Llaguno (http://www.evvisual.com/)).\n4 Temporal Difference Learning and Next Event Estimation\nBesides the known shortcomings of (bidirectional) path tracing [18, Sec.2.4 Problem\nof insufﬁcient techniques], the efﬁciency may be restricted by the approximation\nquality of Q: For example, the smaller the light sources, the ﬁner the required\nresolution of Q to reliably guide rays to hit a light source. This is where next event\nestimation may help [32, 16, 5].\nAlready in [38] the contribution of light sources has been “learned”: A probability\nper light source has been determined by the number of successful shadow rays divided\nby the total number of shadow rays shot. This idea has been reﬁned subsequently\n[17, 37, 2, 36].\nFor reinforcement learning, the state space may be chosen as a regular grid over\nthe scene, where in each grid cell c for each light source l a value Vc,l is stored that is\ninitialized with zero. Whenever a sample on a light source l is visible to a point x to\nbe illuminated in the cell c upon next event estimation, its value\nPage: 9\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\n10\nKen Dahm and Alexander Keller\nV ′\nc,l = (1−α)Vc,l +α ·∥Cl(x)∥∞\n(6)\nis updated using the norm of the contributionCl(x). Building a cumulative distribution\nfunction from all values Vc,l within a cell c, light may be selected by importance\nsampling. Fig. 6 shows the efﬁciency gain of this reinforcement learning method\nover uniform light source selection for 16 paths per pixel.\nIt is interesting to see that this is another relation to reinforcement learning: While\nthe Q-learning equation 5 takes into account the values of the next, non-terminal\nstate, the next state in event estimation is always a terminal state and Q-learning\ncoincides with plain temporal difference learning [27] as in equation 6.\n4.1 Learning Virtual Point Light Sources\nThe vertices generated by tracing photon trajectories (see Sec. 3.3) can be considered\na photon map [11] and may be used in the same way. Furthermore, they may be used\nas a set of virtual point light sources for example the instant radiosity [13] algorithm.\nContinuously updating and learning the measurement contribution functionW [29]\nacross frames and using the same seed for the pseudo- or quasi-random sequences\nallows for generating virtual point light sources that expose a certain coherency\nover time, which reduces temporal artifacts when rendering animations with global\nillumination.\n4.2 Learning Environment Lighting\nRendering sun and sky is usually done by distributing samples proportional to the\nbrightness of pixels in the environment texture. More samples should end up in\nbrighter regions, which is achieved by constructing and sampling from a cumulative\ndistribution function, for example using the alias method [35]. Furthermore, the\nsun may be separated from the sky and simulated separately. The efﬁciency of\nsuch importance sampling is highly dependent on occlusion, i.e. what part of the\nenvironment can be seen from the point to be shaded (see Fig. 1).\nSimilar to Sec. 3.1 and in order to consider the actual contribution including\nocclusion, an action space is deﬁned by partitioning the environment map into tiles\nand learning the importance per tile. Fig. 7 shows the improvement for an example\nsetting.\nPage: 10\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\nLearning Light Transport the Reinforced Way\n11\nFig. 7 Sun and sky illumination at 32 paths per pixel. Top: simple importance sampling consid-\nering only the environment map as a light source. Bottom: Importance sampling with reinforce-\nment learned importance. The enlargements on the right illustrate the improved noise reduction.\nScene courtesy Frank Meinl, Crytek (http://graphics.cs.williams.edu/data/meshes/crytek-sponza-\ncopyright.html).\n5 Results and Discussion\nFig. 4 compares the new reinforcement learning algorithm to common algorithms:\nFor the same budget of light transport paths, the superiority over path tracing with\nimportance sampling according to the reﬂection properties is obvious. A comparison\nwith the Metropolis algorithm for importance sampling [31, 12] reveals much more\nuniform noise lacking the typical splotchy structure inherent with the local space\nexploration of Metropolis samplers. Note, however, that the new reinforcement\nlearning importance sampling scheme could as well be combined with Metropolis\nsampling. Finally, updating Q by Eqn. 1, i.e. the “best possible action” strategy is\ninferior to using the weighted average of all possible next actions according to Eqn. 5.\nIn light transport simulation this is not surprising, as the deviation of the integrand\nfrom its estimated maximum very often is much larger than from a piecewise constant\napproximation.\nThe big gain in quality is due to the dramatic reduction of zero contribution\nlight transport paths (see Fig. 8), even under complex lighting. In Figs. 4a-d, the\nPage: 11\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\n12\nKen Dahm and Alexander Keller\n0\n200\n400\n600\n800\n1000\naccumulated frames\n0\n20000\n40000\n60000\n80000\n100000\n120000\n140000\n160000\nvalid paths per frame\n43.49x\nRL-based IS\nBRDF-based IS\nFig. 8 Using reinforcement learning (RL), the number of paths actually connecting to a light\nsource is dramatically improved over classic importance sampling (IS) using only the bidirectional\nscattering distribution function for importance sampling. As a result, more non-zero contributions\nare accumulated for the same number of paths, see also Fig. 4.\nsame number of paths has been used. In each iteration, for path tracing with and\nwithout reinforcement learning one path has been started per pixel, while for the\nMetropolis variant the number of Markov chains equals the number of pixels of the\nimage. Rendering the image at 1280x720 pixels, each iteration takes 41ms for path\ntracing, 49ms for Metropolis light transport [31, 12], and 51ms for the algorithm\nwith reinforcement learned importance sampling. Hence the 20% overhead is well\npaid off by the level of noise reduction.\nShooting towards where the radiance comes from naturally shortens the average\npath length as can be seen in Fig. 4e. Based on the approach to guide light paths\nusing a pre-trained Gaussian mixture model [33] to represent probabilities, in [34]\nin addition the density of light transport paths is controlled across the scene using\nsplitting and Russian roulette. These ideas have the potential to further improve the\nefﬁciency of our approach.\nWhile the memory requirements for storing our data structure for Q are small, the\ndata structure is not adaptive. An alternative is an adaptive hierarchical approximation\nto Q as used in [19, 23]. Yet, another variant would be learning parameters for lobes\nto guide light transport paths [1]. In principle any data structure that has been used in\ngraphics to approximate irradiance or radiance is a candidate. Which data structure\nPage: 12\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\nLearning Light Transport the Reinforced Way\n13\nand what parameters are best, may depend on the scene to be rendered. For example,\nusing discretized hemispheres limits the resolution with respect to solid angle. If\nthe resolution is chosen too ﬁne, learning is slow, if the resolution is to coarse,\nconvergence is slow.\nGiven that Q asymptotically approximates the incident radiance Li, it is worthwhile\nto investigate how it can be used for the separation of the main part as explored in\n[19, 24] to further speed up light transport simulation or even as an alternative to\nimportance sampling.\nBeyond what we explore, path guiding has been extended to consider product im-\nportance sampling [9] and reinforcement learning [28] offers more policy evaluation\nstrategies to consider.\n6 Conclusion\nGuiding light transport paths has been explored in [19, 10, 4, 24, 1, 33, 23]. However,\nkey to our approach is that by using a representation of Q in Eqn. 5 instead of solving\nthe equation by recursion, i.e. a Neumann series, Q may be learned much faster and\nin fact during sampling light transport paths without any preprocess. This results in a\nnew algorithm to increase the efﬁciency of path tracing by approximating importance\nusing reinforcement learning during image synthesis. Identifying Q-learning and\nlight transport, heuristics have been replaced by physically based functions, and the\nonly parameters that the user may control are the learning rate and the discretization\nof Q.\nThe combination of reinforcement learning and deep neural networks [22, 8, 20,\n21] is an obvious avenue of future research: Representing the radiance on hemispheres\nalready has been successfully explored [26] and the interesting question is how well\nQ can be represented by neural networks.\nAcknowledgements The authors would like to thank Jaroslav Kˇriv´anek, Tero Karras, Toshiya\nHachisuka, and Adrien Gruson for profound discussions and comments.\nReferences\n1. Bashford-Rogers, T., Debattista, K., Chalmers, A.: A signiﬁcance cache for accelerating global\nillumination. Computer Graphics Forum 31(6), 1837–1851 (2012) 1, 5, 6\n2. Benthin, C., Wald, I., Slusallek, P.: A scalable approach to interactive global illumination.\nComputer Graphics Forum (Proc. Eurographics 2003) 22(3), 621–629 (2003) 4\n3. Cline, D., Adams, D., Egbert, P.: Table-driven adaptive importance sampling. Computer\nGraphics Forum 27(4), 1115–1123 (2008) 1\n4. Dutr´e, P., Willems, Y.: Potential-driven Monte Carlo particle tracing for diffuse environments\nwith adaptive probability functions. In: Rendering Techniques 1995 (Proc. 6th Eurographics\nWorkshop on Rendering), pp. 306–315. Springer (1995) 1, 6\nPage: 13\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\n14\nKen Dahm and Alexander Keller\n5. Estevez, C., Kulla, C.: Importance sampling of many lights with adaptive tree splitting. In:\nACM SIGGRAPH 2017 Talks, SIGGRAPH ’17, pp. 33:1–33:2. ACM (2017) 4\n6. Greger, G., Shirley, P., Hubbard, P., Greenberg, D.: The irradiance volume. IEEE Computer\nGraphics and Applications 18(2), 32–43 (1998) 3.1\n7. Hachisuka, T., Ogaki, S., Jensen, H.: Progressive photon mapping. ACM Transactions on\nGraphics 27(5), 130:1–130:8 (2008) 3.2\n8. van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double Q-learning.\nCoRR abs/1509.06461 (2015). URL http://arxiv.org/abs/1509.06461 6\n9. Herholz, S., Elek, O., Vorba, J., Lensch, H., Kˇriv´anek, J.: Product importance sampling for\nlight transport path guiding. In: Proceedings of the Eurographics Symposium on Rendering,\nEGSR ’16, pp. 67–77. Eurographics Association (2016). DOI 10.1111/cgf.12950. URL\nhttps://doi.org/10.1111/cgf.12950 5\n10. Jensen, H.: Importance driven path tracing using the photon map. In: P. Hanrahan, W. Purgath-\nofer (eds.) Rendering Techniques 1995 (Proc. 6th Eurographics Workshop on Rendering), pp.\n326–335. Springer (1995) 1, 6\n11. Jensen, H.: Realistic Image Synthesis Using Photon Mapping. AK Peters (2001) 4.1\n12. Kelemen, C., Szirmay-Kalos, L., Antal, G., Csonka, F.: A simple and robust mutation strategy\nfor the Metropolis light transport algorithm. Computer Graphics Forum 21(3), 531–540 (2002)\n5\n13. Keller, A.: Instant radiosity. In: SIGGRAPH ’97: Proceedings of the 24th annual conference\non Computer graphics and interactive techniques, pp. 49–56 (1997) 4.1\n14. Keller, A.: Quasi-Monte Carlo image synthesis in a nutshell. In: J. Dick, F. Kuo, G. Peters,\nI. Sloan (eds.) Monte Carlo and Quasi-Monte Carlo Methods 2012, pp. 203–238. Springer\n(2013) 3, 3.2\n15. Keller, A., Binder, N.: Deterministic consistent density estimation for light transport simulation.\nIn: J. Dick, F. Kuo, G. Peters, I. Sloan (eds.) Monte Carlo and Quasi-Monte Carlo Methods\n2012, pp. 467–480. Springer (2013) 3.2\n16. Keller, A., W¨achter, C., Raab, M., Seibert, D., Antwerpen, D., Kornd¨orfer, J., Kettner, L.: The\nIray light transport simulation and rendering system. CoRR abs/1705.01263 (2017). URL\nhttp://arxiv.org/abs/1705.01263 4\n17. Keller, A., Wald, I.: Efﬁcient importance sampling techniques for the photon map. In: Proc.\nVISION, MODELING, AND VISUALIZATION, pp. 271–279. IOS Press (2000) 4\n18. Kollig, T., Keller, A.: Efﬁcient bidirectional path tracing by randomized quasi-Monte Carlo\nintegration. In: H. Niederreiter, K. Fang, F. Hickernell (eds.) Monte Carlo and Quasi-Monte\nCarlo Methods 2000, pp. 290–305. Springer (2002) 4\n19. Lafortune, E., Willems, Y.: A 5D tree to reduce the variance of Monte Carlo ray tracing.\nIn: P. Hanrahan, W. Purgathofer (eds.) Rendering Techniques 1995 (Proc. 6th Eurographics\nWorkshop on Rendering), pp. 11–20. Springer (1995) 1, 1, 5, 6\n20. Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D.:\nContinuous control with deep reinforcement learning. CoRR abs/1509.02971 (2015). URL\nhttp://arxiv.org/abs/1509.02971 6\n21. Mnih, V., Badia, A., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu,\nK.: Asynchronous methods for deep reinforcement learning. CoRR abs/1602.01783 (2016).\nURL http://arxiv.org/abs/1602.01783 6\n22. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller,\nM.: Playing Atari with deep reinforcement learning. CoRR abs/1312.5602 (2013). URL\nhttp://arxiv.org/abs/1312.5602 6\n23. M¨uller, T., Gross, M., Nov´ak, J.: Practical path guiding for efﬁcient light-transport simulation.\nIn: Proceedings of the Eurographics Symposium on Rendering (2017) 1, 5, 6\n24. Pegoraro, V., Brownlee, C., Shirley, P., Parker, S.: Towards interactive global illumination\neffects via sequential Monte Carlo adaptation. In: Proceedings of the 3rd IEEE Symposium on\nInteractive Ray Tracing, pp. 107–114 (2008) 1, 5, 6\n25. Pharr, M., Jacob, W., Humphreys, G.: Physically Based Rendering - From Theory to Imple-\nmentation. Morgan Kaufmann, Third Edition (2016) 1, 3\nPage: 14\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\nLearning Light Transport the Reinforced Way\n15\n26. Satilmis, P., Bashford-Rogers, T., Chalmers, A., Debattista, K.: A machine learning driven sky\nmodel. IEEE Computer Graphics and Applications pp. 1–9 (2016) 6\n27. Sutton, R.: Learning to predict by the methods of temporal differences. Machine Learning 3(1),\n9–44 (1988) 4\n28. Sutton, R., Barto, A.: Introduction to Reinforcement Learning, 2nd edn. MIT Press, Cambridge,\nMA, USA (2017) 1, 2, 2, 3.2, 5\n29. Veach, E., Guibas, L.: Bidirectional estimators for light transport. In: Proc. 5th Eurographics\nWorkshop on Rendering, pp. 147 – 161. Darmstadt, Germany (1994) 3.3, 4.1\n30. Veach, E., Guibas, L.: Optimally combining sampling techniques for Monte Carlo rendering.\nIn: SIGGRAPH ’95 Proceedings of the 22nd annual conference on Computer graphics and\ninteractive techniques, pp. 419–428 (1995) 1\n31. Veach, E., Guibas, L.: Metropolis light transport. In: T. Whitted (ed.) Proc. SIGGRAPH 1997,\nAnnual Conference Series, pp. 65–76. ACM SIGGRAPH, Addison Wesley (1997) 1, 5\n32. V´evoda, P., Kˇriv´anek, J.: Adaptive direct illumination sampling. In: SIGGRAPH ASIA 2016\nPosters, pp. 43:1–43:2. ACM, New York, NY, USA (2016) 4\n33. Vorba, J., Karl´ık, O., ˇSik, M., Ritschel, T., Kˇriv´anek, J.: On-line learning of parametric mix-\nture models for light transport simulation. ACM Transactions on Graphics (Proceedings of\nSIGGRAPH 2014) 33(4) (2014) 1, 1, 3.3, 5, 6\n34. Vorba, J., Kˇriv´anek, J.: Adjoint-driven Russian roulette and splitting in light transport simulation.\nACM Transactions on Graphics (Proceedings of SIGGRAPH 2016) 35(4), 1–11 (2016) 5\n35. Vose, M.: A linear algorithm for generating random numbers with a given distribution. IEEE\nTrans. on Software Engineering 17(9), 972–975 (1991) 4.2\n36. Wald, I., Benthin, C., Slusallek, P.: Interactive global illumination in complex and highly\noccluded environments. In: P. Christensen, D. Cohen-Or (eds.) Rendering Techniques 2003\n(Proc. 14th Eurographics Workshop on Rendering), pp. 74–81 (2003) 4\n37. Wald, I., Kollig, T., Benthin, C., Keller, A., Slusallek, P.: Interactive global illumination using\nfast ray tracing. In: P. Debevec, S. Gibson (eds.) Rendering Techniques 2002 (Proc. 13th\nEurographics Workshop on Rendering), pp. 15–24 (2002) 4\n38. Ward, G.: Adaptive shadow testing for ray tracing.\nIn: 2nd Eurographics Workshop on\nRendering. Barcelona, Spain (1991) 4\n39. Watkins, C., Dayan, P.: Q-learning. Machine learning 8(3), 279–292 (1992) 2\nPage: 15\njob: LearningLT\nmacro: svmult.cls\ndate/time: 16-Aug-2017/0:22\n",
  "categories": [
    "cs.LG",
    "cs.GR"
  ],
  "published": "2017-01-25",
  "updated": "2017-08-15"
}