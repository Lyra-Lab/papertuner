{
  "id": "http://arxiv.org/abs/1702.02211v2",
  "title": "Fixing the Infix: Unsupervised Discovery of Root-and-Pattern Morphology",
  "authors": [
    "Tarek Sakakini",
    "Suma Bhat",
    "Pramod Viswanath"
  ],
  "abstract": "We present an unsupervised and language-agnostic method for learning\nroot-and-pattern morphology in Semitic languages. This form of morphology,\nabundant in Semitic languages, has not been handled in prior unsupervised\napproaches. We harness the syntactico-semantic information in distributed word\nrepresentations to solve the long standing problem of root-and-pattern\ndiscovery in Semitic languages. Moreover, we construct an unsupervised root\nextractor based on the learned rules. We prove the validity of learned rules\nacross Arabic, Hebrew, and Amharic, alongside showing that our root extractor\ncompares favorably with a widely used, carefully engineered root extractor:\nISRI.",
  "text": "arXiv:1702.02211v2  [cs.CL]  11 Feb 2017\nFixing the Inﬁx: Unsupervised Discovery of Root-and-Pattern\nMorphology\nTarek Sakakini\nUniversity of Illinois\nSuma Bhat\nUniversity of Illinois\nPramod Viswanath\nUniversity of Illinois\nAbstract\nWe present an unsupervised and language-\nagnostic method for learning root-and-\npattern morphology in Semitic languages.\nWe harness the syntactico-semantic in-\nformation in distributed word representa-\ntions to solve the long standing problem\nof root-and-pattern discovery in Semitic\nlanguages. The root-and-pattern morpho-\nlogical rules we learn in an unsupervised\nmanner are validated by native speakers\nin Arabic, Hebrew, and Amharic. Further,\nthe rules are used to construct an unsuper-\nvised root extractor called JZR, which we\nshow to compare favorably with ISRI, a\nwidely used and carefully engineered root\nextractor for Arabic.\n1\nIntroduction\nMorphological analysis is a core task in natural\nlanguage processing and hence of central inter-\nest in computational linguistics (Kurimo et al.,\n2010)\nwith\nwidespread\napplications\nbene-\nﬁts (Korenius et al., 2004; Skorkovsk´a, 2012;\nSeddah et al.,\n2010;\nSemmar et al.,\n2006;\nLarkey et al., 2002).\nComputational approaches\nto morphological analysis for lemmatization and\nstemming rely on knowledge of the mapping\nfrom inﬂected forms to lemmas.\nHowever,\nthe more widely used systems that learn these\nmappings are restricted to handle only concate-\nnative morphology, learning overt afﬁxes and\ntheir interactions with stems (Creutz and Lagus,\n2005; Soricut and Och, 2015; Lee et al., 2011;\nGoldsmith, 2000; Pasha et al., 2014; Buckwalter,\n2004).\nAs an example, consider the agentive\nformation in Arabic (Table 1):\n(to write, ktb)\nin the root form to (writer, kAtib) in the agent\nform.\nUnlike in English, where the agentive\nRoot\nDerived\nTemplate\nktb\nkAtib (writer)\nC1AC2iC3\nktb\nmaktab (desk)\nmaC1C2aC3\nTable 1: Illustration of Arabic root-and-pattern\nrules on the root ktb (to write).\nformation occurs as a concatenation of the sufﬁx\n“er”, in Arabic, the process involves applying a\nspeciﬁc pattern (C1AC2iC3, in the notation of\nSection 2) to the root ktb to form the word kAtib.\nSuch processes, which constitute a signiﬁcant\ncomponent in Semitic languages, are not captured\nby existing unsupervised systems.\nThe focus\nof this study is a computational approach to\nan important non-concatenative process–that of\nroot-and-pattern morphology.\nCurrent\napproaches\nfor\nroot-and-\npattern\nmorphology\ninvolve\nrule-based\nroot\nextraction\n(Khoja and Garside,\n1999;\nTaghva et al.,\n2005;\nAbabneh et al.,\n2012;\nEl-Beltagy and Rafea, 2011; Alhanini et al., 2011;\nAl-Shalabi and Evens, 1998), and others that re-\nquire training data of word-root pairs (Attia et al.,\n2016; Al-Serhan and Ayesh, 2006), all developed\nfor processing Arabic.\nThe resource-intensive\nnature of these methods not only limits their wider\napplicability to other Semitic languages but also\nprevents from handling the productive process\nmore generally.\nThis\npaper\ndescribes\na\nlanguage-agnostic\n(within the Semitic language family) algorithm\nthat learns root-and-pattern, as well as concate-\nnative morphology in an unsupervised manner re-\nsulting in a root extractor. A novel aspect of our\napproach is the use of word semantics made possi-\nble by relying on distributed word representations.\nSuch a reliance on word representations permits a\ndifferent mechanism of morpheme segmentation,\nwhich is being performed by an abstraction that\nis not obvious at the surface level.\nQualitative\nanalyses show perfect rules acquired for the 3 lan-\nguages. Quantitative analyses of root extraction on\nArabic show performance comparable to carefully\nengineered systems.\n2\nRoot-and-Pattern Morphology\nIllustration\nWe explain root-and-pattern morphology with\nan example.\nOur examples will take the\nform (English translation, English transliteration,\nConsonant-only transliteration), and translitera-\ntion will follow the standard in (Lagally, 1992).\nIn Semitic languages, a pattern (bound morpheme)\nis applied to a root (free morpheme) to gener-\nate a word.\nIn general, the roots consist of\n3 consonants; example: (to write, kataba, ktb)\n(Aljlayl and Frieder, 2002). The patterns encode\nthe placement of the 3 consonants with respect\nto the added letters, an example of which is,\nC1AC2iC3. Here, the vowel “A” is placed between\nthe ﬁrst and the second consonant and the vowel\n“i” between the second and the third consonant.\nApplying this pattern to the 3-consonant root in\nour example, the resulting word is (writer, kAtib,\nktb). This pattern encodes the semantic role of\n“Agent” and hence kAtib is the agent of the verb\nkataba.\n3\nLearning of Morphological Rules\nThe task of morphological rule learning is, given a\ncorpus, we would like to extract patterns that gov-\nern root-and-pattern morphology as well as afﬁxes\nthat govern concatenative morphology. Our pro-\nposed method assumes three models. An ortho-\ngraphic model for concatenative morphology and\nan orthographic model for root-and-pattern mor-\nphology signal all possible morphological (can-\ndidate) rules and all possible (candidate) pairs\nof morphologically related words from an ortho-\ngraphic perspective. Then a semantic model vali-\ndates whether a candidate rule is morphologically\nvalid and whether a pair of words is morphologi-\ncally related.\nExisting approaches towards root-and-pattern\nmorphology in the literature tend to be a set of\northographic rules deﬁning the model; the heavy\nreliance on expert linguistic rules naturally leads\nto poor coverage. Besides, the orthographic rules\ndisregard the semantically motivated morpholog-\nical transformation. For example, in English, the\npair of words (on, only) shows orthographically\na morphological relation via the sufﬁx “ly”. Dis-\nregarding semantic knowledge in such a situation\nwould lead to a false morphological analysis. This\nmotivates our approach which models the relation\nbetween roots, patterns, and words through a com-\nbination of orthographic rules and semantic infor-\nmation available in distributed word representa-\ntions.\n3.1\nOrthographic Models\nTo detect candidate morphological rules from an\northographic standpoint, we deﬁne two models,\none for concatenative morphology, and one for\nroot-and-pattern morphology.\n3.1.1\nConcatenative Morphology\nIn concatenative morphology, afﬁxes attach to a\nstem (Siegel, 1974). Thus we model concatena-\ntive morphology as the process of deletions from\none end of the word followed by insertions on the\nsame end. For example, the words (desk, mak-\ntab, mktb) and (the desk, almaktab, lmktb) are\nconsidered morphologically related from an ortho-\ngraphic standpoint whereby there was 0 deletions\nin the beginning of the word followed by two in-\nsertions (a,l). This would contribute to the candi-\ndate rule (preﬁx, φ, al). Another example would\nbe the pair of words (the desk, almaktab, lmktb)\nand (and desk, wamaktab, wmktb). Based on this\nmodel, they would be linked via two deletions (a,l)\nand two insertions (w,a), and this pair would con-\ntribute to the rule (preﬁx, al, wa). We found a max-\nimum of 6 insertions and deletions to be sufﬁcient\nto model the concatenative relation.\n3.1.2\nRoot-and-Pattern Morphology\nAll Semitic languages share the following two\nproperties of relations between roots and their\npattern-derived words on an orthographic level\n(Greenberg, 1950):\n• Roots are predominantly tri-literal.\n• The root consonants (C1, C2, C3) retain their\nrelative order after combining with a pattern\nto form a word.\nFor example, in Table 1, the word kAtib would\nbe compared to the tri-literal root ktb, and conse-\nquently, the template C1AC2iC3 would be consid-\nered as a candidate root-and-pattern template.\n3.2\nSemantic Model\nNot every pair of words obeying the orthographic\nproperties, constitutes a morphologically related\npair. As an example, (gone, rA.h) and (wounds,\njirA.h) are connected orthographically via the rule\n(preﬁx, φ, ji) although they’re not morphologi-\ncally related.\nIn order to ascertain that a pair\nof words obeying the orthographic properties are\nindeed morphologically related, we make use of\nthe semantic relationship between the words via\nword representations. This is because word repre-\nsentations and their geometry in the vector space\nhave been shown to encode syntactico-semantic\nrelations (Mikolov et al., 2013b).\nFor example,\nconsider the two pairs: (1) ((wrote, kataba, ktb),\n(desk, maktab, mktb)), and, (2) ((played, la‘iba,\nl‘b), (playing ﬁeld, mal‘ab, ml‘b)). We can verify\nthat ⃗vkataba ´ ⃗vmaktab « ⃗vla‘iba ´ ⃗vmal‘ab, to con-\nﬁrm that orthographically related word pairs are\nindeed morphologically related, here through the\npattern (maC1C2aC3) which means the place of\nthe action. On the other hand, alluding to the pre-\nvious example r = (preﬁx, φ, ji), difference vectors\nof elements of the support set of this rule would\nnot show such regularities in the vector space.\n3.3\nAlgorithmic Implementation\nOur approach is inspired by (Soricut and Och,\n2015), which, being limited to concatenative mor-\nphology, serves as the primary mechanism to\ngenerate the concatenative morphological rules\nfor the languages in our study.\nOur contribu-\ntion is in adapting their approach to handle non-\nconcatanative morphology. Our system is imple-\nmented in 4 steps:\n1. Generation of the vocabulary set V and word\nembeddings from corpus (Word representa-\ntion);\n2. Generation of candidate (root, derived) pairs\ngrouped into sets based on the underlying or-\nthographic transformation. These sets repre-\nsent the morphological rules in our system.\n(Candidate generation);\n3. Validation of candidate morphological rules\nusing the semantic and orthographic model\n(Validation of candidate rules).\n4. Validation of candidate rule elements as\nmorphologically\nrelated\npairs\nof\nwords\n(Validation of rule elements).\nThe result is a set of morphological rules and their\nelements, scored semantically and orthographi-\ncally.\nStep 1: Word representation.\nWe rely on\na large enough corpus to generate a vocabulary\nset V made up of all the word types appear-\ning in the corpus, which is then used to generate\nword embeddings for the vocabulary using an al-\ngorithm such as Word2Vec (Mikolov et al., 2013a)\nor Glove (Pennington et al., 2014).\nStep 2: Candidate generation. All pairs of\nwords morphologically related by the same ortho-\ngraphic transformation are grouped into one set to\nrepresent a candidate morphological rule. For ex-\nample, the concatenative rule r = (preﬁx, φ, “al”)\nwould be represented by the support set SSr =\n{(maktab, almaktab), (jaras, aljaras), ...}. Simi-\nlarly, the rule r = (maC1C2aC3), shown in Table\n1, would have SSr = {(ktb, maktab), (l’b, mal’ab),\n...}. We should note that not all generated rules in\nthis step are valid morphologically. We refer to the\nset of all rules as R.\nStep 3: Validation of candidate rules. Due to\nthe “overgeneration” of candidate rules in the pre-\nvious step, we prune the rules using a semantic and\nan orthographic score. Formally, for a given rule\nr with support set SSr, scorer sem(r) = |{(w1, w2),\n(w3, w4) P SSr | cos(w4, w2 - w1 + w3) ą tcos sim}|\ndivided by |SSr|2, where tcos sim is a threshold ap-\npropriately chosen. Also, for every rule r, |SSr|\nreﬂects the quality of r from an orthographic point\nof view. This is captured via scorer orth(r) = |SSr|.\nStep 4: Validation of rule elements. The or-\nthographic model not only overgenerates candi-\ndate rules but also overgenerates pairs of words be-\nlonging to a speciﬁc rule. As an example, consider\nthe words (to go, zhb) and (religion, mazhab) –\nthis pair belongs to the valid root-and-pattern rule\nguided by the pattern maC1C2aC3, and yet, the\nwords are not morphologically related. To score\ninstances within a rule r as valid instances we de-\nploy this semantic score: scorew sem((w1, w2) P\nr) = |{(w3, w4) P SSr | cos(w4, w2 - w1 + w3)\ną tcos sim}| divided by |SSr|. In other words, we\ncheck how well the difference vector of the pair\nof interest ﬁts with the difference vectors of other\npairs within the rule support set SSr.\n4\nJZR: Root Extractor\nWe cast the root extraction task as an iterative op-\ntimization problem. Let Radd be all concatenative\nrules of the form (afﬁx, φ, afﬁx added), as well as\nall root-and-pattern rules, and Rrep be all concate-\nnative rules of the form (afﬁx, afﬁx deleted, afﬁx\nadded). Given a word w whose root needs to be\nextracted, we search for rule r˚, the solution to the\nfollowing optimization problem:\nmax\nr\nscorew semprq\nsubject to\nr P Radd\nscorer semprq ą tr sem\nscorer orthprq ą tr orth\nscorew semppw1, wq P SSrq ą tw sem\nIn the above optimization problem, tr sem, tr orth,\ntw sem are tunable hyperparameters. The solution\nr˚ and w uniquely identify w1. Thus the system\nextracts w1 and iterates over w1 until it reaches a\ntriliteral word. At any stage, if the optimization\nproblem is infeasible we repeat it over Rrep instead\nof Radd. Note that we only consider rules which re-\nsult in a w1 of length less than w to correctly model\nthe chain of morphological processes as well to\nguarantee convergence of algorithm.\nThe system presented here is readily adaptable\nto other morphological tasks, such as morpheme\nsegmentation and morphological reinﬂection and\nnot discussed in this paper.\n5\nExperiments and Results\nIn this section, we evaluate our method in multiple\nways on 3 different languages (Arabic, Hebrew,\nAmharic).\nWe ﬁrst evaluate our method quali-\ntatively by checking the root-and-pattern rules it\ndiscovers in the 3 languages.\nThen we evalu-\nate JZR quantitatively by comparing its accuracy\nagainst the widely used ISRI Arabic root extractor\n(Taghva et al., 2005) on a sample of 1200 words.\n5.1\nExperimental Setup\nFor each of the three languages, we use the readily\navailable Polyglot1 word representations and its\nvocabulary. These 64d word representations were\ncreated based on the Wikipedia in the respective\nlanguage. Arabic and Hebrew embeddings were\nlimited to the top 100K words whereas Amharic,\nbeing a low-resource language was restricted to\nthe top 10K words. Our hyperparameters (tuned\nto ISRI output) were set to the following: tr sur =\n20, tr sem = 0.1 , tw sem = 0.1, tcos sim = 0.5.\n1https://sites.google.com/site/rmyeid/projects/polyglot\nJZR\nJZR (limited)\nISRI\n51.88%\n45.63 %\n61.63%\nTable 2: Performance of the three root extractors\non a sample of 1200 Arabic words.\n5.2\nEvaluation of Root-and-Pattern rules\nWe validate root-and-pattern rules across the\n3 languages by ranking the rules in terms of\nscorer semprq and have a native speaker of each\nlanguage evaluate the top 30 rules. All 30 rules in\neach of the 3 languages were deemed correct, val-\nidating the language-agnosticity and performance\nof our unsupervised approach.\n5.3\nIntrinsic Evaluation on Arabic\nIn this experiment, we consider 3 Arabic root ex-\ntractors: JZR, JZR (limited), and ISRI, and evalu-\nate them on a sample of 1200 words. JZR (limited)\nis a version of JZR limited to concatenative mor-\nphology. Comparison against it reﬂects the added\nvalue of the discovered root-and-pattern rules.\nFrom the results (summarized in Table 2), we\nnotice: (1) JZR compares favorably with ISRI,\na carefully engineered rule-based and language-\nspeciﬁc root extractor (2) Limiting JZR to con-\ncatenative morphology led to an 11.5% relative\ndrop in scores. This reﬂects the signiﬁcance of the\nnon-concatenative rules captured by JZR. We also\nclaim that this drop is an underestimate of the sig-\nniﬁcance of the learned root-and-pattern rules. We\ndiscuss this claim in detail in Appendix A, along\nwith further analyses of the results and sample out-\nputs.\n6\nConclusion\nThis work presents an unsupervised method for\nthe discovery of root-and-pattern morphology in\nSemitic languages. The discovered rules are used\nto extract Semitic roots, which are the basic units\nof these languages. Intrinsic and extrinsic evalu-\nations of these rules allow us to validate our pat-\ntern discovery method as well as our root extrac-\ntion method (JZR), with performance not too far\nfrom a rule-based language-speciﬁc (in this case\nArabic) root extractor.\nReferences\nMohamad\nAbabneh,\nRiyad\nAl-Shalabi,\nGhassan\nKanaan, and Alaa Al-Nobani. 2012. Building an ef-\nfective rule-based light stemmer for arabic language\nto improve search effectiveness. International Arab\nJournal of Information Technology (IAJIT) 9(4).\nH.\nAl-Serhan\nand\nA.\nAyesh.\n2006.\nA triliteral word roots extraction using neural network for arabic.\nIn\n2006\nInternational\nConference\non\nCom-\nputer Engineering and Systems. pages 436–440.\nhttps://doi.org/10.1109/ICCES.2006.320487.\nRiyad\nAl-Shalabi\nand\nMartha\nEvens.\n1998.\nA computational morphology system for arabic.\nIn\nProceedings of\nthe Workshop\non\nCompu-\ntational\nApproaches\nto\nSemitic\nLanguages.\nAssociation\nfor\nComputational\nLinguistics,\nStroudsburg, PA, USA, Semitic ’98, pages 66–72.\nhttp://dl.acm.org/citation.cfm?id=1621753.1621765.\nYasir Alhanini, Mohd Juzaiddin Ab Aziz, et al.\n2011. The enhancement of arabic stemming by us-\ning light stemming and dictionary-based stemming.\nJournal of Software Engineering and Applications\n4(09):522.\nMohammed\nAljlayl\nand\nOphir\nFrieder.\n2002.\nOn arabic search: Improving the retrieval effectiveness via a light stemming approach.\nIn Proceedings of the Eleventh International Confer-\nence on Information and Knowledge Management.\nACM, New York, NY, USA, CIKM ’02, pages\n340–347. https://doi.org/10.1145/584792.584848.\nMohammed Attia, Ayah Zirikly, and Mona Diab. 2016.\nThe power of language music: Arabic lemmatization\nthrough patterns. COLING 2016 page 40.\nTim Buckwalter. 2004. Buckwalter arabic morpholog-\nical analyzer version 2.0. linguistic data consortium,\nuniversity of pennsylvania, 2002. ldc cat alog no.:\nLdc2004l02. Technical report, ISBN 1-58563-324-\n0.\nMathias Creutz and Krista Lagus. 2005. Unsupervised\nmorpheme segmentation and morphology induction\nfrom text corpora using Morfessor 1.0.\nSamhaa R El-Beltagy and Ahmed Rafea. 2011.\nAn\naccuracy-enhanced light stemmer for arabic text.\nACM Transactions on Speech and Language Pro-\ncessing (TSLP) 7(2):2.\nJohn Goldsmith. 2000. Linguistica: An automatic mor-\nphological analyzer. In Proceedings of 36th meeting\nof the Chicago Linguistic Society.\nJoseph H Greenberg. 1950. The patterning of root mor-\nphemes in semitic. Word 6(2):162–181.\nShereen Khoja and Roger Garside. 1999. Stemming\narabic text. Lancaster, UK, Computing Department,\nLancaster University .\nTuomo Korenius, Jorma Laurikkala, Kalervo J¨arvelin,\nand Martti Juhola. 2004. Stemming and lemmati-\nzation in the clustering of ﬁnnish text documents.\nIn Proceedings of the thirteenth ACM international\nconference on Information and knowledge manage-\nment. ACM, pages 625–633.\nMikko\nKurimo,\nSami\nVirpioja,\nVille\nTurunen,\nand\nKrista\nLagus.\n2010.\nMorpho challenge competition 2005–2010: Evaluations and results.\nIn\nProceedings\nof\nthe\n11th\nMeeting\nof\nthe\nACL\nSpecial\nInterest\nGroup\non\nComputa-\ntional\nMorphology\nand\nPhonology.\nAssocia-\ntion for Computational Linguistics, Stroudsburg,\nPA, USA, SIGMORPHON ’10,\npages 87–95.\nhttp://dl.acm.org/citation.cfm?id=1870478.1870489.\nKlaus Lagally. 1992. Arabtextypesetting arabic with\nvowels and ligatures. EuroTEX 92:153–172.\nLeah S Larkey, Lisa Ballesteros, and Margaret E Con-\nnell. 2002. Improving stemming for arabic infor-\nmation retrieval: light stemming and co-occurrence\nanalysis. In Proceedings of the 25th annual inter-\nnational ACM SIGIR conference on Research and\ndevelopment in information retrieval. ACM, pages\n275–282.\nYoong Keok Lee, Aria Haghighi, and Regina Barzi-\nlay. 2011.\nModeling syntactic context improves\nmorphological segmentation.\nIn Proceedings of\nthe Fifteenth Conference on Computational Natural\nLanguage Learning. Association for Computational\nLinguistics, pages 1–9.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a.\nEfﬁcient estimation of word\nrepresentations in vector space.\narXiv preprint\narXiv:1301.3781 .\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations.\nIn Hlt-naacl. volume 13,\npages 746–751.\nArfath Pasha, Mohamed Al-Badrashiny, Mona T Diab,\nAhmed El Kholy, Ramy Eskander, Nizar Habash,\nManoj Pooleery, Owen Rambow, and Ryan Roth.\n2014. Madamira: A fast, comprehensive tool for\nmorphological analysis and disambiguation of ara-\nbic. In LREC. volume 14, pages 1094–1101.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014.\nGlove: Global vectors for word\nrepresentation. In EMNLP. volume 14, pages 1532–\n43.\nDjam´e Seddah, Grzegorz Chrupała, ¨Ozlem C¸ etino˘glu,\nJosef Van Genabith, and Marie Candito. 2010.\nLemmatization and lexicalized statistical parsing of\nmorphologically rich languages: the case of french.\nIn Proceedings of the NAACL HLT 2010 First Work-\nshop on Statistical Parsing of Morphologically-Rich\nLanguages. Association for Computational Linguis-\ntics, pages 85–93.\nNasredine Semmar, Meriama Laib, and Christian\nFluhr. 2006.\nUsing stemming in morphological\nanalysis to improve arabic information retrieval.\nverbum ex machina page 317.\nDorothy Carla Siegel. 1974. Topics in English mor-\nphology..\nPh.D. thesis, Massachusetts Institute of\nTechnology.\nLucie Skorkovsk´a. 2012. Application of lemmatization\nand summarization methods in topic identiﬁcation\nmodule for large scale language modeling data ﬁl-\ntering. In International Conference on Text, Speech\nand Dialogue. Springer, pages 191–198.\nRadu Soricut and Franz Och. 2015. Unsupervised mor-\nphology induction using word embeddings. In Proc.\nNAACL.\nKazem Taghva, Rania Elkhoury, and Jeffrey S Coombs.\n2005. Arabic stemming without a root dictionary. In\nITCC (1). pages 152–157.\nA\nDiscussion\nWe believe that the drop in performance when lim-\niting JZR to concatenative morphology underesti-\nmates the abundance of root-and-pattern morphol-\nogy in Arabic. The reason is that the way we de-\nﬁne concatenative morphology could capture root-\nand-pattern morphology under strict conditions.\nFor the purpose of illustration we have collected\n5 representative examples into Table 3, where cor-\nrect roots are boldfaced. To illustrate this underes-\ntimate, consider as an example, the second word in\nTable 3. All extractors were able to get the second\nexample correct. Although, the word was derived\nusing a pattern, JZR (limited) was still able to get\nit right since the stem change was close to the edge\nof the word. To illustrate this further, a word like\nkitAb is stripped ﬁrst of the “i” using the rule (pre,\nki, k), and similarly stripped of the “A” using a suf-\nﬁx rule. These cases are limited, since to extract\nsuch a rule, this pattern should appear frequently\nwith a word starting (ending) with a “k” (“b”). The\nﬁrst example shows how the limitation to concate-\nnative morphology prevented JZR (limited) from\nremoving the pattern, leading to a non-root word.\nIn the third example, JZR fails for using a valid\nrule on an invalid pair of words, which reﬂects\nthe imperfections in the word embeddings’ linear\nstructure.\nFor purposes of comparison, we also show one-\nto-one comparisons of performance in Table 4.\nTwo key takeaways arise in this table. First, JZR\n(limited) never performs better than JZR, which\nshows the precision of discovered root-and-pattern\nWord\nJZR\nJZR (limited)\nISRI\nlilta‘Ayuˆs\n‘Aˆsa\nta‘Ayuˆs\n‘ayˆs\nfasAdaN\nfasada\nfasada\nfasada\nlilkusUr\nsUr\nsUr\nkasara\n.hukkAmaN\n.hakama\n.hakama\na.hkAm\nwayamta.s.s\nyamut\nyamut\nmta.s\nTable 3: Comparison of roots extracted using JZR\nagainst NLTK’s ISRI stemmer. Correct outputs are\nboldfaced.\nJZR\nJZR (limited)\nISRI\nJZR\n0\n63\n126\nJZR (limited)\n0\n0\n120\nISRI\n224\n281\n0\nTable 4:\nOne-to-one comparison of extractors.\nThe number in the cell shows how many times the\nextractor in that row performed better than the ex-\ntractor in the column.\nrules.\nSecond, JZR performed better than JZR\n(limited) on 63 occasions due to the discovery of\nroot-and-pattern morphology. Moreover, it is in-\nteresting to see that on multiple occasions JZR per-\nformed better than ISRI, which shows that rule-\nbased methods are insufﬁcient and unsupervised\nmethods are needed to ﬁll the gap.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-02-07",
  "updated": "2017-02-11"
}